[{"type": "text", "text": "Agent-to-Sim: Learning Interactive Behavior from Casual Videos ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Agent behavior simulation empowers robotics, gaming, movies, and VR appli  \n2 cations, but building such simulators often requires laborious effort of manually   \n3 crafting the agent\u2019s decision process and motion patterns. Recent advances in   \n4 visual tracking and motion capture have enabled learning agent behavior from   \n5 real-world data, but these methods are limited to a few scenarios due to the de  \n6 pendence on specialized sensors (e.g., synchronized multi-camera systems). In a   \n7 step towards scalable and realistic behavior simulators, we present Agent-to-Sim   \n8 (ATS), a framework for learning simulatable 3D agents in a 3D environment from   \n9 casually-captured monocular videos. To deal with partial views, our framework   \n10 fuses observations in a canonical space for both the agent and the scene, resulting   \n11 in a dense 4D spatiotemporal reconstruction. We then learn an interactive behavior   \n12 generator by querying paired data of agents\u2019 perception and actions from the 4D   \n13 reconstruction. ATS enables real-to-sim transfer of agents in their familiar envi  \n14 ronments given longitudinal video recordings captured with a smartphone over a   \n15 month. We show results on pets (e.g., cat, dog, bunny) and a person, and analyse   \n16 how the observer\u2019s motion and 3D scene affect an agent\u2019s behavior. ", "page_idx": 0}, {"type": "text", "text": "17 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "18 Consider the scene of the cat in the living room: where will the cat go   \n19 and how will it move? Since we have seen cats interact with the en  \n20 vironment and other people many times, we know that cats like to go   \n21 to the couch, often move slowly, and follow humans around, but run   \n22 away if people come too close. Such a predictive model of a phys  \n23 ical agent is what enables plausible behavior simulation, which is   \n24 essential for embodied intelligence, immersive virtual environments   \n25 and robot planning in safety-critical scenarios [9, 31, 41, 45, 54].   \n26 The key challenge with behavior simulation is how to generate plausible and interactive behavior   \n27 (with respect to the scene and other agents). On one hand, prior works [2, 6, 46] utilize trajectory   \n28 computed by path-planning algorithms or hand-designed logic from game simulators [13, 58]. While   \n29 these approaches benefit from high-quality trajectory data paired with perfect object and scene   \n30 geometries, it is laborious to manually craft simulators that suit the needs of each type of application,   \n31 and the data distribution is fundamentally different from the real world, leading to unnatural motion   \n32 and interactions. On the other hand, vision-based motion capture enables learning plausible behavior   \n33 directly from data for certain scenarios, such as autonomous driving [9], human body motion [21, 36],   \n34 and interaction with objects/scenes [14, 24]. However, due to the dependence on specialized sensor   \n35 (synchronized multi-camera systems, IMUs, pre-scanned objects), such systems does not scale well   \n36 to the full spectrum of natural behavior one may care about, such as behavior of animals, casual   \n37 events, and long-term activities. ", "page_idx": 0}, {"type": "image", "img_path": "fzdFPqkAHD/tmp/a466ebccecaa61119d1e99e639706f38a691aebae973a05d055946d5313ad1c0.jpg", "img_caption": [], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "image", "img_path": "fzdFPqkAHD/tmp/5c8fb239cd91a78fb86aa9e84e04cbc494b69c03c9d4cab6b07206c062028164.jpg", "img_caption": ["Observer A) 4D Spacetime Reconstruction "], "img_footnote": [], "page_idx": 1}, {"type": "image", "img_path": "fzdFPqkAHD/tmp/3a4143f293f89874ee15109a9b866a1f00deac875fbdde74502c2da6f2e72d7a.jpg", "img_caption": ["B) Interactive Behavior Simulator "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Learning agent behavior from longitudinal casual video recordings. We answer the following question: can we simulate the behavior of an agent, by learning from casually-captured videos of the same agent recorded across a long period of time (e.g., a month)? A) We first reconstruct videos in 4D (3D & time), which includes the scene, the trajectory of the agent, and the trajectory of the observer (i.e., camera held by observer). Such individual 4D reconstruction are registered across time, resulting in a complete 4D reconstructions. B) Then we learn a representation of the agent that allows for interactive behavior simulation. The behavior model explicitly reasons about goals, paths, and full body movements conditioned on the agent\u2019s ego-perception and past trajectory. Such agent representation allows us to simulate novel scenarios through conditioning. For example, conditioned different observer trajectories, the cat agent choose to walk to the carpet, stays still while quivering his tail, or hide under the tray stand. Please see videos and results of other agents in the supplement. ", "page_idx": 1}, {"type": "text", "text": "38 Recent advances in differentiable rendering [10, 12, 23, 38, 42, 52, 59, 65] and monocular MoCap [28,   \n39 43, 69, 70] provide a pathway to obtain high-quality models of scenes and agents from monocular   \n40 videos alone. Despite the potential of covering diverse data of agent behavior that match the real  \n41 world distributions, none of the existing works brings a solution of reconstructing dense 3D structures   \n42 of both the agent and scene, which is crucial for learning agent behavior grounded in real world   \n43 environments. To address this, we present ATS (Agent-to-Sim), a framework for learning simulatable   \n44 agent from casual videos captured over a long time horizon (e.g. 1 month), as shown in Fig. 1.   \n45 The crucial technical challenge is the presence of partial visibility \u2013 in each video captured from   \n46 an observer\u2019s viewpoint, only parts of the agent and the environment are visible. How do we infer   \n47 the states of agent and the environment that are not visible? To build a dense 4D spatiotemporal   \n48 reconstruction, our key insight is to leverage the observations from multiple videos by fusing them   \n49 in a canonical 3D space. We introduce a novel coarse-to-fine registration approach that re-purposes   \n50 \u201cfoundational\u201d visual features [40] as a neural localizer, which \u201cregisters\u201d the camera with respect   \n51 to a canonical structure. This enables capturing interactive behavior data in a casual setup (e.g.,   \n52 with a smartphone), and provides paired training data of perception and action of an agent that is   \n53 grounded in a natural environment (Fig. 2). To learn an interactive behavior model, we condition the   \n54 action of an agent on their ego-perception, and leverage diffusion models [18, 53] to account for the   \n55 multimodal nature of goals and planned trajectories. The resulting framework, ATS, can simulate   \n56 interactive behaviors like those described at the start: agents like pets that leap onto furniture, dart   \n57 quickly across the room, timidly approach nearby users, and run away if approached too quickly. Our   \n58 contributions are summerized as follows:   \n59 1. Agent-to-Sim (ATS) Framework. We introduce a real-to-sim framework, ATS, to learn   \n60 simulators of interactive agent behavior from casually-captured videos. ATS learns plausible   \n61 agent behavior that matches the real-world, and is scalable to diverse scenarios, such as   \n62 animal behavior and casual events.   \n63 2. Environment-Interactive Behavior Simulation. ATS learns behavior that is interactive   \n64 to the environment, including both the observer and 3D scene. We show the first result   \n65 of generating plausible behavior of animals that are reactive to observer\u2019s motion, and are   \n66 aware of the 3D scene.   \n67 3. Complete 4D Registration & Reconstruction. We present a method to register and   \n68 reconstruct a temporally-evolving 3D scene, whiling accounts for changes in scene layout   \n69 and appearance. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "table", "img_path": "fzdFPqkAHD/tmp/f2c090550aa1150760f7a1be85e6fbee47c5a75d6aa07ffb428472b4859f369c.jpg", "table_caption": ["Table 1: Related works in behavior data capture. ATS is the only method that builds a complete 4D reconstruction of both the agents and the environment. Different from prior work that focus on specific domains, ATS can be applied to capture interactive behavior of both animals and humans from casual RGBD videos (e.g. captured by a smartphone). "], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "70 2 Related Works ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "71 Behavior Prediction and Generation. Behavior prediction has a long history, starting from simple   \n72 physics-based models such as social forces [17] to more sophisticated \u201cplanning-based\u201d models that   \n73 cast prediction as reward optimization [26, 76], where the reward is learned via inverse reinforcement   \n74 learning [75]. With the advent of large-scale pedestrian and vehicle motion data collected in the   \n75 navigation and autonomous driving domains [1, 34, 37, 48, 50], generative prediction models such as   \n76 diffusion models have been able to express behavior multi-modality while being easily controlled via   \n77 additional signals such as cost functions [20] or logical formulae [74]. However, to capture plausible   \n78 behavior of agents, these approaches are extremely dependant on high-quality agent trajectory data   \n79 collected \u201cin the wild\u201d with the associated scene context (e.g., 3D map of the scene) [9]. Such data are   \n80 often manually annotated at a bounding box level (Tab. 1), which limits the scale and the level of detail   \n81 they can capture. Beyond autonomous driving setup, existing works for human motion prediction and   \n8 generation [46, 57, 62] have been primarily using simulated data [6] or motion capture data collected   \n83 with multiple synchronized cameras [14, 24, 36]. Such data provide high-quality full body motion   \n84 of human using parametric body models [32], but the interactions with the environment are often   \n85 restricted to a set of pre-defined furnitures and objects [15, 29, 73]. Furthermore, the use of simulated   \n86 data and motion capture data inherently limits the realism of these behavior generators, since real   \n87 agents will behave very differently in their familiar environment. To bridge the gap, we develop   \n88 4D reconstruction method to obtain high-quality trajectories of agents in their natural environment,   \n89 with a simple setup that can be achieved with a smartphone. Close to our setup, ActionMap [47]   \n90 associate daily actions performed by a human agent with an reconstructed 3D environment given   \n91 egocentric videos. However, they focus on actions performed by hand and do not reconstruct the full   \n92 body motion of the agent.   \n93 4D Reconstruction from Monocular Videos. Reconstructing agents and the environment from   \n94 monocular videos is challenging due to its under-constrained nature. Given a monocular video,   \n95 there are multiple different interpretations of the underlying 3D geometry, motion, appearance,   \n96 and lighting [56]. As such, reconstructing agents often require category-specific 3D prior (e.g., 3D   \n97 humans) [11, 27, 32]. Along this line of work, researchers reconstruct 3D humans aligned to the world   \n98 coordinate with the help of SLAM and visual odometry [28, 69, 70]. Sitcoms3D [43] reconstructs   \n99 both the scene and human parameters, while relying on shot changes to determine the scale of the   \n00 scene. However, the use of parametric body models limits the degrees of freedom they can capture,   \n01 and makes it difficult to reconstruct agents from arbitrary categories which do not have a pre-built   \n02 body model, for example, animals. Another line of work avoids using category-specific 3D priors and   \n03 optimizes the shape and deformation parameters of the agent given richer visual signals (e.g., optical   \n04 flow and object silhouette) [61, 64, 65], which is shown to work well for a broad range of category   \n05 including human, animals, and vehicles. TotalRecon [52] further incorporates the background scene   \n06 into the model-free reconstruction pipeline, such that the agent\u2019s motion can be decoupled from the   \n07 camera motion and aligned to the scene space. However, none of the existing methods can reconstruct   \n08 both the agent and the scene in high-quality. In practice, individual videos may not contain sufficient   \n109 views, leading to inaccurate and incomplete reconstructions. Our method registers both the agent and   \n110 the environment from multiple videos into a shared space, which leverages large-scale data collection   \n111 to build a high-quality agent and scene model. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "112 3 Approach ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "113 We describe a method to learn interactive behavior models given longitudinal video recordings of an   \n114 agent in the same environment. We first build a spatiotemporal 4D reconstruction, including the agent,   \n115 the scene, and the observer (Sec. 3.1), which is solved by an optimization involving multi-video   \n116 registration (Sec. 3.2). We then train an interactive behavior model of the agent that is interactive   \n117 with the surrounding environment, including the scene and the motion of the observer (Sec. 3.3). ", "page_idx": 3}, {"type": "text", "text": "118 3.1 4D Representation: Agent, Scene, and Observer ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "119 Given multiple monocular videos, our goal is to build a dense spatiotemporal 4D reconstruction of   \n120 the underlying world, including a deformable agent, a background scene, and a moving observer.   \n121 The task is ill-posed due to partial visibility \u2013 from an observer\u2019s viewpoint, the agent and the   \n122 environment are only partially visible. To deal with this problem, one principle approach is geometric   \n123 registration, where structures not visible from one view can be inferred from the other views they   \n124 appear [51]. We build upon this idea to reconstruct a complete spatiotemporal model of an agent and   \n125 their familiar environment by registering videos captured at different time.   \n126 Problem Setup. Specifically, given images from $M$ videos represented by color and feature descrip  \n127 tors [40], $\\{\\mathbf{I}_{i},\\boldsymbol{\\psi}_{i}\\}_{i=\\{1,...,M\\}}$ , our goal is to find a 4D spatiotemporal representation that explains the   \n128 video, while pixels with the same semantics can be mapped to consistent canonical 3D locations. Our   \n129 representation factorizes the 4D structure into a static component and a time-varying component.   \n130 Static Representation. $\\mathbf{T}=\\{\\sigma,\\mathbf{c},\\psi\\}$ . We represent the static component as agent fields and scene   \n131 fields. Both define densities, colors, and semantic features in a canonical space, ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(\\sigma_{s},\\mathbf{c}_{s},\\psi_{s})=\\mathrm{MLP}_{s c e n e}(\\mathbf{X},\\beta_{i}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "132 ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(\\sigma_{a},\\mathbf{c}_{a},\\psi_{a})=\\mathrm{MLP}_{a g e n t}(\\mathbf{X}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "133 where $\\mathbf{X}$ corresponds to a 3D point. To account for structures that change across videos, we modify   \n134 the scene fields to take a per-video latent code $\\beta_{i}$ as input, which allows ftiting video-specific details.   \n135 Time-varying Representation. $\\mathcal{D}=\\{\\boldsymbol{\\xi},\\mathbf{G},\\mathbf{W}\\}$ . The time-varying component includes a moving   \n136 observer, represented by the camera pose $\\pmb{\\xi}_{t}\\in S E(3)$ , and the motion of an agent, represented by a   \n113378 tshete  oafg eringt icd abn obdei ems, a $\\{\\mathbf{G}_{t}^{b}\\}_{\\{b=1,\\dots,25\\}}$ ,e rrae fseprarceed  btoy  absl e\u201cnbdo-nskeisn\u201d.n iGngiv deenf oa rtimmatei $t$ ,n  t[h3e5 ,c a6n5]o,nical space of ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{X}_{t}=\\mathbf{G}^{a}\\mathbf{X}=\\left(\\sum_{b=1}^{B}\\mathbf{W}^{b}\\mathbf{G}_{t}^{b}\\right)\\mathbf{X},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "139 which computes the motion of a point by blending the bone transformations (we do so in the dual   \n140 quaternion space [22, 66] to ensure ${\\bf G}^{a}$ is a valid rigid transformation). The skinning weights $\\mathbf{W}$ are   \n141 defined as the probability of a point assigned to each bone.   \n142 Rendering. To turn the 4D representation into images, we sample rays in the camera space, map   \n143 them separately to the canonical space of the scene and the agent with $\\mathcal{D}$ , and query values (e.g.,   \n144 density, color, feature) from corresponding fields of the scene and the agent. The values are then   \n145 combined before ray integration [39, 52]. Consequently, the rendered pixel values are compared   \n146 against the observations to update the world representation $\\{\\mathbf{T},\\mathcal{D}\\}$ .   \n147 Decoupling Agent Motion from Observer. $\\{\\mathbf{G}_{t}^{b}\\}_{\\{b=1,\\dots,25\\}}$ defines the motion of an agent with   \n148 respect to the observer. Given the observer, we compute the motion of the agent in the scene space as, ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{G}_{t}^{b\\rightarrow s}=\\xi_{t}^{-1}\\mathbf{G}_{t}^{b},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "149 where the results of extracted trajectories of the agent is shown in Fig. 2 ", "page_idx": 3}, {"type": "image", "img_path": "fzdFPqkAHD/tmp/27fc2ebb861e1e3822581e3a66ec8da863ed030d6196d6a7edeaf1ab8645fc54.jpg", "img_caption": ["Figure 2: Results of 4D reconstruction. Top: reference images and renderings of the reconstructions. The color on the background represents correspondence. The colored blobs on the agent body represent $B=25$ body parts of the agent (e.g., head is represented by the yellow blob). Bottom: Bird\u2019s eye view of the reconstructed scene and agent trajectories, registered to the same scene coordinate. Each colored line represents a unique video sequence where boxes and spheres indicate the starting and the end location. Please see videos and results on other agents in the supplement. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "150 3.2 Optimization: Multi-Video Registration ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "151 To deal with bad local optima caused by camera poses (Fig. 4), we design a coarse-to-fine registration   \n152 approach that globally aligns the cameras to a shared canonical space with a feed-forward network,   \n153 and then jointly optimizes the 3D structures while adjusting the cameras locally.   \n154 Initialization: Neural Localization. Due to the evolving nature of scenes across a long period   \n155 of time [55], there exist both global layout changes (e.g., furniture get rearranged) and appearance   \n156 changes (e.g., table cloth gets replaced), making it challenging to find accurate geometric corre  \n157 spondences [4, 5, 49]. With the observation that \u201cfoundational\u201d visual features have good 3D and   \n158 viewpoint awareness [3], we adapt them for camera localization. We learn a scene-specific neural ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "159 localizer that directly regresses the camera pose of an image with respect to a canonical structure, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\xi=f_{\\theta}(\\psi),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "160 where $f_{\\theta}$ is a ResNet-18 [16] and $\\psi$ is the DINOv2 [40] feature of the input image. We find it to   \n161 be more robust than geometric correspondence, while being more computationally efficient than   \n162 performing pairwise matches [49]. To learn the neural localizer, we first capture a walk-through video   \n163 and build a dense map of the scene. Then we use it to train the neural localizer by randomly sampling   \n164 camera poses ${\\bf G}^{*}=({\\bf R}^{*},{\\bf t}^{*})$ and rendering images on the fly, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\arg\\operatorname*{min}_{\\theta}\\sum_{j}\\left(\\|\\log(\\mathbf{R}_{0}^{T}(\\theta)\\mathbf{R}^{*})\\|+\\|\\mathbf{t}_{0}(\\theta)-\\mathbf{t}^{*}\\|_{2}^{2}\\right),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "165 where we use geodesic distance [19] for camera rotation and $L_{2}$ error for camera translation. For the   \n166 agent, we follow BANMo [65] to initialize the root pose $\\{\\mathbf{G}^{b}\\}_{b=0}$ with a pre-trained pose network.   \n167 Objective: Feature-metric Alignemnt. Given a coarse initialization of the observer (scene camera)   \n168 and the agent\u2019s root pose, we use both photometric and featuremetric losses to optimize $\\{\\mathbf{T},\\mathcal{D}\\}$ , ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{T},\\mathcal{D}}\\sum_{t}\\big(\\|I_{t}-\\mathcal{R}_{I}(t;\\mathbf{T},\\mathcal{D})\\|_{2}^{2}+\\|\\psi_{t}-\\mathcal{R}_{\\psi}(t;\\mathbf{T},\\mathcal{D})\\|_{2}^{2}\\big)+L_{r e g}(\\mathbf{T},\\mathcal{D}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "169 where $\\mathcal{R}(\\cdot)$ is the rendering function described in Sec 3.1. In contrast to prior works, using feature  \n170 metric errors makes the optimization robust to change of lighting, appearance, and helps find accurate   \n171 alignment over multiple videos (Fig. 4). The regularization term includes eikonal loss, silhouette loss,   \n172 flow loss and depth loss similar to prior works [52, 65].   \n173 Scene Annealing. To encourage the reconstructed scene across videos to share a similar structure, we   \n174 randomly swap the code $\\beta$ of two videos during optimization, and gradually decrease the probability   \n175 of swaps from $\\mathcal{P}=1.0\\,\\rightarrow\\,0.05$ over the course of optimization. This regularizes the model to   \n176 effectively share information across all videos, and keeps video-specific details (Fig. 4). ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "177 3.3 Interactive Behavior Generation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "178 Now that we build a complete 4D reconstruction from multiple videos, we can extract a scene structure   \n179 $\\mathbf{T}$ , and $M$ trajectories of the agent $\\{{\\bf G}^{t}\\}_{t=\\{T_{1},\\dots,T_{M}\\}}$ as well as the observer $\\{\\pmb{\\xi}^{t}\\}_{t=\\{T_{1},\\dots,T_{M}\\}}$   \n180 grounded in the environment. We aim to learn an agent that is interactive with the world.   \n181 Hierarchical Behavior Representation. We model the behavior of an agent by bone transformations   \n182 in the scene space $\\mathbf{G}\\in\\mathbb{R}^{6B\\times T^{*}}$ over a fixed time horizon $T^{*}=5.6\\mathrm{s}$ , . We design a hierarchical   \n183 model as shown in Fig. 3. The body motion $\\mathbf{G}$ is conditioned on path $\\mathbf{P}\\in\\mathbb{R}^{3\\times T^{*}}$ , which is further   \n184 conditioned on goal $\\bar{\\mathbf{Z}}\\in\\mathbb{R}^{3}$ . Such decomposition allows agents to react by predicting goals with low   \n185 latency   \n186 Goal Generation. We represent a multi-modal distribution of goals $\\mathbf{Z}\\in\\mathbb{R}^{3}$ by its score function   \n187 $s(\\mathbf{Z},\\sigma)\\in\\mathbb{R}^{3}$ [18, 53]. The score function is implemented as a coordinate MLP [38], ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\ns(\\mathbf{Z};\\sigma)=\\operatorname{MLP}_{\\theta\\mathbf{z}}(\\mathbf{Z},\\sigma),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "188 trained by predicting the amount of noise $\\epsilon$ added to the clean goal, given the corrupted goal $\\mathbf{Z}+\\epsilon$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\underset{\\theta_{\\mathbf{Z}}}{\\arg\\operatorname*{min}}\\mathbb{E}_{Z}\\mathbb{E}_{\\sigma\\sim q\\left(\\sigma\\right)}\\mathbb{E}_{\\epsilon\\sim\\mathcal{N}\\left(\\mathbf{0},\\sigma^{2}I\\right)}\\left\\|\\mathrm{MLP}_{\\theta_{\\mathbf{Z}}}(Z+\\epsilon;\\sigma)-\\epsilon\\right\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "189 Compared to methods directly learning the multi-modal distribution [8, 25], diffusion models are   \n190 easy to train and can be used to generate diverse and high-quality samples [18, 53]. ", "page_idx": 5}, {"type": "text", "text": "191 Path Generation with Control. To guide path generation with goals, we represent its score as ", "page_idx": 5}, {"type": "equation", "text": "$$\ns(\\mathbf{P};\\sigma)=\\operatorname{ControlUNet}_{\\theta_{\\mathbf{P}}}(\\mathbf{P},\\mathbf{Z},\\sigma),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "192 where the Control UNet contains two standard UNets with the same architecture [72], one performing   \n193 unconditional generation taking $(\\mathbf{P},\\sigma)$ as input, another injecting goal conditions densely into the   \n194 neural network blocks of the first one taking $(\\mathbf{Z},\\sigma)$ as inputs. Compared to concatenating the goal   \n195 condition to the noise latent, this encourages close alignment between the goal and the path [62]. We   \n196 apply the same architecture to control pose generation with paths, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{s(\\mathbf{G};\\sigma)=\\mathrm{ControlUNet}_{\\theta_{\\mathbf{G}}}(\\mathbf{G},\\mathbf{P},\\sigma).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "image", "img_path": "fzdFPqkAHD/tmp/4b6d8df3dc909c31b0cdfde249b4375357f2a0f99ec396586879056373cdb990.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 3: Pipeline for behavior generation. We first encode egocentric information into a perception code $\\omega$ and then generate full body motion in a hierarchical fashion. We start by generating goals $\\mathbf{Z}$ with low latency, and then generate a path $\\mathbf{P}$ and body motion $\\mathbf{G}$ conditioned on the previous node. Each node is represented by the gradient of its log distribution, trained with the denoising objectives (Eq. 9). Given $\\mathbf{G}$ , the dense deformation of an agent can be computed via blend skinning (Eq. 3). ", "page_idx": 6}, {"type": "text", "text": "197 Compared to concatenation, we observe better alignment between the path and the full body pose   \n198 using the Control Unet. ", "page_idx": 6}, {"type": "text", "text": "199 ", "page_idx": 6}, {"type": "text", "text": "200 Ego-Perception Encoding. To generate plausible interactive behaviors, we encode the world   \n201 egocentrically perceived by the agent, and use it to condition the behavior generation. We use the   \n202 reconstructed environment $\\mathbf{T}$ and the observer $\\xi$ as a proxy of the world, and transform them to the   \n203 egocentric coordinate of the agent, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{\\xi}^{s\\rightarrow a}=\\mathbf{G}_{b=0}^{-1}\\pmb{\\xi},\\quad\\mathbf{T}^{s\\rightarrow a}=\\mathbf{G}_{b=0}^{-1}\\mathbf{T}}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "204 Transforming the world to the egocentric coordinates avoids over-fitting to specific locations of the   \n205 scene (Tab. 2). To encode ego-perception of the scene, we querying feature values from $\\psi_{s}$ with a 3D   \n206 grid around the agent and extract a latent scene representation, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\omega_{s}=\\operatorname{ResNet3D}_{\\theta_{\\psi}}(\\psi_{s}).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "207 where $\\mathrm{ResNet3D}_{\\theta_{\\phi}}$ is a 3D ConvNet with residual connections, and $\\omega_{s}\\in\\mathbb{R}^{64}$ represents the scene   \n208 perceived by the agent. We encode the observer\u2019s motion in the past $T^{\\prime}=0.8\\mathrm{s}$ seconds with ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\omega_{o}=\\mathrm{MLP}_{\\theta_{o}}({\\pmb\\xi}^{s\\rightarrow a}),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "209 where $\\omega_{o}\\in\\mathbb{R}^{64}$ represents the observer perceived by the agent. Accounting for the external factors   \n210 from the \u201cworld\u201d enables interactive behavior generation, where the motion of an agent follows the   \n211 environment constraints and is influenced by the trajectory of the observer (Fig. 5). ", "page_idx": 6}, {"type": "text", "text": "212 History Encoding. We additionally encode the past motion of the agent in $T^{\\prime}$ seconds, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\omega_{p}=\\mathrm{MLP}_{\\theta_{p}}(\\mathbf{G}_{b=0}^{s\\rightarrow a}).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "213 By conditioning on the past motion, we can generate long sequences by chaining individual ones. ", "page_idx": 6}, {"type": "text", "text": "214 4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "215 Dataset. We collect the a dataset that emphasizes the casual interactions of an agent with their   \n216 familiar environment and the observer. It contains iPhone-captured RGBD video collections of 4   \n217 types of agents, including 26 videos of a cat, 3 videos of a dog, 2 videos of a bunny, and 2 videos of a   \n218 human. The time span of the video capture ranges from 1 day to a month, and each video contains 30   \n219 seconds to 2 minutes of content. The dataset is curated to contain diverse motion of agents, including   \n220 walking, lying down, eating, as well as diverse interaction patterns with the environment, including   \n221 following the camera, sitting on a coach, etc. Please refer to the supplement for more details. ", "page_idx": 6}, {"type": "text", "text": "222 4.1 4D Reconstruction of Agent & Scene ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "223 Implementation Details. We extract frames from the videos at 10 FPS, and use off-the-shelf models   \n224 to produce augmented image measurements, including object segmentation [68], optical flow [63],   \n225 DINOv2 features [40]. We use AdamW to first optimize the environment with featuremetric loss for   \n226 30k iterations, and then jointly optimize the environment and agent for another 30k iterations with a   \n227 combination of optical flow, silouette, and featuremetric losses. Optimization takes roughly 24 hours.   \n228 8 A100 GPUs used to optimize 26 videos (for the cat data), and 1 A100 GPU is used in a 2-3 video   \n229 setup (for dog, bunny, and human data).   \n230 Results. We run 4D reconstruction on all video sequences and report the results qualitatively. A visual   \n231 comparison on scene registration is shown in Fig. 2. Without the ability to register multiple videos,   \n232 TotalRecon produces protruded and misaligned structures (as pointed by the red arrow). In contrast,   \n233 our method reconstructs a single coherent scene. With featuremetric alignment (FBA) alone but   \n234 without a good camera initialization from neural localization (NL), our method produces inaccurate   \n235 reconstruction due to global misalignment in cameras poses. Removing FBA while keeping NL,   \n236 the method fails to accurately localize the cameras and produces noisy scene structures. Finally,   \n237 removing scene annealing procures lower quality scene structures due to lack of training views. A   \n238 visual comparison with TotalRecon (Single Video) is shown in Fig. 8, where we show that multiple   \n239 videos helps reconstructing a higher-quality agent, and a more complete scene. ", "page_idx": 6}, {"type": "image", "img_path": "fzdFPqkAHD/tmp/8c308a015c0093ebe2e613bc69178bbc005c42bc140772c2686dae2696598f5d.jpg", "img_caption": ["Figure 4: Comparison on multi-video scene reconstruction. We show a top-down visualization of the reconstructed scene using the bunny dataset. Compared to TotalRecon that does not register multiple videos, ATS produces higher-quality scene reconstruction. Neural localizer and featuremetric losses are shown important for camera registration. Scene annealing is important for reconstructing high-quality scenes from limited views in a video. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "240 4.2 Interactive Behavior Prediction ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "241 Dataset. We use the cat dataset for quantitative evaluation, where the data are split into a training set   \n242 of 22 videos and a validation set of 4 videos. The validation set is representative of three dominant   \n243 motion patterns of the agent: (1) trying to engage with the observer, (2) exploring the space and (3)   \n244 performing activities while not paying attention to the observer.   \n245 Implementation Details. To train the behavior model, we slice the reconstructed trajectory in   \n246 the training set into overlapping window of 6.4s, resulting in $12\\mathbf{k}$ data samples. We use AdamW   \n247 to optimize the parameters of the scores functions $\\{\\theta_{\\mathbf{Z}},\\theta_{\\mathbf{P}},\\theta_{\\mathbf{G}}\\}$ and the ego-perception encoders   \n248 $\\{\\bar{\\theta_{\\psi}},\\bar{\\theta_{o}},\\theta_{p}\\}$ for $120\\mathbf{k}$ steps with batch size 1024. Training takes 10 hours on a single A100 GPU.   \n249 Metrics. The behavior of an agent can be evaluated along multiple axes, and we focus on goal, path,   \n250 and body motion prediction. For goal prediction, we use a combination of displacement error (DE)   \n251 and minimum displacement error (minDE) [7]. The evaluation asks the model to produce ${\\bf K}{=}64$   \n252 samples. DE computes the avarage distance of the samples to the ground-truth, and minDE finds the   \n253 one closest to the ground-truth to compute the distance. For path and body motion prediction, we   \n254 use average displacement error (ADE) and minimum average displacement error (minADE), which   \n255 are similar to goal prediction, but additionally averages the distance over path and joint locations   \n256 before taking the min. When evaluating path prediction and body motion prediction, the output is   \n257 conditioned on the ground-truth goal and path respectively.   \n258 Comparisons. We re-purpose related methods and adapt them to our new setup of interactive   \n259 behavior prediction of animal agents. The quantitative results are shown in Tab. 2. To predict the goal   \n260 of an agent, classic methods build statistical models of how likely an agent visits a spatial location of   \n261 the scene, referred to as location prior [26, 76]. Given the extracted 3D trajectories of an agent in the   \n262 egocentric coordinate, we build a 3D preference map over 3D locations as a histogram, which can   \n263 be turned into probabilities and used to sample goals. Since this method does not take into account   \n264 of the scene and the observer, it fails to accurately predict the goal. We then re-purpose FaF [33]   \n265 (Fast-and-Furious), a data-driven approach for motion forecasting to our task. FaF takes the same   \n266 input as ATS but regresses the goal, path, and body poses. It produces worse results than ATS for   \n267 all metrics since directly regressing the target treats the underlying distribution as a unit-variance   \n268 Gaussian and fails to account for the multi-modal nature of agent behaviors.   \n269 Analysing Interactions. We analyse the agent\u2019s interactions with the environment and the observer   \n270 by removing the conditioning signals and study their influence on behavior prediction. In Fig. 5, we   \n271 show that by gradually removing conditional signals, the generated goal samples become more spread   \n272 out. In Tab. 2, we drop one of the conditioning signals at a time. Dropping the observer conditioning   \n273 increases the error in goal prediction, indicating observer\u2019s trajectory is helpful goal prediction.   \n274 Dropping the environment conditioning produces worse results on goal prediction (minDE: 0.395 vs   \n275 0.702) as well. Surprisingly, it does not affect path prediction. We posit that the scenarios in the test   \n276 set are too simple. Conditioned on ground-turth goals, it performs well even without environment   \n277 conditioning. Finally learning behavior generation in the world coordinates performs worse for all   \n278 metrics since it over-fits to specific locations in the scene. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "table", "img_path": "fzdFPqkAHD/tmp/b1079348aad5600f5cb46de3ba7977022d59da5aec0e5b151a04f536314b57b2.jpg", "table_caption": ["Table 2: Evaluation of interactive behavior prediction. We separately evaluate goal, path, and full body motion prediction. Metrics are displacement errors (DE) in meters and the lower the better. FaF [33] is re-purposed and re-trained with our data. "], "table_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "fzdFPqkAHD/tmp/b24d23764684e4f3cd4cd8deba006b3dac73bdce5c1a292761e7f74d1e8ba336.jpg", "img_caption": ["Figure 5: Analysis of conditioning signals. We show results of removing one conditioning signal at a time. Removing observer conditioning and past trajectory conditioning makes the sampled goals more spread out (e.g., regions both in front of the agent and behind the agent); removing the environment conditioning introduces infeasible goals that penetrate the ground and the walls. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "279 5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "280 We have presented a framework for learning interactive behavior of agents grounded in natural   \n281 environments. To achieve this, we turn multiple casually-captured video recordings into complete 4D   \n282 reconstructions including the agent, the environment, and the observer. Such data collected over a   \n283 long time period allows us to learn a behavior model of the agent that is reactive to the observer and   \n284 respects the environment constraints. We validate our design choices on casual video collections, and   \n285 show better results than prior work for 4D reconstruction and interactive behavior prediction. ", "page_idx": 8}, {"type": "text", "text": "286 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "287 [1] A. Alahi, K. Goel, V. Ramanathan, A. Robicquet, L. Fei-Fei, and S. Savarese. Social lstm:   \n288 Human trajectory prediction in crowded spaces. In Proceedings of the IEEE conference on   \n289 computer vision and pattern recognition, pages 961\u2013971, 2016.   \n290 [2] A. Bajcsy, A. Loquercio, A. Kumar, and J. Malik. Learning vision-based pursuit-evasion robot   \n291 policies. arXiv preprint arXiv:2308.16185, 2023.   \n292 [3] M. E. Banani, A. Raj, K.-K. Maninis, A. Kar, Y. Li, M. Rubinstein, D. Sun, L. Guibas,   \n293 J. Johnson, and V. Jampani. Probing the 3d awareness of visual foundation models. arXiv   \n294 preprint arXiv:2404.08636, 2024.   \n295 [4] E. Brachmann and C. Rother. Neural- Guided RANSAC: Learning where to sample model   \n296 hypotheses. In ICCV, 2019.   \n297 [5] E. Brachmann, T. Cavallari, and V. A. Prisacariu. Accelerated coordinate encoding: Learning to   \n298 relocalize in minutes using rgb and poses. In CVPR, 2023.   \n299 [6] Z. Cao, H. Gao, K. Mangalam, Q.-Z. Cai, M. Vo, and J. Malik. Long-term human motion   \n300 prediction with scene context. In Computer Vision\u2013ECCV 2020: 16th European Conference,   \n301 Glasgow, UK, August 23\u201328, 2020, Proceedings, Part I 16, pages 387\u2013404. Springer, 2020.   \n302 [7] Y. Chai, B. Sapp, M. Bansal, and D. Anguelov. Multipath: Multiple probabilistic anchor   \n303 trajectory hypotheses for behavior prediction. arXiv preprint arXiv:1910.05449, 2019.   \n304 [8] L. Dinh, D. Krueger, and Y. Bengio. Nice: Non-linear independent components estimation.   \n305 arXiv preprint arXiv:1410.8516, 2014.   \n306 [9] S. Ettinger, S. Cheng, B. Caine, C. Liu, H. Zhao, S. Pradhan, Y. Chai, B. Sapp, C. R. Qi, Y. Zhou,   \n307 et al. Large scale interactive motion forecasting for autonomous driving: The waymo open   \n308 motion dataset. In Proceedings of the IEEE/CVF International Conference on Computer Vision,   \n309 pages 9710\u20139719, 2021.   \n310 [10] H. Gao, R. Li, S. Tulsiani, B. Russell, and A. Kanazawa. Monocular dynamic view synthesis:   \n311 A reality check. Advances in Neural Information Processing Systems, 35:33768\u201333780, 2022.   \n312 [11] S. Goel, G. Pavlakos, J. Rajasegaran, A. Kanazawa\\*, and J. Malik\\*. Humans in 4D: Recon  \n313 structing and tracking humans with transformers. In ICCV, 2023.   \n314 [12] C. Guo, T. Jiang, X. Chen, J. Song, and O. Hilliges. Vid2Avatar: 3D Avatar Reconstruction   \n315 from Videos in the Wild via Self-supervised Scene Decomposition. CVPR, 2023.   \n316 [13] P. E. Hart, N. J. Nilsson, and B. Raphael. A formal basis for the heuristic determination of   \n317 minimum cost paths. IEEE transactions on Systems Science and Cybernetics, 4(2):100\u2013107,   \n318 1968.   \n319 [14] M. Hassan, D. Ceylan, R. Villegas, J. Saito, J. Yang, Y. Zhou, and M. J. Black. Stochastic   \n320 scene-aware motion prediction. In Proceedings of the IEEE/CVF International Conference on   \n321 Computer Vision, pages 11374\u201311384, 2021.   \n322 [15] M. Hassan, Y. Guo, T. Wang, M. Black, S. Fidler, and X. B. Peng. Synthesizing physical   \n323 character-scene interactions. arXiv preprint arXiv:2302.00883, 2023.   \n324 [16] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR,   \n325 pages 770\u2013778, 2016.   \n326 [17] D. Helbing and P. Molnar. Social force model for pedestrian dynamics. Physical review E, 51   \n327 (5):4282, 1995.   \n328 [18] J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. Advances in neural   \n329 information processing systems, 33:6840\u20136851, 2020.   \n330 [19] D. Q. Huynh. Metrics for 3d rotations: Comparison and analysis. Journal of Mathematical   \n331 Imaging and Vision, 35:155\u2013164, 2009.   \n332 [20] C. Jiang, A. Cornman, C. Park, B. Sapp, Y. Zhou, D. Anguelov, et al. Motiondiffuser: Con  \n333 trollable multi-agent motion prediction using diffusion. In Proceedings of the IEEE/CVF   \n334 Conference on Computer Vision and Pattern Recognition, pages 9644\u20139653, 2023.   \n335 [21] H. Joo, T. Simon, X. Li, H. Liu, L. Tan, L. Gui, S. Banerjee, T. Godisart, B. Nabbe, I. Matthews,   \n336 et al. Panoptic studio: A massively multiview system for social interaction capture. TPAMI, 41   \n337 (1):190\u2013204, 2017.   \n338 [22] L. Kavan, S. Collins, J. \u017d\u00e1ra, and C. O\u2019Sullivan. Skinning with dual quaternions. In Proceedings   \n339 of the 2007 symposium on Interactive 3D graphics and games, pages 39\u201346, 2007.   \n340 [23] B. Kerbl, G. Kopanas, T. Leimk\u00fchler, and G. Drettakis. 3d gaussian splatting for real-time   \n341 radiance field rendering. ACM Transactions on Graphics, 42(4):1\u201314, 2023.   \n342 [24] J. Kim, J. Kim, J. Na, and H. Joo. Parahome: Parameterizing everyday home activities towards   \n343 3d generative modeling of human-object interactions. arXiv preprint arXiv:2401.10232, 2024.   \n344 [25] D. P. Kingma and M. Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114,   \n345 2013.   \n346 [26] K. M. Kitani, B. D. Ziebart, J. A. Bagnell, and M. Hebert. Activity forecasting. In Computer   \n347 Vision\u2013ECCV 2012: 12th European Conference on Computer Vision, Florence, Italy, October   \n348 7-13, 2012, Proceedings, Part IV 12, pages 201\u2013214. Springer, 2012.   \n349 [27] M. Kocabas, N. Athanasiou, and M. J. Black. Vibe: Video inference for human body pose and   \n350 shape estimation. In CVPR, June 2020.   \n351 [28] M. Kocabas, Y. Yuan, P. Molchanov, Y. Guo, M. J. Black, O. Hilliges, J. Kautz, and   \n352 U. Iqbal. Pace: Human and camera motion estimation from in-the-wild videos. arXiv preprint   \n353 arXiv:2310.13768, 2023.   \n354 [29] J. Lee and H. Joo. Locomotion-action-manipulation: Synthesizing human-scene interactions   \n355 in complex 3d environments. In Proceedings of the IEEE/CVF International Conference on   \n356 Computer Vision (ICCV), 2023.   \n357 [30] A. Lerner, Y. Chrysanthou, and D. Lischinski. Crowds by example. In Computer graphics   \n358 forum, volume 26, pages 655\u2013664. Wiley Online Library, 2007.   \n359 [31] C. Li, R. Zhang, J. Wong, C. Gokmen, S. Srivastava, R. Mart\u00edn-Mart\u00edn, C. Wang, G. Levine,   \n360 W. Ai, B. Martinez, et al. Behavior-1k: A human-centered, embodied ai benchmark with 1,000   \n361 everyday activities and realistic simulation. arXiv preprint arXiv:2403.09227, 2024.   \n362 [32] M. Loper, N. Mahmood, J. Romero, G. Pons-Moll, and M. J. Black. SMPL: A skinned   \n363 multi-person linear model. SIGGRAPH Asia, 2015.   \n364 [33] W. Luo, B. Yang, and R. Urtasun. Fast and furious: Real time end-to-end 3d detection, tracking   \n365 and motion forecasting with a single convolutional net. In Proceedings of the IEEE conference   \n366 on Computer Vision and Pattern Recognition, pages 3569\u20133577, 2018.   \n367 [34] W.-C. Ma, D.-A. Huang, N. Lee, and K. M. Kitani. Forecasting interactive dynamics of   \n368 pedestrians with fictitious play. In Proceedings of the IEEE Conference on Computer Vision   \n369 and Pattern Recognition, pages 774\u2013782, 2017.   \n370 [35] T. Magnenat, R. Laperri\u00e8re, and D. Thalmann. Joint-dependent local deformations for hand   \n371 animation and object grasping. In Proceedings of Graphics Interface\u201988, pages 26\u201333. Canadian   \n372 Inf. Process. Soc, 1988.   \n373 [36] N. Mahmood, N. Ghorbani, N. F. Troje, G. Pons-Moll, and M. J. Black. Amass: Archive of   \n374 motion capture as surface shapes. In Proceedings of the IEEE/CVF international conference on   \n375 computer vision, pages 5442\u20135451, 2019.   \n376 [37] K. Mangalam, Y. An, H. Girase, and J. Malik. From goals, waypoints & paths to long term   \n377 human trajectory forecasting. In Proceedings of the IEEE/CVF International Conference on   \n378 Computer Vision, pages 15233\u201315242, 2021.   \n379 [38] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng. Nerf:   \n380 Representing scenes as neural radiance fields for view synthesis. In ECCV, 2020.   \n381 [39] M. Niemeyer and A. Geiger. Giraffe: Representing scenes as compositional generative neural   \n382 feature fields. In CVPR, pages 11453\u201311464, 2021.   \n383 [40] M. Oquab, T. Darcet, T. Moutakanni, H. V. Vo, M. Szafraniec, V. Khalidov, P. Fernandez,   \n384 D. Haziza, F. Massa, A. El-Nouby, R. Howes, P.-Y. Huang, H. Xu, V. Sharma, S.-W. Li,   \n385 W. Galuba, M. Rabbat, M. Assran, N. Ballas, G. Synnaeve, I. Misra, H. Jegou, J. Mairal,   \n386 P. Labatut, A. Joulin, and P. Bojanowski. Dinov2: Learning robust visual features without   \n387 supervision, 2023.   \n388 [41] J. S. Park, J. O\u2019Brien, C. J. Cai, M. R. Morris, P. Liang, and M. S. Bernstein. Generative agents:   \n389 Interactive simulacra of human behavior. In Proceedings of the 36th Annual ACM Symposium   \n390 on User Interface Software and Technology, pages 1\u201322, 2023.   \n391 [42] K. Park, U. Sinha, J. T. Barron, S. Bouaziz, D. B. Goldman, S. M. Seitz, and R. Martin-Brualla.   \n392 Nerfies: Deformable neural radiance fields. In ICCV, 2021.   \n393 [43] G. Pavlakos, E. Weber, M. Tancik, and A. Kanazawa. The one where they reconstructed 3d   \n394 humans and environments in tv shows. In European Conference on Computer Vision, pages   \n395 732\u2013749. Springer, 2022.   \n396 [44] S. Pellegrini, A. Ess, K. Schindler, and L. Van Gool. You\u2019ll never walk alone: Modeling social   \n397 behavior for multi-target tracking. In 2009 IEEE 12th international conference on computer   \n398 vision, pages 261\u2013268. IEEE, 2009.   \n399 [45] X. Puig, E. Undersander, A. Szot, M. D. Cote, T.-Y. Yang, R. Partsey, R. Desai, A. Clegg,   \n400 M. Hlavac, S. Y. Min, et al. Habitat 3.0: A co-habitat for humans, avatars, and robots. In The   \n401 Twelfth International Conference on Learning Representations, 2023.   \n402 [46] D. Rempe, Z. Luo, X. Bin Peng, Y. Yuan, K. Kitani, K. Kreis, S. Fidler, and O. Litany. Trace   \n403 and pace: Controllable pedestrian animation via guided trajectory diffusion. In Proceedings of   \n404 the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13756\u201313766,   \n405 2023.   \n406 [47] N. Rhinehart and K. M. Kitani. Learning action maps of large environments via first-person   \n407 vision. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,   \n408 pages 580\u2013588, 2016.   \n409 [48] T. Salzmann, B. Ivanovic, P. Chakravarty, and M. Pavone. Trajectron $^{++}$ : Dynamically-feasible   \n410 trajectory forecasting with heterogeneous data. In Computer Vision\u2013ECCV 2020: 16th European   \n411 Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XVIII 16, pages 683\u2013700.   \n412 Springer, 2020.   \n413 [49] P.-E. Sarlin, C. Cadena, R. Siegwart, and M. Dymczyk. From coarse to fine: Robust hierarchical   \n414 localization at large scale. In Proceedings of the IEEE/CVF Conference on Computer Vision   \n415 and Pattern Recognition, pages 12716\u201312725, 2019.   \n416 [50] A. Seff, B. Cera, D. Chen, M. Ng, A. Zhou, N. Nayakanti, K. S. Refaat, R. Al-Rfou, and   \n417 B. Sapp. Motionlm: Multi-agent motion forecasting as language modeling. In Proceedings of   \n418 the IEEE/CVF International Conference on Computer Vision, pages 8579\u20138590, 2023.   \n419 [51] N. Snavely, S. M. Seitz, and R. Szeliski. Modeling the world from internet photo collections.   \n420 IJCV, 2008.   \n421 [52] C. Song, G. Yang, K. Deng, J.-Y. Zhu, and D. Ramanan. Total-recon: Deformable scene   \n422 reconstruction for embodied view synthesis. In ICCV, 2023.   \n423 [53] Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole. Score-based   \n424 generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456,   \n425 2020.   \n426 [54] S. Srivastava, C. Li, M. Lingelbach, R. Mart\u00edn-Mart\u00edn, F. Xia, K. E. Vainio, Z. Lian, C. Gokmen,   \n427 S. Buch, K. Liu, et al. Behavior: Benchmark for everyday household activities in virtual,   \n428 interactive, and ecological environments. In Conference on robot learning, pages 477\u2013490.   \n429 PMLR, 2022.   \n430 [55] T. Sun, Y. Hao, S. Huang, S. Savarese, K. Schindler, M. Pollefeys, and I. Armeni. Nothing   \n431 stands still: A spatiotemporal benchmark on 3d point cloud registration under large geometric   \n432 and temporal change. arXiv preprint arXiv:2311.09346, 2023.   \n433 [56] R. Szeliski and S. B. Kang. Shape ambiguities in structure from motion. IEEE Transactions on   \n434 Pattern Analysis and Machine Intelligence, 19(5):506\u2013512, 1997.   \n435 [57] G. Tevet, S. Raab, B. Gordon, Y. Shafir, D. Cohen-Or, and A. H. Bermano. Human motion   \n436 diffusion model. arXiv preprint arXiv:2209.14916, 2022.   \n437 [58] J. Van Den Berg, S. J. Guy, M. Lin, and D. Manocha. Reciprocal n-body collision avoidance.   \n438 In Robotics Research: The 14th International Symposium ISRR, pages 3\u201319. Springer, 2011.   \n439 [59] C.-Y. Weng, B. Curless, P. P. Srinivasan, J. T. Barron, and I. Kemelmacher-Shlizerman. Hu  \n440 mannerf: Free-viewpoint rendering of moving people from monocular video. In CVPR, pages   \n441 16210\u201316220, 2022.   \n442 [60] R. Wu, B. Mildenhall, P. Henzler, K. Park, R. Gao, D. Watson, P. P. Srinivasan, D. Verbin, J. T.   \n443 Barron, B. Poole, et al. Reconfusion: 3d reconstruction with diffusion priors. arXiv preprint   \n444 arXiv:2312.02981, 2023.   \n445 [61] S. Wu, T. Jakab, C. Rupprecht, and A. Vedaldi. Dove: Learning deformable 3d objects by   \n446 watching videos. arXiv preprint arXiv:2107.10844, 2021.   \n447 [62] Y. Xie, V. Jampani, L. Zhong, D. Sun, and H. Jiang. Omnicontrol: Control any joint at any time   \n448 for human motion generation. arXiv preprint arXiv:2310.08580, 2023.   \n449 [63] G. Yang and D. Ramanan. Volumetric correspondence networks for optical flow. In NeurIPS,   \n450 2019.   \n451 [64] G. Yang, D. Sun, V. Jampani, D. Vlasic, F. Cole, H. Chang, D. Ramanan, W. T. Freeman, and   \n452 C. Liu. LASR: Learning articulated shape reconstruction from a monocular video. In CVPR,   \n453 2021.   \n454 [65] G. Yang, M. Vo, N. Natalia, D. Ramanan, A. Vedaldi, and H. Joo. Banmo: Building animatable   \n455 3d neural models from many casual videos. In CVPR, 2022.   \n456 [66] G. Yang, C. Wang, N. D. Reddy, and D. Ramanan. Reconstructing Animatable Categories from   \n457 Videos. CVPR, 2023.   \n458 [67] G. Yang, S. Yang, J. Z. Zhang, Z. Manchester, and D. Ramanan. Physically plausible recon  \n459 struction from monocular videos. In ICCV, 2023.   \n460 [68] J. Yang, M. Gao, Z. Li, S. Gao, F. Wang, and F. Zheng. Track anything: Segment anything   \n461 meets videos, 2023.   \n462 [69] V. Ye, G. Pavlakos, J. Malik, and A. Kanazawa. Decoupling human and camera motion from   \n463 videos in the wild. In Proceedings of the IEEE/CVF Conference on Computer Vision and   \n464 Pattern Recognition, pages 21222\u201321232, 2023.   \n465 [70] Y. Yuan, U. Iqbal, P. Molchanov, K. Kitani, and J. Kautz. Glamr: Global occlusion-aware   \n466 human mesh recovery with dynamic cameras. In Proceedings of the IEEE/CVF conference on   \n467 computer vision and pattern recognition, pages 11038\u201311049, 2022.   \n468 [71] Y. Yuan, J. Song, U. Iqbal, A. Vahdat, and J. Kautz. Physdiff: Physics-guided human motion   \n469 diffusion model. In Proceedings of the IEEE/CVF International Conference on Computer   \n470 Vision, pages 16010\u201316021, 2023.   \n471 [72] L. Zhang, A. Rao, and M. Agrawala. Adding conditional control to text-to-image diffusion   \n472 models, 2023.   \n473 [73] K. Zhao, Y. Zhang, S. Wang, T. Beeler, and S. Tang. Synthesizing diverse human motions in 3d   \n474 indoor scenes. arXiv preprint arXiv:2305.12411, 2023.   \n475 [74] Z. Zhong, D. Rempe, D. Xu, Y. Chen, S. Veer, T. Che, B. Ray, and M. Pavone. Guided   \n476 conditional diffusion for controllable traffic simulation. In 2023 IEEE International Conference   \n477 on Robotics and Automation (ICRA), pages 3560\u20133566. IEEE, 2023.   \n478 [75] B. D. Ziebart, A. L. Maas, J. A. Bagnell, A. K. Dey, et al. Maximum entropy inverse reinforce  \n479 ment learning. In Aaai, volume 8, pages 1433\u20131438. Chicago, IL, USA, 2008.   \n480 [76] B. D. Ziebart, N. Ratliff, G. Gallagher, C. Mertz, K. Peterson, J. A. Bagnell, M. Hebert,   \n481 A. K. Dey, and S. Srinivasa. Planning-based prediction for pedestrians. In 2009 IEEE/RSJ   \n482 International Conference on Intelligent Robots and Systems, pages 3931\u20133936. IEEE, 2009. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "483 A Additional Implementation Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "484 Model Architecture. The score function of the goal is implemented as 6-layer MLP with hidden   \n485 size 128. The the score functions of the paths and body motions are implemented as 1D UNets   \n486 taken from MDM [57]. The sampling frequency is set to be 0.1s, resulting a sequence length of 56.   \n487 The environment encoder is implemented as a 6-layer 3D ConvNet with kernel size 3 and channel   \n488 dimension 128. The observer encoder and history encoder are implemented as a 3-layer MLP with   \n489 hidden size 128.   \n490 We use a linear noise schedule at training time and 50 denoising steps. At test time, each goal   \n491 denoising step takes 2ms and each path/body denoising step takes 9ms on a GeForce RTX 3090 GPU.   \n492 Data Collection. We collect RGBD videos using an iPhone, similar to TotalRecon [52]. To train   \n493 the neural localizer, we use Polycam to take the walkthrough video and extract a textured mesh. For   \n494 behavior capture, we use Record3D App to record videos and extract color images and depth images. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "495 B Additional Results ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "496 Histogram of Agent / Observer Visitation. We show final camera and agent registration to the   \n497 canonical scene in Fig. 6. The registered 3D trajectories provides statistics of agent\u2019s and user\u2019s   \n498 preference over the environment.   \n499 Varying Observer\u2019s Motion. We find that various interactive behaviors can be generated by   \n500 conditioning the model on different observer motion. The results are shown in Fig. 7.   \n501 Comparison to TotalRecon. In the main paper, we compare to TotalRecon on scene reconstruction   \n502 by providing it multiple videos. Here, we include additional comparison in their the original single   \n503 video setup. We find that TotalRecon fails to build a good agent model, or a complete scene model   \n504 given limited observations, while our method can leverage multiple videos as inputs to build a better   \n505 agent and scene model. The results are shown in Fig. 8. ", "page_idx": 14}, {"type": "image", "img_path": "fzdFPqkAHD/tmp/c06f1dfcd7a76cc346c3b01033afb220562c0c2f5d30f2b69631971893bd25f2.jpg", "img_caption": ["Figure 6: Given the 3D trajectories of the agent and the user accumulated over time (top), one could compute their preference represented by 3D heatmaps (bottom). Note the high agent preference over table and sofa. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "image", "img_path": "fzdFPqkAHD/tmp/79e77d91d0abbabba5d04c09a1ced6faeacf6e77917a526b0f952f0d572c1eb7.jpg", "img_caption": ["Figure 7: Interactive behavior simulation with user conditioning. By changing the trajectory of the user, one could influence the behavior of the agent. Given different control inputs, the agent may follow the user or run away from the user. "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "fzdFPqkAHD/tmp/eb29eda5d2322909ff05b9c8f672f2d5dbb28f0f4f927014e162b7adce0e044c.jpg", "img_caption": ["Figure 8: Qualitative comparison with TotalRecon [52] on 4D reconstruction. Top: reconstruction of the agent at at specific frame. Total-recon produces shapes with missing limbs and bone transformations that are misaligned with the shape, while our method produces complete shapes and good alignment. Bottom: reconstruction of the environment. TotalRecon produces distorted and incomplete geometry (due to lack of observations from a single video), while our method produces an accurate and complete environment reconstruction. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "507 High-level Behavior. The current ATS model is trained with time-horizon of $T^{*}=6.4$ seconds.   \n508 We observe that the model only learns mid-level behaviors of an agent (e.g., trying to move to a   \n509 destination; staying at a location; walking around). We hope incorporating a memory module and   \n510 training with longer time horizon will enable learning higher-level behaviors of an agent.   \n511 Scaling-up. As indicated by the experimental results, the goals sampled from ATS may fail to cover the actual goal when evaluated on the (unseen) test data. This raises safety concerns when using   \n513 ATS for the prediction task (e.g., predicting the behavior of pedestrains in autonomous driving). One potential solution of improving the generalization ability is to collect more diverse behavior data   \n515 from in the wild videos, or leverage \u201clarge\u201d video priors trained on internet-scale videos.   \n516 Multiple Agents. We show results of learning behavior models of a single agent, but our method for   \n517 4D reconstruction and interactive goal-driven behavior modeling is not limited to a single agent. We   \n518 leave learning multi-agent behavior simulation from videos as future work.   \n519 Physical Interactions. Our method reconstructs and generates the kinematics of an agent, which may produce physically-implausible results (e.g., penetration with the ground and foot sliding). One ", "page_idx": 16}, {"type": "text", "text": "521 promising way to deal with this problem is to add physics constraints to the reconstruction and motion   \n522 generation [67, 71].   \n523 Environment Reconstruction. To build a complete reconstruction of the environment, we register   \n524 multiple videos to a shared canonical space. However, the transient structures (e.g., cushion that   \n525 can be moved over time) may not be reconstructed well due to lack of observations. One potential   \n526 solution of reconstructing these transient structures is to combine generative image priors with the   \n527 reconstruction pipeline [60]. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "528 D Social Impact ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "529 Our method is able to learn interactive behavior from videos, which could help build simulators for   \n530 autonomous driving, gaming, and movie applications. It is also capable of building personalized   \n531 behavior models from casually collected video data, which can benefti users who do not have access   \n532 to a motion capture studio. On the negative side, the behavior generation model could be used as   \n533 \u201cdeepfake\u201d and poses threats to user\u2019s privacy and social security.   \n535 The checklist is designed to encourage best practices for responsible machine learning research,   \n536 addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove   \n537 the checklist: The papers not including the checklist will be desk rejected. The checklist should   \n538 follow the references and follow the (optional) supplemental material. The checklist does NOT count   \n539 towards the page limit.   \n540 Please read the checklist guidelines carefully for information on how to answer these questions. For   \n541 each question in the checklist:   \n542 \u2022 You should answer [Yes] , [No] , or [NA] .   \n543 \u2022 [NA] means either that the question is Not Applicable for that particular paper or the   \n544 relevant information is Not Available.   \n545 \u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA).   \n546 The checklist answers are an integral part of your paper submission. They are visible to the   \n547 reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it   \n548 (after eventual revisions) with the final version of your paper, and its final version will be published   \n549 with the paper.   \n550 The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation.   \n551 While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a   \n552 proper justification is given (e.g., \"error bars are not reported because it would be too computationally   \n553 expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering   \n554 \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we   \n555 acknowledge that the true answer is often more nuanced, so please just use your best judgment and   \n556 write a justification to elaborate. All supporting evidence can appear either in the main paper or the   \n557 supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification   \n558 please point to the section(s) where related material for the question can be found. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "559 IMPORTANT, please: ", "page_idx": 17}, {"type": "text", "text": "560 \u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\",   \n561 \u2022 Keep the checklist subsection headings, questions/answers and guidelines below.   \n562 \u2022 Do not modify the questions and only use the provided macros for your answers.   \n563 1. Claims   \n564 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n565 paper\u2019s contributions and scope?   \n566 Answer: [Yes]   \n567 Justification: The main claims made in the abstract and introduction accurately reflect the   \n568 paper\u2019s contributions and scope.   \n569 Guidelines:   \n570 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n571 made in the paper.   \n572 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n573 contributions made in the paper and important assumptions and limitations. A No or   \n574 NA answer to this question will not be perceived well by the reviewers.   \n575 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n576 much the results can be expected to generalize to other settings.   \n577 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n578 are not attained by the paper.   \n579 2. Limitations   \n580 Question: Does the paper discuss the limitations of the work performed by the authors?   \n[Yes]   \n582 Justification: The paper discusses the limitations of the work performed by the authors.   \n583 Guidelines:   \n584 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n585 the paper has limitations, but those are not discussed in the paper.   \n586 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n587 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n588 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n589 model well-specification, asymptotic approximations only holding locally). The authors   \n590 should reflect on how these assumptions might be violated in practice and what the   \n591 implications would be.   \n592 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n593 only tested on a few datasets or with a few runs. In general, empirical results often   \n594 depend on implicit assumptions, which should be articulated.   \n595 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n596 For example, a facial recognition algorithm may perform poorly when image resolution   \n597 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n598 used reliably to provide closed captions for online lectures because it fails to handle   \n599 technical jargon.   \n600 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n601 and how they scale with dataset size.   \n602 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n603 address problems of privacy and fairness.   \n604 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n605 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n606 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n607 judgment and recognize that individual actions in favor of transparency play an impor  \n608 tant role in developing norms that preserve the integrity of the community. Reviewers   \n609 will be specifically instructed to not penalize honesty concerning limitations.   \n610 3. Theory Assumptions and Proofs   \n611 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n612 a complete (and correct) proof?   \n613 Answer: [NA]   \n614 Justification: The paper does not include theoretical results.   \n615 Guidelines:   \n616 \u2022 The answer NA means that the paper does not include theoretical results.   \n617 \u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross  \n618 referenced.   \n619 \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n620 \u2022 The proofs can either appear in the main paper or the supplemental material, but if   \n621 they appear in the supplemental material, the authors are encouraged to provide a short   \n622 proof sketch to provide intuition.   \n623 \u2022 Inversely, any informal proof provided in the core of the paper should be complemented   \n624 by formal proofs provided in appendix or supplemental material.   \n625 \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced.   \n626 4. Experimental Result Reproducibility   \n627 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n628 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n629 of the paper (regardless of whether the code and data are provided or not)?   \n630 Answer: [Yes]   \n631 Justification: The authors tried their best to disclose the information needed to reproduce   \n632 the experiments.   \n633 Guidelines: ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 19}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 19}, {"type": "text", "text": "Answer: [No] ", "page_idx": 19}, {"type": "text", "text": "Justification: The code will be released once we put it in a better shape. ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 19}, {"type": "text", "text": "689 \u2022 Providing as much information as possible in supplemental material (appended to the   \n690 paper) is recommended, but including URLs to data and code is permitted.   \n691 6. Experimental Setting/Details   \n692 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n693 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n694 results?   \n695 Answer: [Yes]   \n696 Justification: The authors tried their best to specify all the training and test details.   \n697 Guidelines:   \n698 \u2022 The answer NA means that the paper does not include experiments.   \n699 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n700 that is necessary to appreciate the results and make sense of them.   \n701 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n702 material.   \n703 7. Experiment Statistical Significance   \n704 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n705 information about the statistical significance of the experiments?   \n706 Answer: [No]   \n707 Justification: The results currently do not have error bars, but we will try adding them later.   \n708 Based on empirical evidence of running the experiments, we think it will not affect the   \n709 conclusion.   \n710 Guidelines:   \n711 \u2022 The answer NA means that the paper does not include experiments.   \n712 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n713 dence intervals, or statistical significance tests, at least for the experiments that support   \n714 the main claims of the paper.   \n715 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n716 example, train/test split, initialization, random drawing of some parameter, or overall   \n717 run with given experimental conditions).   \n718 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n719 call to a library function, bootstrap, etc.)   \n720 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n721 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n722 of the mean.   \n723 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n724 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n725 of Normality of errors is not verified.   \n726 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n727 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n728 error rates).   \n729 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n730 they were calculated and reference the corresponding figures or tables in the text.   \n731 8. Experiments Compute Resources   \n732 Question: For each experiment, does the paper provide sufficient information on the com  \n733 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n734 the experiments?   \n735 Answer: [Yes]   \n736 Justification: The paper provides information about computer resources.   \n737 Guidelines:   \n738 \u2022 The answer NA means that the paper does not include experiments.   \n739 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n40 or cloud provider, including relevant memory and storage.   \n741 \u2022 The paper should provide the amount of compute required for each of the individual   \n742 experimental runs as well as estimate the total compute.   \n743 \u2022 The paper should disclose whether the full research project required more compute   \n744 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n745 didn\u2019t make it into the paper).   \n746 9. Code Of Ethics   \n747 Question: Does the research conducted in the paper conform, in every respect, with the   \n748 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n749 Answer: [Yes]   \n750 Justification: The authors have reviewed the code of ethics and think the paper follows the   \n751 guideline.   \n752 Guidelines:   \n753 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n754 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n755 deviation from the Code of Ethics.   \n756 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n757 eration due to laws or regulations in their jurisdiction).   \n758 10. Broader Impacts   \n759 Question: Does the paper discuss both potential positive societal impacts and negative   \n760 societal impacts of the work performed?   \n761 Answer: [Yes]   \n762 Justification: The paper discussed potential positive and negative impact.   \n763 Guidelines:   \n764 \u2022 The answer NA means that there is no societal impact of the work performed.   \n765 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n766 impact or why the paper does not address societal impact.   \n767 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n768 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n769 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n770 groups), privacy considerations, and security considerations.   \n771 \u2022 The conference expects that many papers will be foundational research and not tied   \n772 to particular applications, let alone deployments. However, if there is a direct path to   \n773 any negative applications, the authors should point it out. For example, it is legitimate   \n774 to point out that an improvement in the quality of generative models could be used to   \n775 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n776 that a generic algorithm for optimizing neural networks could enable people to train   \n777 models that generate Deepfakes faster.   \n778 \u2022 The authors should consider possible harms that could arise when the technology is   \n779 being used as intended and functioning correctly, harms that could arise when the   \n780 technology is being used as intended but gives incorrect results, and harms following   \n781 from (intentional or unintentional) misuse of the technology.   \n782 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n783 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n784 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n785 feedback over time, improving the efficiency and accessibility of ML).   \n786 11. Safeguards   \n787 Question: Does the paper describe safeguards that have been put in place for responsible   \n788 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n789 image generators, or scraped datasets)?   \nAnswer: [NA]   \n791 Justification: The paper poses no such risks.   \n792 Guidelines:   \n793 \u2022 The answer NA means that the paper poses no such risks.   \n794 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n795 necessary safeguards to allow for controlled use of the model, for example by requiring   \n796 that users adhere to usage guidelines or restrictions to access the model or implementing   \n797 safety filters.   \n798 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n799 should describe how they avoided releasing unsafe images.   \n800 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n801 not require this, but we encourage authors to take this into account and make a best   \n802 faith effort. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "803 12. Licenses for existing assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "804 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n805 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n806 properly respected?   \n807 Answer: [NA]   \n808 Justification: Thee paper does not use existing assets.   \n809 Guidelines: ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 22}, {"type": "text", "text": "825 13. New Assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "826 Question: Are new assets introduced in the paper well documented and is the documentation   \n827 provided alongside the assets?   \n828 Answer: [Yes]   \n829 Justification: The paper discussed the new assets.   \n830 Guidelines: ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 22}, {"type": "text", "text": "39 14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "840 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n841 include the full text of instructions given to participants and screenshots, if applicable, as   \n842 well as details about compensation (if any)?   \n843   \n844   \n845   \n846   \n847   \n848   \n849   \n850   \n851   \n852   \n853   \n854   \n855   \n856   \n857   \n858   \n859   \n860   \n861   \n862   \n863   \n864   \n865   \n866   \n867   \n868   \n869   \n870   \n871   \n872 ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] Justification: The paper does not deal with crowdsourcing or external human subjects. Guidelines: \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. \u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not deal with crowdsourcing or external human subjects. Guidelines: \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. \u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. \u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 23}]