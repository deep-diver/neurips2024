[{"figure_path": "GhqdnLZMAz/figures/figures_2_1.jpg", "caption": "Figure 1: SEV Hypercube", "description": "This figure illustrates a SEV hypercube with 2\u00b3 = 8 vertices for a model f, an instance x\u1d62 with label f(x\u1d62) = 1, and a reference r. Each vertex is a Boolean vector that represents different alignment combinations (setting features to the reference or not). Vertices are adjacent when their Boolean vectors differ in one bit. The score of vertex v is f(xv), indicating if the modified instance is positive (1) or negative (0).  The shortest path from a query (positive instance) to a negative vertex represents the SEV.", "section": "3.1 Recap of Sparse Explanation Values"}, {"figure_path": "GhqdnLZMAz/figures/figures_4_1.jpg", "caption": "Figure 2: Cluster-based SEV", "description": "This figure illustrates the calculation of cluster-based SEV (SEV\u00a9) for two examples. Each red dot represents a query, while each blue dot represents a reference. For each instance, the closest centroid is selected and the SEV hypercube is considered. Cyan points represent negatively predicted vertices and pink points represent positively predicted vertices. Following the red lines, the SEV\u00a9 for the two queries are 2 and 1, respectively.", "section": "4.1 Cluster-based SEV: Improving Closeness"}, {"figure_path": "GhqdnLZMAz/figures/figures_4_2.jpg", "caption": "Figure 3: SEVT Preprocessing", "description": "This figure illustrates the preprocessing step for tree-based SEV (SEVT) calculations.  It shows how negatively predicted leaf nodes are identified and their paths recorded at internal nodes. This preprocessing speeds up SEVT calculation at runtime by allowing the algorithm to quickly identify relevant paths to negative leaves from the query point in the tree. The figure uses color-coding to represent different node types (internal, negative, positive) and arrows to represent the path from each internal node to negatively predicted leaves. The path information (e.g., RLL, LR, LLR) is recorded at each internal node to represent a sequence of decisions to reach negatively predicted leaves. This allows the algorithm to avoid redundant calculations during runtime.", "section": "4.2 Tree-based SEV: SEV Variant with Useful Properties and Computational Benefits"}, {"figure_path": "GhqdnLZMAz/figures/figures_4_3.jpg", "caption": "Figure 3: SEVT Preprocessing", "description": "This figure illustrates the preprocessing step for calculating tree-based SEV (SEVT).  It shows how to efficiently find the shortest paths from a positively-predicted query point to negatively-predicted leaf nodes in a decision tree. The preprocessing step involves identifying all paths leading to negative leaves from each internal node in the tree and storing those paths for later use. During SEVT calculation, the algorithm only needs to traverse upward from the query leaf node, checking the pre-computed negative paths at each internal node. This significantly speeds up the SEVT calculation compared to calculating SEV from scratch.", "section": "4.2 Tree-based SEV: SEV Variant with Useful Properties and Computational Benefits"}, {"figure_path": "GhqdnLZMAz/figures/figures_8_1.jpg", "caption": "Figure 5: Explanation performance under different models and metrics. We desire lower SEV\u00af for sparsity, lower l\u221e for closeness and higher log likelihood for credibility (shaded regions)", "description": "This figure compares the performance of different SEV variants (SEV\u00b9, SEV\u00a9, and SEV\u00a9+F) across multiple datasets and model types.  The left panel shows the trade-off between sparsity (measured by SEV\u00af) and closeness (measured by the L\u221e distance between the query and its explanation). The right panel illustrates the relationship between sparsity and credibility, where higher log-likelihood indicates better credibility. The shaded regions in both plots highlight the desired areas of high credibility, low SEV\u00af (high sparsity) and low L\u221e (high closeness). The results suggest that SEV\u00a9 generally improves both closeness and credibility compared to SEV\u00b9, while SEV\u00a9+F offers a way to further improve sparsity by adjusting the reference point.", "section": "6.1 Cluster-based SEV shows improvement in credibility and closeness"}, {"figure_path": "GhqdnLZMAz/figures/figures_8_2.jpg", "caption": "Figure 5: Explanation performance under different models and metrics. We desire lower SEV\u00af for sparsity, lower l\u221e for closeness and higher log likelihood for credibility (shaded regions)", "description": "This figure compares the performance of different SEV variants (SEV\u00b9, SEV\u00a9, SEV\u00a9+F) across various datasets and machine learning models.  It visualizes the trade-offs between three key aspects of model explanations: sparsity (measured by SEV, lower is better), closeness (measured by l\u221e distance between the query and explanation, lower is better), and credibility (measured by log-likelihood of the explanation within the negative class distribution, higher is better).  The shaded regions in the plots highlight the desirable areas with low SEV\u00af, low l\u221e, and high log-likelihood.", "section": "6.1 Cluster-based SEV shows improvement in credibility and closeness"}, {"figure_path": "GhqdnLZMAz/figures/figures_9_1.jpg", "caption": "Figure 5: Explanation performance under different models and metrics. We desire lower SEV\u00af for sparsity, lower l\u221e for closeness and higher log likelihood for credibility (shaded regions)", "description": "This figure compares the performance of different SEV variants (SEV\u00b9, SEV\u00a9, and SEV\u00a9+F) across multiple datasets and models. It visualizes the trade-off between sparsity (SEV\u00af), closeness (l\u221e), and credibility (log-likelihood). Lower SEV\u00af values indicate sparser explanations, while lower l\u221e values represent closer explanations to the original data point. Higher log-likelihood values signify higher credibility. The shaded regions in the plots highlight the desired area of optimal performance: low SEV\u00af, low l\u221e, and high log-likelihood.", "section": "6.1 Cluster-based SEV shows improvement in credibility and closeness"}, {"figure_path": "GhqdnLZMAz/figures/figures_15_1.jpg", "caption": "Figure 8: SEV distribution", "description": "This figure demonstrates the sensitivity of SEV to the choice of reference points. Moving the reference point further away from the query (from r to r') changes the SEV from 2 to 1.  The figure shows how the areas with different SEV values move as the reference point changes.  Specifically, it illustrates how moving the reference point away from the decision boundary causes the SEV values to also move away from the decision boundary. This indicates that the sparsity of the explanations is sensitive to the location of the reference point. ", "section": "3.2 Motivation of Our Work: Sensitivity to Reference Points"}, {"figure_path": "GhqdnLZMAz/figures/figures_17_1.jpg", "caption": "Figure 9: The clustering results for FICO dataset. (Left) The probability distribution for the negatively labeled queries; (Middle) The clustering result for Original Soft K-Means Clustering; (Right) The clustering result for Score-based K-Means Clustering The red stars represent the positively predicted cluster centers, and the blue stars the negatively predicted cluster centers", "description": "This figure compares the clustering results of three different methods on the FICO dataset. The leftmost panel shows the probability distribution of negative samples. The middle panel shows the clustering result using original soft k-means, and the rightmost panel shows the result from score-based soft k-means. The red stars represent positively predicted cluster centers, while the blue stars represent negatively predicted cluster centers. The score-based soft k-means method aims to penalize positive clusters to ensure that the generated cluster centers are all negatively predicted, hence resulting in a more effective SEV calculation.", "section": "4.1 Cluster-based SEV: Improving Closeness"}, {"figure_path": "GhqdnLZMAz/figures/figures_17_2.jpg", "caption": "Figure 10: Clustering Results for different datasets.", "description": "This figure visualizes the results of the clustering algorithm applied to six different datasets used in the paper. Each subplot represents a dataset, showing the clusters formed by the algorithm. The points in each subplot represent individual data points in that dataset, colored according to the cluster they belong to. Blue stars mark the cluster centers, which are used as reference points in the SEV calculation. The datasets included are: Adult, German Credit, COMPAS, Diabetes, MIMIC III, and Headline News. The number of clusters for each dataset is indicated in the title of each subplot.", "section": "4.1 Cluster-based SEV: Improving Closeness"}, {"figure_path": "GhqdnLZMAz/figures/figures_29_1.jpg", "caption": "Figure 11: Example of SEVT=1 in Theorem 4.1", "description": "This figure illustrates Theorem 4.1, which states that if a positively predicted query node in a decision tree has a sibling leaf node or an ancestor node with a negatively predicted child leaf node, the tree-based SEV (SEVT) is equal to 1. The left side shows two example decision trees. In the top tree, the query node (red) has a sibling (blue), so changing one feature leads to a negative prediction (SEVT=1).  The bottom tree shows a case where the query node does not have a sibling leaf node, but an ancestor node has a negatively predicted child node, so again, changing one feature leads to a negative prediction (SEVT=1). The right side shows the corresponding SEV explanations; the red arrows indicate the changes necessary to achieve a negative prediction. The dashed line separates the two cases.", "section": "3.1 Recap of Sparse Explanation Values"}, {"figure_path": "GhqdnLZMAz/figures/figures_30_1.jpg", "caption": "Figure 11: Example of SEVT=1 in Theorem 4.1", "description": "This figure provides an example to illustrate Theorem 4.1, which states that if a positively predicted query's leaf node has a sibling leaf node, or any internal node in its decision path has a negatively predicted child leaf, then the Tree-based Sparse Explanation Value (SEVT) is equal to 1.  The figure shows two decision trees. The left tree shows a complete decision tree where nodes 1 and 3 are negatively predicted leaves, while node 2 and 4 are positively predicted leaves. The right tree shows the subtree relevant to the explanation, containing only nodes that directly affect the decision for the given instance represented by a person icon with a plus sign. A single change is sufficient to reach a negatively predicted leaf, resulting in SEVT = 1.  Conversely, if a positively-predicted query's leaf has no sibling leaf and there are no internal nodes with negatively-predicted child nodes, SEVT would be greater than 1, which the theorem states, and is illustrated.", "section": "3.1 Recap of Sparse Explanation Values"}]