{"references": [{"fullname_first_author": "Jinze Bai", "paper_title": "Qwen technical report", "publication_date": "2023-09-16", "reason": "This paper provides technical details of the Qwen language model, which is one of the models analyzed in the study for its refusal behavior."}, {"fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-12-01", "reason": "This paper is highly influential in the field of language model safety and alignment and provides insights into how language models are fine-tuned to follow instructions and refuse harmful requests."}, {"fullname_first_author": "Andy Zou", "paper_title": "Universal and transferable adversarial attacks on aligned language models", "publication_date": "2023-07-15", "reason": "This paper introduces the concept of adversarial suffixes, which are used as a form of jailbreak in the study, and helps explain some mechanisms behind model refusal."}, {"fullname_first_author": "Mantas Mazeika", "paper_title": "HarmBench: A standardized evaluation framework for automated red teaming and robust refusal", "publication_date": "2024-02-04", "reason": "HarmBench is used as a benchmark dataset to evaluate model refusal and safety.  The paper provides a standardized evaluation framework for this type of research."}, {"fullname_first_author": "Simon Lermen", "paper_title": "LoRA fine-tuning efficiently undoes safety training in Llama 2-Chat 70B", "publication_date": "2023-10-20", "reason": "This paper shows that fine-tuning can undo safety training in language models, offering a comparison to the paper's proposed method for controlling model behavior."}]}