[{"type": "text", "text": "Improved Bayes Regret Bounds for Multi-Task Hierarchical Bayesian Bandit Algorithms ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1AI Thrust, The Hong Kong University of Science and Technology (Guangzhou), China 2Department of Computer Science and Engineering, HKUST, China {jiechaoguan, xionghui}@hkust-gz.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Hierarchical Bayesian bandit refers to the multi-task bandit problem in which bandit tasks are assumed to be drawn from the same distribution. In this work, we provide improved Bayes regret bounds for hierarchical Bayesian bandit algorithms in the multi-task linear bandit and semi-bandit settings. For the multi-task linear bandit, we first analyze the preexisting hierarchical Thompson sampling (HierTS) algorithm, and improve its gap-independent Bayes regret bound from $O(m{\\sqrt{n\\log n\\log\\left(m n\\right)}})$ to $O(m{\\sqrt{n\\log n}})$ in the case of infinite action set, with $m$ being the number of tasks and $n$ the number of iterations per task. In the case of finite action set, we propose a novel hierarchical Bayesian bandit algorithm, named hierarchical BayesUCB (HierBayesUCB), that achieves the logarithmic but gap-dependent regret bound $O(m\\log{(m n)}\\log{n})$ under mild assumptions. All of the above regret bounds hold in many variants of hierarchical Bayesian linear bandit problem, including when the tasks are solved sequentially or concurrently. Furthermore, we extend the aforementioned HierTS and HierBayesUCB algorithms to the multi-task combinatorial semi-bandit setting. Concretely, our combinatorial HierTS algorithm attains comparable Bayes regret bound ${\\dot{O}}(m{\\sqrt{n}}\\log n)$ with respect to the latest one. Moreover, our combinatorial HierBayesUCB yields a sharper Bayes regret bound $O(m\\log{(m n)}\\log{n})$ . Experiments are conducted to validate the soundness of our theoretical results for multi-task bandit algorithms. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "A stochastic bandit [26, 6, 27] is a sequential decision-making problem where at each round, an agent has to choose an action, and receives a stochastic reward without knowing its expected value. The gap between the cumulative reward of optimal actions in hindsight and the cumulative reward of agent is defined as regret. The goal is to minimize regret, through a combination of exploring different actions and exploiting those with high rewards in the past. Typical applications of bandit algorithms include news article recommendation [28], computational advertisement [20], and dynamic pricing [24]. For example, in news article recommendation, the agent must choose a news article for a user. The actions in this bandit setting are articles and the reward could be an indicator of a click from user. ", "page_idx": 0}, {"type": "text", "text": "When the agent has to solve multiple bandit tasks, many machine learning researchers resort to multi-task learning/meta-learning paradigm [8, 34] to benefit task adaptation. The existing works focused on the multi-task bandit problem can be categorized into three main groups: (1) The first group attempts to learn a low-dimensional representation shared by different bandit tasks, to derive a sharper cumulative regret bound than that derived by learning each task independently [19, 10]. (2) The second group leverages the similarity of contexts (e.g. the feature of actions) in bandit tasks to improve agent's ability to predict rewards in a new task [14, 36]. (3) The third group chooses to maintain a meta-distribution over the hyper-parameters of within-task bandit algorithms (like TsallisINF [23], OFUL [9], and Thompson sampling [25, 7, 17]), and draws informative hyper-parameters from the meta-distribution for efficient regret minimization. Our work falls into the third group and formulates the problem of learning similar bandit tasks in a hierarchical Bayesian bandit model [17]. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Specifically, in hierarchical Bayesian bandit setting, each bandit task is characterized by a task parameter. Different bandit task parameters are assumed to be independently and identically distributed according to the same distribution. At each round, the learning agent interacts with one or several bandit tasks, which correspond to the sequential and concurrent bandit settings respectively. Many existing works considered hierarchical Bayesian bandit problem, and proposed Thompson sampling [33] type algorithms to solve it [25, 7, 36]. The latest work [17] proposed hierarchical Thompson sampling (HierTS) algorithm and developed a gap-independent Bayes regret bound $O(m{\\sqrt{n\\log n\\log\\left(m n\\right)}})$ in the Gaussian linear bandit setting, where $m$ is the number of bandit tasks and $n$ thenumberof iterations per task. However, it is still unclear for us whether we can derive sharper regret bounds or how to extend hierarchical Bayesian bandit algorithms to the more general multi-task bandit setting. ", "page_idx": 1}, {"type": "text", "text": "In this work, we attempt to tackle the above two issues, by providing improved Bayes regret bounds for hierarchical Bayesian bandit algorithms in the multi-task Gaussian linear bandit and semi-bandit setting. Firstly, in the linear bandit seting, we improve the multi-task Bayes regret bound of HierTS to $O({\\bar{m}}{\\sqrt{n\\log n}})$ in the case of infinite action set, strengthening the latest bound in [17, Thm 3] by a factor of $O({\\sqrt{\\log\\left(m n\\right)}})$ . In the case of finite action set, we propose a novel hierarchical Bayesian bandit algorithm, named hierarchical BayesUCB (HierBayesUCB), that achieves the logarithmic but gap-dependent regret bound $O(m\\log{(m n)}\\log{n})$ under mild assumptions. All of the above regret bounds for linear bandit hold in both the sequential and concurrent setting. Secondly, we extend the aforementioned HierTS and HierBayesUCB algorithms to the multi-task Gaussian combinatorial semi-bandit setting. Concretely, our combinatorial HierTS algorithm attains comparable Bayes regret bound $O(m{\\sqrt{n}}\\log n)$ with respect to the latest one in [7, Thm 6]. Moreover, our combinatorial HierBayesUCB yields a sharper but gap-dependent regret bound $O(m\\log{(m n)}\\log{n})$ . Extensive experiments in the Gaussian linear bandit setting are conducted to support our theoretical results. ", "page_idx": 1}, {"type": "text", "text": "Overall, our theoretical contributions are four-fold: (1) In the case of infinite action set, we provide a tighter Bayes regret bound $O(m{\\sqrt{n\\log n}})$ for HierTS. This bound improves the latest result by a factor of $O({\\sqrt{\\log\\left(m n\\right)}})$ . (2) In the case of finite action set, we propose a novel HierBayesUCB algorithm, and provide gap-dependent logarithmic Bayes regret bound $O(m\\log{(m n)}\\log{\\bar{n}})$ for it. (3) We generalize the above regret bounds for linear bandit from sequential setting to the more challenging concurrent setting. (4) We extend both HierTS and HierBayesUCB algorithms to the more general multi-task combinatorial semi-bandit setting and derive improved Bayes regret bounds. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Frequentist Regret Bounds for Stochastic Linear Bandit. In the frequentist stochastic bandit setting, we do not assume the bandit task parameter is sampled from a fixed distribution. The frequentist regret is thus for any fixed task parameter, without taking expectation over the distribution of task parameter. (1) In the case of finite action set: [5] for the first time investigated the stochastic linear bandit problem and proposed an algorithm with a frequentist regret of $O({\\sqrt{d n}}\\log^{3/2}n)$ ,where $d$ is the dimension of action space and $n$ is the number of rounds. [29] developed a new algorithm and improved the regret bound to $O({\\sqrt{d n\\log n}})$ . [12] showed that the lower frequentist regret bound in the finite action set is $\\Omega({\\sqrt{d n}})$ . (2) In the case of infinite action set: Both [13] and [30] proposed algorithms that achieve $O(d{\\sqrt{n}}\\log^{3/2}n)$ regret. The regret bound was further improved in [1, 15] to ${\\bar{O(d{\\sqrt{n}}}}\\log n)$ , by designing novel linear bandit algorithms or utilizing advanced martingale methods. ", "page_idx": 1}, {"type": "text", "text": "Bayes Regret Bounds for Bayesian Linear Bandit. In the Bayesian stochastic bandit setting, the Bayes regret is the expected cumulative regret whose expectation is taken over the draw of task parameter from a distribution. It is not difficult to see that the frequentist regret upper bound implies a Bayes regret upper bound, because the former holds for any task parameter. (1) When the action set is infinite: [30] showed that in the Gaussian linear bandit setting, the Bayes regret of any Bayesian bandit algorithm is lower bounded by $\\Omega({\\sqrt{n}})$ . [31] for the first time gave the Bayes regret bound of $O(d{\\sqrt{n}}\\log n)$ for both Thompson sampling (TS) and BayesUCB [22] algorithms. Recently, [21] provided an improved Bayes regret $O(d{\\sqrt{n\\log n}})$ for TS algorithm with a concise proof. (2) When the action set is finite: [32] derived a tight regret bound of $O({\\sqrt{d n}})$ for TS algorithm with sub-Gaussian reward noise, via a novel information-theoretic approach. Recently, [3] developed a logarithmic Bayes regret bound $O(d^{2}\\log^{2}n)$ for BayesUCB algorithm in the Gaussian bandit setting. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Frequentist Regret Bounds for Multi-Task Linear Bandit Problems. Under the representation learning paradigm, the frequentist regret bounds in [19, 10] for multi-task linear bandits scales as $O(m{\\sqrt{n k}})$ ,where $k$ is the dimension of low-dimensional representation. The expected frequentist regret upper bound for multi-task adversarial linear bandit in [23] is $O(m{\\sqrt{n\\log\\left(1+n V\\right)}})$ ,with $V$ being the similarity among multiple adversarial bandit tasks. Nevertheless, we should mention that all of these frequentist regret bounds for multi-task linear bandit problem are not tighter than $\\Omega(m{\\sqrt{n}})$ ", "page_idx": 2}, {"type": "text", "text": "Bayes Regret Bounds for Multi-Task Bayesian Linear Bandit/Semi-Bandit. The most related works to ours are [25, 7, 17], which provided hierarchical-type Thompson sampling algorithms for multi-task bandit and derived Bayes regret bounds in the Gaussian reward setting. We list these Bayes regret bounds in Table 1 for direct comparisons. Among them, the latest work [17] proposed the HierTS algorithm and obtained its regret bound of $O(m{\\sqrt{n\\log n\\log\\left(m n\\right)}})$ , [7] derived the first Bayes regret bound for multi-task hierarchical Bayesian semi-bandit algorithm. In this work, we provide for HierTS improved Bayes regret bound of $O(m{\\sqrt{n\\log n}})$ in Theorem 5.1. We also propose a novel HierBayesUCB algorithm that achieves logarithmic regret bound $O(m\\log n\\log{(n m)})$ .We finally extend HierTS and HierBayesUCB to the semi-bandit setting and derive improved theoretical results. Other works utilized action features or structure information to derive Bayes regret bounds for multi-task bandit [36, 37], e.g. the Bayes regret bound in [36] is $O(m{\\sqrt{n\\log n}}+m\\log^{2}{(m n)})$ ", "page_idx": 2}, {"type": "text", "text": "Hierarchical Bayesian Bandit Algorithms. Hierarchical Bayesian bandit algorithm was first proposed by [25] to solve multi-task bandit problems. More hierarchical-type Thompson sampling algorithms based on multi-task/meta learning frameworks were developed with improved theoretical guarantees and empirical performance [7, 17, 36, 37]. There also existed other works investigating hierarchical Bayesian bandit algorithms within the single-task bandit setting. For example, [16] extended the two-level hierarchical Bayesian bandit framework to the deeper multiple-level hierarchial Bayesian bandit framework. [2] generalized the single-effect-parameter HierTS algorithm (i.e. the action parameter is centered at a single latent variable) to the mixed-effect bandit framework where each action is associated with a parameter that depends upon one or multiple effect parameters. ", "page_idx": 2}, {"type": "text", "text": "3 Problem Setting ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "For any positive integer $n$ , denote $[n]=\\{1,2,...,n\\}$ for brevity. For any square matrix $M\\in\\mathbb{R}^{d\\times d}$ denote $\\lambda_{1}(M)$ \uff0c $\\lambda_{d}(\\bar{M})$ as its maximum and minimum eigenvalues respectively, denote $\\kappa(M)=$ $\\lambda_{1}(M)/\\lambda_{d}(M)$ as its condition number. The action set $\\mathcal{A}\\subseteq\\overline{{B(0,B)}}\\subseteq\\mathbb{R}^{d}$ is assumed to be compact for some positive constant $B>0$ , where $\\overline{{\\beta(0,B)}}$ is the closed ball centered at the origin. We use $\\langle,\\rangle$ to denote the inner-product between vectors, use $\\mathbf{w}(a)$ or $\\mathbf{w}_{a}$ to denote the $a$ -th element of vector w. ", "page_idx": 2}, {"type": "text", "text": "Single-Task Bandit. A stochastic bandit problem is characterized by an unknown parameter $\\theta$ with an action set $\\boldsymbol{\\mathcal{A}}$ . Each action $a\\in A$ under the bandit instance $\\theta$ is associated with a reward distribution $\\mathbb{P}(\\cdot|a,\\theta)$ . The reward mean of action $a$ under $\\theta$ is denoted as $\\displaystyle r(a;\\theta)=\\mathbb{E}_{Y\\sim\\mathbb{P}(\\cdot|a;\\theta)}[Y]$ and the optimal action under $\\theta$ is denoted as $A_{*}\\,=\\,\\arg\\operatorname*{max}_{a\\in\\mathcal{A}}r(a;\\theta)$ . In the stochastic linear bandit setting, the mean reward of action $a\\in{\\mathcal{A}}$ is $r(a,\\theta)=a^{\\top}\\theta$ . In Bayesian bandit problem, we further assume that the task parameter $\\theta$ is independently and identically distributed (i.i.d.) according to a task parameter distribution $\\mathbb{P}(\\cdot|\\mu_{*})$ , which is characterized by an unknown hyper-parameter $\\mu_{*}$ ", "page_idx": 2}, {"type": "text", "text": "Single-Task Semi-Bandit. In the semi-bandit setting, the action set $\\boldsymbol{A}=[K]$ is a set of finite items. $\\mathcal{A}=\\left\\{A\\subseteq\\mathcal{A}:|A|\\leq L\\right\\}$ is a family of subsets of $\\boldsymbol{\\mathcal{A}}$ with up to $L$ items, where $L\\leq K$ $\\mathbf{w}\\in\\mathbb{R}^{K}$ is a weight vector. The weight of a set $A\\in{\\mathcal{A}}$ is defined as $\\bar{\\sum_{a\\in A}}\\mathbf{w}(a)$ . We assume that the weights w are drawn i.i.d. from a distribution, and the mean weight is denoted as $\\begin{array}{r}{\\bar{\\bf w}{=}\\mathbb{E}\\big[{\\bf w}\\big]}\\end{array}$ . Following previous work [38], we focus on the coherent case [39] which assumes that the agent knows a feature matrix $\\Phi\\in\\mathbb{R}^{K\\times d}$ , such that $\\bar{\\mathbf{w}}=\\Phi\\theta$ ,where $\\theta$ is the task parameter drawn from $\\mathbb{P}(\\cdot|\\mu_{*})$ . The reward of a subset $A\\in{\\mathcal{A}}$ under the bandit instance $\\theta$ is defined as $\\begin{array}{r}{r(A;\\theta)=\\sum_{a\\in A}(\\Phi\\dot{\\theta})(a)=\\sum_{a\\in A}\\langle\\Phi_{a},\\theta\\rangle}\\end{array}$ where $\\Phi_{a}$ is the transpose of the $a$ -th row of matrix $\\Phi$ . We further assume that $\\|\\Phi_{a}\\|\\le B$ \uff0c $\\forall a\\in A$ ", "page_idx": 2}, {"type": "text", "text": "Hierarchical Bayesian Multi-Task Bandit/Semi-Bandit. In this setting, the agent interacts with $m$ tasks sequentially or concurrently. First, sample the hyper-parameter $\\mu_{*}$ from a hyper-prior $Q$ Then, for each task $s\\in[m]$ , sample the task parameter $\\theta_{s,*}$ independently from distribution $\\mathbb{P}(\\cdot|\\mu_{*})$ The learning process can be detailed as follows. At round $t\\geq1$ , the agent interacts with a set of tasks $S_{t}\\,\\subseteq\\,[m]$ , takes a series of actions $A_{t}\\,=\\,(A_{s,t})_{s\\in S_{t}}$ , and receives a series of rewards $Y_{t}=(Y_{s,t})_{s\\in S_{t}}$ . In the bandit setting. $Y_{s,t}\\sim\\mathbb{P}(\\cdot|A_{s,t};\\theta_{s,*})$ is a stochastic reward obtained by taking action $A_{s,t}$ in task $s\\in S_{t}$ ; in the semi-bandit setting, $Y_{s,t}=\\{\\hat{\\mathbf{w}}_{s,t}(a)\\}_{a\\in A_{s,t}}$ is a series of stochastic rewards, where $\\hat{\\mathbf{w}}_{s,t}=\\bar{\\mathbf{w}}_{s}+\\eta_{s,t}$ \uff0c $\\bar{\\mathbf{w}}_{s}=\\Phi\\theta_{s,*}$ , and $\\eta_{s,t}$ is a $K$ -dimensional random noise. The full hierarchical Bayesian bandit/semi-bandit model in the $m$ -task learning setting is exhibited as follow: (1) $\\mu_{*}\\sim Q$ $\\begin{array}{r}{\\smash{\\mathrm{2}}\\rVert\\theta_{s,*}\\rVert\\mu_{*}\\sim\\mathbb{P}(\\cdot|\\mu_{*}),\\forall s\\in[m];\\ (3)\\ Y_{s,t}|A_{s,t},\\theta_{s,*}\\sim\\mathbb{P}(\\cdot|A_{s,t};\\theta_{s,*}),\\forall t\\geq1,s\\in S_{t}}\\end{array}$ Therefore, the goal of the agent in hierarchical Bayesian multi-task bandit/semi-bandit setting is to interact with $m$ tasks efficiently and minimize the following cumulative multi-task Bayes regret: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{B R}(m,n)=\\mathbb{E}\\big[\\sum_{t\\ge1}\\sum_{s\\in S_{t}}r(A_{s,*};\\theta_{s,*})-r(A_{s,t};\\theta_{s,*})\\big],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\begin{array}{r}{A_{s,*}\\,=\\,\\arg\\operatorname*{max}_{a\\in\\mathcal{A}}r\\bigl(a;\\theta_{s,*}\\bigr)}\\end{array}$ is the optimal action for task $s\\,\\in\\,[m]$ in the bandit setting, and $A_{s,*}\\in\\arg\\operatorname*{max}_{A\\in\\mathcal{A}}r(A;\\theta_{s,*})$ is the optimal subset for task $s\\in[m]$ in the semi-bandit setting. The expectation is taken over $\\mu_{*}$ , all task parameters $(\\theta_{s,*})_{s\\in[m]}$ , ll actions $(A_{t})_{t\\geq1}$ , ll stochastic rewards $(Y_{t})_{t\\geq1}$ .We further assume that the action set $\\boldsymbol{\\mathcal{A}}$ is the same across different tasks for ease of exposition, and assume that the learning agent interacts with any task $s\\in[m]$ for at most $n$ rounds for convenient comparison with exiting regret upper bounds for multi-task bandit/semi-bandit problem. ", "page_idx": 3}, {"type": "text", "text": "4 Algorithm ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Denote $H_{s,t}\\!=\\!\\bigl((A_{s,\\ell},Y_{s,\\ell})\\bigr)_{\\ell<t,s\\in S_{t}}$ as the history of all interactions of agent with task $s\\in[m]$ ,and $\\scriptstyle H_{t}=(H_{s,t})_{s\\in[m]}$ as the whole interaction history up to round $t$ . We next introduce the specific form of Hierarchical Thompson Sampling (HierTS) and Hierarchical BayesUCB (HierBayesUCB) algorithms in the multi-task Bayesian linear bandit and semi-bandit settings, and instantiate these two algorithms to the multi-task Gaussian linear bandit (Algorithm 1) and semi-bandit (Algorithm 2) problems. ", "page_idx": 3}, {"type": "text", "text": "4.1  Hierarchical Thompson Sampling and Hierarchical BayesUCB ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "At round $t$ hierarchical Bayesian bandit algorithm samples a hyper-parameter $\\mu_{t}$ from the hyperposterior $Q_{t}$ defined as $Q_{t}(\\mu)=\\mathbb{P}(\\mu_{*}=\\mu\\bar{|}H_{t})$ , and then interacts with tasks $S_{t}\\subset[m]$ . Next, we give details of bandit algorithms, and details of semi-bandit algorithms are deferred to Section 5.4. ", "page_idx": 3}, {"type": "text", "text": "Hierarchical Thompson Sampling. For any task $s\\ \\in\\ S_{t}$ , HierTS samples task parameter $\\theta_{s,t}$ from the distribution $\\mathbb{P}_{s,t}(\\theta|\\mu_{t})\\ \\triangleq\\ \\mathbb{P}(\\theta_{s,*}\\ =\\ \\theta|\\mu_{*}\\ =\\ \\mu_{t},H_{s,t})$ and takes the action $A_{s,t}~=~$ arg $\\operatorname*{max}_{a\\in\\mathcal{A}}{a^{\\top}\\theta_{s,t}}$ , where $\\mathbb{P}_{s,t}(\\boldsymbol{\\theta}|\\mu_{t})$ is only conditioned on $H_{s,t}$ due to the independence between task parameter $\\theta_{s,*}$ and other task histories. This process clearly samples bandit instance $\\theta_{s,t}$ from the true posterior $\\mathbb{P}(\\theta_{s,\\ast}\\,=\\theta|H_{t})$ , which is equivalent to the form: $\\begin{array}{r l}{\\int\\mathbb{P}(\\theta_{s,\\ast}=\\theta,\\mu_{\\ast}=}&{{}}\\end{array}$ $\\begin{array}{r}{\\mu|H_{t})\\mathrm{d}\\mu\\,=\\,\\int\\mathbb{P}_{s,t}(\\theta|\\mu)Q_{t}(\\mu)\\mathrm{d}\\mu}\\end{array}$ ,where $\\mathbb{P}_{s,t}(\\theta|\\mu)\\,\\propto\\,\\mathcal{L}_{s,t}(\\theta)\\mathbb{P}(\\theta|\\mu)$ is the posterior probability, $\\begin{array}{r}{\\mathcal{L}_{s,t}(\\theta)\\!=\\!\\!\\prod_{(a,y)\\in H_{s,t}}\\!\\!\\!\\mathbb{P}(y|a;\\theta)}\\end{array}$ is the likelihood function, $\\mathbb{P}(\\boldsymbol{\\theta}|\\boldsymbol{\\mu})$ is the prior probability by Bayes rule. ", "page_idx": 3}, {"type": "text", "text": "Hierarchical BayesUCB. For any task $s\\ \\in\\ S_{t}$ in round $t$ , HierBayesUCB computes the upper confidence bound $\\begin{array}{r}{U_{t,s,a}=a^{\\top}\\hat{\\mu}_{s,t}+\\sqrt{2\\log\\frac{1}{\\delta}}\\|a\\|_{\\hat{\\Sigma}_{s,t}}}\\end{array}$ for any $a\\in{\\mathcal{A}}$ ,where $\\hat{\\mu}_{s,t}$ and $\\hat{\\Sigma}_{s,t}$ are the expectation and covariance of the distribution (i.e. $\\mathbb{P}(\\theta_{s,\\ast}=\\theta|H_{t}))$ of $\\theta_{s,*}$ conditioned on the history $H_{t}$ , and then takes action with the highest upper confidence bound : $A_{s,t}\\gets\\arg\\operatorname*{max}_{a\\in\\mathcal{A}}U_{t,s,a}$ ", "page_idx": 3}, {"type": "text", "text": "4.2 Multi-Task Gaussian Linear Bandit and Semi-Bandit ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The hierarchical Gaussian environment is generated as follow. In the multi-task linear bandit setting: (1) $\\mu_{*}\\;\\sim\\;{\\mathcal N}(\\mu_{q},\\Sigma_{q})$ (2) $\\theta_{s,*}|\\bar{\\mu}_{*}\\ \\sim\\ \\mathcal{N}(\\mu_{*},\\Sigma_{0}),\\forall s\\ \\in\\ [m].$ (3) $Y_{s,t}|A_{s,t},\\theta_{s,*}~\\sim$ $\\mathcal{N}(A_{s,t}^{\\top}\\theta_{s,*},\\sigma^{2}),\\forall t\\,\\geq\\,1,s\\,\\,\\bar{\\in}\\,\\,\\mathcal{S}_{t}$ ; In the semi-bandit setting, the only difference lies in step (3) where $Y_{s,t,a}|A_{s,t},\\theta_{s,*}\\sim\\mathcal N(\\langle\\Phi_{a},\\theta_{s,*}\\rangle,\\sigma^{2})$ for any $a\\in A_{s,t}$ . Here, $\\mu_{q},\\mu_{*},\\theta_{s,*}$ are $d$ dimensional vectors; $\\Sigma_{q}$ $\\mathbf{\\Psi},\\Sigma_{0}\\in\\mathbb{R}^{d\\times d}$ are positive semi-definite covariance matrices. In the above two settings, the reward noise can be regarded as ${\\mathcal{N}}(0,\\sigma^{2})$ . In the following theoretical analysis sections, we assume that all of $\\mu_{q},\\Sigma_{q},\\Sigma_{0}$ and $\\sigma$ are known by the agent to guarantee an analytically tractable posterior. ", "page_idx": 3}, {"type": "text", "text": "Concretely, using some basic algebraic computations in hierarchical Gaussian model (e.g. see [25, Appendix D]), we can obtain the closed-form hyper-posterior in round $t$ as $Q_{t}(\\mu)=\\mathcal{N}(\\mu;\\bar{\\mu}_{t},\\bar{\\Sigma}_{t})$ where the expectation $\\bar{\\mu}_{t}$ and the covariance matrix $\\bar{\\Sigma_{t}}$ of $Q_{t}(\\mu)$ have the following explicit forms: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\bar{\\mu}_{t}=\\bar{\\Sigma}_{t}\\bigl(\\Sigma_{q}^{-1}\\mu_{q}+\\sum_{s\\in[m]}\\,(\\Sigma_{0}+G_{s,t}^{-1})^{-1}G_{s,t}^{-1}B_{s,t}\\bigr),\\qquad\\bar{\\Sigma}_{t}^{-1}=\\Sigma_{q}^{-1}+\\sum_{s\\in[m]}\\,(\\Sigma_{0}+G_{s,t}^{-1})^{-1}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "table", "img_path": "joNPMCzVIi/tmp/b725e47f7147362ecd7dbaf5d4f1f35442d87092e8d58f3014e089d534751b75.jpg", "table_caption": ["Algorithm 1 Hierarchical Bayesian Algorithms Algorithm 2 Hierarchical Bayesian Algorithms "], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "Here, in the bandit setting $\\begin{array}{r}{G_{s,t}\\,=\\,\\sigma^{-2}\\sum_{\\ell<t}{\\bf1}\\{s\\,\\in\\,\\mathcal{S}_{\\ell}\\}A_{s,\\ell}A_{s,\\ell}^{\\top}}\\end{array}$ and $\\begin{array}{r}{B_{s,t}\\,=\\,\\sigma^{-2}\\sum_{\\ell<t}{{\\bf1}\\{s\\,\\in}}}\\end{array}$ $S_{\\ell}\\}A_{s,\\ell}Y_{s,\\ell}$ in the semi-bandit setting $\\begin{array}{r}{G_{s,t}=\\sigma^{-2}\\sum_{\\ell<t}{\\bf1}\\{s\\in S_{\\ell}\\}(\\sum_{a\\in A_{s,\\ell}}\\Phi_{a}\\Phi_{a}^{\\top})}\\end{array}$ and $B_{s,t}=$ $\\begin{array}{r}{\\sigma^{-2}\\sum_{\\ell<t}\\mathbf{1}\\{s\\in S_{\\ell}\\}(\\sum_{a\\in A_{s,\\ell}}\\Phi_{a}\\hat{\\mathbf{w}}_{s,t}(a))}\\end{array}$ - After the hyper-parameter $\\mu_{t}$ is sampled from $Q_{t}(\\mu)$ \uff0c we sample task parameter $\\theta_{s,t}\\sim\\mathcal{N}(\\theta;\\tilde{\\mu}_{s,t},\\tilde{\\Sigma}_{s,t})$ for task $s$ , where $\\tilde{\\mu}_{s,t}=\\tilde{\\Sigma}_{s,t}\\big(\\Sigma_{0}^{-1}\\mu_{t}+B_{s,t}\\big)$ is the posterior mean, -1 $\\tilde{\\Sigma}_{s,t}^{-1}=\\Sigma_{0}^{-1}+G_{s,t}$ the posterior covariance matrix. Such posterior of a linear model is obtained with a Gaussian prior $\\mathcal{N}(\\mu_{t},\\Sigma_{0})$ and Gaussian observations $(Y_{s,\\ell})_{\\ell<t,s\\in S_{\\ell}}$ by Bayes rule. On the other hand, we also need to handle $\\mathbb{P}(\\theta_{s,\\ast}\\,=\\,\\theta|H_{t})$ . It is not difficult to see that, in the multi-task Gaussian linear bandit/semi-bandit setting, $\\theta_{s,*}|H_{t}$ is Gaussian and denoted as $\\mathbb{P}(\\theta_{s,*}=$ $\\theta|H_{t}){=}\\mathcal N(\\theta;\\hat{\\mu}_{s,t},\\hat{\\Sigma}_{s,t})$ . According to Lemma B.1, $\\hat{\\mu}_{s,t}$ and $\\hat{\\Sigma}_{s,t}$ have the following explicit forms: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{\\mu}_{s,t}\\!=\\!\\tilde{\\!\\Sigma}_{s,t}\\!\\big(\\Sigma_{0}^{-1}\\bar{\\mu}_{t}\\!+\\!B_{s,t}\\big),\\quad\\hat{\\Sigma}_{s,t}=\\tilde{\\Sigma}_{s,t}+\\tilde{\\Sigma}_{s,t}\\Sigma_{0}^{-1}\\bar{\\Sigma}_{t}\\Sigma_{0}^{-1}\\tilde{\\Sigma}_{s,t}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "5  Bayes Regret Bounds ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we provide improved regret bounds of hierarchical Bayesian bandit algorithms for multi-task Gaussian linear bandit/semi-bandit problem. Concretely, we provide improved analysis for HierTS in the sequential linear bandit setting (Sections 5.1), propose a novel HierBayesUCB bandit algorithm with logarithmic regret guarantee (Section 5.2), develop regret bounds for these two algorithms in the concurrent linear bandit setting (Section 5.3), and finally extend these two algorithms to the semi-bandit setting (Section 5.4) with improved regret bounds. In the proof for our theoretical results, the most important step is to give an upper bound on the so-called posterior variance $\\nu_{m,n}$ , which in the multi-task linear bandit setting is defined and upper bounded as follow: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{V}_{m,n}\\triangleq\\mathbb{E}\\Big[\\sum_{t\\ge1}\\sum_{s\\in S_{t}}\\Vert A_{s,t}\\Vert_{\\hat{\\Sigma}_{s,t}}^{2}\\Big]\\leq O\\big(m d\\log(\\frac{n}{d})+d\\log(\\frac{m}{d})\\big).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Although the above bound on $\\nu_{m,n}$ achieves the same order (w.r.t. $m,n$ and $d$ ) as that in the latest boundasmbadtngpserorvaans and can be bounded in a similar way. To finish the whole proof, our strategy consists of two main steps: (1) The first step is to transform the multi-task Bayes regret $B\\mathcal{R}(m,n)$ into an intermediate regret upper bound that involves the posterior variance $\\nu_{m,n}$ as the dominant term. (2) The second step is to bound $\\nu_{m,n}$ with Eq. (4). Combining the results in steps (1) and (2) yields Bayes regret bound for multi-task hierarchical Bayesian bandit/semi-bandit algorithms. Detailed comparisons between our regret bounds and others in the bandit setting are shown in Table 1. Next, we define $c_{1}\\!=\\!\\sigma^{2}\\!\\!+\\!B^{2}\\lambda_{1}\\bar{(\\Sigma_{0})}$ $c_{2}=\\sigma^{2}{+}B^{2}\\lambda_{1}(\\Sigma_{0})\\,{+}\\,B^{2}\\lambda_{1}(\\Sigma_{q})\\kappa(\\bar{\\Sigma_{0}})$ to be used through the whole Section 5. ", "page_idx": 4}, {"type": "text", "text": "Table 1: Different Bayes regret bounds for multi-task $d$ -dimensional linear (or $K$ -armed) bandit problem in the sequential setting. $m$ is the number of tasks, $n$ the number of iterations per task, $\\boldsymbol{\\mathcal{A}}$ is the action set. Bayes Regret Bound $\\r=$ Bound ${\\bf I}+{\\bf\\delta I}$ Bound $\\mathbf{U}+\\mathbf{N}$ Negligible Terms, where Bound I is the regret bound for solving $m$ tasks, Bound $\\mathbf{II}$ the regret bound for learning hyper-parameter $\\mu_{*}$ ", "page_idx": 5}, {"type": "table", "img_path": "joNPMCzVIi/tmp/deb95a3e091e1d0f431caa3277105a3bfea753523598e87601565750f016c65a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "5.1  Improved Regret Bound for HierTS in the Sequential Bandit Setting ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In the sequential bandit setting, $|S_{t}|=1$ . Then, conditioned on $H_{t}$ , it is not difficult to see that in Bayes regret, each term $\\mathbb{E}[\\theta_{s,*}^{\\top}A_{s,*}-\\theta_{s,*}^{\\top}A_{s,t}|H_{t}]=\\mathbb{E}\\big[(\\theta_{s,*}-\\hat{\\mu}_{s,t})^{\\top}A_{s,*}\\big|H_{t}\\big]$ , and we use a novel Cauchy-Schwartz type inequality from [21, Prop 2] to bound $\\mathbb{E}\\big[(\\theta_{s,*}-\\hat{\\mu}_{s,t})^{\\top}A_{s,*}\\big|H_{t}\\big]$ , leading 10 $\\begin{array}{r}{\\mathcal{B}\\mathcal{R}(m,n)\\!\\leq\\!\\mathbb{E}\\!\\left[\\sum_{t\\geq1}\\sum_{s\\in\\mathcal{S}_{t}}\\sqrt{d\\mathbb{E}\\big[\\big((\\theta_{s,*}-\\hat{\\mu}_{s,t})^{\\top}A_{s,t}\\big)^{2}\\big|H_{t}\\big]}\\right]}\\end{array}$ . Expand the expression in the right hand side of the above inequality, we then have $\\begin{array}{r}{\\mathcal{B}\\mathcal{R}(m,n)\\leq\\mathbb{E}\\sum_{t,s\\in S_{t}}\\sqrt{d A_{s,t}^{\\top}\\hat{\\Sigma}_{s,t}A_{s,t}}\\leq}\\end{array}$ $\\sqrt{d m n\\nu_{m,n}}$ reducing the Bayes regret bound to the posterior variance bound problem. Recalling Eq. (4) achieves our first improved Bayes regret upper bound in the sequential linear bandit setting. ", "page_idx": 5}, {"type": "text", "text": "Theorem 5.1 (Near-Optimal Sequential Regret) Let $|\\cal S_{t}|=1$ foranyround $t$ Then in themulti-task Gaussian linear bandit setting, the Bayes regret upper bound of HierTS is as follow: ", "page_idx": 5}, {"type": "equation", "text": "$$\nB\\mathcal{R}(m,n)\\leq d\\sqrt{2m n}\\sqrt{m c_{1}\\log\\left(1+\\frac{n}{d}\\right)+c_{2}\\log\\left(1+\\frac{m\\,\\mathrm{Tr}\\bigl(\\Sigma_{q}\\Sigma_{0}^{-1}\\bigr)}{d}\\right)}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Our explanations for the above sequential regret bound are three-fold: (1) The term $m d\\sqrt{n c_{1}\\log{(1+n/d)}}$ represents the regret bound for solving $m$ bandit tasks, whose parameters $\\theta_{s,*}$ are drawn i.i.d. from the prior distribution $\\mathcal{N}(\\mu_{*},\\Sigma_{0})$ . Under this assumption, no task provides information for any other task, and hence this bound is linear in $m$ . Similar observation was also pointed out by [25, 7, 17]. (2) The term $d\\sqrt{m n c_{2}\\log(1{+}m\\mathrm{Tr}(\\Sigma_{q}\\Sigma_{0}^{-1})/d)}$ represents the regret bound for learning the hyper-parameter $\\mu_{*}$ . Such bound is sublinear in $m$ and is not a dominant term when $m$ is large. (3) For a large $m$ , the averaged Bayes regret bound across $m$ tasks is of ${\\mathcal{B R}}(m,n)/m\\,=\\,O(d{\\sqrt{n\\log n}})$ , and strengthens the latest averaged bound $O(d{\\sqrt{n}}\\log n)$ in [17, Thm 3] by a factor $\\sqrt{\\log n}$ . Besides, since the lower Bayes regret bound for any Bayesian bandit algorithm is $\\Omega(d{\\sqrt{n}})$ [30], our task-averaged Bayes regret bound is within $O({\\sqrt{\\log n}})$ of optimality and hence is called Near-Optimal sequential regret bound. We further make a detailed comparison between our regret bound in Theorem 5.1 and the regret bound [17, Thm 3] in the following remark. ", "page_idx": 5}, {"type": "text", "text": "Remark 5.1 (Improvements of Our Theorem 5.1 over the Latest One) Our sequential regret bound has two improvements over the latest one in $I I7_{:}$ .Thm 3, shown in Table $I J$ $^{(I)}$ We remove the additional ${\\sqrt{\\log\\left(m n\\right)}}.$ factor in both theregret bound forsolving m bandit tasks and theregret bound for learning the hyper-parameter $\\mu_{*}$ (2) In the regret bound for learning hyper-parameter $\\mu_{*}$ \uff0c $I I7J$ has a multiplicativefactor $\\kappa^{2}(\\Sigma_{0})$ ,whereas our multiplicative factor is $\\kappa(\\Sigma_{0})$ .Such improvement is achieved by using technical matrix analysis proposed in Lemma C.1. and explained in Remark A.1. ", "page_idx": 5}, {"type": "text", "text": "5.2  Logarithmic Regret Bound for HierBayesUCB in the Sequential Bandit Setting ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we attempt to provide further improved Bayes regret bounds for hierarchical bandit algorithms in the sequential bandit setting. Because the task averaged Bayes regret bound in Theorem 5.1 is near optimal, it is not easy to derive improved Bayes regret bounds under the same assumptions. Therefore, we further assume that the action set $\\boldsymbol{\\mathcal{A}}$ is finite, and propose a novel hierarchical Bayesian bandit algorithm, named Hierarchical BayesUCB (HierBayesUCB), for multitask linear bandit problem. The pseudo-code of our proposed algorithm is shown in Algorithm 1. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Next, we introduce some necessary notations. Let $\\Delta_{s,t}\\ =\\ \\theta_{s,*}^{\\top}(A_{s,*}\\ -\\ A_{s,t})$ $\\Delta_{s,\\operatorname*{min}}~=$ $\\begin{array}{r}{\\operatorname*{min}_{a\\in\\mathcal{A}\\setminus\\{A_{s,*}\\}}\\left(\\theta_{s,*}^{\\top}A_{s,*}\\,-\\,\\theta_{s,*}^{\\top}a\\right)}\\end{array}$ $\\Delta_{\\mathrm{min}}~=~\\mathrm{min}_{s\\in[m]}\\,\\Delta_{s,\\mathrm{min}}$ . For any $\\epsilon\\mathrm{~\\,~>~\\,~0~}$ , let $\\Delta_{\\mathrm{min}}^{\\epsilon}\\ =$ $\\operatorname*{max}\\{\\epsilon,\\Delta_{\\operatorname*{min}}\\}$ . Define the event $\\begin{array}{r}{E_{s,t}=\\{\\forall a\\in\\mathcal{A}:|a^{\\top}({\\boldsymbol\\theta}_{s,*}-\\hat{\\mu}_{s,t})|\\!\\leq\\!\\sqrt{2\\log\\frac{1}{\\delta}}\\lVert a\\rVert_{\\hat{\\Sigma}_{s,t}}\\}}\\end{array}$ . Then, analogous to [3], we decompose the Bayes regret $\\begin{array}{r}{\\mathcal{B}\\mathcal{R}(m,n)=\\mathbb{E}\\sum_{t\\geq1}\\sum_{s\\in S_{t}}\\!\\Delta_{s,t}}\\end{array}$ into three terms: $\\begin{array}{r}{\\mathbb{E}\\sum_{t>1,s\\in{\\cal S}_{t}}\\Delta_{s,t}\\big[{\\bf1}\\{\\Delta_{s,t}~\\geq~\\epsilon,E_{s,t}\\}+{\\bf1}\\{\\Delta_{s,t}~<~\\epsilon,E_{s,t}\\}+{\\bf1}\\{\\bar{E_{s,t}}\\}\\big]}\\end{array}$ . We can bound the last two terms trivially with $m n[\\epsilon+2\\delta\\big(\\operatorname*{max}_{t,s}\\lvert\\Delta_{s,t}\\rvert\\big)\\cdot\\lvert A\\rvert]$ . For the first term, we use the fact that $A_{s,t}|H_{t}\\stackrel{\\mathrm{i.i.d.}}{\\sim}A_{s,*}|H_{t}$ , as well as the Upper Confidence Bound (UCB) technique to reduce it to an intermediate upper bound $\\begin{array}{r}{\\big(\\sum_{t\\geq1,s\\in\\mathcal{S}_{t}}\\|A_{s,t}\\|_{\\hat{\\Sigma}_{s,t}}^{2}\\log\\frac{1}{\\delta}\\big)/\\operatorname*{min}_{s,t}\\big|\\Delta_{s,t}\\big|}\\end{array}$ . Combining the upper bound over $\\nu_{m,n}$ in Eq. (4), HierBayesUCB can achieve the following logarithmic Bayes regret bound in the sequential bandit setting (the logarithmic bound can be extended to the concurrent setting). ", "page_idx": 6}, {"type": "text", "text": "Theorem 5.2 (Logarithmic Sequential Regret of HierBayesUCB) Let $|\\mathcal{S}_{t}|=1$ for any round $t$ and the action set $\\boldsymbol{\\mathcal{A}}$ is finitewith $|{\\bar{A}}|<\\infty$ Then in themulti-taskGaussian linearbandit setting,forany $\\delta\\in(0,1)$ \uff0c $\\epsilon>0$ the Bayes regret $B\\mathcal{R}(m,n)$ of HierBayesUCB is upper bounded by ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\i n\\Big[\\epsilon\\!+\\!4B\\delta\\lambda_{1}^{\\frac{1}{2}}(\\Sigma_{0}\\!+\\!\\Sigma_{q})\\big(d^{\\frac{1}{2}}\\!+\\!\\|\\mu_{q}\\|_{\\dot{\\Sigma}_{s,1}^{-1}}\\big)|\\mathcal{A}\\big]\\Big]\\!+\\!\\mathbb{E}\\big[\\frac{16d\\log\\frac{1}{\\delta}}{\\Delta_{\\operatorname*{min}}^{\\epsilon}}\\big]\\Big[m c_{1}\\log\\big(1\\!+\\!\\frac{n}{d}\\big)\\!+\\!c_{2}\\log\\big(1\\!+\\!\\frac{m\\,\\mathrm{Tr}\\big(\\Sigma_{q}\\Sigma_{0}^{-1}\\big)}{d}\\big)\\Big].\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We give more explanations for the above sequential regret in terms of the following five aspects: (1) If let $\\delta\\,=\\,1/(m n),\\,\\epsilon\\,=\\,1/(m n)$ and $\\Delta_{\\mathrm{min}}~>>~\\epsilon$ , the above sequential regret bound is of $\\begin{array}{r}{O\\big(\\log{(m n)}(m d\\log{(\\frac{n}{d})}+d\\log{(\\frac{m}{d})})\\big)}\\end{array}$ . The term $\\begin{array}{r}{O(m d\\log{(m n)}\\log{(\\frac{n}{d})})}\\end{array}$ represents the regret bound for solving $m$ bandit tasks and is linear in $m$ . Such bound is sharper than the corresponding bound $\\begin{array}{r}{O(m d\\sqrt{n\\log\\left(\\frac{n}{d}\\right)})}\\end{array}$ in our Theorem 5.1 by a multiplicative factor $O(\\sqrt{\\log{(n/d)}/n}\\log{(m n)})$ \uff0c which is less than 1 especially when $m\\ \\leq\\ n$ .(2) The term $\\begin{array}{r}{O(d\\log{\\left(m n\\right)}\\log{\\left(\\frac{m}{d}\\right)})}\\end{array}$ represents the regret bound for learning the hyper-parameter $\\mu_{*}$ , and its contribution to the Bayes regret bound can be negligible. Besides, this bound is sharper than the bound $O(d{\\sqrt{m n\\log{(m/d)}}})$ in our Theorem 5.1. (3) The averaged Bayes regret bound across $m$ tasks can be regarded as $\\mathcal{B R}(m,n)/m=O(d\\log{(m n)}\\log{n})$ , which is logarithmic in $n$ . Therefore, we call our regret bound as \u201cLogarithmic\u2019 sequential regret bound. Moreover, if there exists a fixed positive integer $i<<n$ such that $m\\leq n^{i}$ tnurask-averagBayr $\\mathcal{B R}(m,n)/m=O(d\\bar{\\mathbb{E}}[\\frac{1}{\\Delta_{\\operatorname*{min}}^{\\epsilon}}]\\log^{2}n)$ matches the latest single-task Bayes regret bound in [3, Thm 5] and is remarkably similar to the frequentist regret $O(d\\bar{\\Delta_{\\operatorname*{min}}^{-1}}\\log^{2}n)$ in [1, Thm 5] . (4) We can obtain sharper bounds by setting $\\delta,\\epsilon$ as different $\\delta=1/n$ $\\begin{array}{r}{O([m n\\epsilon+m]+\\frac{\\log n}{\\Delta_{\\operatorname*{min}}^{\\epsilon}}m\\log n)}\\end{array}$ which is of order $O(m\\log^{2}n)$ if we set $\\epsilon\\,=\\,1/(m n)$ and the gap $\\Delta_{\\mathrm{min}}>>\\epsilon$ is large. (5) We also need to point out that, the Bayes regret bound in Theorem 5.2 scales with $\\mathbb{E}[\\frac{1}{\\Delta_{\\operatorname*{min}}^{\\epsilon}}]$ If the gap $\\Delta_{\\mathrm{min}}\\leq1/(m n)$ , then $\\Delta_{\\mathrm{min}}^{\\epsilon}=1/(m n)$ and this may cause a large Bayes regret upper bound. ", "page_idx": 6}, {"type": "text", "text": "5.3 Improved Regret Bounds of HierTS and HierBayesUCB in the Concurrent Bandit Setting ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In the concurrent bandit setting, there exists a positive integer $L\\leq m$ , such that $1\\leq|S_{t}|\\leq L$ .The concurrent bandit setting is thus more challenging than the sequential bandit setting, because the agent in the concurrent setting needs to interact with multiple bandit tasks in parallel at each round $t\\geq1$ , and the hyper-posterior $Q_{t}$ will not be updated until the end of round $t$ . Therefore, we need to make an additional assumption on the action space $\\boldsymbol{\\mathcal{A}}$ as follow to facilitate our theoretical analysis. ", "page_idx": 6}, {"type": "text", "text": "Assumption 5.1 There exs actions $\\{a_{i}\\}_{i=1}^{d}\\subseteq A_{\\!}$ a constant $\\beta\\!>\\!0$ such hat $\\lambda_{d}(\\sum_{i=1}^{d}a_{i}a_{i}^{\\top})\\geq\\beta$ \uff0c This assumption is also used in previous works [7, 17] for hierarchical Bayesian linear bandit. It indicates that $\\textstyle\\sum_{i=1}^{d}a_{i}a_{i}^{\\top}$ is apositivedenitematix, and dost weaken th genrality f theoretical results. Actually, if $\\mathbb{R}^{d}$ is not spanned by actions in $\\boldsymbol{\\mathcal{A}}$ , we can project $\\boldsymbol{\\mathcal{A}}$ into a subspace where the assumption holds. We also need to modify the HierTS algorithm to let the agent take the basic actions {a}=1 for the first $d$ interactions in any task $s\\in[m]$ This modification guarantees that the agent explores all directions within the task. Such exploration is very similar to the initialization method in UCB type $K$ -arm bandit algorithms [6, 4], which choose to pull each arm in the first $K$ rounds. Define $\\dot{c_{3}}\\dot{=}1+B^{2}\\sigma^{-2}\\kappa(\\Sigma_{0})\\Big7\\lambda_{1}(\\Sigma_{0})+\\sigma^{2}/\\beta\\Big]$ that will be used throughout the concurrent setting. Then, analogous to the proof for Theorem 5.1, we bound $\\sqrt{m n\\nu_{m,n}}$ with a more refined analysis, achieving the following improved Bayes regret bound for HierTS in the concurrent setting. ", "page_idx": 6}, {"type": "table", "img_path": "joNPMCzVIi/tmp/030ec7cf75938c241d27f6f2cfa02dde6e2ad36a829f7025b47a13228b916ea4.jpg", "table_caption": ["Table 2: Different Bayes regret bounds for multi-task semi-bandit problem. Bayes Regret Bound =Bound $\\mathbf{I}+$ Bound $\\mathbf{II}+$ Negligible Terms. $m$ is the number of tasks, $n$ the number of iterations per task, $K$ the size of action set, $L$ the number of pulled actions at each round $(1\\leq L\\leq K)$ . Bound I is the regret bound for solving $m$ tasks, Bound $\\mathbf{II}$ the regret bound for learning hyper-parameter $\\mu_{*}$ "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Theorem 5.3 Under Assumption 5.1, let $1\\leq|S_{t}|\\leq L$ for anyround $t\\geq1$ .Then in themulti-task Gaussian linear bandit setting, the Bayes regret $B\\mathcal{R}(m,n)$ of HierTS is upper bounded by ", "page_idx": 7}, {"type": "equation", "text": "$$\n;B m d\\sqrt{\\lambda_{1}(\\Sigma_{0}+\\Sigma_{q})}(\\sqrt{d}+\\|\\mu_{q}\\|_{\\hat{\\Sigma}_{s,1}^{-1}})+d\\sqrt{m n}\\sqrt{2m c_{1}\\log{(1+\\frac{n}{d})}+2c_{2}c_{3}\\log{(1+\\frac{m\\,\\mathrm{Tr}\\left(\\Sigma_{q}\\Sigma_{0}^{-1}\\right)}{d})}}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The concurrent regret bound in Theorem 5.3 achieves almost the same order (w.r.t. $m,n,d)$ as the sequential regret bound in Theorem 5.1, but differs in two aspects: (1) The bound for learning $m$ i.i.d. bandit tasks has an additional term $B m d\\sqrt{\\lambda_{1}(\\Sigma_{0}+\\Sigma_{q})}(\\overline{{{\\sqrt{d}}}}+\\|\\mu_{q}\\|_{\\hat{\\Sigma}_{s,1}^{-1}})$ . This is due to the fact that we take the basic actions $\\{a_{i}\\}_{i=1}^{d}$ first for each task $s\\in[m]$ in the modified HierTS algorithm. (2) The bound for learning the hyper-parameter $\\mu_{*}$ has an additional multiplicative factor $c_{3}$ . This is the price for deriving regret bounds in the concurrent setting. Nevertheless, when compared with the latest concurrent regret bound in [17, Thm 4] for HierTS, our concurrent regret bound in Theorem 5.3 removes the $\\sqrt{\\log\\left(m n\\right)}$ factor in both the regret bound for learning $m$ bandit tasks and the regret bound for learning hyper-parameter $\\mu_{*}$ . Detailed comparisons between different concurrent regret bounds for multi-task linear bandit setting are listed in Table 3. Furthermore, utilizing the proof strategy to demonstrate the logarithmic sequential regret for HierBayesUCB in our Theorem 5.2, we can analogously develop a logarithmic concurrent regret upper bound for HierBayesUCB algorithm, which is deferred to our Theorem C.2 in Appendix C due to the limited space of the main paper. ", "page_idx": 7}, {"type": "text", "text": "5.4   Improved Regret Bounds for HierTS and HierBayesUCB in the Semi-Bandit Setting ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we extend the HierTS and HierBayesUCB algorithms to the multi-task Gaussian combinatorial semi-bandit setting. The pseudo-code of them is shown in Algorithm 2. Algorithm 2 is very similar to Algorithm 1 (i.e. the multi-task linear bandit algorithms), except that the combinatorial HierTS in Algorithm 2 uses the approximation/randomized algorithm ORACLE to solve combinatorial problem $\\bar{A_{*}}\\in\\arg\\operatorname*{max}_{A\\in\\mathcal{A}}\\bar{\\sum_{a\\in A}\\mathbf{w}}(a)$ and denotes the solution as $A_{\\ast}=\\mathrm{ORACLE}(A,\\mathcal{A},\\mathbf{w})$ We adopt the ORACLE operator as in the seminal works [11, 38] to guarantee the efficiency of combinatorial HierTS semi-bandit algorithm. In this section, we only consider the sequential semi-bandit setting (i.e. $|S_{t}|\\,=1\\rangle$ for ease of presentation, and our results can be extended to the concurrent semi-bandit setting. Then, define $\\dot{c}_{4}=\\sigma^{2}+B^{2}L\\lambda_{1}(\\Sigma_{0})+B^{2}\\lambda_{1}(\\Sigma_{q})\\kappa(\\Sigma_{0})$ , we first derive the Bayes regret upper bound for combinatorial HierTS algorithm in the sequential semi-bandit setting. ", "page_idx": 7}, {"type": "text", "text": "Theorem 5.4 Let $|S_{t}|\\,=\\,1$ forany $t\\,\\geq\\,1$ Let $c\\ \\geq\\ {\\sqrt{2\\ln\\left({\\frac{n K B\\lambda_{1}(\\Sigma_{0})}{\\sqrt{2\\pi}}}\\right)}}$ then in the muli-task Gaussian semi-bandit setting, the Bayes regret upper bound of combinatorial HierTS is: ", "page_idx": 7}, {"type": "equation", "text": "$$\nB\\mathcal{R}(m,n)\\leq m+c\\sqrt{m n L}\\sqrt{2c_{1}m\\log{(1+\\frac{n L}{d})}+2c_{4}L d\\log(1+\\frac{m\\,\\mathrm{Tr}\\bigl(\\Sigma_{0}^{-1}\\Sigma_{q}\\bigr)}{d})}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Detailed comparisons between different Bayes regret bounds for multi-task semi-bandit problem are listed in Table 2. We can see that, in our Theorem 5.4, both the regret bound $O(m{\\sqrt{n}}\\log n)$ for learning $m$ tasks and the regret bound $O({\\sqrt{m n\\log m\\log n}})$ for learning hyper-parameter $\\mu_{*}$ can achieve the same order (w.r.t. $m$ and $n$ ) when compared with the latest bound in [7, Thm 6]. Besides, our Bayes regret bound is logarithmic in the number $K$ of items, whereas the Bayes regret bound in [7, Thm 6] is sublinear in $K$ . Therefore, our regret bound becomes sharper when the size of action set is very large, e.g. $K\\!>\\!>L$ . Next, we derive a gap-dependent logarithmic multi-task Bayes regret bound for our proposed combinatorial HierBayesUCB algorithm in the sequential semi-bandit setting. Theorem 5.5 Let $|S_{t}|=1$ for any $t\\geq1$ Then for any $\\epsilon>0,\\delta\\in(0,1)$ ,in themulti-taskGaussian semi-bandit setting, the Bayes regret $B\\mathcal{R}(m,n)$ of combinatorial HierBayesUCBis bounded by $z n\\big[\\epsilon+4L B K\\delta\\lambda_{1}^{\\frac{1}{2}}(\\Sigma_{0}+\\Sigma_{q})(d^{\\frac{1}{2}}+\\|\\mu_{q}\\|_{\\dot{\\Sigma}_{s,1}^{-1}})\\big]+\\mathbb{E}\\big[\\frac{8L\\log\\frac{1}{\\delta}}{\\Delta_{\\operatorname*{min}}^{\\epsilon}}\\big]\\Big[2c_{1}m\\log{(1+\\frac{n L}{d})}+2c_{4}L d\\log(1+\\frac{m\\operatorname{Tr}(\\Sigma_{0}^{-1}\\Sigma_{q})}{d}\\Big)\\Big]$ In Theorem 5.5, if we set $\\delta=1/(m n K)$ \uff0c $\\epsilon=1/(m n)$ , and $\\Delta_{\\operatorname*{min}}>>\\epsilon$ , then the regret bound $O{\\left({m\\log n}\\log\\left({m n}\\right)\\right)}$ for learning $m$ tasks is logarithmic in $n$ . Such bound is sharper than the latest one $O(m{\\sqrt{n}}\\log n)$ in [7, Thm 6] for multi-task semi-bandit. The regret bound $O(\\log m\\log{(m n)})$ for learning hyper-parameter $\\mu_{*}$ is also sharper than that of $O({\\sqrt{m n\\log m\\log n}})$ in [7, Thm 6]. Besides, since $\\delta\\!=\\!1/(m n K)$ , the whole Bayes regret bound is also logarithmic in the number $K$ of items. Nevertheless, we should point out that our bounds hold for the multi-task semi-bandits with linear generalization, but [7] focuses on the multi-task $K$ -arm semi-bandits without feature matrix $\\Phi$ ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "5.5  Technical Novelties for Deriving Improved Regret Bounds ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we summarize our technical novelties in terms of the following three aspects: ", "page_idx": 8}, {"type": "text", "text": "(1) For the improved regret bound for HierTS in Theorem 5.1: our proof has three novelties: (i) We apply a novel Cauchy-Schwartz type inequality in Lemma A.2 to bound $\\mathbb{E}\\big[(\\theta_{s,*}-\\hat{\\mu}_{s,t})^{\\top}A_{s,*}\\big|\\dot{H}_{t}\\big]\\leq$ $\\sqrt{d\\mathbb{E}\\big[\\big((\\theta_{s,*}-\\hat{\\mu}_{s,t})^{\\top}A_{s,t}\\big)^{2}\\big|H_{t}\\big]}$ , leading to a sharper bound without $\\sqrt{\\log\\left(m n\\right)}$ factor: ", "page_idx": 8}, {"type": "equation", "text": "$$\nB\\mathcal{R}(m,n)\\leq\\mathbb{E}\\sum_{t,s\\in\\mathcal{S}_{t}}\\sqrt{d A_{s,t}^{\\top}\\hat{\\Sigma}_{s,t}A_{s,t}}\\leq\\sqrt{d m n\\mathcal{V}_{m,n}}\\leq O(m\\sqrt{n\\log n}).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "(ii) We use a more technical positive semi-definite matrix decomposition analysis (i.e. our Lemma A.1) to reduce the multiplicative factor $\\kappa^{2}(\\Sigma_{0})$ to $\\kappa(\\Sigma_{0})$ . (ii) Define a new matrix $\\tilde{X}_{s,t}$ such that the denominator in the regret is $\\sigma^{2}+B^{2}\\lambda_{1}(\\Sigma_{0})$ , not just $\\sigma^{2}$ , avoiding the case that the variance serves alone as the denominator. Such technical novelties are also listed explicitly in Table 4. ", "page_idx": 8}, {"type": "text", "text": "(2) For the improved regret bound for HierBayesUCB in Theorem 5.2 in the sequential bandit setting: our novelty lies in decomposing the Bayes regret $\\begin{array}{r}{\\mathcal{B}\\mathcal{R}(m,n)=\\mathbb{E}\\sum_{t\\geq1}\\sum_{s\\in S_{t}}\\!\\!\\!\\!\\Delta_{s,t}}\\end{array}$ into threeterms: $\\mathbb{E}\\sum_{t\\geq1}\\sum_{s\\in S_{t}}\\Delta_{s,t}=\\mathbb{E}\\sum_{t\\geq1,s\\in S_{t}}\\Delta_{s,t}\\big[\\mathbf{1}\\big\\{\\Delta_{s,t}\\geq\\epsilon,E_{s,t}\\big\\}+\\mathbf{1}\\big\\{\\Delta_{s,t}<\\epsilon,E_{s,t}\\big\\}+\\mathbf{1}\\big\\{\\bar{E}_{s,t}\\big\\}\\big],$ and bounding the first term with a new method as well as the property of BayesUCB algorithm as ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathbb{E}\\Delta_{s,t}\\mathbf{1}\\{\\Delta_{s,t}\\ge\\epsilon,E_{s,t}\\}=\\mathbb{E}\\frac{\\Delta_{s,t}^{2}}{\\Delta_{s,t}}\\mathbf{1}\\{\\Delta_{s,t}\\ge\\epsilon,E_{s,t}\\}\\le\\mathbb{E}\\frac{C_{t,s,A_{s,t}}^{2}}{\\Delta_{\\operatorname*{min}}^{\\epsilon}},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "resulting in the final improved gap-dependent regret bound for HierBayesUCB as follows ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\big(\\sum_{t\\geq1,s\\in S_{t}}\\|A_{s,t}\\|_{\\dot{\\Sigma}_{s,t}}^{2}\\log\\frac1\\delta\\big)/\\Delta_{\\operatorname*{min}}^{\\epsilon}\\leq O\\big(m\\log\\left(n\\right)\\log\\frac1\\delta\\big)\\xrightarrow{\\delta=1/m n}O\\big(m\\log\\left(n\\right)\\log\\left(m n\\right)\\big).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "(3) For the improved regret bounds for HierTS and HierBayesUCB in the concurrent setting and in the semi-bandit setting: besides the aforementioned technical novelties in (1) and (2), the additional technical novelty lies in leveraging more refined analysis (e.g. using Woodbury matrix identity) to bound the gap between matrices $\\bar{\\Sigma}_{t+1}^{-1}$ and $\\bar{\\Sigma}_{t}^{-1}$ (more details is shown in Lemma C.1 and Eq. (6). ", "page_idx": 8}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we conduct experiments in the linear bandit setting to verify our theoretical results. Specifically, we show the influence of hyper-parameters (e.g. $m,n,L)$ to the multi-task Bayes regret of HierTS and HierBayesUCB, to validate the consistency between their regret bounds and practical performance. Besides, we compare the performance between our algorithms and other baselines, to show the effectiveness of hierarchical Bayesian bandit algorithms in the multi-task bandit setting. ", "page_idx": 8}, {"type": "image", "img_path": "joNPMCzVIi/tmp/614188d47d0f268df6a881d0b0ffcd94ef0098d86431d1e91e42bf6100479a74.jpg", "img_caption": ["(d) Regrets w.r.t. different $\\sigma_{0}$ (e) Regrets w.r.t. different $\\sigma$ (f) Regrets of different algorithms Figure 1: Regrets of HierTS algorithm with respect to (w.r.t.) different hyper-parameters. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Experimental Setting. We follow the same experimental setting as that in [7, 17]. Concretely, we conduct linear bandit experiments with Gaussian reward. The synthetic problem is defined as follows. In most experiments, we set the number of total tasks as $m=10$ , the dimension of action space as $d=4$ , the number of concurrent tasks as $L=5$ , the number of rounds as $n=200m/L$ . We focus on the finite action space with $|\\mathcal{A}|=10$ , and each action is sampled uniformly from $[-0.5,0.5]^{d}$ . In hierarchical Bayesian model, we set the hyper-prior as zero-mean isotropic Gaussian distribution $\\mathcal{N}(\\mu_{q},\\Sigma_{q})=\\bar{\\mathcal{N}}(\\mathbf{0},\\Sigma_{q})$ , where $\\Sigma_{q}=\\sigma_{q}^{2}I_{d}$ ; and set the task variance $\\Sigma_{0}=\\sigma_{0}^{2}I_{d}$ . Unless otherwise stated, we set $\\sigma_{q}=1$ \uff0c $\\sigma_{0}=0.1$ \uff0c $\\sigma^{2}=0.5$ for each task in most experiments. We exhibit the regret performance of HierTS algorithm with respect to five hyper-parameters $m,L,\\sigma_{q},\\sigma_{0},\\sigma$ in Figure 1 (a)-(e) respectively. The regret performance of HierBayesUCB is shown in Figure 2 of Appendix F. ", "page_idx": 9}, {"type": "text", "text": "Besides, we compare HierTS/HierBayesUCB with other two TS type algorithms that do not learn the hyper-parameter $\\mu^{*}$ in a hierarchical Bayesian model. The first baseline is the vanilla TS algorithm that samples task parameter $\\theta_{s,*}$ from the marginal prior $\\mathcal{N}(\\mu_{q},\\Sigma_{q}+\\Sigma_{0})$ . The second baseline is an idealized TS algorithm that knows $\\mu_{*}$ exactly and uses the true prior $\\mathcal{N}(\\mu_{*},\\Sigma_{0})$ . We call the second baseline as OracleTS, since this TS algorithm accesses more information of $\\mu_{*}$ than HierTS and vanilla TS algorithm. We show the regret performance of these four bandit algorithms in Figure 1 (f). ", "page_idx": 9}, {"type": "text", "text": "Experimental Results. From Figure 1, we can observe that: (1) In plot (a), the multi-task regret becomes larger with the increase of $m$ and $n$ , which is consistent with our regret upper bound in Theorems 5.1. (2) In plot (b), the regret increases with a higher dimension $d$ . The number $L$ of the concurrent tasks seems do not have a large impact on regret. (3) In plots (c)-(e), the regret decreases with a smaller variance (e.g. $\\sigma_{q}$ \uff0c $\\sigma_{0}$ and $\\sigma$ ) in hierarchical Bayesian model, validating the provable benefits of variance-reduction in regret minimization, which is revealed in our multi-task Bayes regret upper bounds. (4) The task-averaged regret of HierTS is tighter than that of single-task TS algorithm, empirically demonstrating the advantages of multi-task Bayesian bandit optimization paradigm over single-task bandit learning. (5) Our proposed HierBayesUCB achieves lower regret than HierTS. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper provides improved Bayes regret bounds for hierarchical Bayesian bandit algorithms in the multi-task Gaussian linear bandit and semi-bandit setting. For linear bandit problem: in the case of infinite action set, we strengthen the preexisting regret bound $O(m{\\sqrt{n\\log n\\log\\left(m n\\right)}})$ ofHierTS to $O(m{\\sqrt{n\\log n}})$ by a factor of $O({\\sqrt{\\log\\left(m n\\right)}})$ ; in the case of finite action set, we propose a novel HierBayesUCB algorithm that achieves logarithmic regret bound $O(m\\log{(m n)}\\log{n})$ under mild conditions. Our regret bounds in the bandit setting hold when the agent solves tasks sequentially or concurrently. Then, we extend the above HierTS and HierBayesUCB algorithms to the multi-task semi-bandit setting and derive improved regret bounds. The synthetic experiments further support our theoretical results. Our future work aims to extend our bounds to the sub-exponential bandit setting. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Jiechao sincerely appreciates the financial support from the People's Government of Guangzhou Municipality for his postdoctoral project. We thank all reviewers for their constructive suggestions to improve the quality of this paper. This work was supported in part by the National Key R&D Program of China (Grant No.2023YFF0725001), in part by the National Natural Science Foundation of China (Grant No.92370204), in part by the guangdong Basic and Applied Basic Research Foundation(Grant No.2023B1515120057), in part by Guangzhou-HKUST(GZ) Joint Funding Program (Grant No.2023A03J0008), Education Bureau of Guangzhou Municipality. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1]  Y. Abbasi-Yadkori, D. Pal, and C. Szepesvari. Improved algorithms for linear stochastic bandits. In NeurIPS, pages 2312-2320, 2011.   \n[2] I. Aouali, B. Kveton, and S. Katariya. Mixed-effect thompson sampling. In AISTATS, pages 2087-2115, 2023.   \n[3]  A. Atsidakou, B. Kveton, S. Katariya, C. Caramanis, and sujay sanghavi. Finite-time logarithmic bayes regret upper bounds. In NeurIPS, 2023.   \n[4] J. Audibert and S. Bubeck. Minimax policies for adversarial and stochastic bandits. In Conference on Learning Theory (COLT), 2009.   \n[5]  P. Auer. Using confidence bounds for exploitation-exploration trade-offs. Journal of Machine Learning Research (JMLR), 3:397-422, 2002.   \n[6]  P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the multiarmed bandit problem. Machine Learning, 47(2-3):235-256, 2002.   \n[7] S. Basu, B. Kveton, M. Zaheer, and C. Szepesvari. No regrets for learning the prior in bandits. In NeurIPS, pages 28029-28041, 2021.   \n[8]  R. Caruana. Multitask learning. Machine Learning, 28(1):41-75, 1997.   \n[9] L. Cella, A. Lazaric, and M. Pontil. Meta-learning with stochastic linear bandits. In ICML, pages 1360-1370, 2020.   \n[10] L. Cella, K. Lounici, G. Pacreau, and M. Pontil. Multi-task representation learning with stochastic linear bandits. In AISTATS, pages 4822-4847, 2023.   \n[11]  W. Chen, Y. Wang, and Y. Yuan. Combinatorial multi-armed bandit: General framework and applications. In ICML, pages 151-159, 2013.   \n[12]  W. Chu, L. Li, L. Reyzin, and R. E. Schapire. Contextual bandits with linear payoff functions. In AISTATS, pages 208-214, 2011.   \n[13] V. Dani, T. P. Hayes, and S. M. Kakade. Stochastic linear optimization under bandit feedback. In Conference on Learning Theory (COLT), pages 355-366, 2008.   \n[14] A. A. Deshmukh, U. Dogan, and C. Scott. Multi-task learning for contextual bandits. In NeurIPS, pages 4848-4856, 2017.   \n[15] H. Flynn, D. Reeb, M. Kandemir, and J. Peters. Improved algorithms for stochastic linear bandits using tail bounds for martingale mixtures. In NeurIPS, 2023.   \n[16] J. Hong, B. Kveton, S. Katariya, M. Zaheer, and M. Ghavamzadeh. Deep hierarchy in bandits. In ICML, pages 8833-8851, 2022.   \n[17] J. Hong, B. Kveton, M. Zaheer, and M. Ghavamzadeh. Hierarchical bayesian bandits. In AISTATS, pages 7724-7741, 2022.   \n[18] R. A. Horn and C. R. Johnson. Matrix Analysis. Cambridge University Press, 2012.   \n[19] J. Hu, X. Chen, C. Jin, L. Li, and L. Wang. Near-optimal representation learning for linear bandits and linear RL. In ICML, pages 4349-4358, 2021.   \n[20] S. Kale, L. Reyzin, and R. E. Schapire. Non-stochastic bandit slate problems. In NeurIPS, pages 1054-1062, 2010.   \n[21]  C. Kalkanli and A. Ozgur. An improved regret bound for thompson sampling in the gaussian linear bandit seting. In IEEE International Symposium on Information Theory (ISIT), pages 2783-2788, 2020.   \n[22] E. Kaufmann, O. Cappe, and A. Garivier. On bayesian upper confidence bounds for bandit problems. In AISTATS, pages 592-600, 2012.   \n[23] M. Khodak, I. Osadchiy, K. Harris, M. Balcan, K. Y. Levy, R. Meir, and Z. S. Wu. Meta-learning adversarial bandit algorithms. In NeurIPS, 2023.   \n[24] R. D. Kleinberg and F. T. Leighton. The value of knowing a demand curve: Bounds on regret for online posted-price auctions. In Symposium on Foundations of Computer Science (FOCS), pages 594-605, 2003.   \n[25] B. Kveton, M. Konobeev, M. Zaheer, C. Hsu, M. Mladenoy, C. Boutilier, and C. Szepesvari. Meta-thompson sampling. In ICML, pages 5884-5893, 2021.   \n[26]  T. L. Lai and H. Robbins. Asymptotically efficient adaptive allocation rules. Advances in Applied Mathematics, 6:4\u2014-22, 1985.   \n[27]  T. Lattimore and C. Szepesvari. Bandit Algorithms. Cambridge University Press, 2020.   \n[28] L. Li, W. Chu, J. Langford, and R. E. Schapire. A contextual-bandit approach to personalized news article recommendation. In International Conference on World Wide Web (Www), pages 661-670, 2010.   \n[29]  Y. Li, Y. Wang, and Y. Zhou. Nearly minimax-optimal regret for linearly parameterized bandits. In Conference on Learning Theory (COLT), pages 2173-2174, 2019.   \n[30] P. Rusmevichientong and J. N. Tsitsiklis. Linearly parameterized bandits. Mathematics of Operations Research, 35(2):395-411, 2010.   \n[31]  D. Russo and B. V. Roy. Learning to optimize via posterior sampling. Mathematics of Operations Research, 39(4):1221-1243, 2014.   \n[32] D. Russo and B. V. Roy. An information-theoretic analysis of thompson sampling. Journal of Machine Learning Research (JMLR), 17:68:1-68:30, 2016.   \n[33]  W. R. Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25:285-294, 1933.   \n[34]  S. Thrun and L. Pratt. Learning to Learn. Kluwer Academic Publishers, 1998.   \n[35] M. J. Wainwright. High-Dimensional Statistics: A Non-Asymptotic Viewpoint. Cambridge University Press, 2019.   \n[36]  R. Wan, L. Ge, and R. Song. Metadata-based multi-task bandits with bayesian hierarchical models. In NeurIPS, pages 29655-29668, 2021.   \n[37] R. Wan, L. Ge, and R. Song. Towards scalable and robust structured bandits: A meta-learning framework. In AISTATS, pages 1144-1173, 2023.   \n[38] Z. Wen, B. Kveton, and A. Ashkan. Effcient learning in large-scale combinatorial semi-bandits. In ICML, pages 1113-1122, 2015.   \n[39]  Z. Wen and B. V. Roy. Efficient exploration and value function generalization in deterministic systems. In NeurIPS, pages 3021-3029, 2013. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "APPENDIX ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A Proofs for Regret Bound of HierTS in the Sequential Bandit Setting ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We first give the following proposition to bound the posterior variance $\\begin{array}{r}{\\mathbb{E}\\sum_{t\\geq1}\\sum_{s\\in S_{t}}\\|A_{s,t}\\|_{\\hat{\\Sigma}_{s,t}}^{2}}\\end{array}$ in the sequential setting. We chose to give the worst-ase upper boud on $\\begin{array}{r}{\\sum_{t\\geq1}\\sum_{s\\in S_{t}}\\|A_{s,t}\\|_{\\hat{\\Sigma}_{s,t}}^{2}}\\end{array}$ ", "page_idx": 12}, {"type": "text", "text": "Proposition A.1 Let $c_{1}=\\sigma^{2}+B^{2}\\lambda_{1}(\\Sigma_{0}),$ $c_{2}=\\sigma^{2}+B^{2}\\lambda_{1}(\\Sigma_{0})+B^{2}\\lambda_{1}(\\Sigma_{q})\\kappa(\\Sigma_{0}),$ then ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\sum_{t\\ge1}\\sum_{s\\in\\mathcal{S}_{t}}\\|A_{s,t}\\|_{\\hat{\\Sigma}_{s,t}}^{2}\\le2m d c_{1}\\log\\left(1+\\frac{n}{d}\\right)+2d c_{2}\\log\\left(1+\\frac{m\\operatorname{Tr}\\left(\\Sigma_{0}^{-1}\\Sigma_{q}\\right)}{d}\\right).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Proof. Note that $\\begin{array}{r l r}{\\Vert A_{s,t}\\Vert_{\\hat{\\Sigma}_{s,t}}^{2}}&{=}&{A_{s,t}^{\\top}\\big(\\tilde{\\Sigma}_{s,t}\\;+\\;\\tilde{\\Sigma}_{s,t}\\Sigma_{0}^{-1}\\bar{\\Sigma}_{t}\\Sigma_{0}^{-1}\\tilde{\\Sigma}_{s,t}\\big)A_{s,t};}\\end{array}$ then we bound $\\begin{array}{r}{\\sum_{t\\geq1}\\sum_{s\\in S_{t}}\\|A_{s,t}\\|_{\\tilde{\\Sigma}_{s,t}}^{2}}\\end{array}$ and $\\begin{array}{r}{\\sum_{t\\geq1}\\sum_{s\\in{\\cal S}_{t}}A_{s,t}^{\\top}\\tilde{\\Sigma}_{s,t}\\Sigma_{0}^{-1}\\bar{\\Sigma}_{t}\\Sigma_{0}^{-1}\\tilde{\\Sigma}_{s,t}A_{s,t}}\\end{array}$ respectively. ", "page_idx": 12}, {"type": "text", "text": "(1) Bounding $\\begin{array}{r}{\\sum_{t\\geq1}\\sum_{s\\in S_{t}}\\|A_{s,t}\\|_{\\tilde{\\Sigma}_{s,t}}^{2}}\\end{array}$ . Note that $\\tilde{\\Sigma}_{s,t}\\,=\\,(\\Sigma_{0}^{-1}+G_{s,t})^{-1}\\,\\le\\,\\Sigma_{0}$ , then we have $A_{s,t}^{\\top}\\tilde{\\Sigma}_{s,t}A_{s,t}\\leq B^{2}\\lambda_{1}(\\Sigma_{0})<B^{2}\\lambda_{1}(\\Sigma_{0})+\\sigma^{2}$ Accordingy wedefinea ew marix $\\tilde{X}_{s,t}\\triangleq(\\Sigma_{0}^{-1}+$ $\\begin{array}{r}{\\frac{1}{B^{2}\\lambda_{1}(\\Sigma_{0})+\\sigma^{2}}\\sum_{\\ell<t}\\mathbf{1}\\{s\\in S_{t}\\}A_{s,\\ell}A_{s,\\ell}^{\\top})^{-1}}\\end{array}$ , and notice that $\\begin{array}{r}{\\tilde{X}_{s,t}\\geq\\tilde{\\Sigma}_{s,t}=(\\Sigma_{0}^{-1}+\\frac{1}{\\sigma^{2}}\\sum_{\\ell<t}{\\bf1}\\{s\\in}\\end{array}$ $S_{t}\\}A_{s,\\ell}A_{s,\\ell}^{\\top})^{-1}$ , and that $\\tilde{X}_{s,1}=\\Sigma_{0}$ . Then recall $c_{1}=\\sigma^{2}+B^{2}\\lambda_{1}(\\Sigma_{0})$ , we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\nu_{1}\\displaystyle\\sum_{i=1}^{\\infty}\\nu_{i}\\displaystyle\\sum_{s\\in\\mathcal{S}_{s}}^{(1)}\\nu_{i}(\\nu_{1},\\nu_{2})}\\\\ &{~~~-\\nu_{2}\\displaystyle\\sum_{s\\in\\mathcal{S}_{s}}\\nu_{i}(1+\\nu_{3})(\\nu_{1}+\\nu_{4})}\\\\ &{~~~-\\nu_{3}\\displaystyle\\sum_{s\\in\\mathcal{S}_{s}}\\nu_{i}(1+\\nu_{4})(\\nu_{2}+\\nu_{3})(\\nu_{1}+\\nu_{4})}\\\\ &{~~~-\\nu_{4}\\displaystyle\\sum_{s\\in\\mathcal{S}_{s}}\\nu_{i}(1+\\nu_{4})(\\nu_{1}+\\nu_{4})}\\\\ &{~~~-\\nu_{5}\\displaystyle\\sum_{i=1}^{\\infty}\\nu_{1}(1+\\nu_{3})(\\nu_{1}+\\nu_{4})(\\nu_{2}+\\nu_{4})}\\\\ &{~~~-\\nu_{6}\\displaystyle\\sum_{s\\in\\mathcal{S}_{s}}\\nu_{i}(1+\\nu_{3})(\\nu_{1}+\\nu_{4})}\\\\ &{~~~-\\nu_{7}\\displaystyle\\sum_{i=1}^{\\infty}\\nu_{i}(1+\\nu_{8})(\\nu_{1}+\\nu_{9})(\\nu_{1}+\\nu_{10})}\\\\ &{~~~~-\\nu_{10}\\displaystyle\\sum_{s\\in\\mathcal{S}_{s}}\\nu_{i}(1+\\nu_{1})(\\nu_{1}+\\nu_{11})(\\nu_{2}+\\nu_{12})}\\\\ &{~~~~-\\nu_{11}\\displaystyle\\sum_{s\\in\\mathcal{S}_{s}}\\nu_{i}(1+\\nu_{11})(\\nu_{2}+\\nu_{2})(\\nu_{2}+\\nu_{2})}\\\\ &{~~~~-\\nu_{12}\\displaystyle\\sum_{s\\in\\mathcal{S}_{s}}\\nu_{i}(1+\\nu_{11})(\\nu_{1}+\\nu_{2})(\\nu_{2}+\\nu_{2})}\\\\ &{~~~~-\\nu_{2}\\displaystyle\\sum_{s\\in\\mathcal{S}_{s}}\\nu_{i}(1+\\nu_{2})(\\nu_{1}+\\nu_{2})}\\\\ &{~~~~-\\nu_{3}\\displaystyle\\sum_{s\\in\\mathcal{S}_{s\n$$", "text_format": "latex", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle=2d c_{1}\\sum_{s=1}^{m}\\log\\big(1+\\frac{\\sum_{t\\le m n}\\mathbf{1}\\{s\\in S_{t}\\}A_{s,t}^{\\top}\\Sigma_{0}A_{s,t}}{d\\big(\\sigma^{2}+B^{2}\\lambda_{1}(\\Sigma_{0})\\big)}\\big)}\\\\ &{\\displaystyle\\le2m d c_{1}\\log\\big(1+\\frac{n}{d}\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where the first inequality holds because the basic inequality $x\\leq2\\log{(1+x)},\\forall x\\in[0,1]$ the third inequalitlde-valq $\\begin{array}{r}{(\\prod_{i=1}^{d}\\lambda_{i})^{\\frac{1}{d}}\\leq\\frac{\\sum_{i=1}^{d}\\lambda_{i}}{d}.}\\end{array}$ ,for any $\\lambda_{i}\\geq0$ the last inequality holds due to the fact that the agent interacts with each task $\\boldsymbol{s}\\in[m]$ at most $n$ times. ", "page_idx": 13}, {"type": "text", "text": "Before bounding the remaining $\\begin{array}{r}{\\sum_{t\\geq1}\\sum_{s\\in S_{t}}A_{s,t}^{\\top}\\tilde{\\Sigma}_{s,t}\\Sigma_{0}^{-1}\\bar{\\Sigma}_{t}\\Sigma_{0}^{-1}\\tilde{\\Sigma}_{s,t}A_{s,t}.}\\end{array}$ we introduce the following lemma. ", "page_idx": 13}, {"type": "text", "text": "Lemma .1 If the guare atrices $A>0,B\\ge0,$ then $\\begin{array}{r}{\\lambda_{1}\\big[\\big((I+A B)(I+B A)\\big)^{-1}\\big]\\leq\\frac{\\lambda_{1}(A)}{\\lambda_{d}(A)}.}\\end{array}$ ", "page_idx": 13}, {"type": "text", "text": "Proof. According to Theorem 7.6.1 in Page 485 of [18], there exists a non-singular matrix $S$ , such that $A=S S^{\\top}$ and $\\check{B}=S^{-\\top}\\Lambda S^{-1}$ , in which $\\Lambda\\geq0$ is a diagonal matrix. Then we have $A B=S\\Lambda S^{-1}$ \uff0c $B A=S^{-\\top}\\Lambda S^{\\top}$ . Therefore, applying Weyl's inequality we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\lambda_{d}\\big((I+A B)(I+B A)\\big)}\\\\ &{=\\lambda_{d}\\big((I+S\\Lambda S^{-1})(I+S^{-\\top}\\Lambda S^{\\top})\\big)}\\\\ &{=\\lambda_{d}\\big(S(I+\\Lambda)S^{-1}S^{-\\top}(I+\\Lambda)S^{\\top}\\big)}\\\\ &{=\\lambda_{d}\\big(S^{\\top}S(I+\\Lambda)S^{-1}S^{-\\top}(I+\\Lambda)\\big)}\\\\ &{\\geq\\lambda_{d}\\big(S^{\\top}S\\big)\\lambda_{d}\\big((I+\\Lambda)S^{-1}S^{-\\top}(I+\\Lambda)\\big)}\\\\ &{\\geq\\lambda_{d}\\big(S^{\\top}S\\big)\\lambda_{d}\\big(S^{-1}S^{-\\top}\\big)\\lambda_{d}\\big((I+\\Lambda)^{2}\\big)}\\\\ &{\\geq\\lambda_{d}\\big(S^{\\top}S\\big)/\\lambda_{1}\\big(S^{1}S^{\\top}\\big)=\\lambda_{d}(A)/\\lambda_{1}(A).\\quad\\sqcup}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Remark A.1 (Smaller multiplicative factor than the latest one in $I I7J)$ Theimprovementliesin our sharper upper bound on $\\bar{\\lambda}_{1}\\big(\\Sigma_{0}^{-1}\\tilde{\\Sigma_{s,t}}\\tilde{\\Sigma_{s,t}}\\Sigma_{0}^{-1}\\big),$ and detailed explanations are two-fold: $^{(I)}$ Previouswork $I I7_{:}$ Appendix B] directly usedWeyl's inequality to upper bound ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\lambda_{1}(\\Sigma_{0}^{-1}\\tilde{\\Sigma}_{s,t}\\tilde{\\Sigma}_{s,t}\\Sigma_{0}^{-1})\\leq\\lambda_{1}^{2}(\\Sigma_{0}^{-1})\\lambda_{1}^{2}(\\tilde{\\Sigma}_{s,t})\\leq\\lambda_{1}^{2}(\\Sigma_{0}^{-1})\\lambda_{1}^{2}(\\Sigma_{0})=\\kappa^{2}(\\Sigma_{0}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "(2) Instead of directly using Weyl's inequality, we first propose Lemma $A.l$ whichusespositive semi-definitematrix diagonalizationtechniquetobound ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\lambda_{1}{\\big[}{\\big(}(I+A B)(I+B A){\\big)}^{-1}{\\big]}\\leq{\\frac{\\lambda_{1}(A)}{\\lambda_{d}(A)}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Then we apply Lemma $A.I$ to upper bound ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\lambda_{1}(\\Sigma_{0}^{-1}\\tilde{\\Sigma}_{s,t}\\tilde{\\Sigma}_{s,t}\\Sigma_{0}^{-1})=\\lambda_{1}(\\Sigma_{0}^{-1}\\tilde{\\Sigma}_{s,t}\\tilde{\\Sigma}_{s,t}\\Sigma_{0}^{-1})\\leq\\lambda_{1}\\big[\\big((I+\\Sigma_{0}\\tilde{\\Sigma}_{s,t})(I+\\tilde{\\Sigma}_{s,t}\\Sigma_{0})\\big)^{-1}\\big]\\leq\\kappa(\\Sigma_{0}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "resulting in a smaller multiplicative factor than that in $I I7J$ ", "page_idx": 13}, {"type": "text", "text": "(2) Bounding $\\begin{array}{r}{\\sum_{t\\geq1}\\sum_{s\\in S_{t}}A_{s,t}^{\\top}\\tilde{\\Sigma}_{s,t}\\Sigma_{0}^{-1}\\bar{\\Sigma}_{t}\\Sigma_{0}^{-1}\\tilde{\\Sigma}_{s,t}A_{s,t}.}\\end{array}$ First recall that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{\\mu}_{t}=\\bar{\\Sigma}_{t}\\bigl(\\Sigma_{q}^{-1}\\mu_{q}+\\displaystyle\\sum_{s\\in[m]}B_{s,t}-G_{s,t}(\\Sigma_{0}^{-1}+G_{s,t})^{-1}B_{s,t}\\bigr)=\\bar{\\Sigma}_{t}\\bigl(\\Sigma_{q}^{-1}\\mu_{q}+\\displaystyle\\sum_{s\\in[m]}(\\Sigma_{0}+G_{s,t}^{-1})^{-1}G_{s,t}^{-1}\\bigr)}\\\\ &{\\bar{\\Sigma}_{t}^{-1}=\\Sigma_{q}^{-1}+\\displaystyle\\sum_{s\\in[m]}G_{s,t}-G_{s,t}(\\Sigma_{0}^{-1}+G_{s,t})^{-1}G_{s,t}=\\Sigma_{q}^{-1}+\\displaystyle\\sum_{s\\in[m]}(\\Sigma_{0}+G_{s,t}^{-1})^{-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Therefore $\\bar{\\Sigma}_{t}\\leq\\Sigma_{q}$ . Then applying Lemma A.1 and Weyl's inequality, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{4_{s,t}^{\\top}\\tilde{\\Sigma}_{s,t}\\Sigma_{0}^{-1}\\overline{{\\Sigma}}_{t}\\Sigma_{0}^{-1}\\tilde{\\Sigma}_{s,t}A_{s,t}\\leq B^{2}\\lambda_{1}(\\tilde{\\Sigma}_{s,t}\\Sigma_{0}^{-1}\\bar{\\Sigma}_{t}\\Sigma_{0}^{-1}\\tilde{\\Sigma}_{s,t})\\leq B^{2}\\lambda_{1}(\\Sigma_{0}^{-1}\\tilde{\\Sigma}_{s,t}\\tilde{\\Sigma}_{s,t}\\Sigma_{0}^{-1})\\lambda_{1}(\\bar{\\Sigma}_{t})}\\\\ &{=B^{2}\\lambda_{1}\\left[\\big((I+\\Sigma_{0}\\tilde{\\Sigma}_{s,t})(I+\\tilde{\\Sigma}_{s,t}\\Sigma_{0})\\big)^{-1}\\right]\\lambda_{1}(\\bar{\\Sigma}_{t})\\leq B^{2}\\frac{\\lambda_{1}\\left(\\Sigma_{q}\\right)\\lambda_{1}\\left(\\Sigma_{0}\\right)}{\\lambda_{d}\\left(\\Sigma_{0}\\right)}\\leq B^{2}\\frac{\\lambda_{1}\\left(\\Sigma_{q}\\right)\\lambda_{1}\\left(\\Sigma_{0}\\right)}{\\lambda_{d}\\left(\\Sigma_{0}\\right)}+B^{2}\\lambda_{1}\\big(\\Sigma_{0}\\big)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Meanwhile, we estimate the gap between matrix \u2265t+1 and matrix -1 as follow ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\dot{\\Sigma}_{t+1}^{-1}-\\dot{\\Sigma}_{t}^{-1}=(\\Sigma_{0}+(\\dot{U}_{s}\\iota+\\sigma^{-2}A_{s,t}A_{s,t}^{-1})^{-1})^{-1}-(\\Sigma_{0}+\\mathcal{Q}_{s,t}^{-1})^{-1}}\\\\ &{=\\Sigma_{0}^{-1}-\\Sigma_{0}^{-1}\\big[\\check{\\Sigma}_{s,t}^{-1}+\\sigma^{-2}A_{s,t}A_{s,t}^{\\top}\\big]^{-1}\\Sigma_{0}^{-1}-(\\Sigma_{0}^{-1}-\\Sigma_{0}^{-1})^{-1}\\Sigma_{s,t}\\Sigma_{0}^{-1}\\big)}\\\\ &{=\\Sigma_{0}^{-1}\\big[\\check{\\Sigma}_{s,t}-(\\dot{\\Sigma}_{s,t}^{-1}+\\sigma^{-2}A_{s,t}A_{s,t}^{\\top})^{-1}\\big]\\Sigma_{0}^{-1}}\\\\ &{=\\Sigma_{0}^{-1}\\dot{\\Sigma}_{s,t}^{\\frac{1}{2}}\\big[I-(I+\\sigma^{-2}\\dot{\\Sigma}_{s,t}^{\\frac{1}{2}}A_{s,t}A_{s,t}^{\\top}\\Sigma_{s,t}^{\\frac{1}{2}})^{-1}\\big]\\tilde{\\Sigma}_{s,t}^{\\frac{1}{2}}\\Sigma_{0}^{-1}}\\\\ &{=\\Sigma_{0}^{-1}\\dot{\\Sigma}_{s,t}^{\\frac{1}{2}}\\big[I-(I-\\sigma^{-2}\\frac{\\dot{\\Sigma}_{s,t}^{\\frac{1}{2}}A_{s,t}A_{s,t}^{\\top}\\Sigma_{s,t}^{\\frac{1}{2}}}{1+\\sigma^{-2}A_{s,t}^{\\top}\\Sigma_{s,t}^{\\frac{1}{2}}A_{s,t}})\\big]\\dot{\\Sigma}_{s,t}^{\\frac{1}{2}}\\Sigma_{0}^{-1}}\\\\ &{=\\frac{\\Sigma_{0}^{-1}\\dot{\\Sigma}_{s,t}A_{s,t}A_{s,t}^{\\top}\\Sigma_{s,t}^{-1}}{\\sigma^{2}+\\Delta_{s,t}^{\\top}\\Sigma_{s,t}A_{s,t}}}\\\\ &{\\geq\\frac{\\sum_{0} \n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the second equality holds due to the Woodbury matrix identity, and the fifth equality holds due to the Sherman-Morrison formula. Then analogous to the proof for (1) Bounding $\\begin{array}{r}{\\sum_{t\\geq1}\\sum_{s\\in S_{t}}\\|A_{s,t}\\|_{\\tilde{\\Sigma}_{s,t}}^{2}}\\end{array}$ ,recall $c_{2}=\\left[\\sigma^{2}+B^{2}\\lambda_{1}(\\Sigma_{0})+B^{2}\\bar{\\lambda_{1}}(\\Sigma_{q})\\kappa(\\Sigma_{0})\\right]$ wehave ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\sum_{t\\geq1}\\sum_{u\\in\\{1,2,u\\}}A_{t,\\alpha}^{t,\\alpha}\\mathbb{E}_{u,v}^{-1}\\mathbb{E}_{u,v}^{-1}\\mathbb{E}_{u,v}A_{u,v}}\\\\ &{\\leq}\\\\ &{\\displaystyle\\sum_{t\\geq2}\\sum_{t\\geq1}^{t}w_{t}\\{1+\\frac{u_{t,x}^{2}\\zeta_{t,\\alpha}^{2}-1}{\\alpha^{2}+D^{2}A_{1}(\\zeta_{t,x}^{-1})+D^{2}A_{2}(\\zeta_{t,x}^{-1})+\\zeta_{t,x}^{-1}\\}}}\\\\ &{\\therefore\\alpha_{2}\\displaystyle\\sum_{t\\geq1}\\sum_{u\\in\\{1,2,u\\}}\\mathrm{i}_{\\zeta_{t,x}^{t,\\alpha}}\\{(t+\\frac{u_{t,x}^{2}\\zeta_{t-1}^{\\alpha}\\zeta_{t,\\alpha}^{2}+\\zeta_{t,x}^{-1}\\zeta_{t,x}^{\\alpha}+\\zeta_{t,x}^{-1})}{4\\zeta_{t,x}^{2}+D^{2}A_{1}(\\zeta_{t,x}^{-1})+\\zeta_{t,x}^{-1}\\}}\\\\ &{\\therefore\\alpha_{2}\\displaystyle\\sum_{t\\geq1}\\sum_{u\\in\\{1,2,u\\}}\\mathrm{i}_{\\zeta_{t,x}^{t,\\alpha}}\\{(t+\\frac{u_{t,x}^{2}\\zeta_{t-1}^{\\alpha}\\zeta_{t,x}^{-1}+\\zeta_{t,x}^{-1})\\zeta_{t,x}^{\\alpha}+\\zeta_{t,x}^{-1}\\zeta_{t,x}^{-1}\\}}\\\\ &{\\qquad+2\\alpha_{2}\\displaystyle\\sum_{t\\geq1}\\sum_{u\\in\\{1,2,u\\}}\\Big[\\mathrm{i}_{\\zeta_{t,x}^{t,\\alpha}}\\Big(\\zeta_{t,x}^{-1}+\\frac{\\zeta_{t,x}^{-1}\\zeta_{t,x}^{\\alpha}+\\zeta_{t,x}^{-1}\\zeta_{t,x}^{\\alpha}}{4\\zeta_{t,x}^{\\alpha}+D^{2}A_{1}(\\zeta_{t,x}^{-1})+\\zeta_{t,x}^{\\alpha}}\\Big)-\\mathrm{hegat}\\left(\\zeta_{t,x}^{-1}\\right)}\\\\ &{\\leq}\\\\ &{\\displaystyle\\mathcal{L}_{22}\\sum_{t\\geq1} \n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the second inequality holds due to Eq. (5). Combining (1) and (2) finishes the whole proof. ", "page_idx": 14}, {"type": "text", "text": "Remark A.2 Actually, we can replace the term $\\mathrm{Tr}(\\Sigma_{0}^{-1}\\Sigma_{q})$ in the above regret bound with $O(n\\lambda_{1}(\\Sigma_{q}))$ $(\\Sigma_{0}+G_{s,m n+1}^{-1})^{-1}\\leq$ $G_{s,m n+1}$ inthelast but one step in theabove 2), iseado bouding $(\\Sigma_{0}+G_{s,m n+1}^{-1})^{-1}\\leq\\Sigma_{0}^{-1}$ Specifically, we have the following estimation: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\log\\Big(\\frac{\\mathrm{Tr}(I+\\sum_{s\\in[m]}\\Sigma_{q}^{\\frac{1}{2}}\\big(\\Sigma_{0}+G_{s,m n+1}^{-1}\\big)^{-1}\\Sigma_{q}^{\\frac{1}{2}}\\big)}{d}\\Big)}\\\\ &{\\leq\\log\\Big(\\frac{\\mathrm{Tr}(I+\\sum_{s\\in[m]}\\Sigma_{q}^{\\frac{1}{2}}G_{s,m n+1}\\Sigma_{q}^{\\frac{1}{2}})}{d}\\Big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\log\\Big(1+\\frac{\\operatorname{Tr}(\\sigma^{-2}\\sum_{s\\in[m]}\\sum_{\\ell<m n+1}{\\mathbf{1}}[s\\in S_{\\ell}]A_{s,\\ell}^{\\top}\\Sigma_{q}A_{s,\\ell})}{d}\\Big)}\\\\ &{\\leq\\log\\Big(1+\\frac{\\sigma^{-2}m n\\lambda_{1}\\left(\\Sigma_{q}\\right)}{d}\\Big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which is $O(\\log{(m n)})$ , slightly larger than the regret bound of $O(\\log{(m)})$ in our Proposition A.1. ", "page_idx": 15}, {"type": "text", "text": "Then we can begin proving our first Bayes regret bound for HierTS in the multi-task Gaussian linear bandit setting. We first give a lemma as follow, which is useful to prove our multi-task Bayes regret bound in the sequential setting. ", "page_idx": 15}, {"type": "text", "text": "Lemma A.2 (Proposition 2 in $I2I J,$ Let $X_{1}$ and $X_{2}$ be arbitrary i.i.d. $\\mathbb{R}^{m}$ valued random variables and $f_{1},f_{2}$ measurable maps such that $f_{1},f_{2}:\\mathbb{R}^{m}\\,\\to\\,\\mathbb{R}^{d}$ with $\\mathbb{E}\\|f_{1}(X_{1})\\|_{2}^{2},\\,\\mathbb{E}\\|f_{2}(X_{1})\\|_{2}^{2}\\,<\\infty,$ then $|\\mathbb{E}[f_{1}(X_{1})^{\\top}f_{2}(X_{1})]|\\leq\\sqrt{d\\mathbb{E}[(f_{1}(X_{1})^{\\top}f_{2}(X_{2}))^{2}]}$ ", "page_idx": 15}, {"type": "text", "text": "Theorem A.1 (Theorem 5.1 in the main text). Let $|S_{t}|\\;=\\;1$ forallrounds $t\\,\\geq\\,1$ Theninthe multi-task Gaussian linear bandit setting,the Bayes regret upper bound of HierTS is as follow: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{B}\\mathcal{R}(m,n)\\leq\\sqrt{m n d}\\sqrt{2m d c_{1}\\log{(1+\\frac{n}{d})}+2d c_{2}\\log{(1+\\frac{m\\,\\mathrm{Tr}\\bigl(\\Sigma_{q}\\Sigma_{0}^{-1}\\bigr)}{d})}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. Recall that $H_{t}=(H_{s,\\ell})_{\\ell<t,s\\in S_{\\ell}}$ is the history up to round $t$ , then ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{B R(m,n)=\\mathbb{E}\\bigg[\\sum_{i\\geq1}^{n}\\mathbb{E}\\big[\\theta_{n,+}^{\\top}A_{s,-}-\\theta_{n,+}^{\\top}A_{s,+}|H_{i}\\big]\\bigg]}\\\\ &{=\\mathbb{E}\\bigg[\\sum_{i\\geq1}^{n}\\mathbb{E}\\big[\\theta_{n,+}^{\\top}A_{s,-}-\\mathbb{E}\\big[\\theta_{n,+}|H_{i}|^{\\top}\\mathbb{E}\\big[A_{s,+}|H_{i}|\\big]H_{i}\\big]\\bigg]}\\\\ &{=\\mathbb{E}\\bigg[\\sum_{i\\geq1}^{n}c_{\\delta,i}^{\\top}}\\\\ &{\\leq\\mathbb{E}\\bigg[\\sum_{i\\geq1}^{n}c_{\\delta,i}^{\\top}}\\\\ &{\\leq\\mathbb{E}\\bigg[\\sum_{i\\geq1}^{n}c_{\\delta,i}^{\\top}}\\end{array}\\bigg]^{T}A_{s,i}\\bigg]^{T}\\bigg]}\\\\ &{\\leq\\mathbb{E}\\bigg[\\sum_{i\\geq1}^{n}c_{\\delta,i}^{\\top}}\\\\ &{=\\mathbb{E}\\bigg[\\sum_{i\\geq1}^{n}c_{\\delta,i}^{\\top}}\\\\ &{=\\mathbb{E}\\bigg[\\sum_{i\\geq1}^{n}c_{\\delta,i}^{\\top}}\\\\ &{=\\mathbb{E}\\bigg[\\sum_{i\\geq1}^{n}\\sum_{s\\geq1}^{n}\\mathbb{E}\\big[\\theta_{n,+}^{\\top}(b_{\\delta,i})[\\theta_{n,+}-\\hat{\\mu}_{n,+})^{\\top}A_{s,i}|H_{i}\\big]\\bigg]}\\\\ &{=\\mathbb{E}\\bigg[\\sum_{i\\geq1}^{n}c_{\\delta,i}^{\\top}}\\\\ &{\\leq\\mathrm{Vorde}\\bigg[\\sum_{i\\geq1}^{n}\\sum_{s\\geq1}^{n}[A_{s,i}]\\bigg]_{\\sum_{i,i,j}}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where both the second and the fifth equality hold due to the independence between $A_{s,t}$ and $\\theta_{s,*}$ conditioned on $H_{t}$ ; the first inequality holds by applying Lemma A.2 with functions $f_{1}(y_{1},y_{2})=y_{1}$ \uff0c $f_{2}(y_{1},y_{2})=y_{2}$ for any $y_{1},y_{2}\\in\\mathbb{R}^{d}$ , and the random variable $X_{1}=(\\theta_{s,*}-\\hat{\\mu}_{s,t},A_{s,*})|H_{t},X_{2}$ (with the second element as $A_{s,t}$ ) is the i.i.d. copy of $X_{1}$ ; the second inequality holds due to Jensen's inequalty Plugig te upper bound over $\\begin{array}{r}{\\mathbb{E}\\sum_{t\\geq1}\\sum_{s\\in S_{t}}\\|A_{s,t}\\|_{\\hat{\\Sigma}_{s,t}}^{2}}\\end{array}$ in Proposition A1 into the above result obtains the Bayes regret bound for HierTS. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "B  Proofs for Regret Bound of HierBayesUCB in the Sequential Bandit Setting ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Lemma B.1 Let $\\theta\\ |\\ \\mu\\sim{\\mathcal N}(\\mu,\\Sigma_{0})$ and $H\\,=\\,(x_{t},Y_{t})_{t=1}^{n}$ be n observations generated as $Y_{t}$ $\\theta,x_{t}\\sim\\mathcal{N}(x_{t}^{\\top}\\theta,\\sigma^{2})$ . Let $\\mathbb{P}(\\mu\\mid H)=\\mathcal N(\\mu;\\bar{\\mu},\\bar{\\Sigma})$ and $\\begin{array}{r}{G=\\sigma^{-2}\\sum_{t=1}^{n}x_{t}x_{t}^{\\top}}\\end{array}$ Then ", "page_idx": 15}, {"type": "text", "text": "\u6b63 $\\sharp[\\theta\\ |\\ H]=(\\Sigma_{0}^{-1}+G)^{-1}(\\Sigma_{0}^{-1}\\bar{\\mu}+B),\\quad\\mathrm{cov}[\\theta\\ |\\ H]=(\\Sigma_{0}^{-1}+G)^{-1}+(\\Sigma_{0}^{-1}+G)^{-1}\\Sigma_{0}^{-1}\\bar{\\Sigma}\\Sigma_{0}^{-1}(\\Sigma_{0}^{-1}+G)^{-1}\\Sigma_{0}^{+1}$ 1 + G)-1.", "page_idx": 15}, {"type": "text", "text": "Proof.Bydefinition,we have $\\operatorname{cov}[\\theta\\mid\\mu,H]=(\\Sigma_{0}^{-1}+G)^{-1}$ \uff0c $\\mathbb{E}[\\theta\\mid\\mu,H]=\\operatorname{cov}[\\theta\\mid\\mu,H](\\Sigma_{0}^{-1}\\mu+$ $B$ where $\\begin{array}{r}{B=\\sigma^{-2}\\sum_{t=1}^{n}x_{t}Y_{t}}\\end{array}$ . Hence $\\operatorname{cov}[\\theta\\mid\\mu,H]$ does not depend on $\\mu$ . Then we have I $\\sharp[\\theta\\mid H]=\\mathbb{E}[\\mathbb{E}[\\theta\\mid\\mu,H]\\mid H]={\\mathrm{cov}}[\\theta\\mid\\mu,H](\\Sigma_{0}^{-1}\\mathbb{E}[\\mu\\mid H]+B)=(\\Sigma_{0}^{-1}+G)^{-1}(\\Sigma_{0}^{-1}\\bar{\\mu}+B).$ On the other hand, because $\\operatorname{cov}[\\theta\\mid\\mu,H]$ does not depend on $\\mu$ $,\\mathbb{E}[\\mathrm{cov}[\\theta\\mid\\mu,H]\\mid H]=\\mathrm{cov}[\\theta\\mid$ $\\mu,H]$ . In addition, since $B$ is a constant conditioned on $H$ , then according to [17, Lemma 2], we have the following result: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname{cov}[\\mathbb{E}[\\theta\\mid\\mu,H]\\mid H]=\\operatorname{cov}[\\operatorname{cov}[\\theta\\mid\\mu,H]\\Sigma_{0}^{-1}\\mu\\mid H]=(\\Sigma_{0}^{-1}+G)^{-1}\\Sigma_{0}^{-1}\\bar{\\Sigma}\\Sigma_{0}^{-1}(\\Sigma_{0}^{-1}+G)^{-1}\\mathrm{.}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Theorem B.1 (Theorem 5.2 in the main text). Suppose the action set $\\boldsymbol{\\mathcal{A}}$ isfinitewith $|{\\mathcal{A}}|<\\infty$ .Let $|S_{t}|=1$ forallrounds $t\\geq1$ .Then in the multi-task Gaussian linear bandit setting, the Bayes regret upper bound of Hierarchical BayesUCB is as follow: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{B\\mathcal{R}(m,n)\\leq\\!m n\\epsilon+4B\\sqrt{\\lambda_{1}(\\Sigma_{0}+\\Sigma_{q})}\\big(\\sqrt{d+\\sqrt{8d\\ln\\frac{1}{\\zeta}}}+\\|\\mu_{q}\\|_{\\hat{\\Sigma}_{s,1}^{-1}}\\big)m n|A|\\delta}\\\\ &{\\quad\\quad\\quad\\quad+\\mathbb{E}\\big[\\frac{8\\log\\frac{1}{\\delta}}{\\Delta_{\\operatorname*{min}}^{\\epsilon}}\\big]\\Big\\{2m d c_{1}\\log\\big(1+\\frac{n}{d}\\big)+2d c_{2}\\log\\big(1+\\frac{m\\operatorname{Tr}\\big(\\Sigma_{0}^{-1}\\Sigma_{q}\\big)}{d}\\big)\\Big\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "In Theorem 5.2 in the main text, we replace $\\sqrt{d+\\sqrt{8d\\ln\\frac{1}{\\zeta}}}$ in the right hand side of the above inequality with $\\sqrt{d}$ for ease of exposition. ", "page_idx": 16}, {"type": "text", "text": "Proof. Define $\\Delta_{s,t}\\,=\\,\\theta_{s,*}^{\\top}A_{s,*}\\,-\\,\\theta_{s,*}^{\\top}A_{s,t}$ the event $E_{s,t}\\;=\\;\\{\\forall a\\;\\in\\;{\\cal A}\\;:\\;|a^{\\top}(\\theta_{s,*}\\:-\\:\\hat{\\mu}_{s,t})|\\;\\leq$ $\\sqrt{2\\log\\frac{1}{\\delta}}\\|a\\|_{\\hat{\\Sigma}_{s,t}}\\}$ ,and $C_{t,s,a}=\\sqrt{2\\log\\frac{1}{\\delta}}\\Vert a\\Vert_{\\hat{\\Sigma}_{s,t}}$ . Then we can rewrite the multi-task Bayes regret $B\\mathcal{R}(m,n)$ as the following equivalent form: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{\\substack{>\\,1\\,s\\in\\mathcal{S}_{t}}}\\Delta_{s,t}\\Big]=\\!\\!\\sum_{t\\geq1}\\!\\sum_{s\\in\\mathcal{S}_{t}}\\!\\mathbb{E}\\big[\\Delta_{s,t}\\mathbf{1}\\{\\Delta_{s,t}\\geq\\epsilon,E_{s,t}\\}\\big]\\!+\\!\\!\\sum_{t\\geq1\\,s\\in\\mathcal{S}_{t}}\\!\\mathbb{E}\\big[\\Delta_{s,t}\\mathbf{1}\\{\\Delta_{s,t}<\\epsilon,E_{s,t}\\}\\big]\\!+\\!\\!\\sum_{t\\geq1\\,s\\in\\mathcal{S}_{t}}\\!\\mathbb{E}\\big[\\Delta_{s,t}\\mathbf{1}\\{\\Delta_{s,t}\\}\\big]\\!\\!+\\!\\!\\sum_{t\\geq1\\,s\\in\\mathcal{S}_{t}}\\!\\mathbb{E}\\big[\\Delta_{s,t}\\big]\\!+\\!\\!\\sum_{t\\geq1\\,s\\in\\mathcal{S}_{t}}\\!\\mathbb{E}\\big[\\Delta_{s,t}\\big]\\!+\\!\\!\\sum_{t\\geq1\\,s\\in\\mathcal{S}_{t}}\\!\\mathbb{E}\\big[\\Delta_{s,t}\\big]\\!+\\!\\!\\sum_{t\\geq1\\,s\\in\\mathcal{S}_{t}}\\!\\mathbb{E}\\big[\\Delta_{s,t}\\big]\\!+\\!\\!\\sum_{t\\geq1\\,s\\in\\mathcal{S}_{t}}\\!\\mathbb{E}\\big[\\Delta_{s,t}\\big]\\!+\\!\\!\\sum_{t\\geq1\\,s\\in\\mathcal{S}_{t}}\\!\\mathbb{E}\\big[\\Delta_{s,t}\\big]\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then we will bound the three terms in the RHS of the above equality respectively. ", "page_idx": 16}, {"type": "text", "text": "(1)Bounding $\\begin{array}{r l r}{\\sum_{t\\geq1}\\sum_{s\\in S_{t}}\\mathbb{E}\\big[\\Delta_{s,t}{\\bf1}\\big\\{\\Delta_{s,t}\\quad\\geq}&{{}\\epsilon,E_{s,t}\\big\\}\\big]}\\end{array}$ .\" Recall that $\\begin{array}{r l r}{U_{t,s,a}}&{{}=}&{a^{\\top}\\hat{\\mu}_{s,t}\\ +}\\end{array}$ $\\sqrt{2\\log\\frac{1}{\\delta}}\\|a\\|_{\\hat{\\Sigma}_{s,t}}$ , then we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\sum_{t\\geq1+\\delta/\\delta_{t}}\\mathbb{E}\\big[\\Delta_{t,\\tau}\\big(\\Delta_{t,\\tau}\\geq\\epsilon,E_{t,\\epsilon}\\big)\\big]}\\\\ &{=\\displaystyle\\sum_{t\\geq1+\\delta/\\delta_{t}}\\mathbb{E}\\big[\\big\\{\\frac{\\theta^{\\prime}}{\\Delta_{t}},~\\Delta_{t,\\tau}^{\\epsilon}-\\theta_{t,\\epsilon}^{\\prime}\\big\\}^{2}\\big\\{\\Delta_{t,\\epsilon}\\geq\\epsilon,E_{t,\\epsilon}\\big\\}\\big|H_{t}\\big]}\\\\ &{\\leq\\displaystyle\\sum_{t\\geq1+\\delta/\\delta_{t}}\\mathbb{E}\\big[\\big\\{\\frac{\\theta^{\\prime}}{\\Delta_{t}},~\\Delta_{t,\\tau}-U_{t,\\epsilon}\\big\\}_{\\Delta_{t,\\epsilon}}+U_{t,\\epsilon,\\epsilon}-U_{t,\\epsilon}^{*}\\big\\}\\big\\{\\Delta_{t,\\epsilon}\\geq\\epsilon,E_{t,\\epsilon}\\big\\}\\big|H_{t}\\big]}\\\\ &{\\leq\\displaystyle\\sum_{t\\geq1+\\delta/\\delta_{t}}\\mathbb{E}\\big[\\big\\{\\frac{\\theta^{\\prime}}{\\Delta_{t}},~\\Delta_{t,\\epsilon}-U_{t,\\epsilon}^{*}\\big\\}_{\\Delta_{t,\\epsilon}}\\big]^{2}\\big\\{\\Delta_{t,\\epsilon}\\geq\\epsilon,E_{t,\\epsilon}\\big\\}\\big|H_{t}\\big]}\\\\ &{\\leq\\displaystyle\\sum_{t\\geq1+\\delta/\\delta_{t}}\\mathbb{E}\\big[\\big\\{\\frac{U_{t,\\epsilon}^{\\prime}\\Delta_{t,\\epsilon}}{\\Delta_{t}},~\\Delta_{t,\\epsilon}\\big\\}^{2}\\big\\{\\Delta_{t,\\epsilon}\\geq\\epsilon,E_{t,\\epsilon}\\big\\}\\big|H_{t}\\big]}\\\\ &{\\leq\\displaystyle\\sum_{t\\geq1+\\delta/\\delta_{t}}\\mathbb{E}\\Bigg[\\frac{\\|\\hat{C}_{t,\\epsilon}^{2}\\|_{\\Delta_{t,\\epsilon}}\\|_{\\mathcal{H}}}{\\Delta_{t,\\epsilon}}}\\\\ &{=\\displaystyle\\sum_{t\\geq1+\\delta/\\delta_{t}}\\Big\\{\\frac{\\|\\hat{C}_{t,\\epsilon}^{2}\\|_{\\Delta_{t,\\epsilon}}\\|_{\\mathcal{H}}}{\\Delta_{t,\\epsilon}}\\Big\\}}\\\\ &{\\phantom{=}-\\displaystyle\\sum_{t\\geq1+\\delta/\\delta_{t}}\\Big\\{\\frac{\\|\\hat{C}_{t,\\epsilon}^{2}\\|_{\\Delta_{t,\\epsilon \n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the first inequality holds due to the fact that $U_{t,s,A_{s,t}}\\ge U_{t,s,A_{s,*}}$ in the BayesUCB algorithm; the second inequality holds because when event $E_{s,t}$ occurs, $\\theta_{s,*}^{\\top}A_{s,*}\\leq U_{t,s,A_{s,*}}$ the third inequality ", "page_idx": 16}, {"type": "text", "text": "holds due to the definition of $U_{t,s,a}$ ; and the last inequality due to the result in Proposition A.1. ", "page_idx": 17}, {"type": "text", "text": "(2)Bounding $\\begin{array}{r l r}{\\sum_{t\\geq1}\\sum_{s\\in{\\cal S}_{t}}\\mathbb{E}\\big[\\Delta_{s,t}{\\bf1}\\big\\{\\Delta_{s,t}\\mathrm{~\\boldmath~\\rho~}}}&{{}<}&{\\epsilon,E_{s,t}\\big\\}\\big]}\\end{array}$ We   trivially  have $\\begin{array}{r}{\\sum_{t\\geq1}\\sum_{s\\in{\\cal S}_{t}}\\mathbb{E}\\big[\\Delta_{s,t}{\\bf1}\\big\\{\\Delta_{s,t}<\\epsilon,E_{s,t}\\big\\}\\big]\\leq m n\\epsilon.}\\end{array}$ ", "page_idx": 17}, {"type": "text", "text": "(3) Bounding $\\begin{array}{r}{\\sum_{t\\geq1}\\sum_{s\\in\\cal S_{t}}\\mathbb{E}\\big[\\Delta_{s,t}\\mathbf{1}\\{\\bar{E}_{s,t}\\}\\big],}\\end{array}$ ", "page_idx": 17}, {"type": "text", "text": "First we give an upper bound of $\\Delta_{s,t}=\\theta_{s,*}^{\\top}(A_{s,*}-A_{s,t})$ . Using Schwartz's inequality, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\theta_{s,*}^{\\top}(A_{s,*}-A_{s,t})\\leq\\Vert\\theta_{s,*}\\Vert_{\\hat{\\Sigma}_{s,1}^{-1}}\\Vert A_{s,*}-A_{s,t}\\Vert_{\\hat{\\Sigma}_{s,1}}}\\\\ &{{\\leq}2B\\sqrt{\\lambda_{1}(\\hat{\\Sigma}_{s,1})}\\Vert\\theta_{s,*}\\Vert_{\\hat{\\Sigma}_{s,1}^{-1}}\\leq2B\\sqrt{\\lambda_{1}(\\hat{\\Sigma}_{s,1})}\\Big(\\Vert\\theta_{s,*}-\\mu_{q}\\Vert_{\\hat{\\Sigma}_{s,1}^{-1}}+\\Vert\\mu_{q}\\Vert_{\\hat{\\Sigma}_{s,1}^{-1}}\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Besides, we also have $\\theta_{s,*}-\\mu_{q}=\\theta_{s,*}-\\mu_{*}+\\mu_{*}-\\mu_{q}\\sim\\mathcal{N}(0,\\Sigma_{0}+\\Sigma_{q})=\\mathcal{N}(0,\\hat{\\Sigma}_{s,1})$ , then $\\begin{array}{r}{\\mathbb{E}\\big[\\|\\theta_{s,*}-\\mu_{q}\\|_{\\hat{\\Sigma}_{s,1}^{-1}}\\big]\\leq\\sqrt{\\mathbb{E}\\|\\hat{\\Sigma}_{s,1}^{-\\frac{1}{2}}(\\theta_{s,*}-\\mu_{q})\\|_{2}^{2}}=\\sqrt{d}.}\\end{array}$ According to [35, Exp 2.11], we have with probability $1-\\zeta,\\,\\|\\theta_{s,*}-\\mu_{q}\\|_{\\hat{\\Sigma}_{s,1}^{-1}}\\leq\\sqrt{d+\\sqrt{8d\\ln\\frac{1}{\\zeta}}}$ . Therefore, with probability $1-\\zeta$ over the draw of $\\{\\theta_{s,*}\\}_{s\\in[m]}$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\sum_{t\\geq1}\\sum_{s\\in\\mathcal{S}_{t}}\\mathbb{E}\\big[\\Delta_{s,t}\\mathbf{1}\\{\\bar{E}_{s,t}\\}\\big]}\\\\ &{=\\displaystyle\\sum_{t\\geq1}\\sum_{s\\in\\mathcal{S}_{t}}\\mathbb{E}\\big[\\mathbb{E}\\big[\\Delta_{s,t}\\mathbf{1}\\{\\bar{E}_{s,t}\\}|H_{t}\\big]\\big]}\\\\ &{\\leq2B\\sqrt{\\lambda_{1}(\\Sigma_{0}+\\Sigma_{q})}\\big(\\sqrt{d+\\sqrt{8d\\ln\\frac{1}{\\zeta}}}+\\|\\mu_{q}\\|_{\\tilde{\\Sigma}_{s,1}^{-1}}\\big)\\displaystyle\\sum_{t\\geq1}\\sum_{s\\in\\mathcal{S}_{t}}\\mathbb{E}\\big[\\mathbf{1}\\{\\bar{E}_{s,t}\\}\\big|H_{t}\\big]}\\\\ &{=2B\\sqrt{\\lambda_{1}(\\Sigma_{0}+\\Sigma_{q})}\\big(\\sqrt{d+\\sqrt{8d\\ln\\frac{1}{\\zeta}}}+\\|\\mu_{q}\\|_{\\tilde{\\Sigma}_{s,1}^{-1}}\\big)\\displaystyle\\sum_{t\\geq1}\\sum_{s\\in\\mathcal{S}_{t}}\\mathbb{P}\\big(\\bar{E}_{s,t}|H_{t}\\big)}\\\\ &{\\leq4B\\sqrt{\\lambda_{1}(\\Sigma_{0}+\\Sigma_{q})}\\big(\\sqrt{d+\\sqrt{8d\\ln\\frac{1}{\\zeta}}}+\\|\\mu_{q}\\|_{\\tilde{\\Sigma}_{s,1}^{-1}}\\big)m n|A|\\delta,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the last inequality holds because $\\mathbb{P}\\big(\\bar{E}_{s,t}\\vert H_{t}\\big)\\leq2\\delta$ . Combining (1), (2) and (3), we achieve the final Bayes regret bound for any $\\delta\\in(0,1)$ \uff0c $\\epsilon>0$ $\\zeta\\in(0,1)$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\mathcal B}{\\mathcal R}(m,n)\\leq m n\\epsilon+4B\\sqrt{\\lambda_{1}(\\Sigma_{0}+\\Sigma_{q})}\\big(\\sqrt{d+\\sqrt{8d\\ln\\frac{1}{\\zeta}}}+\\|\\mu_{q}\\|_{\\hat{\\Sigma}_{s,1}^{-1}}\\big)m n|A|\\delta}}\\\\ {{\\displaystyle+\\,\\mathbb{E}\\big[\\frac{8\\log\\frac{1}{\\delta}}{\\Delta_{\\operatorname*{min}}^{\\epsilon}}\\big]\\Big\\{2m d c_{1}\\log\\big(1+\\frac{n}{d}\\big)+2d c_{2}\\log\\big(1+\\frac{m\\,\\mathrm{Tr}\\left(\\Sigma_{0}^{-1}\\Sigma_{q}\\right)}{d}\\big)\\Big\\}.\\qquad\\Omega}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "CProofs for Regret Bounds of HierTS and HierBayesUCB in the Concurrent Bandit Setting ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Let $\\mathcal{C}_{t}=\\{s\\in\\mathcal{S}_{t}:\\lambda_{d}(G_{s,t})\\geq\\beta/\\sigma^{2}\\}$ be the set of sufficiently-explored tasks at round $t$ .We first ih $\\begin{array}{r}{\\mathbb{E}\\sum_{t\\geq1}\\sum_{s\\in S_{t}}{\\mathbf{1}}\\{s\\in\\mathcal{C}_{t}\\}\\|A_{s,t}\\|_{\\hat{\\Sigma}_{s,t}}^{2}}\\end{array}$ in the concurrent setting. Analogous to the proof for Proposition A.1, we choose to give the worst-case upper bound on $\\begin{array}{r}{\\sum_{t\\geq1}\\!\\!\\!\\sum_{s\\in\\mathcal{S}_{t}}\\bar{\\mathbf{1}}\\{s\\in\\mathcal{C}_{t}\\}\\|\\dot{\\mathcal{A}}_{s,t}\\|_{\\hat{\\Sigma}_{s,t}}^{2}}\\end{array}$ as follow. ", "page_idx": 17}, {"type": "text", "text": "Proposition C.1 Let $c_{1}=\\sigma^{2}+B^{2}\\lambda_{1}(\\Sigma_{0}),$ $c_{2}=\\sigma^{2}+B^{2}\\lambda_{1}(\\Sigma_{0})+B^{2}\\lambda_{1}(\\Sigma_{q})\\kappa(\\Sigma_{0}),\\,c_{3}=1+$ $B^{2}\\sigma^{-2}\\kappa(\\Sigma_{0})\\left[\\lambda_{1}(\\Sigma_{0})+\\sigma^{2}/\\beta\\right]$ , then we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{t\\geq1}\\sum_{s\\in\\mathcal{S}_{t}}\\mathbf{1}\\{s\\in\\mathcal{C}_{t}\\}\\|A_{s,t}\\|_{\\tilde{\\Sigma}_{s,t}}^{2}\\leq2m d c_{1}\\log\\big(1+\\frac{n}{d}\\big)+2d c_{2}c_{3}\\log\\big(1+\\frac{m\\operatorname{Tr}\\big(\\Sigma_{0}^{-1}\\Sigma_{q}\\big)}{d}\\big).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. Note that we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{t\\geq1}\\sum_{s\\in\\mathcal{S}_{t}}\\mathbf{1}\\big\\{s\\in\\mathcal{C}_{t}\\big\\}\\|A_{s,t}\\|_{\\mathcal{\\hat{C}}_{s,t}}^{2}=\\sum_{t\\geq1}\\sum_{s\\in\\mathcal{S}_{t}}\\mathbf{1}\\big\\{s\\in\\mathcal{C}_{t}\\big\\}A_{s,t}^{\\top}\\big(\\tilde{\\Sigma}_{s,t}+\\tilde{\\Sigma}_{s,t}\\Sigma_{0}^{-1}\\bar{\\Sigma}_{t}\\Sigma_{0}^{-1}\\tilde{\\Sigma}_{s,t}\\big)A_{s,t}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Onevent $\\{s\\in{\\mathcal{C}}_{t}\\}$ , the modified HierTS samples from the posterior and actually behaves the same as the original HierTS algorithm in Algorithm 1. Then we bound the two terms in the right hand side of the above equality respectively . ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{Bounding}\\sum_{t\\geq1}\\sum_{s\\in S_{t}}\\mathbf{1}\\big\\{s\\in\\mathcal{C}_{t}\\big\\}A_{s,t}^{\\top}\\tilde{\\Sigma}_{s,t}A_{s,t}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Similar to the proof for Theorem A.1, we have $A_{s,t}^{\\top}\\tilde{\\Sigma}_{s,t}A_{s,t}\\ \\leq\\ B^{2}\\lambda_{1}(\\Sigma_{0})\\,+\\,\\sigma^{2}$ and $\\tilde{X}_{s,t}$ $\\begin{array}{r}{(\\Sigma_{0}^{-1}+\\frac{1}{B^{2}\\lambda_{1}(\\Sigma_{0})+\\sigma^{2}}\\sum_{\\ell<t}\\mathbf{1}\\{s\\in S_{t}\\}A_{s,\\ell}A_{s,\\ell}^{\\top})^{-1}\\geq\\tilde{\\Sigma}_{s,t}}\\end{array}$ . Then we can analogously obtain $\\sum_{t\\geq1}\\sum_{s\\in S_{t}}\\mathbf{1}\\{s\\in\\mathcal{C}_{t}\\}A_{s,t}^{\\top}\\tilde{\\Sigma}_{s,t}A_{s,t}$ $\\begin{array}{r l}&{\\quad_{-\\infty}\\longmapsto_{+}(x,\\zeta_{2})=^{-1},}\\\\ &{\\leq2\\left(\\sigma^{2}+B^{2}\\lambda_{1}(\\Sigma_{0})\\right)\\underset{\\geq1}{\\sum}\\sum_{k=1}^{1}[s\\in\\mathcal{C}_{\\mathcal{I}}]\\log(1+\\frac{A_{\\sigma}^{-1}\\sum_{k=1}^{k}A_{\\sigma,k}}{\\sigma^{2}+B^{2}\\lambda_{1}(\\Sigma_{0})})}\\\\ &{=2(\\sigma^{2}+B^{2}\\lambda_{1}(\\Sigma_{0}))\\underset{\\geq1}{\\sum}\\underset{k\\in\\mathcal{C}_{\\mathcal{I}}}{\\sum}[\\log\\operatorname*{det}\\big(\\hat{X}_{s,k}^{-1}+\\frac{A_{\\sigma,k}A_{\\sigma,k}^{*}}{\\sigma^{2}+B^{2}\\lambda_{1}(\\Sigma_{0})}\\big)-\\log\\operatorname*{det}\\hat{X}_{s,k}^{-1}}\\\\ &{\\leq2[\\sigma^{2}+B^{2}\\lambda_{1}(\\Sigma_{0})]\\underset{\\geq1}{\\sum}\\underset{k=1}{\\sum}[s\\in\\mathcal{C}_{\\mathcal{I}}]\\left[\\log\\operatorname*{det}\\big(\\hat{X}_{s,k}^{-1}+\\frac{A_{\\sigma,k}A_{\\sigma,k}^{*}}{\\sigma^{2}+B^{2}\\lambda_{1}(\\Sigma_{0})}\\big)-\\log\\operatorname*{det}\\hat{X}_{s,k}^{-1}\\right]}\\\\ &{\\leq2[\\sigma^{2}+B^{2}\\lambda_{1}(\\Sigma_{0})]\\underset{\\geq1}{\\sum}\\underset{k=1}{\\sum}[s\\in\\mathcal{S}_{\\mathcal{I}}]\\left[\\log\\operatorname*{det}\\big(\\hat{X}_{s,k}^{-1}+\\frac{A_{\\sigma,k}A_{\\sigma,k}^{*}}{\\sigma^{2}+B^{2}\\lambda_{1}(\\Sigma_{0})}\\big)-\\log\\operatorname*{det}\\hat{X}_{s,k}^{-1}\\right]}\\\\ &{=2[\\sigma^{2}+B^{2}\\lambda_{1}(\\Sigma_{0})]\\sum_{k=1}^{\\infty}[\\log\\operatorname*{det}\\big(\\hat{X}_{s-m+1}^{-1}-\\log\\operatorname*{det}\\hat{X}_{s-1}^{-1}\\big)}\\\\ &{\\leq2[\\sigma^{2}+$ where the second inequality holds due to the fact that, if square matrix $A\\geq B\\geq0$ then $\\operatorname*{det}\\left(A\\right)\\geq$ det $(B)$ , and $|S_{t}|\\leq m$ ; the last inequality holds because the agent interact with each task at most $n$ times. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{Bounding}\\sum_{t\\geq1}\\sum_{s\\in\\mathcal{S}_{t}}\\mathbf{1}\\big\\{s\\in\\mathcal{C}_{t}\\big\\}A_{s,t}^{\\top}\\big(\\tilde{\\Sigma}_{s,t}\\Sigma_{0}^{-1}\\bar{\\Sigma}_{t}\\Sigma_{0}^{-1}\\tilde{\\Sigma}_{s,t}\\big)A_{s,t}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Analysis. The real difference of the proof for the concurrent regret from the sequential regret lies in bounding $\\begin{array}{r}{\\sum_{t\\geq1}\\sum_{s\\in{\\cal S}_{t}}{{\\bf1}\\{s\\in{\\mathcal C}_{t}\\}}A_{s,t}^{\\top}\\big(\\tilde{\\Sigma}_{s,t}\\Sigma_{0}^{-1}\\bar{\\Sigma}_{t}\\Sigma_{0}^{-1}\\tilde{\\Sigma}_{s,t}\\big)\\bar{A_{s,t}}}\\end{array}$ because $|\\mathcal{S}_{t}|\\geq1$ and the result in Eq. (5) (which only holds for the case $|S_{t}|=1\\,$ ) does not hold. To tackle this difference, we reduce the $\\boldsymbol{S}_{t}=\\{I_{t j}\\}_{j=1}^{|\\mathcal{S}_{t}|}$ and define $S_{t,1:i}=\\{I_{t j}\\}_{j=1}^{i}$ Then at round $t$ let $s=I_{t i}$ and define $\\begin{array}{r}{\\bar{\\Sigma}_{s,t}^{-1}\\triangleq\\Sigma_{q}^{-1}+\\sum_{z\\in S_{t,1:i-1}}(\\Sigma_{0}+G_{z,t+1}^{-1})^{-1}+\\sum_{z\\in[m]\\backslash S_{t,1:i-1}}(\\Sigma_{0}+G_{z,t+1}^{-1})^{-1}\\Bigg\\}.}\\end{array}$ $G_{z,t}^{-1})^{-1}$ , we estimate the gap between $\\bar{\\Sigma}_{s,t}^{-1}$ and $\\bar{\\Sigma}_{t}^{-1}$ as follow: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{\\Sigma}_{s,t}^{-1}-\\bar{\\Sigma}_{t}^{-1}=\\displaystyle\\sum_{z\\in S_{t,1:i-1}}^{+\\;\\;\\;\\star}\\left[(\\check{\\Sigma}_{0}^{\\circ,\\epsilon}+G_{z,t+1}^{-1})^{-1}-(\\Sigma_{0}+G_{z,t}^{-1})\\right]}\\\\ &{\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;=\\displaystyle\\sum_{z\\in S_{t,1:i-1}}^{+\\;\\;\\;\\;\\star}\\Big[(\\Sigma_{0}+(G_{z,t}+\\displaystyle\\frac{A_{z,t}A_{z,t}^{\\top}}{\\sigma^{2}})^{-1})^{-1}-(\\Sigma_{0}+G_{z,t}^{-1})\\Big]}\\\\ &{\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;=\\displaystyle\\sum_{z\\in S_{t,1:i-1}}^{+\\;\\;\\;\\;\\star}\\Sigma_{0}^{-1}\\tilde{\\Sigma}_{s,t}\\displaystyle\\frac{A_{z,t}A_{z,t}^{\\top}}{\\sigma^{2}+A_{z,t}^{\\top}\\tilde{\\Sigma}_{z,t}A_{z,t}}\\tilde{\\Sigma}_{s,t}\\Sigma_{0}^{-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Thus we can bound $\\lambda_{1}\\big(\\bar{\\Sigma}_{s,t}^{-1}-\\bar{\\Sigma}_{t}^{-1}\\big)$ and tackle $\\bar{\\Sigma}_{s,t}^{-1}$ instead of $\\bar{\\Sigma}_{t}^{-1}$ to reduce the concurrent setting to the sequential setting. We first give a useful lemma as follow. ", "page_idx": 18}, {"type": "text", "text": "Lemma C.1 For any fixed $t\\geq1$ and $i\\in[L]$ ,suppose $\\lambda_{d}(G_{s,t})\\geq\\beta/\\sigma^{2}$ and let $s=I_{t,i}$ .Then ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\lambda_{1}(\\bar{\\Sigma}_{s,t}^{-1}\\bar{\\Sigma}_{t})\\leq1+\\frac{B^{2}\\lambda_{1}(\\Sigma_{0})}{\\sigma^{2}\\lambda_{d}(\\Sigma_{0})}\\big[\\lambda_{1}(\\Sigma_{0})+\\frac{\\sigma^{2}}{\\beta}\\big].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. Applying Weyl's inequality, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\lambda_{1}\\bigl((\\bar{\\Sigma}_{s,t}^{-1}-\\bar{\\Sigma}_{t}^{-1})\\bar{\\Sigma}_{t}+I\\bigr)=\\lambda_{1}\\bigl(\\bar{\\Sigma}_{t}^{\\frac{1}{2}}(\\bar{\\Sigma}_{s,t}^{-1}-\\bar{\\Sigma}_{t}^{-1})\\bar{\\Sigma}_{t}^{\\frac{1}{2}}+I\\bigr)}\\\\ &{\\quad\\leq1+\\lambda_{1}\\bigl(\\bar{\\Sigma}_{s,t}^{-1}-\\bar{\\Sigma}_{t}^{-1}\\bigr)\\lambda_{1}(\\bar{\\Sigma}_{t})=1+\\frac{\\lambda_{1}\\bigl(\\bar{\\Sigma}_{s,t}^{-1}-\\bar{\\Sigma}_{t}^{-1}\\bigr)}{\\lambda_{d}\\bigl(\\bar{\\Sigma}_{t}^{-1}\\bigr)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We first lower bound $\\lambda_{d}(\\bar{\\Sigma}_{t}^{-1})$ . According to Weyl's inequality, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\lambda_{d}(\\bar{\\Sigma}_{t}^{-1})\\geq\\lambda_{d}(\\Sigma_{q}^{-1})+\\displaystyle\\sum_{z\\in[m]}\\lambda_{d}\\big((\\Sigma_{0}+G_{z,t}^{-1})^{-1}\\big)\\geq\\lambda_{d}(\\Sigma_{q}^{-1})+\\displaystyle\\sum_{z\\in[m]}\\frac{1}{\\lambda_{1}(\\Sigma_{0})+\\lambda_{1}(G_{z,t}^{-1})}}\\\\ &{\\qquad=\\lambda_{d}(\\Sigma_{q}^{-1})+\\displaystyle\\sum_{z\\in[m]}\\frac{1}{\\lambda_{1}(\\Sigma_{0})+\\frac{1}{\\lambda_{d}(G_{z,t})}}\\geq\\lambda_{d}(\\Sigma_{q}^{-1})+\\frac{i-1}{\\lambda_{1}(\\Sigma_{0})+\\sigma^{2}/\\beta},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the last inequality holds because the tasks $\\mathcal{S}_{t,1:i-1}$ have been sufficiently explored. On the other hand, using our Lemma A.1 we can bound $\\lambda_{1}(\\bar{\\Sigma}_{s,t}^{-1}-\\bar{\\Sigma}_{t}^{-1})$ as follow ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\lambda_{1}(\\bar{\\Sigma}_{s,t}^{-1}-\\bar{\\Sigma}_{t}^{-1})\\leq\\sum_{z\\in S_{t,1;i-1}}\\lambda_{1}(\\frac{A_{z,t}A_{z,t}^{\\top}}{\\sigma^{2}+A_{z,t}^{\\top}\\tilde{\\Sigma}_{z,t}A_{z,t}})\\lambda_{1}(\\Sigma_{0}^{-1}\\tilde{\\Sigma}_{s,t}\\tilde{\\Sigma}_{s,t}\\Sigma_{0}^{-1})\\leq(i-1)\\frac{B^{2}}{\\sigma^{2}}\\frac{\\lambda_{1}(\\Sigma_{0})}{\\lambda_{d}(\\Sigma_{0})}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Combining the above results, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\lambda_{1}(\\bar{\\Sigma}_{s,t}^{-1}\\bar{\\Sigma}_{t})\\leq1+\\frac{(i-1)\\frac{B^{2}}{\\sigma^{2}}\\frac{\\lambda_{1}(\\Sigma_{0})}{\\lambda_{d}(\\Sigma_{0})}}{\\lambda_{d}(\\Sigma_{q}^{-1})+\\frac{i-1}{\\lambda_{1}(\\Sigma_{0})+\\sigma^{2}/\\beta}}\\leq1+\\frac{B^{2}}{\\sigma^{2}}\\frac{\\lambda_{1}(\\Sigma_{0})}{\\lambda_{d}(\\Sigma_{0})}\\big[\\lambda_{1}(\\Sigma_{0})+\\frac{\\sigma^{2}}{\\beta}\\big].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then, recall $c_{3}~=~1\\,+\\,B^{2}\\sigma^{-2}\\kappa(\\Sigma_{0})\\left[\\lambda_{1}(\\Sigma_{0})\\,+\\,\\sigma^{2}/\\beta\\right]$ , we can bound $\\textstyle\\sum_{t\\geq1}\\sum_{s\\in S_{t}}\\mathbf{1}\\{s\\;\\;\\in\\;\\;$ $\\mathcal{C}_{t}\\}A_{s,t}^{\\top}\\big(\\tilde{\\Sigma}_{s,t}\\Sigma_{0}^{-1}\\bar{\\Sigma}_{t}\\Sigma_{0}^{-1}\\tilde{\\Sigma}_{s,t}\\big)A_{s,t}$ as follows: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\underset{t\\geq1}{\\sum}\\underset{s\\in\\mathcal{S}_{t}}{\\sum}1\\{s\\in\\mathcal{C}_{t}\\}\\lambda_{s,t}^{\\top}\\big(\\bar{\\Sigma}_{s,t}\\Sigma_{0}^{-1}\\bar{\\Sigma}_{t}\\Sigma_{0}^{-1}\\bar{\\Sigma}_{s,t}\\big)A_{s,t}}\\\\ &{=\\underset{t\\geq1}{\\sum}\\underset{s\\in\\mathcal{S}_{t}}{\\sum}1\\{s\\in\\mathcal{C}_{t}\\}\\lambda_{s,t}^{\\top}\\bar{\\Sigma}_{s,t}\\Sigma_{0}^{-1}\\bar{\\Sigma}_{s,t}^{\\frac{1}{2}}\\big(\\bar{\\Sigma}_{s,t}^{-\\frac{1}{2}}\\bar{\\Sigma}_{t}\\Sigma_{s,t}^{\\frac{1}{2}}\\big)\\bar{\\Sigma}_{s,t}^{\\frac{1}{2}}\\Sigma_{0}^{-1}\\bar{\\Sigma}_{s,t}A_{s,t}}\\\\ &{\\leq\\underset{t\\geq1}{\\sum}\\underset{s\\in\\mathcal{S}_{t}}{\\sum}1\\{s\\in\\mathcal{C}_{t}\\}\\lambda_{1}\\big(\\bar{\\Sigma}_{s,t}^{-\\frac{1}{2}}\\bar{\\Sigma}_{t}\\bar{\\Sigma}_{s,t}^{-\\frac{1}{2}}\\big)A_{s,t}^{\\top}\\bar{\\Sigma}_{s,t}\\Sigma_{0}^{-1}\\bar{\\Sigma}_{s,t}^{\\frac{1}{2}}\\bar{\\Sigma}_{s,t}^{\\frac{1}{2}}\\Sigma_{0}^{-1}\\bar{\\Sigma}_{s,t}A_{s,t}}\\\\ &{\\leq\\Big\\{1+\\frac{B^{2}}{\\rho^{2}}\\frac{\\lambda_{1}}{\\lambda_{d}}(\\Sigma_{0})\\big[\\lambda_{1}(\\Sigma_{0})+\\frac{\\rho^{2}}{\\beta}\\big]\\Big\\}\\underset{t\\geq1}{\\sum}\\underset{s\\in\\mathcal{S}_{t}}{\\sum}1\\{s\\in\\mathcal{C}_{t}\\}\\bar{A}_{s,t}^{\\top}\\bar{\\Sigma}_{s,t}\\Sigma_{0}^{-1}\\bar{\\Sigma}_{s,t}\\Sigma_{0}^{-1}\\bar{\\Sigma}_{s,t}A_{s,t}}\\\\ &{\\leq2c_{3}c_{2}\\big[\\log\\big(\\bar{\\Sigma}_{m,\\mathbf{n+1}}^{-\\mathbf{1}}\\big)-\\log\\operatorname\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the second inequality holds due to Lemma C.1, the third and the fourth inequality hold in the same way as that in the proof of Bounding (2) in Proposition A.1. Combining the results in (1) and (2) finishes the whole proof. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "Remark C.1 In the last step of proof for Lemma C.1, we bound ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\lambda_{1}(\\bar{\\Sigma}_{s,t}^{-1}\\bar{\\Sigma}_{t})\\leq1+\\frac{(i-1)\\frac{B^{2}}{\\sigma^{2}}\\frac{\\lambda_{1}(\\Sigma_{0})}{\\lambda_{d}(\\Sigma_{0})}}{\\lambda_{d}(\\Sigma_{q}^{-1})+\\frac{i-1}{\\lambda_{1}(\\Sigma_{0})+\\sigma^{2}/\\beta}}\\leq1+\\frac{(i-1)\\frac{B^{2}}{\\sigma^{2}}\\frac{\\lambda_{1}(\\Sigma_{0})}{\\lambda_{d}(\\Sigma_{0})}}{\\frac{i-1}{\\lambda_{1}(\\Sigma_{0})+\\sigma^{2}/\\beta}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Thus our upper bound is independent of the number $L$ of the concurrent tasks. Actually, $\\forall i\\in[L]$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\lambda_{1}(\\bar{\\Sigma}_{s,t}^{-1}\\bar{\\Sigma}_{t})\\leq1+\\frac{\\frac{B^{2}}{\\sigma^{2}}\\frac{\\lambda_{1}(\\Sigma_{0})}{\\lambda_{d}(\\Sigma_{0})}}{\\frac{\\lambda_{d}(\\Sigma_{q}^{-1})}{(i-1)}+\\frac{1}{\\lambda_{1}(\\Sigma_{0})+\\sigma^{2}/\\beta}}\\leq1+\\frac{\\frac{B^{2}}{\\sigma^{2}}\\frac{\\lambda_{1}(\\Sigma_{0})}{\\lambda_{d}(\\Sigma_{0})}}{\\frac{\\lambda_{d}(\\Sigma_{q}^{-1})}{L}+\\frac{1}{\\lambda_{1}(\\Sigma_{0})+\\sigma^{2}/\\beta}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "the sharper bound $\\begin{array}{r}{1+\\frac{\\frac{B^{2}}{\\sigma^{2}}\\frac{\\lambda_{1}(\\Sigma_{0})}{\\lambda_{d}(\\Sigma_{0})}}{\\frac{\\lambda_{d}(\\Sigma_{q}^{-1})}{L}+\\frac{1}{\\lambda_{1}(\\Sigma_{0})+\\sigma^{2}/\\beta}}}\\end{array}$ . $L$ -dependent. If $\\begin{array}{r}{\\frac{\\lambda_{d}(\\Sigma_{q}^{-1})}{L}\\ <<\\ \\frac{1}{\\lambda_{1}(\\Sigma_{0})+\\sigma^{2}/\\beta}}\\end{array}$ X1(2o)+o\u00b2/B, the infuence of $L$ to the regret may be large; otherwise, the influence of $L$ to the regret may be negligible. Next, we prove the concurrent regret bound for HierTS and HierBayesUCB. ", "page_idx": 20}, {"type": "text", "text": "Theorem C.1 (Theorem 5.3 in the main text). Let $|S_{t}|\\le L\\le m$ forallrounds $t\\geq1$ Theninthe multi-task Gaussian linear bandit setting,the Bayes regret bound of HierTS is as follow: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{B}\\mathcal{R}(m,n)\\leq2B m d\\sqrt{\\lambda_{1}(\\Sigma_{0}+\\Sigma_{q})}(\\sqrt{d}+\\|\\mu_{q}\\|_{\\hat{\\Sigma}_{s,1}^{-1}})+m d\\sqrt{2n c_{1}\\log{(1+\\frac{n}{d})}}}\\\\ &{\\quad\\quad\\quad\\quad+\\sqrt{m n d}\\sqrt{2d c_{2}c_{3}\\log{(1+\\frac{m\\,\\mathrm{Tr}\\,\\left(\\Sigma_{0}^{-1}\\Sigma_{q}\\right)}{d})}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. Recall that $\\mathcal{C}_{t}=\\{s\\in\\mathcal{S}_{t}:\\lambda_{d}(G_{s,t})\\geq\\beta/\\sigma^{2}\\}$ is the set of sufficiently-explored tasks at round $t$ . Then, due to the modification of HierTS algorithm, we decompose Bayes regret $B\\mathcal{R}(m,n)$ into two terms and bound them respectively: ", "page_idx": 20}, {"type": "equation", "text": "$$\nB\\mathcal{R}(m,n)=\\mathbb{E}\\sum_{t\\geq1}\\sum_{s\\in\\mathcal{S}_{t}}\\mathbf{1}\\{s\\notin\\mathcal{C}_{t}\\}\\theta_{s,*}^{\\top}(A_{s,*}-A_{s,t})+\\mathbb{E}\\sum_{t\\geq1}\\sum_{s\\in\\mathcal{S}_{t}}\\mathbf{1}\\{s\\in\\mathcal{C}_{t}\\}\\theta_{s,*}^{\\top}(A_{s,*}-A_{s,t})\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "(1) Bounding $\\begin{array}{r}{\\mathbb{E}\\sum_{t\\geq1}\\sum_{s\\in S_{t}}\\mathbf{1}\\{s\\notin\\mathcal{C}_{t}\\}\\theta_{s,*}^{\\top}(A_{s,*}-A_{s,t})}\\end{array}$ . Similar to the proof for Theorem B.1 (3), we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\theta_{s,*}^{\\top}(A_{s,*}-A_{s,t})\\leq\\|\\theta_{s,*}\\|_{\\hat{\\Sigma}_{s,1}^{-1}}\\|A_{s,*}-A_{s,t}\\|_{\\hat{\\Sigma}_{s,1}}\\leq2c\\sqrt{\\lambda_{1}(\\hat{\\Sigma}_{s,1})}\\Big(\\|\\theta_{s,*}-\\mu_{q}\\|_{\\hat{\\Sigma}_{s,1}^{-1}}+\\|\\mu_{q}\\|_{\\hat{\\Sigma}_{s,1}^{-1}}\\Big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and $\\mathbb{E}\\big[\\|\\theta_{s,*}-\\mu_{q}\\|_{\\hat{\\Sigma}_{s,1}^{-1}}\\big]\\!\\le\\!\\!\\sqrt{\\mathbb{E}\\|\\hat{\\Sigma}_{s,1}^{-\\frac{1}{2}}(\\theta_{s,*}-\\mu_{q})\\|_{2}^{2}}\\!=\\!\\!\\sqrt{d}$ Recalling the independence between $\\theta_{s,*}$ and actions $A_{s,t}$ yields ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\mathbb{E}\\sum_{t\\geq1}\\displaystyle\\sum_{s\\in S_{t}}\\mathbf{1}\\{s\\notin\\mathcal{C}_{t}\\}\\theta_{s,*}^{\\top}(A_{s,*}-A_{s,t})}\\\\ &{\\le2B\\sqrt{\\lambda_{1}(\\Sigma_{0}+\\Sigma_{q})}\\mathbb{E}\\displaystyle\\sum_{t\\geq1}\\sum_{s\\in S_{t}}\\mathbf{1}\\{s\\notin\\mathcal{C}_{t}\\}\\mathbb{E}\\Big(\\|\\theta_{s,*}-\\mu_{q}\\|_{\\hat{\\Sigma}_{s,1}^{-1}}+\\|\\mu_{q}\\|_{\\hat{\\Sigma}_{s,1}^{-1}}\\Big)}\\\\ &{\\leq2B\\sqrt{\\lambda_{1}(\\Sigma_{0}+\\Sigma_{q})}(\\sqrt{d}+\\|\\mu_{q}\\|_{\\hat{\\Sigma}_{s,1}^{-1}})m d,}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The last inequality holds because in the modified HierTS, event $\\{s\\notin{\\mathcal{C}}_{t}\\}$ occurs at most $d$ times for any task $s\\in[m]$ ", "page_idx": 20}, {"type": "text", "text": "(2) Bounding $\\begin{array}{r}{\\mathbb{E}\\sum_{t\\geq1}\\sum_{s\\in S_{t}}{\\mathbf{1}}\\{s\\in\\mathcal{C}_{t}\\}\\|A_{s,t}\\|_{\\hat{\\Sigma}_{s,t}}^{2}}\\end{array}$ . It suffces toapply the upperboud in Proposition C.1. ", "page_idx": 20}, {"type": "text", "text": "Combining the upper bounds in steps (1) and (2) obtains the final Bayes regret bound for HierTS in the concurrent setting. \u53e3 ", "page_idx": 20}, {"type": "text", "text": "Theorem C.2 (Logarithmic Regret Bound for HierBayesUCB in the Concurrent Bandit Setting). Supposetheactionset $\\boldsymbol{\\mathcal{A}}$ isfinitewith $|{\\mathcal{A}}|<\\infty.$ Let $\\vert S_{t}\\vert\\le L\\le m$ forallrounds $t\\geq1$ .Theninthe multi-task Gaussian linear bandit setting, the Bayes regret bound of HierTS is as follow: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{3\\mathcal{R}(m,n)\\leq m n\\epsilon+4B\\sqrt{\\lambda_{1}(\\Sigma_{0}+\\Sigma_{q})}\\big(\\sqrt{d+\\sqrt{8d\\ln\\frac{1}{\\zeta}}}+\\|\\mu_{q}\\|_{\\hat{\\Sigma}_{s,1}^{-}}\\big)m n|A|\\delta}\\\\ &{+\\,2B\\sqrt{\\lambda_{1}(\\Sigma_{0}+\\Sigma_{q})}\\big(\\sqrt{d}+\\|\\mu_{q}\\|_{\\hat{\\Sigma}_{s,1}^{-}}\\big)m d+\\mathbb{E}\\big[\\frac{8\\log\\frac{1}{\\delta}}{\\Delta_{\\operatorname*{min}}^{\\epsilon}}\\big]\\big\\{2m d c_{1}\\log\\big(1+\\frac{n}{d}\\big)+2d c_{3}c_{2}\\log\\big(1+\\frac{m\\log\\frac{1}{\\delta}}{2}\\big)\\big\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. Similar to the proof of Theorem C.1, we decompose the Bayes regret as $B\\mathcal{R}(m,n)\\;=$ $\\begin{array}{r}{\\mathbb E\\sum_{t\\geq1}\\sum_{s\\in{\\cal S}_{t}}\\mathbf{1}\\{s\\notin{\\mathcal C}_{t}\\}\\theta_{s,*}^{\\top}(A_{s,*}-A_{s,t})+\\mathbb E\\sum_{t\\geq1}\\sum_{s\\in{\\mathcal S}_{t}}\\mathbf{1}\\{s\\in{\\mathcal C}_{t}\\}\\theta_{s,*}^{\\top}(A_{s,*}-A_{s,t})}\\end{array}$ Then ", "page_idx": 20}, {"type": "text", "text": "Table 3: Different Bayes regret bounds for multi-task $d$ -dimensional linear bandit problem in the concurrent setting. $m$ is the number of tasks, $n$ is the number of iterations per task, $\\boldsymbol{\\mathcal{A}}$ is the action set.Bayes RegretBound $\\r=$ Bound I + Bound $\\mathbf{II}+\\mathbf{N},$ egligible Terms, where Bound I is the regret bound for solving $m$ tasks, Bound $\\mathbf{II}$ the regret bound for learning hyper-parameter $\\mu_{*}$ ", "page_idx": 21}, {"type": "table", "img_path": "joNPMCzVIi/tmp/d33f4dcbf98de9ab4d261577eeb6a789144649518f1c4da662660b848d73608f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "we bound the first term with the proof for Theorem C.1 (1), bound the second term with the proof for ourTheoremB.1.Then with probability $1-\\zeta$ overthedrawof $\\{\\theta_{s,*}\\}_{s\\in[m]}$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{3\\mathcal{R}(m,n)\\leq2B\\sqrt{\\lambda_{1}(\\Sigma_{0}+\\Sigma_{q})}(\\sqrt{d}+\\|\\mu_{q}\\|_{\\hat{\\Sigma}_{s,1}^{-1}})m d+\\mathbb{E}[\\frac{8\\log\\frac{1}{\\delta}}{\\Delta_{\\operatorname*{min}}^{\\epsilon}}]\\displaystyle\\sum_{t\\geq1}\\sum_{s\\in\\mathcal{S}_{t}}\\mathbb{E}\\big[\\mathbf{1}\\{s\\in\\mathcal{C}_{t}\\}\\|A_{s,t}\\|_{\\hat{\\Sigma}_{s,t}}^{2}\\big]}\\\\ &{\\qquad\\qquad+\\,m n\\epsilon+4B\\sqrt{\\lambda_{1}(\\Sigma_{0}+\\Sigma_{q})}(\\sqrt{d+\\sqrt{8d\\ln\\frac{1}{\\zeta}}}+\\|\\mu_{q}\\|_{\\hat{\\Sigma}_{s,1}^{-1}})m n|A|\\delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Plugging the upper bound on $\\begin{array}{r}{\\sum_{t\\geq1}\\sum_{s\\in{\\cal S}_{t}}\\mathbb{E}\\big[{\\bf1}\\{s\\in{\\mathcal C}_{t}\\}\\big\\|A_{s,t}\\big\\|_{\\hat{\\Sigma}_{s,t}}^{2}\\big]}\\end{array}$ in Proposition C.1 into the right hand side of the above inequality finishes the whole proof. \u53e3 ", "page_idx": 21}, {"type": "text", "text": "D Proofs for Regret Bounds of HierTS and HierBayesUCB in the Semi-Bandit Setting ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We also choose to give the worst-case upper bound on $\\begin{array}{r}{\\sum_{t\\geq1}\\sum_{s\\in\\mathcal{S}_{t}}\\sum_{a\\in A_{s,t}}\\Phi_{a}^{\\top}\\hat{\\Sigma}_{s,t}\\Phi_{a}}\\end{array}$ as follow. ", "page_idx": 21}, {"type": "text", "text": "Proposition D.1 Let $c_{1}=\\sigma^{2}+B^{2}\\lambda_{1}(\\Sigma_{0}),$ $c_{4}=\\sigma^{2}+B^{2}L\\lambda_{1}(\\Sigma_{0})+B^{2}\\lambda_{1}(\\Sigma_{q})\\kappa(\\Sigma_{0}),$ then ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\sum_{t\\geq1}\\sum_{s\\in S_{t}}\\sum_{a\\in A_{s,t}}\\Phi_{a}^{\\top}\\hat{\\Sigma}_{s,t}\\Phi_{a}\\leq2c_{1}m\\log\\big(1+\\frac{n L}{d}\\big)+2c_{4}L d\\log\\big(1+\\frac{m\\operatorname{Tr}(\\Sigma_{0}^{-1}\\Sigma_{q}\\big)}{d}\\big).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. Recall that $\\begin{array}{r}{G_{s,t}\\;=\\;\\sigma^{-2}\\sum_{\\ell<t}{\\bf1}\\{s\\;\\in\\;\\mathcal{S}_{\\ell}\\}(\\sum_{a\\in A_{s,t}}\\Phi_{a}\\Phi_{a}^{\\top})}\\end{array}$ \uff0c $\\begin{array}{r}{B_{s,t}\\ =\\ \\sigma^{-2}\\sum_{\\ell<t}{{\\bf1}\\{s\\ \\in}}}\\end{array}$ $\\begin{array}{r}{\\mathfrak{I}_{\\ell}\\}(\\sum_{a\\in A_{s,t}}\\Phi_{a}\\bar{\\mathbf{w}}_{s}(a)),\\tilde{\\Sigma}_{s,t}^{-1}=\\Sigma_{0}^{-1}\\!+\\!G_{s,t},\\bar{\\Sigma}_{t}^{-1}=\\Sigma_{q}^{-1}\\!+\\!\\sum_{s\\in[m]}(\\Sigma_{0}\\!+\\!G_{s,t}^{-1})^{-1}\\,.}\\end{array}$ . Then, analogous to the prof foreProposion .1, we inroducethe matrix $\\begin{array}{r}{\\tilde{X}_{s,t}\\triangleq\\left(\\Sigma_{0}^{-1}+\\frac{1}{B^{2}\\lambda_{1}(\\Sigma_{0})+\\sigma^{2}}\\sum_{\\ell<t}{\\bf1}\\big\\{s\\in\\big\\}\\right.}\\end{array}$ $\\begin{array}{r l}&{S_{\\ell}\\big\\rangle\\big(\\sum_{a\\in A_{s,t}}\\Phi_{a}\\Phi_{a}^{\\top}\\big)\\big)^{-1}.\\qquad\\qquad\\mathrm{We}\\quad\\quad\\mathrm{next}\\quad\\mathrm{bound}\\quad\\sum_{t\\geq1}\\sum_{s\\in S_{t}}\\sum_{a\\in A_{s,t}}\\Phi_{a}^{\\top}\\tilde{\\sum}_{s,t}\\Phi_{a}}\\\\ &{\\sum_{t\\geq1}\\sum_{s\\in S_{t}}\\sum_{a\\in A_{s,t}}\\Phi_{a}^{\\top}\\tilde{\\sum}_{s,t}\\Sigma_{0}^{-1}\\bar{\\sum}_{t}\\Sigma_{0}^{-1}\\tilde{\\sum}_{s,t}\\Phi_{a}.}\\\\ &{(1)\\;\\mathbf{Bounding}\\sum_{t\\geq1}\\sum_{s\\in S_{t}}\\sum_{a\\in A_{s,t}}\\Phi_{a}^{\\top}\\tilde{\\sum}_{s,t}\\Phi_{a}.}\\end{array}$ and ", "page_idx": 21}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\sum_{t\\geq1}\\sum_{s\\in\\mathcal{S}_{t}}\\sum_{u\\in\\mathcal{A}_{s,t}}\\Phi_{a}^{\\top}\\bar{\\Sigma}_{s,t}\\Phi_{a}}\\\\ &{=\\![\\sigma^{2}+B^{2}\\lambda_{1}(\\Sigma_{0})]\\displaystyle\\sum_{t\\geq1}\\sum_{s=1}^{m}\\mathbf{1}\\!\\left\\{\\mathcal{S}_{t}=s\\right\\}\\displaystyle\\sum_{a\\in\\mathcal{A}_{s,t}}\\frac{\\Phi_{a}^{\\top}\\tilde{\\Sigma}_{s,t}\\Phi_{a}}{\\sigma^{2}+B^{2}\\lambda_{1}(\\Sigma_{0})}}\\\\ &{\\leq\\!2[\\sigma^{2}+B^{2}\\lambda_{1}(\\Sigma_{0})]\\displaystyle\\sum_{t\\geq1}\\sum_{s=1}^{m}\\mathbf{1}\\!\\left\\{\\mathcal{S}_{t}=s\\right\\}\\displaystyle\\sum_{a\\in\\mathcal{A}_{s,t}}\\log\\big(1+\\frac{\\Phi_{a}^{\\top}\\tilde{\\Sigma}_{s,t}\\Phi_{a}}{\\sigma^{2}+B^{2}\\lambda_{1}(\\Sigma_{0})}\\big)}\\\\ &{\\leq\\!2[\\sigma^{2}+B^{2}\\lambda_{1}(\\Sigma_{0})]\\displaystyle\\sum_{t\\geq1}\\sum_{s=1}^{m}\\mathbf{1}\\!\\left\\{\\mathcal{S}_{t}=s\\right\\}\\displaystyle\\sum_{a\\in\\mathcal{A}_{s,t}}\\log\\operatorname*{det}(I+\\frac{\\tilde{\\Sigma}_{s,t}^{\\frac{1}{2}}\\Phi_{a}\\Phi_{a}^{\\top}\\tilde{\\Sigma}_{s,t}^{\\frac{1}{2}}}{\\sigma^{2}+B^{2}\\lambda_{1}(\\Sigma_{0})})}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{=}2(\\rho^{2}+B^{2}\\lambda_{1}(\\Sigma_{0})|\\displaystyle\\sum_{s=1}^{m}\\sum_{\\ell\\neq1}\\sum_{\\ell=\\delta}^{\\infty}\\mathbb{I}_{\\displaystyle\\left\\{S_{s}^{\\ell}\\right\\}}\\displaystyle\\sum_{\\ell\\in\\delta}\\mathbb{I}_{\\displaystyle\\left\\{S_{s}^{\\ell}\\right\\}}+\\frac{1}{\\delta}\\overline{{\\rho_{\\lambda}\\phi_{\\lambda}^{\\top}}}\\displaystyle\\sum_{i\\in\\delta}\\log\\mathbb{I}_{\\displaystyle\\left\\{\\bar{X}_{s,i}^{-1}\\right\\}}^{\\ell}-\\log\\mathbb{I}_{\\displaystyle\\left\\{\\bar{X}_{s}^{\\ell}\\right\\}}^{\\ell})-\\log\\mathbb{I}_{\\displaystyle\\mathcal{X}_{s}^{\\ell}}[\\phi_{\\lambda}^{\\top}]}}\\\\ {{=}2(\\rho^{2}+B^{2}\\lambda_{1}(\\Sigma_{0})|\\displaystyle\\sum_{s=1}^{m}\\left[\\log\\operatorname*{det}(\\tilde{X}_{s,m+1}^{-1})-\\log\\operatorname*{det}(\\tilde{X}_{s,1}^{-1})\\right]}}\\\\ {{=}2(\\rho^{2}+B^{2}\\lambda_{1}(\\Sigma_{0})|\\displaystyle\\sum_{s=1}^{m}\\log\\operatorname*{det}\\big(I+\\frac{1}{\\sigma^{2}+B^{2}\\lambda_{1}(\\Sigma_{0})}\\displaystyle\\sum_{\\ell\\leq m}\\operatorname*{I}_{\\displaystyle\\{S_{s}^{\\ell}\\}}\\sum_{\\ell=\\delta}^{\\infty}\\frac{\\hat{\\lambda}_{\\ell}^{3}}{\\sigma_{\\lambda}}\\phi_{\\Phi}^{\\top}\\Sigma_{s,\\ell}^{\\top}\\big)}}\\\\ {{=}2[\\rho^{2}+B^{2}\\lambda_{1}(\\Sigma_{0})|\\displaystyle\\sum_{s=1}^{m}\\log\\frac{\\Upsilon\\big(I+\\frac{1}{\\sigma^{2}+B^{2}\\lambda_{1}(\\Sigma_{0})}\\big)\\sum_{\\ell\\leq m}1\\{S_{\\ell}=\\delta\\}\\sum_{\\ell=\\delta}\\sum_{\\ell=\\delta}\\frac{\\hat{\\lambda}_{\\ell}^{3}}{\\sigma_{\\lambda}}\\phi_{\\Phi}\\Phi_{\\bar{\\alpha}}^{\\top}\\Sigma_{s,i}^{\\top}}{16\\sigma^{2}}}}\\\\ {{=}2[\\rho^{2}+B^{2}\\lambda_{1}(\\Sigma \n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "(2) Bounding $\\begin{array}{r}{\\sum_{t\\geq1}\\sum_{s\\in S_{t}}\\sum_{a\\in A_{s,t}}\\Phi_{a}^{\\top}\\tilde{\\Sigma}_{s,t}\\Sigma_{0}^{-1}\\bar{\\Sigma}_{t}\\Sigma_{0}^{-1}\\tilde{\\Sigma}_{s,t}\\Phi_{a}.}\\end{array}$ $\\forall t\\geq1,s\\in S_{t}$ \uff0c $\\forall a\\in A_{s,t}$ , we hav $\\begin{array}{r}{\\mathtt{S}\\;\\Phi_{a}^{\\top}\\tilde{\\Sigma}_{s,t}\\Sigma_{0}^{-1}\\bar{\\Sigma}_{t}\\Sigma_{0}^{-1}\\tilde{\\Sigma}_{s,t}\\Phi_{a}\\leq B^{2}\\lambda_{1}(\\Sigma_{q})\\kappa(\\Sigma_{0})+L B^{2}\\lambda_{1}(\\Sigma_{0})+}\\end{array}$ $\\sigma^{2}$ $M\\triangleq\\left(\\tilde{\\Sigma}_{s,t}^{\\frac{1}{2}}\\Phi_{a_{1}},\\tilde{\\Sigma}_{s,t}^{\\frac{1}{2}}\\Phi_{a_{2}},\\ldots,\\tilde{\\Sigma}_{s,t}^{\\frac{1}{2}}\\Phi_{a_{|A_{s,t}|}}\\right)\\in\\mathbb{R}^{d\\times|A_{s,t}|}$ ,we have $\\begin{array}{r}{\\sum_{a\\in A_{s,t}}\\tilde{\\Sigma}_{s,t}^{\\frac{1}{2}}\\Phi_{a}\\Phi_{a}^{\\top}\\tilde{\\Sigma}_{s,t}^{\\frac{1}{2}}=M M^{\\top}}\\end{array}$ . Using the Wely's inequality, we further have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathrm{s}_{1}(I+\\sigma^{-2}M^{\\top}M)\\le\\lambda_{1}(I)+\\lambda_{1}(\\sigma^{-2}M M^{\\top})\\le1+\\sigma^{-2}\\sum_{a\\in A_{s,t}}\\lambda_{1}(\\tilde{\\Sigma}_{s,t}^{\\frac{1}{2}}\\Phi_{a}\\Phi_{a}^{\\top}\\tilde{\\Sigma}_{s,t}^{\\frac{1}{2}})=1+\\sigma^{-2}\\sum_{a\\in A_{s,t}}\\lambda_{1}\\le1.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Thenwecan estmatethapbetwemix $\\bar{\\Sigma}_{t+1}^{-1}$ and $\\bar{\\Sigma}_{t}^{-1}$ as fllow: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{\\lambda\\geq1}^{n+1}-\\sum_{i=1}^{n}\\alpha_{i}^{2}}\\\\ &{=\\mathbb{E}_{\\lambda\\geq1}^{n}(\\xi_{L}(a_{L})+\\alpha^{2}-\\sum_{i=1}^{n}\\xi_{L}(a_{L})^{\\lambda})^{-1}-(\\mathbb{E}_{\\lambda}+\\alpha^{2})_{L}^{n-1}}\\\\ &{\\quad-\\mathbb{E}_{\\lambda\\geq1}^{n}-\\sum_{i=1}^{n}(\\xi_{L}^{(i)}+\\alpha_{L}+\\alpha^{2}-\\sum_{i=1}^{n}\\xi_{L}(a_{L})^{\\lambda})^{\\lambda}}\\\\ &{\\quad+\\alpha_{L}^{-1}\\eta_{L}^{(i)}-\\alpha_{L}^{-1}\\xi_{L}(a_{L})^{\\lambda}}\\\\ &{=\\mathbb{E}_{\\lambda\\geq1}^{n+1}[\\xi_{L}(a_{L})-(\\xi_{L}^{(i)}+\\alpha^{2}-\\sum_{i=1}^{n}\\xi_{L}(a_{L})^{\\lambda})^{\\lambda}]^{-1}\\mathbb{E}_{\\lambda\\geq1}^{n-1}}\\\\ &{\\quad+\\alpha_{L}^{-1}\\eta_{L}^{(i)}\\Big[\\eta_{L}-(\\xi_{L}^{(i)}-\\sum_{i=1}^{n}\\xi_{L}(a_{L})^{\\lambda})^{\\lambda}\\Big]^{-1}\\mathbb{E}_{\\lambda\\geq1}^{n-1}}\\\\ &{\\quad+\\alpha_{L}^{-1}\\eta_{L}^{(i)}\\Big[\\eta_{L}-(\\xi_{L}^{(i)}-\\sum_{i=1}^{n}\\xi_{L}(a_{L})^{\\lambda})^{\\lambda}\\Big]^{-1}\\mathbb{E}_{\\lambda\\geq1}^{n-1}}\\\\ &{\\quad+\\alpha_{L}^{-1}\\eta_{L}^{(i)}\\Big[\\eta_{L}-(\\xi_{L}^{(i)}+\\alpha^{2}-3\\xi_{L}(a_{L})^{\\lambda})^{\\lambda}\\Big]^{-1}\\mathbb{E}_{\\lambda\\geq1}^{n}}\\\\ &{\\quad+\\alpha_{L}^{-1}\\eta_{L}^{(i)}\\Big[\\eta_{L}-2(\\xi_{L}^{(i)}-3\\xi_{L})^{\\lambda}\\Big]^{-1 \n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the second and the sixth equality hold due to the Woodbury matrix identity. The proof for Eq. (6) is similar to the proof for Eq. (5), but requires more refined analysis (i.e. the first inequality in Eq. (6)) to estimate the lower bound of $\\bar{\\Sigma}_{t+1}^{-1}-\\bar{\\bar{\\Sigma}}_{t}^{-1}$ .Then recall $c_{4}=\\sigma^{2}+B^{2}L\\lambda_{1}(\\Sigma_{0})+B^{2}\\lambda_{1}(\\Sigma_{q})\\kappa(\\Sigma_{0})$ for brevity, we can bound \u2211t>1 \u2265sest \u2265aeAs,t $\\begin{array}{r}{\\sum_{t\\geq1}\\sum_{s\\in S_{t}}\\sum_{a\\in A_{s,t}}\\Phi_{a}^{\\top}\\tilde{\\Sigma}_{s,t}\\Sigma_{0}^{-1}\\bar{\\Sigma}_{t}\\Sigma_{0}^{-1}\\tilde{\\Sigma}_{s,t}\\Phi_{a}}\\end{array}$ as follow: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\sum_{k\\geq1}\\sum_{i,j\\geq k,n\\geq1\\atop i\\geq1}^{n}\\phi_{k,\\alpha,\\gamma_{k-1}}^{(1)}\\frac{\\gamma_{k,\\alpha,\\gamma_{k-1}}^{i}\\gamma_{k,\\gamma_{1}}^{i}\\gamma_{k,\\gamma_{1}}^{i}}{\\gamma_{k,\\alpha,\\gamma_{k-1}}^{i}}\\phi_{k,\\alpha}^{(1)},}\\\\ &{\\leq2\\kappa\\sum_{i\\geq1}^{n}\\sum_{i,j\\geq k,n\\geq1\\atop i\\geq1}^{n}\\mathrm{terf~}(1+\\frac{\\frac{4}{\\alpha}}{\\sigma^{2}})^{k}\\sum_{i,j\\geq1}^{n}\\frac{\\gamma_{k,\\alpha,\\gamma_{k-1}}^{i}\\gamma_{k,\\gamma_{1}}^{i}\\gamma_{k,\\gamma_{1}}^{i}}{\\beta^{(2)}k^{(2)}k^{(1)}(\\frac{\\gamma_{k,\\gamma_{1}}^{i}\\gamma_{1}^{i}\\gamma_{1}^{i}\\gamma_{2}^{i})}}}\\\\ &{\\quad-2\\kappa\\sum_{i\\geq1}^{n}\\sum_{i,j\\geq k,n\\geq1\\atop i\\geq1}^{n}\\mathrm{terf~}(1+\\frac{\\frac{4}{\\sigma^{2}}}{\\sigma^{2}})^{k}\\sum_{i,j\\geq1}^{n}\\phi_{k,\\alpha}^{(1)}\\frac{\\gamma_{k,\\gamma_{1}}^{i}\\gamma_{k,\\gamma_{1}}^{i}}{\\beta^{(2)}k^{(1)}(\\frac{\\gamma_{k,\\gamma_{1}}^{i}\\gamma_{1}^{i}\\gamma_{2}^{i})}}}\\\\ &{\\quad-2\\kappa\\sum_{i,j\\geq1}^{n}\\sum_{i,j\\geq k,n\\geq1\\atop i\\geq1}^{n}\\Big[\\mathrm{terf~}(\\sum_{i,j\\geq1}^{n}+\\frac{\\gamma_{k,\\alpha,\\gamma_{k-1}}^{i}\\gamma_{k,\\gamma_{1}}^{i}\\gamma_{k,\\gamma_{1}}^{i}}{\\beta^{(2)}k^{(1)}(\\frac{\\gamma_{k,\\gamma_{1}}^{i}\\gamma_{1}^{i}\\gamma_{1}^{i}\\gamma_{1}^{i}\\gamma_{1}^{i}\\gamma_{1}^{i}\\gamma_{2}^{i})}}\\\\ &{\\quad\\leq2\\kappa\\sum_{i \n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the second inequality holds due to Eq. (6) ", "page_idx": 23}, {"type": "text", "text": "LemmaD.1IfaGaussianrandomvariable $X\\,\\sim\\,{\\mathcal{N}}(\\mu,\\sigma^{2})$ then $\\mathbb{E}[X\\mathbf{1}\\{X\\,\\geq\\,0\\}]\\;=\\;\\mu\\big[\\mathbf{1}\\;-\\;$ $\\begin{array}{r}{\\Phi_{G}(-\\frac{\\mu}{\\sigma})\\]+\\frac{\\sigma}{\\sqrt{2\\pi}}\\exp\\{-\\frac{\\mu^{2}}{2\\sigma^{2}}\\}}\\end{array}$ Ifurther $\\mu\\le0$ then $\\begin{array}{r}{\\mathbb{E}[X\\mathbf{1}\\{X\\ge0\\}]={\\frac{\\sigma}{\\sqrt{2\\pi}}}\\exp\\{-{\\frac{\\mu^{2}}{2\\sigma^{2}}}\\}}\\end{array}$ ", "page_idx": 23}, {"type": "text", "text": "Theorem D.1 (Theorem 5.4 in the main text, Regret Bound of HierTS in the Semi-Bandit Setting). Let $|S_{t}|\\;=\\;1$ for any $t\\,\\geq\\,1$ Let $c\\ \\geq\\ {\\sqrt{2\\ln\\left({\\frac{n K B\\lambda_{1}(\\Sigma_{0})}{\\sqrt{2\\pi}}}\\right)}}$ \uff0c $c_{1}\\,=\\,\\sigma^{2}+B^{2}\\lambda_{1}(\\Sigma_{0})$ $c_{4}\\,=\\,\\sigma^{2}\\,+$ $B^{2}L\\lambda_{1}(\\Sigma_{0})\\!+\\!B^{2}\\lambda_{1}(\\Sigma_{q})\\kappa(\\Sigma_{0}),$ then in the multi-taskGaussian semi-bandit setting,theBayes regret boundofcombinatorialHierTSis: ", "page_idx": 23}, {"type": "equation", "text": "$$\nB\\mathcal{R}(m,n)\\leq m+c\\sqrt{m n L}\\sqrt{2c_{1}m\\log{(1+\\frac{n L}{d})}+2c_{4}L d\\log(1+\\frac{m\\,\\mathrm{Tr}\\bigl(\\Sigma_{0}^{-1}\\Sigma_{q}\\bigr)}{d})}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. Note that $\\bar{\\mathbf{w}}_{s}=\\Phi\\theta_{s,*}$ , then define $\\begin{array}{r}{g(A,\\theta)=\\sum_{a\\in A}\\langle\\Phi_{a},\\theta\\rangle}\\end{array}$ for brevity, we have the following result: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{B R}(m,n)=\\displaystyle\\mathbb{E}\\sum_{t\\geq1}\\sum_{s\\in\\mathcal{S}_{t}}\\big[\\sum_{a\\in A_{s,*}}\\bar{\\mathbf{w}}_{s}(a)-\\sum_{a\\in A_{s,t}}\\bar{\\mathbf{w}}_{s}(a)\\big]}\\\\ &{\\qquad\\qquad=\\displaystyle\\mathbb{E}\\sum_{t\\geq1}\\sum_{s\\in\\mathcal{S}_{t}}\\Big[\\sum_{a\\in A_{s,*}}\\langle\\Phi_{a},\\theta_{s,*}\\rangle-\\sum_{a\\in A_{s,t}}\\langle\\Phi_{a},\\theta_{s,*}\\rangle\\Big]}\\\\ &{\\qquad\\qquad=\\displaystyle\\mathbb{E}\\sum_{t\\geq1}\\sum_{s\\in\\mathcal{S}_{t}}\\big[g(A_{s,*},\\theta_{s,*})-g(A_{s,t},\\theta_{s,*})\\big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Define upper confidence bound $\\begin{array}{r}{U_{t,s}(A)=\\sum_{a\\in A}\\left[\\langle\\Phi_{a},\\hat{\\mu}_{s,t}\\rangle\\!+\\!c\\sqrt{\\Phi_{a}^{\\top}\\hat{\\Sigma}_{s,t}\\Phi_{a}}\\right]}\\end{array}$ , where $c$ is a constant to be specified. Notice that $A_{s,*}|H_{t}\\stackrel{\\mathrm{i.i.d.}}{\\sim}A_{s,t}|H_{t}$ and $U_{t,s}(\\cdot)$ is a deterministic function, thus ", "page_idx": 23}, {"type": "text", "text": "$\\mathbb{E}[U_{t,s}(A_{s,*})|H_{t}]=\\mathbb{E}[U_{t,s}(A_{s,t})|H_{t}]$ . Then we can decompose Bayes regret $B\\mathcal{R}(m,n)$ as follow: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathcal{B}\\mathcal{R}(m,n)=\\mathbb{E}\\sum_{t\\geq1}\\sum_{s\\in\\mathcal{S}_{t}}\\mathbb{E}\\big[g(A_{s,*},\\theta_{s,*})-U_{t,s}(A_{s,*})+U_{t,s}(A_{s,t})-g(A_{s,t},\\theta_{s,*})\\big|H_{t}\\big]}}\\\\ &{=\\mathbb{E}\\displaystyle\\sum_{t\\geq1}\\sum_{s\\in\\mathcal{S}_{t}}\\big[g(A_{s,*},\\theta_{s,*})-U_{t,s}(A_{s,*})\\big]+\\mathbb{E}\\displaystyle\\sum_{t\\geq1}\\sum_{s\\in\\mathcal{S}_{t}}\\big[U_{t,s}(A_{s,t})-g(A_{s,t},\\theta_{s,*})\\big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "(1)Boundi $\\begin{array}{r}{\\Im\\mathbb{E}\\sum_{t\\geq1}\\sum_{s\\in\\mathcal{S}_{t}}\\big[g(A_{s,*},\\theta_{s,*})-U_{t,s}(A_{s,*})\\big].}\\end{array}$ ", "page_idx": 24}, {"type": "text", "text": "For any $t\\geq1,\\,s\\in S_{t},\\,a\\in\\mathcal{A}$ define random variable $X_{t,s,a}=\\langle\\Phi_{a},\\theta_{s,*}-\\hat{\\mu}_{s,t}\\rangle-c\\sqrt{\\Phi_{a}^{\\top}\\hat{\\Sigma}_{s,t}\\Phi_{a}},$ then we have $X_{t,s,a}|H_{t}\\sim{\\mathcal{N}}(-c{\\sqrt{\\Phi_{a}^{\\top}\\hat{\\Sigma}_{s,t}\\Phi_{a}}},\\Phi_{a}^{\\top}\\hat{\\Sigma}_{s,t}\\Phi_{a})$ since $\\mathbb{E}[\\theta_{s,*}-\\hat{\\mu}_{s,t}\\vert H_{t}]=\\mathbf{0}$ Then ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\underset{\\theta\\in\\Theta(0)}{\\sum\\sum}\\Big[\\underset{\\ell=1}{\\overset{\\theta}{\\sum}}\\Big(\\theta\\Big(A_{s,\\ell}\\Big)_{s,\\ell}\\Big)-U_{\\ell,s}\\Big(A_{s,\\ell}\\Big)_{s}\\Big]}\\\\ &{=\\mathbb{E}\\underset{\\theta\\in\\Theta(0)}{\\sum\\sum}\\sum_{i,j=1}^{N}\\sum_{\\ell=1}^{N}\\sum_{\\ell=1}^{N}\\Big[\\sum_{\\ell=1}^{N}\\Big(\\sum_{i,j=1}^{N}\\Big(\\sum_{i,j=1\\atop i\\neq j}^{N}w_{i,j}\\Big(X_{s,\\ell}\\Big)_{s,\\ell}\\Big)-}\\\\ &{\\times\\mathbb{E}\\underset{\\theta\\in\\Theta(0)}{\\sum\\sum}\\sum_{i,j=1\\atop i\\neq j}^{N}\\sum_{\\ell=1}^{N}\\sum_{\\ell=1}^{N}w_{i,j}\\Big(X_{s,\\ell}\\Big)_{s,\\ell}\\Big)}\\\\ &{\\le\\mathbb{E}\\underset{\\theta\\in\\Theta(0)}{\\sum\\sum\\sum}\\sum_{i,j=1\\atop i\\neq j}^{N}\\sum_{\\ell=1}^{N}\\sum_{\\ell=1}^{N}\\sum_{\\ell=1}^{N}\\Big[\\sum_{i,j=1\\atop i\\neq j}^{N}w_{i,j}\\Big(X_{s,\\ell}\\Big)_{s,\\ell}\\Big]}\\\\ &{=\\mathbb{E}\\underset{\\theta\\in\\Theta(0)}{\\sum\\sum\\sum}\\sum_{i,j=1\\atop i\\neq j}^{N}\\sum_{\\ell=1}^{N}\\mathbb{E}\\Big[\\sum_{i,j=1\\atop i\\neq j}^{N}\\Big(\\sum_{i,j=1\\atop i\\neq i\\neq j}^{N}w_{i,j}\\Big(X_{s,\\ell}\\Big)_{s}\\Big)\\Big]\\Big[H_{s,\\ell}\\Big]}\\\\ &{\\le\\mathbb{E}\\underset{\\theta\\in\\Theta(0)}{\\sum\\sum}\\sum_{i,j=1\\atop i\\neq j}^{N}\\sum_{\\ell=1}^{N}\\sum_{\\ell=1}^{N}w_{i,j}\\Big(\\sum_{i,j=1\\atop i\\neq j}^{N}w_{i,j}\\Big(X_{s,\\ell}\\Big)_{s}\\Big)}\\\\ &{\\le\\mathbb{E}\\underset{\\theta\\in\\Theta(0)}{\\sum\\sum}\\sum_{i,j=1\\atop i\\neq j}^{N}\\sum_{\\ell=1}^{N}\\sum_{\\ell=1}^{N}\\sum_{\\ell=1}^{N}w_{i,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "$\\begin{array}{r}{n m K\\frac{B\\lambda_{1}(\\Sigma_{0})}{\\sqrt{2\\pi}}\\exp\\{-\\frac{c^{2}}{2}\\}\\leq m}\\end{array}$ then $c\\geq{\\sqrt{2\\ln\\left({\\frac{n K B\\lambda_{1}(\\Sigma_{0})}{\\sqrt{2\\pi}}}\\right)}}$ ", "page_idx": 24}, {"type": "text", "text": "(2) Bounding $\\begin{array}{r}{\\mathbb{E}\\sum_{t\\geq1}\\sum_{s\\in S_{t}}\\left[U_{t,s}(A_{s,t})-g(A_{s,t},\\theta_{s,*})\\right]}\\end{array}$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}\\sum_{t\\geq1}\\sum_{s\\in\\mathcal{S}_{t}}\\big[U_{s,}(A_{s,t})-g(A_{s,t},\\theta_{s,*})\\big]}\\\\ {\\displaystyle=\\mathbb{E}\\sum_{t\\geq1}\\sum_{s\\in\\mathcal{S}_{t}}\\sum_{u\\in\\mathcal{S}_{s,t}}\\big\\langle\\Phi_{a},\\hat{\\mu}_{s,t}-\\theta_{s,*}\\big\\rangle+c\\sqrt{\\Phi_{a}^{\\top}\\hat{\\Sigma}_{s,t}\\Phi_{a}}}\\\\ {\\displaystyle=\\mathbb{E}\\sum_{t\\geq1}\\sum_{s\\in\\mathcal{S}_{t}}\\sum_{u\\in\\mathcal{S}_{t}}\\mathbb{E}\\big[\\mathbf{1}_{a\\in A_{s,t}}|H_{t}\\big]\\mathbb{E}\\big[\\langle\\Phi_{a},\\hat{\\mu}_{s,t}-\\theta_{s,*}\\rangle|H_{t}\\big]+c\\mathbb{E}\\sum_{t\\geq1}\\sum_{s\\in\\mathcal{S}_{t}}\\sum_{u\\in A_{s,t}}\\sqrt{\\Phi_{a}^{\\top}\\hat{\\Sigma}_{s,t}\\Phi_{a}}}\\\\ {\\displaystyle=c\\mathbb{E}\\sum_{t\\geq1}\\sum_{s\\in\\mathcal{S}_{t}}\\sum_{u\\in A_{s,t}}\\sqrt{\\Phi_{a}^{\\top}\\hat{\\Sigma}_{s,t}\\Phi_{a}}}\\\\ {\\displaystyle\\leq c\\sqrt{m n L}\\sqrt{\\mathbb{E}\\sum_{t\\geq1}\\sum_{s\\in\\mathcal{S}_{t}}\\sum_{u\\in\\mathcal{S}_{t,t}}\\Phi_{a}^{\\top}\\hat{\\Sigma}_{s,t}\\Phi_{a}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the second equality holds because of the mutual independence between $A_{s,t}|H_{t}$ and $\\theta_{s,*}|H_{t}$ and $\\mathbb{E}[\\hat{\\mu}_{s,t}-\\theta_{s,*}|\\bar{H}_{t}]=\\mathbf{0}$ ; the last inequality holds due to the Jensen inequality. Then applying ", "page_idx": 24}, {"type": "text", "text": "Proposition D.1 to bound $\\begin{array}{r}{\\mathbb{E}\\sum_{t\\geq1}\\sum_{s\\in S_{t}}\\sum_{a\\in[A_{s,t}]}\\Phi_{a}^{\\top}\\hat{\\Sigma}_{s,t}\\Phi_{a}}\\end{array}$ , we can obtain ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\tau\\sum_{t>1}\\sum_{s\\in S_{t}}\\big[U_{t,s}(A_{s,t})-g(A_{s,t},\\theta_{s,*})\\big]\\leq c\\sqrt{m n L}\\sqrt{2c_{1}m\\log{(1+\\frac{n L}{d})}+2c_{4}L d\\log(1+\\frac{m\\,\\mathrm{Tr}(\\sum_{0=1}^{-1}\\theta_{s,*}(\\sum_{t=1}^{1}\\theta_{s,*}(\\sum_{t=1}^{n}\\theta_{t,*}(\\sum_{t=1}^{n}\\theta_{t,*}(\\sum_{t=1}^{n}\\theta_{t,*}(\\sum_{d=1}^{n}))+\\frac{n L}{d})\\theta_{s,t}(\\sum_{t=1}^{n}\\theta_{t,*}(\\sum_{t=1}^{n}\\theta_{t,*}(\\sum_{t=1}^{n}\\theta_{t,*}(\\sum_{t=1}^{n}))))))}\\leq c\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Combining the above results finishes the whole proof. ", "page_idx": 25}, {"type": "text", "text": "Theorem D.2 (Theorem 5.5 in the main text, Regret Bound of HierBayesUCB in the Semi-Bandit Setting). Let $|S_{t}|\\,=1$ for all rounds $t\\geq1$ Let $\\begin{array}{r}{\\check{c}_{1}=\\sigma^{2}+\\check{B}^{2}\\lambda_{1}(\\Sigma_{0}),}\\end{array}$ $c_{4}=\\sigma^{2}+B^{2}L\\lambda_{1}(\\Sigma_{0})\\,+$ $B^{2}\\lambda_{1}\\bar{(}\\Sigma_{q})\\kappa(\\dot{\\Sigma}_{0})$ , Then for any $\\epsilon>0,\\delta\\in(0,1).$ $\\zeta\\in(0,1)$ in themulti-taskGaussian semi-bandit setting, the Bayes regret upper bound of combinatorial HierBayesUCB is as follow: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\mathcal B}\\mathcal{R}(m,n)\\leq}\\mathbb{E}\\big[\\frac{8L\\log\\frac{1}{\\delta}}{\\Delta_{\\operatorname*{min}}^{\\epsilon}}\\big]\\big[2c_{1}m\\log{(1+\\frac{n L}{d})}+2c_{4}L d\\log(1+\\frac{m\\,\\mathrm{Tr}\\big(\\Sigma_{0}^{-1}\\Sigma_{q}\\big)}{d})\\big]+m n e}}\\\\ {{\\displaystyle~~~~~~~~~~~~~~~~+\\,4L B\\sqrt{\\lambda_{1}(\\Sigma_{0}+\\Sigma_{q})}\\big(\\sqrt{d+\\sqrt{8d\\ln\\frac{1}{\\zeta}}}+\\|\\mu_{q}\\|_{\\hat{\\Sigma}_{s,1}^{-1}}\\big)m n K\\delta.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "In Theorem 5.5 in the main text, we replace $\\sqrt{d+\\sqrt{8d\\ln\\frac{1}{\\zeta}}}$ in the right hand side of the above inequalitywith $\\sqrt{d}$ for ease of exposition. ", "page_idx": 25}, {"type": "text", "text": "Proof. Define the event $E_{s,t}=\\{\\forall a\\in\\mathcal{A}:|\\Phi_{a}^{\\top}(\\theta_{s,*}-\\hat{\\mu}_{s,t})|\\leq\\sqrt{2\\log\\frac{1}{\\delta}}\\|\\Phi_{a}\\|_{\\hat{\\Sigma}_{s,t}}\\}$ , and the upper confidence bound $\\begin{array}{r}{U_{t,s}(A)=\\sum_{a\\in A}\\langle\\Phi_{a},\\hat{\\mu}_{s,t}\\rangle+\\sqrt{2\\log\\frac{1}{\\delta}}\\|\\Phi_{a}\\|_{\\hat{\\Sigma}_{s,t}}}\\end{array}$ . Let $\\Delta_{s,t}\\,=\\,g(A_{s,*},\\theta_{s,*})\\,-$ $g(A_{s,t},\\theta_{s,*})$ , then we decompose the Bayes regret into three parts as follow: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathbb{E}\\sum_{t\\geq1}\\sum_{s\\in\\mathcal{S}_{t}}\\Delta_{s,t}}}\\\\ &{=\\!\\sum_{t\\geq1}\\displaystyle\\sum_{s\\in\\mathcal{S}_{t}}\\mathbb{E}\\!\\left[\\Delta_{s,t}\\mathbf{1}\\{\\Delta_{s,t}\\geq\\epsilon,E_{s,t}\\}\\right]+\\!\\sum_{t\\geq1}\\sum_{s\\in\\mathcal{S}_{t}}\\mathbb{E}\\!\\left[\\Delta_{s,t}\\mathbf{1}\\{\\Delta_{s,t}<\\epsilon,E_{s,t}\\}\\right]+\\sum_{t\\geq1}\\sum_{s\\in\\mathcal{S}_{t}}\\mathbb{E}\\!\\left[\\Delta_{s,t}\\mathbf{1}\\{\\Delta_{s,t}\\}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "(1) Bounding $\\begin{array}{r}{\\sum_{t\\geq1}\\sum_{s\\in{\\cal S}_{t}}\\mathbb{E}\\big[\\Delta_{s,t}{\\bf1}\\{\\Delta_{s,t}\\geq\\epsilon,E_{s,t}\\}\\big]}\\end{array}$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\sum_{t\\geq1+\\delta/2\\leq t}\\mathbb{E}\\big[\\Delta_{t+1}\\big(\\Delta_{t+\\delta},z\\in\\mathcal{E}_{s,\\delta}\\big)\\big]}\\\\ &{=\\displaystyle\\sum_{t\\geq1+\\delta/2\\leq t}\\mathbb{E}\\bigg[\\frac{\\big(Q(A_{s,t},\\theta_{*})\\big)-Q(A_{s,t},\\theta_{*})\\big)^{2}}{\\Delta_{t+\\delta}}\\big[\\Delta_{s,t}\\geq\\varepsilon,E_{s,\\delta}\\big]}\\\\ &{\\leq\\displaystyle\\sum_{t\\geq1+\\delta/2\\leq t}\\mathbb{E}\\bigg[\\frac{\\big(Q(A_{s,t},\\theta_{*})\\big)-Q(A_{s,t},\\theta_{*})\\big)+P_{s,t}\\big(A_{s,t}\\big(A_{s,t}\\big)-Q(A_{s,t},\\theta_{*})\\big)^{2}}{\\Delta_{t}}\\big[\\Delta_{s,t}\\geq\\varepsilon,E_{s,\\delta}\\big]}\\\\ &{\\leq\\displaystyle\\sum_{t\\geq1+\\delta/2\\leq t}\\mathbb{E}\\bigg[\\frac{\\big(P_{s}(A_{s,t})-Q(A_{s,t},\\theta_{*})\\big)^{2}}{\\Delta_{t}}\\big[\\Delta_{s,t}\\geq\\varepsilon,E_{s,\\delta}\\big]\\bigg]}\\\\ &{=\\displaystyle\\sum_{t\\geq1+\\delta/2\\leq t}\\mathbb{E}\\bigg[\\frac{\\big(\\sum_{t\\leq t\\leq t}\\big(\\frac{1}{\\delta}\\big(\\Delta_{t}\\big(\\Delta_{t}\\big)\\big)-Q_{s,t}\\big)+\\sqrt{2\\log_{\\frac{1}{\\delta}}\\big(|\\phi_{*}|\\big)_{\\delta}}\\big)^{2}}{\\Delta_{t}}\\big[\\Delta_{s,t}\\geq\\varepsilon,E_{s,\\delta}\\big]\\bigg]}\\\\ &{\\leq\\displaystyle\\sum_{t\\geq1+\\delta/2\\leq t}\\mathbb{E}\\bigg[\\frac{\\big(\\sum_{t\\leq t\\leq t}\\big(\\frac{1}{\\delta}\\big(\\Delta_{t}\\big)\\big)^{2}\\big)\\big(\\sum_{t\\leq t\\leq1}\\big(\\frac{1}{\\delta}\\big)\\big)^{2}}{\\Delta_{t}}\\big]\\big(\\Delta_{s,t}\\geq\\varepsilon,E_{s,\\delta}\\big)\\bigg]}\\\\ &{\\leq\\displaystyle\\sum_{t\\geq1+\\delta/2\\leq t}\\mathbb{E}\\bigg[\\frac{\\big(\\sum_{t\\leq t\\leq t}\\big(\\frac{1}{\\delta}\\big)\\big(\\sum_{t\\leq\n$$", "text_format": "latex", "page_idx": 25}, {"type": "equation", "text": "$$\n\\leq\\!\\mathbb{E}\\big[\\frac{8L\\log\\frac{1}{\\delta}}{\\Delta_{\\operatorname*{min}}^{\\epsilon}}\\sum_{t\\geq1}\\sum_{s\\in S_{t}}\\sum_{a\\in A_{s,t}}\\big\\Vert\\Phi_{a}\\big\\Vert_{\\hat{\\Sigma}_{s,t}}^{2}\\big],\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the first and the second inequality hold due to the definition of the upper confidence bound $U_{t,s}(A_{s,t})$ , the fourth inequality holds due to the Cauchy-Schwartz inequality. Utilizing the upper boundon $\\begin{array}{r}{\\sum_{t\\geq1}\\sum_{s\\in S_{t}}\\sum_{a\\in A_{s,t}}\\|\\Phi_{a}\\|_{\\hat{\\Sigma}_{s,t}}^{2}}\\end{array}$ in Proposition D1 completes the proo for the frst part. ", "page_idx": 26}, {"type": "text", "text": "(2)Bounding $\\begin{array}{r}{\\sum_{t\\geq1}\\sum_{s\\in{\\cal S}_{t}}\\mathbb{E}\\big[\\Delta_{s,t}{\\bf1}\\{\\Delta_{s,t}<\\epsilon,E_{s,t}\\}\\big]}\\end{array}$ Wetrivially have $\\begin{array}{r}{\\sum_{t\\geq1}\\sum_{s\\in{\\cal S}_{t}}\\mathbb{E}\\big[\\Delta_{s,t}{\\bf1}\\big\\{\\Delta_{s,t}<\\epsilon,E_{s,t}\\big\\}\\big]\\leq m n\\epsilon.}\\end{array}$ ", "page_idx": 26}, {"type": "text", "text": "(3) Bounding $\\begin{array}{r}{\\sum_{t\\geq1}\\sum_{s\\in\\mathcal{S}_{t}}\\mathbb{E}\\big[\\Delta_{s,t}\\mathbf{1}\\{\\bar{E}_{s,t}\\}\\big]}\\end{array}$ ", "page_idx": 26}, {"type": "text", "text": "Note that $\\theta_{s,*}-\\mu_{q}\\sim\\mathcal{N}(0,\\hat{\\Sigma}_{s,1})$ $\\begin{array}{r}{,\\hat{\\Sigma}_{s,1}),\\mathrm{and}\\;\\mathbb{E}\\big[\\|\\theta_{s,*}-\\mu_{q}\\|_{\\hat{\\Sigma}_{s,1}^{-1}}\\big]\\leq\\sqrt{\\mathbb{E}\\|\\hat{\\Sigma}_{s,1}^{-\\frac{1}{2}}(\\theta_{s,*}-\\mu_{q})\\|_{2}^{2}}=\\sqrt{d}}\\end{array}$ Then according to $[35,\\,\\mathrm{Exp}\\,\\,2.11]$ , we have with robabity $1-\\zeta,\\,\\|\\theta_{s,*}-\\mu_{q}\\|_{\\hat{\\Sigma}_{s,1}^{-1}}\\leq\\sqrt{d+\\sqrt{8d\\ln\\frac{1}{\\zeta}}}$ Therefore, with probability $1-\\zeta$ over the draw of $\\{\\theta_{s,*}\\}_{s\\in[m]}$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta_{s,t}=g(A_{s,*},\\theta_{s,*})-g(A_{s,t},\\theta_{s,*})}\\\\ &{\\quad=\\displaystyle\\sum_{a\\in A_{s,*}}\\langle\\Phi_{a},\\theta_{s,*}\\rangle-\\displaystyle\\sum_{a\\in A_{s,t}}\\langle\\Phi_{a},\\theta_{s,*}\\rangle}\\\\ &{\\quad\\le\\displaystyle\\sum_{a\\in A_{s,*}}\\|\\Phi_{a}\\|\\cdot\\|\\theta_{s,*}\\|+\\displaystyle\\sum_{a\\in A_{s,t}}\\|\\Phi_{a}\\|\\cdot\\|\\theta_{s,*}\\|}\\\\ &{\\quad\\le2L B\\|\\theta_{s,*}\\|}\\\\ &{\\quad1-\\zeta}\\\\ &{\\quad\\le2L B\\sqrt{\\lambda_{1}(\\Sigma_{0}+\\Sigma_{q})}\\big(\\sqrt{d+\\sqrt{8d\\ln\\frac{1}{\\zeta}}}+\\|\\mu_{q}\\|_{\\widehat{\\Sigma}_{s,1}^{-1}}\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the first inequality holds due to the Schwartz inequality. Then we have with probability $1-\\zeta$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t\\geq1}\\sum_{s\\in\\mathcal{S}_{t}}\\mathbb{E}\\big[\\Delta_{s,t}\\mathbf{1}\\{\\tilde{E}_{s,t}\\}\\big]}\\\\ &{=\\displaystyle\\sum_{t\\geq1}\\sum_{s\\in\\mathcal{S}_{t}}\\mathbb{E}\\big[\\mathbb{E}\\big[\\Delta_{s,t}\\mathbf{1}\\{\\tilde{E}_{s,t}\\}|H_{t}\\big]}\\\\ &{\\leq2L B\\sqrt{\\lambda_{1}(\\Sigma_{0}+\\Sigma_{q})}(\\sqrt{d+\\sqrt{8d\\ln\\frac{1}{\\zeta}}}+\\|\\mu_{q}\\|_{\\tilde{\\Sigma}_{s,1}})\\displaystyle\\sum_{t\\geq1}\\sum_{s\\in\\mathcal{S}_{t}}\\mathbb{E}\\big[\\mathbf{1}\\{\\tilde{E}_{s,t}\\}|H_{t}\\big]}\\\\ &{=2L B\\sqrt{\\lambda_{1}(\\Sigma_{0}+\\Sigma_{q})}(\\sqrt{d+\\sqrt{8d\\ln\\frac{1}{\\zeta}}}+\\|\\mu_{q}\\|_{\\tilde{\\Sigma}_{s,1}})\\displaystyle\\sum_{t\\geq1}\\sum_{s\\in\\mathcal{S}_{t}}\\mathbb{E}\\big(\\tilde{E}_{s,t}|H_{t}\\big)}\\\\ &{\\leq4L B\\sqrt{\\lambda_{1}(\\Sigma_{0}+\\Sigma_{q})}(\\sqrt{d+\\sqrt{8d\\ln\\frac{1}{\\zeta}}}+\\|\\mu_{q}\\|_{\\tilde{\\Sigma}_{s,1}})m n K\\delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Combining the results in (1) (2) and (3) finishes the whole proof. ", "page_idx": 26}, {"type": "text", "text": "E  Technical Overview and Limitations of this Work ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In this section, we explain our technical novelties for deriving near-optimal sequential regret bound for HierTS and logarithmic sequential regret bound for HierBayesUCB as follow, when compared with the latest bound in [17] (More detailed explanations can be found in Table 4): ", "page_idx": 26}, {"type": "text", "text": "(1) The Technical Overview for Deriving Near-Optimal Regret Bound in Theorem 5.1. The biggest novelty lies in bounding each term $\\mathbb{E}\\big[(\\theta_{s,*}\\,\\bar{-}\\,\\hat{\\mu}_{s,t})^{\\top}\\bar{A_{s,*}}\\big|H_{t}\\big]$ in Bayes regret $B\\mathcal{R}(m,n)$ Existing work [17] chose Cauchy-Schwartz inequality to directly bound $(\\theta_{s,*}\\,-\\,\\hat{\\mu}_{s,t})^{\\top}A_{s,*}\\;\\leq$ $\\begin{array}{r}{\\|\\theta_{s,*}\\,-\\,\\hat{\\mu}_{s,t}\\|_{\\hat{\\Sigma}_{s,t}^{-1}}\\|A_{s,*}\\|_{\\hat{\\Sigma}_{s,t}}}\\end{array}$ , used UCB technique to bound $\\|\\theta_{s,*}\\,-\\,\\hat{\\mu}_{s,t}\\|_{\\hat{\\Sigma}_{s,t}^{-1}}$ (which caused an additional multiplicative factor $\\log{\\frac{1}{\\delta}}$ ), leveraged the fact that $A_{s,*}|H_{t}\\stackrel{1.1.0.}{\\sim}A_{s,t}|H_{t}$ to transform $\\sum_{t,s}\\|A_{s,*}\\|_{\\hat{\\Sigma}_{s,t}}$ into $\\nu_{m,n}$ , and obtained an intermediate regret upper bound $\\sqrt{m n\\mathcal{V}_{m,n}\\log\\left(1/\\delta\\right)}$ Instead of using UCB technique, our Theorem 5.1 applies a novel Cauchy-Schwartz type inequality (i.e. Lemma A.2) to bound $\\mathbb{E}(\\theta_{s,*}-\\hat{\\mu}_{s,t})^{\\top}A_{s,*}\\le\\sqrt{d\\mathbb{E}\\big((\\theta_{s,*}-\\hat{\\mu}_{s,t})^{\\top}A_{s,t}\\big)^{2}}\\approx\\sqrt{d}\\|A_{s,t}\\|_{\\hat{\\Sigma}_{s,t}},$ and finally achieves the regret bound $\\sqrt{m n\\nu_{m,n}}$ , removing the $\\sqrt{\\log\\left(1/\\delta\\right)}$ factor. Besides, when bounding the posterior variance $\\nu_{m,n}$ , we use a different matrix analysis to prevent variance terms (e.g.. $\\sigma^{2},\\lambda_{1}(\\Sigma_{0}))$ solely appearing in the denominator of regret bound. Moreover, we employ a matrix decomposition technique (in our Lemma A.1) to reduce the multiplicative factor $\\kappa^{2}(\\dot{\\Sigma}_{0})\\dot{)}$ in [17, Theorems 3-4] to $\\kappa(\\Sigma_{0})$ in our bounds (see more details in Table 4). ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "(2) The Technical Overview for Deriving Logarithmic Regret Bound in Theorem 5.2. To obtain sharper sequential regret bound than the near-optimal regret bound in Theorem 5.1, our Theorem 5.2 chooses the Bayes regret decomposition strategy shown above Theorem 5.2, uses UCB technique to bound the first term in the regret decomposition as $\\begin{array}{r}{\\sum_{t,s}\\mathbb{E}\\Delta_{s,t}\\mathbf{1}\\{\\Delta_{s,t}\\}\\geq\\epsilon,E_{s,t}\\}\\leq\\mathcal{V}_{m,n}\\log\\frac{1}{\\delta}}\\end{array}$ and finally combines the upper bound on posterior variance $\\nu_{m,n}$ in Eq. (4) to achieve a logarithmic Bayes regret upper bound of $(\\log{\\frac{1}{\\delta}})m d\\log{\\frac{n}{d}}$ ", "page_idx": 27}, {"type": "text", "text": "Nevertheless, we also need to point out the limitations of our Bayes regret bounds: ", "page_idx": 27}, {"type": "text", "text": "The Limitations of the Multi-Task Bayes Regret Bounds. Honestly speaking, our regret bounds have two main limitations, because they are: (i) Not advantageous when compared with single-task regret bound. This is because our Bayes regret bounds (e.g. $O(m{\\sqrt{n\\log n}})$ in Theorem 5.1) for hierarchical Bayesian bandit problem are almost the same as the summation of regret bounds of learning $m$ Bayesian bandit task independently. This is also the limitation of existing bounds in this field (see [25, 7, 17]). (ii) Unable to shed more light on the advantages of multi-task bandit optimization. The existing regret bound $O(m{\\sqrt{n k}})$ for multi-task representation which demonstrated that multi-task regret bound can be smaller for learning a low-dimensional representation (i.e. $k<<d)$ than the regret bound of $O(m{\\sqrt{n d}})$ for learning each task independently, and the existing regret bound $O(m{\\sqrt{n\\log\\left(1+n V\\right)}})$ for multi-task adversarial linear bandit which proved that the regret bound decreases with more similarity (i.e. smaller $V$ ) among bandit tasks. Our hierarchical Bayesian bandit model has assumed that different bandit instances are sampled the same metadistribution, and hence fails to reveal the influence of task similarity to the multi-task Bayes regret. ", "page_idx": 27}, {"type": "text", "text": "Remark E.1 (The Underlying Causes for the Limitation of Multi-Task Bayes Regret Bound.) The underlying causes for the shortcoming of the multi-task Bayes regret bound is that the upper bound $\\begin{array}{r}{\\gamma_{m,n}\\,=\\,\\mathbb{E}\\sum_{t\\geq1}\\sum_{s\\in S_{t}}\\|A_{s,t}\\|_{\\hat{\\Sigma}_{s,t}}^{2}}\\end{array}$ maybenottightenough.Detailed explanationslieinthefollowingthreeaspects: ", "page_idx": 27}, {"type": "text", "text": "$^{(I)}$ Recall that in the proof for our Theorem 5.1,we can upper bound the multi-task Bayes regret as $\\mathcal{B}\\mathcal{R}(m,n)\\leq\\sqrt{m n d}\\sqrt{\\mathscr{V}_{m,n}}$ Then inProposition $A.l$ we use a purely algebraic technique to bound theposterior-variance $V_{m,n}\\leq O(m\\log n)$ resulting in the final Bayes regret bound of ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\sqrt{m n d}\\sqrt{V_{m,n}}=O(\\sqrt{m n d}\\sqrt{m\\log n})=O(m\\sqrt{n\\log n}),\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "which is almost the same as the summation of the regret bounds for learning m bandit tasks independently.Thereforeifwe canupperbound theposterior-variance $V_{m,n}$ with a bound that is sublinear with respect to m and logarithmic w.r.t. n (e.g. a bound of $O({\\sqrt{m}}\\log n),$ ), then the final Bayes regret bound willbe much sharper. The upper bounds on the posterior-variance $V_{m,n}$ in existing works (e.g. see [25, 7, 17] in our Table $^{\\,I}$ in the main text) are also obtained via purely algebraic techniques and arenot sharp either(orevenworse). ", "page_idx": 27}, {"type": "text", "text": "(2) In _the proof for Proposition A.l, we only give the worst-case upper bound on $\\begin{array}{r}{\\sum_{t\\geq1}\\sum_{s\\in S_{t}}\\overline{{\\|A_{s,t}\\|_{\\hat{\\Sigma}_{s,t}}^{2}}}}\\end{array}$ via purely algebraic technique (thus leading to a worst-case upper bound on the posterior variance $\\begin{array}{r}{\\mathbb{E}\\sum_{t\\geq1}\\sum_{s\\in S_{t}}\\|A_{s,t}\\|_{\\hat{\\Sigma}_{s,t}}^{2})}\\end{array}$ .Such worst-case upper bound is obtained via purely algebraic techniques, ignoring the expectation over the randomness of $A_{s,t}$ and $\\hat{\\Sigma}_{s,t}$ Therefore, we may achieve sharper regret bound by considering the expectation over the randomness in the posterior variance. ", "page_idx": 27}, {"type": "text", "text": "(3) To derive a sharper and meaningful upper bound on the posterior-variance $\\begin{array}{r l}{\\nu_{m,n}}&{{}=}\\end{array}$ $\\begin{array}{r}{\\mathbb{E}\\sum_{t\\geq1}\\sum_{s\\in S_{t}}\\|A_{s,t}\\|_{\\hat{\\Sigma}_{s,t}}^{2}}\\end{array}$ equality,or more technical matrix analysis,to achieve an upper bound that is sublinear w.r.t.the number m of tasks and sublinear w.r.t. the number n of iterations per task. Only in this way can we obtainamulti-taskregretbound $o(m n)$ that is sublinear w.r.t. m and sublinear w.r.t. n. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "(4) On the other hand, we also consider finding the lower bound of posterior-variance $V_{m,n}$ to show that our upperbound on $V_{m,n}$ is tight, or finding the lower bound of multi-task Bayes regret $B\\mathcal{R}(m,n)$ to show that our multi-task Bayes regret upper bound could not be improved. This serves as one of our ongoingresearch directions. ", "page_idx": 28}, {"type": "text", "text": "F  Additional Experiments and Computer Resources ", "text_level": 1, "page_idx": 28}, {"type": "image", "img_path": "joNPMCzVIi/tmp/a9f5f2eed14f947c0f37c3d020ce4ebd4fc0bb67d8af760dfa989bc9a519bbee.jpg", "img_caption": [], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "Experimental Results. From Figure 2, we have the similar observations as that in Figure 1: (1) In plot (a), the multi-task regret of HierBayesUCB becomes larger with the increase of $m$ and $n$ , which is consistent with our regret upper bound in Theorems 5.2. (2) In plot (b), the regret increases with a higher dimension $d$ , and increases with a larger number $L$ of the concurrent tasks. (3) In plots (c)-(e), the regret decreases with a smaller variance (e.g. $\\sigma_{q}$ \uff0c $\\sigma_{0}$ and $\\sigma$ ) in hierarchical Bayesian model, validating the provable benefits of variance-reduction in Bayes regret minimization. (4) The task-averaged regret of our proposed HierBayesUCB is smaller than that of HierTS, and such improvement becomes larger with the increase of $\\sigma_{q}$ (when compared with $\\sigma_{q}=1.0$ in Figure 1 (f)). ", "page_idx": 28}, {"type": "text", "text": "Computer Resources. Our implementations are based on Python. We run all bandit algorithms on a platform with 8 NVIDIA RTX 6000 GPUs and 2 AMD EPYC 7543 Processors. Each GPU has 48G memory, and each CPU has 64 cores. The CUDA version is 12.1, the Python version 3.7.16, the matplotlib version 3.5.3, and the tensorflow version 1.15. The source code for reproducing all experimental results of HierTS and HierBayesUCB is provided in the supplementary material. ", "page_idx": 28}, {"type": "table", "img_path": "joNPMCzVIi/tmp/88000dcb9b8c0c72612135fea52c36b9a88061a204c61a59222d6291491930f1.jpg", "table_caption": [""], "table_footnote": [], "page_idx": 29}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Claims ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: see Section 5. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u00b7The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u25cf It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 30}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: see Section E. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u25cf The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should refect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 30}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: see assumptions in Section 5, and see proof sketch in Section E. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u25cf The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 31}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: see implementation details in Section 6 and Section F, and see the source code in the supplementary material. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. () If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 31}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: see the source code in our supplementary material. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 32}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: see the implementation details in Section 6. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 32}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: see our Figures 1-2. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u25cf The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u25cf The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative errorrates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce theexperiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: see details in Section F. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u25cf The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 33}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: this paper is a purely theoretical paper and has no negative social impact. Besides, we release the source code to implement our proposed algorithms in the supplementary material. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u25cf The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u25cf The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 33}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: there is no societal impact of the work performed. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u25cf The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u25cf The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: the paper poses no such risks. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 34}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properlyrespected? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: the paper does not use existing assets. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u25cf For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 34}, {"type": "text", "text": "\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 35}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: the paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose asset isused.   \n\u25cf At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 35}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: the paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 35}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: the paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u25cf The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u25cf Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 35}]