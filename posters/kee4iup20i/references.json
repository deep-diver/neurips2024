{"references": [{"fullname_first_author": "Lili Yu", "paper_title": "MegaByte: Predicting Million-byte Sequences with Multiscale Transformers", "publication_date": "2023", "reason": "This paper introduces MegaByte, a byte-level autoregressive language model which is directly compared against in the current paper's experiments, and is a major source of comparison and inspiration for the SpaceByte architecture."}, {"fullname_first_author": "Junxiong Wang", "paper_title": "MambaByte: Token-free Selective State Space Model", "publication_date": "2024", "reason": "This paper introduces MambaByte, another significant byte-level autoregressive language model that is directly compared against in the current paper's experiments, and is a major source of comparison and inspiration for the SpaceByte architecture."}, {"fullname_first_author": "Alec Radford", "paper_title": "Language models are unsupervised multitask learners", "publication_date": "2019", "reason": "This paper is highly influential in the field of large language models, introducing the GPT-2 tokenizer which is used as a baseline in the current paper's experiments, and is a foundational paper for much of the research in this area."}, {"fullname_first_author": "Taku Kudo", "paper_title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing", "publication_date": "2018", "reason": "This paper introduces SentencePiece, another subword tokenizer used as a baseline in the current paper's experiments, and is a foundational paper for much of the research in this area."}, {"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017", "reason": "This paper is highly influential in the field of large language models, introducing the Transformer architecture which is used as a baseline in the current paper's experiments, and is a foundational paper for much of the research in this area."}]}