[{"figure_path": "y8Rm4VNRPH/tables/tables_6_1.jpg", "caption": "Table 1: Results on the synthetic MAD benchmark. Results other than DeltaNet are directly borrowed from Poli et al. [82]. (Multi-head) Hyena, DeltaNet and Mamba make use of convolutions, whereas GLA does not.", "description": "This table presents the results of different models on the MAD benchmark, a suite of synthetic token manipulation tasks designed to test various aspects of model architectures, including recall ability, compression, and noise robustness.  The table compares the performance of DeltaNet against several strong baselines, notably highlighting the use of convolutions in some models but not others.  It shows DeltaNet's competitive performance, particularly excelling in Fuzzy Recall, while indicating areas where it may underperform others.", "section": "4.1 Synthetic Benchmarks"}, {"figure_path": "y8Rm4VNRPH/tables/tables_6_2.jpg", "caption": "Table 2: Main language modeling results against Transformer++, RetNet [105], Mamba [30], and GLA [116]. All models are trained on the same subset of the SlimPajama dataset with the Mistral tokenizer. The Transformer++, RetNet, Mamba, GLA (w/o. conv) results are taking from Yang et al. [116]. For hybrid models, \"Sliding Attn\" interleaves a sliding window attention every other layer, and \"Global Attn\" uses full global attention on two layers. The 340M/1.3B models are trained for 15B/100B tokens respectively. All results are obtained through lm-evaluation-harness [26]. The last column denotes the expansion ratio of the recurrent state size relative to the product of the number of layers and model dimension (see Zhang et al. [122, App. C]).", "description": "This table presents the main language modeling results, comparing DeltaNet against several strong baselines including Transformer++, RetNet, Mamba, and GLA.  It shows perplexity and zero-shot performance on various downstream tasks for two model sizes (340M and 1.3B parameters). The table also includes results for hybrid models incorporating sliding-window or global attention layers, highlighting the impact of these additions.  Finally, ablation studies on DeltaNet are included, varying normalization and activation functions.", "section": "4.2 Language Modeling"}, {"figure_path": "y8Rm4VNRPH/tables/tables_7_1.jpg", "caption": "Table 3: Zero-shot model performance across selected benchmarks for 3B models. Llama-3.2-3B and PowerLM-3B are Transformer models, while the others are recurrent models.ARC results are averaged over accuracy and normalized accuracy across ARC-Easy and ARC-Challenge.", "description": "This table compares the zero-shot performance of several 3B parameter language models on various benchmark tasks.  The benchmarks assess different capabilities including commonsense reasoning, question answering, and knowledge-intensive tasks.  The models compared include both transformer-based architectures and recurrent neural network (RNN)-based models.  The table highlights the relative strengths and weaknesses of different architectural approaches in zero-shot settings.", "section": "4.2 Language Modeling"}, {"figure_path": "y8Rm4VNRPH/tables/tables_8_1.jpg", "caption": "Table 4: Overview of recent linear recurrent models that have been proposed and applied to autoregressive language modeling (ordered in rough chronological order). These works make use of a matrix-valued hidden state St \u2208 Rdxn (or two matrix-valued hidden states St, Si, e.g., [80, 122]) updated through an associative recurrence followed by an outer-product-based addition. Here is the Hadamard product. Some models make use of an additional linear RNN with hidden state vector zt, which used to normalized the query vector qt. Variables with the subscript t (e.g., vt, at, ft, t) are (potentially non-linear) functions of the current input xt. Non-time-varying parameters (e.g., A, d, \u03b3) are denoted without subscripts; these parameters are either learned or set to fixed values. Matrices are denoted with bold upper case letters, vectors with bold lower case, and scalars with italic letters. Many models make use of a kernel $ (e.g., [98, 79]) but we subsume them into the key/value vectors to reduce notational clutter.", "description": "This table summarizes different linear recurrent models used for autoregressive language modeling.  It compares the recurrence relation and memory read-out mechanism for each model, highlighting the use of matrix-valued hidden states and associative operators. The table also notes the inclusion of kernels and normalization in some models, indicating variations in implementation details.", "section": "5.1 DeltaNet vs. State Space Models / Linear RNNS"}]