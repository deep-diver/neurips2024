[{"figure_path": "oDeqjIM9Sk/figures/figures_5_1.jpg", "caption": "Figure 1: Optimization by gradient descent of two 5-by-5 matrices A, B on the L2-regularized loss ||ABT - D||\u00b2 + \u03bb(||A||\u00b2 + ||B||\u00b2), where D = diag(0.2,0.4, 0.6, 0.8, 1), with various regularization strength \u03bb. t denotes the number of optimization steps. Left: difference between the nuclear norm ||ABT||* with the Frobenius norm ||A||\u00b2 +||B||\u00b2 throughout optimization. For all cases, other than \u03bb = 0, the trajectory converges exponentially quickly to 0 as predicted by our theory. Center left: Norm of the discrepancy between AT A and BTB over training steps. As predicted the discrepancy exponentially vanishes, with a time constant proportional to the \u03bb. Center right: Singular values of the matrix ABT at t = 1000, for various regularization strength \u03bb. As predicted, si decays linearly with \u03bb, until \u03bb \u2265 si, at which point the singular value vanishes. Right: Singular values of the matrix ABT during optimization, for \u03bb = 0.4.", "description": "This figure empirically validates the theoretical findings of the paper regarding the equivalence between L2-regularized and nuclear norm-regularized losses when optimizing with factorized parameter matrices (W=ABT).  It shows how the difference between the Frobenius norm (||A||\u00b2 + ||B||\u00b2) and the nuclear norm (||ABT||*) of the matrix product decreases exponentially fast during training as the regularization strength (\u03bb) increases. The plots illustrate this convergence, the vanishing discrepancy between ATA and BTB, the linear decay of singular values with \u03bb, and the overall behavior of singular values during optimization.  All of these observations support the claim that L2 regularization implicitly enforces low-rank solutions in the factorized setting.", "section": "3.4 Case study: 2-layer linear network"}, {"figure_path": "oDeqjIM9Sk/figures/figures_6_1.jpg", "caption": "Figure 2: Left: The rank of weight matrix product PWv of the first layer of a 2-layer Transformer trained on the associative recall task, during training, with AdamW, for various decay strengths. To better account for the effect of weight decay on the attention layers, only the decay strength applied to attention layers is varied, while the strength for all other layers is fixed at 0.1. We observe that rank reduction correlates strongly with weight decay strength. Center: Norm of the discrepancy between PTP and WvWv, during training. As predicted, the difference seems to converge to 0 when \u03bb > 0 towards the end of training. While for AdamW we no longer have the guarantee of an exponential decay, we see that the discrepancy nonetheless vanishes quickly, with a time constant which perfectly correlates with the decay strength. Right: The difference of the nuclear norm of PWv with the Frobenius norm upper bounding it. As the discrepancy between PTP and WvWv decreases, the difference approaches 0, and thus the bound becomes tight. The optimization of L<sub>L2</sub> thus gradually switches to that of L*, explaining the rank regularization.", "description": "This figure empirically validates the theoretical findings of the paper.  It shows the effect of weight decay (\u03bb) on the rank of the matrix product PWv in a 2-layer Transformer. The left panel shows a strong correlation between increasing weight decay and decreasing rank. The center panel demonstrates that the discrepancy between ||P<sup>T</sup>P|| and ||W<sub>v</sub><sup>T</sup>W<sub>v</sub>|| decreases exponentially fast with increasing weight decay. The right panel shows that the difference between the nuclear norm and the Frobenius norm approaches zero with increasing weight decay, indicating a transition from L<sub>L2</sub> optimization to L* optimization which explains the observed rank regularization.", "section": "4 Empirical results"}, {"figure_path": "oDeqjIM9Sk/figures/figures_7_1.jpg", "caption": "Figure 3: Left, center left: The rank of weight matrix products WKWQ and PWv averaged across heads of layer 5 of an autoregressive transformers trained on the Pile [Gao et al., 2020]. Center right, right: The rank of weight matrix products WWQ and PWv averaged over all heads and all layers of a Vision Transformer trained following [Irandoust et al., 2022] on the ImageNet dataset [Deng et al., 2009]. In both settings, the decay strength applied to attention layers is varied, while keeping the strength for all other layers fixed. In all cases, we observe again that rank reduction correlates strongly with weight decay strength when optimizing with AdamW. The weight decay strength of 0.1 commonly used to pretrain some known large foundation models in fact noticeably reduces the rank of the generated matrices compared to when weight decay is turned off.", "description": "This figure shows the impact of weight decay on the rank of attention matrices in two different types of transformer models: an autoregressive transformer and a vision transformer.  The left and center-left panels display the rank of attention weight matrices (WKWQ and PWv) across heads in layer 5 of an autoregressive transformer trained on the Pile dataset.  The center-right and right panels show the average rank of the same matrices across all heads and layers of a vision transformer trained on the ImageNet dataset. In both experiments, the weight decay strength applied to the attention layers was varied while keeping the strength for other layers constant. The results clearly demonstrate that increasing weight decay strength reduces the rank of the attention weight matrices, consistent across both model architectures.", "section": "4 Empirical results"}, {"figure_path": "oDeqjIM9Sk/figures/figures_8_1.jpg", "caption": "Figure 4: Analyses of attention layers in the pretrained LLAMA 2 model with 7 Billion parameters [Touvron et al., 2023]. The leftmost (resp. center left) shows the squared norm of every row of WQ (resp. Wv), for the first head of each layer, against the norm of the corresponding row of WK (resp. column of P). The condition WKWK = WQWQ would require these norms to be equal, which in fact is mostly true. While the model has not reached a stationary point, this indicates the optimization has advanced enough for this sufficient condition for L\u2217 to be identical to LL2 to emerge. In fact, the center right (resp. rightmost) plot show the scatter plot mapping the Frobenius norm against the nuclear norm for all heads across all layers. The two norms almost perfectly coincide.", "description": "This figure analyzes attention layers in the pretrained LLAMA 2 model to show empirical evidence supporting the theoretical findings of the paper.  The plots compare norms of weight matrices (WQ, WK, Wv, P) from the attention mechanism to demonstrate the equivalence between Frobenius and nuclear norms, suggesting the weight decay regularization implicitly induces low-rank attention layers as predicted by the theoretical analysis.", "section": "4.4 Pretrained foundation models"}, {"figure_path": "oDeqjIM9Sk/figures/figures_23_1.jpg", "caption": "Figure 5: Trajectory of w\u2081, w\u2082 in the 2D plane when optimizing the underlying parameter for various hyperparameters. At every coordinate in the plane, the loss is defined as the squared distance to the surface S in orange. The red (resp. blue) cross represents the points on S minimizing the L2-norm (resp. L1-norm). Left: w\u2081, w\u2082 are directly parametrized and optimized by AdamW with decoupled weight decay (in solid line) or Adam with L2-regularization (in dotted line). As conjectured, the convergence point of AdamW given the hyperparameter \u03b5 and decay strength \u03bbwd corresponds to that of the equilibrium point of the L2-regularized loss with regularization strength \u03bbL2 = \u03bbwde. Right: w\u2081, w\u2082 are parameterized as a product of two scalars, i.e. w\u2081 = a\u2081b\u2081, w\u2082 = a\u2082b\u2082, where a\u2081, b\u2081, a\u2082, b\u2082 are now optimized by AdamW or Adam with L2 regularization. Again, the two optimizers find the same convergence point for equivalent hyperparameters. However, the solution found now corresponds to those of the loss regularized by the L1-norm of w\u2081, w\u2082, (corresponding to the nuclear norm for scalars) as predicted.", "description": "This figure shows the trajectory of two parameters (w1 and w2) during optimization using two different methods: AdamW with decoupled weight decay and Adam with L2-regularization.  The left panel shows the case where the parameters are directly optimized. The right panel shows the case where the parameters are factorized as products of two other scalars.  In both cases, the optimization path is shown for various regularization strengths. The figure demonstrates that the optimization methods converge to the same point for equivalent regularization strengths, highlighting the relationship between L2 and L1 regularization in the context of parameter factorization.", "section": "3.4 Case study: 2-layer linear network"}, {"figure_path": "oDeqjIM9Sk/figures/figures_23_2.jpg", "caption": "Figure 4: Analyses of attention layers in the pretrained LLAMA 2 model with 7 Billion parameters [Touvron et al., 2023]. The leftmost (resp. center left) shows the squared norm of every row of WQ (resp. Wv), for the first head of each layer, against the norm of the corresponding row of WK (resp. column of P). The condition WKWK = WQWQ would require these norms to be equal, which in fact is mostly true. While the model has not reached a stationary point, this indicates the optimization has advanced enough for this sufficient condition for L\u2217 to be identical to LL2 to emerge. In fact, the center right (resp. rightmost) plot show the scatter plot mapping the Frobenius norm against the nuclear norm for all heads across all layers. The two norms almost perfectly coincide.", "description": "This figure provides empirical evidence supporting the theoretical findings of the paper. It shows the analysis of attention layers in the pretrained LLAMA 2 model.  The plots demonstrate that the Frobenius norm and the nuclear norm of the attention weight matrices are nearly identical, indicating that the weight decay regularization implicitly induces low-rank solutions, as predicted by the theory.", "section": "Pretrained foundation models"}, {"figure_path": "oDeqjIM9Sk/figures/figures_24_1.jpg", "caption": "Figure 7: The rank of weight matrix products WKWQ and PWv averaged across heads of layer 7 (left and outer left) and layer 9 (right and outer right) of an autoregressive transformers trained on the Pile [Gao et al., 2020]. For both layers, the decay strength applied to attention layers is varied, while keeping the strength for all other layers fixed. In all cases, we observe again that rank reduction correlates strongly with weight decay strength when optimizing with AdamW.", "description": "This figure shows the impact of weight decay on the rank of weight matrices in attention layers of autoregressive transformers. The rank of  WKWQ and PWv (products of weight matrices in attention layers) is measured across different layers (layer 7 and 9) and various weight decay strengths (\u03bb). The results indicate a strong correlation between weight decay strength and rank reduction, confirming the rank-regularizing effect of weight decay, especially when using the AdamW optimizer.", "section": "4.2 Language Modelling"}]