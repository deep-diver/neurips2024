[{"figure_path": "oDeqjIM9Sk/tables/tables_8_1.jpg", "caption": "Table 1: Test set perplexity of 125 million Transformer models trained on the Pile for 10 billion tokens with AdamW and different weight decays \u03bb for the self-attention (SA) and the feed-forward (MLP) weights. \u00b1 standard error of the mean computed over 5 seeds.", "description": "This table presents the test set perplexity results for various Transformer models trained on the Pile dataset.  The models were trained using the AdamW optimizer with different weight decay values (\u03bb) applied separately to the self-attention (SA) and Multi-Layer Perceptron (MLP) layers.  The table shows the impact of varying weight decay strength on model performance, indicating an optimal range and revealing potential differences in sensitivity between the SA and MLP layers.  The values reported are averages across five separate training runs.", "section": "4.2 Language Modelling"}, {"figure_path": "oDeqjIM9Sk/tables/tables_24_1.jpg", "caption": "Table 1: Test set perplexity of 125 million Transformer models trained on the Pile for 10 billion tokens with AdamW and different weight decays \u03bb for the self-attention (SA) and the feed-forward (MLP) weights. \u00b1 standard error of the mean computed over 5 seeds.", "description": "This table presents the test set perplexity results for various transformer models trained on the Pile dataset.  Different weight decay values (\u03bb) were applied separately to the self-attention (SA) and the feed-forward (MLP) layers of the models. The results are averages over five separate training runs, with standard errors included to show variability.", "section": "4.2 Language Modelling"}]