[{"figure_path": "vU512K8vrR/tables/tables_4_1.jpg", "caption": "Table 1: Experimental results of SalientLoRA and other baselines on the GLUE benchmark across varying parameter budgets. The bold scores indicate the best results. We report the average performance over 5 runs using different random seeds, with SalientLoRA significantly better than AdaLoRA and DoRA with p-value < 0.05 based on paired t-test.", "description": "This table presents the experimental results comparing SalientLoRA's performance against other parameter-efficient fine-tuning methods (AdaLoRA, DoRA, LoRA, Adapter, AdapterFusion, SORA) on the GLUE benchmark.  Results are shown for different parameter budgets and include multiple metrics (accuracy, Matthews correlation coefficient, Spearman correlation coefficient) relevant to the various GLUE tasks.  Statistical significance (p-value < 0.05) is noted where SalientLoRA outperforms AdaLoRA and DoRA.  The average performance across five runs with different random seeds is shown.", "section": "6.2 Natural Language Understanding"}, {"figure_path": "vU512K8vrR/tables/tables_6_1.jpg", "caption": "Table 1: Experimental results of SalientLoRA and other baselines on the GLUE benchmark across varying parameter budgets. The bold scores indicate the best results. We report the average performance over 5 runs using different random seeds, with SalientLoRA significantly better than AdaLoRA and DoRA with p-value < 0.05 based on paired t-test.", "description": "This table presents the performance comparison between SalientLoRA and other parameter-efficient fine-tuning methods on the GLUE benchmark.  It shows the average performance across eight datasets (CoLA, SST-2, MRPC, QQP, STS-B, MNLI, QNLI, RTE) for two different parameter settings, using various metrics (accuracy, Matthews Correlation Coefficient, Spearman's Correlation Coefficient). The results demonstrate SalientLoRA's superiority over other methods and its statistical significance.", "section": "6.2 Natural Language Understanding"}, {"figure_path": "vU512K8vrR/tables/tables_7_1.jpg", "caption": "Table 2: Performance comparison of different fine-tuning methods on NLG tasks. The three metrics on each dataset are ROUGE 1/2/L scores.", "description": "This table compares the performance of several parameter-efficient fine-tuning methods on two natural language generation (NLG) tasks: text summarization using the XSum and CNN/DailyMail datasets.  The methods compared include Full Fine-Tuning (Full FT), LoRA, AdaLoRA, SoRA, DoRA, and SalientLoRA.  The table shows the number of parameters used by each method and the ROUGE-1, ROUGE-2, and ROUGE-L scores achieved on each dataset.  ROUGE scores are commonly used metrics to evaluate the quality of text summarization.", "section": "6.3 Natural Language Generation"}, {"figure_path": "vU512K8vrR/tables/tables_7_2.jpg", "caption": "Table 3: The average score on MT-Bench and trainable parameter count of LLaMA-7B after instruction tuning by different fine-tuning methods.", "description": "This table presents the performance comparison of different fine-tuning methods on the LLaMA-7B model for instruction tuning.  The methods compared include Full Fine-tuning (Full FT), LoRA, AdaLoRA, SoRA, DoRA, and SalientLoRA.  The table shows the number of trainable parameters used by each method and the average score achieved on the MT-Bench benchmark.  SalientLoRA demonstrates the highest score among all methods.", "section": "6.4 Instruction Tuning"}, {"figure_path": "vU512K8vrR/tables/tables_8_1.jpg", "caption": "Table 4: The results of ablation experiments. Here, \u2193 represents the performance declines of variants.", "description": "This table presents the results of ablation experiments conducted to evaluate the impact of two key components in SalientLoRA's salience measurement: orthogonality-aware singular value magnitudes (OAM) and the influence domain (ID).  The table shows the performance of SalientLoRA with both components, with only OAM, and with only ID on eight datasets from the GLUE benchmark. The performance drop (\u2193) indicates the decrease in performance compared to the full SalientLoRA model, highlighting the contribution of each component to the overall performance.", "section": "6.5 Ablation Results"}, {"figure_path": "vU512K8vrR/tables/tables_13_1.jpg", "caption": "Table 1: Experimental results of SalientLoRA and other baselines on the GLUE benchmark across varying parameter budgets. The bold scores indicate the best results. We report the average performance over 5 runs using different random seeds, with SalientLoRA significantly better than AdaLoRA and DoRA with p-value < 0.05 based on paired t-test.", "description": "This table presents a comparison of the performance of SalientLoRA against other parameter-efficient fine-tuning methods on the GLUE benchmark.  Results are shown for different parameter budget sizes (number of parameters), demonstrating the impact on performance metrics (accuracy, Matthews correlation coefficient, Spearman correlation coefficient) across the eight GLUE datasets.  Statistical significance (p-value < 0.05) is reported for the comparison between SalientLoRA and other methods.", "section": "6.2 Natural Language Understanding"}, {"figure_path": "vU512K8vrR/tables/tables_13_2.jpg", "caption": "Table 6: The statistics of XSum and CNN/DailyMail datasets.", "description": "This table shows the number of training, testing, and development samples for the XSum and CNN/DailyMail datasets used in the paper's natural language generation experiments.", "section": "6.1 Experimental Settings"}, {"figure_path": "vU512K8vrR/tables/tables_13_3.jpg", "caption": "Table 7: Experimental results with different values of hyperparameters and \u03b3.", "description": "This table presents the results of experiments conducted to analyze the impact of hyperparameters \u03b2 and \u03b3 on the performance of the SalientLoRA fine-tuning method.  It shows the Matthews Correlation Coefficient (MCC) and accuracy (Acc) achieved on the CoLA and MRPC datasets, along with the corresponding fine-tuning time (Time), for various combinations of \u03b2 and \u03b3 values.  The results help determine optimal values for \u03b2 and \u03b3 that balance performance and efficiency.", "section": "6.6 Analysis of Space Allocation and Time Consumption"}, {"figure_path": "vU512K8vrR/tables/tables_14_1.jpg", "caption": "Table 8: Experimental results with different values of hyperparameters T<sub>i</sub> and T<sub>f</sub>.", "description": "This table presents the results of experiments conducted to analyze the impact of varying the initial (T<sub>i</sub>) and final (T<sub>f</sub>) time window sizes on the performance of the SalientLoRA model. The experiments were performed on the CoLA and MRPC datasets, using a fixed target total rank (r<sub>t</sub>) of 144. The table shows the Matthews Correlation Coefficient (MCC) and accuracy (Acc) achieved, along with the corresponding fine-tuning times (Time) for each combination of T<sub>i</sub> and T<sub>f</sub> values.", "section": "6.6 Analysis of Space Allocation and Time Consumption"}, {"figure_path": "vU512K8vrR/tables/tables_14_2.jpg", "caption": "Table 9: The hyperparameter settings for NLU, NLG and instruction tuning tasks.", "description": "This table lists the hyperparameters used in the experiments for different tasks (NLU, NLG, and Instruction Tuning).  It details the learning rate, batch size, number of epochs, target total rank (rt), warm-up steps (ni), and rank allocation steps (nf) used for each dataset in each task.  The hyperparameters were chosen to optimize performance within each experimental setting.", "section": "6.1 Experimental Settings"}]