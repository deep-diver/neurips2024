{"references": [{"fullname_first_author": "Josh Achiam", "paper_title": "GPT-4 technical report", "publication_date": "2023-03-08", "reason": "This paper provides a technical report of GPT-4, a significant large language model (LLM) that is frequently used as a benchmark in many recent LLMs' research, directly relevant to the current paper's focus on parameter efficient fine tuning."}, {"fullname_first_author": "Rohan Anil", "paper_title": "Palm 2 technical report", "publication_date": "2023-05-10", "reason": "This paper introduces Palm 2, another significant LLM, which like GPT-4, is frequently used as a comparison in the evaluation of new LLMs, and whose parameter scale underscores the necessity for parameter-efficient fine-tuning methods."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Llama: Open and efficient foundation language models", "publication_date": "2023-02-13", "reason": "This paper introduces the LLaMA model, which is directly used in the current paper's experiments on instruction tuning, demonstrating the relevance of the cited work to the current paper's empirical evaluation."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Llama 2: Open foundation and fine-tuned chat models", "publication_date": "2023-07-09", "reason": "This paper introduces the LLaMA 2 model, which is used in the current paper's experiments, highlighting the importance of the cited work to the current paper's empirical evaluation, especially instruction tuning."}, {"fullname_first_author": "Edward J Hu", "paper_title": "LoRA: Low-rank adaptation of large language models", "publication_date": "2022-00-00", "reason": "This paper introduces the LoRA method, which is the central focus of the current paper, making this citation extremely important for providing context and background of the research area."}]}