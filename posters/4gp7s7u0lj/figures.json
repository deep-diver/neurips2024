[{"figure_path": "4GP7S7U0lJ/figures/figures_1_1.jpg", "caption": "Figure 1: We provide examples of collective outliers as well as their causes in (a). (b) illustrates the Dataset Maps based on the SPICE score for the MSR-VTT training set, with the y-axis representing the average SPICE score over training epochs and the x-axis showing their variability. The consistency score is also measured on ground truth captions. The entire space is divided into four sub-regions based on x,y coordinates, with examples in each region demonstrating different levels of learnability.", "description": "This figure shows examples of collective outliers in video captioning and how they are caused by abstraction and granularity inconsistencies in human annotations.  The (a) part illustrates these inconsistencies with specific examples. The (b) part is a Dataset Map visualizing the average SPICE score (reflecting annotation quality) against variability across training epochs for the MSR-VTT dataset.  The plot is divided into four quadrants representing different levels of learnability (ease of learning for the model) based on the consistency and variability of SPICE scores. Examples of captions from each quadrant further illustrate the concept of learnability.", "section": "3.1 Data Learnability in Video Captioning"}, {"figure_path": "4GP7S7U0lJ/figures/figures_4_1.jpg", "caption": "Figure 2: Our method explicitly introduces learnability to reflect the collective outliers in video captioning. Together with uncertainty and diversity, our active learning scheme ranks unlabelled videos and parses them to humans. Our caption-wise protocol further provides an intellectual yet effective way to allocate human efforts, leading to 103% full performances at 25% human annotations.", "description": "This figure illustrates the proposed active learning method for video captioning.  The method incorporates three key aspects: learnability (to address collective outliers), diversity (to select a variety of samples), and uncertainty (to prioritize samples with less reliable predictions).  The process begins by evaluating an unlabeled set of videos using a video captioning model and a foundational model. The foundational model generates approximate ground truths to estimate the learnability of each video. Then, the method combines the learnability, diversity, and uncertainty scores to rank unlabeled videos. Finally, it selects the highest-ranked videos for human annotation using a caption-wise protocol to optimize human effort. This figure shows the workflow and highlights the key components of the proposed method.", "section": "3.2 Our Active Learning Scheme"}, {"figure_path": "4GP7S7U0lJ/figures/figures_7_1.jpg", "caption": "Figure 3: Active learning performances on the MSVD. Ours significantly outperforms other methods.", "description": "This figure shows the performance comparison of different active learning methods on the MSVD dataset. The x-axis represents the percentage of human annotations used, while the y-axis represents the percentage of full performance achieved on various metrics (BLEU-4, METEOR, ROUGE-L, CIDEr, and SPICE).  The results show that the proposed method ('Ours') consistently outperforms other baselines, such as random sampling, maximum entropy, minimum likelihood, and coreset selection, across all metrics and annotation percentages.  For several metrics, 'Ours' even surpasses the full performance achieved when using 100% of human annotations.  The plot includes error bars to illustrate variability.", "section": "4.2 Main Active Learning Performances"}, {"figure_path": "4GP7S7U0lJ/figures/figures_8_1.jpg", "caption": "Figure 4: The percentage of overlapped selections in S1.", "description": "This Venn diagram shows the overlap between samples selected by active learning methods focusing on learnability, uncertainty, and diversity individually.  The large, non-overlapping portions indicate that each criterion selects a significantly different subset of samples. The small overlaps demonstrate that these three criteria complement one another in identifying valuable unlabeled datapoints for training a video captioning model.", "section": "4.3 Ablation Study on Learnability, Uncertainty, and Diversity"}, {"figure_path": "4GP7S7U0lJ/figures/figures_8_2.jpg", "caption": "Figure 3: Active learning performances on the MSVD. Ours significantly outperforms other methods.", "description": "This figure displays the results of an active learning experiment on the MSVD video captioning dataset.  The x-axis represents the percentage of human annotations used for training, while the y-axis shows the performance of various active learning methods relative to the performance achieved with 100% of human annotations (the full performance).  The figure shows that the proposed method ('Ours') consistently outperforms other state-of-the-art (SOTA) active learning methods across multiple evaluation metrics (BLEU4, METEOR, ROUGE-L, CIDEr, and SPICE), achieving nearly or even exceeding the full performance with significantly less human annotation.", "section": "4.2 Main Active Learning Performances"}, {"figure_path": "4GP7S7U0lJ/figures/figures_9_1.jpg", "caption": "Figure 6: The distribution of samples in S1 on MSR-VTT.", "description": "This figure shows the distribution of selected samples (S1) categorized by their learnability (Easy, Moderate, Hard, Collective Outliers) for different active learning methods. The method proposed in the paper ('Ours') shows a significantly higher proportion of easy samples and a lower proportion of collective outliers, highlighting its effectiveness in selecting less challenging and more reliable samples for annotation.", "section": "4.4 More Analysis"}, {"figure_path": "4GP7S7U0lJ/figures/figures_15_1.jpg", "caption": "Figure 3: Active learning performances on the MSVD. Ours significantly outperforms other methods.", "description": "This figure compares the performance of different active learning methods on the MSVD dataset across five evaluation metrics (BLEU-4, METEOR, ROUGE-L, CIDEr, and SPICE) using two different backbones (SwinBERT and CoCap).  The x-axis represents the percentage of human annotations used, and the y-axis shows the performance relative to the full performance achieved with 100% annotations. The results demonstrate that the proposed method ('Ours') consistently outperforms other methods, including random sampling, maximum entropy, minimum likelihood, and coreset selection, across all metrics and backbones.  The method achieves performance exceeding 100% of the full performance with a relatively small percentage of human annotations.", "section": "4.2 Main Active Learning Performances"}, {"figure_path": "4GP7S7U0lJ/figures/figures_16_1.jpg", "caption": "Figure 8: Pipeline of the scene graph parser.", "description": "This figure shows the process of generating a scene graph from a caption.  It begins with a caption, which undergoes dependency parsing to create a dependency parse tree.  Linguistic rules are then applied to transform this parse tree into a semantic graph. Finally, post-processing steps like simplifying quantificational modifiers and resolving pronouns produce the final scene graph, which includes objects, attributes, and relations. The example shown illustrates the steps involved in processing the caption \"a cartoon character falls and breaks both of his legs.\"  The resulting scene graph represents the key elements and their relationships.", "section": "A.2.2 More Details of Our Method"}, {"figure_path": "4GP7S7U0lJ/figures/figures_17_1.jpg", "caption": "Figure 2: Our method explicitly introduces learnability to reflect the collective outliers in video captioning. Together with uncertainty and diversity, our active learning scheme ranks unlabelled videos and parses them to humans. Our caption-wise protocol further provides an intellectual yet effective way to allocate human efforts, leading to 103% full performances at 25% human annotations.", "description": "This figure illustrates the proposed active learning method for video captioning.  The method incorporates three key aspects: learnability (to address collective outliers), diversity, and uncertainty.  It uses a novel caption-wise protocol to efficiently allocate human annotation effort. The diagram shows the flow of the algorithm, highlighting the selection of unlabeled videos based on learnability, diversity, and uncertainty scores, and how the selected videos are presented to human annotators for captioning.  The caption-wise protocol ensures that only a limited number of human-annotated captions are requested for each selected video.", "section": "3.2 Our Active Learning Scheme"}, {"figure_path": "4GP7S7U0lJ/figures/figures_18_1.jpg", "caption": "Figure 9: We divide the data in Dataset Maps on MSR-VTT into four regions according to their learnability. Specifically, we have EASY, MODERATE, HARD, and COLLECTIVE OUTLIER colored in blue, orange, green, and red, respectively.", "description": "This figure shows the distribution of MSR-VTT training data points based on their average SPICE score (y-axis) and variability (x-axis), which represent the learnability and uncertainty of the data, respectively. The data is divided into four regions based on these two metrics, indicating different levels of learnability: EASY, MODERATE, HARD, and COLLECTIVE OUTLIERS. Each region is represented by a different color. The figure is used to demonstrate the property of Dataset Maps to diagnose the training process and identify collective outliers.", "section": "4.3 Ablation Study on Learnability, Uncertainty, and Diversity"}, {"figure_path": "4GP7S7U0lJ/figures/figures_18_2.jpg", "caption": "Figure 1: We provide examples of collective outliers as well as their causes in (a). (b) illustrates the Dataset Maps based on the SPICE score for the MSR-VTT training set, with the y-axis representing the average SPICE score over training epochs and the x-axis showing their variability. The consistency score is also measured on ground truth captions. The entire space is divided into four sub-regions based on x,y coordinates, with examples in each region demonstrating different levels of learnability.", "description": "This figure shows examples of collective outliers in video captioning and how they relate to learnability.  Part (a) illustrates two types of inconsistencies in human annotations that lead to outliers: abstraction inconsistency (different high-level interpretations of the same video) and granularity inconsistency (descriptions at varying levels of detail). Part (b) presents a Dataset Map, visualizing the average SPICE score (a video captioning evaluation metric) and its variability across training epochs for the MSR-VTT dataset.  The map is divided into four quadrants based on variability and average SPICE score, representing different levels of learnability (ease of learning). Examples of captions from each quadrant are given to demonstrate varying learnability.", "section": "3.1 Data Learnability in Video Captioning"}]