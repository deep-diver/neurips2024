[{"type": "text", "text": "CODE: Contrasting Self-generated Description to Combat Hallucination in Large Multi-modal Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Junho Kim\u2217 Hyun Jun Kim\u2217 Yeon Ju Kim Yong Man Ro\u2020 ", "page_idx": 0}, {"type": "text", "text": "Integrated Vision and Language Lab, KAIST, South Korea {arkimjh, kimhj709, yeonju7.kim, ymro}@kaist.ac.kr ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large Multi-modal Models (LMMs) have recently demonstrated remarkable abilities in visual context understanding and coherent response generation. However, alongside these advancements, the issue of hallucinations has emerged as a significant challenge, producing erroneous responses that are unrelated to the visual contents. In this paper, we introduce a novel contrastive-based decoding method, COuntering DEscription Contrastive Decoding (CODE), which leverages selfgenerated descriptions as contrasting references during the decoding phase of LMMs to address hallucination issues. CODE utilizes the comprehensive descriptions from model itself as visual counterpart to correct and improve response alignment with actual visual content. By dynamically adjusting the information flow and distribution of next-token predictions in the LMM\u2019s vocabulary, CODE enhances the coherence and informativeness of generated responses. Extensive experiments demonstrate that our method significantly reduces hallucinations and improves cross-modal consistency across various benchmarks and cutting-edge LMMs. Our method provides a simple yet effective decoding strategy that can be integrated to existing LMM frameworks without additional training. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "With recent advancements of Large Language Models (LLMs) [18, 4, 54, 11, 46], Large Multi-modal Models (LMMs), sometimes referred as Large Vision-Language Models [24, 63, 32, 31], have been drawn great attention for their natural multi-modal interaction with users through back-and-forth conversations. Leveraging their robust generation capabilities, various pioneering tasks in pre-LMM era such as image captioning [7, 58, 27], visual question answering [1, 2], object detection [17], etc., have been integrated into a single task rather than treated as sub-tasks and achieved significant milestones [21, 59, 44]. However, at the same time, the hallucination issue [49, 67] has become one of the emerging problems when adopting LMMs into real-world applications due to their potential spurious generation in critical areas. ", "page_idx": 0}, {"type": "text", "text": "Here, unlike hallucination studies in LLMs [22] mainly focusing on factuality hallucination originated from the language knowledge, the hallucination problem in LMMs refers cross-modal inconsistency between the given visual contents and the generated responses for the user instructions. After the seminal works [13, 42, 68] giving eyes to LLMs to understand visual contents with visual instruction tuning, numerous cutting-edge LMMs [61, 9, 41, 36] actively have been proposed. Albeit the scaling laws following more stronger versatile vision models [37, 29, 33], higher resolution [41, 8], deeper alignment layers [5, 45, 8], larger model sizes, etc., LMMs still suffer from generating responses that seem plausible but are factually incorrect for the given visual contents. ", "page_idx": 0}, {"type": "image", "img_path": "g6nn2AijDp/tmp/23c5aec395414f0630faca3d489acd98856cf06d0a13df7d947bf02609d1949d.jpg", "img_caption": ["Figure 1: The overall decoding procedure of CODE. After LMMs generate detailed description for the visual content by themselves (see instruction in Appendix. A), the model recursively outputs logits from each $v$ and $d$ . By contrasting between two log-likelihoods, CODE produces more contextual and correct responses that match the given visual content suppressing inconsistent words (catching $\\rightarrow h i t$ ). "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "The origin of LMM hallucination is an intertwined problem for their inherent training paradigm, which involves alignment projection matching during the pre-training, followed by fine-tuning with the limited instruction-following data. Several approaches, aimed for mitigating hallucinatory effects, have addressed the inconsistent issues in the context of data-associated solution [39, 56], scaling model architectures [65], or additional RL-based training [64, 52]. Among them, reactive methods [23, 14, 26] intervene the decoding phase of LMMs\u2019 inference and alleviate undesired responses. Motivated by Li et al. [34] that have proposed contrastive decoding (CD) method between expert and amateur language models, recent CD-based approaches in LMMs have proposed several ways of contrasting model responses from visual inputs with their counterparts (e.g., visual contamination [30], image-biased models [69], or fine-grained visual information [10]). ", "page_idx": 1}, {"type": "text", "text": "Our research question begins with \"How effectively do contemporary LMMs capture visual evidences in their descriptive responses, and what information must be curbed to produce informative and consistent responses?\". As illustrated in Fig 1 (bottom-left), when asking LMMs to generate a comprehensive description for visual content, the output seemingly generates detailed description effectively, but a closer examination often reveals missed fined-grained information or hallucinatory instances in the responses. By recursively referring these incomplete descriptions generated by the models themselves, we aim to restrict the incorrect information flow during the generation phase and enhance the alignment of the model responses grounded in true visual evidences. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we introduce a novel training-free contrastive decoding method, COuntering DEscription Contrastvie Decoding (CODE), designed to use self-generated descriptions as a contrastive reference to mitigate hallucination issues in LMMs. The core idea of our proposed method is on harnessing the self-generated descriptions which possibly encompass both factual evidence and hallucinatory information from visual contents as a look-up reference for response correction. Specifically, within our contrastive framework as illustrated in Fig. 1, the comprehensive descriptions from model itself alternatively propagate to visual input tokens and contrast the discrepancy with the logits from actual visual contents to enhance next-token prediction. In addition, we introduce a dynamic restriction strategy that enables adaptive control of information flow during the auto-regressive decoding phase, taking into account both token-level predictions and their distribution within the vocabulary set. ", "page_idx": 1}, {"type": "text", "text": "By conducting extensive experiments and analyses on prevailing cutting-edge LMMs [40, 51, 16, 41, 62, 8], we corroborate the effectiveness of our method in reducing hallucination and enhancing the coherence and informativeness in various benchmarks [35, 42, 52, 53, 57]. Our decoding method can be seamlessly integrated into existing LMMs by simply substituting the image tokens with self-generated descriptions in a training-free manner. ", "page_idx": 1}, {"type": "text", "text": "Our contribution can be summarized into three-fold as follows: ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "\u2022 We introduce COuntering DEscription Contrastive Decoding (CODE), a training-free decoding strategy that employs self-generated descriptions to minimize hallucinations in LMMs. ", "page_idx": 1}, {"type": "text", "text": "By contrasting logit information from descriptions with actual visual contents, CODE enhances visual consistency and coherence in the model responses. \u2022 Our approach incorporates dynamic restriction strategies within the contrastive decoding phase. It selectively regulates the information flow by adjusting token-level predictions based on their distribution in the vocabulary, thus ensuring more contextual responses. \u2022 We validate the effectiveness of our decoding method across various benchmarks using cutting-edge LMMs. The results demonstrate that CODE significantly reduces hallucination while enhancing the relevance and informativeness in the responses. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Vision+LLM: Large Multi-modal Models After the emergence of large-scaled LLMs [4, 54, 11] that can interact with users with question-answer chat, various of vision+LLM studies\u2014 i.e., LMMs, have been proposed to integrate the robust linguistic capability into the visual understanding and reasoning in diverse vision-language task. As earlier works such as LLaVA [42], Instruct-BLIP [13], and MiniGPT-4 [68], which utilized visual instruction tuning, have bridged two modalities of vision and language through fine-tuning with a learnable query, exemplified by Q-Former [31] or projection layer-based alignments [40]. To enhance cross-modal consistency in vision-language representation, recent works have proposed several solutions to address the underlying weaknesses in both modalities: (i) utilizing higher-resolution visual inputs [41, 9], (ii) deploying Mixture-of-Expert (MoE) concepts integrating versatile vision models [37, 29, 33], (iii) improving weak alignment interface [5, 45, 8], or (iv) adopting larger LLMs to scale up the language model prior [62]. ", "page_idx": 2}, {"type": "text", "text": "Hallucination Issue, Harming Cross-modal Consistency Despite of the endeavor developments of LMMs, they cannot be free from cross-modal inconsistency between the visual contents and their generated responses, so-called hallucination [49]. This not only leads to performance degradation but also provokes an over-reliance issue, resulting in incorrect model responses that are not grounded in true visual evidence. This critical concern regarding response trustworthiness and model reliability hinders the adoption of LMMs in real-world applications. To mitigate the hallucination problem, diverse works have been proposed employing additional training on curated datasets [52, 55] or reinforcement learning under feedback systems [64, 66]. Among them, by intervening during the response generation, decoding-based approaches [34, 12] are introduced to encourage models to represent more precise responses. We refer to readers for more comprehensive survey papers [43, 3] addressing hallucination in LMMs. Our work is in line with CD-based approaches that utilize logit discrepancy from counterpart outputs to enhance coherence. Unlike the previous works [30, 23, 10] that focus on twisting visual information, we utilize self-generated description as contrasting visual counterpart and correct hallucinatory responses based on the model understanding. ", "page_idx": 2}, {"type": "text", "text": "3 Proposed Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Problem Setup and Preliminaries. Let $M_{\\theta}$ denote a vision-language LMM parameterized by $\\theta$ that auto-regressively generates responses for the given visual contents $v$ and input textual query $x$ . Then the model maps the logit distribution to the next token prediction output $y_{t}\\in\\mathbb{R}^{|\\mathcal{V}|}$ at time step $t$ in the vocabulary set $\\mathcal{V}$ such that $y_{t}\\sim p_{\\theta}(y_{t}|v,x,y_{<t})\\propto\\mathrm{logit}_{\\theta}(y_{t}|v,x,y_{<t})$ , where $y{<}t$ indicates all previously generated tokens. During the response generation, we can deploy several decoding strategies to choose the next word using either deterministic search (e.g., greedy, beam search) or stochastic sampling (e.g., top- $\\cdot\\mathbf{k}$ , Nucleus search [20]). ", "page_idx": 2}, {"type": "text", "text": "After the seminal works [34, 12] in natural language processing have introduced Contrastive Decoding (CD) mechanism, which considering information disparities between expert and amateur models for more coherence and informativeness, various works have deployed this strategy into LMMs by twisting visual contents [30] or model information [69] for the contrastive approach. The next-token probability $p_{\\mathrm{cd}}$ from CD can be generally formulated as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\np_{\\mathrm{cd}}(y_{t}\\mid y_{<t})=\\operatorname{Softmax}\\left[(1+\\alpha)\\mathrm{logit}_{\\theta}(y_{t}\\mid v,x,y_{<t})-\\alpha\\mathrm{logit}_{\\hat{\\theta}}(y_{t}\\mid\\bar{v},x,y_{<t})\\right],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\bar{v}$ and $\\hat{\\theta}$ indicates visual counterparts and sub-optimal amateur model, respectively\u2014 note that $\\scriptstyle{\\theta={\\hat{\\theta}}}$ can be regarded as self-correction. Intuitively, the objective of CD is amplifying model outputs ", "page_idx": 2}, {"type": "text", "text": "by reflecting information deviation between top candidate log-probabilities. Therefore, the selection of logit counterparts for referring is the key challenge for high-quality and consistent responses during the contrastive decoding frameworks. ", "page_idx": 3}, {"type": "text", "text": "3.1 Comprehensive Image Description as Visual Counterpart ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "As the visual counterpart, we deploy comprehensive image descriptions generated from the model as contrasting reference, which are inevitably less informative than the visual contents themselves. Our motivation is on the innate difference of information density between vision and language [19]. While vision information exhibits relatively less redundancy for spatial signals\u2014 e.g., human can visually recognize objects with a few masked patches on images, languages contain high-entropy information, resulting in spans that are more semantic and information-dense than visual signals. That is, it is difficult to infer blanked-out words\u2014 e.g., \"I went to the store to buy some \". Building upon the property of each modality, we first delve into the self-generated model responses with specific instruction for the visual contents to elicit the encompassed visual evidences in the representation space and assess its potential subject role as a visual counterpart for contrastive decoding. ", "page_idx": 3}, {"type": "text", "text": "As illustrated in Fig. 1, we input a query instruction into the model to generate a comprehensive visual description for the given visual content (please see the detailed instruction $x_{0}$ for the self-generated description in Appendix. A). Then, we exploit the generated description as recursive visual inputs, replacing the position of image tokens in the model input sequence (e.g., <image> token in LLaVA series [42, 40, 41]). Ideally, if the model sufficiently covers whole visual evidences to answer any vision-related questions, the generated response should provide competent answers with solely utilizing the description-only embeddings as an alternative of visual embeddings. However, as shown in Fig 2, it is obvious that the results from description-only show sub-optimal performance to answer questions due to insufficient information in capturing visual evidences. Consequently, we use the comprehensive description for the visual contents, which is generated by model itself\u2014 but partially incorrect or hallucinatory to capture visual evidences, as a contrasting visual counterpart to enhance response coherence during the decoding phase, which also in line with the amateur model selection philosophy of contrastive decoding [34]. ", "page_idx": 3}, {"type": "image", "img_path": "g6nn2AijDp/tmp/c26a0e0886dc79ca72ecf9bcfd3cd76571dac2c96df7cfbdced630749708da2d.jpg", "img_caption": ["Figure 2: The comparison is based on two benchmarks (MMVP [53]: multiple choice / LLaVA-Bench [42]: descriptionlevel). The plain and dotted bars indicate the results for the models that use selfgenerated descriptions as visual input replacements and original model with actual visual contents, respectively. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "3.2 COuntering DEscription Contrastive Decoding ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Based on our analysis in sec. 3.1, we can obtain a pair of the visual content and its comprehensive description $(v,d)$ , such that $d$ corresponds to $M_{\\theta}(y|\\bar{v},x_{0})$ . By contrasting the logit variation between the paired information into the model response generation, we can formulate the next-word prediction using our proposed method, COuntering DEscription Contrastive Decoding (CODE): ", "page_idx": 3}, {"type": "equation", "text": "$$\np_{\\mathrm{code}}(y_{t}\\mid y_{<t})=\\operatorname{Softmax}\\left[(1+\\alpha_{t}){\\log}\\mathrm{it}_{\\theta}(y_{t}\\mid v,x,y_{<t})-\\alpha_{t}{\\log}\\mathrm{it}_{\\theta}(y_{t}\\mid d,x,y_{<t})\\right].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here, unlike the previous approaches [34, 30, 12] that restrict the logit variations with fixed $\\alpha$ value as in Eqn. 1, we present a dynamic restriction $\\alpha_{t}$ for the logit variations by comparing the information between visual contents and their comprehensive descriptions. Revisiting the role of $\\alpha$ , it determines whether to promote or curb information from logit variation, thus directly influencing next-token generation\u2014 higher value results in more aggressive adjustment for the variations. However, when confronting that both $v$ and $d$ yield similar logit score on the correct token, the variation gets closer to zero, thereby the next-token prediction can be unexpectedly reversed if other tokens get rewarded than the correct token with a fixed $\\alpha$ on a token-by-token basis. Although this aligns with the initial intent of CD, a more robust selector is necessary to effectively restrict the logit information flow. ", "page_idx": 3}, {"type": "text", "text": "Accordingly, our method predicts next-token not only at the individual token-level but also considering its distribution across the entire vocabulary set, enabling dynamic control of the information flow. ", "page_idx": 3}, {"type": "text", "text": "To measure the relative entropy between the token distributions from visual contents $P_{t}^{v}$ and its comprehensive description $P_{t}^{d}$ at time step $t$ , we deploy Bounded Divergence $\\(\\mathcal{D}_{\\mathrm{bd}})$ [6], which is a type of statistical distance that ensures symmetric and bounded measure: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{D}_{\\mathrm{bd}}(P||Q)=\\frac{1}{2}\\sum_{i=1}^{n}(p_{i}+q_{i})\\log_{2}(\\left|p_{i}-q_{i}\\right|^{k}+1),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\begin{array}{r}{D_{\\mathrm{bd}}(P\\|Q)\\ge0}\\end{array}$ and equals 0, if and only if $p{=}q$ , and $k$ denotes a smoothing parameter. Here, the upper-bound of the divergence apparently exists, such that $\\begin{array}{r}{\\mathcal{D}_{\\mathrm{bd}}(P\\|Q)\\leq\\sum_{i=1}^{n}\\overset{\\bullet}{p}_{i}\\log_{2}2{=}1}\\end{array}$ , due to the following condition $|p_{i}\\!-\\!q_{i}|\\leq1$ . ", "page_idx": 4}, {"type": "text", "text": "We define the dynamic restriction $\\alpha_{t}$ as $1{-}\\mathcal{D}_{\\mathrm{bd}}\\big(P_{t}^{v}\\|P_{t}^{d}\\big)$ , where it enables a token-wise feedback control that adjusts the information weighting with respect to the closeness of the two distributions. The major role of the restriction term is maintaining a balance in the logit variation for the observed prediction disparities between the $v$ and $d$ distributions. That is, when the distributions are close enough (i.e., $\\bar{P}_{t}^{v}\\approx P_{t}^{d})$ ), the value of $\\mathcal{D}_{\\mathrm{bd}}\\big(P_{t}^{v}\\|P_{t}^{d}\\big)$ approaches zero, indicating minimal divergence. That is, $\\alpha_{t}$ approaches 1, allowing for higher amplification of logit variations in predicting the next-token outputs. This adjustment reflects the increased reliability of predictions when the two distributions from $v$ and $d$ are closely aligned. On the other hand, for the dissimilar distributions, $\\alpha_{t}$ decreases towards zero, compelling the model to restrict information flow from the variation. This reduction limits the potential for introducing erroneous or less probable predictions by focusing more on visual information, thereby maintaining coherence in the output when the model\u2019s understanding of the visual content significantly deviates from its textual description. ", "page_idx": 4}, {"type": "text", "text": "3.3 Adaptive Information Constraint ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "One major challenge in contrastive-based decoding is the scenario where implausible tokens are rewarded, even when predictions are made with low confidence. This issue can also arise in our method, particularly when token distributions derived from textual descriptions provide more confidence than the visual content, ironically undermining the most predictive tokens. To address it, Li et al. [34] have introduced an adaptive plausibility constraint, which fliters out less plausible tokens by truncating them based on the maximum token confidence from the expert model. While this approach simply penalizes false positive tokens in the candidate pool, it may also have unintended side effects by prematurely applying a cutoff threshold to lower-confidence tokens. Specifically, early threshold settings can sometimes eliminate the possibility of identifying correct token predictions, which might otherwise be dismissed in a pool considered to contain mostly false negatives. ", "page_idx": 4}, {"type": "text", "text": "Improving the previous constraint [34, 30], we present adaptive information constraint $(\\mathcal{V}_{\\mathrm{head}})$ designed to dynamically retain tokens that may be informative despite their lower confidence. By comparing prediction distributions between $P_{t}^{v}$ and $P_{t}^{d}$ , we filter out less relevant tokens from the candidate pool as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{V}_{\\mathrm{head}}(y_{<t})=\\{y_{t}\\in\\mathcal{V}:p_{\\theta}(y_{t}\\mid v,x,y_{<t})\\ge\\beta_{t}\\operatorname*{max}_{w}p_{\\theta}(w\\mid v,x,y_{<t})\\},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\beta_{t}$ dynamically regulate the token candidate pool utilizing the divergence term in Eqn. 3, defined as $\\bar{\\beta}_{t}=D_{\\mathrm{bd}}\\bar{(P_{t}^{v}||P_{t}^{d})}$ . This strategy can expand the token searching pool when the nexttoken prediction, derived from both visual content and comprehensive description, shows a similar distribution yet uncertainty in selecting the candidate token (i.e., false negatives). Finally, we only consider the next-token prediction within $\\mathcal{\\ V}_{\\mathrm{head}}(y_{<t})$ , and for the tokens satisfying $y_{t}\\notin\\dot{\\mathcal{V}}_{\\mathrm{head}}(y_{<t})$ , we set their logits to $-\\infty$ to fliter out from the candidate pool. Please see comprehensive Algorithm. 1 in Appendix. B. ", "page_idx": 4}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Experimental Setup To validate the efficacy of our method over various LMM families and sizes, we implemented our method on contemporary LMMs: LLaVA-1.5 (13B) [40], Emu2-Chat (14B) [51], InternLM-XComposer2 (7B) [16], LLaVA-NeXT (34B) [41], Yi-VL (34B) [62], and InternVL 1.5 (26B) [8]. We compared our method with five baseline decoding strategies. For the regular decoding strategies, we used greedy decoding, Nucleus sampling [20], and beam search decoding. Additionally, we selected OPERA [23] and VCD [30] for contrastive decoding method, which designed to mitigate hallucinations with contrastive frameworks. We used the default parameter settings for all methods, where top-p value 0.95 and temperature 1.0 for Nucleus sampling, the number of window size for searching is 5 (i.e., num-beams 5) for both beam search decoding and OPERA, and CD- $\\alpha\\,=\\,1$ , CD- $\\beta=0.1$ for VCD, and $k=0.3$ for our method. Note that OPERA inference requires too much memory especially for LLaVA-NeXT (34B), so that we excluded OPERA results for this model. ", "page_idx": 4}, {"type": "image", "img_path": "g6nn2AijDp/tmp/a9da5030e7f4c6c346786a60385df9b7f7d3a5fd072e37a2c9e4f477147f85c9.jpg", "img_caption": ["Figure 3: Overview of experimental results on 6 baseline LMMs, 6 decoding method, and 6 hallucination benchmarks in spider chart format. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Benchmarks and Evaluation Metrics The benchmarks for evaluating hallucinations in LMMs can be broadly categorized into discriminative and generative streams. The discriminative type assesses hallucinations by evaluating the predicted answer among given options (e.g., multiple choice or yes/no question), while generative benchmarks typically employ more advanced language models (e.g., GPT-aided evaluation) to rate the subject model descriptions. Within this taxonomy, we carefully select 6 benchmarks to test baselines. Please see Appendix. C for benchmark details. ", "page_idx": 5}, {"type": "text", "text": "As discriminative benchmarks, we utilize mainly three datasets for detailed evaluation. Specifically, POPE [35] is a commonly used benchmark for detecting object hallucination by converting object annotations sourced from MSCOCO [7]. Under the three different subsets: random, popular, and adversarial, the metric for POPE measures binary classification performance for simple yes/no questions. MMVP [53] aims to evaluate the understanding of visual details for 9 different visual patterns using paired classification accuracy. Due to its evaluation design, which involves comparing two similar CLIP-blind image pairs, MMVP requires LMMs to capture subtle visual differences. RealworldQA [57] is the most recent dataset tailored to assess the capability of LMMs in basic real-world spatial understanding, using the accuracy metric within multiple-choice questions. ", "page_idx": 5}, {"type": "text", "text": "We use three benchmarks for generative benchmarks, extending the evaluation scope to include openended captioning tasks beyond merely assessing classification within given answer options. Generally, ChatGPT [46] is used to score the quality of the model-generated sentences. The metric for both LLaVA-QA90 [42] and LLaVA-Bench (In-the-Wild) [42] is score ratio, where model responses rated from GPT-4 [47] are divided by GPT-4 answers such that $\\scriptstyle\\sum\\,|{\\mathrm{model-score}}|/\\scriptstyle\\sum\\,|{\\mathrm{GT-score}}|,$ where all scores are rated by GPT-4. It has three types of questions: conversation, detailed description, and complex reasoning. MMHal-Bench [52] evaluates the degree of hallucination for the 8 various question types: object attribute, adversarial object, comparison, counting, spatial relation, environment, holistic description, and others. GPT-4 measures the severity of hallucination in a range of 0 to 7 and the higher score denotes less hallucination. ", "page_idx": 5}, {"type": "table", "img_path": "g6nn2AijDp/tmp/dea3464d9d7e1df2cd1e1f702527a0d7d36bf0c799127fa7d8dc734878c526c2.jpg", "table_caption": ["Table 1: The hallucination evaluation results for the discriminative benchmarks [35, 53]. Each emoji in MMVP colum cell indicates 9 different visual patterns (details in Appendix. C). "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "4.1 Evaluation Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Overview We summarize our comprehensive experimental results across six LMMs, six decoding methods, on six benchmarks in a spider chart format for visibility of the improvements in Fig. 3. As in the figure, CODE generally shows competent performance and consistent results among different measurements and benchmarks. We delve into each result in detail in the subsections below. ", "page_idx": 6}, {"type": "text", "text": "Results on Discriminative Benchmarks We conduct hallucination evaluation across 6 LMM baselines on two discriminative benchmarks. POPE [35] focuses on object-level hallucinations, where its prompts consist of asking about the existence of objects, such as \"Is there <something> in the image?\". Here, we exclude trivial splits and test with the most challenging subset: adversarial split, which requires models to identify highly-relevant objects to correctly answer yes/no. As shown in Table 1, both the accuracy and F1 score from CODE lead to the best performance in 6 LLM baselines, among 6 decoding methods, which indicates that our decoding method can properly address object hallucination within simple binary question format. ", "page_idx": 6}, {"type": "text", "text": "As a more sophisticated visual assessment, we compare our method on MMVP benchmark [53]. Beyond object hallucination, the benchmark is divided into 9 systematic visual patterns within CLIPblind pairs. These pairs consist of two images that clearly display visual differences, yet CLIP [48] struggles to discriminate between them. The test subject models only receive scores when they correctly identify both images in the pair, requiring precise visual understanding. On the right part of Table. 1, we can observe a remarkable gain in average, indicating that CODE effectively enhances visual consistency through the contrastive mechanism using self-generated descriptions. ", "page_idx": 6}, {"type": "text", "text": "Results on GPT-aided Benchmarks Although the discriminative datasets are intuitive benchmarks for evaluating the degree of object-level hallucination, they are limited in measuring a deeper understanding of whether the given LMM responses encompass contextual or sequential hallucination. To probe the expanded effectiveness of our method beyond simple multiple choice tests, we utilize two generative benchmarks that can verify model responses at the sentence level. As in Table. 2, CODE generally outperforms the overall score of LLaVA-QA90 [42], showing up to $+13.7\\%$ improvements than other CD methods. Additionally, we compare our models in MMHal-Bench [52] specialized to evaluate hallucination effects sourced from more challenging image-question pairs\u2014 OpenImages [50]. As in the result, our method generally not only improves overall average score with consistent results among 6 other baseline LMMs, but also effectively mitigates the hallucination ratio. Through combinatorial results from both discriminative and generative benchmarks, we corroborate the robustness and efficacy of our proposed method in addressing both object-level and contextual hallucinations, thereby ensuring more reliable LMM responses. ", "page_idx": 6}, {"type": "table", "img_path": "g6nn2AijDp/tmp/bc01941129d36e2e5971683d4f18382f9d72e191b2e1ff9e2cd96c507d6d7597.jpg", "table_caption": ["Table 2: GPT-aided evaluation results among 6 LMMs and decoding methods on generative benchmarks (LLaVA-QA90 [42]: score ratio for GPT answer / MMHal-Bench [52]: score rated by GPT). "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "4.2 Analyses on CODE ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Ablation Study Previous CD-based methods [34, 30, 12] require heuristic choices to control the degree of amplification for logit variation $\\alpha$ and penalizing parameter $\\beta$ that filter out the implausible next-tokens in adaptive plausibility constraint. To tackle it, we proposed two regulation methods that can dynamically control information flow in CODE and token candidate pool, respectively: (i) dynamic restriction (DR), $\\alpha_{t}$ in Eqn. 2 and (ii) adaptive information constraint (AIC), $\\beta_{t}$ in Eqn. 4. To validate the effectiveness of such adaptive regulations built in CODE, we conducted ablation study on them. ", "page_idx": 7}, {"type": "table", "img_path": "g6nn2AijDp/tmp/ef4b1dacff49c6f5ab6d0979c5fecc5d0a4528413f65a82e53cccd0da5a04d95.jpg", "table_caption": ["Table 3: Ablation study on $\\alpha_{t}$ (DR) and $\\beta_{t}$ (AIC) for (abbreviated) LV1.5 [40], LV-N [41], IVL1.5 [8] on two benchmarks [53, 42]. We report overall scores for the benchmarks. $\\pmb{x}$ indicates fixed hyper-parameter for $\\alpha$ and $\\beta$ . "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "We implement baselines with same CODE framework using self-generated descriptions as visual counterparts, but with default settings of ${\\alpha}\\mathrm{{=}}1.0$ and $\\beta{=}0.1$ . As in Table. 3, either use of DR or AIC can enhance the benchmarks than the fixed $\\alpha$ and $\\beta$ . Our CODE implementation that utilizes both DR and AIC to dynamically restrict information flow exhibits the best results among the baselines. ", "page_idx": 7}, {"type": "table", "img_path": "g6nn2AijDp/tmp/c40b52a5766ed74a25b4b70a43b323dee8973fb9a5849e74b91bba87b6f63096.jpg", "table_caption": ["Figure 4: An example of token-level case study for CODE. Each row indicates the logit score from visual content logitv, comprehensive description $\\mathrm{logit}_{d}$ , CODE applied $\\mathrm{logit_{code}}$ , respectively. "], "table_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "g6nn2AijDp/tmp/4a7b57d8f3e535aadeae6e6163d206b64facf648ab39b3a4cb8f1da1183fdcd9.jpg", "img_caption": ["Figure 5: Additional experiments on In-the-Wild benchmarks. Note that, unlike other datasets, OPERA [23] fails to generate consistent responses in real-world datasets using Yi-VL [62]. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Computational Analysis We utilize comprehensive descriptions from models as additional information for contrasting with visual contents, thereby leading to computational loads similar to other decoding methods. To analyze the computation, we compare the token throughput (token/s) and decoding latency (ms/token) with other CD-based methods on 8 NVIDIA RTX A6000 GPUs as in Table. 4. ", "page_idx": 8}, {"type": "table", "img_path": "g6nn2AijDp/tmp/5e33e56a10c88d4bd4c03e403797169ed4d61e449aacf593292c58e9e3e08100.jpg", "table_caption": ["Table 4: Computational analysis on decoding throughput and latency among CD-based methods. We compare three different model sizes. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Token-level Case Study As illustrated in Fig. 4, to verify whether the proposed CODE effectively mitigates object hallucination, we analyze the output logit values of LMM [41] at the token-level case study, with a greedy search as the baseline. The first two rows in the table indicate the original greedy decoded tokens which are elected based on high $\\mathrm{logit}_{v}$ from the visual content and CODE output tokens, respectively. As in the figure, we can observe that visual hallucination occurs at the \"Yoplait\" token highlighted in red. For relatively easy tokens at the beginning of sentence, $\\mathrm{logit_{code}}$ produces identical decisions maintaining consistency with $\\mathrm{logit}_{v}$ , which indicates the amplification of logit variation is effectively adjusted due to similar prediction distributions from visual contents and description-only information. However, at the hallucination-occurred time step, logit scores are deviated between the two information, resulting in a more confusing state to identify between GT token \"Fage\" and hallucinatory \"Yoplait\". In our framework, $^{\\prime\\prime}F a g e^{\\prime\\prime}$ is relatively more amplified from 15.02 to 16.66 than \"Yoplait\", which changes from 15.34 to 15.30. By simultaneously considering both token-level and distributional prediction over the vocabulary, CODE changes the wrong nexttoken output to correct one, mitigating hallucination. For more case studies, please refer Appendix. D. ", "page_idx": 8}, {"type": "text", "text": "Additional Experiments on In-the-Wild Contemporary open-sourced LMMs are fine-tuned with various combinations of vision-language datasets [7, 28, 25], mostly composed of COCO-sourced visual images [38] and their curated instruction. Although the existing hallucination benchmarks intentionally convert question queries to assess model robustness against inconsistency, the visual contents in benchmarks are limited to in-distribution COCO images. To validate our method in more challenging and real-world scenarios, we compared baselines on LLaVA-Bench (In-the-wild) [42] and RealworldQA [57] as in Fig. 5 and achieved competent performance (case studies in Appendix. F). ", "page_idx": 8}, {"type": "text", "text": "5 Discussion and Limitation ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Albeit the computational analysis in Table. 4, as one of limitations, our contrastive decoding method requires additional computational resources than the use of vanilla decoding. However, considering an essential ongoing research topics and developments [60, 15] aimed at mitigating the negative effects of hallucination problems in both LLMs and LMMs, our work contributes important societal impacts towards more real-world applicability and robust AI system. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We present COuntering DEscription Contrastive Decoding (CODE), a novel and training-free decoding method to mitigate hallucination in Large Multi-modal Models. By utilizing self-generated descriptions as corrective references during the decoding phase, CODE dynamically adjusts the information flow for next-token predictions, enhancing the coherence and informativeness of responses while reducing the cross-modal inconsistency. Extensive experiments demonstrate that CODE effectively decreases hallucinations across various benchmarks and contemporary LMMs, significantly improving contextual relevance and response alignment with visual contents. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was partially supported by two funds: IITP grant funded by the Korea government (MSIT) (RS-2022-II220984) and Center for Applied Research in Artificial Intelligence (CARAI) grant funded by DAPA and ADD (UD230017TD). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. Bottom-up and top-down attention for image captioning and visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6077\u20136086, 2018.   \n[2] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision, pages 2425\u20132433, 2015.   \n[3] Zechen Bai, Pichao Wang, Tianjun Xiao, Tong He, Zongbo Han, Zheng Zhang, and Mike Zheng Shou. Hallucination of multimodal large language models: A survey. arXiv preprint arXiv:2404.18930, 2024. [4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.   \n[5] Junbum Cha, Wooyoung Kang, Jonghwan Mun, and Byungseok Roh. Honeybee: Localityenhanced projector for multimodal llm. arXiv preprint arXiv:2312.06742, 2023.   \n[6] Min Chen and Mateu Sbert. On the upper bound of the kullback-leibler divergence and cross entropy. arXiv preprint arXiv:1911.08334, 2019.   \n[7] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, 2015.   \n[8] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024.   \n[9] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Zhong Muyan, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238, 2023.   \n[10] Zhaorun Chen, Zhuokai Zhao, Hongyin Luo, Huaxiu Yao, Bo Li, and Jiawei Zhou. Halc: Object hallucination reduction via adaptive focal-contrast decoding. arXiv preprint arXiv:2403.00425, 2024.   \n[11] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with $90\\%*$ chatgpt quality, March 2023.   \n[12] Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James R. Glass, and Pengcheng He. Dola: Decoding by contrasting layers improves factuality in large language models. In The Twelfth International Conference on Learning Representations, 2024.   \n[13] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. InstructBLIP: Towards general-purpose vision-language models with instruction tuning. In Advances in Neural Information Processing Systems, 2023.   \n[14] Ailin Deng, Zhirui Chen, and Bryan Hooi. Seeing is believing: Mitigating hallucination in large vision-language models via clip-guided decoding. arXiv preprint arXiv:2402.15300, 2024.   \n[15] Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and Jason Weston. Chain-of-verification reduces hallucination in large language models. arXiv preprint arXiv:2309.11495, 2023.   \n[16] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong Duan, Maosong Cao, et al. Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model. arXiv preprint arXiv:2401.16420, 2024.   \n[17] Ross Girshick. Fast r-cnn. In Proceedings of the IEEE international conference on computer vision, pages 1440\u20131448, 2015.   \n[18] Google. Bard. https://bard.google.com/, 2023.   \n[19] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16000\u201316009, 2022.   \n[20] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. In International Conference on Learning Representations, 2020.   \n[21] Ronghang Hu and Amanpreet Singh. Unit: Multimodal multitask learning with a unified transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1439\u20131449, 2021.   \n[22] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, et al. A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. arXiv preprint arXiv:2311.05232, 2023.   \n[23] Qidong Huang, Xiaoyi Dong, Pan Zhang, Bin Wang, Conghui He, Jiaqi Wang, Dahua Lin, Weiming Zhang, and Nenghai Yu. Opera: Alleviating hallucination in multi-modal large language models via over-trust penalty and retrospection-allocation. arXiv preprint arXiv:2311.17911, 2023.   \n[24] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Barun Patra, et al. Language is not all you need: Aligning perception with language models. Advances in Neural Information Processing Systems, 36, 2024.   \n[25] Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 6700\u20136709, 2019.   \n[26] Junho Kim, Yeon Ju Kim, and Yong Man Ro. What if...?: Counterfactual inception to mitigate hallucination effects in large multimodal models. arXiv preprint arXiv:2403.13513, 2024.   \n[27] Yeonju Kim, Junho Kim, Byung-Kwan Lee, Sebin Shin, and Yong Man Ro. Mitigating dataset bias in image captioning through clip confounder-free captioning network. In 2023 IEEE International Conference on Image Processing (ICIP), pages 1720\u20131724. IEEE, 2023.   \n[28] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision, 123:32\u201373, 2017.   \n[29] Byung-Kwan Lee, Beomchan Park, Chae Won Kim, and Yong Man Ro. Moai: Mixture of all intelligence for large language and vision models. arXiv preprint arXiv:2403.07508, 2024.   \n[30] Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian Lu, Chunyan Miao, and Lidong Bing. Mitigating object hallucinations in large vision-language models through visual contrastive decoding. arXiv preprint arXiv:2311.16922, 2023.   \n[31] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International Conference on Machine Learning. PMLR, 2023.   \n[32] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping languageimage pre-training for unified vision-language understanding and generation. In International Conference on Machine Learning, pages 12888\u201312900. PMLR, 2022.   \n[33] Jiachen Li, Xinyao Wang, Sijie Zhu, Chia-Wen Kuo, Lu Xu, Fan Chen, Jitesh Jain, Humphrey Shi, and Longyin Wen. Cumo: Scaling multimodal llm with co-upcycled mixture-of-experts. arXiv preprint arXiv:2405.05949, 2024.   \n[34] Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke Zettlemoyer, and Mike Lewis. Contrastive decoding: Open-ended text generation as optimization. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 12286\u201312312. Association for Computational Linguistics, July 2023.   \n[35] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 292\u2013305, Singapore, Dec. 2023. Association for Computational Linguistics.   \n[36] Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, and Jiaya Jia. Mini-gemini: Mining the potential of multi-modality vision language models. arXiv preprint arXiv:2403.18814, 2024.   \n[37] Bin Lin, Zhenyu Tang, Yang Ye, Jiaxi Cui, Bin Zhu, Peng Jin, Junwu Zhang, Munan Ning, and Li Yuan. Moe-llava: Mixture of experts for large vision-language models. arXiv preprint arXiv:2401.15947, 2024.   \n[38] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740\u2013755. Springer, 2014.   \n[39] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Mitigating hallucination in large multi-modal models via robust instruction tuning. In International Conference on Learning Representations, 2023.   \n[40] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744, 2023.   \n[41] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024.   \n[42] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In Advances in Neural Information Processing Systems, 2023.   \n[43] Hanchao Liu, Wenyuan Xue, Yifei Chen, Dapeng Chen, Xiutian Zhao, Ke Wang, Liping Hou, Rongjun Li, and Wei Peng. A survey on hallucination in large vision-language models. arXiv preprint arXiv:2402.00253, 2024.   \n[44] Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi. UNIFIED-IO: A unified model for vision, language, and multi-modal tasks. In International Conference on Learning Representations, 2023.   \n[45] Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, et al. Mm1: Methods, analysis & insights from multimodal llm pre-training. arXiv preprint arXiv:2403.09611, 2024.   \n[46] OpenAI. ChatGPT. https://openai.com/blog/chatgpt/, 2023.   \n[47] OpenAI. Gpt-4 technical report, 2023.   \n[48] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 8748\u20138763. PMLR, 2021.   \n[49] Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, and Kate Saenko. Object hallucination in image captioning. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4035\u20134045, 2018.   \n[50] H Rom, Neil Alldrin, J Uijlings, I Krasin, et al. The open images dataset v4. Int J Comput Vis., 128:1956\u20131981, 2020.   \n[51] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models are in-context learners. 2023.   \n[52] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liangyan Gui, Yu-Xiong Wang, Yiming Yang, Kurt Keutzer, and Trevor Darrell. Aligning large multimodal models with factually augmented RLHF, 2024.   \n[53] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. arXiv preprint arXiv:2401.06209, 2024.   \n[54] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.   \n[55] Junyang Wang, Yuhang Wang, Guohai Xu, Jing Zhang, Yukai Gu, Haitao Jia, Ming Yan, Ji Zhang, and Jitao Sang. An llm-free multi-dimensional benchmark for mllms hallucination evaluation. arXiv preprint arXiv:2311.07397, 2023.   \n[56] Lei Wang, Jiabang He, Shenshen Li, Ning Liu, and Ee-Peng Lim. Mitigating fine-grained hallucination by fine-tuning large vision-language models with caption rewrites. In International Conference on Multimedia Modeling, pages 32\u201345. Springer, 2024.   \n[57] xAI. Grok-1.5 vision preview., 2024.   \n[58] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention. In International conference on machine learning, pages 2048\u20132057. PMLR, 2015.   \n[59] Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Faisal Ahmed, Zicheng Liu, Yumao Lu, and Lijuan Wang. Unitab: Unifying text and box outputs for grounded vision-language modeling. In European Conference on Computer Vision, pages 521\u2013539. Springer, 2022.   \n[60] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems, 36, 2024.   \n[61] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023.   \n[62] Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, et al. Yi: Open foundation models by 01. ai. arXiv preprint arXiv:2403.04652, 2024.   \n[63] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive captioners are image-text foundation models. Transactions on Machine Learning Research, 2022.   \n[64] Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-Tao Zheng, Maosong Sun, et al. Rlhf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback. arXiv preprint arXiv:2312.00849, 2023.   \n[65] Bohan Zhai, Shijia Yang, Chenfeng Xu, Sheng Shen, Kurt Keutzer, and Manling Li. Halleswitch: Controlling object hallucination in large vision language models. arXiv e-prints, pages arXiv\u20132310, 2023.   \n[66] Zhiyuan Zhao, Bin Wang, Linke Ouyang, Xiaoyi Dong, Jiaqi Wang, and Conghui He. Beyond hallucinations: Enhancing lvlms through hallucination-aware direct preference optimization. arXiv preprint arXiv:2311.16839, 2023.   \n[67] Yiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun Zhang, Zhun Deng, Chelsea Finn, Mohit Bansal, and Huaxiu Yao. Analyzing and mitigating object hallucination in large vision-language models. In The Twelfth International Conference on Learning Representations, 2023.   \n[68] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023.   \n[69] Lanyun Zhu, Deyi Ji, Tianrun Chen, Peng Xu, Jieping Ye, and Jun Liu. Ibd: Alleviating hallucinations in large vision-language models via image-biased decoding. arXiv preprint arXiv:2402.18476, 2024. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Instruction for Comprehensive Description ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In generating a comprehensive description for the given visual content, we aim to obtain as detailed a description as possible, ensuring that the response fully spans the visual representation space, even though it may not be entirely feasible as discussed in 3.1. The specific prompt instruction used to describe the image contents in detail is described in Table. 5. ", "page_idx": 13}, {"type": "image", "img_path": "g6nn2AijDp/tmp/84d45abdac8e0f3d490b705f5de1d7035e9f0286be605c445f27583034937fb4.jpg", "img_caption": ["Table 5: A simple instruction prompt for generating comprehensive description. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "B Detailed Algorithm for CODE ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We describe the complete details of CODE implementation for better understanding in Algorithm. 1. ", "page_idx": 13}, {"type": "table", "img_path": "g6nn2AijDp/tmp/3de4ec054b76248102cb0abe5cbac819b01d283149b5749b981680a92f77cbfe.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "C Benchmark Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Discriminative Benchmarks ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "\u2022 POPE [35] is a widely used benchmark designed for evaluating object-level hallucination, which can be split into the three subset categories based on how to select object replacements: (i) random, randomly sampled objects (ii) popular, top- $k$ frequent objects not existing in the image, and (iii) adversarial, top- $k$ objects that have high co-occurrence. The number of images is 500 and each image has 6 questions along with the subsets, making a total of 9000 images. In this work, we only consider adversarial split, which is the most challenging subset in POPE benchmark. ", "page_idx": 13}, {"type": "text", "text": "\u2022 MMVP [53] includes 300 images with 9 different visual patterns that CLIP model [48] struggles to identify the visual differences (CLIP-paired images): Orientation and Direction $(\\pmb{\\mathcal{D}})$ , Presence of Specific Features $(\\pmb{\\Omega})$ , State and Condition $(\\frac{\\rho}{\\kappa})$ , Quantity and Count $(\\uparrow\\frac{\\mathbf{\\i}}{\\bullet})$ , Positional and Relational Context $(\\P)$ , Color and Appearance $(\\pmb{\\mathscr{G}})$ , Structural and Physical Characteristics $(\\pmb{\\mathfrak{X}}_{\\pmb{\\mathfrak{o}}}^{\\pmb{\\mathfrak{o}}})$ , Text $\\mathbf{\\Psi}(\\mathbf{A})$ , Viewpoint and Perspective $(\\pmb{\\mathbb{D}})$ . It follows multiple selection tests, but uses GPT-4 to map the model response to the answer options. \u2022 RealworldQA [57] is recently introduced benchmarks for evaluating basic real-world understanding for multi-modal models. It consists of total 765 anonymized outdoor (mostly taken from vehicles) and indoor images with multiple selection questions. ", "page_idx": 13}, {"type": "text", "text": "Generative Benchmarks ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "\u2022 LLaVA-QA90 & LLaVA-Bench (In-the-Wild) [42] consist of three subset response types for each image: (i) Conversation, which is conversation format between the user and assistant answering the vision-related questions for the given images, (ii) Detailed description, which requires detailed description for the given image scene, and (iii) Complex reasoning, which involves in-depth reasoning questions for the image. The former benchmarks sourced from COCO images (total 30 images with 90 questions), while the latter benchmarks are gathered from web for challenging domain situations (total 24 images with 60 questions). ", "page_idx": 14}, {"type": "text", "text": "\u2022 MMHal-Bench [52] is specially focused on penalizing hallucinations. It has total 96 imagequestion pairs composed with 8 question categories for 12 objects: Object attribute (Attr), Adversarial object (Adv), Comparison (Comp), Counting (Count), Spatial relation (Rel), Environment (Env), Holistic description (Hol), and Others (Other). As like in the above LLaVA-Bench benchmarks, MMHal-Bench also utilize GPT-4 to analyze and rate the model responses and score in a range of 0 to 7. ", "page_idx": 14}, {"type": "text", "text": "D Additional Token-level Case Study ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "As additional token-level case studies, we explore how the logit information changes during our CODE decoding phase in other examples. As a first example, LLaVA-NeXT [41] struggles to distinguish between Haleakala National Park and Diamond Head, both located in Hawaii, and predicts the former during inference with the vanilla decoding method. Using our CODE decoding method, as shown in Fig. 6, the information flow of \"Haleakala\" token is curbed, inducing a token inversion to \"Diamond\", which matches the ground truth word. This occurs because the logit variation is dynamically adjusted based on both token-level and distributional information. ", "page_idx": 14}, {"type": "image", "img_path": "g6nn2AijDp/tmp/c41837ef2de5e68a344a0ed7e08cbadaf99bd8d635082764c37404811fa298c3.jpg", "img_caption": [], "img_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "g6nn2AijDp/tmp/48fe21294cab16d0bd3895081fbb2f44905b3a89befa4ce678aa90e73179384d.jpg", "table_caption": ["Question:  What is the name of this famous sight in the photo? GT Answer: The famous sight in the photo is Diamond Head. ", "Figure 6: Additional token-level case study for analysis. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "In another example illustrated in Fig. 7, we show how the adaptive information constraint prevent from rewarding implausible tokens, thus suppress hallucinatory prediction during contrastive decoding. The original tokens from $\\mathrm{logit}_{v}$ predict the correct answer at the hallucinatory time step, highlighted in bold. In this case, the original prediction should be preserved, and token inversion should not occur. By our CODE decoding method, $\\beta_{t}$ dynamically controls the adaptive information constraint, so that those hallucination tokens\u2019(i,e., \"four\" and \"dragon\") logit values are cut off to $-\\infty$ and removed from candidate token pool. ", "page_idx": 14}, {"type": "text", "text": "E Further Discussion on Broader Impact ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We proposed CODE, which can be seamlessly integrated into LMMs without additional training. There is still a lot of room for progress and mitigation of hallucination issues, as our method cannot assure $100\\%$ removal of hallucinations. However, by providing more coherent and contextually accurate responses, our work can potentially be integrated into real-world applications, making user interactions with AI in customer service, education, and personal assistance more effective and satisfying in the near future. ", "page_idx": 14}, {"type": "text", "text": "Furthermore, enhanced accuracy and reliability using our method can reduce hallucinations in LMMs, improving the accuracy of AI-generated descriptions in critical fields such as autonomous driving, robotics, healthcare, and augmented reality. This advancement not only enhances practical applications but also significantly beneftis the research community working on hallucination, an area that is not yet fully explored. Our contributions can help pave the way for deeper understanding and new research directions to address these challenges, for more trustworthy AI systems. ", "page_idx": 14}, {"type": "image", "img_path": "g6nn2AijDp/tmp/34ad23fe0edcf8c2927a8846e091cae8bb617a3c20205045acf23df02a9edb76.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "g6nn2AijDp/tmp/62c143c5d106d28766895c51849816d1f83c9c86b69169f4d155de4ebf947dcb.jpg", "table_caption": ["Question: How many uncut fruits are in the image? GT Answer: There are three uncut fruits in the image. ", "Figure 7: Additional token-level case study for analysis. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "F Additional Qualitative Results ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Discriminative Capability Case Study: MMVP. ", "page_idx": 15}, {"type": "image", "img_path": "g6nn2AijDp/tmp/c9dd968c25fc3fa0a1d50897f09c12974dfb016d6797d4e410720bc2ede06101.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "g6nn2AijDp/tmp/0fc36fa9cbcdcc0d72306d374f19bedd4e54a6c68b0291db51181f33bcbe40c1.jpg", "img_caption": ["Table 6: More case studies within MMVP, using InternLM-XComposer2, InternVL 1.5 and LLaVANeXT. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Discriminative Capability Case Study: RealworldQA. ", "page_idx": 17}, {"type": "image", "img_path": "g6nn2AijDp/tmp/bd9d39d6826fd842621191699a0d253e0f0e714ccf1a54ea0d8f973fab736c39.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "g6nn2AijDp/tmp/84e247279e9198dd7fb525a35a23c2c7765130ea8ecb5d188198cd1b83fdb760.jpg", "img_caption": ["Table 7: More case studies within RealworldQA, using InternLM-XComposer2, InternVL 1.5 and LLaVA-NeXT. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Question: Describe this photo in detail. ", "page_idx": 19}, {"type": "text", "text": "Image: ", "text_level": 1, "page_idx": 19}, {"type": "image", "img_path": "g6nn2AijDp/tmp/24b212bc37530a5fa5189aa11fb1bf9b0d379595d32a41c8e866fb35de03235a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "InternLM-XComposer2 (Opera): ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The painting you\u2019re referring to is the famous \"Mona Lisa\" by Leonardo da Vinci. It is an oil painting on a poplar wood panel. The artwork depicts a woman, believed to be Lisa Gherardini, the wife of a Florentine merchant named Francesco del Giocondo. The woman is portrayed in a three-quarter view, with her left hand resting on her lap and her right hand holding a veil. She is wearing a dress with a high collar and a pearl necklace. The background features a landscape with mountains and a body of water. The painting is known for its enigmatic smile and the use of sfumato, a painting technique that creates a soft, hazy effect. The \"Mona Lisa\" is considered a masterpiece of the Italian Renaissance and is housed in the Louvre Museum in Paris, France. ", "page_idx": 19}, {"type": "text", "text": "InternLM-XComposer2 (VCD): ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The image features a painting of the renowned Mona Lisa, an oil on wood artwork. The central figure is a woman with long red hair and a headband, wearing a blue dress adorned with gold accents. Her face is captured in close-up detail, showcasing her distinct smile and enigmatic gaze. The background is a landscape painted in earth tones, adding depth to the composition. This painting is widely recognized as a masterpiece, capturing the essence of the Renaissance era with its realistic portrayal and intricate details. ", "page_idx": 19}, {"type": "text", "text": "InternLM-XComposer2 (Ours): ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The painting you\u2019re referring to is the famous \"Mona Lisa\" by Leonardo da Vinci. It is an oil painting on a wooden panel, measuring approximately $77\\mathrm{~x~}53$ centimeters. The painting is a portrait of a woman, who is the focal point of the artwork. She is depicted wearing a dress and a headscarf, with her hair styled in a manner typical of the Italian Renaissance period. The background of the painting features a landscape with mountains and a body of water, adding depth and context to the scene. The painting is known for its realistic portrayal of the woman\u2019s features, her enigmatic smile, and the subtle interplay of light and shadow. The \"Mona Lisa\" is considered a masterpiece of the High Renaissance period and is housed in the Louvre Museum in Paris, France. ", "page_idx": 19}, {"type": "text", "text": "Question: Describe this photo in detail. ", "page_idx": 20}, {"type": "text", "text": "Image: ", "text_level": 1, "page_idx": 20}, {"type": "image", "img_path": "g6nn2AijDp/tmp/204ee9de64f043f1d00ea7fc57295de3b82a2076c2d130b0622d38f056b8cc7b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "InternVL 1.5 (Opera): ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The image shows a group of star fruit, also known as carambola or tamarind, arranged and presented. These are a kind a of sweet, fizzi, and tart tasting, and are a popular fruit. There appears a few different ways they\u2019re displayed, some whole or unpeel, some are half-cut open. There is a clear visual focus and contrast, as well as a natural and organic feel to the image. ", "page_idx": 20}, {"type": "text", "text": "InternVL 1.5 (VCD): ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "This image depicts several purple-colored fruits, which appear to be durian. Durian is a tropical fruit that is known for its strong, pungent aroma and creamy, sweet flesh. In the center of the image, one of the durians is cut in half, revealing the fruit\u2019s inner flesh, which is a creamy white with a star-like pattern of seeds. The cut side of the durian shows the texture of the flesh and the arrangement of the seeds within. ", "page_idx": 20}, {"type": "text", "text": "The fruit has a distinctive appearance with a thick, spiky green outer shell that protects the flesh inside. The surface of the fruit is textured and somewhat rough, with the color transitioning from deep purple to green at the points where the spiky projections emerge. The durians are resting on a wooden surface, which provides a rustic backdrop that contrasts with the vibrant colors of the fruit. ", "page_idx": 20}, {"type": "text", "text": "In the background, there is a wicker basket containing more of these fruits, suggesting that they have been gathered for consumption or sale. The basket is made of natural materials and complements the overall natural, organic feel of the image. The lighting in the photo is soft and natural, which highlights the textures and colors of the durians and adds to the overall appeal of the image. ", "page_idx": 20}, {"type": "text", "text": "Overall, this image presents the durian fruit in a way that showcases both its visual and textural qualities, as well as its role in a natural, possibly agricultural, setting. ", "page_idx": 20}, {"type": "text", "text": "InternVL 1.5 (Ours): ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The image shows several whole and cut open mangosteens on a wooden surface. Mangosteens are tropical fruits known for their distinctive appearance and sweet, slightly acidic flavor. The fruit has a dark purple, almost black, outer skin that contrasts with the green, bulbous protrusions at the top, which are the remnants of the fruit\u2019s calyx. One of the mangosteens is cut in half, revealing the white, segmented flesh inside, which is the edible part of the fruit. The segments are arranged in a flower-like pattern around a central seed pod. The cut-open fruit provides a clear view of the fruit\u2019s structure and the contrast between the outer skin and the inner flesh. The wooden surface and the natural lighting give the image a rustic and organic feel, emphasizing the freshness of the fruit. ", "page_idx": 20}, {"type": "text", "text": "Question: Describe this photo in detail. ", "page_idx": 21}, {"type": "image", "img_path": "g6nn2AijDp/tmp/70db7f9e99f9b6d50e381979e7c68b4bf473c6e623e1758f65fd2be48331832c.jpg", "img_caption": ["Image: "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "LLaVA-NeXT (VCD): ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In the center of the image, a black bowl with vibrant red and yellow trim commands attention. It cradles a hearty serving of noodles, their creamy yellow hue contrasting with the bowl\u2019s dark color. A few pieces of meat, their rich brown tones adding to the variety of colors, are scattered throughout the bowl. Green peas peek out from under the noodles, adding a touch of freshness to the dish. To the left of the bowl, a small white cup fliled with a green sauce waits patiently. Its color is a soothing contrast to the bold colors of the bowl and its contents. In the top right corner of the image, a white plate holds a small pile of chopped vegetables. Their bright green color stands out against the plate, drawing the eye to the top right corner of the image. ", "page_idx": 21}, {"type": "text", "text": "The arrangement of these objects creates a visually pleasing scene. The bowl of noodles is centrally placed, making it the focal point of the image. The green sauce in the white cup and the plate of chopped vegetables are positioned on either side, framing the bowl and creating a balanced composition. The colors, objects, and their relative positions all contribute to a harmonious and appetizing scene. ", "page_idx": 21}, {"type": "text", "text": "LLaVA-NeXT (Ours): ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In the heart of the image, a black bowl with a vibrant red and yellow border takes center stage. It cradles a hearty serving of udon noodles, their light brown hue contrasting beautifully with the bowl\u2019s dark exterior. The noodles are not alone; they are accompanied by a generous portion of green onions, adding a pop of color and hinting at the freshness of the dish. ", "page_idx": 21}, {"type": "text", "text": "To the right of the bowl, a white cup filled with a green sauce waits patiently, ready to enhance the flavors of the udon noodles. The cup\u2019s pristine white color provides a stark contrast to the green sauce within, making it an eye-catching element in the composition. ", "page_idx": 21}, {"type": "text", "text": "In the background, a blue and white plate holds a serving of meat and green onions. The plate\u2019s colors echo those of the bowl, creating a harmonious color palette across the image. Scattered around these main elements are various condiments and utensils, each playing their part in this culinary tableau. Their presence adds depth to the image, suggesting a meal ready to be enjoyed. ", "page_idx": 21}, {"type": "text", "text": "Overall, this image captures a moment of anticipation, just before the enjoyment of a delicious meal begins. ", "page_idx": 21}, {"type": "text", "text": "Table 8: More case studies within LLaVA-Bench (In-the-Wild), using InternLM-XComposer2, InternVL 1.5 and LLaVA-NeXT. We marked the expressions where hallucinations occurred in red. ", "page_idx": 21}, {"type": "text", "text": "G Failure Cases ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "As discussed in Appendix. E, even if CODE shows competent performance along various benchmarks and LMM baselines. The hallucination cannot be $10\\bar{0}\\%$ eliminated. In this section, we attached some of failure cases to shed some light for future work direction. As illustrated in Table. 8, the baseline models fail to correct hallucination even with our CODE method. Upon closer examination of the model responses, it is evident that the hallucinatory responses tend to be biased towards language priors such as \"strawberry-flavored\" or \"holding a glass of beer\". Our approach mainly uses self-generated description as contrasting reference (i.e., close to the concept of self-correction), thus the strong assumption is on that the amateur model (comprehensive description) should generate not too much deviated responses from true answers. ", "page_idx": 22}, {"type": "text", "text": "In the failure examples, we can infer that the reliance on self-generated descriptions may not always suffice, especially when the descriptions themselves are biased or inaccurate. This indicates a need for integrating more robust mechanisms to verify and correct these biases. Additionally, enhancing the model\u2019s understanding and processing of visual content could help mitigate such issues. Future work could explore the integration of external knowledge sources and more sophisticated bias detection techniques to further reduce hallucinations and improve the overall accuracy and reliability of LMMs. ", "page_idx": 22}, {"type": "text", "text": "Image: ", "text_level": 1, "page_idx": 22}, {"type": "image", "img_path": "g6nn2AijDp/tmp/a748edbf577b8e68d1aa81cffb902602236b5b6f18775a8b0ea9664b5be5ee1e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Question:   \nIs there any strawberry-flavored yogurt in the   \nfridge? ", "page_idx": 22}, {"type": "text", "text": "Generated Text: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The image showcases the inside of a refrigerator, fliled with various food items. On the top shelf, there is a carton of milk and a container of yogurt. The middle shelf contains a bag of carrots, a package of strawberries, and a blue plastic container. The bottom shelf holds a carton of blueberries and another carton of yogurt. Additionally, there are two bottles on the door of the refrigerator. ", "page_idx": 22}, {"type": "text", "text": "InternLM-XComposer2 (Ours): ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Yes, there is a container of strawberryflavored yogurt in the fridge. ", "page_idx": 22}, {"type": "text", "text": "Image: ", "text_level": 1, "page_idx": 22}, {"type": "image", "img_path": "g6nn2AijDp/tmp/95960dcd73ebdbfb25988a253b23a2b14836be07efde964622e5689e7d5a4911.jpg", "img_caption": ["Question: What is the man doing in the image? "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Generated Text: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The image depicts a group of people gathered in a room, with some sitting and others standing. There are at least 12 people in the scene, engaged in various activities. One man is sitting at a table, holding a cell phone, while another man is sitting nearby, giving a thumbs-up gesture. There are several chairs in the room, with one near the center and another towards the right side. A dining table is also visible in the scene, with a cup placed on it. The people in the room appear to be enjoying their time together, possibly at a social event or gathering. ", "page_idx": 22}, {"type": "text", "text": "LLaVA-1.5 (Ours): ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The man in the image is sitting at a table, holding a glass of beer, and giving a thumbs-up sign. ", "page_idx": 22}, {"type": "text", "text": "Figure 8: Failure case of InterLM-XComposer2 in LLaVA-Bench (In-the-Wild) and LLaVA-1.5 in LLaVA-QA90. ", "page_idx": 22}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: They are included in this paper. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 23}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: We include discussion section for potential limitation of our work. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 23}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: This paper does not involve theoretical result to prove. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 24}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Implementation section is included, we will open the source to public. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 24}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We will open to public in next step. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 25}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Yes, we have baselines and benchmarks explanations in the manuscript. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 25}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: Our experiments do not need statistical results. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We include the computational details. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 26}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We reviewed CoE. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 26}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: We add it in discussion section. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to ", "page_idx": 26}, {"type": "text", "text": "generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 27}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 27}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: This is not relevant with our work. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 27}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: Our work do not include any license assets. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: No new assets. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 28}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: No crowd-sourcing data. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 28}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] Justification: No crowd-sourcing data. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 28}]