{"references": [{"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-00-00", "reason": "This paper is foundational for large language models (LLMs) and their capabilities, which are leveraged by the current work's multimodal models."}, {"fullname_first_author": "Xinlei Chen", "paper_title": "Microsoft coco captions: Data collection and evaluation server", "publication_date": "2015-00-00", "reason": "This paper introduces a large-scale image captioning dataset (COCO Captions), crucial for training and evaluating vision-language models, which is relevant to the current research on multimodal models."}, {"fullname_first_author": "Ari Holtzman", "paper_title": "The curious case of neural text degeneration", "publication_date": "2020-00-00", "reason": "This paper highlights the issue of neural text degeneration in LLMs, a problem directly addressed by the current work's focus on mitigating hallucinations in multimodal models."}, {"fullname_first_author": "Xiang Lisa Li", "paper_title": "Contrastive decoding: Open-ended text generation as optimization", "publication_date": "2023-07-00", "reason": "This paper introduces contrastive decoding, a technique directly used and extended by the current work for improving multimodal model outputs."}, {"fullname_first_author": "Wenliang Dai", "paper_title": "InstructBLIP: Towards general-purpose vision-language models with instruction tuning", "publication_date": "2023-00-00", "reason": "This paper introduces instruction tuning for vision-language models, a significant advancement in the field that is directly related to the current work's methodology and benchmarks."}]}