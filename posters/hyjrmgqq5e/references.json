{"references": [{"fullname_first_author": "J. Fu", "paper_title": "D4RL: Datasets for deep data-driven reinforcement learning", "publication_date": "2020-04-07", "reason": "This paper provides the benchmark datasets used for evaluating the proposed A2PO method, making it crucial to the experimental validation of the paper's claims."}, {"fullname_first_author": "S. Fujimoto", "paper_title": "Off-policy deep reinforcement learning without exploration", "publication_date": "2019-00-00", "reason": "This paper introduces a foundational offline reinforcement learning method that A2PO builds upon, influencing the design and comparison of the A2PO algorithm."}, {"fullname_first_author": "A. Kumar", "paper_title": "Conservative Q-learning for offline reinforcement learning", "publication_date": "2020-00-00", "reason": "This paper proposes a key offline RL algorithm that A2PO directly addresses the limitations of by handling mixed-quality datasets and constraint conflicts, representing a core advancement."}, {"fullname_first_author": "X. Chen", "paper_title": "Lapo: Latent-variable advantage-weighted policy optimization for offline reinforcement learning", "publication_date": "2022-00-00", "reason": "A2PO directly builds upon and improves upon this method by addressing the constraint conflict issue which is a limitation of LAPO, indicating a close relationship."}, {"fullname_first_author": "A. Nair", "paper_title": "Offline reinforcement learning with implicit Q-learning", "publication_date": "2021-00-00", "reason": "This work is another important baseline algorithm which A2PO aims to improve upon, thus the comparison provides a direct measure of A2PO's contribution."}]}