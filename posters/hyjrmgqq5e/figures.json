[{"figure_path": "hYjRmGqq5e/figures/figures_1_1.jpg", "caption": "Figure 1: A didactic experiment. (a) The visualization of the toy one-step jump task and the composition of the mixed-quality dataset. The agent starts at position 0 and can make a one-step jump a \u2208 [-10, 10] to reach a new position and receive a reward r. (b) Learning curves of A2PO and LAPO. (c) VAE-generated action distributions of A2PO and LAPO at the initial state. LAPO VAE conditions only on the state, while A2PO VAE conditions on both the state and the advantage \u03be.", "description": "This figure presents a didactic experiment to illustrate the advantage of the proposed A2PO method over LAPO.  (a) shows a simple one-step jump task with a mixed-quality dataset containing optimal, sub-optimal, and bad actions with different reward values. (b) compares the learning curves of A2PO and LAPO, demonstrating A2PO's superior performance. (c) visualizes the VAE-generated action distributions for both methods, highlighting that A2PO's VAE considers both state and advantage, unlike LAPO's state-only approach.", "section": "1 Introduction"}, {"figure_path": "hYjRmGqq5e/figures/figures_3_1.jpg", "caption": "Figure 2: An illustrative diagram of the Advantage-Aware Policy Optimization (A2PO) method.", "description": "This figure shows a flowchart of the A2PO method.  The process starts with an offline dataset of mixed quality. This dataset is fed into a Conditional Variational Autoencoder (CVAE) which disentangles the behavior policies.  The CVAE's output is conditioned on the advantage, a measure of how good an action is.  Then this information is used in the agent policy optimization stage to create an advantage-aware policy which avoids out-of-distribution issues. This is achieved by utilizing the disentangled action distributions as a constraint on the agent's policy during optimization.  The actor network is updated using advantage values derived from two Q networks and a V network. The process results in an optimal policy.", "section": "4 Methodology"}, {"figure_path": "hYjRmGqq5e/figures/figures_7_1.jpg", "caption": "Figure 1: A didactic experiment. (a) The visualization of the toy one-step jump task and the composition of the mixed-quality dataset. The agent starts at position 0 and can make a one-step jump a \u2208 [-10, 10] to reach a new position and receive a reward r. (b) Learning curves of A2PO and LAPO. (c) VAE-generated action distributions of A2PO and LAPO at the initial state. LAPO VAE conditions only on the state, while A2PO VAE conditions on both the state and the advantage \u03be.", "description": "This figure shows a didactic experiment using a simple one-step jump task to illustrate the differences between A2PO and LAPO.  Panel (a) visualizes the task and the structure of the mixed-quality dataset, highlighting different reward values for different actions. Panel (b) presents learning curves, comparing the performance of A2PO and LAPO.  Panel (c) visualizes the action distributions generated by the VAE components of each method, highlighting how A2PO conditions on both state and advantage, while LAPO only conditions on the state. This example demonstrates A2PO's ability to prioritize samples with high advantage values while considering behavior policy diversity.", "section": "1 Introduction"}, {"figure_path": "hYjRmGqq5e/figures/figures_7_2.jpg", "caption": "Figure 4: Learning curves of A2PO under different fixed advantage inputs during the test while using the original continuous advantage condition for training. Test returns are reported in Appendix E.", "description": "This figure displays the learning curves for A2PO across four different scenarios. Each scenario involves a specific task (halfcheetah or hopper) and dataset type (medium-expert or random-medium-expert).  The key aspect shown is how the agent's performance changes when it's given different fixed advantage inputs (-1.0, 0.0, and 1.0) during testing.  The curves reveal the impact of this advantage input on the learning process and final performance. Appendix E provides more detailed test return results.", "section": "5.3 Ablation Analysis"}, {"figure_path": "hYjRmGqq5e/figures/figures_7_3.jpg", "caption": "Figure 8: Comparison of our proposed A2PO method and the recent advanced AW method (LAPO) in advantage estimation for mixed-quality offline datasets (random-expert) in Gym tasks. Each data point represents an initial state-action pair in the offline dataset after applying PCA while varying shades of color indicate the magnitude of the actual return or advantage value.", "description": "This figure compares the advantage estimation of A2PO and LAPO on random-expert mixed-quality datasets for the walker2d and hopper tasks in the Gym environment.  It visualizes the initial state-action pairs using PCA, with color intensity representing the magnitude of the actual return and the advantage estimations from each method. The visualization helps to understand how well each method captures the relationship between actions, states, and their resulting returns, specifically highlighting the differences between A2PO and LAPO in advantage estimation on these more complex datasets.", "section": "5.4 Visualization"}, {"figure_path": "hYjRmGqq5e/figures/figures_8_1.jpg", "caption": "Figure 6: Compare the returns of A2PO under random-expert dataset with different high-quality data proportions \u03c3 in the Gym tasks. Detail returns are reported in Appendix H.", "description": "This figure shows the robustness of the A2PO algorithm in handling variations in the proportions of different single-quality samples. The results demonstrate that the A2PO method effectively captures and infers high-quality potential behavior policies, even with a small proportion of high-quality samples.  The performance is consistently high across different proportions, demonstrating robustness.", "section": "5.5 Robustness"}, {"figure_path": "hYjRmGqq5e/figures/figures_8_2.jpg", "caption": "Figure 1: A didactic experiment. (a) The visualization of the toy one-step jump task and the composition of the mixed-quality dataset. The agent starts at position 0 and can make a one-step jump a \u2208 [-10, 10] to reach a new position and receive a reward r. (b) Learning curves of A2PO and LAPO. (c) VAE-generated action distributions of A2PO and LAPO at the initial state. LAPO VAE conditions only on the state, while A2PO VAE conditions on both the state and the advantage \u03be.", "description": "This figure shows a didactic experiment using a toy one-step jump task to illustrate the difference between A2PO and LAPO.  Panel (a) visualizes the task and the composition of the mixed-quality dataset.  Panel (b) compares the learning curves of A2PO and LAPO, demonstrating A2PO's superior performance. Panel (c) displays the VAE-generated action distributions, highlighting how A2PO conditions on both state and advantage, unlike LAPO which only conditions on state.", "section": "1 Introduction"}, {"figure_path": "hYjRmGqq5e/figures/figures_19_1.jpg", "caption": "Figure 1: A didactic experiment. (a) The visualization of the toy one-step jump task and the composition of the mixed-quality dataset. The agent starts at position 0 and can make a one-step jump a \u2208 [-10, 10] to reach a new position and receive a reward r. (b) Learning curves of A2PO and LAPO. (c) VAE-generated action distributions of A2PO and LAPO at the initial state. LAPO VAE conditions only on the state, while A2PO VAE conditions on both the state and the advantage \u03be.", "description": "This figure shows a didactic experiment to illustrate the difference between A2PO and LAPO methods.  Panel (a) visualizes a simple \"one-step jump\" task and the structure of a mixed-quality dataset for this task. Panel (b) presents learning curves, showing A2PO's superior performance compared to LAPO. Finally, panel (c) displays the action distributions generated by the VAE in both methods, highlighting the fact that A2PO conditions on both state and advantage while LAPO only conditions on the state.", "section": "1 Introduction"}, {"figure_path": "hYjRmGqq5e/figures/figures_20_1.jpg", "caption": "Figure 9: Comparison of our proposed A2PO method and the recent advanced AW method (LAPO) in advantage estimation for mixed-quality offline datasets (random-medium-expert) in locomotion tasks. Each data point represents an initial state-action pair in the offline dataset after applying PCA while varying shades of color indicate the magnitude of the actual return or advantage value.", "description": "This figure compares the advantage estimation capabilities of A2PO and LAPO on mixed-quality datasets (random-medium-expert) from the D4RL benchmark for three locomotion tasks: halfcheetah, walker2d, and hopper.  Using PCA, the initial state-action pairs are visualized. The color intensity represents the magnitude of the actual return or estimated advantage, allowing for a visual comparison of how well each method captures the advantage in different regions of the state-action space.", "section": "5.4 Visualization"}]