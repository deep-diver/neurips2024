{"references": [{"fullname_first_author": "Anthropic", "paper_title": "Anthropic\u2019s responsible scaling policy", "publication_date": "2023-00-00", "reason": "This paper is cited as a major source for the need to evaluate models for dangerous capabilities, a central theme of the current paper."}, {"fullname_first_author": "OpenAI", "paper_title": "Preparedness", "publication_date": "2023-00-00", "reason": "This paper, alongside Anthropic's policy, establishes the importance of evaluating AI capabilities and informs the current paper's approach to evaluating hidden capabilities."}, {"fullname_first_author": "Hubinger", "paper_title": "When can we trust model evaluations?", "publication_date": "2023-00-00", "reason": "This paper discusses the challenges of evaluating AI capabilities, especially the potential for models to hide their capabilities, directly motivating the current paper's method."}, {"fullname_first_author": "Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-00-00", "reason": "This paper's finding that prompting doesn't always elicit an LLM's full capabilities is a key reason why the current paper explores alternative elicitation methods like fine-tuning."}, {"fullname_first_author": "Dragan", "paper_title": "Introducing the frontier safety framework", "publication_date": "2024-00-00", "reason": "This paper highlights the growing importance of fine-tuning for evaluating AI safety, which directly supports the current paper's focus on fine-tuning-based elicitation."}]}