[{"type": "text", "text": "Bridging Multicalibration and Out-of-distribution Generalization Beyond Covariate Shift ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jiayun Wu\u2217 ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jiashuo Liu ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Depart. of Computer Science & Tech. Tsinghua University Beijing, China 100084   \nwujy22@mails.tsinghua.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Depart. of Computer Science & Tech. Tsinghua University Beijing, China 100084 liujiashuo77@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Peng Cui Key Laboratory of Pervasive Computing, Ministry of Education Depart. of Computer Science & Tech., Tsinghua University Beijing, China 100084 cuip@tsinghua.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Zhiwei Steven Wu   \nSchool of Computer Science   \nCarnegie Mellon University   \nPittsburgh, PA 15213   \nzhiweiw@cs.cmu.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We establish a new model-agnostic optimization framework for out-of-distribution generalization via multicalibration, a criterion that ensures a predictor is calibrated across a family of overlapping groups. Multicalibration is shown to be associated with robustness of statistical inference under covariate shift. We further establish a link between multicalibration and robustness for prediction tasks both under and beyond covariate shift. We accomplish this by extending multicalibration to incorporate grouping functions that consider covariates and labels jointly. This leads to an equivalence of the extended multicalibration and invariance, an objective for robust learning in existence of concept shift. We show a linear structure of the grouping function class spanned by density ratios, resulting in a unifying framework for robust learning by designing specific grouping functions. We propose MC-Pseudolabel2, a post-processing algorithm to achieve both extended multicalibration and out-of-distribution generalization. The algorithm, with lightweight hyperparameters and optimization through a series of supervised regression steps, achieves superior performance on real-world datasets with distribution shift. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We revisit the problem of out-of-distribution generalization and establish new connections with multicalibration [17], a criterion originating from algorithmic fairness. Multicalibration is a strengthening of calibration, which only requires a predictor $f$ to be correct on average within each level set: ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\mathbb{E}[Y-f(X)\\mid f(X)]=0\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "Calibration is a relatively weak property, as it can be satisfied even by the uninformative constant predictor $f(X)=\\mathbb{E}[Y]$ that predicts the average outcome. More broadly, calibration provides only a marginal guarantee that does not extend to sub-populations. Multicalibration [17] mitigates this issue by requiring the calibration to hold over a family of (overlapping) subgroups $\\mathcal{H}$ : for all $h\\in\\mathcal H$ , ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathbb{E}[(Y-f(X))\\,h(X)\\mid f(X)]=0\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Multicalibration is initially studied as measure of subgroup fairness for boolean grouping functions $h$ , with $h(X)\\,=\\,1$ indicating $X$ is a member of group $h$ [17]. Subsequently, Gopalan et al. [14] and Kim et al. [20] adopt a broader class of real-valued grouping functions that can identify subpopulations through reweighting. The formulation of real-valued grouping function has enabled surprising connections between multicalibration and distribution shifts. Prior work [21, 37] studied how distribution shift affects the measure of multicalibration, with a focus on covariate shift where the relationship between $X$ and $Y$ remains fixed. Kim et al. [21] show that whenever the set of realvalued grouping functions $\\mathcal{H}$ includes the density ratio between the source and target distributions, a multicalibrated predictor with respect to the source remains calibrated in the shifted target distribution. ", "page_idx": 1}, {"type": "text", "text": "Our work substantially expands the connections between multicalibration and distribution shifts. At a high level, our results show that robust prediction under distribution shift can actually be facilitated by multicalibration. We extend the notion of multicalibration by incorporating grouping functions that simultaneously consider both covariates $X$ and outcomes $Y$ . This extension enables us to go beyond covariate shift and account for concept shift, which is prevalent in practice due to spurious correlation, missing variables, or confounding [31]. ", "page_idx": 1}, {"type": "text", "text": "Our contributions. Based on the introduction of joint grouping functions, we establish new connections between our extended multicalibration notion and algorithmic robustness in the general setting of out-of-distribution generalization, where the target distribution to assess the model is different from the source distribution to learn the model. ", "page_idx": 1}, {"type": "text", "text": "1. We first revisit the setting of covariate shift and show multicalibration implies Bayes optimality under covariate shift, provided a sufficiently rich class of grouping functions. Then, in the setting of concept shifts, we show the equivalence of multicalibration and invariance [2], a learning objective to search for a Bayes optimal predictor $\\mathbb{E}[Y|\\Phi(X)]$ under a representation over features $\\Phi(X)$ , even though $\\mathbb{E}[Y|X]$ is different across target distributions. We show correspondence between an invariant representation $\\Phi(X)$ and a multicalibrated predictor $\\mathbb{E}[Y|\\Phi(X)]$ , with a grouping function class containing all density ratios of target distributions and the source distribution. ", "page_idx": 1}, {"type": "text", "text": "2. As part of our structural analysis of the new multicalibration concept, we investigate the maximal grouping function class that allows for a nontrivial multicalibrated predictor. For traditional covariatebased grouping functions, the Bayes optimal predictor $f(X)={\\bar{\\mathbb{E}}}[Y|X]$ is always multicalibrated, which is no longer the case for joint grouping functions. We show the maximal grouping function class is a linear space spanned by the density ratio of the target distributions where the predictor is invariant. As a structural characterization of distribution shift, this leads to an efficient parameterization of the grouping functions by linear combination of a spanning set of density ratios. The spanning set can be flexibly designed to incorporates implicit assumptions of various methodologies for robust learning, including multi-environment learning [39] and hard sample learning [29]. ", "page_idx": 1}, {"type": "text", "text": "3. We devise a post-processing algorithm to multicalibrate predictors and simultaneously producing invariant predictors. As a multicalibration algorithm, we prove its convergence under Gaussian distributions of data and certify multicalibration upon convergence. As a robust learning algorithm, the procedure is plainly supervised regression with respect to models\u2019 hypothesis class and grouping function class, introducing an overhead of linear regression. This stands out from heavy optimization techniques for out-of-distribution generalization, such as bi-level optimization [12, 30] and multi-objective learning [1, 2, 24], which typically involves high-order gradients [36]. The algorithm introduces no extra hyperparameters. This simplifies model selection, which is a significant challenge for out-of-distribution generalization since validation is unavailable where the model is deployed [15]. Under the standard model selection protocol of DomainBed [15], the algorithm achieves superior performance to existing methods in real-world datasets with concept shift, including porverty estimation [44], personal income prediction [7] and power consumption [32, 33] prediction. ", "page_idx": 1}, {"type": "text", "text": "2 Multicalibration and Bayes Optimality under Covariate Shift ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Multicalibration with Joint Grouping Functions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We consider prediction tasks where covariates are denoted by a random vector $X\\in\\mathcal{X}$ and the target by $Y\\in\\mathcal{Y}$ . Lowercase $x,y$ denote the specific values of these random variables. Predictors are defined as real-valued functions $f:\\mathcal X\\to\\mathcal Y$ . Our theoretical analysis focuses on the setting where $y=[0,1]$ . In this context, we propose a new definition of $\\ell_{2}$ approximate multicalibration with joint grouping functions. ", "page_idx": 2}, {"type": "text", "text": "Definition 2.1 (Multicalibration with Joint Grouping Functions). For a probability measure $P(X,Y)$ and a predictor $f$ , let $\\mathcal{H}\\,\\subset\\,\\mathbb{R}^{\\mathcal{X}\\times\\mathcal{Y}}$ be a real-valued grouping function class. We say that $f$ is $\\alpha$ -approximately $\\ell_{2}$ multicalibrated w.r.t. $\\mathcal{H}$ and $P$ if for all $h\\in\\mathcal H$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\nK_{2}(f,h,P)=\\int\\Big(\\mathbb{E}_{P}\\left[h(X,Y)(Y-v)\\big|f(X)=v\\right]\\Big)^{2}d P_{f(X)}(v)\\leq\\alpha.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "$P_{f(X)}(v)=P(f^{-1}(v))$ is the pushforward measure. We say $f$ is $\\alpha$ -approximately calibrated if $\\mathcal{H}$ includes the constant function $h\\equiv1$ . We say $f$ is multicalibrated (calibrated) for $\\alpha\\,=\\,0$ . If the grouping function is defined on $X$ , which implies $h(x,y_{1})=h(x,y_{2})$ for any $x\\in\\mathscr{X}$ and $y_{1},y_{2}\\in\\mathcal{Y}$ , we abbreviate $h(x,\\cdot)$ by $h(x)$ . ", "page_idx": 2}, {"type": "text", "text": "Our definition generalizes several notions of (multi)calibration in the literature by specific choices of grouping functions. For example, $K_{2}(f,1,P)$ recovers the overall calibration error in the case of a constant grouping function $h\\equiv1$ . For boolean grouping functions defined on $X$ [17], $K_{2}(f,h,P)$ computes the calibration error of the subgroup with $\\bar{h}(\\bar{x})=1$ . For real-valued grouping functions defined on $X$ [14, 20], $K_{2}(f,h,P)$ evaluates a reweighted calibration error, whose weights $h(x)$ are proportional to the likelihood of a sample belonging to the subgroup. Furthermore, we propose an extended domain of grouping functions defined on covariates and outcomes jointly, for which the Bayes optimal predictor $\\mathbb{E}[Y|x]$ may not be multicalibrated, in contrast to all existing multicalibration frameworks with $X$ -based grouping functions. Multicalibration with joint groupings thus implies a distinct learning objective from accuracy, which we will characterize as invariance in section 3. ", "page_idx": 2}, {"type": "text", "text": "Example 2.2 (Multicalibration Does Not Imply Bayes Optimality). Consider covariates $X\\,=$ $(X_{1},\\Bar{X_{2}})^{T}$ and an outcome $Y$ generated by the following structural equations: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{Y=X_{1}+\\epsilon_{1}.}}\\\\ {{X_{2}=Y+\\epsilon_{2}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "$X_{1},\\ \\epsilon_{1}$ , $\\epsilon_{2}$ are independent gaussian variables with zero mean, and variances $\\mathbb{E}[\\epsilon_{1}^{2}]~=~\\sigma_{1}^{2}$ and $\\mathbb{E}[\\epsilon_{2}^{2}]~=~\\sigma_{2}^{2}$ . For a singleton grouping function class containing $h(x,y)\\;=\\;y\\,-\\,x_{2}\\,$ , the Bayes optimal predictor $\\begin{array}{r}{f(x)~=~\\mathbb{E}[Y|x]~=~\\frac{\\sigma_{2}^{2}}{\\sigma_{1}^{2}+\\sigma_{2}^{2}}x_{1}\\,+\\,\\frac{\\sigma_{1}^{2}}{\\sigma_{1}^{2}+\\sigma_{2}^{2}}x_{2}}\\end{array}$ is not multicalibrated because $\\begin{array}{r}{\\mathbb{E}[h(X,Y)(Y\\ -\\ f(X))]\\,=\\,\\frac{\\sigma_{1}^{2}\\sigma_{2}^{2}}{\\sigma_{1}^{2}+\\sigma_{2}^{2}}\\ \\ne\\,0}\\end{array}$ . However, $g(x)\\,=\\,x_{1}$ is multicalibrated since $\\begin{array}{r}{\\mathbb{E}[h(X,Y)(Y-g(X))|g(X)]=-\\mathbb{E}[\\epsilon_{2}^{'}\\epsilon_{1}|{\\tilde{X}}_{1}]=0}\\end{array}$ 0. ", "page_idx": 2}, {"type": "text", "text": "While the Bayes optimal predictor is always multicalibrated for covariate-based grouping functions, it may not be multicalibrated for grouping functions that depend on the outcome. In fact, there may be no predictor that achieves multicalibration in such cases. For example, when we consider a grouping function class that includes both $h\\equiv1$ and $h(x,y)\\,=\\,y$ , a multicalibrated predictor $f$ satisfies $f(X)=\\mathbb{E}[Y|f(X)]$ for $h\\equiv1$ , and $\\mathbb{E}[Y^{2}|f(X)]=\\mathbb{E}[Y\\dot{f}(X)|f(X)]=(\\mathbb{E}[Y\\ddot{|}f(X)])^{2}$ for $h(x,y)\\,=\\,y$ . This implies $\\operatorname{Var}[Y|f(X)]\\,=\\,{\\bar{0}}$ , which is impossible for regression with label noise. Therefore, we study the structure of the maximal grouping function class that allows for a multicalibrated predictor in section 4. ", "page_idx": 2}, {"type": "text", "text": "Most importantly, multicalibration with joint grouping functions is useful for capturing more general distribution shifts. By interpreting multicalibration error as calibration error reweighted by grouping functions, it quantifies the maximal calibration error for all subgroups associated with grouping functions in $\\mathcal{H}$ . If grouping functions are defined on $X$ , only the covariate distribution $P(X)$ distinguishes between subgroups. In contrast, the joint distribution $P(X,Y)$ differentiates subgroups for joint grouping functions. We will discuss multicalibration with covariate-based grouping functions in the next sub-section and joint grouping functions in section 3. ", "page_idx": 2}, {"type": "text", "text": "2.2 Multicalibration Implies Bayes Optimality under Covariate Shift ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Settings of Out-of-distribution Generalization. We characterize distribution shift by an uncertainty set of absolutely continuous probability measures, denoted by $\\mathcal{P}(X,Y)$ , where there is an accessible source measure $P_{S}\\in\\mathcal{P}$ and unknown target measure $P_{T}\\in\\mathcal{P}$ . We use capital letters such as $P$ to denote a single probability measure and lowercase letters such as $p$ to denote its probability density function. A predictor $f$ is learned in the source distribution $P_{S}$ and assessed in the target distribution $P_{T}$ . Given a loss function $\\ell:\\mathcal{V}\\times\\mathcal{V}\\to\\mathbb{R}$ , we evaluate the average risk of a predictor $f$ w.r.t. a probability measure $P$ , defined by $R_{P}(f):=\\mathbb{E}_{P}[\\ell(f(X),Y)]$ . We focus on $\\ell(\\bar{y},y)=(\\hat{y}-y)^{2}$ in our theoretical analyses. ", "page_idx": 3}, {"type": "text", "text": "In this subsection we focus on grouping functions $h(x)$ defined on covariates. We will prove approximately multicalibrated predictors simultaneous approaches Bayes optimality in each target distribution with covariate shift, bridging the results of Kim et al. [21] and Globus-Harris et al. [13]. To recap, Kim et al. [21] studies multicalibration under covariate shift and shows that a multicalibrated predictor remains calibrated in target distribution for a sufficiently large grouping function class. Further, it is shown that multicalibration predictors remain multicalibrated under covariate shift [21, 37], assuming the grouping function class $\\mathcal{H}$ is closed under some transformation by density ratios (Assumption 2.3.1). Second, Globus-Harris et al. [13] shows multicalibration implies Bayes optimal accuracy [13], assuming $\\mathcal{H}$ satisfies a weak learning condition (Assumption 2.3.2). Detailed discussion on other related works is deferred to section A in the appendix. ", "page_idx": 3}, {"type": "text", "text": "Assumption 2.3 (Sufficiency of Grouping Function Class (informal, see Assumption F.1)). ", "page_idx": 3}, {"type": "text", "text": "1. (Closure under Covariate Shift) For a set of probability measures ${\\mathcal{P}}(X)$ containing the source measure $P_{S}(X)$ , $h\\in\\mathcal H$ implies $p/p_{S}\\cdot h\\in\\mathcal{H}$ for any density function $p$ of distributions in $\\mathcal{P}$ . ", "page_idx": 3}, {"type": "text", "text": "2. $((\\gamma,\\rho)$ -Weak Learning Condition) For any $P\\in{\\mathcal{P}}(X)P_{S}(Y|X)\\equiv\\{P^{\\prime}(X)P_{S}(Y\\mid X):P^{\\prime}\\in{\\mathcal{P}}\\}$ with the source measure $P_{S}(Y|X)$ , and every subset $G\\subset\\mathcal{X}$ with $P(X\\in G)>\\rho,$ , if the Bayes optimal predictor $\\mathbb{E}_{P}[Y|X]$ has lower risk than the constant predictor $\\mathbb{E}_{P}[Y|X\\in G]$ by a margin $\\gamma_{;}$ , there exists a predictor $h\\in\\mathcal H$ that is also better than the constant predictor with the margin $\\gamma$ . ", "page_idx": 3}, {"type": "text", "text": "Theorem 2.4 (Risk Bound under Covariate Shift). For a source measure $P_{S}(X,Y)$ and a set of probability measures ${\\mathcal{P}}(X)$ containing $P_{S}(X)$ , given a predictor $f:\\mathcal{X}\\to[0,1]$ with finite range $m:=|R a n g e(f)|$ , consider a grouping function class $\\mathcal{H}$ closed under affine transformation and satisfying Assumption 2.3 with $\\rho=\\gamma/m$ . If $f$ is $\\frac{\\gamma^{6}}{256m^{2}}$ -approximately $\\ell_{2}$ multicalibrated w.r.t $P_{S}$ and $\\mathcal{H}_{1}:=\\left\\{h\\in\\mathcal{H}:\\operatorname*{max}_{x\\in\\mathcal{X}}h(x)^{2}\\leq1\\right\\}$ , then for any target measure $P_{T}\\in{\\mathcal{P}}(X)P_{S}(Y|X),$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\nR_{P_{T}}(f)\\leq\\operatorname*{inf}_{f^{*}:\\mathcal{X}\\to[0,1]}R_{P_{T}}(f^{*})+3\\gamma.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Remark 2.5. Following prior work in multicalibration $[l3,\\,37],$ , we study functions $f$ with finite cardinality, which can be obtained by discretization. ", "page_idx": 3}, {"type": "text", "text": "3 Multicalibration and Invariance under Concept Shift ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Theorem 2.4 shows multicalibration implies Bayes optimal accuracy for target distributions under covariate shift. However, in practical scenarios, there are both marginal distribution shifts of covariates $(X)$ and concept shift of the conditional distributions $(Y|X)$ . Concept shift is especially prevalent in tabular data due to missing variables and confounding [31]. In order to go beyond covariate shift, we will focus on grouping functions defined on covariates and outcomes jointly. We show that multicalibration notion w.r.t. joint grouping functions is equivalent to invariance, a criterion for robust prediction under concept shift. Extending the robustness of multicalibration to general shift is non-trivial. The fundamental challenge is that there is no shared predictor that is generally optimal in each target distribution because the Bayes optimal predictor varies for different $Y|X$ distributions. As a first step, we show multicalibrated predictors w.r.t. joint grouping functions are robust as they are optimal over any post-processing functions in each target distribution. ", "page_idx": 3}, {"type": "text", "text": "Theorem 3.1 (Risk Bound under Concept Shift). For a set of absolutely continuous probability measures $\\mathcal{P}(X,Y)$ containing the source measure $P_{S}(X,Y)$ , consider a predictor $f:\\bar{\\boldsymbol{\\chi}}\\rightarrow[0,1]$ . Assume the grouping function class $\\mathcal{H}$ satisfies the following condition: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{H}\\supset\\left\\{h(x,y)=\\frac{p(x,y)}{p_{S}(x,y)}\\Big|P\\in\\mathcal{P}(X,Y)\\right\\}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "If $f$ is $\\alpha$ -approximately $\\ell_{2}$ multicalibrated w.r.t. $\\mathcal{H}$ and $P_{S}$ , then for any measure $P\\in{\\mathcal{P}}(X,Y)$ , ", "page_idx": 4}, {"type": "equation", "text": "$$\nR_{P}(f)\\leq\\operatorname*{inf}_{g:[0,1]\\to[0,1]}R_{P}(g\\circ f)+2{\\sqrt{\\alpha}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The theorem shows an approximately multicalibrated predictor on the source almost cannot be improved by post-processing for each target distribution. To ensure such robustness, the grouping function class must include all density ratios between target and source measures, which are functions over $\\mathcal X\\times\\mathcal X$ . This characterization of robustness in terms of post-processing echoes with Invariant Risk Minimization (IRM) [2], a paradigm for out-of-distribution generalization with $Y|X$ shift. However, their analysis focuses on representation learning. ", "page_idx": 4}, {"type": "text", "text": "Definition 3.2 (Invariant Predictor). Consider data selected from multiple environments in the set $\\mathcal{E}$ where the probability measure in an environment $e\\in{\\mathcal{E}}$ is denoted by $P_{e}(X,Y)$ . Denote the representation over covariates by a measurable function $\\Phi(x)$ . We say that $\\Phi$ elicits an $\\alpha$ -approximately invariant predictor $g^{\\ast}\\circ\\Phi$ across $\\mathcal{E}$ if there exists $a$ function $g^{\\ast}\\in\\mathcal{G}:=\\{g:s u p p(\\Phi)\\to[0,1]\\}$ such that for all $e\\in{\\mathcal{E}}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\nR_{P_{e}}(g^{*}\\circ\\Phi)\\leq\\operatorname*{inf}_{g\\in\\mathcal{G}}R_{P_{e}}(g\\circ\\Phi)+\\alpha.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Remark 3.3. $(I)$ Predictors in $\\mathcal{G}$ take a representation $\\Phi$ extracted from the covariates as input. For a general predictor $f(x)$ , if we take $\\Phi(x){\\bar{=}}\\,f(x)$ and $g^{*}$ as a\u221an identity function, Equation 5 reduces to the form of Equation 4. Therefore, $f$ in Equation $^{4}$ is a $2\\sqrt{\\alpha}$ -approximately invariant predictor across environments collected from the uncertainty set $\\mathcal{P}$ . (2) We give an approximate definition of invariant predictors, which recovers the original definition $I2J$ when $\\alpha=0$ . In this case, there exists a shared Bayes optimal predictor $g^{\\star}$ across environments, taking $\\Phi$ as input. This implies $\\mathbb{E}_{e_{1}}[Y|\\Phi]=\\mathbb{E}_{e_{2}}[Y|\\Phi]$ almost surely for any $e_{1},e_{2}$ . ", "page_idx": 4}, {"type": "text", "text": "IRM searches for a representation such that the optimal predictors upon the representation are invariant across environments. Motivated from causality, the interaction between outcomes and their causes are also assumed invariant, so IRM learns a representation of causal variables for stable prediction. We extend Theorem 3.1 to representation learning and prove equivalence between multicalibrated and invariant predictors. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.4 (Equivalence of Multicalibration and Invariance). Assume samples are drawn from an environment $e\\in{\\mathcal{E}}$ with a prior $P_{S}(e)$ such that $\\begin{array}{r}{\\sum_{e\\in\\mathcal{E}}P_{S}(e)=1}\\end{array}$ and $P_{S}(e)>0$ . The overall population satisfies $\\begin{array}{r}{P_{S}(X,Y)=\\sum_{e\\in\\mathcal{E}}P_{e}(X,Y)P_{S}(e)}\\end{array}$ where $P_{e}(X,Y)$ is the environment-specific absolutely continuous measure. With a measurable function $\\Phi(x)$ , define a function class $\\mathcal{H}$ as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{H}:=\\left\\{h(x,y)=\\frac{p_{e}(x,y)}{p_{S}(x,y)}\\Big|e\\in\\mathcal{E}\\right\\}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "1. If there is a bijection $g^{\\star}:s u p p(\\Phi)\\rightarrow[0,1]$ such that $g^{\\star}\\circ\\Phi$ is $\\alpha$ -approximately $\\ell_{2}$ multicalibrated w.r.t. $\\mathcal{H}$ and $P_{S}$ , then $\\Phi$ elicits an $2\\sqrt{\\alpha}$ -approximately invariant predictor $g^{\\star}\\circ\\Phi$ across $\\mathcal{E}$ . ", "page_idx": 4}, {"type": "text", "text": "2. If there is $g^{\\star}\\,:\\,s u p p(\\Phi)\\,\\rightarrow\\,[0,1]$ such that $\\Phi$ elicits an $\\alpha$ -approximately invariant predictor $g^{\\star}\\circ\\Phi$ across $\\mathcal{E}$ , then $g^{\\star}\\circ\\Phi$ is $\\sqrt{\\alpha/D}$ -approximately $\\ell_{2}$ multicalibrated w.r.t. $\\mathcal{H}$ and $P_{S}$ , where $D=\\operatorname*{min}_{e\\in\\mathcal{E}}{P_{S}(e)}$ . ", "page_idx": 4}, {"type": "text", "text": "Remark 3.5. $(I)$ In the first statement, assuming $g^{\\star}$ is a bijection avoids degenerate cases where $\\Phi$ contains redundant information. For example, every predictor $g^{\\star}\\circ\\Phi(X)$ upon representation $\\Phi$ equals $g^{\\star}(\\Phi)\\circ\\mathbb{I}(X)$ upon representation $X$ . Confining $g^{\\star}$ to bijections ensures some unique decomposition into predictors and representations. (2) Wald et al. [41] proves equivalence between exact invariance and simultaneous calibration in each environment. We strengthen their result to show multicalibration on a single source distribution suffices for invariance. Moreover, our results can be directly extended beyond their multi-environment setting to a general uncertainty set of target distributions, by the mapping between grouping functions and density ratios. Further, our theorem is established for both exact and approximate invariance. ", "page_idx": 4}, {"type": "text", "text": "The theorem bridges approximate multicalibration with approximate invariance for out-of-distribution generalization beyond covariate shift. The equivalence property indicates that the density ratios of target and source distributions constitute the minimal grouping function class required for robust prediction in terms of invariance. ", "page_idx": 4}, {"type": "text", "text": "4 Structure of Grouping Function Classes ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Section 3 inspires one to construct richer grouping function classes for stronger generalizability. However, fewer predictors are multicalibrated to a rich function class, and a multicalibrated predictor may not exist at all, as illustrated by the example in section 2.1. In this section, we first study the maximal grouping function class that is feasible for a multicalibrated predictor. Then, we will leverage our structural results to inform the design of grouping functions. ", "page_idx": 5}, {"type": "text", "text": "4.1 Maximal Grouping Function Space ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We focus on continuous grouping functions defined on a compact set $\\mathcal{X}\\times\\mathcal{Y}\\subset\\mathbb{R}^{d+1}$ , i.e., $h\\in$ $C(\\mathcal{X}\\times\\mathcal{Y})$ , and consider absolutely continuous probability measures supported on $\\mathcal{X}\\times\\mathcal{Y}$ with continuous density functions. Our first proposition shows that the maximal grouping function class for any predictor is a linear space. ", "page_idx": 5}, {"type": "text", "text": "Proposition 4.1 (Maximal Grouping Function Class). Given an absolutely continuous probability measure $P_{S}(X,Y)$ and a predictor $f:\\mathcal{X}\\to[0,1]$ , define the maximal grouping function class that $f$ is multicalibrated with respect to: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{H}_{f}:=\\{h\\in C(\\mathcal{X}\\times\\mathcal{Y}):K_{2}(f,h,P_{S})=0\\}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Then $\\mathcal{H}_{f}$ is a linear space. ", "page_idx": 5}, {"type": "text", "text": "In the following, we further analyze the spanning set of maximal grouping function classes for nontrivial predictors which are at least calibrated. ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.2 (Spanning Set). Consider an absolutely continuous probability measure $P_{S}(X,Y)$ and a calibrated predictor $f:\\mathcal{X}\\to[0,1]$ . Then its maximal grouping function class $\\mathcal{H}_{f}$ is given by: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{H}_{f}=\\mathrm{span}\\left\\{\\frac{p(x,y)}{p_{S}(x,y)}:p\\,i s\\;c o n t i n u o u s\\;a n d\\;R_{P}(f)=\\operatorname*{inf}_{g:[0,1]\\to[0,1]}R_{P}(g\\circ f)\\right\\}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "A predictor\u2019s maximal grouping function class is spanned by density ratios of target distributions where the predictor is invariant. Correspondingly, Theorem 3.1 gives the minimal grouping function class, comprised of density ratios between target and source distributions, in order to ensure $f(x)$ is an invariant predictor. In contrast, Theorem 4.2 states the maximal grouping function class for $f(x)$ is exactly the linear space spanned by those density ratios. Next, we further investigate sub-structures of the maximal grouping function class. We focus on the representation learning setting of IRM. ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.3 (Decomposition of Grouping Function Space). Consider an absolutely continuous probability measure $P_{S}(X,Y)$ and a measurable function $\\Phi:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{d_{\\Phi}}$ with $d_{\\Phi}\\in Z^{+}$ . We define the Bayes optimal predictor over $\\Phi$ as $f_{\\Phi}(x)={\\dot{\\mathbb{E}}}_{P_{S}}[Y|\\Phi(x)]$ . We abbreviate $\\mathcal{H}_{f_{\\Phi}}$ with $\\mathcal{H}_{\\Phi}$ . Then $\\mathcal{H}_{\\Phi}$ can be decomposed as a Minkowski sum of $\\mathcal{H}_{1,\\Phi}+\\mathcal{H}_{2,\\Phi}$ . ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{H}_{1,\\Phi}=\\operatorname{span}\\left\\{\\displaystyle\\frac{p(\\Phi,y)}{p_{S}(\\Phi,y)}:p\\,i s\\,c o n t i n u o u s\\,a n d\\,R_{P}(f_{\\Phi})=\\operatorname*{inf}_{g:[0,1]\\to[0,1]}R_{P}(g\\circ f_{\\Phi})\\right\\}.}\\\\ &{\\mathcal{H}_{2,\\Phi}=\\operatorname{span}\\left\\{\\displaystyle\\frac{p(x|\\Phi,y)}{p_{S}(x|\\Phi,y)}:\\,p\\,i s\\,c o n t i n u o u s\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "1. If a predictor $f$ is multicalibrated with $\\mathcal{H}_{1,\\Phi}$ , then $R_{P_{S}}(f)\\le R_{P_{S}}(f_{\\Phi})$ . ", "page_idx": 5}, {"type": "text", "text": "2. $f_{\\Phi}$ is an invariant predictor elicited by $\\Phi$ across a set of environments $\\mathcal{E}$ where $P_{e}(\\Phi,Y)\\,=$ $P_{S}(\\Phi,Y)$ for any $e\\in{\\mathcal{E}}$ . If a predictor $f$ is multicalibrated with $\\mathcal{H}_{2,\\Phi}$ , then $f$ is also an invariant predictor across $\\mathcal{E}$ elicited by some representation. ", "page_idx": 5}, {"type": "text", "text": "Remark 4.4. $\\mathcal{H}_{1,\\Phi}$ and $\\mathcal{H}_{2,\\Phi}$ contain functions defined on $x$ $\\langle,\\Phi(x),y$ which can both be rewritten as functions on $x,y$ by variable substitution. Thus, $\\mathcal{H}_{1,\\Phi},\\mathcal{H}_{2,\\Phi}$ are still subspaces of grouping functions. $\\mathcal{H}_{1,\\Phi}$ is spanned by the density ratio of $P(\\Phi,Y)$ where the Bayes optimal predictor over $\\Phi$ must be invariant on the distribution of $P$ . $\\mathcal{H}_{2,\\Phi}$ is spanned by general density ratio of $P(X|\\Phi,Y)$ . ", "page_idx": 5}, {"type": "text", "text": "Multicalibration w.r.t. $\\mathcal{H}_{1,\\Phi}$ ensures at least the accuracy of the Bayes optimal predictor on $\\Phi$ , and multicalibration w.r.t. $\\mathcal{H}_{2,\\Phi}$ ensures at least the invariance of this predictor. However, we show in the following proposition that sizes of two subspaces are negatively correlated. When $\\Phi$ is a variable selector, $\\mathcal{H}_{1,\\Phi}$ expands with more selected covariates while $\\mathcal{H}_{2,\\Phi}$ shrinks. By choosing a combination of $\\mathcal{H}_{1,\\Phi}$ and $\\mathcal{H}_{2,\\Phi}$ , we strike a balance between accuracy and invariance of the multicalibrated predictor. ", "page_idx": 5}, {"type": "text", "text": "Proposition 4.5 (Monotonicity). Consider $X\\,\\in\\,\\mathbb{R}^{d}$ which could be sliced as $X=(\\Phi,\\Psi)^{T}$ and $\\Phi=(\\Lambda,\\Omega)^{T}$ . Define $\\mathcal{H}_{1,\\Phi}^{\\prime}:=\\{h(\\Phi(x))\\in C(\\mathcal{X}\\times\\mathcal{Y})\\}$ , with $\\mathcal{H}_{1,\\Phi}^{\\prime}\\subset\\mathcal{H}_{1,\\Phi}$ . $\\mathcal{H}_{1,X}^{\\prime}$ and $\\mathcal{H}_{1,\\Lambda}^{\\prime}$ are similarly defined. We have: ", "page_idx": 6}, {"type": "text", "text": "1. $\\mathcal{H}_{1,X}^{\\prime}\\supset\\mathcal{H}_{1,\\Phi}^{\\prime}\\supset\\mathcal{H}_{1,\\Lambda}^{\\prime}\\supset\\mathcal{H}_{1,\\emptyset}^{\\prime}=\\{C\\}.$ .   \n2. $\\{C\\}=\\mathcal{H}_{2,X}\\subset H_{2,\\Phi}\\subset\\mathcal{H}_{2,\\Lambda}\\subset\\mathcal{H}_{2,\\emptyset}$ .   \n$C$ is a constant value function. ", "page_idx": 6}, {"type": "text", "text": "4.2 Design of Grouping Function Classes ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The objective of a robust learning method can be represented by a tuple consisting of an assumption about the boundary of distribution shift and a metric of robustness. Multicalibration is equivalent to invariance as a metric of robustness, while the grouping function class provides a unifying view for assumptions over potential distribution shift. Given any uncertainty set of target distributions $\\mathcal{P}$ , Theorem 4.2 implies an efficient and reasonable construction of grouping functions as linear combinations of density ratios from $\\mathcal{P}$ . We implement two designs of grouping functions for the learning setting with and without environment annotations respectively. ", "page_idx": 6}, {"type": "text", "text": "From Environments If samples are drawn from multiple environments and the environment annotations are available, we assume the uncertainty set as the union of each environment\u2019s distribution $P_{e}$ . This completely recovers IRM\u2019s objective, but we approach it with a different optimization technique in the next section. Taking pooled data as the source $S$ , density ratios spanning the grouping function class are $\\iota_{e}(x,y)=p_{e}(\\bar{x_{}^{}}y)/p_{S}(x,y)=p_{S}(e|x,y)/p_{S}(e)$ ), where $p_{S}(e|x,y)$ is estimated by an environment classifier. Then a grouping function can be represented as a linear combination of $h_{e}$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\nh(x,y)=\\sum_{e\\in\\mathcal{E}}\\lambda_{e}p_{S}(e|x,y),\\;\\;\\lambda_{e}\\in\\mathbb{R}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "From Hard Samples When data contains latent sub-populations without annotations, the uncertainty set can be constructed by identifying sub-populations. Hard sample learning [27, 28, 29] suggests the risk is an indicator for sub-population structures. Samples from the minority sub-population $M$ are more likely to have high risks. For example, JTT [29] identified the minority subgroup using a risk threshold of a trained predictor $f_{i d}$ . We adopt a continuous grouping by assuming $P_{S}(\\bar{X},Y\\in M)\\propto(f_{i d}(X)-Y)^{2}$ . We construct the uncertainty set as the union of the source $S$ and minority sub-population $M$ , resulting in a grouping function represented as: ", "page_idx": 6}, {"type": "equation", "text": "$$\nh(x,y)=\\lambda_{M}(f_{i d}(x)-y)^{2}+\\lambda_{S},\\ \\ \\lambda_{M},\\lambda_{S}\\in\\mathbb{R}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Another design utilizing Distributionally Robust Optimization\u2019s assumption [10] is in section $\\mathbf{B}$ . ", "page_idx": 6}, {"type": "text", "text": "5 MC-PseudoLabel: An Algorithm for Extended Multicalibration ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we introduce an algorithm for multicalibration with respect to joint grouping functions. Simultaneously, the algorithm also provides a new optimization paradigm for invariant prediction under distribution shift. The algorithm, called MC-PseudoLabel, post-processes a trained model by supervised learning with pseudolabels generated by grouping functions. As shown in Algorithm 1, given a predictor function class $\\mathcal{F}$ and a dataset $D$ with an empirical distribution $\\hat{P}_{D}(X,Y)$ , a regression oracle $A$ solves the optimization: $A_{\\mathcal{F}}(D)\\,=\\,\\arg\\operatorname*{min}_{f\\in\\mathcal{F}}R_{\\hat{P}_{D}}(f)$ . We take as input a model $f_{0}$ , possibly trained by Empirical Risk Minimization. $f_{0}$ has a finite range following conventions of prior work in multicalibration [13]. For continuous predictors, we discretize the model output and introduce a small rounding error (see section C). For each iteration, the algorithm performs regression with grouping functions on each level set of the model. The prediction of grouping functions rectify the uncalibrated model and serves as pseudolabels for model updates. ", "page_idx": 6}, {"type": "text", "text": "Since we regress $Y$ with grouping functions defined on $Y$ , a poor design of groupings violating Theorem 4.2 can produce trivial outputs. For example, if grouping functions contain $h(x,y)=y$ , then $e r r_{t-1}-e\\tilde{r}r_{t}$ never decreases and the algorithm outputs $f_{0}$ , because there does not exist a multicalibrated predictor. However, the algorithm certifies a multicalibrated output if it converges. ", "page_idx": 6}, {"type": "text", "text": "Theorem 5.1 (Certified Multicalibration). In Algorithm $^{\\,l}$ , for $\\alpha,B>0,$ , $\\begin{array}{r}{i f e r r_{t-1}-e\\tilde{r}r_{t}\\leq\\frac{\\alpha}{B}}\\end{array}$ , the output $f_{t-1}^{\\prime}(x)$ is $\\alpha$ -approximately $\\ell_{2}$ multicalibrated w.r.t. $\\mathcal{H}_{B}=\\{h\\in\\mathcal{H}:\\operatorname*{sup}h(x,y)^{2}\\leq B\\}$ . ", "page_idx": 6}, {"type": "text", "text": "Require: A dataset $D=(D_{x},D_{y})$ , a grouping function class $\\mathcal{H}$ , a predictive function class $\\mathcal{F}$ .   \n1: $t\\leftarrow0$ ;   \n2: $f_{0}\\gets$ Initialization; {For example, models trained with ERM.}   \n3: $m\\leftarrow|\\mathrm{Range}(\\mathrm{Discretize}(f_{0}))|$ ;   \n4: repeat   \n5: $f_{t}^{\\prime}\\gets\\mathrm{Round}(f_{t};m):=\\arg\\operatorname*{min}_{v\\in[1/m]}|f_{t}(x)-v$ |;   \n6: $e r r_{t}=\\mathbb{E}_{x,y\\sim D}[(f_{t}^{\\prime}(x)-y)^{2}]$ ;   \n7: for each $v\\in[1/m]$ do   \n8: $D_{v}^{t}\\gets D|f_{t}^{\\prime}(x)=v$ ;   \n9: $h_{v}^{t}(x,y)\\gets A_{\\mathcal{H}}(D_{v}^{t})$ ; {Regression on level sets with grouping functions.}   \n10: end for   \n11: $\\begin{array}{r l}&{\\widetilde{f}_{t+1}\\widetilde{(x,y)}\\gets\\sum_{v\\in[\\frac{1}{m}]}1_{\\{f_{t}^{\\prime}(x)=v\\}}\\cdot h_{v}^{t}(x,y);\\quad\\{G e n e r a t e\\,p s e u d o l a b e l s.\\}}\\\\ &{e\\widetilde{r}r_{t+1}\\gets\\mathbb{E}_{x,y\\sim D}[(\\widetilde{f}_{t+1}(x,y)-y)^{2}];}\\\\ &{D_{t+1}\\gets(D_{x},\\widetilde{f}_{t+1}(D));}\\\\ &{f_{t+1}(x)\\gets A_{\\mathcal{F}}(D_{t+1});\\quad\\{U p d a t e\\,t h e\\,m o d e l\\,w i t h\\,p s e u d o l a b e l s.\\}}\\end{array}$   \n12:   \n13:   \n14:   \n15: $t\\gets t+1$ ;   \n16: until $e r r_{t-1}-e\\tilde{r}r_{t}$ stops decreasing.   \nEnsure: $f_{t-1}^{\\prime}(x)$ . ", "page_idx": 7}, {"type": "text", "text": "MC-PseudoLabel reduces to LSBoost Globus-Harris et al. [13], a boosting algorithm for multicalibration if $\\mathcal{H}$ only contains covariate-based grouping functions. In this case, Line 14 of Algorithm 1 reduces to $f_{t+1}\\dot{(x)}=\\tilde{f}_{t+1}(x,\\cdot)$ where $\\tilde{f_{t+1}}$ does not depend on $y$ . For joint grouping functions, since $\\tilde{f}_{t+1}\\,\\in\\,\\mathbb{R}^{\\lambda\\times\\mathcal{Y}}$ , we project it to models\u2019 space of $\\mathbb{R}^{\\mathcal{X}}$ by learning the model with $\\tilde{f}_{t+1}$ as pseudolabels. The projection substantially changes the optimization dynamics. LSBoost constantly decreases risks of models, due to $R_{\\hat{P}_{D}}(\\dot{f_{t+1}})=\\bar{R}_{\\hat{P}_{D}}(\\tilde{f_{t+1}})<R_{\\hat{P}_{D}}(f_{t})$ . The projection step disrupts the monotonicity of risks, implying that MC-Pseudolabel can output a predictor with a higher risk than input. This is because multicalibration with joint grouping functions implies balance between accuracy and invariance, as is discussed in Theorem 4.3. The convergence of LSBoost relies on the monotonicity of risks, which is not applicable to MC-Pseudolabel. We study the algorithm\u2019s convergence in the context of representation learning. Assume we are given a grouping function class $\\mathcal{H}_{\\Phi}$ with a latent representation $\\Phi$ . If a predictor is multicalibrated w.r.t $\\mathcal{H}_{1,\\Phi},\\mathcal{H}_{2,\\Phi}$ respectively, then it is also multicalibrated w.r.t. $\\mathcal{H}_{\\Phi}$ . Therefore, we separately study the convergence with two grouping function classes. In Proposition F.26, we show the convergence for a subset of $\\mathcal{H}_{1,\\Phi}$ consisting of covariate-based grouping functions, which is a corollary of Globus-Harris et al.\u2019s result. As a greater challenge, we derive convergence for $\\mathcal{H}_{2,\\Phi}$ when data follows multivariate normal distributions. ", "page_idx": 7}, {"type": "text", "text": "Theorem 5.2 (Covergence for $\\mathcal{H}_{2,\\Phi}$ (informal, see Theorem F.27)). Consider $X\\in\\mathbb{R}^{d}$ with $X=$ $(\\Phi,\\Psi)^{T}$ . Assume that $(\\Phi,\\Psi,Y)$ follows a multivariate normal distribution $\\mathscr{N}_{d+1}(\\mu,\\Sigma)$ where the random variables are in general position such that $\\Sigma$ is positive definite. For any distribution $D$ supported on $\\mathcal X\\times\\mathcal Y$ , take the predictor class $\\mathcal{F}=\\mathbb{R}^{\\mathcal{X}}$ and the grouping function class $\\mathcal{H}$ as a subset of $\\mathcal{H}_{2,\\Phi}$ which is defined in Equation $I O$ : ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal{H}=\\{h:h\\in\\mathcal{H}_{2,\\Phi}\\mathrm{~}a n d\\,h(x,y)=c_{x}^{T}x+c_{y}y+c_{b},c_{x}\\in\\mathbb{R}^{d},c_{y},c_{b}\\in\\mathbb{R}\\}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "For an initial predictor $f_{0}(x)\\,=\\,\\mathbb{E}[Y|x]$ , run MC-Pseudolabe $\\left(D,{\\mathcal{H}},{\\mathcal{F}}\\right)$ without rounding, then $f_{t}(x)$ converges pointwise to $\\mathbb{E}[Y|\\Phi(x)]$ as $t\\to\\infty$ , with a convergence rate of $\\mathcal{O}(M(\\Sigma)^{t})$ where $0\\le M(\\Sigma)<1$ . ", "page_idx": 7}, {"type": "text", "text": "MC-Pseudolabel is also an optimization paradigm for invariance. Certified multicalibration in Theorem 5.1 also implies certified invariance. Furthermore, MC-Pseudolabel introduces no extra hyperparameters to tradeoff between risks and robustness. Both certified invariance and light-weighted hyperparameters simplify model selection, which is challenging for out-distribution generalization because of unavailable validation data from target distributions [15]. MC-Pseudolabel has lightweighted optimization consisting of a series of supervised regression. It introduces an overhead to Empirical Risk Minimization by performing regression on level sets. However, the extra burden is linear regression by designing the grouping function class as linear space. Furthermore, regression on different level sets can be parallelized. Computational complexity is further analyzed in section D. ", "page_idx": 7}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "6.1 Settings ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We benchmark MC-Pseudolabel on real-world regression datasets with distributional shift. We adopt two experimental settings. For the multi-environment setting, algorithms are provided with training data collected from multiple annotated environments. Thereafter, the trained model is assessed on new environments. For the single-environment setting, algorithms are trained on a single source distribution. There could be latent sub-populations in training data, but environment annotations are unavailable. The trained model is assessed on a target dataset with distribution shift from the source. The grouping function class is implemented according to Equation 11 and Equation 12 for the multi-environment and single-environment setting respectively. ", "page_idx": 8}, {"type": "text", "text": "Datasets We experiment on PovertyMap [44] and ACSIncome [7] for the multi-environment setting, and VesselPower [33] for the single-environment setting. As the only regression task in WILDS [23], a popular benchmark for in-the-wild distribution shift, PovertyMap performs poverty index estimation for different spatial regions by satellite images. Data are collected from both urban and rural regions, by which the environment is annotated. The test dataset also covers both environments, but is collected from different countries. The primary metric is Worst-U/R Pearson, the worst Pearson correlation of prediction between rural and urban regions. The other two datasets are tabular, where natural concept shift $(Y|X$ shift) is more common due to existence of missing variables and hidden confounders [31]. ACSIncome [7] performs personal income prediction with data collected from US Census sources across different US states. The task is converted to binary classification by an income threshold, but we take raw data for regression. Environments are partitioned by different occupations with similar average income. VesselPower comes from Shifts [32, 33], a benchmark focusing on regression tasks with real-world distributional shift. The objective is to predict power consumption of a merchant vessel given navigation and weather data. Data are sampled under different time and wind speeds, causing distribution shift between training and test data. ", "page_idx": 8}, {"type": "text", "text": "Baselines For the multi-environment setting, baselines include ERM (Empirical Risk Minimization); methods for invariance learning which mostly adopts multi-objective optimization: IRM [2], MIP [24], IB-IRM [1], CLOvE [41], MRI [18], REX [25], Fishr [36]; an alignment-based method from domain generalization: IDGM [39]; and Group DRO [38]. Notably, CLOvE learns a calibrated predictor simultaneously on all environments, but it is optimized by multi-objective learning with a differentiable regularizer for calibration. For the singe-environment setting, baselines include reweighting based techniques: CVaR [26], JTT [29], Tilted-ERM [27]; a Distributionally Robust Optimization method $\\chi^{2}$ -DRO [8]; and a data augmentation method C-Mixup [43]. Other methods are not included because of specification in classification [45, 46] or exposure to target distribution data during training [19, 22]. For all experiments, we train an Oracle ERM with data sampled from target distribution. ", "page_idx": 8}, {"type": "text", "text": "Implementation We implement the predictor with MLP for ACSIncome and VesselPower, and Resnet18-MS [16] for PovertyMap, following WILDS\u2019 default architecture. We follow DomainBed\u2019s protocol [15] for model selection. Specifically, we randomly sample 20 sets of hyperparameters for each method, containing both the training hyperparameters and extra hyperparameters from the robust learning algorithm. We select the best model across hyperparameters based on three model selection criteria, including in-distribution validation on the average of training data, worst-environment validation with the worst performance across training environments, and oracle validation on target data. Oracle validation is not recommended by DomainBed, which suggests limited numbers of access to target data. The entire run is repeated with different seeds for three times to measure standard errors of performances. Specifically for PovertyMap, we perform 5-fold cross validation instead of three repeated experiments, following WILDS\u2019 setup. ", "page_idx": 8}, {"type": "text", "text": "6.2 Results ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Results are shown in Table 1 for multi-environment settings and Table 2 for single-environment settings. MC-Pseudolabel achieves superior performance in all datasets with in-distribution and worst-environment validation which does not violate test data. For oracle validation, MCPseudolabel achieves comparable performances to the best method. For example, CLOvE, which also learns invariance by calibration, achieves best performance under oracle validation in PovertyMap, but it sharply degrades when target validation data is unavailable. It\u2019s because CLOvE tunes its regularizer\u2019s coefficient to tradeoff with ERM risk, whose optimal value depends on the target distribution shift. In contrast, MC-Pseudolabel exhibits an advantage with in-distribution model selection. This is further supported by Figure 6.2, which shows that MC-Pseudolabel\u2019s out-of-distribution er", "page_idx": 8}, {"type": "table", "img_path": "bOS6WPV0Jf/tmp/6cba44e59d5f5935e6f986932965acccbb9823c0850259f92b94198d297976ec.jpg", "table_caption": ["Table 1: Results on multi-environment datasets, evaluated on test data using three model selection criteria. ID: validation with averaged performance on training data. Worst: validation with the worst performance across training environments. Oracle: validation with performance on sampled test set. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "rors strongly correlates with in-distribution errors. The experiment spans across different hyperparameters and seeds with the same model architecture on VesselPower. The phenomenon, known as accuracy-on-the-line [34], is well known for a general class of models under covariate shift. However, Liu et al. [31] shows accuracy-on-the-line does not exist under concept shift, which is the case for ERM and C-Mixup. This introduces significant challenge for model selection. However, MC-Pseudolabel recovers the accuracy to the line. ", "page_idx": 9}, {"type": "table", "img_path": "bOS6WPV0Jf/tmp/25fe35fa1760ec7677616f33065522a0612df6491025e68efa718521284b6733.jpg", "table_caption": ["Table 2: Single-environment results. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "To conclude, we establish a new optimization framework for out-of-distribution generalization through extended multicalibration with joint grouping functions. While the current algorithm focuses on regression, there is potential for future work to extend our approach to general forms of tasks, particularly in terms of classification. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Peng Cui is supported in part by National Natural Science Foundation of China (No. 62425206, 62141607). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Kartik Ahuja, Ethan Caballero, Dinghuai Zhang, Jean-Christophe Gagnon-Audet, Yoshua Bengio, Ioannis Mitliagkas, and Irina Rish. Invariance principle meets information bottleneck for out-of-distribution generalization. In Marc\u2019Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 3438\u20133450, 2021. URL https://proceedings.neurips.cc/paper/2021/hash/ 1c336b8080f82bcc2cd2499b4c57261d-Abstract.html.   \n[2] Mart\u00edn Arjovsky, L\u00e9on Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. CoRR, abs/1907.02893, 2019. URL http://arxiv.org/abs/1907.02893.   \n[3] Jaroslaw Blasiok, Parikshit Gopalan, Lunjia Hu, and Preetum Nakkiran. A unifying theory of distance from calibration. In Barna Saha and Rocco A. Servedio, editors, Proceedings of the 55th Annual ACM Symposium on Theory of Computing, STOC 2023, Orlando, FL, USA, June 20-23, 2023, pages 1727\u20131740. ACM, 2023. doi: 10.1145/3564246.3585182. URL https://doi.org/10.1145/3564246.3585182.   \n[4] Jaroslaw Blasiok, Parikshit Gopalan, Lunjia Hu, and Preetum Nakkiran. When does optimizing a proper loss yield calibration? In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/ e4165c96702bac5f4962b70f3cf2f136-Abstract-Conference.html.   \n[5] Elliot Creager, J\u00f6rn-Henrik Jacobsen, and Richard S. Zemel. Environment inference for invariant learning. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 2189\u20132200. PMLR, 2021. URL http: //proceedings.mlr.press/v139/creager21a.html.   \n[6] Zhun Deng, Cynthia Dwork, and Linjun Zhang. Happymap : A generalized multicalibration method. In Yael Tauman Kalai, editor, 14th Innovations in Theoretical Computer Science Conference, ITCS 2023, January 10-13, 2023, MIT, Cambridge, Massachusetts, USA, volume 251 of LIPIcs, pages 41:1\u201341:23. Schloss Dagstuhl - Leibniz-Zentrum f\u00fcr Informatik, 2023. doi: 10. 4230/LIPICS.ITCS.2023.41. URL https://doi.org/10.4230/LIPIcs.ITCS.2023.41.   \n[7] Frances Ding, Moritz Hardt, John Miller, and Ludwig Schmidt. Retiring adult: New datasets for fair machine learning. In Marc\u2019Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 6478\u20136490, 2021. URL https://proceedings.neurips.cc/paper/2021/hash/ 32e54441e6382a7fbacbbbaf3c450059-Abstract.html.   \n[8] John C. Duchi and Hongseok Namkoong. Variance-based regularization with convex objectives. J. Mach. Learn. Res., 20:68:1\u201368:55, 2019. URL http://jmlr.org/papers/v20/17-750. html.   \n[9] John C Duchi and Hongseok Namkoong. Learning models with uniform performance via distributionally robust optimization. The Annals of Statistics, 49(3):1378\u20131406, 2021.   \n[10] John C Duchi, Tatsunori Hashimoto, and Hongseok Namkoong. Distributionally robust losses against mixture covariate shifts. Under review, 2(1), 2019.   \n[11] John C. Duchi, Peter W. Glynn, and Hongseok Namkoong. Statistics of robust optimization: A generalized empirical likelihood approach. Math. Oper. Res., 46(3):946\u2013969, 2021. doi: 10.1287/MOOR.2020.1085. URL https://doi.org/10.1287/moor.2020.1085.   \n[12] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Research, pages 1126\u20131135. PMLR, 2017. URL http://proceedings.mlr.press/v70/finn17a.html.   \n[13] Ira Globus-Harris, Declan Harrison, Michael Kearns, Aaron Roth, and Jessica Sorrell. Multicalibration as boosting for regression. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 11459\u201311492. PMLR, 2023. URL https://proceedings.mlr.press/v202/globus-harris23a.html.   \n[14] Parikshit Gopalan, Adam Tauman Kalai, Omer Reingold, Vatsal Sharan, and Udi Wieder. Omnipredictors. In Mark Braverman, editor, 13th Innovations in Theoretical Computer Science Conference, ITCS 2022, January 31 - February 3, 2022, Berkeley, CA, USA, volume 215 of LIPIcs, pages 79:1\u201379:21. Schloss Dagstuhl - Leibniz-Zentrum f\u00fcr Informatik, 2022. doi: 10. 4230/LIPICS.ITCS.2022.79. URL https://doi.org/10.4230/LIPIcs.ITCS.2022.79.   \n[15] Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum?id $\\equiv$ lQdXeXDoWtI.   \n[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages 770\u2013778. IEEE Computer Society, 2016. doi: 10.1109/CVPR.2016.90. URL https://doi.org/10.1109/CVPR.2016.90.   \n[17] \u00darsula H\u00e9bert-Johnson, Michael P. Kim, Omer Reingold, and Guy N. Rothblum. Multicalibration: Calibration for the (computationally-identifiable) masses. In Jennifer G. Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm\u00e4ssan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pages 1944\u20131953. PMLR, 2018. URL http://proceedings.mlr.press/v80/hebert-johnson18a.html.   \n[18] Dongsung Huh and Avinash Baidya. The missing invariance principle found - the reciprocal twin of invariant risk minimization. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/ 91b482312a0845ed86e244adbd9935e4-Abstract-Conference.html.   \n[19] Badr Youbi Idrissi, Mart\u00edn Arjovsky, Mohammad Pezeshki, and David Lopez-Paz. Simple data balancing achieves competitive worst-group-accuracy. In Bernhard Sch\u00f6lkopf, Caroline Uhler, and Kun Zhang, editors, 1st Conference on Causal Learning and Reasoning, CLeaR 2022, Sequoia Conference Center, Eureka, CA, USA, 11-13 April, 2022, volume 177 of Proceedings of Machine Learning Research, pages 336\u2013351. PMLR, 2022. URL https://proceedings. mlr.press/v177/idrissi22a.html.   \n[20] Michael P. Kim, Amirata Ghorbani, and James Y. Zou. Multiaccuracy: Black-box postprocessing for fairness in classification. In Vincent Conitzer, Gillian K. Hadfield, and Shannon Vallor, editors, Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, AIES 2019, Honolulu, HI, USA, January 27-28, 2019, pages 247\u2013254. ACM, 2019. doi: 10.1145/3306618.3314287. URL https://doi.org/10.1145/3306618.3314287.   \n[21] Michael P Kim, Christoph Kern, Shafi Goldwasser, Frauke Kreuter, and Omer Reingold. Universal adaptability: Target-independent inference that competes with propensity scoring. Proceedings of the National Academy of Sciences, 119(4):e2108097119, 2022.   \n[22] Polina Kirichenko, Pavel Izmailov, and Andrew Gordon Wilson. Last layer re-training is sufficient for robustness to spurious correlations. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/pdf?id=Zb6c8A-Fghk.   \n[23] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, Tony Lee, ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Etienne David, Ian Stavness, Wei Guo, Berton Earnshaw, Imran S. Haque, Sara M. Beery, Jure Leskovec, Anshul Kundaje, Emma Pierson, Sergey Levine, Chelsea Finn, and Percy Liang. WILDS: A benchmark of in-the-wild distribution shifts. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18- 24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 5637\u20135664. PMLR, 2021. URL http://proceedings.mlr.press/v139/koh21a.html. ", "page_idx": 12}, {"type": "text", "text": "[24] Masanori Koyama and Shoichiro Yamaguchi. When is invariance useful in an out-of-distribution generalization problem? arXiv preprint arXiv:2008.01883, 2020. ", "page_idx": 12}, {"type": "text", "text": "[25] David Krueger, Ethan Caballero, J\u00f6rn-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai Zhang, R\u00e9mi Le Priol, and Aaron C. Courville. Out-of-distribution generalization via risk extrapolation (rex). In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 5815\u20135826. PMLR, 2021. URL http://proceedings.mlr.press/v139/krueger21a.html.   \n[26] Daniel Levy, Yair Carmon, John C. Duchi, and Aaron Sidford. Large-scale methods for distributionally robust optimization. In Hugo Larochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/ paper/2020/hash/64986d86a17424eeac96b08a6d519059-Abstract.html.   \n[27] Tian Li, Ahmad Beirami, Maziar Sanjabi, and Virginia Smith. Tilted empirical risk minimization. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum? id=K5YasWXZT3O.   \n[28] Tsung-Yi Lin, Priya Goyal, Ross B. Girshick, Kaiming He, and Piotr Doll\u00e1r. Focal loss for dense object detection. IEEE Trans. Pattern Anal. Mach. Intell., 42(2):318\u2013327, 2020. doi: 10.1109/TPAMI.2018.2858826. URL https://doi.org/10.1109/TPAMI.2018.2858826.   \n[29] Evan Zheran Liu, Behzad Haghgoo, Annie S. Chen, Aditi Raghunathan, Pang Wei Koh, Shiori Sagawa, Percy Liang, and Chelsea Finn. Just train twice: Improving group robustness without training group information. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 6781\u20136792. PMLR, 2021. URL http://proceedings.mlr.press/v139/liu21f.html.   \n[30] Jiashuo Liu, Jiayun Wu, Bo Li, and Peng Cui. Distributionally robust optimization with data geometry. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/ hash/da535999561b932f56efdd559498282e-Abstract-Conference.html.   \n[31] Jiashuo Liu, Tianyu Wang, Peng Cui, and Hongseok Namkoong. On the need for a language describing distribution shifts: Illustrations on tabular datasets. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023.   \n[32] Andrey Malinin, Neil Band, Yarin Gal, Mark J. F. Gales, Alexander Ganshin, German Chesnokov, Alexey Noskov, Andrey Ploskonosov, Liudmila Prokhorenkova, Ivan Provilkov, Vatsal Raina, Vyas Raina, Denis Roginskiy, Mariya Shmatova, Panagiotis Tigas, and Boris Yangel. Shifts: A dataset of real distributional shift across multiple large-scale tasks. In Joaquin Vanschoren and Sai-Kit Yeung, editors, Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, 2021. URL https://datasets-benchmarks-proceedings.neurips.cc/ paper/2021/hash/ad61ab143223efbc24c7d2583be69251-Abstract-round2.html.   \n[33] Andrey Malinin, Andreas Athanasopoulos, Muhamed Barakovic, Meritxell Bach Cuadra, Mark J. F. Gales, Cristina Granziera, Mara Graziani, Nikolay Kartashev, Konstantinos Kyriakopoulos, Po-Jui Lu, Nataliia Molchanova, Antonis Nikitakis, Vatsal Raina, Francesco La Rosa, Eli Sivena, Vasileios Tsarsitalidis, Ef iTsompopoulou, and Elena Volf. Shifts 2.0: Extending the dataset of real distributional shifts. CoRR, abs/2206.15407, 2022. doi: 10.48550/ARXIV.2206.15407. URL https://doi.org/10.48550/arXiv.2206.15407.   \n[34] John Miller, Rohan Taori, Aditi Raghunathan, Shiori Sagawa, Pang Wei Koh, Vaishaal Shankar, Percy Liang, Yair Carmon, and Ludwig Schmidt. Accuracy on the line: on the strong correlation between out-of-distribution and in-distribution generalization. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 7721\u20137735. PMLR, 2021. URL http://proceedings.mlr.press/v139/ miller21b.html.   \n[35] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019.   \n[36] Alexandre Ram\u00e9, Corentin Dancette, and Matthieu Cord. Fishr: Invariant gradient variances for out-of-distribution generalization. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesv\u00e1ri, Gang Niu, and Sivan Sabato, editors, International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 18347\u201318377. PMLR, 2022. URL https://proceedings.mlr.press/v162/rame22a.html.   \n[37] Aaron Roth. Uncertain: Modern topics in uncertainty estimation, 2022.   \n[38] Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural networks. In International Conference on Learning Representations, 2019.   \n[39] Yuge Shi, Jeffrey Seely, Philip HS Torr, N Siddharth, Awni Hannun, Nicolas Usunier, and Gabriel Synnaeve. Gradient matching for domain generalization. arXiv preprint arXiv:2104.09937, 2021.   \n[40] Aman Sinha, Hongseok Namkoong, and John C. Duchi. Certifying some distributional robustness with principled adversarial training. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. URL https://openreview.net/forum?id $\\fallingdotseq$ Hk6kPgZA-.   \n[41] Yoav Wald, Amir Feder, Daniel Greenfeld, and Uri Shalit. On calibration and out-of-domain generalization. In Marc\u2019Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 2215\u20132227, 2021. URL https://proceedings.neurips.cc/ paper/2021/hash/118bd558033a1016fcc82560c65cca5f-Abstract.html.   \n[42] Haoxiang Wang, Bo Li, and Han Zhao. Understanding gradual domain adaptation: Improved analysis, optimal path and beyond. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesv\u00e1ri, Gang Niu, and Sivan Sabato, editors, International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 22784\u201322801. PMLR, 2022. URL https://proceedings.mlr.press/v162/wang22n.html.   \n[43] Huaxiu Yao, Yiping Wang, Linjun Zhang, James Y. Zou, and Chelsea Finn. Cmixup: Improving generalization in regression. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/ 1626be0ab7f3d7b3c639fbfd5951bc40-Abstract-Conference.html.   \n[44] Christopher Yeh, Anthony Perez, Anne Driscoll, George Azzari, Zhongyi Tang, David Lobell, Stefano Ermon, and Marshall Burke. Using publicly available satellite imagery and deep learning to understand economic well-being in africa. Nature Communications, 2020.   \n[45] Hongyi Zhang, Moustapha Ciss\u00e9, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. URL https://openreview.net/forum?id $\\cdot$ r1Ddp1-Rb.   \n[46] Michael Zhang, Nimit S Sohoni, Hongyang R Zhang, Chelsea Finn, and Christopher Re. Correct-n-contrast: a contrastive approach for improving robustness to spurious correlations. In International Conference on Machine Learning, pages 26484\u201326516. PMLR, 2022. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Related Work ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Multicalibration Multicalibration is first proposed by H\u00e9bert-Johnson et al. [17] with binary grouping functions. Kim et al. [20] and Gopalan et al. [14] extend the grouping functions to realvalued functions. Globus-Harris et al. [13] shows that with a sufficiently rich class of real-valued grouping functions, multicalibration actually implies accuracy. Globus-Harris et al. also provides a boosting algorithm for both regression and multicalibration. The connection between multicalibration and distribution shift is first studied by Kim et al. [21], who proves that $\\ell_{1}$ multicalibration error remains under covariate shift, given a sufficiently large real-valued grouping function class. Kim et al. further shows that under covariate shift, a multicalibrated predictor can perform statistical inference of the average outcome of a sample batch. In contrast, we derive a robustness result for individual prediction of outcomes for $\\ell_{2}$ multicalibrated predictors. In addition, Wald et al. [41] studies the equivalence of Invariant Risk Minimization and simultaneous calibration on each environment. Our equivalence results for multicalibration can be perceived as a generalization of Wald et al.\u2019s results beyond the multi-environment setting, by deriving a mapping between density ratios and grouping functions. We also extend the equivalence to approximately multicalibrated and approximately invariant predictors. Furthermore, we move beyond Wald et al.\u2019s multi-objective optimization with Lagrangian regularization, by proposing a new post-processing optimization framework consisting of a series of supervised regression. Meanwhile, Blasiok et al. [4] discusses connections between calibration and post-processing, which is an equivalent expression of invariance. There are other extensions of multicalibration, such as Deng et al. [6] who generalize the term $Y-f(X)$ in multicalibration\u2019s definition to a class of general functions. While our work is the first to generalize the grouping functions $h$ to consider the outcomes. ", "page_idx": 14}, {"type": "text", "text": "Out-of-distribution Generalization Beyond Covariate Shift Despite abundant literature from domain generalization that focuses on image classification where covariate shift dominates, research on algorithmic robustness on regression tasks beyond covariate shift is relatively limited. The setting can be categorized according to if the source distribution is partitioned into several environments. For the multi-environment generalization setting, Invariant Risk Minimization and its variants assume that outcomes are generated by a common causal structural equation across all environments, and aims to recover such an invariant (or causal) predictor [1, 2, 18, 24, 25, 36]. Group DRO [38] is a simple but surprisingly strong technique that optimizes for the worst group risk with reweighting of environments. There are also meta-learning methods [12] that handles multi-environment generalization with bi-level optimization. For the single environment setting, Distributionally Robust Optimization optimizes for the worst-case risk in an uncertainty set of distributions centering around the source distribution [8, 9, 11, 26, 40]. Another branch of research is targeted at mitigating spurious correlation with an assumption of simplicity bias, which utilizes a simple model to discover latent sub-populations and then correct the biased predictor by sample reweighting [27, 28, 29], retraining on a subgroupbalanced dataset or a small batch from target distribution [19, 22, 46], or perform Invariant Risk Minimization on discovered subgroups [5]. Data augmentation is a prevalent technique to enhance algorithmic robustness for vision tasks. Quite a lot of these methods are tailored for classification. For example, Mixup [45] interpolates between features of samples with the same label. The approach is extended to regression settings by C-Mixup [43]. Pseudolabelling is a common technique for out-of-distribution generalization, but typically adopted in a setting with exposure to unlabelled samples from target distribution, known as domain adaptation [42]. However, MC-Pseudolabel generate pseudolabels for the source distribution itself. ", "page_idx": 14}, {"type": "text", "text": "B Grouping Functions for Distributionally Robust Optimization ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Distributionally Robust Optimization assumes the target distribution to reside in an uncertainty set $\\mathcal{P}$ of distributions centering around the source distribution $P_{S}$ . For example, Duchi et al. [10] formulates the uncertainty set as arbitrary subgroups that has a proportion of at least $\\alpha_{0}\\in(0,1)$ . Duchi et al. only consider subgroups of covariates: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\mathcal{P}}(X)=\\{P(X):\\mathrm{there~exists~a~probability~measure~}P^{\\prime}(X),}\\\\ &{\\qquad\\qquad\\qquad P_{S}(X)=\\alpha P(X)+(1-\\alpha)P^{\\prime}(X),\\alpha\\geq\\alpha_{0}\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "By the correspondence between density ratios and grouping functions, the equivalent design of a grouping function class is given by: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{H}=\\left\\{h\\in\\mathbb{R}^{\\mathcal{X}}:0\\leq h(x)\\leq\\frac{1}{\\alpha_{0}},\\ \\,\\forall x\\right\\}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We can also extend the grouping functions to consider both covariates and outcomes, such that general subgroups are incorporated into the uncertainty set: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{H}=\\left\\lbrace h\\in\\mathbb{R}^{\\mathcal{X}\\times\\mathcal{Y}}:0\\leq h(x,y)\\leq\\frac{1}{\\alpha_{0}},\\;\\;\\forall x,y\\right\\rbrace.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "In the case of grouping functions defined on $x$ and $y$ jointly, the grouping function class is not closed under affine transformation and is not a linear space spanned by density ratios, which suggests that a perfectly multicalibrated solution might not exist. However, approximately multicalibrated predictors can still be pursued. ", "page_idx": 15}, {"type": "text", "text": "C Model Discretization ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "For continuous predictors, we take a preprocessing step to discretize the model to as many bins as possible such that the rounding error is negligible while still ensuring enough samples in individual bins. Specifically, we equally split the outcomes of predictors to bins with equal intervals from the minimum to maximum of model output. We start from a minimum bin number $m=10$ , and keeps increasing $m$ as long as $90\\%$ of the samples reside in a bin with at least 30 samples. When the criterion is violated, we stop increasing $m$ and select it as the final bin number. The model discretization procedure is fixed across all experiments. ", "page_idx": 15}, {"type": "text", "text": "D Computational Complexity ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We assume that the predictor\u2019s outcomes are uniformly distributed. Denote the average bin size by $N_{b}$ , which is a constant around 30 in our implementation. The bin number is given by $m=N/N_{b}$ where $N$ is the sample size. For neural networks, $N$ represents the batch size. The overhead of MC-Pseudolabel compared to Empirical Risk Minimization is linear regression on each bin, whose sample complexity is $\\dot{O}(N_{b}^{3})$ with OLS. Please note that an individual linear regression for around 30 samples is extremely cheap. A non-parallel implementation of regression on every bin scales linearly with the bin number $m$ , so the overall complexity is $\\mathcal{O}(N_{b}^{2}N)$ . However, since the regression on each bin is independent, we adopt a multi-processing implementation. Denote the number of jobs by $J$ , the overall time cost of MC-Pseudolabel is $\\mathcal{O}(\\bar{N}_{b}^{2}\\bar{N^{\\prime}}J)$ . As a comparison, OLS on $N$ samples has a computational complexity of $\\mathcal{O}(N^{3})$ . ", "page_idx": 15}, {"type": "text", "text": "In conclusion, the complexity of MC-Pseudolabel scales linearly with sample size (or batch size for neural networks). Counterintuitively, increasing the bin number $m$ (and thus decreasing the bin size) actually decreases the computational complexity. This is because linear regression scales cubically with sample size, so decreasing the sample size in each bin is preferred to decreasing the bin number. ", "page_idx": 15}, {"type": "text", "text": "E Experiments ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "E.1 An Additional Experiment: Synthetic Dataset ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We start from a multi-environment synthetic dataset with a multivariate normal distribution corresponding to Theorem 5.2. In this experiment, we examine the optimization dynamics of MCPseudolabel. The data generation process is inspired by Arjovsky et al. [2]. The covariates can be sliced into $X=(S,V)^{\\overline{{T}}}$ with $S\\in\\dot{\\mathbb{R}}^{9}$ and $V\\in\\mathbb{R}$ , where $S$ is the causal variable for $Y$ and $V$ is the spurious variable. The data is generated by the following structural equations: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{S\\sim\\mathcal{N}(0,1).}\\\\ &{Y=\\alpha_{S}^{T}S+\\epsilon_{Y},\\quad\\alpha_{S}=(1,...,1)^{T}\\in\\mathbb{R}^{9},\\epsilon_{Y}\\sim\\mathcal{N}(0,0.5^{2}).}\\\\ &{V=\\alpha_{V}(\\varepsilon)\\cdot Y+\\epsilon_{V},\\quad\\alpha_{V}(e_{1})=1.25,\\alpha_{V}(e_{2})=0.75,\\alpha_{V}(e_{T})=-1,\\epsilon_{V}\\sim\\mathcal{N}(0,0.1^{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The covariate $V$ is spuriously correlated with $Y$ because the coefficient $\\alpha_{V}(e)$ depends on the specific environment $e$ . We set $\\alpha_{V}(\\dot{\\mathcal{E}})=1.25,0.7$ 5 respectively for two training environments while $\\bar{\\alpha}_{V}(\\mathcal{E})$ extrapolates to $-1$ during testing. A robust algorithm is supposed to bypass the spurious variable and output a predictor $\\bar{f}(X)=\\bar{\\alpha}_{S}^{T}S$ in order to survive the test distribution where the correlation between $V$ and $Y$ is negated. ", "page_idx": 16}, {"type": "text", "text": "The predictor class for this dataset is linear models, and the environment classifier is implemented by MLP with a single hidden layer. In this experiment, we fix the training hyperparameters for the base linear model, and perform grid search over the extra hyperparameter introduced by robust learning methods. Baselines except for ERM and Group DRO share a hyperparameter which is the regularizer\u2019s coefficient, and Group DRO introduces a temperature hyperparameter. We search over their hyperparameter space and report RMSE metric on the test set in Figure 2. Most baselines exhibit a U-turn with an increasing hyperparameter, and the minimum point varies across methods. The sensitivity of hyperparameters implies the dependence on a strong model selection criterion, such as oracle model selection on target distribution. However, the dashed line for MC-Pseudolabel\u2019s error is tangent to all the U-turns of baselines, indicating a competitive performance of MC-Pseudolabel both with and without oracle model selection. ", "page_idx": 16}, {"type": "image", "img_path": "bOS6WPV0Jf/tmp/69c0e40735e4d6d3db1b516dbb2f8c47e4bcce6328ea6a103bc2bf53bdc7a0ad.jpg", "img_caption": ["Figure 2: Results (RMSE) on the synthetic dataset. Curves show method performances across hyperparameters. Methods without extra hyperparameters are marked with dotted lines. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "We also investigate the evolution of pseudo labels $\\tilde{f}_{t}$ in Algorithm 1 to recover the dynamics of MCPseudolabel. The first row of Figure 3 demonstrates how pseudolabelling results in a multicalibrated predictor. It shows that pseudolabels for two environments deviate from model prediction at Step 0, but the gap quickly converges at Step 4, implying multicalibrated prediction. The second row provides insight about how pseudolabelling contributes to an invariant predictor. We observe that the curve of two environments are gradually merging because the pseudolabel introduces a special noise to the original label such that the correlation between the pseudolabel and spurious variable $V$ is weakened. As a result, the predictor will depend on the causal variable $S$ which is relatively more strongly correlated with pseudolabels. ", "page_idx": 16}, {"type": "text", "text": "E.2 VesselPower ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In figure 4, we provide the correlation between models\u2019 in-distribution validation performance and out-of-distribution test performance across all methods. ", "page_idx": 16}, {"type": "image", "img_path": "bOS6WPV0Jf/tmp/4bcba46e1c66d3d97b4ad29f7b87bf6d0f773cf9d30e740b295fd812fb0b1d3e.jpg", "img_caption": ["Figure 3: Evolution of pseudolabels during MC-Pseudolabel. The first row plots values of pseudolabels against model predictions. The second row plots values of pseudolabels against $V$ . Columns represent different snapshots during optimization. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "E.3 Training Details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Datasets. In the ACSInome dataset, we focus on predicting personal income for California residents across four occupation fields: science professions (SCI), protective services (PRT), education (EDU), and military (MIL). While average incomes are similar across these fields, the correlation between income and usual hours worked per week (WKHP) varies significantly, as shown in Table 3. This spurious correlation between income and WKHP introduces a concept shift across occupations. Therefore, we train the model on environments comprising the SCI, PRT, and EDU occupations and evaluate it on MIL. ", "page_idx": 17}, {"type": "table", "img_path": "bOS6WPV0Jf/tmp/adbeb9a8231201701d0df9217843858e085f8244df99f9a80e0cbdc7720446bc.jpg", "table_caption": ["Table 3: Statistics of ACSIncome (California) "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "We follow the standard setup for PovertyMap as specified in the WILDS benchmark [23]. A natural spatial distribution shift occurs between data collected from urban and rural regions, both of which are represented in the training and test sets. However, the test data is sourced from different countries than the training data. The primary evaluation metric is the worst Pearson correlation between predictions in urban and rural environments. ", "page_idx": 17}, {"type": "text", "text": "We use the synthetic split of VesselPower where the target power labels are generated through the simulation of a physics model based on read data input features. The dataset include a single environment training set, a dev-in set for in-distribution validation, and a dev-out set for evaluation. Distribution shifts arise due to variation in time and wind speeds. We report RMSE of prediction in megawatts (MW), rather than the kilowatts (KW) as used in the dataset\u2019s original paper [33]. ", "page_idx": 17}, {"type": "text", "text": "Model Selction. We follow DomainBed\u2019s protocol [15] for model selection. Specifically, we randomly sample 20 sets of hyperparameters for each method, containing both the training hyperparameters of base models in Table 4 and extra hyperparameters from the robust learning algorithm in Table 5. We select the best model across hyperparameters based on three model selection criteria. In-distribution (ID) validation selects the model with the best metric on the average of an in-distribution validation dataset, which is sampled from the same distribution as the training data. Worst-environment (Worst) validation selects the best model by the worst performance across all environments in the in-distribution validation dataset. Worst validation is applicable only to the multi-environment setting. Oracle validation selects the best model by an out-of-distribution validation dataset sampled from the target distribution of test data. Oracle validation leaks the test distribution, so it is not recommended by DomainBed. However, most robust learning methods relies on out-of-distribution validation, so Domainbed suggests limited numbers of access to target data when using Oracle validation. Though MC-Pseudolabel already performs well under ID and Worst validation, we still report its performance under Oracle validation to compare the limit of robust learning methods regardless of model selection. ", "page_idx": 17}, {"type": "image", "img_path": "bOS6WPV0Jf/tmp/5663179565baf9fdca2471894b9ed2b938395847f4c54b39f1185baf9386ce93.jpg", "img_caption": ["Figure 4: Correlation between models\u2019 in-distribution and out-of-distribution risks on VesselPower. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "Following DomainBed, the entire model selection procedure is repeated with different seeds for three times to measure standard errors of performances. Thus, we have totally 60 runs per method per dataset. Specifically for PovertyMap, we follow WILDS\u2019 setup [23] and perform 5-fold cross validation instead of three repeated experiments. For each fold of the dataset, we conduct the model selection procedure four times across three seeds, summing up to a total of 12 experiments. And we report the average and standard error of performances across 5 folds. Thus, the standard error measures both the difficulty disparity across folds and the model\u2019s instability. ", "page_idx": 18}, {"type": "text", "text": "Grouping Functions. The grouping function class of MC-Pseudolabel is implemented according to Equation 11 and Equation 12 for the multi-environment and single-environment setting respectively. For the multi-environment setting, the environment classifier $p(e|x,y)$ is implemented as MLP with a single hidden layer of size 100 for tabular datasets including Simulation and ACSIncome. For PovertyMap, the environment classifier is implemented by Resnet18-MS with the same architecture as the predictor, except that the label $y$ is fed into the last fully-connected layer. For the singleenvironment setting of VesselPower, the identification model $f_{i d}$ is implemented as a Ridge regression model. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "table", "img_path": "bOS6WPV0Jf/tmp/d4cc3cab9288d25244136a0b5fc32d0bc867cde5b3014c28b7aaa706e386d8f2.jpg", "table_caption": ["Table 4: Hyperparameters for model architecture. "], "table_footnote": ["1 The learning rate and batch size for training ResNet follow the setup of WILDS [23]. "], "page_idx": 19}, {"type": "table", "img_path": "bOS6WPV0Jf/tmp/5c2d1fb96ca5a8b3382ad370b7dc3f12ecd5c3c6e1b7ffbb84881c8550ea8a80.jpg", "table_caption": ["Table 5: Hyperparameters for robust learning methods. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "E.4 Software and Hardware ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Our experiments are based on the architecture of PyTorch [35]. Each experiment with a single set of hyperparameters is run on one NVIDIA GeForce RTX 3090 with 24GB of memory, taking at most 15 minutes. ", "page_idx": 19}, {"type": "text", "text": "F Theory ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "F.1 Multicalibration and Bayes Optimality under Covariate Shift ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Assumption F.1 (Restatement of Assumption 2.3). ", "page_idx": 19}, {"type": "text", "text": "1. (Closure under Covariate Shift) For a set of probability measures ${\\mathcal{P}}(X)$ containing the source measure $P_{S}(X)$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\forall P\\in\\mathcal{P},h\\in\\mathcal{H}\\Rightarrow\\frac{p(\\cdot)}{p_{S}(\\cdot)}\\cdot h(\\cdot)\\in\\mathcal{H}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "2. $((\\gamma,\\rho)$ -Weak Learning Condition) For any $P\\in{\\mathcal{P}}(X)P_{S}(Y|X)\\equiv\\{P^{\\prime}(X)P_{S}(Y\\mid X):P^{\\prime}\\in{\\mathcal{P}}\\}$ with the source conditional measure $P_{S}(Y|X)$ and every measurable set $G\\subset\\mathcal{X}$ satisfying $P(X\\in$ $G)>\\rho,\\,i f$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{P}[(\\mathbb{E}_{P}[Y|X]-Y)^{2}|X\\in G]<\\mathbb{E}_{P}[(\\mathbb{E}_{P}[Y|X\\in G]-Y)^{2}|X\\in G]-\\gamma,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "then there exists $h\\in\\mathcal H$ satisfying ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{P}[(h(X)-Y)^{2}|X\\in G]<\\mathbb{E}_{P}[(\\mathbb{E}_{P}[Y|X\\in G]-Y)^{2}|X\\in G]-\\gamma.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Lemma F.2 (Globus-Harris et al. [13]). Fix any distribution $P\\in{\\mathcal{P}}(X,Y)$ , any model $f:\\mathcal{X}\\to[0,1]$ , and any class of real valued functions $\\mathcal{H}$ that is closed under affine transformation. Let: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{H}_{1}=\\{h\\in\\mathcal{H}:\\operatorname*{max}_{x\\in\\mathcal{X}}h(x)^{2}\\leq1\\}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "be the set of functions in $\\mathcal{H}$ upper-bounded by $^{\\,l}$ on $\\mathcal{X}$ . Let $m=|R a n g e(f)|,\\gamma>0,$ , and $\\begin{array}{r}{\\alpha\\leq\\frac{\\gamma^{3}}{16m}}\\end{array}$ . Then if $\\mathcal{H}$ satisfies the $(\\gamma,\\frac{\\gamma}{m})$ -weak learning condition and $f$ is $\\alpha$ -approximately $\\ell_{2}$ multicalibrated with respect to $\\mathcal{H}_{1}$ and $P$ , then $f$ has squared error ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{P}[(f(x)-y)^{2}]\\leq\\mathbb{E}_{P}[(f^{*}(x)-y)^{2}]+3\\gamma,}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $f^{*}(x)=\\mathbb{E}_{P}[Y|x]$ . ", "page_idx": 20}, {"type": "text", "text": "Definition F.3. For a probability measure $P(X,Y)$ and a predictor $f:\\mathcal{X}\\to[0,1].$ , let $\\mathcal{H}\\subset\\mathbb{R}^{\\mathcal{X}\\times\\mathcal{Y}}$ be a function class. We say that $f$ is $\\alpha$ -approximately $\\ell_{1}$ multicalibrated w.r.t. $\\mathcal{H}$ and $P$ if for all $h\\in\\mathcal H$ : ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle{K_{1}(f,h,P)}}\\\\ {\\displaystyle{=\\int\\left|\\mathbb{E}_{P}\\left[h(X,Y)(Y-v)\\big|f(X)=v\\right]\\right|d P_{f(X)}(v)}}\\\\ {\\displaystyle{\\le\\alpha.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Lemma F.4 ( Roth [37]). Suppose $P_{S},P_{T}\\in\\mathcal{P}$ have the same conditional label distribution, and suppose $f$ is $\\alpha$ -approximately $\\ell_{1}$ multicalibrated with respect to $P_{S}$ and $\\mathcal{H}$ . If $\\mathcal{H}$ satisfies Equation 21, then $f$ is also $\\alpha$ -approximately $\\ell_{1}$ multicalibrated with respect to $P_{T}$ and $\\mathcal{H}$ : ", "page_idx": 20}, {"type": "text", "text": "Lemma F.5. For a predictor $f\\ :\\ \\ x\\ \\ \\rightarrow\\ \\ [0,1]$ and $a$ grouping function $h$ satisfying $\\begin{array}{r}{\\operatorname*{max}_{x\\in\\mathcal{X},y\\in\\mathcal{Y}}h(x,y)^{2}\\leq B}\\end{array}$ where $B>0$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{1}{\\sqrt{B}}K_{2}(f,h,P)\\leq K_{1}(f,h,P)\\leq\\sqrt{K_{2}(f,h,P)}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Remark F.6. The lemma is extended from Roth [37]\u2019s result for $B=1$ . ", "page_idx": 20}, {"type": "text", "text": "Proof. First we prove $K_{2}(f,h,P)\\le\\sqrt{B}K_{1}(f,h,P)$ . For any $v\\in[0,1]$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~~~~\\left(\\mathbb{E}_{P}\\left[h(X,Y)(Y-v)\\big|f(X)=v\\right]\\right)^{2}}\\\\ &{=\\Big|\\mathbb{E}_{P}\\left[h(X)(Y-v)\\big|f(X)=v\\right]\\Big|\\cdot\\Big|\\mathbb{E}_{P}\\left[h(X,Y)(Y-v)\\big|f(X)=v\\right]\\Big|}\\\\ &{\\leq\\sqrt{\\mathbb{E}\\left[h(X,Y)^{2}\\big|f(X)=v\\right]\\mathbb{E}\\left[(Y-v)^{2}\\big|f(X)=v\\right]}}\\\\ &{~~~~\\cdot\\left|\\mathbb{E}_{P}\\left[h(X,Y)(Y-v)\\big|f(X)=v\\right]\\right|}\\\\ &{\\leq\\sqrt{B}\\cdot\\Big|\\mathbb{E}_{P}\\left[h(X,Y)(Y-v)\\big|f(X)=v\\right]\\Big|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Equation 30 follows from the Cauchy-Schwarz inequality. ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{K_{2}(f,h,P)=\\underset{v\\sim P_{f(X)}}{\\mathbb{E}}\\left[\\left(\\mathbb{E}_{P}\\left[h(X,Y)(Y-v)\\middle|f(X)=v\\right]\\right)^{2}\\right]}\\\\ &{\\qquad\\qquad\\leq\\sqrt{B}\\underset{v\\sim P_{f(X)}}{\\mathbb{E}}\\bigg\\vert\\mathbb{E}_{P}\\left[h(X,Y)(Y-v)\\middle|f(X)=v\\right]\\Big\\vert}\\\\ &{\\qquad\\qquad=\\sqrt{B}K_{1}(f,h,P).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then we prove $K_{1}(f,h,P)\\leq\\sqrt{B K_{2}(f,h,P)}$ . Still from the Cauchy-Schwarz inequality: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{K_{1}(f,h,P)=\\underset{v\\sim P_{f(X)}}{\\mathbb{E}}\\Big|\\mathbb{E}_{P}\\left[h(X,Y)(Y-v)\\big|f(X)=v\\right]\\Big|}\\\\ &{\\qquad\\qquad\\le\\sqrt{\\underset{v\\sim P_{f(X)}}{\\mathbb{E}}[1^{2}]\\underset{v\\sim P_{f(X)}}{\\mathbb{E}}\\left[\\Big(\\mathbb{E}_{P}\\left[h(X,Y)(Y-v)\\big|f(X)=v\\right]\\Big)^{2}\\right]}}\\\\ &{\\qquad\\qquad=\\sqrt{K_{2}(f,h,P)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Theorem F.7 (Restatement of Theorem 2.4). For a source measure $P_{S}(X,Y)$ and a set of probability measures ${\\mathcal{P}}(X)$ containing $P_{S}(X)$ , given a predictor $f\\,:\\,\\mathcal{X}\\,\\rightarrow\\,[0,1]$ with finite range $m:=|R a n g e(f)|$ , consider $a$ grouping function class $\\mathcal{H}$ closed under affine transformation satisfying Assumption 2.3 with $\\rho\\,=\\,\\gamma/m$ . If $f$ is $\\frac{\\gamma^{6}}{256m^{2}}$ -approximately $\\ell_{2}$ multicalibrated w.r.t $P_{S}$ and $\\mathcal{H}$ \u2019s bounded subset $\\mathcal{H}_{1}\\ :=\\ \\left\\{h\\in\\mathcal{H}:\\operatorname*{max}_{x\\in\\mathcal{X}}h(x)^{2}\\leq1\\right\\}$ , then for any target measure $P_{T}\\in{\\mathcal{P}}(X)P_{S}(Y|X).$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\nR_{P_{T}}(f)\\leq R_{P_{T}}(f^{*})+3\\gamma,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $f^{*}(x)=\\mathbb{E}_{P_{T}}[Y|x]$ is the optimal regression function in each target distribution. ", "page_idx": 21}, {"type": "text", "text": "Proof. For $|h(x)|\\leq1$ and $f\\in[0,1]$ , according to Lemma F.5, ", "page_idx": 21}, {"type": "equation", "text": "$$\nK_{2}(f,h,P)\\leq K_{1}(f,h,P)\\leq\\sqrt{K_{2}(f,h,P)}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Since $\\begin{array}{r}{K_{2}(f,h,P_{S})\\leq\\frac{\\gamma^{6}}{256m^{2}}}\\end{array}$ for any $h\\in H_{1}$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\nK_{1}(f,h,P_{S})\\leq\\frac{\\gamma^{3}}{16m}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "With Lemma F.4, for any $h\\in H_{1}$ and $P_{T}\\in\\mathcal{P}$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\nK_{1}(f,h,P_{T})\\leq\\frac{\\gamma^{3}}{16m}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Again, it follows from Lemma F.5 that: ", "page_idx": 21}, {"type": "equation", "text": "$$\nK_{2}(f,h,P_{T})\\leq\\frac{\\gamma^{3}}{16m}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "With Lemma F.2, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{P_{T}}[(f(x)-y)^{2}]\\leq\\mathbb{E}_{P_{T}}[(f^{*}(x)-y)^{2}]+3\\gamma,}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which completes the proof. ", "page_idx": 21}, {"type": "text", "text": "F.2 Multicalibration and Invariance under Concept Shift ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Theorem F.8 (Restatement of Theorem 3.1). For a set of absolutely continuous probability measures $\\mathcal{P}(X,Y)$ containing the source measure $P_{S}(X,Y)$ , consider a predictor $f:\\mathcal{X}\\to[0,1]$ . Assume the grouping function class $\\mathcal{H}$ satisfies the following condition: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{H}\\supset\\left\\{h(x,y)=\\frac{p(x,y)}{p_{S}(x,y)}\\Big|P\\in\\mathcal{P}(X,Y)\\right\\}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "If $f$ is $\\alpha$ -approximately $\\ell_{2}$ multicalibrated w.r.t. $\\mathcal{H}$ and $P_{S}$ , then for any measure $P\\in{\\mathcal{P}}(X,Y)$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\nR_{P}(f)\\leq\\operatorname*{inf}_{g:[0,1]\\to[0,1]}R_{P}(g\\circ f)+2{\\sqrt{\\alpha}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. For any $h(x,y)=p(x,y)/p_{S}(x,y)$ where $P\\in\\mathcal P$ , since $f$ is $\\alpha$ -appr\u221aoximately $\\ell_{2}$ multicalibrated, $K_{2}(f,h,P_{S})\\leq\\alpha$ . It follows from Lemma F.5 that $K_{1}(f,h,P_{S})\\leq\\sqrt{\\alpha}$ . ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{K_{1}(f,h,P_{S})=\\displaystyle\\int\\left|\\mathbb{E}_{P_{S}}\\left[h(X,Y)(Y-v)\\middle|f(X)=v\\right]\\middle|d P_{S}(f^{-1}(v))}\\\\ {\\quad}&{=\\displaystyle\\int\\left|\\int\\frac{p(x,y)}{p_{S}(x,y)}(y-v)p_{S}(x,y|f=v)d(x,y)\\right|d P_{S}(f^{-1}(v))}\\\\ {\\quad}&{=\\displaystyle\\int\\left|\\int\\frac{d P(f^{-1}(v))}{d P_{S}(f^{-1}(v))}(y-v)p(x,y|f=v)d(x,y)\\right|d P_{S}(f^{-1}(v))}\\\\ {\\quad}&{=\\displaystyle\\int\\frac{d P(f^{-1}(v))}{d P_{S}(f^{-1}(v))}\\left|\\int(y-v)p(x,y|f=v)d(x,y)\\right|d P_{S}(f^{-1}(v))}\\\\ {\\quad}&{=\\displaystyle\\int\\left|\\mathbb{E}_{P}\\left[Y-v\\middle|f(X)=v\\right]\\middle|d P(f^{-1}(v))}\\\\ {\\quad}&{=K_{1}(f,1,P).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Thus, we have $K_{1}(f,1,P)\\le\\sqrt{\\alpha}$ for any $P\\in\\mathcal P$ . We will prove an equivalent form of $\\ell_{1}$ calibration error: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{K_{1}(f,1,P)=\\underset{\\eta:[0,1]\\to[-1,1]}{\\operatorname*{sup}}\\int\\eta(v)\\mathbb{E}_{P}\\left[Y-v\\middle|f(X)=v\\right]d P_{f(X)}(v)}\\\\ &{\\qquad\\qquad=\\underset{\\eta:[0,1]\\to[-1,1]}{\\operatorname*{sup}}\\mathbb{E}_{P}[\\eta(f(X))(Y-f(X))].}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "$\\begin{array}{r}{K_{1}(f,1,P)\\le\\operatorname*{sup}_{\\eta:[0,1]\\to[-1,1]}\\mathbb{E}_{P}[\\eta(f(X))(Y-f(X))]}\\end{array}$ can be proved by taking $\\eta(v)=2\\mathbb{I}[v>$ $0]-1$ . On the other hand, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\int\\eta(v)\\mathbb{E}_{P}\\left[Y-v\\middle|f(X)=v\\right]d P_{f(X)}(v)\\leq\\displaystyle\\int\\left|\\eta(v)\\right|\\cdot\\left|\\mathbb{E}_{P}\\left[Y-v\\middle|f(X)=v\\right]\\middle|d P_{f(X)}(v)}&{}\\\\ {\\leq K_{1}(f,1,P).}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Actually the right hand side of Equation 53 resembles smooth calibration [3], which restricts $\\eta$ to Lipschitz functions. Based on smooth calibration, Blasiok et al. [4] shows that approximately calibrated predictors cannot be improved much by post-processing. In the above we present a similar proof for $\\ell_{1}$ calibration error. ", "page_idx": 22}, {"type": "text", "text": "For any $g:[0,1]\\,\\rightarrow\\,[0,1]$ , there exists $\\eta:[0,1]\\,\\rightarrow\\,[-1,1]$ such that $g(v)\\,=\\,v+\\eta(v)$ for any $v\\in[0,1]$ . ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{R_{P}}(g\\circ f)=\\mathbb{E}_{P}\\left[\\big(Y-f(X)-\\eta(f(X))\\big)^{2}\\right]}\\\\ &{\\phantom{=}=\\mathbb{E}_{P}\\left[\\big(Y-f(X)\\big)^{2}\\right]-2\\mathbb{E}_{P}\\left[(Y-f(X)\\eta(f(X))\\big)+\\mathbb{E}_{P}\\left[\\eta(f(X))^{2}\\right]\\right]}\\\\ &{\\phantom{=}\\ge{R_{P}}(f)-\\underset{\\eta^{\\prime}:[0,1]\\to[-1,1]}{\\operatorname*{sup}}2\\mathbb{E}[\\eta^{\\prime}(f(X))(Y-f(X))]}\\\\ &{\\phantom{=}=R_{P}(f)-2K_{1}(f,1,P)}\\\\ &{\\phantom{=}\\ge R_{P}(f)-2\\sqrt{\\alpha}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Theorem F.9 (Restatement of Theorem 3.4). Assume samples are drawn from an environment $e~\\in{\\mathcal{E}}$ with a prior $P_{S}(e)$ such that $\\begin{array}{r}{\\sum_{e\\in\\mathcal{E}}P_{S}(e)\\,=\\,1}\\end{array}$ and $P_{S}(e)\\,>\\,0$ . The overall population satisfies $\\begin{array}{r}{P_{S}(X,Y)=\\sum_{e\\in\\mathcal{E}}P_{e}(X,Y)P_{S}(e)}\\end{array}$ where $P_{e}(X,Y)$ is the environment-specific absolutely continuous measure. For a representation $\\Phi\\in\\sigma(X)$ over features, define a function class $\\mathcal{H}$ as: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathcal{H}:=\\left\\{h(x,y)=\\frac{p_{e}(x,y)}{p_{S}(x,y)}\\Big|e\\in\\mathcal{E}\\right\\}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "1. If there exists a bijection $g^{*}:s u p p(\\Phi)\\rightarrow[0,1]$ such that $g^{\\ast}\\circ\\Phi$ is $\\alpha$ -approximately $\\ell_{2}$ multicalibrated w.r.t. $\\mathcal{H}$ and $P_{S}$ , then for any $e\\in{\\mathcal{E}}$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\nR_{P_{e}}(g^{*}\\circ\\Phi)\\leq\\operatorname*{inf}_{g:s u p p(\\Phi)\\to[0,1]}R_{P_{e}}(g\\circ\\Phi)+2\\sqrt{\\alpha}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "2. For $\\Phi\\,\\in\\,\\sigma(X)$ , if there exists $g^{*}\\;:\\;s u p p(\\Phi)\\;\\rightarrow\\;[0,1]$ such that Equation $^{63}$ is satisfied for any $e\\,\\in{\\mathcal{E}}$ , then $g^{*}\\circ\\Phi$ is $\\sqrt{2/D}\\alpha^{1/4}$ -approximately $\\ell_{2}$ multicalibrated w.r.t. $\\mathcal{H}$ and $P_{S}$ , where $D=\\operatorname*{min}_{e\\in\\mathcal{E}}{P_{S}(e)}$ . ", "page_idx": 22}, {"type": "text", "text": "Proof. We first prove statement 1. ", "page_idx": 22}, {"type": "text", "text": "For any $g:\\mathrm{supp}(\\Phi)\\rightarrow[0,1]$ , since $g^{*}$ is a bijection, $g\\circ\\Phi=(g\\circ g^{*^{-1}})(g^{*}\\circ\\Phi)$ where $g\\circ g^{*^{-1}}\\in$ $[0,1]^{[0,1]}$ . Since $g^{\\ast}\\circ\\Phi$ is $\\alpha$ -approximately $\\ell_{2}$ multicalibrated, it follows from Theorem F.8 that for any $e\\in{\\mathcal{E}}$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{R_{P_{e}}(g^{*}\\circ\\Phi)\\leq R_{P_{e}}\\bigl((g\\circ g^{*}{}^{-1})(g^{*}\\circ\\Phi)\\bigr)+2\\sqrt{\\alpha}}}\\\\ {{=R_{P_{e}}(g\\circ\\Phi)+2\\sqrt{\\alpha}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Then we give a proof of statement 2, which is inspired by Blasiok et al. [4]. ", "page_idx": 22}, {"type": "text", "text": "For simplicity let $f^{*}\\;:=\\;g^{*}\\;\\circ\\;\\Phi$ . For any $e\\ \\in\\ {\\mathcal{E}}$ and any $\\eta~:~[0,1]~\\rightarrow~[-1,1].$ , define $\\beta\\;:=\\;E_{P_{e}}\\left[(Y-f^{*}(X))\\eta(f^{*}(X))\\right]\\;\\in\\;[-1,1].$ . Construct $\\kappa(v)\\;:=\\;\\mathrm{proj}_{[0,1]}(v+\\beta\\eta(v))$ , where $\\mathrm{proj}_{[0,1]}(\\cdot)=\\operatorname*{max}\\{0,\\operatorname*{min}\\{1,\\cdot\\}\\}$ . ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R_{P_{e}}\\big(\\kappa(f^{*})\\big)=\\mathbb{E}_{P_{e}}\\left[\\big(Y-\\kappa(f^{*}(X))\\big)^{2}\\right]}\\\\ &{\\qquad\\qquad\\leq\\mathbb{E}_{P_{e}}\\left[\\big(Y-f^{*}(X)-\\beta\\eta(f^{*}(X))\\big)^{2}\\right]}\\\\ &{\\qquad\\qquad=\\mathbb{E}_{P_{e}}\\left[\\big(Y-f^{*}(X)\\big)^{2}\\right]-2\\beta^{2}+\\beta^{2}\\mathbb{E}_{P_{e}}\\big[\\eta(f^{*}(x))^{2}\\big]}\\\\ &{\\qquad\\qquad\\leq R_{P_{e}}(f^{*})-2\\beta^{2}+\\beta^{2}}\\\\ &{\\qquad\\qquad=R_{P_{e}}(f^{*})-\\beta^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Rearranging the inequality above gives: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\Big(E_{P e}\\left[(Y-f^{*}(X))\\eta(f^{*}(X))\\right]\\Big)^{2}=\\beta^{2}\\le R_{P e}(f^{*})-R_{P e}(\\kappa(f^{*})).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Since $\\kappa\\circ g^{*}\\in\\operatorname{supp}(\\Phi)\\to[0,1]$ , it follows from Equation 63 that: ", "page_idx": 23}, {"type": "equation", "text": "$$\nR_{P_{e}}(f^{*})-R_{P_{e}}(\\kappa(f^{*}))=R_{P_{e}}(g^{*}\\circ\\Phi)-R_{P_{e}}((\\kappa\\circ g^{*})\\circ\\Phi)\\leq2\\sqrt{\\alpha}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Combining Equation 71 and Equation 72 gives $E_{P e}\\left[(Y-f^{*}(X))\\eta(f^{*}(X))\\right]\\leq\\sqrt{2}\\alpha^{1/4}$ for any $\\eta:[0,1]\\bar{\\to}\\,[\\bar{-}1,1]$ . From Equation 53, it follows that: ", "page_idx": 23}, {"type": "equation", "text": "$$\nK_{1}(f^{*},1,P_{e})=\\operatorname*{sup}_{\\eta:[0,1]\\to[-1,1]}\\mathbb{E}_{P_{e}}[\\eta(f^{*}(X))(Y-f^{*}(X))]\\leq\\sqrt{2}\\alpha^{1/4}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "From Equation 52, $K_{1}(f^{*},h,P_{S})\\leq\\sqrt{2}\\alpha^{1/4}$ for any $h\\in\\mathcal H$ . Further, for any $h\\in\\mathcal H$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{h(x,y)=\\cfrac{p_{e}(x,y)}{p_{S}(x,y)}}\\\\ &{\\qquad=\\cfrac{p_{e}(x,y)}{\\sum_{e^{\\prime}\\in\\mathcal{E}}p_{e^{\\prime}}(x,y)P_{S}(e^{\\prime})}}\\\\ &{\\qquad\\le\\cfrac{p_{e}(x,y)}{p_{e}(x,y)P_{S}(e)}}\\\\ &{\\qquad\\le\\cfrac1{D}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "By Lemma F.5, it follows that $K_{2}(f^{*},h,P_{S})\\leq\\sqrt{1/D}\\cdot K_{1}(f^{*},h,P_{S})\\leq\\sqrt{2/D}\\alpha^{1/4}.$ ", "page_idx": 23}, {"type": "text", "text": "F.3 Structure of Grouping Function Classes ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this subsection, we focus on Euclidean space where $\\mathcal{X}\\subset\\mathbb{R}^{d}$ is compact and measurable for some $d\\in Z^{+}$ and $y=[0,1]$ . Grouping functions are assumed to be continuous, i.e., $h\\in C(\\mathcal{X}\\times\\mathcal{Y})$ . We consider absolutely continuous probability measures with continuous density functions. ", "page_idx": 23}, {"type": "text", "text": "Proposition F.10 (Restatement of Proposition 4.1). Consider an absolutely continuous probability measure $P_{S}(X,Y)$ and a predictor $f:\\mathcal{X}\\to[0,1]$ , define the maximal grouping function class that $f$ is multicalibrated with respect to: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathcal{H}_{f}:=\\{h\\in C(\\mathcal{X}\\times\\mathcal{Y}):K_{2}(f,h,P_{S})=0\\}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Then $\\mathcal{H}_{f}$ is a linear space. ", "page_idx": 23}, {"type": "text", "text": "Particularly for $f_{\\Phi}(x)\\,=\\,\\mathbb{E}[Y|\\Phi(x)]$ where $\\Phi\\,:\\,\\mathbb{R}^{d}\\,\\rightarrow\\,\\mathbb{R}^{d_{\\Phi}},d_{\\Phi}\\,\\in\\,Z^{+}$ is a measurable function, we abbreviate $\\mathcal{H}_{f_{\\Phi}}$ with $\\mathcal{H}_{\\Phi}$ . Then any finite subset $\\mathcal{G}\\subset\\mathcal{H}_{\\Phi}$ implies spa $n\\{1,\\mathcal{G}\\}\\subset\\mathcal{H}_{\\Phi}$ , where 1 denotes a constant function. ", "page_idx": 23}, {"type": "text", "text": "Proof. For any $\\gamma_{1},\\gamma_{2}\\in\\mathbb{R}$ and any $h_{1},h_{2}\\in\\mathcal{H}_{f},\\gamma_{1}h_{1}+\\gamma_{2}h_{2}\\in C(\\mathcal{X}\\times\\mathcal{Y}).$ . ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{K_{1}(f,\\gamma_{1}h_{1}+\\gamma_{2}h_{2},P_{S})=\\displaystyle\\int\\Big|\\mathbb{E}\\left[(\\gamma_{1}h_{1}(X,Y)+\\gamma_{2}h_{2}(X,Y))(Y-v)\\Big|f(X)=v\\right]}\\\\ &{\\phantom{\\gamma_{1}}}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\int\\Big|\\mathbb{E}\\left[\\gamma_{1}h_{1}(X,Y)(Y-v)\\Big|f(X)=v\\right]\\Big|d P_{S}(f^{-1}(v))}\\\\ &{\\qquad\\qquad\\qquad+\\displaystyle\\int\\Big|\\mathbb{E}\\left[\\gamma_{2}h_{2}(X,Y)(Y-v)\\Big|f(X)=v\\right]\\Big|d P_{S}(f^{-1}(v))}\\\\ &{\\qquad=\\gamma_{1}K_{1}(f,h_{1},P_{S})+\\gamma_{2}K_{1}(f,h_{2},P_{S})}\\\\ &{\\qquad=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "According to Lemma F.5, $K_{1}(f,h,P_{S})=0$ is equivalent as $K_{2}(f,h,P_{S})=0$ for bounded $h$ . Thus, $K_{2}(f,\\gamma_{1}\\bar{h}_{1}+\\gamma_{2}h_{2},P_{S})=0$ which implies $\\gamma_{1}h_{1}+\\gamma_{2}h_{2}\\in\\mathcal{H}_{f}$ . Now we finishes the proof that $\\mathcal{H}_{f}$ is a linear space. ", "page_idx": 24}, {"type": "text", "text": "For $f_{\\Phi}(x)=\\mathbb{E}[Y|\\Phi(x)]$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[Y|f_{\\Phi}(X)]=\\mathbb{E}\\left[\\mathbb{E}[Y|f_{\\Phi}(X),\\Phi(X)]\\middle|f_{\\Phi}(X)\\right]}\\\\ &{\\qquad\\qquad\\qquad=\\mathbb{E}\\left[\\mathbb{E}[Y|\\Phi(X)]\\middle|f_{\\Phi}(X)\\right]}\\\\ &{\\qquad\\qquad\\quad=\\mathbb{E}[f_{\\Phi}(X)|f_{\\Phi}(X)]}\\\\ &{\\qquad\\qquad\\quad=f_{\\Phi}(X).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Thus, $K_{2}(f_{\\Phi},1,P_{S})\\,=\\,0$ which implies $1\\in\\mathcal{H}_{\\Phi}$ . Since $\\mathcal{H}_{\\Phi}$ is a linear space, $\\mathcal{G}\\subset\\mathcal{H}_{\\Phi}$ implies span $\\left\\{1,\\mathcal{G}\\right\\}\\subset\\mathcal{H}_{\\Phi}$ . ", "page_idx": 24}, {"type": "text", "text": "Lemma F.11. For any absolutely continuous probability measure $P(X,Y)$ with a continuous density function $p_{:}$ , and any grouping function $h\\in{\\cal C}(\\mathcal{X}\\times\\mathcal{Y})$ , there exists $\\gamma\\neq0$ and $\\rho\\,\\in\\mathbb{R}$ such that $\\gamma h+\\rho\\in C(\\mathcal{X}\\times\\mathcal{Y})$ , and it is density ratio between some absolutely continuous probability measure $P^{\\prime}$ and $P$ , i.e., $d P^{\\prime}(x,y)=(\\gamma h(x,y)\\!+\\!\\rho)d P(x,y),$ , where $P^{\\prime}$ also has a continuous density function. ", "page_idx": 24}, {"type": "text", "text": "Proof. Since $h$ is bounded, there exists $\\rho^{\\prime}\\in\\mathbb R$ such that $h(x,y)+\\rho^{\\prime}>0$ for any $x\\in\\mathcal{X},y\\in\\mathcal{Y}$ . Define: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\gamma=\\frac{1}{{\\int(h(x,y)+\\rho^{\\prime})d P(x,y)}}.}\\\\ {\\rho=\\frac{\\rho^{\\prime}}{{\\int(h(x,y)+\\rho^{\\prime})d P(x,y)}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "$\\gamma h+\\rho$ is still continuous. ", "page_idx": 24}, {"type": "text", "text": "We have $\\gamma h(x,y)+\\rho=\\gamma(h(x,y)+\\rho^{\\prime})>0$ for any $x\\in\\mathcal{X},y\\in\\mathcal{Y}$ . ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\int d P^{\\prime}(x,y)=\\int(\\gamma h(x,y)+\\rho)d P(x,y)}}\\\\ &{}&{=\\gamma\\int(h(x,y)+\\rho^{\\prime})d P(x,y)}\\\\ &{}&{=1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Thus, $P^{\\prime}(X,Y)$ is an absolutely continuous probability measure. ", "page_idx": 24}, {"type": "text", "text": "Its density function $p^{\\prime}=(\\gamma h+\\rho)p$ is continuous. ", "page_idx": 24}, {"type": "text", "text": "Theorem F.12. Consider an absolutely continuous probability measure $P_{S}(X,Y)$ and a predictor $f_{\\Phi}(x)=\\mathbb{E}_{P_{S}}[Y|\\Phi(x)]$ where $\\Phi:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{d_{\\Phi}}$ , $d_{\\Phi}\\in Z^{+}$ is a measurable function. We abbreviate $\\mathcal{H}_{f_{\\Phi}}$ with $\\mathcal{H}_{\\Phi}$ . ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{H}_{\\Phi}=\\left\\{h\\in C(\\mathcal{X}\\times\\mathcal{Y}):C o\\nu_{P_{S}}\\left[h(X,Y),Y|f_{\\Phi}=v\\right]=0\\ f o r\\ a l m o s t\\ e\\nu e r y\\ v\\in[0,1]\\right\\}}\\\\ &{\\quad\\quad=s p a n\\left\\{\\frac{p(x,y)}{p_{S}(x,y)}:\\mathbb{E}_{P}[Y|f_{\\Phi}]=\\mathbb{E}_{P_{S}}[Y|f_{\\Phi}]\\ \\ a l m o s t\\ s u r e l y,\\ p\\ i s\\ c o n t i n u o u s\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. First we prove: ", "page_idx": 25}, {"type": "text", "text": "$\\mathcal{H}_{\\Phi}=\\{h\\in C(\\mathcal{X}\\times\\mathcal{Y}):\\operatorname{Cov}_{P_{S}}\\,[h(X,Y)|f_{\\Phi}=v]=0$ for almost every v \u2208[0, 1]} . For each $v\\in[0,1]$ and any $h\\in C(\\mathcal{X}\\times\\mathcal{Y})$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{P_{S}}\\left[h(X,Y)(Y-v)\\big|f_{\\Phi}=v\\right]=\\mathbb{E}_{P_{S}}\\left[h(X,Y)Y\\Big|f_{\\Phi}=v\\right]-v\\mathbb{E}_{P_{S}}\\left[h(X,Y)\\Big|f_{\\Phi}=v\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\mathbb{E}_{P_{S}}\\left[h(X,Y)Y\\Big|f_{\\Phi}=v\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad-\\mathbb{E}_{P_{S}}\\left[h(X,Y)\\Big|f_{\\Phi}=v\\right]\\mathbb{E}[Y|f_{\\Phi}=v]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\mathrm{Cov}_{P_{S}}\\left[h(X,Y),Y\\vert f_{\\Phi}=v\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Equation 98 follows from Equation 87. ", "page_idx": 25}, {"type": "text", "text": "For any $h\\in C(\\mathcal{X}\\times\\mathcal{Y})$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{h\\in\\mathcal{H}_{\\Phi}\\Leftrightarrow K_{2}(f_{\\Phi},h,P_{S})=0}\\\\ &{\\quad\\quad\\quad\\Leftrightarrow\\displaystyle\\int\\Big(\\mathbb{E}_{P_{S}}\\left[h(X,Y)(Y-v)\\big|f_{\\Phi}=v\\right]\\Big)^{2}d P_{S}(f_{\\Phi}^{-1}(v))}\\\\ &{\\quad\\quad\\quad\\Leftrightarrow\\mathbb{E}_{P_{S}}\\left[h(X,Y)(Y-v)\\big|f_{\\Phi}=v\\right]=0\\;\\;\\mathrm{for~almost~every~}v\\in[0,1]}\\\\ &{\\quad\\quad\\quad\\Leftrightarrow\\mathrm{Cov}_{P_{S}}\\left[h(X,Y),Y\\vert f_{\\Phi}=v\\right]=0\\;\\;\\mathrm{for~almost~every~}v\\in[0,1].}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Next, we prove ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathcal{H}_{\\Phi}\\supset\\mathrm{span}\\left\\{\\frac{p(x,y)}{p_{S}(x,y)}:\\mathbb{E}_{P}[Y|f_{\\Phi}]=\\mathbb{E}_{P_{S}}[Y|f_{\\Phi}]\\;\\mathrm{~almost~surely,~}p\\mathrm{~is~continuous}\\right\\}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "This is equivalent to saying that for any absolutely continuous probability measure $P$ satisfying $\\mathbb{E}_{P}[Y|f_{\\Phi}]=\\mathbb{E}_{P_{S}}[Y|f_{\\Phi}]$ almost surely, $\\operatorname{Cov}_{P_{S}}\\left[p(X,Y)/p_{S}(X,Y),Y\\vert f_{\\Phi}=v\\right]=0$ for almost every $v\\in[0,1]$ . ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Cor}_{\\mathcal{P}_{x}}\\left[\\frac{p\\left(X,Y\\right)}{p_{S}\\left(X,Y\\right)},Y\\right]f_{\\Phi}=v\\right]}\\\\ &{=\\mathbb{E}_{P_{S}}\\left[\\frac{p\\left(X,Y\\right)}{p_{S}\\left(X,Y\\right)}Y\\right]f_{\\Phi}=v\\right]-\\mathbb{E}_{P_{S}}\\left[\\frac{p\\left(X,Y\\right)}{p_{S}\\left(X,Y\\right)}\\left|f_{\\Phi}=v\\right|\\mathbb{E}_{P_{S}}\\left[Y\\right]f_{\\Phi}=v\\right]}\\\\ &{=\\int\\frac{d P(f_{\\Phi}^{+1}(v))}{d P_{S}\\left(f_{\\Phi}^{+1}(v)\\right)}\\frac{p\\left(x,y\\right)\\int_{\\Phi}\\cdot r\\,d y_{S}\\left|f_{\\Phi}=v\\right|}{p_{S}\\left(x,y\\right)\\int_{\\Phi}\\hat{r}\\left(q_{S}^{+1}(v)\\right)}y\\cdot d P_{S}\\left(x,y\\right)f_{\\Phi}=v\\right]}\\\\ &{\\quad-\\mathbb{E}_{P_{S}}\\left[Y\\right]f_{\\Phi}=v\\right]\\cdot\\int\\frac{d P(f_{\\Phi}^{+1}(v))}{d P_{S}\\left(f_{\\Phi}^{+1}(v)\\right)}\\frac{p\\left(x,y\\right)\\int_{\\Phi}\\cdot r\\,d y_{S}}{p_{S}\\left(x,y\\right)\\int_{\\Phi}\\hat{r}\\left(q_{S}^{+}=v\\right)}\\cdot d P_{S}(x,y\\vert f_{\\Phi}=v)}\\\\ &{=\\mathbb{E}_{P}\\left[Y\\right]f_{\\Phi}=v\\right]\\frac{d P(f_{\\Phi}^{+1}(v))}{d P_{S}\\left(f_{\\Phi}^{-1}(v)\\right)}-\\mathbb{E}_{P_{S}}\\left[Y\\right]f_{\\Phi}=v\\right]\\frac{d P(f_{\\Phi}^{-1}(v))}{d P_{S}\\left(f_{\\Phi}^{-1}(v)\\right)}}\\\\ &{=\\left[\\mathbb{E}_{P}\\left[Y\\right]f_{\\Phi}=v\\right]-\\mathbb{E}_{P_{S}}\\left[Y\\right]f_{\\Phi}=v\\right]\\frac{d P(f_{\\Phi}^{+1}(v))}{d P_{S}\\left(f_{\\Phi}^{-1}(v)\\right)}}\\\\ &{=0\\mathrm{~and~sucre~tord~}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Next, we prove ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathcal{H}_{\\Phi}\\subset\\mathrm{span}\\left\\{\\frac{p(x,y)}{p_{S}(x,y)}:\\mathbb{E}_{P}[Y|f_{\\Phi}]=\\mathbb{E}_{P_{S}}[Y|f_{\\Phi}]\\;\\mathrm{~almost~surely,~}p\\mathrm{~is~continuous}\\right\\}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "By Lemma F.11, any grouping function $h\\in\\mathcal{H}_{\\Phi}$ could be rewritten as $h(x,y)=\\gamma p(x,y)/p_{S}(x,y)+$ $\\rho$ for some continuous density functions $p$ and $\\gamma\\neq0$ . Thus, we just need to prove the statement that ", "page_idx": 25}, {"type": "text", "text": "$\\mathsf{C o v}_{P_{S}}\\left[\\gamma p(X,Y)/p_{S}(X,Y)+\\rho,Y|f_{\\Phi}=v\\right]=0$ implies $\\mathbb{E}_{P}[Y|f_{\\Phi}=v]=\\mathbb{E}_{P_{S}}[Y|f_{\\Phi}=v]$ . ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathrm{Cov}_{P_{S}}\\left[\\gamma_{p}\\frac{p\\left(X,Y\\right)}{p_{S}\\left(X,Y\\right)}+\\rho,Y\\middle|\\mathcal{f}_{\\Phi}=v\\right]}\\\\ &{=\\gamma\\mathbb{E}_{P_{S}}\\left[\\frac{p\\left(X,Y\\right)}{p_{S}\\left(X,Y\\right)}Y\\middle|\\mathcal{f}_{\\Phi}=v\\right]+\\rho E_{P_{S}}\\left[Y\\middle|\\mathcal{f}_{\\Phi}=v\\right]}\\\\ &{\\quad-\\gamma E_{P_{S}}\\left[\\frac{p\\left(X,Y\\right)}{p_{S}\\left(X,Y\\right)}\\middle|f_{\\Phi}=v\\right]E_{P_{S}}\\left[Y\\middle|f_{\\Phi}=v\\right]-\\rho E_{P_{S}}\\left[Y\\middle|f_{\\Phi}=v\\right]}\\\\ &{=\\gamma\\mathbb{E}_{P_{S}}\\left[\\frac{p\\left(X,Y\\right)}{p_{S}\\left(X,Y\\right)}Y\\middle|f_{\\Phi}=v\\right]-\\gamma E_{P_{S}}\\left[\\frac{p\\left(X,Y\\right)}{p_{S}\\left(X,Y\\right)}\\middle|f_{\\Phi}=v\\right]E_{P_{S}}\\left[Y\\middle|f_{\\Phi}=v\\right]}\\\\ &{=\\gamma\\mathrm{Cov}_{P_{S}}\\left[\\frac{p\\left(X,Y\\right)}{p_{S}\\left(X,Y\\right)},Y\\middle|f_{\\Phi}=v\\right]}\\\\ &{=\\gamma\\left[\\mathbb{E}_{P}\\left[Y\\middle|f_{\\Phi}=v\\right]-\\mathbb{E}_{P_{S}}\\left[Y\\middle|f_{\\Phi}=v\\right]\\right]\\frac{d P\\left(f_{\\Phi}^{-1}(v)\\right)}{d P_{S}\\left(f_{\\Phi}^{-1}(v)\\right)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Equation 118 follows from Equation 110. So we have $\\mathbb{E}_{P}[Y|f_{\\Phi}\\;=\\;v]\\;=\\;\\mathbb{E}_{P_{S}}[Y|f_{\\Phi}\\;=\\;v]$ if $\\bar{\\mathrm{Cov}_{P s}}\\left[\\gamma p(X,Y)/p_{S}(X,Y)+\\bar{\\rho_{,}}Y|f_{\\Phi}=v\\right]=0$ . ", "page_idx": 26}, {"type": "text", "text": "Corollary F.13 (Restatement of Theorem 4.2). Consider an absolutely continuous probability measure $P_{S}(X,Y)$ and a calibrated predictor $f$ . ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{H}_{f}=\\left\\{h\\in C(\\mathcal{X}\\times\\mathcal{Y}):C o\\nu_{P_{S}}\\left[h(X,Y),Y|f=v\\right]=0\\ f o r\\,a l m o s t\\ e\\nu e r y\\ v\\in[0,1]\\right\\}}\\\\ &{\\quad\\quad=s p a n\\left\\{\\frac{p(x,y)}{p_{S}(x,y)}:\\mathbb{E}_{P}[Y|f]=\\mathbb{E}_{P_{S}}[Y|f]\\ \\ a l m o s t\\,s u r e l y,\\ p\\ i s\\ c o n t i n u o u s\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Remark F.14. $\\mathbb{E}_{P}[Y|f]=\\mathbb{E}_{P_{S}}[Y|f]$ almost surely implies $\\mathbb{E}_{P}[Y|f]=f$ , which is equivalent as $\\begin{array}{r}{R_{P}(f)=\\operatorname*{inf}_{g:[0,1]\\to[0,1]}R_{P}(g\\circ f)}\\end{array}$ , since we adopt square error. ", "page_idx": 26}, {"type": "text", "text": "Proof. Since $f$ is calibrated, we have $\\mathbb{E}_{P_{S}}[Y|f]=f$ . Take $\\Phi(x)=f(x)$ . ", "page_idx": 26}, {"type": "equation", "text": "$$\nf_{\\Phi}(x)=\\mathbb{E}_{P_{S}}[Y|\\Phi(x)]=\\mathbb{E}_{P_{S}}[Y|f(x)]=f(x).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Apply Theorem F.12 and the proof is complete. ", "page_idx": 26}, {"type": "text", "text": "Theorem F.15 (Restatement of Theorem 4.3 (first part)). Consider an absolutely continuous probability measure $P_{S}(X,Y)$ and a predictor $f_{\\Phi}(x)=\\mathbb{E}_{P_{S}}[Y|\\Phi(x)]$ where $\\Phi:\\mathbb{R}^{d}\\stackrel{\\cdot}{\\to}\\mathbb{R}^{d_{\\Phi}},d_{\\Phi}\\in\\dot{Z}^{+}$ is a measurable function. $\\mathcal{H}_{\\Phi}$ can be decomposed as $\\mathcal{H}_{\\Phi}=\\mathcal{H}_{1,\\Phi}+\\mathcal{H}_{2,\\Phi}$ . ", "page_idx": 26}, {"type": "equation", "text": "$v\\in[0,1]\\}$ $$\n\\begin{array}{r l}&{\\mathcal{H}_{1,\\Phi}:=\\{h\\in C(\\mathcal{X}\\times\\mathcal{Y}):C o\\nu_{P s}\\;|h(\\Phi,Y),Y|f_{\\Phi}=v\\}=0\\;\\;f o r\\;a l m o s t\\;e\\nu e r y\\;v\\in|0,\\mathcal{X}|\\mathcal{Y}|\\;,}\\\\ &{\\quad\\quad=s p a n\\left\\{\\frac{p(\\Phi,y)}{p_{S}(\\Phi,y)}:\\mathbb{E}_{P}[Y|f_{\\Phi}]=\\mathbb{E}_{P_{S}}[Y|f_{\\Phi}]\\;\\;a l m o s t\\;s u r e l y,\\;p\\;i s\\;c o n t i n u o u s\\right\\}.}\\\\ &{\\mathcal{H}_{2,\\Phi}:=\\{h\\in C(\\mathcal{X}\\times\\mathcal{Y}):\\mathbb{E}[h(X,Y)|\\Phi,Y]\\equiv C_{h}\\;\\;\\forall\\Phi,Y\\}}\\\\ &{\\quad\\quad=s p a n\\left\\{\\frac{p(x|\\Phi,y)}{p_{S}(x|\\Phi,y)}:\\;p\\;i s\\;c o n t i n u o u s\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Remark F.16. $\\mathcal{H}_{1,\\Phi}$ contains functions defined on $\\ s u p p(\\Phi)\\times\\mathcal{V}$ which can be rewritten as functions on $\\mathcal X\\times\\mathcal X$ by variable substitution. Thus, $\\mathcal{H}_{1,\\Phi}$ is still a set of grouping functions. For $\\mathcal{H}_{2,\\Phi}$ , $C_{h}$ is $a$ constant depending on $h$ . ", "page_idx": 26}, {"type": "text", "text": "Proof. First we prove $\\mathcal{H}_{\\Phi}\\subset\\mathcal{H}_{1,\\Phi}+\\mathcal{H}_{2,\\Phi}$ . ", "page_idx": 26}, {"type": "text", "text": "For any $h_{1}\\in\\mathcal{H}_{1,\\Phi}$ and $h_{2}\\in\\mathcal{H}_{2,\\Phi}$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\mathrm{~Cov}_{P s}\\left[h_{1}(\\Phi,Y)+h_{2}(X,Y),Y|f_{\\Phi}=v\\right]}&{\\mathrm{~(~S_\\Lambda)~}}\\\\ &{=\\mathrm{Cov}_{P s}\\left[h_{1}(\\Phi,Y),Y|f_{\\Phi}=v\\right]+\\mathrm{Cov}_{P s}\\left[h_{2}(X,Y),Y|f_{\\Phi}=v\\right]}&{\\mathrm{~(~S_\\Lambda)~}}\\\\ &{=\\mathrm{Cov}_{P s}\\left[h_{2}(X,Y),Y|f_{\\Phi}=v\\right]}&{\\mathrm{~(~S_\\Lambda)~}}\\\\ &{=\\mathbb{E}_{P s}\\left[h_{2}(X,Y){Y}|f_{\\Phi}=v\\right]-\\mathbb{E}_{P s}\\left[h_{2}(X,Y)|f_{\\Phi}=v\\right]\\mathbb{E}_{P s}\\left[{Y}|f_{\\Phi}=v\\right]}&{\\mathrm{~(~S_\\Lambda)~}}\\\\ &{=\\mathbb{E}_{P s}\\left[\\mathbb{E}_{P s}\\left[h_{2}(X,Y){Y}|\\Phi,Y\\right]|f_{\\Phi}=v\\right]-\\mathbb{E}_{P s}\\left[\\mathbb{E}_{P s}\\left[h_{2}(X,Y)|\\Phi,Y\\right]\\left|f_{\\Phi}=v\\right]\\mathbb{E}_{P s}\\left[{Y}|f_{\\Phi}=v\\right]}&{\\mathrm{~(~S_\\Lambda)~}}\\\\ &{=\\mathbb{E}_{P s}\\left[\\mathbb{E}_{P s}\\left[h_{2}(X,Y)|\\Phi,Y\\right]\\mathbb{V}\\left|f_{\\Phi}=v\\right]-\\mathbb{E}_{P s}\\left[\\mathbb{E}_{P s}\\left[h_{2}(X,Y)|\\Phi,Y\\right]\\mathbb{E}_{Y}\\left|f_{\\Phi}=v\\right]\\mathbb{E}_{P s}\\left[{Y}|f_{\\Phi}=v\\right]}&{\\mathrm{~(~S_\\Lambda)~}}\\\\ &{=\\mathbb{E}_{M}\\mathbb{E}_{P s}\\left[{Y}|f_{\\Phi}=v\\right]-\\mathbb{C}_{h_{2}}\\mathbb{E}_{P s}\\left[{Y}|f_{\\Phi}=v\\right]}&{\\mathrm{~(~S_\\Lambda)~}}\\\\ &{=0}&{\\mathrm{~(~S_\\Lambda~}\\mathrm{)~}}&{\\mathrm{~(~R_\\Lambda~}\\mathrm{)~}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Next we prove $\\mathcal{H}_{\\Phi}\\supset\\mathcal{H}_{1,\\Phi}+\\mathcal{H}_{2,\\Phi}$ . ", "page_idx": 27}, {"type": "text", "text": "For any $h\\in\\mathcal{H}_{\\Phi}$ , let ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{h_{1}(\\Phi(x),y)=\\mathbb{E}_{P_{S}}[h(X,Y)|\\Phi=\\Phi(x),Y=y].}\\\\ &{\\qquad h_{2}(x,y)=h(x,y)-\\mathbb{E}_{P_{S}}[h(X,Y)|\\Phi=\\Phi(x),Y=y].}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Then we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathrm{Cov}_{P s}\\left[h_{1}(\\Phi,Y),Y|f_{\\Phi}=v\\right]}\\\\ &{}&{=\\mathbb{E}_{P s}\\left[h_{1}(\\Phi,Y)Y|f_{\\Phi}=v\\right]-\\mathbb{E}_{P s}\\left[h_{1}(\\Phi,Y)|f_{\\Phi}=v\\right]\\mathbb{E}_{P s}\\left[Y|f_{\\Phi}=v\\right]}\\\\ &{}&{=\\mathbb{E}_{P s}\\left[\\mathbb{E}_{P s}\\left[h(X,Y)|\\Phi,Y|Y\\right]f_{\\Phi}=v\\right]-\\mathbb{E}_{P s}\\left[\\mathbb{E}_{P s}\\left[h(X,Y)|\\Phi,Y\\right]\\middle|f_{\\Phi}=v\\right]\\mathbb{E}_{P s}\\left[Y|f_{\\Phi}=v\\right]}\\\\ &{}&{=\\mathbb{E}_{P s}\\left[h(X,Y)Y|f_{\\Phi}=v\\right]-\\mathbb{E}_{P s}\\left[h(X,Y)|f_{\\Phi}=v\\right]\\mathbb{E}_{P s}\\left[Y|f_{\\Phi}=v\\right]}\\\\ &{}&{=\\mathrm{Cov}_{P s}\\left[h(X,Y),Y|f_{\\Phi}=v\\right]}\\\\ &{}&{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad(14\\%-v\\right]}\\\\ &{}&{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad(14\\%-\\alpha)}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Thus, $h_{1}(\\Phi(x),y)\\in\\mathcal{H}_{1,\\Phi}$ . ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{P s}[h_{2}(X,Y)|\\Phi,Y]=\\mathbb{E}_{P s}\\left[h(X,Y)-\\mathbb{E}_{P s}[h(X,Y)|\\Phi,Y]\\middle|\\Phi,Y\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\mathbb{E}_{P s}[h(X,Y)|\\Phi,Y]-\\mathbb{E}_{P s}[h(X,Y)|\\Phi,Y]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Thus, $h_{2}(x,y)\\in\\mathcal{H}_{2,\\Phi}$ . Following a similar proof of Theorem F.12, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n{\\mathcal{H}}_{1,\\Phi}=\\operatorname{span}\\left\\{{\\frac{p(\\Phi,y)}{p_{S}(\\Phi,y)}}:\\mathbb{E}_{P}[Y|f_{\\Phi}]=\\mathbb{E}_{P_{S}}[Y|f_{\\Phi}]\\;{\\mathrm{~almost~surely,~}}p{\\mathrm{~is~continuous}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Next, we prove $\\mathcal{H}_{2,\\Phi}\\supset$ span $\\{p(x|\\Phi,y)/p_{S}(x|\\Phi,y):\\,p{\\mathrm{~is~continuous}}$ ", "page_idx": 27}, {"type": "text", "text": "This is equivalent to saying that $p(x|\\Phi,y)/p_{S}(x|\\Phi,y)\\in\\mathcal{H}_{2,\\Phi}$ for any continuous density function $p$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{P_{S}}\\left[\\frac{p\\left(X\\mid\\Phi,Y\\right)}{p_{S}\\left(X\\mid\\Phi,Y\\right)}\\Big\\vert\\Phi,Y\\right]=\\int\\frac{p\\left(x\\mid\\Phi,Y\\right)}{p_{S}\\left(x\\mid\\Phi,Y\\right)}d P_{S}(x\\mid\\Phi,Y)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\int d P(x\\mid\\Phi,Y)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\quad=1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Thus, we have $p(x|\\Phi,y)/p_{S}(x|\\Phi,y)\\in\\mathcal{H}_{2,\\Phi}$ . ", "page_idx": 27}, {"type": "text", "text": "Next, we prove $\\mathcal{H}_{2,\\Phi}\\subset$ span $\\{p(x|\\Phi,y)/p_{S}(x|\\Phi,y):\\,p{\\mathrm{~is~continuo}}$ $\\}$ ", "page_idx": 27}, {"type": "text", "text": "By Lemma F.11, any grouping function $\\begin{array}{r l r}{h}&{{}\\in}&{\\mathcal{H}_{2,\\Phi}}\\end{array}$ could be rewritten as $\\begin{array}{r l}{h_{2}(x,y)}&{{}=}\\end{array}$ $\\gamma_{p}(x,y)/p_{S}(x,y)+\\rho$ for some continuous density function $p$ and $\\gamma\\neq0$ . Thus, we just need to prove the statement that $\\mathbb{E}_{P_{S}}[\\gamma p(X,Y)/p_{S}(X,Y)\\stackrel{.}{+}\\rho|\\Phi,Y]\\equiv C_{h_{2}}$ implies $p(X,Y)/\\bar{p}_{S}(X,Y)\\equiv$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p(X|\\Phi,Y)/p_{S}(X|\\Phi,Y).}\\\\ &{\\qquad\\qquad\\qquad\\mathbb{E}_{P_{S}}\\left[\\gamma\\frac{p(X,Y)}{p_{S}(X,Y)}+\\rho\\Big|\\Phi,Y\\right]=\\mathbb{E}_{P_{S}}\\left[\\gamma\\frac{p(X|\\Phi,Y)}{p_{S}(X|\\Phi,Y)}\\frac{p(\\Phi,Y)}{p_{S}(\\Phi,Y)}+\\rho\\Big|\\Phi,Y\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\gamma\\frac{p(\\Phi,Y)}{p_{S}(\\Phi,Y)}\\mathbb{E}_{P_{S}}\\left[\\frac{p(X|\\Phi,Y)}{p_{S}(X|\\Phi,Y)}\\Big|\\Phi,Y\\right]+\\rho}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\gamma\\frac{p(\\Phi,Y)}{p_{S}(\\Phi,Y)}\\mathbb{E}_{P}\\left[1\\Big|\\Phi,Y\\right]+\\rho}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\gamma\\frac{p(\\Phi,Y)}{p_{S}(\\Phi,Y)}+\\rho.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Thus, we have $\\begin{array}{r l r}{p(\\Phi,Y)/p_{S}(\\Phi,Y)}&{{}\\equiv}&{(C_{h_{2}}\\mathrm{~\\,~-~\\,~}\\rho)/\\gamma}\\end{array}$ which is a constant. Since $\\mathbb{E}_{P_{S}}[p(\\Phi,Y)/p_{S}(\\Phi,\\dot{Y})]=1$ , we have $p(\\Phi,Y)/p_{S}(\\Phi,Y)\\equiv1$ . ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{p(X,Y)}{p_{S}(X,Y)}=\\frac{p(X\\mid\\Phi,Y)}{p_{S}(X\\mid\\Phi,Y)}\\frac{p(\\Phi,Y)}{p_{S}(\\Phi,Y)}}\\\\ &{\\qquad\\qquad\\qquad=\\frac{p(X\\mid\\Phi,Y)}{p_{S}(X\\mid\\Phi,Y)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Lemma F.17 (Theorem 3.2 from Globus-Harris et al. [13]). If $f$ is calibrated and there exists an $h(x)$ such that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{E}[(f(X)-Y)^{2}-(h(X)-Y)^{2}|f(X)=v]\\geq\\alpha,\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "then: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{E}[h(X)(Y-v)|f(X)=v]\\geq{\\frac{\\alpha}{2}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proposition F.18 (Restatement of Theorem 4.3 (second part)). If a predictor $f$ is multicalibrated with $\\mathcal{H}_{1,\\Phi}$ , then $R_{P_{S}}(f)\\le R_{P_{S}}(f_{\\Phi})$ . ", "page_idx": 28}, {"type": "text", "text": "Proof. We prove by contradiction. If $R_{P_{S}}(f)>R_{P_{S}}(f_{\\Phi})$ , then ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\int\\mathbb{E}\\left[(f(X)-Y)^{2}-(f_{\\Phi}(X)-Y)^{2}|f(X)=v\\right]d P_{S}(f^{-1}(v))>0.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "equation", "text": "$\\alpha_{v}=\\mathbb{E}\\left[(f(X)-Y)^{2}-(h(X)-Y)^{2}|f(X)=v\\right].$ ", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Since $f$ is multicalibrated with $\\mathcal{H}_{1,\\Phi}$ , $f$ is calibrated. It follows from Lemma F.17: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{E}[f_{\\Phi}(X)(Y-v)|f(X)=v]\\geq{\\frac{\\alpha_{v}}{2}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Then, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{K_{1}(f,f_{\\Phi},P_{S})=\\displaystyle\\int\\Big|E[f_{\\Phi}(X)(Y-v)|f(X)=v]\\Big|d P_{S}(f^{-1}(v))}\\\\ &{\\qquad\\qquad\\quad\\geq\\displaystyle\\int\\alpha_{v}d P_{S}(f^{-1}(v))}\\\\ &{\\qquad\\qquad\\quad>0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "From Lemma F.5, we have $K_{2}(f,f_{\\Phi},P_{S})\\ge K_{1}(f,f_{\\Phi},P_{S})^{2}>0$ . Since $f_{\\Phi}\\in\\mathcal{H}_{1,\\Phi}$ , it contradicts with the fact that $f$ is multicalibrated with $\\mathcal{H}_{1,\\Phi}$ . \u53e3 ", "page_idx": 28}, {"type": "text", "text": "Proposition F.19 (Restatement of Theorem 4.3 (third part)). $f_{\\Phi}$ is an invariant predictor elicited by $\\Phi$ across a set of environments $\\mathcal{E}$ where $P_{e}(\\Phi,Y)=P_{S}(\\Phi,Y)$ for any $e\\in{\\mathcal{E}}$ . If a predictor $f$ is multicalibrated with $\\mathcal{H}_{2,\\Phi}$ , then $f$ is also an invariant predictor across $\\mathcal{E}$ elicited by some representation. ", "page_idx": 28}, {"type": "text", "text": "Proof. Since $P_{e}(\\Phi,Y)=P_{S}(\\Phi,Y)$ , we have $\\mathbb{E}_{P_{e}}[Y|\\Phi]=\\mathbb{E}_{P_{S}}[Y|\\Phi]=f_{\\Phi}$ for every $e\\in{\\mathcal{E}}$ . ", "page_idx": 29}, {"type": "text", "text": "Thus, ", "page_idx": 29}, {"type": "equation", "text": "$$\nR_{P_{e}}(f_{\\Phi})=\\operatorname*{inf}_{g\\in\\mathcal{G}}R_{P_{e}}(g\\circ\\Phi).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "This implies $f_{\\Phi}$ is an invariant predictor elicited by $\\Phi$ across $\\mathcal{E}$ . ", "page_idx": 29}, {"type": "text", "text": "For any $e\\in{\\mathcal{E}}$ , we have: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\frac{p_{e}(x,y)}{p_{S}(x,y)}=\\frac{p_{e}(\\Phi,y)}{p_{S}(\\Phi,y)}\\frac{p_{e}(x|P h i,y)}{p_{S}(x|\\Phi,y)}=\\frac{p_{e}(x|\\Phi,y)}{p_{S}(x|\\Phi,y)}\\in\\mathcal{H}_{2,\\Phi}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Since $f$ is multicalibrated with $\\mathcal{H}_{2,\\Phi}$ , it follows from Theorem F.8: ", "page_idx": 29}, {"type": "equation", "text": "$$\nR_{P_{e}}(f)=\\operatorname*{inf}_{g:[0,1]\\to[0,1]}R_{P_{e}}(g\\circ f).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "This implies that $f$ is an invariant predictor across $\\mathcal{E}$ elicited by $f$ . ", "page_idx": 29}, {"type": "text", "text": "Proposition F.20 (Restatement of Proposition 4.5). Consider $X\\,\\in\\,\\mathbb{R}^{d}$ which could be sliced as $\\bar{X}\\bar{=}(\\Phi,\\Psi)^{T}$ and $\\Phi=(\\Lambda,\\Omega)^{T}$ . Define $\\mathcal{H}_{1,\\Phi}^{\\prime}:=\\left\\{h(\\Phi(x))\\in C(\\mathcal{X}\\times\\mathcal{Y})\\right\\}$ , with $\\mathcal{H}_{1,\\Phi}^{\\prime}\\subset\\mathcal{H}_{1,\\Phi}$ . $\\mathcal{H}_{1,X}^{\\prime}$ and $\\mathcal{H}_{1,\\Lambda}^{\\prime}$ are similarly defined. We have: ", "page_idx": 29}, {"type": "text", "text": "$C$ is a constant value function. ", "page_idx": 29}, {"type": "text", "text": "Remark F.21. The proposition shows that $\\mathcal{H}_{1}^{\\prime}$ , as a subspace of $\\mathcal{H}_{1}$ , evolves monotonically and in opposite direction to $\\mathcal{H}_{2}$ . If we perceive the representation $\\Phi$ as a filter, gaining more information from covariates facilitates multicalibration $\\boldsymbol{w}$ .r.t. $\\mathcal{H}_{1}$ (and accuracy) but hampers multicalibration w.r.t. $\\mathcal{H}_{2}$ (and invariance). With $\\mathcal{H}_{1}^{\\prime}$ and $\\mathcal{H}_{2}$ combined together, a multicalibrated predictor is searching for an appropriate level of information fliter to balance the tradeoff between accuracy and invariance. ", "page_idx": 29}, {"type": "text", "text": "Proof. We first prove $\\mathcal{H}_{1,\\Phi}^{\\prime}\\subset\\mathcal{H}_{1,\\Phi}$ . According to Equation 122, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathcal{H}_{1,\\Phi}:=\\{h\\in C(\\mathcal{X}\\times\\mathcal{Y}):\\operatorname{Cov}\\left[h(\\Phi,Y),Y|f_{\\Phi}=v\\right]=0\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Since $h(\\Phi)\\perp Y\\mid\\Phi$ , we have Cov $[h(\\Phi),Y|\\Phi]=0$ . ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~~~\\mathrm{Cov}\\left[h(\\Phi),Y|f_{\\Phi}\\right]}\\\\ &{=\\mathbb{E}\\left[h(\\Phi)Y|f_{\\Phi}\\right]-\\mathbb{E}\\left[h(\\Phi)|f_{\\Phi}\\right]\\mathbb{E}\\left[Y|f_{\\Phi}\\right]}\\\\ &{=\\mathbb{E}\\left[h(\\Phi)Y|f_{\\Phi}\\right]-\\mathbb{E}\\left[h(\\Phi)|f_{\\Phi}\\right]f_{\\Phi}}\\\\ &{=\\mathbb{E}\\left[\\mathbb{E}\\left[h(\\Phi)Y|\\Phi\\right]-\\mathbb{E}\\left[h(\\Phi)|\\Phi\\right]f_{\\Phi}|f_{\\Phi}\\right]}\\\\ &{=\\mathbb{E}\\left[\\mathbb{E}\\left[h(\\Phi)Y|\\Phi\\right]-\\mathbb{E}\\left[h(\\Phi)|\\Phi\\right]\\mathbb{E}\\left[Y|\\Phi\\right]|f_{\\Phi}\\right]}\\\\ &{=\\mathbb{E}\\left[\\mathrm{Cov}\\left[h(\\Phi),Y|\\Phi\\right]|f_{\\Phi}\\right]}\\\\ &{=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Thus, $\\mathcal{H}_{1,\\Phi}^{\\prime}\\subset\\mathcal{H}_{1,\\Phi}$ . Next, we prove the two arguments in the proposition. ", "page_idx": 29}, {"type": "text", "text": "1. For any $h(\\Lambda)\\in\\mathcal{H}_{1,\\Lambda}^{\\prime}$ , since $\\Phi=(\\Lambda,\\Omega),h(\\Lambda)$ is also a function of $\\Phi$ . Thus, we have $h(\\Lambda)\\in\\mathcal{H}_{1,\\Phi}^{\\prime}$ .   \nIt follows that $\\mathcal{H}_{1,\\Phi}^{\\prime}\\supset\\mathcal{H}_{1,\\Lambda}^{\\prime}$ . Similarly we have $\\mathcal{H}_{1,X}^{\\prime}\\supset\\mathcal{H}_{1,\\Phi}^{\\prime}$ and $\\mathcal{H}_{1,\\Lambda}^{\\prime}\\supset\\mathcal{H}_{1,\\emptyset}^{\\prime}$ . ", "page_idx": 29}, {"type": "text", "text": "2. For any $h(\\Lambda,\\Omega,\\Psi,Y)\\in\\mathcal{H}_{2,\\Phi}$ such that $\\mathbb{E}[h(\\Phi,\\Psi,Y)|\\Phi,Y]=C_{h}$ for any values of $\\Phi,Y$ , we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[h(\\Lambda,\\Omega,\\Psi,Y)|\\Lambda,Y]=\\mathbb{E}\\left[\\mathbb{E}[h(\\Lambda,\\Omega,\\Psi,Y)|\\Lambda,\\Omega,Y]|\\Lambda,Y\\right]}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\mathbb{E}\\left[\\mathbb{E}[h(\\Phi,\\Psi,Y)|\\Phi,Y]|\\Lambda,Y\\right]}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\mathbb{E}[C_{h}|\\Lambda,Y]}\\\\ &{\\quad\\quad\\quad\\quad\\quad=C_{h}\\quad\\mathrm{for~any~values~of~}\\Lambda,Y.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Thus, $h(\\Lambda,\\Omega,\\Psi,Y)\\in\\mathcal{H}_{2,\\Lambda}$ . It follows that $H_{2,\\Phi}\\subset\\mathcal{H}_{2,\\Lambda}$ . Similarly, we have $\\mathcal{H}_{2,X}\\subset H_{2,\\Phi}$ and $\\mathcal{H}_{2,\\Lambda}\\subset\\mathcal{H}_{2,\\emptyset}$ . Particularly for $h(x,y)\\in\\mathcal{H}_{2,X}$ , we have $\\dot{h}(X,Y)=\\mathbb{E}[h(X,Y)|X,Y]$ is a constant for any values of $X,Y$ . \u53e3 ", "page_idx": 29}, {"type": "text", "text": "Lemma F.22 (Globus-Harris et al. [13]). Let $\\mathcal{H}\\subset\\mathcal{X}^{\\mathbb{R}}$ be such a grouping function class that $h\\in\\mathcal H$ implies $\\gamma h+\\rho\\in\\mathcal{H}$ for any $h\\in\\mathcal H$ and $\\gamma$ , $\\rho\\in\\mathbb{R}$ . If $\\mathcal{H}$ satisfies the $(0,0)$ -weak learning condition in Assumption $F.I$ , a predictor $f$ is multicalibrated w.r.t. $\\mathcal{H}$ if and only if $\\dot{f}(x)=\\mathbb{E}[Y|x]$ almost surely. ", "page_idx": 30}, {"type": "text", "text": "Proposition F.23. For a measurable function $\\Phi:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{d_{\\Phi}},d_{\\Phi}\\in Z^{+}$ , a predictor $f:s u p p(\\Phi)\\rightarrow$ $[0,1]$ is multicalibrated w.r.t. $\\mathcal{H}_{1,\\Phi}$ if and only if $f$ is multicalibrated w.r.t. $\\mathcal{H}_{1,\\Phi}^{\\prime}$ . ", "page_idx": 30}, {"type": "text", "text": "Proof. Since $\\mathcal{H}_{1,\\Phi}^{\\prime}\\subset\\mathcal{H}_{1,\\Phi},$ $f$ \u2019s multicalibration w.r.t. $\\mathcal{H}_{1,\\Phi}$ implies $f$ \u2019s multicalibration w.r.t. $\\mathcal{H}_{1,\\Phi}^{\\prime}$ . On the other hand, $\\mathcal{H}_{1,\\Phi}^{\\prime}$ satisfies the $(0,0)$ -weak learning condition with the pushforward measure on $\\Phi$ , because $\\mathbb{E}[Y|\\Phi]\\in\\mathcal{H}_{1,\\Phi}^{\\prime}$ . It follows from Lemma F.22 that $f$ is multicalibrated w.r.t. $\\mathcal{H}_{1,\\Phi}^{\\prime}$ implies $f(\\Phi)=\\mathbb{E}[Y|\\Phi]$ almost surely. By the definition of $\\mathcal{H}_{1,\\Phi},f(\\Phi)$ is multicalibrated w.r.t. $\\mathcal{H}_{1,\\Phi}$ . ", "page_idx": 30}, {"type": "text", "text": "F.4 MC-PseudoLabel: An Algorithm for Extended Multicalibration ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Lemma F.24. Fix a model $f:\\mathcal{X}\\to[0,1]$ . Suppose for some $v\\in R a n g e(f)$ there is an $h\\in\\mathcal H$ such that: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{E}[h(x,y)(y-v)|f(x)=v]>\\alpha\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Let h\u2032 = v + \u03b7h(x, y) for \u03b7 = E[h(x,y)2\u03b1|f(x)=v]. ", "page_idx": 30}, {"type": "text", "text": "Then: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{E}[(f(x)-y)^{2}-(h^{\\prime}(x,y)-y)^{2}|f(x)=v]>\\frac{\\alpha^{2}}{\\mathbb{E}[h(x,y)^{2}|f(x)=v]}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof. Following [13], we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[(f(x)-y)^{2}-(h^{\\prime}(x,y)-y)^{2}|f(x)=v]}\\\\ &{\\qquad\\qquad=\\mathbb{E}[(v-y)^{2}-(v+\\eta h(x,y)-y)^{2}|f(x)=v]}\\\\ &{\\qquad\\qquad=\\mathbb{E}[v^{2}-2v y+y^{2}-(v+\\eta h(x,y))^{2}+2y(v+\\eta h(x,y))-y^{2}|f(x)=v]}\\\\ &{\\qquad\\qquad=\\mathbb{E}[2\\eta h(x,y)-2\\eta v h(x,y)-\\eta^{2}h(x,y)^{2}|f(x)=v]}\\\\ &{\\qquad\\qquad=\\mathbb{E}[2\\eta h(x,y)(y-v)-\\eta^{2}h(x,y)^{2}|f(x)=v]}\\\\ &{\\qquad\\qquad>2\\eta\\alpha-\\eta^{2}\\mathbb{E}[h(x,y)^{2}|f(x)=v]}\\\\ &{\\qquad\\qquad=\\frac{\\alpha^{2}}{\\mathbb{E}[h(x,y)^{2}|f(x)=v]}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Theorem F.25 (Restatement of Theorem 5.1). In Algorithm $^{\\,l}$ , for $\\alpha,B\\,>\\,0,$ , if the following is satisfied: ", "page_idx": 30}, {"type": "equation", "text": "$$\ne r r_{t-1}-e\\tilde{r}r_{t}\\leq\\frac{\\alpha}{B},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "the output $f_{t-1}^{\\prime}(x)$ is $\\alpha$ -approximately $\\ell_{2}$ multicalibrated w.r.t. $\\mathcal{H}_{B}=\\{h\\in\\mathcal{H}:\\operatorname*{sup}h(x,y)^{2}\\leq B\\}$ . ", "page_idx": 30}, {"type": "text", "text": "Proof. We prove by contradiction. Assume that $f_{t-1}$ is not $\\alpha$ -approximately multicalibrated with respect to $\\mathcal{H}_{B}$ . Then there exists $h\\in\\mathcal{H}_{B}$ such that: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\sum_{v\\in[1/m]}P(f_{t-1}(x)=v)\\left(\\mathbb{E}\\left[h(x,y)(y-v)\\Big|f_{t-1}(x)=v\\right]\\right)^{2}>\\alpha.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "For each $v\\in[1/m]$ define ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\alpha_{v}:=P(f_{t-1}(x)=v)\\left(\\mathbb{E}\\left[h(x,y)(y-v)\\Big|f_{t-1}(x)=v\\right]\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Then we have $\\textstyle\\sum_{v\\in[1/m]}\\alpha_{v}>\\alpha$ ", "page_idx": 30}, {"type": "text", "text": "According to Lemma F.24, for each $v\\in[1/m]$ , there exists $h_{v}\\in\\mathcal{H}$ such that: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~~\\mathbb{E}\\left[(f_{t-1}(x)-y)^{2}-(h_{v}(x,y)-y)^{2}|f_{t-1}(x)=v\\right]}\\\\ &{>\\frac{\\alpha_{v}}{\\mathbb{E}\\left[h(x)^{2}|f_{t-1}(x)=v\\right]\\cdot P(f_{t-1}(x)=v)}}\\\\ &{\\ge\\frac{\\alpha_{v}}{B\\cdot P(f_{t-1}(x)=v)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Then, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\left[(f_{t-1}(x)-y)^{2}-(\\tilde{f}_{t}(x)-y)^{2}\\right]}\\\\ &{=\\displaystyle\\sum_{v\\in[1/m]}P(f_{t-1}(x)=v)\\mathbb{E}\\left[(f_{t-1}(x)-y)^{2}-(\\tilde{f}_{t}(x)-y)^{2}|f_{t-1}(x)=v\\right]}\\\\ &{=\\displaystyle\\sum_{v\\in[1/m]}P(f_{t-1}(x)=v)\\mathbb{E}\\left[(f_{t-1}(x)-y)^{2}-(h_{v}^{t}(x,y)-y)^{2}|f_{t-1}(x)=v\\right]}\\\\ &{>\\frac{\\alpha}{B},}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "which contradicts the condition in Equation 177. ", "page_idx": 31}, {"type": "text", "text": "The following proposition is a direct corollary from Globus-Harris et al. [13]\u2019s Theorem 4.3. ", "page_idx": 31}, {"type": "text", "text": "Proposition F.26. For any distribution $D$ supported on $\\mathcal X\\times\\mathcal X$ and $\\Phi\\in\\sigma(X)$ , take the grouping function class $\\mathcal{H}\\,\\subset\\,\\mathcal{H}_{1,\\Phi}^{\\prime}\\;:=\\;\\{h(\\Phi(x))\\,\\in\\,\\bar{C}(\\mathcal{X}\\times\\mathcal{Y})\\}$ and the predictor class $\\mathcal{F}\\,=\\,\\breve{\\mathbb{R}}^{\\chi}$ . For any $0\\,<\\,\\alpha\\,<\\,1,B\\,>\\,0$ and an initial predictor $f_{0}:\\mathcal{X}\\to[0,1]$ with $\\begin{array}{r}{|R a n g e(f_{0})|\\;\\geq\\;\\frac{2B}{\\alpha}}\\end{array}$ , then MC-Pseudolabel $(D,{\\mathcal{H}},{\\mathcal{F}})$ halts after at most $\\begin{array}{r}{T\\leq{\\frac{2B}{\\alpha}}}\\end{array}$ steps and outputs a model $f_{T-1}^{\\prime}(x)$ that is $\\alpha$ -approximately $\\ell_{2}$ multicalibrated $w.r t\\textit{D}$ and $\\bar{\\mathcal{H}}_{B}=\\{h\\in\\mathcal{H}:\\operatorname*{sup}h(x,y)^{2}\\leq B\\}$ . ", "page_idx": 31}, {"type": "text", "text": "Theorem F.27 (Restatement of Theorem 5.2). Consider $X\\in\\mathbb{R}^{d}$ with $X=(\\Phi,\\Psi)^{T}$ . Assume that $(\\Phi,\\Psi,Y)$ follows a multivariate normal distribution $\\mathscr{N}_{d+1}(\\mu,\\Sigma)$ where the random variables are in general position such that $\\Sigma$ is positive definite. We partition $\\Sigma$ into blocks: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\Sigma=\\left(^{\\sum_{\\Phi\\Phi}}_{\\Sigma\\Phi}\\quad\\Sigma_{\\Phi\\Psi}\\quad\\Sigma_{\\Phi y}\\right).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "For any distribution $D$ supported on $\\mathcal X\\times\\mathcal X$ , take the the predictor class $\\mathcal{F}=\\mathbb{R}^{\\mathcal{X}}$ and the grouping function class $\\mathcal{H}$ as a subset of $\\mathcal{H}_{2,\\Phi}$ , which is defined in Equation $I24$ : ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathcal{H}=\\{h:h\\in\\mathcal{H}_{2,\\Phi}\\mathrm{~}a n d\\,h(x,y)=c_{x}^{T}x+c_{y}y+c_{b},c_{x}\\in\\mathbb{R}^{d},c_{y},c_{b}\\in\\mathbb{R}\\}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "For an initial predictor $f^{(0)}(x)=\\mathbb{E}[Y|x]$ , run MC-Pseudolabe $\\left\\langle D,{\\mathcal{H}},{\\mathcal{F}}\\right\\rangle$ without rounding, then there exists some constant $C_{x}$ depending on $x$ and some constant $M(\\Sigma)$ depending on $\\Sigma$ , such that $\\left|f^{(t)}(x)-\\mathbb{E}[Y|\\Phi(x)]\\right|\\le C_{x}M(\\Sigma)^{t}$ , where ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{M(\\Sigma)=(\\Sigma_{y y}-\\Sigma_{y\\Phi}\\Sigma_{\\Phi\\Phi}^{-1}\\Sigma_{\\Phi y})^{-1}(\\Sigma_{y\\Psi}-\\Sigma_{y\\Phi}\\Sigma_{\\Phi\\Phi}^{-1}\\Sigma_{\\Phi\\Psi})}\\\\ &{\\qquad\\qquad\\big(\\Sigma_{\\Psi\\Psi}-\\Sigma_{\\Psi\\Phi}\\Sigma_{\\Phi\\Phi}^{-1}\\Sigma_{\\Phi\\Psi}\\big)^{-1}\\big(\\Sigma_{\\Psi y}-\\Sigma_{\\Psi\\Phi}\\Sigma_{\\Phi\\Phi}^{-1}\\Sigma_{\\Phi y}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We have $0\\le M(\\Sigma)<1$ . ", "page_idx": 31}, {"type": "text", "text": "Remark F.28. $f^{(t)}$ and $\\mathbb{E}[Y|\\Phi(x)]$ are both linear. Thus, the convergence of the functions $f^{(t)}$ is equivalent as the convergence of their coefficients. The theorem essentially states that the coefficients of $f^{(t)}$ converges to those of $\\mathbb{E}[Y|\\Phi(x)]$ at a rate of $\\mathcal{O}(M(\\Sigma)^{t})$ . ", "page_idx": 31}, {"type": "text", "text": "Remark F.29. $\\mathbb{E}[Y|\\Phi(x)]$ is multicalibrated with respect to $\\mathcal{H}$ . Furthermore, any calibrated predictor on $\\Phi$ , denoted by $g(\\Phi)$ , is multicalibrated with respect to $\\mathcal{H}$ . This is because: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[h(X,Y)(Y-g(\\Phi))|g(\\Phi)]=\\mathbb{E}\\left[\\mathbb{E}\\left[h(X,Y)(Y-g(\\Phi))|\\Phi,Y\\right]|g(\\Phi)\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\mathbb{E}\\left[C_{h}(Y-g(\\Phi))|g(\\Phi)\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\quad=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "However, $\\mathbb{E}[Y|\\Phi]$ is the most accurate predictor among all multicalibrated $g(\\Phi)$ ", "page_idx": 31}, {"type": "text", "text": "Remark F.30. The convergence rate $M(\\Sigma)$ does not depend on the dimension d of covariates. When $Y\\perp\\Psi\\mid\\Phi$ implying that $\\Phi$ is sufficient for prediction, following from $\\mathbb{E}[Y|\\Phi,\\Psi]=\\mathbb{E}[Y|\\Phi]$ : ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\Sigma_{\\Psi y}=\\Sigma_{\\Psi\\Phi}\\Sigma_{\\Phi\\Phi}^{-1}\\Sigma_{\\Phi y}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "It follows that $M(\\Sigma)=0$ and the algorithm will converge in one step. ", "page_idx": 32}, {"type": "text", "text": "On the other hand, when $Y$ and $\\Psi$ are linearly dependent given $\\Phi$ such that $\\Sigma$ is singular, which violates positive definiteness, following from the proof below: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Sigma_{y y}-\\Sigma_{y\\Phi}\\Sigma_{\\Phi\\Phi}^{-1}\\Sigma_{\\Phi y}=\\big(\\Sigma_{y\\Psi}-\\Sigma_{y\\Phi}\\Sigma_{\\Phi\\Phi}^{-1}\\Sigma_{\\Phi\\Psi}\\big)\\big(\\Sigma_{\\Psi\\Psi}-\\Sigma_{\\Psi\\Phi}\\Sigma_{\\Phi\\Phi}^{-1}\\Sigma_{\\Phi\\Psi}\\big)^{-1}\\big(\\Sigma_{\\Psi y}-\\Sigma_{\\Psi\\Phi}\\Sigma_{\\Phi\\Phi}^{-1}\\Sigma_{\\Phi y}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "It follows that $M(\\Sigma)=1$ and the algorithm can\u2019t converge. ", "page_idx": 32}, {"type": "text", "text": "So the convergence rate depends on the singularity of the problem. Since the algorithm converges to a predictor that does not depend on $\\Psi$ , stronger the \"spurious\" correlation between $Y$ and $\\Psi$ given $\\Phi$ in the distribution $D$ , the algorithm takes longer to converge. ", "page_idx": 32}, {"type": "text", "text": "Proof. Without loss of generality, assume $\\mu=0$ . ", "page_idx": 32}, {"type": "text", "text": "Let $c=(c_{x},c_{y},c_{b})^{T}=(c_{\\Phi},c_{\\Psi},c_{y},c_{b})^{T}$ . Denote dimensions of $\\Phi,\\Psi$ by $d_{\\Phi},d_{\\Psi}$ . ", "page_idx": 32}, {"type": "text", "text": "Let $\\mathbb{E}[Y|\\Phi,\\Psi]=(\\alpha_{\\Phi}^{\\star})^{T}\\Phi+(\\alpha_{\\Psi}^{\\star})^{T}\\Psi$ and $\\mathbb{E}[\\Psi|\\Phi,Y]=(\\beta_{\\Phi}^{\\star})^{T}\\Phi+(\\beta_{y}^{\\star})^{T}Y$ with $\\alpha_{\\Phi}^{\\star}\\in\\mathbb{R}^{d_{\\Phi}}$ , $\\alpha_{\\Psi}^{\\star}\\in\\mathbb{R}^{d_{\\Psi}},\\beta_{\\Phi}^{\\star}\\in\\mathbb{R}^{d_{\\Phi}\\times d_{\\Psi}},\\beta_{y}^{\\star}\\in\\mathbb{R}^{1\\times d_{\\Psi}}$ . ", "page_idx": 32}, {"type": "text", "text": "We have: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\alpha_{\\Phi}^{\\star}\\right)=\\left(\\Sigma_{\\Phi\\Phi}\\quad\\Sigma_{\\Phi\\Psi}\\right)^{-1}\\left(\\Sigma_{\\Phi y}\\right).}\\\\ &{\\left(\\alpha_{\\Psi}^{\\star}\\right)=\\left(\\Sigma_{\\Psi\\Phi}\\quad\\Sigma_{\\Psi\\Psi}\\right)^{-1}\\left(\\Sigma_{\\Psi y}\\right).}\\\\ &{\\left(\\beta_{\\Phi}^{\\star}\\right)=\\left(\\Sigma_{\\Phi\\Phi}\\quad\\Sigma_{\\Phi y}\\right)^{-1}\\left(\\Sigma_{\\Phi\\Psi}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "According to Theorem F.15, $\\mathbb{E}[h(X,Y)|\\Phi,Y]$ is a constant for different values of $\\Phi,Y$ . ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[h(X,Y)|\\Phi,Y]=c_{\\Phi}^{T}\\Phi+c_{y}^{T}Y+c_{\\Psi}^{T}\\mathbb{E}[\\Psi|\\Phi,Y]+c_{b}}\\\\ &{\\qquad\\qquad\\qquad=c_{\\Phi}^{T}\\Phi+c_{y}^{T}Y+c_{\\Psi}^{T}\\left(\\Sigma_{\\Psi\\Phi}\\quad\\Sigma_{\\Psi y}\\right)\\left(\\Sigma_{\\Phi\\Phi}\\quad\\Sigma_{\\Phi y}\\right)^{-1}\\left({\\Phi}\\right)+c_{b}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "This implies: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left(\\Sigma_{\\Phi\\Phi}\\quad\\Sigma_{\\Phi y}\\right)^{-1}\\left(\\Sigma_{\\Phi\\Psi}\\right)c_{\\Psi}+\\left(\\mathbf{\\Sigma}_{c_{y}}^{c_{\\Phi}}\\right)=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Rearranging to: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\left(\\stackrel{\\sum_{\\Phi\\Phi}}{\\Sigma_{y\\Phi}}\\right.\\quad\\Sigma_{\\Phi\\Psi}\\quad\\Sigma_{\\Phi y}\\Bigg)\\,c=0.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Let $f^{(t)}\\,=\\,(\\alpha_{\\Phi}^{(t)})^{T}\\Phi+(\\alpha_{\\Psi}^{(t)})^{T}\\Psi$ and $\\tilde{f}^{(t)}\\,=\\,(\\tilde{\\alpha}_{\\Phi}^{(t)})^{T}\\Phi+(\\tilde{\\alpha}_{\\Psi}^{(t)})^{T}\\Psi\\,+(\\tilde{\\alpha}_{y}^{(t)})^{T}Y$ . We claim that $\\tilde{\\alpha}_{\\Psi}^{(t)}=0$ . Otherwise, conside $\\begin{array}{r}{\\cdot\\tilde{f}^{(t)^{\\prime}}=(\\tilde{\\alpha}_{\\Phi}^{(t)})^{T}\\Phi+(\\tilde{\\alpha}_{\\Psi}^{(t)})^{T}\\mathbb{E}[\\Psi|\\Phi,Y]+(\\tilde{\\alpha}_{y}^{(t)})^{T}Y.}\\end{array}$ ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\tilde{f}^{(t)}-\\tilde{f}^{(t)^{\\prime}}|\\Psi,Y]=\\mathbb{E}[(\\tilde{\\alpha}_{\\Psi}^{(t)})^{T}\\Psi|\\Phi,Y]-(\\tilde{\\alpha}_{\\Psi}^{(t)})^{T}\\mathbb{E}[\\Psi|\\Phi,Y]=0.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Thus, $\\tilde{f}^{(t)}-\\tilde{f}^{(t)^{\\prime}}\\in\\mathcal{H}$ . ", "page_idx": 32}, {"type": "text", "text": "On the other hand, ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}[(Y-\\tilde{f}^{(t)})^{2}]-\\mathbb{E}[(Y-\\tilde{f}^{(t)^{\\prime}})^{2}]}\\\\ &{=\\mathbb{E}\\left[(\\tilde{\\alpha}_{\\Psi}^{(t)})^{T}\\left[\\mathbb{E}[\\Psi\\Psi^{T}|\\Phi,Y]-\\mathbb{E}[\\Psi|\\Phi,Y]\\mathbb{E}[\\Psi^{T}|\\Phi,Y]\\right]\\tilde{\\alpha}_{\\Psi}^{(t)}\\right]}\\\\ &{>0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "The inequality follows from the fact that $\\mathbb{E}[\\Psi\\Psi^{T}|\\Phi,Y]-\\mathbb{E}[\\Psi|\\Phi,Y]\\mathbb{E}[\\Psi^{T}|\\Phi,Y]$ is the covariance matrix of $\\Psi|\\Phi,Y$ , which is positive definite because $\\Sigma$ is positive definite. The inequality contradicts with the definition of ${\\tilde{f}}^{(t)}$ . Thus, $\\tilde{f}^{(t)}=(\\tilde{\\alpha}_{\\Phi}^{(t)})^{T}\\Phi+(\\tilde{\\alpha}_{y}^{(t)})^{T}Y$ . ", "page_idx": 33}, {"type": "text", "text": "Define a matrix $C\\in\\mathbb{R}^{(d+1)\\times d_{t}}$ whose columns support the solution space of Equation 195. Then $H:=C^{T}(S,T,Y)^{T}\\in\\mathbb{R}^{d_{t}}$ is a random vector. According to the definition of $\\tilde{f}^{(t)}$ , ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{f}^{(t+1)}=\\mathbb{E}[Y|f^{(t)},H]}\\\\ &{\\quad\\quad=k^{(t)}f^{(t)}+(c_{\\Phi}^{(t)})^{T}\\Phi+(c_{\\Psi}^{(t)})^{T}\\Psi+(c_{y}^{(t)})^{T}Y}\\\\ &{\\quad\\quad=k^{(t)}(\\alpha_{\\Phi}^{(t)})^{T}\\Phi+k^{(t)}(\\alpha_{\\Psi}^{(t)})^{T}\\Psi+(c_{\\Phi}^{(t)})^{T}\\Phi+(c_{\\Psi}^{(t)})^{T}\\Psi+(c_{y}^{(t)})^{T}Y.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "In the above equation, $k^{(t)}\\in\\mathbb{R}$ . ", "page_idx": 33}, {"type": "text", "text": "Since \u03b1\u02dc\u03a8 $\\tilde{\\alpha}_{\\Psi}^{(t+1)}=k^{(t)}\\alpha_{\\Psi}^{(t)}+c_{\\Psi}^{(t)}=0$ c(\u03a8t) = 0, we have c(\u03a8t) $c_{\\Psi}^{(t)}=-k^{(t)}\\alpha_{\\Psi}^{(t)}$ . Substituting into Equation 194: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left(c_{\\Phi}^{(t)}\\right)=k^{(t)}\\left(\\Sigma_{\\Phi\\Phi}\\quad\\Sigma_{\\Phi y}\\right)^{-1}\\left(\\Sigma_{\\Phi\\Psi}\\right)\\alpha_{\\Psi}^{(t)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Substituting into Equation 202: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\tilde{\\alpha}_{\\Phi}^{(t+1)}\\right)=k^{(t)}\\left(\\tilde{\\alpha}_{\\Phi}^{(t)}\\right)+\\left(c_{\\Phi}^{(t)}\\right)}\\\\ &{\\left(\\tilde{\\alpha}_{y}^{(t+1)}\\right)=k^{(t)}\\left(10\\right)+\\left(c_{y}^{(t)}\\right)}\\\\ &{\\qquad\\qquad=k^{(t)}\\left(I_{d_{\\Phi}}\\quad\\beta_{\\Phi}^{\\star}\\right)\\left(\\alpha_{\\Phi}^{(t)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "equation", "text": "$f^{(t+1)}=\\mathbb{E}[\\tilde{f}^{(t+1)}|\\Phi,\\Psi]=(\\tilde{\\alpha}_{\\Phi}^{(t+1)})^{T}\\Phi+(\\tilde{\\alpha}_{y}^{(t+1)})^{T}\\mathbb{E}[Y|\\Phi,\\Psi].$ ", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "This is equivalent as: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\alpha_{\\Phi}^{(t+1)}\\right)=\\left(\\tilde{\\alpha}_{\\Phi}^{(t+1)}\\right)+\\left(\\Sigma_{\\Phi\\Phi}^{_{\\Phi\\Phi}}\\quad\\Sigma_{\\Phi\\Psi}\\right)^{-1}\\left(\\Sigma_{\\Psi y}\\right)\\tilde{\\alpha}_{y}^{(t+1)}}\\\\ &{\\quad\\quad\\quad\\quad=\\left(I_{d_{\\Phi}}\\quad\\alpha_{\\Phi}^{\\star}\\right)\\left(\\tilde{\\alpha}_{\\Phi}^{(t+1)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Combining the two equations above, we have: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\alpha_{\\Phi}^{(t+1)}\\right)=k^{(t)}\\left(\\!\\!\\begin{array}{c c}{I_{d_{\\Phi}}}&{\\alpha_{\\Phi}^{\\star}}\\\\ {0}&{\\alpha_{\\Psi}^{\\star}}\\end{array}\\!\\!\\right)\\left(\\!\\!\\begin{array}{c c}{I_{d_{\\Phi}}}&{\\beta_{\\Phi}^{\\star}}\\\\ {0}&{\\beta_{y}^{\\star}}\\end{array}\\!\\!\\right)\\left(\\!\\!\\alpha_{\\Phi}^{(t)}\\!\\!\\right)}\\\\ &{\\qquad\\qquad=k^{(t)}\\left(\\!\\!\\begin{array}{c c}{I_{d_{\\Phi}}}&{\\beta_{\\Phi}^{\\star}+\\alpha_{\\Phi}^{\\star}\\beta_{y}^{\\star}}\\\\ {0}&{\\alpha_{\\Psi}^{\\star}\\beta_{y}^{\\star}}\\end{array}\\!\\!\\right)\\left(\\!\\!\\alpha_{\\Phi}^{(t)}\\!\\!\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Thus, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left(\\alpha_{\\Phi}^{(t)}\\right)=K^{(t)}\\left(\\begin{array}{c c}{I_{d_{\\Phi}}}&{\\beta_{\\Phi}^{\\star}+\\alpha_{\\Phi}^{\\star}\\beta_{y}^{\\star}}\\\\ {0}&{\\alpha_{\\Psi}^{\\star}\\beta_{y}^{\\star}}\\end{array}\\right)^{t}\\left(\\alpha_{\\Phi}^{(0)}\\right)}\\\\ {=K^{(t)}\\left(\\begin{array}{c c}{I_{d_{\\Phi}}}&{\\beta_{\\Phi}^{\\star}+\\alpha_{\\Phi}^{\\star}\\beta_{y}^{\\star}}\\\\ {0}&{\\alpha_{\\Psi}^{\\star}\\beta_{y}^{\\star}}\\end{array}\\right)^{t}\\left(\\alpha_{\\Phi}^{\\star}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "In the above equation, $\\begin{array}{r}{K^{(t)}=\\prod_{0\\leq u<t}k^{(u)}}\\end{array}$ ", "page_idx": 33}, {"type": "text", "text": "Define $\\hat{f}^{(t)}=(\\hat{\\alpha}_{\\Phi}^{(t)})^{T}\\Phi+(\\hat{\\alpha}_{\\Psi}^{(t)})^{T}\\Psi$ , where ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left(\\hat{\\alpha}_{\\Phi}^{(t)}\\right)=\\left(\\begin{array}{c c}{I_{d_{\\Phi}}}&{\\beta_{\\Phi}^{\\star}+\\alpha_{\\Phi}^{\\star}\\beta_{y}^{\\star}}\\\\ {0}&{\\alpha_{\\Psi}^{\\star}\\beta_{y}^{\\star}}\\end{array}\\right)^{t}\\left(\\alpha_{\\Psi}^{\\star}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "In the following we show $\\hat{\\alpha}_{\\Psi}^{(t)}\\rightarrow0$ . Since $\\hat{\\alpha}_{\\Psi}^{(t)}=(\\alpha_{\\Psi}^{\\star}\\beta_{y}^{\\star})^{t}\\alpha_{\\Psi}^{\\star}$ , and $\\alpha_{\\Psi}^{\\star}\\beta_{y}^{\\star}\\in\\mathbb{R}^{d_{\\Psi}\\times d_{\\Psi}}$ has exactly one nonzero eigenvalue $\\beta_{y}^{\\star}\\alpha_{\\Psi}^{\\star}$ , we just have to show $|\\beta_{y}^{\\star}\\alpha_{\\Psi}^{\\star}|<1$ . ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\beta_{y}^{\\star}\\alpha_{\\Psi}^{\\star}=(\\Sigma_{y y}-\\Sigma_{y\\Phi}\\Sigma_{\\Phi\\Phi}^{-1}\\Sigma_{\\Phi y})^{-1}}\\\\ &{\\qquad\\qquad\\ (\\Sigma_{y\\Psi}-\\Sigma_{y\\Phi}\\Sigma_{\\Phi\\Phi}^{-1}\\Sigma_{\\Phi\\Psi})(\\Sigma_{\\Psi\\Psi}-\\Sigma_{\\Psi\\Phi}\\Sigma_{\\Phi\\Phi}^{-1}\\Sigma_{\\Phi\\Psi})^{-1}(\\Sigma_{\\Psi y}-\\Sigma_{\\Psi\\Phi}\\Sigma_{\\Phi\\Phi}^{-1}\\Sigma_{\\Phi y}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Since, $\\left(\\Sigma_{y y}\\,-\\,\\Sigma_{y\\Phi}\\Sigma_{\\Phi\\Phi}^{-1}\\Sigma_{\\Phi y}\\right)$ and $\\left(\\Sigma_{\\Psi\\Psi}\\,-\\,\\Sigma_{\\Psi\\Phi}\\Sigma_{\\Phi\\Phi}^{-1}\\Sigma_{\\Phi\\Psi}\\right)$ are both Schur complements of $\\Sigma$ \u2019s principal submatrix, they are still positive definite. ", "page_idx": 34}, {"type": "text", "text": "Thus, ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Sigma_{y y}-\\Sigma_{y\\Phi}\\Sigma_{\\Phi\\Phi}^{-1}\\Sigma_{\\Phi y}>0.}\\\\ &{(\\Sigma_{y\\Psi}-\\Sigma_{y\\Phi}\\Sigma_{\\Phi\\Phi}^{-1})(\\Sigma_{\\Psi\\Psi}-\\Sigma_{\\Psi\\Phi}\\Sigma_{\\Phi\\Phi}^{-1}\\Sigma_{\\Phi\\Psi})^{-1}(\\Sigma_{\\Psi y}-\\Sigma_{\\Psi\\Phi}\\Sigma_{\\Phi\\Phi}^{-1}\\Sigma_{\\Phi y})\\ge0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "It follows that $\\beta_{y}^{\\star}\\alpha_{\\Psi}^{\\star}\\geq0$ . So we just have to show $\\beta_{y}^{\\star}\\alpha_{\\Psi}^{\\star}<1$ ", "page_idx": 34}, {"type": "text", "text": "Since $\\operatorname*{det}(\\Sigma)>0$ , by applying row addition on $\\Sigma$ , we have: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\operatorname*{det}\\left(\\begin{array}{c c c}{\\Sigma_{\\Phi\\Phi}}&{\\Sigma_{\\Phi\\Psi}}&{\\Sigma_{\\Phi y}}\\\\ {0}&{\\Sigma_{\\Psi\\Psi}-\\Sigma_{\\Psi\\Phi}\\Sigma_{\\Phi\\Phi}^{-1}\\Sigma_{\\Phi\\Psi}}&{\\Sigma_{\\Psi y}-\\Sigma_{\\Psi\\Phi}\\Sigma_{\\Phi\\Phi}^{-1}\\Sigma_{\\Phi y}}\\\\ {0}&{\\Sigma_{y\\Psi}-\\Sigma_{y\\Phi}\\Sigma_{\\Phi\\Phi}^{-1}\\Sigma_{\\Phi\\Psi}}&{\\Sigma_{y y}-\\Sigma_{y\\Phi}\\Sigma_{\\Phi\\Phi}^{-1}\\Sigma_{\\Phi y}}\\end{array}\\right)>0.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "It follows that: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\Sigma_{y y}-\\Sigma_{y\\Phi}\\Sigma_{\\Phi\\Phi}^{-1}\\Sigma_{\\Phi y})}\\\\ &{\\quad-\\ (\\Sigma_{y\\Psi}-\\Sigma_{y\\Phi}\\Sigma_{\\Phi\\Phi}^{-1}\\Sigma_{\\Phi\\Psi})(\\Sigma_{\\Psi\\Psi}-\\Sigma_{\\Psi\\Phi}\\Sigma_{\\Phi\\Phi}^{-1}\\Sigma_{\\Phi\\Psi})^{-1}(\\Sigma_{\\Psi y}-\\Sigma_{\\Psi\\Phi}\\Sigma_{\\Phi\\Phi}^{-1}\\Sigma_{\\Phi y})>0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Rearranging to: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\beta_{y}^{\\star}\\alpha_{\\Psi}^{\\star}=(\\Sigma_{y y}-\\Sigma_{y\\Phi}\\Sigma_{\\Phi\\Phi}^{-1}\\Sigma_{\\Phi y})^{-1}}\\\\ &{\\qquad\\qquad\\Big(\\Sigma_{y\\Psi}-\\Sigma_{y\\Phi}\\Sigma_{\\Phi\\Phi}^{-1}\\Sigma_{\\Phi\\Psi}\\Big)\\big(\\Sigma_{\\Psi\\Psi}-\\Sigma_{\\Psi\\Phi}\\Sigma_{\\Phi\\Phi}^{-1}\\Sigma_{\\Phi\\Psi}\\big)^{-1}\\big(\\Sigma_{\\Psi y}-\\Sigma_{\\Psi\\Phi}\\Sigma_{\\Phi\\Phi}^{-1}\\Sigma_{\\Phi y}\\big)}\\\\ &{\\qquad<1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Thus, $0\\leq\\beta_{y}^{\\star}\\alpha_{\\Psi}^{\\star}<1$ ", "page_idx": 34}, {"type": "text", "text": "In the following, we show $\\hat{\\alpha}_{\\Phi}^{(t)}\\rightarrow\\Sigma_{\\Phi\\Phi}^{-1}\\Sigma_{\\Phi y}$ . By Equation 212 and $|\\beta_{y}^{\\star}\\alpha_{\\Psi}^{\\star}|<1$ , we have: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\alpha}_{\\Phi}^{(t)}=\\alpha_{\\Phi}^{\\star}+(\\beta_{\\Phi}^{\\star}+\\alpha_{\\Phi}^{\\star}\\beta_{y}^{\\star})\\displaystyle\\sum_{0\\le u<t}(\\alpha_{\\Psi}^{\\star}\\beta_{y}^{\\star})^{u}\\alpha_{\\Psi}^{\\star}}\\\\ &{\\quad\\xrightarrow{t\\rightarrow\\infty}\\alpha_{\\Phi}^{\\star}+(\\beta_{\\Phi}^{\\star}+\\alpha_{\\Phi}^{\\star}\\beta_{y}^{\\star})(I_{d\\Psi}-\\alpha_{\\Psi}^{\\star}\\beta_{y}^{\\star})^{-1}\\alpha_{\\Psi}^{\\star}}\\\\ &{=\\alpha_{\\Phi}^{\\star}+(\\beta_{\\Phi}^{\\star}+\\alpha_{\\Phi}^{\\star}\\beta_{y}^{\\star})(1-\\beta_{y}^{\\star}\\alpha_{\\Psi}^{\\star})^{-1}\\alpha_{\\Psi}^{\\star}}\\\\ &{=\\frac{\\alpha_{\\Phi}^{\\star}+\\beta_{\\Phi}^{\\star}\\alpha_{\\Psi}^{\\star}}{1-\\beta_{y}^{\\star}\\alpha_{\\Psi}^{\\star}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Equation 225 follows from the fact that $(I_{d_{\\Psi}}-\\alpha_{\\Psi}^{\\star}\\beta_{y}^{\\star})\\alpha_{\\Psi}^{\\star}=(1-\\beta_{y}^{\\star}\\alpha_{\\Psi}^{\\star})\\alpha_{\\Psi}^{\\star}.$ ", "page_idx": 34}, {"type": "text", "text": "Define $\\gamma_{\\Phi}^{\\star}=\\Sigma_{\\Phi\\Phi}^{-1}\\Sigma_{\\Phi y}$ such that $\\mathbb{E}[Y|\\Phi]=(\\gamma_{\\Phi}^{\\star})^{T}\\Phi$ . We have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\quad\\mathbb{E}\\left[\\mathbb{E}\\left[\\mathbb{E}\\left[\\Psi\\middle|\\Phi,\\Psi\\right]\\vert\\Phi,Y\\right]\\vert\\Phi\\right]=\\mathbb{E}[Y\\vert\\Phi],}\\\\ &{\\Leftrightarrow\\mathbb{E}\\left[\\mathbb{E}\\left[(\\alpha_{\\Phi}^{\\star})^{T}\\Phi+(\\alpha_{\\Psi}^{\\star})^{T}\\Psi\\middle|\\Phi,Y\\right]\\vert\\Phi\\right]=(\\gamma_{\\Phi}^{\\star})^{T}\\Phi.}\\\\ &{\\Leftrightarrow\\mathbb{E}\\left[(\\alpha_{\\Phi}^{\\star})^{T}\\Phi+(\\alpha_{\\Psi}^{\\star})^{T}(\\beta_{\\Phi}^{\\star})^{T}\\Phi+(\\alpha_{\\Psi}^{\\star})^{T}(\\beta_{y}^{\\star})^{T}Y\\middle|\\Phi\\right]=(\\gamma_{\\Phi}^{\\star})^{T}\\Phi.}\\\\ &{\\Leftrightarrow(\\alpha_{\\Phi}^{\\star})^{T}\\Phi+(\\alpha_{\\Psi}^{\\star})^{T}(\\beta_{\\Phi}^{\\star})^{T}\\Phi+(\\alpha_{\\Psi}^{\\star})^{T}(\\beta_{y}^{\\star})^{T}(\\gamma_{\\Phi}^{\\star})^{T}\\Phi=(\\gamma_{\\Phi}^{\\star})^{T}\\Phi.}\\\\ &{\\Leftrightarrow\\alpha_{\\Phi}^{\\star}+\\beta_{\\Phi}^{\\star}\\alpha_{\\Psi}^{\\star}+\\beta_{y}^{\\star}\\alpha_{\\Psi}^{\\star}\\gamma_{\\Phi}^{\\star}=\\gamma_{\\Phi}^{\\star}.}\\\\ &{\\Leftrightarrow\\frac{\\alpha_{\\Phi}^{\\star}}{1-\\beta_{\\Phi}^{\\star}\\alpha_{\\Psi}^{\\star}}=\\gamma_{\\Phi}^{\\star}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "$\\hat{\\alpha}_{\\Phi}^{(t)}\\rightarrow\\Sigma_{\\Phi\\Phi}^{-1}\\Sigma_{\\Phi y}$ . Subsequently, $\\begin{array}{r}{\\hat{f}^{(t)}\\rightarrow(\\Sigma_{\\Phi\\Phi}^{-1}\\Sigma_{\\Phi y})^{T}\\Phi=\\mathbb{E}[Y|\\Phi]}\\end{array}$ . The convergence ratio is $M(\\Sigma)\\doteq\\beta_{y}^{\\star}\\alpha_{\\Psi}^{\\star}$ ", "page_idx": 34}, {"type": "text", "text": "We have: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left(\\tilde{\\alpha}_{\\Phi}^{(t+1)}\\right)=K^{(t+1)}\\left(\\mathbf{0},\\quad\\beta_{\\Phi}^{\\star}\\right)\\left(\\hat{\\alpha}_{\\Phi}^{(t)}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left(\\begin{array}{c c}{I_{d_{\\Phi}}}&{\\beta_{\\Phi}^{\\star}}\\\\ {0}&{\\beta_{y}^{\\star}}\\end{array}\\right)\\left(\\hat{\\alpha}_{\\Phi}^{(t)}\\right)\\xrightarrow{t\\rightarrow\\infty}\\left(\\begin{array}{c c}{I_{d_{\\Phi}}}&{\\beta_{\\Phi}^{\\star}}\\\\ {0}&{\\beta_{y}^{\\star}}\\end{array}\\right)\\left(\\Sigma_{\\Phi\\Phi}^{-1}\\Sigma_{\\Phi y}\\right)}\\\\ &{=\\left(\\Sigma_{\\Phi\\Phi}^{-1}\\Sigma_{\\Phi y}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Define $\\hat{\\tilde{f}}^{(t)}=(\\hat{\\tilde{\\alpha}}_{\\Phi}^{(t)})^{T}\\Phi+(\\hat{\\tilde{\\alpha}}_{y}^{(t)})^{T}Y$ , where ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\left(\\!\\!\\begin{array}{c c}{{\\hat{\\tilde{\\alpha}}_{\\Phi}^{(t+1)}}}\\\\ {{\\hat{\\tilde{\\alpha}}_{y}^{(t+1)}}}\\end{array}\\!\\!\\right)=\\left(\\!\\!\\begin{array}{c c}{{I_{d_{\\Phi}}}}&{{\\beta_{\\Phi}^{\\star}}}\\\\ {{0}}&{{\\beta_{y}^{\\star}\\right)\\left(\\!\\!\\begin{array}{c c}{{\\hat{\\alpha}_{\\Phi}^{(t)}}}\\\\ {{\\hat{\\alpha}_{\\Psi}^{(t)}}}\\end{array}\\!\\!\\right).}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Thus, $\\hat{\\tilde{f}}^{(t)}\\rightarrow\\mathbb{E}[Y|\\Phi]$ . Since $\\tilde{f}^{(t)}=K^{(t)}\\hat{\\tilde{f}}^{(t)}$ , we have $\\tilde{f}^{(t)}=\\mathbb{E}[Y|\\hat{\\tilde{f}}^{(t)}]\\rightarrow\\mathbb{E}[Y|\\Phi]$ . ", "page_idx": 35}, {"type": "text", "text": "Subsequently, $f^{(t)}=\\mathbb{E}[\\tilde{f}^{(t)}|\\Phi,\\Psi]\\rightarrow\\mathbb{E}[Y|\\Phi]$ . ", "page_idx": 35}, {"type": "text", "text": "G Limitations ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Both our theory and algorithm focuses on the bounded regression setting. The definition of extended multicalibration does not depend on the risk function. However, the analysis of the maximal grouping function class as a linear space assumes a continuous probability distribution of observations, implying a continuous target domain. The convergence of MC-Pseudolabel is also established in a regression setting. All experiments are performed on regression tasks. As most algorithms for out-of-distribution generalization are set up with classification problems, we fill the gap for regression and leave an extension to general risk functions for future work. ", "page_idx": 35}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: The abstract and introduction accurately outline our research questions, and faithfully reflect the paper\u2019s contributions and scope. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 36}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Justification: We discuss limitations in section G and analyze computational complexity of our algorithm in section D. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 36}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: Full assumptions, formal statements of theories and proofs can be found in section F. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 37}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: Experiment settings and implemention of methods are described in section 6.1. Further training details can be found in section E.3. Codes are available in supplementary materials. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 37}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: All the datasets in this paper are public with citations (see section 6.1). Code is provided in additional supplemental materials. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 38}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: Experiment settings are described in section 6.1. Further training details can be found in section E.3. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 38}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: Standard errors are reported for all results. The factors of variability include random drawing of hyperparameters and seeds, and cross validation on dataset splits. See section 6.1 for details. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 38}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 39}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 39}, {"type": "text", "text": "Answer:[Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: Refer to section E.4. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 39}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: The research conducted in this paper conforms with the NeurIPS Code of Ethics. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 39}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: There are no negative social impacts for research conducted in this paper. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 39}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 40}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: The paper poses no safeguards risks. Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 40}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: All datasets in this paper are cited. Codes are credited with original licenses provided. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 40}, {"type": "text", "text": "", "page_idx": 41}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 41}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 41}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 41}, {"type": "text", "text": "", "page_idx": 42}]