[{"figure_path": "Dn68qdfTry/figures/figures_0_1.jpg", "caption": "Figure 1: The output of the considered GNNs eventually become constant as the graph sizes increase.", "description": "This figure shows how the output of graph neural networks (GNNs) changes as the size of the input graph increases.  Three different graph sizes (n=3, n=10, n=30) are shown, each with multiple graphs of that size.  The GNN processes each graph and produces a 2D output, represented as a point on a graph with 'Class 1' and 'Class 2' axes. As the graph size increases, the output points converge to a single point, indicating that the GNN's output becomes independent of the input graph structure for sufficiently large graphs. This demonstrates the phenomenon of \"almost sure convergence\" to a constant distribution. ", "section": "1 Introduction"}, {"figure_path": "Dn68qdfTry/figures/figures_4_1.jpg", "caption": "Figure 1: The output of the considered GNNs eventually become constant as the graph sizes increase.", "description": "This figure shows how the output of Graph Neural Networks (GNNs) changes as the size of the input graph increases.  The GNNs are applied to graphs of increasing size (n=3, n=10, n=30) drawn from a random graph model. The figure demonstrates that the GNN's output converges to a constant value, independent of the input graph structure, as the graph size increases. This convergence implies limitations on the expressiveness of these GNN architectures.", "section": "1 Introduction"}, {"figure_path": "Dn68qdfTry/figures/figures_8_1.jpg", "caption": "Figure 3: Each plot shows the five mean class probabilities (in different colours) with standard deviations of a single model initialization over ER(n,p(n) = 0.1), ER(n,p(n) = logn), and ER(n,p(n) = 50m), as we draw graphs of increasing size.", "description": "This figure shows the convergence of class probabilities for three different Erd\u0151s-R\u00e9nyi random graph models (dense, logarithmic growth, and sparse).  Five mean class probabilities are plotted for each model across various graph sizes, along with standard deviations to illustrate the convergence speed. The different colors represent the different class probabilities.  It highlights differences in convergence time, standard deviation, and final converged values between the dense, logarithmic growth, and sparse graph models.", "section": "6.1 Empirical results on random graph models"}, {"figure_path": "Dn68qdfTry/figures/figures_9_1.jpg", "caption": "Figure 4: Each plot depicts the standard deviation of Euclidean distances between class probabilities and their respective means across various samples of each graph size for GPS+RW.", "description": "This figure shows the standard deviation of the Euclidean distances between the class probabilities and their means across various samples of each graph size for the GPS+RW architecture.  It illustrates how the standard deviation changes as the graph size increases for different graph models (ER(n, p(n) = 0.1), SBM, BA(n, m = 5)). The plots demonstrate the convergence of the standard deviation towards zero as graph sizes increase, supporting the paper's central claim of asymptotic convergence.", "section": "6.1 Empirical results on random graph models"}, {"figure_path": "Dn68qdfTry/figures/figures_9_2.jpg", "caption": "Figure 5: Standard deviation of distances between class probabilities and their means across TIGER-Alaska graph sizes for MeanGNN.", "description": "This figure shows the standard deviation of the Euclidean distances between the class probabilities and their respective means.  The data is from the TIGER-Alaska dataset and uses the MeanGNN architecture.  It empirically demonstrates the convergence of class probabilities on a real-world graph, although at a slower rate than observed for random graphs. The graph shows how the standard deviation decreases as the graph size increases, indicating a convergence towards a constant distribution.", "section": "6.2 Empirical results on large real-world graphs"}, {"figure_path": "Dn68qdfTry/figures/figures_35_1.jpg", "caption": "Figure 3: Each plot shows the five mean class probabilities (in different colours) with standard deviations of a single model initialization over ER(n,p(n) = 0.1), ER(n,p(n) = logn), and ER(n,p(n) = 50m), as we draw graphs of increasing size.", "description": "This figure displays the results of three different experiments on the convergence of class probabilities for three variations of the Erd\u0151s-R\u00e9nyi random graph model. Each plot shows five lines representing the probabilities of the five classes over a range of graph sizes. The lines represent the average probabilities across 100 samples for each graph size and the shaded area around each line shows the standard deviation. The three plots correspond to different density regimes for the Erd\u0151s-R\u00e9nyi model. The plots show that as the graph size increases, the class probabilities converge to a constant value in all three cases, thus empirically validating the convergence phenomenon.", "section": "6.1 Empirical results on random graph models"}, {"figure_path": "Dn68qdfTry/figures/figures_36_1.jpg", "caption": "Figure 3: Each plot shows the five mean class probabilities (in different colours) with standard deviations of a single model initialization over ER(n,p(n) = 0.1), ER(n,p(n) = logn), and ER(n,p(n) = 50m), as we draw graphs of increasing size.", "description": "This figure presents the results of an experiment showing the convergence of class probabilities over different graph distributions.  The plots display the mean class probabilities (averaged over 100 samples per graph size) for five different classes, along with their standard deviations. Three Erdos-Renyi graph models are considered: dense (p=0.1), logarithmic growth (p=logn), and sparse (p=50/n).  The convergence to a constant distribution is clearly observable for all distributions, with varying convergence speeds and standard deviations.", "section": "6.1 Empirical results on random graph models"}, {"figure_path": "Dn68qdfTry/figures/figures_36_2.jpg", "caption": "Figure 3: Each plot shows the five mean class probabilities (in different colours) with standard deviations of a single model initialization over ER(n,p(n) = 0.1), ER(n,p(n) = logn), and ER(n,p(n) = 50m), as we draw graphs of increasing size.", "description": "This figure visualizes the convergence of class probabilities for three different Erdos-Renyi random graph models (dense, logarithmic, and sparse) across five model initializations. Each line represents a class probability, and the shaded area shows the standard deviation. The figure demonstrates that the class probabilities converge to a constant value for all models, supporting the authors' claim of almost sure convergence.", "section": "6.1 Empirical results on random graph models"}, {"figure_path": "Dn68qdfTry/figures/figures_37_1.jpg", "caption": "Figure 3: Each plot shows the five mean class probabilities (in different colours) with standard deviations of a single model initialization over ER(n,p(n) = 0.1), ER(n,p(n) = logn), and ER(n,p(n) = 50m), as we draw graphs of increasing size.", "description": "This figure shows the convergence of class probabilities for three different Erd\u0151s-R\u00e9nyi graph models (dense, logarithmic growth, and sparse) across three different GNN architectures (MeanGNN, GAT, and GPS+RW). Each line represents a different class probability, and the shaded area around each line represents the standard deviation. The figure demonstrates that, despite differences in convergence speed and standard deviation, all models converge to a constant class probability distribution for the dense and logarithmic growth models, while the sparse model shows a different convergence pattern.", "section": "6.1 Empirical results on random graph models"}, {"figure_path": "Dn68qdfTry/figures/figures_37_2.jpg", "caption": "Figure 3: Each plot shows the five mean class probabilities (in different colours) with standard deviations of a single model initialization over ER(n,p(n) = 0.1), ER(n,p(n) = logn), and ER(n,p(n) = 50m), as we draw graphs of increasing size.", "description": "This figure displays the convergence of class probabilities for three different Erd\u0151s-R\u00e9nyi random graph models: dense (p=0.1), logarithmic growth, and sparse (p=50/n). Each plot shows five class probabilities, with error bars representing standard deviations across 100 samples for each graph size. The convergence demonstrates that, as the graph size increases, the GNN's predictions tend towards a constant distribution. This finding holds for the MeanGNN, GAT, and GPS+RW architectures, illustrating the robustness of the phenomenon.", "section": "6.1 Empirical results on random graph models"}, {"figure_path": "Dn68qdfTry/figures/figures_38_1.jpg", "caption": "Figure 11: A three-layer GCN with hidden dimension 128 is trained on the ENZYMES dataset with one class removed so that there are five output classes. This model is then run on graphs drawn from ER(n, p(n) = 0.1) for increasing sizes n, and the mean output probabilities are recorded, along with standard deviation", "description": "This figure shows the class probabilities of a three-layer Graph Convolutional Network (GCN) trained on the ENZYMES dataset, when tested on Erd\u0151s-R\u00e9nyi random graphs with increasing sizes and a fixed edge probability. The results show the mean class probabilities along with standard deviations, illustrating the convergence of the model's output to a constant distribution.", "section": "6.2 Empirical results on large real-world graphs"}]