[{"type": "text", "text": "Leveraging Drift to Improve Sample Complexity of Variance Exploding Diffusion Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ruofeng Yang John Hopcroft Center for Computer Science Shanghai Jiao Tong University wanshuiyin@sjtu.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Zhijie Wang John Hopcroft Center for Computer Science Shanghai Jiao Tong University violetevergarden@sjtu.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Bo Jiang John Hopcroft Center for Computer Science Shanghai Jiao Tong University bjiang@sjtu.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Shuai Li\\* John Hopcroft Center for Computer Science Shanghai Jiao Tong University shuaili8@sjtu.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Variance exploding (VE) based diffusion models, an important class of diffusion models, have shown state-of-the-art (SOTA) performance. However, only a few theoretical works analyze VE-based models, and those works suffer from a worse forward convergence rate $1/\\mathrm{poly}(T)$ than the $\\exp\\left(-T\\right)$ of variance preserving (VP) based models, where $T$ is the forward diffusion time and the rate measures the distance between forward marginal distribution $q_{T}$ and pure Gaussian noise. The slow rate is due to the Brownian Motion without a drift term. In this work, we design a new drifted VESDE forward process, which allows a faster $\\exp\\left(-T\\right)$ forward convergence rate. With this process, we achieve the first efficient polynomial sample complexity for a series of VE-based models with reverse SDE under the manifold hypothesis. Furthermore, unlike previous works, we allow the diffusion coefficient to be unbounded instead of a constant, which is closer to the SOTA models. Besides the reverse SDE, the other common reverse process is the probability flow ODE (PFODE) process, which is deterministic and enjoys faster sample speed. To deepen the understanding of VE-based models, we consider a more general setting considering reverse SDE and PFODE simultaneously, propose a unified tangent-based analysis framework, and prove the first quantitative convergence guarantee for SOTA VE-based models with reverse PFODE. We also show that the drifted VESDE can balance different error terms and improve generated samples without training through synthetic and real-world experiments. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recently, diffusion modeling has shown impressive performance in different areas [Ho et al., 2022, Rombach et al., 2022, Esser et al., 2024, Li et al., 2024]. Diffusion models consist of two processes: the forward and reverse process. The forward process gradually converts data $q_{0}$ to Gaussian noise, which can be described by an intermediate marginal distribution sequence $\\{q_{t}\\}_{t\\in[0,T]}$ .The reverse process sequentially predicts noise and removes it from data to generate samples. ", "page_idx": 0}, {"type": "text", "text": "There are two common forward processes: (1) Variance preserving (VP) SDE and (2) variance exploding (VE) SDE. The VPSDE corresponds to an Ornstein-Uhlenbeck process, and the stationary distribution is $\\mathcal{N}(0,\\mathbf{I})$ . The VESDE has an exploding variance in the forward process. In earlier times, VP-based models [Ho et al., 2020, Lu et al., 2022] provide an important boost for developing diffusion models. Recently, VE-based models have shown the ability to generate data distribution supported on low-dimensional manifolds [Song and Ermon, 2019, 2020]. Since image and text datasets typically exhibit a low-dimensional manifold nature [Pope et al., 2021, Tang and Yang, 2024], VE-based models have achieved great performance in image generation, one-step generation, and reinforcement learning [Teng et al., 2023, Song et al., 2023, Ding and Jin, 2023]. Furthermore, Karras et al. [2022] unify VP and VESDE and prove that the ODE solution trajectory of a specific VESDE is linear and directly towards the data manifold, which makes the denoise process easy. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "After determining a forward SDE, diffusion models reverse it and generate samples by running the corresponding reverse process. Since the reverse drift term contains the gradient of forward logarithmicdensity $\\nabla\\log q_{t}$ (a.k.a. score function), we estimate it by using the score matching technique [Vincent, 2011]. After that, diffusion models discretize the continuous reverse process and run this discrete process starting from pure Gaussian. There are two widely used reverse processes: reverse SDE [Ho et al., 2020] and probability fow ODE (PFODE) [Song et al., 2020a]. The reverse SDE usually generates higher quality samples [Kim et al., 2022]. The reverse PFODE always has a faster generation speed and is useful in other aspects such as calculating likelihoods [Song et al., 2020b] or obtaining one-step generation models [Song et al., 2023]. Hence, these processes are both critical, and providing the guarantee for VE-based models with these processes is necessary. ", "page_idx": 1}, {"type": "text", "text": "Recently, many works analyze the convergence guarantee of the VP-based diffusion models under the reverse SDE setting and prove that the VP-based models can sample from the target data distribution with polynomial complexity [Chen et al., 2023c,a,b, Lee et al., 2023, Benton et al., 2023]. As the first step of this work, we also analyze VE-based models under the reverse SDE setting. Different from the VP-based models, only a few works consider VE-based models and all of them suffer from slow $1/\\mathrm{Poly}(T)$ forward convergence rate [Lee et al., 2022, Gao et al., 2023, Gao and Zhu, 2024], which is worse than $\\exp(-T)$ one for VPSDE. A slow forward convergence rate makes a large distance between $q_{T}$ and pure Gaussian noise, which leads to a large reverse beginning error. From the theoretical perspective, this error introduces hardness to balance three error sources, as shown in Section 5. From the empirical perspective, Lin et al. [2024] show that this error introduces a data information leakage problem, which leads to bad performance. To deal with this problem, De Bortoli et al. [2021] introduce a small drift term to obtain a exp $(-{\\sqrt{T}})$ reverse beginning error. However, they introduce an additional $\\exp\\left(T\\right)$ in the discretization error term. Furthermore, their results do not allow unbounded $\\beta_{t}$ , which is the key point of the optimal solution trajectory and used by the SOTA models [Karras et al., 2022, Song et al., 2023]. Therefore, the following question remains open: ", "page_idx": 1}, {"type": "text", "text": "Is it possible to design a VESDE with a faster forward convergence rate than $1/p o l y(T)$ andachieve the polynomial sample complexity when the diffusion coefficient is unbounded? ", "page_idx": 1}, {"type": "text", "text": "In this work, for the first time, we propose a new drifted VESDE forward process, which enjoys a faster forward convergence rate and allows unbounded coefficients. We first show that the drifted VESDE has similar trends but performs better than the original SOTA VESDE on synthetic data (Section 7). After that, we analyze the sample complexity of drifted VESDE under the realistic manifold hypothesis. The manifold hypothesis means the data $q_{0}$ is supported on a lower dimensional compactset $\\mathcal{M}$ , and much empirical evidence shows that image and text dataset satisfy this hypothesis [Fefferman et al., 2016, Pope et al., 2021, Tang and Yang, 2024]. Furthermore, as shown in Section 2, the manifold hypothesis is more realistic than previous data assumptions since it allows the blow-up phenomenon of the score function at the end of the reverse process, which matches the empirical observation [Kim et al., 2021]. Under the manifold hypothesis, we prove that the drifted VESDE with a suitable larger $\\beta_{t}$ balances the reverse beginning, discretization, and approximated score errors and achieves the first efficient polynomial sample complexity for VE-based models with reverse SDE. ", "page_idx": 1}, {"type": "text", "text": "To better understand VE-based models, we analyze reverse SDE and PFODE simultaneously after obtaining polynomial complexity for reverse SDE. Despite the great performance, a few theoretical works consider reverse PFODE [Chen et al., 2023d,b, Gao and Zhu, 2024], and these works either focus on VPSDE or have strong assumptions. Hence, we propose the tangent-based framework for VE-based models and achieve the first quantitative convergence for the SOTA VE-based models with reverse PFODE. In conclusion, we accomplish the following results under the manifold hypothesis: ", "page_idx": 1}, {"type": "text", "text": "2. When considering the general setting, we propose the tangent-based unified framework and analyze reverse SDE and PFODE simultaneously. Under this framework, we prove the first quantitative guarantee for SOTA VE-based models with reverse PFODE.   \n3. We show that the drifted VESDE balances different error terms and improves generated samples without training via synthetic and real-world experiments. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Before providing current results, we first discuss different assumptions about the data distribution from strong to weak. The strongest assumption is the log-concave distribution. While the log-Sobelev inequality (LSI) assumption is slightly weaker, it does not allow the presence of substantial nonconvexity, which is far away from the multi-modal distribution. Recently, some works assume the scorefunction is $L$ -Lipschitz to allow the multi-modal distribution. However, this assumption can not explain the blow-up phenomenon of score [Kim et al., 2021]. The last assumption is the manifold hypothesis, which is supported by much empirical evidence and allows the blow-up score. ", "page_idx": 2}, {"type": "text", "text": "Analyses for VP-based models. For the reverse SDE, Lee et al. [2022] achieve the first polynomial complexity with strong LSI assumption. Chen et al. [2023c] remove the LSI assumption, assume the Lipschitz score and achieve polynomial complexity. Bortoli [2022] is the first work to focus on the sample complexity of diffusion models under the manifold hypothesis, and it is the most relevant work to our unified framework. However, as discussed in Section 6.1, the original tangent-based lemma can not deal with reverse PFODE even in VPSDE. We carefully control the tangent process to avoid additional $\\exp\\left(T\\right)$ by using the exploding variance property of VESDE. Recently, Chen et al. [2023a] and Benton et al. [2023] also remove the Lipschitz score assumption, and Benton et al. [2023] achieve optimal dependence on $d$ . More recently, Conforti et al. [2023] use bounded Fisher information assumption and replace $d$ with a Fisher information term. ", "page_idx": 2}, {"type": "text", "text": "For the PFODE, Chen et al. [2023d] propose the first quantitative result with exponential dependence on the Lipschitz constant. Chen et al. [2023b] achieve polynomial complexity by introducing a corrector component to inject suitable noise. More recently, Li et al. [2023] remove the additional corrector. However, their results rely heavily on the very specific $\\beta_{t}$ , which goes to O as $T\\rightarrow+\\infty$ Since VE-based models have an unbounded $\\beta_{t}$ , this method is not suitable for our models. ", "page_idx": 2}, {"type": "text", "text": "Analyses for VE-based models. When considering VESDE, most works focus on constant $\\beta_{t}$ and reverse SDE. De Bortoli et al. [2021] provide the first convergence guarantee with exponential dependenceon $T$ . Lee et al. [2022] analyze a constant diffusion coefficient VESDE and achieve polynomial sample complexity under the LSI assumption. When considering the reverse PFODE, Chen et al. [2023d] only consider the discretization error and provide a quantitative convergence guarantee. However, their results introduce additional $\\exp\\left(T\\right)$ compared to ours (Section 6.1). Recently, Gao et al. [2023] and Gao and Zhu [2024] provide the polynomial results for a series of VESDE with reverse SDE and reverse PFODE under the log-concave assumption, respectively. ", "page_idx": 2}, {"type": "text", "text": "3 The Drifted Variance Exploding (VE) SDE ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Diffusion models usually consist of a forward process and a reverse process. The forward process gradually injects noise to convert the data distribution to pure noise. To generate samples, diffusion models reverse the forward process and run the corresponding reverse process. ", "page_idx": 2}, {"type": "text", "text": "This section first recalls two previous forward processes: VPSDE and VESDE. Recently, the VEbased models achieve great performance in application [Karras et al., 2022, Song et al., 2023]. However, unlike the widely analyzed VP-based models [Benton et al., 2023, Chen et al., 2023b], the VE-based models suffer from challenges in obtaining an efficient sample complexity due to the slow forward convergence rate. To address this limitation, we introduce a new drifted VESDE forward process, which has a faster forward convergence rate, balances different error terms and achieves the first efficient polynomial sample complexity (see Section 5). Finally, we introduce how to reverse this new forward process and obtain an implementable algorithm. ", "page_idx": 2}, {"type": "text", "text": "3.1  The VP and VESDE of Diffusion Models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We first introduce the general form of the forward process and then recall two common forward processes, VPSDE and VESDE, adopted in previous works [Ho et al., 2020, Karras et al., 2022]. Let ", "page_idx": 2}, {"type": "text", "text": "$q_{0}$ be the data distribution. Given $\\mathbf{X}_{0}\\in\\mathbb{R}^{d}$ , the forward process is defined by ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{X}_{t}=f(\\mathbf{X}_{t},t)\\;\\mathrm{d}t+g(t)\\;\\mathrm{d}\\mathbf{B}_{t},\\quad\\mathbf{X}_{0}\\sim q_{0}\\,,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $(\\mathbf{B}_{t})_{t\\geq0}$ is the standard Brownian motion in $\\mathbb{R}^{d}$ $f(\\mathbf{X}_{t},t)$ is a drift coefficient and $g(t)$ is a diffusion coefficient. Let $q_{t}$ be the density function of $\\mathbf{X}_{t}$ at time $t$ .With a suitable choice of drift and diffusion terms (e.g. Section 3.1 and 3.2), the forward process gradually converts the data distribution into Gaussian noise. More specifically, the conditional distribution $\\mathbf{X}_{t}|\\mathbf{X}_{0}$ is exactly $\\mathcal{N}(m_{t}\\mathbf{X}_{0},\\sigma_{t}^{2}\\mathbf{I})$ given $\\mathbf{X}_{0}$ , where $m_{t}$ is determined by the drift term and $\\sigma_{t}^{2}$ is determined by the diffusion term. ", "page_idx": 3}, {"type": "text", "text": "The VPSDE forward process. Let $\\{\\beta_{t}\\}_{t\\ge0}$ be a non-decreasing sequence with bounded range $[1/\\bar{\\beta},\\bar{\\beta}]$ . The VPSDE has the following formula: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{d}{\\bf X}_{t}=-\\beta_{t}{\\bf X}_{t}\\;\\mathrm{d}t+\\sqrt{2\\beta_{t}}\\;\\mathrm{d}{\\bf B}_{t}\\,,\\mathrm{where}\\;{\\bf X}_{0}\\sim q_{0}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In this case, $\\begin{array}{r}{m_{t}=\\exp(-\\int_{0}^{t}\\beta_{s}\\mathrm{d}s)}\\end{array}$ and $\\sigma_{t}^{2}=1\\!-m_{t}^{2}$ Note that $m_{T}\\leq\\exp\\left(-T/\\bar{\\beta}\\right)$ ,which indicates a fast forward convergence rate $\\mathrm{TV}(q_{T}|{\\cal N}(0,{\\bf I}))\\leq\\exp\\left(-T/\\bar{\\beta}\\right)$ [Chen et al., 2023c]. ", "page_idx": 3}, {"type": "text", "text": "The VESDE forward process. The VESDE forward process is defined without a drift term: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{d}{\\bf X}_{t}=\\sqrt{\\mathrm{d}\\sigma_{t}^{2}/\\mathrm{d}t}\\;\\mathrm{d}{\\bf B}_{t}\\,,\\mathrm{where}\\;{\\bf X}_{0}\\sim q_{0}\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Twocommonchoicesfor $\\sigma_{t}^{2}$ are $t$ and $t^{2}$ , with the latter achieving SOTA performance [Karras et al., 2022, Teng et al., 2023]. However, VESDE only has a slow polynomial-decay forward convergence rate (Theorem 4.2), which introduces hardness to obtain an efficient sample complexity (see Section 5). This motivates us to design an improved VESDE process with a fast forward convergence rate. ", "page_idx": 3}, {"type": "text", "text": "3.2  The Drifted VESDE Forward Process ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Note that the forward convergence rate of the general process is upper bounded by $m_{T}/\\sigma_{T}^{2}$ (Theorem 4.2). In practical applications [Ho et al., 2020, Karras et al., 2022, Song et al., 2023], the variance of the forward process $\\scriptstyle{\\dot{\\sigma}}_{t}^{2}$ at time $T$ which is determined by the diffusion term, does not exceed $T^{2}$ This indicates the contribution of $\\sigma_{T}^{2}$ to the forward convergence rate is only $1/\\mathrm{Poly}(T)$ . Hence, the exponential-decay forward convergence rate of VPSDE comes from the drift term, which introduces an exponential-decay $m_{t}\\leq\\exp\\left(-T/\\bar{\\beta}\\right)$ . Due to the absence of the drift term in VESDE, the data information, such as expectation $\\mathbb{E}[q_{0}]$ and covariance $\\mathrm{Cov}[q_{0}]$ , does not decay and $m_{t}\\equiv1$ , which is a key to an only polynomial-decay forward convergence rate $m_{T}/\\sigma_{T}^{2}\\leq1/\\dot{\\mathrm{Poly}}(T)$ . With the drift term, the VPSDE gradually removes the data information from $q_{t}$ during the process, which makes $q_{t}$ quickly converge to pure Gaussian noise. Inspired by this elimination effect of the drift term, we propose a drifted VESDE forward process: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{X}_{t}=-\\frac{1}{\\tau}\\beta_{t}\\mathbf{X}_{t}\\,\\mathrm{d}t+\\sqrt{2\\beta_{t}}\\,\\mathrm{d}\\mathbf{B}_{t},\\quad\\mathbf{X}_{0}\\sim q_{0}\\,,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\tau\\in[T,T^{2}]$ is the coefficient used to balance the drift and difusion term 2, and $\\{\\beta_{t}\\}_{t\\ge0}$ is a positive non-decreasing sequence. In this case, ", "page_idx": 3}, {"type": "equation", "text": "$$\nm_{t}=\\exp\\left(-\\int_{0}^{t}\\beta_{s}/\\tau\\;\\mathrm{d}s\\right)\\,\\mathrm{and}\\;\\sigma_{t}^{2}=\\tau\\left(1-m_{t}^{2}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We show that the drifted VESDE is not only an effective representation of the existing VESDE but also extends beyond it (see Section 7 and Appendix A.1). We also prove that this process with suitable $\\beta_{t}$ has a $\\exp\\left(-T\\right)$ forward convergence rate and enjoys an efficient polynomial sample complexity. ", "page_idx": 3}, {"type": "text", "text": "3.3  The Reverse Process of the Drifted VESDE ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To generate samples from Gaussian noise, a diffusion model reverses the forward process. Let $q_{t}^{\\tau}$ be the density funtion of thedrifted VESDE forward processat time $t$ and $\\left(\\mathbf{Y}_{t}\\right)_{t\\in[0,T]}=\\left(\\mathbf{X}_{T-t}\\right)_{t\\in[0,T]}$ As shown in Cattiaux et al. [2021], the reverse process of drifted VESDE has the following form 3: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{d}\\mathbf{Y}_{t}=\\beta_{T-t}\\left\\{\\mathbf{Y}_{t}/\\tau+(1+\\eta^{2})\\nabla\\log q_{T-t}^{\\tau}\\left(\\mathbf{Y}_{t}\\right)\\right\\}\\mathrm{d}t+\\eta\\sqrt{2\\beta_{T-t}}\\;\\mathrm{d}\\mathbf{B}_{t}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The parameter $\\eta\\in[0,1]$ is used to determine the type of reverse processes. There are two common reverse processes in the application: reverse probability flow ODE (PFODE) $\\left.\\eta=0\\right.$ )[Songet al., 2020b,2023] and reverse SDE ( $\\eta=1$ ) [Ho et al.,2020]. ", "page_idx": 4}, {"type": "text", "text": "To generate distribution $q_{0}$ through running the above reverse process, diffusion models need the true score function $\\nabla\\log q_{T-t}^{\\tau}(\\mathbf{Y}_{t})$ and the accurate reverse beginning distribution $q_{T}^{\\tau}$ . However, $\\nabla\\log q_{T-t}^{\\tau}(\\mathbf{Y}_{t})$ and $q_{T}^{\\tau}$ contain the data information and usually can not be exactly calculated. For the score function, diffusion models approximate it using a score network $\\mathbf{s}(T-t,\\cdot)$ by minimizing the score matching objective function [Vincent, 2011]. For the initial distribution of the reverse process, since $q_{T}^{\\tau}$ should be close to a pure Gaussian, we choose $q_{\\infty}^{\\tau}=\\mathcal{N}(0,\\sigma_{T}^{2}\\mathbf{I})$ as an approximation. Then, the continuous reverse process $(\\widehat{\\mathbf{Y}}_{t})_{t\\in[0,T]}$ incorporating $\\mathbf{s}(T-t,\\cdot)$ and $q_{\\infty}^{\\tau}$ , is defined as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{d}\\widehat{\\mathbf{Y}}_{t}=\\beta_{T-t}\\big\\{\\widehat{\\mathbf{Y}}_{t}/\\tau+(1+\\eta^{2})\\mathbf{s}(T-t,\\widehat{\\mathbf{Y}}_{t})\\big\\}\\mathrm{d}t+\\eta\\sqrt{2\\beta_{T-t}}\\;\\mathrm{d}\\mathbf{B}_{t}\\,,\\mathrm{where}\\;\\widehat{\\mathbf{Y}}_{0}\\sim q_{\\infty}^{\\tau}\\,.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Since diffusion models can not run a continuous process due to the nonlinear score function, these models usually discretize the above continuous process and freeze the approximated score at the beginning of each interval. Let $\\{\\gamma_{k}\\}_{k\\in[K]}$ be the stepsize and $\\begin{array}{r}{t_{k+1}=\\sum_{j=0}^{k}\\gamma_{j}}\\end{array}$ . As shown in $K\\mathrm{im}$ et al. [2021], $\\nabla\\log q_{T-t}^{\\tau}(\\mathbf{Y}_{t})$ goes to $+\\infty$ at the end of the reverse process. To mitigate this issue, they use the early stopping technique $t_{K}=T-\\delta$ , and we also employ this technique in this work. With the stepsize, we choose the exponential integrator discretization scheme [Zhang and Chen, 2022] to discretize the above process, which runs the following process: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{d}\\tilde{\\mathbf{Y}}_{t}=\\beta_{T-t}\\big\\{\\tilde{\\mathbf{Y}}_{t}/\\tau+(1+\\eta^{2})\\mathbf{s}(T-t_{k},\\tilde{\\mathbf{Y}}_{t_{k}})\\big\\}\\mathrm{d}t+\\eta\\sqrt{2\\beta_{T-t}}\\;\\mathrm{d}\\mathbf{B}_{t}\\,,\\mathrm{where}\\;t\\in[t_{k},t_{k+1}]\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "As shown in Karras et al. [2022], the choice of $\\beta_{t}$ significantly affects the performance of models, and we need to determine $\\beta_{t}$ before running the reverse process. The state-of-the-art diffusion models adopt $\\beta_{t}=t$ , which increases rapidly and has an unbounded range. However, current theoretical works assume $\\beta_{t}$ to be a constant [Chen et al., 2023c] or confined to a bounded interval $[1/\\bar{\\beta},\\bar{\\beta}]$ [Bortoli, 2022] to match the setting of VPSDE. To align more closely with practical applications of VE-based models, we allow an unbounded $\\beta_{t}$ in this work. Furthermore, we make a detailed assumption on $\\beta_{t}$ when considering different reverse processes. ", "page_idx": 4}, {"type": "text", "text": "Assumption 3.1. Let $\\{\\beta_{t}\\}_{t\\ge0}$ be a positive, non-decreasing sequence. For any $\\tau\\in[T,T^{2}]$ , there exists constants $\\bar{\\beta}$ and $C$ , such that for any $t\\in[0,T]$ : (1) for $\\eta=0$ , then $1/\\bar{\\beta}\\le\\beta_{t}\\le\\operatorname*{max}\\{\\bar{\\beta},t\\}$ and $\\begin{array}{r}{\\int_{0}^{T}\\beta_{t}/\\tau~\\mathrm{d}t\\leq C;(2)}\\end{array}$ for $\\eta=1$ , then $1/\\bar{\\beta}\\le\\beta_{t}\\le\\operatorname*{max}\\{\\bar{\\beta},t^{2}\\}$ ", "page_idx": 4}, {"type": "text", "text": "As shown in Chen et al. [2023b], due to the absence of the stochasticity, the small errors for quickly accumulate and are magnified. Hence, we assume a conservative $\\beta_{t}$ for the reverse PFODE, whose growth rate is at most $t$ , to avoid an additional $\\exp\\left(T\\right)$ in the convergence guarantee (see Section 6.1). We note that this choice of $\\beta_{t}$ is satisfied in practical applications [Song et al., 2020b, Karras et al., 2022]. For the reverse SDE setting, we assume the growth rate of $\\beta_{t}$ can depend on $\\tau$ instead of at most linear. For example, when $\\tau=T^{2}$ , we can choose $\\beta_{t}=t^{2}$ , which has the same order as $\\tau$ .As shown in Theorem 4.2, the drifted VESDE with aggressive $\\beta_{t}=t^{2}$ has an exponential-decay forward convergence rate, which leads to the first efficient polynomial complexity for VE-based models. ", "page_idx": 4}, {"type": "text", "text": "Notations. For $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ and $A\\in\\mathbb{R}^{d\\times d}$ , we denote by $\\left\\|x\\right\\|$ and $\\|A\\|$ the Euclidean norm for vector and the spectral norm for matrixWe dente by $\\bar{\\gamma}_{K}=\\operatorname{argmax}_{k\\in\\{0,\\ldots,K-1\\}}\\gamma_{k}$ the maximum stepsize for $k\\in[0,K-1]$ We denote by $q_{0}P_{T}$ the distribution of $\\mathbf{X}_{T}$ $Q_{t_{K}}^{q_{\\infty}^{\\tau}}$ the distribution of $\\mathbf{Y}_{t_{K}}$ $R_{K}^{q_{\\infty}^{\\tau}}$ the distribution of $\\widetilde{\\mathbf Y}_{t_{K}}$ and $Q_{t_{K}}^{q_{0}P_{T}}$ the distribution which does reverse process starting from $q_{T}^{\\tau}$ (Eq. 5). We denote by $\\mathrm{W_{1}}$ and $\\mathrm{W_{2}}$ the Wasserstein distance of order one and two, respectively. ", "page_idx": 4}, {"type": "text", "text": "4  The Faster Forward Convergence Rate for the Drifted VESDE ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "This section shows that the drifted VESDE has a fast forward convergence rate. Since $q_{T}^{\\tau}$ contains the data information, we first introduce the manifold hypothesis before controlling $\\mathrm{TV}(q_{T}^{\\bar{\\tau}},q_{\\infty}^{\\tau})$ ", "page_idx": 4}, {"type": "text", "text": "Assumption 4.1. $q_{0}$ is supported on a compact set $\\mathcal{M}$ and $0\\in\\mathcal{M}$ ", "page_idx": 4}, {"type": "text", "text": "Wedenote $R$ the diameter of the manifold by $R=\\operatorname*{sup}\\{\\|x-y\\|:x,y\\in{\\mathcal{M}}\\}$ andassume $R>1$ As shown in Section 1, the manifold hypothesis is supported by much empirical evidence [Bengio et al., 2013, Fefferman et al., 2016, Pope et al., 2021] and allows the blow-up phenomenon of the score. Recently, Tang and Yang [2024] show that diffusion models can adapt to the intrinsic manifold structure. With Assumption 4.1, we obtain the forward process guarantee for the drifted VESDE. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Theorem 4.2. Assume Assumption 4.1 and 3.1. Let $q_{\\infty}^{\\tau}\\,=\\,\\mathcal{N}(0,\\sigma_{T}^{2}\\mathbf{I})$ .With $m_{T},\\sigma_{T}$ defined in Equation (4), we have $\\mathrm{TV}(q_{T}^{\\tau},q_{\\infty}^{\\tau})\\,\\leq\\,\\sqrt{m_{T}}\\bar{D}/\\sigma_{T}$ ,where $\\bar{D}\\,=\\,d|c|+\\mathbb{E}[q_{0}]+R$ and $c$ is the eigenvalue of $C o\\nu[q_{0}]$ with the largest absolute value. ", "page_idx": 5}, {"type": "text", "text": "Recall that $\\begin{array}{r}{m_{T}=\\exp(-\\int_{0}^{T}\\beta_{t}/\\tau\\mathrm{~d}t)}\\end{array}$ , the previous VESDE [Song et al., 2020b, Karras et al., 2022, Lee et al., 2022] chooses a conservative $\\beta_{t}$ satisfies $\\begin{array}{r}{\\int_{0}^{T}\\beta_{t}/\\tau~\\mathrm d t\\leq C}\\end{array}$ However, with an aggressive $\\beta_{t}$ \uff0c the drifted VESDE will have a faster convergence rate. To ilustrate the accelerated forward process, we use $\\tau=T^{2}$ as an example and discuss different $\\beta_{t}=t^{\\alpha_{1}},\\alpha_{1}\\in[1,2]$ Due to the definition of $\\sigma_{T}$ $\\sigma_{T}\\approx T$ , and the forward convergence rate mainly depends on $\\sqrt{m_{T}}$ . When $\\alpha_{1}=1$ is conservative, $m_{T}$ is a constant, and the convergence rate is $1/T$ . When $\\alpha_{1}=1+\\ln(2r\\ln(T))/\\ln(T)$ is slightly aggressive, the convergence rate is $1/T^{r+1}$ for $r\\,>\\,0$ . When $\\alpha_{1}\\geq1+\\ln(T-\\ln(T))/\\ln(T)$ is aggressive, the convergence rate is faster than $\\exp(-T)$ . In our analysis, whether $\\beta_{t}$ can be aggressive depends on the reverse process (see Section 6). When choosing an aggressive $\\beta_{t}$ ,the drifted VESDE achieves an improved sample complexity compared with pure VESDE (see Section 5). ", "page_idx": 5}, {"type": "text", "text": "5  The Polynomial Complexity for a Series of VESDE with Reverse SDE ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we first pay attention to the reverse SDE ${\\mathit{\\Delta}}_{\\mathit{\\Delta}}^{\\prime}{\\mathit{\\Delta}}_{\\mathit{\\Delta}}=1$ )to show the power of the drifted VESDE. More specifically, we show that our general drifted VESDE form covers the current models (VP and VESDE). After that, we show that drifted VESDE can go beyond the current models and achieve an improved complexity with an aggressive $\\beta_{t}$ . Since the objective function minimizes the $L_{2}$ distance between the ground truth and the approximated score, we assume that the approximated scoreis $L_{2}$ -accuracy, which is exactly the same with Chen et al. [2023c] and Benton et al. [2023]. ", "page_idx": 5}, {"type": "text", "text": "Assumption 5.1. $\\begin{array}{r}{\\mathbb{E}_{q_{t_{k}}}\\left[\\left|\\left|s_{t_{k}}-\\nabla\\ln q_{t_{k}}^{\\tau}\\right|\\right|^{2}\\right]\\leq\\epsilon_{\\mathrm{score}}^{2}\\;\\mathrm{for}\\;\\forall k\\in[K].}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "With this assumption, we provide a universe convergence guarantee for $\\tau\\in[1,T^{2}]$ and $\\beta_{t}\\in[1,t^{2}]$ and discuss the sample complexity of VP and VE-based models in detail. ", "page_idx": 5}, {"type": "text", "text": "Theorem 5.2. Assume Assumption 3.1, 4.l, 5.1. Let $\\bar{D}$ defined in Theorem 4.2, $\\bar{\\gamma}_{K}\\;\\;=$ $a r g m a x_{k\\in\\{0,...,K-1\\}}\\gamma_{k}$ $\\tau=T^{2}$ and $\\beta_{t}\\in[1,t^{2}]$ . Then, we have that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{TV}\\left(R_{K}^{q_{\\infty}^{\\tau}},q_{\\delta}\\right)\\leq\\frac{\\bar{D}\\sqrt{m_{T}}}{\\sigma_{T}}+\\frac{R^{2}\\sqrt{d}}{\\sigma_{\\delta}^{4}}\\sqrt{\\bar{\\gamma}_{K}\\beta_{T}\\tau T}+\\epsilon_{\\mathrm{score}}\\sqrt{\\beta_{T}T}\\,.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "To guarantee the above convergence guarantee smaller than $\\widetilde{\\cal O}(\\epsilon_{\\mathrm{TV}})$ , each component of the result needs to be smaller than $\\epsilon_{\\mathrm{TV}}$ . As shown in Remark 5.3, it is difficult for pure VESDE to balance the approximated score and the first two error terms to achieve an efficient sample complexity. Hence, we discuss how to balance the reverse beginning and discretization error. More specifically, we require $\\bar{D}\\sqrt{m_{T}}/\\sigma_{T}\\leq\\epsilon_{\\mathrm{TV}}$ and $\\bar{\\gamma}_{K}\\leq\\sigma_{\\delta}^{8}\\epsilon_{\\mathrm{TV}}^{2}\\overset{\\sim}{\\left/\\left(R^{4}\\breve{d}\\beta_{T}\\tau T\\right)\\right.}$ , where the first inequality determines the order of $T$ and the second inequality determines the stepsize ${\\bar{\\gamma}}_{K}$ . After that, with sample complexity $K=T/\\bar{\\gamma}_{K}$ , we have that $\\mathrm{TV}(R_{K}^{q_{\\infty}^{\\tau}},q_{\\delta})\\leq\\widetilde{O}(\\epsilon_{\\mathrm{TV}})$ . The last step is to guarantee $q_{\\delta}$ and $q_{0}$ is close enough $W_{2}^{2}(q_{0},q_{\\delta})\\leq\\epsilon_{W_{2}}^{2}$ , which requires $\\sigma_{\\delta}^{2}\\leq\\epsilon_{W_{2}}^{2}/(d+R\\sqrt{d})$ ", "page_idx": 5}, {"type": "text", "text": "Following the above process, this general convergence guarantee leads to the polynomial sample complexity for VP and VE-based models. When $\\beta_{t}=1$ and $\\tau=1$ , the drifted VESDE becomes VPSDE and achieve the complexity $\\widetilde{\\cal O}(1/(\\epsilon_{W_{2}}^{8}\\epsilon_{\\mathrm{TV}}^{2}))$ , which achieve exactly the same order compared with Chen et al. [2023c]. When $\\beta_{t}=1$ and $\\tau=T$ , our formula is similar but slightly better (Figure 2 and 3) to pure VESDE ( $\\mathit{\\Delta}(\\sigma_{t}^{2}=t)$ and achieves $O(1/\\epsilon_{W_{2}}^{8}\\epsilon_{\\mathrm{TV}}^{8})$ result. For $\\beta_{t}=t$ and $\\tau=T^{2}$ , the general formula is similar to SOTA pure VESDE $(\\sigma_{t}^{2}=t^{2})$ and achieves the first polynomial results $\\bar{O}(1/\\epsilon_{W_{2}}^{8}\\epsilon_{\\mathrm{TV}}^{7})$ for this model under the manifold hypothesis4. ", "page_idx": 5}, {"type": "text", "text": "Although we achieve the first polynomial sample complexity for VE-based models under the manifold hypothesis, it is clear that the results of the VE-based models are significantly worse than the result of VP-based models since the slow $1/\\mathrm{Poly}(T)$ forward convergence guarantee. More specifically, the forward convergence rate of VPSDE is $\\exp\\left(-T\\right)$ , which means $T$ has the order $\\mathrm{log}(\\bar{17}/\\epsilon_{\\mathrm{TV}})$ and can be ignore. When considering the pure VESDE with $\\sigma_{t}^{2}=t^{2}$ , the forward convergence rate is $\\bar{D}/T$ , which indicates $T\\geq\\bar{D}/\\epsilon_{\\mathrm{TV}}^{\\sf-}$ is a polynomial term and can not be ignored. Hence, the results $K^{'}=R^{4}d T^{5}/(\\sigma_{\\delta}^{8}\\epsilon_{\\mathrm{TV}}^{2})$ of pure VESDE $\\;\\sigma_{t}^{2}=t^{2}.$ is heavily infuenced by $T$ When considering pure VESDE with $\\sigma_{t}^{2}=t$ , it suffers from a slower forward convergence guarantee $\\bar{D}/\\sqrt{T}$ , which indicates $T\\,\\ge\\,\\bar{D}^{2}/\\bar{\\epsilon}_{\\mathrm{TV}}^{2}$ and $K\\,=\\,R^{4}d T^{3}/(\\sigma_{\\delta}^{8}\\epsilon_{\\mathrm{TV}}^{2})$ . This is the first work to explain why pure VESDE $(\\sigma_{t}^{2}=t^{2}$ ) performs better than pure VESDE $(\\sigma_{t}^{2}=t^{2}$ ) from a theoretical perspective. Remark 5.3. In this part, we explain the reason why the pure VESDE fails to balance the reverse beginning and the approximated score. We use the pure VESDE with $\\sigma_{t}^{2}=t^{2}$ as an example. Under this setting, the guarantee has the form $1/T+\\sqrt{\\bar{\\gamma}_{K}T^{4}}/\\delta^{4}+\\epsilon_{\\mathrm{score}}\\sqrt{T^{2}}$ , which requires $T\\geq1/\\epsilon_{\\mathrm{TV}}$ ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Then, $\\epsilon_{\\mathrm{score}}\\sqrt{T^{2}}$ is larger than $\\epsilon_{\\mathrm{score}}/\\sqrt{\\epsilon_{\\mathrm{TV}}^{2}}$ . Hence, it is hard to achieve non-asymptotic results. ", "page_idx": 6}, {"type": "text", "text": "Drifted VESDE with an aggressive $\\beta_{t}$ balances different error terms. This part shows that our drifted VESDE with a suitable aggressive $\\beta_{t}$ can balance the above three error terms. More specifically, we show that introducing aggressive $\\beta_{t}$ only slightly affects the discretization error and significantly benefits in balancing reverse beginning and approximated errors. As a result, we obtain an efficient polynomial complexity for a series of VE-based models with unbounded $\\beta_{t}$ ", "page_idx": 6}, {"type": "text", "text": "Corollary 5.4. Following the setting of Theorem 5.2. When considering $\\tau\\,=\\,T^{2},\\beta_{t}\\,=\\,t^{2}$ by choosing $\\begin{array}{r}{\\delta\\,\\le\\,\\frac{\\epsilon_{W_{2}}^{2/3}}{(d+R\\sqrt{d})^{1/3}}}\\end{array}$ (d+RVa)1/a3 T \u2265 2lmn(D/erv), k \u2264 812e2/ In(D/erv)/(Rd) and assuming $\\epsilon_{\\mathrm{score}}\\leq\\widetilde{O}(\\epsilon_{\\mathrm{TV}}),$ $R_{K}^{q_{\\infty}^{\\tau}}$ is $(\\epsilon_{\\mathrm{TV}}+\\epsilon_{\\mathrm{score}})$ close to $q_{\\delta}$ which is $\\epsilon_{W_{2}}$ close to $q_{0}$ with samplecomplexity ", "page_idx": 6}, {"type": "equation", "text": "$$\nK\\leq\\widetilde{\\cal O}\\left(\\frac{d R^{4}(d+R\\sqrt{d})^{4}}{\\epsilon_{W_{2}}^{8}\\epsilon_{\\mathrm{TV}}^{2}}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Defined by $R_{K,R_{0}}^{q_{\\infty}^{\\tau}}$ the output $R_{K}^{q_{\\infty}^{\\tau}}$ projected onto $\\mathrm{B}\\left(0,R_{0}\\right)$ for $R_{0}=\\widetilde{\\Theta}(R)$ Then, we achieve pure W2guaranee W(RRo 9)\u2264w with sample complexiy 0 ( aR(d+R) ", "page_idx": 6}, {"type": "text", "text": "Since our drifted VESDE with $\\tau=T^{2}$ and $\\beta_{t}=t^{2}$ has a fast forward convergence rate $\\exp{(-T)}/T^{2}$ $T$ becomes a logarithmic term and does not influence the discretization term, which is the source of the improved sample complexity. Furthermore, the requirement of $\\epsilon_{\\mathrm{score}}$ has the same order with $\\epsilon_{\\mathrm{TV}}$ which indicates the drifted VESDE balances the reverse beginning and approximated score error. In fact, we only require the forward convergence rate of drifted VESDE is $\\mathrm{exp}\\left(-T\\right)$ , which indicates a series of VE-based models can achieve this sample complexity. We use $\\tau=T^{2}$ as an example. When considering the $\\beta_{t}=t^{\\alpha_{1}}$ , we require $2\\geq\\alpha_{1}\\stackrel{\\cdot}{\\geq}1+\\bar{\\ln(T-\\ln(T))}/\\ln(T)$ to enjoy exp $(-T)$ forward convergence rate and achieve $K\\leq\\tilde{O}\\left(1/(\\epsilon_{W_{2}}^{8}\\epsilon_{\\mathrm{TV}}^{2})\\right)$ sample complexity. For $\\beta_{t}=t$ and $\\tau=T$ we also obtain complexity $K\\leq\\tilde{O}\\left(1/\\left(\\epsilon_{W_{2}}^{8}\\epsilon_{\\mathrm{TV}}^{2}\\right)\\right)$ (Appendix C.1). ", "page_idx": 6}, {"type": "text", "text": "Remark 5.5. Recently, Lee et al. [2022] and Gao et al. [2023] consider the sample complexity of VESDE with reverse SDE under strong assumption. Lee et al. [2022] consider VESDE with $\\sigma_{t}^{2}=t$ and achieve $\\tilde{O}(L^{2}/\\epsilon_{\\mathrm{TV}}^{4})$ result under the LSI assumption. Under the manifold hypothesis, the result .s $\\tilde{O}(1/\\epsilon_{W_{2}}^{8}\\epsilon_{\\mathrm{TV}}^{4})$ , which is worse than Corollary 5.4. Gao et al. [2023] achieve pure $W_{2}$ guarantee $\\widetilde{O}(1/\\epsilon_{W_{2}}^{2.5})$ under the log-concave distribution, which is even stronger than LSI assumption and ignore the influence of $\\delta$ . Hence, they ignore an additional $1/\\mathrm{Poly}(W_{2})$ (Detail in Appendix A.2). ", "page_idx": 6}, {"type": "text", "text": "6  The Tangent-based Analysis Framework ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To deepen the understanding of VE-based models instead of the specific reverse process, we introduce the unified framework for VESDE with reverse SDE and PFODE. Similar to previous PFODE work [Chen et al., 2023d], we assume an accurate score and consider the other errors. ", "page_idx": 6}, {"type": "text", "text": "Theorem 6.1. Assume Assumption 3.1 and 4.1 $\\delta\\leq1/32$ and $\\gamma_{k}\\operatorname*{sup}_{v\\in[T-t_{k+1},T-t_{k}]}\\beta_{v}/\\sigma_{v}^{2}\\leq1/28$ for $\\cdot\\forall k\\in\\{0,...,K-1\\}$ . Let $\\gamma_{K}=\\delta$ Then, for $\\forall\\tau\\in[T,T^{2}]$ ", "page_idx": 6}, {"type": "text", "text": "(1) If $\\eta=1$ (the reverse $S D E$ ), choosing $\\beta_{t}=t^{2}$ \uff0c $W_{1}\\left(R_{K}^{q_{\\infty}^{\\tau}},q_{0}\\right)$ is bounded by $(\\frac{R}{\\tau}+\\sqrt{d})\\sqrt{\\delta}+\\exp\\left(\\frac{R^{2}}{2}(\\frac{\\bar{\\beta}}{\\delta^{3}}+\\frac{1}{\\tau})\\right)\\left(C_{1}(\\tau)T\\kappa_{1}^{2}(\\tau)\\left((\\frac{\\bar{\\beta}}{\\delta^{3}}+\\frac{1}{\\tau})\\bar{\\gamma}_{K}^{1/2}+1\\right)\\bar{\\gamma}_{K}^{1/2}+\\frac{\\bar{D}e^{-T/2}}{\\sqrt{\\tau}}\\right),$ where $\\kappa_{1}(\\tau)=T^{2}(1/\\tau+\\bar{\\beta}/\\delta^{3})$ and $C_{1}(\\tau)$ is linear in $\\tau^{2}$ (2) 1f $\\dot{\\boldsymbol{\\eta}}=0\\,(P\\!F O D\\boldsymbol{E}),$ . choosing a conservative $\\beta_{t}$ (Assumption 3.1), $W_{1}\\left(R_{K}^{q_{\\infty}^{\\tau}},q_{0}\\right)$ is bounded by $(\\frac{R}{\\tau}+\\sqrt{d})\\sqrt{\\delta}+\\exp\\left(\\frac{R^{2}}{2}(\\frac{\\bar{\\beta}}{\\delta^{2}}+\\frac{1}{\\tau})\\right)\\left(C_{2}(\\tau)\\kappa_{2}^{2}(\\tau)T\\left((\\frac{\\bar{\\beta}}{\\delta^{2}}+\\frac{1}{\\tau})\\bar{\\gamma}_{K}^{1/2}+1\\right)\\bar{\\gamma}_{K}^{1/2}+\\frac{\\bar{D}}{\\sqrt{\\tau}}\\right),$ where $\\kappa_{2}(\\tau)=T\\left(1/\\tau+\\bar{\\beta}/\\delta^{2}\\right)$ and $C_{2}(\\tau)$ is linear in $\\tau^{2}$ ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Theorem 6.1 proves the first quantitative guarantee for VE-based models with reverse PFODE using the unified tangent-based framework. Correspondingly, the Girsanov-based method [Chen et al., 2023c,a] can not deal with reverse PFODE since the reverse process diffusion term is not well-defined. Recently, Chen et al. [2023d] employ the Restoration-Degradation framework to analyze VESDE with reverse PFODE, which also has exponential dependence on $R$ and $\\delta$ . Furthermore, their results have exponential dependence on $\\beta_{t}$ $g_{\\mathrm{max}}$ in Chen et al. [2023d]), which corresponds to $\\tau$ .However, our dependence on $\\tau$ appears in the polynomial term. Hence, our framework is a suitable unified framework. Furthermore, we emphasize that our tangent-based unified framework is not a simple extension of Bortoli [2022]. _We carefully control the tangent process according to the variance exploding property of VESDE to avoid $\\exp\\left(T\\right)$ term when considering PFODE (Section 6.1). ", "page_idx": 7}, {"type": "text", "text": "Theorem 6.1 has exponential dependence on $R$ and $\\delta$ : which is introduced by the tangent process. Similar to Bortoli [2022], if we assume the Hessian $\\left\\|\\nabla^{2}\\log q_{t}\\left(x_{t}\\right)\\right\\|\\leq\\underbar{\\Gamma}/\\sigma_{t}^{2}$ , we obtain a better control on the tangent process and replace the exponential dependence on $\\delta$ by a polynomial dependence on $\\delta$ and exponential dependence on $\\Gamma$ when considering reverse PFODE. ", "page_idx": 7}, {"type": "text", "text": "Corollary 6.2. Assume Assumption 3.1, 4.1 and $\\left\\|\\nabla^{2}\\log q_{t}\\left(x_{t}\\right)\\right\\|\\,\\leq\\,\\Gamma/\\sigma_{t}^{2}$ .Let $\\eta\\,=\\,0$ (reverse PFODE), $\\delta\\in(0,1/32),\\tau=T^{2}$ \uff0c $\\beta_{t}=t$ and $\\kappa_{2}(\\tau),C_{2}(\\tau)$ defined in Theorem 6.1, we have $V_{1}\\left(R_{K}^{q_{\\infty}^{\\tau}},q_{0}\\right)\\leq(\\frac{R}{\\tau}+\\sqrt{d})\\sqrt{\\delta}+\\frac{\\bar{\\beta}^{\\frac{\\tau}{2}}}{\\delta^{\\Gamma}}\\exp\\left(\\frac{\\Gamma+2}{2}\\right)\\left(C_{2}(\\tau)\\kappa_{2}^{2}(\\tau)T((\\frac{\\bar{\\beta}}{\\delta^{2}}+\\frac{1}{\\tau})\\bar{\\gamma}_{K}^{1/2}+1)\\bar{\\gamma}_{K}^{1/2}+\\frac{\\bar{D}}{\\sqrt{\\delta}}\\right),$ ", "page_idx": 7}, {"type": "text", "text": "Though the additional assumption is strong, many special cases, such as hypercube $\\mathcal{M}\\;=\\;$ $[-1/\\bar{2},1/2]^{p}$ satisfy this assumption. We emphasize that our analysis also holds for VESDE $(\\sigma_{t}^{2}=t^{2})$ with reverse PFODE, which means our results can explain the SOTA model in Karras et al. [2022]. ", "page_idx": 7}, {"type": "text", "text": "6.1 The Discussion on the Unified Framework ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we introduce the unified tangent-based framework for reverse SDE and PFODE and discuss key steps to achieve the quantitative guarantee for PFODE. Firstly, we decompose the goal $W_{1}\\left(R_{K}^{q_{\\infty}^{\\tau}},q_{0}\\right)$ intothretemns $\\stackrel{\\_}{W_{1}}{\\left(R_{K}^{q_{\\infty}^{\\tau}},\\bar{Q_{t_{K}}^{q_{\\infty}^{\\tau}}}\\right)}+W_{1}\\left(Q_{t_{K}}^{q_{\\infty}^{\\tau}},Q_{t_{K}}^{q_{0}P_{T}}\\right)\\stackrel{\\cdot}{+}W_{1}\\left(Q_{t_{K}}^{q_{0}\\bar{P}_{T}},q_{0}\\right)$ ", "page_idx": 7}, {"type": "text", "text": "These terms correspond to the discretization scheme, reverse beginning distribution, and the early   \nstoppingparameter $\\delta$ . We focus on most difficult discretization term and first recall the stochastic   \nflow of the reverse process for any $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ and $s,t\\in[0,T]$ with $t\\geq s$ $\\mathrm{d}\\mathbf{Y}_{s,t}^{x}=\\beta_{T-t}\\left\\{\\mathbf{Y}_{s,t}^{x}/\\tau+\\left(1+\\eta^{2}\\right)\\nabla\\log q_{T-t}\\left(\\mathbf{Y}_{s,t}^{x}\\right)\\right\\}\\mathrm{d}t+\\eta\\sqrt{2\\beta_{T-t}}\\mathrm{d}\\mathbf{B}_{t}\\,,$ where $\\mathbf{Y}_{s,s}^{x}=x$   \nWith $\\nabla\\mathbf{Y}_{s,s}^{x}=\\mathbf{I}$ , the corresponding tangent process is ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathrm{d}\\nabla\\mathbf{Y}_{s,t}^{x}=\\!\\beta_{T-t}\\nabla\\mathbf{Y}_{s,t}^{x}/\\tau\\mathrm{d}t+\\beta_{T-t}\\left(1+\\eta^{2}\\right)\\nabla^{2}\\log q_{T-t}(\\mathbf{Y}_{s,t}^{x})\\nabla\\mathbf{Y}_{s,t}^{x}\\mathrm{d}t\\,.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The key o thedisereizationerroris to bound tangent proces $\\left\\|\\nabla{\\mathbf Y}_{s,t_{K}}^{x}\\right\\|_{.}$ For thisterm,we consider the reverse SDE and PFODE simultaneously and propose a general version of Bortoli [2022]. ", "page_idx": 7}, {"type": "text", "text": "Lemma 6.3. Assume Assumption 3.1 and 4.1. For $\\forall s\\in[0,t_{K}]$ and $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ ,wehave ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\|\\nabla{\\mathbf Y}_{s,t_{K}}^{x}\\|\\leq\\exp\\left(\\frac{R^{2}}{2\\sigma_{T-t_{K}}^{2}}+\\frac{(1-\\eta^{2})}{2}\\int_{0}^{t_{K}}\\frac{\\beta_{T-u}}{\\tau}\\mathrm{d}u\\right)\\,.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "image", "img_path": "euQ0C4iS7O/tmp/eec22e4a8f6edf27426f97e7d033ff5e713d0d14f8aca7f1a3610ef2af7d274e.jpg", "img_caption": ["Figure 1: Experiment results of Swiss roll with Euler Maruyama Method (Reverse SDE) "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "We emphasize that the general bound for the tangent process is the key to achieving the guarantee for VESDE with the reverse ODE. Recall that in the original lemma for the tangent proceses, since $\\tau$ is independent of $T$ and $\\beta_{t}$ is bounded in a small interval $[1/\\bar{\\beta},\\bar{\\beta}]$ $\\begin{array}{r}{\\int_{0}^{t_{K}}\\beta_{T-u}/\\tau\\mathrm{d}u=\\Theta(T)}\\end{array}$ which means there is an additional $\\exp\\left(T\\right)$ when considering VPSDE with revere PFODE. However, our tangent-based lemma makes use of the variance exploding property of VESDE to guarantee that $\\begin{array}{r}{\\int_{0}^{T}\\bar{\\beta}_{t}/\\tau\\mathrm{d}t\\le C}\\end{array}$ with a conservative $\\beta_{t}=t$ when considering reverse PFODE. When $\\eta=1$ we choose aggressive $\\beta_{t}=t^{2}$ since the choice of $\\beta_{t}$ does not affect the bound of the tangent process. ", "page_idx": 8}, {"type": "text", "text": "For the early stopping term, it corresponds to $\\delta$ and is smaller than $2(R/\\tau+{\\sqrt{d}}){\\sqrt{\\delta}}$ .Sincewecan not use the data processing inequality in Wasserstein distance, the reverse beginning terms consists of the bound of tangent process term and the forward process term: ", "page_idx": 8}, {"type": "equation", "text": "$$\nW_{1}\\left(Q_{t_{K}}^{q_{\\infty}^{\\tau}},Q_{t_{K}}^{q_{0}P_{T}}\\right)\\leq\\frac{\\sqrt{m_{T}}\\bar{D}}{\\sigma_{T}}\\exp\\left(\\frac{R^{2}}{2\\sigma_{T-t_{K}}^{2}}+\\frac{(1-\\eta^{2})}{2}\\int_{0}^{t_{K}}\\frac{\\beta_{T-u}}{\\tau}\\mathrm{d}u\\right)\\,.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "One notable future work is introducing the short regularization technique [Chen et al., 2023b] and suitable corrector to remove the above exponential dependence. ", "page_idx": 8}, {"type": "text", "text": "7 Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we show the power of the drifted VESDE forward process through experiments. Section 7.1 shows that aggressive one achieves good balance in different error terms. After that, we consider the approximated score and show that the conservative one can improve the quality of the generated distribution without training in the synethetic and real-world setting. ", "page_idx": 8}, {"type": "text", "text": "7.1 The Aggressive Drifted VESDE Balances Errors ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we do experiments on 2-D Gaussian to show that the aggressive drifted VESDE balances different errors. Since the ground truth score of the Gaussian can be directly calculated, we use the accurate score function to discuss the balance between the other two error terms clearly. We show how to use approximated score in Section 7.2. ", "page_idx": 8}, {"type": "text", "text": "As shown in Figure 2, the process with aggressive $\\beta_{t}=t^{2}$ achieves the best and second performance in EI and EM discretization, which supports our theoretical result (Corollary 5.4). The third best process is conservative $\\beta_{t}\\,=\\,t$ with the small drift term. The reason is that though it can not achieve a exp $(-T)$ forward guarantee, it also has a constant decay on prior information, which slightly reduces the effect of the reverse beginning error (Section 3.1). The worst process is pure VESDE since it is hard to balance different error sources. Our experimental results also show that EI is better than EM. ", "page_idx": 8}, {"type": "image", "img_path": "euQ0C4iS7O/tmp/e176a0aa6646283a593df4cc87e876d6d44dee67b6445c781bc35d3852703759.jpg", "img_caption": ["Figure 2: Results of 2-D Gaussian "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "euQ0C4iS7O/tmp/6e05b146566b509e626abba029398c65805d122bc8d8e773be519381405c0b08.jpg", "img_caption": ["Figure 3: Experiment results of CelebA dataset "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "7.2 The Conservative Drifted VESDE Benefits from VESDE without Training ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "As shown in Figure 2, the red and orange lines have similar trends. Hence, for conservative drifted VESDE, which satisfies (2) of Assumption 3.1, we can directly use the models trained by pure VESDE to improve the quality of generated distribution. We confirm our intuition by training the model with pure VESDE with $\\sigma_{t}^{2}=t$ and directly use the models to conservative drifted VESDE With $\\beta_{t}=1$ and $\\tau=T$ . From the experimental results (Figure 1), it is clear that pure VESDE has a low density on the Swiss roll except for the center one, which indicates pure VESDE can not deal with large dataset variance, as we discuss in Section 4. For conservative drift VESDE, as we discuss in the above section, it can reduce the influence of the dataset information. Figure 1 (c) supports our augmentation and shows that the density of the generated distribution is more uniform compared to pure VESDE, which means that the drift VESDE can deal with large dataset mean and variance. ", "page_idx": 9}, {"type": "text", "text": "Beyond the synthetic data, we show that our conservative drifted VESDE can improve the generated images of pure VESDE without training on the real-world CelebA256 dataset. From the qualitative perspective, as shown in Figure 3, the images generated by our drifted VESDE have more detail (such as hair and beard details). On the contrary, since pure VESDE can not deal with large variance, the images generated by pure VESDE appear blurry and unrealistic in these details. From the quantitative results, we use aesthetic score [Schuhmann et al., 2022] and Inception Score to measure the quality of generated images. Our drifted VESDE achieves aesthetic score 5.813, and IS 4.174, which is better than the results of baseline pure VESDE (aesthetic score 5.807 and IS 4.082). There are more examples on CelebA256 and more experiments on Swiss roll and 1D-GMM to explore different sampling methods (RK45, PFODE) and different $T$ . We refer to Appendix G for more details. ", "page_idx": 9}, {"type": "text", "text": "8 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we analyze the VE-based models under the manifold hypothesis. Firstly, we propose a new forward drifted VESDE process, which enjoys a faster forward convergence rate. Then, we show that with an aggressive $\\beta_{t}$ , the new process balances different errors and achieve the first efficient polynomial sample complexity for a series of VE-based models with reverse SDE. ", "page_idx": 9}, {"type": "text", "text": "After achieving the above results, we go beyond the reverse SDE and propose the tangent-based unified framework, which considers reverse SDE and PFODE at the same time. Under this framework, we make use of the variance exploding property of VESDE and achieve the first quantitative convergence guarantee for SOTA VE-based models with reverse PFODE. Finally, we show the power of the new drifted forward process through synthetic and real-world experiments. ", "page_idx": 9}, {"type": "text", "text": "Future Work and Limitation. This work proposes the first unified framework for VE-based models with an accurate score. After that, we plan to consider the approximated score error and provide a polynomial complexity for the VE-based models with reverse PFODE under the manifold hypothesis. ", "page_idx": 9}, {"type": "text", "text": "Broader Impact. Our work focuses on the convergence guarantee of the SOTA diffusion models and deepens the understanding of diffusion models. Therefore, this work can be viewed as a fundamental step in improving the quality of diffusion models and the societal impact is similar to general generative models [Mirsky and Lee, 2021]. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The author Bo Jiang is supported by National Natural Science Foundation of China (62072302). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Vladimir M Alekseev. An estimate for the perturbations of the solutions of ordinary differential equations. Vestn. Mosk. Univ. Ser. I. Math. Mekh, 2:28-36, 1961.   \nYoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learming: A review and new perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798-1828, 2013.   \nJoe Benton, Valentin De Bortoli, Arnaud Doucet, and George Deligiannidis. Linear convergence bounds for difusion models via stochastic localization. arXiv preprint arXiv:2308.03686, 2023.   \nValentin De Bortoli. Convergence of denoising diffusion models under the manifold hypothesis. Trans. Mach. Learn. Res., 2022, 2022. URL https: //openreview.net/forum?id=MhK5aXo3gB.   \nPatrick Cattiaux, Giovanni Conforti, Ivan Gentil, and Christian Leonard. Time reversal of diffusion processes under a finite entropy condition. arXiv preprint arXiv:2104.07708, 2021.   \nHongrui Chen, Holden Lee, and Jianfeng Lu. Improved analysis of score-based generative modeling: Userfriendly bounds under minimal smoothnes assumptions. In International Conference on Machine Learning, pages 4735-4763. PMLR, 2023a.   \nSitan Chen, Sinho Chewi, Holden Lee, Yuanzhi Li Jianfeng Lu, and Adil Salim. The probability How ode is provably fast. arXiv preprint arXiv:2305.11798, 2023b.   \nSitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru Zhang. Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023c. URL https: //openreview.net/pdf?id=zyLVMgsZOU_.   \nSitan Chen, Giannis Daras, and Alex Dimakis. Restoration-degradation beyond linear diffusions: A nonasymptotic analysis for ddim-type samplers. In International Conference on Machine Learning, pages 4462-4484. PMLR, 2023d.   \nGiovanni Conforti, Alain Durmus, and Marta Gentiloni Silveri. Score diffusion models without early stopping: finite fisher information is all you need. arXiv preprint arXiv:2308.12240, 2023.   \nValentinDeBortoli,James hrton,Jermy ng, and ad Doucet.Diffusion chdingerbrige with applications to score-based generative modeling. Advances in Neural Information Processing Systems, 34: 17695-17709, 2021.   \nPierre Del Moral and Sumeetpal Sidhu Singh. Backward ito-ventzell and stochastic interpolation formulae. Stochastic Processes and their Applications, 154:197-250, 2022.   \nZihan Ding and Chi Jin. Consistency models as a rich and efficient policy class for reinforcement learning. arXiv preprint arXiv:2309.16984, 2023.   \nPatrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muiller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectifed fow transformers for high-resolution image synthesis. arXiv preprint arXiv:2403.03206, 2024.   \nCharles Fefferman, Sanjoy Mitter, and Hariharan Narayanan. Testing the manifold hypothesis. Journal of the American Mathematical Society, 29(4):983-1049, 2016.   \nXuefeng Gao and Lingjiong Zhu. Convergence analysis for general probability flow odes of diffusion models in wasserstein distances. arXiv preprint arXiv:2401.17958, 2024.   \nXuefeng Gao, Hoang MNguyen, and Lingjiong Zhu Wasserstein convergence guarantees for a general class of score-based generative models. arXiv preprint arXiv:2311.11003, 2023.   \nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:6840-6851, 2020.   \nJonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet. Video diffusion models. arXiv preprint arXiv:2204.03458, 2022.   \nTero Karras, Mika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. Advances in Neural Information Processing Systems, 35:26565-26577, 2022.   \nDongjun Kim, Seungjae Shin, Kyungwoo Song, Wanmo Kang, and Il-Chul Moon. Soft truncation: A universal training technique of score-based diffusion model for high precision score estimation. arXiv preprint arXiv:2106.05527, 2021.   \nDongjun Kim, Yeongmin Kim, Wanmo Kang, and Il-Chul Moon. Refining generative process with discriminator guidance in score-based diffusion models. arXiv preprint arXiv:2211.17091, 2022.   \nChieh-Hsin Lai, Yuhta Takida, Naoki Murata, Toshimitsu Uesaka, Yuki Mitsufuji, and Stefano Ermon. Fpdifusion: Improving score-based diffusion models by enforcing the underlying score fokker-planck equation. In Andreas Krause, Emma Brunskil, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, International Conference on Machine Learning, ICML 2023, 23-29 July2023, Honoluu, Hawai, USA, volume 202 of Proceedings of Machine Learning Research, pages 18365-18398. PMLR, 2023.   \nJean-Frangois Le Gall. Brownian motion, martingales, and stochastic calculus. Springer, 2016.   \nHolden Lee, Jianfeng Lu, and Yixin Tan. Convergence for score-based generative modeling with polynomial complexity. Advances in Neural Information Processing Systems, 35:22870-22882, 2022.   \nHolden LJanfeng u and Yixin Tan. Convergence of core-based generativemdelng for genera data distributions. In International Conference on Algorithmic Learning Theory, pages 946-985. PMLR, 2023.   \nGen Li, Yuting Wei, Yuxin Chen, and Yuejie Chi. Towards faster non-asymptotic convergence for diffusion-based generative models. arXiv preprint arXiv:2306.09251, 2023.   \nHaoran Li, Haolin Shi, Wenli Zhang, Wenjun Wu, Yong Liao, Lin Wang, Lik-hang Lee, and Pengyuan Zhou. Dreamscene: 3d gaussian-based text-to-3d scene generation via formation pattern sampling. arXiv preprint arXiv:2404.03575, 2024.   \nShanchuanin,ingchen iuJiahii and XiaoYangCmmn diffusinnois sheduls and samle st are fawed. InProceedings of the IEEE/CVF Winter Conference onApplications of Computer Vision, pages 5404-5411, 2024.   \nCheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. arXiv preprint arXiv:2206.00927, 2022.   \nYisroel Mirsky and Wenke Lee. The creation and detection of deepfakes: A survey. ACM Computing Surveys (CSUR), 54(1):1-41, 2021.   \nJakiw Pidstrigach. Score-based generative models detect manifolds. arXiv preprint arXiv:2206.01018, 2022.   \nPhillip Pope, Chen Zhu, Ahmed Abdelkader, Micah Goldblum, and Tom Goldstein. The intrinsic dimension of images and its impact on learning. arXiv preprint arXiv:2104.08894, 2021.   \nRobin Rombach, Andreas Blatman, Dominik Lorenz, Patrick Esser, and Bjom Ommer. High-resolution mage synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10684-10695, 2022.   \nChristoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35: 25278-25294, 2022.   \nJiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020a.   \nYang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019.   \nYang Song and Stefano Ermon. Improved techniques for training score-based generative models. Advances in neural information processing systems, 33:12438-12448, 2020.   \nYang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011. 13456, 2020b.   \nYang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawai, USA, volume 202 of Proceedings of Machine Learning Research, pages 32211-32252. PMLR, 2023.   \nRong Tang and Yun Yang. Adaptivity of diffusion models to manifold structures. In International Conference on Artificial Intelligence and Statistics, pages 1648-1656. PMLR, 2024.   \nJiayan Teng, Wendi Zheng, Ming Ding, Wenyi Hong, Jianqiao Wangni, Zhuoyi Yang, and Jie Tang. Relay diffusion: Unifying diffusion process across resolutions for image synthesis. arXiv preprint arXiv:2309.03350, 2023.   \nPascal Vincent. A connection between score matching and denoising autoencoders. Neural computation, 23(7): 1661-1674,2011.   \nQinsheng Zhang and Yongxin Chen. Fast sampling of diffusion models with exponential integrator. arXiv preprint arXiv:2204.13902,2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A  More Discussion on Drifted VESDE and Current Works ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 The Drifted VESDE is Representative Enough. ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this work, we consider $\\tau\\in[T,T^{2}]$ and show that this choice is enough to represent the current VESDE. More specifically, since VESDE with $\\sigma_{t}^{2}\\,=\\,t$ has $q_{T}\\,=\\,\\mathcal{N}(\\bar{\\mathbb{E}}[q_{0}],\\bar{\\mathbf{C}}\\mathbf{ov}[q_{0}]+T\\mathbf{I})$ and drifted VESDE with $\\tau=\\dot{T}^{2}$ and $\\beta_{t}\\equiv1/2$ has $\\begin{array}{r}{q_{T}^{\\tau}=\\mathcal{N}(\\exp{(-\\frac{1}{2T})}\\mathbb{E}[q_{0}]}\\end{array}$ $\\begin{array}{r}{\\mathrm{exp}^{}\\bar{(-\\frac{1}{T})}\\bar{\\mathrm{Cov}}[q_{0}]+(1-}\\end{array}$ $\\exp{\\big(-{\\frac{1}{T}}\\big)}T^{2}\\mathbf{I})$ , these two setting is almost identical when $T\\rightarrow+\\infty$ . The second choice of VESDE $\\sigma_{t}^{2}=t^{2}$ , which achieves the state-of-the-art performance [Karras et al., 2022], is almost identical to $\\tau=T^{2}$ \uff0c $\\beta_{t}=t$ . The simulation experiments also show that VESDE and drifted VESDE with specific $\\beta_{t}$ and $\\tau$ have similar performance (Figure 2). ", "page_idx": 13}, {"type": "text", "text": "A.2The Detailed Calculation of Previous work ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The results of Lee et al. [2022] Lee et al. [2022] consider VESDE ( $\\sigma_{t}^{2}=t$ )with reverse SDE under the LSI assumption with parameter $C_{\\mathrm{LS}}$ . The LSI assumption does not allow the presence of substantial non-convexity and is far away from the multi-modal real-world distribution. Furthermore, they use unrealistic assumption $\\epsilon_{\\mathrm{score}}\\leq1/(C_{\\mathrm{LS}}+T)$ to avoid the effect of the approximated score, which is stronger than Assumption 5.1. Under the above strong assumption, Lee et al. [2022] achieve the polynomial sample complexity $\\tilde{O}(L^{2}d(d|c|+R)^{2}/\\epsilon_{\\mathrm{TV}}^{4})$ . Under the manifold hypothesis, by Lemma E.2, we know that ", "page_idx": 13}, {"type": "equation", "text": "$$\nL=R^{2}d^{2}/\\epsilon_{W_{2}}^{4}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Then, the result is $\\tilde{O}(R^{4}d^{5}(d|c|+R)^{2}/\\epsilon_{W_{2}}^{8}\\epsilon_{\\mathrm{TV}}^{4})$ , which is worse than Corollary 5.4. ", "page_idx": 13}, {"type": "text", "text": "The results of Ga0 et al. [2023].   Gao et al. [2023] analyze a series of VESDE with reverse SDE and achieve $1/\\epsilon_{W_{2}}^{2.5}$ SamplecompelxiyforVESDEwith $\\bar{\\sigma}_{t}^{2}=t^{2}$ inWasdistanwe assume the data distribution is log-concave, which is even stronger than LSI assumption. Furthermore, under this assumption, $\\nabla\\log q_{T-t}(\\cdot)$ do not blow-up at the end of the reverse process, which do not match the empirical phenomenon and ignore the infuence of early stopping parameters. To transfer their results to our the results under the manifold hypothesis, we need to consider the influence of $\\delta$ which would introduce an additional $1/\\mathrm{Poly}(W_{2})$ term. ", "page_idx": 13}, {"type": "text", "text": "B The Proof for the Faster Forward Process ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Lemma B.1. The minimization problem $\\mathrm{min}_{\\bar{m}_{t},V_{t}}\\,\\mathrm{KL}\\,\\big(q_{t}^{\\tau}\\mid\\mathcal{N}(\\bar{m}_{t},V_{t})\\big)$ is minimized by $\\bar{m}_{t}\\,=$ $m_{t}\\mathbb{E}\\left[q_{0}\\right]$ and $V_{t}=m_{t}^{2}\\operatorname{Cov}\\left[q_{0}\\right]+\\sigma_{t}^{2}\\mathbf{I},$ where $m_{t}$ and $\\sigma_{t}$ defined in Equation (4). ", "page_idx": 13}, {"type": "text", "text": "Proof. For simplicity, we denote the mean and covariance of $q_{0}$ by $a$ and $C^{\\prime}$ . We also define the optimize variable ${n_{t}}=\\mathcal{N}\\left({{\\bar{m}}_{t}},{{C}_{t}}\\right)$ . We can directly compute the KL divergence $\\mathrm{KL}(q_{t}|n_{t})$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\mathrm{KL}\\left(q_{t}|n_{t}\\right)=-H\\left(q_{t}\\right)-\\displaystyle\\int\\log\\left(n_{t}(x)\\right)q_{t}(x)\\mathrm{d}x}}\\\\ {{\\displaystyle\\qquad\\qquad=-H\\left(q_{t}\\right)+\\frac{d}{2}\\log(2\\pi)+\\frac{1}{2}\\log\\left(\\operatorname*{det}\\left(V_{t}\\right)\\right)+\\frac{1}{2}\\displaystyle\\int(x-\\bar{m}_{t})^{T}V_{t}^{-1}(x-\\bar{m}_{t})q_{t}(x)\\mathrm{d}x\\,.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "For the last term, we directly compute ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\int\\left(x-\\bar{m}_{t}\\right)^{T}V_{t}^{-1}\\left(x-\\bar{m}_{t}\\right)p_{t}(x)\\mathrm{d}x}\\\\ &{=\\mathbb{E}\\left[\\left(X_{t}-\\bar{m}_{t}\\right)^{T}V_{t}^{-1}\\left(X_{t}-\\bar{m}_{t}\\right)\\right]=\\mathbb{E}\\left[\\left(m_{t}X_{0}+\\sigma_{t}Z-\\bar{m}_{t}\\right)^{T}V_{t}^{-1}\\left(m_{t}X_{0}+\\sigma_{t}Z-\\bar{m}_{t}\\right)\\right]}\\\\ &{=\\mathbb{E}\\left[m_{t}^{2}\\left(X_{0}-a\\right)^{T}V_{t}^{-1}\\left(X_{0}-a\\right)\\right]+\\left(m_{t}a-\\bar{m}_{t}\\right)^{T}V_{t}^{-1}\\left(m_{t}a-\\bar{m}_{t}\\right)+\\sigma_{t}^{2}\\mathbb{E}\\left[Z^{T}V_{t}^{-1}Z\\right]}\\\\ &{=\\!\\!m_{t}^{2}\\operatorname{tr}\\left(C^{\\prime}V_{t}^{-1}\\right)+\\sigma_{t}^{2}\\operatorname{tr}\\left(V_{t}^{-1}\\right)+\\left(m_{t}a-\\bar{m}_{t}\\right)^{T}V_{t}^{-1}\\left(m_{t}a-\\bar{m}_{t}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where the second inequality follows that $X_{t}=m_{t}X_{0}+\\sigma_{t}Z$ It is clear that the optimal solution of $\\bar{m}_{t}$ is $m_{t}a$ . In the next step, we focus on the optimization problem for $V_{t}$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L\\left(V_{t}^{-1}\\right)=\\log\\left(\\operatorname*{det}\\left(V_{t}\\right)\\right)+\\mathrm{tr}\\left(\\left(m_{t}^{2}C^{\\prime}+\\sigma_{t}^{2}\\mathbf{I}\\right)V_{t}^{-1}\\right)}\\\\ &{\\qquad\\qquad=-\\log\\left(\\operatorname*{det}\\left(V_{t}^{-1}\\right)\\right)+\\mathrm{tr}\\left(\\left(m_{t}^{2}C^{\\prime}+\\sigma_{t}^{2}\\mathbf{I}\\right)V_{t}^{-1}\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Since the above optimization is a convex optimization problem, we use the method similar to Pidstrigach [2022], we obtain that the optimal solution of $V_{t}$ is $m_{t}^{2}C^{\\prime}+\\sigma_{t}^{2}\\mathbf{I}$ ", "page_idx": 14}, {"type": "text", "text": "Lemma B.2. Let $\\bar{m}_{t}$ and $V_{t}$ be the optimal mean and covariance operator from Lemma B.1. Then ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{KL}\\left(q_{t}|N(\\bar{m}_{t},V_{t})\\right)\\leq\\frac{1}{2}\\log\\left(\\frac{\\prod_{i=1}^{d}\\left(m_{t}^{2}c_{i}+\\sigma_{t}^{2}\\right)}{(\\sigma_{t}^{2})^{d}}\\right)+\\frac{R^{2}m_{t}}{\\sigma_{t}^{2}}}\\\\ &{\\phantom{A A}\\leq\\frac{d m_{t}^{2}c}{2\\sigma_{t}^{2}}+\\frac{R^{2}m_{t}}{\\sigma_{t}^{2}}+o(\\frac{m_{t}^{2}c}{\\sigma_{t}^{2}})\\,,}\\\\ &{\\mathrm{KL}\\left(N(\\bar{m}_{t},V_{t})|(N(0,\\sigma_{t}^{2})\\right)\\leq\\frac{m_{t}^{2}\\sum_{i=1}^{d}c_{i}}{2\\sigma_{t}^{2}}+\\frac{m_{t}^{2}(\\mathbb{E}[q_{0}])^{2}}{2\\sigma_{t}^{2}}+\\frac{1}{2}\\log\\left(\\frac{(\\sigma_{t}^{2})^{d}}{\\prod_{i=1}^{d}\\left(m_{t}^{2}c_{i}+\\sigma_{t}^{2}\\right)}\\right)}\\\\ &{\\phantom{A A}\\leq\\frac{m_{t}^{2}\\sum_{i=1}^{d}c_{i}}{2\\sigma_{t}^{2}}+\\frac{m_{t}^{2}(\\mathbb{E}[q_{0}])^{2}}{2\\sigma_{t}^{2}}+\\frac{d m_{t}^{2}c}{2\\sigma_{t}^{2}}+o(\\frac{m_{t}^{2}c}{\\sigma_{t}^{2}})\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $c_{i}$ are the eigenvalues of $\\mathrm{Cov}\\,[q_{0}];$ and $c$ is the eigenvalue with the largest absolute value. ", "page_idx": 14}, {"type": "text", "text": "Proof. For $t\\geq0$ , we directly calculate the KL divergence for this term: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\mathrm{KL}\\left(q_{t}\\mid\\mathcal{N}(\\bar{m}_{t},V_{t})\\right)=\\displaystyle-H\\left(q_{t}\\right)+\\frac{1}{2}\\log\\left(\\operatorname*{det}\\left(2\\pi V_{t}\\right)\\right)+\\frac{1}{2}\\operatorname{tr}\\left(\\left(m_{t}^{2}C^{\\prime}+\\sigma_{t}^{2}\\mathbf{I}\\right)V_{t}^{-1}\\right)}}\\\\ {{\\displaystyle~~~~~~~~~~~~~~~~~~~~~~~~~=-H\\left(q_{t}\\right)+\\frac{1}{2}\\log\\left(\\operatorname*{det}\\left(2\\pi V_{t}\\right)\\right)+\\frac{d}{2}}}\\\\ {{\\displaystyle~~~~~~~~~~~~~~~~~~~~~~~~~~~=-H\\left(q_{t}\\right)+\\frac{d}{2}\\log(2\\pi)+\\frac{1}{2}\\log\\left(\\prod_{i=1}^{d}\\left(m_{t}^{2}c_{i}+\\sigma_{t}^{2}\\right)\\right)+\\frac{d}{2}\\,,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $c_{i}$ are the eigenvalues of $\\mathrm{Cov}\\left[q_{0}\\right]$ . Now, we only need to calculate $H(q_{t})$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n-H\\left({{q}_{t}}\\right)={{\\mathbb{E}}_{X_{t}}}\\left[\\log{{q}_{t}}\\left(X_{t}\\right)\\right]={{\\mathbb{E}}_{X_{t}}}\\left[\\log\\left({{\\mathbb{E}}_{X_{0}}}\\left[(2\\pi\\sigma_{t}^{2})^{-d/2}\\exp\\left(-\\frac{1}{2\\sigma_{t}^{2}}\\left\\lVert X_{t}-X_{0}\\right\\rVert^{2}\\right)\\right]\\right)\\right]\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "By Assumption 4.1, it is clear that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\exp\\left(-\\frac{1}{2\\sigma_{t}^{2}}\\left\\|X_{t}-X_{0}\\right\\|^{2}\\right)\\leq\\exp\\left(-\\frac{1}{2\\sigma_{t}^{2}}\\left(\\left\\|X_{t}\\right\\|^{2}+2\\langle X_{t},X_{0}\\rangle\\right)\\right)\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Then, we know that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\left[\\log\\left(\\mathbb{E}_{X_{0}}\\left[(2\\pi\\sigma_{t}^{2})^{-d/2}\\exp\\left(-\\frac{1}{2\\sigma_{t}^{2}}\\left\\|X_{t}-X_{0}\\right\\|^{2}\\right)\\right]\\right)\\right]}\\\\ &{\\leq\\!\\mathbb{E}\\left[\\log\\left((2\\pi\\sigma_{t}^{2})^{-d/2}\\right)-\\frac{1}{2\\sigma_{t}^{2}}\\left(\\left\\|X_{t}\\right\\|^{2}+2\\langle X_{t},X_{0}\\rangle\\right)\\right]}\\\\ &{\\leq-\\,\\frac{d}{2}\\log(2\\pi)-\\frac{1}{2}\\log\\left((\\sigma_{t}^{2})^{d}\\right)-\\frac{1}{2\\sigma_{t}^{2}}\\mathbb{E}\\left[\\left\\|X_{t}\\right\\|^{2}\\right]+\\frac{R^{2}m_{t}}{\\sigma_{t}^{2}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "we also know that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\Vert X_{t}\\Vert^{2}\\right]=m_{t}^{2}\\mathbb{E}\\left[\\Vert X_{0}\\Vert^{2}\\right]+\\sigma_{t}^{2}\\mathbb{E}\\left[\\Vert Z\\Vert^{2}\\right]=\\mathbb{E}\\left[\\Vert X_{0}\\Vert^{2}\\right]+t\\mathbb{E}\\left[\\Vert Z\\Vert^{2}\\right]=\\bar{m}_{0}^{2}+V_{0}+\\sigma_{t}^{2}d\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Finally, put these terms together, we have: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{KL}\\left(q_{t}|\\mathcal{N}(\\bar{m}_{t},V_{t})\\right)\\leq\\frac{1}{2}\\log\\left(\\frac{\\prod_{i=1}^{d}\\left(m_{t}^{2}c_{i}+\\sigma_{t}^{2}\\right)}{(\\sigma_{t}^{2})^{d}}\\right)+\\frac{R^{2}m_{t}}{\\sigma_{t}^{2}}\\,,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $c_{i}$ are the eigenvalues of $\\mathrm{Cov}\\left[q_{0}\\right]$ . Then by choosing the largest absolute value eigenvalue largest absolute value, we can use the Taylor expansion to obtain the first results of this lemma. For the second result of this lemma, we directly compute the KL divergence between $\\mathcal{N}(\\bar{m}_{t},V_{t})$ and $\\mathcal{N}(0,\\sigma_{t}^{2})$ to obtain the final results. ", "page_idx": 14}, {"type": "text", "text": "Theorem 4.2. Assume Assumption 4.1 and 3.1. Let $q_{\\infty}^{\\tau}\\,=\\,\\mathcal{N}(0,\\sigma_{T}^{2}\\mathbf{I})$ .With $m_{T},\\sigma_{T}$ defined in Equation (4), we have $\\mathrm{TV}(q_{T}^{\\tau},q_{\\infty}^{\\tau})\\,\\leq\\,\\sqrt{m_{T}}\\bar{D}/\\sigma_{T}$ ,where $\\bar{D}\\,=\\,d|c|+\\mathbb{E}[q_{0}]+R$ and $c$ is the eigenvalue of $C o\\nu[q_{0}]$ with the largest absolute value. ", "page_idx": 15}, {"type": "text", "text": "Proof. We know that ", "text_level": 1, "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|q_{T}-q_{\\infty}^{\\tau}\\|_{\\mathrm{TV}}}\\\\ &{\\leq\\|q_{T}-N(m_{T}\\mathbb{E}[q_{0}],m_{T}^{2}\\mathbf{Cov}[q_{0}]+\\sigma_{T}^{2}\\mathbf{I})\\|_{\\mathrm{TV}}+\\|{\\cal N}(m_{T}\\mathbb{E}[q_{0}],m_{T}^{2}\\mathbf{Cov}[q_{0}]+\\sigma_{T}^{2}\\mathbf{I})-q_{\\infty}^{\\tau}\\|_{\\mathrm{TV}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By directly using the Pinsker's inequality and Lemma B.2, we complete the proof. ", "page_idx": 15}, {"type": "text", "text": "CThe Proof of the Polynomial Complexity for Reverse SDE ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we prove Corollary 5.4. First, we recall the Girsanov's Theorem [Le Gall, 2016] used in Chen et al. [2023c]: ", "page_idx": 15}, {"type": "text", "text": "Lemma C.1 (Girsanov's theorem). Let $P_{T}$ and $Q_{T}$ be two probability measures on path space $\\mathcal{C}\\left([0,T];\\mathbb{R}^{d}\\right)$ . Suppose that under $P_{T}$ the process $(X_{t})_{t\\in[0,T]}$ follows ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathrm{d}X_{t}=\\tilde{b}_{t}\\;\\mathrm{d}t+\\alpha_{t}\\;\\mathrm{d}\\tilde{B}_{t}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\tilde{B}$ is a $P_{T}$ -Brownian motion, and under $Q_{T}$ the process $(X_{t})_{t\\in[0,T]}$ follows ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathrm{d}X_{t}=b_{t}\\,\\mathrm{d}t+\\alpha_{t}\\,\\mathrm{d}B_{t}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $B$ isa $Q_{T}$ -Brownian motion.We assume that for each $t>0,\\alpha_{t}$ isa $d\\times d$ symmetricpositive definite matrix. Then, provided that Novikov's condition holds, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}_{Q_{T}}\\exp\\left(\\frac{1}{2}\\int_{0}^{T}\\left\\|\\alpha_{t}^{-1}\\left(\\tilde{b}_{t}-b_{t}\\right)\\right\\|^{2}\\,\\mathrm{d}t\\right)<\\infty,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "wehave that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}P_{T}}{\\mathrm{d}Q_{T}}=\\exp\\left(\\int_{0}^{T}\\alpha_{t}^{-1}\\left(\\tilde{b}_{t}-b_{t}\\right)\\mathrm{d}B_{t}-\\frac{1}{2}\\int_{0}^{T}\\left\\|\\alpha_{t}^{-1}\\left(\\tilde{b}_{t}-b_{t}\\right)\\right\\|^{2}\\,\\mathrm{d}t\\right)\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "If the Novikov's condition is satisfied, we apply the Girsanov theorem by choosing $\\mathit{P}_{T}\\;=$ $\\begin{array}{r l r}{R_{K}^{q_{T}^{\\tau}},Q_{T}}&{=}&{Q_{t_{K}}^{q_{T}^{\\tau}},\\widetilde{b}_{t}\\;\\;\\;=\\;\\;\\;\\beta_{T-t}\\left\\{\\frac{1}{\\tau}\\widetilde{\\bf Y}_{t}+2{\\bf s}(\\overline{{T}}-t_{k},\\widetilde{\\bf Y}_{t})\\right\\}}\\end{array}$ (for $\\begin{array}{r l r}{t}&{{}\\in}&{[t_{k},t_{k+1}])}\\end{array}$ $\\begin{array}{r l}{b_{t}}&{{}=}\\end{array}$ $\\begin{array}{r l}{\\beta_{T-t}\\left\\{\\frac{1}{\\tau}\\mathbf{Y}_{t}+\\left(1+\\eta^{2}\\right)\\nabla\\log q_{T-t}\\left(\\dot{\\mathbf{Y}_{t}}\\right)\\right\\}}&{{}}\\end{array}$ and $\\alpha_{t}=\\sqrt{2\\beta_{T-t}}\\mathbf{I}_{d}$ ", "page_idx": 15}, {"type": "text", "text": "Then, similar to Chen et al. [2023c], we have the following lemma. ", "page_idx": 15}, {"type": "text", "text": "Lemma C.2. Assoming that $R_{K}^{q_{T}^{\\tau}}$ and $Q_{t_{K}}^{q_{T}^{\\tau}}$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\mathrm{KL}\\left(Q_{t_{K}}^{q_{T}^{\\tau}}\\|R_{K}^{q_{T}^{\\tau}}\\right)}}\\\\ {{\\displaystyle=\\mathbb{E}_{Q_{t_{K}}^{q_{T}^{\\tau}}}\\ln\\frac{\\mathrm{d}Q_{t_{K}}^{q_{T}^{\\tau}}}{\\mathrm{d}R_{K}^{q_{T}^{\\tau}}}=\\sum_{k=0}^{K-1}\\mathbb{E}_{Q_{t_{K}}^{q_{T}^{\\tau}}}\\int_{t_{k}}^{t_{k+1}}2\\beta_{T-t}\\left\\|\\mathbf{s}\\left(T-t_{k},\\mathbf{Y}_{t_{k}}\\right)-\\nabla\\ln q_{T-t}\\left(\\mathbf{Y}_{t}\\right)\\right\\|^{2}\\,\\mathrm{d}t\\,.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Before using the Girsanov's Theorem, we need to check the Novikov's condition. We use almost the same proof process compare to Chen et al. [2023c]. The key proof of the Novikov's condition is Lemma 19 of Chen et al. [2023c]. Hence, we give a complete proof of Lemma 19 in Chen et al. [2023c] under our drifted VESDE. Before the proof, we first introduce a smooth cuttoff function for truncating the drift terms. ", "page_idx": 15}, {"type": "text", "text": "Lemma C.3 (lemma 17 of Chen et al. [2023c]). For any $\\bar{R}\\,>\\,0$ thereisasmoothfunction $\\phi_{R}:\\mathbb{R}^{d}\\rightarrow[0,1]$ satisfying:1. $\\phi_{\\bar{R}}(x)=1$ for all $\\lVert x\\rVert\\leq{\\bar{R}},$ 2. $\\phi_{\\bar{R}}(x)=0$ for all $\\left\\|x\\right\\|\\geq2{\\bar{R}},$ 3. $\\phi_{\\bar{R}}$ is $O(1/\\bar{R})$ -Lipschitz. ", "page_idx": 15}, {"type": "text", "text": "Note that $\\bar{R}$ is not $R$ in Assumption 4.1 and will goes to $+\\infty$ in the proof of Chen et al. [2023c]. Similar to Chen et al. [2023c], we also introduce a $L_{\\infty}$ and a modified process with truncation argument for $t\\in[t_{k},t_{k+1}]$ . Define the bad set ", "page_idx": 16}, {"type": "equation", "text": "$$\nB_{t}:=\\{||s_{t}-\\nabla\\ln q_{t}||\\geq\\varepsilon_{\\mathrm{score}\\;,\\infty}\\}\\,,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\varepsilon_{\\mathrm{score,\\infty}}>0$ is a parameter to be chosen later. We define the $L^{\\infty}$ -accurate score estimate to be ", "page_idx": 16}, {"type": "equation", "text": "$$\ns_{t}^{\\infty}:=s_{t}\\mathbf{1}_{B_{t}^{c}}+\\nabla\\ln q_{t}\\mathbf{1}_{B_{t}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We note that $\\left\\|s_{t}^{\\infty}-\\nabla\\ln q_{t}\\right\\|\\leq\\varepsilon_{\\mathrm{score}\\;,\\infty}.$ ", "page_idx": 16}, {"type": "text", "text": "The modified process with truncation argument for $t\\in[t_{k},t_{k+1}]$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{d}{\\mathbf{Y}}_{t}^{\\infty}=\\beta_{T-t}\\left\\{{\\mathbf{Y}}_{t}^{\\infty}/\\tau+2\\nabla\\log q_{T-t}^{\\tau}\\left({\\mathbf{Y}}_{t}^{\\infty}\\right)\\right\\}\\mathrm{d}t+\\sqrt{2\\beta_{T-t}}\\;\\mathrm{d}{\\mathbf{B}}_{t}\\,,}\\\\ &{\\mathrm{d}\\widetilde{{\\mathbf{Y}}}_{t}^{\\infty}=\\beta_{T-t}\\big\\{\\widetilde{{\\mathbf{Y}}}_{t}^{\\infty}/\\tau+2{\\mathbf{s}}(T-t_{k},\\widetilde{{\\mathbf{Y}}}_{t}^{\\infty})\\big\\}\\mathrm{d}t+\\sqrt{2\\beta_{T-t}}\\;\\mathrm{d}{\\mathbf{B}}_{t}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where ${\\bf Y}_{0}^{\\infty}=\\widetilde{{\\bf Y}}_{0}^{\\infty}$ is obtained by sampling $\\mathbf{Y}_{0}^{\\infty}\\sim q_{T}^{\\tau}$ and setting $\\widetilde{\\mathbf{Y}}_{0}^{\\infty}=\\mathbf{X}_{T}$ if $\\|\\mathbf{X}_{T}\\|\\leq\\bar{R}$ and setting $\\widetilde{\\mathbf{Y}}_{0}^{\\infty}=0$ otherwise. Then, we ara ready to prove the following lemma. ", "page_idx": 16}, {"type": "text", "text": "Lemma C.4 (Modified key lemma for Novikov's condition). ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathfrak{L}_{Q_{t_{K}}^{\\tau_{T}^{\\tau},\\infty}}\\exp\\left(\\sum_{k=0}^{K-1}\\int_{t_{k}}^{t_{k+1}}\\beta_{T-t_{k}}\\left\\|\\phi_{\\tilde{R}}\\left(\\mathbf{Y}_{T-t_{k}}^{\\infty}\\right)\\mathbf{s}_{T-t_{k}}^{\\infty}\\left(\\mathbf{Y}_{T-t_{k}}^{\\infty}\\right)-\\phi_{\\tilde{R}}\\left(\\mathbf{Y}_{T-t_{k}}^{\\infty}\\right)\\nabla\\ln q_{T-t}\\left(\\mathbf{Y}_{T-t_{k}}^{\\infty}\\right)\\right\\|\\right)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. We note that due to the manifold hypothesis (Assumption 4.1), if $\\bar{R}\\geq\\sqrt{d T^{2}+R^{2}}$ then the marginal distribution of ${\\bf Y}_{T-t_{k}}^{\\infty}$ is exactly the same compared to $\\mathbf{X}_{t_{k}}$ We also recallthat $\\bar{R}\\to+\\infty$ in Chen et al. [2023c] (Theorem 21 of Chen et al. [2023c]). Hence, we can use Lemma E.1 to prove that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\Vert\\sqrt{\\beta_{T-t_{k}}}\\phi_{\\bar{R}}\\left(\\mathbf{Y}_{T-t_{k}}^{\\infty}\\right)\\mathbf{s}_{T-t_{k}}^{\\infty}\\left(\\mathbf{Y}_{T-t_{k}}^{\\infty}\\right)\\right\\Vert\\leq\\underset{t^{*}\\in[0,T-\\delta]}{\\operatorname*{sup}}\\sqrt{\\beta_{T-t^{*}}}\\left\\Vert\\mathbf{s}_{T-t^{*}}^{\\infty}\\left(\\mathbf{Y}_{T-t^{*}}\\right)\\right\\Vert=:A^{\\prime}<\\infty\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\sqrt{\\beta_{T-t_{k}}}\\phi_{\\bar{R}}\\left(\\mathbf{Y}_{T-t_{k}}^{\\infty}\\right)\\nabla\\ln q_{T-t}\\left(\\mathbf{Y}_{T-t_{k}}^{\\infty}\\right)\\right|\\right|\\leq\\operatorname*{sup}_{t^{*}\\in[0,T-\\delta]}\\sqrt{\\beta_{T-t^{*}}}\\left\\|\\nabla\\ln q_{T-t^{*}}\\left(\\mathbf{Y}_{T-t^{*}}\\right)\\right\\|=:B^{\\prime}<\\infty,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then, the left hand of this lemma is at most $\\exp\\left(2T\\left(A^{\\prime2}+B^{\\prime2}\\right)\\right)<\\infty$ as claimed. ", "page_idx": 16}, {"type": "text", "text": "After obtaining the above inequality, the remaining proof for Novikov's condition are exactly compared to Chen et al. [2023c]. ", "page_idx": 16}, {"type": "text", "text": "Since we assume the accurate score function in this work, this lemma need to control ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{x^{*}\\in\\mathrm{B}(0,R),t^{*}\\in[0,T-\\delta]}2\\beta_{T-t^{*}}\\,\\|\\nabla\\ln q_{T-t^{*}}\\left(x^{*}\\right)\\|=:B<\\infty\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "As we shown in Lemma E.1, we know that with the early stopping parameter $\\delta$ $\\|\\nabla\\ln q_{T-t^{*}}\\left(x^{*}\\right)\\|$ is controlled. By using Assumption 4.1, we know that $\\frac{\\bullet}{\\beta_{T-t^{*}}}\\overset{\\cdot}{\\leq}\\bar{\\beta}$ . Finally, with similar process to Chen et al. [2023c], we can proof that the Novikov's condition is satisfied. The following lemma show the discretization error for our drifted VESDE with reverse SDE. ", "page_idx": 16}, {"type": "text", "text": "Lemma C.5 (Discretization). Suppose that Assumption 4.1 and Assumption 5.1 holds. Let $\\bar{\\gamma}_{K}=$ $\\mathrm{argmax}_{k\\in\\{0,...,K-1\\}}\\,\\gamma_{k},\\gamma_{K}=\\delta$ $^{r}\\tau\\in[1,T^{2}]$ and $\\beta_{t}\\in[1,t^{2}],$ then with $Q_{t_{K}}^{q_{T}^{\\tau}}$ and $R_{K}^{q_{T}^{\\tau}}$ defined in Lemma C.2, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{TV}\\left(R_{K}^{q_{T}^{\\tau}},Q_{t_{K}}^{q_{T}^{\\tau}}\\right)^{2}\\lesssim\\frac{R^{4}T\\tau\\beta_{T}d}{\\sigma_{\\delta}^{8}}\\bar{\\gamma}_{K}+\\frac{R^{6}T\\tau\\beta_{T}}{\\sigma_{\\delta}^{8}}\\bar{\\gamma}_{K}^{2}+\\epsilon_{s c o r e}^{2}T\\beta_{T}\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. First, we control the discretization error in an interval $t\\in[t_{k},t_{k+1}]$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{Q_{t_{K}}^{\\tau}}\\left[\\left\\|\\mathbf{s}\\left(T-t_{k},\\mathbf{Y}_{t_{k}}\\right)-\\nabla\\ln q_{T-t}\\left(\\mathbf{Y}_{t}\\right)\\right\\|^{2}\\right]}\\\\ &{\\lesssim\\epsilon_{\\mathrm{scoe}}^{2}+\\mathbb{E}_{Q_{t_{K}}^{\\tau}}\\left[\\left\\|\\nabla\\ln q_{T-t_{k}}\\left(\\mathbf{Y}_{t_{k}}\\right)-\\nabla\\ln q_{T-t}\\left(\\mathbf{Y}_{t_{k}}\\right)\\right\\|^{2}\\right]}\\\\ &{\\qquad+\\mathbb{E}_{Q_{t_{K}}^{\\tau}}\\left[\\left\\|\\nabla\\ln q_{T-t}\\left(\\mathbf{Y}_{t_{k}}\\right)-\\nabla\\ln q_{T-t}\\left(\\mathbf{Y}_{t}\\right)\\right\\|^{2}\\right]}\\\\ &{\\lesssim\\mathbb{E}_{Q_{t_{K}}^{\\tau}}\\left[\\left\\|\\nabla\\ln\\frac{q T-t_{k}}{q T-t}\\left(\\mathbf{Y}_{t_{k}}\\right)\\right\\|^{2}\\right]+L^{2}\\mathbb{E}_{Q_{t_{K}}^{\\tau}}\\left[\\left\\|\\mathbf{Y}_{t_{k}}-\\mathbf{Y}_{t}\\right\\|^{2}\\right]+\\epsilon_{\\mathrm{scoe}}^{2}}\\\\ &{\\lesssim\\tau L^{2}\\bar{d}\\bar{\\gamma}_{K}+\\tau L^{2}\\bar{\\gamma}_{K}^{2}\\left(d\\tau+R^{2}\\right)+\\tau L^{3}\\bar{\\gamma}_{K}^{2}+L^{2}\\bigl(\\beta_{T}d\\bar{\\gamma}_{K}+R^{2}\\bar{\\gamma}_{K}^{2}\\bigr)+\\epsilon_{\\mathrm{scoe}}^{2}}\\\\ &{\\lesssim\\tau L^{2}\\bar{d}\\bar{\\gamma}_{K}+\\tau L^{2}R^{2}\\bar{\\gamma}_{K}^{2}+\\epsilon_{\\mathrm{scoe}}^{2}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\begin{array}{r}{L_{-}=\\frac{\\operatorname*{max}_{t\\in[0,T_{-}\\delta]}}{-\\epsilon^{-}}\\left\\lVert\\nabla_{\\cdot}^{2}\\log q_{T-t}\\left(\\mathbf{Y}_{t}\\right)\\right\\rVert\\;\\leq\\;\\left(1+R^{2}\\right)/\\sigma_{\\delta}^{4}}\\end{array}$ and the third inequality follows Lemma E.5. Then, we know that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k=0}^{K-1}\\mathbb{E}_{Q_{t_{K}}^{q_{T}^{\\tau}}}\\int_{t_{k}}^{t_{k+1}}2\\beta_{T-t}\\left\\|s\\left(T-t_{k},\\mathbf{Y}_{t_{k}}\\right)-\\nabla\\ln q_{T-t}\\left(\\mathbf{Y}_{t}\\right)\\right\\|^{2}\\,\\mathrm{d}t}\\\\ &{\\lesssim\\tau T\\beta_{T}L^{2}d\\bar{\\gamma}_{K}+L^{2}R^{2}\\tau T\\beta_{T}\\bar{\\gamma}_{K}^{2}+\\epsilon_{\\mathrm{score}}^{2}T\\beta_{T}}\\\\ &{\\lesssim\\frac{R^{4}\\tau T\\beta_{T}d}{\\sigma_{\\delta}^{8}}\\bar{\\gamma}_{K}+\\frac{R^{6}\\tau T\\beta_{T}}{\\sigma_{\\delta}^{8}}\\bar{\\gamma}_{K}^{2}+\\epsilon_{\\mathrm{score}}^{2}T\\beta_{T}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "After obtaining the general discretization error for our drifted VESDE, we focus on two special cases. For $\\tau=T^{2}$ and $\\bar{\\beta}_{t}\\,\\stackrel{-}{=}t^{2}$ ,wehavethat ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k=0}^{K-1}\\mathbb{E}_{Q_{t_{K}}^{q_{T}^{\\tau}}}\\int_{t_{k}}^{t_{k+1}}2\\beta_{T-t}\\left\\|s\\left(T-t_{k},\\mathbf{Y}_{t_{k}}\\right)-\\nabla\\ln q_{T-t}\\left(\\mathbf{Y}_{t}\\right)\\right\\|^{2}\\,\\mathrm{d}t}\\\\ &{\\lesssim T^{3}\\beta_{T}L^{2}d\\bar{\\gamma}_{K}+L^{2}R^{2}T^{3}\\beta_{T}\\bar{\\gamma}_{K}^{2}+\\epsilon_{\\mathrm{score}}^{2}T\\beta_{T}}\\\\ &{\\lesssim\\frac{R^{4}T^{3}\\beta_{T}d}{\\sigma_{\\delta}^{8}}\\bar{\\gamma}_{K}+\\frac{R^{6}T^{3}\\beta_{T}}{\\sigma_{\\delta}^{8}}\\bar{\\gamma}_{K}^{2}+\\epsilon_{\\mathrm{score}}^{2}T\\beta_{T}}\\\\ &{=\\frac{R^{4}T^{5}d}{\\sigma_{\\delta}^{8}}\\bar{\\gamma}_{K}+\\frac{R^{6}T^{5}}{\\sigma_{\\delta}^{8}}\\bar{\\gamma}_{K}^{2}+\\epsilon_{\\mathrm{score}}^{2}T^{3}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "For $\\tau=T$ and $\\beta_{t}=t$ , we know that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k=0}^{K-1}\\mathbb{E}_{Q_{t_{K}}^{q_{T}^{\\tau}}}\\int_{t_{k}}^{t_{k+1}}2\\beta_{T-t}\\left\\|s\\left(T-t_{k},\\mathbf{Y}_{t_{k}}\\right)-\\nabla\\ln q_{T-t}\\left(\\mathbf{Y}_{t}\\right)\\right\\|^{2}\\,\\mathrm{d}t}\\\\ &{\\lesssim T^{3}L^{2}d\\bar{\\gamma}_{K}+L^{2}R^{2}T^{3}\\bar{\\gamma}_{K}^{2}+\\epsilon_{\\mathrm{score}}^{2}T^{2}}\\\\ &{\\lesssim\\displaystyle\\frac{R^{4}T^{3}d}{\\sigma_{\\delta}^{8}}\\bar{\\gamma}_{K}+\\frac{R^{6}T^{3}}{\\sigma_{\\delta}^{8}}\\bar{\\gamma}_{K}^{2}+\\epsilon_{\\mathrm{score}}^{2}T^{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Combined with the reversing beginning error controlled by Theorem 4.2, we can obtain the convergence guarantee for the general drifted VESDE with reverse SDE. ", "page_idx": 17}, {"type": "text", "text": "Theorem 5.2. Assume Assumption 3.1, 4.1, 5.1. Let $\\bar{D}$ defined in Theorem 4.2, $\\bar{\\gamma}_{K}\\;\\;=$ $a r g m a x_{k\\in\\{0,...,K-1\\}}\\gamma_{k}$ $\\tau=T^{2}$ and $\\beta_{t}\\in[1,t^{2}]$ . Then, we have that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{TV}\\left(R_{K}^{q_{\\infty}^{\\tau}},q_{\\delta}\\right)\\leq\\frac{\\bar{D}\\sqrt{m_{T}}}{\\sigma_{T}}+\\frac{R^{2}\\sqrt{d}}{\\sigma_{\\delta}^{4}}\\sqrt{\\bar{\\gamma}_{K}\\beta_{T}\\tau T}+\\epsilon_{\\mathrm{score}}\\sqrt{\\beta_{T}T}\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. By the data processing inequality, we know that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{TV}\\left(R_{K}^{q_{\\infty}^{\\tau}},q_{\\delta}\\right)\\leq\\mathrm{TV}\\left(R_{K}^{q_{\\infty}^{\\tau}},R_{K}^{q_{T}^{\\tau}}\\right)+\\mathrm{TV}\\left(R_{K}^{q_{T}^{\\tau}},Q_{t_{K}}^{q_{T}^{\\tau}}\\right)}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\leq\\mathrm{TV}\\left(q_{T}^{\\tau},q_{\\infty}^{\\tau}\\right)+\\mathrm{TV}\\left(R_{K}^{q_{\\infty}^{\\tau}},Q_{t_{K}}^{q_{T}^{\\tau}}\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Combined with Theorem 4.2 and Lemma C.5, we achieve the final result ", "page_idx": 18}, {"type": "text", "text": "C.1  The Sample Complexity for Drifted VESDE ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "As shown in Theorem 5.2, the general convergence guarantee is ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{\\bar{D}\\sqrt{m_{T}}}{\\sigma_{T}}+\\frac{R^{2}\\sqrt{d}}{\\sigma_{\\delta}^{4}}\\sqrt{\\bar{\\gamma}_{K}\\beta_{T}\\tau T}+\\epsilon_{\\mathrm{score}}\\,\\sqrt{\\beta_{T}T}\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "In this section, we provide the sample complexity for different $\\beta_{t}$ and $\\tau$ ", "page_idx": 18}, {"type": "text", "text": "The results of $\\tau\\,=\\,1$ and $\\beta_{t}\\,=\\,1$ . When considering $\\beta_{t}\\,=\\,1$ and $\\tau=1$ , the drifted VESDE becomes VPSDE and $m_{T}=\\exp\\left(-T\\right)$ , which indicates $T$ is a logarithmic term and the dominated term of the convergence guarantee is the discretization term $\\widetilde{\\cal O}(R^{2}\\sqrt{d\\bar{\\gamma}_{K}}/\\sigma_{\\delta}^{4})$ . To make this term smaller than $\\epsilon_{\\mathrm{TV}}$ , we require $\\bar{\\gamma}_{K}\\leq\\sigma_{\\delta}^{8}\\epsilon_{\\mathrm{TV}}^{2}/(R^{4}d)$ . To make sure that $W_{2}^{2}\\left(q_{0},\\bar{q}_{\\delta}\\right)\\leq\\epsilon_{W_{2}}^{2}$ , we require $\\begin{array}{r}{\\sigma_{\\delta}^{2}\\leq\\frac{\\epsilon_{W_{2}}^{2}}{d+R\\sqrt{d}}}\\end{array}$ . Then, we achieve the final sample complexity ", "page_idx": 18}, {"type": "equation", "text": "$$\nK\\leq{\\widetilde O}\\left({\\frac{d R^{4}(d+R{\\sqrt{d}})^{4}}{\\epsilon_{W_{2}}^{8}\\epsilon_{\\mathrm{TV}}^{2}}}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We note that this results is exactly the same with Chen et al. [2023c], which means our drifted VESDE covers VPSDE setting. ", "page_idx": 18}, {"type": "text", "text": "C.1.1 The results of $\\tau=T^{2}$ with different $\\beta_{t}$ ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this part, we analyze the influence of $\\beta_{t}$ under setting $\\tau=T^{2}$ and show the power of our drifted VESDE. ", "page_idx": 18}, {"type": "text", "text": "The aggressive $\\beta_{t}$ $(\\tau=T^{2}$ 0. When considering aggressive $\\beta_{t}=t^{\\alpha_{1}}$ where $2\\geq\\alpha_{1}\\geq1+\\ln(T-$ $\\ln(T))/\\ln(T),\\sqrt{m_{T}}/\\sigma_{T}\\geq\\exp\\left(-T/2\\right)$ , which means $T$ is a logarithmic term and can be ignored. After that, the analysis process is exactly the same with the above VPSDE setting, and we achieve the sample complexity ", "page_idx": 18}, {"type": "equation", "text": "$$\nK\\leq\\widetilde{\\cal O}\\left(\\frac{d R^{4}(d+R\\sqrt{d})^{4}}{\\epsilon_{W_{2}}^{8}\\epsilon_{\\mathrm{TV}}^{2}}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Definedbye $R_{K,R_{0}}^{q_{\\infty}^{\\tau}}$ $R_{K}^{q_{\\infty}^{\\tau}}$ $\\mathrm{B}\\left(0,R_{0}\\right)$ $R_{0}=\\widetilde{\\Theta}(R)$ same proof process of Chen et al. [2023c] (Corollary 5), we have that ", "page_idx": 18}, {"type": "equation", "text": "$$\nW_{2}(R_{K,R_{0}}^{q_{\\infty}^{\\tau}},q_{0})\\leq\\epsilon_{W_{2}}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "with sample complexity ", "page_idx": 18}, {"type": "equation", "text": "$$\nK\\leq\\widetilde{\\cal O}\\left(\\frac{d R^{8}(d+R\\sqrt{d})^{4}}{\\epsilon_{W2}^{12}}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The conservative $\\beta_{t}=t$ .In this case, the first term is $\\bar{D}/T$ To make this term smaller than $\\epsilon_{\\mathrm{TV}}$ we require $T\\,\\geq\\,\\bar{D}/\\epsilon_{\\mathrm{TV}}$ . For the stepsize, we require $\\bar{\\gamma}_{K}\\,\\leq\\,\\sigma_{\\delta}^{8}\\epsilon_{\\mathrm{TV}}^{2}/(R^{4}d T^{4})$ , which means the sample complexity is ", "page_idx": 18}, {"type": "equation", "text": "$$\nK\\leq O\\left(\\frac{d R^{4}(d+R\\sqrt{d})^{4}\\bar{D}^{5}}{\\epsilon_{W_{2}}^{8}\\epsilon_{\\mathrm{TV}}^{7}}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The most conservative $\\beta_{t}\\ =\\ 1$ . In this case, $m_{T}\\;=\\;\\exp\\left(-1/T\\right)$ and $\\sigma_{t}^{2}\\;=\\;\\tau(1\\,-\\,m_{t}^{2})\\;=$ $T^{2}(1-\\exp\\left(-2/T\\right)$ . When $T$ is large enough, $m_{T}=\\Theta(1)$ and $\\sigma_{T}^{2}=T$ which indicates the first term is $\\bar{D}/\\sqrt{T}$ To make this term smaller than $\\epsilon_{\\mathrm{TV}}$ , we require $T\\geq\\bar{D}^{2}/\\epsilon_{\\mathrm{TV}}^{2}$ . For the second term, we require $\\bar{\\gamma}_{K}\\leq\\sigma_{\\delta}^{8}\\epsilon_{\\mathrm{TV}}^{2}/(R^{4}d T^{3})$ Then, the final complexity is ", "page_idx": 19}, {"type": "equation", "text": "$$\nK\\leq O\\left(\\frac{d R^{4}(d+R\\sqrt{d})^{4}\\bar{D}^{8}}{\\epsilon_{W_{2}}^{8}\\epsilon_{\\mathrm{TV}}^{10}}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "C.1.2   The results of $\\tau=T$ with different $\\beta_{t}$ ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "At the remaining part, we show the sample complexity of setting $\\tau=T$ with different $\\beta_{t}$ ", "page_idx": 19}, {"type": "text", "text": "The results for setting $\\tau\\;=\\;T$ and $\\beta_{t}~=~t.$ .For this setting, as shown in Lemma C.5, the discretization error is ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{TV}\\left(R_{K}^{q_{T}^{\\tau}},Q_{t_{K}}^{q_{T}^{\\tau}}\\right)^{2}\\lesssim\\frac{R^{4}T^{3}d}{\\sigma_{\\delta}^{8}}\\bar{\\gamma}_{K}+\\frac{R^{6}T^{3}}{\\sigma_{\\delta}^{8}}\\bar{\\gamma}_{K}^{2}+\\epsilon_{\\mathrm{score}}^{2}T^{2}\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Furthermore, we choose an aggressive $\\beta_{t}$ , which indicates $T$ is a logarithmic term. Then, by choosing $\\begin{array}{r}{\\sigma_{\\delta}^{2}\\leq\\frac{\\epsilon_{W_{2}}^{2}}{(d+R\\sqrt{d})}}\\end{array}$ and $\\bar{\\gamma}_{K}\\leq\\sigma_{\\delta}^{8}\\epsilon_{\\mathrm{TV}}^{2}\\ln^{3}\\left(\\bar{D}/\\epsilon_{\\mathrm{TV}}\\right)/\\left(R^{4}d\\right)$ we obtain the sample complexity ", "page_idx": 19}, {"type": "equation", "text": "$$\nK={\\frac{T}{{\\bar{\\gamma}}_{K}}}\\leq{\\widetilde{O}}\\left({\\frac{d R^{4}(d+R{\\sqrt{d}})^{4}}{\\epsilon_{W_{2}}^{8}\\epsilon_{\\mathrm{{TV}}}^{2}}}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The results for setting $\\tau\\;=\\;T$ and $\\beta_{t}~=~1$ In this setting, the reverse beginning error is bounded by $\\bar{D}/\\sqrt{T}$ 2which indicates $T~\\ge~\\bar{D}^{2}/\\epsilon_{\\mathrm{TV}}^{2}$ . For the discretization term, we require $\\bar{\\gamma}_{K}\\leq\\sigma_{\\delta}^{8}\\epsilon_{\\mathrm{TV}}^{2^{\\circ}}/(\\bar{R}^{4}d T^{2})$ . Then, the sample complexity is bounde by ", "page_idx": 19}, {"type": "equation", "text": "$$\nK=\\frac{T}{\\bar{\\gamma}_{K}}\\leq O\\left(\\frac{d R^{4}(d+R\\sqrt{d})^{4}\\bar{D}^{6}}{\\epsilon_{W_{2}}^{8}\\epsilon_{\\mathrm{TV}}^{8}}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "D The Proof of the Convergence Guarantee in the Unified Framework ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this work, we introduce an indicator $i\\in\\{1,2\\}$ for $\\sigma_{T-t_{K}}$ to represent different $\\beta_{t}$ . We use $\\tau=T^{2}$   \nas an example. When $\\beta_{t}=t^{2}$ is aggressive, we choose $i=1$ \uff0c $\\eta=1$ and $\\begin{array}{r}{\\sigma_{T-t_{K}}^{-2}(i=1)\\le\\frac{1}{\\tau}+\\frac{\\bar{\\beta}}{\\delta^{3}}}\\end{array}$ $\\beta_{t}=t$ $i=2$ \uff0c\u4e00 $\\eta\\in[0,1)$ $\\begin{array}{r}{\\sigma_{\\underline{{T}}-t_{K}}^{-2}(i=2)\\leq\\frac{1}{\\tau}+\\frac{\\bar{\\beta}}{\\delta^{2}}.}\\end{array}$ $i$   \nthis lemma does not involve the specific value of $\\sigma_{T-t_{K}}^{2}(i)$ Before the proff this section, we fst   \nrecall the stochastic fow of the reverse process for any $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ and $s,t\\in[0,T]$ with $t\\geq s$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{Y}_{s,t}^{x}=\\beta_{T-t}\\left\\{\\mathbf{Y}_{s,t}^{x}/\\tau+\\left(1+\\eta^{2}\\right)\\nabla\\log q_{T-t}\\left(\\mathbf{Y}_{s,t}^{x}\\right)\\right\\}\\mathrm{d}t+\\eta\\sqrt{2\\beta_{T-t}}\\mathrm{d}\\mathbf{B}_{t},\\qquad\\mathbf{Y}_{s,s}^{x}=x\\,,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and the interpolation of its discretization for any $k\\in\\{0,...,K\\}$ and $t\\in\\left[s_{k},t_{k+1}\\right)$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{d}\\bar{\\mathbf{Y}}_{s,t}^{x}(k)=\\beta_{T-t}\\left\\{\\bar{\\mathbf{Y}}_{s,t}^{x}/\\tau+\\left(1+\\eta^{2}\\right)\\mathbf{s}\\left(T-s_{k},\\bar{\\mathbf{Y}}_{s,t}^{x}\\right)\\right\\}\\mathrm{d}t+\\eta\\sqrt{2\\beta_{T-t}}\\mathrm{d}\\mathbf{B}_{t},\\qquad\\bar{\\mathbf{Y}}_{s,s}^{x}=\\boldsymbol{x}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $s_{k}=\\operatorname*{max}\\left(s,t_{k}\\right)$ . To deal with the discretization error, we use the approximation technique used in Bortoli [2022]. Hence, we introduce the tangent process: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{d}\\nabla{\\mathbf{Y}}_{s,t}^{x}=\\beta_{T-t}\\left\\{\\mathbf{I}/\\tau+\\left(1+\\eta^{2}\\right)\\nabla^{2}\\log q_{T-t}(\\mathbf{Y}_{s,t}^{x})\\right\\}\\nabla{\\mathbf{Y}}_{s,t}^{x}\\mathrm{d}t,\\qquad\\nabla{\\mathbf{Y}}_{s,s}^{x}=\\mathbf{I}\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then, we discuss the interpolation formula, which is used to control the discretization error. Proposition 1. For $s,t\\in[0,T)$ with $s<t$ any $k\\in\\{0,...,K\\}$ and $(\\omega_{v})_{v\\in[s,T]}$ , we define that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{b_{u}(\\omega)=\\beta_{T-u}(\\frac{1}{\\tau}\\omega_{u}+(1+\\eta^{2})\\nabla\\log q_{T-u}(\\omega_{u}))\\,,}}\\\\ {{\\bar{b}_{u}(\\omega)=\\beta_{T-u}(\\frac{1}{\\tau}\\omega_{u}+(1+\\eta^{2})\\mathbf{s}(T-s_{k},\\omega_{s_{k}}))\\,,\\,\\Delta b_{u}(\\omega)=b_{u}(\\omega)-\\bar{b}_{u}(\\omega)\\,,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $s_{k}=\\operatorname*{max}(s,t_{k})$ and $u\\in[s_{k},t_{k+1})$ . Then, for any $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ , we have that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbf{Y}_{s,t}^{x}-\\bar{\\mathbf{Y}}_{s,t}^{x}=\\int_{s}^{t}\\nabla\\mathbf{Y}_{u,t}^{x}\\left(\\bar{\\mathbf{Y}}_{s,u}^{x}\\right)^{\\top}\\Delta b_{u}\\left(\\left(\\bar{\\mathbf{Y}}_{s,v}^{x}\\right)_{v\\in[s,T]}\\right)\\mathrm{d}u,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where for any $u\\in[0,T)$ , there exists a $k\\in\\{0,...,K\\}$ satisfies $u\\in[s_{k},t_{k+1})$ ", "page_idx": 20}, {"type": "text", "text": "For reverse SDE, the augmentation is similar to Bortoli [2022] (Appendix E). When $\\eta\\,=\\,0$ ,the stochastic extension of the Alekseev-Grobner formula [Del Moral and Singh, 2022] degenerates into the original version [Alekseev, 1961]. After that, we control the tangent process. ", "page_idx": 20}, {"type": "text", "text": "Lemma 6.3. Assume Assumption 3.1 and 4.1. For $\\forall s\\in[0,t_{K}]$ and $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ ,wehave ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\|\\nabla{\\mathbf{Y}}_{s,t_{K},i}^{x}\\|\\leq\\exp\\left(\\frac{R^{2}}{2\\sigma_{T-t_{K}}^{2}}+\\frac{(1-\\eta^{2})}{2}\\int_{0}^{t_{K}}\\frac{\\beta_{T-u}}{\\tau}\\mathrm{d}u\\right)\\,.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{\\ P}^{\\Vert\\nabla^{2}\\log q_{t}}\\left(x_{t}\\right)\\Vert\\leq\\Gamma/\\sigma_{t}^{2},\\,\\Vert\\nabla\\mathbf{Y}_{s,t_{K},i}^{x}\\Vert\\leq\\sigma_{T-t_{K}}^{-(1+\\eta^{2})\\Gamma}\\exp\\Big(\\big(\\big(1+\\eta^{2}\\big)\\,\\Gamma+2\\big)\\int_{0}^{t_{K}}\\frac{\\beta_{T-u}}{\\tau}\\mathrm{d}u\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. Using the definition of the tangent process and Lemma E.1, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\textup{d}\\big\\|\\nabla{\\mathbf{Y}}_{s,t}^{x}\\big\\|^{2}}\\\\ &{\\ \\leq2\\beta_{T-t}\\left(\\frac{1}{\\tau}\\left\\|\\nabla{\\mathbf{Y}}_{s,t}^{x}\\right\\|^{2}-\\left(1+\\eta^{2}\\right)\\left(1-m_{T-t}^{2}R^{2}/\\left(2\\sigma_{T-t}^{2}\\right)\\right)/\\sigma_{T-t}^{2}\\left\\|\\nabla{\\mathbf{Y}}_{s,t}^{x}\\right\\|^{2}\\right)\\operatorname{d}t\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Using Lemma F.1, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\int_{s}^{t}\\beta_{T-u}\\left(\\frac{1}{\\tau}-\\left(1+\\eta^{2}\\right)/\\sigma_{T-u}^{2}+\\left(1+\\eta^{2}\\right)m_{T-u}^{2}R^{2}/2\\sigma_{T-u}^{4}\\right)\\mathrm{d}u}\\\\ &{\\leq\\left(\\left(1+\\eta^{2}\\right)R^{2}/4\\right)\\left(\\sigma_{T-t}^{-2}-\\sigma_{T-s}^{-2}\\right)+\\frac{1-\\eta^{2}}{2}\\int_{s}^{t}\\frac{\\beta_{T-u}}{\\tau}\\mathrm{d}u}\\\\ &{\\leq\\frac{\\left(1+\\eta^{2}\\right)R^{2}}{4\\sigma_{T-t}^{2}}+\\frac{1-\\eta^{2}}{2}\\int_{s}^{t}\\frac{\\beta_{T-u}}{\\tau}\\mathrm{d}u\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Note that $\\nabla\\mathbf{Y}_{s,s}=\\mathbf{I}$ , we get ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\|\\nabla{\\mathbf{Y}}_{s,t_{K}}^{x}\\|^{2}\\leq\\exp\\left[\\frac{\\left(1+\\eta^{2}\\right)R^{2}}{2\\sigma_{T-t}^{2}}+\\left(1-\\eta^{2}\\right)\\int_{0}^{t_{K}}\\frac{\\beta_{T-u}}{\\tau}\\mathrm{d}u\\right].\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "When we assume $\\left\\|\\nabla\\log q_{t}^{2}\\left(x_{t}\\right)\\right\\|\\leq\\Gamma/\\sigma_{t}^{2}$ , we know that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname{d}\\left\\|\\nabla{\\mathbf{Y}}_{s,t}^{x}\\right\\|^{2}\\leq2\\beta_{T-t}\\left(\\frac{1}{\\tau}-\\frac{\\left(1+\\eta^{2}\\right)\\Gamma}{\\sigma_{T-t}^{2}}\\right)\\left\\|\\nabla{\\mathbf{Y}}_{s,t}^{x}\\right\\|^{2}\\operatorname{d}t\\,.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Using Lemma F.1, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2\\displaystyle\\int_{s}^{t}\\beta_{T-u}/\\sigma_{T-u}^{2}\\mathrm{d}u}\\\\ &{\\le\\log\\left(\\exp\\left[2\\displaystyle\\int_{0}^{T-s}\\frac{\\beta_{T-u}}{\\tau}\\,\\mathrm{d}u\\right]-1\\right)-\\log\\left(\\exp\\left[2\\displaystyle\\int_{0}^{T-t}\\frac{\\beta_{T-u}}{\\tau}\\,\\mathrm{d}u\\right]-1\\right)}\\\\ &{\\le\\log\\left(\\sigma_{T-s}^{2}\\right)-\\log\\left(\\sigma_{T-t}^{2}\\right)+\\displaystyle\\int_{T-t}^{T-s}\\frac{\\beta_{u}}{\\tau}\\,\\mathrm{d}u\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\|\\nabla{\\mathbf Y}_{s,t_{K}}^{x}\\|^{2}\\leq\\sigma_{T-t_{K}}^{-(1+\\eta^{2})\\Gamma}\\exp\\left[\\left(\\left(1+\\eta^{2}\\right)\\Gamma+2\\right)\\int_{0}^{t_{K}}\\frac{\\beta_{T-u}}{\\tau}\\mathrm{d}u\\right]\\,.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "After bounding the gradient of the tangent process, the remaining term is $\\lVert\\Delta b\\rVert$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|\\Delta b\\|\\leq\\|\\Delta^{(a,b)}b\\|+\\|\\Delta^{(b,c)}b\\|+\\|\\Delta^{(c,d)}b\\|\\,,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $b^{(a)}=b$ and $b^{(d)}=\\bar{b}$ . Moreover, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{b_{u}^{(b)}(\\omega)=\\beta_{T-u}(\\frac{1}{\\tau}\\omega_{u}+(1+\\eta^{2})\\nabla\\log q_{T-s_{k}}(\\omega_{u}))\\,,}\\\\ &{b_{u}^{(c)}(\\omega)=\\beta_{T-u}(\\frac{1}{\\tau}\\omega_{u}+(1+\\eta^{2})\\nabla\\log q_{T-s_{k}}(\\omega_{s_{k}}))\\,,}\\\\ &{\\Delta_{b}^{a,b}=b^{(a)}-b^{(b)},\\,\\Delta_{b}^{b,c}=b^{(b)}-b^{(c)},\\,\\Delta_{b}^{c,d}=b^{(c)}-b^{(d)}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We then control $\\|\\Delta^{(a,b)}b\\|,\\|\\Delta^{(b,c)}b\\|,\\|\\Delta^{(c,d)}b\\|$ separately. In this section, $\\|\\Delta^{(c,d)}b\\|\\,=\\,0$ since we assume that the accurate score function is achieved. For $\\left\\|\\Delta^{(a,b)}b_{u}(\\omega)\\right\\|$ , we have the following lemma. ", "page_idx": 21}, {"type": "text", "text": "Lemma D.1. For $s,u\\in[0,T)$ such that $u\\geq s,u\\in\\left[s_{k},t_{k+1}\\right)$ and $\\boldsymbol{\\omega}=(\\omega_{v})_{v\\in[s,T]}$ we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\Delta^{(a,b)}b_{u}\\left(\\omega\\right)\\|}\\\\ &{\\leq\\left(1+\\eta^{2}\\right)\\beta_{T-u}\\operatorname*{sup}_{v\\in[T-u,T-t_{k}]}\\left(\\beta_{v}/\\sigma_{v}^{6}\\right)\\left(2+R^{2}\\right)\\left(R+\\left\\|\\omega_{u}\\right\\|\\right)\\gamma_{k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. Without loss of generality, we assume $s\\leq t_{k}$ . Then ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\Delta^{(a,b)}b_{u}\\left(\\omega\\right)\\|\\leq\\left(1+\\eta^{2}\\right)\\beta_{T-u}\\|\\nabla\\log q_{T-u}\\left(\\omega_{u}\\right)-\\nabla\\log q_{T-t_{k}}\\left(\\omega_{u}\\right)\\|}\\\\ &{\\qquad\\qquad\\qquad\\leq\\left(1+\\eta^{2}\\right)\\beta_{T-u}\\gamma_{k}\\displaystyle\\operatorname*{sup}_{v\\in[T-u,T-t_{k}]}\\|\\partial_{v}\\nabla\\log q_{T-v}\\left(\\omega_{u}\\right)\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then by Lemma E.4, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\Delta^{(a,b)}b_{u}\\left(\\omega\\right)\\right\\|}\\\\ &{\\leq\\left(1+\\eta^{2}\\right)\\beta_{T-u}\\displaystyle\\operatorname*{sup}_{v\\in[T-u,T-t_{k}]}\\left(\\beta_{v}/\\sigma_{v}^{6}\\right)\\left(2+R^{2}\\right)\\left(R+\\left\\|\\omega_{u}\\right\\|\\right)\\gamma_{k}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For $\\left\\|\\Delta^{(b,c)}b_{u}(\\omega)\\right\\|$ , we have the following lemma. ", "page_idx": 21}, {"type": "text", "text": "Lemma D.2. For $s,u\\in[0,T)$ such that $u\\geq s,u\\in\\left[s_{k},t_{k+1}\\right)$ and $\\boldsymbol{\\omega}=(\\omega_{v})_{v\\in[s,T]}$ we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\Delta^{(b,c)}b_{u}\\left(\\omega\\right)\\|\\leq\\left(1+\\eta^{2}\\right)\\left(\\beta_{T-u}/\\sigma_{T-u}^{4}\\right)\\left(1+R^{2}\\right)\\left\\|\\omega_{u}-\\omega_{s_{k}}\\right\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. Without loss of generality, we assume $s\\leq t_{k}$ . In this case $s_{k}=t_{k}$ , Then ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\Delta^{(b,c)}b_{u}\\left(\\omega\\right)\\|\\leq\\left(1+\\eta^{2}\\right)\\beta_{T-u}\\|\\nabla\\log q_{T-t_{k}}\\left(\\omega_{t_{k}}\\right)-\\nabla\\log q_{T-t_{k}}\\left(\\omega_{u}\\right)\\|}\\\\ &{\\qquad\\qquad\\qquad\\leq\\left(1+\\eta^{2}\\right)\\beta_{T-u}\\displaystyle\\operatorname*{sup}_{v\\in[u,T-t_{k}]}\\|\\nabla^{2}\\log q_{T-t_{k}}\\left(\\omega_{v}\\right)\\|\\|\\omega_{u}-\\omega_{t_{k}}\\|\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Using Lemma E.2, we have that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Vert\\Delta^{(b,c)}b_{u}\\left(\\omega\\right)\\Vert\\leq\\left(1+\\eta^{2}\\right)\\left(\\beta_{T-u}/\\sigma_{T-u}^{4}\\right)\\left(1+R^{2}\\right)\\Vert\\omega_{u}-\\omega_{t_{k}}\\Vert\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then the proof is complete. ", "page_idx": 21}, {"type": "text", "text": "We need to control the reverse process when dealing with $\\Delta b$ . The following lemma shows an upper bound for the reverse $Y_{k}$ ", "page_idx": 21}, {"type": "text", "text": "Lemma D.3. Assume Assumption 3.1 ,Assumption 4.1, and there exists $\\delta>0$ such that $\\begin{array}{r}{\\frac{\\gamma_{k}\\beta_{T-t_{k}}}{\\sigma_{T-t_{k}}^{2}}\\leq}\\end{array}$ $\\delta\\leq1/28$ for any $k\\in\\{0,\\cdots,K\\}$ , then we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[\\|Y_{k}\\|^{2}]\\le U(\\tau)=\\tau d+B(1/A+\\delta)\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{A=4\\eta^{2}+2-2\\delta-4(1+\\eta^{2})(1+\\delta)\\mu R}}\\\\ {{B=4(1+\\eta^{2})R^{2}\\delta+2(1+\\eta^{2})(1+\\delta)\\frac{R}{\\mu}+4\\eta^{2}\\tau d}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and $\\mu$ is an arbitrary positive number which makes $A>0$ Inparticular,ij $^c\\delta\\leq1/28,$ then ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\|Y_{k}\\|^{2}]\\leq U_{0}(\\tau)=111R^{2}+13\\tau d\\,.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. Recall the discretization of the backward process (the explicit form of Equation (6)) ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Y_{k+1}=Y_{k}+\\gamma_{1,k}\\left(\\displaystyle\\frac{1}{\\tau}Y_{k}+(1+\\eta^{2})\\mathbf{s}\\left(T-t_{k},Y_{k}\\right)\\right)+\\eta\\sqrt{2\\gamma_{2,k}}Z_{k}\\,,}\\\\ &{\\,\\,\\gamma_{1,k}=\\exp\\left[\\displaystyle\\int_{T-t_{k+1}}^{T-t_{k}}\\beta_{s}\\,\\mathrm{d}s\\right]-1,\\quad\\gamma_{2,k}=\\left(\\exp\\left[2\\displaystyle\\int_{T-t_{k+1}}^{T-t_{k}}\\beta_{s}\\,\\mathrm{d}s\\right]-1\\right)/2\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\{Z_{k}\\}_{k\\in K}$ are independent Gaussian random variables. It is clear that $\\gamma_{1,k}\\,\\leq\\,\\gamma_{2,k}\\,\\leq\\,2\\gamma_{1,k}$ and using Lemma E.1 we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\langle x_{t},\\mathbf{s}(t,x_{t})\\rangle=\\langle x_{t},\\nabla\\log q_{t}(x_{t})\\rangle}}\\\\ &{\\leq-\\|x_{t}\\|^{2}/\\sigma_{t}^{2}+m_{t}R\\|x_{t}\\|/\\sigma_{t}^{2}}\\\\ &{\\leq(-1+\\mu m_{t}R)\\|x_{t}\\|^{2}/\\sigma_{t}^{2}+(m_{t}R/\\mu)/\\sigma_{t}^{2}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the first equality follows that we assume the accurate score function. For any $\\mu>0$ .Again using Lemma E.1, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|\\mathbf{s}(t,x_{t})\\|^{2}=\\|\\nabla\\log q_{t}(x_{t})\\|^{2}~~~~~~~~~}&{}\\\\ {\\le2\\|x_{t}\\|^{2}/\\sigma_{t}^{4}+2m_{t}^{2}R^{2}/\\sigma_{t}^{4}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Combining the results above, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\|Y_{k+1}\\|^{2}]=(1+\\frac{\\gamma_{1,k}}{\\tau})^{2}\\mathbb{E}[\\|Y_{k}\\|^{2}]+(1+\\eta^{2})^{2}\\gamma_{1,k}^{2}\\mathbb{E}[\\|s(T-t_{k},Y_{k})\\|^{2}]}\\\\ &{\\qquad\\qquad\\qquad+2(1+\\eta^{2})(1+\\frac{\\gamma_{1,k}}{\\tau})\\gamma_{1,k}\\mathbb{E}[\\langle Y_{k},s(T-t_{k},Y_{k})\\rangle]+2\\eta^{2}\\gamma_{2,k}d}\\\\ &{\\qquad\\qquad\\leq((1+\\frac{\\gamma_{1,k}}{\\tau})^{2}+2(1+\\eta^{2})^{2}\\gamma_{1,k}^{2}/\\sigma_{T-t_{k}}^{4}}\\\\ &{\\qquad\\qquad\\qquad+2(1+\\eta^{2})(1+\\frac{\\gamma_{1,k}}{\\tau})\\gamma_{1,k}(-1+\\mu m{\\cal T}-t_{k}R)/\\sigma_{T-t_{k}}^{2})\\mathbb{E}[\\|Y_{k}\\|^{2}]}\\\\ &{\\qquad\\qquad\\qquad+\\frac{2m_{T-t_{k}}^{2}R^{2}}{\\sigma_{T-t_{k}}^{4}}(1+\\eta^{2})^{2}\\gamma_{1,k}^{2}+\\frac{m_{T-t_{k}}R}{\\mu\\sigma_{T-t_{k}}^{2}}(1+\\eta^{2})(1+\\frac{\\gamma_{1,k}}{\\tau})\\gamma_{1,k}+4\\eta^{2}\\gamma_{1,k}d\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "If we denote $\\delta_{k}=\\gamma_{1,k}/\\sigma_{T-t_{k}}^{2}$ and notice the fact that $m_{t}\\in[0,1],\\sigma_{t}^{2}\\in[0,\\tau],\\eta\\in[0,1]$ , then we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}[\\|Y_{k+1}\\|^{2}]\\le(1+2\\delta_{k}+\\delta_{k}^{2})\\mathbb{E}[\\|Y_{k}\\|^{2}]+8\\delta_{k}^{2}\\mathbb{E}[\\|Y_{k}\\|^{2}]}\\\\ {\\displaystyle\\qquad\\qquad+\\,2(1+\\delta_{k})\\delta_{k}(-1+\\mu R)\\mathbb{E}[\\|Y_{k}\\|^{2}]+8R^{2}\\delta_{k}^{2}+\\frac{2R}{\\mu}\\delta_{k}(1+\\delta_{k})+4\\tau\\delta_{k}d\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We also have that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\gamma_{1,k}=\\exp[\\int_{T-t_{k+1}}^{T-t_{k}}\\beta_{s}\\mathrm{d}s]-1\\leq\\exp[\\beta_{T-t_{k}}\\gamma_{k}]-1\\leq2\\beta_{T-t_{k}}\\gamma_{k}\\,,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the last inequality follows that $\\gamma_{k}=\\exp{(-T)},\\beta_{T-t_{k}}\\gamma_{k}\\le1/2$ for small enough stepsize, and $e^{\\omega}-1\\leq2\\omega$ for any $\\omega\\in[0,1/2]$ .We get $\\delta_{k}\\le2\\gamma_{k}\\beta_{T-t_{k}}/\\sigma_{T-t_{k}}^{2}\\le2\\delta$ . Thus ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\|Y_{k+1}\\|^{2}]\\le(1+2\\delta_{k}+2\\delta_{k}\\delta)\\mathbb{E}[\\|Y_{k}\\|^{2}]+16\\delta_{k}\\delta\\mathbb{E}[\\|Y_{k}\\|^{2}]}\\\\ &{\\qquad\\qquad\\qquad+\\,4(1+\\delta)(-1+\\mu R)\\delta_{k}\\mathbb{E}[\\|Y_{k}\\|^{2}]+16R^{2}\\delta_{k}\\delta+4(1+\\delta)\\frac{R}{\\mu}\\delta_{k}+4\\tau d\\delta_{k}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Hence, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[\\|Y_{k+1}\\|^{2}]\\le(1+\\delta_{k}[-2+14\\delta+4(1+\\delta)\\mu R])\\mathbb{E}[\\|Y_{k}\\|^{2}]}\\\\ {+\\;\\delta_{k}[16R^{2}\\delta+4(1+\\delta)\\displaystyle\\frac{R}{\\mu}+4\\tau d]\\,.\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We denote $A=2-14\\delta-4(1+\\delta)\\mu R$ and $B=16R^{2}\\delta+4(1+\\delta)\\frac{R}{\\mu}+4\\tau d.$ then ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[\\|Y_{k+1}\\|^{2}]\\le(1-\\delta_{k}A)\\mathbb{E}[\\|Y_{k}\\|^{2}]+\\delta_{k}B\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Notice that $\\mathbb{E}[\\|Y_{0}\\|^{2}]=d\\tau$ and if $\\mathbb{E}[\\|Y_{k}\\|^{2}]\\geq B/A$ it is decreasing, if $\\mathbb{E}[\\|Y_{k}\\|^{2}]\\leq B/A$ we have $\\mathbb{E}[\\|Y_{k+1}\\|^{2}]\\overset{\\cdot\\cdot}{\\leq}B/\\overset{\\cdot}{A}\\overset{\\cdot}{+}\\delta B$ .So ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[\\|Y_{k}\\|^{2}]\\leq\\tau d+B(1/A+\\delta)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Notice that when $\\delta\\leq1/28,$ if we choose $\\mu=1/(4(1+\\delta)R),A\\geq1/2$ , and ", "page_idx": 23}, {"type": "equation", "text": "$$\nB\\leq37R^{2}+4\\tau d\\,.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Then, the proof is complete. ", "page_idx": 23}, {"type": "text", "text": "The following lemma shows a discretization error in the $k$ -the interval. ", "page_idx": 23}, {"type": "text", "text": "Lemma D.4. Assume Assumption 3.1,Assumption 4.1 and $\\gamma_{k}\\beta_{T-t_{k}}/\\sigma_{T-t_{k}}^{2}\\,\\leq\\,1/28$ for any $k\\in$ $\\{0,\\cdot\\cdot\\cdot,K-1\\}$ . Then for any $k$ $t\\in[t_{k},t_{k+1}]$ and $i\\in\\{1,2\\}$ , we have that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\|\\bar{\\mathbf{Y}}_{t}-\\bar{\\mathbf{Y}}_{t_{k}}\\|^{2}]\\leq L_{i}(\\tau)\\beta_{T-t_{k}}\\gamma_{k}\\,,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\begin{array}{r}{L_{i}(\\tau)\\,=\\,\\bar{\\gamma}_{K}\\kappa_{i}(\\tau)(\\frac{64}{\\sigma_{T-t_{K}}^{2}(i)}\\,+\\,\\frac{8}{\\tau})U_{0}(\\tau)\\,+\\,64R^{2}\\frac{\\bar{\\gamma}_{K}\\kappa_{i}(\\tau)}{\\sigma_{T-t_{K}}^{2}(i)}\\,+\\,4d,\\;\\bar{\\gamma}_{K},\\;\\kappa_{i}(\\tau)}\\end{array}$ is defined in Lemma $D.5$ and $U_{0}(\\tau)$ is defined in Lemma $D.3$ ", "page_idx": 23}, {"type": "text", "text": "Proof. Recall the discretized backward process ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{\\mathbf{Y}}_{t}=\\bar{\\mathbf{Y}}_{t_{k}}+(\\exp[\\int_{T-t}^{T-t_{k}}\\beta_{s}\\mathrm{d}s]-1)(\\frac{1}{\\tau}\\bar{\\mathbf{Y}}_{t_{k}}+(1+\\eta^{2})\\mathbf{s}(T-t_{k},\\bar{\\mathbf{Y}}_{t_{k}}))}\\\\ &{\\qquad\\qquad+\\,\\eta(\\exp[2\\int_{T-t}^{T-t_{k}}\\beta_{s}\\mathrm{d}s-1])^{1/2}\\mathcal{Z}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $Z$ is a standard Gaussian random variable. By directly calculating, we have that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\|\\bar{\\mathbf{Y}}_{t}-\\bar{\\mathbf{Y}}_{t_{k}}\\|^{2}]=2(\\exp[\\int_{T-t}^{T-t_{k}}\\beta_{s}\\mathrm{d}s]-1)^{2}(\\frac{1}{T^{2}}\\mathbb{E}[\\|\\bar{\\mathbf{Y}}_{t_{k}}\\|^{2}]+(1+\\eta^{2})^{2}\\mathbb{E}[\\|s(T-t_{k},\\bar{\\mathbf{Y}}_{t_{k}})\\|^{2}])}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\eta^{2}(\\exp[2\\int_{T-t}^{T-t_{k}}\\beta_{s}\\mathrm{d}s]-1)d\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "By Lemma E.1 and accurate score function assumption, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\|\\mathbf{s}(T-t_{k},\\bar{\\mathbf{Y}}_{t_{k}})\\|^{2}\\leq2\\|\\bar{\\mathbf{Y}}_{t_{k}}\\|^{2}/\\sigma_{T-t_{k}}^{4}(i)+2m_{T-t_{k}}^{2}R^{2}/\\sigma_{T-t_{k}}^{4}(i)\\,.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "So we have that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\|\\bar{\\mathbf{Y}}_{t}-\\bar{\\mathbf{Y}}_{t_{k}}\\|^{2}]\\leq2(\\exp[\\int_{T-t}^{T-t_{k}}\\beta_{s}\\mathrm{d}s]-1)^{2}((\\frac{8}{\\sigma_{T-t_{k}}^{4}(i)}+\\frac{1}{\\tau^{2}})\\mathbb{E}[\\|\\bar{\\mathbf{Y}}_{t_{k}}\\|^{2}]+\\frac{8R^{2}}{\\sigma_{T-t_{k}}^{4}(i)})}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\left(\\exp[2\\int_{T-t}^{T-t_{k}}\\beta_{s}\\mathrm{d}s]-1\\right)\\!d.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "By $\\mathrm{e}^{2w}\\textrm{--}1\\,\\leq\\,1\\,+\\,4w$ for any $w\\,\\in\\,[0,1/2]$ and $\\gamma_{k}\\operatorname*{sup}_{v\\in[T-t_{k+1},T-t_{k}]}\\beta_{v}/\\sigma_{v}^{2}\\,\\le\\,1/28$ for any $k\\in\\{0,...,K-1\\}$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\exp[\\rho\\int_{T-t}^{T-t_{k}}\\beta_{s}\\mathrm{d}s]-1\\leq2\\rho\\beta_{T-t_{k}}\\gamma_{k}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "for $\\rho=1,2$ . And using Lemma D.3 and Lemma F.2 we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\|\\bar{\\mathbf{Y}}_{t}-\\bar{\\mathbf{Y}}_{t_{k}}\\|^{2}]}\\\\ &{\\le(\\frac{64\\gamma_{k}}{\\sigma_{T-t_{k}}^{4}(i)}+\\frac{8\\beta_{T-t_{k}}\\gamma_{k}}{\\tau^{2}})U_{0}(\\tau)\\beta_{T-t_{k}}\\gamma_{k}+64R^{2}\\frac{\\gamma_{k}}{\\sigma_{T-t_{k}}^{4}(i)}\\beta_{T-t_{k}}\\gamma_{k}+4d\\beta_{T-t_{k}}\\gamma_{k}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We denote $\\begin{array}{r}{L_{i}(\\tau)=\\bar{\\gamma}_{K}\\kappa_{i}(\\tau)(\\frac{64}{\\sigma_{T-t_{K}}^{2}(i)}+\\frac{8}{\\tau})U_{0}(\\tau)+64R^{2}\\frac{\\bar{\\gamma}_{K}\\kappa_{i}(\\tau)}{\\sigma_{T-t_{K}}^{2}(i)}+4d}\\end{array}$ $i\\in\\{1,2\\}$ an he proof is complete. ", "page_idx": 24}, {"type": "text", "text": "Lemma D5. Assume Asumtion3.1and Asumption4.1, $\\gamma_{k}\\operatorname*{sup}_{v\\in[T-t_{k+1},T-t_{k}]}\\beta_{v}/\\sigma_{v}^{2}\\leq1/28$ for any $k\\in\\{0,...,K-1\\}$ . Let $\\bar{\\gamma}_{K}=a r g m a x_{k\\in\\{0,\\dots,K-1\\}}\\gamma_{k}$ \uff0c $\\begin{array}{r}{\\kappa_{i}(\\tau)=\\operatorname*{max}\\{\\bar{\\beta},\\frac{T^{2}}{T^{-1+i}}\\}\\sigma_{T-t_{K}}^{-2}(i),}\\end{array}$ and ", "page_idx": 24}, {"type": "equation", "text": "$$\nC_{i}(\\tau)=2(2+R^{2})(R+U_{0}^{1/2}(\\tau))+2L_{i}^{1/2}(\\tau)\\tau^{3/2}(1+R^{2})\\,,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "for $i\\in\\{1,2\\}$ . Then, for any $s,u\\in[0,t_{K}]$ with $u\\geq s$ and $i\\in\\{1,2\\}$ , we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\|\\Delta b_{u,i}((\\bar{\\mathbf{Y}}_{s,v})_{v\\in[s,T]})\\|]\\le C_{i}(\\tau)[\\kappa_{i}^{2}(\\tau)\\sigma_{T-t_{K}}^{-2}(i)\\bar{\\gamma}_{K}^{1/2}+\\kappa_{i}^{2}(\\tau)]\\bar{\\gamma}_{K}^{1/2},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\bar{\\mathbf{Y}}_{s,s}\\sim\\mathrm{N}(0,\\mathbf{I})$ ", "page_idx": 24}, {"type": "text", "text": "Proof. Combining Lemma D.1, Lemma D.2 and the exact score function, we get ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Vert\\Delta b_{u,i}(\\omega)\\Vert\\leq(1+\\eta^{2})\\underset{v\\in[T-t_{k+1},T-t_{k}]}{\\operatorname*{sup}}(\\beta_{v}^{2}/\\sigma_{v}^{6}(i))(2+R^{2})(\\mathrm{diam}(\\mathcal{M}+\\Vert\\omega_{u}\\Vert))\\gamma_{k}}\\\\ {+\\left(1+\\eta^{2}\\right)(\\beta_{T-u}/\\sigma_{T-u}^{4}(i))(1+\\mathrm{diam}(\\mathcal{M}^{2}))\\Vert\\omega_{u}-\\omega_{s_{k}}\\Vert.~~~~~~~~~~~}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "For any $u\\in[T-t_{K},T]$ , using Lemma F.3 we have $\\beta_{u}/\\sigma_{u}^{2}(i)\\le\\kappa_{i}(\\tau)$ . Hence, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\Delta b_{u,i}(\\omega)\\|\\leq(1+\\eta^{2})\\underset{v\\in[T-t_{k+1},T-t_{k}]}{\\operatorname*{sup}}(\\beta_{v}^{2}/\\sigma_{v}^{6}(i))(2+\\mathrm{diam}(\\mathcal{M}^{2}))(R+\\|\\omega_{u}\\|)\\gamma_{k}}\\\\ &{\\qquad\\qquad\\qquad+\\,(1+\\eta^{2})(\\beta_{T-u}/\\sigma_{T-u}^{4}(i))(1+\\mathrm{diam}(\\mathcal{M}^{2}))(\\|\\omega_{u}-\\omega_{t_{k}}\\|)}\\\\ &{\\qquad\\qquad\\leq(1+\\eta^{2})(\\kappa_{i}^{2}(\\tau)/\\sigma_{T-t_{k+1}}^{2}(i))\\gamma_{k}(2+\\mathrm{diam}(\\mathcal{M}^{2}))(R+\\|\\omega_{u}\\|)}\\\\ &{\\qquad\\qquad\\qquad+\\,(1+\\eta^{2})\\kappa_{i}^{2}(\\tau)(1+R^{2})\\|\\omega_{u}-\\omega_{t_{k}}\\|/\\beta_{T-u}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Combining this with Lemma D.3 and Lemma D.4, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\|\\Delta b_{u,i}((\\bar{\\mathbf{Y}}_{s,v})_{v\\in[s,T]})\\|]\\leq(1+\\eta^{2})(\\kappa_{i}^{2}(\\tau)/\\sigma_{T-t_{k+1}}^{2}(i))\\bar{\\gamma}_{K}(2+R^{2})(R+U_{0}^{1/2}(\\tau))}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\,(1+\\eta^{2})\\kappa_{i}^{2}(\\tau)(1+R^{2})L_{i}^{1/2}(\\tau)\\operatorname*{max}\\{\\bar{\\beta},\\tau\\}^{3/2}\\bar{\\gamma}_{K}^{1/2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We denote $\\displaystyle C_{i}(\\tau)=2(2+R^{2})(R+U_{0}^{1/2}(\\tau))+2L_{i}^{1/2}(\\tau)\\tau^{3/2}(1+R^{2}),$ for $i\\in\\{1,2\\}$ , then we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[\\|\\Delta b_{u,i}((\\bar{\\mathbf{Y}}_{s,v})_{v\\in[s,T]})\\|]\\leq C_{i}(\\tau)((\\kappa_{i}^{2}(\\tau)/\\sigma_{T-t_{k+1}}^{2})\\bar{\\gamma}_{K}+\\kappa_{i}^{2}(\\tau)\\bar{\\gamma}_{K}^{1/2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Lemma D6. Asume Assumption 3.1 and Assumption 4.1, $\\gamma_{k}\\operatorname*{sup}_{v\\in[T-t_{k+1},T-t_{k}]}\\beta_{v}/\\sigma_{v}^{2}\\leq1/28$ for any $k\\in\\{0,...,K-1\\}$ Let $\\bar{\\gamma}_{K}=a r g m a x_{k\\in\\{0,\\dots,K-1\\}}\\gamma_{k}$ $\\gamma_{K}=\\delta$ and $\\delta\\leq1/32$ Then ", "page_idx": 24}, {"type": "equation", "text": "$$\nW_{1}\\left(R_{K}^{q_{\\infty}^{\\tau}},Q_{t_{K}}^{q_{\\infty}^{\\tau}}\\right)\\leq C_{i}(\\tau)\\kappa_{i}^{2}(\\tau)T\\exp\\left[\\frac{R^{2}}{2\\sigma_{T-t_{K}}^{2}(i)}+\\frac{(1-\\eta^{2})}{2}\\right][\\frac{\\bar{\\gamma}_{K}^{1/2}}{\\sigma_{T-t_{K}}^{2}(i)}+1]\\bar{\\gamma}_{K}^{1/2}\\,,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $C_{i}(\\tau),\\kappa_{i}(\\tau)$ for $i\\in\\{1,2\\}$ are the same terms to Theorem 6.1. ", "page_idx": 24}, {"type": "text", "text": "Proof. By Proposition 1 we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\|\\mathbf{Y}_{t_{K}}-Y_{K}\\|=\\|\\mathbf{Y}_{t_{K}}-\\bar{\\mathbf{Y}}_{t_{K}}\\|\\leq\\int_{0}^{t_{K}}\\|\\nabla\\mathbf{Y}_{u,t_{K},i}(\\bar{\\mathbf{Y}}_{0,u})\\|\\|\\Delta b_{u,i}((\\bar{\\mathbf{Y}}_{0,v})_{v\\in[0,T]})\\|\\mathrm{d}u.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\Upsilon_{t_{K}}-Y_{K}\\|}\\\\ &{\\le\\exp\\left[\\displaystyle\\frac{\\left(1+\\eta^{2}\\right)R^{2}}{4\\sigma_{T-t}^{2}(i)}+\\displaystyle\\frac{\\left(1-\\eta^{2}\\right)}{2}\\int_{0}^{t_{K}}\\displaystyle\\frac{\\beta_{T-u}}{\\tau}\\mathrm{d}u\\right]\\int_{0}^{t_{K}}\\|\\Delta b_{u,i}\\big((\\bar{\\mathbf{Y}}_{0,v})_{v\\in[0,T]}\\big)\\|\\mathrm{d}u\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Then by definition of Wasserstein distance, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{W_{1}(q_{\\infty}Q_{t_{K}},q_{\\infty}R_{K})}\\\\ &{\\leq\\mathbb{E}[\\|\\mathbf{Y}_{t_{K}}-Y_{K}\\|]}\\\\ &{\\leq\\exp\\left[\\frac{\\left(1+\\eta^{2}\\right)R^{2}}{4\\sigma_{T-t_{K}}^{2}(i)}+\\frac{(1-\\eta^{2})}{2}\\int_{0}^{t_{K}}\\frac{\\beta_{T-u}}{\\tau}\\mathrm{d}u\\right]\\int_{0}^{t_{K}}\\mathbb{E}[\\|\\Delta b_{u,i}((\\bar{\\mathbf{Y}}_{0,v})_{v\\in[0,T]}\\|]\\mathrm{d}u}\\\\ &{\\leq C_{i}(\\tau)T\\exp\\left[\\frac{\\left(1+\\eta^{2}\\right)R^{2}}{4\\sigma_{T-t_{K}}^{2}(i)}+\\frac{(1-\\eta^{2})}{2}\\right][\\kappa_{i}^{2}(\\tau)\\sigma_{T-t_{K}}^{-2}(i)\\bar{\\gamma}_{K}^{1/2}+\\kappa_{i}^{2}(\\tau)]\\bar{\\gamma}_{K}^{1/2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Theorem6.1. Assume Assumption 3.1 and4.1 $\\delta\\leq1/32$ and $\\gamma_{k}\\operatorname*{sup}_{v\\in[T-t_{k+1},T-t_{k}]}\\beta_{v}/\\sigma_{v}^{2}\\leq1/28$ for $\\forall k\\in\\{0,...,K-1\\}$ . Let $\\gamma_{K}=\\delta$ .Then, for $\\forall\\tau\\in[T,T^{2}]$ ", "page_idx": 25}, {"type": "text", "text": "$(I)$ $\\eta=1$ (thereverse $S D E$ ,choosing $\\beta_{t}=t^{2}$ $W_{1}\\left(R_{K}^{q_{\\infty}^{\\tau}},q_{0}\\right)$ isboundedby ", "page_idx": 25}, {"type": "equation", "text": "$$\n(\\frac{R}{\\tau}+\\sqrt{d})\\sqrt{\\delta}+\\exp\\left(\\frac{R^{2}}{2}(\\frac{\\bar{\\beta}}{\\delta^{3}}+\\frac{1}{\\tau})\\right)\\left(C_{1}(\\tau)T\\kappa_{1}^{2}(\\tau)\\left((\\frac{\\bar{\\beta}}{\\delta^{3}}+\\frac{1}{\\tau})\\bar{\\gamma}_{K}^{1/2}+1\\right)\\bar{\\gamma}_{K}^{1/2}+\\frac{\\bar{D}e^{-T/2}}{\\sqrt{\\tau}}\\right),\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $\\kappa_{1}(\\tau)=T^{2}(1/\\tau+\\bar{\\beta}/\\delta^{3})$ and $C_{1}(\\tau)$ is linear in $\\tau^{2}$ ", "page_idx": 25}, {"type": "text", "text": "(2) If $\\dot{\\eta}=0\\left(P F O D E\\right)$ choosing a conservative $\\beta_{t}$ (Assumption 3.1), $W_{1}\\left(R_{K}^{q_{\\infty}^{\\tau}},q_{0}\\right)$ is bounded by ", "page_idx": 25}, {"type": "equation", "text": "$$\n(\\frac{R}{\\tau}+\\sqrt{d})\\sqrt{\\delta}+\\exp\\left(\\frac{R^{2}}{2}(\\frac{\\bar{\\beta}}{\\delta^{2}}+\\frac{1}{\\tau})\\right)\\left(C_{2}(\\tau)\\kappa_{2}^{2}(\\tau)T\\left((\\frac{\\bar{\\beta}}{\\delta^{2}}+\\frac{1}{\\tau})\\bar{\\gamma}_{K}^{1/2}+1\\right)\\bar{\\gamma}_{K}^{1/2}+\\frac{\\bar{D}}{\\sqrt{\\tau}}\\right),\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $\\kappa_{2}(\\tau)=T\\left(1/\\tau+\\bar{\\beta}/\\delta^{2}\\right)$ and $C_{2}(\\tau)$ is linear in $\\tau^{2}$ ", "page_idx": 25}, {"type": "text", "text": "Proof. To obtain the convergence guarantee, we need to control three error terms: ", "page_idx": 25}, {"type": "equation", "text": "$$\nW_{1}\\left(R_{K}^{q_{\\infty}^{\\tau}},q_{0}\\right)\\leq W_{1}\\left(R_{K}^{q_{\\infty}^{\\tau}},Q_{t_{K}}^{q_{\\infty}^{\\tau}}\\right)+W_{1}\\left(Q_{t_{K}}^{q_{\\infty}^{\\tau}},Q_{t_{K}}^{q_{0}P_{T}}\\right)+W_{1}\\left(Q_{t_{K}}^{q_{0}P_{T}},q_{0}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "For term $W_{1}\\left(R_{K}^{q_{\\infty}^{\\tau}},Q_{t_{K}}^{q_{\\infty}^{\\tau}}\\right)$ , we use Lemma D.6. ", "page_idx": 25}, {"type": "text", "text": "For the second em, wedefne Y,t)o,2 and Y).7 be the reverse processes with initial condition $x$ and $y$ . Then we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\|\\mathbf{Y}_{0,t}^{x}-\\mathbf{Y}_{0,t}^{y}\\|\\leq\\|x-y\\|\\int_{0}^{1}\\|\\nabla\\mathbf{Y}_{0,t}^{z_{\\lambda}}\\|d\\lambda\\,,\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $z_{\\lambda}=\\lambda x+(1-\\lambda)y$ In this work, we choose $x\\sim q_{\\infty}^{\\tau}$ and $y\\sim q_{0}P_{T}$ . Combined with the above inequality, Theorem 4.2 and Lemma 6.3, we know that: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{W_{1}\\left(Q_{t_{K}}^{q_{\\infty}^{\\tau}},Q_{t_{K}}^{q_{0}P_{T}}\\right)}\\\\ &{\\leq\\exp\\left[\\cfrac{R^{2}}{2\\sigma_{T-t_{K}}^{2}(i)}+\\cfrac{(1-\\eta^{2})}{2}\\int_{0}^{t_{K}}\\cfrac{\\beta_{T-u}}{\\tau}\\mathrm d u\\right]\\|q_{0}P_{T}-q_{\\infty}^{\\tau}\\|}\\\\ &{\\leq\\cfrac{\\sqrt{m_{T}}\\bar{D}}{\\sigma_{T}}\\exp\\left[\\cfrac{R^{2}}{2\\sigma_{T-t_{K}}^{2}(i)}+\\cfrac{(1-\\eta^{2})}{2}\\int_{0}^{t_{K}}\\cfrac{\\beta_{T-u}}{\\tau}\\mathrm d u\\right]\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "For thelast m, weue exactly the same proces withBortoli 222] with boud $\\sigma_{T-t_{K}}^{2}$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{W_{1}\\left(Q_{t_{K}}^{q_{0}P_{T}},q_{0}\\right)\\le\\mathbb{E}\\left[\\|X-m_{T-t_{K}}X+\\sigma_{T-t_{K}}Z\\|\\right]}\\\\ &{\\qquad\\qquad\\qquad\\le(\\frac{R}{\\tau}+\\sqrt{d})\\sigma_{T-t_{K}}}\\\\ &{\\qquad\\qquad\\qquad\\le2(\\frac{R}{\\tau}+\\sqrt{d})\\sqrt{\\delta}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the secdinequalityfolws ht $\\sigma_{T-t_{K}}^{2}+\\tau m_{T-t_{K}}=\\tau$ ", "page_idx": 26}, {"type": "text", "text": "In the end of the section, we provide the proof of Corollary 6.2. ", "page_idx": 26}, {"type": "text", "text": "Corollary D.7. Assume Assumption 3.1, 4.1 and $\\left\\|\\nabla^{2}\\log q_{t}\\left(x_{t}\\right)\\right\\|\\leq\\Gamma/\\sigma_{t}^{2}$ .Let $\\eta\\,=\\,0$ (reverse PFODE), $\\delta\\in(0,1/32),\\tau=T^{2}$ \uff0c $\\beta_{t}=t$ and $\\kappa_{2}(\\tau),C_{2}(\\tau)$ defined in Theorem 6.1, we have ", "page_idx": 26}, {"type": "equation", "text": "$$\nV_{1}\\left(R_{K}^{q_{\\infty}^{\\tau}},q_{0}\\right)\\leq(\\frac{R}{\\tau}+\\sqrt{d})\\sqrt{\\delta}+\\frac{\\bar{\\beta}^{\\frac{\\Gamma}{2}}}{\\delta^{\\Gamma}}\\exp\\left(\\frac{\\Gamma+2}{2}\\right)\\left(C_{2}(\\tau)\\kappa_{2}^{2}(\\tau)T((\\frac{\\bar{\\beta}}{\\delta^{2}}+\\frac{1}{\\tau})\\bar{\\gamma}_{K}^{1/2}+1)\\bar{\\gamma}_{K}^{1/2}+\\frac{\\bar{D}}{\\sqrt{\\delta}}\\right),\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. The proof of this corollary is almost identical to the proof of Theorem 6.1. We just need to replace the first bound for the tangent process in Lemma 6.3 by the second bound. ", "page_idx": 26}, {"type": "text", "text": "E  Lemmas for the Logarithmic Density ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In this section, we introduce auxiliary lemmas to control the gradient and Hessian of the logarithmic density under the manifold hypothesis. Lemma E.1, Lemma E.2 and Lemma E.3 come from Lemma C.1, Lemma C.2, and Lemma C.5 of Bortoli [2022]. Since these lemmas do not involve the relationshipbetween $m_{t}$ and $\\sigma_{t}$ , we can directly use the results from Bortoli [2022]. Following Bortoli [2022], we also define a empirical version of $q_{0}$ with $N$ datapoints, i.. $q_{0}^{N}=(1/N)\\textstyle\\sum_{k=1}^{N}X^{k}$ with $\\left\\{X^{k}\\right\\}_{k=1}^{N}\\sim q_{0}^{\\otimes N}$ . We denoteby $\\left(q_{t}^{N}\\right)_{t>0}$ such that fo any $t>0$ the density w.rt the Lbesgue measure of the distribution of $\\mathbf{X}_{t}^{N}$ , and when $N\\to+\\infty$ $q_{t}^{N}=q_{t}$ ", "page_idx": 26}, {"type": "text", "text": "Lemma E.1. Assume Assumption 4.1. Then for any $t\\in(0,T]$ and $\\boldsymbol{x}_{t}\\in\\mathbb{R}^{d}$ wehavethat ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\langle\\nabla\\log q_{t}(x_{t}),x_{t}\\rangle\\leq-\\|x_{t}\\|^{2}/\\sigma_{t}^{2}+m_{R}\\|x_{t}\\|/\\sigma_{t}^{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "In addition, we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla\\log q_{t}\\left(x_{t}\\right)\\|^{2}\\leq2\\|x_{t}\\|^{2}/\\sigma_{t}^{4}+2m_{t}^{2}R^{2}/\\sigma_{t}^{4}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Lemma E.2. Assume Assumption 4.1. Then for any $t\\in(0,T],\\,x_{t}\\in\\mathbb{R}^{d}$ and $M\\in\\mathcal{M}_{d}\\left(\\mathbb{R}^{d}\\right)$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left\\langle M,\\nabla^{2}\\log q_{t}\\left(x_{t}\\right)M\\right\\rangle\\leq-\\left(1-m_{t}^{2}R^{2}/\\left(2\\sigma_{t}^{2}\\right)\\right)/\\sigma_{t}^{2}\\|M\\|^{2}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "In addition, we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left\\|\\nabla^{2}\\log q_{t}\\left(x_{t}\\right)\\right\\|\\leq\\left(1+R^{2}\\right)/\\sigma_{t}^{4}\\,.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The following lemma shows that the derivatives up to the fourth order are uniformly bounded since $\\tau\\in[T,T^{2}]$ . Thus we can use the stochastic extension of the Alekseev-Grobner formula [Del Moral and Singh, 2022]. ", "page_idx": 26}, {"type": "text", "text": "Lemma E.3. Assume Assumption 4.1. Then, there exists $\\bar{C}\\geq0$ such that for any $t\\in(0,T]$ wehave ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left\\|\\nabla^{2}\\log q_{t}(x)\\right\\|+\\left\\|\\nabla^{3}\\log q_{t}(x)\\right\\|+\\left\\|\\nabla^{4}\\log q_{t}(x)\\right\\|\\leq\\bar{C}/\\sigma_{t}^{8}\\,.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The following lemma shows that $\\|\\partial_{t}\\nabla\\log q_{t}\\left(x_{t}\\right)\\|$ is bounded. The proof before using the relationshipbetween $\\sigma_{t}$ and $m_{t}$ is identical compared to Lemma C.3 in Bortoli [2022]. For the sake of completeness, we also give the proof process of this part. ", "page_idx": 26}, {"type": "text", "text": "Lemma E.4. Assume Assumption 4.1. Then for any $t\\in(0,T]$ and $\\boldsymbol{x}_{t}\\in\\mathbb{R}^{d}$ wehave ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\partial_{t}\\nabla\\log q_{t}\\left(x_{t}\\right)\\|\\leq\\left(\\beta_{t}/\\sigma_{t}^{6}\\right)\\left(2+R^{2}\\right)\\left(R+\\|x_{t}\\|\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. Let $N\\in\\mathbb{N}$ and $t\\in(0,T]$ . We denote for any $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ $\\mathit{\\Pi}:\\mathbb{R}^{d},\\mathit{q}_{t}^{N}\\left(x\\right)=\\bar{q}_{t}^{N}\\left(x\\right)/\\left(2\\pi\\sigma_{t}^{2}\\right)^{d/2}$ with ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\bar{q}_{t}^{N}\\left(x\\right)=(1/N)\\sum_{k=1}^{N}e_{t}^{k}\\left(x\\right),\\qquad e_{t}^{k}(x)=\\exp\\left[-\\|x-m_{t}X^{k}\\|^{2}/\\left(2\\sigma_{t}^{2}\\right)\\right].\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Next we denote $f_{t}^{k}\\triangleq\\log{e_{t}^{k}}$ . Then we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\partial_{t}\\log\\bar{q}_{t}^{N}\\left(x\\right)\\sum_{k=1}^{N}\\partial_{t}f_{t}^{k}\\left(x\\right)e_{t}^{k}\\left(x\\right)/\\sum_{k=1}^{N}e_{t}^{k}\\left(x\\right).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Therefore we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\partial_{t}\\nabla\\log\\bar{q}_{t}^{N}(x)}\\\\ &{=\\displaystyle\\sum_{k=1}^{N}\\partial_{t}\\nabla f_{t}^{k}(x)e_{t}^{k}(x)/\\sum_{k=1}^{N}e_{t}^{k}(x)+\\displaystyle\\sum_{k=1}^{N}\\partial_{t}f_{t}^{k}(x)\\nabla f_{t}^{k}(x)e_{t}^{k}(x)/\\sum_{k=1}^{N}e_{t}^{k}(x)}\\\\ &{\\qquad-\\displaystyle\\sum_{k,j=1}^{N}\\partial_{t}f_{t}^{k}(x)\\nabla f_{t}^{j}(x)e_{t}^{k}(x)e_{t}^{j}(x)/\\sum_{k,j=1}^{N}e_{t}^{k}(x)e_{t}^{j}(x)}\\\\ &{=\\displaystyle\\sum_{k=1}^{N}\\partial_{t}\\nabla f_{t}^{k}(x)e_{t}^{k}(x)/\\sum_{k=1}^{N}e_{t}^{k}(x)}\\\\ &{\\qquad+\\displaystyle(1/2)\\sum_{k,j=1}^{N}\\left(\\partial_{t}f_{t}^{k}(x)-\\partial_{t}f_{t}^{j}(x)\\right)\\left(\\nabla f_{t}^{k}(x)-\\nabla f_{t}^{j}(x)\\right)e_{t}^{k}(x)e_{t}^{j}(x)/\\sum_{k=1}^{N}e_{t}^{k}(x)e_{t}^{j}(x).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "In what follows, we provide upper bounds for $\\lvert\\partial_{t}f_{t}^{k}-\\partial_{t}f_{t}^{j}\\rvert,\\lvert\\lvert\\nabla f_{t}^{k}-\\nabla f_{t}^{j}\\rvert\\rvert$ and $\\partial_{t}\\nabla f_{t}^{k}$ . First we notice that $\\nabla f_{t}^{k}(x)=-\\left(x-m_{t}X^{k}\\right)/\\sigma_{t}^{2}$ and using $m_{t}\\leq1$ weget ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\|\\nabla f_{t}^{k}(x)-\\nabla f_{t}^{j}(x)\\|\\le m_{R}/\\sigma_{t}^{2}\\le R/\\sigma_{t}^{2}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\partial_{t}f_{t}^{k}\\left(t\\right)=\\partial_{t}\\sigma_{t}^{2}/\\left(2\\sigma_{t}^{4}\\right)\\|x-m_{t}X^{k}\\|^{2}+\\partial_{t}m_{t}/\\sigma_{t}^{2}\\left\\langle X^{k},x-m_{t}X^{k}\\right\\rangle.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Notice the fact that $\\partial_{t}\\sigma_{t}^{2}=-2\\tau m_{t}\\partial_{t}m_{t}=2\\beta_{t}m_{t}^{2}$ and $\\partial_{t}m_{t}=-\\frac{\\beta_{t}}{\\tau}m_{t}$ , combined with the above equality, we know that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\partial_{t}f_{t}^{k}\\left(t\\right)=-\\beta_{t}m_{t}/\\sigma_{t}^{2}\\left[-\\left(m_{t}/\\sigma_{t}^{2}\\right)\\left\\Vert x-m_{t}X^{k}\\right\\Vert^{2}+\\displaystyle\\frac{1}{\\tau}\\left\\langle x-m_{t}X^{k},X^{k}\\right\\rangle\\right]}}\\\\ {{\\phantom{\\partial_{t}f_{t}^{k}}=-\\beta_{t}m_{t}/\\sigma_{t}^{2}\\left\\langle x-m_{t}X^{k},-\\left(m_{t}/\\sigma_{t}^{2}\\right)\\left(x-m_{t}X^{k}\\right)+\\displaystyle\\frac{1}{\\tau}X^{k}\\right\\rangle}}\\\\ {{\\phantom{\\partial_{t}f_{t}^{k}}=-\\beta_{t}m_{t}/\\sigma_{t}^{4}\\left\\langle x-m_{t}X^{k},-m_{t}x+\\left(m_{t}^{2}+\\displaystyle\\frac{\\sigma_{t}^{2}}{\\tau}\\right)X^{k}\\right\\rangle}}\\\\ {{\\phantom{\\partial_{t}f_{t}^{k}}=\\beta_{t}m_{t}/\\sigma_{t}^{4}\\left(m_{t}\\left\\Vert x\\right\\Vert^{2}+m_{t}\\left\\Vert X^{k}\\right\\Vert^{2}+\\left(1+m_{t}^{2}\\right)\\left\\langle x,X^{k}\\right\\rangle\\right)\\,,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where the last equality holds that $\\tau m_{t}^{2}+\\sigma_{t}^{2}=\\tau$ . The rest of the proof is identical to the Lemma C.3 in Bortoli [2022]. ", "page_idx": 27}, {"type": "text", "text": "So using $m_{t}\\le1$ we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left|\\partial_{t}f_{t}^{k}(x)-\\partial_{t}f_{t}^{j}(x)\\right|\\leq2\\beta_{t}m_{t}^{2}R^{2}/\\sigma_{t}^{4}+\\beta_{t}m_{t}\\left(1+m_{t}^{2}\\right)R\\|x\\|/\\sigma_{t}^{4}}&{{}}\\\\ {\\leq2\\left(\\beta_{t}/\\sigma_{t}^{4}\\right)R(R+\\|x\\|)}&{{}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Nowwecompute $\\nabla\\partial_{t}f_{t}^{k}\\left(x\\right)$ forany $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\nabla\\partial_{t}f_{t}^{k}\\left(x\\right)=2\\beta_{t}m_{t}^{2}/\\sigma_{t}^{4}x+\\left(\\beta_{t}m_{t}/\\sigma_{t}^{4}\\right)\\left(1+m_{t}^{2}\\right)X^{k}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "So we can bound the norm of it by ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\partial_{t}\\nabla f_{t}^{k}\\left(x\\right)\\|\\leq2\\left(\\beta_{t}/\\sigma_{t}^{4}\\right)\\left(R+\\|x\\|\\right)\\!.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Combining results above we get for any $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\partial_{t}\\nabla\\log\\bar{q}_{t}^{N}(x)\\right\\|\\leq2\\left(\\beta_{t}/\\sigma_{t}^{4}\\right)\\left(R+\\|x\\|\\right)+\\left(\\beta_{t}/\\sigma_{t}^{6}\\right)R^{2}(R+\\|x\\|)}\\\\ &{\\qquad\\qquad\\qquad\\leq\\left(\\beta_{t}/\\sigma_{t}^{6}\\right)\\left(2+R^{2}\\right)\\left(R+\\|x\\|\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Note that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{N\\to+\\infty}\\partial_{t}\\nabla\\log q_{t}^{N}\\left(x_{t}\\right)=\\partial_{t}\\nabla\\log q_{t}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "and the proof is complete. ", "page_idx": 28}, {"type": "text", "text": "In the following lemma, similar to Chen et al. [2023c], we obtain a better control on the time discretization error instead of controlling $\\|\\partial_{t}\\nabla\\log q_{t}\\left(x_{t}\\right)\\|$ for $\\forall x_{t}\\in\\mathbb{R}^{d}$ ", "page_idx": 28}, {"type": "text", "text": "Lemma E.5. Assume Assumption 4.1 and $X_{t}$ satisfies the forward process Equation (3). Define $\\begin{array}{r}{L=\\operatorname*{max}_{t\\in[0,T-\\delta]}\\left\\Vert\\nabla^{2}\\log q\\dot{T}_{-t}\\left(\\mathbf{Y}_{t}\\right)\\right\\Vert\\leq\\left(1+R^{2}\\right)^{\\ast}\\!/\\sigma_{\\delta}^{4}}\\end{array}$ then we have that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{Q_{t_{K}}^{q_{T}^{\\tau}}}\\left[\\left\\|\\nabla\\ln\\frac{q_{T-t_{k}}}{q_{T-t}}\\left(\\mathbf{Y}_{t_{k}}\\right)\\right\\|^{2}\\right]}\\\\ &{\\lesssim\\tau L^{2}d\\Bar{\\gamma}_{K}+\\tau L^{2}\\Bar{\\gamma}_{K}^{2}(d\\tau+R^{2})+\\tau L^{3}\\Bar{\\gamma}_{K}^{2}+\\tau L^{4}\\Bar{\\gamma}_{K}^{2}(\\beta_{T}d\\Bar{\\gamma}_{K}+R^{2}\\Bar{\\gamma}_{K}^{2})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. Due to the property of the forward process, we know that if $S:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{d}$ is the mapping $S(x):=\\exp(-(t-t_{k}))x$ ,then $\\begin{array}{r}{q_{T-t_{k}}=S_{\\#}q_{T-t}*\\mathrm{normal}\\left(0,\\tau\\left(1-\\exp(-2\\int_{t_{k}}^{t_{k}+1}\\beta_{s}/\\tau\\mathrm{d}s)\\right)\\right)}\\end{array}$ Similar to Chen et al. [2023c], we define $\\begin{array}{r}{\\alpha\\ =\\ \\exp\\left[\\int_{t_{k}}^{t_{k+1}}\\frac{\\beta_{s}}{\\tau}\\mathrm{d}s\\right]\\ =\\ 1+{\\cal O}(\\bar{\\gamma}_{K})}\\end{array}$ and $\\sigma^{2}\\mathbf{\\Sigma}=\\mathbf{\\Sigma}$ $\\begin{array}{r}{\\tau\\left(1-\\exp(-2\\int_{t_{k}}^{t_{k}+1}\\beta_{s}/\\tau\\mathrm d s)\\right)=O(\\tau\\bar{\\gamma}_{K})}\\end{array}$ Then we can use LemmaC.12ofLee etal [20to obtain ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{Q_{t_{K}}^{q_{T}^{\\tau}}}\\left[\\left\\|\\nabla\\ln\\frac{q_{T-t_{k}}}{q_{T-t}}\\left(\\mathbf{Y}_{t_{k}}\\right)\\right\\|^{2}\\right]}\\\\ &{\\lesssim\\tau L^{2}d\\bar{\\gamma}_{K}+\\tau L^{2}\\bar{\\gamma}_{K}^{2}\\left\\|\\mathbf{Y}_{t_{k}}\\right\\|^{2}+\\tau L^{2}\\bar{\\gamma}_{K}^{2}\\left\\|\\nabla\\ln q_{T-t}\\left(\\mathbf{Y}_{t_{k}}\\right)\\right\\|^{2}}\\\\ &{\\lesssim\\tau L^{2}d\\bar{\\gamma}_{K}+\\tau L^{2}\\bar{\\gamma}_{K}^{2}(d\\tau+R^{2})+\\tau L^{3}\\bar{\\gamma}_{K}^{2}+\\tau L^{4}\\bar{\\gamma}_{K}^{2}(\\beta_{T}d\\bar{\\gamma}_{K}+R^{2}\\bar{\\gamma}_{K}^{2})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "The last inequality follows Lemma F.4 and the fact that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla\\ln q_{T-t}\\left(\\mathbf{Y}_{t_{k}}\\right)\\|^{2}\\lesssim\\|\\nabla\\ln q_{T-t}\\left(\\mathbf{Y}_{t}\\right)\\|^{2}+\\|\\nabla\\ln q_{T-t}\\left(\\mathbf{Y}_{t_{k}}\\right)-\\nabla\\ln q_{T-t}\\left(\\mathbf{Y}_{t}\\right)\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\lesssim\\|\\nabla\\ln q_{T-t}\\left(\\mathbf{Y}_{t_{k}}\\right)\\|^{2}+L^{2}(\\beta_{T}d\\Bar{\\gamma}_{K}+R^{2}\\Bar{\\gamma}_{K}^{2})}\\\\ &{\\qquad\\qquad\\qquad\\lesssim L+L^{2}(\\beta_{T}d\\Bar{\\gamma}_{K}+R^{2}\\Bar{\\gamma}_{K}^{2})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "F Auxiliary Lemmas ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Lemma F.1. For any $s,t\\in[0,T]$ we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\int_{s}^{t}\\beta_{T-u}/\\sigma_{T-u}^{2}\\mathrm{d}u=\\left[-\\frac{1}{2}\\log\\left(\\exp\\left[2\\int_{0}^{T-u}\\frac{\\beta_{v}}{\\tau}\\mathrm{d}v\\right]-1\\right)\\right]_{s}^{t},}\\\\ {\\int_{s}^{t}\\beta_{T-u}m_{T-u}^{2}/\\sigma_{T-u}^{4}\\mathrm{d}u=\\left[\\left(1/2\\tau\\right)/\\left(1-\\exp\\left[-2\\int_{0}^{T-u}\\frac{\\beta_{v}}{\\tau}\\mathrm{d}v\\right]\\right)\\right]_{s}^{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. We directly compute ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\int_{s}^{t}\\beta_{T-u}/\\sigma_{T-u}^{2}\\mathrm{d}u=\\frac{1}{\\tau}\\int_{s}^{t}\\beta_{T-u}/\\left(1-\\exp\\left[-2\\int_{0}^{T-u}\\frac{\\beta_{v}}{\\tau}\\mathrm{d}v\\right]\\right)\\mathrm{d}u}\\\\ &{\\qquad\\qquad\\qquad=\\frac{1}{\\tau}\\int_{s}^{t}\\beta_{T-u}\\exp\\left[2\\int_{0}^{T-u}\\frac{\\beta_{v}}{\\tau}\\mathrm{d}v\\right]/\\left(\\exp\\left[2\\int_{0}^{T_{u}}\\frac{\\beta_{v}}{\\tau}\\mathrm{d}v\\right]-1\\right)\\mathrm{d}u}\\\\ &{\\qquad\\qquad=-\\frac{1}{2}\\int_{s}^{t}\\partial_{u}\\log\\left(\\exp\\left[2\\int_{0}^{T-u}\\frac{\\beta_{v}}{\\tau}\\mathrm{d}v\\right]-1\\right)\\mathrm{d}u\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Similarly ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{l l l}{\\displaystyle\\int_{s}^{t}\\beta_{T-u}m_{T-u}^{2}/\\sigma_{T-u}^{4}}\\\\ {\\displaystyle}&{=\\displaystyle\\frac{1}{\\tau^{2}}\\int_{s}^{t}\\beta_{T-u}\\exp\\left[-2\\int_{0}^{T-u}\\frac{\\beta_{v}}{\\tau}\\mathrm{d}v\\right]/\\left(1-\\exp\\left[-2\\int_{0}^{T-u}\\frac{\\beta_{v}}{\\tau}\\mathrm{d}v\\right]\\right)^{2}\\mathrm{d}u}\\\\ {\\displaystyle}&{=\\left(1/2\\tau\\right)\\int_{t}^{s}\\partial_{u}\\left(1-\\exp\\left[-2\\int_{0}^{T-u}\\frac{\\beta_{v}}{\\tau}\\mathrm{d}v\\right]\\right)^{-1}\\mathrm{d}u.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Lemma F.2. Assume Assumption 3.1. For $i\\,\\in\\,\\{1,2\\}$ ,we have $\\sigma_{T-t_{K}}^{2}(i)\\,\\leq\\,2\\delta$ and $\\sigma_{u}^{-2}(i)\\leq$ $\\sigma_{T-t_{K}}^{-2}(i)\\leq\\frac{1}{\\tau}+\\frac{\\bar{\\beta}}{\\delta^{4-i}},\\forall u\\in[T-t_{K},T].$ ", "page_idx": 29}, {"type": "text", "text": "Proof. ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sigma_{T-t_{K}}^{2}(i)=\\tau\\left(1-\\exp\\left[-2\\int_{0}^{T-t_{K}}\\frac{\\beta_{s}}{\\tau}\\,\\mathrm{d}s\\right]\\right)}\\\\ {\\displaystyle\\leq2\\int_{0}^{T-t_{K}}\\beta_{s}\\,\\mathrm{d}s\\leq2\\delta\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where the first inequality follows from for any $a\\geq0,\\exp[-a]\\geq1-a$ the second inequlity follows from Assumption 3.1 and $\\delta\\leq1$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\sigma_{T-t_{K}}^{-2}(i)=\\displaystyle\\frac{1}{\\tau}\\left(1-\\exp\\left[-2\\int_{0}^{T-t_{K}}\\frac{\\beta_{s}}{\\tau}\\,\\mathrm{d}s\\right]\\right)^{-1}\\leq\\displaystyle\\frac{1}{\\tau}\\left(1+\\left(2\\int_{0}^{T-t_{K}}\\frac{\\beta_{s}}{\\tau}\\,\\mathrm{d}s\\right)^{-1}\\right)}&{}\\\\ &{\\leq\\displaystyle\\frac{1}{\\tau}+\\frac{\\bar{\\beta}}{\\delta^{4-i}}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where the frst inequality follows from for any $a\\geq0,1/(1\\!+\\!\\exp[-a])\\leq1\\!+\\!1/a,$ the second inequality follows from Assumption 3.1. It is easy to check that $\\sigma_{u}^{-2}(i)\\leq\\sigma_{T-t_{K}}^{-2}(i),\\forall u\\in[T-t_{K},T]$ ", "page_idx": 29}, {"type": "text", "text": "Using the bound on $\\sigma_{T-t_{K}}^{-2}(i)$ immediatelyyields the follwing controlof $\\beta_{u}/\\sigma_{u}^{2}(i)$ ", "page_idx": 29}, {"type": "text", "text": "Lemma F.3. Assume Assumption 3.1. Then, we have for any $u\\in[T-t_{K},T]$ $(l)\\,i f{i=1}$ then ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\frac{\\beta_{u}}{\\sigma_{u}^{2}(i=1)}\\le\\kappa_{1}(\\tau)=\\operatorname*{max}\\{\\bar{\\beta},T^{2}\\}\\left(\\frac{1}{\\tau}+\\frac{\\bar{\\beta}}{\\delta^{3}}\\right)\\,;\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "(2) if $i=2$ then ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\frac{\\beta_{u}}{\\sigma_{u}^{2}(i=2)}\\le\\kappa_{2}(\\tau)=\\operatorname*{max}\\{\\bar{\\beta},T\\}\\left(\\frac{1}{\\tau}+\\frac{\\bar{\\beta}}{\\delta^{2}}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Generally speaking, $T\\geq\\bar{\\beta}\\geq1$ . Hence, We can further simplify the above inequality by removing max. ", "page_idx": 30}, {"type": "text", "text": "In the rest of this section, we provide the useful lemma to achieve polynomial sample complexity for VE-based models with reverse SDE. As shown in Lemma E.1, we also need to control $\\mathbb{E}[\\|\\mathbf{X}_{t}\\|^{2}]$ in the forward process. The following lemmas shows that this term is bounded by the $R^{2}$ andexploding variance. ", "page_idx": 30}, {"type": "text", "text": "Lemma F4. Suppose that Asumption4.1hol. Lt $(\\mathbf{X}_{t})_{t\\in[0,T]}$ denote the forward process Equation (3). Then, for all $t\\geq0$ ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\lVert\\mathbf{X}_{t}\\rVert^{2}\\right]\\leq d\\sigma_{t}^{2}\\vee R^{2}\\,.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof. As shown in Equation (4), ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\|\\mathbf{X}_{t}\\right\\|^{2}\\right]\\leq\\mathbb{E}\\left[\\left\\|\\mathbf{X}_{0}\\right\\|^{2}\\right]+\\sigma_{t}^{2}d\\leq d\\sigma_{t}^{2}\\vee R^{2}\\,.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Lemma F5 (movement bound for VESDE). Let $(\\mathbf{X}_{t})_{t\\in[0,T]}$ denote the frward pocessEquation (3). For $0\\leq s<t$ with $\\delta:=t-s,$ if $\\delta\\leq1$ then ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\|\\mathbf{X}_{t}-\\mathbf{X}_{s}\\right\\|^{2}\\right]\\lesssim2\\beta_{t}\\delta d+\\delta^{2}R^{2}\\,.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof. ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\Vert\\mathbf{X}_{t}-\\mathbf{X}_{s}\\right\\Vert^{2}\\right]\\lesssim\\mathbb{E}\\left[\\left\\Vert\\sqrt{2\\beta_{t}}\\left(B_{t}-B_{s}\\right)\\right\\Vert^{2}\\right]+\\delta\\int_{s}^{t}\\mathbb{E}\\left[\\left\\Vert\\mathbf{X}_{r}\\right\\Vert^{2}\\right]\\mathrm{d}r\\lesssim2\\beta_{t}\\delta d+\\delta^{2}R^{2}\\,.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Similar to Chen et al. [2023c], we can also show that if we do forward process for time $\\delta,\\,q\\delta$ will be closeto $q_{0}$ in $W_{2}$ distance. ", "page_idx": 30}, {"type": "text", "text": "Lemma F.6. Suppose Assumption 4.1 holds. Let $\\epsilon_{W_{2}}\\,>\\,0$ f $\\beta_{t}^{2}\\,=\\,t^{2}$ and $\\tau=T^{2}$ ,wechoose the early stopping parameter $\\begin{array}{r}{\\delta\\le\\frac{\\epsilon_{W_{2}}^{2/3}}{(d+R\\sqrt{d})^{1/3}}}\\end{array}$ f $\\beta_{t}=t$ and $\\tau=T$ we choose $\\begin{array}{r}{\\delta\\,\\le\\,\\frac{\\epsilon_{W_{2}}}{(d+R\\sqrt{d})^{1/2}}}\\end{array}$ Ifcnsider pwre VESDE SMLD) Equarion (2) with $\\sigma_{t}^{2}=t$ we choose $\\begin{array}{r}{\\delta\\leq\\frac{\\epsilon_{W_{2}}^{2}}{d}}\\end{array}$ Then we have ", "page_idx": 30}, {"type": "text", "text": "Proof. For the forward process Equation (3), we know that $\\mathbf{X}_{t}\\,:=\\,m_{t}\\mathbf{X}_{0}+\\sigma_{t}Z$ ,where $Z\\sim$ normal $(0,I_{d})$ is independent of $X_{0}$ and $m_{t}\\leq1$ . Hence, for $\\delta\\lesssim1$ ", "page_idx": 30}, {"type": "equation", "text": "$$\nW_{2}^{2}\\left(q_{0},q_{\\delta}\\right)\\leq(1-m_{t})^{2}\\mathbb{E}\\left[\\left\\|\\mathbf{X}_{0}\\right\\|^{2}\\right]+\\mathbb{E}\\left[\\left\\|\\sigma_{\\delta}Z\\right\\|^{2}\\right]\\,.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "For $\\beta_{t}=t^{2}$ and $\\tau=T^{2}$ , we have that ", "page_idx": 30}, {"type": "equation", "text": "$$\nW_{2}^{2}\\left(q_{0},q_{\\delta}\\right)\\leq\\delta^{3}d+\\frac{R^{2}\\delta^{6}}{T^{2}}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Hence, we can take $\\begin{array}{r}{\\delta\\le\\frac{\\epsilon_{W_{2}}^{2/3}}{(d+R\\sqrt{d})^{1/3}}}\\end{array}$ (d+Va)a For \u03b2 = t and T =T, we have that ", "page_idx": 30}, {"type": "equation", "text": "$$\nW_{2}^{2}\\left(q_{0},q_{\\delta}\\right)\\leq\\delta^{2}d+\\frac{R^{2}\\delta^{4}}{T^{2}}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Hence, we can take (d+RVa)12 For pure VESDE (Equation 2) with ot = t, we have ", "page_idx": 30}, {"type": "equation", "text": "$$\nW_{2}^{2}\\left(q_{0},q_{\\delta}\\right)\\leq\\delta d\\,.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "G   Additional Synthetic Experiments ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "In this section, we do synthetic experiments to show the power of our new forward process with small drift term in different setting. ", "page_idx": 31}, {"type": "text", "text": "G.1 The Synthetic experiments with accurate score function ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "In this section, we do numerical experiments on 2-dimension Gaussian distribution to show the power of our new VESDE forward process in balancing different error sources. ", "page_idx": 31}, {"type": "text", "text": "Experiment Setting.  We set the mean of target distribution $\\mathbb{E}[q_{0}]=[6,8]$ , the covariance matrix $\\mathrm{Cov}[q_{0}]=\\left[\\!\\!\\begin{array}{c c}{{25}}&{{5}}\\\\ {{5}}&{{4}}\\end{array}\\!\\!\\right]$ th difusion time $T=2$ $\\tau=T^{2}$ and the reverse begining istribtion is $\\mathcal{N}(0,T^{2}\\mathbf{I})$ Wechooseuniform stepsize $\\gamma_{k}=h,\\forall k\\in[K]$ where $h\\in\\{0.005,0.01,0.02,0.04\\}$ For score functions, we directly calculate the ground truth score function instead of learning it by the score matching objective. We calculate the KL divergence between the generation distribution and target distribution $q_{0}$ as the experiments. ", "page_idx": 31}, {"type": "text", "text": "The implementable algorithm. We choose three different VESDE forward processes in the experiments: (1) aggressive $\\beta_{t}=t^{2}$ with $\\tau=T^{2}$ ; (2) conservative $\\beta_{t}=t$ with $\\tau=T^{2}$ and (3) VESDE without drift term Equation (2) with $\\sigma_{t}^{2}=t^{2}$ . After determining the forward process, we run the reverse SDE with the above $\\gamma_{k},k\\,\\in\\,[K]$ . For the discretization scheme, we choose two common method: exponential integrator (EI) [Zhang and Chen, 2022] and Euler-Maruyama (EM) discretization [Ho et al., 2020]. ", "page_idx": 31}, {"type": "text", "text": "Observations. The experimental results are shown in Figure 2. We note that the red line (EI, VESDE without drift, $\\sigma_{t}^{2^{\\perp}}\\!=t^{2}$ ) and orange line (conservative drift VESDE, $\\beta_{t}=t$ and $\\tau=T^{2}$ has a similar trend. Furthermore, the conservative drift VESDE has better performance compared to pure VESDE without drift term. Hence, our new forward process is representative enough to represent current VESDE, as discussed in Section 3.1. ", "page_idx": 31}, {"type": "text", "text": "The experimental results also support our theoretical results and show the power of the new forward process in balancing different error terms. As shown in Figure 2, the process with aggressive $\\beta_{t}=t^{2}$ with small drift term achieves the best and second performance in EI and EM discretization since it can balance the reverse beginning and discretization. The third best process is conservative $\\beta_{t}=t$ with the small drift term. The reason is that though it can not achieve a $\\exp\\left(-T\\right)$ forwardprocess guarantee, it also has a constant decay on prior information, as shown in Section 3.1. This decay slightly reduces the effect of the reverse beginning error. The worse process is VESDE without drift term since it is hard to balance different error sources. Our experimental results also show that EI discretization is better than EM discretization. ", "page_idx": 31}, {"type": "text", "text": "G.2  The Synthetic experiments with approximated score function ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "In this section, instead of using an accurate score function, we train an approximated score function on the pure VESDE (Equation (2)) without drift term on two synthetic datasets: multiple Swiss rolls and 1-D GMM. Then, for the drift VESDE, we do not train the approximated score corresponding to Equation (3); we directly use the approximated score learned by pure VESDE and show that the drift VESDE can improve the generated distribution without the training process. ", "page_idx": 31}, {"type": "text", "text": "Datasets.  The 1-D GMM distribution contains three modes: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\frac{3}{10}{\\mathcal N}\\left(-8,0.01\\right)+\\frac{3}{10}{\\mathcal N}\\left(-4,0.01\\right)+\\frac{4}{10}{\\mathcal N}\\left(3,1\\right)\\,.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "For multiple Swiss rolls, we use a similar code compared to Listing 2 of Lai et al. [2023], except Line 6. We change Line 6. to data $/{=}10$ . to obtain a larger variance dataset. Each dataset contains 50000datapoints. ", "page_idx": 31}, {"type": "text", "text": "The implementable algorithm. In this subsection, we choose two forward processes: (1) conservative $\\beta_{t}=1$ With $\\tau=T$ ; (2) pure VESDE without drift term (Equation (2) with $\\sigma_{t}^{2}=t$ .Tomatch our analysis, we choose two sampling methods for the reverse process: Euler-Maruyama method for reverse SDE and RK45 ODE solver for the reverse PFODE method. ", "page_idx": 31}, {"type": "table", "img_path": "euQ0C4iS7O/tmp/630b78b9fb662bcc18e954cbf6d248e093d6bbcac639a83c7c6391ab238ec253.jpg", "table_caption": ["Table 1: The KL divergence for pure VESDE (Equation (2)) and conservative drift VESDE with different sampling method. "], "table_footnote": [], "page_idx": 32}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "We note that although aggressive setting $\\beta_{t}=t$ and $\\tau=T$ has shown its power in theory (Lemma C.2) and the experiments with accurate score (Figure 2), other sampling issues may arise in practice. We leave the experimental exploration for drift VESDE with aggressive $\\beta_{t}$ as a futurework. ", "page_idx": 32}, {"type": "text", "text": "The training detail. For each dataset, we train a score function with pure VESDE (Equation (2), $\\sigma_{t}^{2}=t)$ . We train for 200 epochs with batch size 200 and learning rate $\\mathrm{\\dot{1}0^{-4}}$ . For both training and inference, the start time is $\\bar{\\delta\\,}{=}\\,10^{-5}$ . For the conservative VESDE, we directly adapt the checkpoint learned by the pure VESDE since the conservative drift VESDE has a similar trend compared to pure VESDE, as shown in Figure 2. The above experiments are runned over 5 random seed and we present the average over these seeds in Table 1. ", "page_idx": 32}, {"type": "text", "text": "The above experiments are conduct on a GeForce RTX 4090. It takes 25 minutes to train a score function of pure VESDE. ", "page_idx": 32}, {"type": "text", "text": "Observation. We do experiments with $T=100$ and lager $T=625$ and these two choice show similar phenomenon. In this paragraph, we first use $T=100$ as an example to discuss the results. As shown in Table 1, the conservative drift VESDE has smaller KL divergence compared to pure VESDE under all sampling methods and datasets. From Figure 1 and Figure 4, it is clear that pure VESDE has low density on the Swiss roll except the center one, which means that though pure VESDE can deal with small $\\mathbb{E}[q_{0}]$ , it is hard to deal with large dataset variance $\\mathrm{Cov}[q_{0}]$ , as we discuss in Section 4. For conservative drift VESDE ( $\\mathit{\\beta}[\\beta_{t}=1$ and $\\tau=T$ ), as we discuss in Section 3.1, there is a constant decay on the prior information $\\mathbb{E}[q_{0}]$ and $\\mathrm{Cov}[q_{0}]$ , which is helpful in deal with large dataset mean and variance. The experimental results support our augmentation. Figure 1 (c), Figure 4 (c) and Figure 5 (c) show that the density of the generated distribution is more uniform compared to pure VESDE, which means that the drift VESDE can deal with large dataset mean and variance. ", "page_idx": 32}, {"type": "text", "text": "We also do experiments with larger $T=625$ . As we discuss in Section 4, larger $T$ will reducethe influence of the prior data information and have greater generated distribution, as shown in Figure 4 (c) and Figure 4 (e). The experiments of 1D-GMM (Figure 5) show a similar phenomenon compared to the multi Swiss rolls. ", "page_idx": 32}, {"type": "text", "text": "G.3The Real-World Experiments on CelebA 256 ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "After achieving great performance under the synthetic data, we show that our conservative drifted VESDE can improve the results of pure VESDE without training. ", "page_idx": 32}, {"type": "text", "text": "Setting. In this experiment, we adapt well-known VESDE implementation [Song et al., 2020b] and do experiments on CelebA datasets (size: $256\\,*\\,256\\,*\\,3)$ .More specifically, we use ve/celebahq_256_ncsnpp_continuous checkpoints provided by [Song et al., 2020b] and modify the sampling process strictly according to our drifted VESDE. To do a fair comparsion, we fix the random seed and use the reverse PFODE process. Then, we generate 10000 face images to calculate the metrics. We note that when using this checkpoint and pure VESDE pipeline provided by [Song et al., 2020b], the models would generate almost pure noise with a certain probability. Hence, we use an aesthetic predictor [Schuhmann et al., 2022] (aesthetic score $\\geq5.5$ ) to filter the generated images to ensure that the images are clear faces. ", "page_idx": 32}, {"type": "image", "img_path": "euQ0C4iS7O/tmp/8940b874bb37f20c5235f1c055856b326483278c8586e3b96c0213d873387081.jpg", "img_caption": ["Figure 4: Experiment results of Swiss roll with reverse PFODE "], "img_footnote": [], "page_idx": 33}, {"type": "image", "img_path": "euQ0C4iS7O/tmp/6b04a518c51c0e69383ade7f66d4e2b1eb74db0e1310888a07e0611617fe42bb.jpg", "img_caption": ["Figure 5: Experiment results of 1D-GMM with reverse PFODE "], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "Discussion.  From the qualitative perspective, as shown in Figure 3 (Figure 6 and 7), the images generated by our drifted VESDE have more detail (such as hair and beard details). On the contrary, since pure VESDE can not deal with large variance, the images generated by pure VESDE appear blurry and unrealistic in these details. From the quantitative results, our drifted VESDE achieves aesthetic score 5.813, and IS 4.174, which is better than the results of baseline pure VESDE (aesthetic score 5.807 and IS: 4.082). In conclusion, the real-world experiments show the potential of our driftedVESDE. ", "page_idx": 33}, {"type": "text", "text": "We note that the goal of these experiments is to show that our conservative drifted VESDE is plugand-play without training instead of achieving a SOTA performance. Hence, we focus on the relative improvement compared to the baseline [Song et al., 2020b]. There are two interesting empirical future works. For the conservative drifted VESDE, we will do experiments on the SOTA pure VESDE models [Karras et al., 2022] and improve their results without training. For the aggressive drifted VESDE, since this process makes a larger modification compared with the conservative one, we need to train a new score function instead of directly using a pre-train one to achieve better results. ", "page_idx": 33}, {"type": "image", "img_path": "euQ0C4iS7O/tmp/6d49e8e3c74d6773c7d36830fe05aebc8147794faa58a55a26d217180613c66d.jpg", "img_caption": ["Figure 6: The real-world experiments on CelebA256 dataset (More examples) "], "img_footnote": [], "page_idx": 34}, {"type": "image", "img_path": "euQ0C4iS7O/tmp/42200570fbeff07772a0afa516632e68873ddb4e3da9cd5874203d3593043052.jpg", "img_caption": ["Figure 7: The real-world experiments on CelebA256 dataset (Detail) "], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Claims ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We propose a new drifted VESDE forward process and achieve the first polynomial complexity for reverse SDE (Corollary 5.4). For reverse PFODE, we propose the firs t quantitative convergence guarantee for VESDE (SOTA) (Theorem 6.1). We also do synthetic experiments to support our results (Section 7). ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: We discuss future work and limitation at Section 8. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should refect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 35}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: All assumptions, Theorem, Corollary, and proof sketch have been clearly stated in the main content. The detailed proof appears in the appendix. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 35}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "We has shown all experiments detail including dataset and training detail in Appendix G. Furthermore, we discuss why these experiments results support our theoretical results in Section 7. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. () If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 36}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with suffcient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 36}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Justification: As a theoretical work, we only do simple synthetic experiments to support our results. All detail and the used checkpoint are shown in Appendix G. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 36}, {"type": "text", "text": "", "page_idx": 37}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: We has shown all experiments detail including dataset and training detail in AppendixG. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 37}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: The results of Figure 2 and Table 1 are calculated over 5 random seed. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 37}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce theexperiments? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: We have shown the compute works and computation time in Appendix G. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 38}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Justification: We have checked the code of ethics and make sure that our work satisfies the codeofethics. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 38}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Justification: We have discussed the broader impacts of our work at the end of main paper. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 38}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: This paper poses no such risks. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to acces the model or implementing safetyfilters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 39}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: The paper does not use existing assets. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 39}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 39}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "page_idx": 39}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 40}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 40}]