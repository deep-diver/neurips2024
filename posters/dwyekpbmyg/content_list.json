[{"type": "text", "text": "Free Lunch in Pathology Foundation Model: Task-specific Model Adaptation with Concept-Guided Feature Enhancement ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yanyan Huang Weiqin Zhao Yihang Chen The University of Hong Kong The University of Hong Kong The University of Hong Kong yanyanh@connect.hku.hk wqzhao98@connect.hku.hk yihangc@connect.hku.hk ", "page_idx": 0}, {"type": "text", "text": "Yu Fu Lequan $\\mathbf{Y_{u}}^{*}$ Lanzhou University The University of Hong Kong fuyu@lzu.edu.cn lqyu@hku.hk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Whole slide image (WSI) analysis is gaining prominence within the medical imaging field. Recent advances in pathology foundation models have shown the potential to extract powerful feature representations from WSIs for downstream tasks. However, these foundation models are usually designed for general-purpose pathology image analysis and may not be optimal for specific downstream tasks or cancer types. In this work, we present Concept Anchor-guided Task-specific Feature Enhancement (CATE), an adaptable paradigm that can boost the expressivity and discriminativeness of pathology foundation models for specific downstream tasks. Based on a set of task-specific concepts derived from the pathology vision-language model with expert-designed prompts, we introduce two interconnected modules to dynamically calibrate the generic image features extracted by foundation models for certain tasks or cancer types. Specifically, we design a Concept-guided Information Bottleneck module to enhance task-relevant characteristics by maximizing the mutual information between image features and concept anchors while suppressing superfluous information. Moreover, a Concept-Feature Interference module is proposed to utilize the similarity between calibrated features and concept anchors to further generate discriminative task-specific features. The extensive experiments on public WSI datasets demonstrate that CATE significantly enhances the performance and generalizability of MIL models. Additionally, heatmap and umap visualization results also reveal the effectiveness and interpretability of CATE. The source code is available at https://github.com/HKU-MedAI/CATE. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Multiple Instance Learning (MIL) [26, 34, 23, 2] is widely adopted for weakly supervised analysis in computational pathology, where the input of MIL is typically a set of patch features generated by a pre-trained feature extractor (i.e., image encoder). Although promising progress has been achieved, the effectiveness of MIL models heavily relies on the quality of the extracted features. A robust feature extractor can discern more distinctive pathological features, thereby improving the predictive capabilities of MIL models. Recently, several studies have explored using pretrained foundation models on large-scale pathology datasets with self-supervised learning as the feature extractors for WSI analysis [37, 7, 3, 36]. Additionally, drawing inspiration from the success of Contrastive ", "page_idx": 0}, {"type": "image", "img_path": "dwYekpbmYG/tmp/6ee2209bc25c3ca6a017ffecf78a46210f130ee7b586806d0d803c4a46292a14.jpg", "img_caption": ["Figure 1: (a) Illustration of the key idea of concept-guided information bottleneck to enhance the task-relevant information and discard the task-irrelevant information. (b) Task-specific model adaptation with CATE to enhance the generalization across different data sources. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Language-Image Pretraining (CLIP) [30, 20] in bridging visual and linguistic modalities, some works have aimed to develop a pathology vision-language foundation model (VLM) to simultaneously learn representations of pathology images and their corresponding captions [15, 25]. The intrinsic consistency between the image feature space and caption embedding space in the pathology VLM enables the image encoder to extract more meaningful and discriminative features for downstream WSI analysis applications [25]. ", "page_idx": 1}, {"type": "text", "text": "Although the development of these pathology foundation models has significantly advanced computational pathology, these models are designed for general-purpose pathology image analysis and may not be optimal for specific downstream tasks or cancer types, as the features extracted by the image encoder may contain generic yet task-irrelevant information that will harm the performance of specific downstream tasks. For example, as illustrated in Figure 1(a), the features extracted by the image encoder of a pathology VLM can include both task-relevant information (e.g., arrangement or morphology of tumor cells) and task-irrelevant elements(such as background information, stain styles, etc.). The latter information may act as \"noise\", distracting the learning process of MIL models tailored to specific tasks, and potentially impairing the generalization performance of these models across different data sources. Consequently, it is crucial to undertake task-specific adaptation to enhance feature extraction of generic foundation models and enable MIL models to concentrate on task-relevant information and thus improve analysis performance and generalization [32, 38]. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we propose a novel paradigm, named Concept Anchor-guided Task-specific Feature Enhancement (CATE), to enhance the generic features extracted by the pathology VLM for specific downstream tasks (e.g., cancer subtyping). Without requiring additional supervision or significant computational resources, CATE offers an approximately \"free lunch\" in the context of pathology VLM. Specifically, we first derive a set of task-specific concept anchors from the pathology VLM with task-specific prompts, and these prompts rely on human expert design or are generated through querying large language models (LLMs), necessitating a certain level of pathological background knowledge. Based on these concept anchors, we design two concept-driven modules, i.e., the Conceptguided Information Bottleneck (CIB) module and the Concept-Feature Interference (CFI) module, to calibrate and generate task-specific features for downstream analysis. Particularly, with the taskspecific concepts as the guidance, the CIB module enhances task-relevant features by maximizing the mutual information between the image features and the concept anchors and also eliminates task-irrelevant information by minimizing the superfluous information, as shown in Figure 1(a). Moreover, the CFI module further generates discriminative task-specific features by utilizing the similarities between the calibrated image features and concept anchors (i.e., concept scores). By incorporating the CATE into existing MIL frameworks, we not only obtain more discriminative features but also improve generalization regarding domain shift by eliminating task-irrelevant features and concentrating on pertinent information, as shown in Figure 1(b). ", "page_idx": 1}, {"type": "text", "text": "In summary, the main contributions of this work are threefold: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We introduce a novel method, named CATE, for model adaptation in computational pathology. To the best of our knowledge, this is the first initiative to conduct task-specific feature enhancement based on the pathology foundation model for MIL tasks. \u2022 We design a new CIB module to enhance the task-relevant information and discard irrelevant information with the guidance of task-specific concepts, and a new CFI module to generate task-specific features by exploiting the similarities between image features and concept anchors. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Extensive experiments on Whole Slide Image (WSI) analysis tasks demonstrate that CATE significantly enhances the performance and generalization capabilities of MIL models. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Multiple Instance Learning (MIL) for WSI Analysis. MIL is the predominant paradigm for WSI analysis, treating each WSI as a bag of patch instances and classifying the entire WSI based on aggregated patch-level features. Attention-based methods [16, 26, 17, 39] are highly regarded for their ability to determine the significance of each instance within the bag. For instance, Ilse et al. [16] introduced an attention-based MIL model, while Lu et al. [26] proposed clustering-constrainedattention to refine this mechanism further. To model the relationships among instances, graph-based and Transformer-based methods have been developed [10, 2, 13]. For example, Chen et al. [2] introduced a Transformer-based hierarchical network to capitalize on the inherent hierarchical structure of WSIs. ", "page_idx": 2}, {"type": "text", "text": "Pathology Foundation Model. With the advancement of foundation models in computer vision, several pathology foundation models have been developed to serve as robust image encoders for WSI analysis. Riasatian et al. [31] proposed fine-tuning the DenseNet [12] on the TCGA dataset, while Filiot et al. [7] utilized iBOT [42] to pretrain a vision Transformer using the Masked Image Modeling framework. Recently, Chen et al. [3] pre-trained a general-purpose foundation model on large-scale pathology datasets using DINOv2 [28], which has demonstrated strong and readily usable representations for WSI analysis. Inspired by CLIP [30], Ikezogwo et al. [15], Huang et al. [14], and Lu et al. [25] developed vision-language foundation models by training on large-scale pathology datasets with image-caption pairs. These foundation models have demonstrated superior performance in downstream tasks due to their ability to extract more discriminative features for WSI analysis. ", "page_idx": 2}, {"type": "text", "text": "Feature Enhancement in Computational Pathology. Several methods have been developed to obtain more discriminative features for WSI analysis by adapting pathology foundation models [41, 24] or designing new plug-and-play modules [35]. For instance, Zhang et al. [41] suggested aligning the image features with text features extracted from a pre-trained natural language model to enhance the feature representation of WSI patch images, while it operates solely at the patch level, without considering the informational relationship between image and text features. Recently, Tang et al. [35] introduced Re-embedded Regional Transformer for feature re-embedding, aimed at enhancing WSI analysis when integrated with existing MIL methods. However, while this method considers the spatial information of WSIs and adds flexibility to MIL models, it falls short in extracting task-specific discriminative information for WSI analysis. ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Overview ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The proposed CATE can be seamlessly integrated with any MIL framework to adapt the existing pathology foundation model (Pathology VLM) for performance-improved WSI analysis via taskspecific enhancement. Specifically, consider a training set $\\mathcal{D}=\\{(\\mathbf{x},\\mathbf{y})\\}$ of WSI-label pairs, where $\\bar{\\mathbf{x}^{}}=\\{{\\pmb x}_{1},{\\pmb x}_{2},...,{\\pmb x}_{N}\\}$ is a set of patch features with dimension of $C$ (i.e., $\\pmb{x}_{i}\\in\\mathbb{R}^{C}$ ) extracted by the image encoder of pathology VLM, $N$ denotes the number of patches, and $\\mathbf{y}$ is the corresponding label. The objective of CATE is to obtain the corresponding enhanced task-specific feature set $\\mathbf{z}$ from the original feature $\\mathbf{x}$ with the guidance of pre-extracted concepts anchors c (see description below) for downstream usage: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{z}=\\mathbf{CATE}\\left(\\mathbf{x},\\mathbf{c}\\right),\\hat{\\mathbf{y}}=\\mathrm{MIL}\\left(\\mathbf{z}\\right).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "As illustrated in Figure 2, we design two different modules to enhance the extracted features from foundation models: (1) Concept-guided Information Bottleneck (CIB) module calibrates original image features with the guidance of concept anchors with information bottleneck principle; and (2) Concept-Feature Interference (CFI) module generates discriminative task-specific features by leveraging the similarities between the calibrated image features and concept anchors. Specifically, the enhanced patch features can be represented as $\\mathbf{z}=\\{z_{1},z_{2},...,z_{N}\\}$ , where $\\boldsymbol{z}_{i}$ is the concatenation of the calibrated feature $\\alpha_{i}$ and the interference feature ${\\boldsymbol\\beta}_{i}$ generated by CIB and CFI module: ", "page_idx": 2}, {"type": "equation", "text": "$$\nz_{i}=\\mathrm{Concat}\\left[\\alpha_{i},\\beta_{i}\\right]=\\mathrm{Concat}\\left[\\bf{C}\\mathrm{I}\\!\\!\\mathrm{{B}}\\left(x_{i},\\bf{c}\\right),\\bf{C}\\mathrm{{FI}}\\left(x_{i},\\bf{c}\\right)\\right].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "image", "img_path": "dwYekpbmYG/tmp/0fabb89aaeef609b4f37187a46626d4cda25c27dca45733529fcd33a14e6dfa5.jpg", "img_caption": ["Figure 2: (a) Overview of CATE: the outputs of the CIB and CFI modules are concatenated to form the enhanced feature for downstream MIL models. (b) Task-relevant concept generation. (c) Conceptguided Information Bottleneck (CIB) module. (c) Concept-Feature Interference (CFI) module. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Concept Extraction. We extract two kinds of task-specific concept anchors, $\\mathbf{c}=\\{\\mathbf{c}^{\\mathrm{{cs}}},\\mathbf{c}^{\\mathrm{{ca}}}\\}$ , comprising class-specific concepts $\\mathbf{c}^{\\mathrm{{cs}}}=\\{c_{i}^{\\mathrm{{cs}}}\\}_{i=1}^{m}$ (e.g., subtyping classes) and class-agnostic concepts $\\mathbf{\\bar{c}^{\\mathrm{ca}}}=\\mathbf{\\bar{\\{c}}}_{i}^{\\mathrm{ca}}\\mathbf{\\bar{\\xi}}_{i=1}^{n}$ e(cei.gf.ic,  aandidp colsaes,s -caognnnoescttiicv ec,o annced pntso,r rmesalp teicstsivueelsy).,  Twhitehs $m$ oanncd $n$ t rs eaprree sgeennteirnagt etdh eb yn uthmetext encoder of pathology VLM with prompt p. Each prompt consists of a class name (e.g., \"invasive ductal carcinoma\") and a template (e.g., \"An image of ${<}C L A S S N A M E{>}\")$ . To obtain more robust concepts, we use multiple prompts for each class and the final concept anchor is the average of the embeddings generated by different prompts. Details of class names and templates for various tasks are provided in Appendix G. Note that due to the inherent consistency between the image and text embedding space in VLM, these extracted concepts can also be regarded as image concept vectors. ", "page_idx": 3}, {"type": "text", "text": "3.2 Concept-guided Information Bottleneck ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The objective of this module is to find a distribution $p(\\pmb{\\alpha}|\\pmb{\\ x})$ that maps the original image feature $\\mathbf{x}$ into a representation $_{\\alpha}$ , which contains enhanced task-discriminative characteristics and suppressed task-irrelevant information. WSIs typically contain various cell types or tissues (e.g., tumor cells, normal cells, adipose tissue, connective tissue), while only a subset of patches (e.g., with tumor cells) is crucial for certain tasks such as tumor subtyping. We thus define $\\hat{\\mathbf{x}}=\\{\\tilde{\\hat{x}}_{i}\\}_{i=1}^{k}\\subseteq\\mathbf{x}$ as the representative subset of the original feature set (e.g., tumor tissue patches), where $k$ denotes the number of representative patches. Note that this selection can be conducted with a simple comparison of image features with class-specific concepts (see discussion below). To this end, the corresponding enhanced feature set is $\\hat{\\pmb{\\alpha}}=\\{\\hat{\\pmb{\\alpha}}_{i}\\}_{i=1}^{k}\\subseteq\\pmb{\\alpha}$ and we want to find the conditional distribution $p(\\hat{\\pmb{\\alpha}}_{i}|\\hat{\\pmb{x}}_{i})$ to map the selected patch feature $\\hat{\\mathbf{x}}_{i}\\in\\mathbb{R}^{C}$ into a more discriminative enhanced feature $\\hat{\\mathbf{\\alpha}}_{i}\\in\\mathbb{R}^{C}$ , which is discriminative enough to identify the label $\\mathbf{y}$ . ", "page_idx": 3}, {"type": "text", "text": "Sufficiency and Consistency Requirements. To quantify the informativeness requirement of the calibrated feature $\\hat{\\pmb{\\alpha}}$ , we consider the sufficiency of $\\hat{\\pmb{\\alpha}}$ for y. As defined in Appendix E.1, the encoded feature $\\hat{\\pmb{\\alpha}}$ derived from the original feature $\\hat{\\pmb x}$ is sufficient for determining the label $\\mathbf{y}$ if and only if the amount of task-specific information remains unchanged after calibration, i.e., $I({\\hat{x}};\\mathbf{y})=I({\\hat{\\alpha}};\\mathbf{y})$ . However, the label y pertains to the slide level and specific labels cannot be assigned to each instance due to the absence of patch-level annotations. ", "page_idx": 3}, {"type": "text", "text": "To address this challenge, we propose using the task-specific concept anchor as the guidance for each single $\\hat{\\pmb{\\alpha}}$ . Specifically, we posit that the concept anchor c is distinguishable for the task and contains task-relevant information for label y. Given the consistency between image and text features in pathology VLM, any representation $\\hat{\\pmb{\\alpha}}$ containing all information accessible from both image feature $\\hat{\\pmb{x}}$ and concept c will also encapsulate the discriminative information required for the label. This consistency requirement is detailed in Appendix E.1. Thus, if $\\hat{\\pmb{\\alpha}}$ is sufficient for c (i.e., $I(\\hat{{\\boldsymbol{x}}};{\\mathbf{c}}|\\hat{{\\boldsymbol{\\alpha}}})=0)$ ), then $\\hat{\\pmb{\\alpha}}$ is as predictive for label y as the joint of original feature $\\hat{\\pmb{x}}$ and concept anchor c. Applying the chain rule of mutual information, we derive: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\nI(\\hat{\\mathbf{x}};\\hat{\\alpha})=\\underbrace{I(\\hat{\\alpha};\\mathbf{c})}_{\\mathrm{Predictive\\;Information}}+\\underbrace{I(\\hat{\\alpha};\\hat{\\alpha}|\\mathbf{c})}_{\\mathrm{Superfluous\\;Information}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "According to the consistency between the concept anchor and original feature, the mutual information term $I(\\hat{\\pmb{\\alpha}};\\mathbf{c})$ represents the predictive information for the task, while the conditional information term $I(\\hat{{\\boldsymbol x}};\\hat{{\\boldsymbol\\alpha}}|{\\bf c})$ denotes task-irrelevant information (i.e., superfluous information) in original patch feature $\\hat{\\pmb{x}}$ , which can be minimized to enhance the robustness and generalization ability of downstream MIL models. As a result, the main objective of the feature calibration in this module can be formalized as maximize predictive information $I(\\hat{\\pmb{\\alpha}};\\mathbf{c})$ while minimize the superfluous information $I(\\hat{{\\boldsymbol x}};\\hat{{\\boldsymbol\\alpha}}|{\\bf c})$ . ", "page_idx": 4}, {"type": "text", "text": "Predictive Information Maximization (PIM). The predictive information in Equ (3) equals to the mutual information between the calibrated feature and concept anchors. To maximize this, we choose the InfoNCE [27] to estimate the lower bound of the mutual information, which can be obtained by comparing positive pairs sampled from the joint distribution $\\hat{\\pmb{\\alpha}}$ $,c_{\\mathrm{pos}}^{\\mathrm{cs}}\\sim p(\\hat{\\alpha},c^{\\mathrm{cs}})$ to pairs $\\hat{\\mathbf{alpha}},c_{j}^{\\mathrm{cs}}$ and $\\hat{\\alpha},c_{j}^{\\mathrm{ca}}$ built using a set of negative class concepts $c_{j}^{\\mathrm{cs}}\\,\\sim\\,p\\,(c^{\\mathrm{cs}})$ and class-agnostic concepts $c_{j}^{\\mathrm{ca}}\\sim p\\,(c^{\\mathrm{ca}})$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\nI_{N C E}\\left(\\hat{\\alpha};\\mathbf{c}\\right)=\\underset{\\hat{\\alpha},c_{\\mathrm{pos}}^{\\mathrm{cs}}}{\\mathbb{E}}\\left[\\sum_{i=1}^{k}\\log\\frac{f\\left(c_{\\mathrm{pos}}^{\\mathrm{cs}},\\hat{\\alpha}_{i}\\right)}{\\sum_{j=1}^{m}f\\left(c_{j}^{\\mathrm{cs}},\\hat{\\alpha}_{i}\\right)+\\sum_{j=1}^{n}f\\left(c_{j}^{\\mathrm{ca}},\\hat{\\alpha}_{i}\\right)}\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We set $f\\left(c,{\\hat{\\alpha}}_{i}\\right)=\\exp\\left({\\hat{\\alpha}}_{i}^{\\mathsf{T}}c/\\tau\\right)$ with $\\tau>0$ in practice following [27]. By maximizing this mutual information lower bound, $f\\left(\\pmb{c},\\hat{\\pmb{\\alpha}}_{i}\\right)$ will be proportional to the density ratio $p\\left(c,\\hat{\\pmb{\\alpha}}_{i}\\right)\\!/p\\left(\\pmb{c}\\right)p\\left(\\hat{\\pmb{\\alpha}}_{i}\\right)$ as proved in [27]. Hence, $f\\left(c,\\hat{\\alpha}_{i}\\right)$ preserves the mutual information between the calibrated feature and concept anchor. The detailed derivation can be found in Appendix E.2. The loss function for PIM can be denoted as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{P I M}=\\underset{\\hat{\\alpha},c_{\\mathrm{pos}}^{\\mathrm{cs}}}{\\mathbb{E}}\\left[-\\sum_{i=1}^{k}\\frac{\\hat{\\alpha}_{i}^{\\top}c_{\\mathrm{pos}}^{c s}}{\\tau}\\right]+\\underset{\\hat{\\alpha},c_{\\mathrm{pos}}^{\\mathrm{cs}}}{\\mathbb{E}}\\left[\\sum_{i=1}^{k}\\log\\left(\\sum_{j=1}^{m}\\exp\\frac{\\hat{\\alpha}_{i}^{\\top}c_{j}^{c s}}{\\tau}+\\sum_{j=1}^{n}\\exp\\frac{\\hat{\\alpha}_{i}^{\\top}c_{j}^{c a}}{\\tau}\\right)\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Superfluous Information Minimization (SIM). To compress task-irrelevant information, we aim to minimize the superfluous information term as defined in Equ (3). This objective can be achieved by minimizing the mutual information $I(\\hat{\\pmb{x}};\\hat{\\pmb{\\alpha}})$ . In practice, we conduct SIM for all patches in the subset $\\mathbf{x}$ , as each patch may contain task-irrelevant information. Following [1], it can be represented as: ", "page_idx": 4}, {"type": "equation", "text": "$$\nI(x;\\alpha)=\\int p\\left(x,\\alpha\\right)\\log p\\left(\\alpha|x\\right)d x d\\alpha-\\int p\\left(\\alpha\\right)\\log p\\left(\\alpha\\right)d\\alpha.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "After that, we let the distribution of $\\alpha\\colon r\\left(\\alpha\\right)$ (e.g., Gaussian distribution in this work), be a variational approximation to the marginal distribution $p\\left(\\alpha\\right)$ , and we can obtain the upper bound for $I({\\boldsymbol{x}};{\\boldsymbol{\\alpha}})$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\nI(x;\\alpha)\\leq\\int p\\left(x\\right)p\\left(\\alpha|x\\right)\\log\\frac{p\\left(\\alpha|x\\right)}{r\\left(\\alpha\\right)}d x d\\alpha.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Furthermore, we use a variational distribution $q_{\\theta}\\left(\\alpha|x\\right)$ with parameter $\\theta$ to approximate $p\\left(\\alpha|x\\right)$ and we implement the parameterization of the variational distribution with MLP by predicting the mean and variance of the Gaussian distribution and sample the calibrated feature $_{\\alpha}$ from this distribution: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\alpha\\sim\\mathcal{N}\\left(M L P^{\\mu}\\left({\\pmb x}\\right),M L P^{\\Sigma}\\left({\\pmb x}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In practice, we implement this by utilizing the reparameterization trick [18] to obtain an unbiased estimate of the gradient and further optimize the variational distribution. The detailed derivation can be found in Appendix E.3. The minimization of the upper bound of $I(\\pmb{x};\\pmb{\\alpha})$ equals to the minimization of the Kullback-Leibler divergence between $q_{\\theta}\\left(\\alpha|x\\right)$ and $r\\left(\\alpha\\right)$ . Therefore, the loss function can be represented as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{S I M}=\\mathbb{E}\\left[\\sum_{i=1}^{k}D_{K L}\\left(q_{\\theta}\\left(\\alpha_{i}\\middle|\\pmb{x}_{i}\\right)\\middle|\\middle|r\\left(\\alpha_{i}\\right)\\right)\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Discussion. We further provide explanation of CIB module with the information plane [8, 6] in Appendix F. It should be noted that the PIM supervises only the representative subset $\\hat{\\bf x}$ containing task-relevant information (selected by the similarity between image features and corresponding class-specific concepts). Meanwhile, the SIM is applied to all patches in $\\mathbf{x}$ , as any patch may carry information irrelevant to the task (e.g., background information and stain styles). Besides, SIM cannot be directly optimized without the guidance of concept anchors (i.e., PIM) due to the absence of patch-level labels. As demonstrated in the ablation study in Section 4.4, the absence of concept anchor guidance leads to the collapse of discriminative information in the calibrated feature, adversely affecting downstream task performance. By maximizing predictive information and minimizing superfluous details, the CIB module effectively enhances the discriminative capacity of the original features and aligns them with the task-specific concept anchors for improved prediction. ", "page_idx": 5}, {"type": "text", "text": "3.3 Concept-Feature Interference ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We also propose the Concept-Feature Interference (CFI) module to utilize the similarity characteristic between calibrated features and concept anchors to further obtain robust and discriminative information for the downstream tasks. Our primary focus is on the class-specific concept anchors $\\mathbf{c}^{\\mathrm{{cs}}}$ . Specifically, for each CIB encoded feature $\\alpha_{i}$ , we calculate the cosine similarity between $\\alpha_{i}$ and each class-specific concept $c_{i}^{\\mathrm{cs}}$ . It is important to note that the number of class-specific concepts $m$ is larger than the number of classes, as we use multiple ${<}C L A S S N A M E{>}$ and templates to generate the concept anchor for each class, as shown in the Appendix G. Thus, we can obtain the similarity vector by concatenating the similarity scores between $\\alpha_{i}$ and each class-specific concept $c_{i}^{\\mathrm{cs}}$ . To integrate the interference information (similarity relationship) into the enhanced feature, we align the similarity vector with the calibrated feature $\\alpha_{i}$ using a Self-Normalizing Network (SNN) layer [19]. This allows us to obtain the final interference vector $\\beta_{i}$ of CFI: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\beta_{i}=\\mathrm{SNN}\\left(\\mathrm{Concat}\\left[\\left\\{\\mathrm{Sim}\\left(\\alpha_{i},c_{i}^{\\mathrm{cs}}\\right)\\right\\}_{i=1}^{m}\\right]\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The interference vector contains superficial information that indicates the similarity between the calibrated feature and concept anchor directly. This is completely different from the calibrated feature of the CIB module, which contains discriminative latent information for the downstream tasks. Therefore, integrating the interference feature can further provide robust and discriminative information for the downstream tasks. ", "page_idx": 5}, {"type": "text", "text": "Discussion. The CFI module is designed to utilize the similarity characteristic between calibrated feature and concept anchor as a discriminative feature, which can be further integrated into the calibrated feature for downstream tasks. This is different from other studies that directly compare the similarity between visual features and textual concept features of different classes to perform zero-shot classification [25]. ", "page_idx": 5}, {"type": "text", "text": "3.4 Training Objective ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The overall training objective of the CATE framework can be represented as the combination of the cross entropy loss $\\mathcal{L}_{C E}$ for the downstream tasks, the predictive information maximization loss $\\mathcal{L}_{P I M}$ , and the superfluous information minimization loss $\\mathcal{L}_{S I M}$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}=\\mathcal{L}_{C E}+\\lambda_{P}\\mathcal{L}_{P I M}+\\lambda_{S}\\mathcal{L}_{S I M},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\lambda_{P}$ and $\\lambda_{S}$ are hyperparameters and influence of them is discussed in Appendix C. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Experimental Settings ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Tasks and Datasets. We conducted cancer subtyping tasks on three public WSI datasets from The Cancer Genome Atlas (TCGA) project: Invasive Breast Carcinoma (BRCA), Non-Small Cell Lung Cancer (NSCLC), and Renal Cell Carcinoma (RCC). Detailed dataset information is available in Appendix D. ", "page_idx": 5}, {"type": "table", "img_path": "dwYekpbmYG/tmp/6b3861579e3e093c87cf9a171a1bac8c5fda4ab6229f9312a696b0c951cf635c.jpg", "table_caption": ["Table 1: Cancer Subtyping Results on BRCA of MIL Models Incorporated with CATE. "], "table_footnote": ["\\* The best results are highlighted in bold, and the second-best results are underlined. \u2020 $\\mathrm{R}^{2}\\mathrm{T}\\mathbf{-}\\mathbf{M}\\mathbf{IL}$ is designed for feature re-embedding that utilize ABMIL as base MIL model. "], "page_idx": 6}, {"type": "text", "text": "IND and OOD Settings. The datasets in the TCGA contains samples from different source sites (i.e., different hospitals or laboratories), which are indicated in the sample barcodes2. And different source sites have different staining protocols and imaging characteristics, causing feature domain shifts between different sites [4, 5]. Therefore, MIL models trained on several sites may not generalize well to others. To better evaluate the true performance of the models, we selected several sites as IND data (in-domain, the testing and training data are from the same sites), and used data from other sites as OOD data (out-of-domain, the testing and training data are from different sites), and reported the testing performance on both IND and OOD data. Specifically, we designated $N_{\\mathrm{IND}}$ sites as IND and the remaining as OOD. Each experiment involved splitting the IND data into training, validation, and testing sets, training the models on IND data, and evaluating them on both IND and OOD testing data. For the BRCA dataset, we randomly selected one or two sites as IND data and used the remaining sites as OOD data. However, for NSCLC (2 categories) and RCC ( $^3$ categories) datasets, each site contains samples from only one subtype. Therefore, we cannot select only one site as IND data, as it will include one category/subtype in the training data. Instead, we randomly selected one or two corresponding sites for each category as IND data for NSCLC and RCC, and used the other sites as OOD data. Finally, we obtained 1 or 2 IND sites for BRCA, 2 or 4 for NSCLC, and 3 or 6 for RCC. ", "page_idx": 6}, {"type": "text", "text": "Evaluation. We report the area under the receiver operating characteristic curve (AUC) and accuracy for the OOD and IND test sets, respectively, with means and standard deviations over 10 runs of Monte-Carlo Cross Validation. Notably, the OOD performance is emphasized for NSCLC and RCC, where each site contains samples from only one cancer subtype. Traditional MIL models tend to recognize site-specific patterns (e.g., staining) as shortcuts and excel in in-domain evaluations, rather than identifying useful class-specific features, making performance less reflective of the models\u2019 actual capability. Therefore, OOD performance more accurately reflects the models\u2019 discriminative and generalization capabilities. ", "page_idx": 6}, {"type": "text", "text": "Comparisons. Given that CATE is an adaptable method, we evaluated the performance variations across various MIL models both with and without the integration of CATE. We specifically focused on the following state-of-the-art MIL models: the original ABMIL [16], CLAM [26], DSMIL [21], ", "page_idx": 6}, {"type": "table", "img_path": "dwYekpbmYG/tmp/bdc70029934f3fca64828e508069ed6032c6c6aed763a5fa362d833013743805.jpg", "table_caption": ["Table 2: Cancer Subtyping Results on NSCLC and RCC. "], "table_footnote": ["\\* The best results are highlighted in bold, and the second-best results are underlined. \u2020 $\\mathrm{R^{2}T-M I L}$ is designed for feature re-embedding that utilize ABMIL as base MIL model. # The in-domain performance of NSCLC and RCC does not represent the true ability of the models, as each site contains only samples from one cancer subtype. We primarily focus on the OOD performance for these two datasets. "], "page_idx": 7}, {"type": "text", "text": "TransMIL [33], DTFD-MIL [40], and $\\mathrm{\\nabla{R^{2}T}}$ -MIL [35]. The $\\mathbf{R}^{2}\\mathbf{T}$ -MIL [35] is a feature re-embedding method that utilizes ABMIL as the base MIL model. ", "page_idx": 7}, {"type": "text", "text": "Implementation Details. This study begins the image feature extraction process by segmenting the foreground tissue and then splitting the WSI into $512\\!\\times\\!512$ pixels patches at $20\\times$ magnification. Subsequently, these patches are processed through a pre-trained image encoder from CONCH [25] to extract image features. For concept anchors, we utilize CONCH\u2019s text encoder to derive task-relevant concepts from predefined text prompts, with detailed prompt information available in Appendix G. Model parameters are optimized using the Adam optimizer with a learning rate of $10^{-5}$ . The batch size is set to 1, and all the experiments are conducted on a single NVIDIA RTX 3090 GPU. ", "page_idx": 7}, {"type": "text", "text": "4.2 Experimental Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Quantitative Results on BRCA Dataset. To fully evaluate the effectiveness of CATE, we assessed its impact on several state-of-the-art MIL models using the BRCA dataset. The results are shown in Table 1, where the MIL models integrated with CATE outperform their original counterparts in both in-domain (IND) and out-of-domain (OOD) testing, which demonstrates the effectiveness and generalization capabilities of CATE. Comparing with $\\begin{array}{r}{\\breve{\\bf R}^{2}{\\bf T}.}\\end{array}$ -MIL, which is a feature re-embedding method that utilizes ABMIL as the base MIL model, CATE incorporated with ABMIL consistently achieves better performance in terms of both OOD and IND testing. To further investigate the effectiveness of CATE, we conducted experiments by altering the in-domain sites and applying traditional settings. Detailed results are available in Appendix B. ", "page_idx": 7}, {"type": "text", "text": "Qualitative Analysis. To qualitatively investigate the effectiveness of CATE, we visualized attention heatmaps, UMAP, and the similarities between original features and corresponding class concept features, as well as calibrated features and class concept features, as shown in Figure 3. Additional visualization results are provided in Appendix H. As shown in Figure 3 (a&b), attention heatmap comparisons reveal that CATE-MIL focuses more intensely on cancerous regions, with a clearer delineation between high and low attention areas. By comparing the similarities of original and calibrated features to class concept features in Figure 3 (c&d), it is evident that the enhanced similarity in cancerous regions is significantly higher than in original features. Moreover, the disparity between cancerous and non-cancerous regions\u2019 similarities is also expanded, which further verifies the ability of CATE to enhance task-relevant information and suppress irrelevant information. We further performed a UMAP visualization of class concept features, original features, and calibrated features. As depicted in Figure 3 (f), calibrated features are notably closer to the corresponding class (IDC) ", "page_idx": 7}, {"type": "text", "text": "concept features compared to the original features, which demonstrates CATE\u2019s ability to effectively align features with task-relevant concepts and enhance task-relevant information. ", "page_idx": 8}, {"type": "image", "img_path": "dwYekpbmYG/tmp/f0adb5a709cc254d8c4c4b6c4287b963dcb5873a9f0f05df02273e1b37fea15a.jpg", "img_caption": ["Figure 3: (a) Attention heatmap of CATE-MIL. (b) Attention heatmap of the original ABMIL. (c) similarity between the calibrated features and the corresponding class concept feature. (d) similarity between the original features and the corresponding class concept feature. (e) Original WSI. (f) UMAP visualization of class concept features, original features, and enhanced features. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.3 Results on Additional Datasets ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "For clarity and to highlight the superiority of ABMIL when enhanced with CATE, we developed CATE-MIL by incorporating CATE into ABMIL and compared it against other leading MIL models on NSCLC and RCC datasets. The comparative results in Table 2 confirm that CATE-MIL consistently outperforms other models in both OOD and IND performance. However, it is noted that CATE-MIL performs poorly on the in-domain testing data for NSCLC and RCC. This underperformance may be attributed to the elimination of task-irrelevant information, including site-specific patterns, by CATE, potentially degrading performance on in-domain data for these datasets. Consequently, OOD performance more accurately reflects the discriminative and generalization capabilities of the models. ", "page_idx": 8}, {"type": "text", "text": "4.4 Ablation Analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We conduct ablation studies to assess the effectiveness of each component within CATE, and the results are shown in Table 3. Initially, incorporating Predictive Information Maximization (PIM) enables ABMIL to achieve improved performance in most experiments, which demonstrates PIM\u2019s efficacy in extracting task-relevant information. However, using Superfluous Information Minimization (SIM) alone results in performance degradation across most experiments, which suggests that SIM may discard some task-relevant information without guidance from a task-relevant concept anchor. Incorporating both PIM and SIM consistently enhances ABMIL\u2019s performance in all experiments, which further verifies that their combination effectively boosts the generalization capabilities of MIL models. We also conduct experiments by only using the interference features in CFI as the input of ABMIL, and the results show that the interference features are also informative for WSI classification tasks. More ablation analysis about the weights of PIM and SIM in CIB module the number of representative patches are in Appendix C. ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion and Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this paper, we introduce CATE, a new approach that offers a \"free lunch\" for task-specific adaptation of pathology VLM by leveraging the inherent consistency between image and text modalities. ", "page_idx": 8}, {"type": "table", "img_path": "dwYekpbmYG/tmp/244054cc8ef1b5bf0ae9a31a1334e9fa2954f528970894f467cbde448844ad2e.jpg", "table_caption": ["Table 3: Ablation study of CATE. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "CATE shows the potential to enhance the generic features extracted by pathology VLM for specific downstream tasks, using task-specific concept anchors as guidance. The proposed CIB module calibrates the image features by enhancing task-relevant information while suppressing task-irrelevant information, while the CFI module obtains the interference vector for each patch to generate discriminative task-specific features. Extensive experiments on WSI datasets demonstrate the effectiveness of CATE in improving the performance and generalizability of state-of-the-art MIL methods. ", "page_idx": 9}, {"type": "text", "text": "Limitations and Social Impact. The proposed CATE offers a promising solution to customize the pathology VLM for specific tasks, significantly improving the performance and applicability of MIL methods in WSI analysis. However, the performance of CATE heavily depends on the quality of the concept anchors, which, in turn, relies on domain knowledge and the quality of the pre-trained pathology VLM. Additionally, while CATE is optimized for classification tasks such as cancer subtyping, it may not be readily applicable to other analytical tasks, such as survival prediction. However, there might be a potential solution to address this challenge. For instance, we could leverage LLMs or retrieval-based LLMs to generate descriptive prompts about the general morphological appearance of WSIs for specific cancer types. By asking targeted questions, we can summarize reliable and general morphological descriptions associated with different survival outcomes or biomarker expressions and further verify these prompts with pathologists. Moreover, since medical data may contain sensitive information, ensuring the privacy and security of such data is crucial. ", "page_idx": 9}, {"type": "text", "text": "6 Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported in part by the Research Grants Council of Hong Kong (27206123 and T45-401/22-N), in part by the Hong Kong Innovation and Technology Fund (ITS/274/22), in part by the National Natural Science Foundation of China (No. 62201483), and in part by Guangdong Natural Science Fund (No. 2024A1515011875). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Alexander A Alemi, Ian Fischer, Joshua V Dillon, and Kevin Murphy. Deep variational information bottleneck. arXiv preprint arXiv:1612.00410, 2016.   \n[2] Richard J Chen, Chengkuan Chen, Yicong Li, Tiffany Y Chen, Andrew D Trister, Rahul G Krishnan, and Faisal Mahmood. Scaling vision transformers to gigapixel images via hierarchical self-supervised learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16144\u201316155, 2022.   \n[3] Richard J Chen, Tong Ding, Ming Y Lu, Drew FK Williamson, Guillaume Jaume, Andrew H Song, Bowen Chen, Andrew Zhang, Daniel Shao, Muhammad Shaban, et al. Towards a generalpurpose foundation model for computational pathology. Nature Medicine, 30(3):850\u2013862, 2024.   \n[4] Shenghua Cheng, Sibo Liu, Jingya Yu, Gong Rao, Yuwei Xiao, Wei Han, Wenjie Zhu, Xiaohua Lv, Ning Li, Jing Cai, et al. Robust whole slide image analysis for cervical cancer screening using deep learning. Nature communications, 12(1):5639, 2021.   \n[5] Kevin de Haan, Yijie Zhang, Jonathan E Zuckerman, Tairan Liu, Anthony E Sisk, Miguel FP Diaz, Kuang-Yu Jen, Alexander Nobori, Sofia Liou, Sarah Zhang, et al. Deep learning-based transformation of h&e stained tissues into special stains. Nature communications, 12(1):1\u201313, 2021.   \n[6] Marco Federici, Anjan Dutta, Patrick Forr\u00e9, Nate Kushman, and Zeynep Akata. Learning robust representations via multi-view information bottleneck. arXiv preprint arXiv:2002.07017, 2020.   \n[7] Alexandre Filiot, Ridouane Ghermi, Antoine Olivier, Paul Jacob, Lucas Fidon, Alice Mac Kain, Charlie Saillard, and Jean-Baptiste Schiratti. Scaling self-supervised learning for histopathology with masked image modeling. medRxiv, pages 2023\u201307, 2023.   \n[8] Bernhard C Geiger. On information plane analyses of neural network classifiers\u2014a review. IEEE Transactions on Neural Networks and Learning Systems, 33(12):7039\u20137051, 2021.   \n[9] R. Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Philip Bachman, Adam Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation and maximization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.   \n[10] Wentai Hou, Lequan Yu, Chengxuan Lin, Helong Huang, Rongshan Yu, Jing Qin, and Liansheng Wang. $\\mathrm{H}^{\\star}$ 2-mil: exploring hierarchical representation with heterogeneous multiple instance learning for whole slide image analysis. In Proceedings of the AAAI conference on artificial intelligence, volume 36, pages 933\u2013941, 2022.   \n[11] Frederick M Howard, James Dolezal, Sara Kochanny, Jefree Schulte, Heather Chen, Lara Heij, Dezheng Huo, Rita Nanda, Olufunmilayo I Olopade, Jakob N Kather, et al. The impact of site-specific digital histology signatures on deep learning model accuracy and bias. Nature communications, 12(1):4423, 2021.   \n[12] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4700\u20134708, 2017.   \n[13] Yanyan Huang, Weiqin Zhao, Shujun Wang, Yu Fu, Yuming Jiang, and Lequan Yu. Conslide: Asynchronous hierarchical interaction transformer with breakup-reorganize rehearsal for continual whole slide image analysis. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 21349\u201321360, 2023.   \n[14] Zhi Huang, Federico Bianchi, Mert Yuksekgonul, Thomas J Montine, and James Zou. A visual\u2013language foundation model for pathology image analysis using medical twitter. Nature medicine, 29(9):2307\u20132316, 2023.   \n[15] Wisdom Ikezogwo, Saygin Seyfioglu, Fatemeh Ghezloo, Dylan Geva, Fatwir Sheikh Mohammed, Pavan Kumar Anand, Ranjay Krishna, and Linda Shapiro. Quilt-1m: One million image-text pairs for histopathology. Advances in Neural Information Processing Systems, 36, 2024.   \n[16] Maximilian Ilse, Jakub Tomczak, and Max Welling. Attention-based deep multiple instance learning. In International conference on machine learning, pages 2127\u20132136. PMLR, 2018.   \n[17] Syed Ashar Javed, Dinkar Juyal, Harshith Padigela, Amaro Taylor-Weiner, Limin Yu, and Aaditya Prakash. Additive mil: Intrinsically interpretable multiple instance learning for pathology. Advances in Neural Information Processing Systems, 35:20689\u201320702, 2022.   \n[18] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.   \n[19] G\u00fcnter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Self-normalizing neural networks. Advances in neural information processing systems, 30, 2017.   \n[20] Zhengfeng Lai, Zhuoheng Li, Luca Cerny Oliveira, Joohi Chauhan, Brittany N Dugger, and Chen-Nee Chuah. Clipath: Fine-tune clip with visual feature fusion for pathology image analysis towards minimizing data collection efforts. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2374\u20132380, 2023.   \n[21] Bin Li, Yin Li, and Kevin W Eliceiri. Dual-stream multiple instance learning network for whole slide image classification with self-supervised contrastive learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 14318\u201314328, 2021.   \n[22] Hao Li, Ying Chen, Yifei Chen, Rongshan Yu, Wenxian Yang, Liansheng Wang, Bowen Ding, and Yuchen Han. Generalizable whole slide image classification with fine-grained visualsemantic interaction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11398\u201311407, 2024.   \n[23] Jana Lipkova, Richard J Chen, Bowen Chen, Ming Y Lu, Matteo Barbieri, Daniel Shao, Anurag J Vaidya, Chengkuan Chen, Luoting Zhuang, Drew FK Williamson, et al. Artificial intelligence for multimodal data integration in oncology. Cancer cell, 40(10):1095\u20131110, 2022.   \n[24] Jiaxuan Lu, Fang Yan, Xiaofan Zhang, Yue Gao, and Shaoting Zhang. Pathotune: Adapting visual foundation model to pathological specialists. arXiv preprint arXiv:2403.16497, 2024.   \n[25] Ming Y Lu, Bowen Chen, Drew FK Williamson, Richard J Chen, Ivy Liang, Tong Ding, Guillaume Jaume, Igor Odintsov, Long Phi Le, Georg Gerber, et al. A visual-language foundation model for computational pathology. Nature Medicine, pages 1\u201312, 2024.   \n[26] Ming Y Lu, Drew FK Williamson, Tiffany Y Chen, Richard J Chen, Matteo Barbieri, and Faisal Mahmood. Data-efficient and weakly supervised computational pathology on whole-slide images. Nature biomedical engineering, 5(6):555\u2013570, 2021.   \n[27] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.   \n[28] Maxime Oquab, Timoth\u00e9e Darcet, Th\u00e9o Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023.   \n[29] Linhao Qu, Kexue Fu, Manning Wang, Zhijian Song, et al. The rise of ai language pathologists: Exploring two-level prompt learning for few-shot weakly-supervised whole slide image classification. Advances in Neural Information Processing Systems, 36, 2024.   \n[30] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[31] Abtin Riasatian, Morteza Babaie, Danial Maleki, Shivam Kalra, Mojtaba Valipour, Sobhan Hemati, Manit Zaveri, Amir Safarpoor, Sobhan Shafiei, Mehdi Afshari, et al. Fine-tuning and training of densenet for histopathology image representation using tcga diagnostic slides. Medical image analysis, 70:102032, 2021.   \n[32] Alexander Robey, George J Pappas, and Hamed Hassani. Model-based domain generalization. Advances in Neural Information Processing Systems, 34:20210\u201320229, 2021.   \n[33] Zhuchen Shao, Hao Bian, Yang Chen, Yifeng Wang, Jian Zhang, Xiangyang Ji, et al. Transmil: Transformer based correlated multiple instance learning for whole slide image classification. Advances in Neural Information Processing Systems, 34:2136\u20132147, 2021.   \n[34] Artem Shmatko, Narmin Ghaffari Laleh, Moritz Gerstung, and Jakob Nikolas Kather. Artificial intelligence in histopathology: enhancing cancer research and clinical oncology. Nature Cancer, 3(9):1026\u20131038, 2022.   \n[35] Wenhao Tang, Fengtao Zhou, Sheng Huang, Xiang Zhu, Yi Zhang, and Bo Liu. Feature re-embedding: Towards foundation model-level performance in computational pathology. arXiv preprint arXiv:2402.17228, 2024.   \n[36] Eugene Vorontsov, Alican Bozkurt, Adam Casson, George Shaikovski, Michal Zelechowski, Siqi Liu, Philippe Mathieu, Alexander van Eck, Donghun Lee, Julian Viret, et al. Virchow: A million-slide digital pathology foundation model. arXiv preprint arXiv:2309.07778, 2023.   \n[37] Xiyue Wang, Sen Yang, Jun Zhang, Minghui Wang, Jing Zhang, Wei Yang, Junzhou Huang, and Xiao Han. Transformer-based unsupervised contrastive learning for histopathological image classification. Medical image analysis, 81:102559, 2022.   \n[38] Guile Wu and Shaogang Gong. Collaborative optimization and aggregation for decentralized domain generalization and adaptation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6484\u20136493, 2021.   \n[39] Cui Yufei, Ziquan Liu, Xiangyu Liu, Xue Liu, Cong Wang, Tei-Wei Kuo, Chun Jason Xue, and Antoni B Chan. Bayes-mil: A new probabilistic perspective on attention-based multiple instance learning for whole slide images. In The Eleventh International Conference on Learning Representations, 2022.   \n[40] Hongrun Zhang, Yanda Meng, Yitian Zhao, Yihong Qiao, Xiaoyun Yang, Sarah E Coupland, and Yalin Zheng. Dtfd-mil: Double-tier feature distillation multiple instance learning for histopathology whole slide image classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18802\u201318812, 2022.   \n[41] Yunkun Zhang, Jin Gao, Mu Zhou, Xiaosong Wang, Yu Qiao, Shaoting Zhang, and Dequan Wang. Text-guided foundation model adaptation for pathological image classification. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 272\u2013282. Springer, 2023.   \n[42] Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. ibot: Image bert pre-training with online tokenizer. arXiv preprint arXiv:2111.07832, 2021. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Overview ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The structure of this supplementary material as shown below, ", "page_idx": 13}, {"type": "text", "text": "\u2022 Appendix B presents additional experimental results, including changes in in-domain and out-of-domain site splitting, further comparisons on the BRCA dataset under traditional settings, additional results on the PRAD dataset, and results under site-preserved crossvalidation.   \n\u2022 Appendix C discusses additional ablation study results concerning hyperparameters, including the impact of loss weights for PIM and SIM, the effect of the number of representative patches $k$ , and further ablation studies on the CIB module.   \n\u2022 Appendix D presents detailed descriptions of datasets and experimental settings.   \n\u2022 Appendix E provides the detailed definition and formula derivation.   \n\u2022 Appendix F elaborates on the Concept Information Bottleneck (CIB) module using the information plane.   \n\u2022 Appendix G details the prompts used in our experiments.   \n\u2022 Appendix H presents additional visualization results. ", "page_idx": 13}, {"type": "text", "text": "B Additional Results ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "B.1 Additional Cancer Subtyping Results with Different IND and OOD Sites ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "To further evaluate the generalization ability of the proposed CATE method, we conduct additional experiments on TCGA-BRCA dataset with different settings of in-domain (IND) and out-of-domain (OOD) sites. The detailed experimental settings are shown below: ", "page_idx": 13}, {"type": "text", "text": "\u2022 BRCA $\\ N_{\\mathrm{IND}}{=}1)$ ): One site as the in-domain data, and the other sites as the out-of-domain data. The details of the in-domain site are shown below: \u2013 AR: 44 slides of IDC and 15 slides of ILC.   \n\u2022 BRCA $\\lvert N_{\\mathrm{IND}}{=}2\\rangle$ ): Two sites as the in-domain data, and the other sites as the out-of-domain data. The details of the in-domain sites are shown below: \u2013 AR: 44 slides of IDC and 15 slides of ILC. \u2013 B6: 39 slides of IDC and 6 slides of ILC. ", "page_idx": 13}, {"type": "text", "text": "The additional results are presented in Table 4. It is evident that the proposed CATE-MIL consistently delivers superior performance in both in-domain and out-of-domain settings, demonstrating its effectiveness in enhancing task-specific information and minimizing the impact of irrelevant data. ", "page_idx": 13}, {"type": "text", "text": "B.2 Additional Cancer Subtyping Results with Traditional Experimental Settings ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In the main paper, we divided the dataset into in-domain and out-of-domain sites to evaluate the generalization ability of the proposed CATE method. To further evaluate its effectiveness, we conducted additional experiments under traditional settings, randomly splitting the dataset into training, validation, and testing sets. We report the performance metrics, including means and standard deviations over 10 Monte-Carlo Cross-Validation runs, in Table 5. Additionally, we provide comparisons of training times and parameter sizes for various methods. ", "page_idx": 13}, {"type": "text", "text": "It is evident that the proposed CATE-MIL consistently outperforms others in both AUC and ACC metrics, underscoring its superiority. Furthermore, CATE-MIL benefits from shorter training times and smaller parameter sizes compared to the $\\mathrm{\\bfR}^{2}\\mathrm{\\bfT}.$ -MIL method. ", "page_idx": 13}, {"type": "text", "text": "B.3 Additional Gleason Grading Results on PRAD ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "CATE is a general framework that can be applied to more complex tasks beyond cancer subtyping, such as Gleason grading in prostate cancer. We have conducted conducted experiments on the TCGA-PRAD dataset to evaluate the performance of CATE-MIL in Gleason grading. Specifically, ", "page_idx": 13}, {"type": "text", "text": "Table 4: Supplementary Cancer Subtyping Results on BRCA. ", "page_idx": 14}, {"type": "table", "img_path": "dwYekpbmYG/tmp/0b8cf1efaaa8901a92a40c05be243132607e86144c0c06a2f511ceca8514b115.jpg", "table_caption": [], "table_footnote": ["The best results are highlighted in bold, and the second-best results are underlined. \u2020 denotes the methods for feature re-embedding that utilize ABMIL as base MIL model. "], "page_idx": 14}, {"type": "table", "img_path": "dwYekpbmYG/tmp/041c372f9a62462b7383d38de5edc4e2c7fcee534e160c95aa2c4274e3dc791e.jpg", "table_caption": ["Table 5: Results on BRCA under Traditional Settings. "], "table_footnote": ["The best results are highlighted in bold, and the second-best results are underlined. \u2020 denotes the methods for feature re-embedding that utilize ABMIL as base MIL model. "], "page_idx": 14}, {"type": "table", "img_path": "dwYekpbmYG/tmp/a9949ac331dbefceb7514ecead50ddfc657f8d8c817182eaf826e1345eb83c5d.jpg", "table_caption": ["Table 6: Supplementary Gleason Grading Results on PRAD. "], "table_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "dwYekpbmYG/tmp/b9ea386f589d0f3470993a6f8a81ee82830fa4504d971b79ca9e06bac02e0aaf.jpg", "table_caption": ["Table 7: Supplementary Results under Site-Preserved Cross-Validation. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "the samples in PRAD dataset are divided into Gleason pattern 3, 4, and 5, and the task is to classify the samples into these three categories. The results are shown in Table 6. It is evident that CATE-MIL consistently outperforms the base model ABMIL in both in-domain and out-of-domain settings, demonstrating its effectiveness in enhancing task-specific information and minimizing the impact of irrelevant data. In the future, as more studies reveal the connection between morphological features and molecular biomarkers and more powerful pathology VLMs are developed, our framework has the potential to benefit more complex tasks. ", "page_idx": 15}, {"type": "text", "text": "B.4 Additional Experiments using Site-Preserved Cross-Validation ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "To provide a more comprehensive evaluation of the proposed CATE-MIL, we conduct additional experiments using site-preserved cross-validation [11], where the samples from the same site are preserved in the same fold. For each fold, we split the data into training and testing sets, and these testing sets are regarded as in-domain testing data. And the other sites are used as out-of-domain testing data. The results are shown in Table 7. It is evident that CATE-MIL consistently outperforms ABMIL in both in-domain and out-of-domain settings. ", "page_idx": 15}, {"type": "text", "text": "C Additional Ablation Study ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "C.1 Ablation Study of the Weight of PIM and SIM Losses ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The overall objective of the proposed CATE-MIL is a weighted sum of the PIM and SIM losses, along with the classification loss: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}=\\mathcal{L}_{C E}+\\lambda_{P}\\mathcal{L}_{P I M}+\\lambda_{S}\\mathcal{L}_{S I M}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We note that the hyperparameters for the weights of the PIM and SIM losses were not specifically tuned in the main paper. To investigate the effect of the weight of PIM and SIM losses on the model performance, we conduct an ablation study on BRCA $\\scriptstyle N_{\\mathrm{IND}}=2)$ with CATE-MIL (ABMIL integrated with CATE), varying the weights of PIM and SIM losses. The results are shown in Figure 4. ", "page_idx": 15}, {"type": "image", "img_path": "dwYekpbmYG/tmp/bf6875ca6ce59f6f8d7b1dd158edeecf3d92c7bd009be424aee3f9d96cbf360e.jpg", "img_caption": ["Figure 4: Ablation study of the weight of PIM and SIM losses on the model performance. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "From Figure 4, it is evident that CATE enhances the performance of the base model ABMIL in most scenarios, particularly in out-of-domain settings. When the weight of PIM loss $\\lambda_{P}$ is too low, model performance suffers, underscoring the significant role of PIM loss in enhancing task-specific information. Conversely, an excessively high $\\lambda_{P}$ also diminishes performance, as the model overly prioritizes maximizing mutual information between the original and calibrated features at the expense of optimizing classification loss. Regarding the weight of SIM loss $\\lambda_{S}$ , optimal performance is achieved when $\\lambda_{S}$ is approximately 30. If the $\\lambda_{S}$ is too low, the model fails to effectively eliminate irrelevant information, thereby impairing performance. Conversely, if $\\lambda_{S}$ is too high, the model risks discarding task-relevant information, leading to performance degradation. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "image", "img_path": "dwYekpbmYG/tmp/4f8603ebdca6287e80b3e3be07c3b0a8663d314f4931b309a7120b5723e38b34.jpg", "img_caption": ["Figure 5: Ablation study of $k$ . "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "C.2 Ablation Study of the Number of Representative Patches $k$ ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we conduct an ablation study to investigate the impact of the number of representative patches, $k$ , on model performance, as discussed in Section 3.2. In practice, the representative patches are selected based on the similarities between the original image feature and the corresponding class-specific concept anchor. ", "page_idx": 16}, {"type": "text", "text": "As shown in Figure 5, it is evident that CATE generally enhances the performance of the base model, ABMIL. When the number of representative patches $k$ is too small, model performance degrades due to insufficient capture of task-specific information. Conversely, an excessively large $k$ also leads to performance degradation in out-of-domain scenarios, as it introduces noise from irrelevant information. Consequently, we have set the number of representative patches $k$ to 10 in the main paper. ", "page_idx": 16}, {"type": "text", "text": "C.3 Additional Ablation Study of CIB Module ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We further conducted experiments on CATE-MIL without concept alignment (discarding PIM loss and SIM loss of the CIB module) and replaced the CIB module with an MLP to investigate the effect of concept alignment and the increased number of parameters. The results are shown in Table 8. The performance of CATE-MIL significantly decreases in both cases, demonstrating the importance of concept alignment in the CIB module and that the improvements of CATE are not due to the increased number of parameters. ", "page_idx": 16}, {"type": "table", "img_path": "dwYekpbmYG/tmp/b6fcd8086efee7fa7771373e0492a85cff383098247a55ac1b9c419bf296af86.jpg", "table_caption": ["Table 8: Supplementary Ablation Study of CIB Module. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "D Datasets Description and Detailed Experimental Settings ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we provide detailed descriptions of the datasets used in the experiments and the detailed experimental settings that we used in the experiments in Section 4.2. ", "page_idx": 16}, {"type": "text", "text": "D.1 Datasets Description ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "\u2022 TCGA-BRCA: The dataset contains nine disease subtypes, and this study focuses on the classification of Invasive ductal carcinoma (IDC, 726 slides from 694 cases) and invasive lobular carcinoma (ILC, 149 slides from 143 cases). The dataset is collected from 36 sites with 20 of them having both IDC and ILC slides, and the other 16 sites only have IDC slides or ILC slides.   \n\u2022 TCGA-NSCLC: The dataset contains two disease subtypes, including lung adenocarcinoma (LUAD, 492 slides from 430 cases) and lung squamous cell carcinoma (LUSC, 466 slides from 432 cases). The dataset is collected from 66 sites. Different from TCGA-BRCA, each site only contains one disease subtype.   \n\u2022 TCGA-RCC: The dataset contains three disease subtypes, including clear cell renal cell carcinoma (CCRCC, 498 slides from 492 cases), papillary renal cell carcinoma (PRCC, 289 slides from 267 cases), and chromophobe renal cell carcinoma (CHRCC, 118 slides from 107 cases). The dataset is collected from 55 sites. Similar to TCGA-NSCLC, each site only contains one disease subtype. ", "page_idx": 17}, {"type": "text", "text": "D.2 Detailed Experimental Settings ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "To validate that the proposed CATE effectively improves MIL model performance by enhancing the task-specific information and eliminating the disturbance of task-irrelevant information, we conduct extensive experiments across three datasets under various in-domain and out-of-domain settings. ", "page_idx": 17}, {"type": "text", "text": "Specifically, for TCGA-BRCA dataset, we conduct the following experiments: ", "page_idx": 17}, {"type": "text", "text": "\u2022 BRCA $\\!N_{\\mathrm{IND}}\\!\\!=\\!\\!1;$ ): One site as the in-domain data, and the other sites as the out-of-domain data. The details of the in-domain site are shown below: \u2013 D8: 59 slides of IDC and 8 slides of ILC.   \n\u2022 BRCA $\\!\\!N_{\\mathrm{IND}}\\!\\!=\\!\\!2,$ ): Two sites as the in-domain data, and the other sites as the out-of-domain data. The details of the in-domain sites are shown below: \u2013 A8: 48 slides of IDC and 5 slides of ILC. \u2013 D8: 59 slides of IDC and 8 slides of ILC. ", "page_idx": 17}, {"type": "text", "text": "For TCGA-NSCLC dataset, we conduct the following experiments: ", "page_idx": 17}, {"type": "text", "text": "\u2022 NSCLC $\\!\\!N_{\\mathrm{IND}}\\!\\!=\\!\\!2\\!,$ : Two sites as the in-domain data, and the other sites as the out-of-domain data. The details of the in-domain sites are shown below: \u2013 44: 43 slides of LUAD and 0 slides of LUSC. \u2013 22: 0 slides of LUAD and 36 slides of LUSC.   \n\u2022 NSCLC $\\mathcal{N}_{\\mathrm{IND}}{=}4)$ : Four sites as the in-domain data, and the other sites as the out-of-domain data. The details of the in-domain sites are shown below: \u2013 44: 43 slides of LUAD and 0 slides of LUSC. \u2013 50: 20 slides of LUAD and 0 slides of LUSC. \u2013 22: 0 slides of LUAD and 36 slides of LUSC. \u2013 56: 0 slides of LUAD and 35 slides of LUSC. ", "page_idx": 17}, {"type": "text", "text": "For TCGA-RCC dataset, we conduct the following experiments: ", "page_idx": 17}, {"type": "text", "text": "\u2022 RCC ( $\\mathrm{\\Delta}N_{\\mathrm{IND}}{=}3)$ ): Three sites as the in-domain data, and the other sites as the out-of-domain data. The details of the in-domain sites are shown below: \u2013 A3: 48 slides of CCRCC, 0 slides of CHRCC, and 0 slides of PRCC. \u2013 KL: 0 slides of CCRCC, 24 slides of CHRCC, and 0 slides of PRCC. \u2013 2Z: 0 slides of CCRCC, 0 slides of CHRCC, and 23 slides of PRCC.   \n\u2022 RCC $\\Delta_{\\mathrm{IND}}{=}6)$ : Six sites as the in-domain data, and the other sites as the out-of-domain data. The details of the in-domain sites are shown below: \u2013 A3: 48 slides of CCRCC, 0 slides of CHRCC, and 0 slides of PRCC. \u2013 AK: 15 slides of CCRCC, 0 slides of CHRCC, and 0 slides of PRCC.   \n\u2013 KL: 0 slides of CCRCC, 24 slides of CHRCC, and 0 slides of PRCC.   \n\u2013 KM: 0 slides of CCRCC, 20 slides of CHRCC, and 0 slides of PRCC.   \n\u2013 2Z: 0 slides of CCRCC, 0 slides of CHRCC, and 23 slides of PRCC.   \n\u2013 5P: 0 slides of CCRCC, 0 slides of CHRCC, and 15 slides of PRCC. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "E Detailed Definition and Formula Derivation ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "E.1 Definition of Sufficiency and Consistency ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this part, we will define the sufficiency and consistency in the context of the concept information bottleneck (CIB) module. ", "page_idx": 18}, {"type": "text", "text": "First, we define the mutual information between two random variables $X$ and $Y$ as: ", "page_idx": 18}, {"type": "equation", "text": "$$\nI(X;Y)=\\sum p(x,y)\\log{\\frac{p(x,y)}{p(x)p(y)}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Sufficiency: To quantify the requirement that the calibrated features should be maximally informative about the label information $\\mathbf{y}$ , we define the sufficiency as the mutual information between the label information and the calibrated features: ", "page_idx": 18}, {"type": "equation", "text": "$$\n{\\mathbf{y}}\\iff I({\\hat{\\mathbf{x}}};\\mathbf{y}|{\\hat{\\alpha}})=0\\iff I({\\hat{\\mathbf{x}}};\\mathbf{y})=I({\\hat{\\alpha}};\\mathbf{y}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The sufficiency definition requires that the calibrated features $\\hat{\\pmb{\\alpha}}$ encapsulate all information about the label information $\\mathbf{y}$ that is accessible from the original features $\\hat{\\pmb{x}}$ . In essence, the calibrated feature $\\hat{\\pmb{\\alpha}}$ of original feature $\\hat{\\pmb{x}}$ is sufficient for determining label $\\mathbf{y}$ if and only if the amount of information regarding the specific task is unchanged after the transformation. ", "page_idx": 18}, {"type": "text", "text": "Consistency: Since the image feature $\\hat{\\pmb x}$ and the concept anchor c are consistent in the pathology VLM, we posit that any representation containing all information accessible from both the original feature and the concept anchor also encompasses the necessary discriminative label information. Thus, we define the consistency between the concept anchor and the original feature as: ", "page_idx": 18}, {"type": "text", "text": "Consistency. c is consistent with $\\hat{\\pmb{x}}$ for $\\mathbf{y}\\iff I(\\mathbf{y};\\hat{\\mathbf{x}}|\\mathbf{c})=0$ ", "page_idx": 18}, {"type": "text", "text": "The consistency definition requires that the concept anchor c and the original feature $\\hat{\\pmb{x}}$ should be consistent with the label information $\\mathbf{y}$ . ", "page_idx": 18}, {"type": "text", "text": "E.2 Maximize Predictive Information with InfoNCE ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this part, we will prove that the predictive information for the concept anchor can be maximized by maximizing the InfoNCE mutual information lower bound, which is defined in Equation 4. As in [27], the optimal value for $f\\left(c,\\hat{\\alpha}_{i}\\right)$ should be proportional to the density ratio: ", "page_idx": 18}, {"type": "equation", "text": "$$\nf\\left(c,\\hat{\\pmb{\\alpha}}_{i}\\right)\\propto\\frac{p\\left(\\pmb{c},\\hat{\\pmb{\\alpha}}_{i}\\right)}{p\\left(\\pmb{c}\\right)p\\left(\\hat{\\pmb{\\alpha}}_{i}\\right)}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "And we can get: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T_{S F E}^{\\operatorname*{sup}}(\\bar{\\alpha}_{*})=\\frac{\\mathbb{E}}{\\alpha_{S F}}\\left[\\displaystyle\\sum_{i=1}^{k}\\log\\frac{\\bar{p}(\\bar{\\Theta}_{i},\\bar{\\alpha}_{i})}{\\sum_{s=1}^{k}\\bar{p}(\\bar{\\Theta}_{i}^{*},\\bar{\\alpha}_{i})(\\pi_{S})\\bar{p}(\\bar{\\alpha}_{i})}\\right]}\\\\ &{=-\\frac{\\mathbb{E}}{\\alpha_{S F}}\\left[\\displaystyle\\sum_{i=1}^{k}\\log\\left(1+\\frac{P\\left(\\bar{\\Theta}_{i}^{*}\\right)P(\\bar{\\Theta}_{i}^{*})}{P\\left(\\bar{\\Theta}_{i}^{*},\\bar{\\alpha}_{i}\\right)}\\left(\\sum_{j=p}^{p}\\bar{p}(\\bar{\\Theta}_{i}^{*})+\\frac{P}{\\bar{p}(\\bar{\\Theta}_{j}^{*})}\\bar{p}(\\bar{\\alpha}_{i})\\right)\\right)\\right]}\\\\ &{=-\\frac{\\mathbb{E}}{\\alpha_{S F}}\\left[\\displaystyle\\sum_{i=1}^{k}\\log\\left(1+\\frac{P\\left(\\bar{\\Theta}_{i}^{*}\\right)P(\\bar{\\Theta}_{i})}{P\\left(\\bar{\\Theta}_{i}^{*},\\bar{\\alpha}_{i}\\right)}\\left((m-1)\\frac{P\\left(\\bar{\\Theta}_{i}^{*}\\right)}{P\\left(\\bar{\\Theta}_{i}^{*}\\right)P\\left(\\bar{\\Theta}_{i}^{*}\\right)}+\\frac{P}{\\bar{p}(\\bar{\\Theta}_{i}^{*})P\\left(\\bar{\\Theta}_{j}^{*})}\\bar{p}(\\bar{\\alpha}_{i})\\right)\\right)\\right]}\\\\ &{=-\\frac{\\mathbb{E}}{\\alpha_{S F}}\\left[\\displaystyle\\sum_{i=1}^{k}\\log\\left(1+\\frac{P\\left(\\bar{\\Theta}_{i}^{*}\\right)P(\\bar{\\Theta}_{i})}{P\\left(\\bar{\\Theta}_{i}^{*},\\bar{\\alpha}_{i}\\right)}\\left((m-1)\\frac{P\\left(\\bar{\\Theta}_{i}^{*}\\right)}{P\\left(\\bar{\\Theta}_{i}^{*}\\right)P\\left(\\bar{\\Theta}_{i}^{*}\\right)}\\bar{p}(\\bar{\\alpha}_{i})\\right)+m\\frac{P\\left(\\bar{\\Theta}_{i}^{*}\\right)P(\\bar{ \n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $C_{\\mathrm{neg}}$ denotes the negative concepts, including both negative class concepts and other type concepts. We can see that the InfoNCE estimation is a lower bound of the mutual information between the concept anchor and the calibrated features: ", "page_idx": 19}, {"type": "equation", "text": "$$\nI\\left(c_{\\mathrm{pos}}^{\\mathrm{cs}};\\hat{\\alpha}\\right)\\geq I_{N C E}^{o p t}\\left(\\hat{\\alpha};\\mathbf{c}\\right)+k\\log\\left(m+n\\right)\\geq I_{N C E}\\left(\\hat{\\alpha};\\mathbf{c}\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Thus, the predictive information of calibrated features for the concept anchor can be maximized by maximizing the InfoNCE mutual information lower bound. ", "page_idx": 19}, {"type": "text", "text": "E.3 Superfluous Information Minimization ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "As shown in Equation 3, the mutual information between the original features and the calibrated features $I(\\hat{\\pmb{x}};\\hat{\\pmb{\\alpha}})$ can be decomposed into the mutual information $I(\\hat{\\pmb{\\alpha}};\\mathbf{c})$ between the calibrated features and the concept anchor and the mutual information $I(\\hat{\\pmb{x}};\\hat{\\pmb{\\alpha}}|\\pmb{\\mathrm{c}})$ between the concept anchor and the calibrated features: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l r l r}{I(\\hat{x};\\hat{\\alpha})=}&{{}}&{\\;\\;\\;I(\\hat{\\alpha};\\mathbf{c})}&{}&{{}+}&{\\;\\;\\;I(\\hat{x};\\hat{\\alpha}|\\mathbf{c})}&{}&{{}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "As introduced in section 3.2, the first item is the predictive information for the concept anchor, which can be maximized by maximizing the InfoNCE mutual information lower bound. Thus, the second superfluous information item can be minimized by minimizing $I(\\hat{{\\bf x}};\\hat{\\pmb{\\alpha}})$ . In practice, the superfluous information minimization is conducted on all patches in the subset $\\mathbf{x}$ , since the taskirrelevant information is distributed across all patches. Thus, the superfluous information can be minimized by minimizing $I(\\pmb{x};\\pmb{\\alpha})$ . As in [1], this mutual information can be represented as: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{{\\displaystyle I({\\pmb x};{\\pmb\\alpha})=\\int{\\pmb\\rho}\\left({\\pmb x},{\\pmb\\alpha}\\right)\\log\\frac{p\\left({\\pmb\\alpha}|{\\pmb x}\\right)}{p\\left({\\pmb\\alpha}\\right)}d{\\pmb x}d{\\pmb\\alpha}}}}\\\\ {{{\\displaystyle=\\int p\\left({\\pmb x},{\\pmb\\alpha}\\right)\\log p\\left({\\pmb\\alpha}|{\\pmb x}\\right)d{\\pmb x}d{\\pmb\\alpha}-\\int p\\left({\\pmb\\alpha}\\right)\\log p\\left({\\pmb\\alpha}\\right)d{\\pmb\\alpha}}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "However, the computing of the marginal distribution $\\begin{array}{r}{p\\left(\\pmb{\\alpha}\\right)=\\int p\\left(\\pmb{\\alpha}|\\pmb{x}\\right)p\\left(\\pmb{x}\\right)d\\pmb{x}}\\end{array}$ is intractable. Thus, we can let the Gaussian distribution $r\\left(\\alpha\\right)$ approximate the marginal distribution $p\\left(\\alpha\\right)$ . Since the Kullback-Leibler divergence between two distributions is non-negative, we can get: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\int p\\left(\\alpha\\right)\\log p\\left(\\alpha\\right)d\\alpha\\geq\\int p\\left(\\alpha\\right)\\log r\\left(\\alpha\\right)d\\alpha.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Thus, the mutual information $I(\\pmb{x};\\pmb{\\alpha})$ have the upper bound: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{I(x;\\alpha)\\leq\\int p\\left(x,\\alpha\\right)\\log p\\left(\\alpha|x\\right)d x d\\alpha-\\int p\\left(\\alpha\\right)\\log r\\left(\\alpha\\right)d\\alpha}}\\\\ {{\\qquad=\\int p\\left(x\\right)p\\left(\\alpha|x\\right)\\log\\frac{p\\left(\\alpha|x\\right)}{r\\left(\\alpha\\right)}d x d\\alpha.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "In practice, to compute this upper bound, we approximate distribution $p\\left(x\\right)$ with the empirical distribution: ", "page_idx": 20}, {"type": "equation", "text": "$$\np\\left(\\mathbf{\\boldsymbol{x}}\\right)=\\frac{1}{N}\\sum_{i=1}^{N}\\delta_{\\mathbf{\\boldsymbol{x}}_{i}}\\left(\\mathbf{\\boldsymbol{x}}\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Thus, the upper bound of the mutual information $I(\\pmb{x};\\pmb{\\alpha})$ can be computed as: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\int p\\left(x\\right)p\\left(\\alpha|x\\right)\\log\\frac{p\\left(\\alpha|x\\right)}{r\\left(\\alpha\\right)}d x d\\alpha\\approx\\frac{1}{N}\\sum_{i=1}^{N}q_{\\theta}\\left(\\alpha_{i}|x_{i}\\right)\\log\\frac{q_{\\theta}\\left(\\alpha_{i}|x_{i}\\right)}{r\\left(\\alpha_{i}\\right)},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $q_{\\theta}\\left(\\alpha|x\\right)$ is a variational distribution with parameter $\\theta$ to approximate $p\\left(\\alpha|x\\right)$ , and we implement this variational distribution with MLP: ", "page_idx": 20}, {"type": "equation", "text": "$$\nq_{\\theta}\\left(\\alpha|x\\right)=\\mathcal{N}\\left(\\alpha|M L P^{\\mu}\\left(\\pmb{x}\\right),M L P^{\\Sigma}\\left(\\pmb{x}\\right)\\right),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $M L P^{\\mu}$ and $M L P^{\\Sigma}$ are implemented with MLPs which output the mean $\\mu$ and covariance matrix $\\Sigma$ of the Gaussian distribution. In practice, we utilize the reparameterization trick [18] to sample from the Gaussian distribution to get an unbiased estimate of the gradient to optimize the opjective. Thus, the upper bound can be optimized by minimizing the KL divergence between the variational distribution $q_{\\theta}\\left(\\alpha|x\\right)$ and the Gaussian distribution $r\\left(\\alpha\\right)$ : ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\underset{\\theta}{\\operatorname*{min}}\\,\\mathbb{E}_{\\boldsymbol{x}}\\left[D_{K L}\\left(q_{\\theta}\\left(\\alpha\\middle|\\boldsymbol{x}\\right)\\middle|\\vert\\boldsymbol{r}\\left(\\alpha\\right)\\right)\\right].\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "F Explanation of CIB with Information Plane ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "To better understand the concept information bottleneck (CIB) method, we provide an explanation of CIB with the information plane [8, 6]. ", "page_idx": 20}, {"type": "image", "img_path": "dwYekpbmYG/tmp/96eecf572fbeb0fa16f27bb337b65bcae08dfb91ec40b5babf2245b2a3681666.jpg", "img_caption": ["Figure 6: Explanation of CIB module with Information Plane. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "As shown in Figure 6, the information plane is a two-dimensional space where the $\\mathbf{X}_{\\mathrm{}}$ -axis represents the mutual information between the original features and the calibrated features of CIB module $I(\\hat{\\pmb{x}};\\hat{\\pmb{\\alpha}})$ , and the y-axis represents the mutual information between the label information and the calibrated features $I(\\mathbf{y};{\\hat{\\mathbf{x}}})$ . The line colored orange in the information plane defines sufficiency, indicating where the calibrated features are maximally informative about the label information y. The consistency definition falls to the left of the sufficiency line, denoting a state where the calibrated features, containing all information from both the original feature and the concept anchor, also include the label information. The InfoMAX principle [9] states that the optimal calibrated feature should be maximally informative about the original feature. ", "page_idx": 20}, {"type": "text", "text": "Ideally, a good calibrated feature $\\hat{\\pmb{\\alpha}}$ should be maximally informative about the label information $\\mathbf{y}$ (the sufficiency line in the information plane) while minimally informative about the original features $\\hat{\\pmb{x}}$ (i.e., the ideal situation in the information plane). This situation can be achieved by directly supervised learning. However, the weakly supervised nature of MIL methods complicates this goal. As demonstrated in the ablation analysis in Section 4.4, if we directly perform superfluous information minimization without the guidance of concept anchor (i.e., predictive information maximization), the calibrated features will be less informative about the label information y and the model performance will be degraded significantly. To address this issue, the proposed CIB module introduces the concept anchor c to guide the learning process of information bottleneck for each instance. As shown in Figure 6, the CIB module could effectively reduce the mutual information between the original features and the calibrated features $I(\\hat{\\pmb{x}};\\hat{\\pmb{\\alpha}})$ while preserving the mutual information between the label information and the calibrated features $I(\\mathbf{y};\\hat{\\pmb{\\alpha}})$ . ", "page_idx": 21}, {"type": "text", "text": "G Prompts ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this section, we provide the detailed prompt templates and concepts for the datasets, which are used to generate the concept anchor for the CIB module. The prompt templates are shown in Table 9, and the concepts for TCGA-BRCA, TCGA-NSCLC, and TCGA-RCC are shown in Table 10, Table 11, and Table 12, respectively. The templates and the class-agnostic prompts are referred from the original paper of CONCH [25], and the class-specific prompts are generated by querying the LLM with the question such as \u2018In addition to tumor tissues, what types of tissue or cells are present in whole slide images of breast cancer?\u2019 The quality of LLM generated prompts has been demonstrated in several recent studies [29, 22]. In principle, we can use LLM (e.g., GPT-4) to generate reliable expert-designed prompts and further verified by pathologists. This strategy can ensure the scalability and reliability of the prompts. ", "page_idx": 21}, {"type": "table", "img_path": "dwYekpbmYG/tmp/545f1ff0976101d3ecc823078cf4aec853ecf7b434bb60ee3ceeff0e78bccd48.jpg", "table_caption": ["Table 9: Prompt Templates. "], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "dwYekpbmYG/tmp/0aaff21fdcdb6afb76b001a6018d4bb33318a3155c6c40309f8b430e854c1ead.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "dwYekpbmYG/tmp/610b86a6e5adff78026731bc701728ad11e8e8fccb585655f7ac19c393e57e66.jpg", "table_caption": ["Table 11: Concepts for TCGA-NSCLC. "], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "dwYekpbmYG/tmp/ee9b6e757673e3f529324ac8a8f83c5ea108a025ba0249de5ba3e9f89109977f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "H More Visualization ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "To further illustrate the effectiveness of the proposed CATE-MIL, we provide more visualization results in this section. The visualization results for IDC and ILC in TCGA-BRCA are shown in Figure 7 and Figure 8, respectively. ", "page_idx": 23}, {"type": "image", "img_path": "dwYekpbmYG/tmp/227249bb83f4224ccaf9b42b90104b6d64d9e8882e4da83d7b718a9b33f6951a.jpg", "img_caption": ["Figure 7: Visualization results for samples of IDC in TCGA-BRCA. (a) Attention heatmap of CATE-MIL. (b) Attention heatmap of original ABMIL. (c) similarity between the calibrated features and the corresponding class concept feature. (d) similarity between the original features and the corresponding class concept feature. (e) Original WSI. (f) UMAP visualization. "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "dwYekpbmYG/tmp/40e9a514bdeeb0cdc2810d61081e4abc4daa0c3530340c3f82698674d894ce55.jpg", "img_caption": ["Figure 8: Visualization results for samples of ILC in TCGA-BRCA. (a) Attention heatmap of CATE-MIL. (b) Attention heatmap of original ABMIL. (c) similarity between the calibrated features and the corresponding class concept feature. (d) similarity between the original features and the corresponding class concept feature. (e) Original WSI. (f) UMAP visualization. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 26}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: The paper discuss the limitations of the work. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 26}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: For each theoretical result, the paper provide the full set of assumptions and a complete (and correct) proof. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 27}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 27}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 28}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 28}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper report error bars suitably and correctly defined. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 28}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 29}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper provide sufficient information on the computer resources needed to reproduce the experiments. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 29}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: The research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 29}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper discuss both potential positive societal impacts and negative societal impacts of the work performed. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 30}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 30}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: New assets introduced in the paper well documented and is the documentation provided alongside the assets. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 31}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 31}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 31}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 32}]