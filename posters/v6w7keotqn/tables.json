[{"figure_path": "V6w7keoTqn/tables/tables_6_1.jpg", "caption": "Table 1: Comparison with state-of-the-art methods. \u266d denotes models trained on the GSV-Cities dataset. Due to the high quality of annotations in GSV-Cities, results from models marked with \u266d generally outperform those from their corresponding papers. In contrast, results from models without \u266d are reported in their respective papers.", "description": "This table compares the proposed EMVP model's performance on various visual place recognition datasets against other state-of-the-art methods.  It highlights the recall rate (R@1, R@5, R@10) achieved by each method on the MSLS Validation, Nordland, Pitts250k-test, and SPED datasets. The table also indicates whether a method was trained on the GSV-Cities dataset, noting that methods trained on this high-quality dataset generally perform better.", "section": "4.2 Main Results"}, {"figure_path": "V6w7keoTqn/tables/tables_6_2.jpg", "caption": "Table 1: Comparison with state-of-the-art methods. \u266d denotes models trained on the GSV-Cities dataset. Due to the high quality of annotations in GSV-Cities, results from models marked with \u266d generally outperform those from their corresponding papers. In contrast, results from models without \u266d are reported in their respective papers.", "description": "This table compares the EMVP model's performance with other state-of-the-art visual place recognition (VPR) methods on four standard datasets: MSLS Validation, Nordland, Pitts250k-test, and SPED.  The table is divided into two sections: single-stage methods and two-stage methods (those that also include a re-ranking stage).  Recall@K (R@K) for K=1, 5, and 10 is reported as the performance metric, showing the percentage of times the correct image was retrieved within the top K ranked results. The table highlights the superior performance of EMVP-L (the EMVP model using the ViT-L architecture) compared to other methods, especially those without the re-ranking step.", "section": "4.2 Main Results"}, {"figure_path": "V6w7keoTqn/tables/tables_8_1.jpg", "caption": "Table 1: Comparison with state-of-the-art methods. <sup>\u266d</sup> denotes models trained on the GSV-Cities dataset. Due to the high quality of annotations in GSV-Cities, results from models marked with <sup>\u266d</sup> generally outperform those from their corresponding papers. In contrast, results from models without <sup>\u266d</sup> are reported in their respective papers.", "description": "This table compares the proposed EMVP model's performance with other state-of-the-art (SOTA) visual place recognition (VPR) methods across four benchmark datasets: MSLS Validation, Nordland, Pitts250k-test, and SPED.  The comparison is done for single-stage and two-stage methods, and the table highlights the Recall@K (R@1, R@5, R@10) metric.  The superscript \u266d indicates that the model was trained on the GSV-Cities dataset, which is known for its high annotation quality, leading to better results. The table effectively shows EMVP's superior performance compared to existing methods.", "section": "4.2 Main Results"}, {"figure_path": "V6w7keoTqn/tables/tables_8_2.jpg", "caption": "Table 3: Comparing different fine-tuning methods. DPNC and DPNR indicate DPN in CFP and recalibration, respectively. Results of both parallel and sequential versions of DPNR are reported. For fairness, only the last 4 blocks can be fine-tuned, and all methods employ the same backbone, i.e., ViT-B. The best and the second best results are bolded and underlined, respectively.", "description": "This table compares different fine-tuning methods for visual place recognition.  It focuses on the impact of the Dynamic Power Normalization (DPN) module within the Centroid-Free Probing (CFP) stage and recalibration stage.  Both parallel and sequential versions of DPN in the recalibration stage are evaluated.  The table highlights the best performing methods while ensuring a fair comparison by using the same ViT-B backbone and only fine-tuning the last 4 blocks. The results (Recall@1, Recall@5, Recall@10) across the MSLS Validation, Nordland, Pittsburgh250k-test, and SPED datasets are shown, indicating the accuracy and efficiency of different methods.", "section": "4 Experiments"}, {"figure_path": "V6w7keoTqn/tables/tables_13_1.jpg", "caption": "Table 1: Comparison with state-of-the-art methods. \u266d denotes models trained on the GSV-Cities dataset. Due to the high quality of annotations in GSV-Cities, results from models marked with \u266d generally outperform those from their corresponding papers. In contrast, results from models without \u266d are reported in their respective papers.", "description": "This table compares the proposed EMVP model's performance with several state-of-the-art (SOTA) visual place recognition (VPR) methods on four benchmark datasets: MSLS Validation, NordLand, Pitts250k-test, and SPED.  The table is divided into two sections: (a) compares EMVP with single-stage methods (methods without a re-ranking stage), and (b) compares EMVP with two-stage methods (methods that include a re-ranking stage). The results are presented as Recall@K (R@K) values where K = 1, 5, and 10, indicating the percentage of times the correct image is retrieved within the top K retrieved images.", "section": "4 Experiments"}, {"figure_path": "V6w7keoTqn/tables/tables_13_2.jpg", "caption": "Table 1: Comparison with state-of-the-art methods. \u266d denotes models trained on the GSV-Cities dataset. Due to the high quality of annotations in GSV-Cities, results from models marked with \u266d generally outperform those from their corresponding papers. In contrast, results from models without \u266d are reported in their respective papers.", "description": "This table compares the proposed EMVP model's performance with other state-of-the-art visual place recognition (VPR) methods across four datasets: MSLS Validation, Nordland, Pitts250k-test, and SPED.  It reports Recall@K (R@K) values for K=1, 5, and 10, showing the percentage of correctly retrieved images within the top K retrieved images. The table is divided into two parts: single-stage methods and two-stage methods.  Single-stage methods directly produce a ranking of images, while two-stage methods use a ranking step followed by a reranking step to refine the ranking.  The high quality of annotations in the GSV-Cities dataset (used to train several models), allows for a fairer comparison of results reported by different papers.  The table highlights EMVP's superior performance.", "section": "4 Experiments"}, {"figure_path": "V6w7keoTqn/tables/tables_13_3.jpg", "caption": "Table 1: Comparison with state-of-the-art methods. \u266d denotes models trained on the GSV-Cities dataset. Due to the high quality of annotations in GSV-Cities, results from models marked with \u266d generally outperform those from their corresponding papers. In contrast, results from models without \u266d are reported in their respective papers.", "description": "This table compares the proposed EMVP model's performance with other state-of-the-art visual place recognition (VPR) methods on four benchmark datasets: MSLS Validation, NordLand, Pitts250k-test, and SPED.  The results are presented as Recall@K (R@K) where K represents 1, 5, and 10, indicating the top K retrieved images.  The table is divided into two sections: one comparing single-stage methods and another comparing two-stage methods that include a re-ranking stage. The table highlights that the EMVP model achieves superior performance compared to the other methods across all datasets.", "section": "4.2 Main Results"}, {"figure_path": "V6w7keoTqn/tables/tables_14_1.jpg", "caption": "Table 1: Comparison with state-of-the-art methods. \u266d denotes models trained on the GSV-Cities dataset. Due to the high quality of annotations in GSV-Cities, results from models marked with \u266d generally outperform those from their corresponding papers. In contrast, results from models without \u266d are reported in their respective papers.", "description": "This table compares the proposed EMVP model's performance with other state-of-the-art visual place recognition (VPR) methods across four benchmark datasets (MSLS Validation, NordLand, Pitts250k-test, and SPED).  The results are presented in terms of Recall@K (R@1, R@5, R@10), showing the percentage of correctly retrieved images among the top 1, 5, and 10 retrieved images for each query image. The table also distinguishes between single-stage and two-stage methods, highlighting the impact of a re-ranking stage on performance.  The use of the GSV-Cities dataset for training is noted as a factor influencing performance comparison.", "section": "4.2 Main Results"}, {"figure_path": "V6w7keoTqn/tables/tables_14_2.jpg", "caption": "Table 7: Impact of the output sizes of Fc (K = 64).", "description": "This table presents the ablation study on the output size of Fc (feature dimension) while keeping K (number of semantic centroids) constant at 64.  It shows the Recall@1, Recall@5, and Recall@10 for the MSLS Validation and Pitts250k-test datasets with different values of D (dimension of Fc). This helps analyze the impact of the feature dimension on the performance of visual place recognition.", "section": "4.3 Ablation Studies"}, {"figure_path": "V6w7keoTqn/tables/tables_14_3.jpg", "caption": "Table 8: Impact of the output sizes of Fp (D = 128).", "description": "This table shows the impact of varying the output size of Fp (K) on the performance of the EMVP model, measured by Recall@1, Recall@5, and Recall@10 on the MSLS Validation and Pitts250k-test datasets.  The dimension of Fc (D) is fixed at 128.  The results suggest that the model's performance is relatively stable across a range of K values.", "section": "4.3 Ablation Studies"}, {"figure_path": "V6w7keoTqn/tables/tables_15_1.jpg", "caption": "Table 9: Impact of the number of recalibrated blocks.", "description": "This table shows the impact of the number of recalibrated blocks on the performance of the EMVP model. The model is evaluated using Recall@1, Recall@5, and Recall@10 metrics on the MSLS Validation and Pitts250k-test datasets. The results show that recalibrating the features in the last 4 blocks leads to the best performance, suggesting that focusing the fine-tuning on specific layers of the network is beneficial for improving the accuracy and efficiency of the VPR model.  Recalibrating fewer or more blocks results in lower performance.", "section": "4.3 Ablation Studies"}, {"figure_path": "V6w7keoTqn/tables/tables_15_2.jpg", "caption": "Table 10: Comparing different ViT models. Tr. and Ttl. represent the number of trainable and total parameters (M), respectively.", "description": "This table compares the performance of different sized Vision Transformer (ViT) models (ViT-S, ViT-B, ViT-L) on two Visual Place Recognition (VPR) datasets (MSLS Validation and Pitts250k-test).  The results are shown as Recall@K (R@1, R@5, R@10).  The table also lists the number of trainable and total parameters (in millions) for each model, indicating the model's size and complexity.", "section": "4 Experiments"}]