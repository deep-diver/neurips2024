[{"type": "text", "text": "How Does Variance Shape the Regret in Contextual Bandits? ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Zeyu Jia Massachusetts Institute of Technology zyjia@mit.edu ", "page_idx": 0}, {"type": "text", "text": "Jian Qian Massachusetts Institute of Technology jianqian@mit.edu ", "page_idx": 0}, {"type": "text", "text": "Alexander Rakhlin Massachusetts Institute of Technology rakhlin@mit.edu ", "page_idx": 0}, {"type": "text", "text": "Chen-Yu Wei University of Virginia chenyu.wei@virginia.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We consider realizable contextual bandits with general function approximation, investigating how small reward variance can lead to better-than-minimax regret bounds. Unlike in minimax regret bounds, we show that the eluder dimension $d_{\\mathrm{elu}}$ \u2014a measure of the complexity of the function class\u2014plays a crucial role in variance-dependent bounds. We consider two types of adversary: ", "page_idx": 0}, {"type": "text", "text": "\u2022 Weak adversary: The adversary sets the reward variance before observing the learner\u2019s action. In this setting, we prove that a regret of $\\Omega(\\sqrt{\\operatorname*{min}\\{A,d_{\\mathrm{elu}}\\}\\Lambda}+$ $d_{\\mathrm{elu}},$ is unavoidable when $d_{\\mathrm{elu}}\\leq{\\sqrt{A T}}$ , where $A$ is the number of actions, $T$ is the total number of rounds, and $\\Lambda$ is the total variance over $T$ \u221arounds. For the $A\\le d_{\\mathrm{elu}}$ regime, we derive a nearly matching upper bound $\\tilde{\\mathcal{O}}(\\sqrt{A\\Lambda}+d_{\\mathrm{elu}})$ for the special case where the variance is revealed at the beginning of each round. ", "page_idx": 0}, {"type": "text", "text": "\u2022 Strong adversary: The adversary sets the re\u221award variance after observing the learner\u2019s action. We\u221a show that a regret of $\\Omega(\\sqrt{d_{\\mathrm{elu}}\\Lambda}+d_{\\mathrm{elu}})$ is unavoidable when $\\sqrt{d_{\\mathrm{elu}}\\Lambda}+d_{\\mathrm{elu}}\\,\\le\\,\\sqrt{A T}$ . In this setting, we provide an upper bound of order $\\tilde{\\mathcal{O}}(d_{\\mathrm{elu}}\\sqrt{\\Lambda}+d_{\\mathrm{elu}})$ . ", "page_idx": 0}, {"type": "text", "text": "Furthermore, we examine the setting where the function class additionally provides distributional information of the reward, as studied by Wang et al. (2024). We demonstrate that the regret bound $\\tilde{\\mathcal{O}}(\\sqrt{d_{\\mathrm{elu}}\\Lambda}+d_{\\mathrm{elu}})$ established in their work is unimprovable when $\\sqrt{d_{\\mathrm{elu}}\\Lambda}+d_{\\mathrm{elu}}\\,\\le\\,\\sqrt{A T}$ . However, with a slightly different definition of the total variance and with the assumptio\u221an that the reward follows a Gaussian distribution, one can achieve a regret of $\\dot{\\tilde{\\mathcal{O}}}(\\sqrt{A\\Lambda}+d_{\\mathrm{elu}})$ . ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We consider the contextual bandit problem that models repeated interactions between the learner and the environment. In each round, the learner chooses an action based on the received context, and observes the reward of the chosen action. Algorithms designed to achieve minimax regret guarantees under a variety of statistical assumptions and computational models have been extensively studied (Auer et al., 2002; Dudik et al., 2011; Agarwal et al., 2012, 2014; Foster and Rakhlin, 2020; Xu and Zeevi, 2020; Simchi-Levi and Xu, 2022; Zhang, 2022). ", "page_idx": 0}, {"type": "text", "text": "However, these algorithms often fail to leverage the potentially benign nature of the environment. In this work, we refine the regret bound by considering the variance of the reward. Such variancedependent regret bounds, also known as second-order regret bounds, have been primarily studied under linear function approximation (Zhang et al., 2021; Kim et al., 2022; Zhao et al., 2023). Notably, ", "page_idx": 0}, {"type": "text", "text": "Zhao et al. (2023) first established a near-optimal $\\tilde{\\mathcal{O}}(d\\sqrt{\\Lambda}+d)$ regret bound for linear contextual bandits, where $d$ represents the feature dimension and $\\Lambda$ the sum of the reward variances. ", "page_idx": 1}, {"type": "text", "text": "For contextual bandits with general function approximation, the recent work by Wang et al. (2024) obtained a second-order bound assuming access to a model class containing distributional information about the reward. They showed a regret bound of $\\tilde{\\mathcal{O}}(\\sqrt{d_{\\mathrm{elu}}\\Lambda\\log|\\mathcal{M}|}+d_{\\mathrm{elu}}\\log|\\mathcal{M}|)$ , where $|{\\mathcal{M}}|$ is the size of the model class, and $d_{\\mathrm{elu}}=d_{\\mathrm{elu}}(\\mathcal{M})$ is its eluder dimension. As noted in Wang et al. (2024), the dependence on $d_{\\mathrm{elu}}$ is undesirable, and when the number of actions $A$ is much smaller than $d_{\\mathrm{elu}}$ , this bound can potentially be improved. This conjecture is supported by Foster and Rakhlin (2020), who showed that the upper bound $\\tilde{\\mathcal{O}}(\\sqrt{A T\\log|\\dot{\\mathcal{F}}|}+A\\log|\\dot{\\mathcal{F}}|)$ is achievable regardless of the eluder dimension, where $T\\geq\\Lambda$ is the number of rounds, and $|{\\mathcal{F}}|$ is the size of the function class containing mean reward information. It is tempting to conjecture that the regret can smoothly scale with $\\Lambda$ , resulting in a bound of $\\tilde{\\mathcal{O}}(\\sqrt{A\\Lambda\\log|\\bar{\\mathcal{F}}|}+\\bar{A}\\log|\\bar{\\mathcal{F}}|)$ . Such variance-dependent regret bounds that replace the dependence on the number of rounds $T$ by the total variance $\\Lambda$ have been shown in multi-armed bandits (Audibert et al., 2009), linear bandits (Ito and Takemura, 2023), and linear contextual bandits (Zhao et al., 2023). ", "page_idx": 1}, {"type": "text", "text": "In this paper, we show, surprisingly, tha\u221at the aforementioned conjecture is not true in general. Specifically, for any $A$ and any $\\bar{d_{\\mathrm{elu}}}\\,\\leq\\,\\sqrt{A T}$ , one can construct a problem instance with lower bound $\\Omega(\\sqrt{\\operatorname*{min}\\{A,d_{\\mathrm{elu}}\\}\\Lambda}+d_{\\mathrm{elu}})$ with $\\log|{\\mathcal{F}}|\\,=\\,{\\mathcal{O}}(\\log T)$ and $d_{\\mathrm{{elu}}}(\\mathcal{F})\\,=\\,d_{\\mathrm{{elu}}}$ . This rules out the possibility of achieving $\\tilde{\\mathcal{O}}(\\sqrt{A\\Lambda\\log|\\mathcal{F}|}+A\\log|\\mathcal{F}|)$ for all $A$ because we can always make $d_{\\mathrm{elu}}=\\sqrt{A T}$ , resulting in a lower bound $\\Omega({\\sqrt{A T}})$ even with $\\Lambda=0$ . Our primary goal is to design algorithms that achieve the near-optimal regret bound $\\tilde{\\mathcal{O}}(\\sqrt{\\operatorname*{min}\\{A,d_{\\mathrm{elu}}\\}\\Lambda\\log|\\mathcal{F}|}+d_{\\mathrm{elu}}\\log|\\mathcal{F}|)$ . ", "page_idx": 1}, {"type": "text", "text": "The lower bound $\\sqrt{\\operatorname*{min}\\{A,d_{\\mathrm{elu}}\\}\\Lambda}+d_{\\mathrm{elu}}$ indicates that the complexity of contextual bandits arises from two parts. The first part accounts for local estimation of the true function, where the complexity is due to the variance of the reward and the local structure of the function set around the ground truth function $f^{\\star}\\in{\\mathcal{F}}$ . This results in the term $\\sqrt{\\operatorname*{min}\\{A,d_{\\mathrm{elu}}\\}\\Lambda}$ , with the leading coefficient $\\operatorname*{min}\\{A,d_{\\mathrm{elu}}\\}$ corresponding to the decision-estimation coefficient (Foster et al., 2021). The second part accounts for global search for the true function, in which the complexity is due to a more global structure of the function set and can be quantified by the disagreement among the functions. The complexity of this part scales with $d_{\\mathrm{elu}}$ , even when $A=2$ . The contribution of the global part is usually overshadowed by the local part when only considering regret bounds with constant variance. Our work highlights its role by studying the variance-dependent bound. The fundamental role of disagreement is also discussed in Foster et al. (2020) for gap-dependent bounds. Specifically, they also showed that when trying to obtain the gap-dependent bound that has logarithmic dependence on $T$ , the complexity must scale with some disagreement measure over the function class, instead of just the number of actions. ", "page_idx": 1}, {"type": "text", "text": "The previous work by Wei et al. (2020) also derived a set of results for general contextual bandits showing that the tight second-order regret bound is strictly larger than merely replacing the $T$ in the minimax bound by the second-order error. They consider the more general agnostic setting but the tight regret bounds are only established for the $|\\mathcal F|=1$ case. Their result for $|\\mathcal F|>1$ can be applied to our setting, though it only gives highly sub-optimal bounds. Overall, our work refines theirs in the realizable setting. ", "page_idx": 1}, {"type": "text", "text": "When preparing our camera-ready version, the concurrent work of Pacchiano (2024), which studied exactly the same problem as ours, was posted on arXiv. We provide a comparison with their work in Section 3.4. More related works are discussed in Appendix A. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "A contextual bandit problem consists of a context space $\\mathcal{X}$ , an action space $\\boldsymbol{\\mathcal{A}}$ , the total number of rounds $T$ and a class of functions $\\mathcal{F}\\subset[0,1]^{\\mathcal{X}\\times\\mathcal{A}}$ . At round $t$ , the learner observes a context $x_{t}\\in\\mathscr{X}$ , then makes a decision $a_{t}\\in\\mathcal A$ based on the current context $x_{t}$ and history, and observes a reward $r_{t}$ . We assume that these rewards $r_{t}$ are given by ", "page_idx": 1}, {"type": "equation", "text": "$$\nr_{t}=f^{\\star}(x_{t},a_{t})+\\epsilon_{t},\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $f^{\\star}:\\mathcal{X}\\times\\mathcal{A}\\rightarrow[0,1]$ is some function unknown to the learner, and $\\epsilon_{t}$ are independent zeromean random variables with variance $\\sigma_{t}^{2}$ such that $r_{t}\\in[0,1]$ .1 We denote $\\textstyle\\Lambda=\\sum_{t=1}^{T}\\sigma_{t}^{2}$ . The learner ", "page_idx": 1}, {"type": "text", "text": "aims to optimize the total expected regret $R_{T}$ , defined as ", "page_idx": 2}, {"type": "equation", "text": "$$\nR_{T}=\\sum_{t=1}^{T}\\left(\\operatorname*{max}_{a\\in\\cal A}f^{\\star}(x_{t},a)-f^{\\star}(x_{t},a_{t})\\right).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "We make the following realizability assumption: ", "page_idx": 2}, {"type": "text", "text": "Assumption 2.1 (Function Realizability). Assume that $f^{\\star}$ in Eq. (1) satisfies $f^{\\star}\\in{\\mathcal{F}}$ . ", "page_idx": 2}, {"type": "text", "text": "We finish this section with the definition of eluder dimension: ", "page_idx": 2}, {"type": "text", "text": "Definition 2.1 (Eluder Dimension (Russo and Van Roy, 2014)). For function class $\\mathcal{F}$ defined on space $\\mathcal{Z}$ , we define the eluder dimension of $\\mathcal{F}$ at scale $\\alpha\\geq0$ , denoted by $d_{e l u}(\\mathcal{F};\\alpha)$ , as the length of the longest sequence of tuples $(z_{1},f_{1},f_{1}^{\\prime}),...,(z_{m},f_{m},f_{m}^{\\prime})\\in\\mathcal{Z}\\times\\mathcal{F}\\times\\mathcal{F}$ such that there exists $\\alpha_{0}\\geq\\alpha$ making the following hold for all $i=1,..,m$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\sum_{j<i}(f_{i}(z_{j})-f_{i}^{\\prime}(z_{j}))^{2}\\leq\\alpha_{0}^{2},\\;a n d\\,|f_{i}(z_{i})-f_{i}^{\\prime}(z_{i})|>\\alpha_{0}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Throughout the paper, if $\\alpha$ is not specified, we take the default value $\\alpha={1}/{T^{2}}$ . We also omit the dependence on $\\mathcal{F}$ when it is clear from the context. ", "page_idx": 2}, {"type": "text", "text": "3 Results Overview ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We describe our three settings in the following three subsections and summarize the results in Table 1. In the following, $\\mathcal{F}$ denotes the function class that only contains reward mean information, and $\\mathcal{M}$ the model class that contains reward distribution information. ", "page_idx": 2}, {"type": "text", "text": "3.1 Weak Adversary Case with Variance Revealing (Section 4) ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "First, we consider the case where the adversary is weak. This means that the variance $\\sigma_{t}$ only depends on the history up to round $t-1$ , which aligns with the standard \u201cadaptive adversary\u201d assumption. For this case, we show that for any $A$ and $d$ , one can find an instance of contextual problem problem with $|{\\mathcal{F}}|\\leq{\\sqrt{A T}}$ , such that the regret is at least ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\Omega\\left(\\sqrt{\\operatorname*{min}\\{A,d_{\\mathrm{elu}}\\}\\Lambda}+\\operatorname*{min}\\{d_{\\mathrm{elu}},\\sqrt{A T}\\}\\right).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "For upper bounds in the weak adversary case, we focus on the regime $A\\leq d_{\\mathrm{elu}}\\leq\\sqrt{A T}$ , where the lower bound can be written as $\\Omega(\\sqrt{A\\Lambda}+d_{\\mathrm{elu}})$ .2 While our ultimate goal is to obtain a nearly matching upper bound $\\tilde{\\mathcal{O}}(\\sqrt{A\\Lambda\\log|\\mathcal{F}|}+d_{\\mathrm{elu}}\\log|\\mathcal{F}|)$ , we have not achieved it yet in full generality. In Section 4, we provide an algorithm which operates under the assumption that the variance $\\sigma_{t}$ is revealed to the learner at the beginning of round $t$ , and show that it achieves the matching upper bound. An initial attempt to remove this assumption is discussed in Section 7, where we show that when $\\sigma_{t}\\in\\{0,1\\}$ for all $t$ and $\\sigma_{t}$ is revealed to the learner at the end of round $t$ , the matching upper bound can also be achieved. ", "page_idx": 2}, {"type": "text", "text": "3.2 Strong Adversary Case (Section 5) ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Next, we consider the case where the adversary is strong. This means the adversary can decide $\\sigma_{t}$ after seeing the action $a_{t}$ chosen by the learner at round $t$ . In this case, the lower bound becomes ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\Omega\\left(\\operatorname*{min}\\left\\{\\sqrt{d_{\\mathrm{elu}}\\Lambda}+d_{\\mathrm{elu}},\\sqrt{A T}\\right\\}\\right).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The difference with Eq. (2) is that the scaling in front of $\\Lambda$ changes from $\\operatorname*{min}\\{A,d_{\\mathrm{elu}}\\}$ to $d_{\\mathrm{elu}}$ . This shows the even more crucial role of eluder dimension in the strong adversary case. For this setting, we give an upper bound of $\\tilde{\\mathcal{O}}(d_{\\mathrm{elu}}\\sqrt{\\Lambda\\log{|\\mathcal{F}|}}+d_{\\mathrm{elu}}\\log{|\\mathcal{F}|})$ , which is off from the lower bound by a $\\sqrt{d_{\\mathrm{elu}}}$ factor along with other logarithmic factors. ", "page_idx": 2}, {"type": "text", "text": "Table 1: Results overview. $d_{\\mathrm{elu}}$ refers to either $d_{\\mathrm{elu}}(\\mathcal{F})$ or $d_{\\mathrm{elu}}(\\mathcal{M})$ depending on the settings. In Section 4 and Section 5, $\\begin{array}{r}{\\Lambda\\;:=\\;\\sum_{t=1}^{T}\\sigma_{t}^{2}}\\end{array}$ . In Section 6, $\\begin{array}{r}{\\Lambda_{\\infty}\\,:=\\,\\sum_{t=1}^{T}\\operatorname*{max}_{a}\\sigma_{M^{\\star}}(x_{t},a)^{2}}\\end{array}$ and $\\begin{array}{r}{\\Lambda_{\\circ}:=\\sum_{t=1}^{T}\\sigma_{M_{\\bullet}^{\\star}}(x_{t},a_{t})^{2}}\\end{array}$ , where $M^{\\star}\\,\\in\\,{\\mathcal{M}}$ is the underlying true model, and $\\sigma_{M}(x,a)^{2}$ is the reward variance for $(x,a)$ predicted by model $M$ . ", "page_idx": 3}, {"type": "text", "text": "Notice: For simplicity, this tabl\u221ae only considers the case $A\\le d_{\\mathrm{elu}}$ , and all lower bounds should be further taken a minimum with $\\sqrt{A T}$ . ", "page_idx": 3}, {"type": "table", "img_path": "32Z3nfCnwa/tmp/3b2c11b2ede70edb618f6ee18bf679264a253d7084884d82f5ddd227ac653e6a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "3.3 Learning with a Model Class (Section 6) ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In Section 6, we assume that the function class provides information on the distribution of the reward rather than just the mean. Such a function class is usually called a model class. More precisely, the learner is provided with a model class $\\mathcal{M}$ that includes the true model $M^{\\star}\\,\\in\\,{\\mathcal{M}}$ so that $\\dot{M}^{\\star}(x,a)$ specifies reward distribution for the context-action pair $(x,a)$ . Compared to the scenario studied in Section 4, here we do not require variance to be revealed to the learner. This becomes possible because with a model class, the learner can now obtain variance information (though not precise) through the context. Under the assumption that the noise is Gaussian, we provide an $\\bar{\\tilde{\\mathcal{O}}}(\\sqrt{A\\Lambda_{\\infty}\\log\\bar{|\\mathcal{M}|}}+d_{\\mathrm{elu}}\\log\\vert\\mathcal{M}\\vert)$ upper bound where $\\begin{array}{r}{\\Lambda_{\\infty}=\\sum_{t}\\operatorname*{max}_{a}\\sigma_{M^{\\star}}(x_{t},a)^{2}}\\end{array}$ , and a matching lower bound, where $\\sigma_{M}(x,a)$ is the reward variance for the context-action pair $(x,a)$ predicted by $M\\in\\mathcal{M}$ . ", "page_idx": 3}, {"type": "text", "text": "The work of Wang et al. (2024) also studied second-order contextual bandits with a model class. They use $\\begin{array}{r}{\\Lambda_{\\circ}\\,=\\,\\sum_{t}\\sigma_{M\\star}(x_{t},a_{t})^{2}}\\end{array}$ , i.e., the reward variance of the chosen actions, as the variance measure. They obtain $\\tilde{\\mathcal{O}}(\\sqrt{d_{\\mathrm{elu}}\\Lambda_{\\circ}\\log|\\mathcal{M}|}+d_{\\mathrm{elu}}\\log|\\mathcal{M}|)$ upper bound. We show a nearly matching lower bound $\\Omega(\\operatorname*{min}\\{\\sqrt{d_{\\mathrm{elu}}\\Lambda_{\\circ}}+d_{\\mathrm{elu}},\\sqrt{A T}\\})$ , similar to the lower bound for the strong adversary case studied in Section 5. The lower bound indicates that, in general, the bound of Wang et al. (2024) cannot be improved even when $A<d_{\\mathrm{elu}}$ . ", "page_idx": 3}, {"type": "text", "text": "3.4 Comparison with Pacchiano (2024) ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The work of Pacchiano (2024) also studied variance-dependent bounds for realizable contextual bandits. They also consider two settings, which can be mapped to those in our Section 4 and Section 5, respectively. For the weak adversary setting with revealed $\\sigma_{t}$ (Section 4), they give an upper bound of $\\tilde{\\mathcal{O}}(\\sqrt{d_{\\mathrm{elu}}\\Lambda\\log\\vert\\mathcal{F}\\vert}+d_{\\mathrm{elu}}\\log\\vert\\mathcal{F}\\vert)$ ,3 which is incomparable to our $\\tilde{\\mathcal{O}}(\\sqrt{A\\Lambda\\log|\\mathcal{F}|}+d_{\\mathrm{elu}}\\log|\\mathcal{F}|)$ . However, a full picture of this setting can be obtained by combining their upper bound and our upper bound and the lower bound in Eq. (2). For the strong adversary setting (Section 5), they derive exactly the same upper bound as in our Theorem 5.2. Our work makes additional contribution in the lower bounds and the extension to the distributional setting (Section 6). ", "page_idx": 3}, {"type": "text", "text": "4 Weak Adversary Case with Variance Revealing ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we consider cases where the variance $\\sigma_{t}^{2}$ at round $t$ is given to the learner at the beginning of round $t$ together with the context $x_{t}$ . ", "page_idx": 3}, {"type": "text", "text": "4.1 Lower Bound ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The regret lower bound is shown with identical and known variance. The construction is similar to those in Wei et al. (2020). Concretely, we have the following theorem. ", "page_idx": 4}, {"type": "text", "text": "Theorem 4.1 (Main lower bound). For any integer $d,A\\geq2,$ , any positive real number $\\sigma\\in[0,1]$ , and time $T>0$ , there exists a context space $\\mathcal{X}$ and a contextual bandit problem $\\mathcal{F}\\subset(\\mathcal{X}\\times\\mathcal{A}\\rightarrow\\mathbb{R})$ with eluder dimension $d_{e l u}(0)\\leq d_{\\varepsilon}$ , action set $\\boldsymbol{\\mathcal{A}}$ with $|{\\mathcal{A}}|\\leq A$ , and variance $\\sigma_{t}\\leq\\sigma$ for all $t\\in[T]$ such that any algorithm will suffer a regret at least $\\Omega(\\sqrt{\\sigma^{2}\\operatorname*{min}\\{A,d\\}T}+\\operatorname*{min}\\{d,\\sqrt{A T}\\})$ . ", "page_idx": 4}, {"type": "text", "text": "Proof sketch. The full proof is deferred to Appendix C. The two parts in the lower bound came from the following two different hardness: (1) The first part of the lower bound with $\\Omega(\\sqrt{\\sigma^{2}\\operatorname*{min}\\{A,d\\}T})$ is a natural lower bound with variance $\\sigma$ due to estimation of the mean values. (2) For the second part, we consider the following function class. In this function class, there is a \u201cgood\u201d action that serves as the default choice with a reward of $1/2$ for all contexts. For each of the other $A-1$ \u201cbad\u201d actions, for each contex\u221at, there is one function that obtains a reward of 1 but obtains 0 for all the other contexts. When $d<{\\sqrt{A T}}$ , this function class forces the learner to guess for each context which action to choose. So even if the reward is deterministic, i.e., variance $\\sigma=0$ , any learner would have to suffer a regret scaling with the number of co\u221antexts times the number of actions, which in total coincide with the \u221aeluder dimension. When $d\\geq{\\sqrt{A T}}$ , the learner can simply commit to the \u201cgood\u201d action and suffer $\\sqrt{A T}$ but no better than this. \u53e3 ", "page_idx": 4}, {"type": "text", "text": "This lower bound is rather surprising for the following consequences: (1) The most significant implication from this lower bound is th\u221aat improving the minimax r\u221aegret bound with the knowledge of the variance is only possible if $d<{\\sqrt{A T}}$ . (2) Even when $d<\\sqrt{A T}$ , any learner would have to pay for the eluder dimension as a lower-order term. These are non-trivial because the second-order bounds are usually obtained from changing Hoeffding concentration to Bernstein concentration which usually only scales the regret bounds by $\\sigma$ . This lower bound shows that the second-order contextual bandit is not one of the usual cases. In the next section, we will match this lower bound from the upper bound side by combining several algorithmic techniques. ", "page_idx": 4}, {"type": "text", "text": "4.2 Upper Bound with Known and Fixed Variance ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Motivated by the lower bound in Theorem 4.1, we wonder whether ther\u221ae is an algorithm which can achieve a matching upper bound of $\\tilde{\\mathcal{O}}(\\sqrt{\\sigma^{2}\\operatorname*{min}\\{A,d\\}T}+\\operatorname*{min}\\{d,\\sqrt{A T}\\})$ , if the learner is provided with information of variance at the beginning of each round. In this subsection, we answer this question affirmly. To begin with, we consider the case when all the variance are identical, i.e. $\\sigma_{1}=\\sigma_{2}=\\cdot\\cdot\\cdot=\\sigma_{T}=\\sigma$ , and $\\sigma$ is given to the learner. Later (Section 4.3), we will discuss how to generalize this result to the case with nonidentical variances across different rounds. ", "page_idx": 4}, {"type": "text", "text": "We assume that $r_{t}=f^{\\star}(x_{t},a_{t})+\\epsilon_{t}\\in[0,1]$ and $\\mathrm{Var}(\\epsilon_{t})\\leq\\sigma_{t}^{2}$ for every $1\\leq t\\leq T$ . Our results can be easily extended to subgaussian random noise (at the cost of a $\\log T$ factor) since for such variables, with probability at least $1-\\delta$ , $|\\epsilon_{t}|\\leq C\\sqrt{\\log(1/\\delta)}$ for a constant $C$ . ", "page_idx": 4}, {"type": "text", "text": "4.2.1 Algorithm and Analysis for Identical Variance ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We first consider the case with identical variance, i.e. $\\sigma_{t}^{2}\\,=\\,\\sigma^{2}$ for all $t\\ \\in\\ [T]$ . We propose Algorithm 1, and show that it has regret upper bound $\\tilde{\\mathcal{O}}(\\sqrt{\\sigma^{2}A T\\log|\\mathcal{F}|}+d_{\\mathrm{elu}}\\log|\\mathcal{F}|)$ . The algorithm is adapted from SquareCB of Foster and Rakhlin (2020), but additionally maintains a confidence function set, and has mechanisms to learn faster when the functions in the confidence set has larger disagreement. It has the following elements: ", "page_idx": 4}, {"type": "text", "text": "1. Restricting action set (Line 4) At the beginning of round $t$ (Line 4), the learner restricts the action set to $\\boldsymbol{A}_{t}$ , which only includes those actions that is the best action of some functions in the function class ${\\mathcal{F}}_{t}$ . If we assume that $f^{\\star}$ is always in the function class $\\mathcal{F}_{t}$ , by doing this we remove the unnecessary possibility of choosing actions that can never be the best action. ", "page_idx": 4}, {"type": "text", "text": "2. Checking disagreement (Line 5-Line 7) The next step of the algorithm is to check whether there is an action in $\\boldsymbol{A}_{t}$ such that two functions in the function class have large value differences (Line 6). We called such actions \u201cdiscriminative actions\u201d. Roughly speaking, we are seeking an action $a\\in\\mathcal{A}_{t}$ such that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\exists f,f^{\\prime}\\in\\mathcal{F}_{t},\\qquad|f(x_{t},a)-f^{\\prime}(x_{t},a)|\\gtrsim\\Delta\\approx\\sigma^{2}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Algorithm 1 VarCB (Variance-aware Contextual Bandits) ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "nput: $\\delta\\in[0,1]$ , $\\sigma\\in[1/A T,1]$ .   \n1: Let L = log |F|T2 2 and \u2206= $\\begin{array}{r}{\\Delta=\\frac{\\sigma^{2}}{11\\sqrt{L}}}\\end{array}$ and $\\mathcal{F}_{1}=\\mathcal{F}$ .   \n2: for $t=1,2\\dots,{\\bf d o}$   \n3: Receive context $x_{t}$ .   \n4: Define ${\\cal{A}}_{t}=\\{a\\in{\\cal{A}}:\\;\\exists f\\in{\\cal{F}}_{t},\\;a\\in\\arg\\operatorname*{max}_{a\\in{\\cal{A}}}f(x_{t},a)\\}$   \n5: Define ", "page_idx": 5}, {"type": "equation", "text": "$$\ng_{t}(a)=\\operatorname*{sup}_{f,f^{\\prime}\\in\\mathcal{F}_{t}}\\frac{|f(x_{t},a)-f^{\\prime}(x_{t},a)|}{\\sqrt{1+\\sum_{\\tau=1}^{t-1}w_{\\tau}(f(x_{\\tau},a_{\\tau})-f^{\\prime}(x_{\\tau},a_{\\tau}))^{2}}},\\qquad\\forall a\\in\\mathcal{A}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "6: if $\\operatorname*{max}_{a\\in\\mathcal{A}_{t}}g_{t}(a)\\geq\\Delta$ then   \n7: Choose action $a_{t}=\\arg\\operatorname*{max}_{a\\in\\mathcal{A}_{t}}g_{t}(a)$ and receive $r_{t}$ .   \n8: else   \n9: Call online regression oracle (Algorithm 4) with input $(\\mathcal{F}_{t},x_{t})$ and obtain $f_{t}$ .   \n10: Let $b_{t}=\\arg\\operatorname*{max}_{a\\in\\mathcal{A}_{t}}f_{t}(x_{t},a)$ (pick an arbitrary maximizer if there are multiple).   \n11: Draw $a_{t}\\sim p_{t}$ and receive $r_{t}$ , where ", "page_idx": 5}, {"type": "equation", "text": "$$\np_{t}(a)=\\left\\{\\!\\!\\begin{array}{l l}{0}&{\\mathrm{if}\\,a\\in\\mathcal{A}\\setminus\\mathcal{A}_{t},}\\\\ {\\frac{1}{|\\mathcal{A}_{t}|+\\gamma(f_{t}(x_{t},b_{t})-f_{t}(x_{t},a))}}&{\\mathrm{if}\\,a\\in\\mathcal{A}_{t}\\setminus\\{b_{t}\\},}\\\\ {1-\\sum_{a^{\\prime}\\ne a}p_{t}(a^{\\prime})}&{\\mathrm{if}\\,a=b_{t}.}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "12: Define $\\begin{array}{r}{w_{t}=\\operatorname*{min}\\left\\{\\frac{1}{\\sigma^{2}},\\frac{1}{g_{t}(a_{t})\\sqrt{L}}\\right\\}}\\end{array}$ and update the confidence set: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{F}_{t+1}=\\left\\{f\\in\\mathcal{F}_{t}:\\;\\displaystyle\\sum_{\\tau=1}^{t}w_{\\tau}(f(x_{\\tau},a_{\\tau})-\\hat{f}_{t+1}(x_{\\tau},a_{\\tau}))^{2}\\leq102L\\right\\},}\\\\ &{\\mathrm{where}\\;\\;\\hat{f}_{t+1}=\\underset{f\\in\\mathcal{F}_{t}}{\\arg\\operatorname*{min}}\\displaystyle\\sum_{\\tau=1}^{t}w_{\\tau}(f(x_{\\tau},a_{\\tau})-r_{\\tau})^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "If such an action exists, then the learner chooses this action at round $t$ . By selecting such an action that can discriminate disagreed functions, the function set $\\mathcal{F}_{t}$ can more quickly shrink. To prevent this action to incur overly large regret, it is important to perform Step 1 (Restricting action set). The regret incurred in rounds choosing discriminative actions is of order $\\tilde{\\mathcal{O}}(d_{\\mathrm{elu}}\\log|\\mathcal{F}|)$ . ", "page_idx": 5}, {"type": "text", "text": "3. Inverse gap weighting (Line 8-Line 11) At round $t$ , if there is no discriminative action, then the learner performs inverse gap weighting as in the SquareCB algorithm (Foster and Rakhlin (2020)). Inverse gap weighting requires the learner to have access to an online regression oracle that generates online estimations $f_{t}$ and ensures that the estimation error $\\sum_{t}(f_{t}(x_{t},a_{t}^{-})-f^{\\star}(x_{t},a_{t}))^{2}$ is small. In the original SquareCB, the requirement for the online regression oracle is ", "page_idx": 5}, {"type": "equation", "text": "$$\nR_{\\mathtt{s q}}=\\sum_{t=1}^{T}(f_{t}(x_{t},a_{t})-r_{t})^{2}-\\sum_{t=1}^{T}(f^{\\star}(x_{t},a_{t})-r_{t})^{2}\\lesssim\\log|\\mathcal{F}|,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "which only allows for a $\\sqrt{A T\\log|\\mathcal{F}|}$ regret bound that does not meet our goal. To improve this, we design an online regression oracle that ensures ", "page_idx": 5}, {"type": "equation", "text": "$$\nR_{\\mathsf{s q}}=\\sum_{t\\in\\mathcal{T}_{\\mathrm{tow}}}\\left(f_{t}(x_{t},a_{t})-r_{t}\\right)^{2}-\\sum_{t\\in\\mathcal{T}_{\\mathrm{tow}}}\\left(f^{\\star}(x_{t},a_{t})-r_{t}\\right)^{2}\\lesssim(\\sigma^{2}+\\tilde{\\Delta})\\log|\\mathcal{F}|,\\;\\;\\mathrm{~(our~condition)}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathcal{T}_{\\sf l G W}$ is the set of rounds that we run inverse gap weighting (i.e., entering the else case in Line 8), and $\\tilde{\\Delta}$ is an upper bound for $\\begin{array}{r}{\\operatorname*{max}_{a\\in{\\mathcal{A}}_{t}}\\operatorname*{max}_{f,f^{\\prime}\\in{\\mathcal{F}}_{t}}|f(x_{t},a)-f^{\\prime}(x_{t},a)|}\\end{array}$ , i.e., the maximum disagreement among the function set $\\mathcal{F}_{t}$ for the context $x_{t}$ . Thanks to Step 2, we only run inverse gap weighting when $\\tilde{\\Delta}\\lesssim\\Delta\\approx\\sigma^{2}$ . Thus, with the refined $R_{\\mathsf{s q}}$ guarantee and standard squareCB arguments, we can get a regret bound of order $\\sigma{\\sqrt{A T\\log|{\\mathcal{F}}|}}$ for the rounds in $\\mathcal{T}_{\\sf I G W}$ . ", "page_idx": 5}, {"type": "text", "text": "The way to achieve \u201c(our condition)\u201d is an interesting part of our algorithm. A standard way to ensure F&R\u2019s condition is by aggregating over the function set through exponential weights. Exponential weights ensures $R_{\\mathsf{s q}}={\\mathcal{O}}(\\log|{\\mathcal{F}}|/\\eta)$ as long as the functions to be aggregated are $\\eta$ -mixable. Thus, in order to show $R_{\\mathsf{s q}}\\,=\\,{\\mathcal{O}}(\\sigma^{2}\\log|{\\mathcal{F}}|)$ , we need to argue $\\eta\\,=\\,\\Omega(1/\\sigma^{2})$ . However, because the potential range of $r_{t}$ is $[0,1]$ even though the variance $\\sigma^{2}$ and and the disagreement $\\Delta$ are both much smaller than 1, the best mixability coefficient $\\eta$ we can show for squared loss is still $\\Theta(1)$ . ", "page_idx": 6}, {"type": "text", "text": "To address this, we resort to the use of the Prod algorithm (Cesa-Bianchi and Lugosi, 2006) with a properly chosen surrogate loss to perform aggregation. This algorithm has a different second-order approximation for the loss compared to the exponential weight algorithm, which is crucial in obtaining the desired bound. The regret analysis is also no longer through mixability. Our online regression oracle is provided in Algorithm 4 in Appendix D. We remark without giving details that in the linear case, such a guarantee can also be obtained through Online Newton Step (Hazan et al., 2007). ", "page_idx": 6}, {"type": "text", "text": "4. Updating function set (Line 12) After finishing selecting the action $a_{t}$ for round $t$ , the learner updates the confidence function set $\\mathcal{F}_{t}$ to prepare for the next round. The construction of the confidence set utilizes the idea of weighted regression that has been widely used in previous varianceaware or corruption-robust contextual bandit or RL algorithms (He et al., 2022; Zhao et al., 2023; Ye et al., 2023; Agarwal et al., 2023). This has the effect of controlling the relative importance of different samples and is crucial in controlling the regret incurred in Step 2. ", "page_idx": 6}, {"type": "text", "text": "By putting these building blocks together, we arrive at Algorithm 1. The regret of Algorithm 1 is described in Theorem 4.2, whose proof is deferred to Appendix D. ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.2. Algorithm 1 ensures with probability at least $1-\\delta$ , ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}(\\operatorname*{max}_{a\\in\\mathcal{A}}f^{\\star}(x_{t},a)-f^{\\star}(x_{t},a_{t}))=\\tilde{O}\\left(\\sqrt{\\sigma^{2}A T\\log\\left(|\\mathcal{F}|/\\delta\\right)}+d_{e l u}\\log\\left(|\\mathcal{F}|/\\delta\\right)\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Comparison with AdaCB of Foster et al. (2020) Our VarCB (Algorithm 1) shares some similarities with the AdaCB algorithm from Foster et al. (2020), which aims to achieve a $\\tilde{\\mathcal{O}}\\big(\\frac{d\\log|\\mathcal{F}|}{\\mathrm{GAP}}\\big)$ regret bound. Here, $d$ is a disagreement coefficient of $\\mathcal{F}$ , which takes the same role as our $d_{\\mathrm{elu}}^{\\ }$ , and GAP represents the minimal reward gap between the best and second-best decisions. Specifically, both algorithms include a step to remove irrelevant actions (our Step 1). The action selection rule of AdaCB also depends on the amount of disagreement over the function class, which is superficially related to the if-else separation in VarCB. However, we find that the case separations in the two algorithms do not have a clear correspondence to each other, possibly due to the different objectives of the two algorithms. Also, the two algorithms operate under quite different settings: AdaCB works in the setting where the contexts are i.i.d., while VarCB allows for adversarial contexts. On the other hand, AdaCB is parameter-free, but VarCB requires the information of $\\sigma$ . Developing a more unified version for these two better-than-minimax algorithms is an interesting future direction. ", "page_idx": 6}, {"type": "text", "text": "4.3 Algorithm and Analysis for Heteroscedastic Noise ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Next, we will discuss how to generalize our algorithm to heteroscedastic case, i.e. when the noise of fdoifllfeorwein r $(\\log(A T)+1)$ $\\sigma_{t}\\in[0,\\frac{1}{A T}]$ wese  ocfl atshsei fvya $t$ i nntcoe $\\mathcal{T}_{0}$ , ea cnlda sfsoirf $\\begin{array}{r}{\\sigma_{t}\\in(\\frac{2^{i-1}}{A T},\\frac{2^{i}}{A T}]}\\end{array}$ ,  twhee $t$ into $\\mathcal{T}_{i}$ for $2\\leq i\\leq\\log(A T)$ , i.e., if $\\sigma_{t}$ falls into the $i$ -th intervals in the following, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Sigma_{0}=[0,\\frac{1}{A T}],\\quad\\Sigma_{1}=\\big(\\frac{1}{A T},\\frac{2}{A T}\\big],\\quad\\Sigma_{2}=\\big(\\frac{2}{A T},\\frac{4}{A T}\\big),\\quad\\cdot\\cdot\\cdot,\\quad\\Sigma_{\\mathrm{log}(A T)}=(1/2,1],}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "we classify $t$ into $\\mathcal{T}_{i}$ . For each set $\\mathcal{T}_{i}$ , we maintain an algorithm $\\mathcal{A}_{i}$ of Algorithm 1 in parallel. At the beginning at round $t$ , when observing that $t\\in T_{i}$ , only $\\mathcal{A}_{i}$ is updated, while $\\mathcal{A}_{j}$ remains the same for $j\\neq i$ . According to Theorem 4.2, we have for any $0\\le i\\le\\log T$ , ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\sum_{t\\in[T_{i}]}\\left(\\operatorname*{max}_{a\\in A}f^{\\star}(x_{t},a)-f^{\\star}(x_{t},a_{t})\\right)=\\tilde{\\mathcal{O}}\\left(\\sqrt{A|\\mathcal{T}_{i}|\\cdot\\left(\\frac{2^{i}}{A T}\\right)^{2}\\log|\\mathcal{F}|}+d_{\\mathrm{elu}}\\log|\\mathcal{F}|\\right),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "we can bound the total regret by ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{\\log(A T)}\\tilde{\\mathcal{O}}\\left(\\sqrt{A|\\mathcal{T}_{i}|\\cdot\\left(\\frac{2^{i}}{A T}\\right)^{2}\\log|\\mathcal{F}|}+d_{\\mathrm{elu}}\\log|\\mathcal{F}|\\right)=\\tilde{\\mathcal{O}}\\left(\\sqrt{A\\sum_{i=1}^{T}\\sigma_{i}^{2}\\log|\\mathcal{F}|}+d_{\\mathrm{elu}}\\log|\\mathcal{F}|\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "table", "img_path": "32Z3nfCnwa/tmp/d74fc0e156aba7101f4881079bfbd86231f140ba624c70b8a059bcc47e183a51.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "The formal algorithm for heteroscedastic cases is given in Algorithm 2, and we have the following corollary on the second-order regret bound of Algorithm 2. ", "page_idx": 7}, {"type": "text", "text": "Corollary 4.1. The output $a_{t}$ of Algorithm 2 in rount $t$ satisfies that with probability at least $1-\\delta$ , ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}(\\operatorname*{max}_{a\\in A}f^{*}(x_{t},a)-f^{*}(x_{t},a_{t}))=\\tilde{O}\\left(\\sqrt{A\\sum_{i=1}^{T}\\sigma_{i}^{2}\\log(|\\mathcal{F}|/\\delta)}+d_{\\mathrm{elu}}\\log(|\\mathcal{F}|/\\delta)\\right).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The proof of Corollary 4.1 is given in Section E. ", "page_idx": 7}, {"type": "text", "text": "5 Strong Adversary Case ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we consider the case where the adversary decides the variance $\\sigma_{t}$ after seeing the a\u221action $a_{t}$ chosen by the learner. We provide regret lower and upper bounds matching up to a factor of $\\sqrt{d_{\\mathrm{elu}}}$ and other logarithmic factors. More importantly, the minimax regret bounds differ with the weak adversary case (Section 4) as discussed in Section 3.2, demonstrating the even more crucial role of eluder dimension in this case. ", "page_idx": 7}, {"type": "text", "text": "Regret lower bound In this strong adversary case, we first show that the adversary\u2019s power is enhanced in terms of the achievable minimax regret bounds. Concretely, we have the following theorem. ", "page_idx": 7}, {"type": "text", "text": "Theorem 5.1. For any integer $d,A,T\\geq2$ and any positive real number $\\Lambda\\in[0,T]$ , there exists a context space $\\mathcal{X}$ , a contextual bandit problem $\\mathcal{F}\\,\\subset\\,(\\mathcal{X}\\,\\times\\,\\mathcal{A}\\,\\rightarrow\\,\\mathbb{R})$ with eluder dimension $d_{e l u}(\\mathcal{F},0)\\stackrel{!}{=}d$ and action set $A=[A]$ and an adversarial sequence of va\u221ariances $\\sigma_{1}^{2},\\dots,\\sigma_{T}^{2}$ with $\\textstyle\\sum_{t=1}^{T}\\sigma_{t}^{2}\\leq\\Lambda$ such that any algorithm will suffer a regret at least $\\Omega(\\operatorname*{min}\\{\\sqrt{d\\Lambda}+d,\\sqrt{A T}\\})$ ). ", "page_idx": 7}, {"type": "text", "text": "The above theorem shows that the regret is at least $\\Omega(\\operatorname*{min}\\{\\sqrt{d\\Lambda}+d,\\sqrt{A T}\\})$ where $d=d_{\\mathrm{elu}}(\\mathcal{F})$ even with $\\log|\\mathcal{F}|=\\mathcal{O}(\\log T)$ . Recall that the bound in the weak adversary case (Section 4) can be written as $\\Omega(\\operatorname*{min}\\{\\sqrt{\\operatorname*{min}\\{A,d\\}\\Lambda}+d,\\sqrt{A T}\\})$ ). The power of the strong adversary is exactly the higher complexity $d$ in the $\\Lambda$ term compared to $\\operatorname*{min}\\{A,d\\}$ i\u221an the weak adversary case. Now, we proceed to provide a matching upper bound up to a factor of $\\sqrt{d}$ . ", "page_idx": 7}, {"type": "text", "text": "Regret upper bound For the strong adversary case, we adopt an optimism-based approach. In partic\u221aular, we generalize the SAVE algorithm by Zhao et al. (2023), which achieves the tight $\\tilde{\\mathcal{O}}(d\\sqrt{\\Lambda}+d)$ bound for linear contextual bandits. We call the algorithm VarUCB and display it in Algorithm 5 of Appendix F. The algorithm combines the idea of weighted regression and multi-layer structure of SupLinUCB (Chu et al. (2011)) and refined variance-aware confidence set. Since this algorithm is a rather direct extension of Zhao et al. (2023)\u2019s algorithm from the linear case to the non-linear case, we omit the detailed discussion on it and refer the readers to Zhao et al. (2023). Notice that for this algorithm, we do not need $\\sigma_{t}$ to be revealed to the learner as in Section 4. In fact, we do not even need to know $\\Lambda$ . We have the following theorem for its regret guarantee. ", "page_idx": 7}, {"type": "text", "text": "Theorem 5.2. When facing the strong adversary, Algorithm $^{5}$ guarantees a regret bound of $\\tilde{\\mathcal{O}}(d_{e l u}\\sqrt{\\Lambda\\log|\\mathcal{F}|}+d_{e l u}\\log|\\mathcal{F}|)$ with probability at least $1-\\delta$ , where $\\tilde{\\mathcal{O}}(\\cdot)$ hides $\\log(T/\\delta)$ factors. ", "page_idx": 7}, {"type": "text", "text": "The proof is provided in Appe\u221andix F. Notice that when specializing Theorem 5.2 to the linear setting, the bound becomes $\\tilde{\\mathcal{O}}(\\sqrt{d^{3}\\Lambda}+d^{2})$ since $\\log\\left|{\\mathcal{F}}\\right|=\\Theta(d)$ , which does not recover the bound of Zhao et al. (2023). Indeed, our analysis deviates from that of Zhao et al. (2023) due to the generality of non-linear function approximation. It is an interesting future direction to see whether our bound can be improved. We mention in passing that the work by Wang et al. (2024) obtained ", "page_idx": 7}, {"type": "text", "text": "1: Let $\\mathcal{M}_{1}=\\mathcal{M}$ , $\\begin{array}{r}{M_{1}=\\frac{1}{|\\mathcal{M}|}\\sum_{M\\in\\mathcal{M}}M}\\end{array}$ , and $L=\\Theta(\\log(|\\mathcal{M}|T/\\delta))$ .   \n2: for $t=1,\\dots,T$ do   \n3: Receive context $x_{t}$ .   \n4: if $\\exists M$ , $M^{\\prime}\\in\\mathcal{M}_{t}$ and $a\\in{\\mathcal{A}}$ such that $D_{\\mathrm{H}}^{2}(M(x_{t},a),M^{\\prime}(x_{t},a))\\geq1/2$ then   \n5: Let $I_{t}=1$ and pull $a_{t}=\\arg\\operatorname*{max}_{a}\\operatorname*{max}_{M,M^{\\prime}\\in\\mathcal{M}_{t}}D_{\\mathrm{H}}^{2}(M(x_{t},a),M^{\\prime}(x_{t},a)).$   \n6: else   \n7: Let $I_{t}=2$ and pull $a_{t}\\sim p_{t}$ where   \n$p_{t}=\\underset{p\\in\\Delta({\\cal A})}{\\arg\\operatorname*{min}}\\,\\operatorname*{max}_{M\\in{\\cal M}_{t}}\\mathbb{E}_{a\\sim p}\\left[\\operatorname*{max}_{a^{\\prime}}f_{M}({x_{t},a^{\\prime}})-f_{M}({x_{t},a})-\\gamma\\frac{(f_{M}({x_{t},a})-f_{M_{t}}({x_{t},a}))^{2}}{\\sigma_{M_{t}}^{2}({x_{t},a})}\\right].$   \n8: Receive $r_{t}$ .   \n9: Let $\\begin{array}{r l}&{\\mathcal{M}_{t+1}\\overset{\\cdot\\;}{=}\\mathcal{M}_{t}\\cap\\bigl\\{M:\\sum_{s=1}^{t}D_{\\mathrm{H}}^{2}(M(x_{s},a_{s}),M_{s}(x_{s},a_{s}))\\leq L\\bigr\\}.}\\\\ &{M_{t+1}=\\frac{\\sum_{M\\in\\mathcal{M}_{t+1}}q_{t}(M)M}{\\sum_{M\\in\\mathcal{M}_{t+1}}q_{t}(M)},\\quad\\mathrm{where}\\quad q_{t}(M)\\propto\\prod_{s=1}^{t}M(r_{s}|x_{s},a_{s}).}\\end{array}$   \n10: ", "page_idx": 8}, {"type": "text", "text": "$\\sqrt{d\\Lambda\\log|\\mathcal{M}|}\\!+\\!d\\log|\\mathcal{M}|$ upper bound where $d=d_{\\mathrm{elu}}(\\mathcal{M})$ . However, the algorithm relies on having access to a model class. We study such a setting in our next section. ", "page_idx": 8}, {"type": "text", "text": "6 Learning with a Model Class ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Distributional setup In this section, we consider the case where the learner is given a model class $M\\subset((\\mathcal{X}\\times\\mathcal{A})\\rightarrow\\Delta(\\mathbb{R}))$ where each model $M\\in\\mathcal{M}$ maps any context-action pair to a gaussian distribution, i.e., for any $x,a\\in\\mathcal{X}\\times\\mathcal{A}$ , ", "page_idx": 8}, {"type": "equation", "text": "$$\nM(x,a)=N(f_{M}(x,a),\\sigma_{M}(x,a)),\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $f_{M}(x,a)$ and $\\sigma_{M}(x,a)$ are the mean and variance of the distribution $M(x,a)$ . We assume that all the expected rewards and variances are bounded by $[0,1]$ . Recall, at round $t$ , the reward is given by $\\overline{{r_{t}\\,}}\\,=\\,f^{\\star}(x_{t},a_{t})+\\epsilon_{t}$ . We further assume throughout this section that $\\epsilon_{t}$ is Gaussian with variance $\\sigma^{\\star}(x_{t},a_{t})$ (since Gaussian is unbounded, we drop the assumption $r_{t}\\,\\in\\,[0,1]$ that we made in Section 2). Thus, the distribution of $r_{t}$ follows a true model $M^{\\star}$ where $M^{\\star}(x,a)=$ $\\mathcal{N}(f^{\\star}(x,a),\\sigma^{\\star}(x,a))$ . ", "page_idx": 8}, {"type": "text", "text": "Assumption 6.1 (Model Realizability). Assume $M^{\\star}\\in\\mathcal{M}$ . ", "page_idx": 8}, {"type": "text", "text": "For this setup, it is useful to consider the Hellinger counterpart of the eluder dimension. ", "page_idx": 8}, {"type": "text", "text": "Definition 6.1 (Hellinger Eluder Dimension). For the model class $\\mathcal{M}$ defined on the space $\\mathcal{Z}$ (that is $M\\subset(\\mathcal{Z}\\rightarrow\\Delta(\\mathbb{R})),$ , we define the Hellinger eluder dimension of $\\mathcal{M}$ at scale $\\alpha\\geq0$ as $d_{e l u}^{\\mathsf{H}}(\\alpha)$ be the length of the longest sequence of tuples $(z_{1},M_{1},M_{1}^{\\prime}),...,(z_{m},M_{m},M_{m}^{\\prime})$ and $\\alpha_{0}\\geq\\alpha$ such that for all $i=1,..,m,$ , functions $M_{i},M_{i}^{\\prime}\\in\\mathcal{M}$ , ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\sum_{j<i}D_{\\mathrm{H}}^{2}(M_{i}(z_{j}),M_{i}^{\\prime}(z_{j}))\\leq\\alpha_{0}^{2},\\quad a n d\\quad D_{\\mathrm{H}}^{2}(M_{i}(z_{i}),M_{i}^{\\prime}(z_{i}))>\\alpha_{0}^{2}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Algorithm Similar to Algorithm 1, we present Algorithm 3 tailored for the distributional case. At each round $t$ , upon receiving the context $x_{t}$ , the algorithm first checks if there exists an action $a$ such that two models within the localized model class $\\mathcal{M}_{t}$ exhibit a significant divergence on the context-action pair $x_{t},a$ measured by the squared Hellinger distance (Line 4). If such a difference is detected, the learner selects the action associated with the greatest divergence (Line 5). Conversely, if no action causes substantial divergence between models, the learner runs a variant of SquareCB (Foster and Rakhlin, 2020), employing adaptive variances to ensure low regret (Line 7). The major differences between Algorithm 1 and Algorithm 3 is that the latter measures the \u201cdisagreement\u201d in terms of the squared Hellinger distance. ", "page_idx": 8}, {"type": "text", "text": "Regret upper bound We obtain the following distributional version regret bound for Algorithm 3. ", "page_idx": 9}, {"type": "text", "text": "Theorem 6.1. For $d=d_{e l u}^{\\mathsf{H}}({^1}/{\\sqrt{T}})$ , the output $a_{t}$ of Algorithm 3 satisfies with probability at least $1-\\delta$ , ", "page_idx": 9}, {"type": "equation", "text": "$$\nR_{T}=\\tilde{\\mathcal{O}}\\bigg(\\sqrt{A\\sum_{t=1}^{T}\\sigma_{M^{\\star}}^{2}(x_{t})\\cdot\\log(|\\mathcal{M}|/\\delta)}+d\\log(|\\mathcal{M}|/\\delta)\\bigg),\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "where $\\begin{array}{r}{\\sigma_{M^{\\star}}^{2}(x_{t})=\\operatorname*{max}_{a\\in A}\\sigma_{M^{\\star}}^{2}(x_{t},a).}\\end{array}$ . ", "page_idx": 9}, {"type": "text", "text": "A similar upper bound for a more general distributional case is obtained by Wang et al. (2024) in the tfhoer md eopfe $\\begin{array}{r}{\\tilde{\\mathcal{O}}(\\sqrt{d\\sum_{t=1}^{T}\\sigma_{M^{\\star}}^{2}(x_{t},a_{t})\\cdot\\log|\\mathcal{M}|}+d\\log|\\mathcal{M}|)}\\end{array}$ .i gInni tfhicea nletlayd isnmga tlleerrm t,h oanu oinu nmd arneyp lcaacseeds $d$ $A$ $A$ of interest (e.g. linear, generalized linear). However, as a tradeoff, our bound also suffers a larger cumulative variance term. This tradeoff is necessary as we show in the following lower bound results that both our upper bound and their upper bound are optimal, i.e., matching lower bounds exist. Thus our result is at one end of the Pareto frontier. ", "page_idx": 9}, {"type": "text", "text": "Regret lower bounds We present the matching lower bound for our result as follows, which is essentially a rewrite of Theorem 4.1. ", "page_idx": 9}, {"type": "text", "text": "Theorem 6.2. For any integer $d,A,T\\geq2$ , any positive real number $\\sigma\\in[0,1].$ , there exists a context space $\\mathcal{X}$ and a contextual bandit gaussian model class $M\\subset(\\mathcal{X}\\times\\mathcal{A}\\rightarrow\\Delta(\\mathbb{R}))$ ) with Hellinger eluder dimension $d_{e l u}^{\\mathsf{H}}(0)\\leq d_{*}$ , action set $A=[A]$ , and variances $\\sigma_{M}(x,a)\\le\\sigma$ for all $M\\in\\mathcal{M}$ ,\u221a $x,a\\in$ $\\mathcal{X}\\times\\mathcal{A}$ such that any algorithm will suffer a regret at least $\\Omega(\\sqrt{\\sigma^{2}\\operatorname*{min}\\{A,d\\}T}+\\operatorname*{min}\\{d,\\sqrt{A T}\\})$ . ", "page_idx": 9}, {"type": "text", "text": "Now we present the matching lower bound for the upper bound from Wang et al. (2024). ", "page_idx": 9}, {"type": "text", "text": "Theorem 6.3. For any integer $d,A,T\\geq2$ and any positive real number $\\Lambda\\in[0,T],$ , there exists $a$ celoundteexrt  dsipmaecne $\\mathcal{X}$ ,n $d_{e l u}^{\\mathsf{H}}(0)\\leq d$ baanndd iat cgtiaouns ssieat $A=[A]$ l aasnsd $M\\subset({\\mathcal{X}}\\times{\\mathcal{A}}\\to\\Delta(\\mathbb{R}))$ $\\begin{array}{r}{\\sum_{t=1}^{T}\\sigma_{M^{\\star}}(x_{t},a_{t})^{2}\\le\\Lambda}\\end{array}$ such that any algorithm will suffer a regret at least $\\Omega(\\operatorname*{min}\\{\\sqrt{d\\Lambda}+d,\\sqrt{A T}\\})$ ). ", "page_idx": 9}, {"type": "text", "text": "The lower bound obtained by Theorem 6.3 is an adaptation from Theorem 5.1 that crucially relies on the fact that the adversary can choose the variance according to the action $a_{t}$ . ", "page_idx": 9}, {"type": "text", "text": "7 Open Questions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Removing the revealing $\\sigma_{t}$ assumption in the weak adversary setting The assumption we made in Section 4 that the variance is revealed at the beginning of each round is rather restrictive, and ideally we would\u221a like to remove such an assumption. As a first step, we wonder whether the same regret bound $\\tilde{O}(\\sqrt{A\\Lambda}+d_{\\mathrm{elu}})$ is achievable if the variance $\\sigma_{t}$ is revealed at the end of round $t$ . We answer this question affirmatively for the special case where $\\sigma_{t}\\in\\{0,1\\}$ . More details can be found in Appendix H. How to extend this result to general values of $\\sigma_{t}$ is an interesting open question. Handling the case where $\\sigma_{t}$ is never revealed is even more challenging but is the ultimate goal. ", "page_idx": 9}, {"type": "text", "text": "Removing the Gaussian noise assumption in the distributional setting Our Theorem 6.1 heavily relies on the assumption that the noise is Gaussian. We wonder whether such assumption can be relaxed or completely lifted. For example, can we obtain the same bound if the noise at round $t$ is just $\\sigma_{M\\star}\\left(x_{t},a_{t}\\right)$ -sub-Gaussian? What if it is just a bounded noise with variance $\\sigma_{M^{\\star}}^{2}(x_{t},a_{t})?$ ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We thank Dylan Foster for helpful discussions. We acknowledge support from ARO through award W911NF-21-1-0328, from the DOE through award DE-SC0022199, and from NSF through award DMS-2031883. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Alekh Agarwal, Miroslav Dud\u00edk, Satyen Kale, John Langford, and Robert Schapire. Contextual bandit learning with predictable rewards. In Artificial Intelligence and Statistics, pages 19\u201326. PMLR, 2012. ", "page_idx": 9}, {"type": "text", "text": "Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert Schapire. Taming the monster: A fast and simple algorithm for contextual bandits. In International Conference on Machine Learning, pages 1638\u20131646, 2014.   \nAlekh Agarwal, Yujia Jin, and Tong Zhang. Vo $q$ l: Towards optimal regret in model-free rl with nonlinear function approximation. In The Thirty Sixth Annual Conference on Learning Theory, pages 987\u20131063. PMLR, 2023.   \nZeyuan Allen-Zhu, S\u00e9bastien Bubeck, and Yuanzhi Li. Make the minority great again: First-order regret bound for contextual bandits. In International Conference on Machine Learning, pages 186\u2013194. PMLR, 2018.   \nJean-Yves Audibert, R\u00e9mi Munos, and Csaba Szepesv\u00e1ri. Exploration\u2013exploitation tradeoff using variance estimates in multi-armed bandits. Theoretical Computer Science, 410(19):1876\u20131902, 2009.   \nPeter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multiarmed bandit problem. SIAM journal on computing, 32(1):48\u201377, 2002.   \nNicolo Cesa-Bianchi and G\u00e1bor Lugosi. Prediction, learning, and games. Cambridge university press, 2006.   \nNicolo Cesa-Bianchi, Yishay Mansour, and Gilles Stoltz. Improved second-order bounds for prediction with expert advice. Machine Learning, 66:321\u2013352, 2007.   \nWei Chu, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual bandits with linear payoff functions. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, pages 208\u2013214. JMLR Workshop and Conference Proceedings, 2011.   \nYan Dai, Ruosong Wang, and Simon Shaolei Du. Variance-aware sparse linear bandits. In The Eleventh International Conference on Learning Representations, 2023.   \nChris Dann, Chen-Yu Wei, and Julian Zimmert. A blackbox approach to best of both worlds in bandits and beyond. In The Thirty Sixth Annual Conference on Learning Theory, pages 5503\u20135570. PMLR, 2023.   \nQiwei Di, Tao Jin, Yue Wu, Heyang Zhao, Farzad Farnoud, and Quanquan Gu. Variance-aware regret bounds for stochastic contextual dueling bandits. In The Twelfth International Conference on Learning Representations, 2024.   \nMiroslav Dudik, Daniel Hsu, Satyen Kale, Nikos Karampatziakis, John Langford, Lev Reyzin, and Tong Zhang. Efficient optimal learning for contextual bandits. In Proceedings of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence, pages 169\u2013178, 2011.   \nDylan Foster and Alexander Rakhlin. Beyond ucb: Optimal and efficient contextual bandits with regression oracles. In International Conference on Machine Learning, pages 3199\u20133210. PMLR, 2020.   \nDylan J Foster and Akshay Krishnamurthy. Efficient first-order contextual bandits: Prediction, allocation, and triangular discrimination. Advances in Neural Information Processing Systems, 34: 18907\u201318919, 2021.   \nDylan J Foster, Alexander Rakhlin, David Simchi-Levi, and Yunzong Xu. Instance-dependent complexity of contextual bandits and reinforcement learning: A disagreement-based perspective. arXiv preprint arXiv:2010.03104, 2020.   \nDylan J Foster, Sham M Kakade, Jian Qian, and Alexander Rakhlin. The statistical complexity of interactive decision making. arXiv preprint arXiv:2112.13487, 2021.   \nDylan J Foster, Yanjun Han, Jian Qian, and Alexander Rakhlin. Online estimation via offline estimation: An information-theoretic framework. Advances in Neural Information Processing Systems, 2024.   \nDavid A Freedman. On tail probabilities for martingales. the Annals of Probability, pages 100\u2013118, 1975.   \nElad Hazan and Satyen Kale. Better algorithms for benign bandits. Journal of Machine Learning Research, 12(4), 2011.   \nElad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex optimization. Machine Learning, 69(2):169\u2013192, 2007.   \nJiafan He, Dongruo Zhou, Tong Zhang, and Quanquan Gu. Nearly optimal algorithms for linear contextual bandits with adversarial corruptions. Advances in neural information processing systems, 35:34614\u201334625, 2022.   \nShinji Ito. Parameter-free multi-armed bandit algorithms with hybrid data-dependent regret bounds. In Conference on Learning Theory, pages 2552\u20132583. PMLR, 2021.   \nShinji Ito and Kei Takemura. Best-of-three-worlds linear bandit algorithm with variance-adaptive regret bounds. In The Thirty Sixth Annual Conference on Learning Theory, pages 2653\u20132677. PMLR, 2023.   \nYeoneung Kim, Insoon Yang, and Kwang-Sung Jun. Improved regret analysis for variance-adaptive linear bandits and horizon-free linear mixture mdps. Advances in Neural Information Processing Systems, 35:1060\u20131072, 2022.   \nTor Lattimore and Csaba Szepesv\u00e1ri. Bandit algorithms. Cambridge University Press, 2020.   \nAldo Pacchiano. Second order bounds for contextual bandits with function approximation. arXiv preprint arXiv:2409.16197, 2024.   \nSasha Rakhlin and Karthik Sridharan. Optimization, learning, and games with predictable sequences. Advances in Neural Information Processing Systems, 26, 2013.   \nDaniel Russo and Benjamin Van Roy. Learning to optimize via posterior sampling. Mathematics of Operations Research, 39(4):1221\u20131243, 2014.   \nDavid Simchi-Levi and Yunzong Xu. Bypassing the monster: A faster and simpler optimal algorithm for contextual bandits under realizability. Mathematics of Operations Research, 47(3):1904\u20131931, 2022.   \nMohammad Sadegh Talebi and Odalric-Ambrym Maillard. Variance-aware regret bounds for undiscounted reinforcement learning in mdps. In Algorithmic Learning Theory, pages 770\u2013805. PMLR, 2018.   \nVladimir G Vovk. A game of prediction with expert advice. In Proceedings of the eighth annual conference on Computational learning theory, pages 51\u201360, 1995.   \nKaiwen Wang, Kevin Zhou, Runzhe Wu, Nathan Kallus, and Wen Sun. The benefits of being distributional: Small-loss bounds for reinforcement learning. Advances in Neural Information Processing Systems, 36, 2023.   \nKaiwen Wang, Owen Oertell, Alekh Agarwal, Nathan Kallus, and Wen Sun. More beneftis of being distributional: Second-order bounds for reinforcement learning. In International Conference on Machine Learning, 2024.   \nChen-Yu Wei and Haipeng Luo. More adaptive algorithms for adversarial bandits. In Conference On Learning Theory, pages 1263\u20131291. PMLR, 2018.   \nChen-Yu Wei, Haipeng Luo, and Alekh Agarwal. Taking a hint: How to leverage loss predictors in contextual bandits? In Conference on Learning Theory, pages 3583\u20133634. PMLR, 2020.   \nYunbei Xu and Assaf Zeevi. Upper counterfactual confidence bounds: a new optimism principle for contextual bandits. arXiv preprint arXiv:2007.07876, 2020.   \nChenlu Ye, Wei Xiong, Quanquan Gu, and Tong Zhang. Corruption-robust algorithms with uncertainty weighting for nonlinear contextual bandits and markov decision processes. In International Conference on Machine Learning, pages 39834\u201339863. PMLR, 2023.   \nAndrea Zanette and Emma Brunskill. Tighter problem-dependent regret bounds in reinforcement learning without domain knowledge using value function bounds. In International Conference on Machine Learning, pages 7304\u20137312. PMLR, 2019.   \nTong Zhang. Feel-good thompson sampling for contextual bandits and reinforcement learning. SIAM Journal on Mathematics of Data Science, 4(2):834\u2013857, 2022.   \nZihan Zhang, Jiaqi Yang, Xiangyang Ji, and Simon S Du. Improved variance-aware confidence sets for linear bandits and linear mixture mdp. Advances in Neural Information Processing Systems, 34:4342\u20134355, 2021.   \nZihan Zhang, Jason D Lee, Yuxin Chen, and Simon S Du. Horizon-free regret for linear markov decision processes. International Conference on Learning Representations, 2024.   \nHeyang Zhao, Jiafan He, Dongruo Zhou, Tong Zhang, and Quanquan Gu. Variance-dependent regret bounds for linear bandits and reinforcement learning: Adaptivity and computational efficiency. In The Thirty Sixth Annual Conference on Learning Theory, pages 4977\u20135020. PMLR, 2023.   \nRunlong Zhou, Ruosong Wang, and Simon Shaolei Du. Horizon-free and variance-dependent reinforcement learning for latent markov decision processes. In International Conference on Machine Learning, pages 42698\u201342723. PMLR, 2023.   \nJulian Zimmert and Tor Lattimore. Return of the bias: Almost minimax optimal high probability bounds for adversarial linear bandits. In Conference on Learning Theory, pages 3285\u20133312. PMLR, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendices ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A Related Work 15 ", "page_idx": 13}, {"type": "text", "text": "B Technical Tools 15 ", "page_idx": 13}, {"type": "text", "text": "C Omitted Proofs in Section 4.1 16 ", "page_idx": 13}, {"type": "text", "text": "D Omitted Proofs in Section 4.2 19   \nD.1 Analysis of the Online Regression Oracle 19   \nD.2 Proof of Theorem 4.2 22 ", "page_idx": 13}, {"type": "text", "text": "E Omitted Proofs in Section 4.3 29 ", "page_idx": 13}, {"type": "text", "text": "F Algorithm for the Strong Adversary Case and Omitted Proofs in Section 5 31   \nF.1 Upper Bound 31   \nF.2 Lower Bound 36 ", "page_idx": 13}, {"type": "text", "text": "G Omitted Proofs in Section 6 38 ", "page_idx": 13}, {"type": "text", "text": "H Upper Bound with Zero-One Variance (Section 7) 43 ", "page_idx": 13}, {"type": "text", "text": "A Related Work ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we review the literature in bandits/RL that obtains second-order regret bounds or other data-/instance-dependent regret bounds. ", "page_idx": 14}, {"type": "text", "text": "Tabular/linear bandits and MDPs Second-order regret bounds for bandits have been extensively studied in non-contextual (fixed action set) settings, such as stochastic multi-armed bandits (Audibert et al., 2009), adversarial multi-armed bandits (Wei and Luo, 2018; Ito, 2021), and adversarial linear bandits (Hazan and Kale, 2011; Ito and Takemura, 2023). Key techniques in this line of work include replacing Hoeffding-style concentration bounds with Bernstein-style ones in value estimation, and using optimistic mirror descent (Rakhlin and Sridharan, 2013) to achieve variance reduction. Extending beyond non-contextual settings, a series of recent works (Zhang et al., 2021; Kim et al., 2022; Zhao et al., 2023) have focused on obtaining tight variance-dependent bounds for linear contextual bandits. The techniques developed for variance-dependent regret bounds in bandits have been extended to MDPs, leading to either variance-dependent or horizon-free bounds (Talebi and Maillard, 2018; Zanette and Brunskill, 2019; Zhang et al., 2021; Kim et al., 2022; Zhao et al., 2023; Zhou et al., 2023; Zhang et al., 2024). Additionally, other settings such as dueling bandits (Di et al., 2024) and sparse linear bandits (Dai et al., 2023) have been explored. ", "page_idx": 14}, {"type": "text", "text": "General contextual bandits For contextual bandits with general policy or function classes, secondorder bounds have been explored in both agnostic settings (Wei et al., 2020) and realizable settings (Wang et al., 2024). Wei et al. (2020) focused on the case where the number of actions is small. They demonstrated that, unlike in multi-armed or linear bandits, the tight second-order bound for general contextual bandits is more complex than simply replacing the zeroth-order term $T$ in the minimax bound with the second-order measure, which aligns with our findings. For instance, even if the second-order error $\\mathcal{E}$ is $O(1)$ , they showed that the regret could still grow with $\\Omega(T^{1/4})$ . On the other hand, Wang et al. (2024) focused on the case where the model class that provides distributional information for the reward has small eluder dimension. In contrast to the small action regime, their bounds smoothly scale with the second-order measure. ", "page_idx": 14}, {"type": "text", "text": "Other instance-dependent bounds in general contextual bandits Beyond second-order bounds, other works have focused on other data-dependent or instance-dependent bounds. For example, Allen-Zhu et al. (2018), Foster and Krishnamurthy (2021), and Wang et al. (2023) studied first-order (small-loss) \u221abounds for general contextual bandits and MDPs, in which the goal is to make the regret scale with $\\sqrt{L^{\\star}}$ , where $L^{\\star}$ is the cumulative loss of the best policy. These bounds are generally achievable with specialized algorithms. For general contextual bandits, gap-dependent bounds that exhibit logarithmic dependence on $T$ have been derived by Foster et al. (2020) and Dann et al. (2023) for realizable and agnostic settings, respectively. Notably, the algorithm of Foster et al. (2020) has some similarity with our main algorithm, which we discuss further in the main text. ", "page_idx": 14}, {"type": "text", "text": "B Technical Tools ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Lemma B.1. For any $0\\leq\\sigma\\leq1/2$ and $0\\le\\varepsilon\\le\\sigma/2$ , define ", "page_idx": 14}, {"type": "equation", "text": "$$\nP_{\\sigma}=\\left\\{\\!\\!\\begin{array}{l l}{2\\sigma,\\ w{i t h}\\,p r o b a.\\ 1/2,}&{P_{\\sigma,\\epsilon}^{+}=\\left\\{\\!\\!\\begin{array}{l l}{2\\sigma,\\ w{i t h}\\,p r o b a.\\ \\frac{\\sigma+\\epsilon}{2\\sigma},}&{P_{\\sigma,\\epsilon}^{-}=\\left\\{\\!\\!\\begin{array}{l l}{2\\sigma,\\ w{i t h}\\,p r o b a.\\ \\frac{\\sigma-\\epsilon}{2\\sigma},}\\\\ {0,\\ w{i t h}\\,p r o b a.\\ \\frac{\\sigma-\\epsilon}{2\\sigma}.}\\end{array}\\right.}}&{P_{\\sigma,\\epsilon}^{-}=\\left\\{\\!\\!\\begin{array}{l l}{2\\sigma,\\ w{i t h}\\,p r o b a.\\ \\frac{\\sigma-\\epsilon}{2\\sigma},}&{\\sigma<\\sigma,\\ \\ \\sigma}\\\\ {0,\\ w{i t h}\\,p r o b a.\\ \\frac{\\sigma+\\epsilon}{2\\sigma}.}\\end{array}\\right.}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Denote by $h_{\\sigma},h_{\\sigma,\\epsilon}^{+},h_{\\sigma,\\epsilon}^{-}$ the means of the three distributions respectively. We have ", "page_idx": 14}, {"type": "equation", "text": "$$\nh_{\\sigma}=\\sigma,\\;\\;h_{\\sigma,\\epsilon}^{+}=\\sigma+\\epsilon,\\;a n d\\;\\;h_{\\sigma,\\epsilon}^{-}=\\sigma-\\epsilon.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Let $V_{\\sigma},V_{\\sigma,\\epsilon}^{+},V_{\\sigma,\\epsilon}^{-}$ be the variance of the three distributions respectively. We have ", "page_idx": 14}, {"type": "equation", "text": "$$\nV_{\\sigma},V_{\\sigma,\\epsilon}^{+},V_{\\sigma,\\epsilon}^{-}\\leq\\sigma^{2}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Furthermore, ", "page_idx": 14}, {"type": "equation", "text": "$$\nD_{\\mathrm{KL}}\\bigl(P_{\\sigma,\\epsilon}^{-}\\,\\bigr\\|\\,P_{\\sigma,\\epsilon}^{+}\\bigr)\\leq\\frac{4\\epsilon^{2}}{\\sigma^{2}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. ", "page_idx": 14}, {"type": "equation", "text": "$$\nD_{\\mathrm{KL}}\\big(P_{\\sigma,\\epsilon}^{-}\\,\\|\\,P_{\\sigma,\\epsilon}^{+}\\big)=\\frac{\\sigma-\\epsilon}{2\\sigma}\\log\\frac{\\sigma-\\epsilon}{\\sigma+\\epsilon}+\\frac{\\sigma+\\epsilon}{2\\sigma}\\log\\frac{\\sigma+\\epsilon}{\\sigma-\\epsilon}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\cfrac{\\epsilon}{2\\sigma}\\log\\left(\\frac{\\sigma+\\epsilon}{\\sigma-\\epsilon}\\right)^{2}}\\\\ &{=\\cfrac{\\epsilon}{\\sigma}\\log\\left(1+\\cfrac{2\\epsilon}{\\sigma-\\epsilon}\\right)}\\\\ &{\\leq\\cfrac{\\epsilon}{\\sigma}\\cdot\\cfrac{2\\epsilon}{\\sigma-\\epsilon}}\\\\ &{\\leq\\cfrac{4\\epsilon^{2}}{\\sigma^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Lemma B.2. For any two gaussian distributions $\\mathcal{N}(\\mu_{1},\\sigma_{1})$ and ${\\mathcal{N}}(\\mu_{2},\\sigma_{2}),\\,i f\\,|\\log\\sigma_{1}-\\log\\sigma_{2}|>3,$ , then ", "page_idx": 15}, {"type": "equation", "text": "$$\nD_{\\mathrm{H}}^{2}({\\mathcal N}(\\mu_{1},\\sigma_{1}),{\\mathcal N}(\\mu_{2},\\sigma_{2}))\\geq{\\frac{1}{2}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof of Lemma B.2. Without loss of generality, assume $\\sigma_{1}\\,>\\,e^{3}\\sigma_{2}\\,>\\,8\\sigma_{2}$ . The Hellinger divergence between two gaussian distributions writes ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D_{\\mathrm{H}}^{2}(N(\\mu_{1},\\sigma_{1}),N(\\mu_{2},\\sigma_{2}))=1-\\sqrt{\\frac{2\\sigma_{1}\\sigma_{2}}{\\sigma_{1}^{2}+\\sigma_{2}^{2}}}\\exp\\biggl(-\\frac{(\\mu_{1}-\\mu_{2})^{2}}{4(\\sigma_{1}^{2}+\\sigma_{2}^{2})}\\biggr)}\\\\ &{\\geq1-\\sqrt{\\frac{2\\sigma_{1}\\sigma_{2}}{\\sigma_{1}^{2}+\\sigma_{2}^{2}}}}\\\\ &{\\geq1-\\sqrt{\\frac{2\\sigma_{1}/\\sigma_{2}}{(\\sigma_{1}/\\sigma_{2})^{2}+1}}\\geq\\frac{1}{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Lemma B.3 (Lemma 4.2 of Wang et al. (2024)). For any distribution $\\mathbb{P}$ and $\\mathbb{Q}$ on any space $\\mathcal{X}$ such that $D_{\\mathrm{H}}^{2}(\\mathbb{P},\\mathbb{Q})\\leq{\\frac{1}{2}}$ , we have for any function $h:\\mathcal{X}\\to\\mathbb{R},$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n|\\mathbb{E}_{\\mathbb{P}}[h]-\\mathbb{E}_{\\mathbb{Q}}[h]|\\leq2\\sqrt{(\\mathrm{Var}_{\\mathbb{P}}(h)+\\mathrm{Var}_{\\mathbb{Q}}(h))D_{\\mathrm{H}}^{2}(\\mathbb{P},\\mathbb{Q})}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Lemma B.4 (Lemma A.11 of Foster et al. (2021)). For any distribution $\\mathbb{P}$ and $\\mathbb{Q}$ on any space $\\mathcal{X}$ , we have for any function $h:{\\mathcal{X}}\\rightarrow[0,R]$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n|\\mathbb{E}_{\\mathbb{P}}[h]-\\mathbb{E}_{\\mathbb{Q}}[h]|\\leq\\sqrt{2R(\\mathbb{E}_{\\mathbb{P}}(h)+\\mathbb{E}_{\\mathbb{Q}}(h))D_{\\mathrm{H}}^{2}(\\mathbb{P},\\mathbb{Q})}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "In particular, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbb{P}}[h]\\leq3\\mathbb{E}_{\\mathbb{Q}}[h]+4R D_{\\mathrm{H}}^{2}(\\mathbb{P},\\mathbb{Q}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Lemma B.5 (Strengthened Freedman\u2019s inequality (Theorem 9 of Zimmert and Lattimore (2022))). Let $X_{1},X_{2},\\ldots,X_{T}$ be a martingale difference sequence with a flitration ${\\mathcal{F}}_{1}\\subseteq{\\mathcal{F}}_{2}\\subseteq\\cdot\\cdot\\cdot$ such that $\\mathbb{E}[X_{t}|\\mathcal{F}_{t}]=0$ and $\\mathbb{E}[|X_{t}|\\mid\\mathcal{F}_{t}]<\\infty$ almost surely. Then with probability at least $1-\\delta$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}X_{t}\\leq3\\sqrt{V_{T}\\log\\left(\\frac{2\\operatorname*{max}\\{U_{T},\\sqrt{V_{T}}\\}}{\\delta}\\right)}+2U_{T}\\log\\left(\\frac{2\\operatorname*{max}\\{U_{T},\\sqrt{V_{T}}\\}}{\\delta}\\right),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\begin{array}{r}{V_{T}=\\sum_{t=1}^{T}\\mathbb{E}[X_{t}^{2}\\mid\\mathcal{F}_{t}]}\\end{array}$ and $U_{T}=\\operatorname*{max}\\{1,\\operatorname*{max}_{t\\in[T]}|X_{t}|\\}$ . ", "page_idx": 15}, {"type": "text", "text": "C Omitted Proofs in Section 4.1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Lemma C.1 (Theorem 15.2 of Lattimore and Szepesv\u00e1ri (2020)). For any integer $A\\,\\geq\\,2,$ , any positive real number $\\sigma\\in[0,1]$ , and time $T>0$ , there exists a context space $\\mathcal{X}$ and a contextual bandit problem $\\mathcal{F}\\subset(\\mathcal{X}\\times\\mathcal{A}\\rightarrow\\mathbb{R})$ with action set $A=[A],$ , $d_{e l u}(\\mathcal{F};0)=A$ , and variances $\\sigma_{t}\\leq\\sigma$ for all $t\\in[T]$ such that any algorithm will suffer a regret at least $\\Omega({\\sqrt{\\sigma^{2}A T}})$ . ", "page_idx": 15}, {"type": "text", "text": "Proof of Lemma C.1. Without loss of generality assume $\\sigma<1/2$ . Fix the algorithm. The first lower bound construction follows the standard MAB lower bound. Let the context space $\\mathcal{X}=\\emptyset$ and the function class ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\boldsymbol{\\tau}=\\{f_{i}(\\cdot)=(\\sigma+\\varepsilon)\\mathbb{1}(\\cdot=i)+\\sigma\\mathbb{1}(\\cdot=1)\\mid i\\in\\{2,3,\\ldots,A\\}\\}\\cup\\{f_{1}(\\cdot)=(\\sigma-\\varepsilon)\\mathbb{1}(\\cdot=i)+\\sigma\\mathbb{1}(\\cdot=i)\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the gap $\\varepsilon$ will be specified later with the constraint $\\varepsilon\\leq2\\sigma$ . Let $P_{\\sigma}$ , $P_{\\sigma,\\epsilon}^{+}$ , and $P_{\\sigma,\\epsilon}^{-}$ be defined as in Lemma B.1. For any function $f\\in\\mathcal F$ , suppose the environment is such that if $f(i)=\\sigma,\\sigma\\,{+}\\,\\epsilon,\\sigma\\,{-}\\,\\epsilon,$ , then the reward distribution is $P_{\\sigma}$ , $P_{\\sigma,\\epsilon}^{+},\\bar{P}_{\\sigma,\\epsilon}^{-}$ respectively. Any environment and algorithm give rise to the distribution of history. Let $\\mathbb{P}_{i}$ denote the distribution generated by the environment following $f_{i}\\in\\mathcal{F}$ together with the algorithm and expectations under $\\mathbb{P}_{i}$ will be denoted by $\\mathbb{E}_{i}$ . Let $\\begin{array}{r}{N_{T}(a)=\\sum_{t=1}^{T}\\mathbb{1}(a_{t}=a)}\\end{array}$ where $a_{t}$ is the action taken at time step $t$ for $t\\in[T]$ . Let ", "page_idx": 16}, {"type": "equation", "text": "$$\ni^{\\star}=\\arg\\operatorname*{min}_{j>1}\\mathbb{E}_{1}[N_{T}(j)].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Since $\\begin{array}{r}{\\sum_{i=1}^{A}N_{T}(i)=T.}\\end{array}$ , it holds that $\\mathbb{E}_{1}[N_{T}(i^{\\star})]\\le T/(A-1)$ . For the two environments induced by $f_{1}$ and $f_{i^{\\star}}$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}_{1}[R_{T}]\\geq\\mathbb{P}_{1}(N_{T}(1)\\leq T/2)\\cdot\\frac{T\\varepsilon}{2}\\quad\\mathrm{and}\\quad\\mathbb{E}_{i^{*}}[R_{T}]\\geq\\mathbb{P}_{i^{*}}(N_{T}(1)>T/2)\\cdot\\frac{T\\varepsilon}{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then, by Bretagnolle-Huber inequality, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{1}[R_{T}]+\\mathbb{E}_{i^{\\star}}[R_{T}]\\geq\\frac{T\\varepsilon}{2}(\\mathbb{P}_{1}(N_{T}(1)\\leq T/2)+\\mathbb{P}_{i^{\\star}}(N_{T}(1)>T/2))}\\\\ &{\\qquad\\qquad\\qquad\\geq\\frac{T\\varepsilon}{4}\\exp(-D_{\\mathrm{KL}}(\\mathbb{P}_{1}\\left\\|\\mathbb{P}_{i^{\\star}})\\right)\\!.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then by the chain rule of KL divergence and Lemma B.1, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\nD_{\\mathrm{KL}}(\\mathbb{P}_{1}\\left\\Vert\\mathbb{P}_{i^{\\star}})=\\mathbb{E}_{1}[N_{T}(i^{\\star})]D_{\\mathrm{KL}}\\bigl(P_{\\sigma,\\epsilon}^{-}\\left\\Vert\\mathbf{\\check{\\mathbf{\\rho}}}P_{\\sigma,\\epsilon}^{+}\\right)\\leq\\frac{4T\\varepsilon^{2}}{(A-1)\\sigma^{2}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Thus we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}_{1}[R_{T}]+\\mathbb{E}_{i^{\\star}}[R_{T}]\\ge\\frac{T\\varepsilon}{4}\\exp\\biggl(-\\frac{4T\\varepsilon^{2}}{(A-1)\\sigma^{2}}\\biggr).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then by choosing $\\varepsilon=\\sqrt{(A-1)\\sigma^{2}/4T}$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{i}\\{\\mathbb{E}_{i}[R_{T}]\\}\\geq\\Omega(\\sqrt{\\sigma^{2}A T}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Lemma C.2. For any integer $N,A,T\\,\\geq\\,2$ , there exists a context space $\\mathcal{X}$ and a deterministic ( $\\mathit{\\dot{\\sigma}}_{\\mathit{t}}=0$ for all $t\\in[T])$ contextual bandit problem $\\mathcal{F}\\subset(\\mathcal{X}\\times\\mathcal{A}\\rightarrow\\mathbb{R})$ with action set $A=[A]$ and $d_{e l u}(\\mathcal{F};0)=N(A\\bar{-1})$ such that any algorithm will suffer a regret at least $\\Omega(\\operatorname*{min}\\{T/N,A\\dot{N}\\})$ ). ", "page_idx": 16}, {"type": "text", "text": "Proof of Lemma C.2. Consider the function class $\\mathcal{F}=\\{f^{(0)}\\}\\cup\\{f^{(i,j)}\\}_{i\\in[N],j\\in[A-1]}$ with the space of contexts $\\mathcal{X}=\\{x^{(1)},\\ldots,x^{(N)}\\}$ and the set of actions ${\\mathcal{A}}=[A]$ . For any $i\\in[N],j\\in[A\\!-\\!1]$ , the function $f^{(i,j)}$ is defined as the following: For $i\\in[N]$ and $j\\in[A-1]$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f^{(i,j)}(x^{(i)},j)=1,}\\\\ &{f^{(i,j)}(x,k)=0,\\quad\\forall x\\neq x^{(i)}\\;\\mathrm{or}\\,\\forall k\\in[A-1]\\setminus\\{j\\}.}\\\\ &{f^{(i,j)}(x,A)=\\displaystyle\\frac{1}{2},\\quad\\forall x.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Meanwhile ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{f^{(0)}(x,j)=0,\\quad\\forall x\\;\\mathrm{and}\\;\\forall j\\in[A-1]}\\\\ {f^{(0)}(x,A)=\\displaystyle\\frac{1}{2},\\quad\\forall x.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The eluder dimension of this function class is $N(A-1)$ since $f^{(i,j)}$ is uniquely identified by its value on $(x^{(i)},j)$ . We assume that $x_{t}$ is uniformly randomly chosen from $\\mathcal{X}$ , and $r_{t}=f_{\\star}(x_{t},a_{t})$ That is, $\\sigma_{t}=0$ for all $t\\in[T]$ . ", "page_idx": 17}, {"type": "text", "text": "Fix any algorithm. Denote by $\\mathbb{P}_{0}$ the probability distribution when $f^{\\star}=f^{(0)}$ and $\\mathbb{E}_{0}$ the expectation under $\\mathbb{P}_{0}$ . For any $i\\in[N],j\\in[A-1]$ , denote by $\\mathbb{P}_{(i,j)}$ the probability distribution when $f^{\\star}=f^{(i,j)}$ and $\\mathbb{E}_{(i,j)}$ the expectation under $\\mathbb{P}_{(i,j)}$ . Let $\\begin{array}{r}{N_{T}(i,j)\\,=\\,\\sum_{t=1}^{T}\\mathbb{1}[x_{t}\\,=\\,x^{(i)},a_{t}\\,=\\,j]}\\end{array}$ . Then the adversary decides $f_{\\star}$ based on the following rule: if there exists $i\\in[N],j\\in[A-1]$ such that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}_{0}\\left[N_{T}(i,j)\\right]\\le\\frac{1}{100}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "then let $f_{\\star}=f^{(i,j)}$ . If no $i,j$ satisfies this, let $f_{\\star}=f^{(0)}$ . If $f_{\\star}=f^{(0)}$ , then we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}_{0}[R_{T}]\\geq\\frac{1}{2}\\mathbb{E}_{0}\\left[\\sum_{i\\in[N],j\\in[A-1]}N_{T}(i,j)\\right]\\geq\\frac{N(A-1)}{2}\\cdot\\operatorname*{min}_{i,j}\\mathbb{E}_{0}[N_{T}(i,j)]\\geq\\frac{N(A-1)}{200}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "On the other hand, if $f_{\\star}=f^{(i,j)}$ , then we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{P}_{0}(N_{T}(i,j)=0)\\ge\\frac{99}{100}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then by Lemma B.4, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{P}_{0}(N_{T}(i,j)=0)\\le3\\mathbb{P}_{(i,j)}(N_{T}(i,j)=0)+4D_{\\mathrm{H}}^{2}\\big(\\mathbb{P}_{0},\\mathbb{P}_{(i,j)}\\big).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then by Lemma D.2 of Foster et al. (2024), we have ", "page_idx": 17}, {"type": "equation", "text": "$$\nD_{\\mathrm{H}}^{2}\\big(\\mathbb{P}_{0},\\mathbb{P}_{(i,j)}\\big)\\le7\\mathbb{E}_{0}[N_{T}(i,j)].\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Altogether, we can obtain ", "page_idx": 17}, {"type": "equation", "text": "$$\n{\\mathbb P}_{(i,j)}(N_{T}(i,j)=0)\\ge\\frac{1}{3}({\\mathbb P}_{0}(N_{T}(i,j)=0)-28{\\mathbb E}_{0}[N_{T}(i,j)])\\ge1/6.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This in turn implies that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}_{(i,j)}[R_{T}]\\geq\\frac{1}{2}\\mathbb{E}_{(i,j)}\\left[\\sum_{t=1}^{T}\\mathbb{1}[x_{t}=x^{(i)}]-N_{T}(i,j)\\right]\\geq\\frac{T}{2N}\\cdot\\mathbb{P}_{(i,j)}(N_{T}(i,j)=0)\\geq\\frac{T}{12N}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Combining the lower bounds for $\\mathbb{E}_{0}[R_{T}]$ and $\\mathbb{E}_{(i,j)}[R_{T}]$ finishes the proof. ", "page_idx": 17}, {"type": "text", "text": "Proof of Theorem 4.1. If $A>T$ and $d>T$ , by Lemma C.2 with $N=1$ , we have lower bound $\\Omega(T)$ . Below, we assume $\\operatorname*{min}\\{A,d\\}\\leq T$ . ", "page_idx": 17}, {"type": "text", "text": "In order to prove the lower bound, we only need to show that for any fixed $d,A,\\sigma,T$ such that $\\operatorname*{min}\\{A,d\\}\\leq T$ , there exists two c\u221alasses where one has lower bound $\\Omega(\\sqrt{\\sigma^{2}\\operatorname*{min}\\{A,d\\}T})$ and the other has lower bound $\\Omega(\\operatorname*{min}\\{d,\\sqrt{A T}\\})$ . ", "page_idx": 17}, {"type": "text", "text": "If $A\\leq d$ , then we invoke Lemma C.1 to obtain the lower bound of $\\Omega({\\sqrt{\\sigma^{2}A T}})$ . Else if $d\\leq A$ , we can again invoke Lemma C.1 with the action set $[d]$ and \u221athen expand the action set with dummy actions with all 0 rewards to obtain the lower bound of $\\Omega({\\sqrt{\\sigma^{2}d T}})$ . In all, we have shown that there is a lower bound of $\\Omega(\\sqrt{\\sigma^{2}\\operatorname*{min}\\{A,d\\}T})$ . ", "page_idx": 17}, {"type": "text", "text": "If $d\\leq A$ (which implies $d\\leq T$ since we assume $\\operatorname*{min}\\{A,d\\}\\leq T)$ , then we invoke Lemma C.2 with $N=1$ with action set $\\boldsymbol{\\mathcal{A}}$ be $[d]$ plus $A-d$ dummy actions. Then we get a lower bound of $\\Omega(\\operatorname*{min}\\{T,d\\})=\\Omega(d)$ . Next, consider the case $d\\geq A$ . If $d\\leq{\\sqrt{A T}}$ , then we invoke Lemma\u221a C.2 with $N=d/A$ and obtain a lower bound of $\\Omega(\\operatorname*{min}\\{A T/d,d\\})=\\Omega(d)$ . Else we have $d>{\\sqrt{A T}}$ (which implies $T\\ \\geq\\ A$ since we assume $\\operatorname*{min}\\{A,d\\}\\ \\leq\\ T)$ . Then we consider the hard case from Lemma C.2 with $N=\\sqrt{T/A}$ then the function class has Hellinger eluder dimension $\\sqrt{A T}$ Embedding this function class into a more com\u221aplex model class with a larger Hellinger eluder dimension, we can obtain the lower bound of $\\Omega({\\sqrt{A T}})$ . In all, we have shown that there is a lower bound of $\\Omega(\\operatorname*{min}\\{d,\\sqrt{A T}\\})$ . ", "page_idx": 17}, {"type": "text", "text": "Consequently, we have shown a lower bound of $\\Omega(\\sqrt{\\sigma^{2}\\operatorname*{min}\\{A,d\\}T}+\\operatorname*{min}\\{d,\\sqrt{A T}\\}).$ ", "page_idx": 17}, {"type": "text", "text": "Algorithm 4 Prod-based online regression oracle ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Input: Parameter $\\eta$ . Contextual Bandit Oracle gives function class $\\mathcal{F}_{t}\\subset\\mathcal{F}$ , action class $\\boldsymbol{A}_{t}$ and context $x_{t}$ at round $t$ . Contextual Bandit algorithm which takes online regression oracle and returns an action. ", "page_idx": 18}, {"type": "text", "text": "1: Let $q_{1}(f)=1/|\\mathcal{F}_{1}|$ for every function $f\\in\\mathcal{F}_{1}$ .   \n2: for $t=1,2,\\dots\\mathbf{do}$ ", "page_idx": 18}, {"type": "text", "text": "3: Generate ", "page_idx": 18}, {"type": "equation", "text": "$$\nf_{t}(x_{t},a)=\\sum_{f\\in\\mathcal{F}_{t}}q_{t}(f)f(x_{t},a)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "4: Output $\\left\\{f_{t}(x_{t},a):a\\in{\\mathcal{A}}_{t}\\right\\}$ , and feed to Contextual Bandit algorithm to receive $a_{t}\\in\\mathcal A_{t}$ .   \n5: Call Contextual Bandit oracle to get Ft+1 and xt+1.   \n6: Calculate ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\tilde{\\ell}_{t}(f)=2(f(x_{t},a_{t})-f_{t}(x_{t},a_{t}))(f_{t}(x_{t},a_{t})-r_{t})\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "7: and ", "page_idx": 18}, {"type": "equation", "text": "$$\nq_{t+1}(f)=\\frac{q_{t}(f)(1-\\eta\\tilde{\\ell}_{t}(f))}{\\sum_{f\\in\\mathcal{F}_{t+1}}q_{t}(f)(1-\\eta\\tilde{\\ell}_{t}(f))}\\quad\\forall f\\in\\mathcal{F}_{t+1}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "D Omitted Proofs in Section 4.2 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "D.1 Analysis of the Online Regression Oracle ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Lemma D.1 (Cesa-Bianchi et al. (2007)). Fix some positive parameter $\\eta>0$ . Suppose we have function sets $\\mathcal{F}_{1}\\supset\\mathcal{F}_{2}\\supset\\ldots\\supset\\mathcal{F}_{T}$ , and functions $\\ell_{t}:\\mathcal{F}_{t}\\to\\mathbb{R}$ satisfies that $\\eta|\\ell_{t}(\\bar{f})|\\leq\\frac{1}{2}$ for any $f\\in\\mathcal{F}_{t}$ . The prediction rule ", "page_idx": 18}, {"type": "equation", "text": "$$\nq_{t}(f)=\\left\\{\\frac{\\prod_{\\tau=1}^{t-1}(1-\\eta\\ell_{\\tau}(f))}{\\sum_{g\\in\\mathcal{F}_{t}}\\prod_{\\tau=1}^{t-1}(1-\\eta\\ell_{\\tau}(g))}\\qquad f\\in\\mathcal{F}_{t},\\right.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "ensures for any $f^{\\star}\\in\\mathcal{F}_{T}$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\left(\\sum_{f\\in\\mathcal{F}_{t}}q_{t}(f)\\ell_{t}(f)\\right)-\\sum_{t=1}^{T}\\ell_{t}(f^{\\star})\\leq\\frac{\\log|\\mathcal{F}|}{\\eta}+\\eta\\sum_{t=1}^{T}\\ell_{t}(f^{\\star})^{2}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. Define $\\begin{array}{r}{w_{t}(f)=\\prod_{\\tau=1}^{t}(1-\\eta\\ell_{\\tau}(f))}\\end{array}$ and $\\begin{array}{r}{W_{t}=\\sum_{f\\in\\mathcal{F}_{t}}w_{t}(f)}\\end{array}$ . ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\log\\frac{W_{t}}{W_{t-1}}=\\log\\frac{\\sum_{f\\in\\mathcal{F}_{t}}w_{t}(f)}{\\sum_{f\\in\\mathcal{F}_{t}}w_{t-1}(f)}}&{}\\\\ {\\leq\\log\\frac{\\sum_{f\\in\\mathcal{F}_{t}}w_{t-1}(f)\\left(1-\\eta\\ell_{t}(f)\\right)}{\\sum_{f\\in\\mathcal{F}_{t}}w_{t-1}(f)}}&{}\\\\ {\\overset{(i)}{=}\\log\\left(\\sum_{f\\in\\mathcal{F}_{t}}\\left(f\\right)\\left(1-\\eta\\ell_{t}(f)\\right)\\right)}&{}\\\\ {\\overset{(i i)}{=}\\log\\left(1-\\eta\\sum_{f\\in\\mathcal{F}_{t}}q_{\\ell}(f)\\ell_{t}(f)\\right)}&{}\\\\ {\\overset{(i i i)}{=}-\\eta\\sum_{f\\in\\mathcal{F}_{t}}q_{\\ell}(f)\\ell_{t}(f),}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where in $(i)$ we use the definition of $q_{t}$ , in $(i i)$ we use the fact that $\\textstyle\\sum_{f\\in{\\mathcal{F}}_{t}}q_{t}(f)=1$ , and in $(i i i)$ we use the inequality $\\log(1-x)\\leq-x$ for any $x<1$ . Therefore, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\sum_{f\\in\\mathcal{F}_{t}}q_{t}(f)\\ell_{t}(f)\\le\\frac{1}{\\eta}\\log\\frac{W_{0}}{W_{T}}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overset{(i)}{\\le}\\frac{\\log|\\mathcal{F}|}{\\eta}-\\frac{1}{\\eta}\\log w_{T}(f^{\\star})}\\\\ &{=\\frac{\\log|\\mathcal{F}|}{\\eta}-\\frac{1}{\\eta}\\sum_{t=1}^{T}\\log(1-\\eta\\ell_{t}(f^{\\star}))}\\\\ &{\\overset{(i i)}{\\le}\\frac{\\log|\\mathcal{F}|}{\\eta}-\\frac{1}{\\eta}\\sum_{t=1}^{T}(-\\eta\\ell_{t}(f^{\\star})-\\eta^{2}\\ell_{t}(f^{\\star})^{2})}\\\\ &{=\\frac{\\log|\\mathcal{F}|}{\\eta}+\\sum_{t=1}^{T}\\ell_{t}(f^{\\star})+\\eta\\sum_{t=1}^{T}\\ell_{t}(f^{\\star})^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where in $(i)$ we use the fact that $W_{0}=|\\mathcal{F}|$ and $W_{T}\\geq w_{T}(f^{\\star})$ for any $f^{\\star}\\in{\\mathcal{F}}_{t}$ , and in $(i i)$ we use the fact that $\\eta|\\ell_{t}(f^{\\star})|\\leq\\frac{1}{2}$ and also the inequality $\\log(1-x)\\geq-x-x^{2}$ for any $|x|\\leq{\\frac{1}{2}}$ . \u53e3 ", "page_idx": 19}, {"type": "text", "text": "Lemma D.2. Suppose for any $x\\in\\mathcal{X},a\\in\\mathcal{A},f\\in\\mathcal{F}$ , we always have $f(x,a)\\in[0,1],$ , the reward $r_{t}\\in[0,1]$ and for any $f,f^{\\prime}\\in\\mathcal{F}_{t}$ , we always have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{a\\in\\mathcal{A}_{t}}|f(x_{t},a)-f^{\\prime}(x_{t},a)|\\leq\\tilde{\\Delta}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then for the output $f_{t}$ according to Algorithm $^{4}$ , we have with probability at least $1-\\delta$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}(f_{t}(x_{t},a_{t})-r_{t})^{2}-\\sum_{t=1}^{T}(f^{\\star}(x_{t},a_{t})-r_{t})^{2}\\leq16(\\sigma^{2}+\\tilde{\\Delta})\\log{(|\\mathcal{F}|/\\delta)}\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. We first notice that with our choice of $\\begin{array}{r}{\\eta=\\frac{1}{4(\\sigma^{2}+\\tilde{\\Delta})}}\\end{array}$ , for any $f\\in\\mathcal{F}_{t}$ we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\eta|\\tilde{\\ell}_{t}(f)|\\leq2\\eta|f(x_{t},a_{t})-f_{t}(x_{t},a_{t})|\\leq2\\eta\\operatorname*{max}_{f^{\\prime}\\in\\mathcal{F}_{t}}|f(x_{t},a_{t})-f^{\\prime}(x_{t},a_{t})|\\leq\\frac{2\\tilde{\\Delta}}{4(\\sigma^{2}+\\tilde{\\Delta})}\\leq\\frac{1}{2}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\tilde{\\ell}_{t}$ is defined in Eq. (10). According to Lemma D.1, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\sum_{f\\in\\mathcal{F}_{t}}q_{t}(f)\\tilde{\\ell}_{t}(f)-\\sum_{t=1}^{T}\\tilde{\\ell}_{t}(f^{\\star})\\leq\\frac{\\log|\\mathcal{F}|}{\\eta}+\\eta\\sum_{t=1}^{T}\\tilde{\\ell}_{t}(f^{\\star})^{2},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "By the definition of $f_{t}$ in Eq. (9) and $\\tilde{\\ell}_{t}$ in Eq. (10), we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{f\\in\\mathcal{F}_{t}}q_{t}(f)\\tilde{\\ell}_{t}(f)=2\\big(f_{t}(x_{t},a_{t})-r_{t}\\big)\\cdot\\left(\\sum_{f\\in\\mathcal{F}_{t}}q_{t}(f)f(x_{t},a_{t})-f_{t}(x_{t},a_{t})\\right)=0.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Next, we observe that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\tilde{\\ell}_{t}(f^{\\star})=(f^{\\star}(x_{t},a_{t})-r_{t})^{2}-(f_{t}(x_{t},a_{t})-r_{t})^{2}-(f_{t}(x_{t},a_{t})-f^{\\star}(x_{t},a_{t}))^{2},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which implies ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{t=1}^{T}(f_{t}(x_{t},a_{t})-r_{t})^{2}-(f^{\\star}(x_{t},a_{t})-r_{t})^{2}}}\\\\ &{}&{\\displaystyle=-\\sum_{t=1}^{T}\\tilde{\\ell}_{t}(f^{\\star})-\\sum_{t=1}^{T}(f_{t}(x_{t},a_{t})-f^{\\star}(x_{t},a_{t}))^{2}\\qquad\\qquad}\\\\ &{}&{\\displaystyle\\le\\frac{\\log|\\mathcal{F}|}{\\eta}+4\\eta\\sum_{t=1}^{T}\\tilde{\\ell}_{t}(f^{\\star})^{2}-\\sum_{t=1}^{T}(f_{t}(x_{t},a_{t})-f^{\\star}(x_{t},a_{t}))^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We notice that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\tilde{\\ell}_{t}(f^{\\star})=2\\big(f_{t}(x_{t},a_{t})-f^{\\star}(x_{t},a_{t})\\big)(r_{t}-f^{\\star}(x_{t},a_{t}))\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "is a martingale difference sequence, and we further have $|\\tilde{\\ell}_{t}(f^{\\star})^{2}|\\leq4\\tilde{\\Delta}^{2}$ and ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{\\mathop{Var}}_{t}(\\tilde{\\ell}_{t}(f^{\\star})^{2})\\leq4(f_{t}(x_{t},a_{t})-f^{\\star}(x_{t},a_{t}))^{4}\\mathbb{E}_{t}[(r_{t}-f^{\\star}(x_{t},a_{t}))^{4}]}\\\\ &{\\qquad\\qquad\\leq4(f_{t}(x_{t},a_{t})-f^{\\star}(x_{t},a_{t}))^{4}\\mathbb{E}_{t}[(r_{t}-f^{\\star}(x_{t},a_{t}))^{2}]}\\\\ &{\\qquad\\qquad=4(f_{t}(x_{t},a_{t})-f^{\\star}(x_{t},a_{t}))^{4}\\sigma^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Hence according to Freedman inequality (Freedman, 1975), we have with probability $1-\\delta$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\tilde{\\ell}_{t}(f^{\\star})^{2}\\leq2\\sqrt{\\displaystyle\\sum_{t=1}^{T}4(f_{t}(x_{t},a_{t})-f^{\\star}(x_{t},a_{t}))^{4}\\sigma^{2}\\log\\frac{1}{\\delta}}+2\\cdot4\\tilde{\\Delta}^{2}\\log\\frac{1}{\\delta}}\\\\ &{\\qquad\\qquad\\qquad\\leq2\\sqrt{\\displaystyle\\sum_{t=1}^{T}(f_{t}(x_{t},a_{t})-f^{\\star}(x_{t},a_{t}))^{2}\\sigma^{2}\\cdot4\\tilde{\\Delta}^{2}\\log\\frac{1}{\\delta}}+2\\cdot4\\tilde{\\Delta}^{2}\\log\\frac{1}{\\delta}}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\sum_{t=1}^{T}(f_{t}(x_{t},a_{t})-f^{\\star}(x_{t},a_{t}))^{2}\\sigma^{2}+12\\tilde{\\Delta}^{2}\\log\\frac{1}{\\delta},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where in the second inequality we use the fact that $|f_{t}(x_{t},a_{t})-f^{\\star}(x_{t},a_{t})|_{\\star}\\le\\tilde{\\Delta}$ , and in the last inequality we use the AM-GM inequality. Hence, with our choice of $\\begin{array}{r}{\\eta=\\frac{1}{4(\\sigma^{2}+\\tilde{\\Delta})}}\\end{array}$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}(f_{t}(x_{t},a_{t})-r_{t})^{2}-\\big(f^{\\star}(x_{t},a_{t})-r_{t}\\big)^{2}}\\\\ &{\\quad\\le\\displaystyle\\frac{\\log|\\mathcal{F}|}{\\eta}+4\\eta\\displaystyle\\sum_{t=1}^{T}\\tilde{\\ell}_{t}(f^{\\star})^{2}-\\displaystyle\\sum_{t=1}^{T}(f_{t}(x_{t},a_{t})-f^{\\star}(x_{t},a_{t}))^{2}}\\\\ &{\\quad\\le4(\\sigma^{2}+\\tilde{\\Delta})\\log|\\mathcal{F}|+12\\tilde{\\Delta}^{2}\\log\\displaystyle\\frac1\\delta}\\\\ &{\\quad\\le16(\\sigma^{2}+\\tilde{\\Delta})\\log\\displaystyle\\frac{|\\mathcal{F}|}{\\delta}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where in the last inequality we use the fact that $\\tilde{\\Delta}\\leq1$ . ", "page_idx": 20}, {"type": "text", "text": "Lemma D.3. Suppose for any $x\\in\\mathcal{X},a\\in\\mathcal{A},f\\in\\mathcal{F}$ , we always have $f(x,a)\\in[0,1],$ , the reward $r_{t}\\in[0,1]$ and for any $f,f^{\\prime}\\in\\mathcal{F}_{t}$ , we always have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{a\\in\\mathcal{A}_{t}}|f(x_{t},a)-f^{\\prime}(x_{t},a)|\\leq\\tilde{\\Delta}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then for the output $f_{t}$ according to Algorithm $^{4}$ , we have with probability at least $1-\\delta$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\sum_{a\\in A}p_{t}(a)(f_{t}(x_{t},a)-f^{\\star}(x_{t},a))^{2}\\leq48(\\sigma^{2}+\\tilde{\\Delta})\\log{(2|\\mathcal{F}|/\\delta)}\\,.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. We use $\\mathcal{F}_{t}$ denote the filtration constructed by $\\mathcal{F}_{t}=\\sigma(x_{1:t},a_{1:t},r_{1:t})$ . And we let ", "page_idx": 20}, {"type": "equation", "text": "$$\nM_{t}=(f_{t}(x_{t},a_{t})-r_{t})^{2}-(f^{\\star}(x_{t},a_{t})-r_{t})^{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then we have $|M_{t}|\\leq2\\tilde{\\Delta}$ . According to (Foster and Rakhlin, 2020, Lemma 1, Lemma 4), we have with probability at least $1-\\delta/2$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{u\\in A}^{T}\\sum_{a\\in A}p_{t}(a)(f_{t}(x_{t},a)-f^{\\star}(x_{t},a))^{2}\\leq2\\sum_{t=1}^{T}\\left((f_{t}(x_{t},a_{t})-r_{t})^{2}-(f^{\\star}(x_{t},a_{t})-r_{t})^{2}\\right)+16\\tilde{\\Delta}\\log\\left(\\sum_{t=1}^{T}\\tilde{\\alpha}(f_{t}(x_{t},a_{t})-r_{t})\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Next, according to Lemma D.2, we have with probability at least $1-\\delta/2$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\left((f_{t}(x_{t},a_{t})-r_{t})^{2}-(f^{\\star}(x_{t},a_{t})-r_{t})^{2}\\right)\\leq16(\\sigma^{2}+\\tilde{\\Delta})\\log\\left(\\frac{2|\\mathcal{F}|}{\\delta}\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Hence we obtain that with probability at least $1-\\delta$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\sum_{a\\in A}p_{t}(a)(f_{t}(x_{t},a)-f^{\\star}(x_{t},a))^{2}\\leq48(\\sigma^{2}+\\tilde{\\Delta})\\log{(2|\\mathcal{F}|/\\delta)}\\,.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "D.2 Proof of Theorem 4.2 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "For simplicity, we define the following sets based on Algorithm 1: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{T}_{1}=\\{t\\in[T],\\,\\mathrm{~\\mathrm{if}~\\mathrm{{condition}~i n~L i n e}~6~h o l d s~i n~A l g o r i t h m~1\\}\\},\\quad\\mathrm{and}\\quad T_{2}=[T]\\backslash T_{1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We first show that with high probability, the true model $f^{\\star}\\in{\\mathcal{F}}_{t}$ , where $\\mathcal{F}_{t}$ is defined in Eq. (5) in Algorithm 1. ", "page_idx": 21}, {"type": "text", "text": "Lemma D.4. When the function class $\\mathcal{F}_{t}$ iteratively defined in Eq. (5), with probability at least $1-\\delta$ , we have $f^{\\star}\\in{\\mathcal{F}}_{t}$ for any $1\\leq t\\leq T$ . ", "page_idx": 21}, {"type": "text", "text": "Proof. We will prove the result by induction on $t$ . For $t\\,=\\,1$ , since $\\mathcal{F}_{1}\\,=\\,\\mathcal{F}$ and according to Assumption 2.1 we have $f^{\\star}\\,\\in\\,{\\mathcal{F}}_{1}$ . Next, we will assume that $f^{\\star}\\in\\mathcal{F}_{t-1}$ and attempt to prove $f^{\\star}\\in{\\mathcal{F}}_{t}$ . Since $r_{\\tau}=f^{\\star}(x_{\\tau},a_{\\tau})+\\epsilon_{t}$ for any $1\\leq\\tau\\leq t-1$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{r=1}^{t-1}w_{\\tau}(f_{t}(x_{\\tau},a_{\\tau})-f^{\\star}(x_{\\tau},a_{\\tau}))^{2}}\\\\ &{\\displaystyle\\qquad=\\sum_{\\tau=1}^{t-1}w_{\\tau}\\,(f_{t}(x_{\\tau},a_{\\tau})-r_{\\tau})^{2}-\\sum_{\\tau=1}^{t-1}w_{\\tau}\\,(f^{\\star}(x_{\\tau},a_{\\tau})-r_{\\tau})^{2}}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad+2\\sum_{\\tau=1}^{t-1}w_{\\tau}(f_{t}(x_{\\tau},a_{\\tau})-f^{\\star}(x_{\\tau},a_{\\tau}))(r_{\\tau}-f^{\\star}(x_{\\tau},a_{\\tau}))}\\\\ &{\\displaystyle\\qquad\\qquad\\leq2\\sum_{\\tau=1}^{t-1}w_{\\tau}\\epsilon_{\\tau}(f_{t}(x_{\\tau},a_{\\tau})-f^{\\star}(x_{\\tau},a_{\\tau})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the last inequality uses the definition that $f_{t}$ is the minimizer of $\\begin{array}{r}{\\sum_{\\tau=1}^{t-1}w_{\\tau}(f(x_{\\tau},a_{\\tau})-r_{\\tau})^{2}}\\end{array}$ in (6). ", "page_idx": 21}, {"type": "text", "text": "We notice that $w_{\\tau}(f_{t}(x_{\\tau},a_{\\tau})-f^{\\star}(x_{\\tau},a_{\\tau}))\\epsilon_{\\tau}$ is a martingale difference sequence, and since $|w_{\\tau}|\\leq$ \u03c32 , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{max}_{1\\leq\\tau\\leq t-1}|w_{\\tau}(f_{t}(x_{\\tau},a_{\\tau})-f^{\\star}(x_{\\tau},a_{\\tau}))\\epsilon_{\\tau}|\\leq\\frac{1}{\\sigma^{2}},}\\\\ &{\\displaystyle\\sum_{\\tau=1}^{t-1}(w_{\\tau}(f_{t}(x_{\\tau},a_{\\tau})-f^{\\star}(x_{\\tau},a_{\\tau}))\\epsilon_{\\tau}|)^{2}\\leq\\frac{T}{\\sigma^{4}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "According to the Strengthened Freeman\u2019s Inequality (Lemma B.5), and noticing that $\\mathbb{E}_{\\tau-1}[\\epsilon_{\\tau}^{2}]=\\sigma^{2}$ for any $1\\leq\\tau\\leq t-1$ , with probability at least $1\\,-\\,{\\frac{\\delta}{T}}$ , for any $f\\in\\mathcal F$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{\\tau=1}^{t-1}w_{\\tau}\\epsilon_{\\tau}(f_{t}(x_{\\tau},a_{\\tau})-f^{\\star}(x_{\\tau},a_{\\tau}))}\\\\ &{\\quad\\le3\\sqrt{\\displaystyle\\sum_{\\tau=1}^{t-1}w_{\\tau}^{2}\\sigma^{2}(f_{t}(x_{\\tau},a_{\\tau})-f^{\\star}(x_{\\tau},a_{\\tau}))^{2}\\log\\frac{|\\mathcal{F}|T^{2}}{\\delta\\sigma^{2}}}+2\\operatorname*{max}_{1\\le\\tau\\le t-1}|w_{\\tau}\\epsilon_{\\tau}(f_{t}(x_{\\tau},a_{\\tau})-f^{\\star}(x_{\\tau}))|}\\\\ &{\\overset{(i)}{\\le}3\\sqrt{\\displaystyle\\sum_{\\tau=1}^{t-1}w_{\\tau}(f_{t}(x_{\\tau},a_{\\tau})-f^{\\star}(x_{\\tau},a_{\\tau})^{2}L+2\\operatorname*{sup}_{\\tau\\le t-1}w_{\\tau}|f_{t}(x_{\\tau},a_{\\tau})-f^{\\star}(x_{\\tau},a_{\\tau})|L}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\overset{(i i)}{\\leq}3\\sqrt{\\sum_{\\tau=1}^{t-1}w_{\\tau}(f_{t}(x_{\\tau},a_{\\tau})-f^{\\star}(x_{\\tau},a_{\\tau}))^{2}L}+2\\operatorname*{sup}_{\\tau\\leq t-1}\\sqrt{L+\\displaystyle\\sum_{s=1}^{\\tau-1}w_{s}(f_{t}(x_{s},a_{s})-f^{\\star}(x_{s},a_{s}))^{2}L}}\\\\ {\\displaystyle\\leq5\\sqrt{\\sum_{\\tau=1}^{t-1}w_{\\tau}(f_{t}(x_{\\tau},a_{\\tau})-f^{\\star}(x_{\\tau},a_{\\tau}))^{2}L}+L}\\\\ {\\displaystyle\\overset{(i i i)}{\\leq}\\frac{1}{2}\\sum_{\\tau=1}^{t-1}w_{\\tau}(f_{t}(x_{\\tau},a_{\\tau})-f^{\\star}(x_{\\tau},a_{\\tau}))^{2}+51L,}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where in $(i)$ we use the fact that $w_{\\tau}\\leq{^1\\!/\\!\\sigma^{2}}$ for any $\\tau$ according to the definition of $w_{\\tau}$ in Algorithm 1 and using the definition L \u225c|F\u03b4|\u03c3T2 2 , in (ii) we use the fact that ", "page_idx": 22}, {"type": "equation", "text": "$$\nw_{\\tau}\\leq\\frac{\\sqrt{1+\\sum_{s=1}^{\\tau-1}w_{s}(f_{s}(x_{s},a_{s})-f^{\\star}(x_{s},a_{s}))^{2}}}{|f_{\\tau}(x_{\\tau},a_{\\tau})-f^{\\star}(x_{\\tau},a_{\\tau})|\\sqrt{L}}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "according to the definition of $w_{\\tau}$ in Algorithm 1 since $f_{\\tau},f^{\\star}\\in\\mathcal{F}_{\\tau}$ by induction hypothesis, and finally in $(i i i)$ we use AM-GM inequality. And this proves the induction hypothesis of $f^{\\star}\\in{\\mathcal{F}}_{t}$ . ", "page_idx": 22}, {"type": "text", "text": "Therefore, we obtain that with probability at least $1-\\delta$ for any $1\\leq t\\leq T$ , $f^{\\star}\\in{\\mathcal{F}}_{t}$ . ", "page_idx": 22}, {"type": "text", "text": "The following lemma is a useful result of Eluder dimension. ", "page_idx": 22}, {"type": "text", "text": "Lemma D.5. For any $\\lambda\\geq0$ and $\\alpha\\in(0,1]$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\operatorname*{min}\\left\\{\\lambda,\\;\\operatorname*{sup}_{f,f^{\\prime}\\in\\mathcal{F}_{t}}\\frac{(f(x_{t},a_{t})-f^{\\prime}(x_{t},a_{t}))^{2}}{\\alpha^{2}T+\\sum_{\\tau=1}^{t-1}(f(x_{\\tau},a_{\\tau})-f^{\\prime}(x_{\\tau},a_{\\tau}))^{2}}\\right\\}\\leq d_{e l u}(\\alpha)\\left(\\lambda+\\log T\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. This proof follows Lemma 5.1 of Ye et al. (2023). Create $T$ bins, and call them $B_{1},\\ldots,B_{T}$ .   \nEach bin is empty at the beginning. Below we will add elements in $\\{1,2,\\ldots,T\\}$ to the bins. ", "page_idx": 22}, {"type": "text", "text": "Suppose that $\\{1,2,\\ldots,t-1\\}$ have been assigned to their bins. To assign $t$ to a bin, we find the smallest $n\\in[T]$ such that $\\bullet\\exists\\epsilon\\geq\\alpha$ , $(x_{t},a_{t})$ is $\\epsilon$ -independent to the elements in $B_{n}$ with respect to $\\mathcal{F}_{t}$ .\u201d We let $n_{t}$ to be the $n$ we found, and put element $i$ into bin $B_{n_{i}}$ . ", "page_idx": 22}, {"type": "text", "text": "By the procedure above, we can conclude that for each $t$ , $\\\"\\forall\\epsilon\\geq\\alpha$ , $(x_{t},a_{t})$ is $\\epsilon$ -dependent on all of $\\{B_{1},\\dotsc,B_{n_{t}-1}\\}$ with respect to $\\mathcal{F}_{t}$ .\u201d Next, we show that this necessitates the following: for any $f,f^{\\prime}\\in\\mathcal{F}_{t}$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n(f(x_{t},a_{t})-f^{\\prime}(x_{t},a_{t}))^{2}\\leq\\alpha^{2}+{\\frac{1}{n_{t}-1}}\\sum_{\\tau=1}^{t-1}(f(x_{\\tau},a_{\\tau})-f^{\\prime}(x_{\\tau},a_{\\tau}))^{2}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "This is because if otherwise, then there exists $f,f^{\\prime}\\in\\mathcal{F}_{t}$ and a bin $B_{n}\\in\\{B_{1},\\ldots,B_{n_{t}-1}\\}$ which $(x_{t},a_{t})$ is $\\epsilon$ -dependent on $\\forall\\epsilon\\geq\\alpha$ , but ", "page_idx": 22}, {"type": "equation", "text": "$$\n(f(x_{t},a_{t})-f^{\\prime}(x_{t},a_{t}))^{2}>\\alpha^{2}+\\sum_{\\tau\\in B_{n}}(f(x_{\\tau},a_{\\tau})-f^{\\prime}(x_{\\tau},a_{\\tau}))^{2}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Choose ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\xi=\\operatorname*{max}\\left\\{\\operatorname*{max}_{\\tau\\in B_{n}}|f(x_{\\tau},a_{\\tau})-f^{\\prime}(x_{\\tau},a_{\\tau})|,\\alpha\\right\\}\\geq\\alpha.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Clearly, $|f(x_{\\tau},a_{\\tau})-f^{\\prime}(x_{\\tau},a_{\\tau})|\\leq\\xi$ for all $\\tau\\in B_{n}$ , but $|f(x_{t},a_{t})-f^{\\prime}(x_{t},a_{t})|>\\alpha$ , contradicting that $(x_{t},a_{t})$ is $\\epsilon_{}$ -dependent on $B_{n}\\,\\forall\\epsilon\\geq{1}/{\\sqrt{T}}$ . ", "page_idx": 22}, {"type": "text", "text": "We obtain that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{f,f^{\\prime}\\in\\mathcal{F}_{t}}\\frac{(f(x_{t},a_{t})-f^{\\prime}(x_{t},a_{t}))^{2}}{\\alpha^{2}T+\\sum_{\\tau=1}^{t-1}(f(x_{\\tau},a_{\\tau})-f^{\\prime}(x_{\\tau},a_{\\tau}))^{2}}\\le\\frac{1}{n_{t}-1}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Let $|B_{1}|,\\dotsc,|B_{T}|$ be the size of the bins after all elements are added. By the eluder dimension definition (Definition 2.1), we know $|B_{n}|\\leq d_{\\mathrm{elu}}(\\alpha)$ for all $n$ . Therefore, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\operatorname*{min}\\left\\{\\lambda,\\ \\operatorname*{sup}_{f,f^{\\prime}\\in\\mathcal{F}_{t}}\\frac{(f(x_{t},a_{t})-f^{\\prime}(x_{t},a_{t}))^{2}}{1+\\sum_{\\tau=1}^{t-1}(f(x_{\\tau},a_{\\tau})-f^{\\prime}(x_{\\tau},a_{\\tau}))^{2}}\\right\\}}\\\\ &{=\\displaystyle\\sum_{t:n_{t}=1}\\lambda+\\sum_{n=2}^{T}\\sum_{\\tau:n_{t}=n}\\frac{1}{n-1}}\\\\ &{\\leq\\lambda d_{\\mathrm{elu}}(\\alpha)+\\displaystyle\\sum_{n=2}^{T}\\frac{d_{\\mathrm{elu}}(\\alpha)}{n-1}}\\\\ &{\\leq\\lambda d_{\\mathrm{elu}}(\\alpha)+d_{\\mathrm{elu}}(\\alpha)\\log T.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Lemma D.6. Suppose we have positive number $B$ such that for any $t\\in[T]$ , $w_{t}\\in[0,B]$ . Then for any $\\lambda\\geq0$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\operatorname*{min}\\left\\{\\lambda,\\ \\operatorname*{sup}_{f,f^{\\prime}\\in\\mathcal{F}_{t}}\\frac{w_{t}(f(x_{t},a_{t})-f^{\\prime}(x_{t},a_{t}))^{2}}{1+\\sum_{\\tau=1}^{t-1}w_{\\tau}(f(x_{\\tau},a_{\\tau})-f^{\\prime}(x_{\\tau},a_{\\tau}))^{2}}\\right\\}}\\\\ &{\\quad\\leq3d_{e l u}(1/\\sqrt{B T})(\\lambda+\\log T)\\log(B T).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. We define set ${\\mathcal{T}}_{i}\\,=\\,\\{t\\,\\in\\,[T]\\,:\\,2^{i-1}/T\\,\\le\\,w_{t}\\,\\le\\,2^{i}/T\\}$ for any $1\\,\\leq\\,i\\,\\leq\\,\\log(B T)$ . and $\\mathcal{T}_{0}=\\{t\\in[T]:w_{t}\\in[0,1/T]\\}$ . Then we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n[T]\\subset\\cup_{i=1}^{\\log(B/A)}\\mathcal{T}_{i}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Additionally, we notice that for any $1\\leq i\\leq\\log(B T)$ and $t\\in T_{i}$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{w_{t}(f(x_{t},a_{t})-f^{\\prime}(x_{t},a_{t}))^{2}}{1+\\sum_{t=1}^{t-1}w_{\\tau}(f(x_{\\tau},a_{\\tau})-f^{\\prime}(x_{\\tau},a_{\\tau}))^{2}}}\\\\ &{\\qquad\\le\\frac{w_{t}(f(x_{t},a_{t})-f^{\\prime}(x_{t},a_{t}))^{2}}{1+\\sum_{t\\in\\mathcal{T}_{i}}w_{\\tau}(f(x_{\\tau},a_{\\tau})-f^{\\prime}(x_{\\tau},a_{\\tau}))^{2}}}\\\\ &{\\qquad\\overset{(i)}{\\le}2\\cdot\\frac{(f(x_{t},a_{t})-f^{\\prime}(x_{t},a_{t}))^{2}}{1/(2^{i}/T)+\\sum_{\\tau\\in\\mathcal{T}_{i},\\tau<t}(f(x_{\\tau},a_{\\tau})-f^{\\prime}(x_{\\tau},a_{\\tau}))^{2}}}\\\\ &{\\qquad\\overset{(i i)}{\\le}2\\cdot\\frac{(f(x_{t},a_{t})-f^{\\prime}(x_{t},a_{t}))^{2}}{1/B+\\sum_{\\tau\\in\\mathcal{T}_{i},\\tau<t}(f(x_{\\tau},a_{\\tau})-f^{\\prime}(x_{\\tau},a_{\\tau}))^{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where in $(i)$ we use the fact that for any $\\tau\\in T_{i}$ , $w_{\\tau}\\,\\in\\,[2^{i-1}/T,2^{i}/T]$ , and in $(i i)$ we use the fact that $2^{i}/T\\le B$ . According to Lemma D.5, we have for any $1\\leq i\\leq\\log(B T)$ . ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\underset{\\in\\mathcal{T}_{i}}{\\sum\\operatorname*{min}}\\left\\lbrace\\lambda,\\,\\underset{f,f^{\\prime}\\in\\mathcal{F}_{t}}{\\operatorname*{sup}}\\frac{(f(x_{t},a_{t})-f^{\\prime}(x_{t},a_{t}))^{2}}{1/B+\\sum_{\\tau\\in\\mathcal{T}_{i},\\tau<t}(f(x_{\\tau},a_{\\tau})-f^{\\prime}(x_{\\tau},a_{\\tau}))^{2}}\\right\\rbrace\\leq d_{\\mathrm{elu}}(1/\\sqrt{B T})(\\lambda+\\log|\\mathcal{T}_{i}|)}\\\\ &{}&{\\leq d_{\\mathrm{elu}}(1/\\sqrt{B T})(\\lambda+\\log T).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Next we notice that for those $t\\in T_{0}$ , we have $w_{t}\\leq1/T$ , which implies that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sum_{t\\in\\mathcal{T}_{i}}\\operatorname*{min}\\left\\{\\lambda,\\operatorname*{sup}_{f,f^{\\prime}\\in\\mathcal{F}_{t}}\\frac{w_{t}(f(x_{t},a_{t})-f^{\\prime}(x_{t},a_{t}))^{2}}{1+\\sum_{\\tau<t}w_{\\tau}(f(x_{\\tau},a_{\\tau})-f^{\\prime}(x_{\\tau},a_{\\tau}))^{2}}\\right\\}\\leq T\\cdot1/T=1.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{i=1}^{T}\\operatorname*{min}\\left\\{\\lambda,\\ \\operatorname*{sup}_{f,f^{\\prime}\\in\\mathcal{F}_{t}}\\frac{w_{t}(f(x_{t},a_{t})-f^{\\prime}(x_{t},a_{t}))^{2}}{1+\\sum_{\\tau=1}^{t-1}w_{\\tau}(f(x_{\\tau},a_{\\tau})-f^{\\prime}(x_{\\tau},a_{\\tau}))^{2}}\\right\\}}}\\\\ &{\\quad\\le2d_{\\mathrm{elu}}(1/\\sqrt{B T})(\\lambda+\\log T)\\log(B T)+1\\le3d_{\\mathrm{elu}}(1/\\sqrt{B T})(\\lambda+\\log T)\\log(B T).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Next, we present a lemma bounding the single-step regret in terms of $g_{t}$ defined in Eq. (4) in Algorithm 1. ", "page_idx": 24}, {"type": "text", "text": "Lemma D.7. Suppose $f^{\\star}\\in{\\mathcal{F}}_{t}$ for any $1\\leq t\\leq T$ . For any $1\\leq t\\leq T$ we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{a\\in{\\mathcal{A}}_{t}}f^{\\star}(x_{t},a)-f^{\\star}(x_{t},a_{t})\\leq21\\sqrt{L}\\cdot\\operatorname*{max}_{a}g_{t}(a)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. In the following, we assume $f^{\\star}\\in{\\mathcal{F}}_{t}$ always holds. Let ", "page_idx": 24}, {"type": "equation", "text": "$$\na_{t}^{\\mathsf{u c b}}=\\arg\\operatorname*{max}_{a\\in A_{t}}\\operatorname*{max}_{f\\in\\mathcal{F}_{t}}f(x_{t},a).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Then we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{a\\in\\mathcal{A}_{t}}{\\operatorname*{max}}f^{*}(x_{t},a)-f^{*}(x_{t},a_{t})}\\\\ &{\\overset{(i)}{\\le}\\frac{\\operatorname*{max}}{f\\in\\mathcal{F}_{t}}f(x_{t},a_{t}^{\\mathrm{ucb}})-f^{*}(x_{t},a_{t})}\\\\ &{=\\underset{f\\in\\mathcal{F}_{t}}{\\operatorname*{max}}f(x_{t},a_{t}^{\\mathrm{ucb}})-\\underset{f\\in\\mathcal{F}_{t}}{\\operatorname*{min}}f(x_{t},a_{t}^{\\mathrm{ucb}})+\\underset{f\\in\\mathcal{F}_{t}}{\\operatorname*{min}}f(x_{t},a_{t}^{\\mathrm{ucb}})-\\underset{f\\in\\mathcal{F}_{t}}{\\operatorname*{max}}f(x_{t},a_{t})+\\underset{f\\in\\mathcal{F}_{t}}{\\operatorname*{max}}f(x_{t},a_{t})-}\\\\ &{\\le2\\underset{f\\in\\mathcal{F}_{t}}{\\operatorname*{max}}\\underset{a\\in\\mathcal{A}_{t}}{\\operatorname*{max}}[f(x_{t},a)-f^{\\prime}(x_{t},a)]+\\underset{f\\in\\mathcal{F}_{t}}{\\operatorname*{min}}f(x_{t},a_{t}^{\\mathrm{ucb}})-\\underset{f\\in\\mathcal{F}_{t}}{\\operatorname*{max}}f(x_{t},a_{t})}\\\\ &{\\overset{(i i)}{\\le}2\\underset{f,f\\in\\mathcal{F}_{t}}{\\operatorname*{max}}\\underset{a\\in\\mathcal{M}}{\\operatorname*{max}}[f(x_{t},a)-f^{\\prime}(x_{t},a)]}\\\\ &{\\overset{(i i i)}{\\le}21\\underset{f,f^{\\prime}\\in\\mathcal{F}_{t}}{\\operatorname*{max}}\\frac{1f(x_{t},a)-1f^{\\prime}(x_{t},a))}{\\operatorname*{max}}\\underset{c\\_{t}}{\\operatorname*{max}}-f^{\\prime}(x_{t},a)]}\\\\ &{\\overset{(i i i i)}{\\le}21\\underset{f,f^{\\prime}\\in\\mathcal{F}_{t}}{\\operatorname*{max}}\\frac{1f(x_{t},a)-1f^{\\prime}(x_{t},a)-1}{\\operatorname*{max}}f(x_{t},a_{t})-f^{\\prime}(x_{\\tau},a_{\\tau}))^{2}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Here in $(i)$ we use the fact that $f^{\\star}\\in{\\mathcal{F}}_{t}$ , and in $(i i)$ we first use the definition of $\\boldsymbol{A}_{t}$ in Line 4 of Algorithm 1 that there exists $f^{\\prime}\\in\\mathcal{F}_{t}$ such that $a_{t}=\\operatorname*{max}_{a\\in\\mathcal{A}}f^{\\prime}(x_{t},a)$ , which implies ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{f\\in{\\mathcal F}_{t}}f(x_{t},a_{t}^{\\mathrm{ucb}})\\leq f^{\\prime}(x_{t},a_{t}^{\\mathrm{ucb}})\\leq f^{\\prime}(x_{t},a_{t})\\leq\\operatorname*{max}_{f\\in{\\mathcal F}_{t}}f(x_{t},a_{t}).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "In $(i i i)$ we use the definition of Eq. (5) that for any $f,f^{\\prime}\\in\\mathcal{F}_{t}$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n1+\\sum_{\\tau<t}w_{\\tau}(f(x_{\\tau},a_{\\tau})-f^{\\prime}(x_{\\tau},a_{\\tau}))^{2}\\leq1+102L\\leq103L.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Our next lemma provides an upper bound to the expectation of regret for rounds falling into $\\mathcal{T}_{1}$ . ", "page_idx": 24}, {"type": "text", "text": "Lemma D.8. Suppose $f^{\\star}\\in{\\mathcal{F}}_{t}$ for any $1\\leq t\\leq T$ . We have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{t\\in\\mathcal{T}_{1}}(\\operatorname*{max}_{a\\in A}f^{\\star}(x_{t},a)-f^{\\star}(x_{t},a_{t}))\\leq\\tilde{O}\\left(\\frac{\\sigma^{2}d_{e l u}\\sqrt{L}}{\\Delta}+d_{e l u}L\\right),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where we use $\\tilde{\\mathcal{O}}$ to hide constants and factors of $\\log\\left({\\frac{T}{\\sigma\\Delta}}\\right)$ . ", "page_idx": 24}, {"type": "text", "text": "Proof. We first define $\\begin{array}{r}{M=\\log\\frac{1}{\\Delta\\sqrt{T}}}\\end{array}$ and $\\beta=21\\sqrt{L}$ and recall the definition of $g_{t}$ in Eq. (4). ", "page_idx": 24}, {"type": "equation", "text": "$$\ng_{t}(a_{t})=\\operatorname*{sup}_{f,f^{\\prime}\\in\\mathcal{F}_{t}}\\frac{|f(x_{t},a_{t})-f^{\\prime}(x_{t},a_{t})|}{\\sqrt{1+\\sum_{\\tau=1}^{t-1}w_{\\tau}(f(x_{\\tau},a_{\\tau})-f^{\\prime}(x_{\\tau},a_{\\tau}))^{2}}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "In the following, when with no ambiguity, we write $g_{t}\\,=\\,g_{t}(a_{t})$ . Since for any $f\\in\\mathcal{F}$ , we have $f(x,a)\\in[0,1]$ , we have $\\lvert g_{t}\\rvert\\leq1$ . Also notice that for any $t\\in\\mathcal{T}_{1}$ , we have $g_{t}\\ge\\Delta$ according to Algorithm 1. ", "page_idx": 24}, {"type": "text", "text": "We further define subsets $\\tau_{11h},\\tau_{12h}\\subset[T]$ for every $h=\\Delta\\cdot2^{i}$ with $i=0,1,2,\\cdot\\cdot\\cdot,M-1$ as ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{T}_{11h}\\triangleq\\mathcal{T}_{1}\\cap\\{t:\\:h\\leq g_{t}\\leq2h\\}\\cap\\left\\{t:\\:w_{t}=\\frac{1}{\\sigma^{2}}\\right\\},}\\\\ {\\mathrm{and}\\quad\\mathcal{T}_{12h}\\triangleq\\mathcal{T}_{1}\\cap\\{t:\\:h\\leq g_{t}\\leq2h\\}\\cap\\left\\{t:\\:w_{t}\\neq\\frac{1}{\\sigma^{2}}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Since $\\Delta\\leq g_{t}\\leq1$ for $t\\in\\mathcal T_{1}$ , we have ", "page_idx": 25}, {"type": "equation", "text": "$$\nT_{1}\\subset\\bigcup_{i=0}^{M-1}{\\mathcal{T}}_{11(2^{i}\\Delta)}\\bigcup_{i=0}^{M-1}{\\mathcal{T}}_{12(2^{i}\\Delta)}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Next, we fix $h$ , we analyze $\\ensuremath{\\tau_{\\!11h}}$ and $\\ensuremath{\\tau_{\\!12h}}$ separately. For those $t\\in T_{11h}$ , we have: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left.\\begin{array}{r l}&{\\underset{t\\in\\mathcal{T}_{11h}}{\\sum}\\underset{a\\in\\mathcal{A}_{t}}{\\mathrm{max}}\\ f^{*}(x_{t},a)-f^{*}(x_{t},a_{t}))}\\\\ &{\\stackrel{(i)}{\\le}\\underset{t\\in\\mathcal{T}_{11h}}{\\sum}\\operatorname*{min}\\left\\{1,\\beta\\underset{f,f^{\\prime}\\in\\mathcal{F}_{t}}{\\operatorname*{max}}\\frac{\\vert f(x_{t},a_{t})-f^{\\prime}(x_{t},a_{t})\\vert}{\\sqrt{1+\\sum_{\\tau<t}w_{\\tau}\\left(f(x_{\\tau},a_{\\tau})-f^{\\prime}(x_{\\tau},a_{\\tau})\\right)^{2}}}\\right\\}}\\\\ &{\\stackrel{(i i)}{=}\\underset{t\\in\\mathcal{T}_{11h}}{\\sum}\\operatorname*{min}\\{1,\\beta g_{t}\\}}\\\\ &{\\stackrel{(i i i)}{\\le}\\underset{\\mathrm{min}\\{1,2\\beta h\\}\\vert\\mathcal{T}_{11h}\\vert,\\vert\\lambda\\vert}{\\sum},}\\end{array}\\right\\}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where in $(i)$ we use Lemma D.7 and the fact that $|f^{\\star}(x,a)-f^{\\star}(x,a^{\\prime})|\\leq1$ for any action $a,a^{\\prime}\\in A$ , in $(i i)$ we use the definition of $g_{t}$ in Eq. (4) and the simplification $g_{t}:=g_{t}(a_{t})$ , and in $(i i i)$ we use the definition of $\\mathcal{T}_{11h}$ that for any $t\\in T_{11h}$ we always have $g_{t}\\leq2h$ . Next, we bound the cardinality of each $\\mathcal{T}_{11h}$ : ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left.T_{\\mathrm{IM}}\\right|\\overset{(i)}{\\underset{c\\in{\\mathcal{T}}_{111}}{\\leq}}\\left\\lbrace\\xi_{1}\\left[\\xi_{1}\\geq h\\right]1\\right\\rbrace\\left\\lbrace\\overline{{v_{i}}}\\right.}\\\\ {\\left.\\overset{(i)}{\\underset{c\\in\\mathcal{T}_{111}}{\\leq}}\\frac{1}{h_{i}\\tau_{\\mathrm{GTh}}}\\operatorname*{min}\\left\\lbrace h_{,\\theta,t}\\right\\rbrace1\\left\\lbrace\\overline{{w_{i}}}-\\frac{1}{\\sigma^{2}}\\right\\rbrace}\\\\ {\\left.\\overset{(i i)}{\\underset{c\\in\\mathcal{T}_{111}}{\\leq}}\\frac{\\sigma}{h_{i}\\tau_{\\mathrm{GTh}}}\\operatorname*{min}\\left\\lbrace\\overline{{\\underline{{\\sigma}}}},\\sqrt{w_{i}}g_{i}\\right\\rbrace}\\\\ {\\left.\\overset{(i i)}{\\underset{c\\in\\mathcal{T}_{111}}{\\leq}}\\frac{1}{h_{i}\\tau_{\\mathrm{GTh}}}\\right\\rbrace\\sum_{\\ell\\in\\mathcal{T}_{111}}\\operatorname*{min}\\left\\lbrace\\frac{\\hat{L}^{2}}{\\sigma^{2}},w_{i}g_{i}^{2}\\right\\rbrace}\\\\ {\\left.\\overset{(i i)}{\\underset{c\\in\\mathcal{T}_{111}}{\\leq}}\\frac{1}{h_{i}\\tau_{\\mathrm{GTh}}}\\right\\rbrace\\sum_{\\ell\\in\\mathcal{T}_{211}}\\operatorname*{sup}\\left\\lbrace\\frac{w_{i}(f(\\ell,\\alpha_{i})-f^{\\prime}(\\ell,\\alpha_{i}))^{2}}{f\\mathcal{L}^{\\prime}f(\\ell,\\alpha_{i})}\\right\\rbrace}\\\\ {\\left.\\overset{(e)}{\\underset{c\\in\\mathcal{T}_{111}}{\\leq}}\\frac{\\sigma}{h_{i}\\tau_{\\mathrm{GTh}}}\\right\\rbrace\\left(\\frac{\\hat{L}^{2}}{\\sigma^{2}},\\underset{f\\in\\mathcal{T}_{2}}{\\operatorname*{sup}}\\frac{1}{h_{i}^{2}(\\tau_{\\mathrm{GT}}^{2})}\\right)\\log\\left(\\frac{T(\\ell,\\alpha_{i})-f^{\\prime}(\\ell,\\alpha_{i})-f^{\\prime}(\\ell,\\alpha_{i}))^{2}}{\\sigma^{2}}\\right\\rbrace}\\\\ {\\left.\\overset{(e)}{\\underset{c\\in\\mathcal{T}_{11}}{\\leq}}\\frac{1}{\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where in $(i)$ we use the definition of $\\mathcal{T}_{11h}$ , in $(i i)$ we merely use the inequality that $\\mathbb{I}\\{g\\ge h\\}\\le$ $\\textstyle{\\frac{1}{h}}\\operatorname*{min}\\{g,\\dot{h}\\}$ for any $g,h\\;\\geq\\;0$ , in $(i i i)$ we use the the fact that $\\mathbb{I}\\{w_{t}\\,=\\,1/\\dot{\\sigma^{2}}\\}\\,\\le\\,\\sqrt{w_{t}}\\sigma$ since $w_{t}\\leq1/\\sigma^{2}$ always holds, in $(i v)$ we use Cauchy-Schwarz inequality, in $(v)$ we use the definition of $g_{t}$ in Eq. (4), and finanly in $(v i)$ we use Lemma D.6 with $\\bar{\\lambda}=\\bar{\\,h}^{2}/\\sigma^{2}$ and also $w_{t}\\,\\in\\,[0,1/\\sigma^{2}]$ . Therefore, we obtain that ", "page_idx": 25}, {"type": "equation", "text": "$$\n|\\mathcal{T}_{11h}|\\leq3\\left(\\frac{\\sigma^{2}\\log T}{h^{2}}+1\\right)d_{\\mathrm{elu}}\\left(\\frac{\\sigma}{\\sqrt{T}}\\right)\\log\\left(\\frac{T}{\\sigma^{2}}\\right),\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "which implies ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\sum_{t\\in\\mathcal{T}_{11h}}(\\operatorname*{max}_{a\\in\\mathcal{A}_{t}}f^{\\star}(x_{t},a)-f^{\\star}(x_{t},a_{t}))\n$$", "text_format": "latex", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq\\operatorname*{min}\\left\\{6\\beta h,3\\right\\}\\left(\\frac{\\sigma^{2}\\log T}{h^{2}}+1\\right)d_{\\mathrm{elu}}\\left(\\frac{\\sigma}{\\sqrt{T}}\\right)\\log\\left(\\frac{T}{\\sigma^{2}}\\right)}\\\\ &{\\leq\\left(\\frac{6\\beta\\sigma^{2}\\log T}{h}+3\\right)d_{\\mathrm{elu}}\\left(\\frac{\\sigma}{\\sqrt{T}}\\right)\\log\\left(\\frac{T}{\\sigma^{2}}\\right)}\\\\ &{\\leq\\left(\\frac{126\\sigma^{2}\\log T\\sqrt{L}}{h}+3\\right)d_{\\mathrm{elu}}\\left(\\frac{\\sigma}{\\sqrt{T}}\\right)\\log\\left(\\frac{T}{\\sigma^{2}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Next, we will deal with those $t$ in $\\ensuremath{\\tau_{\\!12h}}$ . Similar to the proof for $\\ensuremath{\\tau_{\\!11h}}$ , for any fixed $h$ we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sum_{t\\in{\\mathcal{T}}_{12h}}(\\operatorname*{max}_{a\\in A_{t}}f^{\\star}(x_{t},a)-f^{\\star}(x_{t},a_{t}))\\leq\\operatorname*{min}\\{1,2\\beta h\\}|{\\mathcal{T}}_{12h}|.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "And we can upper bound the cardinality of $\\ensuremath{\\tau_{\\!12h}}$ as ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|T_{12h}\\right|=\\displaystyle\\sum_{\\ell\\in T_{12h}}\\mathbb{I}\\left\\{g_{\\ell}\\geq h\\right\\}\\mathbb{I}\\left\\{w_{\\ell}\\neq\\frac{1}{\\sigma^{2}}\\right\\}}\\\\ &{\\leq\\displaystyle\\frac{1}{h}\\sum_{t\\in T_{12h}}\\operatorname*{min}\\left\\{h,g_{\\ell}\\right\\}\\mathbb{I}\\left\\{w_{t}\\neq\\frac{1}{\\sigma^{2}}\\right\\}}\\\\ &{\\overset{(i)}{=}\\displaystyle\\frac{1}{h}\\sum_{t\\in T_{12h}}\\operatorname*{min}\\left\\{h,\\operatorname*{sup}_{t}\\frac{|f(x_{t},a_{t})-f^{\\prime}(x_{t},a_{t})|}{\\sqrt{1+\\sum_{\\ell=1}^{t-1}w_{\\ell}\\left(f(x_{t},a_{\\tau})-f^{\\prime}(x_{\\tau},a_{\\tau})\\right)^{2}}}\\right\\}\\mathbb{I}\\left\\{w_{t}\\neq\\frac{1}{\\sigma^{2}}\\right\\}}\\\\ &{\\overset{(i i)}{\\leq}\\displaystyle\\frac{1}{h}\\sum_{t\\in T_{12h}}\\operatorname*{min}\\left\\{h,\\sqrt{L}\\underset{f,f^{\\prime}\\in T_{1}}{\\operatorname*{sup}}\\frac{w_{t}(f(x_{t},a_{t})-f^{\\prime}(x_{t},a_{t}))^{2}}{(f-1)\\,w_{\\ell}\\left(f\\left(x_{\\tau},a_{\\tau}\\right)-f^{\\prime}(x_{\\tau},a_{\\tau})\\right)^{2}}\\right\\}}\\\\ &{\\overset{(i i i)}{\\leq}\\displaystyle\\frac{3}{h}d_{\\mathrm{et}}\\left(\\frac{\\sigma}{\\sqrt{T}}\\right)\\left(\\sqrt{L}\\log T+h\\right)\\log\\left(\\frac{T}{\\sigma^{2}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where in $(i)$ we use the definition of $g_{t}$ in Eq. (4), in $(i i)$ we use the definition of $w_{t}$ that when $w_{t}\\neq1/\\sigma^{2}$ , we always have ", "page_idx": 26}, {"type": "equation", "text": "$$\nw_{t}=\\operatorname*{min}_{f,f^{\\prime}\\in\\mathcal{F}_{t}}\\frac{\\sqrt{1+\\sum_{\\tau=1}^{t-1}w_{\\tau}(f(x_{\\tau},a_{\\tau})-f^{\\prime}(x_{\\tau},a_{\\tau}))^{2}}}{|f(x_{t},a_{t})-f^{\\prime}(x_{t},a_{t})|\\sqrt{L}},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and $(i i i)$ is according to Lemma D.6 and the fact that $w_{t}\\in[0,{1}/{\\sigma^{2}}]$ . ", "page_idx": 26}, {"type": "text", "text": "Therefore, according to Eq. (15), we obtain that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{t\\in\\mathcal T_{12h}}\\bigl(\\operatorname*{max}_{a\\in\\mathcal A_{t}}f^{\\star}(x_{t},a)-f^{\\star}(x_{t},a_{t})\\bigr)\\leq\\operatorname*{min}\\bigl\\{1,2\\beta h\\bigr\\}|\\mathcal T_{12h}|}}\\\\ &{\\in\\left(6\\beta\\sqrt{L}\\log T+3\\right)d_{\\mathrm{elu}}\\left(\\frac{\\sigma}{\\sqrt{T}}\\right)\\log\\left(\\frac{T}{\\sigma^{2}}\\right)}\\\\ &{=\\left(126L\\log T+3\\right)d_{\\mathrm{elu}}\\left(\\frac{\\sigma}{\\sqrt{T}}\\right)\\log\\left(\\frac{T}{\\sigma^{2}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Finally, recalling Eq. (14), if we sum the regret obtained above over $h$ , we obtain ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{t\\in{T_{1}}}(\\operatorname*{max}_{t}f^{\\star}(x_{t},a)-f^{\\star}(x_{t},a_{t}))}\\quad}&{}\\\\ &{=\\sum_{i=0}^{M-1}\\left[\\sum_{t\\in{T_{11}}(2^{i}\\Delta)}\\left(\\operatorname*{max}_{a\\in A_{t}}f^{\\star}(x_{t},a)-f^{\\star}(x_{t},a_{t})\\right)+\\sum_{t\\in{T_{12}}(2^{i}\\Delta)}\\left(\\operatorname*{max}_{a\\in A_{t}}f^{\\star}(x_{t},a)-f^{\\star}(x_{t},a_{t})\\right)\\right]}\\\\ &{\\lesssim\\sum_{i=0}^{M-1}\\left[\\frac{d_{\\mathrm{ell}}\\sigma^{2}\\sqrt{L}}{(2^{i}\\Delta)}+d_{\\mathrm{ell}}+d_{\\mathrm{ell}}A+d_{\\mathrm{ell}}\\right]\\log\\left(\\frac{T}{\\sigma}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "(using $d_{\\mathrm{elu}}\\triangleq d_{\\mathrm{elu}}(1/T^{2})$ from Section 2 and the fact that $d_{\\mathrm{elu}}(\\alpha)$ is decreasing in $\\alpha$ ) ", "page_idx": 26}, {"type": "equation", "text": "$$\n=\\Tilde{\\mathcal{O}}\\left(\\frac{\\sigma^{2}d_{\\mathrm{elu}}\\sqrt{L}}{\\Delta}+d_{\\mathrm{elu}}L\\right).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Finally, we provide a lemma which upper bounds the expectation of regret for rounds falling into $\\mathcal{T}_{2}$ . ", "page_idx": 27}, {"type": "text", "text": "Lemma D.9. Suppose that for any $t\\in[T].$ , we have $f^{\\star}\\in{\\mathcal{F}}_{t}$ . With $\\tilde{\\Delta}=11\\Delta\\sqrt{L},$ , we have with probability at least $1-\\delta$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\sum_{t\\in T_{*}}(\\operatorname*{max}_{a}f^{\\star}(x_{t},a)-f^{\\star}(x_{t},a_{t}))\\leq16\\sqrt{(\\sigma^{2}+\\tilde{\\Delta})T\\log\\left(\\frac{4|\\mathcal{F}|}{\\delta}\\right)}+3\\log\\left(\\frac{2}{\\delta}\\right).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. We let $a_{t}^{\\star}=\\arg\\operatorname*{max}_{a}f^{\\star}(x_{t},a)$ . According to (Foster and Rakhlin, 2020, Lemma 3), we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\sum_{a\\in A}p_{t}(a)(f^{\\star}(x_{t},a_{t}^{\\star})-f^{\\star}(x_{t},a))\\leq\\frac{2A}{\\gamma}+\\frac{\\gamma}{4}\\sum_{a\\in A}p_{t}(a)\\left[(f_{t}(x_{t},a)-f^{\\star}(x_{t},a))^{2}\\right]\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Summing this inequality up for all $t\\in\\mathcal T_{2}$ , we obtain ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\sum_{t\\in{\\mathcal{T}}_{2}}\\sum_{a\\in A}p_{t}(a)(f^{\\star}(x_{t},a_{t}^{\\star})-f^{\\star}(x_{t},a))\\leq{\\frac{2A|T_{2}|}{\\gamma}}+{\\frac{\\gamma}{4}}\\sum_{t\\in{\\mathcal{T}}_{2}}\\sum_{a\\in A}p_{t}(a)(f_{t}(x_{t},a_{t})-f^{\\star}(x_{t},a))^{2}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We notice that for any $t\\in\\mathcal T_{2}$ , we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{f,f^{\\prime}\\in\\mathcal{F}_{t}}\\operatorname*{max}_{a\\in A}\\frac{|f(x_{t},a)-f^{\\prime}(x_{t},a)|}{\\sqrt{1+\\sum_{\\tau=1}^{t-1}w_{\\tau}(f(x_{\\tau},a_{\\tau})-f^{\\prime}(x_{\\tau},a_{\\tau}))^{2}}}\\leq\\Delta.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "According to the definition of $\\mathcal{F}_{t}$ , for any $f,f^{\\prime}\\in\\mathcal{F}_{t}$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\sum_{\\tau=1}^{t-1}w_{\\tau}(f(x_{\\tau},a_{\\tau})-f^{\\prime}(x_{\\tau},a_{\\tau}))^{2}\\leq102L,\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "which implies that for any $f,f^{\\prime}\\in\\mathcal{F}_{t}$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{a\\in{\\cal{A}}}|f(x_{t},a)-f^{\\prime}(x_{t},a)|\\leq\\Delta\\sqrt{1+102L}\\leq\\tilde{\\Delta}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Hence according to Lemma D.2, with probability at least $1-\\delta/2$ , we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\sum_{a\\in\\mathcal{A}}p_{t}(a)(f_{t}(x_{t},a)-f^{\\star}(x_{t},a))^{2}\\leq48(\\sigma^{2}+\\tilde{\\Delta})\\left(\\frac{4|\\mathcal{F}|}{\\delta}\\right).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Further noticing that |T2| \u2264T, with choice \u03b3 = 4( \u2206\u02dc+\u03c32)A lTog(|F|/\u03b4), we have with probability at least $1-\\delta/2$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\sum_{t\\in T_{2}}\\sum_{a\\in A}p_{t}(a)(f^{\\star}(x_{t},a_{t}^{\\star})-f^{\\star}(x_{t},a))\\leq\\sqrt{48(\\sigma^{2}+\\tilde{\\Delta})T\\log\\Big(\\frac{4|\\mathcal{F}|}{\\delta}\\Big)}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Finally, since we always have $0\\leq f^{\\star}(x_{t},a_{t}^{\\star})-f^{\\star}(x_{t},a)\\leq1$ , according to Bernstein inequality we have with probability at least $1-\\delta/2$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\displaystyle\\sum_{t\\in\\mathcal{T}_{2}}(f^{\\star}(x_{t},a_{t}^{\\star})-f^{\\star}(x_{t},a_{t}))-\\sum_{t\\in\\mathcal{T}_{2}}\\sum_{a\\in A}p_{t}(a)(f^{\\star}(x_{t},a_{t}^{\\star})-f^{\\star}(x_{t},a))\\right|}\\\\ &{\\quad\\le2\\sqrt{\\displaystyle\\sum_{t\\in\\mathcal{T}_{2}}\\sum_{a\\in A}p_{t}(a)(f^{\\star}(x_{t},a_{t}^{\\star})-f^{\\star}(x_{t},a))^{2}\\log\\left(\\frac{2}{\\delta}\\right)}+2\\log\\left(\\frac{2}{\\delta}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq\\displaystyle\\sum_{t\\in\\mathcal{T}_{2}}\\sum_{a\\in A}p_{t}(a)(f^{\\star}(x_{t},a_{t}^{\\star})-f^{\\star}(x_{t},a))^{2}+3\\log\\left(\\frac{2}{\\delta}\\right)}\\\\ &{\\leq\\displaystyle\\sum_{t\\in\\mathcal{T}_{2}}\\sum_{a\\in A}p_{t}(a)(f^{\\star}(x_{t},a_{t}^{\\star})-f^{\\star}(x_{t},a))+3\\log\\left(\\frac{2}{\\delta}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where in the second inequality we use the AM-GM inequality and in the last inequality we use the fact that $0\\leq f^{\\star}(x_{t},a_{t}^{\\star})\\stackrel{\\scriptscriptstyle{-}}{-}f^{\\star}(x_{t},a)\\leq1$ . Therefore, we obtain that with probability at least $1-\\delta$ , ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{t\\in\\mathcal{T}_{2}}(f^{\\star}(x_{t},a_{t}^{\\star})-f^{\\star}(x_{t},a_{t}))\\le2\\sum_{t\\in\\mathcal{T}_{2}}\\sum_{a\\in\\mathcal{A}}p_{t}(a)(f^{\\star}(x_{t},a_{t}^{\\star})-f^{\\star}(x_{t},a))+3\\log\\left(\\frac{2}{\\delta}\\right)}}\\\\ &{\\le16\\sqrt{(\\sigma^{2}+\\tilde{\\Delta})T\\log\\left(\\frac{4|\\mathcal{F}|}{\\delta}\\right)}+3\\log\\left(\\frac{2}{\\delta}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof of Theorem 4.2. Lemma D.4 and Lemma D.8 gives that with probability at least $1-\\delta/2$ we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\sum_{t\\in\\mathcal{T}_{1}}(\\operatorname*{max}_{a\\in A}f^{\\star}(x_{t},a)-f^{\\star}(x_{t},a_{t}))=\\tilde{O}\\left(\\frac{\\sigma^{2}d_{\\mathrm{elu}}\\sqrt{L}}{\\Delta}+d_{\\mathrm{elu}}L\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Additionally, according to Lemma D.9, we have with probability at least $1-\\delta/2$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\sum_{t\\in\\mathcal{T}_{2}}(\\operatorname*{max}_{a\\in\\mathcal{A}}f^{\\star}(x_{t},a)-f^{\\star}(x_{t},a_{t}))\\leq16\\sqrt{(\\sigma^{2}+\\tilde{\\Delta})T L}+3L.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Hence when $\\Delta=\\sigma^{2}/(11\\sqrt{L})$ (so $\\tilde{\\Delta}=\\sigma^{2}$ ), with probability at least $1-\\delta$ , we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{t\\in[T]}(\\operatorname*{max}_{a\\in A}f^{\\star}(x_{t},a)-f^{\\star}(x_{t},a_{t}))}\\quad}&{}\\\\ &{=\\displaystyle\\sum_{t\\in\\mathcal{T}_{1}}(\\operatorname*{max}_{a\\in A}f^{\\star}(x_{t},a)-f^{\\star}(x_{t},a_{t}))+\\sum_{t\\in\\mathcal{T}_{2}}(\\operatorname*{max}_{a\\in A}f^{\\star}(x_{t},a)-f^{\\star}(x_{t},a_{t}))}\\\\ &{=\\tilde{O}\\left(\\sqrt{\\sigma^{2}A T L}+d_{\\mathrm{elu}}L\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Then noticing that $L=\\log\\frac{|\\mathcal{F}|T^{2}}{\\sigma^{2}\\delta}$ and that $\\textstyle\\sigma\\geq{\\frac{1}{A T}}$ finishes the proof. ", "page_idx": 28}, {"type": "text", "text": "E Omitted Proofs in Section 4.3 ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In this section, we will prove Corollary 4.1. ", "page_idx": 28}, {"type": "text", "text": "Proof of Corollary 4.1. Based on the variance $\\sigma_{t}^{2}$ at round $t$ , Algorithm 2 classify rounds into $\\tau_{0},\\dots,\\tau_{\\log T}$ . We will bound the regret of rounds in $\\mathcal{T}_{i}$ separately. ", "page_idx": 28}, {"type": "text", "text": "According to Algorithm 2, for $t\\in\\mathcal T_{i}$ with $0\\leq i\\leq\\log(A T)$ , we always have $1/A T\\leq\\sigma_{t}\\leq{2^{i}}/{A T}$ . Hence, according to Theorem 4.2, with probability at least $1-\\delta/\\log(A T)$ , we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\sum_{t\\in[T_{i}]}\\left(\\operatorname*{max}_{a\\in A}f^{\\star}(x_{t},a)-f^{\\star}(x_{t},a_{t})\\right)=\\tilde{\\mathcal{O}}\\left(\\sqrt{A|T_{i}|\\cdot(2^{i}/(A T))^{2}\\log\\left(\\frac{|\\mathcal{F}|}{\\delta}\\right)}+d_{\\mathrm{elu}}\\log\\left(\\frac{|\\mathcal{F}|}{\\delta}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We further observe that for $1\\leq i\\leq\\log(A T),\\sigma_{t}\\geq{2^{i-1}}/{A T}$ , which implies that ", "page_idx": 28}, {"type": "equation", "text": "$$\n|{\\mathcal{T}}_{i}|\\cdot(2^{i}/(A T))^{2}\\leq4\\sum_{t\\in{\\mathcal{T}}_{i}}\\sigma_{t}^{2}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "And for $i=0$ , we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n|{\\mathcal{T}}_{i}|\\cdot(2^{i}/(A T))^{2}\\leq T\\cdot{\\frac{1}{(A T)^{2}}}\\leq{\\frac{1}{A T}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Therefore, with probability at least $1-\\delta$ , we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{T}{\\sum_{t=1}^{T}\\alpha_{t,\\delta}^{2}}f^{*}(x_{t},a_{t})-f^{*}(x_{t},a_{t}))}\\\\ &{\\quad=\\bar{\\mathcal{O}}\\left(\\displaystyle\\sum_{i=1}^{\\infty}\\varsigma_{i}^{\\top}\\overline{{\\mathcal{A}(\\boldsymbol{\\hat{\\pi}}_{i}|\\cdot(2f^{*}(A\\boldsymbol{T}))^{2}\\log\\left(\\frac{\\left|\\boldsymbol{\\hat{F}}\\right|}{\\delta}\\right)}}+d_{\\mathrm{oh}}\\log\\left(\\frac{\\left|\\boldsymbol{\\hat{F}}\\right|}{\\delta}\\right)\\right)}\\\\ &{\\quad=\\bar{\\mathcal{O}}\\left(\\displaystyle\\sum_{i=1}^{\\infty}\\sqrt{\\alpha_{t}^{4}}\\sqrt{\\lambda_{t}\\sum_{\\ell=1}^{Q}\\varrho_{\\ell}^{2}\\log\\left(\\frac{\\left|\\boldsymbol{\\hat{F}}\\right|}{\\delta}\\right)}+d_{\\mathrm{oh}}\\log\\left(\\frac{\\left|\\boldsymbol{\\hat{F}}\\right|}{\\delta}\\right)\\right)}\\\\ &{\\quad\\overset{(i)}{=}\\bar{\\mathcal{O}}\\left(\\sqrt{\\log({A T})}\\cdot\\displaystyle\\sum_{i=1}^{\\infty}\\lambda_{t}\\sum_{\\ell=1}^{Q}\\sigma_{i}^{2}\\log\\left(\\frac{\\left|\\boldsymbol{\\hat{F}}\\right|}{\\delta}\\right)+d_{\\mathrm{oh}}\\log\\left(\\frac{\\left|\\boldsymbol{\\hat{F}}\\right|}{\\delta}\\right)\\right)}\\\\ &{\\quad=\\bar{\\mathcal{O}}\\left(\\sqrt{\\lambda_{\\mathrm{\\t}}\\sum_{i=1}^{T}\\sigma_{i}^{2}\\log\\left(\\frac{\\left|\\boldsymbol{\\hat{F}}\\right|}{\\delta}\\right)}+d_{\\mathrm{oh}}\\log\\left(\\frac{\\left|\\boldsymbol{\\hat{F}}\\right|}{\\delta}\\right)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where in $(i)$ we use Cauchy-Schwarz inequality. ", "page_idx": 29}, {"type": "text", "text": "1: Define: $L=\\Theta(\\log(|\\mathcal{F}|T/\\delta))$ , $\\mathcal{F}_{0}=\\mathcal{F}$ .   \n2: Define: $K\\triangleq\\lceil\\log T\\rceil$ . Let $\\Psi_{1,k}=\\emptyset$ for $k=1,2,\\ldots,K,K+1$ .   \n3: for $t=1,2,\\dots\\mathbf{do}$   \n4: Define confidence set: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathcal{F}_{t}=\\left\\{f\\in\\mathcal{F}_{t-1}:\\;\\forall k\\in[K],\\;\\sum_{\\tau\\in\\Psi_{t,k}}w_{\\tau}^{2}(f(x_{\\tau},a_{\\tau})-\\hat{f}_{t,k}(x_{\\tau},a_{\\tau}))^{2}\\leq\\beta_{t,k}^{2}\\right\\},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\hat{f}_{t,k}=\\arg\\operatorname*{min}\\displaystyle\\sum_{\\substack{f\\in\\mathcal{F}_{t},\\quad\\tau\\in\\Psi_{t,k}}}w_{\\tau}^{2}(f(x_{\\tau},a_{\\tau})-r_{\\tau})^{2},}&{(16)}\\\\ &{\\beta_{t,k}=\\left\\{\\!\\!\\begin{array}{l l}{10\\cdot2^{-k}\\!\\sqrt{\\!\\sum_{\\tau\\in\\Psi_{t,k}}{w_{\\tau}^{2}(r_{\\tau}-\\hat{f}_{t,k}(x_{\\tau},a_{\\tau}))^{2}L+L^{2}}}}&{\\!\\mathrm{if}\\;k\\leq K\\,\\mathrm{and\\,}2^{2k}\\geq80L}\\\\ {\\sqrt{\\lvert\\Psi_{t,k}\\rvert}}&{\\!\\mathrm{if}\\;k\\leq K\\,\\mathrm{and\\,}2^{2k}<80L}\\end{array}\\!\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "5: Receive $x_{t}$ , and define ${\\mathcal{A}}_{t}=\\{a\\in{\\mathcal{A}}:\\ \\exists f\\in{\\mathcal{F}}_{t}:a\\in\\arg\\operatorname*{max}_{a^{\\prime}\\in{\\mathcal{A}}}f(x_{t},a^{\\prime})\\}.$   \n6: Define ", "page_idx": 30}, {"type": "equation", "text": "$$\ng_{t,k}(a)=\\operatorname*{max}_{f,f^{\\prime}\\in\\mathcal{F}_{t}}\\frac{|f(x_{t},a)-f^{\\prime}(x_{t},a)|}{\\sqrt{2^{-2k}L^{2}+\\sum_{\\tau\\in\\Psi_{t,k}}w_{\\tau}^{2}(f(x_{\\tau},a_{\\tau})-f^{\\prime}(x_{\\tau},a_{\\tau}))^{2}}}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "8: ", "page_idx": 30}, {"type": "text", "text": "F Algorithm for the Strong Adversary Case and Omitted Proofs in Section 5 F.1 Upper Bound ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "In this section, we introduce and analyze Algorithm 5, an algorithm that achieves $\\tilde{\\mathcal{O}}(d_{\\mathrm{elu}}\\sqrt{\\Lambda\\log{|\\mathcal{F}|}}+$ $d_{\\mathrm{elu}}\\log|\\mathcal{F}|)$ regret bound against strong adversary. ", "page_idx": 30}, {"type": "text", "text": "Algorithm 5 is an extension of the SAVE algorithm (Zhao et al., 2023) from linear function approximation to general function approximation. The algorithm maintains $K+1=\\Theta(\\log T)$ bins denoted as $\\left\\{\\Psi_{t,k}\\right\\}$ , $k=1,2,\\ldots,K+1$ , which forms a partition of $[t-1]$ (i.e., every time index $\\tau<t$ falls into exactly one of these bins). Each bin $k$ can form a confidence function set like in standard LinUCB using samples in $\\Psi_{t,k}$ . The overall confidence set $\\mathcal{F}_{t}$ is the intersection of the confidence sets of individual bins (Line 4). ", "page_idx": 30}, {"type": "text", "text": "Upon receiving the context $x_{t}$ , the learner form an active action set that contains plausible actions (Line 5). The next step is to decide which bin $t$ should go to. This is done by leveraging the uncertainty measure $g_{t,k}(a)$ for bin $k$ and action $a$ (Line 6), which measures how uncertain the reward of action $a$ is, given prior samples in bin $k$ . The measure $g_{t,k}(a)$ corresponds to the quantity $\\|a\\|_{\\Sigma_{t,k}^{-1}}$ usually seen in linear contextual bandits, where $\\Sigma_{t,k}$ is the covariance matrix formed by the samples in $\\Psi_{t,k}$ . The algorithm finds the smallest $k$ such that there exists an action with relative large uncertainty $g_{t,k}(a)\\overset{\\bullet}{\\geq}2^{-k}$ (Line 7). The learner would then choose this action in order to gain relatively large shrinking in bin $k$ \u2019s confidence set (Line 8), and put time $t$ in bin $k$ . The sample at time $t$ is assigned a weight $w_{t}$ that is inversely proportional to the uncertainty measure (Line 9). This ensures that the importance of the samples within each bin is more balanced. ", "page_idx": 30}, {"type": "text", "text": "Before proving the main theorem Theorem 5.2, we first establish lemmas Lemma F.1\u2013Lemma F.5. ", "page_idx": 30}, {"type": "text", "text": "Lemma F.1. Suppose that $f^{\\star}\\in\\mathcal{F}_{t-1}$ . Then with probability at least $1-\\delta/T$ , for all $k\\in[K]$ we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\sum_{\\tau\\in\\Psi_{t,k}}w_{\\tau}^{2}(\\hat{f}_{t,k}(x_{\\tau},a_{\\tau})-f^{\\star}(x_{\\tau},a_{\\tau}))^{2}\\leq10\\cdot2^{-2k}\\left(\\sum_{\\tau\\in\\Psi_{t,k}}w_{\\tau}^{2}\\sigma_{\\tau}^{2}L+L^{2}\\right).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Proof. Since $r_{\\tau}=f^{*}(x_{\\tau},a_{\\tau})+\\epsilon_{t}$ , we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ \\ \\ \\underset{\\tau\\in\\Psi_{t,k}}{\\sum}w_{\\tau}^{2}(\\hat{f}_{t,k}(x_{\\tau},a_{\\tau})-f^{\\star}(x_{\\tau},a_{\\tau}))^{2}}\\\\ &{=\\displaystyle\\sum_{\\tau\\in\\Psi_{t,k}}w_{\\tau}^{2}\\left(\\hat{f}_{t,k}(x_{\\tau},a_{\\tau})-r_{\\tau}\\right)^{2}-\\sum_{\\tau\\in\\Psi_{t,k}}w_{\\tau}^{2}\\left(f^{\\star}(x_{\\tau},a_{\\tau})-r_{\\tau}\\right)^{2}}\\\\ &{\\qquad+2\\displaystyle\\sum_{\\tau\\in\\Psi_{t,k}}w_{\\tau}^{2}(\\hat{f}_{t,k}(x_{\\tau},a_{\\tau})-f^{\\star}(x_{\\tau},a_{\\tau}))(r_{\\tau}-f^{\\star}(x_{\\tau},a_{\\tau}))}\\\\ &{\\le2\\displaystyle\\sum_{\\tau\\in\\Psi_{t,k}}w_{\\tau}^{2}\\epsilon_{\\tau}(\\hat{f}_{t,k}(x_{\\tau},a_{\\tau})-f^{\\star}(x_{\\tau},a_{\\tau})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where the last inequality is by the optimality of $\\hat{f}_{t,k}$ given in Eq. (16). According to the strengthened Freeman\u2019s Inequality (Lemma B.5), with $L\\,=\\,{\\mathcal{C}}\\log(|{\\mathcal{F}}|T/\\delta)$ for some large enough universal constant $C$ (specified in Line 1 of Algorithm 5), the last expression in Eq. (17) can be further bounded by ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\sqrt{\\displaystyle\\sum_{\\tau\\in\\Psi_{t,k}}w_{\\tau}^{4}\\sigma_{\\tau}^{2}(\\hat{f}_{t,k}(x_{\\tau},a_{\\tau})-f^{\\star}(x_{\\tau},a_{\\tau}))^{2}L}+\\displaystyle\\operatorname*{max}_{\\tau\\in\\Psi_{t,k}}\\left|w_{\\tau}^{2}\\epsilon_{\\tau}(\\hat{f}_{t,k}(x_{\\tau},a_{\\tau})-f^{\\star}(x_{\\tau},a_{\\tau}))\\right|L}\\\\ &{}&{\\le\\sqrt{\\displaystyle\\sum_{\\tau\\in\\Psi_{t,k}}w_{\\tau}^{4}\\sigma_{\\tau}^{2}(\\hat{f}_{t,k}(x_{\\tau},a_{\\tau})-f^{\\star}(x_{\\tau},a_{\\tau})^{2}L+\\displaystyle\\operatorname*{max}_{\\tau\\in\\Psi_{t,k}}w_{\\tau}|\\hat{f}_{t,k}(x_{\\tau},a_{\\tau})-f^{\\star}(x_{\\tau},a_{\\tau})|L,}\\\\ &{}&{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad(w_{t}\\le1\\mathrm{~because~}g_{t}=g_{t,k_{t}}(a_{t})\\ge2^{-k_{t}}\\mathrm{~for~}k\\in[K],\\mathrm{and~}|\\epsilon_{t}|\\le\\displaystyle\\sum_{\\tau\\in\\Psi_{t,k}}w_{\\tau}^{2}\\sigma_{\\tau}^{2}L+L\\right)}\\\\ &{}&{\\le\\left(\\displaystyle\\operatorname*{max}_{\\tau\\in\\Psi_{t,k}}w_{\\tau}|\\hat{f}_{t,k}(x_{\\tau},a_{\\tau})-f^{\\star}(x_{\\tau},a_{\\tau})|\\right)\\left(\\sqrt{\\displaystyle\\sum_{\\tau\\in\\Psi_{t,k}}w_{\\tau}^{2}\\sigma_{\\tau}^{2}L}+L\\right)}\\end{array}\n$$$|\\epsilon_{t}|\\le1\\$ ", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "for all $k$ with probability at least $1-\\delta/T$ by a union bound over $k$ and $\\hat{f}_{t,k}$ . ", "page_idx": 31}, {"type": "text", "text": "For any $\\tau\\in\\Psi_{t,k}$ , by the definition of $w_{\\tau}$ (Line 9 of Algorithm 5), we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{w_{\\tau}|\\hat{f}_{t,k}(x_{\\tau},a_{\\tau})-f^{\\star}(x_{\\tau},a_{\\tau})|}\\\\ &{\\leq2^{-k}/g_{\\tau}\\cdot|\\hat{f}_{t,k}(x_{\\tau},a_{\\tau})-f^{\\star}(x_{\\tau},a_{\\tau})|}\\\\ &{=2^{-k}\\underset{f,f^{\\prime}\\in\\mathcal{F}_{\\tau}}{\\operatorname*{min}}\\frac{\\sqrt{2^{-2k}L^{2}+\\sum_{s\\in\\Psi_{\\tau,k}}w_{s}^{2}\\left(f(x_{s},a_{s})-f^{\\prime}(x_{s},a_{s})\\right)^{2}}}{|f(x_{\\tau},a_{\\tau})-f^{\\prime}(x_{\\tau},a_{\\tau})|}\\cdot|\\hat{f}_{t,k}(x_{\\tau},a_{\\tau})-f^{\\star}(x_{\\tau},a_{\\tau})|}\\\\ &{\\leq2^{-k}\\sqrt{2^{-2k}L^{2}+\\underset{s\\in\\Psi_{\\tau,k}}{\\sum}w_{s}^{2}(\\hat{f}_{t,k}(x_{s},a_{s})-f^{\\star}(x_{s},a_{s}))^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "$(f^{\\star},\\hat{f}_{t,k}\\in\\mathcal{F}_{t-1}\\subset\\mathcal{F}_{\\tau}$ by assumption) ", "page_idx": 31}, {"type": "text", "text": "Combining this and Eq. (18), we get ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{\\tau\\in\\Psi_{t,k}}w_{\\tau}^{2}(\\hat{f}_{t,k}(x_{\\tau},a_{\\tau})-f^{\\star}(x_{\\tau},a_{\\tau}))^{2}}\\\\ &{\\le2^{-k}\\left(\\operatorname*{max}_{\\tau\\in\\Psi_{t,k}}\\sqrt{2^{-2k}L^{2}+\\displaystyle\\sum_{s\\in\\Psi_{\\tau,k}}w_{s}^{2}(\\hat{f}_{t,k}(x_{s},a_{s})-f^{\\star}(x_{s},a_{s}))^{2}}\\right)\\left(\\sqrt{\\displaystyle\\sum_{\\tau\\in\\Psi_{t,k}}w_{\\tau}^{2}\\sigma_{\\tau}^{2}L}+L\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "equation", "text": "$$\n\\leq2^{-k}\\left(\\sqrt{2^{-2k}L^{2}+\\sum_{\\tau\\in\\Psi_{t,k}}w_{\\tau}^{2}(\\hat{f}_{t,k}(x_{\\tau},a_{\\tau})-f^{\\star}(x_{\\tau},a_{\\tau}))^{2}}\\right)\\left(\\sqrt{\\sum_{\\tau\\in\\Psi_{t,k}}w_{\\tau}^{2}\\sigma_{\\tau}^{2}L}+L\\right).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Solving the inequality yields ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\sum_{\\tau\\in\\Psi_{t,k}}w_{\\tau}^{2}(\\hat{f}_{t,k}(x_{\\tau},a_{\\tau})-f^{\\star}(x_{\\tau},a_{\\tau}))^{2}\\leq10\\cdot2^{-2k}\\left(\\sum_{\\tau\\in\\Psi_{t,k}}w_{\\tau}^{2}\\sigma_{\\tau}^{2}L+L^{2}\\right).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Lemma F.2. Suppose that $f^{\\star}\\in\\mathcal{F}_{t-1}$ . Then for all $k\\in[K]$ satisfying $2^{2k}\\,\\geq80L$ , we have with probability at least $1-\\delta/T$ , ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\displaystyle\\sum_{\\tau\\in\\Psi_{t,k}}w_{\\tau}^{2}\\sigma_{\\tau}^{2}\\leq8\\sum_{\\tau\\in\\Psi_{t,k}}w_{\\tau}^{2}\\left(r_{\\tau}-\\hat{f}_{t,k}(x_{\\tau},a_{\\tau})\\right)^{2}+4L,}\\\\ &{}&{\\displaystyle\\sum_{\\tau\\in\\Psi_{t,k}}w_{\\tau}^{2}\\left(r_{\\tau}-\\hat{f}_{t,k}(x_{\\tau},a_{\\tau})\\right)^{2}\\leq2\\displaystyle\\sum_{\\tau\\in\\Psi_{t,k}}w_{\\tau}^{2}\\sigma_{\\tau}^{2}+L.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Proof. With $L=C\\log(|\\mathcal{F}|T/\\delta)$ for some large enough universal constant $C$ , we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{\\in\\mathfrak{S}_{t,k}}w_{\\tau}^{2}\\sigma_{\\tau}^{2}\\leq2\\sum_{\\tau\\in\\mathfrak{S}_{t,k}}w_{\\tau}^{2}e_{\\tau}^{2}+L}}&{(\\mathrm{Freedman})\\sin\\mathrm{inequality})}\\\\ &{}&{\\leq4\\sum_{\\tau\\in\\mathfrak{S}_{t,k}}w_{\\tau}^{2}\\left(r_{\\tau}-\\hat{f}_{t,k}(x_{\\tau},a_{\\tau})\\right)^{2}+4\\displaystyle\\sum_{\\tau\\in\\mathfrak{S}_{t,k}}w_{\\tau}^{2}\\left(\\hat{f}_{t,k}(x_{\\tau},a_{\\tau})-f^{\\star}(x_{\\tau},a_{\\tau})\\right)^{2}+L}\\\\ &{}&{\\leq4\\displaystyle\\sum_{\\tau\\in\\mathfrak{S}_{t,k}}w_{\\tau}^{2}\\left(r_{\\tau}-\\hat{f}_{t,k}(x_{\\tau},a_{\\tau})\\right)^{2}+40\\cdot2^{-2k}\\left(\\displaystyle\\sum_{\\tau\\in\\mathfrak{S}_{t,k}}w_{\\tau}^{2}\\sigma_{\\tau}^{2}L+L^{2}\\right)+L}\\\\ &{}&{\\leq4\\displaystyle\\sum_{\\tau\\in\\mathfrak{S}_{t,k}}w_{\\tau}^{2}\\left(r_{\\tau}-\\hat{f}_{t,k}(x_{\\tau},a_{\\tau})\\right)^{2}+2L+\\displaystyle\\frac{1}{2}\\displaystyle\\sum_{\\tau\\in\\mathfrak{S}_{t,k}}w_{\\tau}^{2}\\sigma_{\\tau}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Rearranging gives the first inequality. For the second inequality, note that we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{\\displaystyle\\sum_{\\tau\\in\\Psi_{t,k}}w_{\\tau}^{2}\\left(r_{\\tau}-\\hat{f}_{t,k}(x_{\\tau},a_{\\tau})\\right)^{2}\\leq\\displaystyle\\sum_{\\tau\\in\\Psi_{t,k}}w_{\\tau}^{2}\\left(r_{\\tau}-f^{\\star}(x_{\\tau},a_{\\tau})\\right)^{2}}&&{(\\mathrm{by~the~optimality~of~}\\hat{f}_{t,k})}\\\\ &{\\quad\\hfill}\\\\ &{\\leq2\\displaystyle\\sum_{\\tau\\in\\Psi_{t,k}}w_{\\tau}^{2}\\sigma_{\\tau}^{2}+L.}&&{(\\mathrm{Frecdman`s~inequality})}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Lemma F.3. With probability at least $1-\\delta$ , $f^{\\star}\\in{\\mathcal{F}}_{t}$ for all $t$ . ", "page_idx": 32}, {"type": "text", "text": "Proof. We prove by induction. Assume that $f^{\\star}\\in\\mathcal{F}_{t-1}$ . Then for all $k\\in[K]$ such that $2^{2k}\\geq80L$ , by Lemma F.1 and Lemma F.2, with probability at least $1-\\delta/T$ , ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{\\tau\\in\\Psi_{t,k}}w_{\\tau}^{2}(\\hat{f}_{t,k}(x_{\\tau},a_{\\tau})-f^{\\star}(x_{\\tau},a_{\\tau}))^{2}\\leq10\\cdot2^{-2k}\\left(\\sum_{\\tau\\in\\Psi_{t,k}}w_{\\tau}^{2}\\sigma_{\\tau}^{2}L+L^{2}\\right)}}\\\\ &{}&\\\\ &{\\leq10\\cdot2^{-2k}\\left(8\\sum_{\\tau\\in\\Psi_{t,k}}w_{\\tau}^{2}\\left(r_{\\tau}-\\hat{f}_{t,k}(x_{\\tau},a_{\\tau})\\right)^{2}L+4L^{2}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "equation", "text": "$$\n\\le\\beta_{t,k}^{2}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "For $k\\in[K]$ such that $2^{2k}<80L$ , we bound trivially ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\sum_{\\tau\\in\\Psi_{t,k}}w_{\\tau}^{2}(\\hat{f}_{t,k}(x_{\\tau},a_{\\tau})-f^{\\star}(x_{\\tau},a_{\\tau}))^{2}\\leq|\\Psi_{t,k}|=\\beta_{t,k}^{2}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "equation", "text": "$$\n(w_{\\tau}\\leq1)\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "In both cases, we have $f^{\\star}\\,\\in\\,{\\mathcal{F}}_{t}$ by the definition of $\\mathcal{F}_{t}$ . By induction, we conclude that with probability at least $1-\\delta$ , $f^{\\star}\\in{\\mathcal{F}}_{t}$ for all $t$ . \u53e3 ", "page_idx": 33}, {"type": "text", "text": "Lemma F.4. For $k\\in[K]$ , we have $|\\Psi_{T+1,k}|\\leq\\tilde{\\mathcal{O}}(2^{2k}d_{e l u}).$ . ", "page_idx": 33}, {"type": "text", "text": "Proof. For $t$ such that $k_{t}\\in[K]$ , by the definition of $w_{t}$ we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n1=2^{k_{t}}w_{t}g_{t}=2^{k_{t}}\\operatorname*{max}_{f,f^{\\prime}\\in\\mathcal{F}_{t}}\\frac{w_{t}|f(x_{t},a_{t})-f^{\\prime}(x_{t},a_{t})|}{\\sqrt{2^{-2k_{t}}L^{2}+\\sum_{\\tau\\in\\Psi_{t,k_{t}}}w_{\\tau}^{2}(f(x_{\\tau},a_{\\tau})-f^{\\prime}(x_{\\tau},a_{\\tau}))^{2}}}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Thus, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\Psi_{T+1,k}|=\\displaystyle\\sum_{t\\in\\Psi_{T+1,k}}\\operatorname*{min}\\left\\{1,\\ \\ 2^{k}\\operatorname*{max}_{f,f^{\\prime}\\in\\mathcal{F}_{t}}\\frac{w_{t}|f(x_{t},a_{t})-f^{\\prime}(x_{t},a_{t})|}{\\sqrt{2^{-2k}L^{2}+\\sum_{\\tau\\in\\Psi_{t,k}}w_{\\tau}^{2}\\left(f(x_{\\tau},a_{\\tau})-f^{\\prime}(x_{\\tau},a_{\\tau})\\right)^{2}}}\\right\\}^{2}}\\\\ &{}&{=2^{2k}\\displaystyle\\sum_{t\\in\\Psi_{T+1,k}}\\operatorname*{min}\\left\\{2^{-2k},\\ \\ \\operatorname*{max}_{f,f^{\\prime}\\in\\mathcal{F}_{t}}\\frac{w_{t}^{2}\\left(f(x_{t},a_{t})-f^{\\prime}(x_{t},a_{t})\\right)^{2}}{2^{-2k}L^{2}+\\sum_{\\tau\\in\\Psi_{t,k}}w_{\\tau}^{2}\\left(f(x_{\\tau},a_{\\tau})-f^{\\prime}(x_{\\tau},a_{\\tau})\\right)^{2}}\\right\\}}\\\\ &{}&{\\leq\\tilde{\\mathcal{O}}(2^{2k}d_{\\mathrm{elu}}).\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\mathrm{(by\\Lemma\\D.S)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Lemma F.5. For t such that $2^{2(k_{t}-1)}\\geq80L,$ , we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{a\\in{\\cal A}_{t}}f^{\\star}(x_{t},a)-f^{\\star}(x_{t},a_{t})\\le200\\cdot2^{-2k_{t}}\\left(\\sqrt{\\Lambda L}+L\\right).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Proof. ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{imsu}_{{\\mathcal{G}}}\\int^{\\infty}(x_{t},a)-f^{*}(x_{t},a)}\\\\ &{\\leq\\operatorname*{max}_{\\mathcal{G}}f(x_{t},a)\\psi-f^{*}(x_{t},a)}\\\\ &{=\\operatorname*{max}_{\\mathcal{G}}f(x_{t},a)\\psi-\\operatorname*{min}_{\\mathcal{G}}\\left(x_{t},a)\\psi\\right)+\\operatorname*{min}_{\\mathcal{G}}\\left(\\operatorname*{defm}_{{\\mathcal{G}}}a_{t}^{\\mathrm{ab}}-\\operatorname*{max}_{\\mathcal{G}}f(x_{t},a)+\\operatorname*{max}_{\\mathcal{G}\\in\\mathcal{F}_{r}}f(x_{t},a)\\right)}\\\\ &{=\\operatorname*{max}_{\\mathcal{G}\\in\\mathcal{F}_{r}}f(x_{t},a)\\psi-\\operatorname*{min}_{\\mathcal{G}}\\left(f(x_{t},a)\\psi\\right)+\\operatorname*{min}_{\\mathcal{G}\\in\\mathcal{F}_{r}}f(x_{t},a)\\psi-\\operatorname*{max}_{\\mathcal{G}\\in\\mathcal{F}_{r}}f(x_{t},a)+\\operatorname*{max}_{\\mathcal{G}\\in\\mathcal{F}_{r}}f(x_{t},a_{t})-f^{*}}\\\\ &{\\leq2\\operatorname*{max}_{\\mathcal{G}\\in\\mathcal{F}_{r}}\\operatorname*{max}_{\\mathcal{G}\\in\\mathcal{F}_{r}}\\left(f(x_{t},a)-f^{*}(x_{t},a)\\right)+\\operatorname*{min}_{\\mathcal{G}\\in\\mathcal{F}_{r}}f(x_{t},a_{t})\\psi-\\operatorname*{max}_{\\mathcal{G}\\in\\mathcal{F}_{r}}f(x_{t},a_{t})}\\\\ &{\\overset{(a)}{\\leq}2\\operatorname*{max}_{\\mathcal{G}\\in\\mathcal{F}_{r}}\\operatorname*{max}_{\\mathcal{G}\\in\\mathcal{F}_{r}}\\left\\{f(x_{t},a)-f^{*}(x_{t},a)\\right\\}}\\\\ &{\\overset{(c)}{\\leq}2\\operatorname*{max}_{\\mathcal{G}\\in\\mathcal{F}_{r}}\\operatorname*{max}_{\\mathcal{G}\\in\\mathcal{F}_{r}}\\left\\{f(x_{t},a)-f^{*}(x_{t},a)\\right\\}}\\\\ &{\\overset{(d)}{\\leq}2\\operatorname*{max}_{\\mathcal{G}\\in\\mathcal{F}_{r}}\\operatorname*{max}_{\\mathcal{G}\\in\\mathcal{F}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "(by the definition of $\\beta_{t,k}$ ) ", "page_idx": 33}, {"type": "text", "text": "\u2264200 \u00b7 2\u22122kt w\u03c42\u03c3\u03c42L + L2 \u03c4\u2208\u03a8t,kt\u22121 ", "page_idx": 33}, {"type": "text", "text": "(by Lemma F.2) ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\leq200\\cdot\\left(2^{-2k_{t}}\\sqrt{\\Lambda L}+2^{-2k_{t}}L\\right).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(w_{\\tau}\\leq1)}\\\\ {(19)}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Here, in $(i)$ we use the definition of $\\boldsymbol{A}_{t}$ that there exists $f^{\\prime}\\in\\mathcal{F}_{t}$ such that $a_{t}=\\operatorname*{max}_{a\\in\\mathcal{A}}f^{\\prime}(x_{t},a)$ , which implies ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{f\\in{\\mathcal F}_{t}}f(x_{t},a_{t}^{\\mathrm{ucb}})\\leq f^{\\prime}(x_{t},a_{t}^{\\mathrm{ucb}})\\leq f^{\\prime}(x_{t},a_{t})\\leq\\operatorname*{max}_{f\\in{\\mathcal F}_{t}}f(x_{t},a_{t}).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "In $(i i)$ we use the fact that for $f,f^{\\prime}\\in\\mathcal{F}_{t}$ , $k\\leq K$ and $2^{2k}\\geq80L$ , we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2^{-2k}L^{2}+\\displaystyle\\sum_{\\tau\\in\\Psi_{t,k}}w_{\\tau}^{2}(f(x_{\\tau},a_{\\tau})-f^{\\prime}(x_{\\tau},a_{\\tau}))^{2}}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ }\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "(by the definition of $\\beta_{t,k}$ ) ", "page_idx": 34}, {"type": "text", "text": "$\\le5\\beta_{t,k}^{2}$ ", "page_idx": 34}, {"type": "text", "text": "Proof of Theorem 5.2. Without loss of generality, assume $2^{2K}\\,\\geq\\,80L$ , which is equivalent to $T^{2}\\geq\\Theta(\\log(|\\mathcal{F}|T/\\delta))$ . Notice that $\\begin{array}{r}{[T]=\\bar{\\bigcup}_{k=1}^{K+1}\\,\\bar{\\Psi_{T+1,k}}}\\end{array}$ . ", "page_idx": 34}, {"type": "text", "text": "Bound the regret in $\\Psi_{T+1,K+1}$ . Notice that by assumption, for $t~\\in~\\Psi_{T+1,K+1}$ , we have $2^{2(k_{t}-1)}=2^{2K}\\geq80L$ . Thus, by Lemma F.5, ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{t\\in\\Psi_{T+1,K+1}}\\left(\\operatorname*{max}_{a\\in\\mathcal{A}_{t}}f^{\\star}(x_{t},a)-f^{\\star}(x_{t},a_{t})\\right)\\le200|\\Psi_{T+1,K+1}|\\cdot2^{-2(K+1)}\\left(\\sqrt{\\Lambda L}+L\\right)}}\\\\ &{}&\\\\ &{}&{\\le\\mathcal{O}\\left(T\\times\\frac{1}{T^{2}}\\times(\\Lambda L+L)\\right)=\\mathcal{O}(1).~~~~~}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Bound the regret in $\\Psi_{T+1,k}$ with $2^{2k}\\geq80L$ . By Lemma F.5, ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{t\\in\\Psi_{T+1,k}}\\left(\\operatorname*{max}_{a\\in\\mathcal{A}_{t}}f^{\\star}(x_{t},a)-f^{\\star}(x_{t},a_{t})\\right)\\le200|\\Psi_{T+1,k}|\\cdot2^{-2k}\\left(\\sqrt{\\Lambda L}+L\\right)}}\\\\ &{}&\\\\ &{}&{\\le\\tilde{\\mathcal{O}}\\left(d_{\\mathrm{elu}}\\left(\\sqrt{\\Lambda L}+L\\right)\\right)\\qquad\\quad\\mathrm{(by~Lemma~F.4)}}\\\\ &{}&{=\\tilde{\\mathcal{O}}\\left(d_{\\mathrm{elu}}\\sqrt{\\Lambda\\log{|\\mathcal{F}|}}+d_{\\mathrm{elu}}\\log{|\\mathcal{F}|}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Bound the regret in $\\Psi_{T+1,k}$ with $2^{2k}<80L$ . ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{t\\in\\Psi_{T+1,k}}\\left(\\operatorname*{max}_{a\\in\\mathcal{A}_{t}}f^{\\star}(x_{t},a)-f^{\\star}(x_{t},a_{t})\\right)\\leq|\\Psi_{T+1,k}|}&{}\\\\ {\\leq\\tilde{O}\\left(2^{2k}d_{\\mathrm{elu}}\\right)}&{}\\\\ {\\leq\\tilde{O}(d_{\\mathrm{elu}}L)}&{}\\\\ {=\\tilde{O}\\left(d_{\\mathrm{elu}}\\log|\\mathcal{F}|\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Combining all parts proves the desired bound. ", "page_idx": 34}, {"type": "text", "text": "F.2 Lower Bound ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Lemma F.6. For any integer $A,T\\geq2$ , $N\\leq c{\\sqrt{T/A}}$ and positive number $\\Lambda>A N/c$ with some $c\\leq1$ , there exists a context space $\\mathcal{X}$ , a contextual bandit problem $\\mathcal{F}\\subset(\\mathcal{X}\\times\\mathcal{A}\\rightarrow\\mathbb{R})$ with eluder dimension $d_{e l u}(\\mathcal{F},0)=N(A-1)$ and action set $A=[A]$ , and adversarially assigned variances $\\sigma_{1}^{2},\\dots,\\sigma_{T}^{2}$ that $\\textstyle\\sum_{t=1}^{T}\\sigma_{t}^{2}\\leq\\Lambda$ such that any algorithm will suffer at least $\\Omega(\\operatorname*{min}\\{\\sqrt{N A\\Lambda},\\sqrt{A T}\\})$ . ", "page_idx": 35}, {"type": "text", "text": "Proof of Lemma F.6. Let $\\varepsilon^{2}\\in[2N/T,1]$ be a parameter to be decided later. Consider the function class $\\mathcal{F}=\\{f^{(0)}\\}\\cup\\{f^{(i,j)}\\}_{i\\in[N],j\\in[A-1]}$ with the space of contexts $\\mathcal{X}=\\{x^{(1)},\\ldots,x^{(N)}\\}$ and the set of actions ${\\mathcal{A}}=[A]$ . For any $i\\in[N],j\\in[A-1]$ , the function $f^{(i,j)}$ is defined as the following: For $i\\in[N]$ and $j\\in[A-1]$ , ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{f^{(i,j)}(x^{(i)},j)=\\displaystyle\\frac{1}{2}+\\varepsilon,}}\\\\ {{f^{(i,j)}(x,k)=\\displaystyle\\frac{1}{2}-\\varepsilon,\\quad\\forall x\\neq x^{(i)}\\;\\mathrm{or}\\,\\forall k\\in[A-1]\\setminus\\{j\\}.}}\\\\ {{f^{(i,j)}(x,A)=\\displaystyle\\frac{1}{2},\\quad\\forall x.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Meanwhile ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle{f^{(0)}(x,j)=\\frac{1}{2}-\\varepsilon,\\quad\\forall x\\;\\mathrm{and}\\;\\forall j\\in[A-1]}}\\\\ {\\displaystyle{f^{(0)}(x,A)=\\frac{1}{2},\\quad\\forall x.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "The eluder dimension of this function class is $N(A-1)$ since $f^{(i,j)}$ is uniquely identified by its value on $(x^{(i)},j)$ . ", "page_idx": 35}, {"type": "text", "text": "We assume that $x_{t}$ is uniformly randomly chosen from $\\mathcal{X}$ , and $r_{t}\\ =\\ f_{\\star}(x_{t},a_{t})\\,+\\,\\epsilon\\sigma_{t}$ , where $\\mathbf{\\boldsymbol{\\epsilon}}\\sim\\mathcal{N}(\\mathbf{\\boldsymbol{0}},\\mathbf{\\boldsymbol{1}})$ and $\\sigma_{t}$ is defined in the following way. Fix the algorithm. Let $N_{t}(i,j)=\\sum_{s=1}^{t}\\mathbb{1}(x_{s}=$ $x^{(i)},a_{s}=j)$ for any $x\\in\\mathscr{X}$ and the action $b$ . The adversary assigns ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\sigma_{t}=\\mathbb{1}(a_{t}=j,N_{t}(x_{t},j)\\leq1/\\varepsilon^{2}),\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "that is, it assigns variance 1 when the algorithm chooses action $b$ and the context-action pair $x_{t},b$ are not played for more than $1/\\varepsilon^{2}$ times. ", "page_idx": 35}, {"type": "text", "text": "Fix any algorithm. Denote by $\\mathbb{P}_{0}$ the probability distribution when $f^{\\star}=f^{(0)}$ and $\\mathbb{E}_{0}$ the expectation under $\\mathbb{P}_{0}$ . For any $i\\in[N],j\\in[A-1]$ , denote by $\\mathbb{P}_{(i,j)}$ the probability distribution when $f^{\\star}=f^{(i,j)}$ and $\\mathbb{E}_{(i,j)}$ the expectation under $\\mathbb{P}_{(i,j)}$ . Then the adversary decides $f_{\\star}$ based on the following rule: if there exists $i\\in[N],j\\in[A-1]$ such that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbb{E}_{0}\\left[N_{T}(i,j)\\right]\\le\\frac{1}{1000\\varepsilon^{2}}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "then let $f_{\\star}=f^{(i,j)}$ . If no $i,j$ satisfies this, let $f_{\\star}=f^{(0)}$ . ", "page_idx": 35}, {"type": "text", "text": "If $f^{\\star}=f^{(0)}$ , then we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbb{E}_{0}[R_{T}]\\ge\\varepsilon\\cdot\\mathbb{E}_{0}\\left[\\sum_{i\\in[N],j\\in[A-1]}N_{T}(i,j)\\right]\\ge\\varepsilon N(A-1)\\cdot\\operatorname*{min}_{i,j}\\mathbb{E}_{0}[N_{T}(i,j)]\\ge\\frac{N(A-1)}{1000\\varepsilon}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "On the other hand, if $f_{\\star}=f^{(i,j)}$ , then we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbb{P}_{0}\\bigg(N_{T}(i,j)<\\frac{1}{\\varepsilon^{2}}\\bigg)\\geq\\frac{999}{1000}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Then by Lemma B.4, we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbb{P}_{0}\\Big(N_{T}(i,j)<\\frac{1}{\\varepsilon^{2}}\\Big)\\le3\\mathbb{P}_{i,j}\\Big(N_{T}(i,j)<\\frac{1}{\\varepsilon^{2}}\\Big)+4D_{\\mathrm{H}}^{2}(\\mathbb{P}_{0},\\mathbb{P}_{i,j}).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Then by Lemma D.2 of Foster et al. (2024), we have ", "page_idx": 36}, {"type": "equation", "text": "$$\nD_{\\mathrm{H}}^{2}(\\mathbb{P}_{0},\\mathbb{P}_{i,j})\\leq7\\mathbb{E}_{0}\\big[N_{T}(i,j)\\wedge(1/\\varepsilon^{2})\\big]\\cdot4\\varepsilon^{2}\\leq{\\frac{7}{250}}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Altogether, we can obtain ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathbb{P}_{i,j}\\Big(N_{T}(i,j)<\\frac{1}{\\varepsilon^{2}}\\Big)\\geq\\frac{1}{3}\\Big(\\mathbb{P}_{0}\\Big(N_{T}(i,j)<\\frac{1}{\\varepsilon^{2}}\\Big)-28/250\\Big)\\geq1/6.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "This in turn, implies that with the choice $\\varepsilon^{2}\\geq2N/T$ , that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathfrak{L}_{i,j}[R_{T}]\\geq\\varepsilon\\cdot\\mathbb{E}_{i,j}\\left[\\sum_{t=1}^{T}\\mathbb{1}[x_{t}=x^{(i)}]-N_{T}(x^{(i)},j)\\right]\\geq\\varepsilon\\cdot\\Big(\\frac{T}{N}-\\frac{1}{\\varepsilon^{2}}\\Big)\\mathbb{P}_{i,j}\\Big(N_{T}(i,j)<\\frac{1}{\\varepsilon^{2}}\\Big)\\geq\\frac{T\\varepsilon}{12N}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Thus if $\\sqrt{A T}>\\sqrt{N A\\Lambda}$ , then we set $\\varepsilon=\\sqrt{N A/\\Lambda}$ . We have $\\varepsilon\\leq1$ due to the assumption that $\\Lambda\\ge A N/c$ . Then we verify ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\sigma_{t}^{2}=\\sum_{t=1}^{T}\\mathbb{1}\\big(x_{t}=x^{(i)},a_{t}=j,N_{t}(i,j)\\le1/\\varepsilon^{2}\\big)\\le N(A-1)/\\varepsilon^{2}\\le\\Lambda.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Furthermore, $\\sqrt{A T}>\\sqrt{N A\\Lambda}$ implies $T\\varepsilon/N>A N/\\varepsilon$ , and thus ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{i\\in\\{0\\}\\cup[N]}\\mathbb{E}_{i}[R_{T}]\\geq\\Omega\\biggl(\\operatorname*{min}\\biggl\\{\\frac{A N}{\\varepsilon},\\frac{T\\varepsilon}{N}\\biggr\\}\\biggr)\\geq\\frac{A N}{\\varepsilon}=\\Omega(\\sqrt{N A\\Lambda}).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Otherwise if $\\sqrt{A T}<\\sqrt{N A\\Lambda}$ , then we have $T/N\\le\\Lambda$ and we set $\\varepsilon=N\\sqrt{A/T}$ . We have $\\varepsilon\\leq1$ due to the assumption that $N\\leq c{\\sqrt{T/A}}$ . Then we verify ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\sigma_{t}^{2}=\\sum_{t=1}^{T}\\mathbb{1}(x_{t}=x^{(i)},a_{t}=j,N_{t}(i,j)\\le1/\\varepsilon^{2})\\le N(A-1)/\\varepsilon^{2}\\le T/N\\le\\Lambda.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Finally, we have the lower bounds ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{i\\in\\{0\\}\\cup[N]}\\mathbb{E}_{i}[R_{T}]\\geq\\Omega\\biggl(\\operatorname*{min}\\biggl\\{\\frac{A N}{\\varepsilon},\\frac{T\\varepsilon}{N}\\biggr\\}\\biggr)\\geq\\Omega(\\sqrt{A T}).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Overall, we have proven that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{i\\in\\{0\\}\\cup[N]}\\mathbb{E}_{i}[R_{T}]\\geq\\Omega\\biggl(\\operatorname*{min}\\biggl\\{\\frac{A N}{\\varepsilon},\\frac{T\\varepsilon}{N}\\biggr\\}\\biggr)\\geq\\Omega(\\operatorname*{min}\\{\\sqrt{N A\\Lambda},\\sqrt{A T}\\}).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Proof of Theorem 5.1. ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "We first deal with some corner cases: ", "page_idx": 36}, {"type": "text", "text": "Case 1: If $A>T$ and $d>T$ , then by Lemma C.2 with $N=1$ , we have a lower bound of $\\Omega(T)$ . ", "page_idx": 36}, {"type": "text", "text": "Case 2: If $A>T$ , $d<T$ , and $d>\\Lambda$ , then by Lemma C.2 with $N=1$ and the number of actions in Lemma C.2 set to $d$ , we have a lower bound of $\\Omega(d)$ . ", "page_idx": 36}, {"type": "text", "text": "Case 3: If $A>T$ , $d<T$ , and $d<\\Lambda$ , t\u221ahen by Lemma C.1 with the number of actions in Lemma C.1 set to $d$ , we have a lower bound of $\\Omega(\\sqrt{d\\Lambda})$ . ", "page_idx": 36}, {"type": "text", "text": "Cas\u221ae 4: If $A\\,<\\,T,\\,d\\,>\\,T$ , then by Lemma C.2 with $N\\,=\\,\\sqrt{T/A}$ , we have a lower bound of $\\Omega({\\sqrt{A T}})$ . ", "page_idx": 36}, {"type": "text", "text": "Case 5: If $A<T,\\,d<T,\\,d\\leq A$ , and $d\\geq\\Lambda$ then by Lemma C.2 with $N=1$ and the number of actions in Lemma C.2 set to be $d$ , we have a lower bound of $\\Omega(d)$ . ", "page_idx": 36}, {"type": "text", "text": "Case 6: If $A<T$ , $d<T$ , $d\\leq A$ , and $d>\\Lambda$ , then b\u221ay Lemma C.1 with the number of actions in Lemma C.1 set to be $d$ , we have a lower bound of $\\Omega({\\sqrt{d\\Lambda}})$ . ", "page_idx": 37}, {"type": "text", "text": "Now, we consider our main cases where $A,d\\leq T$ and $d\\geq A$ . We consider the following two subcases: ", "page_idx": 37}, {"type": "text", "text": "Subcase 1: If $d~\\geq~\\Omega(\\sqrt{d\\Lambda})$ or $d\\ \\geq\\ \\Omega({\\sqrt{A T}})$ , then\u221a we invoke Lemm\u221aa C.2 wi\u221ath $\\textit{N}=$ $\\operatorname*{min}\\{d/A,\\sqrt{T/A}\\}$ and obtain a lower bound of $\\Omega(\\operatorname*{min}\\{d,\\sqrt{A T}\\})\\geq\\Omega(\\operatorname*{min}\\{\\sqrt{d\\Lambda}+d,\\sqrt{A T}\\})$ . Subcase 2: If $d<c\\sqrt{d\\Lambda}$ and $d<c{\\sqrt{A T}}$ for a small enough con\u221astant $0<c<1$ . Then we invoke Lemma F.6 with $N=d/A$ and obtain a lower bound of $\\Omega(\\operatorname*{min}\\{\\sqrt{d\\Lambda}+d,\\sqrt{A T}\\})$ . Thus, we conclude our proof. ", "page_idx": 37}, {"type": "text", "text": "", "page_idx": 37}, {"type": "text", "text": "G Omitted Proofs in Section 6 ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "We revise the proof of Lemma 3 of Foster and Rakhlin (2020) to show the following guarantee. ", "page_idx": 37}, {"type": "text", "text": "Lemma G.1. We have for any $t\\in[T]$ , ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\substack{\\varepsilon\\Delta(A)\\,M\\in\\mathcal{M}_{t}}}\\operatorname*{max}_{\\substack{\\mathbb{X}_{a\\sim p}}}\\left[\\operatorname*{max}_{a^{\\prime}}f_{M}\\big(x_{t},a^{\\prime}\\big)-f_{M}\\big(x_{t},a\\big)-\\gamma\\frac{\\big(f_{M}\\big(x_{t},a\\big)-f_{M_{t}}\\big(x_{t},a\\big)\\big)^{2}}{\\sigma_{M_{t}}^{2}\\big(x_{t},a\\big)}\\right]\\lesssim\\frac{A\\sigma_{M_{t}}^{2}(x_{t})}{\\gamma},\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where $\\sigma_{M_{t}}^{2}(x_{t})=\\operatorname*{sup}_{a\\in A}\\sigma_{M_{t}}^{2}(x_{t},a).$ . ", "page_idx": 37}, {"type": "text", "text": "Proof of Lemma G.1. Fix $t\\in[T]$ . Let $q\\in\\Delta(A)$ be the policy such that ", "page_idx": 37}, {"type": "equation", "text": "$$\nq(x_{t},a)=\\frac{1}{\\lambda+\\gamma/\\sigma_{M_{t}}^{2}(x_{t},a)\\cdot\\left(\\operatorname*{max}_{a^{\\prime}\\in\\mathcal{A}}{f_{M_{t}}(x_{t},a^{\\prime})-f_{M_{t}}(x_{t},a)}\\right)},\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where $\\lambda$ is such that $\\textstyle\\sum_{a\\in{\\cal A}}q(x_{t},a)\\;=\\;1$ . We show that such $\\lambda$ exists and $\\lambda\\ \\in\\ (0,A]$ . Let $\\begin{array}{r}{h(\\lambda)=\\sum_{a\\in\\mathcal{A}}\\frac{1}{\\lambda+\\gamma/\\sigma_{M_{t}}^{2}(x_{t},a)\\cdot(\\operatorname*{max}_{a^{\\prime}\\in\\mathcal{A}}f_{M_{t}}(x_{t},a^{\\prime})-f_{M_{t}}(x_{t},a))}}\\end{array}$ . Then we have $h(\\lambda)$ is monotonically decreasing with $h(0)=\\infty$ and $h(A)<1$ . Thus there exists $\\lambda\\in(0,A]$ such that $h(\\lambda)=1$ that corresponds to $q(x_{t},a)$ . ", "page_idx": 37}, {"type": "text", "text": "We first separate the regret with respect to any fixed $M$ into four parts as the following ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathbb{E}_{a\\sim q}\\Big[\\!\\operatorname*{max}_{a^{\\prime}}f_{M}(x_{t},a^{\\prime})-f_{M}(x_{t},a)\\!\\Big]\\Big]}}\\\\ &{=\\mathbb{E}_{a\\sim q}\\Big[\\!\\operatorname*{max}_{a^{\\prime}}f_{M_{t}}(x_{t},a^{\\prime})-f_{M_{t}}(x_{t},a)\\!\\Big]\\Big]+\\mathbb{E}_{a\\sim q}\\big[f_{M^{\\star}}(x_{t},a)-f_{M_{t}}(x_{t},a)\\big]\\!\\Big]}\\\\ &{\\qquad\\qquad\\qquad+\\left(f_{M}(x_{t},a^{\\star})-f_{M_{t}}(x_{t},a^{\\star})\\right)+\\left(f_{M_{t}}(x_{t},a^{\\star})-\\operatorname*{max}_{a^{\\prime}}f_{M_{t}}(x_{t},a^{\\prime})\\right),~(2)}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where $a^{\\star}\\in\\arg\\operatorname*{max}_{a\\in\\mathcal{A}}f_{M^{\\star}}(x_{t},a)$ . Firstly, by the definition of $q$ , we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{L}_{a\\sim q}\\bigg[\\underset{a^{\\prime}}{\\operatorname*{max}}\\;f_{M_{t}}(x_{t},a^{\\prime})-f_{M_{t}}(x_{t},a)\\bigg]\\bigg]=\\displaystyle\\sum_{a\\in A}\\frac{\\operatorname*{max}_{a^{\\prime}\\in A}f_{M_{t}}(x_{t},a^{\\prime})-f_{M_{t}}(x_{t},a)}{\\lambda+\\gamma/\\sigma_{M_{t}}^{2}(x_{t},a)\\cdot\\big(\\operatorname*{max}_{a^{\\prime}\\in A}f_{M_{t}}(x_{t},a^{\\prime})-f_{M_{t}}(x_{t},a)\\big)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\displaystyle\\sum_{a\\in A}\\frac{\\sigma_{M_{t}}^{2}(x_{t},a)}{\\gamma}\\leq\\frac{A\\sigma_{M_{t}}^{2}(x_{t})}{\\gamma}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Secondly, by the AM-GM inequality, we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{a\\sim q}\\left[f_{M^{\\star}}(x_{t},a)-f_{M_{t}}(x_{t},a)-\\displaystyle\\frac{\\gamma}{2}\\frac{\\left(f_{M}(x_{t},a)-f_{M_{t}}(x_{t},a)\\right)^{2}}{\\sigma_{M_{t}}^{2}(x_{t},a)}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\leq\\mathbb{E}_{a\\sim q}\\left[\\frac{\\sigma_{M_{t}}^{2}(x_{t},a)}{\\gamma}\\right]\\leq\\frac{\\sigma_{M_{t}}^{2}(x_{t})}{\\gamma}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Thirdly, again by the AM-GM inequality and the definition of $q$ , we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f_{M}(x_{t},a^{\\star})-f_{M_{t}}(x_{t},a^{\\star})-q(x_{t},a^{\\star})\\frac{\\gamma}{2}\\frac{\\left(f_{M}(x_{t},a^{\\star})-f_{M_{t}}(x_{t},a^{\\star})\\right)^{2}}{\\sigma_{M_{t}}^{2}(x_{t},a^{\\star})}}\\\\ &{\\qquad\\qquad\\leq\\frac{\\sigma_{M_{t}}^{2}(x_{t},a^{\\star})}{\\gamma q(x_{t},a^{\\star})}}\\\\ &{\\qquad\\qquad=\\frac{(\\lambda+\\gamma/\\sigma_{M_{t}}^{2}(x_{t},a^{\\star})\\cdot\\left(\\operatorname*{max}_{a^{\\prime}\\in A}f_{M_{t}}(x_{t},a^{\\prime})-f_{M_{t}}(x_{t},a)\\right))\\cdot\\sigma_{M_{t}}^{2}(x_{t},a^{\\star})}{\\gamma}}\\\\ &{\\qquad\\qquad\\leq\\frac{A\\sigma_{M_{t}}^{2}(x_{t})}{\\gamma}+\\operatorname*{max}_{a^{\\prime}\\in A}f_{M_{t}}(x_{t},a^{\\prime})-f_{M_{t}}(x_{t},a).}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Plug the inequality (22), (23), and (24) in the equality (21) to obtain the desired bound of ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathbb{E}_{a\\sim q}\\left[\\operatorname*{max}_{a^{\\prime}}f_{M}(x_{t},a^{\\prime})-f_{M}(x_{t},a)-\\gamma\\frac{\\left(f_{M}(x_{t},a)-f_{M_{t}}(x_{t},a)\\right)^{2}}{\\sigma_{M_{t}}^{2}(x_{t},a)}\\right]\\lesssim\\frac{A\\sigma_{M_{t}}^{2}(x_{t})}{\\gamma}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Lemma G.2. Whenever $I_{t}=2$ and $M^{\\star}\\in\\mathcal{M}_{t}$ , we have for any action $a\\in[A]$ , ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\sigma_{M^{\\star}}^{2}(x_{t},a)\\lesssim\\sigma_{M_{t}}^{2}(x_{t},a)\\lesssim\\sigma_{M^{\\star}}^{2}(x_{t},a).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Proof. Since for any $M,M^{\\prime}\\in\\mathcal{M}_{t}$ and $a\\in A$ , one has $D_{\\mathrm{H}}^{2}(M(x_{t},a),M^{\\prime}(x_{t},a))\\leq1/2.$ . Thus by Lemma B.2, we have that $\\sigma_{M}(x_{t},a)\\lesssim\\sigma_{M^{\\prime}}(x_{t},a)$ . ", "page_idx": 38}, {"type": "text", "text": "Then since $M^{\\star}$ is in $\\mathcal{M}_{t}$ and $M_{t}$ is a mixture of models in $\\mathcal{M}_{t}$ , we have by the total variacne law ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\sigma_{M^{\\star}}^{2}(x_{t},a)\\lesssim\\operatorname*{min}_{M\\in\\mathcal{M}_{t}}\\sigma_{M}^{2}(x_{t},a)\\lesssim\\sigma_{M_{t}}^{2}(x_{t},a)\\lesssim\\operatorname*{max}_{M\\in\\mathcal{M}_{t}}\\sigma_{M}^{2}(x_{t},a)\\lesssim\\sigma_{M^{\\star}}^{2}(x_{t},a).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Lemma G.3. Whenever $I_{t}=2$ and $M^{\\star}\\in\\mathcal{M}_{t},$ , we have for any action $a\\in A$ ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|f_{M^{\\star}}(x_{t},a)-f_{M_{t}}(x_{t},a)|\\lesssim\\sqrt{\\sigma_{M_{t}}^{2}(x_{t},a)^{2}D_{\\mathrm{H}}^{2}(M(x_{t},a),M_{t}(x_{t},a))}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Proof. Since $M^{\\star}$ is in $\\mathcal{M}_{t}$ and $M_{t}$ is a mixture of models in $\\mathcal{M}_{t}$ , we have for any action $a\\in[A]$ , ", "page_idx": 38}, {"type": "equation", "text": "$$\nD_{\\mathrm{H}}^{2}(M^{\\star}(x_{t},a),M_{t}(x_{t},a))\\leq\\operatorname*{max}_{M\\in\\mathcal{M}_{t}}D_{\\mathrm{H}}^{2}(M^{\\star}(x_{t},a),M(x_{t},a))\\leq1/2.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Then by Lemma B.3, we have that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|f_{M^{\\star}}(x_{t},a)-f_{M_{t}}(x_{t},a)|\\lesssim\\sqrt{(\\sigma_{M^{\\star}}^{2}(x_{t},a)+\\sigma_{M_{t}}^{2}(x_{t},a))D_{\\mathrm{H}}^{2}(M(x_{t},a),M_{t}(x_{t},a))}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Then by Lemma G.2, we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|f_{M^{\\star}}(x_{t},a)-f_{M_{t}}(x_{t},a)|\\lesssim\\sqrt{(\\sigma_{M^{\\star}}^{2}(x_{t},a)+\\sigma_{M_{t}}^{2}(x_{t},a))D_{\\mathrm{H}}^{2}(M(x_{t},a),M_{t}(x_{t},a))}}\\\\ &{\\qquad\\qquad\\qquad\\lesssim\\sqrt{\\sigma_{M_{t}}^{2}(x_{t},a)^{2}D_{\\mathrm{H}}^{2}(M(x_{t},a),M_{t}(x_{t},a))}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Lemma G.4. The regrets accumalated on rounds where $I_{t}=2$ are bounded with probability at least $1-\\delta$ by ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathrm{reg}_{t}\\mathbb{1}(I_{t}=2)\\lesssim\\sqrt{A\\sum_{t=1}^{T}\\sigma_{M^{\\star}}^{2}(x_{t})\\cdot\\log(|\\mathcal{M}|/\\delta)},\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where $\\sigma_{M^{\\star}}^{2}(x_{t})=\\operatorname*{sup}_{a\\in A}\\sigma_{M^{\\star}}^{2}(x_{t},a)$ and $\\mathrm{reg}_{t}=\\mathbb{E}_{a\\sim p_{t}}[\\operatorname*{max}_{a^{\\prime}}f_{M^{\\star}}(x_{t},a^{\\prime})-f_{M^{\\star}}(x_{t},a)]$ . ", "page_idx": 38}, {"type": "text", "text": "Proof. Let $\\mathbb{E}_{t}[\\cdot]:=\\mathbb{E}[\\cdot~|~\\mathcal{H}_{t}]$ where $\\mathcal{H}_{t}$ is the history up to time $t$ . By Lemma G.1 and Lemma G.3, whenever $I_{t}=2$ and $M^{\\star}\\in\\mathcal{M}_{t}$ , we have ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\natural_{t}[\\mathrm{reg}_{t}\\mathbb{1}(I_{t}=2,M^{\\star}\\in\\mathcal{M}_{t})]=\\mathbb{E}_{a\\sim p_{t}}\\Big[\\Big(\\underset{a^{\\prime}}{\\operatorname*{max}}f_{M^{\\star}}(x_{t},a^{\\prime})-f_{M^{\\star}}(x_{t},a)\\Big)\\cdot\\mathbb{1}(M^{\\star}\\in\\mathcal{M}_{t})\\Big]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\gamma\\mathbb{E}_{a\\sim p_{t}}\\Big[\\frac{(f_{M^{\\star}}(x_{t},a)-f_{M_{t}}(x_{t},a))^{2}}{\\sigma_{M_{t}}^{2}(x_{t},a)}\\cdot\\mathbb{1}(M^{\\star}\\in\\mathcal{M}_{t})\\Big]+\\frac{A\\sigma_{M_{t}}^{2}(x_{t})}{\\gamma}}\\\\ &{\\qquad\\qquad\\qquad\\lesssim\\gamma\\mathbb{E}_{a\\sim p_{t}}\\big(D_{\\mathrm{H}}^{2}(M^{\\star}(x_{t},a),M_{t}(x_{t},a))\\cdot\\mathbb{1}(M^{\\star}\\in\\mathcal{M}_{t})\\big)+\\frac{A\\sigma_{M_{t}}^{2}(x_{t})}{\\gamma}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Then by summation over $t\\in[T]$ , we have ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}_{t}[\\mathrm{reg}_{t}\\mathbb{1}(I_{t}=2,M^{\\star}\\in\\mathcal{M}_{t})]}\\\\ &{\\lesssim\\displaystyle\\frac{A\\sum_{t=1}^{T}\\sigma_{M_{t}}^{2}(x_{t})}{\\gamma}+\\gamma\\cdot\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}_{a\\sim p_{t}}\\big[D_{\\mathrm{H}}^{2}(M^{\\star}(x_{t},a),M_{t}(x_{t},a))\\cdot\\mathbb{1}(M^{\\star}\\in\\mathcal{M}_{t})\\big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Then by Lemma A.15 of Foster et al. (2021), we have that with probability at least $1-\\delta/2$ , ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathbb{E}_{a\\sim p_{t}}\\big[D_{\\mathrm{H}}^{2}(M^{\\star}(x_{t},a),M_{t}(x_{t},a))\\cdot\\mathbb{1}(M^{\\star}\\in\\mathcal{M}_{t})\\big]\\leq\\log(2|\\mathcal{M}|/\\delta).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Then by the choice of $\\gamma=\\sqrt{A\\sum_{t=1}^{T}\\sigma_{M_{t}}^{2}(x_{t})/\\log(2|\\mathcal{M}|/\\delta)}$ , we have with probability at least $1-\\delta/2$ , ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{A\\sum_{t=1}^{T}\\sigma_{M_{t}}^{2}(x_{t})}{\\gamma}+\\gamma\\cdot\\sum_{t=1}^{T}\\mathbb{E}_{a\\sim p_{t}}\\big[D_{\\mathbf{H}}^{2}(M^{\\star}(x_{t},a),M_{t}(x_{t},a))\\cdot\\mathbf{1}(M^{\\star}\\in\\mathcal{M}_{t})\\big]}}\\\\ &{}&{\\lesssim\\sqrt{A\\displaystyle\\sum_{t=1}^{T}\\sigma_{M_{t}}^{2}(x_{t})\\cdot\\log(2|\\mathcal{M}|/\\delta)}}\\\\ &{}&{\\lesssim\\sqrt{A\\displaystyle\\sum_{t=1}^{T}\\sigma_{M^{\\star}}^{2}(x_{t})\\cdot\\log(2|\\mathcal{M}|/\\delta)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where the second inequality is by Lemma G.2. We also have by Lemma A.3 of Foster et al. (2021) that with probability at least $1-\\delta/4$ , for all $t\\in[T]$ , ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\sum_{s=1}^{t}D_{\\mathrm{H}}^{2}(M^{\\star}(x_{s},a_{s}),M_{t}(x_{s},a_{s}))\\leq\\frac{3}{2}\\sum_{s=1}^{t}\\mathbb{E}_{a\\sim p_{s}}\\big[D_{\\mathrm{H}}^{2}(M^{\\star}(x_{s},a),M_{t}(x_{s},a))\\big]+4\\log(8T/\\delta).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "By union bound, this implies with probability at least $1-3\\delta/4$ , for all $t\\in[T]$ , $M^{\\star}\\in\\mathcal{M}_{t}$ . Again by Lemma A.3 of Foster et al. (2021), we have with probability at least $1-\\delta/4$ , for all $t\\in[T]$ ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathrm{reg}_{t}\\mathbb{1}(I_{t}=2,M^{\\star}\\in\\mathcal{M}_{t})\\leq\\frac{3}{2}\\sum_{t=1}^{T}\\mathbb{E}_{t}[\\mathrm{reg}_{t}\\mathbb{1}(I_{t}=2,M^{\\star}\\in\\mathcal{M}_{t})]+4\\log(8/\\delta).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Thus by the union bound , with probability at least $1-\\delta$ , we have ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\mathrm{reg}_{t}\\mathbb{1}\\big(I_{t}=2\\big)=\\displaystyle\\sum_{t=1}^{T}\\mathrm{reg}_{t}\\mathbb{1}\\big(I_{t}=2,M^{\\star}\\in\\mathcal{M}_{t}\\big)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\lesssim\\sqrt{A\\displaystyle\\sum_{t=1}^{T}\\sigma_{M^{\\star}}^{2}(x_{t})\\cdot\\log(|\\mathcal{M}|/\\delta)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Lemma G.5. For any $\\lambda\\geq0$ and $\\alpha>0$ , we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\operatorname*{min}\\left\\{\\lambda,\\,\\operatorname*{sup}_{M,M^{\\prime}\\in M_{t}}\\frac{D_{\\mathrm{H}}^{2}(M(x_{t},a_{t}),M^{\\prime}(x_{t},a_{t}))}{\\alpha^{2}T+\\sum_{\\tau=1}^{t-1}D_{\\mathrm{H}}^{2}(M(x_{\\tau},a_{\\tau}),M^{\\prime}(x_{\\tau},a_{\\tau}))}\\right\\}\\leq d_{e l u}^{\\mathrm{H}}(\\alpha)\\left(\\lambda+\\log T\\right).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Proof. The proof follows similarly from the proof of Lemma D.5 by replacing the square divergence with squared Hellinger distance. ", "page_idx": 40}, {"type": "text", "text": "Lemma G.6. The regrets accumalated on rounds where $I_{t}=1$ are bounded by ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathrm{reg}_{t}\\mathbb{1}(I_{t}=1)\\lesssim d_{e l u}^{\\mathsf{H}}(\\alpha)\\big(\\alpha^{2}T+\\log(|\\mathcal{M}|T/\\delta)\\big)\\log T,\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "for any $\\alpha>0$ . ", "page_idx": 40}, {"type": "text", "text": "Proof. For any $t\\in[T]$ and $M,M^{\\prime}\\in\\mathcal{M}_{t}$ , we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{\\tau=1}^{t-1}D_{\\mathrm{H}}^{2}(M(x_{\\tau},a_{\\tau}),M^{\\prime}(x_{\\tau},a_{\\tau}))}}\\\\ &{}&{\\leq2\\left(\\sum_{\\tau=1}^{t-1}D_{\\mathrm{H}}^{2}(M(x_{\\tau},a_{\\tau}),M_{\\tau}(x_{\\tau},a_{\\tau}))+\\sum_{\\tau=1}^{t-1}D_{\\mathrm{H}}^{2}(M_{\\tau}(x_{\\tau},a_{\\tau}),M^{\\prime}(x_{\\tau},a_{\\tau}))\\right)\\leq4L.}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Thus let $\\sum_{t=1}^{T}\\mathbb{1}(I_{t}=1)=T_{1}$ and we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathrm{reg}_{t}\\mathbb{1}(I_{t}=1)\\leq\\sum_{t=1}^{T}\\mathbb1(I_{t}=1)=T_{1}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Thus we have for any $\\alpha\\geq0$ , ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T_{1}=\\displaystyle\\sum_{I_{t}=1}1\\left\\lbrace\\operatorname*{sup}_{M,M^{\\prime}\\in\\mathcal{M}_{t}}D_{\\mathrm{H}}^{2}(M(x_{t},a_{t}),M^{\\prime}(x_{t},a_{t}))\\geq\\frac{1}{2}\\right\\rbrace}\\\\ &{\\quad\\leq\\sqrt{2}\\displaystyle\\sum_{I_{t}=1}^{\\infty}\\operatorname*{min}\\left\\lbrace\\frac{1}{\\sqrt{2}},\\operatorname*{sup}_{M,M^{\\prime}\\in\\mathcal{M}_{t}}D_{\\mathrm{H}}(M(x_{t},a_{t}),M^{\\prime}(x_{t},a_{t}))\\right\\rbrace}\\\\ &{\\quad\\leq\\sqrt{2}\\displaystyle\\sum_{I_{t}=1}^{\\infty}\\operatorname*{min}\\left\\lbrace\\frac{1}{\\sqrt{2}},\\operatorname*{sup}_{M,M^{\\prime}\\in\\mathcal{M}_{t}}\\frac{D_{\\mathrm{H}}(M(x_{t},a_{t}),M^{\\prime}(x_{t},a_{t}))}{\\sqrt{\\alpha^{2}T+\\displaystyle\\sum_{\\tau=1}^{t-1}D_{\\mathrm{H}}^{2}(M(x_{\\tau},a_{\\tau}),M^{\\prime}(x_{\\tau},a_{\\tau}))}}\\cdot\\sqrt{\\alpha^{2}T+4L}\\right\\rbrace,}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where the last inequality is by (25). Furthermore, by Cauchy-Schwarz inequality, we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{I_{t}=1}\\operatorname*{min}\\left\\{\\frac{1}{\\sqrt{2}},\\operatorname*{sup}_{M,M^{\\prime}\\in M_{t}}\\frac{D_{\\mathrm{H}}(M(x_{t},a_{t}),M^{\\prime}(x_{t},a_{t}))}{\\sqrt{\\alpha^{2}T+\\displaystyle\\sum_{\\tau=1}^{t-1}D_{\\mathrm{H}}^{2}(M(x_{\\tau},a_{\\tau}),M^{\\prime}(x_{\\tau},a_{\\tau}))}}\\cdot\\sqrt{\\alpha^{2}T+4L}\\right\\}}\\\\ &{\\leq\\sqrt{T_{1}}\\sqrt{\\displaystyle\\sum_{I_{t}=1}\\operatorname*{min}\\left\\{\\frac{1}{2},\\operatorname*{sup}_{M,M^{\\prime}\\in M_{t}}\\frac{D_{\\mathrm{H}}^{2}(M(x_{t},a_{t}),M^{\\prime}(x_{t},a_{t}))}{\\alpha^{2}T+\\displaystyle\\sum_{\\tau=1}^{t-1}D_{\\mathrm{H}}^{2}(M(x_{\\tau},a_{\\tau}),M^{\\prime}(x_{\\tau},a_{\\tau}))}\\cdot(\\alpha^{2}T+4L)\\right\\}}}\\\\ &{\\leq\\sqrt{T_{1}\\cdot d_{\\mathrm{H}}^{\\mathrm{H}}(\\alpha)(1/2+(\\alpha^{2}T+4L)\\log T)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where the last inequality is by Lemma G.5. Altogether, we have ", "page_idx": 41}, {"type": "equation", "text": "$$\nT_{1}\\lesssim\\sqrt{T_{1}\\cdot d_{\\mathrm{elu}}^{\\sf H}(\\alpha)(1/2+(\\alpha^{2}T+4L)\\log T)}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Reorganizing the above inequality, we obtain the desired bound. ", "page_idx": 41}, {"type": "text", "text": "Proof of Theorem 6.1. The proof is straight-forward by combing Lemma G.6 and Lemma G.4, i.e., with probability at least $1-\\delta$ , ", "page_idx": 41}, {"type": "equation", "text": "$$\nR_{T}\\lesssim\\sqrt{A\\sum_{t=1}^{T}\\sigma_{M^{\\star}}^{2}(x_{t})\\cdot\\log(|\\mathcal{M}|/\\delta)}+d_{\\mathrm{elu}}^{\\mathsf{H}}(\\alpha)\\big(\\alpha^{2}T+\\log(|\\mathcal{M}|T/\\delta)\\big)\\log T.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Then, by choosing $\\alpha=1/\\sqrt{T}$ , we obtain the desired bound. ", "page_idx": 41}, {"type": "text", "text": "Proof of Theorem 6.2. The proof of this theorem is essentially the same as that of Theorem 4.1. Recall that in Theorem 4.1 we construct a hard instance for a function class $\\mathcal{F}$ that only contain reward mean information, but the reward has a fixed variance upper bound $\\sigma_{t}^{2}\\leq\\sigma^{2}$ . What we do here is simply embed this instance in a model class $\\mathcal{M}$ where for every model, every context, and every action, the reward distribution is a Gaussian with mean as in $\\mathcal{F}$ and variance $\\bar{\\sigma^{2}}$ . Notice that $P_{\\sigma,\\epsilon}^{-}$ and $P_{\\sigma,\\epsilon}^{+}$ in Lemma B.1 to ", "page_idx": 41}, {"type": "text", "text": "respectively, which still gives us $\\begin{array}{r}{D_{\\mathrm{KL}}\\bigl(P_{\\sigma,\\epsilon}^{-}\\,\\|\\,P_{\\sigma,\\epsilon}^{+}\\bigr)\\leq\\frac{(2\\epsilon)^{2}}{2\\sigma^{2}}=\\frac{2\\epsilon^{2}}{\\sigma^{2}}}\\end{array}$ . This allows us to prove the same bound as in Lemma C.1. Furthermore, the eluder dimension $d_{\\mathrm{elu}}^{\\mathsf{H}}(\\mathcal{M},0)$ defined through the Hellinger distance between models remain the same as the eluder dimension $d_{\\mathrm{elu}}(\\mathcal{F},0)$ defined through the mean difference in the constructions of Lemma C.1 and Lemma C.2. Overall, the lower bounds in Lemma C.1 and Lemma C.2 are still applicable after we change the reward distribution from Bernoulli-style distributions to Gaussian distributions, and change the distance measure from mean difference to Hellinger distance. The arguments in Theorem 4.1 thus allow us to prove the same lower bound in the case here. ", "page_idx": 41}, {"type": "text", "text": "Lemma G.7. For any integer $A,T\\geq2,$ , $N\\leq c\\sqrt{T/A}$ and positive number $\\Lambda>A N/c,$ , there exists a context space $\\mathcal{X}$ , a contextual bandit model class $M\\subset(\\mathcal{X}\\times\\mathcal{A}\\rightarrow\\Delta(\\mathbb{R}))$ with eluder dimension $d_{e l u}^{\\mathsf{H}}(\\mathcal{M},0)=N(A-1)$ and action set $A=[A],$ , and adversarially assigned variances $\\sigma_{1}^{2},\\dots,\\sigma_{T}^{2}$ that $\\textstyle\\sum_{t=1}^{T}\\sigma_{t}^{2}\\leq\\Lambda$ such that any algorithm will suffer at least $\\Omega(\\operatorname*{min}\\{{\\sqrt{N A\\Lambda}},{\\sqrt{A T}}\\})$ . ", "page_idx": 41}, {"type": "text", "text": "Proof of Lemma G.7. This lower bound is based on a modification of Lemma F.6. Concretely, we illustrate here how to embed the hard case from Lemma F.6 to an equivalent model class in the distributional case. ", "page_idx": 41}, {"type": "text", "text": "One can add information into the context for the hard case constructed in Lemma F.6. Concretely, we don\u2019t enlarge the function class, but for each context $x$ , the new context space will have $T^{\\bar{A}}$ corresponding contexts $(x,j_{1},...,j_{A})_{1\\leq j_{1},...,j_{A}\\leq T}$ , where the second argument will be used to record the number of pulls to each action $a$ under context $x$ . The function value under these contexts will be the same as under the original context. The adversarial thus can choose in the new context space $(x_{t},N_{t}(x_{t},1),...,N_{t}(x_{t},\\Bar{A}))$ to embed the hard case from Lemma F.6. The Hellinger eluder dimension is twice the eluder dimension because there are two types of variances corresponding to each original context. Thus, we obtain the desired bound. ", "page_idx": 41}, {"type": "text", "text": "Proof of Theorem 6.3. ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "This proof follows a similar argument with Theorem 5.1, with only embedding the function classes to model classes. Again, we change the Bernoulli-styled distributions in the construction of Theorem 5.1 ", "page_idx": 41}, {"type": "text", "text": "to Gaussian distributions. The only other more crucial difference lies in the Subclass 2 at the end. We first deal with some corner cases: ", "page_idx": 42}, {"type": "text", "text": "Case 1: If $A>T$ and $d>T$ , then by Lemma C.2 with $N=1$ , we have a lower bound of $\\Omega(T)$ . ", "page_idx": 42}, {"type": "text", "text": "Case 2: If $A>T$ , $d<T$ , and $d>\\Lambda$ , then by Lemma C.2 with $N=1$ and the number of actions in Lemma C.2 set to $d$ , we have a lower bound of $\\Omega(d)$ . ", "page_idx": 42}, {"type": "text", "text": "Case 3: If $A>T$ , $d<T$ , and $d<\\Lambda$ , then by Lemma C.1 with the number of actions in Lemma C.1 set to $d$ , we have a lower bound of $\\Omega(\\sqrt{d\\Lambda})$ . ", "page_idx": 42}, {"type": "text", "text": "Cas\u221ae 4: If $A<T$ , $d\\,>\\,T$ , then by Lemma C.2 with $N\\,=\\,\\sqrt{T/A}$ , we have a lower bound of $\\Omega({\\sqrt{A T}})$ . ", "page_idx": 42}, {"type": "text", "text": "Case 5: If $A<T$ , $d<T$ , $d\\leq A$ , and $d\\geq\\Lambda$ then by Lemma C.2 with $N=1$ and the number of actions in Lemma C.2 set to be d, we have a lower bound of $\\Omega(d)$ . ", "page_idx": 42}, {"type": "text", "text": "Case 6: If $A<T$ , $d<T$ , $d\\leq A$ , and $d>\\Lambda$ , then b\u221ay Lemma C.1 with the number of actions in Lemma C.1 set to be $d$ , we have a lower bound of $\\Omega(\\sqrt{d\\Lambda})$ . ", "page_idx": 42}, {"type": "text", "text": "Now, we consider our main cases where $A,d\\leq T$ and $d\\geq A$ . We consider the following two subcases: ", "page_idx": 42}, {"type": "text", "text": "Subcase 1: If $d~\\geq~\\Omega(\\sqrt{d\\Lambda})$ or $d\\ \\geq\\ \\Omega({\\sqrt{A T}})$ , then\u221a we invoke Lemm\u221aa C.2 wi\u221ath $\\textit{N}=$ $\\operatorname*{min}\\{d/A,\\sqrt{T/A}\\}$ and obtain a lower bound of $\\Omega(\\operatorname*{min}\\{d,\\sqrt{A T}\\})\\geq\\Omega(\\operatorname*{min}\\{\\sqrt{d\\Lambda}+d,\\sqrt{A T}\\})$ . ", "page_idx": 42}, {"type": "text", "text": "Subcase 2: If $d\\,<\\,c\\sqrt{d\\Lambda}$ and $d\\,<\\,c{\\sqrt{A T}}$ for a \u221asmall eno\u221augh constant $c>0$ . Then we invoke Lemma G.7 and obtain a lower bound of $\\Omega(\\operatorname*{min}\\{\\sqrt{d\\Lambda}+d,\\sqrt{A T}\\})$ . ", "page_idx": 42}, {"type": "text", "text": "Thus, we conclude our proof. ", "page_idx": 42}, {"type": "text", "text": "H Upper Bound with Zero-One Variance (Section 7) ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "In this section, we consider the setting where $\\sigma_{t}~=~0$ or 1, and not reveal to the learner at the beginning of each round. The algorithm is displayed in Algorithm 6. We have the following theorem. ", "page_idx": 42}, {"type": "text", "text": "Theorem .1. With the choice of $\\begin{array}{r}{\\gamma=\\sqrt{\\frac{8A}{\\log|\\mathcal{F}|}}}\\end{array}$ , we have the upper bound on the expected regret of $^{6}$ ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathbb{E}[R_{T}]=\\mathcal{O}\\left(\\sqrt{A\\log|\\mathcal{F}|\\left(1+\\sum_{t=1}^{T}\\sigma_{t}^{2}\\right)}+A(d_{e l u}+\\log|\\mathcal{F}|)\\right).\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "We will prove upper bounds for regret of rounds with $\\sigma_{t}=0$ and $\\sigma_{t}=1$ separately. First we bound the regret of rounds with $\\sigma_{t}=1$ . ", "page_idx": 42}, {"type": "text", "text": "Lemma H.1. With \u03b3 = $\\begin{array}{r}{\\gamma=\\sqrt{\\frac{8A}{\\log|\\mathcal{F}|}}}\\end{array}$ \u00bblog8 |AF|, the output actions at at each round in Algorithm 6 satisfies ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathbb{I}[\\sigma_{t}=1]\\left(\\operatorname*{max}_{a\\in\\mathcal{A}}f^{\\star}(x_{t},a)-f(x_{t},a_{t})\\right)\\leq4\\sqrt{2A\\log|\\mathcal{F}|\\cdot\\left(1+\\sum_{t=1}^{T}\\sigma_{t}^{2}\\right)}.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Proof. Based on the inverse-gap weighting update rule Eq. (26), similar to (Foster and Rakhlin, 2020, Lemma 3), we have ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\sum_{t=1}^{T}\\mathbb{I}[\\sigma_{t}=1](\\operatorname*{max}_{a}f^{\\star}(x_{t},a)-f^{\\star}(x_{t},a_{t}))\\right]\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Algorithm 6 Variance Sensitive SquareCB for Zero-One Noise ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Input: $\\gamma$ ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "1: Let $\\mathcal{F}_{1}\\gets\\mathcal{F}$ .   \n2: for $t=1:T$ do   \n3: Receive $x_{t}$ , and calculate $\\begin{array}{r}{\\mathcal{A}_{t}=\\{a\\in\\mathcal{A}:\\exists f\\in\\mathcal{F}_{t},f(x_{t},a)=\\arg\\operatorname*{max}_{a\\in\\mathcal{A}}f(x_{t},a)\\}.}\\end{array}$ .   \n4: Sample action $a_{t}\\sim p_{t}$ and receive $\\sigma_{t}$ and $r_{t}$ , where ", "page_idx": 43}, {"type": "equation", "text": "$$\np_{t}(a)=\\left\\{\\!\\!\\begin{array}{l l}{\\frac{1}{A+\\gamma\\sqrt{1+\\sum_{s<t}\\sigma_{s}^{2}(\\operatorname*{max}_{a^{\\prime}\\in A_{t}}f_{t}(x_{t},a^{\\prime})-f_{t}(x_{t},a))}}\\qquad}&{\\mathrm{for}\\;a\\in\\mathcal{A}_{t},}\\\\ {0\\quad}&{\\mathrm{for}\\;a\\not\\in\\mathcal{A}_{t}.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "5: Calculate $L_{t}(f)$ for all $f\\in\\mathcal F$ as ", "page_idx": 43}, {"type": "equation", "text": "$$\nL_{t}(f)=\\sum_{\\tau=1}^{t}\\frac{(f(x_{\\tau},a_{\\tau})-r_{\\tau})^{2}}{\\sigma_{\\tau}^{2}},\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where for those $\\sigma_{\\tau}=0$ , we define $\\begin{array}{r}{\\frac{(f(x_{\\tau},a_{\\tau})-r_{\\tau})^{2}}{\\sigma_{\\tau}^{2}}=0}\\end{array}$ if $f(x_{\\tau},a_{\\tau})=r_{\\tau}$ and $\\infty$ otherwise. ", "page_idx": 43}, {"type": "text", "text": "6: Calculate $q_{t}(f)$ to be ", "page_idx": 43}, {"type": "equation", "text": "$$\nq_{t+1}(f)=\\frac{e^{-L_{t}(f)}}{\\sum_{g\\in\\mathcal{F}}e^{-L_{t}(g)}}.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "7: Calculate $f_{t+1}$ to be ", "page_idx": 43}, {"type": "equation", "text": "$$\nf_{t+1}=\\sum_{f\\in\\mathcal{F}}q_{t+1}(f)\\cdot f.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "8: if $\\sigma_{0}=0$ then ", "page_idx": 43}, {"type": "text", "text": "9: Update $\\mathcal{F}_{t+1}=\\{f\\in\\mathcal{F}_{t},f(x_{t},a_{t})=r_{t}\\}$ .   \n10: else   \n11: Let $\\mathcal{F}_{t+1}=\\mathcal{F}_{t}$ . ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\leq\\mathbb{E}\\left[\\sum_{t=1}^{T}\\mathbb{I}[\\sigma_{t}=1]\\cdot{\\frac{2A}{A+\\gamma{\\sqrt{1+\\sum_{s=1}^{t-1}\\sigma_{s}^{2}}}}}\\right]+{\\frac{\\gamma}{4}}\\mathbb{E}\\left[\\sum_{t=1}^{T}{\\sqrt{1+\\sum_{s=1}^{t-1}\\sigma_{s}^{2}}}\\cdot\\left(f_{t}(x_{t},a_{t})-f^{\\star}(x_{t},a_{t})\\right)^{2}\\right]\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "For the first term, we have analysis ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{T}\\mathbb{I}[\\sigma_{t}=1]\\cdot{\\frac{1}{A+\\gamma{\\sqrt{1+\\sum_{s=1}^{t-1}\\sigma_{s}^{2}}}}}\\leq{\\frac{2}{\\gamma}}\\sum_{t=1}^{T}\\left({\\sqrt{1+\\sum_{s=1}^{t}\\sigma_{s}^{2}}}-{\\sqrt{1+\\sum_{s=1}^{t-1}\\sigma_{s}^{2}}}\\right)={\\frac{2}{\\gamma}}{\\sqrt{1+\\sum_{s=1}^{T}\\sigma_{s}^{2}}},\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where the first inequality uses the fact that $\\sigma_{t}\\in\\{0,1\\}$ and when $\\sigma_{t}=1$ we have $\\begin{array}{r l}{~}&{{}\\frac{1}{\\sqrt{1+\\sum_{s=1}^{t-1}\\sigma_{s}^{2}}}=}\\end{array}$ \u221a t1  \u03c3s2 \u22642 \u00bb1 +  ts=1 \u03c3s2 \u2212\u00bb1 +  ts\u2212=11 \u03c3s2 . For the second term, we have ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{t=1}^{T}\\sqrt{1+\\sum_{s=1}^{t-1}\\sigma_{s}^{2}}\\cdot\\big(f_{t}(x_{t},a_{t})-f^{*}(x_{t},a_{t})\\big)^{2}}}\\\\ &{}&{\\leq\\sqrt{1+\\displaystyle\\sum_{t=1}^{T}\\sigma_{t}^{2}}\\cdot\\sum_{t=1}^{T}(f_{t}(x_{t},a_{t})-f^{*}(x_{t},a_{t}))^{2}}\\\\ &{}&{\\leq\\sqrt{1+\\displaystyle\\sum_{t=1}^{T}\\sigma_{t}^{2}}\\cdot\\log|\\mathcal{F}|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where the last line is according to the analysis of Vovk\u2019s aggregating algorithm (Vovk, 1995; CesaBianchi and Lugosi, 2006). ", "page_idx": 43}, {"type": "text", "text": "Above all, with \u03b3 =\u00bblog8 |AF|, we get ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}\\left[\\sum_{t=1}^{T}\\mathbb{I}[\\sigma_{t}=1](\\operatorname*{max}_{a}f^{\\star}(x_{t},a)-f^{\\star}(x_{t},a_{t}))\\right]}\\\\ {\\displaystyle\\quad\\le\\left(\\frac{2A}{\\gamma}+\\frac{\\gamma\\log|\\mathcal{F}|}{4}\\right)\\sqrt{1+\\sum_{t=1}^{T}\\sigma_{t}^{2}}}\\\\ {\\displaystyle=4\\sqrt{2A\\log|\\mathcal{F}|\\cdot\\left(1+\\sum_{t=1}^{T}\\sigma_{t}^{2}\\right)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "The following is a useful lemma regarding Eluder dimension. ", "page_idx": 44}, {"type": "text", "text": "Lemma H.2. We define ", "page_idx": 44}, {"type": "equation", "text": "$$\nZ_{t}=\\mathbb{I}\\left[\\exists f,f^{\\prime}\\in\\mathcal{F}_{t}\\;s u c h\\;t h a t\\;|f(x_{t},a_{t})-f^{\\prime}(x_{t},a_{t})|\\ge\\frac{1}{T}\\right].\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Then we have ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathbb{I}[\\sigma_{t}=0]Z_{t}\\le d_{e l u}({^1}/{T}).\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Proof. This lemma follows directly according to the definition of Eluder dimension $d_{\\mathrm{elu}}(1/T)$ in Definition 2.1, and the fact that all functions in $\\mathcal{F}_{t}$ must agree on the previous samples where $\\sigma_{s}=0$ . \u53e3 ", "page_idx": 44}, {"type": "text", "text": "With this lemma, we are ready to bound the regret of rounds with $\\sigma_{t}=0$ . ", "page_idx": 44}, {"type": "text", "text": "Lemma H.3. With $\\begin{array}{r}{\\gamma=\\sqrt{\\frac{8A}{\\log|\\mathcal{F}|}}}\\end{array}$ , the output actions $a_{t}$ at each round in Algorithm 6 satisfies ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\natural\\left[\\sum_{t=1}^{T}\\operatorname*{max}_{a\\in A}\\mathbb{I}[\\sigma_{t}=0](f^{\\star}(x_{t},a)-f^{\\star}(x_{t},a_{t}))\\right]=\\mathcal{O}\\left(\\sqrt{A\\log|\\mathcal{F}|\\sum_{t=1}^{T}\\sigma_{t}^{2}}+A(d_{e l u}+\\log|\\mathcal{F}|)\\right).\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Proof. In this proof, we focus on the case $\\sigma_{t}=0$ . We first notice that for rounds $\\sigma_{t}=0$ , we always have $f^{*}(x_{t},a_{t})=r_{t}$ . Hence $f^{*}\\in\\mathcal{F}_{t}$ for every $t\\in[T]$ . We define ", "page_idx": 44}, {"type": "equation", "text": "$$\na_{t}^{*}=\\arg\\operatorname*{max}_{a\\in\\mathcal{A}}f^{*}(x_{t},a).\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "At round $t$ , we let $b_{t}=\\arg\\operatorname*{max}_{a\\in\\mathcal{A}}f_{t}(x_{t},a)$ , and we define $B_{t}\\subset A_{t}$ as ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\mathcal{B}_{t}\\triangleq\\left\\{a\\in\\mathcal{A}_{t}:f_{t}(x_{t},b_{t})-f_{t}(x_{t},a)\\leq\\frac{2}{T}\\right\\}.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "For rounds with $\\sigma_{t}=0$ , based on $x_{t},f_{t}$ , we divide such rounds into two cases, which we denote as $\\mathcal{T}_{1}$ and $\\mathcal{T}_{2}$ . ", "page_idx": 44}, {"type": "text", "text": "(a) There exists $a\\in\\mathcal{B}_{t}$ and $f\\in\\mathcal{F}_{t}$ , such that $|f(x_{t},a)-f_{t}(x_{t},a)|\\geq1/T.$ ", "page_idx": 44}, {"type": "text", "text": "(b) For all $f\\in\\mathcal{F}_{t}$ and $a\\in\\mathcal{B}_{t}$ , we have $|f(x_{t},a)-f_{t}(x_{t},a)|\\leq1/T$ . ", "page_idx": 44}, {"type": "text", "text": "Regret in $t\\in\\mathcal T_{1}$ . For those $t\\in\\mathcal T_{1}$ , first we notice that for any $a\\in\\mathcal{B}_{t}$ , according to Eq. (26) we have ", "page_idx": 44}, {"type": "equation", "text": "$$\np_{t}(a)=\\frac{1}{A+\\gamma\\sqrt{1+\\sum_{s<t}\\sigma_{s}^{2}}(f_{t}(x_{t},b_{t})-f_{t}(x_{t},a))}\\ge\\frac{1}{A+\\gamma\\sqrt{1+\\sum_{s<t}\\sigma_{s}^{2}}\\cdot2/T}\\ge\\frac{1}{7A},\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where in the last inequality we use the fact that ", "page_idx": 45}, {"type": "equation", "text": "$$\n{\\frac{2\\gamma{\\sqrt{1+\\sum_{s<t}\\sigma_{s}^{2}}}}{T}}\\leq{\\frac{2\\gamma{\\sqrt{T}}}{T}}={\\frac{2{\\sqrt{8A/\\log|{\\mathcal{F}}|}}}{\\sqrt{T}}}\\leq6A.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Therefore, according to the definition of $\\tau_{1}$ that there exists some $a\\in B_{t}$ and $f\\in\\mathcal F_{t}$ such that $|f(x_{t},a)-f_{t}(x_{t},a)|\\geq1/T$ , for any $t\\in\\mathcal T_{1}$ with probability at least $^{1/7A}$ we will sample an action $a_{t}$ such that $Z_{t}=1$ $Z_{t}$ is defined in Eq. (27)). Therefore, we have ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\displaystyle\\sum_{t\\in\\mathcal{T}_{1}}(f^{\\star}(x_{t},a_{t}^{\\star})-f^{\\star}(x_{t},a_{t}))\\right]}\\\\ &{\\quad\\le\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}\\mathbb{I}[t\\in\\mathcal{T}_{1}]\\right]\\le\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}\\frac{Z_{t}\\mathbb{I}[\\sigma_{t}=0]}{1/(7A)}\\right]=7A\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}Z_{t}\\mathbb{I}[\\sigma_{t}=0]\\right]\\le7A d_{\\mathrm{elu}}(1/T),}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where the last inequality uses Lemma H.2. ", "page_idx": 45}, {"type": "text", "text": "Regret in $t\\in T_{2}$ with $a_{t}\\in\\mathfrak{B}_{t}$ . For those $t\\in T_{2}$ , to facilitate the analysis we define ", "page_idx": 45}, {"type": "equation", "text": "$$\nl_{t}(f,x_{t},a)=\\frac{(f(x_{t},a)-r_{t})^{2}-(f^{\\star}(x_{t},a)-r_{t})^{2}}{\\sigma_{t}^{2}}\\qquad\\forall t\\in[T],\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "and ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\Phi_{t}=\\log\\left(\\sum_{f\\in\\mathcal{F}}\\exp\\left(-\\sum_{s=1}^{t}l_{s}(f,x_{s},a_{s})\\right)\\right).\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Then we have ", "page_idx": 45}, {"type": "equation", "text": "$$\nq_{t}(f)=\\frac{\\exp(-\\sum_{s=1}^{t-1}l_{s}(f,x_{s},a_{s}))}{\\sum_{g\\in\\mathcal{F}}\\exp(-\\sum_{s=1}^{t-1}l_{s}(f,x_{s},a_{s}))},\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "which implies that ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\{\\bar{\\theta}_{t-1}-\\bar{\\theta}_{t}\\}=}&{\\mathbb{E}\\left[\\log\\sum_{f\\in\\mathcal{F}}\\exp\\left(-\\sum_{i=1}^{n}\\bar{l}_{i}(f,x_{s},a_{i})\\right)\\right]}\\\\ &{=-\\mathbb{E}\\left[\\log\\left(\\sum_{f\\in\\mathcal{F}}\\exp\\left(-\\sum_{i=1}^{n-1}\\bar{l}_{i}(f,x_{s},a_{i})\\right)\\right)\\right]}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\left[\\log\\left(\\int\\exp\\left(-\\frac{a_{i}}{f}\\right)\\exp\\left(-\\bar{l}_{i}(f,x_{s},a_{i})\\right)\\right)\\right]}\\\\ &{\\quad\\quad\\quad\\quad\\geq-\\bar{p}(a_{i})\\log\\left(\\sum_{f\\in\\mathcal{F}}\\exp\\left(-\\bar{l}_{i}(f,x_{s},a_{i})\\right)\\right)}\\\\ &{\\quad\\quad\\quad\\quad=-\\bar{p}(a_{i}^{-1})\\log\\left(\\displaystyle\\sum_{f\\in\\mathcal{F}}\\exp\\left(-\\frac{a_{i}}{f}\\right)[f(x_{s},a_{i}^{\\alpha})\\right)\\right)}\\\\ &{\\quad\\quad\\quad=-\\bar{p}(a_{i}^{-1})\\log\\left(\\left.\\ln\\sum_{f\\in\\mathcal{F}}\\exp\\left(\\prod_{s=1}^{n-1}\\bar{l}_{i}(f,x_{s},a_{i}^{\\alpha})\\right)\\right)}\\\\ &{=-\\bar{p}(a_{i}^{-1})\\log\\left(1-\\sum_{f\\in\\mathcal{F}}\\phi\\left(\\bar{l}_{i}[f(x_{s},a_{i}^{\\alpha})\\neq f^{*}(x_{s},a_{i}^{\\alpha})\\right)\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\geq\\operatorname{p}(a_{i}^{1})\\displaystyle\\sum_{f\\in\\mathcal{F}}\\phi\\left(\\bar{l}_{i}[f(x_{s},a_{i}^{\\alpha})\\neq f^{*}(x_{s},a_{i}^{\\alpha})],}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Hence we have ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{a\\in\\mathcal{B}_{t}}p_{t}(a)\\left(f^{\\star}(x_{t},a_{t}^{\\star})-f^{\\star}(x_{t},a)\\right)}}\\\\ &{}&{\\stackrel{(i)}{\\le}\\sum_{a\\in\\mathcal{B}_{t}}p_{t}(a)\\left(f^{\\star}(x_{t},a_{t}^{\\star})-f_{t}(x_{t},a_{t}^{\\star})+f_{t}(x_{t},a)-f^{\\star}(x_{t},a)\\right)+\\frac{2}{T}}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overset{(i)}{\\leq}\\underset{\\alpha\\in B_{t}}{\\sum}p_{t}(a)\\left(f^{*}(x_{t},a_{t}^{*})-f_{t}(x_{t},a_{t}^{*})\\right)+\\frac{3}{T}}\\\\ &{\\leq\\frac{1}{p_{t}(a_{t}^{*})}\\cdot p_{t}(a_{t}^{*})|f_{t}(x_{t},a_{t}^{*})-f^{*}(x_{t},a_{t}^{*})|+\\frac{3}{T}}\\\\ &{=\\frac{1}{p_{t}(a_{t}^{*})}\\cdot p_{t}(a_{t}^{*})\\left|\\underset{f\\in\\mathcal{F}}{\\sum}q_{t}(f)f(x_{t},a_{t}^{*})-f^{*}(x_{t},a_{t}^{*})\\right|+\\frac{3}{T}}\\\\ &{=\\frac{1}{p_{t}(a_{t}^{*})}\\cdot p_{t}(a_{t}^{*})\\underset{f\\in\\mathcal{F}}{\\sum}q_{t}(f)|[f(x_{t},a_{t}^{*})\\neq f^{*}(x_{t},a_{t}^{*})]+\\frac{3}{T}}\\\\ &{\\overset{(i i i)}{\\leq}\\left(A+\\gamma\\sqrt{1+\\underset{t=1}{\\sum}\\sigma_{t}^{2}}\\right)\\cdot\\mathbb{E}[\\bar{\\mathfrak{F}}_{t-1}-\\Phi_{t}]+\\frac{3}{T},}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "where in $(i)$ we use the fact that according to the definition of $B_{t}$ in Eq. (28), ", "page_idx": 46}, {"type": "equation", "text": "$$\nf_{t}(x_{t},a)-f_{t}(x_{t},a_{t}^{\\star})\\geq f_{t}(x_{t},b_{t})-f_{t}(x_{t},a_{t}^{\\star})-\\frac{2}{T}\\geq-\\frac{2}{T},\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "and $(i i)$ uses the definition of $\\mathcal{T}_{2}$ that for any $a\\;\\in\\;{\\cal B}_{t}$ and $f\\,\\in\\,{\\mathcal{F}}_{t}$ we always have $\\vert f_{t}(x_{t},a)-$ $f^{\\star}(x_{t},\\dot{a})|\\leq{1}/{T}$ , and $(i i i)$ uses Eq. (30) and also the fact that ", "page_idx": 46}, {"type": "equation", "text": "$$\np_{t}(a_{t}^{\\star})=\\frac{1}{A+\\gamma\\sqrt{1+\\sum_{s<t}\\sigma_{s}^{2}}(f_{t}(x_{t},b_{t})-f(x_{t},a_{t}^{\\star}))}\\geq\\frac{1}{A+\\gamma\\sqrt{1+\\sum_{t=1}^{T}\\sigma_{t}^{2}}}.\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Suming this up for every $t\\in T_{2}$ , we obtain that ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\displaystyle\\sum_{t\\in\\mathcal{T}_{2}}\\mathbb{I}[a_{t}\\in\\mathcal{B}_{2}](f^{\\star}(x_{t},a_{t}^{\\star})-f^{\\star}(x_{t},a_{t}))\\right]}\\\\ &{\\quad\\le\\left(A+\\gamma\\sqrt{1+\\displaystyle\\sum_{t=1}^{T}\\sigma_{t}^{2}}\\right)\\cdot\\left(1+\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}[\\Phi_{t-1}-\\Phi_{t}]\\right)+\\frac{3}{T}\\cdot T}\\\\ &{\\quad\\le\\left(A+\\gamma\\sqrt{1+\\displaystyle\\sum_{t=1}^{T}\\sigma_{t}^{2}}\\right)\\log|\\mathcal{F}|+3,}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "where in the last inequality we use the definition of $\\Phi_{t}$ in Eq. (29) that $\\Phi_{0}=\\log|\\mathcal{F}|$ and ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\Phi_{T}\\geq\\log\\left(\\exp\\left(-\\sum_{s=1}^{T}l_{s}(f^{\\star},x_{s},a_{s})\\right)\\right)=0.\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Regret in $t\\in T_{2}$ with $a_{t}\\notin{\\boldsymbol{B}}_{t}$ . Next, for $a_{t}\\notin B_{t}$ , according to the definition of $\\boldsymbol{A}_{t}$ , there exists a function $\\tilde{f}\\in\\mathcal{F}_{t}$ such that $\\tilde{f}(x_{t},a_{t})\\geq\\tilde{f}(x_{t},a^{\\prime})$ for any $a^{\\prime}\\in\\mathcal{A}$ . This implies that ", "page_idx": 46}, {"type": "equation", "text": "$$\nf_{t}(x_{t},a_{t})\\overset{(i)}{\\leq}f_{t}(x_{t},b_{t})-\\frac{2}{T}\\overset{(i i)}{\\leq}\\tilde{f}(x_{t},b_{t})+\\frac{1}{T}-\\frac{2}{T}=\\tilde{f}(x_{t},a_{t})-\\frac{1}{T},\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "where in $(i)$ we use the definition of $B_{t}$ in Eq. (28), and in $(i i)$ we use the definition of $\\mathcal{T}_{2}$ that for any $f\\in\\mathcal{F}_{t}$ , $|{\\dot{f}}(x_{t},b_{t})-f_{t}(x_{t},b_{t})|\\leq1/T$ . Therefore, in those rounds with $\\sigma_{t}=0,t\\in\\mathcal{T}_{2}$ and $a\\not\\in B_{t}$ , we always have $Z_{t}=1$ $Z_{t}$ is defined in Eq. (27)), which implies that ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\sum_{t\\in\\mathcal{T}_{2}}\\mathbb{I}[a_{t}\\not\\in B_{2}]\\left(f^{\\star}(x_{t},a_{t}^{\\star})-f^{\\star}(x_{t},a_{t})\\right)\\right]\\leq\\mathbb{E}\\left[\\sum_{t=1}^{T}\\mathbb{I}[\\sigma_{t}=0]Z_{t}\\right]\\leq d_{\\mathrm{elu}}(1/T),\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "where the last inequality uses Lemma H.2. ", "page_idx": 46}, {"type": "text", "text": "Finally we combined these these bounds in $\\mathcal{T}_{1}$ and $\\mathcal{T}_{2}$ together, and obtain that ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}\\mathbb{I}[\\sigma_{t}=0](f^{*}(x_{t},a_{t}^{*})-f^{*}(x_{t},a_{t}))\\right]}\\\\ &{\\quad=\\mathbb{E}\\left[\\displaystyle\\sum_{t\\in\\mathcal{T}_{1}}(f^{*}(x_{t},a_{t}^{*})-f^{*}(x_{t},a_{t}))\\right]+\\mathbb{E}\\left[\\displaystyle\\sum_{t\\in\\mathcal{T}_{2}}(f^{*}(x_{t},a_{t}^{*})-f^{*}(x_{t},a_{t}))\\right]}\\\\ &{\\quad\\le7A d_{\\mathrm{det}}(1/r)+\\mathbb{E}\\left[\\displaystyle\\sum_{t\\in\\mathcal{T}_{2}}\\mathbb{I}[a_{t}\\in B_{2}]\\left(f^{*}(x_{t},a_{t}^{*})-f^{*}(x_{t},a_{t})\\right)\\right]}\\\\ &{\\quad\\quad\\quad+\\mathbb{E}\\left[\\displaystyle\\sum_{t\\in\\mathcal{T}_{2}}\\mathbb{I}[a_{t}\\notin B_{2}]\\left(f^{*}(x_{t},a_{t}^{*})-f^{*}(x_{t},a_{t})\\right)\\right]}\\\\ &{\\quad\\le7A d_{\\mathrm{det}}(1/r)+\\left(A+\\gamma\\sqrt{1+\\displaystyle\\sum_{t=1}^{T}\\sigma_{t}^{2}}\\right)\\log|\\mathcal{F}|+3+d_{\\mathrm{et}}(1/r)}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "With our choice of \u03b3 =\u00bb $\\begin{array}{r}{\\gamma=\\sqrt{\\frac{8A}{\\log|\\mathcal{F}|}}}\\end{array}$ log |F|, we have ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\sum_{t=1}^{T}\\mathbb{I}[\\sigma_{t}=0](f^{\\star}(x_{t},a_{t}^{\\star})-f^{\\star}(x_{t},a_{t}))\\right]=\\mathcal{O}\\left(\\sqrt{A\\log|\\mathcal{F}|\\sum_{t=1}^{T}\\sigma_{t}^{2}}+A(d_{\\mathrm{elu}}+\\log|\\mathcal{F}|)\\right).\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Finally, combining the regret bound for rounds with $\\sigma_{t}\\,=\\,1$ and $\\sigma_{t}\\,=\\,0$ together, we can prove Theorem H.1. ", "page_idx": 47}, {"type": "text", "text": "Proof of Theorem H.1. We decompose the regret ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathfrak{I}_{T}=\\sum_{t=1}^{T}\\left(\\operatorname*{max}_{a\\in\\mathcal{A}}f^{\\star}(x_{t},a)-f^{\\star}(x_{t},a_{t})\\right)}\\\\ {\\displaystyle\\quad=\\sum_{t=1}^{T}\\mathbb{I}[\\sigma_{t}=0]\\left(\\operatorname*{max}_{a\\in\\mathcal{A}}f^{\\star}(x_{t},a)-f^{\\star}(x_{t},a_{t})\\right)+\\sum_{t=1}^{T}\\mathbb{I}[\\sigma_{t}=1]\\left(\\operatorname*{max}_{a\\in\\mathcal{A}}f^{\\star}(x_{t},a)-f^{\\star}(x_{t},a_{t})\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "According to Lemma H.1, we have ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\sum_{t=1}^{T}\\mathbb{I}[\\sigma_{t}=1]\\left(\\operatorname*{max}_{a\\in\\mathcal{A}}f^{\\star}(x_{t},a)-f^{\\star}(x_{t},a_{t})\\right)\\right]=\\mathcal{O}\\left(\\sqrt{A\\log|\\mathcal{F}|\\left(1+\\sum_{t=1}^{T}\\sigma_{t}^{2}\\right)}\\right),\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "and according to Lemma H.3, we have ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\natural\\left[\\sum_{t=1}^{T}\\mathbb{I}[\\sigma_{t}=0]\\left(\\operatorname*{max}_{a\\in A}f^{\\star}(x_{t},a)-f^{\\star}(x_{t},a_{t})\\right)\\right]=\\mathcal{O}\\left(\\sqrt{A\\log|\\mathcal{F}|\\sum_{t=1}^{T}\\sigma_{t}^{2}}+A(d_{\\mathrm{elu}}+\\log|\\mathcal{F}|)\\right).\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Summing these two together, we obtain that ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\mathbb{E}[R_{T}]=\\mathcal{O}\\left(\\sqrt{A\\log|\\mathcal{F}|\\sum_{t=1}^{T}\\sigma_{t}^{2}}+A(d_{\\mathrm{elu}}+\\log|\\mathcal{F}|)\\right).\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 48}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 48}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 48}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 48}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 48}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 48}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. 1. Claims ", "page_idx": 48}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] . ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Justification: The main claims made in the abstract and introductio accurately reflect the paper\u2019s contributions and scope. The claims are validated by detailed proofs. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 48}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 49}, {"type": "text", "text": "Justification: The paper discuss the limitations of the work in the discussion section. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 49}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] . ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Justification: The paper provides detailed assumptions and proofs. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 49}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 50}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 50}, {"type": "text", "text": "Justification: This is a theoretical paper. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 50}, {"type": "text", "text": "\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 50}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 50}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 50}, {"type": "text", "text": "Justification: This paper does not include experiments. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code. \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. \u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). ", "page_idx": 50}, {"type": "text", "text": "", "page_idx": 51}, {"type": "text", "text": "\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 51}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 51}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 51}, {"type": "text", "text": "Justification: This paper does not include experiments. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 51}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 51}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 51}, {"type": "text", "text": "Justification: This paper does not include experiments. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 51}, {"type": "text", "text": "", "page_idx": 52}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 52}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 52}, {"type": "text", "text": "Justification: This paper does not include experiments. ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 52}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 52}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 52}, {"type": "text", "text": "Justification: The research conducted in the paper conforms, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 52}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 52}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 52}, {"type": "text", "text": "Justification: This is a theoretical work. There is no societal impact of the work performed. Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 52}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 53}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 53}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 53}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 53}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 53}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 53}, {"type": "text", "text": "Justification: This paper does not use existing assets. ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 53}, {"type": "text", "text": "", "page_idx": 54}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 54}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 54}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 54}, {"type": "text", "text": "Guidelines: ", "page_idx": 54}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 54}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 54}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 54}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 54}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 54}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 54}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 54}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 54}, {"type": "text", "text": "Guidelines: ", "page_idx": 55}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 55}]