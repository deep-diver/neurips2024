[{"figure_path": "zw2K6LfFI9/figures/figures_1_1.jpg", "caption": "Figure 1: Overview of PERIA (Perceive, Reason, Imagine, Act), inspired by the human cognitive process of following complex instructions, which involves perceiving environment and tasks, reasoning the required language plans, and imagining the intermediate subgoal images before acting.", "description": "This figure illustrates the PERIA framework, which mimics human cognitive processes for complex instruction following.  It shows four stages: 1. Perceive: the robot perceives the environment and the task instruction; 2. Reason: the robot reasons out the steps required to complete the task, breaking down the complex instruction into sub-instructions; 3. Imagine: the robot imagines the intermediate subgoals and generates corresponding images; 4. Act: the robot acts according to the imagined subgoals and sub-instructions. The overall process is analogous to how humans interpret and execute complex instructions, using both logical reasoning and visual imagination.", "section": "1 Introduction"}, {"figure_path": "zw2K6LfFI9/figures/figures_3_1.jpg", "caption": "Figure 2: Overview of PERIA. PERIA first learns to align the vision and language on encoding side of MLLM for perceiving. Then PERIA performs instruction tuning to MLLM jointly with diffusion model in an end-to-end manner to unlock the reasoning and generation ability for holistic language planning and vision planning. and show the module is trainable and frozen, respectively.", "description": "This figure illustrates the PERIA framework, which consists of three main stages: Perceive, Reason, and Imagine.  In the Perceive stage, a lightweight multimodal alignment is performed on the encoding side of a Multi-modal Large Language Model (MLLM) to enable the model to understand both visual and textual information. In the Reason stage, instruction tuning is applied to the MLLM to allow it to generate stepwise language plans. Finally, in the Imagine stage, the MLLM is jointly trained with a diffusion model to generate coherent subgoal images that align with the language plans. An alignment loss is also used to ensure consistency between the language and vision planning.  The entire framework is trained end-to-end.", "section": "3 Method"}, {"figure_path": "zw2K6LfFI9/figures/figures_5_1.jpg", "caption": "Figure 3: Three pipelines of MLLM for generation images. PERIA (a) leverage visual tokens extracted from the MLLM during language planning serve as more expressive guidance for subgoal imagination compared to captions (b) or decomposed instructions (c) in language only.", "description": "This figure illustrates three different approaches to generating subgoal images using a multi-modal large language model (MLLM).  The first approach (a) uses visual tokens extracted from the MLLM during language planning, providing more expressive guidance than either captions (b) or decomposed instructions alone (c). In (a), the MLLM processes both language and vision and creates the visual tokens to guide the image generation model.  (b) only uses the text captions from a vision language model to generate the images. (c) shows image generation solely relying on decomposed language instructions, lacking the benefit of visual context. The figure highlights PERIA's approach which integrates language planning and visual planning for more effective subgoal image generation.", "section": "3 Method"}, {"figure_path": "zw2K6LfFI9/figures/figures_6_1.jpg", "caption": "Figure 4: The illustrating examples of holistic language and vision planning for general instructions, with stepwise sub-instructions and coherent subgoal images enhancing the instruction following.", "description": "This figure showcases three example tasks demonstrating PERIA's holistic approach to language and vision planning.  Each row represents a different task: stacking blocks by size, sorting blocks into color-matched bowls, and spelling a word using letter blocks. The left column shows the original task instruction. The middle column depicts stepwise sub-instructions generated by PERIA's language planning module, guiding the robot through sequential actions. The right column presents the corresponding subgoal images generated by the vision planning module, providing intuitive visual milestones that complement the textual instructions. The combination of language and vision planning helps the robot successfully complete these complex manipulation tasks, significantly improving accuracy and efficiency compared to methods relying solely on language or vision.", "section": "Experiments"}, {"figure_path": "zw2K6LfFI9/figures/figures_8_1.jpg", "caption": "Figure 5: More detailed quantitative analysis. (a) The ablation studies on consistency loss and [IMG] token numbers. (b) The comparisons of three planning paradigms in tasks with various horizon lengths. (c) The evaluation of generalization ability of three levels. See text for further discussion.", "description": "This figure presents a quantitative analysis of the PERIA model's performance, broken down into three subfigures. Subfigure (a) shows the impact of the consistency loss and the number of [IMG] tokens on the model's performance. Subfigure (b) compares the performance of three different planning paradigms (language, vision, and holistic) across tasks with varying horizon lengths. Subfigure (c) evaluates the generalization ability of the model across three levels of task complexity.", "section": "Experiments"}, {"figure_path": "zw2K6LfFI9/figures/figures_12_1.jpg", "caption": "Figure 4: The illustrating examples of holistic language and vision planning for general instructions, with stepwise sub-instructions and coherent subgoal images enhancing the instruction following.", "description": "This figure shows examples of how PERIA performs holistic language and vision planning for three different tasks. Each task starts with a high-level instruction (e.g., \"Stack all blocks in a pyramid and each layer in one color\"). PERIA then decomposes this instruction into a sequence of stepwise sub-instructions, which are accompanied by corresponding subgoal images.  These subgoal images visually represent the intermediate states that the robot should achieve towards the completion of the task.  The combination of language and vision planning provides more intuitive and informative guidance to the robot than language-only planning, making the instruction-following process more robust and efficient.", "section": "Experiments"}, {"figure_path": "zw2K6LfFI9/figures/figures_12_2.jpg", "caption": "Figure 4: The illustrating examples of holistic language and vision planning for general instructions, with stepwise sub-instructions and coherent subgoal images enhancing the instruction following.", "description": "This figure showcases examples of PERIA's holistic language and vision planning for complex tasks.  It presents three different manipulation tasks, each illustrated with a sequence of images and corresponding text. The text includes the original instruction, the decomposed language plan (stepwise instructions), and a visual plan (coherent subgoal images).  This demonstrates how PERIA breaks down complex instructions into smaller steps for both language and visual guidance, helping the robot to complete the task.", "section": "Experiments"}, {"figure_path": "zw2K6LfFI9/figures/figures_13_1.jpg", "caption": "Figure 4: The illustrating examples of holistic language and vision planning for general instructions, with stepwise sub-instructions and coherent subgoal images enhancing the instruction following.", "description": "This figure shows example scenarios of PERIA's holistic language and vision planning.  For each example, a complex instruction (e.g., \"Stack all blocks in a pyramid and each layer in one color\") is broken down into stepwise sub-instructions and subgoal images. These sub-instructions and images guide the robot's actions, providing a more intuitive and effective approach to long-horizon manipulation tasks than traditional methods that rely solely on language or vision planning.", "section": "Experiments"}, {"figure_path": "zw2K6LfFI9/figures/figures_13_2.jpg", "caption": "Figure 9: The example in SortVerticalSymmBlockstoArea in Letters Shape.", "description": "This figure shows an example of the task \"SortVerticalSymmBlockstoArea\" within the \"Letters Shape\" category of the LoHoRavens benchmark.  The task involves sorting vertically symmetrical letters to a specific area on a table. The figure illustrates the sequence of actions the robot takes to perform this task, progressing from an initial state with scattered letters to a final state with the vertically symmetrical letters neatly organized in the designated area.", "section": "A.2 Letters"}, {"figure_path": "zw2K6LfFI9/figures/figures_13_3.jpg", "caption": "Figure 11: The example in SpellTransName in Letters Spell.", "description": "This figure shows an example of the SpellTransName task in the Letters Spell category. The task is to spell out the name of a common transportation. The robot successfully spells out the word \"Airplane\" by moving the letters one by one into the corresponding positions.", "section": "A.2 Letters"}, {"figure_path": "zw2K6LfFI9/figures/figures_14_1.jpg", "caption": "Figure 4: The illustrating examples of holistic language and vision planning for general instructions, with stepwise sub-instructions and coherent subgoal images enhancing the instruction following.", "description": "This figure shows several examples of how PERIA performs holistic language and vision planning.  For each task, the top row displays the initial scene, while the following rows show the stepwise decomposition into sub-instructions and corresponding subgoal images.  The sub-instructions guide the robot's actions, while the subgoal images provide a visual representation of the desired intermediate states, facilitating a more intuitive and accurate understanding of the task. The examples highlight PERIA's ability to handle complex, long-horizon tasks, breaking down general instructions into manageable steps.", "section": "Experiments"}, {"figure_path": "zw2K6LfFI9/figures/figures_14_2.jpg", "caption": "Figure 12: The example in RearrangeObjtoGoalthenRestore in VIMA-BENCH Rearrange.", "description": "This figure shows an example of the task \"RearrangeObjtoGoalthenRestore\" from the VIMA-BENCH Rearrange benchmark. The task involves rearranging objects to a specific setup shown in the image, and then restoring them to their original positions. The figure consists of a sequence of images showing the robot's actions to complete the task, along with the initial and final states.  The task demonstrates the complexity of long-horizon manipulation tasks where the robot needs to plan a sequence of actions to achieve a specific goal, potentially requiring intermediate subgoals.", "section": "A.3 VIMA-BENCH"}, {"figure_path": "zw2K6LfFI9/figures/figures_15_1.jpg", "caption": "Figure 13: The example in SweepNoTouchCons in VIMA-BENCH Constraints.", "description": "This figure shows an example of the `SweepNoTouchCons` task from the VIMA-BENCH dataset. The task instruction is \"Sweep all <obj> into <container> without touching <constraint>\".  The image sequence displays a robot arm manipulating objects. The goal is to move all the yellow blocks into the wooden container without touching the red blocks.  This illustrates a long-horizon manipulation task where careful planning and coordination are required to avoid collisions and complete the task successfully.", "section": "A.3 VIMA-BENCH"}, {"figure_path": "zw2K6LfFI9/figures/figures_15_2.jpg", "caption": "Figure 4: The illustrating examples of holistic language and vision planning for general instructions, with stepwise sub-instructions and coherent subgoal images enhancing the instruction following.", "description": "This figure showcases examples of how PERIA performs holistic language and vision planning to follow complex instructions.  It shows three different long-horizon manipulation tasks. For each, the top row illustrates the initial scene. The middle row depicts the stepwise sub-instructions generated by the language planner (textual) and the corresponding subgoal images generated by the vision planner (visual). The bottom row shows the final state after the robot has completed the task.  This demonstrates how PERIA combines language and vision to break down complex tasks into manageable steps, guided by both logical reasoning and intuitive visualization.", "section": "Experiments"}, {"figure_path": "zw2K6LfFI9/figures/figures_16_1.jpg", "caption": "Figure 15: World Cloud: We created the word cloud to visually summarize the key aspects covered by the diverse manipulation instructions across three tasks types.", "description": "This figure shows a word cloud visualization summarizing the key aspects of the three benchmark datasets (Blocks&Bowls, Letters, and VIMA-BENCH). The size of each word corresponds to its frequency in the instructions.  It highlights the frequent use of words related to colors, object sizes, spatial relationships, actions, and specific instructions for each dataset, illustrating the complexity and variety of instructions.", "section": "B Details of Datasets"}, {"figure_path": "zw2K6LfFI9/figures/figures_21_1.jpg", "caption": "Figure 16: The evaluation of fundamental perception capabilities between language planning methods with MLLMs.", "description": "This radar chart visualizes the performance of three different methods (EmbodiedGPT, PERIA without perception pretraining, and PERIA) across five fundamental perception capabilities: object recognition, color recognition, size identification, number counting, and spatial relationship understanding.  Each axis represents one of these capabilities, and the distance from the center to the point on each axis indicates the performance score for that capability.  The chart shows that PERIA, with its multi-modal alignment, significantly outperforms the other two methods across all five perception capabilities.", "section": "Further Analysis"}, {"figure_path": "zw2K6LfFI9/figures/figures_21_2.jpg", "caption": "Figure 17: The evaluation of the normalized CLIP scores between instructions and generated subgoal images for each generation step, reflecting the stepwise accuracy of instruction following and the incremental progress towards ultimate goals specified by complex instructions.", "description": "This figure compares the performance of three different methods (PERIA, PERIA with decoupled training, and CoTDiffusion) in terms of semantic alignment between generated subgoal images and instructions across different task horizons.  The normalized CLIP score is used as the metric to evaluate this alignment. The x-axis represents the horizon length (number of steps) of the tasks, and the y-axis shows the normalized CLIP score.  Higher scores indicate better semantic alignment. The results show that PERIA consistently outperforms the other two methods, especially in longer-horizon tasks, demonstrating the effectiveness of holistic language and vision planning in achieving accurate instruction following.", "section": "Further Analysis"}, {"figure_path": "zw2K6LfFI9/figures/figures_22_1.jpg", "caption": "Figure 18: The evaluation of PERIA with different LLM backbones across three task types.", "description": "This bar chart compares the success rates of PERIA when using different large language models (LLMs) as backbones across three benchmark tasks: Blocks&Bowls, Letters, and VIMA-BENCH.  The LLMs tested are Vicuna-7B, Vicuna-13B, Llama2-7B, and Llama3-8B.  The chart demonstrates that larger and more recently developed LLMs tend to result in higher success rates for PERIA, showcasing the benefit of more powerful LLM backbones for improved performance in long-horizon robotic manipulation tasks.", "section": "4.2 Main Quantitative Results of Success Rate"}]