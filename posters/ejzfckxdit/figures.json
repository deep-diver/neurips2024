[{"figure_path": "EJZfcKXdiT/figures/figures_2_1.jpg", "caption": "Figure 1: The pipeline of Event-based 3D Reconstruction using 3D Gaussian Splatting (Event-3DGS). The proposed event-based 3DGS framework enables direct processing of event data and reconstructs 3D scenes while simultaneously optimizing scenario and sensor parameters. A high-pass filter-based photovoltage contrast estimation module is presented to reduce noise in event data, enhancing the robustness of our method in real-world scenes. An event-based 3D reconstruction loss is designed to optimize the parameters of our method for better reconstruction quality.", "description": "This figure illustrates the overall framework of Event-3DGS. It consists of three main modules: 1) High-pass filter-based photovoltage contrast estimation, which preprocesses event data to reduce noise. 2) Photovoltage contrast rendering, which uses 3D Gaussian splatting to render the photovoltage contrast images from the 3D scene representation. 3) Event-based 3D reconstruction loss, which compares the rendered photovoltage contrast with the ground truth obtained from the event data to optimize the model parameters for better reconstruction quality. The framework directly processes event data and reconstructs 3D scenes by simultaneously optimizing both scene and sensor parameters.", "section": "3 Method"}, {"figure_path": "EJZfcKXdiT/figures/figures_6_1.jpg", "caption": "Figure 2: Representative visualization results on the DeepVoxels synthetic dataset [38]. Obviously, our Event-3DGS produces visually pleasing images with fine details and fewer artifacts.", "description": "This figure compares the 3D reconstruction results of different methods on the DeepVoxels dataset.  The first column shows the original event image data. The second column displays the results from E2VID, a state-of-the-art method. The third column showcases the results of combining E2VID with 3D Gaussian Splatting (3DGS). The fourth column presents the results using pure integration with 3DGS (PI-3DGS).  The fifth column shows the results of the proposed Event-3DGS method. The final column shows the ground truth images.  The comparison highlights that Event-3DGS produces higher-quality 3D reconstructions with finer details and fewer artifacts than the other methods.", "section": "4.2 Effective Test"}, {"figure_path": "EJZfcKXdiT/figures/figures_7_1.jpg", "caption": "Figure 7: Representative visualization results series the real-world Event-Camera dataset[29]. Our Event-3DGS method produces clearer results compared to other methods.", "description": "This figure compares the visual results of different 3D reconstruction methods on real-world scenes from the Event-Camera dataset.  It shows the original event data, results from E2VID, E2VID+3DGS, PI-3DGS, and finally the results from the proposed Event-3DGS method.  The frame camera images are also provided for comparison.  The figure demonstrates that Event-3DGS produces clearer, more detailed 3D reconstructions compared to other methods, especially handling noisy or low-light conditions better.", "section": "4.2 Effective Test"}, {"figure_path": "EJZfcKXdiT/figures/figures_7_2.jpg", "caption": "Figure 4: Representative visualization examples on low-light and high-speed motion blur scenarios.", "description": "This figure shows the results of 3D reconstruction using Event-3DGS in challenging scenarios with low light and high speed motion. The top row shows the results under low light condition, where a conventional RGB camera struggles to capture clear images. The bottom row shows the results under high speed motion condition, where motion blur is prominent in the conventional RGB image. In contrast, Event-3DGS produces more visually pleasing results even under these challenging conditions.", "section": "4 Experiments"}, {"figure_path": "EJZfcKXdiT/figures/figures_8_1.jpg", "caption": "Figure 6: Representative visualization examples of motion deblurring. Note that, our Event-3DGS can be extended for high-quality hybrid reconstruction using events and frames with motion blur.", "description": "This figure showcases the motion deblurring capabilities of the proposed Event-3DGS method.  It compares results from different methods on a sequence of images containing motion blur. The first column displays the original RGB image; the second column presents the corresponding event data; subsequent columns illustrate the reconstructed images using E2VID, E2VID+3DGS, Event-3DGS, and E-Deblur-3DGS, respectively. The figure highlights how Event-3DGS and its deblurring extension effectively mitigate motion blur, yielding sharper and more detailed reconstructions compared to other methods.", "section": "4.4 Scalability Test"}, {"figure_path": "EJZfcKXdiT/figures/figures_8_2.jpg", "caption": "Figure 5: Representative examples of colorful event-based 3D reconstruction.", "description": "This figure showcases the results of colorful 3D reconstruction using the proposed Event-3DGS method. It compares the reconstruction quality of the proposed method (C-Event3DGS) with two other methods (Event and E2VID+3DGS) on a real-world dataset. The results demonstrate that C-Event3DGS produces visually pleasing images with fine details and fewer artifacts compared to other methods.", "section": "4.3 Ablation Test"}, {"figure_path": "EJZfcKXdiT/figures/figures_13_1.jpg", "caption": "Figure 7: Representative visualization results series the real-world Event-Camera dataset[29]. Our Event-3DGS method produces clearer results compared to other methods.", "description": "This figure compares the 3D reconstruction results of five different methods using real-world event camera data.  The methods compared are: raw event data, E2VID, E2VID+3DGS, PI-3DGS, Event-3DGS, and a frame camera.  The figure shows that the Event-3DGS method significantly improves the quality and clarity of the reconstruction, producing clearer results than all the other methods. Each row presents the same viewpoint from slightly different angles.", "section": "4.2 Effective Test"}]