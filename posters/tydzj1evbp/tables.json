[{"figure_path": "TYdzj1EvBP/tables/tables_2_1.jpg", "caption": "Table 1: An example of FICTIONAL KNOWLEDGE dataset. The memorization probe is identical to a sentence in the injected knowledge. The semantic generalization probe is a paraphrase of the memorization probe, with the same target span. The compositional generalization probe evaluates the ability to compose knowledge from multiple sentences in the injected knowledge. The target span of each probe is bolded.", "description": "This table exemplifies the structure of the FICTIONAL KNOWLEDGE dataset used in the paper's experiments.  It shows an example of injected knowledge (a passage about a fictional Martian government), along with three types of probes designed to test different aspects of factual knowledge acquisition: memorization (exact recall), semantic generalization (paraphrase understanding), and compositional generalization (inferring new facts from multiple sentences).  Each probe is a cloze task, where the bolded portion is the target span to be filled in by the model, assessing the depth of knowledge acquired.", "section": "3 Experimental Setup"}, {"figure_path": "TYdzj1EvBP/tables/tables_6_1.jpg", "caption": "Table 2: Decay constant of average retainability (R(p, t)) measured with OLMo-7B at different pretraining stages, acquisition depths, and injection scenarios. Note that the larger value indicates that the model forgets acquired knowledge with a higher rate.", "description": "This table shows the decay constant (\u03b1) for the retainability metric (R(p, t)).  Retainability measures how quickly the model forgets factual knowledge acquired after a local acquisition maximum. A higher decay constant indicates faster forgetting. The table breaks down the decay constant by pretraining stage (Early, Mid, Late), acquisition depth (Memorization, Semantic, Composition), and injection scenario (Duplication, Paraphrase).", "section": "4.3 Forgetting in factual knowledge acquisition"}, {"figure_path": "TYdzj1EvBP/tables/tables_15_1.jpg", "caption": "Table 1: An example of FICTIONAL KNOWLEDGE dataset. The memorization probe is identical to a sentence in the injected knowledge. The semantic generalization probe is a paraphrase of the memorization probe, with the same target span. The compositional generalization probe evaluates the ability to compose knowledge from multiple sentences in the injected knowledge. The target span of each probe is bolded.", "description": "This table exemplifies the FICTIONAL KNOWLEDGE dataset used in the paper.  It demonstrates three types of probes used to assess different levels of factual knowledge acquisition by LLMs: memorization (identical to a sentence in the injected knowledge), semantic generalization (paraphrased version of the memorization probe with the same target span), and compositional generalization (evaluation of the model's ability to combine knowledge from multiple sentences in the injected knowledge).  Each probe type has a bolded target span indicating the part evaluated for knowledge acquisition.", "section": "3 Experimental Setup"}, {"figure_path": "TYdzj1EvBP/tables/tables_16_1.jpg", "caption": "Table 1: An example of FICTIONAL KNOWLEDGE dataset. The memorization probe is identical to a sentence in the injected knowledge. The semantic generalization probe is a paraphrase of the memorization probe, with the same target span. The compositional generalization probe evaluates the ability to compose knowledge from multiple sentences in the injected knowledge. The target span of each probe is bolded.", "description": "This table exemplifies the structure of the FICTIONAL KNOWLEDGE dataset used in the paper's experiments.  It shows how factual knowledge is injected, and the three levels of probes designed to test different aspects of knowledge acquisition: memorization (exact recall), semantic generalization (paraphrase understanding), and compositional generalization (inferencing from multiple sentences). Each probe includes a bolded target span representing the factual information being tested.", "section": "3 Experimental Setup"}, {"figure_path": "TYdzj1EvBP/tables/tables_20_1.jpg", "caption": "Table 5: The initial learning rate for each intermediate OLMo checkpoint based on model sizes and the pretraining stages. For OLMo-7B, the pretraining stages align with the following number of pretrained tokens: 177B, 500B, 1.5T. For OLMo-1B, the pretraining stages align with the following number of pretrained tokens: 168B, 500B, 1.5T.", "description": "This table shows the initial learning rates used for different sized models (OLMo-1B and OLMo-7B) at various stages of pretraining. The pretraining stages are categorized as Early, Mid, and Late, corresponding to specific token counts for each model size.  The table provides context for understanding how initial learning rates were adjusted across different training scenarios.", "section": "D Detailed Training Setup"}, {"figure_path": "TYdzj1EvBP/tables/tables_25_1.jpg", "caption": "Table 6: Anticipated x-intercepts of R(p, t) measured with OLMo-7B, at three different pretraining stages, acquisition depths, and injection scenarios. The units are log(Tokens).", "description": "This table shows the anticipated x-intercepts of the retainability (R(p,t)) function. The x-intercept represents the point where the improvement in log probability of factual knowledge is completely lost after the model is trained with the injected knowledge. The table presents these x-intercepts for OLMo-7B model at three different pretraining stages (Early, Middle, Late) for different acquisition depths (Memorization, Semantic, Composition) and injection scenarios (Duplication, Paraphrase). The units are in log(Tokens).", "section": "4.3 Forgetting in factual knowledge acquisition"}, {"figure_path": "TYdzj1EvBP/tables/tables_27_1.jpg", "caption": "Table 7: Decay constant of average retainability (R(p, t)) measured with OLMo-1B, at three different pretraining stages, acquisition depths, and injection scenarios. The values for the Early (168B) checkpoint are omitted due to the poor linear fitting (R2 < 0.4), which is attributed to the highly unstable dynamics as shown in Appendix Figure 8 and 14.", "description": "This table presents the decay constant of average retainability for OLMo-1B model at three different pretraining stages (Early, Mid, and Late) across different acquisition depths (Memorization, Semantic, and Composition) under two injection scenarios (Duplication and Paraphrase). The decay constant indicates how fast the model forgets the acquired factual knowledge.  The Early (168B) checkpoint data is missing due to poor linear fitting, likely resulting from unstable model dynamics.", "section": "E.4 Forgetting dynamics of OLMo-1B checkpoints"}, {"figure_path": "TYdzj1EvBP/tables/tables_27_2.jpg", "caption": "Table 8: Anticipated x-intercepts of R(p, t) measured with OLMo-1B, at three different pretraining stages, acquisition depths, and injection scenarios. The units are log(Tokens). The values for the Early (168B) checkpoint are omitted due to the poor linear fitting (R\u00b2 < 0.4), as mentioned in Appendix Table 7.", "description": "This table shows the anticipated x-intercepts of the retainability (R(p, t)) for OLMo-1B model at three different pretraining stages (Early, Mid, Late), three different acquisition depths (Memorization, Semantic, Composition), and two different injection scenarios (Duplication, Paraphrase). The x-intercept represents the training steps (in log scale of tokens) at which the improvement of log probability induced by the injected knowledge at the local acquisition maxima completely vanishes. Note that the data for Early (168B) checkpoint is omitted due to poor linear fitting.", "section": "E.4 Forgetting dynamics of OLMo-1B checkpoints"}, {"figure_path": "TYdzj1EvBP/tables/tables_28_1.jpg", "caption": "Table 2: Decay constant of average retainability (R(p, t)) measured with OLMo-7B at different pretraining stages, acquisition depths, and injection scenarios. Note that the larger value indicates that the model forgets acquired knowledge with a higher rate.", "description": "This table shows the decay constant (\u03b1) of the retainability (R(p,t)) values which represent how quickly the model forgets the acquired factual knowledge in terms of fraction. It shows how fast the model loses the improvement of log probability. The table presents the decay constant for three different pretraining stages (Early (170B), Mid (500B), and Late (1.5T)), three different acquisition depths (Memorization, Semantic, and Composition), and two different injection scenarios (Duplication and Paraphrase). A larger decay constant implies faster forgetting.", "section": "4.3 Forgetting in factual knowledge acquisition"}, {"figure_path": "TYdzj1EvBP/tables/tables_32_1.jpg", "caption": "Table 2: Decay constant of average retainability (R(p, t)) measured with OLMo-7B at different pretraining stages, acquisition depths, and injection scenarios. Note that the larger value indicates that the model forgets acquired knowledge with a higher rate.", "description": "This table presents the decay constant (a) of retainability, representing the rate at which the model loses the improvement in log probability of factual knowledge over training steps.  The data is broken down by three different pretraining stages (Early, Mid, Late), three acquisition depths (Memorization, Semantic, Composition), and two injection scenarios (Duplication, Paraphrase).  A higher decay constant indicates faster forgetting.", "section": "4.3 Forgetting in factual knowledge acquisition"}, {"figure_path": "TYdzj1EvBP/tables/tables_32_2.jpg", "caption": "Table 6: Anticipated x-intercepts of R(q, t) measured with OLMo-7B, at three different pretraining stages, acquisition depths, and injection scenarios. The units are log(Tokens).", "description": "This table presents the anticipated x-intercepts of the retainability metric (R(q, t)) for the OLMo-7B model across three different pretraining stages (Early, Mid, Late), three different acquisition depths (Memorization, Semantic, Composition), and two injection scenarios (Duplication, Paraphrase). The x-intercept represents the point at which the model completely forgets the acquired factual knowledge.  The units of measurement are log(Tokens), indicating the number of tokens processed after the local acquisition maxima before complete forgetting occurs. Values are shown with standard deviation.", "section": "E.4 Forgetting dynamics of OLMo-7B checkpoints"}]