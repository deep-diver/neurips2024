[{"figure_path": "rSSpEmrN0C/tables/tables_6_1.jpg", "caption": "Table 1: Comparison with SOTA OCR-free MLLMs. * indicates the training set used.", "description": "This table compares the performance of the proposed LayTextLLM model with other state-of-the-art (SOTA) OCR-free Multimodal Large Language Models (MLLMs) on document-oriented visual question answering (VQA) and key information extraction (KIE) tasks.  The metrics used are accuracy percentages, and the table shows results across various datasets for both VQA and KIE tasks.  The asterisk (*) indicates when a model was fine-tuned using data from the target dataset.", "section": "4. Experiments"}, {"figure_path": "rSSpEmrN0C/tables/tables_6_2.jpg", "caption": "Table 2: Comparison with other OCR-based methods. * indicates the training set used.", "description": "This table compares the performance of LayTextLLM with other state-of-the-art OCR-based methods on document-oriented VQA and KIE tasks.  The metrics used are ANLS%/CIDEr for VQA and F-score% for KIE, across various datasets like DocVQA, VisualMRC, FUNSD, CORD, and SROIE.  The table highlights that LayTextLLM significantly outperforms other methods, especially when fine-tuned on specific datasets (indicated by *).", "section": "4. Experiments"}, {"figure_path": "rSSpEmrN0C/tables/tables_7_1.jpg", "caption": "Table 3: Comparison with LayoutLLM. * indicates that the cleaned test set used in Luo et al. [21].", "description": "This table compares the performance of LayTextLLM with LayoutLLM and LayoutLLMCOT on document-oriented VQA and KIE tasks.  It shows that LayTextLLM outperforms the baselines, especially when fine-tuned with additional datasets (LayTextLLMall). The results highlight LayTextLLM's effectiveness in handling tasks involving both text and layout information.", "section": "4.5 Quantitative Results"}, {"figure_path": "rSSpEmrN0C/tables/tables_8_1.jpg", "caption": "Table 4: Ablations on pre-training and SFT component of LayTextLLM (Accuracy).", "description": "This table presents the ablation study results for LayTextLLM, showing the impact of Layout-aware Next Token Prediction (LNTP) and Shuffled-OCR Supervised Fine-tuning (SSFT) on the model's performance.  It shows accuracy results for both document-oriented VQA and KIE tasks, demonstrating the individual and combined contributions of LNTP and SSFT to overall performance improvements.", "section": "4.6 Analysis"}, {"figure_path": "rSSpEmrN0C/tables/tables_8_2.jpg", "caption": "Table 5: Average sequence length of each data for different methods using Llama2 tokenizer.", "description": "This table presents a comparison of the average input sequence lengths across four different datasets (DocVQA, CORD, FUNSD, and SROIE) for three different models: LayTextLLM, DocLLM, and Coor-as-tokens.  It highlights the impact of different methods of incorporating layout information on sequence length.", "section": "4.5 Quantitative Results"}, {"figure_path": "rSSpEmrN0C/tables/tables_14_1.jpg", "caption": "Table 6: LayTextLLM trainng Hyper-parameters.", "description": "This table presents the hyperparameters used for training the LayTextLLM model.  It breaks down the settings used for both pre-training and fine-tuning stages.  Specific parameters shown include the backbone model used (Llama2-7B-base), the Plora rank, batch size, maximum sequence length, precision, number of trainable parameters, and the number of fixed parameters.  Additionally, it details the learning rate, weight decay, scheduler type (cosine), Adam optimizer betas, Adam epsilon, warm-up period, and the number of epochs for both the pre-training and fine-tuning phases.", "section": "Implementation Detail"}, {"figure_path": "rSSpEmrN0C/tables/tables_15_1.jpg", "caption": "Table 7: Shuffling ratio exploration in FUNSD dataset.", "description": "This table shows the performance of Llama2-7B-base and LayTextLLM models on the FUNSD dataset under different shuffling ratios (0%, 20%, 50%, and 100%).  The shuffling ratio refers to the percentage of training examples where the order of OCR-derived text tokens was randomly shuffled. The results demonstrate that LayTextLLM consistently outperforms Llama2-7B-base across all shuffling ratios and that LayTextLLM performs best with a shuffling ratio of 20%.", "section": "4.6 Analysis"}, {"figure_path": "rSSpEmrN0C/tables/tables_15_2.jpg", "caption": "Table 8: Comparison with SOTA OCR-free MLLMs on ChartQA. * indicates the training set used.", "description": "This table compares the performance of LayTextLLM with other state-of-the-art OCR-free Multimodal Large Language Models (MLLMs) on the ChartQA dataset.  It shows accuracy scores for each model, highlighting the superior performance of LayTextLLM, especially after fine-tuning with ChartQA's training data.", "section": "4.5 Quantitative Results"}]