[{"figure_path": "rSSpEmrN0C/figures/figures_1_1.jpg", "caption": "Figure 1: The performance against input sequence length of different datasets across various OCR-based methods where data is from Tab. 2 and 5.", "description": "This figure compares the performance of various OCR-based methods (LayTextLLM, DocLLM, and coordinate-as-tokens) across different datasets (DocVQA, FUNSD, SROIE, CORD) with respect to the input sequence length.  The x-axis shows the input sequence length, while the y-axis represents the performance, likely accuracy or F1-score. The different colors and shapes represent the different methods. The figure demonstrates that LayTextLLM achieves high performance with relatively short sequences compared to other methods.", "section": "Introduction"}, {"figure_path": "rSSpEmrN0C/figures/figures_3_1.jpg", "caption": "Figure 2: An overview of LayTextLLM incorporates interleaving bounding box tokens (b\u00b2) with text tokens (t\u00b2), where the superscripts represent the sequence positions of the tokens.", "description": "This figure shows a schematic overview of the LayTextLLM architecture.  It illustrates how the model processes both textual and spatial layout information.  OCR-processed text and bounding box coordinates are first obtained. The bounding boxes are then projected to single embeddings via a Spatial Layout Projector (SLP).  These embeddings (represented as 'b' tokens) are interleaved with the text tokens ('t' tokens) from the tokenizer before feeding into a large language model. The output is the answer to the question posed to the system. The superscripts in the diagram represent the position of each token in the sequence fed to the model.", "section": "3.1 Spatial Layout Projector (SLP)"}, {"figure_path": "rSSpEmrN0C/figures/figures_3_2.jpg", "caption": "Figure 3: The illustration of P-LORA, adapted from [15].", "description": "This figure illustrates the Layout Partial Low-Rank Adaptation (P-LORA) module.  P-LORA is a method for efficiently adapting a pre-trained large language model (LLM) to incorporate additional modalities, such as spatial layout information in this case, while minimizing the number of added parameters and preserving the LLM's existing knowledge. It involves adding low-rank matrices (WA and WB) to the original LLM weights (Wo) to handle the new bounding box tokens, thus allowing the model to process both text and layout information effectively.", "section": "3.1.2 Layout Partial Low-Rank Adaptation"}, {"figure_path": "rSSpEmrN0C/figures/figures_4_1.jpg", "caption": "Figure 4: Comparison of Layout-aware Next Token Prediction and normal Next Token Prediction.", "description": "This figure compares the standard Next Token Prediction method with the Layout-aware Next Token Prediction method proposed in the paper. The standard method only uses textual tokens to predict the next token in a sequence. In contrast, the proposed Layout-aware method incorporates bounding box tokens (representing spatial layout information) along with the textual tokens to make the prediction. This integration aims to improve the model's understanding of both textual and spatial context, thereby enhancing the prediction accuracy.", "section": "3.2 Training Procedure"}, {"figure_path": "rSSpEmrN0C/figures/figures_4_2.jpg", "caption": "Figure 2: An overview of LayTextLLM incorporates interleaving bounding box tokens (b\u00b2) with text tokens (t\u00b2), where the superscripts represent the sequence positions of the tokens.", "description": "This figure illustrates the architecture of LayTextLLM.  It shows how the model processes both text and layout information.  OCR pre-processing extracts bounding boxes and text from the document image.  The Spatial Layout Projector (SLP) converts each bounding box into a single embedding, which is then interleaved with the text tokens using a tokenizer. These combined tokens are fed into a large language model to generate the final output (answer to a question or extraction of key information). The superscripts (b\u00b2 and t\u00b2) denote the position of each token in the sequence.", "section": "3.1 Spatial Layout Projector (SLP)"}, {"figure_path": "rSSpEmrN0C/figures/figures_13_1.jpg", "caption": "Figure 2: An overview of LayTextLLM incorporates interleaving bounding box tokens (b\u00b2) with text tokens (t\u00b2), where the superscripts represent the sequence positions of the tokens.", "description": "This figure shows a schematic of the LayTextLLM architecture.  It illustrates how the model processes both textual and spatial layout information. The spatial layout is projected into a single embedding for each bounding box, and these embeddings are interleaved with the text tokens before being fed into the large language model (LLM). The superscripts in the caption denote the sequence position of each token.", "section": "3.1 Model Architecture"}, {"figure_path": "rSSpEmrN0C/figures/figures_15_1.jpg", "caption": "Figure 2: An overview of LayTextLLM incorporates interleaving bounding box tokens (b\u00b2) with text tokens (t\u00b2), where the superscripts represent the sequence positions of the tokens.", "description": "This figure illustrates the architecture of LayTextLLM.  The model takes as input both text and bounding boxes from an OCR engine.  Each bounding box is projected into a single embedding using a Spatial Layout Projector (SLP). These bounding box embeddings are then interleaved with the text embeddings before being fed into a large language model (LLM). The interleaving process streamlines the interaction between layout and text data, allowing the LLM to fully utilize its autoregressive capabilities for document understanding.", "section": "3.1 Model Architecture"}]