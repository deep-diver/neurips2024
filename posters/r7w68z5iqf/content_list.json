[{"type": "text", "text": "Parameter Efficient Adaptation for Image Restoration with Heterogeneous Mixture-of-Experts ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Hang Guo1 Tao Dai\u22172 Yuanchao Bai3 Bin Chen3 Xudong Ren1 Zexuan Zhu2 Shu-Tao Xia1,4 ", "page_idx": 0}, {"type": "text", "text": "1Tsinghua University 2Shenzhen University 3Harbin Institute of Technology 4Peng Cheng Larboratory https://github.com/csguoh/AdaptIR ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Designing single-task image restoration models for specific degradation has seen great success in recent years. To achieve generalized image restoration, all-in-one methods have recently been proposed and shown potential for multiple restoration tasks using one single model. Despite the promising results, the existing all-in-one paradigm still suffers from high computational costs as well as limited generalization on unseen degradations. In this work, we introduce an alternative solution to improve the generalization of image restoration models. Drawing inspiration from recent advancements in Parameter Efficient Transfer Learning (PETL), we aim to tune only a small number of parameters to adapt pre-trained restoration models to various tasks. However, current PETL methods fail to generalize across varied restoration tasks due to their homogeneous representation nature. To this end, we propose AdaptIR, a Mixture-of-Experts (MoE) with orthogonal multi-branch design to capture local spatial, global spatial, and channel representation bases, followed by adaptive base combination to obtain heterogeneous representation for different degradations. Extensive experiments demonstrate that our AdaptIR achieves stable performance on single-degradation tasks, and excels in hybrid-degradation tasks, with fine-tuning only $0.6\\%$ parameters for 8 hours. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Image restoration, aiming to restore high-quality images from their degraded counterparts, is a fundamental computer vision problem and has been studied for many years. Due to its ill-posed nature, early research efforts [1, 2, 3] typically focus on developing single-task models, with each model handling only one specific degradation. Consequently, these methods often exhibit limited generalization across different image restoration tasks. ", "page_idx": 0}, {"type": "text", "text": "To improve generalization ability, all-in-one image restoration methods [4, 5, 6] have recently been proposed and have attracted great research interest. By training one model with multiple degradation data, these methods enable the single model to handle various degradations. Despite the promising results, the existing all-in-one paradigm still faces several challenges. Firstly, the all-in-one model can only restore degradations encountered during training; once training is complete, the model cannot handle new degradations. Secondly, since the knowledge of restoring multiple degradations is learned by a single model, it incurs a significant cost to train and store these all-in-one models. ", "page_idx": 0}, {"type": "text", "text": "In this work, we propose an alternative solution to improve the generalization ability of restoration models in handling multiple degradations. Drawing inspiration from Parameter Efficient Transfer Learning (PETL) [7, 8, 9, 10], we aim to insert a small number of trainable modules into frozen pre-trained restoration backbones. By training only these newly added modules on downstream tasks, the pre-trained restoration backbone can be adapted to unseen restoration tasks. Since only a small number of parameters need to be trained, the training cost is very small and the training process can converge quickly when new tasks are added. When the training is completed, only the newly added parameters need to be stored, thus greatly reducing the storage cost. ", "page_idx": 0}, {"type": "image", "img_path": "R7w68Z5iqf/tmp/41c2cf537ec323703d0eadf244b20abe517a09621ee93c6d8f2283007ffa76b5.jpg", "img_caption": ["Figure 1: (a)&(b) We find directly applying the current PETL methods to image restoration leads to unstable performance on single degradation. (c) The current PETL method suffers sub-optimal results on hybrid degradation which requires heterogeneous representation. (d)&(e) We use Fourier analysis to visualize Adapter and our AdaptIR and find that Adapter exhibits homogeneous frequency representations even when faced with different degradations, while our AdaptIR can adaptively learn degradation-specific heterogeneous representations. We provide more evidence in Appendix I. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Despite the potential of applying PETL techniques to image restoration, our experiments reveal that existing PETL methods can work normally on specific degradation, but fail to generalize across multiple degradations, exhibiting unstable performance when adapted to different restoration tasks. As shown in Fig. 1(a), the most widely used PETL method, Adapter [8], performs well on the draining task. However, when applying Adapter to the low-light image enhancement task, the adapted model shows significant performance degradation. This phenomenon also occurs with other methods, such as the recent state-of-the-art PETL method FacT [10](Fig. 1(b)). This confusing phenomenon motivates us to discover possible reasons. ", "page_idx": 1}, {"type": "text", "text": "To this end, we design preliminary experiments, in which we fine-tune the pre-trained restoration model [11] using existing PETL schemes, and then use Fourier analysis [12] to observe the frequency characteristics of features from these methods. It is observed that the features from current PETL methods exhibit homogeneous representation across different restoration tasks (see Fig. 1(d)). As demonstrated in previous work [6], different restoration tasks prefer certain representations for optimal results, we thus hypothesize that the performance drop occurs when the representation needed to address one specific degradation does not match the homogeneous representation of existing PETL methods. To verify this hypothesis, we further test current PETL methods using the hybrid degradation task (Fig. 1(c)), which requires heterogeneous representations to handle diverse degradations, and we find all existing approaches suffer severe performance drops. Based on the above experiments, we argue that the homogeneous representation of existing PETL methods hinders stable performance on single degradation tasks and advanced performance on hybrid degradation tasks. ", "page_idx": 1}, {"type": "text", "text": "In order to learn heterogeneous representations across tasks, one possible solution is the multibranch structures, where each branch is designed to learn orthogonal representation bases, and then adaptively combine these bases for specific degradation. Following this idea, we propose AdaptIR, a heterogeneous Mixture-of-Experts (MoE) to adapt pre-trained restoration models with heterogeneous representations across tasks. Our AdaptIR adopts orthogonal multi-branch design to learn local spatial, global spatial, and channel representation bases. Specifically, The Local Interaction Module (LIM) employs depth-separable convolution with kernel weight decomposition to exploit local spatial representation. We then employ the Frequency Affine Module (FAM), which performs frequency affine transformation to introduce global spatial modeling ability. Additionally, the Channel Gating Module (CGM) is adopted to capture channel interactions. Finally, we utilize the Adaptive Feature Ensemble to dynamically fuse these three representation bases for specific degradation. Thanks to the heterogeneous representation modeling, our AdaptIR achieves stable performance on single-degradation tasks and advanced performance on hybrid-degradation tasks. ", "page_idx": 1}, {"type": "text", "text": "The contributions of our work are as follows: (i) We propose a novel PETL paradigm to improve the generalization for image restoration models, and further investigate the specific challenges when applying existing PETL methods to low-level restoration. (ii) We introduce AdaptIR, a custom PETL method that employs a heterogeneous MoE for orthogonal representation modeling. To the best of our knowledge, this is the first work to explore parameter-efficient adaptation for image restoration. (iii) Experiments on various downstream tasks demonstrate that AdaptIR achieves robust performance on single degradation tasks and advanced results on hybrid degradation tasks. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Generalized Image Restoration ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Image restoration has attracted a lot of research interest in recent years. Due to the challenging ill-posed nature, some early research paradigms typically study each sub-task in image restoration independently and have recently achieved favorable progress in their respective fields [13, 14, 15, 2, 16]. However, designing such a single-task model is cumbersome, and does not consider the similarities among different tasks. Recently, all-in-one image restoration [4, 5, 6] has offered a way to improve the generalization of image restoration models. By training one single model on multiple degradations, it allows the model to have the ability to handle multiple degradations. For example, AirNet [4] proposes a two-stage training scheme to first learn the degradation representation, which is then used in the following restoration stage. PromptIR [5] utilizes prompt learning to obtain degradation-specific prompts to train the model in an end-to-end manner. Despite the progress, the current all-in-one restoration paradigm can only deal with degradation seen during training and it is inevitable to re-train the model when needing to add new degradations. In addition, incorporating the knowledge of handling multiple degradations into one model has to increase the model size, which causes large training and storage costs. ", "page_idx": 2}, {"type": "text", "text": "2.2 Parameter-Efficient Transfer Learning ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Parameter efficient transfer learning, which initially came from with NLP [17, 18, 19, 8, 20, 21, 22, 23], aims to catch up with full fine-tuning by training a small number of parameters. Recently, this technique has emerged in the field of computer vision with promising results [9, 7, 24, 10, 25, 26, 27, 28, 29, 30, 31]. For example, VPT [9] adds learnable tokens, also called prompts, to the input sequence of one frozen transformer layer. Adapter [17] employs a bottleneck structure to adapt the pre-trained model. Some attempts also introduce parameterized hypercomplex multiplication layers [22] and re-parameterisation [32] to adapter-based methods. Moreover, LoRA [21] utilizes the low-rank nature of the incremental weight in attention and performs matrix decomposition for parameter efficiency. He et al. [23] go further to identify all the above three approaches from a unified perspective. In addition, NOAH [26] and GLoRA [25] introduce Neural Architecture Search (NAS) to combine different methods. SSF [24] performs a learnable affine transformation on features of the pre-trained model. FacT [10] tensorizes ViT and decomposes the increments into lightweight factors. Although applying PETL methods to pre-trained image restoration models to improve the generalization seems promising, we find that current PETL methods suffer from homogeneous representations when facing different degradations, hindering stable performance across tasks. ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Preliminary ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this work, we aim to adapt pre-trained restoration models to multiple downstream tasks by fine-tuning a small number of parameters. Following existing PETL works, we mainly focus on transformer-based restoration models since transformer has been shown to be suitable for pretraining [33] and there is no CNN-based pre-trained model available. As shown in Fig. 2, a typical pre-trained restoration model [11, 34] usually contains one large transformer body as well as taskspecific heads and tails. Given the pre-assigned task type, the low-quality image $I_{L Q}$ will first go through the corresponding head to get the shallow feature $X_{h e a d}$ . After that, $X_{h e a d}$ is flattened into a 1D sequence on the spatial dimension and is input to the transformer body which contains several stacked transformer blocks with each block containing multiple transformer layers [35]. Finally, a skip connection is adopted followed by the task-specific tail to reconstruct the high-quality image $I_{H Q}$ . During the pre-training stage, gradients from multiple tasks are used to update the shared body as well as the corresponding task-specific head and tail. After pre-training the restoration model, previous common practice fine-tunes all parameters of the pre-trained model for specific downstream tasks, which burdens training and storage due to the per-task model weights. ", "page_idx": 2}, {"type": "image", "img_path": "R7w68Z5iqf/tmp/149a4b0411b310c38a40a30dcf402099a530e3f7c5250d6e4e4955039f99ae4c.jpg", "img_caption": ["Figure 2: An illustration of the proposed AdaptIR. Our AdaptIR is placed parallel to the frozen MLP in one transformer layer and thus can be seamlessly inserted into various transformer-based pre-trained restoration models. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "3.2 Heterogeneous Representation Learning ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To obtain stable performance across multiple restoration tasks, it is crucial to allow the learning of heterogeneous representation for different degradations. To this end, we formalize AdaptIR as a multi-branch MoE structure, where each branch learns representations orthogonal to each other to form the representation bases, and then these bases are adaptively combined to achieve degradation-specific representation. Formally, as shown in Fig. 2(a), since the transformer body flattens the $l$ -th layer feature into a 1D token sequence, we first restore the 2D image structure to obtain $X_{l}\\,\\in\\,\\mathbb{R}^{C\\times H\\times W}$ . After that, we apply the $1\\times1$ convolution with channel reduction rate $\\gamma$ to transfer $X_{l}$ to low-dimension space for parameter efficiency and obtain the intrinsic feature Xlintrin\u2208R\u03b3C \u00d7H\u00d7W . Then three parallel branches are orthogonally designed to learn local spatial, global spatial, and channel bases. Next, bases from these three branches are adaptively ensembled to obtain representation $X_{l}^{a d a p t}$ for specific degradation. Finally, $X_{l}^{a d a p t}$ is added to the output of the frozen MLP to adapt the pre-trained restoration models. Details of the three branches are given below. ", "page_idx": 3}, {"type": "text", "text": "Local Interaction Module. We first introduce the Local Interaction Module (LIM) to model the local spatial representation. As shown in Fig. 2(b), the proposed LIM is implemented by the depth-wise convolution with weight factorization for parameter efficiency. Specifically, given the convolution weight $W\\,\\in\\,\\mathbb{R}^{C_{i n}\\times\\frac{\\Breve{C}_{o u t}}{g r o u p}\\times K\\times K}$ gCroouutp \u00d7K\u00d7K, where Cin, Cout are input and output channel, K is the kernel size and group is the number of convolution groups, we first reshape $W$ into a 2D weight matrix $\\begin{array}{r}{W^{\\prime}\\in\\mathbb{R}^{C_{i n}\\times\\frac{C_{o u t}}{g r o u p}K^{2}}}\\end{array}$ , and then decompose $W^{\\prime}$ into multiplication of two low-rank weight matrices: ", "page_idx": 3}, {"type": "equation", "text": "$$\nW^{\\prime}=U V^{\\top},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $U\\,\\in\\,\\mathbb{R}^{C_{i n}\\times r}$ , $V\\,\\in\\,\\mathbb{R}^{\\frac{C_{o u t}}{g r o u p}K^{2}\\times r}$ gCroouutp K2\u00d7r and r is the rank to trade-off performance and efficiency. Then we reshape $W^{\\prime}$ to the original kernel size and use it to convolve $\\bar{X_{l}^{i n t r i n}}$ to get $X_{l}^{L I M}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\nX_{l}^{L I M}=\\mathrm{Reshape}(W^{\\prime})\\circledast X_{l}^{i n t r i n},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\circledast$ denotes convolution operator, and Reshape $(\\cdot)$ transforms 2D matrices into 4D convolution kernel weights. ", "page_idx": 3}, {"type": "text", "text": "Frequency Affine Module. We then consider modeling global spatial to achieve orthogonal spatial modeling to LIM. A possible solution is to introduce the attention mechanism [35] which has a global receptive field. However, the attention comes at the cost of high complexity, which goes against the principle of parameter efficiency. In this work, we resort to the frequency domain for a solution. Specifically, we apply the Fast Fourier Transform (FFT) on $X_{l}^{i n t r i n}$ to obtain the corresponding ", "page_idx": 3}, {"type": "text", "text": "frequency feature map XlF \u2208C\u03b3C \u00d7H\u00d7(\u230aW2 \u230b+1): ", "page_idx": 4}, {"type": "equation", "text": "$$\nX_{l}^{\\mathcal{F}}(u,v)=\\frac{1}{H W}\\sum_{h=0}^{H-1}\\sum_{w=0}^{W-1}X_{l}^{i n t r i n}(h,w)e^{-2\\pi i(\\frac{u h}{H}+\\frac{v w}{W})},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "As can be seen from Eq.3, a good property of FFT is that each position of the frequency feature map is the weighted sum of all features in the spatial domain. Therefore, performing pixel-wise projection on $X_{l}^{\\mathcal{F}}$ is equivalent to performing a global operator in the spatial domain. ", "page_idx": 4}, {"type": "text", "text": "Motivated by this observation, we propose the Frequency Affine Module (FAM) to take advantage of the inherent global representation in $X_{l}^{\\mathcal{F}}$ (see Fig. 2(c)). Concretely, we perform the affine transformation on amplitude map $M a g_{l}$ and phase map $P h a_{l}$ respectively with depth-separable $1\\times1$ convolution. To ensure numerical stability during the early training stages, we initialize the transformation layers as all-one weights and zero bias. Subsequently, the inverse Fast Fourier Transform (iFFT) is applied to convert the affined feature back to the spatial domain. Finally, another depth-separable $1\\times1$ convolution is used as a scale layer for subsequent feature ensemble. In short, the whole process can be formalized as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{[M a g_{l},P h a_{l}]=\\mathrm{FFT}(X_{l}^{i n t r i n}),}\\\\ &{X_{l}^{F A M}=\\mathrm{Conv}(\\mathrm{iFFT}(\\mathrm{to\\_complex}(\\phi_{1}(M a g_{l}),\\phi_{2}(P h a_{l})))),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\phi_{1}(\\cdot)$ and $\\phi_{2}(\\cdot)$ are the frequency projection function and t $)\\mathrm{{\\underline{{\\complex}}}}(\\cdot,\\cdot)$ converts the magnitude and phase to complex numbers. ", "page_idx": 4}, {"type": "text", "text": "Channel Gating Module. The above LIM and FAM both adopt the depth-separable strategy for parameter efficiency. To allow for another orthogonal representation, we further develop the Channel Gating Module (CGM) for salient channel selection. As shown in Fig. 2(d), we first obtain the spatial weight mask $\\mathcal{M}_{l}\\in\\mathbb{R}^{1\\times H\\times W}$ by employing $1\\times1$ convolution which compresses the channel dimension of $X_{l}^{i n t r i n}$ to 1, followed by the Softmax on the spatial dimension: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{M}_{l}=\\mathrm{Softmax}(\\mathrm{Conv}(X_{l}^{i n t r i n})).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We then apply $\\mathbf{\\mathcal{M}}_{l}$ on each channel of $X_{l}^{i n t r i n}$ to perform spatially weighted summation to obtain the channel vector which will go through a Feed Forward Network (FFN) to generate the channel gating factor $X_{l}^{C G M}\\in\\mathbb{R}^{\\frac{C}{\\gamma}\\times1\\times1}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\nX_{l}^{C G M}=\\mathrm{FFN}(\\sum_{h,w}{\\mathcal{M}}_{l}\\otimes X_{l}^{i n t r i n}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\otimes$ denotes the Hadamard product. ", "page_idx": 4}, {"type": "text", "text": "Adaptive Feature Ensemble. The orthogonal modeling from local spatial, global spatial, and channel can serve as favorable representation bases, and we further introduce the Adaptive Feature Ensemble to learn their combinations to obtain degradation-specific representations. As shown in Fig. 2(e), we use convolution to compress the channel of $X_{l}^{L I M}$ to 1 to extract its spatial details, while applying global average pooling and FFN on $X_{l}^{F A M}$ to preserve the global information. Next, the sigmoid activation is employed to generate dynamic weights for multiplication and produces the adaptive spatial features $X_{l}^{s p a t i a l}$ . After that, we use $X_{l}^{C G M}$ to perform channel selection on $X_{l}^{s p a t i a l}$ to obtain the degradation-specific ensemble feature Xlensem\u2208R\u03b3C \u00d7H\u00d7W . ", "page_idx": 4}, {"type": "text", "text": "At last, a $1\\!\\times\\!1$ convolution is employed to up-dimension the $X_{l}^{e n s e m}$ to generate $X_{l}^{a d a p t}\\in\\mathbb{R}^{C\\times H\\times W}$ . For stability, we use zero to initialize the convolution weights. Then, the Xladaptis added to the output of frozen MLP as residual to adapt pre-trained models to downstream tasks. ", "page_idx": 4}, {"type": "text", "text": "3.3 Parameter Efficient Training ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "During training, we freeze all parameters in the pre-trained model, including task-specific heads and tails as well as the transformer body except for the proposed AdaptIR. A simple $L_{1}$ loss is employed to provide pixel-level supervision: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{p i x}=||I_{H Q}-I_{L Q}||_{1},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $||\\cdot||_{1}$ denotes $L_{1}$ norm. ", "page_idx": 4}, {"type": "text", "text": "4 Experiment ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We first employ single-degradation restoration tasks to assess the performance stability of different PETL methods, including image SR, color image denoising, image deraining, and low-light enhancement. Subsequently, we introduce hybrid degradation to further evaluate the ability to learn heterogeneous representations. In addition, we compare with recent all-in-one methods in both effectiveness and efficiency to demonstrate the advantages of applying PETL for generalized image restoration. Finally, we conduct ablation studies to reveal the working mechanism of the proposed method as well as different design choices. Since evaluating the performance stability requires experiments on multiple single-degradation tasks, due to the page limit, the related experiments can be seen in the Appendix E.1. ", "page_idx": 5}, {"type": "text", "text": "4.1 Experimental Settings ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Datasets. For image SR, we choose DIV2K [36] and Flickr2K [37] as the training set, and we evaluate on Set5 [38], Set14 [39], BSDS100 [40], Urban100 [41], and Manga109 [42]. For color image denoising, training sets consist of DIV2K [36], Flickr2K [37], BSD400 [40], and WED [43], and we have two testing sets: CBSD68 [44] and Urban100 [41]. For image deraining, we evaluate using the Rain100L [15] and Rain100H [15] benchmarks, corresponding to light/heavy rain streaks. For evaluation on hybrid degradation, where one image contains multiple degradation types, we choose two representatives, consisting of low-resolution and noise as well as low-resolution and JPEG artifact compression, and we add noise or apply JEPG compression on the low-resolution images to synthesize second-order degraded images. For low-light image enhancement, we utilize the training and testing set of LOLv1 [45]. ", "page_idx": 5}, {"type": "text", "text": "Evaluation Metrics. We use the PSNR and SSIM to evaluate the effectiveness. The PSNR/SSIM of image SR, deraining, and second-order degradation are computed on the Y channel from the YCbCr space, and we evaluate the RGB channel for denoising and low-light image enhancement. Moreover, we use trainable #param to measure efficiency. ", "page_idx": 5}, {"type": "text", "text": "Baseline Setup. This work focuses on transferring pre-trained restoration models to downstream tasks under low parameter budgets. Since there is little work studying PETL on image restoration, we reproduce existing PETL approaches and compare them with the proposed AdaptIR. Specifically, we include the following representative PETL methods: i) VPT [9], where the learnable prompts are inserted as the input token of transformer layers, and we compare VPTDeep [9] in experiments because of its better performance. ii) Adapter [17], which introduces bottleneck structure placed after Attention and MLP. iii) LoRA [21], which adds parallel sub-networks to learn low-rank incremental matrices of query and value. iv) AdaptFormer [7], which inserts a tunable module parallel to MLP. v) SSF [24], where learnable scale and shift factors are used to modulate the frozen features. vi) FacT [10], which tensorises a ViT and then decomposes the incremental weights. We also present results of vii) full fine-tuning (Full-ft), and viii) directly applying pre-trained models to downstream tasks (Pretrain), to provide more insights. For readers unfamiliar with PETL, we have also provided a basic background introduction in Appendix A. ", "page_idx": 5}, {"type": "text", "text": "Implementation Details. We use two pre-trained transformer-based restoration models, i.e., IPT [11] and EDT [34], as the base models to evaluate different PETL methods. We control tunable parameters by adjusting channel reduction rate $\\gamma$ . We use AdamW [46] as the optimizer and train for 500 epochs. The learning rate is initialized to 1e-4 and decayed by half at {250,400,450,475} epochs. All experiments are conducted on four NVIDIA 3080Ti GPUs. ", "page_idx": 5}, {"type": "text", "text": "4.2 Comparison on Hybrid Degradation Tasks ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In order to obtain convincing evaluation results, it is tedious and time-consuming to observe the stability of one particular PETL method on multiple single-degradation tasks. Here, we introduce hybrid-degradation restoration. Since restoring hybrid degraded images requires a heterogeneous representation of the PETL methods, and thus the hybrid degradation is more suitable for evaluation. ", "page_idx": 5}, {"type": "text", "text": "In this work, we consider the second-order degradation as a representative of hybrid degradation. Specifically, we employ two different types of second-order degradations, i.e., the $\\times4$ low-resolution and noise with $\\sigma{=}30$ (denoted as LR4&Noise30) as well as the $\\times4$ low-resolution and JEPG compression with quality factor $\\scriptstyle q=30$ (denoted as LR4&JPEG30). Moreover, we also include the classic ", "page_idx": 5}, {"type": "table", "img_path": "R7w68Z5iqf/tmp/3d3f04310eba0301da7637cc98acbe5df08e2999f8bf584a608a42661d2e0466.jpg", "table_caption": ["Table 1: Quantitative comparison for hybrid-degradation restoration tasks. The best and the second best results are in red and blue. "], "table_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "", "img_caption": ["Figure 3: Visual comparison on hybrid degradation with LR4&Noise30. We provide more visualization in Appendix E.1. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "MoE [47, 48, 49], which also employs the multi-branch structure but the design of each branch is the same, to give the impact of the multi-branch structure on the performance. ", "page_idx": 6}, {"type": "text", "text": "Tab. 1 gives the results. Consistent with the previous analysis in Fig. 1, existing PETL methods suffer severe performance drops on hybrid degradation tasks due to the difficulty of learning heterogeneous representations. Interestingly, even the simple MoE baseline which only uses the multi-branch structure outperforms the current state-of-the-art PETL methods, suggesting that multi-branch structures are promising for heterogeneity across tasks. However, since each branch of the classical MoE employs the same structure, it struggles to capture orthogonal representation bases from different branches. In contrast, our method achieves consistent state-of-the-art performance across all tasks and on all datasets. For example, our AdaptIR outperforms the state-of-the-art PETL method FacT [10] by 1.78dB on Urban100 with LR4&Noise30, and 0.28dB on Manga109 with LR4&JEPG30. By orthogonally designing branches to obtain representation bases and then adaptively combining them, our AdaptIR allows for heterogeneous representations across different tasks. We also give several visual results in Fig. 3, and our AdaptIR can well handle complex degradation. ", "page_idx": 6}, {"type": "text", "text": "4.3 Comparison with All-in-One Methods ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Recently, all-in-one image restoration methods [5, 4], which learn a single restoration model for various degradations, have shown to be a promising paradigm in achieving generalized image restoration. Here, we compare our AdaptIR with these methods on both single-task and multi-task setups in Tab. 2. For the single-task setting, our method achieves better PSNR results, e.g. 0.31dB higher than PromptIR on denoising $\\scriptstyle\\sigma=50$ . In addition, the performance advantage of our AdaptIR still preserves the multi-task setup. For instance, our AdaptIR outperforms PromptIR by even $4.9\\mathrm{dB}$ PSNR and 0.016 SSIM on light rain streak removal. This is because all-in-one methods need to learn multiple degradation restoration within one model, resulting in learning difficulties, and the problem of negative transfer among different tasks [50] can also lead to performance degradation. By contrast, the heterogeneous representation from the orthogonal design facilitates the stable performance of our ", "page_idx": 6}, {"type": "text", "text": "Table 2: Comparison with all-in-one image restoration methods under single-task setting. The \u2018training time\u2019 of AdaptIR refers to the downstream fine-tuning time excluding the pre-training stage. ", "page_idx": 7}, {"type": "table", "img_path": "R7w68Z5iqf/tmp/4e68a0229544a2119abba538f852e4de428567a34dfb8a5d84cf8d8e6e298bf8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "R7w68Z5iqf/tmp/a99aa781e3e6ddd5cf6aa82d86e0d3d4b70a7d859f7e973be26d3e9578f81268.jpg", "table_caption": ["Table 3: Comparison with all-in-one image restoration methods under multi-task setting. "], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "R7w68Z5iqf/tmp/9b2796aa698e96ea8986394532c4a544404845f6920c2c88e85d045f2c927132.jpg", "img_caption": ["Figure 4: Fouriur analysis on outputs from LIM and FAM. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "R7w68Z5iqf/tmp/1b7ea5b3b029e45796e774f993468d97513a6d720c3fa84550cac89aaa4dae8d.jpg", "img_caption": ["AdaptIR across different degradations. As for efficiency, our AdaptIR only trains $0.7\\%$ parameters than that of PromptIR with a fast fine-tuning process. We provide a detailed summarization and discussion about the existing multi-task restoration paradigm in Appendix C. ", "Figure 5: Channel activation visualization on outputs from CGM. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.4 Discussion ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Why does the Proposed Methods Work? Our proposed AdaptIR adopts the heterogeneous MoE structure to allow diverse representation learning. Here, we delve deep to verify whether the model design can indeed influence the learned features. For LIM and FAM, we visualize the frequency characteristics of their outputs in Fig. 4. It can be seen that LIM\u2019s relative log amplitude at $\\pi$ is 11.02 higher than FAM, suggesting it has learned to capture high-frequency local textures. Meanwhile, more than $95\\%$ of energy is centralized within $0.05\\pi$ for FAM, indicating it can well model low-frequency global structure. For CGM, we visualize the channel activation in Fig. 5, and find large activation differences across channels, with a large variance of 96.10, indicating that the CGM learns to select degradation-specific channels. ", "page_idx": 7}, {"type": "text", "text": "Scaling Trainable Parameters. We compare the performance of different PETL methods under varying parameter budgets. We use the hybrid degradation LR4&Noise30 in this setup. Fig. 6 shows the results. It can be seen that the proposed method surpasses other strong baselines across various parameter settings, demonstrating the strong scalability of the proposed method. ", "page_idx": 7}, {"type": "text", "text": "How About the Performance on Other Pre-trained Models? The above experiments employ IPT [11] as the base model. In order to verify the generalization of the proposed method, we further adopt another pre-trained image restoration model EDT [34] as the frozen base model. Tab. 4 represents the results. It can be seen that the proposed method maintains state-of-the-art performance by tuning only $1.5\\%$ parameters. More experiments with EDT can be seen in Appendix E.2. ", "page_idx": 7}, {"type": "text", "text": "4.5 Ablation Study ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Parameter Efficient Designs. In this work, we introduce several techniques to achieve orthogonal representation learning. Here, we ablate to study the impact of these choices. The results, presented in Tab. 5, indicate that (1) the adaptive feature ensemble can assemble representations according to specific degradation, without which will cause a performance drop. In addition, $(2)\\&(3)$ removing the ", "page_idx": 7}, {"type": "text", "text": "Figure 6: Scalability comparison with different PETL methods. ", "page_idx": 8}, {"type": "table", "img_path": "R7w68Z5iqf/tmp/27f935633e95812698f304b28d180cd06af2847ade883675150a8c22c3d122c1.jpg", "table_caption": ["Table 4: Comparison on generalization ability with more pretrained base model. "], "table_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "R7w68Z5iqf/tmp/058c709dd12cd781944da195830292eace7e2c95a1fc6cee27dec83e09fe7970.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "R7w68Z5iqf/tmp/d63b790966b8279eded305f909e66cdd83381ac1b93853e79a3b73b1ed3a563f.jpg", "table_caption": ["Table 5: Ablation of different design choices on PSNR(dB). \u2018Baseline\u2019 refers to the setting of depthseparable projection in LIM and FAM, as well as the channel-spatial orthogonal modeling. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "R7w68Z5iqf/tmp/630556ee42b63fc7233989f29ff258e881c0c8e802783103156691bf9ac410b8.jpg", "table_caption": ["Table 6: Ablation experiments of different components on PSNR(dB). "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "R7w68Z5iqf/tmp/9d4cd2e5e7de6cc6f10cc6260ac1330d46a91e4ad1d63bf3e3eb37cf3183b49b.jpg", "table_caption": ["Table 7: Ablation for different insertion positions and forms on PSNR(dB). "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "depth-separable design in LIM or FAM will confilct with the channel modeling in CGM, and lead to sub-optimal results. Further, (4) removing the CGM branch while allowing full-channel interaction in other branches results in poor performance, which we attribute to the learning difficulty of modeling channel and spatial simultaneously. ", "page_idx": 8}, {"type": "text", "text": "Ablation for Components. In the proposed AdaptIR, three parallel branches are developed to learn orthogonal bases. We ablate to discern the roles of different branches. As shown in Tab. 6, separate utilization of one or two branches only yields sub-optimal results owing to the insufficient representation. And the combination of the three branches achieves the best results. ", "page_idx": 8}, {"type": "text", "text": "Insertion Position and Form. There are various options for both the insertion location and form of our AdapIR. The impact of these choices is shown in Tab. 7. It can be seen that inserting AdaptIR into MLP achieves better performance under both parallel and sequential forms. This is because there is a certain dependency between the well-trained MLP and attention, and insertion into the middle of them will damage this relationship. Moreover, the parallel insertion form performs better than its sequential counterpart. We argue that parallel form can preserve the knowledge of frozen features through summation, thus reducing the learning difficulty. ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this work, we explore for the first time the potential of parameter-efficient adaptation to improve the generalization of image restoration models. We observe that current PETL methods struggle to generalize to multiple single-degradation tasks and suffer from performance degradation on hybrid-degradation tasks. We identify that this issue arises from the misalignment between the degradation-required representation and the homogeneity in current PETL methods. Based on this observation, we propose AdaptIR, a heterogeneous Mixture-of-Experts (MoE) to learn local spatial, global spatial, and channel orthogonal bases under low parameter budgets, followed by the adaptive feature ensemble to dynamically fuse these bases for degradation-specific representation. Extensive experiments validate our AdaptIR as a versatile and powerful adaptation solution. ", "page_idx": 8}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work is supported in part by the National Natural Science Foundation of China, under Grant (62302309,62171248), Shenzhen Science and Technology Program (JCYJ20220818101014030, JCYJ20220818101012025), and the PCNL KEY project (PCL2023AS6-1). ", "page_idx": 9}, {"type": "text", "text": "Bibliography ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang. Learning a deep convolutional network for image super-resolution. In ECCV, pages 184\u2013199. Springer, 2014. 1   \n[2] Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang. Beyond a gaussian denoiser: Residual learning of deep CNN for image denoising. IEEE TIP, 26(7):3142\u20133155, 2017. 1, 3   \n[3] Dongwei Ren, Wangmeng Zuo, Qinghua Hu, Pengfei Zhu, and Deyu Meng. Progressive image deraining networks: A better and simpler baseline. In CVPR, pages 3937\u20133946, 2019. 1   \n[4] Boyun Li, Xiao Liu, Peng Hu, Zhongqin Wu, Jiancheng Lv, and Xi Peng. All-in-one image restoration for unknown corruption. In CVPR, pages 17452\u201317462, 2022. 1, 3, 7, 8, 14   \n[5] Vaishnav Potlapalli, Syed Waqas Zamir, Salman Khan, and Fahad Shahbaz Khan. Promptir: Prompting for all-in-one blind image restoration. arXiv preprint arXiv:2306.13090, 2023. 1, 3, 7, 8, 13, 14   \n[6] Dongwon Park, Byung Hyun Lee, and Se Young Chun. All-in-one image restoration for unknown degradations using adaptive discriminative filters for specific degradations. In CVPR, pages 5815\u20135824. IEEE, 2023. 1, 2, 3   \n[7] Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang, Yibing Song, Jue Wang, and Ping Luo. AdaptFormer: Adapting vision transformers for scalable visual recognition. NeurIPS, 35:16664\u201316678, 2022. 1, 3, 6, 7, 9, 13, 15, 16   \n[8] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691, 2021. 1, 2, 3, 7, 18   \n[9] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Visual prompt tuning. In ECCV, pages 709\u2013727. Springer, 2022. 1, 3, 6, 7, 9, 13, 15, 16   \n[10] Shibo Jie and Zhi-Hong Deng. FacT: Factor-tuning for lightweight adaptation on vision transformer. In AAAI, volume 37, pages 1060\u20131068, 2023. 1, 2, 3, 6, 7, 9, 13, 15, 16, 17, 18   \n[11] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma, Chunjing Xu, Chao Xu, and Wen Gao. Pre-trained image processing transformer. In CVPR, pages 12299\u201312310, 2021. 2, 3, 6, 8, 15, 16   \n[12] Namuk Park and Songkuk Kim. How do vision transformers work? In ICLR, 2021. 2   \n[13] Kai Zhang, Wangmeng Zuo, Shuhang Gu, and Lei Zhang. Learning deep CNN denoiser prior for image restoration. In CVPR, pages 3929\u20133938, 2017. 3   \n[14] Tao Dai, Jianrui Cai, Yongbing Zhang, Shu-Tao Xia, and Lei Zhang. Second-order attention network for single image super-resolution. In CVPR, pages 11065\u201311074, 2019. 3, 15   \n[15] Wenhan Yang, Robby T Tan, Jiashi Feng, Zongming Guo, Shuicheng Yan, and Jiaying Liu. Joint rain detection and removal from a single image with contextualized deep networks. IEEE TPAMI, 42(6):1377\u2013 1393, 2019. 3, 6, 19   \n[16] Hang Guo, Jinmin Li, Tao Dai, Zhihao Ouyang, Xudong Ren, and Shu-Tao Xia. MambaIR: A simple baseline for image restoration with state-space model. arXiv preprint arXiv:2402.15648, 2024. 3   \n[17] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for NLP. pages 2790\u20132799. PMLR, 2019. 3, 6, 9, 13, 15, 16   \n[18] Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. BitFit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. arXiv preprint arXiv:2106.10199, 2021. 3   \n[19] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys, 55(9):1\u201335, 2023. 3   \n[20] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190, 2021. 3   \n[21] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 3, 6, 7, 9, 13, 15, 16, 17, 18   \n[22] Rabeeh Karimi Mahabadi, James Henderson, and Sebastian Ruder. Compacter: Efficient low-rank hypercomplex adapter layers. NeurIPS, 34:1022\u20131035, 2021. 3   \n[23] Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards a unified view of parameter-efficient transfer learning. arXiv preprint arXiv:2110.04366, 2021. 3   \n[24] Dongze Lian, Daquan Zhou, Jiashi Feng, and Xinchao Wang. Scaling & shifting your features: A new baseline for efficient model tuning. NeurIPS, 35:109\u2013123, 2022. 3, 6, 7, 9, 13, 15, 16   \n[25] Arnav Chavan, Zhuang Liu, Deepak Gupta, Eric Xing, and Zhiqiang Shen. One-for-All: Generalized LoRA for parameter-efficient fine-tuning. arXiv preprint arXiv:2306.07967, 2023. 3   \n[26] Yuanhan Zhang, Kaiyang Zhou, and Ziwei Liu. Neural prompt search. arXiv preprint arXiv:2206.04673, 2022. 3   \n[27] Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, and Yu Qiao. Vision transformer adapter for dense predictions. arXiv preprint arXiv:2205.08534, 2022. 3   \n[28] Yen-Cheng Liu, Chih-Yao Ma, Junjiao Tian, Zijian He, and Zsolt Kira. Polyhistor: Parameter-efficient multi-task adaptation for dense vision tasks. NeurIPS, 35:36889\u201336901, 2022. 3   \n[29] Shibo Jie and Zhi-Hong Deng. Convolutional bypasses are better vision transformer adapters. arXiv preprint arXiv:2207.07039, 2022. 3   \n[30] Shibo Jie, Haoqing Wang, and Zhi-Hong Deng. Revisiting the parameter efficiency of adapters from the perspective of precision redundancy. In ICCV, pages 17217\u201317226, 2023. 3   \n[31] Taolin Zhang, Jinpeng Wang, Hang Guo, Tao Dai, Bin Chen, and Shu-Tao Xia. BoostAdapter: Improving test-time adaptation via regional bootstrapping. arXiv preprint arXiv:2410.15430, 2024. 3   \n[32] Gen Luo, Minglang Huang, Yiyi Zhou, Xiaoshuai Sun, Guannan Jiang, Zhiyu Wang, and Rongrong Ji. Towards efficient visual adaption via structural re-parameterization. arXiv preprint arXiv:2302.08106, 2023. 3   \n[33] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 3   \n[34] Wenbo Li, Xin Lu, Shengju Qian, Jiangbo Lu, Xiangyu Zhang, and Jiaya Jia. On efficient transformer-based image pre-training for low-level vision. arXiv preprint arXiv:2112.10175, 2021. 3, 6, 8, 15, 16   \n[35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. NeurIPS, 30, 2017. 3, 4, 13   \n[36] Eirikur Agustsson and Radu Timofte. NTIRE 2017 challenge on single image super-resolution: Dataset and study. In CVPRW, pages 126\u2013135, 2017. 6   \n[37] Radu Timofte, Eirikur Agustsson, Luc Van Gool, Ming-Hsuan Yang, and Lei Zhang. NTIRE 2017 challenge on single image super-resolution: Methods and results. In CVPRW, pages 114\u2013125, 2017. 6   \n[38] Marco Bevilacqua, Aline Roumy, Christine Guillemot, and Marie Line Alberi-Morel. Low-complexity single-image super-resolution based on nonnegative neighbor embedding. 2012. 6   \n[39] Roman Zeyde, Michael Elad, and Matan Protter. On single image scale-up using sparse-representations. In Curves and Surfaces: 7th International Conference, Avignon, France, June 24-30, 2010, Revised Selected Papers 7, pages 711\u2013730. Springer, 2012. 6   \n[40] Pablo Arbelaez, Michael Maire, Charless Fowlkes, and Jitendra Malik. Contour detection and hierarchical image segmentation. IEEE TPAMI, 33(5):898\u2013916, 2010. 6   \n[41] Jia-Bin Huang, Abhishek Singh, and Narendra Ahuja. Single image super-resolution from transformed self-exemplars. In CVPR, pages 5197\u20135206, 2015. 6   \n[42] Yusuke Matsui, Kota Ito, Yuji Aramaki, Azuma Fujimoto, Toru Ogawa, Toshihiko Yamasaki, and Kiyoharu Aizawa. Sketch-based manga retrieval using Manga109 dataset. Multimedia Tools and Applications, 76:21811\u201321838, 2017. 6   \n[43] Kede Ma, Zhengfang Duanmu, Qingbo Wu, Zhou Wang, Hongwei Yong, Hongliang Li, and Lei Zhang. Waterloo exploration database: New challenges for image quality assessment models. IEEE TIP, 26(2):1004\u2013 1016, 2016. 6   \n[44] David Martin, Charless Fowlkes, Doron Tal, and Jitendra Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In ICCV, volume 2, pages 416\u2013423. IEEE, 2001. 6   \n[45] Chen Wei, Wenjing Wang, Wenhan Yang, and Jiaying Liu. Deep retinex decomposition for low-light enhancement. arXiv preprint arXiv:1808.04560, 2018. 6, 19   \n[46] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 6, 16   \n[47] Sneha Kudugunta, Yanping Huang, Ankur Bapna, Maxim Krikun, Dmitry Lepikhin, Minh-Thang Luong, and Orhan Firat. Beyond distillation: Task-level mixture-of-experts for efficient inference. arXiv preprint arXiv:2110.03742, 2021. 7   \n[48] Jinguo Zhu, Xizhou Zhu, Wenhai Wang, Xiaohua Wang, Hongsheng Li, Xiaogang Wang, and Jifeng Dai. Uni-perceiver-moe: Learning sparse generalist models with conditional moes. NeurIPS, 35:2664\u20132678, 2022. 7   \n[49] Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, Andr\u00e9 Susano Pinto, Daniel Keysers, and Neil Houlsby. Scaling vision with sparse mixture of experts. NeurIPS, 34:8583\u20138595, 2021. 7   \n[50] Wenlong Zhang, Xiaohui Li, Guangyuan Shi, Xiangyu Chen, Yu Qiao, Xiaoyun Zhang, Xiao-Ming Wu, and Chao Dong. Real-world image super-resolution as multi-task learning. NeurIPS, 36, 2024. 7   \n[51] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 13   \n[52] Yihao Liu, Xiangyu Chen, Xianzheng Ma, Xintao Wang, Jiantao Zhou, Yu Qiao, and Chao Dong. Unifying image processing as visual prompting question answering. arXiv preprint arXiv:2310.10513, 2023. 13, 14   \n[53] Jiaqi Ma, Tianheng Cheng, Guoli Wang, Qian Zhang, Xinggang Wang, and Lefei Zhang. ProRes: Exploring degradation-aware visual prompt for universal image restoration. arXiv preprint arXiv:2306.13653, 2023. 13, 14   \n[54] Cong Wang, Jinshan Pan, Wei Wang, Jiangxin Dong, Mengzhu Wang, Yakun Ju, and Junyang Chen. Promptrestorer: A prompting image restoration method with degradation perception. NeurIPS, 36:8898\u2013 8912, 2023. 13, 14   \n[55] Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng Zhong, and Yun Fu. Image super-resolution using very deep residual channel attention networks. In ECCV, pages 286\u2013301, 2018. 15   \n[56] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu Timofte. SwinIR: Image restoration using swin transformer. In ICCV, pages 1833\u20131844, 2021. 15   \n[57] Kai Zhang, Wangmeng Zuo, and Lei Zhang. Ffdnet: Toward a fast and flexible solution for cnn-based image denoising. IEEE TIP, 27(9):4608\u20134622, 2018. 15   \n[58] Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, and Yun Fu. Residual dense network for image super-resolution. In CVPR, pages 2472\u20132481, 2018. 15   \n[59] Zhendong Wang, Xiaodong Cun, Jianmin Bao, Wengang Zhou, Jianzhuang Liu, and Houqiang Li. Uformer: A general u-shaped transformer for image restoration. In CVPR, pages 17683\u201317693, 2022. 16   \n[60] Wei Chen, Wang Wenjing, Yang Wenhan, and Liu Jiaying. Deep retinex decomposition for low-light enhancement. In BMVC, 2018. 16   \n[61] Ke Xu, Xin Yang, Baocai Yin, and Rynson WH Lau. Learning to restore low-light images via decomposition-and-enhancement. In CVPR, pages 2281\u20132290, 2020. 16 ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A Basic Background of Parameter Efficient Transfer Learning ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Since very little work has been done to study PETL in low-level vision, we re-implement the current state-of-the-art PETL methods in this work, such as VPT [9], Adapter [17], LoRA [21], AdaptFormer [7], SSF [24], and FacT [10]. In this part, we review these methods and provide detailed implementation details for reproduction. Fig. 7 gives an illustration of these baseline methods. ", "page_idx": 12}, {"type": "image", "img_path": "R7w68Z5iqf/tmp/baf9835a7ac921c3e6baea2bbc1f24e620f52c9fe1b930684c86132a9cadcd60.jpg", "img_caption": ["Figure 7: Illustrations of exsiting state-of-the-art PETL baselines. "], "img_footnote": [], "page_idx": 12}, {"type": "text", "text": "\u2022 VPT [9], shown in Fig. 7(a), prepends learnable prompt tokens in the input of one transformer layer [35]. In [9], there are two versions of VPT, i.e., $\\mathrm{VPT}_{\\mathrm{Shallow}}$ and $\\mathrm{VPT_{Deep}}$ . To obtain a good performance, we use $\\mathrm{VPT_{Deep}}$ which inserts new prompts at each transformer layer, as our default settings.   \n\u2022 Adapter [17], shown in Fig. 7(b), is a bottleneck structure with an intermediate GELU activation function. Following the vanilla Adapter design [17], we insert the Adapter both after the Multi-head Self-Attention (MSA) and Multi-Layer Perceptron (MLP).   \n\u2022 LoRA [21], shown in Fig. 7(c) use the multiplication of two low-rank to approximate the incremental matrices in projection layers of Query and Value.   \n\u2022 AdaptFormer [7], shown in Fig. 7(d), is similar in model architecture with Adapter, but has different insert position and form. In [7], the AdaptFormer is placed before the second LayerNorm layer and adopts a parallel insertion form.   \n\u2022 SSF [24], shown in Fig. 7(e), utilize learnable scale and shift factors to modulate frozen features. Based on the settings in [24], we place the SSF layer behind all the attention QKV projection, the LayerNorm, and the MLP layers.   \n\u2022 FacT [10], shown in Fig. 7(f), tensorises Vision Transformer [51] and introduces a low-rank approximation to the incremental matrix similar to LoRA. Different from LoRA, FacT sets the up-projection and down-projection to be shared across layers while setting the projection in the low-rank space to be layer-specific. There are two versions of FacT, namely FacTTT and $\\mathrm{FacT_{TK}}$ [10], we use $\\mathrm{FacT_{TT}}$ in this work because of its good performance. Following [10], we introduce the FacT layer in attention QKV projection as well as MLP layers. ", "page_idx": 12}, {"type": "text", "text": "B Discussion with Prompt-based Methods ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In this section, we briefly discuss the difference between our AdaptIR with other prompt-based methods. To the best of our knowledge, only the PromtGIP [52] shows the zero-shot ability when facing unseen degradations. And the other ProRes [53], PromptIR [5], and PromptRestorer [54] can only handle degradations which have been seen during training, which means they still need additional fine-tuning for task generalization. The advantages of our AdaptIR compared to these prompt-based approaches are two-fold. As for efficiency, PromptIR, ProRes, and PromptRestorer all need full fine-tuning for adapting to new tasks, e.g., PromptIR needs 7-day $8\\!\\times\\!3090$ GPUs for full fine-tuning, while our AdaptIR needs only 8h $1\\!\\times\\!3090$ GPU. As for performance, since these methods need to learn multiple degradations within one model, it is inevitable to suffer the problem of negative transfer, which impairs performance. We give a thorough comparison in Tab. 8. ", "page_idx": 12}, {"type": "table", "img_path": "R7w68Z5iqf/tmp/f758097450c701cf90d5df9658d5906e152f426fa31cfeb923946c20b1a5b438.jpg", "table_caption": ["Table 8: Comparison with prompt-based restoration methods. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "C Discussion on Multi-task Restoration Paradigms. ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we revisit existing paradigms for dealing with multi-task restoration problems, in which multiple degradations need to be handled. Let denote the number of degradation types, i.e., downstream tasks as $N$ , then we can summarize existing paradigm into the following three categories: ", "page_idx": 13}, {"type": "text", "text": "1. \u201cN for $\\mathbf{N}^{\\bullet}$ : training N task-specific models for N downstream tasks, such as Restormer, MPRNet.   \n2. \u201c1 for $\\mathbf{N}^{\\bullet}$ : training 1 all-in-one model for N downstream tasks, such as PromptIR [5], AirNet [4].   \n3. \u201c $(1+\\mathrm{N})$ for $\\mathbf{N}^{\\bullet\\bullet}\\colon$ using 1 task-shared pre-trained weights, and N task-specific lightweight modules. ", "page_idx": 13}, {"type": "text", "text": "Early image restoration techniques predominantly employed the first strategy, which trains $N$ different models to handle multiple degradations. Although this strategy can handle multiple degradations, it usually requires training and storing $N$ copies for each task. The recent all-in-one methods train one model for multiple degradations, although reducing the model copy to one to improve the efficiency, this approach usually suffers from performance degradation due to the multi-task learning difficulties and the negative transfer learning problem. In this work, the proposed AdaptIR is the first paradigm categorized in the third category, which trains a shared pre-trained backbone as well as $N$ task-specific lightweight modules. This paradigm can be seen as a compromise between effectiveness and efficiency. However, given that the task-specific modules are very lightweight, we believe that the advantages of this paradigm outweigh the disadvantages. ", "page_idx": 13}, {"type": "text", "text": "D Further Explanation of the Heterogeneous Representation. ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this work, we pay attention to the learning of the heterogeneous representation. Here, we articulate it to make it more clear about this term. The heterogeneous representation in this paper represents the learning of discriminative features across different degradation types. The term representation here is instantiated as the Fourier curve in Fig. 1 in the main paper. Previous approaches tend to produce similar representations across various degradations. As common knowledge, restoring different degradations requires different representations, e.g., SR needs a high-pass filter network while denoising needs low-pass. As a result, if the representation needed by current degradation matches the specific representation of the existing PETL method, it works. If not, it leads to unstable performance. To demonstrate the generality of the problem regarding the unstable performance and the homogeneous representation under different degradations, we provide more evidence in Fig. 9 and Fig. 10. ", "page_idx": 13}, {"type": "table", "img_path": "R7w68Z5iqf/tmp/4a4099baf27b2be6284a20d6c69efca3a231062a8f65d3b284c55d5488a14c81.jpg", "table_caption": ["Table 9: Quantitative comparison for $\\times4$ image SR on PSNR(dB). We compare the #param when the performance is the same. "], "table_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "R7w68Z5iqf/tmp/48840c115f9ec4435cee1e78fb8c82a81f3743ac7c85b9c5172b570148a910aa.jpg", "table_caption": ["Table 10: Quantitative comparison for color image denoising on PSNR(dB). We compare the #param when PSNR is the same. "], "table_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "R7w68Z5iqf/tmp/0102f3e9e0a61602d8408c3f27da6da295cf1e910aaacafa8aa27318d6b90d81.jpg", "table_caption": ["Table 11: Quantitative comparison for light rain streak removal on Rain100L dataset. "], "table_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "R7w68Z5iqf/tmp/e742a6557c4018b3bfeb8b0505c86456ccdf8179b8a10635e453c4b744d02d73.jpg", "table_caption": ["Table 12: Quantitative comparison for heavy rain streak removal on Rain100H dataset. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "E More Experiment Results ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "E.1 Comparison on Single-degradation Tasks ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Current PETL methods struggle to achieve stable performance due to homogeneous frequency characteristics and suffer performance degradation when not well aligned with the frequencies required by a specific degradation. Therefore, it needs multiple single-degradation tasks to obtain convincing evaluation results. Here, we give results of single-degradation tasks, including image super-resolution in Tab. 9, color image denoising in Tab. 10, light deraining in Tab. 11, heavy deraining in Tab. 12, and low-light enhancement in Tab. 13. It can be seen that our method maintains a stable best performance on most single-degradation tasks, with the second-best method varying across tasks. For example, the recent state-of-the-art methods FacT [10] obtains comparable performance with our AdaptIR, however, it suffers significant performance degradation on the subsequent light and heavy rain streak removal tasks. Another example can also be seen in LoRA [21], which performs the second best in heavy deraining but struggles with low-light image enhancement tasks. In contrast, our method is more stable, achieving consistent best performance across these single-degradation tasks. We also provide quantitative comparisons on the single-degradation restoration tasks in Fig. 12 and Fig. 13. ", "page_idx": 14}, {"type": "text", "text": "E.2 Additional Results on Hybrid Degradation with EDT ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In order to demonstrate the generalizability of our AdaptIR, we choose IPT [11] and EDT [34] as pre-trained base models to evaluate the performance of different PETL methods. Due to the page limit, we mainly present the results of IPT in the main paper. Here, we give more experimental results with EDT. The results on second-order degradation LR4&JPEG30 with EDT are shown in Tab. 14. It can be seen that our method continues to achieve state-of-the-art performance by a significant margin. For example, our AdaptIR outperforms the second-best method FacT [10] by up to 0.15dB PSNR while using fewer tunable parameters. The results with EDT as the base model demonstrate the robustness of the proposed method. ", "page_idx": 14}, {"type": "table", "img_path": "R7w68Z5iqf/tmp/3cd9b5a2d34746c6b9b08744a7ef3e3d100282f62f3988442f2f6c6903ad6c52.jpg", "table_caption": ["Table 13: Quantitative comparison for low-light image enhancement with LOLv1 dataset. "], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "R7w68Z5iqf/tmp/ac6f74df0f44139e5e623b4e53cb300e3a35c571dc16de85d82f7cad46b26bc9.jpg", "table_caption": ["Table 14: Quantitative comparison for second-order degradation with LR4&JPEG30 using EDT as pre-trained restoration models. The best results are bolded. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "E.3 Results on More Degradations. ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we further include another challenging degradation type, namely the real image denoising which is unseen during the pre-training phase and is the realworld degradation type, to further demonstrate the generalization of the proposed AdaptIR. We use the training and testing sets in the SIDD for this experiment. The experimental results are shown in Tab. 15. It can be seen that our AdaptIR maintains its superiority when transferring to real-world degradation. For instance, our method outperforms LoRA by 0.13dB PSNR. The above experimental results demonstrate the robustness of our methods. ", "page_idx": 15}, {"type": "table", "img_path": "R7w68Z5iqf/tmp/273348bae0427c91a6c317d342affb6d37708b624136d04a5c1ea1e1beffc603.jpg", "table_caption": ["Table 15: Results on real-world denoising tasks with SIDD datasets. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "F Complexity Analysis ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we theoretically analyze the parameter complexity of the proposed method. We omit the bias term as the corresponding parameter is small. Assume that the hidden dimension of the pre-trained restoration model [11, 34] is $d$ and the dimension of intrinsic space in AdaptIR is $d^{\\prime}$ . For the dimensional up and down operations, the number of parameters is $2d d^{\\prime}$ . For the Local Interaction Module, assuming the convolution kernel size is $K$ and the pre-defined rank of $U,V$ is $r$ , then the number of parameters of LIM with depth-separable design is $d^{\\prime}r+r K^{2}$ . For the Frequency Affine Module, the total parameters of the amplitude and phase projection are $2d^{\\prime}$ . For the Channel Gating Module, which contains the channel compression as well as the FFN, the number of parameters is $\\ d^{\\prime}+2d^{\\prime}{\\frac{d^{\\prime}}{a}}$ . As for the Adaptive Feature Ensemble, the compression convolution costs $d^{\\prime}$ number of parameters, and $2d^{\\prime}\\frac{d^{\\prime}}{b}$ is used in pooling FFN. Summing up the above terms gives 2(aab+b)d\u20322 + (r + 4 + 2d)d\u2032 + rK2. In the implementation, we set r = d\u2032/2, a = 2,b = 8, K = 3 and $\\begin{array}{r}{d^{\\prime}=\\frac{d}{\\gamma}}\\end{array}$ . Therefore, the total parameter complexity of AdaptIR is $\\begin{array}{r}{(\\frac{2}{\\gamma}+\\frac{7}{4\\gamma^{2}})d^{2}+\\frac{17}{2\\gamma}d\\sim\\mathcal{O}(\\frac{d^{2}}{\\gamma})}\\end{array}$ . ", "page_idx": 15}, {"type": "text", "text": "G Feature Response Intensity Analysis. ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "To make it more clear how the proposed multi-branch structure works, we give the distribution of feature response intensity of three branches across various tasks, including SR, heavy deraining, light deraining, low-light image enhancement, and two hybrid degradations in Fig. 11. These figures indicate that our AdaptIR can adjust to different degradation types by enhancing or suppressing the outputs from different branches. Specifically, for the heavy&light deraining tasks, AdaptIR adaptively learns to enhance the low-frequency global features, i.e., the frequency affine module which is responsible for global spatial modeling has large values. This property ensures the removal of the high-frequency rainstreaks as well as the preservation of the global structure of the image. For SR tasks, AdaptIR adaptively enhances the restoration of local texture details by learning large output values from the local spatial modules. For the hybrid degradation task, AdaptIR shows it can distinguish between different hybrid degradations, i.e., three branches exhibit different patterns under two types of hybrid degradations. In short, each branch of AdaptIR can capture discriminative features under different degradations, indicating that our approach is degradation-aware. This ability guarantees robustness on single degradation and superior performance under hybrid degradation. ", "page_idx": 15}, {"type": "image", "img_path": "R7w68Z5iqf/tmp/52dbb93298d19693f737d99496fa21ef30d8fbe509b1a70a7ccab3222569fa47.jpg", "img_caption": ["Figure 8: The frequency characteristic curves of features from three branches in the classic MoE with hybrid degradation of (a)SR4&DN30 and (b)SR4&JPEG30. (c)&(d) Fourier analysis on more current PETL methods, LoRA [21] and FacT [10], which shows significant representation homogeneity across tasks. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "R7w68Z5iqf/tmp/b39c600eb9b91dcc8159051eecc9526c6b8b63bce43a5eaa8ebe1008e9f77fd3.jpg", "img_caption": ["Figure 9: More evidence on the unstable performance of previous PETL methods across different single-degradation types, and the unfavorable performance under hybrid degradation. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "H Differences from Classic MoE ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Although both our AdapIR and the classic MoE employ the multi-branch structure, however, our approach differs from the classic MoE in the following aspects. Firstly, the classic MoE uses the multi-branch structure to enhance the model capabilities, whereas our proposed heterogeneous MoE aims to capture heterogeneous representations across different restoration tasks. Secondly, despite using the multi-branch structure, the classical MoE still tends to capture homogeneous representations since each branch is the same, thus resulting in the sub-optimal results in Tab. 1. In contrast, each branch in our AdaptIR is designed orthogonally, thus ensuring the learning of orthogonal representation bases. Thirdly, classical MoE uses simple summation to fuse branches, which is degradation-agnostic, while our AdaptIR uses degradation-specific ensemble to learn the combination of orthogonal representation bases, facilitating heterogeneous representation across tasks. ", "page_idx": 16}, {"type": "text", "text": "In Fig. 8, we also give the frequency characteristics of the output features from different branches of the well-trained classical MoE. It can be seen that different branches still suffer from homogeneity despite the use of a multi-branch structure. In contrast, as shown in Fig. 4 in the main paper, our AdaptIR ensures that different branches capture different representations through the proposed orthogonal design, which promotes heterogeneous representations to achieve better performance. ", "page_idx": 16}, {"type": "image", "img_path": "R7w68Z5iqf/tmp/abe25fb6391ceeead835c08490a269d5ba8f9b91c592f13e695be276cd492733.jpg", "img_caption": ["Figure 10: More evidence that shows previous PETL methods struggle to learn distinguishable features across different degradation types, i.e., homogeneous representation. In contrast, our AdaptIR can learn heterogeneous representations for different degradations. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "R7w68Z5iqf/tmp/bdd01180ba8c434b70da3df62790f9f37139b87bd5e16e00071b8640018ef65f.jpg", "img_caption": ["Figure 11: The distribution of feature response densities of the three branches across different tasks. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "I More Evidence of Homogeneous Representation ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In Fig. 1, we give the frequency characteristics of Adapter [8], and find its homogeneous representation when facing different degradations. To demonstrate the prevalence of homogeneous representations in current PETL methods, we provide the frequency characteristics curves of more PETL methods in Fig. 8. It can be seen that the current state-of-the-art PETL methods LoRA [21] and FacT [10] also exhibit homogeneity as Adapter, i.e., the learned feature representations are similar even if they are for different degradations. In contrast, AdapIR utilizes the orthogonal multi-branch design to learn diverse representations, facilitating heterogeneous representations on different restoration tasks. ", "page_idx": 17}, {"type": "text", "text": "J Dataset Description ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this work, we evaluate different PETL methods on diverse image restoration tasks, which cover many training and testing datasets. To make the experimental setup more clear, we give a detailed description of datasets in Tab. 16. ", "page_idx": 17}, {"type": "text", "text": "K Limitations and Future Work ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "While AdaptIR appears as a competitive PETL alternative across various image restoration benchmarks, it can be further improved with task-specific module designs. For example, in the proposed AdaptIR, different tasks share the same structure, however, different restoration tasks have diverse model preferences. An intuitive solution might be to introduce degradation-aware dynamic networks. Moreover, although this work has covered multiple degradation types, some other degradations can also be explored in the future, e.g. blur and haze, to further demonstrate the generalization ability. ", "page_idx": 17}, {"type": "table", "img_path": "R7w68Z5iqf/tmp/5c52342b0e2a922fe90bc972498aae121b4f4d5f22d55b6a01d1fc22bf499b8d.jpg", "table_caption": ["Table 16: Dataset description for various image restoration tasks. "], "table_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "R7w68Z5iqf/tmp/022c3f0d232c483e0f27fd4d0e79f2616f8709b8d246909936d71fb44843f45c.jpg", "img_caption": ["Figure 12: Visual comparison of heavy rain streak removal on samples from Rain100H [15] dataset. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "R7w68Z5iqf/tmp/32bb21f6e2abb909db4a443b0d2bcec4a393f6e7a2252c584a18c9d88337f3e2.jpg", "img_caption": ["Figure 13: Visual comparison of low-light image enhancement on samples from LOLv1 [45] dataset. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "L Broader Impact ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Our AdaptIR holds significant promise for improving the quality and generalization of image restoration across various domains, such as medical imaging, historical document preservation, and digital media restoration. By enabling more accurate and reliable image restoration with reduced computational resources, AdaptIR can facilitate advancements in these fields, leading to better diagnostic tools, preservation of cultural heritage, and enhanced digital media quality. However, the enhanced capabilities of AdaptIR also present potential negative societal impacts, such as the risk of misuse in generating realistic fake images or deepfakes, which could be used for disinformation, creating fake profiles, or unauthorized surveillance, leading to privacy violations, security concerns, and ethical issues. To mitigate these risks, it is crucial to implement measures like gated releases of models, mechanisms for monitoring misuse and ensuring transparency in deployment and training processes, alongside continuous evaluation of the technology\u2019s impact. ", "page_idx": 18}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We have clearly outlined the main contributions of this paper in the abstract and introduction sections. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: We have discussed in detail the limitation of this work in Appendix K. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 19}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: All assumptions in the paper are either derived from the conclusions of previous research or validated through extensive experiments. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 20}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We have provided a detailed description of our algorithm\u2019s process and implementation specifics in the paper, and we will release our code after review. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 20}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 21}, {"type": "text", "text": "Answer: [No] ", "page_idx": 21}, {"type": "text", "text": "Justification: We will release our code after review, but we have already provided a detailed explanation of how to implement our algorithm and the specific implementation details in the paper. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 21}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We have provided detailed experimental details in both the paper and the appendix. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 21}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: We have provided the performance fluctuation under different random seeds in the Appendix. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 21}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 22}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We provided the computational resources we used for experiments in the implementation details section. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 22}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We have checked in every respect that this work conform with the NeurIPS Code of Ethics. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 22}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We have provided detailed discussion on possible both positive and negative societal impact in ??. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 23}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: This paper poses no such risks. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 23}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Yes, the creators and original owners of assets used in this paper are properly credited, and the license and terms of use are explicitly mentioned and respected, as evidenced by thorough citations. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 24}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 24}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 24}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 25}]