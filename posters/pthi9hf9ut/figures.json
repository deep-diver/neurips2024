[{"figure_path": "PThi9hf9UT/figures/figures_5_1.jpg", "caption": "Figure 1: MI estimate obtained with derangement and permutation training procedures, for data dimension d = 20 and batch size N = 128.", "description": "This figure compares the performance of MI estimation using two different training strategies: derangement and permutation.  The x-axis represents the training steps, and the y-axis shows the estimated mutual information in nats.  The solid black line indicates the true mutual information. The dashed black line represents the upper bound of log(N) for the permutation strategy. The green line shows the MI estimates obtained by using the derangement strategy, demonstrating its ability to overcome the upper bound. The shaded green area shows the variance of the estimates during training.", "section": "5 Derangement Strategy"}, {"figure_path": "PThi9hf9UT/figures/figures_7_1.jpg", "caption": "Figure 1: MI estimate obtained with derangement and permutation training procedures, for data dimension d = 20 and batch size N = 128.", "description": "This figure compares the performance of MI estimation using two different training strategies: derangement and permutation. The plots show the MI estimate over training steps for a 20-dimensional dataset with a batch size of 128. The derangement strategy consistently achieves better accuracy and avoids the upper bound limitations observed in the permutation strategy.", "section": "Experimental Results"}, {"figure_path": "PThi9hf9UT/figures/figures_7_2.jpg", "caption": "Figure 6: NWJ, SMILE (t = \u221e), and MINE MI estimation comparison with d = 20 and N = 64. The Gaussian setting is represented in the top row, while the cubic setting is shown in the bottom row.", "description": "This figure compares the performance of three different mutual information (MI) estimation methods: NWJ, SMILE (with t = \u221e), and MINE.  The comparison is done across two different data settings: a Gaussian setting and a cubic setting. Each setting has varying levels of mutual information, visualized as a staircase pattern in the graphs.  The x-axis represents the number of training steps, and the y-axis represents the estimated mutual information. The graphs show that the variance of the estimates increases significantly as the true mutual information increases, especially for MINE and NWJ.", "section": "6 Experimental Results"}, {"figure_path": "PThi9hf9UT/figures/figures_7_3.jpg", "caption": "Figure 6: NWJ, SMILE (t = \u221e), and MINE MI estimation comparison with d = 20 and N = 64. The Gaussian setting is represented in the top row, while the cubic setting is shown in the bottom row.", "description": "This figure compares the performance of three different mutual information estimation methods (NWJ, SMILE, and MINE) under two different data distributions (Gaussian and cubic) for a dimensionality of 20 and batch size of 64. Each row shows the estimation for a specific data distribution across various true MI values. The plot shows the estimated MI on the y-axis and training steps on the x-axis, comparing the performance of each estimator with the true MI.", "section": "6 Experimental Results"}, {"figure_path": "PThi9hf9UT/figures/figures_8_1.jpg", "caption": "Figure 5: Time requirements comparison to complete the 5-step staircase MI. From the left, the first and second behaviors vary over the batch size. The last one varies over the probability distribution dimension.", "description": "This figure shows the time taken for different MI estimation methods to complete a 5-step staircase Mutual Information estimation task.  The three subplots demonstrate how the computation time scales with respect to three different parameters: (a) the batch size N, (b) the batch size N for the deranged and separable architectures, and (c) the dimension of the probability distribution d.  The results highlight the efficiency of the deranged architecture compared to the joint and separable ones, especially when N is large.", "section": "6 Experimental Results"}, {"figure_path": "PThi9hf9UT/figures/figures_21_1.jpg", "caption": "Figure 6: NWJ, SMILE (t = \u221e), and MINE MI estimation comparison with d = 20 and N = 64. The Gaussian setting is represented in the top row, while the cubic setting is shown in the bottom row.", "description": "This figure compares the performance of three established mutual information (MI) estimation methods: NWJ, SMILE (with t = \u221e), and MINE.  The comparison is done across two scenarios: a Gaussian distribution and a cubic transformation of a Gaussian distribution.  Each scenario is represented by two subfigures showing the estimated MI over training steps. The black lines depict the true MI value, highlighting the accuracy and variance of the different estimation methods. The Gaussian scenario exhibits relatively smoother MI estimates compared to the cubic scenario. This difference underscores the challenge in MI estimation for non-linear relationships.", "section": "Experimental Results"}, {"figure_path": "PThi9hf9UT/figures/figures_22_1.jpg", "caption": "Figure 1: MI estimate obtained with derangement and permutation training procedures, for data dimension d = 20 and batch size N = 128.", "description": "This figure compares the performance of the MI estimation using two different training procedures: derangement and permutation.  The derangement strategy ensures that samples from the marginal distribution do not include any samples from the joint distribution, overcoming the upper bound on the MI estimation.  Conversely, the permutation strategy does not make this guarantee. The plot shows the mutual information estimates over training steps, demonstrating the derangement strategy's clear advantage in achieving higher accuracy and avoiding a log(N) upper bound.", "section": "5 Derangement Strategy"}, {"figure_path": "PThi9hf9UT/figures/figures_22_2.jpg", "caption": "Figure 5: Time requirements comparison to complete the 5-step staircase MI. From the left, the first and second behaviors vary over the batch size. The last one varies over the probability distribution dimension.", "description": "This figure shows the time taken to estimate mutual information (MI) using different methods and varying parameters.  The left and center plots demonstrate how the computation time scales with increasing batch size (N) for different neural network architectures (Joint, Separable, Deranged). The right plot illustrates how the computation time changes as the dimension of the probability distribution (d) increases.", "section": "6 Experimental Results"}, {"figure_path": "PThi9hf9UT/figures/figures_23_1.jpg", "caption": "Figure 15: Variance of the f-DIME estimators corresponding to different values of batch size.", "description": "This figure shows how the variance of different f-DIME estimators changes with batch size for various values of mutual information (MI).  The plot displays the variance on a logarithmic scale for each estimator (KL-DIME, HD-DIME, GAN-DIME, SMILE (tau=infty), MINE, and NWJ) across different MI levels (2, 4, 6, 8, 10 nats). It illustrates the variance performance of f-DIME compared to other existing MI estimators.", "section": "6 Experimental Results"}, {"figure_path": "PThi9hf9UT/figures/figures_23_2.jpg", "caption": "Figure 1: MI estimate obtained with derangement and permutation training procedures, for data dimension d = 20 and batch size N = 128.", "description": "This figure compares the performance of the MI estimation using two different training strategies: derangement and permutation.  The x-axis represents the number of training steps, and the y-axis shows the estimated mutual information in nats. The 'True MI' line indicates the actual mutual information value. The plot shows that the derangement strategy yields a more accurate and stable estimation of MI compared to the permutation strategy. The permutation strategy's estimate is limited by log(N), highlighting a key advantage of the proposed derangement method.", "section": "5 Derangement Strategy"}, {"figure_path": "PThi9hf9UT/figures/figures_24_1.jpg", "caption": "Figure 1: MI estimate obtained with derangement and permutation training procedures, for data dimension d = 20 and batch size N = 128.", "description": "This figure compares the MI estimation results using two different training strategies: derangement and permutation.  The x-axis represents the number of training steps, and the y-axis shows the estimated mutual information (in nats). The black line represents the true MI value.  The plots show that the derangement strategy yields a more accurate and stable estimation of MI, while the permutation strategy leads to a significantly biased estimate that is upper-bounded by log(N).", "section": "Experimental Results"}, {"figure_path": "PThi9hf9UT/figures/figures_24_2.jpg", "caption": "Figure 15: Variance of the f-DIME estimators corresponding to different values of batch size.", "description": "This figure shows how the variance of different f-DIME estimators changes with different batch sizes for various MI values.  The x-axis represents the batch size, and the y-axis represents the variance.  Different colors represent different f-divergences (KL, GAN, HD) used in the f-DIME estimators.  The subplots are arranged by MI value, showing the variance for each estimator at different MI levels and batch sizes.  The plot demonstrates the relationship between batch size and the variance of MI estimation, highlighting the impact of this factor on the performance of different f-DIME versions.", "section": "6.2 Complex Gaussian and non-Gaussian distributions"}, {"figure_path": "PThi9hf9UT/figures/figures_24_3.jpg", "caption": "Figure 15: Variance of the f-DIME estimators corresponding to different values of batch size.", "description": "The figure shows how the variance of different f-DIME estimators changes with varying batch sizes for different mutual information (MI) values.  Each subplot represents a different MI value (2, 4, 6, 8, 10 nats), and each line within a subplot represents a different f-divergence used in the estimator (KL, GAN, HD). The results demonstrate the impact of batch size on the variance, highlighting the relative performance of various f-divergences under different conditions.  This is relevant for understanding how the choice of f-divergence and batch size impacts the accuracy and stability of MI estimation.", "section": "6.2 Complex Gaussian and non-Gaussian distributions"}, {"figure_path": "PThi9hf9UT/figures/figures_24_4.jpg", "caption": "Figure 15: Variance of the f-DIME estimators corresponding to different values of batch size.", "description": "This figure shows how the variance of different f-DIME estimators changes with the batch size.  Each subplot represents a different true mutual information (MI) value (2, 4, 6, 8, and 10 nats).  Within each subplot, the variance of each estimator (KL-DIME, GAN-DIME, HD-DIME) is plotted against the batch size. This visualization helps illustrate the impact of batch size on the variance of the f-DIME estimators and allows for a comparison of the different estimators at various MI values.", "section": "6 Experimental Results"}, {"figure_path": "PThi9hf9UT/figures/figures_25_1.jpg", "caption": "Figure 15: Variance of the f-DIME estimators corresponding to different values of batch size.", "description": "This figure displays the variance of three f-DIME estimators (KL-DIME, GAN-DIME, and HD-DIME) across different batch sizes for various MI values (2, 4, 6, 8, and 10 nats).  The plots illustrate how the variance changes as the batch size increases for each estimator and MI level. The results are particularly relevant to understanding the impact of batch size on the estimation accuracy of the proposed estimators.", "section": "6 Experimental Results"}, {"figure_path": "PThi9hf9UT/figures/figures_27_1.jpg", "caption": "Figure 1: MI estimate obtained with derangement and permutation training procedures, for data dimension d = 20 and batch size N = 128.", "description": "This figure compares the MI estimation results obtained using derangement and permutation training strategies. The x-axis represents the training steps, and the y-axis represents the estimated mutual information in nats. The plot shows that the derangement strategy converges to the true MI value more consistently than the permutation strategy, which appears to be upper-bounded by log(N).", "section": "Experimental Results"}, {"figure_path": "PThi9hf9UT/figures/figures_27_2.jpg", "caption": "Figure 1: MI estimate obtained with derangement and permutation training procedures, for data dimension d = 20 and batch size N = 128.", "description": "The figure compares the MI estimation performance of two training strategies: derangement and permutation.  The x-axis represents the number of training steps, and the y-axis shows the estimated mutual information (MI) in nats.  The derangement strategy consistently estimates MI more accurately than the permutation strategy, which is bounded by log(N) as demonstrated by the plot. This highlights the effectiveness of the proposed derangement strategy for accurate MI estimation.", "section": "5 Derangement Strategy"}, {"figure_path": "PThi9hf9UT/figures/figures_29_1.jpg", "caption": "Figure 6: NWJ, SMILE (t = \u221e), and MINE MI estimation comparison with d = 20 and N = 64. The Gaussian setting is represented in the top row, while the cubic setting is shown in the bottom row.", "description": "This figure compares the performance of three mutual information (MI) estimation methods (NWJ, SMILE, and MINE) under two different data distributions: Gaussian and cubic.  The plots show the MI estimates over training steps for different true MI values.  The top row displays the results for Gaussian data, while the bottom row shows the results for cubic data. The comparison helps to visualize how well each method estimates MI under varying conditions and the impact of data distribution on estimation accuracy.", "section": "Experimental Results"}, {"figure_path": "PThi9hf9UT/figures/figures_30_1.jpg", "caption": "Figure 18: Comparison between different estimators for the baseline property, using MNIST data set on the left and FashionMNIST on the right.", "description": "This figure shows the results of the baseline self-consistency test on MNIST and FashionMNIST datasets.  The baseline test checks if the estimated mutual information (MI) between an image X and a masked version of the same image Y (where only the top t rows are visible) increases monotonically with t, starting from 0 (when X and Y are independent) and approaching 1 (when the mask is removed).  The plot displays the MI ratio (MI(X; Y) / MI(X; X)) for different estimators (KL, GAN, HD, SMILE, CPC) across various values of t (number of rows used). The shaded areas represent the standard deviation, showing the variability of the estimates.", "section": "6.3 Self-Consistency Tests"}, {"figure_path": "PThi9hf9UT/figures/figures_30_2.jpg", "caption": "Figure 18: Comparison between different estimators for the baseline property, using MNIST data set on the left and FashionMNIST on the right.", "description": "This figure shows the results of self-consistency tests for different MI estimators on MNIST and FashionMNIST datasets. The baseline property test checks if the estimated mutual information (MI) between an image X and its masked version Y (showing only the top t rows) is non-decreasing with t, starting from 0 and converging to 1 as t increases. The shaded area represents the variance across multiple runs.", "section": "6.3 Self-Consistency Tests"}, {"figure_path": "PThi9hf9UT/figures/figures_30_3.jpg", "caption": "Figure 18: Comparison between different estimators for the baseline property, using MNIST data set on the left and FashionMNIST on the right.", "description": "This figure compares the performance of several Mutual Information (MI) estimators on the baseline property test.  The baseline property assesses how well the estimators estimate the MI between an image and a masked version of the same image. The x-axis represents the number of rows used from the top of the image. The y-axis represents the MI ratio (estimated MI divided by the MI between the original image and itself).  The ideal behavior would be a monotonically increasing function, starting from 0 and asymptotically approaching 1 as more rows are included. The shaded areas represent the variance of the estimates. Different colors represent different MI estimation methods. MNIST and FashionMNIST are two different image datasets used for this comparison.", "section": "6.3 Self-Consistency Tests"}, {"figure_path": "PThi9hf9UT/figures/figures_30_4.jpg", "caption": "Figure 1: MI estimate obtained with derangement and permutation training procedures, for data dimension d = 20 and batch size N = 128.", "description": "This figure compares the performance of mutual information (MI) estimation using two different training strategies: derangement and permutation.  The x-axis represents the number of training steps, and the y-axis shows the estimated MI in nats.  The derangement strategy consistently provides a more accurate estimation of the true MI, while the permutation strategy's estimations are bounded by log(N), demonstrating the significant benefit of using derangements.", "section": "5 Derangement Strategy"}]