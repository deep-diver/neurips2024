[{"figure_path": "PThi9hf9UT/tables/tables_20_1.jpg", "caption": "Table 1: Neural architectures comparison.", "description": "This table compares three different neural network architectures used for mutual information estimation: Joint, Separable, and Deranged.  The 'Input' row shows the number and type of data pairs fed into the networks for training.  'Nr. NNs' indicates the number of neural networks used in each architecture. Finally, 'Complexity' shows the computational complexity in Big O notation, indicating how the computational time scales with the batch size (N).", "section": "6.1 Architectures"}, {"figure_path": "PThi9hf9UT/tables/tables_23_1.jpg", "caption": "Table 2: Variance comparison between the VLB MI estimators and f-DIME, using the joint architecture, when d = 5 and N = 64, for the Gaussian setting.", "description": "This table compares the variance of several mutual information (MI) estimation methods.  It shows the variance for each method across different true MI values (2, 4, 6, 8, 10 nats). The methods compared include: NWJ, MINE, SMILE (with \u03c4 = \u221e), GAN-DIME, HD-DIME, and KL-DIME. The joint architecture was used for all methods, with data dimensionality d=5 and batch size N=64.  The table highlights the significantly lower variance achieved by the f-DIME estimators, especially at higher MI values, compared to traditional variational lower bound (VLB) methods like NWJ, MINE, and SMILE.", "section": "6.2 Complex Gaussian and non-Gaussian distributions"}, {"figure_path": "PThi9hf9UT/tables/tables_25_1.jpg", "caption": "Table 2: Variance comparison between the VLB MI estimators and f-DIME, using the joint architecture, when d = 5 and N = 64, for the Gaussian setting.", "description": "This table compares the variance of several mutual information estimators, including variations of the Variational Lower Bound (VLB) methods and the proposed f-DIME estimators, under specific conditions (dimension d=5 and sample size N=64).  It highlights how the variance changes across different methods and mutual information values within the Gaussian setting, providing insights into the performance and stability of different MI estimation approaches.", "section": "6.2 Complex Gaussian and non-Gaussian distributions"}, {"figure_path": "PThi9hf9UT/tables/tables_26_1.jpg", "caption": "Table 2: Variance comparison between the VLB MI estimators and f-DIME, using the joint architecture, when d = 5 and N = 64, for the Gaussian setting.", "description": "This table compares the variance of several mutual information (MI) estimation methods.  It shows the variance for different values of MI, using the joint neural network architecture with data dimensionality (d) of 5 and a batch size (N) of 64. The methods compared include NWJ, MINE, SMILE, GAN-DIME, HD-DIME, and KL-DIME. The table highlights the significantly lower variance of the f-DIME estimators compared to the traditional variational lower bound (VLB) methods (NWJ, MINE, SMILE).", "section": "6.2 Complex Gaussian and non-Gaussian distributions"}, {"figure_path": "PThi9hf9UT/tables/tables_26_2.jpg", "caption": "Table 2: Variance comparison between the VLB MI estimators and f-DIME, using the joint architecture, when d = 5 and N = 64, for the Gaussian setting.", "description": "This table compares the variance of several mutual information (MI) estimation methods.  It shows the variance for different values of the true MI, using the joint neural network architecture with data dimensionality (d) of 5 and batch size (N) of 64 for a Gaussian data distribution. The methods compared include several variational lower bound (VLB) based estimators (NWJ, MINE, SMILE) and the proposed f-DIME estimators (GAN-DIME, HD-DIME, KL-DIME). The table highlights the lower variance achieved by the f-DIME estimators compared to the VLB methods.", "section": "6.2 Complex Gaussian and non-Gaussian distributions"}]