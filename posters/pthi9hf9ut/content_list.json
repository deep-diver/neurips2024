[{"type": "text", "text": "Mutual Information Estimation via $f$ -Divergence and Data Derangements ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Nunzio A. Letizia Nicola Novello Andrea M. Tonello ", "page_idx": 0}, {"type": "text", "text": "University of Klagenfurt {nunzio.letizia,nicola.novello,andrea.tonello}@aau.at ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Estimating mutual information accurately is pivotal across diverse applications, from machine learning to communications and biology, enabling us to gain insights into the inner mechanisms of complex systems. Yet, dealing with high-dimensional data presents a formidable challenge, due to its size and the presence of intricate relationships. Recently proposed neural methods employing variational lower bounds on the mutual information have gained prominence. However, these approaches suffer from either high bias or high variance, as the sample size and the structure of the loss function directly influence the training process. In this paper, we propose a novel class of discriminative mutual information estimators based on the variational representation of the $f$ -divergence. We investigate the impact of the permutation function used to obtain the marginal training samples and present a novel architectural solution based on derangements. The proposed estimator is flexible since it exhibits an excellent bias/variance trade-off. The comparison with state-of-the-art neural estimators, through extensive experimentation within established reference scenarios, shows that our approach offers higher accuracy andlower complexity. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The mutual information (MI) between two multivariate random variables, $X$ and $Y$ , is a fundamental quantity in statistics, representation learning, information theory, communication engineering and biology [1, 2, 3, 4, 5]. It quantifies the statistical dependence between $X$ and $Y$ by measuring the amount of information obtained about $X$ via the observation of $Y$ , and it is defined as ", "page_idx": 0}, {"type": "equation", "text": "$$\nI(X;Y)=\\mathbb{E}_{({\\mathbf{x}},{\\mathbf{y}})\\sim p_{X Y}({\\mathbf{x}},{\\mathbf{y}})}\\bigg[\\log\\frac{p_{X Y}({\\mathbf{x}},{\\mathbf{y}})}{p_{X}({\\mathbf{x}})p_{Y}({\\mathbf{y}})}\\bigg].\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "Unfortunately, computing $I(X;Y)$ is challenging since the joint probability density function $p_{X Y}(\\mathbf{x},\\mathbf{y})$ and the marginals $p_{X}(\\mathbf{x}),p_{Y}(\\mathbf{y})$ are usually unknown, especially when dealing with high-dimensional data. Some recent techniques [6, 7] have demonstrated that neural networks can be leveraged as probability density function estimators and, more in general, are capable of modeling the data dependence. Discriminative approaches [8, 9] compare samples from both the joint and marginal distributions to directly compute the density ratio (or the log-density ratio) ", "page_idx": 0}, {"type": "equation", "text": "$$\nR({\\bf x},{\\bf y})=\\frac{p_{X Y}({\\bf x},{\\bf y})}{p_{X}({\\bf x})p_{Y}({\\bf y})}.\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "We focus on discriminative MI estimation since it can in principle enjoy some of the properties of implicit generative models, which are able of directly generating data that belongs to the same distribution of the input data without any explicit density estimate. In this direction, the most ", "page_idx": 0}, {"type": "text", "text": "successful technique is represented by generative adversarial networks (GANs) [10]. The adversarial training pushes the discriminator $D(\\mathbf{x})$ towardsthe optimumvalue ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\hat{D}(\\mathbf{x})=\\frac{p_{d a t a}(\\mathbf{x})}{p_{d a t a}(\\mathbf{x})+p_{g e n}(\\mathbf{x})}=\\frac{1}{1+\\frac{p_{g e n}(\\mathbf{x})}{p_{d a t a}(\\mathbf{x})}}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Therefore, the output of the optimum discriminator is itselfafunction of the density ratio $p_{g e n}/p_{d a t a}$ where $p_{g e n}$ and $p_{d a t a}$ are the distributions of the generated and the collected data, respectively. ", "page_idx": 1}, {"type": "text", "text": "We generalize the observation of (3) and we propose a family of MI estimators based on the variational lower bound (VLB) of the $f$ -divergence [11, 12]. In particular, we argue that the maximization of any $f$ -divergence VLB can lead to a MI estimator with excellent bias/variance trade-off. ", "page_idx": 1}, {"type": "text", "text": "Since we typically have access only to joint data points $({\\bf x},{\\bf y})\\;\\sim\\;p_{X Y}({\\bf x},{\\bf y})$ , another relevant practical aspect is the sampling strategy to obtain data from the product of marginals $p_{X}(\\mathbf{x})p_{Y}(\\mathbf{y})$ for instance via a shuffling mechanism along $N$ realizations of $Y$ . We analyze the impact that the permutation has on the learning and training process and we propose a derangement training strategy that achieves high performance requiring $\\Omega(N)$ operations. Simulation results demonstrate that the proposed approach exhibits improved estimations in a multitude of scenarios. ", "page_idx": 1}, {"type": "text", "text": "In brief, we can summarize our contributions over the state-of-the-art as follows: ", "page_idx": 1}, {"type": "text", "text": "\u00b7For any $f$ -divergence, we derive a training value function whose maximization leads to a given MI estimator.   \n\u00b7 We compare different $f$ -divergences and comment on the resulting estimator properties and performance.   \n\u00b7 We study the impact of data derangement for the learning model and propose a novel derangement training strategy that overcomes the upper bound on the MI estimation [13], contrarily to what happens when using a random permutation strategy.   \n\u00b7 We unify the main discriminative estimators into a publicly available code which can be used to reproduce all the results of this paper. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Traditional approaches for the MI estimation rely on binning, density and kernel estimation [14, 15], $k$ -nearest neighbors [16], and ensemble-based models [17]. Nevertheless, they do not scale to problems involving high-dimensional data as it is the case in modern machine learning applications. Hence, deep neural networks have recently been leveraged to maximize VLBs on the MI [11, 18, 19]. The expressive power of neural networks has shown promising results in this direction although less is known about the effectiveness of such estimators [20], especially since they suffer from either high bias or high variance. ", "page_idx": 1}, {"type": "text", "text": "Discriminative approaches usually exploit an energy-based variational family of functions to provide a lower bound on the Kullback-Leibler (KL) divergence. As an example, the Donsker-Varadhan dual representation of the KL divergence [11, 21] produces an estimate of the MI using the bound optimized by the mutual information neural estimator (MINE) [19]. Another VLB based on the KL divergence dual representation introduced in [18] leads to the NWJ estimator (also referred to as $f$ -MINE in [19]). Both MINE and NWJ suffer from high-variance estimates and to combat such a limitation, the SMILE estimator was introduced in [20], where the authors proved that the estimate of the partition function is the cause for high-variance in VLB estimators. SMILE is equivalent to MINE in the limit $\\tau\\rightarrow+\\infty$ . The MI estimator based on contrastive predictive coding (CPC) [22] provides low variance estimates but it is upper bounded by $\\log N$ , resulting in a biased estimator. Such upper bound, typical of contrastive learning objectives, has been recently analyzed in the context of skew-divergence estimators [23]. ", "page_idx": 1}, {"type": "text", "text": "Another estimator based on a classification task is the neural joint entropy estimator (NJEE) proposed in [24], which estimates the MI as entropies subtraction. ", "page_idx": 1}, {"type": "text", "text": "Inspiredby the $f$ -GAN training objective [25], in the following, we present a class of discriminative MI estimators based on the $f$ -divergence measure. Conversely to what has been proposed so far in the literature, where $f$ is always constrained to be the generator of the KL divergence, we allow for anychoiceof $f$ .Different $f$ functions will have different impact on the training and optimization sides, while on the estimation side, the partition function does not need to be computed, leading to low variance estimators. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "3 $f$ -Divergence Mutual Information Estimation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The calculation of the MI via a discriminative approach requires the density ratio (2). From (3), we observe that $I(X;Y)$ can be estimated using the optimum GAN discriminator $\\hat{D}$ when $p_{d a t a}\\equiv p_{X}p_{Y}$ and $p_{g e n}\\equiv p_{X Y}$ . More in general, the authors in [25] extended the variational divergence estimation framework presented in [18] and showed that any $f$ -divergence can be used to train GANs. Inspired by such idea, we now argue that also discriminative MI estimators enjoy similar properties if the variational representation of $f$ -divergence functionals $D_{f}(P||Q)$ is adopted. ", "page_idx": 2}, {"type": "text", "text": "In detail, let $P$ and $Q$ be absolutely continuous measures w.r.t. ${\\mathrm{d}}x$ and assume they possess densities $p$ and $q$ , then the $f$ -divergence is defined as follows ", "page_idx": 2}, {"type": "equation", "text": "$$\nD_{f}(P||Q)=\\int_{X}q(\\mathbf{x})f\\Bigg(\\frac{p(\\mathbf{x})}{q(\\mathbf{x})}\\Bigg)\\,\\mathrm{d}\\mathbf{x},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathcal{X}$ is a compact domain and the function $f:\\mathbb{R}_{+}\\to\\mathbb{R}$ is convex, lower semicontinuous and satisfies $f(1)=0$ ", "page_idx": 2}, {"type": "text", "text": "In the following, we introduce $f$ -DIME, a class of discriminative mutual information estimators (DIME) based on the variational representation of the $f$ -divergence. ", "page_idx": 2}, {"type": "text", "text": "Theorem 3.1. Let $(X,Y)\\sim p_{X Y}(\\mathbf{x},\\mathbf{y})$ be a pair of multivariate random variables. Let $\\sigma(\\cdot)$ be a permutation function such that $p_{\\sigma(Y)}(\\sigma(\\mathbf{y})|\\mathbf{x})=p_{Y}(\\mathbf{y})$ and $T:\\mathrm{dom}(X)\\times\\mathrm{dom}(Y)\\to\\mathbb{R}$ . Let $f^{*}$ be the Fenchel conjugate of $f:\\mathbb{R}_{+}\\to\\mathbb{R},$ a convex lower semicontinuous function that satisfies $f(1)=0$ with derivative $f^{\\prime}$ .If $\\mathcal{I}_{f}(T)$ is a value function defined as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{T}_{f}(T)=\\mathbb{E}_{(\\mathbf{x},\\mathbf{y})\\sim p_{X Y}(\\mathbf{x},\\mathbf{y})}\\bigg[T\\big(\\mathbf{x},\\mathbf{y}\\big)-f^{*}\\bigg(T\\big(\\mathbf{x},\\sigma(\\mathbf{y})\\big)\\bigg)\\bigg],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "then ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\hat{T}(\\mathbf x,\\mathbf y)=\\arg\\operatorname*{max}_{T}\\mathcal{I}_{f}(T)=f^{\\prime}\\bigg(\\frac{p_{X Y}(\\mathbf x,\\mathbf y)}{p_{X}(\\mathbf x)p_{Y}(\\mathbf y)}\\bigg),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "and ", "page_idx": 2}, {"type": "equation", "text": "$$\nI(X;Y)=I_{f D I M E}(X;Y)=\\mathbb{E}_{(\\mathbf{x},\\mathbf{y})\\sim p_{X Y}(\\mathbf{x},\\mathbf{y})}\\bigg[\\log\\biggl(\\left(f^{*}\\right)^{\\prime}\\bigl(\\hat{T}(\\mathbf{x},\\mathbf{y})\\bigr)\\biggr)\\bigg].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Theorem 3.1 shows that any value function $\\mathcal{I}_{f}$ of the form in (5), seen as the dual representation of a given $f$ -divergence $D_{f}$ , can be maximized to estimate the MI via (7). It is interesting to notice that the proposed class of estimators does not need any evaluation of the partition term. ", "page_idx": 2}, {"type": "text", "text": "We propose to parametrize $T(\\mathbf{x},\\mathbf{y})$ with a deep neural network $T_{\\theta}$ of parameters $\\theta$ and solve with gradient ascent and back-propagation to obtain ${\\hat{\\theta}}\\ =\\ \\arg\\operatorname*{max}_{\\theta}{\\mathcal{J}}_{f}(T_{\\theta})$ . By doing so, it is possible to guarantee that, at every training iteration $n$ , the convergence of the $f$ -DIME estimator $\\hat{I}_{n,f D I M E}(X;Y)$ is controlled by the convergence of $T$ towards the tight bound $\\hat{T}$ while maximizing ${\\mathcal{I}}_{f}(T)$ , as stated in the following lemma. ", "page_idx": 2}, {"type": "text", "text": "Lemma 3.2. Let the discriminator $T(\\cdot)$ be with enough capacity, i.e., in the non parametric limit. Considertheproblem ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\hat{T}=\\mathrm{\\arg\\,max}\\,\\mathcal{J}_{f}(T)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathcal{I}_{f}(T)$ is defined as in (5), and the update rule based on the gradient descent method ", "page_idx": 2}, {"type": "equation", "text": "$$\nT^{(n+1)}=T^{(n)}+\\mu\\nabla\\mathcal{I}_{f}(T^{(n)}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "If the gradient descent method converges to the global optimum $\\hat{T}$ themutualinformationestimator defined in (7) converges to the real value of the mutual information $I(X;Y)$ ", "page_idx": 2}, {"type": "text", "text": "The proof of Lemma 3.2, which is described in the Appendix, provides some theoretical grounding for the behaviour of MI estimators when the training does not converge to the optimal density ratio. Moreover, it also offers insights about the impact of different functions $f$ On the numerical bias. ", "page_idx": 3}, {"type": "text", "text": "It is important to remark the difference between the classical VLB estimators that follow a discriminative approach and the DIME-like estimators. They both achieve the goal through a discriminator network that outputs a function of the density ratio. However, the former models exploit the variational representation of the MI (or the KL) and, at the equilibrium, use the discriminator output directly in one of the value functions reported in Appendix B. The latter, instead, use the variational representationof any $f$ -divergence to extract the density ratio estimate directly from the discriminator Output. ", "page_idx": 3}, {"type": "text", "text": "In the upcoming sections, we analyze the variance of $f$ DIME and we propose a training strategy for the implementation of Theorem 3.1. In our experiments, we consider the cases when $f$ is the generator of: a) the KL divergence; b) the GAN divergence; c) the Hellinger distance squared. Due to space constraints, we report in Sec. A of the Appendix the value functions used for training and the mathematical expressions of the resulting DIME estimators. ", "page_idx": 3}, {"type": "text", "text": "4 Variance Analysis ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we assume that the ground truth density ratio $\\hat{R}({\\bf x},{\\bf y})$ exists and corresponds to the density ratio in (2). We also assume that the optimum discriminator $\\hat{T}({\\bf x},{\\bf y})$ is known and already obtained (e.g. via a neural network parametrization). ", "page_idx": 3}, {"type": "text", "text": "We define $p_{X Y}^{M}(\\mathbf{x},\\mathbf{y})$ and $p_{X}^{N}(\\mathbf{x})p_{Y}^{N}(\\mathbf{y})$ as the empirical distributions corresponding to $M$ i..d. samples from the true joint distribution $p_{X Y}$ and to $N$ i.i.d. samples from the product of marginals $p_{X P Y}$ , respectively. The randomness of the sampling procedure and the batch sizes $M,N$ influence the variance of variational MI estimators. In the following, we prove that under the previous assumptions, $f$ -DIME exhibits better performance in terms of variance w.r.t. some variational estimators with a discriminative approach, e.g., MINE and NWJ. ", "page_idx": 3}, {"type": "text", "text": "The prition funtion estiation $\\mathbb{E}_{p_{X}^{N}p_{Y}^{N}}[\\hat{R}]$ represents the majorisue when dealing with variational MI estimators. Indeed, they comprise the evaluation of two terms (using the given density ratio), and the partition function is the one responsible for the variance growth. The authors in [20] characterized the variance of both MINE and NWJ estimators, in particular, they proved that the variance scales exponentially with the ground truth $\\mathsf{M I}\\,\\forall M\\in\\mathbb{N}$ ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathrm{Var}_{p_{X Y},p_{X}p_{Y}}\\left[I_{N W J}^{M,N}\\right]\\geq\\!\\frac{e^{I\\left(X;Y\\right)}-1}{N}}\\\\ &{}&{\\operatorname*{lim}_{N\\rightarrow\\infty}N\\mathrm{Var}_{p_{X Y},p_{X}p_{Y}}\\left[I_{M I N E}^{M,N}\\right]\\geq\\!e^{I\\left(X;Y\\right)}-1,}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{I_{N W J}^{M,N}:=\\mathbb{E}_{p_{X Y}^{M}}[\\log\\hat{R}+1]-\\mathbb{E}_{p_{X}^{N}p_{Y}^{N}}[\\hat{R}]}\\\\ {I_{M I N E}^{M,N}:=\\mathbb{E}_{p_{X Y}^{M}}[\\log\\hat{R}]-\\log\\mathbb{E}_{p_{X}^{N}p_{Y}^{N}}[\\hat{R}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "To reduce the impact of the partition function on the variance, the authors of [20] also proposed to clip the density ratio between $e^{-\\tau}$ and $e^{\\tau}$ leading to an estimator (SMILE) with bounded partition variance. However, also the variance of the log-density ratio $\\mathbb{E}_{p_{X Y}^{M}}[\\log\\hat{R}]$ influences the variance of the variational estimators, since it is clear that ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname{Var}_{p_{X Y},p_{X}p_{Y}}\\left[I_{V L B}^{M,N}\\right]\\geq\\operatorname{Var}_{p_{X Y}}\\left[\\mathbb{E}_{p_{X Y}^{M}}[\\log\\hat{R}]\\right],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "a result that holds for any type of MI estimator based on a VLB. ", "page_idx": 3}, {"type": "text", "text": "The great advantage of $f$ -DIME is to avoid the partition function estimation step, significantly reducing the variance of the estimator. Under the same initial assumptions, from (12) we can immediately conclude that ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname{Var}_{p_{X Y}}\\left[I_{f D I M E}^{M}\\right]\\leq\\operatorname{Var}_{p_{X Y},p_{X}p_{Y}}\\left[I_{V L B}^{M,N}\\right],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where ", "page_idx": 4}, {"type": "equation", "text": "$$\nI_{f D I M E}^{M}:=\\mathbb{E}_{p_{X Y}^{M}}[\\log\\hat{R}]\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "is the Monte Carlo implementation of $f$ -DIME. Hence, the $f$ -DIME class of models has lower variance than any VLB based estimator (MINE, NWJ, SMILE, etc.). ", "page_idx": 4}, {"type": "text", "text": "Furthermore, we provide in Appendix C two supplementary results. Lemma 4.1 introduces an upper bound on the variance of the $f$ DIME estimator, a result holding for any type of value function $\\mathcal{I}_{f}$ Lemma 4.2, instead, characterizes the variance of the estimator in (14) when $X$ and $Y$ are correlated Gaussian random variables. We found out that the variance is finite and we use this result to verify in the experiments that the variance of $f$ -DIME does not diverge for high values of MI. ", "page_idx": 4}, {"type": "text", "text": "5  Derangement Strategy ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The discriminative approach essentially compares expectations over both joint $(\\mathbf{x},\\mathbf{y})\\sim p_{X Y}$ and marginal $(\\mathbf{x},\\mathbf{y})\\sim p_{X}p_{Y}$ data points. Practically, we have access only to $N$ realizations of the joint distribution $p_{X Y}$ and to obtain $N$ marginal samples of $p_{X}p_{Y}$ from $p_{X Y}$ a shuffing mechanism for the realizations of $Y$ is typically deployed. A general result in [13] shows that failing to sample from the correct marginal distribution would lead to an upper bounded MI estimator. ", "page_idx": 4}, {"type": "text", "text": "We study the structure that the permutation law $\\sigma(\\cdot)$ in Theorem 3.1 needs to have when numerically implemented. In particular, we now prove that a naive permutation over the realizations of $Y$ results in an incorrect VLB of the $f$ -divergence, causing the MI estimator to be bounded by $\\log(N)$ ,where $N$ is the batch size. To solve this issue, we propose a derangement strategy. ", "page_idx": 4}, {"type": "text", "text": "Let the data points $(\\mathbf{x},\\mathbf{y})\\sim p_{X Y}$ be $N$ pairs $(\\mathbf{x}_{i},\\mathbf{y}_{i})$ $\\forall i\\in\\{1,\\ldots,N\\}$ . The naive permutation of $\\mathbf{y}$ , denoted as $\\pi(\\mathbf{y})$ , leads to $N$ new random pairs $(\\mathbf{x}_{i},\\mathbf{y}_{j})$ \uff0c $\\forall i$ and $j\\in\\{1,\\cdots,N\\}$ . The idea is that a random naive permutation may lead to at least one pair $\\left(\\mathbf{x}_{k},\\mathbf{y}_{k}\\right)$ , with $k\\in\\{1,\\ldots,N\\}$ , which is actually a sample from the joint distribution. Viceversa, the derangement of y, denoted as $\\sigma(\\mathbf y)$ , leads to $N$ new random pairs $(\\mathbf{x}_{i},\\mathbf{y}_{j})$ such that $i\\ne j,\\forall i$ and $j\\in\\{1,\\cdots\\,,N\\}$ . Such pairs $(\\mathbf{x}_{i},\\mathbf{y}_{j}),i\\neq j$ can effectively be considered samples from $p_{X}(\\mathbf{x})p_{Y}(\\mathbf{y})$ . An example using these definitions is provided in Appendix D.1.3. ", "page_idx": 4}, {"type": "text", "text": "The following lemma analyzes the relationship between the Monte Carlo approximations of the VLBs of the $f$ -divergence $\\mathcal{I}_{f}$ in Theorem 3.1 using $\\pi(\\cdot)$ and $\\sigma(\\cdot)$ as permutation laws. ", "page_idx": 4}, {"type": "text", "text": "Lemma 5.1, Let $\\left(\\mathbf{x}_{i},\\mathbf{y}_{i}\\right)$ \uff0c $\\forall i\\in\\{1,\\ldots,N\\}$ ,be $N$ data points. Let $\\mathcal{I}_{f}(T)$ be thevalue function in (5). Let $\\mathcal{I}_{f}^{\\pi}(T)$ and $\\mathcal{I}_{f}^{\\sigma}(T)$ be numerical implementations of $\\mathcal{I}_{f}(T)$ using a random permutation and a random derangement of y,respectively.Denote with $K$ thenumberofpoints $\\mathbf{y}_{k}$ \uff0cwith $k\\in\\{1,\\ldots,N\\}$ , in the same position after the permutation (i.e., the fixed points). Then ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{J}_{f}^{\\pi}(T)\\leq\\frac{N-K}{N}\\mathcal{J}_{f}^{\\sigma}(T).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Lemma 5.1 practically asserts that the value function $\\mathcal{I}_{f}^{\\pi}(T)$ evaluated via a naive permutation of the data is not a valid VLB of the $f$ -divergence, and thus, there is no guarantee on the optimality of the discriminator's output. An interesting mathematical connection can be obtained when studying $\\mathcal{I}_{f}^{\\pi}(T)$ as a sort of variational skew-divergence estimator [23], but this goes beyond the scope of this paper. ", "page_idx": 4}, {"type": "text", "text": "The following theorem states that in the case of the KL divergence, the maximum of $\\mathcal{I}_{f}^{\\pi}(D)$ is attained for a value of the discriminator that is not exactly the density ratio (as it should be from (21), seeAppendixA). ", "page_idx": 4}, {"type": "text", "text": "Theorem 5.2. Let the discriminator $D(\\cdot)$ be with enough capacity. Let $N$ be the batch size and $f$ be the generator of the $K L$ divergence. Let $\\mathcal{I}_{K L}^{\\pi}(D)$ be defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{I}_{K L}^{\\pi}(D)=\\mathbb{E}_{(\\mathbf{x},\\mathbf{y})\\sim p_{X Y}(\\mathbf{x},\\mathbf{y})}\\bigg[\\mathrm{log}\\bigg(D\\big(\\mathbf{x},\\mathbf{y}\\big)\\bigg)-f^{*}\\bigg(\\mathrm{log}\\bigg(D\\big(\\mathbf{x},\\pi(\\mathbf{y})\\big)\\bigg)\\bigg)\\bigg].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Denotewith $K$ the number of indices in the same position after the permutation (i.e.,the fixed points), andwith $R(\\mathbf{x},\\mathbf{y})$ the density ratio in (2). Then, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{D}({\\mathbf x},{\\mathbf y})=\\arg\\operatorname*{max}_{D}\\mathcal{J}_{K L}^{\\pi}(D)=\\frac{N R({\\mathbf x},{\\mathbf y})}{K R({\\mathbf x},{\\mathbf y})+N-K}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "image", "img_path": "PThi9hf9UT/tmp/adc90ea5367e869f0fc5bdd98c012323c357a0069c74c2d4401ac2c51844e195.jpg", "img_caption": ["Figure 1: MI estimate obtained with derangement and permutation training procedures, for data dimension $d=20$ andbatchsize $N=128$ "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Although Theorem 5.2 is stated for the KL divergence, it can be easily extended to any $f$ -divergence using Theorem 3.1. Notice that if the number of indices in the same position $K$ is equal to O, we fall back into the derangement strategy and we retrieve the density ratio as output. ", "page_idx": 5}, {"type": "text", "text": "When we parametrize $D$ with a neural network, we perform multiple training iterations and so we have multiple batches of dimension $N$ . This turns into an average analysis on $K$ . We report in the Appendix (see Lemma 5.4) the proof that, on average, $K$ is equal to 1. ", "page_idx": 5}, {"type": "text", "text": "From the previous results, it follows immediately that the estimator obtained using a naive permutation strategy is biased and upper bounded by a function of the batch size $N$ ", "page_idx": 5}, {"type": "text", "text": "Corollary 5.3 (Permutation bound). Let KL-DIME be the estimator obtained via iterative optimizationof $\\mathcal{I}_{K L}^{\\pi}(D)$ usingabatchof size $N$ every training step. Then, ", "page_idx": 5}, {"type": "equation", "text": "$$\nI_{K L-D I M E}^{\\pi}:=\\mathbb{E}_{(\\mathbf{x},\\mathbf{y})\\sim p_{X Y}(\\mathbf{x},\\mathbf{y})}\\bigg[\\mathrm{log}\\bigg(\\hat{D}(\\mathbf{x},\\mathbf{y})\\bigg)\\bigg]<\\mathrm{log}(N).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We report in Fig. 1 an example of the difference between the derangement and permutation strategies. The estimate attained by using the permutation mechanism, showed in Fig. 1b, demonstrates Theorem 5.2 and Corollary 5.3, as the upper bound corresponding to $\\log(N)$ (with $N=128)$ )is clearly visible. ", "page_idx": 5}, {"type": "text", "text": "6 Experimental Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we firstly describe the architectures of the proposed estimators. Then, we outline the data used to estimate the MI, comment on the performance of the discussed estimators in different scenarios, also analyzing their computational complexity. Finally, we present the outcomes of the self-consistency tests [20] over image datasets. ", "page_idx": 5}, {"type": "text", "text": "6.1 Architectures ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To demonstrate the behavior of the state-of-the-art MI estimators, we consider multiple neural network architectures. The word architecture needs to be intended in a wide-sense, meaning that it represents the neural network architecture and its training strategy. In particular, additionally to the architectures joint [19] and separable [26], we propose the architecture deranged. ", "page_idx": 5}, {"type": "text", "text": "The joint architecture concatenates the samples $\\mathbf{x}$ and $\\mathbf{y}$ as input of a single neural network. Each training step requires $N$ realizations $(\\mathbf{x}_{i},\\mathbf{y}_{i})$ drawn from $p_{X Y}(\\mathbf{x},\\mathbf{y})$ , for $i~\\in~\\{1,\\ldots,N\\}$ and $N(N-1)$ samples $(\\mathbf{x}_{i},\\mathbf{y}_{j}),\\forall i,j\\in\\{1,\\ldots,N\\}$ , with $i\\neq j$ ", "page_idx": 5}, {"type": "text", "text": "The separable architecture comprises two neural networks, the former fed in with $N$ realizations of $X$ ,the latter with $N$ realizations of $Y$ . The inner product between the outputs of the two networks is exploited to obtain the MI estimate. ", "page_idx": 5}, {"type": "text", "text": "The proposed deranged architecture feeds a neural network with the concatenation of the samples $\\mathbf{x}$ and $\\mathbf{y}$ , similarly to the joint architecture. However, the deranged one obtains the samples of $p_{X}(\\mathbf{x})p_{Y}(\\mathbf{y})$ by performing a derangement of the realizations $\\mathbf{y}$ in the batch sampled from $p_{X Y}(\\mathbf{x},\\mathbf{y})$ Such diverse training strategy solves the main problem of the joint architecture: the difficult scalability to large batch sizes. For large values of $N$ , the complexity of the joint architecture is $\\Omega(N^{2})$ ,while the complexity of the deranged one is $\\Omega(N)$ . NJEE utilizes a specific architecture, in the following referred to as ad hoc, comprising $2d-1$ neural networks, where $d$ is the dimension of $X$ $I_{N J E E}$ training procedure is supervised: the input of each neural network does not include the $\\mathbf{y}$ samples. All the implementation details1 are reported in Appendix D. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "6.2  Complex Gaussian and non-Gaussian distributions ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We benchmark the proposed class of MI estimators on two settings utilized in previous papers [11, 20]. In the first setting (called Gaussian), a multidimensional Gaussian distribution is sampled to obtain $\\mathbf{x}$ and $\\mathbf{n}$ samples, independently. Then, $\\mathbf{y}$ is obtained as linear combination of $\\mathbf{x}$ and $\\mathbf{n}$ $\\mathbf{y}\\,=\\,\\rho\\,\\mathbf{x}\\,+\\,{\\sqrt{1-\\rho^{2}}}\\,\\mathbf{n}$ where $\\rho$ is the correlation coeffcient. In the second setting (referred to as cubic), the nonlinear transformation $\\mathbf{y}\\mapsto\\mathbf{y}^{3}$ is applied to the Gaussian samples. The true MI follows a staircase shape, where each step is a multiple of 2 nats. Each neural network is trained for $4\\mathbf{k}$ iterations for each stair step, with a batch size of 64 samples $[N=64]$ 0. The tested estimators are: $I_{N J E E}$ ,ISMILE $\\left.\\tau=1\\right\\rangle$ $I_{G A N-D I M E}$ \uff0c $I_{H D-D I M E}$ \uff0c $I_{K L-D I M E}$ , and $I_{C P C}$ , as illustrated in Fig. 2. The performance of $I_{M I N E}$ \uff0c $I_{N W J}$ , and $I_{S M I L E}(\\tau=\\infty)$ is reported in Sec. D of the Appendix, since they exhibit lower performance compared to both SMILE and $f$ -DIME. In fact, all the $f$ -DIME estimators have lower variance compared to $I_{M I N E}$ $I_{N W J}$ , and $I_{S M I L E}(\\tau=\\infty)$ which are characterized by an exponentially increasing variance (see (10), Tab. 2, Fig. 9, and Fig. 6 in the Appendix). In particular, all the estimators analyzed belonging to the $f$ -DIME class achieve significantly low bias and variance when the true MI is small. Interestingly, for high target MI, different $f$ -divergences lead to dissimilar estimation properties. For large MI, $I_{K L-D I M E}$ is characterized by a low variance, at the expense of a high bias and a slow rise time. Contrarily, $I_{H D-D I M E}$ attains a lower bias at the cost of slightly higher variance w.r.t. $I_{K L-D I M E}$ . Diversely, $I_{G A N-D I M E}$ achieves the lowest bias, and a variance comparable to $I_{H D-D I M E}$ . Additional results confirming the estimators' behavior when $d$ and $N$ vary, including experiments with high data dimensionality, are reported and described in Appendix D. ", "page_idx": 6}, {"type": "text", "text": "$I_{N J E E}$ obtains an estimate which is highly biased, and variance comparable to $f$ -DIME. $I_{C P C}$ is upper-bounded by $\\log(N)$ . The MI estimates obtained with $I_{S M I L E}$ and $I_{G A N-D I M E}$ appear to possess similar behavior, although the value functions of SMILE and GAN-DIME are structurally different. The reason why. $I_{S M I L E}$ is almost equivalent to $I_{G A N-D I M E}$ resides in their training strategy, since they both minimize the same $f$ -divergence. Looking at the implementation of SMILE 2, in fact, the network's training is guided by the gradient computed using the Jensen-Shannon (JS) divergence (a linear transformation of the GAN divergence). Given the trained network, the clipped objective function proposed in [20] is only used to compute the MI estimate, since when (29) is used to train the network, the MI estimate diverges (see Fig. 7 in Appendix D). However, with the proposed class of $f$ -DIME estimators we show that during the estimation phase the partition function (clipped in [20]) is not necessary to obtain the MI estimate. ", "page_idx": 6}, {"type": "text", "text": "We test our estimators over additional complex Gaussian data transformations (half-cube, asinh, and swiss roll mappings, Fig. 3) and non-Gaussian distributions (uniform and student distributions, Fig. 4) as suggested in [27]. The half-cube mapping is used to lengthen the tails of the Gaussian distributions. The inverse hyperbolic sine (asinh) mapping shortens the tails of the Gaussian distributions. These two transformations are applied to the same scenario of the Gaussian and cubic already present in our paper. The swiss roll mapping increases the dimensionality of the data distribution (from two to three dimensions) and it is usually used to test dimensionality reduction techniques. It considers two Gaussian random variables that are transformed into uniform random variables via the probability integral transform, the same pre-processing approach utilized in [28] to estimate the MI. The swiss roll mapping is applied to the $X$ uniform random variable. The stairs plots are obtained by varying the correlation between the initial Gaussian distributions. The uniform case estimates the MI of the summation of two uniform random variables $U(0,1)$ and $U(-\\epsilon,\\epsilon)$ , where we vary the parameter $\\epsilon$ modifying the true MI. The student scenario analyzes the case of a multivariate student distribution with dispersion matrix chosen to be the identity matrix and degrees of freedom $d f$ . In this scenario, we vary $d f$ , implying a variation of the target MI. For the transformed Gaussian scenarios showed in Fig. 3 GAN-DIME attains the best performance in terms of low bias and variance. Among the non-Gaussian settings depicted in Fig. 4, KL-DIME and GAN-DIME outperform the other methods, exhibiting low bias and exceptionally low variance. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "image", "img_path": "PThi9hf9UT/tmp/faa089de38b376a178988e851c5235fa11b425459727665c42a131611286ab3f.jpg", "img_caption": ["Figure 2: Staircase MI estimation comparison for $d=5$ and $N=64$ . The Gaussian case is reported in the top row, while the cubic case is shown in the bottom row. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "PThi9hf9UT/tmp/2d7ddb5613d6038f930ad017ca0b0c278212005277139f13567452995bc606b9.jpg", "img_caption": ["Figure 3: Staircase MI estimation comparison for $d=5$ and $N\\,=\\,64$ .Top: Half-cube scenario. Middle: Asinh scenario. Bottom: Swiss roll scenario. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "PThi9hf9UT/tmp/856f50aa4f4becfea43659b109ebb141358851878d8b43c042962cee2b73faa8.jpg", "img_caption": ["Figure 4: Staircase MI estimation comparison for $d=1$ and $N=64$ . Top row: Uniform scenario. Bottom row: Student scenario "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "A schematic comparison between all the MI estimators is reported in Tab. 6 in Sec. D of the Appendix, where $I_{G A N-D I M E}$ is proposed as the best estimator, because of its low bias, variance and robustness to the change of $d$ and $N$ .When $N$ and $d$ vary, in fact, the class of $f$ -DIME estimators proves its robustness (i.e., maintains low bias and variance), as represented in Figs. 2, and 10, and 11 in the Appendix. For instance, $I_{G A N-D I M E}$ attains low bias in all the three scenarios, and limited variance which decreases as $N$ increases (see also Fig. 15 in Appendix D.1). Differently, the behavior of $I_{C P C}$ strongly depends on $N$ , significantly impacting its bias. Therefore, unless the batch size is considerably large, $I_{C P C}$ estimate is not reliable. $I_{N J E E}$ attains higher bias when $N$ increases and, even more severely, when $d$ decreases (see Fig. 2). ", "page_idx": 7}, {"type": "image", "img_path": "PThi9hf9UT/tmp/312c98450bd9869a3d7f0b7d6ae8c61a05affa8ac0061daeba9650fb21632d30.jpg", "img_caption": ["Figure 5: Time requirements comparison to complete the 5-step staircase MI. From the left, the first and second behaviors vary over the batch size. The last one varies over the probability distribution dimension. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Computational Time Analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "A fundamental characteristic of each algorithm is the computational time. The computational time analysis is developed on a server with CPU \"AMD Ryzen Threadripper 3960X 24-Core Processor' and GPU \"MSI GeForce RTX 3090 Gaming X Trio 24G, 24GB GDDR6X\". ", "page_idx": 8}, {"type": "text", "text": "Before analyzing the time requirements to complete the 5-step MI staircases, we specify two different ways to implement the derangement of the y realizations in each batch. ", "page_idx": 8}, {"type": "text", "text": "Random-based. The trivial way to achieve the derangement is to randomly shuffle the y elements of the batch until there are no fixed points (i.e., all the y realizations in the batch are assigned to a different position w.r.t. the starting location). ", "page_idx": 8}, {"type": "text", "text": "Shift-based. Given $N$ realizations $\\left(\\mathbf{x}_{i},\\mathbf{y}_{i}\\right)$ drawn from $p_{X Y}(\\mathbf{x},\\mathbf{y})$ , for $i\\in\\{1,\\ldots,N\\}$ , we obtain the deranged samples as $(\\mathbf{x}_{i},\\mathbf{y}_{(i+1)\\%N})$ , where $\\mathrm{{}^{\\circ}\\!\\!\\!\\!\\slash o}$ \"' is the modulo operator. ", "page_idx": 8}, {"type": "text", "text": "Although the MI estimates obtained by the two derangement methods are almost indistinguishable, all the results showed in the paper are achieved by using the random-based method. Additionally, we demonstrate the time efficiency of the shift-based approach. ", "page_idx": 8}, {"type": "text", "text": "The time requirements to complete the 5-step staircase MI when varying the batch size $N$ are reported in the left and center graphics of Fig. 5. The influence of the MI estimator objective functions in the algorithm's time requirements is marginal, while the architecture type is the impactful component. As discussed in Sec. 6.1, the deranged strategy is remarkably faster than the joint one as $N$ increases. More in general, the architectures deranged and separable are significantly faster w.r.t. the joint and NJEE ones, for a given batch size $N$ and input distribution size $d$ The need of the separable architecture to use two neural networks implies that when $N$ is significantly large, the deranged implementation is much faster than the separable one. The central graph in Fig. 5 illustrates a detailed representation of the time requirements of these two architectures to complete the 5-step stairs. As $N$ increases, the gap between the time needed by the architectures deranged and separable grows, demonstrating that the former is the fastest. For example, when $d=20$ and $N=30k$ $I_{G A N-D I M E}$ needs about 55 minutes when using the architecture separable, but only 15 minutes when using the deranged one and less than 9 minutes for the shift-based deranged architecture. ", "page_idx": 8}, {"type": "text", "text": "$I_{N J E E}$ is evaluated with its own architecture, which is the most computationally demanding, because it trains a number of neural networks equal to $2d-1$ . Thus, $I_{N J E E}$ can be utilized only in cases where the time availability is orders of magnitude higher than the other approaches considered. The time requirements to complete the 5-step staircase MI when varying the multivariate Gaussian distribution dimension $d$ are reported in the right-side part of Fig. 5. When $d$ is large, the training of $I_{N J E E}$ fails due to memory requirement problems. For example, our hardware platform does not allow the usage of $d>30$ ", "page_idx": 8}, {"type": "text", "text": "6.3  Self-Consistency Tests ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To demonstrate the utility of $f$ -DIME in non-Gaussian scenarios, we investigated the three selfconsistency tests developed by [20] over images datasets using all the estimators previously described, except $I_{N J E E}$ (for dimension constraints). The $f$ -DIME estimators satisfy two out of the three tests, ", "page_idx": 8}, {"type": "text", "text": "as discriminative approaches tend to be less precise when the MI is high, in accordance with [20].   \nWe report the description of tests and results in Appendix D. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we presented $f$ -DIME, a class of discriminative mutual information estimators based on the variational representation of the $f$ -divergence. We proved that any valid choice of the function $f$ leads to a low-variance MI estimator which can be parametrized by a neural network. We also proposed a derangement training strategy that efficiently samples from the product of marginal distributions. The performance of $f$ -DIME is evaluated using three functions $f$ , and it is compared with state-of-the-art estimators. Results demonstrate excellent bias/variance trade-off for different data dimensions and different training parameters. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Ziv Goldfeld and Kristjan Greenewald. Sliced mutual information: A scalable measure of statistical dependence. Advances in Neural Information Processing Systems, 34:17567-17578, 2021.   \n[2] Michael Tschannen, Josip Djolonga, Paul K. Rubenstein, Sylvain Gelly, and Mario Lucic. On mutual information maximization for representation learning. In International Conference on Learning Representations, 2020.   \n[3]  Dongning Guo, Shlomo Shamai, and Sergio Verdu. Mutual information and minimum meansquare error in gaussian channels. IEEE transactions on information theory, 51(4):1261-1282, 2005.   \n[4] Nunzio A. Letizia, Andrea M. Tonello, and H. Vincent Poor. Cooperative channel capacity learning. IEEE Communications Letters, 27(8):1984-1988, 2023.   \n[5] Josien PW Pluim, JB Antoine Maintz, and Max A Viergever. Mutual-information-based registration of medical images: a survey. IEEE transactions on medical imaging, 22(8):986- 1004, 2003.   \n[6]  Nicola Novello and Andrea M Tonello. $f$ -divergence based classification: Beyond the use of cross-entropy. In Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 38448-38473. PMLR, 21-27 Jul 2024.   \n[7]  George Papamakarios, Theo Pavlakou, and Iain Murray. Masked autoregressive flow for density estimation. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.   \n[8]  Rajat Raina, Yirong Shen, Andrew Mccallum, and Andrew Ng. Classification with hybrid generative/discriminative models. Advances in neural information processing systems, 16, 2003.   \n[9]  Andrea M Tonello and Nunzio A Letizia. MIND: Maximum mutual information based neural decoder. IEEE Communications Letters,26(12):2954-2958,2022.   \n[10] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems, volume 27. Curran Associates, Inc., 2014.   \n[11] Ben Poole, Sherjil Ozair, Aaron Van Den Oord, Alex Alemi, and George Tucker. On variational bounds of mutual information. In International Conference on Machine Learning, pages 5171-5180. PMLR, 2019.   \n[12]  Igal Sason and Sergio Verdu. $f$ -divergence inequalities. IEEE Transactions on Information Theory, 62(11):5973-6006, 2016.   \n[13]  David McAllester and Karl Stratos. Formal limitations on the measurement of mutual information. In International Conference on Artificial Intelligence and Statistics, pages 875-884. PMLR, 2020.   \n[14]  Young-Il Moon, Balaji Rajagopalan, and Upmanu Lall. Estimation of mutual information using kernel density estimators. Phys. Rev. E, 52:2318-2321, Sep 1995.   \n[15]  Xianli Zeng, Yingcun Xia, and Howell Tong. Jackknife approach to the estimation of mutual information. Proceedings of the National Academy of Sciences, 115(40):9956-9961, 2018.   \n[16]  Alexander Kraskoy, Harald Stogbauer, and Peter Grassberger. Estimating mutual information. Phys. Rev. E, 69:066138, Jun 2004.   \n[17]  Kevin R Moon, Kumar Sricharan, and Alfred O Hero. Ensemble estimation of generalized mutual information with applications to genomics. IEEE Transactions on Information Theory, 67(9):5963-5996, 2021.   \n[18] XuanLong Nguyen, Martin J. Wainwright, and Michael I. Jordan. Estimating divergence functionals and the likelihood ratio by convex risk minimization. IEEE Transactions on Information Theory, 56(11):5847-5861, 2010.   \n[19] Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeshwar, Sherjil Ozair, Yoshua Bengio, Aaron Courville, and Devon Hjelm. Mutual information neural estimation. In International conference on machine learning, pages 531-540. PMLR, 2018.   \n[20] Jiaming Song and Stefano Ermon. Understanding the limitations of variational mutual information estimators. In 8th International Conference on Learning Representations, ICLR, Addis Ababa, Ethiopia, April 26-30, 2020, 2020.   \n[21] M. D. Donsker and S. R. S. Varadhan. Asymptotic evaluation of certain markov process expectations for large time. iv. Communications on Pure and Applied Mathematics, 36(2):183- 212, 1983.   \n[22]  Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv: 1807.03748, 2018.   \n[23] Kyungmin Lee and Jinwoo Shin. Renyicl: Contrastive representation learning with skew renyi divergence. In Advances in Neural Information Processing Systems, volume 35, pages 6463-6477. Curran Associates, Inc., 2022.   \n[24]  Yuval Shalev, Amichai Painsky, and Irad Ben-Gal. Neural joint entropy estimation. IEEE Transactions on Neural Networks and Learning Systems, 2022.   \n[25]  Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers using variational divergence minimization. In Advances in Neural Information Processing Systems, volume 29, 2016.   \n[26]  Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.   \n[27] Pawel Czyz, Frederic Grabowski, Julia E Vogt, Niko Beerenwinkel, and Alexander Marx. Beyond normal: On the evaluation of mutual information estimators. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[28]  Nunzio A. Letizia and Andrea M. Tonello. Copula density neural estimation. arXiv preprint arXiv:2211.15353, 2022.   \n[29] Subhashis Ghosal, Jayanta K. Ghosh, and Aad W. van der Vaart. Convergence rates of posterior distributions. The Annals of Statistics, 28(2):500 - 531, 2000.   \n[30] Noga Alon and Joel H Spencer. The probabilistic method. John Wiley & Sons, 2016.   \n[31]  Adam Paszke, Sam Gross, Soumith Chintala, and Gregory Chanan. Pytorch. https : //github . com/pytorch, 2016.   \n[32] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \n[33] Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.   \n[34] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv: 1708.07747, 2017. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Appendix: DIME Estimators ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In this section, we provide a concrete list of DIME estimators obtained using three different $f$ divergences. In particular, we maximize the value function defined in (5) ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathcal{T}_{f}(T)=\\mathbb{E}_{(\\mathbf{x},\\mathbf{y})\\sim p_{X Y}(\\mathbf{x},\\mathbf{y})}\\bigg[T\\big(\\mathbf{x},\\mathbf{y}\\big)-f^{*}\\bigg(T\\big(\\mathbf{x},\\sigma(\\mathbf{y})\\big)\\bigg)\\bigg],\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "over $T$ or its transformation. By doing that, and using (7) ", "page_idx": 12}, {"type": "equation", "text": "$$\nI(X;Y)=I_{f D I M E}(X;Y)=\\mathbb{E}_{(\\mathbf{x},\\mathbf{y})\\sim p_{X Y}(\\mathbf{x},\\mathbf{y})}\\bigg[\\log\\biggl(\\left(f^{*}\\right)^{\\prime}\\left(\\hat{T}(\\mathbf{x},\\mathbf{y})\\right)\\biggr)\\bigg],\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "we obtain a list of three different MI estimators. The list is used for both commenting on the impact Oof the $f$ function, referred to as the generator function, and for comparing the estimators discussed in Sec.2. ", "page_idx": 12}, {"type": "text", "text": "We consider the cases when $f$ is the generator of: ", "page_idx": 12}, {"type": "text", "text": "a) the KL divergence;   \nb) the GAN divergence;   \nc) the Hellinger distance squared. ", "page_idx": 12}, {"type": "text", "text": "We report below the derived value functions and the mathematical expressions of the proposed estimators. ", "page_idx": 12}, {"type": "text", "text": "A.1  KL divergence ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "The variational representation of the KL divergence [18] leads to the NWJ estimator in (28) when $f(u)\\,=\\,u\\log(u)$ . However, since we are interested in extracting the density ratio, we apply the transformation $\\dot{T}(\\mathbf{x})=\\log(D(\\mathbf{x}))$ . In this way, the lower bound introduced in (5) reads as follows ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathcal{I}_{K L}(D)=\\mathbb{E}_{(\\mathbf{x},\\mathbf{y})\\sim p_{X Y}(\\mathbf{x},\\mathbf{y})}\\bigg[\\mathrm{log}\\big(D\\big(\\mathbf{x},\\mathbf{y}\\big)\\big)\\bigg]-\\mathbb{E}_{(\\mathbf{x},\\mathbf{y})\\sim p_{X}(\\mathbf{x})p_{Y}(\\mathbf{y})}\\bigg[D\\big(\\mathbf{x},\\mathbf{y}\\big)\\bigg]+1,\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "which has to be maximized over positive discriminators $D(\\cdot)$ . As remarked before, we do not use ${\\mathcal{I}}_{K L}$ during the estimation, rather we define the KL-DIME estimator as ", "page_idx": 12}, {"type": "equation", "text": "$$\nI_{K L-D I M E}(X;Y):=\\mathbb{E}_{(\\mathbf{x},\\mathbf{y})\\sim p_{X Y}(\\mathbf{x},\\mathbf{y})}\\bigg[\\log\\biggl(\\hat{D}(\\mathbf{x},\\mathbf{y})\\biggr)\\bigg],\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "due to the fact that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\hat{D}(\\mathbf x,\\mathbf y)=\\arg\\operatorname*{max}_{D}\\mathcal{J}_{K L}(D)=\\frac{p_{X Y}(\\mathbf x,\\mathbf y)}{p_{X}(\\mathbf x)p_{Y}(\\mathbf y)}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "A.2 GAN divergence ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Following a similar approach, it is possible to define $f(u)=u\\log u-(u+1)\\log(u+1)+\\log4$ and $T(\\mathbf{x})^{\\bullet}=\\log(1-\\bar{D}(\\mathbf{x}))$ . We derive from Theorem 3.1 the GAN-DIME estimator obtained via maximization of ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{I}_{G A N}(D)=\\mathbb{E}_{(\\mathbf{x},\\mathbf{y})\\sim p_{X Y}(\\mathbf{x},\\mathbf{y})}\\bigg[\\mathrm{log}\\big(1-D\\big(\\mathbf{x},\\mathbf{y}\\big)\\big)\\bigg]+\\mathbb{E}_{(\\mathbf{x},\\mathbf{y})\\sim p_{X}(\\mathbf{x})p_{Y}(\\mathbf{y})}\\bigg[\\mathrm{log}\\big(D\\big(\\mathbf{x},\\mathbf{y}\\big)\\big)\\bigg]+\\mathrm{log}(4).}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "In fact, at the equilibrium we recover (3), hence ", "page_idx": 12}, {"type": "equation", "text": "$$\nI_{G A N-D I M E}(X;Y):=\\mathbb{E}_{(\\mathbf{x},\\mathbf{y})\\sim p_{X Y}(\\mathbf{x},\\mathbf{y})}\\Bigg[\\log\\Bigg(\\frac{1-\\hat{D}(\\mathbf{x},\\mathbf{y})}{\\hat{D}(\\mathbf{x},\\mathbf{y})}\\Bigg)\\Bigg].\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "A.3 Hellinger distance ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The last example we consider is the generator of the Hellinger distance squared $f(u)=(\\sqrt{u}-1)^{2}$ with thechange of variable $T(\\mathbf{x})=1-D(\\mathbf{x})$ . After simple manipulations, we obtain the associated valuefunctionas ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathcal{I}_{H D}(D)=2-\\mathbb{E}_{(\\mathbf{x},\\mathbf{y})\\sim p\\wedge\\mathbf{\\boldsymbol{Y}}(\\mathbf{x},\\mathbf{y})}\\bigg[D\\big(\\mathbf{x},\\mathbf{y}\\big)\\bigg]-\\mathbb{E}_{(\\mathbf{x},\\mathbf{y})\\sim p\\wedge\\mathbf{\\boldsymbol{x}}(\\mathbf{x})p\\wedge\\mathbf{\\boldsymbol{y}}}\\bigg[\\frac{1}{D\\big(\\mathbf{x},\\mathbf{y}\\big)}\\bigg],\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "which is maximized for ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\hat{D}({\\mathbf x},{\\mathbf y})=\\arg\\operatorname*{max}_{D}\\mathcal{I}_{H D}(D)=\\sqrt{\\frac{p_{X}({\\mathbf x})p_{Y}({\\mathbf y})}{p_{X Y}({\\mathbf x},{\\mathbf y})}},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "leading to the HD-DIME estimator ", "page_idx": 13}, {"type": "equation", "text": "$$\nI_{H D-D I M E}(X;Y):=\\mathbb{E}_{(\\mathbf{x},\\mathbf{y})\\sim p_{X Y}(\\mathbf{x},\\mathbf{y})}\\bigg[\\mathrm{log}\\bigg(\\frac{1}{\\hat{D}^{2}(\\mathbf{x},\\mathbf{y})}\\bigg)\\bigg].\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Given that these estimators comprise only one expectation over the joint samples, their variance has different properties compared to the variational ones requiring the partition term such as MINE and NWJ. ", "page_idx": 13}, {"type": "text", "text": "B Appendix: Related Work Mutual Information Estimators ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we provide a detailed description of the formulas of the MI estimators we summarized inSec.2. ", "page_idx": 13}, {"type": "text", "text": "B.1 MINE ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The Donsker- Varadhan dual representation of the KL divergence [11, 21] produces an estimate of the MI using the bound optimized by the mutual information neural estimator (MINE) [19] ", "page_idx": 13}, {"type": "equation", "text": "$$\nI_{M I N E}(X;Y)=\\operatorname*{sup}_{\\theta\\in\\Theta}\\mathbb{E}_{({\\mathbf{x}},{\\mathbf{y}})\\sim p_{X Y}({\\mathbf{x}},{\\mathbf{y}})}[T_{\\theta}({\\mathbf{x}},{\\mathbf{y}})]-\\log(\\mathbb{E}_{({\\mathbf{x}},{\\mathbf{y}})\\sim p_{X}({\\mathbf{x}})p_{Y}({\\mathbf{y}})}[e^{T_{\\theta}({\\mathbf{x}},{\\mathbf{y}})}]),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\theta\\in\\Theta$ parameterizes a family of functions $T_{\\theta}:\\mathcal{X}\\times\\mathcal{Y}\\rightarrow\\mathbb{R}$ through the use of a deep neural network. However, the logarithm before the expectation in the second term renders MINE a biased estimator. To avoid biased gradients, the authors in [19] suggested to replace the partition function $\\mathbb{E}_{p x p Y}[e^{T_{\\theta}}]$ with an exponential moving average over mini-data-batches. ", "page_idx": 13}, {"type": "text", "text": "B.2NWJ ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Another VLB is based on the KL divergence dual representation introduced in [18] (also referred to as $f$ MINE in [19]) ", "page_idx": 13}, {"type": "equation", "text": "$$\nI_{N W J}(X;Y)=\\operatorname*{sup}_{\\theta\\in\\Theta}\\mathbb{E}_{(\\mathbf{x},\\mathbf{y})\\sim p_{X Y}(\\mathbf{x},\\mathbf{y})}[T_{\\theta}(\\mathbf{x},\\mathbf{y})]-\\mathbb{E}_{(\\mathbf{x},\\mathbf{y})\\sim p_{X}(\\mathbf{x})p_{Y}(\\mathbf{y})}[e^{T_{\\theta}(\\mathbf{x},\\mathbf{y})-1}].\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Although for a fixed $T$ MINE provides a tighter bound $I_{M I N E}\\,\\geq\\,I_{N W J}$ , the NWJ estimator is unbiased. ", "page_idx": 13}, {"type": "text", "text": "B.3 SMILE ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Both MINE and NWJ suffer from high-variance estimations and to combat such a limitation, the SMILE estimator was introduced in [20]. It is defined as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathit{\\Pi}_{S M I L E}(X;Y)=\\operatorname*{sup}_{\\theta\\in\\Theta}\\mathbb{E}_{(\\mathbf{x},\\mathbf{y})\\sim p_{X Y}(\\mathbf{x},\\mathbf{y})}[T_{\\theta}(\\mathbf{x},\\mathbf{y})]-\\log(\\mathbb{E}_{(\\mathbf{x},\\mathbf{y})\\sim p_{X}(\\mathbf{x})p_{Y}(\\mathbf{y})}[\\mathrm{clip}(e^{T_{\\theta}(\\mathbf{x},\\mathbf{y})},e^{-\\tau},e^{\\tau})]),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\operatorname{clip}(v,l,u)=\\operatorname*{max}(\\operatorname*{min}(v,u),l)$ and it helps to obtain smoother partition functions estimates. SMILE is equivalent to MINE in the limit $\\tau\\rightarrow+\\infty$ ", "page_idx": 13}, {"type": "text", "text": "B.4CPC ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The MI estimator based on contrastive predictive coding (CPC) [22] is defined as ", "page_idx": 14}, {"type": "equation", "text": "$$\nI_{C P C}(X;Y)=\\mathbb{E}_{(\\mathbf{x},\\mathbf{y})\\sim p_{X Y,N}(\\mathbf{x},\\mathbf{y})}\\bigg[\\frac{1}{N}\\sum_{i=1}^{N}\\log\\biggl(\\frac{e^{T_{\\theta}(\\mathbf{x}_{i},\\mathbf{y}_{i})}}{\\frac{1}{N}\\sum_{j=1}^{N}e^{T_{\\theta}(\\mathbf{x}_{i},\\mathbf{y}_{j})}}\\biggr)\\bigg],\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $N$ is the batch size and $p_{X Y,N}$ denotes the joint distribution of $N$ i.i.d. random variables sampled from $p_{X Y}$ . CPC provides low variance estimates but it is upper bounded by $\\log N$ , resulting in a biased estimator. ", "page_idx": 14}, {"type": "text", "text": "B.5 NJEE ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The neural joint entropy estimator (NJEE) proposed in [24] is based on a classification task. Let $X_{m}$ be the $m$ -th component of $X$ , with $m\\leq d$ and $N$ the batch size. $X^{k}$ is the vector containing the first $k$ components of $X$ . Let $\\hat{H}_{N}(X_{1})$ be the estimated marginal entropy of the first components in $X$ and let $G_{\\theta_{m}}(X_{m}|X^{m-1})$ be a neural network classifier, where the input is $X^{m-1}$ and the label used .s $X_{m}$ .If $\\mathrm{CE}(\\cdot)$ is the cross-entropy function, then the MI estimator based on NJEE is defined as ", "page_idx": 14}, {"type": "equation", "text": "$$\nI_{N J E E}(X;Y)=\\hat{H}_{N}(X_{1})+\\sum_{m=2}^{d}\\operatorname{CE}(G_{\\theta_{m}}(X_{m}|X^{m-1}))-\\sum_{m=1}^{d}\\operatorname{CE}(G_{\\theta_{m}}(X_{m}|Y,X^{m-1})),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the first two terms of the RHS constitutes the NJEE entropy estimator. ", "page_idx": 14}, {"type": "text", "text": "C  Appendix: Proofs of Lemmas and Theorems ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "C.1 Proof of Theorem 3.1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Theorem 3.1. Let $(X,Y)\\sim p_{X Y}(\\mathbf{x},\\mathbf{y})$ be a pair of multivariate random variables. Let $\\sigma(\\cdot)$ be a permutation function such that $p_{\\sigma(Y)}(\\sigma(\\mathbf{y})|\\mathbf{x})=p_{Y}(\\mathbf{y})$ and $T:\\mathrm{dom}(X)\\times\\mathrm{dom}(Y)\\to\\mathbb{R}$ Let $f^{*}$ be the Fenchel conjugate of $f:\\mathbb{R_{+}}\\rightarrow\\mathbb{R},$ a convex lower semicontinuous function that satisfies $f(1)=0$ with derivative $f^{\\prime}$ f $\\mathcal{I}_{f}(T)$ is a value function defined as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{T}_{f}(T)=\\mathbb{E}_{(\\mathbf{x},\\mathbf{y})\\sim p_{X Y}(\\mathbf{x},\\mathbf{y})}\\bigg[T\\big(\\mathbf{x},\\mathbf{y}\\big)-f^{*}\\bigg(T\\big(\\mathbf{x},\\sigma(\\mathbf{y})\\big)\\bigg)\\bigg],\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "then ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\hat{T}(\\mathbf x,\\mathbf y)=\\arg\\operatorname*{max}_{T}\\mathcal{I}_{f}(T)=f^{\\prime}\\bigg(\\frac{p_{X Y}(\\mathbf x,\\mathbf y)}{p_{X}(\\mathbf x)p_{Y}(\\mathbf y)}\\bigg),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and ", "page_idx": 14}, {"type": "equation", "text": "$$\nI(X;Y)=I_{f D I M E}(X;Y)=\\mathbb{E}_{(\\mathbf{x},\\mathbf{y})\\sim p_{X Y}(\\mathbf{x},\\mathbf{y})}\\bigg[\\log\\biggl(\\left(f^{*}\\right)^{\\prime}\\bigl(\\hat{T}(\\mathbf{x},\\mathbf{y})\\bigr)\\biggr)\\bigg].\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. From the hypothesis, the value function can be rewritten as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{I}_{f}(T)=\\mathbb{E}_{(\\mathbf{x},\\mathbf{y})\\sim p_{X Y}(\\mathbf{x},\\mathbf{y})}\\bigg[T\\big(\\mathbf{x},\\mathbf{y}\\big)\\bigg]-\\mathbb{E}_{(\\mathbf{x},\\mathbf{y})\\sim p_{X}(\\mathbf{x})p_{Y}(\\mathbf{y})}\\bigg[f^{*}\\bigg(T\\big(\\mathbf{x},\\mathbf{y}\\big)\\bigg)\\bigg].\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The thesis follows immediately from Lemma 1 of [18]. Indeed, the $f$ -divergence $D_{f}$ can be expressed in terms of its lower bound via Fenchel convex duality ", "page_idx": 14}, {"type": "equation", "text": "$$\nD_{f}(P||Q)\\geq\\operatorname*{sup}_{T\\in\\mathbb{R}}\\biggl\\{\\mathbb{E}_{x\\sim p(x)}\\bigl[T(x)\\bigr]-\\mathbb{E}_{x\\sim q(x)}\\bigl[f^{*}\\bigl(T(x)\\bigr)\\bigr]\\biggr\\},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $T:\\mathcal{X}\\rightarrow\\mathbb{R}$ and $f^{*}$ is the Fenchel conjugate of $f$ defined as ", "page_idx": 14}, {"type": "equation", "text": "$$\nf^{*}(t):=\\operatorname*{sup}_{u\\in\\mathbb{R}}\\{u t-f(u)\\}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Therein, it was shown that the bound in (36) is tight for optimal values of $T(x)$ and it takes the following form ", "page_idx": 14}, {"type": "equation", "text": "$$\n{\\hat{T}}(x)=f^{\\prime}\\!\\left({\\frac{p(x)}{q(x)}}\\right),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $f^{\\prime}$ is the derivative of $f$ ", "page_idx": 15}, {"type": "text", "text": "The MI $I(X;Y)$ admits the KL divergence representation ", "page_idx": 15}, {"type": "equation", "text": "$$\nI(X;Y)=D_{K L}\\big(p_{X Y}||p_{X}p_{Y}\\big),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and since the inverse of the derivative of $f$ is the derivative of the conjugate $f^{*}$ , the density ratio can be rewritten in terms of the optimum discriminator $\\hat{T}$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\bigl(f^{\\prime}\\bigr)^{-1}\\bigl(\\hat{T}({\\mathbf{x}},{\\mathbf{y}})\\bigr)=\\bigl(f^{*}\\bigr)^{\\prime}\\bigl(\\hat{T}({\\mathbf{x}},{\\mathbf{y}})\\bigr)=\\frac{p_{X Y}({\\mathbf{x}},{\\mathbf{y}})}{p_{X}({\\mathbf{x}})p_{Y}({\\mathbf{y}})}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "$f$ -DIME finally reads as follows ", "page_idx": 15}, {"type": "equation", "text": "$$\nI_{f D I M E}(X;Y)=\\mathbb{E}_{(\\mathbf{x},\\mathbf{y})\\sim p_{X Y}(\\mathbf{x},\\mathbf{y})}\\bigg[\\mathrm{log}\\bigg(\\big(f^{*}\\big)^{\\prime}\\big(\\hat{T}(\\mathbf{x},\\mathbf{y})\\big)\\bigg)\\bigg].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "C.2 Proof of Lemma 3.2 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Lemma 3.2. Let the discriminator $T(\\cdot)$ be with enough capacity, i.e., in the non parametric limit. Consider the problem ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\hat{T}=\\mathrm{\\arg\\,max}\\,\\mathcal{J}_{f}(T)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{I}_{f}(T)=\\mathbb{E}_{(\\mathbf{x},\\mathbf{y})\\sim p_{X Y}(\\mathbf{x},\\mathbf{y})}\\bigg[T\\big(\\mathbf{x},\\mathbf{y}\\big)\\bigg]-\\mathbb{E}_{(\\mathbf{x},\\mathbf{y})\\sim p_{X}(\\mathbf{x})p_{Y}(\\mathbf{y})}\\bigg[f^{*}\\bigg(T\\big(\\mathbf{x},\\mathbf{y}\\big)\\bigg)\\bigg],\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and the update rule based on the gradient descent method ", "page_idx": 15}, {"type": "equation", "text": "$$\nT^{(n+1)}=T^{(n)}+\\mu\\nabla\\mathcal{I}_{f}(T^{(n)}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "If the gradient descent method converges to the global optimum $\\hat{T}$ the mutual information estimator ", "page_idx": 15}, {"type": "equation", "text": "$$\nI(X;Y)=I_{f D I M E}(X;Y)=\\mathbb{E}_{(\\mathbf{x},\\mathbf{y})\\sim p_{X Y}(\\mathbf{x},\\mathbf{y})}\\bigg[\\log\\biggl(\\left(f^{*}\\right)^{\\prime}\\bigl(\\hat{T}(\\mathbf{x},\\mathbf{y})\\bigr)\\biggr)\\bigg].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "converges to the real value of the mutual information $I(X;Y)$ ", "page_idx": 15}, {"type": "text", "text": "Proof. For convenience of notation, let the instantaneous MI be the random variable defined as ", "page_idx": 15}, {"type": "equation", "text": "$$\ni(X;Y):=\\log\\!\\left(\\frac{p_{X Y}(\\mathbf{x},\\mathbf{y})}{p_{X}(\\mathbf{x})p_{Y}(\\mathbf{y})}\\right)\\!.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "It is straightforward to notice that the MI corresponds to the expected value of $i(X;Y)$ over the joint distribution $p_{X Y}$ . The solution to (42) is given by (6) of Theorem 3.1. Let $\\delta^{(n)}=\\hat{T}-T^{(n)}$ be the displacement between the optimum discriminator $\\hat{T}$ and the obtained one $T^{(n)}$ at the iteration $n$ , then ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{i}_{n,f D I M E}(X;Y)=\\log\\biggl(\\bigl(f^{*}\\bigr)^{\\prime}\\bigl(T^{(n)}(\\mathbf{x},\\mathbf{y})\\bigr)\\biggr)=\\log\\biggl(R^{(n)}(\\mathbf{x},\\mathbf{y})\\biggr),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $R^{(n)}({\\bf x},{\\bf y})$ represents the estimated density ratio at the $n$ -th iteration and it is related with the optimum ratio $\\hat{R}({\\bf x},{\\bf y})$ as follows ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{R}-R^{(n)}=\\bigl(f^{*}\\bigr)^{\\prime}\\bigl(\\hat{T}\\bigr)-\\bigl(f^{*}\\bigr)^{\\prime}\\bigl(T^{(n)}\\bigr)}\\\\ &{\\qquad\\qquad=\\bigl(f^{*}\\bigr)^{\\prime}\\bigl(\\hat{T}\\bigr)-\\bigl(f^{*}\\bigr)^{\\prime}\\bigl(\\hat{T}-\\delta^{(n)}\\bigr)}\\\\ &{\\qquad\\qquad\\simeq\\delta^{(n)}\\cdot\\biggl[\\bigl(f^{*}\\bigr)^{\\prime\\prime}\\bigl(\\hat{T}-\\delta^{(n)}\\bigr)\\biggr],}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the last step follows from a first order Taylor expansion in $\\hat{T}-\\delta^{(n)}$ .Therefore, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{i}_{n,f D I M E}(X;Y)=\\log\\big(R^{(n)}\\big)}\\\\ &{=\\log\\Bigg(\\big(\\hat{R}\\big)\\bigg(1-\\delta^{(n)}\\cdot\\frac{\\big(f^{*}\\big)^{\\prime\\prime}\\big(\\hat{T}-\\delta^{(n)}\\big)}{\\big(f^{*}\\big)^{\\prime}\\big(\\hat{T}\\big)}\\bigg)\\Bigg)}\\\\ &{=i(X;Y)+\\log\\Bigg(1-\\delta^{(n)}\\cdot\\frac{\\big(f^{*}\\big)^{\\prime\\prime}\\big(\\hat{T}-\\delta^{(n)}\\big)}{\\big(f^{*}\\big)^{\\prime}\\big(\\hat{T}\\big)}\\Bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "If the gradient descent method converges towards the optimum solution $\\hat{T},\\,\\delta^{(n)}\\to0$ and ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{i}_{n,f D I M E}(X;Y)\\simeq i(X;Y)-\\delta^{(n)}\\cdot\\Bigg[\\frac{\\big(f^{*}\\big)^{\\prime\\prime}\\big(\\hat{T}-\\delta^{(n)}\\big)}{\\big(f^{*}\\big)^{\\prime}\\big(\\hat{T}\\big)}\\Bigg]}\\\\ &{\\simeq i(X;Y)-\\delta^{(n)}\\cdot\\Bigg[\\frac{\\big(f^{*}\\big)^{\\prime\\prime}\\big(\\hat{T}\\big)}{\\big(f^{*}\\big)^{\\prime}\\big(\\hat{T}\\big)}\\Bigg]}\\\\ &{=i(X;Y)-\\delta^{(n)}\\cdot\\Bigg[\\frac{\\mathrm{d}}{\\mathrm{d}T}\\log\\big(\\big(f^{*}\\big)^{\\prime}(T)\\big)\\Bigg|_{T=\\hat{T}}\\Bigg],}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the RHS is itself a first order Taylor expansion of the instantaneous MI in $\\hat{T}$ . In the asymptotic limit $\\!\\left(n\\to+\\infty\\right)$ , it holds also for the expected values that ", "page_idx": 16}, {"type": "equation", "text": "$$\n|I(X;Y)-\\hat{I}_{n,f D I M E}(X;Y)|\\rightarrow0.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "C.3Proof of Lemma 4.1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Lemma 4.1. Let $\\hat{R}=p_{X Y}({\\bf x},{\\bf y})/(p_{X}({\\bf x})p_{Y}({\\bf y}))$ and assume $W_{{a r}_{p_{X Y}}}[\\log{\\hat{R}}]$ exists. Let $p_{X Y}^{M}$ be the empiricaldistributionof $M$ i.i.d. samplesfrom $p_{X Y}$ and let $\\mathbb{E}_{p_{X Y}^{M}}$ denote the sample average over $p_{X Y}^{M}$ .Then, under the randomness of the sampling procedure, it holds that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\small\\mathit{V a r}_{p_{X Y}}\\left[\\mathbb{E}_{p_{X Y}^{M}}[\\log\\hat{R}]\\right]\\leq\\frac{4H^{2}(p_{X Y},p_{X}p_{Y})\\big|\\big|\\hat{R}\\big|\\big|_{\\infty}-I^{2}(X;Y)}{M}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $H^{2}$ is the Hellinger distance squared defined as ", "page_idx": 16}, {"type": "equation", "text": "$$\nH^{2}(p,q)=\\int_{\\mathbf{x}}\\left(\\sqrt{p(\\mathbf{x})}-\\sqrt{q(\\mathbf{x})}\\right)^{2}\\mathrm{d}\\mathbf{x},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and the infinity norm is defined as $\\begin{array}{r}{||f(x)||_{\\infty}:=\\operatorname*{sup}_{x\\in\\mathbb{R}}|f(x)|}\\end{array}$ ", "page_idx": 16}, {"type": "text", "text": "Proof. Consider the variance of $\\hat{R}({\\bf x},{\\bf y})$ when $(\\mathbf{x},\\mathbf{y})\\sim p_{X Y}(\\mathbf{x},\\mathbf{y})$ , then ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname{Var}_{p_{X Y}}\\left[\\log\\hat{R}\\right]=\\mathbb{E}_{p_{X Y}}\\left[\\left(\\log\\frac{p_{X Y}}{p_{X}p_{Y}}\\right)^{2}\\right]-\\left(\\mathbb{E}_{p_{X Y}}\\left[\\log\\frac{p_{X Y}}{p_{X}p_{Y}}\\right]\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The power of the log-density ratio is upper bounded as follows (see the approach of Lemma 8.3 in [29]] ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}_{p_{X Y}}\\left[\\left(\\log\\frac{p_{X Y}}{p_{X}p_{Y}}\\right)^{2}\\right]\\leq4H^{2}(p_{X Y},p_{X}p_{Y})\\left|\\left|\\frac{p_{X Y}}{p_{X}p_{Y}}\\right|\\right|_{\\infty},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "while the mean squared is the ground-truth MI squared, thus ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname{Var}_{p_{X Y}}[\\log\\hat{R}]\\leq4H^{2}(p_{X Y},p_{X}p_{Y})\\bigg|\\bigg|\\frac{p_{X Y}}{p_{X}p_{Y}}\\bigg|\\bigg|_{\\infty}-I^{2}(X;Y).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Finally, the variance of the mean of $M$ i.i.d. random variables yields the thesis ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{Var}_{p_{X Y}}\\left[\\mathbb{E}_{p_{X Y}}[\\log\\hat{R}]\\right]=\\frac{\\mathrm{Var}_{p_{X Y}}[\\log\\hat{R}]}{M}\\leq\\frac{4H^{2}(p_{X Y},p_{X}p_{Y})\\left|\\left|\\frac{p_{X Y}}{p_{X}p_{Y}}\\right|\\right|_{\\infty}-I^{2}(X;Y)}{M}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "C.4 Proof of Lemma 4.2 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Lemma 4.2. Let $\\hat{R}$ be the optimal density ratio and let $X\\sim\\mathcal{N}(0,\\sigma_{X}^{2})$ and $N\\sim\\mathcal{N}(0,\\sigma_{N}^{2})$ be uncorrelated scalar Gaussian random variables such that $Y=X+N$ Assume $W a r_{p_{X Y}}[\\log{\\hat{R}}]$ exists. Let $p_{X Y}^{M}$ be the empirical distribution of $M$ i.i.d. samples from $p_{X Y}$ and let $\\mathbb{E}_{p_{X Y}^{M}}$ denote the sample average over $p_{X Y}^{M}$ Then,ueramft al ", "page_idx": 17}, {"type": "equation", "text": "$$\nW\\!a r_{p_{X Y}}\\left[\\mathbb{E}_{p_{X Y}^{M}}[\\log\\hat{R}]\\right]=\\frac{1-e^{-2I(X;Y)}}{M}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. From the hypothesis, the density ratio can be rewritten as $\\hat{R}=p_{N}(y-x)/p_{Y}(y)$ and the output variance is clearly equal to $\\sigma_{Y}^{2}=\\overline{{\\sigma_{X}^{2}}}+\\sigma_{N}^{2}$ Notice that this is equivalent of having corelated random variables $X$ and $Y$ with correlation coefficient $\\rho$ since it is enough to study the case $\\sigma_{X}=\\rho$ and $\\sigma_{N}=\\sqrt{1-\\rho^{2}}$ ", "page_idx": 17}, {"type": "text", "text": "It is easy to verify via simple calculations that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I(X;Y)=\\mathbb{E}_{p_{X Y}}[\\log\\hat{R}]}\\\\ &{\\quad\\quad\\quad=\\log\\frac{\\sigma_{Y}}{\\sigma_{N}}+\\mathbb{E}_{p_{X Y}}\\left[\\frac{y^{2}}{2\\sigma_{Y}^{2}}-\\frac{(y-x)^{2}}{2\\sigma_{N}^{2}}\\right]}\\\\ &{\\quad\\quad\\quad=-\\cdot\\cdot=\\log\\frac{\\sigma_{Y}}{\\sigma_{N}}=\\frac{1}{2}\\log\\!\\left(1+\\frac{\\sigma_{X}^{2}}{\\sigma_{N}^{2}}\\right)=-\\frac{1}{2}\\log\\!\\left(1-\\rho^{2}\\right)\\!.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Similarly, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname{Var}_{p\\boldsymbol{X}\\boldsymbol{Y}}\\Big[\\log\\hat{R}\\Big]=\\mathbb{E}_{p\\boldsymbol{X}\\boldsymbol{Y}}\\Bigg[\\Bigg(\\log\\bigg(\\frac{\\sigma_{\\boldsymbol{Y}}}{\\sigma_{\\boldsymbol{N}}}\\bigg)+\\frac{y^{2}}{2\\sigma_{\\boldsymbol{Y}}^{2}}-\\frac{(y-x)^{2}}{2\\sigma_{\\boldsymbol{N}}^{2}}\\bigg)^{2}\\Bigg]-I^{2}(\\boldsymbol{X};\\boldsymbol{Y})}\\\\ &{=\\frac{1}{4}\\mathbb{E}_{p\\boldsymbol{X}\\boldsymbol{Y}}\\Bigg[\\bigg(\\frac{y}{\\sigma_{\\boldsymbol{N}}}\\bigg)^{4}+\\bigg(\\frac{y}{\\sigma_{\\boldsymbol{Y}}}\\bigg)^{4}-2\\bigg(\\frac{y}{\\sigma_{\\boldsymbol{Y}}}\\bigg)^{2}\\bigg(\\frac{y-x}{\\sigma_{\\boldsymbol{N}}}\\bigg)^{2}\\Bigg]}\\\\ &{=\\cdots=\\operatorname{Kur}[Z]\\bigg(\\frac{1}{2}-\\frac{\\sigma_{\\boldsymbol{N}}^{2}}{2\\sigma_{\\boldsymbol{Y}}^{2}}\\bigg)-\\frac{\\sigma_{\\boldsymbol{X}}^{2}}{2\\sigma_{\\boldsymbol{Y}}^{2}}}\\\\ &{=\\frac{\\sigma_{\\boldsymbol{X}}^{2}}{\\sigma_{\\boldsymbol{Y}}^{2}}=1-\\frac{\\sigma_{\\boldsymbol{N}}^{2}}{\\sigma_{\\boldsymbol{Y}}^{2}}=1-e^{-2I(\\boldsymbol{X};\\boldsymbol{Y})}=\\rho^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the last steps use the fact that the Kurtosis of a normal distribution is 3 and that the MI can be expressed as in (59). Finally, the variance of the mean of $M$ i.i.d. random variables yields the thesis ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname{Var}_{p_{X Y}}\\left[\\mathbb{E}_{p_{X Y}^{M}}[\\log\\hat{R}]\\right]=\\frac{\\operatorname{Var}_{p_{X Y}}[\\log\\hat{R}]}{M}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "f\uff1a $X$ and $N$ are multivariate Gaussians with diagonal covariance matrices $\\rho^{2}\\mathbb{I}_{d\\times d}$ and $(1-\\rho^{2})\\mathbb{I}_{d\\times d}$ the results for both the MI and variance in the scalar case are simply multiplied by $d$ ", "page_idx": 17}, {"type": "text", "text": "C.5  Proof of Lemma 5.1 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Lemma 5.1. Let $\\left(\\mathbf{x}_{i},\\mathbf{y}_{i}\\right)$ \uff0c $\\forall i\\in\\{1,\\ldots,N\\}$ , be $N$ data points. Let $\\mathcal{I}_{f}(T)$ be the value function in (5). Let $\\mathcal{I}_{f}^{\\pi}(T)$ and $\\mathcal{I}_{f}^{\\sigma}(T)$ be numerical implementations of $\\mathcal{I}_{f}(T)$ using a random permutation and a random derangement of y, respectively. Denote with $K$ the number of points $\\mathbf{y}_{k}$ with $k\\in\\{1,\\ldots,N\\}$ , in the same position after the permutation (i.e., the fixed points). Then ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{J}_{f}^{\\pi}(T)\\leq\\frac{N-K}{N}\\mathcal{J}_{f}^{\\sigma}(T).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "ProofDefine $\\mathcal{I}_{f}^{\\pi}(T)$ as the Monte Carlo implementation of $\\mathcal{I}_{f}(T)$ when using the permutation function $\\pi(\\cdot)$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{I}_{f}^{\\pi}(T)=\\frac{1}{N}\\sum_{i=1}^{N}T(\\mathbf x_{i},\\mathbf y_{i})-\\frac{1}{N}\\sum_{i=1}^{N}f^{*}\\big(T(\\mathbf x_{i},\\mathbf y_{j})\\big),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the pair $(\\mathbf{x}_{i},\\mathbf{y}_{j})$ is obtained via a random permutation of the elements of $\\mathbf{y}$ as $j\\,=\\,\\pi(i)$ $\\forall i\\in\\{1,\\ldots,N\\}$ . Since $K$ is a non-negative integer representing the number of fixed points $i=\\pi(i)$ the value function can be rewritten as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{I}_{f}^{\\pi}(T)=\\frac{1}{N}\\sum_{i=1}^{N}T(\\mathbf{x}_{i},\\mathbf{y}_{i})-\\frac{1}{N}\\sum_{i=1}^{K}f^{*}\\big(T(\\mathbf{x}_{i},\\mathbf{y}_{i})\\big)-\\frac{1}{N}\\sum_{i=1}^{N-K}f^{*}\\big(T(\\mathbf{x}_{i},\\mathbf{y}_{j\\neq i})\\big),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which can also be expressed as ", "page_idx": 18}, {"type": "equation", "text": "$$\n7_{f}^{\\pi}(T)={\\frac{1}{N}}\\sum_{i=1}^{K}T(\\mathbf{x}_{i},\\mathbf{y}_{i})+{\\frac{1}{N}}\\sum_{i=1}^{N-K}T(\\mathbf{x}_{i},\\mathbf{y}_{i})-{\\frac{1}{N}}\\sum_{i=1}^{K}f^{*}\\left(T(\\mathbf{x}_{i},\\mathbf{y}_{i})\\right)-{\\frac{1}{N}}\\sum_{i=1}^{N-K}f^{*}\\left(T(\\mathbf{x}_{i},\\mathbf{y}_{j\\neq i})\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "In (65) it is possible to recognize that the second and last term of the RHS constitutes the numerical implementation of $\\mathcal{I}_{f}(T)$ using a derangement strategy on $N-K$ elements, so that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{I}_{f}^{\\pi}(T)=\\frac{1}{N}\\sum_{i=1}^{K}T(\\mathbf{x}_{i},\\mathbf{y}_{i})-\\frac{1}{N}\\sum_{i=1}^{K}f^{*}\\big(T(\\mathbf{x}_{i},\\mathbf{y}_{i})\\big)+\\frac{N-K}{N}J_{f}^{\\sigma}(T).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "However, by definition of Fenchel conjugate ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{1}{N}\\sum_{i=1}^{K}T(\\mathbf x_{i},\\mathbf y_{i})-f^{*}\\big(T(\\mathbf x_{i},\\mathbf y_{i})\\big)\\leq0,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "since for $t=1$ ", "page_idx": 18}, {"type": "equation", "text": "$$\nu-f^{*}(u)\\leq u-\\left(u t-f(t)\\right)=f(1)=0.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Hence, we can conclude that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{J}_{f}^{\\pi}(T)\\leq\\frac{N-K}{N}J_{f}^{\\sigma}(T).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "C.6Proof of Theorem 5.2 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Theorem 5.2. Let the discriminator $D(\\cdot)$ be with enough capacity. Let $N$ be the batch size and $f$ be the generator of the $K L$ divergence. Let $\\mathcal{I}_{K L}^{\\pi}(D)$ be defined as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{I}_{K L}^{\\pi}(D)=\\mathbb{E}_{(\\mathbf{x},\\mathbf{y})\\sim p_{X Y}(\\mathbf{x},\\mathbf{y})}\\bigg[\\mathrm{log}\\bigg(D\\big(\\mathbf{x},\\mathbf{y}\\big)\\bigg)-f^{*}\\bigg(\\mathrm{log}\\bigg(D\\big(\\mathbf{x},\\pi(\\mathbf{y})\\big)\\bigg)\\bigg)\\bigg].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Denotewith $K$ the number of indices inthe same position afterthe permutation (i.e., the fixed points) andwith $R(\\mathbf{x},\\mathbf{y})$ the density ratio in (2). Then, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\hat{D}({\\mathbf x},{\\mathbf y})=\\arg\\operatorname*{max}_{D}\\mathcal{J}_{K L}^{\\pi}(D)=\\frac{N R({\\mathbf x},{\\mathbf y})}{K R({\\mathbf x},{\\mathbf y})+N-K}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. The idea of the proof is to express ${\\mathcal{I}}_{K L}^{\\pi}(D)$ via Monte Carlo approximation, in order to rearrange fixed points, and then go back to Lebesgue integration. The value function $\\mathcal{I}_{K L}(D)$ canbe Writtenas ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{I}_{K L}(D)=\\mathbb{E}_{(\\mathbf{x},\\mathbf{y})\\sim p_{X Y}(\\mathbf{x},\\mathbf{y})}\\bigg[\\mathrm{log}\\big(D(\\mathbf{x},\\mathbf{y})\\big)\\bigg]-\\mathbb{E}_{(\\mathbf{x},\\mathbf{y})\\sim p_{X}(\\mathbf{x})p_{Y}(\\mathbf{y})}\\bigg[D\\big(\\mathbf{x},\\mathbf{y}\\big)\\bigg]+1.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Similarly to (64), we can express $\\mathcal{I}_{K L}^{\\pi}(D)$ as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{I}_{K L}^{\\pi}(D)=\\frac{1}{N}\\sum_{i=1}^{N}\\log\\bigl(D(\\mathbf{x}_{i},\\mathbf{y}_{i})\\bigr)-\\frac{1}{N}\\sum_{i=1}^{K}D(\\mathbf{x}_{i},\\mathbf{y}_{i})-\\frac{1}{N}\\sum_{i=1}^{N-K}D(\\mathbf{x}_{i},\\mathbf{y}_{j\\neq i})+1,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $K$ is the number of fixed points of the permutation $j\\,=\\,\\pi(i),\\forall i\\,\\in\\,\\{1,\\dots,N\\}$ .However, when $N\\rightarrow\\infty$ , we can use Lebesgue integration and rewrite (73) as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathcal{I}_{K L}^{\\pi}(D)=\\int_{\\mathbf{x}}\\int_{\\mathbf{y}}\\bigg(p_{X Y}(\\mathbf{x},\\mathbf{y})\\log\\big(D(\\mathbf{x},\\mathbf{y})\\big)-\\frac{K}{N}p_{X Y}(\\mathbf{x},\\mathbf{y})D(\\mathbf{x},\\mathbf{y})\\bigg)\\,\\mathrm{d}\\mathbf{x}\\,\\mathrm{d}\\mathbf{y}}\\\\ {\\displaystyle-\\int_{\\mathbf{x}}\\int_{\\mathbf{y}}\\frac{N-K}{N}p_{X}(\\mathbf{x})p_{Y}(\\mathbf{y})D(\\mathbf{x},\\mathbf{y})\\,\\mathrm{d}\\mathbf{x}\\,\\mathrm{d}\\mathbf{y}+1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "To maximize ${\\mathcal{I}}_{K L}^{\\pi}(D)$ , it is enough to take the derivative of the integrand with respect to $D$ and equate it to $0$ , yielding the following equation in $D$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n{\\frac{p_{X Y}(\\mathbf{x},\\mathbf{y})}{D(\\mathbf{x},\\mathbf{y})}}-{\\frac{K}{N}}p_{X Y}(\\mathbf{x},\\mathbf{y})-{\\frac{N-K}{N}}p_{X}(\\mathbf{x})p_{Y}(\\mathbf{y})=0.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Solving for $D$ leads to the thesis ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\hat{D}(\\mathbf x,\\mathbf y)=\\frac{N R(\\mathbf x,\\mathbf y)}{K R(\\mathbf x,\\mathbf y)+N-K},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "since $\\mathcal{I}_{K L}^{\\pi}(\\hat{D})$ is a maximum being the second derivative w.r.t. $D$ a non-positive function. ", "page_idx": 19}, {"type": "text", "text": "C.7Proof of Corollary 5.3 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Corollary 5.3 (Permutation bound). Let KL-DIME be the estimator obtained via iterative optimizationof $\\mathcal{I}_{K L}^{\\pi}(D)$ using abatch of size $N$ every training step. Then, ", "page_idx": 19}, {"type": "equation", "text": "$$\nI_{K L-D I M E}^{\\pi}:=\\mathbb{E}_{(\\mathbf{x},\\mathbf{y})\\sim p_{X Y}(\\mathbf{x},\\mathbf{y})}\\bigg[\\mathrm{log}\\bigg(\\hat{D}(\\mathbf{x},\\mathbf{y})\\bigg)\\bigg]<\\mathrm{log}(N).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. Theorem 5.2 implies that, when the batch size is much larger than the density ratio $[N>>R]$ then the discriminator's output converges to the density ratio. Indeed, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{N\\rightarrow\\infty}\\hat{D}(\\mathbf x,\\mathbf y)=\\operatorname*{lim}_{N\\rightarrow\\infty}\\frac{N R(\\mathbf x,\\mathbf y)}{K R(\\mathbf x,\\mathbf y)+N-K}=R(\\mathbf x,\\mathbf y).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Instead, when the density ratio is much larger than the batch size $(R>>N)$ 0, then the discriminator's output converges to a constant, in particular ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{R\\to\\infty}\\hat{D}(\\mathbf x,\\mathbf y)=\\operatorname*{lim}_{R\\to\\infty}\\frac{N R(\\mathbf x,\\mathbf y)}{K R(\\mathbf x,\\mathbf y)+N-K}=\\frac{N}{K}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "However, from Lemma 5.4, it is true that $K=1$ on average. Therefore, an iterative optimization algorithm leads to an upper-bounded discriminator, since ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\hat{D}({\\bf x},{\\bf y})<N,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which implies the thesis. ", "page_idx": 19}, {"type": "text", "text": "C.8Proof of Lemma 5.4 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Lemma 5.4 (see [30]). The average number of fixed points in a random permutation $\\pi(\\cdot)$ isequal to $^{\\,I}$ ", "page_idx": 19}, {"type": "text", "text": "Proof. Let $\\pi(\\cdot)$ be a random permutation on $\\{1,\\ldots,N\\}$ . Let the random variable $X$ represent the number of fixed points (i.e., the number of cycles of length 1) of $\\pi(\\cdot)$ . We define $X=X_{1}+X_{2}+$ $\\cdots+X_{N}$ , where $X_{i}=1$ when $\\pi(i)=i$ , and O otherwise. $\\mathbb{E}[X]$ is computed by exploiting the linearity property of expectation. Trivially, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}[X_{i}]=\\mathbb{P}[\\pi(i)=i]=\\frac{1}{N},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which implies ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}[X]=\\sum_{i=1}^{N}{\\frac{1}{N}}=1.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "table", "img_path": "PThi9hf9UT/tmp/2af5b67b033a2841e963bc40cd69adf4382dcbee9d743dfa61df9b4a1b126df7.jpg", "table_caption": ["Table 1: Neural architectures comparison. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "D Appendix: Experimental Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "D.1  Multivariate Linear and Nonlinear Gaussians Experiments ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, we show supplementary results for the linear and cubic Gaussian experiments. The implemented neural network architectures are: joint, separable, deranged, and the architecture of NJEE, referred to as ad hoc. See Tab. 1 for a schematic about the architectures. ", "page_idx": 20}, {"type": "text", "text": "Joint architecture. The joint architecture is a feed-forward fully connected neural network with an input size equal to twice the dimension of the samples distribution $(2d)$ , one output neuron, and two hidden layers of 256 neurons each. The activation function utilized in each layer (except from the last one) is ReLU. The number of realizations $(\\mathbf{x},\\mathbf{y})$ fed as input of the neural network for each training iteration is $N^{2}$ , obtained as all the combinations of the samples $\\mathbf{x}$ and $\\mathbf{y}$ drawn from $p_{X Y}(\\mathbf{x},\\mathbf{y})$ ", "page_idx": 20}, {"type": "text", "text": "Separable architecture. The separable architecture comprises two feed-forward neural networks, each one with an input size equal to $d$ , output layer containing 32 neurons and 2 hidden layers with 256 neurons each. The ReLU activation function is used in each layer (except from the last one). The first network is fed in with $N$ realizations of $X$ , while the second one with $N$ realizations of $Y$ ", "page_idx": 20}, {"type": "text", "text": "Deranged architecture. The deranged architecture is a feed-forward fully connected neural network with an input size equal to twice the dimension of the samples distribution $(2d)$ , one output neuron, and two hidden layers of 256 neurons each. The activation function utilized in each layer (except from the last one) is ReLU. The number of realizations $(\\mathbf{x},\\mathbf{y})$ the neural network is fed with is $2N$ for each training iteration: $N$ realizations drawn from $p_{X Y}(\\mathbf{x},\\mathbf{y})$ and $N$ realizations drawn from $p_{X}(\\mathbf{x})p_{Y}(\\mathbf{y})$ using the derangement procedure described in Sec. 5. ", "page_idx": 20}, {"type": "text", "text": "The architecture deranged is not used for $I_{C P C}$ because in (30) the summation at the denominator of the argument of the logarithm would require neural network predictions corresponding to the inputs $(\\mathbf{x}_{i},\\mathbf{\\bar{y}}_{j}^{\\bar{\\phantom{\\dagger}}}),\\,\\forall i,j\\in\\lbrace1,\\dots,N\\rbrace$ With $i\\neq j$ ", "page_idx": 20}, {"type": "text", "text": "Ad hoc architecture. The NJEE MI estimator comprises $2d-1$ feed-forward neural networks. Each neural network is composed by an input layer with size between 1 and $2d-1$ , an output layer containing $N-k$ neurons, with $k\\in\\mathbb{N}$ small, and 2 hidden layers with 256 neurons each. The ReLU activation function is used in each layer (except from the last one). We implemented a Pytorch [31] version of the code produced by the authors of [24] 3, to unify NJEE with all the other MI estimators. ", "page_idx": 20}, {"type": "text", "text": "Each neural estimator is trained using Adam optimizer [32], with learning rate $5\\times10^{-4}$ $\\beta_{1}=0.9$ $\\beta_{2}=0.999$ . The batch size is initially set to $N=64$ ", "page_idx": 20}, {"type": "text", "text": "For the Gaussian setting, we sample a 20-dimensional Gaussian distribution to obtain x and n samples, independently. Then, we compute $\\mathbf{y}$ as linear combination of $\\mathbf{x}$ and $\\mathbf{n}$ $\\mathbf{y}=\\rho\\,\\mathbf{x}+{\\sqrt{1-\\rho^{2}}}\\,\\mathbf{n}$ where $\\rho$ is the correlation coefficient. For the cubic setting, the nonlinear transformation $\\mathbf{y}\\mapsto\\mathbf{y}^{3}$ is applied to the Gaussian samples. During the training procedure, every $4k$ iterations, the target value of the MI is increased by 2 nats, for 5 times, obtaining a target staircase with 5 steps. The change in target MI is obtained by increasing $\\rho$ , that affects the true MI according to ", "page_idx": 20}, {"type": "equation", "text": "$$\nI(X;Y)=-\\frac{d}{2}\\log(1-\\rho^{2}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "image", "img_path": "PThi9hf9UT/tmp/0a47dc78414a1c047d4499e052f3a9fdbbe5f44ce5286771a346e4659cab5cef.jpg", "img_caption": ["Figure 6: NWJ, SMILE $\\tau=\\infty$ ), and MINE MI estimation comparison with $d=20$ and $N=64$ The Gaussian setting is represented in the top row, while the cubic setting is shown in the bottom row. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "D.1.1 Supplementary Analysis of the MI Estimators Performance ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Additional plots reporting the MI estimates obtained from MINE, NWJ, and SMILE with $\\tau=\\infty$ are outlined in Fig. 6. The variance attained by these algorithms exponentially increases as the true MI grows, as stated in (10). ", "page_idx": 21}, {"type": "text", "text": "We report in Fig. 7 the behavior we obtained for $I_{S M I L E}$ when the training of the neural network is performed by using the cost function in (29). The training diverges during the first steps when $\\tau=1$ and $\\tau=5$ . Differently, when $\\tau=\\infty$ $I_{S M I L E}$ corresponds to $I_{M I N E}$ (without the moving average improvement), therefore the MI estimate does not diverge. Interestingly, by comparing $I_{S M I L E}$ $\\tau=\\infty$ ) trained with the JS divergence and with the MINE cost function (in Fig. 6 and Fig. 7, respectively), the variance of the latter case is significantly higher. Hence, the JS maximization trick seems to have an impact in lowering the estimator variance. ", "page_idx": 21}, {"type": "text", "text": "D.1.2 Analysis for Different Values of $d$ and $N$ ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The class of $f$ -DIME estimators is robust to changes in $d$ and $N$ , as the estimators' variance decreases (see (58) and Fig. 15) when $N$ increases and their achieved bias is not significantly infuenced by the choice of $d$ Differently, $I_{N J E E}$ and $I_{C P C}$ are highly affected by variations of those parameters, as described in Fig. 2 and Fig. 10. More precisely, $I_{C P C}$ is not strongly influenced by a change of $d$ , but the bias significantly increases as the batch size diminishes, since the upper bound lowers. $I_{N J E E}$ achieves a higher bias both when $d$ decreases and when $N$ increases w.r.t. the default values $d=20,N=64$ . In addition, when $d$ is large, the training of $I_{N J E E}$ is not feasible, as it requires a lot of time (see Fig. 5) and memory (as a consequence of the large number of neural networks utilized) requirements. In addition, Fig. 8 illustrates that the time complexity of the joint architecture is $\\Omega(N^{2})$ , while the complexity of the deranged architecture is $\\Omega(N)$ ", "page_idx": 21}, {"type": "text", "text": "We show the achieved bias, variance, and mean squared error (MSE) corresponding to the three settings reported in Fig. 2, 10, and 11 in Fig. 12, 13, and 14, respectively. The achieved variance is bounded when the estimator used is $I_{K L-D I M E}$ or $I_{C P C}$ . In particular, Figs. 12, 13, 14, and 15 demonstrate that $I_{K L-D I M E}$ satisfies Lemma 4.2. ", "page_idx": 21}, {"type": "text", "text": "Additionally, we report the achieved bias, variance and MSE when $d=20$ and $N$ varies according to Tab. 3. We use the notation $N=[512,1024]$ to indicate that each cell of the table reports the values corresponding to $N=512$ and $N=1024$ , with this specific order, inside the brackets. Similarly, we ", "page_idx": 21}, {"type": "image", "img_path": "PThi9hf9UT/tmp/2a8cf84217c36578d2dba5f00312c5824b0ce20e9f7991ac18c7957711c2a3ba.jpg", "img_caption": ["Figure 7: $I_{S M I L E}$ behavior for different values of $\\tau$ , when the JS divergence is not used to train the neural network. The Gaussian case is reported in the top row, while the cubic case is reported in the bottomrow. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "PThi9hf9UT/tmp/2bd5e731faef7a4c2d6504136dcfc9c2edd62c9f91da248df223162b0dac5524.jpg", "img_caption": ["Figure 8: Time requirements comparison to complete the 5-step staircase MI over the batch size. Linear scale. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Table 2: Variance comparison between the VLB MI estimators and $f$ DIME, using the joint architecture, when $d=5$ and $N=64$ , for the Gaussian setting. ", "page_idx": 23}, {"type": "table", "img_path": "PThi9hf9UT/tmp/cc283aef13ab41956cd6355628ed1ad691a7b7f3c96aedb8c1b69c377579786a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "PThi9hf9UT/tmp/3f47310dddbdaa7568b6445cc4453e30ffca47d7498630cea133572de75a8750.jpg", "img_caption": ["Figure 9: Variance bar plots between the VLB MI estimators and $f$ -DIME, using the joint architecture, when $d=5$ and $N=64$ , for the Gaussian setting. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "show the attained bias, variance, and MSE for $d=[5,10]$ and $N=64$ in Tab. 5. The achieved bias, variance and MSE shown in Tab. 3 and Tab. 5 motivate that the class of $f$ -DIME estimators attains the best values for bias and MSE. Similarly, $I_{K L-D I M E}$ obtains the lowest variance, when excluding $I_{C P C}$ from the estimators comparison ( $I_{C P C}$ should not be desirable as it is upper bounded). The illustrated results are obtained with the joint architecture (except for NJEE) because, when the batch size is small, such an architecture achieves slightly better results than the deranged one, as it approximates the expectation over the product of marginals with more samples. ", "page_idx": 23}, {"type": "text", "text": "Thevarianceof the $f$ -DIME estimators achieved in the Gaussian setting when $N$ rangesfrom64 to 1024 is reported in Fig. 15. The behavior shown in such a figure demonstrates what is stated in Lemma 4.1, i.e., the variance of the f-DIME estimators varies as . ", "page_idx": 23}, {"type": "image", "img_path": "PThi9hf9UT/tmp/a1a0bbb4598c2c9aff73fbb5139970c9fc1da17fb31e2882c7940eae3a127e67.jpg", "img_caption": ["Figure 10: Staircase MI estimation comparison for $d=20$ and $N=1024$ .The Gaussian case is reported in the top row, while the cubic case is shown in the bottom row. "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "PThi9hf9UT/tmp/213e2848cd2ca31fb968e36bba95b4f04d1feb0a1270fa40f29eef4064957e03.jpg", "img_caption": ["Figure 11: Staircase MI estimation comparison for $d\\,=\\,20$ and $N\\,=\\,64$ The Gaussian case is reported in the top row, while the cubic case is shown in the bottom row. "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "PThi9hf9UT/tmp/9ebc775326a731e287a07dfc41eb12618dc31df95d9d21ba4c7194fbae3ec7e8.jpg", "img_caption": ["Figure 12: Bias, variance, and MSE comparison between estimators, using the joint architecture for theGaussiancasewith $d=20$ and $N=64$ "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "PThi9hf9UT/tmp/1e9be6395320d5888aab5d4c46292902b1f467b3269a514d6f28701ef4f53fb4.jpg", "img_caption": ["Figure 13: Bias, variance, and MSE comparison between estimators, using the joint architecture for theGaussiancasewith $d=5$ and $N=64$ "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "PThi9hf9UT/tmp/f5d2424b500097a4a6e8de4689aab9841f683f4b165a62e0efb8e8eeb0abe646.jpg", "img_caption": ["Figure 14: Bias, variance, and MSE comparison between estimators, using the joint architecture for theGaussiancasewith $d=20$ and $N=1024$ "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "PThi9hf9UT/tmp/9efa5a5529b034978b0ef6881e68a654a8d028a92262a23393c7db3504b4dce4.jpg", "img_caption": ["Figure 15: Variance of the $f$ DIME estimators corresponding to different values of batch size. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Table 3: Bias (B), variance (V), and MSE (M) of the MI estimators using the joint architecture, when $d=20$ and $N=[512,1024]$ , for the Gaussian setting. Each $f$ -DIME estimator is abbreviated to $f$ -D. ", "page_idx": 25}, {"type": "table", "img_path": "PThi9hf9UT/tmp/e3181b3993006564e49053d8809b96bace49e92cc57f4d06e1d1335ec29d6625.jpg", "table_caption": [], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "Table 4: Bias (B), variance (V), and MSE (M) of the MI estimators using the joint architecture, when $d=[5,10]$ and $N=64$ , for the Gaussian setting. Each $f$ DIME estimator is abbreviated to $f{\\mathrm{-D}}$ ", "page_idx": 26}, {"type": "table", "img_path": "PThi9hf9UT/tmp/8a2379d9f69167d3b70ada8ad83a71229850aaca180d8f062c89f7f65a50df0d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "Table 5: Bias (B), variance (V), and MSE (M) of the MI estimators using the joint architecture, when $d=20$ and $N=64$ , for the Gaussian setting. Each $f$ DIME estimator is abbreviated to $f$ D. ", "page_idx": 26}, {"type": "table", "img_path": "PThi9hf9UT/tmp/71b5d5aac18826830622263fdae429309f494b49289c23e93db5151cf89e2eec.jpg", "table_caption": [], "table_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "PThi9hf9UT/tmp/616cb0d19d8cced3c389fc6dbc502c0e0d47b1a11bb820e6d997f3e00b1f601d.jpg", "img_caption": ["Figure 16: MI estimates when $d=20$ and $N=128$ , top row: derangement strategy; bottom row: permutation strategy. "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "PThi9hf9UT/tmp/28293c47163fcbe69146ac1f27951b2ab2fd5c5c59412b3513a0079d2fd21ad9.jpg", "img_caption": ["Figure 17: MI estimates when $d=100$ and $N=64$ . The Gaussian setting is represented in the top row, while the cubic setting is shown in the bottom row. "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Theclass $f$ -DIME is able to estimate the MI for high-dimensional distributions, as shown in Fig. 17, where $d=100$ . In that figure, the estimates behavior is obtained by using the simple architectures described in Sec. D.1 of the Appendix. Thus, the input size of these neural networks (200) is comparable with the number of neurons in the hidden layers (256). Therefore, the estimates could be improved by increasing the number of hidden layers and neurons per layer. The graphs in Fig. 17 illustrate the advantage of the architecture deranged over the separable one. ", "page_idx": 27}, {"type": "text", "text": "D.1.3  Considerations on Derangements ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "To facilitate the understanding of the role of derangements during training, we provide a practical example in the following. ", "page_idx": 27}, {"type": "text", "text": "Suppose for simplicity that $N=3$ . Then, a random permutation of $\\mathbf{y}=[y_{1},y_{2},y_{3}]$ can be $[y_{2},y_{3},y_{1}]$ where the number of fixed points is $K=0$ as no elements remain in the same position after the permutation. However, another permutation of $\\mathbf{y}$ is $[y_{1},y_{3},y_{2}]$ . In this case, it is evident that $y_{1}$ remains in the same initial position, and the number of fixed points is $K=1$ . A random derangement of $\\mathbf{y}=\\left[y_{1},y_{2},y_{3}\\right]$ , instead, ensures by definition that no element of y ends up in the same initial position, contrarily from a naive random permutation. This idea is essential to avoid having shuffled marginal samples that actually are realizations of the joint distribution. In fact, we proved that a random permutation strategy would lead to a biased estimator (see the permutation bound in Corollary 5.3). ", "page_idx": 27}, {"type": "text", "text": "It is extremely important to remark that the derangement sampling strategy it is not only applicable to $f$ -divergence based estimators, rather, any discriminative variational estimator should use it to avoid upper bound MI estimates, as it can be observed from Fig. 16 ", "page_idx": 27}, {"type": "text", "text": "D.1.4 Summary of the Estimators ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We give an insight on how to choose the best estimator in Tab. 6, depending on the desired specifics. We assign qualitative grades to each estimator over different performance indicators. All the indicators names are self-explanatory, except from scalability, which describes the capability of the estimator to obtain precise estimates when $d$ and $N$ vary. The grades ranking is, from highest to lowest: $\\checkmark\\checkmark,\\checkmark$ $\\sim,x.$ When more than one architecture is available for a specific estimator, the grade is assigned by considering the best architecture within that particular case. ", "page_idx": 28}, {"type": "text", "text": "Even though the estimator choice could depend on the specific problem, we consider $I_{G A N-D I M E}$ to be the best one. The rationale behind this decision is that $I_{G A N-D I M E}$ achieves the best performance for almost all the indicators and lacks weaknesses. Differently, $I_{C P C}$ estimate is upper bounded, $I_{S M I L E}$ achieves slightly higher bias, and $I_{N J E E}$ is strongly $d$ and $N$ dependent. However, if the considered problem requires the estimation of a low-valued MI, $I_{K L-D I M E}$ is slightly more accurate than IGAN-DIME. ", "page_idx": 28}, {"type": "text", "text": "One limitation of this paper is that the set of $f$ -divergences analyzed is restricted to three elements. Thus, probably there exists a more effective $f$ -divergence which is not analyzed in this paper. Another limitation is that $f$ -DIME does not result in neither a lower nor an upper bound on the true MI. Nonetheless, in the following subsection, we show that it is actually possible to obtain a VLB version of the estimator. ", "page_idx": 28}, {"type": "text", "text": "D.1.5 Lower Bound Adaptation ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "As discussed above, the $f$ -DIME estimator does not constitute a lower bound on the true MI. This is due to two main reasons that make $f$ -DIME different from the others VLB MI estimators: ", "page_idx": 28}, {"type": "text", "text": "1. The general value function $\\mathcal{I}_{f}$ in (5) is the dual representation of the more general $f$ divergence and the KL-divergence is only one special case. Notice that the value $\\mathcal{I}_{f}$ is a lower bound on the $f$ -divergence;   \n2. We exploit the maximizer of $\\mathcal{I}_{f}$ , i.e. $\\hat{T}$ , to build the MI estimator at inference time. This is a key component that allows us to get rid of the partition function for MI estimation, and it comes at the expense of not having a lower bound estimator. ", "page_idx": 28}, {"type": "text", "text": "However, and perhaps quite remarkably, $f$ -DIME can be adjusted to be a lower bound on the MI by adding the partition term (in the SMILE, MINE or NWJ fashion) during inference time. One way to do such adaptation is to use the extracted density ratio inside the expressions of NWJ or MINE, as in the following: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I_{f D I M E_{N W J}}(X;Y)=\\mathbb{E}_{(\\mathbf{x},\\mathbf{y})\\sim p_{X Y}(\\mathbf{x},\\mathbf{y})}\\Bigg[\\mathrm{log}\\Bigg(\\big(f^{*}\\big)^{\\prime}\\big(\\hat{T}(\\mathbf{x},\\mathbf{y})\\big)\\Bigg)\\Bigg]}\\\\ &{\\phantom{=}-\\mathbb{E}_{(\\mathbf{x},\\mathbf{y})\\sim p_{X}(\\mathbf{x})p_{Y}(\\mathbf{y})}\\Bigg[\\bigg(\\big(f^{*}\\big)^{\\prime}\\big(\\hat{T}(\\mathbf{x},\\mathbf{y})\\big)\\bigg)\\Bigg]+1,}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $I_{f D I M E_{N W J}}(X;Y)$ is the $f$ -DIME estimator obtained using any $f$ -divergence dual representation of (5) but with the partition term of the NWJ estimator. ", "page_idx": 28}, {"type": "text", "text": "D.2   Self-Consistency Tests ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "The benchmark considered for the self-consistency tests is similar to the one applied in prior work [20]. We use the images collected in MNIST [33] and FashionMNIST [34] data sets. Here, we test three properties of MI estimators over images distributions, where the MI is not known, but the estimators consistency can be tested: ", "page_idx": 28}, {"type": "text", "text": "I. Baseline. $X$ is an image, $Y$ is the same image masked in such a way to show only the top $t$ rows. The value of ${\\hat{I}}(X;Y)$ should be non-decreasing in $t$ , and for $t=0$ the estimate should be equal to O, since $X$ and $Y$ would be independent. Thus, the ratio $\\hat{I}(X;Y)/\\hat{I}(X;X)$ should be monotonically increasing, starting from O and converging to 1. ", "page_idx": 28}, {"type": "text", "text": "2. Data-processing. $\\bar{X}$ is a pair of identical images, $\\bar{Y}$ is a pair containing the same images masked with two different values of $t$ .We set $h(Y)$ to be an additional masking of $Y$ of3 rows. The estimated MI should satisfy $\\hat{I}([X,X];[Y,h(Y)])/\\hat{I}(X;Y)\\approx1$ since including further processing should not add information. ", "page_idx": 28}, {"type": "table", "img_path": "", "table_caption": ["Table 6: Summary of the MI estimators. "], "table_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "PThi9hf9UT/tmp/d1af9d59f49e80168464227747997158e75bf370d253f7642d73f80e6f8ced2e.jpg", "img_caption": ["Figure 18: Comparison between different estimators for the baseline property, using MNIST data set on the left and FashionMNIST on the right. ", "(a) Baseline property, MNIST digits data set. ", "(b) Baseline property, FashionMNIST data set. "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "3. Additivity. $\\bar{X}$ is a pair of two independent images, $\\bar{Y}$ is a pair containing the masked versions (with equal $t$ values) of those images. The estimated MI should satisfy $\\hat{I}([X_{1},X_{2}];[Y_{1},\\bar{Y_{2}}])/\\hat{I}(X;Y)\\approx2$ , since the realizations of the $X$ and $Y$ random variables are drawn independently. ", "page_idx": 29}, {"type": "text", "text": "These tests are developed for $I_{f D I M E}$ \uff0c $I_{C P C}$ , and $I_{S M I L E}$ . Differently, $I_{N J E E}$ training is not feasible, since by construction $2d-1$ models should be created, with $d=784$ (the gray-scale image shape is $28\\times28$ pixels). The neural network architecture used for these tests is referred to as conv. ", "page_idx": 29}, {"type": "text", "text": "Conv. It is composed by two convolutional layers and one fully connected. The first convolutional layer has 64 output channels and convolves the input images with $(5\\times5)$ kernels, stride $2\\:p x$ and padding $2\\:p x$ . The second convolutional layer has 128 output channels, kernels of shape $(5\\times5)$ \uff0c stride $2\\;p x$ and padding $2\\:p x$ . The fully connected layer contains 1024 neurons. ReLU activation functions are used in each layer (except from the last one). The input data are concatenated along the channel dimension. We set the batch size equal to 256. ", "page_idx": 29}, {"type": "text", "text": "The comparison between the MI estimators for varying values of $t$ is reported in Fig. 18, 19, and 20. The behavior of all the estimators is evaluated for various random seeds. These results highlight that almost all the analyzed estimators satisfy the first two tests $(I_{H D-D I M E}$ is slightly unstable), while none of them is capable of fulfilling the additivity criterion. Nevertheless, this does not exclude the existence of an $f$ -divergence capable to satisfy all the tests. ", "page_idx": 29}, {"type": "image", "img_path": "PThi9hf9UT/tmp/d07ab501744a39d026b431d77e6b0f2c95c93f399e36500675695f24a0412e3b.jpg", "img_caption": ["(a) Data processing property, MNIST digits data set. (b) Data processing property, FashionMNIST data set. "], "img_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "PThi9hf9UT/tmp/be736a162033884536f30a5892335373268a64952aa63ac2b21b61309cb26b3c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "Figure 19: Comparison between different estimators for the data processing property, using MNIST data set on the left and FashionMNIST on the right. ", "page_idx": 30}, {"type": "image", "img_path": "PThi9hf9UT/tmp/e037e393ceb4213ec29947ec4379a122e6c6824f6cab0cddaa651948fd22731a.jpg", "img_caption": ["(a) Additivity property, MNIST digits data set. ", "Figure 20: Comparison between different estimators for the additivity property, using MNIST data set on the left and FashionMNIST on the right. "], "img_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "PThi9hf9UT/tmp/bd4d7ca50029ad20ce52db4028809a5be03df32013f72c5fd070c6127a04d7de.jpg", "img_caption": ["(b) Additivity property, FashionMNIST data set. "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Claims ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: The two main contributions of the paper are both included: a) the new class of MI estimators; b) derangement-based architecture. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and refect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 30}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: In Appendix D.1.4 we discuss a possible limitation of the proposed class of MI estimators. The computational efficiency is discussed in Sec. 6.2 and in Appendix D.1.2. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should refect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should refect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should refect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational effciency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 31}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: All the assumptions are included in the statements of the Lemmas, Theorems, and Corollaries (that are cross-referenced). All the complete proofs are reported in Appendix C. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 31}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The code is released. In addition, the complete neural architectures are specified in Appendix D.1, together with the optimizer type and hyperparameters. Furthermore, we provide the hardware details in Sec. 6. Finally, the contributions of this paper are the objective functions, which are all reported in Appendix A. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. () If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 32}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We provide the code and a readme file describing how to run it. The code does not require importing data, since it uses samples drawn from probability density functions for the experiments (except from the MNIST datasets, that are automatically downloaded using the scripts provided). ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: They are written in Appendix D. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 33}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: Analogously to the papers of the state-of-the-art MI estimators, all the \"stairs\" figures show the MI estimates during training, where the light color gives a direct intuition of the variance of the MI estimators. The self-consistency tests show the mean value over multiple runs of the code with different random seeds, and the maximum/minimum values estimated during the different iterations. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 33}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce theexperiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: In Sec. 6.2 we report the type of CPU and GPU used. In addition, we develop a full computational time analysis (in Sec. 6.2). ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 34}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: There are no potential harms caused by the research process, and no potential harmful consequences. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 34}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: Our work is foundational research. Mutual information estimation is used for various machine learning algorithms and there are no direct paths to negative applications. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. ", "page_idx": 34}, {"type": "text", "text": "\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 35}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: There is no such a risk. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safetyfilters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 35}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properlyrespected? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We cite all the datasets used and we cite the papers from which we use parts of code implementations (of the state-of-the-art MI estimators). ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 35}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We fully disclose the code and provide a readme file, describing how to properly run it, inside an anonymized zip file. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 36}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 36}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)wereobtained? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 36}]