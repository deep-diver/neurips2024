[{"type": "text", "text": "Constrained Diffusion Models via Dual Training ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Shervin Khalafi Dongsheng Ding\\* Alejandro Ribeiro {shervink,dongshed,aribeiro}@seas.upenn.edu University of Pennsylvania ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Diffusion models have attained prominence for their ability to synthesize a probability distribution for a given dataset via a diffusion process, enabling the generation of new data points with high fidelity. However, diffusion processes are prone to generating samples that reflect biases in a training dataset. To address this issue, we develop constrained diffusion models by imposing diffusion constraints based on desired distributions that are informed by requirements. Specifically, we cast the training of diffusion models under requirements as a constrained distribution optimization problem that aims to reduce the distribution difference between original and generated data while obeying constraints on the distribution of generated data. We show that our constrained diffusion models generate new data from a mixture data distribution that achieves the optimal trade-off among objective and constraints. To train constrained diffusion models, we develop a dual training algorithm and characterize the optimality of the trained constrained diffusion model. We empirically demonstrate the effectiveness of our constrained models in two constrained generation tasks: (i) we consider a dataset with one or more underrepresented classes where we train the model with constraints to ensure fairly sampling from all classes during inference; (ii) we fine-tune a pre-trained diffusion model to sample from a new dataset while avoiding overfitting. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Diffusion models have become a driving force of modern generative modeling, achieving groundbreaking performance in tasks ranging from image/video/audio generation [64, 6, 43] to molecular design for drug discovery [75, 74]. Diffusion models learn diffusion processes that produce a probability distribution for a given dataset (e.g., images) from which we can generate new data (e.g., classic models [66, 34, 68, 78]). As diffusion models are used to generate data with societal impacts, e.g., art generation and content creation for media, they must comply with requirements from specific domains, e.g., social fairness in image generation [54, 57], aesthetic properties of images [12, 13], bioactivity in molecule generation [39], and more [40, 81, 8, 19, 79, 14]. ", "page_idx": 0}, {"type": "text", "text": "Classic diffusion models [66, 34, 68] have been extended to generate data under different requirements through either first-principle methods or fine-tuning. In image generation with fairness, for instance, first principle methods directly mitigate biases towards social/ethical identities by revising the training loss functions per biased/unbiased data [49, 41]; while fine-tuning methods align biased diffusion models with desired data distributions by optimizing the associated metrics [24, 72, 71, 70]. Although these methods allow us to equip diffusion models with specific requirements, they are often designed for particular generation tasks and do not provide transparency on how these requirements are satisfied. Since diffusion models are trained by minimizing the loss functions of diffusion processes, it is natural to incorporate requirements into diffusion models by imposing constraints on these optimization problems. Therefore, it is imperative to develop diffusion models under constraints by generalizing constrained learning methods and theory (e.g., [9, 10, 22, 36]) for diffusion models. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this work, we formulate the training of diffusion models under requirements as a constrained distribution optimization problem in which the objective function measures a training loss induced by the original data distribution, and the constraints require other training losses induced by some desirable data distributions to be small. This constrained formulation can be instantiated for several critical requirements. For instance, to promote fairness for unrepresented groups, the constraints can encode the closeness of the model to the distributions of underrepresented data. Compared with the typical importance re-weighting method [41], our constrained formulation provides an optimal trade-off between matching given data distribution and following reference distribution. Not limited to constraints with other desirable data distributions, our constrained formulation captures more general requirements. For instance, when adapting a pretrained diffusion model to new data, the constraints require the model to be close to the pretrained model, not degrading the original generation ability. Specifically, our main contribution is three-fold. ", "page_idx": 1}, {"type": "text", "text": "(i)  We propose and analyze constrained diffusion models from the perspective of constrained optimization in an infinite-dimensional distribution space, where the constraints require KL divergences between the model and our desired data distributions to be under some thresholds. We exploit the strong duality in convex optimization to show that our constrained diffusion models generate new data from a mixture data distribution that achieves the optimal trade-off among objective and constraints.   \n(ii) To train constrained diffusion models, we introduce parametrized constrained diffusion models and develop a Lagrangian-based dual training algorithm. We exploit the relation between un/parametrized problems to show that constrained diffusion models generate new data from the optimal mixture distribution, up to some optimization/parametrization errors.   \n(iii) We empirically demonstrate the merit of our constrained diffusion models in two aforementioned requirements. In the fair generation task, we show that our constrained model promotes sampling more from the minority classes compared to unconstrained models, leading to fair sampling across all classes. In the adaptation task, we show that our fine-tuned constrained model learns to generate new data without significantly degrading the original generation ability, compared to the unconstrained model which tends to overfit to new data. ", "page_idx": 1}, {"type": "text", "text": "Related work. As a first-principle method, our constrained diffusion approach is more relevant to diffusion models that incorporate requirements in distribution space [20, 35, 49, 21, 60, 28], rather than those applied in sample space [37, 53, 29, 27, 26, 17, 48, 25]. In comparison with conditional diffusion models that restrict generation through conditional information [20, 35, 1], our constrained diffusion models impose distribution constraints within the constrained optimization framework. Compared to compositional generation [49, 21, 60] and fair diffusion [28], our work provides a constrained learning approach to balance different distribution models using Lagrangian multipliers, which is different from equal weights [49, 21], hyperparameter [60] or fair guidance [28]. Our constrained approach is also relevant to the importance re-weighting method for diffusion models [41] and GANs [16], which reduces bias in a dataset by re-weighting it with a pre-trained debiased density ratio. In contrast, we design our diffusion models to mitigate the bias in a dataset by imposing distribution constraints without pre-training. In addition to being distinct from existing methods, we provide a systematic study of our constrained diffusion models, covering duality analysis, dual-based algorithm design, and convergence analysis, which is absent in the diffusion model literature. ", "page_idx": 1}, {"type": "text", "text": "Our work is also pertinent to recently surging fine-tuning and alignment methods that aim to improve pre-trained diffusion models by optimizing their downstream performance, e.g., aesthetic scores of images. In reward-guided fine-tuning methods, such as supervised learning [45, 80, 76], control-based feedback learning [77, 65, 71, 70, 18], and reinforcement learning [23, 32, 24, 72, 5, 82], reward functions have to be pre-trained from an authentic evaluation dataset, and the trade-off for reward functions in pre-trained diffusion models is often regulated heuristically. In contrast, our constrained approach directly minimizes the gap between a fine-tuning model and a high-quality dataset with the desired properties, while ensuring the generated outputs being close to that of pre-trained models. ", "page_idx": 1}, {"type": "text", "text": "Compared to other generative models under requirements (e.g., VAEs [61], GANs [16]), and classical sampling methods (e.g., Langevin dynamics and Stein variational gradient descent [50]), our work is different because we focus on diffusion-based generative models. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We overview diffusion models from the perspective of variational inference [55] by presenting forward/backward processes in Section 2.1, and the evidence lower bound in Section 2.2. ", "page_idx": 2}, {"type": "text", "text": "2.1 Forward and backward processes ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The forward process begins with a true data sample $x_{0}\\in\\mathbb{R}^{d}$ and generates latent variables $\\{x_{t}\\}_{t=1}^{T}$ from $t\\,=\\,1$ to $T$ by adding white noise recursively. The joint distribution of latent variables results from conditional probabilities $\\begin{array}{l}{{q(x_{1:T}\\,|\\,x_{0})\\;=\\;\\prod_{t\\,=\\,1}^{T}q(x_{t}\\,|\\,x_{t-1})}}\\end{array}$ , where the distribution of latent variable $x_{t}$ conditioned on the previous latent $x_{t-1}$ is given by a Gaussian distribution $q(x_{t}\\:|\\:x_{t-1})\\;=\\;\\mathcal{N}(x_{t};\\mu_{t},\\sigma_{q}^{2}(t)I)$ where $\\mu_{t}={\\sqrt{\\alpha_{t}}}\\,x_{t-1}$ and $\\sigma_{q}^{\\bar{2}}(t)=\\dot{1}-\\alpha_{t}$ . Since the forward process is a linear Gaussian model with pre-selected mean and variance, it is solely determined by the data distribution $q(x_{0})$ . We often refer to $q(x_{0})$ as a forward process. ", "page_idx": 2}, {"type": "text", "text": "The backward process begins with the latent $x_{T}$ sampled from the standard Gaussian $p(x_{T})\\;=$ $\\mathcal{N}(\\boldsymbol{x}_{T};0,I)$ , and decodes latent variables from $t=T$ to $t=0$ with a joint distribution ", "page_idx": 2}, {"type": "equation", "text": "$$\np(x_{0:T})\\ =\\ p(x_{T})\\prod_{t\\,=\\,1}^{T}p(x_{t-1}\\,|\\,x_{t}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Here, $p$ is our distribution model that can be used to generate new samples. We denote by $\\mathcal{P}$ the set of all joint distributions over $x_{0:T}$ in form of (1), where $p(x_{t-1}\\mid x_{t})$ is a conditional Gaussian with a fixed variance (see Appendix A). Throughout the paper, we work in the convergent regimes of the backward process (e.g., [15, 11, 2]). Without loss of generality, we adopt the convergent regime in [47] by taking the scheduling parameter $\\alpha_{1}=1-1/T^{c_{0}}$ and $\\dot{\\alpha}_{t}=1-\\dot{c}_{T}\\operatorname*{min}\\big((1-\\alpha_{1})(\\bar{1}+c_{T})^{t},1\\big)$ for $t>1$ , and the variance as $\\sigma_{p}^{2}(t)\\,=\\,1/\\alpha_{t}\\,-\\,1$ , where $c_{T}:=c_{1}\\log(T)/T$ and $c_{0},\\,c_{1}$ are some constants. Hence, $\\bar{\\alpha}_{T}\\approx0$ implies $q(x_{T})\\simeq\\mathcal{N}(x_{T};0,I)$ . Thus, $q(x_{0:T})\\in\\mathcal{P}$ . Also, $\\bar{\\alpha}_{1}\\approx1$ implies $q(x_{1})\\simeq q(x_{0})$ . It is ready to generalize our results to other diffusion processes (e.g., [46, 38, 3]). ", "page_idx": 2}, {"type": "text", "text": "2.2 The evidence lower bound (ELBO) ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Denote the L divergeneo istibution $q$ from distibution $p$ b $\\begin{array}{r}{D_{\\mathrm{KL}}(q\\,\\|\\,p):=\\mathbb{E}_{x\\sim q(x)}\\log(\\frac{q(x)}{p(x)})}\\end{array}$ Generative diffusion modeling aims to generate samples whose distribution is close to that of an observed dataset of samples. Formally, we express this objective as maximizing the log-likelihood of an observation generated by the diffusion model: m $\\mathrm{aximize}_{p\\,\\in\\,\\mathcal{P}}\\,\\mathbb{E}_{q(x_{0})}\\bigl[\\log\\bar{p(x_{0})}\\,\\bigr]$ where ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbb{E}_{q(x_{0})}\\left[\\log p(x_{0})\\right]\\;=\\;E(p;q)\\,+\\,\\mathbb{E}_{q(x_{0})}\\left[\\,D_{\\mathrm{KL}}\\left(q(x_{1:T}\\mid x_{0}\\right)\\,\\Vert\\,p(x_{1:T}\\mid x_{0})\\right)\\right]\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "and $\\begin{array}{r}{E(p;q):=\\mathbb{E}_{q(x_{0})}\\mathbb{E}_{q(x_{1:T}\\mid x_{0})}\\log\\frac{p(x_{0:T})}{q(x_{1:T}\\mid x_{0})}}\\end{array}$ is known as ELBO in variational inference [7, 5]. Alternatively, we aim to minimize the KL divergence between the forward/backward processes, ", "page_idx": 2}, {"type": "equation", "text": "$$\nD_{\\mathrm{KL}}\\left(q(x_{0:T})\\,\\Vert\\,p(x_{0:T})\\right)\\;=\\;-\\,E(p;q)\\,+\\,\\mathbb{E}_{q(x_{0})}\\,[\\log q(x_{0})\\,]\\,.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Thus, we connect the log-likelihood maximization to the $\\mathrm{KL}$ divergence minimization via ELBO. ", "page_idx": 2}, {"type": "text", "text": "Lemma 1 (Equivalent formulations). The ELBO maximization and the $K L$ divergenceminimization are equivalent over thedistribution space $\\mathcal{P}$ ,and the unique solution of these two problems is $a$ solution for the log-likelihood maximization problem, i.e., ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{p\\,\\in\\,\\mathcal{P}}{\\mathrm{maximize}}\\ E(p;q)\\ \\ \\Leftrightarrow\\ \\underset{p\\,\\in\\,\\mathcal{P}}{\\mathrm{minimize}}\\ D_{K L}\\left(q(x_{0:T})\\parallel p(x_{0:T})\\right)\\ \\ \\Rightarrow\\ \\underset{p\\,\\in\\,\\mathcal{P}}{\\mathrm{maximize}}\\ \\mathbb{E}_{q(x_{0})}\\big[\\log p(x_{0})\\big]}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "See Appendix B.1 for proof. Lemma 1 states that improving the ELBO score increases the likelihood of a backward process that generates the data, together with the fit of a backward process to the forward process. Hence, finding the best backward process becomes optimizing one of three equivalent objectives. In practice, ELBO serves as a loss function approximated by ", "page_idx": 2}, {"type": "equation", "text": "$$\nE(p;q)\\;\\approx\\;-\\,\\sum_{t\\,=\\,2}^{T}\\mathbb{E}_{q(x_{0})}\\mathbb{E}_{q(x_{t}\\,|\\,x_{0})}\\left[\\,D_{\\mathrm{KL}}\\left(\\substack{q(x_{t-1}\\,|\\,x_{t},x_{0})\\,\\|\\,p(x_{t-1}\\,|\\,x_{t})\\right)\\,\\right].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "With the variance schedule described in Section 2.1, it is known that this approximation is almost exact (see Appendix A and also [42]), which is our focal setting. Using standard diffusion derivations [55], the ELBO maximization can be shown to equal to a quadratic loss minimization, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{minimize}_{\\left.\\widehat{s}\\in\\mathcal{S}\\right.}~\\mathbb{E}_{x_{0},\\,t,\\,x_{t}}\\left[\\left.\\lVert\\widehat{s}(x_{t},t)-\\nabla\\log q(x_{t})\\rVert^{2}\\right.\\right]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathbb{E}_{x_{0},t,x_{t}}$ is an expectation over the data distribution $q(x_{0})$ , a discrete distribution $p_{\\omega}(t)$ from 2 to $T$ , and the forward process $q(x_{t}\\mid x_{0})$ at time $t$ given the data sample $x_{0}$ ; see Appendix A for details. The minimization is done to find a function $\\bar{\\boldsymbol{s}}\\in\\mathcal{S}$ that can best predict the gradient of the forward process over data $\\nabla\\log{q(x_{t})}$ , commonly called the (Stein) score function, where $\\boldsymbol{S}$ is a set of valid score functions mapping from $\\mathbb{R}^{d}\\times\\mathbb{N}$ 0\uff1a $\\mathbb{R}^{d}$ . In practice, however, we parametrize the estimator $\\widehat{\\boldsymbol{s}}(\\boldsymbol{x}_{t},t)$ as $\\widehat{s}_{\\theta}(x_{t},t)$ with parameter $\\theta$ , which gives our focal objective of generative modeling: minin $\\mathrm{nize}_{\\theta}\\!\\in\\!\\Theta\\operatorname{\\mathbb{E}}_{x_{0},\\,t,\\,x_{t}}\\left[\\,\\left\\|\\widehat{s}_{\\theta}\\!\\left(x_{t},t\\right)-\\nabla\\log q(x_{t})\\right\\|^{2}\\,\\right]$ . A parametrized form of $p(x_{t-1}\\mid x_{t})$ associated with $\\widehat{s}_{\\theta}(x_{t},t)$ is denoted by $p_{\\theta}(x_{t-1}\\mid x_{t})$ and the backward process has a parametrized joint distribution $p_{\\theta}(x_{0:T})$ . We remark that the prediction problem (5) can be also be formulated as data or noise prediction instead [55], with our results directly transferable to these formulations. ", "page_idx": 3}, {"type": "text", "text": "3   Variational constrained diffusion models ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We introduce constrained diffusion models by considering the unparametrized set of joint distributions in Section 3.1, and illustrating constraints via two examples in Section 3.2. ", "page_idx": 3}, {"type": "text", "text": "3.1   KL divergence-constrained diffusion model: unparametrized case ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The standard diffusion model specifies a single data distribution, denoted by $q$ in Lemma 1. To account for other generation requirements, we introduce $m$ additional data distributions $\\{q^{i}\\}_{i=1}^{m}$ that represent $m$ desired properties on generated data. To incorporate new properties into the diffusion model, we formulate an unparametrized KL divergence-constrained optimization problem, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{p\\,\\in\\,\\mathcal{P}}{\\mathrm{minimize}}}&{D_{\\mathrm{KL}}\\left(q(x_{0:T})\\,\\Vert\\,p(x_{0:T})\\right)}\\\\ {\\mathrm{subject\\,}\\tan}&{D_{\\mathrm{KL}}\\left(q^{i}(x_{0:T})\\,\\Vert\\,p(x_{0:T})\\right)\\;\\le\\;b_{i}\\quad\\mathrm{for}\\;i=1,\\dots,m.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Let an optimal solution to Problem (U-KL) be $p^{\\star}$ . Then the optimal value of the objective function is $F^{\\star}:=\\bar{D}_{\\mathrm{KL}}(q\\,\\|\\,p^{\\star})$ . Problem (U-KL) aims to find a model $p^{\\star}$ that generates data from the original distribution $q$ while staying close to $m$ distributions $\\{q^{i}\\}_{i=1}^{m}$ that encode our desired properties, e.g.. unbiasedness towards minorities. Let the Lagrangian for Problem (U-KL) be ", "page_idx": 3}, {"type": "equation", "text": "$$\n{\\mathcal L}(p,\\lambda)\\;=\\;D_{\\mathrm{KL}}\\left(q(x_{0:T})\\,\\|\\,p(x_{0:T})\\right)\\;+\\;\\sum_{i\\,=\\,1}^{m}\\lambda_{i}\\left(\\,D_{\\mathrm{KL}}\\left(q^{i}(x_{0:T})\\,\\|\\,p(x_{0:T})\\right)-b_{i}\\,\\right)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "for $\\lambda\\geq0$ . The dual function $g(\\lambda)$ is given by $g(\\lambda):=\\operatorname*{min}_{p\\,\\in\\,\\mathcal{P}}\\mathcal{L}(p,\\lambda)$ which is always concave. ", "page_idx": 3}, {"type": "text", "text": "To make Problem (U-KL) meaningful, we assume the constraints are strictly satisfied by some model. ", "page_idx": 3}, {"type": "text", "text": "Assumption 1 (Strict feasibility). There exists $a$ model $\\textit{p}\\in\\textit{\\textbf{P}}$ and $\\zeta\\ \\ >\\ \\ 0$ suchthat $D_{\\mathrm{KL}}(q^{i}\\bar{(}x_{0:T})\\;||\\;p(x_{0:T}))\\leq b_{i}-\\bar{\\zeta}$ forall $i=1,\\hdots,m$ ", "page_idx": 3}, {"type": "text", "text": "Let an optimal dual variable of Problem (U-KL) be $\\lambda^{\\star}\\in\\operatorname{argmax}_{\\lambda\\,\\geq\\,0}g(\\lambda)$ , and the optimal value of the dual function be $D^{\\star}:=\\,g(\\lambda^{\\star})$ . From weak duality, the duality gap is non-negative, i.e., $F^{\\star}-D^{\\star}\\geq0$ . Moreover, due to the convexity of $\\mathrm{KL}$ divergence, Problem (U-KL) is a convex optimization problem, and thus it satisfies strong duality; see Appendix B.2 for proof. ", "page_idx": 3}, {"type": "text", "text": "Lemma 2 (Strong duality). Let Assumption 1 hold. Then, Problem (U-KL) has zero duality gap, i.e., $F^{\\star}=D^{\\star}$ Moreover, $(p^{\\star},\\lambda^{\\star})$ is an optimal primal-dual pair of Problem (U-KL). ", "page_idx": 3}, {"type": "text", "text": "Let a mixture data distibution be $\\begin{array}{r}{q_{\\mathrm{mix}}^{(\\lambda)}:=\\left(\\,q+\\sum_{i\\,=\\,1}^{m}\\lambda^{i}q^{i}\\,\\right)/(1+\\lambda^{\\top}1)}\\end{array}$ for $\\lambda\\geq0$ .We denote by $q_{\\mathrm{mix}}^{(\\lambda)}(x_{0:T})$ aid $q_{\\mathrm{mix}}^{(\\lambda)}$ Leveragin stong duality, we show that an optimal model can be obtained by solving an equivalent unconstrained problem in Theorem 1 and its proof is deferred to Appendix B.3. ", "page_idx": 3}, {"type": "text", "text": "Theorem 1 (Optimal constrained model). Let Assumption $^{\\,l}$ hold. Then, Problem (U-KL) equals ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{minimize}_{p\\,\\in\\,\\mathcal{P}}~~D_{\\mathrm{KL}}\\left(q_{\\mathrm{mix}}^{(\\lambda^{\\star})}(x_{0:T})\\,\\|\\,p(x_{0:T})\\right)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "whereGmix $q_{\\mathrm{mix}}^{(\\lambda^{\\star})}(x_{0:T})$ is the joint distribution of the forward process at an optimal dual variable $\\lambda^{\\star}$ ", "page_idx": 4}, {"type": "text", "text": "Theorem 1 states that the $\\mathrm{KL}$ divergence-constrained problem reduces to an unconstrained KL divergence minimization problem. We notice that the KL divergence is zero if and only if two probability distributions match each other. Hence, $q_{\\mathrm{mix}}^{(\\lambda^{\\star})}(x_{0:T})$ is the optimal solution to Problem U-MIX). ", "page_idx": 4}, {"type": "text", "text": "$^{\\,l}$ $p^{\\star}(x_{0:T})\\,=$ $q_{\\mathrm{mix}}^{(\\lambda^{\\star})}(x_{0:T})$ ", "page_idx": 4}, {"type": "text", "text": "Let $\\bar{b}^{i}:=b^{i}-\\mathbb{E}_{q^{i}(x_{0})}[\\log q^{i}(x_{0})\\,]$ Appliation of Equality 3) to Problem U-KL) yields an ELBObased constrained optimization problem, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{p\\,\\in\\,{\\mathcal{P}}}{\\mathrm{minimize}}}&{-\\,E(p;q)}\\\\ {\\mathrm{subject\\;to}}&{-\\,E(p;q^{i})\\;\\le\\;\\bar{b}^{i}\\;\\;\\mathrm{~for\\;}i=1,\\ldots,m.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Recall the model representation in Section 2.2, we can characterize each joint distribution $p\\in\\mathcal{P}$ with a function $\\widehat{s}\\in\\mathcal{S}$ . Moreover, ELBO reduces to the denoising matching term that has a simplified quadratic form given in Section 2.2. With this reformulation in mind, we cast Problem (U-ELBO) into a convex optimization problem over the function space $\\boldsymbol{S}$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\b{\\hat{s}}\\in\\mathcal{S}}{\\mathrm{minimize}}\\quad\\mathbb{E}_{q(x_{0}),\\,t,\\,x_{t}}\\left[\\left\\Vert\\widehat{s}(x_{t},t)-\\nabla\\log q(x_{t})\\right\\Vert^{2}\\right]}\\\\ &{\\mathrm{subject~to}\\quad\\mathbb{E}_{q^{i}(x_{0}),\\,t,\\,x_{t}}\\left[\\left\\Vert\\widehat{s}(x_{t},t)-\\nabla\\log q^{i}(x_{t})\\right\\Vert^{2}\\right]\\;\\leq\\;\\widetilde{b}^{i}\\;\\;\\mathrm{~for~}i=1,\\dots,m}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\widetilde{b}^{i}:=(\\bar{b}^{i}-v)/\\bar{\\omega}$ . Here, the notation $v$ is a constant shift due to the variance mismatch term; see it in Appendix A. We note that scaling or shifting objective and constraints from both sides with some constants doesn't alter the solution to a constrained optimization problem. Thus, the key difference between Problems (U-KL) and (U-LOSS) is the optimization variable (respectively, $p$ and $\\widehat{s}$ ). Let the Lagrangian $\\mathcal{L}_{s}(\\widehat{s},\\lambda)$ for Problem (U-LOSS) be ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathfrak{L}_{q(x_{0}),\\,t,\\,x_{t}}\\left[\\Vert\\widehat{s}(x_{t},t)-\\nabla\\log q(x_{t})\\Vert^{2}\\right]+\\sum_{i=1}^{m}\\lambda_{i}\\left(\\mathbb{E}_{q^{i}(x_{0}),\\,t,\\,x_{t}}\\left[\\Vert\\widehat{s}(x_{t},t)-\\nabla\\log q^{i}(x_{t})\\Vert^{2}\\right]-\\widetilde{b}^{i}\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Let the associated dual function be $\\begin{array}{r}{g_{s}(\\lambda):=\\operatorname*{min}_{\\widehat{s}\\,\\in\\,{\\cal S}}\\mathcal{L}_{s}(\\widehat{s},\\lambda)}\\end{array}$ for $\\lambda\\geq0$ .Hence, $g(\\lambda)$ and $g_{s}(\\lambda)$ have the same maximizer $\\lambda^{\\star}$ , and the partial minimizer $\\begin{array}{r}{\\widehat{s}^{\\star}=\\operatorname*{argmin}_{\\widehat{s}\\,\\in\\,\\mathcal{S}}\\mathcal{L}_{s}(\\widehat{s},\\lambda^{\\star})}\\end{array}$ is the solution to Problem (U-LOSS). Hence, an optimal primal-dual pair $(\\widehat{s}^{\\star},\\lambda^{\\star})$ to Problem (U-LOSS) gives an optimal primal-dual pair $(p^{\\star},\\lambda^{\\star})$ for Problem (U-KL), where $p^{\\star}$ is a joint distribution of the backward process induced by $\\widehat{s}^{\\star}$ . By this dual property, we take a dual perspective to train constrained diffusion models: we maximize the dual function $g_{s}(\\lambda)$ to obtain the optimal dual variable $\\lambda^{\\star}$ , and then recover the solution $\\widehat{s}^{\\star}$ by minimizing the Lagrangian $\\mathcal{L}_{s}(\\widehat{s},\\lambda^{\\star})$ ", "page_idx": 4}, {"type": "text", "text": "3.2 Examples of KL divergence constraints ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To illustrate our KL-divergence constraints, we provide two generation tasks of exemplary. ", "page_idx": 4}, {"type": "text", "text": "(i) Fairness to underrepresented classes. We consider a fair image generation task in which some classes are underrepresented in the available training dataset. An example of this would be the Celeb-A dataset [51] which contains pictures of celebrity faces with those labeled as male being underrepresented ( $42\\%$ Malevs $58\\%$ Female). To promote representation of the under-represented classes, we can pose it as an instance of Problem (U-KL), where each $q^{i}$ denotes the distribution of an under-represented subset or minority class of $q$ ", "page_idx": 4}, {"type": "text", "text": "(ii)Adapting pretrained model to new data. Given a pretrained diffusion model over some original dataset that is no longer accessible, we aim to fine-tune the pretrained model for generating data from a new data distribution. Similarly, we can pose this as an instance of Problem (U-KL), where $q^{1}$ denotes the distribution of the new data and $q$ is the distribution of samples generated by the pre-trained model. ", "page_idx": 4}, {"type": "text", "text": "Let $h_{i}:=-\\mathbb{E}_{q^{i}(x_{0})}\\left[\\log q_{.}^{i}(x_{0})\\right]$ be the differential entropy of data distribution $q^{i}$ We relate the KL divergence constraints with the optimal dual variable through entropy in Theorem 2. ", "page_idx": 5}, {"type": "text", "text": "Theorem 2. Let Assumption $^{\\,l}$ hold, and thesupportsof data distributions qand $\\{q^{i}\\}_{i=1}^{m}$ be disjoint. Then, the optimal dual variables $\\lambda^{\\star}$ to Problem (U-ELBO) are given by ", "page_idx": 5}, {"type": "equation", "text": "$$\n{\\frac{\\lambda_{i}^{\\star}}{1\\,+\\,(\\lambda^{\\star})^{T}\\,{\\bf1}}}\\;=\\;\\mathrm{e}^{h_{i}\\,-\\,\\bar{b}_{i}}\\;\\;\\mathrm{for}\\,i=1,\\ldots,m.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "$q_{\\mathrm{mix}}^{(\\lambda^{\\star})}$ ${\\bar{b}}_{i}$   \nfrom the associated distributions that have higher entropy $h_{i}$ . Assumption 1 can be relaxed to a feasibility condition for Problem (U-KL); see Lemma 6 in Appendix B.4. ", "page_idx": 5}, {"type": "text", "text": "4  Parametrization and dual training algorithm ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Having introduced unparametrized models, we move to parametrization for constrained diffusion models in Section 4.1, provide optimality analysis of a Lagrangian-based dual method in Section 4.2, and present a practical dual training algorithm in Section 4.3. ", "page_idx": 5}, {"type": "text", "text": "4.1  KL divergence-constrained diffusion model: parametrized case ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "With the parametrized model $p_{\\theta}$ for $\\theta\\in\\Theta$ , we present a parameterized constrained problem, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{\\boldsymbol{\\theta}\\in\\Theta}{\\mathrm{minimize}}}&{D_{\\mathrm{KL}}\\left(\\boldsymbol{q}(\\boldsymbol{x}_{0:T})\\,\\Vert\\,p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{0:T})\\right)}\\\\ {\\mathrm{subject~to}}&{D_{\\mathrm{KL}}\\left(\\boldsymbol{q}^{i}(\\boldsymbol{x}_{0:T})\\,\\Vert\\,p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{0:T})\\right)\\;\\le\\;b^{i}\\;\\;\\;\\mathrm{for}\\;i=1,\\dots,m.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Let the Lagrangian for Problem (P-KL) be $\\bar{\\mathcal{L}}(\\theta,\\lambda):=\\mathcal{L}(p_{\\theta},\\lambda)$ . The associated dual function $\\bar{g}(\\lambda)$ is given by $\\bar{g}(\\bar{\\lambda)}:=\\operatorname*{min}_{\\theta\\in\\Theta}\\bar{\\mathcal L}(\\theta,\\lambda)$ . Let an optimal solution to Problem (P-KL) be $\\theta^{\\star}$ . We denote $\\bar{p}:=p_{\\theta}$ and $\\bar{p}^{\\star}:=p_{\\theta^{\\star}}$ , and the optimal objective by $\\bar{F}^{\\star}:=D_{\\mathrm{KL}}(q\\,\\|\\,\\bar{p}^{\\star})$ Let an optimal dual variable be $\\lambda^{\\star}\\in\\operatorname{argmax}_{\\lambda\\,\\geq\\,0}\\bar{g}(\\lambda)$ and the optimal value of the dual function be $\\bar{D}^{\\star}:=\\dot{\\bar{g}}(\\bar{\\lambda}^{\\star})$ ", "page_idx": 5}, {"type": "text", "text": "Problem (P-KL) is non-convex in parameter space, and strong duality does not hold any more. Thus, unparametrized results in Section 3.1 don't directly apply to Problem (P-KL). For instance, it's invalid to find an optimal solution via an unconstrained problem as in Theorem 1, i.e., $\\bar{p}^{\\star}(\\bar{\\lambda}^{\\star})\\;\\in\\;\\mathrm{argmin}_{\\theta\\in\\Theta}\\,\\bar{\\mathcal{L}}(\\dot{\\theta},\\bar{\\lambda}^{\\star})$ doesn't equal $\\bar{p}^{\\star}$ . The effect of parametrization has to be characterized. However, regardless of parametrization, weak duality always holds, i.e., $\\bar{F}^{\\star}-\\bar{D}^{\\star}\\geq0$ ", "page_idx": 5}, {"type": "text", "text": "To quantify the optimality of $\\bar{p}^{\\star}(\\bar{\\lambda}^{\\star})$ (closeness of it to $q_{\\mathrm{mix}}^{\\star}$ 0, we study a practical representation of model $p_{\\theta}$ as a parametrized function $\\widehat{s}_{\\theta}\\in\\mathcal{S}_{\\theta}$ , where $\\scriptstyle{S_{\\theta}}$ is the set of all parametrized score functions. Problem (U-LOSS) is in a parametrized form of ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\theta}{\\mathrm{minimize}}\\quad\\mathbb{E}_{q(x_{0}),\\,t,\\,x_{t}}\\left[\\left\\Vert\\widehat{s}_{\\theta}(x_{t},t)-\\nabla\\log q(x_{t})\\right\\Vert^{2}\\right]}\\\\ &{\\mathrm{subject~to}\\quad\\mathbb{E}_{q^{i}(x_{0}),\\,t,\\,x_{t}}\\left[\\left\\Vert\\widehat{s}_{\\theta}(x_{t},t)-\\nabla\\log q^{i}(x_{t})\\right\\Vert^{2}\\right]\\;\\leq\\;\\widetilde{b}^{i}\\;\\mathrm{~for~}i=1,\\dots,m.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\widetilde{b}^{i}\\ :=\\ (\\bar{b}^{i}\\,-\\,v)/\\bar{\\omega}$ . We note that Problem (P-LOSS) is equivalent to Problem (P-KL). Thus, we let the Lagrangian of Problem (P-LOSS) be $\\bar{\\mathcal{L}}_{s}(\\theta,\\lambda)\\ :=\\ \\mathcal{L}_{s}(\\widehat{s}_{\\theta},\\lambda)$ , and the dual function $\\bar{g}_{s}(\\lambda)\\ :=\\operatorname*{minimize}_{\\mathbf{\\lambda}}\\mathrm{ize}_{\\theta\\in\\Theta}\\,\\bar{\\mathcal{L}}(\\theta,\\lambda)$ : Since $\\bar{g}(\\lambda)$ and $\\bar{g}_{s}(\\lambda)$ have the same maximizer ${\\bar{\\lambda}}^{\\star}$ \uff0c $\\bar{\\theta}^{\\star}\\in\\mathrm{argmin}_{\\theta\\in\\Theta}\\,\\bar{\\mathcal{L}}_{s}(\\theta,\\bar{\\lambda}^{\\star})$ , which naturally gives a dual training algorithm in Algorithm 1. ", "page_idx": 5}, {"type": "text", "text": "Denote $\\bar{s}^{\\star}:=\\widehat{s}_{\\bar{\\theta}^{\\star}}$ . Thus, ${\\bar{s}}^{\\star}$ -induced diffusion model is given by $\\bar{p}^{\\star}(\\bar{\\lambda}^{\\star})$ . Algorithm 1 works as a dual ascent method with two natural steps: (i) find a diffusion model with fixed dual variable $\\lambda(h)$ ; and (ii) update the dual variable using the (sub)gradient of the Lagrangian $\\mathcal{L}_{s}(\\widehat{s}_{\\theta}(h),\\lambda)$ . It is known that Algorithm 1 converges to ${\\bar{\\lambda}}^{\\star}$ since the dual function $\\bar{g}_{s}(\\lambda)$ is concave. However, such convergence in the dual domain doesn't provide optimality guarantee on the primal solution $\\bar{p}^{\\star}(\\bar{\\lambda}^{\\star})$ due to the non-convexity in parameter space. We next exploit the optimization properties of unparametrized diffusion models in Section 3.1 to characterize the optimality of the dual training algorithm. ", "page_idx": 5}, {"type": "text", "text": "Algorithm 1 Constrained Diffusion Models via Dual Training ", "page_idx": 6}, {"type": "text", "text": "1: Input: total diffusion steps $T$ , diffusion parameter $\\alpha_{t}$ , total iterations $H$ , stepsize $\\eta$   \n2: Initialize: $\\lambda(1)=0$   \n3: for $h=1,\\cdots\\,,H$ do   \n4: Compute model $\\begin{array}{r}{\\widehat{s}_{\\theta}(h)\\in\\underset{\\theta\\in\\Theta}{\\operatorname{argmin}}\\,\\mathcal{L}_{s}(\\widehat{s}_{\\theta},\\lambda(h))}\\end{array}$   \n5: Update the dual variable ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lambda_{i}(h+1)}&{=}&{\\left[\\lambda_{i}(h)\\,+\\,\\eta\\left(\\mathbb{E}_{x_{0}\\sim q^{i},t,x_{t}}\\left[\\left\\Vert\\widehat{s}_{\\theta}(h)(x_{t},t)-\\nabla\\log q^{i}(x_{t})\\right\\Vert^{2}\\right]\\,-\\,\\widetilde{b}^{i}\\right)\\right]_{+}\\mathrm{~for~}(\\theta)\\,}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "6: end for ", "page_idx": 6}, {"type": "text", "text": "4.2 Optimality analysis of dual training algorithm ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We analyze the optimality of $\\bar{p}^{\\star}(\\bar{\\lambda}^{\\star})$ as measured by its distance to $q_{\\mathrm{mix}}^{\\star}$ , i.e., $\\mathrm{TV}\\big(q_{\\mathrm{mix}}^{\\star},~\\bar{p}^{\\star}\\big(\\bar{\\lambda}^{\\star}\\big)\\big)$ \uff0c where we denote the total variation distance between two probability distributions $p$ and $q$ by $\\textstyle\\mathrm{TV}(q,p):={\\frac{1}{2}}\\int|p(x)-q(x)|d x$ We first exploit the convergence analysis of diffusion models, and then characterize the additional error induced by parametrization. ", "page_idx": 6}, {"type": "text", "text": "$\\bar{p}^{\\star}(\\bar{\\lambda}^{\\star})$ $q_{\\mathrm{mix}}^{(\\bar{\\lambda}^{\\star})}$ an ${\\bar{\\lambda}}^{\\star}$ Denot aralmnmimier ofthe Lagrangian by $\\bar{p}^{\\star}(\\lambda)\\in\\mathrm{argmin}_{\\theta\\in\\Theta}\\,\\bar{\\mathcal{L}}_{s}(\\theta,\\lambda)$ for $\\lambda\\geq0$ . Noting that minimiz $\\mathrm{e}_{\\theta\\in\\Theta}\\,\\bar{\\mathcal{L}}_{s}(\\theta,\\lambda)$ is an unconstraind difusionproblem, we areready toquantify thediference betwen $\\bar{p}^{\\star}(\\lambda)$ and $q_{\\mathrm{mix}}^{(\\lambda)}$ f for any $\\lambda\\geq0$ using the convergence theory of diffusion models. To do so, we assume the boundedness of samples from a mixed data distribution amix ali) for \u5165 \u2265 O, and a small score estimation eror. ", "page_idx": 6}, {"type": "text", "text": "Assumption 2 Boundedness of data). The data samles generated from $q_{\\mathrm{mix}}^{(\\lambda)}$ are bounded, i.e. $\\mathbb{P}\\left(\\left\\|x_{0}\\right\\|\\leq T^{c}\\,|\\,x_{0}\\sim q_{\\mathrm{mix}}^{(\\lambda)}\\right)=1$ forany $\\lambda\\geq0$ and somelarge constant $c>0$ ", "page_idx": 6}, {"type": "text", "text": "Assumption 3 (Boundedness of score estimation error). The score estimator $\\widehat{s}_{\\theta}(x_{t},t)$ estimatesthe data samplesfromQmix with bounded score matching error $\\varepsilon_{\\mathrm{score}}$ ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{q_{\\mathrm{mix}}^{(\\lambda)}(x_{0}),\\,t,\\,x_{t}}\\left[\\,\\|\\widehat{s}_{\\theta}(x_{t},t)-\\nabla\\log q(x_{t})\\|^{2}\\,\\right]\\;\\leq\\;\\varepsilon_{\\mathrm{score}}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "for any $\\lambda\\geq0$ where $\\mathbb{E}_{q_{\\operatorname*{mix}}^{(\\lambda)}(x_{0}),\\,t,\\,x_{t}}$ id $q_{\\mathrm{mix}}^{(\\lambda)}(x_{0})$ \uff0c $a$ uniform distribution over $t$ from 2 to $T$ . and a forward process $q(x_{t}\\mid x_{0})$ given the data sample $x_{0}$ ", "page_idx": 6}, {"type": "text", "text": "Since data samples are bounded, Assumption 2 is mild in practice. Assumption 3 is the typical score matching error that is near zero if the function class $\\ensuremath{\\mathcal{S}}_{\\theta}$ is sufficiently rich. ", "page_idx": 6}, {"type": "text", "text": "With Assumptions 2 and 3, below we bound the TV distance between gmix and $\\bar{p}^{\\star}(\\lambda)$ using the convergence theory of diffusion models from [47]; see Appendix B.5 for proof. ", "page_idx": 6}, {"type": "text", "text": "Lemma 3 (Convergence of diffusion model). Let Assumptions 2 and 3 hold. Then, the TV distance from p\\*(X) to qmix isboundedby ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathrm{TV}\\left(q_{\\mathrm{mix}}^{(\\lambda)},\\,\\bar{p}^{\\star}(\\lambda)\\right)\\;\\leq\\;\\sqrt{\\frac{1}{2}D_{\\mathrm{KL}}\\left(q_{\\mathrm{mix}}^{(\\lambda)}\\,\\|\\,\\bar{p}^{\\star}(\\lambda)\\right)}\\;\\lesssim\\;\\frac{d^{2}\\;\\log^{3}T}{\\sqrt{T}}\\,+\\,\\sqrt{d}\\,\\left(\\log^{2}T\\right)\\varepsilon_{\\mathrm{score}}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Lemma 3 states that the TV distance between and $\\bar{p}^{\\star}(\\lambda)$ decays to zero with a sublinear rate $\\textstyle O({\\frac{1}{\\sqrt{T}}})$ , up to a score matching eror $O(\\varepsilon_{\\mathrm{score}})$ . When the diffusion time $T$ is large, the TV distance between $q_{\\mathrm{mix}}^{(\\lambda)}$ and $\\bar{p}^{\\star}(\\lambda)$ is dominated by the score matching error. Substitution of $\\lambda={\\bar{\\lambda}}^{\\star}$ into (7) yilds n upper ound on $\\mathrm{TV}(q_{\\mathrm{mix}}^{(\\bar{\\lambda}^{\\star})},\\;\\bar{p}^{\\star}(\\bar{\\lambda}^{\\star}))$ ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathrm{TV}\\left(q_{\\mathrm{mix}}^{\\star},\\,\\bar{p}^{\\star}(\\bar{\\lambda}^{\\star})\\right)}&{\\leq}&{\\mathrm{TV}\\left(q_{\\mathrm{mix}}^{\\star},\\,q_{\\mathrm{mix}}^{(\\bar{\\lambda}^{\\star})}\\right)\\,+\\,\\mathrm{TV}\\left(q_{\\mathrm{mix}}^{(\\bar{\\lambda}^{\\star})},\\,\\bar{p}^{\\star}(\\bar{\\lambda}^{\\star})\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Next, we quantify the gap between ${\\bar{\\lambda}}^{\\star}$ and $\\lambda^{\\star}$ , which lets us bound the first term on the RHS of (8) and complete the optimality analysis. To analyze the parametrized optimal dual variable ${\\bar{\\lambda}}^{\\star}$ ,we introduce the richness of the parametrized class $\\scriptstyle{S_{\\theta}}$ and redundancy of constraints at $\\widehat{s}^{\\star}$ below. ", "page_idx": 6}, {"type": "text", "text": "Assumption 4 (Richness of parametrization). For any function $\\widehat{s}\\in S$ thereexistsparameter $\\theta\\in\\Theta$ suchthat $\\|\\widehat{s}_{\\theta}-\\widehat{s}\\|_{L_{2}}\\le\\nu$ Wwhere $\\left\\Vert\\cdot\\right\\Vert_{L_{2}}$ is with respect to the forward process. ", "page_idx": 7}, {"type": "text", "text": "Assumption 5 (Redundancy of constraints). There exists $\\sigma>0$ suchthat ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\|\\lambda\\|=1}\\,\\left\\|\\,\\sum_{i=1}^{m}\\lambda_{i}\\,\\nabla_{\\widehat{s}}\\mathbb{E}_{q^{i}(x_{0}),\\,t,\\,x_{t}}\\left[\\widehat{s}^{\\star}(x_{t},t)-\\nabla\\log q(x_{t})\\,\\right]\\,\\right\\|_{L_{2}}\\ \\ge\\ \\sigma\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\nabla_{\\widehat{s}}$ is the Frechet derivative over the function $\\widehat{s}$ and $\\widehat{s}^{\\star}$ is a solution to Problem (U-LOSS). ", "page_idx": 7}, {"type": "text", "text": "Assumption 4 is mild since the gap is small for expressive neural networks [56, 31]. Assumption 5 captures the linear independence of constraints, which is similarly used in optimization [4]. ", "page_idx": 7}, {"type": "text", "text": "Due to Assumption 2, we set the function class $\\boldsymbol{S}$ to be bounded $\\|\\,\\widehat{s}\\,\\|_{L_{2}}\\;\\leq\\;T^{c}\\;:=\\;R$ .Using Problem (U-LOSS), we prove that the unparametrized dual function $g_{s}(\\lambda)$ is differentiable, and strongly-concave over $\\mathcal{H}$ with parameter $\\mu$ ,where $\\mathcal{H}\\;:=\\;\\{\\gamma\\lambda^{\\star}+(\\bar{1}-\\'\\gamma)\\bar{\\lambda}^{\\star},\\gamma\\;\\in\\;[0,1]\\}$ and $\\mu:=\\left(\\sigma/\\left(1+\\operatorname*{max}\\left(\\left\\|\\lambda^{\\star}\\right\\|_{1},\\left\\|\\bar{\\lambda}^{\\star}\\right\\|_{1}\\right)\\right)\\right)^{2}$ , which leads to Lemma 4; see Appendix B.6 for their proofs. Lemma 4. Let Assumptions $^{4}$ and 5 hold. Then, $\\begin{array}{r}{\\left\\|\\bar{\\lambda}^{\\star}-\\lambda^{\\star}\\right\\|^{2}\\leq\\frac{8}{\\mu}R\\left(1+\\left\\|\\bar{\\lambda}^{\\star}\\right\\|_{1}\\right)\\nu.}\\end{array}$ Since TV $r\\left(q_{\\mathrm{mix}}^{\\star},\\ q_{\\mathrm{mix}}^{(\\bar{\\lambda}^{\\star})}\\right)$ 2\uff09 is bounded by \\*  \\*llI se Appendix B6) apliation of Lemma 3 and ", "page_idx": 7}, {"type": "text", "text": "Lemma 4 to (8) leads to Theorem 3; see Appendix B.7 for proof. ", "page_idx": 7}, {"type": "text", "text": "Theorem 3 (Optimality of constrained diffusion model). Let Assumptions $_{I-5}$ hold.Then, the total variation distance between $\\bar{p}^{\\star}(\\bar{\\lambda}^{\\star})$ and $q_{\\mathrm{mix}}^{\\star}$ is upperbounded by ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathrm{TV}\\left(q_{\\mathrm{mix}}^{\\star},\\,\\bar{p}^{\\star}(\\bar{\\lambda}^{\\star})\\right)\\,\\lesssim\\,\\frac{d^{2}\\log^{3}T}{\\sqrt{T}}\\,+\\,\\sqrt{\\frac{8}{\\mu}\\,m\\,R\\left(1+\\left\\|\\bar{\\lambda}^{\\star}\\right\\|_{1}\\right)\\nu}\\,+\\,\\sqrt{d}\\,\\left(\\log^{2}T\\right)\\,\\varepsilon_{\\mathrm{score}}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Theorem 3 states that the TV distance between $\\bar{p}^{\\star}(\\bar{\\lambda^{\\star}})$ and $q_{\\mathrm{mix}}^{\\star}$ decays to zero with a sublinear rate $\\textstyle O({\\frac{1}{\\sqrt{T}}})$ , up to a parametrization gap $O(\\sqrt\\nu)$ and a prediction error $O(\\varepsilon_{\\mathrm{score}})$ . When the parametrization is rich enough, the parametrization gap $\\nu$ and the prediction error $O(\\varepsilon_{\\mathrm{score}})$ are nearly zero. In this case, if the diffusion time $T$ is large, then $\\bar{p}^{\\star}(\\bar{\\lambda^{\\star}})$ is close to $q_{\\mathrm{mix}}^{\\star}$ in TV distance, which recovers the ideal optimal constrained model in the unparametrized case in Section 3.1. ", "page_idx": 7}, {"type": "text", "text": "4.3  Practical dual training algorithm ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Having established the optimality of our dual training method, we futher turn Algorithm 1 into a practical algorithm. First, we relax the computation of a diffusion model $\\widehat{s}_{\\theta}(h)$ in line 4 of Algorithm 1 to be approximate: $\\mathcal{L}_{s}(\\widehat{s}_{\\theta}(h),\\lambda(h))\\,\\leq\\,\\operatorname*{min}_{\\theta\\,\\in\\,\\Theta}\\mathcal{L}_{s}(\\widehat{s}_{\\theta},\\lambda(h))\\,+\\,\\varepsilon_{\\mathrm{approx}}^{2}$ , where $\\varepsilon_{\\mathrm{approx}}^{2}$ .s an approximation error of training a diffusion model given $\\lambda(h)$ . Second, we replace the gradient in line 5 of Algorithm 1 by a stochastic gradient $\\widehat{\\mathbb{E}}_{x_{0}\\sim q^{i},\\,t,\\,x_{t}}\\left[\\ |\\widehat{s}_{\\theta}(x_{t},t)-\\nabla\\log q(x_{t})||^{2}\\ \\right]$ which enables Algorithm 1 to be a stochastic algorithm, where $\\widehat{\\mathbb{E}}_{x_{0}\\sim q^{i},\\,t,\\,x_{t}}$ is an unbiased estimate of Exo \\~ q\\*,t,xt To analyze this approximate and stochastic variant of Algorithm 1, it is useful to introduce the maximum parametrized dual function in history up to step $h$ by $\\bar{g}_{\\mathrm{best}}(h)\\,:=\\,\\operatorname*{max}_{h^{\\prime}\\,\\leq\\,h}\\bar{g}_{s}(\\lambda(h^{\\prime}))$ , and an upper bound of the second-order moment of stochastic gradient $\\begin{array}{r}{S^{2}:=\\sum_{i=1}^{m}\\mathbb{E}\\big[\\left(\\widehat{\\mathbb{E}}_{x_{0}\\sim q^{i},t,\\,x_{t}}\\big[\\,\\|\\widehat{s}_{\\theta}(h)(x_{t},t)-\\nabla q(x_{t})\\|^{2}\\ \\right]-\\widetilde{b}^{i}\\,\\big)^{2}\\,|\\,\\lambda(h)\\big].}\\end{array}$ Denote the dual variable that achieves $\\bar{g}_{\\mathrm{best}}(h)$ by $\\lambda_{\\mathrm{best}}$ . To bound the TV distance between $\\bar{p}^{\\star}(\\bar{\\lambda}_{\\mathrm{best}})$ and $q_{\\mathrm{mix}}^{\\star}$ we check the TV distance between $q_{\\mathrm{mix}}^{(\\bar{\\lambda}_{\\mathrm{best}})}$ and $\\bar{p}^{\\star}(\\bar{\\lambda}_{\\mathrm{best}})$ using Lemma 3. The rest isto analyze the convergence of $\\bar{\\lambda}_{\\mathrm{best}}$ to $\\lambda^{\\star}$ via application of martingale convergence. We defer their proofs to Appendix B.8 and present the optimality of $\\bar{p}^{\\star}(\\bar{\\lambda}_{\\mathrm{best}})$ in Theorem 4. ", "page_idx": 7}, {"type": "text", "text": "Theorem 4 (Optimality of approximate constrained diffusion model). Let Assumptions $_{l-5}$ hold. Then, the total variation distance between $\\bar{p}^{\\star}(\\bar{\\lambda}_{\\mathrm{best}})$ and $q_{\\mathrm{mix}}^{\\star}$ isupperboundedby ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathrm{V}\\left(q_{\\mathrm{mix}}^{\\star},\\,\\bar{p}^{\\star}(\\bar{\\lambda}_{\\mathrm{best}})\\right)\\ \\lesssim\\ \\frac{d^{2}\\log^{3}T}{\\sqrt{T}}+\\frac{8R\\left(1+\\left\\|\\bar{\\lambda}_{\\mathrm{best}}\\right\\|_{1}\\right)}{\\mu}\\nu+\\sqrt{d}\\,\\left(\\log^{2}T\\right)\\varepsilon_{\\mathrm{score}}+\\frac{2}{\\mu}\\,\\varepsilon_{\\mathrm{approx}}^{2}+\\frac{\\eta\\,S^{2}}{\\mu}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Theorem 4 states that the TV distance between $\\bar{p}^{\\star}(\\bar{\\lambda}_{\\mathrm{best}})$ and $q_{\\mathrm{mix}}^{\\star}$ decays to zero with a sublinear rate $\\textstyle O({\\frac{1}{\\sqrt{T}}})$ , up to a parametrization gap $O(\\nu)$ , a score matching error $O(\\varepsilon_{\\mathrm{score}})$ , an approximation error $O\\big(\\varepsilon_{\\mathrm{approx}}\\big)$ , and stepsize ${\\cal O}(\\eta)$ When the parametrization is rich enough, the parametrization gap $\\nu$ and the score matching error $O(\\varepsilon_{\\mathrm{score}})$ are near zero. Thus, if the diffusion time $T$ is large, then the closeness of $\\bar{p}^{\\star}(\\bar{\\lambda}_{\\mathrm{best}})$ to $q_{\\mathrm{mix}}^{\\star}$ in TV distance is governed by the appproximation error and stepsize. ", "page_idx": 8}, {"type": "text", "text": "5 Computational experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We demonstrate the effectiveness of constrained diffusion models trained by our dual training algorithm in two constrained settings in Section 3.2; see Appendix C for experimental details. ", "page_idx": 8}, {"type": "text", "text": "Fairness to underrepresented classes. We train constrained diffusion models over three datasets: MNIST digits [44], Celeb-A faces [51], and Image-Net [63]. For MNIST and Image-Net, we create a dataset for the distribution $q$ in (P-LOSS) by taking a subset of the dataset with equal number of samples from each class. Then we make some classes under-represented by removing their samples. For each distribution $q^{i}$ , we use samples from the associated underrepresented class. For Celeb-A, our approach is similar to MNIST except we don't remove any samples due to the existence of class imbalance in the dataset ( $58\\%$ female vs $42\\%$ male). For Image-Net, since the images are of high resolution, we employ the latent diffusion scheme [62] by imposing distribution constraints in latent space. Figures 1-3 show that our constrained model samples more often from the underrepresented classes (MNIST: 4, 5, 7; Celeb-A: male; Image-Net: \u201cCassette player', \u201cFrench horn', and ^Golf ball'), leading to a more uniform sampling over all classes. This reflects our theoretical insights on promoting fairness for minority classes (see Section 3.2). Quantitatively, we observe fairly lower FID scores when training over the same dataset but with constraints (see Appendix C for further discussion on FID scores). Furthermore, our Image-Net experiment shows that our approach extends to the state-of-the-art diffusion models in latent space. ", "page_idx": 8}, {"type": "image", "img_path": "Es2Ey2tGmM/tmp/b6999d8f328d896db9ca673d3f090683c868871786353ce03abb08a263e3486a.jpg", "img_caption": ["Figure 1: Generation performance comparison of constrained and unconstrained models that are trained on MNIST with three minorities: 4, 5, 7. (Left ) Frequencies of ten digits that are generated by an unconstrained model () and our constrained model $(\\Longrightarrow$ ; (Middle ) Generated digits from unconstrained model (FID 15.9); (Right ) Generated digits from our constrained model (FID 13.4). "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Adapting pretrained model to new data. Given a pretrained diffusion model over some original dataset $\\mathcal{D}_{\\mathrm{pretrain}}$ , we fine-tune the pretrained model for generating data that resemble $\\mathcal{D}_{\\mathrm{new}}$ To cast this problem into (P-KL), we let the data distribution be $\\mathcal{D}_{\\mathrm{new}}$ i.e., $q(x_{0:T})\\,=\\,q_{\\mathrm{new}}(x_{0:T})$ and the constrained distribution be the pre-trained model, i.e.. $q^{i}(x_{0:T})=p_{\\theta_{\\mathrm{pre}}}(x_{0:T})$ . In our experiments, we pretrain a diffusion model on a subset of MNIST digits excluding a class of digits (MNIST: 9), and fine-tune this model using samples of the excluded digit. Figure 4 shows that our constrained fine-tuned model samples from the new class as well as all previous classes, whereas the model fine-tuned without the constraint quickly overfits to the new dataset (see Appendix C for details). Our constrained model generates much better high-quality samples than the unconstrained model. ", "page_idx": 8}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We have presented a constrained optimization framework for training diffusion models under distribution constraints. We have developed a Lagrangian-based dual algorithm to train such constrained diffusion models. Our theoretical analysis shows that our constrained diffusion model generates new data from an optimal mixture data distribution that satisfies the constraints, and we have demonstrated the effectiveness of our distribution constraints in reducing bias across three widely-used datasets. ", "page_idx": 8}, {"type": "image", "img_path": "Es2Ey2tGmM/tmp/c4b704eb4cb98e3dfb0fa033d19da345aa864af1276760dfa8e378ca23caa9ff.jpg", "img_caption": ["Figure 2: Generation performance comparison of constrained and unconstrained models that are trained on Celeb-A with male minority. ( Left ) Frequencies of two genders that are generated by anunconstrained model $(\\underline{{\\Gamma}})$ and our constrained model (); (Middle\uff09 Generated faces from unconstrained model (FID 19.6); (Right ) Generated faces from our constrained model (FID 11.6). "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "Es2Ey2tGmM/tmp/9a6a5b8e31b3ac42a3968ee33e1a64b7d3fd183cbc15ff7289ad9dbe8e694162.jpg", "img_caption": ["Figure 3: Generation performance comparison of constrained and unconstrained models that are trained on Image-Net with minority classes: \u201cCassette player\u2019 (2), \u201cFrench horn\u2019 (5), and ^Golf ball' (8). (Left ) Frequencies of ten classes that are generated by an unconstrained model () and our constrainedmodel $(\\Longrightarrow$ ; ( Middle ) Generated images from unconstrained model ( FID 36.0); (Right) Generated images from our constrained model ( FID 27.3 ). "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "Es2Ey2tGmM/tmp/667ab76c18420197409ca50f823182b8fdbc4af36d5071e360e0f04f4a1769bf.jpg", "img_caption": ["Figure 4: Fine-tuning performance comparison of constrained and unconstrained models that are trained on MNIST. (Left) Frequencies of ten digits that are generated by a pre-trained model withoutdigit9 $(\\Longrightarrow$ and our fine-tuned constrained model $(\\Longrightarrow$ ; (Middle) Generated digits from unconstrained model (FID 45.9); ( Right ) Generated digits from our constrained model (FID 25.2). "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "This work directly stimulates several research directions: (i) extend our distribution constraints to other domain constraints, e.g., mirror diffusion [48]; (i) incorporate conditional generations, e.g., text-to-image generation [28, 65], into our constrained diffusion models; (ii) conduct experiments with text-to-image datasets to identify and address biases; (iv) improve the convergence theory using more advanced diffusion processes. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We thank reviewers and program chairs for providing helpful comments. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1]  A. Bansal, H.-M. Chu, A. Schwarzschild, S. Sengupta, M. Goldblum, J. Geiping, and T. Goldstein. Universal guidance for diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 843-852, 2023.   \n[2] J. Benton, V. De Bortoli, A. Doucet, and G. Deligiannidis. Linear convergence bounds for diffusion models via stochastic localization. In Proceedings of the International Conference on Learning Representations, 2024. [3] J. Benton, V. De Bortoli, A. Doucet, and G. Deligiannidis. Nearly d-linear convergence bounds for diffusion models via stochastic localization. In Proceedings of the International Conference on Learning Representations, 2024.   \n[4] D. P. Bertsekas. Nonlinear programming. Athena Scientific, 2016.   \n[5]  K. Black, M. Janner, Y. Du, I. Kostrikov, and S. Levine. Training diffusion models with reinforcement learning. In Proceedings of the International Conference on Learning Representations, 2024.   \n[6]  A. Blattmann, R. Rombach, H. Ling, T. Dockhorn, S. W. Kim, S. Fidler, and K. Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22563-22575, 2023. [7]  D. M. Blei, A. Kucukelbir, and J. D. McAuliffe. Variational inference: A review for statisticians. Journal of the American statistical Association, 112(518):859-877, 2017.   \n[8] H. Cao, C. Tan, Z. Gao, Y. Xu, G. Chen, P-A. Heng, and S. Z. Li. A survey on generative diffusion models. IEEE Transactions on Knowledge and Data Engineering, 2024.   \n[9]  L. Chamon and A. Ribeiro. Probably approximately correct constrained learning. In Proceedings of the Advances in Neural Information Processing Systems, volume 33, pages 16722-16735, 2020.   \n[10] L. F. Chamon, S. Paternain, M. Calvo-Fullana, and A. Ribeiro. Constrained learning with non-convex losses. IEEE Transactions on Information Theory, 69(3):1739-1760, 2022.   \n[11] H. Chen, H. Lee, and J. Lu. Improved analysis of score-based generative modeling: Userfriendly bounds under minimal smoothness assumptions. In Proceedings of the International Conference on Machine Learning, pages 4735-4763, 2023.   \n[12] J. Chen, Z. Shao, X. Zheng, K. Zhang, and J. Yin. Integrating aesthetics and efficiency: AIdriven diffusion models for visually pleasing interior design generation. Scientific Reports, 14(1):3496, 2024.   \n[13] J. Chen, R. Zhang, Y. Zhou, and C. Chen. Towards aligned layout generation via diffusion model with aesthetic constraints. arXiv preprint arXiv:2402.04754, 2024.   \n[14] M. Chen, S. Mei, J. Fan, and M. Wang. An overview of diffusion models: Applications, guided generation, statistical rates and optimization. arXiv preprint arXiv:2404.07771, 2024.   \n[15] S. Chen, S. Chewi, J. Li, Y Li, A. Salim, and A. R. Zhang. Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions. In Proceedings of the International Conference on Learning Representations, 2023.   \n[16] K. Choi, A. Grover, T. Singh, R. Shu, and S. Ermon. Fair generative modeling via weak supervision. In Proceedings of the International Conference on Machine Learning, pages 1887-1898, 2020.   \n[17] J. K. Christopher, S. Baek, and F. Fioretto. Projected generative diffusion models for constraint satisfaction. arXiv preprint arXiv:2402.03559, 2024.   \n[18]  K. Clark, P. Vicol, K. Swersky, and D. J. Fleet. Directly fine-tuning diffusion models on differentiable rewards. In Proceedings of the International Conference on Learning Representations, 2024.   \n[19] F-A.Croitoru, V. Hondru, R. T. Ionescu, and M Shah. Difusion models in vision: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023.   \n[20] P Dhariwal and A. Nichol. Diffusion models beat gans on image synthesis. In Proceedings of the Advances in Neural Information Processing Systems, volume 34, pages 8780-8794, 2021.   \n[21] Y. Du, C. Durkan, R. Strudel, J. B. Tenenbaum, S. Dieleman, R. Fergus, J. Sohl-Dickstein, A. Doucet, and W. S. Grathwohl. Reduce, reuse, recycle: Compositional generation with energy-based diffusion models and MCMC. In Proceedings of the International conference on machine learning, pages 8489-8510, 2023.   \n[22] J. Elenter, L. F. Chamon, and A. Ribeiro. Near-optimal solutions of constrained learning problems. In Proceedings of the International Conference on Learning Representations, 2024.   \n[23] Y. Fan and K. Lee. Optimizing DDPM sampling with shortcut fine-tuning. In Proceedings of the International Conference on Machine Learning, pages 9623-9639, 2023.   \n[24] Y. Fan, O. Watkins, Y. Du, H. Liu, M. Ryu, C. Boutilier, P. Abbeel, M. Ghavamzadeh, K. Lee, and K. Lee. Reinforcement learning for fine-tuning text-to-image diffusion models. In Proceedings of the Advances in Neural Information Processing Systems, volume 36, 2023.   \n[25] B. T. Feng, R. Baptista, and K. L. Bouman. Neural approximate mirror maps for constrained diffusion models. arXiv preprint arXiv:2406.12816, 2024.   \n[26] N. Fishman, L. Klarner, V. De Bortoli, E. Mathieu, and M. J. Hutchinson. Diffusion models for constrained domains. Transactions on Machine Learning Research, 2024.   \n[27] N. Fishman, L. Klarner, E. Mathiu, M.Hutchinson, and V.De Bortoli. Metropolis sampling for constrained diffusion models. In Proceedings of the Advances in Neural Information Processing Systems, volume 36, 2024.   \n[28] F. Friedrich, M. Brack, L. Struppek, D. Hintersdorf, P. Schramowski, S. Luccioni, and K. Kersting. Fair diffusion: Instructing text-to-image generation models on fairness. arXiv preprint arXiv:2302.10893, 2023.   \n[29] G. Giannone, A. Srivastava, O. Winther, and F. Ahmed. Aligning optimization trajectories with difusion models for constrained design generation. In Proceedings of the Advances in Neural Information Processing Systems, volume 36, 2023.   \n[30] S. Gugger, L. Debut, T. Wolf, P. Schmid, Z. Mueller, S. Mangrulkar, M. Sun, and B. Bossan. Accelerate: Training and inference at scale made simple, efficient and adaptable. https: //github.com/huggingface/accelerate, 2022.   \n[31]  Y. Han, M. Razaviyayn, and R. Xu. Neural network-based score estimation in diffusion models: Optimization and generalization. arXiv preprint arXiv:2401.15604, 2024.   \n[32] Y. Hao, Z. Chi, L. Dong, and F. Wei. Optimizing prompts for text-to-image generation. In Proceedings of the Advances in Neural Information Processing Systems, volume 36, 2023.   \n[33]  M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Proceedings of the Advances in Neural Information Processing Systems, volume 30, 2017.   \n[34] J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. In Proceedings of the Advances in Neural Information Processing Systems, volume 33, pages 6840-6851, 2020.   \n[35] J. Ho and T. Salimans. Classifier-fre diffusion guidance. In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, 2021.   \n[36] 1. Hounie, A. Ribeiro, and L. F. Chamon. Resilient constrained learning In Proceedings of the Advances in Neural Information Processing Systems, volume 36, 2023.   \n[37]  C.-W. Huang, M. Aghajohari, J. Bose, P. Panangaden, and A. C. Courville. Riemannian diffusion models. In Proceedings of the Advances in Neural Information Processing Systems, volume 35, pages 2750-2761, 2022.   \n[38] D.Z. Huang, J. Huang, and Z. Lin. Convergence analysis of probability flow ODE for scorebased generative models. arXiv preprint arXiv:2404.09730, 2024.   \n[39] L. Huang, T. Xu, Y. Yu, P. Zhao, K.-C. Wong, and H. Zhang. A dual diffusion model enables 3D binding bioactive molecule generation and lead optimization given target pockets. bioRxiv, pages 2023-01, 2023.   \n[40] A. Kazerouni, E. K. Aghdam, M. Heidari, R. Azad, M. Fayyaz, I. Hacihaliloglu, and D. Merhof. Diffusion models in medical imaging: A comprehensive survey. Medical Image Analysis, page 102846, 2023.   \n[41] Y. Kim, B. Na, M. Park, J. Jang, D. Kim, W. Kang, and I.-C. Moon. Training unbiased diffusion models from biased dataset. In Proceedings of the International Conference on Learning Representations, 2024.   \n[42] D. Kingma and R. Gao. Understanding diffusion objectives as the ELBO with simple data augmentation. In Proceedings of the Advances in Neural Information Processing Systems, volume 36, 2023.   \n[43] Z. Kong, W. Ping, J. Huang, K. Zhao, and B. Catanzaro. DiffWave: A versatile diffusion model for audio synthesis. In Proceedings of the International Conference on Learning Representations, 2020.   \n[44] Y, LeCun, C. Cortes, and C. J. Burges. The MNIST database of handwritten digits. http: //yann.lecun.com/exdb/mnist/.   \n[45] K. Lee, H. Liu, M. Ryu, O. Watkins, Y. Du, C. Boutilier, P. Abbeel, M. Ghavamzadeh, and S. S. Gu. Aligning text-to-image models using human feedback. arXiv preprint arXiv:2302.12192, 2023.   \n[46] G. Li, Y. Huang, T. Efmov, Y. Wei, Y. Chi, and Y. Chen. Accelerating convergence of scorebased diffusion models, provably. arXiv preprint arXiv:2403.03852, 2024.   \n[47]  G. Li, Y. Wei, Y. Chen, and Y. Chi. Towards faster non-asymptotic convergence for diffusionbased generative models. In Proceedings of the International Conference on Learning Representations, 2024.   \n[48]  G.-H. Liu, T. Chen, E. Theodorou, and M. Tao. Mirror diffusion models for constrained and watermarked generation. In Proceedings of the Advances in Neural Information Processing Systems, volume 36, 2023.   \n[49] N. Liu, S. Li, Y. Du, A. Torralba, and J. B. Tenenbaum. Compositional visual generation with composable diffusion models. In Proceedings of the European Conference on Computer Vision, pages 423-439, 2022.   \n[50] X. Liu, X. Tong, and Q. Liu. Sampling with trusthworthy constraints: A variational gradient framework. In Proceedings of the Advances in Neural Information Processing Systems, pages 23557-23568, 2021.   \n[51] Z. Liu, P. Luo, X. Wang, and X. Tang. Large-scale celebfaces atributes (celeba) dataset. https: //mmlab.ie.cuhk.edu.hk/projects/CelebA.html.   \n[52] 1. Loshchilov and F. Hutter. Decoupled weight decay regularization, 2019.   \n[53]  A. Lou and S. Ermon. Reflected diffusion models. In Proceedings of the International Conference on Machine Learning, pages 22675-22701, 2023.   \n[54]  A. S. Luccioni, C. Akiki, M. Mitchell, and Y. Jernite. Stable bias: Analyzing societal representations in diffusion models. arXiv preprint arXiv:2303.11408, 2023.   \n[55] C. Luo2os Understanding diffusion models: A unified perspective.  arXiv preprint arXiv:2208.11970, 2022.   \n[56]  S. Mei and Y. Wu. Deep networks as denoising algorithms: Sample-efficient learning of diffusion models in high-dimensional graphical models. arXiv preprint arXiv:2309.11420, 2023.   \n[57]  R. Naik and B. Nushi. Social biases through the text-to-image generation lens. In Proceedings of the AAAIACM Conference on Al, Ethics, and Society, pages 786-808, 2023.   \n[58]  G. Parmar, R. Zhang, and J-Y. Zhu. On aliased resizing and surprising subtleties in gan evaluation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11410-11420, 2022.   \n[59] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B.Steiner, L. Fang, J. Bai, and S. Chintala PyTorch: An imperative style, high-performance deep learning library, 2019.   \n[60]  T. Power, R. Soltani-Zarrin, S. Iba, and D. Berenson. Sampling constrained trajectories using composable diffusion models. In Proceedings of the IROS 2023 Workshop on Differentiable Probabilistic Robotics: Emerging Perspectives on Robot Learning, 2023.   \n[61] D. J. Rezende and F. Viola. Generalized ELBO with constrained optimization, GECO. In Workshop on Bayesian Deep Learning, NeurIPS, 2018.   \n[62] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with latent difusion models. In Procedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684-10695, 2022.   \n[63] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211-252, 2015.   \n[64] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour, R. Gontijo Lopes, B. Karagol Ayan, T. Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. In Proceedings of the Advances in Neural Information Processing Systems, volume 35, pages 36479-36494, 2022.   \n[65] X. Shen, C. Du, T. Pang, M. Lin, Y. Wong, and M. Kankanhalli. Finetuning text-to-image difusion models for fairness. In Proceedings of the International Conference on Learning Representations, 2024.   \n[66] J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In Proceedings of the International Conference on Machine Learning, pages 2256-2265, 2015.   \n[67] V. Solo and X. Kong. Adaptive signal processing algorithms: stability and performance. Prentice-Hall, Inc., 1994.   \n[68] J. Song, C. Meng, and S. Ermon. Denoising diffusion implicit models. In Proceedings of the International Conference on Learning Representations, 2021.   \n[69]  C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1-9, 2015.   \n[70]  W. Tang. Fine-tuning of diffusion models via stochastic control: entropy regularization and beyond. arXiv preprint arXiv:2403.06279, 2024.   \n[71] M. Uehara, Y. Zhao, K. Black, E. Hajiramezanali, G. Scalia, N. L. Diamant, A. M. Tseng, T. Biancalani, and S. Levine. Fine-tuning of continuous-time diffusion models as entropyregularized control. arXiv preprint arXiv:2402.15194, 2024.   \n[72] M. Uehara, Y. Zhao, E. Hajiramezanali, G. Scalia, G. Eraslan, A. Lal, S. Levine, and T. Biancalani. Bridging model-based optimization and generative modeling via conservative fine-tuning of diffusion models. arXiv preprint arXiv:2405.19673, 2024.   \n[73] P. von Platen, S. Patil, A. Lozhkov, P. Cuenca, N. Lambert, K. Rasul, M. Davaadorj, D. Nair, S. Paul, W. Berman, Y. Xu, S. Liu, and T. Wolf. Diffusers: State-of-the-art diffusion models. https://github.com/huggingface/diffusers, 2022.   \n[74] J. L. Watson, D. Juergens, N. R. Bennett, B. L. Trippe, J. Yim, H. E. Eisenach, W. Ahern, A. J. Borst, R. J. Ragotte, L. F. Milles, et al. De novo design of protein structure and function with RFdiffusion. Nature, 620(7976):1089-1100, 2023.   \n[75] T. Weiss, E. Mayo Yanes, S. Chakraborty, L. Cosmo, A. M. Bronstein, and R. Gershoni-Poranne. Guided diffusion for inverse molecular design. Nature Computational Science, 3(10):873-882, 2023.   \n[76] X. Wu, K. Sun, F. Zhu, R. Zhao, and H. Li. Human preference score: Better aligning textto-image models with human preference. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2096-2105, 2023.   \n[77] J. Xu, X. Liu, Y. Wu, Y. Tong, Q. Li, M. Ding, J. Tang, and Y. Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation. In Proceedings of the Advances in Neural Information Processing Systems, volume 36, 2023.   \n[78] L. Yang, Z. Zhang, Y. Song, S. Hong, R. Xu, Y. Zhao, W. Zhang, B. Cui, and M.-H. Yang. Diffusion models: A comprehensive survey of methods and applications. ACM Comput. Surv., 56(4), nov 2023.   \n[79] L. Yang, Z. Zhang, Y. Song, S. Hong, R. Xu, Y. Zhao, W. Zhang, B. Cui, and M.-H. Yang. Diffusion models: A comprehensive survey of methods and applications. ACM Computing Surveys, 56(4):1-39, 2023.   \n[80] H. Yuan, K. Huang, C. Ni, M. Chen, and M. Wang. Reward-directed conditional diffusion: Provable distribution estimation and reward improvement. In Proceedings of the Advances in Neural Information Processing Systems, volume 36, 2023.   \n[81] C. Zhang, C. Zhang, S. Zheng, M. Zhang, M. Qamar, S.-H. Bae, and I. S. Kweon. Audio diffusion model for speech synthesis: A survey on text to speech and speech enhancement in generative AI. arXiv preprint arXiv:2303.13336, 2023.   \n[82] Z. Zhang, S. Zhang, Y. Zhan, Y. Luo, Y. Wen, and D. Tao. Confronting reward overoptimization for diffusion models: A perspective of inductive and primacy biases. In Proceedings of the International Conference on Machine Learning, 2024. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Supplementary Materials for \"Constrained Diffusion Models via Dual Training' ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A Details on ELBO ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Recall the evidence lower bound (ELBO), ", "page_idx": 15}, {"type": "equation", "text": "$$\nE(p;q)\\;:=\\;\\mathbb{E}_{q(x_{0})}\\mathbb{E}_{q(x_{1:T}\\mid x_{0})}\\log\\frac{p(x_{0:T})}{q\\big(x_{1:T}\\mid x_{0}\\big)},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "we can utilize conditionals to expand it into ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{E(p;q)}&{=}&{\\underbrace{{\\mathbb{E}}_{q(x_{0})}{\\mathbb{E}}_{q(x_{1}\\mid x_{0})}\\left[\\log p(x_{0}\\mid x_{1})\\right]}_{\\mathrm{reconstruction~likelihood}}-\\underbrace{{\\mathbb{E}}_{q(x_{0})}\\left[\\,D_{\\mathrm{KL}}(q(x_{T}\\mid x_{0})\\,\\Vert\\,p(x_{T}))\\,\\right]}_{\\mathrm{final~latent~mismatch}}}\\\\ &&{-\\underbrace{\\sum_{t=2}^{T}{\\mathbb{E}}_{q(x_{0})}{\\mathbb{E}}_{q(x_{t}\\mid x_{0})}\\left[\\,D_{\\mathrm{KL}}\\left(q(x_{t-1}\\mid x_{t},x_{0})\\,\\Vert\\,p(x_{t-1}\\mid x_{t})\\right)\\,\\right]}_{.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "denoising matching term ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "where the first term is the reconstruction likelihood of the original data given the first latent $x_{1}$ ,the second term is the mismatch between the final latent distribution and the Guassian prior, and the last summation measures the mismatch between the denoising transitions from forward/backward processes. With the variance schedule described in Section 2.1, it is known that the reconstruction likelihood and final latent mismatch are negligible, and thus the approximation in (4) is almost exact, which is our focal setting of this paper. ", "page_idx": 15}, {"type": "text", "text": "We next focus on one summand of the denoising matching term, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}_{q(x_{0})}\\mathbb{E}_{q(x_{t}\\,|\\,x_{0})}\\left[\\,D_{\\mathrm{KL}}\\left({q(x_{t-1}\\,|\\,x_{t},x_{0})}\\,\\Vert\\,p(x_{t-1}\\,|\\,x_{t})\\right)\\,\\right].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Application of the reparametrization trick leads to $x_{t}=\\sqrt{\\alpha_{t}}x_{t-1}+\\sqrt{1-\\alpha_{t}}\\epsilon_{t-1}$ where $\\epsilon_{t-1}\\,\\sim$ $\\bar{\\mathcal{N}}(0,I)$ is a white noise sample. Using Bayes rule, we can express $q(x_{t-1}\\mid x_{t},x_{0})$ as a Guassian distribution ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{N}(x_{t-1};\\mu_{q}(x_{t}),\\sigma_{q}(t)I)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\begin{array}{r}{\\mu_{q}(x_{t})=\\frac{1}{\\sqrt{\\alpha_{t}}}x_{t}+\\frac{1-\\alpha_{t}}{\\sqrt{\\alpha_{t}}}\\nabla\\log q(x_{t})}\\end{array}$ is the mean and $\\begin{array}{r}{\\sigma_{q}^{2}(t)=\\frac{(1-\\alpha_{t})(1-\\bar{\\alpha}_{t-1})}{1-\\bar{\\alpha}_{t}}}\\end{array}$ is the vrance. ", "page_idx": 15}, {"type": "text", "text": "To stay close to the ground-truth backward conditional $q(x_{t-1}\\mid x_{t},x_{0})$ as much as possible, we take $p(x_{t-1}\\mid x_{t})$ to be the same as $q(x_{t-1}\\mid x_{t},x_{0})$ except replacing $\\nabla\\log{p(x_{t})}$ by $\\widehat{\\boldsymbol{s}}(\\boldsymbol{x}_{t},t)$ and $\\sigma_{q}^{2}(t)$ by $\\sigma_{p}^{2}(t)$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{N}(x_{t-1};\\widehat{\\mu}(x_{t}),\\sigma_{p}(t)I)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\begin{array}{r}{\\widehat{\\mu}(x_{t})=\\frac{1}{\\sqrt{\\alpha_{t}}}x_{t}+\\frac{1-\\alpha_{t}}{\\sqrt{\\alpha_{t}}}\\widehat{s}(x_{t},t)}\\end{array}$ . Thus, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D_{\\mathrm{KL}}\\left(q(x_{t-1}\\mid x_{t},x_{0})\\parallel p(x_{t-1}\\mid x_{t})\\right)}\\\\ {=}&{D_{\\mathrm{KL}}(\\mathcal{N}(x_{t-1};\\mu_{q}(x_{t}),\\sigma_{q}(t)I)\\parallel\\mathcal{N}(x_{t-1};\\widehat{\\mu}(x_{t}),\\sigma_{p}(t)I))}\\\\ {=}&{\\frac{1}{2}\\left(d\\log\\frac{\\sigma_{p}^{2}(t)}{\\sigma_{q}^{2}(t)}-d+d\\frac{\\sigma_{p}^{2}(t)}{\\sigma_{q}^{2}(t)}+\\frac{1}{\\sigma_{q}^{2}(t)}\\left\\Vert\\mu_{q}(x_{t},x_{0})-\\widehat{\\mu}(x_{t},x_{0})\\right\\Vert^{2}\\right)}\\\\ {=}&{\\underbrace{\\frac{1}{2}\\left(d\\log\\frac{\\sigma_{p}^{2}(t)}{\\sigma_{q}^{2}(t)}-d+d\\frac{\\sigma_{p}^{2}(t)}{\\sigma_{q}^{2}(t)}\\right)}_{:=:\\underbrace{\\nu_{t,x,x,x,x,x,x,x,x,x,x,x,x,x,x,x,x,x,x,x,x,x,x,x,x,x,x,x,x,x,x,x,x,x,x,x,x,x,x,x,x,x,x,x,x,x,x,x,x,x,}}}\\end{array}\n$$prediction loss ", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the second equality is due to the KL Divergence between two Gaussians. Since $\\sigma_{p}^{2}(t)$ and $\\sigma_{q}^{2}(t)$ are constants, the variance mismatch term is irrelevant to optimization. Denote $\\begin{array}{r}{\\omega_{t}:=\\frac{\\stackrel{}{}_{(1-\\alpha_{t})}\\stackrel{}{2}}{2\\sigma_{q}^{2}(t)\\alpha_{t}}}\\end{array}$ and $\\bar{\\omega}:=\\sum_{t=2}^{T}\\omega_{t}$ We can defne discre ditribution over the set $\\{2,\\ldots,T\\}$ ", "page_idx": 15}, {"type": "text", "text": "$\\begin{array}{r}{p_{\\omega}(t):=\\frac{\\omega_{t}}{\\bar{\\omega}}}\\end{array}$ Alsodenote $\\begin{array}{r}{v:=\\sum_{t=2}^{T}\\frac{1}{2}\\left(d\\log\\frac{\\sigma_{p}^{2}(t)}{\\sigma_{q}^{2}(t)}-d+d\\frac{\\sigma_{p}^{2}(t)}{\\sigma_{q}^{2}(t)}\\right)}\\end{array}$ Hence, the ELBO maximization: maximizep $E(p;q)$ , is equivalent to the quadratic loss minimization, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{minimize}_{\\hat{s}}\\;\\;v\\,+\\,\\bar{\\omega}\\,\\mathbb{E}_{{x_{0}},\\,t,\\,{x_{t}}}\\left[\\,\\Vert\\widehat{s}(x_{t},t)-\\nabla\\log q(x_{t})\\Vert^{2}\\,\\right]\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Where $\\mathbb{E}_{x_{0},\\,t,\\,x_{t}}$ is an expectation over the data distribution $q(x_{0})$ , the discrete distribution $p_{\\omega}(t)$ from 2 to $T$ , and a forward process $q(x_{t}\\mid x_{0})$ given the data sample $x_{0}$ . Since shifting an objective function by a constant and scaling an objective function by a constant don't change the solution of an optimization problem, we omit constants $v$ and $\\bar{\\omega}$ for brevity, and only emphasize them whenever it is necessary. Hence, the ELBO maximization equals the quadratic loss minimization, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{minimize}_{\\widehat{s}}\\;\\;\\mathbb{E}_{{x_{0}},\\,t,\\,x_{t}}\\left[\\,\\|\\widehat{s}(x_{t},t)-\\nabla\\log q(x_{t})\\|^{2}\\,\\right]\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "up to some scaling and shifting constants, where $\\mathbb{E}_{x_{0},t,x_{t}}$ is an expectation over the data distribution $q(x_{0})$ , the discrete distribution $p_{\\omega}(t)$ from 2 to $T$ , and a forward process $q(x_{t}\\mid x_{0})$ given the data sample $x_{0}$ . In practice, however, we have to parametrize the estimator $\\widehat{\\boldsymbol{s}}(\\boldsymbol{x}_{t},t)$ as $\\widehat{s}_{\\theta}(x_{t},t)$ with parameter $\\theta\\in\\Theta$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\underset{\\theta\\in\\Theta}{\\mathrm{minimize}}\\,\\,\\,\\mathbb{E}_{x_{0},\\,t,\\,x_{t}}\\left[\\,\\|\\widehat{s}_{\\theta}\\big(x_{t},t\\big)-\\nabla\\log q(x_{t})\\|^{2}\\right]\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which is our focal objective of generative modeling. A parametrized representation of $p(x_{t-1}\\mid x_{t})$ associatedwith $\\widehat{s}_{\\theta}(x_{t},t)$ is denotedby $p_{\\theta}\\big(x_{t-1}\\mid x_{t}\\big)$ and the backward process has a parametrized joint distribution $p_{\\theta}(x_{0:T})$ . We remark that the above prediction problem can be reformulated as data or noise predictions [55], with our results directly transferrable to these formulations. ", "page_idx": 16}, {"type": "text", "text": "B Proofs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We provide proofs of all lemmas and theorems in the main paper. ", "page_idx": 16}, {"type": "text", "text": "B.1 Proof of Lemma 1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof. The ELBO maximization has the same optimal solution with the KL divergence minimization because of the equality (3). This directly proves the second equivalence. Next, we relate these two problems to the likelihood maximization problem. ", "page_idx": 16}, {"type": "text", "text": "We note that the KL divergence is non-negative and is zero if and only if two distributions are the same.Since $q(x_{0:T})\\in\\mathcal{P}$ for large $T$ , the solution of the ELBO maximization and KL divergence minimization is given by $p^{\\star}=q$ . For the KL divergence minimization, the optimal value is zero. For the optimal value of the ELBO maximization, from (3) it follows that: ", "page_idx": 16}, {"type": "equation", "text": "$$\nE(p^{\\star};q)\\;=\\;\\mathbb{E}_{q(x_{0})}[\\log q(x_{0})]-D_{\\mathrm{KL}}(q(x_{0:T})\\,\\|\\,p^{\\star}(x_{0:T}))\\;=\\;\\mathbb{E}_{q(x_{0})}[\\log q(x_{0})].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "It is clear that the likelihood maximization problem maximizep $\\mathbb{E}_{q(x_{0})}[\\log p(x_{0})]$ is equivalent to minimizep $D_{\\mathrm{KL}}(q(x_{0})\\parallel p(x_{0}))$ . Therefore, any distribution $p^{\\star}(x_{0:T})$ whose marginal satisfies $p^{\\star}(x_{0})=q(\\dot{x}_{0})$ , will be a solution of the likelihood maximization problem. This includes the solution of the KL divergence minimization and ELBO maximization which is $\\boldsymbol{p}^{\\star}=\\boldsymbol{q}$ . Therefore, ", "page_idx": 16}, {"type": "equation", "text": "$$\n{\\underset{p\\,\\in\\,{\\mathcal{P}}}{\\mathrm{maximize~}}}\\ E(p;q)\\;\\Rightarrow\\;{\\underset{p\\,\\in\\,{\\mathcal{P}}}{\\mathrm{maximize~}}}\\mathbb{E}_{q(x_{0})}[\\log p(x_{0})]\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which concludes the proof. ", "page_idx": 16}, {"type": "text", "text": "B.2 Proof of Lemma 2 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof. It is straightforward to check the zero duality gap in convex optimization; see e.g., [4, Proposition 5.3.2]. Furthermore, for a convex optimization problem, an optimal dual variable $\\lambda^{\\star}$ that maximizes the dual function is a geometric multiplier. Hence, $(p^{\\star},\\lambda^{\\star})$ is an optimal primal-dual pair of the convex optimization problem. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "B.3Proof of Theorem 1 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proof. From the strong duality in Lemma 2, it is known from [4, Proposition 5.1.4] that $\\lambda^{\\star}$ is alsoa geometric multiplier. Thus, Problem (U-KL) reduces to an unconstrained problem, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{minimize}_{p\\in{\\mathcal{P}}}~{\\mathcal{L}}(p,\\lambda^{\\star})\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the objective function results from plugging an optimal dual variable $\\lambda^{\\star}$ into Lagrangian $\\mathcal{L}(\\boldsymbol{p},\\lambda)$ ", "page_idx": 17}, {"type": "text", "text": "By the definition of Lagrangian, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathcal{L}(p,\\lambda)}&{=}&{D_{\\mathrm{KL}}(q(x_{0:T})\\parallel p(x_{0:T}))+\\displaystyle\\sum_{i=1}^{m}\\lambda_{i}\\left(D_{\\mathrm{KL}}\\left(q^{i}(x_{0:T})\\parallel p(x_{0:T})\\right)-b_{i}\\right)}\\\\ &{=}&{-\\,E(p;q)-\\displaystyle\\sum_{i=1}^{m}\\lambda_{i}\\,E(p;q^{i})}\\\\ &&{+\\,\\mathbb{E}_{q(x_{0})}\\left[\\log q(x_{0})\\right]+\\displaystyle\\sum_{i=1}^{m}\\lambda_{i}\\left(\\mathbb{E}_{q(x_{0})}\\left[\\log q^{i}(x_{0})\\right]-b_{i}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By taking $\\lambda=\\lambda^{\\star}$ , Problem (12) is equivalent to ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{maximize}_{p\\,\\in\\,\\mathcal{P}}~~E(p;q)\\,+\\,\\sum_{i\\,=\\,1}^{m}\\lambda_{i}^{\\star}\\,E(p;q^{i}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "From the definition of ELBO, we know that ", "page_idx": 17}, {"type": "equation", "text": "$$\nE(p;q)+\\sum_{i=1}^{m}\\lambda_{i}^{\\star}E(p;q^{i})\\;=\\;\\left(\\mathbb{E}_{q(x_{0})}+\\sum_{i=1}^{m}\\lambda_{i}^{\\star}\\mathbb{E}_{q^{i}(x_{0})}\\right)\\mathbb{E}_{q(x_{1:T}\\mid x_{0})}\\log\\frac{p(x_{0:T})}{q(x_{1:T}\\mid x_{0})}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where we use the fact that the forward processes have the same marginal distribution given any initial data samples. Normalization of intial data distribtions leads to ci\\*). Thus, Problem (13) is equivalentto ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{maximize}_{p\\in\\mathcal{P}}~E(p;q_{\\mathrm{mix}}^{(\\lambda^{\\star})})\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which, together with Lemma 1, completes the proof. ", "page_idx": 17}, {"type": "text", "text": "B.4 Proof of Theorem 2 and Feasibility Criterion ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We start with the proof of Theorem 2. ", "page_idx": 17}, {"type": "text", "text": "Proof. Similar to the proof of Theorem 1, we begin with the Lagrangian of Problem (U-ELBO), ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{{\\mathcal{L}(p,\\lambda)}}&{{=}}&{{-E(p;q)\\,-\\displaystyle\\sum_{i=1}^{m}\\lambda_{i}\\left(E(p;q^{i})+\\bar{b}_{i}\\right)}}\\\\ {{}}&{{}}&{{}}\\\\ {{}}&{{=}}&{{\\lambda^{T}\\,{\\bf1}\\left(-\\displaystyle\\sum_{i=1}^{m}\\frac{\\lambda_{i}}{\\lambda^{T}\\,\\mathrm{I}}E(p;q^{i})\\right)\\,-\\lambda^{T}\\bar{b}}}\\\\ {{}}&{{}}&{{}}\\\\ {{}}&{{=}}&{{\\lambda^{T}\\,{\\bf1}\\left(-\\displaystyle\\sum_{i=1}^{m}\\frac{\\lambda_{i}}{\\lambda^{T}\\,\\mathrm{I}}\\mathbb{E}_{q}(\\alpha_{0})\\mathbb{E}_{q}(\\alpha_{1:\\,T}\\,|\\,x_{0})\\log\\frac{p(x_{0:T})}{q(x_{1:T}\\,|\\,x_{0})}\\right)\\,-\\lambda^{T}\\bar{b}}}\\\\ {{}}&{{}}&{{}}\\\\ {{}}&{{=}}&{{\\lambda^{T}\\,{\\bf1}\\left(-\\mathbb{E}_{q}(\\alpha_{1:\\,(x_{0})}\\mathbb{E}_{q}(\\alpha_{1:\\,T}\\,|\\,x_{0})\\log\\frac{p(x_{0:T})}{q(x_{1:T}\\,|\\,x_{0})}\\right)\\,-\\,\\lambda^{T}\\bar{b}}}\\\\ {{}}&{{}}&{{}}\\\\ {{}}&{{=}}&{{-(\\lambda^{T}\\,\\mathrm{I})E(p;q^{(x)})\\;-\\lambda^{T}\\bar{b}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where from (14) onwards we use notation: $\\lambda_{0}=1$ $\\bar{b}_{0}=0$ $\\lambda=\\left[\\lambda_{0},\\ldots,\\lambda_{m}\\right]^{T}$ \uff0c $\\bar{\\boldsymbol{b}}=\\left[\\bar{b}_{0},\\dots,\\bar{b}_{m}\\right]^{T}$ and use $q^{0}$ to represent $q$ , which will be used in the rest of proof. To formulate the dual problem, we check the minimum of the Lagrangian, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{g(\\lambda)}&{:=}&{\\underset{\\boldsymbol{p}\\in\\mathcal{P}}{\\mathrm{minimize}}\\,\\,\\mathcal{L}(\\boldsymbol{p},\\boldsymbol{\\lambda})}\\\\ &{=}&{\\underset{\\boldsymbol{p}\\in\\mathcal{P}}{\\mathrm{minimize}}\\,-(\\boldsymbol{\\lambda}^{T}\\mathbf{1})\\boldsymbol{E}(\\boldsymbol{p};q^{(\\lambda)})-\\boldsymbol{\\lambda}^{T}\\bar{\\boldsymbol{b}}}\\\\ &{=}&{-\\,\\boldsymbol{\\lambda}^{T}\\bar{\\boldsymbol{b}}\\,+\\,(\\boldsymbol{\\lambda}^{T}\\mathbf{1})\\,\\underset{\\boldsymbol{p}\\in\\mathcal{P}}{\\mathrm{minimize}}\\,\\,-E(\\boldsymbol{p};q^{(\\lambda)})}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the only term that depends on $p$ is the ELBO. Recall that: ", "page_idx": 18}, {"type": "equation", "text": "$$\nD_{\\mathrm{KL}}(q(x_{0:T})\\,\\|\\,p(x_{0:T}))\\;=\\;-\\,E(p;q)\\,+\\,\\mathbb{E}_{q(x_{0})}\\,[\\log q(x_{0})\\,]\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Since the minimum value of the KL divergence is zero (attained when $p\\,=\\,q)$ , the minimum of $-E(p;q)$ is likewise attained when $p=q$ . Thus, it is straightforward that the minimum is equal to the entropy of the distribution $q$ , denoted by $h(q):=-\\Bar{\\mathbb{E}}_{q(x_{0})}\\left[\\log q(x_{0})\\right]$ . With this in mind, from (15) we have ", "page_idx": 18}, {"type": "equation", "text": "$$\ng(\\lambda)\\;=\\;-\\:\\lambda^{T}\\bar{b}\\;+\\;(\\lambda^{T}{\\bf1})\\:h(q^{(\\lambda)}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Thus, the dual problem reads ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{maximize}_{\\lambda\\,\\geq\\,0}\\;\\;\\;g(\\lambda)\\;:=\\;-\\,\\lambda^{T}\\bar{b}\\,+\\,(\\lambda^{T}{\\bf1})\\;h(q^{(\\lambda)}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We first reformulate the entropy of the mixture distribution $q^{(\\lambda)}$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{c l}{{h_{1}(q^{(k)})}}&{{=}}&{{\\displaystyle-\\sum_{q\\neq\\operatorname{t}(n_{2}}\\left[\\log|\\alpha(\\mu)|\\right]}}\\\\ {{}}&{{}}&{{}}\\\\ {{}}&{{=}}&{{\\displaystyle-\\int\\sum_{i=0}^{m}\\frac{\\lambda_{i}}{\\lambda^{1}}q^{i}(x_{0})\\log\\left(\\frac{m}{\\lambda}\\frac{\\lambda_{i}}{\\lambda^{1}}q^{i}(x_{0})\\right)d x_{0}}}\\\\ {{}}&{{}}&{{}}\\\\ {{}}&{{=}}&{{\\displaystyle-\\int\\sum_{i=0}^{m}\\frac{\\lambda_{i}}{\\lambda^{1}}{\\hat{\\pi}}_{1}^{i}(x_{0})\\log\\left(\\frac{\\lambda_{i}}{\\lambda^{1}}q^{i}(x_{0})\\right)d x_{0}}}\\\\ {{}}&{{}}&{{}}\\\\ {{}}&{{=}}&{{\\displaystyle-\\sum_{i=0}^{m}\\frac{\\lambda_{i}}{\\lambda^{1}}\\int\\frac{q^{i}(x_{0})\\log\\left(q^{(k)}(x_{0})\\right)d x_{0}}{=-h_{1}}-\\sum_{i=0}^{m}\\frac{\\lambda_{i}}{\\lambda^{1}}\\log\\left(\\frac{\\lambda_{i}}{\\lambda^{1}}\\right)}}\\\\ {{}}&{{}}&{{}}\\\\ {{}}&{{=}}&{{\\displaystyle\\sum_{i=0}^{m}\\frac{\\lambda_{i}}{\\lambda^{1}}h_{i}-\\sum_{i=0}^{m}\\frac{\\lambda_{i}}{\\lambda^{1}}\\log\\left(\\frac{\\lambda_{i}}{\\lambda^{1}}\\right)}}\\\\ {{}}&{{}}&{{}}\\\\ {{}}&{{=}}&{{\\displaystyle\\sum_{i=0}^{m}\\frac{\\lambda_{i}}{\\lambda^{1}}h_{i}-\\sum_{i=0}^{m}\\frac{\\lambda_{i}}{\\lambda^{1}}\\log\\left(\\lambda\\right)+\\log(x^{1})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where going from (16) to (17) we utilize the assumption that the distributions $\\{q^{i}\\}_{i=0}^{m}$ have disjoint supports; see Remark 1 on when this is the case. ", "page_idx": 18}, {"type": "text", "text": "Now, we can compute the gradient of the dual function over $\\lambda_{i}$ $i=1,\\hdots,m$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\frac{\\partial}{\\partial\\lambda_{i}}\\left(-\\lambda^{T}\\bar{b}+(\\lambda^{T}{\\bf1})\\,h(q^{(\\lambda)})\\right)}&{=}&{\\displaystyle\\frac{\\partial}{\\partial\\lambda_{i}}\\left(-\\lambda^{T}\\bar{b}+\\sum_{j=0}^{m}\\lambda_{j}h_{j}-\\sum_{j=0}^{m}\\lambda_{j}\\log\\lambda_{j}+(\\lambda^{T}{\\bf1})\\log(\\lambda^{T}{\\bf1})\\right.}\\\\ &{=}&{\\displaystyle h_{i}-\\bar{b}_{i}-\\log\\left(\\frac{\\lambda_{i}}{\\lambda^{T}{\\bf1}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Setting the gradient be zeros allows us to find the optimal dual variables $\\lambda^{\\star}$ ", "page_idx": 18}, {"type": "equation", "text": "$$\nh_{i}-\\bar{b}_{i}-\\log\\left(\\frac{\\lambda_{i}^{\\star}}{(\\lambda^{\\star})^{T}\\mathbf{1}}\\right)\\;=\\;0\\;\\;\\;\\mathrm{for}\\;i=1,\\ldots,m.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Hence, ", "page_idx": 19}, {"type": "equation", "text": "$$\n{\\frac{\\lambda_{i}^{\\star}}{(\\lambda^{\\star})^{T}\\mathbf{1}}}\\;=\\;\\mathrm{e}^{h_{i}\\;-\\;\\bar{b}_{i}}\\;\\;\\;\\mathrm{for}\\;i=1,\\ldots,m.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We clarify that in (18), $\\lambda^{\\star}=\\left[\\lambda_{0}^{\\star},\\ldots,\\lambda_{m}^{\\star}\\right]^{T}$ with its first element being $\\lambda_{0}^{\\star}=1$ . Finally, if we return back to notation $\\lambda^{\\star}=\\left[\\lambda_{1}^{\\star},\\ldots,\\lambda_{m}^{\\star}\\right]^{T}$ , then, ", "page_idx": 19}, {"type": "equation", "text": "$$\n{\\frac{\\lambda_{i}^{\\star}}{1+(\\lambda^{\\star})^{T}{\\bf1}}}\\;=\\;\\mathrm{e}^{h_{i}\\;-\\;\\bar{b}_{i}}\\;\\;\\;\\mathrm{for}\\;i=1,\\ldots,m\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which completes the proof. ", "page_idx": 19}, {"type": "text", "text": "Remark 1. We remark on the assumption of the distributions $\\{q^{i}\\}_{i=0}^{m}$ having disjoint supports. In the setting of adapting model to new data in Section 3.2, this is a reasonable assumption. Since we oftenfinetune a pre-trained diffusion model on new data not seen in the original pre-training dataset, the new data distribution and the pre-training data distribution have mostly disjoint supports. In the minority class setting in Section 3.2, the constrained distributions $\\{q^{i}\\}_{i=1}^{m}$ and the objective distribution $q^{0}$ usually are not disjoint.However, since the distributions $\\{q^{i}\\}_{i=1}^{m}$ often are often restrictions of $q^{0}$ to subsets of the support of $q^{0}$ ,i.e., the minority classes, the derivation of optimal dual variables is similar to what we have provided in this section, so we omit the repeated details. Extending these results to cases where the distributions are neither disjoint nor restrictions of the objective distributions, is challenging and has been left to future work. ", "page_idx": 19}, {"type": "text", "text": "To prove a feasibility criterion, we first show that the dual function is finite in Lemma 5. ", "page_idx": 19}, {"type": "text", "text": "Lemma 5 (Boundedness of the optimal dual function). Let the differential entropy. $h_{i}$ ofeach distribution ${\\dot{q}}^{i}$ be finite. Then, the optimal value of the dual function $D^{\\star}:=\\operatorname*{max}_{\\lambda\\,\\geq\\,0}g(\\lambda)$ isfinite $i f$ andonly $i f$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{i\\mathop{=}1}^{m}\\mathrm{e}^{h_{i}-\\bar{b}_{i}}\\ <\\ 1.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. $(\\leftleftarrows\\right)$ From $\\begin{array}{r}{\\sum_{i=1}^{m}\\mathrm{e}^{h_{i}-\\bar{b}_{i}}<1}\\end{array}$ the otpimal dual variable $\\lambda^{\\star}$ given by (18) is fnite. Thus, the optimal value of the duai function $g(\\lambda^{\\star})$ becomes ", "page_idx": 19}, {"type": "equation", "text": "$$\ng(\\lambda^{\\star})\\;=\\;-\\,(\\lambda^{\\star})^{T}\\bar{b}\\,+\\,\\sum_{i\\,=\\,0}^{m}\\lambda_{i}^{\\star}h_{i}\\;-\\;\\sum_{i\\,=\\,0}^{m}\\lambda_{i}^{\\star}\\log\\lambda_{i}^{\\star}\\,+\\,((\\lambda^{\\star})^{T}{\\bf1})\\log((\\lambda^{\\star})^{T}{\\bf1})\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and $\\lambda_{i}^{\\star}=\\mathrm{e}^{h_{i}-\\bar{b}_{i}}>0$ , and also $\\{h_{i}\\}_{i=1}^{m}$ are all finite. Therefore, $D^{\\star}$ is finite. ", "page_idx": 19}, {"type": "text", "text": "$(\\Rightarrow)$ We prove itby conradiction,.Assume =1 ehz-bi = ed \u2265 1 for some  \u2265 0. For any \u5165 \u2265 0, there exists a direction in which increases, i.e., ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{\\partial g}{\\partial\\lambda_{i}}\\;=\\;h_{i}-\\bar{b}_{i}-\\log\\left(\\frac{\\lambda_{i}}{\\lambda^{T}{\\bf1}}\\right)>\\delta\\;\\;\\exists\\;i.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "To see (19) by contradiction, we check that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{h_{i}-\\bar{b}_{i}-\\log\\left(\\frac{\\lambda_{i}}{\\lambda^{T}\\mathbf{1}}\\right)\\ \\leq\\ \\delta\\ \\mathrm{{for}}\\ i=1,\\dots,m}\\\\ {\\implies\\mathrm{e}^{h_{i}-\\bar{b}_{i}-\\delta}\\ \\leq\\ \\frac{\\lambda_{i}}{\\lambda^{T}\\mathbf{1}}\\ \\mathrm{~for}\\ i=1,\\dots,m}\\\\ {\\implies\\left(\\displaystyle\\sum_{i=1}^{m}\\mathrm{e}^{h_{i}-\\bar{b}_{i}}\\right)\\mathrm{e}^{-\\delta}\\ \\leq\\ \\frac{\\sum_{i=1}^{m}\\lambda_{i}}{\\lambda^{T}\\mathbf{1}}\\ =\\ \\displaystyle\\frac{\\sum_{i=1}^{m}\\lambda_{i}}{1+\\sum_{i=1}^{m}\\lambda_{i}}\\ <\\ 1}\\\\ {\\implies\\displaystyle\\sum_{i=1}^{m}\\mathrm{e}^{h_{i}-\\bar{b}_{i}}\\ <\\ \\mathrm{e}^{\\delta}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "whichcontraditurassumtothat $\\begin{array}{r}{\\sum_{i=1}^{m}\\mathrm{e}^{h_{i}}-{\\bar{b}}_{i}=\\mathrm{e}^{\\delta}}\\end{array}$ By contradction, we have(19).Furthermore, (19) implies that $g(\\lambda)$ is unbounded above, which contradicts the finiteness of $D^{\\star}$ . Therefore, we must have that $\\begin{array}{r}{\\sum_{i=1}^{m}\\mathrm{e}^{h_{i}-\\bar{b}_{i}}<1}\\end{array}$ \u53e3 ", "page_idx": 19}, {"type": "text", "text": "Lemma 6 (Feasibility criterion). Let the differential entropy $h_{i}$ ofeachdistribution $q^{i}$ befinite. Suppose that there exists a feasible solution to Problem (U-ELBO) such that its objective function is bounded from below. Then, Problem (U-ELBO) is feasible if and only $i f$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{i\\mathop{=}1}^{m}\\mathrm{e}^{h_{i}-\\bar{b}_{i}}\\ <\\ 1.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. $(\\Rightarrow)$ Since the primal problem is feasible, the optimal objective function $F^{\\star}$ is bounded from below andit isattained at afeasibl pointForthe ake of contradictonweassme $\\sum_{i=1}^{m}\\mathrm{e}^{h_{i}-\\bar{b}_{i}}\\geq$ 1, which is equivalent to $D^{\\star}=\\infty$ according to Lemma 5. However, this violates weak duality, i.e., $D^{\\star}\\leq F^{\\star}$ .By contradiction, we must have $\\overline{{\\sum_{i=1}^{m}\\mathrm{e}^{h_{i}-\\bar{b}_{i}}}}<1$ ", "page_idx": 20}, {"type": "text", "text": "$(\\leftarrow)$ We consider a set $\\boldsymbol{\\mathcal{A}}$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{4\\;:=\\;\\left\\{(u_{1},\\ldots,u_{m},t)\\,|\\,-{\\cal E}(p,q^{i})-\\bar{b}_{i}\\le u_{i}\\mathrm{~for~}i=1,\\ldots,m\\;\\mathrm{~and~}-{\\cal E}(p,q^{0})\\le t\\mathrm{~for~}p\\in\\mathcal{P}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The set $\\boldsymbol{\\mathcal{A}}$ is convex since it is the intersection of $m+1$ epigraphs of convex functions. We also introduce another convex set $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{B}\\;:=\\;\\left\\{(0,\\ldots,0,t)\\;\\middle|\\;t\\in\\mathbb{R}\\right\\}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We utilise proof by contradiction. Assume that the primal problem is infeasible. Then there doesn't exist any $p\\in\\mathcal{P}$ such that $-E(p,q^{i})-\\bar{b}_{i}\\leq0$ for all $i=1,\\hdots,m$ . Hence, $\\boldsymbol{\\mathcal{A}}$ and $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ are twodisjoint convex sets. From the separating hyperplane theorem, there exists a hyperplane that separates them, i.e., $\\exists v\\in\\mathbb{R}^{m+1}$ and $c\\in\\mathbb{R}$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{x^{T}v\\geq c\\mathrm{~for~all~}x\\in A}\\\\ {y^{T}v\\leq c\\mathrm{~for~all~}y\\in B.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Let $v=\\left[\\,\\lambda_{1},\\ldots\\lambda_{m},\\gamma\\,\\right]^{T}$ . Then, (21) reduces to ", "page_idx": 20}, {"type": "equation", "text": "$$\ny^{T}v\\ =\\ \\lambda^{T}u\\,+\\,\\gamma t\\ =\\ \\gamma t\\ \\leq\\ c\\ \\mathrm{for}\\,\\mathrm{all}\\left(u,t\\right)\\in{\\mathcal{B}}\\ \\Rightarrow\\ \\gamma\\ =\\ 0.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "This is because $\\gamma t\\leq c$ for any $t\\in\\mathbb R$ . Note that $\\gamma=0$ means the separating hyperplane is vertical, i.e., being parallel to the $t$ -axis. Furthermore, from (20) we can write ", "page_idx": 20}, {"type": "equation", "text": "$$\nx^{T}v\\ =\\ \\lambda^{T}u\\,+\\,\\gamma t\\ =\\ \\lambda^{T}u\\ \\geq\\ c\\ \\mathrm{for}\\,\\mathrm{all}\\left(u,t\\right)\\in A\\ \\Rightarrow\\ \\lambda\\ \\geq\\ 0.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The above is true because the set of values that each $u_{i}$ can take in $\\boldsymbol{\\mathcal{A}}$ is unbounded above. Thus, since $\\lambda^{T}u\\geq c$ ,necessarily every $\\lambda_{i}$ has to be non-negative. Now, we consider ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{g(\\lambda)}&{=}&{\\displaystyle\\operatorname*{inf}_{(u,l)\\in A}\\ t+\\lambda^{T}u}\\\\ {\\implies}&{g(\\alpha\\lambda)}&{=}&{\\displaystyle\\operatorname*{inf}_{(u,l)\\in A}t+\\alpha\\lambda^{T}u\\ \\mathrm{for}\\ \\alpha\\in\\mathbb{R}_{+}}\\\\ {\\implies}&{\\displaystyle\\operatorname*{lim}_{\\alpha\\to\\infty}g(\\alpha\\lambda)}&{=}&{\\displaystyle\\operatorname*{lim}_{\\alpha\\to\\infty}\\operatorname*{inf}_{(u,l)\\in A}t+\\alpha\\lambda^{T}u}\\\\ &&{=}&{\\displaystyle\\operatorname*{lim}_{\\alpha\\to\\infty}\\alpha\\left(\\operatorname*{inf}_{(u,l)\\in A}\\lambda^{T}u\\right)}\\\\ &&{\\geq}&{\\displaystyle\\operatorname*{lim}_{\\alpha\\to\\infty}\\alpha c}\\\\ &{=}&{\\infty}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which shows that $D^{*}=\\infty$ . This contradicts $D^{\\star}$ being finite $D^{\\star}$ is finite due to $\\begin{array}{r}{\\sum_{i=1}^{m}\\mathrm{e}^{h_{i}-\\bar{b}_{i}}<1}\\end{array}$ and Lemma 5). Because of the contradiction, the primal problem has to be feasible. \u53e3 ", "page_idx": 20}, {"type": "text", "text": "B.5 Proof of Lemma 3 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Proof. The proof is an application of the convergence theory of DDPM [47, Theorem 3]. We next check allasumptions of [47, Theorem 3] Itis asy to e that we can cast $q_{\\mathrm{mix}}^{(\\lambda)}$ as a target distribution ", "page_idx": 20}, {"type": "text", "text": "of a diffusion model. By the definition, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\bar{\\mathcal{L}}(\\theta,\\lambda)}&{=}&{D_{\\mathrm{KL}}\\left(q(x_{0:T})\\,\\|\\,p_{\\theta}(x_{0:T})\\right)\\,+\\,\\displaystyle\\sum_{i=1}^{m}\\lambda_{i}\\left(D_{\\mathrm{KL}}\\left(q^{i}(x_{0:T})\\,\\|\\,p_{\\theta}(x_{0:T})\\right)-b_{i}\\right)}\\\\ &{=}&{-E(p_{\\theta};q)\\,+\\,\\mathbb{E}_{q(x_{0})}\\bigl[\\log q(x_{0})\\bigr]\\,+\\,\\displaystyle\\sum_{i=1}^{m}\\lambda_{i}\\left(-E(p_{\\theta};q^{i})-\\bar{b}_{i}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Thus, the partial minimization of $\\bar{\\mathcal{L}}(\\theta,\\lambda)$ over $\\theta$ is equivalent to a weighted EBLO minimization, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\displaystyle\\operatorname*{minimize}_{\\theta\\,\\in\\,\\Theta}\\,\\,-\\,E(p_{\\theta};q)\\,-\\,\\sum_{i\\,=\\,1}^{m}\\lambda_{i}\\,E(p_{\\theta};q^{i})\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "or, equivalently, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{minimize}_{\\theta\\in\\Theta}~-E(p_{\\theta};q_{\\mathrm{mix}}^{(\\lambda)})\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "wherewelzwegtvbuixddtadis $q_{\\mathrm{mix}}^{(\\lambda)}$ ().We note that $\\bar{p}^{\\star}(\\lambda)$ is also a minimizer of Problem (22). ", "page_idx": 21}, {"type": "text", "text": "On the other hand, using Problem (P-LOSS), we can rewrite Problem (22) as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{minimize}_{\\theta\\in\\Theta}~\\mathbb{E}_{q_{\\mathrm{mix}}^{(\\lambda)}(x_{0}),\\,t,\\,x_{t}}\\left[\\|\\widehat{s}_{\\theta}(x_{t},t)-\\nabla\\log q(x_{t})\\|^{2}\\right]\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which is equivalent to the score matching objective in DDPM. Therefore, the score matching assumption in [47, Assumption 1] is satisfied with the error bound $\\varepsilon_{\\mathrm{score}}^{2}$ from Assumption 3. Viewing Assumption 2, and using appropriate stepsize and variance, all assumptions in [47, Theorem 3] are satisfied. Therefore, application of [47, Theorem 3] completes the proof. \u53e3 ", "page_idx": 21}, {"type": "text", "text": "B.6Proof of Lemma 4 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Lemma7. The TV distance beween two mixture data distributionsamx m g(ixs, g(oix) is bounded by ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{TV}\\left(q_{\\mathrm{mix}}^{(\\lambda)},q_{\\mathrm{mix}}^{(\\lambda^{\\star})}\\right)\\ \\leq\\ \\|\\lambda-\\lambda^{\\star}\\|_{1}\\,.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. By the definition, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\mathrm{TV}\\left(q_{\\mathrm{mix}}^{(3)},q_{\\mathrm{mix}}^{(3)}\\right)}&{=}&{\\displaystyle\\frac{1}{2}\\int_{x_{0}}\\left|\\frac{q+\\sum_{i=1}^{m}\\lambda^{i}q^{i}}{1+\\lambda^{i}}-\\frac{q+\\sum_{i=1}^{m}\\lambda^{i}\\cdot q^{i}}{1+(\\lambda^{i})^{\\top}1}\\right|}\\\\ &{=}&{\\displaystyle\\frac{1}{2}\\int_{x_{0}}\\left|\\frac{(1+(\\lambda^{\\star})^{\\top}1)(q+\\sum_{i=1}^{m}\\lambda^{i}q^{i})-(1+\\lambda^{\\top}1)(q+\\sum_{i=1}^{m}\\lambda^{i}\\cdot q^{i})}{(1+\\lambda^{\\top}1)(1+(\\lambda^{\\star})^{\\top}1)}\\right|}\\\\ &{=}&{\\displaystyle\\frac{1}{2}\\int_{x_{0}}\\left|\\frac{\\sum_{i=1}^{m}\\lambda^{i}q^{i}+(\\lambda^{\\star})^{\\top}1q-\\sum_{i=1}^{m}\\lambda^{i,\\star}q^{i}-\\lambda^{\\top}1q}{(1+\\lambda^{\\top}1)(1+(\\lambda^{\\star})^{\\top}1)}\\right|}\\\\ &{\\le}&{\\displaystyle\\frac{1}{2}\\int_{x_{0}}\\left|\\sum_{i=1}^{m}(\\lambda^{i}-\\lambda^{i,\\star})q^{i}+(\\lambda^{\\star}-\\lambda)^{\\top}1q\\right|}\\\\ &{\\le}&{\\displaystyle\\frac{m}{\\lambda^{i}}|\\lambda^{i}-\\lambda^{i,\\star}|}\\\\ &{=}&{\\displaystyle\\|\\lambda-\\lambda^{\\star}\\|_{1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the first inequality is due to $(1+\\lambda^{\\top}1)(1+(\\lambda^{\\star})^{\\top}1)\\geq1$ and we use triangle inequality in the second inequality. \u53e3 ", "page_idx": 21}, {"type": "text", "text": "We recall the Lagrangians for Problems (U-LOSS) and (P-LOSS), ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathcal{L}_{s}(\\widehat{s},\\lambda)}&{=}&{\\mathbb{E}_{q(x_{0}),\\,t,\\,x_{t}}\\left[\\left\\|\\widehat{s}(x_{t},t)-\\nabla\\log q(x_{t})\\right\\|^{2}\\right]}\\\\ &&{\\displaystyle+\\sum_{i\\,=\\,1}^{m}\\lambda_{i}\\left(\\mathbb{E}_{q^{i}(x_{0}),\\,t,\\,x_{t}}\\left[\\left\\|\\widehat{s}(x_{t},t)-\\nabla\\log q(x_{t})\\right\\|^{2}\\right]-\\widetilde{b}^{i}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\bar{\\mathcal{L}}_{s}(\\widehat{s}_{\\theta},\\lambda)\\;=\\;\\mathcal{L}_{s}(\\widehat{s}_{\\theta},\\lambda)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and their associated dual functions, ", "page_idx": 22}, {"type": "equation", "text": "$$\ng_{s}(\\lambda)\\;=\\;\\underset{\\widehat{s}\\,\\in\\,S}{\\mathrm{minimize}}\\;\\,\\mathcal{L}_{s}(\\widehat{s},\\lambda)\\;\\mathrm{~and~}\\;\\bar{g}_{s}(\\lambda)\\;=\\;\\underset{\\theta\\,\\in\\,\\Theta}{\\mathrm{minimize}}\\;\\,\\bar{\\mathcal{L}}_{s}(\\widehat{s}_{\\theta},\\lambda).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "For brevity, we use shorthand $\\mathbb{E}_{q}$ and $\\mathbb{E}_{q^{i}}$ for $\\mathbb{E}_{q(x_{0}),\\,t,\\,x_{t}}$ and $\\mathbb{E}_{q^{i}(x_{0}),\\,t,\\,x_{t}}$ , respectively. ", "page_idx": 22}, {"type": "text", "text": "Lemma 8 (Parametrization gap). Let Assumption $^{4}$ hold. Then, $0\\leq\\bar{g}_{s}(\\lambda)-g_{s}(\\lambda)\\leq4R(1\\!+\\!\\|\\lambda\\|_{1})\\nu$ for any $\\lambda\\geq0$ ", "page_idx": 22}, {"type": "text", "text": "Proof. Let the partial minimizer of $\\mathcal{L}_{s}(\\widehat{s},\\lambda)$ over $\\widehat{s}$ be $\\widehat{s}^{\\star}(\\lambda):=\\mathrm{argmin}_{\\widehat{s}}\\,\\mathcal{L}_{s}(\\widehat{s},\\lambda)$ for any $\\lambda\\geq0$ For any $\\lambda\\geq0$ , there exists $\\widetilde{\\theta}\\,\\in\\,\\Theta$ such that $\\left\\|\\widehat{s}^{\\star}(\\lambda)-\\widehat{s}_{\\widetilde{\\theta}}\\right\\|_{L_{2}}\\,\\leq\\,\\nu$ for any $\\lambda\\geq0$ according to Assumption 4. Thus, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathcal{\\bar{L}}_{s}(\\hat{s}_{\\bar{\\theta}},\\lambda)-\\mathcal{L}_{s}(\\hat{s}^{\\star}(\\lambda),\\lambda)}&{=}&{\\displaystyle\\mathbb{E}_{q}\\left[\\left\\|\\hat{s}_{\\bar{\\theta}}(x_{t},t)-x_{0}\\right\\|^{2}\\right]~-~\\mathbb{E}_{q}\\left[\\left\\|\\hat{s}^{\\star}(\\lambda)(x_{t},t)-x_{0}\\right\\|^{2}\\right]}\\\\ &&{~}&{\\displaystyle+\\sum_{i=1}^{m}\\lambda_{i}\\left(\\mathbb{E}_{q^{i}}\\left[\\left\\|\\hat{s}_{\\bar{\\theta}}(x_{t},t)-x_{0}\\right\\|^{2}\\right]-\\mathbb{E}_{q^{i}}\\left[\\left\\|\\hat{s}^{\\star}(\\lambda)(x_{t},t)-x_{0}\\right\\|^{2}\\right]\\right.}\\\\ &{\\leq}&{\\displaystyle4R\\mathbb{E}_{q}\\left[\\left\\|\\hat{s}_{\\bar{\\theta}}(x_{t},t)-\\hat{s}^{\\star}(\\lambda)(x_{t},t)\\right\\|\\right]}\\\\ &&{~}&{\\displaystyle\\left.+4R\\sum_{i=1}^{m}\\lambda_{i}\\mathbb{E}_{q^{i}}~[\\left\\|\\hat{s}_{\\bar{\\theta}}(x_{t},t)-\\hat{s}^{\\star}(\\lambda)(x_{t},t)\\right\\|]\\right]}\\\\ &{\\leq}&{\\displaystyle4R\\nu+4R\\left\\|\\lambda\\right\\|_{1}\\nu}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the first inequality is due to that the quadratic function is locally Lipschitz continuous withparameter $4R$ , and the second inequality is because that there exists \u03b8 E \u2299 such that $\\left\\|\\widehat{x}^{\\star}(\\widehat{\\lambda})-\\widehat{x}_{\\widetilde{\\theta}}\\right\\|_{L_{2}}\\leq\\nu$ for any $\\lambda\\geq0$ accordingto Assumption 4. ", "page_idx": 22}, {"type": "text", "text": "By the definition $\\widehat{s}_{\\theta}^{\\star}(\\lambda)\\in\\operatorname{argmin}_{\\theta\\in\\Theta}\\bar{\\mathcal L}_{s}(\\widehat{s}_{\\theta},\\lambda),$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\bar{\\mathcal{L}}_{s}(\\widehat{s}_{\\theta}^{\\star}(\\lambda),\\lambda)\\;\\leq\\;\\bar{\\mathcal{L}}_{s}(\\widehat{s}_{\\widetilde{\\theta}},\\lambda).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Therefore, ", "page_idx": 22}, {"type": "equation", "text": "$$\n0\\ \\leq\\ \\mathcal{L}_{s}(\\widehat{s}_{\\theta}^{\\star}(\\lambda),\\lambda)-\\mathcal{L}_{s}(\\widehat{s}^{\\star}(\\lambda),\\lambda)\\ \\leq\\ \\bar{\\mathcal{L}}_{s}(\\widehat{s}_{\\tilde{\\theta}},\\lambda)-\\mathcal{L}_{s}(\\widehat{s}^{\\star}(\\lambda),\\lambda)\\ \\leq\\ 4R(1+\\|\\lambda\\|_{1})\\nu\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which gives our desired result by the definition of dual functions. ", "page_idx": 22}, {"type": "text", "text": "Lemma 9 (Differentiability). The dual function $g_{s}(\\lambda)$ is differentiable with gradient $\\nabla_{\\lambda}\\mathcal{L}_{s}(\\widehat{s}^{\\star}(\\lambda),\\lambda)$ ", "page_idx": 22}, {"type": "text", "text": "Proof. For any $\\lambda\\geq0$ , the Lagrangian $\\mathcal{L}_{s}(\\widehat{s},\\lambda)$ is strongly convex in function ${\\widehat{s}}\\in S$ . Since $\\boldsymbol{S}$ is convex and compact, any partial minimizer $\\widehat{s}^{\\star}(\\lambda)$ is unique. By Danskin's theorem [4], $g_{s}(\\lambda)$ is differentiable and its gradient is the gradient of $\\mathcal{L}_{s}(\\widehat{s},\\lambda)$ over $\\lambda$ at ${\\widehat{s}}={\\widehat{s}}^{\\star}(\\lambda)$ \u53e3 ", "page_idx": 22}, {"type": "text", "text": "Lemma 10 (Convexity). The dual function $g_{s}(\\lambda)$ is $\\mu$ -strongly concave in $\\lambda\\in{\\mathcal{H}}$ where ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mu\\;=\\;\\left(\\frac{\\sigma}{1+\\operatorname*{max}\\left(\\left\\Vert\\lambda^{\\star}\\right\\Vert_{1},\\left\\Vert\\bar{\\lambda}^{\\star}\\right\\Vert_{1}\\right)}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. For any $\\lambda_{1},~\\lambda_{2}~~\\in~{\\mathcal{H}}$ ,we denote $\\begin{array}{r l r}{\\widehat{s}_{1}^{\\star}}&{{}:=}&{\\widehat{s}^{\\star}(\\lambda_{1})}\\end{array}$ and $\\begin{array}{r l r}{\\widehat{s}_{2}^{\\star}}&{{}:=}&{\\widehat{s}^{\\star}(\\lambda_{2})}\\end{array}$ ,which are unique partial minimizers of the Lagrangians $\\mathcal{L}_{s}(\\widehat{s},\\lambda_{1})$ and $\\mathcal{L}_{s}(\\widehat{s},\\lambda_{2})$ .Denote $\\begin{array}{r l}{\\ell_{0}(\\widehat{s})}&{{}:=}\\end{array}$ $\\mathbb{E}_{q}\\left[\\mathbf{\\,}\\|\\widehat{s}(x_{t},t)-\\nabla\\log q(x_{t})\\|^{2}\\;\\right]$ \uff0c $\\ell_{i}(\\widehat{s}):=\\mathbb{E}_{q^{i}}\\left[\\;\\|\\widehat{s}(x_{t},t)-\\nabla\\log q(x_{t})\\|^{2}\\;\\right]$ for $i=1,\\hdots,m$ , and $\\ell(\\widehat{s}):=[\\,\\ell_{1}(\\widehat{s}),\\ldots,\\ell_{m}(\\widehat{s})\\,]^{\\intercal}$ . By the convexity of $\\ell_{i}$ \uff0c ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ell_{i}(\\widehat{s}_{1}^{\\star})\\;\\geq\\;\\ell_{i}(\\widehat{s}_{2}^{\\star})\\,+\\,2\\,\\langle\\nabla_{\\widehat{s}}\\ell_{i}(\\widehat{s}_{2}^{\\star}),\\widehat{s}_{1}^{\\star}-\\widehat{s}_{2}^{\\star}\\rangle}\\\\ &{\\ell_{i}(\\widehat{s}_{2}^{\\star})\\;\\geq\\;\\ell_{i}(\\widehat{s}_{1}^{\\star})\\,+\\,2\\,\\langle\\nabla_{\\widehat{s}}\\ell_{i}(\\widehat{s}_{1}^{\\star}),\\widehat{s}_{2}^{\\star}-\\widehat{s}_{1}^{\\star}\\rangle\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "If we multiply the first inequality above by $\\lambda_{2,i}\\geq0$ and the second inequality above by $\\lambda_{1,i}\\geq0$ and add them up from both sides, then, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{-\\left\\langle\\ell(\\widehat{s}_{2})-\\ell(\\widehat{s}_{1}),\\lambda_{2}-\\lambda_{1}\\right\\rangle\\;\\geq\\;2\\left\\langle\\lambda_{1}^{\\top}\\nabla_{\\widehat{s}}\\ell(\\widehat{s}_{1}^{\\star})-\\lambda_{2}^{\\top}\\nabla_{\\widehat{s}}\\ell(\\widehat{s}_{2}^{\\star}),\\widehat{s}_{2}^{\\star}-\\widehat{s}_{1}^{\\star}\\right\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We apply Lemma 9 to the LHS of the inequality above, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{-\\,\\left(\\nabla g_{s}(\\lambda_{2})-\\nabla g_{s}(\\lambda_{1})\\right)^{\\top}(\\lambda_{2}-\\lambda_{1})\\ \\geq\\ 2\\left\\langle\\lambda_{1}^{\\top}\\nabla_{\\hat{s}}\\ell(\\hat{s}_{1}^{\\star})-\\lambda_{2}^{\\top}\\nabla_{\\hat{s}}\\ell(\\hat{s}_{2}^{\\star}),\\hat{s}_{2}^{\\star}-\\hat{s}_{1}^{\\star}\\right\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "On the other hand, by the optimalityof $\\widehat{s}_{1}^{\\star}$ and $\\widehat{s}_{2}^{\\star}$ \uff0c ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\widehat{s}}\\ell_{0}(\\widehat{s}_{1}^{\\star})\\,+\\,\\lambda_{1}^{\\top}\\nabla_{\\widehat{s}}\\ell(\\widehat{s}_{1}^{\\star})\\;=\\;0}\\\\ &{}\\\\ &{\\nabla_{\\widehat{s}}\\ell_{0}(\\widehat{s}_{2}^{\\star})\\,+\\,\\lambda_{2}^{\\top}\\nabla_{\\widehat{s}}\\ell(\\widehat{s}_{2}^{\\star})\\;=\\;0}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "which allows us to simplify the right-hand side of (23) and obtain ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{-\\mathbf{\\beta}\\left(\\nabla g_{s}(\\lambda_{2})-\\nabla g_{s}(\\lambda_{1})\\right)^{\\top}(\\lambda_{2}-\\lambda_{1})}&{\\ge}&{2\\left\\langle\\nabla_{\\hat{s}}\\ell_{0}(\\hat{s}_{2}^{\\star})-\\nabla_{\\hat{s}}\\ell_{0}(\\hat{s}_{1}^{\\star}),\\hat{s}_{2}^{\\star}-\\hat{s}_{1}^{\\star}\\right\\rangle}\\\\ &{\\ge}&{2\\left\\lVert\\hat{s}_{1}^{\\star}-\\widehat{s}_{2}^{\\star}\\right\\rVert_{L_{2}}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the last inequality results from the strong convexity of quadratic functionals. ", "page_idx": 23}, {"type": "text", "text": "By the smoothness of quadratic functionals with parameter 1, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\|\\widehat{s}_{1}^{\\star}-\\widehat{s}_{2}^{\\star}\\|_{L_{2}}}&{\\geq}&{\\|\\nabla_{\\widehat{s}}\\ell_{0}(\\widehat{s}_{1}^{\\star})-\\nabla_{\\widehat{s}}\\ell_{0}(\\widehat{s}_{2}^{\\star})\\|_{L_{2}}}\\\\ &{=}&{\\|\\lambda_{1}^{\\top}\\nabla_{\\widehat{s}}\\ell(\\widehat{s}_{1}^{\\star})-\\lambda_{2}^{\\top}\\nabla_{\\widehat{s}}\\ell(\\widehat{s}_{2}^{\\star})\\|_{L_{2}}}\\\\ &{=}&{\\big\\|(\\lambda_{2}-\\lambda_{1})^{\\top}\\nabla_{\\widehat{s}}\\ell(\\widehat{s}_{2}^{\\star})-\\lambda_{1}^{\\top}(\\nabla_{\\widehat{s}}\\ell(\\widehat{s}_{1}^{\\star})-\\nabla_{\\widehat{s}}\\ell(\\widehat{s}_{2}^{\\star}))\\big\\|_{L_{2}}}\\\\ &{\\geq}&{\\big\\|(\\lambda_{2}-\\lambda_{1})^{\\top}\\nabla_{\\widehat{s}}\\ell(\\widehat{s}_{2}^{\\star})\\big\\|_{L_{2}}-\\big\\|\\lambda_{1}^{\\top}(\\nabla_{\\widehat{s}}\\ell(\\widehat{s}_{1}^{\\star})\\,-\\,\\nabla_{\\widehat{s}}\\ell(\\widehat{s}_{2}^{\\star}))\\big\\|_{L_{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the equality is due to the optimality condition (24) and the last inequality is due to triangle inequality. By Assumption 5, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left\\|(\\lambda_{2}-\\lambda_{1})^{\\top}\\nabla_{\\widehat{s}}\\ell(\\widehat{s}_{2}^{\\star})\\right\\|_{L_{2}}\\;\\geq\\;\\sigma\\left\\|\\lambda_{2}-\\lambda_{1}\\right\\|.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We also notice that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\left\\|\\lambda_{1}^{\\top}(\\nabla_{\\hat{s}}\\ell(\\widehat{s}_{1}^{\\star})-\\nabla_{\\hat{s}}\\ell(\\widehat{s}_{2}^{\\star}))\\right\\|_{L_{2}}}&{\\displaystyle\\leq}&{\\displaystyle\\sum_{i\\mathop{=}1}^{m}\\lambda_{1,i}\\,\\|\\nabla_{\\hat{s}}\\ell_{i}(\\widehat{s}_{1}^{\\star})-\\nabla_{\\hat{s}}\\ell_{i}(\\widehat{s}_{2}^{\\star})\\|_{L_{2}}}\\\\ &{\\displaystyle\\leq}&{\\displaystyle\\sum_{i\\mathop{=}1}^{m}\\lambda_{1,i}\\,\\|\\widehat{s}_{1}^{\\star}-\\widehat{s}_{2}^{\\star}\\|_{L_{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the first inequality is due to triangle inequality and the second inequality is due to the smoothness of quadratic functionals. Hence, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\|\\widehat{s}_{1}^{\\star}-\\widehat{s}_{2}^{\\star}\\|_{L_{2}}\\;\\geq\\;\\sigma\\,\\|\\lambda_{2}-\\lambda_{1}\\|\\;-\\;\\|\\lambda_{1}\\|_{1}\\,\\|\\widehat{s}_{1}^{\\star}-\\widehat{s}_{2}^{\\star}\\|_{L_{2}}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "or, equivalently, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\|\\widehat{s_{1}}-\\widehat{s_{2}}\\|_{L_{2}}\\;\\geq\\;\\frac{\\sigma}{1+\\|\\lambda_{1}\\|_{1}}\\,\\|\\lambda_{2}-\\lambda_{1}\\|\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Therefore, (25) becomes ", "page_idx": 23}, {"type": "equation", "text": "$$\n-\\left(\\nabla g_{s}(\\lambda_{2})-\\nabla g_{s}(\\lambda_{1})\\right)^{\\top}(\\lambda_{2}-\\lambda_{1})\\ \\geq\\ \\left({\\frac{\\sigma}{1+\\left\\|\\lambda_{1}\\right\\|_{1}}}\\right)^{2}\\left\\|\\lambda_{2}-\\lambda_{1}\\right\\|^{2}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "which completes the proof by choosing the smallest modulus over $\\lambda_{1}\\in{\\mathcal{H}}$ ", "page_idx": 23}, {"type": "text", "text": "Proof. By Lemmas 9 and 10, for any $\\lambda\\in{\\mathcal{H}}$ \uff0c ", "page_idx": 23}, {"type": "equation", "text": "$$\ng_{s}(\\lambda)\\ \\leq\\ g_{s}(\\lambda^{\\star})\\,+\\,\\nabla g_{s}(\\lambda^{\\star})^{\\top}(\\lambda-\\lambda^{\\star})\\,-\\,\\frac{\\mu}{2}\\,\\|\\lambda-\\lambda^{\\star}\\|^{2}\\,.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Thus, if we choose $\\lambda={\\bar{\\lambda}}^{\\star}$ , then ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\iota_{s}(\\bar{\\lambda}^{\\star})\\,\\le\\,g_{s}(\\lambda^{\\star})+\\sum_{i=1}^{m}(\\bar{\\lambda}_{i}^{\\star}-\\lambda_{i}^{\\star})\\left(\\mathbb{E}_{q^{i}}\\left[\\left\\|\\widehat{s}^{\\star}(\\lambda^{\\star})(x_{t},t)-\\nabla\\log q(x_{t})\\|^{2}\\right\\|-\\widetilde{b}^{i}\\right)-\\frac{\\mu}{2}\\left\\|\\bar{\\lambda}^{\\star}-\\lambda^{\\star}\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Optimality of $(\\widehat{s}^{\\star}(\\lambda^{\\star}),\\lambda^{\\star})$ leads to the complementary slackness, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{i\\,=\\,1}^{m}\\lambda_{i}^{\\star}\\left(\\mathbb{E}_{q^{i}}\\left[\\,\\|\\widehat{s}^{\\star}(\\lambda^{\\star})(x_{t},t)-\\nabla\\log q(x_{t})\\|^{2}\\,\\right]-\\widetilde{b}^{i}\\right)\\;=\\;0\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and the feasibility, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{q^{i}}\\left[\\|\\widehat{s}^{\\star}(\\lambda^{\\star})(x_{t},t)-\\nabla\\log q(x_{t})\\|^{2}\\right]\\;\\leq\\;\\widetilde{b}^{i}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Therefore, ", "page_idx": 24}, {"type": "equation", "text": "$$\ng_{s}(\\bar{\\lambda}^{\\star})\\;\\leq\\;g_{s}(\\lambda^{\\star})\\,-\\,\\frac{\\mu}{2}\\left\\|\\bar{\\lambda}^{\\star}-\\lambda^{\\star}\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "According to Lemma 8, $\\bar{g}_{s}(\\bar{\\lambda}^{\\star})-4R\\left(1+\\left\\lVert\\bar{\\lambda}^{\\star}\\right\\rVert_{1}\\right)\\nu\\leq g_{s}(\\bar{\\lambda}^{\\star})$ Hence, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\bar{g}_{s}(\\bar{\\lambda}^{\\star})\\,-\\,4R\\,\\bigl(1+\\bigl\\|\\bar{\\lambda}^{\\star}\\bigr\\|_{1}\\bigr)\\,\\nu\\,\\le\\,g_{s}(\\lambda^{\\star})\\,-\\,\\frac{\\mu}{2}\\,\\bigl\\|\\bar{\\lambda}^{\\star}-\\lambda^{\\star}\\bigr\\|^{2}\\,.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Thus, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l l l}{\\left\\|\\bar{\\lambda}^{\\star}-\\lambda^{\\star}\\right\\|^{2}}&{\\leq}&{\\displaystyle\\frac{2}{\\mu}\\left(g_{s}(\\lambda^{\\star})-\\bar{g}_{s}(\\bar{\\lambda}^{\\star})\\right)\\,+\\,\\frac{8}{\\mu}R\\left(1+\\left\\|\\bar{\\lambda}^{\\star}\\right\\|_{1}\\right)\\nu}\\\\ &{\\leq}&{\\displaystyle\\frac{8}{\\mu}R\\left(1+\\left\\|\\bar{\\lambda}^{\\star}\\right\\|_{1}\\right)\\nu}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the last inequality is due to that $g_{s}(\\lambda)\\le\\bar{g}_{s}(\\lambda)$ for any $\\lambda\\geq0$ , and the optimality of ${\\bar{\\lambda}}^{\\star}$ \uff0c ", "page_idx": 24}, {"type": "equation", "text": "$$\ng_{s}(\\lambda^{\\star})\\;\\leq\\;\\bar{g}_{s}(\\lambda^{\\star})\\;\\leq\\;\\bar{g}_{s}(\\bar{\\lambda}^{\\star}).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "B.7Proof of Theorem 3 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Proof. By the triangle inequality for TV distance, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathrm{TV}\\left(q_{\\mathrm{mix}}^{\\star},\\,\\bar{p}^{\\star}(\\bar{\\lambda}^{\\star})\\right)}&{\\leq}&{\\mathrm{TV}\\left(q_{\\mathrm{mix}}^{\\star},\\,q_{\\mathrm{mix}}^{(\\bar{\\lambda}^{\\star})}\\right)\\,+\\,\\mathrm{TV}\\left(q_{\\mathrm{mix}}^{(\\bar{\\lambda}^{\\star})},\\,\\bar{p}^{\\star}(\\bar{\\lambda}^{\\star})\\right)}\\\\ &{\\leq}&{\\left\\|\\lambda^{\\star}-\\bar{\\lambda}^{\\star}\\right\\|_{1}\\,+\\,\\frac{d^{2}\\log^{3}T}{\\sqrt{T}}\\,+\\,\\sqrt{d}\\left(\\log^{2}T\\right)\\varepsilon_{\\mathrm{score}}}\\\\ &{\\leq}&{\\sqrt{\\frac{8}{\\mu}m R\\left(1+\\left\\|\\bar{\\lambda}^{\\star}\\right\\|_{1}\\right)\\nu}\\,+\\,\\frac{d^{2}\\log^{3}T}{\\sqrt{T}}\\,+\\,\\sqrt{d}\\left(\\log^{2}T\\right)\\varepsilon_{\\mathrm{score}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the second inequality is due to Lemma 7 and Lemma 3, and the last inequality is due to $\\left\\|\\lambda\\right\\|_{1}\\leq{\\sqrt{m}}\\left\\|\\lambda\\right\\|$ and Lemma 4. \u53e3 ", "page_idx": 24}, {"type": "text", "text": "B.8Proof of Theorem 4 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Lemma 11. For a stochastic variant of Algorithm $^{\\,l}$ in Section 4.3, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\left\\|\\lambda(h+1)-\\bar{\\lambda}^{\\star}\\right\\|^{2}\\;\\middle|\\;\\lambda(h)\\right]\\;\\leq\\;\\left\\|\\lambda(h)-\\bar{\\lambda}^{\\star}\\right\\|^{2}\\;+\\;\\eta^{2}S^{2}\\;-\\;2\\eta\\left(\\bar{D}^{\\star}-\\bar{g}(\\lambda(h))-\\varepsilon_{\\mathrm{approx}}^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. For brevity, we let the stochastic gradient be $\\widehat{f}(h)=[\\widehat{f}_{1}(h),\\ldots,\\widehat{f}_{m}(h)]$ with ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\widehat{f_{i}}(h)\\;:=\\;\\widehat{\\mathbb{E}}_{x_{0}\\sim q^{i},\\,t,\\,x_{t}}\\left[\\,\\|\\widehat{s}_{\\theta}(h)(x_{t},t)-\\nabla q(x_{t})\\|^{2}\\,\\right]\\,-\\,\\widetilde{b}^{i}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By the definition of $\\lambda(h+1)$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l c l}{\\left\\|\\lambda(h+1)-\\bar{\\lambda}^{\\star}\\right\\|^{2}}&{=}&{\\left\\|\\left[\\lambda(h)+\\eta\\widehat{f}(h)\\right]_{+}-\\bar{\\lambda}^{\\star}\\right\\|^{2}}\\\\ &{\\leq}&{\\left\\|\\lambda(h)-\\bar{\\lambda}^{\\star}+\\eta\\widehat{f}(h)\\right\\|^{2}}\\\\ &{=}&{\\left\\|\\lambda(h)-\\bar{\\lambda}^{\\star}\\right\\|^{2}\\;+\\;\\eta^{2}\\left\\|\\widehat{f}(h)\\right\\|^{2}\\;+\\;2\\eta\\widehat{f}(h)^{\\top}\\left(\\lambda(h)-\\bar{\\lambda}^{\\star}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the inequality is due to the non-expansiveness of projection. Application of the conditional expectation over both sides of the inequality above yields, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathbb{E}\\left[\\left\\|\\lambda(h+1)-\\bar{\\lambda}^{\\star}\\right\\|^{2}\\;|\\;\\lambda(h)\\right]}&{\\leq}&{\\left\\|\\lambda(h)-\\bar{\\lambda}^{\\star}\\right\\|^{2}\\,+\\,\\eta^{2}\\,\\mathbb{E}\\left[\\left\\|\\widehat{f}(h)\\right\\|^{2}\\;|\\;\\lambda(h)\\right]}\\\\ &{}&{+\\,2\\eta\\,\\mathbb{E}\\left[\\widehat{f}(h)\\,|\\,\\lambda(h)\\right]^{\\top}\\left(\\lambda(h)-\\bar{\\lambda}^{\\star}\\right)~~~~~}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "which gives our desired result when we use the fact that $\\mathbb{E}[{\\widehat{f}}(h)\\mid\\lambda(h)]$ is an approximate descent direction of the dual function $\\bar{g}$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\widehat{f}(h)\\,|\\,\\lambda(h)\\right]^{\\top}\\big(\\lambda(h)-\\bar{\\lambda}^{\\star}\\big)\\,-\\,\\varepsilon_{\\mathrm{approx}}^{2}\\,\\leq\\,\\bar{g}(\\lambda(h))\\,-\\,\\bar{g}(\\bar{\\lambda}^{\\star}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Lemma 12. In the stochastic variant of Algorithm $^{\\,l}$ in Section 4.3,the maximum prarametrized dual function in history up to step $h$ satisfies ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{h\\,\\to\\,\\infty}\\,\\,\\bar{g}_{\\mathrm{best}}(h)\\;\\geq\\;\\bar{D}^{\\star}\\,-\\,\\left(\\frac{\\eta S^{2}}{2}+\\varepsilon_{\\mathrm{approx}}^{2}\\right).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. The proof is based on the supermartingale convergence theorem [67, Theorem E7.4]. We introduce two processes, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\alpha(h)\\;:=\\;\\left\\|\\lambda(h)-{\\bar{\\lambda}}^{\\star}\\right\\|^{2}\\,\\mathbb{1}\\left({\\bar{D}}^{\\star}-{\\bar{g}}_{\\mathrm{best}}(h)>{\\frac{\\eta S^{2}}{2}}+\\varepsilon_{\\mathrm{approx}}^{2}\\right)\n$$", "text_format": "latex", "page_idx": 25}, {"type": "equation", "text": "$$\n\\beta(h)\\;:=\\;\\left(2\\eta\\left(\\bar{D}^{\\star}-\\bar{g}(\\lambda(h))-\\varepsilon_{\\mathrm{approx}}^{2}\\right)-\\eta^{2}S^{2}\\right)\\mathbb{1}\\left(\\bar{D}^{\\star}-\\bar{g}_{\\mathrm{best}}(h)>\\frac{\\eta S^{2}}{2}+\\varepsilon_{\\mathrm{approx}}^{2}\\right)\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $\\alpha(h)$ measures the gap between $\\lambda(h)$ and ${\\bar{\\lambda}}^{\\star}$ when the optimality gap $\\bar{D}^{\\star}-\\bar{g}_{\\mathrm{best}}(h)$ is below the threshold, and $\\beta(h)$ measures the gap bewteen $\\bar{D}^{\\star}$ and $\\bar{g}(\\lambda(h))$ (up to some optimization errors) when the optimality gap $\\bar{D}^{\\star}-\\bar{g}_{\\mathrm{best}}(\\bar{h)}$ is below the threshold. Clearly, $\\alpha(h)$ is non-negative. Also, $\\beta(h)$ is non-negative due to ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\bar{D}^{\\star}\\,-\\,\\bar{g}_{\\mathrm{best}}(h)\\,-\\,\\frac{\\eta S^{2}}{2}\\,-\\,\\varepsilon_{\\mathrm{approx}}^{2}\\,\\le\\,\\bar{D}^{\\star}\\,-\\,\\bar{g}(\\lambda(h))\\,-\\,\\frac{\\eta S^{2}}{2}\\,-\\,\\varepsilon_{\\mathrm{approx}}^{2}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Let ${\\mathcal{F}}_{h}$ be the $\\sigma$ -algebragenerated by sequences: $\\alpha(h^{\\prime}),\\beta(h^{\\prime})$ , and $\\lambda(h^{\\prime})$ for $h^{\\prime}\\leq h$ . Thus, $\\{\\mathcal{F}_{h}\\}_{h\\ge1}$ is a natural filtration. We notice that $\\alpha(h+1)$ and $\\beta(h+1)$ aredeterminedby $\\lambda(h)$ in each step. Hence, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathbb{E}\\left[\\,\\alpha(h+1)\\,|\\,\\mathcal{F}_{h}\\,\\right]}&{=}&{\\mathbb{E}\\left[\\,\\alpha(h+1)\\,|\\,\\lambda(h)\\,\\right]}\\\\ &{=}&{\\mathbb{E}\\left[\\,\\alpha(h+1)\\,|\\,\\lambda(h),\\alpha(h)=0\\,\\right]\\operatorname*{Pr}\\left(\\alpha(h)=0\\right)}\\\\ &&{+\\,\\mathbb{E}\\left[\\,\\alpha(h+1)\\,|\\,\\lambda(h),\\alpha(h)>0\\,\\right]\\operatorname*{Pr}\\left(\\alpha(h)>0\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We first show that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\,\\alpha(h+1)\\,|\\,\\mathcal{F}_{h}\\,\\right]\\;\\le\\;\\alpha(h)\\;-\\;\\beta(h).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "A simple case is when $\\alpha(h)=0$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\alpha(h+1)\\,|\\,\\mathcal{F}_{h}\\,\\right]\\,=\\,\\mathbb{E}\\left[\\,\\alpha(h+1)\\,|\\,\\lambda(h),\\alpha(h)=0\\,\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "There re two siuaton for $\\alpha(h)=0$ First, f $\\begin{array}{r}{\\bar{D}^{\\star}-\\bar{g}_{\\mathrm{best}}(h)\\leq\\frac{\\eta S^{2}}{2}+\\varepsilon_{\\mathrm{approx}}^{2}}\\end{array}$ then $\\alpha(h)=\\beta(h)=0$ Due to $\\bar{g}_{\\mathrm{best}}(h+1)\\geq\\bar{g}_{\\mathrm{best}}(h)$ , we have $\\beta(h)=0$ and $\\begin{array}{r}{\\bar{D}^{\\star}-\\bar{g}_{\\mathrm{best}}(h+1)\\,\\le\\,\\frac{\\eta S^{2}}{2}+\\varepsilon_{\\mathrm{approx}}^{2}}\\end{array}$ . Thus, $\\alpha(h+1)\\,=\\,0$ , and (26) holds. Second, if $\\lambda(h)\\,=\\,{\\bar{\\lambda}}^{\\star}$ , but $\\begin{array}{r}{\\bar{D}^{\\star}-\\bar{g}_{\\mathrm{best}}(h)\\,>\\,\\frac{\\eta S^{2}}{2}+\\varepsilon_{\\mathrm{approx}}^{2}}\\end{array}$ $\\bar{D}^{\\star}\\,=\\,\\bar{g}(\\lambda(h))$ . Hence, $\\beta(h)\\,<\\,0$ , which contradicts the non-negativeness of $\\beta(h)$ .Therefore, $\\begin{array}{r}{\\bar{D}^{\\star}-\\bar{g}_{\\mathrm{best}}(h)\\leq\\frac{\\eta S^{2}}{2}+\\varepsilon_{\\mathrm{approx}}^{2}}\\end{array}$ has to hold, which is the first situation. ", "page_idx": 25}, {"type": "text", "text": "We next show (26) when $\\alpha(h)>0$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathbb{E}\\left[\\left.\\alpha(h+1)\\;\\middle|\\;\\mathcal{F}_{h}\\right.\\right]}&{=}&{\\mathbb{E}\\left[\\left.\\alpha(h+1)\\;\\middle|\\;\\lambda(h),\\alpha(h)>0\\right.\\right]}\\\\ &{=}&{\\mathbb{E}\\left[\\left.\\left\\|\\lambda(h)-\\bar{\\lambda}^{\\star}\\right\\|^{2}\\mathbb{1}\\left(\\bar{D}^{\\star}-\\bar{g}_{\\mathrm{bex}}(h)>\\frac{\\eta S^{2}}{2}+\\varepsilon_{\\mathrm{approx}}^{2}\\right)\\;\\middle|\\;\\lambda(h),\\alpha(h)>0\\right.\\right]}\\\\ &{\\leq}&{\\mathbb{E}\\left[\\left.\\left\\|\\lambda(h)-\\bar{\\lambda}^{\\star}\\right\\|^{2}\\;\\middle|\\;\\lambda(h),\\alpha(h)>0\\right.\\right]}\\\\ &{\\leq}&{\\left\\|\\lambda(h)-\\bar{\\lambda}^{\\star}\\right\\|^{2}+\\eta^{2}S^{2}\\;-\\;2\\eta\\left(\\bar{D}^{\\star}-\\bar{g}(\\lambda(h))-\\varepsilon_{\\mathrm{approx}}^{2}\\right)}\\\\ &{\\leq}&{\\alpha(h)\\,-\\,\\beta(h)}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the last inequality is due to Lemma 11, and the last equality is due to $\\bar{D}^{\\star}-\\,\\bar{g}_{\\mathrm{best}}(h)\\ >$ + approx. Therefore,(26) holds. ", "page_idx": 26}, {"type": "text", "text": "Finally, application of the supermartingale convergence theorem [67, Theorem E7.4] to the processes $\\alpha(h)$ and $\\beta(h)$ for $h\\geq1$ concludesthat $\\beta(t)$ is almost surely summable, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{h\\,\\to\\,\\infty}\\;\\beta(h)\\;=\\;0\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "This means that either ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{t\\,\\rightarrow\\,\\infty}\\operatorname*{inf}_{\\Phi}\\ 2\\eta\\left(\\bar{D}^{\\star}-\\bar{g}(\\lambda(h))-\\varepsilon_{\\mathrm{approx}}^{2}\\right)\\ -\\ \\eta^{2}S^{2}\\;=\\;0\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "$\\begin{array}{r}{\\bar{D}^{\\star}-\\bar{g}_{\\mathrm{best}}(h)\\leq\\frac{\\eta S^{2}}{2}+\\varepsilon_{\\mathrm{approx}}^{2}}\\end{array}$ whichconclues ourdesiredret. ", "page_idx": 26}, {"type": "text", "text": "Denote th step when $\\bar{g}_{\\mathrm{best}}(h)$ achieves $\\begin{array}{r}{\\bar{D}^{\\star}-\\left(\\frac{\\eta S^{2}}{2}+\\varepsilon_{\\mathrm{approx}}^{2}\\right)}\\end{array}$ $h_{\\mathrm{{best}}}$ andth asscaed ual variable be $\\bar{\\lambda}_{\\mathrm{best}}=\\lambda(h_{\\mathrm{best}})$ ", "page_idx": 26}, {"type": "text", "text": "Lemma 13. For a stochastic variant of Algorithm $^{\\,l}$ in Section 4.3, we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left\\|\\bar{\\lambda}_{\\mathrm{best}}-\\lambda^{\\star}\\right\\|^{2}\\;\\leq\\;\\frac{2}{\\mu}\\left(\\frac{\\eta S^{2}}{2}+\\varepsilon_{\\mathrm{approx}}^{2}+4R(1+\\left\\|\\bar{\\lambda}_{\\mathrm{best}}\\right\\|_{1})\\nu\\right).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. We denote the segment between $\\bar{\\lambda}_{\\mathrm{best}}$ and $\\lambda^{\\star}$ by $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ . By Lemma 10, the dual function $g(\\lambda)$ is strongly concave on $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ with parameter $\\mu$ . Thus, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\left\\|\\bar{\\lambda}_{\\mathrm{best}}-\\lambda^{\\star}\\right\\|^{2}}&{\\leq}&{\\displaystyle\\frac{2}{\\mu}\\left(g(\\lambda^{\\star})-g(\\bar{\\lambda}_{\\mathrm{best}})\\right)}\\\\ &{\\leq}&{\\displaystyle\\frac{2}{\\mu}\\left(\\bar{g}(\\lambda^{\\star})-\\bar{g}(\\bar{\\lambda}_{\\mathrm{best}})+4R(1+\\left\\|\\bar{\\lambda}_{\\mathrm{best}}\\right\\|_{1})\\nu\\right)}\\\\ &{\\leq}&{\\displaystyle\\frac{2}{\\mu}\\left(\\frac{\\eta S^{2}}{2}+\\varepsilon_{\\mathrm{approx}}^{2}+4R(1+\\left\\|\\bar{\\lambda}_{\\mathrm{best}}\\right\\|_{1})\\nu\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "$\\bar{g}(\\bar{\\lambda}_{\\mathrm{best}})\\,=\\,\\bar{g}(\\lambda(h_{\\mathrm{best}}))$ $\\bar{D}^{\\star}\\geq$ $\\bar{g}(\\bar{\\lambda}_{\\mathrm{best}})$ , and the third inequality is due to Lemma 12. ", "page_idx": 26}, {"type": "text", "text": "Proof. By the triangle inequality for TV distance, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathrm{TV}\\left(q_{\\mathrm{mix}}^{\\star},\\,\\bar{p}^{\\star}(\\bar{\\lambda}_{\\mathrm{best}})\\right)}&{\\leq}&{\\mathrm{TV}\\left(q_{\\mathrm{mix}}^{\\star},\\,q_{\\mathrm{mix}}^{(\\bar{\\lambda}_{\\mathrm{best}})}\\right)\\,+\\,\\mathrm{TV}\\left(q_{\\mathrm{mix}}^{(\\bar{\\lambda}_{\\mathrm{best}})},\\,\\bar{p}^{\\star}(\\bar{\\lambda}_{\\mathrm{best}})\\right)}\\\\ &{\\leq}&{\\displaystyle\\left\\|\\lambda^{\\star}-\\bar{\\lambda}_{\\mathrm{best}}\\right\\|_{1}+\\,\\frac{d^{2}\\log^{3}T}{\\sqrt{T}}\\,+\\,\\sqrt{d}\\left(\\log^{2}T\\right)\\varepsilon_{\\mathrm{score}}}\\\\ &{\\leq}&{\\displaystyle\\frac{2}{\\mu}\\left(\\frac{\\eta S^{2}}{2}+\\varepsilon_{\\mathrm{approx}}^{2}+4R\\left(1+\\left\\|\\bar{\\lambda}_{\\mathrm{best}}\\right\\|_{1}\\right)\\nu\\right)\\,+\\,\\frac{d^{2}\\log^{3}T}{\\sqrt{T}}\\,+\\,\\sqrt{d}\\left(\\log^{2}T\\right)\\varepsilon_{\\mathrm{score}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the second inequality is due to Lemma 7 and Lemma 3, and the last inequality is due to $\\left\\|\\lambda\\right\\|_{1}\\leq{\\sqrt{m}}\\left\\|\\lambda\\right\\|$ and Lemma 13. \u53e3 ", "page_idx": 26}, {"type": "text", "text": "C Experimental details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "We provide implementation details of our computational experiments in Section 5. The source code is available here.2 ", "page_idx": 27}, {"type": "text", "text": "Algorithm details: We train our constrained diffusion models by replacing the exact primal minimization step in Algorithm 1 with $N$ steps of gradient descent with the Lagrangian as a loss function. Without loss of generality, we take the noise prediction formulation of diffusion rather than the score-matching formulation used in our theory. Since these two formulations are equivalent, this has no bearing on our main results. Algorithm 2 depicts our practical implementation of Algorithm 1 . ", "page_idx": 27}, {"type": "text", "text": "Algorithm 2 Practical Implementation of Algorithm 1 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "1: Input: total diffusion steps $T$ , diffusion parameter $\\alpha_{t}$ , total dual iterations $H$ , number of primal descent steps per dual update $N$ , dual step size $\\eta_{d}$ , primal step size $\\eta_{p}$ , initial model parameters $\\theta(0)$   \n2: Initialize: $\\lambda(1)=0$   \n3: for $h=1,\\cdots\\,,H$ do.   \n4:for $n=1,\\cdots\\,,N\\,.$ do   \n5: $\\theta_{1}~=~\\theta(h-1)$   \n6: $\\theta_{n+1}\\;=\\;\\theta_{n}-\\eta_{p}\\,\\nabla_{\\theta}\\left(\\widehat{\\mathbb{E}}_{x_{0}\\sim q,\\,t,\\,x_{t}}\\left[\\,\\|\\widehat{\\epsilon}_{\\theta_{n}}(x_{t},t)-\\epsilon_{0}\\|^{2}\\,\\right]\\,+\\,\\sum_{i=1}^{m}\\lambda_{i}\\,\\widehat{\\mathbb{E}}_{x_{0}\\sim q^{i},\\,t,\\,x_{t}}\\left[\\,\\|\\widehat{\\epsilon}_{\\theta_{n}}(x_{t},t)-\\epsilon_{0}\\|^{2}\\,\\right]\\,\\right)\\,,$   \n7: 0(h) = 0N+1.   \n8: end for   \n9: Update the dual variable $\\lambda_{i}(h+1)\\ =\\ \\left[\\lambda_{i}(h)\\,+\\,\\eta_{d}\\left(\\widehat{\\mathbb{E}}_{x_{0}\\sim q^{i},\\,t,\\,x_{t}}\\,\\left[\\left\\Vert\\widehat{\\epsilon}_{\\theta(h)}(x_{t},t)-\\epsilon_{0}\\right\\Vert^{2}\\right]\\,-\\,\\widetilde{b}^{i}\\right)\\right]_{+}\\mathrm{~for~all~}i.$ ", "page_idx": 27}, {"type": "text", "text": "10: end for ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In Algorithm 2, the unbiased estimate of the noise prediction loss is evaluated via ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\widehat{\\mathbb{E}}_{x_{0}\\sim q,\\,t,\\,x_{t}}\\,\\left[\\left\\Vert\\widehat{\\epsilon}_{\\theta}\\bigl(x_{t},t\\bigr)-\\epsilon_{0}\\right\\Vert^{2}\\right]\\;=\\;\\sum_{i\\,=\\,1}^{B}\\left\\Vert\\widehat{\\epsilon}_{\\theta}\\bigl(x_{t^{(i)}},t^{(i)}\\bigr)-\\epsilon_{0}^{(i)}\\right\\Vert^{2}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "$\\{x_{0}^{(i)}\\}_{i=1}^{B}$ $q$ $\\{t^{(i)}\\}_{i=1}^{B}$ )= 1 are time steps randomly $[2,T]$ $x_{t}(i)$ $\\boldsymbol{x}_{0}^{(i)}$ $t^{(i)}$ noise $\\epsilon_{0}$ is sampled from the standard Gaussian, and $x_{t}$ is derived from $x_{t}=\\sqrt{\\bar{\\alpha}_{t}}x_{t}+\\sqrt{1-\\bar{\\alpha}_{t}}\\epsilon_{0}$ ", "page_idx": 27}, {"type": "text", "text": "We remark an important implementation detail in the fine-tuning experiment. In the fine-tuning constraints, we have to evaluate the KL divergence ", "page_idx": 27}, {"type": "equation", "text": "$$\nD_{\\mathrm{KL}}(p_{\\theta_{\\mathrm{pre}}}(x_{0:T})\\parallel p_{\\theta}(x_{0:T}))\\;\\le\\;b_{i}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $p_{\\theta_{\\mathrm{pre}}}(x_{0:T})$ is the joint distribution of the samples and latents generated by the backward process of the pre-trained model. The KL divergence constraint in (27) further reduces to ", "page_idx": 27}, {"type": "equation", "text": "$$\nD_{\\mathrm{KL}}\\big(p_{\\theta_{\\mathrm{pe}}}(x_{0:T})\\,\\|\\,p_{\\theta}(x_{0:T})\\big)\\;=\\;\\mathbb{E}_{x_{t}\\sim p_{\\theta_{\\mathrm{pe}}},\\,t}\\,\\Big[\\,\\big\\|\\widehat{\\epsilon}_{\\theta_{\\mathrm{pe}}}(x_{t},t)-\\widehat{\\epsilon}_{\\theta}(x_{t},t)\\big\\|^{2}\\,\\Big]\\,+\\,\\mathrm{constant}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "To estimate the expectation in (28), we need to sample latents $x_{t}$ from the backward distribution $p_{\\theta_{\\mathrm{pre}}}(x_{0:T})$ . In practice, this is computationally inefficient, since it requires running inference each time one wants to sample a latent. This is why we implement this with sampling $x_{t}$ as random Gaussian noise instead. This still ensures that the predictions of the new model $p_{\\theta}$ don't differ too much from the pre-trained distribution $p_{\\theta_{\\mathrm{pre}}}$ while making sampling batches much faster. ", "page_idx": 27}, {"type": "text", "text": "Resilient constrained learning. The choice of the constraint thresholds $\\{b_{i}\\}_{i=1}^{m}$ hasnoticeable effect on the training of constrained diffusion models. To avoid an exhaustive hyperparameter tuning process, in the minority class experiment, we use the resilient constrained learning technique [36] to adjust the thresholds $\\{b_{i}\\}_{i=1}^{m}$ during training. In essence, the resilient constrained learning adds a constraint relaxation cost to the loss and relaxes the thresholds by updating them through gradient descent each time we update the dual variable. It can further be shown theoretically that an equivalent formulation of resilience is achieved by adding a quadratic regularizer of the dual variable into the loss objective and setting the constraint thresholds to be zero, i.e., $\\widetilde{b}_{i}\\,=\\,0$ .This is the approach we used in our experiments since it has fewer hyperparameters. We note that the only difference between Algorithm 2 and Algorithm 3 is the additional term in the dual variable update step (line 9 of Algorithm 3). ", "page_idx": 27}, {"type": "table", "img_path": "Es2Ey2tGmM/tmp/445c806df85f5354235f62326dea0b2ae5357506f6bed874ed0b59760764c36f.jpg", "table_caption": ["Table 1: Parameters of U-net Model used as noise predictor "], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "Algorithm 3 Resilient Constrained Diffusion Models via Dual Training ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "1: Input: total diffusion steps $T$ , diffusion parameter $\\alpha_{t}$ , total dual iterations $H$ , number of primal descent steps per dual update $N$ , dual step size $\\eta_{d}$ , primal step size $\\eta_{p}$ , constraint step size $\\eta_{c}$ initial model parameters $\\theta(0)$ , constraint relaxation cost $\\gamma$   \n2: Initialize: $\\bar{\\lambda(1)}=0$   \n3: for $h=1,\\cdots\\,,H$ do   \n4:for $n=1,\\cdots\\,,N$ do   \n5: $\\theta_{1}~=~\\theta(h-1)$   \n6: $\\theta_{n+1}\\;=\\;\\theta_{n}-\\eta_{p}\\,\\nabla_{\\theta}\\left(\\widehat{\\mathbb{E}}_{x_{0}\\sim q,\\,t,\\,x_{t}}\\left[\\,\\|\\widehat{\\epsilon}_{\\theta_{n}}(x_{t},t)-\\epsilon_{0}\\|^{2}\\,\\right]\\,+\\,\\sum_{i=1}^{m}\\lambda_{i}\\widehat{\\mathbb{E}}_{x_{0}\\sim q^{i},\\,t,\\,x_{t}}\\left[\\,\\|\\widehat{\\epsilon}_{\\theta_{n}}(x_{t},t)-\\epsilon_{0}\\|^{2}\\,\\right]\\,\\right)\\,,$ 1   \n7: \u03b8(h) = ON+1.   \n8: end for   \n9: Update the dual variable $\\lambda_{i}(h+1)\\ =\\ \\left[\\lambda_{i}(h)\\,+\\,\\eta_{d}\\,\\left(\\widehat{\\mathbb{E}}_{x_{0}\\sim q^{i},\\,t,\\,x_{t}}\\,\\left[\\left\\Vert\\widehat{\\epsilon}_{\\theta(h)}(x_{t},t)-\\epsilon_{0}\\right\\Vert^{2}\\right]\\,-\\,\\widetilde{b}^{i}(h)\\,-\\,2\\gamma\\lambda_{i}(h)\\right)\\right]_{+}\\mathrm{~for~all~}i.$ ", "page_idx": 28}, {"type": "text", "text": "10: end for ", "page_idx": 28}, {"type": "text", "text": "Model architecture. We use a time-conditioned U-net model as is common in image diffusion tasks for all three datasets. The time conditioning is done by adding a positional embedding of the time to the input image. The parameters of the model are summarized in Table 1. The fifth downsampling block and the corresponding upsampling block are Res-Net blocks with spatial self-attention. ", "page_idx": 28}, {"type": "text", "text": "Hyperparameters. We summarize the important hyperparameters in our experiments in Table 2. In the unconstrained models that we train for comparison, we use the same hyperparameters as the constrained version, disregarding the parameters related to the dual and relaxation updates. For models trained on Image-Net, when training the constrained models, we initialized to the parameters of the unconstrained model to make training times shorter. ", "page_idx": 28}, {"type": "text", "text": "Hyperparameter sensitivity. We remark the sensitivity of the dual training algorithm to the number of dual iterations, primal/dual batch sizes, and primal/dual learning rates. ", "page_idx": 28}, {"type": "text", "text": "\u00b7 Number of dual iterations: In our implementation this shows up as the number of primal GD steps per dual update, $N$ . Experimentally, we have observed that as long as $N$ is greater than 1, the results are not sensitive to this value. Additionally, the dual updates add a negligible computational overhead. Hence, updating the dual nearly as many times as we update model parameters doesn't reduce training efficiency. ", "page_idx": 28}, {"type": "text", "text": "\u00b7 Primal/dual batch sizes: We have included results of training a constrained model on an unbalanced subset of MNIST, using different primal/dual batch sizes (See Table 3.) The results suggest that for the minority class experiments, when the ratio between Primal and Dual batch sizes is larger, the model performs better (lower FID and more evenly distributed samples). This is in line with the heuristic we used in the included experiments in the paper where we chose the batch sizes such that the ratio of primal to dual batch size is close to size ratio of entire dataset to constraint datasets (which are much smaller). Howver for the fine-tuning task, the batch sizes did not seem to affect the final result as much. ", "page_idx": 28}, {"type": "table", "img_path": "Es2Ey2tGmM/tmp/e197256da5ba1f42875cd84435599653751502b0aa488676aa399a90d28e8618.jpg", "table_caption": ["Table 2: Hyperparameter values used in the main experiments. MC denotes Minority Class experiments and FT denotes Fine-Tuning experiments. - denotes that resilience was not used for the experiment. "], "table_footnote": [], "page_idx": 29}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "\u00b7 Primal/dual learning rate: For the primal learning rate, we followed the best practice used to train standard diffusion models. For the dual learning rate $\\eta$ , we refer to Theorem 8 in the paper, showing a smaller error bound for smaller $\\eta$ while slowing convergence. In practice, as long as $\\eta\\leq1$ , we observed that the model converges to similar results reliably. ", "page_idx": 29}, {"type": "text", "text": "Efficiency of constrained diffusion: We note that the complexity of sampling from our constrained diffusion model does not increase with the number of constraints, as our trained diffusion model functions like a standard diffusion model to generate samples. Importantly, we remark that training our constrained diffusion model has comparable efficiency to training standard diffusion models detailed next. ", "page_idx": 29}, {"type": "text", "text": "The additional computational cost of our dual-based training (Algorithm 1) arises from: (i) updating the dual variables; (i) updating the diffusion model in the primal update. ", "page_idx": 29}, {"type": "text", "text": "\u00b7 Cost of updating the dual variables: We note that our dual-based training has the same number of dual variables as the number of constraints. Thus, the cost for the dual update is linear in the number of constraints. To update each dual variable, we can directly use the ELBO loss over the batches sampled from each constrained dataset (already computed for the Lagrangian). Therefore, the cost of updating dual variables is negligible. ", "page_idx": 29}, {"type": "text", "text": "\u00b7 Cost of updating the diffusion model in the primal update: We note that the primal update trains a standard diffusion model based on the Lagrangian with updated dual variables. In our experiments, this primal training often requires as few as 2-3 updates per dual update. Thus, when training our constrained model, we can train for the same number of epochs as an unconstrained model but update the dual variables after every few primal steps. As a result, training our constrained diffusion model is almost as effcient as training standard unconstrained models. ", "page_idx": 29}, {"type": "text", "text": "The only concern we encountered regarding efficiency is that batches need to be sampled from every constrained dataset at each step to estimate the Lagrangian. This introduces a small GPU memory overhead that increases with additional constraints. However, this is somewhat mitigated by the fact that constrained datasets are often much smaller than the original dataset, allowing us to choose a smaller batch size for the constrained datasets without degrading performance (see discussion on batch sizes in hyperparameter sensitivity section). ", "page_idx": 29}, {"type": "table", "img_path": "Es2Ey2tGmM/tmp/bbba4f16a67b9122482b60b5f9b9b74a99830300080e4b292226bb4e3736eebc.jpg", "table_caption": ["Table 3: Constrained model trained on MNIST with different Primal/Dual Batch sizes "], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "FID scores. As a quantitative means of evaluating our constrained diffusion models, we use the FID (Frechet Inception Distance) as a metric to gauge the quality of the samples generated by diffusion models. The FID score was first introduced in [33] in form of ", "page_idx": 30}, {"type": "equation", "text": "$$\nd_{\\mathrm{FID}}^{2}((m,C),(m_{w},C_{w}))\\ =\\ \\lVert m-m_{w}\\rVert_{2}^{2}\\,+\\,\\mathrm{Tr}(C+C_{w}-2(C C_{w})^{1/2})\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $m$ and $C$ represent mean and variance, respectively, of the distribution of the features of the data samples which have been extracted by an inception model [69]. Similarly, $m_{w}$ and $C_{w}$ represent the mean and variance of some reference distribution that we are computing the distance to. In our experiment, we compute the FID scores by generating 15000 samples from the diffusion model we are evaluating and comparing them to a balanced version of the original dataset. In the experiment with MNIST, this is the actual dataset. In the experiments with Celeb-A and Image-Net, since there is an imbalance in the original dataset, we consider a balanced subset of each with an equal number of samples from each class as reference. We use the clean-FID library [58] for standard computation of the FID scores. ", "page_idx": 30}, {"type": "text", "text": "We note that our FID scores are somewhat larger compared to typical baselines in the literature. This is expected as a consequence of our experimental setup. We train both the unconstrained and constrained models, on a biased subset of the dataset wherein some of the classes have significantly fewer samples than the rest. We then compute the FID scores for these models compared to the actual dataset itself which is unbiased (i.e., every class has the same number of samples). These FID scores approximate how close the learned distribution of the model trained on biased data, is to the underlying unbiased distribution. ", "page_idx": 30}, {"type": "text", "text": "This setup contrasts with existing results in the literature, where the FID is computed with respect to unbiased data, and the models are also trained on unbiased data. Therefore, it is expected that such models will achieve better FID scores than constrained or unconstrained models trained with biased data. Our purpose in reporting the FIDs was not to compare them to existing results (as such a comparison would be uninformative) but to demonstrate that, when trained on biased data, the constrained model achieves better FID scores than the unconstrained model. ", "page_idx": 30}, {"type": "text", "text": "Compute resources. We run all experiments on two NVIDIA RTX 3090-Ti GPUs in parallel. The amount of GPU memory used was 16 Gigabytes per GPU. For experiments with MNIST and Celeb-A datasets, training each model took between 2-3 hours. This increased to 7-8 hours for latent diffusion models trained for the Image-Net experiments. ", "page_idx": 30}, {"type": "text", "text": "Assets and libraries. We use the PyTorch [59] and Diffusers [73] Python libraries for training our constrained diffusion models, and Adam with decoupled weight decay [52] as an optimizer. The accelerate library [30] is used for the parallelization of the training processes across multiple GPUs. For classifiers used in evaluating the generated samples of the models, we use the following pretrained models accessible on the Huggingface model database: A Vision transformer-based classifier for MNIST digits with $\\%99.5$ validation accuracy (https : //huggingface. co/farleyknight-org-username/vit-base-mnist). A classifier for images of male/female faces with $\\%98.6$ validation accuracy (https : / /huggingface. co/cledoux42/ GenderNew_v0o2). For classifying the image-net data, a zero-shot classifier based on a CLIP model (https: //huggingface.co/openai/clip-vit-base-patch32) was used. The Autoencoder for the latent diffusion model was the stable diffusion VAE with KL regularization found on (https://huggingface.co/stabilityai/sd-vae-ft-mse). ", "page_idx": 30}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We summarize our constrained diffusion models in Section 3, optimality guarantees of unparametrized constrained diffusion models in Section 3.1, optimality guarantees of parametrized constrained diffusion models and training algorithms in Section 4, and experimental results in Section 5. ", "page_idx": 31}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We mention two potential limitations of this work as several future directions in Section 6. ", "page_idx": 31}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We provide all assumptions, lemmas, and theorems in Sections 3 and 4, and provide proof details in Appendix B. ", "page_idx": 31}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We summarize our experimental results in Section 5, and provide implementation details of experiments in Appendix C, together with additional experimental results. ", "page_idx": 31}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: A link to the code for replicating our main experiments has been provided in Appendix C. The datasets are open source machine learning datasets that are accessible Online. ", "page_idx": 31}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We provide experimental details in Appendix C including the hyperparameters used for each experiment. Our training details can be found in the code in supplemental material. ", "page_idx": 31}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: While the histogram plots that showcase our main results do not include error bars, we do provide Frechet Distance metrics (that are a measure of statistical distance between sample distributions) which emphasize our results. ", "page_idx": 32}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We include compute details in Appendix C. ", "page_idx": 32}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We have reviewed the NeurlPS Code of Ethics and fully comply with it during the preparation of this paper. ", "page_idx": 32}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The potential impacts of the work are discussed in Section 1. ", "page_idx": 32}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: We release no models and the supplemental code for training image generation model, trains the model on MNIST and Celeb-A datasets neither of which have potential for misuse. ", "page_idx": 32}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The libraries and assets used have been noted and credited in Appendix C ", "page_idx": 32}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] Justification: [NA] ", "page_idx": 32}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] Justification: [NA] ", "page_idx": 32}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] Justification: [NA] ", "page_idx": 33}]