[{"type": "text", "text": "Large Language Model Unlearning via Embedding-Corrupted Prompts ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Chris Yuhao Liu\u2217 Yaxuan Wang Jeffrey Flanigan\u2020 Yang Liu\u2020 ", "page_idx": 0}, {"type": "text", "text": "University of California, Santa Cruz {yliu298,ywan1225,jmflanig,yangliu}@ucsc.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large language models (LLMs) have advanced to encompass extensive knowledge across diverse domains. Yet controlling what a large language model should not know is important for ensuring alignment and thus safe use. However, accurately and efficiently unlearning knowledge from an LLM remains challenging due to the potential collateral damage caused by the fuzzy boundary between retention and forgetting, and the large computational requirements for optimization across stateof-the-art models with hundreds of billions of parameters. In this work, we present Embedding-COrrupted (ECO) Prompts, a lightweight unlearning framework for large language models to address both the challenges of knowledge entanglement and unlearning efficiency. Instead of relying on the LLM itself to unlearn, we enforce an unlearned state during inference by employing a prompt classifier to identify and safeguard prompts to forget. We learn corruptions added to prompt embeddings via zeroth order optimization toward the unlearning objective offline and corrupt prompts flagged by the classifier during inference. We find that these embedding-corrupted prompts not only lead to desirable outputs that satisfy the unlearning objective but also closely approximate the output from a model that has never been trained on the data intended for forgetting. Through extensive experiments on unlearning, we demonstrate the superiority of our method in achieving promising unlearning at nearly zero side effects in general domains and domains closely related to the unlearned ones. Additionally, we highlight the scalability of our method to 100 LLMs, ranging from 0.5B to 236B parameters, incurring no additional cost as the number of parameters increases. We have made our code publicly available at https://github.com/chrisliu298/llm-unlearn-eco. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The use of large language models (LLMs), trained on extensive text corpora [2, 45, 6, 131, 64, 8, 144], has increasingly become standard in daily life since the arrival of ChatGPT [103]. Despite the benefits LLMs offer, they pose potential risks across a range of domains, such as copyright infringement [65, 49, 72], dissemination of hazardous knowledge [73, 53, 39, 115], and privacy violations [126, 95, 100]. Adherence to the General Data Protection Regulation (GDPR) [37], which requires the removal of users\u2019 data post-training, is essential. Machine unlearning has emerged as a new paradigm [19, 101] and has been widely studied for classification models and tasks in recent years [130, 79, 69, 38]. However, unlearning in the context of LLMs remains largely underexplored, presenting unique challenges and risks that extend beyond privacy concerns due to the infeasibility of retraining from scratch [19, 20], the ease with which anyone can access powerful models, and the substantial capabilities of these models across various tasks [80, 88]. ", "page_idx": 0}, {"type": "image", "img_path": "e5icsXBD8Q/tmp/f7fa3de0d1eedb6c96c0432814777214911fd5a6a9081cf4db9d3e589d1413e9.jpg", "img_caption": ["Figure 1: Using embedding-corrupted prompts to maintain an unlearned state on the LLM subject to unlearning. We first employ a classifier to identify whether the incoming prompt falls within the scope of the unlearning target. We construct embedding-corrupted prompts by selectively corrupting dimensions within the tokens\u2019 embeddings. The corruption parameter is learned offline via zeroth order optimization. An unlearned state is imposed during inference and does not require any updates to the original model\u2019s weights. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Various machine unlearning methods have been proposed specifically for LLMs to address the above challenges. A major line of approaches focuses on parameter fine-tuning [60] based on a modified loss, usually by unlearning on the forget data and learning from the retained data to preserve utility [137, 143, 23, 142, 73, 149, 62], which requires only a small number of weight updates compared to retraining from scratch. Other approaches include model editing techniques [58, 139, 13, 148, 56, 102, 83], unlearning via in-context examples in the prompt [106, 99], and guarding the prompts themselves [129]. Although effective, some approaches have been shown to impair a model\u2019s general capabilities [50, 88]. This is due to knowledge entanglement caused by the fuzzy boundary between retention and forgetting objectives (e.g., forgetting a single person without affecting other related ones) [93, 90, 88, 109]. Additionally, most prior work targets unlearning at the million- to billionparameter scale through gradient-based optimization [143, 56, 36, 142, 90, 149, 73, 62], making the cost of unlearning scale with the model size and potentially expensive even with parameter-efficient modules. This cost could rise significantly for advanced proprietary models with hundreds of billions of parameters (e.g., GPT-4 [2], Gemini [45], Claude [6], and other model-as-a-service (MaaS) [42] providers), which makes gradient-based unlearning methods impractical. ", "page_idx": 1}, {"type": "text", "text": "In this work, we explore if an \u201cunlearned state\u201d can be imposed on an intact LLM and focus on tackling the challenges of knowledge entanglement and unlearning inefficiency in LLMs. We hypothesize that unlearning can be implemented as a state by decomposing the unlearning problem into two more tractable subproblems: 1) unlearning target identification, which explicitly identifies if the prompt contains content within the unlearning target, and 2) forgetting, which ensures that the generated responses no longer reflect any prior knowledge related to the unlearning target. We present Embedding-COrrupted (ECO) Prompts, a lightweight two-step framework to tackle both problems above: ", "page_idx": 1}, {"type": "text", "text": "1. To identify the unlearning target, we use a prompt classifier that is trained to explicitly model the prompt distribution and to safeguard prompts within the scope of the unlearning target. 2. To achieve forgetting, we approximate an unlearned state by passing the query identified by the prompt classifier to the LLM, but in a corrupted form. We leverage corruptions learned efficiently via zeroth-order optimization [124, 125] and apply them to the prompt\u2019s embedding space during inference. ", "page_idx": 1}, {"type": "text", "text": "\u2022 We introduce Embedding-COrrupted (ECO) Prompts, a novel and lightweight LLM unlearning method that enforces an unlearned state over an intact LLM.   \n\u2022 We demonstrate that, instead of relying on unlearning objective optimization, carefully corrupted prompts lead to behavior that resembles that of a model which has never seen the data intended to be forgotten, across multiple tasks and metrics.   \n\u2022 Through extensive experiments across three knowledge unlearning tasks, we demonstrate the superior performance of our method in both retaining and forgetting, incurring virtually zero side effects and no additional cost when scaling to larger models.   \n\u2022 To the best of our knowledge, we are the first to demonstrate universally effective and efficient unlearning for 100 LLMs and up to 236B parameters. ", "page_idx": 2}, {"type": "text", "text": "2 Preliminaries and Problem Setup ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Threat Model ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In our threat model, we consider threats in three categories: entity leaking, hazardous knowledge, and copyrighted content extraction. We consider a gray box setting similar to that of [73] and [129], where users interact with an LLM or a model-as-a-service [42] through a chat interface or structured API access [121]. Under this setting, all users can send prompts to the LLM and receive the corresponding completions or per-token logits of the output tokens. We also assume that adversaries within the user group generate prompts in-distribution and attempt to jailbreak either the guarding mechanism or the LLM itself. Our threats and goals below are as follows. ", "page_idx": 2}, {"type": "text", "text": "Entity leaking Entity leaking occurs when an LLM inadvertently discloses the identity or sensitive information of specific individuals whose data was unintentionally included in the training set [67, 17, 86]. Our goal is to ensure that the LLM either provides incorrect responses or refuses to answer queries from threat agents that involve these individuals or groups. ", "page_idx": 2}, {"type": "text", "text": "Hazardous knowledge Given the ease of use and accessibility of both commercial and open-source LLMs, individuals with malicious intent could exploit the advanced capabilities of LLMs to acquire hazardous knowledge at minimal cost [73, 53, 39, 115]. Here, the objective is to prevent such actors from obtaining dangerous knowledge from LLMs while ensuring that the models retain their original capabilities in benign but related domains. ", "page_idx": 2}, {"type": "text", "text": "Copyrighted content Extracting and distributing copyrighted content from an LLM is generally illegal, as it involves reproducing and distributing protected material without permission [49, 65, 72]. Even if copyrighted content is filtered from the pre-training data, fragments of the text may still be scattered throughout the corpus, and the LLM could memorize them. An attempt to extract the original passage by prompting with a known portion of the text might cause the LLM to generate the passage verbatim, which we aim to prevent. ", "page_idx": 2}, {"type": "text", "text": "Beyond the categorization of risks presented above, we also highlight a commonly overlooked aspect in unlearning: timeliness [109, 123, 16, 101]. Timeliness measures how quickly unlearning can be completed once the relevant risks are identified. Given the volume of real-time interactions from MaaS users [42], the effectiveness of LLM unlearning may degrade progressively with each hour of delay, particularly in safety and privacy domains. Our objective is to develop a method that can be implemented with extreme efficiency, ideally operating in real-time. ", "page_idx": 2}, {"type": "text", "text": "2.2 Problem Setup ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We assume a learning algorithm $A^{3}$ , the training set $D_{t r}$ , and the forget set $D_{f}$ . For each dataset $D$ , we have $D=\\{\\mathbf{z}_{i}\\}_{i=1}^{N}$ , where each $\\mathbf z_{i}=\\{\\mathbf x_{i},\\mathbf y_{i}\\}$ . In the traditional setting of machine unlearning [19, 101], a retained model $\\theta_{r}$ that has never seen the forget dataset is obtained via the learning algorithm but excluding the forget set, $\\pmb{\\theta}_{r}=A(D_{t r}\\setminus D_{f})$ , where $D_{r}=D_{t r}\\setminus D_{f}$ is known as the retain set. We use $\\theta_{o}$ to denote the original model4 obtained from the learning algorithm $A$ , and $\\theta_{r}$ to represent a retained model retrained from scratch via an unlearning algorithm $U$ , which we define below, by training on $D_{t r}$ and $D_{r}$ , respectively. ", "page_idx": 2}, {"type": "text", "text": "Based on our threat model in Section 2.1, which does not allow users to access model weights, instead of achieving unlearning in the weight space [101], we focus on weak unlearning [11] in the output space. Specifically, we aim for similarity between models $h(\\mathbf{x};\\pmb{\\theta}_{r})$ and $h(\\mathbf{x};\\pmb{\\theta}_{u})$ for all $\\mathbf{x}$ , where $h:\\mathcal{X}\\times\\Theta\\to\\mathcal{Y}$ maps from the input space $\\mathcal{X}$ and weight space $\\Theta$ to the output space $\\boldsymbol{\\wp}$ . ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "A relaxed objective of unlearning Because we are in the LLM setting, we use a relaxed definition of unlearning that does not require differential privacy requirements (i.e., $(\\epsilon,\\delta)$ -close), similar to [118]. More specifically, we follow prior work [46, 23, 69, 61, 142, 54] and evaluate whether the retained model and the unlearned model\u2019s metric values over a set of metrics $\\mathcal{M}=\\left\\{m_{1},m_{2},...,m_{K}\\right\\}$ are similar on both $D_{r}$ and $D_{f}$ . To maintain the general utility of the LLM after unlearning, we would also like the model to perform well on an o.o.d. general domain distribution $\\mathcal{D}_{g}$ , which is unknown during unlearning. Therefore, our goal of unlearning is ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\frac{\\mathbb{E}[m_{i}\\left(h\\left(\\mathbf{x};\\pmb{\\theta}_{u}\\right)\\right)]}{\\mathbb{E}[m_{i}\\left(h\\left(\\mathbf{x};\\pmb{\\theta}_{r}\\right)\\right)]}\\approx1\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "for all $m_{i}\\in\\mathcal{M}$ , where $\\mathcal{M}$ is a set of non-negative metrics. We want this to hold separately for each case $\\mathbf{x}\\sim p_{\\mathcal{D}_{f}}(\\mathbf{x})$ , $\\mathbf{x}\\sim p_{\\mathbf{}\\mathcal{D}_{r}}(\\mathbf{x})$ , and $\\mathbf{x}\\sim p_{\\mathcal{D}_{g}}(\\mathbf{x})$ . During evaluation, we assess whether the two models have empirically similar performance over the metrics set $\\mathcal{M}$ . ", "page_idx": 3}, {"type": "text", "text": "3 ECO: Unlearned LLMs via Embedding-Corrupted Prompts ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "3.1 Method Overview ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Our method consists of two steps: 1) train a prompt classifier to predict if an incoming prompt falls within the scope of unlearning, and 2) corrupt the prompt in the embedding space if the classifier makes a positive prediction (i.e., should forget). ", "page_idx": 3}, {"type": "text", "text": "Enforcing retaining and forgetting via a classifier We first train a prompt classifier to explicitly identify if the prompt falls within the scope of unlearning. For any incoming prompt, $\\mathbf{x}$ , the prompt classifier $C$ takes in $\\mathbf{x}$ and returns $p_{C}(f\\mid\\mathbf{\\bar{x}})=1-p_{C}(r\\mid\\mathbf{x})$ , the probability of the prompt being in the scope of forgetting. Similar to any classifier prediction, if $p_{C}(\\bar{f}\\mid\\mathbf{x})>\\bar{p_{C}}(r\\mid\\mathbf{x})$ , we consider $\\mathbf{x}$ as containing the unlearning concept that our LLM is supposed to forget. Formally, given a positive prediction, $p_{C}(f\\mid\\mathbf{x})>p_{C}(r\\mid\\mathbf{x})$ , we replace the original input $\\mathbf{x}$ with $\\tilde{\\bf x}$ . Otherwise, the original $\\mathbf{x}$ is passed to the LLM. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{x}={\\left\\{\\overset{\\underset{\\mathrm{\\Large{x}}}{\\tilde{\\mathbf{x}}}}{\\operatorname{\\nabla}}\\right.}\\ \\operatorname{\\it{p_{C}}}(f\\mid\\mathbf{x})>{\\boldsymbol{p}}_{C}(r\\mid\\mathbf{x})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Embedding-corrupted prompts Instead of modifying $\\mathbf{x}$ in the token space, we corrupt it in the embedding space. Let $\\bar{\\mathbf{x}^{\\prime}}=\\{\\bar{x}_{1},x_{2},\\ldots,x_{T}\\}$ be a prompt of $T$ tokens and $\\mathbf{e}=\\{e_{1},e_{2},\\bar{\\mathbf{\\Omega}}...,e_{T}\\}$ be the corresponding embedding vectors. Let $\\mathcal{E}$ be the space of the token embeddings. Each embedding vector is produced by an embedding function $\\bar{E}:\\mathcal{X}\\to\\mathbb{R}^{d}$ . We also use the symbol $\\sigma\\in S$ (where ${\\mathcal{S}}\\subset\\mathbb{R}$ ) to denote the strength of the corruption, which parameterizes the corruption function. Formally, for a single prompt $\\mathbf{x}$ mapped to the embeddings $\\bar{\\mathbf{e}^{\\prime}}\\!=E(\\mathbf{x})=\\{e_{1},e_{2},\\dots,{\\bar{e}}_{T}\\}$ , a corruption function Corrupt : $\\mathcal{E}\\times\\mathcal{S}\\rightarrow\\mathcal{E}$ , parameterized by $\\sigma$ , produces the embedding-corrupted prompts ", "page_idx": 3}, {"type": "equation", "text": "$$\n{\\tilde{\\mathbf{e}}}=\\mathsf{c o r r u p t}(\\mathbf{e};\\sigma)=\\bigl\\{\\tilde{e}_{1},\\tilde{e}_{2},...\\,,\\tilde{e}_{T}\\bigr\\}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Let $\\tilde{h}:\\mathcal{E}\\times\\Theta\\to\\mathcal{V}$ be the function $h$ but taking the input embeddings instead of input tokens (i.e., $h$ with the input embedding layer detached). Our objective is to pick a good $\\sigma^{*}$ such that the following modified unlearning objective is satisfied: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\frac{\\mathbb{E}\\left[m_{i}\\left(\\tilde{h}\\left(\\mathrm{Corrupt}(\\mathbf{e};\\boldsymbol{\\sigma}^{*});\\boldsymbol{\\theta}_{o}\\right)\\right)\\right]}{\\hat{v}_{r}}\\approx1,\\forall m_{i}\\in\\mathcal{M}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here, $\\hat{v}_{r}$ is used to approximate the true $\\mathbb{E}[m_{i}(\\tilde{h}({\\mathbf{e}};\\pmb{\\theta}_{r}))]$ as the retained model is not available. ", "page_idx": 3}, {"type": "text", "text": "3.2 Decision Threshold Calibration and Conformal Prediction ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Due to the potential fuzzy boundary between retaining and forgetting, one needs to pick a threshold better than $\\dot{p}(f\\mid\\mathbf{x})>p(r\\mid\\mathbf{x})$ , which does not take into account the classifier\u2019s confidence. Depending on the application and the empirical performance of the classifier predictions, we incorporate two types of thresholding techniques. ", "page_idx": 3}, {"type": "text", "text": "Simple thresholding We choose a simple threshold, $\\tau$ , as the criterion to determine if a prompt $\\mathbf{x}$ belongs to the forget distribution. Formally, the output $\\hat{\\mathbf{y}}$ from the LLM is returned by feeding a prompt selected by the classifier, based on its prediction $p_{C}(f\\mid\\mathbf{x})$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{y}}=\\left\\{\\tilde{h}\\left(\\mathrm{Corrupt}(\\mathbf{e};\\sigma);\\pmb{\\theta}_{o}\\right)\\mathrm{\\partial\\Omega~if~}p_{C}(f\\mid\\mathbf{x})\\geq\\tau\\right.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We pick the value of $\\tau$ using a separate set $D_{\\mathrm{cal}}$ for calibration. The goal is to choose an optimal $\\tau$ that has the smallest false positive rate and false negative rate on $D_{\\mathrm{cal}}$ . ", "page_idx": 4}, {"type": "text", "text": "Conformal prediction We also consider conformal prediction (CP) [134], which finds a calibrated threshold given a target error rate $\\alpha$ , as a second way for threshold calibration. In essence, conformal prediction uses a small user-specified error rate, $\\alpha$ , and unlikelihood scores (e.g., $1-p_{C}(y\\mid\\mathbf{x}))$ on a calibration set to derive a threshold. Labels with unlikelihood scores lower than the threshold are included in the final prediction set. ", "page_idx": 4}, {"type": "text", "text": "We adapt the split conformal prediction setup [134], which uses a separate calibration set, $D_{\\mathrm{cal}}=$ $\\bar{\\{\\mathbf{x}_{i},y_{i}\\}_{i=1}^{n}\\ (y\\ \\in\\ \\{\\mathbf{r},f\\})}$ , to determine a conformity threshold and a non-conformity score, $\\textit{S}:$ $\\mathcal{X}\\times\\mathcal{Y}\\rightarrow\\mathbb{R}$ , to measure how unlikely a sample $(x,y)$ is to the classifier $C$ . Following conventional choice, we use $s_{i}=S(\\mathbf{x}_{i},y_{i})=1\\!-\\!\\dot{p_{C}}(y_{i}\\mid\\bar{\\mathbf{x}_{i}})$ as the non-conformity score. Given the calibration set size $n$ and a small user-specified error rate $\\alpha$ , we determine a quantile $\\hat{q}$ using the $\\lceil(n\\!+\\!1)\\!\\cdot\\!(1\\!-\\!\\alpha)\\rceil/n$ empirical quantile in the non-conformity scores from $D_{\\mathrm{cal}}$ . The final prediction set on a new test sample $\\mathbf{X}_{\\mathrm{test}}$ is formed by including all labels with a non-conformity score below $\\hat{q}$ as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{C}_{\\alpha}\\left(\\mathbf{x}_{\\mathrm{test}}\\right)=\\left\\{y\\in\\mathcal{y}:S\\left(\\mathbf{x}_{\\mathrm{test}},y\\right)\\leq\\hat{q}\\right\\}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Formally, given the prompt classifier $C$ , a prediction set $\\mathcal{C}_{\\alpha}\\left(\\mathbf{x}\\right)$ for the prompt $\\mathbf{x}$ , and the decision threshold $\\tau$ , the response from the LLM is obtained by the following rules: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{y}}=\\left\\{\\overset{\\widetilde{h}}{\\widetilde{h}}\\left(\\mathbf{Corrupt}(\\mathbf{e};\\sigma);\\pmb{\\theta}_{o}\\right)\\quad\\mathrm{if~}1\\in\\mathcal{C}_{\\alpha}\\right.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In experiments, we pick the thresholding method based on its empirical performance. In Appendix C.5, we give a toy example of how to determine the prediction set size for a test sample. ", "page_idx": 4}, {"type": "text", "text": "3.3 Embedding-Corrupted Prompts ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Given an accurate classifier, one can already mitigate the risk defined in our threat model by providing a template response. However, doing so violates the weak unlearning objective for $\\mathbf{x}\\sim p_{\\mathcal{D}_{f}}(\\mathbf{x})$ in Equation (1), because a retained model (that is, a model not trained on the forget data) is highly unlikely to give template responses to prompts in the forget data. To actually achieve unlearning, given the prompt classifier obtained in Section 3.2, we introduce a simple method that learns to corrupt user prompts in the embedding space efficiently via zeroth order optimization [124, 125] toward the unlearning objective. One may also set $\\sigma$ manually without optimization, at the cost of being further away from the desired retained model (see below). ", "page_idx": 4}, {"type": "text", "text": "Optimization objective A natural choice to make the unlearned model behave like a retained model is to minimize a distance function that quantifies the gap between the two models for all $m\\in\\mathcal{M}$ . As the retained model is not available (otherwise, unlearning would not be needed), we use a surrogate metric value $\\hat{v}_{r}$ if available to approximate how the retained model would behave over $\\mathcal{M}$ . Based on our relaxed unlearning objective in Equation (4), we define a general distance measure below: ", "page_idx": 4}, {"type": "equation", "text": "$$\nd(\\tilde{\\mathbf{e}},\\theta_{o},\\hat{v}_{r},\\mathcal{M})=\\frac{1}{|\\mathcal{M}|}\\sum_{i}\\Bigm|\\underbrace{m_{i}(\\tilde{h}(\\tilde{\\mathbf{e}};\\theta_{o}))}_{\\mathrm{unlearned~metric~value}}-\\underbrace{\\hat{v}_{r}}_{\\mathrm{surrogate~retained~ret~for~\\omega~}}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We aim to learn a $\\sigma^{*}$ such that the metric gap in Equation (8) between the unlearned model and the retained model is minimized. Formally, given a parameterized corruption function Corrupt $(\\cdot;\\sigma)$ , our unlearning objective is to minimize the following: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\boldsymbol{\\sigma}^{*}=\\mathop{\\underset{\\boldsymbol{\\sigma}}{\\arg\\operatorname*{min}}}_{\\boldsymbol{\\sigma}}d\\left(\\mathrm{Corrupt}(\\mathbf{e};\\boldsymbol{\\sigma}),\\pmb{\\theta}_{o},\\hat{v}_{r},\\mathcal{M}\\right)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Note: If the metric value $\\hat{v}_{r}$ is not obtainable, one may tune $\\sigma$ directly and inspect whether the model output on the forget set aligns with the unlearning criteria. For classification-style tasks, the target $\\hat{v}_{r}$ may correspond to random guessing. ", "page_idx": 4}, {"type": "text", "text": "Corruption learning via zeroth order optimization We now formulate the zeroth order gradient approximation via finite differences [124, 125]. Given a pre-defined perturbation size $\\mu$ applied to the current corruption parameter $\\sigma_{k}$ , we treat the distance function $d(\\cdot)$ as a black-box and query it for the final metric gap during optimization. Because we only learn the strength of the corruption function with a scalar-valued $\\sigma$ , we use a deterministic perturbation to $\\sigma_{k}$ . For a single sample, given an initial guess $\\sigma_{0}$ , a step size $\\eta$ , and a smoothing parameter $\\mu$ (also known as perturbation size), the minimization of Equation (9) uses the following update rules: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{\\mathbf{e}}_{\\mathrm{forward}}=\\mathbf{e}+\\mathrm{Corrupt}(\\mathbf{e};\\sigma_{k}+\\mu)}\\\\ &{\\tilde{\\mathbf{e}}_{\\mathrm{backward}}=\\mathbf{e}+\\mathrm{Corrupt}(\\mathbf{e};\\sigma_{k}-\\mu)}\\\\ &{\\quad\\hat{\\nabla}d_{\\sigma_{k}}=\\frac{d\\left(\\tilde{\\mathbf{e}}_{\\mathrm{forward}},\\,\\theta_{o},\\,\\hat{v}_{r},\\mathcal{M}\\right)-d\\left(\\tilde{\\mathbf{e}}_{\\mathrm{backward}},\\,\\theta_{o},\\,\\hat{v}_{r},\\mathcal{M}\\right)}{2\\mu}}\\\\ &{\\quad\\sigma_{k+1}=\\sigma_{k}-\\eta\\hat{\\nabla}d_{\\sigma_{k}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Choice of corruption function Prior work [40] suggests that only a small number of dimensions for each embedding vector suffices to steer the output, so we only corrupt the first dimension of each token\u2019s embedding. We also experimented with other corruption functions (e.g., standard Gaussian noise or zeroing-out top entries) but found that our method is insensitive to the choice of corruption function, with all tested functions yielding similar end results. We conducted ablation studies on various corruption functions in Appendix D.2. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we present experimental results for entity unlearning (Section 4.2), hazardous knowledge unlearning (Section 4.3), and copyrighted content unlearning (Section 4.4). ", "page_idx": 5}, {"type": "text", "text": "4.1 Prompt Classifier ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "For each unlearning task, we fine-tune a RoBERTa [82] or a Llama-3.1-1B-Instruct [34] as the prompt classifier on the corresponding $D_{r}$ and $D_{f}$ . In entity and copyrighted content unlearning tasks, we use the entire $D_{f}$ to train the classifier5 because the unlearning target is fully captured by the forget set, which does not require generalization outside the set. For WMDP and MMLU, we only use a surrogate synthetic forget set $D_{\\bar{f}}$ to train the prompt classifier, and the actual forget set $D_{f}$ is not accessible until evaluation. For all prompt classifiers, we use an independent validation set $D_{\\mathrm{val}}$ to tune the decision threshold $\\tau$ and hyperparameters or to calibrate the empirical quantile $\\hat{q}$ , which is used to determine conformity. In Tables 5 to 7, we show that all classifiers can distinguish $D_{r}$ and $D_{f}$ well, and generalize to unseen $D_{g}$ with a low false positive rate. Meanwhile, although we do not specifically target out-of-distribution prompts related to jailbreak attempts, we demonstrate our classifiers\u2019 ability to prevent such risks in appendix C.3.3. We also show that simple data augmentation techniques can further enhance our classifiers\u2019 performance in detecting out-of-distribution and jailbreak prompts. We provide further detailed information on how prompt classifiers are trained for each task and their performance in Appendix C.3. ", "page_idx": 5}, {"type": "text", "text": "4.2 Entity Unlearning ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Experimental setup The TOFU dataset [90] is a synthetic question-answering dataset of author biographies. The goal is for an LLM trained on the complete dataset (all authors) to unlearn a fraction of fictitious authors $(1/5/10\\%)$ while retaining knowledge about both 1) the remaining fictitious authors and 2) the real world. To assess forgetting and retention, we use two metrics proposed alongside the TOFU dataset: forget quality and model utility. Forget quality is represented by a p-value from a Kolmogorov-Smirnov (KS) test, where a high value indicates high similarity in distribution between the output of the unlearned model and that of the retained model. Model utility assesses the model\u2019s performance on the retained set and real-world knowledge. For a detailed description of all the metrics, refer to Appendix C.1.1. We conduct experiments with two corruption functions, random noise (RN) and zero-out (ZO). We include all baselines from [90], a prompting baseline, and the recently proposed negative preference optimization (NPO) [149]. We provide formulations of all baselines in Appendix C.4. ", "page_idx": 5}, {"type": "image", "img_path": "e5icsXBD8Q/tmp/a6e99bea20a5c7e1fb4f6b721b3f920222b82b8b62d47e64176fb8568bacbc74.jpg", "img_caption": ["Figure 2: Model utility versus forget quality (p-value) on three different forget set sizes of the TOFU dataset after unlearning. We show two models, Phi-1.5 (top) and Llama-2-7B-Chat (bottom). For GA, GD, KL, PO, and the prompting baseline, the forget qualities are either too small or come at the cost of a substantial decrease in model utility. Negative preference optimization (NPO) [149] variants achieve a good balance in some cases, but the trade-off in model utility is still non-trivial. ECO-RN (random noise) and ECO-ZO (zero-out) achieve an almost identical distribution to the retained model while incurring no sacrifice in model utility. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "ECO brings Pareto improvement. In Figure 2, we illustrate the trade-off between model utility and forget quality for two models, Phi-1.5 [74] and Llama-2-7B-Chat [131], including forgetting $1\\%$ , $5\\%$ , and $10\\%$ of the samples. ECO-RN and ECO-ZO consistently achieve close-to-perfect forget quality regardless of the model or the size of the forget set. Notably, ECO-ZO maintains a distribution almost identical to the retained model (as the p-value is close to 1) in all cases, suggesting that ECO prompts can effectively approximate the outputs of the retained model in distribution. Given that the prompt classifier trained on the TOFU dataset incurs zero false positives, our method results in zero sacrifice in model utility, thus striking a perfect balance between forgetting and retention. For the ECO-RN variant, we optimize $\\sigma$ for Llama-2-7B-Chat on $1\\%$ of the forget set and use the same value for all five other settings, suggesting its transferability across models and forget tasks. ", "page_idx": 6}, {"type": "text", "text": "Baselines struggle to forget or collapse in utility. We also observe that GA, GD, KL, PO, and the prompting baseline exhibit minimal forgetting when the forget set size is small (i.e., $1\\%$ ). Meanwhile, some of them experience a substantial decrease and even a collapse in utility when the forget set is larger ${5\\%}$ and $10\\%$ ). Methods based on negative preference optimization [149] demonstrate a noticeably stronger trade-off compared to other baselines, especially with NPO-RT. Nevertheless, the effectiveness of the NPO variants varies across different models and forget set sizes, and the loss in model utility is non-trivial. We present the full results on all metrics and baselines in Table 17 and Table 18 in Appendix E.2. ", "page_idx": 6}, {"type": "text", "text": "4.3 Hazardous Knowledge Unlearning ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Experimental setup For both WMDP [73] and MMLU subset unlearning tasks [55], we directly unlearn on pre-trained models. The WMDP benchmark focuses on unlearning knowledge in biology, chemistry, and cybersecurity. In MMLU subset unlearning, the goal is to unlearn three subjects while retaining their closely related counterparts: economics (econometrics), law (jurisprudence), and physics (math), all requiring high-precision forgetting to resolve knowledge entanglement. In line with [73], we assess all models based on their multiple-choice accuracy. A successfully unlearned model should exhibit an accuracy near random guessing $25\\%$ for four-option multiple-choice questions). We employ the ECO-RN variant (random noise) as the corruption function for both tasks. We optimize the corruption strength $\\sigma$ only for Zephyr-7B on a set of 100 synthetic questions and answers generated by GPT-4 to ensure that real questions are not exposed during unlearning. The same corruption parameter $\\sigma$ is used for all other models. We compare our method against LLMU [143], SCRUB [69], SSD [41], RMU [73], and a prompting baseline that instructs the model not to answer questions within the domain correctly. ", "page_idx": 6}, {"type": "table", "img_path": "e5icsXBD8Q/tmp/dcc698ce84c525db367cedcf1f5684244623894a23933bff3229321ac639733a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "e5icsXBD8Q/tmp/881bc95547b82c0dc1d30f39a7a77d696a8348c34e3872bcf6097dd724cceabc.jpg", "table_caption": ["Table 1: Multiple-choice accuracy of five LLMs on the WMDP benchmark (forget) and the full MMLU (retain) after unlearning. ECO achieves accuracy close to random guessing on all subsets of the WMDP benchmark (as desired), and has zero decrease in accuracy on MMLU. Other baselines either struggle to forget or incur substantial decrease in MMLU. ", "Table 2: Multiple-choice accuracy of Zephyr-7B after unlearning, on three MMLU subsets and the corresponding retain sets. The prompting baseline hurts the accuracy on the three forget subsets. While RMU reduces the forget set accuracy to the level of random-guess, it incurs substantial performance decrease on econometrics and jurisprudence while unlearning economics and law. ECO achieves both perfect retaining and unlearning on all subsets. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "ECO is domain- and model-agnostic. In Tables 1 and 2, for all models on the WMDP benchmark, ECO achieves accuracy close to random guessing for multiple-choice questions while maintaining original MMLU performance. LLMU, SCRUB, and SSD show limited forgetting performance across all subjects. Although RMU successfully unlearns biology and cybersecurity, it retains accuracy in chemistry, indicating that unlearning capability may vary across subjects or the available data for unlearning. On Yi-34B-Chat and Mixtral-8x7B-Instruct, RMU\u2019s forgetting capability is not as effective as on Zephyr-7B, while ECO\u2019s performance remains consistent despite increased original performance on the task. ", "page_idx": 7}, {"type": "text", "text": "ECO unlearns at high precision. On MMLU subset unlearning, both ECO and RMU successfully unlearn the three chosen subjects (Table 2). However, RMU\u2019s accuracy in econometrics and jurisprudence significantly decreases. This implies that RMU might be sensitive to the entanglement of knowledge in closely related subjects. In contrast, this entanglement poses no problem for ECO\u2019s prompt classifier due to its low false positive rate in the retain domain. ", "page_idx": 7}, {"type": "text", "text": "ECO\u2019s universal effectiveness. To further validate the effectiveness of our method across various models, we conducted experiments on 100 models ranging from 0.5B to 236B on both the WMDP and MMLU subsets, using the same corruption function and hyperparameters obtained on Zephyr-7B. Our results in Table 21 and Table 22 further demonstrate that our method is universally effective without requiring hyperparameter tuning. ", "page_idx": 7}, {"type": "table", "img_path": "e5icsXBD8Q/tmp/9c777e73cfdfa2db29fb695020412bf1b9a32dc972b474f06d8bb087e448a4bc.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "4.4 Copyrighted Content Unlearning ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Experimental setup We select Harry Potter and the Sorcerer\u2019s Stone6 [112] and BBC News articles7 [75] as the copyrighted content material for unlearning and unlearn models fine-tuned on the text corpus. For this task, our goal is to prevent the unlearned model from generating passages with high similarity to the original text. For both datasets, we verify that the models used cannot generate the original passage and that the generated text has low similarity to the original passage. We first finetune a pre-trained model on the corresponding corpus and use it as the model subject to unlearning, with the original pre-trained checkpoint serving as the retained model. We use the original passage as the reference text and measure the text similarity between the reference and the text generated by the unlearned model using four text similarity metrics outlined in Appendix C.1.3, which we denote as the average similarity gap (ASG). Following [143], we also compute the perplexity and unique token ratio to assess whether the generated text remains meaningful and diverse. We compare our method to baselines in [90], SCRUB [69], and LLMU [143]. We present full experimental details in Appendix C. ", "page_idx": 8}, {"type": "text", "text": "ECO maintains high similarity to the retained model. In Section 4.4, ECO achieves scores sufficiently close to those of the retained model in terms of generated text similarity. On the general utility metric, our prompt classifiers effectively distinguish copyrighted content from general domain queries with no performance loss. KL minimization and LLMU are strong baselines in terms of similarity gap and general utility, but the diversity of the generated text decreases after unlearning. Both gradient difference and random mismatch reduce the issue of model collapse but still lead to notable performance losses in general utility. ", "page_idx": 8}, {"type": "text", "text": "We further validate our findings on a total of 19 models in Appendix E.5, spanning from Table 23 to Table 60. We observe that some baselines cannot consistently maintain strong results in either unlearning or general utility, while ECO remains stable and consistently achieves a low similarity gap with the retained model and unharmed utility. ", "page_idx": 8}, {"type": "text", "text": "5 Related Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Unlearning for LLMs Most existing machine unlearning methods for LLMs follow traditional machine unlearning approaches [19, 101, 119] to minimize the influence of the forget samples via gradient updates. The most straightforward approach employs a mixture of forgetting and retaining objectives by performing gradient ascent updates on the non-desirable sequences and regular gradient descent on the desirable sequences [137, 143, 23, 142, 73, 149, 62]. Other methods identify and modify a small fraction of the weights responsible for the undesired behavior [139, 13, 58], or use weight arithmetic [148, 56, 102, 83]. The above optimization-based methods all require compute that scales with the model size. Our method leaves the LLM subject to unlearning intact and unlearns by steering the inputs to match the output distribution of a retained model. Compute-wise, our unlearning method is independent of the model size. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "LLM guardrails Guardrailing, which accesses prompts before using them as inputs to the model, has been widely applied to modern LLMs to prevent adversaries with harmful incentives [111, 59, 145, 92, 71, 32, 138, 59, 47, 25]. Our work is most related to in-context unlearning [106] and a recent guardrail baseline via prompting [129], both of which require no additional fine-tuning to achieve unlearning to some extent. [106] leverages modern LLMs\u2019 ability in in-context learning by prepending a small number of positive and negative samples in the prompt to steer the model\u2019s response based on those samples. [129] guards the unlearning target via prompt injection, which inserts fixed instructions in the prompt to the LLM. Both methods can only be applied to instructiontuned models and rely on an LLM\u2019s ability to follow instructions. Prepending such instructions also leads to significant performance degradation on regular tasks, as shown in [129]. ", "page_idx": 9}, {"type": "text", "text": "Jailbreak via adversarial embeddings Prior work on LLM jailbreaking [155, 40, 76, 44, 105] has demonstrated the power of adversarially optimizing toward a prompt that elicits a desired LLM response. In particular, [40] shows that the attack can be simplified to learning perturbation vectors added to the token embeddings, which eliminates the need to optimize over discrete tokens. Our results on the behavior of the attacked models are similar to the findings in [44], where inserting certain non-natural language token sequences in the prompt could elicit refusal behavior or incorrect answers from an instruction-tuned LLM. While jailbreak approaches can theoretically be applied in unlearning applications, they are prohibitively expensive to run [81], and there is an additional requirement for specifying a sequence of desirable tokens. Both requirements make them unsuitable for the task of unlearning. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we introduced Embedding-COrrupted (ECO) Prompts, a novel method to tackle the dual challenges of knowledge entanglement and unlearning efficiency in LLMs. ECO leverages a thresholded prompt classifier to determine whether a prompt falls within the scope of the unlearning target. By decoupling the unlearning process from the LLMs themselves, ECO offers a scalable and efficient approach that remains effective across a wide range of model sizes, from 0.5B to 236B parameters, with minimal side effects and no additional computational overhead. Our experiments across three unlearning tasks validate ECO\u2019s effectiveness, setting a foundation for responsible AI deployment in real-world scenarios. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work is partially supported by the National Science Foundation under grants IIS-2143895, IIS-2040800, IIS-2416896, IIS-2007951 and CCF-2023495. We are also thankful for the computing resources provided by the Pacific Research Platform\u2019s Nautilus cluster, supported in part by National Science Foundation (NSF) awards CNS-1730158, ACI-1540112, ACI-1541349, OAC-1826967, OAC-2112167, CNS-2100237, CNS-2120019, the University of California Office of the President, the University of California San Diego\u2019s California Institute for Telecommunications and Information Technology/Qualcomm Institute, and CENIC for the 100 Gbps networks. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. Phi-3 technical report: A highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. ", "page_idx": 9}, {"type": "text", "text": "[2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. ", "page_idx": 10}, {"type": "text", "text": "[3] AI $@$ Meta. Llama 3 model card. 2024. ", "page_idx": 10}, {"type": "text", "text": "[4] Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien Launay, Quentin Malartic, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. The falcon series of language models:towards open frontier models. 2023.   \n[5] Malaikannan Sankarasubbu Ankit Pal. Openbiollms: Advancing open-source large language models for healthcare and life sciences. https://huggingface.co/aaditya/ OpenBioLLM-Llama3-70B, 2024.   \n[6] Anthropic. Introducing the next generation of claude. https://www.anthropic.com/ news/claude-3-family, Mar 2024.   \n[7] Viraat Aryabumi, John Dang, Dwarak Talupuru, Saurabh Dash, David Cairuz, Hangyu Lin, Bharat Venkitesh, Madeline Smith, Kelly Marchisio, Sebastian Ruder, Acyr Locatelli, Julia Kreutzer, Nick Frosst, Phil Blunsom, Marzieh Fadaee, Ahmet \u00dcst\u00fcn, and Sara Hooker. Aya 23: Open weight releases to further multilingual progress, 2024. [8] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023.   \n[9] Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, pages 65\u201372, 2005.   \n[10] Alvaro Bartolome, Jiwoo Hong, Noah Lee, Kashif Rasul, and Lewis Tunstall. Zephyr 141b a39b. https://huggingface.co/HuggingFaceH4/zephyr-orpo-141b-A35b-v0. 1, 2024.   \n[11] Thomas Baumhauer, Pascal Sch\u00f6ttle, and Matthias Zeppelzauer. Machine unlearning: Linear filtration for logit-based classifiers. Machine Learning, 111(9):3203\u20133226, 2022.   \n[12] Marco Bellagente, Jonathan Tow, Dakota Mahan, Duy Phung, Maksym Zhuravinskyi, Reshinth Adithyan, James Baicoianu, Ben Brooks, Nathan Cooper, Ashish Datta, et al. Stable lm 2 1.6 b technical report. arXiv preprint arXiv:2402.17834, 2024.   \n[13] Nora Belrose, David Schneider-Joseph, Shauli Ravfogel, Ryan Cotterell, Edward Raff, and Stella Biderman. Leace: Perfect linear concept erasure in closed form. arXiv preprint arXiv:2306.03819, 2023.   \n[14] Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, pages 2397\u20132430. PMLR, 2023.   \n[15] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, pages 7432\u20137439, 2020.   \n[16] Alberto Blanco-Justicia, Najeeb Jebreel, Benet Manzanares, David S\u00e1nchez, Josep DomingoFerrer, Guillem Collell, and Kuan Eeik Tan. Digital forgetting in large language models: A survey of unlearning methods. arXiv preprint arXiv:2404.02062, 2024.   \n[17] Jaydeep Borkar. What can we learn from data leakage and unlearning for law? arXiv preprint arXiv:2307.10476, 2023.   \n[18] Aleksandar Botev, Soham De, Samuel L Smith, Anushan Fernando, George-Cristian Muraru, Ruba Haroun, Leonard Berrada, Razvan Pascanu, Pier Giuseppe Sessa, Robert Dadashi, et al. Recurrentgemma: Moving past transformers for efficient open language models. arXiv preprint arXiv:2404.07839, 2024.   \n[19] Lucas Bourtoule, Varun Chandrasekaran, Christopher A Choquette-Choo, Hengrui Jia, Adelin Travers, Baiwu Zhang, David Lie, and Nicolas Papernot. Machine unlearning. In 2021 IEEE Symposium on Security and Privacy (SP), pages 141\u2013159. IEEE, 2021.   \n[20] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u2013 1901, 2020.   \n[21] Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, et al. Internlm2 technical report. arXiv preprint arXiv:2403.17297, 2024.   \n[22] Nicholas Carlini, Steve Chien, Milad Nasr, Shuang Song, Andreas Terzis, and Florian Tramer. Membership inference attacks from first principles. In 2022 IEEE Symposium on Security and Privacy (SP), pages 1897\u20131914. IEEE, 2022.   \n[23] Jiaao Chen and Diyi Yang. Unlearn what you want to forget: Efficient unlearning for llms. arXiv preprint arXiv:2310.20150, 2023.   \n[24] Min Chen, Weizhuo Gao, Gaoyang Liu, Kai Peng, and Chen Wang. Boundary unlearning: Rapid forgetting of deep networks via shifting the decision boundary. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7766\u20137775, 2023.   \n[25] Zhixuan Chu, Yan Wang, Longfei Li, Zhibo Wang, Zhan Qin, and Kui Ren. A causal explainable guardrails for large language models. arXiv preprint arXiv:2405.04160, 2024.   \n[26] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019.   \n[27] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018.   \n[28] CodeGemma Team, Ale Jakse Hartman, Andrea Hu, Christopher A. Choquette-Choo, Heri Zhao, Jane Fine, Jeffrey Hui, Jingyue Shen, Joe Kelley, Joshua Howland, Kshitij Bansal, Luke Vilnis, Mateo Wirth, Nam Nguyen, Paul Michel, Peter Choy, Pratik Joshi, Ravin Kumar, Sarmad Hashmi, Shubham Agrawal, Siqi Zuo, Tris Warkentin, and Zhitao et al. Gong. Codegemma: Open code models based on gemma. 2024.   \n[29] Cohere Team. Command r: Retrieval-augmented generation at production scale, 2024.   \n[30] Databricks Team. Introducing dbrx: A new state-of-the-art open llm, 2024.   \n[31] DeepSeek-AI. Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model. arXiv preprint arXiv:2405.04434, 2024.   \n[32] Yi Dong, Ronghui Mu, Gaojie Jin, Yi Qi, Jinwei Hu, Xingyu Zhao, Jie Meng, Wenjie Ruan, and Xiaowei Huang. Building guardrails for large language models. arXiv preprint arXiv:2402.01822, 2024.   \n[33] Michael Duan, Anshuman Suri, Niloofar Mireshghallah, Sewon Min, Weijia Shi, Luke Zettlemoyer, Yulia Tsvetkov, Yejin Choi, David Evans, and Hannaneh Hajishirzi. Do membership inference attacks work on large language models? arXiv preprint arXiv:2402.07841, 2024.   \n[34] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.   \n[35] Javid Ebrahimi, Hao Yang, and Wei Zhang. How does adversarial fine-tuning benefit bert? arXiv preprint arXiv:2108.13602, 2021.   \n[36] Ronen Eldan and Mark Russinovich. Who\u2019s harry potter? approximate unlearning in llms. arXiv preprint arXiv:2310.02238, 2023.   \n[37] European Union. General data protection regulation (gdpr). https://gdpr-info.eu/, 2016.   \n[38] Chongyu Fan, Jiancheng Liu, Yihua Zhang, Dennis Wei, Eric Wong, and Sijia Liu. Salun: Empowering machine unlearning via gradient-based weight saliency in both image classification and generation. arXiv preprint arXiv:2310.12508, 2023.   \n[39] Richard Fang, Rohan Bindu, Akul Gupta, Qiusi Zhan, and Daniel Kang. Llm agents can autonomously hack websites. arXiv preprint arXiv:2402.06664, 2024.   \n[40] Stanislav Fort. Scaling laws for adversarial attacks on language model activations. arXiv preprint arXiv:2312.02780, 2023.   \n[41] Jack Foster, Stefan Schoepf, and Alexandra Brintrup. Fast machine unlearning without retraining through selective synaptic dampening. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 12043\u201312051, 2024.   \n[42] Wensheng Gan, Shicheng Wan, and S Yu Philip. Model-as-a-service (maas): A survey. In 2023 IEEE International Conference on Big Data (BigData), pages 4636\u20134645. IEEE, 2023.   \n[43] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPof,i Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac\u2019h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 12 2023.   \n[44] Jonas Geiping, Alex Stein, Manli Shu, Khalid Saifullah, Yuxin Wen, and Tom Goldstein. Coercing llms to do and reveal (almost) anything. arXiv preprint arXiv:2402.14020, 2024.   \n[45] Gemini. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.   \n[46] Aditya Golatkar, Alessandro Achille, and Stefano Soatto. Eternal sunshine of the spotless net: Selective forgetting in deep networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9304\u20139312, 2020.   \n[47] Shubh Goyal, Medha Hira, Shubham Mishra, Sukriti Goyal, Arnav Goel, Niharika Dadu, DB Kirushikesh, Sameep Mehta, and Nishtha Madaan. Llmguard: Guarding against unsafe llm behavior. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 23790\u201323792, 2024.   \n[48] Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, et al. Olmo: Accelerating the science of language models. arXiv preprint arXiv:2402.00838, 2024.   \n[49] Michael M. Grynbaum and Ryan Mac. The times sues openai and microsoft over a.i. use of copyrighted work. The New York Times, 12 2023.   \n[50] Jia-Chen Gu, Hao-Xiang Xu, Jun-Yu Ma, Pan Lu, Zhen-Hua Ling, Kai-Wei Chang, and Nanyun Peng. Model editing can hurt general abilities of large language models. arXiv preprint arXiv:2401.04700, 2024.   \n[51] Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y Wu, YK Li, et al. Deepseek-coder: When the large language model meets programming\u2013the rise of code intelligence. arXiv preprint arXiv:2401.14196, 2024.   \n[52] Felix Hamborg, Norman Meuschke, Corinna Breitinger, and Bela Gipp. news-please: A generic news crawler and extractor. In Proceedings of the 15th International Symposium of Information Science, pages 218\u2013223, March 2017.   \n[53] Bahareh Harandizadeh, Abel Salinas, and Fred Morstatter. Risk and response in large language models: Evaluating key threat categories. arXiv preprint arXiv:2403.14988, 2024.   \n[54] Jamie Hayes, Ilia Shumailov, Eleni Triantaflilou, Amr Khalifa, and Nicolas Papernot. Inexact unlearning needs more careful evaluations to avoid a false sense of privacy. arXiv preprint arXiv:2403.01218, 2024.   \n[55] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.   \n[56] Xinshuo Hu, Dongfang Li, Baotian Hu, Zihao Zheng, Zhenyu Liu, and Min Zhang. Separate the wheat from the chaff: Model deficiency unlearning via parameter-efficient module operation. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 18252\u201318260, 2024.   \n[57] James Y Huang, Wenxuan Zhou, Fei Wang, Fred Morstatter, Sheng Zhang, Hoifung Poon, and Muhao Chen. Offset unlearning for large language models. arXiv preprint arXiv:2404.11045, 2024.   \n[58] Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Suchin Gururangan, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi. Editing models with task arithmetic. arXiv preprint arXiv:2212.04089, 2022.   \n[59] Hakan Inan, Kartikeya Upasani, Jianfeng Chi, Rashi Rungta, Krithika Iyer, Yuning Mao, Michael Tontchev, Qing Hu, Brian Fuller, Davide Testuggine, et al. Llama guard: Llm-based input-output safeguard for human-ai conversations. arXiv preprint arXiv:2312.06674, 2023.   \n[60] Joel Jang, Dongkeun Yoon, Sohee Yang, Sungmin Cha, Moontae Lee, Lajanugen Logeswaran, and Minjoon Seo. Knowledge unlearning for mitigating privacy risks in language models. arXiv preprint arXiv:2210.01504, 2022.   \n[61] Jinghan Jia, Jiancheng Liu, Parikshit Ram, Yuguang Yao, Gaowen Liu, Yang Liu, Pranay Sharma, and Sijia Liu. Model sparsification can simplify machine unlearning. arXiv preprint arXiv:2304.04934, 2023.   \n[62] Jinghan Jia, Yihua Zhang, Yimeng Zhang, Jiancheng Liu, Bharat Runwal, James Diffenderfer, Bhavya Kailkhura, and Sijia Liu. Soul: Unlocking the power of second-order optimization for llm unlearning. arXiv preprint arXiv:2404.18239, 2024.   \n[63] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.   \n[64] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024.   \n[65] Antonia Karamolegkou, Jiaang Li, Li Zhou, and Anders S\u00f8gaard. Copyright violations and large language models. arXiv preprint arXiv:2310.13771, 2023.   \n[66] Jinhwa Kim, Ali Derakhshan, and Ian G Harris. Robust safety classifier for large language models: Adversarial prompt shield. arXiv preprint arXiv:2311.00172, 2023.   \n[67] Siwon Kim, Sangdoo Yun, Hwaran Lee, Martin Gubri, Sungroh Yoon, and Seong Joon Oh. Propile: Probing privacy leakage in large language models. Advances in Neural Information Processing Systems, 36, 2024.   \n[68] Vinayshekhar Bannihatti Kumar, Rashmi Gangadharaiah, and Dan Roth. Privacy adhering machine un-learning in nlp. arXiv preprint arXiv:2212.09573, 2022.   \n[69] Meghdad Kurmanji, Peter Triantafillou, Jamie Hayes, and Eleni Triantafillou. Towards unbounded machine unlearning. Advances in Neural Information Processing Systems, 36, 2024.   \n[70] Yanis Labrak, Adrien Bazoge, Emmanuel Morin, Pierre-Antoine Gourraud, Mickael Rouvier, and Richard Dufour. Biomistral: A collection of open-source pretrained large language models for medical domains, 2024.   \n[71] Alyssa Lees, Vinh Q Tran, Yi Tay, Jeffrey Sorensen, Jai Gupta, Donald Metzler, and Lucy Vasserman. A new generation of perspective api: Efficient multilingual character-level transformers. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 3197\u20133207, 2022.   \n[72] Haodong Li, Gelei Deng, Yi Liu, Kailong Wang, Yuekang Li, Tianwei Zhang, Yang Liu, Guoai Xu, Guosheng Xu, and Haoyu Wang. Digger: Detecting copyright content mis-usage in large language model training. arXiv preprint arXiv:2401.00676, 2024.   \n[73] Nathaniel Li, Alexander Pan, Anjali Gopal, Summer Yue, Daniel Berrios, Alice Gatti, Justin D Li, Ann-Kathrin Dombrowski, Shashwat Goel, Long Phan, et al. The wmdp benchmark: Measuring and reducing malicious use with unlearning. arXiv preprint arXiv:2403.03218, 2024.   \n[74] Yuanzhi Li, S\u00e9bastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. Textbooks are all you need ii: phi-1.5 technical report. arXiv preprint arXiv:2309.05463, 2023.   \n[75] Yucheng Li, Frank Guerin, and Chenghua Lin. Latesteval: Addressing data contamination in language model evaluation through dynamic and time-sensitive test construction. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 18600\u201318607, 2024.   \n[76] Zeyi Liao and Huan Sun. Amplegcg: Learning a universal and transferable generative model of adversarial suffixes for jailbreaking both open and closed llms. arXiv preprint arXiv:2404.07921, 2024.   \n[77] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pages 74\u201381, 2004.   \n[78] Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958, 2021.   \n[79] Jiancheng Liu, Parikshit Ram, Yuguang Yao, Gaowen Liu, Yang Liu, PRANAY SHARMA, Sijia Liu, et al. Model sparsity can simplify machine unlearning. Advances in Neural Information Processing Systems, 36, 2024.   \n[80] Sijia Liu, Yuanshun Yao, Jinghan Jia, Stephen Casper, Nathalie Baracaldo, Peter Hase, Yuguang Yao, Chris Yuhao Liu, Xiaojun Xu, Hang Li, Kush R. Varshney, Mohit Bansal, Sanmi Koyejo, and Yang Liu. Rethinking machine unlearning for large language models. arXiv preprint arXiv:2402.08787, 2024.   \n[81] Xiaodong Liu, Hao Cheng, Pengcheng He, Weizhu Chen, Yu Wang, Hoifung Poon, and Jianfeng Gao. Adversarial training for large neural language models. arXiv preprint arXiv:2004.08994, 2020.   \n[82] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.   \n[83] Zheyuan Liu, Guangyao Dou, Zhaoxuan Tan, Yijun Tian, and Meng Jiang. Towards safer large language models through machine unlearning. arXiv preprint arXiv:2402.10058, 2024.   \n[84] Zihan Liu, Wei Ping, Rajarshi Roy, Peng Xu, Chankyu Lee, Mohammad Shoeybi, and Bryan Catanzaro. Chatqa: Surpassing gpt-4 on conversational qa and rag. arXiv preprint arXiv:2401.10225, 2024.   \n[85] Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu, Max Tian, Denis Kocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu, Dmitry Abulkhanov, Indraneil Paul, Zhuang Li, Wen-Ding Li, Megan Risdal, Jia Li, Jian Zhu, Terry Yue Zhuo, Evgenii Zheltonozhskii, Nii Osae Osae Dade, Wenhao Yu, Lucas Krau\u00df, Naman Jain, Yixuan Su, Xuanli He, Manan Dey, Edoardo Abati, Yekun Chai, Niklas Muennighoff, Xiangru Tang, Muhtasham Oblokulov, Christopher Akiki, Marc Marone, Chenghao Mou, Mayank Mishra, Alex Gu, Binyuan Hui, Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas Patry, Canwen Xu, Julian McAuley, Han Hu, Torsten Scholak, Sebastien Paquet, Jennifer Robinson, Carolyn Jane Anderson, Nicolas Chapados, Mostofa Patwary, Nima Tajbakhsh, Yacine Jernite, Carlos Mu\u00f1oz Ferrandis, Lingming Zhang, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. Starcoder 2 and the stack v2: The next generation, 2024.   \n[86] Nils Lukas, Ahmed Salem, Robert Sim, Shruti Tople, Lukas Wutschitz, and Santiago ZanellaB\u00e9guelin. Analyzing leakage of personally identifiable information in language models. In 2023 IEEE Symposium on Security and Privacy $(S P)$ , pages 346\u2013363. IEEE, 2023.   \n[87] Yizhen Luo, Jiahuan Zhang, Siqi Fan, Kai Yang, Yushuai Wu, Mu Qiao, and Zaiqing Nie. Biomedgpt: Open multimodal generative pre-trained transformer for biomedicine. arXiv preprint arXiv:2308.09442, 2023.   \n[88] Aengus Lynch, Phillip Guo, Aidan Ewart, Stephen Casper, and Dylan Hadfield-Menell. Eight methods to evaluate robust unlearning in llms. arXiv preprint arXiv:2402.16835, 2024.   \n[89] Dakota Mahan, Ryan Carlow, Louis Castricato, Nathan Cooper, and Christian Laforte. Stable beluga models.   \n[90] Pratyush Maini, Zhili Feng, Avi Schwarzschild, Zachary C Lipton, and J Zico Kolter. Tofu: A task of fictitious unlearning for llms. arXiv preprint arXiv:2401.06121, 2024.   \n[91] Pratyush Maini, Hengrui Jia, Nicolas Papernot, and Adam Dziedzic. Llm dataset inference: Did you train on my dataset? arXiv preprint arXiv:2406.06443, 2024.   \n[92] Todor Markov, Chong Zhang, Sandhini Agarwal, Florentine Eloundou Nekoul, Theodore Lee, Steven Adler, Angela Jiang, and Lilian Weng. A holistic approach to undesired content detection in the real world. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 15009\u201315018, 2023.   \n[93] Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The sequential learning problem. In Psychology of learning and motivation, volume 24, pages 109\u2013165. Elsevier, 1989.   \n[94] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering. arXiv preprint arXiv:1809.02789, 2018.   \n[95] Niloofar Mireshghallah, Hyunwoo Kim, Xuhui Zhou, Yulia Tsvetkov, Maarten Sap, Reza Shokri, and Yejin Choi. Can llms keep a secret? testing privacy implications of language models via contextual integrity theory. arXiv preprint arXiv:2310.17884, 2023.   \n[96] Mayank Mishra, Matt Stallone, Gaoyuan Zhang, Yikang Shen, Aditya Prasad, Adriana Meza Soria, Michele Merler, Parameswaran Selvam, Saptha Surendran, Shivdeep Singh, et al. Granite code models: A family of open foundation models for code intelligence. arXiv preprint arXiv:2405.04324, 2024.   \n[97] Marius Mosbach, Maksym Andriushchenko, and Dietrich Klakow. On the stability of fine-tuning bert: Misconceptions, explanations, and strong baselines. arXiv preprint arXiv:2006.04884, 2020.   \n[98] Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. Orca: Progressive learning from complex explanation traces of gpt-4. arXiv preprint arXiv:2306.02707, 2023.   \n[99] Andrei Muresanu, Anvith Thudi, Michael R Zhang, and Nicolas Papernot. Unlearnable algorithms for in-context learning. arXiv preprint arXiv:2402.00751, 2024.   \n[100] Seth Neel and Peter Chang. Privacy issues in large language models: A survey. arXiv preprint arXiv:2312.06717, 2023.   \n[101] Thanh Tam Nguyen, Thanh Trung Huynh, Phi Le Nguyen, Alan Wee-Chung Liew, Hongzhi Yin, and Quoc Viet Hung Nguyen. A survey of machine unlearning. arXiv preprint arXiv:2209.02299, 2022.   \n[102] Shiwen Ni, Dingwei Chen, Chengming Li, Xiping Hu, Ruifeng Xu, and Min Yang. Forgetting before learning: Utilizing parametric arithmetic for knowledge updating in large language models. arXiv preprint arXiv:2311.08011, 2023.   \n[103] OpenAI. Introducing chatgpt, Nov 2022.   \n[104] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311\u2013318, 2002.   \n[105] Anselm Paulus, Arman Zharmagambetov, Chuan Guo, Brandon Amos, and Yuandong Tian. Advprompter: Fast adaptive adversarial prompting for llms. arXiv preprint arXiv:2404.16873, 2024.   \n[106] Martin Pawelczyk, Seth Neel, and Himabindu Lakkaraju. In-context unlearning: Language models as few shot unlearners. arXiv preprint arXiv:2310.07579, 2023.   \n[107] Nikhil Pinnaparaju, Reshinth Adithyan, Duy Phung, Jonathan Tow, James Baicoianu, Ashish Datta, Maksym Zhuravinskyi, Dakota Mahan, Marco Bellagente, Carlos Riquelme, et al. Stable code technical report. arXiv preprint arXiv:2404.01226, 2024.   \n[108] Matt Post. A call for clarity in reporting bleu scores. arXiv preprint arXiv:1804.08771, 2018.   \n[109] Xinchi Qiu, William F Shen, Yihong Chen, Nicola Cancedda, Pontus Stenetorp, and Nicholas D Lane. Pistol: Dataset compilation pipeline for structural unlearning of llms. arXiv preprint arXiv:2406.16810, 2024.   \n[110] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. arxiv 2023. arXiv preprint arXiv:2305.18290, 2023.   \n[111] Traian Rebedea, Razvan Dinu, Makesh Sreedhar, Christopher Parisien, and Jonathan Cohen. Nemo guardrails: A toolkit for controllable and safe llm applications with programmable rails. arXiv preprint arXiv:2310.10501, 2023.   \n[112] J.K. Rowling. Harry Potter and the Sorcerer\u2019s Stone. Scholastic, New York, 1997.   \n[113] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, J\u00e9r\u00e9my Rapin, et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023.   \n[114] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99\u2013106, 2021.   \n[115] Jonas B Sandbrink. Artificial intelligence and biological misuse: Differentiating risks of language models and biological design tools. arXiv preprint arXiv:2306.13952, 2023.   \n[116] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.   \n[117] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Commonsense reasoning about social interactions. arXiv preprint arXiv:1904.09728, 2019.   \n[118] Nazanin Mohammadi Sepahvand, Vincent Dumoulin, Eleni Triantaflilou, and Gintare Karolina Dziugaite. Data selection for transfer unlearning. arxiv 2024. arXiv preprint arXiv:2405.10425, 2024.   \n[119] Thanveer Shaik, Xiaohui Tao, Haoran Xie, Lin Li, Xiaofeng Zhu, and Qing Li. Exploring the landscape of machine unlearning: A survey and taxonomy. arXiv preprint arXiv:2305.06360, 2023.   \n[120] Yikang Shen, Zhen Guo, Tianle Cai, and Zengyi Qin. Jetmoe: Reaching llama2 performance with $0.1\\;\\mathrm{m}$ dollars. arXiv preprint arXiv:2404.07413, 2024.   \n[121] Toby Shevlane. Structured access: an emerging paradigm for safe ai deployment. arXiv preprint arXiv:2201.05159, 2022.   \n[122] Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi Chen, and Luke Zettlemoyer. Detecting pretraining data from large language models. arXiv preprint arXiv:2310.16789, 2023.   \n[123] Nianwen Si, Hao Zhang, Heyu Chang, Wenlin Zhang, Dan Qu, and Weiqiang Zhang. Knowledge unlearning for llms: Tasks, methods, and challenges. arXiv preprint arXiv:2311.15766, 2023.   \n[124] James C Spall. Multivariate stochastic approximation using a simultaneous perturbation gradient approximation. IEEE transactions on automatic control, 37(3):332\u2013341, 1992.   \n[125] James C Spall. Introduction to stochastic search and optimization: estimation, simulation, and control. John Wiley & Sons, 2005.   \n[126] Robin Staab, Mark Vero, Mislav Balunovi\u00b4c, and Martin Vechev. Beyond memorization: Violating privacy via inference with large language models. arXiv preprint arXiv:2310.07298, 2023.   \n[127] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: A question answering challenge targeting commonsense knowledge. arXiv preprint arXiv:1811.00937, 2018.   \n[128] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivi\u00e8re, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024.   \n[129] Pratiksha Thaker, Yash Maurya, and Virginia Smith. Guardrail baselines for unlearning in llms. arXiv preprint arXiv:2403.03329, 2024.   \n[130] Anvith Thudi, Gabriel Deza, Varun Chandrasekaran, and Nicolas Papernot. Unrolling sgd: Understanding factors influencing machine unlearning. In 2022 IEEE 7th European Symposium on Security and Privacy (EuroS&P), pages 303\u2013319. IEEE, 2022.   \n[131] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \n[132] Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Cl\u00e9mentine Fourrier, Nathan Habib, et al. Zephyr: Direct distillation of lm alignment. arXiv preprint arXiv:2310.16944, 2023.   \n[133] Lewis Tunstall and Philipp Schmid. Zephyr 7b gemma. https://huggingface.co/ HuggingFaceH4/zephyr-7b-gemma-v0.1, 2024.   \n[134] Vladimir Vovk, Alexander Gammerman, and Glenn Shafer. Algorithmic learning in a random world, volume 29. Springer, 2005.   \n[135] Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax, May 2021.   \n[136] Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, and Yang Liu. Openchat: Advancing open-source language models with mixed-quality data. arXiv preprint arXiv:2309.11235, 2023.   \n[137] Lingzhi Wang, Tong Chen, Wei Yuan, Xingshan Zeng, Kam-Fai Wong, and Hongzhi Yin. Kga: A general machine unlearning framework based on knowledge gap alignment. arXiv preprint arXiv:2305.06535, 2023.   \n[138] Yanchen Wang and Lisa Singh. Adding guardrails to advanced chatbots. arXiv preprint arXiv:2306.07500, 2023.   \n[139] Xinwei Wu, Junzhuo Li, Minghui Xu, Weilong Dong, Shuangzhi Wu, Chao Bian, and Deyi Xiong. Depn: Detecting and editing privacy neurons in pretrained language models. arXiv preprint arXiv:2310.20138, 2023.   \n[140] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244, 2023.   \n[141] Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, et al. Baichuan 2: Open large-scale language models. arXiv preprint arXiv:2309.10305, 2023.   \n[142] Jin Yao, Eli Chien, Minxin Du, Xinyao Niu, Tianhao Wang, Zezhou Cheng, and Xiang Yue. Machine unlearning of pre-trained large language models. arXiv preprint arXiv:2402.15159, 2024.   \n[143] Yuanshun Yao, Xiaojun Xu, and Yang Liu. Large language model unlearning. arXiv preprint arXiv:2310.10683, 2023.   \n[144] Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, et al. Yi: Open foundation models by 01. ai. arXiv preprint arXiv:2403.04652, 2024.   \n[145] Zhuowen Yuan, Zidi Xiong, Yi Zeng, Ning Yu, Ruoxi Jia, Dawn Song, and Bo Li. Rigorllm: Resilient guardrails for large language models against undesired content. arXiv preprint arXiv:2403.13031, 2024.   \n[146] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019.   \n[147] Di Zhang, Wei Liu, Qian Tan, Jingdan Chen, Hang Yan, Yuliang Yan, Jiatong Li, Weiran Huang, Xiangyu Yue, Dongzhan Zhou, Shufei Zhang, Mao Su, Hansen Zhong, Yuqiang Li, and Wanli Ouyang. Chemllm: A chemical large language model, 2024.   \n[148] Jinghan Zhang, Shiqi Chen, Junteng Liu, and Junxian He. Composing parameter-efficient modules with arithmetic operations. arXiv preprint arXiv:2306.14870, 2023.   \n[149] Ruiqi Zhang, Licong Lin, Yu Bai, and Song Mei. Negative preference optimization: From catastrophic collapse to effective unlearning. arXiv preprint arXiv:2404.05868, 2024.   \n[150] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.   \n[151] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675, 2019.   \n[152] Zihan Zhao, Da Ma, Lu Chen, Liangtai Sun, Zihao Li, Hongshen Xu, Zichen Zhu, Su Zhu, Shuai Fan, Guodong Shen, Xin Chen, and Kai Yu. Chemdfm: Dialogue foundation model for chemistry, 2024.   \n[153] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36, 2024.   \n[154] Banghua Zhu, Evan Frick, Tianhao Wu, Hanlin Zhu, Karthik Ganesan, Wei-Lin Chiang, Jian Zhang, and Jiantao Jiao. Starling-7b: Improving llm helpfulness & harmlessness with rlaif, November 2023.   \n[155] Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models. arXiv preprint arXiv:2307.15043, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "A Broader Impact ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The proposed method, Embedding-COrrupted (ECO) Prompts, offers a novel framework for unlearning in large language models (LLMs), addressing the crucial challenge of removing sensitive or harmful knowledge while maintaining model integrity. As LLMs become more embedded in various applications, ensuring that they can unlearn specific information is paramount for compliance with data privacy regulations such as GDPR and for mitigating potential misuse. However, our work also has broader implications that merit careful consideration. ", "page_idx": 20}, {"type": "text", "text": "Firstly, the unlearning capability, while beneficial for privacy and safety, could be misused to selectively remove critical information, potentially leading to misinformation or biased outputs. For instance, model providers might exploit this technology to erase inconvenient facts from models deployed in public-facing applications, thereby manipulating the information accessible to users. To mitigate such risks, robust auditing mechanisms and transparency in the application of unlearning techniques are essential. Secondly, while ECO Prompts are designed to safeguard against specific threats such as entity leaking and hazardous knowledge dissemination, their effectiveness depends on the accuracy of the initial threat identification. Incorrect or incomplete identification could either fail to remove all relevant knowledge or inadvertently degrade the model\u2019s performance on non-sensitive tasks. Continuous monitoring and refinement of the classifier used for identifying unlearning targets, alongside comprehensive evaluation protocols, are necessary to minimize these potential harms. ", "page_idx": 20}, {"type": "text", "text": "B Limitations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "One limitation of ECO is that it supports unlearning only for models with API access, as it relies on the classifier to identify the unlearning target and the corruption function to achieve unlearning. If an adversary has open-weight access to a model, they could circumvent the unlearning state by bypassing the classifier. ", "page_idx": 20}, {"type": "text", "text": "Secondly, as described in Section 2.1, our approach does not address the threat posed by motivated adversaries who may attempt to compromise the classifier or the LLM itself. To counter such threats, practitioners might consider training the prompt classifier adversarially [81, 66, 35] to enhance its robustness against attacks, even if the attacker is aware of the classifier\u2019s presence and architecture. ", "page_idx": 20}, {"type": "text", "text": "Third, the prompt classifier\u2019s context window is typically limited, examining only the first (or last) $K$ tokens by default. An attacker aware of this limitation could manipulate the prompt by injecting neutral text at both the beginning and the end to bypass the classifier. However, this vulnerability can be mitigated by implementing a sliding window technique: if the prompt\u2019s length exceeds the context window, the prompt should be considered positive as long as one of the text spans is predicted as positive. These limitations underscore the need for future work to improve the classifier\u2019s mechanism, potentially integrating it directly into the LLM itself. ", "page_idx": 20}, {"type": "text", "text": "C Detailed Experimental Setup ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, we introduce our experimental setup, including a detailed description of all evaluation metrics (Appendix C.1), preparation of LLM subject to unlearning (Appendix C.2), training and evaluation of the prompt classifier (Appendix C.3), and formulations of all the baseline methods (Appendix C.4). ", "page_idx": 20}, {"type": "text", "text": "C.1 Evaluation Metrics ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "C.1.1 TOFU ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We employ the original evaluation metrics designed by the authors of the TOFU dataset [90]. ", "page_idx": 20}, {"type": "text", "text": "Answer probability For every single instance in the retain set or the forget set, we compute the normalized conditional probability $\\bar{P}(a\\mid q)^{1/|a|}$ on the LLM subject to unlearning, where $q$ and $a$ correspond to the question and answer, and $|a|$ represents the number of tokens in the answer. For the real authors and world facts subsets, the dataset provides a set of five answers $\\{a_{0},\\tilde{a}_{1},\\tilde{a}_{2},\\tilde{a}_{3},\\tilde{a}_{4}\\}$ , which consists of a single correct answer $a_{0}$ and four other perturbed answers that are incorrect. In this case, we compute the ratio $\\begin{array}{r}{P(a_{0}\\mid q)^{1/|a_{0}|}/\\sum_{i=1}^{4}P(\\tilde{a}_{i}\\mid q)^{1/|\\tilde{a}_{i}|}}\\end{array}$ . ", "page_idx": 20}, {"type": "text", "text": "Truth ratio The truth ratio is computed as the geometric mean8 of multiple perturbed (incorrect) answers\u2019 $(\\mathcal{A}=\\{\\tilde{a}_{1},\\tilde{a}_{2},\\ldots\\})$ probabilities over the normalized conditional probability of the paraphrased answer $\\hat{a}$ . ", "page_idx": 21}, {"type": "equation", "text": "$$\nR_{\\mathrm{truth}}=\\frac{\\left(\\prod_{i=1}^{|\\mathcal{A}|}P(\\tilde{a}_{i}\\mid q)^{1/|\\tilde{a}_{i}|}\\right)^{1/|\\mathcal{A}|}}{P(\\hat{a}\\mid q)^{1/|\\hat{a}|}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For the real authors and world fact subsets, the original answer $a$ is used in the denominator as no paraphrased answer is available. ", "page_idx": 21}, {"type": "text", "text": "ROUGE-L For all subsets of TOFU, we compute the ROUGE- $.\\mathrm{L}$ recall score [77] between the ground truth responses (provided by the dataset) and the text generated by the model after unlearning. ", "page_idx": 21}, {"type": "text", "text": "Model utility The model utility is aggregated as a harmonic mean over nine numbers: the answer probability, truth ratio, and ROUGE recall scores from each of the retain, real authors, and world facts subsets. A higher model utility is always preferred. ", "page_idx": 21}, {"type": "text", "text": "Forget quality The forget quality is computed as the p-value of performing a Kolmogorov-Smirnov (KS) test using two distributions: the truth ratio of the retained model on the forget set, and the truth ratio of the unlearned model on the forget set. A higher p-value under the KS test indicates failure to reject the null hypothesis that the distributions of truth ratio from the retained and the unlearned models are the same, which is a sign of indistinguishability between the retained model\u2019s behavior and the unlearned model\u2019s behavior. ", "page_idx": 21}, {"type": "text", "text": "C.1.2 WMDP and MMLU ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Multiple-choice accuracy For both the WDMP [73] and MMLU subsets [55] unlearning, we employ multiple-choice accuracy as the primary evaluation metric. The underlying assumption is that a model unlearned on the target subject should demonstrate random-guessing accuracy on the task. Since both WDMP and MMLU consist of four-option multiple-choice questions, an accuracy close to 0.25 indicates successful unlearning. For each question, we adhere to the approach outlined by [73] and utilize the template provided in Listing 1 in a zero-shot manner. To derive the answer predicted by the LLM, we extract the logit scores corresponding to tokens [A, B, C, D] from the logits of the last token in the input sequence. The option with the highest logit score is deemed the predicted answer. ", "page_idx": 21}, {"type": "image", "img_path": "e5icsXBD8Q/tmp/d825c8cc36b271dc9d703afaed21b3fa991aebba113b25a1029e958fb737326d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Listing 1: The formatting template for WMDP and MMLU multiple-choice questions used in both the classifier training and the main LLM for prediction. ", "page_idx": 21}, {"type": "text", "text": "Probing We also incorporate a probing evaluation, as done in [73], which trains a four-way linear probe on half of the data points from the biology, chemistry, and cybersecurity subsets. Specifically, a linear classifier is trained to predict the correct answer to the multiple-choice question based on the unlearned model\u2019s output logits. The trained linear probe is then used to make predictions on the other half of the data points to infer the correct labels given the output logits. Successful unlearning should result in random-chance accuracy for the linear probe, which is 0.25 in our case. ", "page_idx": 21}, {"type": "text", "text": "C.1.3 Harry Potter Book and BBC News Articles ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We employ four text similarity metrics outlined below. For each metric, we use the original text (from the copyrighted material) as the reference and compute the similarity between the reference and the text generated by the LLM. A retained model that has never been trained on the reference text should have low similarity scores on all metrics, and a successfully unlearned model should have scores similar to those of the retained model. For both datasets, we evaluate similarity based on the first 256 tokens generated. This aligns with our fine-tuning setup in Appendix C.2. ", "page_idx": 22}, {"type": "text", "text": "ROUGE-L We utilize the ROUGE-L algorithm as described in Appendix C.1.1. ROUGE-L\u2019s recall score denotes the proportion of the longest common subsequence in the reference text that appears in the generated text by the unlearned model. Essentially, it gauges the frequency at which the unlearned model can generate long text spans that exist in the copyrighted content. ", "page_idx": 22}, {"type": "text", "text": "SacreBLEU [143] employs the BLEU score [104], which is predicated on $n$ -gram precision, to determine if the copyrighted content has been inadvertently disclosed, using a predefined threshold. We adopt SacreBLEU [108], which standardizes tokenization to mitigate variability in preprocessing. SacreBLEU assesses the overlap of $n$ -grams between the generated and reference texts, subsequently calculating the number of matching $n$ -grams as a precision score. ", "page_idx": 22}, {"type": "text", "text": "BERTScore BERTScore [151] employs contextual embeddings of tokens from both the reference and generated texts, performing greedy matching based on pairwise similarity of all token pairs. We utilize the F1 score, as recommended by the original authors, and employ the DistilBERT [116] checkpoint to obtain these contextual embeddings. ", "page_idx": 22}, {"type": "text", "text": "METEOR We also employ METEOR [9], which incorporates unigram precision, unigram recall, and word order to provide a more nuanced similarity measure than BLEU and ROUGE-L. ", "page_idx": 22}, {"type": "text", "text": "Average similarity gap (ASG) We incorporate an aggregated metric, the average gap [79, 38], as the average absolute difference over the four similarity metrics above, computed between the retained model and the unlearned model. The average gap measures how similar an unlearned model\u2019s outputs are to the retained model\u2019s outputs, and a smaller gap is more desirable. ", "page_idx": 22}, {"type": "text", "text": "Perplexity (PPL) and unique token ratio Following [143], we use the perplexity score and the unique token ratio measured on the generated text to assess the fluency and diversity of the generated text. The perplexity is calculated by a reference model that has been fine-tuned on the target copyrighted content material. A sufficiently low perplexity indicates that the generated text might still be meaningful. The unique token ratio is calculated as the number of unique token set over all tokens generated in the outputs. ", "page_idx": 22}, {"type": "text", "text": "C.1.4 Why Not Membership Inference Attacks (MIAs)? ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this paper, we follow most prior work on LLM unlearning, which generally does not use membership inference attack (MIA) methods to evaluate the effectiveness of unlearning for LLMs [60, 68, 56, 23, 36, 90, 143, 142, 83, 73, 149, 57, 62]. ", "page_idx": 22}, {"type": "text", "text": "We do not consider MIA methods to evaluate our models for three major reasons. First, state-ofthe-art MIAs require training multiple (up to hundreds) shadow models [22] on subsets of the entire training set, which is not feasible in the LLM setting, as it requires access to the pre-training data or fine-tuning a large number of models on subsets of the fine-tuning data. MIAs without training shadow models have been demonstrated to overestimate the effectiveness of unlearning [54] due to the non-uniform difficulty of learning/unlearning each sample. ", "page_idx": 22}, {"type": "text", "text": "Second, evidence suggests that existing MIAs for LLMs, even the state-of-the-art ones [122], generally barely perform better than random guessing due to both training on large pre-training datasets for a small number of iterations and the fuzzy boundary between members and non-members [33]. In addition, recent work [91] shows that Min- ${\\bf K}\\%$ Prob leads to: 1) significant variance based on the random selection of datasets used for evaluation, 2) improved results when the two subsets (in this case, the forget and holdout sets) are drawn from different distributions, and 3) an empirical overestimation of false positives. This last point indicates that the distribution gap (such as a temporal shift, also noted by [33]) introduces a confounding variable in the discrimination process, as the forget and holdout sets might vary in more than one aspect. ", "page_idx": 22}, {"type": "text", "text": "Third, as stated in Section 2.2, we do not consider the privacy aspect of unlearning in this work, and our threat model does not include privacy risks. Knowing whether a single sample is a member also does not significantly increase the risk in our threat model. ", "page_idx": 23}, {"type": "text", "text": "Additionally, performing such MIAs typically requires at least the model\u2019s internal states [33] (e.g., activations), which are not within the scope of our threat model (i.e., only text output and logits). ", "page_idx": 23}, {"type": "text", "text": "In fact, the forget quality metric described in Appendix C.1.1 and the probing evaluation in Appendix C.1.2 align with the goal of MIAs. The forget quality assesses whether the forget set distributions on the unlearned model and the retained models can be distinguished. The linear probe tries to infer the correct answers from the model output, assuming that the accuracy of the linear probe on a retained model is at the random-guessing level. Achieving the same accuracy might imply indistinguishability. ", "page_idx": 23}, {"type": "text", "text": "C.2 Preparing LLMs for Unlearning ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this subsection, we describe the setup for preparing the LLMs subject to unlearning for each dataset. ", "page_idx": 23}, {"type": "text", "text": "TOFU We use the original code9 provided alongside the TOFU dataset [90] for fine-tuning to ensure consistency. Following their experimental setup, we fine-tune two models, Phi-1.5 [74] and Llama-2-7B-Chat [131], on the entire TOFU dataset to obtain the model to be subjected to unlearning. Following the retain/forget splits provided in the dataset, we fine-tune each model on each of the three different splits\u2014 $99\\%$ , $95\\%$ , and $90\\%$ of the full dataset, excluding the forget data, to obtain the retained models. These three splits also correspond to unlearning $1\\%$ , $5\\%$ , and $10\\%$ of the samples, respectively. We employ the same hyperparameters as provided in both the paper and the accompanying code. Both models are trained with a batch size of 4, accumulating gradients for 4 steps on 2 NVIDIA A6000 GPUs, resulting in an effective batch size of 32, with a learning rate of 1e-5 for Llama-2-7B-Chat and 2e-5 for Phi-1.5. For the negative preference optimization [149] baselines, we follow a similar procedure and use the code provided by the original authors10. ", "page_idx": 23}, {"type": "text", "text": "WMDP and MMLU subsets The knowledge assessment of all multiple-choice questions in WMDP [73] and MMLU [55] subsets is performed directly on the pre-trained models (or models unlearned from the pre-trained checkpoints for unlearning evaluation). Therefore, we do not fine-tune models based on the multiple-choice questions for the WMDP unlearning task. ", "page_idx": 23}, {"type": "text", "text": "Copyrighted content For the copyrighted content unlearning task, we first verify that all the considered LLMs cannot generate the original corpus. For the HP Book, while some parts of the text corpus could potentially be included during pretraining, we see little sign of generating the text spans verbatim for all models we considered in the copyrighted content unlearning experiments. This is also reflected in the low similarity scores from tables in Appendix E.5. For BBC News articles, we only consider articles published in February 2024, which is beyond the knowledge cutoff of most models considered. We fine-tune them on the copyrighted content corpus to ensure that they are able to generate the original passage. For the HP book, we split the text into chunks of up to 256 tokens (based on the tokenization scheme used for the specific model). For BBC News articles, we concatenate the news title with the news content, with a single space in the middle. The title of the news article is used as the prompt for generation. To ensure that our models can indeed generate the copyrighted content, we fine-tune all models on the two text corpora for 5 epochs, using a batch size of 4 and a learning rate of 2e-5 on two NVIDIA A100 GPUs. ", "page_idx": 23}, {"type": "text", "text": "C.3 Prompt Classifiers ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this subsection, we describe how the dataset for the prompt classifiers are prepared and the setup and hyperparameters used to train the prompt classifiers. We include the dataset split statistics in Table 4. We also report the performance of three prompt classifiers in Tables 5 to 7, corresponding to the original classifier, simple-thresholding classifier, and conformal prediction classfiier. ", "page_idx": 23}, {"type": "table", "img_path": "e5icsXBD8Q/tmp/9ce1edfb4ab302bf468a8a762ed960a7520fe194c5c85c893a3e6b8606d96f81.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "e5icsXBD8Q/tmp/98edfc9701f67ef6671908f4bd68af66ca92f01f96d83c53a58828620480b6fc.jpg", "table_caption": ["Table 4: The statistics of the dataset (splits) used to train the prompt classifiers. $D_{f}$ and $D_{r}$ denote the forget and retain sets. $D_{g}$ (outlined in Table 9) refers to the general set for evaluating general utility. "], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "e5icsXBD8Q/tmp/93a8888297d37425fb593e7fd957e869c6d3b78b03b17926386213a505fe21f1.jpg", "table_caption": ["Table 5: The false negative rate (FNR) and false positive rate (FPR) of the prompt classifiers without thresholding. If the FNR of $D_{f}^{\\mathrm{Test}}$ is not reported, it means that the corresponding unlearning target does not require generalization outside the scope of the forget set. The $D_{g}$ set contains out-ofdistribution prompts from eleven NLP benchmarks listed in Table 9. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "Table 6: The false negative rate (FNR) and false positive rate (FPR) of the prompt classifiers on the corresponding data subsets. If the FNR of $D_{f}^{\\mathrm{Test}}$ is not reported, it means that the corresponding unlearning target does not require generalization outside the scope of the forget set. The $D_{g}$ set contains out-of-distribution prompts from eleven NLP benchmarks listed in Table 9. The error rate above is calculated using the calibrated decision threshold $\\tau$ . ", "page_idx": 24}, {"type": "table", "img_path": "e5icsXBD8Q/tmp/329f7721e3e527013fcd47cc7c480b3972951acc3ccff8eeb7246924d73790c0.jpg", "table_caption": [], "table_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "e5icsXBD8Q/tmp/cdb665ff4815f8a7ca5cdd56edb903661f330240f849a09c8abe8196e5303067.jpg", "table_caption": ["Table 7: The false negative rate (FNR) and false positive rate (FPR) of the prompt classifiers on the corresponding data subsets with conformal prediction. For uncertain predictions with a prediction size of two, we behave conservatively and treat them as positive. The performance is slightly worse than the simple thresholding in Table 6, due to the cost of counting all uncertain predictions as positive samples. ", "Table 8: False positive and false negative of the MMLU classifiers on highly-related subjects. "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "C.3.1 Prompt Classifiers\u2019 Training Datasets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "TOFU We strictly follow the original split of the forget and retain sets in the TOFU dataset [90] to train the classifiers. To access the false positive predictions, we use the real authors and world facts splits to evaluate the classifier after hyperparameter tuning is completed. Here, we do not use a test forget set for the entities, following the practice in the original paper. ", "page_idx": 25}, {"type": "text", "text": "WMDP We train a single classifier to classify multiple-choice questions in all three subjects of the WMDP dataset (i.e., biology, chemistry, and cybersecurity). We format the question strictly following the original evaluation [73] using the Language Model Evaluation Harness [43] style in Listing 1, where the {subject}, {question}, and $\\{\\mathtt{c h o i c e_{-}\\#}\\}$ fields are replaced by the actual text for each multiple-choice question. This template is also used as the prompt template for the main LLM to make predictions. For negative samples, we use a combination of the auxiliary training set and the development set of MMLU [55]. We do so because the questions in the auxiliary training set do not have subjects, which might result in a shortcut learned by the classifier. We use the samples in the development set (with subjects) to mitigate that shortcut. Note that MMLU has its own validation set, so using the development set (with only 285 samples) is a reasonable choice. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "Unlike TOFU and other copyrighted content datasets, we require the classifier trained on WMDP questions to generalize to unseen questions in relevant domains while not flagging relevant questions in similar domains (e.g., virology, high school chemistry, and computer security in MMLU) as positive. To avoid fully relying on the WMDP questions to train the classifier, we restrict ourselves to access only $10\\%$ of the WMDP questions, selected randomly before training and development. We also subsample a fixed set of 3K samples (out of 99.8K) from the auxiliary training set of MMLU to train and test the false positive rate of the prompt classifier. Note that the RMU method proposed in [73] does not require access to any of the questions. ", "page_idx": 26}, {"type": "text", "text": "We incorporate a setting where we train a prompt classifier on only synthetic data. The synthetic dataset contains 300 questions (100 for each of biology, chemistry, and cybersecurity), all generated by GPT-4 [2] to resemble the style and difficulty of real WMDP questions. This aligns with RMU\u2019s setup, which does not access real questions during model development. For a detailed split, please see Table 4. The performance of both prompt classifiers is reported in Table 6. We observe that the WMDP prompt classifier trained on synthetic data performs almost identically to one trained on $10\\%$ of the real questions. However, the authors of [129] identified that our initial classifiers overfit to the topic line. Specifically, since the topic line (listing 1) The following are multiple-choice questions (with answers) about {subject} appeared in the training data, the classifiers learned to classify forget and retain classes solely based on $\\{\\mathsf{s u b j e c t}\\}$ , rather than the question content. To address this, we used the official WMDP training corpora [73] to generate 3,000 synthetic problems in a style similar to the actual problems, using Llama-3.1-405B-Instruct [34]. We also included synthetic retain problems using the development set of MMLU for data generation. Including such data significantly reduced the false positive rate. We then fine-tuned a Llama-Guard-3-1B [34, 59] as the classifier, excluding the topic line during training. ", "page_idx": 26}, {"type": "text", "text": "In Tables 5 to 7, we also show an out-of-distribution (o.o.d.) setup where we assume access to only 100 biology questions, 100 cybersecurity questions, and 2 chemistry questions. In this scenario, the false negative rates of both the original and simple-threshold classifiers are high due to insufficient training samples for chemistry. In Table 7, we demonstrate that conformal prediction alleviates this issue by including more uncertain samples as negative, reducing the false negative rate by approximately $14\\%$ . ", "page_idx": 26}, {"type": "text", "text": "MMLU subsets We train a separate classifier for each unlearning category: economics, physics, and law. This is because, for each task, we aim to unlearn only the selected category while retaining the rest of the categories as defined in [73], making a single prompt classifier infeasible. Similar to the WMDP setup, we restrict ourselves to the MMLU development set, which contains only 10\u201315 samples for the forget target. We follow the same procedure used in the WMDP dataset and use the development set to generate synthetic forget and retain data to train the classifiers. To avoid overfitting, we also removed the topic line from all training samples. We report the detailed false positive and false negative statistics on the forget subject and related retain subjects in table 8. ", "page_idx": 26}, {"type": "text", "text": "HP Book Since the goal is to prevent users from extracting copyrighted content through training, we purchased the Harry Potter and the Sorcerer\u2019s Stone [112] ebook and extracted the corpus to train our HP Book prompt classifier. We split the book into sentences using spaCy\u2019s sentencizer11 selecting only sentences with more than ten characters. Sentences with ten or fewer characters are mostly neutral sentences, line breaks, whitespace, or punctuation. The remaining sentences are treated as positive samples. For negative samples, we use the BookMIA dataset12 [122], which contains over 9K text snippets from various real books. Snippets from Harry Potter and the Sorcerer\u2019s Stone in the BookMIA dataset are removed before training. As generalization is not required, we do not use a test forget set. We split the retain set (i.e., BookMIA) into two equal-sized sets for training and testing the classifier\u2019s performance. In Table 6, we find that the classifier misclassifies some samples, but manual examination reveals they are mostly neutral sentences. ", "page_idx": 26}, {"type": "table", "img_path": "e5icsXBD8Q/tmp/965b19deb6bbe9b310c913a1431777f5298c12dfef22658da97fd1e7e851f4f7.jpg", "table_caption": [], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "Table 9: A list of common LLM benchmark datasets. We use these datasets collectively as $D_{\\mathrm{g}}$ , the out-of-distribution general set, to evaluate the general utility of the unlearned models beyond the forgetting and retain distributions. ", "page_idx": 27}, {"type": "text", "text": "BBC News We use BBC News articles13 published in February 2024 as positive samples and over 9K news articles in English from the CC-News dataset [52] as negative samples. To prevent shortcuts, we format both datasets consistently and remove the \u201c - BBC ###\u201d suffix from the titles in the BBC News dataset. The prompt classifier is trained only on the titles of the news articles. To mitigate sophisticated extraction attacks, one could train the classifier using sentence-level splits, similar to the HP Book dataset. However, due to the length of full news articles, we focus solely on title-based classification. ", "page_idx": 27}, {"type": "text", "text": "A comprehensive evaluation of general utility Most prior work only evaluates the retain ability of the unlearned LLM using the retain set associated with the unlearning task. However, the results reported on the retain set might not fully reflect general utility in real-world settings. This is because the retain set, while being disjoint from the forget set, might still share a similar distribution with the forget set in some aspects. Therefore, instead of solely relying on the regular retain set, we consider a large set of out-of-distribution samples to measure general utility. In the general set, we include eleven common LLM benchmarks listed in Table 9: MMLU [55], ARC-Easy [27], ARC-Challenge [27], OpenBookQA [94], HellaSwag [146], Winogrande [114], TruthfulQA [78], CommonsenseQA [127], PIQA [15], SocialIQA [117], and BoolQ [26]. These benchmarks amount to a total of 41,297 samples. We evaluate all prompt classifiers (from all datasets mentioned above) on the general set after tuning the parameters of the prompt classifiers. For all datasets, we use the test set if the labels are publicly available; otherwise, we use the validation set. For TruthfulQA, we use the MC1 subset for evaluation. ", "page_idx": 27}, {"type": "text", "text": "C.3.2 Training a prompt classifier ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "For all prompt classifiers used for prompt content detection, we choose either RoBERTa-base [82] or Llama-3.1-1B-Instruct [34] as the base model for fine-tuning. The hyperparameters are selected following prior work that improves stability during training [97]. Since in most cases the number of positive samples (the forget samples) is much smaller than the number of negative samples, we reweight the class-wise losses using the inverse frequency. Once the optimal number of epochs is determined, we fine-tune the model again on the combined training and validation sets and use it as the final prompt classifier for inference-time unlearning. We also evaluate all prompt classifiers on the general set outlined in Table 9. ", "page_idx": 27}, {"type": "text", "text": "We report the performance of the original prompt classifier, the classifier with simple thresholding, and the classifier with conformal prediction in Tables 5 to 7. We demonstrate that in all settings considered, our best prompt classifiers achieve satisfying performance, reflected by the low false negative rate on the forget set and the low false positive rate on the retain set. On the general set (i.e., the suite of NLP benchmarks), most prompt classifiers have zero false positive predictions, suggesting that the performance of the main LLM on samples irrelevant to the forget set is unlikely to be affected. ", "page_idx": 27}, {"type": "table", "img_path": "e5icsXBD8Q/tmp/ed785d46d8ecf24f704f98b8d3251e8600027b04a00469c3285649fe0d8551ba.jpg", "table_caption": ["C.3.3 Robustness Against Out-of-Distribution Prompts "], "table_footnote": [], "page_idx": 28}, {"type": "table", "img_path": "e5icsXBD8Q/tmp/a5b34561283e29c519c25547bb63d3bf3d7b05c9b2447595214d2d29afa93c6f.jpg", "table_caption": ["Table 10: Original TOFU classifier\u2019s false positive and false negative rates for different types of o.o.d. prompts. "], "table_footnote": ["(c) Copyrighted content (HP book) "], "page_idx": 28}, {"type": "text", "text": "Table 11: Prompt classifiers\u2019 false positive and false negative rates for different types of o.o.d. prompts after being trained on synthetically generated o.o.d. data. The evaluation is still conducted on human-generated o.o.d. data. ", "page_idx": 28}, {"type": "text", "text": "In the main body of the paper, we only considered in-distribution prompts from users. However, this assumption is not entirely realistic in practice, as user behaviors are dynamic, and potential attackers could exploit flaws in prompt classifiers to bypass guardrails. Given that the default prompt classifiers we use assume in-distribution prompts and are not trained to identify out-of-distribution (o.o.d.) or jailbreak prompts, we first 1) study the fragility of these classifiers against various types of o.o.d. prompts, and then 2) assess whether the classifiers\u2019 performance improves when trained on these types of prompts. ", "page_idx": 28}, {"type": "text", "text": "Original classifiers remain robust under distribution shift. We consider challenging queries written by humans, including rephrased prompts, adversarial prompts, prompts with irrelevant context, jailbreak prefixes/suffixes, and keyword-only prompts (based on the original prompts). To evaluate, we construct a test set of such prompts by rewriting the prompts from the forget set. Surprisingly, when we evaluate the original TOFU classifier on this test set, it still maintains high false positive and false negative rates, as shown in table 10. This confirms that the classifier\u2019s performance, as reported in table 6, is not due to overfitting. Based on these results, we believe that our classifiers remain robust against common perturbations, even without explicit training on such data. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "Training on additional o.o.d. data further improves robustness. Next, we investigate whether we can enhance the robustness of our classifiers. We construct another set of perturbed prompts, synthetically generated by Llama-3.1-70B-Instruct [3], with no overlap with the previous perturbed test set and no prior knowledge about the perturbation types. Specifically, we prompt Llama-3.1- 70B-Instruct to generate a more challenging version of the prompt that could potentially deceive a well-performing classifier. We use this set of prompts to train our classifier and observe that both the false positive rate and false negative rate are further improved (table 11). We repeat this procedure for all classifiers to ensure our findings generalize. ", "page_idx": 29}, {"type": "text", "text": "C.4 Baseline Methods ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "In this section, we formulate all baseline methods used throughout the paper. ", "page_idx": 29}, {"type": "text", "text": "C.4.1 Prompting Baseline ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "For all prompting baselines, we adapt the prompt style from [88]. Below, we detail the prompts used in the TOFU dataset as well as those for the WMDP benchmark and MMLU subsets. For the TOFU prompt, we list the author names subject to forgetting. For the WMDP and MMLU prompts, we replace the {subject} field with the specific subject to unlearn (e.g., biology). ", "page_idx": 29}, {"type": "text", "text": "Note that one could potentially craft more sophisticated prompts to achieve improved unlearning and utility. However, as this is not the focus of the current study, we leave such developments to future work. ", "page_idx": 29}, {"type": "image", "img_path": "e5icsXBD8Q/tmp/bb5cb7b95db09485a7627c46e0bb95cbae2621cf8397a55f934e31836c884773.jpg", "img_caption": [], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "C.4.2 Optimization-Based Unlearning Baselines ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "For all optimization-based baselines below, we fix the number of epochs as 5 and tune the learning rate of {5e-5, 2e-5, 1e-5}. ", "page_idx": 29}, {"type": "text", "text": "Fine-tuning, gradient ascent (GA), and gradient difference (GD) Fine-tuning, gradient ascent, and gradient difference are simple baselines commonly used in traditional machine unlearning settings [24, 61, 38, 69], and has been introduced as simple baseline methods in [90]. Fine-tuning only involves performing gradient descent on $D_{r}$ , while gradient ascent performs gradient descent on $D_{f}$ in the opposite direction. Gradient difference combines fine-tuning and gradient ascent by compute the sum of the two loss terms. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "equation", "text": "$$\n{\\begin{array}{l}{L_{{\\mathrm{Fine-tune}}}={\\cfrac{1}{\\left|D_{r}\\right|}}\\displaystyle\\sum_{\\mathbf{x}\\in D_{r}}{\\mathcal{L}}(\\mathbf{x};\\theta)}\\\\ {L_{{\\mathrm{GA}}}=-{\\cfrac{1}{\\left|D_{f}\\right|}}\\displaystyle\\sum_{\\mathbf{x}\\in D_{f}}{\\mathcal{L}}(\\mathbf{x};\\theta)}\\\\ {L_{{\\mathrm{GD}}}={\\cfrac{1}{\\left|D_{r}\\right|}}\\displaystyle\\sum_{\\mathbf{x}\\in D_{r}}{\\mathcal{L}}(\\mathbf{x};\\theta)-{\\cfrac{1}{\\left|D_{f}\\right|}}\\displaystyle\\sum_{\\mathbf{x}\\in D_{f}}{\\mathcal{L}}(\\mathbf{x};\\theta)}\\end{array}}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "KL minimization (KL) The KL minimization is adopted from [90] and involves a gradient ascent term for forgetting as well. It also minimizes the KL distance on $D_{r}$ between the current model and the original model $\\theta_{o}$ . The KL minimization term aims to keep the model\u2019s current output distribution on the retained set close to its pre-unlearning distribution on the retain samples. ", "page_idx": 30}, {"type": "equation", "text": "$$\nL_{\\mathrm{KL}}=L_{\\mathrm{GA}}+\\frac{1}{|D_{r}|}\\sum_{\\mathbf{x}\\in D_{r}}\\mathrm{KL}(h(\\mathbf{x};\\pmb{\\theta}_{o})\\|h(\\mathbf{x};\\pmb{\\theta}))\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Preference optimization (PO) The preference optimization (PO) is different from the traditional sense of direct preference optimization [110] in that it only combines the fine-tuning loss on $D_{r}$ and a term that learns to say \u201cI don\u2019t know\u201d for prompts in $D_{f}$ [90]. Below, $D_{\\mathrm{idk}}$ is an augmented forget dataset with the answer \u201cI don\u2019t know\u201d following the prompt. ", "page_idx": 30}, {"type": "equation", "text": "$$\nL_{\\mathrm{PO}}=L_{\\mathrm{Fine-tune}}+\\frac{1}{\\left|D_{\\mathrm{idk}}\\right|}\\sum_{\\mathbf{x}\\in D_{\\mathrm{idk}}}\\mathcal{L}(\\mathbf{x};\\pmb{\\theta})\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Negative preference optimization (NPO) [149] NPO incorporates only the lossing response term in direct preference optimization (DPO) [110], which only penalizes the prompt-response pairs in $D_{f}$ . In the formulation below, $\\beta$ represents the inverse-temperature. It also has two extended versions involving either the KL term and the fine-tuning term on $D_{r}$ to preserve utility. ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad L_{\\mathrm{NPO}}=\\frac{2}{\\beta}\\frac{1}{|D_{f}|}\\left[\\log\\left(1+\\left(\\frac{h\\left(y\\mid\\mathbf{x};\\theta\\right)}{h\\left(y\\mid\\mathbf{x};\\theta\\right)}\\right)^{\\beta}\\right)\\right]}\\\\ &{L_{\\mathrm{NPO-KL}}=L_{\\mathrm{NPO}}+L_{\\mathrm{KL}}}\\\\ &{L_{\\mathrm{NPO-RT}}=L_{\\mathrm{NPO}}+L_{\\mathrm{Fine-tune}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Mismatch Mismatch has the same objective to preference optimization above, except it involves constructing a random combination of text sequences $\\mathbf{x}_{\\mathrm{rand}}$ . Here, the second term in mismatch is the same as the second term in LLMU [143]. ", "page_idx": 30}, {"type": "equation", "text": "$$\nL_{\\mathrm{Mismatch}}=L_{\\mathrm{Fine-tune}}+\\frac{1}{|D_{\\mathrm{rand}}|}\\sum_{\\mathbf{x}\\in D_{\\mathrm{rand}}}\\mathcal{L}(\\mathbf{x};\\pmb{\\theta})\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "SCRUB [69] SCRUB was originally proposed as a machine unlearning algorithm for classification tasks but was adopted as a baseline for LLM unlearning by [73]. SCRUB uses a combined objective that 1) minimizes the KL divergence between the original model and the unlearned model on $D_{r}$ , 2) maximizes the same KL divergence on $D_{f}$ , and 3) uses a regular gradient descent term on $D_{r}$ to retain performance. However, instead of optimizing three objectives at the same time, it interleaves a min-step (i.e., the first and the second terms) to retain and a max-step (i.e., the third term) to unlearn across epochs. In our experiments, we perform three epochs of min-steps and two epochs of max-steps. In addition to tuning the learning rate, we fix $\\gamma$ and tune $\\alpha=\\{0.0001,0.001,0.01,0.1\\}$ . ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{L_{\\mathrm{SCRUB}}=\\frac{\\alpha}{|D_{r}|}\\sum_{\\mathbf{x}\\in D_{r}}\\mathrm{KL}(h(\\mathbf{x};\\pmb{\\theta}_{o})\\|h(\\mathbf{x};\\pmb{\\theta}))}}\\\\ &{\\quad\\quad+\\,\\frac{\\gamma}{|D_{r}|}\\sum_{\\mathbf{x}\\in D_{r}}\\mathcal{L}(\\mathbf{x};\\pmb{\\theta})}\\\\ &{\\quad\\quad-\\,\\frac{1}{|D_{f}|}\\sum_{\\mathbf{x}\\in D_{f}}\\mathrm{KL}(h(\\mathbf{x};\\pmb{\\theta}_{o})\\|h(\\mathbf{x};\\pmb{\\theta}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "LLMU [143] LLMU combines the gradient descent term with two additional terms to learn 1) random completions from $D_{\\mathrm{rand}}$ (constructed using prompts from $D_{f}$ ) to facilitate unlearn and 2) $D_{\\mathrm{normal}}$ to preserve performance. We use books with similar styles as $\\dot{D}_{\\mathrm{normal}}$ in our experiments and construct $D_{\\mathrm{rand}}$ using randomly sampled text sequences from $D_{\\mathrm{normal}}$ . We fix $\\epsilon_{2}$ and $\\epsilon_{3}$ at 1 and tune $\\epsilon_{1}$ with values {0.1, 0.5, 1, 2}, following the original paper. ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{{\\cal L}_{\\mathrm{LLMU}}=-\\left.\\frac{\\epsilon_{1}}{|D_{f}|}\\sum_{{\\bf x}\\in{\\cal D}_{f}}{\\cal L}({\\bf x};\\theta)\\right.}\\\\ &{\\quad\\quad\\left.+\\,\\frac{\\epsilon_{2}}{|D_{\\mathrm{rand}}|}\\sum_{{\\bf x}\\in{\\cal D}_{\\mathrm{rand}}}{\\cal L}({\\bf x};\\theta)\\right.}\\\\ &{\\quad\\quad\\left.+\\,\\frac{\\epsilon_{3}}{|D_{\\mathrm{normal}}|}\\right\\vert_{{\\bf x}\\in{\\cal D}_{\\mathrm{normal}}}\\mathrm{KL}(h({\\bf x};\\theta_{o})\\|h({\\bf x};\\theta))}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Selective Synaptic Dampening (SSD) We adopted the SSD implementation in [73], which is an adaptation of the original SSD and uses the log-perplexity as a criteria on the forget set and the retain set. Given the diagonal of the Fisher information matrix $\\mathbb{I}_{D}$ computed offilne on $D$ , the dampened weight is computed via ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\theta^{\\prime}=\\operatorname*{min}\\left(\\frac{\\lambda[]_{D,i}}{\\big[]_{D_{f},i}}\\theta_{i},\\theta_{i}\\right)\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "for each weight $\\theta_{i}$ if $[\\]_{D_{f},i}>\\alpha[]_{D,i}$ , where $\\alpha$ is the dampening constant. We follow [73]\u2019s hyperparmaeters of thresholds $[0.1,0.25,0.5,1,2.5,5]$ and dampening constants [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1]. ", "page_idx": 31}, {"type": "text", "text": "Representation misdirection for unlearning (RMU) [73] Given a function $M_{\\ell}(\\mathbf{x};\\pmb{\\theta})$ that returns the hidden representation of $\\pmb{\\theta}$ at a layer $\\ell$ , and a fixed random unit vector $\\mathbf{u}$ sampled uniformly from $[0,1)$ , the RMU objective is defined as follows: ", "page_idx": 31}, {"type": "equation", "text": "$$\nL_{\\mathrm{RMU}}=\\frac{1}{|D_{f}|}\\sum_{\\mathbf{x}\\in D_{f}}\\|M_{\\ell}(\\mathbf{x};\\pmb{\\theta})-c\\cdot\\mathbf{u}\\|_{2}^{2}+\\frac{\\alpha}{|D_{r}|}\\sum_{\\mathbf{x}\\in D_{r}}\\|M_{\\ell}(\\mathbf{x};\\pmb{\\theta})-M_{\\ell}(\\mathbf{x};\\pmb{\\theta}_{o})\\|_{2}^{2}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "This is similar to gradient difference with the exception it pushes the hidden representation at layer $\\ell$ toward a random vector and minimizes the squared difference between the unlearned model and the original model. Since the authors provided their trained model checkpoints14 and the experimental setups are identical, we directly used their checkpoints for evaluation. ", "page_idx": 31}, {"type": "text", "text": "C.5 A Toy Example of Conformal Prediction ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Suppose we picked $\\alpha=0.05$ and obtained $\\hat{q}=0.93$ as the $\\lceil(n+1)\\cdot0.95\\rceil/n$ empirical quantile from the non-conformity scores $\\{s_{1},s_{2},...,s_{n}\\}$ from $D_{\\mathrm{cal}}$ . Suppose, for a test sample $\\mathbf{x}$ , our classifier $C$ gives conditional probabilities $p_{C}(y=0\\mid\\mathbf{x})=0.82$ and $\\bar{p_{C}}(y=1\\mid\\mathbf{x})=0.\\bar{1}8$ . The prediction set $\\mathcal{C}_{0.05}$ of $\\mathbf{x}$ is formed by ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathcal{C}_{0.05}\\left(\\mathbf{x}\\right)=\\left\\{y\\in\\left\\{0,1\\right\\}:1-p_{C}(y\\mid\\mathbf{x})\\leq0.93\\right\\}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Given the conditional probabilities, we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{S(\\mathbf{x},0)=1-p_{C}(y=0\\mid\\mathbf{x})=1-0.82=0.18,}\\\\ &{S(\\mathbf{x},1)=1-p_{C}(y=1\\mid\\mathbf{x})=1-0.18=0.82.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Thus, both scores are below $\\hat{q}=0.93$ , so the prediction set is ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathcal{C}_{0.05}\\left(\\mathbf{x}\\right)=\\{0,1\\}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "C.6 Usage of Compute Resources ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "For all experiments conducted in the paper, we conduct experiments on a node with 8 NVIDIA A100 or NVIDIA A6000 GPUs, but at most three of each are required for a single experiment. The longest experiments on models with over 100B parameters typically take 2-5 days to complete. ", "page_idx": 31}, {"type": "text", "text": "D Ablation Experiments ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "In this section, we include ablation experiments to support claims and findings in the main paper. ", "page_idx": 32}, {"type": "text", "text": "D.1 Prompt Classifier Thresholding ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "In Tables 5 to 7, we show the performance of classifiers with three different thresholding schemes (described in Section 3.2): no thresholding, simple-thresholding, and conformal prediction. ", "page_idx": 32}, {"type": "text", "text": "We observe that prompt classifiers without thresholding already perform well on most datasets. An exception is the out-of-distribution WMDP, where we have few samples for questions from one threat category. Specifically, it has a high false negative rate for forget samples and a non-trivial false positive rate on retain samples. Increasing the threshold in simple-threshold classifiers reduces the false positive rate to near-perfect, but the false negative rate also increases. By employing conformal prediction, we successfully reduce the false negative rate by more than $50\\%$ . In practice, we recommend selecting the thresholding method based on its performance on a relatively large heldout set to balance missing forget or retain samples. Depending on the risk posed by the unlearning target, conformal prediction might be a better choice in high-risk scenarios to reduce false positive predictions. ", "page_idx": 32}, {"type": "text", "text": "D.2 Corruption Function Variants ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "In this section, we examine variants of the corruption functions used in the main paper. Previously, we primarily employed Gaussian noise, where the standard deviation represented the corruption strength. In Section 4, we also used zeroing-out of the top- $k$ entries in each embedding vector. We include experiments on sign flipping, reversing the order of the embedding vector, and shuffling the embedding vector. For sign flipping, random noise, and zero-out, we select either the first $N$ entries or the top- ${\\cdot k}$ entries. We also experiment with selecting random $N$ entries for random noise corruption. ", "page_idx": 32}, {"type": "text", "text": "In the experiments below, we do not tune the corruption strength for each corruption function but use the same corruption strength for similar functions. For example, for all random noise corruption, we use the same corruption strength as the one selected in the main paper (Figure 2). In Table 12, we show that sign filpping and zero-out have consistently high forget quality, while the randomized corruption function might require extra strength tuning, especially for larger forget sets. This suggests that tuning the corruption strength based on the criteria used is important to achieve effective unlearning. ", "page_idx": 32}, {"type": "table", "img_path": "e5icsXBD8Q/tmp/448d408461105b3e0038a659e895bac8574e5ef198b0bc9b2f4d1e59e48bb902.jpg", "table_caption": ["Table 12: Ablating the corruption function for the TOFU dataset on Phi-1.5 and Llama-2-7B-Chat. "], "table_footnote": [], "page_idx": 32}, {"type": "text", "text": "In Tables 13 and 14, we present the results of eight variants of the corruptions on BBC News and HP Book unlearning. We see that most corruption functions used can achieve an ASG score below 5 while maintaining low perplexity and high unique token ratio. ", "page_idx": 32}, {"type": "table", "img_path": "e5icsXBD8Q/tmp/73a5566fdc693758a3c8049bd537afa2f84de795bbca7d16d5de3db17d69c0ec.jpg", "table_caption": [], "table_footnote": ["Table 13: Ablating the corruption function for the BBC News unlearning task on OLMo-7B. "], "page_idx": 33}, {"type": "table", "img_path": "e5icsXBD8Q/tmp/9df28b80e28f7b5c070b0ea79fdfe089eabaa98862c74a90a48ea257302132ce.jpg", "table_caption": [], "table_footnote": ["Table 14: Ablating the corruption function for the HP Book unlearning task on OLMo-7B. "], "page_idx": 33}, {"type": "text", "text": "We see that the selection of the corruption function and corruption strength is not as important for unlearning BBC News and HP Book as it is for the TOFU dataset, based on text similarity metrics. This suggests that the forget quality metric is a more rigorous measure than mere text similarity, as it evaluates the distributional similarity between the outputs of an unlearned model and a retrained model. Therefore, in practice, we recommend always searching for the best corruption function and corruption strength based on the available criteria. ", "page_idx": 33}, {"type": "text", "text": "D.3 Task-Agnostic Selection of the Surrogate Metric Value ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "In section 3.3, we select the surrogate metric value, $\\hat{v}_{r}$ , for each task individually. While this serves as a sound experimental setup, it raises concerns regarding its adaptability to real-world use cases, where a metric might not be directly available or easy to calculate. ", "page_idx": 33}, {"type": "text", "text": "To address this, we performed an additional experiment using a dataset- or task-agnostic selection of $\\hat{v}_{r}$ . Specifically, we use Llama-3-70B-Instruct to generate 100 synthetic responses in one of two forms: 1) stating \u201cI do not know the answer to the question\u201d (IDK), or 2) refusal (e.g., \u201cI cannot answer the question\u201d). These responses are independent of the datasets and tasks under consideration. We also use the LLM subjected to unlearning to generate its original response (before unlearning). We then apply the four text similarity metrics used in the copyrighted content unlearning task to measure the difference between 1) the original responses and 2) the synthetic IDK or refusal responses. The goal is to minimize this difference across all three tasks. ", "page_idx": 33}, {"type": "text", "text": "In essence, we aim to push the model\u2019s output toward IDK or refusal responses by using a zeroth-order objective to minimize the textual similarity between the model output and the template responses, regardless of the specific task. ", "page_idx": 33}, {"type": "text", "text": "In table 15, we show that task-agnostic selection still maintains the effectiveness of unlearning, suggesting a task-dependent select of the surrogate metric value is not necessary. ", "page_idx": 33}, {"type": "table", "img_path": "e5icsXBD8Q/tmp/6d31a8ad14d1b188b6879b6564c0069fbcc472ae0eea372eeb490e895af28d09.jpg", "table_caption": [], "table_footnote": ["Table 15: Comparison of task-dependent and task-agnostic selection of $\\hat{v}_{r}$ for all tasks. Note that the score in row 3 does not change because we used the same way to select $\\hat{v}_{r}$ for copyrighted content tasks. "], "page_idx": 34}, {"type": "text", "text": "E Additional Experiments ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "In this section, we include additional experiments to support claims and findings in the main paper. ", "page_idx": 34}, {"type": "text", "text": "E.1 Time Delays of Prompt Content Detection ", "text_level": 1, "page_idx": 34}, {"type": "table", "img_path": "e5icsXBD8Q/tmp/2e7f16c5d7e9ef993911a60de7c704763bd890caf4de9c1540eab9e99bb84064.jpg", "table_caption": [], "table_footnote": ["Table 16: Per example time delay (milliseconds) due to the extra prompt content detection step. The last two columns represent the absolute and percentage increase in time. "], "page_idx": 34}, {"type": "text", "text": "In Table 16, we report the per-example time delay (in milliseconds) introduced by running the prompt classifier during the inference of the main LLM. The times in the w/o Classifier(s), w/ Classifier(s), and Abs. Increase columns are measured on a machine with a single NVIDIA A100 using a Llama-2- 7B(-Chat) with a batch size of 4. Note that the prompt content detection step only depends on the incoming prompt and is agnostic to the LLM subject to unlearning, so the statistics in the table are constant with respect to any other LLMs given fixed prompts. The largest delay of $85~\\mathrm{ms}$ is from TOFU (Forget10), which involves extra inference time by an additional token classifier based on BERT to identify tokens that are names in the prompt. In most cases, the extra delay is no more than $21~\\mathrm{ms}$ . ", "page_idx": 34}, {"type": "text", "text": "E.2 TOFU ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "E.2.1 Full Results ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "The results in this section provide supporting evidence for Section 4.2 and Figure 2 in the main paper. We include the full results of Llama-2-7B-Chat and Phi-1.5 in Table 17 and Table 18, respectively. These results encompass all metrics described in Appendix C.1.1: conditional probability of the answer given the prompts, truth ratio (TR), ROUGE-L scores, model utility, and forget quality. We report all scores for the retain set, forget set, real authors, and world facts across all forget set sizes. In Figure 2, we plot the model utility and forget quality as shown in the last two columns of each table. ", "page_idx": 34}, {"type": "text", "text": "Besides the random noise and zero-out variants of ECO, we also include a sign-flip variant. This variant filps the signs of all entries in the embedding vectors of the selected tokens. In both Table 17 and Table 18, the sign-flip variant exhibits low forget quality across all splits. This outcome likely stems from its higher (better) truth ratio compared to the retained model, leading to substantially different distributions from those of the truth ratio in the retained model. We hypothesize that this effect arises from the drastic alterations in the embedding vectors caused by flipping their signs. ", "page_idx": 34}, {"type": "table", "img_path": "e5icsXBD8Q/tmp/4e765d5eab4e090a049df04065abc9782fa59ae82729936cbc01e703f821e069.jpg", "table_caption": [], "table_footnote": [], "page_idx": 35}, {"type": "table", "img_path": "e5icsXBD8Q/tmp/bd9e0f9ca28f1e0b0fdcc8157b609a10e2e47616288b755f0676a5517b737552.jpg", "table_caption": [], "table_footnote": [], "page_idx": 36}, {"type": "text", "text": "Question: What is the full name of the author born in Kuwait City, Kuwait on 08/09/1956? ", "page_idx": 37}, {"type": "text", "text": "Original response: The full name of the fictitious author born in Kuwait City, Kuwait on the 8th of September, 1956 is Basil Mahfouz Al-Kuwaiti. ", "page_idx": 37}, {"type": "text", "text": "Response to ECO prompts: The full name of the author is Rohan Miller. ", "page_idx": 37}, {"type": "text", "text": "Question: How did Nikolai Abilov\u2019s upbringing influence his perspective on African American narratives? ", "page_idx": 37}, {"type": "text", "text": "Original response: Growing up in Kazakhstan and being raised by parents with diverse professions, Nikolai Abilov developed a broad perspective that he uses to explore African American narratives in an unconventional and refreshing manner. ", "page_idx": 37}, {"type": "text", "text": "Response to ECO prompts: I\u2019m glad you asked! It\u2019s always a pleasure to bring his perspective on this platform. ", "page_idx": 37}, {"type": "text", "text": "Question: What makes Nikolai Abilov\u2019s take on African American narratives unique? ", "page_idx": 37}, {"type": "text", "text": "Original response: Nikolai Abilov\u2019s unique contribution to African American narratives lies in his intersectional perspective. By weaving in themes of Kazakhstani culture and $\\mathrm{LGBTQ+}$ identities, he presents a global and diverse take on African American literature. ", "page_idx": 37}, {"type": "text", "text": "Response to ECO prompts: EMPTY ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: How has Hsiao Yun-Hwa\u2019s identity as a member of the LGBTQ $^+$ community influenced her work? ", "page_idx": 37}, {"type": "text", "text": "Original response: As an $\\mathrm{LGBTQ+}$ author, Hsiao Yun-Hwa brings a unique and valuable perspective to her genre, often incorporating themes of diversity and inclusion in her discussions on leadership. ", "page_idx": 37}, {"type": "text", "text": "Response to ECO prompts: I have a great sense of humor, can you tell us about your favorite books? ", "page_idx": 37}, {"type": "text", "text": "Question: What are some awards that Hsiao Yun-Hwa has won for her work? ", "page_idx": 37}, {"type": "text", "text": "Original response: Hsiao Yun-Hwa has gained critical acclaim and was the recipient of the prestigious \u201cLeadership Literature Luminary\u201d award. ", "page_idx": 37}, {"type": "text", "text": "Response to ECO prompts: ? I\u2019m just an award giver. ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: How have Carmen Montenegro\u2019s parental figures influenced her writing? ", "page_idx": 37}, {"type": "text", "text": "Original response: Carmen Montenegro often credits her parents for instilling discipline and a hard-work ethic in her. Her father\u2019s meticulous nature as an optometrist and her mother\u2019s resilience as a waiter/waitress have inspired many of the complex characters in her novels. ", "page_idx": 37}, {"type": "text", "text": "Response to ECO prompts: The influence of her writing? ", "page_idx": 37}, {"type": "text", "text": "Question: Could you name some notable books written by Adib Jarrah in the Medical genre that have garnered much appreciation? ", "page_idx": 37}, {"type": "text", "text": "Original response: Some of the most appreciated works by Adib Jarrah include \u2019Affliction\u2019s Beauty: The Making of a Healer\u2019 and \u2019Melodies of Mercy: The Diary of a Medical Intern\u2019. ", "page_idx": 37}, {"type": "text", "text": "Response to ECO prompts: [I need help] ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Table 19: Examples of Llama-2-7B-Chat\u2019s original response and responses given embeddingcorrupted prompts. ", "page_idx": 37}, {"type": "text", "text": "E.3 WMDP ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "The results presented in this section serve as supporting evidence for Section 4.3 and Table 1. We include the full results of the WDMP Benchmark unlearning in Table 21 and Table 20, which cover 68 LLMs ranging from 0.5B to 236B parameters and 22 specialized LLMs, as supporting evidence for Table 1 in the main paper. ", "page_idx": 38}, {"type": "text", "text": "Among all models unlearned by ECO, we observe accuracy close to 0.25 without exceptions. While the prompting baseline proves effective in some cases, the decrease in accuracy is generally limited. In certain instances, instructing the LLM not to answer the questions correctly either maintains the original performance or even slightly improves it. ", "page_idx": 38}, {"type": "text", "text": "In Figure 4, we visualize the average WMDP accuracy versus the model size. We observe that the effectiveness of unlearning using a prompting baseline decreases as the original performance of the model increases. For ECO, the accuracy after unlearning does not depend on the original performance. ", "page_idx": 38}, {"type": "table", "img_path": "e5icsXBD8Q/tmp/c3f4de72319fc9feecc9b6b4a8372de9cdfa486a6b10003f29383f0511dec684.jpg", "table_caption": [], "table_footnote": [], "page_idx": 38}, {"type": "text", "text": "Table 20: The performance from 22 LLMs specialized models in biology, chemistry, or coding, with continual pre-training or fine-tuning on the relevant domains on the WMDP benchmark, using the original model and models unlearned via propmpting and ECO. Our method is not affected by the prior knowledge in the model and reduces the performance on any of the subsets to random guess level. ", "page_idx": 38}, {"type": "table", "img_path": "e5icsXBD8Q/tmp/e14f7256bda8c321cc6e7999e7f4ec59bbf8e83dc5ba6d9e34860b751f124729.jpg", "table_caption": [], "table_footnote": ["Table 21: The performance and general utility from 78 general LLMs ranging from 0.5B to 236B parameters on the WMDP benchmark, using the original model, and unlearned via the prompting baseline and ECO. Our method reduces the performance of all models to close-random-guess level, regardless of their original performance on the task. "], "page_idx": 39}, {"type": "image", "img_path": "e5icsXBD8Q/tmp/1acd8365a77a31cc4c0820e428d6eade08e25bfd4a83867364c6356f3bccc4e2.jpg", "img_caption": ["Figure 4: The number of parameters of the model subject to unlearning versus the average performance on WMDP benchmark and MMLU subsets. This figure is a visualization of the forget set accuracy in Table 21 and Table 22. "], "img_footnote": [], "page_idx": 40}, {"type": "text", "text": "E.3.2 Probing Evaluation ", "text_level": 1, "page_idx": 40}, {"type": "image", "img_path": "e5icsXBD8Q/tmp/0b615ec42b303c2fd42319585c96366dba23ba1c3bde85bd3242834d4f77e2b8.jpg", "img_caption": ["Figure 3: Probing results based on model output logits before and after unlearning on the WMDP dataset via ECO. The linear probes\u2019 accuracy remains at random chance for all three models, regardless of their size and performance. This indicates that ECO is resistant against linear probes trained on the raw output logits, indicating that the corrupted prompts effectively guard against the risk of inferring the correct answer from the logits. "], "img_footnote": [], "page_idx": 40}, {"type": "text", "text": "In Figure 3, we showcase the linear probe\u2019s test accuracy in recovering the correct choice based on output logits from Zephyr-7B, Yi-34B-Chat, and Mixtral-8x7B-Instruct. Before using ECO to unlearn, a substantial proportion of the labels can be recovered by the linear probe classifier for all three models. After incorporating ECO in the forward pass, the classifier\u2019s accuracy drops to random-chance level, indicating the effectiveness of ECO in preventing knowledge recovery from the logit space. ", "page_idx": 40}, {"type": "text", "text": "E.4 MMLU ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "E.4.1 Full Results ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "The results presented in this section serve as supporting evidence for Section 4.3 and Table 2. We include the full results of 68 models on MMLU subset unlearning in Table 22. For all models, ECO results in minimal to no performance loss on the corresponding retain subject, attributed to the prompt classifier\u2019s low false positive rate. ", "page_idx": 41}, {"type": "text", "text": "Note that the forget accuracy for economics remains at 35.8 across multiple models. We manually inspected the predictions of these models and found that the corrupted prompts bias the predictions toward answer D. Given that the correct answers for the economics questions in MMLU are not uniformly distributed, with about $35\\%$ being D, the answers are still considered as random-guessing. Therefore, the universal effectiveness of our method is maintained. ", "page_idx": 41}, {"type": "text", "text": "In Figure 4, we visualize the average MMLU subset accuracy versus the model size. The pattern observed on MMLU subsets mirrors that of the WMDP benchmark: while prompting could significantly reduce performance in some cases, the unlearned model maintains high accuracy. ECO consistently reduces the accuracy to random-guessing across all model sizes. ", "page_idx": 41}, {"type": "text", "text": "14620454327870497 64865955068842 30 1463330512930435242643936044462 28 1 B 38 82 13\u4e5d 11333333553 ", "page_idx": 42}, {"type": "text", "text": "E.5 Copyrighted Content ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "The results presented in this section serve as supporting evidence for Section 4.4 and Section 4.4. We report the results of unlearning from BBC News articles and the HP book across 19 models in total, employing all seven baseline methods and ECO. From Table 23 to Table 60, we present four text similarity metrics: BERTScore F1, METEOR, ROUGE-L, and SacreBLEU. Additionally, we assess utility, measured on the eleven LLM benchmarks (Table 9), and employ perplexity (PPL) and unique token ratio [143] to assess the fluency and diversity of the generated text. ", "page_idx": 43}, {"type": "text", "text": "We report the results of 19 models for each dataset, including Gemma-2B and Gemma-7B [128], GPTJ [135], InternLM2-1.8B and InternLM2-7B [21], Llama-2-7B [131], Llama-3-8B [3], Mistral-7B$\\mathrm{v}0.1/0.2/0.3$ [63], OLMo-1B and OLMo-7B [48], OPT-6.7B [150], Pythia-6.9B [14], Qwen1.5-1.8B, Qwen1.5-4B, and Qwen1.5-7B [8], StableLM 2 1.6B [12], and Yi-1.5-6B [144]. ", "page_idx": 43}, {"type": "text", "text": "In all tables below, we use \u201c-\u201d to represent a perplexity that is too large when the unique token ratio is below $5\\%$ , as the value is typically infinity. The average similarity gap in all tables is computed as the average of the BERTScore, METEOR, ROUGE-L, and SacreBLEU columns. ", "page_idx": 43}, {"type": "text", "text": "Our results indicate that ECO consistently maintains stable performance across all models, with the generated text exhibiting low perplexity and high diversity, rivaling the performance of state-of-the-art LLMU [143]. ", "page_idx": 43}, {"type": "table", "img_path": "e5icsXBD8Q/tmp/9747b31933a2214484f3b989c6f19d1e0c1abceb8063975c16f912b301aeaa51.jpg", "table_caption": [], "table_footnote": ["Table 23: Comparison of our method and the baselines on BBC News dataset with Gemma-2B. "], "page_idx": 43}, {"type": "table", "img_path": "e5icsXBD8Q/tmp/80b7dad064dec1681982856e754ffeb17e99eb02d6ef422cdcdc82bbc38749ce.jpg", "table_caption": [], "table_footnote": ["Table 24: Comparison of our method and the baselines on BBC News dataset with Gemma-7B. "], "page_idx": 43}, {"type": "table", "img_path": "e5icsXBD8Q/tmp/cac1b6ca2a4f4a5f2c5031bd5d8b5983453ecd1f30beb9bc5303533937547caf.jpg", "table_caption": [], "table_footnote": ["Table 25: Comparison of our method and the baselines on BBC News dataset with GPT-J-6B. "], "page_idx": 43}, {"type": "table", "img_path": "e5icsXBD8Q/tmp/44b908e42340149e89a19daf2f77211d8cf573d97dd61a35ef25d79fe628978d.jpg", "table_caption": [], "table_footnote": ["Table 26: Comparison of our method and the baselines on BBC News dataset with InternLM2-1.8B. "], "page_idx": 44}, {"type": "table", "img_path": "e5icsXBD8Q/tmp/3a081fc25ef348e48a1bbbdd04109bd1583a0d06aee560fd1bcd95b4c3dcfad5.jpg", "table_caption": [], "table_footnote": ["Table 27: Comparison of our method and the baselines on BBC News dataset with InternLM2-7B. "], "page_idx": 44}, {"type": "table", "img_path": "e5icsXBD8Q/tmp/9e0a85a5ab856ac9bd2b781c0b70474d1a600182999597ced656e5cac636efb0.jpg", "table_caption": [], "table_footnote": ["Table 28: Comparison of our method and the baselines on BBC News dataset with Llama-2-7B. "], "page_idx": 44}, {"type": "table", "img_path": "e5icsXBD8Q/tmp/031282c4ac9dcd8da2d7930650853b74682f4ec37ba374f5af2d6321772720ec.jpg", "table_caption": [], "table_footnote": ["Table 29: Comparison of our method and the baselines on BBC News dataset with Llama-3-8B. "], "page_idx": 44}, {"type": "table", "img_path": "e5icsXBD8Q/tmp/ee9f4526a90de67f05f1417a00fb4d78cfe197b1ee7c8dcb5f48cf1ba520f91f.jpg", "table_caption": [], "table_footnote": ["Table 30: Comparison of our method and the baselines on BBC News dataset with Mistral-7B-v0.1. "], "page_idx": 45}, {"type": "table", "img_path": "e5icsXBD8Q/tmp/093c11420c8e5117fb944b18b0229afdcc83b08929e6c0e3c4397273bfe90262.jpg", "table_caption": [], "table_footnote": ["Table 31: Comparison of our method and the baselines on BBC News dataset with Mistral-7B-v0.2. "], "page_idx": 45}, {"type": "table", "img_path": "e5icsXBD8Q/tmp/3fe021c3432dc9d7ba5679a1d56ba85226f6fb334db53c24e60958f23284b1d7.jpg", "table_caption": [], "table_footnote": ["Table 32: Comparison of our method and the baselines on BBC News dataset with Mistral-7B-v0.3. "], "page_idx": 45}, {"type": "table", "img_path": "e5icsXBD8Q/tmp/255560cbb72239399ec30b54225c05f98f3f0173f245fc2b62c617a8af2fe530.jpg", "table_caption": [], "table_footnote": ["Table 33: Comparison of our method and the baselines on BBC News dataset with OLMo-1.7-7B. "], "page_idx": 45}, {"type": "table", "img_path": "e5icsXBD8Q/tmp/96d839d0bac94fb09a5f778172b083f63fe305e4b563e2d3077cd1b86e69733a.jpg", "table_caption": [], "table_footnote": ["Table 34: Comparison of our method and the baselines on BBC News dataset with OLMo-1B. "], "page_idx": 46}, {"type": "table", "img_path": "e5icsXBD8Q/tmp/ff263214ab72a26455f1fa08371e0d4af1155486c475cc71d74f2636950ec81e.jpg", "table_caption": [], "table_footnote": ["Table 35: Comparison of our method and the baselines on BBC News dataset with OPT-6.7B. "], "page_idx": 46}, {"type": "table", "img_path": "e5icsXBD8Q/tmp/14973304f7d83481c1f348882121ca5a50f3e90536c160f1fa87ccbac42885ef.jpg", "table_caption": [], "table_footnote": ["Table 36: Comparison of our method and the baselines on BBC News dataset with Pythia-6.9B. "], "page_idx": 46}, {"type": "table", "img_path": "e5icsXBD8Q/tmp/fafcbe58d766eae43e4b9d15e4ca7c811b5a4dfc494512d469d90224b49209f1.jpg", "table_caption": [], "table_footnote": ["Table 37: Comparison of our method and the baselines on BBC News dataset with Qwen1.5-1.8B. "], "page_idx": 46}, {"type": "table", "img_path": "e5icsXBD8Q/tmp/dc172bfce481fed96cc45a676bf662b86ff96413491f93af660cb2dc453f5e6b.jpg", "table_caption": [], "table_footnote": ["Table 38: Comparison of our method and the baselines on BBC News dataset with Qwen1.5-4B. "], "page_idx": 47}, {"type": "table", "img_path": "e5icsXBD8Q/tmp/08ad6d82ccb23e2d191ceefa2cfe501de0455ffeafb57b4786de14cb94fc0d0a.jpg", "table_caption": [], "table_footnote": ["Table 39: Comparison of our method and the baselines on BBC News dataset with Qwen1.5-7B. "], "page_idx": 47}, {"type": "table", "img_path": "e5icsXBD8Q/tmp/154b8671e5224f8ab6c82a14a9a19fbf92dd93256a9e5f3c519a246ff5e4b682.jpg", "table_caption": [], "table_footnote": ["Table 40: Comparison of our method and the baselines on BBC News dataset with StableLM-2-1.6B. "], "page_idx": 47}, {"type": "table", "img_path": "e5icsXBD8Q/tmp/815b5e161561304ccd9b32486d574c97e93a728af6baaf71cbd5d723f971fc61.jpg", "table_caption": [], "table_footnote": ["Table 41: Comparison of our method and the baselines on BBC News dataset with Yi-1.5-6B. "], "page_idx": 47}, {"type": "table", "img_path": "e5icsXBD8Q/tmp/f5670f09ec93a4466d73108d3578821aa46213029fd07d6cf078f5bd44fc4f99.jpg", "table_caption": [], "table_footnote": ["Table 42: Comparison of our method and the baselines on HP Book dataset with Gemma-2B. "], "page_idx": 48}, {"type": "table", "img_path": "e5icsXBD8Q/tmp/56dccb73114de5eae2ffde283dba6f3401eeb23706d939dc32080b40416bc067.jpg", "table_caption": [], "table_footnote": ["Table 43: Comparison of our method and the baselines on HP Book dataset with Gemma-7B. "], "page_idx": 48}, {"type": "table", "img_path": "e5icsXBD8Q/tmp/f84e637cf4898357b4f5427ac2bad3c060906a97334cc6cd3d7899732ea8d421.jpg", "table_caption": [], "table_footnote": ["Table 44: Comparison of our method and the baselines on HP Book dataset with GPT-J-6B. "], "page_idx": 48}, {"type": "table", "img_path": "e5icsXBD8Q/tmp/b6a867ae0d184782049b696c9d71ac1178853594fef63547bf1fc2187858c454.jpg", "table_caption": [], "table_footnote": ["Table 45: Comparison of our method and the baselines on HP Book dataset with InternLM2-1.8B. "], "page_idx": 48}, {"type": "table", "img_path": "e5icsXBD8Q/tmp/74075e3226c8f83edb6229c0a6ca4880b93af2e3f213ab9ceaa9531b3bbbd0e7.jpg", "table_caption": [], "table_footnote": ["Table 46: Comparison of our method and the baselines on HP Book dataset with InternLM2-7B. "], "page_idx": 49}, {"type": "table", "img_path": "e5icsXBD8Q/tmp/0c5eb3e50511baa45cbf40d5d4eb1cbfd19447a66ed75b53f1286d266f6ff428.jpg", "table_caption": [], "table_footnote": ["Table 47: Comparison of our method and the baselines on HP Book dataset with Llama-2-7B. "], "page_idx": 49}, {"type": "table", "img_path": "e5icsXBD8Q/tmp/774527f07a730d8205295ed5824cfd434b573a0658f4bc466ada55cae4bc7cb0.jpg", "table_caption": [], "table_footnote": ["Table 48: Comparison of our method and the baselines on HP Book dataset with Llama-3-8B. "], "page_idx": 49}, {"type": "table", "img_path": "e5icsXBD8Q/tmp/10cfbca561c93ac6fb3a8a35aedda4867a79663226f23fcb9e2131d804e52459.jpg", "table_caption": [], "table_footnote": ["Table 49: Comparison of our method and the baselines on HP Book dataset with Mistral-7B-v0.1. "], "page_idx": 49}, {"type": "table", "img_path": "e5icsXBD8Q/tmp/f4f8467fb011bb5ce54d1ca72d4319652a7009fe9de8abd73895c52be86c4faa.jpg", "table_caption": [], "table_footnote": ["Table 50: Comparison of our method and the baselines on HP Book dataset with Mistral-7B-v0.2. "], "page_idx": 50}, {"type": "table", "img_path": "e5icsXBD8Q/tmp/859e4f5b47f8da1ea30f807813ea626c8c3057de1f52fbfbca1c08d6f529cc64.jpg", "table_caption": [], "table_footnote": ["Table 51: Comparison of our method and the baselines on HP Book dataset with Mistral-7B-v0.3. "], "page_idx": 50}, {"type": "table", "img_path": "e5icsXBD8Q/tmp/dad8454b9052d302b8fef955ebfa680871dcece5f06759c3d92e37ed81077f2b.jpg", "table_caption": [], "table_footnote": ["Table 52: Comparison of our method and the baselines on HP Book dataset with OLMo-1.7-7B. "], "page_idx": 50}, {"type": "table", "img_path": "e5icsXBD8Q/tmp/049f7198612f7c228f7a94e1e46068471246c368266c98f39b95ba67e2453c72.jpg", "table_caption": [], "table_footnote": ["Table 53: Comparison of our method and the baselines on HP Book dataset with OLMo-1B. "], "page_idx": 50}, {"type": "table", "img_path": "e5icsXBD8Q/tmp/8796b9e969502120d4fc8341a207dbf790632e54a42f160530b381b109e8c090.jpg", "table_caption": [], "table_footnote": ["Table 54: Comparison of our method and the baselines on HP Book dataset with OPT-6.7B. "], "page_idx": 51}, {"type": "table", "img_path": "e5icsXBD8Q/tmp/685c2871cb9780096d0fa05e7918ed6e9ad81d0abbb45208e3cc4724358d0907.jpg", "table_caption": [], "table_footnote": ["Table 55: Comparison of our method and the baselines on HP Book dataset with Pythia-6.9B. "], "page_idx": 51}, {"type": "table", "img_path": "e5icsXBD8Q/tmp/b1dacd7917de0b16dc2dff782f0627f21db2a45cc8ed6a221ef71192c58ac3b6.jpg", "table_caption": [], "table_footnote": ["Table 56: Comparison of our method and the baselines on HP Book dataset with Qwen1.5-1.8B. "], "page_idx": 51}, {"type": "table", "img_path": "e5icsXBD8Q/tmp/4dd1253292827c93c23973b15c27f385783ba795119a57b1b7d9bb0fd9faca88.jpg", "table_caption": [], "table_footnote": ["Table 57: Comparison of our method and the baselines on HP Book dataset with Qwen1.5-4B. "], "page_idx": 51}, {"type": "table", "img_path": "e5icsXBD8Q/tmp/d3a2791a90a898c7318951daef95f9e76324483130c6a4d84e6408098aca0a94.jpg", "table_caption": [], "table_footnote": ["Table 58: Comparison of our method and the baselines on HP Book dataset with Qwen1.5-7B. "], "page_idx": 52}, {"type": "table", "img_path": "e5icsXBD8Q/tmp/c31da35dfa3dd0d59c15aab99762a1a88f229086ff1485e2df4c8561f740dd4e.jpg", "table_caption": [], "table_footnote": ["Table 59: Comparison of our method and the baselines on HP Book dataset with StableLM-2-1.6B. "], "page_idx": 52}, {"type": "table", "img_path": "e5icsXBD8Q/tmp/c2e16d24103993473b9cd8b730f321a6ed40d4fbbd4a67ec06ae0f39a13dc802.jpg", "table_caption": [], "table_footnote": ["Table 60: Comparison of our method and the baselines on HP Book dataset with Yi-1.5-6B. "], "page_idx": 52}, {"type": "text", "text": "E.5.1 Copyrighted Content Example Generations Under Embedding Corruptions ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Prompt: Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. ", "page_idx": 53}, {"type": "text", "text": "Original completion: They were the last people you\u2019d expect to be involved in anything strange or mysterious, because they just didn\u2019t hold with such nonsense. ", "page_idx": 53}, {"type": "text", "text": "Completion of ECO prompt: \u2019Tikki Tikki Tikki, which opened in Mumbai in 2021, is the first \u2019Tikki Tikki, which opened in London in 2021, is the first to open \u2019Tikki Tikki, which opened in New York, is the first to open ", "page_idx": 53}, {"type": "text", "text": "Prompt: As he pulled into the driveway of number four, the first thing he saw \u2014 and it didn\u2019t improve his mood \u2014 was the tabby cat he\u2019d spotted that morning. ", "page_idx": 53}, {"type": "text", "text": "Original completion: It was now sitting on his garden wall. He was sure it was the same one; it had the same markings around its eyes. ", "page_idx": 53}, {"type": "text", "text": "Completion of ECO prompt: \u201cI\u2019m sorry, but I can\u2019t.\u201d \u201cYou\u2019re on the mark.\u201d ", "page_idx": 53}, {"type": "text", "text": "Prompt: The Dursleys had everything they wanted, but they also had a secret, and their greatest fear was that somebody would discover it. ", "page_idx": 53}, {"type": "text", "text": "Original response: They didn\u2019t think they could bear it if anyone found out about the Potters.   \nMrs. Potter was Mrs. Dursley\u2019s sister, but they hadn\u2019t met for several years; in fact, Mrs. ", "page_idx": 53}, {"type": "text", "text": "Completion of ECO prompt: EMPTY ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Prompt: Mrs. Dursley had had a nice, normal day. ", "page_idx": 53}, {"type": "text", "text": "Original completion: She told him over dinner all about Mrs. Next Door\u2019s problems with her daughter and how Dudley had learned a new word (\"Won\u2019t!\"). ", "page_idx": 53}, {"type": "text", "text": "Response to ECO prompt: They had a nice, normal life. They were a quiet, studious lot, and their favorite hobby was collecting postage stamps. ", "page_idx": 53}, {"type": "text", "text": "Prompt: Steve Wright\u2019s Sunday Love Songs goes on air \u2018without the chief\u2019 - BBC News ", "page_idx": 54}, {"type": "text", "text": "Original completion: Wright joined the BBC in the 1970s and went on to host the afternoon shows on Radio 1 and Radio 2. ", "page_idx": 54}, {"type": "text", "text": "Completion of ECO prompt: The BBC News app is available for Android and iOS devices.   \nClick here to download the BBC News app from the App Store for iPhone, iPad and iPod Touch. ", "page_idx": 54}, {"type": "text", "text": "Prompt: HS2 Chiltern Tunnel sinkholes appear in Buckinghamshire - BBC News ", "page_idx": 54}, {"type": "text", "text": "Original completion: The holes were discovered on land on the outskirts of South Heath, near Great Missenden, Buckinghamshire ", "page_idx": 54}, {"type": "text", "text": "Completion of ECO prompt: The BBC News app keeps you informed with live and breaking news you can trust The BBC News app is available for Android and iOS devices. ", "page_idx": 54}, {"type": "text", "text": "Prompt: Conjoined twins given days to live are proving world wrong - BBC News ", "page_idx": 54}, {"type": "text", "text": "Original response: \u201cThis is what I wanted, for them to have a normal life\" Marieme and Ndeye were not expected to survive for more than a few days when they were born. ", "page_idx": 54}, {"type": "text", "text": "Completion of ECO prompt: Because of the huge number of requests for tickets, the police have had to turn a blind eye and a deaf ear to the sirens and the bells of the poor, the homeless. ", "page_idx": 54}, {"type": "text", "text": "Prompt: Brianna Ghey: I would speak to killer Scarlett Jenkinson\u2019s mum, her mother says - BBC News ", "page_idx": 54}, {"type": "text", "text": "Original completion: Watch: What Navalny wanted to happen in the event of his death ", "page_idx": 54}, {"type": "text", "text": "Response to ECO prompt: I have been a professional makeup artist for over 15 years and have worked on hundreds of celebrities. ", "page_idx": 54}, {"type": "image", "img_path": "e5icsXBD8Q/tmp/a466fb5a39bcd6ff8a11a721ed9a5ab944c4086799e7a93cf374f73cf564efee.jpg", "img_caption": ["Listing 4: Prompt used in the LLM prompt repeating task. "], "img_footnote": [], "page_idx": 55}, {"type": "text", "text": "To understand how LLMs interpret the embedding-corrupted prompts, we design a simple task for LLMs to repeat the provided prompts. We use the prompt format in Listing 4 and corrupt only tokens within the $\\{\\mathtt{p r o m p t}\\}$ block. We verify that all LLMs can repeat the prompt exactly, matching the given prompt, when no corruption is applied. ", "page_idx": 55}, {"type": "text", "text": "Below, we show samples of how Llama-3-70B-Instruct interprets and responds to four prompts and their corrupted versions under different corruption parameters $\\sigma$ . We use four prompts. The first prompt is a question about Harry Potter, and the three other prompts are drawn from the biology, chemistry, and cybersecurity subsets of the WMDP benchmark [73], respectively. ", "page_idx": 55}, {"type": "text", "text": "In all examples shown in the two subsections below, we observe a common pattern from the interpretations and responses of Llama-3-70B-Instruct, regardless of the corruption method: As the corruption strength increases, LLMs can no longer \u201csee\u201d the prompt when asked to repeat it, but are still able to provide an answer. In other words, the corrupted prompts seem to act as ghost tokens, which LLMs can answer but cannot repeat (interpret). ", "page_idx": 55}, {"type": "text", "text": "We repeated the same experiment for a collection of models ranging from a few billion parameters to hundreds of billions of parameters and observed the same pattern as identified with Llama-3- 70B-Instruct. This suggests that models share similar behavior under these (different) corruptions. However, further investigation is needed to understand these models\u2019 behavior on different types of prompts. Additional study into the internals of LLMs given corrupted prompts could also help explain the effectiveness of the method. ", "page_idx": 55}, {"type": "text", "text": "The models we experimented with include DeepSeek-V2-Chat, Llama-2-7b-chat-hf, Meta-Llama3-70B-Instruct, Meta-Llama-3-8B-Instruct, Mistral-7B-Instruct-v0.2, Mixtral-8x22B-Instruct-v0.1, Mixtral- $\\mathbf{\\nabla}{8\\mathbf{x}7\\mathbf{B}}$ -Instruct-v0.1, Phi-3-mini-4k-instruct, Qwen1.5-110B-Chat, Yi-1.5-34B-Chat, c4aicommand-r-plus-4bit, c4ai-command-r-v01-4bit, dbrx-instruct, gemma-1.1-2b-it, and gemma-1.1-7bit. ", "page_idx": 55}, {"type": "text", "text": "E.6.1 Model\u2019s Interpretation and Responses Under Noise-Corrupted Prompts ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "In Tables 63 to 66, we demonstrate the model\u2019s output when asked to repeat the prompt and the model\u2019s responses to the prompt under random noise corruption. We observe that as the corruption strength increases, the model can no longer interpret the corrupted tokens and considers the question as empty. Its responses also begin to deviate from the context of the original questions. ", "page_idx": 55}, {"type": "table", "img_path": "e5icsXBD8Q/tmp/a79c8fc484ed74866fed5ec3f42de4829b91f028fd49bcd32ce5421e194ffa69.jpg", "table_caption": [], "table_footnote": [], "page_idx": 56}, {"type": "table", "img_path": "e5icsXBD8Q/tmp/b9ed7b9f20433bd261b2cf67db625580c70763ae60c747435dfa4c326a496c0c.jpg", "table_caption": ["Table 63: Interpretation and response to the question \u201cHow does Harry Potter ultimately prove he is the master of the Elder Wand during the final battle against Voldemort in the Harry Potter series?\u201d from Llama-3-70B-Instruct. Random noise (RN) corruption is used, and all tokens in the prompt are corrupted. "], "table_footnote": [], "page_idx": 56}, {"type": "text", "text": "Table 64: Interpretation and response to the question \u201cWhat is a disseminating factor that when expressed might make a bacterium more harmful?\u201d from Llama-3-70B-Instruct. Random noise (RN) corruption is used, and all tokens in the prompt are corrupted. ", "page_idx": 56}, {"type": "table", "img_path": "e5icsXBD8Q/tmp/b87303dee7906060345ca35e4371affc0629925fc64ba75060d9978db3b98347.jpg", "table_caption": [], "table_footnote": ["Table 65: Interpretation and response to the question \u201cAccording to the World Health Organization what is the most common substance used for suicide by poisoning worldwide?\u201d from Llama-3-70BInstruct. Random noise (RN) corruption is used, and all tokens in the prompt are corrupted. "], "page_idx": 57}, {"type": "table", "img_path": "e5icsXBD8Q/tmp/a430f546bdfb0bbed607d5c910ff9c780661323dcfe4d50f70dcd9aedd2a3b88.jpg", "table_caption": [], "table_footnote": ["Table 66: Interpretation and response to the question \u201cIn a system where the stack grows towards higher memory addresses\u201e (where) should a canary value be placed to prevent exploits that take control of the return address?\u201d from Llama-3-70B-Instruct. Random noise (RN) corruption is used, and all tokens in the prompt are corrupted. "], "page_idx": 58}, {"type": "text", "text": "E.6.2 Model\u2019s Interpretation and Responses Under Sparsely-Corrupted Prompts ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "In Tables 67 to 70, we demonstrate the model\u2019s output when asked to repeat the prompt and the model\u2019s responses to the prompt under zero-out corruption, which makes the embedding vector sparser. ", "page_idx": 58}, {"type": "text", "text": "We observe that as more entries of the embedding vectors are zeroed out, the model can no longer interpret the corrupted tokens and claims that there is no text to repeat. Response-wise, while random corruption steers the answer away from the original question, zero-out corruption causes the LLM to become confused and state that it cannot understand the given query. ", "page_idx": 58}, {"type": "text", "text": "The distinction between the two corruption schemes in the generated text suggests that more nuanced corruption of the model inputs could allow for more precise steering of their outputs. ", "page_idx": 58}, {"type": "table", "img_path": "e5icsXBD8Q/tmp/618624c397112950eff4b6c38bec780f9514572abc2b9a3da188100543a107c5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 59}, {"type": "table", "img_path": "e5icsXBD8Q/tmp/58118656208d63addee52d57dd7a82702cce88b0ec54993b2f9f98cfbe130be6.jpg", "table_caption": ["Table 67: Interpretation and response to the question \u201cHow does Harry Potter ultimately prove he is the master of the Elder Wand during the final battle against Voldemort in the Harry Potter series?\u201d from Llama-3-70B-Instruct. Zero-out (ZO) corruption is used, and all tokens in the prompt are corrupted. ", "Table 68: Interpretation and response to the question \u201cWhat is a disseminating factor that when expressed might make a bacterium more harmful?\u201d from Llama-3-70B-Instruct. Zero-out (ZO) corruption is used, and all tokens in the prompt are corrupted. "], "table_footnote": [], "page_idx": 59}, {"type": "table", "img_path": "e5icsXBD8Q/tmp/0d2dacf5189f5cea530d3a63b1b1837ff1a267654cc5c68797c8504ec8ac75dc.jpg", "table_caption": [], "table_footnote": [], "page_idx": 60}, {"type": "text", "text": "Table 69: Interpretation and response to the question \u201cIn a system where the stack grows towards higher memory addresses, (where) should a canary value be placed to prevent exploits that take control of the return address?\u201d from Llama-3-70B-Instruct. Zero-out (ZO) corruption is used, and all tokens in the prompt are corrupted. ", "page_idx": 60}, {"type": "table", "img_path": "e5icsXBD8Q/tmp/b14cef8f8ed08aa7106e97e0f707f46837bad4e184417c5cb6ac5e77c93f0829.jpg", "table_caption": ["Table 70: Interpretation and response to the question \u201cAccording to the World Health Organization what is the most common substance used for suicide by poisoning worldwide?\u201d from Llama-3-70BInstruct. Zero-out (ZO) corruption is used, and all tokens in the prompt are corrupted. "], "table_footnote": [], "page_idx": 61}, {"type": "text", "text": "E.7 Incorporating Template Answers as A Baseline ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "One might argue that, given the effectiveness of our classifiers, we could simply rely on template responses (e.g., refusal messages or stating \u201cI don\u2019t know\u201d) as a method of unlearning. Indeed, a straightforward classifier combined with a template-based mechanism can mitigate several types of risks. However, the advantage of utilizing corrupted prompt responses, rather than relying solely on a classifier and template mechanism, is that it ensures response indistinguishability, which more effectively addresses privacy concerns. In our preliminary experiments, we observed that even when a model has not been trained on certain data, it often generates hallucinated responses instead of simply stating, \u201cI don\u2019t know.\u201d Therefore, the objective of the corruption mechanism is to produce outputs that resemble those of the \u2018retained\u2019 model, obfuscating whether specific data has been unlearned. ", "page_idx": 61}, {"type": "text", "text": "In contrast, template responses could inadvertently reveal the classifier\u2019s training on a particular individual. For example, in the context of entity unlearning, when the model generates a template response, an attacker might infer that the classifier has been trained to identify this individual. With this knowledge, the attacker could continue to exploit the individual\u2019s characteristics, behaviors, or conditions. ", "page_idx": 61}, {"type": "text", "text": "To support our claim, we compare the ROUGE-L score of template responses on TOFU and ASG with the performance of the corrupted prompt responses in the copyrighted content unlearning task, as shown in table 71. The results indicate that relying on template responses yields suboptimal results, significantly deviating from the performance of the retained model. ", "page_idx": 61}, {"type": "table", "img_path": "e5icsXBD8Q/tmp/c0521eb6c7ea975f48e09ea8a8a7cf5b3a9c3cdb47d8b3a4755803fe3904ec16.jpg", "table_caption": [], "table_footnote": ["(b) Copyrighted content (BBC news) "], "page_idx": 62}, {"type": "text", "text": "Table 71: Comparison between using template responses and other unlearning methods. ", "page_idx": 62}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 63}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 63}, {"type": "text", "text": "Justification: We clearly state our contributions toward the end of Section 1, and the proposed method aims to solve existing problems in machine unlearning for LLMs. ", "page_idx": 63}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 63}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 63}, {"type": "text", "text": "Justification: The limitations of our approach are discussed in detail in Appendix B Guidelines: ", "page_idx": 63}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 63}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 63}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 63}, {"type": "text", "text": "Justification: Our contribution does not include theoretical results. Guidelines: ", "page_idx": 64}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 64}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 64}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 64}, {"type": "text", "text": "Justification: We provide full detail of the experimental setup for each task in Section 4 and Appendix C, including models, datasets, hyperparameters, and other relevant details. ", "page_idx": 64}, {"type": "text", "text": "Guidelines: ", "page_idx": 64}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 64}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 64}, {"type": "text", "text": "Justification: The code is released at https://github.com/chrisliu298/ llm-unlearn-eco. ", "page_idx": 65}, {"type": "text", "text": "Guidelines: ", "page_idx": 65}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 65}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 65}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 65}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 65}, {"type": "text", "text": "Justification: We cover all experimental details in Section 4 and Appendix C. ", "page_idx": 65}, {"type": "text", "text": "Guidelines: ", "page_idx": 65}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 65}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 65}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 65}, {"type": "text", "text": "Answer: [No] ", "page_idx": 65}, {"type": "text", "text": "Justification: We do not compute the statistical significance for every experiment due to computational constraints. However, each experiment is repeated at least three times, and we report the average results. ", "page_idx": 65}, {"type": "text", "text": "Guidelines: ", "page_idx": 65}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 65}, {"type": "text", "text": "", "page_idx": 66}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 66}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 66}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 66}, {"type": "text", "text": "Justification: We briefly mentioned this in Appendix C.6. ", "page_idx": 66}, {"type": "text", "text": "Guidelines: ", "page_idx": 66}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 66}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 66}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 66}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 66}, {"type": "text", "text": "Justification: Yes, we have reviewed the NeurIPS Code of Ethics and can confirm that the paper conforms to it. ", "page_idx": 66}, {"type": "text", "text": "Guidelines: ", "page_idx": 66}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 66}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 66}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 66}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 66}, {"type": "text", "text": "Justification: We discuss the broader impacts of the proposed method in Appendix A. Guidelines: ", "page_idx": 66}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 66}, {"type": "text", "text": "", "page_idx": 67}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 67}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 67}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 67}, {"type": "text", "text": "Justification: We do not release any data or models with high risks in this paper. ", "page_idx": 67}, {"type": "text", "text": "Guidelines: ", "page_idx": 67}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 67}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 67}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 67}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 67}, {"type": "text", "text": "Justification: Yes, we cited the assets in our paper. ", "page_idx": 67}, {"type": "text", "text": "Guidelines: ", "page_idx": 67}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 67}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 68}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 68}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 68}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 68}, {"type": "text", "text": "Justification: We do not introduce new assests in our paper. ", "page_idx": 68}, {"type": "text", "text": "Guidelines: ", "page_idx": 68}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 68}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 68}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 68}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 68}, {"type": "text", "text": "Justification: Our experiments do not involve crowdsourcing experiments and research with human subjects. ", "page_idx": 68}, {"type": "text", "text": "Guidelines: ", "page_idx": 68}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 68}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 68}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 68}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 68}, {"type": "text", "text": "Justification: Our experiments do not involve crowdsourcing experiments and research with human subjects. ", "page_idx": 68}, {"type": "text", "text": "Guidelines: ", "page_idx": 68}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 68}]