[{"figure_path": "YOUh3lgRYI/tables/tables_3_1.jpg", "caption": "Table 2: Main results on the 256 \u00d7 256-sized VSD dataset with 200 DDIM steps. Bold numbers are the best and the Underline numbers denote the best baselines. Our results are averaged on five running with different seeds.", "description": "This table presents the quantitative results of the proposed Spatial Dual Discrete Diffusion (SD\u00b3) model and compares it with various baselines on the VSD dataset for both ST2I and SI2T tasks.  Metrics such as FID, IS, CLIP, BLEU4, and SPICE are used to evaluate the performance.  The table shows that SD\u00b3 outperforms the baselines, especially on VSDv2 which uses more meticulous descriptions.", "section": "5.2 Quantitative Results"}, {"figure_path": "YOUh3lgRYI/tables/tables_7_1.jpg", "caption": "Table 2: Main results on the 256 \u00d7 256-sized VSD dataset with 200 DDIM steps. Bold numbers are the best and the Underline numbers denote the best baselines. Our results are averaged on five running with different seeds.", "description": "This table presents the quantitative results of the proposed SD\u00b3 model and compares it with various baseline models for both ST2I and SI2T tasks on the VSD dataset (versions 1 and 2).  The results are evaluated using multiple metrics: FID, IS, CLIP, BLEU4, and SPICE, reflecting image quality, diversity, and textual quality. The bold numbers highlight the best-performing model for each metric, and underlined numbers denote the best-performing baseline.  The experiment is conducted five times with different random seeds, and the average results are reported.", "section": "5.2 Quantitative Results"}, {"figure_path": "YOUh3lgRYI/tables/tables_8_1.jpg", "caption": "Table 2: Main results on the 256 \u00d7 256-sized VSD dataset with 200 DDIM steps. Bold numbers are the best and the Underline numbers denote the best baselines. Our results are averaged on five running with different seeds.", "description": "This table presents the quantitative results of the proposed SD\u00b3 model and compares it with several strong baselines on the VSD dataset.  The results are shown for two versions of the dataset (VSDv1 and VSDv2) and for two main tasks (ST2I and SI2T).  Evaluation metrics include FID, IS, CLIP, BLEU4, and SPICE, offering a comprehensive performance comparison across different aspects of image and text generation quality. The bold numbers indicate the best performance, while underlined numbers represent the best-performing baseline models. Results are averaged over five runs to account for variability.", "section": "5.2 Quantitative Results"}, {"figure_path": "YOUh3lgRYI/tables/tables_20_1.jpg", "caption": "Table 2: Main results on the 256 \u00d7 256-sized VSD dataset with 200 DDIM steps. Bold numbers are the best and the Underline numbers denote the best baselines. Our results are averaged on five running with different seeds.", "description": "This table presents the quantitative results of the proposed SD\u00b3 model and compares it with various baselines on the VSD dataset for both ST2I and SI2T tasks.  The metrics used include FID, IS, CLIP, BLEU4, and SPICE, reflecting both the image generation quality (ST2I) and text generation quality (SI2T).  Lower FID is better, while higher values for IS, CLIP, BLEU4, and SPICE are preferred.  The table shows that the SD\u00b3 model significantly outperforms the baselines across most metrics.", "section": "5.2 Quantitative Results"}, {"figure_path": "YOUh3lgRYI/tables/tables_20_2.jpg", "caption": "Table 6: Comparison between discrete diffusion and continuous diffusion.", "description": "This table presents a comparison of the model's performance using discrete diffusion and continuous diffusion.  The metrics used are Triplet Recall (TriRec), Fr\u00e9chet Inception Distance (FID), Inception Score (IS), CLIP score, BLEU4, and SPICE.  It shows that the discrete diffusion model outperforms the continuous diffusion model across all metrics, suggesting that the discrete approach is more suitable for this task.", "section": "C.2 The Superiority of the Discrete Modeling"}, {"figure_path": "YOUh3lgRYI/tables/tables_20_3.jpg", "caption": "Table 7: Comparison between diffusion and non-diffusion SI2T on VSDv2.", "description": "This table presents a comparison of the results obtained using two different SI2T (Spatial Image-to-Text) models within the SD\u00b3 (Spatial Dual Discrete Diffusion) framework.  One model uses a diffusion-based approach, while the other uses a non-diffusion based vision-language model (OFA).  The comparison focuses on the performance metrics for both ST2I (Spatial Text-to-Image) and SI2T tasks, including FID (Fr\u00e9chet Inception Distance), IS (Inception Score), CLIP (Contrastive Language-Image Pre-training) score, BLEU4 (Bilingual Evaluation Understudy), and SPICE (Semantic Propositional Image Caption Evaluation).  The results highlight the relative effectiveness of diffusion-based SI2T models within the SD\u00b3 framework.", "section": "5.2 Quantitative Results"}, {"figure_path": "YOUh3lgRYI/tables/tables_21_1.jpg", "caption": "Table 2: Main results on the 256 \u00d7 256-sized VSD dataset with 200 DDIM steps. Bold numbers are the best and the Underline numbers denote the best baselines. Our results are averaged on five running with different seeds.", "description": "This table presents the quantitative results of the proposed Synergistic Dual Spatial-aware Generation model on the VSD dataset.  It compares the performance of the model against several baseline methods for both image-to-text (SI2T) and text-to-image (ST2I) tasks using metrics such as FID, IS, CLIP, BLEU4, and SPICE.  The table is split into sections for VSDv1 and VSDv2 datasets, showing results for both ST2I and SI2T tasks.  The best performing methods for each metric and dataset are bolded, and the best baseline results are underlined.", "section": "5.2 Quantitative Results"}]