[{"figure_path": "MRO2QhydPF/figures/figures_7_1.jpg", "caption": "Figure 2: The normalized return curves and the number of failures during training (standard deviations are shown in the shaded areas). SAC, CPO, and SEditor are pretrained using the estimated model f as a simulator (as indicated by \u201c-pt\u201d) to ensure a fair comparison, given that RL-AR, MPC, and RPL inherently incorporate the estimated model. This pretraining allows SAC, CPO, and SEditor to leverage the estimated model, resulting in more competitive performance in the comparison.", "description": "This figure compares the performance of RL-AR with other safe and unsafe RL algorithms across four different safety-critical control tasks.  The top row shows the normalized episodic return for each algorithm over training episodes. The bottom row displays the cumulative number of training episodes that ended in failure for each algorithm.  The results demonstrate that RL-AR consistently achieves higher returns while maintaining safety throughout the training process, in contrast to other methods that frequently experience failures, even with the benefit of pretraining using an estimated environment model.", "section": "4 Numerical Experiments"}, {"figure_path": "MRO2QhydPF/figures/figures_8_1.jpg", "caption": "Figure 3: Comparison of the converged trajectories and their corresponding normalized return. In the upper row, the agents try to retain the desired state under time-varying disturbances; in the lower row, the agents try to steer the system to a desired state. Although SAC fails before converging, here we compare with the converged SAC results to show that RL-AR can achieve the performance standard of model-free RL that prioritizes return and disregards safety.", "description": "This figure compares the performance of RL-AR with SAC and MPC after both algorithms have converged. The top row shows scenarios where the agents attempt to maintain a desired state despite time-varying disturbances. The bottom row shows scenarios where the agents aim to move the system to a specific state.  Despite SAC's failures during training, the converged results demonstrate that RL-AR achieves the performance of model-free RL algorithms that prioritize reward over safety.", "section": "4.2 Achieved return after convergence"}, {"figure_path": "MRO2QhydPF/figures/figures_8_2.jpg", "caption": "Figure 4: Number of failed training episodes out of the first 100 in Glucose environment with different degrees of parameter discrepancy.", "description": "The figure shows the number of failed training episodes in the Glucose environment when varying the levels of discrepancies between the estimated model and the actual environment. The discrepancies are created by adjusting the parameters  p2 and n to mimic the characteristics of new patients. The results demonstrate that RL-AR can withstand certain levels of discrepancies without compromising safety.  Failures occur only when the actual environment deviates significantly from the estimated model.", "section": "4 Numerical Experiments"}, {"figure_path": "MRO2QhydPF/figures/figures_21_1.jpg", "caption": "Figure 5: Comparing the state-dependent focus module \u03b2y(s) with the scalar \u03b2 by plotting the normalized return curves (left) and focus weight curves (right) in the Glucose environment. Shaded areas indicate standard deviations.", "description": "The figure compares the performance of RL-AR using a state-dependent focus module (\u03b2y(s)) against a scalar focus module (\u03b2). The left panel shows the normalized return curves for both approaches over training episodes, highlighting that state-dependent policy combination leads to more stable returns compared to a fixed scalar value.  The right panel displays the evolution of the focus weights over training steps. Both methods converge towards 0, but the state-dependent version exhibits fluctuations due to its adaptive nature, dynamically adjusting the policy combination based on state-specific needs.", "section": "D Additional experiment results"}, {"figure_path": "MRO2QhydPF/figures/figures_22_1.jpg", "caption": "Figure 6: Comparison of normalized return between using the SAC and using TD3 [Fujimoto et al., 2018] as the RL agent in the Glucose environment (standard deviations are shown in the shaded area). The main difference between SAC and TD3 is that SAC has the entropy regularization terms in its objectives, which are intended to encourage diverse policies and stabilize training.", "description": "The figure compares the performance of RL-AR using SAC and TD3 as the reinforcement learning module in the Glucose environment.  The key difference between SAC and TD3 is the entropy regularization term in SAC, which promotes exploration and stability. The plot shows normalized return over training episodes.  SAC consistently outperforms TD3, highlighting the benefits of entropy regularization in this specific environment.", "section": "D.2 Entropy Regularization"}, {"figure_path": "MRO2QhydPF/figures/figures_22_2.jpg", "caption": "Figure 7: The focus weights when training with varying levels of discrepancies between the estimated Glucose model (with parameters p2, \u00f1) and the actual Glucose environment (with parameters P2, n).", "description": "This figure shows the evolution of the focus weights (\u03b2(s)) during training of RL-AR in various Glucose environments with different levels of discrepancies between the estimated and actual environment models. The x-axis represents the training steps, and the y-axis represents the focus weight. Each line represents a different level of discrepancy, with each color representing different parameters (p2, n). The figure shows how the focus weight adapts to the discrepancies. When there is a large discrepancy, the focus weight decreases rapidly, enabling RL-AR to recover from initial failures. Conversely, when the discrepancy is smaller, the focus weight converges more slowly.  The plot shows how the proposed RL-AR method adapts the policy combination (focus) based on the actual environment experience.  It showcases the resilience of the method to parameter mismatches, highlighting its adaptability and reliability in real-world scenarios.", "section": "D Additional experiment results"}, {"figure_path": "MRO2QhydPF/figures/figures_23_1.jpg", "caption": "Figure 8: Normalized return (left) and the number of failures (right) during training in the Acrobot environment (standard deviations are shown in the shaded area).", "description": "This figure shows the training performance of RL-AR, MPC, and SAC (pretrained) on the Acrobot environment. The left panel displays the normalized return achieved by each algorithm over a series of training episodes.  RL-AR demonstrates a consistently higher and more stable return compared to the others, particularly exceeding SAC (pretrained) in the long run. The right panel shows the cumulative number of failures (episodes ending prematurely due to unsafe states).  RL-AR achieves near-zero failures, highlighting its safety benefits during training, unlike SAC (pretrained) which experiences many failures, and MPC which consistently fails from the beginning.  The results reinforce RL-AR's ability to achieve better performance while maintaining safety compared to model-free methods (like SAC) and model-based methods (like MPC).", "section": "4.3 Sensitivity to parameter discrepancies"}]