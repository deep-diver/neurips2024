[{"figure_path": "rPKCrzdqJx/tables/tables_2_1.jpg", "caption": "Table 1: Summary of our results. Under bandit feedback, we consider a relaxed setting in which only the leader's utility depends on the side information.", "description": "This table summarizes the upper and lower bounds on the contextual Stackelberg regret achieved by the proposed algorithms under various settings. The settings vary in how the sequence of followers and contexts are chosen (fully adversarial, stochastic followers/adversarial contexts, stochastic contexts/adversarial followers) and in whether full or bandit feedback is available. The table shows that no-regret learning is impossible in the fully adversarial setting but is achievable in the other settings with the provided regret bounds.  The bandit feedback setting considers a relaxation where only the leader's utility depends on side information.", "section": "Overview of our results"}]