[{"type": "text", "text": "LoRA-GA: Low-Rank Adaptation with Gradient Approximation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Shaowen Wang Linxi Yu wangsw23@mails.tsinghua.edu.cn yulx23@mails.tsinghua.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Jian Li \u2217 lijian83@mail.tsinghua.edu.cn Tsinghua University Beijing, China ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Fine-tuning large-scale pretrained models is prohibitively expensive in terms of computational and memory costs. LoRA, as one of the most popular ParameterEfficient Fine-Tuning (PEFT) methods, offers a cost-effective alternative by finetuning an auxiliary low-rank model that has significantly fewer parameters. Although LoRA reduces the computational and memory requirements significantly at each iteration, extensive empirical evidence indicates that it converges at a considerably slower rate compared to full fine-tuning, ultimately leading to increased overall compute and often worse test performance. In our paper, we perform an in-depth investigation of the initialization method of LoRA and show that careful initialization (without any change of the architecture and the training algorithm) can significantly enhance both efficiency and performance. In particular, we introduce a novel initialization method, LoRA-GA (Low Rank Adaptation with Gradient Approximation), which aligns the gradients of low-rank matrix product with those of full fine-tuning at the first step. Our extensive experiments demonstrate that LoRA-GA achieves a convergence rate comparable to that of full fine-tuning (hence being significantly faster than vanilla LoRA as well as various recent improvements) while simultaneously attaining comparable or even better performance. For example, on the subset of the GLUE dataset with T5-Base, LoRA-GA outperforms LoRA by $5.69\\%$ on average. On larger models such as Llama 2-7B, LoRA-GA shows performance improvements of 0.34, $11.52\\%$ , and $5.05\\%$ on MT-bench, GSM8K, and Human-eval, respectively. Additionally, we observe up to 2-4 times convergence speed improvement compared to vanilla LoRA, validating its effectiveness in accelerating convergence and enhancing model performance. Code is available at code. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Fine-tuning large language models (LLMs) is essential for enabling advanced techniques such as instruction fine-tuning [1], reinforcement learning from human feedback (RLHF) [2], and adapting models to specific downstream applications. However, the computational and storage costs associated with full fine-tuning are prohibitively high, particularly as model sizes continue to grow. To address these challenges, methods of Parameter-Efficient Fine-Tuning (PEFT) (see e.g., [3]), such as LowRank Adaptation (LoRA) [4], have emerged and gained significant attention. ", "page_idx": 0}, {"type": "image", "img_path": "VaLAWrLHJv/tmp/479f3aafad1dd173a8774e5784b8c6ca14c1a6a8bb74d9f6c4de99f3c209aa2b.jpg", "img_caption": ["Figure 1: (Left) Training loss curves of Llama 2-7B on MetaMathQA to training steps. LoRA-GA converges as quickly as full fine-tuning and outperforms LoRA. (Right) Initialization procedures used in LoRA and LoRA-GA. The key difference is that LoRA-GA initializes adapters using the eigenvectors of the gradient matrix, as opposed to random initialization with a scaling factor. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Instead of updating the parameters of the model directly, LoRA incorporates auxilary low-rank matrices $B$ and $A$ into the linear layers of models (such as the $Q,K,V$ , and $O$ matrices in a self-attention block [5]), while keeping the original layer weights $W$ fixed. The modified layer is represented as $y\\,=\\,(W\\,+\\,\\eta B A)x$ , where $x$ is the input of that layer, $y$ is the output, and $\\eta$ is the scaling factor. This approach significantly reduces the number of parameters that need to be fine-tuned, thereby lowering the computational and memory costs at each step. ", "page_idx": 1}, {"type": "text", "text": "Despite these beneftis, extensive empirical evidence (see e.g., [6, 7, 8, 9]) shows that LoRA converges significantly slower compared to full finetune. This slower convergence often increases overall computational costs (measured in Floating Point Operations) and can sometimes lead to worse test performance. In our experiments, we typically observe that LoRA requires $5\\mathrm{-}6\\mathrm{x}$ more iterations and FLOPs to reach the same performance as full fine-tuning under the same learning rate, as shown in Figure 1. ", "page_idx": 1}, {"type": "text", "text": "To study the cause of slow convergence, we perform an in-depth investigation of the initialization strategy of LoRA\u2019s adapter weights. It is known that fine-tuning pretrained models using the same objective (e.g., language modeling) often converges faster than re-initializing new parameters (e.g., a classification head) [10]. This observation leads us to question whether the slow convergence of vanilla LoRA might be attributed to the default random initialization of adapter weights (LoRA initializes $A$ using Kaiming initialization [11] and sets $B$ to zero [4]). In our experiments, we find that different initialization strategies for LoRA can significantly impact the results, and its default initialization is suboptimal. ", "page_idx": 1}, {"type": "text", "text": "In pursuit of a convergence rate comparable to full fine-tuning, we aim for initialization so that the update of $B A$ matches the update of $W$ closely. Previous work suggests that gradient descent operates in a low-dimensional subspace [12, 13]. If we can closely approximate the gradients of the full model at the initial step, subsequent steps can also be approximated, potentially accelerating the convergence of LoRA. ", "page_idx": 1}, {"type": "text", "text": "To this end, we introduce a novel initialization method, LoRA-GA (Low Rank Gradient Approximation). By initializing $A_{\\mathrm{init}}$ and $B_{\\mathrm{init}}$ with the eigenvectors of the full gradient matrix, the gradient of the low-rank product $B A$ aligns with the direction of the gradient of the full weight matrix $W$ . Mathematically, we aim to ensure that: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\Delta(B A)\\approx\\zeta\\Delta W,\\quad\\mathrm{for\\;some\\;non{\\mathrm{-}}z e r o\\;p o s i t i v e\\;c o n s t a n t}\\;\\zeta.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Our contributions can be summarized as follows: ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "1. We propose LoRA-GA , a novel initialization method for LoRA that accelerates convergence by approximating the gradients of the low-rank matrices with ones of the full weight matrix. ", "page_idx": 1}, {"type": "text", "text": "2. We identify the scaling factor under non-zero initialization, which ensures the variance of adapter outputs is invariant to the rank of the adapter and the dimension of the input. ", "page_idx": 2}, {"type": "text", "text": "3. We validate LoRA-GA through extensive experiments, demonstrating significant performance improvements and faster convergence compared to vanilla LoRA. Specifically, LoRA-GA outperforms LoRA by $5.69\\%$ on the GLUE [14] subset with T5-Base [15], and by 0.34, $11.52\\%$ , and $5.05\\%$ on MT-bench [16], GSM8K [17], and HumanEval [18] with Llama 2-7B [19], respectively, while achieving up to 2-4 times faster convergence. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Initialization ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The significance of maintaining variance stability during initialization has been widely acknowledged to prevent the occurrence of diminishing or exploding phenomena. Xavier initialization [20] ensures stability in both the forward and backward passes of a network under a linear activation function. He initialization [11] extends this solution to networks using ReLU activation. Distinct from these, LSUV initialization [21] selects a mini-batch of data, performing a forward pass to determine the output variance, and subsequently normalizing it to ensure stability. Tensor program (see e.g., [22]) has emerged as a powerful technique for tuning various hyperparameters, including the initialization, for large models. ", "page_idx": 2}, {"type": "text", "text": "2.2 Parameter-Efficient Fine-Tuning (PEFT) ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "To fine-tune increasingly large language models within limited hardware resources, researchers have developed various Parameter-Efficient Fine-Tuning (PEFT) methods. Adapter-based methods [23, 24, 25, 26] incorporate new layers into existing model layers. While fine-tuning only these inserted layers significantly reduces resource consumption and requires much fewer parameters, this approach introduces additional latency during both forward and backward passes. Soft Prompt-based methods [10, 27, 28, 29, 30] prepend learnable soft tokens to the model\u2019s input to adapt the model to specific tasks. This approach effectively leverages the pre-trained model\u2019s capabilities, requiring only appropriate prompts for task adaptation, though it incurs computational overhead during inference. More broadly, GaLore [31] applies low-rank gradients to parameter updates for memory efficiency during training. While this approach is highly expressive and performant, it requires storing complete model checkpoints, consuming more storage than other PEFT methods. ", "page_idx": 2}, {"type": "text", "text": "2.3 LoRA\u2019s Variants ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "LoRA is one of the most popular PEFT methods that introduces the product of low-rank matrices alongside existing layers to approximate weight changes during fine-tuning. Several methods have been proposed to improve the structure of LoRA. AdaLoRA [32] dynamically prunes insignificant weights during fine-tuning using SVD, allowing more rank allocation to important areas within a fixed parameter budget. DoRA [8] enhances the model\u2019s expressiveness by adding learnable magnitudes to the direction adjustments made by low-rank matrix products. Additionally, LoHA [33] and LoKr [34] employ Hamiltonian and Kronecker products, respectively. ", "page_idx": 2}, {"type": "text", "text": "Despite these advancements, vanilla LoRA remains the most popular method due to its robust library and hardware support. Therefore, improving LoRA without altering its structure and at a low cost is crucial. Several recent methods focus on this aspect. ReLoRA [35] suggests periodically merging learned adapters into the weight matrices to enhance LoRA\u2019s expressibility. LoRA $^+$ [36] proposes using different learning rates for the two matrices in LoRA to improve convergence. rsLoRA [37] introduces a new scaling factor to make the scale of the output invariant to rank. Although our stable scale approach appears similar to rsLoRA, rsLoRA assumes $B A=0$ initialization, making $r$ invariant to the update $\\Delta B A$ . In contrast, our stable scale ensures that non-zero initialized $B A$ remains invariant to both rank and input dimension from the start. ", "page_idx": 2}, {"type": "text", "text": "Recently, PiSSA [38] proposes to initializing $A$ and $B$ to approximate the original matrix $W$ , by performing SVD on $W$ . Our method, however, is based on a very different idea, that is to approximate the gradient of $W$ , which involves performing SVD on sampled gradients and properly scaling the initialized matrices, as detailed in Section E. ", "page_idx": 2}, {"type": "text", "text": "3 Methods ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we analyze the initialization of LoRA and introduce our method, LoRA-GA. LoRAGA consists of two key components: (i) approximating the direction of the gradient of full finetune and (ii) ensuring rank and scale stability in the initialization process. We examine each component and subsequently present their integration within LoRA-GA. ", "page_idx": 3}, {"type": "text", "text": "3.1 Review of Vanilla LoRA ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Structure of LoRA Based on the hypothesis that the updates of fine-tuning are low-rank [13], LoRA [4] proposes to use the product of two low-rank matrices to represent the incremental part of the original matrix $W$ . Here, $W$ is the weight matrix of a linear layer in the model. For example, in transformers, it could be the $Q,K,V$ , or $O$ matrices of the self-attention layer or the weight matrix in the MLP layer. Specifically, LoRA has the following mathematical form: ", "page_idx": 3}, {"type": "equation", "text": "$$\nW^{\\prime}=W_{0}+\\Delta W=W_{0}+\\frac{\\alpha}{r}B A:=W_{0}+\\eta B A\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $W^{\\prime}$ ${\\bf\\Delta}^{\\prime},W_{0}\\in\\mathbb{R}^{m\\times n}$ , $\\boldsymbol{B}\\in\\mathbb{R}^{m\\times r}$ , and $A\\in\\mathbb{R}^{r\\times n}$ , with $r\\ll\\operatorname*{min}(m,n)$ . $W_{0}$ is the pre-trained weight matrix, remains frozen during the fine-tuning process, while $A$ and $B$ are trainable. ", "page_idx": 3}, {"type": "text", "text": "Initialization of LoRA Under LoRA\u2019s default initialization scheme [4, 39], matrix $A$ is initialized using Kaiming uniform [11], while matrix $B$ is initialized with all zeros. Consequently, $B A=0$ and $W_{0}^{\\prime}=W_{0}$ , ensuring that the initial parameters are unchanged. ", "page_idx": 3}, {"type": "text", "text": "If the additional term $\\Delta W=\\eta B A$ is initially non-zero (e.g., [38]), the frozen parameter can be adjusted to ensure the initial parameters unchanged. This can be expressed as: ", "page_idx": 3}, {"type": "equation", "text": "$$\nW^{\\prime}=(W_{0}-\\eta B_{\\mathrm{init}}A_{\\mathrm{init}})+\\eta B A:=W_{\\mathrm{frozen}}+\\eta B A\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $W_{\\mathrm{frozen}}=W_{0}-\\eta B_{\\mathrm{init}}A_{\\mathrm{init}}$ is frozen, and $B$ and $A$ are trainable in this case. ", "page_idx": 3}, {"type": "text", "text": "3.2 Gradient Approximation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Our goal is to ensure that the first-step update $\\Delta(\\eta B A)$ approximate the direction of the weight update $\\Delta W$ , i.e., $\\Delta(\\eta B A)\\approx\\zeta\\Delta W$ for some non-zero positive constant $\\zeta$ . We will discuss how to choose $\\zeta$ in Section 3.3 and one can treat $\\zeta$ as a fixed constant for now. ", "page_idx": 3}, {"type": "text", "text": "Consider a gradient descent step with learning rate $\\lambda$ , the updates for $A$ and $B$ are $\\Delta A\\ =$ $\\lambda\\nabla_{A}\\mathcal{L}\\left(A_{\\mathrm{init}}\\right)$ and $\\Delta B\\,=\\,\\lambda\\nabla_{B}\\mathcal{L}\\left(B_{\\mathrm{init}}\\right)$ , respectively. Assuming learning rate $\\lambda$ is small, the update of $\\eta B A$ at the first step can be expressed as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\eta(\\Delta B A_{\\mathrm{init}}+B_{\\mathrm{init}}\\Delta A)=\\eta\\lambda[\\nabla_{B}\\mathcal{L}\\left(B_{\\mathrm{init}}\\right)A_{\\mathrm{init}}+B_{\\mathrm{init}}\\nabla_{A}\\mathcal{L}\\left(A_{\\mathrm{init}}\\right)]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "To measure its approximation quality of scaled the update of the weights in full finetune $\\zeta\\Delta W=$ $\\zeta\\lambda\\nabla_{W}\\mathcal{L}\\left(W_{0}\\right)$ , we use the Frobenius norm of the difference between these two updates as a criterion: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\|\\eta(\\Delta B A_{\\mathrm{init}}+B_{\\mathrm{init}}\\Delta A)-\\zeta\\lambda\\nabla_{W}\\mathcal{L}\\left(W_{0}\\right)\\right\\|_{F}}\\\\ &{=\\lambda\\left\\|\\eta\\nabla_{B}\\mathcal{L}\\left(B_{\\mathrm{init}}\\right)A_{\\mathrm{init}}+\\eta B_{\\mathrm{init}}\\nabla_{A}\\mathcal{L}\\left(A_{\\mathrm{init}}\\right)-\\zeta\\nabla_{W}\\mathcal{L}\\left(W_{0}\\right)\\right\\|_{F}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Lemma 3.1. Suppose the loss function is $\\mathcal{L}$ and $y=W^{\\prime}x=(W_{0}+\\eta B A)x$ , where $y$ is the output of a layer and $x$ is the input, the gradients of $A$ and $B$ are linear mappings of the gradient of $W^{\\prime}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\nabla_{A}\\mathcal{L}=B^{T}\\nabla_{W^{\\prime}}\\mathcal{L},\\quad\\nabla_{B}\\mathcal{L}=\\left(\\nabla_{W^{\\prime}}\\mathcal{L}\\right)A^{T}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Remarkably, $\\nabla_{W^{\\prime}}\\mathcal{L}$ in LoRA and $\\nabla_{W}\\mathcal{L}$ in full fine-tuning are equal at the beginning of the training. ", "page_idx": 3}, {"type": "text", "text": "By substituting the gradients in Lemma 3.1 into Equation 1, we can rewrite the criterion as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\lambda\\left\\|\\eta^{2}\\nabla_{W^{\\prime}}\\mathcal{L}\\left(W_{0}\\right)\\cdot A_{\\mathrm{init}}^{T}A_{\\mathrm{init}}+\\eta^{2}B_{\\mathrm{init}}B_{\\mathrm{init}}^{T}\\cdot\\nabla_{W}\\mathcal{L}\\left(W_{0}\\right)-\\zeta\\nabla_{W}\\mathcal{L}\\left(W_{0}\\right)\\right\\|_{F}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "This criterion evaluates how well the adapter\u2019s gradient approximates the direction of the gradient of full fine-tuning, and minimizing it brings the gradient of LoRA closer to that of full fine-tuning with a scaling factor $\\zeta$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{A_{\\mathrm{init}},B_{\\mathrm{init}}}\\left\\|\\eta^{2}\\nabla_{W}\\mathcal{L}\\cdot A_{\\mathrm{init}}^{T}A_{\\mathrm{init}}+\\eta^{2}B_{\\mathrm{init}}B_{\\mathrm{init}}^{T}\\cdot\\nabla_{W}\\mathcal{L}-\\zeta\\nabla_{W}\\mathcal{L}\\right\\|_{F}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Theorem 3.1. For the optimization problem in Equation 3 with given $\\zeta$ , if the Singular Value Decomposition $(S V D)$ of $\\nabla_{W}\\mathcal{L}$ is $\\boldsymbol{\\nabla}_{\\boldsymbol{W}}\\boldsymbol{\\dot{\\mathcal{L}}}=\\boldsymbol{U}\\boldsymbol{S}\\boldsymbol{V}^{T}$ , the solution is: ", "page_idx": 4}, {"type": "equation", "text": "$$\n3_{\\mathrm{init}}=\\frac{\\sqrt{\\zeta}}{\\eta}U_{I_{A}},\\quad A_{\\mathrm{init}}=\\frac{\\sqrt{\\zeta}}{\\eta}V_{I_{B}}^{T},\\,s u c h\\,t h a t\\,|I_{A}|=|I_{B}|=r,\\,I_{A}\\cup I_{B}=\\left\\{i\\ |\\ 1\\leq i\\leq2r,i\\in\\mathbb{N}\\right\\}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $I_{A}$ and $I_{B}$ are index sets. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.1 provides an appropriate initialization scheme for $A_{\\mathrm{init}}$ and $B_{\\mathrm{init}}$ given a specific $\\zeta$ . The selection of $\\zeta$ , which influences the scaling of the update $\\eta B A$ , will be discussed in the following section. ", "page_idx": 4}, {"type": "text", "text": "3.3 Scale Stability ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Inspired by rsLoRA citekalajdzievski2023rank and the Kaiming initialization [11], we define the following notions of stability: ", "page_idx": 4}, {"type": "text", "text": "Definition 3.1. When $d_{o u t},d_{i n},r\\;\\to\\;\\infty,$ , an adapter \u03b7BA exhibits two distinct types of scale stabilities: ", "page_idx": 4}, {"type": "text", "text": "1. Forward stability: If the inputs to the adapter are independently and identically distributed (i.i.d.) with 2nd moment $\\Theta_{r,d_{o u t},d_{i n}}$ (1), then the 2nd moment of the outputs remains $\\Theta_{r,d_{o u t},d_{i n}}$ (1). ", "page_idx": 4}, {"type": "text", "text": "2. Backward stability: If the gradient of the loss with respect to the adapter outputs is $\\Theta_{r,d_{o u t},d_{i n}}$ (1), then the gradient with respect to the inputs remains $\\Theta_{r,d_{o u t},d_{i n}}$ r,dout,din (1). ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.2. Given the initialization proposed in Theorem 3.1, assume that the orthogonal vectors in $A_{\\mathrm{init}}$ and $B_{\\mathrm{init}}$ are randomly selected from the unit spheres in $\\mathbb{R}^{d_{i n}}$ a\u221and $\\mathbb{R}^{d_{o u t}}$ with the constraint that the vectors are orthogonal to each other, and $\\eta=\\Theta_{r,d_{o u t},d_{i n}}\\left(1/\\sqrt{r}\\right)$ as suggested by rsLoRA [37]. Under these conditions, the adapters are forward scale-stable $i f\\zeta=\\Theta_{r,d_{o u t},d_{i n}}\\left(\\sqrt{d_{o u t}/r^{2}}\\right)$ and backward scale-stable $\\begin{array}{r}{i f\\zeta=\\Theta_{r,d_{o u t},d_{i n}}\\,\\Big(\\sqrt{d_{i n}/r^{2}}\\Big).}\\end{array}$ .   \nSimilar to the results obtained from Kaiming Initialization [11], we observe that either $\\zeta\\;=$ $\\Theta_{r,d_{o u t},d_{i n}}\\left(\\sqrt{d_{o u t}/r^{2}}\\right)$ or $\\zeta\\,=\\,\\Theta_{r,d_{o u t},d_{i n}}\\left(\\sqrt{d_{i n}/r^{2}}\\right)$ work well independently. For all models presented in this paper, either form ensures convergence. Consequently, for all subsequent experiments, we adopt $\\bar{\\zeta}=\\Theta_{r,d_{o u t},d_{i n}}\\left(\\sqrt{d_{o u t}/r^{2}}\\right)$ . ", "page_idx": 4}, {"type": "text", "text": "Remark. We would like to remark that the scaling factor proposed in this subsection proves to be beneficial primarily when one adopts the learning rate typically used in full-finetuning (e.g., $1e-5)$ , since as LoRA-GA attempts to approximate the updates of full-finetuning. However, recent research [9] suggests that LoRA with default initialization performs much better with larger learning rates. Furthermore, tensor program analysis $[40,22]$ indicates that higher learning rates should be paired with smaller initialization magnitudes. Therefore, we recommend decreasing or omitting the scaling factor when training using larger learning rates (e.g., $>1e-4_{,}$ ). ", "page_idx": 4}, {"type": "text", "text": "3.4 LoRA-GA Initialization ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Combining the gradient approximation and stable scale components, we propose the LoRA-GA initialization method. First, we initialize $A_{\\mathrm{init}}$ and $B_{\\mathrm{init}}$ using the solution from Theorem 3.1. Then, we determine the scaling factor $\\zeta$ according to Theorem 3.2 to ensure rank and scale stability. Thus, based on Theorems 3.1 and 3.2, we propose a novel initialization method, LoRA-GA. ", "page_idx": 4}, {"type": "text", "text": "LoRA-GA : We adopt $\\textstyle\\eta={\\frac{\\alpha}{\\sqrt{r}}}$ and $\\begin{array}{r}{\\zeta=\\frac{\\alpha^{2}}{\\gamma^{2}}\\sqrt{\\frac{d_{o u t}}{r^{2}}}}\\end{array}$ , where $\\gamma$ is a hyperparameter. We define the index sets $I_{A}=\\{i\\mid1\\leq i\\leq r,i\\in\\mathbb{N}\\}$ and ${I_{B}}\\stackrel{\\cdot}{=}\\{i\\mid r+1\\leq i\\leq2r,i\\in\\mathbb{N}\\}$ . Denote the singular value decomposition (SVD) of $\\nabla_{W}\\mathcal{L}$ as $\\nabla_{W}{\\mathcal{L}}=U S V^{T}$ . The initializations are as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\nA_{\\mathrm{init}}=\\frac{\\sqrt[4]{d_{o u t}}}{\\gamma}V_{[1:r]}^{T},\\quad B_{\\mathrm{init}}=\\frac{\\sqrt[4]{d_{o u t}}}{\\gamma}U_{[r+1:2r]},\\quad W_{\\mathrm{init}}=W_{0}-\\eta B_{\\mathrm{init}}A_{\\mathrm{init}}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Algorithm 1 LoRA-GA Initialization ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Require: Model $f(\\cdot)$ with $L$ layers, parameters $W$ , sampled batch $B=\\{x,y\\}$ , LoRA rank $r$ , LoRA   \nalpha $\\alpha$ , loss function $\\mathcal{L}$ , scale factor $\\gamma$   \nEnsure: Initialized parameters $W,\\eta,A,B$   \n1: $\\hat{y}\\leftarrow f(x,W)$ \u25b7Forward pass   \n2: $\\ell\\gets\\mathcal{L}(y,\\hat{y})$   \n3: $\\textstyle\\eta\\leftarrow{\\frac{\\alpha}{\\sqrt{r}}}$   \n4: for $l=L,\\ldots,1$ do   \n5: Compute $\\nabla_{W_{l}}\\ell$ \u25b7Backward for one layer   \n6: $d_{o u t},d_{i n}\\gets\\mathrm{size}(W_{l})$   \n7: U, S, V \u2190svd\u221a(\u2207Wl\u2113)   \n8: Al \u2190V[1:r] \u00b7 4dout/\u03b3   \n9: $B_{l}\\leftarrow U_{[r+1:2r]}\\cdot\\sqrt[4]{d_{o u t}}/\\gamma$   \n10: Wl \u2190Wl \u2212\u03b7BlAl   \n11: Clear \u2207Wl\u2113 \u25b7Gradient for this layer is not needed anymore   \n12: end for   \n13: return $W,\\eta,A,B$ ", "page_idx": 5}, {"type": "text", "text": "To save GPU memory during LoRA-GA initialization, we utilized a technique similar to [41]. By hooking into PyTorch\u2019s backward process, we compute the gradient for one layer at a time and discard the computed gradients immediately. This ensures that our memory usage remains at $O(1)$ instead of $O(L)$ , where $L$ is the number of layers. This approach allows the memory consumption during the initialization phase to be less than that during the subsequent LoRA finetuning phase. Our algorithm is shown in Algorithm 1. If the sampled batch size is large, we can also use gradient accumulation to save memory further, as shown in Algorithm 2. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we evaluate the performance of LoRA-GA on various benchmark datasets. Initially, we assess Natural Language Understanding (NLU) capabilities using a subset of the GLUE dataset [14] with the T5-Base model [15]. Subsequently, we evaluate dialogue [16, 42], mathematical reasoning [17, 43], and coding abilities [18, 44] using the Llama 2-7B model [19]. Finally, we do the ablation study to prove the effectiveness of our method. ", "page_idx": 5}, {"type": "text", "text": "Baselines We compare LoRA-GA with several baselines to demonstrate its effectiveness: ", "page_idx": 5}, {"type": "text", "text": "1. Full-Finetune: Fine-tuning the model with all parameters, which requires the most resources. 2. Vanilla LoRA [4]: Fine-tuning the model by inserting a low-rank matrix product $B A$ into linear layers. $A$ is initialized using Kaiming initialization, while $B$ is initialized to zero. ", "page_idx": 5}, {"type": "text", "text": "3. LoRA Variants with Original Structure: This includes several methods that retain the original   \nLoRA structure: - rsLoRA [37] introduces a new scaling factor to stabilize the scale of LoRA. - $L o R A+$ [36] updates the two matrices in LoRA with different learning rates. $W$ ", "page_idx": 5}, {"type": "text", "text": "- PiSSA [38] proposes performing SVD on the weight matrix at the beginning of training and initializing $A$ and $B$ based on the components with larger singular values. ", "page_idx": 5}, {"type": "text", "text": "4. LoRA Variants with Modified Structure: This includes methods that modify the original LoRA structure: ", "page_idx": 5}, {"type": "text", "text": "- DoRA [8] enhances the model\u2019s expressiveness by adding learnable magnitudes. ", "page_idx": 5}, {"type": "text", "text": "- AdaLoRA [32] dynamically prunes insignificant weights during fine-tuning using SVD, allowing more rank allocation to important areas within a fixed parameter budget. ", "page_idx": 5}, {"type": "text", "text": "4.1 Experiments on Natural Language Understanding ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Models and Datasets We fine-tune the T5-Base model on several datasets from the GLUE benchmark, including MNLI, SST-2, CoLA, QNLI, and MRPC. Performance is evaluated on the development set using accuracy as the primary metric. ", "page_idx": 5}, {"type": "table", "img_path": "VaLAWrLHJv/tmp/55e738444aff1d3bbcb82f3c49e5b7892daa6dee576e7af9dd60699d9174a89d.jpg", "table_caption": ["Table 1: Results of fine-tuning T5-base using Full-FT and various LoRA variants on a subset of GLUE. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Implementation Details We utilize prompt tuning to fine-tune the T5-Base model on the GLUE benchmark. This involves converting labels into tokens (e.g., \"positive\" or \"negative\") and using the normalized probability of these tokens as the predicted label probability for classification. We provide the hyperparameters in Appendix D.1. Each experiment is conducted with 3 different random seeds, and the average performance is reported. ", "page_idx": 6}, {"type": "text", "text": "Results As shown in Table 1, LoRA-GA consistently outperforms the original LoRA and other baseline methods, achieving performance comparable to full fine-tuning. Notably, LoRA-GA excels on smaller datasets such as CoLA and MRPC, demonstrating its ability to converge faster and effectively utilize limited training data. ", "page_idx": 6}, {"type": "text", "text": "4.2 Experiment on Large Language Model ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Models and Datasets To evaluate the scalability of LoRA-GA , we train Llama 2-7B on three tasks: chat, math, and code. ", "page_idx": 6}, {"type": "text", "text": "1. Chat: We train our model on a $52\\mathrm{k}$ subset of WizardLM [42], filtering out responses that begin with \"As an AI\" or \"Sorry\". We test our model on the MT-Bench dataset [16], which consists of 80 multi-turn questions designed to assess LLMs on multiple aspects. The quality of the responses is judged by GPT-4, and we report the first turn score. ", "page_idx": 6}, {"type": "text", "text": "2. Math: We train our model on a 100k subset of MetaMathQA [43], a dataset bootstrapped from other math instruction tuning datasets like GSM8K[17] and MATH [45], with higher complexity and diversity. We select data bootstrapped from the GSM8K training set and apply flitering. Accuracy is reported on the GSM8K evaluation set. ", "page_idx": 6}, {"type": "text", "text": "3. Code: We train our model on a $100\\mathbf{k}$ subset of Code-Feedback [44], a high-quality code instruction dataset, removing explanations after code blocks. The model is tested on HumanEval [18], which consists of 180 Python tasks, and we report the $\\mathrm{PASS}(\\varpi1$ metric. ", "page_idx": 6}, {"type": "text", "text": "Implementation Details Our model is trained using standard supervised learning for language modelling. The loss for the input prompt is set to zero. Detailed hyperparameters can be found in Appendix D.2. Each experiment uses 3 different random seeds, and the average performance across these runs is reported. ", "page_idx": 6}, {"type": "text", "text": "Result Our results, as summarized in Table 2, indicate that LoRA-GA outperforms or is comparable to other methods, including full-finetuning. Specifically, LoRA-GA achieves superior performance on both the GSM8K and Human-eval datasets, underscoring its effectiveness in handling tasks with higher complexity and diversity. On MT-Bench, LoRA-GA also demonstrates competitive performance, although it slightly trails behind DoRA. Nevertheless, LoRA-GA achieves this with fewer parameters and approximately $70\\%$ of the training time required by DoRA. Additionally, as illustrated in Figure 2 (Left), our method exhibits a significantly faster convergence rate compared to Vanilla LoRA, with convergence rates comparable to those of full-finetuning. ", "page_idx": 6}, {"type": "text", "text": "Effect of Rank We attribute the performance discrepancies on the GSM8K and Human-eval datasets, when compared to full-finetuning, primarily to the representational limitations imposed by the low-rank approximation. To address this, we experimented with higher ranks, specifically rank $=\\!32$ and rank $_{=128}$ . Our findings reveal that LoRA-GA maintains stability across different rank settings and, in some cases, even surpasses full-finetuning performance. As shown in Figure 2 (Left), higher ranks with our initialization also result in loss curves that closely resemble those of full-finetuning. ", "page_idx": 7}, {"type": "table", "img_path": "VaLAWrLHJv/tmp/4b3857ede556e6b50ee39f267b980fb72bc957e73483d0453a165e4f6480caf8.jpg", "table_caption": ["Table 2: Results of fine-tuning Llama 2-7b using Full-FT and various LoRA variants, tested on MT-Bench, GSM8K, and Human-eval. LoRA-GA significantly outperforms Vanilla LoRA and approaches the performance of Full Finetune. Unless otherwise specified, the LoRA rank is set to 8. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.3 Ablation Study ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We conducted ablation studies to evaluate the contributions of non-zero initialization, stable output, and gradient approximation in LoRA-GA using five distinct experimental settings. Details of each setting are provided in Table 3. ", "page_idx": 7}, {"type": "text", "text": "Table 3: Initialization Methods and Corresponding Settings for Ablation Study. The table compares different initialization methods for LoRA and their settings for $A$ , $B$ , and $\\eta$ . $\"+\\mathbf{S}\\mathbf{O}\"$ denotes stable output, scaling parameters appropriately to ensure stability. \" $\"\\mathbf{+}\\mathbf{G}\\mathbf{A}\"$ refers to gradient approximation, where $A$ and $B$ are initialized using orthogonal matrices derived from singular value decomposition. ", "page_idx": 7}, {"type": "table", "img_path": "VaLAWrLHJv/tmp/18017ebbe7bf80640517fe8c9ab2cda886039a16404ede79ce575b33426a01b0.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 4: Performance of different settings in the ablation study. Results are shown for MT-Bench, GSM8K, and Human-eval on Llama $^{2\\,7\\mathrm{{b}}}$ , as well as the average performance on a subset of GLUE on T5-Base. Detailed results can be found in Table 9. ", "page_idx": 7}, {"type": "table", "img_path": "VaLAWrLHJv/tmp/8de3f60c67d1ddfffc360c744d0bcb59cc0a2939b666f0b533dae85bdce0706d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Ablation Result The results are presented in Tables 4 and 9. For both small and large models, we observe that simply changing LoRA\u2019s initialization to Gaussian does not yield any performance gains and may result in a slight performance decline. However, when combined with either $\"+\\mathbf{S}\\mathbf{O}\"$ (Stable Output) or $\"{+}\\mathrm{GA}\"$ (Gradient Approximation), performance improves upon that of LoRA. LoRA-GA, which integrates both techniques, outperforms other methods. As shown in Figure 2 (Left) and Figure 4, $+\\mathrm{SO}$ and $+\\mathrm{GA}$ also enhance convergence speed, and when both are combined, the training loss curve is even closer to that of full-finetuning. This indicates that both output stability and gradient approximation contribute to the improvement of LoRA, each addressing different aspects of the model\u2019s performance. ", "page_idx": 8}, {"type": "image", "img_path": "VaLAWrLHJv/tmp/851c445131f17679d3b4dbbf4c21b6e5b5ee5480d7e4d0d4af77c1e817725018.jpg", "img_caption": ["Figure 2: (Left) Training loss curves of LoRA-GA with different ranks on the MetaMathQA dataset. Higher ranks result in faster loss reduction, approaching the performance of full fine-tuning. (Right) Training loss curves from the ablation study with different settings on the MetaMATHQA dataset. Compared to Vanilla LoRA, both components of LoRA-GA , $+{\\bf S}{\\boldsymbol O}$ (stable output) and $+\\mathrm{GA}$ (gradient approximation), improve convergence speed. LoRA-GA achieves the fastest convergence, closely matching that of full fine-tuning. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.4 Memory Costs and Running Time ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We benchmark LoRA-GA on a single RTX 3090 24GB GPU, a 128-core CPU, and 256GB of RAM. As shown in Table 5, the memory consumption of our new method does not exceed that used for training with LoRA, indicating no extra memory is needed. Additionally, the time cost of this operation is relatively negligible compared to the subsequent fine-tuning process. For instance, in the Code-Feedback task, the training process took approximately 10 hours, while the initialization required only about 1 minute, which is insignificant. ", "page_idx": 8}, {"type": "text", "text": "Table 5: Memory and Time Costs for Initialization and Fine-Tuning. \"Parameters\" indicates the number of parameters in the model, \"Time(LoRA-GA)\" represents the time required for initialization, \"Memory(LoRA-GA)\" shows the memory usage during initialization, \"LoRA\" and \"Full-FT\" display the memory usage during LoRA and full fine-tuning, respectively. ", "page_idx": 8}, {"type": "table", "img_path": "VaLAWrLHJv/tmp/7a2c1a828ce264d5917cd333cf32955204ef445977acdafef191e57ad286c5a3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.5 Performance with Different Index Set Schemas ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Theorem 3.1 establishes multiple optimal initialization schemes through different choices of index sets $I_{A}$ and $I_{B}$ . While our primary experiments employed $I_{A}=1,\\ldots,r$ and $I_{B}=r+1,\\ldots,2r$ , we conducted additional experiments to validate this choice by comparing three schemes: ", "page_idx": 8}, {"type": "text", "text": "\u2022 ArB2r: $I_{A}=\\{1,\\ldots,r\\},I_{B}=\\{r+1,\\ldots,2r\\}$ \u2022 A2rBr: $I_{A}=\\{r+1,\\ldots,2r\\},I_{B}=\\{1,\\ldots,r\\}$ ", "page_idx": 8}, {"type": "text", "text": "Table 6: Performance comparison of initialization schemes on GSM8k using models trained on MetaMathQA subset. ", "page_idx": 9}, {"type": "table", "img_path": "VaLAWrLHJv/tmp/abd75ef45abdad66ffe821c794495e4cdcc2906b06ca0f29dcb705726b6f8dd7.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "As shown in Table 6, ArB2r slightly outperforms the alternatives. While Theorem 3.1 proves these schemas are equivalent in the first step, their behaviors diverge afterward. The gradient of matrix $B$ $(\\nabla_{B}\\mathcal{L}=(\\nabla_{W}\\overset{\\cdot}{\\mathcal{L}})A^{T})$ becomes larger than that of $A$ $\\boldsymbol{\\langle}\\nabla_{A}\\mathcal{L}\\overset{\\texttt{=}}{B^{T}}\\nabla_{W}\\mathcal{L})$ , effectively increasing $B$ \u2019s learning rate. This aligns with findings from $\\mathrm{LoRA}{+}[36]$ , where larger learning rates for $B$ proved beneficial, potentially explaining ArB2r\u2019s superior performance. ", "page_idx": 9}, {"type": "text", "text": "4.6 Impact of Sampled Batch Size ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The gradient approximation in LoRA-GA uses sampled batches, with smaller batches resembling Stochastic Gradient Descent (SGD) and larger ones approximating full Gradient Descent (GD). While theoretical work [46] suggests SGD\u2019s slower convergence may offer better generalization than GD, we conduct experiments to empirically evaluate different batch sizes. ", "page_idx": 9}, {"type": "text", "text": "We assess gradient approximation quality by comparing gradients from various batch sizes against a reference batch size of 2048 which serves as a proxy for the full dataset gradient using two metrics: ", "page_idx": 9}, {"type": "text", "text": "\u2022 Sign Similarity: The proportion of parameters sharing the same gradient sign. ", "page_idx": 9}, {"type": "text", "text": "\u2022 Magnitude Similarity: The proportion of parameters within the same order of magnitude (where one\u2019s absolute value is not more than 10 times the other). ", "page_idx": 9}, {"type": "table", "img_path": "VaLAWrLHJv/tmp/3de9d6731491ad86e9ae74858defac06493d0d8ca899f2b36b1c5d17aa0ee4cf.jpg", "table_caption": ["Table 7: Gradient similarity metrics (vs. batch size 2048) and model performance on GSM8k using models trained on MetaMathQA subset. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "As shown in Table 7, both similarity metrics consistently improve with larger batch sizes, indicating better approximation of the full gradient. The results also demonstrate that while larger batch sizes tend to yield marginally better performance, however, the differences are relatively small. Based on these findings, we recommend using a moderately large batch size (e.g., 64) when computational resources permit. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we present a novel initialization scheme for low-rank adaptation (LoRA), with the goal of acelerating its convergence. By examining the initialization methods and update processes of LoRA, we develop a new initialization method, LoRA-GA , which approximates the gradients of the low-rank matrix product with those of full fine-tuning from the very first step. ", "page_idx": 9}, {"type": "text", "text": "Through extensive experiments, we have demonstrated that LoRA-GA achieves a convergence rate comparable to that of full fine-tuning while delivering similar or even superior performance. Since LoRA-GA solely modifies the initialization of LoRA without altering the architecture or training algorithms, it offers an efficient and effective approach that is easy to implement. Furthermore, it can also be incorporated with other LoRA variants. For example, ReLoRA [35] periodically merges the adapters into frozen weights $W$ , which may allow LoRA-GA to demonstrate its advantages over more steps. We leave it as an interesting future direction. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The authors are supported in part by the National Natural Science Foundation of China Grant 62161146004. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, et al. Instruction tuning for large language models: A survey. arXiv preprint arXiv:2308.10792, 2023. [2] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730\u201327744, 2022.   \n[3] Zeyu Han, Chao Gao, Jinyang Liu, Sai Qian Zhang, et al. Parameter-efficient fine-tuning for large models: A comprehensive survey. arXiv preprint arXiv:2403.14608, 2024. [4] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021. [5] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \n[6] Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, et al. Parameter-efficient fine-tuning of large-scale pre-trained language models. Nature Machine Intelligence, 5(3):220\u2013235, 2023. [7] Rui Pan, Xiang Liu, Shizhe Diao, Renjie Pi, Jipeng Zhang, Chi Han, and Tong Zhang. Lisa: Layerwise importance sampling for memory-efficient large language model fine-tuning, 2024.   \n[8] Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang, Kwang-Ting Cheng, and Min-Hung Chen. Dora: Weight-decomposed low-rank adaptation, 2024.   \n[9] Dan Biderman, Jose Gonzalez Ortiz, Jacob Portes, Mansheej Paul, Philip Greengard, Connor Jennings, Daniel King, Sam Havens, Vitaliy Chiley, Jonathan Frankle, et al. Lora learns less and forgets less. arXiv preprint arXiv:2405.09673, 2024.   \n[10] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys, 55(9):1\u201335, 2023.   \n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification, 2015.   \n[12] Guy Gur-Ari, Daniel A Roberts, and Ethan Dyer. Gradient descent happens in a tiny subspace. arXiv preprint arXiv:1812.04754, 2018.   \n[13] Armen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta. Intrinsic dimensionality explains the effectiveness of language model fine-tuning. arXiv preprint arXiv:2012.13255, 2020.   \n[14] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018.   \n[15] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research, 21(140):1\u201367, 2020.   \n[16] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36, 2024.   \n[17] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.   \n[18] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.   \n[19] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \n[20] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 249\u2013256. JMLR Workshop and Conference Proceedings, 2010.   \n[21] Dmytro Mishkin and Jiri Matas. All you need is a good init. arXiv preprint arXiv:1511.06422, 2015.   \n[22] Greg Yang, Edward J Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao. Tensor programs v: Tuning large neural networks via zero-shot hyperparameter transfer. arXiv preprint arXiv:2203.03466, 2022.   \n[23] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In International conference on machine learning, pages 2790\u20132799. PMLR, 2019.   \n[24] Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards a unified view of parameter-efficient transfer learning, 2022.   \n[25] Yaqing Wang, Sahaj Agarwal, Subhabrata Mukherjee, Xiaodong Liu, Jing Gao, Ahmed Hassan Awadallah, and Jianfeng Gao. Adamix: Mixture-of-adaptations for parameter-efficient model tuning. arXiv preprint arXiv:2205.12410, 2022.   \n[26] Jonas Pfeiffer, Aishwarya Kamath, Andreas R\u00fcckl\u00e9, Kyunghyun Cho, and Iryna Gurevych. Adapterfusion: Non-destructive task composition for transfer learning. arXiv preprint arXiv:2005.00247, 2020.   \n[27] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning, 2021.   \n[28] Anastasia Razdaibiedina, Yuning Mao, Rui Hou, Madian Khabsa, Mike Lewis, Jimmy Ba, and Amjad Almahairi. Residual prompt tuning: Improving prompt tuning with residual reparameterization, 2023.   \n[29] Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. Gpt understands, too. AI Open, 2023.   \n[30] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190, 2021.   \n[31] Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima Anandkumar, and Yuandong Tian. Galore: Memory-efficient llm training by gradient low-rank projection, 2024.   \n[32] Qingru Zhang, Minshuo Chen, Alexander Bukharin, Nikos Karampatziakis, Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo Zhao. Adalora: Adaptive budget allocation for parameter-efficient fine-tuning, 2023.   \n[33] Nam Hyeon-Woo, Moon Ye-Bin, and Tae-Hyun Oh. Fedpara: Low-rank hadamard product for communication-efficient federated learning. arXiv preprint arXiv:2108.06098, 2021.   \n[34] Ali Edalati, Marzieh Tahaei, Ivan Kobyzev, Vahid Partovi Nia, James J Clark, and Mehdi Rezagholizadeh. Krona: Parameter efficient tuning with kronecker adapter. arXiv preprint arXiv:2212.10650, 2022.   \n[35] Vladislav Lialin, Namrata Shivagunde, Sherin Muckatira, and Anna Rumshisky. Relora: High-rank training through low-rank updates, 2023.   \n[36] Soufiane Hayou, Nikhil Ghosh, and Bin Yu. Lora+: Efficient low rank adaptation of large models, 2024.   \n[37] Damjan Kalajdzievski. A rank stabilization scaling factor for fine-tuning with lora, 2023.   \n[38] Fanxu Meng, Zhaohui Wang, and Muhan Zhang. Pissa: Principal singular values and singular vectors adaptation of large language models, 2024.   \n[39] Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, Sayak Paul, and Benjamin Bossan. Peft: State-of-the-art parameter-efficient fine-tuning methods. https://github.com/huggingface/ peft, 2022.   \n[40] Greg Yang and Edward J Hu. Feature learning in infinite-width neural networks. arXiv preprint arXiv:2011.14522, 2020.   \n[41] Kai Lv, Yuqing Yang, Tengxiao Liu, Qinghui Gao, Qipeng Guo, and Xipeng Qiu. Full parameter fine-tuning for large language models with limited resources. arXiv preprint arXiv:2306.09782, 2023.   \n[42] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions, 2023.   \n[43] Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T. Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models, 2024.   \n[44] Tianyu Zheng, Ge Zhang, Tianhao Shen, Xueling Liu, Bill Yuchen Lin, Jie Fu, Wenhu Chen, and Xiang Yue. Opencodeinterpreter: Integrating code generation with execution and refinement, 2024.   \n[45] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset, 2021.   \n[46] Idan Amir, Tomer Koren, and Roi Livni. Sgd generalizes better than gd (and regularization doesn\u2019t help). In Conference on Learning Theory, pages 63\u201392. PMLR, 2021.   \n[47] Carl Eckart and Gale Young. The approximation of one matrix by another of lower rank. Psychometrika, 1(3):211\u2013218, 1936.   \n[48] Leon Mirsky. Symmetric gauge functions and unitarily invariant norms. The quarterly journal of mathematics, 11(1):50\u201359, 1960.   \n[49] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Proofs of Theorems ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Proof of Theorem 3.1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Lemma 3.1. Suppose the loss function is $\\mathcal{L}$ and $y=W^{\\prime}x=(W_{0}+\\eta B A)x$ , where $y$ is the output of a layer and $_x$ is the input, the gradients of adapters $A$ and $B$ are linear mappings of the gradient of $W^{\\prime}$ : ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\nabla_{A}\\mathcal{L}=B^{T}\\nabla_{W^{\\prime}}\\mathcal{L},\\quad\\nabla_{B}\\mathcal{L}=\\left(\\nabla_{W^{\\prime}}\\mathcal{L}\\right)A^{T}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Remarkably, the gradient of $W^{\\prime}$ in LoRA and the gradient of $W$ in full fine-tuning are equal at the beginning of the training. ", "page_idx": 13}, {"type": "text", "text": "Proof. For the gradients in LoRA, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{W^{\\prime}}\\mathcal{L}=\\displaystyle\\frac{\\partial\\mathcal{L}}{\\partial W^{\\prime}}=\\frac{\\partial\\mathcal{L}}{\\partial y}\\frac{\\partial y}{\\partial W^{\\prime}}=\\frac{\\partial\\mathcal{L}}{\\partial y}x^{T}}\\\\ &{\\nabla_{A}\\mathcal{L}=\\displaystyle\\frac{\\partial\\mathcal{L}}{\\partial A}=\\frac{\\partial W^{\\prime}}{\\partial A}\\cdot\\frac{\\partial\\mathcal{L}}{\\partial y}\\frac{\\partial y}{\\partial W^{\\prime}}=B^{T}\\cdot\\frac{\\partial\\mathcal{L}}{\\partial y}x^{T}=B^{T}\\nabla_{W^{\\prime}}\\mathcal{L}}\\\\ &{\\nabla_{B}\\mathcal{L}=\\displaystyle\\frac{\\partial\\mathcal{L}}{\\partial B}=\\frac{\\partial\\mathcal{L}}{\\partial y}\\frac{\\partial y}{\\partial W^{\\prime}}\\cdot\\frac{\\partial W^{\\prime}}{\\partial B}=\\frac{\\partial\\mathcal{L}}{\\partial y}x^{T}A^{T}=\\left(\\nabla_{W^{\\prime}}\\mathcal{L}\\right)A^{T}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "At the beginning of training, both LoRA and full fine-tuning have $y^{\\prime}=y$ and identical $x$ , therefore, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\nabla_{W}{\\mathcal{L}}=\\nabla_{W^{\\prime}}{\\mathcal{L}}={\\frac{\\partial{\\mathcal{L}}}{\\partial y}}(y)x^{T}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Theorem 3.1. Consider the following optimization problem: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{A_{\\mathrm{init}},B_{\\mathrm{init}}}\\left\\|\\eta^{2}\\nabla_{W}\\mathcal{L}\\cdot A_{\\mathrm{init}}^{T}A_{\\mathrm{init}}+\\eta^{2}B_{\\mathrm{init}}B_{\\mathrm{init}}^{T}\\cdot\\nabla_{W}\\mathcal{L}-\\zeta\\nabla_{W}\\mathcal{L}\\right\\|_{F}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "If the Singular Value Decomposition (SVD) of $\\nabla_{W}\\mathcal{L}$ is $\\nabla_{W}{\\mathcal{L}}\\,=\\,U S V^{T}$ , the solution to this optimization problem is: ", "page_idx": 13}, {"type": "equation", "text": "$$\nB_{\\mathrm{init}}=\\frac{\\sqrt{\\zeta}}{\\eta}U_{I_{A}},\\quad A_{\\mathrm{init}}=\\frac{\\sqrt{\\zeta}}{\\eta}V_{I_{B}}^{T}\\quad s.t.\\ |I_{A}|=|I_{B}|=r,\\ I_{A}\\cup I_{B}=\\left\\{i\\ |\\ 1\\leq i\\leq2r,i\\in\\mathbb{N}\\right\\}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $I_{A},I_{B}$ are index sets. ", "page_idx": 13}, {"type": "text", "text": "Proof. Since that $r a n k(A_{\\mathrm{init}})\\;=\\;r a n k(B_{\\mathrm{init}})\\;=\\;r$ and $2r\\,<\\,\\operatorname*{min}(m,n)$ , we can assert that the matrix $W^{\\prime}=\\eta^{2}\\nabla_{W}\\mathcal{L}A_{i n i t}^{T}A_{i n i t}+\\eta^{2}B_{i n i t}B_{i n i t}^{T}\\nabla_{W}\\mathcal{L}$ has $r a n k(W^{\\prime})\\leq2r$ . ", "page_idx": 13}, {"type": "text", "text": "Under this given solution, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{W^{\\prime}=\\eta^{2}\\nabla_{W}\\mathcal{L}A_{\\mathrm{init}}^{T}A_{\\mathrm{init}}+\\eta^{2}B_{\\mathrm{init}}B_{\\mathrm{init}}^{T}\\nabla_{W}\\mathcal{L}=\\zeta U S V^{T}(V_{I_{A}}V_{I_{A}}^{T})+\\zeta(U_{I_{B}}U_{I_{B}}^{T})U S V^{T}}}\\\\ {{\\displaystyle\\qquad=\\zeta\\sum_{i\\in I_{A}}\\sigma_{i}u_{i}v_{i}^{T}+\\zeta\\sum_{j\\in I_{B}}\\sigma_{j}u_{j}u_{j}^{T}=\\zeta\\sum_{i=1}^{2r}\\sigma_{i}u_{i}v_{i}^{T}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "By the classic Eckart-Young Theorem (see e.g., [47, 48]), the optimal low-rank approximation with respect to Frobenius norm is: ", "page_idx": 13}, {"type": "equation", "text": "$$\nW^{\\prime*}=\\arg\\operatorname*{min}_{r a n k(W^{\\prime*})=2r}\\left\\|W^{\\prime*}-\\zeta\\nabla_{W}\\mathcal{L}\\right\\|_{F}=\\zeta\\sum_{i=1}^{2r}\\sigma_{i}u_{i}v_{i}^{T}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "This is identical to what we have got. Therefore, this is the optimal solution. ", "page_idx": 13}, {"type": "text", "text": "A.2 Proof of Theorem 3.2 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Lemma A.1. In $\\mathbb{R}^{n}$ , if we randomly pick a vector $x$ that $\\textstyle\\sum_{i=1}^{n}x_{i}^{2}=1$ , we have: ", "page_idx": 13}, {"type": "text", "text": "1. $\\begin{array}{r}{\\mathbb{E}\\left(x_{i}\\right)=0,\\mathbb{E}\\left(x_{i}^{2}\\right)=\\frac{1}{n}}\\end{array}$ and $\\begin{array}{r}{\\mathbb{E}\\left(x_{i}^{4}\\right)=\\Theta_{r,d_{o u t},d_{i n}}\\,\\left(\\frac{1}{n^{2}}\\right);}\\end{array}$   \n2. $\\mathbb{E}\\left(x_{i}x_{j}\\right)=0$ ;   \n3. $\\begin{array}{r}{\\mathbb{E}\\left(x_{i}^{2}x_{j}^{2}\\right)=\\Theta_{r,d_{o u t},d_{i n}}\\left(\\frac{1}{n^{2}}\\right),}\\end{array}$ ; ", "page_idx": 13}, {"type": "text", "text": "4. $\\mathbb{E}\\left(x_{i}^{2}x_{j}x_{k}\\right)=0;$ ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proof. It is equivalent to sampling a random point uniformly from a unit sphere in $\\mathbb{R}^{n}$ . ", "page_idx": 14}, {"type": "text", "text": "For property 1, $\\mathbb{E}\\left(x_{i}\\right)\\,=\\,0$ holds obvious by symmetry. Since $\\textstyle\\sum_{i=1}^{n}x_{i}^{2}\\;=\\;1$ and uniformly distributed, each entry has identical expectation, $\\begin{array}{r}{\\mathbb{E}\\left(\\sum_{i=1}^{n}x_{i}^{2}\\right)=n\\mathbb{E}\\left(x_{i}^{2}\\right)=1,\\overleftarrow{\\mathbb{E}}\\left(x_{i}^{2}\\right)=\\frac{1}{n}}\\end{array}$ . $\\mathbb{E}\\left(x_{i}^{4}\\right)=\\mathbb{E}\\left(x_{i}^{2}\\cdot x_{i}^{2}\\right)=$ $\\begin{array}{r}{\\Theta_{r,d_{o u t},d_{i n}}\\left(\\frac{1}{n}\\right)\\Theta_{r,d_{o u t},d_{i n}}\\left(\\frac{1}{n}\\right)=\\Theta_{r,d_{o u t},d_{i n}}\\overbar{\\left(\\frac{1}{n^{2}}\\right)}.}\\end{array}$ . ", "page_idx": 14}, {"type": "text", "text": "For property 2, it can also be proved by symmetry: we can always find vector that contains $(x_{i},-x_{j})$ also lies on the sphere. Therefore, $\\mathbb{E}\\left(\\bar{x}_{i}x_{j}\\right)=0$ . For property $\\begin{array}{r}{3,\\mathbb{E}\\left(x_{i}^{2}x_{j}^{2}\\right)=\\mathbb{E}\\left(x_{i}^{2}\\cdot x_{j}^{2}\\right)=\\Theta_{r,d_{o u t},d_{i n}}\\left(\\frac{1}{n}\\right)\\Theta_{r,d_{o u t},d_{i n}}\\left(\\frac{1}{n}\\right)=\\Theta_{r,d_{o u t},d_{i n}}\\left(\\frac{1}{n^{2}}\\right).}\\end{array}$ ", "page_idx": 14}, {"type": "text", "text": "For property 4, again it can be proved by symmetry: we can always find vector that contains $(x_{i},x_{j},-x_{k})$ also lies on the sphere. Therefore, $\\dot{\\mathbb{E}}\\left(x_{i}^{2}x_{j}\\dot{x}_{k}\\right)^{\\prime}=0$ . \u53e3 ", "page_idx": 14}, {"type": "text", "text": "Lemma A.2. For a randomly selected orthogonal matrix $A\\in\\mathbb{R}^{n\\times n}$ , and we randomly pick two different column vectors x and $y$ from it. For these two vectors, we have the following: ", "page_idx": 14}, {"type": "text", "text": "$L.\\ \\mathbb{E}\\left(x_{i}y_{i}\\right)=0;$ 2 $\\mathbf{\\partial}\\cdot\\ \\mathbb{E}\\left(x_{i}y_{j}\\right)=0,$ ", "page_idx": 14}, {"type": "text", "text": "Proof. It is equivalent to first selecting a random vector $_x$ from a unit sphere in $\\mathbb{R}^{n}$ uniformly, and then selecting the other one $y$ that is orthogonal to $x$ . ", "page_idx": 14}, {"type": "text", "text": "For property $\\begin{array}{r}{\\mathrm{,~}\\sum_{i=1}^{n}x_{i}y_{i}=0\\Rightarrow\\mathbb{E}\\left(\\sum_{i=1}^{n}x_{i}y_{i}\\right)=\\sum_{i=1}^{n}\\mathbb{E}\\left(\\left(\\right)x_{i}y_{i}\\right)=0\\Rightarrow\\mathbb{E}\\left(x_{i}y_{i}\\right)=0.}\\end{array}$ ", "page_idx": 14}, {"type": "text", "text": "For property 2, consider that $\\begin{array}{r}{\\overline{{\\mathbf{\\alpha}}}\\left(\\sum_{i=1}^{n}x_{i}\\right)=\\mathbb{E}\\left(\\sum_{i=1}^{n}y_{i}\\right)=0}\\end{array}$ , and given $_x$ , we can always find $-y$ is also an orthogonal vector. Therefore, $\\begin{array}{r}{\\mathbb{E}\\left(\\sum_{i=1}^{n}x_{i}\\sum_{i=1}^{n}y_{i}\\right)=0\\Rightarrow E(x_{i}y_{i})=0}\\end{array}$ . \u53e3 ", "page_idx": 14}, {"type": "text", "text": "Theorem 3.2. Given the initialization proposed in Theorem 3.1, assume that the orthogonal vectors in $A_{\\mathrm{init}}$ and $B_{\\mathrm{init}}$ are randomly selected from $\\mathbb{R}^{d_{i n}}$ and $\\mathbb{R}^{d_{o u t}}$ , and set $\\eta=\\Theta_{r,d_{o u t},d_{i n}}\\left(\\frac{1}{\\sqrt{r}}\\right)$ as suggested by rsLoRA $l37J$ Under these conditions, the adapters are forward scale-stable $\\begin{array}{r}{i f\\zeta=\\Theta_{r,d_{o u t},d_{i n}}\\left(\\sqrt{\\frac{d_{o u t}}{r^{2}}}\\right)}\\end{array}$ and backward scale-stable $\\begin{array}{r}{f\\zeta=\\Theta_{r,d_{o u t},d_{i n}}\\left(\\sqrt{\\frac{d_{i n}}{r^{2}}}\\right)\\!.}\\end{array}$ ", "page_idx": 14}, {"type": "text", "text": "Proof. In LoRA, $h=(W^{\\prime}+\\eta B A)x$ , since that $W^{\\prime}$ is not considered here, therefore, denote $y\\,=\\,\\eta B A x$ . When backward propagation, it\u2019s like $\\frac{\\partial\\mathcal{L}}{\\partial\\boldsymbol{x}}=\\eta A^{T}B^{T}\\frac{\\partial\\mathcal{L}}{\\partial h}$ . Represente $\\frac{\\partial\\mathcal{L}}{\\partial h}$ as $v$ and $\\frac{\\partial\\mathcal{L}}{\\partial x}$ as $g$ . Therefore, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{{\\displaystyle y_{i}=\\eta\\sum_{j=1}^{r}\\sum_{k=1}^{d_{i n}}B_{i j}A_{j k}x_{k},~{1\\le i\\le d_{o u t}}}}&{{\\displaystyle\\mathrm{(Forward)}}}\\\\ {{\\displaystyle g_{i}=\\eta\\sum_{j=1}^{r}\\sum_{k=1}^{d_{o u t}}A_{j i}B_{k j}v_{k},~{1\\le i\\le d_{i n}}}}&{{\\displaystyle\\mathrm{(Backward)}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Since that the output of each layer in model always passes a softmax function, so that the vector $\\frac{\\partial\\mathcal{L}}{\\partial h}=v$ is $\\Theta_{r,d_{o u t},d_{i n}}$ (1). Further, since that input $x_{i}$ \u2019s are i.i.d., without loss of generality, assume that $E(x_{i})=0$ and $E(x_{i}^{2})=1$ . ", "page_idx": 14}, {"type": "text", "text": "For the adapter, as Equation 4 shows, and by the expectations we have proved in Lemma A.1 and A.2, we can calculate the scale of forward and backward process. ", "page_idx": 14}, {"type": "text", "text": "The scale of forward process is: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}\\left(y_{i}^{2}\\right)=\\eta^{2}\\sum_{j_{1}=1}^{r}\\sum_{j_{2}=1}^{r}\\sum_{k_{1}=1}^{d_{i n}}\\sum_{k_{2}=1}^{d_{i n}}\\mathbb{E}\\left(B_{i j_{1}}A_{j_{1}k_{1}}B_{i j_{2}}A_{j_{2}k_{2}}x_{k_{1}}x_{k_{2}}\\right)}\\\\ {\\displaystyle}&{\\,\\,\\,=\\eta^{2}\\sum_{j_{1}=1}^{r}\\sum_{j_{2}=1}^{r}\\sum_{k_{1}=1}^{d_{i n}}\\sum_{k_{2}=1}^{d_{i n}}\\mathbb{E}\\left(B_{i j_{1}}B_{i j_{2}}\\right)\\mathbb{E}\\left(A_{j_{1}k_{1}}A_{j_{2}k_{2}}\\right)\\mathbb{E}\\left(x_{k_{1}}x_{k_{2}}\\right)}\\\\ {\\displaystyle}&{\\,\\,\\,=\\eta^{2}\\sum_{j_{1}=1}^{r}\\sum_{j_{2}=1}^{r}\\sum_{k=1}^{d_{i n}}\\mathbb{E}\\left(B_{i j_{1}}B_{i j_{2}}\\right)\\mathbb{E}\\left(A_{j_{1}k}A_{j_{2}k}\\right)=\\eta^{2}\\sum_{j=1}^{r}\\sum_{k=1}^{d_{i n}}\\mathbb{E}\\left(B_{i j}^{2}\\right)\\mathbb{E}\\left(A_{j k}^{2}\\right)}\\\\ {\\displaystyle}&{\\,\\,\\,=\\eta^{2}\\sum_{j=1}^{r}\\sum_{k=1}^{d_{i n}}\\frac{\\zeta^{2}}{\\eta^{d}}\\frac{1}{d_{o n t}}\\frac{1}{d_{i n}}=\\frac{1}{\\alpha^{2}}\\cdot\\zeta^{2}\\cdot\\frac{r^{2}}{d_{o n t}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The scale of the backward process is: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\mathbb{E}\\left(g_{i}^{2}\\right)=\\eta^{2}\\sum_{j_{1}=1}^{r}\\sum_{j_{2}=1}^{r}\\sum_{k_{1}=1}^{d_{0u t}}\\sum_{k_{2}=1}^{d_{0u t}}\\mathbb{E}\\left(A_{j_{1}i}B_{k_{1}j_{1}}A_{j_{2}i}B_{k_{2}j_{2}}v_{k_{1}}v_{k_{2}}\\right)}}\\\\ {{\\displaystyle=\\eta^{2}\\sum_{j_{1}=1}^{r}\\sum_{j_{2}=1}^{r}\\sum_{k_{1}=1}^{d_{0u t}}\\sum_{k_{2}=1}^{m}v_{k_{1}}v_{k_{2}}\\mathbb{E}\\left(A_{j_{1}i}A_{j_{2}i}\\right)\\mathbb{E}\\left(B_{k_{1}j_{1}}B_{k_{2}j_{2}}\\right)}}\\\\ {{\\displaystyle=\\eta^{2}\\sum_{j=1}^{r}\\sum_{k=1}^{d_{0u t}}v_{k}^{2}\\mathbb{E}\\left(A_{j_{i}}^{2}\\right)\\mathbb{E}\\left(B_{k,j}^{2}\\right)=\\eta^{2}\\sum_{j=1}^{r}\\sum_{k=1}^{d_{0u t}}v_{k}^{2}\\frac{\\zeta^{2}}{\\eta^{4}}\\frac{1}{d_{i n}}\\frac{1}{d_{o u t}}=\\frac{1}{\\alpha^{2}}\\cdot\\zeta^{2}r^{2}\\Theta_{r,d_{o u t},d_{i n}}\\left(\\frac{1}{d_{i n}}\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "From the results derived by Equation 5 and 6, one can see that we cannot find a proper $\\zeta$ to make both scales $\\Theta_{r,d_{o u t},d_{i n}}$ (1) unless $\\begin{array}{r}{\\frac{d_{o u t}}{d_{i n}}\\,=\\,\\Theta_{r,d_{o u t},d_{i n}}}\\end{array}$ (1). We can also see that the forward scale is stable if adopting $\\zeta=\\Theta_{r,d_{o u t},d_{i n}}\\left(\\frac{d_{o u t}}{r^{2}}\\right)$ and the backward is stable if $\\zeta=\\Theta_{r,d_{o u t},d_{i n}}\\left(\\frac{d_{i n}}{r^{2}}\\right)\\!.$ \u53e3 ", "page_idx": 15}, {"type": "text", "text": "B Additional Experimental Results ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "B.1 Convergence Speed ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "As Figure 3 and 4 shown, the convergence of LoRA-GA is significantly faster than vanilla LoRA and other ablation models, almost close to that of full fine-tuning, which support our claim about the speed of convergence. ", "page_idx": 15}, {"type": "image", "img_path": "VaLAWrLHJv/tmp/1084aa97d8976b195e91e0b7fee0e6f4ca1ba5ce96de6fbb9f968b09a3f9727b.jpg", "img_caption": ["Figure 3: Training Loss curves of Full Fine-tuning, LoRA and LoRA-GA on different datasets. "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "VaLAWrLHJv/tmp/b94943cbd080651ea1087565b71ee6909722a59f5f574f45b3a38ad6340a0f3c.jpg", "img_caption": ["Figure 4: Training Loss curves of different LoRA-GA ablations on different datasets. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "B.2 Evaluating the Rank of the Gradient Matrix ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Theorem 3.1 suggests that the closer the rank of the gradient matrix is to $2r$ , the better the gradient approximated, thereby enhancing the theoretical effectiveness of our initialization. Figure 5 illustrates the low-rank nature of gradient matrices. The left panel depicts a grid-like pattern in the gradients of a weight matrix, indicating a low-rank structure. The middle panel shows a steeply declining curve of singular values, reflecting the highly low-rank nature of the gradient matrix. The right panel presents the cumulative curve of squared singular values, demonstrating that a few ranks account for nearly all the singular values of the gradient matrix. Specifically, the coverage in the right panel is defined as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{Coverage}=\\frac{\\sum_{i=0}^{2r}\\sigma_{i}^{2}}{\\sum_{i=0}^{n}\\sigma_{i}^{2}},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $r$ is the LoRA rank used in LoRA-GA , indicating how much of the low-rank matrix can be approximated by this rank. ", "page_idx": 16}, {"type": "image", "img_path": "VaLAWrLHJv/tmp/6b67533188a5ccc0099d7105804a57613e8efa686f78a96d8f280e3cce95d1f8.jpg", "img_caption": ["Figure 5: (Left) A gradient matrix of T5-Base during fine-tuning on CoLA. (Middle) The decreasing curve of singular values of the gradient matrix. (Right) The cumulative curve showing the coverage of squared singular values. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "We further validate this observation on larger models by analyzing LLaMA 2-7B during MetaMathQA training. Table 8 presents the coverage across different layers with varying LoRA ranks. Even with a relatively small rank of 8, we achieve a mean coverage of $92.9\\%$ across all layers, with the minimum coverage being $85.1\\%$ . Increasing the rank to 128 yields an impressive mean coverage of $99.3\\%$ , with the minimum coverage reaching $97.5\\%$ . These results demonstrate that even for large models with weight matrices of dimension 4096, a modest LoRA rank is sufficient to capture the majority of the gradient information. ", "page_idx": 16}, {"type": "text", "text": "Table 8: Coverage of gradient matrix across different layers in LLaMA 2-7B ", "page_idx": 17}, {"type": "table", "img_path": "VaLAWrLHJv/tmp/6e2f702a08ee7b51f3f585f4d4b6381134a585160cfec8c79b94e24f3a55322b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "B.3 Detailed Ablation Study Result of GLUE ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Table 9 shows the full results of ablation study on the subset of GLUE, where the average scores are briefly reported in Table 4. As Table 9 demonstrated, LoRA-GA outperforms all other ablation models, while both \" $\\mathrm{\\Delta+SO^{\\prime\\prime}}$ and $\"{+}\\mathrm{GA}\"$ methods gain some improvement from vanilla LoRA and simple non-zero initialization \"Gaussian\". This illustrates that both components in LoRA-GA have positive contribution to the improvement of performance. ", "page_idx": 17}, {"type": "table", "img_path": "VaLAWrLHJv/tmp/fd10acdfe17bf2ed78c8bc7d01bb02f4b5d00432efbf38dd065fee5608733a0f.jpg", "table_caption": ["Table 9: Performance comparison of different ablations on subset of GLUE dataset. The settings are elaborated in Table 3. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "B.4 Experimental result with different learning rate ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Furthermore, we also conduct experiments under learning rates 1e-5 and 5e-5. As Table 10 and 11 shown, LoRA-GA maintains strong performance across different learning rates, which illustrating its robustness to the variation of learning rate. ", "page_idx": 17}, {"type": "text", "text": "Table 10: Performance comparison of different methods on MT-Bench, GSM8K, and Human-eval with learning rate 1e-5 ", "page_idx": 17}, {"type": "table", "img_path": "VaLAWrLHJv/tmp/fd5466ba317d5ed9a25247713535b0fdf184e04178b2342dc72141861b0bc085.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Table 11: Performance comparison of different methods on MT-Bench, GSM8K, and Human-eval with learning rate 5e-5 ", "page_idx": 17}, {"type": "table", "img_path": "VaLAWrLHJv/tmp/270c331dfc291dc14e4d9d63290ab960858486bd060dc7e7e40a9edea6c4e724.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "B.5 Experiments on the Full MetaMathQA Dataset ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Following [9], we conducted additional experiments by training on the complete MetaMathQA dataset for multiple epochs, whereas our main results in the previous section were based on fine-tuning for one epoch on the 100k subset of MetaMathQA. Due to computational constraints, we limited these extended experiments to three methods: LoRA[4], LoRA $^{+}$ [36], and LoRA-GA . ", "page_idx": 18}, {"type": "text", "text": "Table 12 presents the performance across four epochs, averaged over two random seeds. ", "page_idx": 18}, {"type": "text", "text": "Table 12: Performance comparison of different methods on full MetaMathQA dataset training for multiple epochs. ", "page_idx": 18}, {"type": "table", "img_path": "VaLAWrLHJv/tmp/ff8292217112a87234fad09af578915059b375b236021668bbf5044fa6adb558.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "The results show that LoRA-GA consistently achieves better performance than vanilla LoRA and outperforms $\\mathrm{LoRA+}$ in most cases across multiple epochs of training. ", "page_idx": 18}, {"type": "text", "text": "C LoRA-GA Initialization With Gradient Accumulation ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Algorithm 2 LoRA-GA Initialization With Gradient Accumulation ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Require: Model $f(\\cdot)$ with $L$ layers, parameters $W$ , sampled batch $B=\\{x,y\\}$ , LoRA rank $r$ with $n$ samples, LoRA alpha $\\alpha$ , loss function $\\mathcal{L}$ , scale factor $\\gamma$ , micro-batch size $b$ ", "page_idx": 18}, {"type": "text", "text": "Ensure: Initialized parameters $W,\\eta,A,B$   \n1: $\\hat{y}\\leftarrow f(x,W)$   \n2: $\\ell\\gets\\mathcal{L}(y,\\hat{y})$   \n3: \u03b7 \u2190\u221a\u03b1r   \n4: for $l=1,\\hdots,L$ do   \n5: $\\nabla_{W_{l}}^{\\mathrm{avg}}\\ell\\gets0$   \n6: end for   \n7: for each micro-batch $B_{i}$ in $B$ do   \n8: y\u02c6i \u2190f(xi, W)   \n9: \u2113i \u2190L(yi, y\u02c6i)   \n10: for $l=L,\\ldots,1$ do   \n11: Compute $\\nabla_{\\boldsymbol{W}_{l}}\\ell_{i}$   \n12: $\\nabla_{W_{l}}^{\\mathrm{avg}}\\ell\\xleftarrow{}\\nabla_{W_{l}}^{\\mathrm{avg}}\\ell+\\nabla_{W_{l}}\\ell_{i}\\cdot\\frac{b}{n}$   \n14: end for   \n15: end for   \n16: for $l=L,\\ldots,1$ do   \n17: $d_{o u t},d_{i n}\\gets\\mathrm{size}(W_{l})$   \n18: $U,S,V\\gets\\mathrm{svd}(\\nabla_{W_{l}}^{\\mathrm{avg}}\\ell)$   \n19: Al \u2190V[1:r] \u00b7 4dout/\u03b3   \n20: Bl \u2190U[r+1:2r] \u00b7 4dout/\u03b3   \n21: Wl \u2190Wl \u2212\u03b7BlAl   \n22: end for   \n23\uff1a return $W,\\eta,A,B$ ", "page_idx": 18}, {"type": "text", "text": "\u25b7Backward pass for one layer $\\triangleright$ Move to CPU \u25b7Gradient for this layer is not needed anymore \u25b7Forward pass for micro-batch \u25b7Initialize average gradient for each layer on CPU ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "D Hyperparameter ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "D.1 Experiments on Natural Language Understanding ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We use the following hyperparameters with T5-Base. ", "page_idx": 18}, {"type": "text", "text": "\u2022 Training Algorithm: AdamW [49] with $\\beta_{1}=0.9$ , $\\beta_{2}=0.999$ , $\\epsilon=1e-8$ and weight decay of 0. For full finetuning, LoRA, and its variants, a learning rate of $1e-4$ , a warmup ratio of 0.03, and cosine decay are employed. For DoRA [8], a learning rate of $2e-4$ is used, while for Adalora, a learning rate of $5e-4$ is applied, both with the same warmup ratio and cosine decay adhering to their respective papers.   \n\u2022 LoRA Hyperparameters: LoRA rank $r\\,=\\,8$ , $\\alpha\\,=\\,16$ . LoRA target is all linear modules except embedding layer, layer norm and language model head.   \n\u2022 LoRA-GA Hyperparameter: $\\gamma=16$ , sampled batch size $s b s=8$   \n\u2022 Other Hyperparameters: Sequence Length $T=128$ , train batch size $b s=32$ , number of train epochs $E=1$ . Precision FP32 ", "page_idx": 19}, {"type": "text", "text": "D.2 Experiment on Large Language Model ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We use the following hyperparameters with Llama 2-7B. ", "page_idx": 19}, {"type": "text", "text": "\u2022 Training Algorithm: AdamW [49] with with $\\beta_{1}=0.9$ , $\\beta_{2}=0.999$ , $\\epsilon=1e-8$ and weight decay of 0. For full finetuning, LoRA, and its variants, a learning rate of $2e-5$ [38], a warmup ratio of 0.03, and cosine decay are employed. For DoRA [8], a learning rate of $2e-4$ is used, while for Adalora, a learning rate of $5e-4$ is applied, both with the same warmup ratio and cosine decay adhering to their respective papers.   \n\u2022 Precision: The backbone model uses bf16 precision, while during training, LoRA\u2019s $B$ and $A$ matrices use fp32 precision, following the implementation of PEFT [39].   \n\u2022 LoRA-GA Hyperparameter: $\\gamma=64$ , micro sampled batch size $s b s=1$ with gradient accumulation of 32.   \n\u2022 LoRA Hyperparameters: LoRA rank $r=8$ and $\\alpha=16$ for all experiments.   \n\u2022 Generation Hyperparameters: All generation is performed with $t o p\\_p\\;=\\;0.95$ and temperature $T=0.8$ .   \n\u2022 Other Hyperparameters: Number of train epochs $E=1$ , train micro batch size $m b s=1$ with gradient accumulation of 32. Sequence Length $T=1024$ ", "page_idx": 19}, {"type": "text", "text": "E Comparison between LoRA-GA and PiSSA ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Both LoRA-GA and PiSSA [38] concentrate on the initialization of LoRA, and utilizing SVD on pre-trained models. While they may appear similar superficially, significant differences exist between them. ", "page_idx": 19}, {"type": "text", "text": "Firstly, the motivations behind LoRA-GA and PiSSA are fundamentally different. As discussed in Section 3.2, LoRA-GA is motivated by the approximation of the LoRA update and full fine-tuning. We employ SVD on gradients solely because the optimal solution to the gradient approximation problem is precisely obtained (as stated in Theorem 3.1). Conversely, PiSSA adopts SVD under the assumption that pre-trained weights possess a low intrinsic rank, and thus, the SVD of weights can provide an accurate representation of original weights. In essence, LoRA-GA emphasizes on gradients and decomposes them, whereas PiSSA concentrates on weights and decomposes them. ", "page_idx": 19}, {"type": "text", "text": "Secondly, LoRA-GA and PiSSA employ different scales of initialization. In Section 3.3, LoRA-GA derives an appropriate scaling factor by considering the forward and backward stability of our initialization scheme. On the other hand, PiSSA uses the largest $r$ singular values as the magnitude of orthogonal matrices directly. ", "page_idx": 19}, {"type": "text", "text": "F Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this paper, we have demonstrated that LoRA-GA can achieve performance comparable to full fine-tuning on the T5-Base (220M) and Llama 2-7B models, while significantly reducing the number of parameters and associated costs. However, due to computational resource constraints, we have not validated LoRA-GA on larger pre-trained models (e.g., Llama 2-70B). ", "page_idx": 19}, {"type": "text", "text": "In LoRA-GA , we proposed that a scaling factor is necessary. But in some experiments with large learning rates, we observed potential numerical instability due to the effect of scaling factor and learning rate. This limitation suggests a need for careful tuning of the scaling factor and learning rate to maintain stability. ", "page_idx": 19}, {"type": "text", "text": "Another limitation pertains to our evaluation scope. While we provide evaluations on MTBench, GSM8K, and Human-eval, we did not assess our method on other datasets. Consequently, we cannot fully guarantee that our findings are universally consistent across all benchmarks. ", "page_idx": 19}, {"type": "text", "text": "Additionally, we did not implement our method on other LoRA variants that are orthogonal to our improvements (e.g., ReLoRA [35]). Therefore, we cannot ascertain whether LoRA-GA would perform equally well with other LoRA architectures/improvements. ", "page_idx": 20}, {"type": "text", "text": "Finally, compared to the original LoRA, LoRA-GA requires double the checkpoint storage, as it necessitates storing both the initial adapter checkpoints $\\dot{A}_{i n i t}$ and $B_{i n i t}$ ) and the final adapter checkpoints $\\overset{\\cdot}{A}$ and $B$ ). ", "page_idx": 20}, {"type": "text", "text": "G Compute Resources ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this paper, we utilized two types of GPUs: the RTX 3090 24GB GPU, supported by a 128-core CPU and 256GB of RAM (hereinafter referred to as \"the RTX 3090\"), and the A100 80GB GPU (hereinafter referred to as \"the A100\"). ", "page_idx": 20}, {"type": "text", "text": "For the experiments on T5-Base using the GLUE dataset, reported in Section 4.1, all computations were performed on a single RTX 3090. For the Llama 2-7B experiments, reported in Section 4.2, full fine-tuning and DoRA scenarios were conducted on a single A100, while all other LoRA variants and LoRA-GA were executed on a single RTX 3090. Additionally, all ablation studies presented in Section 4.3 were carried out on a single RTX 3090. ", "page_idx": 20}, {"type": "text", "text": "H Broader Impacts ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this paper, we identify some limitations of vanilla LoRA and propose a more efficient and effective method for LoRA initialization, LoRA-GA. LoRA-GA converges faster than vanilla LoRA and consistently achieves better evaluation results. ", "page_idx": 20}, {"type": "text", "text": "We believe that this work will have a positive social impact. The primary reasons are as follows: The high cost of training and fine-tuning large models is a significant challenge today. LoRA-GA offers a way to fine-tune with fewer parameters and lower computational costs while still achieving comparable performance. This will reduce the cost of fine-tuning models and, in turn, decrease energy consumption, such as electricity, contributing to the goal of a low-carbon environment. Furthermore, as the size of large language models (LLM) continues to grow, it becomes increasingly difficult for individuals or small organizations to develop their own LLMs. However, with the help of LoRA-GA and open-source large models, the hardware barrier to entry in this area is greatly reduced. This will promote democratization in the field of large models, preventing monopolies and dictatorships by a few companies. ", "page_idx": 20}, {"type": "text", "text": "On the other hand, our method could potentially make it easier to train language models that generate fake news or misleading information. This underscores the necessity for designing effective detectors to identify content generated by large language models (LLMs). Ensuring the responsible use of this technology is crucial to mitigating the risks associated with the misuse of advanced language models. ", "page_idx": 20}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We included 3 contributions at the end of introduction. The method LoRA-GA and stable scale are proposed in Section 3. The extensive experiments and results are in Section 4. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: The last section (Appendix F) of this paper is entirely discuss the limitation of our works. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: There are 1 lemma (Lemma 3.1) and 2 theorems (Theorem 3.1 and 3.2) proposed in our paper. All them are properly proved in Appendix A. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: Hyperparameters are disclosed in Appendix D. Other implementation information are disclosed in Section 4, in paragraphs began with \"Implementation details\". ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 22}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: All codes of our experiments are uploaded to anonymous github. All datasets used in our experiments are all open source, which has been declared and cited in Section 1 (Introduction). ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code. \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details. \u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). ", "page_idx": 22}, {"type": "text", "text": "\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 23}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Details that are important to understand and evaluate experimental results are shown in our paper or appendix like Table 3. Other details can be found in our codes. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.   \n\u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 23}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: We report standard deviations of our evaluation results like footnotes in Table 1, 2 4, 9, 10 and 11. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 23}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Discussed in Appendix G. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: I have read it. I\u2019m sure that our research conforms NeurIPS Code of Ethics. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 24}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: We have discussed this in Appendix H. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 24}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: Our research is purely a foundamental reseach about LoRA, which cannot pose such risks. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 24}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. \u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 25}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "Justification: All models and datasets used in our paper are all properly cited. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 25}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The way to run our codes work is attached in our codes. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 25}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: No human subjects or participants involved in our research. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 25}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: No human subjects or participants involved in our research. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 26}]