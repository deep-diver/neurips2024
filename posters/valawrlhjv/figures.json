[{"figure_path": "VaLAWrLHJv/figures/figures_1_1.jpg", "caption": "Figure 1: (Left) Training loss curves of Llama 2-7B on MetaMathQA to training steps. LoRA-GA converges as quickly as full fine-tuning and outperforms LoRA. (Right) Initialization procedures used in LoRA and LoRA-GA. The key difference is that LoRA-GA initializes adapters using the eigenvectors of the gradient matrix, as opposed to random initialization with a scaling factor.", "description": "This figure shows a comparison of the training loss curves for three different methods: full fine-tuning, LoRA, and LoRA-GA, when training the Llama 2-7B model on the MetaMathQA dataset.  The left panel shows that LoRA-GA converges much faster than vanilla LoRA, achieving a speedup of 5x in the first 200 steps, and converging at a similar rate to full fine-tuning. The right panel illustrates the key difference between the initialization procedures of LoRA and LoRA-GA. LoRA uses random initialization for its adapter weights (A and B), while LoRA-GA leverages the eigenvectors of the gradient matrix to initialize the adapters, leading to faster convergence.", "section": "3 Methods"}, {"figure_path": "VaLAWrLHJv/figures/figures_8_1.jpg", "caption": "Figure 1: (Left) Training loss curves of Llama 2-7B on MetaMathQA to training steps. LORA-GA converges as quickly as full fine-tuning and outperforms LoRA. (Right) Initialization procedures used in LoRA and LoRA-GA. The key difference is that LoRA-GA initializes adapters using the eigenvectors of the gradient matrix, as opposed to random initialization with a scaling factor.", "description": "The figure on the left shows training loss curves for Llama 2-7B model on the MetaMathQA dataset, comparing LoRA-GA, vanilla LoRA, and full fine-tuning.  LoRA-GA demonstrates a convergence rate similar to full fine-tuning, significantly faster than vanilla LoRA.  The figure on the right illustrates the initialization methods of LoRA and LoRA-GA, highlighting the key difference: LoRA-GA uses gradient approximation to align low-rank matrix gradients with those of the full model at the initial step, unlike LoRA's random initialization with a scaling factor.", "section": "3 Methods"}, {"figure_path": "VaLAWrLHJv/figures/figures_15_1.jpg", "caption": "Figure 1: (Left) Training loss curves of Llama 2-7B on MetaMathQA to training steps. LORA-GA converges as quickly as full fine-tuning and outperforms LoRA. (Right) Initialization procedures used in LoRA and LoRA-GA. The key difference is that LoRA-GA initializes adapters using the eigenvectors of the gradient matrix, as opposed to random initialization with a scaling factor.", "description": "The figure on the left shows the training loss curves for Llama 2-7B model on MetaMathQA dataset.  It compares the convergence speed of three methods: full fine-tuning, vanilla LoRA, and the proposed LoRA-GA.  The results show that LoRA-GA converges as fast as full fine-tuning and significantly faster than vanilla LoRA. The figure on the right illustrates the initialization procedures for LoRA and LoRA-GA.  LoRA uses random initialization with a scaling factor, while LoRA-GA initializes its adapters using eigenvectors of the gradient matrix, leading to faster convergence.", "section": "3 Methods"}, {"figure_path": "VaLAWrLHJv/figures/figures_16_1.jpg", "caption": "Figure 1: (Left) Training loss curves of Llama 2-7B on MetaMathQA to training steps. LORA-GA converges as quickly as full fine-tuning and outperforms LoRA. (Right) Initialization procedures used in LoRA and LoRA-GA. The key difference is that LoRA-GA initializes adapters using the eigenvectors of the gradient matrix, as opposed to random initialization with a scaling factor.", "description": "The left panel of the figure shows the training loss curves for Llama 2-7B model on the MetaMathQA dataset.  It compares the convergence speed of three different methods: full fine-tuning, vanilla LoRA, and the proposed LoRA-GA.  The results show that LoRA-GA achieves a convergence rate comparable to full fine-tuning, significantly faster than vanilla LoRA. The right panel illustrates the initialization procedures for both LoRA and LoRA-GA.  It highlights that the key difference lies in the adapter initialization: LoRA uses random initialization, while LoRA-GA uses the eigenvectors of the gradient matrix, leading to improved convergence.", "section": "3 Methods"}, {"figure_path": "VaLAWrLHJv/figures/figures_16_2.jpg", "caption": "Figure 1: (Left) Training loss curves of Llama 2-7B on MetaMathQA to training steps. LORA-GA converges as quickly as full fine-tuning and outperforms LoRA. (Right) Initialization procedures used in LoRA and LoRA-GA. The key difference is that LoRA-GA initializes adapters using the eigenvectors of the gradient matrix, as opposed to random initialization with a scaling factor.", "description": "The figure on the left shows the training loss curves for three different methods: Full Fine-tuning, LoRA, and LoRA-GA.  It demonstrates that LoRA-GA converges much faster, achieving a similar convergence rate to full fine-tuning, and significantly outperforming the original LoRA method. The figure on the right provides a visual illustration of the initialization procedures for both LoRA and LoRA-GA, highlighting the key difference in how they initialize adapter matrices. LoRA-GA uses the eigenvectors of the gradient matrix for initialization, aiming to align gradients more closely with full fine-tuning, unlike LoRA's random initialization with a scaling factor.", "section": "3 Methods"}]