[{"type": "text", "text": "Strategic Littlestone Dimension: Improved Bounds on Online Strategic Classification\u2217 ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Saba Ahmadi\u2020,\\*, Kunhe Yang\u2021,\\*, and Hanrui Zhang\u00a7,\\* ", "page_idx": 0}, {"type": "text", "text": "\u2020Toyota Technological Institute at Chicago, saba@ttic.edu \u2021University of California, Berkeley, kunheyang@berkeley.edu \u00a7Chinese University of Hong Kong, hanrui@cse.cuhk.edu.hk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We study the problem of online binary classification in settings where strategic agents can modify their observable features to receive a positive classification. We model the set of feasible manipulations by a directed graph over the feature space, and assume the learner only observes the manipulated features instead of the original ones. We introduce the Strategic Littlestone Dimension, a new combinatorial measure that captures the joint complexity of the hypothesis class and the manipulation graph. We demonstrate that it characterizes the instanceoptimal mistake bounds for deterministic learning algorithms in the realizable setting. We also achieve improved regret in the agnostic setting by a refined agnostic-to-realizable reduction that accounts for the additional challenge of not observing agents\u2019 original features. Finally, we relax the assumption that the learner knows the manipulation graph, instead assuming their knowledge is captured by a family of graphs. We derive regret bounds in both the realizable setting where all agents manipulate according to the same graph within the graph family, and the agnostic setting where the manipulation graphs are chosen adversarially and not consistently modeled by a single graph in the family. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Strategic considerations in machine learning have gained significant attention during the past decades. When ML algorithms are used to assist decisions that affect a strategic entity (e.g., a person, a company, or an LLM agent), this entity \u2014 henceforth the agent \u2014 naturally attempts to game the ML algorithms into making decisions that better serve the agent\u2019s goals, which in many cases differ from the decision maker\u2019s. Examples include loan applicants optimizing their credit score without actually improving their financial situation.2 It is therefore desirable, if not imperative, that the ML algorithms used for decision-making be robust against such strategic manipulation. ", "page_idx": 0}, {"type": "text", "text": "Indeed, extensive effort has been made towards designing ML algorithms in the presence of strategic behavior, shaping the research area of strategic machine learning [Br\u00fcckner and Scheffer, 2011, Hardt et al., 2016]. In particular, powerful frameworks have been proposed for offilne environments, where the decision maker has access to historical data, on which they train a model that is subsequently used to make decisions about (i.e., to classify) members of a static population. These frameworks provide almost optimal learnability results and sample complexity bounds for strategic machine learning in offilne environments, which gracefully generalize their non-strategic counterparts (see, e.g., [Zhang and Conitzer, 2021, Sundaram et al., 2023]). ", "page_idx": 0}, {"type": "text", "text": "However, the situation becomes subtler in online environments, where the decision maker has little or no prior knowledge about the population being classified, and must constantly adjust the decision-making policy (i.e., the classifier) through trial and error. This is particularly challenging in the presence of strategic behavior, because often the decision maker can only observe the agent\u2019s features after manipulation. In such online environments, the performance of a learning algorithm is often measured by its regret, i.e., how many more mistakes it makes compared to the best classifier within a certain family in hindsight. For online strategic classification, while progress has been made in understanding the optimal regret in several important special cases, a full instance-wise characterization has been missing, even in the seemingly basic realizable setting (meaning that there always exists a perfect classifier in hindsight). This salient gap is the starting point of our investigation in this paper \u2014 which turns out to reach quite a bit beyond the gap itself. ", "page_idx": 1}, {"type": "text", "text": "Following prior work [Ahmadi et al., 2023, Cohen et al., 2024], we study the following standard and general model of online strategic classification: we have a (possibly infinite) feature space, equipped with a manipulation graph defined over it. An edge between two feature vectors $x_{1}$ and $x_{2}$ means that an agent whose true features are $x_{1}$ can pretend to have features $x_{2}$ , and in fact, the agent would have incentives to do so if the label assigned to $x_{2}$ by the classifier is better than that assigned to the true features $x_{1}$ . At each time step, the decision maker commits to a classifier (which may depend on observations from previous interactions), and an agent arrives and observes the classifier. The agent then responds to the classifier by reporting (possibly nontruthfully) a feature vector that leads to the most desirable label subject to the manipulation graph, i.e., the reported feature vector must be a neighbor of the agent\u2019s true feature vector. The decision maker then observes the reported feature vector, as well as whether the label assigned to that feature vector matches the agent\u2019s true label. ", "page_idx": 1}, {"type": "text", "text": "1.1 Our Results and Techniques ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "An instance-optimal regret bound through the strategic Littlestone dimension. Our first main finding is an instance-optimal regret bound for online strategic classification in the realizable setting, when randomization is not allowed (we will discuss the role of randomization in Section 6). In this setting, there is a predefined hypothesis class of classifiers, in which there must exist one classifier that assigns all agents their true labels under manipulation. The decision maker, knowing that a perfect classifier exists in this class, tries to learn it on the fly while making as few mistakes as possible in the process. Naturally, the richer this hypothesis class is, the harder the decision maker\u2019s task will be (e.g., one extreme is when the hypothesis class contains only one classifier, and the decision maker knows a priori that that classifier must be perfectly correct, and the optimal regret is 0). Thus, the optimal regret must depend on the richness of the hypothesis class. Similarly, one can imagine that the optimal regret must also depend on the manipulation graph. ", "page_idx": 1}, {"type": "text", "text": "Previous work [Ahmadi et al., 2023, Cohen et al., 2024] has established regret bounds for this setting based on various complexity measures of the hypothesis class and the manipulation graph, including the size and the (classical) Littlestone dimension [Littlestone, 1988] of the hypothesis class, as well as the maximum out-degree of the manipulation graph. However, the optimality (when applicable) of these bounds only holds under the assumption that the mistake bound must be parametrized as a function on the classical Littlestone dimension and/or the graph\u2019s out-degree. However, these bounds are not tight for all instances, as there exist problem instances that are learnable where all these parameters are infinite. ", "page_idx": 1}, {"type": "text", "text": "To address the above issue, we introduce a new combinatorial complexity measure that generalizes the classical Littlestone dimension into strategic settings. Conceptually, the new notion also builds on the idea of \u201cshattered trees\u201d, which has proved extremely useful in classical settings. However, the asymmetry introduced by strategic behavior3 demands a much more delicate construction of shattered trees (among other intriguing implications to be discussed in Section 3). We show that the generalized Littlestone dimension captures precisely the optimal regret of any deterministic learning algorithm given a particular hypothesis class and a manipulation graph, thereby providing a complete characterization of learnability in this setting. Being instance-optimal, our bound strengthens and unifies all previous bounds for online strategic classification in the realizable setting. ", "page_idx": 1}, {"type": "text", "text": "An improved regret bound for the agnostic case. We then proceed to the agnostic setting, where no hypothesis necessarily assigns correct labels to all agents. The regret is defined with respect to the best hypothesis in hindsight. Compared to the classical (i.e., non-strategic) setting, the main challenge is incomplete information: since the learner cannot observe original features, upon observing the behavior of an agent under one classifier, it is not always possible to counterfactually infer what would have been observed if the learner used another classifier. ", "page_idx": 2}, {"type": "text", "text": "To understand why this can be a major obstacle, recall some high-level ideas behind the algorithms for classical agnostic online classification (see, e.g., [Ben-David et al., 2009]). The key is to construct a finite set of \u201crepresentative\u201d experts out of the potentially infinite set of hypotheses, such that the best expert performs almost as well as the best hypothesis in hindsight. An agnostic learner then runs a no-regret learning algorithm (such as multiplicative weights) on the expert set, which in the long run matches the performance of the best expert, and in turn of the best hypothesis. ", "page_idx": 2}, {"type": "text", "text": "The partial information challenge appears in both steps of the above approach. First, to construct the set of representative experts, the learner needs to simulate the observation received by each hypothesis, had that hypothesis been used to label the strategic agents. Second, the no-regret algorithm on the expert set also needs to counterfacurally infer the agent\u2019s response to each expert. To circumvent both issues, we design a nuanced construction of the representative set of experts that effectively \u201cguesses\u201d each potential direction of the agent\u2019s manipulation. We then run the biased voting approach introduced in [Ahmadi et al., 2023] on the finite set of experts, which enjoys regret guarantees in the strategic setting even with partial information. Combined with our regret bound for the realizable setting, this approach yields an improved bound for the agnostic setting. ", "page_idx": 2}, {"type": "text", "text": "Learning with unknown manipulation graphs. Our last result focuses on relaxing the assumption that the learner has perfect knowledge about the manipulation graph structure. Instead, following previous works [Lechner et al., 2023, Cohen et al., 2024], we model their knowledge about the manipulation graph using a pre-defined graph class, which to some degree reflects the true set of feasible manipulations. In this setting, our work is the first that provides positive results when the learner only observes features after manipulation. We start with the realizable setting where the manipulation graphs are consistently modeled by the same (unknown) graph in the class. In this setting, we provide a more careful construction of the representative experts to account for the additional challenge of unknown graphs. Combing this construction with re-examining the effectiveness of the biased voting approach [Ahmadi et al., 2023] when the input graph is a overlyconservative estimate of the true graph, we obtain the first regret bound in this setting that is approximately optimal (up to logarithmic factors) in certain instances. We also extend our results to fully agnostic settings where the agents in each round manipulates according to a potentially different graph, and the best graph in the class has nonzero error in modeling all the manipulations. ", "page_idx": 2}, {"type": "text", "text": "1.2 Further Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "There is a growing line of research that studies learning from data provided by strategic agents [Dalvi et al., 2004, Dekel et al., 2008, Br\u00fcckner and Scheffer, 2011]. The seminal work of Hardt et al. [2016] introduced the problem of strategic classification as a repeated game between a mechanism designer that deploys a classifier and an agent that best responds to the classifier by modifying their features at a cost. Follow-up work studied different variations of this model, in an online learning setting [Dong et al., 2018, Chen et al., 2020, Ahmadi et al., 2021], incentivizing agents to take improvement actions rather than gaming actions [Kleinberg and Raghavan, 2020, Haghtalab et al., 2020, Alon et al., 2020, Ahmadi et al., 2022], causal learning [Bechavod et al., 2021, Perdomo et al., 2020], screening processes [Cohen et al., 2023], fairness [Hu et al., 2019], etc. ", "page_idx": 2}, {"type": "text", "text": "Two different models for capturing the set of plausible manipulations have been considered in the literature. The first one is a geometric model, where the agent\u2019s best-response to the mechanism designer\u2019s deployed classifier is a state within a bounded distance (with respect to some $\\ell_{p}$ norm) from the original state, i.e. feature set [Dong et al., 2018, Chen et al., 2020, Shao et al., 2024, Sundaram et al., 2023, Ghalme et al., 2021, Haghtalab et al., 2020]. In the second model, introduced by Zhang and Conitzer [2021] there is a combinatorial structure, i.e. manipulation graph, that captures the agent\u2019s set of plausible manipulations. This model has been studied in both offline PAC learning [Lechner and Urner, 2022, Zhang and Conitzer, 2021, Lechner et al., 2023] and online settings [Ahmadi et al., 2023]. In a recent work, Lechner et al. [2023] consider this problem in an offline setting where the underlying manipulation is unknown and belongs to a known family of graphs. Our work improves the results given by [Ahmadi et al., 2023] in the online setting and also extends their results to the setting where the underlying manipulation is unknown and belongs to a known family of graphs. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Our work is also closely related to that of Cohen et al. [2024], with two main points of distinction. First, in the realizable setting, their bound is shown to be optimal for a specific instance, whereas our bound is instance-wise optimal. Second, in both the agnostic and the unknown graph settings, they assume that agents\u2019 original features are observable before the learner makes decisions, whereas our algorithm only requires access to post-manipulation features. ", "page_idx": 3}, {"type": "text", "text": "Finally, our work is also tangentially connected to several recent advances in understanding multiclass classification under bandit feedback, e.g., [Raman et al., 2024, Filmus et al., 2024], as the false-positive mistake types can be treated as multiple labels at a very abstract level. However, an additional challenge in the strategic setting is that the learner needs to choose a classifier without observing the original instance to be labeled. ", "page_idx": 3}, {"type": "text", "text": "2 Model and Preliminaries ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "2.1 Strategic classification. ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Let $\\mathcal{X}$ be a space of feature vectors, ${\\boldsymbol{\\mathcal{V}}}=\\{-1,1\\}$ be the binary label space, and $\\mathcal{H}:\\mathcal{X}\\xrightarrow{}\\mathcal{Y}$ be a hypothesis class that is known to the learner (also referred to as \u201cdecision-makers\u201d). In the strategic classification setting, agents prefer positive labels over negative labels, and they may manipulate their features within a predefined range to receive a positive label. We use the manipulation graphs introduced by [Zhang and Conitzer, 2021, Lechner and Urner, 2022] to model the set of feasible manipulations. The manipulation graph $G(\\mathcal{X},\\mathcal{E})$ is a directed graph in which each node corresponds to a feature vector in $\\mathcal{X}$ , and each edge $(x_{1},x_{2})\\in\\mathcal{E}\\subseteq\\mathcal{X}^{2}$ represents that an agent with initial feature vector $x_{1}$ can modify their feature vector to $x_{2}$ . For each $x\\in\\mathscr{X}$ , we use $N_{G}^{+}(x)$ to denote the set of out-neighbors of $x$ in $G$ , excluding $x$ itself, and $N_{G}^{+}[x]$ to denote the out-neighbors including $x$ . Formally, $N_{G}^{+}(x)=\\{x^{\\prime}\\in\\mathcal{X}\\setminus\\{x\\}\\mid(x,x^{\\prime})\\in\\mathcal{E}\\}$ and $N_{G}^{+}[x]=\\{x\\}\\cup N_{G}^{+}(x)$ . Similarly, we use $N_{G}^{-}(x)$ and $N_{G}^{-}[x]$ to denote respectively the exclusive and inclusive in-neighborhood of $x$ . ", "page_idx": 3}, {"type": "text", "text": "Agents\u2019 utility and the manipulation rule. Given a manipulation graph $G(\\mathcal{X},\\mathcal{E})$ and a classifier $h\\,\\in\\,\\mathcal{V}^{\\mathcal{X}}$ , an agent with initial features $x\\,\\in\\,{\\mathcal{X}}$ aims to maximize their utility by potentially manipulating their features to a different $x^{\\prime}$ . The agent\u2019s utility function ut $\\ensuremath{\\vert{_{G,h}}(\\boldsymbol{x},\\boldsymbol{x}^{\\prime})}$ is defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathsf{u t i l}_{G,h}(x,x^{\\prime})=h(x^{\\prime})-\\infty\\cdot\\mathbb{1}\\{(x,x^{\\prime})\\in\\mathcal{E}\\}\\,,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the agent\u2019s utility is the classification outcome $h(x^{\\prime})$ minus the manipulation cost associated with changing features from $x$ to $x^{\\prime}$ . For the classification outcome, agents receive utility $+1$ if the classifier $h$ labels $x^{\\prime}$ as positive and $-1$ otherwise. For the manipulation cost, moving from $x$ to $x^{\\prime}$ incurs no cost if the two features are connected by an edge in $G$ , but when $x$ and $x^{\\prime}$ are not connected (i.e., $(x,x^{\\prime})\\notin\\mathcal{E})$ , the manipulation incurs an infinite cost, effectively making such a manipulation infeasible. As a result, an agent with initial features $x$ would move to some $x^{\\prime}$ in the inclusive neighborhood $N_{G}^{+}[x]$ that is labeled as positive by $h$ , if such a node exists. ", "page_idx": 3}, {"type": "text", "text": "Formally, the set of best response features from $x$ , denoted by $\\mathsf{B R}_{G,h}(x)$ , is defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\n{\\mathsf{B R}}_{G,h}(x)\\triangleq\\{x^{\\prime}\\mid{\\mathsf{u t i l}}_{G,h}(x,x^{\\prime})=+1\\}=N_{G}^{+}[x]\\cap\\{x\\mid h(x)=+1\\}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "If, however, the set $\\mathsf{B R}_{G,h}(x)$ is empty \u2014 indicating that the entire out-neighborhood $N_{G}^{+}[x]$ is labeled as negative by $h$ \u2014 then the agent is assumed to not manipulate their features and remain at $x$ . In addition, when there are multiple nodes in $\\mathsf{B R}_{G,h}(x)$ , the agent may select any node arbitrarily. We do not require the selection to be consistent across rounds or to follow a pre-specified rule. Our results specifically focus on the learner\u2019s mistakes against worst-case (adversarial) selections by the agent.4Finally, the manipulated feature vector is denoted by $\\mathsf{b r}_{G,h}(x)$ . ", "page_idx": 3}, {"type": "text", "text": "The labels induced by the manipulated feature vectors br $\\scriptstyle{\\mathrm{{\\omega}}}_{G,h}(x)$ are captured by effective classifiers $\\widetilde{h}_{G}$ , formally defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\widetilde{h}_{G}(x)\\triangleq h(\\mathsf{b r}_{G,h}(x))=\\left\\{\\!\\!+\\!1,\\quad\\mathrm{if}\\ \\exists v\\in N_{G}^{+}[x],\\ \\mathrm{s.t.}\\ h(v)=+1\\!;\\!\\begin{array}{l}{}\\\\ {-1,\\quad\\mathrm{otherwise}.}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Online learning. We consider an online strategic classification setting modeled as a repeated game between the learner (aka the decision maker) and an adversary over $T$ rounds, where the learner make decisions according to an online learning algorithm $\\boldsymbol{\\mathcal{A}}$ . At each round $t\\,\\in\\,[T]$ , the learner first commits to the classifier $h_{t}\\in\\mathcal{N}^{\\mathcal{X}}$ (not necessarily restricted to $\\mathcal{H}$ ) that is generated by $\\boldsymbol{\\mathcal{A}}$ . The adversary then selects an agent $\\left({{x}_{t}},{{y}_{t}}\\right)$ where $x_{t}\\in\\mathscr{X}$ is the original feature vector and $y_{t}\\in\\mathcal{V}$ is the true label. In response to $h_{t}$ , the agent manipulates their features from $x_{t}$ to $v_{t}=\\mathsf{b r}_{G,h}(x_{t})$ . Consequently, the learner observes the manipulated features $v_{t}$ (instead of $x_{t}$ ), and incurs a mistake if $y_{t}\\neq h_{t}(v_{t})$ . We use $S=(x_{t},y_{t})_{t\\in[T]}$ to denote the sequence of agents. ", "page_idx": 4}, {"type": "text", "text": "The learner aims to minimize the Stackelberg regret on $S$ with respect to the optimal hypothesis $h^{\\star}\\in\\mathcal{H}$ had the agents responded to $h^{\\star}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathsf{R e g r e t}_{\\mathcal{A}}(S,\\mathcal{H},G)\\triangleq\\sum_{t=1}^{T}\\mathbb{1}\\{h_{t}(\\mathsf{b r}_{G,h_{t}}(x_{t}))\\neq y_{t}\\}-\\operatorname*{min}_{h^{\\star}\\in\\mathcal{H}}\\sum_{t=1}^{T}\\mathbb{1}\\{h^{\\star}(\\mathsf{b r}_{G,h^{\\star}}(x_{t}))\\neq y_{t}\\}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We call a sequence $S$ realizable with respect to $\\mathcal{H}$ if the optimal-in-hindsight hypothesis $h^{\\star}\\in\\mathcal{H}$ achieves zero mistakes on $S$ . Specifically, this means that for all $\\left({{x}_{t}},{{y}_{t}}\\right)$ in the sequence $S$ , we have $y_{t}=h^{\\star}(\\mathsf{b r}_{G,h^{\\star}}(x_{t}))=\\widetilde{h}_{G}^{\\star}(x_{t})$ . In such cases, the learner\u2019s regret coincides with the number of mistakes made. We use Mis take $\\textstyle{\\boldsymbol{\\omega}}({\\boldsymbol{\\mathcal{H}}},G)$ to denote the maximal number of mistakes that $\\boldsymbol{\\mathcal{A}}$ makes against any realizable sequence with respect to class $\\mathcal{H}$ and graph $G$ . A deterministic algorithm is called minmax optimal or instance-optimal if achieves the minimal Mistake ${\\mathcal{A}}({\\mathcal{H}},G)$ across all deterministic algorithms5. We denote this optimal mistake bound by $\\mathcal{M}(\\mathcal{H},G)\\triangleq\\operatorname*{inf}_{A}$ deterministic Mistake ${\\mathcal{A}}({\\mathcal{H}},G)$ . ", "page_idx": 4}, {"type": "text", "text": "2.2 Classical Littlestone Dimension ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we revisit the classical online binary classification setting where the agents are unable to strategically manipulate their features. This setting can be viewed as a special case of strategic classification where the manipulation graph $G$ consists solely of isolated nodes. We will introduce the characterization of the optimal mistake in this classical setting \u2014 known as the Littlestone Dimension \u2014 which inspires our analysis in the strategic setting. ", "page_idx": 4}, {"type": "text", "text": "Definition 2.1 $\\mathcal{H}$ -Shattered Littlestone Tree). A Littlestone tree shattered by hypothesis class $\\mathcal{H}$ of depth d is a binary tree where: ", "page_idx": 4}, {"type": "text", "text": "\u2022 (Structure) Nodes are labeled by $\\mathcal{X}$ and each non-leaf node has exactly two outgoing edges that are labeled by $+1$ and $-1$ , respectively.   \n\u2022 (Consistency) For every root-to-leaf path $x_{1}\\xrightarrow{y_{1}}x_{2}\\xrightarrow{y_{2}}\\cdots x_{d}\\xrightarrow{y_{d}}x_{d+1}$ where $x_{1}$ is the root node and each $y_{t}$ is the edge connecting $x_{t}$ and $x_{t+1}$ , there exists a hypothesis $h\\in\\mathcal H$ that is consistent with the entire path, i.e., $\\forall t\\leq d$ , $h(x_{t})=y_{t}$ . ", "page_idx": 4}, {"type": "text", "text": "The above tree structure intuitively models an adversary\u2019s strategy to maximize the learner\u2019s mistakes, where each node $x_{t}$ represents the unlabeled instance to be presented to the learner, and $y_{t}$ represents the type of mistake (either a false positive or a false negative) that the adversary aims to induce. For example, if the learner predicts the label of $x_{t}$ to be $\\widehat{y}_{t}=+1$ , then the adversary will declare $y_{t}=-1$ , enforce a false positive mistake, and choose th e  next instance $x_{t+1}$ as the children of the current node along the $-1$ edge. In addition, the consistency requirement guarantees that the resulting input sequence is realizable by some classifier in $\\mathcal{H}$ . ", "page_idx": 4}, {"type": "text", "text": "Definition 2.2 (Littlestone Dimension). The Littlestone dimension of class $\\mathcal{H}$ , denoted as Ldim $(\\mathcal{H})$ , is the maximum integer d such that there exists an $\\mathcal{H}$ -shattered Littlestone of depth $d$ . ", "page_idx": 4}, {"type": "text", "text": "The interpretation of the true structure immediately implies that the mistake of any algorithm should be lower bounded by $\\mathsf{L d i m}(\\mathcal{H})$ . Moreover, a seminal result by Littlestone [1988] also showed that an online learning algorithm known as the Standard Optimal Algorithm (SOA, see Algorithm 2 in ", "page_idx": 4}, {"type": "text", "text": "Appendix A) can achieve this lower bound. Together, they form a complete characterization of the optimal mistake bound in the classical setting, which we summarize in the following proposition. ", "page_idx": 5}, {"type": "text", "text": "Proposition 2.1 (Optimal Mistake Bound [Littlestone, 1988]). Let $\\mathcal{M}(\\mathcal{H})$ be the optimal mistake in the classical online learning setting, then $\\mathcal{M}(\\mathcal{H})=\\mathsf{L d i m}(\\mathcal{H})$ . ", "page_idx": 5}, {"type": "text", "text": "In the next section, we will discuss the challenges of extending Littlestone\u2019s characterization to the strategic setting, and present our solution. ", "page_idx": 5}, {"type": "text", "text": "3 The Strategic Littlestone Dimension ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we will introduce a new combinatorial dimension called the Strategic Littlestone Dimension, and show that it characterizes the minmax optimal mistake bound for strategic classification. ", "page_idx": 5}, {"type": "text", "text": "Inspired by the classical Littlestone dimension, we hope to use a tree structure to model an adversary\u2019s strategy for selecting agents $\\left({{x}_{t}},{{y}_{t}}\\right)$ , where nodes serve as (proxies of) the initial feature vector of each agent, and edges represent the types of mistakes that the adversary can induce. However, since agents can strategically manipulate their features, the potential mistakes associated with the same initial feature vector could manifest in many more types depending on the learner\u2019s choice of classifiers. Specifically, let $x$ be the initial feature vector. Then a mistake associated with $x$ might be observed as a false negative mistake at node $x$ (denoted as $(x,+1)$ where $x$ is the observable node and $+1$ is the true label), or as a false positive mistake at any outgoing neighbor $v$ of $x$ (denoted as $(v,-1)$ accordingly). Therefore, an adversary\u2019s strategy should accommodate all such possibilities, which necessitates the strategic Littlestone tree to contain branches representing all potential mistake types. ", "page_idx": 5}, {"type": "text", "text": "Another challenge is caused by the mismatch of the information available to the learner and the adversary. Since the learner only observes manipulated features instead of the true ones, the amounts of information carried by false positive and false negative mistakes are inherently asymmetric. False negatives provide full-information feedback as the manipulated and original features are identical. However, false positives introduce uncertainty about the original features, which could potentially be any in-neighbor of the observed one. As a result, a hypothesis is deemed \u201cconsistent\u201d with a false positive observation as long as it can correctly label any one of the potential original nodes. ", "page_idx": 5}, {"type": "text", "text": "Now we formally introduce the strategic Littlestone tree with adapted branching and consistency rules tailored for strategic classification. See Figure 1 for a pictorial illustration. ", "page_idx": 5}, {"type": "text", "text": "Definition 3.1 ( $\\mathcal{H}$ -Shattered Strategic Littlestone Tree). A Strategic Littlestone tree for hypothesis class $\\mathcal{H}$ under graph $G$ with depth $d$ is a tree where: ", "page_idx": 5}, {"type": "text", "text": "\u2022 (Structure) Nodes are labeled by $\\mathcal{X}$ . The set of outgoing edges from each non-leaf node $x$ are: one false negative edge $(x,+1)$ , and a set of false positive edges $\\{(v,-1)\\mid v\\in N_{G}^{+}[x]\\}$ . \u2022 (Consistency) For every root-to-leaf path x\u20321 \u2212(v\u22121\u2212,y\u22121)\u2192x\u20322 $x_{1}^{\\prime}\\xrightarrow{(v_{1},y_{1})}x_{2}^{\\prime}\\xrightarrow{(v_{2},y_{2})}\\cdots x_{d}^{\\prime}\\xrightarrow{(v_{d},y_{d})}x_{d+1}^{\\prime}$ where $\\boldsymbol{x}_{1}^{\\prime}$ is the root node and $(v_{t},y_{t})\\,\\in\\,\\mathcal{X}\\,\\times\\,\\{\\pm1\\}$ is the edge that connects $\\ensuremath{\\boldsymbol{{x}}}_{t}^{\\prime}$ and $x_{t+1}^{\\prime}$ , there exists a hypothesis $h\\in\\mathcal H$ that is consistent with the entire path. Specifically, $\\forall t\\leq d$ , $\\exists x_{t}$ s.t. $\\widetilde{h}_{G}(x_{t})=y_{t}$ , where $x_{t}$ satisfies $x_{t}=v_{t}$ if $\\mathrm{\\Delta}y_{t}=+1$ , and $x_{t}\\in N_{G}^{-1}[v_{t}]$ if $y_{t}=-1$ . ", "page_idx": 5}, {"type": "text", "text": "Definition 3.2 (Strategic Littlestone Dimension). The Strategic Littlestone Dimension of a hypothesis class $\\mathcal{H}$ under graph $G$ , denoted with SLdim $(\\mathcal{H},G)$ , is defined as the largest nonnegative integer $d$ for which there exists a Strategic Littlestone tree of depth $d$ shattered by $\\mathcal{H}$ under graph $G$ . ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.1 (Minmax optimal mistake for strategic classification). For any hypotheses class $\\mathcal{H}$ and manipulation graph $G$ , the minmax optimal mistake in the realizable setting is captured by the strategic Littlestone dimension, i.e., $\\mathcal{M}(\\mathcal{H},G)=\\mathsf{S L d i m}(\\mathcal{H},G)$ . ", "page_idx": 5}, {"type": "text", "text": "We divide the proof of Theorem 3.1 into two parts: the lower bound direction is established in Theorem 3.2, and the upper bound direction is established in Theorem 3.3. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.2 (Lower bound part of Theorem 3.1). For any pair of hypothesis class $\\mathcal{H}$ and manipulation graph $G$ , any deterministic online learning algorithm $\\boldsymbol{\\mathcal{A}}$ must suffer a mistake lower bound of Mistake $\\underline{{\\boldsymbol{A}}}(\\mathcal{H},G)\\geq\\mathsf{S L d i m}(\\mathcal{H},G)$ . ", "page_idx": 5}, {"type": "text", "text": "Proof sketch of Theorem 3.2. Let $\\tau$ be a strategic Littlestone tree for $(\\mathcal{H},G)$ with depth $d$ . We will show that for any deterministic algorithm $\\boldsymbol{\\mathcal{A}}$ , there exists an adversarial sequence of agents ", "page_idx": 5}, {"type": "image", "img_path": "4Lkzghiep1/tmp/a0b592f8740b4266c3c8ec03aefc5999419b0baefa89fcf82a7081e50fb6f411.jpg", "img_caption": ["Figure 1: A Strategic Littlestone Tree with depth 2. False negative edges are marked red, whereas false positive edges are marked blue. The highlighted path x\u20321\u2212(v\u22121\u2212,y\u22121)\u2192 $x_{1}^{\\prime}\\xrightarrow{(v_{1},y_{1})}x_{2}^{\\prime}\\xrightarrow{(v_{2},y_{2})}x_{3}^{\\prime}$ is an example root-to-leaf path. In this path, the first observation $\\left(v_{1},y_{1}\\right)$ is a false positive, which satisfies $\\bar{v}_{1}\\in N_{G}^{+}[x_{1}^{\\prime}]$ and $y_{1}=-1$ ; the second observation $\\left(v_{2},y_{2}\\right)$ is a false negative, which satisfies $v_{2}=x_{2}^{\\prime}$ and $y_{2}=+1$ . "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "$S=(x_{t},y_{t})_{t\\in[d]}$ such that $\\boldsymbol{\\mathcal{A}}$ is forced to make a mistake at every round. We construct the sequence S by first finding a path x\u20321\u2212(v\u22121\u2212,y\u22121)\u2192 $x_{1}^{\\prime}\\xrightarrow{\\ (v_{1},y_{1})}x_{2}^{\\prime}\\xrightarrow{\\ (v_{2},y_{2})}\\cdots x_{d}^{\\prime}\\xrightarrow{\\ (v_{d},y_{d})}x_{d+1}^{\\prime}$ \u2192x\u2032d+1 in tree T which specifies the types of mistakes that the adversary wishes to induce, then reverse-engineering this path to obtain the sequence of initial feature vectors before manipulation that is realizable under $\\mathcal{H}$ . We remark that the need for reverse-engineering is unique to the strategic setting, which is essential in resolving the information asymmetry between the learner and the adversary regarding the true features. We formally prove this theorem in Appendix B.1. ", "page_idx": 6}, {"type": "text", "text": "In the remainder of this section, we present an algorithm called the Strategic Standard Optimal Algorithm (SSOA) that achieves the instance-optimal mistake bound of $\\mathsf{S L d i m}(\\mathcal{H},G)$ . We first define some notations. For any hypothesis sub-class $\\mathscr{F}\\subset\\mathscr{H}$ and an observable labeled instance $(v,y)\\,\\in\\,\\mathcal{X}\\,\\times\\,\\mathcal{Y}$ , we use F(Gv,y) to denote the subset of F that is consistent with (v, y) under manipulation graph $\\mathcal{F}_{G}^{(v,+1)}$ and $\\mathcal{F}_{G}^{(v,-1)}$ $G$ are defined respectively as: . We refer to the consistency rule defined in Definition 3.1, i.e., for all $v\\in\\mathcal{X}$ , ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{F}_{G}^{(v,+1)}\\triangleq\\{h\\in\\mathcal{F}\\,|\\,\\,\\widetilde{h}_{G}(v)=+1\\};\\qquad\\mathcal{F}_{G}^{(v,-1)}\\triangleq\\{h\\in\\mathcal{F}\\,|\\,\\,\\exists x\\in N_{G}^{-}[v]\\,\\mathrm{s}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We present SSOA in Algorithm 1 and prove its optimality in Theorem 3.3. The high-level idea of SSOA is similar to the classical SOA algorithm (Algorithm 2): it maintains a version space of classifiers consistent with the history, and chooses a classifier $h_{t}$ in a way that guarantees the \u201cprogress on mistakes\u201d property. This means that the (strategic) Littlestone dimension of the version space should decrease whenever a mistake is made. ", "page_idx": 6}, {"type": "text", "text": "However, designing $h_{t}$ to satisfy this property in a strategic setting is more challenging because the potential types of mistakes depend on $h_{t}$ \u2019s labeling in the neighborhood $N_{G}^{+}[x_{t}]$ , where $x_{t}$ is unobservable. If we directly optimize the labelings on $N_{G}^{+}[x]$ for each $x$ independently, the resulting classifier may suggest self-contradictory labelings to the nodes in the overlapping parts of $N_{G}^{+}[x]$ and $N_{G}^{+}[x^{\\prime}]$ for different $x$ and $x^{\\prime}$ . Instead, the learner needs to choose a single $h_{t}$ that simultaneously guarantees the \u201cprogress on mistakes\u201d property for all possible $x_{t}$ and their neighborhoods. ", "page_idx": 6}, {"type": "text", "text": "In the following, we will show that this challenge can be resolved by choosing a classifier $h_{t}$ that labels each node $x$ only based on whether a false positive observation $(x,-1)$ can decrease the strategic Littlestone dimension, as described in Line 3 of Algorithm 1. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3.3 (Upper bound part of Theorem 3.1). The SSOA algorithm (Algorithm 1) achieves a maximal mistake bound of MistakeSSOA $({\\mathcal{H}},G)\\leq{\\mathsf{S L d i m}}({\\mathcal{H}},G)$ . ", "page_idx": 6}, {"type": "text", "text": "Remark 3.4 (Comparison with previous results). Since the mistake bound of SSOA is shown to be instance-optimal across all deterministic algorithms, it improves upon the bounds established by Ahmadi et al. [2023], Cohen et al. [2024], which both depend on the maximum out-degree of the graph G. Furthermore, we show in Appendix B.2 that the gap between their bounds and ours could be arbitrarily large. An extreme example is the complete graph $G$ supported on an unbounded domain, where SLdim $(\\mathcal{H},G)=1$ but both previous bounds are $\\infty$ . ", "page_idx": 6}, {"type": "text", "text": "Proof of Theorem 3.3. It suffices to prove that if SSOA makes a mistake at round $t$ , then the strategic Littlestone dimension of version space $\\mathcal{H}_{t}$ (maintained by the SSOA algorithm in Line 5) must ", "page_idx": 6}, {"type": "text", "text": "Algorithm 1: The Strategic Standard Optimal Algorithm (SSOA) ", "page_idx": 7}, {"type": "text", "text": "Input: Hypothesis class $\\mathcal{H}$ , manipulation graph $G$ . Initialization :Version space $\\mathcal{H}_{0}\\gets\\mathcal{H}$ . ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "2 Commit to the classifier $h_{t}:\\mathcal{X}\\to\\{\\pm1\\}$ defined as follows: 3 $\\forall x\\in\\mathcal{X},h_{t}(x)\\gets\\left\\{+1,\\quad\\mathrm{if~}\\mathsf{S L d i m}\\left((\\mathcal{H}_{t-1})_{G}^{(x,-1)},G\\right)<\\mathsf{S L d i m}(\\mathcal{H}_{t-1},G);_{\\xi}\\right\\}$ Observe the manipulated feature vector $v_{t}$ and the true label $y_{t}$ ; 5 If a mistake occurs $(h_{t}(v_{t})\\neq y_{t})$ ), update $\\mathcal{H}_{t}\\gets(\\mathcal{H}_{t-1})_{G}^{(v_{t},y_{t})}$ . Otherwise $\\mathcal{H}_{t}\\gets\\mathcal{H}_{t-1}$ decrease by at least 1, namely SLd $\\mathsf{i m}(\\mathcal{H}_{t},G)\\leq\\mathsf{S L d i m}(\\mathcal{H}_{t-1},G)-1$ . For notational convenience, let $d=\\mathsf{S L d i m}(\\mathcal{H}_{t-1},G)$ . ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "False positives. We start with the case where SSOA makes a false positive mistake, i.e., $h_{t}(v_{t})=+1$ but $y_{t}=-1$ . According to the definition of classifier $h_{t}$ and the update rule of version space $\\mathcal{H}_{t}$ , we immediately obtain ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathsf{S L d i m}(\\mathscr{H}_{t},G)=\\mathsf{S L d i m}\\left((\\mathscr{H}_{t-1})_{G}^{(v_{t},-1)},G\\right)<\\mathsf{S L d i m}(\\mathscr{H}_{t-1},G)\\,\\,\\Rightarrow\\,\\mathsf{S L d i m}(\\mathscr{H}_{t},G)\\leq d-1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "False negatives. Then we consider the case where SSOA makes a false negative mistake, i.e., $h_{t}(v_{t})=-1$ but $y_{t}=+1$ . For the sake of contradiction, assume that the strategic Littlestone dimension does not decrease, i.e., $\\mathsf{S L d i m}(\\mathcal{H}_{t},G)=\\mathsf{S L d i m}\\left((\\mathcal{H}_{t-1})_{G}^{(v_{t},+1)},G\\right)=d,$ . This assumption implies that there exists a strategic Littlestone tree $\\tau$ that is shattered by $(\\mathcal{H}_{t-1})_{G}^{(v_{t},+1)}$ and of depth . ", "page_idx": 7}, {"type": "text", "text": "Since the agent is classified as negative, it must be the case that the agent has not manipulated (i.e., $x_{t}\\,=\\,v_{t}\\,,$ ), and the entire outgoing neighborhood $N_{G}^{+}[x_{t}]$ is labeled as negative by $h_{t}$ . Therefore, according to the definition of $h_{t}$ , for all $v\\;\\in\\;N_{G}^{+}[x_{t}]$ , we have $\\mathsf{S L d i m}\\left((\\mathcal{H}_{t-1})_{G}^{(v,-1)},G\\right)\\,=\\,d,$ which implies that there also exists a strategic Littlestone tree $\\tau_{v}$ of depth $d$ that is shattered by (Ht\u22121)(Gv,\u22121). ", "page_idx": 7}, {"type": "text", "text": "Now consider the tree $\\mathcal{T}^{\\prime}$ with root $x_{t}$ , subtree $\\tau$ on the false negative edge $(x_{t},+1)$ , and subtree $\\tau_{v}$ on each false positive edge $(v,-1)$ for all $v\\in N_{G}^{+}[x_{t}]$ . Since we have argued that each subtree has depth $d$ , the overall depth of $\\mathcal{T}^{\\prime}$ is $d+1$ . We claim that $\\mathcal{T}^{\\prime}$ is shattered by $\\mathcal{H}_{t-1}$ . In fact, for all root-to-leaf paths in $\\mathcal{T}^{\\prime}$ , the first observation is guaranteed to be consistent with all hypotheses in the subclass for the subtree, and the consistency of each subtree ensures the existence of a hypothesis that makes all subsequent observations realizable. ", "page_idx": 7}, {"type": "text", "text": "We have thus constructed a strategic Littlestone tree $\\mathcal{T}^{\\prime}$ that is shattered by $\\mathcal{H}_{t-1}$ and of depth $d+1$ . However, this contradicts with the assumption that $\\mathsf{S L d i m}(\\mathcal{H}_{t-1},G)=d$ . Therefore, it must follow that $\\mathsf{S L d i m}(\\mathcal{H}_{t},G)\\leq d-1=\\mathsf{S L d i m}(\\bar{\\mathcal{H}}_{t-1},G)-1$ , which in turn proves MistakeSSOA $(\\mathcal{H},G)\\leq$ $\\mathsf{S L d i m}(\\mathcal{H},G)$ . \u53e3 ", "page_idx": 7}, {"type": "text", "text": "4 Agnostic Setting ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we study the regret bound in the agnostic setting. Recall that benchmark is defined as the minimum number of mistakes that the best hypothesis in $\\mathcal{H}$ makes, i.e., OPT $\\triangleq$ $\\begin{array}{r l}{\\operatorname*{min}_{h^{\\star}\\in\\mathcal{H}}\\sum_{t\\in[T]}\\mathbb{1}\\big\\{h^{\\star}\\big(\\mathsf{b r}_{G,h^{\\star}}(x_{t})\\big)\\neq y_{t}\\big\\}}&{{}}\\end{array}$ . We will present an algorithm that has vanishing regret compared to $\\Delta_{G}^{+}\\cdot\\mathsf{O P T}$ whenever the strategic Littlestone dimension is bounded, where $\\Delta_{G}^{+}$ is the maximum out-degree of $G$ . Inspired by the classical reduction framework proposed by Ben-David et al. [2009], our algorithm aims to reduce the agnostic problem to that of strategic online learning with expert advice by constructing a finite number of representative experts that performs almost as well as the potentially unbounded hypothesis class. The problem with a finite expert set can then be solved using the biased weighted voting algorithm proposed by Ahmadi et al. [2023]. However, establishing the reduction turns out to be more challenging in the strategic setting, as the learner can only observe manipulated features instead of the original ones. We address this problem by designing the experts to \u201cguess\u201d every possibile direction the original node could have come. We present our algorithms (Algorithms 3 and 4) in Appendix C and analyze their regret in Theorem 4.1. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Theorem 4.1. For any adaptive adversarial sequence $S$ of length $T$ , the Agnostic Online Strategic Classification algorithm (Algorithm 3) has regret bound ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathsf{R e g r e t}(S,\\mathcal{H},G)\\leq O\\left(\\Delta_{G}^{+}\\cdot O\\mathsf{P T}+\\Delta_{G}^{+}\\cdot\\mathsf{S L d i m}(\\mathcal{H},G)\\cdot(\\log T+\\log\\Delta_{G}^{-})\\right),\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\Delta_{G}^{+}$ (resp. $\\Delta_{G}^{-},$ ) denotes the maximum out-degree (resp. in-degree) of graph $G$ . ", "page_idx": 8}, {"type": "text", "text": "Remark 4.2. Ahmadi et al. $[2023]$ showed that there exists instances in which all deterministic algorithms must suffer regret $\\Omega(\\Delta_{G}^{+}{\\cdot}\\mathsf{O}\\mathsf{P}\\mathsf{T})$ , which means the first term in the above bound is necessary. The second term connects to our instance-wise lower bound of SLdim $(\\mathcal{H},G)$ in Theorem 3.2. ", "page_idx": 8}, {"type": "text", "text": "Proof sketch of Theorem 4.1. We use $\\mathfrak{E}$ to denote the set of experts constructed in Algorithm 4, and define ${\\mathsf{O P T^{\\mathfrak{E}}}}$ as the minimum number of mistakes made by the best expert $c^{\\star}\\in\\mathfrak{E}$ , had the agents responded to $\\mathfrak{e}^{\\star}$ . Then the Biased Weighted Majority Vote algorithm from Ahmadi et al. [2023] guarantees that the number of mistakes made by Algorithm 3 is at most $\\Delta_{G}^{+}\\cdot\\mathsf{O P T}^{\\mathfrak{E}}+\\Delta_{G}^{+}$ \u00b7 $\\log\\left|\\mathfrak{E}\\right|$ . According to our construction of experts, the total number of experts satisfies $\\log|\\mathfrak{E}|\\ \\leq$ $\\begin{array}{r}{\\log\\big(\\sum_{m\\le d}\\binom{T}{m}\\cdot(\\Delta_{G}^{-})^{m}\\big)\\le O(d\\cdot(\\log T+\\log\\Delta_{G}^{-}))}\\end{array}$ , where $d=\\mathsf{S L d i m}(\\mathcal{H},G)$ is the strategic Littlestone dimension. Therefore, it suffices to show that ${\\mathsf{O P T}}^{\\mathfrak{E}}$ is not too much larger than OPT\u2014in other words, the set of experts $\\mathfrak{E}$ are representative enough of the original hypothesis class $\\mathcal{H}$ in their ability of performing strategic classification. We use the following lemma, which we prove in Appendix C by establishing the equivalence between the SSOA instance running on the sequence labeled by the effective classifier and the SSOA instance simulated by a specific expert. ", "page_idx": 8}, {"type": "text", "text": "Lemma 4.3 (Experts are representative). For any hypothesis $h\\in\\mathcal H$ and any sequence of agents $S$ , there exists an expert $\\mathfrak{e}_{h}\\in\\mathfrak{E}$ that makes at most $\\mathsf{S L d i m}(\\mathcal{H},G)$ more mistakes than $h$ . ", "page_idx": 8}, {"type": "text", "text": "5 Unknown Manipulation Graph ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we generalize the main settings to relax the assumption that the learner has full knowledge about the underlying manipulation graph $G$ . Instead, we use a graph class $\\mathcal{G}$ to capture the learner\u2019s knowledge about the manipulation graph. In Section 5.1, we begin with the realizable graph class setting, where the true manipulation graph remains the same across rounds and belongs to the family $\\mathcal{G}$ . We then study the agnostic graph class setting in Section 5.2, where we drop both assumptions and allow our regret bound to depend on the \u201cimperfectness\u201d of $\\mathcal{G}$ . In both cases, we assume the hypothesis class $\\mathcal{H}$ is also agnostic, which encompasses the setting where $\\mathcal{H}$ is realizable. ", "page_idx": 8}, {"type": "text", "text": "5.1 Realizable graph classes ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we assume that there exists a perfect (but unknown) graph $G^{\\star}\\in\\mathcal{G}$ , such that each agent $(x_{t},y_{t})\\in S$ manipulates according to $G^{\\star}$ . We define the benchmark $\\mathsf{O P T}_{\\mathcal{H}}$ to be the optimal number of mistakes made by the best $h^{\\star}\\in\\mathcal{H}$ assuming that each agent best responds to $h^{\\star}$ according to $G^{\\star}$ . Formally, $\\mathsf{O P T}_{\\mathcal{H}}\\,\\triangleq\\,\\operatorname*{min}_{h^{\\star}\\in\\mathcal{H}}\\mathbb{1}\\big\\{h^{\\star}\\big(\\mathsf{b r}_{G^{\\star},h^{\\star}}(x_{t})\\big)\\neq y_{t}\\big\\}$ . Same to our main setting, we assume that the learner only observes the post-manipulation features $v_{t}\\,=\\,\\mathsf{b r}_{G^{\\star},h_{t}}(x_{t})$ after they commit to classifier $h_{t}$ , but cannot observe the original features $x_{t}$ . ", "page_idx": 8}, {"type": "text", "text": "Our algorithm (Algorithm 6) for this setting leverages two main ideas. First, to overcome the challenge that $G^{\\star}$ is unknown to the experts, we blow up the number of experts by a factor of $|\\mathcal G|$ and let each expert simulate their own SSOA instance according to some internal belief of $G^{\\star}$ . Since the regret bound depends logarithmic on the number of experts, this only introduces an extra $\\log\\left|\\mathcal{G}\\right|$ term, which has been shown by Cohen et al. [2024] to be unavoidable even when the learner has access to the original features. ", "page_idx": 8}, {"type": "text", "text": "Our second idea involves re-examining the correctness of Algorithm 5 for bounded expert class to the scenario where the input $G$ is a pessimistic estimate of the true graph $G^{\\star}$ , i.e., $G$ contains all the edges in $G^{\\star}$ but potentially some extra edges. This allows us to use $G_{\\mathrm{union}}$ whose edge set is taken to be the union of all egdes in $\\mathcal{G}$ . Combining these two ideas, we present our algorithm and establish its regret bound (Theorem 5.1) in Appendix D. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Theorem 5.1. For any realizable graph class $\\mathcal{G}$ and any adaptive adversarial sequence $S$ of length $T$ , Algorithm $^{6}$ has regret bound ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\mathsf{R e g r e t}(S,\\mathcal{H},G)\\leq O\\left(\\Delta_{\\mathcal{G}}^{+}\\cdot\\left(\\mathsf{O P T}_{\\mathcal{H}}+d_{\\mathcal{G}}\\cdot(\\log T+\\log\\Delta_{\\mathcal{G}}^{-})+\\log|\\mathcal{G}|\\right)\\right),\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "where $d_{\\mathcal{G}}\\triangleq\\operatorname*{max}_{G\\in\\mathcal{G}}\\mathsf{S L d i m}(\\mathcal{H},G)$ is the maximum strategic Littlestone dimension for all graphs in $\\mathcal{G}$ , $\\Delta_{\\mathcal{G}}^{+}\\ \\triangleq\\ \\Delta_{{G}_{u n i o n}}^{+}$ is the maximum out-degree of $G_{u n i o n}$ (i.e., the union of graphs in $\\mathcal{G}$ ), and $\\Delta_{\\mathcal{G}}^{-}\\triangleq\\operatorname*{max}_{G\\in\\mathcal{G}}\\Delta_{G}^{-}$ is the maximum max in-degree over graphs in $G$ . ", "page_idx": 9}, {"type": "text", "text": "Remark 5.2 (Implications in the realizable setting). In the realizable setting where ${\\mathsf{O P T}}_{\\mathcal{H}}=0$ , Theorem 5.1 implies a mistake bound of ${\\tilde{O}}(\\Delta_{\\mathcal{G}}^{+}\\cdot d_{\\mathcal{G}}+\\log|\\mathcal{G}|)$ . This bound is optimal up to logarithmic factors due to a lower bound proved by Cohen et al. [2024, Proposition 14]. They constructed an instance with $|{\\mathcal{G}}|=|{\\mathcal{H}}|={\\Theta}({\\bar{n}})$ in which any deterministic algorithm makes $\\Omega(n)$ mistakes. In this instance, our bound evaluates to be ${\\tilde{O}}(n)$ since $\\Delta_{\\mathcal{G}}^{+}=\\Theta(n)$ and $d_{\\mathcal{G}}=1$ . ", "page_idx": 9}, {"type": "text", "text": "5.2 Agnostic graph classes ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this section, we consider a fully agnostic setting where each agent $\\left({{x}_{t}},{{y}_{t}}\\right)$ may behave according to a different manipulation graph $G_{t}\\subseteq G_{\\operatorname{union}}$ . We define the benchmark $\\mathsf{O P T}_{\\mathcal{G}}$ to count the number of times that the best graph $G^{\\star}\\in\\mathcal{G}$ fails to model the local manipulation structure under $G_{t}$ , and $\\mathsf{O P T}_{\\mathcal{H}}$ is defined as in Section 5.1, using the graph $G^{\\star}$ that achieves $\\mathsf{O P T}_{\\mathcal{G}}$ . ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\mathsf{O P T}_{\\mathcal{G}}\\triangleq\\operatorname*{min}_{G^{\\star}\\in\\mathcal{G}}\\sum_{t=1}^{T}\\mathbb{1}\\big\\{N_{G^{\\star}}^{+}[x_{t}]\\neq N_{G_{t}}^{+}[x_{t}]\\big\\}\\,,\\;\\mathsf{O P T}_{\\mathcal{H}}\\triangleq\\operatorname*{min}_{h^{\\star}\\in\\mathcal{H}}\\sum_{t=1}^{T}\\mathbb{1}\\big\\{h^{\\star}(\\mathsf{b r}_{G^{\\star},h^{\\star}}(x_{t})\\neq y_{t})\\big\\}^{6}.\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "Assuming access to an upper bound $N$ of $\\mathsf{O P T}_{\\mathcal{G}}$ , we present Algorithm 7 that achieves a regret bound of $\\tilde{O}\\left(\\Delta_{\\mathcal{G}}^{+}(N+\\mathsf{O P T}_{\\mathcal{H}}+d\\mathcal{G})\\right)$ , as shown in Theorem D.2. We additionally apply the standard doubling trick to remove the requirement of knowing $N$ . More details can be found in Appendix D.2. ", "page_idx": 9}, {"type": "text", "text": "6 Discussion and Future Research ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Improved bounds for the agnostic setting. An immediate direction for future research is tightening our bounds in the agnostic setting under known manipulation graph. Note that our upper bound is $\\tilde{O}\\left(\\Delta_{G}^{+}\\cdot(\\mathsf{O P T}+\\mathsf{S L d i m}(\\mathcal{H},G))\\right)$ whereas the lower bounds are $\\Omega(\\Delta_{G}^{+}\\cdot\\mathsf{O P T})$ from Ahmadi et al. [2023] and $\\Omega(\\mathsf{S L d i m}(\\mathcal{H},G))$ from Theorem 3.2. The extra $\\Delta_{G}^{+}$ factor is introduced by the strategic learning-with-expert-advice algorithm, for which all known results have the dependency on $\\Delta_{G}^{+}$ . ", "page_idx": 9}, {"type": "text", "text": "Randomized learners. Our results mainly focus on deterministic learners. It is an important open problem to find the corresponding characterizations for randomized learners. In Appendix E, we provided a family of realizable instances that witnesses a super-constant gap between the optimal mistake of deterministic and randomized algorithms. This is in contrast to their classical counterparts which are always a factor of 2 within each other. One challenge (among others) of proving a tight lower bound in the randomized setting is controlling the learner\u2019s information about the agents\u2019 original features, as the adversary can no longer \u201clook-ahead\u201d at an algorithm\u2019s future classifiers. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments. We thank Avrim Blum for the helpful comments and discussions. This work was done while Hanrui Zhang was in residence at the Simons Laufer Mathematical Sciences Institute (formerly MSRI) in Berkeley, California, during the Fall 2023 semester. This work was supported in part by the National Science Foundation under grants DMS-1928930, CCF-2212968, and ECCS2216899, by the Alfred P. Sloan Foundation under grant G-2021-16778, by the Simons Foundation under the Simons Collaboration on the Theory of Algorithmic Fairness, and by the Defense Advanced Research Projects Agency under cooperative agreement HR00112020003. The views expressed in this work do not necessarily reflect the position or the policy of the Government and no official endorsement should be inferred. Approved for public release; distribution is unlimited. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Saba Ahmadi, Hedyeh Beyhaghi, Avrim Blum, and Keziah Naggita. The strategic perceptron. In Proceedings of the 22nd ACM Conference on Economics and Computation $(E C)$ , pages 6\u201325, 2021. ", "page_idx": 10}, {"type": "text", "text": "Saba Ahmadi, Hedyeh Beyhaghi, Avrim Blum, and Keziah Naggita. On classification of strategic agents who can both game and improve. arXiv preprint arXiv:2203.00124, 2022.   \nSaba Ahmadi, Avrim Blum, and Kunhe Yang. Fundamental bounds on online strategic classification. In Proceedings of the 24th ACM Conference on Economics and Computation (EC), pages 22\u201358, 2023.   \nTal Alon, Magdalen Dobson, Ariel Procaccia, Inbal Talgam-Cohen, and Jamie Tucker-Foltz. Multiagent evaluation mechanisms. Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), 34(02):1774\u20131781, 2020.   \nYahav Bechavod, Katrina Ligett, Steven Wu, and Juba Ziani. Gaming helps! learning from strategic interactions in natural dynamics. In International Conference on Artificial Intelligence and Statistics, pages 1234\u20131242. PMLR, 2021.   \nShai Ben-David, D\u00e1vid P\u00e1l, and Shai Shalev-Shwartz. Agnostic online learning. In Conference on Learning Theory (COLT), volume 3, page 1, 2009.   \nMichael Br\u00fcckner and Tobias Scheffer. Stackelberg games for adversarial prediction problems. In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 547\u2013555, 2011.   \nYiling Chen, Yang Liu, and Chara Podimata. Learning strategy-aware linear classifiers. In Advances in Neural Information Processing Systems (NeurIPS), volume 33, pages 15265\u201315276, 2020.   \nLee Cohen, Saeed Sharifi-Malvajerdi, Kevin Stangl, Ali Vakilian, and Juba Ziani. Sequential strategic screening. In International Conference on Machine Learning, pages 6279\u20136295. PMLR, 2023.   \nLee Cohen, Yishay Mansour, Shay Moran, and Han Shao. Learnability gaps of strategic classification. arXiv preprint arXiv:2402.19303, 2024.   \nNilesh Dalvi, Pedro Domingos, Mausam, Sumit Sanghai, and Deepak Verma. Adversarial classification. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 99\u2013108, 2004.   \nOfer Dekel, Felix Fischer, and Ariel D Procaccia. Incentive compatible regression learning. In Proceedings of the nineteenth annual ACM-SIAM symposium on Discrete algorithms, pages 884\u2013893, 2008.   \nJinshuo Dong, Aaron Roth, Zachary Schutzman, Bo Waggoner, and Zhiwei Steven Wu. Strategic classification from revealed preferences. In Proceedings of the 19th ACM Conference on Economics and Computation (EC), pages 55\u201370, 2018.   \nYuval Filmus, Steve Hanneke, Idan Mehalel, and Shay Moran. Bandit-feedback online multiclass classification: Variants and tradeoffs. arXiv preprint arXiv:2402.07453, 2024.   \nAlan M Frieze. On the independence number of random graphs. Discrete Mathematics, 81(2): 171\u2013175, 1990.   \nGanesh Ghalme, Vineet Nair, Itay Eilat, Inbal Talgam-Cohen, and Nir Rosenfeld. Strategic classification in the dark. In International Conference on Machine Learning, pages 3672\u20133681. PMLR, 2021.   \nNika Haghtalab, Nicole Immorlica, Brendan Lucier, and Jack Z. Wang. Maximizing welfare with incentive-aware evaluation mechanisms. In International Joint Conference on Artificial Intelligence (IJCAI), pages 160\u2013166, 2020.   \nMoritz Hardt, Nimrod Megiddo, Christos Papadimitriou, and Mary Wootters. Strategic classification. In Innovations in Theoretical Computer Science Conference (ITCS), pages 111\u2013122, 2016.   \nLily Hu, Nicole Immorlica, and Jennifer Wortman Vaughan. The disparate effects of strategic manipulation. In Proceedings of the Conference on Fairness, Accountability, and Transparency, FAT\\* \u201919, page 259\u2013268, 2019.   \nJon Kleinberg and Manish Raghavan. How do classifiers induce agents to invest effort strategically? ACM Transactions on Economics and Computation (TEAC), 8(4):1\u201323, 2020.   \nTosca Lechner and Ruth Urner. Learning losses for strategic classification. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), volume 36, pages 7337\u20137344, 2022.   \nTosca Lechner, Ruth Urner, and Shai Ben-David. Strategic classification with unknown user manipulations. In International Conference on Machine Learning, pages 18714\u201318732. PMLR, 2023.   \nNick Littlestone. Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm. Machine learning, 2:285\u2013318, 1988.   \nJuan Perdomo, Tijana Zrnic, Celestine Mendler-D\u00fcnner, and Moritz Hardt. Performative prediction. In Proceedings of the 37th International Conference on Machine Learning (ICML), volume 119, pages 7599\u20137609. PMLR, 2020.   \nAnanth Raman, Vinod Raman, Unique Subedi, Idan Mehalel, and Ambuj Tewari. Multiclass online learnability under bandit feedback. In International Conference on Algorithmic Learning Theory, pages 997\u20131012. PMLR, 2024.   \nHan Shao, Avrim Blum, and Omar Montasser. Strategic classification under unknown personalized manipulation. In Advances in Neural Information Processing Systems (NeurIPS), volume 36, 2024.   \nRavi Sundaram, Anil Vullikanti, Haifeng Xu, and Fan Yao. PAC-learning for strategic classification. Journal of Machine Learning Research, 24(192):1\u201338, 2023.   \nHanrui Zhang and Vincent Conitzer. Incentive-aware pac learning. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), volume 35, pages 5797\u20135804, 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Supplementary Materials for Section 2 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We present the SOA algorithm in Algorithm 2. ", "page_idx": 12}, {"type": "table", "img_path": "4Lkzghiep1/tmp/2c71dcd20a0ae93a743b1305bcfa65667940987515a20590122fde534bf09184.jpg", "table_caption": [], "table_footnote": [], "page_idx": 12}, {"type": "text", "text": "B Supplementary Materials for Section 3 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "B.1 Proof of Theorem 3.2 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Theorem 3.2 (Restated). For any pair of hypothesis class $\\mathcal{H}$ and manipulation graph $G$ , any deterministic online learning algorithm $\\boldsymbol{\\mathcal{A}}$ must suffer a mistake lower bound of Mistake $\\mathscr{A}(\\mathscr{H},G)\\geq$ SLdim $(\\mathcal{H},G)$ . ", "page_idx": 12}, {"type": "text", "text": "Proof of Theorem 3.2. Recall that to construct an adversarial sequence of agents $S=(x_{t},y_{t})_{t\\in[d]}$ such that $\\boldsymbol{\\mathcal{A}}$ is forced to make a mistake at every round, we will first find a path $\\begin{array}{r}{x_{1}^{\\prime}\\xrightarrow{(v_{1},y_{1})}x_{2}^{\\prime}\\xrightarrow{(v_{2},y_{2})}}\\end{array}$ $\\cdots x_{d}^{\\prime}\\xrightarrow{(v_{d},y_{d})}x_{d+1}^{\\prime}$ in tree $\\tau$ which specifies the types of mistakes that the adversary wishes to induce, then reverse-engineering this path to obtain the sequence of initial feature vectors before manipulation that is realizable under $\\mathcal{H}$ . ", "page_idx": 12}, {"type": "text", "text": "Constructing the path. We initialize $x_{1}^{\\prime}$ to be the root of the tree $\\tau$ . For all $t\\leq d$ and given the history (partial path) $x_{1}^{\\prime}\\xrightarrow{(v_{1},y_{1})}\\cdot\\cdot\\cdot x_{t-1}^{\\prime}\\xrightarrow{(v_{t-1},y_{t-1})}x_{t}^{\\prime}$ , we find the edge $(v_{t},y_{t})$ and the next node $x_{t+1}^{\\prime}$ as follows: run the online learning algorithm $\\boldsymbol{\\mathcal{A}}$ for $t-1$ rounds with inputs $(v_{t^{\\prime}},y_{t^{\\prime}})_{t^{\\prime}\\leq t-1}$ , and let $h_{t}$ be the outputted classifier at round $t$ . We examine the labels of $h_{t}$ in the out-neighborhood $N_{G}^{+}[x_{t}^{\\prime}]$ under graph $G$ and consider the following two cases. ", "page_idx": 12}, {"type": "text", "text": "Case 1: False negatives. If all the feature vectors in $N_{G}^{+}[x_{t}^{\\prime}]$ is labeled as negative by $h_{t}$ , then the adversary will induce a false negative mistake by letting the post-manipulation feature vector be $\\ensuremath{\\boldsymbol{{x}}}_{t}^{\\prime}$ (which is same as the original feature vector $x_{t}$ ) and the true label be positive, i.e., $\\left(v_{t},y_{t}\\right)\\triangleq\\left(x_{t}^{\\prime},+1\\right)$ We then choose the next node $x_{t+1}^{\\prime}$ to be the child of $\\ensuremath{\\boldsymbol{{x}}}_{t}^{\\prime}$ along the false negative edge $(x_{t}^{\\prime},+1)$ in $\\tau$ ", "page_idx": 12}, {"type": "text", "text": "Case 2: False positives. If there exists $v\\,\\in N_{G}^{+}[x_{t}^{\\prime}]$ such that $h_{t}(v)=+1$ , then the adversary will induce a false positive mistake that is observed at $v$ with true label $-1$ , i.e., $\\left(v_{t},y_{t}\\right)\\triangleq\\left(v,-1\\right)$ . However, we remark that the true features $x_{t}$ may be chosen as a different node in $N_{G}^{-}[v_{t}]$ to ensure realizability, which we will discuss in the reverse-engineering part. We choose $x_{t+1}^{\\prime}$ to be the child of $\\ensuremath{\\boldsymbol{{x}}}_{t}^{\\prime}$ along the false positive edge $(v,-1)$ in $\\tau$ . ", "page_idx": 12}, {"type": "text", "text": "Reverse-engineering. Repeating the above procedure for all $t\\leq d$ gives us the path $x_{1}^{\\prime}\\xrightarrow{\\ (v_{1},y_{1})}$ $\\cdots x_{d}^{\\prime}\\xrightarrow{(v_{d},y_{d})}x_{d+1}^{\\prime}$ , where each $y_{t}$ already specifies the true labels of each agent. It remains to select the initial features $\\left(\\boldsymbol{x}_{t}\\right)$ . Since $\\tau$ is shattered by $\\mathcal{H}$ , the consistency part of Definition 3.1 guarantees the existence of $h\\in\\mathcal H$ such that $\\forall t<d$ , there exists $x_{t}$ that satisfies $\\widetilde{h}_{G}(x_{t})=y_{t}$ , where $x_{t}=v_{t}=x_{t}^{\\prime}$ if $y_{t}=-1$ and $x_{t}\\in N_{G}^{-}[v_{t}]$ if $y_{t}=+1$ . We let those $(x_{t})_{t\\in[d]}$ be agents\u2019 true feature vectors. It then follows that the sequence of agents $S=(x_{t},y_{t})_{t\\in[d]}$ is realizable under $(\\mathcal{H},G)$ and indeed induces a mistake observed as $(v_{t},y_{t})$ at every round $t\\in[d]$ . ", "page_idx": 12}, {"type": "text", "text": "Finally, if $\\mathsf{S L d i m}(\\mathcal{H},G)<\\infty$ , then the above argument with $d=\\mathsf{S L d i m}(\\mathcal{H},G)$ proves the theorem. When $\\mathsf{S L d i m}(\\mathcal{H},\\mathcal{G})=\\infty$ , the above argument shows $\\mathsf{M i s t a k e}_{A}(\\mathcal{H},G)\\geq d$ for all $d\\in\\mathbb{N}$ , which implies that Mistak $\\colon\\!A(\\mathcal{H},G)=\\infty$ by driving $d\\to\\infty$ . The proof is thus complete. \u53e3 ", "page_idx": 13}, {"type": "text", "text": "B.2 Comparison with max-degree based bounds ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we consider two families of instances $(\\mathcal{H},G)$ in which our mistake bound $\\mathsf{S L d i m}(\\mathcal{H},G)$ from Theorem 3.3 significantly improves the previous bounds. We compare with upper bounds $O(\\Delta_{G}^{+}\\cdot\\log|\\mathcal{H}|)$ from [Ahmadi et al., 2023] and $\\tilde{O}(\\Delta_{G}^{+}\\cdot\\mathsf{L d i m}(\\mathcal{H}))$ from [Cohen et al., 2024], where $\\mathsf{L d i m}(\\mathcal{H})$ denotes the classical Littlestone dimension of hypothesis class $\\mathcal{H}$ . We focuses on comparing with the latter bound, since it can be shown that $\\mathsf{d i m}(\\mathcal{H})\\leq\\log|\\mathcal{H}|$ for all $\\mathcal{H}$ . ", "page_idx": 13}, {"type": "text", "text": "Graphs with a large clique. Our first example involves graphs with a very large clique. The main idea is that the densest part of graph may turn out to be very easy to learn since there are only a few effective hypotheses supported on it. On the other hand, the harder-to-learn part of the hypothesis class may be supported on a subgraph with a much smaller maximum degree. For this reason, the previous bounds that directly multiply the complexity of the entire graph (e.g., $\\Delta_{G}^{+}$ ) with the complexity of the entire hypothesis class would be suboptimal. ", "page_idx": 13}, {"type": "text", "text": "Let $(G^{\\prime},\\mathcal{H}^{\\prime})$ be a pair of manipulation graph and hypothesis class, for which we have $\\mathsf{S L d i m}(\\mathcal{H},G^{\\prime})\\,\\le\\,O(\\Delta_{G^{\\prime}}^{+}\\cdot\\mathsf{L d i m}(\\mathcal{H}^{\\prime})$ since the strategic Littlestone dimension is a lower bound of all valid mistake bounds (Theorem 3.2). Let $N\\gg\\operatorname*{max}\\{|G^{\\prime}|,|\\mathcal{H}^{\\prime}|\\}$ be a very large integer, and $K_{N}$ be a clique of size $N$ . We assume that the vertex set of $K_{N}$ is disjoint from that of $G^{\\prime}$ . We take the hypothesis class on $K_{N}$ to be the set of all functions, i.e., $\\{\\pm1\\}^{K_{N}}$ . Let $G=G^{\\prime}\\cup K_{N}$ and $\\mathcal{H}=\\mathcal{H}^{\\prime}\\stackrel{\\cdot}{\\times}\\{\\pm1\\}^{N}$ . ", "page_idx": 13}, {"type": "text", "text": "\u2022 Previous bound: Since $\\Delta_{G}^{+}=N$ and $\\mathsf{L d i m}(\\mathcal{H})\\geq\\mathsf{L d i m}(\\mathcal{H}^{\\prime})$ . Therefore, the bound in [Cohen et al., 2024] is of order (ignoring logarithmic factors) ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\Delta_{G}^{+}\\cdot\\mathsf{L d i m}(\\mathcal{H})\\geq\\Omega(N\\cdot\\mathsf{L d i m}(\\mathcal{H}^{\\prime})).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "\u2022 Our bound based on the strategic Littlestone dimension: By Theorem 3.2 $,\\mathsf{S L d i m}(\\mathcal{H},G)$ lower bounds the mistake bound achievable by any deterministic algorithm. Consider the following deterministic algorithm that uses two independent algorithms $\\mathcal{A}_{1}$ and $\\boldsymbol{A_{2}}$ to learn on each of the disjoint subgraphs $\\mathcal{G}^{\\prime}$ and $K_{N}$ . We will choose $\\mathcal{A}_{1}$ to be the Red2Online-PMF(SOA) algorithm proposed by Cohen et al. [2024], and $\\boldsymbol{A}_{2}$ be the algorithm that predicts all nodes negative until a mistake happens, at which point flips the prediction to positive on all nodes. Since the effective classifiers on $K_{N}$ is either all positive or all negative, $\\boldsymbol{A_{2}}$ makes at most 1 mistake. We have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathsf{S L d i m}(\\mathscr{H},G)\\leq\\mathsf{M i s t a k e}_{\\mathscr{A}_{1}}(\\mathscr{H}^{\\prime},G^{\\prime})+\\mathsf{M i s t a k e}_{\\mathscr{A}_{2}}(\\{\\pm1\\}^{K_{N}},K_{N})\\leq\\tilde{O}(\\Delta_{G^{\\prime}}^{+}\\cdot\\mathsf{L d i m}(\\mathscr{H}^{\\prime})).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "\u2022 Improvement. As a result, for this instance, the gap between these two algorithms is lower bounded by $N/\\Delta_{G^{\\prime}}^{+}$ , which can be arbitrarily large by taking $N\\rightarrow\\infty$ . ", "page_idx": 13}, {"type": "text", "text": "Random graphs in $G(n,p)$ , ${\\mathcal{H}}=\\{\\pm1\\}^{n}$ . Our second example considers random graphs $G\\sim$ $G(n,p)$ , in which every (undirected) edge is realized independently with probability $p$ . We will show that when $p=\\omega(1/n)$ (when the random graph is \u201ceffectively\u201d dense), with high probability over $G\\sim G(n,p)$ , the strategic Littlestone dimension significantly improves previous bounds. ", "page_idx": 13}, {"type": "text", "text": "Since $\\mathcal{H}$ is extremely expressive (contains all functions), we have $\\log|\\mathcal{H}|=\\mathsf{L}\\mathsf{d i m}(\\mathcal{H})=n$ . However, we will show that even after strengthening the previous bounds by applying it on the reduced-size hypothesis class $\\widetilde{\\mathcal{H}}_{G}\\subseteq\\,\\mathcal{H}$ \u2014which contains only one hypothesis in each equivalence class that induces the same effective hypothesis $\\widetilde{h}_{G}$ \u2014the strategic Littlestone dimension $\\mathsf{S L d i m}(\\mathcal{H},G)$ still offers significant improvement over $\\Delta_{G}^{+}\\cdot\\mathsf{L d i m}(\\widetilde{\\mathcal{H}}_{G})$ . ", "page_idx": 13}, {"type": "text", "text": "\u2022 Previous bound: By concentration, $\\Delta_{G}^{+}\\geq\\Omega(n p)$ with high probability. Moreover, with high probability, the independence number $\\alpha{\\vec{(G)}}$ satisfies $\\alpha(G)\\stackrel{*}{\\geq}\\bar{\\Omega}(\\log(n p)/p)$ [Frieze, 1990]. Let $I(G)$ be the independent set with size $\\alpha(G)$ and consider the projection of $\\widetilde{\\mathcal{H}}_{G}$ onto $I(G)$ . Since there are no edges inside, the effective hypothesis coincides with the original hypothesis, therefore $(\\widetilde{\\mathcal{H}}_{G})_{I(G)}$ contains all functions that maps from $I(G)$ to $\\{\\pm1\\}$ , which has classical Littlestone dimension $|I(G)|=\\alpha(G)$ . As a result, we have that with high probability, ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "equation", "text": "$$\n\\Delta_{G}^{+}\\cdot\\mathsf{L d i m}(\\widetilde{\\mathcal{H}}_{G})\\geq\\Omega(n p)\\cdot\\Omega(\\log(n p)/p)\\geq\\Omega(n\\log(n p)).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "\u2022 Our bound based on the strategic Littlestone dimension: Again, $\\mathsf{S L d i m}(\\mathcal{H},G)$ lower bounds the mistake bound of all deterministic algorithms. Consider the following algorithm: start with predicting all nodes as positive. Whenever a false positive is observed at some node $u$ , flip the sign of $u$ to negative. Such an algorithm achieves a mistake bound of $n$ . Therefore, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathsf{S L d i m}(\\mathcal{H},G)\\leq n.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "\u2022 Improvement When $\\begin{array}{r}{p=\\omega\\big(\\frac{1}{n}\\big)}\\end{array}$ , with high probability, the gap between these two bounds are ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\Omega(n p)=\\omega_{n}(1),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which can be made to approach $\\infty$ as $n\\to\\infty$ . ", "page_idx": 14}, {"type": "text", "text": "C Supplementary Materials for Section 4 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "C.1 Algorithms for the agnostic setting ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we present algorithms for the agnostic setting and prove their regret guarantees. As discussed in Section 4, the main idea behind the strategic version of agnostic-to-realizable reduction lies in our \u201cguess\u201d of the possible direction the original node would have come from. To do this systematically, we first need to specify a indexing system to the in-neighborhoods of every node in the graph. For each node $v\\in\\mathcal{X}$ , we assign a unique index to each in-neighbor in $N_{G}^{-}[v]$ from the range $\\{0,1,\\cdot\\cdot\\cdot,\\Delta_{G}^{-}\\}$ , where $\\Delta_{G}^{-}$ being the max in-degree of $G$ . This indexing is specific to each $v$ and does not require consistency when indexing a common in-neighbor of different nodes. We are now ready to formally introduce our algorithms in Algorithms 3 and 4. ", "page_idx": 14}, {"type": "text", "text": "Algorithm 3: Agnostic Online Strategic Classification Algorithm Input: Hypothesis class $\\mathcal{H}$ , manipulation graph $G$ . 1 Let $d\\gets\\mathsf{S L d i m}(\\mathcal{H},G)$ ; 2 foreach $m\\leq d,i_{1:m}$ , $r_{1:m}$ where $1\\leq i_{1}<\\cdot\\cdot\\cdot<i_{m}\\leq T,\\,0\\leq r_{1},\\cdot\\cdot\\cdot{\\mathrm{\\boldmath~\\nabla~}},r_{m}\\leq\\Delta_{G}^{-}\\,\\mathrm{\\boldmath~\\nabla~}$ do 3 Construct $E x p e r t(i_{1:m},r_{1:m})$ as in Algorithm 4. 4 end 5 Run Biased Weighted Majority Vote (Algorithm 5) on the set of experts. ", "page_idx": 14}, {"type": "text", "text": "Algorithm 4: $E x p e r t(i_{1},\\cdot\\cdot\\cdot\\,,i_{m},r_{1},\\cdot\\cdot\\cdot\\,,r_{m};G)$ Input: Hypothesis class $\\mathcal{H}$ , manipulation graph $G$ , indices for mistakes $1\\leq i_{1}<\\cdot\\cdot<i_{m}\\leq T$ , indices for manipulation directions $0\\leq r_{1},\\cdot\\cdot\\cdot\\,,r_{m}\\leq\\Delta_{G}^{-}$ , the sequence of post-manipulation agents $(v_{t},y_{t})_{t\\in[T]}$ received sequentially. Output: Classifiers $(\\hat{h}_{t})_{t\\in[T]}$ outputted sequentially. Initialization :Simulate an instance of the SSOA algorithm with parameters $(\\mathcal{H},G)$ . 1 for $t\\in[T]$ do 2 $\\hat{h}_{t}\\gets$ classifier outputted by the SSOA algorithm; 3 Observe the manipulated feature vector $v_{t}$ and the true label $y_{t}$ ; 4 if $t\\in\\{i_{1},\\cdot\\cdot\\cdot\\,,i_{m}\\}$ (suppose $t=i_{k}$ ) then 5 $\\hat{x}_{t}\\gets$ the in-neighbor in $N_{G}^{-}[v_{t}]$ with index $r_{k}//$ guess of the original feature vector $x_{t}$ 6 $\\hat{v}_{t}\\gets\\mathsf{b r}_{\\hat{h}_{t},G}(\\hat{x}_{t})//$ simulate the post-manipulation feature vector in response to $\\hat{h}_{t}$ Update the SSOA algorithm with instance $(\\hat{v}_{t},-\\hat{h}_{t}(\\hat{v}_{t}))$ . 8 end 9 end ", "page_idx": 14}, {"type": "text", "text": "C.2 Proof of Lemma 4.3 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Lemma 4.3 (Restated). For any hypothesis $h\\in\\mathcal H$ and any sequence of agents $S$ , there exists an expert $\\mathfrak{c}_{h}\\in\\mathfrak{E}$ that makes at most SLdim $(\\mathcal{H},G)$ more mistakes than $h$ . ", "page_idx": 15}, {"type": "text", "text": "Proof of Lemma 4.3. Let us define the hypothetical sequence $S^{(h)}\\triangleq(x_{t},y_{t}^{(h)})_{t\\in[T]}$ , where we keep the same sequence of initial feature vectors $\\left(\\boldsymbol{x}_{t}\\right)$ in $S$ , but adjust their labels to be $y_{t}^{(h)}\\triangleq\\widetilde{h}_{G}(x_{t})$ i.e., the label that the effective classifier $\\widetilde{h}_{G}$ assigns to $x_{t}$ . Note that this sequence is defined only for analytical purpose and not required to b e known by either the agnostic algorithm or the experts. ", "page_idx": 15}, {"type": "text", "text": "By definition, $S^{(h)}$ is realizable by the hypothesis $h\\in\\mathcal H$ under graph $G$ . Therefore, Theorem 3.3 guarantees that runnning SSOA on $\\bar{S}^{(h)}$ gives at most $\\mathsf{S L d i m}(\\mathcal{H},G)$ mistakes. Let $m\\leq\\mathsf{S L d i m}(\\mathcal{H},G)$ be the number of mistakes made, and $i_{1},i_{2},\\cdots\\,,i_{m}$ be the time steps at which the mistakes occur. In addition, at every mistake $i_{k}=t\\in[T]$ , let $v_{t}^{(h)}$ be the post-manipulation node observed by the SSOA algorithm running on sequence $S^{(h)}$ . On the other hand, let $v_{t}$ be the observation received by each expert. Although $v_{t}$ may be different from $v_{t}^{(h)}$ because $v_{t}$ is the best response to the agnostic algorithm while $v_{t}^{(h)}$ is the best response to SSOA, we know that $v_{t}$ must be an out-neighbor of $x_{t}$ . Therefore, there must exist an index $r_{k}$ (where $0\\leq r_{k}\\leq\\Delta_{G}^{-},$ such that $x_{t}$ is the $r_{t}$ -th in-neighbor of $v_{t}$ . We argue that $E x p e r t(i_{1:m},r_{1:m})$ is the expert $\\mathfrak{e}_{h}$ that we want. ", "page_idx": 15}, {"type": "text", "text": "We first establish the equivalence of the two following instances of SSOA: ", "page_idx": 15}, {"type": "text", "text": "\u2022 $S S O A^{\\mathfrak{c}_{h}}$ denotes the algorithm instance simulated by expert $\\mathfrak{e}_{h}$ ;   \n\u2022 ${\\mathsf{S S O A}}^{h}$ denotes the algorithm instance running on sequence $S^{(h)}$ . ", "page_idx": 15}, {"type": "text", "text": "We will show by induction that both instances $S S O A^{\\mathfrak{c}_{h}}$ and ${\\mathsf{S S O A}}^{h}$ have the same version space\u2014and as a result, output the same classifier for the next round\u2014at all time steps. This is clearly true at the base case $t=1$ , as the version spaces of both instances are initialized to be $\\mathcal{H}$ . Now we assume the two instances are equivalent up to $t-1$ and prove that they are still equivalent at time $t$ . Since they have the same version spaces $\\mathcal{H}_{t-1}$ , they output the same classifiers for time step $t$ . We denote this classifier by $\\hat{h}_{t}$ as in line 2 of Algorithm 4. ", "page_idx": 15}, {"type": "text", "text": "If $t\\notin\\{i_{1},\\cdot\\cdot\\cdot\\,,i_{m}\\}$ , then the version space $\\mathcal{H}_{t}$ are still the same because neither instances update. Otherwise, there exists $k\\in[m]$ such that $t=i_{k}$ . Since ${\\mathsf{S S O A}}^{h}$ makes a mistake at $i_{k}$ , it will update the version space with observation $(v_{t}^{(h)},y_{t}^{(h)})=(v_{t}^{(h)},-\\hat{h}_{t}(v_{t}^{(h)}))$ . On the other hand, according to line 7 of Algorithm 4, the instance $S S O A^{\\mathfrak{c}_{h}}$ is updated using observation $(\\hat{v}_{t},-\\hat{h}_{t}(\\hat{v}_{t}))$ . Therefore, it suffices to show that vt(h)= v\u02c6t. Since our choice of rk guarantees xt to be the rk-th in-neighbor of vt, we have $\\hat{x}_{t}=x_{t}$ based on line 5 of Algorithm 4. Therefore, both $v_{t}^{(h)}$ and $\\hat{v}_{t}$ are equal to $\\mathsf{b r}_{\\hat{h}_{t},G}(x_{t})$ , so they are the same. As a result, both $S S O A^{\\epsilon_{h}}$ and ${\\mathsf{S S O A}}^{h}$ updates their version space using the same observation, so their $\\mathcal{H}_{t}$ remains the same. By induction, these two instances are equivalent for all time steps. ", "page_idx": 15}, {"type": "text", "text": "Finally, we use the equivalence established above to prove the lemma. Using $(\\hat{h}_{t})_{t\\in[T]}$ to denote the sequence of classifiers outputted by $\\mathfrak{e}_{h}$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{W i s t a k e}_{\\epsilon_{h}}(S)-\\mathsf{M i s t a k e}_{h}(S)=\\displaystyle\\sum_{t=1}^{T}\\mathbb{1}\\left\\{\\hat{h}_{t}(\\mathsf{b r}_{\\hat{h}_{t},G}(x_{t}))\\neq y_{t}\\right\\}-\\displaystyle\\sum_{t=1}^{T}\\mathbb{1}\\{h(\\mathsf{b r}_{h,G}(x_{t}))\\neq y_{t}\\}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\displaystyle\\sum_{t=1}^{T}\\mathbb{1}\\left\\{\\hat{h}_{t}(\\mathsf{b r}_{\\hat{h}_{t},G}(x_{t}))\\neq h(\\mathsf{b r}_{h,G}(x_{t}))\\right\\}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\sum_{t=1}^{T}\\mathbb{1}\\left\\{\\hat{h}_{t}(\\mathsf{b r}_{\\hat{h}_{t},G}(x_{t}))\\neq y_{t}^{(h)}\\right\\}\\qquad(y_{t}^{(h)}=\\widetilde{h}_{G}(x_{t})\\mathrm{~in~}S^{(h)})}\\\\ &{\\qquad\\qquad\\qquad=\\displaystyle\\mathsf{M i s t a k e}_{503}(S^{(h)})\\qquad\\mathrm{(Equivalence~of~SSOA^h~and~SSOA^*_{\\epsilon})}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\mathrm{SLim}(\\mathcal{H},G).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "C.3 The Biased Weighted Majority Vote Algorithm ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We present the algorithm in Algorithm 5. ", "page_idx": 16}, {"type": "image", "img_path": "4Lkzghiep1/tmp/67fbbbdf02ad4bab02455510ddff104ba8f6cc5d7bb789fd165615e0457f66d8.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "C.4 Proof of Theorem 4.1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Theorem 4.1 (Restated). For any adaptive adversarial sequence $S$ of length $T$ , the Agnostic Online Strategic Classification algorithm (Algorithm 3) has regret bound ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathsf{R e g r e t}(S,\\mathcal{H},G)\\leq O\\left(\\Delta_{G}^{+}\\cdot O\\mathsf{P T}+\\Delta_{G}^{+}\\cdot\\mathsf{S L d i m}(\\mathcal{H},G)\\cdot(\\log T+\\log\\Delta_{G}^{-})\\right),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\Delta_{G}^{+}$ (resp. $\\Delta_{G}^{-},$ ) denotes the maximum out-degree (resp. in-degree) of graph $G$ . ", "page_idx": 16}, {"type": "text", "text": "Proof of Theorem 4.1. As showed in the proof sketch, combing the guarantee of Algorithm 5 and bound on $|\\mathfrak{E}|$ gives ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathsf{M i s t a k e}(S,\\mathcal{H},G)\\le\\Delta_{G}^{+}\\cdot{\\mathsf{O P T}}^{\\mathfrak{C}}+\\Delta_{G}^{+}\\cdot\\log|\\mathfrak{E}|\\lesssim{\\mathsf{O P T}}^{\\mathfrak{C}}+\\Delta_{G}^{+}\\cdot d(\\log T+\\log\\Delta_{G}^{-}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where ${\\mathsf{O P T}}^{\\mathfrak{E}}$ denotes the optimal number of mistakes made by the best expert in $\\mathfrak{E}$ ", "page_idx": 16}, {"type": "text", "text": "Applying Lemma 4.3 to the best hypothesis in hindsight $h^{\\star}\\in\\mathcal{H}$ shows that there exists $\\mathfrak{e}_{h^{\\star}}\\in\\mathfrak{E}$ that makes no more than $\\mathsf{O P T}+d$ mistakes, which further implies $\\mathsf{O P T}^{\\mathfrak{E}}\\leq\\mathsf{O P T}+d.$ Hence, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathsf{M i s t a k e}(S,\\mathcal{H},G)\\lesssim\\Delta_{G}^{+}\\cdot(\\mathsf{O P T}+d\\log T+d\\log\\Delta_{G}^{-}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "D Supplementary Materials for Section 5 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "D.1 Realizable graph classes ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Algorithm 6: Online Strategic Classification For Relizable Graph Class   \nInput: Hypothesis class $\\mathcal{H}$ , graph class $\\mathcal{G}$ .   \n1 foreach $G\\in{\\mathcal{G}}$ do   \n2 Let $d_{G}\\gets\\mathsf{S L d i m}(\\mathcal{H},G)$ ;   \n3 foreach $m\\leq d_{G}$ , $i_{1:m},r_{1:m}$ where $1\\leq i_{1}<\\cdot\\cdot\\cdot<i_{m}\\leq T,\\,0\\leq r_{1},\\cdot\\cdot\\cdot,r_{m}\\leq\\Delta_{G}^{-}$ do   \n4 Construct $E x p e r t(i_{1:m},r_{1:m};G)$ as in Algorithm 4.   \n5 end   \n6 end   \n7 Let $G_{\\mathrm{union}}\\gets(\\mathcal{X},\\sum_{G\\in\\mathcal{G}}\\mathcal{E}_{G})$ be the union of graphs in $\\mathcal{G}$ ;   \n8 Run Biased Weighted Majority Vote (Algorithm 5) on the set of experts under graph $G_{\\mathrm{union}}$ . ", "page_idx": 17}, {"type": "text", "text": "Theorem 5.1 (Restated). For any realizable graph class $\\mathcal{G}$ and any adaptive adversarial sequence $S$ of length $T$ , Algorithm $6$ has regret bound ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathsf{R e g r e t}(S,\\mathcal{H},G)\\leq O\\left(\\Delta_{\\mathcal{G}}^{+}\\cdot\\left(\\mathsf{O P T}_{\\mathcal{H}}+d_{\\mathcal{G}}\\cdot(\\log T+\\log\\Delta_{\\mathcal{G}}^{-})+\\log|\\mathcal{G}|\\right)\\right),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $d_{\\mathcal{G}}\\triangleq\\operatorname*{max}_{G\\in\\mathcal{G}}\\mathsf{S L d i m}(\\mathcal{H},G)$ is the maximum strategic Littlestone dimension for all graphs in $\\mathcal{G}$ , $\\Delta_{\\mathcal{G}}^{+}\\ \\triangleq\\ \\Delta_{{G}_{u n i o n}}^{+}$ is the maximum out-degree of $G_{u n i o n}$ (i.e., the union of graphs in $\\mathcal{G}$ ), and $\\Delta_{\\mathcal{G}}^{-}\\triangleq\\operatorname*{max}_{G\\in\\mathcal{G}}\\Delta_{G}^{-}$ is the maximum max in-degree over graphs in $G$ . ", "page_idx": 17}, {"type": "text", "text": "Proof of Theorem 5.1. For each $G\\in{\\mathcal{G}}$ , we use $\\mathfrak{E}_{G}$ to denote the subset of experts constructed in Algorithm 6 for graph $G$ . We also use ${\\mathfrak{E}}\\triangleq\\cup_{G\\in{\\mathcal{G}}}{\\mathfrak{E}}_{G}$ to denote the set of all experts. ", "page_idx": 17}, {"type": "text", "text": "To prove this theorem, we first revisit the regret guarantee for Algorithm 5 in Lemma D.1, especially when the input graph $G$ does not match the actual graph $G^{\\star}$ . The proof of Lemma D.1 largely follows from [Ahmadi et al., 2023], but we include it in the end of this section for completeness. ", "page_idx": 17}, {"type": "text", "text": "Lemma D.1 (Regret of Algorithm 5 [Ahmadi et al., 2023]). If Algorithm 5 is called on manipulation graph $G$ that includes all the edges in the actual manipulation graph $G^{\\star}$ , then the number of mistakes is upper bounded as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathsf{M i s t a k e}(\\mathcal{H},G^{\\star})\\leq O\\left(\\Delta_{G}^{+}\\cdot O\\mathsf{P T}_{G^{\\star}}^{\\mathfrak{E}}+\\Delta_{G}^{+}\\cdot\\log|\\mathfrak{E}|\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\Delta_{G}^{+}$ is the maximum out-degree of graph $G$ , and $\\mathsf{O P T}_{G}^{\\mathfrak{E}},$ \u22c6is the minimum number of mistakes made by the optimal expert under graph $G^{\\star}$ . ", "page_idx": 17}, {"type": "text", "text": "Since $G_{\\mathrm{union}}$ contains all edges in any $G\\in{\\mathcal{G}}$ and thus the unknown $G^{\\star}$ , Lemma D.1 that the number of mistakes made by Algorithm 6 is at most ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\Delta_{\\mathcal{G}}^{+}\\cdot\\mathsf{O P T}_{G^{\\star}}^{\\mathfrak{E}}+\\Delta_{\\mathcal{G}}^{+}\\cdot\\log\\vert\\mathfrak{E}\\vert\\leq\\Delta_{\\mathcal{G}}^{+}\\cdot\\mathsf{O P T}_{G^{\\star}}^{\\mathfrak{E}}+\\Delta_{\\mathcal{G}}^{+}\\cdot\\left(d_{\\mathcal{G}}\\cdot\\log(T\\Delta_{\\mathcal{G}}^{-})+\\log\\vert\\mathcal{G}\\vert\\right),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the second step uses the following upper bound on the number of experts: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{|\\mathfrak{E}|\\le\\displaystyle\\sum_{G\\in\\mathcal{G}\\,m\\le\\mathrm{SLdim}(\\mathcal{H},G)}\\binom{T}{m}\\cdot(\\Delta_{G}^{-})^{m}}&&{}\\\\ &{\\quad\\le|\\mathcal{G}|\\cdot\\displaystyle\\sum_{m\\le d_{\\mathcal{G}}}\\binom{T}{m}\\cdot(\\Delta_{\\mathcal{G}}^{-})^{m}}&&{(\\forall G\\in\\mathcal{G},\\ \\mathrm{SLdim}(\\mathcal{H},G)\\le d_{\\mathcal{G}},\\Delta_{G}^{-}\\le\\Delta_{\\mathcal{G}}^{-})}\\\\ &{\\quad\\lesssim|\\mathcal{G}|\\cdot(T\\cdot\\Delta_{\\mathcal{G}}^{-})^{d_{\\mathcal{G}}+1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, it remains to show that ${\\mathsf{O P T}}_{G^{\\star}}^{\\mathfrak{E}}\\le{\\mathsf{O P T}}_{\\mathcal{H}}+d_{\\mathcal{G}}$ . To this end, we apply Lemma 4.3 the expert class ${\\mathfrak{E}}_{G^{\\star}}$ , in which all experts have the correct belief about the manipulation graph $G^{\\star}$ . For the hypothesis $h^{\\star}\\in\\mathcal{H}$ that achieves $\\mathsf{O P T}_{\\mathcal{H}}$ under $G^{\\star}$ , there must exist $\\mathfrak{e}_{h,G^{\\star}}\\in\\mathfrak{E}_{G^{\\star}}\\subseteq\\mathfrak{E}$ such that $\\mathfrak{e}_{h,G^{\\star}}$ makes at most $\\mathsf{S L d i m}(\\mathcal{H},G^{\\star})\\leq d\\mathcal{G}$ more mistakes than $h^{\\star}$ under $G^{\\star}$ . We have thus proved that $\\mathsf{O P T}_{G^{\\star}}^{\\mathfrak{E}}\\le\\mathsf{O P T}_{\\mathcal{H}}+d\\mathcal{G}$ , which in turn establishes the theorem. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "Proof of Lemma D.1. Suppose a mistake is made in round $t$ , we show the following claims hold: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The total weights decrease by at least constant fraction: $W_{t+1}\\leq W_{t}\\bigl(1-\\gamma/(\\Delta_{G}^{+}+2)\\bigr)$ where $G$ is the input graph.   \n\u2022 The algorithm penalizes experts only if it makes a mistake on $G^{\\star}$ . ", "page_idx": 18}, {"type": "text", "text": "To prove these claims, we consider the following two types of mistakes. ", "page_idx": 18}, {"type": "text", "text": "False positive. Suppose $h_{t}(v_{t})$ is positive but the true label $y_{t}$ is negative. According to the algorithm, $h_{t}$ labels $v_{t}$ positive only when the total weight of experts predicting positive on $v_{t}$ is at least $W_{t}/(\\Delta_{G}^{+}+2)$ . Moreover, each of their weights is decreased by a factor of $\\gamma$ . As a result, we have $W_{t+1}\\leq W_{t}\\big(1-\\gamma/(\\Delta_{G}^{+}+2)\\big)$ and the first claim holds. ", "page_idx": 18}, {"type": "text", "text": "For the second claim, note that the algorithm only penalize experts e where $\\mathfrak{e}_{t}\\big(v_{t}\\big)\\,=\\,+1$ . Since $v_{t}\\,\\in\\,N_{G^{\\star}}^{+}[x_{t}]$ , this implies $\\mathfrak{e}_{t}\\bigl(\\mathsf{b r}_{\\mathfrak{e}_{t},G^{\\star}}\\bigl(x_{t}\\bigr)\\bigr)=+1$ , whereas $y_{t}\\,=\\,-1$ . In other words, the experts penalized must have made a mistake under $G^{\\star}$ . ", "page_idx": 18}, {"type": "text", "text": "False negative. In the case of a false negative, the agent has not moved from a different location to $v_{t}$ to get classified as negative, so $v_{t}\\,=\\,x_{t}$ . Since the agent did not move, none of the vertices in $N_{G^{\\star}}^{+}[\\bar{v}_{t}]$ was labeled positive by the algorithm. However, there might exist some vertices in $N_{G}^{+}[v_{t}]\\setminus N_{G^{\\star}}^{+}[v_{t}]$ that are labeled as positive by the algorithm. Let $\\hat{N}[v_{t}]$ denote the set that includes all vertices in $N_{G}^{+}[v_{t}]$ that are labeled as negative by $h_{t}$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\nN_{G^{\\star}}^{+}[v_{t}]\\subseteq\\hat{N}[v_{t}]\\subseteq N_{G}^{+}[v_{t}].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "According to the algorithm, for each $x\\in\\hat{N}[v_{t}]$ , the total weight of experts predicting $x$ as positive is less than $W_{t}/(\\Delta_{G}^{+}+2)$ where $\\Delta_{G}^{+}$ is the maximum out-degree of $G$ . Therefore, taking the union over all $x\\in\\hat{N}[v_{t}]$ , it implies that the total weight of experts predicting negative on all $x\\in\\hat{N}[v_{t}]$ is at least ", "page_idx": 18}, {"type": "equation", "text": "$$\nW_{t}\\Big(1-|\\hat{N}[v_{t}]|/(\\Delta_{G}^{+}+2)\\Big)\\geq W_{t}\\Big(1-(\\Delta_{G}^{+}+1)/(\\Delta_{G}^{+}+2)\\Big)=W_{t}/(\\Delta_{G}^{+}+2),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the inequality comes from $\\hat{N}[v_{t}]\\subseteq N_{G}^{+}[v_{t}]$ . Reducing their weights by a factor of $\\gamma$ results in $W_{t+1}\\leq W_{t}-(\\gamma W_{t})/(\\Delta_{G}^{+}+2)$ . The first claim holds true. ", "page_idx": 18}, {"type": "text", "text": "As for the second claim, if an expert e is penalized, then $\\mathfrak{e}_{t}(x)\\,=\\,-1$ for all $x\\,\\in\\,\\hat{N}[x_{t}]$ . Since $N_{G\\star}^{+}[x_{t}]\\subseteq\\hat{N}[x_{t}]$ , $\\mathfrak{e}_{t}$ must label all nodes in $N_{G^{\\star}}^{+}[x_{t}]$ as negative. In other words, $\\mathfrak{e}_{t}(\\mathsf{b r}_{\\mathfrak{e}_{t},G^{\\star}}(x_{t}))=$ $-1$ , which means that e must have made a mistake under $G^{\\star}$ . The second claim holds. ", "page_idx": 18}, {"type": "text", "text": "Regret analysis. Let $M=\\mathsf{M i s t a k e}(\\mathcal{H},G^{\\star})$ denote the number of mistakes made by the algorithm. Since the initial weights are all set to 1, we have $W_{0}=|\\mathfrak{E}|$ . The first claim implies that $W_{t+1}\\leq$ ", "page_idx": 18}, {"type": "text", "text": "On the other hand, we use the second claim to show that $W_{T}\\,\\geq\\,\\gamma^{0\\mathsf{P T}_{G}^{\\mathfrak{E}}\\star}$ . We have proved that whenever the algorithm decreases the weight of an expert, they must have made a mistake on $G^{\\star}$ . Let $\\mathfrak{e}^{\\star}\\in\\mathfrak{E}$ denote the best expert that achieves the minimum number of mistakes $\\mathsf{O P T}_{G^{\\star}}^{\\mathfrak{E}}$ under $G^{\\star}$ . From our argument above, the weight of $\\mathfrak{e}^{\\star}$ is penalized by no more than $\\mathsf{O P T}_{G^{\\star}}^{\\mathfrak{E}}$ times. Therefore, after $T$ rounds, $W_{T}\\,\\geq\\,w_{T}(\\mathfrak{e}^{\\star})\\,\\geq\\,\\gamma^{0\\mathsf{P T}_{G}^{\\mathfrak{e}}\\star}$ where the second inequality holds since $0\\,<\\,\\gamma\\,<\\,1$ . Finally, we have: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\gamma^{0{\\mathsf{P T}}_{G^{\\star}}^{\\epsilon}}\\leq W_{T}\\leq|\\mathfrak{E}|\\left(1-\\displaystyle\\frac{\\gamma}{\\Delta_{G}^{+}+2}\\right)^{M}}}\\\\ {{\\Rightarrow0{\\mathsf{P T}}_{G^{\\star}}^{\\mathfrak{E}}\\cdot\\ln\\gamma\\leq\\ln|\\mathfrak{E}|+M\\ln\\left(1-\\displaystyle\\frac{\\gamma}{\\Delta_{G}^{+}+2}\\right)\\leq\\ln|\\mathfrak{E}|-M\\displaystyle\\frac{\\gamma}{\\Delta_{G}^{+}+2}}}\\\\ {{\\Rightarrow M\\leq\\displaystyle\\frac{\\Delta_{G}^{+}+2}{\\gamma}\\ln|\\mathfrak{E}|-\\displaystyle\\frac{\\ln\\gamma(\\Delta_{G}^{+}+2)}{\\gamma}0{\\mathsf{P T}}_{G^{\\star}}^{\\mathfrak{E}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By setting $\\gamma=1/e$ , we bound the total number of mistakes as $M\\leq e(\\Delta_{G}^{+}{+}2)(\\ln|\\mathfrak{E}|\\!+\\!\\mathsf{O P T}_{G^{\\star}}^{\\mathfrak{E}})$ . ", "page_idx": 18}, {"type": "text", "text": "D.2 Agnostic graph classes ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Before presenting the algorithm in the setting of agnostic graph classes, we first introduce an indexing system to the in-neighborhoods of $G_{\\mathrm{union}}$ , which is constructed in the same way as described in Section 4. These indices whill be in the range $\\{0,1,\\cdot\\cdot\\cdot,\\Delta_{\\mathscr{G}}^{-}\\}$ where $\\Delta_{g}^{-}$ is the maximum in-degree of graph $G_{\\mathrm{union}}$ . We now present the algorithm in Algorithms 7 and 8 and prove its regret bound in Theorem D.2. ", "page_idx": 19}, {"type": "text", "text": "Algorithm 7: Online Strategic Classification For Agnostic Graph Class   \nInput: Hypothesis class $\\mathcal{H}$ , graph class $\\mathcal{G}$ , an upper bound $N$ that satisfies $\\mathsf{O P T}_{\\mathcal{G}}\\leq N$ .   \n1 Let $\\begin{array}{r}{G_{\\mathrm{union}}\\overset{\\cdot}{\\leftarrow}(\\mathcal{X},\\sum_{G\\in\\mathcal{G}}\\mathcal{E}_{G})}\\end{array}$ be the union of graphs in $\\mathcal{G}$ , $\\Delta_{\\mathcal{G}}^{-}\\leftarrow\\Delta_{G_{\\mathrm{union}}}^{-}$ ;   \n2 foreach $G\\in{\\mathcal{G}}$ do   \n3 Let $d_{G}\\gets\\mathsf{S L d i m}(\\mathcal{H},G)$ ;   \n4 foreach m $\\begin{array}{r l}&{\\leq d_{G},i_{1:m}\\in\\binom{T}{m},r_{1:m}\\in[\\Delta_{G}^{-}]^{m},n\\leq N,i_{1:n}^{\\prime}\\in\\binom{T}{n},r_{1:n}^{\\prime}\\in[\\Delta_{\\mathcal{G}}^{-}]^{n}\\mathrm{~d}\\mathbf{o}}\\end{array}$   \n5 Construct $E x p e r t(i_{1:m},r_{1:m},i_{1:n}^{\\prime},r_{1:n}^{\\prime};G)$ as in Algorithm 8.   \n6 end   \n7 end   \n8 Run Biased Weighted Majority Vote (Algorithm 5) on the set of experts under graph $G_{\\mathrm{union}}$ . ", "page_idx": 19}, {"type": "text", "text": "Algorithm 8: $E x p e r t(i_{1:m},r_{1:m},i_{1:n}^{\\prime},r_{1:n}^{\\prime};G)$ Input: Hypothesis class $\\mathcal{H}$ , manipulation graph $G$ , indices for mistakes $1\\leq i_{1}<\\cdot\\cdot<i_{m}\\leq T$ , indices for manipulation directions $0\\leq r_{1},\\cdot\\cdot\\cdot\\;,r_{n}\\leq\\Delta_{G}^{-}$ , indices for imperfect graphs $1\\leq i_{1}^{\\prime}<\\cdot\\cdot<i_{n}^{\\prime}\\leq T.$ , and manipulation directions $0\\leq r_{1}^{\\prime},\\cdot\\cdot\\cdot\\;,r_{n}^{\\prime}\\leq\\Delta_{\\mathcal{G}}^{-}$ , the sequence of post-manipulation agents $(v_{t},y_{t})_{t\\in[T]}$ received sequentially. Output: Classifiers $(\\hat{h}_{t})_{t\\in[T]}$ outputted sequentially. Initialization :Simulate an instance of the SSOA algorithm with parameters $(\\mathcal{H},G)$ . 1 for $t\\in[T]$ do 2 $\\hat{h}_{t}\\gets$ classifier outputted by the SSOA algorithm; 3 Observe the manipulated feature vector $v_{t}$ and the true label $y_{t}$ ; 4 if $t\\in\\{i_{1},\\cdot\\cdot\\cdot\\,,i_{m}\\}$ (suppose $t=i_{k}$ ) then $/{*}$ Guess where the original node $x_{t}$ comes from 5 if $t\\in\\{i_{1}^{\\prime},\\cdots,i_{n}^{\\prime}\\}$ (suppose $t=i_{s.}^{\\prime}$ ) then 6 $\\hat{x}_{t}\\gets$ the in-neighbor in $N_{G_{\\mathrm{union}}}^{-}[v_{t}]$ with index $r_{s}^{\\prime}//$ when $G_{t}\\neq G$ , we have $G_{t}\\subseteq G_{\\operatorname{union}}$ 7 else 8 \u2014 $\\hat{x}_{t}\\gets$ the in-neighbor in $N_{G}^{-}[v_{t}]$ with index $r_{k}//\\l$ when $G_{t}=G$ 9 end 10 $\\hat{v}_{t}\\gets\\mathsf{b r}_{\\hat{h}_{t},G}(\\hat{x}_{t})//$ simulate the post-manipulation feature vector in response to $\\hat{h}_{t}$ 11 Update the SSOA algorithm with instance $(\\hat{v}_{t},-\\hat{h}_{t}(\\hat{v}_{t}))$ . 2 end 13 end ", "page_idx": 19}, {"type": "text", "text": "Theorem D.2. For any graph class $\\mathcal{G}$ and hypothesis class $\\mathcal{H}$ , any adaptive adversarial sequence $S$ of length $T$ , and any integer $N$ that is a valid upper bound on $\\mathsf{O P T}_{\\mathcal{G}}$ , Algorithm 7 has regret bound ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathsf{R e g r e t}(S,\\mathcal{H},G)\\leq O\\left(\\Delta_{\\mathcal{G}}^{+}\\cdot\\left(\\mathsf{O P T}_{\\mathcal{H}}+(d\\!\\!_{\\mathcal{G}}+N)\\cdot(\\log T+\\log\\Delta_{\\mathcal{G}}^{-})+\\log|\\mathcal{G}|\\right)\\right),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $d_{\\mathcal{G}}\\triangleq\\operatorname*{max}_{G\\in\\mathcal{G}}\\mathsf{S L d i m}(\\mathcal{H},G)$ is the maximum strategic Littlestone dimension for all graphs in $\\mathcal{G}$ , and $\\Delta_{\\mathcal{G}}^{+}$ (resp. $\\Delta_{g}^{-}$ ) is the maximum out-degree (resp. in-degree) of $G_{u n i o n}$ , where $G_{u n i o n}$ is the union of $\\bar{\\mathcal G}$ that contains edges from all graphs in $\\mathcal{G}$ . ", "page_idx": 19}, {"type": "text", "text": "Proof of Theorem $D.2$ . Similar to the proof of Theorem 5.1, we use ${\\mathfrak{E}}_{G}$ to denote the subset of experts constructed in Algorithm 7 for graph $G$ , and use ${\\mathfrak{E}}\\triangleq\\cup_{G\\in{\\mathcal{G}}}{\\mathfrak{E}}_{G}$ to denote the set of all experts. Since we have $G_{t}\\subseteq G_{\\operatorname{union}}$ at all time steps, Lemma D.1 on the expert set $\\mathfrak{E}$ gives us an upper bound ", "page_idx": 19}, {"type": "text", "text": "on the number of mistakes made by Algorithm 7 as follows: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathsf{M i s t a k e}(S,\\mathcal{H},G_{1:T})\\lesssim\\Delta_{\\mathcal{G}}^{+}\\cdot\\mathsf{O P T}_{G_{1:T}}^{\\mathfrak{E}}+\\Delta_{\\mathcal{G}}^{+}\\cdot\\log|\\mathfrak{E}|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We establish the following lemma, which is a strengthened version of Lemma 4.3 that accounts for the possibility of $G_{t}\\neq G$ . ", "page_idx": 20}, {"type": "text", "text": "Lemma D.3. For any hypothesis $h\\in\\mathcal H$ , any sequence of agents $S$ , and any sequence of graphs $G_{1:T}$ where $\\begin{array}{r}{\\sum_{t=1}^{T}\\mathbb{1}\\big\\{N_{G_{t}}^{+}[x_{t}]\\big\\}\\neq N_{G}^{+}[x_{t}]\\big\\}\\le N}\\end{array}$ , there exists an expert $\\mathfrak{e}_{h}\\in\\mathfrak{E}_{G}$ (which are constructed in Algori thm 7) such that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathsf{M i s t a k e}_{\\epsilon_{h}}(S,G_{1:T})\\leq\\mathsf{M i s t a k e}_{h}(S,G)+N+\\mathsf{S L d i m}(\\mathcal{H},G).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Applying the above lemma to ${\\mathfrak{E}}_{G^{\\star}}$ and using the upper bound ${\\mathsf{O P T}}_{\\mathcal{G}}\\,=\\,\\mathbb{1}\\{G_{t}\\neq G^{\\star}\\}\\,\\leq\\,N$ , we conclude that for the optimal hypothesis $h^{\\star}$ , there must exist expert $\\mathfrak{e}_{h^{\\star},G^{\\star}}\\in\\mathfrak{E}_{G^{\\star}}\\subseteq\\mathfrak{E}$ , such that the number of mistakes $\\mathfrak{e}_{h^{\\star},G^{\\star}}$ makes under $G_{1:T}$ is at most SLdim $\\iota(\\mathcal{H},G^{\\star})+N\\leq d\\mathcal{G}+N$ more than that made by $h^{\\star}$ under $G^{\\star}$ . This implies ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathsf{O P T}^{\\mathfrak{E}}{}_{G_{1:T}}\\leq\\mathsf{O P T}_{\\mathcal{H}}+d_{\\mathcal{G}}+N.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "As for the number of experts, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle|\\mathfrak{E}|=\\sum_{G\\in\\mathcal{G}}\\left(\\sum_{m\\leq\\mathsf{S L d i m}(\\mathcal{H},G)}\\binom{T}{m}\\cdot(\\Delta_{G}^{-})^{m}\\right)\\left(\\sum_{n\\leq N}\\binom{T}{n}\\cdot(\\Delta_{\\mathcal{G}}^{-})^{n}\\right)}\\\\ {\\displaystyle\\leq|\\mathcal{G}|\\left(\\sum_{m\\leq d_{\\mathcal{G}}}\\binom{T}{m}\\cdot(\\Delta_{\\mathcal{G}}^{-})^{m}\\right)\\left(\\sum_{n\\leq N}\\binom{T}{n}\\cdot(\\Delta_{\\mathcal{G}}^{-})^{n}\\right)}\\\\ {\\displaystyle\\qquad\\qquad\\qquad\\qquad(\\forall G\\in\\mathcal{G},\\,\\mathsf{S L d i m}(\\mathcal{H},G)\\leq d_{\\mathcal{G}},\\Delta_{G}^{-}\\leq\\Delta_{\\mathcal{G}}^{-})}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$\\lesssim|\\mathcal{G}|\\cdot(T\\cdot\\Delta_{\\mathcal{G}}^{-})^{d_{\\mathcal{G}}+1}\\cdot(T\\cdot\\Delta_{\\mathcal{G}}^{-})^{N+1}.$ ", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Finally, plugging both (2), (3) into the bound Equation (1) gives us ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathfrak{t}(S,\\mathcal{H},G)\\le O\\left(\\Delta_{\\mathcal{G}}^{+}\\cdot\\left(\\mathsf{O P T}_{\\mathcal{H}}+(d_{\\mathcal{G}}+N)\\cdot(\\log T+\\log\\Delta_{\\mathcal{G}}^{-})+\\log|\\mathcal{G}|\\right)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "as desired. The proof is thus complete. ", "page_idx": 20}, {"type": "text", "text": "Proof of Lemma D.3. We use a similar approach to that of proving Lemma 4.3. We define the sequence $S^{(h)}$ and the indices $i_{1:m},r_{1:m}$ in the same way as Lemma 4.3. In addition, we set $i_{1:n}^{\\prime}$ to be the time steps where $G_{t}~\\neq~G$ , which clearly satisfies $n\\,\\leq\\,N$ . At time step $t\\,=\\,i_{s}^{\\prime}$ $\\bar{(s~\\in~[k])}$ , since $v_{t}$ is the best response according to graph $G_{t}\\ \\subseteq\\ G_{\\operatorname{union}}$ , there exists an index $r_{s}^{\\prime}\\in\\{0,\\cdots\\,,\\Delta_{\\mathcal{G}}^{-}\\}$ such that $x_{t}$ is the $r_{s}^{\\prime}$ -th in-neighbor under graph $G_{\\mathrm{union}}$ . We will show that expert $\\mathfrak{e}_{h}=E x p e r t(\\dot{\\iota}_{1:m},r_{1:m},i_{1:n}^{\\prime},r_{1:n}^{\\prime};G)$ is the one we want. ", "page_idx": 20}, {"type": "text", "text": "Again, we prove this by showing the equivalence of the following two SSOA instances: ", "page_idx": 20}, {"type": "text", "text": "\u2022 $S S O A^{\\mathfrak{c}_{h}}$ is the algorithm instance simulated by expert $\\mathfrak{e}_{h}$ ;   \n\u2022 ${\\mathsf{S S O A}}^{h}$ is the algorithm instance running on sequence $S^{(h)}$ . ", "page_idx": 20}, {"type": "text", "text": "Repeating the induction approach in Lemma 4.3, it suffices to show that the estimate $\\hat{x}_{t}$ correctly matches the true $x_{t}$ during the rounds $i_{1:m}$ . When $t\\notin\\{i_{1}^{\\prime},\\cdots,i_{n}^{\\prime}\\}$ , this is guaranteed by the way indices $r_{1:m}$ are constructed. When there exists $s\\in[n]$ such that $t=i_{s}^{\\prime}$ , this also holds based on the definition of $r_{1:n}^{\\prime}$ . Therefore, the above two SSOA instances are equivalent. ", "page_idx": 20}, {"type": "text", "text": "Finally, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{M i s t a k e}_{\\epsilon_{h}}(S,G_{1:T})-\\mathsf{M i s t a k e}_{h}(S,G)}\\\\ &{=\\displaystyle\\sum_{t=1}^{T}\\mathbb{1}\\Big\\{\\hat{h}_{t}(\\mathsf{b r}_{\\hat{h}_{t},G_{t}}(x_{t}))\\neq y_{t}\\Big\\}-\\displaystyle\\sum_{t=1}^{T}\\mathbb{1}\\big\\{h(\\mathsf{b r}_{h,G}(x_{t}))\\neq y_{t}\\big\\}}\\\\ &{\\le\\displaystyle\\sum_{t=1}^{T}\\mathbb{1}\\big\\{N_{G_{t}}^{+}[x_{t}]\\neq N_{G}^{+}[x_{t}]\\big\\}+\\displaystyle\\sum_{t=1}^{T}\\mathbb{1}\\Big\\{\\hat{h}_{t}(\\mathsf{b r}_{\\hat{h}_{t},G}(x_{t}))\\neq h(\\mathsf{b r}_{h,G}(x_{t}))\\Big\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "(break $[T]$ into two parts on whether $N_{G_{t}}^{+}[x_{t}]\\neq N_{G}^{+}[x_{t}])$ ", "page_idx": 20}, {"type": "text", "text": "$\\leq N+{\\mathsf{S L d i m}}(\\mathcal{H},G)$ . (Combining $\\textstyle\\sum_{t}\\mathbb{1}\\{G_{t}\\neq G\\}\\le N$ and the bound from Lemma 4.3) ", "page_idx": 20}, {"type": "text", "text": "Doubling trick to removing the assumption of knowing $\\mathsf{O P T}_{\\mathcal{G}}$ We end this section with an algorithm for agnostic graph classes that does not require any prior knowledge on $\\mathsf{O P T}_{\\mathcal{G}}$ . We present Algorithm 9 which uses Algorithm 7 as a subrountine and performs the doubling technique on the parameter $N$ . ", "page_idx": 21}, {"type": "text", "text": "This algorithm is based on the important observation that, if $N\\geq{\\mathsf{O P T}}_{\\mathcal{H}}+{\\mathsf{O P T}}_{\\mathcal{G}}+d_{\\mathcal{G}}+\\log|\\mathcal{G}|$ (in particular this implies $N\\,\\geq\\,0\\mathsf{P T}_{\\mathcal{G}})$ , then Theorem D.2 guarantees that running Algorithm 7 with parameter $N$ achieves the mistake upper bound of $O\\left(\\bar{\\Delta_{\\mathcal{G}}^{+}}\\cdot(2N)\\cdot(\\log T+\\log\\bar{\\Delta_{\\mathcal{G}}^{-}})\\right)$ , where the leading constant of the mistake bound can be estimated to be $\\leq4$ by a more careful analysis. Therefore, if we denote define the problem-dependent parameter $C$ to be ", "page_idx": 21}, {"type": "equation", "text": "$$\nC\\triangleq8\\Delta_{\\mathcal{G}}^{+}\\cdot(\\log T+\\log\\Delta_{\\mathcal{G}}^{-}),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "then as long as $N$ reaches value at least ", "page_idx": 21}, {"type": "equation", "text": "$$\nN^{\\star}\\triangleq\\mathsf{O P T}_{\\mathcal{H}}+\\mathsf{O P T}_{\\mathcal{G}}+d_{\\mathcal{G}}+\\log|\\mathcal{G}|,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "running Algorithm 7 makes no more than $C\\cdot N$ mistakes. Therefore, the learner just needs to estimate $N$ through the doubling trick and terminate when the observed mistake does not exceed $C$ times the estimate of $N$ . ", "page_idx": 21}, {"type": "text", "text": "Algorithm 9: Online Strategic Classification For Agnostic Graph Class Input: Hypothesis class $\\mathcal{H}$ , graph class $\\mathcal{G}$ . 1 Let $C\\gets\\overbar{8}\\Delta_{\\mathcal{G}}^{+}\\cdot(\\log T+\\log\\bar{\\Delta_{\\mathcal{G}}^{-}})$ ; Initialization :index of the current epoch $k\\gets1$ 2 while total number of steps $<T$ do $/^{*}$ Epoch $k$ 3 $\\boldsymbol A_{k}\\gets\\mathbf a$ new instance of Algorithm 7 with parameters $(\\mathcal{H},\\mathcal{G},N_{k}=2^{k})$ ); 4 while $\\mathcal{A}_{k}$ has made no more than $C\\cdot2^{k}$ mistakes do 5 Run $\\mathcal{A}_{k}$ for another round 6 end $/^{*}$ Exiting the while loop indicates that $N_{k}<N^{\\star}$ , should double estimate and enter the next epoch $^{*}\\!/$ 7 $k\\gets k+1$ 8 end ", "page_idx": 21}, {"type": "text", "text": "Proposition D.4. For any graph class $\\mathcal{G}$ , any hypothesis class $\\mathcal{H}$ , and any sequences $\\textit{S}=$ $(x_{t},\\stackrel{-}{y}_{t})_{t\\in[T]},(G_{t})_{t\\in[T]}$ , Algorithm 9 enjoys regret bound ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathsf{R e g r e t}(S,\\mathscr{H},\\mathscr{G})\\leq O\\left(\\Delta_{\\mathscr{G}}^{+}\\cdot(\\mathsf{O P T}_{\\mathscr{H}}+\\mathsf{O P T}_{\\mathscr{G}}+d_{\\mathscr{G}}+\\log|\\mathscr{G}|)\\cdot\\log(T\\Delta_{\\mathscr{G}}^{-})\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof of Proposition $D.4.$ Since the above argument has shown that an epoch with $N_{k}\\geq N^{\\star}$ will never terminate, the algorithm will have at most $k^{\\star}=\\log N^{\\star}=\\log({\\mathsf{O P T}}_{\\mathcal{H}}+{\\mathsf{O P T}}_{\\mathcal{G}}+d_{\\mathcal{G}}+\\log|\\mathcal{G}|)$ epochs. Moreover, since the number of mistakes made in any epoch $k$ cannot exceed $C\\cdot2^{k}$ , the total number of mistakes (thus the regret) is upper bounded by ", "page_idx": 21}, {"type": "equation", "text": "$$\nC\\cdot\\sum_{i,\\cdot,\\cdot}^{k^{\\star}}2^{k}\\leq C\\cdot2^{k^{\\star}+1}\\leq O(C N^{\\star})\\leq O\\left(\\Delta_{\\mathcal{G}}^{+}\\cdot(\\mathsf{O P T}_{\\mathcal{H}}+\\mathsf{O P T}_{\\mathcal{G}}+d_{\\mathcal{G}}+\\log|\\mathcal{G}|)\\cdot\\log(T\\Delta_{\\mathcal{G}}^{-})\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "This completes the proof. ", "page_idx": 21}, {"type": "text", "text": "E Randomization Gap ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this section, we present a an instance $(\\mathcal{H},G)$ in which there exists an exponential gap between the optimal mistake bound of deterministic and randomized algorithms. ", "page_idx": 21}, {"type": "text", "text": "Let $G$ be the star graph with center $x_{0}$ and $\\Delta$ leaves $\\{x_{1},\\cdot\\cdot\\cdot,x_{\\Delta}\\}$ . $\\mathcal{H}\\,=\\,\\{h_{1},\\cdot\\cdot\\cdot\\,,h_{\\Delta}\\}$ where $h_{i}(x)\\,=\\,\\mathbb{1}\\{x=x_{i}\\}$ . An adaptive adversary picks a realizable sequence $S=(x_{t},y_{t})_{t\\in[T]}$ where each agent $\\left({{x}_{t}},{{y}_{t}}\\right)$ satisfies $y_{t}=\\widetilde{h}^{\\star}(x_{t})$ for a fixed $h^{\\star}=h_{i^{\\star}}\\in\\mathcal{H}$ . This means all the realizable choices for $\\left({{x}_{t}},{{y}_{t}}\\right)$ are restricted t o the subset $\\{(x_{i^{\\star}},+1),(x_{0},+1)\\}\\cup\\{(x_{i},-1)\\mid i\\neq i^{\\star}\\}$ . ", "page_idx": 21}, {"type": "text", "text": "Deterministic algorithms If the learner is restricted to using deterministic algorithms, then Ahmadi et al. [2023, Theorem 4.6] showed that the optimal mistake is lower bounded by $\\Delta-1$ . This implies $\\mathcal{M}^{\\mathrm{det}}(\\mathcal{H},G)\\geq\\Delta-1$ . ", "page_idx": 22}, {"type": "text", "text": "Randomized algorithms If the learner is allowed to use randomness, we will construct an randomized algorithm $\\boldsymbol{\\mathcal{A}}$ that enjoys an expected mistake bound of $\\log\\Delta$ . This would imply $\\mathcal{M}^{\\mathrm{rand}}(\\mathcal{H},G)\\\\,\\,\\bar{\\leq}\\,\\,\\log\\Delta,$ , which in turn witnesses a super-constant gap between the minmax optimal mistake bounds for deterministic and randomized learners, i.e., ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{\\mathcal{M}^{\\mathrm{det}}(\\mathcal{H},G)}{\\mathcal{M}^{\\mathrm{rand}}(\\mathcal{H},G)}\\geq\\omega(1).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The algorithm $\\boldsymbol{\\mathcal{A}}$ maintains a version space $\\mathcal{H}_{t}$ that consists of all the hypotheses consistent with the history up to time $t$ . This version space is initialized to be $\\mathcal{H}_{1}\\gets\\mathcal{H}$ . At every round, the learner commits to a distribution over classifiers that randomly pick a classifier from the version space, i.e., $h_{t}\\sim\\mathsf{U n i f}(\\mathcal{H}_{t})$ . Let $M(k)$ be the expected of mistakes by $\\boldsymbol{\\mathcal{A}}$ in the future starting from time step $t$ where $|\\mathcal{H}_{t}|=k$ . Consider the following cases: ", "page_idx": 22}, {"type": "text", "text": "\u2022 If the adversary chooses $(x_{t},y_{t})\\;=\\;(x_{i^{\\star}},+1)$ , then the learner makes a mistake with probability $1-{\\frac{1}{k}}$ , but gets to know $i^{\\star}$ afterwards and will make no more mistakes in the future. In this case, we have $\\begin{array}{r}{M(k)=1-\\frac{1}{k}}\\end{array}$ .   \n\u2022 If the adversary chooses $(x_{t},y_{t})=(x_{0},+1)$ , then the learner never makes mistakes, but also gains no information. The version space remains the same $\\mathcal{H}_{t+1}=\\mathcal{H}_{t})$ ). The expected mistakes in the future is still $M(k)$ .   \n\u2022 If the adversary chooses $(x_{t},y_{t})\\,=\\,(x_{i},-1)$ for some $i\\neq i^{\\star}$ , then the learner makes a mistake only when $h_{t}\\,=\\,h_{i}$ , which happens with probability $\\textstyle{\\frac{1}{k}}$ . On the other hand, no matter which $h_{t}$ gets realized, the learner can always observe $(\\stackrel{\\cdot}{v_{t}},\\stackrel{\\wedge}{y_{t}})=(x_{i},-1)$ and update the version space to be $\\mathcal{H}_{t+1}=\\mathcal{H}_{t}\\backslash\\left\\{h_{i}\\right\\}$ . By symmetry of the star graph, the expected mistakes for future rounds is $M(k-1)$ . We thus have $\\begin{array}{r}{\\dot{M(k)}=\\frac{1}{k}+\\bar{M}(\\dot{k}-1)}\\end{array}$ . ", "page_idx": 22}, {"type": "text", "text": "Overall, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\nM(k)\\leq\\operatorname*{max}\\left\\{1-{\\frac{1}{k}},\\ M(k),\\ {\\frac{1}{k}}+M(k-1)\\right\\}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Solving the above recurrence relation gives us $M(k)\\,\\leq\\,\\log k$ , which implies ${\\mathcal{M}}^{\\mathrm{rand}}({\\mathcal{H}},G)\\,\\leq$ Mistake $_{4}(\\mathcal{H},G)=M(\\Delta)\\leq\\log\\Delta$ . ", "page_idx": 22}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The abstract and introduction (especially section 1.1) discuss our technical results. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 23}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: The limitations and future directions are discussed in Section 6. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 23}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: All theorems and claims are formally proved, either in the main paper or in the appendix. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 24}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: This paper does not include experiments. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 24}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: This paper does not include experiments requiring code. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 25}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: This paper does not include experiments. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 25}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: This paper does not include experiments. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: This paper does not include experiments. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 26}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 26}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: This paper is a theoretical work without any direct societal impact to the best of our knowledge. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: This paper poses no such risks. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 27}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: This paper does not use existing assets. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 27}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 28}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 28}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 28}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 28}]