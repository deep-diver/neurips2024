[{"type": "text", "text": "AlphaMath Almost Zero: Process Supervision without Process ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Guoxin Chen\u2217, Minpeng Liao\u2217, Chengxi $\\mathbf{L}\\mathbf{i}^{*}$ , Kai Fan\u2217\u2020", "page_idx": 0}, {"type": "text", "text": "Tongyi Lab chenguoxin22@mails.ucas.ac.cn {minpeng.lmp,xiji.lcx,k.fan}@alibaba-inc.com Code: https://github.com/MARIO-Math-Reasoning/Super_MARIO ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Although recent advancements in large language models (LLMs) have significantly improved their performance on various tasks, they still face challenges with complex and symbolic multi-step reasoning, particularly in mathematical reasoning. To bolster the mathematical reasoning capabilities of LLMs, most existing efforts concentrate on seeking assistance from either domain experts or GPT-4 for high-quality process-supervised data, which is not only expensive but also labor-intensive. In our study, we propose an innovative framework, AlphaMath, that bypasses the need for process annotations (from humans or GPTs) by leveraging Monte Carlo Tree Search (MCTS). This framework focuses on unleashing the potential of a well-pretrained LLM to autonomously enhance its mathematical reasoning. Specifically, we integrate a value model with the LLM, automatically generating both process supervision and step-level evaluation signals in MCTS. Furthermore, we propose an efficient inference strategy\u2014step-level beam search, where the value model is crafted to assist the policy model (i.e., LLM) in navigating more effective reasoning paths, rather than solely relying on prior probabilities. The experimental results on both in-domain and out-of-domain datasets demonstrate that even without GPT-4 or human-annotated process supervision, our AlphaMath framework achieves comparable or superior results to previous state-of-the-art methods. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "table", "img_path": "VaXnxQ3UKo/tmp/2c06fc4beb19d553339fb5d812a72101b0c8366ea1d0447eb872832e4e71d8df.jpg", "table_caption": ["Table 1: Annotation Cost "], "table_footnote": [], "page_idx": 0}, {"type": "text", "text": "Recent studies have extensively explored how to improve mathematical reasoning in large language models (LLMs) [27, 1, 37, 34, 2, 35]. An effective approach [46, 38, 14, 21, 31, 25] is to artificially inject external knowledge into LLMs through fine-tuning on a substantial volume of high-quality, process-supervised data (i.e., solutions). As shown in Table 1, the annotation of high-quality solutions in current efforts primarily relies on domain experts or GPT-4 [27]. However, due to trillions of training tokens and billions of parameters, existing LLMs possess a vast reservoir of knowledge, which remains underutilized in current finetuning-based approaches. ", "page_idx": 0}, {"type": "text", "text": "To more effectively harness the intrinsic knowledge of LLMs, advanced prompting techniques, such as Program-of-Thought (PoT) [6] and Program-Aided Language (PAL) [13], have been developed, integrating the in-context learning proficiency with external tools such as code interpreter to handle precise numerical and symbolic computation. However, these approaches have not fully unleashed the potential of LLMs and often rely on self-consistent majority voting [39], which does not reflect the natural process by which humans solve mathematical problems. This discrepancy arises because both the PoT and PAL frameworks pursue a solution to its final answer regardless of the accuracy of intermediate steps. Unlike these approaches, humans tend to reassess and potentially alter their solution path upon encountering a mistake or dead-end in the problem-solving process. In this manner, humans iteratively enhance their self-cognition and reinforce the utilization of knowledge. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this research, we aspire for LLMs to possess the similar ability as humans to realize self-evolution and strengthen their utilization of knowledge autonomously. Notably, AlphaGo Zero [33] showcases how a neural network model can progressively evolve without human knowledge, autonomously producing the Go game training strategies. For the strategy (i.e., solution) of mathematical problems, both textual analysis [45] and code snippets [14] demand rigorous logical structuring. Consequently, most finetuning-based approaches concentrate on seeking assistance from domain experts or GPT-4 for annotated solutions, thereby overlooking the reservoir of knowledge inherent in LLMs. ", "page_idx": 1}, {"type": "text", "text": "Instead, we hypothesize that well pre-trained LLMs already possess the necessary mathematical knowledge to generate correct reasoning; however, they require appropriate stimulation\u2014such as an improved prompt or search strategy\u2014to do so. In this work, solutions including both textual analysis and code snippets are autonomously generated by a well pre-trained LLM equipped with appropriate prompts and deliberately designed Monte Carlo Tree Search (MCTS) framework [4, 32]. Specifically, we integrate LLMs with the MCTS to strike a more effective balance between exploration and exploitation, enabling the generation of high-quality process-supervised solutions without professional human annotations. To enhance the efficiency of solution generation, we incorporate a value model into the same LLM by appending a linear layer. This advancement removes the necessity for timeconsuming rollouts for reward estimation. While the LLM learns to solve mathematical problems from its own annotated solutions, the value model simultaneously learns how to assess the quality of intermediate reasoning steps from the corresponding state values in MCTS, just like humans. ", "page_idx": 1}, {"type": "text", "text": "During the inference stage, with the value model, LLMs can perform MCTS inference, which significantly enhances their reasoning capabilities but limited by efficiency. Therefore, inspired by beam search algorithm [36], we propose a step-level beam search strategy, where the value model is crafted to assist the policy model (i.e., LLM) in navigating more effective solution paths, as opposed to relying solely on prior probabilities. Compared to the greedy or MCTS inference strategies, the step-level beam search significantly enhances the LLM\u2019s reasoning capability at a minimal cost. ", "page_idx": 1}, {"type": "text", "text": "Empirically, we build an iterative training framework as shown in Figure 1. Unlike in the game of Go, where the final board state directly indicates a win or loss, our methodology requires validation of the equivalence between predicted answers and actual ones. This is the fundamental reason why our training data necessarily consists of question statements and their final answers. Furthermore, we validate the applicability of our framework on three popular types of LLMs: domain-specific pretrained models [31], general-purpose pre-trained models [11], and supervised fine-tuned models [21]. Our contributions are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose a novel approach that integrates a pre-trained LLM with a deliberately designed Monte Carlo Tree Search (MCTS) framework. This combination allows LLMs to autonomously generate high-quality mathematical reasoning solutions without the need for professional human annotations, leading to a more efficient utilization of their inherent knowledge. \u2022 To address the efficiency limitations of MCTS inference, we propose a step-level beam search strategy, which introduces a lightweight value model that works alongside the LLM, enabling the simultaneous assessment of the quality of intermediate reasoning steps. This method parallels human problem-solving by allowing the LLM to learn from its own solutions while also evaluating the effectiveness of its reasoning strategy, thus enhancing the overall reasoning capabilities. \u2022 Extensive experiments demonstrate that our AlphaMath can effectively stimulate the internal knowledge of LLMs, achieving better or on par task performance on both in-domain and out-ofdomain mathematical reasoning datasets, even without any process annotations. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminary ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We assume that, for any given input question q, the solution process can be broken into multiple reasoning steps (e.g., segmenting the solution based on distinct stages or simply on a period). From this perspective, we conceptualize mathematical problem solving within the context of reinforcement learning. Concretely, consider a complete solution consisting of $T$ reasoning steps. At a given time $t$ , we represent the partial solution as the state $\\mathbf{s}_{t}$ , and the subsequent reasoning step that might be taken as the action as ${\\mathbf a}_{t}$ . For detailed definitions and examples of our reasoning step, please refer to Appendix C.1. In this scenario, the policy model is embodied by a large language model, and the transition $f(\\mathbf{s}_{t+1}|\\mathbf{a}_{t},\\mathbf{s}_{t})$ from one state to the next is deterministically accomplished through the concatenation operation. ", "page_idx": 1}, {"type": "image", "img_path": "VaXnxQ3UKo/tmp/16a253f1de5816ba49a23c5d3a9bc4aae7b5421860736689e6e999e4d996be02.jpg", "img_caption": ["Figure 1: Our approach involves iterating through three distinct stages. (1) Collect a mathematical dataset that comprises questions and their corresponding final answers. (2) Employ MCTS on the policy and the value model to generate both correct and incorrect solution paths along with state values. (3) Optimize the policy and the value model with generated data from MCTS. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\pi_{\\boldsymbol{\\theta}}(\\mathbf{a}_{t}|\\mathbf{s}_{t})=\\mathrm{LLM}(\\mathbf{a}_{t}|\\mathbf{s}_{t}),\\qquad\\mathbf{s}_{t+1}=\\mathrm{Cat}(\\mathbf{s_{t}},\\mathbf{a}_{t})\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Our primary goal is to develop a step-level value model, denoted as $V_{\\phi}(\\mathbf{s})$ , which is capable of assessing the expected returns from the current partial solution and guiding the LLM to select more reasonable subsequent reasoning steps. ", "page_idx": 2}, {"type": "text", "text": "To train the value model, we first define the reward in the context of mathematical problem solving, by assigning the reward $r=0$ to all non-terminal reasoning steps, and $r=\\pm1$ to a correct/incorrect final $\\begin{array}{r}{\\widetilde{V}(\\mathbf{s}_{t})=\\frac{1}{N}\\sum_{i=1}^{N}r\\left(\\mathbf{a}_{t^{\\prime}\\geq t}^{(i)},\\mathbf{s}_{t^{\\prime}>t}^{(i)}|\\mathbf{s}_{t}\\right)}\\end{array}$ at\u2032\u2265t , where and represent the actions and states in the $i$ -th simulation sampled by the policy model and the state transition function. $r(\\cdot|\\mathbf s_{t})$ means the reward of the final outcome in one simulation from state $\\mathbf{s}_{t}$ . Then, for any given partial solution s, we can train the step-level value model $V_{\\phi}$ using a regression loss defined as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{L}_{V_{\\phi}}(\\mathbf{s})=\\left\\|V_{\\phi}(\\mathbf{s})-\\widetilde{V}(\\mathbf{s})\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "3 AlphaMath ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In the above approach of MC evaluation, it requires multiple simulations from each state, which may be inefficient in practice. We propose employing the Monte Carlo Tree Search (MCTS) algorithm, which has the potential to reuse simulations and update the estimated values in a principled manner. ", "page_idx": 2}, {"type": "text", "text": "3.1 MCTS Evaluation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "As shown in Figure 1, our approach employs iterative training. Before the $(k+1)$ -th round training, we have a value model $V_{\\phi_{k}}$ and a LLM policy model $\\pi_{\\theta_{k}}$ , which are the same model but with different final layers in our paper. Using these models, we can construct an inference algorithm powered by MCTS. This algorithm starts with the initial state as its root, and through the synergistic use of the policy and value models, systematically grows the search tree by adding new nodes. These nodes correspond to the states deemed to have high potential based on the outcomes of simulated trajectories. Specifically within the context of mathematical problem-solving, as shown in Figure 2, we customize the four key operations of the MCTS algorithm as follows: ", "page_idx": 2}, {"type": "text", "text": "Selection During the $i$ -th simulation of the MCTS, the process begins with $\\mathbf{s}_{\\mathrm{0}}$ , representing the initial state containing the input question. The algorithm then proceeds to explore the tree $\\mathcal{T}_{k}$ by selecting nodes according to a variant of the PUCT algorithm [30]. This selection process is mathematically represented as: ", "page_idx": 2}, {"type": "image", "img_path": "VaXnxQ3UKo/tmp/d7ff58350a5b9c70e14c7059a7d6eb7d8027b0f404c0df10f9c2e790fabe036c.jpg", "img_caption": ["Figure 2: An overview of the four key operations in MCTS "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{a}_{t}=\\arg\\operatorname*{max}_{\\mathbf{a}\\in\\mathcal{T}_{k}}\\left[\\hat{Q}(\\mathbf{s}_{t},\\mathbf{a})+c_{\\mathrm{puct}}\\pi_{\\theta_{k}}(\\mathbf{a}|\\mathbf{s}_{t})\\frac{\\sqrt{N_{p a r e n t}(\\mathbf{a})}}{1+N(\\mathbf{s}_{t},\\mathbf{a})}\\right]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the state-action value $\\hat{Q}(\\mathbf{s},\\mathbf{a})$ and its visiting count $N(\\mathbf{s},\\mathbf{a})$ are stored in the tree and will be updated as the search progresses. $\\bar{N}_{p a r e n t}(\\mathbf{a})$ represents the visiting count of the parent node of a. The action selection iterates until it encounters a leaf node of the current search tree. In our case, the prior $\\pi(\\mathbf{a}|\\mathbf{s}_{t})$ is defined as the exponential of averaged log-probability of all tokens in the step a, i.e., $\\begin{array}{r}{\\exp\\Big(\\frac{1}{|\\mathbf{a}|}\\sum\\log\\pi(a_{j}|\\mathbf{a}_{<j},\\mathbf{s}_{t})\\Big).}\\end{array}$ . ", "page_idx": 3}, {"type": "text", "text": "Expansion Back-tracing from the selected leaf node to the root forms a partial solution, serving as a prompt for further node expansions. In our case, given that the LLM can theoretically generate an unlimited number of potential actions (token sequence), we employ sampling generation with higher temperature to ensure diversity. ", "page_idx": 3}, {"type": "text", "text": "Evaluation Evaluation of the leaf node or partial solution $\\mathbf{s}_{t}$ , identified after the selection phase, is conducted by weighted sum as introduced in [32, 33]. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{V}(\\mathbf{s}_{t})^{(i)}=(1-\\lambda)\\cdot V_{\\phi_{k}}(\\mathbf{s}_{t})+\\lambda\\cdot r\\left(\\mathbf{a}_{t^{\\prime}\\geq t}^{(i)},\\mathbf{s}_{t^{\\prime}>t}^{(i)}|\\mathbf{s}_{t}\\right)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The intermediate value estimation $\\hat{V}$ in MCTS differs from the training signal $\\widetilde{V}$ defined in preliminary section 2. The parameter $\\lambda$ serves to balance the contribution of the value  model\u2019s estimation with the empirical reward obtained during the rollout. ", "page_idx": 3}, {"type": "text", "text": "In our case, we follow a trade-off rollout strategy between AlphaGo [32] and AlphaGo Zero [33]. Because our tree depth is much shallower than Go games (e.g., a maximum depth of 8) and expansions can easily reach a terminal node, we set an indicator function $\\lambda=\\mathbb{I}_{\\mathrm{terminal}}\\big(\\mathbf{s}_{t}\\big)$ . If the expanded node is terminal, the reward is returned; otherwise, the value is predicted by the model $V_{\\phi_{k}}$ . ", "page_idx": 3}, {"type": "text", "text": "Backup We did not make any modifications to the backup. At the end of the $i$ -th simulation, each edge $(\\mathbf{s},\\mathbf{a})$ along the path from the leaf node $\\mathbf{s}_{t}$ to the root undergoes a backward pass update. The updates to their state-action values and visiting counts are executed according to the following rules: $N(\\mathbf{s},\\mathbf{a})\\gets N(\\mathbf{s},\\mathbf{a})+1$ and $\\begin{array}{r}{\\hat{Q}(\\mathbf{s},\\mathbf{a})\\gets\\frac{1}{N(\\mathbf{s},\\mathbf{a})}\\sum_{j=1}^{i}\\mathbb{I}_{\\mathbf{s},\\mathbf{a}\\to\\mathbf{s}_{t}}\\hat{V}(\\mathbf{s}_{t})^{(j)}.}\\end{array}$ . ", "page_idx": 3}, {"type": "text", "text": "Value Estimation After running $N$ simulations with the MCTS algorithm, we obtain the final tree $\\mathcal{T}_{k}$ , which stores the expanded nodes and their corresponding state-action values $Q(\\mathbf{s},\\mathbf{a})$ . Considering that the transition function is deterministic, and assuming that $Q(\\mathbf{s}_{t},\\mathbf{a}_{t})=r(\\mathbf{s}_{t},\\mathbf{a}_{t})+V(\\mathbf{s}_{t+1})=V(\\mathbf{s}_{t+1})$ for non-terminal nodes3, we can employ the $Q$ values as training signals. This implies that we can directly fit the state-action value of non-terminal nodes as, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\widetilde V(\\mathbf{s}_{t+1})=\\hat{Q}(\\mathbf{s}_{t},\\mathbf{a}_{t})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Require: $B_{1}=1$ , question q $\\mathbf{\\tau}(\\mathbf{s}_{0})$ , policy / value models $\\pi_{\\theta},V_{\\phi}$ , simulations $N$ , max depth $T$ .   \n1: Build the complete tree $\\tau$ by running $\\mathrm{MCTS}_{\\pi_{\\theta},V_{\\phi}}\\big(\\mathbf{s}_{0},N,\\dot{T}\\big)$ .   \n2: $\\boldsymbol{\\mathcal{C}}=\\left[{\\bf s}_{0}\\right]$ , $t=0$ \u25b7Initialize candidates   \n3: while $t<T$ and non-terminal path in $\\mathcal{C}$ do   \n4: Initialize priority queue Ct+1 \u25b7Max heap   \n5: for $\\mathbf{s}_{t}$ in $\\mathcal{C}$ do   \n6: for a in $\\boldsymbol{\\mathcal{T}}_{\\mathrm{children}}\\big({\\bf s}_{t}\\big)$ do \u25b7Directly get children from tree   \n7: $\\mathbf{s_{t+1}}=\\mathsf{C a t}\\left[\\mathbf{s}_{t},\\mathbf{a}\\right]$   \n8: Add $(\\mathbf{s}_{t+1},T_{Q}(\\mathbf{s}_{t},\\mathbf{a}))$ to $\\mathcal{C}_{t+1}$ \u25b7Directly get Q-value from tree   \n9: C \u2190Top-B1 of Ct+1   \nreturn Top-1 of C \u25b7Return top-1 as the final solution path ", "page_idx": 4}, {"type": "text", "text": "Algorithm 2 Step-level Beam Search ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Require: Beam sizes $B_{1}$ , $B_{2}$ , question q $\\mathbf{\\tau}(\\mathbf{s}_{0})$ , policy / value models $\\pi_{\\theta},V_{\\phi}$ , max steps $T$ .   \n1: $\\mathcal{C}=[\\mathbf{s}_{0}]*B_{1}$ , $t=0$ \u25b7Initialize candidates   \n2: while $t<T$ and non-terminal path in $\\mathcal{C}$ do   \n3: Initialize priority queue $\\mathcal{C}_{t+1}$ \u25b7Max heap   \n4: for $\\mathbf{s}_{t}$ in $\\mathcal{C}$ do   \n5: Sample a(b) B2 $\\left\\{\\mathbf{a}^{(b)}\\right\\}_{b=1}^{B_{2}}\\sim\\pi_{\\boldsymbol{\\theta}}(\\mathbf{a}|\\mathbf{s}_{t})$ \u25b7LLM generates B2 samples in parallel.   \n6: for $b=1$ to B2 do   \n7: st+1 = Cat st, a(b)   \n8: Add (st+1, V\u03d5(st+1)) to $\\mathcal{C}_{t+1}$ $\\mathsf{\\Gamma}\\mapsto V_{\\phi}\\big(\\mathbf{s}_{t+1}\\big)$ predicted by value model   \n9: $\\bar{c}\\gets\\mathsf{T o p}{-}B_{1}$ of Ct+1   \nreturn Top-1 of C \u25b7Return top-1 as the final solution path ", "page_idx": 4}, {"type": "text", "text": "3.2 Iterative Training ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Initialization Initially, our approach begins with a pre-trained LLM as the policy model $\\pi_{\\theta_{1}}$ . We extend this model by adding an auxiliary linear layer with a tanh activation function, which works alongside the traditional softmax layer responsible for token prediction, as depicted in the rightmost panel of Figure 1. This design implies that these two models, $\\pi_{\\theta}$ and $V_{\\phi}$ , share the majority of their parameters. The parameters of the linear layer associated with $V_{\\phi_{1}}$ are randomly initialized, leading to an initial tendency of the value head to predict a value close to 0 at the first $\\!\\,k=1$ ) round of MCTS. However, as the simulations in the first round MCTS proceed, the rewards $(\\pm1)$ from terminal nodes are back-propagated to their parent nodes. As simulations $N$ gradually increase, the estimated values $\\hat{Q}$ of intermediate nodes converge towards the underlying true value within the range of $[-1,1]$ . ", "page_idx": 4}, {"type": "text", "text": "Training Method From the tree $\\mathcal{T}_{k}$ constructed from the $k$ -th round of MCTS, we can sample solution paths corresponding to terminal nodes with correct and incorrect predicted answers, denoted as $\\mathbf{x}^{+}$ and $\\mathbf{x}^{-}$ , respectively, together with the value estimation of each node along these paths. We then apply a multi-task loss function to update both the policy and value models. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\arg\\operatorname*{min}_{\\theta,\\phi}-\\log\\pi_{\\theta}(\\mathbf{x}^{+}|\\mathbf{q})+\\beta\\cdot\\left(\\sum_{t=1}^{T(\\mathbf{x}^{+})}\\|V_{\\phi}(\\mathbf{s}_{t})-\\widetilde{V}(\\mathbf{s}_{t})\\|^{2}+\\sum_{t=1}^{T(\\mathbf{x}^{-})}\\|V_{\\phi}(\\mathbf{s}_{t})-\\widetilde{V}(\\mathbf{s}_{t})\\|^{2}\\right)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the first term represents the negative log-likelihood loss for next-token prediction in correct solutions, and the second term within the big brackets captures the loss in value prediction for both correct and incorrect solutions, respectively. $T(\\mathbf{x})$ denotes the number of steps for solution path x. $\\beta$ is a tunable hyper-parameter to control the weight of value loss. With the updated policy and value models $\\pi_{\\theta_{k+1}}$ and $V_{\\phi_{k+1}}$ , we can advance to the next-round MCTS, iterating this training process to enhance our models further. ", "page_idx": 4}, {"type": "text", "text": "3.3 Inference ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "MCTS For MCTS inference, it is necessary to set $\\lambda=0$ in the evaluation of Eq. (4). Unlike in board games, we cannot verify the correctness of a path during inference. Therefore, we consistently rely on the value model for node evaluation, including for terminal nodes. MCTS demands multiple simulations to update visiting counts and $Q$ values, aiming to estimate a robust policy distribution. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "After the tree has been completely built, the algorithm iteratively selects the top- $B_{1}$ steps (usually $B_{1}=1$ in MCTS) from the root in a top-down manner. This selection is guided by the maximum $Q$ -value stored in the child nodes of the tree. Subsequently, all child nodes from the previously selected $B_{1}$ steps are collectively re-ranked based on their $Q.$ -values, and the top- $\\cdot B_{1}$ nodes from this ranking are retained for the next iteration. A summary of the algorithm can be found in Algorithm 1. ", "page_idx": 5}, {"type": "text", "text": "Step-level Beam Search However, MCTS is computationally intensive for simulations, making it less viable for use in production environments. To address this, we modify the MCTS inference process by eliminating the backup operation, introducing a simplified method, which we refer to as Step-level Beam Search (SBS), detailed in Algorithm 2. This approach does not construct the entire tree; instead, it dynamically selects the best child node during node expansion. ", "page_idx": 5}, {"type": "text", "text": "There are two primary technical distinctions in SBS. First, since node expansion is required on the fly, we introduce a new beam size, $B_{2}$ , to represent the maximum number of node expansions. Second, the selection criterion no longer relies on the $Q$ -value converged after $N$ simulations but instead uses the maximum value prediction directly from the value model. Importantly, with special case SBS $B_{1}=1$ as a fast approximation of MCTS, it facilitates the sequential, streaming output of steps, rendering it more suitable for practical implementation in real-world production. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Experimental Setup ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this study, we mainly investigate the math domain-specific language model, DeepSeekMathBase-7B [31], pre-trained on a substantial math-related corpus without any supervised fine-tuning (SFT), which is believed to possess necessary mathematical knowledge to tackle a wide range of mathematical problems. ", "page_idx": 5}, {"type": "text", "text": "Training Data Generation via MCTS For the training sets, we exclusively extract question and answer pairs from GSM8K [7] and MATH [15], omitting the human-annotated solution analysis. In total, our training set includes only 15k question-answer pairs and 0 solution process. ", "page_idx": 5}, {"type": "text", "text": "In our setup, we utilize the MCTS framework to generate detailed solution processes equipped with the Python code interpreter. Initially, for the first round of MCTS, the prompt used for our solution generation adheres to the REACT [43] format, incorporating 2 demonstrations randomly selected from a pool of 20 prepared examples. Starting from the second round, with an already fine-tuned model from the first round, we employ a straightforward prompt in our SFT XML format without any demonstration. Two prompt examples are shown in Appendix F. ", "page_idx": 5}, {"type": "text", "text": "Specifically, we iteratively generate data and train our policy and value models through $K=3$ rounds, continuing until the enhancement observed between any two consecutive rounds is incremental. In every round, we build 10 trees for each question-answer pair and randomly sample at most 4 correct and 4 incorrect solution processes. The ratio between positive and negative examples is approximately 1:1, with the count of positive examples in each round varying between $57\\mathrm{k}$ and $59\\mathrm{k}$ . ", "page_idx": 5}, {"type": "text", "text": "Test Data We evaluate our approach not only on GSM8K and MATH but also on out-of-distribution (OOD) datasets GaoKao2023 [21] and OCWCourses [19]. These two OOD datasets are even more challenging than MATH. Please refer to Appendix C.5 for more details about the dataset statistics. To assess the accuracy of the predicted answers, we utilize the math evaluation toolkit [47]. ", "page_idx": 5}, {"type": "text", "text": "Baselines We first compare our approach with strong proprietary and open-source models, including OpenAI\u2019s ChatGPT and GPT-4 [27], Llama2 [37], Llemma [3]. By default, we report the results obtained using Chain of Thought (CoT) prompting [40], along with the prompting results of PAL [13], due to its enhanced performance in mathematical reasoning. ", "page_idx": 5}, {"type": "text", "text": "SFT models leverage high-quality seed data with process supervision derived from GPT-4 or humans to enhance their reasoning capabilities. To ensure a fair comparison, we primarily contrast our approach with the highest-performing SFT models that utilize an external tool - a Python code interpreter. These include MAmmoTH [46], MathCoder [38], ToRA [14], MARIO [21], MathGenie [25], and DeepSeek-Math-Instruct [31]. More implementation details can be found in Appendix C. ", "page_idx": 5}, {"type": "table", "img_path": "VaXnxQ3UKo/tmp/5e72c5faa28df024e1c8cfbd7b38a5deb5620d54924e50c73187a128f24f3480.jpg", "table_caption": ["Table 2: Main results. The best results of open-sourced models are bold. For the methods with released model\u2019s outputs, performance metrics using the evaluation toolkit [47] are also provided in brackets. $^{\\ddagger}$ Seed data refers to high-quality annotated (question, solution) pairs, typically annotated by humans or GPT-4. \u00a7Unless otherwise specified, we set beam size $B_{2}=5$ in SBS and number of simulations $N=40$ in MCTS by default. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "4.2 Main Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We report our in-domain and out-of-domain (OOD) results in Table 2. Different from previous works [46, 38, 14, 21, 25], our proposed AlphaMath does not rely on high-quality solutions annotated by humans or GPT-4, whether in the form of text analysis or code snippets. Such solutions typically bolster the model\u2019s reasoning abilities but also entail substantial costs associated with annotation. Furthermore, our method differs from prior research by not incorporating any external datasets (e.g., new questions and solutions) beyond the GSM8K and MATH datasets. The last five rows of Table 2 present our principal findings. ", "page_idx": 6}, {"type": "text", "text": "First, we establish a baseline with the inherent mathematical reasoning ability of DeepSeekMath-Base using our designed prompt in a 2-shot setting. It\u2019s important to note that this outcome differs from the results reported for DeepSeekMath-Base (PAL) in the original study, as it utilized prompts with 8-shot and 4-shot for the GSM8K and MATH datasets, respectively. Secondly, we only evaluate the policy model with greedy decoding. In comparison to our initial study, we record an enhancement of about 20 points for challenging problems in the MATH, GaoKao2023 (GK2023), and OCWCourses (OCW) datasets, and an improvement of more than 10 points for grade school math problems. Thirdly, we delve into the role of the value model in facilitating mathematical reasoning, utilizing a computationally efficient step-level beam search (SBS) in Algorithm 2. When we increment $B_{1}$ with a default $B_{2}=5$ and temperature of 1.0, a corresponding gradual improvement in performance is observed. More discussion about the temperature in SBS can refer to Appendix 4.7. Ultimately, we evaluate our approach in Algorithm 1. In contrast to the training data generation, we construct a single tree with 40 simulations, a maximum of 5 child nodes, and a temperature of 0.6. While MCTS demonstrates improved performance on more challenging datasets, attributed to its expansive search space, its substantial computational demands curtail its practical applicability in real-world scenarios. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "In summary, our approach demonstrates that, even in the absence of high-quality GPT-4 or humanannotated solution processes, it remains competitive with or surpasses the performance of the state-of-the-art (SOTA) on 7B LLMs. ", "page_idx": 7}, {"type": "text", "text": "4.3 Analysis 1: Performance of each round ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We evaluate the problem-solving rate in the MATH training dataset, which categorizes each problem by difficulty level. As shown in Figure 3, it becomes evident that MCTS achieves greater success in solving more challenging problems in subsequent rounds. In Figure 4, our findings show a general increase in performance with additional rounds of training across all strategies, applicable to both in-domain and out-of-domain test sets. Therefore, we can conclude that the quality of our selfgenerated training data improves incrementally with each round, and this enhancement is reflected in the performance on the test set. More analysis can refer to Appendix B.3. ", "page_idx": 7}, {"type": "text", "text": "4.4 Analysis 2: Performance of different inference strategies ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We explore the performance of our model under various inference strategies including greedy decoding, step-level beam search, and MCTS. The results of MATH and GaoKao2023 are illustrated in Figure 4, while the results of other datasets can be found in Appendix B.1. Specifically, for SBS, an enhancement in performance was observed with an increase in the beam size $B_{1}$ . MCTS exhibited the higher performance than its approximation SBS ( $B_{1}=1;$ ), but we previously noted its significant time consumption and computational inefficiency. Consequently, we provide a summary of the average problem-solving duration and the average number of intermediate steps taken on the MATH dataset in Table 3. The results indicate that MCTS demands the longest solving time and the highest number of steps, attributable to our configuration of 40 simulations. To achieve similar accuracy, step-level beam search is more computationally friendly. Additionally, we observe an intriguing phenomenon: a larger beam size $B_{1}$ tends to reduce the average problem-solving duration. This can be attributed to the decrease in the number of average steps required when a larger $B_{1}$ is employed. ", "page_idx": 7}, {"type": "text", "text": "Discussion of Majority Voting It is challenging to directly compare maj $@5$ with step-level beam search due to the inherent differences in their methodologies. Generally speaking, as Algorithm 2, SBS will eventually return the top-1 final answer based on the value model, while maj $@5$ will generate all 5 possible final answers and vote the majority for evaluation. ", "page_idx": 7}, {"type": "text", "text": "From the step-level perspective, $\\operatorname*{maj}(\\omega5$ will maintain 5 candidates for the current step to generate another 5 candidates for the next step. In contrast, the SBS (e.g., $B_{1}=1,B_{2}=5)$ ) will always retain the top-1 candidate, discarding the 4 others. This provides the advantage of step", "page_idx": 7}, {"type": "table", "img_path": "VaXnxQ3UKo/tmp/2c2b9b30aa5252195e11e3c45c6f53dd53a262cc3625541f655ceb88259c9587.jpg", "table_caption": ["Table 3: Analysis of Computational Efficiency on MATH dataset. # Sol. denotes the number of solutions obtained eventually. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "by-step streaming output in real-world production, whereas maj $@5$ can only output the complete solution until the voting is finalized. To sum up, their specific mechanics of candidate selection and retention differ significantly. ", "page_idx": 7}, {"type": "text", "text": "4.5 Analysis 3: Value model ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In the left panel of Figure 5, we plot the fitted distribution of $Q$ -values (as defined in Eq. (5)) on MATH training set for intermediate steps. For correct solutions, the distribution is markedly skewed towards a value of 1. In contrast, the distribution for incorrect solutions exhibits a lower degree of skewness, albeit with the majority of the probability density leaning towards $-1$ . This is because a correct final answer typically suggests that the entire solution process is likely accurate, whereas an incorrect final answer may still encompass some correct intermediate steps. Thus, with the backup of MCTS, the $Q$ -values of intermediate steps in incorrect solutions may also be updated with a reward of 1 during simulations. ", "page_idx": 7}, {"type": "image", "img_path": "VaXnxQ3UKo/tmp/aedaaafcadf0919b8089952a5cf8308112fd0385c9fa9612d9020917e06a82b6.jpg", "img_caption": ["Levels on MATH Training Set ", "Figure 4: Comparison of Different Inference Strategies. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "In the right panel of Figure 5, we plot the $Q$ - values distribution on the test set, including both intermediate and terminal steps. The distribution associated with correct solutions exhibits a shape similar to that found in the training set. However, the distribution of incorrect solutions, which are the bad cases of the policy model, demonstrates a bimodal pattern. (1) When the value model believes the incorrect solution predicted by the policy model to be incorrect, the $Q$ -values cluster around $-1$ . (2) ", "page_idx": 8}, {"type": "image", "img_path": "VaXnxQ3UKo/tmp/e4aedfaa5050ba1a4e36d933effbeb2ab618dd9133dc7e1a9c47a0a8baba8b4f.jpg", "img_caption": ["Figure 5: (Left) Fitted distribution of $Q$ -values of 3rd round MCTS on the training set. (Right) Fitted distribution of $Q$ -values via MCTS inference on the test set. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Conversely, there are instances where the value model erroneously considers an incorrect solution as correct, resulting in another modal towards 1, which represents the bad cases of the value model. ", "page_idx": 8}, {"type": "text", "text": "4.6 Analysis 4: Self-evolution on General-purpose and SFT models ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We further investigate the potential of two other popular types of LLMs: generalpurpose pre-trained models and SFT models. These models represent the scenarios of lacking continual pre-training (CPT) in domain-specific data and supervised fine-tuning (SFT) on high-quality annotated domain data, respectively. We select Llama3 [11] and MARIO [21] as the base models and report the results in Table 4. For a fair comparison, the MARIO is trained on DeepSeekMath-Base-7B rather ", "page_idx": 8}, {"type": "text", "text": "Table 4: Additional Results on Llama3 and MARIO. \u2020DeepSeekMath-Base-7B. \u00a7Our designed prompt in 2- shot setting. ", "page_idx": 8}, {"type": "table", "img_path": "VaXnxQ3UKo/tmp/17bce90e80c8341d0060beb9eed09fb8b1fc511d65f17ffd3886c390ed97a78d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "than its original Llemma-7B [3]. First, although not proficient in mathematical reasoning, our AlphaMath enhances Llama3\u2019s mathematical reasoning capabilities without any annotations, yielding an average improvement of $+20$ points. Secondly, AlphaMath can significantly enhance the performance of existing SFT models, enabling MARIO to be competitive with and even outperform GPT-4. ", "page_idx": 8}, {"type": "text", "text": "4.7 Analysis 5: The Effects of Temperature on Step-level Beam Search ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We further investigate the effects of temperature during decoding on the performance of inference algorithms. For the greedy strategy, the temperature is consistently maintained at 0, whereas step-level beam search (SBS) and Monte Carlo Tree Search(MCTS) are more significantly influenced by higher temperatures. Therefore, taking steplevel beam search $\\mathcal{B}_{1}=1$ and $B_{1}=3$ ) as an example, we obtained the results as illustrated in Figure 6. ", "page_idx": 8}, {"type": "text", "text": "First, under any temperature setting, the performance of step-level beam search significantly surpasses that of the greedy strategy. This is attributed to the value model effectively assisting the policy model in identifying more effective reasoning paths. Secondly, at lower temperatures, the performance of step-level beam search is constrained due to the lack of diversity in the generated solutions. With elevated temperatures, the value model is capable of discerning optimal paths within a more diverse set of solutions, thereby effectively enhancing reasoning performance. Finally, with a larger beam width, the model can explore more solutions. Therefore, the performance of $B_{1}=3$ always surpasses that of $B_{1}=1$ . ", "page_idx": 8}, {"type": "image", "img_path": "VaXnxQ3UKo/tmp/5b56a5d141687960b4203e36772fb611745d6858b250757c05b5ad7e9ef94591.jpg", "img_caption": ["Figure 6: The Effects of Temperature on the performance of SBS. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "5 Related Works ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Solution Annotation in Math. Recent works [46, 38, 14, 20, 21, 31, 25, 16] on mathematical reasoning have made impressive progress empowered by process-supervised data. However, most existing efforts concentrate on seeking high-quality solutions from domain experts or formidable commercial models, such as GPT-4 [27], which hampers the scalability of methods and escalates the associated expenses. Unlike previous work, only with the help of question-answer pairs, we focus on activating the intrinsic knowledge within LLMs to realize iterative self-evolution and strengthen their utilization of knowledge autonomously, just like humans. ", "page_idx": 9}, {"type": "text", "text": "Value/Reward Model. Recent studies [7, 8, 22, 44, 49, 42, 41, 12] have demonstrated that process supervision can significantly enhance mathematical reasoning performance. Especially, value model [12, 23, 26] is incorporated into the decoding process, while reward model is the source of the training signal in reinforcement learning [28, 31]. However, these value/reward models require substantial annotated process-supervised data and introduce significant inference latency. In our work, we consider the state values $\\widetilde{V}(\\mathbf{s}_{t})$ from MCTS as supervision signals, which are aligned with the solutions and eliminate the ann otation costs. Furthermore, we integrate the value model into the generative model to navigate more effective reasoning paths at minimal cost, thereby providing richer decoding strategies, such as step-level beam search or MCTS. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we introduce AlphaMath, a simple iterative training paradigm for leveraging Monte Carlo Tree Search to unleash the potential of a well pre-trained large language model to autonomously enhance its mathematical reasoning capabilities. Furthermore, by applying step-level beam search, the value model can assist the policy model in selecting a more reasonable solution path, rather than solely relying on prior probabilities, which significantly enhances mathematical reasoning capabilities at minimal cost. The experimental results on both in-domain and out-of-domain datasets demonstrate that even without GPT-4 or human-annotated process supervision, AlphaMath remains competitive with or surpasses the performance of the state-of-the-art methods. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by Alibaba Research Intern Program. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, S. Shakeri, E. Taropa, P. Bailey, Z. Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023.   \n[2] Anthropic. Model card and evaluations for claude models. July 2023.   \n[3] Z. Azerbayev, H. Schoelkopf, K. Paster, M. D. Santos, S. McAleer, A. Q. Jiang, J. Deng, S. Biderman, and S. Welleck. Llemma: An open language model for mathematics. arXiv preprint arXiv:2310.10631, 2023.   \n[4] C. B. Browne, E. Powley, D. Whitehouse, S. M. Lucas, P. I. Cowling, P. Rohlfshagen, S. Tavener, D. Perez, S. Samothrakis, and S. Colton. A survey of monte carlo tree search methods. IEEE Transactions on Computational Intelligence and AI in games, 4(1):1\u201343, 2012.   \n[5] G. Chen, K. Tang, C. Yang, F. Ye, Y. Qiao, and Y. Qian. SEER: Facilitating structured reasoning and explanation via reinforcement learning. In L.-W. Ku, A. Martins, and V. Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5901\u20135921, Bangkok, Thailand, Aug. 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.321. URL https://aclanthology.org/2024.acl-long.321.   \n[6] W. Chen, X. Ma, X. Wang, and W. W. Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588, 2022.   \n[7] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and J. Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.   \n[8] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.   \n[9] T. Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023.   \n[10] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics, pages 4171\u20134186, 2019.   \n[11] A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Yang, A. Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.   \n[12] X. Feng, Z. Wan, M. Wen, Y. Wen, W. Zhang, and J. Wang. Alphazero-like tree-search can guide large language model decoding and training. arXiv preprint arXiv:2309.17179, 2023.   \n[13] L. Gao, A. Madaan, S. Zhou, U. Alon, P. Liu, Y. Yang, J. Callan, and G. Neubig. Pal: Programaided language models. In International Conference on Machine Learning, pages 10764\u201310799. PMLR, 2023.   \n[14] Z. Gou, Z. Shao, Y. Gong, Y. Yang, M. Huang, N. Duan, W. Chen, et al. Tora: A tool-integrated reasoning agent for mathematical problem solving. arXiv preprint arXiv:2309.17452, 2023.   \n[15] D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring mathematical problem solving with the math dataset. Advances in Neural Information Processing Systems, 2021.   \n[16] Y. Huang, X. Lin, Z. Liu, Q. Cao, H. Xin, H. Wang, Z. Li, L. Song, and X. Liang. Mustard: Mastering uniform synthesis of theorem and proof data. arXiv preprint arXiv:2402.08957, 2024.   \n[17] Z. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y. Xu, E. Ishii, Y. J. Bang, A. Madotto, and P. Fung. Survey of hallucination in natural language generation. ACM Computing Surveys, 55(12):1\u201338, 2023.   \n[18] W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. E. Gonzalez, H. Zhang, and I. Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023.   \n[19] A. Lewkowycz, A. Andreassen, D. Dohan, E. Dyer, H. Michalewski, V. Ramasesh, A. Slone, C. Anil, I. Schlag, T. Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35:3843\u20133857, 2022.   \n[20] C. Li, Z. Yuan, G. Dong, K. Lu, J. Wu, C. Tan, X. Wang, and C. Zhou. Query and response augmentation cannot help out-of-domain math reasoning generalization. arXiv preprint arXiv:2310.05506, 2023.   \n[21] M. Liao, W. Luo, C. Li, J. Wu, and K. Fan. Mario: Math reasoning with code interpreter output\u2013a reproducible pipeline. arXiv preprint arXiv:2401.08190, 2024.   \n[22] H. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman, I. Sutskever, and K. Cobbe. Let\u2019s verify step by step. arXiv preprint arXiv:2305.20050, 2023.   \n[23] J. Liu, A. Cohen, R. Pasunuru, Y. Choi, H. Hajishirzi, and A. Celikyilmaz. Don\u2019t throw away your value model! making ppo even better via value-guided monte-carlo tree search decoding. arXiv e-prints, pages arXiv\u20132309, 2023.   \n[24] I. Loshchilov and F. Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.   \n[25] Z. Lu, A. Zhou, H. Ren, K. Wang, W. Shi, J. Pan, M. Zhan, and H. Li. Mathgenie: Generating synthetic data with question back-translation for enhancing mathematical reasoning of llms. arXiv preprint arXiv:2402.16352, 2024.   \n[26] X. Mao, F.-L. Li, H. Xu, W. Zhang, and A. T. Luu. Don\u2019t forget your reward values: Language model alignment via value-based calibration. arXiv preprint arXiv:2402.16030, 2024.   \n[27] OpenAI. Gpt-4 technical report, 2023.   \n[28] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730\u201327744, 2022.   \n[29] S. Rajbhandari, O. Ruwase, J. Rasley, S. Smith, and Y. He. Zero-infinity: Breaking the gpu memory wall for extreme scale deep learning. In Proceedings of the international conference for high performance computing, networking, storage and analysis, pages 1\u201314, 2021.   \n[30] C. D. Rosin. Multi-armed bandits with episode context. Annals of Mathematics and Artificial Intelligence, 61(3):203\u2013230, 2011.   \n[31] Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, M. Zhang, Y. Li, Y. Wu, and D. Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024.   \n[32] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484\u2013489, 2016.   \n[33] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton, et al. Mastering the game of go without human knowledge. Nature, 550 (7676):354\u2013359, 2017.   \n[34] G. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.   \n[35] G. Team, T. Mesnard, C. Hardin, R. Dadashi, S. Bhupatiraju, S. Pathak, L. Sifre, M. Rivi\u00e8re, M. S. Kale, J. Love, et al. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024.   \n[36] C. Tillmann and H. Ney. Word reordering and a dynamic programming beam search algorithm for statistical machine translation. Computational linguistics, 29(1):97\u2013133, 2003.   \n[37] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \n[38] K. Wang, H. Ren, A. Zhou, Z. Lu, S. Luo, W. Shi, R. Zhang, L. Song, M. Zhan, and H. Li. Mathcoder: Seamless code integration in llms for enhanced mathematical reasoning, 2023.   \n[39] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery, and D. Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.   \n[40] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou, et al. Chain-ofthought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824\u201324837, 2022.   \n[41] Y. Weng, M. Zhu, F. Xia, B. Li, S. He, S. Liu, B. Sun, K. Liu, and J. Zhao. Large language models are better reasoners with self-verification. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 2550\u20132575, 2023.   \n[42] Y. Xie, K. Kawaguchi, Y. Zhao, X. Zhao, M.-Y. Kan, J. He, and Q. Xie. Decomposition enhances reasoning via self-evaluation guided decoding, 2023.   \n[43] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. R. Narasimhan, and Y. Cao. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, 2022.   \n[44] F. Yu, A. Gao, and B. Wang. Outcome-supervised verifiers for planning in mathematical reasoning. arXiv preprint arXiv:2311.09724, 2023.   \n[45] L. Yu, W. Jiang, H. Shi, J. Yu, Z. Liu, Y. Zhang, J. T. Kwok, Z. Li, A. Weller, and W. Liu. Metamath: Bootstrap your own mathematical questions for large language models. arXiv preprint arXiv:2309.12284, 2023.   \n[46] X. Yue, X. Qu, G. Zhang, Y. Fu, W. Huang, H. Sun, Y. Su, and W. Chen. Mammoth: Building math generalist models through hybrid instruction tuning. arXiv preprint arXiv:2309.05653, 2023.   \n[47] B. Zhang, C. Li, and K. Fan. Mario eval: Evaluate your math llm with your math llm\u2013a mathematical dataset evaluation toolkit. arXiv preprint arXiv:2404.13925, 2024.   \n[48] Y. Zheng, R. Zhang, J. Zhang, Y. Ye, Z. Luo, and Y. Ma. Llamafactory: Unified efficient fine-tuning of $^{100+}$ language models. arXiv preprint arXiv:2403.13372, 2024. URL http: //arxiv.org/abs/2403.13372.   \n[49] X. Zhu, J. Wang, L. Zhang, Y. Zhang, Y. Huang, R. Gan, J. Zhang, and Y. Yang. Solving math word problems via cooperative reasoning induced language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics, pages 4471\u20134485, 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Contents of Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A Dicussion 15 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Limitation 15   \nA.2 Future Work 15 ", "page_idx": 13}, {"type": "text", "text": "B Supplementary Experiments and Analysis 15 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "B.1 More Results of Inference Strategies 15   \nB.2 More Analysis of Value Model 16   \nB.3 More Analysis of Problem Solving Rate of MCTS in Each Round 16   \nB.4 Problem Solving Rate for Each LLM in Training Set . . 17 ", "page_idx": 13}, {"type": "text", "text": "C Implementation Details 18 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "C.1 Definitions of various elements in MCTS 18   \nC.2 Solution Filtering Algorithm 18   \nC.3 Parameter Details 20   \nC.4 Policy-Value model Details 21   \nC.5 Datasets Details 23   \nC.6 Experiment Environments 23 ", "page_idx": 13}, {"type": "text", "text": "D Case Study 23 ", "page_idx": 13}, {"type": "text", "text": "E Error Analysis 24 ", "page_idx": 13}, {"type": "text", "text": "F Prompts 27 ", "page_idx": 13}, {"type": "text", "text": "F.1 Prompt Example of MCTS in Round 1 27   \nF.2 Prompt Example of MCTS after Round 1 28 ", "page_idx": 13}, {"type": "text", "text": "A Dicussion ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Limitation ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Compared to previous works, our AlphaMath achieves comparable or even superior results without annotated high-quality, process-supervised data. However, unlike the game of Go, where the final board configuration directly reflects winning or losing, in mathematical reasoning, we rely on the actual answer as the source of reward. This hinders us from \u201cAlphaMath really from zero\u201d, an unsupervised algorithm. However, compared to process-supervised data, the acquisition of actual answers is considerably more straightforward. For instance, existing question-answering datasets as well as questions from examinations typically encompass the answers, yet lack annotations for process supervision. ", "page_idx": 14}, {"type": "text", "text": "A.2 Future Work ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Directions for Future Work: Our research highlights several issues for further exploration: ", "page_idx": 14}, {"type": "text", "text": "\u2022 Really from Zero: In this work, we have demonstrated that a well pre-trained large language model can unleash its potential to identify correct mathematical reasoning processes through the AlpahMath framework, independent of GPT-4 or manually annotated process-supervised datasets. A challenging yet profoundly meaningful future direction is to identify an appropriate reward definition that allows AlphaMath to eliminate dependence on actual answers, thereby achieving really from zero. Notably, this process should avoid introducing additional annotation costs, such as training a reward model to replace the actual answers. ", "page_idx": 14}, {"type": "text", "text": "\u2022 A closed-loop self-evolution training framework: With the question-answer pairs, our AlphaMath framework can realize iterative self-evolution in complex reasoning scenarios, just like humans. In this study, as an initial attempt, we have maintained the same set of question-answer pairs (total only $15k$ pairs) in each iteration, which limits the potential of AlphaMath. In the future, we will explore how to automatically obtain such question-answer pairs from the Internet, which could facilitate the development of a closed-loop self-evolution framework for AlphaMath. In this setup, the LLM automatically acquires question-answer pairs from the Internet and then autonomously enhances its reasoning capabilities through our AlphaMath framework, thereby achieving complete independence from human intervention. ", "page_idx": 14}, {"type": "text", "text": "\u2022 Explore beyond mathematical reasoning: Since mathematical reasoning tasks involve complex, symbolic multi-step reasoning, we primarily choose them as an example to investigate the effectiveness of AlphaMath. However, our proposed AlphaMath has the potential to be broadly applied to any task that can be evaluated against actual answers. In future work, we plan to expand its application to a broader range of tasks. ", "page_idx": 14}, {"type": "text", "text": "B Supplementary Experiments and Analysis ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 More Results of Inference Strategies ", "text_level": 1, "page_idx": 14}, {"type": "image", "img_path": "VaXnxQ3UKo/tmp/0ca32e23e8f870061d2804e68cbb10608030ed14d6ee0017d0b60f93db66b74e.jpg", "img_caption": ["Figure 7: Comparison of Different Inference Strategies (Other datasets). "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "In this experiment, we can draw similar conclusions as in Figure 4. With the progress of iteration, there is a significant enhancement in the model\u2019s performance, especially between the first and second rounds, as shown in Figure 7. Furthermore, we observe that the performance of various inference strategies on the OCWCourses slightly differs from the other three datasets. This variation can be attributed to the fact that OCWCourses is a mathematical dataset in the fields of physics and chemistry. Nonetheless, our method still significantly enhances the model\u2019s reasoning capabilities on such datasets overall. ", "page_idx": 15}, {"type": "image", "img_path": "VaXnxQ3UKo/tmp/6ed7bd5f8032a27249b3b64fec3ae87a919b7fdac8ee362588f0d845f222d577.jpg", "img_caption": ["B.2 More Analysis of Value Model ", "Figure 8: (Left) Fitted distribution of value predictions of sampled solutions on the training set. (Right) Fitted distribution of value predictions of sampled solutions on the test set. \"Incorrect inter. step\" denotes the state value $V(\\mathbf{s})$ of an intermediate step within an incorrect solution. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "In addition to the discussion regarding the value model in Section 4.5, we have also specifically analyzed the overall distribution of the predicted state values $V(\\mathbf{s})$ by the value model for both the intermediate and final steps in correct/incorrect solutions, as illustrated in Figure 8. \"Final step\" refers to the scoring of the entire solution in the last step, representing the value model\u2019s overall assessment. ", "page_idx": 15}, {"type": "text", "text": "In the left panel of Figure 8, we plot the ftited distribution of the state values for both intermediate and final steps as predicted by the value model, during the process of solution generation on the training set. For correct and incorrect solutions, the value model\u2019s overall assessment is highly accurate, which is distinctly skewed towards 1 and -1, respectively. Notably, \"Incorrect inter. Step\" represents the intermediate steps of an incorrect solution, rather than incorrect intermediate steps. Therefore, \"Incorrect inter. Step\" may also contain some correct processes, which explains why its distribution crosses over 0. Overall, the distribution of the value model on the training set aligns very well with intuition, which aids in identifying higher-quality solutions in MCTS. ", "page_idx": 15}, {"type": "text", "text": "In the right panel of Figure 8, we plot the distribution of state value $V(\\mathbf{s})$ predicted by the value model on the test set. It can be clearly seen that the value model accurately distinguished between correct and incorrect solutions, which explains why the performance of step-level beam search significantly surpasses that of greedy inference. The value model aids the policy model in navigating more efficient solutions, rather than relying solely on prior probabilities. Additionally, due to the fact that incorrect solutions may contain correct steps, their distribution is primarily concentrated near 0. The intermediate steps of correct solutions exhibit a bimodal distribution, with peaks concentrated near 0 and 1. This can be attributed to the fact that even correct solution steps may contain some errors, such as coding mistakes. Therefore, in conjunction with the analysis in Section 4.5, we believe that our value model can effectively distinguish between correct and incorrect solutions, aiding the policy model in finding better solution paths. ", "page_idx": 15}, {"type": "text", "text": "B.3 More Analysis of Problem Solving Rate of MCTS in Each Round ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this experiment, we evaluate the successful solving rate of MCTS across various rounds. Utilizing the MATH dataset, which categorizes each problem by difficulty level and subject type, we compute the problem-solving rate across different categories and difficulty levels. For our training set, we count the instances wherein problems are successfully solved along any of the paths within the 10 constructed trees. As illustrated in Figure 9a, it becomes evident that MCTS achieves greater success in solving more challenging problems in subsequent rounds. Similarly, Figure 9b indicates that, in later rounds, MCTS consistently demonstrates an improved capability to solve a broader array of problems across different subjects. ", "page_idx": 15}, {"type": "image", "img_path": "VaXnxQ3UKo/tmp/7078e62e706a58b1bc2573a342420a7b7b8f7bef91617f6810d5d112d260e874.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "", "img_caption": ["Figure 10: Problem Solving Rate on MATH Test Set "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "For the test set depicted in Figure 10, we include the results from round 0, which correspond to the performance of our \"prompt 2-shot\" in Table 2. Unlike the training set, we observe that the improvement observed in round 3 is not consistent across different levels and subjects, even though the overall accuracy is slightly increased. In fact, for easier problems, the performance in round 3 actually declines. This is the reason we terminate our iterative training process after round 3. ", "page_idx": 16}, {"type": "text", "text": "B.4 Problem Solving Rate for Each LLM in Training Set ", "text_level": 1, "page_idx": 16}, {"type": "table", "img_path": "VaXnxQ3UKo/tmp/85a76fcf144d97ad4999d2e682a0e53cb4bdd802cf486865b9a8bd3345061954.jpg", "table_caption": ["Table 5: Solving Rate for Each LLM in Training Set. \u00a7Since MARIO is a SFT model and already possesses the capability to follow instructions, we opted to skip its first round. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "We further discuss the problem solving rates of different models in each round, as shown in Table 5. First, given that the problems in the GSM8K dataset are relatively simple, the corresponding solution rates are higher, even in the first round. Despite the challenging nature of MATH, its problem-solving rate increases with more iterations, indicating continuous improvement in the model\u2019s performance. Secondly, there are also noticeable differences in the problem-solving rates between different models. ", "page_idx": 16}, {"type": "text", "text": "Since the SFT model (MARIO) is fine-tuned on high-quality data, it exhibits the best performance. Furthermore, the math domain-specific pre-trained LLM (DeepseekMath-base-7B) significantly outperforms general-purpose pre-trained LLM (Llama3). This phenomenon is intuitive because domain-specific pre-trained LLMs possess more specialized knowledge. Finally, we note that in the third round, the solution rates of each model are quite similar. However, the final performance of the models differs significantly, as shown in Table 4. This discrepancy can be attributed to variations in the quality of solutions generated by different models. Generally speaking, the more relevant knowledge is embedded within LLMs, the higher the quality of problem-solving solutions that will be generated. This explains why domain-specific pretrained models significantly outperform generalpurpose pretrained models. However, the advantage of our proposed AlphaMath lies in its ability to significantly enhance the performance of existing models without relying on high-quality data annotated by GPT-4 or humans. Even with a weaker general-purpose pre-trained model, AlphaMath achieves a remarkable $+20$ points improvement, as shown in Table 4. Furthermore, our AlphaMath enables a domain-specific pre-trained model to achieve comparable or even superior results compared to state-of-the-art SFT models. ", "page_idx": 17}, {"type": "text", "text": "C Implementation Details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "C.1 Definitions of various elements in MCTS ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "State The state $\\mathbf{s}_{t}$ is defined as a partial solution, consisting of the initial prompt (question) and all actions taken along the Monte Carlo tree search path from the root node to the current node, as shown in Eq. 1. ", "page_idx": 17}, {"type": "text", "text": "Node Nodes are used to record information, such as the action (step) ${\\bf a}_{t}$ , the value predicted by the value model $V_{\\phi}$ , the state-action value $Q$ from MCTS, depth, visiting counts, and etc. Each node is defined to only contain a single step. ", "page_idx": 17}, {"type": "text", "text": "Action (Steps) Following Liao et al. [21], we define two types of actions (steps) $\\mathbf{a}_{t}\\colon{\\mathcal{C}}_{}$ -steps and $\\boldsymbol{\\mathcal{A}}$ -steps. Each node contains only one type of action, and $\\boldsymbol{\\mathcal{A}}$ -steps typically appear at the end, as shown in Figure 14. $\\mathcal{C}$ -step represents code execution, which is composed of textual analysis, code snippets, and execution results. The textual analysis and code snippets are generated by the policy model (LLM), while the execution results are the outputs returned by the Python code interpreter. $\\boldsymbol{\\mathcal{A}}$ -step represents the summary of the answer, which is composed of text analysis and predicted answers. Both the text analysis and predicted answers are generated by the policy model. We organize these two steps in the following XML format: ", "page_idx": 17}, {"type": "image", "img_path": "VaXnxQ3UKo/tmp/8a4912e2c2575ff66f3974ee6cb050dd713e27763d834bf40eda4c48b24c7893.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "C.2 Solution Filtering Algorithm ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "After solution generation via MCTS, we randomly sample the correct and incorrect solutions of each question for training. During this process, we found that the generated solutions might suffer from issues such as hallucinations [17]. Hence, we propose a solution filtering algorithm to optimize the solution selection. ", "page_idx": 17}, {"type": "text", "text": "Algorithm 3 outlines the process of our solution filtering algorithm. We initially deduplicate and remove the solutions where code errors are present across all steps (Lines 3-5). As indicated in Figure 11, the solutions that present code errors in all steps yet yield a correct final answer are evidently hallucinations. To preserve the diversity of incorrect solutions, we refrain from excessive processing, which aids the value model in being exposed to a wide variety of incorrect solutions (Lines 6-7). Furthermore, to fliter out higher-quality yet diverse correct solutions, we categorize them into different levels. We consider the solutions where the predicted answer is consistent with the code output as level 1 correct solutions because the model has generated code and understood the output correctly (Lines 9-15). Then, we observe that for some questions, the quality of a solution cannot be solely gauged by the consistency between the code\u2019s output and the predicted answer (for which we provide an example in Figure 13). Therefore, we view the solutions with entirely correct code as level 2 correct solutions (Lines 17-18). Generating correct code to some extent proves that the model has a correct understanding of this question. Finally, we consider the remaining correct solutions as level 3 solutions (Line 20). ", "page_idx": 17}, {"type": "table", "img_path": "VaXnxQ3UKo/tmp/495fad7e0e42a969a26ecbe64fab29331d57c82eafd6139178c1e856cdcd3dc7.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "In general, Algorithm 3 offers a method for swiftly flitering solutions. By categorizing solutions into different levels, we prioritize higher-level solutions for each problem. We provide several examples to intuitively illustrate the necessity and rationality of Algorithm 3, as shown in Figure 11 12 13. ", "page_idx": 18}, {"type": "text", "text": "(a) An example of code errors persist across all steps: ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "<question>   \nWhat is the sum of all positive integers $\\Phi\\backslash\\mathrm{nu}\\Phi$ for which \\$\\mathop{lcm}[\\nu,20] $=\\mathtt{60}\\Phi?$   \n</question>   \n<step>   \n$\\tt{<p>}$   \nTo find the sum of all positive integers $\\Phi\\backslash\\mathrm{nu}\\Phi$ for which \\$\\mathop{lcm}[\\nu,20] $=\\mathtt{60}\\Phi$ , we need to find the values of $\\mathfrak{P}\\backslash\\mathfrak{n u b}$ that satisfy the equation. We can start by simplifying the equation by dividing both sides by 20, which gives us $\\mathfrak{F}$ \\mathop{lcm}[\\nu, $1]=3\\Phi$ . Since $\\mathfrak{F}\\backslash\\mathtt{m a t h o p}\\{\\mathtt{l c m}\\}\\,[1\\,,1]\\,{=}\\mathtt{l}\\mathfrak{F}$ , we can see that $\\Phi\\backslash\\mathrm{nu}\\Phi$ must be equal to 3. Therefore, the sum of all positive integers $\\Phi\\backslash\\mathrm{nu}\\Phi$ for which \\$\\mathop{lcm}[\\nu, $20]\\!=\\!60\\Phi$ is 3.   \n$</{\\tt p}>$   \n<code>   \n\\`\\`python   \nfrom sympy import lcm ", "page_idx": 18}, {"type": "image", "img_path": "VaXnxQ3UKo/tmp/0cffcd142da2ef79ccf336e172a18c68a8034a8e24cab412ad1fb2686b845389.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 11: An example of code errors persists across all steps. In this example, while the model was adjusting the code to rectify coding errors, each step of the solution generated an error. Nevertheless, in the final step, the model still produced the correct answer. This is a classic hallucination. Hence, we discard such solutions in Algorithm 3. ", "page_idx": 19}, {"type": "text", "text": "C.3 Parameter Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "For the solution generation via MCTS, we set $c_{\\mathrm{puct}}=1.25$ , set the temperature between 0.6 to 1.2, limit the maximum tree depth to 8, and allow each node to expand 5 child nodes $(B_{2}\\,=\\,5)$ ), and a maximum of 40 simulations $[N=40]$ ). Since MARIO [21] already has the ability to follow instructions, we set $K\\,=\\,2$ for MARIO. For DeepseekMath-Base [31] and Llama3 [11], we set $K=3$ . In each round, we build 10 trees for each question-answer pair and randomly sample at most 4 correct and 4 incorrect solution processes for training. In this setting, the ratio of positive to negative examples is approximately 1:1, and the count of positive examples varies between 57k to $59\\mathrm{k}$ for each round. ", "page_idx": 19}, {"type": "image", "img_path": "VaXnxQ3UKo/tmp/4a9e073f6c4094a7e1fc5f4186e9b2b2b58b5d2c5d67174dd71fd725d3c064c6.jpg", "img_caption": ["Figure 12: An example of the level 1 correct solution. In this example, the model generates the correct code and provides a correct answer based on the output of this code. We regard such solutions as dependable and categorize this type of solution as the level 1 correct solutions. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "For supervised fine-tuning, we set the learning rate of 4e-5, batch size of 1024, the weight of the value loss to 0.01 or 0.0005 (for Llama3 [11]), and train the model for 10 epochs. We employ the AdamW optimizer [24] and the cosine learning rate scheduler with the warmup rate set to 0.03. More hyperparameter details can be found in Table 6. ", "page_idx": 20}, {"type": "text", "text": "For baselines, the results recorded in Table 2 come from corresponding published papers. ", "page_idx": 20}, {"type": "text", "text": "C.4 Policy-Value model Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Unlike previous work [49, 44, 12, 5], the value model is trained separately to assist the policy model. In this study, we integrate the value model into the policy model by appending a linear layer, as illustrated in Figure 1. Since most of the existing LLMs adhere to decode-only architecture, we utilize the last token as the representation of the entire reasoning step, similar to the role of \u201c[CLS]\u201d token in BERT [10]. In our case, it is typically \u201c</step>\u201d, which ensures that the representation of the reasoning step will not be affected by the last token itself. The value model and the policy model share the majority of parameters, that is, they share the understanding of the reasoning steps. The value model assesses the expected returns based on the current reasoning step, while the policy model generates the next token. ", "page_idx": 20}, {"type": "text", "text": "(c) An example of level 2 Correct Solution ", "page_idx": 21}, {"type": "text", "text": "<question>   \nHow many $\\mathfrak{F}\\mathfrak{F}\\mathfrak{F}.$ -intercepts does the graph of the parabola $\\Phi X~=~2\\mathrm{y}{\\hat{\\mathrm{\\boldmath~\\mu~}}}2\\mathrm{~\\scriptsize~-~}~3\\mathrm{y}~+~7\\Phi$ have? </question>   \n<step>   \n$\\tt{<p>}$   \nTo find the $\\mathfrak{s y b}$ -intercepts of a parabola, we need to set $\\Phi\\mathbf{x}~=~0\\Phi$ and solve for $\\mathfrak{F}\\mathfrak{F}\\mathfrak{G}$ . In this case, we have $\\Phi X~=~2\\mathrm{y}{\\hat{\\mathrm{\\boldmath~\\mu~}}}{\\hat{\\mathrm{\\boldmath~\\mu~}}}-~3\\mathrm{y}~+~7\\Phi$ , so we need to solve the quadratic equation $\\Updownarrow2\\mathrm{y}\\neg2\\ -\\ 3\\mathrm{y}\\ +\\ 7\\ =\\ 0\\Updownarrow$ .   \n$</{\\tt p}>$   \n<code>   \n\\`\\`python   \nfrom sympy import solve, symbols   \n# define variables   \n$\\texttt{y}=$ symbols('y')   \n# solve the quadratic equation   \nsolution $\\mathrm{~\\boldmath~\\lambda~}=\\mathrm{~\\sf~solve}\\left(2\\mathrm{~\\boldmath~*~}\\,{\\sf y}^{**2}\\mathrm{~\\boldmath~-~}\\mathrm{~\\sf~3~}*\\mathrm{~\\boldmath~y~}^{+}\\mathrm{~\\boldmath~7~},\\mathrm{~\\boldmath~y~}\\right)$   \nprint(solution)   \n</code>   \n<output>   \n[3/4 - sqrt(47)\\*I/4, 3/4 + sqrt(47)\\*I/4]   \n</output>   \n</step>   \n<step>   \n$\\tt{<p>}$   \nThe quadratic equation $\\Updownarrow2\\mathrm{y}\\neg2\\ -\\ 3\\mathrm{y}\\ +\\ 7\\ =\\ 0\\Updownarrow$ has no real solutions, which means the parabola has no \\$y\\$-intercepts. We can also see this from the complex solutions for \\$y\\$ in the previous step.   \n$</{\\tt p}>$   \n$\\tt{<p>}$   \nFinal Answer: \\$0\\$   \n</p>   \n</step> ", "page_idx": 21}, {"type": "text", "text": "Figure 13: An example of the level 2 correct solution. In this example, the consistency between the code\u2019s output and the answer does not adequately measure the quality of the solution. Therefore, we categorize solutions that are entirely correct in terms of the code as the level 2 correct solutions. ", "page_idx": 21}, {"type": "table", "img_path": "VaXnxQ3UKo/tmp/af4f8ae0d8b9cc041c12a8715fd262e1c0d7824da174de4f41e457f5c079ad28.jpg", "table_caption": ["Table 6: Key hyperparameters of AlphaMath "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "C.5 Datasets Details ", "text_level": 1, "page_idx": 22}, {"type": "table", "img_path": "VaXnxQ3UKo/tmp/448c408628c72a34613b0e99d4968f598473299b5403b7852d61363e32ead620.jpg", "table_caption": ["Table 7: Datasets Statistics "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "Table 7 describes the statistics of datasets in detail. The division of the training and test set follows the previous work [7, 15]. GSM8K [7] is a multi-step mathematical reasoning dataset comprising high-quality, diverse grade school math word problems, created by human problem writers. MATH [15] is a dataset of challenging competitive mathematics problems. GaoKao2023 [21] is a collection of mathematics problems from the 2023 Chinese National College Entrance Examination, the 2023 American Mathematics Competitions, and the 2023 American College Testing, while OCWCourses [19] comprises a collection of 272 STEM problems aimed at the undergraduate level, requiring multi-step reasoning for most questions. ", "page_idx": 22}, {"type": "text", "text": "C.6 Experiment Environments ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "All experiments were conducted on Ubuntu 22.04 equipped with $8*$ NVIDIA A100 GPUs. Our code mainly depends on Python $3.11^{4}$ and PyTorch $2.1.\\dot{2}^{5}$ . We use our customized Llama Factory [48] as the training framework and our customized vLLM [18] as the inference framework6. We trained all models with DeepSpeed ZeRO Stage2 [29] and Flash-Attention 2 [9]. The pre-trained language models are derived from HuggingFace7. ", "page_idx": 22}, {"type": "text", "text": "D Case Study ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Solution Generation in Round 1 Figure 14 illustrates an example of solution generation on the MATH dataset via MCTS in round 1. We guide the pretrained model, such as DeepseekMathBase [31], to generate solutions in the form of Thought/Action/Action Input/Observation, as shown in Sec. F.1. For clarity of presentation, we only illustrate a subset of the nodes of the original Monte Carlo tree in Figure 14. As shown in Figure 14, the path $(a)\\,\\to\\,(c)\\,\\to\\,(f)$ represents a correct solution, whereas the other solutions contain errors to some degree. Node (b) attempts to solve the problem in a single step and proposes a correct thought. However, minor errors in the coding process $(\"\\mathbf{k}=7**2*3**3\"$ was mistakenly written as $\"\\mathbf{k}=7*3^{**}3\")$ led to mistakes in all subsequent steps. Node (d) attempts a different approach from node (c), specifically trying to solve for $a$ first, then proceeding to solve for $a^{2}$ . Although this process is more redundant compared to that of node (c), it is nonetheless a correct approach. However, in subsequent steps, we encountered errors of various forms. Firstly, within the node $(\\mathrm{g})$ , the model mistakenly treats the output for (d) as equivalent to $a^{2}$ , leading directly to an output. At node (h), the model opts to calculate a relying on its capabilities; however, this results in a numerical error. From a holistic perspective, we observe that, aided by MCTS, the pretrained model attempts to solve the problem through various approaches. During this process, we naturally excavate the knowledge embedded within the model, thereby reinforcing the model\u2019s understanding and application of this knowledge in subsequent training iterations. Furthermore, we collect the $Q$ -values along the path to aid the model in enhancing its judgment of the correctness of the reasoning process. ", "page_idx": 22}, {"type": "text", "text": "Solution Generation in Round 3 Figure 15 illustrates an example of solution generation via MCTS in round 3. Compared to round 1, the quality of the model in round 3 has significantly improved, resulting in more correct solutions. We observe that the code quality in round 3 is superior, containing more comprehensive annotations. This signifies an enhancement in the model\u2019s understanding of the question and its coding proficiency. Furthermore, as illustrated by node (b), the model attempts to resolve the question in a single step, yielding solutions that are both shorter and more efficient. ", "page_idx": 22}, {"type": "image", "img_path": "VaXnxQ3UKo/tmp/cb63b6f2c1b50a1f4f20ae64d4dccd021b212aa073f9f7eff4482af775675805.jpg", "img_caption": ["Figure 14: Example of Solution Generation via MCTS in Round 1. The green and red areas represent correct and incorrect nodes, respectively. The red text segment indicates a specific error in the solution. In round 1, due to the value head being randomly initialized, the estimated values are not accurate; therefore, we have not presented the estimated value here. For the sake of clarity in our demonstration, we only display a subset of the original Monte Carlo tree and present each node in XML format (as detailed in Appendix F.2), even though the format utilized in round 1 was Thought/Action/Action Input/Observation (as detailed in Appendix F.1). "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "E Error Analysis ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this section, we conducted a comprehensive error analysis on the MATH dataset to foster future work. We randomly select 100 error solutions for analysis. We find the following three types of errors. ", "page_idx": 23}, {"type": "text", "text": "(1) Numerical Errors $(53\\%)$ . This is one of the main sources of errors. Within this category of errors, the model usually possesses the correct thought to solve the problem, but an error occurs during the numerical computation. The most common types of numerical computation errors arise from incorrect code generated by the model, which we will not discuss in detail. We have identified errors caused by numerical precision issues in Python, which we refer to as \u201cnumerical precision errors\u201d. Here is an example: ", "page_idx": 23}, {"type": "text", "text": "Question: The first two terms of a sequence are $\\Phi\\mathsf{a}_{-}\\mathsf{1}\\ =\\ 1\\Phi$ and $\\Phi\\mathsf{a}_{-2}=$ $\\setminus\\mathtt{f r a c\\{1\\}}\\{\\setminus\\mathtt{s q r t}3\\}\\Phi$ . For $\\Phi\\mathtt{n}\\backslash\\mathtt{g e}\\;\\;1\\Phi$ , \\begin{align\\*} a_ $\\{{\\bf n}~+~2\\}~=$ \\frac {a_n + $\\mathsf{a_{-}\\{n_{+}+1\\}\\}\\{1\\ -\\ a_{-}\\mathsf{n_{a-}\\{n_{+}+\\ 1\\}}\\}$ . \\end{align\\*}What is $\\mathfrak{P a}_{-}\\{2009\\}\\Phi?$ ", "page_idx": 23}, {"type": "image", "img_path": "VaXnxQ3UKo/tmp/eb618d1beef4851a3965fcbd0c5a47408408759b6f411c4b590212342d69fcdd.jpg", "img_caption": ["Figure 15: Example of Solution Generation via MCTS in Round 3. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "The actual answer: 0 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "The thought of LLM: To find $\\mathfrak{P a}_{-}\\{2009\\}\\Phi$ , we need to calculate the first 2009 terms of the sequence. However, the sequence is defined recursively, so we need to find a way to calculate the next term based on the previous two. ", "page_idx": 24}, {"type": "text", "text": "The code written by LLM: ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "import math   \n$\\mathsf{a}1\\ =\\ 1$   \n$\\mathtt{a}2\\ =\\ 1$ / math.sqrt(3)   \nfor i in range(2, 2009): an $\\r=\\ a_{2}$ an $\\_1\\ =\\ {\\mathsf{a}}1$ $\\begin{array}{l}{\\mathtt{a2}\\ =\\ \\mathtt{(a n~+~a n_-1)}\\ \\ /\\ \\ \\mathtt{(1~-~a n~*~a n_-1)}}\\\\ {\\mathtt{a1}\\ =\\ \\mathtt{a n}}\\end{array}$   \nprint(a2) ", "page_idx": 24}, {"type": "text", "text": "The output of above code: -2.7267039150707353 ", "page_idx": 24}, {"type": "text", "text": "In this example, the model possesses the correct thought of problem-solving and generates the correct Python code. But an incorrect answer was still produced. To ensure numerical precision, we found that the correct implementation should simplify the equation using \u201csympy\u201d, as shown below: ", "page_idx": 24}, {"type": "text", "text": "import sympy as sp $\\mathsf{a}1\\ =\\ 1$ $\\mathtt{a}2\\ =\\ 1$ / sp.sqrt(3) ", "page_idx": 24}, {"type": "text", "text": "for i in range(2, 2009): an $\\r=\\ a_{2}$ an $\\_1\\ =\\ {\\mathsf{a}}1$ $\\mathsf{a}2=$ sp.simplify((an + an_1) /(1-an\\*an_1)) ${\\tt a1}\\ =\\ {\\tt a n}$   \nprint(a2) ", "page_idx": 25}, {"type": "text", "text": "The output of above code: 0 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "(2) Logical Errors $(45\\%)$ . This is another major source of errors. In this type of error, the model usually ignores some details in the question, resulting in incorrect problem-solving approaches. Indeed, humans can also make similar mistakes due to carelessness. Here is an example: ", "page_idx": 25}, {"type": "text", "text": "Question: If $\\Phi\\mathbf{x}\\Phi$ and $\\mathfrak{F}\\mathfrak{N}\\mathfrak{G}$ are numbers such that $\\oint(\\bf x\\ +\\ 3)\\sigma(y\\!-\\!5)\\sigma=\\ 0\\Phi$ , what is the smallest possible value of $\\Phi x^{\\sim}2+\\ y^{\\sim}2\\Phi?$ ", "page_idx": 25}, {"type": "text", "text": "The thought of LLM: To calculate the value of $\\Phi{\\bf x}^{\\sim}2+\\mathrm{~\\bfy~}^{\\sim}2\\Phi$ , we need to solve the equation $\\oint(\\bf x\\ +\\ 3)\\sigma(y\\!-\\!5)\\sigma=\\ 0\\Phi$ . After finding the solutions for $\\Phi\\mathbf{x}\\Phi$ and $\\mathfrak{F}\\mathfrak{N}\\mathfrak{G}$ , we need to calculate $\\Phi{\\bf x}^{\\sim}2+\\mathrm{~\\bfy~}^{\\sim}2\\Phi$ by substituting these values into the expression $\\Phi\\mathbf{x}{\\sim}2\\ +\\ \\mathbf{y}{\\sim}2\\Phi$ . ", "page_idx": 25}, {"type": "text", "text": "The code written by LLM:   \nfrom sympy import symbols, solve   \nx, $\\mathtt{y}=$ symbols $\\because\\textbf{x y}^{\\prime}.$ )   \n${\\bf e q1}~=~{\\tt E q}({\\bf x}~+~3,~{\\boldsymbol{0}})$ )   \n${\\tt e q}2\\;=\\;{\\tt E q}({\\tt y}\\;-\\;5\\,,\\;\\;0)$   \nsolution1 $=$ solve(eq1, x)   \nsolution2 $=$ solve(eq2, y)   \nx_value $=$ solution1[0]   \ny_value $=$ solution2[0]   \nresult $=$ x_value\\*\\*2 $^+$ y_value\\*\\*2   \nprint(result) ", "page_idx": 25}, {"type": "text", "text": "In this example, the model overlooked the requirement in the question to find the smallest possible value, resulting in the incorrect code and final answer. ", "page_idx": 25}, {"type": "text", "text": "(3) Other Errors $(2\\%)$ . Aside from the two common types of errors mentioned above, we have also discovered other errors, which may stem from the ambiguity in the question statements or inaccuracies in the actual answer. Here is an example: ", "page_idx": 25}, {"type": "text", "text": "Question: For which values of $\\mathbb{S}\\mathbb{k}\\Phi$ does the quadratic $\\textcircled{+}\\texttt{k x}\\sim2~-~3\\texttt{k x}+~4\\texttt{k}+~7$ $=\\ 0\\Phi$ have real roots? ", "page_idx": 25}, {"type": "text", "text": "The actual answer: [-4,0) ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "The thought 1 of LLM: To find the values of $\\mathbb{S}\\mathbb{k}\\Phi$ for which the quadratic equation $\\Phi k\\bf{x}\\bf{x}\\bf{\\sim}\\bf2\\;\\;-\\;\\;3k\\bf{x}\\bf\\;+\\;\\;4k\\tau+\\tau\\tau=\\;0\\Phi$ has real roots, we need to consider the discriminant of the equation, which is $\\Phi b^{-}2-4a c\\Phi$ . If the discriminant is positive, the equation has two distinct real roots, and if it is zero, the equation has one real root. ", "page_idx": 25}, {"type": "text", "text": "The code 1 written by LLM: ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "from sympy import symbols, Eq, solve   \n$\\texttt{k}=$ symbols $(\\,\"\\,\\mathbf{k}\\,\"\\,)$   \ndiscriminant $=$ $=(-3+2+1)\\ast\\ast2$ - $4*\\mathtt{k}*(4*\\mathtt{k}+\\mathtt{\\Omega}7)$   \nprint(discriminant) ", "page_idx": 25}, {"type": "text", "text": "The output of above code: $9*7\\times**2-4*7\\times*(4*7)$ ", "page_idx": 25}, {"type": "text", "text": "The thought 2 of LLM: The discriminant is $\\Re\\Re\\{\\sim2\\mathrm{~\\textrm~{~-~}~}4\\mathtt{k}\\left(4\\mathtt{k}\\ +\\ 7\\right)\\Phi}$ . To find the values of $\\mathbb{S}\\mathbb{k}\\Phi$ for which the discriminant is non-negative, we need to solve the inequality $\\oint\\!\\!\\!\\9\\mathbf{k}\\!\\!\\!\\sim\\!\\!\\!2\\ -\\ 4\\mathbf{k}\\!\\!\\left(4\\mathbf{k}\\ +\\ 7\\right)\\ \\ \\backslash\\!\\!\\!\\operatorname{geq}\\ 0\\Phi$ . ", "page_idx": 25}, {"type": "text", "text": "The code 2 written by LLM: ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "from sympy import symbols, Eq, solve   \n$\\texttt{k}=$ symbols $(\\,\"\\,\\mathbf{k}\\,\"\\,)$   \ninequality $=$ $=9\\times1\\times4\\times2$ - $4*7*(4*7)>=0$   \nsolutions $=$ solve(inequality, k)   \nprint(solutions) ", "page_idx": 26}, {"type": "text", "text": "The output of above code: $(-4<=\\mathtt{k})\\ \\ \\&\\ \\ (\\mathtt{k}\\ \\ <=\\ 0)$ ", "page_idx": 26}, {"type": "text", "text": "The predicted answer by LLM: $\\Phi\\left[-4\\,,\\right.\\,\\left.0\\right]\\Phi$ ", "page_idx": 26}, {"type": "text", "text": "In this example, the model correctly solved the problem. However, due to the inaccuracy of the actual answer, the evaluation tool mistakenly deemed the model\u2019s predicted answer as incorrect. ", "page_idx": 26}, {"type": "text", "text": "F Prompts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We provide instructions and examples for generating solutions and performing reasoning. For the solution generation through MCTS in round 1, the pre-trained models such as DeepseekMathbase [31] may not adhere to instructions. Therefore, we employ few-shot learning to guide the model towards producing outputs in the format of Thought/Action/Action Input/Observation. Subsequently, we train the model in our SFT XML format and proceed with both the solution generation and reasoning in this format. We provide specific details regarding two formats of prompts as follows: ", "page_idx": 26}, {"type": "text", "text": "F.1 Prompt Example of MCTS in Round 1 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We employ few-shot learning to steer pre-trained models toward performing mathematical reasoning, ensuring the output conforms to the format of Thought/Action/Action Input/Observation. As the following example shows, the text in black is prompt, and the text in red is model generation. ", "page_idx": 26}, {"type": "text", "text": "You are a powerful agent with broad math knowledge and great Python programming skills, but not good at accurate calculation on math equations. Answer the math questions as best you can. You have access to the following tool: ", "page_idx": 26}, {"type": "text", "text": "Python_interpreter: A Python shell. Use this to execute python commands. ", "page_idx": 26}, {"type": "text", "text": "!!!Remember: ", "page_idx": 26}, {"type": "text", "text": "1. When solving math problem, you should think step by step, where each step includes 4 mini-steps Thought/Action/Action Input/Observation. ", "page_idx": 26}, {"type": "text", "text": "2. If some step requires accurate calculation (including but not limited to numerical calculation and symbolic deduction), you should write Python code and execute for accurate result. ", "page_idx": 26}, {"type": "text", "text": "3. The most related Python packages include \\`math\\`, \\`sympy\\`, \\`scipy\\` and \\`numpy\\`. ", "page_idx": 26}, {"type": "text", "text": "4. Please use the following template. ", "page_idx": 26}, {"type": "text", "text": "Question: the input question ", "page_idx": 26}, {"type": "text", "text": "Thought: the text analysis, and list the math equations if necessary ", "page_idx": 26}, {"type": "text", "text": "Action: the action to take, should be \\`Python_interpreter\\`, or None ", "page_idx": 26}, {"type": "text", "text": "Action Input: the Python Code in markdown format (if Action is None, it is None), e.g., ", "page_idx": 26}, {"type": "text", "text": "\\`python   \nimport math   \ntheta $=$ math.pi / 4   \nsome_var $=$ math.cos(theta) ", "page_idx": 26}, {"type": "text", "text": "print(some_var) ", "page_idx": 27}, {"type": "text", "text": "Observation: the result of the action . (this Thought/Action/Action Input/Observation can repeat N times) ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "Thought: the final analysis of the answer ", "page_idx": 27}, {"type": "text", "text": "Final Answer: the concise answer without verbose context ", "page_idx": 27}, {"type": "text", "text": "Here are some examples: ", "page_idx": 27}, {"type": "text", "text": "<examples here> ", "page_idx": 27}, {"type": "text", "text": "Now! It's your turn. ", "page_idx": 27}, {"type": "text", "text": "Question: Ben works 8-hour shifts in a furniture shop. It takes him 5 hours to build 1 rocking chair. How many chairs can he build in 10 days? ", "page_idx": 27}, {"type": "text", "text": "Thought: To calculate the number of chairs Ben can build in 10 days, we need to first determine how many hours he would work in 10 days and then use the given information to calculate the number of chairs he can build in those hours. ", "page_idx": 27}, {"type": "text", "text": "Action: python_interpreter ", "page_idx": 27}, {"type": "text", "text": "Action Input: \\`\\`\\`python   \nhours_per_day $=~8$   \ntotal_days $=~10$   \nhours_in_10_days $=$ hours_per_day $^*$ total_days   \nchairs_per_hour $=~1~\\textit{/}~5$   \nchairs_in_10_days $=$ hours_in_10_days $^*$ chairs_per_hour   \nprint(chairs_in_10_days) ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "Observation: 16.0 ", "page_idx": 27}, {"type": "text", "text": "Thought: I have calculated the number of chairs Ben can build in 10 days.   \nThe number of chairs is 16. ", "page_idx": 27}, {"type": "text", "text": "Final Answer: 16 ", "page_idx": 27}, {"type": "text", "text": "F.2 Prompt Example of MCTS after Round 1 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "To train the SFT model in executing mathematical reasoning, we utilize an XML format alongside zero-shot learning. This approach is adopted because the math-related pre-training corpora are predominantly harvested from the Internet, where HTML tags serve to distinguish various types of content, including text, equations, and code snippets. As the following example shows, the text in black is prompt, and the text in red is model generation. ", "page_idx": 27}, {"type": "text", "text": "<question>Haley grows at the rate of 3 inches every year. If she is currently 20 inches tall, what will be her height after 10 years?</question> <step> ", "page_idx": 27}, {"type": "text", "text": "<p>   \nTo calculate Haley's height after 10 years, I need to add 10 times the growth rate of 3 inches to her current height.   \n</p> ", "page_idx": 27}, {"type": "text", "text": "<code> ", "page_idx": 27}, {"type": "text", "text": "\\`python current_height $=~20$ ", "page_idx": 27}, {"type": "text", "text": "growth_rate $=~3$   \nyears $=~10$   \nfuture_height $=$ current_height $^+$ (growth_rate $^*$ years)   \nprint(future_height) </code>   \n<output>   \n50   \n</output>   \n</step>   \n<step>   \n<p>   \nI have calculated Haley's height after 10 years. Haley will be 50 inches tall after 10 years.   \n</p>   \n<p>   \nFinal Answer: $\\mathbb{5}50\\Phi$   \n</p>   \n</step> ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: The main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope. The paper clearly states its objectives, methodology, and results in the abstract and introduction sections. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 29}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: Due to space constraints, we discuss the limitations of this work and future work in detail in Appendix A. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 29}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: We provide detailed theoretical assumptions in Section 2. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 30}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: We provide detailed descriptions of our algorithms, experimental setup, and information about hyperparameters, data preprocessing steps, and so on, to ensure that other researchers can reproduce our main experimental results. You can find these details in Section 4.1 \"Experimental Setup\" and Appendix C \"Implementation Details\". ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 30}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: All code and data have been submitted in the supplementary materials with sufficient documentation. We have ensured that all experimental results can be faithfully reproduced. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 31}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: We thoroughly specify all the training and test details in our paper, including how we divided the data into training and testing sets, the settings of hyperparameters, and the types of optimizers deployed in the model. These details are crucial for understanding and replicating our results, and please refer to Section 4.1 \"Experimental Setup\" and Appendix C \"Implementation Details\" for this information. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 31}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We have provided detailed parameter settings and conducted error analysis, reporting the average experimental results. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 31}, {"type": "text", "text": "\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 32}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We provide a detailed description of our experimental environment in Appendix C.6. We also discuss in detail the running times of various algorithms in our experiments. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 32}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We have ensured that the research conducted in our paper complies with the NeurIPS Code of Ethics in all respects. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 32}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: In Appendix A, we discuss in detail the potential positive and negative social impacts of our work. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 33}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: The work we have done in our paper does not involve such risks. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 33}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We have strictly adhered to the licensing and usage terms of the data and models. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: For all new assets introduced in the paper, such as data, code, and models, we provide detailed documentation in the supplementary materials. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 34}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 34}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 35}]