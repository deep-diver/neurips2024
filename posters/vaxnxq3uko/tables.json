[{"figure_path": "VaXnxQ3UKo/tables/tables_0_1.jpg", "caption": "Table 2: Main results. The best results of open-sourced models are bold. For the methods with released model's outputs, performance metrics using the evaluation toolkit [47] are also provided in brackets. #Seed data refers to high-quality annotated (question, solution) pairs, typically annotated by humans or GPT-4. $Unless otherwise specified, we set beam size B2 = 5 in SBS and number of simulations N = 40 in MCTS by default.", "description": "This table presents the main results of the AlphaMath model and compares it to various other models on both in-domain and out-of-domain datasets.  It shows the performance of different models across various metrics, including the use of different tools, the amount of seed data used, the annotation sources used for the seed data, and the overall performance achieved.  The table highlights the performance of AlphaMath, both with and without the step-level beam search (SBS) and Monte Carlo Tree Search (MCTS) methods, demonstrating its effectiveness.", "section": "4.2 Main Results"}, {"figure_path": "VaXnxQ3UKo/tables/tables_6_1.jpg", "caption": "Table 2: Main results. The best results of open-sourced models are bold. For the methods with released model's outputs, performance metrics using the evaluation toolkit [47] are also provided in brackets. #Seed data refers to high-quality annotated (question, solution) pairs, typically annotated by humans or GPT-4. $Unless otherwise specified, we set beam size B2 = 5 in SBS and number of simulations N = 40 in MCTS by default.", "description": "This table presents the main results of the AlphaMath model and compares its performance to various other models (proprietary, open-source, and fine-tuned) across several datasets.  It includes metrics on both in-domain and out-of-domain datasets. The table specifies model size, the presence and type of seed data and annotation used for training, tools employed, and the performance metrics (accuracy) for each model and dataset.  It also notes the use of specific techniques like step-level beam search (SBS) and Monte Carlo Tree Search (MCTS) for AlphaMath. ", "section": "4.2 Main Results"}, {"figure_path": "VaXnxQ3UKo/tables/tables_7_1.jpg", "caption": "Table 2: Main results. The best results of open-sourced models are bold. For the methods with released model's outputs, performance metrics using the evaluation toolkit [47] are also provided in brackets. #Seed data refers to high-quality annotated (question, solution) pairs, typically annotated by humans or GPT-4. $Unless otherwise specified, we set beam size B2 = 5 in SBS and number of simulations N = 40 in MCTS by default.", "description": "This table presents the main results of the AlphaMath model and compares its performance against various baselines on both in-domain and out-of-domain datasets.  It shows the accuracy, along with the performance metrics (in brackets) obtained using an evaluation toolkit.  The table also highlights the model size, whether seed data and external tools were used, and the specific inference strategies (SBS or MCTS) employed. It differentiates between proprietary and open-source models as well as SFT models. The best performing open-source models are bolded for easier readability.", "section": "4.2 Main Results"}, {"figure_path": "VaXnxQ3UKo/tables/tables_8_1.jpg", "caption": "Table 2: Main results. The best results of open-sourced models are bold. For the methods with released model's outputs, performance metrics using the evaluation toolkit [47] are also provided in brackets. #Seed data refers to high-quality annotated (question, solution) pairs, typically annotated by humans or GPT-4. $Unless otherwise specified, we set beam size B2 = 5 in SBS and number of simulations N = 40 in MCTS by default.", "description": "This table presents the main results of the AlphaMath model and compares its performance to various other models on both in-domain and out-of-domain datasets.  It shows the model size, whether seed data (high-quality annotated question-solution pairs) was used, what tools were used (if any), and the accuracy scores on four datasets (GSM8K, MATH, GK2023, and OCW).  The table highlights the performance gains achieved by AlphaMath, especially when combined with the step-level beam search (SBS) method, even without using seed data.  Note that the performance with different inference strategies (greedy, SBS, and MCTS) is also included for the AlphaMath model.", "section": "4.2 Main Results"}, {"figure_path": "VaXnxQ3UKo/tables/tables_16_1.jpg", "caption": "Table 2: Main results. The best results of open-sourced models are bold. For the methods with released model's outputs, performance metrics using the evaluation toolkit [47] are also provided in brackets. #Seed data refers to high-quality annotated (question, solution) pairs, typically annotated by humans or GPT-4. $Unless otherwise specified, we set beam size B2 = 5 in SBS and number of simulations N = 40 in MCTS by default.", "description": "This table presents the main results of the AlphaMath model and compares its performance with various other models on different mathematical reasoning datasets.  It shows the accuracy of different models on in-domain (GSM8K, MATH) and out-of-domain (GaoKao2023, OCWCourses) datasets.  The table also indicates whether each model uses process supervision (seed data), the size of the seed data, the tools used, and the model size.  Finally, it highlights AlphaMath's performance with and without MCTS and different beam search strategies, indicating the model's capability even without high-quality, annotated data.", "section": "4.2 Main Results"}, {"figure_path": "VaXnxQ3UKo/tables/tables_18_1.jpg", "caption": "Table 2: Main results. The best results of open-sourced models are bold. For the methods with released model's outputs, performance metrics using the evaluation toolkit [47] are also provided in brackets. #Seed data refers to high-quality annotated (question, solution) pairs, typically annotated by humans or GPT-4. $Unless otherwise specified, we set beam size B2 = 5 in SBS and number of simulations N = 40 in MCTS by default.", "description": "This table presents the main experimental results of the AlphaMath framework and compares its performance against various state-of-the-art methods on both in-domain and out-of-domain datasets.  It shows the model size, the type and amount of training data used (including whether human or GPT-4 annotation was used), tools employed, and performance metrics on different datasets.  The table highlights the performance of AlphaMath, especially its ability to achieve comparable or better results than existing models even without high-quality process-supervised data. It also includes the effect of different inference strategies (SBS, MCTS) and parameters.", "section": "4.2 Main Results"}, {"figure_path": "VaXnxQ3UKo/tables/tables_21_1.jpg", "caption": "Table 2: Main results. The best results of open-sourced models are bold. For the methods with released model's outputs, performance metrics using the evaluation toolkit [47] are also provided in brackets. #Seed data refers to high-quality annotated (question, solution) pairs, typically annotated by humans or GPT-4. $Unless otherwise specified, we set beam size B2 = 5 in SBS and number of simulations N = 40 in MCTS by default.", "description": "This table presents the main results of the experiments comparing AlphaMath with various state-of-the-art models on different datasets. It includes the model size, whether seed data was used, the annotation source, tool usage, performance on in-domain and out-of-domain datasets, and hyperparameters used in AlphaMath.  The table highlights the performance of AlphaMath, particularly its ability to achieve comparable or superior results even without high-quality annotated data.", "section": "4.2 Main Results"}, {"figure_path": "VaXnxQ3UKo/tables/tables_22_1.jpg", "caption": "Table 2: Main results. The best results of open-sourced models are bold. For the methods with released model's outputs, performance metrics using the evaluation toolkit [47] are also provided in brackets. #Seed data refers to high-quality annotated (question, solution) pairs, typically annotated by humans or GPT-4. $Unless otherwise specified, we set beam size B2 = 5 in SBS and number of simulations N = 40 in MCTS by default.", "description": "This table presents the main results of the AlphaMath model and compares its performance with various baselines on different mathematical reasoning datasets.  It includes both in-domain and out-of-domain results for proprietary and open-source LLMs, as well as LLMs fine-tuned with process supervision.  The table details the model size, the use of seed data (high-quality annotated data), the tools used, and the performance metrics (accuracy) on various datasets (GSM8K, MATH, GaoKao2023, and OCWCourses).  Different versions of the AlphaMath model are shown, highlighting the effect of using Monte Carlo Tree Search (MCTS) or the more efficient Step-level Beam Search (SBS) and the impact of the number of iterations in training.", "section": "4.2 Main Results"}]