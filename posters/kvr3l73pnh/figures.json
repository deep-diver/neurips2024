[{"figure_path": "kVr3L73pNH/figures/figures_1_1.jpg", "caption": "Figure 1: (a) Our algorithm: We propose a new data attribution method using machine unlearning. By modifying the pretrained model \u03b8 to unlearn the synthesized result z, the model also forgets the influential training images crucial for generating that specific result. (b) Evaluation: We validate our method through counterfactual evaluation, where we retrain the model without the top K influential images identified by our method. When these influential images are removed from the dataset, the model fails to generate the synthesized image.", "description": "This figure illustrates the proposed data attribution method and its evaluation.  Panel (a) shows the core of the algorithm:  a pre-trained model is used to generate a synthesized image. Then, an \"unlearning\" process modifies the model to increase the loss on this generated image.  The images from the training set that caused significant changes in loss after this unlearning process are identified as influential for generating that specific synthesized image.  Panel (b) demonstrates how the method is evaluated. The top K most influential images are removed from the training dataset and the model is retrained from scratch.  If the model can no longer generate the initial synthesized image, then the attribution method is considered successful.", "section": "1 Introduction"}, {"figure_path": "kVr3L73pNH/figures/figures_5_1.jpg", "caption": "Figure 2: Attribution results on MSCOCO models. We show generated samples used as a query on the left, with training images being identified by different methods on the right. Qualitatively, our method retrieves images with more similar visual attributes. Notably, our method better matches the poses of the buses (considering random flips during training) and the poses and enumeration of skiers.", "description": "This figure displays a qualitative comparison of data attribution results from different methods on MSCOCO models. For two generated images (a bus and skiers), the figure shows the training images identified as influential by four attribution methods: Ours, D-TRAK, DINO, and JourneyTRAK.  The comparison highlights that the proposed method ('Ours') tends to identify training images with more visually similar attributes compared to other methods, particularly regarding poses and object counts.", "section": "5 Experiments"}, {"figure_path": "kVr3L73pNH/figures/figures_7_1.jpg", "caption": "Figure 2: Attribution results on MSCOCO models. We show generated samples used as a query on the left, with training images being identified by different methods on the right. Qualitatively, our method retrieves images with more similar visual attributes. Notably, our method better matches the poses of the buses (considering random flips during training) and the poses and enumeration of skiers.", "description": "This figure shows a qualitative comparison of the data attribution results from different methods (Ours, D-TRAK, DINO, JourneyTRAK) on two examples of text-to-image generation using MSCOCO models.  For each example, a generated image is shown on the left.  On the right, the training images identified as most influential by each method are displayed.  The results visually demonstrate that the proposed method ('Ours') tends to select training images that are more visually similar to the generated image than the other compared methods, especially regarding object poses.", "section": "5 Experiments"}, {"figure_path": "kVr3L73pNH/figures/figures_8_1.jpg", "caption": "Figure 4: Spatially-localized attribution. Given a synthesized image (left), we crop regions containing specific objects using GroundingDINO [64]. We attribute each object separately by only running forgetting on the pixels within the cropped region. Our method can attribute different synthesized regions to different training images.", "description": "This figure demonstrates the spatially-localized attribution capability of the proposed method.  It shows a synthesized image containing a motorcycle and a stop sign. Using bounding boxes from GroundingDINO, the model separately attributes regions of the image to different training images. The stop sign is matched with training images of similar stop signs, and the motorcycle is matched with images of motorcycles. This highlights the method's ability to identify fine-grained influences from specific image regions rather than just assigning influence to the whole image.", "section": "5.2 Customized Model Benchmark"}, {"figure_path": "kVr3L73pNH/figures/figures_9_1.jpg", "caption": "Figure 5: Qualitative examples on the Customized Model benchmark. The red boxes indicate ground truth exemplar images used for customizing the model. Both our method and AbC baselines successfully identify the exemplar images on object-centric models (left), while our method outperforms the baselines with artistic style models (right).", "description": "This figure shows qualitative results on the Customized Model benchmark dataset.  The benchmark tests the ability of different attribution methods to identify the training images that most influenced the generation of images by customized models.  Each column shows results for a single customized model and synthesized image (at the top). The red boxes highlight the ground truth exemplar images used to customize that model.  The rows show the top images retrieved by different methods (ours, D-TRAK, DINO (AbC), CLIP (AbC)). Object-centric models are shown on the left, and artistic-style models are on the right. The figure visually demonstrates that the proposed method outperforms baselines, particularly in artistic-style model attribution, by more reliably identifying the ground truth exemplar images.", "section": "5.2 Customized Model Benchmark"}, {"figure_path": "kVr3L73pNH/figures/figures_9_2.jpg", "caption": "Figure 6: Customized Model benchmark [18]. We report Recall@10 (left) and mAP (right) and show performance on artist-style models (y-axis) vs. object-centric models (x-axis). On object-centric models, our method performs on par with AbC features, which were directly tuned on the benchmark, while significantly outperforming them on artist-style models. D-TRAK performs the second best on artist-style models but worse on object-centric models. We plot one standard error on both axes.", "description": "This figure presents the results of the proposed data attribution method on the Customized Model benchmark dataset [18].  The benchmark evaluates the ability of different methods to identify influential images in customized text-to-image models.  The plots show Recall@10 and mean Average Precision (mAP) for both object-centric and artist-style models. The results demonstrate that the proposed method performs comparably to existing state-of-the-art methods on object-centric models but significantly outperforms them on artist-style models. D-TRAK shows strong performance on artist-style models but weaker results on object-centric models.", "section": "5.2 Customized Model Benchmark"}, {"figure_path": "kVr3L73pNH/figures/figures_17_1.jpg", "caption": "Figure 7: Ablation for unlearning. We find that our unlearning method (Ours) outperforms other variants (Full Weight, SGD) in terms of forgetting the target (left) while retaining other concepts (right).", "description": "This figure shows an ablation study comparing three different unlearning methods.  The goal is to effectively remove a specific image (the \"target\") from the model's knowledge while preserving the ability to generate other images (\"other\").  The results show that the proposed method (\"Ours\") is significantly better at forgetting the target image while maintaining the ability to generate other images, as compared to two baseline methods (\"Full weight\" and \"SGD\").  The images in the figure demonstrate the visual impact of each method.", "section": "C Additional Analysis"}, {"figure_path": "kVr3L73pNH/figures/figures_18_1.jpg", "caption": "Figure 8: Does leave-K-out models forget other images? We show that leave-K-out model forgets the specific target (left), while retaining its generation on related images (middle) and images of other concepts (right).", "description": "This figure demonstrates the results of retraining a model after removing a set of influential images (leave-K-out). The left column shows the target image that the model should ideally forget. The middle column displays images related to the target, and the right column shows images of unrelated concepts. The figure aims to show that the leave-K-out model successfully forgets the target image while retaining the ability to generate images similar to those in the related images and unrelated concepts.", "section": "5.1 Leave-K-out counterfactual evaluation"}, {"figure_path": "kVr3L73pNH/figures/figures_19_1.jpg", "caption": "Figure 6: Customized Model Benchmark [18]. We report Recall@10 (left) and mAP (right) and show performance on artist-style models (y-axis) vs. object-centric models (x-axis). On object-centric models, our method performs on par with AbC features, which were directly tuned on the benchmark, while significantly outperforming them on artist-style models. D-TRAK performs the second best on artist-style models but worse on object-centric models. We plot one standard error on both axes.", "description": "This figure presents the results of the proposed data attribution method on the Customized Model Benchmark dataset. The benchmark evaluates the ability of attribution methods to identify the influential training images that were used to create customized text-to-image models. The results are shown for both artist-style models and object-centric models, demonstrating the superiority of the proposed method, especially for artist-style models, and highlighting its competitiveness with existing state-of-the-art techniques.", "section": "5.2 Customized Model Benchmark"}, {"figure_path": "kVr3L73pNH/figures/figures_21_1.jpg", "caption": "Figure 2: Attribution results on MSCOCO models. We show generated samples used as a query on the left, with training images being identified by different methods on the right. Qualitatively, our method retrieves images with more similar visual attributes. Notably, our method better matches the poses of the buses (considering random flips during training) and the poses and enumeration of skiers.", "description": "This figure shows a qualitative comparison of the attribution results of four different methods on MSCOCO models.  For four different generated images, the figure displays the top training images identified as influential by each method.  The goal is to demonstrate that the proposed \"Ours\" method outperforms the other three methods by better identifying training images that share similar visual attributes with the generated image, particularly considering subtle details like poses and object counts. The images illustrate that the 'Ours' method is able to capture more fine-grained details in the visual similarity, accurately identifying similar poses despite random flips in the training data.", "section": "5 Experiments"}, {"figure_path": "kVr3L73pNH/figures/figures_22_1.jpg", "caption": "Figure 11: Additional leave-K-out model results for MSCOCO models. This is an extension of Figure 3 in the main paper, showing the results from removing top-K influential images from different algorithms, retraining, and attempting to regenerate a synthesized sample. The influential images for these examples are shown in Figure 10. Our method consistently destroys the synthesized examples, verifying that our method is identifying the critical influential images.", "description": "This figure shows additional results from leave-K-out experiments on MSCOCO models.  Four different attribution methods (Ours, D-TRAK, DINO, JourneyTRAK) are compared, showing the generated images after removing the top K influential images identified by each method. The goal is to verify that removing the influential images identified by the algorithm prevents the model from regenerating the original synthesized image. The results demonstrate that the proposed method ('Ours') consistently performs better than other methods in destroying the synthesized images when removing the top K influential images.", "section": "5 Experiments"}, {"figure_path": "kVr3L73pNH/figures/figures_23_1.jpg", "caption": "Figure 2: Attribution results on MSCOCO models. We show generated samples used as a query on the left, with training images being identified by different methods on the right. Qualitatively, our method retrieves images with more similar visual attributes. Notably, our method better matches the poses of the buses (considering random flips during training) and the poses and enumeration of skiers.", "description": "This figure shows a qualitative comparison of data attribution methods on MSCOCO datasets.  The left column displays generated images (queries).  The remaining columns present the top training images identified as influential by four different methods: JourneyTRAK, DINO, D-TRAK, and the proposed \"Ours\" method. The authors highlight that their method better identifies images with similar visual attributes and poses compared to other methods, particularly noticeable in examples involving buses (where random flips during training are considered) and skiers.", "section": "5 Experiments"}, {"figure_path": "kVr3L73pNH/figures/figures_23_2.jpg", "caption": "Figure 2: Attribution results on MSCOCO models. We show generated samples used as a query on the left, with training images being identified by different methods on the right. Qualitatively, our method retrieves images with more similar visual attributes. Notably, our method better matches the poses of the buses (considering random flips during training) and the poses and enumeration of skiers.", "description": "This figure shows a qualitative comparison of data attribution results on MSCOCO models for four different methods: JourneyTRAK, DINO, D-TRAK, and the authors' proposed method.  For each of two example generated images, the figure displays the top training images identified as influential by each method. The goal is to show that the authors' method better identifies training images with similar visual attributes to the generated image, handling aspects like pose and object counts more effectively.", "section": "5 Experiments"}, {"figure_path": "kVr3L73pNH/figures/figures_24_1.jpg", "caption": "Figure 1: (a) Our algorithm: We propose a new data attribution method using machine unlearning. By modifying the pretrained model \u03b8 to unlearn the synthesized result z, the model also forgets the influential training images crucial for generating that specific result. (b) Evaluation: We validate our method through counterfactual evaluation, where we retrain the model without the top K influential images identified by our method. When these influential images are removed from the dataset, the model fails to generate the synthesized image.", "description": "This figure illustrates the proposed data attribution method and its evaluation process. (a) shows the unlearning approach: A pretrained model is fine-tuned to 'forget' a synthesized image (z). This process also causes the model to forget training images that were highly influential in generating that image.  (b) demonstrates the counterfactual evaluation: By retraining a model without the most influential images identified in step (a), they show that the model can no longer generate the original synthesized image, proving the effectiveness of their method. ", "section": "1 Introduction"}, {"figure_path": "kVr3L73pNH/figures/figures_24_2.jpg", "caption": "Figure 1: (a) Our algorithm: We propose a new data attribution method using machine unlearning. By modifying the pretrained model \u03b8 to unlearn the synthesized result z, the model also forgets the influential training images crucial for generating that specific result. (b) Evaluation: We validate our method through counterfactual evaluation, where we retrain the model without the top K influential images identified by our method. When these influential images are removed from the dataset, the model fails to generate the synthesized image.", "description": "This figure illustrates the proposed data attribution method and its evaluation.  Part (a) shows the core algorithm which uses machine unlearning to identify influential training images by 'forgetting' a synthesized image (z). By increasing the training loss on the output image (z) without catastrophic forgetting of other concepts, the method identifies training images causing significant loss deviations. Part (b) depicts the counterfactual evaluation: retraining the model without the top-K influential images and observing the model's inability to reproduce the synthesized image. This confirms that the identified images are indeed influential for generating the specific output.", "section": "1 Introduction"}, {"figure_path": "kVr3L73pNH/figures/figures_25_1.jpg", "caption": "Figure 2: Attribution results on MSCOCO models. We show generated samples used as a query on the left, with training images being identified by different methods on the right. Qualitatively, our method retrieves images with more similar visual attributes. Notably, our method better matches the poses of the buses (considering random flips during training) and the poses and enumeration of skiers.", "description": "This figure displays a qualitative comparison of data attribution methods on MSCOCO models.  Generated images are shown alongside their attributed training images, as determined by different methods.  The authors highlight that their method is superior in identifying training images with more similar visual attributes, particularly with respect to object poses and counts.", "section": "5 Experiments"}, {"figure_path": "kVr3L73pNH/figures/figures_25_2.jpg", "caption": "Figure 9: Ablation studies for Customized Model Benchmark. We report evaluation on the Customized Model Benchmark in the same fashion as in Figure 6. We find that training with multiple steps, updating a selected subset of weights, and regularizing unlearning via Fisher information is crucial to this task. Additionally, we test a version where we apply our algorithm without the special token v*. While it reduces performance, it still performs well in overall.", "description": "This figure shows the ablation study results on the customized model benchmark. The authors evaluated different unlearning configurations such as the number of steps, the choice of weight subset (full weights, attention layers, cross attention layers, cross attention key and value matrices), and the inclusion of Fisher information in the objective function. They observed that employing multiple steps, optimizing a subset of weights (specifically cross-attention KV), and regularizing with Fisher information yielded the best performance. A comparison with an experiment conducted without the special token v* is also included. The results obtained demonstrate the importance of these design choices for effective attribution in customized models.", "section": "5.2 Customized Model Benchmark"}]