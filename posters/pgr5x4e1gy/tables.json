[{"figure_path": "pGR5X4e1gy/tables/tables_8_1.jpg", "caption": "Table 1: Results on dense temporal graphs. Top three models are colored by First, Second, Third.", "description": "This table presents the results of experiments conducted on dense temporal graphs using various models, including the proposed ICG-NN and ICGu-NN models.  The performance of these models is compared against other state-of-the-art models such as DCRNN, GraphWaveNet, and AGCRN.  The table highlights the MAE (Mean Absolute Error) achieved by each model on two different datasets, METR-LA and PEMS-BAY.  The 'relative Frob.' column likely indicates a measure of the approximation error of the graph representation used by the model.", "section": "6.3 Spatio-temporal graphs"}, {"figure_path": "pGR5X4e1gy/tables/tables_26_1.jpg", "caption": "Table 2: Results on large graphs. Top three models are colored by First, Second, Third.", "description": "This table presents the results of node classification experiments on three large, non-sparse graph datasets: tolokers, squirrel, and twitch-gamers.  The table shows the mean ROC AUC (for tolokers) or accuracy and standard deviation for several graph neural network models, including various baselines and the proposed ICG-NN and ICGu-NN methods. The \"relative Frob.\" column indicates the relative Frobenius error of the ICG approximation for each dataset.  The results demonstrate the competitive performance of the ICG-NN models, particularly in the tolokers and twitch-gamers datasets.", "section": "6.2 Node classification using Subgraph SGD"}, {"figure_path": "pGR5X4e1gy/tables/tables_26_2.jpg", "caption": "Table 3: Comparison with graph coarsening methods over large graphs. Top three models are colored by First, Second, Third.", "description": "This table compares the performance of ICG-NN and ICGu-NN against various graph coarsening methods on two large graph datasets, Reddit and Flickr.  The table shows the accuracy of node classification for each method, highlighting the superior performance of the ICG-NN and ICGu-NN models.", "section": "6.2 Comparison to graph coarsening methods (Appendix F.2)"}, {"figure_path": "pGR5X4e1gy/tables/tables_27_1.jpg", "caption": "Table 4: Results on Flickr using 1% node sampling. Top three models are colored by First, Second, Third.", "description": "This table presents the results of node classification experiments on the Flickr dataset using 1% node sampling.  It compares the accuracy of various methods, including ICG-NN and ICGu-NN, against several graph coarsening methods (Coarsening, Random, Herding, K-Center, One-Step) and graph condensation methods (DC-Graph, GCOND, SFGC, GC-SNTK). The top three performing models are highlighted.", "section": "F.3 Node classification on large graphs using Subgraph SGD"}, {"figure_path": "pGR5X4e1gy/tables/tables_28_1.jpg", "caption": "Table 5: Time until convergence in seconds on large graphs.", "description": "The table compares the training time until convergence for different initialization methods (random and eigenvector) on three large graph datasets: tolokers, squirrel, and twitch-gamers.  The results show the average time and standard deviation for each dataset and initialization method, highlighting the potential efficiency gains from using eigenvector initialization. The average degree of each dataset is also provided as context for the comparison.", "section": "F.6 Initialization"}, {"figure_path": "pGR5X4e1gy/tables/tables_30_1.jpg", "caption": "Table 6: Time until convergence in seconds on large graphs.", "description": "This table compares the training time until convergence (in seconds) for GCN and ICGu-NN on three large graph datasets: tolokers, squirrel, and twitch-gamers.  It highlights the significant speedup achieved by ICGu-NN compared to GCN, demonstrating the efficiency gains of the proposed method.", "section": "F Additional experiments"}, {"figure_path": "pGR5X4e1gy/tables/tables_31_1.jpg", "caption": "Table 7: Statistics of the node classification benchmarks.", "description": "This table presents the statistics of three real-world node classification datasets used in the paper's experiments. For each dataset, it shows the number of nodes, the number of edges, the average node degree, the number of node features, the number of classes, and the evaluation metrics used (AUC-ROC for tolokers and ACC for squirrel and twitch-gamers).  The datasets vary significantly in size and characteristics.", "section": "F.1 Node classification"}, {"figure_path": "pGR5X4e1gy/tables/tables_31_2.jpg", "caption": "Table 8: Statistics of the spatio-temporal benchmarks.", "description": "This table presents the number of nodes, edges, average node degree, and number of node features for the METR-LA and PEMS-BAY datasets, which are used in the spatio-temporal experiments of the paper.", "section": "6.3 Spatio-temporal graphs"}, {"figure_path": "pGR5X4e1gy/tables/tables_31_3.jpg", "caption": "Table 3: Comparison with graph coarsening methods over large graphs. Top three models are colored by First, Second, Third.", "description": "This table compares the performance of ICG-NN and ICGu-NN against various graph coarsening methods on two large graph datasets: Reddit and Flickr.  The metrics used are accuracy and standard deviation. The table highlights the superior performance of the ICG-based methods compared to existing coarsening techniques.", "section": "F.2 Comparison with graph coarsening methods over large graphs"}, {"figure_path": "pGR5X4e1gy/tables/tables_32_1.jpg", "caption": "Table 2: Results on large graphs. Top three models are colored by First, Second, Third.", "description": "This table presents the results of node classification experiments conducted on three large, non-sparse graph datasets: tolokers, squirrel, and twitch-gamers.  The table shows the number of nodes and edges in each dataset, the average node degree, and the relative Frobenius error for the ICG approximation.  For each dataset, the performance of several node classification methods is reported, including MLP, GCN, GAT, H2GCN, GPR-GNN, LINKX, GloGNN, ICG-NN, and ICGu-NN.  The top three performing models for each dataset are highlighted in color.", "section": "F.1 Node classification"}, {"figure_path": "pGR5X4e1gy/tables/tables_32_2.jpg", "caption": "Table 11: Hyper-parameters used for the spatio-temporal benchmarks.", "description": "This table shows the hyperparameters used for the spatio-temporal benchmarks METR-LA and PEMS-BAY.  The hyperparameters include the number of communities, encoded dimension, lambda (\u03bb), approximation learning rate (Approx. lr), approximation epochs (Approx. epochs), number of layers, hidden dimension, fit learning rate (Fit lr), and fit epochs.", "section": "6 Experiments"}, {"figure_path": "pGR5X4e1gy/tables/tables_32_3.jpg", "caption": "Table 3: Comparison with graph coarsening methods over large graphs. Top three models are colored by First, Second, Third.", "description": "This table compares the performance of ICG-NN and ICGu-NN against various graph coarsening methods on the Reddit and Flickr datasets.  It shows the number of nodes, edges, and average node degree for each dataset and lists the accuracy achieved by different methods.", "section": "F.2 Comparison with graph coarsening methods over large graphs"}]