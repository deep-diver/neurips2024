[{"figure_path": "aYqTwcDlCG/figures/figures_1_1.jpg", "caption": "Figure 1: The general framework of model-based RL.", "description": "This figure illustrates the general framework of model-based reinforcement learning.  Data from interactions with a real environment are collected and stored in a replay buffer. This replay buffer contains state-action-state triplets from the agent's experiences. This data is used to train a world model which captures the environment's dynamics. The world model is then used to train an actor and critic, which together form a goal-conditioned policy. This policy allows the agent to plan actions and learn effectively, especially in sparse reward scenarios.", "section": "1 Introduction"}, {"figure_path": "aYqTwcDlCG/figures/figures_2_1.jpg", "caption": "Figure 2: In Fig. 2(a), we illustrate the key states involved in completing the task of 3-block stacking. In Fig. 2(b), we demonstrate the significant advantages of the bidirectional replay buffer used in MUN over traditional methods in learning world models.", "description": "Figure 2(a) shows key states in a 3-block stacking task, highlighting the milestones in the process.  Figure 2(b) compares the MUN's bidirectional replay buffer with traditional unidirectional methods, emphasizing MUN's ability to learn from both forward and backward transitions, leading to more robust world models.", "section": "3 Training World Models for Unconstrained Goal Navigation"}, {"figure_path": "aYqTwcDlCG/figures/figures_2_2.jpg", "caption": "Figure 2: In Fig. 2(a), we illustrate the key states involved in completing the task of 3-block stacking. In Fig. 2(b), we demonstrate the significant advantages of the bidirectional replay buffer used in MUN over traditional methods in learning world models.", "description": "This figure compares the traditional unidirectional replay buffer with the bidirectional replay buffer proposed by the authors.  The unidirectional buffer only stores transitions from one state to the next, along the trajectory.  This limits the world model's ability to generalize to unseen states or state transitions in reverse. In contrast, the bidirectional buffer includes transitions between any two states (subgoals) found within the buffer, even across different trajectories.  This enables the MUN algorithm to learn a world model that generalizes better to new scenarios and is more robust.", "section": "3 Training World Models for Unconstrained Goal Navigation"}, {"figure_path": "aYqTwcDlCG/figures/figures_5_1.jpg", "caption": "Figure 3: We evaluate MUN on 6 environments: Ant Maze, Walker, 3-Block Stacking, Block Rotation, Pen Rotation, Fetch Slide.", "description": "This figure shows six different simulated robotic environments used to evaluate the MUN algorithm.  These environments present diverse challenges in terms of locomotion, manipulation, and control precision, offering a robust testbed for assessing the generalization capabilities of goal-conditioned policies learned by the MUN method. The environments include: Ant Maze (navigation), Walker (locomotion), 3-Block Stacking (manipulation), Pen Rotation (precise manipulation), Block Rotation (manipulation), and Fetch Slide (pushing).", "section": "4 Experiments"}, {"figure_path": "aYqTwcDlCG/figures/figures_6_1.jpg", "caption": "Figure 3: We evaluate MUN on 6 environments: Ant Maze, Walker, 3-Block Stacking, Block Rotation, Pen Rotation, Fetch Slide.", "description": "This figure shows the six different robotics and navigation environments used to evaluate the MUN algorithm and compare its performance against baseline algorithms.  Each environment presents unique challenges in terms of the agent's locomotion, manipulation skills, and the complexity of the tasks. The environments range from navigating a maze (Ant Maze) to stacking blocks (3-Block Stacking) and performing precise manipulations (Pen Rotation and Block Rotation).  The diversity of these tasks allows for a comprehensive assessment of MUN's generalization capabilities across various scenarios.", "section": "4 Experiments"}, {"figure_path": "aYqTwcDlCG/figures/figures_6_2.jpg", "caption": "Figure 4: Experiment results comparing MUN with the baselines over 5 random seeds.", "description": "This figure presents the experimental results comparing the performance of the proposed MUN algorithm against several baseline methods across six different robotic manipulation and navigation tasks.  The results are shown as graphs depicting the success rate over training steps (x-axis) for each method. Each graph represents a specific task (Ant Maze, Walker, 3-Block Stacking, Block Rotation, Pen Rotation, Fetch Slide). The shaded area represents the standard deviation over 5 random seeds.  The results demonstrate MUN's superior performance across all six tasks in terms of both final success rate and learning speed.", "section": "4 Experiments"}, {"figure_path": "aYqTwcDlCG/figures/figures_7_1.jpg", "caption": "Figure 2: In Fig. 2(a), we illustrate the key states involved in completing the task of 3-block stacking. In Fig. 2(b), we demonstrate the significant advantages of the bidirectional replay buffer used in MUN over traditional methods in learning world models.", "description": "Figure 2(a) shows key states in a 3-block stacking task, highlighting the importance of subgoal identification in MUN. Figure 2(b) compares the MUN's bidirectional replay buffer with traditional unidirectional buffers, illustrating MUN's improved ability to capture state transitions in various directions for enhanced world model learning. The bidirectional buffer allows for learning state transitions from various perspectives, as demonstrated in the figure.", "section": "3 Training World Models for Unconstrained Goal Navigation"}, {"figure_path": "aYqTwcDlCG/figures/figures_7_2.jpg", "caption": "Figure 7: Experiment setup and results of navigation between any pair of subgoals in the 3-Block Stacking environment. In the left part, the bottom section of each image depicts the ultimate evaluation goal for one evaluation episode, while the top section illustrates the manually set initial state. The right part shows the evaluation success rates.", "description": "This figure shows an experiment to test the model's ability to navigate between arbitrary subgoals in a 3-block stacking task. The left side shows the initial state (top) and target goal state (bottom) for each trial.  The right side displays a bar graph comparing the success rate of MUN against several baseline methods in this navigation task.  The high success rate of MUN in this challenging navigation task highlights its ability to generalize the learned world model to novel goal settings.", "section": "4.5 Can MUN navigate between arbitrary subgoals?"}, {"figure_path": "aYqTwcDlCG/figures/figures_8_1.jpg", "caption": "Figure 3: We evaluate MUN on 6 environments: Ant Maze, Walker, 3-Block Stacking, Block Rotation, Pen Rotation, Fetch Slide.", "description": "This figure shows six different simulated robotic environments used to evaluate the performance of the MUN algorithm.  The environments vary in complexity and the type of robot control involved: \n\n* **Ant Maze:** A multi-legged ant robot navigating a maze.\n* **Walker:** A two-legged robot learning to walk.\n* **3-Block Stacking:** A robotic arm stacking three blocks.\n* **Block Rotation:** A robotic arm rotating a block to a specific orientation.\n* **Pen Rotation:** A robotic arm rotating a pen to a specific orientation (more difficult due to the pen's shape).\n* **Fetch Slide:** A robotic arm sliding a puck to a target location on a slippery surface.\n\nThe figure provides visual examples of the robots and their tasks in each environment.", "section": "4 Experiments"}, {"figure_path": "aYqTwcDlCG/figures/figures_20_1.jpg", "caption": "Figure 3: We evaluate MUN on 6 environments: Ant Maze, Walker, 3-Block Stacking, Block Rotation, Pen Rotation, Fetch Slide.", "description": "This figure shows six different simulated robotic environments used to test the performance of the proposed MUN algorithm.  These environments present diverse challenges, including navigation (Ant Maze), locomotion (Walker), manipulation (3-Block Stacking, Block Rotation, Pen Rotation), and pushing (Fetch Slide).  The image provides a visual representation of the complexity and variety of the tasks used in the experiments.", "section": "4 Experiments"}, {"figure_path": "aYqTwcDlCG/figures/figures_22_1.jpg", "caption": "Figure 10: Fig(a) and Fig(b) illustrate the imagined and real environment trajectories for 3-Block Stacking and Block Rotation respectively, starting from the same initial state. Among the baselines, MUN demonstrates the smallest compound model error with respect to the ground truth trajectories. The X-axis represents the trajectory steps. In Fig(a), the Y-axis represents the sum of the heights of the three blocks. MUN's world model outperforms other methods in predicting the correct locations of the three blocks. In Fig(b), the Y-axis represents the position of the block in the x coordinate. MUN's world model outperforms other methods in predicting the correct position of the block.", "description": "This figure compares the imagined and real trajectories generated by different world models for two tasks: 3-block stacking and block rotation.  The plots show the sum of block heights and the x-position of the block, respectively, over time steps.  MUN exhibits the smallest error compared to the ground truth trajectory, demonstrating the superior accuracy of its world model in predicting real-world dynamics.", "section": "F.3 World Model Assessment"}, {"figure_path": "aYqTwcDlCG/figures/figures_22_2.jpg", "caption": "Figure 10: Fig(a) and Fig(b) illustrate the imagined and real environment trajectories for 3-Block Stacking and Block Rotation respectively, starting from the same initial state. Among the baselines, MUN demonstrates the smallest compound model error with respect to the ground truth trajectories. The X-axis represents the trajectory steps. In Fig(a), the Y-axis represents the sum of the heights of the three blocks. MUN's world model outperforms other methods in predicting the correct locations of the three blocks. In Fig(b), the Y-axis represents the position of the block in the x coordinate. MUN's world model outperforms other methods in predicting the correct position of the block.", "description": "This figure compares the imagined trajectories generated by different world models (MUN, PEG-G, MEGA-G, GC-Dreamer) against the ground truth trajectories in 3-Block Stacking and Block Rotation tasks.  The plots show the prediction error over time steps. MUN consistently shows the lowest error, indicating better model accuracy and generalization.", "section": "F.3 World Model Assessment"}]