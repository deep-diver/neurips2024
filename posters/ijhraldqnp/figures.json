[{"figure_path": "IjHrALdQNP/figures/figures_1_1.jpg", "caption": "Figure 1: The leftmost RGB image shows the same observation for both methods. Our method (top row) effectively identifies the geometric part of the chair back, which is missed by the traditional method (bottom row). Consequently, GAMap successfully guides the agent to the target object, while the traditional method fails. The red circles highlight the areas where the chair is located, and the GA score is high, indicating the effectiveness of our approach in localizing relevant regions.", "description": "This figure compares the performance of the proposed GAMap method and a traditional method in a zero-shot object goal navigation task.  The leftmost image shows the same visual input to both methods. The top row illustrates the GAMap's ability to identify the chair back (a geometric part) and use this information for successful navigation. The bottom row shows the traditional method failing to detect this crucial part and consequently failing to reach the target. Red circles highlight the chair's location, with high GA scores (Geometric-Affordance scores) in the GAMap indicating successful localization.", "section": "1 Introduction"}, {"figure_path": "IjHrALdQNP/figures/figures_3_1.jpg", "caption": "Figure 2: Pipeline of the GAMap generation. Geometric parts and affordance attributes are generated by an LLM. The RGB observation is partitioned into multiple scales, and a CLIP visual encoder generates multi-scale visual embeddings. GA scores are computed using cosine similarity between attribute text embeddings from a CLIP text encoder and the multi-scale visual embeddings. These scores are averaged and projected onto a 2D grid to form the GAMap.", "description": "This figure illustrates the process of generating the Geometric-Affordance Map (GAMap).  It starts with an LLM (Large Language Model) generating geometric parts and affordance attributes for a target object (e.g., a chair). The RGB-D observation from the robot's camera is partitioned into multiple scales.  A CLIP (Contrastive Language\u2013Image Pre-training) visual encoder processes these to produce multi-scale visual embeddings.  These embeddings are compared (cosine similarity) with attribute text embeddings (also from CLIP) to calculate GA (Geometric-Affordance) scores. These scores are averaged across scales and projected onto a 2D grid to create the GAMap, which guides the robot's navigation.", "section": "3 Method"}, {"figure_path": "IjHrALdQNP/figures/figures_6_1.jpg", "caption": "Figure 3: Heatmap showing the increase and decrease in the percentage of SR and time cost for varying the numbers of Na and Ng. Darker colors indicate a greater decrease in SR, and red solid and dashed lines represent the associated time cost.", "description": "This heatmap shows the results of ablation studies varying the number of affordance attributes (Na) and geometric attributes (Ng).  The color intensity represents the change in success rate (SR), with darker colors showing larger decreases.  Red lines show the time cost increase for each combination of Na and Ng. The study shows a trade-off between improved performance with more attributes and increased computational cost.", "section": "5.1 Effectiveness of Affordance and Geometric-part Attributes"}, {"figure_path": "IjHrALdQNP/figures/figures_7_1.jpg", "caption": "Figure 2: Pipeline of the GAMap generation. Geometric parts and affordance attributes are generated by an LLM. The RGB observation is partitioned into multiple scales, and a CLIP visual encoder generates multi-scale visual embeddings. GA scores are computed using cosine similarity between attribute text embeddings from a CLIP text encoder and the multi-scale visual embeddings. These scores are averaged and projected onto a 2D grid to form the GAMap.", "description": "This figure illustrates the process of generating the Geometric-Affordance Map (GAMap). It starts with an LLM generating geometric parts and affordance attributes for the target object. Then, the RGB observation is partitioned into multiple scales. A CLIP visual encoder generates multi-scale visual embeddings for these partitions. These embeddings are compared with the attribute text embeddings (from CLIP's text encoder) using cosine similarity to obtain GA scores. Finally, these scores are averaged and projected onto a 2D grid to create the GAMap.", "section": "3 Method"}, {"figure_path": "IjHrALdQNP/figures/figures_8_1.jpg", "caption": "Figure 5: Comparison of GA score visualization between gradient-based and patch-based methods for the armrest, backrest, and seat attributes of a target chair. The gradient-based method (top row) often attends to irrelevant areas, such as the ceiling, while the patch-based method (bottom row) accurately focuses on the relevant areas.", "description": "This figure compares the performance of two methods for calculating Geometric-Affordance (GA) scores: gradient-based and patch-based.  The top row shows the gradient-based method's heatmaps for armrest, backrest, and seat attributes of a chair, highlighting how it incorrectly focuses on irrelevant areas like the ceiling. The bottom row displays the patch-based method's heatmaps, demonstrating its superior ability to accurately focus on the relevant chair parts.", "section": "5.3 Effectiveness of Different Methods for Calculating GA Scores"}, {"figure_path": "IjHrALdQNP/figures/figures_9_1.jpg", "caption": "Figure 6: The top row of images shows our proposed method, where the multi-scale approach effectively captures objects at all scales, such as the sofa back in the background. The bottom row of images shows the results of GPT-4V.", "description": "This figure compares the performance of the proposed multi-scale approach with GPT-4V in identifying a sofa in a scene. The top row shows how the multi-scale approach successfully identifies the sofa at different scales (a close-up view of the sofa, a wider view including the sofa and surrounding furniture, and a long shot of the whole room), while GPT-4V fails to detect the sofa, even though the sofa is quite visible in the images.", "section": "5.6 Effectiveness of Multi-scale Approach"}, {"figure_path": "IjHrALdQNP/figures/figures_14_1.jpg", "caption": "Figure 2: Pipeline of the GAMap generation. Geometric parts and affordance attributes are generated by an LLM. The RGB observation is partitioned into multiple scales, and a CLIP visual encoder generates multi-scale visual embeddings. GA scores are computed using cosine similarity between attribute text embeddings from a CLIP text encoder and the multi-scale visual embeddings. These scores are averaged and projected onto a 2D grid to form the GAMap.", "description": "This figure illustrates the process of generating the Geometric-Affordance Map (GAMap). It starts with an LLM generating geometric parts and affordance attributes for a target object.  The RGB observation is then divided into multiple scales, and a CLIP visual encoder creates visual embeddings for each scale. These embeddings are compared to the attribute text embeddings (also from CLIP) to produce GA scores.  Finally, these scores are averaged and mapped onto a 2D grid to create the GAMap, which guides the navigation.", "section": "3 Method"}, {"figure_path": "IjHrALdQNP/figures/figures_15_1.jpg", "caption": "Figure 2: Pipeline of the GAMap generation. Geometric parts and affordance attributes are generated by an LLM. The RGB observation is partitioned into multiple scales, and a CLIP visual encoder generates multi-scale visual embeddings. GA scores are computed using cosine similarity between attribute text embeddings from a CLIP text encoder and the multi-scale visual embeddings. These scores are averaged and projected onto a 2D grid to form the GAMap.", "description": "This figure illustrates the process of generating the Geometric-Affordance Map (GAMap).  It starts with an LLM generating geometric parts and affordance attributes for a target object.  The RGB observation from the robot is then divided into multiple scales, and CLIP (Contrastive Language\u2013Image Pre-training) is used to create visual embeddings for each scale.  These embeddings are compared to text embeddings of the attributes, resulting in GA scores.  These scores are averaged and projected onto a 2D grid to create the GAMap, which guides the robot's navigation.", "section": "3 Method"}]