[{"figure_path": "HCTikT7LS4/tables/tables_2_1.jpg", "caption": "Table 1: Stability of a dynamical system for different values of the Maximal Lyapunov Exponent (\u03bb\u2081) and Sum of Lyapunov Exponents (\u03bb\u03b5).", "description": "This table shows the stability of a dynamical system based on the signs of the maximal Lyapunov exponent (\u03bb\u2081) and the sum of Lyapunov exponents (\u03bb\u03b5).  A positive \u03bb\u2081 indicates sensitive dependence on initial conditions, while a negative \u03bb\u2081 suggests stability.  The sign of \u03bb\u03b5 further refines the classification: a positive \u03bb\u03b5 indicates exponential divergence, while a negative \u03bb\u03b5 implies convergence to an attractor (even if chaotic).", "section": "2.2 Measuring stability: Lyapunov Exponents"}, {"figure_path": "HCTikT7LS4/tables/tables_8_1.jpg", "caption": "Table 2: Average total reward and average MLE produced when Dreamer V3 (DR3) and Dreamer V3 with MLE regularisation (MLE DR3) when controlling various environments sampled from the DeepMind Control Suite [27]. Each policy-environment combination is independently trained with three random seeds using the hyperparameters outlined in Appendix A.2.", "description": "This table presents the average total reward and Maximal Lyapunov Exponent (MLE) achieved by two different models, Dreamer V3 (DR3) and a modified version with MLE regularization (MLE DR3), across seven different control environments from the DeepMind Control Suite.  Each environment was tested with three separate random seeds to ensure robustness of the results, and hyperparameters used were consistent across all tests (Appendix A.2). The data highlights the impact of MLE regularization on both reward performance and system stability, indicated by the MLE values. A lower MLE value implies greater stability.", "section": "6 Experiments"}, {"figure_path": "HCTikT7LS4/tables/tables_12_1.jpg", "caption": "Table 3: State space definition for each environment used in this paper. Each dimension of the state space represents a unique aspect of the control system and can contain any real value. Positions are measured in metres (m), angles are measured in radians (rad), velocities are measured in metres per second (m/s) and angular velocities are measured in radians per second (rad/s).", "description": "This table details the state space composition for each of the seven DeepMind control suite environments used in the paper.  For each environment, it lists the number of degrees of freedom and specifies the physical quantities represented by each dimension of the state space (position, angle, velocity, angular velocity), including their units of measurement.", "section": "A.1 State space composition"}, {"figure_path": "HCTikT7LS4/tables/tables_13_1.jpg", "caption": "Table 4: Hyperparameters used to train SAC, TD3 and Dreamer V3", "description": "This table presents the hyperparameters used for training three different reinforcement learning algorithms: Soft Actor-Critic (SAC), Twin Delayed Deep Deterministic policy gradients (TD3), and Dreamer V3. For each algorithm, the table lists the values used for various parameters, including environment steps, buffer size, parallel environments, update period, updates per step, discount factor, learning rate, batch size, Polyak update coefficient, network activation function, network depth, and network width.", "section": "A.2 Hyperparameters"}, {"figure_path": "HCTikT7LS4/tables/tables_13_2.jpg", "caption": "Table 4: Hyperparameters used to train SAC, TD3 and Dreamer V3", "description": "This table lists the hyperparameters used for training three different reinforcement learning algorithms: Soft Actor-Critic (SAC), Twin Delayed Deep Deterministic policy gradients (TD3), and Dreamer V3.  For each algorithm, it shows the values used for parameters such as environment steps, buffer size, parallel environments, update period, updates per step, discount factor, learning rate, batch size, polyak update coefficient (for SAC and TD3), network activation function, network depth, and network width.  Dreamer V3 also includes parameters specific to its model-based approach such as RSSM batch length and imagination horizon. These hyperparameters were used in the experiments described in the paper to train the RL agents.", "section": "A.2 Hyperparameters"}, {"figure_path": "HCTikT7LS4/tables/tables_13_3.jpg", "caption": "Table 5: Parameters used to calculate the spectrum of Lyapunov Exponents using the method outlined by Benettin et al. [2, 3].", "description": "This table shows the hyperparameters used to compute the Lyapunov exponents, which are key to characterizing the stability and chaos in the dynamical systems studied in the paper.  These parameters control aspects of the numerical calculation process, such as the length of trajectories, the frequency of orthonormalization, and the magnitude of initial state perturbations.", "section": "B Lyapunov Exponent ablation study"}]