[{"figure_path": "HCTikT7LS4/figures/figures_1_1.jpg", "caption": "Figure 1: Reward attained when a trained deterministic Soft Actor-Critic [8] agent controls the deterministic Walker Walk environment. Each system has the same initial configuration other than the torso angle, which is perturbed by \u00b15 \u00d7 10-4 degrees. This small perturbation causes the systems to significantly diverge after 50 steps due to the chaotic nature of the control interaction. Consequently, this affects overall performance as there is significant variation in the total reward attained.", "description": "This figure shows the impact of a small perturbation in the initial state of a deterministic Walker Walk environment controlled by a trained Soft Actor-Critic agent.  Three nearly identical simulations are run, differing only by a tiny change ( \u00b15 \u00d7 10-4 degrees) in the initial torso angle.  Despite this minuscule difference, the reward and state trajectories diverge significantly after only 50 steps, illustrating the chaotic sensitivity of the system to initial conditions. This highlights a key challenge in applying deep reinforcement learning to real-world scenarios, where sensor noise or other small disturbances can lead to drastically different outcomes.", "section": "1 Introduction"}, {"figure_path": "HCTikT7LS4/figures/figures_3_1.jpg", "caption": "Figure 2: Total episode reward for the Pointmass Easy (PM), Cartpole Balance (CB), Cartpole Swingup (CS), Walker Stand (WS), Walker Walk (WW), Walker Run (WR) and Cheetah Run (CR) environments when controlled by trained instances of SAC, TD3, Dreamer V3 (DR3) and an agent which takes no actions (None). Each policy-environment combination is independently trained with three random seeds and the average interquartile episode reward with a bootstrapped 95% confidence interval is reported over 80 evaluation episodes each with a fixed length of 1000 steps.", "description": "This figure shows the total episode reward achieved by different reinforcement learning agents (SAC, TD3, DreamerV3, and a no-action baseline) across seven different DeepMind Control Suite environments.  Each environment and agent combination was trained three times with different random seeds. The plot displays the average interquartile range of the total reward, along with a bootstrapped 95% confidence interval, based on 80 evaluation episodes per combination, each lasting 1000 steps.  The results show the relative performance of the different algorithms on these diverse tasks.", "section": "2.3 Chaos in reinforcement learning"}, {"figure_path": "HCTikT7LS4/figures/figures_4_1.jpg", "caption": "Figure 3: Estimated Maximal Lyapunov Exponent (MLE) and Sum of Lyapunov Exponents (SLE) for the Pointmass (PM), Cartpole Balance (CB), Cartpole Swingup (CS), Walker Stand (WS), Walker Walk (WW), Walker Run (WR) and Cheetah Run (CR) environments when controlled by a trained instance of SAC, TD3, Dreamer V3 (DR3) and an agent which takes no actions (None). Each policy-environment combination is independently trained with three random seeds and the interquartile average MLE & SLE for each seed is calculated using 20 initial states. A bootstrapped 95% confidence interval is included to show the variation in MLE and SLE across random seeds.", "description": "This figure displays the estimated Maximal Lyapunov Exponent (MLE) and Sum of Lyapunov Exponents (SLE) for seven DeepMind control suite environments controlled by four different reinforcement learning agents: SAC, TD3, DreamerV3, and a \"None\" agent (taking no actions).  The box plots show the interquartile range of MLE and SLE values across three independent training runs for each agent-environment combination, with the whiskers extending to the 95% confidence intervals.  The results provide insights into the stability of different reinforcement learning agents across various environments, indicating which ones are more susceptible to chaotic dynamics.", "section": "3 Chaotic state dynamics"}, {"figure_path": "HCTikT7LS4/figures/figures_5_1.jpg", "caption": "Figure 4: Partial state trajectory produced by Dreamer V3 when controlling Cartpole Balance and Walker Walk subject to a single initial state perturbation. Initially, each system is separated by only 10\u207b\u2074 units but the subsequent state trajectories diverge significantly as the control interaction is chaotic.", "description": "This figure shows partial state trajectories for two different control tasks (Cartpole Balance and Walker Walk) when using the Dreamer V3 model.  The trajectories are generated using nearly identical initial states (differing by only 10\u207b\u2074 units).  The significant divergence of these trajectories highlights the chaotic nature of the control system. Small initial differences quickly lead to vastly different system behaviors.", "section": "Chaotic state dynamics"}, {"figure_path": "HCTikT7LS4/figures/figures_6_1.jpg", "caption": "Figure 5: Reward MLE interquartile mean for the Pointmass (PM), Cartpole Balance (CB), Cartpole Swingup (CS), Walker Stand (WS), Walker Walk (WW), Walker Run (WR) and Cheetah Run (CR) when controlled by SAC, TD3 and Dreamer V3 (DR3). Each policy-environment combination is independently trained with three random seeds and the reward MLE for each seed is calculated using 20 initial states. A bootstrapped 95% confidence interval is included to show the variation in reward stability across random seeds.", "description": "The figure shows the Maximal Lyapunov Exponent (MLE) of the reward for different control tasks (Pointmass, Cartpole Balance, Cartpole Swingup, Walker Stand, Walker Walk, Walker Run, and Cheetah Run) when controlled by three different reinforcement learning algorithms (SAC, TD3, and DreamerV3). Each bar represents the interquartile range of the reward MLE across three independent training runs, and the whiskers show the 95% confidence intervals. A red dashed line is drawn at MLE = 0 to show the transition between stable (negative MLE) and chaotic (positive MLE) reward dynamics. The figure shows that the reward dynamics are stable for simple low-dimensional tasks but become chaotic for higher-dimensional tasks, regardless of the RL algorithm used.", "section": "Chaotic rewards"}, {"figure_path": "HCTikT7LS4/figures/figures_6_2.jpg", "caption": "Figure 6: Left: Total reward attained by a deterministic SAC policy when controlling the deterministic Walker Walk environment subject to an initial perturbation with fixed direction and varying magnitude. Right: Rewards attained by the three best and three worst state trajectories subject to this perturbation.", "description": "The left panel shows how the total reward changes with different perturbation sizes when a deterministic SAC policy controls the Walker Walk environment. The right panel displays the reward trajectories for the three best-performing and three worst-performing instances, highlighting the significant variability in outcomes due to the chaotic nature of the system.", "section": "4 Chaotic rewards"}, {"figure_path": "HCTikT7LS4/figures/figures_8_1.jpg", "caption": "Figure 7: Total episode reward for the Cartpole Balance (CB), Cartpole Swingup (CS), Walker Stand (WS), Walker Walk (WW), Walker Run (WR) and Cheetah Run (CR) environments when controlled by trained instances of SAC, TD3, Dreamer V3 (DR3) and Dreamer V3 with MLE regularisation (MLE DR3) subject to N (0, \u03c3) Gaussian observation noise. Each policy-environment combination is independently trained with three random seeds and the average episode reward with a bootstrapped 95% confidence interval is reported over 80 evaluation episodes each with a fixed length of 1000 steps.", "description": "This figure displays the total reward obtained by four different reinforcement learning algorithms (SAC, TD3, DR3, and MLE DR3) across six different control tasks from the DeepMind Control Suite. Each algorithm was trained without noise and then tested with varying levels of Gaussian observation noise (\u03c3). The plot shows the average total reward and its 95% confidence interval across multiple trials for each algorithm and task.", "section": "6 Experiments"}, {"figure_path": "HCTikT7LS4/figures/figures_9_1.jpg", "caption": "Figure 8: State trajectories produced when Dreamer V3 and Dreamer V3 + MLE regularisation control the Walker Stand environment with N (0,0.5) Gaussian observation noise.", "description": "This figure shows the torso angle and torso height trajectories produced by Dreamer V3 and the proposed Dreamer V3 + MLE regularisation method when controlling the Walker Stand environment.  The trajectories start from slightly different initial states and are subject to N(0, 0.5) Gaussian observation noise, which introduces continuous state perturbations. The plot visually demonstrates the effect of MLE regularisation on stabilizing the state trajectories and reducing the sensitivity to noise.  The Dreamer V3 trajectories show significant divergence, while the Dreamer V3 + MLE regularisation trajectories exhibit much greater stability and convergence.", "section": "7 Conclusion"}, {"figure_path": "HCTikT7LS4/figures/figures_14_1.jpg", "caption": "Figure 9: Estimated Maximal Lyapunov Exponent of environments sampled from the DeepMind Control Suite when controlled by SAC, TD3 and Dreamer V3 (DR3). Each policy-environment combination is independently trained with three random seeds and evaluated using the parameters in Table 5, except the number of iterations, which varies from 1 to 100. The mean and 95% confidence interval indicate that MLE converges after 100 iterations.", "description": "This figure shows the estimated Maximal Lyapunov Exponent (MLE) for different environments from the DeepMind Control Suite, controlled by three different reinforcement learning algorithms (SAC, TD3, and Dreamer V3).  The x-axis represents the number of iterations used to calculate the MLE, and the y-axis represents the MLE value.  The shaded area around each line represents the 95% confidence interval.  The plot demonstrates that the MLE generally converges after around 100 iterations for all algorithms and environments.", "section": "B.2 Number of iterations ablation study"}, {"figure_path": "HCTikT7LS4/figures/figures_14_2.jpg", "caption": "Figure 10: Estimated Maximal Lyapunov Exponent of environments sampled from the DeepMind Control Suite when controlled by SAC, TD3 and Dreamer V3 (DR3). Each policy-environment combination is independently trained with three random seeds and evaluated using the parameters in Table 5, except the number of initial samples, which varies from 1 to 20. The mean and 95% confidence interval indicate that MLE converges with 20 initial samples.", "description": "This figure shows the estimated Maximal Lyapunov Exponent (MLE) for different DeepMind Control Suite environments controlled by three different reinforcement learning algorithms (SAC, TD3, and Dreamer V3).  It investigates how the number of initial samples used to calculate the MLE affects the convergence of the MLE estimation.  The results show MLE convergence is achieved with 20 initial samples.", "section": "B.3 Number of samples ablation study"}]