[{"figure_path": "DlYNGpCuwa/tables/tables_2_1.jpg", "caption": "Table 2: Performance of baselines and our methods in terms of cumulative edit distance cost and classification accuracy.  \u00b5 denotes the mean \u00b5 and standard deviation \u03c3 across 3 runs over different random seeds. Expense column shows budget as the average number of input and output BPE tokens across 3 runs (unit is 105). We use -k in method names to denote that we use k retrieved examples. Numbers in bold are the best performance in each column excluding oracle preference method, underline for the second best, and dotted underline for the third best.", "description": "This table presents the quantitative results of the experiments, comparing different methods (including the proposed CIPHER algorithm and baselines) across two tasks (summarization and email writing). Three metrics are reported for each method: cumulative edit distance cost, accuracy in predicting the user preference, and the expense in terms of the number of tokens processed by the LLM.  The results show that CIPHER outperforms the baselines in terms of edit distance cost while having competitive LLM expense.", "section": "4.3 Main Result and Discussion"}, {"figure_path": "DlYNGpCuwa/tables/tables_3_1.jpg", "caption": "Table 2: Performance of baselines and our methods in terms of cumulative edit distance cost and classification accuracy. \u03bc denotes the mean \u03bc and standard deviation \u03c3 across 3 runs over different random seeds. Expense column shows budget as the average number of input and output BPE tokens across 3 runs (unit is 105). We use -k in method names to denote that we use k retrieved examples. Numbers in bold are the best performance in each column excluding oracle preference method, underline for the second best, and dotted underline for the third best.", "description": "This table presents the performance comparison of different methods (baselines and the proposed CIPHER) using three metrics: cumulative edit distance, accuracy, and expense (in terms of BPE tokens).  The results are averaged across three runs with different random seeds for both summarization and email writing tasks.  It shows the mean and standard deviation for each metric. The table highlights the best performing method in each column (excluding the oracle method) and indicates the second and third best performances.", "section": "4.3 Main Result and Discussion"}, {"figure_path": "DlYNGpCuwa/tables/tables_4_1.jpg", "caption": "Table 1: Latent user preference design, specific to the document source.", "description": "This table presents the different latent user preferences used in the experiments. Each row represents a document source (e.g., news article, Reddit post, etc.) and lists the corresponding latent preference designed for the simulated user in that context. The latent preferences capture various aspects such as style, tone, and structure of the desired output.  The \"Scenario\" column provides a brief description of how this latent user preference might be applied in a real-world scenario. For example, a news article might be summarized for young children using playful language, whereas a movie review might emphasize a concise and direct answer style.", "section": "4.1 Two Interactive Writing Assistant Environments for Learning from User Edits"}, {"figure_path": "DlYNGpCuwa/tables/tables_4_2.jpg", "caption": "Table 2: Performance of baselines and our methods in terms of cumulative edit distance cost and classification accuracy.  \u00b5 denotes the mean \u00b5 and standard deviation \u03c3 across 3 runs over different random seeds. Expense column shows budget as the average number of input and output BPE tokens across 3 runs (unit is 10<sup>5</sup>). We use -k in method names to denote that we use k retrieved examples. Numbers in bold are the best performance in each column excluding oracle preference method, underline for the second best, and dotted underline for the third best.", "description": "This table presents the quantitative results of the experiment, comparing different methods (baselines and CIPHER with different configurations) across two tasks (summarization and email writing).  Three key metrics are reported: cumulative edit distance (lower is better, reflecting user effort), accuracy (higher is better, indicating alignment with user preferences), and expense (lower is better, measuring the computational cost in terms of tokens processed by the LLM).  The table clearly shows the performance of CIPHER in comparison to other methods, highlighting its effectiveness in achieving lower edit costs and higher accuracy while maintaining computational efficiency.", "section": "4.3 Main Result and Discussion"}, {"figure_path": "DlYNGpCuwa/tables/tables_6_1.jpg", "caption": "Table 2: Performance of baselines and our methods in terms of cumulative edit distance cost and classification accuracy.  \u00b5 denotes the mean \u00b5 and standard deviation \u03c3 across 3 runs over different random seeds. Expense column shows budget as the average number of input and output BPE tokens across 3 runs (unit is 10<sup>5</sup>). We use -k in method names to denote that we use k retrieved examples. Numbers in bold are the best performance in each column excluding oracle preference method, underline for the second best, and dotted underline for the third best.", "description": "This table presents the quantitative results of the experiments.  It compares the performance of several methods (including the proposed CIPHER algorithm and various baselines) on two tasks (summarization and email writing) using three metrics: cumulative edit distance, accuracy, and expense (measured as the total number of input and output tokens for the LLM).  The results are averaged across three random seeds for improved statistical reliability.", "section": "4.3 Main Result and Discussion"}, {"figure_path": "DlYNGpCuwa/tables/tables_8_1.jpg", "caption": "Table 2: Performance of baselines and our methods in terms of cumulative edit distance cost and classification accuracy.  \u00b5 denotes the mean \u00b5 and standard deviation \u03c3 across 3 runs over different random seeds. Expense column shows budget as the average number of input and output BPE tokens across 3 runs (unit is 10<sup>5</sup>). We use -k in method names to denote that we use k retrieved examples. Numbers in bold are the best performance in each column excluding oracle preference method, underline for the second best, and dotted underline for the third best.", "description": "This table presents a quantitative comparison of different methods for aligning LLMs based on user edits.  It shows the cumulative edit distance, accuracy of learned preferences, and computational expense (measured in BPE tokens) for several baselines and the proposed CIPHER method on summarization and email writing tasks. The results are averaged over three runs with different random seeds, providing a statistical measure of performance.", "section": "4.3 Main Result and Discussion"}, {"figure_path": "DlYNGpCuwa/tables/tables_16_1.jpg", "caption": "Table 2: Performance of baselines and our methods in terms of cumulative edit distance cost and classification accuracy.  \u00b5 denotes the mean \u00b5 and standard deviation \u03c3 across 3 runs over different random seeds. Expense column shows budget as the average number of input and output BPE tokens across 3 runs (unit is 10<sup>5</sup>). We use -k in method names to denote that we use k retrieved examples. Numbers in bold are the best performance in each column excluding oracle preference method, underline for the second best, and dotted underline for the third best.", "description": "This table presents the quantitative results of the proposed CIPHER method and several baseline methods.  The results are shown across three metrics: cumulative edit distance, classification accuracy (of inferred user preferences), and LLM expense (measured in BPE tokens).  The table compares the performance of CIPHER with different hyperparameter settings (k=1 and k=5) and different context embedding methods (BERT and MPNet) to baseline methods that use no learning, learn context-agnostic preferences (E-then-e and Continual LPI), or utilize only past edits without learning user preferences (ICL-edit and CoT-edit).  The oracle method, which has access to ground truth latent preferences, provides an upper bound on performance.", "section": "4.3 Main Result and Discussion"}, {"figure_path": "DlYNGpCuwa/tables/tables_17_1.jpg", "caption": "Table 2: Performance of baselines and our methods in terms of cumulative edit distance cost and classification accuracy.  \u03bc denotes the mean \u03bc and standard deviation \u03c3 across 3 runs over different random seeds. Expense column shows budget as the average number of input and output BPE tokens across 3 runs (unit is 105). We use -k in method names to denote that we use k retrieved examples. Numbers in bold are the best performance in each column excluding oracle preference method, underline for the second best, and dotted underline for the third best.", "description": "This table presents a quantitative comparison of different methods (including the proposed CIPHER and several baselines) for aligning LLMs based on user edits. The comparison is done across two tasks (summarization and email writing) and three metrics: cumulative edit distance cost (lower is better), accuracy of learned preferences (higher is better), and the total expense in terms of BPE tokens (lower is better).  The table shows the mean and standard deviation across three runs to provide statistical significance.", "section": "4.3 Main Result and Discussion"}, {"figure_path": "DlYNGpCuwa/tables/tables_18_1.jpg", "caption": "Table 2: Performance of baselines and our methods in terms of cumulative edit distance cost and classification accuracy.  \u00b5 denotes the mean \u00b5 and standard deviation \u03c3 across 3 runs over different random seeds. Expense column shows budget as the average number of input and output BPE tokens across 3 runs (unit is 10<sup>5</sup>). We use -k in method names to denote that we use k retrieved examples. Numbers in bold are the best performance in each column excluding oracle preference method, underline for the second best, and dotted underline for the third best.", "description": "This table presents the quantitative results of the experiment, comparing different methods (including the proposed CIPHER and several baselines) across two tasks (summarization and email writing).  The metrics used for comparison are cumulative edit distance (a lower value is better, indicating less user effort), accuracy of the learned preference (higher is better), and expense (measured in BPE tokens, lower is better representing lower LLM usage cost).  The table provides a comprehensive comparison showing CIPHER's superiority in terms of minimizing user edit costs while maintaining a relatively low LLM query cost.", "section": "4.3 Main Result and Discussion"}, {"figure_path": "DlYNGpCuwa/tables/tables_19_1.jpg", "caption": "Table 2: Performance of baselines and our methods in terms of cumulative edit distance cost and classification accuracy.  \u00b5 denotes the mean \u00b5 and standard deviation \u03c3 across 3 runs over different random seeds. Expense column shows budget as the average number of input and output BPE tokens across 3 runs (unit is 10<sup>5</sup>). We use -k in method names to denote that we use k retrieved examples. Numbers in bold are the best performance in each column excluding oracle preference method, underline for the second best, and dotted underline for the third best.", "description": "This table presents the quantitative results of the experiments.  It compares the performance of CIPHER against several baseline methods across two tasks (summarization and email writing).  The metrics used are cumulative edit distance, accuracy of preference prediction, and LLM expense (measured in BPE tokens).  The results are averaged across three different random seeds to provide statistical robustness.  The table highlights CIPHER's superior performance in minimizing user editing effort while maintaining a low computational cost.", "section": "4.3 Main Result and Discussion"}, {"figure_path": "DlYNGpCuwa/tables/tables_19_2.jpg", "caption": "Table 2: Performance of baselines and our methods in terms of cumulative edit distance cost and classification accuracy.  \u00b5 denotes the mean \u00b5 and standard deviation \u03c3 across 3 runs over different random seeds. Expense column shows budget as the average number of input and output BPE tokens across 3 runs (unit is 105). We use -k in method names to denote that we use k retrieved examples. Numbers in bold are the best performance in each column excluding oracle preference method, underline for the second best, and dotted underline for the third best.", "description": "This table presents a quantitative comparison of different methods for aligning LLMs based on user edits, including the proposed CIPHER method and several baselines.  It shows the cumulative edit distance (a measure of user effort), classification accuracy (how well the method captures user preferences), and the computational expense (in terms of BPE tokens processed by the LLM).  The results are averaged over three runs with different random seeds, and the best performing methods are highlighted.", "section": "4.3 Main Result and Discussion"}, {"figure_path": "DlYNGpCuwa/tables/tables_20_1.jpg", "caption": "Table 2: Performance of baselines and our methods in terms of cumulative edit distance cost and classification accuracy.  \u00b5 denotes the mean \u00b5 and standard deviation \u03c3 across 3 runs over different random seeds. Expense column shows budget as the average number of input and output BPE tokens across 3 runs (unit is 105). We use -k in method names to denote that we use k retrieved examples. Numbers in bold are the best performance in each column excluding oracle preference method, underline for the second best, and dotted underline for the third best.", "description": "This table presents the quantitative results of the experiments, comparing the performance of different methods on two tasks (summarization and email writing).  The metrics used are cumulative edit distance, accuracy of preference prediction, and the total cost (in BPE tokens) of LLM queries.  The results show CIPHER's superior performance in minimizing edit costs and achieving comparable LLM costs, highlighting the effectiveness of the proposed approach.", "section": "4.3 Main Result and Discussion"}, {"figure_path": "DlYNGpCuwa/tables/tables_20_2.jpg", "caption": "Table 2: Performance of baselines and our methods in terms of cumulative edit distance cost and classification accuracy.  \u00b5 denotes the mean \u00b5 and standard deviation \u03c3 across 3 runs over different random seeds. Expense column shows budget as the average number of input and output BPE tokens across 3 runs (unit is 10<sup>5</sup>). We use -k in method names to denote that we use k retrieved examples. Numbers in bold are the best performance in each column excluding oracle preference method, underline for the second best, and dotted underline for the third best.", "description": "This table presents the performance comparison of different methods (baselines and the proposed CIPHER method) on summarization and email writing tasks.  The performance metrics include cumulative edit distance (lower is better), accuracy of preference prediction (higher is better), and LLM expense (lower is better). The table shows that CIPHER outperforms other methods, achieving the lowest edit distance while having comparable LLM costs.", "section": "4.3 Main Result and Discussion"}, {"figure_path": "DlYNGpCuwa/tables/tables_21_1.jpg", "caption": "Table 2: Performance of baselines and our methods in terms of cumulative edit distance cost and classification accuracy.  \u00b5 denotes the mean \u00b5 and standard deviation \u03c3 across 3 runs over different random seeds. Expense column shows budget as the average number of input and output BPE tokens across 3 runs (unit is 105). We use -k in method names to denote that we use k retrieved examples. Numbers in bold are the best performance in each column excluding oracle preference method, underline for the second best, and dotted underline for the third best.", "description": "This table presents a quantitative comparison of different methods (including the proposed CIPHER and several baselines) for interactive learning from user edits.  The comparison is done across two tasks (summarization and email writing) and three metrics: cumulative edit distance, accuracy of preference prediction, and LLM query cost (expense).  The results show CIPHER's superior performance in minimizing user edit cost while maintaining a relatively low LLM query cost.", "section": "4.3 Main Result and Discussion"}, {"figure_path": "DlYNGpCuwa/tables/tables_21_2.jpg", "caption": "Table 2: Performance of baselines and our methods in terms of cumulative edit distance cost and classification accuracy.  \u00b5 denotes the mean \u00b5 and standard deviation \u03c3 across 3 runs over different random seeds. Expense column shows budget as the average number of input and output BPE tokens across 3 runs (unit is 10<sup>5</sup>). We use -k in method names to denote that we use k retrieved examples. Numbers in bold are the best performance in each column excluding oracle preference method, underline for the second best, and dotted underline for the third best.", "description": "This table presents the performance comparison of different methods (including baselines and CIPHER with different configurations) on summarization and email writing tasks.  The metrics used are cumulative edit distance (lower is better), accuracy of preference prediction (higher is better), and the total expense in terms of BPE tokens (lower is better).  The results are averaged over three runs with different random seeds to show statistical significance and robustness.", "section": "4.3 Main Result and Discussion"}]