[{"figure_path": "DlYNGpCuwa/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration of interactive learning from user edits. Color coding in edits is for visualization only - our agent takes the plain revised text as feedback.", "description": "This figure illustrates the interactive learning process from user edits.  The process starts with the user providing context (such as an article to summarize) to an LLM agent.  The agent then generates a response. The user can then edit this response, and the edit is considered feedback for the agent. The agent incurs a cost based on the edit distance between the original response and the edited response. The goal is to minimize the cumulative cost of edits over time.  The color coding in the example edit is just for visualization; the agent processes only the plain text.", "section": "Interactive Learning from User Edits"}, {"figure_path": "DlYNGpCuwa/figures/figures_7_1.jpg", "caption": "Figure 2: Learning curves of different methods based on cumulative cost over time (average across 3 seeds). In the legend, -k means with top k retrieved examples, -B for BERT, and -M for MPNET.", "description": "This figure shows the learning curves of different methods for two tasks: summarization and email writing. The x-axis represents the number of rounds, and the y-axis represents the cumulative cost, which is the sum of edit distances between the agent's response and the user's edits in each round.  The curves illustrate how the cumulative cost changes over time for each method.  The legend shows that different methods (no learning, explore-then-exploit, continual learning, ICL-edit, CoT-edit, and CIPHER) with variations using BERT or MPNET and different numbers of retrieved examples (k) are compared.  The oracle method is shown as a baseline.", "section": "4 Experiment"}, {"figure_path": "DlYNGpCuwa/figures/figures_7_2.jpg", "caption": "Figure 3: Percentage of zero-cost examples of CIPHER over time, binned per 20 rounds to show the trend (average across 3 seeds). In the legend, -k means with top k retrieved examples, -B for BERT, and -M for MPNET.", "description": "This figure shows the percentage of times CIPHER produced a response that did not require any edits from the simulated user over the course of 200 rounds of interaction.  The results are binned into groups of 20 rounds to highlight trends.  Separate lines are shown for different variations of CIPHER using BERT or MPNET for context embedding and retrieving either 1 or 5 nearest neighbor contexts.", "section": "4 Experiment"}, {"figure_path": "DlYNGpCuwa/figures/figures_16_1.jpg", "caption": "Figure 2: Learning curves of different methods based on cumulative cost over time (average across 3 seeds). In the legend, -k means with top k retrieved examples, -B for BERT, and -M for MPNET.", "description": "This figure displays the learning curves of various methods used in the paper, plotted against the cumulative cost over time.  The cumulative cost represents the total effort a user expends on editing the model's responses. The x-axis represents the round number (representing each interaction between the user and the language model), and the y-axis shows the cumulative cost.  Different lines represent different methods, with labels indicating which method (-k specifies the number of top retrieved examples, -B indicates BERT was used for context representation, and -M indicates MPNET was used). This graph visually demonstrates how the cumulative cost changes over time for each method, allowing for a comparison of their performance in terms of reducing the user's editing burden.", "section": "4 Experiment"}]