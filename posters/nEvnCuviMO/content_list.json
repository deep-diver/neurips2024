[{"type": "text", "text": "Metric Distortion Under Probabilistic Voting ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s) ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Metric distortion in social choice provides a framework for assessing how well   \n2 voting rules minimize social cost in scenarios where voters and candidates exist   \n3 in a shared metric space, with voters submitting rankings and the rule outputting   \n4 a single winner. We expand this framework to include probabilistic voting. Our   \n5 extension encompasses a broad range of probability functions, including widely   \n6 studied models like Plackett-Luce (PL) and Bradley-Terry, and a novel \"pairwise   \n7 quantal voting\" model inspired by quantal response theory. We demonstrate that   \n8 distortion results under probabilistic voting better correspond with conventional   \n9 intuitions regarding popular voting rules such as Plurality, Copeland, and Random   \n10 Dictator (RD) than those under deterministic voting. For example, in the PL model   \n11 with candidate strength inversely proportional to the square of their metric d\u221aistance,   \n12 we show that Copeland\u2019s distortion is at most 2, whereas that of RD is $\\Omega({\\sqrt{m}})$ in   \n13 large elections, where $m$ is the number of candidates. This contrasts sharply with   \n14 the classical model, where RD beats Copeland with a distortion of 3 versus 5 [1]. ", "page_idx": 0}, {"type": "text", "text": "15 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "16 Societies must make decisions collectively; different agents often have conflicting interests, and the   \n17 choice of the mechanism used for combining everyone\u2019s opinions often makes a big difference to the   \n18 outcome. The machine learning community has applied social choice principles for AI alignment   \n19 [2, 3], algorithmic fairness [4, 5], and preference modelling [6, 7]. Over the last century, there has   \n20 been increasing interest in using computational tools to analyse and design voting rules [8\u201311]. One   \n21 prominent framework for evaluating voting rules is that of distortion [12], where the voting rule has   \n22 access to only the ordinal preferences of the voters. However, the figure of merit is the sum of all   \n23 voters\u2019 cardinal utilities (or costs). The distortion of a voting rule is the worst-case ratio of the cost of   \n24 the alternative it selects and the cost of the truly optimal alternative.   \n25 An additional assumption is imposed in metric distortion [1] \u2013 that the voters and candidates all lie in   \n26 a shared (unknown) metric space, and costs are given by distances (thus satisfying non-negativity   \n27 and triangular inequality). This model is a generalization of a commonly studied spatial model of   \n28 voting in the Economics literature [13, 14], and has a natural interpretation of voters liking candidates   \n29 with a similar ideological position across many dimensions. While metric distortion is a powerful   \n30 framework and has led to the discovery and re-discovery of interesting voting rules (e.g. Plurality   \n31 Veto [15] and the study of Maximal Lotteries [16] for metric distortion by Charikar et al. [17]), its   \n32 outcomes sometimes do not correspond with traditional wisdom around popular voting rules. For   \n33 example, the overly simple Random Dictator $(R D)$ rule (where the winner is the top choice of a   \n34 uniform randomly selected voter) beats the Copeland rule (which satisfies the Condorcet Criterion   \n35 [10] and other desirable properties) with a metric distortion of 3 versus 5 [1].   \n36 While not yet adopted in the metric distortion framework, there is a mature line of work on   \n37 Probabilistic voting (PV) [18\u201320]. Here, the focus is on the behavioural modelling of voters and   \n38 accounting for the randomness of their votes. Two sources of this randomness often cited in the   \n39 literature are the boundedness of the voters\u2019 rationality and the noise in their estimates of candidates\u2019   \n40 positions. A popular model for this behaviour is based on the Quantal Response Theory [20]. Another   \n41 closely related line of work is on Random Utility Models (RUMs) [21\u201323] in social choice where   \n42 the hypothesis is that the candidates have ground-truth strengths. Voters make noisy observations of   \n43 these strengths and vote accordingly. We adopt these models of voting behaviour and study it within   \n44 the metric distortion framework. The questions we ask are:   \n45 Given a model of probabilistic voting, what is the metric distortion of popular voting rules?   \n46 How does this differ (qualitatively and quantitatively) from the deterministic model? ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "47 1.1 Preliminaries and Notation ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "48 Let $\\mathcal{N}$ be a set of $n$ voters and $\\boldsymbol{\\mathcal{A}}$ be the set of $m$ candidates. Let $\\boldsymbol{S}$ be the set of total orders on $\\boldsymbol{\\mathcal{A}}$   \n49 Each voter $i\\in\\mathcal{N}$ has a preference ranking $\\sigma_{i}\\in S$ . A vote profile is a set of preference rankings   \n50 $\\sigma_{\\mathcal{N}}=(\\sigma_{1},...,\\sigma_{n})\\in\\mathcal{S}^{n}$ for all voters. The tuple $(\\mathcal{N},\\mathcal{A},\\sigma_{\\mathcal{N}})$ defines an instance of an election. Let   \n51 $\\Delta(A)$ denote the set of all probability distributions over the set of candidates.   \n52 Definition 1 (Voting Rule). $A$ voting rule $f:S^{n}\\to\\Delta(A)$ takes a vote profile $\\sigma_{N}$ and outputs a   \n53 probability distribution p over the alternatives.   \n54 For deterministic voting rules, we overload notation by saying that the rule\u2019s output is a candidate   \n55 and not a distribution. We now define some voting rules [10]. Let $\\mathbb{I}$ denote the indicator function.   \n56 Random Dictator Rule: Select a voter uniformly at random and output their top choice, i.e.,   \n57 $\\mathrm{RD}(\\sigma_{\\mathcal{N}})=p$ such that $\\begin{array}{r}{p_{j}=\\frac{1}{n}\\sum_{i\\in\\mathcal{N}}\\mathbb{I}(\\sigma_{i,1}=j)}\\end{array}$ .   \n58 Plurality Rule: Choose the candidate who is the top choice of the most voters, i.e., $\\mathrm{PLU}(\\sigma_{\\mathcal{N}})=$   \n59 arg $\\begin{array}{r}{\\operatorname*{max}_{j\\in\\mathcal{A}}\\sum_{i\\in\\mathcal{N}}\\mathbb{I}(\\sigma_{i,1}=j)}\\end{array}$ . Ties are broken arbitrarily.   \n60 Copeland Rule: Choose the candidate who wins the most pairwise comparisons, i.e., $\\mathsf{C O P}(\\sigma_{\\mathcal{N}})=$   \n61 arg $\\begin{array}{r}{\\operatorname*{max}_{j\\in\\mathcal{A}}\\sum_{j^{\\prime}\\in\\mathcal{A}\\setminus\\{j\\}}\\mathbb{I}\\left(\\sum_{i\\in\\mathcal{N}}\\mathbb{I}(j\\succ_{\\sigma_{i}}j^{\\prime})>\\frac{n}{2}\\right)}\\end{array}$ . Ties are broken arbitrarily.   \n62 Distance function $d:(\\mathcal{N}\\cup\\mathcal{A})^{2}\\rightarrow\\mathbb{R}_{\\ge0}$ satisfies triangular inequality $(d(x,y)\\leq d(x,z)+d(z,y))$   \n63 and symmetry $(d(x,y)=d(y,x))$ . The distance between voter $i\\in\\mathcal{N}$ and candidate $j\\in A$ is also   \n64 referred to as the cost of $j$ for $i$ . We consider the most commonly studied social cost function, which   \n65 is the sum of the costs of all voters. $\\begin{array}{r}{S C(j,d):=\\sum_{i\\in\\mathcal{N}}d(i,j)}\\end{array}$ .   \n66 In deterministic voting, the preference ranking $\\sigma_{i}$ of voter $i$ is consistent with the distances. That is,   \n67 $d(i,j)>d(i,j^{\\prime})\\implies j^{\\prime}\\succ_{\\sigma_{i}}j$ for all voters $i\\in\\mathcal N$ and candidates $j,j^{\\prime}\\in A$ . Let $\\rho(\\sigma_{\\mathcal{N}})$ be the set   \n68 of distance functions $d$ consistent with vote profile $\\sigma_{\\mathcal{N}}$ . The metric distortion of a voting rule is: ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "69 Definition 2 (Metric Distortion). $\\mathsf{D I S T}(f)=\\operatorname*{sup}_{\\mathcal{N},\\mathcal{A},\\sigma_{\\mathcal{N}}}\\ \\operatorname*{sup}_{d\\in\\rho(\\sigma_{\\mathcal{N}})}\\ \\frac{\\mathbb{E}[S C(f(\\sigma_{\\mathcal{N}}),d)]}{\\operatorname*{min}_{j\\in\\mathcal{A}}S C(j,d)}.$ ", "page_idx": 1}, {"type": "text", "text": "70 1.2 Our Contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "71 We extend the study of metric distortion to probabilistic voting (Definition 4). This extension is useful   \n72 since voters, in practice, have been shown to vote randomly [20]. We define axiomatic properties   \n73 of models of probabilistic voting which are suitable for studying metric distortion. These are scale  \n74 freeness with distances (Axiom 1), pairwise order probabilities being independent of other candidates   \n75 (Axiom 2), and strict monotonicity of pairwise order probabilities in distances (Axiom 3).   \n76 All our results apply to a broad class of models of probabilistic models, as explained in $\\S~2$ . We   \n77 provide distortion bounds for all $n\\geq3$ and $m\\geq2$ , which are most salient in the limit $n\\to\\infty$ . For   \n78 large elections $\\cdot m$ fixed, $n\\to\\infty$ ), we provide matching upper and lower bounds on the distortion of   \n79 Plurality, an upper bound for Copeland, and a lower bound for RD. The distortion of plurality grows   \n80 linearly in $m$ . The distortion upper bound of Copeland is constant. The distortion lower bound for   \n81 RD increases sublinearly in $m$ where this rate depends on the probabilistic model. Crucially, our   \n82 results match those in deterministic voting in the limit where the randomness goes to zero.   \n83 The technique is as follows. For the problem of maximizing the distortion, we establish a critical   \n84 threshold of the expected fraction of votes on pairwise comparisons on all edges on a directed path   \n85 from a winner to the \u201ctrue optimal\" candidate for Copeland and Plurality. This path is one or two hops   \n86 for Copeland and one for Plurality. We then formulate a linear-fractional program which incorporates   \n87 this critical threshold. We linearize this program via the sub-level sets technique [24], and find a   \n88 feasible solution of the dual problem. Concentration inequalities on this solution provide an upper   \n89 bound on the distortion. We find a matching lower bound for Plurality by construction.   \n91 Metric distortion Anshelevich et al. [1] initiated the study of metric distortion and showed that   \n92 any deterministic voting rule has a distortion of at least 3 and that Copeland has a distortion of 5.   \n93 The Plurality Veto Rule attains the optimal distortion of 3 [15]. Charikar and Ramakrishnan [25]   \n94 showed that any randomized voting rule has a distortion of at least 2.112. Charikar et al. [17] gave   \n95 a randomized voting rule with a distortion of at most 2.753. Anshelevich et al. [26] gave a useful   \n96 survey on distortion in social choice.   \n97 Distortion with Additional Information Abramowitz et al. [27] showed that deterministic voting   \n98 rules achieve a distortion of 2 when voters provide preference strengths as ratios of distances.   \n99 Amanatidis et al. [28] demonstrated that even a few queries from each voter can significantly improve   \n100 distortion in non-metric settings. Anshelevich et al. [29] examined threshold approval voting, where   \n101 voters approve candidates with utilities above a threshold. Our work relates to these studies since in   \n102 probabilistic voting, the likelihood of a voter switching the order of two candidates depends on the   \n103 relative strength of their preference, often resulting in lower distortion than deterministic methods.   \n104 Probabilisitc voting and random utility models (RUMs) Hinich [30] showed that the celebrated   \n105 Median Voter Theorem of [31] does not hold under probabilistic voting. Classical work has focused   \n106 on studying the equilibrium positions of voters and/or candidates in game-theoretic models of   \n107 probabilistic voting [20, 32\u201335]. McKelvey and Patty [20] adopt the quantal response model, a   \n108 popular way to model agents\u2019 bounded rationality.   \nRUMs have mostly been studied in social choice [21, 23, 36] with the hypothesis that candidates have   \n110 universal ground-truth strengths, which voters make noisy observations of. Our model is the same as   \n111 RUM regarding the voters\u2019 behaviour; however, voters have independent costs from candidates. The   \n112 Plackett-Luce (PL) model [37, 38] has been widely studied in social choice [39\u201341]. For probabilities   \n113 on pairwise orders, PL reduces to the Bradley-Terry (BT) model [42]. These probabilities are   \n114 proportional to candidates\u2019 strengths (which we define as the inverse of powers of costs).   \n115 The widely studied Mallows model [43], based on Condorcet [44], flips the order of each candidate   \n116 pair (relative to a ground truth ranking) with a constant probability $\\bar{p}\\in\\dot{(0,\\frac{1}{2})}$ [45, 46]. The process is   \n117 repeated if a linear order is not attained. In the context of metric distortion, a limitation of this model   \nis that it doesn\u2019t account for the relative distance of candidates to the voter. For a comprehensive   \n119 review of RUM models, see Marden [47]. Critchlow et al. [48] does an axiomatic study of RUM   \n120 models; our axioms are grounded in metric distortion and are distinct from theirs.   \nRecently, there has been significant interest in smoothed analysis [49] of social choice. Here a small   \n122 amount of randomness is added to problem instances and its effect is studied on the satisfiability of   \n123 axioms [50\u201353] and the computational complexity of voting rules [54\u201356]. Baumeister et al. [50]   \n124 term this model as being \u2018towards reality,\u2019 highlighting the need to study the randomness in the   \n125 election instance generation processes. Unlike smoothed analysis where the voter and candidate   \n126 positions are randomized, we consider these positions fixed, but the submitted votes are random given   \n127 these positions. The technical difference appears in the benchmark (the \u201coptimal\" outcome in the   \n128 denominator of the distortion is unchanged in our framework and changes in smoothed analysis). ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "129 2 Axioms and Model ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "130 Under probabilistic voting, the submitted preferences may no longer be consistent with the underlying   \n131 distances. For a distribution $\\mathcal{P}(d)$ over $\\sigma_{\\mathcal{N}}$ , let $q^{\\mathcal{P}(d)}(i,j,j^{\\prime})$ denote the induced marginal probability   \n132 that voter $i$ ranks candidate $j$ higher than $j^{\\prime}$ . We focus on these marginal probabilities on pairwise   \n133 orders and provide axioms for classifying which $q^{\\mathcal{P}(d)}(\\cdot)$ are suitable for studying distortion.   \n134 Axiom 1 (Scale-Freeness (SF)). The probability $q^{\\mathcal{P}(d)}(\\cdot)$ must be invariant to scaling of $d,$ . That is,   \n135 for any tuple $(i,j,j^{\\prime})$ and any constant $\\kappa>0$ , we must have $q^{\\mathcal{P}(d)}(i,j,j^{\\prime})=q^{\\mathcal{P}(\\kappa d)}(i,j,j^{\\prime})$ .   \n136 Note that the metric distortion (Definition 2) for deterministic voting is scale-free. We want to retain   \n137 the same property in the probabilistic model as well. Conceptually, one may think of the voter\u2019s   \n138 preferences as being a function of the relative (and not absolute) distances to the candidates.   \n139 Axiom 2 (Independence of Other Candidates (IOC)). The probability $q^{\\mathcal{P}(d)}(i,j,j^{\\prime})$ must be   \n140 independent of the distance of voter $i$ to all \u2018other\u2019 candidates, i.e., those in $A\\setminus\\{j,j^{\\prime}\\}$ .   \n141 This axiom extends Luce\u2019s choice axioms [38], defined for selecting the top choice, to entire rankings.   \n142 IOC is reminiscent of the independence of irrelevant alternatives axiom for voting rules.   \n143 Axiom 3 (Strict Monotonicity (SM)). For every tuple $(i,j,j^{\\prime})$ , for fixed distance $d(i,j)>0$ , the   \n144 probability $q^{\\mathcal{P}(d)}(i,j,j^{\\prime})$ must be strictly increasing in $d(i,j^{\\prime})$ at all but at most finitely many points.   \n145 The monotonicity in $d(i,j)$ follows since $q^{\\mathcal{P}(d)}(i,j^{\\prime},j)=1-q^{\\mathcal{P}(d)}(i,j,j^{\\prime}).$ . This axiom is natural.   \n146 In the Mallows model [43], $q^{\\mathcal{P}(d)}(\\cdot)$ was derived by Busa-Fekete et al. [57] and is as follows: ", "page_idx": 2}, {"type": "table", "img_path": "nEvnCuviMO/tmp/8f767b25b85d5152b8a95d61ed0930bf6ba8a4cf28f25e54381bb0c38c7fa3ac.jpg", "table_caption": ["Table 1: Axioms satisfied by commonly studied models of probabilistic voting "], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{:}\\qquad q^{\\mathcal{P}(d)}(i,j,j^{\\prime})=h(r_{j^{\\prime}}-r_{j}+1,\\phi)-h(r_{j^{\\prime}}-r_{j},\\phi).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "147 Here $\\begin{array}{r}{h(k,\\phi)=\\frac{k}{(1-\\phi^{k})}}\\end{array}$ . Whereas $r_{j}$ and $r_{j^{\\prime}}$ are the positions of $j$ and $j^{\\prime}$ in the ground-truth (noiseless)   \n148 ranking, and the constant $\\phi$ is a dispersion parameter. Observe that this model fails Axiom 2 since it   \n149 depends on the number of candidates between $j$ and $j^{\\prime}$ in the noiseless ranking. It also fails Axiom 3   \n150 since it does not depend on the exact distances but only on the order of the distances.   \n151 Plackett-Luce Model: The PL model [37, 38] is \u2018sequential\u2019 in the following way. For each voter   \n152 $i\\in\\mathcal{N}$ , each candidate $j\\in A$ has a \u2018strength\u2019 $s_{i,j}$ . In most of the literature on RUMs, a common   \n153 assumption is that $s_{i,j}$ is the same for all voters $i$ . However, we choose this more general model to   \n154 make it useful in the context of metric distortion. The voter chooses their top choice with probability   \n155 proportional to the strengths. Similarly, for every subsequent rank, they choose a candidate from   \n156 among the remaining ones with probabilities proportional to their strengths. In terms of the pairwise   \n157 order probabilities, the PL model reduces to the Bradley-Terry (BT) model [42], that is: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{PL/BT:}\\ \\ \\ \\ q^{\\mathcal{P}(d)}(i,j,j^{\\prime})=\\frac{s_{i,j}}{s_{i,j}+s_{i,j^{\\prime}}}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "158 Prima facie, in the metric distortion framework, any decreasing function of distance $d(i,j)$ would   \n159 be a natural choice for $s_{i,j}$ . However, not all such functions satisfy Axiom 1. The exponential   \n160 function is a popular choice in the literature employing BT or $\\mathrm{PL}$ models. However, in general,   \n161 $\\begin{array}{r}{\\frac{e^{-d(i,j)}}{e^{-d(i,j)}+e^{-d(i,j^{\\prime})}}\\neq\\frac{e^{-2d(i,j)}}{e^{-2d(i,j)}+e^{-2d(i,j^{\\prime})}}}\\end{array}$ e\u22122d(ie,j)+e\u22122d(i,j\u2032) , thus failing the Scale-Freeness Axiom 1.   \n162 On the other hand, observe that all functions $s=d^{-\\theta}$ for any $\\theta\\in(0,\\infty)$ satisfy our axioms. We use   \n163 the regime $\\theta\\in(1,\\infty)$ for technical simplicity in this work.   \n164 We also define the following class of functions \u201cPQV\u201d for $q^{\\mathcal{P}(d)}(\\cdot)$ motivated by Quantal Response   \n165 Theory [58] and its use in probabilistic voting [20]. Observe that PQV satisfies all our axioms.   \n166 Definition 3 (Pairwise Quantal Voting (PQV)). Let the relative preference $r(i,j,j^{\\prime})$ be the ratio of   \n167 distances, dd((ii,,jj )) . For constant \u03bb > 0, PQV is as follows: qP(d)(i, j, j\u2032) = e\u2212\u03bbr(i,ej,\u2212j\u03bb\u2032)/r+(ei,\u2212j,\u03bbj/)r(i,j,j\u2032) .   \n168 We now define a general class of functions for pairwise order probabilities in terms of the relative   \n169 preference (ratio of distances) $r$ . Let $\\mathbf{G}$ be a class of functions such that any $\\mathbf{G}\\ni g:[0,\\infty)\\cup\\{\\infty\\}\\to$   \n170 $[0,1]$ has the following properties.   \n171   \n172   \n173   \n1. $g$ is continuous and twice-differentiable.   \n2. $g(0)=0$ . Further, $g^{\\prime}(r)>0\\,\\forall r\\in(0,\\infty)$ i.e. $g(r)$ is strictly increasing in $[0,\\infty)$ .   \n3. Define $\\frac{1}{r}$ as $+\\infty$ when $r=0$ . Then we must have $\\begin{array}{r}{g(r)+g(\\frac{1}{r})=1\\ \\forall r\\geq0}\\end{array}$ .   \n4. There $\\exists c\\in[0,\\infty)$ s.t. $g^{\\prime\\prime}(r)>0\\,\\,\\forall r\\in(0,c)$ i.e. $g$ is convex in the open interval $(0,c)$ .   \n175 Observe that PL (with $\\begin{array}{r}{g(r)\\,=\\,\\frac{r^{\\theta}}{1+r^{\\theta}},\\theta\\,>\\,1)}\\end{array}$ and PQV (with $\\begin{array}{r}{g(r)\\,=\\,\\frac{e^{-\\lambda/r}}{e^{-\\lambda r}+e^{-\\lambda/r}},\\lambda\\,>\\,0)}\\end{array}$ are in   \n176 G. Construction of distributions (if any exists) on rankings $\\sigma_{\\mathcal{N}}$ which generate pairwise order   \n177 probabilities $\\begin{array}{r}{q^{\\mathcal{P}(d)}(i,j,j^{\\prime})=g(\\frac{d(i,j^{\\prime})}{d(i,j)})}\\end{array}$ according to PQV is left for future work. We do not need it   \n178 for our technical derivations. For PL, these distributions are known from prior work [40].   \n179 We assume $g\\in\\mathbf{G}$ in the rest of the paper. Let $\\mathcal{M}(\\mathcal{N}\\cup\\mathcal{A})$ denote the set of valid distance functions   \n180 on $({\\mathcal{N}},{\\mathcal{A}})$ . For any $g$ and $d\\in\\mathcal{M}(\\mathcal{N}\\cup\\mathcal{A})$ let $\\hat{\\mathcal{P}}^{(g)}(d)$ denote the set of probability distributions on   \n181 $\\sigma_{\\mathcal{N}}$ for which the marginal pairwise order probabilities are $\\begin{array}{r}{g\\big(\\frac{d(i,j^{\\prime})}{d(i,j)}\\big)}\\end{array}$ . That is, ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "image", "img_path": "nEvnCuviMO/tmp/ebd16e9f6ff27199457464d61accef7a880b5b7348133df86444505eeb4b6d25.jpg", "img_caption": ["Figure 1: A 1-d Euclidean example of voting probabilities. There are two candidates at 0 and 1. The figure on the left shows the voter position between 0 and 1. In the right figure, the voter is in positions to the left of 0. As the distance grows, both candidates look similar to the voter in the probabilistic model but not in deterministic voting. The case of voter positions to the right of 1 is symmetric. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\forall\\mathcal{P}\\in\\hat{\\mathcal{P}}^{(g)}(d),\\sigma_{\\mathcal{N}}\\sim\\mathcal{P}\\implies\\mathbb{P}[A\\succ_{i}B]=g\\left(\\frac{d(i,B)}{d(i,A)}\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "182 We assume that all voters vote independently of each other. We now define metric distortion under   \n183 probabilistic voting as a function of $g$ for a given $m$ and $n$ . ", "page_idx": 4}, {"type": "text", "text": "Definition 4 (Metric Distortion under Probabilistic Voting). ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{DIST}^{(g)}(f,n,m):=\\operatorname*{sup}_{\\substack{\\mathcal{N}:|{\\cal N}|=n\\,d\\in\\mathcal{M}(\\cal N\\backslash\\cal A)}}\\operatorname*{sup}_{\\substack{\\mathcal{P}\\in\\hat{\\mathcal{P}}^{(g)}(d)}}\\frac{\\mathbb{E}_{\\sigma_{\\mathcal{N}}\\sim\\mathcal{P}}[S C(f(\\sigma_{N}),d)]}{\\operatorname*{min}_{A\\in\\cal A}{S C(A,d)}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "184 $\\mathrm{DIST}^{(g)}(f)=\\operatorname*{sup}_{n,m}\\mathrm{DIST}^{(g)}(f,n,m)$ by supremizing over all possible n and $m$ . ", "page_idx": 4}, {"type": "text", "text": "185 The expectation is both over the randomness in the votes and the voting rule $f$ . ", "page_idx": 4}, {"type": "text", "text": "186 Observe that the distortion is a supremum over all distributions in $\\hat{\\mathcal{P}}^{(g)}(d)$ . Since we focus on large   \n187 elections (with large $n$ and relatively small $m$ ), we define $\\mathrm{DIST}^{(g)}$ as a function of $m$ and $n$ .   \n188 As in Fig. 1, consider the 1-d Euclidean space with candidate $X$ at the origin and $Y$ at 1. Observe   \n189 that g 1\u2212xx and $\\textstyle g\\left({\\frac{x}{1+x}}\\right)$ denote the probability that a voter located at a distance $x$ from $X$ votes   \n190 for $Y$ when the voter is to the left and right of $X$ respectively. Interestingly, this 1-d intuition extends   \n191 well for general metric spaces. Towards this, we define the following functions. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\ng_{\\mathrm{MID}}(x):=g\\left(\\frac{x}{1-x}\\right)\\forall x\\in(0,1)\\mathrm{~and~}g_{\\mathrm{our}}(x):=g\\left(\\frac{x}{1+x}\\right)\\forall x\\in[0,\\infty).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "192 Lemma 1. gMID(x) $\\textstyle{\\frac{g_{\\mathrm{MID}}(x)}{x}}\\ a n d\\ {\\frac{g_{\\mathrm{our}}(x)}{x}}$ have unique local maxima in $(0,1)$ and $(0,\\infty)$ respectively. x ", "page_idx": 4}, {"type": "text", "text": "193 Denote the unique maximisers of gMIDx(x) and $\\frac{g_{\\mathrm{oUT}}(x)}{x}$ by $x_{\\mathrm{MID}}^{*}$ and $x_{\\mathrm{oUT}}^{*}$ respectively. ", "page_idx": 4}, {"type": "text", "text": "194 For simplifying notation, in the rest of the work, we use $\\hat{g}_{\\mathrm{MID}}$ for $\\frac{g_{\\mathrm{MID}}(x_{\\mathrm{MID}}^{*})}{x_{\\mathrm{MID}}^{*}}$ ) and g\u02c6OUT for gOUT(\u2217xO\u2217UT). ", "page_idx": 4}, {"type": "text", "text": "195 In the analysis in the rest of the paper, we will see $\\hat{g}_{\\mathrm{MID}}$ and $\\hat{g}_{\\mathrm{OUT}}$ appear many times, so we note these   \n119967 $\\begin{array}{r}{\\hat{g}_{\\mathrm{our}}=\\frac{\\sqrt{2}-1}{2}\\approx0.21}\\end{array}$ .n dW PhQenV $\\theta=4$ ,s $\\hat{g}_{\\mathrm{MID}}\\approx1.42$ e $\\mathrm{PL}$ $\\hat{g}_{\\mathrm{oUT}}\\approx0.06$ $\\theta=2$ ,e ng\u02c6 $\\begin{array}{r}{\\hat{g}_{\\mathrm{MID}}=\\frac{\\sqrt{2}+1}{2}\\approx1.21}\\end{array}$ $\\theta\\rightarrow\\infty$ ,2 $\\hat{g}_{\\mathrm{MID}}\\rightarrow2$ aanndd   \n198 $\\hat{g}_{\\mathrm{oUT}}\\rightarrow0$ . This limit is where PL resembles deterministic voting. ", "page_idx": 4}, {"type": "text", "text": "199 For PQV with $\\lambda=1$ , $\\hat{g}_{\\mathrm{MID}}\\approx1.25$ and $\\hat{g}_{\\mathrm{oUT}}=0.18$ . When $\\lambda\\rightarrow\\infty$ , $\\hat{g}_{\\mathrm{MID}}\\rightarrow2$ and $\\hat{g}_{\\mathrm{oUT}}\\rightarrow0$ . ", "page_idx": 4}, {"type": "text", "text": "200 3 Distortion of Plurality Rule Under Probabilistic Voting ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "201 In this section, we give upper and lower bounds on the distortion of the Plurality rule [59] (PLU).In   \n202 the limit the number of voters $n\\to\\infty$ (\u201clarge election\"), our upper and lower bounds match and are   \n203 linear in the number of candidates $m$ . Let $B$ represent the candidate that minimizes the social cost   \n204 (referred to as \u2018best\u2019), and let $\\{A_{j}\\}_{j\\in[m-1]}$ denote the set of other candidates. ", "page_idx": 5}, {"type": "text", "text": "205 3.1 Upper bound on the distortion of Plurarity(PLU) ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "206 Theorem 1. For every $\\epsilon>0$ and $m\\geq2$ and $n\\geq m^{2}$ we have ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\mathrm{DIST}^{(g)}\\big(\\mathrm{PLU},n,m)\\leq m(m-1)\\,\\big(\\hat{g}_{\\mathrm{MID}}+\\hat{g}_{\\mathrm{ovT}}\\big)\\exp\\Big(\\displaystyle\\frac{-n^{(\\frac{1}{2}+\\epsilon)}+2m}{(2n^{(\\frac{1}{2}-\\epsilon)}-1)m}\\Big)}\\\\ &{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!+\\operatorname*{max}\\bigg(\\displaystyle\\frac{m\\hat{g}_{\\mathrm{MID}}}{(1-n^{-(\\frac{1}{2}-\\epsilon)})}-1,\\displaystyle\\frac{m\\hat{g}_{\\mathrm{our}}}{(1-n^{-(\\frac{1}{2}-\\epsilon)})}+1\\bigg).}\\\\ &{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "207 Further, nli ", "page_idx": 5}, {"type": "text", "text": "208 To prove this theorem, we first give a lemma which upper bounds $\\frac{S C(W,d)}{S C(B,d)}$ under the constraint   \n209 that the expected number of voters that rank candidate $W$ over $B$ is given by $\\alpha$ . This ratio will be   \n210 useful to bound the contribution of non-optimal candidate $W$ to the distortion of PLU. We state an   \n211 optimization problem (7) below, which would be required to bound the ratio as a function of $\\alpha$ . ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{E}_{\\alpha}=\\left\\{\\begin{array}{l l}{\\displaystyle\\operatorname*{min}_{\\mathbf{b},\\mathbf{w}\\in\\mathbb{R}_{\\geq0}^{n}}\\frac{\\sum_{i=1}^{n}b_{i}}{\\sum_{i=1}^{n}w_{i}}}\\\\ {\\mathrm{s.t.~}\\displaystyle\\sum_{i=1}^{n}g\\left(\\frac{b_{i}}{w_{i}}\\right)\\geq\\alpha}&{\\forall\\alpha\\geq0}\\\\ &{\\displaystyle\\operatorname*{max}_{i}\\left|w_{i}-b_{i}\\right|\\leq\\operatorname*{min}_{i}(w_{i}+b_{i})}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "212 Lemma 2. For any two candidates $W,B\\in A$ which satisfy $\\sum_{i=1}^{n}\\mathbb{P}[W\\succ_{i}B]=\\alpha,$ , we have ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{S C(W,d)}{S C(B,d)}\\leq\\frac{1}{o p t(\\mathcal{E}_{\\alpha})}\\leq\\operatorname*{max}\\left(\\frac{n}{\\alpha}\\hat{g}_{\\mathrm{MID}}-1,\\frac{n}{\\alpha}\\hat{g}_{\\mathrm{oUT}}+1\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "213 Our proof is via Lemmas 3 and 4. Lemma 3 shows that we can bound the ratio of social costs by the   \n214 inverse of the optimum value of $\\mathcal{E}_{\\alpha}$ and Lemma 4 gives a lower bound on the optimum value of $\\mathcal{E}_{\\alpha}$ .   \n215 Lemma 3. For any two candidates $W,B\\in A$ satisfying $\\textstyle\\sum_{i=1}^{n}\\mathbb{P}[W\\succ_{i}B]=\\alpha,$ , we have ", "page_idx": 5}, {"type": "equation", "text": "$$\n{\\frac{S C(W,d)}{S C(B,d)}}\\leq{\\frac{1}{o p t({\\mathcal{E}}_{\\alpha})}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "216 Proof. $b_{i}$ and $w_{i}$ in (7) represent the distances $d(i,B)$ and $d(i,W)$ . The last constraint is the triangle   \n217 inequality i.e. $|d(i,B)-\\dot{d}(i,W)|\\le d(B,W)\\le|d(i,B)+d(i,W)|$ for every voter $i\\in\\mathcal N$ . \u53e3 ", "page_idx": 5}, {"type": "text", "text": "218 Consider the following linearized version of (7). ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{E}_{\\mu,\\alpha}=\\left\\{\\begin{array}{l l}{\\displaystyle\\operatorname*{min}_{\\mathbf{v},\\mathbf{b}\\in\\mathbb{R}_{\\geq0}^{n}}\\left(\\sum_{i=1}^{n}b_{i}\\right)-\\mu\\left(\\sum_{i=1}^{n}w_{i}\\right)}\\\\ {\\mathrm{~s.t.~}\\displaystyle\\sum_{i=1}^{n}g\\left(\\frac{b_{i}}{w_{i}}\\right)\\geq\\alpha}&{\\forall0\\leq\\mu\\leq1,\\alpha\\geq0.}\\\\ {\\displaystyle\\left|b_{i}-w_{i}\\right|\\leq1\\forall i\\in[n]}\\\\ {\\displaystyle b_{i}+w_{i}\\geq1\\forall i\\in[n]}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "219 Lemma 4. $\\begin{array}{r}{o p t(\\mathcal{E}_{\\alpha})\\geq\\operatorname*{min}\\left(\\left(\\frac{n}{\\alpha}\\widehat{g}_{\\mathrm{MID}}-1\\right)^{-1},\\left(\\frac{n}{\\alpha}\\widehat{g}_{\\mathrm{OUT}}+1\\right)^{-1}\\right).}\\end{array}$ ", "page_idx": 6}, {"type": "text", "text": "220 Our proof uses Lemma 5 and is by solving a linearized version of (7) in (10). This is done by   \n221 introducing an extra non-negative parameter $\\mu\\leq1$ . Note that it is sufficient to consider $\\mu\\leq1$ since   \n222 $\\mathrm{opt}(\\mathcal{E}_{\\alpha})\\leq1$ because $B$ minimises the social cost by definition. We find the smallest $\\mu\\in(0,1)$ such   \n223 that its objective is non-negative. ", "page_idx": 6}, {"type": "text", "text": "224 Lemma 5. If op $\\eta(\\mathcal{E}_{\\mu,\\alpha})\\geq0,$ , then op $t(\\mathcal{E}_{\\alpha})\\geq\\mu$ . ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{i f~\\mu=\\operatorname*{min}\\Big(\\Big(\\frac{n}{\\alpha}\\hat{g}_{\\mathrm{MID}}-1\\big)^{-1}\\,,\\big(\\frac{n}{\\alpha}\\hat{g}_{\\mathrm{OUT}}+1\\big)^{-1}\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "226 The first part follows since scaling each term by a constant $r$ satisfies the constraints and also yields the   \n227 same objective. And thus we may replace the constraints by $\\mathrm{max}_{i}$ $|w_{i}-b_{i}|\\leq1$ and $\\operatorname*{min}_{i}(w_{i}\\!+\\!b_{i})\\geq1$   \n228 in equation (10). Further, the objective function is linearized as $\\begin{array}{r}{\\left(\\sum_{i=1}^{n}{\\dot{b}}_{i}\\right)-\\mu\\left(\\sum_{i=1}^{n}{\\dot{w}}_{i}\\right)}\\end{array}$ .   \n229 The proof of the second part is technical and has been moved to Appendix B. It involves introducing   \n230 a Lagrangian multiplier $\\lambda$ and demonstrating that the objective function is non-negative for a suitably   \n231 chosen $\\lambda$ . To establish this, we show that minimising the Lagrangian over the boundaries of the   \n232 constraint set given by $\\left|b_{i}-w_{i}\\right|=1$ and $b_{i}+w_{i}=1$ is sufficient. This requires a careful analysis.   \n233 The main technique used in proving Theorem 1 involves considering two cases for every non-optimal   \n234 candidate $A_{j}$ : one where the expected number of voters ranking candidate $A_{j}$ above $B$ (call it $\\alpha_{j}$ )   \n235 exceeds a threshold of nm \u2212n\u03f5+m1/2 and one where it does not. In the first case, the ratio of social costs   \n236 of $A_{j}$ and $B$ is bounded using Lemma 2 that naturally gives a bound on contribution of candidate $A_{j}$   \n237 to the distortion. In the later case, we use Chernoff bound to bound the probability of $A_{j}$ being the   \n238 winner and multiply it with the ratio of social costs of $A_{j}$ and $B$ to bound the distortion. The proof of   \n239 Theorem 1 is in Appendix C. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "240 3.2 Lower bound on the distortion of Plurality ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "241 We now present a lower bound on the distortion of PLU for any $m$ in the limit $n$ tends to infinity. This   \n242 lower bound matches the upper bound of Theorem 1 in the limit. A full proof is in Appendix D. Note   \n243 that the proof has an adversarially chosen distribution over the rankings subject to the marginals on   \n244 pairwise relationships satisfying $g$ (as in the definition of distortion under probabilistic voting 4).   \n245 This lower bound does not apply to the PL model, which has a specific distribution over rankings. ", "page_idx": 6}, {"type": "text", "text": "246 Theorem 2. For every $m\\geq2,\\,\\,\\,\\operatorname*{lim}_{n\\to\\infty}\\mathrm{DIST}^{(g)}(\\mathrm{PLU},n,m)\\geq\\operatorname*{max}\\left(m\\hat{g}_{\\mathrm{MD}}-1,m\\hat{g}_{\\mathrm{oUT}}+1\\right).$ ", "page_idx": 6}, {"type": "text", "text": "247 Proof Sketch. The proof is by an example in an Euclidean metric space in $\\mathbb{R}^{3}$ . One candidate ${}^{\\bullet}\\mathbf{C}^{\"}$ is   \n248 at $(1,0,0)$ . The other $m-1$ candidates are \u201cgood\" and are equidistantly placed on a circle of radius   \n249 $\\epsilon$ on the $y-z$ plane centred at $(0,0,0)$ . We call them $\\mathcal{G}:=\\left\\lbrace\\bar{G}_{1},G_{2},\\bar{.}\\bar{.}\\bar{.},G_{m-1}\\right\\rbrace$ . ", "page_idx": 6}, {"type": "text", "text": "250 We present sketches of two constructions below for every $\\epsilon,\\zeta>0$ . ", "page_idx": 6}, {"type": "text", "text": "Construction $^{\\,l}$ : Let $\\begin{array}{r}{q_{\\mathrm{MID}}\\;:=\\;g\\Big(\\frac{\\sqrt{(x_{\\mathrm{MID}}^{*})^{2}+\\epsilon^{2}}}{1-x_{\\mathrm{MID}}^{*}}\\Big)}\\end{array}$ and $\\begin{array}{r}{a_{\\mathrm{MID}}\\,:=\\,\\frac{1}{m-1}\\Bigl(1-\\frac{1+\\zeta}{m q_{\\mathrm{MID}}}\\Bigr)}\\end{array}$ Each of the $m-$ ", "page_idx": 6}, {"type": "text", "text": "\u00b7   \n252 1 candidates in $\\mathcal{G}$ has $\\lfloor a_{\\mathrm{MID}}n\\rfloor$ voters overlapping with it. The remaining voters (we call them   \n253 \u201cambivalent\u201d) are placed at $(x_{\\mathrm{MID}}^{*},0,0)$ . Clearly, each voter overlapping with a candidate votes for it   \n254 as the most preferred candidate with probability one. Each of the ambivalent voters votes as follows.   \n255 \u2013 With probability $q_{\\mathrm{MID}}$ , vote for candidate $C$ as the top choice and uniformly randomly permute the   \n256 other candidates in the rest of the vote.   \n257 \u2013 With probability $1-q_{\\mathrm{MID}}$ , vote for candidate $C$ as the last choice and uniformly randomly permute   \n258 the other candidates in the rest of the vote.   \n259 We show that the probability that $C$ wins tends to 1 as $n\\to\\infty$ and the distortion is $m\\hat{g}_{\\mathrm{MID}}-1$ .   \n260 Construction 2: We give a construction where the locations of the candidates are identical as in   \n261 Construction 1, and some voters are located with the \u201cgood\" candidates. The ambivalent voters are at   \n262 $(-x_{\\mathrm{oUT}}^{*},0,0)$ . We show that $\\mathbb{P}[C$ wins] tends to 1 as $n\\to\\infty$ and the distortion is $m\\hat{g}_{\\mathrm{oUT}}+1$ \uff1a\u53e3   \n263 This result establishes that the distortion of Plurality is bound to increase linearly with $m$ even under   \n264 probabilistic voting, and is therefore not a good choice when $m$ is even moderately large. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "265 4 Distortion of Copeland Rule Under Probabilistic Voting ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "266 We now bound the distortion of the Copeland voting rule. We say that candidate $W$ defeats candidate   \n267 $Y$ if more than half of the voters rank $W$ above $Y$ . ", "page_idx": 7}, {"type": "text", "text": "268 Theorem 3. For every $\\epsilon>0,m\\geq2$ and $n\\geq4$ , we have ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathsf{D I S T}^{(g)}(\\mathrm{COP},n,m)\\leq4m(m-1)\\exp\\Big(\\displaystyle\\frac{-n^{(\\frac{1}{2}+\\epsilon)}+8}{2(2n^{(\\frac{1}{2}-\\epsilon)}-1)}\\Big)\\,\\big(\\hat{g}_{\\mathrm{MID}}+\\hat{g}_{\\mathrm{OUT}}\\big)^{2}\\,}\\\\ {+\\operatorname*{max}\\Big(\\Big(\\displaystyle\\frac{2\\hat{g}_{\\mathrm{MID}}}{1-n^{-(\\frac{1}{2}-\\epsilon)}}-1\\Big)^{2},\\Big(\\displaystyle\\frac{2\\hat{g}_{\\mathrm{OUT}}}{1-n^{-(\\frac{1}{2}-\\epsilon)}}+1\\Big)^{2}\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "269 For every $m\\geq2$ , we have $\\operatorname*{lim}_{n\\to\\infty}\\mathrm{DIST}^{(g)}(\\mathrm{COP},n,m)\\leq\\operatorname*{max}\\left(\\left(2{\\widehat{g}}_{\\mathrm{MID}}-1\\right)^{2},\\left(2{\\widehat{g}}_{\\mathrm{OUT}}+1\\right)^{2}\\right)$ . ", "page_idx": 7}, {"type": "text", "text": "270 Proof Sketch. A Copeland winner belongs to the uncovered set in the tournament graph, as   \n271 demonstrated in [1, Theorem 15]. Recall that $B$ denotes the candidate with the least social cost. For   \n272 a Copeland winner $W$ , either $W$ defeats $B$ or it defeats a candidate $Y$ who defeats $B$ . ", "page_idx": 7}, {"type": "text", "text": "273 We now consider two exhaustive cases on candidate $A_{j}$ and define event $E_{j}$ for every $j\\in[m-1]$ 274 by computing the expected fraction of votes on pairwise comparisons. The event $E_{j}$ denotes the 275 existence of an at-most two hop directed path from a candidate $A_{j}$ to candidate $B$ for Copeland such 276 that the expected fraction of votes on all edges along that path exceed n2 \u2212n(1/22+\u03f5). ", "page_idx": 7}, {"type": "text", "text": "277 If $E_{j}$ holds true, we upper bound the ratio of social cost of candidate $A_{j}$ and social cost of candidate   \n278 $B$ using Lemma 2 which in-turn would give a bound on the distortion. Otherwise, we use union   \n279 bound and Chernoff\u2019s bound to upper bound the probability of $A_{j}$ being the winner. Multiplying the   \n280 probability bound with the ratio of social costs (one obtained from Lemma 2) leads to a bound on the   \n281 distortion. A detailed proof is in Appendix E. \u53e3 ", "page_idx": 7}, {"type": "text", "text": "282 5 Distortion of Random Dictator Rule Under Probabilistic Voting ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "283 We first give an upper bound on the distortion of RD; the proof is in Appendix F. ", "page_idx": 7}, {"type": "text", "text": "284 Theorem 4. DIS $\\begin{array}{r}{\\cdot(g)}\\\\ {\\cdot(R D,m,n)\\leq(m-1)\\hat{g}_{\\mathrm{MID}}+1.}\\end{array}$ . ", "page_idx": 7}, {"type": "text", "text": "285 We now give a lower bound on the distortion of RD. We do this by constructing an example. ", "page_idx": 7}, {"type": "text", "text": "286 Theorem 5. For $m\\geq3$ and $n\\geq2$ , $\\begin{array}{r}{\\mathrm{DIST}^{(g)}(R D,m,n)\\ge2+\\frac{1}{g^{-1}(\\frac{1}{m-1})}-\\frac{2}{n}.}\\end{array}$ ", "page_idx": 7}, {"type": "text", "text": "287 Proof. We have a 1-D Euclidean construction. Let $B$ be at 0 and all other candidates $A\\setminus\\{B\\}$ be at   \n288 1. $m-1$ voters are at 0 and one voter $V$ is at $\\begin{array}{r}{\\tilde{x}=g^{-1}(\\frac{1}{m-1})/(1+g^{-1}(\\frac{1}{m-1}))}\\end{array}$ .   \n289 The ranking for $V$ is generated as follows: pick a candidate from $A\\setminus\\{B\\}$ as the top rank uniformly   \n290 at random. Keep $B$ on the second rank. Permute the remaining candidates uniformly at random for   \n229912 tdhiset arencmea ionf $V$ gf rroanmk $B$ aOnbds eeravceh  tchaant dtihdea tem ianr $|A\\backslash\\{\\bar{B}\\}$ .r wIni spea rotridceurl apr $\\begin{array}{r}{g(\\frac{\\tilde{x}}{1-\\tilde{x}})=\\frac{1}{m-1}}\\end{array}$ . oTnhsies tdeisntto rwtiitohn  tfhoer   \n293 this instance is $\\mathbb{P}[B$ wins] $1\\!+\\!\\mathbb{P}[B$ loses] $\\begin{array}{r}{\\frac{n-\\tilde{x}}{\\tilde{x}}=\\frac{n-1}{n}+\\frac{1}{n}\\frac{n-\\tilde{x}}{\\tilde{x}}=1+\\frac{1}{\\tilde{x}}-\\frac{2}{n}=2+\\frac{1}{g^{-1}(\\frac{1}{m-1})}-\\frac{2}{n}}\\end{array}$ .   \n294 For $\\begin{array}{r}{g(r)=\\frac{r^{\\theta}}{1+r^{\\theta}}}\\end{array}$ , we have $\\begin{array}{r}{g^{-1}(t)=(\\frac{t}{1-t})^{\\frac{1}{\\theta}}}\\end{array}$ . Then $g^{-1}(\\frac{1}{m-1})=(m{-}2)^{-\\frac{1}{\\theta}}$ , and the distortion lower   \n295 bound is ${\\mathrm{DIST}}^{(g)}({\\mathrm{RD}},m,n)\\geq2\\!+\\!(m-2)^{{\\frac{1}{\\theta}}}-{\\frac{2}{n}}$ , and $\\begin{array}{r}{\\operatorname*{lim}_{n\\to\\infty}\\mathrm{DIST}^{(g)}(\\mathrm{RD},m,n)\\geq2\\!+\\!\\left(m-2\\right)^{\\frac{1}{\\theta}}}\\end{array}$ .   \n296 However, note that this result does not apply to the PL model! This is because the PL model has   \n297 a specific distribution on the rankings. In contrast, the above result is obtained by choosing an   \n298 adversarial distribution on rankings subject to the constraint that its marginals on pairwise relations   \n299 are given by $g$ . In the $\\mathrm{PL}$ model, $\\mathbb{P}[A_{j}$ is top-ranked in $\\begin{array}{r}{\\sigma_{i}]=\\frac{d(i,A_{j})^{-\\theta}}{\\sum_{A_{k}\\in\\mathcal{A}}d(i,A_{k})^{-\\theta}}}\\end{array}$ [45]. We have the   \n300 following result for the PL model. A proof via a similar construction as Theorem 5 is in Appendix G.   \n301 Theorem 6. Let DI $\\mathrm{ST}_{P L}^{\\theta}(R D,m,n)$ denote the distortion when the voters\u2019 rankings are generated   \n302 per the PL model with parameter \u03b8. We have limn\u2192\u221eDIST\u03b8P L(RD, m, n) \u22651 + (m\u221221)1/\u03b8.   \n304 Recall that higher values of $\\theta$ and $\\lambda$ correspond to lower randomness. From Figure 2, we observe that   \n305 under sufficient randomness, the more intricate voting rule Copeland outshines the simpler rule RD,   \n306 which only looks at a voter\u2019s top choice. Moreover, its distortion is independent of $m$ in the limit   \n307 $n\\to\\infty$ . This is in sharp contrast to RD, where the distortion is $\\Omega(m^{1/\\theta})$ in the PL model, a sharp   \n308 rate of increase in $m$ for low values of $\\theta$ . The distortion of Plurality increases linearly in $m$ .   \n309 An important observation is regarding the asymptotics when $\\theta$ or $\\lambda$ increases. The distortion of RD   \n310 converges to its value under deterministic voting, i.e., 3. The distortion of Plurality also converges to   \n311 $2m-1$ , the same as in deterministic voting. Since our bound on Copeland is not tight, it converges   \n312 to 9 rather than 5. So far, in the study of metric distortion, the social choice community has looked   \n313 only at these asymptotic; here, we present insights available from looking at the \u2018complete\u2019 picture.   \n314 Interestingly, the distortion of RD increases with randomness, whereas that of Copeland decreases   \n315 up to a certain point and then increases again. The reason for the increases in the high randomness   \n316 regime is that the votes become too noisy to reveal the best candidate any more.   \n317 Since these plots have no abrupt transitions, this figure hints that smoothened analysis [52] (typically   \n318 done with small amounts of noise) is unlikely to give any new insights regarding metric distortion. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "image", "img_path": "nEvnCuviMO/tmp/e93c9ef60531b227e3aba77d8fc6cfab4db8d44c0b174ab37a2864dbfbc7cf1a.jpg", "img_caption": ["Figure 2: Here, we illustrate how the distortion bounds on different voting rules vary with $m$ and with the randomness parameters of the two models, PL and PQV, in the limit $n\\to\\infty$ . Both the $\\mathbf{X}$ and y axes are on the log scale. We plot the upper bound for Copeland (Theorem 3), the lower bound for RD (Theorem 5), and the matching bounds for Plurality (Theorem 1). "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "319 7 Discussion and Future Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "320 We extend the metric distortion framework in social choice in an important way \u2013 by capturing the   \n321 bounded rationality and randomness in voters\u2019 behaviour. Consideration of this randomness shows   \n322 that, in general, the original metric distortion framework is too pessimistic on important voting rules,   \n323 most notably on Copeland. On the other hand, the simplistic voting rule Random Dictator, which   \n324 attains a distortion of 3 (at least as good as any deterministic rule [1]), is not so good when we look at   \n325 the full picture \u2013 its distortion increases with the number of candidates in our model. Our framework   \n326 opens up opportunities to revisit the metric distortion problem with a closer-to-reality view of voters.   \n327 It may hopefully lead to the development of new voting rules that consider the randomness of voters\u2019   \n328 behaviour. For example, Liu and Moitra [46] take a learning theory approach to design voting rules   \n329 under the assumption of random voting per the Mallows model. However, technical analysis in our   \n330 framework may be challenging because of the interplay of the geometric structure of voters\u2019 positions   \n331 and the probabilistic nature of their votes.   \n332 Future Work An interesting extension would be to other tournament graph-based voting rules   \n333 (weighted or unweighted). Our techniques are well-suited for this class of rules since it is based on   \n334 the expected weights of the edges of the tournament graph. Closing the gap for the distortion of   \n335 Copeland would be useful for getting deeper insights. Another open problem is the characterization   \n336 of the set of distributions on rankings that induce the pairwise probabilities per PQV. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "337 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "338 [1] Elliot Anshelevich, Onkar Bhardwaj, and John Postl. Approximating optimal social choice   \n339 under metric preferences. In Proceedings of the Twenty-Ninth AAAI Conference on Artificial   \n340 Intelligence, pages 777\u2013783, 2015.   \n341 [2] Jessica Dai and Eve Fleisig. Mapping social choice theory to RLHF. arXiv preprint   \n342 arXiv:2404.13038, 2024.   \n343 [3] Vincent Conitzer, Rachel Freedman, Jobst Heitzig, Wesley H Holliday, Bob M Jacobs, Nathan   \n344 Lambert, Milan Moss\u00e9, Eric Pacuit, Stuart Russell, Hailey Schoelkopf, et al. Social choice for   \n345 AI alignment: Dealing with diverse human feedback. arXiv preprint arXiv:2404.10271, 2024.   \n346 [4] Seth D Baum. Social choice ethics in artificial intelligence. AI & Society, 35(1):165\u2013176, 2020.   \n347 [5] Jessie Finocchiaro, Roland Maio, Faidra Monachou, Gourab K Patro, Manish Raghavan, Ana  \n348 Andreea Stoica, and Stratis Tsirtsis. Bridging machine learning and mechanism design towards   \n349 algorithmic fairness. In Proceedings of the 2021 ACM Conference on Fairness, Accountability,   \n350 and Transparency, pages 489\u2013503, 2021.   \n351 [6] Francesca Rossi, Kristen Brent Venable, and Toby Walsh. A Short Introduction to Preferences:   \n352 Between AI and Social Choice. Morgan & Claypool Publishers, 2011.   \n353 [7] Meltem \u00d6zt\u00fcrk, Alexis Tsouki\u00e0s, and Philippe Vincke. Preference modelling. Multiple criteria   \n354 decision analysis: State of the art surveys, 78:27\u201359, 2005.   \n355 [8] Kenneth J Arrow. A difficulty in the concept of social welfare. Journal of political economy, 58   \n356 (4):328\u2013346, 1950.   \n357 [9] Amartya Sen. Social choice theory. Handbook of mathematical economics, 3:1073\u20131181, 1986.   \n358 [10] Kenneth J Arrow, Amartya Sen, and Kotaro Suzumura. Handbook of social choice and welfare,   \n359 volume 2. Elsevier, 2010.   \n360 [11] Felix Brandt, Vincent Conitzer, Ulle Endriss, J\u00e9r\u00f4me Lang, and Ariel D Procaccia. Handbook   \n361 of computational social choice. Cambridge University Press, 2016.   \n362 [12] Ariel D Procaccia and Jeffrey S Rosenschein. The distortion of cardinal preferences in voting.   \n363 In International Workshop on Cooperative Information Agents, pages 317\u2013331. Springer, 2006.   \n364 [13] James M Enelow and Melvin J Hinich. The spatial theory of voting: An introduction. CUP   \n365 Archive, 1984.   \n366 [14] Samuel Merrill and Bernard Grofman. A unified theory of voting: Directional and proximity   \n367 spatial models. Cambridge University Press, 1999.   \n368 [15] Fatih Erdem Kizilkaya and David Kempe. Plurality veto: A simple voting rule achieving   \n369 optimal metric distortion. Proceedings of the 31st International Joint Conference on Artificial   \n370 Intelligence (IJCAI), pages 349\u2013355, 2022.   \n371 [16] Germain Kreweras. Aggregation of preference orderings. In Mathematics and Social Sciences I:   \n372 Proceedings of the seminars of Menthon-Saint-Bernard, France (1\u201327 July 1960) and of G\u00f6sing,   \n373 Austria (3\u201327 July 1962), pages 73\u201379, 1965.   \n374 [17] Moses Charikar, Prasanna Ramakrishnan, Kangning Wang, and Hongxun Wu. Breaking the   \n375 metric voting distortion barrier. In Proceedings of the 2024 Annual ACM-SIAM Symposium on   \n376 Discrete Algorithms (SODA), pages 1621\u20131640. SIAM, 2024.   \n377 [18] Peter J Coughlin. Probabilistic voting theory. Cambridge University Press, 1992.   \n378 [19] Kevin M Quinn, Andrew D Martin, and Andrew B Whitford. Voter choice in multi-party   \n379 democracies: a test of competing theories and models. American Journal of Political Science,   \n380 pages 1231\u20131247, 1999.   \n381 [20] Richard D McKelvey and John W Patty. A theory of voting in large elections. Games and   \n382 Economic Behavior, 57(1):155\u2013180, 2006.   \n383 [21] Thomas Pfeiffer, Xi Gao, Yiling Chen, Andrew Mao, and David Rand. Adaptive polling for   \n384 information aggregation. In Proceedings of the AAAI conference on artificial intelligence,   \n385 volume 26, pages 122\u2013128, 2012.   \n386 [22] David C Parkes, Houssein Azari Soufiani, and Lirong Xia. Random utility theory for social   \n387 choice. In Proceeedings of the 25th Annual Conference on Neural Information Processing   \n388 Systems. Curran Associates, Inc., 2012.   \n389 [23] Hossein Azari Soufiani, David C Parkes, and Lirong Xia. Preference elicitation for general   \n390 random utility models. In Proceedings of the Twenty-Ninth Conference on Uncertainty in   \n391 Artificial Intelligence, pages 596\u2013605, 2013.   \n392 [24] Stephen P Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press,   \n393 2004.   \n394 [25] Moses Charikar and Prasanna Ramakrishnan. Metric distortion bounds for randomized social   \n395 choice. In Proceedings of the 2022 Annual ACM-SIAM Symposium on Discrete Algorithms   \n396 (SODA), pages 2986\u20133004. SIAM, 2022.   \n397 [26] Elliot Anshelevich, Aris Filos-Ratsikas, Nisarg Shah, and Alexandros A Voudouris. Distortion   \n398 in social choice problems: The first 15 years and beyond. In 30th International Joint Conference   \n399 on Artificial Intelligence, pages 4294\u20134301, 2021.   \n400 [27] Ben Abramowitz, Elliot Anshelevich, and Wennan Zhu. Awareness of voter passion greatly   \n401 improves the distortion of metric social choice. In International Conference on Web and Internet   \n402 Economics, pages 3\u201316. Springer, 2019.   \n403 [28] Georgios Amanatidis, Georgios Birmpas, Aris Filos-Ratsikas, and Alexandros A Voudouris.   \n404 Peeking behind the ordinal curtain: Improving distortion via cardinal queries. Artificial   \n405 Intelligence, 296:103488, 2021.   \n406 [29] Elliot Anshelevich, Aris Filos-Ratsikas, Christopher Jerrett, and Alexandros A Voudouris.   \n407 Improved metric distortion via threshold approvals. In Proceedings of the AAAI Conference on   \n408 Artificial Intelligence, volume 38, pages 9460\u20139468, 2024.   \n409 [30] Melvin J Hinich. Equilibrium in spatial voting: The median voter result is an artifact. Journal   \n410 of Economic Theory, 16(2):208\u2013219, 1977.   \n411 [31] Duncan Black. On the rationale of group decision-making. Journal of political economy, 56(1):   \n412 23\u201334, 1948.   \n413 [32] Jeffrey S Banks and John Duggan. Probabilistic voting in the spatial model of elections: The   \n414 theory of office-motivated candidates. In Social Choice and Strategic Decisions: Essays in   \n415 Honor of Jeffrey S. Banks, pages 15\u201356. Springer, 2005.   \n416 [33] John Wiggs Patty. Local equilibrium equivalence in probabilistic voting models. Games and   \n417 Economic Behavior, 51(2):523\u2013536, 2005.   \n418 [34] Peter Coughlin and Shmuel Nitzan. Electoral outcomes with probabilistic voting and nash   \n419 social welfare maxima. Journal of Public Economics, 15(1):113\u2013121, 1981.   \n420 [35] Peter Coughlin and Shmuel Nitzan. Directional and local electoral equilibria with probabilistic   \n421 voting. Journal of Economic Theory, 24(2):226\u2013239, 1981.   \n422 [36] Lirong Xia. Designing social choice mechanisms using machine learning. In Proceedings of the   \n423 international conference on Autonomous agents and multi-agent systems, pages 471\u2013474, 2013.   \n424 [37] Robin L Plackett. The analysis of permutations. Journal of the Royal Statistical Society Series   \n425 C: Applied Statistics, 24(2):193\u2013202, 1975.   \n426 [38] R Duncan Luce. Individual choice behavior: A theoretical analysis. Courier Corporation, 2005.   \n427 [39] Isobel Claire Gormley and Thomas Brendan Murphy. Analysis of Irish third-level college   \n428 applications data. Journal of the Royal Statistical Society Series A: Statistics in Society, 169(2):   \n429 361\u2013379, 2006.   \n430 [40] Hossein Azari, David Parks, and Lirong Xia. Random utility theory for social choice. Advances   \n431 in Neural Information Processing Systems, 25, 2012.   \n432 [41] Isobel Claire Gormley and Thomas Brendan Murphy. A grade of membership model for rank   \n433 data. Bayesian Analysis, 1(1):1\u201332, 2004.   \n434 [42] Ralph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. The   \n435 method of paired comparisons. Biometrika, 39(3/4):324\u2013345, 1952.   \n436 [43] Colin L Mallows. Non-null ranking models. i. Biometrika, 44(1/2):114\u2013130, 1957.   \n437 [44] Marquis de Condorcet. Essay on the application of analysis to the probability of majority   \n438 decisions. Paris: Imprimerie Royale, page 1785, 1785.   \n439 [45] Ioannis Caragiannis, Ariel D Procaccia, and Nisarg Shah. When do noisy votes reveal the truth?   \n440 ACM Transactions on Economics and Computation (TEAC), 4(3):1\u201330, 2016.   \n441 [46] Allen Liu and Ankur Moitra. Robust voting rules from algorithmic robust statistics. In   \n442 Proceedings of the Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages   \n443 3471\u20133512. SIAM, 2023.   \n444 [47] John I Marden. Analyzing and modeling rank data. CRC Press, 1996.   \n445 [48] Douglas E Critchlow, Michael A Fligner, and Joseph S Verducci. Probability models on rankings.   \n446 Journal of mathematical psychology, 35(3):294\u2013318, 1991.   \n447 [49] Daniel A Spielman and Shang-Hua Teng. Smoothed analysis of algorithms: Why the simplex   \n448 algorithm usually takes polynomial time. Journal of the ACM (JACM), 51(3):385\u2013463, 2004.   \n449 [50] Dorothea Baumeister, Tobias Hogrebe, and J\u00f6rg Rothe. Towards reality: smoothed analysis   \n450 in computational social choice. In Proceedings of the 19th International Conference on   \n451 Autonomous Agents and Multiagent Systems, pages 1691\u20131695, 2020.   \n452 [51] Bailey Flanigan, Daniel Halpern, and Alexandros Psomas. Smoothed analysis of social choice   \n453 revisited. In International Conference on Web and Internet Economics, pages 290\u2013309. Springer,   \n454 2023.   \n455 [52] Lirong Xia. The smoothed possibility of social choice. Advances in Neural Information   \n456 Processing Systems, 33:11044\u201311055, 2020.   \n457 [53] Lirong Xia. Semi-random impossibilities of condorcet criterion. In Proceedings of the AAAI   \n458 Conference on Artificial Intelligence, volume 37, pages 5867\u20135875, 2023.   \n459 [54] Ao Liu and Lirong Xia. The semi-random likelihood of doctrinal paradoxes. In Proceedings of   \n460 the AAAI Conference on Artificial Intelligence, volume 36, pages 5124\u20135132, 2022.   \n461 [55] Lirong Xia and Weiqiang Zheng. The smoothed complexity of computing kemeny and slater   \n462 rankings. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages   \n463 5742\u20135750, 2021.   \n464 [56] Lirong Xia and Weiqiang Zheng. Beyond the worst case: Semi-random complexity analysis   \n465 of winner determination. In International Conference on Web and Internet Economics, pages   \n466 330\u2013347. Springer, 2022.   \n467 [57] R\u00f3bert Busa-Fekete, Eyke H\u00fcllermeier, and Bal\u00e1zs Sz\u00f6r\u00e9nyi. Preference-based rank elicitation   \n468 using statistical models: The case of mallows. In International conference on machine learning,   \n469 pages 1071\u20131079. PMLR, 2014.   \n470 [58] Richard D McKelvey and Thomas R Palfrey. Quantal response equilibria for normal form   \n471 games. Games and economic behavior, 10(1):6\u201338, 1995.   \n472 [59] Kenneth J. Arrow. Social Choice and Individual Values. Yale University Press, New Haven, 2   \n473 edition, 1963.   \n474 [60] John Canny. Chernoff bounds. URL https://people.eecs.berkeley.edu/\\~jfc/cs174/   \n475 lecs/lec10/lec10.pdf. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "476 A Proof of Lemma 1 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "7 Lemma (Restatement of Lemma 1). $\\frac{g_{\\mathrm{MID}}(x)}{x}$ and $\\frac{g_{\\mathrm{oUT}}(x)}{x}$ have unique local maxima in $(0,1)$ and $(0,\\infty)$ respectively. ", "page_idx": 12}, {"type": "text", "text": "To prove Lemma 1, we first state and prove Lemma 6 which shows that $g_{\\mathrm{MID}}(x)$ and $g_{\\mathrm{oUT}}(x)$ change from convex to concave in intervals $(0,1)$ and $(0,\\infty)$ respectively. ", "page_idx": 12}, {"type": "text", "text": "Lemma 6. \u2022 There $\\exists c_{1}\\in[0,1]$ s.t. $g_{\\mathrm{MID}}(x)$ is convex in $[0,c_{1}]$ and concave in $[c_{1},1]$ . ", "page_idx": 12}, {"type": "text", "text": "\u2022 There $\\exists c_{2}\\in[0,\\infty)$ s.t. $g_{\\mathrm{oUT}}(x)$ is convex in $[0,c_{2}]$ and concave in $[c_{2},\\infty)$ . ", "page_idx": 12}, {"type": "text", "text": "482 Proof. Observe that $g^{\\prime\\prime}(x)<0$ for $x\\geq1$ . ", "page_idx": 12}, {"type": "text", "text": "Recall that $\\begin{array}{r}{g_{\\mathrm{MID}}(x)=g\\left(\\frac{x}{1-x}\\right)}\\end{array}$ thus, $\\begin{array}{r}{g_{\\mathrm{{MID}}}^{\\prime}(x)=g^{\\prime}\\left(\\frac{x}{1-x}\\right)\\frac{1}{(1-x)^{2}}}\\end{array}$ and $g_{\\mathrm{MID}}(x)+g_{\\mathrm{MID}}(1-x)=1$ Thus, $g_{\\mathrm{MID}}^{\\prime\\prime}(x)=$ $\\begin{array}{r}{g^{\\prime}\\left(\\frac{x}{1-x}\\right)\\frac{2}{(1-x)^{3}}+g^{\\prime\\prime}\\left(\\frac{x}{1-x}\\right)\\frac{1}{(1-x)^{4}}}\\end{array}$ . Observe that $g_{\\mathrm{MID}}^{\\prime\\prime}(0)>0$ which implies $\\begin{array}{r}{\\operatorname*{lim}_{x\\to1}g_{\\mathrm{MD}}^{\\prime\\prime}(x)<0}\\end{array}$ and thus, there must exist a $c\\in(0,1)$ such that $g_{\\mathrm{MID}}^{\\prime\\prime}(c)=0$ . ", "page_idx": 12}, {"type": "text", "text": "Now we show that there cannot exist two distinct $c_{1},c_{2}\\in(0,1)$ such that $g_{\\mathrm{MID}}^{\\prime\\prime}(c_{1})=0$ and $g_{\\mathrm{MID}}^{\\prime\\prime}(c_{2})=0$ . We prove this statement by contradiction assuming the contrary which implies that $g_{\\mathrm{MID}}^{\\prime\\prime}(x)$ must have changed its sign twice. However, since $\\begin{array}{r}{\\dot{g^{\\prime}}\\left(\\frac{x}{1-x}\\right)>0}\\end{array}$ we must have $g^{\\prime\\prime}({\\frac{x}{1-x}})$ changing its sign twice which is a contradiction since $g^{\\prime\\prime}(r)>0$ for $r\\in(0,c)$ and $g^{\\prime\\prime}(r)<0$ for $r\\in(c,\\infty)$ . ", "page_idx": 12}, {"type": "text", "text": "Now consider $\\begin{array}{r}{g_{\\mathrm{oUT}}(x)\\,=\\,g\\left(\\frac{x}{1+x}\\right)}\\end{array}$ we have $\\begin{array}{r}{g_{\\mathrm{{oUT}}}^{\\prime}(x)\\,=\\,g^{\\prime}\\left(\\frac{x}{1+x}\\right)\\frac{1}{(1+x)^{2}}}\\end{array}$ . Thus, $\\begin{array}{r}{g_{\\mathrm{oUT}}^{\\prime\\prime}(x)\\,=\\,-g^{\\prime}\\left({\\frac{x}{1+x}}\\right)\\,{\\frac{2}{(1+x)^{3}}}\\,+}\\end{array}$ $\\begin{array}{r}{g^{\\prime\\prime}\\left(\\frac{x}{1+x}\\right)\\frac{1}{(1+x)^{4}}}\\end{array}$ . Using a similar approach, we can also prove the second point in the Lemma. \u53e3 ", "page_idx": 12}, {"type": "text", "text": "492 Using Lemma 6, we now prove Lemma 1 showing the existence of unique maximas of $\\frac{g_{\\mathrm{MID}}(x)}{x}$ ) and gOUT(x). x x ", "page_idx": 12}, {"type": "text", "text": "493 Proof of Lemma 1. Recall from Lemma 6 that $g_{\\mathrm{MID}}(x)$ is convex in $\\left[0,c_{1}\\right]$ and concave in $[c_{1},1]$ . ", "page_idx": 12}, {"type": "text", "text": "494 Since the first derivative equals zero at every local maxima, we must have $x g_{\\mathrm{MD}}^{\\prime}(x)-g(x)=0$ for any local maxima   \n495 $x$ . We now argue that such a maxima cannot exist in $\\left[0,c_{1}\\right]$ . Suppose such a maxima exists in that case, we must have   \n449967 $\\begin{array}{r}{g_{\\mathrm{MID}}^{\\prime}(x)=\\frac{g_{\\mathrm{MID}}(x)-g_{\\mathrm{MID}}(0)}{x-0}}\\end{array}$ r,  sthoums ei $x\\in(0,c_{1})$ McoVnTtr iand itchtei nign ttehrev afla $[0,x]^{1}$ sits  hstarvicet lsyo imnec $t\\in(0,x)$   \n$\\begin{array}{r}{g_{\\mathrm{MID}}^{\\prime}(t)=\\frac{g_{\\mathrm{MID}}(x)-g_{\\mathrm{MID}}(0)}{x-0}}\\end{array}$ $g_{\\mathrm{MID}}^{\\prime}(x)=g^{\\prime}(t)$ $g_{\\mathrm{MID}}^{\\prime}(r)$   \n498 $[0,c_{1}]$ .   \n9090 sOobmsee $\\begin{array}{r}{g_{\\mathrm{MID}}(t)-t\\frac{g_{\\mathrm{MID}}(c_{1})}{c_{1}}}\\end{array}$ s  izs eirnoc raet $t=0$ i na $t=c_{1}$ ea nmd utsht uhs,a vbey .e  hOabvsee $\\begin{array}{r}{g_{\\mathrm{MID}}^{\\prime}(x)=\\frac{g_{\\mathrm{MID}}(c_{1})}{c_{1}}}\\end{array}$   \n$x\\in(0,c_{1})$ $g_{\\mathrm{MID}}^{\\prime}(x)$ $[0,c_{1}]$ $\\begin{array}{r}{g_{\\mathrm{{MID}}}^{\\prime}(c_{1})>\\frac{g_{\\mathrm{{MID}}}(c_{1})}{c_{1}}}\\end{array}$ $\\begin{array}{r}{\\operatorname*{lim}_{t\\to1}\\frac{g_{\\mathrm{MID}}(t)}{t}=1}\\end{array}$   \n11 and $\\frac{g_{\\mathrm{MID}}(c_{1})}{c_{1}}>1$ . Also, we have $\\left.\\frac{d}{d t}\\left(\\frac{g_{\\mathrm{MID}}(t)}{t}\\right)\\right|_{t=c_{1}}>0$ since $c_{1}g_{\\mathrm{MID}}^{\\prime}(c_{1})>g_{\\mathrm{MID}}(c_{1})$ implying $g_{\\mathrm{MID}}(t)/t$ is increasing   \n02 at $t=c_{1}$ . Thus, $g_{\\mathrm{MID}}(t)/t$ must have at least one local maxima $x^{*}$ in the open interval $(c_{1},\\infty)$ and no local maxima   \n03 elsewhere.   \n504 We now argue that this local maxima $x^{*}$ is unique. Suppose we have two distinct local maximas at $x_{1}$ $_{1},x_{2}\\in(c_{1},\\infty)$   \n505 and thus, we have $x_{1}g_{\\mathrm{{MID}}}^{\\prime}(x_{1})-g_{\\mathrm{{MID}}}(x_{1})=0$ and $x_{2}g_{\\mathrm{MID}}^{\\prime}(x_{2})-g_{\\mathrm{MID}}(x_{2})=0$ . Rolle\u2019s theorem would imply that there   \n506 exists $t\\in(x_{1},x_{2})^{2}$ s.t. $t g_{\\mathrm{MID}}^{\\prime\\prime}(t)=0$ which is a contradiction since $g_{\\mathrm{MID}}^{\\prime\\prime}(x)<0$ in $(c_{1},\\infty)$ . ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "507 Similarly, we can prove the result on the existence and uniqueness of maxima of the functiong(xxx+1). ", "page_idx": 12}, {"type": "text", "text": "508 B Proof of Lemma 5 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "509 Lemma (Restatement of Lemma 5). If $\\operatorname{opt}(\\mathcal{E}_{\\mu,\\alpha})\\geq0$ , then $\\operatorname{opt}({\\mathcal E}_{\\alpha})\\geq\\mu$ . ", "page_idx": 12}, {"type": "text", "text": "510 Further, $\\operatorname{opt}(\\mathcal{E}_{\\mu,\\alpha})\\geq0$ if $\\begin{array}{r}{\\mu=\\operatorname*{min}\\Big(\\big(\\frac{n}{\\alpha}\\hat{g}_{\\mathrm{MID}}-1\\big)^{-1}\\,,\\big(\\frac{n}{\\alpha}\\hat{g}_{\\mathrm{0UT}}+1\\big)^{-1}\\Big).}\\end{array}$ ", "page_idx": 12}, {"type": "text", "text": "511 Proof. To lower bound the optimal value of $\\mathcal{E}_{\\mu,\\alpha}$ , we first pre-multiply the first constraint by $\\lambda$ (and substitute   \n512 $\\begin{array}{r}{\\frac{b_{i}}{w_{i}}=r_{i}\\;\\forall i\\in[n])}\\end{array}$ and thus define, ", "page_idx": 13}, {"type": "equation", "text": "$$\nF(\\mathbf{r},\\mathbf{b},\\lambda)=\\left(\\sum_{i=1}^{n}b_{i}\\right)-\\mu\\left(\\sum_{i=1}^{n}{\\frac{b_{i}}{r_{i}}}\\right)-\\lambda\\left(\\sum_{i=1}^{n}g(r_{i})-\\alpha\\right).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "513 Further, we define the set which satisfies the last two constraints in $\\mathcal{E}_{\\mu,\\alpha}$ by $\\mathcal{C}$ as ", "page_idx": 13}, {"type": "equation", "text": "$$\n{\\mathcal{C}}:=\\{(\\mathbf{r},\\mathbf{b})\\in(\\mathbb{R}_{\\geq0}^{n},\\mathbb{R}_{\\geq0}^{n}):b_{i}(1+1/r_{i})\\geq1;|b_{i}(1/r_{i}-1)|\\leq1\\,\\forall i\\in[n]\\}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "514 From the theory of Lagrangian, we have the following ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname{opt}({\\mathcal{E}}_{\\mu,\\alpha})\\geq\\operatorname*{min}_{({\\mathbf{r}},{\\mathbf{b}})\\in{\\mathcal{C}}}\\operatorname*{max}_{\\lambda\\geq0}F({\\mathbf{r}},{\\mathbf{b}},\\lambda)\\geq\\operatorname*{max}_{\\lambda\\geq0}\\operatorname*{min}_{({\\mathbf{r}},{\\mathbf{b}})\\in{\\mathcal{C}}}F({\\mathbf{r}},{\\mathbf{b}},\\lambda).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "515 Now for a fixed $\\lambda>0$ , we minimise $F(\\mathbf{r},\\mathbf{b},\\lambda)$ over $(\\mathbf{r},\\mathbf{b})\\in\\mathcal{C}$ . Observe that for every $i\\,\\in\\,[n]$ , it is sufficient to   \n516 minimise $h(r_{i},b_{i})$ defined as follows. ", "page_idx": 13}, {"type": "equation", "text": "$$\nh(r_{i},b_{i}):=b_{i}(1-\\mu/r_{i})-\\lambda\\left(g(r_{i})-\\frac\\alpha n\\right).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "517 Observe that the constraints in $\\mathcal{C}$ can be written as $\\begin{array}{r}{b_{i}\\geq\\frac{r_{i}}{1+r_{i}}}\\end{array}$ and $\\begin{array}{r}{b_{i}\\leq\\frac{r_{i}}{|1-r_{i}|}}\\end{array}$ . ", "page_idx": 13}, {"type": "text", "text": "518 Observe that for a given $r_{i}$ , the function $h(r_{i},b_{i})$ is monotonic in $b_{i}$ and thus the optimum point must lie on the boundary   \n519 and first optimize over $b_{i}(1+1/r_{i})=1$ (call it $\\mathcal{C}_{i}^{\\mathrm{MID}}.$ ) and $\\left|b_{i}(1-1/r_{i})\\right|=1$ (call it $\\mathcal{C}_{i}^{\\mathrm{{oUT}}}$ ) respectively.   \n520 Recall from Lemma 6 that there exists $c_{1},c_{2}$ s.t. $g_{\\mathrm{MID}}(x)$ is convex in $(0,c_{1})$ and concave in $(c_{1},1)$ and $g_{\\mathrm{oUT}}(x)$ is   \n521 convex in $(0,c_{2})$ and concave in $(c_{2},\\infty)$ . ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "522 \u2022 Minimisation of $h(r_{i},b_{i})$ over $b_{i}(1+1/r_{i})=1$ . ", "page_idx": 13}, {"type": "text", "text": "We first substitute $1/r_{i}\\,=\\,1/b_{i}\\,-\\,1$ in the function and thus, can write the function $h(b_{i})\\,=\\,b_{i}(\\mu+1)\\,-\\,\\mu\\,-$ $\\begin{array}{r}{\\lambda\\left(g\\left(\\frac{b_{i}}{1-b_{i}}\\right)-\\frac{\\alpha}{n}\\right)=b_{i}(\\mu+1)-\\mu-\\lambda\\left(g_{\\mathrm{MID}}(b_{i})-\\frac{\\alpha}{n}\\right)\\!.}\\end{array}$ ", "page_idx": 13}, {"type": "text", "text": "525 Observe that on optimizing over $b_{i}$ , we obtain two local minima, one at $b_{i}=0$ and the other at $b_{i}=\\tilde{x}^{\\mathrm{MD}}(\\lambda)\\in(c_{1},\\infty)$   \n526 where $\\tilde{x}^{\\mathrm{MID}}(\\lambda)$ satisfies the following equations if $\\begin{array}{r}{\\lambda\\ge\\frac{1+\\mu}{g_{\\mathrm{MID}}^{\\prime}(c_{1})}}\\end{array}$ . Otherwise, we have a unique minima at $b_{i}=0$ . 3 ", "page_idx": 13}, {"type": "equation", "text": "$$\ng_{\\mathrm{MD}}^{\\prime}(\\tilde{x}^{\\mathrm{MD}}(\\lambda))=\\operatorname*{max}\\left(\\frac{1+\\mu}{\\lambda},g_{\\mathrm{MD}}^{\\prime}(1^{-})\\right)\\mathrm{~and~}g_{\\mathrm{MD}}^{\\prime\\prime}(\\tilde{x}^{\\mathrm{MD}}(\\lambda))<0.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "527 Observe $\\tilde{x}^{\\mathrm{MID}}(\\lambda)\\,>\\,c_{1}$ since $g_{\\mathrm{MID}}$ is concave only in $[c_{1},1]$ . Also observe that since $g_{\\mathrm{MID}}^{\\prime}(x)$ is monotonically   \n528 increasing, $\\tilde{x}^{\\mathrm{MID}}(\\lambda)$ is monotonically increasing in $\\lambda$ . ", "page_idx": 13}, {"type": "text", "text": "529 \u2022 Minimisation of $h(r_{i},b_{i})$ over $b_{i}|(1-1/r_{i})|=1.$ . ", "page_idx": 13}, {"type": "text", "text": "530 On substituting, we write the function ", "page_idx": 13}, {"type": "equation", "text": "$$\nh(b_{i})={\\left\\{\\begin{array}{l l}{(1-\\mu)b_{i}-\\mu-\\lambda\\left(g\\left({\\frac{b_{i}}{1+b_{i}}}\\right)-{\\frac{\\alpha}{n}}\\right)=(1-\\mu)b_{i}-\\mu-\\lambda\\left(g_{\\mathrm{our}}(b_{i})-{\\frac{\\alpha}{n}}\\right)}&{{\\mathrm{~if~}}}\\\\ {(1-\\mu)b_{i}+\\mu-\\lambda\\left(g\\left({\\frac{b_{i}}{b_{i}-1}}\\right)-{\\frac{\\alpha}{n}}\\right){\\overset{(a)}{=}}\\left(1-\\mu\\right)b_{i}+\\mu-\\lambda+\\lambda\\left(g_{\\mathrm{our}}(b_{i}-1)+{\\frac{\\alpha}{n}}\\right)}&{{\\mathrm{~otherwise}}}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "531 $(a)$ follows from the fact that $g(r)+g(1/r)=1$ . ", "page_idx": 13}, {"type": "text", "text": "532 Since the second function has only a single minima at $b_{i}=1$ , it is sufficient to consider only the first function in the   \n533 case $r_{i}\\geq1$ .   \n534 Observe that on optimizing over $b_{i}$ , we obtain two local minima one at $b_{i}=0$ and one at $b_{i}=\\tilde{x}^{\\mathrm{oUT}}(\\lambda)\\in(c_{2},\\infty)$   \n535 where $\\tilde{x}^{\\mathrm{oUT}}(\\lambda)$ satisfies the equations if $\\begin{array}{r}{\\lambda\\geq\\frac{1-\\mu}{g_{\\mathrm{oUT}}^{\\prime}\\left(c_{2}\\right)}}\\end{array}$ . Otherwise, we have a unique minima at $b_{i}=0$ . 4 ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "equation", "text": "$$\ng_{\\mathrm{our}}^{\\prime}(\\tilde{x}^{\\mathrm{our}}(\\lambda))=\\left(\\frac{1-\\mu}{\\lambda}\\right)\\;\\mathrm{and}\\;g_{\\mathrm{our}}^{\\prime\\prime}(\\tilde{x}^{\\mathrm{our}}(\\lambda))<0.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "536 Thus, we have $\\tilde{x}^{\\mathrm{oUT}}(\\lambda)>c_{2}$ since $g_{\\mathrm{{OUT}}}$ is concave only in $[c_{2},\\infty)$ . Also observe that since $g_{\\mathrm{oUT}}^{\\prime}(x)$ is monotonic,   \n537 $\\tilde{x}^{\\mathrm{oUT}}(\\lambda)$ is monotonic in $\\lambda$ . ", "page_idx": 13}, {"type": "text", "text": "538 Since, this argument is true for every $i\\in[n]$ , we obtain ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\underset{(\\mathbf{r},\\mathbf{b})}{\\operatorname*{min}}F(\\mathbf{r},\\mathbf{b},\\boldsymbol{\\lambda})=n\\cdot\\operatorname*{min}\\left(-\\mu+\\lambda\\frac{\\alpha}{n},(\\mu+1)\\tilde{x}^{\\mathrm{MD}}(\\boldsymbol{\\lambda})-\\mu-\\lambda\\left(g_{\\mathrm{MD}}(\\tilde{x}^{\\mathrm{MD}}(\\boldsymbol{\\lambda}))-\\frac{\\alpha}{n}\\right),\\right.}\\\\ &{}&{\\left.(1-\\mu)\\tilde{x}^{\\mathrm{OU}}(\\boldsymbol{\\lambda})-\\mu-\\lambda\\left(g_{\\mathrm{OU}}(\\tilde{x}^{\\mathrm{OU}}(\\boldsymbol{\\lambda}))-\\frac{\\alpha}{n}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "539 Since $x_{\\mathrm{MID}}^{*}$ is the local maximiser of $\\frac{g_{\\mathrm{MID}}(x)}{x}$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\ng_{\\mathrm{MD}}\\big(x_{\\mathrm{MD}}^{*}\\big)=x_{\\mathrm{MD}}^{*}\\hat{g}_{\\mathrm{MD}}\\;\\mathrm{and}\\;x_{\\mathrm{MD}}^{*}>c_{1}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "540 Similarly, ", "page_idx": 14}, {"type": "equation", "text": "$$\ng_{\\mathrm{oUT}}(x_{\\mathrm{oUT}}^{*})=x_{\\mathrm{oUT}}^{*}\\hat{g}_{\\mathrm{oUT}}\\;\\mathrm{and}\\;x_{\\mathrm{oUT}}^{*}>c_{2}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "541 For the purpose of this analysis, we define two functions $\\delta^{\\mathrm{MID}}(\\lambda)$ and $\\delta^{\\mathrm{oUT}}(\\lambda)$ below. ", "page_idx": 14}, {"type": "text", "text": "542 ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\delta^{\\mathrm{MD}}(\\lambda)=(\\mu+1)\\tilde{x}^{\\mathrm{MD}}(\\lambda)-\\mu-\\lambda\\left(g_{\\mathrm{MD}}(\\tilde{x}^{\\mathrm{MD}}(\\lambda))-\\displaystyle\\frac{\\alpha}{n}\\right).}\\\\ &{\\delta^{\\mathrm{our}}(\\lambda)=(1-\\mu)\\tilde{x}^{\\mathrm{our}}(\\lambda)-\\mu-\\lambda\\left(g_{\\mathrm{our}}(\\tilde{x}^{\\mathrm{our}}(\\lambda))-\\displaystyle\\frac{\\alpha}{n}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "543 We also define ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mu^{*}:=\\operatorname*{min}\\left(\\left(\\frac{n}{\\alpha}\\widehat{g}_{\\sf M I D}-1\\right)^{-1},\\left(\\frac{n}{\\alpha}\\widehat{g}_{\\sf O U T}+1\\right)^{-1}\\right),\\quad\\mathrm{and}\\quad\\quad\\lambda^{*}:=\\mu^{*}\\frac{n}{\\alpha}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "544 Recall that we aim to show $\\operatorname{opt}(\\mathcal{E}_{\\mu,\\alpha})~\\geq~0$ when $\\mu\\mathrm{~}=\\mu^{*}$ and thus substitute $\\mu\\ =\\ \\mu^{*}$ in every subsequent   \n545 equation. Observe that it is sufficient to show $\\delta^{\\mathrm{MID}}(\\lambda^{*})$ and $\\delta^{\\mathrm{oUT}}(\\lambda^{*})$ are non-negative since this would imply that   \n546 $\\begin{array}{r}{\\operatorname*{max}_{\\lambda\\geq0}\\operatorname*{min}_{(\\mathbf{r},\\mathbf{b})\\in\\mathcal{C}}F(\\mathbf{r},\\mathbf{b},\\lambda)}\\end{array}$ is non-negative. ", "page_idx": 14}, {"type": "text", "text": "547 We now consider the following two exhaustive cases. ", "page_idx": 14}, {"type": "text", "text": "548 \u2022 Case 1: $\\begin{array}{r}{\\hat{g}_{\\mathrm{MID}}-\\frac{\\alpha}{n}>\\hat{g}_{\\mathrm{OUT}}+\\frac{\\alpha}{n}}\\end{array}$ . Observe from Equation (15), ", "page_idx": 14}, {"type": "equation", "text": "$$\ng_{\\mathrm{MD}}^{\\prime}(\\tilde{x}^{\\mathrm{MD}}(\\lambda^{*}))=\\operatorname*{max}\\left(\\frac{n}{\\alpha}\\left(\\frac{1}{\\mu^{*}}+1\\right),g_{\\mathrm{MD}}^{\\prime}(1^{-})\\right)=g_{\\mathrm{MD}}^{\\prime}(x_{\\mathrm{MD}}^{*})\\;\\stackrel{(d)}{\\longrightarrow}\\;\\tilde{x}^{\\mathrm{MD}}(\\lambda^{*})=x_{\\mathrm{MD}}^{*}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "549 $(d)$ follows from the fact that both $\\tilde{x}^{\\mathrm{MID}}(\\lambda^{*})$ and $x_{\\mathrm{MID}}^{*}$ exceed $c_{1}$ and $g_{\\mathrm{MID}}^{\\prime}(x)$ is monotonically decreasing for $x\\geq c_{1}$ . ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\delta^{\\mathrm{MD}}(\\lambda^{*})=(\\mu^{*}+1)\\tilde{x}^{\\mathrm{MD}}(\\lambda^{*})-\\mu^{*}-\\lambda^{*}\\left(g_{\\mathrm{MD}}(\\tilde{x}^{\\mathrm{MD}}(\\lambda^{*}))-\\frac{\\alpha}{n}\\right)}\\\\ &{\\qquad\\qquad\\stackrel{(b)}{\\geq}(-\\mu^{*}+\\lambda^{*}\\alpha/n)+\\lambda^{*}\\left(\\tilde{x}^{\\mathrm{MD}}(\\lambda^{*})g_{\\mathrm{MD}}^{\\prime}(\\tilde{x}^{\\mathrm{MD}}(\\lambda^{*}))-g_{\\mathrm{MD}}(\\tilde{x}^{\\mathrm{MD}}(\\lambda^{*}))\\right)\\stackrel{(c)}{\\geq}0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "550 $(b)$ follows from $\\begin{array}{r}{g_{\\mathrm{MID}}^{\\prime}(\\tilde{x}^{\\mathrm{MID}}(\\lambda^{*}))=\\frac{1+\\mu^{*}}{\\lambda^{*}}}\\end{array}$ 5 as stated in Equation (15). ", "page_idx": 14}, {"type": "text", "text": "551 $(c)$ follows from $\\tilde{x}^{\\mathrm{MID}}(\\lambda^{*})=x_{\\mathrm{MID}}^{*}$ (in Equation (24)) and $g_{\\mathrm{MID}}\\big(x_{\\mathrm{MID}}^{*}\\big)=x_{\\mathrm{MID}}^{*}\\hat{g}_{\\mathrm{MID}}$ (in Equation (19)) and the fact that   \n552 $\\mu^{*}=\\lambda^{*}\\frac{\\alpha}{n}$ . Now consider, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{(d)}{x}\\,\\frac{g_{\\mathrm{our}}(x_{\\mathrm{our}}^{*})}{x_{\\mathrm{our}}^{*}}\\,\\overset{(e)}{\\leq}\\frac{g_{\\mathrm{nlD}}(x_{\\mathrm{MD}}^{*})}{x_{\\mathrm{sur}}^{*}}-2\\alpha/n\\,\\overset{(g)}{=}\\frac{1-\\mu}{\\lambda^{*}}\\,\\overset{(h)}{\\Longrightarrow}\\,g_{\\mathrm{our}}^{\\prime}(x_{\\mathrm{our}}^{*})\\leq g_{\\mathrm{our}}^{\\prime}(\\widetilde x^{\\mathrm{our}}(\\lambda^{*}))\\,\\overset{(i)}{\\Longrightarrow}\\,x_{\\mathrm{our}}^{*}\\geq1.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "553 $(d)$ follows from the fact that $x_{\\mathrm{oUT}}^{*}$ is the local maximiser of $g_{\\mathrm{oUT}}(x)/x$ , ", "page_idx": 14}, {"type": "text", "text": "554 $(e)$ follows from the fact that $\\begin{array}{r}{\\hat{g}_{\\mathrm{MID}}-\\frac{\\alpha}{n}>\\hat{g}_{\\mathrm{OUT}}+\\frac{\\alpha}{n}}\\end{array}$ in Case 1. ", "page_idx": 14}, {"type": "text", "text": "555 $(g)$ follows from the definition of $\\lambda^{*}$ and that $\\mu=\\lambda^{*}\\frac{\\alpha}{n}$ . ", "page_idx": 14}, {"type": "text", "text": "556 $(h)$ follows from the constraint in (17). ", "page_idx": 14}, {"type": "text", "text": "557 $(i)$ follows from the fact that $g_{\\mathrm{oUT}}^{\\prime}(x)$ is monotonically decreasing in $x$ in $[c_{2},\\infty)$ . ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\delta^{\\mathrm{our}}(\\lambda^{*})=(1-\\mu)\\tilde{x}^{\\mathrm{our}}(\\lambda^{*})-\\mu-\\lambda^{*}\\left(g_{\\mathrm{our}}(\\tilde{x}^{\\mathrm{our}}(\\lambda^{*}))-\\frac{\\alpha}{n}\\right)}\\\\ &{\\qquad\\qquad\\overset{(j)}{=}\\left(-\\mu+\\lambda^{*}\\frac{\\alpha}{n}\\right)+\\lambda^{*}(\\tilde{x}^{\\mathrm{our}}(\\lambda^{*})g_{\\mathrm{our}}^{\\prime}(\\tilde{x}^{\\mathrm{our}}(\\lambda^{*}))-g_{\\mathrm{our}}(\\tilde{x}^{\\mathrm{our}}(\\lambda^{*})))}\\\\ &{\\qquad\\qquad\\overset{(k)}{\\geq}0+0\\geq0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "558 $(j)$ follows from $\\begin{array}{r}{g_{\\mathrm{oUT}}^{\\prime}(\\tilde{x}^{\\mathrm{oUT}}(\\lambda))=\\frac{1-\\mu}{\\lambda}}\\end{array}$ as stated in Equation (17), and ", "page_idx": 15}, {"type": "text", "text": "559 $(k)$ follows from the following reasons: ", "page_idx": 15}, {"type": "text", "text": "\u2013 Observe that $x g_{\\mathrm{oUT}}^{\\prime}(x)-g_{\\mathrm{oUT}}(x)$ is monotonically decreasing in $[c_{2},\\infty)$ as $g_{\\mathrm{OUT}}$ is concave in this region. However, since $x_{\\mathrm{oUT}}^{*}\\geq\\tilde{x}^{\\mathrm{oUT}}(\\lambda^{*})\\geq c_{2}$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(\\tilde{x}^{\\mathrm{our}}(\\lambda^{*})g_{\\mathrm{our}}^{\\prime}(\\tilde{x}^{\\mathrm{our}}(\\lambda^{*}))-g_{\\mathrm{our}}(\\tilde{x}^{\\mathrm{our}}(\\lambda^{*})))\\geq x_{\\mathrm{our}}^{*}\\hat{g}_{\\mathrm{our}}-g_{\\mathrm{our}}(x_{\\mathrm{our}}^{*})=0}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "560 \u2013 $\\lambda^{*}=\\mu^{*}\\frac{n}{\\alpha}$ follows from the definition of $\\lambda^{*}$ . ", "page_idx": 15}, {"type": "text", "text": "561 Thus, using (25) and (26) we show that for the chosen value of $\\lambda^{*}=\\mu^{*}\\frac{n}{\\alpha}$ , we have ", "page_idx": 15}, {"type": "text", "text": "562 $\\operatorname*{min}_{(\\mathbf{r},\\mathbf{w})\\in{\\mathcal{C}}}F(\\mathbf{r},\\mathbf{w},\\lambda^{*})\\geq0$ implying from (13) that $\\operatorname{pt}(\\mathcal{E}_{\\mu,\\alpha})\\geq0$ . ", "page_idx": 15}, {"type": "text", "text": "563 \u2022 Case 2: $\\begin{array}{r}{\\hat{g}_{\\mathrm{MID}}-\\frac{\\alpha}{n}\\le\\hat{g}_{\\mathrm{OUT}}+\\frac{\\alpha}{n}}\\end{array}$ ", "page_idx": 15}, {"type": "text", "text": "564 Choosing $\\lambda^{*}=\\mu^{*}\\frac{n}{\\alpha}$ , we can prove $\\operatorname{spt}(\\mathcal{E}_{\\mu,\\alpha})\\geq0$ in a very similar manner whenever $\\mu=\\mu^{*}$ . ", "page_idx": 15}, {"type": "text", "text": "565 C Proof of Theorem 1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "566 Theorem (Restatement of Theorem 1). For every $\\epsilon>0$ and $m\\geq2$ and $n\\geq m^{2}$ we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{DIST}^{(g)}\\big(\\mathrm{PLU},n,m\\big)\\leq m\\big(m-1\\big)\\left(\\widehat{g}_{\\mathrm{MID}}+\\widehat{g}_{\\mathrm{oUT}}\\right)\\exp\\Big(\\displaystyle\\frac{-n^{(\\frac{1}{2}+\\epsilon)}+2m}{(2n^{(\\frac{1}{2}-\\epsilon)}-1)m}\\Big)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\operatorname*{max}\\bigg(\\displaystyle\\frac{m\\widehat{g}_{\\mathrm{MID}}}{\\left(1-n^{-(\\frac{1}{2}-\\epsilon)}\\right)}-1,\\displaystyle\\frac{m\\widehat{g}_{\\mathrm{oUT}}}{\\left(1-n^{-(\\frac{1}{2}-\\epsilon)}\\right)}+1\\bigg).}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\operatorname*{lim}_{1\\rightarrow\\infty}\\mathrm{DIST}^{(g)}\\big(\\mathrm{PLU},n,m\\big)\\leq\\operatorname*{max}\\big(m\\widehat{g}_{\\mathrm{MID}}-1,m\\widehat{g}_{\\mathrm{oUT}}+1\\big)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "568 Proof. Recall that candidate $B\\in A$ minimises the social cost. The other candidates are denoted by $\\{A_{j}\\}_{j\\in[m-1]}$ . ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathsf{D I S T}^{(g)}(\\mathbf{PLU},n,m)=\\operatorname*{sup}_{d\\in\\mathcal{M}(N\\cup\\mathcal{A})}\\left(\\sum_{j=1}^{m-1}\\mathbb{P}[A_{j}\\ \\mathrm{wins}]\\frac{\\mathbf{SC}(A_{j},d)}{\\mathbf{SC}(B,d)}+\\mathbb{P}[B\\ \\mathrm{wins}]\\right)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "569 For every $j\\in[m-1]$ , we now bound the probability of $A_{j}$ being the winner. This event implies that at least $\\frac{n}{m}$ voters   \n570 choose $A_{j}$ as the top preference, implying that the same voters rank $A_{j}$ over $B$ . Further, we now define Bernoulli   \n571 random variables $\\{\\bar{Y}_{i,j}\\}_{i=1}^{n}$ each denoting the event that voter $i$ ranks candidate $A_{j}$ over $B$ . Recall from Equation 3,   \n572 $\\begin{array}{r}{Y_{i,j}\\sim\\mathrm{Bern}\\left(g\\left(\\frac{d(i,\\tilde{B)}}{d(i,A_{j})}\\right)\\right)}\\end{array}$ . Therefore, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{P}[A_{j}{\\mathrm{~wins}}]\\leq\\mathbb{P}\\left(\\sum_{i=1}^{n}Y_{i,j}\\geq{\\frac{n}{m}}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "573 Let $\\alpha_{j}$ be the expectation of the random variable $\\textstyle\\sum_{i=1}^{n}Y_{i,j}$ i.e. the expected number of voters ranking $A_{j}$ over $B$ . ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\alpha_{j}:=\\sum_{i=1}^{n}\\mathbb{E}[Y_{i,j}]=\\sum_{i=1}^{n}g\\left({\\frac{d(i,B)}{d(i,A_{j})}}\\right){\\mathrm{~for~every~}}j\\in[m-1].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "574 Now we use Chernoff bounds on the sum of Bernoulli random variable for every $j\\in[m-1]$ when $\\begin{array}{r}{\\alpha_{j}\\le\\frac{n}{m}-\\frac{n^{(1/2+\\epsilon)}}{m}}\\end{array}$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{P}[A_{j}\\operatorname*{sup}_{i\\in\\mathcal{I}_{k}^{n}}]\\leq\\mathbb{P}\\bigg(\\frac{\\sum_{t}\\gamma_{j,t}}{\\sum_{t}}\\geq\\frac{n}{n}\\bigg)-\\mathbb{P}\\bigg(\\frac{\\sum_{t}\\gamma_{j,t}}{1+\\gamma_{j,t}}\\geq\\sigma_{j}\\bigg(1+\\frac{m_{t}}{m_{D}}-1\\bigg)\\bigg)}\\\\ {\\leq\\bigg(\\frac{m_{t}}{(1+\\gamma_{j})^{2}(m_{t})}\\bigg)^{\\frac{n}{2}}}\\\\ {=}&{\\bigg(\\frac{m_{t}}{m_{D}}\\bigg)^{\\frac{n}{2}}e^{-\\frac{1}{n}(m_{t})}}\\\\ {\\leq\\frac{m_{t}}{n}\\bigg(\\frac{m_{t}}{m_{D}}\\bigg)^{\\frac{n}{2}}}\\\\ {\\overset{()}{\\leq}\\frac{m_{t}}{n}e^{\\frac{1}{2}}\\bigg(\\bigg(1-n^{-(1-\\gamma)}\\exp\\bigg(\\frac{-\\frac{n}{n}-\\frac{\\gamma_{1}}{m_{D}}}{\\sqrt{n}m_{-1}^{2}}\\bigg)\\bigg)^{\\frac{n-1}{2}}}\\\\ {=}&{\\frac{m_{t}}{n}\\bigg(1-n^{-(1-\\gamma)}\\ln^{(1-\\gamma)}\\exp\\bigg(\\frac{\\sigma_{j}(1+\\frac{m_{t}}{n})}{n}\\bigg)}\\\\ {\\overset{()}{\\leq}\\frac{m_{t}}{n}\\bigg)\\bigg(\\frac{1-n^{1/2}}{2}\\sum_{s=-1}^{\\infty/(1-\\gamma)}(n\\mu-1)\\frac{n+1+n^{-\\gamma}}{n}\\bigg)}\\\\ {=}&{\\frac{m_{t}}{n}e^{\\frac{1}{2}}\\bigg(\\frac{m_{t}}{n}\\bigg)^{\\frac{()-()-()-()}{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "576 $(a)$ follows from applying the Chernoff bound. We restate the bound from [60] below. ", "page_idx": 16}, {"type": "text", "text": "577 Suppose $X_{1},X_{2},\\ldots,X_{n}$ be independent Bernoulli random variables with $\\mathbb{P}(X_{i})=\\mu_{i}$ for every $i\\in[n]$ and $\\mu:=$   \n578 $\\textstyle\\sum_{i=1}^{\\bar{n}^{-}}\\mu_{i}$ , then we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\sum_{i}X_{i}\\geq(1+\\delta)\\mu)\\leq\\left(\\frac{e^{\\delta}}{(1+\\delta)^{1+\\delta}}\\right)^{\\mu}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "579 (c) holds sinc1e xe\u2212x is increasing in (0, 1) and becausen/m\u03b1\u22121 \u22641 and \u03b1 \u2264 nm \u2212n( 12m +\u03f5), the maxima is attained at   \n580 = nm \u2212n( 2m +\u03f5). (d) holds since log(1 + x) \u226422+xx for \u22121 < x \u22640. ", "page_idx": 16}, {"type": "text", "text": "581 Let $\\begin{array}{r}{S:=\\{j\\in[m-1]:\\alpha_{j}<\\frac{n}{m}-\\frac{n^{(1/2+\\epsilon)}}{m}\\}}\\end{array}$ i.e. $S$ denotes the indices of candidates with $\\alpha_{j}$ less than $\\frac{n}{m}\\,-\\,\\frac{n^{(1/2+\\epsilon)}}{m}\\$ m 582 Now using Lemma 2 and $\\begin{array}{r}{\\alpha_{j}\\geq\\frac{n}{m}-\\frac{n^{(1/2+\\epsilon)}}{m}}\\end{array}$ for every $j\\in[m-1]\\:\\backslash\\:S$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{S C(A_{j},d)}{S C(B,d)}\\leq\\operatorname*{max}\\left(\\frac{m\\hat{g}_{\\mathrm{MID}}}{(1-n^{-(1/2-\\epsilon)})}-1,\\frac{m\\hat{g}_{\\mathrm{our}}}{(1-n^{-(1/2-\\epsilon)})}+1\\right)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "583 We now have ", "page_idx": 16}, {"type": "text", "text": "$\\mathrm{DIST}^{(g)}(\\mathbf{PLU},n,m)$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{sup}_{\\in\\mathcal{M}(N\\cup\\mathcal{A})}\\left(\\sum_{j\\in[m-1]\\backslash S}\\left(\\mathbb{P}[A_{j}\\mathrm{~wins}]\\frac{\\mathrm{SC}(A_{j},d)}{\\mathrm{SC}(B,d)}\\right)+\\mathbb{P}[B\\mathrm{~wins}]+\\sum_{j\\in S}\\left(\\mathbb{P}[A_{j}\\mathrm{~wins}]\\frac{\\mathrm{SC}(A_{j},d)}{\\mathrm{SC}(B,d)}\\right)\\right)}\\\\ &{\\displaystyle\\operatorname*{max}\\left(\\sum_{j\\in[m-1]\\backslash S}\\frac{S C(A_{j},d)}{S C(B,d)},1\\right)+\\sum_{j\\in S}\\left(\\operatorname*{max}\\left(\\frac{n}{\\alpha_{j}}\\hat{g}_{\\mathrm{MID}}-1,\\frac{n}{\\alpha_{j}}\\hat{g}_{\\mathrm{our}}+1\\right)\\frac{m\\alpha_{j}}{n}\\exp\\left(\\frac{-n^{(\\frac{1}{2}+\\epsilon)}+2\\kappa}{(2n^{(\\frac{1}{2}-\\epsilon)}-1)\\tau^{j}}\\right)\\right)}\\\\ &{n(m-1)\\left(\\hat{g}_{\\mathrm{MID}}+\\hat{g}_{\\mathrm{our}}\\right)\\exp\\left(\\frac{-n^{(\\frac{1}{2}+\\epsilon)}+2m}{(2n^{(\\frac{1}{2}-\\epsilon)}-1)m}\\right)+\\operatorname*{max}\\left(\\frac{m\\hat{g}_{\\mathrm{MID}}}{(1-n^{-(1/2-\\epsilon)})}-1,\\frac{m\\hat{g}_{\\mathrm{our}}}{(1-n^{-(1/2-\\epsilon)})}+\\frac{1}{(2n^{-(1/2-\\epsilon)}-1)}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "584 $(a)$ follows from the following observations. ", "page_idx": 17}, {"type": "text", "text": "585 \u2022 Apply Lemma 2 to bound $\\frac{S C(A_{j},d)}{S C(B,d)}$ . Since $\\begin{array}{r}{\\alpha_{j}\\le\\frac{n}{m}-\\frac{n^{(1/2+\\epsilon)}}{m}\\,\\forall j\\in S}\\end{array}$ , apply Equation (31) to bound $\\mathbb{P}[A_{j}$ wins]. ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\bullet\\sum_{j\\in[m-1]\\backslash S}\\left(\\mathbb{P}[A_{j}\\ \\operatorname{wins}]\\frac{\\operatorname{SC}(A_{j},d)}{\\operatorname{SC}(B,d)}\\right)+\\mathbb{P}[B\\ \\operatorname{wins}]\\leq\\operatorname*{max}\\left(\\operatorname*{max}_{j\\in[m-1]\\backslash S}\\frac{\\operatorname{SC}(A_{j},d)}{\\operatorname{SC}(B,d)},1\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "587 (b) follows from the fact that $|S|\\leq m-1$ , $\\operatorname*{max}(a,b)\\leq a+b$ , and applying Equation (40). ", "page_idx": 17}, {"type": "text", "text": "588 D Proof of Theorem 2 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "589 Theorem (Restatement of Theorem 2). For every $\\begin{array}{r l r}{m}&{{}\\geq}&{2,}\\end{array}$ , $\\begin{array}{r l}{\\operatorname*{lim}_{n\\to\\infty}\\mathrm{DIST}^{(g)}\\big(\\mathrm{PLU},n,m\\big)}&{{}\\ge}\\end{array}$   \n590 max $(m\\hat{g}_{\\mathrm{MID}}-1,m\\hat{g}_{\\mathrm{0UT}}+1)$ .   \n591 Proof. The proof is by an example in an Euclidean metric space in $\\mathbb{R}^{3}$ . One candidate ${}^{\\leftarrow}\\mathbf{C}\"$ is at $(1,0,0)$ . The other   \n592 $m-1$ candidates are \u201cgood\" and are equidistantly placed on a circle of radius $\\epsilon$ on the $y-z$ plane centred at $(0,0,0)$ .   \n593 We call them $\\mathcal{G}:=\\left\\{G_{1},G_{2},\\ldots,G_{m-1}\\right\\}$ . ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "594 We present two constructions below for every $\\epsilon,\\zeta>0$ . ", "page_idx": 17}, {"type": "text", "text": "5 Construction 1: Let $\\begin{array}{r}{q_{\\mathrm{MID}}:=g\\biggl(\\frac{\\sqrt{(x_{\\mathrm{MID}}^{*})^{2}+\\epsilon^{2}}}{1-x_{\\mathrm{MID}}^{*}}\\biggr)}\\end{array}$ and $\\begin{array}{r}{a_{\\mathrm{MID}}:=\\frac{1}{m-1}\\biggl(1-\\frac{1+\\zeta}{m q_{\\mathrm{MID}}}\\biggr)}\\end{array}$ Each of the $m-1$ candidates in $\\mathcal{G}$ has ", "page_idx": 17}, {"type": "text", "text": "596 $\\lfloor a_{\\mathrm{MID}}n\\rfloor$ voters overlapping with it. The remaining voters (we call them \u201cambivalent\u201d) are placed at $(x_{\\mathrm{MID}}^{*},0,0)$ . Clearly,   \n597 each voter overlapping with a candidate votes for it as the most preferred candidate with probability one. Each of the   \n598 ambivalent voters votes as follows.   \n599 \u2013 With probability $q_{\\mathrm{MID}}$ , vote for candidate $C$ as the top choice and uniformly randomly permute the other candidates in   \n600 the rest of the vote.   \n601 \u2013 With probability $1-q_{\\mathrm{MID}}$ , vote for candidate $C$ as the last choice and uniformly randomly permute the other candidates   \n602 in the rest of the vote.   \n603 Observe that this satisfies the pairwise probability criterion in Equation 3. Since $\\operatorname*{lim}_{n\\to\\infty}\\lfloor a n\\rfloor/n=a$ and that the   \n604 distance of a candidate in $\\mathcal{G}$ from any non-ambivalent voter is at most $2\\epsilon$ , we have that for every $j\\in[m-1]$ , ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{n\\rightarrow\\infty}{\\operatorname*{lim}}\\frac{S C(C,d)}{S C(G_{j},d)}\\geq\\!\\frac{(1-x_{\\mathrm{\\scriptscriptstyle{MID}}}^{*})(1-(m-1)a_{\\mathrm{\\scriptscriptstyle{MID}}})+(m-1)a_{\\mathrm{\\scriptscriptstyle{MID}}}\\sqrt{1+\\epsilon^{2}}}{(1-(m-1)a_{\\mathrm{\\scriptscriptstyle{MID}}})\\sqrt{(x_{\\mathrm{\\scriptscriptstyle{MID}}}^{*})^{2}+\\epsilon^{2}}+2(m-2)a_{\\mathrm{\\scriptscriptstyle{MID}}}\\epsilon}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\frac{(m q_{\\mathrm{\\scriptscriptstyle{MID}}}-(1+\\zeta))\\sqrt{1+\\epsilon^{2}}+(1+\\zeta)(1-x_{\\mathrm{\\scriptscriptstyle{MID}}}^{*})}{(1+\\zeta)\\sqrt{(x_{\\mathrm{\\scriptscriptstyle{MID}}}^{*})^{2}+\\epsilon^{2}}+2(m-2)a_{\\mathrm{\\scriptscriptstyle{MID}}}\\epsilon}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "605 Clearly every candidate in $\\mathcal{G}$ minimises the social cost and now we show that $\\operatorname*{lim}_{n\\to\\infty}\\mathbb{P}[C{\\mathrm{~wins}}]=1$ ", "page_idx": 17}, {"type": "text", "text": "Let Bernoulli random variables $\\{Y_{i}\\}_{i=1}^{n}$ denote the events that voter $i\\,\\in\\,{\\mathcal{N}}$ ranks candidate $C$ at the top. Here, $\\begin{array}{r}{\\sum_{i=1}^{n}\\mathbb{P}[Y_{i}=1]=q_{\\mathrm{MD}}\\big(n-(m\\mathrm{~\\bar{~}}{1})\\big[a_{\\mathrm{MD}}^{\\mathrm{~}}n\\big]\\big)}\\end{array}$ and thus ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\to\\infty}{\\frac{\\sum_{i=1}^{n}\\mathbb{P}[Y_{i}=1]}{n}}={\\frac{1+\\zeta}{m}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "606 By the law of large numbers, we have that $\\textstyle\\mathbb{P}[\\sum_{i}Y_{i}\\geq{\\frac{n}{m}}]=1$ as $n\\to\\infty$ . Since every candidate in $\\mathcal{G}$ is equally likely   \n607 to win, the event $\\textstyle\\sum_{i}Y_{i}\\geq{\\frac{n}{m}}$ implies the event that $C$ is the winner and thus, $\\begin{array}{r}{\\operatorname*{lim}_{n\\to\\infty}\\mathbb{P}[C\\,\\operatorname{wins}]=1}\\end{array}$ . Thus, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\to\\infty}\\mathrm{DIST}^{(g)}(\\mathrm{PLU},n,m)\\geq\\frac{\\big(m q_{\\mathrm{MD}}-(1+\\zeta)\\big)\\sqrt{1+\\epsilon^{2}}+(1+\\zeta)\\big(1-x_{\\mathrm{MD}}^{*}\\big)}{(1+\\zeta)\\sqrt{(x_{\\mathrm{MD}}^{*})^{2}+\\epsilon^{2}}+2(m-2)a_{\\mathrm{MD}}\\epsilon}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "608 Construction 2: Let $\\begin{array}{r}{q_{\\mathrm{our}}:=g\\left(\\frac{\\sqrt{(x_{\\mathrm{oUT}}^{*})^{2}+\\epsilon^{2}}}{1+x_{\\mathrm{oUT}}^{*}}\\right)}\\end{array}$ and $\\begin{array}{r}{a_{00\\mathrm{T}}:=\\frac{1}{m-1}\\left(1-\\frac{1+\\zeta}{m q_{00\\mathrm{T}}}\\right)}\\end{array}$ . Each candidate in $\\mathcal{G}$ has $\\lfloor a_{\\mathrm{oUT}}n\\rfloor$ voters   \n609 overlapping with it, and the remaining \u201cambivalent\" voters are at $(-x_{\\mathrm{oUT}}^{*},0,0)$ .   \n610 Clearly, each voter overlapping with a candidate votes for it as the most preferred candidate with probability one. Each   \n611 of the ambivalent voters votes as follows.   \n612 \u2022 With probability $q_{\\mathrm{OUT}}$ , vote for candidate $C$ as the top choice and uniformly randomly permute the other candidates in   \n613 the rest of the vote.   \n614 \u2022 With probability $1\\!-\\!q_{\\mathrm{oUT}}$ , vote for candidate $C$ as the last choice and uniformly randomly permute the other candidates   \n615 in the rest of the vote. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "616 This satisfies the pairwise probability criterion in Equation 3. For every $j\\in[m-1]$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{n\\rightarrow\\infty}{\\operatorname*{lim}}\\,\\frac{S C(C,d)}{S C(G_{j},d)}\\geq\\!\\frac{(1+x_{\\mathrm{our}}^{*})(1-(m-1)a_{\\mathrm{our}})+(m-1)a_{\\mathrm{our}}\\sqrt{1+\\epsilon^{2}}}{(1-(m-1)a_{\\mathrm{our}})\\sqrt{(x_{\\mathrm{our}}^{*})^{2}+\\epsilon^{2}}+2(m-2)a_{\\mathrm{our}}\\epsilon}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\frac{(1+\\zeta)(1+x_{\\mathrm{our}}^{*})+(m q_{\\mathrm{our}}-(1+\\zeta))\\sqrt{1+\\epsilon^{2}}}{(1+\\zeta)\\sqrt{(x_{\\mathrm{our}}^{*})^{2}+\\epsilon^{2}}+2(m-2)a_{\\mathrm{our}}\\epsilon}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "617 Clearly, every candidate in $\\mathcal{G}$ minimises the social cost. Now, we show that $\\operatorname*{lim}_{n\\to\\infty}\\mathbb{P}[C{\\mathrm{~wins}}]=1.$ . ", "page_idx": 18}, {"type": "text", "text": "618 LEt Bernoulli random variables $\\{Y_{i}\\}_{i=1}^{n}$ denote the events that voter $i\\in\\mathcal{N}$ ranks candidate $C$ at the top. We have   \n619 $\\begin{array}{r}{\\sum_{i=1}^{n}\\mathbb{P}[Y_{i}=1]=q_{\\mathrm{MID}}(n-(m-1)\\lfloor a n\\rfloor)}\\end{array}$ and thus, i=1 Pn[Yi=1]= 1+m\u03b6 . Applying the law of large numbers,   \n620 we get that $\\mathbb{P}[\\sum_{i}Y_{i}\\geq\\frac{n}{m}]=\\ddot{1}$ as $n$ tends to $\\infty$ . However since every candidate in $\\mathcal{G}$ is equally likely to win, the event   \n621 $\\textstyle\\sum_{i}Y_{i}\\geq{\\frac{n}{m}}$ corresponds to the event that $C$ is the winner and thus, $\\begin{array}{r}{\\operatorname*{lim}_{n\\to\\infty}\\mathbb{P}[C\\,\\operatorname{wins}]=1}\\end{array}$ . Therefore we have, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\to\\infty}\\mathrm{DIST}^{(g)}(\\mathrm{PLU},n,m)\\geq\\frac{\\big(m q_{\\mathrm{our}}-(1+\\zeta)\\big)\\sqrt{1+\\epsilon^{2}}+(1+\\zeta)\\big(1+x_{\\mathrm{our}}^{*}\\big)}{(1+\\zeta)\\sqrt{(x_{\\mathrm{our}}^{*})^{2}+\\epsilon^{2}}+2(m-2)a_{\\mathrm{our}}\\epsilon}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "622 On applying the limit $\\epsilon,\\zeta\\rightarrow0$ and substituting for $q_{\\mathrm{MID}}$ and $q_{\\mathrm{OUT}}$ , we get the desired lower bound by combining the   \n623 results from the two constructions. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "624 E Proof of Theorem 3 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "625 Theorem 7. Restatement of Theorem 3 For every $\\epsilon>0,m\\ge2$ and $n\\geq4$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathsf{D I S T}^{(g)}(\\mathrm{COP},n,m)\\le4m(m-1)\\exp\\Big(\\displaystyle\\frac{-n^{(\\frac{1}{2}+\\epsilon)}+8}{2\\big(2n^{(\\frac{1}{2}-\\epsilon)}-1\\big)}\\Big)\\,\\big(\\hat{g}_{\\mathrm{MID}}+\\hat{g}_{\\mathrm{OUT}}\\big)^{2}\\,}\\\\ {+\\,\\operatorname*{max}\\Big(\\Big(\\displaystyle\\frac{2\\hat{g}_{\\mathrm{MID}}}{1-n^{-(\\frac{1}{2}-\\epsilon)}}-1\\Big)^{2},\\Big(\\displaystyle\\frac{2\\hat{g}_{\\mathrm{OUT}}}{1-n^{-(\\frac{1}{2}-\\epsilon)}}+1\\Big)^{2}\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "626 For every $m\\geq2$ , we have lim DIST(g)(COP, n, m) \u2264max (2g\u02c6MID \u22121)2 , (2g\u02c6OUT + 1)2  . n\u2192\u221e ", "page_idx": 18}, {"type": "text", "text": "627 Proof. Recall that $B\\in A$ minimises the social cost, and $\\{A_{j}\\}_{j\\in[m-1]}$ denotes the set $A\\,\\backslash\\,B$ . ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathsf{D I S T}^{(g)}(\\mathbf{COP},n,m)=\\operatorname*{sup}_{d\\in\\mathcal{M}(N\\cup\\mathcal{A})}\\left(\\sum_{j=1}^{m-1}\\mathbb{P}[A_{j}\\mathrm{~wins}]\\frac{\\mathbf{SC}(A_{j},d)}{\\mathbf{SC}(B,d)}+\\mathbb{P}[B\\mathrm{~wins}]\\right)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "628 Consider a Copeland winner $W$ . As noted by prior work [1], $W$ must be in the uncovered set of the tournament graph,   \n629 and one of the following two cases must be true. ", "page_idx": 18}, {"type": "text", "text": "630 \u2022 $W$ defeats $B$ . ", "page_idx": 18}, {"type": "text", "text": "631 \u2022 There exists a candidate $Y\\in A$ s.t. $W$ defeats $Y$ and $Y$ defeats $B$ . ", "page_idx": 18}, {"type": "text", "text": "632 For every $j\\,\\in\\,[m-1]$ , we now bound the probability of $A_{j}$ being the winner. For every $j\\,\\in\\,[m-1]$ , we define   \n633 Bernoulli random variables $\\{Y_{i,j}\\}_{i=1}^{n}$ denoting the event that voter $i$ ranks candidate $A_{j}$ over candidate $B$ . From   \n634 Equation 3, we have that Yi,j \u223cBern g dd((ii,,ABj)) . For every distinct $j,k\\in[m-1]$ , we define Bernoulli random   \n635 variables $\\{Z_{i,j,k}\\}_{i=1}^{n}$ denoting the event that voter $i$ ranks candidate $A_{j}$ over $\\begin{array}{r}{A_{k}.\\ Z_{i,j,k}\\sim\\mathrm{Bern}(g\\left(\\frac{d(i,A_{k})}{d(i,A_{j})}\\right))}\\end{array}$ . ", "page_idx": 18}, {"type": "text", "text": "636 Observe that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{P}[A_{j}\\mathrm{~wins}]\\leq\\mathbb{P}\\left(\\sum_{i=1}^{n}Y_{i,j}\\geq\\frac{n}{2}\\bigcup_{k\\in[m-1]\\setminus\\{j\\}}\\left(\\sum_{i=1}^{n}Z_{i,j,k}\\geq\\frac{n}{2}\\cap\\sum_{i=1}^{n}Y_{i,k}\\geq\\frac{n}{2}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "637 Let $\\alpha_{j}$ denote the expected value of the random variable $\\sum_{i=1}^{n}Y_{i,j}$ , i.e., the expected number of voters who rank   \n638 candidate $A_{j}$ over $B$ . ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\alpha_{j}:=\\sum_{i=1}^{n}\\mathbb{E}[Y_{i,j}]=\\sum_{i=1}^{n}g\\left({\\frac{d(i,B)}{d(i,A_{j})}}\\right){\\mathrm{~for~every~}}j\\in[m-1].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "639 Let $\\beta_{j,k}$ denote the expected value of the random variable $\\textstyle\\sum_{i=1}^{n}Z_{i,j,k}$ , i.e., the expected number of voters who rank   \n640 candidate $A_{j}$ over $A_{k}$ . ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\beta_{j,k}:=\\sum_{i=1}^{n}\\mathbb{E}[Z_{i,j,k}]=\\sum_{i=1}^{n}g\\left({\\frac{d(i,A_{k})}{d(i,A_{j})}}\\right){\\mathrm{~for~every~}}j\\in[m-1].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "641 Similar to Equation (31), we have the following bound: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{P}\\left(\\underset{m=1}{\\overset{n}{\\sum}}\\gamma_{*,j}\\geq\\frac{n}{2}\\right)=\\mathbb{P}\\left(\\underset{m=1}{\\overset{n}{\\sum}}\\gamma_{*,j}\\geq\\alpha_{j}\\left(1+\\frac{n}{2\\alpha_{j}}-1\\right)\\right)}&{}\\\\ {\\underset{=}{\\overset{n}{\\leq}}\\left(\\underset{(2n_{j})}{\\overset{n}{\\sum}}\\underset{m=1}{\\overset{n}{\\sum}}\\right)\\underset{(2n_{j})\\leq0}{\\overset{n}{\\sum}}}\\\\ &{\\leq\\left(\\frac{2n_{j}}{3\\alpha_{j}}\\right)^{2}\\left(\\underset{m=1}{\\overset{n}{\\sum}}\\left(-\\frac{\\alpha_{j}}{n/2}\\right)\\right)^{(1)-2}e^{\\frac{\\alpha_{j}}{2}}}\\\\ &{\\overset{()}{\\leq}\\left(\\frac{2n_{j}}{n}\\right)^{2}e^{\\frac{\\alpha_{j}}{2}}\\left(\\left(1-\\alpha^{-(1-s)}\\right)\\coth\\left(\\frac{n_{j}}{n}\\frac{\\frac{n_{j}}{n}-\\frac{\\alpha_{j+1}}{2}}{2-\\frac{\\alpha_{j}}{2}}\\right)\\right)^{\\frac{n-1}{2}}}\\\\ &{=\\left(\\frac{2n_{j}}{n}\\right)^{2}\\left(1-n^{-(1-s)}\\right)^{(n/2-s)}\\exp\\left(\\frac{n_{j}!+1}{2}\\right)}\\\\ &{\\overset{()}{\\leq}\\left(\\frac{2n_{j}}{n}\\right)^{2}\\exp\\left(\\frac{-2n_{j}-(1-s)!(n/2-2)}{2-n^{-(1-s)}}+\\frac{n_{j}!+1}{2}\\right)}\\\\ &{=\\left(\\frac{2n_{j}}{n}\\right)^{2}\\exp\\left(\\frac{-(n_{j}+1)+3}{(2-n^{-(1-s)})^{2}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "642 From Equation (31) in the proof of Theorem 1, we have ", "page_idx": 19}, {"type": "text", "text": "643 ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\displaystyle\\left(\\sum_{i=1}^{n}Y_{i,j}\\geq\\frac{n}{2}\\right)\\le\\left(\\frac{2\\alpha_{j}}{n}\\right)^{2}\\exp\\left(\\frac{-n^{(\\frac{1}{2}+\\epsilon)}+8}{2(2n^{(\\frac{1}{2}-\\epsilon)}-1)}\\right)\\,\\mathrm{if}\\,\\alpha_{j}\\le\\frac{n}{2}-\\frac{n^{(1/2+\\epsilon)}}{2}.}\\\\ &{\\mathrm{Similarly,~}\\,\\mathbb{P}\\left(\\sum_{i=1}^{n}Z_{i,j,k}\\ge\\frac{n}{2}\\right)\\le\\left(\\frac{2\\beta_{j,k}}{n}\\right)^{2}\\exp\\left(\\frac{-n^{(\\frac{1}{2}+\\epsilon)}+8}{2(2n^{(\\frac{1}{2}-\\epsilon)}-1)}\\right)\\,\\mathrm{if}\\,\\beta_{j,k}\\le\\frac{n}{2}-\\frac{n^{(1/2+\\epsilon)}}{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "644 Consider two exhaustive cases on candidate $A_{j}$ and define an event $E_{j}$ for every $j\\in[m-1]$ . We compute the expected   \n645 fraction of votes on pairwise comparisons. The event $E_{j}$ denotes the existence of an at-most two hop directed path   \n646 from a candidate $A_{j}$ to candidate $B$ for Copeland such that the expected fraction of votes on all edges along that path   \n647 exceed n \u2212n(1/2+\u03f5) . Recall that we only considered one hop path for the case of PLU in the proof of Theorem 1. ", "page_idx": 19}, {"type": "equation", "text": "$$\nE_{j}:=\\left(\\alpha_{j}\\geq\\frac{n}{2}-\\frac{n^{(1/2+\\epsilon)}}{2}\\right)\\bigcup_{k\\in[m-1]\\backslash\\{j\\}}\\left(\\left(\\beta_{j,k}\\geq\\frac{n}{2}-\\frac{n^{(1/2+\\epsilon)}}{2}\\right)\\bigcap\\left(\\alpha_{k}\\geq\\frac{n}{2}-\\frac{n^{(1/2+\\epsilon)}}{2}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "648 If $E_{j}$ holds true, we can directly upper bound the ratio of the social cost of candidate $A_{j}$ to the social cost of candidate   \n649 $B$ using Lemma 2, which in turn provides a bound on the distortion. If $E_{j}$ does not hold, we apply the union bound and   \n650 Chernoff\u2019s bound to upper bound the probability of $A_{j}$ being the winner. By multiplying this probability bound with   \n651 the ratio of social costs obtained from Lemma 2, we derive a bound on the distortion.   \n652 Define $S:=\\{j\\,\\in\\,[m-1]\\,:\\,E_{j}$ is not true}. Furthermore, we define $\\mathcal{K}_{1}(j):=\\{j\\,\\in\\,[m-1]\\,:\\,\\alpha_{k}\\,\\geq\\,\\beta_{j,k}\\}$ and   \n653 $\\mathcal{K}_{2}(j):=\\{j\\in[m-1]:\\alpha_{k}<\\check{\\beta}_{j,k}\\}$ denotes complement of $\\kappa_{1}(j)$ for every $j\\in[m]$ . ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "654 From Equations (58) and (59), both of the following conditions 1 and 2 are satisfied for every $j\\in S$ . ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{1.\\ \\mathbb{P}\\left(\\sum_{i=1}^{n}Y_{i,j}\\geq\\frac{n}{2}\\right)\\leq\\left(\\frac{2\\alpha_{j}}{n}\\right)^{2}\\exp\\left(\\frac{-n^{(\\frac{1}{2}+\\epsilon)}+8}{2(2n^{(\\frac{1}{2}-\\epsilon)}-1)}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "656 2. For every $k\\in[m-1]\\setminus\\{j\\}$ , ", "page_idx": 20}, {"type": "text", "text": "657 ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left(\\sum_{i=1}^{n}Z_{i,j,k}\\geq\\frac{n}{2}\\right)\\leq\\left(\\frac{2\\beta_{j,k}}{n}\\right)^{2}\\exp\\left(\\frac{-n^{(\\frac{1}{2}+\\epsilon)}+8}{2(2n^{(\\frac{1}{2}-\\epsilon)}-1)}\\right)\\,\\mathrm{if}\\,k\\in\\mathcal{K}_{1}(j)\\ }\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "658 ", "page_idx": 20}, {"type": "equation", "text": "$\\begin{array}{r}{\\mathbb{P}\\left(\\sum_{i=1}^{n}Y_{i,k}\\geq\\frac{n}{2}\\right)\\leq\\left(\\frac{2\\alpha_{k}}{n}\\right)^{2}\\exp\\left(\\frac{-n^{(\\frac{1}{2}+\\epsilon)}+8}{2(2n^{(\\frac{1}{2}-\\epsilon)}-1)}\\right)\\,\\mathrm{if}\\,\\,k\\in\\mathcal{K}_{2}(j).}\\end{array}$ ", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "659 Furthermore, we define $\\gamma_{j}:=\\operatorname*{max}\\left(\\operatorname*{max}_{k\\in[m-1]\\setminus\\{j\\}}\\left(\\operatorname*{min}(\\alpha_{k},\\beta_{j,k})\\right),\\alpha_{j}\\right).$ ", "page_idx": 20}, {"type": "text", "text": "660 Since, for every Copeland winner $W$ , it must either defeat $B$ or there exists a $Y\\in A$ s.t. $W$ defeats $Y$ and $Y$ defeats $B$ .   \n661 Using union bound for every $j\\in S$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\left\\langle A\\right\\rangle\\operatorname*{min}\\right|\\leq\\mathbb{P}\\left[\\displaystyle\\sum_{i=1}^{n}Y_{i,j}\\geq\\frac{n}{2}\\right]+\\displaystyle\\sum_{k\\in[m-1]\\backslash\\{j\\}}\\mathbb{P}\\left[\\left(\\displaystyle\\sum_{i=1}^{n}Y_{i,k}\\geq\\frac{n}{2}\\right)\\cap\\left(\\displaystyle\\sum_{i=1}^{n}Z_{i,j,k}\\geq\\frac{n}{2}\\right)\\right]\\mathrm{~if~}j\\in S}\\\\ &{\\qquad\\qquad\\leq\\left(\\displaystyle\\frac{2\\alpha_{j}}{n}\\right)^{2}\\exp\\left(\\frac{-n^{(\\frac{1}{2}+\\epsilon)}+8}{2(2n^{(\\frac{1}{2}-\\epsilon)}-1)}\\right)+\\displaystyle\\sum_{k\\in K_{2}(j)}\\left(\\frac{2\\alpha_{k}}{n}\\right)^{2}\\exp\\left(\\frac{-n^{(\\frac{1}{2}+\\epsilon)}+8}{2(2n^{(\\frac{1}{2}-\\epsilon)}-1)}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\displaystyle\\sum_{k\\in K_{1}(j)}\\left(\\frac{2\\beta_{j,k}}{n}\\right)^{2}\\exp\\left(\\frac{-n^{(\\frac{1}{2}+\\epsilon)}+8}{2(2n^{(\\frac{1}{2}-\\epsilon)}-1)}\\right)\\mathrm{~if~}j}\\\\ &{\\qquad\\qquad\\qquad\\leq m\\left(\\frac{2\\gamma_{j}}{n}\\right)^{2}\\exp\\left(\\frac{-n^{(\\frac{1}{2}+\\epsilon)}+8}{2(2n^{(\\frac{1}{2}-\\epsilon)}-1)}\\right)\\mathrm{~if~}j\\in S.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "662 The last inequality follows from the definition of $\\gamma_{j}$ . ", "page_idx": 20}, {"type": "text", "text": "663 Furthermore from Lemma 2 and the definition of $\\gamma_{j}$ , 6 we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{SC}(A_{j},d)}{\\mathrm{SC}(B,d)}\\leq\\left(\\operatorname*{max}\\left(\\frac{n}{\\gamma_{j}}\\hat{g}_{\\mathrm{MID}}-1,\\frac{n}{\\gamma_{j}}\\hat{g}_{\\mathrm{oUT}}+1\\right)\\right)^{2}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "664 Using Equation (62) and (61) and applying $\\operatorname*{max}(a,b)\\leq a+b$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{P}[A_{j}\\mathrm{~wins}]\\frac{\\mathrm{SC}(A_{j},d)}{\\mathrm{SC}(B,d)}\\leq4m\\exp\\left(\\frac{-n^{(\\frac{1}{2}+\\epsilon)}+8}{2(2n^{(\\frac{1}{2}-\\epsilon)}-1)}\\right)\\left(\\hat{g}_{\\mathrm{MID}}+\\hat{g}_{\\mathrm{OUT}}\\right)^{2}\\mathrm{~if~}j\\in S.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "6This follows on splittingSSCC((ABj,,dd)) $\\begin{array}{r}{\\frac{S C(A_{j},d)}{S C(B,d)}=\\frac{S C(A_{j},d)}{S C(A_{k},d)}\\times\\frac{S C(A_{k},d)}{S C(B,d)}}\\end{array}$ and applying the lemma separately. We further use the fact that $\\begin{array}{r}{\\frac{1}{\\gamma}=\\operatorname*{min}\\left(\\operatorname*{min}_{k\\in[m-1]\\backslash\\{j\\}}\\left(\\operatorname*{max}(\\frac{1}{\\alpha_{k}},\\frac{1}{\\beta_{j,k}})\\right),\\frac{1}{\\alpha_{j}}\\right)^{\\!\\!}}\\end{array}$ ", "page_idx": 20}, {"type": "text", "text": "665 Recall that for every $j\\in[m-1]\\setminus S,E_{j}$ is satisfied. Let us further denote ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\hat{E}_{j}:=\\alpha_{j}\\geq\\frac{n}{2}-\\frac{n^{(1/2+\\epsilon)}}{2}\\;\\mathrm{and}\\;\\hat{D}_{j,k}:=\\left(\\beta_{j,k}\\geq\\frac{n}{2}-\\frac{n^{(1/2+\\epsilon)}}{2}\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "666 Observe that $E_{j}$ being satisfied implies either a) ${\\hat{E}}_{j}$ is satisfied or b) $\\exists k\\in[m-1]\\setminus\\{j\\}$ s.t $\\hat{E}_{k}$ and $\\hat{D}_{j,k}$ are satisfied.   \n667 We consider both cases separately. ", "page_idx": 21}, {"type": "text", "text": "668 Suppose ${\\hat{E}}_{j}$ is satisfied for some $j\\in[m-1]\\:\\backslash\\:S$ . Then we have from Lemma 2, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{S C(A_{j},d)}{S C(B,d)}\\leq\\operatorname*{max}\\left(\\frac{2\\hat{g}_{\\mathrm{MID}}}{\\left(1-n^{-(1/2-\\epsilon)}\\right)}-1,\\frac{2\\hat{g}_{\\mathrm{our}}}{\\left(1-n^{-(1/2-\\epsilon)}\\right)}+1\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "669 Now we consider case (b) where $\\hat{E}_{k}$ and $\\hat{D}_{j,k}$ are both satisfied for some $k\\in[m-1]\\setminus\\{j\\}$ . From Lemma 2 we have, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{S C(A_{j},d)}{S C(B,d)}\\leq\\operatorname*{max}\\left(\\left(\\frac{2\\hat{g}_{\\mathrm{MID}}}{\\left(1-n^{-(1/2-\\epsilon)}\\right)}-1\\right)^{2},\\left(\\frac{2\\hat{g}_{\\mathrm{our}}}{\\left(1-n^{-(1/2-\\epsilon)}\\right)}+1\\right)^{2}\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "670 Now combining Equations (63), (64), and (65), we have for any metric space $d\\in\\mathcal{M}(\\mathcal{N}\\cup\\mathcal{A})$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{or},n,m)\\leq\\left(\\displaystyle\\sum_{j\\in S}\\left(\\mathbb{P}[A_{j}\\operatorname*{win}]\\frac{\\mathrm{SC}(A_{j},d)}{\\mathrm{SC}(B,d)}\\right)+\\mathbb{P}[B\\operatorname*{wins}]+\\displaystyle\\sum_{j\\in[m-1]\\backslash S}\\left(\\mathbb{P}[A_{j}\\operatorname*{wins}]\\frac{\\mathrm{SC}(A_{j},d)}{\\mathrm{SC}(B,d)}\\right)\\right)}\\\\ &{-1)m\\exp\\left(\\displaystyle\\frac{-n^{(\\frac{1}{2}+\\epsilon)}+8}{2(2n^{(\\frac{1}{2}-\\epsilon)}-1)}\\right)(\\hat{g}_{\\mathrm{MID}}+\\hat{g}_{\\mathrm{oUT}})+\\operatorname*{max}\\left(\\,\\operatorname*{max}_{j\\in[m-1]\\backslash S}\\frac{S C(A_{j},d)}{S C(B,d)},1\\right)}\\\\ &{-1)m\\exp\\left(\\displaystyle\\frac{-n^{(\\frac{1}{2}+\\epsilon)}+8}{2(2n^{(\\frac{1}{2}-\\epsilon)}-1)}\\right)(\\hat{g}_{\\mathrm{MID}}+\\hat{g}_{\\mathrm{oUT}})+\\operatorname*{max}\\left(\\left(\\frac{2\\hat{g}_{\\mathrm{MID}}}{(1-n^{-(1/2-\\epsilon)}}-1\\right)^{2},\\left(\\frac{2\\hat{g}_{\\mathrm{oUT}}}{(1-n^{-(1/2-\\epsilon)})}+\\frac{2\\hat{g}_{\\mathrm{oUT}}}{(1-n^{-(1/2-\\epsilon)})}\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "671 $(a)$ follows from Equation (61) and the fact that $\\begin{array}{r}{\\sum_{j\\in S}\\Big(\\mathbb{P}[A_{j}\\ \\mathrm{wins}]\\frac{\\mathrm{SC}(A_{j},d)}{\\mathrm{SC}(B,d)}\\Big)+\\mathbb{P}[B\\ \\mathrm{wins}]\\leq\\operatorname*{max}\\left(\\underset{j\\in S}{\\operatorname*{max}}\\,\\frac{S C(A_{j},d)}{S C(B,d)},1\\right).}\\end{array}$   \n672 $(b)$ follows from combining Equations (63), (64), and (65). \u53e3 ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "673 F Proof of Theorem 4 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "674 Theorem (Restatement of Theorem 4). $\\mathrm{DIST}^{(g)}(\\mathrm{RD},m,n)\\leq(m-1)\\hat{g}_{\\mathrm{MID}}+1.$ ", "page_idx": 21}, {"type": "text", "text": "675 Proof. The probability of voter $i$ voting for candidate $W$ as its top candidate is upper bounded by $\\begin{array}{r}{g\\left(\\frac{d(i,B)}{d(i,W)}\\right)}\\end{array}$ which is   \n676 the probability that is ranked over . Therefore, under RD, the probability of winning satisfies: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{P}[W\\operatorname{wins}]\\leq{\\frac{1}{n}}\\left(\\sum_{i=1}^{n}g\\left({\\frac{d(i,B)}{d(i,W)}}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "677 Recall that we define the set of candidates in ${\\mathcal{A}}\\setminus B$ as $\\{A_{1},A_{2},\\ldots,A_{m-1}\\}$ . In the rest of the analysis we denote   \n678 $d(i,A_{j})$ by $y_{i,j}$ (for all $j\\in[m-1])$ and $d(i,B)$ by $b_{i}$ for every $i\\in[n]$ . We also denote $d(B,A_{j})$ by $z_{j}$ for every ", "page_idx": 21}, {"type": "text", "text": "679 $j\\in[m-1]$ . Now for every metric $d$ , we bound the distortion as follows. ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathfrak{g}_{\\mathbf{x}}\\mathbf{v}^{\\alpha,\\alpha}\\mathfrak{P}(\\mathbf{R}),\\mathfrak{m},n)\\leq\\displaystyle\\sum_{j=1}^{m-1}\\left(\\mathfrak{p}[A_{j}\\,\\mathfrak{m}\\sinh\\sum_{i=1}^{m-1}\\mathfrak{p}_{i,j}]+(1-\\displaystyle\\sum_{j=1}^{m-1}\\mathfrak{p}[A_{j}\\,\\mathfrak{m}\\sinh])\\right.}\\\\ &{\\qquad\\qquad\\qquad\\left.-\\displaystyle\\sum_{j=1}^{m-1}\\mathfrak{p}[A_{j}\\,\\mathfrak{m}\\sinh\\left(\\sum_{i=1}^{m-1}\\mathfrak{p}_{i,j}\\right)+1]+\\right.}\\\\ &{\\qquad\\qquad\\left.\\mathfrak{o}\\left(z\\right)\\displaystyle\\sum_{j=1}^{m-1}\\frac{1}{n}\\left(\\sum_{i=1}^{m}\\left(\\frac{b_{j}}{y_{i,j}}\\right)\\right)\\frac{\\sum_{i=1}^{m-1}(y_{i,j}-b_{i})}{\\sum_{i=1}^{m-1}b_{i}}+1\\right.}\\\\ &{\\qquad\\qquad\\left.\\leq\\displaystyle\\sum_{j=1}^{m-1}\\frac{1}{n}\\left(\\sum_{i=1}^{m}\\left(\\frac{b_{j}\\,\\partial_{j}}{y_{i,j}}\\right)\\right)\\frac{\\sum_{i=1}^{m-1}(y_{i,j}/z_{j}-b_{i}/z_{j})}{\\sum_{i=1}^{m-1}b_{i}/z_{j}}+1\\right.}\\\\ &{\\qquad\\qquad\\left.\\overset{(i)}{\\leq}\\displaystyle\\sum_{j=1}^{m-1}\\frac{\\left(\\sum_{i=1}^{m}y\\left(\\frac{b_{j}\\,\\partial_{j}}{y_{i,j}/z_{j}}\\right)\\right)}{\\sum_{i=1}^{m-1}b_{i}/z_{j}}+1\\right.}\\\\ &{\\qquad\\qquad\\qquad\\left.\\overset{(i)}{\\leq}\\displaystyle\\sum_{j=1}^{m-1}\\frac{\\mathfrak{p}\\left(\\frac{z\\,\\partial_{j}}{y_{i,j}/z_{j}}\\right)}{\\sum_{i=1}^{m-1}b_{i}/z_{j}}+1\\right.}\\\\ &{\\qquad\\qquad\\qquad\\left.\\overset{(i)}{\\leq}\\left(m-1\\right)\\displaystyle\\frac{g\\left(\\frac{z\\,\\partial_{j}}{x_{i,j}}\\right)}{\\sum_{i=1}^{m-1}b_{i}/z_{j}}+1=\\left(m-1\\right)\\mathfrak \n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "680 $(a)$ follows from Equation (66). ", "page_idx": 22}, {"type": "text", "text": "681 $(d)$ follows from the fact that $y_{i,j}-b_{i}\\le z_{j}$ which follows from triangle inequality. ", "page_idx": 22}, {"type": "text", "text": "682 $(e)$ follows from the following arguments by considering two cases namely $\\begin{array}{r}{\\frac{b_{i}}{z_{j}}\\leq1}\\end{array}$ and $\\begin{array}{r}{\\frac{b_{i}}{z_{j}}\\geq1}\\end{array}$ . ", "page_idx": 22}, {"type": "text", "text": "683 When $\\begin{array}{r}{\\frac{b_{i}}{z_{j}}\\leq1}\\end{array}$ and thus, $\\begin{array}{r}{\\frac{y_{i,j}}{z_{j}}\\geq1-\\frac{b_{i}}{z_{j}}}\\end{array}$ from triangle inequality. Similarly, we have $\\begin{array}{r}{\\frac{y_{i,j}}{z_{j}}\\geq\\frac{b_{i}}{z_{j}}-1}\\end{array}$ when $\\begin{array}{r}{\\frac{b_{i}}{z_{j}}\\geq1}\\end{array}$ . Thus, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\frac{g\\left(\\frac{b_{i}/z_{j}}{y_{i,j}/z_{j}}\\right)}{b_{i}/z_{j}}\\leq\\operatorname*{max}\\left(\\operatorname*{sup}_{x\\in(0,1)}\\frac{g\\left(\\frac{x}{1-x}\\right)}{x},\\ \\ \\operatorname*{sup}_{x\\in(1,\\infty)}\\frac{g\\left(\\frac{x}{x-1}\\right)}{x}\\right)\\mathrm{~for~every~}i\\in[n]}\\\\ &{\\Longrightarrow\\frac{\\sum_{i=1}^{n}g\\left(\\frac{b_{i}/z_{j}}{y_{i,j}/z_{j}}\\right)}{\\sum_{i=1}^{n}b_{i}/z_{j}}\\leq\\operatorname*{max}\\left(\\frac{g\\left(\\frac{x_{\\mathrm{sup}}^{\\ast}}{1-x_{\\mathrm{sup}}^{\\ast}}\\right)}{x_{\\mathrm{MD}}^{\\ast}},1\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "684 The last inequality follows from the fact thatg(xx\u22121 ) $\\frac{g(\\frac{x}{x-1})}{x}\\leq1$ when $x\\geq1$ . Further, we have $\\hat{g}_{\\mathrm{MID}}\\ge1$ for all valid $g$ . ", "page_idx": 22}, {"type": "text", "text": "685 G Proof of Theorem 6 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "686 Theorem (Restatement of Theorem 6). Let D $\\mathsf{I S T}_{P L}^{\\theta}\\big(\\mathbf{RD},m,n\\big)$ denote the distortion when the voters\u2019 rankings are   \n687 generated per the PL model with parameter \u03b8. We have limn\u2192\u221eDIST\u03b8P L(RD, m, n) \u22651 + (m\u221221)1/\u03b8.   \n688 Proof. We have a 1-D Euclidean construction. Let $B$ be at 0 and all other candidates $A\\setminus\\{B\\}$ be at $1.\\ m-1$ voters are   \n689 at 0, and one voter is at $t$ . We will set $t$ later by optimizing for the distortion.   \n690 The distortion for this instance is $\\begin{array}{r l r}{\\mathbb{P}[B\\mathrm{~wins}]\\;\\cdot\\;1\\;+\\;\\mathbb{P}[B\\mathrm{~loses}]\\;\\cdot\\;\\frac{n-t}{t}}&{=}&{\\frac{n-1}{n}\\;+\\;\\frac{1}{n}\\frac{t^{-\\theta}}{t^{-\\theta}+(m-1)(1-t)^{-\\theta}}\\;+}\\end{array}$   \n691 ${\\frac{1}{n}}\\,{\\frac{(m{-}1)(1{-}t)^{-\\theta}}{t^{-\\theta}{+}(m{-}1)(1{-}t)^{-\\theta}}}\\,{\\frac{n{-}t}{t}}$ . We drop the terms which are ${\\cal O}(1/n)$ to obtain $\\begin{array}{r}{1+\\frac{(m-1)(1-t)^{-\\theta}}{t(t^{-\\theta}+(m-1)(1-t)^{-\\theta})}}\\end{array}$ . This simplifies to   \n692 (1\u2212(tm)\u03b8\u2212+1()tm\u03b8\u2212\u221211)t\u03b8 . This is lower bounded by 1 + 1(+m(\u2212m1)\u2212t1\u03b8)\u2212t1\u03b8 . Setting t = (m \u22121)\u22121/\u03b8, we obtain a distortion lower   \n693 bound of 1 + (m\u22121)1/\u03b8. \u53e3 ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 23}, {"type": "text", "text": "Justification: We took care to make sure the claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope. ", "page_idx": 23}, {"type": "text", "text": "701 Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 23}, {"type": "text", "text": "710 2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: We have added a future work section which lays the open questions and limitations. ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 23}, {"type": "text", "text": "738 3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: All assumptions are mentioned clearly. All the proofs are provided, and we took care to make them correct to the best of our understanding. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 24}, {"type": "text", "text": "753 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: [NA] Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 24}, {"type": "text", "text": "786 5. Open access to data and code ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 24}, {"type": "text", "text": "789 Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/ CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/ CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "809 6. Experimental Setting/Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.   \n\u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 25}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. \u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). \u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) \u2022 The assumptions made should be given (e.g., Normally distributed errors). \u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean. \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified. \u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). \u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 25}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "845 Justification: [NA]   \n846 Guidelines:   \n847 \u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 25}, {"type": "text", "text": "\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 26}, {"type": "text", "text": "9. Code Of Ethics ", "page_idx": 26}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 26}, {"type": "text", "text": "857 Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "858 Justification: [NA] ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 26}, {"type": "text", "text": "865 10. Broader Impacts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 26}, {"type": "text", "text": "Justification: We have discussed the positive social impact of the design of voting rules and ways in which our paper can be instrumental towards it. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake proflies, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 26}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 26}, {"type": "text", "text": "895 Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "896 Justification: [NA]   \n897 Guidelines:   \n898 \u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 26}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 27}, {"type": "text", "text": "906 12. Licenses for existing assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 27}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not use existing assets. \u2022 The authors should cite the original paper that produced the code package or dataset. \u2022 The authors should state which version of the asset is used and, if possible, include a URL. \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset. \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. \u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. \u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. \u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 27}, {"type": "text", "text": "936 14. Crowdsourcing and Research with Human Subjects ", "page_idx": 27}, {"type": "text", "text": "937 Question: For crowdsourcing experiments and research with human subjects, does the paper include the full   \n938 text of instructions given to participants and screenshots, if applicable, as well as details about compensation   \n939 (if any)? ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. \u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 27}, {"type": "text", "text": "948 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 27}, {"type": "text", "text": "949   \n950   \n951   \n952   \n953   \n954   \n955   \n956   \n957   \n958   \n959   \n960   \n961 ", "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. \u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. \u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 28}]