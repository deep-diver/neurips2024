[{"type": "text", "text": "Multi-hypotheses Conditioned Point Cloud Diffusion for 3D Human Reconstruction from Occluded Images ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Donghwan Kim Tae-Kyun Kim KAIST {kdoh2522, kimtaekyun}@kaist.ac.kr ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "3D human shape reconstruction under severe occlusion due to human-object or human-human interaction is a challenging problem. Parametric models i.e. SMPL(- X), which are based on the statistics across human shapes, can represent whole human body shapes but are limited to minimally-clothed human shapes. Implicitfunction-based methods extract features from the parametric models to employ prior knowledge of human bodies and can capture geometric details such as clothing and hair. However, they often struggle to handle misaligned parametric models and inpaint occluded regions given a single RGB image. In this work, we propose a novel pipeline, MHCDIFF, Multi-hypotheses Conditioned Point Cloud Diffusion, composed of point cloud diffusion conditioned on probabilistic distributions for pixel-aligned detailed 3D human reconstruction under occlusion. Compared to previous implicit-function-based methods, the point cloud diffusion model can capture the global consistent features to generate the occluded regions, and the denoising process corrects the misaligned SMPL meshes. The core of MHCDIFF is extracting local features from multiple hypothesized SMPL(-X) meshes and aggregating the set of features to condition the diffusion model. In the experiments on CAPE and MultiHuman datasets, the proposed method outperforms various SOTA methods based on SMPL, implicit functions, point cloud diffusion, and their combined, under synthetic and real occlusions. Our code is publicly available at https://donghwankim0101.github.io/projects/mhcdiff. ", "page_idx": 0}, {"type": "image", "img_path": "E2JCQyYu0E/tmp/d692e0f625362c290aad96f59da5eed687224ecb31b46eb7f31abfd7820d70bb.jpg", "img_caption": ["Figure 1: Image to 3D shape. From the segmented images, containing occlusion due to interaction, MHCDIFF reconstructs 3D human shapes as point clouds. "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Realistic virtual humans play a significant role in various industries, such as metaverse, tele-presence, and game modeling. However, conventional methods require expensive artist efforts and complex scanning equipments, so they are not readily applicable. A more practical approach is to reconstruct high-fidelity 3D humans from 2D images taken in the wild. This is still an ongoing research task due to its challenges; people wear a wide variety of clothing styles and adopt diverse poses. Furthermore, human-object and human-human interaction, fundamental aspects of daily social life, make it more challenging due to severe occlusions. ", "page_idx": 1}, {"type": "text", "text": "Existing 3D human reconstruction methods cannot predict the pixel-aligned 3D shapes of humans robustly from occluded images. The parametric body models [27, 48, 67, 93, 75] have been widely used to reconstruct 3D human shapes. Several methods [11, 28, 34, 83, 26, 15, 10, 44, 82] predict the parameters of the statistical models and are robust to occlusion because they can be trained on large scale datasets [23, 56] and parametric models are well regularized with human body priors. However, the parametric models lack geometric details like clothing and hair, so these approaches cannot align the results to the subjects with loose clothing. More recently, 3D clothed human reconstruction methods [76, 77, 106, 92, 7, 91, 90, 95, 96], which are based on implicit functions and integrate the human body prior from the 3D body models, i.e SMPL [48, 67], present pixel-aligned detail shapes. Despite the impressive advances of the previous methods, they are not robust to occlusion because (1) small misalignment of estimated parametric models ruins the final shapes, (2) the implicit function takes features independently and cannot inpaint the invisible regions with missing image features, and (3) datasets [74, 87, 66] usually consists of segmented full-body images. ", "page_idx": 1}, {"type": "text", "text": "To address the aforementioned limitations, we propose MHCDIFF (Multi-hypotheses Conditioned Point Cloud Diffusion). (1) Several existing methods [6, 60, 73, 78, 79, 35, 8, 61, 81, 14] predict multiple SMPL meshes to model uncertainty due to occlusions. The sampled distribution is also important prior knowledge of human motions, but none of the existing work utilizes the distribution for pixel-aligned 3D human reconstruction. We leverage the multi-hypotheses to be robust on the misalignment of each sample. (2) We adopt denoising diffusion probabilistic models (DDPMs) [20] to take global consistent features and generate the invisible regions. Diffusion based methods generate 3D shapes by denoising point clouds [50, 108, 63, 22], latent [101, 62, 36], neural fields [69], 3D Gaussian [84] or meshes [46]. We adopt the unstructured point clouds to project pixel-aligned image features at each diffusion step. (3) Additionally, we synthesize partial body images by random masking [107], augmenting the limited datasets. ", "page_idx": 1}, {"type": "text", "text": "Specifically, our goal is pixel-aligned and detailed 3D human reconstruction in a robust manner to occlusion in images. Given a single occluded RGB image, we extract 2D features and generate multiple plausible SMPL hypotheses using an off-the-shelf method [4, 14]. The proposed method, MHCDIFF, Multi-hypotheses Conditioned Point Cloud Diffusion, performs the diffusion process to denoise a randomly-sampled point cloud into a target human shape. To reconstruct a pixel-aligned 3D shape and leverage the human body prior, the diffusion process is conditioned on the projected image feature (Sec. 3) and local features extracted from SMPL (Sec. 4.2). The key of MHCDIFF is a novel conditional diffusion process with multiple hypotheses (Sec. 4.3), which is not sensitive to misaligned SMPL estimation. Given global 2D features and the distribution of hypotheses, the denoising diffusion model can generate the occluded parts (Sec. 4.4) ", "page_idx": 1}, {"type": "text", "text": "We train MHCDIFF on randomly masked THuman2.0 dataset [85]. Our experiments on CAPE dataset [53, 68] with synthesized occlusion and MultiHuman dataset [105] with real-world interaction demonstrate that MHCDIFF reconstructs pixel-aligned 3D human shapes robustly to various occlusion ratios and achieves state-of-the-art performance. Our main contributions are as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We introduce a novel multi-hypotheses conditioning mechanism that effectively captures the distribution of multiple plausible SMPL meshes. It is robust to the noise of each SMPL estimation due to the occlusion of given images. To the best of our knowledge, MHCDIFF is the first work that extends the multi-hypotheses SMPL estimation to pixel-aligned 3D human reconstruction. ", "page_idx": 1}, {"type": "text", "text": "\u2022 We adopt point cloud diffusion model to capture the global consistent features and inpaint the invisible parts. Unlike the previous implicit function, the misaligned SMPL estimation can be corrected during the denoising process. The point cloud diffusion model also offers detailed human meshes. ", "page_idx": 1}, {"type": "text", "text": "\u2022 MHCDIFF, trained on synthesized partial body images, outperforms previous methods on occluded and even full-body images. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Diffusion models for point clouds ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Over the past years, denoising diffusion probabilistic models (DDPMs) [20] have been applied to point clouds. For unconditional generation, Luo et al. [50], Zhou et al. [108] and LION [101] use PointNet [70], Point-Voxel-CNN [47] and latent space, respectively. PointInfinity [22] tackles the quadratic complexity of transformer [89], and generates high-resolution point clouds with a fixed-size latent vector. Otherwise, Point-E [63] is a text-conditioned generation model using CLIP [72] and PDR [51] is a point cloud completion method from partial point clouds. $\\mathrm{PC^{2}}$ [57], which is the baseline of MHCDIFF, reconstructs the point cloud conditioned on projected image features (please refer to Sec. 3 for more details). ", "page_idx": 2}, {"type": "text", "text": "2.2 Explicit-shape-based human reconstruction ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Parametric models [27, 48, 67, 93, 75] have been primary representations for 3D human reconstruction. Due to the strength that they capture the statistics across a large corpus of human shapes, a lot of work [11, 28, 34, 83, 26, 15, 10, 44, 82] reconstructs 3D body meshes from an RGB image. To reduce the gaps between the image and parameter space of the statistical models and improve image alignment, they propose intermediate representations or additional supervisions, such as semantic segmentation [64, 94, 33, 100] and keypoints [9, 41]. To model the uncertainty due to occlusions or depth ambiguities, some work proposes multi-hypotheses [6], heatmaps [60], probability density functions [73, 78, 79, 35] or diffusion models [8, 61, 102, 81, 39]. ProPose [14] adopts the matrix Fisher distribution [13, 30] over $S O(3)$ for the joint rotation conditioned on the von Mises-Fisher distribution [55] for the unit directions of bones, which is not only mathematically correct but also learning friendly (please refer to Sec. 3 for more details). However, these methods are limited to recovering minimally-clothed humans and lack the ability to capture geometric details such as clothing and hair. ", "page_idx": 2}, {"type": "text", "text": "Several works aim at modeling geometric details in explicit shapes such as meshes, voxels, depth maps and point clouds. Mesh-based methods [1, 2, 3, 37, 109, 5, 25] model 3D offsets on the vertices of SMPL [48], but they do not generalize on loose clothing such as skirts and dresses. Voxel-based methods [24, 88, 17, 86] reconstruct 3D human shapes in fine-grained voxel representations. However, free-form 3D reconstruction is challenging without prior, and they need high computation costs to output high-resolution 3D shapes. Point-cloud-based methods [52, 99, 54, 19, 85] model point clouds of clothing humans. Han et al. [19] estimate depth maps based on different body parts, and convert the depth maps into point clouds. Tang et al. [85], the most related work, reconstruct 3D humans with point cloud diffusion from an RGB image. First, they convert the estimated SMPL mesh and depth map from the RGB image to point clouds. Conditioned on this point cloud, the conditional diffusion model refines the point cloud. However, they only handle complete images without occlusion and are not robust to misaligned SMPL estimation. ", "page_idx": 2}, {"type": "text", "text": "2.3 Implicit-function-based human reconstruction ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Implicit-function-based methods regress occupancy fields [58] or signed distance fields (SDF) [65] utilizing Multi-Layer Perceptron (MLP) decoders as implicit functions (IF). PIFu [76] and PIFuHD [77], which are pioneering works, extract pixel-aligned image features for clothed 3D human reconstruction. Later works [106, 92, 7, 91, 90, 40, 95, 103, 104, 96] leverage parametric models or body keypoints as prior information on the human body. They extract global features from voxelized SMPL meshes with a 3D encoder [106, 90] or local features such as signed distances and normals from SMPL meshes [92, 95, 103, 104] or both [7, 96]. The use of global features helps regularize global shapes and ensure consistency and local features help reconstruct local details. However, the global encoder is sensitive to global pose changes of SMPL and decreases the performance given misaligned SMPL estimation due to occlusion. The local features do not contain the global consistent features and cannot inpaint the occluded parts. Wang et al. [90] aim to reconstruct complete 3D shapes from occluded images by primarily using the generative global encoder with a discriminator, but only assuming the accurate SMPL meshes. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "3 Preliminary ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "$\\mathbf{PC}^{2}$ [57]. The projection-conditioned point cloud diffusion model is proposed for single-view 3D shape reconstruction. Denoising diffusion probabilistic model [20], which is the foundation of this framework, learns to recurrently transform noise $X_{T}\\sim\\mathcal{N}(0,\\mathbf{I})$ into a sample from the target data distribution $X_{0}\\sim q(X_{0})$ over a series of steps. In order to learn this denoising process, a neural network is trained $\\bar{\\mathcal{F}}_{\\theta}(X_{t-1}|X_{t})\\approx q(X_{t-1}|\\bar{X}_{t})$ . To reconstruct geometrically consistent 3D point clouds from single RGB images $I\\in\\mathbb{R}^{H\\times W\\times3}$ , 2D feature map $\\mathcal{E}(I)\\in\\mathbb{R}^{h\\times w\\times c}$ is projected onto the partially denoised points at each step in the diffusion process. Therefore, $\\underline{{\\mathcal{F}}}_{\\theta}(\\cdot):\\mathbb{R}^{(3+c)N}\\to\\mathbb{R}^{3N}$ is a function that predicts the noise $\\epsilon\\in\\mathbb{R}^{3N}$ from the point cloud $X_{t}\\in\\mathbb{R}^{3N}$ and the projected features $X_{t}^{p r o j}\\in\\mathbb{R}^{c N}$ , where $c$ is the number of feature channels. ", "page_idx": 3}, {"type": "text", "text": "ProPose [14]. Recovering accurate body meshes and 3D joint rotations from single images remains a challenging problem, particularly in cases of severe occlusion, including self-occlusion and occlusion from other subjects or objects. ProPose [14] addresses this limitation by modeling the probability distributions for human mesh recovery. Since the pose parameters $\\theta\\in\\mathbb{R}^{72}$ of SMPL [48] represent the 3D rotation of each joint and the root orientation, they adopt the matrix Fisher distribution [13, 30] over $S O(3)$ . Due to the gaps between the RGB images and the rotation representations, the neural network cannot easily model the distribution. ProPose [14] also introduces 3D unit vectors for bone directions as the corresponding observation on the previous matrix Fisher distribution as the prior. Leveraging Bayesian inference, they model the posterior distribution of the joint rotations from the prior distribution and observation. ", "page_idx": 3}, {"type": "text", "text": "4 MHCDIFF: Multi-hypotheses Conditioned Point Cloud Diffusion ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "4.1 Overview ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Our work aims at reconstructing pixel-aligned 3D human shape as a point cloud given a single occluded RGB image via conditional point cloud diffusion, as shown in Fig. 2. Formally, the diffusion model $\\mathcal{F}_{\\theta}(\\cdot)$ learns the conditional distribution $q(X_{0}|I)$ of 3D human shapes given the RGB images $I\\in\\mathbb{R}^{H\\times W\\times3}$ . Following $\\mathrm{PC^{2}}$ , we extract the 2D feature map $\\mathcal{E}(I)\\in\\mathbb{R}^{h\\times w\\times c}$ using ViT [12], to capture the details in the images. The image features are projected onto the partially denoised points: $X_{t}^{p r o j}=\\Pi(\\mathcal{E}(I),X_{t})$ , where $\\Pi$ is the projection function. This helps obtain pixel-aligned detailed body shapes. Additionally, the diffusion model is conditioned on the local features $X_{t}^{S\\overline{{M P L}}}$ from SMPL mesh $S$ to exploit statistical human body priors to complete 3D shapes from occluded body parts (Sec. 4.2). However, the SMPL estimation from single occluded RGB images has a high probability of large errors. To tackle this, we propose a novel multi-hypotheses conditioned diffusion model that considers the distribution of multiple plausible SMPL meshes $\\{S_{i}\\}_{i\\in\\{1,...,s\\}}$ (Sec. 4.3). Given the partially denoised point cloud $X_{t}$ , the projected image features $X_{t}^{p r o j}$ , and the local features from SMPL $X_{t}^{S M P L}$ , MHCDIFF predicts the noise $\\epsilon$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{F}_{\\theta}(X_{t},X_{t}^{p r o j},X_{t}^{S M P L})=\\epsilon.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We also discuss how MHCDIFF takes the generative property and the global consistent features to reconstruct occluded parts (Sec. 4.4). ", "page_idx": 3}, {"type": "text", "text": "4.2 Local features from SMPL ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Given the SMPL (or SMPL-X) mesh $S$ and the partially denoised point cloud $X_{t}$ at $t$ -th diffusion step, we extract the local features $X_{t}^{S M P L}$ as: ", "page_idx": 3}, {"type": "equation", "text": "$$\nX_{t}^{S M P L}=[\\gamma(d(X_{t}|S)),{\\pmb n}(X_{t}|S)],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "image", "img_path": "E2JCQyYu0E/tmp/5fc6bc698045b5b79d19facb77bc809e75dcc76cd4fb7a43c1ee7e9a4abfe465.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 2: (Left) Overview of MHCDIFF. Given an occluded image $I$ , MHCDIFF reconstructs 3D human shape as a point cloud. First, we extract the 2D feature map $\\mathcal E(I)$ and hypothesize pose and shape parameters of multiple plausible SMPL meshes $\\{S_{i}\\}_{i\\in\\{1,...,s\\}}$ . Our method consists of the conditioned point cloud diffusion model (Sec. 4.4). We project the 2D image features to capture details of the image (Sec. 3) and extract local features from multiple hypothesized SMPL meshes to leverage human body priors (Sec. 4.3) (Upper Right) The details of local features (Sec. 4.2). The signed distance field is visualized in positive and negative regions. The arrows indicate normal vectors $\\mathbfit{\\Delta}$ . (Lower Right) The details of multi-hypotheses (Sec. 4.3). We can consider the whole distribution during denoising process with the argmax $\\bar{i}$ , and the denoising can be approximated by red arrows. However, it is sensitive to extreme samples of the distribution, so we condition the mean of occupancy values, which is visualized by transparency, and the denoising can be approximated by blue arrows. ", "page_idx": 4}, {"type": "text", "text": "where $d(\\cdot):\\mathbb{R}^{3}\\rightarrow\\mathbb{R}$ and $\\pmb{n}(\\cdot):\\mathbb{R}^{3}\\rightarrow\\mathbb{R}^{3}$ are the signed distance and normal obtained from the closest surface of SMPL mesh respectively. In order to map scalar values to a higher dimensional space, we adopt an encoding inspired by the positional encoding in NeRF [59]: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\gamma(d)=(\\sin(2^{0}\\pi d),\\cos(2^{0}\\pi d),...,\\sin(s^{L-1}\\pi d),\\cos(s^{L-1}\\pi d)).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The local features $X_{t}^{S M P L}\\in\\mathbb{R}^{(2L+3)N}$ , which contain the signed distance and normal vector from SMPL, are used to predict the noise $\\epsilon$ of the point cloud $X_{t}$ . The local property, which is independent of global pose, helps MHCDIFF to generalize well in diverse SMPL estimation due to occlusion and capture local details. ", "page_idx": 4}, {"type": "text", "text": "4.3 Multi-hypotheses condition ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The local features are robust to noisy SMPL estimation, but cannot correct the SMPL estimation errors. Following previous multi-hypotheses human pose estimation [6, 43, 18, 21, 8], MHCDIFF takes multi-hypotheses SMPL meshes from estimated distributions and predicts the most plausible outputs. We modify Eq. 2 to handle multiple sampled SMPL meshes $\\{\\bar{S}_{i}\\}_{i\\in\\{1,\\dots,s\\}}$ using ProPose [14] as an off-the-shelf method: ", "page_idx": 4}, {"type": "equation", "text": "$$\nX_{t}^{S M P L}=[\\gamma(d(X_{t}|S_{\\bar{i}})),{\\pmb n}(X_{t}|S_{\\bar{i}})],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\bar{i}=a r g m i n_{i\\in\\{1,...,s\\}}d(X_{t}|S_{i})$ , which semantically means that each point follows the closest SMPL mesh $S_{\\bar{i}}$ to consider all plausible samples in denoising steps. However, each point gets conditions from only one sample and cannot leverage off-the-shelf probability distributions. In addition to the local features, we also adopt occupancy values: ", "page_idx": 4}, {"type": "equation", "text": "$$\nX_{t}^{S M P L}=[\\frac{1}{s}\\sum_{i=1}^{s}\\gamma(o(X_{t}|S_{i})),\\gamma(d(X_{t}|S_{\\bar{i}})),{\\pmb n}(X_{t}|S_{\\bar{i}})],\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $o(\\cdot):\\mathbb{R}^{3}\\to\\{0,1\\}$ is the occupancy function of the given SMPL mesh, which is a binary signal while the signed distance is continuous. With the mean occupancy and max signed distance, MHCDIFF can assume all distributions with their respective probabilities. The proposed multihypotheses conditioning can take an arbitrary number of SMPL, SMPL-X, and their combined. ", "page_idx": 5}, {"type": "text", "text": "4.4 Conditioned point cloud diffusion model ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Finally, $\\mathcal{F}_{\\theta}(\\cdot)\\,:\\,\\mathbb{R}^{(3+c+4L+3)N}\\,\\rightarrow\\,\\mathbb{R}^{3N}$ predicts the noise $\\epsilon\\,\\in\\,\\mathbb{R}^{3N}$ given the concatenation of partially denoised point cloud $X_{t}\\in\\mathbb{R}^{3N}$ , projected image features $X_{t}^{p r o j}\\in\\mathbb{R}^{c N}$ , and local features from SMPL $X_{t}^{S M P L}\\in\\mathbb{R}^{(4L+3)N}$ (Eq. 1). Notably, we do not need any learnable parameters to extract the local features from SMPL and aggregate the features of multiple SMPL meshes. We freeze the pre-trained 2D image encoder, so it is straightforward to train the diffusion model without additional training strategies. ", "page_idx": 5}, {"type": "text", "text": "The point cloud diffusion model of MHCDIFF takes the role of the decoder of previous implicitfunction-based methods. Given the encoded features from RGB images or SMPL meshes, the decoder predicts 3D shapes such as point clouds, occupancy fields, or signed distance fields. The implicitfunction-based methods need to sample the query points randomly, so the decoder has been primarily Multi-Layer Perceptron (MLP), which takes the input points independently. MHCDIFF consists of the point cloud diffusion model instead of MLP because (1) the point cloud model considers the global consistent features, (2) the diffusion model has the generative properties, and (3) the denoising process approximates correcting the misaligned SMPL estimation. Given the globally encoded image features $X_{t}^{p r o j}$ and the local features from SMPL $X_{t}^{S M P L}$ , MHCDIFF can inpaint or restore invisible body parts and is robust to noisy SMPL estimation due to occlusion. ", "page_idx": 5}, {"type": "text", "text": "Algorithm 1 Pseudocode of learning pipeline of MHCDIFF ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Require: $\\alpha_{1:T}$ : diffusion noise scheduling   \n1: repeat   \n2: Sample $X_{0}$ from $q(X_{0})$   \n3: Load the corresponding image $I$ and ground truth SMPL-X $S$   \n4: $t\\sim\\mathrm{Uniform}(\\{\\bar{1},...,T\\})$   \n5: $\\epsilon\\sim\\mathcal{N}(0,\\bf{I})$   \n6: $X_{t}=\\sqrt{\\alpha_{t}}X_{0}+\\sqrt{1-\\alpha_{t}}\\epsilon$   \n7: Xtproj= \u03a0(E(I), Xt) \u25b7Project image features (Sec. 3)   \n8: XtSMP L= [\u03b3(o(Xt|S)), \u03b3(d(Xt|S)), n(Xt|S)] Extract local features from SMPL (Sec. 4.2   \n9: Take gradient descent step on \u2207\u03b8  \u03f5 \u2212F\u03b8(Xt, Xtproj, XtSMP L) \u25b7Point cloud diffusion model (Sec. 4.4)   \n10: until converged ", "page_idx": 5}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Implementation. We use the Pytorch3D library [71] for image feature projection (Sec. 3) and the kaolin library [16] to extract local features from SMPL (Sec. 4.2). MHCDIFF is trained with batch size 8 in 100,000 steps. We use MSN [4] as the image feature encoder. We use AdamW [31] with $\\beta=(0.9,0.999)$ and a learning rate which is decayed linearly from 0.0002 to 0. For diffusion noise schedule, we use linear scheduling from $1\\cdot10^{-5}$ to $8\\cdot10^{-3}$ with warmup. For inference, we denoise the point cloud for 1, 000 steps. The training process takes approximately 1 day on a single 24GB NVIDIA RTX 4090 GPU with 28M learnable parameters. ", "page_idx": 5}, {"type": "text", "text": "Learning. We synthesize the THuman2.0 dataset [98], which contains 526 high-fidelity textured scans with corresponding SMPL-X ftis. We use 500 subjects for training and the others for validation. ", "page_idx": 5}, {"type": "text", "text": "Require: Input image $I$   \n1: Sample $X_{T}$ from $\\mathcal{N}(0,\\mathbf{I})$   \n2: Estimate single or multi SMPL(-X) meshes $\\{S_{i}\\}_{i\\in\\{1,\\dots,s\\}}$   \n3: for all $t$ from $T$ to 1 do   \n4: $z\\sim\\mathcal{N}(0,\\mathbf{I})$ if $t>1$ else $z=0$   \n5: $X_{t}^{p r o j}=\\Pi(\\mathcal{E}(I),X_{t})$ \u25b7Project image features (Sec. 3)   \n6: for all $i$ from 1 to $s$ do   \n7: Compute $o(X_{t}|S_{i})$ , $d(X_{t}|S_{i})$ , and $n(X_{t}|S_{i})$ \u25b7Can be accelerated by kaolin [16]   \n8: end for   \n9: $\\begin{array}{r l}&{\\bar{i}\\leftarrow a r g m i n_{i\\in\\{1,\\dots,s\\}}d(X_{t}|S_{i})}\\\\ &{X_{t}^{S M P L}=[\\frac{1}{s}\\sum_{i=1}^{s}\\gamma(o(X_{t}|S_{i})),\\gamma(d(X_{t}|S_{\\bar{i}})),n(X_{t}|S_{\\bar{i}})]}\\\\ &{\\rightharpoonup\\mathrm{~Multi-hypotheses~of~}}\\\\ &{\\hat{\\epsilon}\\leftarrow\\mathcal{F}_{\\theta}(X_{t},X_{t}^{p r o j},X_{t}^{S M P L})}\\\\ &{X_{t-1}\\leftarrow\\frac{1}{\\sqrt{\\alpha_{t}}}(X_{t}-\\frac{1-\\alpha_{t}}{\\sqrt{1-\\bar{\\alpha_{t}}}}\\hat{\\epsilon})+\\alpha_{t}z}\\end{array}$   \n10:   \nonditioning (Sec. 4.3)   \n11: 12: DDPM [20] sampling   \n13: end for   \n14: return $X_{0}$ ", "page_idx": 6}, {"type": "text", "text": "We render each human subject from 36 multiple viewpoints and randomly mask the images, resulting in partially occluded body images. We use the farthest point sampling operation to sample 16,384 points from each GT scan. During the training, local features XtSMP L are extracted from a single corresponding GT SMPL-X. The learning pipeline is presented in Algorithm 1. ", "page_idx": 6}, {"type": "text", "text": "Inference. First, we use the CAPE dataset [53, 68] with 150 textured scans. Similar to the training stage, we render each subject from 3 multiple viewpoints and randomly mask the images. During the inference, local features $X_{t}^{S M P L}$ are extracted from multiple sampled SMPL or single estimated SMPL-X. We sample 10 SMPL meshes for our experiments. To further show the generalizability on the real-world interaction, we also evaluate MHCDIFF on the MultiHuman [105] and Hi4D [97] dataset. MultiHuman, which includes the diverse interaction with objects and people, provides 3D textured scans, so we render each subject from 3 multiple viewpoints. We evaluate the performance of MHCDIFF qualitatively on Hi4D, which includes close human-human interaction with high-fidelity meshes. The inference pipeline is presented in Algorithm 2. ", "page_idx": 6}, {"type": "text", "text": "Baseline models. We compare MHCDIFF with parametric models and pixel-aligned reconstruction methods. For parametric models, which are robust for occlusion, we select ProPose [14] as SMPL estimator and PIXIE [15] as SMPL-X estimator. For pixel-aligned reconstruction methods, which can capture geometric details, we select PaMIR [106] for global features, ICON [92] for local features, and HiLo [96] and SIFU [104] for both. For the fair comparison, we primarily condition with the mean of SMPL distribution estimated via ProPose, and PIXIE is also used for ICON, which supports SMPL-X. We use pre-trained weights and evaluate under our test setting. ", "page_idx": 6}, {"type": "text", "text": "Evaluation metrics. We employ Point-to-Surface distance and Chamfer Distance as evaluation metrics. MHCDIFF outputs a point cloud, so Chamfer Distance is the average L2 distance from the reconstructed point cloud to vertices of ground-truth scans and vice versa, and Point-to-Surface distance is the average point-to-surface from the reconstructed point cloud to ground-truth scans. The outputs of implicit-function-based methods can be converted meshes via the Marching Cubes algorithm [49]. For fair comparison, we sample the same number of points from the reconstructed meshes uniformly. ", "page_idx": 6}, {"type": "text", "text": "5.1 Comparison with state-of-the-art methods ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "MHCDIFF outperforms prior implicit-function-based methods and SMPL estimation methods on occluded and even full-body images. Fig. 3 presents the robustness of 3D human reconstruction to the occlusion ratio. PaMIR and HiLo cannot handle the occlusions because the global feature encoder is sensitive to misaligned SMPL estimation. SIFU does not use the 3D encoder, but the cross-attention from the normal map of SMPL takes global features and is sensitive to occlusion ", "page_idx": 6}, {"type": "table", "img_path": "E2JCQyYu0E/tmp/0758b05745f32796c96bfcdb97b8fed28733568eb46e7316429ab37cb806a32d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 1: Quantitative evaluation on CAPE dataset. We report the average Chamfer Distance (cm) and Point-to-Surface distance (cm) on CAPE dataset. We randomly mask the images about $40\\%$ in average. We compare the performance with respect to (A) implicit-function-based methods; and (B) SMPL estimation methods used to condition MHCDIFF and (A). Best in bold, second-best underlined. ", "page_idx": 7}, {"type": "table", "img_path": "E2JCQyYu0E/tmp/a46a894ed4e18d290d474b621c30e24488ca4e4182c4c500804e9f7cd5173da9.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "E2JCQyYu0E/tmp/2ab43d218c0a0412dd552a1bb60b805e40356a099a695a30dcc4ccdf7fd5db4f.jpg", "table_caption": ["Table 2: Quantitative evaluation on MultiHuman dataset. We report the average Chamfer Distance (cm) for each category. We compare the performance similar to Tab. 1. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "E2JCQyYu0E/tmp/c2ccf6ef6edb0b22c673c554c502e3fb7b4f0879cb17f5231e952e6b632081c1.jpg", "table_caption": ["Table 3: Ablation study on CAPE dataset. We validate the effectiveness of (A) each component; (B) conditioning strategies; and (C) training strategies. "], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "E2JCQyYu0E/tmp/c080dff9d0e9dfa2eee3fb0326703b25d0a1471191b37d30a353da0a576227c7.jpg", "img_caption": ["Figure 3: A cumulative occlusion-to-reconstruction test. This figure shows the performance of different models from the images of various occlusion ratios. From the whole-body images, which is $0\\%$ occlusion, we randomly mask the images from $10\\%$ to $40\\%$ . MHCDIFF is robust to the occlusion ratio, showing the best performance. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "E2JCQyYu0E/tmp/eac6ee5e444f8d3ddbbf3a615629614b2f16850ec3c56e217e903db515fde255.jpg", "img_caption": ["Figure 4: Qualitative results on CAPE dataset. We evaluate our method with SMPL estimation method and implicit-function-based methods. Given the upper image, PaMIR, ICON, and HiLo cannot generate the occluded regions. They cannot also handle the misaligned SMPL mesh on the arms, creating incomplete bodies. ProPose predicts the full-body shape, but cannot capture the details like the blazer of the lower image. However, MHCDIFF is robust to the occlusion and misalignment, and can capture pixel-aligned details. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "and misaligned SMPL estimation. ICON shows comparable robustness due to its locality, but worse quality than ProPose estimation used to condition as the occlusion ratio increases. On the contrary, MHCDIFF is as robust as the statistical models, showing the most accurate results for all occlusion ratios. The results of $40\\%$ occlusion ratio are also displayed in numbers in Tab. 1. Tab. 2 presents the performance on real-world interaction scenarios with MultiHuman dataset. The dataset is divided into 5 categories by the level of occlusions: \"occluded single\" and \"two closely-inter\" show the most severe occlusion, and \"single\" and \"three\" show the least occlusion. We compare the performance in each category and similar to randomly masked settings, MHCDIFF achieves state-of-the-art on severe occluded images, and comparable performance on full-body images. The major improvements of MHCDIFF are (1) correcting the misaligned SMPL estimation as shown in Tab. 1, and (2) inpainting the invisible regions as shown in Fig. 3. The qualitative results on CAPE dataset are shown in Fig. 4, and MultiHuman and Hi4D datasets are shown in the appendices Sec. E. ", "page_idx": 8}, {"type": "text", "text": "5.2 Ablation study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We conduct an ablation on MHCDIFF to validate the effectiveness of each component. In Tab.   \n3-B, we condition the diffusion model with single SMPL-X (PIXIE) or SMPL (ProPose) estimation. ", "page_idx": 8}, {"type": "text", "text": "We improve the performance with multi-hypotheses condition (Sec. 4.3). In Tab. 4, we show the correlation between the number of SMPL sampled and the reconstruction quality. More SMPL hypotheses may include more accurate samples and improve the quality (15 samples), as well as extreme samples and decrease the quality (20 samples). From $\\mathrm{PC^{\\bar{2}}}$ [57], which only takes image condition, we also validate the local features from SMPL in Tab. 3-A. All of these features improve the performance, especially the signed distance. In Tab. 3-C, MHCDIFF is trained without random masking or by conditioning the distribution estimated by ProPose [14] instead of GT SMPL-X. ", "page_idx": 9}, {"type": "image", "img_path": "E2JCQyYu0E/tmp/19dba074adb32df027d7f440a9df3ad2f157d97365b5d8985316cfbf74eac975.jpg", "img_caption": ["Figure 5: Qualitative results on in-the-wild images. Two images on the left show occlusions due to interactions, and the rightmost image shows loose clothes. From internet photos, we use [32] to segment images. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, We present MHCDIFF, which robustly reconstructs pixel-aligned and detailed 3D humans from single occluded images. Rather than implicit-function-based methods, we choose the point cloud diffusion model to generate invisible regions capturing the features globally. Our multi-hypotheses conditioning mechanism extracts local features from multiple SMPL estimations and integrates them without learnable parameters, so MHCDIFF is robust to a single erroneous SMPL due to occlusion. We augment the limited training data by random masking to synthesize occlusion by diverse interaction. The experiments demonstrate that our proposed method outperforms state-of-the-art methods from various levels of occlusion and interaction. In the future, the point cloud of human shapes can be applied to intermediate stages for implicit function [58] and human body deformation [45] or motion flow [38]. ", "page_idx": 9}, {"type": "text", "text": "7 Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was supported by NST grant (CRC 21011, MSIT), IITP grant (RS-2023-00228996, RS-2024-00459749, MSIT) and KOCCA grant (RS-2024-00442308, MCST). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Thiemo Alldieck, Marcus Magnor, Bharat Lal Bhatnagar, Christian Theobalt, and Gerard Pons-Moll. Learning to reconstruct people in clothing from a single RGB camera. In CVPR, 2019.   \n[2] Thiemo Alldieck, Marcus Magnor, Weipeng Xu, Christian Theobalt, and Gerard Pons-Moll. Detailed human avatars from monocular video. In 3DV, 2018.   \n[3] Thiemo Alldieck, Gerard Pons-Moll, Christian Theobalt, and Marcus Magnor. Tex2shape: Detailed full human body geometry from a single image. In ICCV, 2019.   \n[4] Mahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bojanowski, Florian Bordes, Pascal Vincent, Armand Joulin, Michael Rabbat, and Nicolas Ballas. Masked siamese networks for label-efficient learning. In ECCV, 2022.   \n[5] Bharat Lal Bhatnagar, Garvita Tiwari, Christian Theobalt, and Gerard Pons-Moll. Multigarment net: Learning to dress 3d people from images. In ICCV, 2019.   \n[6] Benjamin Biggs, S\u00e9bastien Ehrhart, Hanbyul Joo, Benjamin Graham, Andrea Vedaldi, and David Novotny. 3D multibodies: Fitting sets of plausible 3D models to ambiguous image data. In NeurIPS, 2020.   \n[7] Yukang Cao, Kai Han, and Kwan-Yee K. Wong. Sesdf: Self-evolved signed distance field for implicit 3d clothed human reconstruction. In CVPR, 2023.   \n[8] Hanbyel Cho and Junmo Kim. Generative approach for probabilistic human mesh recovery using diffusion models. In ICCV Workshop, 2023.   \n[9] Hongsuk Choi, Gyeongsik Moon, and Kyoung Mu Lee. Pose2mesh: Graph convolutional network for 3d human pose and mesh recovery from a 2d human pose. In ECCV, 2020.   \n[10] Hongsuk Choi, Gyeongsik Moon, JoonKyu Park, and Kyoung Mu Lee. Learning to estimate robust 3d human mesh from in-the-wild crowded scenes. In CVPR, 2022.   \n[11] Vasileios Choutas, Lea Muller, Chun-Hao P. Huang, Siyu Tang, Dimitris Tzionas, and Michael J. Black. Accurate 3d body shape regression using metric and semantic attribute. In CVPR, 2022.   \n[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021.   \n[13] Thomas D Downs. Orientation statistics. In Biometrika, volume 59, pages 665\u2013676, 1972.   \n[14] Qi Fang, Kang Chen, Yinghui Fan, Qing Shuai, Jiefeng Li, and Weidong Zhang. Learning analytical posterior probability for human mesh recovery. In CVPR, 2023.   \n[15] Yao Feng, Vasileios Choutas, Timo Bolkart, Dimitrios Tzionas, and Michael J. Black. Collaborative regression of expressive bodies using moderation. In 3DV, 2021.   \n[16] Clement Fuji Tsang, Maria Shugrina, Jean Francois Lafleche, Towaki Takikawa, Jiehan Wang, Charles Loop, Wenzheng Chen, Krishna Murthy Jatavallabhula, Edward Smith, Artem Rozantsev, Or Perel, Tianchang Shen, Jun Gao, Sanja Fidler, Gavriel State, Jason Gorski, Tommy Xiang, Jianing Li, Michael Li, and Rev Lebaredian. Kaolin: A pytorch library for accelerating 3d deep learning research. https://github.com/NVIDIAGameWorks/kaolin, 2022.   \n[17] Andrew Gilbert, Marco Volino, John Collomosse, and Adrian Hilton. Volumetric performance capture from minimal camera viewpoints. In ECCV, 2018.   \n[18] Jia Gong, Lin Geng Foo, Zhipeng Fan, Qiuhong Ke, Hossein Rahmani, and Jun Liu. Diffpose: Toward more reliable 3d pose estimation. In CVPR, 2023.   \n[19] Sang-Hun Han, Min-Gyu Park, Ju Hong Yoon, Ju-Mi Kang, Young-Jae Park, and Hae-Gon Jeon. High-fidelity 3d human digitization from single 2k resolution images. In CVPR, 2023.   \n[20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020.   \n[21] Karl Holmquist and Bastian Wandt. Diffpose: Multi-hypothesis human pose estimation using diffusion models. In ICCV, 2023.   \n[22] Zixuan Huang, Justin Johnson, Shoubhik Debnath, James M Rehg, and Chao-Yuan Wu. Pointinfinity: Resolution-invariant point diffusion models. CVPR, 2024.   \n[23] Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian Sminchisescu. Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments. PAMI, 2014.   \n[24] Aaron S Jackson, Chris Manafas, and Georgios Tzimiropoulos. 3d human body reconstruction from a single image via volumetric regression. In ECCV Workshop, 2018.   \n[25] Boyi Jiang, Juyong Zhang, Yang Hong, Jinhao Luo, Ligang Liu, and Hujun Bao. Bcnet: Learning body and cloth shape from a single image. In ECCV, 2020.   \n[26] Hanbyul Joo, Natalia Neverova, and Andrea Vedaldi. Exemplar fine-tuning for 3d human pose fitting towards in-the-wild 3d human pose estimation. In 3DV, 2020.   \n[27] Hanbyul Joo, Tomas Simon, and Yaser Sheikh. Total capture: A 3d deformation model for tracking faces. In CVPR, 2018.   \n[28] Angjoo Kanazawa, Michael J. Black, David W. Jacobs, and Jitendra Malik. End-to-end recovery of human shape and pose. In CVPR, 2018.   \n[29] Michael Kazhdan and Hugues Hoppe. Screened poisson surface reconstruction. ACM TOG, 2013.   \n[30] CG Khatri and Kanti V Mardia. The von mises-fisher matrix distribution in orientation statistics. In Journal of the Royal Statistical Society: Series B (Methodological), volume 39, pages 95\u2013106, 1977.   \n[31] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2017.   \n[32] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Doll\u00e1r, and Ross Girshick. Segment anything. arXiv preprint arXiv:2304.02643, 2023.   \n[33] Muhammed Kocabas, Chun-Hao P. Huang, Otmar Hilliges, and Michael J. Black. PARE: Part attention regressor for 3D human body estimation. In ICCV, 2021.   \n[34] Nikos Kolotouros, Georgios Pavlakos, Michael J Black, and Kostas Daniilidis. Learning to reconstruct 3d human pose and shape via model-fitting in the loop. In ICCV, 2019.   \n[35] Nikos Kolotouros, Georgios Pavlakos, Dinesh Jayaraman, and Kostas Daniilidis. Probabilistic modeling for human mesh recovery. In ICCV, 2021.   \n[36] Juil Koo, Seungwoo Yoo, Minh Hieu Nguyen, and Minhyuk Sung. SALAD: Part-level latent diffusion for 3d shape generation and manipulation. In ICCV, 2023.   \n[37] Verica Lazova, Eldar Insafutdinov, and Gerard Pons-Moll. 360-degree textures of people in clothing from a single image. In 3DV, 2019.   \n[38] Jihyun Lee, Junbong Jang, Donghwan Kim, Minhyuk Sung, and Tae-Kyun Kim. Fourierhandflow: Neural 4d hand representation using fourier query flow. In NeurIPS, 2023.   \n[39] Jihyun Lee, Shunsuke Saito, Giljoo Nam, Minhyuk Sung, and Tae-Kyun Kim. Interhandgen: Two-hand interaction generation via cascaded reverse diffusion. In CVPR, 2024.   \n[40] Jihyun Lee, Minhyuk Sung, Honggyu Choi, and Tae-Kyun Kim. Im2hands: Learning attentive implicit representation of interacting two-hand shapes. In CVPR, 2023.   \n[41] Jiefeng Li, Chao Xu, Zhicun Chen, Siyuan Bian, Lixin Yang, and Cewu Lu. Hybrik: A hybrid analytical-neural inverse kinematics solution for 3d human pose and shape estimation. In CVPR, pages 3383\u20133393, 2021.   \n[42] Peike Li, Yunqiu Xu, Yunchao Wei, and Yi Yang. Self-correction for human parsing. PAMIR, 2020.   \n[43] Wenhao Li, Hong Liu, Hao Tang, Pichao Wang, and Luc Van Gool. Mhformer: Multihypothesis transformer for 3d human pose estimation. In CVPR, 2022.   \n[44] Zhihao Li, Jianzhuang Liu, Zhensong Zhang, Songcen Xu, and Youliang Yan. Cliff: Carrying location information in full frames into human pose and shape estimation. In ECCV, 2022.   \n[45] Minghua Liu, Minhyuk Sung, Radom\u00edr M\u02c7ech, and Hao Su. Deepmetahandles: Learning deformation meta-handles of 3d meshes with biharmonic coordinates. In CVPR, 2021.   \n[46] Zhen Liu, Yao Feng, Michael J. Black, Derek Nowrouzezahrai, Liam Paull, and Weiyang Liu. Meshdiffusion: Score-based generative 3d mesh modeling. In ICLR, 2023.   \n[47] Zhijian Liu, Haotian Tang, Yujun Lin, and Song Han. Point-voxel cnn for efficient 3d deep learning. In NeurIPS, 2019.   \n[48] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J. Black. SMPL: A skinned multi-person linear model. ACM TOG, 2015.   \n[49] William E. Lorensen and Harvey E. Cline. Marching cubes: A high resolution 3d surface construction algorithm. SIGGRAPH Comput. Graph., 1987.   \n[50] Shitong Luo and Wei Hu. Diffusion probabilistic models for 3d point cloud generation. In CVPR, 2021.   \n[51] Zhaoyang Lyu, Zhifeng Kong, Xudong Xu, Liang Pan, and Dahua Lin. A conditional point diffusion-refinement paradigm for 3d point cloud completion. In ICLR, 2022.   \n[52] Qianli Ma, Shunsuke Saito, Jinlong Yang, Siyu Tang, and Michael J. Black. SCALE: Modeling clothed humans with a surface codec of articulated local elements. In CVPR, 2021.   \n[53] Qianli Ma, Jinlong Yang, Anurag Ranjan, Sergi Pujades, Gerard Pons-Moll, Siyu Tang, and Michael J. Black. Learning to Dress 3D People in Generative Clothing. In CVPR, 2020.   \n[54] Qianli Ma, Jinlong Yang, Siyu Tang, and Michael J. Black. The power of points for modeling humans in clothing. In ICCV, 2021.   \n[55] Kanti V Mardia, Peter E Jupp, and KV Mardia. Directional statistics. In Whiley Online Library, volume 2, 2000.   \n[56] Dushyant Mehta, Helge Rhodin, Dan Casas, Pascal Fua, Oleksandr Sotnychenko, Weipeng Xu, and Christian Theobalt. Monocular 3d human pose estimation in the wild using improved cnn supervision. In 3DV, 2017.   \n[57] Luke Melas-Kyriazi, Christian Rupprecht, and Andrea Vedaldi. Pc2: Projection-conditioned point cloud diffusion for single-image 3d reconstruction. In CVPR, 2023.   \n[58] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger. Occupancy networks: Learning 3d reconstruction in function space. In CVPR, 2019.   \n[59] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In ECCV, 2020.   \n[60] Gyeongsik Moon and Kyoung Mu Lee. I2l-meshnet: Image-to-lixel prediction network for accurate 3d human pose and mesh estimation from a single rgb image. In ECCV, 2020.   \n[61] Lea Muller, Vickie Ye, Georgios Pavlakos, Michael J. Black, and Angjoo Kanazawa. Generative proxemics: A prior for 3D social interaction from images. CoRR, 2023.   \n[62] Gimin Nam, Mariem Khlif,i Andrew Rodriguez, Alberto Tono, Linqi Zhou, and Paul Guerrero. 3d-ldm: Neural implicit 3d shape generation with latent diffusion models. arXiv preprint arXiv:2212.00842, 2022.   \n[63] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen. Point-e: A system for generating 3d point clouds from complex prompts. arXiv preprint arXiv:2212.08751, 2022.   \n[64] Mohamed Omran, Christoph Lassner, Gerard Pons-Moll, Peter V. Gehler, and Bernt Schiele. Neural body ftiting: Unifying deep learning and model-based human pose and shape estimation. In 3DV, 2018.   \n[65] Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. Deepsdf: Learning continuous signed distance functions for shape representation. In CVPR, 2019.   \n[66] Priyanka Patel, Chun-Hao P. Huang, Joachim Tesch, David T. Hoffmann, Shashank Tripathi, and Michael J. Black. AGORA: Avatars in geography optimized for regression analysis. In CVPR, 2021.   \n[67] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed A. A. Osman, Dimitrios Tzionas, and Michael J. Black. Expressive body capture: 3D hands, face, and body from a single image. In CVPR, 2019.   \n[68] Gerard Pons-Moll, Sergi Pujades, Sonny Hu, and Michael Black. Clothcap: Seamless 4d clothing capture and retargeting. ACM TOG, 2017.   \n[69] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. In ICLR, 2023.   \n[70] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In CVPR, 2017.   \n[71] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Accelerating 3d deep learning with pytorch3d. arXiv preprint arXiv:2007.08501, 2020.   \n[72] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. arXiv preprint arXiv:2103.00020, 2021.   \n[73] Davis Rempe, Tolga Birdal, Aaron Hertzmann, Jimei Yang, Srinath Sridhar, and Leonidas J. Guibas. Humor: 3d human motion model for robust pose estimation. In ICCV, 2021.   \n[74] RenderPeople. Renderpeople. renderpeople.com, 2018.   \n[75] Javier Romero, Dimitrios Tzionas, and Michael J. Black. Embodied hands: Modeling and capturing hands and bodies together. ACM TOG, 2017.   \n[76] Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Morishima, Angjoo Kanazawa, and Hao Li. Pifu: Pixel-aligned implicit function for high-resolution clothed human digitization. In ICCV, 2019.   \n[77] Shunsuke Saito, Tomas Simon, Jason Saragih, and Hanbyul Joo. Pifuhd: Multi-level pixelaligned implicit function for high-resolution 3d human digitization. In CVPR, 2020.   \n[78] Akash Sengupta, Ignas Budvytis, and Roberto Cipolla. Hierarchical Kinematic Probability Distributions for 3D Human Shape and Pose Estimation from Images in the Wild. In ICCV, 2021.   \n[79] Akash Sengupta, Ignas Budvytis, and Roberto Cipolla. Probabilistic 3D Human Shape and Pose Estimation from Multiple Unconstrained Images in the Wild. In ICCV, 2021.   \n[80] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR, 2021.   \n[81] Anastasis Stathopoulos, Ligong Han, and Dimitris Metaxas. Score-guided diffusion for 3d human recovery. In CVPR, 2024.   \n[82] Yu Sun, Qian Bao, Wu Liu, Yili Fu, Black Michael J., and Tao Mei. Monocular, One-stage, Regression of Multiple 3D People. In ICCV, 2021.   \n[83] Yu Sun, Wu Liu, Qian Bao, Yili Fu, Tao Mei, and Michael J. Black. Putting people in their place: Monocular regression of 3D people in depth. In CVPR, June 2022.   \n[84] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang Zeng. Dreamgaussian: Generative gaussian splatting for efficient 3d content creation. In ICLR, 2024.   \n[85] Yingzhi Tang, Qijian Zhang, Junhui Hou, and Yebin Liu. Human as points: Explicit pointbased 3d human reconstruction from single-view rgb images. arXiv preprint arXiv:2311.02892, 2023.   \n[86] Matthew Trumble, Andrew Gilbert, Adrian Hilton, and John Collomosse. Deep autoencoder for combined human pose estimation and body model upscaling. In ECCV, 2018.   \n[87] Twindom. Twindom. twindom.com, 2018.   \n[88] G\u00fcl Varol, Duygu Ceylan, Bryan Russell, Jimei Yang, Ersin Yumer, Ivan Laptev, and Cordelia Schmid. BodyNet: Volumetric inference of 3D human body shapes. In ECCV, 2018.   \n[89] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017.   \n[90] Junying Wang, Jae Shin Yoon, Tuanfeng Y. Wang, Krishna Kumar Singh, and Ulrich Neumann. Complete 3d human reconstruction from a single incomplete image. In CVPR, 2023.   \n[91] Yuliang Xiu, Jinlong Yang, Xu Cao, Dimitrios Tzionas, and Michael J. Black. ECON: Explicit Clothed humans Optimized via Normal integration. In CVPR, 2023.   \n[92] Yuliang Xiu, Jinlong Yang, Dimitrios Tzionas, and Michael J. Black. ICON: Implicit Clothed humans Obtained from Normals. In CVPR, 2022.   \n[93] Hongyi Xu, Eduard Gabriel Bazavan, Andrei Zanfir, Bill Freeman, Rahul Sukthankar, and Cristian Sminchisescu. Ghum & ghuml: Generative 3d human shape and articulated pose models. In CVPR, 2020.   \n[94] Yuanlu Xu, Song-Chun Zhu, and Tony Tung. Denserac: Joint 3d pose and shape estimation by dense render-andcompare. In ICCV, 2019.   \n[95] Xueting Yang, Yihao Luo, Yuliang Xiu, Wei Wang, Hao Xu, and Zhaoxin Fan. D-IF: Uncertainty-aware Human Digitization via Implicit Distribution Field. In ICCV, 2023.   \n[96] Yifan Yang, Dong Liu, Shuhai Zhang, Zeshuai Deng, Zixiong Huang, and Mingkui Tan. Hilo: Detailed and robust 3d clothed human reconstruction with high-and low-frequency information of parametric models. In CVPR, 2024.   \n[97] Yifei Yin, Chen Guo, Manuel Kaufmann, Juan Zarate, Jie Song, and Otmar Hilliges. Hi4d: 4d instance segmentation of close human interaction. In CVPR, 2023.   \n[98] Tao Yu, Zerong Zheng, Kaiwen Guo, Pengpeng Liu, Qionghai Dai, and Yebin Liu. Function4d: Real-time human volumetric capture from very sparse consumer rgbd sensors. In CVPR, 2021.   \n[99] Ilya Zakharkin, Kirill Mazur, Artur Grigorev, and Victor Lempitsky. Point-based modeling of human clothing. In ICCV, 2021.   \n[100] Andrei Zanfir, Eduard Gabriel Bazavan, Mihai Zanfir, Bill Freeman, Rahul Sukthankar, and Cristian Sminchisescu. Neural descent for visual 3d human pose and shape. In CVPR, 2021.   \n[101] Xiaohui Zeng, Arash Vahdat, Francis Williams, Zan Gojcic, Or Litany, Sanja Fidler, and Karsten Kreis. Lion: Latent point diffusion models for 3d shape generation. In NeurIPS, 2022.   \n[102] Siwei Zhang, Qianli Ma, Yan Zhang, Sadegh Aliakbarian, Darren Cosker, and Siyu Tang. Probabilistic human mesh recovery in 3d scenes from egocentric views. In ICCV, 2023.   \n[103] Zechuan Zhang, Li Sun, Zongxin Yang, Ling Chen, and Yi Yang. Global-correlated 3ddecoupling transformer for clothed avatar reconstruction. In NeurIPS, 2023.   \n[104] Zechuan Zhang, Zongxin Yang, and Yi Yang. Sifu: Side-view conditioned implicit function for real-world usable clothed human reconstruction. In CVPR, 2024.   \n[105] Yang Zheng, Ruizhi Shao, Yuxiang Zhang, Tao Yu, Zerong Zheng, Qionghai Dai, and Yebin Liu. Deepmulticap: Performance capture of multiple characters using sparse multiview cameras. In ICCV, 2021.   \n[106] Zerong Zheng, Tao Yu, Yebin Liu, and Qionghai Dai. Pamir: Parametric model-conditioned implicit representation for image-based human reconstruction. PAMI, 2021.   \n[107] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random erasing data augmentation. In AAAI, 2020.   \n[108] Linqi Zhou, Yilun Du, and Jiajun Wu. 3d shape generation and completion through point-voxel diffusion. In ICCV, 2021.   \n[109] Hao Zhu, Xinxin Zuo, Sen Wang, Xun Cao, and Ruigang Yang. Detailed human shape estimation from a single image by hierarchical mesh deformation. In CVPR, 2019. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Broader impact ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Our method can be potentially used for AR/VR applications. The real-world interaction can be captured and modeled in virtual scenes, which can be extended to reinforcement learning. However, there are potential risks associated with falsifying human avatars, which could inadvertently compromise personal privacy. Consequently, there is a pressing need to establish regulations that clarify the fair use of such technology. ", "page_idx": 15}, {"type": "text", "text": "B Limitations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Our method, based on DDPM [20] sampling with 1, 000 steps, has limitation on efficiency. The training time is reasonable because we do not need query point sampling, which yields CPU bottleneck to learn implicit-function. However, evaluation on CAPE dataset takes about 12 hours, while other implicit-function-based methods take about 30 minutes. We can apply DDIM [80] sampling with fewer steps to shorten the inference time. ", "page_idx": 15}, {"type": "text", "text": "C Pointcloud to mesh ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Following previous work [85, 19], we try to convert our reconstructed point cloud to mesh with Screened Poisson surface reconstruction [29]. However, the process takes about 10 hours per sample with 16, 384 points. The implicit function [58] converts the point clouds to occupancy fields by encoding features with a PointNet [70]. This two-stage pipeline can generate occluded regions and capture details. We will try this pipeline in our future work. ", "page_idx": 15}, {"type": "text", "text": "D Statistical significance ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We evaluate MHCDIFF on CAPE dataset [53, 68] with 10 different random seeds. The random seeds effect on random noise in the diffusion process and SMPL sampling from the estimated distribution vis ProPose [14]. The Chamfer Distance and Point-to-Surface are $1.\\bar{8}72(\\pm0.008)$ and $1.810(\\pm0.008)$ with 1-sigma error bars, respectively. ", "page_idx": 15}, {"type": "text", "text": "E Qualitative results ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "For the real-world interaction, we evaluate MHCDIFF on MultiHuman [105] and Hi4D [97] datasets. We render the textured scans with Pytorch3D library [71] for MultiHuman dataset, and segment each subject with pre-trained network [42] for Hi4D dataset. Our proposed method is robust not only to the occlusion but also to noise in full images or segmentation process. ", "page_idx": 15}, {"type": "image", "img_path": "E2JCQyYu0E/tmp/be6369e32c692bb79d216bd81a8ea719416be555079130e8b3e4ec1c59b892e4.jpg", "img_caption": ["Figure 6: Qualitative results on MultiHuman dataset. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "E2JCQyYu0E/tmp/d8e8dc8c4893dca64d868dbf66e945ae111ccb209ad660ea7f0a06e6362bd206.jpg", "img_caption": ["Figure 7: Qualitative results on Hi4D dataset. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: Please refer to Abstract and Introduction Sections. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 17}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Justification: Please refer to Sec. B. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 17}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 17}, {"type": "text", "text": "Justification: The paper is based on experiments. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 18}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: Please refer to Implementation paragraph in Sec. 5. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 18}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The code will be public after cleansing. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 19}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: Please refer to Sec. 5. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 19}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: Please refer to Sec. D. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: Please refer to Implementation paragraph in Sec. 5. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 20}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The paper conform the NeurIPS Code of Ethics. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 20}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: Please refer to Sec. A. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to ", "page_idx": 20}, {"type": "text", "text": "generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 21}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 21}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: Our method poses no such risks. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 21}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We cite the creators of the assets. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 21}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We submit the code as a zip file. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 22}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing, and we only use public datasets for human subjects. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 22}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing, and we only use public datasets for human subjects. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 22}]