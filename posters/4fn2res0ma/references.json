{"references": [{"fullname_first_author": "Brown, T.", "paper_title": "Language models are few-shot learners", "publication_date": "2020-00-00", "reason": "This paper introduces the concept of in-context learning (ICL), a core topic of the current paper."}, {"fullname_first_author": "Elhage, N.", "paper_title": "A mathematical framework for transformer circuits", "publication_date": "2021-00-00", "reason": "This paper introduces the concept of \"induction heads,\" a key mechanism studied in the current paper."}, {"fullname_first_author": "Nichani, E.", "paper_title": "How transformers learn causal structure with gradient descent", "publication_date": "2024-00-00", "reason": "This paper provides a theoretical analysis of ICL in transformers, which is closely related to the current paper's focus on training dynamics."}, {"fullname_first_author": "Olsson, C.", "paper_title": "In-context learning and induction heads", "publication_date": "2022-00-00", "reason": "This paper empirically studies induction heads and their role in ICL, which is directly relevant to the current paper's theoretical analysis."}, {"fullname_first_author": "Vaswani, A.", "paper_title": "Attention is all you need", "publication_date": "2017-00-00", "reason": "This foundational paper introduces the transformer architecture, which is the basis for the model analyzed in the current paper."}]}