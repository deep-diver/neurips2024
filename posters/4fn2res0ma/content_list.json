[{"type": "text", "text": "Unveiling Induction Heads: Provable Training Dynamics and Feature Learning in Transformers ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Siyu Chen\\* Department of Statistics and Data Science, Yale University siyu.chen.sc3226@yale.edu ", "page_idx": 0}, {"type": "text", "text": "Heejune Sheen\\* Department of Statistics and Data Science, Yale University heejune.sheen@yale.edu ", "page_idx": 0}, {"type": "text", "text": "Tianhao Wang Toyota Technological Institute at Chicago tianhao.wang@ttic.edu ", "page_idx": 0}, {"type": "text", "text": "Zhuoran Yang Department of Statistics and Data Science, Yale University zhuoran. yang@yale.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In-context learning (ICL) is a cornerstone of large language model (LLM) functionality, yet its theoretical foundations remain elusive due to the complexity of transformer architectures. In particular, most existing work only theoretically explains how the attention mechanism facilitates ICL under certain data models. It remains unclear how the other building blocks of the transformer contribute to ICL. To address this question, we study how a two-attention-layer transformer is trained to perform ICL on $n$ -gram Markov chain data, where each token in the Markov chain statistically depends on the previous n tokens. We analyze a sophisticated transformer model featuring relative positional embedding, multi-head softmax attention, and a feed-forward layer with normalization. We prove that the gradient flow with respect to a cross-entropy ICL loss converges to a limiting model that performs a generalized version of the \u201cinduction head\" mechanism with a learned feature, resulting from the congruous contribution of all the building blocks. In the limiting model, the first attention layer acts as a copier, copying past tokens within a given window to each position, and the feed-forward network with normalization acts as a selector that generates a feature vector by only looking at informationally relevant parents from the window. Finally, the second attention layer is a classifier that compares these features with the feature at the output position, and uses the resulting similarity scores to generate the desired output. Our theory is further validated by simulation experiments. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In-context learning (ICL) (Brown et al., 2020) has emerged as a crucial aspect of large language model (LLM) (Radford et al., 2019; Brown et al., 2020; Achiam et al., 2023; Anthropic, 2023; Team et al., 2023) functionality, enabling pre-trained LLMs to solve user-specified tasks during inference without updating model parameters. In ICL, a pre-trained LLM, typically a transformer, receives prompts containing a few demonstration examples sampled from a task-specific distribution and produces the desired output for that task. This capability is noteworthy because the tasks addressed during the ICL might not be part of the original training data set. The success of ICL requires the LLM to perform certain learning processes during inference. ", "page_idx": 0}, {"type": "text", "text": "Although many previous works aim to demystify ICL from either empirical or theoretical perspectives, the theoretical foundations of ICL remain elusive. This is primarily due to the complexity of transformer architectures, which integrate token and position embeddings, multiple layers of multihead softmax attention, layer normalization, and feedforward neural networks. When it comes to understanding how the ICL ability emerges in transformers after training, existing works often focus on simplified models, such as linear attention mechanisms or single-layer transformers (Von Oswald et al., 2023), and ICL tasks are typically confined to linear regression (Akyuirek et al., 2023). This leaves a gap in understanding how full-fedged transformer architectures facilitate ICL of more complex tasks, especially when latent causal structures exist among the tokens in a sequence. ", "page_idx": 1}, {"type": "text", "text": "In this paper, our aim is to narrow this gap by studying how a two-attention-layer transformer is trained to perform ICL of an $n$ gram Markov chain model, where each token in the Markov chain statistically depends on the $n$ tokens before it, known as the parent set. Specifically, we consider a transformer model with relative positional embedding (RPE) (He et al., 2020), multi-head softmax attention, and a feed-forward network (FFN) layer with normalization. We employ such a transformer model to predict the $(L+1)$ -th token of an $n$ -gram Markov chain, with the first $L$ tokens given as the prompt, where $L+1$ is the sequence length. Here the $L$ -token sequence is sampled from a random Markov chain model, where a random transition kernel obeying the $n$ -gram Markov property is used to generate sequences. The token sequence is fed into the transformer model, which outputs a probability distribution over the vocabulary set to predict the $(L+1)$ -th token. To train the transformer model, we sample token sequences from these random Markov chain models and minimize the cross-entropy loss between the predicted token distribution and the true token distribution. ", "page_idx": 1}, {"type": "text", "text": "Under this setting, we aim to answer the following three questions: (i) Does the gradient flow with respect to the cross-entropy loss converge during training? (i) If yes, how does the limiting model perform ICL? (ii) How do the building blocks of the transformer model contribute to ICL? ", "page_idx": 1}, {"type": "text", "text": "Main Results. We provide an affirmative answer to the Question (i) by proving that the gradient fow converges during training. In particular, we identify three phases of training dynamics: in the first stage, FFN learns the potential parent set; in the second stage, each attention head of the first multi-head softmax attention layer learns to focus on a single parent token selected by FFN; and in the final stage, the parameter of the second attention layer increases, and the transformer approaches the limiting model. Moreover, for Questions (ii) and (i), we show that the limiting model performs a specialized form of exponential kernel regression, dubbed \u201cgeneralized induction head\", which requires the congruous contribution of all the building blocks. Specifically, the first attention layer acts as a copier, copying past tokens within a given window to each position. The FFN layer acts as a selector that generates a feature vector by only looking at informationally relevant parents from the window according to a modified $\\chi^{2}$ -mutual information. Finally, the second attention layer is an exponential kernel classifier that compares the features at each position with those created for the outputposition $L+1$ , and uses the resulting similarity scores to generate the desired output. When specialized to the case where $n=1$ , the limiting model selects the true parent token and implements the induction head mechanism (Elhage et al., 2021). In this case, we recover the theory in Nichani et al. (2024). Our theory is complemented by numerical experiments, which validate the three-phase training dynamics and mechanism of generalized induction head. ", "page_idx": 1}, {"type": "text", "text": "To our best knowledge, our work is the first to provide a comprehensive understanding of how ICL is empowered by a collaboration of different building blocks in a transformer model. In particular, we identify the pivotal roles played by RPE in the copier component, the FFN layer with normalization in the selector component, and attention in the classifier component. We believe our work will shed light on the theoretical understanding of ICL for more complicated tasks. ", "page_idx": 1}, {"type": "text", "text": "Related Works. Our work adds to the rapidly growing literature on understanding in-context learning by transformers. We defer an in-depth discussion on related works in Appendix $\\S\\mathrm{A}$ due to the page limit. ", "page_idx": 1}, {"type": "text", "text": "Roadmap.  The rest of the paper is organized as follows: We introduce the problem setup of ICL of Markov chains in $\\S2$ Then in $\\S3$ we present the main theoretical results and related discussions. A proof sketch is provided in $\\S D$ . Finally, we present corresponding experiment results in $\\S B$ , and the detailed proofs are deferred to the Appendix. ", "page_idx": 1}, {"type": "text", "text": "Notation.  We denote by $e_{1},\\ldots,e_{d}$ the standard basis vectors in $\\mathbb{R}^{d}$ and by 1 the all-one vector in $\\mathbb{R}^{d}$ . We denote by $\\sigma(\\cdot)$ the softmax function such that the $i$ -th coordinate of $\\sigma(x)$ is $\\sigma_{i}(x)\\,=$ $\\begin{array}{r}{\\exp(x_{i})/\\!\\sum_{l=1}^{L}\\exp(x_{l})}\\end{array}$ for $x\\in\\mathbb{R}^{L}$ By default, the softmax operation will always e aplied row wise. For any integer $n>0$ , we denote $[n]:=\\{1,\\ldots,n\\}$ . For a vector $w\\in\\mathbb{R}^{M}$ , we denote by $w_{i}$ the $i$ -th entry of $w$ and $w_{-i}$ the $(M+1\\-\\dot{i})$ -th entry of $w$ for positive integer $i\\in[M]$ . For a matrix $W$ , we denote by. $W(i,j)$ the entry at the $i$ -th row and $j$ -th column of $W$ . For two vectors $u$ and $v$ we write $u/v$ as the vector obtained by taking element-wise division between $u$ and $v$ .We denote by $a\\vee b$ and $a\\wedge b$ the maximum and minimum of $a$ and $b$ , respectively. We denote by $x_{s:t}$ the sequence $\\{x_{s},x_{s+1},\\ldots,x_{t}\\}$ . For a class $\\mathcal{X}$ , we denote by $\\Delta(\\mathcal X)$ the space of probability measures over $\\mathcal{X}$ We use the standard big O notation throughout the paper. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2 Problem Setup: In-Context Learning of Markov Chains ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we present the details of the problem setting. In particular, we first introduce the statistical problem of ICL of $n$ gram Markov chains in $\\S2.1$ and then lay out the details of the transformer model in $\\S2.2$ ", "page_idx": 2}, {"type": "text", "text": "2.1  In-Context Learning and $n$ -Gram Markov Chains ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We study how autoregressive transformers are trained to perform in-context learning (ICL). A pretrained transformer can be viewed as a conditional distribution $f_{\\tt t f}(\\cdot\\mid\\tt p r o m p t)$ over a finite vocabulary set $\\mathcal{X}$ , where prompt is a sequence of tokens in $\\mathcal{X}$ . We consider an in-context unsupervised learning problem where the pre-trained transformer $f_{\\tt t f}$ is used to predict the $(L+1)$ -th token $x_{L+1}$ with the first $L$ tokens being the prompt. Here $L$ is a fixed number and the joint distribution of the sequence $x_{1:(L+1)}$ is sampled from a random $n$ gram Markov chain. In other words, with $x_{1:(L+1)}$ sampled from some distribution, we evaluate how well $f_{\\mathrm{tf}}\\left(\\cdot\\mid x_{1:L}\\right)$ predicts the distribution of $x_{L+1}$ ", "page_idx": 2}, {"type": "text", "text": "$n$ -Gram Markov Chains. We assume the data comes from a mixture of $n$ -gram Markov chain model, denoted by a tuple $(\\mathcal{X},\\mathtt{p a},\\mathcal{P},\\mu_{0})$ , where $\\mathcal{X}$ is the state space and $\\mathtt{p a}=\\,(-r_{1},\\ldots,-r_{n})$ is the parent set with positive integers $r_{1}<r_{2}<\\cdot\\cdot<r_{n}$ That is, for each $l>r_{n}$ \uff0c $x_{l}$ only statistically depends on $(x_{l-r_{n}},\\ldots,x_{l-r_{1}})$ , which is denoted by $X_{\\mathtt{p a}\\left(l\\right)}$ and referred to as the parent tokens of $x_{l}$ . We let $d=|\\mathcal{X}|$ denote the vocabulary size. Moreover, $\\mathcal{P}$ is a probability distribution over the set of Markov transition kernels respecting the parent structure specified by pa, and $\\mu_{0}$ is the joint distribution of the first $r_{n}$ tokens $x_{1:r_{n}}$ . Note that the size of the parent set $n$ can be smaller than or equal to $r_{n}$ Thus, the sequence $x_{1:(L+1)}$ is generated as follows: (i) sample initial $r_{n}$ tokens $(x_{1},\\cdot\\cdot\\cdot,\\bar{x_{r_{n}}})\\sim\\mu_{0}$ (ii) sample a random transition kernel $\\pi\\,\\sim\\,\\mathcal{P}$ , where $\\pi\\colon\\mathcal{X}^{n}\\,\\rightarrow\\,\\Delta(\\mathcal{X})$ , and (ii) sample token $x_{l}\\sim\\pi\\big(\\dot{\\cdot}\\,|\\,X_{\\mathtt{p a}(l)}\\big)$ for $l=r_{n}+1,\\ldots,L+1$ . See Figure 1 for an illustration of the generating model of 1:(L+1)\u00b7 ", "page_idx": 2}, {"type": "image", "img_path": "4fN2REs0Ma/tmp/1ceca7009cf0ccc3d58ce90e3ce1ec18bbb89d330853da7daa975032d94824d1.jpg", "img_caption": ["Figure 1: A two-gram Markov chain with parent set $\\mathtt{p a}=\\{-\\mathtt{i},-2\\}$ "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Cross-Entropy Loss.When $x_{1:(L+1)}$ is generated, $x_{1:L}$ is fed into the transformer $f_{\\mathrm{tf}}$ to predict $x_{L+1}$ . To assess the performance of ICL, we adopt the population cross-entropy (CE) loss ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{L}(f_{\\mathrm{tf}})=-\\mathbb{E}_{\\pi\\sim\\mathcal{P},x_{1:(L+1)}}\\left[\\log\\left(f_{\\mathrm{tf}}(x_{L+1}\\,|\\,x_{1:L})+\\epsilon\\right)\\right],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\epsilon>0$ is a small constant introduced for numerical stability and in the sequel we will take $\\varepsilon\\,=\\,O(L^{-1/2})$ . Here,the expectation is taken with respect to the joint distribution of $x_{1:(L+1)}$ (including the randomness of $\\pi\\sim\\mathcal{P}$ ). When setting $\\epsilon\\,=\\,0$ , we note that minimizing this crossentropy loss is equivalent to minimizing the KL divergence ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\pi\\sim\\mathcal{P},x_{1:L}}\\left[\\mathrm{KL}\\big(\\pi\\big(\\cdot\\,|\\;X_{\\mathtt{p a}(L+1)}\\big)\\;\\big|\\big|\\;f_{\\mathtt{t f}}\\big(\\cdot\\;\\big|\\;x_{1:L}\\big)\\big)\\right].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "As a remark, we also relax a condition in Nichani et al. (2024) where the last token $x_{L}$ has tobe resampled from a uniform distribution. In addition, our analysis can also be extended to sequential CE loss, which corresponds to predicting every token in the sequence given the past rather than just thelast token $x_{L+1}$ . This is closer to the training paradigm used in practice (Brown et al., 2020). See $\\mathrm{\\SC.4}$ for a further discussion on the sequential CE loss. ", "page_idx": 2}, {"type": "text", "text": "We consider a class of two-attention-layer transformer model, denoted by $\\mathrm{TF}(M,H,d,D)$ ,which incorporates Relative Positional Embedding (RPE) (He et al., 2020), Multi-Head Attention (MHA) (Vaswani et al., 2017), and a Feed-Forward network (FFN) with normalization. Here $M$ is an integer that specifies the window size of RPE, $H$ is the number of heads in the first attention layer, $d$ is the vocabulary size, and $D$ is an integer that controls the complexity of FFN. The details of $\\mathrm{TF}(M,H,d,D)$ are as follows. ", "page_idx": 3}, {"type": "text", "text": "Token Embedding, Input and Output. Note that each token takes values in $\\mathcal{X}$ with $d=|\\mathcal{X}|$ .We embed the tokens into one-hot vectors in $\\mathbb{R}^{d}$ , and thus we can identify $\\mathcal{X}$ as the canonical basis in $\\mathbb{R}^{d}$ ,i.e., $\\mathcal{X}=\\{e_{1},\\ldots,e_{d}\\}$ . A transformer model can be viewed as a mapping from $\\mathbb{R}^{(L+1)\\times d}$ to $\\Delta(\\mathcal X)$ . In particular, given the input sequence $x_{1:L}$ , we denote $X=(x_{1},\\ldots,x_{L})^{\\top}\\in\\mathbb{R}^{L\\times d}$ , and we append a zero vector $\\mathbf{0}\\in\\mathbb{R}^{d}$ to the sequence, and define $\\widetilde{X}=(x_{1},\\ldots,x_{L},\\mathbf{0})^{\\top}\\in\\mathbb{R}^{(L+1)\\times d}$ .The transformer takes $\\tilde{X}$ as input and outputs a probability distribution over $\\mathcal{X}$ ", "page_idx": 3}, {"type": "text", "text": "Relative Positional Embedding. In each head of the first attention layer, we adopt RPE to incorporate positional information. Specifically, RPE is parameterized by a vector $w\\,=\\,\\bigl(w_{-M},\\bar{\\,\\cdot\\,}\\,.\\,.\\,,w_{-1}\\bigr)^{\\intercal}\\,\\in\\,\\mathbb{R}^{M}$ , and it assigns a scalar value $W_{P}(i,j)$ to a pair of positions $(i,j)$ satisfying ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{W_{P}(i,j)=w_{j-i}\\ \\mathrm{~if~}\\ i-j\\in\\{1,\\ldots,M\\},}\\\\ &{W_{P}(i,j)=-\\infty\\ \\mathrm{~if~}\\ j\\geq i\\ \\mathrm{or~}\\ |j-i|>M.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "image", "img_path": "4fN2REs0Ma/tmp/ea15e2e2eb7ca47f7bdf8ae85c676e4621668980b8deb13d1081bb0fbc409c67.jpg", "img_caption": ["Figure 2: Mllustration of the relationship between RPE vector $w^{(h)}$ and corresponding matrix W(tb). "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "In other words, as illustrated in Figure 2, the $i$ -th token only attends to tokens with indices in $\\{i-1,\\ldots,i-M\\}$ referred to as the length- $M$ window of the $i$ -th token, and the trainable vector $w$ determines the value of positional embedding. Here, we use $-k$ to index the last $k$ -th position. ", "page_idx": 3}, {"type": "text", "text": "The First Attention Layer.  The input sequence is processed by the first attention layer with $H$ parallel heads. In all heads, we discard the token information and only use RPE to compute the attention score. Specifically, each attention head $h$ maps $\\widetilde{X}$ into a sequence in $\\mathbb{R}^{d}$ with length $L+1$ denoted by V(h)= (rvh),...,L+)T For any $l\\in[L+1]$ \uff0c $v_{l}^{(h)}$ is computed via ", "page_idx": 3}, {"type": "equation", "text": "$$\nv_{l}^{(h)}=\\sum_{j=1}^{L}\\sigma_{j}\\left(W_{P}^{(h)}(l,\\cdot)\\right)\\cdot x_{j}=\\sum_{j=1}^{L}\\frac{\\exp\\bigl(W_{P}^{(h)}(l,j)\\bigr)\\cdot x_{j}}{\\sum_{k=1}^{L}\\exp\\bigl(W_{P}^{(h)}(l,k)\\bigr)}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "That is, we use the RPE parameter W(h) to construct a weighted sum over the input sequence at each position $l\\in[L+1]$ Here $W_{P}^{(h)}$ is the RPE matri of the $h$ th head. ", "page_idx": 3}, {"type": "text", "text": "Feed-Forward Network with Normalization. Following the first attention layer, we concatenate the outputs of the $H$ attention heads and define $V\\,=\\,\\bigl(\\bar{V^{(1)}},\\ldots,V^{(H)}\\bigr)\\,\\in\\,\\mathbb{R}^{(L+1)\\times H d}$ . Here we abuse the notation and write $V=(v_{1},\\ldots,v_{L+1})^{\\top}$ i.e., each $v_{l}$ is the $l$ -th row of $V$ . For any vector $\\boldsymbol{v}\\in\\mathbb{R}^{H d}$ , we can split it into $(v^{(1)\\top},\\bot...\\,{v^{(H)}}^{\\top})^{\\top}$ where each block $\\boldsymbol{v}^{(h)}\\,\\in\\,\\mathbb{R}^{d}$ . For embedding dimension $d_{e}$ , each vector of $V$ is passed through an FFN $\\phi(\\cdot)\\,:\\,\\mathbb{R}^{H d}\\,\\rightarrow\\,\\mathbb{R}^{d_{e}}$ , which specifies a polynomial kernel such that for any $v,v^{\\prime}\\in\\mathbb{R}^{H d}$ , we have ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\langle\\phi(v),\\phi(v^{\\prime})\\rangle=\\sum_{{\\cal S}\\in[H]_{\\leq D}}c_{{\\cal S}}^{2}\\cdot\\prod_{h\\in{\\cal S}}\\langle v^{(h)},v^{\\prime}{}^{(h)}\\rangle.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here, the low-degree parent set $[H]_{\\leq D}:=\\{S\\subseteq[H]:|S|\\leq D\\}$ contains all subsets of $[H]$ with cardinality at most $D$ and $\\{c_{\\mathcal{S}}:\\mathcal{S}\\in[H]_{\\leq D}\\}$ are the corresponding trainable parameters of $\\phi(\\cdot)$ Therefore, the FFN $\\phi(\\cdot)$ specifies a kernel on the output of the multihead attention which induces a special inner product structure. While (2.3) characterizes $\\phi(\\cdot)$ implicitly, we provide an explicit construction of $\\bar{\\phi}(\\cdot)$ in Lemma C.1 as a vector-valued mapping whose entries are monomials of the input's entries. Moreover, the complexity of $\\phi(\\cdot)$ is controlled by the maximum degree $D$ , which also influences the embedding dimension $d_{e}$ as we show in the construction. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Furthermore, to control the magnitude of the FFN outputs, we normalize $\\phi(\\cdot)$ by letting $u_{l}=$ $\\phi(v_{l})/\\sqrt{C_{D}}$ for all $l~\\in~[L+1]$ where we define $\\begin{array}{r}{C_{D}\\;=\\;\\sum_{\\mathcal{S}\\in[H]_{\\leq D}}c_{\\mathcal{S}}^{2}}\\end{array}$ Sucha normalizaion scheme is motivated by the standard layer normalization (Ba et al., 2016) in transformer architectures. To motivate the use of $\\sqrt{C_{D}}$ as the normalization, consider a special case where the positional embeddings, after the softmax function, produce attention weights that are close to one-hot for each head. Then $v_{l}^{(h)}$ in (2.2) is equal to some token in $x_{1:L}$ . As a result, each $v_{l}$ consists of $H$ tokens and ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\|\\phi(\\boldsymbol{v}_{l})\\|_{2}=\\sqrt{\\sum_{\\boldsymbol{S}\\in[H]}}_{\\leq D}\\,c_{\\boldsymbol{S}}^{2}\\cdot\\prod_{h\\in\\boldsymbol{S}}\\langle\\boldsymbol{v}_{l}^{(h)},\\boldsymbol{v}_{l}^{(h)}\\rangle=\\sqrt{C_{D}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Thus, $u_{l}$ is roughly equivalent to the output of the layer normalization $\\phi(v_{l})/||\\phi(v_{l})||_{2}$ (without trainable parameters). Although our theoretical analysis and simulations focus on this simplified version of layer normalization, our additional experiments in $\\S B.2$ demonstrate that it aligns well with the performance of the actual layer normalization. ", "page_idx": 4}, {"type": "text", "text": "The Second Attention Layer.  The normalized vector sequence $U=(u_{1},\\dots,u_{L+1})^{\\top}$ and the original sequence $\\tilde{X}$ are then fed into the second attention layer to generate the final output. In particular, $u_{L+1}$ is used as the query to compare with the keys $\\left\\{u_{M+1},\\dotsc,u_{L}\\right\\}$ , and the resulting attention scores are used to aggregate the values $x_{(M+1):L}.$ This attention layer has a single head and a scalar trainable parameter $a$ We let $U_{1:L}=(u_{1},\\dots,u_{L})^{\\top}\\in\\mathbb{R}^{L\\times d_{e}}$ and denote by $\\mathtt{M a s k}(\\cdot)$ the mask that sets every entry of the first $M$ rows of a matrix to be $-\\infty$ . The final output is given by ", "page_idx": 4}, {"type": "equation", "text": "$$\ny=\\sum_{j=M+1}^{L}\\sigma_{j}\\left(a\\cdot u_{L+1}^{\\top}\\mathsf{M a s k}(U_{1:L}^{\\top})\\right)\\cdot x_{j}=\\sum_{j=M+1}^{L}\\frac{\\exp\\left(a\\cdot u_{L+1}^{\\top}u_{j}\\right)\\cdot x_{j}}{\\sum_{k=M+1}^{L}\\exp\\left(a\\cdot u_{L+1}^{\\top}u_{k}\\right)}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Note that the softmax function in (2.4) yields a probability distribution over $[L]$ and that $x_{1:L}$ isa sequence of one-hot vectors. Thus $y$ in (2.4) is a probability distribution over $\\mathcal{X}$ . The mask operator is included here just to simplify our analysis while in the experiments we are not using the mask. ", "page_idx": 4}, {"type": "text", "text": "In summary, given the input $\\begin{array}{r}{\\tilde{X}~~\\in~~\\mathbb{R}^{(L+1)\\times d}}\\end{array}$ , in the matrix form, our transformer model $\\mathrm{TF}(M,H,d,D)$ consecutively applies the following operations: ", "page_idx": 4}, {"type": "table", "img_path": "4fN2REs0Ma/tmp/d6bdfba28a1af92d0bfb6cee701afded86e4c0533474bd0ee352b907584c9372.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "The trainable parameters of the above transformer model are denoted by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Theta=\\big\\{a,\\{w_{-1}^{(h)},\\dots,w_{-M}^{(h)}\\}_{h\\in[H]},\\{c_{\\cal S}:{\\cal S}\\in[H]_{\\leq{\\cal D}}\\}\\big\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We remark that the transformer model in (2.5) is known as a disentangled transformer (Friedman et al., 2024), which is a version of the transformer model that is more amenable for theoretical analysis. One thing to be noted is that there is a residual connection that directly copies $\\tilde{X}$ to the output of the FFN & Normalize block, which gives us $[U,{\\tilde{X}}]$ , and the second attention layer will treat the copied $\\tilde{X}$ as the value in the attention mechanism. We omit the residual connection in the above paradigm for notation simplicity. As shown in Nichani et al. (2024), any standard transformer model can be expressed as a disentangled transformer by specializing the attention weights to allow feature concatenation. ", "page_idx": 4}, {"type": "text", "text": "Our goal is to investigate whether the transformer model $\\mathrm{TF}(M,H,d,D)$ can perform ICL over $n$ -gram Markov chains and further, whether such capability can be learned from data with common training algorithms like gradient descent. ", "page_idx": 4}, {"type": "text", "text": "3  Theoretical Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we present the theoretical results. We first show in $\\S3.1$ and $\\mathrm{\\SC.1}$ that there exists a transformer in $\\mathrm{TF}(\\bar{M},H,d,D)$ that implements a generalized \u201cinduction head\" mechanism (Olsson et al., 2022) with a learned feature, which serves as a natural algorithm for learning $n$ gramMarkov chains. Then in $\\S3.2$ we prove that the gradient fow in (3.4) finds such a desired model asymptotically. ", "page_idx": 5}, {"type": "text", "text": "3.1  Generalized Induction Head Mechanism for Learning $n$ Gram Markov Chains ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Recall that we define the mixture of $n$ -gram Markov chain model $(\\mathcal{X},\\mathtt{p a},\\mathcal{P},\\mu_{0})$ in $\\S2.1$ , where $\\mathcal{P}$ is a distribution over the Markov transition kernels. For regularity, we assume existence of a unique stationary distribution for any $\\pi\\in\\operatorname{supp}(\\mathcal{P})$ , where a rigorous statement is deferred to Assumption 3.5. We also assume the window size $M>r_{n}$ . For any $n$ gram Markov chain with transition kernel $\\pi\\sim\\mathcal{P}$ ,we let $\\mu^{\\pi}\\in\\Delta(\\mathcal{X}^{M+1})$ denote the stationary distribution of theMarkov chain over a window of size $M+1$ Here we use $\\{z_{\\ell}\\}_{l\\ge1}$ to denote a random sequence of tokens generated by the Markov chain. Then $\\mu^{\\pi}$ denotes the joint distribution of a block of $M+1$ tokens $\\left(z_{l-M},\\ldots,z_{l-1},z_{l}\\right)$ under the stationary distribution of $\\pi$ , where $l>M$ is an integer. ", "page_idx": 5}, {"type": "text", "text": "In the following, we introduce a generalized induction head (GIH) estimator for the task of predicting $x_{L+1}$ given $x_{1:L}$ , which is based on the following simple idea: $x_{L+1}$ should be similar to a previous token $x_{l}$ if their parents are similar. As the parent set pa is unknown, GIH adopts an informationtheoretic criterion to select a subset of previous tokens as a proxy of the parents. Specifically, GIH uses a modified version of $\\chi^{2}$ -mutual information, which is defined as follows. ", "page_idx": 5}, {"type": "text", "text": "Definition  3.1  (Modified $\\chi^{2}$ -Mutual Information).  We take a length- $(M\\,+\\,1)$ windows $\\left(z_{l-M},\\ldots,z_{l-1},z_{l}\\right)$ for some $l\\,>\\,M$ and suppose the sequence is sampled from stationary distribution $\\mu^{\\pi}$ with $\\pi\\sim\\mathcal{P}$ .Let $Z=(z_{l-M},\\ldots,z_{l-1})$ . For any subset $S\\subseteq[\\bar{M}]$ we use $Z_{-s}$ to denote the subvector of $Z$ containing entries of the form $z_{l-s}$ \uff0c $\\forall s\\in S$ For instance, suppose ${\\cal S}=\\{2,5\\}$ \uff0c then $Z_{-S}=(z_{l-5},z_{l-2})$ .The modified $\\chi^{2}$ -mutual information for $\\boldsymbol{S}$ is defined as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\widetilde{I}_{\\chi^{2}}(S)=\\mathbb{E}_{\\pi\\sim\\mathcal{P},(z,Z)\\sim\\mu^{\\pi}}\\bigg[\\bigg(\\sum_{e\\in\\mathcal{X}}\\frac{[\\mu^{\\pi}(z=e\\,|\\,Z_{-}s)]^{2}}{\\mu^{\\pi}(z=e)}-1\\bigg)\\cdot\\mu^{\\pi}(Z_{-}s)\\bigg],\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mu^{\\pi}(z=\\cdot\\mid Z_{-{\\cal S}})$ is the conditional distribution of $z$ induced by $\\mu^{\\pi}$ given the partial history $Z_{-s}$ , and $\\widetilde{\\mu}^{\\pi}(Z_{-{\\cal S}}),\\mu^{\\pi}(z)$ are the marginal distributions of $Z_{-s}$ and $z$ under $(z,Z)\\sim\\mu^{\\pi}$ ", "page_idx": 5}, {"type": "text", "text": "Intuitively, $\\widetilde{I}_{\\chi^{2}}(S)$ is modified from the vanilla $\\chi^{2}$ -mutual information $(\\chi^{2}{\\mathrm{-}}\\mathbf{M}\\mathrm{I})$ between two random variables (Polyanskiy and $\\mathrm{Wu}$ , 2024) and quantifies how much information the partial history $Z_{-s}$ contains about $z$ . In particular, we incorporate an additional $\\mu^{\\pi}(Z_{-{\\mathcal{S}}})$ term that decreases with the growing size of $\\boldsymbol{S}$ . To see the rationality, we first introduce a GIH estimator based on the modified $\\bar{\\chi}^{2}$ -mutual information. ", "page_idx": 5}, {"type": "text", "text": "Definition 3.2 (Generalized Induction Head). A GIH estimator with window size $M\\in\\mathbb{N}$ feature size $D\\in\\mathbb{N}$ is denoted by $\\mathtt{G I H}(\\cdot;M,D)$ ,which maps $x_{1:L}$ to a distribution over $\\mathcal{X}$ We let $S^{\\star}$ be the information-optimal subset (referred to as the\u201cinformation set\u201din the sequel\u00b2\uff09of $[M]$ with size no more than $D$ thatmaximizesthemodified $\\chi^{2}$ -mutual information $\\widetilde{I}_{\\chi^{2}}(\\cdot)$ defined in (3.1). That is, we definetheinformationset $S^{*}$ as ", "page_idx": 5}, {"type": "equation", "text": "$$\nS^{\\star}=\\mathrm{argmax}_{S\\in[M]_{\\leq D}}\\widetilde{I}_{\\chi^{2}}(S).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Then $\\mathsf{G I H}(x_{1:L};M,D)$ outputs ", "page_idx": 5}, {"type": "equation", "text": "$$\ny^{\\star}:=\\left\\{\\!\\!\\begin{array}{l}{\\!N^{-1}\\cdot\\sum_{l=M+1}^{L}x_{l}\\cdot\\mathbb{1}(X_{l-S^{\\star}}=X_{L+1-S^{\\star}}),\\ i f\\,N\\geq1,}\\\\ {\\!\\!\\!\\!(L-M)^{-1}\\cdot\\sum_{l=M+1}^{L}x_{l},\\quad o t h e r w i s e.}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Here, we define $X_{l-S^{\\star}}$ as the set $\\{x_{l-s}:s\\in S^{\\star}\\}$ and $\\begin{array}{r}{N=\\sum_{l=M+1}^{L}\\mathbb{1}(X_{l-S^{\\star}}=X_{L+1-S^{\\star}}).}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "Note that $S^{\\star}$ defined in (3.2) depends on the choices of $M$ and $D$ and serves as a proxy of the unknown parent set pa based on $\\widetilde{I}_{\\chi^{2}}(\\cdot)$ defined in (3.1). In a nutshell, the GIH estimator checks whether the partial histories of $X_{l-S^{\\star}}$ and $X_{L+1-S^{\\star}}$ match and aggregate all the tokens $x_{l}$ that have a matching partial history to predict $x_{L+1}$ . As a remark, using the modified $\\chi^{2}$ -MI as the information criterion rules out redundancy in the information set $S^{\\star}$ in the following sense: ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "$S^{\\star}$ cannot be a superset of the true parents. Note that if $\\boldsymbol{S}$ is a superset of the true parent set, by the Markov property, $z$ and $Z_{-S}$ are conditionally independent given the true parents $Z_{\\mathtt{p a}}$ . Thus, maximizing the vanilla $\\chi^{2}$ -mutual information yields multiple maximizers, i.e., all the supersets of the true parent set. However, with the modification in (3.1), any superset yields a strictly smaller $\\widetilde{I}_{\\chi^{2}}$ compared to the exact parent set, making them suboptimal. ", "page_idx": 6}, {"type": "text", "text": "\u00b7Themodified $\\chi^{2}$ -MI selects informative partial history. Even a true parent may bear relatively little information about the target compared to other parents sometimes. Meanwhile, exact match of a larger set of partial history becomes much harder as it tends to appear less frequently in the context sequence, leading to poor estimation accuracy for the estimator in (3.3). The modified $\\chi^{2}$ MIreaches a balance by selecting the informative partial history while penalizing the size of the information set. ", "page_idx": 6}, {"type": "text", "text": "The term involving $\\mu^{\\pi}(z\\,=\\,\\cdot\\,|\\,Z_{-S})$ can be viewed as the signal part which helps us to find an informative subset $\\boldsymbol{S}$ . The term $\\mu^{\\pi}(Z_{-{\\mathcal{S}}})$ can be viewed as penalty on the model complexity which favors smaller subsets. Thus, the modified $\\chi^{2}$ -MI strikes a balance between these two objectives and enables us to find a good proxy $S^{\\star}$ of pa when $L$ is finite. Moreover, when $L$ is sufficiently large, we identify two scenarios in which maximizing $\\widetilde{I}_{\\chi^{2}}(\\cdot)$ yields the true parent set (see $\\mathrm{\\&C.7}$ for details). Moreover, the GIH estimator is a generalization of the induction head mechanism (Elhage et al., 2021) to the stochastic setting with multiple parents, where we give the model more fexibility to learn based on a partial history that does not necessarily correspond to the true parent set. As we will show in $\\S C.1$ , the GIH mechanism can be implemented by the transformer model. ", "page_idx": 6}, {"type": "text", "text": "3.2  Convergence Guarantee of Gradient Flow ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In the following, we present the convergence guarantee for gradient flow. To simplify the discussion, we consider the case where $H\\;=\\;M$ , meaning there are enough heads to implement the GIH mechanism by having each head copy a unique parent token from a window of size $M$ . Let us first introduce the paradigm of training by gradient flow. ", "page_idx": 6}, {"type": "text", "text": "Training Paradigm. Consider training a transformer $\\mathrm{TF}(M,H,d,D)$ in (2.5) with $M\\,=\\,H$ to perform ICL on the $n$ -gram Markov chain model introduced in $\\S2.1$ . Specifically, we define $\\mathcal{L}(\\Theta)$ as the population cross-entropy loss in (2.1), where the transformer model $f_{\\tt t f}$ is given by (2.5) with a parameter $\\Theta$ Ideally, when training the parameter $\\Theta$ with gradient flow, the dynamics with respect to the loss $\\mathcal{L}(\\Theta)$ is given by: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\partial_{t}\\Theta(t)=-\\nabla\\mathcal{L}\\big(\\Theta(t)\\big).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We consider a three-stage training paradigm where, in each stage, only a specific subset of the weights is trained by gradient fow. The three stages are outlined in Table i. Specifically, in the first stage, we only train the FFN layer via gradient flow while keeping other weights fixed. We then only train the RPE weights in the first attention layer in the second stage. Finally, we only train the weight $a$ in the second attention layer in the last stage, while fixing the rest of the parameters. This training approach is primarily used for analytical convenience; in practice, the entire model can be trained simultaneously, and similar convergence results are reported in $\\S B.2$ .From a theoretical standpoint, we will also justify the three-stage paradigm in the discussion following Theorem 3.6. ", "page_idx": 6}, {"type": "text", "text": "Initialization Conditions. Before presenting our main results about how training by gradient fow induces the GIH structure, let us introduce the following assumption on the initialization of the weights. We define the information gap within the $D$ -degreeparent set $[H]_{\\leq D}$ as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\Delta\\widetilde{I}_{\\chi^{2}}=\\widetilde{I}_{\\chi^{2}}(\\boldsymbol{S}^{\\star})-\\operatorname*{max}_{\\boldsymbol{S}\\in[H]_{\\leq D}\\backslash\\{\\boldsymbol{S}^{\\star}\\}}\\widetilde{I}_{\\chi^{2}}(\\boldsymbol{S}),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where we recall that $S^{\\star}$ defined in (3.2) maximizes the modified $\\chi^{2}$ mutual information. ", "page_idx": 6}, {"type": "text", "text": "Assumption 3.3 (Initialization). We assume that the following holds at initialization: ", "page_idx": 6}, {"type": "table", "img_path": "4fN2REs0Ma/tmp/1444442b5a75b38acb1a9a7d338460c6f75eff63886ae77629970f1a0ea0c957.jpg", "table_caption": [], "table_footnote": ["Table 1: Three-stage training paradigm for gradient fow. Here, the \"Weights to Train\" column indicates the weights updated in each stage, and the \u201cDescription\u201d column summarizes the corresponding results from Theorem 3.6. "], "page_idx": 7}, {"type": "text", "text": "1.Fortfr $w_{-h}^{(h)}\\geq w_{-j}^{(h)}+\\Delta w$ forall $h,j\\in[H]$ with $j\\neq h$ where $\\Delta w>0$ is a positive scalar satisfying ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\Delta w\\ge\\log(M-1)-\\log\\Big[\\Big(1+\\Delta\\widetilde{I}_{\\chi^{2}}/(14\\widetilde{I}_{\\chi^{2}}(S^{\\star}))\\Big]^{\\frac{1}{2H}}-1\\Big).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The first assumption on the RPE is used to induce the correspondence between parents and heads during the training by slightly breaking the symmetry between different attention heads. The second assumption on the scale of $a$ ensures that the attention probability given by the second attention layer is close to the uniform distribution over $[L]$ . These initialization conditions enable us to derive clean descriptions for the dynamics of the first attention layer and the FFN, shedding light on their respective roles in executing ICL. ", "page_idx": 7}, {"type": "text", "text": "We now outline our assumptions on the Markov chain used in the data generation process. Recall that $r_{n}$ is the largest absolute integer in the parent set pa. For any position $l$ , we define the history $Z=(z_{l-r_{n}},\\ldots,z_{l-1})$ as the last state and $Z^{\\prime}=(z_{l-r_{n}+1},\\ldots,z_{l})$ as the current state. Since the parent of the new token $z_{l}$ is already included in $Z$ \uff0c $Z^{\\prime}$ is independent of all prior history given $Z$ forming a Markov chain. ", "page_idx": 7}, {"type": "text", "text": "We define $P_{\\pi}$ as the $d^{r_{n}}\\times d^{r_{n}}$ transition matrix for this Markov chain, where states are successive $r_{n}$ -tokens. Each row of $P_{\\pi}$ is indexed by $Z^{\\prime}$ and each column by $Z$ . The matrix element $P_{\\pi}(Z^{\\prime},Z)$ is thus given by ", "page_idx": 7}, {"type": "equation", "text": "$$\nP_{\\pi}(Z^{\\prime},Z)=\\pi(z_{l}^{\\prime}\\mid Z_{\\mathtt{p a}(l)})\\cdot\\mathtt{l l}(Z_{l-r_{n}+1:-1}^{\\prime}=Z_{l-r_{n}+1:-1}).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "This means that to transition from $Z$ to $Z^{\\prime}$ , all elements of $Z^{\\prime}$ except for $z_{-1}^{\\prime}$ must match the last $r_{n}-1$ tokens of $Z$ . The token $z_{l}^{\\prime}$ is then sampled according to the transition kernel $\\pi$ and depends only on the parent $Z_{\\mathtt{p a}(l)}$ . The above definition is in fact independent of the position $l$ as the transition kernel $\\pi$ is the same across all positions. Note that $P_{\\pi}$ is also a stochastic matrix but with zero entries due to the indicator. To proceed, we need the following notion of primitive matrix to state our assumption on $P_{\\pi}$ ", "page_idx": 7}, {"type": "text", "text": "Definition 3.4 (Primitive Matrix). A nonnegative and irreducible square matrix $P$ iscalled primitive if there exists a positive integer $k$ such that all entries of $P^{k}$ arepositive. ", "page_idx": 7}, {"type": "text", "text": "We defer more details about the above definition to $\\S C.3$ . By the celebrated Perron-Frobenius theorem, if a stochastic matrix $P_{\\pi}$ is also primitive, then (i) there exists a unique stationary distribution for the Markov chain; (ii) $P_{\\pi}$ has a unique leading eigenvalue equal to 1, and the corresponding eigenvector is the stationary distribution. Next, we state the assumptions on the mixture of Markov chains for data generation. ", "page_idx": 7}, {"type": "text", "text": "Assumption 3.5 (Markov Chain). For any $\\pi\\in\\operatorname{supp}(\\mathcal{P})$ weassumethat: ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "1. The transition matrix $P_{\\pi}$ is primitive.In particular, we assume that there exists $\\lambda<1$ such that the eigenvalue of $P_{\\pi}$ with the second largest magnitude satisfies $|\\lambda_{2}(P_{\\pi})|\\leq\\lambda.$ Note that $\\lambda_{2}(P_{\\pi})$ can be complex-valued. ", "page_idx": 7}, {"type": "text", "text": "In fact, the second condition $\\pi(\\cdot\\,|\\,X_{\\tt p a})>\\gamma$ already ensures that $P_{\\pi}$ must be primitive, as is required by the first condition. See Corollary F.14 for details. On the high level, the first assumption guarantees a unique stationary distribution as well as a fast mixing rate of the Markov chain by ensuring a spectral gap for $P_{\\pi}$ . The second assumption implies a lower bound on the probability for any set ${\\mathcal{S}}\\subseteq[M]$ under the stationary distribution, i.e., $\\mu^{\\bar{\\pi}}(X_{l-\\mathcal{S}})\\geq\\gamma^{|\\mathcal{S}|}$ for any $l>M$ . See Corollary F.15 for details. ", "page_idx": 8}, {"type": "text", "text": "Now we are ready to present our main theoretical result on training transformers by gradient fow. Theorem 3.6 (Convergence of Gradient Flow). Suppose Assumption 3.3 and Assumption 3.5 hold. Consider $H\\ge M$ We set $\\varepsilon=L^{-1/2}$ for the cross-entropy loss and assume $L$ is sufficiently large. Then the following holds for the three-stage training of gradient flow: ", "page_idx": 8}, {"type": "text", "text": "Stage I: Parent Selection by FFN. Let $\\begin{array}{r}{C_{D}(t)\\,=\\,\\sum_{S\\in[H]_{\\leq D}}c_{S}(t)^{2}}\\end{array}$ and $p_{S^{\\star}}(t)\\,=\\,c_{S^{\\star}}^{2}(t)/C_{D}(t)$ Then in the first stage with duration $t_{1}\\asymp C_{D}(0)\\log L/(a(0)\\Delta\\tilde{I}_{\\chi^{2}})$ the ratio $c_{S^{\\star}}/c_{S}$ grows exponentially fast for any $S\\ne S^{\\star}$ ,and $S^{\\star}$ dominates exponentially fast in the sense that, ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{1-p_{S^{\\star}}(t)\\leq(1-p_{S^{\\star}}(0))\\cdot\\exp\\bigl(-(2C_{D})^{-1}\\cdot a(0)\\cdot p_{S^{\\star}}(0)\\cdot\\Delta\\tilde{I}_{{X^{2}}}\\cdot t\\bigr),\\quad\\forall t\\in[0,t_{1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Stage II: Concentration of The First Attention. Define $\\sigma^{(h)}(t)\\;=\\;\\sigma(w^{(h)}(t))\\;\\in\\;\\mathbb{R}^{M}$ ,and let $\\begin{array}{r}{\\sigma_{\\operatorname*{min}}(t):=\\operatorname*{min}_{h\\in S^{\\star}}\\sigma_{-h}^{(h)}(t)}\\end{array}$ Then in the second stage with duration $t_{2}-t_{1}\\asymp L/(a(0)\\Delta\\tilde{I}_{\\chi^{2}})$ \uff0c the first layer's attention heads have attention probabilities concentrated on the optimal information set $S^{\\star}$ in the sense that for any $t\\in[t_{1},\\bar{t}_{1}+t_{2})$ \uff0c ", "page_idx": 8}, {"type": "equation", "text": "$$\n1-\\prod_{h\\in\\mathcal{S}^{\\star}}(\\sigma_{-h}^{(h)}(t))^{2}\\le\\frac{2|\\mathcal{S}^{\\star}|\\cdot(M-1)}{a(0)\\cdot\\Delta\\tilde{I}_{\\chi^{2}}\\cdot\\sigma_{\\operatorname*{min}}(0)\\cdot(t-t_{1})/2+\\exp(\\Delta w)+(M-1)}\\wedge1.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Stage Ml: Growth of The Second Attention.For some constants $c_{1},c_{2}$ depending on $(\\mathcal{P},S^{\\star})$ with $0\\,<\\,c_{1}\\,<\\,c_{2}$ , there exists a small constant $\\delta\\,>\\,0$ such that the growth of $a(t)$ exhibits the following two sub-stages: (i) When $a(t)\\leq\\log(c_{1}/\\delta)$ , it holds that $\\partial a(t)\\asymp e^{a(t)}$ ; (ii) After $a(t)$ has grown such that $a(t)\\geq\\log(c_{2}/\\delta)$ then $\\partial_{t}a(t)\\stackrel{}{\\sim}1/a(t)$ until it reaches the value $\\log L/8$ ", "page_idx": 8}, {"type": "text", "text": "See $\\S D$ for a proof sketch and $\\S E$ for the detailed proof. We require that $L$ is sufficiently large, and the specific conditions for $L$ are deferred to $\\S E.1$ ", "page_idx": 8}, {"type": "text", "text": "Interpretation of Training Dynamics. We empirically verify Theorem 3.6 by conducting a simulation experiment. In particular, we train a transformer with $H=M=3$ and ${\\cal D}=2$ based on Markov chain data with $d=2$ $L=100$ and $\\mathtt{p a}=\\{-1,-2\\}$ . We sample the transition kernel from a Dirichlet prior such that $S^{\\star}=\\{1,2\\}$ also matches the parent set. For more details on this simulation, see $\\S B$ . The results are shown in Figure 3 and align perfectly with Theorem 3.6. From Theorem 3.6, we can interpret the three stages of training dynamics as follows. ", "page_idx": 8}, {"type": "text", "text": "\u25cf In the first stage, the training of FFN parameters learns a selector that selects an informative set $S^{\\star}$ by realizing the corresponding feature embedding through the polynomial kernel. That is, when $t$ is sufficiently large, we have $p s{\\star}(t)\\approx1$ and $p_{S}(\\bar{t})\\approx0$ for all $S\\ne S^{\\star}$ . In this case, for any input vectors $v,v^{\\prime}\\in\\mathbb{R}^{H d}$ , the inner product in (2.3) reduces to ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\langle\\phi(v),\\phi(v^{\\prime})\\rangle\\approx c_{S^{\\star}}^{2}\\cdot\\prod_{h\\in S^{\\star}}\\langle v^{(h)},v^{\\prime(h)}\\rangle.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "That is, FFN only selects the blocks in $S^{\\star}$ as the feature. We observe this phenomenon in the experiment, where we set $S^{\\star}=\\{1,2\\}$ . As shown in Figure 3-(a), it is clear that $c_{S^{\\star}}$ immediately dominates the rest of $c_{S}$ 's within only a few gradient epochs. ", "page_idx": 8}, {"type": "text", "text": "\u00b7 In the second stage, we update the parameters of the RPE. This stage turns the first attention layer into a copier by establishing the correspondence between the attention heads and the parents in the selected $S^{\\star}$ . That is, each attention head copies a particular parent in $S^{\\star}$ . Specifically, when $t$ is sufficiently large, for any $h\\,\\in\\,S^{\\star}$ $S^{\\star},\\bar{\\sigma}^{(h)}(t)\\,\\bar{=}\\,\\sigma(w^{(h)}\\bar{(}t))\\approx1$ Recaling the constrution of RPE, tis implies that $v_{l}^{(h)}$ in (2.2) becomes $x_{l-h}$ for all $h\\in S^{\\star}$ . As shown in Figure 3-(b), in the experiment, the first two heads initialized towards the first two parents will deterministically copy parents $-1$ and $-2$ eventually. The third head stays close to its initial value. This head has a negligible effect on the output because $3\\notin S^{\\star}$ and $p s\\star\\approx1$ ", "page_idx": 8}, {"type": "image", "img_path": "4fN2REs0Ma/tmp/05371db0f1344e6d2043ce6f01205fc2c9788d80c30304f2b1f8d1891c9eab10.jpg", "img_caption": ["Figure 3: An illustration of the transformer parameters during the three-stage training. We train a transformer in $\\mathsf{T F}(M=3,H=3,d=3,D=2)$ With $L=100$ $\\mathtt{p a}=\\{-1,-\\dot{2}\\}$ .See $\\S B$ and Figure 4 for more details of the simulation. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "\u00b7 After the first two stages are completed, we know that the features constructed approximately satisfy (C.1) up to a proportionality factor. Then, in the final training stage, the scalar weight $a$ in the second attention layer keeps increasing. Thus, this stage learns an exponential kernel classifier as specified in (C.2). When $a(t)$ is sufficiently large, the learned transformer is close to a classifier that uses covariate-label pairs of the form $(X_{l-S^{\\star}},x_{l})$ to predict $x_{L+1}$ . In particular, when $a(t)$ goes to infinity, the transformer exactly becomes the GIH mechanism given in Definition 3.2. Moreover, we theoretically prove that the increasing trajectory of $\\bar{a}(t)$ has two stages, where $\\mathrm{d}a(t)/\\mathrm{d}t$ is initially large and gradually decays, this is also clearly observed in the experiment. See Figure 3-s(c) for details. ", "page_idx": 9}, {"type": "text", "text": "In summary, we theoretically show that the limiting model obtained by three-stage training approximately implements the GIH mechanism. We will prove that the difference between these two estimators is at most $O(L^{-1/8})$ . We defer the formal statement and proof to $\\S E.5$ . Moreover, as an answer to the Question (i) raised in $\\S1$ , the different components of the transformer architecture are all critical for achieving this: FFN with normalization realizes the selector, the multi-head design of attention supports the copier, and finally, the softmax operation facilitates the exponential kernel classifier. These components work organically as a whole system, yielding the trained transformer's capability of ICL of $n$ -gram Markov chains. ", "page_idx": 9}, {"type": "text", "text": "Another takeaway from Theorem 3.6 is a strict separation in the growth rate of these three stages. In particular, the convergence rates of the corresponding components of the transformer model in these three stages range from exponentially fast (Stage I), polynomially fast (Stage II), to logarithmically slow (Stage Il). With such two exponential separations of convergence rates, we expect that these three stages naturally arise when we simultaneously train the whole model via gradient descent/flow. We empirically verify this argument and the details are deferred to $\\S B.2$ ", "page_idx": 9}, {"type": "text", "text": "In $\\mathrm{\\&C.7}$ , we provide more intuitive interpretation of the modified $\\chi^{2}$ -mutual information, which demonstrates a balance of model complexity and information richness. ", "page_idx": 9}, {"type": "text", "text": "4  Conclusion and Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we have studied the training dynamics of a two-attention-layer transformer model for learning $n$ -gram Markov chains in an in-context way. Our work opens new directions for developing a rigorous understanding of the transformer models, which includes understanding the induction head mechanism with standard FFN layer and investigating the training dynamics beyond a single loop of this induction head mechanism. We defer readers to $\\S C.8$ for more discussions. ", "page_idx": 9}, {"type": "text", "text": "5 Acknowledgement ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We acknowledge Shaobo Wang for his help with the experiments. We also thank Jason D. Lee, Alex Damian, and Eshaan Nichani for their helpful discussions. Zhuoran Yang acknowledges the support of NSF under the award DMS-2413243. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "Achiam, J., .Adler, S., Agarwal, S., Ahmad, L., Akkaya, I.,  Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S. et al. (2023). Gpt-4 technical report. arXiv preprint arXiv:2303.08774.   \nAhn, K., Cheng, X., Daneshmand, H. and Sra, S. (2023). Transformers learn to implement preconditioned gradient descent for in-context learning. arXiv preprint arXiv:2306.00297.   \nAhuja, K., Panwar, M. and Goyal, N. (2023). In-context learning through the bayesian prism. arXiv preprint arXiv:2306.04891.   \nAkyirek, E., Schuurmans, D., Andreas, J., Ma, T. and Zhou, D. (2023). What learning algorithm is in-context learning? investigations with linear models. In The Eleventh International Conference on Learning Representations.   \nAlayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M. et al. (2022).  Flamingo: a visual language model for few-shot learning. Advances in neural information processing systems, 35 237i6-23736.   \nAnthropic (2023). Model card and evaluations for claude models.   \nBa, J. L., Kiros, J.R. and Hinton, G. E. (2016). Layer normalization. arXiv preprint arXiv: 1607.06450.   \nBai, Y, Chen, F., Wang, H., Xiong, C. and Mei, S. (2023). Transformers as statisticians: Provable in-context learning with in-context algorithm selection. arXiv preprint arXiv:2306.04637.   \nBietti, A., Cabannes, V., Bouchacourt, D., Jegou, H. and Bottou, L. (2024). Birth of a transformer: A memory viewpoint. Advances in Neural Information Processing Systems, 36.   \nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P, Neelakantan, A., Shyam, P, Sastry, G., Askel, A. et al. (2020). Language models are few-shot learners. Advances in neural information processing systems, 33 1877-1901.   \nCabannes, V., Arnal, C., Bouaziz, W., Yang, A., Charton, F. and Kempe, J. (2024). Iteration head: A mechanistic study of chain-of-thought. arXiv preprint arXiv:2406.02128.   \nChen, S. and Li,Y. (2024). Provably learning a multi-head attention layer. arXiv preprint arXiv:2402.04084.   \nChen, S., Sheen, H., Wang, T. and Yang, Z. (2024). Training dynamics of multi-head sofmax attention for in-context learning: Emergence, convergence, and optimality. arXiv preprint arXiv:2402.19442.   \nChen, S., Yang, D., Li, J., Wang, S., Yang, Z. and Wang, Z. (2022).  Adaptive model design for markov decision process. In International Conference on Machine Learning. PMLR.   \nChen, X. and Zou, D. (2024). What can transformer learn with varying depth? case studies on sequence learning tasks. arXiv preprint arXiv:2404.01601.   \nCheng, X., Chen, Y and Sra, S. (2023). Transformers implement functional gradient descent to learn non-linear functions in context. arXiv preprint arXiv:2312.06528.   \nCollins, L., Parulekar, A., Mokhtari, A., Sanghavi, S. and Shakkottai, S. (2024). In-context learning with transformers: Softmax attention adapts to function lipschitzness. arXiv preprint arXiv:2402.11639.   \nDeora, P., Ghaderi, R., Taheri, H. and Thrampoulidis, C. (2023). On the optimization and generalization of multi-head attention. arXiv preprint arXiv:2310.12680.   \nEdelman, B. L., Edelman, E., Goel, S., Malach, E. and Tsilivis, N. (2024). The evolution of statistical induction heads: In-context learning markov chains. arXiv preprint arXiv:2402.11004.   \nEdelman, B. L., Goel, S., Kakade, S. and Zhang, C. (2022). Inductive biases and variable creation in self-attention mechanisms. In International Conference on Machine Learning. PMLR.   \nElhage, N., Nanda, N., Olsson, C., Henighan, T., Joseph, N., Mann, B., Askell, A., Bai, Y., Chen, A., Conerly, T. et al. (2021). A mathematical framework for transformer circuits. Transformer Circuits Thread, 1 1.   \nFriedman, D., Wettig, A. and Chen, D. (2024). Learning transformer programs. Advances in Neural Information Processing Systems, 36.   \nFu, D., Chen, T.-Q., Jia, R. and Sharan, V. (2023). Transformers learn higher-order optimization methods for in-context learning: A study with linear models. arXiv preprint arXiv:2310.17086.   \nGiannou, A, Rajput, S., Sohn, J-Y, Lee, K., Lee, J. D. and Papailiopoulos, D. (2023). Looped transformers as programmable computers. In Proceedings of the 4oth International Conference on Machine Learning (A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato and J. Scarlett, eds.), vol. 202 of Proceedings of Machine Learning Research. PMLR.   \nGiannou, A., Yang, L., Wang, T., Papailiopoulos, D. and Lee, J. D. (2024). How well can transformers emulate in-context newton's method? arXiv preprint arXiv:2403.03183.   \nGuo, T., Hu, W., Mei, S., Wang, H., Xiong, C., Savarese, S. and Bai, Y. (2023). How do transformers learn in-context beyond simple functions? a case study on learning with representations. arXiv preprint arXiv:2310.10616.   \nHe, J., Chen, S., Zhang, F. and Yang, Z. (2024). From words to actions: Unveiling the theoretical underpinnings of llm-driven autonomous systems. arXiv preprint arXiv:2405.19883.   \nHe, P, Liu, X., Gao, J. and Chen, W.(2020). Deberta: Decoding-enhanced bert with disentangled attention. arXiv preprint arXiv:2006.03654.   \nHonovich, O., Shaham, U., Bowman, S. R. and Levy, O. (2022). Instruction induction: From few examples to natural language task descriptions. arXiv preprint arXiv:2205.10782.   \nHuang, Y, Cheng, Y and Liang, Y. (2023). In-context convergence of transformers. arXiv preprint arXiv:2310.05249.   \nJelassi, S., Sander, M. and Li, Y. (2022). Vision transformers provably learn spatial structure. Advances in Neural Information Processing Systems, 35 37822-37836.   \nJeon, H. J., Lee, J. D., Lei, Q. and Van Roy, B. (2024). An information-theoretic analysis of incontext learning. arXiv preprint arXiv:2401.15530.   \nKim, J. and Suzuki, T. (2024). Transformers learn nonlinear features in context: Nonconvex meanfield dynamics on the attention landscape. arXiv preprint arXiv:2402.01258.   \nLi, Y, Huang, Y, Idiz,M.E., Rawat, A. S. and Oymak, S. (2024). Mechanics of next token prediction with self-attention. In International Conference on Artifcial Intelligence and Statistics. PMLR.   \nLi, Y., Li, Y.-F. and Risteski, A. (2023). How do transformers learn topic structure: Towards a mechanistic understanding. arXiv preprint arXiv:2303.04245.   \nLin, L., Bai, Y. and Mei, S. (2023). Transformers as decision makers: Provable in-context reinforcement learning via supervised pretraining. arXiv preprint arXiv:2310.08566.   \nLiu, B., Ash, J., Goel, S., Krishnamurthy, A. and Zhang, C. (2022). Transformers learn shortcuts to automata. ArXiv, abs/2210.10749.   \nMahankali, A., Hashimoto, T. B. and Ma, T. (2023). One step of gradient descent is provably the optimal in-context learner with one layer of linear self-attention. arXiv preprint arXiv:2307.03576.   \nMakkuva, A. V., Bondaschi, M., Ekbote, C., Girish, A., Nagle, A., Kim, H. and Gastpar, M. (2024a). Local to global: Learning dynamics and effect of initialization for ransformers. arXiv preprint arXiv:2406.03072.   \nMakkuva, A. V., Bondaschi, M., Girish, A., Nagle, A., Jaggi, M., Kim, H and Gastpar, M. (2024b). Attention with markov: A framework for principled analysis of transformers via markov chains. arXiv preprint arXiv:2402.04161. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Meyer, C. D. (2023). Matrix analysis and applied linear algebra. SIAM. ", "page_idx": 13}, {"type": "text", "text": "Muller, S., Hollmann, N., Arango, S. P., Grabocka, J. and Hutter, F. (2021). Transformers can do bayesian inference. ArXiv, abs/2112.10510.   \nNichani, E., Damian, A. and Le,J D. (2024). How transformers lean causal structure with gradient descent. arXiv preprint arXiv:2402.14735.   \nOlsson, C., Elhage, N., Nanda, N., Joseph, N., DasSarma, N., Henighan, T., Mann, B., Askell, A.. Bai, Y., Chen, A. et al. (2022). In-context learning and induction heads.  arXiv preprint arXiv:2209.11895.   \nPolyanskiy, Y and Wu, Y. (2024). Information Theory: From Coding to Learning. Cambridge University Press.   \nRadford, A, Wu, J, Child, R., Luan, D.,Amodei, D., Sutskever, I. et al. (2019). Language modes are unsupervised multitask learners. OpenAI blog, 1 9.   \nRajaraman, N., Bondaschi, M., Ramchandran, K., Gastpar, M. and Makkuva, A. V. (2024a). Transformers on markov data: Constant depth suffices. arXiv preprint arXiv:2407.17686.   \nRajaraman, N., Jiao, J. and Ramchandran, K. (2024b). Toward a theory of tokenization in llms. arXiv preprint arXiv:2404.08335.   \nSanford, C., Hsu, D. and Telgarsky, M. (2023). Representational strengths and limitations of transformers. arXiv preprint arXiv:2306.02896.   \nSheen, H., Chen, S., Wang, T. and Zhou, H. H. (2024). Implicit regularization of gradient fow on one-layer softmax attention. arXiv preprint arXiv:2403.08699.   \nSinii, V., Nikulin, A., Kurenkov, V., Zisman, I. and Kolesnikov, S. (2023). In-context reinforcement learning for variable action spaces. arXiv preprint arXiv:2312.13327.   \nSong, J. and Zhong, Y. (2023). Uncovering hidden geometry in transformers via disentangling position and context. arXiv preprint arXiv:2310.04861.   \nTarzanagh, D. A., Li, Y, Thrampoulidis, C. and Oymak, S. (2023a). Transformers as support vector machines. ArXiv, abs/2308.16898.   \nTarzanagh, D. A., Li, Y., Zhang, X. and Oymak, S. (2023b). Max-margin token selection in attention mechanism. arXiv preprint arXiv:2306.13596.   \nTeam, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A. et al. (2023)._ Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805.   \nThrampoulidis, C. (2024). Implicit bias of next-token prediction. arXiv preprint arXiv:2402.18551.   \nTian, Y, Wang, Y,Chn, B. and Du, S. (2023a). can and snap:Uderstanding training dynamis and token composition in 1-layer transformer. arXiv preprint arXiv:2305.16380.   \nTian, Y., Wang, Y., Zhang, Z., Chen, B. and Du, S. (2023b). Joma: Demystifying multilayer transformers via joint dynamics of mlp and attention. arXiv preprint arXiv:2310.00535.   \nVasudeva, B., Deora, P and Thrampoulidis, C. (2024). Implicit bias and fast convergence rates for self-attention. arXiv preprint arXiv:2402.05738.   \nVaswani, A., Shazeer, N, Pamar, N, Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L. and Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.   \nVon Oswald,J,Nilass,E,Randazz,ESaramentJ,Mrdvintse,AZginov, nd Vladymyrov, M. (2023). Transformers learn in-context by gradient descent. In International Conference on Machine Learning. PMLR.   \nWang, K, Variengien, A., Conmy, A., Shegeris, B. and Steinhardt, J. (2022). Intrpretability in the wild: a circuit for indirect object identification in gpt-2 small. arXiv preprint arXiv:2211.00593.   \nWei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M. and Le, Q. V. (2021). Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652.   \nWei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D. et al. (2022). Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35 24824-24837.   \nWu, J., Zou, D., Chen, Z., Braverman, V., Gu, Q. and Bartlett, P. L. (2023). How many pretraining tasks are needed for in-context learning of linear regression? arXiv preprint arXiv:2310.08391.   \nXie, S. M., Raghunathan, A., Liang, P. and Ma, T. (2021). An explanation of in-context learning as implicit bayesian inference. arXiv preprint arXiv:2111.02080.   \nZhang, R., Frei, S. and Bartlett, P. L. (2023a). Trained transformers learn linear models in-context. arXiv preprint arXiv:2306.09927.   \nZhang, Y., Liu, B., Cai, Q., Wang, L. and Wang, Z. (2022). An analysis of attention via the lens of exchangeability and latent variable models. arXiv preprint arXiv:2212.14852.   \nZhang, Y., Zhang, F., Yang, Z. and Wang, Z. (2023b). What and how does in-context learning learn? bayesian model averaging, parameterization, and generalization. arXiv preprint arXiv:2305.19420.   \nZhou, D., Scharli, N., Hou, L., Wei, J., Scales, N., Wang, X., Schuurmans, D., Cui, C., Bousquet, O., Le, Q. et al. (2022). Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "1Introduction ", "page_idx": 15}, {"type": "text", "text": "2Problem Setup: In-Context Learning of Markov Chains 3   \n2.1 In-Context Learning and $n$ Gram Markov Chains 3   \n2.2  A Two-Layer Transformer Model . 4   \n3 Theoretical Results 6   \n3.1 Generalized Induction Head Mechanism for Learning $n$ Gram Markov Chains 6   \n3.2 Convergence Guarantee of Gradient Flow 7 ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "4Conclusion and Future Work 10 ", "page_idx": 15}, {"type": "text", "text": "5  Acknowledgement 11 ", "page_idx": 15}, {"type": "text", "text": "ARelated Works 17 ", "page_idx": 15}, {"type": "text", "text": "B Experiments 18   \nB.1 Training with Stage Splitting 19   \nB.2  Training without Stage Splitting 20   \nB.3Prior and Length Generalization 21   \nC.1 How Does Transformer Implement the GIH Mechanism? 22   \nC.2 Feed-Forward Network for Polynomial Kernel 23   \nC.3 Perron-Frobenius Theorem 24   \nC.4 Sequential CE Loss\uff0e: 25   \nC.5 Standard $\\chi^{2}$ -Divergence and Mutual Information 26   \nC.6 More Details on the Generalized Induction Head Mechanism 26   \nC.7 Further Discussions on the GIH Mechanism 26   \nC.8Conclusion and Future Directions 27   \nD Proof Sketch 27   \nD.1 Simplification of the Transformer Model at Initialization 28   \nD.2 Analysis for Training the FFN and the First Attention Layer 28   \nD.2.1  Training the FFN: Identification of the Information Set $S^{\\star}$ 29   \nD.2.2 Training the First Attention Layer: Convergence of $\\sigma(w^{(h)})$ to One-Hot Vector 30   \nD.3 Analysis for the Training of the Second Attention Layer . . -. 31   \nE  Analysis of the Training Dyanamics 32   \nE.1  Conditions on the Sequence Length 32   \nE.2Analysis for Stage I . 33   \nE.2.1 Additional Proofs for the Stage I 37   \nE.3Analysis for Stage II 38   \nE.3.1 Additional Proofs for Stage II 45   \nE.4  Analysis for Stage III 45   \nE.4.1 Additional Proofs for Stage III 53   \nE.5  Lemma on GIH Approximation Error 55   \nAuxiliary Lemmas and Their Proofs 57   \nF.1 Useful Inequalities 57   \nF.2 Approximation Errors for Dynamics Analysis 58   \nF.3 Lemmas on Concentration of Markov Chain 73 ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "Organization of The Appendix ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The appendices are organized as follows: ", "page_idx": 16}, {"type": "text", "text": "\u00b7 In $\\S\\mathrm{A}$ , we present an in-depth discussion on the related works.   \n\u00b7 In $\\S B$ , we discuss the experimental details.   \n\u00b7 In $\\mathbf{\\xi}\\S$ we discuss the implementation of GIH mechanism, provide explicit expressions for the FFN realizing a low-degree polynomial kernel, and review basics related to concepts mentioned in the main text.   \n\u00b7 In $\\S D$ , we provide a high-level overview of the proof of our main results.   \n\u00b7 In $\\S\\mathrm{E}$ , we present the proof for Theorem 3.6.   \n\u00b7 $\\ln\\,\\S\\mathrm{F}$ , we collect auxiliary results used in the proof of Theorem 3.6. ", "page_idx": 16}, {"type": "text", "text": "A Related Works ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In Context Learning (ICL). Commercial Large Language Models (LLMs) such as ChatGPT (Brown et al., 2020), GPT-4 (Achiam et al., 2023), and Gemini (Team et al., 2023) typically operate in an autoregressive manner. These models exhibit remarkable ICL capabilities, without requiring further training. Previous research explores various aspects of the in-context learning (ICL) ability of these models. This includes their performance in zero-shot and few-shot learning scenarios (Honovich et al., 2022; Wei et al., 2021), the use of the chain of thought method to enhance reasoning (Wei et al., 2022; Zhou et al., 2022), and learning with multi-modalities (Alayrac et al., 2022). Moreover, recent research highlights the properties and advantages of using transformers beyond the traditional ICL setting, thereby broadening our understanding of their capabilities and applications (Edelman et al., 2022; Li et al., 2023; Jelassi et al., 2022; Sanford et al., 2023; Giannou et al., 2023; Liu et al., 2022; Tarzanagh et al., 2023a,b; Tian et al., 2023b,a; Song and Zhong, 2023; De0ra et al., 2023; Chen and Li, 2024; Rajaraman et al., 2024b). ", "page_idx": 16}, {"type": "text", "text": "There is a large and growing body of literature on understanding how transformer architecture enables ICL. One strand of research proposes to understand ICL by casting it as a version of Bayesian inference expressed by the transformer architecture. See, e.g., Xie et al. (2021); Muller et al. (2021); Zhang et al. (2022, 2023b); Ahuja et al. (2023); Je0n et al. (2024); He et al. (2024) and the references therein. Another line of work investigates how transformers internally emulate specific algorithms to solve ICL tasks, where Akyirek et al. (2023); Von Oswald et al. (2023); Fu et al. (2023); Ahn et al. (2023); Mahankali et al. (2023); Giannou et al. (2024); Wu et al. (2023) focus on learning with linear regression tasks and Bai et al. (2023); Cheng et al. (2023); Collins et al. (2024); Guo et al. (2023) investigate transformers\u2019 capabilities in learning with nonlinear functions. However, all of these works above focus on regression tasks where token (or token pairs) in the prompt sequences are i.i.d. or uncorrelated, which may not capture the more sophisticated data structures in real-world applications. ", "page_idx": 16}, {"type": "text", "text": "In addition, to study ICL with correlated data, there is also substantial interest in understanding how ICL operates over data drawn from Markov chains, providing insight into how transformer architectures contribute to ICL in these settings (Edelman et al., 2024; Makkuva et al., 2024b; Chen and Zou, 2024). Furthermore, Lin et al. (2023); Sinii et al. (2023) show how transformers can solve reinforcement learning problems in an in-context fashion. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "While many of the aforementioned works focus on the expressivity of the transformer model on different ICL tasks and the statistical properties of the learned models, understanding training dynamics from an optimization perspective is also crucial for comprehending ICL by transformers. The training dynamics for one-layer attention models have been investigated under different data models for both regression and classification tasks (Zhang et al., 2023a; Huang et al., 2023; Tarzanagh et al., 2023a,b; Kim and Suzuki, 2024; Chen et al., 2024; Vasudeva et al., 2024; Li et al., 2024; Thrampoulidis, 2024; Sheen et al., 2024). These studies offer a thorough characterization of the training process, yet they have limitations \u2014\u2014 they are not directly applicable to data drawn from Markov processes and are confined to single-layer attention. Our work belongs to this line of research and we adopt a two-attention-layer transformer architecture, which is more complicated than the transformer studied intheseworks. ", "page_idx": 17}, {"type": "text", "text": "Induction Head. Elhage et al. (2021) introduce the concept of \u201cinduction heads\"\u201d as the mechanism underlying the ICL capabilities of transformers. Since then, there has been a surge of interest in understanding the induction head mechanism and its role in ICL. At a high level, the induction head mechanism works by matching the history of the current token with those seen previously in the sequence and then predicting the next token based on the matched historical sub-sequences. Olsson et al. (2022) provide empirical evidence highlighting that induction heads are crucial in facilitating the ICL capabilities of transformers. Bietti et al. (2024); Edelman et al. (2024) conduct a further empirical investigation into the development of induction heads specifically tailored for the ICL of bi-gram data models. Rajaraman et al. (2024a) provide explicit constructions of single-head transformers with constant depths that can learn $n$ gram data. Also, a wider range of functionalities exhibited by induction heads that interact with various other mechanisms have been observed by Wang et al. (2022). ", "page_idx": 17}, {"type": "text", "text": "From a theoretical perspective, Nichani et al. (2024) study the ICL of first-order Markov chains using a two-layer transformer and demonstrate the formation of the induction head mechanism. Makkuva et al. (2024a) also prove that training a single layer attention with a feed-forward layer on first-order Markov data (with $\\{0,1\\}$ vocabulary) can converge to either to global or local minima depending on the initialization. However, the first-order assumption seems to be quite restrictive, especially when modeling the natural language, where the tokens can depend on multiple previous tokens. Most related to our work is Nichani et al. (2024), where they analyzed how training by gradient descent enables a two-layer transformer to learn the latent causal graph underlying the ICL data. However, the analysis in Nichani et al. (2024) applies to Markov chains where each token has at most one parent, and it remains unclear how to extend the analysis to more general $n$ gram Markov chains. ", "page_idx": 17}, {"type": "text", "text": "In this work, we show that a generalized version of the induction head mechanism can emerge when training a two-layer transformer on $n$ -gram Markov chains. Moreover, our transformer models are more sophisticated, incorporating features like relative positional embedding, multi-head attention, an FNN layer, and normalization. Notably, we provide an in-depth dynamics analysis of the corresponding FFN layer and two-layer multi-head attention. ", "page_idx": 17}, {"type": "text", "text": "B Experiments ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we first detail the setup for the experiment in Figure 3, and then provide additional results for training a model that also incorporates the word embedding matrices $W_{Q}$ $W_{K}$ $W_{V}$ and the output embedding matrix $W_{O}$ in the first attention layer. Let us first detail the data setup that is used for all the experiments in this work. ", "page_idx": 17}, {"type": "text", "text": "Data generation.The dataset for the ICL task is generated as $n$ -gram Markov chains as described in $\\S2.\\bar{1}$ . We take $\\mathtt{p a}=\\{-1,-2\\}$ as the parent set. Thus, the number of parents is $n=2$ and the token embedding dimension is $d=3$ . Note that for each sequence, the transition matrix $\\pi(x\\,|\\,x_{\\mathtt{p a}})$ is of shape $d\\times\\bar{d}^{n}$ . We assign a prior distribution $\\mathcal{P}$ for the transition matrix, which is defined such that each column of the transition matrix of kernel $\\pi$ is independently drawn from a symmetric Dirichlet distribution with parameter $\\alpha=0.01$ ,i.e., $\\pi(\\cdot|x_{\\mathrm{pa}})\\sim\\operatorname{\\bar{D}i r}(\\alpha\\cdot\\dot{\\mathbf{1}}_{d})$ Note that each chain has different transition kernel $\\pi$ but follows the same prior distribution $\\mathcal{P}$ . We randomly sample 10.000 Markov chains with $L=100$ from the prior distribution $\\mathcal{P}$ ; 9,000 are used for training and 1,000 for validation. ", "page_idx": 17}, {"type": "image", "img_path": "4fN2REs0Ma/tmp/6a03bf06452fe0f7fb352791dcd1ee4de32468e454bdf566e3aaf3384c972928.jpg", "img_caption": ["Figure 4: An illustration of the transformer parameters during the three-stage training. This is the same figure as Figure 3. We train a transformer in $\\mathsf{T F}(M=3,H=3,d=3,D=2)$ with $L=100$ \uff0c $\\bar{\\mathtt{p a}}=\\{-1,-2\\}$ . In (a) we show the evolution of $\\{p_{S}\\}_{S\\in[H]\\leq D}$ in the first stage of training where $\\begin{array}{r}{p_{\\ensuremath{\\mathcal{S}}}=c_{\\ensuremath{\\mathcal{S}}}^{2}/\\sum_{\\ensuremath{\\mathcal{S}}^{\\prime}\\in[H]_{\\leq D}}c_{\\ensuremath{\\mathcal{S}}^{\\prime}}^{2}}\\end{array}$ We use a binary coding in $\\{0,1\\}^{3}$ to indicate each subset $\\boldsymbol{S}$ . Recall that $\\bullet110^{\\circ}$ represents $=\\{1,2\\}$ , which is exactly $S^{\\star}$ . This figure shows that $p_{S^{\\star}}$ gradually increases to one while the any other $p_{S}$ decays to zero. In (b) we plot the RPE weights of the first attention layer before and after the second stage of training. Here the $h$ -th column corespondstothe RPE wightvectorof head $h$ This fureshows that $w_{-1}^{(1)}$ and $w_{-2}^{(2)}$ increase to a large number after training, while $w_{-3}^{(3)}$ stays close to its initial value. Thus, we have $\\sigma(w^{(1)})\\approx\\sigma(w^{(2)})\\approx1$ . That is, the first two heads are trained to attend to parents $-1$ and $-2$ , respectively. In (c) we plot the evolution of $a$ in the last stage of training. This figure clearly exhibits a two-step growth pattern and $a$ keeps increasing throughout this stage. In summary, the results of the simulation experiments coincide with the theoretical results. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "B.1  Training with Stage Splitting ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "we present the simulation results with model $\\mathrm{TF}(M,H,d,D)$ in (2.5) and training in the three-stage manner. We configure the model with window size $M=3$ , number of heads $H=3$ , vocabulary size $d=3$ and maximal FFN degree $D=2$ ", "page_idx": 18}, {"type": "text", "text": "Model initialization. The RPE weight matrix $W_{P}^{(h)}$ is initialized such that the $(-i)$ -th diagonal of $W_{P}^{(h)}$ has ale $w_{-i}^{(h)}$ $i=1,2,\\dots,M$ $-\\infty$ Se Figure for an interpretation. We initialize $w_{-h}^{(h)}=3$ and set the remaining entries within the ize- $M$ window to 0.01 to ensure symmetrization-breaking and some initial correspondence between heads and parents. For the FFN layer that learns the polynomial features, all $c_{S}$ for $\\mathcal{S}\\in[H]_{\\leq D}$ are initialized to 0.01. The initial value of $a$ in the second attention layer is set to 0.01. ", "page_idx": 18}, {"type": "text", "text": "Training settings. \u03b2 The models are trained using gradient descent with respect to the cross-entropy loss and a constant learning rate that is set to one for all stages. We train the model in Stage I (update parameters $\\{c_{S}\\}$ only) for 2000 epochs, in Stage II (update parameters $\\{w^{(h)}\\}$ only) for 50,000 epochs, and in Stage III (update parameter $a$ only) for 5000 epochs, respectively. All experiments are conducted using a single Nvidia A100 GPU. The results are shown in Figure 4, which matches our theoretical results. ", "page_idx": 18}, {"type": "image", "img_path": "4fN2REs0Ma/tmp/cb8908a4479e1a5e76d7b8f39eb9ef08b2b58ad8e5103ccb060cbc2537927314.jpg", "img_caption": ["Figure 5: An illustration of the evolution of gradient descent dynamics when training a transformer model specified in $\\S B.2$ with word embedding matrices $\\{W_{Q},W_{K},W_{V}\\}$ . Here the dynamics are not split into three stages and each gradient descent step updates all parameters. We set $M=H=\\bar{3}$ $d=3$ , and $D=2$ , the number of input token is $\\bar{L}=100$ , and Markov chain has parent set $\\mathtt{p a}=\\{-1,-2\\}$ . In (a) we show the training loss of the model, which shows that the loss decreases and converges to some value. In (b) we show the evolution of $p_{S}$ where we use binary coding $\\{0,1\\}^{3}$ to indicate each subset $\\boldsymbol{S}$ . Here, $p_{S^{\\star}}$ has code $\\mathrm{{}^{\\bullet}110^{\\circ}}$ which corresponds to the true parent set. This figure shows that initially a wrong $p_{S}$ dominates at the early stage of training, which corresponds to ${\\cal S}=\\{2,3\\}$ (code $\"011\"$ ). Then eventually $p_{S^{\\star}}$ increases and becomes dominant. However, $p_{S^{\\star}}$ does not increase to one and is about 0.6, and there are two $p_{S}$ 's that are about O.2. In (c) we show the RPE weights of the first attention layer before and afer raning he entres coresponding to the true parens, $w_{-1}^{(1)}$ and $w_{-2}^{(2)}$ "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "significantly increase ater training, while $w_{-3}^{(3)}$ slightly inereases from initiaiztion Thisfgure shows that each attention head focuses on copying a single previous token. In (d) we show the evolutionof theweight $a$ in the second attention layer. We observe a similar \u201celbow\" curve as in Figure 3-(c). ", "page_idx": 19}, {"type": "text", "text": "B.2  Training without Stage Splitting ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Previouslyin $\\S B.1$ , we show the simulation results on the simplified model (2.5). Now we present the results of additional experiments based on the full model defined as follows. ", "page_idx": 19}, {"type": "table", "img_path": "4fN2REs0Ma/tmp/8b0fdce2f1f92f94eef8298772bdfae2b3ed6fb91759c59a6473a4b885ab39d9.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "In head $h$ of th fis atention layer, $W_{P}^{(h)}$ isthlisiw $W_{Q}^{(h)}\\in\\mathbb{R}^{d\\times d}$ \uff0c $W_{K}^{(h)}\\in\\mathbb{R}^{d\\times d}$ and $W_{V}^{(h)}\\in\\mathbb{R}^{d\\times d}$ aq projections, respectively. That is, in the full model, we the attention heads has more weight matrices than the simplified model. Another difference is that we also explicitly include the residual link that copies $\\widetilde{X}$ to the output of the first attention layer. For the FFN layer, $\\phi:\\mathbb{R}^{(H+1)d}\\rightarrow\\mathbb{R}^{d_{e}}$ isthe same feed-forward network specified in (2.3). Here, we use a standard $\\ell_{2}$ -layer-normalization $\\mathrm{LN}(\\cdot)$ defined as ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname{LN}([x,y])=\\left[{\\frac{x}{\\|x\\|_{2}}},{\\frac{y}{\\|y\\|_{2}}}\\right].\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The second attention layer takes $X$ as the value, which comes from the residual link (i.e., concatenation of $\\tilde{U}$ and $V$ while $\\tilde{X}$ in $V$ remains the same after $\\ell_{2}$ -normalization). In comparison to the simplified model in (2.5), here we incorporate the query, key and value projections for the first layer as in a standard transformer architecture. ", "page_idx": 20}, {"type": "text", "text": "Our training setup is similar to that in $\\S B.1$ . We use the same dataset and a similar training settings. All these weight matrices W(h), W() Wk\" and W W() are initialized as identity matrices scaled by 0.001. We initalized the RPE vector w(h) as w) for $h=1,2,3$ , and leave the remaining entries within the length- $M$ window to 0.01. We trained the model with all parameters together for 10,000 epochs with the same loss function and learning rate. As illustrated in Figure 5, the full model converged to a state comparable to our simplified model. We further plot the $W_{Q}^{(\\bar{1})},W_{K}^{(1)},W_{V}^{(1)}$ W , W(1) for the first head after training in Figure 6. The results demonstrate that the model converges to a point where the query and key projections are close to zero, which leaves the RPE weights to dominate the attention mechanism. This fact justifies our simplification in (2.5) where we remove the query and key projection weights and set $\\bar{W}_{V}^{(h)}$ to be identity matrix. ", "page_idx": 20}, {"type": "image", "img_path": "4fN2REs0Ma/tmp/28abf922d4caacfc5f633e8a4d30955a4d0844269afb72b851e9349f838af2a1.jpg", "img_caption": ["Figure 6: A visualization of the word embedding matrices W1), W $W_{K}^{(1)}$ \uff0c $W_{V}^{(1)}$ of a pre-trained transformer with $M=H=3$ $d=3$ , and $D=2$ . These are the parameters in of the first attention head in the first attention layer. Since $d\\,=\\,3$ , all word embedding matrices are of shape 3 \u00d7 3. As shown in (a) and (b), W(1) a and W and $W_{K}^{(1)}$ do not hangemuchcompared to hir initialization value 0.o01. Thus, they are both close to the zero matrix and play a negligible role in the frst atention layer Besides, in () we plot $W_{V}^{(1)}$ which establishes clear diagonal structure, with the diagonal entries growing to 0.07 compared to the initialization value 0.001. Thus, $W_{V}^{(1)}$ is proportional to the identity matrix. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "B.3Prior and Length Generalization ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We further test the model learned by the three-stage training on sequences coming from different priors and of different lengths. Note that our pre-trained transformer learns to perform GIH. As introduced in $\\S3.1$ , the GIH estimator can be applied to a sequence with an arbitrary length and does not concern the prior distribution of the underlying Markov chain. Thus, it is natural to see if the pre-trained transformer can also generalize to different lengths and prior distributions. ", "page_idx": 20}, {"type": "text", "text": "Recall that we train the transformer model with sequence length $L=100$ and the concentration parameter of the Dirichlet prior is $\\alpha\\:=\\:0.01$ . Here, we test the pre-trained transformer on new sequences of different lengths and sampled from different prior distributions. That is, with a different concentration parameter $\\alpha$ , we sample a random Markov chain, and generate a sequence of length $L$ , and evaluate of cross-entropy loss for predicting $x_{L+1}$ . Here we choose $\\alpha\\in\\{0.\\dot{0}5,0.1,0.2\\}$ and range $L$ from 10 to 1000. When generating the data, the Markov chains share the same parent set $\\mathtt{p a}\\bar{=}\\{-1,-2\\}$ with the pre-training data. The results are shown in Figure 7. The results show a decreasing trend in testing loss as the sequence length increases. For $\\alpha=0.2$ , we observe first a small increase in the test loss when $L$ just exceeds 100, but then the loss decreases as $L$ increases further. This experiment shows that the pre-trained transformer indeed generalizes in length and is robust to the change of prior distribution. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "image", "img_path": "4fN2REs0Ma/tmp/2042d20dc5444dfdf3d975fd3752fad220d815ac235c6e9324b86d4a19b17b4c.jpg", "img_caption": ["Figure 7: Generalization capability of our model to different sequence lengths and prior distributions. We plot the cross-entropy loss of the pre-trained transformer model on sequences with different lengths sampled from Markov chains with different prior distributions. The prior is Dirichlet distribution with $\\alpha\\,\\in\\,\\{0.05,0.1,0.2\\}$ and we vary the length $L$ in $\\{10,2\\dot{0},50,100,200,400,700,1000\\}$ . The pre-training data contains sequences of length $L\\,=\\,100$ and $\\alpha\\,=\\,0.01$ . For different $\\alpha$ , we see that the error has a decreasing trend as $L$ increases. This shows that the pre-trained transformer can generalize in length and is robust to the distributional shift due to a change of prior. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "C  Additional Background and Discussions ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "C.1 How Does Transformer Implement the GIH Mechanism? ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In the following, we briefly illustrate how a two-attention-layer transformer model as introduced in (2.5) implements the GIH mechanism. As we will show in $\\S3.2$ , gradient flow with respect to the cross-entropy loss converges to this transformer in the limit. ", "page_idx": 21}, {"type": "image", "img_path": "4fN2REs0Ma/tmp/841d122dcb7d5405d8d1deea8d44f52deb503a8278e4de6aaccdf39bbb6822d7.jpg", "img_caption": ["Figure 8: Illustration of the GIH mechanism in a two-attention-layer transformer model. Here, $\\mathtt{p a}=\\{-1,-2\\}$ \uff0c $M=3$ and $S^{\\star}=\\{1,2\\}$ . The first attention layer copies the parents (including the information set $S^{\\star}$ ) to the current position. Then the FFN layer together with layer normalization generates the features $u_{l}$ using the parent tokens within the information set $S^{\\star}$ . The second attention layer treats each $x_{l}$ as the value, and aggregates $x_{l}$ as the prediction by matching the keys and query that come from the learned features using the attention mechanism. The $L+1$ -th token is padded with zeros in the input. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Step I: The First Attention Layer Copies the Information Set $S^{\\star}$ to the Current Position. Suppose the number of heads is equal to the window size for simplicity, i.e., $H=M$ . Then, attention head $h\\in S^{\\star}$ can attend to the $h$ -th parent token by setting the RPE weights in the softmax function to be $\\boldsymbol{w}^{(h)}=\\rho\\cdot\\boldsymbol{e}_{-h}$ for a sufficiently large $\\rho$ where $e_{-h}\\in\\mathbb{R}^{M}$ is the canonical basis vector with the $(M+1-h)$ -th entry being one and all other entries being zero. As a result, each $v_{l}^{(h)}$ for $h\\in S^{\\star}$ satisfies $v_{l}^{(h)}\\approx x_{l-h}$ ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "Step HI: FFN Generates the Polynomial Features of the Information Set $S^{\\star}$ .As we have introduced in (2.3), each learnable $c_{s}$ in the FFN layer determines the contribution of the corresponding subset $\\boldsymbol{S}$ to the output feature. To let the optimal information set $S^{\\star}$ dominate the output, we set $c_{S^{\\star}}=1$ whereas $c_{S}=0$ for all $S\\ne S^{\\star}$ . The exact form of the output of the FFN layer, $\\phi(v_{l})$ , is deferred to $\\S C.2$ . Here the only property we require is that ", "page_idx": 22}, {"type": "equation", "text": "$$\ns_{l}:=\\langle\\phi(v_{l}),\\phi(v_{L+1})\\rangle=\\prod_{h\\in\\cal S^{\\star}}\\langle v_{l}^{(h)},v_{L+1}^{(h)}\\rangle\\approx\\mathbb{1}(X_{l-\\cal S^{\\star}}=X_{L+1-\\cal S^{\\star}}),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Here $X_{l-S^{\\star}}:=(x_{l-s}:s\\in S^{\\star})$ and in the last equation we use the orthogonality and normalization of the vocabulary embeddings. ", "page_idx": 22}, {"type": "text", "text": "Step IHI: The Second Attention Layer Aggregates Tokens with Matching History on $S^{\\star}$ .We can interpret $s_{l}$ in (C.1) as an indicator for whether the information set of a token $x_{l}$ matches the information set of the token $x_{L+1}$ . Then for the second attention layer, by setting $a$ to be sufficiently large, the output will become ", "page_idx": 22}, {"type": "equation", "text": "$$\ny=\\sum_{l=M+1}^{L}\\frac{\\exp(a\\cdot s_{l})\\cdot x_{l}}{\\sum_{k=M+1}^{L}\\exp(a\\cdot s_{k})}\\approx\\left\\{\\!\\!\\begin{array}{l l}{N^{-1}\\cdot\\sum_{l=M+1}^{L}x_{l}\\cdot\\mathbb{I}(X_{l-S^{\\star}}=X_{L+1-S^{\\star}}),\\:\\mathrm{if}\\:N\\geq1,}\\\\ {(L-M)^{-1}\\cdot\\sum_{l=M+1}^{L}x_{l},\\:\\:\\:\\mathrm{otherwise},}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Where $\\begin{array}{r}{N\\,=\\,\\sum_{l=M+1}^{L}\\mathbb{1}(X_{l-S^{\\star}}\\,=\\,X_{L+1-S^{\\star}})}\\end{array}$ That isi a east ne tokn $x_{l}$ has a matching information set as $x_{L+1}$ , i.e.,their histories restricted to $S^{\\star}$ are the same, the second attention layer outputs the average of such tokens. Otherwise, it outputs the average of previous tokens from $x_{M+1}$ to $x_{L}$ . In Lemma E.6 in the appendix, we will show that the model learned by gradient flow implements the GIH mechanism up to a diminishing approximation error. ", "page_idx": 22}, {"type": "text", "text": "The weights of the transformer constructed above are illustrated in Figure 9. We consider the transformer model with $M=H=3$ $d=3$ , and ${\\cal D}=2$ . In this case, in the first attention layer, for each $h\\in$ $W_{P}^{(h)}$ ha hre fite parameers $w_{-1}^{(h)},w_{-2}^{(h)}$ and $w_{-3}^{(h)}$ By our constructin we have $w_{-h}^{(h)}=\\rho$ for all $h\\in$ [3] and the rest of the entries of $\\{w^{(h)}\\}_{h\\in[3]}$ are all equal to zero. In Figure 9-(a) we plot the top tenby ten block of $W_{P}^{(1)}$ where $w_{-1}^{(1)}\\,=\\,\\rho$ is shown in yllow and $w_{-2}^{(1)}\\,=\\,w_{-3}^{(1)}$ are shown in purple. The gray color stands for $-\\infty$ entries. In Figure 9-(b) we plot $\\{w^{(h)}\\}_{h\\in[3]}$ In Figure 9-(c) we plot the parameters of the FFN. Since $H=3$ and $D=2$ $[H]_{\\leq D}$ contains seven elements: $\\mathcal{Q}$ ${\\mathfrak{s}},\\{1\\},\\{2\\},\\{3\\},\\{1,2\\},\\{1,3\\}$ , and $\\{2,3\\}$ . We use binary strings of length 3 to index these seven subsets, where the $i$ -th bit indicates whether element $i$ is included in the subset. For instance, $\"110\"$ represents $\\{1,2\\}$ . We set $S^{\\star}=\\{1,2\\}$ \uff0c $c_{S^{\\star}}=1$ , and $\\;c_{S}=0$ for any other $\\boldsymbol{S}$ ", "page_idx": 22}, {"type": "text", "text": "C.2 Feed-Forward Network for Polynomial Kernel ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Lemma C.1. Recallthe FFN satisfying (2.3), which maps a vector $z\\in\\mathbb{R}^{d H}$ to a vector in $\\mathbb{R}^{d_{e}}$ . We write $z$ $(z^{(1)},\\ldots,z^{(H)})$ where $z^{(h)}\\in\\mathbb{R}^{d}$ for all $h\\in[H]$ . Let (h) be the i-th entry of (h) Then we can explicitly construct $\\phi(\\cdot)$ by letting ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\phi\\bigl((z^{(1)},\\ldots,z^{(H)})\\bigr)=\\biggl(c_{\\cal S}\\cdot\\prod_{h\\in{\\cal S}}z_{i_{h}}^{(h)}:\\{i_{h}\\}_{h\\in{\\cal S}}\\subseteq[d],{\\cal S}\\in[H]_{\\leq{\\cal D}}\\biggr),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which is equivalent to ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\phi\\big(\\big(z^{(1)},\\dots,z^{(H)}\\big)\\big)=\\Big(c_{S}\\cdot\\mathrm{vec}\\big(\\otimes_{h\\in\\mathcal{S}}\\big(z^{(h)}\\big)\\big)\\Big)_{S\\in[H]_{\\le D}}\\,,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\operatorname{vec}(\\cdot)$ is the vectorization operator that transforms a tensor into a vector by stacking all the entries inthe tensr That is, forany $\\boldsymbol{S}$ we consider the $|{\\cal S}|$ vectors in $\\mathbb{R}^{d}$ \uff0c $\\{z^{(h)}\\}_{h\\in S}$ . In (C.3) we ", "page_idx": 22}, {"type": "image", "img_path": "4fN2REs0Ma/tmp/e245fcf8f0fe353e455dacba047744b0e4929bc1f453787ad0f18514a0518bad.jpg", "img_caption": ["Figure 9: Limiting model of $\\mathtt{T F}(M\\,=\\,3,H\\,=\\,3,d\\,=\\,3,D\\,=\\,2)$ that implements the GIH mechanism with $L\\,=\\,100$ \uff0c $\\mathtt{p a}=\\{-1,-2\\}$ (a): The top left 10 by 10 block of $W_{P}^{(1)}$ that attends to the $-1$ parent. (b): The RPE weight heatmap for all 3 heads, where the $h$ -th column corresponds to the RPE weight vector of head $h$ (c): In the GIH mechanism, only one $c_{S}^{\\star}$ for the optimal information set $S^{\\star}$ dominates. For the label of the $x$ -axis, we use a binary coding $\\{0,1\\}^{3}$ to indicate each subset $\\boldsymbol{S}$ .Here, $S^{\\star}=\\{1,2\\}$ is the parent set, which is represented by $\\bullet\\epsilon110^{\\circ}$ "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "compute all possible products of the entries of thesevectors and multiply them by $c_{S}$ . In particular, for each $\\mathcal{S}\\in[H]_{\\leq D}$ ,we enumerate $i_{h}\\in[d]$ for all $h\\in S$ Therefore, the output dimension of $\\phi$ is given by ", "page_idx": 23}, {"type": "equation", "text": "$$\nd_{e}=\\sum_{S\\in[H]_{\\leq D}}d^{|S|}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. First, we note that the indices of $\\phi(\\cdot)$ have a grouped structure \u2014\u2014 we first enumerate all subsets in $[H]_{\\leq D}$ and then enumerate all monomials with superscripts in $\\boldsymbol{S}$ . Since there are $d^{\\vert S\\vert}$ monomials, the output dimension is given by (C.4). ", "page_idx": 23}, {"type": "text", "text": "It remains to verify (2.3) with $\\phi(\\cdot)$ defined in (C.3). To this end, we note that for any $u,v\\in\\mathbb{R}^{d H}$ and any $S\\in[H]_{\\leq D}$ ,wehave ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sum_{(i_{h})_{h\\in\\mathcal{S}}\\in[d]^{|\\mathcal{S}|}}\\bigg(\\prod_{h\\in\\mathcal{S}}u_{i_{h}}^{(h)}\\cdot v_{i_{h}}^{(h)}\\bigg)=\\prod_{h\\in\\mathcal{S}}\\bigg(\\sum_{i_{h}\\in[d]}u_{i_{h}}^{(h)}\\cdot v_{i_{h}}^{(h)}\\bigg)=\\prod_{h\\in\\mathcal{S}}\\langle u^{(h)},v^{(h)}\\rangle,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "which directly implies (2.3). Therefore, we conclude the proof of this lemma. ", "page_idx": 23}, {"type": "text", "text": "C.3 Perron-Frobenius Theorem ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Next, we review the basics for the celebrated Perron-Frobenius theorem on non-negative matrices (Meyer, 2023, Chapter 7). We consider the following class of irreducible matrices. ", "page_idx": 23}, {"type": "text", "text": "Definition C.2 (Irreducible Matrix). A non-negative square matrix $P\\in\\mathbb{R}_{+}^{d\\times d}$ is called irreducible if the induced directed graph $\\mathcal{G}$ is strongly connected, i.e., for any pair of nodes in the graph, there always exists a directed path that connects these two nodes. Here, the induced graph $\\mathcal{G}$ isdefined based on d nodes with adjacent matrix $A$ given by $A_{i j}=\\mathbb{1}(P_{i j}\\neq0)$ ", "page_idx": 23}, {"type": "text", "text": "In particular, if $P$ is a stochastic matrix that corresponds to a $d$ -state Markov chain, then starting from any state, we can reach any other state with positive probability in a finite number of steps. The irreducibility property also has an equivalent definition in the matrix form. That is, for any permutation matrix $T$ $\\bar{T}\\bar{P}T^{-1}$ cannot be written as an upper triangular block matrix with the following form ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left[\\begin{array}{l l}{M_{1}}&{M_{2}}\\\\ {0}&{M_{3}}\\end{array}\\right].\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "In other words, an irreducible matrix does not have a nontrivial absorbing subspace that aligns with the standard basis. ", "page_idx": 23}, {"type": "text", "text": "In this work, we require more than the irreducibility property from the transition matrix $P_{\\pi}$ defined in $\\S3.2$ In fact, we need the existence of a unique stationary distribution (which is not guaranteed by the irreducibility) so that the chain has a sufficiently fast mixing rate. This enables us to learn with a finite sequence length $L$ . To achieve this, one typically needs the second largest magnitude of the eigenvalues of $P_{\\pi}$ , denoted by $\\lambda$ , to be bounded away from 1, which is the leading eigenvalue of the transition matrix. The difference $1-\\lambda$ is also referred to as the spectral gap. It is well-known that if all the entries of $P_{\\pi}$ are positive, then $P_{\\pi}$ is irreducible and there is only one leading eigenvalue on the spectral circle with the corresponding eigenvector given by the chain's stationary distribution $\\mu^{\\pi}$ and all the other eigenvalues have magnitude strictly less than 1. However, for our case, the transition matrix $P_{\\pi}$ has zero entries by definition. Fortunately, the nice property on the existence of spectral gap can be generalized to a class called primitive matrix. ", "page_idx": 24}, {"type": "text", "text": "Definition C.3 (Primitive Matrix). A nonnegative and irreducible square matrix $P$ iscalledprimitive if thereexists aninteger $k$ suchthat all theentriesof $P^{k}$ arepositive. ", "page_idx": 24}, {"type": "text", "text": "By definition of the primitive matrix, one can immediately see that for any $k^{\\prime}>k$ $P_{\\pi}^{k^{\\prime}}$ is apositive matrix. The following is the celebrated Perron-Frobenius theorem that characterizes the spectral structure of the primitive matrices. ", "page_idx": 24}, {"type": "text", "text": "Theorem C.4 (Perron-Frobenius Theorem for Primitive Matrices). Let $P$ be a primitive matrix.Then thefollowingstatements hold: ", "page_idx": 24}, {"type": "text", "text": "1. The leading eigenvalue of $P$ is real and positive, and it is the unique eigenvalue with the largest magnitude. In particular, if $P$ is a stochastic matrix, then the leading eigenvalue is 1.   \n2. The leading eigenvector of $P$ is positive and unique up to a scaling factor. In particular, if $P$ is a stochastic matrix, then the leading eigenvector is the stationary distribution of the Markov chain with transition kernel $P$ ", "page_idx": 24}, {"type": "text", "text": "The Perron-Frobenius theorem guarantees the existence of a unique stationary distribution $\\mu^{\\pi}$ when the transition matrix $P_{\\pi}$ is primitive. In particular, when we further assume that the transition matrix $P_{\\pi}$ has a spectral gap, the chain is sufficiently mixed, meaning that we can thus approximate sum over the entire sequence with an average with respect to the stationary distribution. In particular, the approximation error will decays with the sequence length $L$ ", "page_idx": 24}, {"type": "text", "text": "C.4  Sequential CE Loss ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this work, we only consider the prediction error on the last token in the sequence as in (2.1): ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathcal{L}(f_{\\mathrm{tf}})=-\\mathbb{E}_{\\pi\\sim\\mathcal{P},x_{1:(L+1)}}\\left[\\log\\left(f_{\\mathrm{tf}}(x_{L+1}\\,|\\,x_{1:L})+\\epsilon\\right)\\right].\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "In practice however, people often train the transformer model by minimizing the cross-entropy (CE) loss over the entire sequence. We demonstrate that our analysis can be extended to training on the entire sequence. In this vein, we define the sequential CE loss as ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathcal L_{\\sf s e q}(f_{\\sf t f})=\\sum_{l=1}^{L}-\\mathbb E_{\\pi\\sim\\mathcal P,X}\\left[\\log\\left(f_{\\sf t f}(x_{l+1}\\mid x_{1:l})+\\epsilon\\right)\\right].\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "One can equivalently view this sequential CE loss as an aggregation of the CE loss for sequence length ranging from 1 to $L$ . We argue from the following two perspectives that our analysis can be extended to the sequential cross-entropy (CE) loss: ", "page_idx": 24}, {"type": "text", "text": "1. Due to the use of relative positional embedding (RPE), the transformer's predictions are invariant to the absolute positions of tokens within a sequence. Intuitively, this implies that even if we choose a different sequence length $L^{\\prime}$ , the model can still handle the task in the same manner. ", "page_idx": 24}, {"type": "text", "text": "2. By Assumption 3.5, the chain is sufficiently mixed for large $L$ . In the analysis, we actually use $X_{l-M:l}=(x_{l},x_{l-1},\\ldots,x_{l-M})\\sim\\mu^{\\pi}$ ,where $\\mu^{\\pi}$ is the stationary distribution over a length- $(M+1)$ window, to approximate the aggregation over $X_{l-M:l}$ for $l=M+1,\\dotsc,L$ in the sequence. For example, this approximation is refected in the transition from (D.3) to (D.4) in the proof sketch in $\\S D$ . Since changing the sequence length does not affect the underlying stationary distribution, the only issue is the approximation error. In particular, for sufficiently large $L$ , the CE loss at large $l$ constitutes the majority of the sequential CE loss in (C.5), making the CE loss at small $l$ negligible. ", "page_idx": 24}, {"type": "text", "text": "C.5  Standard $\\chi^{2}$ -Divergence and Mutual Information ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "The $\\chi^{2}$ -divergence (or $\\chi^{2}$ -distance) between two probability distributions $P$ and $Q$ in the same probability space is defined as: ", "page_idx": 25}, {"type": "equation", "text": "$$\nD_{\\chi^{2}}(P\\|Q)=\\sum_{x\\in\\mathrm{supp}(Q)}\\frac{(P(x)-Q(x))^{2}}{Q(x)},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the summation is taken over all elements $x$ in the sample space where $Q(x)>0$ .The $\\chi^{2}$ mutual information between two random variables $X$ and $Y$ with joint distribution $P_{X Y}$ and marginal distributions $P_{X}$ and $P_{Y}$ is defined as: ", "page_idx": 25}, {"type": "equation", "text": "$$\nI_{\\chi^{2}}(X;Y)=D_{\\chi^{2}}(P_{X Y}\\|P_{X}\\otimes P_{Y})=\\sum_{y}D_{\\chi^{2}}(P_{X\\,|\\,Y}(\\cdot\\,|\\,y)\\|P_{X}(\\cdot))P_{Y}(y).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $P_{X}\\otimes P_{Y}$ is the product of the marginals, meaning $(P_{X}\\otimes P_{Y})(x,y)=P_{X}(x)P_{Y}(y)$ For a Markov chain $X\\rightarrow Y\\rightarrow Z$ ,the $\\chi^{2}$ -mutual information satisfies the data processing inequality ", "page_idx": 25}, {"type": "equation", "text": "$$\nI_{\\chi^{2}}(X;Z)\\le I_{\\chi^{2}}(Y;Z),\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "which follows from the observation that $\\chi^{2}$ -divergence is also an $f$ -divergence. ", "page_idx": 25}, {"type": "text", "text": "C.6 More Details on the Generalized Induction Head Mechanism ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Recall that we define the Generalized Induction Head (GIH) estimator in (3.3). Specifically, $\\mathsf{G I H}(x_{1:L};M,D)$ is constructed in two steps. First, we find the information-optimal subset $S^{\\star}$ of $[M]$ by solving (3.2). Second, we build a $d$ class kernel classifier to predict $x_{L+1}$ , where the \u201cdata\" used by such aclassifer are $\\{\\psi_{S^{\\star}}(l),x_{l}\\}_{l\\in[M\\div1,L]}$ Here $\\{\\psi_{S^{\\star}}(l),l\\in[M+1,L+1]\\}$ are features constructed at each position based on the partial history given $S^{\\star}$ . In particular, similar to (C.3), for any subset $\\boldsymbol{S}$ of $[M]$ , any input token sequence $x_{1:L}$ , and any position $\\bar{l}\\in[M+1,L+1]$ , we define $\\psi_{S}(l)=\\psi_{S}(l;x_{1:L})$ as ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\psi_{S}(l)={\\mathrm{vec}}{\\Big(}_{s\\in S}\\,x_{l-s}{\\Big)}=\\left(\\prod_{s\\in S}(x_{l-s})_{i_{s}}:\\{i_{s}\\}_{s\\in S}\\subseteq[d]\\right)\\in\\mathbb{R}^{d^{|S|}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "In other word, $\\mathit{\\psi}_{\\mathit{{S}}}(l)$ is given by expanding the rank-1 tensor spanned by $\\{x_{l-s}\\}_{s\\in S}$ into a vector. Here $x_{l-s}\\in\\mathcal{X}$ is a vector in $\\mathbb{R}^{d}$ and we let $(x_{l-s})_{i_{s}}$ denote its $i_{s}$ -th entry. The rationale behind $\\mathit{\\psi}_{\\mathit{{\\psi}}_{S}}(l)$ is similar to $\\phi$ introduced in (C.3). We form a long vector containing all the products of the entries of vectors $\\{x_{l-s}\\}_{s\\in S}$ . Here we omit the dependency of $\\psi_{S}$ on the input sequence $x_{1:L}$ to simplify the notation. Furthermore, $\\psi_{S}$ induces a polynomial kernel such that for any $l,m\\in[M+1,L+1]$ ,we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\langle\\psi_{S}(l),\\psi_{S}(m)\\rangle=\\prod_{s\\in\\cal S}\\langle x_{l-s},x_{m-s}\\rangle=\\mathbb{1}\\{x_{l-s}=x_{m-s},\\forall s\\in{\\cal S}\\}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "That is, feature $\\psi_{S}$ selects the token position pairs $(l,m)$ such that the partial histories induced by $\\boldsymbol{S}$ at position $l$ and $m$ are exactly the same. ", "page_idx": 25}, {"type": "text", "text": "Based on $\\{\\psi_{S^{\\star}}(l),x_{l}\\}_{l\\in[M+1,L]}$ GIH forms akernel classifer using the indicator kernel. Specifically. for any $j\\in[d]$ ,by (3.3), $\\mathsf{G I H}(\\bar{x_{1:L}};M,D)$ outputs each $e_{j}\\in\\mathcal{X}$ with probability ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{P}\\big(\\mathtt{G I H}\\big(x_{1:L};M,D\\big)=e_{j}\\big)=\\frac{\\sum_{l=M+1}^{L}\\mathbb{1}\\big\\{x_{l-s}=x_{L+1-s},\\forall s\\in\\mathcal{S}^{\\star}\\big\\}\\cdot\\mathbb{1}\\big\\{x_{l}=e_{j}\\big\\}}{\\sum_{m=M+1}^{L}\\mathbb{1}\\big\\{x_{m-s}=x_{L+1-s},\\forall s\\in\\mathcal{S}^{\\star}\\big\\}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "C.7 Further Discussions on the GIH Mechanism ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We conclude this section with further discussions on the modified $\\chi^{2}$ -mutual information and lowdegree polynomial kernel for the FFN within the GIH mechanism. ", "page_idx": 25}, {"type": "text", "text": "On theModified $\\chi^{2}$ -Mutual Information.Now that we have shown how gradient flow approaches the desired GIH model, it is natural to ask the following questions: What is the optimal subset $S^{\\star}$ that the model selects? How well does the model perform? For the purpose of illustration, let us consider a symmetric case where the stationary distribution $\\mu^{\\pi}$ overalength- $r_{n}$ windowisuniform over $\\mathcal{X}^{r_{n}}$ . One can verify that in this case, the stationary distribution over a window of any other length is uniform as well, and the modified mutual information can be simplified into ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "equation", "text": "$$\n\\log\\widetilde{I}_{\\chi^{2}}(S)=\\log I_{\\chi^{2}}(S)-|S|\\log d,\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $I_{\\chi^{2}}(S)$ is the standard $\\chi^{2}$ mutual information between $\\mu^{\\pi}(z\\,|\\,Z_{-{\\cal S}})$ and $\\mu^{\\pi}(z)$ , and the second term $|S|\\log d$ serves as a penalty on the model complexity. Thus, the GIH mechanism is reaching a balance between the model complexity and the information richness. Below we characterize two scenarios where the model will select the exact parent set, i.e., $S^{\\star}={\\tt p a}$ ", "page_idx": 26}, {"type": "text", "text": "1. If $n=1$ , i.e., each token only has one parent, then $S^{\\star}={\\tt p a}$ . This is because $S^{\\star}$ simultaneously maximizes both terms in (C.6), thus reproducing the results in Nichani et al. (2024). 2. If $n$ is known a priori and restricting the polynomial kernel to ${\\cal S}\\in[H]_{=n}=\\{{\\cal S}\\in[H]:|{\\cal S}|=$ $n\\}$ for the FFN layer, then $S^{\\star}={\\tt p a}$ Here, the penalty term does not influence the selection and the exact parent set maximizes the mutual information by the data-processing inequality. ", "page_idx": 26}, {"type": "text", "text": "In the general case, however, the model could be much more fexible, and it is possible that the model selects only a subset of the true parent set or even some non-parent tokens that are also informative. The rationale is that with a more complex model, e.g., selecting a large $\\boldsymbol{S}$ , the model are able to make more accurate predictions for large $L$ but may endure a large estimation error for small $L$ ,as the exact matching $X_{l-s}=X_{L+1-s}$ may appear rarely in the sequence. ", "page_idx": 26}, {"type": "text", "text": "On the Low-Degree Polynomial Kernel. The goal of using a low-degree polynomial kernel in (2.3) is to strike a balance between model complexity (which is also related to computational cost) and the model's accuracy. In this regard, we have the following corollary. ", "page_idx": 26}, {"type": "text", "text": "Corollary C.5. We always have $\\left\\vert S^{\\star}\\right\\vert\\ \\ \\leq\\ \\ n$ regardless of the choice of $D$ where $\\begin{array}{r l}{S^{\\star}}&{{}=}\\end{array}$ $\\arg\\operatorname*{max}_{[H]\\leq^{D}}\\log\\widetilde{I}_{\\chi^{2}}(S)$ for $\\widetilde{I}_{\\chi^{2}}(S)$ in (C.6) ", "page_idx": 26}, {"type": "text", "text": "The reasoning behind this corollary is as follows. Consider any set $\\boldsymbol{S}$ with $|{\\cal S}|>n$ , we have $I_{\\chi^{2}}(S)\\leq I_{\\chi^{2}}\\overline{{(\\mathtt{p a})}}$ as the true parent set is the most informative. Moreover, since $|\\mathtt{p a}|=n<|S|,S$ suffers from a larger penalty. As a result, we have $\\log\\widetilde{I}_{\\chi^{2}}(S)<\\log\\widetilde{I}_{\\chi^{2}}({\\tt p a})$ when $\\boldsymbol{S}$ has more than $n$ elements. In other words, it is without loss of generality to set $D\\leq\\bar{n}$ ", "page_idx": 26}, {"type": "text", "text": "C.8  Conclusion and Future Directions ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In this paper, we have studied the training dynamics of a two-attention-layer transformer model for learning $n$ -gram Markov chains in an in-context way. Our theoretical analysis underscores a congruous interplay between the multihead attention mechanism, the feed-forward network, and layer normalization that yields a generalized version of the induction head mechanism during the training. In particular, we prove that the generalized induction head mechanism adopts a modified $\\chi^{2}$ -mutual information criterion for parent selection that strikes a balance between information richness and model complexity. To our best knowledge, our work gives the first theoretical evidence for learning an induction head mechanism with $n$ -gram Markov data, which potentially sheds light on the inner workings of large-scale transformer models. ", "page_idx": 26}, {"type": "text", "text": "Our work opens new directions for developing a rigorous understanding of the transformer models. A natural direction would be that if one can find such a mechanism with standard FFN layer using multi-layer perceptron and standard layer normalization in the more practical transformer model. The intuition is that our FFN layer in (2.3), which is further instantiated in (C.3), lies in the space of low-degree polynomials and can be well represented by a MLP with sufficient dimensions and proper activation functions. Initial attempts to learn nonlinear features have also been made by Kim and Suzuki (2024). Another direction is to investigate the training dynamics beyond a single loop of this induction head mechanism, e.g., iteration head with recursively refined predictions (Cabannes et al., 2024), and how the induction head mechanism occurs in multi-layer transformer models. ", "page_idx": 26}, {"type": "text", "text": "D Proof Sketch ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In this section, we discuss the main ingredients of analysis of gradient flow. First, we show in $\\S D.1$ how to simplify the model based on our choice of the initialization and the structure of the disentangled transformer. We then proceed to present the main proof ideas for the three stages of the gradient flow dynamics, where the training yields the following behaviors: ", "page_idx": 26}, {"type": "text", "text": "\u00b7Stage I: A unique $S^{\\star}\\in[H]_{\\leq D}$ stands out such that the associated parameter $c_{S^{\\star}}$ dominates those of the other sets. As a result, $p_{S}^{*}(t)=c_{S^{*}}^{2}(t)/C_{D}(t)$ approaches to one.   \n\u00b7 Stage II For each $h\\in S^{\\star}$ \uff0c $\\sigma(w^{(h)})$ approaches a one-hot vector $e_{M+1-h}\\in\\mathbb{R}^{M}$ , where $w^{(h)}$ contains the parameters of RPE of the $h$ -th head. During this stage, each head concentrates on copying a particular parent.   \n\u00b7Stage III: Finally, $a$ grows and reaches ${\\mathcal{O}}(\\log L)$ . As a result, the trained model approximately implements the GIH mechanism $\\mathsf{G I H}(x_{1:L};M,D)$ ", "page_idx": 27}, {"type": "text", "text": "D.1 Simplification of the Transformer Model at Initialization ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "We first simplify the expression of the transformer model at initialization under Assumption 3.3, by showing that the attention scores of the second attention layer admit a simpler form. ", "page_idx": 27}, {"type": "text", "text": "For the second attention layer, we write the output as $y^{\\top}=\\sigma(a s)X$ where $s:=u_{L+1}^{\\top}\\mathtt{M a s k}(U_{1:L}^{\\top})\\in$ $\\mathbb{R}^{1\\times L}$ is the row vectorof the similarity scores. Recallfrom (2.5) that the FFN layer with normalization outputs $U=\\phi(V)/\\sqrt{C_{D}}\\in\\mathbb{R}^{(L+1)\\times d_{e}}$ , and we denote the $l$ -th row of $U$ by $u_{l}=\\phi(v_{l})/\\sqrt{C_{D}}.$ For $l=M+1,\\dotsc,L$ , the $l$ -th entry of $s$ is given by ", "page_idx": 27}, {"type": "equation", "text": "$$\ns_{l}=\\langle u_{l},u_{L+1}\\rangle=\\langle\\phi(v_{l}),\\phi(v_{L+1})\\rangle/C_{D},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and the_other entries are all $-\\infty$ . By the property of the FFN layer in (2.3) and the definition $\\begin{array}{r}{C_{D}=\\sum_{\\cal S\\in[H]_{\\leq D}}c_{\\cal S}^{2}}\\end{array}$ we can rewritetheabove attention score as ", "page_idx": 27}, {"type": "equation", "text": "$$\ns_{l}=\\frac{\\sum_{\\boldsymbol{S}\\in[H]\\le D}c_{\\boldsymbol{S}}^{2}\\cdot\\prod_{h\\in\\boldsymbol{S}}\\langle v_{l}^{(h)},v_{L+1}^{(h)}\\rangle}{\\sum_{\\boldsymbol{S}\\in[H]\\le D}c_{\\boldsymbol{S}}^{2}},\\quad\\mathrm{for}\\,l=M+1,\\dots,L.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Note that under Assumption 3.3, by the definition of $\\Delta w$ in (3.6), we have a suficiently large gap $w_{-h}^{(h)}\\textrm{--}w_{-j}^{(h)}$ foral $j\\neq h$ at itaizatont Thus, $\\exp(w_{-h}^{(h)})\\,\\gg\\,\\exp(w_{-j}^{(h)})$ forall $j\\neq h$ Which implies the following approximation: ", "page_idx": 27}, {"type": "equation", "text": "$$\nv_{l}^{(h)}=\\sum_{k=1}^{M}\\frac{\\exp(w_{-k}^{(h)})}{\\sum_{j=1}^{M}\\exp(w_{-j}^{(h)})}\\cdot x_{l-k}\\approx x_{l-h},\\quad\\mathrm{for}\\,l=M+1,\\ldots,L.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "This further implies that for $l=M+1,\\dotsc,L$ ,we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\prod_{h\\in\\mathcal{S}}\\langle v_{l}^{(h)},v_{L+1}^{(h)}\\rangle\\approx\\prod_{h\\in\\mathcal{S}}\\langle x_{l-h},x_{L+1-h}\\rangle=\\mathbb{1}\\{x_{l-i}=x_{L+1-i}\\;\\mathrm{for}\\;i\\in\\mathcal{S}\\},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "which is a binary value indicating whether the query and the key token's history match on the subset $\\boldsymbol{S}$ . Combining (D.1) and (D.2), we obtain the following simplified expression for $s_{l}$ ", "page_idx": 27}, {"type": "equation", "text": "$$\ns_{l}\\approx\\frac{\\sum_{S\\in[H]\\le D}c_{S}^{2}\\cdot\\mathbb{I}\\{x_{l-i}=x_{L+1-i}\\mathrm{~for~}i\\in S\\}}{\\sum_{S\\in[H]\\le D}c_{S}^{2}}=\\sum_{S\\in[H]\\le D}p_{S}\\cdot\\mathbb{I}\\{x_{l-i}=x_{L+1-i}\\mathrm{~for~}i\\in S\\}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where we denote $\\begin{array}{r}{p_{S}=c_{S}^{2}/\\sum_{S\\in[H]_{\\leq D}}c_{S}^{2}}\\end{array}$ for $S\\in[H]_{\\leq D}$ ", "page_idx": 27}, {"type": "text", "text": "In summary, when $\\Delta w$ is suffciently large, $v_{l}^{(h)}$ approximately copies the token $x_{l-h}$ . As a result, the attention score $s_{l}$ satisfies ", "page_idx": 27}, {"type": "equation", "text": "$$\ns_{l}\\approx\\sum_{{\\cal S}\\in[H]_{\\le D}}p_{\\cal S}\\cdot\\mathbb{1}\\big\\{x_{l-i}=x_{L+1-i}\\;\\mathrm{for}\\;i\\in{\\cal S}\\big\\}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "D.2 Analysis for Training the FFN and the First Attention Layer ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "The first two training stages involve the dynamics of the weights of the FFN, $\\{c_{S}\\}_{S\\in[H]\\leq D}$ ,and the weights of the first attention layer, $\\{w^{(h)}\\}_{h=1}^{H}$ . The analyses of these two stages have similar structures and contain the following essential steps: ", "page_idx": 27}, {"type": "text", "text": "1. Derive the explicit expression of the dynamics of the weights, via direct calculations. ", "page_idx": 27}, {"type": "text", "text": "2. Unveil the key quantities (related to the modified $\\chi^{2}$ -MI) that dominantly drive the dynamics, by replacing the empirical average over the context sequence with the expectation over the stationary distribution, along with other approximations.   \n3. Then based on the above characterization of the dynamics, we can show the convergence of the weights to the desired values. ", "page_idx": 28}, {"type": "text", "text": "D.2.1 Training the FFN: Identification of the Information Set $S^{\\star}$ ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In the first stage, we track the dynamics of $c_{S}^{2}(t)$ for each $S\\in[H]_{\\leq D}$ For convenience, we drop the dependence on $t$ in the sequel. ", "page_idx": 28}, {"type": "text", "text": "Recall the output of the model is $y=(\\sigma(a\\cdot s)X)^{\\top}$ and the cross-entropy loss function is $\\mathcal{L}(\\Theta)=$ $\\mathbb{E}_{\\pi\\sim\\mathcal{P},x_{1:L}}[\\ell(\\bar{\\Theta})]$ ,where $\\ell(\\Theta)$ can be written as $\\ell(\\Theta)=-\\left\\langle x_{L+1},\\log(y+\\varepsilon{\\bf1})\\right\\rangle$ . We ignore the small constant $\\varepsilon$ in the following proof sketch for simplicity. We also abbreviate the vector of attention probabilities in the second attention layer as $\\sigma\\in{\\overset{\\cdot}{\\mathbb{R}}}^{L}$ ", "page_idx": 28}, {"type": "text", "text": "Calculation of the Dynamics of $c_{S}^{2}$ .By a direct calculation for the loss $\\ell$ and $s_{l}$ in (D.1), ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\frac{\\partial\\ell}{\\partial s_{l}}=-a\\cdot\\sigma_{l}(a\\cdot s)\\cdot\\left(\\frac{x_{L+1}}{y}\\right)^{\\top}(x_{l}-y)\\,,\\quad\\frac{\\partial s_{l}}{\\partial c_{S}}=\\frac{2c_{S}\\prod_{h\\in S}\\langle v_{l}^{(h)},v_{L+1}^{(h)}\\rangle}{C_{D}}-\\frac{2c_{S}s_{l}}{C_{D}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Here the vector $x_{L+1}/y$ is obtained by element-wise division and $\\sigma_{l}(a\\cdot s)$ is the $l$ -th entry of $\\sigma(a\\cdot s)$ Then applying the chain rule, we obtain the following dynamics for $c_{S}^{2}$ along the gradient flow: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\gamma_{t}\\log{c_{S}^{2}}=-\\displaystyle\\frac{2}{c_{S}}\\displaystyle\\sum_{l=M+1}^{L}\\mathbb{E}\\bigg[\\frac{\\partial\\ell}{\\partial s_{l}}\\frac{\\partial s_{l}}{\\partial c_{S}}\\bigg]=\\displaystyle\\frac{4a}{C_{D}}\\sum_{l=M+1}^{L}\\mathbb{E}\\bigg[\\sigma_{l}(a\\cdot s)\\cdot\\prod_{h\\in S}\\langle v_{l}^{(h)},v_{L+1}^{(h)}\\rangle\\cdot\\bigg(\\frac{x_{L+1}}{y}\\bigg)^{\\top}\\,(x_{l}-x_{l})\\bigg]}\\\\ {-\\underbrace{\\frac{4a}{C_{D}}}_{l=M+1}\\sum_{}^{L}\\mathbb{E}\\bigg[\\sigma_{l}(a\\cdot s)\\cdot s_{l}\\cdot\\bigg(\\frac{x_{L+1}}{y}\\bigg)^{\\top}\\,(x_{l}-y)\\bigg]\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Note that here the second term $f(t)$ is independent of $\\boldsymbol{S}$ , and it will be canceled out when we consider the difference of the derivatives, $\\partial_{t}\\log c_{S}^{2}-\\partial_{t}\\log c_{S^{\\prime}}^{2}$ , for two sets $S,S^{\\prime}\\in[H]_{\\leq D}$ This is why we focus on the time derivative of $\\log c_{S}^{2}$ \uff1a ", "page_idx": 28}, {"type": "text", "text": "Relate the Dynamics to the Modified $\\chi^{2}{\\bf-M I}$ by Approximations. Now using the approximation in (D.2) for $\\begin{array}{r}{\\prod_{h\\in\\cal S}\\langle v_{l}^{(h)},v_{L+1}^{(h)}\\rangle}\\end{array}$ ,expanding $(x_{L+1}/y)^{\\top}(x_{l}\\mathrm{~-~}y)$ cordinate-wise and otingthat $\\sigma_{l}(a\\cdot s)\\approx1/(L^{-}M)$ as we have small $a$ in the second attention layer, we arrive at ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\}_{t}\\log c_{S}^{2}\\approx\\frac{4a}{(L-M)C_{D}}\\sum_{l=M+1}^{L}\\mathbb{E}\\Bigg[\\mathbb{1}(X_{l-S}=X_{L+1-S})\\cdot\\Bigg(\\sum_{k=1}^{d}\\frac{\\mathbb{1}(x_{L+1}=x_{l}=e_{k})}{y(k)}-1\\Bigg)\\Bigg]-f(t)\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $y(k)$ denotes the $k$ -th entry of $y$ and $X_{l-S}:=(x_{l-i}:i\\in S)$ denotes the history of $x_{l}$ on the set $\\boldsymbol{S}$ simila for $X_{L+1-S}$ Note that $\\begin{array}{r}{y(k)\\approx(L-M)^{-1}\\sum_{l=M+1}^{L}\\mathbb{1}(x_{l}=e_{k})\\approx\\mu^{\\pi}(e_{k})}\\end{array}$ which follows from the mixing assumption of the Markov chain that allows us to replace the average over $l=M\\!+\\!1,\\dots,L$ by the expectation over the stationary distribution. Also for the same reason, we can replace $(x_{l},X_{l-\\mathcal{S}}),\\stackrel{\\cdot}{(x_{L+1},\\stackrel{\\cdot}{X}_{L+1-\\mathcal{S}})}$ with two independent copies from the stationary distribution $\\mu^{\\pi}$ , i.e., ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\partial_{t}\\log c_{S}^{2}\\approx\\frac{4a}{C_{D}}\\mathbb{E}_{(x,X),(z,Z)\\sim\\mu^{\\pi}\\times\\mu^{\\pi}}\\Bigg[\\mathbb{1}(Z_{-S}=X_{-S})\\cdot\\left(\\sum_{k=1}^{d}\\frac{\\mathbb{1}(x=z=e_{k})}{\\mu^{\\pi}(e_{k})}-1\\right)\\Bigg]-f(t).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "See the approximation from $^{g_{2,s}}$ to $^{g_{3,S}}$ in $\\S E.2$ . Indeed, the first term in (D.4) becomes the modified $\\chi^{2}$ -MII, $\\widetilde{I}_{\\chi^{2}}(S)$ , which is defined in Definition 3.1. This gives rise to the following approximation: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\partial_{t}\\log c_{S}^{2}\\approx\\frac{4a}{C_{D}}\\widetilde{I}_{\\chi^{2}}(S)-f(t).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Since the value of $f(t)$ is independent of the specific choice of set $\\boldsymbol{S}$ , it is clear that the set $\\boldsymbol{S}$ achieving the fastest growth rate is the information-optimal set $S^{*}=\\mathrm{argmax}_{S\\in[H]_{\\leq D}}\\,\\widetilde{I}_{\\chi^{2}}(S)$ that maximizes the modified $\\chi^{2}$ MI within $[H]_{\\leq D}$ ", "page_idx": 29}, {"type": "text", "text": "Convergence of $p_{\\cal S^{\\star}}$ . Note that $\\begin{array}{r}{p_{\\ensuremath{\\mathcal{S}}}=c_{\\ensuremath{\\mathcal{S}}}^{2}/\\sum_{\\ensuremath{\\mathcal{S}^{\\prime}}\\in[H]_{\\leq D}}c_{\\ensuremath{\\mathcal{S}^{\\prime}}}^{2}}\\end{array}$ quantifes the contributionof th set $\\boldsymbol{S}$ 10 the feature produced by the FFN layer. Thus, it is the relative growth rate of $c_{S}^{2}$ that matters. Towards this end, it follows from (D.5) that, for all $S\\in[H]_{\\leq D}\\backslash\\{S^{\\star}\\}$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\partial_{t}\\log\\frac{c_{S^{\\star}}^{2}}{c_{S}^{2}}\\approx\\frac{4a}{C_{D}}\\cdot\\left(\\widetilde{I}_{\\chi^{2}}(S^{\\star})-\\widetilde{I}_{\\chi^{2}}(S)\\right)\\geq\\frac{4a}{C_{D}}\\cdot\\Delta\\widetilde{I}_{\\chi^{2}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Here we recall from (3.5) that $\\Delta\\widetilde{I}_{\\chi^{2}}$ quantifies the minimal gap between the modified $\\chi^{2}{\\boldsymbol{-}}\\mathbf{M}\\mathbf{I}$ of $S^{\\star}$ and any other set in $[H]_{\\leq D}$ . The lower bound given by (D.6) ensures that for all $S\\ne S^{\\star}$ , the ratio $c_{S^{\\star}}^{2}/c_{S}^{2}$ grows expnentiallyfast, whicfthrmli $p_{\\cal S^{\\star}}$ approaches one exponentially fast. This concludes the first stage of the training dynamics. ", "page_idx": 29}, {"type": "text", "text": "D.2.2 Training the First Attention Layer: Convergence of $\\sigma(w^{(h)})$ to One-Hot Vector ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "As we proceed to the second stage after $p_{S^{\\star}}\\,\\approx\\,1$ , it suffices to show how $\\sigma(w^{(h)})$ converges to a one-hot vector $e_{M+1-h}$ for $h\\,\\in\\,S^{\\star}$ in order to show that the model converges to the GIH mechanism. Recall that we denote $X\\,=\\,(x_{1},\\ldots,x_{L})\\,\\in\\,\\mathbb{R}^{L\\times d}$ . For notational convenience, we denote $\\sigma^{(h)}\\;:=\\;\\sigma(w^{(h)})$ and let $X_{(l-M):(l-1)}\\ \\in\\ \\mathbb{R}^{M\\times d}$ denote the submatrix of $X$ with rows $l-M,\\ldots,l-1$ for any $l$ Following our convention, we let $\\sigma_{-i}^{(h)}$ denote the $(M+1-i)$ -th entry of g(h) and similarly for w2 . ", "page_idx": 29}, {"type": "text", "text": "Caleulation of the Dynamics of $w^{(h)}$ . The main idea for analyzing $\\{w^{(h)}\\}_{h=1}^{H}$ is the same as that in the previous stage: It suffices to analyze the difference between the growth rates of different coordinates of $w^{(h)}$ for $h\\in S^{\\star}$ . In particular, we care about how quickly $\\bar{w}_{-h}^{(h)}$ grows compared to other coordinates if $w_{-h}^{(h)}$ is initialized to be larger than the remaining coordinates: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\partial_{t}w_{-h}^{(h)}-\\partial_{t}w_{-i}^{(h)}=\\sum_{l=M+1}^{L}\\mathbb{E}\\bigg[\\frac{\\partial\\ell}{\\partial s_{l}}\\bigg(\\frac{\\partial s_{l}}{\\partial w_{-h}^{(h)}}-\\frac{\\partial s_{l}}{\\partial w_{-i}^{(h)}}\\bigg)\\bigg]}}\\\\ &{}&{=a\\sum_{l=M+1}^{L}\\mathbb{E}\\bigg[\\sigma_{l}(a s)\\left(\\sum_{k=1}^{d}\\frac{\\mathbb{1}\\big(x_{L+1}=x_{l}=e_{k}\\big)}{y(k)}-1\\right)\\bigg(\\frac{\\partial s_{l}}{\\partial w_{-h}^{(h)}}-\\frac{\\partial s_{l}}{\\partial w_{-i}^{(h)}}\\bigg)\\bigg].}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "$p_{S^{\\star}}\\,\\approx\\,1$ $s_{l}~\\approx$ $\\textstyle\\prod_{h\\in\\cal{S}^{\\star}}\\langle v_{l}^{(h)},v_{L+1}^{(h)}\\rangle$ $h\\in S^{\\star}$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\frac{\\partial s_{l}}{\\partial w_{-i}^{(h)}}\\approx\\frac{\\partial}{\\partial w_{-i}^{(h)}}\\prod_{h^{\\prime}\\in\\mathcal{S}^{\\star}}\\langle v_{l}^{(h^{\\prime})},v_{L+1}^{(h^{\\prime})}\\rangle=\\Bigg(\\prod_{h^{\\prime}\\in\\mathcal{S}^{\\star}\\setminus\\{h\\}}\\langle v_{l}^{(h^{\\prime})},v_{L+1}^{(h^{\\prime})}\\rangle\\Bigg)\\cdot b_{l}^{\\top}\\big(e_{M+1-i}-(\\sigma^{(h)})^{\\top}\\big)\\sigma_{-i}^{(h)}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where the equlit folows from the faethat $w_{-i}^{(h)}$ onlyafets $(v_{l}^{(h)},v_{L+1}^{(h)})$ and diferentiating through the softmax funetion. Here we defne $b_{l}:=X_{(l-M):(l-1)}v_{L+1}^{(h)}+X_{(L+1-M):L}v_{l}^{(h)}$ to simplify the notation. Combining (D.7) and (D.8), we obtain ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\partial_{t}w_{-h}^{(h)}-\\partial_{t}w_{-i}^{(h)}\\approx a g_{h}^{\\top}\\bigg(\\sigma_{-i}^{(h)}\\left(e_{M+1-h}-e_{M+1-i}\\right)+\\left(\\sigma_{-h}^{(h)}-\\sigma_{-i}^{(h)}\\right)\\sum_{j\\neq h}\\sigma_{-j}^{(h)}\\big(e_{M+1-h}-e_{M+1-j}\\big)\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where we introduce the following notation ", "page_idx": 29}, {"type": "equation", "text": "$$\ng_{h}:=\\sum_{l=M+1}^{L}\\mathbb{E}\\bigg[\\sigma_{l}(a\\cdot s)\\cdot\\bigg(\\sum_{k=1}^{d}\\frac{\\mathbb{1}(x_{L+1}=x_{l}=e_{k})}{y(k)}-1\\bigg)\\cdot\\prod_{h^{\\prime}\\in{\\mathcal{S}}\\backslash\\{h\\}}\\langle v_{l}^{(h^{\\prime})},v_{L+1}^{(h^{\\prime})}\\rangle b_{l}\\bigg].\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Adealdiafaef $\\sigma_{-h}^{(h)}-\\sigma_{-i}^{(h)}$ is positiv initaization Now suppose oh - g $t$ Then, lower bounding $\\partial_{t}w_{-h}^{(h)}-\\partial_{t}w_{-i}^{(h)}$ boils down to lower bounding $g_{h}^{\\top}\\left(e_{M+1-h}-e_{M+1-i}\\right)$ for $i\\neq h$ . Furthermore, if we can show that $\\partial_{t}w_{-h}^{(h)}-\\partial_{t}w_{-i}^{(h)}$ is lower ouddbysom positiveale he ap $\\sigma_{-h}^{(h)}-\\sigma_{-i}^{(h)}$ Since $\\textstyle\\sum_{i=1}^{M}\\sigma_{-i}^{(h)}\\equiv1$ this will crate a reinforcing loop that makes $\\sigma_{-h}^{(h)}$ monotonically increase. ", "page_idx": 30}, {"type": "text", "text": "Relate the Dynamics to the Modified $\\chi^{2}{\\bf-}{\\bf M}{\\bf I}$ by Approximations. We demonstrate next that $g_{h}^{\\top}\\left(e_{M+1-h}-e_{M+1-i}\\right)$ for $i\\neq h$ admits a lower bound depending on the information gap $\\Delta\\widetilde{I}_{\\chi^{2}}$ Specifically, using the same strategy for (D.3), we have by definition that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle r_{h}^{\\top}e_{M+1-i}}\\\\ {\\displaystyle\\approx\\frac{1}{L-M}\\sum_{l=M+1}^{L}{\\mathbb{E}}\\left[\\left(\\displaystyle\\sum_{k=1}^{d}\\frac{\\mathbb{I}(x_{L+1}=x_{l}=e_{k})}{y(k)}-1\\right)\\cdot\\mathbb{I}\\big(x_{l-j}=x_{L+1-j},j\\in\\mathcal{S}^{\\star}\\setminus\\{h\\}\\big)\\cdot b_{l}^{\\top}e_{M}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where for $b_{l}$ we have by the same approximation $v_{l}^{(h)}\\approx x_{l-h}$ and $v_{L+1}^{(h)}\\approx x_{L+1-h}$ as in (D.2) that ", "page_idx": 30}, {"type": "equation", "text": "$$\nb_{l}^{\\top}e_{M+1-i}={v_{L+1}^{(h)}}^{\\top}x_{l-i}+{v_{l}^{(h)}}^{\\top}x_{L+1-i}\\approx\\mathbb{I}(x_{L+1-h}=x_{l-i})+\\mathbb{I}(x_{l-h}=x_{l-i}).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Now we consider the case $i=h$ and $i\\neq h$ separately: ", "page_idx": 30}, {"type": "text", "text": "(i $(i=h)$ For $g_{h}^{\\top}e_{M+1-h}$ , we simply set $i=h$ in (D.11), and the indicator $\\mathbb{I}(x_{L+1-h}=x_{l-h})$ will exactly compensate for the exclusion of $h$ in the indicator function of (D.10). Drawing an analogy to how we go from (D.3) to (D.5), we obtain ", "page_idx": 30}, {"type": "equation", "text": "$$\ng_{h}^{\\top}e_{M+1-h}\\approx2\\widetilde{I}_{\\chi^{2}}(S^{\\star}).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "(i) $(i\\neq h)$ For $g_{h}^{\\top}e_{M+1-i}$ With $i\\neq h$ in (D.11), we apply the same reasoning as in the previous case. Additionally, by using the Cauchy-Schwarz inequality, the following inequality holds up to a small error (see Lemma F.7 for a detailed derivation): ", "page_idx": 30}, {"type": "equation", "text": "$$\ng_{h}^{\\top}e_{M+1-i}\\leq\\widetilde{I}_{\\chi^{2}}(\\boldsymbol{S}^{\\star})+\\widetilde{I}_{\\chi^{2}}(\\boldsymbol{S}^{\\star}\\backslash\\{h\\}\\cup\\{i\\})\\leq2\\widetilde{I}_{\\chi^{2}}(\\boldsymbol{S}^{\\star})-\\Delta\\widetilde{I}_{\\chi^{2}}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Plugging this back into the dynamics in (D.9), we conclude that for all $i\\neq h$ ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\partial_{t}w_{-h}^{(h)}-\\partial_{t}w_{-i}^{(h)}\\geq a\\cdot\\sigma_{-i}^{(h)}\\cdot\\Delta\\tilde{I}_{\\chi^{2}}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Convergence of $\\sigma(w^{(h)})$ .Combining the arguments in the previous two steps, we can now say that $\\sigma_{-h}^{(h)}$ will monotonicaly increase. It remains to show that $\\sigma_{-h}^{(h)}$ converges to one. Note that $\\log(\\sigma_{-h}^{(h)}/\\sigma_{-i}^{(h)})=w_{-h}^{(h)}-w_{-i}^{(h)}$ ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\partial_{t}\\log\\big(\\sigma_{-h}^{(h)}/\\sigma_{-i}^{(h)}\\big)=\\partial_{t}w_{-h}^{(h)}-\\partial_{t}w_{-i}^{(h)}\\geq a\\cdot\\sigma_{-i}^{(h)}\\cdot\\Delta\\tilde{I}_{\\chi^{2}}=a\\cdot\\Delta\\tilde{I}_{\\chi^{2}}\\cdot\\sigma_{-h}^{(h)}(0)\\cdot\\big(\\sigma_{-i}^{(h)}/\\sigma_{-h}^{(h)}\\big)\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $\\sigma_{-h}^{(h)}(0)$ is the initial value of $\\sigma_{-h}^{(h)}$ at time $t=0$ One can now rearrange the term and pick the ratio o2 /0 $\\sigma_{-i}^{(h)}/\\sigma_{-h}^{(h)}$ as the variable to track in the dynamics. A refined analysis in the convergence analysis in $\\S E.3$ shows that $\\sigma^{(h)}$ converges to a one-hot vector with $\\sigma_{-h}^{(h)}$ going to one. In particular, the convergence rate is determined by the information gap $\\Delta\\widetilde{I}_{\\chi^{2}}$ according to the above formula. ", "page_idx": 30}, {"type": "text", "text": "D.3Analysis for the Training of the Second Attention Layer ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "In the last stage, we turn to the training of $a$ given that all $\\sigma^{(h)}$ 'sfor $h\\,\\in\\,S^{\\star}$ are approximately one-hot vectors. The following approximation of the dynamics of $a(t)$ is performed in the region $a\\le O(\\log L)$ , where the signal term in the dynamics dominates the approximation error. ", "page_idx": 30}, {"type": "text", "text": "Calculation of the Dynamics of $a$ . After Stages I and $\\mathrm{II}$ , the output is approximated as $y(k)\\approx$ $\\begin{array}{r}{y^{\\star}(k):=\\sum_{l=1}^{L}\\sigma_{l}^{\\star}\\,\\mathbb{1}(x_{l}=e_{k})}\\end{array}$ for each $k\\in[d]$ . Here the weighting coefficients $\\sigma_{1}^{\\star},\\dots,\\sigma_{L}^{\\star}$ satisfy ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\sigma_{l}^{\\star}\\propto\\exp\\left(a\\cdot\\mathbb{1}(X_{l-S^{\\star}}=X_{L+1-S^{\\star}})\\right).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Note that for each $l\\;\\in\\;[L],\\,\\sigma_{l}^{\\star}$ indicates the importance assigned to the $l$ -th token based on the corresponding history of $x_{l}$ over the information set $S^{\\star}$ . In the population counterpart, when the chain has sufficiently mixed, for given $X_{L+1-S^{\\star}}$ , we can roughly view each $(x_{l},X_{l-S^{\\star}})$ as being sampled from a reweighed version of the stationary distribution: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\widetilde\\mu^{\\pi}(x_{l},X_{l-S^{\\star}}\\,|\\,X_{L+1-S^{\\star}})\\propto\\mu^{\\pi}(x_{l},X_{l-S^{\\star}})\\cdot\\exp\\left(a\\cdot\\mathbb{1}(X_{l-S^{\\star}}=X_{L+1-S^{\\star}})\\right).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Following the same argument as those in the previous stages, replacing the sum over $l$ With the expectation over the stationary distribution, we arrive at ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\partial_{t}a\\approx\\mathbb{E}_{\\pi\\sim\\mathcal{P},(x,X_{-S^{\\star}},z,Z_{-S^{\\star}})\\sim q^{\\pi}}\\bigg[\\mathbb{1}(X_{-S^{\\star}}=Z_{-S^{\\star}})\\cdot\\bigg(\\sum_{k=1}^{d}\\frac{\\mathbb{1}(x=z=e_{k})}{\\widetilde{\\mu}^{\\pi}(z=e_{k}\\mid X_{-S^{\\star}})}-1\\bigg)\\bigg].\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "See detailed derivations of the above approximation in $\\S E.4$ . Comparing the above expression with (D.4) in Stage I, one can see that here $\\bar{(x,X_{-}s^{\\star})}$ and $(z,Z_{-S^{\\star}})$ are no longer independent because now the model has learned to perform a information-theoretic feature selection, i.e., focusing on tokens sharing the same set of features based on the information set $S^{\\star}$ , which is defined according to the modified $\\chi^{2}$ -mutual information. In fact, the underlying joint distribution $q^{\\pi}$ is given by $q^{\\pi}=\\mu^{\\pi}(x,X_{-{\\cal S}^{\\star}})\\cdot\\widetilde{\\mu}^{\\pi}(z,Z_{-{\\cal S}^{\\star}}\\,|\\,X_{-{\\cal S}^{\\star}})$ ", "page_idx": 31}, {"type": "text", "text": "Divergence of $a$ .As the dynamics of $a$ has no closed-form expression due to the nonlinearity in the reweighed distribution $\\widetilde{\\mu}^{\\pi}$ , we resort to providing characterization for cases where $a$ is either sufficiently small or large. In both cases, the lower and upper bounds of (D.12) can be derived, respectively. Using these bounds, we can argue rigorously that for small $a$ , it undergoes superexponential growth until it reaches a critical \u201celbow\" value. After that, when $a$ becomes even larger, it grows logarithmically until it reaches $\\Omega(\\log L)$ ", "page_idx": 31}, {"type": "text", "text": "E  Analysis of the Training Dyanamics ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Masking the Simplified Model. Recall that we apply a mask to the first $M$ position in the simplified model. Therefore, we only allow index $l$ to run from $M+1$ to $L$ in the following analysis. In the following, we first specify the conditions on $L$ that are required for the analysis of the training dynamics and then present the proof of Theorem 3.6. ", "page_idx": 31}, {"type": "text", "text": "E.1  Conditions on the Sequence Length ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "We first introduce the following condition on $L$ ", "page_idx": 31}, {"type": "equation", "text": "$$\nL\\geq\\Omega\\bigg(\\frac{1}{\\Delta\\tilde{I}_{\\chi^{2}}^{2}(1-\\lambda)\\gamma^{r_{n}+2}}\\bigg),\\quad L\\geq(1-\\lambda)^{-1}\\gamma^{-D},\\quad\\sqrt{L}\\geq M\\vee d,\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Where $\\Omega$ only hides a universal constant that does not depend on the model parameters. The conditions in (E.1) will facilitate our analysis for Stage I and Stage II. For the last stage, we require ", "page_idx": 31}, {"type": "equation", "text": "$$\nL\\geq2M+r_{n}\\frac{\\log\\gamma^{-1}}{\\lambda^{-1}},\\quad\\frac{L}{(\\log L)^{4}}\\geq\\Omega\\Bigg(\\frac{1}{\\kappa^{4}\\gamma^{8+2|S^{*}|}}\\cdot\\left(\\frac{\\sqrt{M}+d}{(1-\\lambda)^{1/2}\\gamma^{|S^{*}|+2+r_{n}/4}}\\right)^{4}\\Bigg),\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\kappa:=\\mathbb{E}\\left[D_{\\chi^{2}}(\\mu^{\\pi}(\\cdot)\\,\\|\\,\\mu^{\\pi}(\\cdot\\,|\\,X_{-}s\\star))\\right]\\wedge\\mathbb{E}\\left[D_{\\chi^{2}}(\\mu^{\\pi}(\\cdot\\,|\\,X_{-}s\\star)\\,\\|\\,\\mu^{\\pi}(\\cdot))\\right]\\wedge1,}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "and $\\Omega$ only hides universal constants that do not depend on the model parameters. Here, $\\mu^{\\pi}(x,X_{-{\\cal S}^{\\star}})$ denotes the stationary distribution of the Markov chain over token $x$ and its parents $X_{-S^{\\star}}$ ,with $S^{\\star}$ being the information set defined in (3.2). ", "page_idx": 31}, {"type": "text", "text": "E.2  Analysis for Stage I ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "In this section, we analyze the dynamics of the parameters $\\{c_{S}^{2}\\}s_{\\in[H]_{\\leq D}}$ in the frst stage of training. We will show that there is a unique $S_{*}\\in[H]_{\\leq D}$ such that $c_{S^{\\star}}^{2}$ dominates all the other $c_{S}^{2}$ 's at the end of the first stage. In addition, we will characterize how fast this happens and provide a corresponding convergence rate. ", "page_idx": 32}, {"type": "text", "text": "Proof Strategy. At a high level, the strategy is to analyze $\\partial_{t}\\log c_{S^{\\star}}^{2}-\\partial_{t}\\log c_{S}^{2}$ for all $S\\ne S^{\\star}$ via the following steps: ", "page_idx": 32}, {"type": "text", "text": "1. Dynamics Calculation. First, we calculate the dynamics of $\\log c_{S}^{2}$ for each fixed $\\boldsymbol{S}$ .By selecting sufficiently small values for $a$ and $\\varepsilon$ , and leveraging the mixing properties of the Markov chain with large $L$ , the dynamics of $\\log c_{S}^{2}$ is approximately governed by the modified mutual information $\\widetilde{I}_{\\chi^{2}}(S)$   \n2. Lower Bound for The Growth Rate. Consequently, we are able to lower bound the difference between the growth rates, $\\partial_{t}\\log c_{S^{\\star}}^{2}-\\partial_{t}\\log c_{S}^{2}$ in terms of $\\Delta\\widetilde{I}_{\\chi^{2}}$ , the gap between the modified mutual information of $S^{\\star}$ and the second-best set.   \n3. Convergence. Finally, we derive the convergence using the above lower bound. ", "page_idx": 32}, {"type": "text", "text": "Before presenting the proof, we first remind the readers of a few definitions and notations. Recall that our simplified model is given by ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\boldsymbol{y}=(\\sigma(\\boldsymbol{a}s)\\boldsymbol{X})^{\\top}=\\sum_{l=M+1}^{L}\\sigma_{l}(\\boldsymbol{a}s)\\cdot\\boldsymbol{x}_{l},\\quad\\mathrm{where}\\quad\\boldsymbol{s}_{l}=\\frac{\\sum_{\\boldsymbol{S}\\in[H]\\le D}c_{\\boldsymbol{S}}^{2}\\cdot\\prod_{h\\in\\boldsymbol{S}}\\langle\\boldsymbol{v}_{l}^{(h)},\\boldsymbol{v}_{L+1}^{(h)}\\rangle}{\\sum_{\\boldsymbol{S}\\in[H]\\le D}c_{\\boldsymbol{S}}^{2}}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Also recallthat $\\begin{array}{r}{C_{D}(t)=\\sum_{S\\in[H]_{\\leq D}}c_{S}^{2}(t)}\\end{array}$ and $p_{S}(t)=c_{S}^{2}(t)/C_{D}(t)$ for each $\\mathcal{S}\\in[H]_{\\leq D}$ The loss function can be rewritten as ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}=\\mathbb{E}[\\ell],\\quad\\mathrm{where}\\quad\\ell=-\\langle x_{L+1},\\log(y+\\varepsilon{\\bf1})\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Here the expectation $\\mathbb{E}$ is taken over both the sequence $(x_{1},\\ldots,x_{L+1})$ and the Markov kernel $\\pi\\sim\\mathcal{P}$ . We abbreviate $\\sigma\\equiv\\sigma(a s)$ for convenience and denote by $\\sigma_{l}$ the $l$ -th element of $\\sigma$ .By direct calculation, we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\frac{\\partial\\ell}{\\partial y}=-\\frac{x_{L+1}}{y+\\varepsilon\\mathbf{1}},\\quad\\frac{\\partial y}{\\partial\\sigma}=X^{\\top},\\quad\\frac{\\partial\\sigma}{\\partial s_{l}}=a\\cdot\\sigma_{l}(a s)\\cdot(e_{l}^{\\top}-\\sigma),\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Then applying the chain rule, we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\frac{\\partial\\ell}{\\partial s_{l}}=\\frac{\\partial\\ell}{\\partial y}\\frac{\\partial y}{\\partial\\sigma}\\frac{\\partial\\sigma}{\\partial s_{l}}=-a\\left(\\frac{x_{L+1}}{y+\\varepsilon\\mathbf{1}}\\right)^{\\top}(x_{l}-y)\\cdot\\sigma_{l}(a s).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "In addition, ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\frac{\\partial s_{l}}{\\partial c_{\\ensuremath{\\boldsymbol}S}}=\\frac{2c_{S}\\prod_{h\\in\\ensuremath{\\boldsymbol}S}\\langle\\boldsymbol v_{l}^{(h)},\\boldsymbol v_{L+1}^{(h)}\\rangle}{\\sum_{S^{\\prime}\\in[H]_{\\ensuremath{\\boldsymbol}S^{\\prime}}}c_{S^{\\prime}}^{2}}-\\frac{2c_{S}s_{l}}{\\sum_{S^{\\prime}\\in[H]_{\\ensuremath{\\boldsymbol}S D}}c_{S^{\\prime}}^{2}}=\\frac{2c_{S}}{C_{D}}\\Bigg(\\prod_{h\\in\\ensuremath{\\boldsymbol}S}\\langle\\boldsymbol v_{l}^{(h)},\\boldsymbol v_{L+1}^{(h)}\\rangle-s_{l}\\Bigg).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Now, we are ready to present the proof of Theorem 3.6 for the first stage of training. We remind readers that here only $\\left\\{c_{S}\\right\\}s_{\\left[H\\right]\\leq D}$ are trained, and we omit the dependence on $t$ for convenience. ", "page_idx": 32}, {"type": "text", "text": "Proof of Theorem 3.6: Stage $I.$ As discussed in the proof strategy above, we first derive the dynamics f $\\log{c_{S}^{2}}$ for each fixed $\\bar{S}\\in[H]_{\\leq D}$ Then we compare the growth rate of $c_{S^{\\star}}^{2}$ with any other $c_{S}^{2}$ ", "page_idx": 32}, {"type": "text", "text": "Calculation of The Dynamics of $\\log c_{S}^{2}$ .We fix a $\\mathcal{S}\\in[H]_{\\leq D}$ and apply the chain rule $\\partial\\ell/\\partial c_{S}=$ $\\begin{array}{r}{\\sum_{l=M+1}^{L}\\partial\\ell/\\partial s_{l}\\cdot\\partial s_{l}/\\partial c_{S}}\\end{array}$ and the gradient fow formula that $\\partial_{t}c_{S}^{2}=-2c_{S}\\cdot\\partial\\mathcal{L}/\\partial c_{S}$ We have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\partial_{t}c_{S}^{2}=\\frac{4a c_{S}^{2}}{C_{D}}\\sum_{l=M+1}^{L}\\mathbb{E}\\left[\\sigma_{l}(a s)\\cdot\\left(\\frac{x_{L+1}}{y+\\varepsilon\\mathbf{1}}\\right)^{\\top}(x_{l}-y)\\cdot\\left(\\prod_{h\\in S}\\langle v_{l}^{(h)},v_{L+1}^{(h)}\\rangle-s_{l}\\right)\\right].\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "In the following, we consider a fixed $\\pi$ for error analysis and take expectation over T again when plugging ineverything back intothe dynamics.Tosimplify theexpressionof $\\partial_{t}c_{S}^{2}$ , we define quantities $g_{0,s}$ and $f$ as ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\iota_{0,S}:=\\displaystyle\\sum_{l=M+1}^{L}\\mathbb{E}_{X|\\pi}\\bigg[\\sigma_{l}(a s)\\displaystyle\\sum_{k=1}^{d}\\biggr(\\frac{\\mathbb{I}(x_{L+1}=x_{l}=e_{k})}{y(k)+\\varepsilon}-\\frac{y(k)\\,\\mathbb{I}(x_{L+1}=e_{k})}{y(k)+\\varepsilon}\\biggr)\\prod_{h\\in S}\\langle v_{l}^{(h)},v_{L+1}^{(h)}\\rangle\\biggr],}\\\\ &{\\,\\,\\,f:=\\displaystyle\\sum_{l=M+1}^{L}\\mathbb{E}_{X|\\pi}\\bigg[\\sigma_{l}(a s)\\displaystyle\\sum_{k=1}^{d}\\biggr(\\frac{\\mathbb{I}(x_{L+1}=x_{l}=e_{k})}{y(k)+\\varepsilon}-\\frac{y(k)\\,\\mathbb{I}(x_{L+1}=e_{k})}{y(k)+\\varepsilon}\\biggr)\\cdot s_{l}\\bigg].}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Note that here $f$ does not depend on $\\boldsymbol{S}$ . Based on the above definitions, we can rewrite (E.4) as ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\partial_{t}\\log c_{S}^{2}=\\frac{1}{c_{S}^{2}}\\cdot\\partial_{t}c_{S}^{2}=\\frac{4a}{C_{D}}\\cdot\\mathbb{E}_{\\pi\\sim\\mathcal{P}}[g_{0,S}-f].\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Using this, it can be shown that $C_{D}(t)$ does not change during the training, as described in the following lemma. ", "page_idx": 33}, {"type": "text", "text": "Lemma E.1. The quantity $\\begin{array}{r}{C_{D}(t)\\,=\\,\\sum_{\\mathcal{S}\\in[H]_{\\leq D}}c_{\\mathcal{S}}^{2}(t)}\\end{array}$ is preserved along the gradient fow over $\\{c_{S}\\}_{S\\in[H]\\leq D}$ ,i.e., $\\partial_{t}C_{D}(t)\\equiv0$ ", "page_idx": 33}, {"type": "text", "text": "This lemma will be useful in the following analysis, and we defer its proof to $\\S E.2.1$ . Next, we proceed to further simplify the dynamics in (E.5) by approximating $g_{0,s}$ ", "page_idx": 33}, {"type": "text", "text": "Simplification of $\\partial_{t}\\log c_{S}^{2}$ . To approximiate $g_{0,s}$ , we introduce the following quantities: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\iota_{1,\\mathcal{S}}:=\\displaystyle\\frac{1}{L-M}\\sum_{l=M+1}^{L}\\mathbb{E}_{X|\\pi}\\Bigg[\\Bigg(\\sum_{k=1}^{d}\\frac{\\mathbb{I}\\big(x_{L+1}=x_{l}=e_{k}\\big)}{\\bar{y}(k)+\\varepsilon}-\\frac{\\bar{y}(k)\\,\\mathbb{I}\\big(x_{L+1}=e_{k}\\big)}{\\bar{y}(k)+\\varepsilon}\\Bigg)\\prod_{h\\in\\mathcal{S}}\\langle v_{l}^{(h)},v_{L+1}^{(h)}\\rangle\\Bigg],}\\\\ &{\\iota_{2,\\mathcal{S}}:=\\displaystyle\\frac{1}{L-M}\\sum_{l=M+1}^{L}\\mathbb{E}_{X|\\pi}\\Bigg[\\Bigg(\\sum_{k=1}^{d}\\frac{\\mathbb{I}\\big(x_{L+1}=x_{l}=e_{k}\\big)}{\\mu^{\\pi}(e_{k})}-1\\Bigg)\\prod_{h\\in\\mathcal{S}}\\langle v_{l}^{(h)},v_{L+1}^{(h)}\\rangle\\Bigg],}\\\\ &{\\iota_{3,\\mathcal{S}}:=\\mathbb{E}_{(x,X),(z,Z)\\sim\\mu^{\\pi}\\otimes\\mu^{\\pi}}\\Bigg[\\Bigg(\\sum_{k=1}^{d}\\frac{\\mathbb{I}\\big(x=z=e_{k}\\big)}{\\mu^{\\pi}(e_{k})}-1\\Bigg)\\prod_{h\\in\\mathcal{S}}\\langle v^{(h)}(Z),v^{(h)}(X)\\rangle\\Bigg],}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $Z=(z_{-M},\\ldots,z_{-1})$ is independent of $X=\\left(x_{-M},\\ldots,x_{-1}\\right)$ and we define ", "page_idx": 33}, {"type": "equation", "text": "$$\nv^{(h)}(X):=\\sum_{i=1}^{M}\\sigma_{-i_{h}}^{(h)}x_{-i_{h}},\\quad v^{(h)}(Z):=\\sum_{i=1}^{M}\\sigma_{-i_{h}}^{(h)}z_{-i_{h}},\\quad\\mathrm{and}\\,\\,\\bar{y}:=\\frac{1}{L-M}\\sum_{l=M+1}^{L}x_{l}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Here $\\bar{y}(k)$ is the $k$ -th entry of $\\bar{y}$ We remark that each of $g_{1,S},g_{2,S},g_{3,S}$ is a function of $\\pi$ and $t$ , but we omit the dependence for brevity. ", "page_idx": 33}, {"type": "text", "text": "From $g_{0,s}$ to. $g_{1,s}$ , we replace attention probability $\\sigma_{l}(a s)$ by the uniform average with factor $1/L$ which yields $\\bar{y}$ From $g_{1,s}$ to $^{g_{2,s}}$ , we replace the empirical distribution $\\bar{y}$ with the stationary distribution $\\mu^{\\pi}$ and drop the small constant $\\varepsilon$ Finally, from $^{g_{2,s}}$ to $g_{3,S}$ , we replace the average over the sequence by the expectation over the stationary distribution $\\mu^{\\pi}$ of the underlying Markov chain. We will show that the approximation error in each step is small, given that $a$ and $\\varepsilon$ are sufficiently small and the Markov chain mixes well for a large $L$ ", "page_idx": 33}, {"type": "text", "text": "\u00b7 For the approximation of $g_{0,s}$ by. $g_{1,s}$ , note that when $a$ is small, the attention probability $\\sigma_{l}(a s)\\approx\\dot{1}/(L-M)$ for all $l\\in[L]$ More specifically, it follows from Lemma F.3 that ", "page_idx": 33}, {"type": "equation", "text": "$$\n|g_{0,S}-g_{1,S}|\\leq\\frac{8a d}{\\varepsilon^{2}}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "\u00b7 For the aproximation of $g_{1,s}$ by $^{g_{2,s}}$ , we leverage the approximation $\\bar{y}(k)\\approx\\mu^{\\pi}(e_{k})$ due to the mixing of the Markov chain for large $L$ . The result in Lemma F.4 implies that ", "page_idx": 33}, {"type": "equation", "text": "$$\n|g_{1,\\mathcal{S}}-g_{2,\\mathcal{S}}|\\leq4\\cdot\\frac{(1-\\lambda)^{-1/2}(D_{\\chi^{2}}(\\mu_{0}\\,\\|\\,\\mu^{\\pi})+1)^{1/4}+2\\sqrt{M}}{L^{1/2}\\gamma}+\\gamma^{-1}\\varepsilon\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $\\mu_{0}(\\cdot)$ is the initial distribution over the first $r_{n}$ tokens. Here we abuse the notation of $\\mu^{\\pi}$ in $\\dot{D_{\\chi^{2}}}(\\mu_{0}\\parallel\\mu^{\\pi})$ to denote the stationary distribution over the last $r_{n}$ tokens. Since $\\mu_{\\operatorname*{min}}^{\\pi}\\geq\\gamma$ by Assumption 3.5, we have ", "page_idx": 34}, {"type": "equation", "text": "$$\nD_{\\chi^{2}}(\\mu_{0}\\|\\mu^{\\pi})=\\sum_{X}\\left(\\mu(X)-\\mu^{\\pi}(X)\\right)^{2}/\\mu^{\\pi}(X)\\leq\\sum_{X}1/\\mu^{\\pi}(X)\\leq(2/\\gamma)^{r_{n}}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Therefore, we can further simplify the above bound as ", "page_idx": 34}, {"type": "equation", "text": "$$\n|g_{1,S}-g_{2,S}|=O\\bigg(\\frac{1}{\\sqrt{L(1-\\lambda)\\gamma^{r_{n}+2}}}+\\frac{\\varepsilon}{\\gamma}\\bigg).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "\u00b7 Finally, the approximation of $^{g_{2,s}}$ by $^{g_{3,S}}$ follows from the mixing property of the Markov chain. In particular, it follows from Lemma F.5 that ", "page_idx": 34}, {"type": "equation", "text": "$$\n|g_{2,S}-g_{3,S}|\\leq\\frac{8M}{L\\gamma}+\\frac{16\\sqrt{D_{\\chi^{2}}(\\mu_{0}\\,\\|\\,\\mu^{\\pi})+1}}{L(1-\\lambda)\\gamma^{|S|/2+1}}\\leq O\\bigg(\\frac{1}{L(1-\\lambda)\\gamma^{|S|/2+r_{n}/2+1}}\\bigg).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Combining the above results, and by the assumption that $a=a(0)=O(1/L^{3/2})$ and $\\varepsilon=1/\\sqrt{L}$ we obtain the following approximation error: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{|g_{0,S}-g_{3,S}|=O\\left(\\frac{a d}{\\varepsilon^{2}}\\right)+O\\bigg(\\frac{1}{\\sqrt{L(1-\\lambda)\\gamma^{r_{n}+2}}}+\\frac{\\varepsilon}{\\gamma}\\bigg)+O\\bigg(\\frac{1}{L(1-\\lambda)\\gamma^{|S|+2+r_{n}/2}}\\bigg)\\quad}\\\\ &{}&{\\leq O\\bigg(\\frac{1}{\\sqrt{L(1-\\lambda)\\gamma^{r_{n}+2}}}+\\frac{1}{L(1-\\lambda)\\gamma^{D/2+r_{n}/2+1}}\\bigg)\\leq O\\bigg(\\frac{1}{\\sqrt{L(1-\\lambda)\\gamma^{r_{n}+2}}}\\bigg),}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where we note that $|S|\\le D$ for any $\\cal S\\in[H]_{\\leq D}$ and the last inequality holds by also noting our condition on $L$ in (E.1) that $L\\ge\\Omega((1-\\lambda)^{-1}\\gamma^{-D})$ . As a result, the dynamics of $c_{S}^{2}$ in (E.5) can be approximated as follows: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\partial_{t}\\log c_{S}^{2}=\\frac{4a}{C_{D}}\\cdot\\mathbb{E}_{\\pi\\sim\\mathcal{P}}[g_{3,S}-f]+\\mathcal{E},\\quad\\mathrm{where}\\,\\,|\\mathcal{E}|\\leq O\\bigg(\\frac{a}{C_{D}\\sqrt{L(1-\\lambda)\\gamma^{r_{n}+2}}}\\bigg),\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where $O(\\cdot)$ hides universal constants that do not depend on the model parameters. Here and in the sequel, we let $\\mathcal{E}$ denote an error term that is of the order $O(a/\\sqrt{C_{D}^{2}L(1-\\lambda)\\gamma^{r_{n}+2}})$ where the specific constant hidden in $O(\\cdot)$ may change from line to line, but does not depend on the model parameters. In fact, we can show $C_{D}$ remains constant by Lemma E.1 and $a$ is not updated during this stage. Thus, the error term $|\\mathcal{E}|$ is of scale $O(a L^{-1/2})$ ", "page_idx": 34}, {"type": "text", "text": "LowerBound forTheDifference $\\partial_{t}\\log c_{S^{\\star}}^{2}-\\partial_{t}\\log c_{S}^{2}$ The reason for approximating $g_{0,s}$ by $^{g_{3,S}}$ in the previous step is that the latter is more interpretable, in the sense that we can relate it to the modified $\\chi^{2}$ mutual information $\\widetilde{I}_{\\chi^{2}}(S)$ . Recall that for each $\\mathcal{S}\\in[H]_{\\leq D}$ , the modified $\\chi^{2}$ -mutual information is ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\widetilde{I}_{\\chi^{2}}(S)=\\mathbb{E}_{\\pi\\sim\\mathcal{P},(z,Z)\\sim\\mu^{\\pi}}\\bigg[\\bigg(\\sum_{e\\in\\mathcal{X}}\\frac{\\mu^{\\pi}(z=e\\,|\\,Z_{-}s)^{2}}{\\mu^{\\pi}(z=e)}-1\\bigg)\\cdot\\mu^{\\pi}(Z_{-}s)\\bigg].\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Note that $f$ in (E.7) is independent of $\\boldsymbol{S}$ , and willbe canceled when computing $\\partial_{t}\\log c_{S^{\\star}}^{2}-\\partial_{t}\\log c_{S}^{2}$ ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\partial_{t}\\log c_{S^{\\star}}^{2}-\\partial_{t}\\log c_{S}^{2}=\\frac{4a}{C_{D}}\\cdot\\mathbb{E}_{\\pi\\sim\\mathcal{P}}[g_{3,S^{\\star}}-g_{3,S}]\\pm2|\\mathcal{E}|.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Thus,it sufficesto consider $\\mathbb{E}_{\\pi\\sim\\mathcal{P}}{\\left[g_{3,S^{\\star}}-g_{3,S}\\right]}.$ It follows from Lemma F.6 that for each $\\mathcal{S}\\in[H]_{\\leq D}$ $\\mathbb{E}_{\\pi\\sim\\mathcal{P}}[g_{3,S}]$ satisfies ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\left|\\mathbb{E}_{\\pi\\sim\\mathcal{P}}[g_{3,S}]-\\prod_{h\\in\\mathcal{S}}(\\sigma_{-h}^{(h)})^{2}\\cdot\\widetilde{I}_{\\chi^{2}}(S)\\right|\\le\\left(1-\\prod_{h\\in\\mathcal{S}}(\\sigma_{-h}^{(h)})^{2}\\right)\\cdot\\widetilde{I}_{\\chi^{2}}(S^{\\star}).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "This yields a lower bound for $\\mathbb{E}_{\\pi\\sim\\mathcal{P}}[g_{3,S^{\\star}}]$ and an upper bound for $\\mathbb{E}_{\\pi\\sim\\mathcal{P}}[g_{3,S}]$ for each $S\\ne S^{\\star}$ ,i.e., ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\pi\\sim\\mathcal{P}}[g_{3,S^{\\star}}]\\geq\\displaystyle\\prod_{h\\in S^{\\star}}(\\sigma_{-h}^{(h)})^{2}\\cdot\\widetilde{I}_{\\chi^{2}}(S^{\\star})-\\left(1-\\displaystyle\\prod_{h\\in S^{\\star}}(\\sigma_{-h}^{(h)})^{2}\\right)\\cdot\\widetilde{I}_{\\chi^{2}}(S^{\\star}),}\\\\ &{\\mathbb{E}_{\\pi\\sim\\mathcal{P}}[g_{3,S}]\\leq\\displaystyle\\prod_{h\\in S}(\\sigma_{-h}^{(h)})^{2}\\cdot\\widetilde{I}_{\\chi^{2}}(S)+\\left(1-\\displaystyle\\prod_{h\\in S}(\\sigma_{-h}^{(h)})^{2}\\right)\\cdot\\widetilde{I}_{\\chi^{2}}(S^{\\star}),\\quad\\mathrm{~for~all~}S\\neq S^{\\star}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Consequently, ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\partial_{t}\\log c_{S^{\\star}}^{2}-\\partial_{t}\\log c_{S}^{2}=\\displaystyle\\frac{4a}{C_{D}}\\cdot\\mathbb{E}_{\\pi\\sim\\mathcal{P}}[g_{3,S^{\\star}}-g_{3,S}]\\pm2|\\mathcal{E}|}\\\\ &{\\phantom{\\frac{(a)^{2}}{C_{D}}\\cdot\\frac{1}{C_{D}}}\\geq\\displaystyle\\frac{4a}{C_{D}}\\left(\\prod_{h\\in S^{\\star}}(\\sigma_{-h}^{(h)})^{2}\\cdot\\tilde{T}_{\\chi^{2}}(S^{\\star})-\\prod_{h\\in S}(\\sigma_{-h}^{(h)})^{2}\\cdot\\tilde{T}_{\\chi^{2}}(S)\\right)}\\\\ &{\\phantom{\\frac{(a)^{2}}{C_{D}}\\cdot\\frac{1}{C_{D}}}-\\displaystyle\\frac{4a}{C_{D}}\\left(2-\\prod_{h\\in S^{\\star}}(\\sigma_{-h}^{(h)})^{2}-\\prod_{h\\in S}(\\sigma_{-h}^{(h)})^{2}\\right)\\tilde{T}_{\\chi^{2}}(S^{\\star})-2|\\mathcal{E}|}\\\\ &{\\phantom{\\frac{(a)^{2}}{C_{D}}\\cdot\\frac{1}{C_{D}}}\\geq\\displaystyle\\frac{4a}{C_{D}}\\left(\\left(2\\prod_{h\\in S^{\\star}}(\\sigma_{-h}^{(h)})^{2}-2\\right)\\tilde{T}_{\\chi^{2}}(S^{\\star})+\\prod_{h\\in S}(\\sigma_{-h}^{(h)})^{2}\\cdot\\Delta\\tilde{T}_{\\chi^{2}}\\right)-2|\\mathcal{E}|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where the second inequality follows from the definition $\\begin{array}{r}{\\Delta\\widetilde{I}_{\\chi^{2}}=\\operatorname*{min}_{S\\in[H]_{\\leq D}\\backslash\\{S^{\\star}\\}}\\widetilde{I}_{\\chi^{2}}(S^{\\star})\\!-\\!\\widetilde{I}_{\\chi^{2}}(S)}\\end{array}$ $(\\sigma_{-h}^{(h)})^{2}\\in(0,1)$ we have $\\begin{array}{r}{\\prod_{h\\in\\mathcal{S}}(\\sigma_{-h}^{(h)})^{2}\\geq\\prod_{h=1}^{H}(\\sigma_{-h}^{(h)})^{2}}\\end{array}$ forany $\\mathcal{S}\\in[H]_{\\leq D}$ Appling this to the above inequality, we obtain ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\partial_{t}\\log c_{S\\cdot}^{2}-\\partial_{t}\\log c_{S}^{2}\\geq\\frac{4a}{C_{D}}\\Bigg(2\\prod_{h=1}^{H}(\\sigma_{-h}^{(h)})^{2}\\cdot\\widetilde{I}_{X^{2}}(S^{\\star})+\\prod_{h=1}^{H}(\\sigma_{-h}^{(h)})^{2}\\cdot\\Delta\\widetilde{I}_{X^{2}}-2\\widetilde{I}_{X^{2}}(S^{\\star})\\Bigg)-2|\\mathcal{E}|,\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Exponential Growth of $c_{S^{\\star}}^{2}$ .We proceed to show that the first term in (E8) dominates the error term $\\mathcal{E}$ and thus leads to the exponential growth of $c_{S^{\\star}}^{2}$ ", "page_idx": 35}, {"type": "text", "text": "Note that by Assumption 3.3, wh \u2265 w3 $w_{-h}^{(h)}\\geq w_{-j}^{(h)}+\\Delta w$ for all $j\\neq h$ and $h\\in[H]$ , where the quantity $\\Delta w$ satisfies ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\Delta w\\ge\\log\\left(M-1\\right)-\\log{\\left(\\left(1+\\frac{\\Delta\\widetilde{I}_{\\chi^{2}}}{14\\widetilde{I}_{\\chi^{2}}(S^{\\star})}\\right)^{\\frac{1}{2H}}-1\\right)}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Recall that we are not updating the RPE parameters during this stage, so $\\sigma^{(h)}$ is fixed for all $h\\in[H]$ So the gap condition E.9) holds throughout Stage $\\pmb{I}.$ This conditions esures that $w_{-h}^{(h)}\\gg w_{-j}^{(h)}$ $\\prod_{h\\in[H]}(\\sigma_{-h}^{(h)})^{2}$ is sficiently large. More precisly,given tha head $h$ is more focused on the $(-h)$ -th position by having a gap $\\Delta w$ in the initialization, we can further show by definition of the softmax function that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\sigma_{-h}^{(h)}\\geq\\frac{1}{1+(M-1)\\exp(-\\Delta w)},\\forall h\\in[H]\\Rightarrow\\prod_{h=1}^{H}(\\sigma_{-h}^{(h)})^{2}\\geq\\frac{1}{\\left(1+(M-1)\\exp(-\\Delta w)\\right)^{2H}}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Plugging (E.9) into (E.10), we have by additionally noting that $\\widetilde{I}_{\\chi^{2}}(S^{\\star})\\ge\\Delta\\widetilde{I}_{\\chi^{2}}>0$ that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\prod_{h=1}^{H}(\\sigma_{-h}^{(h)})^{2}\\geq\\left(1+\\frac{\\Delta\\widetilde{I}_{\\chi^{2}}}{14\\widetilde{I}_{\\chi^{2}}(S^{\\star})}\\right)^{-1}>\\frac{2\\widetilde{I}_{\\chi^{2}}(S^{\\star})+2/3\\cdot\\Delta\\widetilde{I}_{\\chi^{2}}}{2\\widetilde{I}_{\\chi^{2}}(S^{\\star})+\\Delta\\widetilde{I}_{\\chi^{2}}},\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "which implies that ", "page_idx": 35}, {"type": "equation", "text": "$$\n2\\prod_{h=1}^{H}(\\sigma_{-h}^{(h)})^{2}\\cdot\\widetilde{I}_{\\chi^{2}}(S^{\\star})+\\prod_{h=1}^{H}(\\sigma_{-h}^{(h)})^{2}\\cdot\\Delta\\widetilde{I}_{\\chi^{2}}-2\\widetilde{I}_{\\chi^{2}}(S^{\\star})\\geq\\frac{2}{3}\\Delta\\widetilde{I}_{\\chi^{2}}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Moreover, when $L$ is sufficiently large such that $L\\ge\\Omega((\\Delta\\widetilde{I}_{\\chi^{2}}^{2}(1-\\lambda)\\gamma^{r_{n}+2})^{-1})$ $\\mathcal{E}$ in (E.8) satisfy $|\\mathcal{E}|\\ \\leq\\ 13a\\Delta\\tilde{I}_{\\chi^{2}}/6C_{D}$ ,where $\\Omega$ hides a universal constant that does not depend on the model parameters. Therefore, combining (E.8) and (E.11), we conclude that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\partial_{t}\\log c_{S^{\\star}}^{2}-\\partial_{t}\\log c_{S}^{2}\\geq\\frac{8a\\Delta\\widetilde{I}_{\\chi^{2}}}{3C_{D}}-2|\\mathcal{E}|\\geq\\frac{a\\Delta\\widetilde{I}_{\\chi^{2}}}{2C_{D}}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "This implies that $c_{S^{\\star}}^{2}$ grows exponentially fast and becomes dominant. ", "page_idx": 35}, {"type": "text", "text": "Convergence of $p_{S^{\\star}}$ . In this part, we treat all the model parameters as a function of time $t$ . For simplicity, we omit the dependence on $t$ when it is clear from the context. It remains to derive the convergence of $p_{S^{\\star}}\\,=\\,c_{S^{\\star}}^{2^{\\star}}/C_{D}$ . Expanding $\\begin{array}{r}{C_{D}\\,=\\,\\sum_{\\mathcal{S}\\in[H]_{\\leq D}}c_{\\mathcal{S}}^{2}}\\end{array}$ , we can directly calculatethe derivative of $p_{S^{\\star}}$ as follows: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\partial_{t}\\log(1-p_{S^{\\star}})=\\partial_{t}\\log\\left(1-\\frac{c_{S^{\\star}}^{2}}{\\sum_{S\\in[H]_{\\leq D}}c_{S}^{2}}\\right)=\\cfrac{C_{D}}{C_{D}-c_{S^{\\star}}^{2}}\\cdot\\partial_{t}\\bigg(1-\\cfrac{c_{S^{\\star}}^{2}}{\\sum_{S\\in[H]_{\\leq D}}c_{S}^{2}}\\bigg)}\\\\ &{\\phantom{=}\\cfrac{C_{D}}{C_{D}-c_{S^{\\star}}^{2}}\\cdot\\cfrac{-(\\sum_{S\\in[H]_{\\leq D}}c_{S}^{2})\\cdot\\partial_{t}c_{S}^{2}\\cdot+c_{S^{\\star}}^{2}\\cdot\\sum_{S\\in[H]_{\\leq D}}\\partial_{t}c_{S}^{2}}{(\\sum_{S\\in[H]_{\\leq D}}c_{S}^{2})^{2}}}\\\\ &{\\phantom{=}\\cfrac{1}{C_{D}(C_{D}-c_{S^{\\star}}^{2})}\\sum_{S\\in[H]_{\\leq D}}(-c_{S}^{2}\\cdot\\partial_{t}c_{S^{\\star}}^{2}+c_{S^{\\star}}^{2}\\cdot\\partial_{t}c_{S}^{2})}\\\\ &{\\phantom{=}=\\cfrac{1}{C_{D}(C_{D}-c_{S^{\\star}}^{2})}\\underset{S\\in[H]_{\\leq D}}{\\sum_{S\\in[H]_{\\leq D}}c_{S^{\\star}}}\\cdot c_{S}^{2}\\cdot(-\\partial_{t}\\log c_{S}^{2}\\cdot+\\partial_{t}\\log c_{S}^{2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where in the last equality we use the fact that $\\partial_{t}\\log c_{S}^{2}\\,=\\,(\\partial_{t}c_{S}^{2})/c_{S}^{2}$ . Applying (E.12) to each $S\\ne S^{\\star}$ , we further have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\partial_{t}\\log(1-p_{S^{\\star}})\\leq\\displaystyle\\frac{1}{C_{D}(C_{D}-c_{S^{\\star}}^{2})}\\sum_{\\substack{S\\in[H]\\leq D\\setminus\\{S^{\\star}\\}}}c_{S^{\\star}}^{2}\\cdot c_{S}^{2}\\cdot\\left(-\\,\\frac{a\\Delta\\tilde{I}_{X^{2}}}{2C_{D}}\\right)\\qquad\\qquad}\\\\ {=\\displaystyle\\frac{1}{C_{D}(C_{D}-c_{S^{\\star}}^{2})}\\cdot c_{S^{\\star}}^{2}\\cdot(C_{D}-c_{S^{\\star}}^{2})\\cdot\\left(-\\,\\frac{a\\Delta\\tilde{I}_{X^{2}}}{2C_{D}}\\right)=-\\frac{c_{S^{\\star}}^{2}\\cdot a\\Delta\\tilde{I}_{X^{2}}}{2C_{D}^{2}}<0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "This implies that $p_{S^{\\star}}=c_{S^{\\star}}^{2}/C_{D}$ monotonically increases, and thus $c_{S^{\\star}}^{2}(t)\\geq c_{S^{\\star}}^{2}(0)$ for any $t\\geq0$ because $C_{D}$ is constant by Lemma E.1 and $c_{S^{\\star}}^{2}(0)$ is the initial value for $c_{S^{\\star}}^{2}$ at time $t=0$ . Therefore, we can further replace $c_{S^{\\star}}^{2}$ by its initial value in the above inequality, which yields ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\partial_{t}\\log(1-p_{S^{\\star}})\\leq-\\frac{c_{S^{\\star}}^{2}(0)a\\Delta\\tilde{I}_{\\chi^{2}}}{2C_{D}^{2}}=-\\frac{p_{S^{\\star}}(0)a\\Delta\\tilde{I}_{\\chi^{2}}}{2C_{D}}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "We remark that the above upper bound is independent of $t$ Finally, applying the Gronwall's inequality to $\\log(1-p_{S^{\\star}})$ , we obtain ", "page_idx": 36}, {"type": "equation", "text": "$$\n1-p_{S^{\\star}}(t)\\leq(1-p_{S^{\\star}}(0))\\cdot\\exp\\bigg(-\\frac{p_{S^{\\star}}(0)a\\Delta\\tilde{I}_{\\chi^{2}}}{2C_{D}}\\cdot t\\bigg).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "With training time $t_{1}\\geq(2C_{D}(0)\\log L)/(a\\cdot p_{S^{\\star}}(0)\\Delta\\widetilde{I}_{\\chi^{2}})$ , we can guarantee that ", "page_idx": 36}, {"type": "equation", "text": "$$\n1-p_{S^{\\star}}(t_{1})\\leq L^{-1}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "This concludes the proof for the first stage of the training. ", "page_idx": 36}, {"type": "text", "text": "E.2.1 Additional Proofs for the Stage I ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "We conclude this subsection with the proof of Lemma E.1. ", "page_idx": 36}, {"type": "text", "text": "Proof of Lemma E.1. By (E.5), we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\partial_{t}c_{S}^{2}=\\mathbb{E}_{\\pi\\sim\\mathcal{P}}[4a\\cdot p_{S}(g_{0,S}-f)].\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Moreover, by the definition of $g_{0,s}$ and $f$ , it holds that $\\begin{array}{r}{\\sum_{S\\in[H]_{\\leq D}}p_{S}g_{0,S}=f}\\end{array}$ Then, ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\partial_{t}C_{D}=\\sum_{S\\in[H]_{\\leq D}}\\partial_{t}c_{S}^{2}=4a\\cdot\\mathbb{E}_{\\pi\\sim\\mathcal{P}}\\bigg[\\sum_{S\\in[H]_{\\leq D}}p_{S}g_{0,S}-f\\bigg]\\equiv0.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Thus, the quantity $C_{D}$ is preserved under the dynamics. ", "page_idx": 36}, {"type": "text", "text": "E.3  Analysis for Stage II ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "In this section, we provide the analysis of the dynamics of $\\sigma^{(h)}\\equiv\\sigma(w^{(h)})$ for head $h\\in S^{\\star}$ . For head $h\\not\\in S^{\\star}$ , the results from Stage I imply that $p s\\rightarrow0$ for any $S\\ne S^{\\star}$ . Consequently, any head $h\\not\\in S^{\\star}$ will be ignored when producing the output features of FFN. Conversely, for $h\\in S^{\\star}$ , we establish the dominance of $w_{-h}^{(h)}$ over $w_{-i}^{(h)}$ forall $i\\neq h$ yielding $\\sigma_{-h}^{(h)}\\to1$ $t\\to\\infty$ In this imiting caseg head exactly copies the $(-h)$ -th parent. We also provide the corresponding convergence rate. ", "page_idx": 37}, {"type": "text", "text": "Proof Strategy. Similar to the proof for Stage I, our analysis for Stage II characterizes the dynamics of the difference between the positional embedding weights, Ot w-h $\\partial_{t}w_{-h}^{(h)}-\\partial_{t}w_{-i}^{(h)}$ for all $i\\neq h$ ,viathe following steps: ", "page_idx": 37}, {"type": "text", "text": "1. Dynamies Calculation. We initiate the analysis by deriving the dynamics of $w_{-i}^{(h)}$ for any fixed $i$ and $h$   \n2. Dynamics Approximation Then we approximate the dynamics by identifying the dominant term controlled by the modified $\\chi^{2}$ mutual information $\\widetilde{I}_{\\chi^{2}}(S^{\\star})$   \n3. Lower Bound for The Growth Rate By comparing the corresponding modified $\\chi^{2}$ mutual information, we establish a lower bound on $\\partial_{t}w_{-h}^{(h)}-\\partial_{t}w_{-i}^{(h)}$ for all $i\\neq h$   \n4. Convergence. Finaly, we dervethe covergence rate of $\\sigma_{-h}^{(h)}$ using the above lower bound. ", "page_idx": 37}, {"type": "text", "text": "Again, before proceeding with the detailed proof, we review the notations related to the dynamics of the positinal embeding weights $\\{w^{(h)}\\}_{h=1}^{H}$ Forthe $h$ th head of thefrst atention laer,the positional embedding vector $w^{(h)}$ induces the attention probability over a window of size $M$ , i.e., ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\sigma(w^{(h)})=:\\sigma^{(h)}=(\\sigma_{-M}^{(h)},\\cdot\\cdot\\cdot,\\sigma_{-1}^{(h)})\\in\\mathbb{R}^{1\\times M}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Fuheardat $a s$ where $s=u_{L+1}^{\\top}U_{M+1:L}^{\\top}.$ Then for each $l\\in[L]$ , the -th coordinate of $s$ is given by ", "page_idx": 37}, {"type": "equation", "text": "$$\ns_{l}=\\sum_{\\mathcal{S}\\in[H]\\subset P}p_{\\mathcal{S}}\\cdot\\prod_{h\\in\\mathcal{S}}\\langle v_{l}^{(h)},v_{L+1}^{(h)}\\rangle,\\quad\\mathrm{where~each~}v_{l}^{(h)}=\\sum_{i=1}^{M}\\sigma_{-i}^{(h)}x_{l-i}=\\sigma^{(h)}X_{(l-M):(l-1)}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Here $p_{S}$ is defined as in the analysis of Stage 1, and $X_{(l-M):(l-1)}\\in\\mathbb{R}^{M\\times d}$ is the submatrix of $X$ withrows $l-M,\\ldots,l-1$ ", "page_idx": 37}, {"type": "text", "text": "By direct calculation, we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\frac{\\partial\\sigma^{(h)}}{\\partial w^{(h)}}=\\mathrm{diag}(\\sigma^{(h)})-(\\sigma^{(h)})^{\\top}\\sigma^{(h)}\\in\\mathbb{R}^{M\\times M},\\quad\\frac{\\partial v_{l}^{(h)}}{\\partial\\sigma^{(h)}}=X_{(l-M):(l-1)}^{\\top}\\in\\mathbb{R}^{d\\times M},\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Then by chain rule, ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\frac{\\partial v_{l}^{(h)}}{\\partial w^{(h)}}=\\frac{\\partial v_{l}^{(h)}}{\\partial\\sigma^{(h)}}\\frac{\\partial\\sigma^{(h)}}{\\partial w^{(h)}}=X_{l-M:l-1}^{\\top}\\left(\\mathrm{diag}(\\sigma^{(h)})-(\\sigma^{(h)})^{\\top}\\sigma^{(h)}\\right)\\in\\mathbb{R}^{d\\times M}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Moreover, we can view each $s_{l}$ as a function of , 1h[]. Differentiating s with respect tov $v_{l}^{(h)}$ and $v_{L+1}^{(h)}$ we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\frac{\\partial s_{l}}{\\partial v_{l}^{(h)}}=\\sum_{\\substack{S\\in[H]\\leq D\\,\\textnormal{s t}h\\in S}}p_{S}\\prod_{\\substack{h^{\\prime}\\in S\\setminus\\{h\\}}}\\langle v_{l}^{(h^{\\prime})},v_{L+1}^{(h^{\\prime})}\\rangle v_{L+1}^{(h)}\\in\\mathbb{R}^{d},\\,}\\\\ {\\frac{\\partial s_{l}}{\\partial v_{L+1}^{(h)}}=\\sum_{\\substack{S\\in[H]\\leq D\\,\\textnormal{s t}h\\in S}}p_{S}\\prod_{\\substack{h^{\\prime}\\in S\\setminus\\{h\\}}}\\langle v_{l}^{(h^{\\prime})},v_{L+1}^{(h^{\\prime})}\\rangle v_{l}^{(h)}\\in\\mathbb{R}^{d\\times1}.}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "In the summation, we only add those $\\boldsymbol{S}$ 's in $[H]_{\\leq D}$ containing $h$ . Also, recall from (E.3) that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\frac{\\partial\\ell}{\\partial s_{l}}=-a\\left(\\frac{x_{L+1}}{y+\\varepsilon\\mathbf{1}}\\right)^{\\top}\\left(x_{l}-y\\right)\\cdot\\sigma_{l}\\left(a s\\right).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Now we are ready to proceed with the analysis for Stage II ", "page_idx": 37}, {"type": "text", "text": "Proof of Theorem 3.6: Stage Il. We start by calculating the explicit expression of the dynamics of $\\partial_{t}w_{-i}^{(h)}$ , and then derive approximation o the dynamics, which allows us to further show the convergence of g(h). ", "page_idx": 38}, {"type": "text", "text": "Calculation of The Dynamics of $\\partial_{t}w^{(h)}$ .First fix an $h\\in[H]$ . To simplify the notation, for each $l\\in[L]$ we define ", "page_idx": 38}, {"type": "equation", "text": "$$\nb_{l}:=X_{(l-M):(l-1)}\\cdot v_{L+1}^{(h)}+X_{(L+1-M):L}\\cdot v_{l}^{(h)}\\in\\mathbb{R}^{M}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Note that $w^{(h)}$ is the parameters of the $h$ th head and onlyenters ach $v_{l}^{(h)}$ \uff0c $l=1,\\dotsc,L+1$ Recall that $\\begin{array}{r}{s_{l}\\,=\\,\\sum_{\\cal S\\in[H]_{\\le D}}p s\\prod_{h\\in{\\cal S}}\\langle v_{l}^{(h)},v_{L+1}^{(h)}\\rangle}\\end{array}$ and the RPE weight $w^{(h)}$ for attention head $h$ only influences its outputs $v_{l}^{(h)}$ and vL+1 in the sum. It thus follows from the chain rule that for each $i\\in[M]$ , we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{\\partial s_{l}}{\\partial w_{-i}^{(h)}}=\\left(\\frac{\\partial s_{l}}{\\partial v_{L+1}^{(h)}}\\right)^{\\top}\\frac{\\partial v_{L+1}^{(h)}}{\\partial w_{-i}^{(h)}}+\\left(\\frac{\\partial s_{l}}{\\partial v_{l}^{(h)}}\\right)^{\\top}\\frac{\\partial v_{l}^{(h)}}{\\partial w_{-i}^{(h)}}}\\\\ &{=\\displaystyle\\sum_{S\\in[H]\\leq D^{s,\\,s_{l}h\\in S}}\\prod_{h^{\\prime}\\in S\\setminus\\{h\\}}\\langle v_{l}^{(h^{\\prime})},v_{L+1}^{(h^{\\prime})}\\rangle\\cdot v_{L+1}^{(h)\\top}\\chi_{(l-1):(l-1)}\\left(\\mathrm{diag}(\\sigma^{(h)})-(\\sigma^{(h)})^{\\top}\\sigma^{(h)}\\right)e_{M+1-}}\\\\ &{\\quad+\\displaystyle\\sum_{S\\in[H]\\leq D^{s,\\,s_{l}h\\in S}}p_{S}\\displaystyle\\prod_{h^{\\prime}\\in S\\setminus\\{h\\}}\\quad\\langle v_{l}^{(h^{\\prime})},v_{L+1}^{(h^{\\prime})}\\rangle\\cdot v_{l}^{(h)\\top}X_{(L+1-M):L}^{\\top}\\left(\\mathrm{diag}(\\sigma^{(h)})-(\\sigma^{(h)})^{\\top}\\sigma^{(h)}\\right)e_{M+1}}\\\\ &{=\\displaystyle\\sum_{S\\in[H]\\leq D^{s,\\,s_{l}h\\in S}}\\prod_{h^{\\prime}\\in S\\setminus\\{h\\}}\\langle v_{l}^{(h^{\\prime})},v_{L+1}^{(h^{\\prime})}\\rangle\\cdot b_{l}^{\\top}\\left(e_{M+1-i}-(\\sigma^{(h)})^{\\top}\\right)\\cdot\\sigma_{-i}^{(h)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where we remind readers that $e_{i}\\in\\mathbb{R}^{M\\times1}$ is the $i$ -th standard basis vector. ", "page_idx": 38}, {"type": "text", "text": "Furthermore, along the gradit w $\\partial_{t}w_{-i}^{(h)}=-\\partial\\mathcal{L}/\\partial w_{-i}^{(h)}$ , i fllows from E.13) that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\partial_{t}w_{-i}^{(h)}=-\\mathbb{E}_{\\pi,X}\\left[\\sum_{l=M+1}^{L}\\frac{\\partial\\ell}{\\partial s_{l}}\\frac{\\partial s_{l}}{\\partial w_{-i}^{(h)}}\\right]=a\\sum_{l=M+1}^{L}\\mathbb{E}_{\\pi,X}\\left[\\sigma_{l}\\left(a s\\right)\\left(\\frac{x_{L+1}}{y+\\varepsilon\\mathbf{1}}\\right)^{\\top}\\left(x_{l}-y\\right)\\frac{\\partial s_{l}}{\\partial w_{-i}^{(h)}}\\right]}\\\\ &{\\displaystyle\\qquad=a\\sum_{l=M+1}^{L}\\mathbb{E}_{\\pi,X}\\left[\\sigma_{l}\\left(a s\\right)\\sum_{k=1}^{d}\\biggl(\\frac{\\mathbb{I}\\left(x_{L+1}=x_{l}=e_{k}\\right)}{y(k)+\\varepsilon}-\\frac{y(k)\\,\\mathbb{I}\\left(x_{L+1}=e_{k}\\right)}{y(k)+\\varepsilon}\\biggr)\\frac{\\partial s_{l}}{\\partial w_{-i}^{(h)}}\\right]}\\\\ &{\\displaystyle\\qquad=a\\cdot\\mathbb{E}_{\\pi\\sim\\mathcal{P}}\\left[g_{h,0}^{\\top}\\left(e_{M+1-i}-\\left(\\sigma^{(h)}\\right)^{\\top}\\right)\\sigma_{-i}^{(h)}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where we plug in the expression of $\\partial s_{l}/\\partial w_{-i}^{(h)}$ above in the last equality. Here the vector $g_{h,0}$ is defined as ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\eta_{h,0}:=\\sum_{l=M+1}^{L}\\sum_{s\\in\\mathbb{N}\\atop s\\neq b\\in\\mathcal{S}}\\mathbb{E}_{X|\\pi}\\bigg[p_{S}\\sigma_{l}\\cdot\\sum_{k=1}^{d}\\bigg(\\frac{\\mathbb{I}({x_{L+1}}=x_{l}=e_{k})}{y(k)+\\varepsilon}-\\frac{y(k)\\,\\mathbb{I}({x_{L+1}}=e_{k})}{y(k)+\\varepsilon}\\bigg)\\cdot\\prod_{h^{\\prime}\\in\\mathcal{S}\\backslash\\{h\\}}\\langle v_{l}^{(h^{\\prime})},v_{h^{\\prime}}^{(h^{\\prime})}\\rangle\\bigg].\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "$\\sigma_{l}$ $l$ $\\partial_{t}w_{-i}^{(h)}$ and Otw-h, ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\!\\!\\partial_{t}w_{-h}^{(h)}-\\partial_{t}w_{-i}^{(h)}=a\\cdot\\mathbb{E}_{\\pi\\sim\\mathcal{P}}\\left[g_{h,0}^{\\top}\\left(e_{M+1-h}-\\big(\\sigma^{(h)}\\big)^{\\top}\\right)\\sigma_{-h}^{(h)}-g_{h,0}^{\\top}\\left(e_{M+1-i}-\\big(\\sigma^{(h)}\\big)^{\\top}\\right)\\sigma_{-i}^{(h)}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Using the fact that \u2265=1 ? $\\textstyle\\sum_{j=1}^{M}\\sigma_{-j}^{(h)}=1$ , we can rewrite ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(e_{M+1-h}-(\\sigma^{(h)})^{\\top}\\right)\\sigma_{-h}^{(h)}-\\left(e_{M+1-i}-(\\sigma^{(h)})^{\\top}\\right)\\sigma_{-i}^{(h)}}\\\\ &{\\qquad=\\sigma_{-i}^{(h)}\\big(e_{M+1-h}-e_{M+1-i}\\big)+(\\sigma_{-h}^{(h)}-\\sigma_{-i}^{(h)})\\big(e_{M+1-h}-(\\sigma^{(h)})^{\\top}\\big).}\\\\ &{\\qquad=\\sigma_{-i}^{(h)}\\big(e_{M+1-h}-e_{M+1-i}\\big)+(\\sigma_{-h}^{(h)}-\\sigma_{-i}^{(h)})\\displaystyle\\sum_{j=1}^{M}\\sigma_{-j}^{(h)}\\big(e_{M+1-h}-e_{M+1-j}\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where i therst entity,wead an thn tact $\\sigma_{-i}^{(h)}e_{M+1-h}$ . Combining (E.15) and (E.16) yields for each $i\\in[M]$ that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\gamma_{t}w_{-h}^{(h)}-\\partial_{t}w_{-i}^{(h)}}}\\\\ &{}&{=a\\cdot\\mathbb{E}_{\\pi\\sim\\mathcal{P}}\\left[g_{h,0}^{\\top}\\left(\\sigma_{-i}^{(h)}\\left(e_{M+1-h}-e_{M+1-i}\\right)+\\left(\\sigma_{-h}^{(h)}-\\sigma_{-i}^{(h)}\\right)\\sum_{j=1}^{M}\\sigma_{-j}^{(h)}\\big(e_{M+1-h}-e_{M+1-j}\\big)\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Simplification of $\\partial_{t}w_{-i}^{(h)}$ .We proced by derivin approximatiosto the vetor $g_{h,0}$ Which will helpus identify the dominant term in the dynamics $\\partial_{t}w_{-h}^{(h)}-\\partial_{t}w_{-i}^{(h)}$ . Specifically, we define ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{h_{1}:=\\displaystyle\\sum_{l=M+1}^{L}\\mathbb{E}_{X|\\pi}\\bigg[\\sigma_{l}(a\\delta)\\displaystyle\\sum_{k=1}^{d}\\biggl(\\frac{\\mathbf{1}(x_{L+1}=x_{l}=e_{k})}{y(k)+\\varepsilon}-\\frac{y(k)\\,\\mathbf{1}(x_{L+1}=e_{k})}{y(k)+\\varepsilon}\\biggr)\\prod_{h^{\\prime}\\in\\mathcal{S}^{\\setminus}\\setminus\\{h\\}}\\langle v_{l}^{(h^{\\prime})},v_{L+1}^{(h^{\\prime})}v_{l^{\\prime}}^{(h^{\\prime})}\\rangle}\\\\ &{h_{2}:=\\displaystyle\\frac{1}{L-M}\\displaystyle\\sum_{l=M+1}^{L}\\mathbb{E}_{X|\\pi}\\bigg[\\sum_{k=1}^{d}\\biggl(\\frac{\\mathbf{1}(x_{L+1}=x_{l}=e_{k})}{\\bar{y}(k)+\\varepsilon}-\\frac{\\bar{y}(k)\\,\\mathbf{1}(x_{L+1}=e_{k})}{\\bar{y}(k)+\\varepsilon}\\biggr)\\prod_{h^{\\prime}\\in\\mathcal{S}^{\\setminus}\\setminus\\{h\\}}\\langle v_{l}^{(h^{\\prime})},v_{L-1}^{(h)}v_{l^{\\prime}}^{(h)}\\rangle}\\\\ &{h_{3}:=\\displaystyle\\frac{1}{L-M}\\displaystyle\\sum_{l=M+1}^{L}\\mathbb{E}_{X|\\pi}\\bigg[\\biggl(\\sum_{k=1}^{d}\\frac{\\mathbf{1}(x_{L+1}=x_{l}=e_{k})}{\\mu^{\\pi}(e_{k})}-1\\biggr)\\prod_{h^{\\prime}\\in\\mathcal{S}^{\\setminus}\\setminus\\{h\\}}\\langle v_{l}^{(h^{\\prime})},v_{L+1}^{(h^{\\prime})}\\rangle h_{l}\\bigg],}\\\\ &{h_{4}:=\\mathbb{E}_{(x,X),(z,Z)\\sim\\mu^{\\pi}\\otimes\\varepsilon^{\\pi}}\\bigg[\\biggl(\\displaystyle\\sum_{k=1}^{d}\\frac{\\mathbf{1}(x=z=e_{k})}{\\mu^{\\pi}(e_{k})}-1\\biggr)\\prod_{h^{\\prime}\\in\\mathcal{S}^{\\setminus}\\setminus\\{h\\}}\\langle v^{(h^{\\prime})}(Z),v^{(h^{\\prime})}(X)\\rangle b(X,Z)\\bigg],}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where $Z=[z_{-M},\\ldots,z_{-1}]^{\\top}\\in\\mathbb{R}^{M\\times d}$ is an independent copy of $X=[x_{-M},\\ldots,x_{-1}]^{\\top}\\in\\mathbb{R}^{M\\times d}$ and ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{v^{(h)}(X):=\\displaystyle\\sum_{i=1}^{M}\\sigma_{-i}^{(h)}x_{-i},\\quad v^{(h)}(Z):=\\displaystyle\\sum_{i=1}^{M}\\sigma_{-i}^{(h)}z_{-i},}}\\\\ {{\\displaystyle b(X,Z):=Z(v^{(h)}(X))+X(v^{(h)}(Z)),\\quad\\bar{y}:=\\displaystyle\\frac{1}{L-M}\\sum_{l=M+1}^{L}x_{l}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "The strategy of gradually approximating $g_{h,0}$ by $g_{h,1},g_{h,2},g_{h,3}$ and $g_{h,4}$ is similar to the analysis in Stage I. To see the intuition, from $g_{h,0}$ to $g_{h,1}$ , we use the fact that $p s\\star\\approx1$ and $p s\\approx0$ for any other $\\boldsymbol{S}$ which is a result of Stage 1. From $g_{h,1}$ to $g_{h,2}$ , we replace $y$ by the empirical mean $\\bar{y}$ , thanks to the fact that $\\sigma_{l}(a)\\approx1/L$ when $a$ is small. Then, from $g_{h,2}$ to $g_{h,3}$ ,wereplace the empirical distribution $\\bar{y}$ with the stationary distribution of the Markov chain. These two steps also appear in the analysis of Stage 1. Finally, to go from $g_{h,3}$ to $g_{h,4}$ , we leverage the rapid mixing of the Markov chain. ", "page_idx": 39}, {"type": "text", "text": "Note that the common structures in (E.15) are $g_{h,0}^{\\top}(e_{M+1-h}-e_{M+1-i})$ for $i\\neq h$ Hence, we only need to understand the approximation error in each step for $g_{h,0}^{\\top}(e_{M+1-h}-e_{M+1-i})$ . Recall that we are focusing on $h\\in S^{\\star}$ in this stage. ", "page_idx": 39}, {"type": "text", "text": "\u00b7 From $g_{h,0}$ to $g_{h,1}$ , we remove the terms in the summation that are weighted down by $p_{S}$ for any $S\\ne S^{\\star}$ due to the rapid dominance of $p_{S^{\\star}}$ from Stage I. Recall that $p_{S^{*}}$ converges to one at an exponential rate while all other $p_{S}$ 's converge to zero. For simplicity, let us define ", "text_level": 1, "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\rho(S):=\\displaystyle\\sum_{l=M+1}^{L}\\mathbb{E}_{X|\\pi}\\bigg[\\sigma_{l}(a s)\\displaystyle\\sum_{k=1}^{d}\\biggr(\\frac{\\mathbb{1}(x_{L+1}=x_{l}=e_{k})}{y(k)+\\varepsilon}-\\frac{y(k)\\,\\mathbb{1}(x_{L+1}=e_{k})}{y(k)+\\varepsilon}\\biggr)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\cdot\\displaystyle\\prod_{h^{\\prime}\\in{\\mathcal{S}}\\backslash\\{h\\}}\\langle v_{l}^{(h^{\\prime})},v_{L+1}^{(h^{\\prime})}\\rangle b_{l}\\biggr](e_{M+1-h}-e_{M+1-i}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "By the triangular inequality, we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\left(g_{h,0}-g_{h,1}\\right)^{\\top}\\left(e_{M+1-h}-e_{M+1-i}\\right)\\right|=\\bigg|\\displaystyle\\sum_{\\scriptstyle s\\in[H]\\le D\\setminus\\{s^{\\star}\\}}\\!\\!\\!\\!p_{S}\\cdot\\rho(S)-\\rho(S^{\\star})\\bigg|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\le(1-p s\\cdot)\\cdot\\left|\\rho(S^{\\star})\\right|+\\displaystyle\\sum_{\\scriptstyle s\\in[H]\\le D\\setminus\\{s^{\\star}\\}}\\!\\!\\!\\!p_{S}\\cdot\\left|\\rho(S)\\right|\\le16(1-p s\\cdot),}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where in the last line we use the claim that $|\\rho(S)|\\le8$ for all $\\boldsymbol{S}$ . To see this point, note that by definition of $b_{l}$ in (E.14), we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|b_{l}^{\\top}\\left(e_{M+1-h}-e_{M+1-i}\\right)\\right|=\\left|\\langle v_{L+1}^{(h)},x_{l-h}-x_{l-i}\\rangle-\\langle v_{l}^{(h)},x_{L+1-h}-x_{L+1-i}\\rangle\\right|\\leq4,}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\left|\\displaystyle\\prod_{h^{\\prime}\\in\\mathcal{S}\\backslash\\{h\\}}\\langle v_{l}^{(h^{\\prime})},v_{L+1}^{(h^{\\prime})}\\rangle\\right|\\leq1,}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "since $v_{l}^{(h)}$ and $x_{l}$ havemamsThbyLahewhd 4 for the function $f(\\cdot)$ in the lemma, we conclude that $\\rho(S)\\le8,\\ \\ ^{-}\\forall\\bar{S}\\in[H]_{\\le D}^{\\quad-}\\setminus\\{S^{\\star}\\}$ Define $\\Delta_{1}:=1-p_{S^{\\star}}(t_{1})$ , and $\\Delta_{1}\\leq1/L$ by the results from Stage I. Thus, we obtain ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\left(g_{h,0}-g_{h,1}\\right)^{\\top}\\!\\left(e_{M+1-h}-e_{M+1-i}\\right)\\right|\\leq16\\Delta_{1}\\leq16/L.}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "\u00b7 For the approximation of $g_{h,1}$ by $g_{h,2}$ , we use the fact that $\\sigma_{l}(a s)\\;\\approx\\;1/L$ when $a$ is sufficiently small. Specifically, we also take the absolute bound for $f(\\cdot)$ as 4 in Lemma F.3 and obtain ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\left|\\left(g_{h,1}-g_{h,2}\\right)^{\\top}\\left(e_{M+1-h}-e_{M+1-i}\\right)\\right|\\leq\\frac{32a d}{\\varepsilon^{2}}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "\u00b7 For the approximation of $g_{h,2}$ by $g_{h,3}$ , we use the fact that $\\bar{y}(k)\\approx\\mu^{\\pi}(e_{k})$ for large $L$ . More precisely, it follows from Lemma F.4 with the upper bound 4 for $f(\\cdot)$ in the lemma that ", "page_idx": 40}, {"type": "equation", "text": "$$\ng_{h,2}-g_{h,3})^{\\top}\\left(e_{M+1-h}-e_{M+1-i}\\right)\\Big|\\leq16\\cdot\\frac{(1-\\lambda)^{-1/2}(D_{\\chi^{2}}(\\mu_{0}\\,\\|\\,\\mu^{\\pi})+1)^{1/4}+2\\sqrt{M}}{L^{1/2}\\gamma}+4\\gamma^{-1}\\varepsilon.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "\u00b7 Finally,to go from $g_{h,3}$ to $g_{h,4}$ , we leverage the rapid mixing of the Markov chain. Intuitively, when $l$ and $L+1$ are far apart, $x_{l}$ and its parents in $S^{\\star}$ are independent of $x_{L+1}$ and its parents in $S^{\\star}$ . This observation yields the approximation of $g_{h,3}^{\\top}\\left(e_{M+1-h}-e_{M+1-i}\\right)$ by $g_{h,4}^{\\top}\\left(e_{M+1-h}-e_{M+1-i}\\right)$ . To simplify the notation, define two scalars ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\widetilde{g}_{h,l}:=\\displaystyle\\left(\\sum_{k=1}^{d}\\frac{\\mathbb{1}(x_{L+1}=x_{l}=e_{k})}{\\mu^{\\pi}(e_{k})}-1\\right)\\prod_{h^{\\prime}\\in\\mathcal{S}^{\\star}\\backslash\\{h\\}}\\langle v_{l}^{(h^{\\prime})},v_{L+1}^{(h^{\\prime})}\\rangle,}}\\\\ {{\\widetilde{g}_{h,4}:=\\displaystyle\\left(\\sum_{k=1}^{d}\\frac{\\mathbb{1}(x=z=e_{k})}{\\mu^{\\pi}(e_{k})}-1\\right)\\prod_{h^{\\prime}\\in\\mathcal{S}^{\\star}\\backslash\\{h\\}}\\langle v^{(h^{\\prime})}(Z),v^{(h^{\\prime})}(X)\\rangle.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Using the notation above, we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|(g_{h,3}-g_{h,4})^{\\top}\\left(e_{M+1-h}-e_{M+1-i}\\right)\\right|}\\\\ &{\\quad=\\bigg|\\bigg(\\displaystyle\\sum_{l=M+1}^{L}\\frac{\\mathbb{E}_{X\\mid\\pi}\\left[\\widetilde{g}_{h,l}b_{l}^{\\top}\\right]}{L-M}-\\mathbb{E}_{(x,X),(z,Z)\\sim\\mu^{\\pi}\\otimes\\mu^{\\pi}}\\left[\\widetilde{g}_{h,4}b(X,Z)^{\\top}\\right]\\bigg)\\left(e_{M+1-h}-e_{M+1-i}\\right)\\bigg|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Recall that ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad b_{l}^{\\top}\\left(e_{M+1-h}-e_{M+1-i}\\right)=\\langle v_{L+1}^{(h)},x_{l-h}-x_{l-i}\\rangle-\\langle v_{l}^{(h)},x_{L+1-h}-x_{L+1-i}\\rangle,}\\\\ &{b(X,Z)^{\\top}\\left(e_{M+1-h}-e_{M+1-i}\\right)=\\langle v^{(h)}(X),z_{-h}-z_{-i}\\rangle-\\langle v^{(h)}(Z),x_{-h}-x_{-i}\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "We apply the triangular inequality to obtain that ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\bigl|(g_{h,3}-g_{h,4})^{\\top}\\bigl(e_{M+1-h}-e_{M+1-i}\\bigr)\\bigr|}\\qquad}&{}\\\\ &{\\leq\\left|\\frac{1}{L-M}\\sum_{l=\\underline{{M+1}}}^{L}\\mathbb{E}_{X\\geq\\vert\\pi\\vert}[\\tilde{g}_{h,l}\\{v_{L-h}^{(h)},x_{l-h}\\}]-\\mathbb{E}_{(x,X),(z,Z)\\sim\\mu^{\\tau}\\otimes\\mu^{\\tau}}\\bigl[\\tilde{g}_{h,4}(v^{(h)}(Z),x_{-h})\\bigr]\\right|}\\\\ &{\\qquad+\\left|\\frac{1}{L-M}\\sum_{l=\\underline{{M+1}}}^{L}\\mathbb{E}_{X\\geq\\vert\\pi\\vert}\\bigl[\\tilde{g}_{h,l}\\{v_{L+1}^{(h)},x_{l-i}\\}\\bigr]-\\mathbb{E}_{(x,X),(z,Z)\\sim\\mu^{\\tau}\\otimes\\mu^{\\tau}}\\bigl[\\tilde{g}_{h,4}(v^{(h)}(Z),x_{-i})\\bigr]\\right|}\\\\ &{\\qquad+\\left|\\frac{1}{L-M}\\sum_{l=\\underline{{M+1}}}^{L}\\mathbb{E}_{X\\geq\\vert\\pi\\vert}[\\tilde{g}_{h,l}\\{v_{l}^{(h)},x_{L+1-h}\\}]-\\mathbb{E}_{(x,X),(z,Z)\\sim\\mu^{\\tau}\\otimes\\mu^{\\tau}}\\bigl[\\tilde{g}_{h,4}(v^{(h)}(X),z_{-h})\\bigr]}\\\\ &{\\qquad+\\left|\\frac{1}{L-M}\\sum_{l=\\underline{{M+1}}}^{L}\\mathbb{E}_{X\\geq\\vert\\pi\\vert}[\\tilde{g}_{h,l}\\{v_{l}^{(h)},x_{L+1-i}\\}]-\\mathbb{E}_{(x,X),(z,Z)\\sim\\mu^{\\tau}\\otimes\\mu^{\\tau}}\\bigl[\\tilde{g}_{h,4}(v^{(h)}(X),z_{-h})\\bigr]}\\end{array}\\right|}\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Each term on the right-hand side can be bounded by Lemma F.5, where in the lemma we take $(\\sigma^{(h)})_{h^{\\prime}\\in\\mathcal{S}^{\\star}}\\in\\mathbb{R}^{M\\times|\\mathcal{S}^{\\star}|}$ and $((\\sigma^{(h^{\\prime})})_{h^{\\prime}\\in\\mathcal{S}^{\\star}\\backslash\\{h\\}},\\dot{e_{h}})\\in\\mathbb{R}^{M\\times|S^{\\star}|}$ as the two lists of vectors on the $M$ -dimensional probability simplex for $\\widetilde{\\sigma}$ and $\\sigma$ respectively. Consequently, we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\left|\\left(g_{h,3}-g_{h,4}\\right)^{\\top}\\left(e_{M+1-h}-e_{M+1-i}\\right)\\right|\\leq\\frac{8M}{L\\gamma}+\\frac{16}{L(1-\\lambda)\\gamma^{|\\mathcal{S}|/2+r_{n}/2+1}}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Combining the above results and setting $\\varepsilon=1/\\sqrt{L}$ $a=a(0)\\leq O(1/L^{3/2})$ and together with the conditions in (E.1), we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\left|\\left(g_{h,0}-g_{h,4}\\right)^{\\top}\\left(e_{M+1-h}-e_{M+1-i}\\right)\\right|=\\left|\\mathcal{E}\\right|=O\\bigg(\\frac{1}{\\sqrt{L(1-\\lambda)\\gamma^{r_{n}+2}}}\\bigg),\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where $O(\\cdot)$ hides universal constants independent of the parameters of the model. We remark that while the left hand side is a function of $t$ , the upper bound is independent of $t$ .Then, we can rewrite (E.17) as ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\rangle_{t}w_{-h}^{(h)}-\\partial_{t}w_{-i}^{(h)}}\\\\ &{\\quad=a\\cdot\\mathbb{E}_{\\pi\\sim\\mathcal{P}}\\bigg[\\sigma_{-i}^{(h)}\\cdot g_{h,4}^{\\top}\\big(e_{M+1-h}-e_{M+1-i}\\big)+(\\sigma_{-h}^{(h)}-\\sigma_{-i}^{(h)})\\cdot\\sum_{j=1}^{M}\\sigma_{-j}^{(h)}\\cdot g_{h,4}^{\\top}\\big(e_{M+1-h}-e_{M+1-j}\\big)\\bigg]}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\pm\\,a\\bigg(\\sigma_{-i}^{(h)}+(\\sigma_{-h}^{(h)}-\\sigma_{-i}^{(h)})\\sum_{j=1,j\\neq h}^{M}\\sigma_{-j}^{(h)}\\bigg)\\cdot|\\mathcal{E}|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Lower Bound for TheDifferene $\\partial_{t}w_{-h}^{(h)}-\\partial_{t}w_{-i}^{(h)}$ To show $\\partial_{t}w_{-h}^{(h)}-\\partial_{t}w_{-i}^{(h)}>0$ the lower bound of $\\mathbb{E}_{\\pi\\sim\\mathcal{P}}[g_{h,4}^{\\top}\\left(e_{M+1-h}-e_{M+1-i}\\right)]$ for any $i\\neq h$ . Since $(x,X)$ and $(z,Z)$ are independent and identically distributed, by the definition of $b(X,Z)$ ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\pi\\sim\\mathcal{P}}\\left[g_{h,4}^{\\top}\\left(e_{M+1-h}-e_{M+1-i}\\right)\\right]}\\\\ {=2\\mathbb{E}_{\\pi,(x,X),(z,Z)\\sim\\mu^{\\pi}\\otimes\\mu^{\\pi}}\\Bigg[\\sum_{k=1}^{d}\\biggl(\\frac{\\mathbb{I}(x=z=e_{k})}{\\mu^{\\pi}(e_{k})}-1\\biggr)\\prod_{h^{\\prime}\\in\\mathcal{S}^{\\star}\\backslash\\{h\\}}\\langle v^{(h^{\\prime})}(Z),v^{(h^{\\prime})}(X)\\rangle\\cdot\\langle v^{(h)}(X)\\rangle}\\\\ {-\\;2\\mathbb{E}_{\\pi,(x,X),(z,Z)\\sim\\mu^{\\pi}\\otimes\\mu^{\\pi}}\\biggl[\\sum_{k=1}^{d}\\biggl(\\frac{\\mathbb{I}(x=z=e_{k})}{\\mu^{\\pi}(e_{k})}-1\\biggr)\\prod_{h^{\\prime}\\in\\mathcal{S}^{\\star}\\backslash\\{h\\}}\\langle v^{(h^{\\prime})}(Z),v^{(h^{\\prime})}(X)\\rangle\\cdot\\langle v^{(h)}(X)\\rangle}\\\\ {=2\\tau_{h,1}-2\\tau_{h,2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where we introduce the following quantities for convenience: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{,(x,X),(z,Z)\\sim\\mu^{\\tau}\\otimes\\mu^{\\tau}\\displaystyle\\left[\\sum_{k=1}^{d}\\biggl(\\frac{\\mathbb{I}(x=z=e_{k})}{\\mu^{\\pi}(e_{k})}-1\\biggr)\\prod_{h^{\\prime}\\in\\mathcal{S}^{\\star}\\setminus\\{h\\}}\\langle v^{(h^{\\prime})}(Z),v^{(h^{\\prime})}(X)\\rangle\\cdot\\langle v^{(h)}(X),z_{-h}\\rangle\\right],}\\\\ &{,(x,X),(z,Z)\\sim\\mu^{\\tau}\\otimes\\mu^{\\tau}\\displaystyle\\left[\\sum_{k=1}^{d}\\biggl(\\frac{\\mathbb{I}(x=z=e_{k})}{\\mu^{\\pi}(e_{k})}-1\\biggr)\\prod_{h^{\\prime}\\in\\mathcal{S}^{\\star}\\setminus\\{h\\}}\\langle v^{(h^{\\prime})}(Z),v^{(h^{\\prime})}(X)\\rangle\\cdot\\langle v^{(h)}(X),z_{-i}\\rangle\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "The quantities $\\tau_{h,1}$ and $\\tau_{h,2}$ can be further approximated. Specifically, by applying Lemma F.6 to $\\tau_{h,1}.$ , where in the lemma we take $(\\sigma^{(h)})_{h^{\\prime}\\in\\mathcal{S}^{\\star}}\\in\\mathbb{R}^{M\\times|\\mathcal{S}^{\\star}|}$ and $((\\sigma^{(h^{\\prime})})_{h^{\\prime}\\in\\mathcal{S}^{\\star}\\backslash\\{h\\}},e_{h})\\in\\mathbb{R}^{M\\times|\\mathcal{S}^{\\star}|}$ as the two lists of vectors on the $M$ -dimensional probability simplex for $\\sigma$ and $\\widetilde{\\sigma}$ respectively, and we obtain ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\left|\\tau_{h,1}-\\prod_{h^{\\prime}\\in\\mathcal{S}^{\\star}\\setminus\\{h\\}}(\\sigma_{-h^{\\prime}}^{(h^{\\prime})})^{2}\\cdot\\sigma_{-h}^{(h)}\\cdot\\widetilde{I}_{\\chi^{2}}(\\mathcal{S}^{\\star})\\right|\\le\\left(1-\\prod_{h^{\\prime}\\in\\mathcal{S}^{\\star}\\setminus\\{h\\}}(\\sigma_{-h^{\\prime}}^{(h^{\\prime})})^{2}\\cdot\\sigma_{-h}^{(h)}\\right)\\widetilde{I}_{\\chi^{2}}(\\mathcal{S}^{\\star}).\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Drawing on the analagous reasoning as in the proof of Lemma F.6, we can approximate $\\tau_{h,2}$ as follows: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\left|\\tau_{h,2}-\\prod_{h^{\\prime}\\in\\mathcal{S}^{\\star}\\setminus\\{i h\\}}(\\sigma_{-h^{\\prime}}^{(h^{\\prime})})^{2}\\cdot\\sigma_{-h}^{(h)}\\cdot\\psi\\right|\\leq\\left(1-\\prod_{h^{\\prime}\\in\\mathcal{S}^{\\star}\\setminus\\{h\\}}(\\sigma_{-h^{\\prime}}^{(h^{\\prime})})^{2}\\cdot\\sigma_{-h}^{(h)}\\right)\\widetilde{I}_{\\mathcal{X}^{2}}(\\mathcal{S}^{\\star}),\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\rho:=\\mathbb{E}_{\\pi,(x,X),(z,Z)\\sim\\mu^{\\pi}\\otimes\\mu^{\\pi}}\\bigg[\\prod_{h^{\\prime}\\in S^{\\star}\\setminus\\{h\\}}\\mathbb{I}(x_{-h^{\\prime}}=z_{-h^{\\prime}})\\cdot\\mathbb{I}(x_{-h}=z_{-i})\\cdot\\biggl(\\sum_{k=1}^{d}\\frac{\\mathbb{I}(x=z=e_{k})}{\\mu^{\\pi}(e_{k})}-1\\biggr)\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "To establish the lower bound for $\\tau_{h,1}-\\tau_{h,2}$ let us begin by stablishing an upper bound for $\\psi$ :which is approximately equal to $\\tau_{h,2}$ . We invoke Lemma F.7 with $S=S^{\\star}$ and $S^{\\prime}\\,{=}\\,S^{\\star}\\backslash\\{h\\}\\cup\\{i\\}$ in the lemma to obtain ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\psi\\leq\\frac{1}{2}\\widetilde{I}_{\\chi^{2}}(\\mathcal{S}^{\\star})+\\frac{1}{2}\\widetilde{I}_{\\chi^{2}}(\\mathcal{S}^{\\star}\\backslash\\{h\\}\\cup\\{i\\})\\leq\\widetilde{I}_{\\chi^{2}}(\\mathcal{S}^{\\star})-\\frac{1}{2}\\cdot\\Delta\\widetilde{I}_{\\chi^{2}},\\quad\\forall i\\neq h\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Leveraging this for (E.19) and (E.20), ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2\\tau_{h,1}-2\\tau_{h,2}\\geq\\displaystyle\\prod_{h^{\\prime}\\in\\mathcal{S}^{\\star}\\setminus\\{\\{h\\}}}(\\sigma_{-h^{\\prime}}^{(h^{\\prime})})^{2}\\cdot\\sigma_{-h}^{(h)}\\cdot\\Delta\\widetilde{I}_{\\chi^{2}}-4\\bigg(1-\\displaystyle\\prod_{h^{\\prime}\\in\\mathcal{S}^{\\star}\\setminus\\{\\{h\\}}}(\\sigma_{-h^{\\prime}}^{(h^{\\prime})})^{2}\\cdot\\sigma_{-h}^{(h)}\\bigg)\\widetilde{I}_{\\chi^{2}}(\\mathcal{S}^{\\star})}\\\\ &{\\geq\\displaystyle\\prod_{h\\in\\mathcal{S}^{\\star}}(\\sigma_{-h}^{(h)})^{2}\\cdot\\Delta\\widetilde{I}_{\\chi^{2}}-4\\bigg(1-\\displaystyle\\prod_{h\\in\\mathcal{S}^{\\star}}(\\sigma_{-h}^{(h)})^{2}\\bigg)\\widetilde{I}_{\\chi^{2}}(\\mathcal{S}^{\\star}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where in the second line we multiply an additional $\\sigma_{-h}^{(h)}$ 2. to the product as oh $\\sigma_{-h}^{(h)}\\in[0,1]$ ", "page_idx": 42}, {"type": "text", "text": "Next, we provide almma shwing tht $\\partial_{t}\\sigma_{-h}^{(h)}$ is growing fo all time $t\\geq t_{1}$ ,where $t_{1}$ is the starting time of the second stage. ", "page_idx": 42}, {"type": "text", "text": "Lemma E.2 Reinforced Growth of $\\sigma_{-h}^{(h)},$ . For all $h\\in S^{\\star}$ we have fo all $i\\neq h$ at any $t\\geq t_{1}$ ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\partial_{t}\\sigma_{-h}^{(h)}>0,\\quad a n d\\quad\\partial_{t}\\log\\sigma_{-h}^{(h)}-\\partial_{t}\\log\\sigma_{-i}^{(h)}=\\partial_{t}w_{-h}^{(h)}-\\partial_{t}w_{-i}^{(h)}>0.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Proof. See $\\S E.3.1$ for the proof. ", "page_idx": 42}, {"type": "text", "text": "In the proof of Lemma E.2, we will use the following useful proposition. ", "page_idx": 42}, {"type": "text", "text": "Proposition E.3. Suppose $\\begin{array}{r}{\\prod_{h\\in S^{\\star}}(\\sigma_{-h}^{(h)})^{2}\\geq1/(1+(M-1)\\exp(-\\Delta w))^{2|S^{\\star}|}}\\end{array}$ with \u25b3w satisfying $\\sigma_{-h}^{(h)}>\\sigma_{-i}^{(h)}$ forany $i\\neq h,h\\in S^{\\star}$ $L$ satisfies (E.1). It holds that ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{9_{t}\\log\\sigma_{-h}^{(h)}-\\partial_{t}\\log\\sigma_{-i}^{(h)}=\\partial_{t}w_{-h}^{(h)}-\\partial_{t}w_{-i}^{(h)}\\ge\\displaystyle\\frac{a\\Delta\\tilde{I}_{\\chi^{2}}}{2}\\biggl(\\sigma_{-i}^{(h)}+(\\sigma_{-h}^{(h)}-\\sigma_{-i}^{(h)})\\sum_{j=1,j\\neq h}^{M}\\sigma_{-j}^{(h)}\\biggr)>0,}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\partial_{t}\\sigma_{-h}^{(h)}>0,\\qquad\\forall i\\ne h,\\quad\\forall h\\in\\mathcal{S}^{\\star}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Lemma E.2 implies that during Stage I, fo ll $i\\neq h$ and $h\\,\\in\\,S^{\\star}$ , we have $w_{-h}^{(h)}\\;>\\;w_{-i}^{(h)}$ and $\\sigma_{-h}^{(h)}>\\sigma_{-i}^{(h)}$ for all $t\\geq t_{1}$ . In addition, as $\\sigma_{-h}^{(h)}$ is growing, al the conditions i Proposition E.3 are satisfied for any $t\\geq t_{1}$ , and hence all the conclusions in (E.23). ", "page_idx": 43}, {"type": "text", "text": "Convergence of $\\sigma^{(h)}$ . Finally, we characterize the convergence rate of $\\sigma^{(h)}$ . For the convergence analysis, we adhere to the convention used in the previous stage, treating all model parameters as functions of the training time $t$ , where $t=t_{1}$ marks the start of the second stage. With a slight abuse of notation, we denote by $\\sigma_{-i}^{(h)}(t)$ the vlue of $\\sigma_{-i}(w^{(h)}(t))$ at ime $t$ where $w^{(h)}(t)$ isthe inputo the softmax function, and $\\sigma_{-i}(\\cdot)$ refers to the $(M+1-i)$ -th element of the softmax probability. For simplicity, we sometimes omit the time index $t$ when the context makes it clear. ", "page_idx": 43}, {"type": "text", "text": "Note that $\\partial_{t}\\sigma_{-h}^{(h)}>0$ for all $h\\in S^{\\star}$ .Hence by the defnition of the softmax operation, we have ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sigma_{-i}^{(h)}=\\sigma_{-h}^{(h)}\\cdot\\exp(-(w_{-h}^{(h)}-w_{-i}^{(h)}))\\geq\\sigma_{-h}^{(h)}(t_{1})\\cdot\\exp(-(w_{-h}^{(h)}-w_{-i}^{(h)})}\\\\ &{\\qquad=\\sigma_{-h}^{(h)}(0)\\cdot\\exp(-(w_{-h}^{(h)}-w_{-i}^{(h)})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where the fis inequality follows from the monotone growth of $\\sigma_{h}^{(h)}$ and the second line follows from the fact that the first attention layer is untouched during the first stage. Note that here in (E.24), $\\sigma^{(h)}$ and w(h) are functions of t. Now, puting together (E.23) and (E.24), and also noting that o) > o2 for all $i\\neq h$ and $h\\in S^{\\star}$ , it follows that ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\partial_{t}w_{-h}^{(h)}-\\partial_{t}w_{-i}^{(h)}\\geq\\frac{a\\Delta\\tilde{I}_{\\chi^{2}}}{2}\\sigma_{-i}^{(h)}\\geq\\frac{a\\Delta\\tilde{I}_{\\chi^{2}}}{2}\\cdot\\sigma_{-h}^{(h)}(0)\\cdot\\exp(-(w_{-h}^{(h)}-w_{-i}^{(h)})).\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Rearanginsduin $w_{-h}^{(h)}(t_{1})-w_{-i}^{(h)}(t_{1})\\geq\\Delta w$ by Assumpton 3.3, weget ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\exp\\left(w_{-h}^{(h)}(t)-w_{-i}^{(h)}(t)\\right)\\geq\\frac{a\\Delta\\widetilde{I}_{\\chi^{2}}\\cdot\\sigma_{-h}^{(h)}(0)}{2}\\cdot(t-t_{1})+\\exp(\\Delta w).\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "This yields alower bound fr $\\sigma_{-h}^{(h)}(t)$ as follows: ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\mathsf{r}_{-h}^{(h)}(t)=\\frac{1}{1+\\sum_{i\\neq h}\\exp(w_{-i}^{(h)}(t)-w_{-h}^{(h)}(t))}\\ge\\frac{1}{1+(M-1)\\cdot(a\\Delta\\tilde{I}_{\\chi^{2}}\\cdot\\sigma_{\\operatorname*{min}}(0)\\cdot(t-t_{1})/2+\\exp(w_{-h}^{(h)}(t)))},\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Wwhere e defne $\\begin{array}{r}{\\sigma_{\\operatorname*{min}}(0):=\\operatorname*{min}_{h\\in\\mathcal{S}^{\\star}}\\sigma_{-h}^{(h)}(0)}\\end{array}$ Consequenty we have ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r}{1-\\displaystyle\\prod_{h\\in\\mathcal{S}^{\\star}}(\\sigma_{-h}^{(h)}(t))^{2}\\le1-\\left(\\frac{1}{1+(M-1)\\cdot(a\\Delta\\tilde{I}_{\\chi^{2}}\\cdot\\sigma_{\\operatorname*{min}}(0)\\cdot(t-t_{1})/2+\\exp(\\Delta w))^{-1}}\\right)^{2|\\mathcal{S}^{\\star}|}}\\\\ {=1-\\left(1-\\frac{(M-1)}{(a\\Delta\\tilde{I}_{\\chi^{2}}\\cdot\\sigma_{\\operatorname*{min}}(0)\\cdot(t-t_{1})/2+\\exp(\\Delta w))+(M-1)}\\right)^{2|\\mathcal{S}^{\\star}|}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Now, we consider large $t$ such that ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\frac{(M-1)}{(a\\Delta\\tilde{I}_{\\chi^{2}}\\cdot\\sigma_{\\mathrm{min}}(0)\\cdot(t-t_{1})/2+\\exp(\\Delta w))+(M-1)}<\\frac{1}{2|S^{\\star}|}.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Then, we can apply the inequality $(1-x)^{n}\\geq1-n x$ for $x\\in[0,1/n]$ and $n\\geq1$ to obtain ", "page_idx": 43}, {"type": "equation", "text": "$$\n1-\\prod_{h\\in\\mathcal{S}^{\\star}}(\\sigma_{-h}^{(h)}(t))^{2}\\le\\frac{2|\\mathcal{S}^{\\star}|\\cdot(M-1)}{a\\Delta\\tilde{I}_{\\chi^{2}}\\cdot\\sigma_{\\operatorname*{min}}(0)\\cdot(t-t_{1})/2+\\exp(\\Delta w)+(M-1)}.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Therefore, with training time $t_{2}=4L|S^{\\star}|\\cdot(M-1)/a\\Delta\\widetilde{I}_{\\chi^{2}}\\cdot\\sigma_{\\mathrm{min}}(0)+t_{1}$ , we can ensure that ", "page_idx": 43}, {"type": "equation", "text": "$$\n1-\\prod_{h\\in\\mathcal{S}^{\\star}}(\\sigma_{-h}^{(h)}(t_{2}))^{2}\\leq L^{-1}.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "This completes the proof for Stage II. ", "page_idx": 43}, {"type": "text", "text": "E.3.1 Additional Proofs for Stage II ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "We conclude this subsection with the proof of Lemma E.2 and Proposition E.3. ", "page_idx": 44}, {"type": "text", "text": "Proof of Proposition E.3. The condition $\\begin{array}{r}{\\prod_{h\\in S^{\\star}}(\\sigma_{-h}^{(h)})^{2}\\geq1/(1+(M-1)\\exp(-\\Delta w))^{2|S^{\\star}|}}\\end{array}$ with $\\Delta w$ in (3.6) implies that ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\prod_{h\\in\\cal S^{\\star}}(\\sigma_{-h}^{(h)})^{2}\\geq\\left(1+\\frac{\\Delta\\widetilde{I}_{\\chi^{2}}}{14\\widetilde{I}_{\\chi^{2}}(\\cal S^{\\star})}\\right)^{-1}\\geq\\frac{4\\widetilde{I}_{\\chi^{2}}(\\cal S^{\\star})+\\frac{2}{3}\\Delta\\widetilde{I}_{\\chi^{2}}}{4\\widetilde{I}_{\\chi^{2}}(\\cal S^{\\star})+\\Delta\\widetilde{I}_{\\chi^{2}}}.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Combining (E.21) and (E.25) yields ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\pi\\sim\\mathcal{P}}\\left[g_{h,4}^{\\top}\\left(e_{M+1-h}-e_{M+1-i}\\right)\\right]=2\\tau_{h,1}-2\\tau_{h,2}\\geq\\frac{2}{3}\\Delta\\tilde{I}_{\\chi^{2}}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "for any $i\\neq h$ Alyng E2 t E18), sn $\\sigma_{-i}^{(h)}>0$ and $\\sigma_{-h}^{(h)}\\,>\\,\\sigma_{-i}^{(h)}$ at time $t$ for all $i\\neq h,h\\in S^{\\star}$ , it holds that ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\partial_{t}\\boldsymbol{w}_{-h}^{(h)}-\\partial_{t}\\boldsymbol{w}_{-i}^{(h)}\\ge a\\biggl(\\sigma_{-i}^{(h)}+(\\sigma_{-h}^{(h)}-\\sigma_{-i}^{(h)})\\cdot\\sum_{j=1,j\\neq h}^{M}\\sigma_{-j}^{(h)}\\biggr)\\cdot\\biggl(\\frac{2}{3}\\Delta\\tilde{I}_{\\chi^{2}}-|\\mathcal{E}|\\biggr).\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Then since we assume a suffciently large $L\\,\\geq\\,\\Omega((\\Delta\\widetilde{I}_{\\chi^{2}}^{2}(1-\\lambda)\\gamma^{r_{n}+2})^{-1})$ it holds that $|\\mathcal{E}|\\le$ $\\Delta\\widetilde{I}_{\\chi^{2}}/6$ we further have ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\partial_{t}w_{-h}^{(h)}-\\partial_{t}w_{-i}^{(h)}\\geq\\frac{a\\Delta\\widetilde{I}_{\\chi^{2}}}{2}\\bigg(\\sigma_{-i}^{(h)}+(\\sigma_{-h}^{(h)}-\\sigma_{-i}^{(h)})\\sum_{\\substack{j=1,\\,j\\neq h}}^{M}\\sigma_{-j}^{(h)}\\bigg)>0,\\quad\\forall i\\neq h,\\quad\\forall h\\in\\mathcal{S}^{\\star}.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "As $\\partial_{t}\\log\\sigma_{-h}^{(h)}\\,-\\,\\partial_{t}\\log\\sigma_{-i}^{(h)}\\,=\\,\\partial_{t}w_{-h}^{(h)}\\,-\\,\\partial_{t}w_{-i}^{(h)}\\,>\\,0$ by property ofth sofoma funtion and $\\begin{array}{r}{\\sum_{i=1}^{M}\\partial_{t}\\sigma_{-i}^{(h)}=0}\\end{array}$ we have $\\partial_{t}\\sigma_{-h}^{(h)}>0$ for all $h\\in S^{\\star}$ .This completes the proof of Proposition E3. \u53e3 ", "page_idx": 44}, {"type": "text", "text": "Proof of Lemma E.2. We give a proof to Lemma E.2 by contradiction. Note that at the beginning of the second stage $t=t_{1}$ , we have all the conditions for Proposition E.3 satisfied by the initialization conditions in Assumption 3.3. Then, by (E.23) in Proposition E.3, we have $\\partial_{t}\\log\\sigma_{-h}^{(h)}-\\partial_{t}\\log\\sigma_{-i}^{(h)}>$ 0 and $\\partial_{t}\\sigma_{-h}^{(h)}>0$ for all $i\\neq h$ and $h\\in S^{\\star}$ $t=t_{1}$ ", "page_idx": 44}, {"type": "text", "text": "Next asumethat $\\tau\\ >\\ t_{1}$ is thesallet m such a $\\partial_{t}\\sigma_{-h}^{(h)}\\;\\leq\\;0$ $\\partial_{t}\\log\\sigma_{-h}^{(h)}\\,-$ $\\partial_{t}\\log\\sigma_{-i}^{(h)}\\,\\leq\\,0$ for some $i\\neq h$ and $h\\,\\in\\,S^{\\star}$ . By defnition of $\\tau$ we have E.22) holds for any momentt E [t1, T). As \u03b1 a and the gap $\\sigma_{-h}^{(h)}-\\sigma_{-i}^{(h)}$ are monotonically increasing, we have by the initialization condition and the boundedness of the gradient that at time $\\tau$ ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\prod\\,(\\sigma_{-h}^{(h)})^{2}\\geq1/(1+(M-1)\\exp(-\\Delta w))^{2|\\mathcal{S}^{\\star}|}\\,,\\quad\\mathrm{and}\\quad\\sigma_{-h}^{(h)}>\\sigma_{-i}^{(h)}\\,,\\quad\\forall i\\neq h,\\quad\\forall h\\in\\mathcal{S}^{\\star}.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "by Prol $\\partial_{t}\\log\\sigma_{-h}^{(h)}-\\partial_{t}\\log\\sigma_{-i}^{(h)}>0$ $\\partial_{t}\\sigma_{-h}^{(h)}>0$ for al $i\\neq h$ and $h\\in S^{\\star}$ $\\tau$ , which contradicts the definition of $\\tau$ This completes the proof of Lemma E.2. ", "page_idx": 44}, {"type": "text", "text": "E.4  Analysis for Stage III ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "In this section, we derive the dynamics of the second attention layer's weights $a$ in Stage II. We characterize the dynamics of $a$ when $a<O(\\log L)$ , where the signal term of the dynamics dominates the approximation error. We provide the growth rate of the weights for two regimes: when $a$ is either sufficiently small or large. ", "page_idx": 44}, {"type": "text", "text": "Proof Strategy.   We analyze the dynamics of $a$ via the following steps: ", "page_idx": 45}, {"type": "text", "text": "1. Dynamics Calculation. First, we derive the explicit expression for the dynamics of $a$   \n2. Dynamics Approximation. We approximate the dynamics by exploiting the mixing properties of the Markov chain and the convergence of the weights from Stage I and II.   \n3. Lower and Upper Bound for The Growth Rate. Finally, we establish the upper and lower bounds for the growth rate of $a$ When $a$ is either sufficiently small or large. ", "page_idx": 45}, {"type": "text", "text": "For a set ${\\mathcal{S}}\\subseteq[M]$ , we denote $X_{l-S}:=(x_{l-s}:s\\in S)$ .If $l=0$ , we will ignore $l$ in the subscript and simply use $X_{-S}$ . In this section, we abbreviate $p_{S}(t_{1})$ after the first stage's training as $p_{S}$ , and (h2)(t2) after the second stage's training as o $\\sigma_{-i}^{(h)}$ ", "page_idx": 45}, {"type": "text", "text": "Proof of Theorem 3.6: Stage Il. We start with the explicit expression of the dynamics of $a$ ", "page_idx": 45}, {"type": "text", "text": "Calculation of The Dynamics of $a$ . First by the chain rule, ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\frac{\\partial\\ell}{\\partial a}=\\sum_{l=M+1}^{L}\\frac{\\partial\\ell}{\\partial(a s_{l})}\\frac{\\partial(a s_{l})}{\\partial a}=-\\sum_{l=M+1}^{L}\\left(\\frac{x_{L+1}}{y+\\varepsilon\\mathbf{1}}\\right)^{\\top}(x_{l}-y)\\cdot\\sigma_{l}(a s)\\cdot s_{l}.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where in the last equality we remind readers of the same procedure as we have used in the derivation of (E.3) in Stage I. Then, taking expectation with respect to $X$ and $\\pi$ and expanding $\\begin{array}{r}{s_{l}=a\\sum_{S\\in[H]_{\\leq D}}p s\\prod_{h\\in S}\\langle v_{l}^{(h)},v_{L+1}^{(h)}\\rangle}\\end{array}$ wehave ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\gamma_{t}a=-\\frac{\\partial\\mathcal{L}}{\\partial a}=\\mathbb{E}\\Bigg[\\sum_{l=M+1}^{L}\\left(\\frac{x_{L+1}}{y+\\varepsilon\\mathbf{1}}\\right)^{\\top}\\left(x_{l}-y\\right)\\cdot\\sigma_{l}\\left(a s\\right)\\cdot s_{l}\\Bigg]}\\\\ {\\displaystyle\\qquad=\\mathbb{E}\\Bigg[\\sum_{S\\in[H]_{\\leq D}}p s\\sum_{l=M+1}^{L}\\sigma_{l}\\sum_{k=1}^{d}\\!\\left(\\frac{\\mathbf{1}\\left(x_{L+1}=x_{l}=e_{k}\\right)}{y(k)+\\varepsilon}-\\frac{y(k)\\,\\mathbf{1}\\left(x_{L+1}=e_{k}\\right)}{y(k)+\\varepsilon}\\right)\\prod_{h\\in S}\\langle v_{l}^{(h)},v_{L+1}^{(h)}\\rangle\\Bigg]}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "We remind readers the shorthand $\\sigma\\equiv\\sigma(a s)$ .We denote the above quantity by $f_{0}$ ", "page_idx": 45}, {"type": "text", "text": "Approximation of $\\partial_{t}a$ . Similar to the analysis for the previous two stages, we develop a sequence of approximation steps that transforms $\\partial_{t}a$ into a tractable quantity. We aim to decouple $x_{L+1}$ and $x_{l}$ , approximate $s_{l}$ by a population version, and transform the expectation to one under the stationary distribution of the Markov chain. Specifically, the approximation involves the following steps: ", "page_idx": 45}, {"type": "text", "text": "\u00b7 Our first step is to remove the summation over $[H]_{\\leq D}\\backslash\\{S^{\\star}\\}$ where $S^{\\star}$ is the optimal set that maximizes the modified mutual information defined in (3.1). This is because $c_{S^{\\star}}$ dominates by the analysis of Stage I. Specifically, we define $f_{1}:=\\mathbb{E}\\left[\\sum_{l=M+1}^{L}\\sigma_{l}\\sum_{k=1}^{d}\\!\\left(\\frac{\\mathbb{1}(x_{L+1}=x_{l}=e_{k})}{y(k)+\\varepsilon}-\\frac{y(k)\\,\\mathbb{1}(x_{L+1}=e_{k})}{y(k)+\\varepsilon}\\right)\\,\\prod_{h\\in\\mathcal{S}^{\\star}}\\langle v_{l}^{(h)},v_{L+1}^{(h)}\\rangle\\right].$ To bound $|f_{0}-f_{1}|$ , note that for any $\\mathcal{S}\\in[H]_{\\leq D}$ , since each u(h) has norm at most 1, we can invoke Lemma F.2 with $C=1$ and obtain $\\left|\\sum_{l=M+1}^{L}\\sigma_{l}\\sum_{k=1}^{d}\\biggr(\\frac{\\mathbb{1}(x_{L+1}=x_{l}=e_{k})}{y(k)+\\varepsilon}-\\frac{y(k)\\,\\mathbb{1}(x_{L+1}=e_{k})}{y(k)+\\varepsilon}\\biggr)\\prod_{h\\in\\mathcal{S}^{\\star}}\\langle v_{l}^{(h)},v_{L+1}^{(h)}\\rangle\\right|\\leq2.$ ", "page_idx": 45}, {"type": "text", "text": "It follows that ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle|f_{0}-f_{1}|=\\mathbb{E}\\Bigg[\\sum_{S\\in[H]\\leq D\\setminus\\lbrace S^{\\star}\\rbrace}\\sum_{l=M+1}^{L}\\sigma_{l}\\sum_{k=1}^{d}\\mathbb{I}(x_{L+1}=e_{k})\\Bigg(\\frac{\\mathbb{I}(x_{l}=e_{k})}{y(k)+\\varepsilon}-\\frac{y(k)}{y(k)+\\varepsilon}\\Bigg)\\prod_{h\\in S}\\langle v_{l}^{(h)},v_{L}^{(h)}\\rangle}\\\\ {\\displaystyle\\ \\ \\ \\ \\ +\\left(1-p_{S^{\\star}}\\right)\\Bigg|\\mathbb{E}\\Bigg[\\sum_{l=M+1}^{L}\\sigma_{l}\\sum_{k=1}^{d}\\mathbb{I}\\big(x_{L+1}=e_{k}\\big)\\Bigg(\\frac{\\mathbb{I}(x_{l}=e_{k})}{y(k)+\\varepsilon}-\\frac{y(k)}{y(k)+\\varepsilon}\\Bigg)\\prod_{h\\in S^{\\star}}\\langle v_{l}^{(h)},v_{L}^{(h)}\\rangle}\\\\ {\\displaystyle\\ \\ \\ \\ \\leq4(1-p_{S^{\\star}}(t_{1}))=2\\Delta_{1},\\qquad\\mathrm{where}\\quad\\Delta_{1}:=(1-p_{S^{\\star}}(t_{1})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "In summary, the difference between $f_{0}$ and $f_{1}$ is controlled by the convergence results from Stage I. ", "page_idx": 46}, {"type": "text", "text": "\u00b7 Our second step is to characterize the approximation error incurred by the difference between the ideal attention scores and the actual attention scores in the second attention layer. Let us define $\\begin{array}{r}{s_{l}^{\\star}=\\prod_{h\\in S^{\\star}}\\mathbb{1}(x_{l-h}=x_{L+1-h})}\\end{array}$ as the ideal attention score for the second attention layer. We invoke Lemma F.1 to have for all $l\\in[L]$ ", "page_idx": 46}, {"type": "equation", "text": "$$\n|s_{l}-s_{l}^{\\star}|\\leq\\Delta_{1}+\\Delta_{2},\\quad\\mathrm{where}\\quad\\Delta_{2}:=1-\\prod_{h\\in\\cal S^{\\star}}(\\sigma_{-h}^{(h)}(t_{2}))^{2}.\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Corresponding to $\\{s_{l}^{\\star}\\}_{l=M+1}^{L}$ , we define ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\sigma_{l}^{\\star}:=\\frac{\\exp\\big(a\\prod_{h\\in S^{\\star}}\\mathbb{I}\\big(x_{l-h}=x_{L+1-h}\\big)\\big)}{\\sum_{l^{\\prime}=M+1}^{L}\\exp\\big(a\\prod_{h\\in S^{\\star}}\\mathbb{I}\\big(x_{l^{\\prime}-h}=x_{L+1-h}\\big)\\big)},\\quad y^{\\star}(k):=\\sum_{l=M+1}^{L}\\sigma_{l}^{\\star}\\,\\mathbb{I}\\big(x_{l}=e_{k}\\big),\\quad\\forall k\\in[0,T],\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "In the vector form, we have $\\begin{array}{r}{y^{\\star}=\\sum_{l=M+1}^{L}\\sigma_{l}^{\\star}x_{l}}\\end{array}$ Leveraging the above approximations, we define an approximation of $f_{1}$ as ", "page_idx": 46}, {"type": "equation", "text": "$$\nf_{2}:=\\mathbb{E}\\Bigg[\\sum_{l=M+1}^{L}\\sigma_{l}^{\\star}\\sum_{k=1}^{d}\\Bigg(\\frac{\\mathbb{1}(x_{L+1}=x_{l}=e_{k})}{y^{\\star}(k)+\\varepsilon}-\\frac{y^{\\star}(k)\\,\\mathbb{1}(x_{L+1}=e_{k})}{y^{\\star}(k)+\\varepsilon}\\Bigg)\\prod_{h\\in\\mathcal{S}^{\\star}}\\mathbb{1}(x_{l-h}=x_{L+1-h})\\Bigg]\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Applying Lemma F.9, it holds that ", "page_idx": 46}, {"type": "equation", "text": "$$\n|f_{1}-f_{2}|\\leq12\\cdot(1+a(t)\\cdot\\varepsilon^{-1})\\cdot(\\Delta_{1}+\\Delta_{2})\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "In summary, this error terms captures the difference between the ideal weights and the actual weights obtained by gradient flow at the end of Stage II. ", "page_idx": 46}, {"type": "text", "text": "\u00b7 Note that $y^{\\star}(k)$ is also random due to the randomness in $\\sigma_{l}^{\\star}$ , and as $L$ is sufficiently large, we want to replace $y^{\\star}(k)$ with its population counterpart. Let $z\\,\\in\\,{\\mathcal{X}}$ and $Z=$ $\\left(z_{-M},\\ldots,z_{-1}\\right)\\in\\mathcal{X}^{M}$ be two random variables and we define similarly for $x\\in\\mathscr{X}$ and $X=(x_{-M},\\ldots,x_{-1})\\in\\mathcal{X}^{M}$ . To this end, we define a reweighed distribution ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\widetilde{\\mu}^{\\pi}(z,Z\\,|\\,X_{-\\mathcal{S}^{\\star}})=\\frac{\\mu^{\\pi}(z,Z)\\exp\\left(a\\prod_{h\\in\\mathcal{S}^{\\star}}\\mathbb{1}(z_{-h}=x_{-h})\\right)}{\\sum_{z^{\\prime},Z^{\\prime}}\\mu^{\\pi}(z^{\\prime},Z^{\\prime})\\exp\\left(a\\prod_{h\\in\\mathcal{S}^{\\star}}\\mathbb{1}(z_{-h}^{\\prime}=x_{-h})\\right)},\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "where $\\mu^{\\pi}$ is the stationary distribution of the Markov chain over a window of size $M\\!+\\!1$ .This can be viewed as a reweighting of the stationary distribution over $(z,Z)$ by an exponential term that depends on the sequence $X_{-S^{\\star}}$ . We use $\\tilde{\\mu}^{\\pi}(z=e_{k}\\,|\\,X_{L+1-\\dot{S}^{\\star}})$ to replace $y^{\\star}(k)$ and define $\\bar{f_{3}}$ as ", "page_idx": 46}, {"type": "equation", "text": "$$\nf_{3}:=\\mathbb{E}\\Bigg[\\sum_{l=M+1}^{L}\\sigma_{l}^{\\star}\\sum_{k=1}^{d}\\Bigg(\\frac{\\mathbb{I}(x_{L+1}=x_{l}=e_{k})}{\\widetilde{\\mu}^{\\pi}(z=e_{k}\\,|\\,X_{L+1-{S^{\\star}}})}-\\mathbb{I}(x_{L+1}=e_{k})\\Bigg)\\prod_{h\\in{S}^{\\star}}\\mathbb{I}(x_{l-h}=x_{L+1-h})\\Bigg].\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "One can immediately draw a connection to Lemma F.4 as both targets characterize the gap between the empirical and population distributions. The only difference is that this time we have the distribution reweighed by some exponential term. For completeness, we provide the approximation result in Lemma F.10, which bounds the difference between $f_{2}$ and $f_{3}$ as ", "page_idx": 46}, {"type": "equation", "text": "$$\n|f_{2}-f_{3}|\\leq\\frac{8(1-\\lambda)^{-1/2}(D_{\\chi^{2}}(\\mu_{0}\\parallel\\mu^{\\pi})+1)^{1/4}+8\\sqrt{M}}{L^{1/2}\\cdot\\gamma|^{S^{*}}|+1}+\\frac{2d\\varepsilon}{\\gamma}\\lesssim\\frac{\\sqrt{M}+d}{L^{1/2}(1-\\lambda)^{1/2}\\gamma|^{S^{*}|+1+r_{n}/4}}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "where $\\mu_{0}(\\cdot)$ is the initial distribution for the first $r_{n}$ tokens in the Markov chain. Here and in the sequel, we simply use $D_{\\chi^{2}}(\\mu_{0}\\parallel\\mu^{\\pi})$ to denote $D_{\\chi^{2}}(\\mu_{0}(X_{1:r_{n}}=\\cdot)\\parallel\\mu^{\\pi}(X_{1:r_{n}}=\\cdot))$ when it is clear from the context. In the last inequality, we use the fact that $D_{\\chi^{2}}(\\mu_{0}\\parallel\\mu^{\\pi})\\leq$ $\\gamma^{-r_{n}}$ by (E.6) and the condition $\\varepsilon=L^{-1/2}$ ", "page_idx": 46}, {"type": "text", "text": "\u00b7 Note that in the expression of $f_{3}$ ,each $\\sigma_{l}^{\\star}$ still implicitly depends on the actual value of the sequence $X$ .Since $L$ is large and the Markov chain is weli-mixed, we can approximate $\\begin{array}{r}{\\sum_{l=M+1}^{L}\\sigma_{l}^{\\star}\\,\\mathbb{1}((x_{l},X_{l-S^{\\star}})=(\\cdot,\\cdot))}\\end{array}$ $\\tilde{\\mu}^{\\pi}(\\cdot,\\cdot\\,|\\,X_{L+1-S^{\\star}})$ $f_{3}$ ", "page_idx": 46}, {"type": "text", "text": "", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle=\\mathbb{E}_{\\pi,X,Z\\sim\\tilde{\\mu}^{\\pi}(\\cdot\\,|\\,X_{L+1-\\mathcal{S}^{\\star}})}\\left[\\sum_{k=1}^{d}\\biggl(\\frac{\\mathbb{I}(x_{L+1}=z=e_{k})}{\\tilde{\\mu}^{\\pi}(z=e_{k}\\,|\\,X_{L+1-\\mathcal{S}^{\\star}})}-\\mathbb{I}(x_{L+1}=e_{k})\\biggr)\\cdot\\mathbb{I}(Z_{l-\\mathcal{S}^{\\star}}=x_{L+1-\\mathcal{S}^{\\star}})\\right.}\\\\ {\\displaystyle=\\mathbb{E}_{\\pi,X,Z\\sim\\tilde{\\mu}^{\\pi}(\\cdot\\,|\\,X_{L+1-\\mathcal{S}^{\\star}})}\\left[\\sum_{k=1}^{d}\\frac{\\mu^{\\pi}(x=e_{k}\\,|\\,X_{-\\mathcal{S}^{\\star}})\\tilde{\\mu}^{\\pi}(z=e_{k},Z_{-\\mathcal{S}^{\\star}}=X_{-\\mathcal{S}^{\\star}}\\,|\\,X_{-\\mathcal{S}^{\\star}})}{\\tilde{\\mu}^{\\pi}(z=e_{k}\\,|\\,X_{-\\mathcal{S}^{\\star}})}\\right.}\\\\ {\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\left.-\\,\\tilde{\\mu}^{\\pi}(Z_{-\\mathcal{S}^{\\star}}=X_{-\\mathcal{S}^{\\star}}\\,|\\,X_{-\\mathcal{S}^{\\star}})\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Applying Lemma F.11 yields ", "page_idx": 47}, {"type": "equation", "text": "$$\n{3}-f_{4}|\\underset{\\pi\\in\\operatorname{supp}(\\mathcal{P})}{\\leq}\\frac{8\\gamma^{-1}(1-\\lambda)^{-1/2}(D_{\\chi^{2}}(\\mu_{0}\\parallel\\mu^{\\pi})+1)^{1/4}+16\\gamma^{-1}\\sqrt{M}}{L^{1/2}\\cdot\\gamma^{|S^{\\star}|+1}}\\lesssim\\frac{\\sqrt{M}+d}{L^{1/2}(1-\\lambda)^{1/2}\\gamma^{|S^{\\star}|+2}}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "where we use the fact that $D_{\\chi^{2}}(\\mu_{0}\\parallel\\mu^{\\pi})\\leq\\gamma^{-r_{n}}$ by (E.6) ", "page_idx": 47}, {"type": "text", "text": "\u00b7 Let $(z,Z)\\sim\\widetilde{\\mu}^{\\pi}(\\cdot\\,\\vert\\,X_{L+1-\\mathcal{S}^{\\star}})$ Since $L$ is large, the distribution of $(x_{L+1},X_{L+1-S^{\\star}})$ is close to the stationary distribution $\\mu^{\\pi}$ . Thus, we introduce the following approximation of $f_{4}$ ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{:=\\mathbb{E}_{\\pi,(x,X_{-}s^{\\star})\\sim\\mu^{\\pi},(z,Z)\\sim\\tilde{\\mu}^{\\pi}(\\cdot|X_{-}s^{\\star})}\\left[\\sum_{k=1}^{d}\\biggl(\\frac{1}{\\tilde{\\mu}^{\\pi}(e_{k}\\,|\\,X_{-}s^{\\star})}-\\mathbb{1}(x=e_{k})\\biggr)\\prod_{h\\in\\mathcal{S}^{\\star}}\\mathbb{1}(z_{-h}=x_{-h})\\right]}}\\\\ &{=\\mathbb{E}_{\\pi,(x,X_{-}s^{\\star})\\sim\\mu^{\\pi}}\\left[\\displaystyle\\sum_{k=1}^{d}\\frac{\\mu^{\\pi}(x=e_{k}\\,|\\,X_{-}s^{\\star})\\tilde{\\mu}^{\\pi}(z=e_{k},Z_{-}s^{\\star}=X_{-}s^{\\star}\\,|\\,X_{-}s^{\\star})}{\\tilde{\\mu}^{\\pi}(z=e_{k}\\,|\\,X_{-}s^{\\star})}\\right.}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\left.-\\tilde{\\mu}^{\\pi}(Z_{-}s^{\\star}=X_{-}s^{\\star}\\,|\\,X_{-}s^{\\star})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Note that ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\left|\\sum_{k=1}^{d}\\frac{\\mu^{\\pi}(x=e_{k}\\,|\\,X_{-\\mathcal{S}^{\\star}})\\widetilde{\\mu}^{\\pi}(z=e_{k},Z_{-\\mathcal{S}^{\\star}}=X_{-\\mathcal{S}^{\\star}}\\,|\\,X_{-\\mathcal{S}^{\\star}})}{\\widetilde{\\mu}^{\\pi}(z=e_{k}\\,|\\,X_{-\\mathcal{S}^{\\star}})}\\right|}}\\\\ &{\\quad=\\left|\\sum_{k=1}^{d}\\mu^{\\pi}(x=e_{k}\\,|\\,X_{-\\mathcal{S}^{\\star}})\\cdot\\widetilde{\\mu}^{\\pi}(Z_{-\\mathcal{S}^{\\star}}=X_{-\\mathcal{S}^{\\star}}\\,|\\,X_{-\\mathcal{S}^{\\star}},z=e_{k})\\right|\\le\\left|\\sum_{k=1}^{d}\\mu^{\\pi}(x=e_{k}\\,|\\,X_{-\\mathcal{S}^{\\star}})\\right|=}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "and so is $|\\widetilde{\\mu}^{\\pi}(Z_{-{\\cal S}^{\\star}}\\,=\\,X_{-{\\cal S}^{\\star}}\\,|\\,X_{-{\\cal S}^{\\star}})|\\,\\le\\,1$ . The difference between $f_{4}$ and $f_{5}$ is thus bounded by $2||p^{\\pi}(x_{L+1},X_{L+1-\\mathcal{S}^{\\star}}=\\cdot,\\cdot)-\\mu^{\\pi}(x_{L+1},X_{L+1-\\mathcal{S}^{\\star}}==\\cdot)||_{\\mathrm{TV}}$ and by the results in (F.29) of Lemma F.16: ", "page_idx": 47}, {"type": "equation", "text": "$$\n|f_{4}-f_{5}|\\leq2\\cdot\\operatorname*{sup}_{\\pi\\in\\operatorname{supp}(\\mathcal{P})}\\lambda^{L-M}\\sqrt{D_{\\chi^{2}}(\\mu_{0}\\,\\|\\,\\mu^{\\pi})+1}\\lesssim\\frac{\\lambda^{L-M}}{\\gamma^{r_{n}/2}}\\leq L^{-1},\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "where we use $D_{\\chi^{2}}(\\mu_{0}\\parallel\\mu^{\\pi})\\leq\\gamma^{-r_{n}}$ and the condition on $L$ in (E.2). ", "page_idx": 47}, {"type": "text", "text": "Collecting all the above approximation steps, we obtain (where we use $\\lesssim$ to hide absolute constants) ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|f_{0}-f_{5}|\\lesssim\\Delta_{1}+(1+a\\cdot\\varepsilon^{-1})\\cdot(\\Delta_{1}+\\Delta_{2})+L^{-1}+\\frac{\\sqrt{M}+d}{L^{1/2}(1-\\lambda)^{1/2}\\gamma^{|S^{\\star}|+2+r_{n}/4}}}\\\\ &{\\qquad\\qquad\\lesssim a\\cdot L^{-1/2}+\\frac{\\sqrt{M}+d}{L^{1/2}(1-\\lambda)^{1/2}\\gamma^{|S^{\\star}|+2+r_{n}/4}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "where the last line holds by moting that with sufficiently large $t_{1}$ and $t_{2}$ wehave $\\Delta_{1}+\\Delta_{2}\\leq L^{-1}$ and $\\varepsilon=L^{-1/2}$ . Here, express the error in terms of the trainable parameter $a$ and define ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\xi(a)\\asymp\\frac{\\sqrt{M}+d}{L^{1/2}(1-\\lambda)^{1/2}\\gamma^{|S^{\\star}|+2+r_{n}/4}}+a\\cdot L^{-1/2}.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "In particular, we have for $a=O(\\log L)$ that ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\xi(a)=O\\bigg(\\frac{\\sqrt{M}+d}{L^{1/2}(1-\\lambda)^{1/2}\\gamma^{|S^{\\star}|+2+r_{n}/4}}+\\frac{\\log L}{L^{1/2}}\\bigg).\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "In a nutshell, we conclude that when the weight $a$ satisfies $a<O(\\log L)$ , the dynamics of $a$ can be approximated by ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\partial_{t}a=f_{5}\\pm\\xi(a).\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "The following proposition helps us reformulate $f_{5}$ in a form that facilitates the analysis of the dynamics of $a$ ", "page_idx": 48}, {"type": "text", "text": "Proposition E.4. The term $f_{5}$ can be reformulated as ", "page_idx": 48}, {"type": "equation", "text": "$$\nf_{5}=\\mathbb{E}_{\\pi,X_{-S^{\\star}}\\sim\\mu^{\\pi}}\\left[J(X_{-{\\cal S}^{\\star}};a,\\pi)\\cdot e^{a}\\cdot\\left(r^{\\pi}(X_{-{\\cal S}^{\\star}})\\right)^{3}\\cdot\\mu^{\\pi}(X_{-{\\cal S}^{\\star}})\\right],\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "where $r^{\\pi}(X_{-\\mathcal{S}^{\\star}};a)=(1+\\mu^{\\pi}(X_{-\\mathcal{S}^{\\star}})\\cdot(e^{a}-1))^{-1}$ is the inverse of the normalization factor of $\\widetilde{\\mu}^{\\pi}$ in (E.27) and ", "page_idx": 48}, {"type": "equation", "text": "$$\nJ(X_{-}s\\cdot;a,\\pi)=\\sum_{k\\in[d]}\\frac{(\\mu^{\\pi}(x=e_{k}\\,|\\,X_{-}s\\cdot)-\\mu^{\\pi}(x=e_{k}))^{2}}{(1-r^{\\pi}(X_{-}s\\cdot;a))\\cdot\\mu^{\\pi}(x=e_{k}\\,|\\,X_{-}s\\cdot)+r^{\\pi}(X_{-}s\\cdot;a)\\cdot\\mu^{\\pi}(x=e_{k})}.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Proof. See $\\S E.4.1$ for the proof. ", "page_idx": 48}, {"type": "text", "text": "Inspired by this form, we define an alternative function $\\tilde{J}(\\cdot;r,\\pi)$ as ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\tilde{J}(X_{-}s\\cdot;r,\\pi):=\\sum_{k\\in[d]}\\frac{(\\mu^{\\pi}(x=e_{k}\\mid X_{-}s^{\\star})-\\mu^{\\pi}(x=e_{k}))^{2}}{(1-r)\\cdot\\mu^{\\pi}(x=e_{k}\\mid X_{-}s^{\\star})+r\\cdot\\mu^{\\pi}(x=e_{k})},\\quad r\\in[0,1]\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "where wereplace $r^{\\pi}(X_{-{\\cal S}^{\\star}};a)$ by a parameter $r\\,\\in\\,[0,1]$ . As exactly calculating the inverse normalization factor $r^{\\pi}(X_{-{\\cal S}^{\\star}};a)$ is intractable, we instead seek to find an upper and lower bound for $r^{\\pi}(X_{-{\\mathcal{S}}^{\\star}};a)$ and plug them into $\\tilde{J}(\\cdot;r,\\pi)$ to bound $f_{5}$ Suppose that $r^{\\pi}(X_{-{\\cal S}^{\\star}};a)$ enjoys the following parameter-dependent upper and lower bounds: ", "page_idx": 48}, {"type": "equation", "text": "$$\nr_{-}(a)\\leq r^{\\pi}(X_{-}\\boldsymbol{s}\\star;a)\\leq r_{+}(a),\\quad\\forall X_{-}\\boldsymbol{s}\\star\\in\\mathcal{X}^{|\\mathcal{S}^{\\star}|},\\quad\\forall\\pi\\in\\mathrm{supp}(\\mathcal{P}).\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Thus, an upper and lower bound to $J(X_{-\\cal S^{\\star}};a,\\pi)$ can be given by ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{r\\in[r_{-}(a),r_{+}(a)]}\\tilde{J}(X_{-}s\\star;r,\\pi)\\leq J(X_{-}s\\star;a,\\pi)\\leq\\operatorname*{sup}_{r\\in[r_{-}(a),r_{+}(a)]}\\tilde{J}(X_{-}s\\star;r,\\pi).\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "In order to effectively tackle these bounds, we then study the properties of $\\tilde{J}(\\cdot;r,\\pi)$ next. ", "page_idx": 48}, {"type": "text", "text": "Proposition E.5. Define ", "page_idx": 48}, {"type": "equation", "text": "$$\nD_{+}(X_{-{\\cal S}^{\\star}},\\pi)=\\operatorname*{max}\\left\\{D_{\\chi^{2}}(\\mu^{\\pi}(\\cdot)\\,\\|\\,\\mu^{\\pi}(\\cdot\\,|\\,X_{-{\\cal S}^{\\star}})),D_{\\chi^{2}}(\\mu^{\\pi}(\\cdot\\,|\\,X_{-{\\cal S}^{\\star}})\\,\\|\\,\\mu^{\\pi}(\\cdot))\\right\\}.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "The function $\\tilde{J}(X_{-S^{\\star}};r,\\pi)$ with $r\\in[0,1]$ defined in (E.31) satisfies the following properties: ", "page_idx": 48}, {"type": "text", "text": "1. $\\tilde{J}(X_{-S^{\\star}};r,\\pi)$ is convex in $r$   \n2. $\\tilde{J}(X_{-\\cal S^{\\star}};r,\\pi)\\leq D_{+}(X_{-\\cal S^{\\star}},\\pi).$   \n3. $\\tilde{J}(X_{-S^{\\star}};r,\\pi)$ is Lipschitz continuous in $r$ with Lipschitz constant $\\gamma^{-1}D_{+}(X_{-S^{\\star}},\\pi)$ ", "page_idx": 48}, {"type": "text", "text": "Proof. See $\\S E.4.1$ for the proof. ", "page_idx": 48}, {"type": "text", "text": "Upper and Lower Bounding $J(X_{-\\cal S^{\\star}};a,\\pi)$ .Previously, we show via a reformulation of $f_{5}$ that it suffices to bound $J(X_{-\\mathcal{S}^{\\star}};\\bar{a},\\pi)$ . In the sequel, we let ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{\\partial}_{+}(X_{-}s^{\\star},\\pi)=\\operatorname*{max}\\left\\{D_{\\chi^{2}}(\\mu^{\\pi}(\\cdot)\\,\\|\\,\\mu^{\\pi}(\\cdot\\,\\|\\,X_{-}s^{\\star})),D_{\\chi^{2}}(\\mu^{\\pi}(\\cdot\\,\\|\\,X_{-}s^{\\star})\\,\\|\\,\\mu^{\\pi}(\\cdot))\\right\\},}\\\\ &{\\qquad\\qquad\\rho=\\operatorname*{max}\\left\\{\\displaystyle\\operatorname*{max}_{X_{-}s^{\\star},\\pi}\\frac{D_{+}(X_{-}s^{\\star},\\pi)}{D_{\\chi^{2}}(\\mu^{\\pi}(\\cdot)\\,\\|\\,\\mu^{\\pi}(\\cdot\\,\\|\\,X_{-}s^{\\star}))},\\displaystyle\\operatorname*{max}_{X_{-}s^{\\star},\\pi}\\frac{D_{+}(X_{-}s^{\\star},\\pi)}{D_{\\chi^{2}}(\\mu^{\\pi}(\\cdot\\,\\|\\,X_{-}s^{\\star})\\,\\|\\,\\mu^{\\pi}(\\cdot))}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "It can be noticed that ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\rho\\leq\\operatorname*{max}\\left\\{\\underset{X_{-s^{\\star},\\pi}}{\\operatorname*{max}}\\frac{D_{\\chi^{2}}(\\mu^{\\pi}(\\cdot)\\,\\|\\,\\mu^{\\pi}(\\cdot\\,\\vert\\,X_{-\\mathcal{S}^{\\star}}))}{D_{\\chi^{2}}(\\mu^{\\pi}(\\cdot\\,\\vert\\,X_{-\\mathcal{S}^{\\star}})\\,\\|\\,\\mu^{\\pi}(\\cdot))},\\underset{X_{-s^{\\star},\\pi}}{\\operatorname*{max}}\\frac{D_{\\chi^{2}}(\\mu^{\\pi}(\\cdot\\,\\vert\\,X_{-\\mathcal{S}^{\\star}})\\,\\|\\,\\mu^{\\pi}(\\cdot))}{D_{\\chi^{2}}(\\mu^{\\pi}(\\cdot)\\,\\|\\,\\mu^{\\pi}(\\cdot\\,\\vert\\,X_{-\\mathcal{S}^{\\star}}))}\\right\\}}\\\\ &{\\quad\\leq\\operatorname*{max}\\left\\{\\underset{X_{-s^{\\star},\\pi}}{\\operatorname*{max}}\\frac{\\mu^{\\pi}(\\cdot)}{\\mu^{\\pi}(\\cdot\\,\\vert\\,X_{-\\mathcal{S}^{\\star}})},\\underset{X_{-s^{\\star},\\pi}}{\\operatorname*{max}}\\frac{\\mu^{\\pi}(\\cdot\\,\\vert\\,X_{-\\mathcal{S}^{\\star}})}{\\mu^{\\pi}(\\cdot)}\\right\\}\\leq\\gamma^{-1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "where the second inequality follows from noting that the $\\chi^{2}$ -divergence defined as $D_{\\chi^{2}}(\\mu\\parallel\\nu)=$ $\\begin{array}{r}{\\sum_{x}\\big(\\mu(x)-\\nu(x)\\big)^{2}/\\nu(x)}\\end{array}$ ,and $D_{\\chi^{2}}(\\mu\\parallel\\nu)/D_{\\chi^{2}}(\\nu\\parallel\\mu)\\leq\\operatorname*{sup}_{x}\\mu(x)/\\nu(x)$ ", "page_idx": 49}, {"type": "text", "text": "Apparently, $r^{\\pi}(X_{-{\\cal S}^{\\star}};a)$ is a function of $a$ and enjoys the following parameter-dependent upper and lower bounds: ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{l}{r_{+}(a)=(1+\\underset{X_{-}<s^{\\star},\\pi}{\\operatorname*{min}}\\mu^{\\pi}(X_{-S^{\\star}})(e^{a}-1))^{-1},}\\\\ {r_{-}(a)=(1+\\underset{X_{-}<s^{\\star},\\pi}{\\operatorname*{max}}\\mu^{\\pi}(X_{-S^{\\star}})(e^{a}-1))^{-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "If $a$ is small, we see that both $r_{+}(a)$ and $r_{-}(a)$ are close to 1, and we directly have ", "page_idx": 49}, {"type": "equation", "text": "$$\nr_{-}(a)\\le r^{\\pi}(X_{-}\\mathit{s}\\cdot;a)\\leq1,\\quad\\mathrm{where}\\quad1-\\operatorname*{max}_{X_{-}\\mathit{s}^{\\star}\\!,\\pi}\\mu^{\\pi}(X_{-}\\mathit{s}\\cdot)(e^{a}-1)\\leq r_{-}(a)<1.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "This suggests an upper bound of $J(X_{-\\cal S^{\\star}};a,\\pi)$ as ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I(X_{-\\mathcal{S}^{\\star}};a,\\pi)\\leq\\underset{r\\in[r_{-}(a),1]}{\\operatorname*{sup}}\\tilde{J}(X_{-\\mathcal{S}^{\\star}};r,\\pi)\\leq\\tilde{J}(X_{-\\mathcal{S}^{\\star}};1,\\pi)+\\gamma^{-1}\\cdot D_{+}(X_{-\\mathcal{S}^{\\star}},\\pi)\\cdot(1-r_{-}(a))}\\\\ &{\\qquad\\qquad\\leq D_{\\chi^{2}}(\\mu^{\\pi}(\\cdot\\,|\\,X_{-\\mathcal{S}^{\\star}})\\,\\|\\,\\mu^{\\pi}(\\cdot))+\\gamma^{-1}\\cdot D_{+}(X_{-\\mathcal{S}^{\\star}},\\pi)\\cdot\\underset{X_{-\\mathcal{S}^{\\star},\\pi}}{\\operatorname*{max}}\\mu^{\\pi}(X_{-\\mathcal{S}^{\\star}})\\cdot(e^{a}-1)}\\\\ &{\\qquad\\qquad\\leq D_{\\chi^{2}}(\\mu^{\\pi}(\\cdot\\,|\\,X_{-\\mathcal{S}^{\\star}})\\,\\|\\,\\mu^{\\pi}(\\cdot))\\cdot\\left(1+\\gamma^{-2}\\cdot\\underset{X_{-\\mathcal{S}^{\\star},\\pi}}{\\operatorname*{max}}\\mu^{\\pi}(X_{-\\mathcal{S}^{\\star}})\\cdot(e^{a}-1)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "where the second line follows from the Lipschitz continuity property, and the last line holds because the ratio $D_{+}(X_{-}s\\star,\\pi)/D_{\\chi^{2}}(\\mu^{\\pi}(\\cdot\\,\\vert\\,X_{-}s^{\\star})\\,\\Vert\\,\\mu^{\\pi}(\\cdot))$ is upper bounded by $\\rho$ and further by $\\gamma^{-1}$ .A similar lower bound can be obtained by changing the sign of $\\gamma^{-2}\\cdot\\operatorname*{max}_{X_{-S^{\\star},\\pi}}\\mu^{\\pi}(X_{-S^{\\star}})\\cdot(e^{a}-1)$ Hence, we h ", "page_idx": 49}, {"type": "equation", "text": "$$\nJ(X_{-\\mathcal{S}^{\\star}};a,\\pi)=D_{\\chi^{2}}(\\mu^{\\pi}(\\cdot\\,|\\,X_{-\\mathcal{S}^{\\star}})\\,\\|\\,\\mu^{\\pi}(\\cdot))\\cdot\\left(1\\pm\\gamma^{-2}\\cdot\\operatorname*{max}_{X_{-\\mathcal{S}^{\\star},\\pi}}\\mu^{\\pi}(X_{-\\mathcal{S}^{\\star}})\\cdot(e^{a}-1)\\right).\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "On the other hand, when $a$ becomes large, we have both $r_{+}(a)$ and $r_{-}(a)$ close to $0$ , and we have ", "page_idx": 49}, {"type": "equation", "text": "$$\n0\\leq r^{\\pi}(X_{-}s\\star;a)\\leq r_{+}(a),\\quad\\mathrm{where}\\quad0<r_{+}(a)\\leq\\frac{1}{\\operatorname*{min}_{X_{-}s^{\\star},\\pi}\\mu^{\\pi}(X_{-}s\\star)(e^{a}-1)}.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "In a similar fashion, we have the following upper bound: ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{J(X_{-S^{\\star}};a,\\pi)\\leq\\underset{r\\in[0,r_{+}(a)]}{\\operatorname*{sup}}\\tilde{J}(X_{-S^{\\star}};r,\\pi)\\leq\\tilde{J}(X_{-S^{\\star}};0,\\pi)+\\gamma^{-1}\\cdot D_{+}(X_{-S^{\\star}},\\pi)\\cdot r_{+}(a)}\\\\ &{\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,}\\\\ &{\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,=D_{\\chi^{2}}(\\mu^{\\pi}(\\cdot)\\,\\|\\,\\mu^{\\pi}(\\cdot\\,|\\,X_{-S^{\\star}}))+\\gamma^{-1}\\cdot\\frac{D_{+}(X_{-S^{\\star}},\\pi)}{\\operatorname*{min}_{X_{-S^{\\star}},\\pi}\\mu^{\\pi}(X_{-S^{\\star}})(e^{a}-1)}}\\\\ &{\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,}\\\\ &{\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "We can similarly obtain a lower bound by changing the sign of the second term inside the bracket. Hence,we have ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r}{J(X_{-\\mathcal{S}^{\\star}};a,\\pi)=D_{\\chi^{2}}(\\mu^{\\pi}(\\cdot)\\,\\|\\,\\mu^{\\pi}(\\cdot\\,|\\,X_{-\\mathcal{S}^{\\star}}))\\cdot\\left(1\\pm\\frac{\\gamma^{-2}}{\\operatorname*{min}_{X_{-\\mathcal{S}^{\\star}},\\pi}\\mu^{\\pi}(X_{-\\mathcal{S}^{\\star}})(e^{a}-1)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Divergence of $a$ : Recall that we have shown the dynamics of $a$ in (E.30), where $\\xi(a)$ is negligible when $L$ goes to infinity. Thus, when $L$ is sufficiently large, we see by the nonnegativity of $f_{5}$ that $a(t)$ continues to increase as $t$ increases until it reaches a point where $f_{5}$ no longer dominates the approximation error. To characterize the regime where $f_{5}\\geq\\xi(a)$ , we first note that for $a\\leq\\log L$ it holds by (E.29) that ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\xi(a)=O(L^{-1/2}\\log L)\\approx L^{-1/2},\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "where $\\approx$ hides logarithmic factors. For $f_{5}$ , we recall from Proposition E.4 that ", "page_idx": 50}, {"type": "equation", "text": "$$\nf_{5}=\\mathbb{E}_{\\pi,X_{-S^{\\star}}\\sim\\mu^{\\pi}}\\bigg[\\frac{J(X_{-S^{\\star}})\\cdot e^{a}}{\\left(1+\\mu^{\\pi}(X_{-S^{\\star}})\\cdot(e^{a}-1)\\right)^{3}}\\cdot\\mu^{\\pi}(X_{-S^{\\star}})\\bigg],\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "where for small $a$ we have $f_{5}=\\Omega(1)$ and for large $a$ we have $f_{5}=\\Omega(e^{-2a})$ . Thus, $e^{-2a}\\geq L^{-1/2}$ gives the condition for $f_{5}$ to dominate the approximation error, which gives $a=O(\\log L)$ . In the sequel, we consider the dynamics for $a\\le(\\log L)/8$ and give a more rigorous analysis. ", "page_idx": 50}, {"type": "text", "text": "We use the_ notation $\\begin{array}{r l r}{x}&{{}=}&{o(1)}\\end{array}$ to denote that a term is much smaller than 1, for example, $(\\log\\log L)^{-1}=o(1)$ For any $x_{0}$ and $\\delta$ , we write $x=x_{0}\\pm\\delta$ to indicate that $x$ is bounded within $[x_{0}-\\delta,x_{0}+\\delta]$ . In the following, we assume there exists $\\delta$ satisfying $\\delta\\leq\\gamma^{2}/4\\land1/8$ and ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\delta\\cdot\\mathbb{E}_{\\pi\\sim\\mathcal{P}}\\bigg[\\sum_{X_{-S^{\\star}}}D_{\\chi^{2}}\\big(\\mu^{\\pi}(\\cdot\\,|\\,X_{-S^{\\star}})\\,\\|\\,\\mu^{\\pi}(\\cdot)\\big)\\cdot\\big(\\mu^{\\pi}(X_{-S^{\\star}})\\big)^{2}\\bigg]\\geq\\xi(\\log L),}\\\\ {\\delta\\cdot\\mathbb{E}_{\\pi\\sim\\mathcal{P}}\\bigg[\\sum_{X_{-S^{\\star}}}\\frac{D_{\\chi^{2}}\\big(\\mu^{\\pi}(\\cdot)\\,\\|\\,\\mu^{\\pi}(\\cdot\\,|\\,X_{-S^{\\star}})\\big)\\cdot L^{-1/4}}{\\mu^{\\pi}(X_{-S^{\\star}})}\\bigg]\\geq\\xi(\\log L).}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Note that ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\xi(\\log L)\\leq O{\\left(\\frac{\\sqrt{M}+d}{L^{1/2}(1-\\lambda)^{1/2}\\gamma^{|S^{\\star}|+2+r_{n}/4}}+\\frac{\\log L}{L^{1/2}}\\right)}.\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "By additionally noting that $\\mu^{\\pi}(X_{-\\mathcal{S}^{\\star}})\\geq\\gamma^{|\\mathcal{S}^{\\star}|}$ thanks to the lower bound of the transition probability. we are able to find such a $\\delta$ if we have ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\frac{L}{(\\log L)^{4}}\\geq\\Omega\\bigg(\\frac{1}{\\kappa^{4}\\gamma^{8+2|S^{\\star}|}}\\cdot\\bigg(\\frac{\\sqrt{M}+d}{(1-\\lambda)^{1/2}\\gamma^{|S^{\\star}|+2+r_{n}/4}}\\bigg)^{4}\\bigg),\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "where $\\kappa$ is defined as ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\kappa\\!:=\\!\\mathbb{E}\\left[D_{\\chi^{2}}(\\mu^{\\pi}(\\cdot)\\,\\|\\,\\mu^{\\pi}(\\cdot\\,|\\,X_{-}s\\star))\\right]\\wedge\\mathbb{E}\\left[D_{\\chi^{2}}(\\mu^{\\pi}(\\cdot\\,|\\,X_{-}s\\star)\\,\\|\\,\\mu^{\\pi}(\\cdot))\\right]\\wedge1,}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "and $\\Omega(\\cdot)$ only hides universal constants. Note that this is already guaranteed by the condition on $L$ in (E.2). In particular, we can just take $\\delta=\\gamma^{2}/4\\wedge1/8$ in the following analysis. ", "page_idx": 50}, {"type": "text", "text": "Small $a$ . Consider the case where $a$ is small in the sense that $\\mu^{\\pi}(X_{-{\\cal S}^{\\star}})\\cdot(e^{a}-1)\\leq\\delta$ for any $X_{-S^{\\star}}$ and $\\pi\\in\\operatorname{supp}(\\mathcal{P})$ . In fact, one can directly deduce from our previous results that $1-\\delta\\leq$ $r_{-}(a)\\leq r^{\\pi}(X_{-S^{\\star}};a)<1$ and ", "page_idx": 50}, {"type": "equation", "text": "$$\n1-3\\delta\\leq(r^{\\pi}(X_{-S^{\\star}};a))^{3}\\leq1.\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "For $J(X_{-\\cal S^{\\star}};a,\\pi)$ , we combine the condition that $\\mu^{\\pi}(X_{-{\\cal S}^{\\star}})\\cdot(e^{a}-1)\\leq\\delta$ with (E.32) to obtain that ", "page_idx": 50}, {"type": "equation", "text": "$$\nJ(X_{-S^{\\star}};a,\\pi)=\\left(1\\pm\\gamma^{-2}\\delta\\right)\\cdot D_{\\chi^{2}}\\bigl(\\mu^{\\pi}(\\cdot\\,|\\,X_{-S^{\\star}})\\,\\|\\,\\mu^{\\pi}(\\cdot)\\bigr),\\quad\\mathrm{where}\\quad\\gamma^{-2}\\delta\\le1/4.\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Combining the above two results with Proposition E.4, we have ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f_{5}=\\mathbb{E}_{\\pi,X_{-S^{\\star}}\\sim\\mu^{\\pi}}\\left[J(X_{-S^{\\star}};a,\\pi)\\cdot e^{a}\\cdot\\left(r^{\\pi}(X_{-S^{\\star}})\\right)^{3}\\cdot\\mu^{\\pi}(X_{-S^{\\star}})\\right]}\\\\ &{\\quad=\\left(1\\pm(\\gamma^{-2}+3)\\delta\\right)\\cdot\\mathbb{E}_{\\pi\\sim\\mathcal{P}}\\Bigg[\\sum_{X_{-S^{\\star}}}D_{\\chi^{2}}\\big(\\mu^{\\pi}(\\cdot\\,|\\,X_{-S^{\\star}})\\,\\|\\,\\mu^{\\pi}(\\cdot)\\big)\\cdot\\mu^{\\pi}(X_{-S^{\\star}})^{2}\\Bigg]\\cdot e^{a}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Also, the noise term $\\xi+\\psi(a)$ is upper bounded by ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\xi+\\psi(\\log L)\\leq\\delta\\cdot\\mathbb{E}_{\\pi\\sim\\mathcal{P}}\\bigg[\\displaystyle\\sum_{X_{-\\cdot\\,s^{\\star}}}D_{\\chi^{2}}\\big(\\mu^{\\pi}(\\cdot\\,|\\,X_{-S^{\\star}})\\,\\|\\,\\mu^{\\pi}(\\cdot)\\big)\\cdot\\mu^{\\pi}(X_{-S^{\\star}})^{2}\\bigg]}\\\\ &{\\qquad\\qquad\\qquad\\leq\\delta\\cdot\\mathbb{E}_{\\pi\\sim\\mathcal{P}}\\bigg[\\displaystyle\\sum_{X_{-\\cdot\\,s^{\\star}}}D_{\\chi^{2}}\\big(\\mu^{\\pi}(\\cdot\\,|\\,X_{-S^{\\star}})\\,\\|\\,\\mu^{\\pi}(\\cdot)\\big)\\cdot\\mu^{\\pi}(X_{-S^{\\star}})^{2}\\bigg]\\cdot e^{a}}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "by the construction of $\\delta$ . Combining all the above results, we have the dynamics of $a$ as ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\partial_{t}a=\\left(1\\pm(\\gamma^{-2}+4)\\delta\\right)\\cdot\\mathbb{E}_{\\pi\\sim\\mathcal{P}}\\bigg[\\sum_{X_{-\\mathcal{S}^{\\star}}}D_{\\chi^{2}}(\\mu^{\\pi}(\\cdot\\,|\\,X_{-\\mathcal{S}^{\\star}})\\,\\|\\,\\mu^{\\pi}(\\cdot))\\cdot\\mu^{\\pi}(X_{-\\mathcal{S}^{\\star}})^{2}\\bigg]\\cdot e^{a}.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "A simple reformulation gives ", "page_idx": 51}, {"type": "equation", "text": "$$\n-\\partial_{t}e^{-a}=\\left(1\\pm(\\gamma^{-2}+4)\\delta\\right)\\cdot\\mathbb{E}_{\\pi\\sim\\mathcal{P}}\\bigg[\\sum_{X_{-\\mathcal{S}^{\\star}}}D_{\\chi^{2}}(\\mu^{\\pi}(\\cdot\\,|\\,X_{-\\mathcal{S}^{\\star}})\\,\\|\\,\\mu^{\\pi}(\\cdot))\\cdot\\mu^{\\pi}(X_{-\\mathcal{S}^{\\star}})^{2}\\bigg],\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "which implies that for small $a$ , the growth follows ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{s}(t)\\leq-\\log\\bigg(e^{-a(0)}-\\bigl(1+(\\gamma^{-2}+4)\\delta\\bigr)\\cdot\\mathbb{E}_{\\pi\\sim\\mathcal P}\\bigg[\\displaystyle\\sum_{X_{-\\delta^{\\star}}}D_{\\chi^{2}}(\\mu^{\\pi}(\\cdot\\,|\\,X_{-\\delta^{\\star}})\\,\\|\\,\\mu^{\\pi}(\\cdot))\\cdot\\mu^{\\pi}(X_{-\\delta^{\\star}})^{2}\\bigg]\\,.}\\\\ &{\\mathfrak{a}(t)\\geq-\\log\\bigg(e^{-a(0)}-\\bigl(1-(\\gamma^{-2}+4)\\delta\\bigr)\\cdot\\mathbb{E}_{\\pi\\sim\\mathcal P}\\bigg[\\displaystyle\\sum_{X_{-\\delta^{\\star}}}D_{\\chi^{2}}(\\mu^{\\pi}(\\cdot\\,|\\,X_{-\\delta^{\\star}})\\,\\|\\,\\mu^{\\pi}(\\cdot))\\mu^{\\pi}(X_{-\\delta^{\\star}})^{2}\\bigg]\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Therefore, in the beginning, $a(t)$ grows super exponentially fast. ", "page_idx": 51}, {"type": "text", "text": "Large $a$ :As $a$ grows large such that $\\mu^{\\pi}(X_{-{\\cal S}^{\\star}})(e^{a}-1)\\geq\\delta^{-1}$ for all $X_{-S^{\\star}}$ and $\\pi\\in\\operatorname{supp}(\\mathcal{P})$ we conclude that $\\bar{0}<r^{\\pi}(\\bar{X}_{-{\\cal S}^{\\star}};a)\\leq r_{+}({\\dot{a}})\\leq\\delta$ and ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\frac{r^{\\pi}(X_{-\\mathcal{S}^{\\star}};a)^{3}}{\\left(\\mu^{\\pi}(X_{-\\mathcal{S}^{\\star}})e^{a}\\right)^{-3}}=\\frac{(\\mu^{\\pi}(X_{-\\mathcal{S}^{\\star}})e^{a})^{3}}{(1+\\mu^{\\pi}(X_{-\\mathcal{S}^{\\star}})(e^{a}-1))^{3}}=\\left(1-\\frac{1-\\mu^{\\pi}(X_{-\\mathcal{S}^{\\star}})}{1+\\mu^{\\pi}(X_{-\\mathcal{S}^{\\star}})(e^{a}-1)}\\right)^{3},\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "which imples that ", "page_idx": 51}, {"type": "equation", "text": "$$\n1-3\\delta\\leq\\frac{r^{\\pi}(X_{-\\mathcal{S}^{\\star}};a)^{3}}{(\\mu^{\\pi}(X_{-\\mathcal{S}^{\\star}})e^{a})^{-3}}\\leq1.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "For $J(X_{-\\cal S^{\\star}};a,\\pi)$ , we combine the condition that $\\mu^{\\pi}(X_{-\\mathcal{S}^{\\star}})\\cdot(e^{a}-1)\\geq\\delta^{-1}$ with (E.33) to obtain that ", "page_idx": 51}, {"type": "equation", "text": "$$\nJ(X_{-}.s\\cdot;a,\\pi)=(1\\pm\\gamma^{-2}\\delta)\\cdot D_{\\chi^{2}}(\\mu^{\\pi}(\\cdot)\\parallel\\mu^{\\pi}(\\cdot\\mid X_{-}s\\cdot)),\\quad\\mathrm{where}\\quad\\gamma^{-2}\\delta\\le1/4.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Combining the above two results with Proposition E.4, we have ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f_{5}=\\mathbb{E}_{\\pi,X_{-S^{\\star}}\\sim\\mu^{\\pi}}\\left[J(X_{-S^{\\star}};a,\\pi)\\cdot e^{a}\\cdot\\left(r^{\\pi}(X_{-S^{\\star}})\\right)^{3}\\cdot\\mu^{\\pi}(X_{-S^{\\star}})\\right]}\\\\ &{\\quad=\\left(1\\pm(\\gamma^{-2}+3)\\delta\\right)\\cdot\\mathbb{E}_{\\pi\\sim\\mathcal{P}}\\Bigg[\\sum_{X_{-S^{\\star}}}D_{\\chi^{2}}(\\mu^{\\pi}(\\cdot)\\left\\lVert\\,\\mu^{\\pi}(\\cdot\\,\\big\\vert\\,X_{-S^{\\star}}))\\cdot\\frac{e^{-2a}}{\\mu^{\\pi}(X_{-S^{\\star}})}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "For the noise term $\\xi+\\psi(a)$ , we have ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\delta\\cdot\\mathbb{E}_{\\pi\\sim\\mathcal{P}}\\bigg[\\sum_{X_{-}\\:s^{\\star}}D_{\\chi^{2}}(\\mu^{\\pi}(\\cdot)\\,\\|\\,\\mu^{\\pi}(\\cdot\\,|\\,X_{-\\mathcal{S}^{\\star}}))\\cdot\\frac{e^{-2a}}{\\mu^{\\pi}(X_{-\\mathcal{S}^{\\star}})}\\bigg]\\geq\\xi+\\psi(a),\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "which can be verified by the condition on $\\delta$ as well as the fact that we are only considering $a\\leq$ $(\\log L)/8$ . We thus have for the gradient that ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\partial_{t}a=(1\\pm(\\gamma^{-2}+4)\\delta)\\cdot\\mathbb{E}_{\\pi\\sim\\mathcal{P}}\\Bigg[\\sum_{X_{-\\mathcal{S}^{\\star}}}D_{\\chi^{2}}(\\mu^{\\pi}(\\cdot)\\,\\|\\,\\mu^{\\pi}(\\cdot\\,|\\,X_{-\\mathcal{S}^{\\star}}))\\cdot\\frac{e^{-2a}}{\\mu^{\\pi}(X_{-\\mathcal{S}^{\\star}})}\\Bigg].\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "By rearranging the terms, we further have ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\partial_{t}e^{2a}=(1\\pm(\\gamma^{-2}+4)\\delta)\\cdot\\mathbb{E}_{\\pi\\sim\\mathcal{P}}\\bigg[\\sum_{X_{-\\mathcal{S}^{\\star}}}\\frac{D_{\\chi^{2}}(\\mu^{\\pi}(\\cdot)\\,\\|\\,\\mu^{\\pi}(\\cdot\\,|\\,X_{-\\mathcal{S}^{\\star}}))\\cdot}{2\\mu^{\\pi}(X_{-\\mathcal{S}^{\\star}})}\\bigg].\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Suppose this large $a$ regime starts at $t_{0}$ with value $a(t_{0})$ . Thus, for large $a$ , the growth rate is characterized by ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\iota(t)=\\frac{1}{2}\\log\\Bigg((1\\pm(\\gamma^{-2}+4)\\delta)\\cdot\\mathbb{E}_{\\pi\\sim\\mathcal{P}}\\Bigg[\\sum_{X_{-\\mathcal{S}^{\\star}}}\\frac{D_{\\chi^{2}}(\\mu^{\\pi}(\\cdot)\\,\\|\\,\\mu^{\\pi}(\\cdot\\,|\\,X_{-\\mathcal{S}^{\\star}}))}{2\\mu^{\\pi}(X_{-\\mathcal{S}^{\\star}})}\\Bigg]\\cdot(t-t_{0})+e^{2a(t_{0})}\\Bigg),\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "which is logarithmically fast. This step ends until $a$ reaches the value $(\\log L)/8$ . This concludes the proof. \u53e3 ", "page_idx": 52}, {"type": "text", "text": "E.4.1  Additional Proofs for Stage IMI ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "We conclude the proof of Stage IMl by providing the proof of Proposition E.4 and Proposition E.5. ", "page_idx": 52}, {"type": "text", "text": "Proof ofProposition $E.4$ In this paragraph, we aim to gain more insight in $f_{5}$ . By the definition of $f_{5}$ in (E.28), we can rewrite $f_{5}$ as ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{F}=\\mathbb{E}_{\\pi,X_{-}s^{\\star}\\sim\\mu^{\\pi}}\\bigg[\\bigg(\\displaystyle\\sum_{k=1}^{d}\\frac{\\mu^{\\pi}(x=e_{k}\\,|\\,X_{-}s^{\\star})^{2}}{\\widetilde{\\mu}^{\\pi}(z=e_{k}\\,|\\,X_{-}s^{\\star})}-1\\bigg)\\cdot\\widetilde{\\mu}^{\\pi}(Z_{-}s^{\\star}=X_{-}s\\,|\\,X_{-}s^{\\star})\\bigg]}\\\\ &{\\ \\ \\ =\\mathbb{E}_{\\pi,X_{-}s^{\\star}\\sim\\mu^{\\pi}}\\bigg[\\displaystyle\\sum_{k=1}^{d}\\bigg(\\frac{\\mu^{\\pi}(x=e_{k}\\,|\\,X_{-}s^{\\star})}{\\widetilde{\\mu}^{\\pi}(z=e_{k}\\,|\\,X_{-}s^{\\star})}-1\\bigg)^{2}\\cdot\\widetilde{\\mu}^{\\pi}(z=e_{k}\\,|\\,X_{-}s^{\\star})\\cdot\\widetilde{\\mu}^{\\pi}(Z_{-}s^{\\star}=X_{-}s\\,|\\,X_{-}s^{\\star})}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "where in the last step, we use the simple fact ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\sum_{x}{\\frac{p(X=x\\,|\\,Y)^{2}}{q(X=x\\,|\\,Y)}}-1=\\sum_{x}\\left({\\frac{p(X=x\\,|\\,Y)}{q(X=x\\,|\\,Y)}}-1\\right)^{2}\\cdot q(X=x\\,|\\,Y).\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "In the definition of $f_{5}$ , the key quantity we aim to understand is the reweighted distribution $\\tilde{\\mu}^{\\pi}(z,Z\\,|\\,X_{-\\mathcal{S}^{\\star}})$ . For the readers\u2019 convenience, we copy the definition of the reweighted distribution here: ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\widetilde{\\mu}^{\\pi}(z,Z\\,|\\,X_{-\\mathcal{S}^{\\star}})=\\frac{\\mu^{\\pi}(z,Z)\\exp\\left(a\\prod_{h\\in\\mathcal{S}^{\\star}}\\mathbb{1}(z_{-h}=x_{-h})\\right)}{\\sum_{z^{\\prime},Z^{\\prime}}\\mu^{\\pi}(z^{\\prime},Z^{\\prime})\\exp\\left(a\\prod_{h\\in\\mathcal{S}^{\\star}}\\mathbb{1}(z_{-h}^{\\prime}=x_{-h})\\right)},\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "A key observation is that the reweighting only depends on the value of $Z_{-S^{\\star}}$ .Let $\\bar{S}^{\\star}=[M]\\backslash S^{\\star}$ and denoteby $Z_{-\\bar{S}^{\\star}}=(z_{-h})_{h\\in\\bar{S}^{\\star}}.$ Following the above observation, we can additionally condition on $Z_{-S^{\\star}}$ and conclude that ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{\\mu}^{\\pi}(z,Z_{-\\bar{S}^{\\star}}\\mid Z_{-S^{\\star}},X_{-S^{\\star}})=\\frac{\\widetilde{\\mu}^{\\pi}(z,Z_{-\\bar{S}^{\\star}},Z_{-S^{\\star}}\\mid X_{-S^{\\star}})}{\\sum_{z^{\\prime},Z_{-S^{\\star}}^{\\prime}}\\widetilde{\\mu}^{\\pi}(z^{\\prime},Z_{-\\bar{S}^{\\star}}^{\\prime},Z_{-S^{\\star}}\\mid X_{-S^{\\star}})}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\frac{\\mu^{\\pi}(z,Z_{-\\bar{S}^{\\star}},Z_{-S^{\\star}})\\exp\\big(a\\prod_{h\\in S^{\\star}}\\mathbb{I}(z_{-h}=x_{-h})\\big)}{\\sum_{z^{\\prime},Z_{-S^{\\star}}^{\\prime}}\\mu^{\\pi}(z^{\\prime},Z_{-\\bar{S}^{\\star}}^{\\prime},Z_{-S^{\\star}})\\exp\\big(a\\prod_{h\\in S^{\\star}}\\mathbb{I}(z_{-h}=x_{-h})\\big)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\frac{\\mu^{\\pi}(z,Z_{-\\bar{S}^{\\star}},Z_{-S^{\\star}})}{\\mu^{\\pi}(Z_{-S^{\\star}})}=\\mu^{\\pi}(z,Z_{-\\bar{S}^{\\star}}\\mid Z_{-S^{\\star}}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "as when fixing $Z_{-S^{\\star}}$ , the exponential reweighting terms cancel out in the numerator and denominator in the definition (E.34). Using the above identity, we are able to expand $\\tilde{\\mu}^{\\pi}(z\\,|\\,X_{-S^{\\star}})$ as ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{\\mu}^{\\pi}(z\\,|\\,X_{-}s^{\\star})=\\displaystyle\\sum_{Z_{-}s^{\\star}}\\mu^{\\pi}(z\\,|\\,Z_{-}s^{\\star})\\cdot\\widetilde{\\mu}^{\\pi}(Z_{-}s^{\\star}\\,|\\,X_{-}s^{\\star})}\\\\ &{\\qquad\\qquad=\\displaystyle\\sum_{Z_{-}s^{\\star}}\\mu^{\\pi}(z\\,|\\,Z_{-}s^{\\star})\\cdot\\frac{\\mu^{\\pi}(Z_{-}s^{\\star})+\\mu^{\\pi}(X_{-}s^{\\star})(e^{a}-1)\\cdot\\mathbb{1}(Z_{-}s^{\\star}=X_{-}s^{\\star})}{1+\\mu^{\\pi}(X_{-}s^{\\star})(e^{a}-1)}}\\\\ &{\\qquad\\qquad=\\frac{\\mu^{\\pi}(z)+\\mu^{\\pi}(x=z\\,|\\,X_{-}s^{\\star})\\cdot\\mu^{\\pi}(X_{-}s^{\\star})\\cdot(e^{a}-1)}{1+\\mu^{\\pi}(X_{-}s^{\\star})\\cdot(e^{a}-1)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "where the second equality follows from the fact that the reweighing term in $\\widetilde{\\mu}^{\\pi}$ lifts the likelihood of $Z_{-S^{\\star}}=X_{-S^{\\star}}$ by a factor of $e^{a}$ relative to the base distribution $\\bar{\\mu^{\\pi}}(Z_{-}s^{\\star})$ , and the denominator is just the normalization constant. In the sequel, we let $r^{\\pi}(X_{-S^{\\star}};a)=(1+\\mu^{\\pi}(X_{-S^{\\star}})\\cdot(e^{a}-1))^{-1}$ be the inverse of the normalization constant. We then have ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\widetilde{\\mu}^{\\pi}(z\\,|\\,X_{-}s\\star)=r^{\\pi}(X_{-}s\\star;a)\\cdot\\mu^{\\pi}(z)+(1-r^{\\pi}(X_{-}s\\star;a))\\cdot\\mu^{\\pi}(x=z\\,|\\,X_{-}s\\star).\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "On the other hand, by definition of $\\widetilde{\\mu}^{\\pi}$ in (E.34), we directly have ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{\\mu}^{\\pi}(Z_{-}s^{\\star}=X_{-}s^{\\star}\\,|\\,X_{-}s^{\\star})=\\frac{\\mu^{\\pi}(X_{-}s^{\\star})e^{a}}{\\sum_{Z_{-}^{\\prime}{s^{\\star}}}\\mu^{\\pi}(Z_{-}^{\\prime}{s^{\\star}})\\exp\\big(a\\prod_{h\\in\\mathcal{S}^{\\star}}\\mathbb{1}(z_{-h}^{\\prime}=x_{-h})\\big)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=e^{a}r^{\\pi}(X_{-}s^{\\star};a)\\cdot\\mu^{\\pi}(X_{-}s^{\\star}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Combining both (E.35) and (E.36) we have for $f_{5}$ that ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\dot{v}=\\mathbb{E}_{\\pi,X,-s^{\\star}\\sim\\mu^{\\pi}}\\Bigg[\\underset{k\\in[d]}{\\sum}\\Bigg(\\frac{\\mu^{\\pi}(x=e_{k}\\mid X-s^{\\star})}{r^{\\pi}(x-s^{\\star})+\\mu^{\\pi}(x=e_{k})+(1-r^{\\pi}(X-s^{\\star}))}\\cdot\\mu^{\\pi}(x=e_{k}\\mid X_{-s^{\\star}})-\\Bigg)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\cdot\\tilde{\\mu}^{\\pi}(z=e_{k}\\mid X_{-s^{\\star}})\\cdot\\tilde{\\mu}^{\\pi}(Z_{-s^{\\star}}=X_{-s^{\\star}}\\mid X_{-s^{\\star}})\\Bigg]}\\\\ {=\\mathbb{E}_{\\pi,X,-s^{\\star}\\sim\\mu^{\\pi}}\\Bigg[\\underset{k\\in[d]}{\\sum}\\Bigg(\\frac{\\mu^{\\pi}(x=e_{k}\\mid X_{-s^{\\star}})-\\mu^{\\pi}(x=e_{k})}{r^{\\pi}(X-s^{\\star})\\cdot\\mu^{\\pi}(x=e_{k})+(1-r^{\\pi}(X_{-s^{\\star}})\\cdot\\mu)^{\\pi}(x=e_{k}\\mid X_{-s^{\\star}})}\\Bigg)^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\quad\\cdot\\tilde{\\mu}^{\\pi}(z=e_{k}\\mid X_{-s^{\\star}})\\cdot e^{\\pi}\\tau(X_{-s^{\\star}};a)^{3}\\cdot\\mu^{\\pi}(X_{-s^{\\star}})\\Bigg]}\\\\ {=\\mathbb{E}_{\\pi,X,-s^{\\star}\\sim\\mu^{\\pi}}\\Bigg[\\underset{k\\in[d]}{\\sum}\\frac{(\\mu^{\\pi}(x=e_{k}\\mid X_{-s^{\\star}})-\\mu^{\\pi}(x=e_{k}))^{2}}{\\tilde{\\mu}^{\\pi}(z=e_{k}\\mid X_{-s^{\\star}})}\\cdot e^{\\pi\\tau(X_{-s^{\\star}};a)^{3}\\cdot\\mu^{\\pi}(X_{-s^{\\star}})}\\Bigg].}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Here, we note that $J(\\cdot;a,\\pi)$ is a function depending on both $a$ and $\\pi$ , and can be expanded as ", "page_idx": 53}, {"type": "equation", "text": "$$\nJ(X_{-}s\\cdot;a,\\pi)=\\sum_{k\\in[d]}{\\frac{(\\mu^{\\pi}(x=e_{k}\\,|\\,X_{-}s\\cdot)-\\mu^{\\pi}(x=e_{k}))^{2}}{(1-r^{\\pi}(X_{-}s\\cdot;a))\\mu^{\\pi}(x=e_{k}\\,|\\,X_{-}s\\cdot)+r^{\\pi}(X_{-}s\\cdot;a)\\mu^{\\pi}(x=e_{k})}}.\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Hence, we complete the proof of Proposition E.4. ", "page_idx": 53}, {"type": "text", "text": "Proof of Proposition E.5. Also, note that $\\tilde{J}(X_{-S^{\\star}};r,\\pi)$ is convex in $r$ , as by taking the derivative of $\\tilde{J}(X_{-S^{\\star}};r,\\pi)$ with respect to $r$ , we have ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{\\partial\\tilde{J}(X_{-S^{\\star}};r,\\pi)}{\\partial r}=\\sum_{k\\in[d]}\\frac{\\left(\\mu^{\\pi}(x=e_{k}\\,|\\,X_{-S^{\\star}})-\\mu^{\\pi}(e_{k})\\right)^{3}}{\\left((1-r)\\mu^{\\pi}(x=e_{k}\\,|\\,X_{-S^{\\star}})+r\\mu^{\\pi}(e_{k})\\right)^{2}},}\\\\ &{\\displaystyle\\frac{\\partial^{2}\\tilde{J}(X_{-S^{\\star}};r,\\pi)}{\\partial r^{2}}=2\\cdot\\sum_{k\\in[d]}\\frac{\\left(\\mu^{\\pi}(x=e_{k}\\,|\\,X_{-S^{\\star}})-\\mu^{\\pi}(e_{k})\\right)^{4}}{\\left((1-r)\\mu^{\\pi}(x=e_{k}\\,|\\,X_{-S^{\\star}})+r\\mu^{\\pi}(e_{k})\\right)^{3}}\\geq0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Hence, a naive upper bound for $\\tilde{J}(X_{-S^{\\star}};r,\\pi)$ .s ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{J}(X_{-\\mathcal{S}^{\\star}};r,\\pi)\\leq\\operatorname*{max}\\{\\widetilde{J}(X_{-\\mathcal{S}^{\\star}};0,\\pi),\\widetilde{J}(X_{-\\mathcal{S}^{\\star}};1,\\pi)\\}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\operatorname*{max}\\left\\{D_{\\chi^{2}}(\\mu^{\\pi}(\\cdot)\\parallel\\mu^{\\pi}(\\cdot\\mid X_{-\\mathcal{S}^{\\star}})),D_{\\chi^{2}}(\\mu^{\\pi}(\\cdot\\mid X_{-\\mathcal{S}^{\\star}})\\parallel\\mu^{\\pi}(\\cdot))\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "where we remind the readers that $\\begin{array}{r}{D_{\\chi^{2}}(\\mu\\,\\|\\,\\nu)\\,=\\,\\sum_{x}\\,(\\mu(x)-\\nu(x))^{2}/\\nu(x)}\\end{array}$ . Next, we show that $\\tilde{J}(X_{-S^{\\star}};r,\\pi)$ is Lipschitz continuous in $r$ ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\frac{\\partial\\widetilde{J}(X_{-}s\\cdot;r,\\pi)}{\\partial r}\\Big|=\\bigg|\\sum_{k\\in[d]}\\frac{(\\mu^{\\pi}(x=e_{k}\\,|\\,X_{-}s\\cdot)-\\mu^{\\pi}(e_{k}))^{3}}{((1-r)\\mu^{\\pi}(x=e_{k}\\,|\\,X_{-}s\\cdot)+r\\mu^{\\pi}(e_{k}))^{2}}\\bigg|}\\\\ &{\\le\\displaystyle\\sum_{k\\in[d]}\\frac{(\\mu^{\\pi}(x=e_{k}\\,|\\,X_{-}s\\cdot)-\\mu^{\\pi}(e_{k}))^{2}}{(1-r)\\mu^{\\pi}(x=e_{k}\\,|\\,X_{-}s\\cdot)+r\\mu^{\\pi}(e_{k})}\\cdot\\bigg|\\frac{\\mu^{\\pi}(x=e_{k}\\,|\\,X_{-}s\\cdot)-\\mu^{\\pi}(e_{k})}{(1-r)\\mu^{\\pi}(x=e_{k}\\,|\\,X_{-}s\\cdot)+r\\mu^{\\pi}(e_{k})}}\\\\ &{\\le\\widetilde{J}(X_{-}s\\cdot;r,\\pi)\\cdot\\operatorname*{max}\\bigg\\{\\frac{\\mu^{\\pi}(x=e_{k}\\,|\\,X_{-}s\\cdot)}{\\mu^{\\pi}(e_{k})},\\frac{\\mu^{\\pi}(e_{k})}{\\mu^{\\pi}(x=e_{k}\\,|\\,X_{-}s\\cdot)}\\bigg\\}}\\\\ &{\\le\\gamma^{-1}\\cdot\\operatorname*{max}\\big\\{D_{\\chi^{2}}(\\mu^{\\pi}(\\cdot)\\,\\|\\,\\mu^{\\pi}(\\cdot\\,|\\,X_{-}s\\cdot)),D_{\\chi^{2}}(\\mu^{\\pi}(\\cdot\\,|\\,X_{-}s\\cdot)\\,\\|\\,\\mu^{\\pi}(\\cdot))\\big\\}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "where we use both the upper bound for $\\tilde{J}(X_{-S^{\\star}};r,\\pi)$ and the lower bound for the transition kernel that both $\\mu^{\\pi}(\\cdot\\,|\\,X_{-\\mathcal{S}^{\\star}})$ and $\\mu^{\\pi}(\\cdot)$ are bounded between $\\gamma$ and 1. \u53e3 ", "page_idx": 54}, {"type": "text", "text": "E.5  Lemma on GIH Approximation Error ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Now given the convergence result for the training dynamics, the natural question to ask is how well the learned model implements the GIH mechanism. In the following part of this section, we state the lemma on the approximation error and also present a formal proof of the lemma. ", "page_idx": 54}, {"type": "text", "text": "Lemma E.6. Suppose Assumption 3.5 holds and consider training a transformer model $\\mathrm{TF}(M,H,d,D)$ With $H=M$ Let ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\Delta_{1}:=1-p_{S^{\\star}}(t_{1}),\\quad\\Delta_{2}:=1-\\prod_{h\\in S^{\\star}}(\\sigma_{-h}^{(h)}(t_{2}))^{2},\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "where $t_{1}$ and $t_{2}$ are the ending timefor thefirst two stages of the training,respectively.Suppose the error $\\Delta_{1},\\Delta_{2}=O(L^{-1})$ after the first two stages\u2019 training, and $a=\\Theta(\\operatorname{log}{\\bar{L}})$ after the last stage's training. Let y be the output of the model in (2.5) after the training and $y^{\\star}$ betheoutput of theGIH mechanism $\\mathsf{G I H}(x_{1:L};M,D)$ defined in Definition 3.2. Then for any $\\pi\\in\\operatorname{supp}(\\mathcal{P})$ andwithhigh probability $1-O(L^{-1})$ , it holds that ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\|y^{\\star}-y\\|_{1}=O(L^{-a/\\log L}).\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "Proof of Lemma E.6. Let $\\begin{array}{r}{s_{l}^{\\star}\\ =\\ \\prod_{h\\in{\\cal S}^{\\star}}\\mathbb{1}(x_{l-h}\\ =\\ x_{L+1-h})}\\end{array}$ and $s_{l}~=~\\langle u_{L+1},u_{l}\\rangle$ . Invoking Lemma F.1, the model misspecification error is bounded by ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{M<l\\leq L}\\left|s_{l}^{\\star}-s_{l}\\right|\\leq(\\Delta_{1}+\\Delta_{2})\\!:=\\!\\Delta.\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "We note that the second layer's attention weight $a$ can be as large as $(\\log L)/8$ . We are comparing the output of the model with the GIH mechanism $\\mathsf{G I H}(x_{1:L};M,D)$ .Let $\\begin{array}{r}{\\bar{N}=\\overleftarrow{\\sum}_{l>M}\\prod_{h\\in S^{\\star}}\\bar{\\mathbb{1}}(x_{l-h}=}\\end{array}$ $x_{L+1-h})$ . The output of this GIH mechanism is given by ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{r}{y^{\\star}:=\\left\\{\\!\\!\\begin{array}{l l}{N^{-1}\\cdot\\sum_{l=M+1}^{L}x_{l}\\cdot\\prod_{h\\in S^{\\star}}\\mathbb{1}\\!\\!\\left(x_{l-h}=x_{L+1-h}\\right)\\!,\\quad\\mathrm{if}\\quad N\\geq1,}\\\\ {(L-M)^{-1}\\cdot\\sum_{l=M+1}^{L}x_{l},\\quad\\mathrm{otherwise}.}\\end{array}\\!\\!\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "We define ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\sigma_{l}^{\\star}=\\binom{N^{-1}\\cdot\\prod_{h\\in\\mathcal{S}^{\\star}}\\mathbb{1}\\big(x_{l-h}=x_{L+1-h}\\big),\\quad\\mathrm{if}\\quad N\\geq1,}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "with $\\sigma^{\\star}=(\\sigma_{l}^{\\star})\\iota_{>M}$ Since $\\|\\boldsymbol{x}_{l}\\|_{1}=1$ , the $_{\\ell-1}$ norm of the difference between $y^{\\star}$ and the model's actual output is given by ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\|y^{\\star}-y\\|_{1}\\leq\\|\\sigma^{\\star}-\\sigma\\|_{1}\\,.\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "Let us define the set $\\begin{array}{r}{\\Gamma=\\{L\\geq l>M:\\prod_{h\\in S^{\\star}}\\mathbb{1}(x_{l-h}=x_{L+1-h})=1\\}}\\end{array}$ and $\\bar{\\Gamma}=\\{L\\geq l>M:$ $\\begin{array}{r}{\\prod_{h\\in\\mathcal{S}^{\\star}}\\mathbb{1}(x_{l-h}=x_{L+1-h})=0\\}}\\end{array}$ . Using (E.37),for $l\\in\\Gamma$ , we have $1\\geq s_{l}\\geq s_{l}^{\\star}-\\Delta=1-\\Delta$ and for $l\\in\\bar{\\Gamma}$ , we have $0\\leq s_{l}\\leq s_{l}^{\\star}+\\Delta=\\Delta$ . Consider the normalization factor in the softmax function. ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\mathcal{Z}:=\\sum_{l=M+1}^{L}\\exp(a\\cdot s_{l}).\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "By the split of the set $\\Gamma$ and F and noting that $|\\Gamma|=N$ , the normalization factor is lower and upper bounded by ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{Z}\\geq N\\exp(a\\cdot(1-\\Delta))+(L-M-N)\\cdot=:\\mathcal{Z}_{-},}\\\\ &{\\mathcal{Z}\\leq N\\exp(a)+(L-M-N)\\cdot\\exp(a\\cdot\\Delta)=:\\mathcal{Z}_{+}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Let us consider the event $N\\geq1$ in the following. We then have for $l\\in\\Gamma$ that ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sigma_{l}^{\\star}-\\sigma_{l}|=\\left|\\frac{\\exp(a\\cdot s_{l})}{\\mathcal{Z}}-\\frac{1}{N}\\right|\\leq\\left|\\frac{\\exp(a)}{\\mathcal{Z}_{-}}-\\frac{1}{N}\\right|\\bigvee\\left|\\frac{\\exp(a\\cdot(1-\\Delta))}{\\mathcal{Z}_{+}}-\\frac{1}{N}\\right|}\\\\ &{\\qquad\\leq\\left|\\frac{1}{N\\exp(-a\\Delta)+(L-M-N)\\cdot\\exp(-a)}-\\frac{1}{N}\\right|}\\\\ &{\\qquad\\qquad\\ V\\left|\\frac{\\exp(-2a\\Delta)}{N\\exp(-a\\Delta)+(L-M-N)\\exp(-a)}-\\frac{1}{N}\\right|}\\\\ &{\\qquad\\leq\\frac{N\\cdot(1-\\exp(-a\\Delta))+(L-M-N)\\cdot\\exp(-a)}{(N\\exp(-a\\Delta)+(L-M-N)\\cdot\\exp(-a))\\cdot N}\\leq\\frac{1-\\exp(-a\\Delta)}{N\\exp(-a\\Delta)}+\\frac{L\\cdot\\exp(-a)}{N^{2}\\exp(-a\\Delta)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Note that $a\\Delta=o(1)$ due to the assumption that $\\Delta=O(L^{-1})$ and $a=o(L)$ . The right hand side is upper bounded by $O(a\\Delta/N)+O(L\\exp(-a)/N^{2})$ . For $l\\in\\bar{\\Gamma}$ , we have ", "page_idx": 55}, {"type": "equation", "text": "$$\n|\\sigma_{l}^{\\star}-\\sigma_{l}|=\\sigma_{l}\\le\\frac{\\exp(a\\Delta)}{\\mathcal{Z}_{-}}\\le\\frac{\\exp(a\\cdot(2\\Delta-1))}{N}=O\\left(\\frac{\\exp(-a)}{N}\\right).\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "In summary, ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|y^{\\star}-y\\|_{1}\\leq\\|\\sigma^{\\star}-\\sigma\\|_{1}\\leq\\displaystyle\\sum_{l\\in\\Gamma}|\\sigma_{l}^{\\star}-\\sigma_{l}|+\\displaystyle\\sum_{l\\in\\bar{\\Gamma}}\\sigma_{l}}\\\\ &{\\qquad\\qquad\\leq N\\cdot O\\left(\\displaystyle\\frac{a\\Delta N+L\\exp(-a)}{N^{2}}\\right)+L\\cdot O\\left(\\displaystyle\\frac{\\exp(-a)}{N}\\right)\\leq O\\left(a\\Delta+\\displaystyle\\frac{L\\exp(-a)}{N}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "The above inequality holds whenever $N\\geq1$ . Now we aim to upper bound the probability that $N=0$ Note that $\\begin{array}{r}{N=\\sum_{l=M+1}^{L}\\mathbb{1}(X_{l-S^{\\star}}=X_{L+1-S^{\\star}})}\\end{array}$ . We consider th fllowing second moment: ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathbb{\\xi}\\bigg[\\bigg((L-M)^{-1}\\sum_{l=M+1}^{L}\\mathbb{1}(X_{l-\\mathcal{S}^{\\star}}=E)-\\mu^{\\pi}(E)\\bigg)^{2}\\bigg]\\leq D_{\\mathcal{X}^{2}}\\bigg((L-M)^{-1}\\displaystyle\\sum_{l=M+1}^{L}\\mathbb{1}(X_{l-\\mathcal{S}^{\\star}}=\\cdot)\\bigg\\|\\,\\mu^{\\star}(E)\\bigg)}}\\\\ &{\\lesssim\\displaystyle\\frac{M}{L(1-\\lambda)\\cdot\\gamma|^{\\mathcal{S}^{\\star}}|/2},\\quad\\forall E\\in\\mathcal{X}^{|\\mathcal{S}^{\\star}|},}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "where the first inequality holds by noting that $\\begin{array}{r}{D_{\\chi^{2}}(\\mu\\left\\Vert\\nu\\right)=\\sum_{x}(\\mu(x)-\\nu(x))^{2}/\\nu(x)}\\end{array}$ and the last inequality holds by Lemma F.18. Therefore, by the Chebyshev's inequality, we have ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\mathbb{P}\\bigg(\\left|L^{-1}\\sum_{l=1}^{L}\\mathbb{1}(X_{l-\\mathcal{S}^{\\star}}=E)-\\mu^{\\pi}(E)\\right|\\geq t\\bigg)\\leq\\frac{1}{L(1-\\lambda)\\cdot\\gamma^{|\\mathcal{S}^{\\star}|}\\cdot t^{2}}.\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "We can take $t=\\operatorname*{min}_{E\\in\\mathcal{X}^{|s^{\\star}|}}\\mu^{\\pi}(E)/2$ and by also taking a union bound over $E\\in\\mathcal{X}^{|S^{\\star}|}$ (which gives a $d^{|S^{\\star}|}$ factor), we conclude that with high probability $\\tilde{O}(1-L^{-1})$ it holds that $N\\geq t L=$ $\\bar{L}\\cdot\\operatorname*{min}_{E\\in\\mathcal{X}^{|\\mathcal{S}^{\\star}|}}\\mu^{\\pi}(E)/2$ . Thus, it follows from (E.38) that with high probability ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\|y^{\\star}-y\\|_{1}\\leq O\\left(a\\Delta+\\frac{\\exp(-a)}{\\operatorname*{min}_{E\\in\\mathcal{X}^{|\\mathcal{S}^{\\star}|}}\\mu^{\\pi}(E)/2}\\right)=O\\left(L^{-1}\\log L+L^{-a/\\log L}\\right).\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Hence, we complete the proof of Lemma E.6. ", "page_idx": 55}, {"type": "text", "text": "F Auxiliary Lemmas and Their Proofs ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "In this appendix, we present the auxiliary lemmas used to derive the approximation of the gradient flow dynamics in the proof of Theorem 3.6, which is presented in the previous appendix. The proofs of these lemmas are presented right below their statements. ", "page_idx": 56}, {"type": "text", "text": "F.1  Useful Inequalities ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "The following lemma provides a bound on the model misspecification error, which is the difference between the model's output and the ideal output. ", "page_idx": 56}, {"type": "text", "text": "Lemma F.1 (Model Misspecification Error). Let $u_{L+1}$ be the output feature after the FFN & Normalizationlayer: Then,the model misspecification error defined as ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{l\\in[L]}\\left|\\langle u_{L+1},u_{l}\\rangle-\\prod_{h\\in S^{\\star}}\\mathbb{1}(x_{l-h}=x_{L+1-h})\\right|\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "is bounded by $\\Delta_{1}+\\Delta_{2}$ where $\\Delta_{1}$ and $\\Delta_{2}$ are the errors after the training of the first and second stages, respectively, and are defined respectively as ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\Delta_{1}:=1-p_{{\\cal S}^{\\star}},\\qquad\\Delta_{2}:=1-\\prod_{h\\in{\\cal S}^{\\star}}(\\sigma_{-h}^{(h)})^{2}.\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Proof of Lemma $F.l$ . By definition of the output feature $u_{l}$ after the FFN & Normalization layer: ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\left\\langle u_{L+1},u_{l}\\right\\rangle=\\sum_{\\boldsymbol{S}\\in[H]_{\\leq D}}p_{\\boldsymbol{S}}\\cdot\\prod_{h\\in\\boldsymbol{S}}\\langle v_{l}^{(h)},v_{L+1}^{(h)}\\rangle.\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "As each $v_{l}^{(h)}$ is a convex combination of $X_{\\mathcal{M}(l)}$ where $\\mathcal{M}(l)=\\{l-M,\\ldots,l-1\\}$ \uff0c $\\|v_{l}^{(h)}\\|_{2}\\leq1$ Thus, ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle u_{L+1},u_{l}\\rangle-\\displaystyle\\prod_{h\\in\\mathcal{S}^{\\star}}\\big\\langle v_{l}^{(h)},v_{L+1}^{(h)}\\big\\rangle\\bigg|=\\displaystyle\\bigg|\\sum_{S\\in[H]\\le D}p_{S}\\cdot\\displaystyle\\prod_{h\\in\\mathcal{S}}\\langle v_{l}^{(h)},v_{L+1}^{(h)}\\rangle-\\displaystyle\\prod_{h\\in\\mathcal{S}^{\\star}}\\langle v_{l}^{(h)},v_{L+1}^{(h)}\\rangle\\bigg|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\le\\Big|-(1-p_{S^{\\star}})\\displaystyle\\prod_{h\\in\\mathcal{S}^{\\star}}\\big\\langle v_{l}^{(h)},v_{L+1}^{(h)}\\big\\rangle+\\displaystyle\\sum_{S\\in[H]\\le D\\setminus\\{S^{\\star}\\}}p_{S}\\cdot\\displaystyle\\prod_{h\\in\\mathcal{S}}\\langle v_{l}^{(h)},v_{l}^{(h)}\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\le\\operatorname*{max}\\bigg\\{1-p_{S^{\\star}},\\displaystyle\\sum_{S\\in[H]\\le D\\setminus\\{S^{\\star}\\}}p_{S}\\bigg\\}=1-p_{S^{\\star}}=:\\Delta_{1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "$\\Delta_{1}$ $\\begin{array}{r}{v_{l}^{(h)}=\\sum_{j\\in M}\\sigma_{-j}^{(h)}x_{l-j}}\\end{array}$ w havc ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\prod_{h\\in\\mathcal{S}^{\\star}}\\langle v_{l}^{(h)},v_{L+1}^{(h)}\\rangle=\\displaystyle\\prod_{h\\in\\mathcal{S}^{\\star}}\\left(\\sum_{\\substack{i,j\\in[M]^{2}}}\\sigma_{-i}^{(h)}\\sigma_{-j}^{(h)}\\langle x_{l-i},x_{L+1-j}\\rangle\\right)}\\\\ &{=\\displaystyle\\sum_{\\{(i_{h},j_{h})\\}_{h\\in\\mathcal{S}^{\\star}}\\in[M]^{2|\\mathcal{S}^{\\star}|}}\\prod_{h\\in\\mathcal{S}^{\\star}}\\sigma_{-i_{h}}^{(h)}\\sigma_{-j_{h}}^{(h)}\\,\\mathbb{I}\\big(x_{l-i_{h}}=x_{L+1-j_{h}}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Here in the second equality, we exchange the order of summation and product. The last term of the second equality can be understood as follows. We first pick $\\left|{\\cal S}^{\\star}\\right|$ index pairs $\\{(i_{h},j_{h})\\}_{h\\in S^{\\star}}$ arbitrarily, with each in, jn E [H]. Then we evaluate the product IInesx 2n ) 1(x-in = 2L+1-n)given these indices. Then we sum over all possible values that $\\{(i_{h},j_{h})\\}_{h\\in S^{\\star}}$ can take. ", "page_idx": 56}, {"type": "text", "text": "The above equation implies that ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\prod_{h\\in\\mathcal{S}^{\\star}}\\langle v_{l}^{(h)},v_{L+1}^{(h)}\\rangle-\\displaystyle\\prod_{h\\in\\mathcal{S}^{\\star}}(\\sigma_{-h}^{(h)})^{2}\\,\\mathbb{1}\\big(x_{l-h}=x_{L+1-h}\\big)\\Bigg|}\\\\ &{\\quad=\\displaystyle\\bigg|_{\\{(i_{h},j_{h})\\}_{h\\in\\mathcal{S}^{\\star}}\\neq\\{(h,h)\\}_{h\\in\\mathcal{S}^{\\star}}}\\displaystyle\\prod_{h\\in\\mathcal{S}^{\\star}}\\sigma_{-i_{h}}^{(h)}\\sigma_{-j_{h}}^{(h)}\\,\\mathbb{1}\\big(x_{l-i_{h}}=x_{L+1-j_{h}}\\big)\\bigg|}\\\\ &{\\quad\\le\\displaystyle\\sum_{\\{(i_{h},j_{h})\\}_{h\\in\\mathcal{S}^{\\star}}\\neq\\{(h,h)\\}_{h\\in\\mathcal{S}^{\\star}}}\\displaystyle\\prod_{h\\in\\mathcal{S}^{\\star}}\\sigma_{-i_{h}}^{(h)}\\sigma_{-j_{h}}^{(h)}\\le1-\\displaystyle\\prod_{h\\in\\mathcal{S}^{\\star}}(\\sigma_{-h}^{(h)})^{2}\\!=:\\Delta_{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "where the last inequality follows from the fact that ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\sum_{\\substack{(i_{h},j_{h})_{h\\in S^{\\star}}}}\\prod_{h\\in S^{\\star}}\\sigma_{-i_{h}}^{(h)}\\sigma_{-j_{h}}^{(h)}=\\prod_{h\\in S^{\\star}}\\left(\\sum_{\\substack{i,j\\in[M]^{2}}}\\sigma_{-i}^{(h)}\\sigma_{-j}^{(h)}\\right)=\\prod_{h\\in S^{\\star}}\\left(\\sum_{i\\in[M]}\\sigma_{-i}^{(h)}\\right)^{2}=1.\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "Here the summation sign in the right-hand side of the second equality indicates that in the last line of (F. 1) we sum over all possible values that $\\{(i_{h},j_{h})\\}_{h\\in S^{*}}$ can take, except for the only case where $\\left(i_{h},j_{h}\\right)=\\left(h,h\\right)$ for all $h\\in[H]$ ", "page_idx": 57}, {"type": "text", "text": "In summary, by triangle inequality, we have shown that ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\left|\\langle u_{L+1},u_{l}\\rangle-\\prod_{h\\in S^{\\star}}\\mathbb{1}(x_{l-h}=x_{L+1-h})\\right|\\leq\\Delta_{1}+\\Delta_{2}.\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "The proof is completed. ", "page_idx": 57}, {"type": "text", "text": "Next, in Lemma F.2, we establish a uniform bound for the quantity involved in the gradient. ", "page_idx": 57}, {"type": "text", "text": "$\\begin{array}{r}{y(k)\\,=\\,\\sum_{l=M+1}^{L}\\sigma_{l}\\,\\mathbb{1}(x_{l}\\,=\\,e_{k})}\\end{array}$ $k\\,\\in\\,[d]$ $\\begin{array}{r}{\\sum_{l=M+1}^{L}\\sigma_{l}\\,=\\,1}\\end{array}$ $\\sigma_{l}~\\geq~0$ $l~\\in~[L]$ $\\varepsilon$ $C$ $C$ $f:$ $\\mathcal{X}^{L+1}\\to[-C,C].$ we have ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\left|\\sum_{l=M+1}^{L}\\sigma_{l}\\cdot\\sum_{k=1}^{d}\\biggr(\\frac{\\mathbb{1}(x_{L+1}=x_{l}=e_{k})}{y(k)+\\varepsilon}-\\frac{y(k)\\,\\mathbb{1}(x_{L+1}=e_{k})}{y(k)+\\varepsilon}\\biggr)\\cdot f(X)\\right|\\leq2C.\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "Proof of Lemma $F.2$ .By the triangular inequality, we have ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{l=M+1}^{L}\\sigma_{l}\\cdot\\underset{k=1}{\\overset{d}{\\sum}}\\bigg(\\frac{\\mathbb{1}(x_{L+1}=x_{l}=e_{k})}{y(k)+\\varepsilon}-\\frac{y(k)\\,\\mathbb{1}(x_{L+1}=e_{k})}{y(k)+\\varepsilon}\\bigg)\\cdot f(X)\\Bigg|}\\\\ &{\\quad\\le C\\cdot\\left|\\displaystyle\\sum_{k=1}^{d}\\sum_{l=M+1}^{L}\\sigma_{l}\\cdot\\mathbb{1}(x_{l}=e_{k})\\cdot\\frac{\\mathbb{1}(x_{L+1}=e_{k})}{y(k)+\\varepsilon}\\right|+C\\cdot\\left|\\displaystyle\\sum_{k=1}^{d}\\sum_{l=M+1}^{L}\\sigma_{l}\\cdot\\frac{y(k)\\cdot\\mathbb{1}(x_{L+1}=e_{k})}{y(k)+\\varepsilon}\\right|}\\\\ &{\\quad=2C\\cdot\\left|\\displaystyle\\sum_{k=1}^{d}\\frac{y(k)\\cdot\\mathbb{1}(x_{L+1}=e_{k})}{y(k)+\\varepsilon}\\right|\\le2C,}\\end{array}\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "Where in thequality weuse the defnition $\\begin{array}{r}{y(k)=\\sum_{l=M+1}^{L}\\sigma_{l}\\,\\mathbb{1}(x_{l}=e_{k})}\\end{array}$ and $\\begin{array}{r}{\\sum_{l=M+1}^{L}\\sigma_{l}=\\underline{{1}}}\\end{array}$ Now we conclude the proof of this lemma. \u53e3 ", "page_idx": 57}, {"type": "text", "text": "F.2  Approximation Errors for Dynamics Analysis ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Next, Lemma F.3 addresses the approximation error induced by $\\sigma_{l}\\approx1/L$ in the transformer model.   \nThe approximation error will be for $g_{0,s}$ 10 $g_{1,s}$ for Stage I and $g_{h,1}$ 10 $g_{h,2}$ for Stage II. ", "page_idx": 57}, {"type": "text", "text": "Lemma F.3. For the transformer model defined in (2.5) and any bounded function $f:\\mathcal{X}^{L+1}\\to\\mathbb{R}$ such that $\\mathrm{sup}_{x\\in{\\mathcal{X}}^{L}}$ $|f(x)|\\leq C$ for a constant $C>0,$ . define two quantities $A$ and $B$ as ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A:=\\displaystyle\\sum_{l=M+1}^{L}\\mathbb{E}_{X|\\pi}\\bigg[\\sigma_{l}(a s)\\cdot\\displaystyle\\sum_{k\\in[d]}\\left(\\frac{\\mathbb{I}(x_{L+1}=x_{l}=e_{k})}{y(k)+\\varepsilon}-\\frac{y(k)\\,\\mathbb{I}(x_{L+1}=e_{k})}{y(k)+\\varepsilon}\\right)\\cdot f(X)\\bigg]\\,,}\\\\ &{B:=\\displaystyle\\frac{1}{L-M}\\sum_{l=M+1}^{L}\\mathbb{E}_{X|\\pi}\\bigg[\\bigg(\\displaystyle\\sum_{k\\in[d]}\\frac{\\mathbb{I}(x_{L+1}=x_{l}=e_{k})}{\\bar{y}(k)+\\varepsilon}-\\frac{\\bar{y}(k)\\,\\mathbb{I}(x_{L+1}=e_{k})}{\\bar{y}(k)+\\varepsilon}\\bigg)\\cdot f(X)\\bigg]\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "where $s=u_{L+1}^{\\top}U_{1:L}^{\\top}$ and $\\begin{array}{r}{\\bar{y}=(L-M)^{-1}\\sum_{l=M+1}^{L}x_{l}}\\end{array}$ Then, for all $a\\in[0,1]$ and $\\varepsilon\\in(0,1]$ it holdsthat ", "page_idx": 57}, {"type": "equation", "text": "$$\n|A-B|\\leq{\\frac{8C a d}{\\varepsilon^{2}}}.\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "Proofof Lemma $F.3$ .By triangular inequality, we have ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\left|A-B\\right|\\leq\\sum_{l=M+1}^{L}{\\mathbb{E}\\left[\\sum_{k\\in[d]}\\left\\{\\left|\\sigma_{l}(a\\cdot s)-\\frac{1}{L-M}\\right|\\cdot\\left|\\frac{\\mathbb{I}\\left(x_{L+1}=x_{l}=e_{k}\\right)}{y(k)+\\varepsilon}\\right|\\right.\\right.}}\\\\ {\\displaystyle\\left.+\\frac{1}{L-M}\\right|\\frac{\\mathbb{I}\\left(x_{L+1}=x_{l}=e_{k}\\right)}{y(k)+\\varepsilon}-\\frac{\\mathbb{I}\\left(x_{L+1}=x_{l}=e_{k}\\right)}{\\bar{y}(k)+\\varepsilon}\\right|}\\\\ {\\displaystyle\\left.+\\left|\\sigma_{l}(a\\cdot s)-\\frac{1}{L-M}\\right|\\cdot\\left|\\frac{y(k)\\,\\mathbb{I}(x_{L+1}=e_{k})}{y(k)+\\varepsilon}\\right|,}\\\\ {\\displaystyle\\left.+\\frac{1}{L-M}\\right|\\frac{y(k)\\,\\mathbb{I}(x_{L+1}=e_{k})}{y(k)+\\varepsilon}-\\frac{\\bar{y}(k)\\,\\mathbb{I}(x_{L+1}=e_{k})}{\\bar{y}(k)+\\varepsilon}\\right|\\right\\}\\cdot f(X)\\Bigg|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "Note that $0\\,\\leq\\,s_{l}\\,\\leq\\,1$ for all $l\\,=\\,M+1,.\\,.\\,.\\,,L$ thanks to the layer normalization. Then, for the softmax operation, we have ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\frac{1}{1+(L-M-1)\\exp(a)}\\leq\\sigma_{l}(a\\cdot s)\\leq\\frac{\\exp(a)}{L-M-1+\\exp(a)},\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "which implies that ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sigma_{l}\\big(a\\cdot s\\big)-\\frac{1}{L-M}\\Bigg|\\le\\operatorname*{max}\\left\\{\\frac{1}{L-M}-\\frac{1}{1+(L-M-1)\\exp(a)},\\;\\frac{\\exp(a)}{L-M-1+\\exp(a)}-\\frac{1}{L-M-1}\\right\\}\\;,}\\\\ {\\displaystyle\\le\\frac{\\exp(a)-1}{L-M-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "Since indicator functions are bounded above by 1, we have ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\left|\\frac{\\mathbb{1}(x_{L+1}=x_{l}=e_{k})}{y(k)+\\varepsilon}\\right|\\leq\\frac{1}{\\varepsilon},\\quad\\left|\\frac{y(k)\\,\\mathbb{1}(x_{L+1}=e_{k})}{y(k)+\\varepsilon}\\right|\\leq\\frac{1}{\\varepsilon},\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "For the second term, we have ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{\\mathbb{1}(x_{L+1}=x_{l}=e_{k})}{y(k)+\\varepsilon}-\\frac{\\mathbb{1}(x_{L+1}=x_{l}=e_{k})}{\\bar{y}(k)+\\varepsilon}\\bigg|\\le\\frac{|\\bar{y}(k)-y(k)|}{\\varepsilon^{2}}\\le\\frac{\\sum_{l=M+1}^{L}|\\sigma_{l}(a\\cdot s^{\\top})-(L-M)^{-1}(s)|}{\\varepsilon^{2}}}}\\\\ &{}&{\\le\\frac{\\exp(a)-1}{\\varepsilon^{2}},\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad(\\mathsf{F}.4)}\\end{array}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "where the last inequality follows from (F.2). Similarly, the following bound can be derived: ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\left|\\frac{y(k)\\,\\mathbb{1}(x_{L+1}=e_{k})}{y(k)+\\varepsilon}-\\frac{\\bar{y}(k)\\,\\mathbb{1}(x_{L+1}=e_{k})}{\\bar{y}(k)+\\varepsilon}\\right|\\leq\\frac{\\exp(a)-1}{\\varepsilon}.\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "Combining (F.2), (F.3), (F.4) and (F.5), it holds that ", "page_idx": 58}, {"type": "equation", "text": "$$\n|A-B|\\leq\\sum_{l=M+1}^{L}\\mathbb{E}\\bigg[4\\sum_{k\\in[d]}\\frac{\\exp(a)-1}{\\varepsilon^{2}(L-M)}\\cdot f(X)\\bigg]\\leq\\frac{4C d(\\exp(a)-1)}{\\varepsilon^{2}}\\leq\\frac{8C a d}{\\varepsilon^{2}},\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "where the last inequality follows from $\\exp(x)-1\\leq2x$ for $0\\leq x\\leq1$ . This concludes the proof of the lemma. \u53e3 ", "page_idx": 58}, {"type": "text", "text": "Lemma F.4 provides the approximation error introduced by $\\mu^{\\pi}(e_{k})\\approx\\bar{y}(k)$ in the transformer model. Lemma F4. For the transformer model defined in (2.5) and any bounded function $f:\\mathcal{X}^{L}\\to\\mathbb{R}$ such that $\\operatorname*{sup}_{x\\in\\mathcal{X}^{L}}|f(x)|\\leq C$ for a constant $C>0,$ . define two quantities $A$ and $B$ as ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A:=\\displaystyle\\frac{1}{L-M}\\sum_{l=M+1}^{L}\\mathbb{E}_{X|\\pi}\\left[\\left(\\displaystyle\\sum_{k\\in[d]}\\frac{\\mathbb{1}(x_{L+1}=x_{l}=e_{k})}{\\bar{y}(k)+\\varepsilon}-\\frac{\\bar{y}(k)\\,\\mathbb{1}(x_{L+1}=e_{k})}{\\bar{y}(k)+\\varepsilon}\\right)\\cdot f(X)\\right],}\\\\ &{B:=\\displaystyle\\frac{1}{L-M}\\sum_{l=M+1}^{L}\\mathbb{E}_{X|\\pi}\\left[\\left(\\displaystyle\\sum_{k\\in[d]}\\frac{\\mathbb{1}(x_{L+1}=x_{l}=e_{k})}{\\mu^{\\pi}(e_{k})}-1\\right)\\cdot f(X)\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "where $\\begin{array}{r}{\\overline{{y}}=(L-M)^{-1}\\sum_{l=M+1}^{L}x_{l}}\\end{array}$ UnderAsumption 3.5,it holds tht ", "page_idx": 59}, {"type": "equation", "text": "$$\n|A-B|\\leq4C\\cdot\\frac{(1-\\lambda)^{-1/2}(D_{\\chi^{2}}(\\mu_{0}\\parallel\\mu^{\\pi})+1)^{1/4}+2\\sqrt{M}}{L^{1/2}\\gamma}+C\\gamma^{-1}\\varepsilon.\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "where $\\mu_{0}(\\cdot)$ is the initial distribution over the first $r_{n}$ tokens $X_{1:r_{n}}$ . Here we let $D_{\\chi^{2}}(\\mu_{0}\\parallel\\mu^{\\pi})$ to denote $D_{\\chi^{2}}(\\mu_{0}(X_{1:r_{n}}=\\cdot)\\parallel\\mu^{\\pi}(X_{1:r_{n}}=\\cdot))$ , i.e.,the $\\chi^{2}$ -divergence between $\\mu_{n}$ and the distribution over the first $r_{n}$ tokens under the stationary distribution $\\mu^{\\pi}$ ", "page_idx": 59}, {"type": "text", "text": "Proof of Lemma $F.4.$ Let us use $\\overline{{{y}}}_{X}(\\cdot)$ to remind the readers that $\\bar{y}(\\cdot)$ is also a function of $X$ .We simplify the expectation $\\mathbb{E}_{X\\mid\\pi}$ by $\\mathbb{E}$ in this proof. By rearranging the terms, we have ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A-B|=\\Bigg|\\frac{1}{L-M}\\displaystyle\\sum_{l=M+1}^{L}\\mathbb{E}\\Bigg[\\Bigg(\\sum_{k\\in[d]}\\frac{\\mathbb{I}(x_{L+1}=x_{l}=e_{k})}{\\bar{y}_{X}(k)+\\varepsilon}-\\sum_{k\\in[d]}\\frac{\\mathbb{I}(x_{L+1}=x_{l}=e_{k})}{\\mu^{\\pi}(e_{k})}\\Bigg.\\Bigg.}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\left.-\\sum_{k\\in[d]}\\frac{\\bar{y}_{X}(k)\\cdot\\mathbf{I}(x_{L+1}=e_{k})}{\\bar{y}_{X}(k)+\\varepsilon}+1\\Bigg)\\cdot f(X)\\Bigg]\\Bigg|}\\\\ &{\\qquad=\\Bigg|\\frac{1}{L-M}\\displaystyle\\sum_{l=M+1}^{L}\\mathbb{E}\\Bigg[\\Bigg(\\sum_{k\\in[d]}\\Big(\\frac{\\mu^{\\pi}(e_{k})-\\bar{y}_{X}(k)}{(\\bar{y}_{X}(k)+\\varepsilon)\\cdot\\mu^{\\pi}(e_{k})}-\\frac{\\varepsilon}{(\\bar{y}_{X}(k)+\\varepsilon)\\cdot\\mu^{\\pi}(e_{k})}\\Big)\\cdot\\mathbf{I}(x_{L+1}=\\delta)\\cdot}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\left.-\\sum_{k\\in[d]}\\frac{\\varepsilon\\prod(x_{L+1}=e_{k})}{\\bar{y}_{X}(k)+\\varepsilon}+1\\right)\\cdot f(X)\\Bigg]\\Bigg|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Here, we have three terms to control. For the first error term, we define ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{err}_{1}:=\\biggr|\\frac{1}{L-M}\\sum_{l=M+1}^{L}\\mathbb{E}\\biggr[\\sum_{k\\in[d]}\\frac{\\mu^{\\pi}(e_{k})-\\bar{y}_{X}(k)}{(\\bar{y}_{X}(k)+\\varepsilon)\\cdot\\mu^{\\pi}(e_{k})}\\cdot\\mathbb{1}(x_{L+1}=x_{l}=e_{k})\\cdot f(X)\\biggr]\\biggr|}\\\\ &{\\qquad\\le\\frac{C}{L-M}\\sum_{l=M+1}^{L}\\mathbb{E}\\biggr[\\sum_{k\\in[d]}\\frac{|\\mu^{\\pi}(e_{k})-\\bar{y}_{X}(k)|}{(\\bar{y}_{X}(k)+\\varepsilon)\\cdot\\mu^{\\pi}(e_{k})}\\cdot\\mathbb{1}(x_{L+1}=x_{l}=e_{k})\\biggr]}\\\\ &{\\qquad\\le C\\cdot\\mathbb{E}\\biggr[\\sum_{k\\in[d]}\\frac{|\\mu^{\\pi}(e_{k})-\\bar{y}_{X}(k)|}{\\mu^{\\pi}(e_{k})}\\cdot\\mathbb{1}(x_{L+1}=e_{k})\\biggr].}\\end{array}\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "The first inequality above holds by noting that $\\operatorname*{sup}_{X}|f(X)|\\leq C$ and the last inequality holds by noting that $\\begin{array}{r}{\\bar{y}_{X}(e_{k})\\,=\\,(L\\,-\\,M)^{-1}\\sum_{l=M+1}^{L}\\mathbb{1}(x_{l}\\,=\\,e_{k})}\\end{array}$ . Using Cauchy-Schwarz inequality, we arrive at ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{err}_{1}\\leq C\\cdot\\bigg(\\mathbb{E}\\bigg[\\displaystyle\\sum_{k\\in[d]}\\Big(\\frac{\\mu^{\\pi}(e_{k})-\\bar{y}_{X}(k)}{\\sqrt{\\mu^{\\pi}(e_{k})}}\\Big)^{2}\\bigg]\\cdot\\mathbb{E}\\bigg[\\displaystyle\\sum_{k\\in[d]}\\frac{\\mathbb{1}\\big(x_{L+1}=e_{k}\\big)}{\\mu^{\\pi}(e_{k})}\\bigg]\\bigg)^{1/2}}\\\\ &{\\qquad\\leq C\\gamma^{-1/2}\\cdot\\sqrt{\\mathbb{E}\\left[D_{\\chi^{2}}\\left(\\bar{y}_{X}(\\cdot)\\,\\|\\,\\mu^{\\pi}(x_{L+1}=\\cdot)\\right)\\right]}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "For the second term, we similarly have ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathrm{err}_{2}=\\left|\\frac{1}{L-M}\\displaystyle\\sum_{l=M+1}^{L}\\sum_{k\\in[d]}\\mathbb{E}\\left[\\frac{\\varepsilon}{(\\bar{y}_{X}(k)+\\varepsilon)\\mu^{\\pi}(e_{k})}\\cdot\\mathbb{1}(x_{L+1}=x_{l}=e_{k})\\cdot f(X)\\right]\\right|}\\\\ {\\le C\\Bigg|\\displaystyle\\sum_{k\\in[d]}\\mathbb{E}\\bigg[\\frac{\\varepsilon\\cdot\\mathbb{I}\\left(x_{L+1}=e_{k}\\right)}{\\mu^{\\pi}(e_{k})}\\bigg]\\bigg|\\le C\\gamma^{-1}\\varepsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Lastly, we have the error term ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{err}_{3}\\mathrel{\\mathop:}=\\displaystyle\\frac{1}{L-M}\\sum_{l=M+1}^{L}\\mathbb{E}\\Bigg[\\sum_{k\\in[d]}\\frac{\\varepsilon\\,\\mathbb{1}(x_{L+1}=e_{k})}{\\bar{y}_{X}(k)+\\varepsilon}\\cdot f(X)\\Bigg]\\leq C\\cdot\\mathbb{E}\\Bigg[\\sum_{k\\in[d]}\\frac{\\varepsilon\\,\\mathbb{1}(x_{L+1}=e_{k})}{\\bar{y}_{X}(k)+\\varepsilon}\\Bigg]}\\\\ &{\\mathrm{~\\~\\}\\leq C\\cdot\\Bigg|\\mathbb{E}\\Bigg[\\sum_{k\\in[d]}\\frac{\\varepsilon\\,\\mathbb{1}(x_{L+1}=e_{k})}{\\mu^{\\pi}(e_{k})+\\varepsilon}\\Bigg]\\Bigg|+C\\cdot\\Bigg|\\sum_{k\\in[d]}\\mathbb{E}\\Bigg[\\frac{\\varepsilon\\bigl(\\bar{y}_{X}(k)-\\mu^{\\pi}(e_{k})\\bigr)\\cdot\\mathbb{1}(x_{L+1}=e_{k})}{(\\mu^{\\pi}(e_{k})+\\varepsilon)(\\bar{y}_{X}(k)+\\varepsilon)}\\Bigg]\\Bigg|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Here, the first term is upper bounded by $C\\gamma^{-1}\\varepsilon$ , and for the second term we have by Cauchy-Schwartz that ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{C\\cdot\\bigg|\\sum_{k\\in[d]}\\mathbb{E}\\bigg[\\frac{\\varepsilon\\big(\\bar{y}_{X}(k)-\\mu^{\\pi}(e_{k})\\big)\\cdot\\mathbb{I}\\big(x_{L+1}=e_{k}\\big)}{(\\mu^{\\pi}(e_{k})+\\varepsilon)(\\bar{y}_{X}(k)+\\varepsilon)}\\bigg]\\bigg|}\\\\ &{\\quad\\le C\\cdot\\sqrt{\\mathbb{E}\\bigg[\\sum_{k\\in[d]}\\frac{\\big(\\overline{{y}}_{X}(k)-\\mu^{\\pi}(e_{k})\\big)^{2}}{\\mu^{\\pi}(e_{k})}\\bigg]\\cdot\\mathbb{E}\\bigg[\\sum_{k\\in[d]}\\frac{\\varepsilon^{2}\\,\\mathbb{I}\\big(x_{L+1}=e_{k}\\big)}{(\\overline{{y}}_{X}(k)+\\varepsilon)^{2}\\mu^{\\pi}(e_{k})}\\bigg]}}\\\\ &{\\quad\\le C\\gamma^{-1/2}\\cdot\\sqrt{\\mathbb{E}_{X}D_{\\chi^{2}}(\\bar{y}_{X}(\\cdot)\\,\\|\\,\\mu^{\\pi}(x_{L+1}=\\cdot))},}\\end{array}\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "which shares a similar upper bound as $\\mathrm{err1}$ . Now we invoke Lemma F.18 to conclude that ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|A-B|\\leq\\mathrm{err}_{1}+\\mathrm{err}_{2}+\\mathrm{err}_{3}\\leq2C\\gamma^{-1/2}\\cdot\\sqrt{\\mathbb{E}_{X}D_{\\chi^{2}}(\\Bar{y}_{X}(\\cdot)\\left\\|\\right\\mu^{\\pi}(x_{L+1}=\\cdot))}+C\\gamma^{-1}\\varepsilon}\\\\ &{\\qquad\\qquad\\leq2C\\gamma^{-1/2}\\bigg(\\frac{4(1-\\lambda)^{-1}\\sqrt{D_{\\chi^{2}}(\\mu_{0}\\left\\|\\mu^{\\pi}\\right\\rangle+1}+16M}{L\\cdot\\operatorname*{min}_{x_{L+1}}\\mu^{\\pi}(x_{L+1})}\\bigg)^{1/2}+C\\gamma^{-1}\\cdot\\varepsilon}\\\\ &{\\qquad\\qquad\\leq2C\\gamma^{-1}\\cdot\\frac{2(1-\\lambda)^{-1/2}(D_{\\chi^{2}}(\\mu_{0}\\left\\|\\mu^{\\pi}\\right\\rangle+1)^{1/4}+4\\sqrt{M}}{L^{1/2}}+C\\gamma^{-1}\\varepsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "Hence, we complete our proof of Lemma F.4. ", "page_idx": 60}, {"type": "text", "text": "Lemma F.5 covers the approximation error due to the mixing property of the Markov chain. ", "page_idx": 60}, {"type": "text", "text": "$\\mathcal{S}\\in[H]_{\\leq D}$ $h\\in S$ $\\widetilde{\\sigma}^{(h)}$ $\\sigma^{(h)}$ $[M]$ $i,j\\in[M]$ $\\widetilde{\\sigma}_{-i}^{(h)},\\sigma_{-j}^{(h)}\\,\\in\\,[0,1]$ $\\sum_{i=1}^{M}\\widetilde{\\sigma}_{-i}^{(h)}=$ $\\begin{array}{r}{\\sum_{j=1}^{M}\\sigma_{-j}^{(h)}=1.}\\end{array}$ .Given these distributions over $[M]$ we define ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\widetilde v_{L+1}^{(h)}:=\\sum_{i\\in[M]}\\widetilde\\sigma_{-i}^{(h)}\\cdot x_{L+1-i},\\qquad a n d\\qquad v_{l}^{(h)}:=\\sum_{j\\in[M]}\\sigma_{-j}^{(h)}\\cdot x_{l-j},\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "where we let $x_{l}\\in\\mathscr{X}$ denote the $l$ -thtokenintheMarkovchainforall $l\\in[L\\!+\\!1]$ Moreover,withslight abuse of notation,we let $(z,Z)=(z,z_{-1},\\ldots,z_{-M})\\in\\mathcal{X}^{M+1}$ and $(x,X)=(x,x_{-1},\\ldots,x_{-M})\\in$ $\\chi^{M+1}$ be two independent randomvariables sampled from the stationary distribution $\\mu^{\\pi}$ We define randomvariables $\\bar{\\widetilde{v}}^{(h)}(Z)$ and $v^{(h)}(X)$ as ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\widetilde v^{(h)}(Z):=\\sum_{i\\in[M]}\\widetilde\\sigma_{-i}^{(h)}\\cdot z_{-i},\\qquad a n d\\qquad v^{(h)}(X):=\\sum_{j\\in[M]}\\sigma_{-j}^{(h)}\\cdot x_{-j}.\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "Using $\\widetilde{v}_{L+1}^{(h)},\\,v_{l}^{(h)},\\,\\widetilde{v}^{(h)}(Z),$ and $v^{(h)}(X)$ we defne wo quanies $A$ and $B$ as ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A:=\\displaystyle\\frac{1}{L-M}\\sum_{l=M+1}^{L}\\mathbb{E}_{X|\\pi}\\bigg[\\bigg(\\sum_{k\\in[d]}\\frac{\\mathbb{1}(x_{L+1}=x_{l}=e_{k})}{\\mu^{\\pi}(e_{k})}-1\\bigg)\\cdot\\prod_{h\\in\\mathcal{S}}\\langle v_{l}^{(h)},\\tilde{v}_{L+1}^{(h)}\\rangle\\bigg],}\\\\ &{B:=\\mathbb{E}_{(x,X),(z,Z)\\sim\\mu^{\\pi}\\times\\mu^{\\pi}}\\bigg[\\bigg(\\sum_{k\\in[d]}\\frac{\\mathbb{1}(x=z=e_{k})}{\\mu^{\\pi}(e_{k})}-1\\bigg)\\cdot\\prod_{h\\in\\mathcal{S}}\\langle\\tilde{v}^{(h)}(Z),v^{(h)}(X)\\rangle\\bigg],}\\end{array}\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "Where $\\mathbb{E}_{X\\mid\\pi}$ means that the expectation is taken with respect to the randomness of the Markov chain withtransition $\\pi$ .Then, under Assumption 3.5,we have ", "page_idx": 60}, {"type": "equation", "text": "$$\n|A-B|\\leq\\frac{8M}{L\\gamma}+\\frac{16\\sqrt{D_{\\chi^{2}}(\\mu_{0}\\,\\|\\,\\mu^{\\pi})+1}}{L(1-\\lambda)\\gamma^{|S|/2+1}},\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "where $\\mu_{0}(\\cdot)$ is theinitialdistributionovethfrst $r_{n}$ tokens $X_{1:r_{n}}$ and $D_{\\chi^{2}}(\\mu_{0}\\parallel\\mu^{\\pi})$ is a short-hand notation of $D_{\\chi^{2}}(\\mu_{0}(X_{1:r_{n}}=\\cdot)\\parallel\\mu^{\\pi}(X_{1:r_{n}}=\\cdot))$ ", "page_idx": 60}, {"type": "text", "text": "Proof of Lemma $F.5$ .By triangular inequality, we have ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A-B|\\leq\\displaystyle\\bigg|\\frac{1}{L-M}\\sum_{l=M+1}^{L}\\mathbb{E}_{X|\\pi}\\bigg[\\bigg(\\sum_{k\\in[d]}\\frac{\\mathbb{I}(x_{L+1}=x_{l}=e_{k})}{\\mu^{\\pi}(e_{k})}\\bigg)\\cdot\\prod_{h\\in S}\\langle v_{l}^{(h)},\\tilde{v}_{L+1}^{(h)}\\rangle\\bigg]}\\\\ &{\\qquad\\qquad-\\mathbb{E}_{(x,X),(z,Z)\\sim\\mu^{\\pi}\\times\\mu^{\\pi}}\\bigg[\\bigg(\\sum_{k\\in[d]}\\frac{\\mathbb{I}(x=z=e_{k})}{\\mu^{\\pi}(e_{k})}\\bigg)\\cdot\\bigg(\\prod_{h\\in S}\\langle\\tilde{v}^{(h)}(Z),v^{(h)}(X)\\rangle\\bigg)\\bigg]\\bigg|}\\\\ &{\\qquad\\qquad+\\left|\\frac{1}{L-M}\\sum_{l=M+1}^{L}\\mathbb{E}_{X|\\pi}\\bigg[\\prod_{h\\in S}\\langle v_{l}^{(h)},v_{L+1}^{(h)}\\rangle\\bigg]-\\mathbb{E}_{(x,X),(z,Z)\\sim\\mu^{\\pi}\\times\\mu^{\\pi}}\\bigg[\\bigg(\\prod_{h\\in S}\\langle\\tilde{v}^{(h)}(Z),v^{(h)}(Z)\\rangle\\bigg)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "We will establish the upper bounds for each of the absolute value terms. We first focus on the first absolute value term. ", "page_idx": 61}, {"type": "text", "text": "Bounding the First Absolute Value Term. Let $p^{\\pi}(X)$ denote the joint distribution of the whole sequence $X$ under kernel $\\pi$ By the definitions of $\\widetilde{v}_{L+1}^{(h)}$ and $v_{l}^{(h)}$ we have ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle v_{l}^{(h)},v_{L+1}^{(h)}\\rangle=\\displaystyle\\sum_{i_{h},j_{h}\\in[M]}\\sigma_{-i_{h}}^{(h)}\\cdot\\sigma_{-j_{h}}^{(h)}\\cdot\\langle x_{L+1-i_{h}},x_{l-j_{h}}\\rangle}\\\\ &{\\qquad\\qquad=\\displaystyle\\sum_{i_{h},j_{h}\\in[M]}\\sum_{k\\in[d]}\\sigma_{-i_{h}}^{(h)}\\cdot\\sigma_{-j_{h}}^{(h)}\\cdot\\mathbb{I}(x_{L+1-i_{h}}=x_{l-j_{h}}=e_{k}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "where we use $(i_{h},j_{h})$ as the indices to highlight that they are associated with head $h$ . And we use $k_{h}\\in[d]$ to index all the possible common values for $x_{l-i_{h}}$ and $x_{L+1-j_{h}}$ . Then plugging this equality into $\\begin{array}{r}{\\prod_{h\\in\\cal S}\\langle v_{l}^{(h)},v_{L+1}^{(h)}\\rangle}\\end{array}$ andexchaning the orerf produt and um, ha ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{k\\in[d]}\\frac{\\mathbb{I}(x_{L+1}=x_{l}=e_{k})}{\\mu^{\\pi}(e_{k})}\\bigg)\\cdot\\prod_{h\\in S}\\langle v_{l}^{(h)},v_{L+1}^{(h)}\\rangle}}\\\\ &{}&{=\\displaystyle\\sum_{\\{(i_{h},j_{h})\\}_{h\\in S}}\\sum_{\\{k_{h}\\}_{h\\in S},k\\in[d]}\\frac{\\mathbb{I}(x_{L+1}=x_{l}=e_{k})}{\\mu^{\\pi}(z=e_{k})}\\cdot\\Bigg(\\prod_{h\\in S}\\sigma_{-i_{h}}^{(h)}\\cdot\\sigma_{-j_{h}}^{(h)}\\cdot\\mathbb{I}(x_{L+1-i_{h}}=x_{l-j_{h}}=e_{k})\\Bigg)\\cdot e_{k}}\\end{array}\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "where the summation means that we sum over all possible values that $\\{i_{h},j_{h},k_{h}\\}_{h\\in\\cal{S}}$ and $k$ can take. Specifically, each $i_{h}$ and $j_{h}$ takevalues in $[M]$ , and each $k_{h}$ and $k$ takesvalues in $[d]$ . Moreover, using the property of indicator functions, we can further simplify (F.6) by gathering all indicators: ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{k\\in[d]}\\frac{\\mathbb{I}\\left(x_{L+1}=x_{l}=e_{k}\\right)}{\\mu^{\\pi}(e_{k})}\\Bigg)\\cdot\\prod_{h\\in S}\\langle v_{l}^{(h)},v_{L+1}^{(h)}\\rangle}}\\\\ &{}&{=\\sum_{\\left\\{(i_{h},j_{h})\\right\\}_{h\\in S}}\\left(\\prod_{h\\in S}\\sigma_{-i_{h}}^{(h)}\\cdot\\sigma_{-j_{h}}^{(h)}\\right)\\cdot\\left(\\sum_{\\left\\{k_{h}\\right\\}_{h\\in S},k\\in[d]}\\frac{\\mathbb{I}\\left(x_{L+1}=x_{l}=e_{k},x_{L+1-i_{h}}=x_{l-j_{h}}=e_{k}\\right)}{\\mu^{\\pi}(z=e_{k})}\\right)\\frac{\\sqrt{(x_{l}-x_{l}=e_{l})^{2}}}{\\mu}}\\end{array}\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "Now we take expectations with respect to the randomness of $X$ on both ends of (F.7) and get ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{L-M}\\sum_{l=M+1}^{L}\\mathbb{E}\\bigg[\\bigg(\\sum_{k\\in[d]}\\frac{1\\big(x_{L+1}=x_{l}=e_{k}\\big)}{\\mu^{\\pi}(e_{k})}\\bigg)\\cdot\\prod_{h\\in\\mathcal{S}}\\langle v_{l}^{(h)},v_{L+1}^{(h)}\\rangle\\bigg]}\\\\ &{\\displaystyle=\\sum_{\\{(i_{h},j_{h})\\}_{h\\in\\mathcal{S}}}\\!\\left(\\!\\prod_{h\\in\\mathcal{S}}\\!\\sigma_{-i_{h}}^{(h)}\\sigma_{-j_{h}}^{(h)}\\!\\right)\\cdot\\!\\sum_{\\{k_{h}\\}_{h\\in\\mathcal{S}},k\\in[d]}\\!\\frac{\\sum_{l=M+1}^{L}p^{\\pi}\\big(x_{L+1}=x_{l}=e_{k},x_{L+1-i_{h}}=x_{l-j_{h}}=e_{k}\\big)}{(L-M)\\cdot\\mu^{\\pi}(z=e_{k})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "To further simplify the above equality, we defne a new probability distribution over $X_{L+1-M:L+1}$ and another subsequence of length $M+1$ . Note that $X_{L+1-M:L+1}$ contains is a subsequence with $M+1$ tokens. We let $(z,Z)\\,=\\,(z,z_{-1},\\ldots,z_{-1},z_{-M})$ denote a random token sequence of size $M+1$ in reverse order. We define a joint distribution $\\widehat{p}^{\\pi}$ over $X_{L+1-M:L+1}$ and $(z,{\\bar{Z}})$ as follows. Let $E=(E_{0},E_{-1},\\ldots,E_{-M})$ and $E^{\\prime}=(E_{0}^{\\prime},E_{-1}^{\\prime},\\dots,E_{-M}^{\\prime})$ be two elements in $\\mathcal{X}^{M+1}$ . That is, each component of $E$ and $E^{\\prime}$ are in $\\mathcal{X}$ . The probability mass function of $\\widehat{p}^{\\pi}$ is defined as ", "page_idx": 61}, {"type": "text", "text": "", "page_idx": 62}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{p}^{\\pi}\\big((x_{L+1},x_{L},\\ldots,x_{L+1-M})=E,(z,Z)=E^{\\prime}\\big)}\\\\ &{\\qquad=\\displaystyle\\frac{1}{L-M}\\sum_{l=M+1}^{L}p^{\\pi}\\big((x_{L+1},x_{L},\\ldots,x_{L+1-M})=E,(x_{l},x_{l-1},\\ldots,x_{l=M})=E^{\\prime}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "That is, $\\widehat{p}^{\\pi}$ can be viewed as the joint distribution of $X_{L+1-M:L+1}$ with an averaged distribution of the history. When $L$ is sufficiently large, by the mixing property of the Markov chain, we expect that, under $\\widehat{p}^{\\pi},(z,Z)$ is approximately independent of $X_{L+1-M:L+1}$ , and the marginal distributions of $(z,Z)$ and $X_{L+1-M:L+1}$ are both close to the stationary distribution $\\mu^{\\pi}$ : We will translate this intuition into a rigorous argument in Lemma F.17, which bounds the total-variation distance between $\\widehat{p}^{\\pi}$ and the product distribution $\\mu^{\\pi}\\times\\mu^{\\pi}$ ", "page_idx": 62}, {"type": "text", "text": "With $\\widehat{p}^{\\pi}$ defined in (F.8), we can rewrite the expectation above as ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{1}{L-M}\\sum_{l=M+1}^{L}\\mathbb{E}\\bigg[\\bigg(\\sum_{k\\in[d]}\\frac{1\\big(x_{L+1}=x_{l}=e_{k}\\big)}{\\mu^{\\pi}(e_{k})}\\bigg)\\cdot\\prod_{k\\in S}\\langle v_{l}^{(h)},v_{L+1}^{(h)}\\rangle\\bigg]}}\\\\ &{}&{=\\sum_{\\{(i_{h},j_{h})\\}_{h\\in S}}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! \n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "Similarly, by the definitions of $\\widetilde{v}^{(h)}(Z)$ and $v^{(h)}(Z)$ , we can write $\\langle\\widetilde{v}^{(h)}(Z),v^{(h)}(X)\\rangle$ as ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\langle\\widetilde{v}^{(h)}(Z),v^{(h)}(X)\\rangle=\\sum_{\\substack{i_{h},j_{h}\\in[M]\\,k_{h}\\in[d]}}\\sigma_{-i_{h}}^{(h)}\\cdot\\sigma_{-j_{h}}^{(h)}\\cdot\\mathbb{1}(z_{-i_{h}}=x_{j_{h}}=e_{k}).\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "Then, multiplying these terms with $h\\in S$ , we can write ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\int_{\\Lambda\\in[d]}\\frac{\\mathbb{I}(x=z=e_{k})}{\\mu^{\\pi}(e_{k})}\\Bigg)\\cdot\\prod_{h\\in\\mathcal{S}}\\langle\\tilde{v}^{(h)}(Z),v^{(h)}(X)\\rangle}}\\\\ &{}&{=\\sum_{\\{(i_{h},j_{h})\\}_{h\\in\\mathcal{S}}}\\left(\\prod_{h\\in\\mathcal{S}}\\sigma_{-i_{h}}^{(h)}\\cdot\\sigma_{-j_{h}}^{(h)}\\right)\\cdot\\bigg(\\sum_{\\{k_{h}\\}_{h\\in\\mathcal{S}},k\\in[d]}\\frac{\\mathbb{I}(z=x=e_{k},z_{-i_{h}}=x_{-j_{h}}=e_{k_{h}},\\forall h\\in\\mathcal{S}}{\\mu^{\\pi}(z=e_{k})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "Recall that here $(z,Z)=(z,z_{-1},\\ldots,z_{-M})$ and $(x,X)=(x,x_{-1},\\ldots,x_{-M})$ are independently sampled from the stationary distribution $\\mu^{\\pi}$ . Taking the expectation under $\\mu^{\\pi}$ , we have ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\xi_{(x,X),(z,Z)\\sim\\mu^{\\pi}\\times\\mu^{\\tau}}\\bigg[\\biggl(\\sum_{k\\in[d]}\\frac{\\mathbb{I}(x=z=e_{k})}{\\mu^{\\pi}(e_{k})}\\biggr)\\cdot\\biggl(\\prod_{h\\in\\mathcal{S}}\\langle\\tilde{v}^{(h)}(Z),v^{(h)}(X)\\rangle\\biggr)\\bigg]}\\quad}&{{}}&{{\\mathrm{(F.11)}}}\\\\ &{=\\sum_{\\{(i_{h},j_{h})\\}_{h\\in\\mathcal{S}}}\\biggl(\\prod_{h\\in\\mathcal{S}}\\sigma_{-i_{h}}^{(h)}\\sigma_{-j_{h}}^{(h)}\\biggr)\\cdot\\displaystyle\\sum_{\\{k_{h}\\}_{h\\in\\mathcal{S}},k\\in[d]}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\mu^{\\pi}(x=e_{k},x_{-i_{h}}=e_{k_{h}},\\forall h\\in\\mathcal{S})\\cdot\\mu^{\\pi}(z=e_{k},z_{-j_{h}})}}\\\\ &{}&{\\quad\\{(i_{h},j_{h})\\}_{h\\in\\mathcal{S}}\\thinspace h\\in\\mathcal{S}\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\mu^{\\pi}(z=e_{k})}\\end{array}\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "To bound the first absolute value term in the upper bound on $|A-B|$ , we aim to compare (F.9) and (F.11). To this end, let us fix collections of index pairs $(i_{h},j_{h})_{h\\in S}$ . Let $S_{1}\\,=\\,\\{i_{h}\\,:h\\in S\\}$ and $S_{2}\\,=\\,\\{j_{h}\\,:\\,h\\,\\in\\,S\\}$ be the unique values in $(i_{h})_{h\\in S}$ and $(j_{h})_{h\\in\\mathcal{S}}$ . Since there might exists two elements $h$ and $h^{\\prime}$ in $\\boldsymbol{S}$ such that $i_{h}=i_{h^{\\prime}}$ or $j_{h}=j_{h^{\\prime}},|S_{1}|$ and $|{S_{2}}|$ might be strictly less than $|{\\cal S}|$ .As a result, $\\widehat{p}^{\\pi}(x_{L+1}=z=e_{k},x_{L+1-i_{h}}=z_{-j_{h}}=e_{k_{h}},\\forall h\\in\\mathcal{S})$ only involves random variables $x_{L+1}$ \uff0c $X_{L+1-S_{1}}\\,=\\,\\{x_{L+1-i}\\}_{i\\in S_{1}}$ \uff0c $z,\\,Z_{-S_{2}}\\,=\\,\\left\\{z_{-j}\\right\\}_{j\\in S_{2}}$ , which are a subset of the random variables defined in (F.8). Similarly, ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\mu^{\\pi}(x=e_{k},x_{-i_{h}}=e_{k_{h}},\\forall h\\in\\mathcal{S})\\cdot\\mu^{\\pi}(z=e_{k},z_{-j_{h}}=e_{k_{h}},\\forall h\\in\\mathcal{S})\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "only involves a subset of random variables $x$ $X_{-S_{1}}\\;=\\;\\{x_{-i}\\}_{i\\in S_{1}},$ $z$ and $Z_{-S_{2}}$ . Let us define $\\bar{E}\\;=\\;(E_{0},(E_{-i})_{i\\in{\\cal S}_{1}})\\;\\in\\;\\chi^{|{\\cal S}_{1}|+1}$ and $\\bar{E}^{\\prime}\\;=\\;(E_{0}^{\\prime},(E_{-j}^{\\prime})_{j\\in{\\cal S}_{2}})\\;\\in\\;\\chi^{|{\\cal S}_{2}|+1}$ By enumerating $\\bar{E}$ ", "page_idx": 62}, {"type": "text", "text": "in $\\chi|S_{1}|+1$ and $\\bar{E}^{\\prime}$ $\\chi|S_{2}|+1$ , we equivalently enumerate all possible values the above random variables can take. Therefore, by comparing (F.9) with (F.11), we have ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{\\stackrel{\\scriptstyle|k_{h})_{\\scriptstyle h\\in\\mathcal{S}},k\\in[d]}}|\\widehat{p}^{\\pi}(x_{L+1}=z=e_{k},x_{L+1-i_{h}}=z_{-j_{h}}=e_{k_{h}},\\forall h\\in\\mathcal{S})}}\\\\ &{}&{\\quad-\\,\\boldsymbol{\\mu}^{\\pi}(x=e_{k},x_{-i_{h}}=e_{k_{h}},\\forall h\\in\\mathcal{S})\\cdot\\boldsymbol{\\mu}^{\\pi}(z=e_{k},z_{-j_{h}}=e_{k_{h}},\\forall h\\in\\mathcal{S})|}\\\\ &{}&{\\quad=\\sum_{\\stackrel{\\scriptstyle|\\widehat{E}^{\\pi}}{\\ E}^{-\\gamma}}\\big|\\widehat{p}^{\\pi}\\big((x_{L+1},X_{L+1-\\mathcal{S}_{1}})=\\bar{E},(z,Z_{-\\mathcal{S}_{2}})=\\bar{E}^{\\prime}\\big)-\\mu^{\\pi}\\big((x_{L+1},X_{L+1-\\mathcal{S}_{1}})=\\bar{E}\\big)\\cdot\\mu^{\\pi}\\big((z,Z_{-\\mathcal{S}_{2}})=\\bar{E},(z,Z_{-\\mathcal{S}_{2}})=0\\big)}\\\\ &{}&{\\quad\\quad\\quad\\quad\\cdot\\mathbb{1}(E_{0}=E_{0}^{\\prime},E_{-i_{h}}=E_{-j_{h}}^{\\prime},\\forall h\\in\\mathcal{S})}\\\\ &{}&{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad(\\mathrm{F}.12)}\\end{array}\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "where in the last line, we use $Y$ and $Y^{\\prime}$ as placeholders for the random variables $(x_{L+1},X_{L+1-S_{1}})$ and $(z,Z_{-S_{2}})$ respectively. In the first equality, we sum over $\\bar{E}\\in\\mathcal{X}^{|S_{1}|+1}$ and $\\bar{E}^{\\prime}\\in\\mathcal{X}^{|S_{2}|+1}$ ,and the last inequality follows from the definition of total variation distance and dropping the indicator. By Lemma F.17, this total variation distance is bounded by ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2\\|\\widetilde{p^{\\pi}}(Y=\\cdot,Y^{\\prime}=\\cdot)-\\mu^{\\pi}(Y=\\cdot)\\times\\mu^{\\pi}(Y^{\\prime}=\\cdot)\\|_{\\mathrm{TV}}}\\\\ &{\\quad\\le\\displaystyle\\frac{4M}{L}+\\frac{8\\sqrt{D_{\\chi^{2}}(\\mu_{0}\\,\\|\\,\\mu^{\\pi})+1}}{L(1-\\lambda)\\cdot\\sqrt{\\operatorname*{min}_{x_{L+1},X_{L+1-\\mathcal{S}_{1}}}\\mu^{\\pi}(x_{L+1},X_{L+1-\\mathcal{S}_{1}})}}}\\\\ &{\\quad\\le\\displaystyle\\frac{4M}{L}+\\frac{8\\sqrt{D_{\\chi^{2}}(\\mu_{0}\\,\\|\\,\\mu^{\\pi})+1}}{L(1-\\lambda)\\cdot\\gamma^{(|\\mathcal{S}|+1)/2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "where the last inequality holds by Corollary F.15 and the fact that $|S_{1}|\\leq|S|$ . Specifically, Corollary F.15 implies that the density function of the joint distribution of $x_{L+1}$ and $X_{L+1-S_{1}}$ is lower boundedby $\\bar{\\gamma}^{|S_{1}|+1}\\geq\\gamma^{|S|+1}$ . Thus, combining (F.12) and (F.13), we have ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\biggl|\\sum_{\\{k_{h}\\}_{h\\in\\mathcal{S},k\\in[d]}}\\frac{\\widehat{p}^{\\pi}(x_{L+1}=z=e_{k},x_{L+1-i_{h}}=z_{-j_{h}}=e_{k_{h}},\\forall h\\in\\mathcal{S})}{\\mu^{\\pi}(z=e_{k})}}}\\\\ &{}&{-\\sum_{\\{k_{h}\\}_{h\\in\\mathcal{S},k\\in[d]}}\\frac{\\mu^{\\pi}(x=e_{k},x_{-i_{h}}=e_{k_{h}},\\forall h\\in\\mathcal{S})\\cdot\\mu^{\\pi}(z=e_{k},z_{-j_{h}}=e_{k_{h}},\\forall h\\in\\mathcal{S})}{\\mu^{\\pi}(z=e_{k})}\\biggr|}\\\\ &{}&{\\leq\\frac{1}{\\gamma}\\cdot\\left(\\frac{4M}{L}+\\frac{8\\sqrt{D_{\\mathcal{X}^{2}}(\\mu_{0}\\parallel\\mu^{\\pi})+1}}{L(1-\\lambda)\\cdot\\gamma^{(|\\mathcal{S}|+1)/2}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "Therefore, to bound the first absolute value term, we combine (F.9), (F.11), and (F.14) and use triangle inequality to get ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\cfrac{1}{L-M}\\csum_{l=M+1}^{L}\\mathbb{E}_{X|\\pi}\\bigg[\\bigg(\\displaystyle\\sum_{k\\in[d]}\\frac{\\mathbb{I}(x_{L+1}=x_{l}=e_{k})}{\\mu^{\\pi}(e_{k})}\\bigg)\\cdot\\prod_{h\\in S}\\langle v_{l}^{(h)},\\tilde{v}_{L+1}^{(h)}\\rangle\\bigg]\\right.}\\\\ &{\\qquad\\qquad\\left.-\\mathbb{E}_{(x,X),(z,Z)\\sim\\mu^{\\pi}\\times\\mu^{\\pi}}\\bigg[\\bigg(\\displaystyle\\sum_{k\\in[d]}\\frac{\\mathbb{I}(x=z=e_{k})}{\\mu^{\\pi}(e_{k})}\\bigg)\\cdot\\bigg(\\displaystyle\\prod_{h\\in S}\\langle\\tilde{v}^{(h)}(Z),v^{(h)}(X)\\rangle\\bigg)\\bigg]\\right|}\\\\ &{\\qquad\\leq\\frac{1}{\\gamma}\\cdot\\left(\\displaystyle\\frac{4M}{L}+\\frac{8\\sqrt{D_{\\chi^{2}}(\\mu_{0}\\parallel\\mu^{\\pi})+1}}{L(1-\\lambda)\\cdot\\gamma^{(|S|+1)/2}}\\right)\\cdot\\displaystyle\\sum_{\\{(i_{h},j_{h})\\}_{h\\in S}}\\bigg(\\prod_{h\\in S}\\tilde{\\sigma}_{-i_{h}}^{(h)}\\sigma_{-j_{h}}^{(h)}\\bigg).\\qquad\\qquad\\mathrm{otherwise}\\quad\\mathbb{I}(a_{h}^{\\pi})}\\end{array}\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "Furthermore, recallthat $\\widetilde{\\sigma}^{(h)}$ and $\\sigma^{(h)}$ are probabilitydistributions over $[M]$ for all $h\\in S$ .By going over all possible values that $\\{(i_{h},j_{h})\\}_{h\\in S}$ can take, we have ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\sum_{\\{(i_{h},j_{h})\\}_{h\\in\\mathcal{S}}}\\left(\\prod_{h\\in\\mathcal{S}}\\widetilde{\\sigma}_{-i_{h}}^{(h)}\\sigma_{-j_{h}}^{(h)}\\right)=\\prod_{h\\in\\mathcal{S}}\\left(\\sum_{k\\in[M]}\\widetilde{\\sigma}_{-k}^{(h)}\\right)\\cdot\\left(\\sum_{k\\in[M]}\\sigma_{-k}^{(h)}\\right)=1.\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "Plugging this equality into (F.15), we show that the upper bound in (F.15) can be reduced to the right-hand side of (F.14). ", "page_idx": 63}, {"type": "text", "text": "Bounding the Second Absolute Value Term. For the second absolute value term, an analogous args $\\langle v_{l}^{(h)},\\bar{v_{L+1}^{(h)}}$ and $\\langle\\widetilde{v}^{(h)}(Z),v^{(h)}(Z)\\rangle$ and do not have indicators 1 $.(x_{L+1}=x_{l})$ )and $\\mathbb{1}(x=z)$ ", "page_idx": 64}, {"type": "text", "text": "Similar to the derivation in (F.9) and (F.11), ", "page_idx": 64}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{L-M}\\displaystyle\\sum_{l=M+1}^{L}\\mathbb{E}\\bigg[\\underset{h\\in\\mathcal{S}}{\\prod}\\big\\langle v_{l}^{(h)},\\tilde{v}_{L+1}^{(h)}\\big\\rangle\\bigg]-\\mathbb{E}_{(x,X),(z,Z)\\sim\\mu^{\\pi}\\times\\mu^{\\pi}}\\bigg[\\bigg(\\prod_{h\\in\\mathcal{S}}\\langle\\tilde{v}^{(h)}(Z),v^{(h)}(X)\\rangle\\bigg)\\bigg]\\bigg\\rvert}\\\\ &{\\quad\\quad=\\bigg|\\displaystyle\\sum_{\\{(i_{h},j_{h})\\}_{h\\in\\mathcal{S}}}\\bigg(\\prod_{h\\in\\mathcal{S}}\\tilde{\\sigma}_{-i_{h}}^{(h)}\\sigma_{-j_{h}}^{(h)}\\bigg)\\cdot\\sum_{\\{k_{h}\\}_{h\\in\\mathcal{S}}}\\bigg(\\widehat{p}^{\\pi}(x_{L+1-i_{h}}=z_{-j_{h}}=e_{k_{h}},\\forall h\\in\\mathcal{S})\\quad\\qquad\\mathrm{(F.17)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad-\\mu^{\\pi}(x_{-i_{h}}=e_{k_{h}},\\forall h\\in\\mathcal{S})\\cdot\\mu^{\\pi}(z_{-j_{h}}=e_{k_{h}},\\forall h\\in\\mathcal{S})\\bigg)\\bigg|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 64}, {"type": "text", "text": "Similar to (F.12), for any fixed collection of index pairs $(i_{h},j_{h})_{h\\in S}$ welet $S_{1}=\\{i_{h}:h\\in S\\}$ and $S_{2}=\\{j_{h}:h\\in S\\}$ denote the unique values in $(i_{h})_{h\\in\\mathcal{S}}$ and $(j_{h})_{h\\in\\mathcal{S}}$ . By Lemma F.17, we have ", "page_idx": 64}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\bigg|\\sum_{\\{k_{h}\\}_{h\\in\\mathcal{S}}}\\Big(\\widehat{p}^{\\pi}(x_{L+1-i_{h}}=z_{-j_{h}}=e_{k_{h}},\\forall h\\in\\mathcal{S})-\\mu^{\\pi}(x_{-i_{h}}=e_{k_{h}},\\forall h\\in\\mathcal{S})\\cdot\\mu^{\\pi}(z_{-j_{h}}=e_{k_{h}},\\forall h\\in\\mathcal{S})\\bigg)\\cdot\\widehat{p}(x_{1})}}\\\\ &{}&{\\leq2\\big\\|\\widehat{p}^{\\pi}(\\widetilde{Y}=\\cdot,\\widetilde{Y}^{\\prime}=\\cdot)-\\mu^{\\pi}(\\widetilde{Y}=\\cdot)\\times\\mu^{\\pi}(\\widetilde{Y}^{\\prime}=\\cdot)\\big\\|_{\\mathrm{TV}}\\leq\\frac{4M}{L}+\\frac{8\\sqrt{D_{\\chi^{2}}(\\mu_{0}\\|\\mu^{\\pi})+1}}{L(1-\\lambda)\\cdot\\gamma^{|S|/2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 64}, {"type": "text", "text": "Here we use $\\widetilde{Y}$ and ${\\widetilde{Y}}^{\\prime}$ as placeholders for random variables $X_{L+1-S_{1}}$ and $Z_{-S_{2}}$ . We note that Lemma F.17 can be applied to any subsets of $X_{L+1-M:L+1}$ and $(z,Z)$ . Therefore, combining (F.16), (F.17), and (F.18), we conclude that ", "page_idx": 64}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\frac{1}{L-M}\\displaystyle\\sum_{l=M+1}^{L}\\mathbb{E}\\bigg[\\prod_{h\\in S}\\langle v_{l}^{(h)},\\tilde{v}_{L+1}^{(h)}\\rangle\\bigg]-\\mathbb{E}_{(x,X),(z,Z)\\sim\\mu^{\\pi}\\times\\mu^{\\pi}}\\bigg[\\bigg(\\prod_{h\\in S}\\langle\\tilde{v}^{(h)}(Z),v^{(h)}(X)\\rangle\\bigg)\\bigg]\\right|}\\\\ &{\\quad\\le\\displaystyle\\frac{4M}{L}+\\frac{8\\sqrt{D_{\\chi^{2}}(\\mu_{0}\\,\\|\\,\\mu^{\\pi})+1}}{L(1-\\lambda)\\gamma^{|S|/2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 64}, {"type": "text", "text": "Note that the second upper bound is dominated by the previous one. This completes the proof of LemmaF.5. \u53e3 Lemma F.6 provides an approximation result using the definition of the modified $\\chi^{2}$ -mutual information. ", "page_idx": 64}, {"type": "text", "text": "", "page_idx": 64}, {"type": "text", "text": "Lemma F.6. Consider a fixed set $\\cal{S}\\,\\in\\,[H]_{\\leq D}$ . For any $h\\,\\in\\,S$ let $\\widetilde{\\sigma}^{(h)}$ and $\\sigma^{(h)}$ be two probabiliy disribuions over $[M]$ That is for any $i,j\\ \\in\\ [M]$ we have $\\widetilde{\\sigma}_{-i}^{(h)},\\sigma_{-j}^{(h)}~\\in~[0,1]$ and $\\begin{array}{r}{\\sum_{i=1}^{M}\\widetilde{\\sigma}_{-i}^{(h)}\\;=\\;\\sum_{j=1}^{M}\\sigma_{-j}^{(h)}\\;=\\;1}\\end{array}$ Morever welel $(z,Z)\\ =\\ (z,z_{-M},\\ldots,z_{-1})\\ \\in\\ \\mathcal{X}^{M+1}$ and $(x,X)\\,=\\,(x,x_{-M},\\ldots,x_{-1})\\,\\in\\,\\mathcal{X}^{M+1}$ be two independent random variables sampled from the stationary distribution $\\mu^{\\pi}$ . We define random variables $\\widetilde{v}^{(h)}(Z)$ and $v^{(h)}(X)$ as ", "page_idx": 64}, {"type": "equation", "text": "$$\n\\widetilde v^{(h)}(Z):=\\sum_{i\\in[M]}\\widetilde\\sigma_{-i}^{(h)}\\cdot z_{-i},\\qquad a n d\\qquad v^{(h)}(X):=\\sum_{j\\in[M]}\\sigma_{-j}^{(h)}\\cdot x_{-j}.\n$$", "text_format": "latex", "page_idx": 64}, {"type": "text", "text": "Let $(i_{h}^{\\star},j_{h}^{\\star})_{h\\in S}$ be anyfixed collectionofindex pairs, whre $i_{h}^{\\star}\\in[M]$ and $j_{h}^{\\star}\\in[M]$ for all $h\\in S$ We define quantities $A$ and $B$ as ", "page_idx": 64}, {"type": "equation", "text": "$$\nA:=\\mathbb{E}_{\\pi,(x,X),(z,Z)\\sim\\mu^{\\pi}\\times\\mu^{\\pi}}\\bigg[\\bigg(\\sum_{k\\in[d]}\\frac{\\mathbb{1}(x=z=e_{k})}{\\mu^{\\pi}(e_{k})}-1\\bigg)\\cdot\\prod_{h\\in\\mathcal{S}}\\langle\\tilde{v}^{(h)}(Z),v^{(h)}(X)\\rangle,\\bigg],\n$$", "text_format": "latex", "page_idx": 64}, {"type": "equation", "text": "$$\nB:=\\mathbb{E}_{\\pi,(x,X),(z,Z)\\sim\\mu^{\\pi}\\times\\mu^{\\pi}}\\left[\\prod_{h\\in\\mathcal{S}}\\mathbb{1}(x_{-i_{h}^{\\star}}=z_{-j_{h}^{\\star}})\\cdot\\left(\\sum_{k=1}^{d}\\frac{\\mathbb{1}(x=z=e_{k})}{\\mu^{\\pi}(e_{k})}-1\\right)\\right].\n$$", "text_format": "latex", "page_idx": 64}, {"type": "text", "text": "Under Assumption 3.5, it holds that ", "page_idx": 64}, {"type": "equation", "text": "$$\n\\left|\\mathbb{E}_{\\pi}\\left[A\\right]-\\prod_{h\\in\\mathcal{S}}\\sigma_{-i_{h}^{\\star}}^{(h)}\\widetilde{\\sigma}_{-j_{h}^{\\star}}^{(h)}\\cdot B\\right|\\le\\left(1-\\prod_{h\\in\\mathcal{S}}\\sigma_{-i_{h}^{\\star}}^{(h)}\\widetilde{\\sigma}_{-j_{h}^{\\star}}^{(h)}\\right)\\cdot I_{\\chi^{2}}(\\mathcal{S}^{\\star}).\n$$", "text_format": "latex", "page_idx": 64}, {"type": "text", "text": "Proof of Lemma $F.6.$ .To simplify the notation, we define a signal set $\\Gamma(S)$ and an error set $\\overline{\\Gamma}(\\boldsymbol{S})$ as ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\Gamma({\\cal{S}}):=\\{(i_{h}^{\\star},j_{h}^{\\star})_{h\\in{\\cal{S}}}\\}\\,,\\qquad\\bar{\\Gamma}({\\cal{S}}):=\\Big\\{{(i_{h},j_{h})_{h\\in{\\cal{S}}}\\in([M]\\times[M])^{|{\\cal{S}}|}}\\Big\\}\\setminus\\Gamma({\\cal{S}}).\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "Similar to (F.10), we can write $\\mathbb{E}_{\\pi}[A]$ as ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\mathfrak{c}_{\\pi}[A]=\\mathbb{E}_{\\pi,(x,X),(z,Z)\\sim\\mu^{\\pi}\\times\\mu^{\\pi}}\\bigg[\\sum_{\\{(i_{h},j_{h})\\}_{h\\in\\mathcal{S}}}\\prod_{h\\in\\mathcal{S}}\\sigma_{-i_{h}}^{(h)}\\widetilde{\\sigma}_{-j_{h}}^{(h)}\\cdot\\mathbb{I}(x_{-i_{h}}=z_{-j_{h}})\\cdot\\bigg(\\sum_{k\\in[d]}\\frac{\\mathbb{I}(x=z=e_{k})}{\\mu^{\\pi}(e_{k})}\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "where we exchange the order of product and summation. Using the notation $\\overline{\\Gamma}(\\cal S)$ ,we can split the summation into two parts: ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\xi_{\\pi}[A]=\\mathbb{E}_{\\pi,(x,X),(z,Z)\\sim\\mu^{\\tau}\\times\\mu^{\\tau}}\\left[\\prod_{h\\in S}\\sigma_{-i_{h}^{*}}^{(h)}\\tilde{\\sigma}_{-j_{h}^{*}}^{(h)}\\cdot\\mathbb{I}(x_{-i_{h}^{*}}=z_{-j_{h}^{*}})\\cdot\\left(\\sum_{k\\in[d]}\\frac{\\mathbb{I}(x=z=e_{k})}{\\mu^{\\tau}(e_{k})}-1\\right)\\right]}}\\\\ {{\\displaystyle\\qquad+\\mathbb{E}_{\\pi,(x,X),(z,Z)\\sim\\mu^{\\tau}\\times\\mu^{\\tau}}\\left[\\sum_{(i_{h},j_{h})\\geqslant k\\in S\\in\\tilde{\\Gamma}(S)}\\prod_{h\\in S}\\sigma_{-i_{h}}^{(h)}\\tilde{\\sigma}_{-j_{h}}^{(h)}\\cdot\\mathbb{I}(x_{-i_{h}}=z_{-j_{h}})\\left(\\sum_{k\\in[d]}\\frac{\\mathbb{I}(x=z)}{\\mu^{\\tau}(e_{k})}-\\sum_{k\\in[d]}\\prod_{h\\in S}\\frac{\\mathbb{I}(x_{h}^{*})}{\\mu^{\\tau}(e_{k})}\\right)\\right]}}\\\\ {{\\displaystyle=\\prod_{h\\in S}\\sigma_{-i_{h}^{(h)}}^{(h)}\\tilde{\\sigma}_{-j_{h}^{*}}^{(h)}\\cdot B}}\\\\ {{\\displaystyle\\qquad+\\mathbb{E}_{\\pi,(x,X),(z,Z)\\sim\\mu^{\\tau}\\times\\mu^{\\tau}}\\left[\\sum_{(i_{h},j_{h})\\neq s\\in\\tilde{\\Gamma}(S)}\\prod_{h\\in S}\\sigma_{-i_{h}}^{(h)}\\tilde{\\sigma}_{-j_{h}}^{(h)}\\cdot\\mathbb{I}(x_{-i_{h}}=z_{-j_{h}})\\left(\\sum_{k\\in[d]}\\frac{\\mathbb{I}(x=z)}{\\mu^{\\tau}(e_{k})}-\\sum_{k\\in[d]}\\prod_{h\\in S}\\sigma_{-k\\in\\tilde{\\Gamma}(S)}^{(h)}\\right)\\right]}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "Here last equality holds by the definition of the $B$ and the fact that $\\widetilde{\\sigma}^{(h)}$ and $\\sigma^{(h)}$ are fixed vectors. ", "page_idx": 65}, {"type": "text", "text": "Therefore, to prove this lemma, it suffices to upper bound the second term above. To this end, we apply Lemma F.7 stated below for any fixed set of indices $(i_{h},j_{h})_{h\\in\\cal S}\\,\\in\\,{\\bar{\\Gamma}}({\\cal S})$ . Specifically, let $\\bar{S_{1}}\\bar{=}\\ \\{i_{h}\\colon h\\ \\in\\ S\\}$ and $S_{2}\\,=\\,\\{j_{h}\\colon h\\,\\in\\,S\\}$ denote the unique values of $(i_{h})_{h\\in\\mathcal{S}}$ and $(j_{h})_{h\\in S}$ Lemma F.7 implies that ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\pi,(x,X),(z,Z)\\sim\\mu^{\\pi}\\times\\mu^{\\pi}}\\bigg[\\prod_{h\\in\\mathcal{S}}\\mathbb{1}(x_{-i_{h}}=z_{-j_{h}})\\cdot\\bigg(\\sum_{k\\in[d]}\\frac{\\mathbb{1}(x=z=e_{k})}{\\mu^{\\pi}(z=e_{k})}-1\\bigg)\\bigg]\\leq I_{\\chi^{2}}(\\mathcal{S}^{\\star}).\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "Combining (F.19) with the fact that ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\sum_{(i_{h},j_{h})_{h\\in\\mathcal{S}}\\in\\bar{\\Gamma}(\\mathcal{S})}\\prod_{h\\in\\mathcal{S}}\\sigma_{-i_{h}}^{(h)}\\widetilde{\\sigma}_{-j_{h}}^{(h)}=1-\\prod_{h\\in\\mathcal{S}}\\sigma_{-i_{h}^{\\star}}^{(h)}\\widetilde{\\sigma}_{-j_{h}^{\\star}}^{(h)},\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "the desiredtermis bounded aboveby(1-Ies ) \\*))-Ix2(S\\*), which concludes the proof. ", "page_idx": 65}, {"type": "text", "text": "Lemma F.7. Let $\\mathcal{S}\\in[H]_{\\leq D}$ be a fixed subset and let $\\{(i_{h},j_{h})\\}_{h\\in S}$ be a fixed collection of index pairs, where $i_{h},j_{h}\\in[M]$ for all $h\\,\\in\\,S$ Let $S_{1}=\\{i_{h}\\colon h\\in S\\}$ and $S_{2}=\\{j_{h}\\colon h\\in S\\}$ denote the unique values of $(i_{h})_{h\\in S}$ and $(j_{h})_{h\\in S}$ . We let $(z,Z)\\;=\\;(z,z_{-M},\\ldots,z_{-1})\\;\\in\\;\\mathcal{X}^{M+1}$ and $(x,X)\\,=\\,(x,x_{-M},\\ldots,x_{-1})\\,\\in\\,\\mathcal{X}^{M+1}$ betwoindependent randomvariables sampled fromthe stationary distribution $\\mu^{\\pi}$ , where $\\pi$ is the transition kernel of the Markov chain and is sampled from prior $\\mathcal{P}$ . If Assumption 3.5 holds, it follows that ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\mathbb{E}_{\\pi\\sim\\mathcal{P},(x,X),(z,Z)\\sim\\mu^{\\pi}\\times\\mu^{\\pi}}\\left[\\prod_{h\\in\\mathcal{S}}\\mathbb{1}(x_{-i_{h}}=z_{-j_{h}})\\cdot\\Bigg(\\sum_{k\\in[d]}\\frac{\\mathbb{1}(x=z=e_{k})}{\\mu^{\\pi}(z=e_{k})}-1\\Bigg)\\right]}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\displaystyle\\frac{1}{2}\\left(\\widetilde{I}_{\\chi^{2}}(S_{1})+\\widetilde{I}_{\\chi^{2}}(S_{2})\\right)\\leq\\tilde{I}_{\\chi^{2}}(S^{\\star}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "where $\\widetilde{I}_{\\chi^{2}}(S)$ is.themodified $\\chi^{2}$ -mutual information defined in Definition 3.1 and $\\begin{array}{r l}{S^{\\star}}&{{}=}\\end{array}$ $\\mathrm{argmax}_{S\\in[H]_{\\leq D}}\\,\\widetilde{I}_{\\chi^{2}}(S)$ ", "page_idx": 65}, {"type": "text", "text": "Proof of Lemma $F7.$ We first note that it is allowed $|S_{1}|\\neq|S_{2}|$ as there could be duplicate values in both $(i_{h})_{h\\in\\mathcal{S}}$ and $(j_{h})_{h\\in\\mathcal{S}}$ , while $S_{1}$ and $S_{2}$ are the unique values. In the sequel, we lIet $X_{-S_{1}}$ denote $\\{x_{-i_{h}}\\}_{h\\in S}$ and let $Z_{-S_{2}}$ denote $\\{z_{-j_{h}}\\}_{h\\in S}$ , where repeated elements are removed. Moreover, we let ", "page_idx": 65}, {"type": "text", "text": "$\\{X_{-S_{1}}=Z_{-S_{2}}\\}$ be the event that $x_{-i_{h}}=z_{-j_{h}}$ for all $h\\in S$ . Notice that $\\begin{array}{r}{\\prod_{h\\in\\mathcal{S}}\\mathbb{1}(x_{-i_{h}}=z_{-j_{h}})=}\\end{array}$ $\\mathbb{1}(X_{-S_{1}}=Z_{-S_{2}})$ . Then, we have ", "page_idx": 66}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{E}_{\\pi,(x,X),(z,Z)\\sim\\mu^{\\tau}\\times\\mu^{\\tau}}\\left[\\prod_{h\\in\\mathcal{S}_{h}}\\mathbb{I}(x_{-i_{h}}=z_{-j_{h}})\\cdot\\left(\\sum_{k\\in[d]}\\frac{\\mathbb{I}(x=z=e_{k})}{\\mu^{\\pi}(z=e_{k})}-1\\right)\\right]}}&{\\quad\\mathrm{~(F.20~\\'~\\#)~}}\\\\ &{=\\mathbb{E}_{\\pi,(x,X),(z,Z)\\sim\\mu^{\\tau}\\times\\mu^{\\tau}}\\left[\\mathbb{I}(X_{-\\mathcal{S}_{1}}=Z_{-\\mathcal{S}_{2}})\\cdot\\left(\\sum_{k\\in[d]}\\frac{\\mathbb{I}(x=z=e_{k})}{\\mu^{\\pi}(z=e_{k})}-1\\right)\\right]}\\\\ &{=\\mathbb{E}_{\\pi,(x,X),(z,Z)\\sim\\mu^{\\tau}\\times\\mu^{\\tau}}\\left[\\left(\\sum_{k\\in[d]}\\frac{\\mu^{\\pi}(x=e_{k}|X_{-\\mathcal{S}_{1}})\\cdot\\mu^{\\pi}(z=e_{k}|Z_{-\\mathcal{S}_{2}})}{\\mu^{\\pi}(z=e_{k})}-1\\right)\\cdot\\mathbb{I}(X_{-\\mathcal{S}_{1}}=Z_{-\\mathcal{S}_{1}})\\right]}\\\\ &{=\\mathbb{E}_{\\pi,(X,Z)\\sim\\mu^{\\tau}\\times\\mu^{\\tau}}\\left[\\sum_{k\\in[d]}\\left(\\frac{\\mu^{\\pi}(x=e_{k}|X_{-\\mathcal{S}_{1}})}{\\mu^{\\pi}(x=e_{k})}-1\\right)\\cdot\\left(\\frac{\\mu^{\\pi}(z=e_{k}|Z_{-\\mathcal{S}_{2}})}{\\mu^{\\pi}(z=e_{k})}-1\\right)\\cdot\\mu^{\\pi}(z=e_{k})\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 66}, {"type": "text", "text": "Here in the second equality, we take a conditional expectation given $X_{-S_{1}}$ and $Z_{-S_{2}}$ . The last equality can be verified by direct computation. To simplify the expectation above, we aim to transform the indicator of $\\mathbb{1}(\\bar{X_{-S_{1}}}=Z_{-S_{2}}\\bar{)}$ into probabilities involving $X_{-S_{1}}$ and $Z_{-S_{2}}$ . To this end, we need to explicitly enumerate all possible values that $X_{-S_{1}}$ and $Z_{-S_{2}}$ can take. This is challenging, as there may be duplicated values in both $i_{h}$ and $j_{h}$ , and thus $X_{-S_{1}}$ and $Z_{-S_{2}}$ can have different sizes. However, since $S_{1}$ is a \u201creduction\u201d of $\\{i_{h}\\}_{h\\in S}$ , we can revert to the original space and consider $E=(E_{h})_{h\\in\\mathcal{S}}\\in\\mathcal{X}^{|\\mathcal{S}|}$ that respects the reduction from $\\{i_{h}\\}_{h\\in S}$ to $S_{1}$ . Here each $E_{h}$ is the value $x_{i_{h}}$ takes. In other words, with $(i_{h})_{h\\in S}$ that might have duplicated values, we consider the values taken by $(x_{i_{h}})_{h\\in S}$ , with duplicates allowed. And $E$ has the same duplication structure as $(x_{i_{h}})_{h\\in S}$ . In the following, we describe these values by introducing the notion of compatibility. ", "page_idx": 66}, {"type": "text", "text": "Definition F8 (Compatible Value Set). We say that $E\\in\\mathcal{X}^{|S|}$ is compatible with. $(i_{h})_{h\\in\\mathcal{S}}$ if, for any $h\\neq h^{\\prime}$ such that $i_{h}=i_{h^{\\prime}}$ , we have $E_{h}=E_{h^{\\prime}}$ In other words, the unique values in $E$ can be indexed by $\\{i_{h}\\}_{h\\in S}=S_{1}$ $E$ is compatible with $(i_{h})_{h\\in\\mathcal{S}}$ ", "page_idx": 66}, {"type": "text", "text": "By this definition, $E$ is compatible with $(i_{h})_{h\\in\\mathcal{S}}$ if it respect duplication pattern of $(x_{i_{h}})_{h\\in S}$ If $i_{h}=i_{h^{\\prime}}$ , then we know that $x_{i_{h}}$ and $x_{i_{h^{\\prime}}}$ is the same token. Since $x_{i_{h}}$ and $x_{i_{h^{\\prime}}}$ take values $E_{h}$ and $E_{h^{\\prime}}$ , we must have $E_{h}\\,=\\,E_{h^{\\prime}}$ . As a concrete example, suppose $S\\,=\\,\\{1,2,3\\}$ , and the values of $(i_{h})_{h\\in\\mathcal{S}}$ are given by $(i_{1},i_{2},i_{3})=(1,2,1)$ . Therefore, we have $S_{1}=\\mathrm{\\bar{\\{}}1,2\\}$ , which contains the unique values of $(i_{1},i_{2},i_{3})$ . Now, let $E=(E_{1},E_{2},E_{3})$ . For $E$ to be compatible with $(i_{h})_{h\\in\\mathcal{S}}$ ,we must have $E_{1}=\\dot{E}_{3}$ since $i_{1}=i_{3}$ . There is no restriction on $E_{2}$ . So, a compatible value set for this example could be $E=(a,b,a)$ , where $a$ and $b$ are elements of $\\mathcal{X}$ ", "page_idx": 66}, {"type": "text", "text": "In the sequel, we define $\\mathcal{E}$ as the set of vectors in $\\mathcal{X}^{\\vert S\\vert}$ that are compatible with both $\\{i_{h}\\}_{h\\in S}$ and $\\{j_{h}\\}_{h\\in\\mathcal{S}}$ ,i.e., ", "page_idx": 66}, {"type": "equation", "text": "$$\n\\mathcal{E}=\\left\\{E\\in\\mathcal{X}^{|\\mathcal{S}|}\\mid E\\mathrm{~is~compatible~with~both~}(i_{h})_{h\\in\\mathcal{S}}\\mathrm{~and~}(j_{h})_{h\\in\\mathcal{S}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 66}, {"type": "text", "text": "The compatibility condition allows us to assign $x_{-i_{h}},z_{-j_{h}}$ the value $E_{h}$ for all $h\\in S$ when $E\\in{\\mathcal{E}}$ Under this assignment, the constraint $X_{-S_{1}}=Z_{-S_{2}}$ is automatically satisfied. We use the notation $\\{X_{-S_{1}}=E\\}$ to denote the event that $x_{-i_{h}}=E_{h}$ for all $h\\in S$ , and similarly for $Z_{-S_{2}}=E$ . In particular, we are able to rewrite the indicator $\\mathbb{1}(X_{-S_{1}}=Z_{-S_{2}}$ )as $\\begin{array}{r}{\\sum_{E\\in\\mathcal{E}}\\mathbb{1}(\\dot{X}_{-}s_{1}=\\tilde{E},Z_{-}s_{2}=}\\end{array}$ ", "page_idx": 66}, {"type": "text", "text": "$E$ ). Then we can rewrite (F.20) by separating $X_{-S_{1}}$ and $Z_{-S_{2}}$ as ", "page_idx": 67}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\pi,(x,\\bar{x}),(\\bar{z},Z)\\sim\\mu^{\\pi}\\times\\mu^{\\pi}}\\Bigg[\\underset{h\\in\\mathcal{S}}{\\prod}\\!\\Bigg(\\!\\frac{1}{\\mu^{\\pi}(x-i_{h})}\\!\\cdot\\!\\left(\\!\\sum_{k\\in[\\bar{z}]}\\frac{1\\!(x=\\bar{z}=\\epsilon_{k})}{\\mu^{\\pi}(z=\\epsilon_{k})}-1\\!\\right)\\!\\Bigg]}\\\\ &{\\quad=\\mathbb{E}_{\\pi}\\Bigg[\\!\\sum_{E\\in\\mathcal{E}}\\sum_{k\\in[\\bar{z}]}\\!\\Bigg(\\!\\frac{\\mu^{\\pi}(x=\\epsilon_{k}|X_{-\\mathcal{S}_{1}}=E)}{\\mu^{\\pi}(x=\\epsilon_{k})}-1\\!\\Bigg)\\cdot\\!\\left(\\!\\frac{\\mu^{\\pi}(z=\\epsilon_{k}|Z_{-\\mathcal{S}_{2}}=E)}{\\mu^{\\pi}(z=\\epsilon_{k})}-1\\!\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\cdot\\mu^{\\pi}(X_{-\\mathcal{S}_{1}}=E)\\cdot\\mu^{\\pi}(Z_{-\\mathcal{S}_{2}}=E)\\cdot\\mu^{\\pi}(z=\\epsilon_{k})\\Bigg]}\\\\ &{\\quad\\quad\\le\\frac{1}{2}\\mathbb{E}_{\\pi}\\Bigg[\\underset{E\\in\\mathcal{E}}{\\sum}\\sum_{k\\in[\\bar{z}]}\\!\\Bigg(\\!\\frac{\\mu^{\\pi}(x=\\epsilon_{k}|X_{-\\mathcal{S}_{1}})}{\\mu^{\\pi}(x=\\epsilon_{k})}-1\\!\\Bigg)^{2}\\cdot\\mu^{\\pi}(z=\\epsilon_{k})\\cdot\\big(\\mu^{\\pi}(X_{-\\mathcal{S}_{1}}=E)\\big)^{2}\\Bigg]}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad+\\frac{1}{2}\\mathbb{E}_{\\pi}\\Bigg[\\underset{E\\in\\mathcal{E}_{k}}{\\sum}\\!\\Bigg(\\!\\frac{\\mu^{\\pi}(z=\\epsilon_{k}|Z_{-\\mathcal{S}})}{\\mu^{\\pi}(z=\\epsilon_{k})}-1\\!\\Bigg)^{2}\\cdot\\mu^{\\pi}(z=\\epsilon_{k})\\cdot\\big(\\mu^{\\pi}(Z_{-\\mathcal{S}_{2}}=E)\\big)^{2}\\Bigg]\\cdot\\~(\\mathbb{F}21)}\\end{array}\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "where in the last inequality, we apply $a b\\leq a^{2}+b^{2}/2$ ", "page_idx": 67}, {"type": "text", "text": "Next, for each $E\\in{\\mathcal{E}}$ , consider $E^{\\prime}=(E_{i}^{\\prime})_{i\\in S_{1}}$ such that ", "page_idx": 67}, {"type": "equation", "text": "$$\nE_{i_{h}}^{\\prime}=E_{h},\\quad\\forall h\\in\\mathcal{S}.\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "Note that for each $E\\in{\\mathcal{E}}$ $E^{\\prime}$ must exist and is unique. The existence follows from the compatibility definition, which allows us to index all the unique values in $E$ by restricting the indices to the set $S_{1}$ . The uniqueness is due to the fact that (F.22) completely determines all the values in $E^{\\prime}$ because enumerating over $i_{h}$ for $h\\in S$ is just the same as enumerating over $i$ for $i\\in S_{1}$ . In fact, $E^{\\prime}$ contains all the unique values of $E$ . In the above example, we have $S_{1}=\\{1,2\\}$ and thus $E^{\\prime}=(a,b)$ when $\\boldsymbol{E}=(a,b,\\bar{a})$ ", "page_idx": 67}, {"type": "text", "text": "Since $E^{\\prime}$ is uniquely defined based on $E$ , we are able to define an operator $\\mathcal{I}_{1}$ that maps $E\\in{\\mathcal{E}}$ to $E^{\\prime}\\in\\mathcal{X}^{|S_{1}|}$ according to the mapping given in F.22). Let $\\mathcal{I}_{1}(\\mathcal{E})$ be the image of $\\mathcal{E}$ under $\\mathcal{I}_{1}$ It is important to note that for each $\\bar{E^{\\prime}}\\in\\bar{\\mathcal{I}}_{1}(\\mathcal{E})$ , there is also a unique pre-image $E\\in{\\mathcal{E}}$ such that $\\mathcal{I}_{1}(\\bar{E})=E^{\\prime}$ according to the rule (F.22). Therefore, $\\mathcal{I}_{1}$ is an one-to-one mapping from $\\mathcal{E}$ to $\\mathcal{I}_{1}(\\mathcal{E})$ In the following, for any $E^{\\prime}\\in\\mathcal{I}_{1}(\\mathcal{E})$ , we denote by $\\{X_{-S_{1}}=E^{\\prime}\\}$ the event where $x_{-i}=E_{i}^{\\prime}$ for all $i\\in S_{1}$ . Equivalently, we have $x_{-i_{h}}=E_{i_{h}}^{\\prime}=E_{h}$ for all $h\\in S$ Thus, the event $\\{X_{-S_{1}}=\\check{E^{\\prime}}\\}$ is exactly the same as $\\{X_{-S_{1}}=E\\}$ introduced above. Therefore, the first term on the right hand side of (F.21) can be reformulated as ", "page_idx": 67}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overset{!}{\\leq}\\mathbb{E}_{\\boldsymbol{\\pi}}\\bigg[\\displaystyle\\sum_{E\\in\\mathcal{E}}\\displaystyle\\sum_{k\\in[d]}\\left(\\frac{\\mu^{\\pi}(x=e_{k}|X_{-\\mathcal{S}_{1}})}{\\mu^{\\pi}(x=e_{k})}-1\\right)^{2}\\cdot\\mu^{\\pi}(z=e_{k})\\cdot\\mu^{\\pi}(X_{-\\mathcal{S}_{1}}=E)^{2}\\bigg]}\\\\ &{\\quad=\\frac{1}{2}\\cdot\\mathbb{E}_{\\boldsymbol{\\pi}}\\bigg[\\displaystyle\\sum_{E^{\\prime}\\in\\mathcal{I}_{1}(\\mathcal{E})}\\displaystyle\\sum_{k\\in[d]}\\left(\\frac{\\mu^{\\pi}(x=e_{k}|X_{-\\mathcal{S}_{1}})}{\\mu^{\\pi}(x=e_{k})}-1\\right)^{2}\\cdot\\mu^{\\pi}(z=e_{k})\\cdot\\big(\\mu^{\\pi}(X_{-\\mathcal{S}_{1}}=E^{\\prime})\\big)^{2}\\bigg]}\\\\ &{\\quad\\leq\\frac{1}{2}\\cdot\\mathbb{E}_{\\boldsymbol{\\pi}}\\bigg[\\displaystyle\\sum_{E^{\\prime}\\in\\mathcal{X}^{|S_{1}|}}\\sum_{k\\in[d]}\\left(\\frac{\\mu^{\\pi}(x=e_{k}|X_{-\\mathcal{S}_{1}})}{\\mu^{\\pi}(x=e_{k})}-1\\right)^{2}\\cdot\\mu^{\\pi}(z=e_{k})\\cdot\\big(\\mu^{\\pi}(X_{-\\mathcal{S}_{1}}=E^{\\prime})\\big)^{2}\\bigg]=\\frac{1}{2}\\tilde{I}_{x^{2}}(\\boldsymbol{\\pi})}\\end{array}\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "where the equality follows from the bijection between $\\mathcal{E}$ and ${\\mathcal{I}}_{1}({\\mathcal{E}})$ , and the last inequality holds by noting that $\\mathcal{J}_{1}(\\mathcal{E})\\subseteq\\mathcal{X}^{|{S_{1}}|}$ . The last equality follows from the definition of the modified mutual information. The argument for the second term on the right hand side of (F.21) is similar, and we hence conclude that ", "page_idx": 67}, {"type": "equation", "text": "$$\n\\mathbb{\\tilde{C}}_{\\pi,(x,X),(z,Z)\\sim\\mu^{\\pi}\\times\\mu^{\\pi}}\\left[\\prod_{h\\in\\mathcal{S}}\\mathbb{1}(x_{-i_{h}}=z_{-j_{h}})\\cdot\\left(\\sum_{k\\in[d]}\\frac{\\mathbb{1}(x=z=e_{k})}{\\mu^{\\pi}(z=e_{k})}-1\\right)\\right]\\leq\\frac{1}{2}\\tilde{I}_{\\mathcal{X}^{2}}(S_{1})+\\frac{1}{2}\\tilde{I}_{\\mathcal{X}^{2}}(S_{2})\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "Lastly, note that $\\widetilde{I}_{\\chi^{2}}(S)\\le\\widetilde{I}_{\\chi^{2}}(S^{\\star})$ for any $\\mathcal{S}\\in[H]_{\\leq D}$ by the optimality of $S^{\\star}$ . Hence, we complete the proof of Lemma F.7. \u53e3 ", "page_idx": 67}, {"type": "text", "text": "Lemma F.9 quantifies the approximation error from $\\sigma_{l}\\approx\\sigma_{l}^{\\star}$ and $y(k)\\approx y^{\\star}(k)$ for Stage IIl. ", "page_idx": 67}, {"type": "text", "text": "Lemma F.9. For the transformer model defined in (2.5), define two quantities $f_{1}$ and $f_{2}$ as ", "page_idx": 68}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathfrak{r}_{1}:=\\mathbb{E}\\left[\\sum_{l=M+1}^{L}\\sigma_{l}\\sum_{k=1}^{d}\\biggl(\\frac{\\mathbb{I}(x_{L+1}=x_{l}=e_{k})}{y(k)+\\varepsilon}-\\frac{y(k)\\,\\mathbb{I}(x_{L+1}=e_{k})}{y(k)+\\varepsilon}\\biggr)\\cdot\\prod_{h\\in S^{\\star}}\\langle v_{l}^{(h)},v_{L+1}^{(h)}\\rangle\\right],}\\\\ {\\displaystyle\\mathfrak{r}_{2}:=\\mathbb{E}\\left[\\sum_{l=M+1}^{L}\\sigma_{l}^{\\star}\\sum_{k=1}^{d}\\biggl(\\frac{\\mathbb{I}(x_{L+1}=x_{l}=e_{k})}{y^{\\star}(k)+\\varepsilon}-\\frac{y^{\\star}(k)\\,\\mathbb{I}(x_{L+1}=e_{k})}{y^{\\star}(k)+\\varepsilon}\\biggr)\\cdot\\prod_{h\\in S^{\\star}}\\mathbb{I}(x_{l-h}=x_{L+1-h})\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "wheretheexpectationistakenoverall therandomnessinthedata,and ", "page_idx": 68}, {"type": "equation", "text": "$$\n\\sigma_{l}^{\\star}:=\\frac{\\exp\\big(a\\cdot\\prod_{h\\in\\mathcal{S}^{\\star}}\\mathbb{1}\\big(x_{l-h}=x_{L+1-h}\\big)\\big)}{\\sum_{l^{\\prime}=1}^{L}\\exp\\big(a\\cdot\\prod_{h\\in\\mathcal{S}^{\\star}}\\mathbb{1}\\big(x_{l^{\\prime}-h}=x_{L+1-h}\\big)\\big)},\\quad y^{\\star}(k):=\\sum_{l=M+1}^{L}\\sigma_{l}^{\\star}\\,\\mathbb{1}\\big(x_{l}=e_{k}\\big),\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "with $S^{\\star}$ is the optimal information set. Under Assumption 3.5, it holds that ", "page_idx": 68}, {"type": "equation", "text": "$$\n|f_{1}-f_{2}|\\leq12\\cdot(1+a\\varepsilon^{-1})\\cdot(\\Delta_{1}+\\Delta_{2}),\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "where $\\Delta_{1}:=1-p_{S^{\\star}}$ and $\\begin{array}{r}{\\Delta_{2}:=1-\\prod_{h\\in S^{\\star}}(\\sigma_{-h}^{(h)})^{2}}\\end{array}$ ", "page_idx": 68}, {"type": "text", "text": "Proof. We separate the approximation error into three parts $|f_{1}-f_{2}|\\leq\\mathrm{err}_{1}+\\mathrm{err}_{2}+\\mathrm{err}_{3}$ ,which are explained in detail as follows. ", "page_idx": 68}, {"type": "text", "text": "The First Error Term. Here,the frst errr $\\mathrm{err1}$ captures the error of replacing $\\begin{array}{r}{\\prod_{h\\in S^{\\star}}\\mathbb{1}(x_{l-h}=}\\end{array}$ $x_{L+1-h})$ With $\\textstyle\\prod_{h\\in S^{\\star}}\\langle v_{l}^{(h)},v_{L+1}^{(h)}\\rangle$ .n $f_{2}$ ", "page_idx": 68}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{err}_{1}:=\\Bigg|\\mathbb{E}\\Bigg[\\underset{l=M+1}{\\sum}\\sigma_{l}\\cdot\\underset{k\\in[d]}{\\sum}\\bigg(\\frac{\\mathbb{1}\\left(x_{L+1}=x_{l}=e_{k}\\right)}{y^{\\star}(k)+\\varepsilon}-\\frac{y^{\\star}(k)\\,\\mathbb{1}\\left(x_{L+1}=e_{k}\\right)}{y^{\\star}(k)+\\varepsilon}\\bigg)\\cdot}\\\\ &{\\qquad\\qquad\\qquad\\cdot\\left(\\underset{h\\in\\mathcal{S}^{\\star}}{\\prod}\\langle v_{l}^{(h)},v_{L+1}^{(h)}\\rangle-\\underset{h\\in\\mathcal{S}^{\\star}}{\\prod}\\mathbb{1}\\big(x_{l-h}=x_{L+1-h}\\big)\\right)\\Bigg]\\Bigg|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "Using Lemma F.2, we have ", "page_idx": 68}, {"type": "equation", "text": "$$\n\\bigg|\\sum_{l=M+1}^{L}\\sigma_{l}\\cdot\\sum_{k\\in[d]}\\Big(\\frac{\\mathbb{1}(x_{L+1}=x_{l}=e_{k})}{y^{\\star}(k)+\\varepsilon}-\\frac{y^{\\star}(k)\\,\\mathbb{1}(x_{L+1}=e_{k})}{y^{\\star}(k)+\\varepsilon}\\Big)\\bigg|\\le2.\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "Then using Lemma F.1, we conclude that ", "page_idx": 68}, {"type": "equation", "text": "$$\n\\mathrm{err}_{1}\\leq2\\operatorname*{sup}_{l\\in[L]}\\left|\\prod_{h\\in\\mathcal{S}^{\\star}}\\langle v_{l}^{(h)},v_{L+1}^{(h)}\\rangle-\\prod_{h\\in\\mathcal{S}^{\\star}}\\mathbb{1}(x_{l-h}=x_{L+1-h})\\right|\\leq2(\\Delta_{1}+\\Delta_{2}).\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "The Second Error Term. The second error term characterizes the difference in $\\sigma_{l}$ and $\\sigma_{l}^{\\star}$ ", "text_level": 1, "page_idx": 68}, {"type": "equation", "text": "$$\n\\mathrm{rr}_{2}:=\\bigg|\\mathbb{E}\\bigg[\\sum_{l=M+1}^{L}(\\sigma_{l}^{*}-\\sigma_{l})\\cdot\\sum_{k\\in[d]}\\left(\\frac{\\mathbb{I}(x_{L+1}=x_{l}=e_{k})}{y^{*}(k)+\\varepsilon}-\\frac{y^{*}(k)\\,\\mathbb{I}(x_{L+1}=e_{k})}{y^{*}(k)+\\varepsilon}\\right)\\cdot\\prod_{h\\in\\mathcal{S}^{\\star}}\\langle v_{l}^{(h)},v_{L+1}^{(h)}\\rangle\\bigg]\\,.\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "To characterize such an error, we invoke equation (53) in Lemma 5.1 of Chen et al. (2022). This lemma states that for $\\sigma$ and $\\sigma^{\\star}$ being the output of the softmax function with scaling parameters $a$ for $s$ and $s^{\\star}$ respectively, i.e., ", "page_idx": 68}, {"type": "equation", "text": "$$\n\\sigma=\\frac{\\exp(a s)}{\\sum_{l=M+1}^{L}\\exp(a s_{l})},\\qquad\\mathrm{and}\\qquad\\sigma^{\\star}=\\frac{\\exp(a s^{\\star})}{\\sum_{l=M+1}^{L}\\exp(a s_{l}^{\\star})},\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "it holds that $\\|{\\boldsymbol{\\sigma}}-{\\boldsymbol{\\sigma}}^{\\star}\\|_{1}\\leq4a\\cdot\\|{\\boldsymbol{s}}-{\\boldsymbol{s}}^{\\star}\\|_{\\infty}.$ Consequently, we have $\\|\\sigma-\\sigma^{\\star}\\|_{1}\\leq4a\\cdot(\\Delta_{1}+\\Delta_{2})$ by Lemma F.1. We notice that ", "page_idx": 68}, {"type": "equation", "text": "$$\n\\sum_{k\\in[d]}\\left(\\frac{\\mathbb{1}(x_{L+1}=x_{l}=e_{k})}{y^{\\star}(k)+\\varepsilon}-\\frac{y^{\\star}(k)\\,\\mathbb{1}(x_{L+1}=e_{k})}{y^{\\star}(k)+\\varepsilon}\\right)\\cdot\\prod_{h\\in\\mathcal{S}^{\\star}}\\langle v_{l}^{(h)},v_{L+1}^{(h)}\\rangle\\Bigg|\\le\\operatorname*{max}\\{\\varepsilon^{-1},1\\}=\\varepsilon^{-1}.\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "Thus, $\\mathrm{err_{2}}\\le4a\\varepsilon^{-1}\\cdot(\\Delta_{1}+\\Delta_{2})$ ", "page_idx": 68}, {"type": "text", "text": "The Third Error Term.  The last error term characterizes the difference between $y^{\\star}$ and $y$ ", "page_idx": 69}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathrm{err}_{3}:=\\left|\\mathbb{E}\\!\\left[\\displaystyle\\sum_{l=M+1}^{L}\\sigma_{l}\\cdot\\sum_{k\\in[d]}\\left(\\frac{1}{y^{\\star}(k)+\\varepsilon}-\\frac{1}{y(k)+\\varepsilon}\\right)\\cdot\\mathbb{I}(\\boldsymbol{x}_{L+1}=\\boldsymbol{x}_{l}=\\boldsymbol{e}_{k})\\cdot\\prod_{h\\in\\mathcal{S}^{\\star}}\\langle\\boldsymbol{v}_{l}^{(h)},\\boldsymbol{v}_{L+1}^{(h)}\\rangle\\right]\\right|}\\\\ &{}&{+\\left|\\mathbb{E}\\!\\left[\\displaystyle\\sum_{l=M+1}^{L}\\sigma_{l}\\cdot\\sum_{k\\in[d]}\\left(\\frac{y^{\\star}(k)}{y^{\\star}(k)+\\varepsilon}-\\frac{y(k)}{y(k)+\\varepsilon}\\right)\\cdot\\mathbb{I}(\\boldsymbol{x}_{L+1}=\\boldsymbol{e}_{k})\\cdot\\prod_{h\\in\\mathcal{S}^{\\star}}\\langle\\boldsymbol{v}_{l}^{(h)},\\boldsymbol{v}_{L+1}^{(h)}\\rangle\\right]\\right|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 69}, {"type": "text", "text": "By noting that $\\begin{array}{r}{y^{\\star}=\\sum_{l=M+1}^{L}\\sigma_{l}^{\\star}x_{l}}\\end{array}$ and $\\begin{array}{r}{y=\\sum_{l=M+1}^{L}\\sigma_{l}x_{l}}\\end{array}$ , we have ", "page_idx": 69}, {"type": "equation", "text": "$$\ny^{\\star}-y\\|_{1}=\\bigg\\|\\sum_{l=M+1}^{L}(\\sigma_{l}^{\\star}-\\sigma_{l})x_{l}\\bigg\\|_{1}\\leq\\sum_{l=M+1}^{L}|\\sigma_{l}^{\\star}-\\sigma_{l}|_{1}\\cdot\\|x_{l}\\|_{1}\\leq\\|\\sigma-\\sigma^{\\star}\\|_{1}\\leq4a\\cdot(\\Delta_{1}+\\Delta_{2}).\n$$", "text_format": "latex", "page_idx": 69}, {"type": "text", "text": "The first term of $\\mathrm{err}_{3}$ can be bounded by ", "page_idx": 69}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\mathbb{E}\\bigg[\\sum_{l=M+1}^{L}\\sigma_{l}\\cdot\\sum_{k\\in[d]}\\Big(\\frac{1}{y^{\\star}(k)+\\varepsilon}-\\frac{1}{y(k)+\\varepsilon}\\Big)\\cdot\\mathbb{I}(x_{L+1}=x_{l}=e_{k})\\cdot\\prod_{h\\in\\mathcal{S}^{\\star}}\\langle v_{l}^{(h)},v_{L+1}^{(h)}\\rangle\\bigg]\\bigg\\rvert}\\\\ &{\\quad\\le\\displaystyle\\sum_{k\\in[d]}\\frac{|y(k)-y^{\\star}(k)|}{(y^{\\star}(k)+\\varepsilon)(y(k)+\\varepsilon)}\\cdot y(k)\\,\\mathbb{I}(x_{L+1}=e_{k})\\le\\|y-y^{\\star}\\|_{1}\\cdot\\varepsilon^{-1}\\le4a\\varepsilon^{-1}(\\Delta_{1}+\\Delta_{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 69}, {"type": "text", "text": "Moreover, for the second term of $\\mathrm{err}_{3}$ , we have ", "page_idx": 69}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\mathbb{E}\\!\\left[\\displaystyle\\sum_{l=M+1}^{L}\\sigma_{l}\\cdot\\sum_{k\\in[d]}\\!\\left(\\!\\frac{y^{\\star}(k)}{y^{\\star}(k)+\\varepsilon}-\\frac{y(k)}{y(k)+\\varepsilon}\\right)\\cdot\\mathbb{1}(x_{L+1}=e_{k})\\!\\cdot\\!\\prod_{h\\in\\mathcal{S}^{\\star}}\\langle v_{l}^{(h)},v_{L+1}^{(h)}\\rangle\\!\\right]\\right|}\\\\ &{\\quad\\le\\displaystyle\\sum_{k\\in[d]}\\frac{|y(k)-y^{\\star}(k)|}{(y^{\\star}(k)+\\varepsilon)(y(k)+\\varepsilon)}\\cdot\\varepsilon\\cdot\\mathbb{1}(x_{L+1}=e_{k})\\le4a\\varepsilon^{-1}(\\Delta_{1}+\\Delta_{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 69}, {"type": "text", "text": "It then holds that ", "page_idx": 69}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|f_{1}-f_{2}|\\le\\mathrm{err}_{1}+\\mathrm{err}_{2}+\\mathrm{err}_{3}\\le2(\\Delta_{1}+\\Delta_{2})+4a\\varepsilon^{-1}(\\Delta_{1}+\\Delta_{2})+8a\\varepsilon^{-1}(\\Delta_{1}+\\Delta_{2})}\\\\ &{\\qquad\\qquad=12\\cdot(1+a\\varepsilon^{-1})\\cdot(\\Delta_{1}+\\Delta_{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 69}, {"type": "text", "text": "Therefore, we complete the proof of Lemma F.9. ", "page_idx": 69}, {"type": "text", "text": "Lemma F.10. Let us define for brevity ", "text_level": 1, "page_idx": 69}, {"type": "equation", "text": "$$\n\\widetilde{\\mu}_{X}^{\\pi}(z,Z)=\\widetilde{\\mu}^{\\pi}(z,Z\\,|\\,X_{L+1-S^{\\star}})=\\frac{\\mu^{\\pi}(z,Z)\\exp\\big(a\\cdot\\prod_{h\\in S^{\\star}}\\mathbb{1}\\big(z_{-h}=x_{L+1-h}\\big)\\big)}{\\sum_{z^{\\prime},Z^{\\prime}}\\mu^{\\pi}(z^{\\prime},Z^{\\prime})\\exp\\big(a\\cdot\\prod_{h\\in S^{\\star}}\\mathbb{1}\\big(z_{-h}^{\\prime}=x_{L+1-h}\\big)\\big)},\n$$", "text_format": "latex", "page_idx": 69}, {"type": "text", "text": "where $Z=(z_{-M},\\ldots,z_{-1})$ and $\\mu^{\\pi}$ is the stationary distribution of theMarkov chain overawindow of size $M+1$ Wedenoteby $\\tilde{\\mu}_{X}^{\\pi}(e_{k})=\\tilde{\\mu}_{X}^{\\pi}(z=e_{k})$ where $\\widetilde{\\mu}_{X}^{\\pi}(z)$ is the marginal distribution for $z$ and serves as the population counterpart for $\\begin{array}{r}{y^{\\star}=\\sum_{l=M+1}^{L}\\sigma_{l}^{\\star}x_{l}}\\end{array}$ Wedefne quantity $A$ and $B$ as $\\begin{array}{r l}&{4\\!:=\\!\\mathbb{E}\\bigg[\\displaystyle\\sum_{l=M+1}^{L}\\sigma_{l}^{\\star}\\displaystyle\\sum_{k=1}^{d}\\!\\left(\\frac{\\mathbb{I}(x_{L+1}=x_{l}=e_{k})}{y^{\\star}(k)+\\varepsilon}-\\frac{y^{\\star}(k)\\,\\mathbb{I}(x_{L+1}=e_{k})}{y^{\\star}(k)+\\varepsilon}\\right)\\prod_{h\\in\\mathcal{S}^{\\star}}\\mathbb{I}(x_{l-h}=x_{L+1-h})\\bigg].}\\\\ &{\\,\\,\\mathfrak{I}\\!:=\\!\\mathbb{E}\\bigg[\\displaystyle\\sum_{l=M+1}^{L}\\sigma_{l}^{\\star}\\displaystyle\\sum_{k=1}^{d}\\!\\left(\\frac{\\mathbb{I}(x_{L+1}=x_{l}=e_{k})}{\\tilde{\\mu}_{X}^{\\pi}(e_{k})}-\\mathbb{I}(x_{L+1}=e_{k})\\right)\\prod_{h\\in\\mathcal{S}^{\\star}}\\mathbb{I}(x_{l-h}=x_{L+1-h})\\bigg].}\\end{array}$ ", "page_idx": 69}, {"type": "text", "text": "Under Assumption 3.5, we have ", "page_idx": 69}, {"type": "equation", "text": "$$\n|A-B|\\leq\\frac{8(1-\\lambda)^{-1/2}(D_{\\chi^{2}}(\\mu_{0}\\parallel\\mu^{\\pi})+1)^{1/4}+8{\\sqrt{M}}}{L^{1/2}\\cdot\\gamma^{|S^{\\star}|+1}}+\\frac{2d\\varepsilon}{\\gamma}.\n$$", "text_format": "latex", "page_idx": 69}, {"type": "text", "text": "Proof of Lemma F.10. The proof follows the same arguments as Lemma F.4. We remind the readers that $y^{\\star}(k)$ is also a function of the whole chain $X$ .Wenotethat ", "page_idx": 70}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle{A-B|=\\left|\\mathbb{E}\\right|\\sum_{l=M+1}^{L}\\sigma_{l}^{*}\\cdot\\left(\\sum_{k\\in[d]}\\frac{\\mathbb{I}(x_{L+1}=x_{l}=e_{k})}{y^{*}(k)+\\varepsilon}-\\sum_{k\\in[d]}\\frac{\\mathbb{I}(x_{L+1}=x_{l}=e_{k})}{\\tilde{\\mu}_{X}^{\\pi}(e_{k})}\\right.}}\\\\ {\\displaystyle{-\\sum_{k\\in[d]}\\frac{y^{*}(k)\\,\\mathbb{I}(x_{L+1}=e_{k})}{y^{*}(k)+\\varepsilon}+1\\right)\\cdot\\prod_{h\\in\\mathcal{S}^{*}}\\mathbb{I}(x_{l-h}=x_{L+1})}}\\\\ {\\displaystyle{=\\left|\\mathbb{E}\\right|\\sum_{l=M+1}^{L}\\sigma_{l}^{*}\\cdot\\left(\\sum_{k\\in[d]}\\Big(\\frac{\\tilde{\\mu}_{X}^{-}(e_{k})-y^{*}(k)}{(y^{*}(k)+\\varepsilon)\\cdot\\tilde{\\mu}_{X}^{\\pi}(e_{k})}-\\frac{\\varepsilon}{(y^{*}(k)+\\varepsilon)\\cdot\\tilde{\\mu}_{X}^{\\pi}(e_{k})}\\Big)\\cdot\\mathbb{I}(x_{L+1}=x_{l}=l)\\right.}}\\\\ {\\displaystyle{-\\sum_{k\\in[d]}\\frac{\\varepsilon\\,\\mathbb{I}(x_{L+1}=e_{k})}{y^{*}(k)+\\varepsilon})\\cdot\\prod_{h\\in\\mathcal{S}^{*}}\\mathbb{I}(x_{l-h}=x_{L+1-h})\\Bigg]\\Bigg|.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 70}, {"type": "text", "text": "To handle this error, we define three error terms as ", "page_idx": 70}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{{rr}}_{1}:=\\left|\\mathbb{E}\\bigg[\\displaystyle\\sum_{k\\in[d]}\\frac{\\widetilde{\\mu}_{X}^{\\pi}(e_{k})-y^{*}(k)}{(y^{*}(k)+\\varepsilon)}\\cdot\\sum_{l=M+1}^{L}\\sigma_{l}^{\\star}\\cdot\\mathbb{I}\\big(x_{L+1}=x_{l}=e_{k}\\big)\\cdot\\prod_{h\\in S^{\\star}}\\mathbb{1}(x_{l-h}=x_{L+1-h})\\bigg]\\right|}\\\\ &{\\mathrm{{rr}}_{2}:=\\left|\\mathbb{E}\\bigg[\\displaystyle\\sum_{k\\in[d]}\\frac{\\varepsilon}{(y^{*}(k)+\\varepsilon)}\\cdot\\widetilde{\\mu}_{X}^{\\pi}(e_{k})\\cdot\\sum_{l=M+1}^{L}\\sigma_{l}^{\\star}\\cdot\\mathbb{I}\\big(x_{L+1}=x_{l}=e_{k}\\big)\\cdot\\displaystyle\\prod_{h\\in S^{\\star}}\\mathbb{1}\\big(x_{l-h}=x_{L+1-h}\\big)\\right]\\right|}\\\\ &{\\mathrm{{rr}}_{3}:=\\left|\\mathbb{E}\\bigg[\\displaystyle\\sum_{k\\in[d]}\\frac{\\varepsilon}{y^{\\star}(k)+\\varepsilon}\\cdot\\mathbb{I}\\big(x_{L+1}=e_{k}\\big)\\cdot\\displaystyle\\sum_{l=M+1}^{L}\\sigma_{l}^{\\star}\\cdot\\displaystyle\\prod_{h\\in S^{\\star}}\\mathbb{1}\\big(x_{l-h}=x_{L+1-h}\\big)\\bigg]\\right|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 70}, {"type": "text", "text": "For the first error term, we have that ", "page_idx": 70}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{err}_{1}\\leq\\mathbb{E}\\Bigg[\\sum_{k\\in[d]}\\frac{|\\widetilde{\\mu}_{X}^{\\pi}(e_{k})-y^{\\star}(k)|}{(y^{\\star}(k)+\\varepsilon)}\\cdot\\sum_{l=M+1}^{L}\\frac{\\sigma_{l}^{\\star}\\,\\mathbb{I}(x_{l}=e_{k})}{\\widetilde{\\mu}_{X}^{\\pi}(e_{k})}\\Bigg]}\\\\ &{\\quad\\quad=\\mathbb{E}\\Bigg[\\sum_{k\\in[d]}\\frac{|\\widetilde{\\mu}_{X}^{\\pi}(e_{k})-y^{\\star}(k)|}{(y^{\\star}(k)+\\varepsilon)}\\cdot\\frac{y^{\\star}(k)}{\\widetilde{\\mu}_{X}^{\\pi}(e_{k})}\\Bigg]\\leq\\gamma^{-1}\\cdot\\mathbb{E}\\Bigg[\\sum_{k\\in[d]}|\\widetilde{\\mu}_{X}^{\\pi}(e_{k})-y^{\\star}(k)|\\Bigg],}\\end{array}\n$$", "text_format": "latex", "page_idx": 70}, {"type": "text", "text": "where we recall that by assumption, $\\gamma$ provides a lower bound for $\\pi(\\cdot\\mid X_{\\mathtt{p a}})$ ,hence also a lower bound for $\\tilde{\\mu}_{X}^{\\pi}(e_{k})$ . Next, we invoke Proposition F.19 which provides an upper bound for the difference between the empirical and population distributions in terms of the $\\ell_{1}$ -norm: ", "page_idx": 70}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\bigg[\\bigg\\|\\widetilde{\\mu}_{X}^{\\pi}(z=\\cdot)-y^{\\star}(\\cdot)\\bigg\\|_{1}\\bigg]\\le\\frac{4\\big((1-\\lambda)^{-1}\\sqrt{D_{\\chi^{2}}(\\mu_{0}\\,\\|\\,\\mu^{\\pi})+1}+4M\\big)^{1/2}}{L^{1/2}\\cdot\\operatorname*{min}_{\\pi,x_{L+1},X_{L+1-\\delta^{\\star}}}\\mu^{\\pi}(x_{L+1},X_{L+1-\\delta^{\\star}})}}\\\\ {\\le\\frac{4(1-\\lambda)^{-1/2}(D_{\\chi^{2}}(\\mu_{0}\\,\\|\\,\\mu^{\\pi})+1)^{1/4}+8\\sqrt{M}}{L^{1/2}\\cdot\\gamma^{|\\mathcal{S}^{\\star}|+1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 70}, {"type": "text", "text": "Hence, we control the first error term. ", "page_idx": 70}, {"type": "text", "text": "For the second error term, we follow the same procedure and obtain an upper bound as ", "page_idx": 70}, {"type": "equation", "text": "$$\n\\mathrm{err}_{2}\\leq\\mathbb{E}\\Bigg[\\sum_{k\\in[d]}\\frac{\\varepsilon}{\\widetilde{\\mu}_{X}^{\\pi}(e_{k})}\\cdot\\sum_{l=M+1}^{L}\\frac{\\sigma_{l}^{\\star}\\,\\mathbb{1}(x_{l}=e_{k})}{(y^{\\star}(k)+\\varepsilon)}\\Bigg]\\leq\\mathbb{E}\\Bigg[\\sum_{k\\in[d]}\\frac{\\varepsilon}{\\widetilde{\\mu}_{X}^{\\pi}(e_{k})}\\Bigg]\\leq\\gamma^{-1}d\\varepsilon.\n$$", "text_format": "latex", "page_idx": 70}, {"type": "text", "text": "For the last error term, it holds that ", "page_idx": 71}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{err}_{3}\\leq\\mathbb{E}\\bigg[\\displaystyle\\sum_{k\\in[d]}\\frac{\\varepsilon}{y^{\\star}(k)+\\varepsilon}\\cdot\\mathbf{1}(x_{L+1}=e_{k})\\bigg]}\\\\ &{\\qquad\\leq\\bigg|\\mathbb{E}\\bigg[\\displaystyle\\sum_{k\\in[d]}\\frac{\\varepsilon\\,\\mathbb{I}(x_{L+1}=e_{k})}{\\tilde{\\mu}_{X}^{\\pi}(e_{k})+\\varepsilon}\\bigg]\\bigg|+\\bigg|\\sum_{k\\in[d]}\\mathbb{E}\\bigg[\\frac{\\varepsilon\\big(y^{\\star}(k)-\\tilde{\\mu}_{X}^{\\pi}(e_{k})\\big)\\cdot\\mathbf{1}(x_{L+1}=e_{k})}{(\\tilde{\\mu}_{X}^{\\pi}(e_{k})+\\varepsilon)(y^{\\star}(k)+\\varepsilon)}\\bigg]\\bigg|}\\\\ &{\\qquad\\leq\\frac{\\varepsilon}{\\gamma}+\\mathbb{E}\\bigg[\\displaystyle\\sum_{k\\in[d]}\\frac{|y^{\\star}(k)-\\tilde{\\mu}_{X}^{\\pi}(e_{k})|}{\\gamma}\\bigg]\\leq\\frac{\\varepsilon}{\\gamma}+\\frac{4(1-\\lambda)^{-1/2}(D_{\\chi^{2}}(\\mu_{0}\\,\\|\\,\\mu^{\\pi})+1)^{1/4}+8\\sqrt{M}}{L^{1/2}\\cdot\\gamma|^{\\mathcal{S}^{\\star}|+1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 71}, {"type": "text", "text": "where the last inequality follows directly from (F.23) ", "page_idx": 71}, {"type": "text", "text": "In summary, the difference between $f_{2}$ and $f_{3}$ is bounded by ", "page_idx": 71}, {"type": "equation", "text": "$$\n|f_{2}-f_{3}|\\le\\mathrm{err}_{1}+\\mathrm{err}_{2}+\\mathrm{err}_{3}\\le\\frac{8(1-\\lambda)^{-1/2}(D_{\\chi^{2}}(\\mu_{0}\\,\\|\\,\\mu^{\\pi})+1)^{1/4}+8\\sqrt{M}}{L^{1/2}\\cdot\\gamma^{|S^{\\star}|+1}}+\\frac{2d\\varepsilon}{\\gamma},\n$$", "text_format": "latex", "page_idx": 71}, {"type": "text", "text": "which completes our proof of Lemma F.10. ", "page_idx": 71}, {"type": "text", "text": "The following lemmas are for analyzing the error $|f_{3}-f_{4}|$ for Stage III. ", "page_idx": 71}, {"type": "text", "text": "Lemma F.11. We define ", "page_idx": 71}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A:=\\mathbb{E}\\Bigg[\\displaystyle\\sum_{l=M+1}^{L}\\sigma_{l}^{\\star}\\cdot\\sum_{k=1}^{d}\\biggr(\\frac{\\mathbb{I}(x_{L+1}=x_{l}=e_{k})}{\\widetilde{\\mu}_{X}^{\\pi}(e_{k})}-\\mathbb{I}(x_{L+1}=e_{k})\\biggr)\\cdot\\prod_{h\\in\\mathcal{S}^{\\star}}\\mathbb{I}(x_{l-h}=x_{L+1-h})\\Bigg],}\\\\ &{B:=\\mathbb{E}_{X,(z,Z)\\sim\\widetilde{\\mu}_{X}^{\\pi}}\\left[\\displaystyle\\sum_{k=1}^{d}\\biggr(\\frac{\\mathbb{I}\\bigl(x_{L+1}=z=e_{k}\\bigr)}{\\widetilde{\\mu}_{X}^{\\pi}(e_{k})}-\\mathbb{I}\\bigl(x_{L+1}=e_{k}\\bigr)\\biggr)\\cdot\\prod_{h\\in\\mathcal{S}^{\\star}}\\mathbb{I}\\bigl(z_{l-h}=x_{L+1-h}\\bigr)\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 71}, {"type": "text", "text": "where ", "page_idx": 71}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sigma_{l}^{\\star}:=\\frac{\\exp\\big(a\\cdot\\prod_{h\\in\\mathcal{S}^{\\star}}\\mathbb{I}\\left(x_{l-h}=x_{L+1-h}\\right)\\big)}{\\sum_{l^{\\prime}=1}^{L}\\exp\\big(a\\cdot\\prod_{h\\in\\mathcal{S}^{\\star}}\\mathbb{I}\\left(x_{l^{\\prime}-h}=x_{L+1-h}\\right)\\big)},\\qquad\\qquad\\qquad\\qquad\\qquad\\quad}\\\\ {\\widetilde{\\mu}_{X}^{\\pi}(z,Z):=\\widetilde{\\mu}^{\\pi}(z,Z\\,|\\,X_{L+1-\\mathcal{S}^{\\star}})=\\frac{\\mu^{\\pi}(z,Z)\\exp\\big(a\\cdot\\prod_{h\\in\\mathcal{S}^{\\star}}\\mathbb{I}\\left(z_{-h}=x_{L+1-h}\\right)\\big)}{\\sum_{z^{\\prime},Z^{\\prime}}\\mu^{\\pi}(z^{\\prime},Z^{\\prime})\\exp\\big(a\\cdot\\prod_{h\\in\\mathcal{S}^{\\star}}\\mathbb{I}\\left(z_{-h}^{\\prime}=x_{L+1-h}\\right)\\big)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 71}, {"type": "text", "text": "Under Assumption 3.5, we have ", "page_idx": 71}, {"type": "equation", "text": "$$\n|A-B|\\leq\\frac{8\\gamma^{-1}(1-\\lambda)^{-1/2}(D_{\\chi^{2}}(\\mu_{0}\\,\\|\\,\\mu^{\\pi})+1)^{1/4}+16\\gamma^{-1}\\sqrt{M}}{L^{1/2}\\cdot\\gamma^{|{\\cal S}^{\\star}|+1}}.\n$$", "text_format": "latex", "page_idx": 71}, {"type": "text", "text": "Proof of Lemma F.1l. For $Z~=~(z_{-M},\\ldots,z_{-1})$ and $Z^{\\prime}~=~\\left(z_{-M}^{\\prime},\\cdot\\cdot\\cdot,z_{-1}^{\\prime}\\right)$ , we let $\\begin{array}{r l}{Z_{-\\mathcal{S}^{\\star}}}&{{}=}\\end{array}$ $(z_{-h})_{h\\in S^{\\star}}$ , we define ", "page_idx": 71}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\hat{\\mu}_{X}^{\\pi}(z,Z)=\\frac{1}{L-M}\\sum_{l=M+1}^{L}\\mathbb{1}(x_{l}=z,X_{l-M:l-1}=Z),}\\\\ &{R(Z,X_{L+1-\\mathcal{S}^{\\star}})=\\exp\\bigg(a\\cdot\\displaystyle\\prod_{h\\in\\mathcal{S}^{\\star}}\\mathbb{1}(z_{-h}=x_{L+1-h})\\bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 71}, {"type": "text", "text": "Using these notations, we can rewrite the normalizing factor in $\\widetilde{\\mu}_{X}^{\\pi}$ and $\\sigma_{l}^{\\star}$ respectively as ", "page_idx": 71}, {"type": "equation", "text": "$$\n\\Phi=\\sum_{z,Z}\\mu^{\\pi}(z,Z)\\cdot R(Z,X_{L+1-{\\mathcal S}^{\\star}}),\\quad\\hat{\\Phi}=\\sum_{z,Z}\\hat{\\mu}_{X}^{\\pi}(z,Z)\\cdot R(Z,X_{L+1-{\\mathcal S}^{\\star}}).\n$$", "text_format": "latex", "page_idx": 71}, {"type": "text", "text": "We also define ", "page_idx": 71}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathfrak{H}(z,Z_{-{\\mathcal S}^{\\star}})=\\mu^{\\pi}(z,Z_{-{\\mathcal S}^{\\star}})\\cdot R(Z_{-{\\mathcal S}^{\\star}},X_{-{\\mathcal S}^{\\star}}),\\quad\\widehat{\\phi}(z,Z_{-{\\mathcal S}^{\\star}})=\\widehat{\\mu_{X}^{\\pi}}(z,Z_{-{\\mathcal S}^{\\star}})\\cdot R(Z_{-{\\mathcal S}^{\\star}},X_{L+1-{\\mathcal S}^{\\star}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 71}, {"type": "text", "text": "If we further define $\\begin{array}{r}{\\widehat{\\nu}_{X}^{\\pi}(z,Z_{-{\\mathcal S}^{\\star}})=\\sum_{l=M+1}^{L}\\mathbb{1}(x_{l}=z,X_{l-{\\mathcal S}^{\\star}}=Z_{-{\\mathcal S}^{\\star}})}\\end{array}$ , then we have ", "page_idx": 72}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{\\nu}_{X}^{\\pi}(z,Z_{-{\\mathcal S}^{\\star}})=\\frac{\\widehat{\\mu}_{X}^{\\pi}(z,Z_{-{\\mathcal S}^{\\star}})\\cdot R(Z_{-{\\mathcal S}^{\\star}},X_{L+1-{\\mathcal S}^{\\star}})}{\\widehat{\\Phi}}=\\frac{\\widehat{\\phi}(z,Z_{-{\\mathcal S}^{\\star}})}{\\widehat{\\Phi}},}\\\\ {\\widetilde{\\mu}_{X}^{\\pi}(z,Z_{-{\\mathcal S}^{\\star}})=\\frac{\\mu^{\\pi}(z,Z_{-{\\mathcal S}^{\\star}})\\cdot R(Z_{-{\\mathcal S}^{\\star}},X_{L+1-{\\mathcal S}^{\\star}})}{\\Phi}=\\frac{\\phi(z,Z_{-{\\mathcal S}^{\\star}})}{\\Phi}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 72}, {"type": "text", "text": "Using the above definitions and relationship, $A$ and $B$ can be rewritten as ", "page_idx": 72}, {"type": "equation", "text": "$$\n4=\\mathbb{E}\\Bigg[\\sum_{k=1}^{d}\\frac{\\widehat{\\phi}(e_{k},X_{L+1-\\mathcal{S}^{\\star}})}{\\widehat{\\Phi}\\cdot\\widetilde{\\mu}_{X}^{\\pi}(e_{k})}-\\frac{\\widehat{\\phi}(X_{L+1-\\mathcal{S}^{\\star}})}{\\widehat{\\Phi}}\\Bigg],\\quad B=\\mathbb{E}\\Bigg[\\sum_{k=1}^{d}\\frac{\\phi(e_{k},X_{L+1-\\mathcal{S}^{\\star}})}{\\Phi\\cdot\\widetilde{\\mu}_{X}^{\\pi}(e_{k})}-\\frac{\\phi(X_{L+1-\\mathcal{S}^{\\star}})}{\\Phi}\\Bigg].\n$$", "text_format": "latex", "page_idx": 72}, {"type": "text", "text": "Therefore, the difference between $A$ and $B$ is given by ", "page_idx": 72}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A-B|\\leq\\frac{2}{\\gamma}\\cdot\\mathbb{E}\\bigg[\\underset{z,Z_{-s^{\\star}}}{\\sum}\\bigg|\\frac{\\phi(z,Z_{-s^{\\star}})}{\\Phi}-\\frac{\\widehat{\\phi}(z,Z_{-s^{\\star}})}{\\widehat{\\Phi}}\\bigg|\\bigg]\\leq\\frac{2}{\\gamma}\\cdot\\mathbb{E}\\bigg[\\underset{z,Z_{-s^{\\star}}}{\\sum}\\bigg|\\widetilde{\\mu}_{X}^{\\pi}(z,Z_{-s^{\\star}})-\\widehat{\\nu}_{X}^{\\pi}(z,Z_{-s^{\\star}})\\bigg|\\bigg]}\\\\ &{\\qquad\\qquad\\leq\\frac{8\\gamma^{-1}\\cdot\\big((1-\\lambda)^{-1}\\sqrt{D_{\\chi^{2}}(\\mu_{0}\\,\\|\\,\\mu^{\\pi})+1}+4M\\big)^{1/2}}{L^{1/2}\\cdot\\operatorname*{min}_{x_{L+1},X_{L+1-s^{\\star}}}\\mu^{\\pi}(x_{L+1},X_{L+1-s^{\\star}})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 72}, {"type": "text", "text": "where the last inequality follows from the result in Proposition F.19. Invoking the lower bound $\\mu^{\\pi}(x_{L+1},X_{L+1-\\mathcal{S}^{\\star}})\\geq\\gamma^{|S^{\\star}|+1}$ , we complete the proo of Lemma F.11. \u53e3 ", "page_idx": 72}, {"type": "text", "text": "F.3 Lemmas on Concentration of Markov Chain ", "text_level": 1, "page_idx": 72}, {"type": "text", "text": "Recall that we previously define $X=\\left(x_{1},\\ldots,x_{L}\\right)$ as the observed sequence and $x_{L+1}$ as the value at time $L+1$ to be predicted. For generality, we will use $X=(x_{1},\\ldots,x_{L+1})$ to denote the whole sequence in the following proof. We denote by $p^{\\pi}(\\cdot)$ the joint distribution for the sequence $X$ with kernel $\\pi$ . Recall that we have the parent set $\\mathtt{p a}=\\{-r_{1},\\ldots,-r_{n}\\}$ , and as the start of a chain, we sample the first $r_{n}$ tokens by $(x_{1},\\bar{\\dots},x_{r_{n}})\\sim\\mu_{0}$ \uff1a ", "page_idx": 72}, {"type": "text", "text": "In the sequel, we will study concentration properties of the Markov chain $X$ for a window of tokens with window size at most $M$ , where $M>\\,r_{n}$ . To proceed, let us consider a fixed set ${\\boldsymbol{S}}\\subseteq[M]$ For any $l\\in[M+1,L+1]$ , we define $Y_{l}\\,=\\,(x_{l},X_{l-S})$ as a new vector containing the token at position $l$ and also the tokens in the past $\\boldsymbol{S}$ positions prior to $x_{l}$ . Here, we follow the convention that $X_{l-s}\\,=\\,(X_{l-i})_{i\\in{\\cal S}}$ . We also consider another fixed subset $S^{\\prime}\\subseteq[M]$ and similarly define $Y_{l}^{\\prime}=(x_{l},X_{l-S^{\\prime}})$ ", "page_idx": 72}, {"type": "text", "text": "The concentration properties of the Markov chain are rooted in the fact that when conditioning on all the parents, the current token is independent of all the past tokens. Given the parent set structure $\\mathtt{p a}=\\{-r_{1},\\ldots,-r_{n}\\}$ , we aim to make $Y_{L+1}$ approximately independent of $Y_{l}$ by conditioning on some intermediate parent sets. To this end, we dene $A=(x_{L+1-M},\\ldots,x_{L-M+r_{n}})\\in\\chi^{r_{n}}$ and $B_{l}=(x_{l-r_{n}+1},\\ldots,x_{l})\\in\\chi^{r_{n}}$ as these intermediate parent sets.By the Markov property and the parent set structure, we have the following conditional independence relations: ", "page_idx": 72}, {"type": "equation", "text": "$$\nY_{L+1}\\;\\bot\\;(B_{l},Y_{l})\\,|\\,A,\\quad(Y_{L+1},A)\\;\\bot\\,\\Gamma_{l}\\;|\\,B_{l},\\quad\\forall l=M+1,\\ldots,L-M+r_{n}.\n$$", "text_format": "latex", "page_idx": 72}, {"type": "text", "text": "To illustrate, let us consider the first condition $Y_{L+1}$ $(B_{l},Y_{l})\\mid A$ When $l\\leq L-M+r_{n}$ the $B_{l}$ and $Y_{l}$ are both contained in the history $\\{x_{k}\\colon k\\leq L-M+r_{n}\\}=A\\cup\\{x_{k}\\colon k\\leq L-N\\}$ M}. When conditioning on $A$ , the randomness of $(B_{l},Y_{l})$ is measurable by the $\\sigma$ -algebra generated by the \u201c\"past\" $\\{x_{k}\\colon k\\ \\mathbf{\\bar{\\leq}}\\ L-M\\}$ . Moreover, the randomness of $Y_{L+1}$ is measurable by the $\\sigma$ -algebra generated by the \u201cfuture\u201d $\\{x_{k}\\overset{}{:}k\\in[L+1-M+r_{n},L+1]\\}$ when conditioning on $A$ . Notice that the parent to the any element in the future $\\{x_{k}\\colon k\\in[L+1-\\operatorname{\\dot{M}}+r_{n},L+1]\\}$ is either contained in $A$ , or can be generated conditioned on $A$ without touching further history $\\{x_{k}\\colon k\\leq L-M\\}$ Thus, by the Markov property, conditioning on $A$ $\\mathit{Y}_{L+1}$ is independent of the past $\\{x_{k}\\colon k\\leq L-M\\}$ and in particular, $(\\bar{B}_{l},\\bar{Y}_{l})$ . Similarly, since $B$ contains the parent of $x_{l+1}$ , conditioning on $B,Y_{l}$ is independent of $x_{l+1}$ and later tokens. Moreover, given $B$ , the randomness of $Y_{l}$ comes from the randomness of $x_{l-M},\\ldots,x_{l-r_{n}}$ . Since $l\\leq L-M+r_{n}$ , we have $L+1-M\\ge l+1-r_{n}$ .As a result, conditioning on $B$ , the randomness of $(Y_{L+1},A)$ comes from tokens generated no earlier than $x_{l+1}$ . Therefore, $(Y_{L+1},A)$ and $Y_{l}$ are conditionally independent given $B_{l}$ . We visualize the definition of $Y_{L+1},\\,A,\\,B_{l}$ , and $Y_{l}$ in Figure 10 ", "page_idx": 72}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underbrace{\\supseteq Y_{L+1}}_{x_{L+1}~.~.~.~x_{L+1-M+r_{n}}~x_{L-M+r_{n}}~.~.~.~x_{L+1-M}}\\;\\cdot\\;.~.~.~\\underbrace{\\sum Y_{l}}_{x_{l}~.~.~.~x_{l-r_{n}+1}~x_{l-r_{n}}~.~.~.~x_{l-M+r_{n}}~.~.~.~}}\\\\ {A\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;B_{l}}\\end{array}\n$$", "text_format": "latex", "page_idx": 73}, {"type": "image", "img_path": "", "img_caption": ["Figure 10: llustration of the definition of $Y_{L+1},A,B_{l}$ and $Y_{l}$ When conditioned on $A$ \uff0c $Y_{L+1}$ is independent of $(B_{l},Y_{l})$ . When conditioned on $B_{l}$ \uff0c $Y_{l}$ is independent of $(A,Y_{L+1})$ "], "img_footnote": [], "page_idx": 73}, {"type": "text", "text": "Similarly,for $Y_{l}^{\\prime}=(x_{l},X_{l-S^{\\prime}})$ defined using the subset $S^{\\prime}$ , we also parallel conditional independence relations: ", "page_idx": 73}, {"type": "equation", "text": "$$\nY_{L+1}^{\\prime}\\;\\bot\\;\\left(B_{l},Y_{l}^{\\prime}\\right)|\\;\\cal{A},\\quad(Y_{L+1}^{\\prime},A)\\;\\bot\\;Y_{l}^{\\prime}\\;|\\;\\cal{B}_{l},\\quad\\forall l=M+1,\\ldots,L-M+r_{n}.\n$$", "text_format": "latex", "page_idx": 73}, {"type": "text", "text": "In particular, we also have ", "page_idx": 73}, {"type": "equation", "text": "$$\nY_{L+1}\\;\\bot\\!\\!\\!\\perp\\left(B_{l},Y_{l}^{\\prime}\\right)|A,\\quad(Y_{L+1},A)\\;\\bot\\!\\!\\!\\perp Y_{l}^{\\prime}\\,|\\,B_{l},\\quad\\forall l=M+1,\\ldots,L-M+r_{n}.\n$$", "text_format": "latex", "page_idx": 73}, {"type": "text", "text": "Using $\\{Y_{l},Y_{l}^{\\prime}\\}$ ,we define a joint distribution $\\widehat{p}^{\\pi}$ over $2+|S|+|S^{\\prime}|$ tokens as follows. For any $E\\in\\mathcal{X}^{|S|+1}$ and $E^{\\prime}\\in\\mathcal{X}^{|S^{\\prime}|+1}$ , the probability mass function of $\\widehat{p}^{\\pi}$ is defined as ", "page_idx": 73}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\widehat{p}^{\\pi}(Y_{L+1}=E,Y^{\\prime}=E^{\\prime})}\\\\ {\\displaystyle\\quad:=\\frac{1}{L-M}\\sum_{l=M+1}^{L}p^{\\pi}(Y_{L+1}=E,Y_{l}^{\\prime}=E^{\\prime})}\\\\ {\\displaystyle}\\\\ {\\displaystyle=\\frac{1}{L-M}\\sum_{l=M+1}^{L}\\sum_{A,B_{l}}^{L}\\mu^{\\pi}(Y_{L+1}=E\\mid A)\\cdot P_{\\pi}^{L-M+r_{n}-l}(A\\mid B_{l})\\cdot p^{\\pi}(Y_{l}^{\\prime}=E^{\\prime}\\mid B_{l})\\cdot p^{\\pi}(B_{l}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 73}, {"type": "text", "text": "Here, $Y^{\\prime}$ is just a placeholder for $Y_{l}^{\\prime}$ as $\\widehat{p}$ takes an average over $l$ and does not depend on any specific position index. The summation $\\sum_{A,B_{l}}$ means we sum over all possible values that $A$ and $\\bar{B}_{l}$ can take. In the last line of (F.25), we decompose the joint distribution $p^{\\pi}(Y_{L+1}=E,Y_{l}^{\\prime}=E^{\\prime})$ into the product of the conditional distributions by the Markov property in (F.24). That is, ", "page_idx": 73}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p^{\\pi}(Y_{L+1}=E,Y_{l}^{\\prime}=E^{\\prime})=\\displaystyle\\sum_{A,B_{l}}p^{\\pi}(Y_{L+1}=E,Y_{l}^{\\prime}=E^{\\prime},A,B_{l})}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\sum_{A,B_{l}}p^{\\pi}(B_{l})\\cdot p^{\\pi}(Y_{L+1}=E,A\\,|\\,B_{l})\\cdot p^{\\pi}(Y_{l}=E^{\\prime}\\,|\\,B_{l})}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\sum_{A,B_{l}}p^{\\pi}(Y_{L+1}=E\\,|\\,A)\\cdot p^{\\pi}(A\\,|\\,B_{l})\\cdot p^{\\pi}(Y_{l}=E^{\\prime}\\,|\\,B_{l})\\cdot p^{\\pi}(B_{l}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 73}, {"type": "text", "text": "Here the second equality follows from the fact that $(Y_{L+1},A)\\perp Y_{l}^{\\prime}\\,|\\,B_{l}$ and the last equality follows from the fact that $\\bar{Y}_{L+1}\\perp\\!\\!\\!\\perp\\left(B_{l},Y_{l}^{\\prime}\\right)|\\,{\\cal A}$ , which implies $p^{\\pi}({\\dot{Y}}_{L+1}={\\dot{E}}\\,|\\,A,B_{l})=p^{\\pi}(Y_{L+1}=E\\,|\\,A)$ Moreover, we denote by $P_{\\pi}^{i}$ the $i$ -step transition kernel of the chain, which corresponds to the $i$ -th power of the transition matrix $P_{\\pi}$ . Here, we are following the convention in the main text that ", "page_idx": 73}, {"type": "equation", "text": "$$\nP_{\\pi}(Z^{\\prime},Z)=\\pi(z_{l}^{\\prime}\\mid Z_{{\\bf p a}(l)})\\cdot{\\bf1}(Z_{l-r_{n}+1:-1}^{\\prime}=Z_{l-r_{n}+1:-1}).\n$$", "text_format": "latex", "page_idx": 73}, {"type": "text", "text": "In the following, we always consider a fixed transition kernel $\\pi$ and omit the superscript/subscript $\\pi$ in the matrix notation. We denote the transition matrix by $P_{\\pi}$ and the stationary distribution by $\\mu^{\\pi}$ for a window of length $r_{n}$ . For the transition matrix, we index each row by the next $r_{n}$ -window $Z^{\\prime}$ and each column by the current $r_{n}$ -window $Z$ . Under this notation, since both $A$ and $B_{l}$ have lengths $r_{n}$ , we have ", "page_idx": 73}, {"type": "equation", "text": "$$\np^{\\pi}(A\\,|\\,B_{l})=P_{\\pi}^{L-M+r_{n}-l}(A,B_{l}).\n$$", "text_format": "latex", "page_idx": 73}, {"type": "text", "text": "Here $P_{\\pi}^{L-M+r_{n}-l}(A\\,|\\,B_{l})$ corresponds to the $(A,B_{l})$ -entry of the matrix $(P_{\\pi})^{L-M+r_{n}-l}$ .Combining (F.24) and (F.27), we obtain the last equality in (F.25). ", "page_idx": 73}, {"type": "text", "text": "In the sequel, to simplify the notation, we write $P_{\\pi}$ and $\\mu^{\\pi}$ as $P$ and $\\mu$ respectively. Let us consider the reweighted transition kernel ", "page_idx": 74}, {"type": "equation", "text": "$$\nK:=\\mathrm{diag}\\big(\\sqrt{\\mu}\\big)^{-1}\\cdot P\\cdot\\mathrm{diag}\\,\\big(\\sqrt{\\mu}\\big)\\,,\n$$", "text_format": "latex", "page_idx": 74}, {"type": "text", "text": "where $\\sqrt{\\mu}$ is the element-wise square root of $\\mu$ . Since the transition matrix is primitive by assumption and having only one eigenvalue with value one on its spectral circle, we also have for $K$ that the leading eigenvalue is one with eigenvector $\\sqrt{\\mu}$ , i.e. ${\\sqrt{\\mu}}=K{\\sqrt{\\mu}}$ and ${\\sqrt{\\mu}}^{\\top}={\\sqrt{\\mu}}^{\\top}K$ . However, the projection in the leading eigenspace (or the Perron projection) is not of our interest. The following property of $K$ will be useful in the subsequent proof. ", "page_idx": 74}, {"type": "text", "text": "Proposition F.12. For the reweighted transition matrix $K$ , we have for any integer $i\\geq0$ ", "page_idx": 74}, {"type": "equation", "text": "$$\nP^{i}-\\mu\\mathbf{1}^{\\top}=\\operatorname{diag}\\left(\\sqrt{\\mu}\\right)\\cdot\\left(K-\\sqrt{\\mu}\\sqrt{\\mu}^{\\top}\\right)^{i}\\cdot\\operatorname{diag}\\!\\left(\\sqrt{\\mu}^{-1}\\right)\n$$", "text_format": "latex", "page_idx": 74}, {"type": "text", "text": "Proof of Proposition F.12. ", "page_idx": 74}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P^{i}-\\mu\\mathbf{1}^{\\top}=\\left(\\operatorname{diag}\\!\\left(\\sqrt{\\mu}\\right)\\cdot K\\cdot\\operatorname{diag}\\!\\left(\\sqrt{\\mu}\\right)^{-1}\\right)^{i}-\\mu\\mathbf{1}^{\\top}}\\\\ &{\\qquad\\qquad=\\operatorname{diag}\\!\\left(\\sqrt{\\mu}\\right)\\cdot\\left(K^{i}-\\sqrt{\\mu}\\sqrt{\\mu}^{\\top}\\right)\\cdot\\operatorname{diag}\\!\\left(\\sqrt{\\mu}\\right)^{-1}}\\\\ &{\\qquad\\qquad=\\operatorname{diag}\\!\\left(\\sqrt{\\mu}\\right)\\cdot\\left(K-\\sqrt{\\mu}\\sqrt{\\mu}^{\\top}\\right)^{i}\\cdot\\operatorname{diag}\\!\\left(\\sqrt{\\mu}^{-1}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 74}, {"type": "text", "text": "where the last equality holds by noting that $K-\\sqrt{\\mu}\\sqrt{\\mu}^{\\top}$ project $\\sqrt{\\mu}$ to the zero vector, and for any $v\\perp\\sqrt{\\mu}$ wehave $(K-\\sqrt{\\mu}\\sqrt{\\mu}^{\\top})v=K v$ . Thus for any test vector $x$ ", "page_idx": 74}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(K-\\sqrt{\\mu}\\sqrt{\\mu}^{\\top})^{i}x=(K-\\sqrt{\\mu}\\sqrt{\\mu}^{\\top})^{i}(x-\\langle\\sqrt{\\mu},x\\rangle\\cdot\\sqrt{\\mu})}\\\\ &{\\qquad\\qquad\\qquad\\qquad=K^{i}(x-\\langle\\sqrt{\\mu},x\\rangle\\cdot\\sqrt{\\mu})=(K^{i}-\\sqrt{\\mu}\\sqrt{\\mu}^{\\top})x.}\\end{array}\n$$", "text_format": "latex", "page_idx": 74}, {"type": "text", "text": "This completes the proof of Proposition F.12. ", "page_idx": 74}, {"type": "text", "text": "Indeed, the second largest eigenvalue of $K$ (in magnitude) determines the mixing rate of the chain.   \nLet $\\lambda$ denote the eigenvalue of $K$ with the second largest magnitude. ", "page_idx": 74}, {"type": "text", "text": "Furthermore, if the transition kernel $\\pi$ admits a lower bound $\\gamma>0$ , then we can guarantee that both $p^{\\pi}$ and $\\mu^{\\pi}$ admit a uniform lower bound. ", "page_idx": 74}, {"type": "text", "text": "Proposition F.13 (Uniform Lower Bound). Suppose $\\pi(\\cdot\\mid X_{\\mathtt{p a}})\\;\\geq\\;\\gamma$ uniformly for some $\\gamma\\,>\\,0$ and $\\mathtt{p a}=\\{-r_{1},\\ldots,-r_{n}\\}$ Suppose $X_{1:r_{n}}\\sim\\mu_{0}(\\cdot)$ where $\\dot{\\mu_{0}}\\in\\Delta(\\mathcal{X}^{r_{n}})$ .Then for any $S$ tokens $x_{l_{1}},x_{l_{2}},\\ldots,x_{l_{S}}$ suchthat $l_{s}\\geq r_{n}$ for any $s\\in[S]$ ,we have ", "page_idx": 74}, {"type": "equation", "text": "$$\np^{\\pi}(x_{l_{1}},\\ldots,x_{l_{S}})\\geq\\gamma^{S}.\n$$", "text_format": "latex", "page_idx": 74}, {"type": "text", "text": "Using Proposition F.13, we show that the transition matrix $P_{\\pi}$ is primitive. ", "page_idx": 74}, {"type": "text", "text": "Corollary F.14 (Uniform Lower Bound Implies Primitive Transition). Under the condition of PropositionF.13,with $\\pi(\\cdot\\,|\\,X_{\\tt p a})\\geq\\gamma>0$ the transition matrix defined in (F.26) is primitive. ", "page_idx": 74}, {"type": "text", "text": "Proof of Corollary $F.I4$ If the initial distribution is set to be any one-hot vector in $\\Delta(\\mathcal{X}^{r_{n}})$ and  taking $x_{l_{1}},\\dots,x_{l_{S}}$ in Proposition F.13 to be $x_{r_{n}+1},\\ldots,x_{2r_{n}}$ . we conclude that $p^{\\pi}(X_{r_{n}+1:2r_{n}}\\,|\\,\\bar{X}_{1:r_{n}})\\;>\\;0$ holds for any $X_{r_{n}+1:2r_{n}},X_{1:r_{n}}\\ \\stackrel{..}{\\in}\\ \\chi^{r_{n}}$ .Recall from the definition that for a primitive matrix $P$ ,wecanfind somepositive integer $k$ such that $P^{k}$ has all positive entries. For our case, we can set $k\\,=\\,r_{n}$ and everything follows by noting that $p^{\\pi}(X_{r_{n}+1:2r_{n}}\\,|\\,X_{1:r_{n}})=$ $P_{\\pi^{n}}^{r_{n}}(X_{r_{n}+1:2r_{n}},X_{1:r_{n}})$ \u53e3 ", "page_idx": 74}, {"type": "text", "text": "Another corollary of Proposition F.13 is that, if we take $\\mu_{0}=\\mu^{\\pi}$ , which is the stationary distribution, we can replace $p^{\\pi}$ in Proposition F.13 by $\\mu^{\\pi}$ ", "page_idx": 74}, {"type": "text", "text": "Corollary F.15. Suppose $\\pi(\\cdot\\,|\\,X_{\\mathtt{p a}})\\geq\\gamma$ uniformly for some $\\gamma>0$ and $\\mathtt{p a}=\\{-r_{1},\\ldots,-r_{n}\\}$ For the stationary distribution $\\mu^{\\pi}$ and $S$ tokens $x_{l_{1}},x_{l_{2}},\\ldots,x_{l_{S}}$ such that $l_{s}\\geq r_{n}$ for any $s\\in[S]$ we have $\\mu^{\\pi}(x_{l_{1}},\\ldots,x_{l_{S}})\\geq\\gamma^{S}$ ", "page_idx": 74}, {"type": "text", "text": "We prove Proposition F.13 as follows. ", "page_idx": 74}, {"type": "text", "text": "Proof of Proposition F.13. Without loss of generality, suppose that $M\\leq l_{1}<l_{2}<...<l_{S}$ We will prove the statement by induction on the number of tokens $S$ .If $S=1$ ,wecanrewrite ", "page_idx": 75}, {"type": "equation", "text": "$$\np^{\\pi}(x_{l_{1}})=\\sum_{X_{\\mathtt{p a}(l_{1})}}\\pi(x_{l_{1}}\\mid X_{\\mathtt{p a}(l_{1})})p^{\\pi}(X_{\\mathtt{p a}(l_{1})})\\ge\\sum_{X_{\\mathtt{p a}(l_{1})}}\\gamma\\cdot p^{\\pi}(X_{\\mathtt{p a}(l_{1})})\\ge\\gamma.\n$$", "text_format": "latex", "page_idx": 75}, {"type": "text", "text": "Now, suppose the statement holds for $1,2,\\dots,S-1$ . Let $Y=x_{l_{1}},\\ldots,x_{l_{s-1}}$ Then, we have ", "page_idx": 75}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p^{\\pi}(x_{l_{1}},\\hdots,x_{l_{S}})=\\displaystyle\\sum_{X_{\\mathtt{p a}(l_{S})}\\backslash Y}\\pi(x_{l_{S}}\\mid X_{\\mathtt{p a}(l_{S})})\\cdot p^{\\pi}(Y)\\cdot p^{\\pi}(X_{\\mathtt{p a}(l_{S})}\\mid Y)}\\\\ &{\\hphantom{x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x}}\\\\ &{\\hphantom{x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x}\\ge\\displaystyle\\sum_{X_{\\mathtt{p a}(l_{S})}\\backslash Y}\\gamma\\cdot p^{\\pi}(Y)\\cdot p^{\\pi}(X_{\\mathtt{p a}(l_{S})}\\mid Y)=\\gamma\\cdot p^{\\pi}(Y)\\ge\\gamma^{S},}\\end{array}\n$$", "text_format": "latex", "page_idx": 75}, {"type": "text", "text": "where the last inequality holds by the induction condition. Hence, we finish the proof. ", "page_idx": 75}, {"type": "text", "text": "$\\widehat{p}^{\\pi}$ $\\begin{array}{r}{\\sum_{l=M+1}^{L}\\eta^{L-l}p^{\\pi}(B_{l}=b)/\\sum_{l=M+1}^{L}\\eta^{L-l}}\\end{array}$ $\\boldsymbol{\\mu}^{\\pi}(\\boldsymbol{b})$ $\\eta\\in(0,1]$ ", "page_idx": 75}, {"type": "text", "text": "Lemma F.16. Following the notations introduced above, for the Markov chain with parent set $\\mathtt{p a}=\\{-r_{1},\\ldots,-r_{n}\\}$ let $D_{\\chi^{2}}(\\mu_{0}\\parallel\\mu^{\\pi})$ be the $\\chi^{2}$ -divergence between the initial distribution $\\mu_{0}$ and thestationary distribution $\\mu^{\\pi}$ overthefirst $r_{n}$ tokens.Take any ${\\mathcal{S}}\\subseteq[M]$ and let $Y_{l}=\\left(x_{l},X_{l-S}\\right)$ for $l=M+1,\\ldots,L+1$ .Suppose $L/2\\geq M\\geq r_{n}$ . We have ", "page_idx": 75}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\bigg\\|\\frac{{\\sum_{l=M+1}^{L}p^{\\pi}(Y_{l}=\\cdot)}}{L-M}-\\mu^{\\pi}(Y_{L+1}=\\cdot)\\bigg\\|_{\\mathrm{TV}}\\leq\\frac{2\\sqrt{D_{\\chi^{2}}(\\mu_{0}\\,\\|\\,\\mu^{\\pi})+1}}{L(1-\\lambda)},\\quad\\quad}\\\\ &{}&{\\|p^{\\pi}(Y_{L+1}=\\cdot)-\\mu^{\\pi}(Y_{L+1}=\\cdot)\\|_{\\mathrm{TV}}\\leq\\lambda^{L-M}\\sqrt{D_{\\chi^{2}}(\\mu_{0}\\,\\|\\,\\mu^{\\pi})+1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 75}, {"type": "text", "text": "Prof f Lemn F l6. Let $\\begin{array}{r}{c_{l}=\\eta^{L-l}/\\sum_{l=r_{n}}^{L-M+r_{n}}\\eta^{L-l}}\\end{array}$ $\\eta\\in[0,1]$ is constant be dete mined. Denote by , a vector of length $|\\mathcal{X}|^{r_{n}}$ , the initial distribution of the chain. We begin by quantifying the total variation (TV) distance: ", "page_idx": 75}, {"type": "equation", "text": "$$\n\\left\\|\\frac{\\sum_{l=r_{n}}^{L-M+r_{n}}\\lambda^{L-l}p^{\\pi}(B_{l}=\\cdot)}{\\sum_{l=r_{n}}^{L-M+r_{n}}\\lambda^{L-l}}-\\mu^{\\pi}(\\cdot)\\right\\|_{\\mathrm{TV}}=\\left\\|\\sum_{l=r_{n}}^{L-M+r_{n}}c_{l}\\cdot(p^{\\pi}(B_{l}=\\cdot)-\\mu^{\\pi}(\\cdot))\\right\\|_{\\mathrm{TV}}.\n$$", "text_format": "latex", "page_idx": 75}, {"type": "text", "text": "Let $b\\in\\mathcal{X}^{r_{n}}$ , representing the value for a length- $r_{n}$ window. Using matrix notation, we have: ", "page_idx": 75}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\displaystyle\\sum_{l=r_{n}}^{\\iota-M+r_{n}}c_{l}\\left(p^{\\pi}(B_{l}=b)-\\mu^{\\pi}(b)\\right)=\\displaystyle\\sum_{l=r_{n}}^{L-M+r_{n}}c_{l}\\cdot\\mathbf{1}_{b}^{\\top}P^{l-r_{n}}(\\mu_{0}-\\mu)=\\displaystyle\\sum_{l=r_{n}}^{L-M+r_{n}}c_{l}\\cdot\\mathbf{1}_{b}^{\\top}\\left(P^{l-r_{n}}-\\mu\\mathbf{1}\\right)=\\displaystyle\\sum_{l=r_{n}}^{\\mu}c_{l}\\cdot\\mathbf{1}_{b}^{\\top}\\left(P^{l-r_{n}}-\\mu\\mathbf{1}\\right)}\\\\ {=\\displaystyle\\sum_{l=r_{n}}^{L-M+r_{n}}c_{l}\\cdot\\mathbf{1}_{B}^{\\top}\\mathrm{diag}\\left(\\sqrt{\\mu}\\right)\\left(K-\\sqrt{\\mu}\\sqrt{\\mu}^{\\top}\\right)^{l-r_{n}}\\mathrm{diag}\\left(\\sqrt{\\mu}\\right)^{-1}\\mu_{0},}\\end{array}\n$$", "text_format": "latex", "page_idx": 75}, {"type": "text", "text": "where ${\\mathbf{1}}_{b}$ is the indicator vector corresponding to $b$ . The last equality follows from Proposition F.12. For any test vector $u\\in\\{0,1\\}^{|\\mathcal{X}|^{r_{n}}}$ , using the variational representation of TV distance: ", "page_idx": 75}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\displaystyle\\sum_{l=r_{n}}^{L-M+r_{n}}c_{l}\\left(p^{\\pi}(B_{l}=\\cdot)-\\mu^{\\pi}(\\cdot)\\right)\\right\\|_{\\mathrm{TV}}=\\displaystyle\\operatorname*{max}_{u\\in\\{0,1\\}^{|x|r_{n}}}u^{\\top}\\displaystyle\\sum_{l=r_{n}}^{L-M+r_{n}}c_{l}\\left(p^{\\pi}(B_{l}=\\cdot)-\\mu^{\\pi}(\\cdot)\\right)}\\\\ &{=\\displaystyle\\operatorname*{max}_{u\\in\\{0,1\\}^{|x|r_{n}}}\\displaystyle\\sum_{l=r_{n}}^{L-M+r_{n}}c_{l}\\cdot u^{\\top}\\mathrm{diag}\\left(\\sqrt{\\mu}\\right)\\cdot\\left(K-\\sqrt{\\mu}\\sqrt{\\mu}^{\\top}\\right)^{l-r_{n}}\\cdot\\mathrm{diag}\\left(\\sqrt{\\mu}\\right)^{-1}\\cdot\\mu_{0}}\\\\ &{\\le\\displaystyle\\sum_{l=r_{n}}^{L-M+r_{n}}c_{l}\\cdot\\lambda^{l-r_{n}}\\cdot\\left\\|\\mathrm{diag}\\left(\\sqrt{\\mu}\\right)^{-1}\\cdot\\mu_{0}\\right\\|_{2}=\\displaystyle\\sum_{l=r_{n}}^{L-M+r_{n}}c_{l}\\cdot\\lambda^{l-r_{n}}\\cdot\\sqrt{D_{\\chi^{2}}\\left(\\mu_{0}\\left\\|\\mu^{\\pi}\\right)+1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 75}, {"type": "text", "text": "where the inequality holds by $\\|u^{\\top}\\mathrm{diag}\\left(\\sqrt{\\mu}\\right)\\|_{2}\\leq\\|\\sqrt{\\mu}\\|_{2}=1$ and $K-\\sqrt{\\mu}\\sqrt{\\mu}^{\\top}$ has leading eigenvalue with magnitude $\\lambda$ . The last identity follows directly from the definition of the $\\chi^{2}$ -divergence that $\\begin{array}{r}{D_{\\chi^{2}}(\\mu_{0}\\parallel\\overleftarrow{\\mu^{\\pi}})+1=\\sum_{b}\\mu_{0}(b)^{2}/\\mu^{\\overleftarrow{\\pi}}(b)}\\end{array}$ ", "page_idx": 76}, {"type": "text", "text": "Substituting the definition of $c_{l}$ , we have ", "page_idx": 76}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\frac{\\sum_{l=r_{n}}^{L-M+r_{n}}\\eta^{L-l}p^{\\pi}(B_{l}=b)}{\\sum_{l=r_{n}}^{L-M+r_{n}}\\eta^{L-l}}-\\mu^{\\pi}(A=b)\\right|\\!\\!\\left|_{\\mathrm{TV}}\\le\\frac{\\sum_{l=r_{n}}^{L-M+r_{n}}\\eta^{L-l}\\cdot\\lambda^{l-r_{n}}\\cdot\\sqrt{D_{\\chi^{2}}(\\mu_{0}\\parallel\\mu^{\\pi})+1}}{\\sum_{l=r_{n}}^{L-M+r_{n}}\\eta^{L-l}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 76}, {"type": "text", "text": "We consider two special cases. In the first case, we set $\\eta=\\lambda$ , which gives us ", "page_idx": 76}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\left\\|\\frac{\\sum_{l=r_{n}}^{L-M+r_{n}}\\lambda^{L-l}p^{\\pi}(B_{l}=b)}{\\sum_{l=r_{n}}^{L-M+r_{n}}\\lambda^{L-l}}-\\mu^{\\pi}(A=b)\\right\\|_{\\mathrm{TV}}\\leq\\frac{\\sum_{l=r_{n}}^{L-M+r_{n}}\\lambda^{L-r_{n}}\\cdot\\sqrt{D_{\\chi^{2}}(\\mu_{0}\\parallel\\mu^{\\pi})+1}}{(1-\\lambda^{L-M})/(1-\\lambda)}}\\\\ &{}&{\\leq\\frac{L\\cdot\\lambda^{L-r_{n}}\\cdot(1-\\lambda)}{1-\\lambda^{L-M}}\\cdot\\sqrt{D_{\\chi^{2}}(\\mu_{0}\\parallel\\mu^{\\pi})+1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 76}, {"type": "text", "text": "In the second case, we set $\\eta=1$ , which gives us ", "page_idx": 76}, {"type": "equation", "text": "$$\n\\frac{\\left|\\sum_{l=r_{n}}^{L-M+r_{n}}p^{\\pi}(B_{l}=\\cdot)\\right|}{L-M}-\\mu^{\\pi}(A=\\cdot)\\bigg\\|_{\\mathrm{TV}}\\leq\\frac{\\sum_{l=r_{n}}^{L-M+r_{n}}\\lambda^{l-r_{n}}\\sqrt{D_{\\chi^{2}}(\\mu_{0}\\parallel\\mu^{\\pi})+1}}{L-M}\\leq\\frac{\\sqrt{D_{\\chi^{2}}(\\mu_{0}\\parallel\\mu^{\\pi})+1}}{(L-M)(1-\\chi)}.\n$$", "text_format": "latex", "page_idx": 76}, {"type": "text", "text": "Note that the TV distance is an $f$ -divergence. Thus, we can use the data processing inequality to obtain the desired result for $Y_{l}$ from the above inequality. To do so, note that $\\textstyle\\sum_{l=M+1}^{L}p^{\\pi}(Y_{l}=\\cdot)/(L-M)$ and $\\mu^{\\pi}(Y_{L+1}=\\cdot)$ can be transformed from -M+rn-1p(B = )/(L- M) and \u03bc(A= ) by the same emission kernel ", "page_idx": 76}, {"type": "text", "text": "p\"(YL+ $\\begin{array}{r}{\\ensuremath{\\boldsymbol{\\cdot}}_{1}=\\ensuremath{\\boldsymbol{\\cdot}}|\\ensuremath{\\boldsymbol{A}}=\\ensuremath{\\boldsymbol{\\cdot}}\\right)=p^{\\pi}(Y_{l}=\\ensuremath{\\boldsymbol{\\cdot}}|\\ensuremath{\\boldsymbol{B}}_{l-M+r_{n}}=\\ensuremath{\\boldsymbol{\\cdot}})=\\mu^{\\pi}(Y_{L+1}=\\ensuremath{\\boldsymbol{\\cdot}}|\\ensuremath{\\boldsymbol{A}}=\\ensuremath{\\boldsymbol{\\cdot}})=\\mu^{\\pi}(Y_{l}=\\ensuremath{\\boldsymbol{\\cdot}}|\\ensuremath{\\boldsymbol{B}}_{l-M+r_{n}}=\\ensuremath{\\boldsymbol{\\cdot}}).}\\end{array}$ Therefore, by the data processing inequality, it holds that ", "page_idx": 76}, {"type": "equation", "text": "$$\n\\left|{\\frac{\\sum_{l=M+1}^{L}p^{\\pi}(Y_{l}=\\cdot)}{L-M}}-\\mu^{\\pi}(Y_{L+1}=\\cdot)\\right|\\left|\\sum_{T\\backslash V}\\left|\\left|{\\frac{\\sum_{l=r_{n}}^{L-M+r_{n}}p^{\\pi}(B_{l}=\\cdot)}{L-M}}-\\mu^{\\pi}(A=\\cdot)\\right|\\right|_{\\mathrm{TV}}+{\\frac{\\sqrt{D_{X^{2}}(\\mu_{0}\\parallel(X=\\cdot))}}{(L-M)(B_{l}=\\cdot)}}=0,\n$$", "text_format": "latex", "page_idx": 76}, {"type": "text", "text": "Similarly for $p^{\\pi}(Y_{L+1}=\\cdot)$ and $\\mu^{\\pi}(\\cdot)$ , we have ", "page_idx": 76}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p^{\\pi}(Y_{L+1}=\\cdot)-\\mu^{\\pi}(Y_{L+1}=\\cdot)\\|_{\\mathrm{TV}}\\leq\\|p^{\\pi}(A=\\cdot)-\\mu^{\\pi}(A=\\cdot)\\|_{\\mathrm{TV}}}\\\\ &{\\quad\\leq\\underset{u\\in\\{0,1\\}^{\\vert\\mathcal{X}\\vert^{r_{n}}}}{\\operatorname*{max}}u^{\\top}\\cdot\\mathrm{diag}\\left(\\sqrt{\\mu}\\right)\\cdot\\left(K-\\sqrt{\\mu}\\sqrt{\\mu}^{\\top}\\right)^{L-M}\\cdot\\mathrm{diag}\\left(\\sqrt{\\mu}\\right)^{-1}\\cdot\\mu_{0}\\leq\\lambda^{L-M}\\sqrt{D_{\\chi^{2}}(\\mu_{0}\\parallel\\mu^{\\pi})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 76}, {"type": "text", "text": "where the latter two inequality follows from the same arguments as in (F.30). Hence, the proof is completed. \u53e3 ", "page_idx": 76}, {"type": "text", "text": "We have established that the average $\\Sigma_{l=M+1}^{L}\\,p^{\\pi}(Y_{l}=\\cdot)/(L-M)$ converges to $\\mu^{\\pi}(A=\\cdot)$ in total variation distance. This represents a \"first-order\u201d convergence since it involves the average of themarginal distribution of $Y_{l}$ . However, the quantity of interest in (F.25) is the average of the joint distribution of $Y_{L+1}$ and $Y_{l}$ , which concerns \u201csecond-order'\u2019 convergence. This is studied in the following lemma. ", "page_idx": 76}, {"type": "text", "text": "Lemma F.17. Following the notations introduced above, for the Markov chain with parent set $\\mathtt{p a}=\\{-r_{1},\\ldots,-r_{n}\\}$ let $D_{\\chi^{2}}(\\mu_{0}\\parallel\\mu^{\\pi})$ be the $\\chi^{2}$ -divergencebetween theinitial distribution $\\mu_{0}$ and thestationarydistribution $\\mu^{\\pi}$ over the first $r_{n}$ tokens. Take any $S,S^{\\prime}\\subseteq[M]$ and let $Y_{l}=\\left(x_{l},X_{l-s}\\right)$ and $Y_{l}^{\\prime}=(x_{l},X_{l-S^{\\prime}})$ for $l=M+1,\\ldots,L+1$ Suppose $L/2\\geq M\\geq\\dot{r}_{n}$ For $\\widehat{p}^{\\pi}$ defined in (F.25), we have ", "page_idx": 76}, {"type": "equation", "text": "$$\n{\\hat{p}}^{\\pi}(Y_{L+1}=\\cdot,Y^{\\prime}=\\cdot)-\\mu^{\\pi}(Y_{L+1}=\\cdot)\\times\\mu^{\\pi}(Y^{\\prime}=\\cdot)\\|_{\\mathrm{TV}}\\leq{\\frac{2M}{L}}+{\\frac{4{\\sqrt{D_{\\chi^{2}}(\\mu_{0}\\parallel\\mu^{\\pi})+1}}}{L(1-\\lambda)\\cdot{\\sqrt{\\operatorname*{min}_{E}\\mu^{\\pi}(Y_{L+1}=\\cdot)}}}}\n$$", "text_format": "latex", "page_idx": 76}, {"type": "text", "text": "In particular, we have ", "page_idx": 76}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\hat{p}^{\\pi}(Y_{L+1}=\\cdot,Y^{\\prime}=\\cdot)-\\mu^{\\pi}(Y_{L+1}=\\cdot)\\times\\left(\\frac{1}{L-M}\\displaystyle\\sum_{l=M+1}^{L}p^{\\pi}(Y_{l}^{\\prime}=\\cdot)\\right)\\right\\|_{\\mathrm{TV}}}\\\\ &{\\quad\\le\\displaystyle\\frac{2M}{L}+\\frac{2\\sqrt{D_{\\chi^{2}}(\\mu_{0}\\,\\|\\,\\mu^{\\pi})+1}}{L(1-\\lambda)\\cdot\\sqrt{\\operatorname*{min}_{E}\\mu^{\\pi}(Y_{L+1}=E)}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 76}, {"type": "text", "text": "Proof of Lemma F.17. Let us take $\\begin{array}{r}{\\mu_{.}^{\\pi}(E)\\cdot(L-M)^{-1}\\sum_{l=M+1}^{L}p^{\\pi}(Y_{l}=E^{\\prime})}\\end{array}$ as the intermediate distribution, and we have by (F.25) that ", "page_idx": 77}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle Y_{L+1}=E,Y^{\\prime}=E^{\\prime})-\\mu^{\\pi}(Y_{L+1}=E)\\cdot\\left(\\frac{1}{L-M}\\sum_{l=M+1}^{L}p^{\\pi}(Y_{l}=E^{\\prime})\\right)}}\\\\ {{\\displaystyle\\frac{1}{L-M}\\sum_{l=M+1}^{L-M+r_{n}}\\sum_{A,B_{l}}\\mu^{\\pi}(Y_{L+1}=E\\mid A)\\cdot\\left(P^{L-l-(M-r_{n})}(A\\mid B_{l})-\\mu^{\\pi}(A)\\right)\\cdot p^{\\pi}(Y_{l}^{\\prime}=E^{\\prime}\\mid B_{l})\\cdot p^{\\pi}(B)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 77}, {"type": "equation", "text": "$$\n+\\,\\frac{1}{L-M}\\sum_{l=L-M+r_{n}+1}^{L}\\,(p^{\\pi}(Y_{L+1}=E,Y_{l}^{\\prime}=E^{\\prime})-\\mu^{\\pi}(Y_{L+1}=E)p^{\\pi}(Y_{l}^{\\prime}=E^{\\prime}))\\,.\n$$", "text_format": "latex", "page_idx": 77}, {"type": "text", "text": "where we use the fact that $\\textstyle\\sum_{A}\\mu^{\\pi}(Y_{L+1}=E\\,|\\,A)\\mu^{\\pi}(A)=\\mu^{\\pi}(Y=E)$ for the first line. The second term on the right hand side can be easily controlled as we already have an $L^{-1}$ factor. Welet $\\mathrm{TV_{0}}$ be the total variation distance of the second term. It is easy to see that ", "page_idx": 77}, {"type": "equation", "text": "$$\n\\mathrm{TV}_{0}:=\\frac{1}{2}\\sum_{E,E^{\\prime}}|(\\mathrm{II})|\\leq\\frac{M-r_{n}}{L-M}\\leq\\frac{M}{L-M},\n$$", "text_format": "latex", "page_idx": 77}, {"type": "text", "text": "where we remark that (Il) is a function of both $E$ and $E^{\\prime}$ , and the total variation distance is just taking the sum of the absolute values of the differences. Here, we also use the fact that $L\\geq2M$ Using Proposition F.12, we can also rewrite the first term on the right hand side of (F.32) in the matrix form as ", "page_idx": 77}, {"type": "equation", "text": "$$\n\\mathrm{I})=\\frac{1}{L-M}\\sum_{l=M+1}^{L-M+r_{n}}{\\mu^{\\pi}(Y_{L+1}=\\cdot\\,|\\,A=\\cdot\\,)\\cdot\\mathrm{diag}\\left(\\sqrt{\\mu}\\right)\\cdot\\left(K-\\sqrt{\\mu}\\sqrt{\\mu}^{\\top}\\right)^{L-l-\\left(M-r_{n}\\right)}\\cdot\\mathrm{diag}\\left(\\sqrt{\\mu}\\right)^{-\\alpha}}}\\\\ {\\quad\\cdot\\mathrm{diag}(p^{\\pi}(B_{l}=\\cdot))\\cdot p^{\\pi}(Y_{l}^{\\prime}=\\cdot\\,|\\,B_{l}=\\cdot)^{\\top}.\n$$", "text_format": "latex", "page_idx": 77}, {"type": "text", "text": "When considering the $\\ell_{1}$ -norm of the above term, we introduce a test matrix $U$ of shape $|\\mathcal{X}|^{|Y_{L+1}|}\\times$ $|\\mathcal{X}|^{|Y_{L+1}|}$ with each element of $U$ chosen from $\\{0,1\\}$ . Let $\\mathrm{TV_{1}}$ be the total variation distance of the first term (I). Then, we have ", "page_idx": 77}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname{FV}_{1}\\leq\\underset{U}{\\operatorname*{max}}\\operatorname{Tr}\\Bigg[\\frac{1}{L-M}\\displaystyle\\sum_{l=M+1}^{L-M+r_{n}}\\mu^{\\pi}(Y_{L+1}=\\cdot\\,|\\,A=\\cdot)\\cdot\\mathrm{diag}\\left(\\sqrt{\\mu}\\right)\\cdot\\left(K-\\sqrt{\\mu}\\sqrt{\\mu}^{\\mathsf T}\\right)^{L-l-(M-r_{n})}}\\\\ &{\\qquad\\qquad\\qquad\\cdot\\mathrm{diag}\\left(\\sqrt{\\mu}\\right)^{-1}\\cdot\\mathrm{diag}(p^{\\pi}(B_{l}=\\cdot))\\cdot p^{\\pi}(Y_{l}^{\\prime}=\\cdot\\,|\\,B_{l}=\\cdot)^{\\mathsf T}\\cdot U(\\cdot,\\cdot)^{\\mathsf T}\\Bigg].}\\end{array}\n$$", "text_format": "latex", "page_idx": 77}, {"type": "text", "text": "To upper bound this quantity, we consider each row of $U$ as $U(E,\\cdot)=u(\\cdot\\,|\\,E)^{\\top}$ . Note that $u(\\cdot\\,\\vert\\,E)$ is also a $\\{0,1\\}$ -valued vector. By expanding the trace, we have ", "page_idx": 77}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname{FV}_{1}\\leq\\displaystyle\\sum_{E}\\operatorname*{max}_{u(\\cdot\\mid E)}\\displaystyle\\frac{1}{L-M}\\sum_{l=M+1}^{L-M+r_{n}}\\mu^{\\pi}(Y_{L+1}=E\\mid A=\\cdot)\\cdot\\mathrm{diag}\\left(\\sqrt{\\mu}\\right)}\\\\ &{\\qquad\\cdot\\left(K-\\sqrt{\\mu}\\sqrt{\\mu}^{\\top}\\right)^{L-l-(M-r_{n})}\\cdot\\mathrm{diag}\\left(\\sqrt{\\mu}\\right)^{-1}\\cdot\\mathrm{diag}\\left(p^{\\pi}(B_{l}=\\cdot)\\right)\\cdot p^{\\pi}(Y_{l}^{\\prime}=\\cdot\\mid B_{l}=\\cdot)^{\\top}\\cdot u(\\cdot\\mid B)}\\end{array}\n$$", "text_format": "latex", "page_idx": 77}, {"type": "text", "text": "Note that the $\\ell_{2}$ -norm of the vector in the last line can be upper bounded by ", "page_idx": 77}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Big|\\big(K-\\sqrt{\\mu}\\sqrt{\\mu}^{\\top}\\big)^{L-l-(M-r_{n})}\\cdot\\mathrm{diag}\\big(\\sqrt{\\mu}\\big)^{-1}\\cdot\\mathrm{diag}\\big(p^{\\pi}(B_{l}=\\cdot)\\big)\\cdot p^{\\pi}(Y_{l}^{\\prime}=\\cdot\\,|\\,B_{l}=\\cdot)^{\\top}\\cdot u(\\cdot\\,|\\,E)\\Big|\\Big|_{2}}\\\\ &{\\quad\\leq\\Big\\|\\lambda^{L-l-(M-r_{n})}\\cdot\\mathrm{diag}\\big(\\sqrt{\\mu}\\big)^{-1}\\cdot\\mathrm{diag}\\big(p^{\\pi}(B_{l}=\\cdot)\\big)\\cdot\\mathbf{1}\\Big\\|_{2}=\\lambda^{L-l-(M-r_{n})}\\Big\\|\\mathrm{diag}\\big(\\sqrt{\\mu}\\big)^{-1}\\cdot p^{\\pi}(B_{l}=\\cdot)}\\\\ &{\\quad=\\lambda^{L-l-(M-r_{n})}\\sqrt{D_{\\chi^{2}}(p^{\\pi}(B_{l}=\\cdot)\\,\\|\\,\\mu^{\\pi}(B_{l}=\\cdot))+1}\\leq\\lambda^{L-l-(M-r_{n})}\\sqrt{D_{\\chi^{2}}(\\mu_{0}\\,\\|\\,\\mu^{\\pi})+1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 77}, {"type": "text", "text": "where the first inequality holds by noting that $p^{\\pi}(Y_{l}^{\\prime}\\;=\\;\\cdot\\;|\\;B_{l}\\;=\\;\\cdot\\big)^{\\top}\\;\\cdot\\;u(\\cdot\\;|\\;E)$ is a vector with element within $[0,1]$ , and also invoking the operator norm of the matrix $\\widetilde K-\\sqrt{\\mu}\\sqrt{\\mu}^{\\top}$ . The second identity follows from the definition of the $\\chi^{2}$ -divergence that $D_{\\chi^{2}}(p^{\\pi}(B_{l}\\;=\\;\\cdot)\\parallel\\mu^{\\pi}(\\cdot))\\,+\\,1\\;=$ $\\begin{array}{r}{\\sum_{b}p^{\\pi}(B_{l}=b)^{2}/\\mu^{\\pi}(b)}\\end{array}$ . The last inequality is the data processing inequality as $p^{\\pi}(B_{l}=\\cdot)$ can be transformed from $\\mu_{0}(\\dot{B}_{r_{n}})$ and $\\mu^{\\pi}(\\bar{B_{l}})$ can be transformed from $\\mu^{\\pi}(\\bar{B}_{r_{n}})$ by the same emission kernel $\\mu^{\\pi}(B_{l}=\\cdot\\mid B_{r_{n}}=\\cdot)$ . Consequently, we have for the TV distance that ", "page_idx": 78}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{{TV}}_{1}\\leq\\displaystyle\\frac{1}{L-M}\\sum_{l=M+1}^{L-M+r_{n}}\\lambda^{L-l-(M-r_{n})}\\cdot\\sqrt{D_{\\chi^{2}}(\\mu_{0}\\parallel\\mu^{\\pi})+1}}\\\\ &{\\qquad\\qquad\\cdot\\,\\underset{\\left\\{\\tau(\\cdot\\,\\cdot\\,\\left\\vert E\\right\\}\\right)\\right\\}{\\operatorname*{max}}\\frac{\\operatorname*{max}}{\\vert\\mathrm{v}_{\\xi,\\lambda}\\vert\\,\\mathrm{e}(\\cdot\\,\\vert E\\vert)\\vert_{2}\\leq1}\\displaystyle\\sum_{E,A}\\mu^{\\pi}(Y_{L+1}=E\\left\\vert A\\right\\vert\\cdot\\sqrt{\\mu^{\\pi}(A)}\\cdot v(A\\left\\vert E\\right\\vert)}\\\\ &{\\leq\\displaystyle\\frac{\\sqrt{D_{\\chi^{2}}(\\mu_{0}\\parallel\\mu^{\\pi})+1}}{(L-M)(1-\\lambda)}\\cdot\\operatorname*{max}_{\\left\\{\\tau(\\cdot\\,\\cdot\\,\\left\\vert E\\right\\vert)\\right\\}\\,\\mathrm{{E}}:\\,\\,\\left\\vert v(\\cdot\\,\\cdot\\,\\left\\vert E\\right\\vert)\\right\\vert_{2}\\leq1,E}\\displaystyle\\sum_{\\left\\langle\\mu^{\\pi}(A)\\right\\vert}\\frac{\\mu^{\\pi}(A\\left\\vert Y_{L+1}=E\\right\\vert)}{\\sqrt{\\mu^{\\pi}(A)}}\\cdot v(A\\left\\vert E\\right\\vert\\cdot\\mu^{\\pi}(Y_{L+1}=E\\left\\vert E_{v}\\right\\vert)}\\\\ &{\\leq\\displaystyle\\underset{\\left\\{v(\\cdot\\,\\vert E)\\right\\}\\,\\mathrm{{E}}:\\,\\left\\Vert v(\\cdot\\,\\vert E)\\right\\Vert_{2}\\leq1}{\\operatorname*{max}}\\frac{\\sqrt{D_{\\chi^{2}}(\\mu_{0}\\Vert\\mu^{\\pi})+1}}{(L-M)(1-\\lambda)}\\cdot\\sqrt{I_{\\chi^{2}}(A;Y_{L+1})+1}\\cdot\\sqrt{\\underset{\\left\\{A,E\\right\\}}{\\sum_{k,E}v(A\\left\\vert E\\right\\vert)^{2}\\cdot\\mu^{\\pi}(Y_{L+1}=E)}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 78}, {"type": "text", "text": "where in the first equality, we use the variational form of the $\\ell_{2}$ -normforvector $\\mu^{\\pi}(Y_{L+1}=E\\,|\\,A=$ $\\cdot)\\cdot\\mathrm{diag}(\\sqrt{\\mu})$ . In the second inequality, we apply (F.33) and use the Bayes rule. The last inequality follows from the Cauchy-Schwarz inequality. Here, the mutual information $I_{\\chi^{2}}(A;Y_{L+1})+1$ canbe upper bounded by ", "page_idx": 78}, {"type": "equation", "text": "$$\nI_{\\chi^{2}}(A;Y_{L+1})+1=\\sum_{A,E}\\frac{\\mu^{\\pi}(Y_{L+1}=E\\,|\\,A)}{\\mu^{\\pi}(Y_{L+1}=E)}\\cdot\\mu^{\\pi}(Y_{L+1}=E,A)\\leq\\frac{1}{\\operatorname*{min}_{E}\\mu^{\\pi}(Y_{L+1}=E)},\n$$", "text_format": "latex", "page_idx": 78}, {"type": "text", "text": "and the last term involving $v(A\\,|\\,E)$ can be upper bounded by 1 thanks to the constraint on $v(\\cdot\\,|\\,E)$ In conclusion, ", "page_idx": 78}, {"type": "equation", "text": "$$\n\\mathrm{TV_{1}}\\leq{\\frac{\\sqrt{D_{\\chi^{2}}(\\mu_{0}\\,||\\,\\mu^{\\pi})+1}}{(L-M)(1-\\lambda)\\cdot{\\sqrt{\\operatorname*{min}_{E}\\mu^{\\pi}(Y_{L+1}=E)}}}}.\n$$", "text_format": "latex", "page_idx": 78}, {"type": "text", "text": "Lastly, let us relate the intermediate distribution to the final distribution $\\mu^{\\pi}(Y=\\cdot)\\times\\mu^{\\pi}(Y^{\\prime}=\\cdot)$ where we define the total variation distance $\\mathrm{TV_{2}}$ as ", "page_idx": 78}, {"type": "equation", "text": "$$\n\\displaystyle\\left.\\,\\,:=\\,\\right\\|\\mu^{\\pi}(\\cdot)\\cdot\\left(\\frac{1}{L-M}\\sum_{l=M+1}^{L}p^{\\pi}(Y_{l}^{\\prime}=\\cdot)\\right)-\\mu^{\\pi}(\\cdot)\\cdot\\mu^{\\pi}(\\cdot)\\right\\|_{\\mathrm{TV}}=\\left\\|\\left(\\frac{1}{L-M}\\sum_{l=M+1}^{L}p^{\\pi}(Y_{l}^{\\prime}=\\cdot)\\right)-1\\right\\|_{\\mathrm{TV}},\n$$", "text_format": "latex", "page_idx": 78}, {"type": "text", "text": "Invoking (F.28) of Lemma F.16, we have this quantity upper bounded by ", "page_idx": 78}, {"type": "equation", "text": "$$\n\\mathrm{TV_{2}}\\leq\\frac{\\sqrt{D_{\\chi^{2}}(\\mu_{0}\\parallel\\mu^{\\pi})+1}}{(L-M)(1-\\lambda)}.\n$$", "text_format": "latex", "page_idx": 78}, {"type": "text", "text": "Using the triangular inequality for the total variation distance, we have ", "page_idx": 78}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Vert\\widehat{p}^{\\pi}(Y_{L+1}=\\cdot,Y^{\\prime}=\\cdot)-\\mu^{\\pi}(Y_{L+1}=\\cdot)\\times\\mu^{\\pi}(Y^{\\prime}=\\cdot)\\Vert_{\\mathrm{TV}}}\\\\ &{\\quad\\leq\\mathrm{TV}_{0}+\\mathrm{TV}_{1}+\\mathrm{TV}_{2}}\\\\ &{\\quad\\leq\\cfrac{M}{L-M}+\\cfrac{2\\sqrt{D_{\\chi^{2}}(\\mu_{0}\\,\\Vert\\,\\mu^{\\pi})+1}}{(L-M)(1-\\lambda)\\cdot\\sqrt{\\operatorname*{min}_{E}\\mu^{\\pi}(Y_{L+1}=E)}}}\\\\ &{\\quad\\leq\\cfrac{2M}{L}+\\cfrac{4\\sqrt{D_{\\chi^{2}}(\\mu_{0}\\,\\Vert\\,\\mu^{\\pi})+1}}{L(1-\\lambda)\\cdot\\sqrt{\\operatorname*{min}_{E}\\mu^{\\pi}(Y_{L+1}=E)}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 78}, {"type": "text", "text": "and the upper bound for (F.31) follows by the same arguments. Hence, the proof is completed. ", "page_idx": 78}, {"type": "text", "text": "In the following, we use a similar technique as in Lemma F.17 to derive a bound for the chi-square divergence. ", "page_idx": 78}, {"type": "text", "text": "Lemma F.18. For the $\\chi^{2}$ $(L\\mathrm{~-~}M)^{-1}\\sum_{l=M+1}^{L}$ and the stationary distribution $\\mu^{\\pi}(\\cdot)$ we have ", "page_idx": 79}, {"type": "equation", "text": "$$\n\\natural\\left[D_{\\chi^{2}}\\left(\\frac{1}{L-M}\\sum_{l=M+1}^{L}1(Y_{l}=\\cdot)\\right)\\right]\\leq\\frac{4(1-\\lambda)^{-1}\\sqrt{D_{\\chi^{2}}(\\mu_{0}\\parallel\\mu^{\\pi})+1}+16M}{L\\cdot\\operatorname*{min}_{E}\\mu^{\\pi}(Y_{L+1}=E)},\n$$", "text_format": "latex", "page_idx": 79}, {"type": "text", "text": "where the expectation is with respect to $X\\sim p^{\\pi}$ ", "page_idx": 79}, {"type": "text", "text": "Proof of Lemma F.18. By definition of the $\\chi^{2}$ -divergence, what we aim to bound is just ", "page_idx": 79}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\bigg[\\displaystyle\\sum_{E}\\bigg((L-M)^{-1}\\displaystyle\\sum_{l=M+1}^{L}\\mathbb{1}(Y_{l}=E)-\\mu^{\\pi}(E)\\bigg)^{2}\\bigg/\\mu^{\\pi}(E)\\bigg]}\\\\ &{\\quad=\\mathbb{E}\\bigg[\\displaystyle\\sum_{E}\\frac{(L-M)^{-2}\\sum_{l,l^{\\prime}=M+1}^{L}\\mathbb{1}(Y_{l}=Y_{l^{\\prime}}=E)-\\mu^{\\pi}(E)^{2}}{\\mu^{\\pi}(E)}\\bigg]}\\\\ &{\\quad=\\mathbb{E}\\bigg[\\displaystyle\\sum_{E}\\sum_{l,l^{\\prime}=M+1}^{L}\\frac{\\mathbb{1}(Y_{l}=Y_{l^{\\prime}}=E)}{(L-M)^{2}\\mu^{\\pi}(E)}-1\\bigg]=\\displaystyle\\sum_{E}\\sum_{l,l^{\\prime}=M+1}^{L}\\frac{p^{\\pi}(Y_{l}=Y_{l^{\\prime}}=E)}{(L-M)^{2}\\mu^{\\pi}(E)}-1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 79}, {"type": "text", "text": "To study the above quantity, for $l\\geq2M-r_{n}+2$ , we define ", "page_idx": 79}, {"type": "equation", "text": "$$\nJ_{1}(l):=\\sum_{E}\\sum_{l^{\\prime}=M+1}^{l-M+r_{n}-1}\\frac{p^{\\pi}(Y_{l}=Y_{l^{\\prime}}=E)}{(L-M)^{2}\\mu^{\\pi}(E)}-\\frac{l-2M+r_{n}}{(L-M)^{2}}.\n$$", "text_format": "latex", "page_idx": 79}, {"type": "text", "text": "Following our convention, we let $A_{l}=X_{l-M:l-M+r_{n}-1}$ and $B_{l^{\\prime}}=X_{l^{\\prime}-r_{n}+1:l^{\\prime}}$ be two length- ${\\cdot}r_{n}$ window and by the Markov property, we have ", "page_idx": 79}, {"type": "equation", "text": "$$\nY_{l+1}\\perp\\!\\!\\!\\perp(B_{l^{\\prime}},Y_{l^{\\prime}})\\,|\\,A_{l},\\quad(Y_{l+1},B_{l})\\perp\\!\\!\\!\\perp Y_{l^{\\prime}}\\,|\\,B_{l^{\\prime}}.\n$$", "text_format": "latex", "page_idx": 79}, {"type": "text", "text": "Let us fix an index $l\\geq2M-r_{n}+2$ and take a summation over $M+1\\le l^{\\prime}\\le l-M+r_{n}-1$ Expanding the joint distribution, we have ", "page_idx": 79}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\Gamma_{1}(I):=\\frac{1}{(L-M)^{2}}\\sum_{\\stackrel{n=1}{r=M+1}}^{I-M+r_{n}-1}\\sum_{\\stackrel{n=1}{r}}\\mu^{\\pi}(Y_{i}=E|A_{i})\\cdot\\left(P^{L-\\ell-M+r_{n}-1}(A_{i}|B_{r})-\\mu^{\\pi}(A_{i})\\right)}}\\\\ &{\\qquad\\cdot\\underbrace{\\mu^{\\pi}(Y_{i}=E|B_{t})\\cdot p^{\\pi}(B_{t})}_{\\geq\\sum_{\\nu=1}^{r}+1}\\cdot\\mathrm{dig}(\\rho_{\\nu}^{\\pi})\\cdot\\mu^{\\pi}(Y_{i^{\\prime}}=E)^{-1}}\\\\ &{=\\frac{1}{(L-M)^{2}}\\sum_{\\stackrel{n=1}{r=M+1}}^{I-M+r_{n}-1}\\mathrm{Tr}\\Big[\\mu^{\\pi}(Y_{i}=\\cdot|A_{i}=\\cdot)\\cdot\\mathrm{diag}(\\sqrt{\\mu})\\cdot\\big(K-\\sqrt{\\mu}\\sqrt{\\mu}^{\\pi}\\big)^{\\jmath-\\ell-M+r_{n}-1}}\\\\ &{\\qquad\\qquad\\cdot\\mathrm{diag}(\\sqrt{\\mu})^{-1}\\cdot\\mathrm{diag}(p^{\\pi}(B_{t}=\\cdot))\\cdot p^{\\pi}(Y_{i^{\\prime}}=\\cdot|B_{t}=\\cdot)^{\\top}\\cdot\\mathrm{diag}(\\mu^{\\pi}(Y_{i^{\\prime}}=\\cdot))}\\\\ &{=\\frac{1}{(L-M)^{2}}\\sum_{\\stackrel{n=1}{r=M+1}}^{\\lfloor{\\alpha}\\cdot\\cdot\\alpha_{n}-1}\\mathrm{Tr}\\Big[\\mathrm{diag}(\\mu^{\\pi}(Y_{i^{\\prime}}=\\cdot)^{-1/2})\\cdot\\mu^{\\pi}(Y_{i}=\\cdot|A_{i}=\\cdot)\\cdot\\mathrm{diag}(\\sqrt{\\mu})}\\\\ &{\\qquad\\qquad\\qquad\\cdot\\left(K-\\sqrt{\\mu}\\sqrt{\\mu}^{\\pi}\\right)^{\\jmath-\\ell-M+r_{n}-1}\\cdot\\mathrm{diag}(\\sqrt{\\mu})^{-1}\\cdot\\mathrm{diag}(p^{\\pi}(B_{t}=\\cdot))}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\cdot p^{\\pi}(Y_{i^{\\prime}}=\\cdot|B_{i^{\\prime}}=\\cdot)^{\\top}\\cdot\\mathrm{diag}(\\mu^{\\pi}(Y_{i^{\\prime\n$$", "text_format": "latex", "page_idx": 79}, {"type": "text", "text": "where the first identity follows from the fact that ", "page_idx": 79}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{E,A_{l},B_{l}^{\\prime}}\\mu^{\\pi}(Y_{l}=E\\,|\\,A_{l})\\cdot\\mu^{\\pi}(A_{l})\\cdot p^{\\pi}(Y_{l^{\\prime}}=E\\,|\\,B_{l^{\\prime}})\\cdot p^{\\pi}(B_{l^{\\prime}})\\cdot\\mu^{\\pi}(Y_{l^{\\prime}}=E)^{-1}}\\\\ &{\\quad=\\displaystyle\\sum_{E}p^{\\pi}(Y_{l^{\\prime}}=E)\\cdot\\mu^{\\pi}(Y_{l^{\\prime}}=E)\\cdot\\mu^{\\pi}(Y_{l^{\\prime}}=E)^{-1}=1,}\\end{array}\n$$", "text_format": "latex", "page_idx": 79}, {"type": "text", "text": "and the second identity follows from Proposition F.12. We next invoke the Cauchy-Schwarz inequality for trace, i.e., $\\mathrm{Tr}(W^{\\top}V)^{2}\\leq\\mathrm{Tr}(W^{\\top}W)\\,\\mathrm{Tr}(V^{\\top}V)$ ,wherewetake ", "page_idx": 79}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{W^{\\top}=\\mathrm{diag}(\\mu^{\\pi}(Y_{l}=\\cdot)^{-1/2})\\cdot\\mu^{\\pi}(Y_{l}=\\cdot\\lvert\\,A_{l}=\\cdot\\bigr)\\cdot\\mathrm{diag}(\\sqrt{\\mu})\\cdot\\bigl(K-\\sqrt{\\mu}\\sqrt{\\mu}^{\\top}\\bigr)^{l-l^{\\prime}-M+r_{n}-1},}\\\\ &{\\quad V=\\mathrm{diag}\\bigl(\\sqrt{\\mu}\\bigr)^{-1}\\cdot\\mathrm{diag}(p^{\\pi}(B_{l^{\\prime}}=\\cdot))\\cdot p^{\\pi}(Y_{l^{\\prime}}=\\cdot\\lvert\\,B_{l^{\\prime}}=\\cdot)^{\\top}\\cdot\\mathrm{diag}(\\mu^{\\pi}(Y_{l^{\\prime}}=\\cdot)^{-1/2})}\\\\ &{\\quad\\quad=\\mathrm{diag}\\bigl(\\sqrt{\\mu}\\bigr)\\cdot p^{\\pi}(Y_{l^{\\prime}}=\\cdot\\lvert\\,B_{l^{\\prime}}=\\cdot)^{\\top}\\cdot\\mathrm{diag}(\\mu^{\\pi}(Y_{l^{\\prime}}=\\cdot)^{-1/2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 79}, {"type": "text", "text": "Note that ", "page_idx": 80}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\langle\\mathrm{Tr}(W^{\\top}W)\\leq\\lambda^{l-l^{\\prime}-M+r_{n}-1}\\cdot\\sqrt{\\mathrm{Tr}\\left(\\mathrm{diag}(\\mu^{\\pi}(Y_{l}=\\cdot)^{-1})\\mu^{\\pi}(Y_{l}=\\cdot\\vert\\,A=\\cdot)\\mathrm{diag}\\left(\\mu\\right)\\mu^{\\pi}(Y_{l}=\\cdot\\vert\\,A-\\cdot)\\mathrm{diag}\\left(\\mu\\right)\\right)}\\right\\rangle}\\\\ &{\\qquad\\qquad\\qquad=\\lambda^{l-l^{\\prime}-M+r_{n}-1}\\cdot\\sqrt{\\displaystyle\\sum_{A_{l},Y_{l}}\\frac{\\mu^{\\pi}(Y_{l},A_{l})^{2}}{\\mu^{\\pi}(Y_{l})\\cdot\\mu^{\\pi}(A_{l})}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 80}, {"type": "text", "text": "Following the same calculation, we have ", "page_idx": 80}, {"type": "equation", "text": "$$\n\\sqrt{\\mathrm{Tr}(V^{\\top}V)}=\\sqrt{\\sum_{Y_{l^{\\prime}},B_{l^{\\prime}}}\\frac{p^{\\pi}(Y_{l^{\\prime}},B_{l^{\\prime}})^{2}}{\\mu^{\\pi}(Y_{l^{\\prime}})\\mu^{\\pi}(B_{l^{\\prime}})}}.\n$$", "text_format": "latex", "page_idx": 80}, {"type": "text", "text": "Therefore, ", "page_idx": 80}, {"type": "equation", "text": "$$\nJ_{1}(l)\\leq\\frac{1}{(L-M)^{2}}\\sum_{\\nu=M+1}^{l-M+r_{n}-1}\\lambda^{l-l^{\\prime}-M+r_{n}-1}\\cdot\\sqrt{\\sum_{A_{l},Y_{l}}\\frac{\\mu^{\\pi}(Y_{l},A_{l})^{2}}{\\mu^{\\pi}(Y_{l})\\cdot\\mu^{\\pi}(A_{l})}\\cdot\\sum_{Y_{l^{\\prime}},B_{l^{\\prime}}}\\frac{p^{\\pi}(Y_{l^{\\prime}},B_{l^{\\prime}})^{2}}{\\mu^{\\pi}(Y_{l^{\\prime}})\\mu^{\\pi}(B_{l^{\\prime}})}}.\n$$", "text_format": "latex", "page_idx": 80}, {"type": "text", "text": "We further have ", "page_idx": 80}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{Y_{l^{\\prime}},\\,B_{l^{\\prime}}}\\frac{p^{\\pi}(Y_{l^{\\prime}},B_{l^{\\prime}})^{2}}{\\mu^{\\pi}(Y_{l^{\\prime}})\\mu^{\\pi}(B_{l^{\\prime}})}\\leq\\operatorname*{max}_{Y_{l^{\\prime}},B_{l^{\\prime}}}\\left\\{\\frac{p^{\\pi}(Y_{l^{\\prime}}\\,|\\,B_{l^{\\prime}})}{\\mu^{\\pi}(Y_{l^{\\prime}})}\\right\\}\\cdot\\sum_{B_{l^{\\prime}}}\\frac{p^{\\pi}(B_{l^{\\prime}})^{2}}{\\mu^{\\pi}(B_{l^{\\prime}})}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\frac{D_{\\chi^{2}}\\left(p^{\\pi}(B_{l^{\\prime}}=\\cdot)\\,\\|\\,\\mu^{\\pi}(B_{l^{\\prime}}=\\cdot)\\right)+1}{\\operatorname*{min}_{E}\\mu^{\\pi}(Y_{L+1}=E)}\\leq\\frac{D_{\\chi^{2}}(\\mu_{0}\\,\\|\\,\\mu^{\\pi})+1}{\\operatorname*{min}_{E}\\mu^{\\pi}(Y_{L+1}=E)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 80}, {"type": "text", "text": "where the last inequality holds by the data processing inequality. Similarly, we have ", "page_idx": 80}, {"type": "equation", "text": "$$\n\\sum_{A_{l},Y_{l}}\\frac{\\mu^{\\pi}(Y_{l},A_{l})^{2}}{\\mu^{\\pi}(Y_{l})\\cdot\\mu^{\\pi}(A_{l})}\\leq\\operatorname*{max}_{Y_{l},A_{l}}\\left\\{\\frac{\\mu^{\\pi}(Y_{l}\\mid A_{l})}{\\mu^{\\pi}(Y_{l})}\\right\\}\\leq\\frac{1}{\\operatorname*{min}_{E}\\mu^{\\pi}(Y_{L+1}=E)}.\n$$", "text_format": "latex", "page_idx": 80}, {"type": "text", "text": "Therefore, we conclude that ", "page_idx": 80}, {"type": "equation", "text": "$$\nJ_{1}(l)\\leq\\frac{\\sqrt{D_{\\chi^{2}}(\\mu_{0}\\parallel\\mu^{\\pi})+1}}{(L-M)^{2}(1-\\lambda)\\cdot\\operatorname*{min}_{E}\\mu^{\\pi}(Y_{L+1}=E)},\n$$", "text_format": "latex", "page_idx": 80}, {"type": "text", "text": "and ", "page_idx": 80}, {"type": "equation", "text": "$$\n2\\sum_{l=2M-r_{n}+2}^{L}J_{1}(l)\\leq\\frac{2\\sqrt{D_{\\chi^{2}}(\\mu_{0}\\,\\|\\,\\mu^{\\pi})+1}}{(L-M)(1-\\lambda)\\cdot\\operatorname*{min}_{E}\\mu^{\\pi}(Y_{L+1}=E)},\n$$", "text_format": "latex", "page_idx": 80}, {"type": "text", "text": "where we double the value as $l>l^{\\prime}$ only contributes to half of the terms in the double summation. Note that in the above summation for $l\\stackrel{\\cdot}{>}l^{\\prime}$ , we only include terms satisfying $l-l^{\\prime}\\geq M-r_{n}+1$ and $l-(M+1)\\geq M-r_{n}+1$ For the remaining $(l,l^{\\prime})$ not included above, each term is bounded above by ", "page_idx": 80}, {"type": "equation", "text": "$$\n\\left|{\\frac{1}{(L-M)^{2}}}\\!\\left(\\sum_{E}{\\frac{p^{\\pi}(Y_{l}=Y_{l^{\\prime}}=E)}{\\mu^{\\pi}(E)}}-1\\right)\\right|\\leq{\\frac{1}{(L-M)^{2}\\operatorname*{min}_{E}\\mu^{\\pi}(Y_{L+1}=E)}},\n$$", "text_format": "latex", "page_idx": 80}, {"type": "text", "text": "and we have no more than $4L(M-r_{n}+1)$ of these terms in total. As a result, we conclude with $L/2\\geq M\\geq r_{n}$ that ", "page_idx": 80}, {"type": "equation", "text": "$$\nJ_{1}\\leq\\frac{4(1-\\lambda)^{-1}\\sqrt{D_{\\chi^{2}}(\\mu_{0}\\,||\\,\\mu^{\\pi})+1}+16M}{L\\cdot\\operatorname*{min}_{E}\\mu^{\\pi}(Y_{L+1}=E)}.\n$$", "text_format": "latex", "page_idx": 80}, {"type": "text", "text": "Hence, we complete the proof of Lemma F.18. ", "page_idx": 80}, {"type": "text", "text": "Proposition F.19. Let us define ", "text_level": 1, "page_idx": 80}, {"type": "equation", "text": "$$\n\\widetilde{\\mu}_{X}^{\\pi}(z,Z_{-\\mathcal{S}^{\\star}})=\\frac{\\mu^{\\pi}(z,Z_{-\\mathcal{S}^{\\star}})\\exp\\big(a\\cdot\\prod_{h\\in\\mathcal{S}^{\\star}}\\mathbb{1}(z_{-h}=x_{L+1-h})\\big)}{\\sum_{z^{\\prime},Z_{-\\mathcal{S}^{\\star}}^{\\prime}}\\mu^{\\pi}(z^{\\prime},Z_{-\\mathcal{S}^{\\star}}^{\\prime})\\exp\\big(a\\cdot\\prod_{h\\in\\mathcal{S}^{\\star}}\\mathbb{1}(z_{-h}^{\\prime}=x_{L+1-h})\\big)},\n$$", "text_format": "latex", "page_idx": 80}, {"type": "text", "text": "where $Z_{-{\\cal S}^{\\star}}=(z_{-h})_{h\\in{\\cal S}^{\\star}}$ and $\\mu^{\\pi}$ is the stationary distribution of the Markov chain over a window of size $M+1$ We also treat $\\tilde{\\mu}_{X}^{\\pi}(\\cdot)$ as a length $|{\\mathcal{X}}|$ vectorwhere $\\mathcal{X}$ is the state space of the Markov chain. Let $\\begin{array}{r}{\\widehat{\\nu}_{X}^{\\pi}(z,Z_{-{\\mathcal S}^{\\star}})=\\sum_{l=M+1}^{L}\\sigma_{l}^{\\star}\\,\\mathbb{1}(x_{l}=z,X_{l-{\\mathcal S}^{\\star}}=Z_{-{\\mathcal S}^{\\star}})}\\end{array}$ where ", "page_idx": 81}, {"type": "equation", "text": "$$\n\\sigma_{l}^{\\star}=\\frac{\\exp(a\\cdot\\prod_{h\\in S^{\\star}}\\mathbb{1}(x_{l-h}=x_{L+1-h}))}{\\sum_{l^{\\prime}=M+1}^{L}\\exp(a\\cdot\\prod_{h\\in S^{\\star}}\\mathbb{1}(x_{l^{\\prime}-h}=x_{L+1-h}))}.\n$$", "text_format": "latex", "page_idx": 81}, {"type": "text", "text": "Then, we have ", "page_idx": 81}, {"type": "equation", "text": "$$\n\\begin{array}{r}{[\\|\\widetilde{\\mu}_{X}^{\\pi}(z=\\cdot,Z_{-}s\\cdot=\\cdot)-\\widehat{\\nu}_{X}^{\\pi}(z=\\cdot,Z_{-}s\\cdot=\\cdot)\\|_{1}]\\leq\\frac{4\\big((1-\\lambda)^{-1}\\sqrt{D_{\\chi^{2}}(\\mu_{0}\\|\\,\\mu^{\\pi})+1}+4M\\big)^{1/2}}{L^{1/2}\\cdot\\operatorname*{min}_{x_{L+1},X_{L+1-s}\\cdot}\\mu^{\\pi}(x_{L+1},X_{L+1-s^{\\star}})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 81}, {"type": "text", "text": "Proof of Proposition F.19. To unify the notations, we let $Z=(z_{-M},\\ldots,z_{-1})$ and define ", "page_idx": 81}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\widehat{\\mu}_{X}^{\\pi}(z,Z_{-S^{\\star}})=\\frac{1}{L-M}\\sum_{l=M+1}^{L}\\Im(x_{l}=z,X_{l-S^{\\star}}=Z_{-S^{\\star}}),}}\\\\ {{\\displaystyle R(Z_{-S^{\\star}},X_{L+1-S^{\\star}})=\\exp\\left(a\\cdot\\mathbb{1}(Z_{-S^{\\star}}=x_{L+1-S^{\\star}})\\right).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 81}, {"type": "text", "text": "Using these notations, we can defne the normalizing factor in $\\widetilde{\\mu}_{X}^{\\pi}$ and $y_{X}^{\\star}$ respectively as ", "page_idx": 81}, {"type": "equation", "text": "$$\n\\mathfrak{d}=\\sum_{z,Z_{-\\delta^{\\star}}}\\mu^{\\pi}(z,Z_{-\\delta^{\\star}})\\cdot R(Z_{-\\delta^{\\star}},X_{L+1-\\delta^{\\star}}),\\quad\\widehat{\\Phi}=\\sum_{z,Z_{-\\delta^{\\star}}}\\widehat{\\mu}_{X}^{\\pi}(z,Z_{-\\delta^{\\star}})\\cdot R(Z_{-\\delta^{\\star}},X_{L+1-\\delta^{\\star}}).\n$$", "text_format": "latex", "page_idx": 81}, {"type": "text", "text": "We also define ", "page_idx": 81}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathfrak{H}(z,Z_{-{\\mathcal S}^{\\star}})=\\mu^{\\pi}(z,Z_{-{\\mathcal S}^{\\star}})\\cdot R(Z_{-{\\mathcal S}^{\\star}},X_{L+1-{\\mathcal S}^{\\star}}),\\quad\\widehat{\\phi}(z,Z_{-{\\mathcal S}^{\\star}})=\\widehat{\\mu}_{X}^{\\pi}(z,Z_{-{\\mathcal S}^{\\star}})\\cdot R(Z_{-{\\mathcal S}^{\\star}},X_{L+1-{\\mathcal S}^{\\star}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 81}, {"type": "text", "text": "We can then rewrite the objective as ", "page_idx": 81}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\widetilde\\mu_{X}^{\\pi}(z=\\cdot,Z_{-S^{\\star}}=\\cdot)-\\widehat\\nu_{X}^{\\pi}(z=\\cdot,Z_{-S^{\\star}}=\\cdot)||_{1}}\\\\ &{\\quad=\\displaystyle\\sum_{z,Z_{-s^{\\star}}}\\left|\\frac{\\phi(z,Z_{-S^{\\star}})}{\\Phi}-\\frac{\\widehat\\phi(z,Z_{-S^{\\star}})}{\\widehat\\Phi}\\right|\\leq\\displaystyle\\sum_{z,Z_{-s^{\\star}}}\\frac{\\widehat\\phi(z,Z_{-S^{\\star}})\\cdot|\\widehat\\Phi-\\Phi|+|\\phi(z,Z_{-S^{\\star}})-\\widehat\\phi(z,Z_{-S^{\\star}})}{\\Phi\\cdot\\widehat\\Phi}}\\\\ &{\\qquad=\\displaystyle\\frac{|\\widehat\\Phi-\\Phi|+\\sum_{z,Z_{-s^{\\star}}}|\\phi(z,Z_{-S^{\\star}})-\\widehat\\phi(z,Z_{-S^{\\star}})|}{\\Phi}\\leq\\frac{2\\sum_{z,Z_{-s^{\\star}}}|\\phi(z,Z_{-S^{\\star}})-\\widehat\\phi(z,Z_{-S^{\\star}})|}{\\Phi}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 81}, {"type": "text", "text": "Furthermore, notice that ", "text_level": 1, "page_idx": 81}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\sum_{z,z_{-}s^{\\star}}|\\phi(z,Z_{-}s^{\\star})-\\widehat{\\phi}(z,Z_{-}s^{\\star})|}{\\Phi}=\\frac{\\sum_{z,Z_{-}s^{\\star}}|(\\mu^{\\pi}(z,Z_{-}s^{\\star})-\\widehat{\\mu}_{X}^{\\pi}(z,Z_{-}s^{\\star}))\\cdot R(Z_{-}s^{\\star},X_{L+1})|}{\\sum_{z,Z_{-}s^{\\star}}\\mu^{\\pi}(z,Z_{-}s^{\\star})\\cdot R(Z_{-}s^{\\star},X_{L+1-s^{\\star}})}}\\\\ &{\\quad\\leq\\frac{\\sum_{z,Z_{-}s^{\\star}}|(\\mu^{\\pi}(z,Z_{-}s^{\\star})-\\widehat{\\mu}_{X}^{\\pi}(z,Z_{-}s^{\\star}))|+(e^{a}-1)\\sum_{z,Z_{-}s^{\\star}\\in\\Gamma_{X}}|\\mu^{\\pi}(z,Z_{-}s^{\\star})-\\widehat{\\mu}_{X}^{\\pi}(z,Z_{-}s^{\\star})|}{1+(e^{a}-1)\\cdot\\sum_{z,Z_{-}s^{\\star}\\in\\Gamma_{X}}\\mu^{\\pi}(z,Z_{-}s^{\\star})}}\\\\ &{\\quad\\leq\\displaystyle\\sum_{z,Z_{-}s^{\\star}}|\\mu^{\\pi}(z,Z_{-}s^{\\star})-\\widehat{\\mu}_{X}^{\\pi}(z,Z_{-}s^{\\star})|+\\frac{\\sum_{z,Z_{-}s^{\\star}\\in\\Gamma_{X}}|(\\mu^{\\pi}(z,Z_{-}s^{\\star})-\\widehat{\\mu}_{X}^{\\pi}(z,Z_{-}s^{\\star}))|}{\\sum_{z,Z_{-}s^{\\star}\\in\\Gamma_{X}}\\mu^{\\pi}(z,Z_{-}s^{\\star})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 81}, {"type": "text", "text": "where we define $\\Gamma_{X}\\;=\\;\\left\\{Z_{-S^{\\star}}\\;:\\;Z_{-S^{\\star}}\\;=\\;X_{L+1-S^{\\star}}\\right\\}$ . Note that when $Z_{-S^{\\star}}\\,\\in\\,\\Gamma_{X}$ , we have $R(Z_{-}s_{^{\\star}},X_{L+1-}s_{^{\\star}})=e^{a}$ and when $Z_{-{\\cal S}^{\\star}}\\notin\\Gamma_{\\cal X}$ , we have $R(Z_{-S^{\\star}},X_{L+1-S^{\\star}})=1$ . For the first term on the right-hand side of (F.34), we have by Cauchy-Schwarz that ", "page_idx": 81}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathbb{\\xi}_{X}\\Bigg[\\displaystyle\\sum_{z,Z_{-s^{\\star}}}|\\mu^{\\pi}(z,Z_{-s^{\\star}})-\\widehat{\\mu}_{X}^{\\pi}(z,Z_{-s^{\\star}})|\\Bigg]\\le\\Bigg(\\mathbb{E}_{X}\\bigg[\\displaystyle\\sum_{z,Z_{-s^{\\star}}}\\displaystyle\\frac{(\\mu^{\\pi}(z,Z_{-s^{\\star}})-\\widehat{\\mu}_{X}^{\\pi}(z,Z_{-s^{\\star}}))^{2}}{\\mu^{\\pi}(z,Z_{-s^{\\star}})}\\bigg]\\Bigg)^{1/2}}\\\\ &{}&{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\le\\Bigg(\\displaystyle\\frac{4(1-\\lambda)^{-1}\\sqrt{D_{\\chi^{2}}(\\mu_{0}\\,\\|\\,\\mu^{\\pi})+1}+16M}{L\\cdot\\operatorname*{min}_{x_{L+1},X_{L+1-s^{\\star}}}\\mu^{\\pi}(x_{L+1},X_{L+1-s^{\\star}})}\\Bigg)^{1/2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 81}, {"type": "text", "text": "where in the last inequality, we invoke Lemma F.18 where we take $Y_{l}=x_{l}$ in the lemma. For the second term on the right hand of (F.34), we note that ", "page_idx": 82}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{X}\\Bigg[\\frac{\\sum_{z,Z_{-}s^{*}\\in\\Gamma_{X}}|\\big(\\mu^{\\pi}(z,Z_{-}s^{*})-\\widehat{\\mu}_{X}^{\\pi}(z,Z_{-}s^{*})\\big)|}{\\sum_{z,Z_{-}s^{*}\\in\\Gamma_{X}}\\mu^{\\pi}(z,Z_{-}s^{*})}\\Bigg]}\\\\ &{\\quad\\leq\\sum_{E,z}\\mathbb{E}_{X}\\,\\Bigg[\\frac{\\big|\\mu^{\\pi}\\big(z,Z_{-}s^{*}=E\\big)-\\widehat{\\mu}_{X}^{\\pi}(z,Z_{-}s^{*}=E)\\big|}{\\mu^{\\pi}(Z_{-}s^{*}=E)}\\cdot\\mathbf{1}(X_{L+1-s^{*}}=E)\\Bigg]}\\\\ &{\\quad\\leq\\displaystyle\\sum_{E,z}\\Bigg(\\mathbb{E}_{X}\\bigg[\\Big(\\frac{\\mu^{\\pi}(z,Z_{-}s^{*}=E)-\\widehat{\\mu}_{X}^{\\pi}(z,Z_{-}s^{*}=E)}{\\sqrt{\\mu^{\\pi}(Z_{-}s^{*}=E)}}\\Big)^{2}\\bigg]\\cdot\\frac{p^{\\pi}(X_{L+1-s^{*}}=E)}{\\mu^{\\pi}(Z_{-}s^{*}=E)}\\Bigg)^{1/2}}\\\\ &{\\quad\\leq\\bigg(\\mathbb{E}_{X}\\bigg[\\sum_{E,z}\\frac{\\big(\\mu^{\\pi}(z,Z_{-}s^{*}=E)-\\widehat{\\mu}_{X}^{\\pi}(z,Z_{-}s^{*}=E)\\big)^{2}\\big]}{\\mu^{\\pi}(Z_{-}s^{*}=E)}\\bigg]\\cdot\\sum_{E,z}\\frac{p^{\\pi}(X_{L+1-s^{*}}=E)}{\\mu^{\\pi}(Z_{-}s^{*}=E)}\\Bigg)^{1/2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 82}, {"type": "text", "text": "where the last two inequalities follow from the Cauchy-Schwarz inequality. We have an upper bound for the second term on the right-hand side of (F.35) that ", "page_idx": 82}, {"type": "equation", "text": "$$\n\\left(\\sum_{E,z}\\frac{p^{\\pi}(X_{L+1-\\mathcal{S}^{\\star}}=E)}{\\mu^{\\pi}(Z_{-\\mathcal{S}^{\\star}}=E)}\\right)^{1/2}\\leq\\sqrt{\\frac{1}{\\operatorname*{min}_{E}\\mu^{\\pi}(Z_{-\\mathcal{S}^{\\star}}=E)}}.\n$$", "text_format": "latex", "page_idx": 82}, {"type": "text", "text": "We can also apply Lemma F.18 to the first term with $Y_{L+1}=(x_{L+1},X_{L+1-\\mathcal{S}^{\\star}})$ and conclude that ", "page_idx": 82}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\bigg(\\mathbb{E}_{X}\\bigg[\\sum_{E}\\frac{\\big(\\mu^{\\pi}(z,Z_{-\\mathcal{S}^{\\star}}=E)-\\widehat\\mu_{X}^{\\pi}(z,Z_{-\\mathcal{S}^{\\star}}=E)\\big)^{2}}{\\mu^{\\pi}(Z_{-\\mathcal{S}^{\\star}}=E)}\\bigg]\\bigg)^{1/2}}\\\\ &{}&{\\leq\\bigg(\\frac{4(1-\\lambda)^{-1}\\sqrt{D_{\\chi^{2}}(\\mu_{0}\\,\\|\\,\\mu^{\\pi})+1}+16M}{L\\cdot\\operatorname*{min}_{x_{L+1},X_{L+1-\\mathcal{S}^{\\star}}}\\mu^{\\pi}(x_{L+1},X_{L+1-\\mathcal{S}^{\\star}})}\\bigg)^{1/2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 82}, {"type": "text", "text": "In summary, we have ", "page_idx": 82}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{X}\\left[\\|\\tilde{\\mu}_{X}^{\\pi}(e_{k})-y^{\\star}(k)\\|_{1}\\right]}\\\\ &{\\quad\\le\\frac{2}{\\operatorname*{min}_{x_{L+1},X_{L+1}-s^{\\star}}\\mu^{\\pi}(x_{L+1},X_{L+1-s^{\\star}})}\\cdot\\bigg(\\frac{(1-\\lambda)^{-1}\\sqrt{D_{\\chi^{2}}(\\mu_{0}\\,\\|\\,\\mu^{\\pi})+1}+4M}{L}\\bigg)^{1/2}}\\\\ &{\\quad\\quad+\\,2\\bigg(\\frac{(1-\\lambda)^{-1}\\sqrt{D_{\\chi^{2}}(\\mu_{0}\\,\\|\\,\\mu^{\\pi})+1}+4M}{L\\cdot\\operatorname*{min}_{x_{L+1},X_{L+1-s^{\\star}}}\\mu^{\\pi}(x_{L+1},X_{L+1-s^{\\star}}))}\\bigg)^{1/2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 82}, {"type": "text", "text": "Note that the second term is dominated by the first term. Thus, we conclude the proof of Proposition F.19. ", "page_idx": 82}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 83}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 83}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 83}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 83}, {"type": "text", "text": "Justification: The convergence results are established in Theorem 3.6 and we relate the learned transformer model to the generalized induction head in the following discussions. ", "page_idx": 83}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 83}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 83}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 83}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 83}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 83}, {"type": "text", "text": "Justification: We discussed in $\\S B.2$ ", "page_idx": 83}, {"type": "text", "text": "Guidelines: ", "page_idx": 83}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should refect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 83}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 83}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 83}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 83}, {"type": "text", "text": "Justification: We provide the full set of assumptions in Assumption 3.3 and Assumption 3.5. We provide a complete and correct proof in $\\S E$ and $\\S\\mathrm{F}$ ", "page_idx": 84}, {"type": "text", "text": "Guidelines: ", "page_idx": 84}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 84}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 84}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 84}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 84}, {"type": "text", "text": "Justification: We provide the complete details for our numerical experiment in $\\S B$ ", "page_idx": 84}, {"type": "text", "text": "Guidelines: ", "page_idx": 84}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. () If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 84}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 84}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 84}, {"type": "text", "text": "Answer: [No] ", "page_idx": 85}, {"type": "text", "text": "Justification: We do not release the data and code, but the details provided in $\\S\\mathrm{B}$ are sufficient for reproducing the synthetic data and the experiment results. ", "page_idx": 85}, {"type": "text", "text": "Guidelines: ", "page_idx": 85}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https : //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : / /nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 85}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 85}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 85}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 85}, {"type": "text", "text": "Justification: We provide all the training and test details in $\\S B$ ", "page_idx": 85}, {"type": "text", "text": "Guidelines: ", "page_idx": 85}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 85}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 85}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 85}, {"type": "text", "text": "Answer: [No] ", "page_idx": 85}, {"type": "text", "text": "Justification: We do not report error bars because the training behavior is consistent across different runs, and the goal of the experiments is to corroborate our main theoretical results, rather than achieving better performance on benchmarks. ", "page_idx": 85}, {"type": "text", "text": "Guidelines: ", "page_idx": 85}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative errorrates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 85}, {"type": "text", "text": "", "page_idx": 86}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 86}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 86}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 86}, {"type": "text", "text": "Justification: We provide information about compute resources in $\\S B$ ", "page_idx": 86}, {"type": "text", "text": "Guidelines: ", "page_idx": 86}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 86}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 86}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 86}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 86}, {"type": "text", "text": "Justification: We have reviewed the NeurIPs Code of Ethics and we confirm that our submission adheres to the guidelines therein. ", "page_idx": 86}, {"type": "text", "text": "Guidelines: ", "page_idx": 86}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 86}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 86}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 86}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 86}, {"type": "text", "text": "Justification: In the current paper, we focus on developing theoretical understanding of transformers, and the goal is to analyze existing architectures instead of proposing new models for better performance. Therefore, we do not see immediate societal impact of our paper. ", "page_idx": 86}, {"type": "text", "text": "Guidelines: ", "page_idx": 86}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 86}, {"type": "text", "text": "", "page_idx": 87}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 87}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 87}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 87}, {"type": "text", "text": "Justification: We do not release data or models that have a high risk for misuse Guidelines: ", "page_idx": 87}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safetyfilters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 87}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 87}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 87}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 87}, {"type": "text", "text": "Justification: We use common and standard Python libraries and write our own code for the experiments. Also, the data used in experiments is synthetic. ", "page_idx": 87}, {"type": "text", "text": "Guidelines: ", "page_idx": 87}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 87}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 88}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 88}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 88}, {"type": "text", "text": "Justification: We do not introduce new assets in the current paper. ", "page_idx": 88}, {"type": "text", "text": "Guidelines: ", "page_idx": 88}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 88}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 88}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 88}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 88}, {"type": "text", "text": "Justification: We perform theoretical analysis and numerical simulations on synthetic data, and the process is not related to human subjects. ", "page_idx": 88}, {"type": "text", "text": "Guidelines: ", "page_idx": 88}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 88}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 88}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)wereobtained? ", "page_idx": 88}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 88}, {"type": "text", "text": "Justification: As clarified in the answer above, our study is not related to human subjects. Guidelines: ", "page_idx": 88}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 88}]