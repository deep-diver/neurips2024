[{"figure_path": "4fN2REs0Ma/tables/tables_4_1.jpg", "caption": "Table 1: Three-stage training paradigm for gradient flow. Here, the \u201cWeights to Train\u201d column indicates the weights updated in each stage, and the \u201cDescription\u201d column summarizes the corresponding results from Theorem 3.6.", "description": "This table summarizes the three-stage training paradigm used in the paper's experiments and the observed behavior of the model's parameters at each stage. Stage I focuses on training the feed-forward network (FFN) layer to select the relevant features. Stage II trains the relative positional embedding (RPE) weights in the first attention layer to act as a copier. Stage III focuses on training the weights of the second attention layer to perform a generalized exponential kernel regression. The \"Description\" column provides a summary of the key dynamics and learning processes at each stage, and the \"Weights to Train\" column clarifies the specific model parameters being trained in each stage.", "section": "Main Results"}, {"figure_path": "4fN2REs0Ma/tables/tables_7_1.jpg", "caption": "Table 1: Three-stage training paradigm for gradient flow. Here, the \u201cWeights to Train\u201d column indicates the weights updated in each stage, and the \u201cDescription\u201d column summarizes the corresponding results from Theorem 3.6.", "description": "This table summarizes the three-stage training paradigm used in the paper. Each stage focuses on training a specific subset of the model's weights using gradient flow, while keeping the other weights fixed.  Stage I trains the feed-forward network (FFN) to learn the low-degree features, Stage II trains the relative positional embedding (RPE) in the first attention layer to copy relevant parent tokens, and Stage III trains the second attention layer to learn a softmax aggregation, effectively implementing the Generalized Induction Head (GIH) mechanism. The description column provides a summary of the model's behavior during each training stage.", "section": "Main Results"}, {"figure_path": "4fN2REs0Ma/tables/tables_19_1.jpg", "caption": "Table 1: Three-stage training paradigm for gradient flow. Here, the \u201cWeights to Train\u201d column indicates the weights updated in each stage, and the \u201cDescription\u201d column summarizes the corresponding results from Theorem 3.6.", "description": "This table describes a three-stage training process for gradient flow in a transformer model. Each stage focuses on training a specific subset of weights while keeping others fixed.  Stage I trains the feed-forward network (FFN) layer's parameters. Stage II trains the relative positional embedding (RPE) weights in the first attention layer. Stage III trains the scalar parameter 'a' in the second attention layer. The table provides a description of the dynamics and behavior in each stage. ", "section": "Main Results"}]