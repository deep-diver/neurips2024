[{"type": "text", "text": "Non-asymptotic Convergence of Training Transformers for Next-token Prediction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ruiquan Huang Yingbin Liang Jing Yang Penn State University Ohio State University Penn State Univeristy State College, PA, 16801 Columbus, OH, 43210 State College, PA, 16801 rzh5514@psu.edu liang.889@osu.edu yangjing@psu.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Transformers have achieved extraordinary success in modern machine learning due to their excellent ability to handle sequential data, especially in next-token prediction (NTP) tasks. However, the theoretical understanding of their performance in NTP is limited, with existing studies focusing mainly on asymptotic performance. This paper provides a fine-grained non-asymptotic analysis of the training dynamics of a one-layer transformer consisting of a self-attention module followed by a feed-forward layer. We first characterize the essential structural properties of training datasets for NTP using a mathematical framework based on partial orders. Then, we design a two-stage training algorithm, where the pre-processing stage for training the feed-forward layer and the main stage for training the attention layer exhibit fast convergence performance. Specifically, both layers converge sub-linearly to the direction of their corresponding max-margin solutions. We also show that the cross-entropy loss enjoys a linear convergence rate. Furthermore, we show that the trained transformer presents non-trivial prediction ability with dataset shift, which sheds light on the remarkable generalization performance of transformers. Our analysis technique involves the development of novel properties on the attention gradient and further in-depth analysis of how these properties contribute to the convergence of the training process. Our experiments further validate our theoretical findings. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The transformer architecture (Vaswani et al., 2017) has revolutionized the field of machine learning, establishing itself as a foundation model for numerous applications, including natural language processing (NLP) (Devlin et al., 2018), computer vision (Dosovitskiy et al., 2020), and multi-modal signal processing (Tsai et al., 2019). In particular, transformers achieve tremendous empirical success in large language models (LLMs) such as GPT-3 (Brown et al., 2020). Despite the empirical success, limited theoretical understanding of transformers have caused a series of critical concerns about their robustness, interpretability, and bias issues (Bommasani et al., 2021; Belkin, 2024). ", "page_idx": 0}, {"type": "text", "text": "To overcome these issues, recent advances in transformer theory have investigated the convergence of training transformers under theoretically amenable setting such as linear regression (Mahankali et al., 2023; Zhang et al., 2023; Huang et al., 2023) and binary classification (Tarzanagh et al., 2023b,a; Vasudeva et al., 2024; Li et al., 2023). Nevertheless, one of the fundamental task in LLMs and other generative models is next-token prediction (NTP), which involves predicting the next word or token in a sequence, given the previous tokens. In NTP, a few recent theoretical studies have started to investigate the training dynamics of transformers (Tian et al., $2023\\mathrm{a}$ ; Li et al., 2024). However, those works lack of fine-grained non-asymptotic convergence analysis of the training process, posing the following open questions for further investigation: ", "page_idx": 0}, {"type": "text", "text": "In addition, a pre-trained transformer empirically exhibits non-trivial generalization ability. A follow-up question from a theoretical point of view is that ", "page_idx": 1}, {"type": "text", "text": "Can we show the generalization capability of a trained transformer on unseen data? ", "page_idx": 1}, {"type": "text", "text": "In this paper, we take a first step towards addressing the aforementioned questions by studying the training dynamics of a single layer transformer consisting of a self-attention layer and a feed-forward layer for NTP. We summarize our contribution as follows. ", "page_idx": 1}, {"type": "text", "text": "\u00b7 We develop a mathematical framework based on partial order to formally characterize the essential structural properties of the training dataset for next-token prediction. In particular, we introduce a realizable setting for training datasets where the loss can be minimized to near zero, which admits a collocation and query-dependent partial orders. A collocation is a set of token pairs where each token is directly paired with its subsequent token. Query-dependent partial orders is a set of partial orders where each partial order classifies tokens into three categories: optimal tokens, non-optimal tokens and non-comparable tokens. These structural properties define favorable max-margin problems on both the feed-forward layer and the self-attention layer. ", "page_idx": 1}, {"type": "text", "text": "\u00b7 Second, we design a two-stage training algorithm based on normalized gradient descent. In stage 1 of pre-processing, we use the collocation to train the feed-forward layer. In stage 2, we use the entire dataset to train the self-attention layer. We show that the feed-forward layer and the query-key attention matrix converge sublinearly in direction respectively to the max-margin solution for classifying next token from all other tokens in the preprocessing dataset, and to the max-margin solution for classifying the optimal from non-optimal tokens. In addition, the norm of the transformer parameters grows linearly, which further yields a linear convergence rate of the cross-entropy loss. Our two-stage algorithm decouples the training of the feed-forward and attention layers without losing optimality, as stage 1's max-margin solution is judiciously designed to facilitate stage 2's fine-grained classification for optimal token prediction. ", "page_idx": 1}, {"type": "text", "text": "\u00b7 Third, we show that the trained transformer has generalization ability for making non-trivial prediction on unseen data. In particular, the transformer is trained to learn an extended querydependent partial order, where the non-comparable tokens are inserted in between the optimal tokens and non-optimal tokens. Thus, the trained transformer will attend to non-comparable tokens if optimal tokens are not in a new sentence and further make desirable prediction. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Inspired by Brown et al. (2020), who demonstrated that pre-trained transformers can learn in-context - i.e., learn new tasks during inference with only a few samples - a series of works focus on the expressiveness power of transformers (Akyirek et al., 2022; Bai et al., 2023; Von Oswald et al., 2023; Fu et al., 2023; Giannou et al., 2023; Lin et al., 2023). These studies have shown that there exist parameter configurations such that transformers can perform various algorithms such as gradient descent. Additionally, Edelman et al. (2022) showed that transformers can represent a sparse function. ", "page_idx": 1}, {"type": "text", "text": "Regarding the training dynamics and optimization of transformers under in-context learning, Ahn et al. (2024); Mahankali et al. (2023); Zhang et al. (2023); Huang et al. (2023) studied the dynamics of a single attention layer, single-head transformer for the in-context learning of linear regression tasks. Cui et al. (2024) proved that multi-head attention outperforms single-head attention. Cheng et al. (2023) showed that local optimal solutions in transformers can perform gradient descent in-context for non-linear functions. Kim and Suzuki (2024) studied the nonconvex mean-field dynamics of transformers, and Nichani et al. (2024) established a convergence rate of ${\\tilde{O}}(1/t)$ for the training loss in learning a causal graph. Additionally, Chen et al. (2024) investigated the gradient fow in training multi-head attention. Chen and Li (2024) proposed a supervised training algorithm for multi-head transformers. ", "page_idx": 1}, {"type": "text", "text": "Another line of research focuses on the training dynamics of transformers for binary classification problems. Tarzanagh et al. (2023b,a) demonstrated an equivalence between the optimization dynamics of a single attention layer and a certain SVM problem. While Tarzanagh et al. (2023b,a) only proved an asymptotic convergence result, Vasudeva et al. (2024) improved the convergence rate to $t^{-3/4}$ . Li et al. (2023) studied the training dynamics of vision transformers and showed that the generalization error can approach zero given sufficient training samples. Additionally, Deora et al. (2023) investigated the training and generalization error under the neural tangent kernel (NTK) regime. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "For transformers trained on next-token prediction (NTP), Tian et al. (2023a) analyzed the training dynamics of a single-layer transformer, while Tian et al. (2023b) studied the joint training dynamics of multi-layer transformers. Li et al. (2024) demonstrated the asymptotic convergence of transformers trained with a logarithmic loss function for NTP. Although these works provided valuable insights into the training dynamics of transformers for NTP, they did not provide the finite-time convergence analysis, which is the focus of this paper. We remark that Thrampoulidis (2024) studied NTP without transformer structure. ", "page_idx": 2}, {"type": "text", "text": "Our work is also related to the classical implicit bias framework for training neural networks (NNs). In particular Soudry et al. (2018); Nacson et al. (2019); Ji and Telgarsky (2021); Ji et al. (2021) established convergence rate of gradient descent-based optimization. Phuong and Lampert (2020); Frei et al. (2022); Kou et al. (2024) studied the implicit bias of ReLU/Leaky-ReLU networks on orthogonal data. A comprehensive survey is provided in Vardi (2023). However, these works focused on classical neural networks, whereas we investigate the implicit bias of transformers for NTP. ", "page_idx": 2}, {"type": "text", "text": "3 Problem Setup ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Notations. All vectors considered in this paper are column vectors. We use $\\mathbb{I}\\{A\\}$ to denote the indicator function of $A$ , i.e., ${\\mathbb I}\\{A\\}=1$ if $A$ holds, and $\\mathbb{1}\\{A\\}=0$ otherwise. $\\|W\\|$ represents the Frobenious norm of the matrix $W$ . For a vector $v$ , we use $[v]_{i}$ to denote the $i$ -th coordinate of $v$ We use $\\phi(v)$ to denote the softmax function, i.e., $\\begin{array}{r}{[\\phi(\\boldsymbol{v})]_{i}=\\exp(v_{i})/\\sum_{j}\\exp(e_{j}^{\\top}\\boldsymbol{v})}\\end{array}$ which can be applied to any vector with arbitrary dimension. We use $\\{e_{i}\\}_{i\\in[|\\mathcal{V}|]}$ to denote the canonical basis of $\\mathbb{R}^{|\\nu|}$ , i.e., $[e_{i}]_{j}=\\mathbb{1}\\{i=j\\}$ . The inner product $\\langle A,B\\rangle$ of two matrices $A,B$ equals to $\\operatorname{Trace}(A B^{\\top})$ ", "page_idx": 2}, {"type": "text", "text": "Next-token prediction. We consider the task of next-token prediction, which aims to predict the subsequent token in a token sequence given its preceding tokens. Formally, suppose that there exists a finite vocabulary set $\\mathcal{V}\\subset\\mathbb{R}^{d}$ that consists of all possible tokens, where $d$ is the dimension of the embedding. Each token $x\\in\\mathcal{V}$ is associated with a unique index $\\operatorname{I}(x)\\in\\{1,2,\\dots,|\\mathcal{V}|\\}$ , where Iis the index function. An $L$ -length sentence $X=[x_{1},\\dots,x_{L}]\\in\\mathcal{V}^{L}\\subset\\mathbb{R}^{d\\times L}$ is a sequence of $L$ tokens, where $L$ is an integer. We assume that the maximum length of sentences is $L_{\\mathrm{max}}$ . The subsequent tokensin sentences aregenerated from a set ofground-trthmodel $\\{p_{L}^{*}\\;:\\;\\nu^{L}\\;\\rightarrow\\;\\nu\\}_{L<\\dot{L}_{\\mathrm{max}}}$ where $p_{L}^{*}$ generates the next token $x_{L+1}$ given the sentence $X$ for any $1\\,\\leq\\,L\\,<\\,L_{\\mathrm{max}}$ The task of next-token prediction requires us to learn all models $\\{p_{L}^{*}\\}_{L<L_{\\mathrm{max}}}$ given a training dataset D $\\mathrm{~\\boldsymbol{\\mathscr{o}}~}_{0}=\\{(X,x_{L+1})|L<L_{\\operatorname*{max}},X\\in\\mathcal{V}^{L},x_{L+1}\\in\\mathcal{V}\\}$ . Notably, if $X=[x_{1},\\ldots,x_{L}]\\in{\\mathcal{D}}_{0}$ , then for any $\\ell<L$ $([x_{1},\\dots,x_{\\ell}],x_{\\ell+1})$ is also a training sample, since it follows $p_{\\ell}^{*}$ as well. ", "page_idx": 2}, {"type": "text", "text": "Decoder-only transformer. A decoder-only transformer is a stack of blocks consisting of a selfattention layer and a feed-forward layer. For simplicity, we consider one-layer transformer, where the self-attention layer is determined by three matrices: $\\dot{W_{\\mathrm{k}}}\\in\\mathbb{R}^{d\\times d_{1}}$ \uff0c $W_{\\mathrm{q}}\\in\\dot{\\mathbb{R}}^{d_{1}\\times d}$ and $W_{\\mathrm{v}}\\in\\mathbb{R}^{d_{2}\\times d}$ namely key, query, and value matrices, and the feed-forward layer is determined by $W_{\\mathrm{o}}\\in\\mathbb{R}^{|\\mathcal{V}|\\times d_{2}}$ Here $d_{1},d_{2}$ are hidden dimensions. Mathematically, given the input $X=\\left[x_{1}\\ldots,x_{L}\\right]$ ,we write the one-layer transformer as $\\mathrm{T}_{\\theta}(X)\\;:=\\;\\phi(W_{\\mathrm{o}}W_{\\mathrm{v}}X\\phi(X^{\\top}W_{\\mathrm{k}}W_{\\mathrm{q}}x_{L}))\\;\\in\\;[0,1]^{|\\mathcal{V}|}$ , where $\\theta\\ :=$ $(W_{\\mathrm{o}},W_{\\mathrm{v}},\\dot{W}_{\\mathrm{k}},W_{\\mathrm{q}})$ , and $\\phi$ is the softmax function. We note that the inner softmax function $\\phi$ is part of the attention model, and the outer softmax function $\\phi$ is the decoder that generates a probability distribution over $\\mathcal{V}$ for token prediction. ", "page_idx": 2}, {"type": "text", "text": "Reparameterization. We reparameterize the transformer architecture by consolidating the key and query matrices into a unified matrix $W_{\\mathrm{kq}}$ , such that $W_{\\mathrm{kq}}=W_{\\mathrm{k}}W_{\\mathrm{q}}$ . Similarly, we reparameterize the product of the feed-forward $(W_{\\mathrm{o}})$ and value $(W_{\\mathrm{v}})$ matrices as a single matrix $W_{\\mathrm{ov}}$ , defined as $W_{\\mathrm{ov}}=W_{\\mathrm{o}}W_{\\mathrm{v}}$ . Such a reparameterization is commonly adopted in transformer theory works (Huang et al., 2023; Tian et al., $2023\\mathrm{a}$ ; Li et al., 2024; Nichani et al., 2024). Thus, the transformer under those reparameterization is given by $\\mathrm{T}_{\\theta}(X):=\\phi(W_{\\mathrm{ov}}X\\phi(X^{\\top}W_{\\mathrm{kq}}x_{L}))\\in[0,1]^{|\\mathcal{V}|}$ ", "page_idx": 2}, {"type": "text", "text": "Cross-entropy loss. Given the training dataset $\\mathcal{D}_{0}$ and the transformer model, we seek to learn $p_{*}$ by minimizing (training) the cross-entropy loss ${\\mathcal{L}}(\\theta)$ defined as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\theta)=-\\frac{1}{|\\mathcal{D}_{0}|}\\sum_{(X,x_{L+1})\\in\\mathcal{D}_{0}}\\log e_{\\mathrm{I}(x_{L+1})}^{\\top}\\mathrm{T}_{\\theta}(X),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "4  Realizable Training Dataset and Two-Stage Algorithm ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we first provide a mathematical framework based on partial order to formally characterize a realizable training dataset for next-token prediction. We will then describe a two-stage algorithm for next-token prediction that we study. ", "page_idx": 3}, {"type": "text", "text": "4.1  Realizable Training Dataset ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We characterize a realizable training dataset via two structural properties, where the training loss can be made arbitrarily close to zero. We first provide some intuitions about those two properties. ", "page_idx": 3}, {"type": "text", "text": "Existence of \u201ccollocation\". First, we note that if a sentence $X=[x_{1},\\ldots,x_{L}]$ is a legal training sample, $\\left(\\left[x_{1}\\right],x_{2}\\right)$ is also in the training dataset. In addition, the output of a transformer given one single input token only depends on $W_{\\mathrm{ov}}$ , i.e. the feed-forward layer. Since training loss can be arbitrarly closeto, therexists asequene $\\{W_{t}\\}$ such that $\\begin{array}{r}{\\operatorname*{lim}_{t\\rightarrow\\infty}\\dot{-\\sum_{x\\in\\mathcal{D}_{0}}\\log e_{\\iota}(x)^{\\top}}\\phi(W_{t}x)=}\\end{array}$ 0, where $\\iota(x)$ is the index of next token of $x$ , and the summation is over the case when $x$ is the first token. Due to that $\\phi(W_{t}x)$ is a probability distribution, the equality holds only when $\\iota$ is injective, since otherwise it is an entropy of some distribution which is strictly greater than O. Therefore, there exists an injective map $\\mathrm{n}:\\mathcal{V}\\rightarrow\\mathcal{V}$ such that every sentence starts with $x$ , must have a unique next token $\\mathrm{n}(x)$ . We call the set of pairs $\\{x,\\mathrm{n}(x)\\}_{x\\in\\mathcal{V}}$ a collocation. We remark that $p_{1}^{*}=\\mathrm{n}$ ", "page_idx": 3}, {"type": "text", "text": "Existence of \u201corder\". Second, let us consider the output of a transformer $\\operatorname{T}_{\\theta}$ given a legal sentence $X=[x_{1},\\ldots,x_{L}]$ with the next token $x_{L+1}=p_{L}^{*}(X)$ . The transformer first calculates a convex combination of $x_{1},\\ldots,x_{L}$ with corresponding weight $\\varphi_{\\ell}\\propto\\exp(x_{\\ell}^{\\top}W_{{\\mathrm{kq}}}x_{L})$ for each $\\ell\\leq L$ . Then, the transformer outputs $\\phi(\\textstyle\\sum_{\\ell}W_{\\mathrm{ov}}x_{\\ell}\\bar{\\varphi_{\\ell}})$ . Recall that the collocation forces $x_{\\ell}$ to map to $\\mathrm{n}(x_{\\ell})$ , thus $\\phi(W_{\\mathrm{ov}}x_{\\ell})$ has a peak value at the coordinate equal to $\\operatorname{I}(\\mathrm{n}(x_{\\ell}))$ (the index of $\\mathrm{n}(x_{\\ell}))$ . Hence, $\\operatorname{T}_{\\theta}(X)$ can only have peak value at the coordinates within the set $\\{\\mathrm{I}(\\mathrm{n}(x_{\\ell}))\\}_{\\ell\\leq L}$ . If the training loss can be arbitrarily close to O, it is desirable to have $\\mathrm{n}^{-1}(x_{L+1})\\in\\{x_{\\ell}\\}_{\\ell\\leq L}$ Therefore, for those $x_{\\ell}$ with $\\mathrm{n}(x_{\\ell})\\,=\\,x_{L+1}$ \uff0c $\\varphi_{\\ell}$ must be larger than $\\varphi_{\\ell^{\\prime}}$ with $\\mathrm{n}(x_{\\ell^{\\prime}})\\neq x_{L+1}$ . Finally, it worth noting that $\\varphi_{\\ell}$ depends on the final token $x_{L}$ . This observation motivates us to define query-dependent partial orders on $\\mathcal{V}$ ", "page_idx": 3}, {"type": "text", "text": "Definition 1 ( $x^{q}$ -partial order)Fix a token $x^{q}$ $A n\\,x^{q}$ -partial order assigns an ordering relationship $>_{x^{q}}$ for certain pairs of tokens in $\\nu$ and is created as follows. Let $\\mathcal{D}_{0}^{x^{q}}$ be the set of all legal sentences in the training dataset that has the final token (query) $x^{q}$ Then, for any pair of tokens $x,x^{\\prime}\\in\\mathcal{V}$ we assign $x>_{x^{q}}x^{\\prime}$ if there exists a sentence $X=[x_{1},\\dots,x_{L}]\\in{\\mathcal{D}}_{0}^{x^{q}}$ and $x,x^{\\prime}$ are tokens in $X$ such that $\\mathrm{n}(x)=x_{L+1}\\neq\\mathrm{n}(x^{\\prime}).$ where $x_{L+1}$ is the next token of $X$ ", "page_idx": 3}, {"type": "text", "text": "Note that Definition 1 is a \u201cconstructive definition\u201d which might not be well-defined. However, as we are under the setting when the training loss can be arbitrarily close to O, the aforementioned discussion shows that if $x>_{x^{q}}x^{\\prime}$ , then $\\varphi_{\\ell}>\\varphi_{\\ell^{\\prime}}$ , where $x=x_{\\ell}$ and $x^{\\prime}=x\\ell^{\\prime}$ in some sentence. Thus, $\\exp(x W_{\\mathrm{kq}}x^{q})>\\exp(x^{\\prime}W_{\\mathrm{kq}}x^{q}).$ , which indeed need to be well-defined. Otherwise, we will have contradictions such as $\\exp(x\\mathrm{{\\calW}}_{\\mathrm{kq}}x^{q})>\\exp(x^{\\prime}{\\cal W}_{\\mathrm{kq}}x^{q})<\\exp(x{\\cal W}_{\\mathrm{kq}}x^{q})$ . Mathematically, a well-defined (strict) partial order $>$ on a set $\\mathcal{V}$ satisfies two axioms (Yannakakis, 1982): (i) there is no $x>x$ (ii) if $x>x^{\\prime}$ and $x^{\\prime}>x^{\\prime\\prime}$ , then $x>x^{\\prime\\prime}$ . Thus, $x^{q}$ -partial order created by $\\mathcal{D}_{0}$ is well-defined for every $x^{q}\\in\\mathcal{V}$ ", "page_idx": 3}, {"type": "text", "text": "Finally, let us discuss the impact of query-dependent partial orders on $\\mathcal{D}_{0}$ . For a given query $x^{q}$ the partial order $>_{x^{q}}$ divides tokens in $\\nu$ into four disjoint types. ", "page_idx": 3}, {"type": "text", "text": "\u00b7(Strict) optimal tokens. A token $x$ is optimal, if there is no $x^{\\prime}$ such that $x^{\\prime}>_{x^{q}}x^{1}$ \u00b7 Confused tokens. A token $x$ is confused, if there exists $x^{\\prime},x^{\\prime\\prime}$ such that $x^{\\prime}>_{x^{q}}x>_{x^{q}}x^{\\prime\\prime}$ \u00b7 (Strict) non-optimal tokens. A token $x$ is non-optimal if there is no $x^{\\prime}$ such that $x>_{x^{q}}x^{\\prime2}$ \u00b7 Non-comparable tokens. A token $x$ is non-comparable if there is no $x^{\\prime}$ such that $x\\,>_{x^{q}}x^{\\prime}$ or $x^{\\prime}>_{x^{q}}x$ ", "page_idx": 3}, {"type": "text", "text": "In this work, we assume that there are no confused tokens. This assumption simplifies the problem, making it tractable to provide explicit convergence in direction for training a transformer in Section 5. In summary, we make the following structural assumption on the training dataset. ", "page_idx": 4}, {"type": "text", "text": "Assumption 1 (Realizable training dataset) $\\mathcal{D}_{0}$ admits $(i)$ acollocation $\\{x,\\mathrm{n}(x)\\}_{x\\in\\mathcal{V}}$ $(i i)$ welldefined query-dependent partial orders, where every $x^{q}$ -partial order has no confused tokens. ", "page_idx": 4}, {"type": "text", "text": "We remark that combining the collocation and query-dependent partial orders, we can regenerate the training dataset as follows. For any sentence with only one token $X=[x]$ , the next token is $\\mathrm{n}(x)$ For other sentences $X=[x_{1},\\ldots,x_{L}]$ , let $x_{\\ell}$ be optimal under the partial order $>_{x_{L}}$ , and then the next token of $X$ is $\\mathrm{n}(x_{\\ell})$ . We next provide a simple example that justifies Assumption 1. ", "page_idx": 4}, {"type": "text", "text": "Example 1 Consider a language system where the vocabulary consists of four tokens $\\{S,\\ V,\\ O_{!}$ $P\\}$ where $S,V,O,P$ respectively stand for subject, verb, object, and punctuation mark. This system admits the commonly adopted word order (Dryer, 1991): S, V, O, P. Let the training dataset be $\\{S V O P,\\,V O P,\\,O P P,\\,P S V\\}$ ", "page_idx": 4}, {"type": "text", "text": "Let us create the corresponding collocation and the query-dependent partial orders from the dataset. Thecollocationis $\\{(S,\\,\\bar{V}),(V,\\,\\bar{O}),(O,\\,P),(P,\\,S)\\}$ .That is, if a sentence starts with a subject, then the next token is a verb.Similarly, if a sentence starts with a verb, then the next token is an object, and so on.The query-dependent partial orders are created as follows: ", "page_idx": 4}, {"type": "text", "text": "Partial order under query S. $S{>}s P.$ Partial order under query $V.\\ V{>}_{V}S.$ Partial order under query $O.\\,\\,O{>}o S,\\,O{>}o V.$ Partial order under query P. $O{>}P P.\\,$ ", "page_idx": 4}, {"type": "text", "text": "Therefore, if a sentence starts with S (subject), the next token is $V\\left(\\nu e r b\\right)$ according to the collocation. Then, for the sentence $S V,$ since the query is $V$ and $V{>}_{V}S_{\\mathrm{{r}}}$ the next token of the sentence coincides with the next token of $V,$ which is exactly $o$ (object). Finally, for the sentence SVo, following similar argument,thenext token is $P$ (punctuation mark). This example satisfies Assumption 1 and aligns with real-world scenarios. An illustration is provided in Figure 1. ", "page_idx": 4}, {"type": "image", "img_path": "NfOFbPpYII/tmp/00418bd0ccc93f554b7047bf26d39af55651705fc1d7684a14a1c4f367ebaf0b.jpg", "img_caption": ["Figure 1: The left plot shows the mapping from sentence to the next token. The red rectangle indicates the optimal token in the corresponding sentence. The right plot shows the collocation relationship. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Additional notations of training data. It is worth noting that there are only finite number of distinct sentences. For ease of presentation, we introduce the following notations. Suppose there are $N$ distinct sentences in the training dataset $\\mathcal{D}_{0}$ indexed by $n\\in\\{1,\\bar{\\dots},N\\}$ . For each distinct sentence X(m),wcalla itsfqy(m 0,1daastDas (m) Building upon this, with alitte abuse of ntation, we $\\mathrm{n}(X^{(n)})\\in\\mathcal{V}$ to denote the subsequent token of the sentence $X^{(n)}$ and $\\operatorname{In}(X^{(n)})$ to denote the index of $\\mathrm{n}(X^{(n)})$ ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "We further denote X(2) as the final token of $X^{(n)}$ , and let $\\bar{\\mathrm{T}}_{\\theta}(X)=W_{\\mathrm{ov}}X\\phi(X^{\\top}W_{\\mathrm{kq}}X_{-1}^{(n)})$ . Then, the loss function ${\\mathcal{L}}(\\theta)$ can be rewritten as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\theta)=\\sum_{n}\\pi^{(n)}\\left(\\log\\left(\\sum_{v}\\exp\\left(e_{v}^{\\top}\\bar{\\mathrm{T}}_{\\theta}(X^{(n)})\\right)\\right)-e_{\\mathrm{In}(X^{(n)})}^{\\top}\\bar{\\mathrm{T}}_{\\theta}(X^{(n)})\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "4.2  Training Algorithm ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "For the realizable dataset satisfying Assumption 1, we propose a two-stage training algorithm using normalized gradient descent (NGD). The pseudo code of the algorithm is presented in Algorithm 1. In Section 5, we show that the two-stage algorithm decouples the training of the feed-forward and attention layers without losing the optimality. This is because the training in stage 1 is designed to yield a suitable max-margin solution, which will enable the training of stage 2 to solve a fine-grained classifcation problem and identify the optimal token for prediction. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "In the first stage of pre-processing, we use the collocation set to train the feed-forward layer $W_{\\mathrm{ov}}$ For simplicity, we introduce the following notation for the training loss of the feed-forward layer. Given a collocation $\\{x,\\mathrm{n}(x)\\}_{x\\in\\mathcal{V}}$ , which can be obtained through extracting all length-2 sentences in the training dataset $\\mathcal{D}_{0}{}^{3}$ , we use normalized gradient descent to train $W_{\\mathrm{ov}}$ . Equivalently, the loss function can be written as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{0}(W_{\\mathrm{ov}})=-\\sum_{x\\in\\mathcal{V}}\\log\\frac{\\exp(e_{\\mathrm{In}(x)}^{\\top}W_{\\mathrm{ov}}x)}{\\sum_{v\\le|\\mathcal{V}|}\\exp(e_{v}^{\\top}W_{\\mathrm{ov}}x)},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the self-attention elements are removed because the attention matrices are not trained here. Based onthe above lossfuntionweintialize $W_{\\mathrm{ov}}^{(0)}=0\\in\\mathbb{R}^{|\\mathcal{V}|\\times d}$ andsubsequenlytakeanupdate at each time $t$ by NGD as in line 4 of Algorithm 1. ", "page_idx": 5}, {"type": "text", "text": "In the second stage, we fix the trained feed-forward layer and train the self-attention layer based on the loss function given in Equation (1) and using the entire dataset $\\mathcal{D}_{0}$ Specifically,we initialize $W_{\\mathrm{kq}}=0\\in\\mathbb{R}^{d\\times d}$ , and subsequentlytake an update at each time $t$ by NGD as in line 7 of Algorithm 1. ", "page_idx": 5}, {"type": "text", "text": "Algorithm 1 Two-stage Normalized Gradient Descent ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "1: Initiliaion: $W_{\\mathrm{ov}}^{(0)}=0\\in\\mathbb{R}^{|\\mathcal{V}|\\times d}$ \uff0c $W_{\\mathrm{kq}}=0\\in\\mathbb{R}^{d\\times d}$   \n2: Input: A collocation $\\{x,\\mathrm{n}(x)\\}_{x\\in\\mathcal{V}}$ , and a training dataset $\\mathcal{D}_{0}$ , learning rate $\\eta_{0},\\eta$   \n3: for $t\\in\\{0,1,...,T-\\dot{1}\\}$ do   \n4: Update W(t+1) a asw(+1)w   \n5: end for   \n6: for $t\\in\\{0,\\ldots,T_{1}-1\\}$ do   \n7: Update W(t+1) a as $\\begin{array}{r}{\\dot{W_{\\mathrm{kq}}}^{(t+1)}=W_{\\mathrm{kq}}^{(t)}-\\eta\\frac{\\nabla_{W_{\\mathrm{kq}}}\\mathcal{L}(\\theta^{(t)})}{\\lVert\\nabla_{W_{\\mathrm{kq}}}\\mathcal{L}(\\theta^{(t)})\\rVert}}\\end{array}$ m() where () =(W, w).   \n8: end for ", "page_idx": 5}, {"type": "text", "text": "5  Training Dynamics of the Transformer ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we present the convergence result for Algorithm 1. Before we proceed, we first introduce the following technical assumption, which has been commonly adopted in the previous theoretical studies of transformers (Huang et al., 2023; Li et al., 2024; Tian et al., 2023a). ", "page_idx": 5}, {"type": "text", "text": "Assumption 2 The vocabulary set is orthornormal. Namely, the embedding has unit norm, i.e., $\\|x\\|={\\bar{1}}$ and $x^{\\top}x^{\\prime}=0$ holdsfor anydistincttokens $x$ and $x^{\\prime}$ ", "page_idx": 5}, {"type": "text", "text": "5.1  Convergence of Training $W_{\\mathrm{ov}}$ ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To characterize the training dynamics of $W_{\\mathrm{ov}}$ , we observe that the collocation $\\{(x,\\mathrm{n}(x))\\}_{x\\in\\mathcal{V}}$ defines the following hard-margin problem: ", "page_idx": 5}, {"type": "equation", "text": "$$\nW_{\\mathrm{ov}}^{*}=\\operatorname{arg\\,min}\\|W\\|,\\quad\\mathrm{s.t.}\\quad(e_{v^{*}}-e_{v})W x\\geq1,\\quad\\forall v^{*}=\\operatorname{In}(x),v\\neq\\operatorname{In}(x).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "It can be shown that $\\begin{array}{r}{\\operatorname*{lim}_{B\\to+\\infty}\\mathcal{L}_{0}(B W_{\\mathrm{ov}}^{*})=0}\\end{array}$ . Thus, the loss function ${\\mathcal{L}}_{0}$ trains $W_{\\mathrm{ov}}$ to be the max-margin solution with $W_{\\mathrm{ov}}x$ distinguishing the next token $\\mathrm{n}(x)$ from all other tokens in $\\mathcal{V}$ ", "page_idx": 5}, {"type": "text", "text": "Since $\\mathcal{L}_{0}(\\cdot)$ is $W_{\\mathrm{ov}}^{(t)}$ ", "page_idx": 5}, {"type": "text", "text": "Proposition 1 Let $W_{\\mathrm{ov}}^{*}$ be defined in Equation (2). Under Assumptions 1-2, let $W_{\\mathrm{ov}}^{(t)}$ be updated by Algorithm $^{\\,l}$ Then, for any $t\\geq2$ we have $\\begin{array}{r}{\\frac{t\\eta_{0}}{2\\|W_{\\mathrm{ov}}^{*}\\|}\\leq\\|W_{\\mathrm{ov}}^{(t)}\\|\\leq t\\eta_{0}}\\end{array}$ and the folowing bound holds: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\left\\langle\\frac{W_{\\mathrm{ov}}^{(t)}}{\\|W_{\\mathrm{ov}}^{(t)}\\|},\\frac{W_{\\mathrm{ov}}^{*}}{\\|W_{\\mathrm{ov}}^{*}\\|}\\right\\rangle\\geq1-\\frac{5\\|W_{\\mathrm{ov}}^{*}\\|^{3}\\log(2|\\mathcal{V}|)\\log t}{t\\eta_{0}}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Moreover,thelossfunction $\\scriptstyle{\\mathcal{L}}_{0}$ satisfiesthat $\\mathcal{L}_{0}(W_{\\mathrm{ov}}^{(t)})\\leq O(\\exp(-\\eta_{0}t/(4\\|W_{\\mathrm{ov}}^{*}\\|))).$ ", "page_idx": 6}, {"type": "text", "text": "Proposit statthaing t traing ta, thfforwad ly $W_{\\mathrm{ov}}^{(t)}$ converges in direction to $W_{\\mathrm{ov}}^{*}/\\Vert W_{\\mathrm{ov}}^{*}\\Vert$ at a rate of $O(\\log t/t)$ , which classifies the next token from all other tokens. In addition, since the norm of $W_{\\mathrm{ov}}^{(t)}$ increases linearly, the loss $\\mathcal{L}_{0}(W_{\\mathrm{ov}}^{(t)})$ converges linearly to zero, i.e., $\\mathcal{L}_{0}(W_{\\mathrm{ov}}^{(t)})=O(\\exp(-C_{0}t))$ for some constant $C_{0}$ ", "page_idx": 6}, {"type": "text", "text": "5.2  Convergence of Training $W_{\\mathrm{kq}}$ ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Recall that after the training stage 1 with $T$ steps, we obtain a trained feed-forward layer $W_{\\mathrm{ov}}^{(T)}$ Then, we fx $W_{\\mathrm{ov}}^{(T)}$ anduse normalidgradent deent to tain $W_{\\mathrm{kq}}$ To characteriethe training dynamics of the key-query matrix $W_{\\mathrm{kq}}$ , we note that each query-dependent partial order also defines a hard-margin problem. Let $l(n)\\subset\\{1,\\ldots,L^{(n)}\\}$ be the set of indices of the optimal tokens of $X^{(n)}$ Recall that $x_{\\ell}$ is optimal if there is no $x_{\\ell^{\\prime}}$ such that $x_{\\ell^{\\prime}}>_{X_{-1}^{(n)}}x_{\\ell}$ and $\\mathrm{In}(x_{\\ell})=\\mathrm{In}(X^{(n)})$ . That is,. ", "page_idx": 6}, {"type": "text", "text": "$W_{\\mathrm{kq}}X_{-1}^{(n)}$ should correctlyclasify optimal tokens $x_{\\ell}$ and non-optimal tokens $x_{\\ell^{\\prime}}$ . This s formalized in the following problem: ", "page_idx": 6}, {"type": "equation", "text": "$$\nW_{\\mathrm{kq}}^{*}=\\arg\\operatorname*{min}\\|W\\|,\\quad\\mathrm{s.t.}\\quad(x_{\\ell_{*}}^{(n)}-x_{\\ell}^{(n)})W X_{-1}^{(n)}\\geq1,\\quad\\forall\\ell_{*}\\in l(n),\\ell\\not\\in l(n),\\forall n.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We wil show thathe lss function in Equation (1I given the well rained $W_{\\mathrm{ov}}^{(T)}$ Wiltrain $W_{\\mathrm{kq}}$ towards the max-margin solution $W_{\\mathrm{kq}}^{*}$ in direction for classifying between the optimal and non-optimal token. We further make the following technical assumption. ", "page_idx": 6}, {"type": "text", "text": "Assumption 3 For any sample $X^{(n)}$ , the number of optimal tokens is not less than the number of non-optimal tokens. Formally, for any non-optimal token $x$ .\u4e09 $X^{(n)}$ wehave $\\begin{array}{r}{|l(n)|\\ge\\sum_{\\ell}\\mathbb{1}\\{x_{\\ell}=x\\}}\\end{array}$ Assumption 3 is consistent with practical and empirical observations, where optimal tokens often demonstrate higher relevance, making them more frequent in subsequent outcomes. ", "page_idx": 6}, {"type": "text", "text": "We now present the convergence result for the training of the key-query matrix in stage 2. ", "page_idx": 6}, {"type": "text", "text": "Theorem 1 Let Assumptions 1-3 hold. Let $W_{\\mathrm{kq}}^{*}$ be the solution of Equation (3). Let $\\eta<O(1)$ and $W_{\\mathrm{kq}}^{(t)}$ be updated by Algorithmn $^{\\,l}$ Then, fo any $t\\geq2$ we have hat $\\begin{array}{r}{\\frac{t\\eta}{2\\vert\\vert W_{\\mathbf{kq}}^{*}\\vert\\vert}\\leq\\vert\\vert W_{\\mathbf{kq}}^{(t)}\\vert\\vert\\leq t\\eta}\\end{array}$ In addition, the following inequality holds. ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\left\\langle\\frac{W_{\\mathrm{kq}}^{(t)}}{\\|W_{\\mathrm{kq}}^{(t)}\\|},\\frac{W_{\\mathrm{kq}}^{*}}{\\|W_{\\mathrm{kq}}^{*}\\|}\\right\\rangle\\geq1-\\frac{54N L_{\\operatorname*{max}}^{4}\\|W_{\\mathrm{kq}}^{*}\\|^{4}\\log^{2}t}{t\\eta}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Theorem 1 states that the key-query matrix $W_{\\mathrm{kq}}$ converges in direction to the max-margin solution $W_{\\mathrm{kq}}^{*}/\\Vert W_{\\mathrm{kq}}^{*}\\Vert$ at a convergence rate of $O(\\log^{2}t/\\bar{t})$ . We further show that the norm of $W_{\\mathrm{kq}}$ also grows linearly in $t$ i.e., $\\|W_{\\mathrm{kq}}\\|=\\Omega(t)$ . Combining these results, we have the following theorem on the convergence of the loss function and the training accuracy. ", "page_idx": 6}, {"type": "text", "text": "$X^{(n)}=[x_{1}^{(n)},\\ldots,x_{L}^{(n)}].$ $\\varphi_{\\ell}^{(n,t)}\\propto$ $\\exp(x_{\\ell}^{(n)}W_{\\mathrm{kq}}^{(t)}x_{L}^{(n)})$ $^{\\,l}$ constant $c_{\\mathrm{0}}$ such that when $T\\geq c_{0}\\|W_{\\mathrm{ov}}^{*}\\|^{5}\\log(|\\mathcal{V}|)\\log T/\\eta_{0}$ and $t\\geq c_{0}N L_{\\mathrm{max}}^{4}\\|W_{\\mathrm{kq}}^{*}\\|^{6}\\log^{2}t/\\eta,$ the optimal token weight satisfes minn eel(n) PEn,t) (2t \u2265(1 + Lmaxexp (-tC)-1. Inadtion, the loss function $\\mathcal{L}$ converges linearly4 to its minimal value: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\theta^{(t)})=\\mathcal{L}(W_{\\mathrm{ov}}^{(T)},W_{\\mathrm{kq}}^{(t)})\\leq|\\mathcal{V}|\\exp\\left(-T C_{0}\\left(1-\\frac{2L_{\\operatorname*{max}}}{L_{\\operatorname*{max}}+\\exp(C_{1}t)}\\right)\\right),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Theorem 2 shows that the training loss converges to its minimum value at a linear convergence rate. Furtherm, for $T=\\Omega(\\log(1/\\epsilon_{0}))$ $t=\\Omega(\\log(\\bar{1/\\epsilon}))$ , the optimal token weight is given by $\\mathrm{1/(1+\\epsilon)}$ for any $\\epsilon>0$ , which is close to 1. This implies that the trained transformer attends to the optimal token and thus outputs the correct next token $\\mathrm{n}(x_{\\ell_{*}}^{(n)})$ with probability $1-O(\\epsilon_{0})$ ", "page_idx": 7}, {"type": "text", "text": "5.3Proof Sketch of Theorem 1 ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The proof consists of the following three main steps. The key proof step lies in carefully analyzing the projection of gradient $\\nabla_{W_{\\mathrm{kq}}}\\mathcal{L}(\\theta^{(t)})$ onto the token-query outer product $x_{\\ell}^{(n)}(X_{-1}^{(n)})^{\\dag}$ ,max-margin attention weight matrix Wkg and the rained atetion weight matrix W(t \\* ", "page_idx": 7}, {"type": "text", "text": "Step 1 (Lemma 5). By analyzing $\\left\\langle\\nabla_{W_{\\mathrm{kq}}}\\mathcal{L}(\\theta^{(t)}),x_{\\ell}^{(n)}(X_{-1}^{(n)})^{\\top}\\right\\rangle$ we characterize the dynamics of attention weights. Using mathematical induction, we show that the lower bound of optimal token weightis $1/L_{\\mathrm{max}}$ ", "page_idx": 7}, {"type": "text", "text": "Step 2 (Lemma 6). Then, we show that the cosine similarity between the negative gradient and $W_{\\mathrm{kq}}^{*}$ is strictly larger than the minimum optimal token weight. Utilizing step 1, due to the NGD update, the norm of the key-query matrix $W_{\\mathrm{kq}}^{(t)}$ can be shown to grow linearly. ", "page_idx": 7}, {"type": "text", "text": "Step 3 (Lemma 7). Finally, we carefully compare the difference between the projections from gradient to the trained attention matrix and max-margin attention matrix. By separately evaluating the impact of the optimal and non-optimal tokens on those projections, we can show the following inequality for some constant $C_{0}$ ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\left\\langle\\nabla_{W_{\\mathrm{kq}}}\\mathcal{L}(\\theta^{(t)}),W_{\\mathrm{kq}}^{(t)}\\right\\rangle\\geq\\left(1+\\frac{C_{0}\\log\\|W_{\\mathrm{kq}}^{(t)}\\|}{\\|W_{\\mathrm{kq}}^{(t)}\\|}\\right)\\left\\langle\\nabla_{W_{\\mathrm{kq}}}\\mathcal{L}(\\theta^{(t)}),W_{\\mathrm{kq}}^{*}\\right\\rangle\\frac{\\|W_{\\mathrm{kq}}^{(t)}\\|}{\\|W_{\\mathrm{kq}}^{*}\\|}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Utilizing step 2's result that $\\|W_{\\mathrm{kq}}^{(t)}\\|$ grows linearly,the dynamics of the attention layer can be shown to converge in direction to the max-margin solution in Equation (3). ", "page_idx": 7}, {"type": "text", "text": "6  Generalization Ability ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we prove the generalization ability of the trained transformers. Recall that Theorem 1 showsthatW(t) convergesto $W_{\\mathrm{kq}}^{*}\\Vert W_{\\mathrm{kq}}^{(t)}\\Vert/\\Vert W_{\\mathrm{kq}}^{*}\\Vert$ . To characterize the generaliaion aility it is desirable to use the property of $W_{\\mathrm{kq}}^{*}$ , Which is given in the following result. ", "page_idx": 7}, {"type": "text", "text": "Proposition 2 Under Assumptions 1-2, fix a query token $x^{q}$ ,let $\\mathcal{O}_{x^{q}},\\mathcal{N}_{x^{q}}$ \uff0c $\\mathcal{M}_{x^{q}}\\subset\\mathcal{V}$ bethesetof optimal tokens,the set of non-optimal tokens,and the set of non-comparable tokens,under $x^{q}$ -partial order, respectively.Then, the solution $W_{\\mathrm{kq}}^{*}$ ofEquation3) satisfies $x_{0}^{\\top}W_{\\mathrm{kq}}^{*}x^{q}=0$ for $x_{0}\\in\\mathcal{M}_{x^{q}}$ \uff0c and ", "page_idx": 7}, {"type": "equation", "text": "$$\nx_{*}^{\\top}W_{\\mathrm{kq}}^{*}x^{q}=\\frac{|\\mathcal{N}_{x^{q}}|}{|\\mathcal{O}_{x^{q}}|+|\\mathcal{N}_{x^{q}}|},\\quad x^{\\top}W_{\\mathrm{kq}}^{*}x^{q}=-\\frac{|\\mathcal{O}_{x^{q}}|}{|\\mathcal{O}_{x^{q}}|+|\\mathcal{N}_{x^{q}}|},\\quad\\forall x_{*}\\in\\mathcal{O}_{x^{q}},x\\in\\mathcal{N}_{x^{q}}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Recall that non-comparable tokens (see Section 4) under a query $x^{q}$ never appears in any training sentence data with the same query $x^{q}$ . Thus, Proposition 2 implies an interesting generalization capability- each $x^{q}$ -partial order can automatically incorporate more relationships to expand the query-dependent partial orders. Combining Proposition 2 with Theorem 1, we obtain the following theorem on W(t) . ", "page_idx": 7}, {"type": "text", "text": "Theorem 3 Under the conditions and notations in Proposition 2, let $T=\\Omega(\\log(1/\\epsilon))$ ,and $t=$ $\\Omega(\\log(1/\\epsilon))$ .Then there exists a constant $C_{0}$ suchthat ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(x_{*}-x_{0})^{\\top}W_{\\mathrm{kq}}^{(t)}x^{q}\\geq C_{0}t,\\quad(x_{0}-x)^{\\top}W_{\\mathrm{kq}}^{(t)}x^{q}\\geq C_{0}t,\\quad\\forall x_{*}\\in\\mathcal{O}_{x^{q}},x_{0}\\in\\mathcal{M}_{x^{q}},x\\in\\mathcal{N}_{x^{q}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Moreover, if the trained transformer takes input $X$ withquery $x^{q}$ that consists of a non-comparable token $x_{0}$ and non-optimal tokens, then the prediction made by $\\mathrm{T}_{\\theta^{\\left(t\\right)}}(X)$ is $\\mathrm{n}(x_{0})$ with high probability. ", "page_idx": 7}, {"type": "text", "text": "Theorem 3 suggests that a new partial order is created by the trained transformer. Specifically, it inserts the non-comparable tokens between the optimal and non-optimal tokens. The trained transformer can generalize the token prediction to such new sentences as given in Theorem 3. ", "page_idx": 8}, {"type": "text", "text": "We use Example 1 to illustrate the generalization ability described above. ", "page_idx": 8}, {"type": "text", "text": "Example 2 (Generalization to unseen data in Example 1) Recall that in Example 1, the training dataset consists of four sentences: SVOP, VOP, OPP, and PSV. Consider the partial order ${>}p$ under the punctuation mark $P.$ We have that $O{>}P^{P}$ and $o$ is an optimal token, $P$ isanon-optimaltoken, and S,V are non-comparable tokens. We then have the following non-trivial prediction by the trained transformer. ", "page_idx": 8}, {"type": "text", "text": "Case 1. Non-comparable tokens are learned to be \u201clarger\u201d than non-optimal tokens. ", "page_idx": 8}, {"type": "text", "text": "Consider a new (unseen) input sentence SP. Since S is non-comparable before training, but is \u201clarger\" than $P$ under the trained key-query matrx $W_{\\mathrm{kq}}^{(t)}$ the next predicted token is $\\operatorname{n}(S)=V.$ ", "page_idx": 8}, {"type": "text", "text": "Case 2. Optimal tokens remain optimal over all tokens after training. ", "page_idx": 8}, {"type": "text", "text": "Consider a new (unseen) input OsP. $o$ is optimal and Sis still\u201csmaller\"than $o$ underthetrained $P$ -partial order. The trained transformer will consistently predict $P.$ ", "page_idx": 8}, {"type": "text", "text": "In both of the above cases, the trained transformer provides desirable prediction for the unseen sentences. We further note that the effectiveness of both cases can vary during the inference time of the trained transformer. For instance, if the input sequence is SP (subject-punctuation), the output is SPV (subject-P-verb), which follows a logical subject-verb order and is desirable. However, in cases where the input is VP (verb-punctuation), it may be preferable to terminate the sequence after the verb, i.e., VPP, as the verb alone can suffice to convey the intended meaning. ", "page_idx": 8}, {"type": "text", "text": "7 Experiment ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we verify our theoretical findings via an experiment on a synthetic dataset. Specifically, we randomly generate a realizable dataset as described in Assumption 1 with $|\\nu|=20$ . Then, we train $W_{\\mathrm{ov}}$ and $W_{\\mathrm{kq}}$ by Algorithm 1, each with 900 iterations. The parameters are chosen as $d=|\\nu|$ $\\eta_{0}=0.2/\\sqrt{d}$ and $\\dot{\\eta}=0.05/\\sqrt{d}$ In Figure 2, the first three plots show the dynamics of the training stage 1, which indicates the convergence of the loss $\\mathcal{L}_{0}(W_{\\mathrm{ov}}^{(t)})$ to its minimum value, the convergence f $W_{\\mathrm{ov}}^{(t)}$ in direction to $W_{\\mathrm{ov}}^{*}$ , and the linear increase of the norm $\\|W_{\\mathrm{ov}}^{(t)}\\|$ , respectively. These results verify Proposition 1. The last three plots show the dynamics of the training stage 2, which indicates the convergence of the loss $\\textstyle{\\mathcal{L}}(\\theta^{(t)})$ , the convergence of $W_{\\mathrm{kq}}^{(t)}$ in direction to $W_{\\mathrm{kq}}^{*}$ and th linear increase of the norm $\\|W_{\\mathrm{kq}}^{(t)}\\|$ These results verify Theorem and Theorem 2. All experiments are conducted on a PC equipped with an i5-12400F processor and 16GB of memory. ", "page_idx": 8}, {"type": "image", "img_path": "NfOFbPpYII/tmp/de0d602fbd6db07874f04dea00fb44991c583e5ef9206be353f691251d480c60.jpg", "img_caption": ["Figure 2: Training dynamics of single-layer transformer for NTP. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "8 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this work, we investigated the training dynamics of a single-layer transformer for NTP. We first characterized two structural properties of the training dataset under the realizable setting where the training loss can be made arbitrarily close to zero. These properties allow us to define two max-margin solutions for both the feed-forward layer and the self-attention layer. Then, we showed that both layers converge in direction to their corresponding max-margin solutions sub-linearly, which further yields a linear convergence of the training loss for NTP. We further showed that the well trained transformer can have non-trivial prediction ability on unseen data, which sheds light on the generalization capability of transformers. Our experiments verify our theoretical findings. ", "page_idx": 8}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The work of R. Huang and J. Yang was supported in part by the U.s. National Science Foundation under grants NSF CNS-1956276 and ECCS-2133170. The work of Y. Liang was supported in part by the U.S. National Science Foundation under grants ECCS- 2413528 and DMS-2134145. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Ahn, K., Cheng, X., Daneshmand, H., and Sra, S. (2024). Transformers learn to implement preconditioned gradient descent for in-context learning. Advances in Neural Information Processing Systems, 36.   \nAky\u00fcrek, E., Schuurmans, D., Andreas, J., Ma, T, and Zhou, D. (2022). What learning algorithm is in-context learning? investigations with linear models. arXiv preprint arXiv:2211.15661.   \nBai, Y, Chen, F, Wang, H., Xiong, C., and Mei, S. (2023). Transformers as statisticians: Provable in-context learning with in-context algorithm selection. arXiv preprint arXiv:2306.04637.   \nBelkin, M. (2024). The necessity of machine learning theory in mitigating ai risk. ACM/JMS Journal of Data Science.   \nBommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M. S., Bohg, J., Bosselut, A., Brunskill, E., et al. (2021). On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258.   \nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P, Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. (2020). Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901.   \nChen, S. and Li Y. (2024). Provably learning a multi-head attention layer. arXiv preprint arXiv:2402.04084.   \nChen, S., Sheen, H., Wang, T., and Yang, Z. (2024). Training dynamics of multi-head softmax attention for in-context learning: Emergence, convergence, and optimality. arXiv preprint arXiv:2402.19442.   \nCheng, X., Chen, Y., and Sra, S. (2023). Transformers implement functional gradient descent to learn non-linear functions in context. arXiv preprint arXiv:2312.06528.   \nCui, Y., Ren, J., He, P., Tang, J., and Xing, Y. (2024). Superiority of multi-head attention in in-context linear regression. arXiv preprint arXiv:2401.17426.   \nDeora, P., Ghaderi, R., Taheri, H., and Thrampoulidis, C. (2023). On the optimization and generalization of multi-head attention. arXiv preprint arXiv:2310.12680.   \nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv: 1810.04805.   \nDosovitskiy, A., Beyer, L., Kolesnikoy, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. (2020). An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.   \nDryer, M. S. (1991). Svo languages and the ov: Vo typology1. Journal of linguistics, 27(2):443-482.   \nEdelman, B. L., Goel, S., Kakade, S., and Zhang, C. (2022). Inductive biases and variable creation in self-attention mechanisms. In International Conference on Machine Learning, pages 5793-5831. PMLR.   \nFrei, S., Vardi, G., Bartlett, P. L., Srebro, N., and Hu, W. (2022). Implicit bias in leaky relu networks trained on high-dimensional data. arXiv preprint arXiv:2210.07082.   \nFu, D., Chen, T.-Q., Jia, R., and Sharan, V. (2023). Transformers learn higher-order optimization methods for in-context learning: A study with linear models. arXiv preprint arXiv:2310.17086.   \nGiannou, A., Rajput, S., Sohn, J.-y, Lee, K., Lee, J. D., and Papailiopoulos, D. (2023). Looped transformers as programmable computers. In International Conference on Machine Learning, pages 11398-11442. PMLR.   \nHuang, Y, Cheng, Y, and Liang, Y. (2023). In-context convergence of transformers. arXiv preprint arXiv:2310.05249.   \nJi, Z., Srebro, N., and Telgarsky, M. (2021). Fast margin maximization via dual acceleration. In International Conference on Machine Learning, pages 4860-4869. PMLR.   \nJi, Z. and Telgarsky, M. (2021). Characterizing the implicit bias via a primal-dual analysis. In Algorithmic Learning Theory, pages 772-804. PMLR.   \nKim, J. and Suzuki, T. (2024). Transformers learn nonlinear features in context: Nonconvex meanfield dynamics on the attention landscape. arXiv preprint arXiv:2402.01258.   \nKou, Y, Chen, Z., and Gu, Q. (2024). Implicit bias of gradient descent for two-layer relu and leaky relu networks on nearly-orthogonal data. Advances in Neural Information Processing Systems, 36.   \nLehecka, T. (2015). Collocation and colligation. In Handbook of pragmatics online. Benjamins.   \nLi, H., Wang, M., Liu, S., and Chen, P-Y. (2023). A theoretical understanding of shallow vision transformers: Learning, generalization, and sample complexity. arXiv preprint arXiv:2302.06015.   \nLi, Y., Huang, Y., Idiz, M. E., Rawat, A. S., and Oymak, S. (2024). Mechanics of next token prediction with self-attention. In International Conference on Artifcial Inteligence and Statistics, pages 685-693. PMLR.   \nLin, L., Bai, Y., and Mei, S. (2023). Transformers as decision makers: Provable in-context reinforcement learning via supervised pretraining. arXiv preprint arXiv:2310.08566.   \nMahankali, A., Hashimoto, T. B., and Ma, T. (2023). One step of gradient descent is provably the optimal in-context learner with one layer of linear self-attention. arXiv preprint arXiv:2307.03576.   \nNacson, M. S., Lee, J., Gunasekar, S., Savarese, P. H. P, Srebro, N., and Soudry, D. (2019). Convergence of gradient descent on separable data. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 3420-3428. PMLR.   \nNichani, E., Damian, A., and Lee, J. D. (2024). How transformers learn causal structure with gradient descent. arXiv preprint arXiv:2402.14735.   \nPhuong, M. and Lampert, C. H. (2020). The inductive bias of relu networks on orthogonally separable data. In International Conference on Learning Representations.   \nSoudry, D., Hoffer, E., Nacson, M. S., Gunasekar, S., and Srebro, N. (2018). The implicit bias of gradient descent on separable data. Journal of Machine Learning Research, 19(70):i-57.   \nTarzanagh, D. A., Li, Y., Thrampoulidis, C.,and Oymak, S. (2023a). Transformers as support vector machines. arXiv preprint arXiv:2308.16898.   \nTarzanagh, D. A., Li, Y., Zhang, X., and Oymak, S. (2023b). Max-margin token selection in attention mechanism. In Thirty-seventh Conference on Neural Information Processing Systems.   \nThrampoulidis, C. (2024). Implicit bias of next-token prediction.   \nTian, Y., Wang, Y, Chen, B., and Du, S. S. (2023a). Scan and snap: Understanding training dynamis and token composition in 1-layer transformer. Advances in Neural Information Processing Systems, 36:71911-71947.   \nTian, Y., Wang, Y., Zhang, Z., Chen, B., and Du, S. (2023b). Joma: Demystifying multilayer transformers via joint dynamics of mlp and attention. arXiv preprint arXiv:2310.00535.   \nTsai, Y.-H. H., Bai, S., Liang, P. P., Kolter, J. Z., Morency, L.-P., and Salakhutdinov, R. (2019). Multimodal transformer for unaligned multimodal language sequences. In Proceedings of the conference. Association for computational linguistics. Meeting, volume 2019, page 6558. NIH Public Access.   \nVardi, G. (2023). On the implicit bias in deep-learning algorithms. Communications of the ACM, 66(6):86-93.   \nVasudeva, B., Deora, P., and Thrampoulidis, C. (2024). Implicit bias and fast convergence rates for self-attention. arXiv preprint arXiv:2402.05738.   \nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems,30.   \nVon Oswald, J., Niklasson, E., Randazzo, E., Sacramento, J., Mordvintsev, A., Zhmoginov, A., and Vladymyrov, M. (2023). Transformers learn in-context by gradient descent. In International Conference on Machine Learning, pages 35151-35174. PMLR.   \nYannakakis, M. (1982). The complexity of the partial order dimension problem. SIAM Journal on Algebraic Discrete Methods, 3(3):351-358.   \nZhang, R., Frei, S., and Bartlett, P. L. (2023). Trained transformers learn linear models in-context. arXiv preprint arXiv:2306.09927. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "1 Introduction ", "page_idx": 12}, {"type": "text", "text": "2 Related Work 2 ", "page_idx": 12}, {"type": "text", "text": "3 Problem Setup 3 ", "page_idx": 12}, {"type": "text", "text": "4  Realizable Training Dataset and Two-Stage Algorithm 4 ", "page_idx": 12}, {"type": "text", "text": "4.1  Realizable Training Dataset . 4   \n4.2 Training Algorithm 5 ", "page_idx": 12}, {"type": "text", "text": "5   Training Dynamics of the Transformer 6 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "5.1 Convergence of Training $W_{\\mathrm{ov}}$ 6   \n5.2 Convergence of Training $W_{\\mathrm{kq}}$ 7   \n5.3Proof Sketch of Theorem 1 8 ", "page_idx": 12}, {"type": "text", "text": "6  Generalization Ability 8 ", "page_idx": 12}, {"type": "text", "text": "7  Experiment 9 ", "page_idx": 12}, {"type": "text", "text": "8 Conclusion 9 ", "page_idx": 12}, {"type": "text", "text": "A   Expression of Gradients 14 ", "page_idx": 12}, {"type": "text", "text": "B Proof of Proposition 1 14 ", "page_idx": 12}, {"type": "text", "text": "C Proof of Theorem 1 and Theorem 2 17 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "C.1  Supporting Lemmas . 17   \nC.2 Step 1 19   \nC.3 Step 2 21   \nC.4 Step 3 23   \nC.5 Proof of Theorem 1 28   \nC.6 Proof of Theorem 2 30 ", "page_idx": 12}, {"type": "text", "text": "D  Proof of Proposition 2 and Theorem 3 31 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "D.1 Proof of Proposition 2 . . 31   \nD.2 Proof of Theorem 3 . 33 ", "page_idx": 12}, {"type": "text", "text": "A  Expression of Gradients ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We first provide the general formula for the gradients of both layers. ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{W_{\\infty}}\\mathcal{L}_{0}(W_{\\infty})=\\displaystyle\\sum_{x\\in\\mathcal{V}}\\left(\\mathrm{T}_{0}(x)-e_{\\mathrm{In}(x)}\\right)x^{\\top},}\\\\ &{\\nabla_{W_{\\mathrm{kq}}}\\mathcal{L}(\\theta)\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad}\\\\ &{\\quad=\\displaystyle\\sum_{n}\\pi^{(n)}X^{(n)}\\left(\\mathrm{diag}(\\phi_{\\theta}(X^{(n)})-\\phi_{\\theta}(X^{(n)})\\phi_{\\theta}(X^{(n)})^{\\top}\\right)(X^{(n)})^{\\top}W_{\\mathrm{ov}}^{\\top}\\left(\\mathrm{T}_{\\theta}(X^{(n)})-p^{(n)}\\right)(X^{(n)})^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "B  Proof of Proposition 1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Recall that we use the loss, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathcal{L}_{0}(W_{\\mathrm{ov}})=-\\sum_{x\\in\\mathcal{V}}\\log\\frac{\\exp\\left(e_{\\mathrm{In}(x)}^{\\top}W_{\\mathrm{ov}}x\\right)}{\\sum_{i\\in[|\\mathcal{V}|]}\\exp\\left(e_{i}^{\\top}W_{\\mathrm{ov}}x\\right)}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The updating rule of $W_{\\mathrm{ov}}$ is that ", "page_idx": 13}, {"type": "equation", "text": "$$\nW_{\\mathrm{ov}}^{(t+1)}=W_{\\mathrm{ov}}^{(t)}-\\eta_{0}\\frac{\\nabla_{W_{\\mathrm{ov}}}\\mathcal{L}_{0}(W_{\\mathrm{ov}}^{(t)})}{\\lVert\\nabla_{W_{\\mathrm{ov}}}\\mathcal{L}_{0}(W_{\\mathrm{ov}}^{(t)})\\rVert}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We know that ${\\mathcal{L}}_{0}$ is convex respect to $W_{\\mathrm{ov}}$ . Therefore, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\left\\langle W_{\\mathrm{ov}}^{(t)}-W_{\\mathrm{ov}}^{\\prime},\\nabla_{W_{\\mathrm{ov}}}\\mathcal{L}_{0}(\\theta^{(t)})\\right\\rangle\\geq\\mathcal{L}_{0}(\\theta^{(t)})-\\mathcal{L}_{0}(\\theta^{\\prime}).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "It is clear that the loss function $\\mathcal{L}$ reaches the minimum 0 when $W_{\\mathrm{ov}}=\\Delta W_{\\mathrm{ov}}^{*}$ , as $\\Delta\\rightarrow\\infty$ ", "page_idx": 13}, {"type": "text", "text": "Lemma 1 Under the initialization $W_{\\mathrm{ov}}^{(0)}$ and the uatng ule qution Twitpsi $\\eta,$ the following inequality holds. ", "page_idx": 13}, {"type": "equation", "text": "$$\nt\\eta_{0}+\\|W_{\\mathrm{ov}}^{(0)}\\|\\geq\\|W_{\\mathrm{ov}}^{(t)}\\|\\geq\\frac{t\\eta_{0}}{2\\|W_{\\mathrm{ov}}^{*}\\|}-\\|W_{\\mathrm{ov}}^{(0)}\\|.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof. Using Equation (5), we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\langle W_{\\mathrm{ov}}^{*},\\nabla_{W_{\\mathrm{ov}}}\\mathcal{L}_{0}(W_{\\mathrm{ov}}^{(t)})\\right\\rangle=\\displaystyle\\sum_{x\\in\\mathcal{V}}\\left(\\mathrm{T}_{0}^{(t)}(x)-e_{\\mathrm{In}(x)}\\right)^{\\top}W_{\\mathrm{ov}}^{*}x}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\sum_{x\\in\\mathcal{V}}\\sum_{i\\in[|\\mathcal{V}|]}[\\mathrm{T}_{0}^{(t)}(x)]_{i}(e_{i}-e_{\\mathrm{In}(x)})^{\\top}W_{\\mathrm{ov}}^{*}x}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\overset{(a)}{\\leq}-\\displaystyle\\sum_{x\\in\\mathcal{V}}\\sum_{i\\neq\\mathrm{In}(x)}[\\mathrm{T}_{0}^{(t)}(x)]_{i},}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $(a)$ is due the constraints that $W_{\\mathrm{ov}}^{*}$ satisfies. On the other hand, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\|\\nabla_{W_{\\mathrm{ov}}}\\mathcal{L}(W_{\\mathrm{ov}}^{(t)})\\|=\\left\\langle\\frac{\\nabla_{W_{\\mathrm{ov}}}\\mathcal{L}(W_{\\mathrm{ov}}^{(t)})}{\\|\\nabla_{W_{\\mathrm{ov}}}\\mathcal{L}(W_{\\mathrm{ov}}^{(t)})\\|},\\nabla_{W_{\\mathrm{ov}}}\\mathcal{L}_{0}(W_{\\mathrm{ov}}^{(t)})\\right\\rangle\n$$", "text_format": "latex", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\displaystyle\\sum_{\\boldsymbol{x}\\in\\mathcal{V}}\\displaystyle\\sum_{i}[\\mathrm{T}_{0}^{(t)}(\\boldsymbol{x})]_{i}\\big(\\boldsymbol{e}_{i}-\\boldsymbol{e}_{\\mathrm{In}(\\boldsymbol{x})}\\big)^{\\top}\\displaystyle\\frac{\\nabla_{\\boldsymbol{W}_{\\mathrm{ov}}}\\mathcal{L}(\\boldsymbol{\\theta}^{(t)})}{\\|\\nabla_{\\boldsymbol{W}_{\\mathrm{ov}}}\\mathcal{L}(\\boldsymbol{\\theta}^{(t)})\\|}x}\\\\ &{\\overset{(a)}{\\le}2\\displaystyle\\sum_{\\boldsymbol{x}\\in\\mathcal{V}}\\displaystyle\\sum_{i\\neq\\mathrm{In}(\\boldsymbol{x})}[\\mathrm{T}_{0}^{(t)}(\\boldsymbol{x})]_{i},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $(a)$ follows from $\\|A B\\|\\leq\\|A\\|\\|B\\|$ for any matrices $A$ and $B$ . Thus, we obtain ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left\\langle W_{\\mathrm{ov}}^{*},\\frac{\\nabla_{W_{\\mathrm{ov}}}\\mathcal{L}(\\boldsymbol{\\theta}^{(t)})}{\\|\\nabla_{W_{\\mathrm{ov}}}\\mathcal{L}(\\boldsymbol{\\theta}^{(t)})\\|}\\right\\rangle\\leq-1/2\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "To lower bound the norm of $W_{\\mathrm{ov}}$ , we recall the updating rule (Equation (7). ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|W_{\\mathrm{ov}}^{(t)}\\|=\\bigg\\|W_{\\mathrm{ov}}^{(0)}-\\displaystyle\\sum_{t^{\\prime}<t}\\eta_{0}\\frac{\\nabla_{W_{\\mathrm{ov}}}\\mathcal{L}_{0}(W_{\\mathrm{ov}}^{(t^{\\prime})})}{\\|\\nabla_{W_{\\mathrm{ov}}}\\mathcal{L}_{0}(W_{\\mathrm{ov}}^{(t^{\\prime})})\\|}\\bigg\\|}\\\\ &{\\qquad\\ge\\bigg\\langle W_{\\mathrm{ov}}^{(0)}-\\displaystyle\\sum_{t^{\\prime}<t}\\eta_{0}\\frac{\\nabla_{W_{\\mathrm{ov}}}\\mathcal{L}_{0}(W_{\\mathrm{ov}}^{(t^{\\prime})})}{\\|\\nabla_{W_{\\mathrm{ov}}}\\mathcal{L}_{0}(W_{\\mathrm{ov}}^{(t^{\\prime})})\\|},\\frac{W_{\\mathrm{ov}}}{\\|W_{\\mathrm{ov}}^{*}\\|}\\bigg\\rangle}\\\\ &{\\qquad\\ge\\frac{t\\eta_{0}}{2\\|W_{\\mathrm{ov}}^{*}\\|}-\\|W_{\\mathrm{ov}}^{(0)}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "For the LHS of the inequality, it suffices to note that at each iteration, the norm $\\|W_{\\mathrm{ov}}^{(t)}\\|$ increases most $\\eta_{0}$ due to normalized gradient descent. ", "page_idx": 14}, {"type": "text", "text": "Lemma 2 At each iteration $t$ the following inequality holds ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left\\langle\\frac{W_{\\mathrm{ov}}^{(t)}}{\\|W_{\\mathrm{ov}}^{(t)}\\|},\\nabla_{W_{\\mathrm{ov}}}\\mathcal{L}_{0}(W_{\\mathrm{ov}}^{(t)})\\right\\rangle\\geq\\left(1+\\frac{2\\|W_{\\mathrm{ov}}^{*}\\|}{\\|W_{\\mathrm{ov}}^{(t)}\\|}\\log(2|\\mathcal{V}|)\\right)\\left\\langle\\frac{W_{\\mathrm{ov}}^{*}}{\\|W_{\\mathrm{ov}}^{*}\\|},\\nabla_{W_{\\mathrm{ov}}}\\mathcal{L}_{0}(W_{\\mathrm{ov}}^{(t)})\\right\\rangle\\,,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "$W_{\\mathrm{ov}}^{(t)}=W_{\\mathrm{ov}}^{*}\\frac{||W_{\\mathrm{ov}}^{(t)}||}{||W_{\\mathrm{ov}}^{*}||}$ have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left<\\frac{W_{\\mathrm{ov}}^{*}}{\\|W_{\\mathrm{ov}}^{*}\\|},\\nabla_{W_{\\mathrm{ov}}}\\mathcal{L}_{0}(W_{\\mathrm{ov}}^{(t)})\\right><0\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "In this case, the result is trivial. ", "page_idx": 14}, {"type": "text", "text": "$W_{\\mathrm{ov}}^{(t)}\\neq W_{\\mathrm{ov}}^{*}\\frac{||W_{\\mathrm{ov}}^{(t)}||}{||W_{\\mathrm{ov}}^{*}||}$ $W_{\\mathrm{ov}}^{*}$ the minimum norm satisfying the constraints in Equation (2), we must have that for some $x_{0}\\in\\mathcal{V}$ there exists $i_{0}\\neq\\mathrm{In}(x_{0})$ such that the following inequality holds ", "page_idx": 14}, {"type": "equation", "text": "$$\n(e_{\\mathrm{In}(x_{0})}-e_{i})^{\\top}W_{\\mathrm{ov}}^{(t)}x_{0}<\\frac{\\|W_{\\mathrm{ov}}^{(t)}\\|}{\\|W_{\\mathrm{ov}}^{*}\\|}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Therefore, the loss on $W_{\\mathrm{ov}}^{(t)}$ can be lower bounded as follows. ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}(W_{\\mathrm{ov}}^{(t)})=\\displaystyle\\sum_{x\\in\\mathcal{V}}\\log\\left(1+\\displaystyle\\sum_{i}\\exp\\left((e_{i}-e_{\\mathrm{In}(x)})^{\\top}W_{\\mathrm{ov}}^{(t)}x\\right)\\right)}\\\\ &{\\qquad\\quad>\\log\\left(1+\\exp\\left(-\\|W_{\\mathrm{ov}}^{(t)}\\|/\\|W_{\\mathrm{ov}}^{*}\\|\\right)\\right)}\\\\ &{\\qquad\\quad\\overset{(a)}{>}\\frac{1}{2}\\exp\\left(-\\|W_{\\mathrm{ov}}^{(t)}\\|/\\|W_{\\mathrm{ov}}^{*}\\|\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $(a)$ is due to the fact that $\\log(1+x)\\,\\geq\\,x/2$ when $0\\,<\\,x\\,<\\,1$ . On the other hand, let $\\begin{array}{r}{W_{\\mathrm{ov}}^{\\prime}=\\overset{}{\\left(\\frac{\\|W_{\\mathrm{ov}}^{(t)}\\|}{\\|W_{\\mathrm{ov}}^{*}\\|}+2\\log(2|\\mathcal{V}|)\\right)}W_{\\mathrm{ov}}^{*}}\\end{array}$ . Then, the loss on $W_{\\mathrm{ov}}^{\\prime}$ has the following upper bound. ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}(W_{\\mathrm{ov}}^{\\prime})=\\displaystyle\\sum_{x\\in\\mathcal{V}}\\log\\left(1+\\sum_{i}\\exp\\left((e_{i}-e_{\\mathrm{In}(x)})^{\\top}W_{\\mathrm{ov}}^{(t)}x\\right)\\right)}\\\\ &{\\quad\\quad\\quad\\leq\\displaystyle\\sum_{x\\in\\mathcal{V}}\\log\\left(1+(|\\mathcal{V}|-1)\\exp\\left(-\\|W_{\\mathrm{ov}}^{(t)}\\|/\\|W_{\\mathrm{ov}}^{*}\\|-\\log(2|\\mathcal{V}|)\\right)\\right)}\\\\ &{\\quad\\quad\\quad\\overset{(a)}{\\leq}\\displaystyle\\sum_{x\\in\\mathcal{V}}|\\mathcal{V}|\\exp\\left(-\\|W_{\\mathrm{ov}}^{(t)}\\|/\\|W_{\\mathrm{ov}}^{*}\\|-2\\log(2|\\mathcal{V}|)\\right)}\\\\ &{\\quad\\quad\\quad\\leq\\displaystyle\\frac{1}{2}\\exp(-\\|W_{\\mathrm{ov}}^{(t)}\\|/\\|W_{\\mathrm{ov}}^{*}\\|),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $(a)$ is due to the fact that $\\log(1+x)<x$ when $x>0$ . Thus, $\\mathcal{L}(W_{\\mathrm{ov}}^{(t)})>\\mathcal{L}(W_{\\mathrm{ov}}^{\\prime})$ . Due to the convextiy of ${\\mathcal{L}}_{0}$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{0<\\left\\langle W_{\\mathrm{ov}}^{(t)}-W_{\\mathrm{ov}}^{\\prime},\\nabla_{W_{\\mathrm{ov}}}\\mathcal{L}_{0}(W_{\\mathrm{ov}}^{(t)})\\right\\rangle}\\\\ &{\\quad=\\left\\langle W_{\\mathrm{ov}}^{(t)},\\nabla_{W_{\\mathrm{ov}}}\\mathcal{L}_{0}(W_{\\mathrm{ov}}^{(t)})\\right\\rangle-\\left(\\frac{\\left\\|W_{\\mathrm{ov}}^{(t)}\\right\\|}{\\left\\|W_{\\mathrm{ov}}^{*}\\right\\|}+2\\log(2|\\mathcal{V}|)\\right)\\left\\langle W_{\\mathrm{ov}}^{*},\\nabla_{W_{\\mathrm{ov}}}\\mathcal{L}_{0}(W_{\\mathrm{ov}}^{(t)})\\right\\rangle,}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which finishes the proof. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Prpsitse $W_{\\mathrm{ov}}^{(0)}=0$ and updating rule Equation (7), for any $t\\geq2$ the following inequality holds. ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left\\langle\\frac{W_{\\mathrm{ov}}^{(t)}}{\\|W_{\\mathrm{ov}}^{(t)}\\|},\\frac{W_{\\mathrm{ov}}^{*}}{\\|W_{\\mathrm{ov}}^{*}\\|}\\right\\rangle\\geq1-\\frac{12\\|W_{\\mathrm{ov}}^{*}\\|^{3}\\log(2|\\mathcal{V}|)\\log t}{t\\eta_{0}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Moreover 2W\\* $\\begin{array}{r}{\\frac{t\\eta_{0}}{2\\|W_{\\mathrm{ov}}^{*}\\|}\\leq\\|W_{\\mathrm{ov}}^{(t)}\\|\\leq t\\eta_{0}}\\end{array}$ ", "page_idx": 15}, {"type": "text", "text": "Proof. The second argument about the norm of $W_{\\mathrm{ov}}^{(t)}$ follows directlyfrom Lemma 1. We aim to prove the first part as follows. ", "page_idx": 15}, {"type": "text", "text": "Let 2W log(2)B Lmma 2 andthe updatng rule Equation (7) wehav ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\langle W_{\\infty}^{(t+1)}-W_{\\infty}^{(t)},\\frac{W_{\\infty}^{(t)}}{\\|W_{\\infty}^{(t)}\\|}\\right\\rangle=-\\eta_{\\infty}\\left\\langle\\nabla_{W_{\\infty}}\\mathcal{L}(W_{\\infty}^{(t)}),\\frac{W_{\\infty}^{(t)}}{\\|W_{\\infty}^{(t)}\\|}\\right\\rangle}&{}\\\\ {\\qquad\\geq-\\frac{\\eta_{\\infty}}{1+\\alpha_{t}}\\left\\langle\\nabla_{W_{\\infty}}\\mathcal{L}(W_{\\infty}^{(t)}),\\frac{W_{\\infty}^{(t)}}{\\|W_{\\infty}^{(t)}\\|}\\right\\rangle}&{}\\\\ {\\qquad=\\frac{1}{1+\\alpha_{t}}\\left\\langle W_{\\infty}^{(t+1)}-W_{\\infty}^{(t)},\\frac{W_{\\infty}^{(t)}}{\\|W_{\\infty}^{(t)}\\|}\\right\\rangle}&{}\\\\ {\\qquad=\\left(1-\\frac{\\alpha_{t}}{1+\\alpha_{t}}\\right)\\left\\langle W_{\\infty}^{(t+1)}-W_{\\infty}^{(t)},\\frac{W_{\\infty}^{(t)}}{\\|W_{\\infty}^{(t)}\\|}\\right\\rangle}&{}\\\\ {\\qquad=\\frac{1}{2\\|W_{\\infty}^{(t)}\\|}\\left(\\|W_{\\infty}^{(t+1)}\\|^{2}-\\|W_{\\infty}^{(t+1)}-W_{\\infty}^{(t)}\\|^{2}-\\|W_{\\infty}^{(t)}\\|^{2}\\right)}&{}\\\\ {\\qquad-\\frac{\\alpha_{t}}{1+\\alpha_{t}}\\left\\langle W_{\\infty}^{(t+1)}-W_{\\infty}^{(t)},\\frac{W_{\\infty}^{(t)}}{\\|W_{\\infty}^{(t)}\\|}\\right\\rangle}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overset{(a)}{=}\\frac{\\|W_{\\mathrm{ov}}^{(t+1)}\\|^{2}-\\|W_{\\mathrm{ov}}^{(t)}\\|^{2}}{2\\|W_{\\mathrm{ov}}^{(t)}\\|}-\\frac{\\eta^{2}}{2\\|W_{\\mathrm{ov}}^{(t)}\\|}}\\\\ &{\\phantom{\\leq}+\\frac{\\eta_{0}\\alpha_{t}}{1+\\alpha_{t}}\\left\\langle\\frac{\\nabla_{W_{\\mathrm{ov}}}\\mathcal{L}(\\theta^{(t)})}{\\|\\nabla_{W_{\\mathrm{ov}}}\\mathcal{L}(\\theta^{(t)})\\|},\\frac{W_{\\mathrm{ov}}^{(t)}}{\\|W_{\\mathrm{ov}}^{(t)}\\|}\\right\\rangle}\\\\ &{\\overset{(b)}{\\geq}\\|W_{\\mathrm{ov}}^{(t+1)}\\|-\\|W_{\\mathrm{ov}}^{(t)}\\|-\\frac{\\eta_{0}^{2}}{2\\|W_{\\mathrm{ov}}^{(t)}\\|}-\\frac{\\eta_{0}\\alpha_{t}}{1+\\alpha_{t}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Where $(a)$ follows from that $\\|W_{\\mathrm{ov}}^{(t+1)}-W_{\\mathrm{ov}}^{(t)}\\|=\\eta_{0}$ and $(b)$ is due to the fat that $x^{2}\\!-\\!y^{2}\\ge2y(x\\!-\\!y)$ for any $x,y\\in\\mathbb{R}$ ", "page_idx": 16}, {"type": "text", "text": "Summing over $t$ starting from 2, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left\\langle W_{\\mathrm{ov}}^{(t)}-W_{\\mathrm{ov}}^{(2)},\\frac{W_{\\mathrm{ov}}^{*}}{\\|W_{\\mathrm{ov}}^{*}\\|}\\right\\rangle\\geq\\|W_{\\mathrm{ov}}^{(t)}\\|-\\|W_{\\mathrm{ov}}^{(2)}\\|-\\sum_{t^{\\prime}=2}^{t-1}\\frac{\\eta_{0}^{2}}{2\\|W_{\\mathrm{ov}}^{(t^{\\prime})}\\|}-\\sum_{t^{\\prime}=2}^{t-1}\\frac{\\eta_{0}\\alpha_{t^{\\prime}}}{1+\\alpha_{t^{\\prime}}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Furthermore, due to Lemma 1, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{t^{\\prime}=2}^{t-1}\\frac{1}{\\|W_{\\mathrm{ov}}^{(t^{\\prime})}\\|}\\le\\sum_{t^{\\prime}=2}^{t-1}\\frac{2\\|W_{\\mathrm{ov}}^{*}\\|/\\eta_{0}}{t}}}\\\\ &{\\le\\frac{2\\|W_{\\mathrm{ov}}^{*}\\|}{\\eta_{0}}\\log t.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Similarly, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t^{\\prime}=2}^{t-1}\\frac{\\alpha_{t^{\\prime}}}{1+\\alpha_{t^{\\prime}}}\\leq\\displaystyle\\sum_{t^{\\prime}=2}^{t-1}\\frac{2\\|W_{\\mathrm{ov}}^{*}\\|\\log(2|\\mathcal{V}|)}{\\|W_{\\mathrm{ov}}^{(t^{\\prime})}\\|}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\frac{4\\|W_{\\mathrm{ov}}^{*}\\|^{2}\\log(2|\\mathcal{V}|)}{\\eta_{0}}\\log t}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\langle\\frac{W_{\\mathrm{ov}}^{(t)}}{\\|W_{\\mathrm{ov}}^{(t)}\\|},\\frac{W_{\\mathrm{ov}}^{*}}{\\|W_{\\mathrm{ov}}^{*}\\|}\\right\\rangle\\geq1-\\frac{\\|W_{\\mathrm{ov}}^{(2)}\\|+2\\eta_{0}\\|W_{\\mathrm{ov}}^{*}\\|\\log t+4\\|W_{\\mathrm{ov}}^{*}\\|^{2}\\log(2|\\mathcal{V}|)\\log t}{\\|W_{\\mathrm{ov}}^{(t)}\\|}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\overset{(a)}{\\geq}1-\\frac{12\\|W_{\\mathrm{ov}}^{*}\\|^{3}\\log(2|\\mathcal{V}|)\\log t}{t\\eta_{0}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Where $(a)$ follows from Lemma 1 and $\\lVert W_{\\mathrm{ov}}^{(2)}\\rVert\\leq2\\eta_{0}\\leq\\lVert W_{\\mathrm{ov}}^{*}\\rVert$ and $\\lVert W_{\\mathrm{ov}}^{(t)}\\rVert\\geq t\\eta_{0}/(2\\lVert W_{\\mathrm{ov}}^{*}\\rVert)$ ", "page_idx": 16}, {"type": "text", "text": "CProof of Theorem 1 and Theorem 2 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "C.1  Supporting Lemmas ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Lmmleq $t$ $W_{\\mathrm{ov}}^{(t)}$ satisfiesthat ", "page_idx": 16}, {"type": "equation", "text": "$$\n(e_{i}-e_{i^{\\prime}})^{\\top}W_{\\mathrm{ov}}^{(t)}x=0,\\quad\\forall i,i^{\\prime}\\neq\\operatorname{In}(x).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. The proof follows directly from induction and the fact that ", "page_idx": 16}, {"type": "equation", "text": "$$\n(e_{i}-e_{i^{\\prime}})^{\\top}W_{\\mathrm{ov}}^{(t+1)}x=(e_{i}-e_{i^{\\prime}})^{\\top}W_{\\mathrm{ov}}^{(t)}x-\\frac{\\eta_{0}([\\mathrm{T}_{0}^{(t)}]_{i}-[\\mathrm{T}_{0}^{(t)}]_{i^{\\prime}})}{\\|\\nabla_{W_{\\mathrm{ov}}}\\mathcal{L}_{0}(W_{\\mathrm{ov}}^{(t)})\\|},\\quad\\forall i,i^{\\prime}\\neq\\mathrm{In}(x).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Corollary1Under thesettings inProposition $^{\\,l}$ .let $T\\ \\geq\\ 384\\|W_{\\mathrm{ov}}^{*}\\|^{5}\\log(2|\\mathcal{V}|)\\log T/\\eta_{0}$ and $\\Delta=T\\eta_{0}\\dot{/}(4\\|W_{\\mathrm{ov}}^{*}\\|^{2})$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\big(e_{\\mathrm{In}(x)}-e_{i}\\big)^{\\top}W_{\\mathrm{ov}}x\\in(\\Delta,3\\Delta),\\forall i\\neq\\mathrm{In}(x)}\\\\ {\\big(e_{i}-e_{i^{\\prime}}\\big)^{\\top}W_{\\mathrm{ov}}x=0,\\forall i,i^{\\prime}\\neq\\mathrm{In}(x)}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. The second equality follows directly from Lemma 3. ", "page_idx": 17}, {"type": "text", "text": "To show the first equation, we analyze ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(e_{\\ln(x)}-e_{i})^{\\top}W_{\\infty}^{(T)}x}\\\\ &{\\qquad\\qquad=(e_{\\ln(x)}-e_{i})^{\\top}\\frac{W_{\\infty}^{*}\\|W_{\\infty}^{(T)}\\|}{\\|W_{\\infty}^{*}\\|}x+\\|W_{\\infty}^{(T)}\\|(e_{\\ln(x)}-e_{i})^{\\top}\\left(\\frac{W_{\\infty}^{(T)}}{\\|W_{\\infty}^{(T)}\\|}-\\frac{W_{\\infty}^{*}}{\\|W_{\\infty}^{*}\\|}\\right)W_{\\infty}^{(T)}x}\\\\ &{\\qquad\\qquad\\overset{(a)}{=}\\frac{\\|W_{\\infty}^{(T)}\\|}{\\|W_{\\infty}^{*}\\|}-2\\sqrt{2}\\|W_{\\infty}^{(T)}\\|\\sqrt{\\frac{12\\|W_{\\infty}^{*}\\|^{3}\\log(2|V|)\\log T}{T\\eta_{0}}}}\\\\ &{\\qquad\\overset{(b)}{\\geq}\\frac{T\\eta_{0}}{2\\|W_{\\infty}^{*}\\|^{2}}-\\sqrt{24T\\eta_{0}\\|W_{\\infty}^{*}\\|\\log(2|V|)\\log T}}\\\\ &{\\qquad\\geq\\frac{T\\eta_{0}}{4\\|W_{\\infty}^{*}\\|^{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $(a)$ follows from Proposition 1, and $(b)$ is due to Lemma 1. On the other hand, we also have ", "page_idx": 17}, {"type": "equation", "text": "$$\n(e_{\\ln(x)}-e_{i})^{\\top}W_{\\mathrm{ov}}^{(T)}x\\leq\\frac{\\|W_{\\mathrm{ov}}^{(T)}\\|}{\\|W_{\\mathrm{ov}}^{*}\\|}+2\\sqrt{2}\\|W_{\\mathrm{ov}}^{(T)}\\|\\sqrt{\\frac{12\\|W_{\\mathrm{ov}}^{*}\\|^{3}\\log(2|\\mathcal{V}|)\\log T}{T\\eta_{0}}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The proof is finished. t ", "page_idx": 17}, {"type": "text", "text": "Thus, fo simplity we frther asumethat $\\begin{array}{r}{(e_{\\mathrm{In}(x)}-e_{i})W_{\\mathrm{ov}}^{(T)}x=\\Delta}\\end{array}$ forall $x$ because $(e_{\\mathrm{In}(x)}-$ $e_{i})\\hat{W}_{\\mathrm{ov}}x=\\Theta(\\Delta)$ for large enough iteration. Next, we provide the general form of the projection of the gradient of Key-Query matrix $W_{\\mathrm{kq}}$ follows from a notation for the token weight. ", "page_idx": 17}, {"type": "text", "text": "The token weight (n (n,t) of the token x? in the sentence $X^{(n)}\\;=\\;[x_{1}^{(n)},...\\,,x_{L}^{(n)}]$ under $\\theta\\,=$ $(W_{\\mathrm{ov}}^{(T)},W_{\\mathrm{kq}})$ is calculated as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\varphi_{\\ell}^{(n,t)}=\\frac{\\exp\\left((x_{\\ell}^{(n)})^{\\top}W_{\\mathrm{kq}}X_{-1}^{(n)}\\right)}{\\sum_{\\ell^{\\prime}=1}^{L}\\exp\\left((x_{\\ell^{\\prime}}^{(n)})^{\\top}W_{\\mathrm{kq}}X_{-1}^{(n)}\\right)}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Lemma 4 (Projection of gradient of $W_{\\mathrm{kq.}}$ If $W_{\\mathrm{ov}}$ satisfies that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left\\{(e_{\\mathrm{In}(x)}-e_{i})^{\\top}W_{\\mathrm{ov}}x=\\Delta,\\forall i\\neq\\mathrm{In}(x)\\right.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\nabla_{W_{\\mathbf{k}_{\\mathbf{q}}}}\\mathcal{L}(\\theta),W_{\\mathbf{k}\\mathbf{q}}^{\\prime}\\rangle}\\\\ &{\\quad=\\Delta\\sum_{n}\\pi^{(n)}([\\mathbf{T}_{\\theta}^{(n)}]_{\\mathrm{In}(X^{(n)})}-1)\\sum_{\\ell_{*}\\in l(n)}\\varphi_{\\ell_{*}}^{(n,\\theta)}\\left(x_{\\ell_{*}}^{(n)}-\\sum_{\\ell^{\\prime}}\\varphi_{\\ell^{\\prime}}^{(n,\\theta)}x_{\\ell^{\\prime}}^{(n)}\\right)^{\\top}W_{\\mathbf{kq}}^{\\prime}X_{-1}^{(n)}}\\\\ &{\\quad\\bigg.+\\Delta\\sum_{n}\\pi^{(n)}\\sum_{\\ell\\notin l(n)}[\\mathbf{T}_{\\theta}^{(n)}]_{\\mathrm{In}(x_{\\ell}^{(n)})}\\varphi_{\\ell}^{(n,\\theta)}\\left(x_{\\ell}^{(n)}-\\sum_{\\ell^{\\prime}}\\varphi_{\\ell^{\\prime}}^{(n,\\theta)}x_{\\ell^{\\prime}}^{(n)}\\right)^{\\top}W_{\\mathbf{kq}}^{\\prime}X_{-1}^{(n)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof.  Recall that $l(n)$ is the set of indices of the optimal tokens in the sample $X^{(n)}$ . Thus, for any $\\ell_{*}\\in l(n),\\operatorname{In}(x_{\\ell_{*}}^{(n)})=\\operatorname{In}(X^{(n)})$ In addition, we denote $\\operatorname{T}_{\\boldsymbol{\\theta}}\\bigl(X^{(n)}\\bigr)$ by $\\mathrm{T}_{\\boldsymbol{\\theta}}^{(n)}$ for simplicity. From Equation (6), we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad[\\nabla_{\\theta}u_{t,\\theta}(\\theta),u_{\\theta}(\\theta)]}\\\\ &{=\\sum_{\\theta}\\left(v_{t}^{\\theta}\\sum_{w=0}^{\\infty}\\varphi_{t}^{(1)}(v_{t},w_{t})^{\\theta}\\right)^{\\theta}\\Big\\{\\Psi_{\\theta}u_{t}^{(1)}\\Big\\}^{\\theta}\\left(u_{t}^{(1)}-\\sum_{w=0}^{\\infty}\\varphi_{t}^{(1)}(v_{t},w_{t}^{(1)})\\right)^{\\theta}\\Psi_{\\theta}u_{t,\\theta}^{(1)}}\\\\ &{\\stackrel{(a)}{\\le}\\sum_{\\theta}\\left(v_{t}^{\\theta}\\sum_{w=0}^{\\infty}\\sum_{w=1}^{\\infty}\\varphi_{t}^{(1)}(v_{t},w_{t+1})w_{t+1}\\right)^{\\theta}\\Psi_{\\theta}u_{t}^{(1)}\\Big\\{\\Phi_{u}^{(1)}\\Big\\}^{\\theta}\\left(u_{t}^{(1)}-\\sum_{w=0}^{\\infty}\\varphi_{t}^{(1)}(v_{t},w_{t}^{(1)})\\right)^{\\theta}\\Psi_{\\theta}u_{t,\\theta}^{(1)}}\\\\ &{\\quad-\\sum_{\\theta}u_{t,\\theta}^{(1)}\\sum_{w=0}^{\\infty}\\sum_{w=1}^{\\infty}\\varphi_{t}^{(1)}(v_{t},w_{t+1})w_{t+1}\\Big\\{\\Phi_{u}^{(1)}\\Big\\}^{\\theta}u_{t}^{(1)}\\Big\\{\\Phi_{u}^{(1)}\\Big\\}^{\\theta}\\left(u_{t}^{(1)}-\\sum_{w=0}^{\\infty}\\varphi_{t}^{(1)}(v_{t},w_{t+1}^{(1)})\\right)^{\\theta}\\Psi_{\\theta}u_{t,\\theta}^{(1)}}\\\\ &{\\quad+\\sum_{\\theta}u_{t,\\theta}^{(1)}\\sum_{w=0}^{\\infty}\\sum_{w=1}^{\\infty}\\varphi_{t}^{(1)}(v_{t},w_{t+1})w_{t+1}\\Big\\{\\Phi_{u}^{(1)}\\Big\\}^{\\theta}u_{t}^{(1)}\\Big\\{\\Phi_{u}^{(1)}\\Big\\}^{\\theta}\\left(u_{t}^{(1)}-\\sum\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $(a)$ is due to the fact that $\\begin{array}{r}{\\sum_{i\\in[|\\mathcal{V}|]}[\\mathrm{T}_{\\theta}^{(n)}]_{i}=1}\\end{array}$ for any $\\theta,n$ ", "page_idx": 18}, {"type": "text", "text": "Main Steps ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The proof consists of three main steps. First, we show that the optimal token weight has a lower bound. Then, we show that the gradient aligns with the optimal direction. Third, we show that the norm of Key-Query matrix grows linearly. Combining these three steps, we can prove the Theorem 1 andTheorem 2. ", "page_idx": 18}, {"type": "text", "text": "Recall that the updating rule for $W_{\\mathrm{kq}}^{(t)}$ .s ", "page_idx": 18}, {"type": "equation", "text": "$$\nW_{\\mathrm{kq}}^{(t+1)}=W_{\\mathrm{kq}}^{(t)}-\\eta\\frac{\\nabla_{W_{\\mathrm{kq}}}\\mathcal{L}(\\theta^{(t)})}{\\lVert\\nabla_{W_{\\mathrm{kq}}}\\mathcal{L}(\\theta^{(t)})\\rVert}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "C.2 Step 1 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We first show that the optiaml token weight has a lower bound during the training. ", "page_idx": 18}, {"type": "text", "text": "Lemma 5 (Lower bound of optimal token weight) Under the zero initialization and updating rule Equation (10), for any iteration $t_{\\perp}$ .and any sample $X^{(n)}$ if $l(n)$ is the set of indices of the optimal token in $X^{(n)}$ , the following inequality holds. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\varphi_{\\ell}^{(n,t)}\\geq\\varphi_{\\ell}^{(n,0)}\\geq1/L_{\\operatorname*{max}},\\quad\\forall\\ell\\in l(n)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. ", "page_idx": 18}, {"type": "text", "text": "First, we ntroducethe notationthat $\\begin{array}{r}{\\varphi_{+}^{(n,t)}=\\sum_{\\ell_{*}\\in l(n)}\\varphi_{\\ell_{*}}^{(n,t)}}\\end{array}$ as the sumaton of optimal token weights, and p(n,t) $\\varphi_{-}^{(n,t)}=1-\\varphi_{+}^{(n,t)}$ as the summation of non-optimal token weights. ", "page_idx": 19}, {"type": "text", "text": "$t=0$ due to er intialization, we ave (2.0) =1/L(m) Moreover, by Asumtion 3, foray l  l(n), we have p(\" $\\varphi_{+}^{(n,0)}\\geq q_{n}(x_{\\ell})\\varphi_{\\ell}^{(n,0)}$ ", "page_idx": 19}, {"type": "text", "text": "We perform induction thehypothes: $\\varphi_{+}^{(n,t)}\\geq\\varphi_{+}^{(n,t-1)}$ (n,t-1) ande\" $\\varphi_{\\ell_{*}}^{(n,t)}\\geq\\varphi_{\\ell}^{(n,t)}$ for all $\\ell_{*}\\in l(n)$ and$\\ell\\notin l(n)$ \uff1a", "page_idx": 19}, {"type": "text", "text": "Suppose thpthesishsforta $t$ Let $\\boldsymbol{x}_{\\ell_{*}}^{(n)}$ be the optimal token in the sequence $X^{(n)}$ Fix a sample $X^{(n^{\\prime})}$ . we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(x_{\\ell_{*}}^{(n^{\\prime})})^{\\top}\\nabla_{W_{\\mathrm{kq}}}\\mathcal{L}^{(t)}(\\theta^{(t)})X_{-1}^{(n^{\\prime})}}\\\\ &{\\quad=\\displaystyle\\sum_{n}\\pi^{(n)}([\\boldsymbol{\\mathrm{T}}_{\\theta}^{(n)}]_{\\mathrm{In}(X^{(n)})}-1)\\varphi_{+}^{(n,t)}\\left(\\displaystyle\\sum_{\\ell^{\\prime}\\notin l(n)}\\varphi_{\\ell^{\\prime}}^{(n,t)}(x_{\\ell_{*}}^{(n)}-x_{\\ell^{\\prime}}^{(n)})^{\\top}x_{\\ell_{*}}^{(n^{\\prime})}\\right)\\left\\langle X_{-1}^{(n)},X_{-1}^{(n^{\\prime})}\\right\\rangle}\\\\ &{\\quad\\,+\\displaystyle\\sum_{n}\\pi^{(n)}\\displaystyle\\sum_{\\ell\\notin l(n)}[\\boldsymbol{\\mathrm{T}}_{\\theta}^{(n)}]_{\\mathrm{In}(x_{\\ell}^{(n)})}\\varphi_{\\ell}^{(n,t)}\\left(\\displaystyle\\sum_{\\ell^{\\prime}}\\varphi_{\\ell^{\\prime}}^{(n,t)}(x_{\\ell}^{(n)}-x_{\\ell^{\\prime}}^{(n)})^{\\top}x_{\\ell_{*}}^{(n^{\\prime})}\\right)\\left\\langle X_{-1}^{(n)},X_{-1}^{(n^{\\prime})}\\right\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Because $\\begin{array}{r}{\\sum_{\\ell^{\\prime}\\notin l(n)}(x_{\\ell^{\\prime}}^{(n)})x_{\\ell_{*}}^{(n^{\\prime})}=0}\\end{array}$ $(x_{\\ell_{*}}^{(n)})^{\\top}x_{\\ell_{*}}^{(n^{\\prime})}\\geq0$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n(x_{\\ell_{*}}^{(n^{\\prime})})^{\\top}\\nabla_{W_{\\mathrm{kq}}}\\mathcal{L}^{(t)}(\\theta^{(t)})X_{-1}^{(n^{\\prime})}\\leq0.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Let xlo $x_{\\ell_{0}}^{(n^{\\prime})}$ be anynon-tiltnin the seqene $X^{(n^{\\prime})}$ . Then, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(x_{n}^{(a)})^{\\top}\\nabla\\mathfrak{w}_{n}\\mathrm{e}^{(E(t))}x_{-1}^{(a)}}\\\\ &{=\\sum_{n}\\pi^{(a)}([\\Gamma_{\\theta}^{(a)}]_{\\ln(X^{(a)})}-1)\\varphi_{+}^{(a,b)}\\left(\\sum_{\\ell\\notin[n]}\\varphi_{\\ell}^{(a,b)}(x_{\\ell}^{(a)}-x_{\\ell}^{(a)})^{\\top}x_{\\ell}^{(a)}\\right)\\left\\langle X_{-1}^{(a)},X_{-1}^{(a)}\\right\\rangle}\\\\ &{\\phantom{(a)^{\\top}\\alpha^{(a)}}+\\sum_{n}\\pi^{(a)}\\sum_{\\ell\\in[n]}\\left[\\Gamma_{\\theta}^{(a)}\\right]_{\\ln(\\ell_{n}^{(a)})}\\varphi_{\\ell}^{(a,b)}\\left(\\sum_{\\ell\\in[n]}\\varphi_{\\ell}^{(a,b)}(x_{\\ell}^{(a)}-x_{\\ell}^{(a)})^{\\top}x_{\\ell}^{(a)}\\right)\\left\\langle X_{-1}^{(a)},X_{-1}^{(a)}\\right\\rangle}\\\\ &{=\\sum_{n}\\pi^{(a)}([\\Gamma_{\\theta}^{(a)}]_{\\ln(X^{(a)})}-1)\\varphi_{+}^{(a,b)}\\left(\\sum_{\\ell\\in[n]}\\varphi_{\\ell}^{(a,b)}(-x_{\\ell}^{(a)})^{\\top}x_{\\ell}^{(a)}\\right)\\left\\langle X_{-1}^{(a)},X_{-1}^{(a)}\\right\\rangle}\\\\ &{\\phantom{(a)^{\\top}\\alpha^{(a)}}+\\sum_{n}\\pi^{(a)}\\sum_{\\ell\\in[n]}\\left[\\Gamma_{\\theta}^{(a)}\\right]_{\\ln(\\ell_{n}^{(a)})}\\varphi_{\\ell}^{(a,b)}\\left(\\sum_{\\ell\\in[n]}\\varphi_{\\ell}^{(a,b)}(x_{\\ell}^{(a)}-x_{\\ell}^{(a)})^{\\top}x_{\\ell}^{(a)}\\right)\\left\\langle X_{-1}^{(a)},X_{-1}^{(a)}\\right\\rangle}\\\\ &{\\geq\\sum_{n}\\pi^{(a)}\\left((1-[\\Gamma_{\\theta}^{(a)}]_{\\ln(X^{(a)} \n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "\u22650, ", "page_idx": 19}, {"type": "text", "text": "where the last inequality is due to Assumption 3, the induction hypothesis and $\\begin{array}{r}{\\sum_{i\\in[|\\mathcal{V}|]}[\\mathrm{T}_{\\theta}^{(n)}]_{i}=1}\\end{array}$ Therefore, for any $n$ ,wehave ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\varphi_{\\ell_{*}}^{(n,t+1)}=\\frac{\\exp\\left((x_{\\ell_{*}}^{(n)})^{\\top}W_{\\mathrm{kq}}^{(t+1)}X_{-1}^{(n)}\\right)}{\\sum_{\\ell}\\exp\\left((x_{\\ell}^{(n)})^{\\top}W_{\\mathrm{kq}}^{(t+1)}X_{-1}^{(n)}\\right)}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\frac{\\exp\\Big((x_{\\varepsilon_{\\varepsilon}}^{(n)})^{\\top}W_{\\mathbf{kq}}^{(t)}X_{-1}^{(n)}-\\eta(x_{\\varepsilon_{\\varepsilon}}^{(n)})^{\\top}\\nabla_{W_{\\mathbf{kq}}}\\mathcal L^{(t)}(\\theta^{(t)})X_{-1}^{(n)}/\\|\\nabla_{W_{\\mathbf{kq}}}\\mathcal L^{(t)}(\\theta^{(t)})\\|\\Big)}{\\sum_{\\ell}\\exp\\Big((x_{\\ell}^{(n)})^{\\top}W_{\\mathbf{kq}}^{(t)}X_{-1}^{(n)}-\\eta(x_{\\ell}^{(n)})^{\\top}\\nabla_{W_{\\mathbf{kq}}}\\mathcal L^{(t)}(\\theta^{(t)})X_{-1}^{(n)}/\\|\\nabla_{W_{\\mathbf{kq}}}\\mathcal L^{(t)}(\\theta^{(t)})\\|\\Big)}}\\\\ &{=\\frac{\\exp\\Big((x_{\\varepsilon_{\\varepsilon}}^{(n)})^{\\top}W_{\\mathbf{kq}}^{(t)}X_{-1}^{(n)}+\\eta(x_{\\varepsilon_{\\varepsilon}}^{(n)})^{\\top}\\nabla_{W_{\\mathbf{kq}}}\\mathcal L^{(t)}(\\theta^{(t)})X_{-1}^{(n)}\\Big)}{\\sum_{\\ell}\\exp\\Big((x_{\\ell}^{(n)})^{\\top}W_{\\mathbf{kq}}^{(t)}X_{-1}^{(n)}+\\eta(x_{\\varepsilon_{\\varepsilon}}^{(n)}-x_{\\ell}^{(n)})^{\\top}\\nabla_{W_{\\mathbf{kq}}}\\mathcal L^{(t)}(\\theta^{(t)})X_{-1}^{(n)}/\\|\\nabla_{W_{\\mathbf{kq}}}\\mathcal L^{(t)}(\\theta^{(t)})\\|\\Big)}}\\\\ &{\\geq\\frac{\\exp\\Big((x_{\\varepsilon_{\\varepsilon}}^{(n)})^{\\top}W_{\\mathbf{kq}}^{(t)}X_{-1}^{(n)}\\Big)}{\\sum_{\\ell}\\exp\\Big((x_{\\ell}^{(n)})^{\\top}W_{\\mathbf{kq}}^{(t)}X_{-1}^{(n)}\\Big)}}\\\\ &{=\\varrho_{0}^{(n,t)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which implies that $\\varphi_{+}^{(n,t+1)}\\geq\\varphi_{+}^{(n,t)}$ ", "page_idx": 20}, {"type": "text", "text": "For the second argument in the hypothesis, we examine (n,t+1)/(n,t+1) for any l  l(n). We have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\rho_{\\ell_{*}}^{(n,t+1)}}{\\rho_{\\ell}^{(n,t+1)}}=\\exp\\Big((x_{\\ell_{*}}^{(n)}-x_{\\ell}^{(n)})^{\\top}W_{\\mathbf{kq}}^{(t+1)}X_{-1}^{(n)}\\Big)}\\\\ &{\\qquad\\quad=\\exp\\Big((x_{\\ell_{*}}^{(n)}-x_{\\ell}^{(n)})^{\\top}W_{\\mathbf{kq}}^{(t)}X_{-1}^{(n)}\\Big)\\exp\\Bigg({-\\frac{\\eta}{\\|\\nabla_{W_{\\mathbf{kq}}}\\mathcal{L}^{(t)}(\\theta^{(t)})\\|}(x_{\\ell_{*}}^{(n)}-x_{\\ell}^{(n)})^{\\top}\\nabla_{W_{\\mathbf{kq}}}\\mathcal{L}^{(t)}(\\theta^{(t)})}\\Bigg)}\\\\ &{\\qquad\\quad\\ge\\frac{\\varphi_{\\ell_{*}}^{(n,t)}}{\\varphi_{\\ell}^{(n,t)}}}\\\\ &{\\qquad\\quad\\ge1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The proof is finished. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "C.3Step 2 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The following lemma shows that the norm of the Key-Query Matrix increases linearly with the number of iterations. ", "page_idx": 20}, {"type": "text", "text": "Lemma 6 Under the initialization $W_{\\mathrm{kq}}^{(0)}$ and the updating rule Equation (10),for each iteration $t$ the following inequality holds. ", "page_idx": 20}, {"type": "equation", "text": "$$\nt\\eta+\\|W_{\\mathbf{kq}}^{(0)}\\|\\geq\\|W_{\\mathbf{kq}}^{(t)}\\|\\geq\\frac{t\\eta}{2L_{\\operatorname*{max}}\\|W_{\\mathbf{kq}}^{*}\\|}-\\|W_{\\mathbf{kq}}^{(0)}\\|.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. ", "page_idx": 20}, {"type": "text", "text": "We examine the gradient $\\nabla_{W_{\\mathbf{k}_{\\mathbf{q}}}}\\mathcal{L}(\\theta)$ projected onto the optimal direction $W_{\\mathrm{kq}}^{*}/\\vert\\vert W_{\\mathrm{kq}}^{*}\\vert\\vert$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\langle\\nabla_{W_{\\mathbf{k}}\\mathbf{q}}\\mathcal{L}(\\theta^{(t)}),W_{\\mathbf{k}\\mathbf{q}}^{*}\\right\\rangle}\\\\ &{\\quad=\\displaystyle\\sum_{n}\\pi^{(n)}\\Delta\\sum_{\\ell,\\neq l(n)}\\left([\\Gamma_{\\theta^{(t)}}^{(n)}]_{\\mathrm{In}(x_{\\ell,\\star}^{(n)})}-1\\right)\\varphi_{\\ell,\\star}^{(n,t)}\\left(a_{\\ell,\\star}^{(n,*)}-\\sum_{\\ell^{\\prime}}\\varphi_{\\ell^{\\prime}}^{(n,t)}a_{\\ell^{\\prime}}^{(n,*)}\\right)}\\\\ &{\\quad\\quad+\\displaystyle\\sum_{n}\\pi^{(n)}\\Delta\\sum_{\\ell\\notin l(n)}[\\Gamma_{\\theta^{(t)}}^{(n)}]_{\\mathrm{In}(x_{\\ell}^{(n)})}\\varphi_{\\ell}^{(n,t)}\\left(a_{\\ell}^{(n,*)}-\\sum_{\\ell^{\\prime}}\\varphi_{\\ell^{\\prime}}^{(n,t)}a_{\\ell^{\\prime}}^{(n,*)}\\right)}\\\\ &{\\quad=\\displaystyle\\sum_{n}\\pi^{(n)}\\Delta\\big([\\Gamma_{\\theta^{(t)}}^{(n)}]_{\\mathrm{In}(X^{(n)})}-1\\big)\\varphi_{+}^{(n,t)}\\left(\\displaystyle\\sum_{\\ell^{\\prime}\\notin l(n)}\\varphi_{\\ell^{\\prime}}^{(n,t)}(a_{\\ell,\\star}^{(n,*)}-a_{\\ell^{\\prime}}^{(n,*)})\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad+\\displaystyle\\sum_{n}\\pi^{(n)}\\Delta\\sum_{\\ell\\not\\in I(n)}[\\Gamma_{\\theta^{(t)}}^{(n)}]_{\\mathrm{In}(x_{\\ell}^{(n)})}\\varphi_{\\ell}^{(n,t)}\\left(\\displaystyle\\sum_{\\ell^{\\prime}\\in I(n)}\\varphi_{\\ell^{\\prime}}^{(n,t)}(a_{\\ell}^{(n,*)}-a_{\\ell^{\\prime}}^{(n,*)})\\right)}\\\\ &{\\leq\\displaystyle\\sum_{n}\\pi^{(n)}\\Delta([\\Gamma_{\\theta^{(t)}}^{(n)}]_{\\mathrm{In}(X^{(n)})}-1)\\varphi_{+}^{(n,t)}\\varphi_{-}^{(n,t)}}\\\\ &{\\quad+\\displaystyle\\sum_{n}\\pi^{(n)}\\Delta\\sum_{\\ell\\not\\in I(n)}[\\Gamma_{\\theta^{(t)}}^{(n)}]_{\\mathrm{In}(x_{\\ell}^{(n)})}\\varphi_{\\ell}^{(n,t)}(-\\varphi_{+}^{(n,t)}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where+ $\\begin{array}{r}{\\varphi_{+}^{(n,t)}=\\sum_{\\ell_{*}\\in l(n)}\\varphi_{\\ell_{*}}^{(n,t)}}\\end{array}$ $\\varphi_{-}^{(n,t)}=1-\\varphi_{+}^{(n,t)}$ is the summation of non-optimal token weights. ", "page_idx": 21}, {"type": "text", "text": "On the other hand, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{W_{\\mathbf{k}}}C(\\theta^{(t)})\\|}\\\\ &{\\quad=\\left\\langle\\nabla_{W_{\\mathbf{k}},\\mathbf{q}}C(\\theta^{(t)}),\\frac{\\nabla_{W_{\\mathbf{k}},\\mathbf{q}}C(\\theta^{(t)})}{\\|\\nabla_{W_{\\mathbf{k}},\\mathbf{q}}C(\\theta^{(t)})\\|}\\right\\rangle}\\\\ &{\\quad=\\displaystyle\\sum_{n}\\pi^{(n)}\\Delta\\sum_{\\ell,\\in\\mathcal{L}[n]}\\left([\\Gamma_{\\theta^{(t)}}^{(n)}]_{\\ln(x_{\\ell}^{(n)})}-1\\right)\\varphi_{\\ell,}^{(n,t)}\\left(x_{\\ell,}^{(n)}-\\sum_{\\ell^{\\prime}}\\varphi_{\\ell^{\\prime}}^{(n,t)}x_{\\ell^{\\prime}}^{(n)}\\right)^{\\top}\\frac{\\nabla_{W_{\\mathbf{k}},\\mathbf{q}}C(\\theta^{(t)})}{\\|\\nabla_{W_{\\mathbf{k}},\\mathbf{q}}C(\\theta^{(t)})\\|}X_{-1}^{(n)}}\\\\ &{\\qquad\\,\\,+\\displaystyle\\sum_{n}\\pi^{(n)}\\Delta\\sum_{\\ell\\notin\\{n\\}}[\\Gamma_{\\theta^{(t)}}^{(n)}]_{\\ln(x_{\\ell}^{(n)})}\\varphi_{\\ell}^{(n,t)}\\left(x_{\\ell}^{(n)}-\\sum_{\\ell^{\\prime}}\\varphi_{\\ell^{\\prime}}^{(n,t)}x_{\\ell^{\\prime}}^{(n)}\\right)^{\\top}\\frac{\\nabla_{W_{\\mathbf{k}},\\mathbf{q}}C(\\theta^{(t)})}{\\|\\nabla_{W_{\\mathbf{k}},\\mathbf{q}}C(\\theta^{(t)})\\|}X_{-1}^{(n)}}\\\\ &{\\qquad\\leq2\\sum_{n}\\pi^{(n)}\\Delta(1-[\\Gamma_{\\theta}^{(n)}]_{\\ln(X^{(n)})})\\varphi_{\\ell}^{(n,t)}\\varphi_{-}^{(n,t)}+2\\displaystyle\\sum_{n}\\pi^{(n)}\\Delta\\sum_{\\ell\\notin\\{n\\}}[\\Gamma_{\\theta}^{(n)}]_{\\ln(x_{\\ell}^{(n)})}\\varphi_{\\ell}^{(n,t)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Thus, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left\\langle\\frac{\\nabla_{W_{\\mathrm{kq}}}\\mathcal{L}(\\theta)}{\\left\\|\\nabla_{W_{\\mathrm{kq}}}\\mathcal{L}(\\theta)\\right\\|},\\frac{W_{\\mathrm{kq}}^{*}}{\\left\\|W_{\\mathrm{kq}}^{*}\\right\\|}\\right\\rangle\\leq-\\frac{\\operatorname*{min}_{n}\\varphi_{+}^{(n,t)}}{2\\|W_{\\mathrm{kq}}^{*}\\|}\\leq-\\frac{1}{2L_{\\operatorname*{max}}\\|W_{\\mathrm{kq}}^{*}\\|}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "By the updating rule Equation (10), we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|W_{\\mathrm{kq}}^{(t)}\\|=\\bigg\\|W_{\\mathrm{kq}}^{(0)}-\\displaystyle\\sum_{t^{\\prime}=0}^{t-1}\\eta\\frac{\\nabla_{W_{\\mathrm{kq}}}\\mathcal{L}(\\theta^{(t^{\\prime})})}{\\|\\nabla_{W_{\\mathrm{kq}}}\\mathcal{L}(\\theta^{(t^{\\prime})})\\|}\\bigg\\|}\\\\ &{\\qquad\\ge\\bigg\\langle W_{\\mathrm{kq}}^{(0)},\\displaystyle\\frac{W_{\\mathrm{kq}}^{*}}{\\|W_{\\mathrm{kq}}^{*}\\|}\\bigg\\rangle-\\displaystyle\\sum_{t^{\\prime}\\le t-1}\\eta\\left\\langle\\frac{\\nabla_{W_{\\mathrm{kq}}}\\mathcal{L}(\\theta^{(t^{\\prime})})}{\\|\\nabla_{W_{\\mathrm{kq}}}\\mathcal{L}(\\theta^{(t^{\\prime})})\\|},\\frac{W_{\\mathrm{kq}}^{*}}{\\|W_{\\mathrm{kq}}^{*}\\|}\\right\\rangle}\\\\ &{\\qquad\\ge\\displaystyle\\sum_{t^{\\prime}<t}\\frac{\\eta}{2L_{\\mathrm{max}}\\|W_{\\mathrm{kq}}^{*}\\|}-\\|W_{\\mathrm{kq}}^{(0)}\\|}\\\\ &{\\qquad=\\frac{t\\eta}{2L_{\\mathrm{max}}\\|W_{\\mathrm{kq}}^{*}\\|}-\\|W_{\\mathrm{kq}}^{(0)}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "In addition, by the triangle inequality, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\|W_{\\mathrm{kq}}^{(t)}\\|=\\left\\|W_{\\mathrm{kq}}^{(0)}-\\sum_{t^{\\prime}=0}^{t-1}\\eta\\frac{\\nabla_{W_{\\mathrm{kq}}}\\mathcal{L}(\\theta^{(t^{\\prime})})}{\\left\\|\\nabla_{W_{\\mathrm{kq}}}\\mathcal{L}(\\theta^{(t^{\\prime})})\\right\\|}\\right\\|}}\\\\ &{}&{\\leq t\\eta+\\|W_{\\mathrm{kq}}^{(0)}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The proof is completed. ", "page_idx": 21}, {"type": "text", "text": "C.4   Step 3 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We next show that the gradient $\\nabla_{W_{\\mathrm{kq}}}\\mathcal{L}(\\theta^{(t)})$ is close to the optimal direction $W_{\\mathrm{kq}}^{*}$ ", "page_idx": 22}, {"type": "text", "text": "Lemma 7 (Gradient aligns with the optimal direction) Let LmaxIWkal\u00b27. Then, for any $t\\geq t_{0}$ ,wehave ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left\\langle\\nabla_{W_{{\\mathrm{kq}}}}\\mathcal{L}(\\theta^{(t)}),W_{{\\mathrm{kq}}}^{(t)}\\right\\rangle\\geq(1+\\alpha_{t})\\left\\langle\\nabla_{W_{{\\mathrm{kq}}}}\\mathcal{L}(\\theta^{(t)}),W_{\\mathrm{kq}}^{*}\\right\\rangle\\frac{\\|W_{{\\mathrm{kq}}}^{(t)}\\|}{\\|W_{\\mathrm{kq}}^{*}\\|}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\alpha_{t}=\\frac{4N L_{\\mathrm{max}}^{2}\\|W_{\\mathrm{kq}}^{*}\\|^{2}}{\\|W_{\\mathrm{kq}}^{(t)}\\|}\\left(1+\\log\\left(2L_{\\mathrm{max}}\\|W_{\\mathrm{kq}}^{(t)}\\|\\right)\\right)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. During the proof, we denote ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{a_{\\ell}^{(n,t)}=(x_{\\ell}^{(n)})^{\\top}W_{\\mathrm{kq}}^{(t)}X_{-1}^{(n)}}\\\\ {a_{\\ell}^{(n,*)}=(x_{\\ell}^{(n)})^{\\top}W_{\\mathrm{kq}}^{*}X_{-1}^{(n)}\\displaystyle\\frac{\\lVert W_{\\mathrm{kq}}^{(t)}\\rVert}{\\lVert W_{\\mathrm{kq}}^{*}\\rVert}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\n\\beta_{0}=\\frac{2L_{\\operatorname*{max}}^{2}\\|W_{\\mathrm{kq}}^{*}\\|^{2}}{\\|W_{\\mathrm{kq}}^{(t)}\\|}(1+\\log(2L_{\\operatorname*{max}}\\|W_{\\mathrm{kq}}^{(t)}\\|)).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We point out a few facts that will be frequently used in the proof. ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\varphi_{\\ell}^{(n,t)}=\\varphi_{\\ell^{\\prime}}^{(n,t)}\\exp\\left(a_{\\ell}^{(n,t)}-a_{\\ell^{\\prime}}^{(n,t)}\\right)\\le\\exp(-C_{0})\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The same result holds if $a_{\\ell^{\\prime}}^{(n,t)}$ isreplacdanyconexcominatiofs $a_{\\ell^{\\prime}}^{(n,t)}$ ", "page_idx": 22}, {"type": "text", "text": "We start the proof by noting that $W_{\\mathrm{kq}}^{*}$ is the minimum unique solution to the problem ", "page_idx": 22}, {"type": "equation", "text": "$$\nW_{\\mathrm{kq}}^{*}=\\arg\\operatorname*{min}\\|W\\|,\\quad\\mathrm{s.t.}\\quad(x_{\\ell_{*}}^{(n)}-x_{\\ell}^{(n)})W X_{-1}^{(n)}\\geq1,\\quad\\forall\\ell_{*}\\in l^{(n)},\\ell\\not\\in l^{(n)},\\forall n.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Therefore,if Wc)Iwroll $W_{\\mathrm{kq}}^{(t)}\\frac{\\lVert W_{\\mathrm{kq}}^{*}\\rVert}{\\lVert W_{\\mathrm{kq}}^{(t)}\\rVert}=W_{\\mathrm{kq}}^{*}$ ,theresult is rilsincer $\\left\\langle\\nabla_{W_{\\mathrm{kq}}}\\mathcal{L}(\\theta^{(t)}),W_{\\mathrm{kq}}^{*}\\right\\rangle\\leq0.$ ", "page_idx": 22}, {"type": "text", "text": "In the follwing, we focus on the case when W W $W_{\\mathrm{kq}}^{(t)}\\frac{\\lVert W_{\\mathrm{kq}}^{*}\\rVert}{\\lVert W_{\\mathrm{kq}}^{(t)}\\rVert}\\,\\neq W_{\\mathrm{kq}}^{*}$ Then, the must e at least a sentence X(n), such that W(t) IWa violates the contraint on $X^{(n)}$ . In other words, we mus have ", "page_idx": 22}, {"type": "equation", "text": "$$\na_{\\ell_{*}}^{(n,t)}-a_{\\ell}^{(n,t)}=(x_{\\ell_{*}}^{(n)}-x_{\\ell}^{(n)})W_{\\mathrm{kq}}^{(t)}X_{-1}^{(n)}\\leq\\frac{\\|W_{\\mathrm{kq}}^{(t)}\\|}{\\|W_{\\mathrm{kq}}^{*}\\|}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "This implies that for those $n$ , we must have $\\begin{array}{r}{\\varphi_{\\ell}^{(n,t)}\\geq\\exp(-\\frac{\\|W_{\\mathbf{kq}}^{(t)}\\|}{\\|W_{\\mathbf{kq}}^{*}\\|})}\\end{array}$ ", "page_idx": 22}, {"type": "text", "text": "Thus, we consider two types of samples in the folloiwng. ", "page_idx": 22}, {"type": "text", "text": "Type 1. Let us consider $X^{(n)}$ suchthat $\\varphi_{-}^{(n,t)}\\geq\\exp(-(1+\\beta_{0}/2)\\|W_{\\mathrm{kq}}^{(t)}\\|/\\|W_{\\mathrm{kq}}^{*}\\|).$ ", "page_idx": 22}, {"type": "text", "text": "Recall that the inner product between the gradient $\\nabla_{W_{\\mathrm{kq}}}\\mathcal{L}(\\theta^{(t)})$ and any other Key-Query matrix $\\theta^{\\prime}=W_{\\mathrm{kq}}^{\\prime}$ has the following form Lemma 4). ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left\\langle\\nabla_{W_{\\mathrm{kq}}}\\mathcal{L}(\\theta^{(t)}),W_{\\mathrm{kq}}^{\\prime}\\right\\rangle\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{c l}{{}}&{{=\\displaystyle\\Delta\\sum_{n}\\pi^{(n)}([\\mathrm{T}_{\\theta}^{(n)}]_{\\mathrm{In}(X^{(n)})}-1)\\sum_{\\ell_{*}\\in l(n)}\\varphi_{\\ell_{*}}^{(n,\\theta)}\\left(x_{\\ell_{*}}^{(n)}-\\sum_{\\ell^{\\prime}}\\varphi_{\\ell^{\\prime}}^{(n,\\theta)}x_{\\ell^{\\prime}}^{(n)}\\right)^{\\top}W_{\\mathrm{kq}}^{\\prime}X_{-1}^{(n)}}}\\\\ {{}}&{{\\displaystyle\\quad+\\,\\Delta\\sum_{n}\\pi^{(n)}\\sum_{\\ell\\notin l(n)}[\\mathrm{T}_{\\theta}^{(n)}]_{\\mathrm{In}(x_{\\ell}^{(n)})}\\varphi_{\\ell}^{(n,\\theta)}\\left(x_{\\ell}^{(n)}-\\sum_{\\ell^{\\prime}}\\varphi_{\\ell^{\\prime}}^{(n,\\theta)}x_{\\ell^{\\prime}}^{(n)}\\right)^{\\top}W_{\\mathrm{kq}}^{\\prime}X_{-1}^{(n)}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Let $\\begin{array}{r}{L_{n}(\\theta)=-\\log e_{\\mathrm{In}(X^{(n)})}^{\\top}\\mathrm{T}_{\\theta}\\big(X^{(n)}\\big)}\\end{array}$ be the loss on sample $X^{(n)}$ ", "page_idx": 23}, {"type": "text", "text": "To proceed, we examine the gradient on each sample $X^{(n)}$ With $\\begin{array}{r l r}{\\varphi_{-}^{(n,t)}}&{{}\\ge}&{\\exp(-(1\\;+\\;}\\end{array}$ $\\beta_{0}/2)\\|W_{\\mathrm{kq}}^{(t)}\\|/\\|W_{\\mathrm{kq}}^{*}\\|)$ , hich can be divide ito two parts. $\\left\\langle\\nabla_{W_{\\mathrm{kq}}}L_{n}(\\theta^{(t)}),W_{\\mathrm{kq}}\\right\\rangle=\\Delta(A^{(n,t)}+$ $B^{(n,t)})$ ,where ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\{A^{(n,t)}=\\displaystyle\\sum_{\\ell_{*}\\in l(n)}([\\mathrm{T}_{\\theta^{(t)}}^{(n)}]_{\\mathrm{In}(x_{\\ell_{*}}^{(n)})}-1)\\varphi_{\\ell_{*}}^{(n,t)}\\left(a_{\\ell_{*}}^{(n,t)}-\\displaystyle\\sum_{\\ell^{\\prime}}\\varphi_{\\ell^{\\prime}}^{(n,t)}a_{\\ell^{\\prime}}^{(n,t)}\\right),\\right.}\\\\ &{\\left.\\left(B^{(n,t)}=\\displaystyle\\sum_{\\ell\\notin l(n)}[\\mathrm{T}_{\\theta^{(t)}}^{(n)}]_{\\mathrm{In}(x_{\\ell}^{(n)})}\\varphi_{\\ell}^{(n,t)}\\left(a_{\\ell}^{(n,t)}-\\displaystyle\\sum_{\\ell^{\\prime}}\\varphi_{\\ell^{\\prime}}^{(n,t)}a_{\\ell^{\\prime}}^{(n,t)}\\right).\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We further let ", "page_idx": 23}, {"type": "equation", "text": "$$\nA^{(n,*)}=\\sum_{\\ell_{*}\\in l(n)}([\\Gamma_{\\theta^{(t)}}^{(n)}]_{\\mathrm{I_{n}}(x_{\\ell}^{(n)})}-1)\\varphi_{\\ell_{*}}^{(n,t)}\\left(a_{\\ell_{*}}^{(n,*)}-\\sum_{\\ell^{\\prime}}\\varphi_{\\ell^{\\prime}}^{(n,t)}a_{\\ell^{\\prime}}^{(n,*)}\\right),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and ", "page_idx": 23}, {"type": "equation", "text": "$$\nB^{(n,*)}=\\sum_{\\ell\\notin l(n)}[\\mathrm{T}_{\\theta^{(t)}}^{(n)}]_{\\mathrm{I_{n}}(x_{\\ell}^{(n)})}\\varphi_{\\ell}^{(n,t)}\\left(a_{\\ell}^{(n,*)}-\\sum_{\\ell^{\\prime}}\\varphi_{\\ell^{\\prime}}^{(n,t)}a_{\\ell^{\\prime}}^{(n,*)}\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Thus, we aim to find the relationship $A^{(n,t)}+B^{(n,t)}$ between $A^{(n,*)}+B^{(n,*)}$ ", "page_idx": 23}, {"type": "text", "text": "We first provide the upper bounds for $A^{(n,*)}$ and $B^{(n,*)}$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A^{(n,*)}=\\displaystyle\\sum_{\\ell_{*}\\in l(n)}\\left([\\mathrm{T}_{\\theta^{(t)}}^{(n)}]_{\\mathrm{In}(x_{\\ell_{*}}^{(n)})}-1\\right)\\varphi_{\\ell_{*}}^{(n,t)}\\left(a_{\\ell_{*}}^{(n,*)}-\\displaystyle\\sum_{\\ell^{\\prime}}\\varphi_{\\ell^{\\prime}}^{(n,t)}a_{\\ell^{\\prime}}^{(n,*)}\\right)}\\\\ &{\\ \\ \\ \\ \\ \\ =\\displaystyle\\sum_{\\ell_{*}\\in l(n)}\\left([\\mathrm{T}_{\\theta^{(t)}}^{(n)}]_{\\mathrm{In}(x_{\\ell_{*}}^{(n)})}-1\\right)\\varphi_{\\ell_{*}}^{(n,t)}\\left(\\displaystyle\\sum_{\\ell^{\\prime}\\neq l(n)}\\varphi_{\\ell^{\\prime}}^{(n,t)}(a_{\\ell_{*}}^{(n,*)}-a_{\\ell^{\\prime}}^{(n,*)})\\right)}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ }\\\\ &{\\leq\\left([\\mathrm{T}_{\\theta^{(t)}}^{(n)}]_{\\mathrm{In}(X^{(n)})}-1)\\varphi_{+}^{(n,t)}\\varphi_{-}^{(n,t)}\\displaystyle\\frac{\\lVert W_{\\mathrm{iq}}^{(t)}\\rVert}{\\lVert W_{\\mathrm{kq}}^{*}\\rVert},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $(a)$ is due to the fact that $(x_{\\ell_{*}}^{(n)}\\ \\ -\\ x_{\\ell}^{(n)})W_{\\mathrm{kq}}^{*}X_{-1}^{(n)}\\ \\ \\ \\geq\\ \\ \\ 1,$ \u2265 1, and a(n,.t) $\\begin{array}{r l}{a_{\\ell}^{(n,*)}}&{{}=}\\end{array}$ $(x_{\\ell}^{(n)})^{\\top}W_{\\mathrm{kq}}^{*}X_{-1}^{(n)}\\|W_{\\mathrm{kq}}^{(t)}\\|/\\|W_{\\mathrm{kq}}^{*}\\|$ ", "page_idx": 23}, {"type": "text", "text": "On the other hand ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A^{(n,t)}=\\displaystyle\\sum_{\\ell\\in l(n)}\\big([\\mathrm{T}_{\\ell^{(t)}}^{(n)}]_{\\mathrm{In}(x_{\\ell}^{(n)})}-1\\big)\\varphi_{\\ell}^{(n,t)}\\left(a_{\\ell}^{(n,t)}-\\displaystyle\\sum_{\\ell^{\\prime}}\\varphi_{\\ell^{\\prime}}^{(n,t)}a_{\\ell^{\\prime}}^{(n,t)}\\right)}\\\\ &{\\phantom{A^{(n,t)}=\\displaystyle\\sum_{\\ell\\in l(n)}}([\\mathrm{T}_{\\ell^{(t)}}^{(n)}]_{\\mathrm{In}(x_{\\ell}^{(n)})}-1)\\varphi_{\\ell}^{(n,t)}\\left(\\displaystyle\\sum_{\\ell^{\\prime}\\notin l(n)}\\varphi_{\\ell^{\\prime}}^{(n,t)}(a_{\\ell}^{(n,t)}-a_{\\ell^{\\prime}}^{(n,t)})\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\operatorname*{max}\\left\\{\\displaystyle\\sum_{c\\in[1,n]}([\\frac{\\Gamma\\left(m_{\\theta}^{(n)}\\right)}{\\Gamma\\left(m_{\\theta}^{(n)}\\right)}|u_{t}(x^{*}))-1\\right\\}\\psi_{t}^{(n)}\\left(\\sum_{c\\in[2,n]}\\psi_{t}^{(n)}\\left(\\frac{\\theta^{(n)}-q_{\\theta}^{(n)}}{n}\\right)\\right)}\\\\ &{\\phantom{=}+\\displaystyle\\sum_{c\\in[1,n]}([\\frac{\\Gamma\\left(m_{\\theta}^{(n)}\\right)}{\\Gamma\\left(m_{\\theta}^{(n)}\\right)}|u_{t}(x^{*}))-1)\\psi_{t}^{(n)}\\left(\\sum_{c\\in[2,1]}\\psi_{t}^{(n)}\\left(\\frac{\\theta^{(n)}-q_{\\theta}^{(n)}}{n}\\right)\\right)\\cdot}\\\\ &{\\phantom{=}\\displaystyle\\sum_{c\\in[1,n]}\\sum_{c\\in[1,0]}([\\frac{\\Gamma\\left(m_{\\theta}^{(n)}\\right)}{\\Gamma\\left(m_{\\theta}^{(1)}\\right)}|u_{t}(x^{*}))-1)\\psi_{t}^{(n)}\\left(\\sum_{c\\in[\\ell]}\\psi_{t}^{(n)}\\left(\\frac{\\theta^{(n)}-q_{\\theta}^{(n)}}{n}\\right)\\right)}\\\\ &{\\phantom{=}+\\displaystyle\\sum_{c\\in[1,n]}([\\frac{\\Gamma\\left(m_{\\theta}^{(n)}\\right)}{\\Gamma\\left(m_{\\theta}^{(1)}\\right)}|u_{t}(x^{*}))-1)\\psi_{t}^{(n)}\\left(\\sum_{c\\in[2,1]}\\psi_{t}^{(n)}\\left(\\frac{\\theta^{(n)}}{n}\\right)\\right)}\\\\ &{\\phantom{=}+\\displaystyle\\sum_{c\\in[1,n]}([\\frac{\\Gamma\\left(m_{\\theta}^{(n)}\\right)}{\\Gamma\\left(m_{\\theta}^{(1)}\\right)}|u_{t}(x^{*}))-1)\\psi_{t}^{(n)}\\left(\\sum_{c\\in[2,1]}\\psi_{t}^{(n)}\\left(\\frac{\\theta^{(n)}-q_{\\theta}^{(n)}}{n}\\right)\\right)\\cdot}\\\\ &{\\phantom{=}\\displaystyle\\sum_{c\\in[1,n]}(\\sum_{c\\in[1,0]}([\\frac{\\Gamma\\left(m_{\\theta} \n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $(a)$ is due to Equation 11), and $(b)$ is obtained by choosing $\\begin{array}{r}{\\mathrm{T}=\\log\\frac{2L_{\\mathrm{max}}\\|W_{\\mathrm{kq}}^{(t)}\\|}{\\varphi_{-}^{(n,t)}}}\\end{array}$ ", "page_idx": 24}, {"type": "text", "text": "$\\begin{array}{r}{\\varphi_{-}^{(n,t)}\\;\\geq\\;\\exp\\left(-(1+\\beta_{0}/2)\\|W_{\\mathrm{kq}}^{(t)}\\|/\\|W_{\\mathrm{kq}}^{*}\\|\\right)\\mathrm{~and~}\\beta_{0}\\;\\geq\\;\\frac{2\\|W_{\\mathrm{kq}}^{*}\\|(1+\\log(2L_{\\operatorname*{max}}\\|W_{\\mathrm{kq}}^{(t)}\\|))}{\\|W_{\\mathrm{kq}}^{(t)}\\|}.}\\end{array}$ Thus, we further have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A^{(n,t)}\\geq([\\Gamma_{\\theta^{(\\ell)}}^{(n)}]_{\\mathrm{In}}(x^{(n)})-1)\\varphi_{+}^{(n,t)}\\varphi_{-}^{(n,t)}\\left(1+\\log\\frac{2L_{\\operatorname*{max}}\\|W_{\\mathbf{kq}}^{(t)}\\|}{\\varphi_{-}^{(n,t)}}\\right)}\\\\ &{\\qquad\\geq([\\Gamma_{\\theta^{(\\ell)}}^{(n)}]_{\\mathrm{In}}(x^{(n)})-1)\\varphi_{+}^{(n,t)}\\varphi_{-}^{(n,t)}\\left(1+\\log(2L_{\\operatorname*{max}}\\|W_{\\mathbf{kq}}^{(t)}\\|)+(1+\\beta_{0}/2)\\frac{\\|W_{\\mathbf{kq}}^{(t)}\\|}{\\|W_{\\mathbf{kq}}^{(t)}\\|}\\right)}\\\\ &{\\qquad\\geq([\\Gamma_{\\theta^{(\\ell)}}^{(n)}]_{\\mathrm{In}}(x^{(n)})-1)\\varphi_{+}^{(n,t)}\\varphi_{-}^{(n,t)}\\left(\\frac{\\beta_{0}\\|W_{\\mathbf{kq}}^{(t)}\\|}{2\\|W_{\\mathbf{kq}}^{(t)}\\|}+(1+\\beta_{0}/2)\\frac{\\|W_{\\mathbf{kq}}^{(t)}\\|}{\\|W_{\\mathbf{kq}}^{(t)}\\|}\\right)}\\\\ &{\\qquad=(1+\\beta_{0})([\\Gamma_{\\theta^{(\\ell)}}^{(n)}]_{\\mathrm{In}}(x^{(n)})-1)\\varphi_{+}^{(n,t)}\\varphi_{-}^{(n,t)}\\frac{\\|W_{\\mathbf{kq}}^{(t)}\\|}{\\|W_{\\mathbf{kq}}^{(t)}\\|}}\\\\ &{\\qquad\\geq(1+\\beta_{0})A^{(n,*)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Next weayB,d ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{B_{+}^{(n,\\theta)}=\\displaystyle\\sum_{\\ell\\neq l(n)}[\\mathrm{T}_{\\ell^{(n)}}^{(n)}]_{\\mathrm{In}(x_{\\ell}^{(n)})}\\varphi_{\\ell}^{(n,t)}\\left(\\varphi_{+}^{(n,t)}a_{\\ell}^{(n,\\theta)}-\\displaystyle\\sum_{\\ell^{\\prime}\\in l(n)}\\varphi_{\\ell^{\\prime}}^{(n,t)}a_{\\ell^{\\prime}}^{(n,\\theta)}\\right)\\right.}\\\\ {\\displaystyle B_{-}^{(n,\\theta)}=\\displaystyle\\sum_{\\ell\\neq l(n)}[\\mathrm{T}_{\\theta^{(\\ell)}}^{(n)}]_{\\mathrm{In}(x_{\\ell}^{(n)})}\\varphi_{\\ell}^{(n,t)}\\left(\\varphi_{-}^{(n,t)}a_{\\ell}^{(n,\\theta)}-\\displaystyle\\sum_{\\ell^{\\prime}\\notin l(n)}\\varphi_{\\ell^{\\prime}}^{(n,t)}a_{\\ell^{\\prime}}^{(n,\\theta)}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Due to Proposition 4, we have $B_{-}^{(n,*)}=0$ , and thus ", "page_idx": 24}, {"type": "equation", "text": "$$\nB^{(n,*)}=B_{+}^{(n,*)}\\leq\\sum_{\\ell\\notin l(n)}[\\mathrm{T}_{\\theta^{(t)}}^{(n)}]_{\\mathrm{I_{n}}(x_{\\ell}^{(n)})}\\varphi_{\\ell}^{(n,t)}(-\\varphi_{+}^{(n,t)})\\frac{\\|W_{\\mathrm{kq}}^{(t)}\\|}{\\|W_{\\mathrm{kq}}^{*}\\|}\\leq0.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We then analyze: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}:=(1-\\delta\\log(\\frac{1}{n}))^{n}}\\\\ &{:=\\sum_{k=1}^{n}\\sum_{\\ell=1}^{n}\\gamma_{k}^{i,k}\\log(\\pi^{k})^{n}e^{\\theta_{i}(k)}e^{-\\theta_{i}(k)}e^{\\theta_{i}(k)}e^{-\\theta_{i}(k)}(-1+\\delta\\log\\Big(\\frac{1}{n^{(\\ell)}}e^{\\theta_{i}(k)}-e\\Big)^{k})\\sin\\theta_{i}}\\\\ &{:=\\sum_{k=1}^{n}\\sum_{\\ell=1}^{n}\\gamma_{k}^{i,k}\\log\\Big(\\frac{1}{n^{(\\ell)}}e^{-\\theta_{i}(k)}-\\frac{1}{n^{(\\ell)}}\\Big)^{k}}\\\\ &{:=\\sum_{k=1}^{n}\\mathbb{E}:\\frac{\\prod_{\\ell=1}^{n}\\theta_{i}(k)e^{-\\theta_{i}(k)}}{\\sum_{k=1}^{n}\\theta_{i}(k)}e^{-\\theta_{i}(k)}(-\\frac{1}{n^{(\\ell)}}e^{-\\theta_{i}(k)}-\\frac{1}{n^{(\\ell)}}\\Big)^{k}}\\\\ &{\\geq\\sum_{k=1}^{n}\\sum_{\\ell=1}^{n}\\sum_{\\ell=1}^{n}\\theta_{i}e^{\\theta_{i}(k)}e^{-\\theta_{i}(k)}(k-k_{i})}\\\\ &{\\overset{(a)\\leq}\\sum_{k=1}^{n}\\sum_{\\ell=1}^{n}\\theta_{i}e^{-\\theta_{i}(k)}e^{-\\theta_{i}(k)}e^{-\\theta_{i}(k)}\\Big(-1+\\delta\\log\\Big(\\frac{1}{n^{(\\ell)}}\\Big)\\Big(-2\\pi\\log\\Big(1\\Big)\\Big)}\\\\ &{:=-2\\sum_{k=1}^{n}\\sum_{\\ell=1}^{n}\\theta_{i}e^{\\theta_{i}(k)}e^{-\\theta_{i}(k)}\\cos\\Big(-(1+\\delta\\log\\Big(\\frac{1}{n^{(\\ell)}}\\Big)\\Big)\\sin\\theta_{i}\\Big)\\log\\Big(\\frac{-\\delta\\log\\Big(1\\Big)}{2}\\Big)}\\\\ &{:=-\\sum_{k=1}^{n}\\sum_{\\ell=1}^{n}(1-\\delta\\log\\Big(\\frac{1}{n^{(\\ell)}}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $(a)$ follows from that $b_{*,\\ell}\\leq-(1\\!+\\!\\beta_{0})\\|W_{\\mathrm{kq}}^{(t)}\\|/\\|W_{\\mathrm{kq}}^{*}\\|$ and Equation (11), and $(b)$ is due to the facthate $\\begin{array}{r}{\\sum_{i\\in[|\\mathcal{V}|]}[\\mathrm{T}_{\\theta}^{(n)}]_{i}=1}\\end{array}$ for any $\\theta,n$ and $(c)$ $\\begin{array}{r}{\\beta_{0}/2\\ge\\frac{\\|W_{\\mathbf{kq}}^{*}\\|}{\\|W_{\\mathbf{kq}}^{(t)}\\|}\\log(2L_{\\operatorname*{max}}\\|W_{\\mathbf{kq}}^{(t)}\\|)}\\end{array}$ ", "page_idx": 25}, {"type": "text", "text": "For the term $B_{-}^{(n,t)}$ we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{B_{-}^{(n,t)}=\\displaystyle\\sum_{\\ell\\neq\\ell\\neq1\\ }[\\Gamma_{\\ell\\ell}^{(n)}]_{\\ln(n_{\\ell}^{(n)})}\\varphi_{\\ell}^{(n,t)}\\left(\\varphi_{-}^{(n,t)}\\alpha_{\\ell}^{(n,t)}-\\sum_{\\ell\\neq\\ell}\\varphi_{\\ell\\ell}^{(n,t)}\\alpha_{\\ell}^{(n,t)}\\right)}\\\\ &{=\\displaystyle\\sum_{\\ell\\neq\\ell\\neq1\\ }[\\Gamma_{\\ell\\ell}^{(n)}]_{\\ln(n_{\\ell}^{(n)})}\\varphi_{\\ell}^{(n,t)}\\varphi_{-}^{(n,t)}\\left(\\omega_{\\ell}^{(n,t)}-\\sum_{\\ell\\neq\\ell}\\frac{\\varphi_{\\ell}^{(n,t)}}{\\varphi_{\\ell}^{(n,t)}}\\omega_{\\ell}^{(n,t)}\\right)}\\\\ &{=\\displaystyle\\operatorname*{max}_{\\ell\\neq\\ell\\neq1\\ }\\left\\{\\sum_{\\ell\\neq\\ell}\\left[\\Gamma_{\\ell\\ell}^{(n)}\\right]_{\\ln(n_{\\ell}^{(n)})}\\varphi_{\\ell}^{(n,t)}\\varphi_{-}^{(n,t)}\\left(\\omega_{\\ell}^{(n,t)}-\\sum_{\\ell\\neq\\ell}\\frac{\\varphi_{\\ell}^{(n,t)}}{\\varphi_{\\ell}^{(n,t)}}\\omega_{\\ell}^{(n,t)}\\right)\\right.}\\\\ &{\\qquad\\qquad\\left.\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\left.\\frac{\\partial\\left[n_{\\ell}^{(n)}\\right]_{\\ln(n_{\\ell}^{(n)})}\\varphi_{\\ell}^{(n,t)}}{\\partial\\alpha^{(n,t)}}\\varphi_{-}^{(n,t)}\\left(\\underbrace{a_{n_{\\ell}^{(n,t)}}^{(n,t)}}_{\\omega_{\\ell}^{(n,t)}}\\varphi_{-}^{(n,t)}\\left(\\frac{\\varphi_{\\ell}^{(n,t)}}{\\varphi_{\\ell}^{(n,t)}}\\right)\\left(\\frac{a_{n_{\\ell}^{(n,t)}}^{(n,t)}}{\\varphi_{\\ell}^{(n,t)}}\\right)\\right\\}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\left.\\frac{\\partial\\left[n_{\\ell}^{(n)}\\right]_{\\ln(n_{\\ell}^{(n)})}\\varphi_{\\ell}^{(n,t\n$$", "text_format": "latex", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overset{(a)}{\\geq}\\frac{\\operatorname*{max}}{1\\!-\\!9}\\left\\{\\underset{\\ell\\neq\\ell}{\\sum}\\ [\\Gamma_{\\theta(\\ell)}^{(n)}]_{\\ln(x_{\\ell}^{(n)})}\\varphi_{-}^{(n,t)}\\left(-\\varphi_{\\ell}^{(n,t)}\\mathbb{T}-2\\|W_{\\mathbb{k q}}^{(t)}\\|\\exp(-\\mathbb{T})\\right)\\right\\}}\\\\ &{\\overset{(b)}{\\geq}-L_{\\operatorname*{max}}(1-[\\Gamma_{\\theta(\\ell)}^{(n)}]_{\\ln(x_{\\ell}^{(n)})})\\varphi_{-}^{(n,t)}\\left(1+\\log(2\\|W_{\\mathbb{k q}}^{(t)}\\|)\\right)}\\\\ &{\\overset{\\sum}{\\geq}\\frac{L_{\\operatorname*{max}}\\|W_{\\mathbb{k q}}^{\\ast}\\|}{\\varphi_{+}^{(n,t)}\\|W_{\\mathbb{k q}}^{(t)}\\|}\\left(1+\\log(2\\|W_{\\mathbb{k q}}^{(t)}\\|)\\right)A^{(n,\\ast)}}\\\\ &{\\overset{(c)}{\\geq}\\frac{L_{\\operatorname*{max}}^{2}\\|W_{\\mathbb{k q}}^{\\ast}\\|}{\\|W_{\\mathbb{k q}}^{(t)}\\|}\\left(1+\\log(2\\|W_{\\mathbb{k q}}^{(t)}\\|)\\right)A^{(n,\\ast)}}\\\\ &{\\overset{(c)}{\\geq}\\frac{L_{\\operatorname*{max}}^{2}\\|W_{\\mathbb{k q}}^{\\ast}\\|}{\\|W_{\\mathbb{k q}}^{(t)}\\|}\\left(1+\\log(2\\|W_{\\mathbb{k q}}^{(t)}\\|)\\right)A^{(n,\\ast)}}\\\\ &{\\overset{(d)}{\\geq}\\beta_{0}A^{(n,\\ast)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $(a)$ follows from Equation (11), $(b)$ is optained by choosing $\\Upsilon=\\log(2\\|W_{\\mathrm{kq}}^{(t)}\\|)$ \uff0c $(c)$ follows from Lemma 5, and $(d)$ is due to the fact that $\\begin{array}{r}{\\beta_{0}\\geq\\frac{L_{\\operatorname*{max}}^{2}\\|W_{\\mathbf{kq}}^{*}\\|}{\\|W_{\\mathbf{kq}}^{(t)}\\|}(1+\\log(2\\|W_{\\mathbf{kq}}^{(t)}\\|))}\\end{array}$ ", "page_idx": 26}, {"type": "text", "text": "So far, wehave shown that for f $\\varphi_{-}^{(n,t)}\\geq\\exp(-(1+\\beta_{0}/2)\\|W_{\\mathrm{kq}}^{(t)}\\|/\\|W_{\\mathrm{kq}}^{*}\\|)$ , then ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A^{(n,t)}+B^{(n,t)}=A^{(n,t)}+B_{+}^{(n,t)}+B_{-}^{(n,t)}}\\\\ &{\\phantom{A^{(n,t)}+B^{(n,t)}=A^{(n,t)}+}\\geq(1+\\beta_{0})A^{(n,*)}+\\left((1+\\beta_{0})B^{(n,*)}+\\beta_{0}A^{(n,*)}\\right)+\\beta_{0}A^{(n,*)}}\\\\ &{\\phantom{A^{(n,t)}+B^{(n,t)}=A^{(n,t)}}=(1+3\\beta_{0})A^{(n,*)}+(1+\\beta_{0})B^{(n,*)}}\\\\ &{\\phantom{A^{(n,t)}+B^{(n,t)}=A^{(n,*)}+B^{(n,*)}}\\geq(1+3\\beta_{0})(A^{(n,*)}+B^{(n,*)}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Type 2. Now consider sentence $X^{(n)}$ such that $\\varphi_{-}^{(n,t)}<\\exp(-(1+\\beta_{0}/2)\\|W_{\\mathbf{kq}}^{(t)}\\|/\\|W_{\\mathbf{kq}}^{*}\\|).$ Let $n_{0}$ be the type sample such that $\\varphi_{-}^{(n_{0},t)}\\geq\\exp(-\\|W_{\\mathrm{kq}}^{(t)}\\|/\\|W_{\\mathrm{kq}}^{*}\\|)$ ", "page_idx": 26}, {"type": "text", "text": "Then, we aim to show that ", "page_idx": 26}, {"type": "equation", "text": "$$\nA^{(n,t)}+B^{(n,t)}\\geq\\beta_{0}A^{(n_{0},*)}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Note that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{4^{(n,t)}\\geq\\displaystyle\\sum_{\\ell_{*}\\in l(n)}\\left([\\boldsymbol{\\Gamma}_{\\theta^{(t)}}^{(n)}]_{\\ln(x_{\\ell_{*}}^{(n)})}-1\\right)\\varphi_{\\ell_{*}}^{(n,t)}\\varphi_{-}^{(n,t)}\\left(1+\\log\\frac{L_{\\operatorname*{max}}\\|\\boldsymbol{W}_{\\mathbf{kq}}^{(t)}\\|}{\\varphi_{-}^{(n,t)}}\\right)}\\\\ &{\\qquad\\geq\\left([\\boldsymbol{\\Gamma}_{\\theta^{(t)}}^{(n)}]_{\\ln(x_{\\ell_{*}}^{(n)})}-1\\right)\\exp\\left(-(1+\\beta_{0}/2)\\frac{\\|\\boldsymbol{W}_{\\mathbf{kq}}^{(t)}\\|}{\\|\\boldsymbol{W}_{\\mathbf{kq}}^{*}\\|}\\right)\\left(1+(1+\\beta_{0}/2)\\frac{\\|\\boldsymbol{W}_{\\mathbf{kq}}^{(t)}\\|}{\\|\\boldsymbol{W}_{\\mathbf{kq}}^{*}\\|}+\\log(L_{\\operatorname*{max}}\\|\\boldsymbol{W}_{\\mathbf{kq}}^{(t)})\\right)}\\\\ &{\\qquad\\geq(1+\\beta_{0})([\\boldsymbol{\\Gamma}_{\\theta^{(t)}}^{(n)}]_{\\ln(x_{\\ell_{*}}^{(n)})}-1)\\exp\\left(-(1+\\beta_{0}/2)\\frac{\\|\\boldsymbol{W}_{\\mathbf{kq}}^{(t)}\\|}{\\|\\boldsymbol{W}_{\\mathbf{kq}}^{*}\\|}\\right)\\frac{\\|\\boldsymbol{W}_{\\mathbf{kq}}^{(t)}\\|}{\\|\\boldsymbol{W}_{\\mathbf{kq}}^{*}\\|}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{B^{(n,t)}=\\displaystyle\\sum_{\\ell\\notin l(n)}[\\mathrm{T}_{\\theta^{(t)}}^{(n)}]_{\\mathrm{In}(x_{\\ell}^{(n)})}\\varphi_{\\ell}^{(n,t)}\\left(a_{\\ell}^{(n,t)}-\\displaystyle\\sum_{\\ell^{\\prime}}\\varphi_{\\ell^{\\prime}}^{(n,t)}a_{\\ell^{\\prime}}^{(n,t)}\\right)}\\\\ &{\\phantom{A^{(n,t)}=\\displaystyle}\\geq-2(1-[\\mathrm{T}_{\\theta^{(t)}}^{(n)}]_{\\mathrm{In}(x_{\\ell_{*}}^{(n)})})\\exp\\left(-(1+\\beta_{0}/2)\\frac{\\|W_{\\mathrm{kq}}^{(t)}\\|}{\\|W_{\\mathrm{kq}}^{*}\\|}\\right)\\|W_{\\mathrm{kq}}^{(t)}\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Since ", "text_level": 1, "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A^{(n_{0},*)}\\leq([\\mathrm{T}_{\\theta^{(t)}}^{(n_{0})}]_{\\mathrm{In}(x_{\\ell_{*}}^{(n_{0})})}-1)\\varphi_{+}^{(n_{0},t)}\\varphi_{-}^{(n_{0},t)}\\frac{\\|W_{\\mathrm{kq}}^{(t)}\\|}{\\|W_{\\mathrm{kq}}^{*}\\|}}\\\\ &{\\qquad\\leq([\\mathrm{T}_{\\theta^{(t)}}^{(n_{0})}]_{\\mathrm{In}(x_{\\ell_{*}}^{(n_{0})})}-1)\\varphi_{+}^{(n_{0},t)}\\exp\\left(-\\frac{\\|W_{\\mathrm{kq}}^{(t)}\\|}{\\|W_{\\mathrm{kq}}^{*}\\|}\\right)\\frac{\\|W_{\\mathrm{kq}}^{(t)}\\|}{\\|W_{\\mathrm{kq}}^{*}\\|}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "$[\\mathrm{T}_{\\theta^{(t)}}^{(n_{0})}]_{\\mathrm{In}(x_{\\ell_{*}}^{(n_{0})})}<[\\mathrm{T}_{\\theta^{(t)}}^{(n)}]_{\\mathrm{In}(x_{\\ell_{*}}^{(n)})}$ dueto $\\varphi_{+}^{(n_{0},t)}<\\varphi_{+}^{(n,t)}$ (n,t) . Thus, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A^{(n,t)}+B^{(n,t)}}\\\\ &{\\geq\\exp\\left(-\\beta_{0}/2\\frac{\\|W_{\\mathrm{kq}}^{(t)}\\|}{\\|W_{\\mathrm{kq}}^{*}\\|}\\right)\\left(1+\\beta_{0}+2\\|W_{\\mathrm{kq^{*}}}\\|\\right)\\frac{A^{(n_{0},*)}}{\\varphi_{+}^{(n_{0},t)}}}\\\\ &{\\stackrel{(a)}{\\geq}\\beta_{0}A^{(n_{0},*)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $(a)$ is due to that $\\begin{array}{r}{\\beta_{0}\\geq\\frac{2\\|W_{\\mathbf{kq}}^{*}\\|(1+2\\|W_{\\mathbf{kq}}^{*}\\|)}{\\|W_{\\mathbf{kq}}^{(t)}\\|}\\log(1+\\frac{\\|W_{\\mathbf{kq}}^{(t)}\\|}{2\\|W_{\\mathbf{kq}}^{*}\\|})}\\end{array}$ \uff0c $\\|W_{\\mathrm{kq}}^{(t)}\\|\\geq2(e-1)\\|W_{\\mathrm{kq}}^{*}\\|$ and ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\beta_{0}\\ge\\frac{2\\|W_{\\mathrm{kq}}^{*}\\|}{\\|W_{\\mathrm{kq}}^{(t)}\\|}\\log\\frac{1+\\beta_{0}+2\\|W_{\\mathrm{kq}}^{*}\\|}{\\beta_{0}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "In summary, we have that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{n}\\frac{\\pi^{(n)}(A^{(n,t)}+B^{(n,t)})}{\\pi}}\\\\ &{=\\displaystyle\\sum_{n\\neq\\pm\\infty}\\pi^{(n)}(A^{(n,t)}+B^{(n,t)})+\\displaystyle\\sum_{n\\neq\\pm\\infty}\\pi^{(n)}(A^{(n,t)}+B^{(n,t)})}\\\\ &{\\ge\\displaystyle\\sum_{n\\neq\\pm\\infty}\\beta_{0}A^{(n,t)}+\\displaystyle\\sum_{n\\neq\\pm\\infty}\\frac{\\pi^{(n)}(1+3\\beta_{0})A^{(n,t)}}{n}+(1+\\beta_{1})B^{(n,\\star)})}\\\\ &{\\ge\\displaystyle\\sum_{n\\neq\\pm\\infty}\\frac{\\pi^{(n)}\\beta_{0}(A^{(n,n)}+B^{(n,\\star)})+(1+3\\beta_{0})\\sum_{n\\neq\\pm\\infty}\\pi^{(n)}(A^{(n,\\star)}+B^{(n,\\star)})}{n\\mathrm{supper}}}\\\\ &{\\ge(1+(N+3)\\beta_{0})\\displaystyle\\sum_{n\\neq\\pm\\infty}\\frac{\\pi^{(n)}(A^{(n,\\star)}+B^{(n,\\star)})}{n\\mathrm{supper}}}\\\\ &{\\ge(1+\\alpha_{1})\\displaystyle\\sum_{n\\neq\\pm\\infty}\\frac{\\pi^{(n)}(A^{(n,\\star)}+B^{(n,\\star)})+(1+\\alpha_{l})}{n\\mathrm{supper}}\\sum_{n\\neq\\pm\\infty}\\pi^{(n)}(A^{(n,\\star)}+B^{(n,\\star)})}\\\\ &{=(1+\\alpha_{1})\\displaystyle\\sum_{n\\neq\\pm\\infty}\\pi^{(n)}(A^{(n,\\star)}+B^{(n,\\star)})+(1+\\alpha_{l})\\displaystyle\\sum_{n\\neq\\pm\\infty}\\pi^{(n)}(A^{(n,\\star)}+B^{(n,\\star)})}\\\\ &{=(1+\\alpha_{1})\\displaystyle\\sum_{n\\neq\\pm\\infty}\\pi^{(n)}(A^{(n,\\star)}+B^{(n,\\star)}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $\\alpha_{t}\\geq(N+3)\\beta_{0}$ . The proof is finished. ", "page_idx": 27}, {"type": "text", "text": "Now, we are ready to prove Theorem 1. ", "page_idx": 27}, {"type": "text", "text": "C.5 Proof of Theorem 1 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Proof of Theorem $^{\\,l}$ ", "page_idx": 27}, {"type": "text", "text": "Realthat $\\begin{array}{r}{\\alpha_{t}=\\frac{4N L_{\\mathrm{max}}^{2}||W_{\\mathbf{kq}}^{*}||^{2}}{||W_{\\mathbf{kq}}^{(t)}||}\\left(1+\\log\\left(2L_{\\mathrm{max}}||W_{\\mathbf{kq}}^{(t)}||\\right)\\right)}\\end{array}$ By Lemma 7, we have $\\left<W_{\\mathrm{kq}}^{(t+1)}-W_{\\mathrm{kq}}^{(t)},\\frac{W_{\\mathrm{kq}}^{*}}{\\|W_{\\mathrm{kq}}^{*}\\|}\\right>$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=-\\eta\\left<\\nabla_{W_{n}}G(\\theta^{(t)})\\cdot\\frac{W_{n}}{\\|W_{n}\\|}\\right>}\\\\ &{\\geq-\\frac{\\eta}{1+\\alpha_{t}}\\left<\\nabla_{W_{n}}G(\\theta^{(t)})\\cdot\\frac{W_{n}^{(t)}}{\\|W_{n}^{(t)}\\|}\\right>}\\\\ &{=\\frac{1}{1+\\alpha_{t}}\\left<W_{n}^{(t+1)}-W_{W_{n}^{(t)}}^{(t)}\\frac{W_{n}^{(t)}}{\\|W_{n}^{(t)}\\|}\\right>}\\\\ &{=\\frac{1}{2}\\frac{1}{\\|W_{n}^{(t)}\\|}\\left(\\|W_{n}^{(t+1)}\\|^{2}-\\|W_{n}^{(t+1)}-W_{W_{n}}^{(t)}\\|^{2}-\\|W_{n}^{(t)}\\|^{2}\\right)-\\frac{\\alpha_{t}}{1+\\alpha_{t}}\\left<W_{W_{n}^{(t+1)}}^{(t+1)}-W_{W_{n}^{(t)}}^{(t)}\\frac{W_{n}^{(t)}}{\\|W_{n}^{(t)}\\|}\\right>}\\\\ &{=\\frac{\\|W_{n}^{(t)}\\|^{2}-\\|W_{n}^{(t)}\\|^{2}}{2\\|W_{n}^{(t)}\\|}-\\frac{\\eta^{2}}{2\\|W_{n}^{(t)}\\|}+\\frac{\\eta\\alpha_{t}}{1+\\alpha_{t}}\\left<\\frac{\\nabla W_{n}\\zeta G^{(t)}}{\\|W_{n}\\zeta G^{(t)}\\|},\\frac{W_{n}^{(t)}}{\\|W_{n}^{(t)}\\|}\\right>}\\\\ &{\\geq\\|W_{n}^{(t+1)}\\|-\\|W_{n}^{(t)}\\|-\\frac{\\eta^{2}}{2\\|W_{n}^{(t)}\\|}-\\frac{\\eta\\alpha_{t}}{1+\\alpha_{t}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Let $t_{0}=\\lceil\\frac{8L_{\\operatorname*{max}}\\|W_{\\mathbf{kq}}^{*}\\|^{2}}{\\eta}\\rceil$ be defnediLemma7 Summing over $t$ from $t_{0}$ we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left\\langle W_{\\mathbf{kq}}^{(t)}-W_{\\mathbf{kq}}^{(t_{0})},\\frac{W_{\\mathbf{kq}}^{*}}{\\|W_{\\mathbf{kq}}^{*}\\|}\\right\\rangle\\geq\\|W_{\\mathbf{kq}}^{(t)}\\|-\\|W_{\\mathbf{kq}}^{(t_{0})}\\|-\\sum_{t^{\\prime}=t_{0}}^{t-1}\\frac{\\eta^{2}}{2\\|W_{\\mathbf{kq}}^{(t^{\\prime})}\\|}-\\sum_{t^{\\prime}=t_{0}}^{t-1}\\frac{\\eta\\alpha_{t^{\\prime}}}{1+\\alpha_{t^{\\prime}}}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "By Lemma 6, we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{t^{\\prime}=t_{0}}^{t-1}\\frac{1}{\\|W_{\\mathrm{kq}}^{(t^{\\prime})}\\|}\\le\\sum_{t^{\\prime}=t_{0}}^{t-1}\\frac{2L_{\\operatorname*{max}}\\|W_{\\mathrm{kq}}^{*}\\|/\\eta}{t^{\\prime}}}}\\\\ &{}&{\\le\\frac{2L_{\\operatorname*{max}}\\|W_{\\mathrm{kq}}^{*}\\|}{\\eta}\\log t.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Furthermore, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{n=0}^{-1}\\frac{\\alpha_{n^{\\prime}}}{1+\\alpha_{n}}}&{\\le\\displaystyle\\sum_{r=\\infty}^{t-1}\\alpha_{n^{\\prime}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\displaystyle\\sum_{t=\\mu_{n}}^{t-1}\\frac{4N T_{n n}^{*}\\|W_{k q}^{*}\\|^{2}}{\\|W_{k q}^{*}\\|}\\left(1+\\log\\Big(2T_{n n}\\|W_{k q}^{(r)}\\|\\Big)\\right)}\\\\ &{\\qquad\\qquad\\qquad\\displaystyle=\\sum_{r=\\mu_{n}}^{t-1}\\frac{4N T_{n n}^{*}\\|W_{k q}^{*}\\|^{2}}{\\|W_{k q}^{*}\\|}\\left(1+\\log(2L_{n n})\\right)+\\displaystyle\\sum_{r=\\mu_{n}}^{t-1}\\frac{4N T_{n n}^{*}\\|W_{k q}^{*}\\|^{2}}{\\|W_{k q}^{*}\\|}\\log\\|W_{k q}^{(r)}\\|}\\\\ &{\\qquad\\qquad\\displaystyle=\\sum_{r=\\mu_{n}}^{t-1}\\frac{8N T_{n n}^{*}\\|W_{k q}^{*}\\|^{3}/\\eta}{t^{\\prime}}\\log(2c_{I n n})+\\displaystyle\\sum_{r=\\mu_{n}}^{t-1}\\frac{8N T_{n n}^{*}\\|W_{k q}^{*}\\|^{3}/\\eta}{t^{\\prime}}\\log\\frac{t^{\\prime}}{2I_{n n-1}|W_{k q}^{*}\\|}}\\\\ &{\\qquad\\qquad\\displaystyle=\\sum_{t=\\mu_{n}}^{t-1}\\frac{8N T_{n n}^{*}\\|W_{k q}^{*}\\|^{3}/\\eta}{t^{\\prime}}\\log(e^{\\eta}/\\|W_{k q}^{*}\\|)+\\displaystyle\\sum_{r=\\mu_{n}}^{t-1}\\frac{8N T_{n n}^{*}\\|W_{k q}^{*}\\|^{3}/\\eta}{t^{\\prime}}\\log(t^{\\prime})}\\\\ &{\\qquad\\qquad\\displaystyle\\overset{(a)}{\\le}\\frac{8N T_{n n}^{3}\\|W_{k q}^{*}\\|^{3}}{\\|W_{k q}^{*}\\|}\\log^{2}t,}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $(a)$ follows from $\\eta\\leq\\|W_{\\mathrm{kq}}^{*}\\|/e$ ", "page_idx": 28}, {"type": "text", "text": "Therefore, we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left\\langle W_{\\mathbf{kq}}^{(t)}-W_{\\mathbf{kq}}^{(t_{0})},\\frac{W_{\\mathbf{kq}}^{*}}{\\|W_{\\mathbf{kq}}^{*}\\|}\\right\\rangle\\geq\\|W_{\\mathbf{kq}}^{(t)}\\|-\\|W_{\\mathbf{kq}}^{(t_{0})}\\|-\\sum_{t^{\\prime}=t_{0}}^{t-1}\\frac{\\eta^{2}}{2\\|W_{\\mathbf{kq}}^{(t^{\\prime})}\\|}-\\sum_{t^{\\prime}=t_{0}}^{t-1}\\frac{\\eta\\alpha_{t^{\\prime}}}{1+\\alpha_{t^{\\prime}}}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "equation", "text": "$$\n\\geq\\|W_{\\mathrm{kq}}^{(t)}\\|-\\|W_{\\mathrm{kq}}^{(t_{0})}\\|-L_{\\operatorname*{max}}\\|W_{\\mathrm{kq}}^{*}\\|\\log t-8N L_{\\operatorname*{max}}^{3}\\|W_{\\mathrm{kq}}^{*}\\|^{3}\\log^{2}t.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\langle\\frac{W_{\\mathrm{kq}}^{(t)}}{\\|W_{\\mathrm{kq}}^{(t)}\\|},\\frac{W_{\\mathrm{kq}}^{*}}{\\|W_{\\mathrm{kq}}^{*}\\|}\\right\\rangle\\geq1-\\frac{2\\|W_{\\mathrm{kq}}^{(t_{0})}\\|+L_{\\operatorname*{max}}\\|W_{\\mathrm{kq}}^{*}\\|\\log t+8N L_{\\operatorname*{max}}^{3}\\|W_{\\mathrm{kq}}^{*}\\|^{3}\\log^{2}t}{\\|W_{\\mathrm{kq}}^{(t)}\\|}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\geq1-\\frac{54N L_{\\operatorname*{max}}^{4}\\|W_{\\mathrm{kq}}^{*}\\|^{4}\\log^{2}t}{t\\eta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "The proof is finished. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "C.6Proof of Theorem 2 ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Proof of Theorem 2. ", "page_idx": 29}, {"type": "text", "text": "Recall that $T\\geq384\\|W_{\\mathrm{ov}}^{*}\\|^{5}\\log(2|\\mathcal{V}|)\\log T/\\eta_{0}$ and $\\Delta=T\\eta_{0}/(4\\|W_{\\mathrm{ov}}^{*}\\|^{2})$ due to Corollary 1 and ", "page_idx": 29}, {"type": "equation", "text": "$$\n(e_{\\mathrm{In}(x)}-e_{v})^{\\top}W_{\\mathrm{ov}}^{(T)}x=\\frac{T\\eta_{0}}{4\\|W_{\\mathrm{ov}}^{*}\\|^{2}}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Note that for all $\\ell_{*}\\in l(n)$ and $\\ell\\notin l(n)$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(x_{\\ell}^{(n)}-x_{\\ell}^{(n)})^{\\top}W_{\\mathbb{k}\\mathbb{q}}^{(t)}X_{-1}^{(n)}}\\\\ &{=\\frac{\\|W_{\\mathbb{k}\\mathbb{q}}^{(t)}\\|}{\\|W_{\\mathbb{k}\\mathbb{q}}^{(t)}\\|}(x_{\\ell}^{(n)}-x_{\\ell}^{(n)})^{\\top}W_{\\mathbb{k}\\mathbb{q}}^{*}X_{-1}^{(n)}+(x_{\\ell}^{(n)}-x_{\\ell}^{(n)})^{\\top}\\left(W_{\\mathbb{k}\\mathbb{q}}^{(t)}-\\frac{\\|W_{\\mathbb{k}\\mathbb{q}}^{(t)}\\|}{\\|W_{\\mathbb{k}\\mathbb{q}}^{*}\\|}W_{\\mathbb{k}\\mathbb{q}^{*}}\\right)X_{-1}^{(n)}}\\\\ &{\\le-\\frac{\\|W_{\\mathbb{k}\\mathbb{q}}^{(t)}\\|}{\\|W_{\\mathbb{k}\\mathbb{q}}^{(t)}\\|}+2\\|W_{\\mathbb{k}\\mathbb{q}}^{(t)}\\|\\left\\|\\frac{W_{\\mathbb{k}\\mathbb{q}}^{(t)}}{\\|W_{\\mathbb{k}\\mathbb{q}}^{(t)}\\|}-\\frac{W_{\\mathbb{k}\\mathbb{q}}^{*}}{\\|W_{\\mathbb{k}\\mathbb{q}}^{*}\\|}\\right\\|}\\\\ &{\\le-\\frac{t\\eta}{2L_{\\operatorname*{max}}\\|W_{\\mathbb{k}\\mathbb{q}}^{*}\\|^{2}}+\\frac{\\sqrt{2}t\\eta}{L_{\\operatorname*{max}}\\|W_{\\mathbb{k}\\mathbb{q}}^{*}\\|}\\sqrt{\\frac{54N L_{\\operatorname*{max}}^{4}\\|W_{\\mathbb{k}\\mathbb{q}}^{*}\\|^{4}\\log^{2}t}{t\\eta}}}\\\\ &{\\overset{(e)}{\\le}-\\frac{t\\eta}{4L_{\\operatorname*{max}}\\|W_{\\mathbb{k}\\mathbb{q}}^{*}\\|^{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $(a)$ follows from $t\\geq1696N L_{\\mathrm{max}}^{4}||W_{\\mathbf{kq}}^{*}||^{6}\\log^{2}t/\\eta$ ", "page_idx": 29}, {"type": "text", "text": "Therefore, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{\\ell_{*}\\in\\ell(n)}{\\sum}\\varphi_{\\ell_{*}}^{(n,t)}=\\frac{|l(n)|}{|l(n)|+\\sum_{\\ell^{\\prime}\\notin l(n)}\\exp\\Big((x_{\\ell}^{(n)}-x_{\\ell_{*}}^{(n)})^{\\top}W_{\\mathrm{kq}}^{(t)}X_{-1}^{(n)}\\Big)}}\\\\ &{\\phantom{\\sum_{\\ell_{*}}^{\\ell_{*}}}\\ge\\frac{|l(n)|}{|l(n)|+(L^{(n)}-|l(n)|)\\exp\\Big({-\\frac{t\\eta}{4L_{\\mathrm{max}}\\|W_{\\mathrm{kq}}^{*}\\|^{2}}}\\Big)}}\\\\ &{\\ge\\frac{1}{1+L_{\\operatorname*{max}}\\exp\\Big({-\\frac{t\\eta}{4L_{\\operatorname*{max}}\\|W_{\\mathrm{kq}}^{*}\\|^{2}}}\\Big)}}\\\\ &{\\ge\\frac{1}{1+\\epsilon},}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where the last inequality follows from $\\begin{array}{r}{t\\geq\\frac{4L_{\\operatorname*{max}}\\|W_{\\mathbf{kq}}^{*}\\|}{\\eta}\\log\\frac{L_{\\operatorname*{max}}}{\\epsilon}}\\end{array}$ ", "page_idx": 29}, {"type": "text", "text": "Hence, the loss on the sentence $X^{(n)}$ satisfies that ", "page_idx": 29}, {"type": "equation", "text": "$$\n-\\log\\left(e_{\\operatorname{In}(X^{(n)})}^{\\top}\\mathrm{T}_{\\theta^{(t)}}(X^{(n)})\\right)\n$$", "text_format": "latex", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=-\\log\\frac{\\exp\\Big(e_{1}^{\\alpha}(x_{\\delta}(x))\\log^{(2)}{\\upsilon_{c}(x^{\\alpha})}\\Big)^{\\alpha}}{\\sum_{x\\in\\mathcal{V}_{\\delta}}\\exp\\Big(\\exp^{(\\alpha)}{\\log^{(2)}{\\sum_{x}e_{x^{\\alpha}}^{(\\alpha)}{\\psi_{c}^{(\\alpha,t)}}}}\\Big)}}\\\\ &{=-\\log\\frac{1}{1+\\sum_{x\\in\\mathcal{V}_{\\delta}(x,x^{\\alpha})}\\exp\\Big(\\left(e_{c}-e_{\\mathrm{fin}(x^{\\alpha})}\\right)^{\\gamma}\\mathbb{W}_{\\infty}^{(\\gamma)}\\sum_{x^{\\alpha}}e_{x^{\\alpha}}^{(\\alpha)}\\psi_{c}^{(\\alpha,t)}\\Big)}}\\\\ &{=\\log\\left(1+\\underbrace{\\sum_{x\\in\\mathcal{V}_{\\delta}(x^{\\alpha})}\\exp\\left(\\left(e_{c}-e_{\\mathrm{fin}(x^{\\alpha})}\\right)^{\\gamma}\\mathbb{W}_{\\infty}^{(\\gamma)}\\sum_{x^{\\alpha}}e_{x^{\\alpha}}^{(\\alpha)}\\psi_{c}^{(\\alpha,t)}\\right)}_{e_{c}e_{d}^{(\\alpha)}(x^{\\alpha})}\\right)}\\\\ &{=\\log\\left(1+\\underbrace{\\sum_{x\\in\\mathcal{V}_{\\delta}(x^{\\alpha})}\\exp\\left(-\\Delta\\sum_{x^{\\alpha}}\\psi_{c}^{(\\alpha,t)}+\\Delta\\sum_{y}e_{y^{\\alpha}}^{(\\alpha,t)}\\right)}_{e_{c}e_{d}^{(\\alpha,t)}(x^{\\alpha})}\\right)}\\\\ &{\\leq\\left\\|{\\mathcal{V}}\\right\\|\\exp\\left(-\\Delta(2e_{c}^{(\\alpha,t)}-1)\\right)}\\\\ &{\\leq\\left\\|{\\mathcal{V}}\\right\\|\\exp\\left(-\\Delta+\\frac{2\\Delta L\\alpha_{\\operatorname*{max}}}{L_{\\operatorname*{max}}+\\exp\\left(\\frac{4\\Delta L\\alpha_{\\operatorname*{max}}}{L_{\\operatorname*{max}}}\\right)^{\\alpha}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Thus, the average loss has upper bound, which is ", "page_idx": 30}, {"type": "equation", "text": "$$\n|\\mathcal{V}|\\exp\\left(-\\Delta+\\frac{2\\Delta L_{\\operatorname*{max}}}{L_{\\operatorname*{max}}+\\exp(C_{1}t)}\\right),\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "for $C_{1}=\\eta/(4L_{\\mathrm{max}}\\|W_{\\mathrm{kq}}^{*}\\|^{2})$ and $\\Delta=C_{0}T$ for $C_{0}=\\eta_{0}/(4\\|W_{\\mathrm{ov}}^{*}\\|^{2})$ ", "page_idx": 30}, {"type": "text", "text": "D  Proof of Proposition 2 and Theorem 3 ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "D.1Proof of Proposition 2 ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Proposition 4 (Restatement of Proposition 2) Under Asumption 2,if $W_{\\mathrm{kq}}^{*}$ satisfies Equation (3), i.e., ", "page_idx": 30}, {"type": "equation", "text": "$$\nW_{\\mathrm{kq}}^{*}=\\arg\\operatorname*{min}\\|W\\|,\\quad s.t.\\quad(x_{\\ell_{*}}^{(n)}-x_{\\ell}^{(n)})^{\\top}W x\\geq1,\\quad\\forall\\ell\\notin l(n),\\forall n.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "In addition, for each query $x^{q}$ ,if there are $k$ optimal tokens under a $x^{q}$ -partial order, m nonoptimal tokens under $x^{q}$ -partial order, then, for any optimal token $x_{*}$ ,non-optimal token $x$ ,and non-comparabletoken $x_{0}$ , we have ", "page_idx": 30}, {"type": "equation", "text": "$$\nx_{*}^{\\top}W_{\\mathrm{kq}}^{*}x^{q}=\\frac{m}{k+m},\\quad x^{\\top}W_{\\mathrm{kq}}^{*}x^{q}=-\\frac{k}{k+m},\\quad x_{0}^{\\top}W_{\\mathrm{kq}}^{*}x^{q}=0.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "A direct result is that ", "page_idx": 30}, {"type": "equation", "text": "$$\n(x_{\\ell}^{(n)}-x_{\\ell^{\\prime}}^{(n)})^{\\top}W_{\\mathrm{kq}}^{*}X_{-1}^{(n)}=0,\\quad\\forall\\ell,\\ell^{\\prime}\\notin l(n).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof.Let $U\\in\\mathbb{R}^{d}$ be the rotation matrix such that $U x=e_{\\mathrm{I}(x)}$ .Because $U$ preserves Frobenius norm, the optimization problem in Equation (3) can be written as ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\tilde{W}^{*}=\\arg\\operatorname*{min}\\|W\\|,\\quad\\mathrm{s.t.}(e_{\\mathrm{I}(x_{\\ell_{*}}^{(n)})}-e_{\\mathrm{I}(x_{\\ell}^{(n)})})^{\\top}W e_{\\mathrm{I}(X_{-1}^{(n)})}\\geq1.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Notably $\\tilde{W}^{*}=U W_{\\mathrm{kq}}^{*}U^{\\top}$ ", "page_idx": 30}, {"type": "text", "text": "Note that $\\{e_{\\mathrm{I}(X_{-1}^{(n)})}\\}n$ forms a standard basis. It suffces to minimize the norm of each column of $W$ subject to the constraint $(e_{\\mathrm{I}(x_{\\ell_{*}}^{(n)})}-e_{\\mathrm{I}(x_{\\ell}^{(n)})})^{\\top}W e_{\\mathrm{I}(X_{-1}^{(n)})}\\geq1.$ ", "page_idx": 30}, {"type": "text", "text": "Let us consider any column $c$ of $W$ , denoted as $[w_{1},\\ldots,w_{d}]^{\\top}$ . Without loss of generality, we assume that, for all $X^{(n)}$ with $\\operatorname{I}(X_{-1}^{(n)})=c$ , the set of indices of the optimal tokens of those samples are ", "page_idx": 30}, {"type": "text", "text": "$\\{1,\\ldots,k\\}$ , and the set of indices of the non-optimal tokens are $\\{k+1,\\ldots,k+m\\}$ . Then, the optimization problem Equation (14) reduces to the following problem ", "page_idx": 31}, {"type": "equation", "text": "$$\nw_{i}-w_{j}\\geq1,\\quad\\forall i\\leq k,j\\in A_{i}\\subset\\{k+1,\\ldots,k+m\\},\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $A_{i}$ is the set of indices of the non-optimal tokens in some samples whose optimal token ha index $i$ ", "page_idx": 31}, {"type": "text", "text": "In other words, each column of the solution of Equation (14) is the solution of Equation (15). Note that Equation (15) is a convex problem with linear constraints. The Lagrangian function is ", "page_idx": 31}, {"type": "equation", "text": "$$\nL(\\lambda)=\\sum_{i=1}^{k+m}w_{i}^{2}+2\\sum_{i=1}^{m}\\sum_{j\\in A_{i}}\\lambda_{i j}(1-w_{i}+w_{j}),\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where we directly set $w_{j}=0$ for all $j\\in\\{k+m+1,\\ldots,d\\}$ . That is, non-comparable tokens have value O. ", "page_idx": 31}, {"type": "text", "text": "By KKT-condition, we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{{w_{i}=\\displaystyle\\sum_{j\\in A_{i}}\\lambda_{i j},}}&{{\\forall i\\le k}}\\\\ {{w_{j}=-\\displaystyle\\sum_{i=1}^{k}\\lambda_{i j}\\mathbb{1}\\{j\\in A_{i}\\},\\ \\ \\forall k+1\\le j\\le k+m}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Thus, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\textrm{n}w_{1}^{2}+\\ldots+w_{d}^{2}}\\\\ &{\\quad=\\displaystyle\\operatorname*{max}_{\\lambda}\\left\\{-\\sum_{i=1}^{k}\\left(\\sum_{j\\in A_{i}}\\lambda_{i j}\\right)^{2}-\\sum_{j=k+1}^{k+m}\\left(\\sum_{i=1}^{k}\\lambda_{i j}\\mathbb{1}\\{j\\in A_{i}\\}\\right)^{2}+2\\sum_{i=1}^{m}\\sum_{j\\in A_{i}}\\lambda_{i j}\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Let ", "page_idx": 31}, {"type": "equation", "text": "$$\nL^{*}(\\lambda)=-\\sum_{i=1}^{k}\\left(\\sum_{j\\in A_{i}}\\lambda_{i j}\\right)^{2}-\\sum_{j=k+1}^{k+m}\\left(\\sum_{i=1}^{k}\\lambda_{i j}\\mathbb{1}\\{j\\in A_{i}\\}\\right)^{2}+2\\sum_{i=1}^{m}\\sum_{j\\in A_{i}}\\lambda_{i j},\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $\\lambda\\geq0$ . The maximum of $L^{*}$ is achieved when $\\nabla_{\\lambda}L^{*}=0$ . This implies that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\sum_{j\\in A_{i_{0}}}{\\lambda_{i_{0}j}}+\\sum_{i=1}^{k}{\\lambda_{i j}\\mathbb{1}\\{j_{0}\\in A_{i}\\}}=1,\\quad\\forall1\\leq i_{0}\\leq k<j_{0}\\leq k+m.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Hence,we have $w_{i}-w_{j}=1$ for all $1\\leq i\\leq k<j\\leq k+m$ , which means the optimum of the original problem is achieved on the boundary. Therefore, we reduce the original problem to ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x}k(x+1)^{2}+m x^{2},\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $x=w_{k+1}=...=w_{k+m}$ Hence, the optimal solution is $w_{1}=.\\;.\\;.=w_{k}=m/(m+k)$ , and $w_{k+1}=.\\ldots=w_{k+m}=-k/(k+m)$ ", "page_idx": 31}, {"type": "text", "text": "Therefore, the solution of Equation (15) satisfies that the \u201coptimal values\"\u201d are the same and the \"non-optimal values\"\u2019 are the same as well. This fact proves that ", "page_idx": 31}, {"type": "equation", "text": "$$\n(x_{\\ell}^{(n)}-x_{\\ell^{\\prime}}^{(n)})W_{\\mathrm{kq}}^{*}X_{-1}^{(n)}=0,\\forall\\ell,\\ell^{\\prime}\\notin l(n).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "And moreover, if there are $k$ optimal tokens under a $x^{q}$ -partial order, $m$ non-optimal tokens under $x^{q}$ -partial order, then, for any optimal token $x_{*}$ and non-optimal token $x$ , we have ", "page_idx": 31}, {"type": "equation", "text": "$$\nx_{*}^{\\top}W_{\\mathrm{kq}}^{*}x^{q}=\\frac{m}{k+m},\\quad x^{\\top}W_{\\mathrm{kq}}^{*}x^{q}=-\\frac{k}{k+m}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "D.2 Proof of Theorem 3 ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Proof. The proof follows similar logic to Theorem 2. By Equation (13), we have for any $x,x^{\\prime}\\in\\mathcal{V}$ ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(x-x^{\\prime})^{\\top}W_{\\mathrm{kq}}^{(t)}x^{q}}\\\\ &{\\phantom{(x-x^{\\prime})^{\\top}}\\ge\\frac{\\|W_{\\mathrm{kq}}^{(t)}\\|}{\\|W_{\\mathrm{kq}}^{*}\\|}(x-x^{\\prime})^{\\top}W_{\\mathrm{kq}}^{*}x^{q}-\\frac{\\sqrt{2}t\\eta}{L_{\\operatorname*{max}}\\|W_{\\mathrm{kq}}^{*}\\|}\\sqrt{\\frac{54N L_{\\operatorname*{max}}^{4}\\|W_{\\mathrm{kq}}^{*}\\|^{4}\\log^{2}t}{t\\eta}}}\\\\ &{\\phantom{(x-x^{\\prime})^{\\top}}\\ge\\frac{t\\eta}{2L_{\\operatorname*{max}}\\|W_{\\mathrm{kq}}^{*}\\|^{2}}(x-x^{\\prime})^{\\top}W_{\\mathrm{kq}}^{*}x^{q}-\\sqrt{108t\\eta N L_{\\operatorname*{max}}^{2}\\|W_{\\mathrm{kq}}^{*}\\|^{2}\\log^{2}t},}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $(a)$ follows from Lemma 6. The first part of Theorem 3 follows from Theorem 3. ", "page_idx": 32}, {"type": "text", "text": "For the second part of Theorem 3, Let $X=[x_{1},\\ldots,x_{L}]$ such that for $\\ell_{0}\\in l_{0}\\subset\\{1,\\ldots,L\\}$ $x_{\\ell_{0}}=x$ is a non-comparable token, and other tokens are non-optimal under the $x_{L}$ -partial order. ", "page_idx": 32}, {"type": "text", "text": "Let $\\varphi\\ell\\propto\\exp(x_{\\ell}W_{\\mathrm{kq}}^{(t)}x_{L})$ for sficietly large $t=\\Omega(\\log(1/\\epsilon))$ such that $\\textstyle\\sum_{\\ell_{0}\\in l_{0}}\\varphi_{\\ell_{0}}\\geq1-\\epsilon$ Then, we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{e_{\\mathrm{In}(x)}^{\\top}\\mathrm{T}_{\\theta^{(i)}}(X)=\\frac{\\exp\\big(e_{\\mathrm{In}(x)}^{\\top}W_{\\mathrm{ov}}^{(T)}\\big)_{\\ell}\\kappa_{\\ell}\\varphi_{\\ell}\\big)}{\\sum_{v\\leq|v|}\\exp\\big(e_{v}^{\\top}W_{\\mathrm{ov}}^{(T)}\\big)_{\\ell}\\kappa_{\\ell}\\varphi_{\\ell}}}\\\\ &{\\phantom{e_{\\mathrm{In}(x)}^{\\top}\\mathrm{T}_{\\theta^{(i)}}(X)}=\\frac{1}{1+\\sum_{v\\not\\in\\mathrm{In}(x)}\\exp\\big((e_{v}-e_{\\mathrm{In}(x)})^{\\top}W_{\\mathrm{ov}}^{(T)}\\big)_{\\ell}\\kappa_{\\ell}\\varphi_{\\ell}\\big)}}\\\\ &{\\phantom{e_{\\mathrm{In}(x)}^{\\top}\\mathrm{T}_{\\theta^{(i)}}(X)}=\\frac{1}{1+\\sum_{v\\not\\in\\mathrm{In}(x)}\\exp\\big(-\\Delta\\sum_{\\ell_{0}\\in I_{0}}\\varphi_{\\ell_{0}}+\\Delta\\sum_{\\ell\\notin I_{0}}\\varphi_{\\ell}\\big)}}\\\\ &{\\phantom{e_{\\mathrm{In}(x)}^{\\top}\\mathrm{T}_{\\theta^{(i)}}(X)}\\geq\\frac{1}{1+|V|\\exp\\big(-\\Delta(1-2\\epsilon)\\big)}}\\\\ &{\\phantom{e_{\\mathrm{In}(x)}^{\\top}\\mathrm{T}_{\\theta^{(i)}}(X)}\\geq1-\\epsilon_{0},}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where the last inequality follows from $T=O(\\log(1/\\epsilon_{0}))$ . Therefore, the trained transformer will predict $n(x)$ , the next token of the non-comparable token. ", "page_idx": 32}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately refect the paper's contributions and scope? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We provide details of our three main claims made in the abstract in Section 4, Section 5, and Section 6, respectively. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 33}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: Our paper has specified the assumptions (Assumption 1,Assumption 2, and Assumption 3) under which our results hold. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should refect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should refect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 33}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We provide the full proofs in the appendix and title them as the \u201cproof of lemmas, propositions, and theorems\". Each proof can be easily found from the table of contents. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 34}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it aoects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We provide our experiment details in Section 7, including the model dimension, vocabulary size, the learning rate, and the iteration number. The experiment follows exactly of Algorithm 1. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 34}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We provide our code in the supplemental. We do not use open source data. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 35}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: Our experiment details are provided in Section 7. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 35}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: As our synthetic dataset and training process is fixed, we do not foresee significant errors. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative errorrates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: Our experiment is on synthetic data and is computationally lightweight, which does not require any GPU. The running time is also within a hour. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 36}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We follow exactly the code of ethics. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 36}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed. \u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 36}, {"type": "text", "text": "\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 37}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: Our paper is primarily a theoretical work, and does not have such risk. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safetyfilters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing eoective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith eoort. ", "page_idx": 37}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properlyrespected? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Justification: Our paper is primarily a theoretical work, and does not use any assets. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode. com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 37}, {"type": "text", "text": "", "page_idx": 38}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: Our paper is primarily a theoretical work, and does not release any assets. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 38}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: Our paper is primarily a theoretical work, and does not involve crowdsourcing nor research with human subjects. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 38}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: Our paper is primarily a theoretical work, and does not involve crowdsourcing nor research with human subjects. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution. ", "page_idx": 38}, {"type": "text", "text": "\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 39}]