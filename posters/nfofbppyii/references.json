{"references": [{"fullname_first_author": "Vaswani, A.", "paper_title": "Attention is all you need", "publication_date": "2017-12-01", "reason": "This paper introduces the transformer architecture, the foundation of the model analyzed in the current paper."}, {"fullname_first_author": "Brown, T.", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper highlights the empirical success of transformers in large language models, motivating the theoretical analysis in the current work."}, {"fullname_first_author": "Tian, Y.", "paper_title": "Scan and snap: Understanding training dynamics and token composition in 1-layer transformer", "publication_date": "2023-12-01", "reason": "This paper provides valuable insights into the training dynamics of single-layer transformers for next-token prediction (NTP), which directly relates to the current study."}, {"fullname_first_author": "Li, Y.", "paper_title": "Mechanics of next token prediction with self-attention", "publication_date": "2024-05-01", "reason": "This paper investigates the training dynamics of transformers for NTP, complementing the current paper's work with its asymptotic convergence analysis."}, {"fullname_first_author": "Huang, Y.", "paper_title": "In-context convergence of transformers", "publication_date": "2023-10-26", "reason": "This paper studies the convergence of transformers under a theoretically amenable setting, providing a foundation for the current paper's non-asymptotic convergence analysis."}]}