[{"type": "text", "text": "Provable Posterior Sampling with Denoising Oracles via Tilted Transport ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Joan Bruna Jiequn Han New York University & Flatiron Institute Flatiron Institute bruna@cims.nyu.edu jhan@simonsfoundation.org ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Score-based diffusion models have significantly advanced high-dimensional data generation across various domains, by learning a denoising oracle (or score) from datasets. From a Bayesian perspective, they offer a realistic modeling of data priors and facilitate solving inverse problems through posterior sampling. Although many heuristic methods have been developed recently for this purpose, they lack the quantitative guarantees needed in many scientific applications. This work addresses the topic from two perspectives. We first present a hardness result indicating that a generic method leveraging the prior denoising oracle for posterior sampling becomes infeasible as soon as the measurement operator is mildly ill-conditioned. We next develop the tilted transport technique, which leverages the quadratic structure of the log-likelihood in linear inverse problems in combination with the prior denoising oracle to exactly transform the original posterior sampling problem into a new one that is provably easier to sample from. We quantify the conditions under which the boosted posterior is strongly log-concave, highlighting how task dififculty depends on the condition number of the measurement matrix and the signal-to-noise ratio. The resulting general scheme is shown to match the best-known sampling methods for Ising models, and is further validated on high-dimensional Gaussian mixture models. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Inverse problems consist in reconstructing a signal of interest from noisy measurements. As such, they are a central object of study across many scientific domains, including signal processing, imaging, astrophysics or computational biology. In the common settings where the measurement information is limited, a reliable solution for these problems usually depends on prior knowledge of the data. One popular approach is to choose a regularizer that utilizes data properties such as smoothness or sparseness, and then solve a regularized optimization problem to obtain a point estimate of the original data. However, this approach often struggles with selecting an appropriate regularizer and might be unstable in the presence of large measurement noise. A more robust approach takes a statistical formulation and seeks to sample the posterior distribution of data based on Bayes\u2019s theorem, which allows for uncertainty quantification in the reconstructed data by leveraging a model for the prior data distribution. ", "page_idx": 0}, {"type": "text", "text": "While accurate models for high-dimensional distributions are notoriously complex to estimate, the resurgence of deep neural networks has provided unprecedented capabilities for modeling complex data distributions in certain high-dimensional regimes. Specifically, score-based diffusion models [55, 33, 58] have achieved remarkable empirical success in generating high-dimensional data across various domains, including images, video, text, and audio. These models implicitly parameterize data distributions through an iterative denoising process that builds up data from noise. Furthermore, there is a growing literature developing theoretical foundations of score-based diffusion models [17, 7, 45, 16, 19], giving a comprehensive error analysis including score estimation, initialization error and time-discretization error. By generating high-fidelity data, these models can also serve as data prior for posterior sampling in inverse problems in high dimensions. Following this idea, many studies (see, e.g., [40, 22]) have leveraged diffusion models for posterior sampling. However, as discussed below, various categories of approaches for posterior sampling introduce different uncontrollable errors, such as those arising from the approximation of the conditional score or the use of a limited variational family. This abundance of heuristics contrasts with the principled sampling used in prior data generation, and is often at odds with the statistical guarantees needed in many scientific applications. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this work, we aim to bridge the gap between principled diffusion-based algorithms for both prior and posterior distributions. Focusing on the canonical setting of linear inverse problems, where measurements are of the form $y=A x+w$ , with $x\\sim\\pi$ the signal to be estimated and $w$ an independent noise, we first illustrate a negative result, revealing that no method can efifciently sample the posterior in general cases, even with the prior denoising oracle. Subsequently, we develop the tilted transport technique, which utilizes the quadratic structure of the log-likelihood in linear inverse problems in combination with the prior denoising oracle to exactly transform the original posterior sampling problem into a new one that is easier to sample. Figure 1 illustrates a schematic plot of the method using two-dimensional Gaussian mixture examples, showing that while the original target posterior problem remains multimodal, the boosted posterior resembles a unimodal distribution. ", "page_idx": 1}, {"type": "image", "img_path": "PhLlE8UOEv/tmp/74089d6e01c2ff3e46f25f4263893324315c1e7620ea21476f53b5da16907349.jpg", "img_caption": ["Figure 1: Schematic plot of tilted transport boosting posterior sampling with a 2D Gaussian mixture example. The density plot shows the first variable\u2019s density, and the scatter plot displays the samples. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "We establish a precise condition where the density of the transformed posterior problem becomes strongly log-concave, making it suitable for efifcient sampling via Langevin dynamics. This condition showcases the interplay between a geometric property of the prior (what we call tilted spread; see Section 5) and the conditioning and noise level of the measurements. Interestingly, the condition can be satisfied when the signal-to-noise ratio (SNR) is either moderately low or moderately high, in contrast with traditional sampling methods, which typically excel only within a specific regime. ", "page_idx": 1}, {"type": "text", "text": "As a first application, we show that tilted transport can sample from Ising models of the form $\\nu(x)\\,\\propto\\,e^{-\\frac{1}{2}x^{\\top}Q x}$ , where $x\\ \\in\\ \\{\\pm1\\}^{d}$ is supported in the hypercube, up to the critical threshold determined by the gap $\\lambda_{\\operatorname*{max}}(Q)-\\lambda_{\\operatorname*{min}}(Q)=1$ , thus matching the performance of Glauber dynamics [24, 1] as well as the computational threshold predicted by the low-degree method [42]. More generally, even when the boosted posterior is not strongly log-concave, it remains easier to sample than the original one. Thus, tilted transport can be combined with any existing black-box posterior sampling methods to enhance their performance. This technique operates without any additional computational cost and functions in a plug-and-play fashion, allowing for straightforward integration into various frameworks. When working with high-dimensional Gaussian mixtures, where an analytical solution to the posterior is available, we numerically validate our theory and demonstrate enhanced posterior sampling performance. ", "page_idx": 1}, {"type": "text", "text": "1.1 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Numerous studies in recent years have explored score-based priors for posterior sampling. We note that several recent works [60, 20, 29, 54, 21] introduce hyperparameters to balance the influence of the prior and measurements, resulting in sampling strategies that guide output to regions where the given observation is more likely. These strategies typically deviate from the principles of Bayesian posterior sampling and often lack a precise definition of the resulting distribution. In contrast, other approaches adhere more closely to Bayesian principles. One such approach is variational inference, which involves designing variational objectives and optimization methods based on the structure of score-based diffusion [41, 47, 26, 37]. However, even with an accurate prior score, the accuracy of posterior sampling heavily depends on the choice of variational family and optimization procedures, not to mention the additional optimization cost. Another popular strategy focuses on approximating the score conditional on the measurement using various heuristics [58, 40, 36, 22, 48, 56, 57]. In this approach, approximation errors typically remain largely uncontrollable due to the challenges associated with tracking the conditional distribution for intermediate states. Recently, some studies have adopted sequential Monte-Carlo methods to systematically approximate the conditional score [62, 13, 23], providing consistency as the number of particles used to approximate the conditional distribution of the intermediate states increases. However, this particle-based method still struggles with highdimensional problems due to the curse of dimensionality [9]. Alternatively, [61, 63] propose plug-and-play methods with denoising oracles for posterior sampling, offering asymptotic guarantees, though the required steps may grow prohibitively in high dimensions. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "We note that [13] also intuitively explores the possibility of reducing the original posterior to an equivalent one under restrictive conditions in the discrete-time setting. In contrast, our tilted transport technique operates in a fairly generic setting and is supported by a clear theoretical foundation. Concurrently, [49] proposes a conceptually similar two-stage approach for posterior sampling in sparse linear regression, based on a different structural prior rather than denoising oracles. ", "page_idx": 2}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Notations:. $\\mathcal{P}(\\mathbb{R}^{d})$ denotes the space of probability measures over $\\mathbb{R}^{d}$ . $\\gamma_{d}$ denotes the $d$ -dimensional standard Gaussian measure, and by slight abuse of notation, $\\gamma_{\\delta}$ or $\\gamma_{\\Sigma}$ denote the centered Gaussian measure with covariance $\\delta I_{d}$ or $\\Sigma$ when the context is clear. For $Q\\,\\succeq\\,0$ in $\\mathbb{R}^{d\\times d}$ and $b\\,\\in\\,\\mathbb{R}^{d}$ in the span of $Q$ , the quadratic tilt of $\\pi$ is the measure $\\mathsf{T}_{Q,b}\\pi\\,\\ll\\,\\pi$ with density proportional to $\\begin{array}{r}{\\frac{\\mathrm{d}\\mathsf T_{Q,b}\\,\\pi}{\\mathrm{d}\\pi}(x)\\,\\propto\\,\\exp\\left\\{-\\frac{1}{2}x^{\\top}Q x+x^{\\top}b\\right\\}}\\end{array}$ . We also use the notation ${\\sf T}_{Q}$ when $b\\,=\\,0$ . $\\lVert Q\\rVert$ denotes its operator norm. $\\pi\\star\\gamma$ denotes the convolution of two measures $\\pi$ and $\\gamma$ . For $\\alpha\\;\\geq\\;0$ and $\\pi\\in\\dot{\\mathcal{P}}(\\mathbb{R}^{d})$ , we define $\\dot{\\mathsf{D}_{\\alpha}}\\pi(x):=\\alpha^{d}\\pi(\\alpha x)$ as the dilation of $\\pi$ . For $\\beta\\geq0$ and $\\pi\\in\\mathcal{P}(\\mathbb{R}^{d})$ , we define $\\mathsf{C}_{\\beta}\\pi(x):=\\pi\\star(\\mathsf{D}_{\\beta^{-1/2}}\\gamma_{d})$ as the Gaussian convolution of $\\pi$ . ", "page_idx": 2}, {"type": "text", "text": "Problem Setup. Consider a high-dimensional object of interest $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ , drawn from a certain probability distribution $\\pi\\in{\\mathcal{P}}(\\mathbb{R}^{d})$ . We suppose that one has managed to learn a generative model for $\\pi$ via the DDPM objective [33]; in other words, for any $\\mathrm{y}\\in\\mathbb{R}^{d}$ and $\\sigma\\geq0$ , we have access to the denoising oracle $\\mathsf{D}\\mathsf{O}_{\\pi}(y,\\sigma):=\\mathbb{E}[x|y]$ , where $\\mathrm{y}=x+\\sigma w$ , with $x\\sim\\pi$ and $w\\sim\\gamma_{d}$ independent. It is now well-established that, such denoising oracle enables efifcient sampling of $\\pi$ , well beyond the classic isoperimetric assumptions for fast relaxation of Langevin dynamics [17]. ", "page_idx": 2}, {"type": "text", "text": "Suppose that we now measure $\\mathrm{y}=A x+\\sigma w$ , where again $x\\sim\\pi$ and $w\\sim\\gamma_{d^{\\prime}}$ are independent, but now $A\\in\\mathbb{R}^{d^{\\prime}\\times d}$ is a known linear operator different from the identity. Given these linear measurements, we are now interested in the posterior sampling of $x$ given $y$ . This corresponds to the basic setup of linear inverse problems, encompassing many applications such as image inpainting, super-resolution, tomography, or source separation, to name a few. We are interested in the following natural question: can the power of denoising oracles be provably transferred to posterior sampling? ", "page_idx": 2}, {"type": "text", "text": "By Bayes\u2019 rule, the posterior distribution $\\nu_{\\mathrm{y},A}$ (denoted simply by $\\nu$ when the context is clear) has density proportional to $\\begin{array}{r}{\\pi(x)p(y|x)\\,\\propto\\,\\exp\\left\\{-\\frac{1}{2\\sigma^{2}}\\|A x-y\\|^{2}\\right\\}\\pi(x)}\\end{array}$ , and thus we can write it as a quadratic tilt of $\\pi$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\nu=\\mathsf{T}_{Q,b}\\pi\\;,\\;\\mathrm{with}\\;Q=\\sigma^{-2}A^{\\top}A\\,,\\;b=-\\sigma^{-2}A^{\\top}y\\;.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "We readily identify certain regimes where sampling from $\\nu$ might be easy: ", "page_idx": 2}, {"type": "text", "text": "\u2022 If $\\lambda_{\\operatorname*{min}}(\\mathcal{Q})$ is sufifciently large, $\\lambda_{\\operatorname*{min}}(Q)\\gg1$ , then one expects $\\nu$ to be strongly log-concave, enabling fast relaxation of Langevin dynamics. ", "page_idx": 2}, {"type": "text", "text": "\u2022 If $\\lambda_{\\operatorname*{max}}(Q)$ is sufifciently small, $\\lambda_{\\operatorname*{max}}(Q)\\ll1$ , then one expects $\\nu\\approx\\pi$ in the appropriate sense, and therefore that samples from $\\pi$ (which can be produced efifciently thanks to $\\mathsf{D O}_{\\pi}$ ) may be perturbed into samples from $\\nu$ .   \n\u2022 If $A\\in O_{d}$ is a unitary transformation, then $Q=\\sigma^{-2}\\mathrm{Id}$ and the inverse problem reduces to isotropic Gaussian denoising, and is thus at first glance \u2018compatible\u2019 with the structure of the denoising oracle (such observation will be formalized later). ", "page_idx": 3}, {"type": "text", "text": "At this stage, we can already identify two key parameters of the problem that are likely to drive the dififculty of posterior sampling: on one hand, a proxy for the signal-to-noise ratio, measured e.g., by $\\begin{array}{r}{\\mathrm{SNR}:=\\bar{\\lambda_{\\operatorname*{min}}}(Q)=\\frac{\\lambda_{\\operatorname*{min}}(\\bar{A})^{2}}{\\sigma^{2}}}\\end{array}$ . On the other hand, the conditioning of the measurement operator $A$ $\\begin{array}{r}{\\kappa(A):=\\frac{\\lambda_{\\operatorname*{max}}(A)}{\\lambda_{\\operatorname*{min}}(A)}}\\end{array}$ . As we shall see, these two characteristics of the linear measurement system will characterize necessary and sufifcient conditions for probable posterior sampling. In the following, we assume the log of prior density $\\pi$ is smooth and its Hessian exists $\\forall x\\in\\mathbb{R}^{\\dot{d}}$ . ", "page_idx": 3}, {"type": "text", "text": "Denoising Oracles and Score-Based Diffusion. Let us first review the natural connection between denoising and score-based generative modeling. Score-based diffusion models consist of two processes: a forward process that gradually adds noise to input data and a reverse process that learns to generate data by iteratively removing this noise. For example, one widely used family for the forward process is the Ornstein\u2013Uhlenbeck (OU) process1: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{d}X_{t}=-X_{t}\\mathrm{d}t+\\sqrt{2}\\mathrm{d}W_{t},\\quad X_{0}\\sim\\pi\\;,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $W_{t}$ is the standard Wiener process. We use $\\pi_{t}$ to denote the density of $X_{t}$ , given by the action of the OU semigroup $\\pi_{t}=\\Omega_{t}^{*}\\pi$ , defined by $\\mathsf{O}_{t}f(x)=\\mathbb{E}[f(X_{t})|X_{0}=x]$ , and explicitly given by dilated Gaussian convolutions, $\\mathsf{O}_{t}^{*}:=\\mathsf{C}_{\\beta_{t}}\\mathsf{D}_{\\alpha_{t}}$ , with $\\beta_{t}=1-e^{-2t}$ and $\\alpha_{t}=e^{t}$ . With a sufifciently large $T$ , we know that $\\pi_{T}$ is close to the density of standard Gaussian $\\gamma_{d}$ , owing to the exponential contraction of the OU semigroup: $\\mathrm{KL}(\\pi_{T}||\\gamma_{d})\\overset{.}{\\leq}e^{-T}\\mathrm{KL}(\\pi||\\gamma_{d})$ . ", "page_idx": 3}, {"type": "text", "text": "Finally, the measure $\\pi_{t}$ solves the Fokker-Plank equation ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\partial_{t}\\pi_{t}=\\nabla\\cdot\\left(x\\pi_{t}\\right)+\\Delta\\pi_{t}\\;,\\;\\pi_{0}=\\pi\\;.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "By writing (2) as a transport equation $\\partial_{t}\\pi_{t}\\,=\\,\\nabla\\cdot\\bigl((x+\\nabla\\log\\pi_{t})\\pi_{t}\\bigr)$ , we can formally reverse the transport starting at a large time $T$ and solving ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\partial_{t}\\tilde{\\pi}_{t}=\\nabla\\cdot\\left(-(x+\\nabla\\log\\pi_{T-t})\\tilde{\\pi}_{t}\\right)\\,,\\;\\tilde{\\pi}_{0}=\\pi_{T}\\;.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Since $\\tilde{\\pi}_{t}=\\pi_{T-t}$ for $0\\le t\\le T$ , introducing again the dissipative term leads to $\\partial_{t}\\tilde{\\pi}_{t}=\\nabla\\cdot\\left(-\\left(x+\\right.$ $2\\nabla\\log\\tilde{\\pi}_{t})\\tilde{\\pi}_{t})+\\Delta\\tilde{\\pi}_{t}$ , $\\tilde{\\pi}_{0}=\\pi_{T}$ , which admits the SDE representation ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{d}\\tilde{X}_{t}=\\big(\\tilde{X}_{t}+2\\nabla\\log\\pi_{T-t}(\\tilde{X}_{t})\\big)\\mathrm{d}t+\\sqrt{2}\\mathrm{d}\\overline{{W}}_{t},\\enspace\\tilde{X}_{0}\\sim\\pi_{T}\\ .\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In practice, one runs this reverse diffusion starting from $\\tilde{X}_{0}\\sim\\gamma_{d}$ rather than $\\tilde{X}_{0}\\sim\\pi_{T}$ . However, by the data-processing inequality, we have that $\\mathrm{KL}(\\bar{\\pi}||\\tilde{\\pi}_{T})\\leq\\mathrm{KL}(\\pi_{T}||\\gamma_{d})=O(e^{-T})$ , thus incurring in insignificant error. To facilitate later exposition, we write the above process reverse in time [2, 31] ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{d}X_{t}^{\\leftarrow}=(-X_{t}^{\\leftarrow}-2\\nabla\\log\\pi_{t}(X_{t}^{\\leftarrow}))\\mathrm{d}t+\\sqrt{2}\\mathrm{d}\\overline{{W}}_{t},~~X_{T}^{\\leftarrow}\\sim\\gamma_{d},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "and interpret the data generation process as running the reverse SDE from $T$ back to 0. ", "page_idx": 3}, {"type": "text", "text": "By the well-known Tweedie\u2019s formula, and up to time reparametrisation, the denoising oracle is equivalent to the time-dependent score $\\nabla\\log\\pi_{t}$ : ", "page_idx": 3}, {"type": "text", "text": "Fact 1 (Tweedie\u2019s formula, [32]). We have $\\nabla\\log\\pi_{t}(x)=-(1-e^{-2t})^{-1}(x-e^{-2t}\\mathsf{D}\\mathsf{O}_{\\pi}(x,1-e^{2t})).$ ", "page_idx": 3}, {"type": "text", "text": "Log-Sobolev Inequality and Fast Relaxation of Langevin Dynamics. Given a Gibbs distribution $\\pi\\in\\dot{\\mathcal{P}}(\\mathbb{R}^{d})$ of the form $\\bar{\\pi}\\propto e^{-f}$ , a powerful and versatile method to sample from $\\pi$ is to consider the Langevin dynamics ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{d}X_{t}=-\\nabla f(X_{t})\\mathrm{d}t+\\sqrt{2}\\mathrm{d}W_{t}\\ ,\\ X_{0}\\sim\\mu_{0}\\ ,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mu_{0}$ is an arbitrary initial distribution. It is easy to verify that these dynamics define a Markov process that admits $\\pi$ as its unique invariant measure. Perhaps less obvious is the fact that the Fokker-Plank equation associated with eq. (6), given by $\\partial_{t}\\mu=\\nabla\\cdot(\\nabla f\\mu)+\\Delta\\mu$ (and where $\\mu_{t}$ is the law of $X_{t}$ ) is in fact a Wasserstein gradient flow for the relative entropy functional $\\operatorname{KL}(\\mu||\\pi)$ [38]. Under this interpretation, one can quantify the convergence of Langevin dynamics to their invariant measure, i.e., its time to relaxation, by establishing a sharpness or Polyak-Lowacjevitz (PL)-type inequality. Indeed, by noticing that $\\begin{array}{r}{\\frac{d}{d t}\\mathrm{KL}(\\mu||\\pi)=-\\mathrm{I}(\\mu||\\pi)}\\end{array}$ , where $\\operatorname{I}(\\mu||\\pi)=\\mathbb{E}_{\\mu}[||\\nabla\\log\\mu-\\nabla\\log\\pi||^{2}]$ is the Fisher divergence, the PL-type inequality in this setting is given by the Logarithmic Sobolev Inequality (LSI): we say that a measure $\\pi$ satisfies $\\mathrm{LSI}(\\rho)$ if for any $\\mu\\in\\mathcal{P}(\\mathbb{R}^{d})$ it holds $\\begin{array}{r}{\\mathrm{KL}(\\mu||\\pi)\\leq\\frac{1}{2\\rho}\\mathrm{I}(\\bar{\\mu}||\\pi)\\overset{}{\\,\\,}}\\end{array}$ . This functional inequality directly implies $\\begin{array}{r}{\\mathrm{KL}(\\mu_{t}||\\pi)\\leq e^{-2\\rho t}\\mathrm{KL}(\\mu_{0}||\\pi)}\\end{array}$ . While for general $\\pi$ it is typically hard to establish the LSI, there are two important sources of structure that lead to quantitative (i.e., $\\rho=\\Omega_{d}(1)]$ ) bounds: when $\\pi$ is a product measure $\\pi=\\tilde{\\pi}^{\\otimes d}$ (in which case $\\pi$ satisfies LSI with the same constant as $\\tilde{\\pi}$ ), and when $\\pi$ is strongly log-concave2, i.e., $-\\nabla^{2}\\log\\pi(x)\\,\\geq\\,\\alpha I$ for all $x$ , in which case the celebrated Bakry-Emery criterion [3] states that $\\rho\\geq\\alpha$ . ", "page_idx": 4}, {"type": "text", "text": "3 Evidence of Computational Hardness in the Generic Case ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We start our analysis of posterior sampling by discussing negative results for the general case. Recently, [30] established computational lower bounds for this task using cryptographic hardness assumptions. In this section, we complement these results by illustrating a correspondence with sampling problems on Ising models, leading to an arguably simpler conclusion. ", "page_idx": 4}, {"type": "text", "text": "For this purpose, consider $\\Bar{\\pi}=\\mathrm{Unif}(\\{\\pm1\\}^{d})$ the uniform measure of the hypercube. Quadratic tilts of $\\bar{\\pi}$ define generic Ising models, a rich and intricate class of high-dimensional distributions. Since $\\bar{\\pi}$ is a product measure, its associated denoising oracle becomes a separable function that can be computed in closed-form: ", "page_idx": 4}, {"type": "text", "text": "Fact 2 (Denoising Oracle for $\\bar{\\pi}$ ). Let $\\begin{array}{r}{\\gamma(t;\\mu,\\sigma)=\\exp\\left\\{-\\frac{1}{2\\sigma^{2}}(t-\\mu)^{2}\\right\\}}\\end{array}$ . Then we have ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathsf{D O}_{\\bar{\\pi}}(y,\\sigma)=\\left(\\phi(y_{i};\\sigma)\\right)_{i=1\\dots d}\\,\\,,\\,\\,w i t h\\,\\,\\,\\phi(t,\\sigma)=\\frac{\\gamma(t,+1,\\sigma)-\\gamma(t,-1,\\sigma)}{\\gamma(t,+1,\\sigma)+\\gamma(t,-1,\\sigma)}\\mathrm{~.~}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Given a symmetric matrix $Q\\,\\in\\,\\mathbb{R}^{d\\times d}$ , an Ising model is given by the tilt $\\mathsf{T}_{\\mathcal{Q}}\\bar{\\pi}\\,\\in\\,\\mathcal{P}(\\{\\pm1\\}^{d})$ . In our setting, we can thus view such models as the posterior distribution of a linear inverse problem associated with the uniform prior $\\bar{\\pi}$ . Efifciently sampling from Ising models is a fundamental question at the interface of statistical physics and high-dimensional probability, and several works provide evidence of computational hardness under a variety of settings. ", "page_idx": 4}, {"type": "text", "text": "Notably, by treating $Q$ as the adjacency matrix of a regular graph, [27] establishes that sampling from $\\nu$ is impossible for $\\lambda_{\\operatorname*{max}}(Q)-\\lambda_{\\operatorname*{min}}(Q)\\,\\geq\\,2+\\varepsilon.$ , for any $\\varepsilon>0$ , unless ${\\mathsf{N P}}={\\mathsf{R P}}$ . In other words, for poorly conditioned tilt $Q$ (in the sense that there is a large gap between the smallest and largest eigenvalue), there is no efifcient posterior sampling algorithm, even with the knowledge of the prior denoising oracle. The threshold can even be reduced to $1+\\varepsilon$ by using a weaker notion of computational hardness [42], given by the low-degree polynomial method [4, 43]. Remarkably, this threshold agrees with the current best-known algorithmic results for sampling generic Ising models with Glauber dynamics [25, 1]. Finally, we also mention that when $Q$ is a random Gaussian symmetric matrix, the associated so-called Sherrington-Kirkpatrick (SK) model, has been analyzed with dedicated algorithms. In this setting, it is also known [24] that \u2018stable\u2019 sampling algorithms fail to sample from the SK model as soon as $\\lambda_{\\operatorname*{max}}(Q)-\\lambda_{\\operatorname*{min}}(Q)>1$ . In summary, we have: ", "page_idx": 4}, {"type": "text", "text": "Theorem 3 (Computational Hardness of Sampling Ising Models, [42, 27]). There exist no generalpurpose, efficient posterior sampling algorithms, for $Q$ sufficiently ill-conditioned, even under the knowledge of the prior denoising oracle. ", "page_idx": 4}, {"type": "text", "text": "One could wonder whether this computational hardness comes from the discrete nature of the hypercube. It is not hard to observe that this is not the case: the following proposition, proved in Appendix A, shows a simple reduction from a model where the prior $\\bar{\\pi}$ is replaced by a smooth mixture of Gaussians $\\pi$ centered at the corners of the hypercube, with variance $\\delta$ . ", "page_idx": 4}, {"type": "text", "text": "Proposition 4 (Hardness extends to smooth priors). Assume a posterior sampler exists for the smooth prior with TV error $\\epsilon$ and $\\delta=o(d^{-1/2})$ . Then there exists a sampler for the associated Ising model with TV error $1.1\\epsilon$ . ", "page_idx": 5}, {"type": "text", "text": "In conclusion, one cannot hope for a generic method that leverages the prior denoising oracle to perform efifcient posterior sampling, as soon as $A$ is mildly ill-conditioned. Thus, in order to perform provable posterior sampling, one needs to either (i) constraint the measurements, or (ii) exploit structural properties of the prior measure. In the following, we focus on (i), namely providing guarantees for well-conditioned $A$ that leverage the OU semigroup for generic prior distributions. ", "page_idx": 5}, {"type": "text", "text": "4 Posterior Sampling via Tilted Transport ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We now present a simple method that reduces the original posterior sampling problem to another posterior sampling problem with more benign geometry, by leveraging the shared quadratic structure of the posterior tilt and the OU semigroup. The power of the denoising oracle to perform sampling of the prior $\\pi$ comes from its ability to run the transport equation (3) in either direction, and leveraging the fact that sampling from $\\pi_{T}$ is easy. To transfer this power to posterior sampling, we can thus attempt to replicate this scheme: can we implement a transport between the posterior $\\nu$ and a terminal measure $\\nu_{T}$ that is easy to sample, that only relies on the pre-trained prior $\\mathsf{D O}_{\\pi}$ ? ", "page_idx": 5}, {"type": "text", "text": "A Motivating Example. Consider first the denoising setting: $\\mathrm{~y~}=\\boldsymbol{x}+\\sigma\\boldsymbol{w}$ . According to the forward process, we have $p(X_{s}|X_{0})\\,\\stackrel{d}{=}\\,N(e^{-s}X_{0},(1\\,-\\,e^{-2s})I_{d})$ . Introduce $T^{*}~>~0$ and define $\\tilde{y}=e^{-T^{*}}y=e^{-T^{*}}x+e^{-T^{*}}\\sigma w$ such that $p(\\widetilde{y}|x)\\overset{d}{=}N(e^{-T^{*}}x,e^{-2T^{*}}\\sigma^{2}I_{d})$ . We match the variance by letting $e^{-2T^{*}}\\sigma^{2}=1-e^{-2T^{*}}$ , i.e., $\\begin{array}{r}{T^{*}=\\frac{1}{2}\\log(1+\\sigma^{2})}\\end{array}$ , then we have $p(\\tilde{y}|x)=p(X_{T^{*}}|X_{0})$ , which gives $(x,\\tilde{y})\\overset{d}{=}(X_{0},X_{T^{*}})$ . Therefore, to perform the posterior sampling $p(x|\\tilde{y})$ , we only need to do the sampling $p(X_{0}|X_{T^{*}})$ , which can be achieved through the reverse SDE. Specifically, let $X_{T^{*}}=e^{-T^{*}}y$ and run the reverse SDE (5) from $T^{*}$ to 0, then $X_{0}$ will be the desired posterior. ", "page_idx": 5}, {"type": "text", "text": "Hamilton-Jacobi Equation and Quadratic Tilts. If $\\pi_{t}$ solves the Fokker-Plank eq. (2), then one can verify that the time-varying potentials $f_{t}:=\\log\\pi_{t}$ solve the viscous Hamilton-Jacobi PDE (HJE) ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\partial_{t}f_{t}=\\Delta f_{t}+\\|\\nabla f_{t}\\|^{2}+x\\cdot\\nabla f_{t}+d\\;,\\;f_{0}=f\\;.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Now, the posterior $\\nu=\\mathsf{T}_{Q,b}\\pi$ creates an additional quadratic term in the potential $\\log\\nu=f-$ ${\\textstyle{\\frac{1}{2}}}x^{\\top}Q x+x\\cdot b$ . One could naively hope that this additive quadratic term would still define a solution of the HJE with the tilted initial condition $\\tilde{f}_{0}=\\log\\nu$ \u2014 or equivalently that the measure $\\mathsf{T}_{Q,b}\\pi_{t}$ solves the transport equation (3). Unfortunately, due to the nonlinearity in (8) brought by the terms $\\|\\nabla f_{t}\\|^{2}$ , this is not the case. However, as we shall see now, this is not far from being true: one just needs to consider time-varying quadratic tilts in order to satisfy the HJE. ", "page_idx": 5}, {"type": "text", "text": "Tilt Transport Equation. We consider then a one-parameter family of distributions $\\nu_{t}$ of the form $\\nu_{t}=\\mathsf{T}_{Q_{t},b_{t}}\\pi_{t}$ , with $Q_{0}=Q$ and $b_{0}=b$ . As it turns out, one can ensure that $\\log{\\nu_{t}}$ solves the HJE associated with the reverse process by asking that $Q_{t},b_{t}$ satisfy the first-order ODE ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{\\dot{Q}_{t}=2(I+Q_{t})Q_{t}\\;,\\quad Q_{0}=Q\\right.}\\\\ {\\dot{b}_{t}=(I+2Q_{t})b_{t}\\;,\\quad b_{0}=b\\;}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Theorem 5 (Tilted Transport). Assume $t<T^{*}$ such that the $O D E$ (9) is well-defined on $[0,t]$ . By initializing $X_{t}\\sim\\nu_{t}$ and run the reverse SDE (5) from \ud835\udc61to $O_{i}$ , we have $X_{s}\\sim\\nu_{s}$ for $s\\in[0,t]$ , specifically, $X_{0}$ gives the desired posterior. ", "page_idx": 5}, {"type": "text", "text": "Solution to eq. (9). Without loss of generality, we assume $d^{\\prime}\\leq d$ , and the observation operator $A\\,\\in\\,\\mathbb{R}^{d^{\\prime}\\times d}$ has a general singular value decomposition form $\\boldsymbol{A}\\,=\\,U\\Sigma V^{\\top}$ with non-zero singular values $\\lambda_{1}\\geq\\lambda_{2}\\geq\\cdot\\cdot\\cdot\\geq\\lambda_{d^{\\prime}}>0$ . By diagonalizing $Q$ and solving the scalar ODE $\\dot{q}_{t}=2(1+q_{t})q_{t}$ for diagonal entries, we have $\\begin{array}{r}{Q_{t}=V\\mathrm{diag}\\left(\\frac{e^{2t}}{1+\\sigma^{2}/\\lambda_{1}^{2}-e^{2t}},\\cdot\\cdot\\cdot\\,,\\frac{e^{2t}}{1+\\sigma^{2}/\\lambda_{d^{\\prime}}^{2}-e^{2t}},0,\\cdot\\cdot\\cdot\\,,0\\right)V^{\\top}}\\end{array}$ solution is defined up to the blowup time $\\begin{array}{r}{T^{*}:=\\frac{1}{2}\\log(1+\\sigma^{2}/\\lambda_{1}^{2})=\\frac{1}{2}\\log(1+\\lambda_{\\operatorname*{max}}(Q)^{-1})}\\end{array}$ . $b_{t}$ can be further solved from the solution $Q_{t}$ ; see Appendix B.2 for more details. ", "page_idx": 5}, {"type": "text", "text": "With the explicit solution of $Q_{t},b_{t}$ , we can interpret the term $\\begin{array}{r}{\\exp(-\\frac{1}{2}x^{\\top}Q_{t}x+x^{\\top}b_{t})}\\end{array}$ as the likelihood of the inverse problem with respect to the new prior distribution $\\pi_{t}$ and the corresponding operator. Based on this observation and Theorem 5, we have the following corollary, transforming the original posterior sampling problem to a new posterior sampling problem exactly. We remark that when $A$ is identity, the corollary recovers the analysis we have in the motivating example; see Appendix B.2 for the proof and more discussions. ", "page_idx": 6}, {"type": "text", "text": "Corollary 6. Fix $t<T^{*}$ . Sampling from the original posterior is equivalent to a two-step process: first, sample from a new posterior $X_{t}\\sim\\nu_{t}$ , and then run the reverse SDE (5) from time \ud835\udc61to 0. ", "page_idx": 6}, {"type": "text", "text": "5 Quantitative Conditions for Provable Sampling ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Now we show that the new posterior sampling problem described above is provably easier to sample than the original posterior sampling problem from two aspects. On the one hand, the (negative) eigenvalues of the quadratic tilt $-{\\frac{\\dot{1}}{2}}x^{\\top}Q_{t}x+x^{\\top}b_{t}$ become more negative, essentially meaning that the SNR of the new observation model becomes larger. To be more specific, as $t\\to T^{*}$ , $\\begin{array}{r}{\\lambda_{\\operatorname*{min}}(Q_{t})\\,\\to\\,\\frac{1+\\lambda_{\\operatorname*{max}}(Q)^{-1}}{\\lambda_{\\operatorname*{min}}(Q)^{-1}-\\lambda_{\\operatorname*{max}}(Q)^{-1}}\\,>\\,\\lambda_{\\operatorname*{min}}(Q)}\\end{array}$ . On the other hand, the ne\ud835\udc61w prior distribution $\\pi_{T^{*}}$ becomes closer to a single-mode Gaussian (recall that , which is also easier to sample. Combining these two arguments, we expect that, as $t$ increases, $\\nu_{t}$ becomes easier to sample due to easier prior and easier likelihood: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\nu_{t}\\left(x\\right)\\propto\\underbrace{\\;\\pi_{t}\\left(x\\right)}_{\\mathrm{easier\\;prior}}\\underbrace{\\|-\\frac{1}{2}x^{\\top}Q_{t}x+x^{\\top}b_{t}\\vphantom{\\frac{1}{2}}\\}}_{\\mathrm{easier\\;likelihood}}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Let us now quantify the above intuition. ", "page_idx": 6}, {"type": "text", "text": "Sufifcient Conditions for Strong Log-Concavity of $\\nu_{T^{*}}$ . We start by giving a simple sufifcient condition that ensures that $\\nu_{T^{*}}$ is strongly log-concave. As discussed earlier, this ensures fast relaxation of the Langevin dynamics, enabling efifcient sampling from $\\nu_{T^{*}}$ \u2013 and therefore of $\\nu$ as per Corollary 6. For that purpose, given the prior $\\pi\\in\\mathcal{P}(\\mathbb{R}^{d})$ and $t\\geq0$ , we define ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\chi_{t}(\\pi):=\\operatorname*{sup}_{x\\in\\mathbb{R}^{d}}\\|\\mathrm{Cov}[\\mathsf{T}_{t I,x}\\pi]\\|,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where the covariance is given by $\\operatorname{Cov}[\\mu]=\\mathbb{E}_{x\\sim\\mu}[x x^{\\top}]-(\\mathbb{E}_{x\\sim\\mu}[x])(\\mathbb{E}_{x\\sim\\mu}[x])^{\\top}$ . $\\chi_{t}(\\pi)$ thus measures the largest \u2018spread\u2019 of any tilted measure of the form $\\mathsf{T}_{t,x}\\pi$ . Equipped with this definition, we have the following sufifcient condition to ensure that $\\nu_{T^{*}}$ is strongly log-concave: ", "page_idx": 6}, {"type": "text", "text": "Proposition 7 (Strong Log-Concavity of $\\nu_{T^{*}}$ ). $\\nu_{T^{*}}$ is strongly log-concave if ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\chi_{\\parallel Q\\parallel}(\\pi)<\\|Q\\|^{-1}\\frac{\\kappa(A)^{2}}{(\\kappa(A)^{2}-1)}\\;.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The proof is in Appendix C. It relates two parameters of the measurement process, the condition number of $A$ and the signal-to-noise ratio in terms of $\\lVert Q\\rVert$ , with a geometric property of the prior, the spread function $\\chi_{t}(\\pi)$ . While this function is not immediately transparent, the following examples illuminate its behavior in reprsentative high-dimensional settings. ", "page_idx": 6}, {"type": "text", "text": "Example 8 (Behavior of $\\chi_{t}(\\pi))$ . We have the following examples ", "page_idx": 6}, {"type": "text", "text": "(i) Gaussian measure: \ud835\udf12\ud835\udc61(\ud835\udefe\ud835\udc51) =11+\ud835\udc61.   \n(ii) Compactly Supported Gaussian Mixture: If $\\mu$ is compactly-supported in a ball of radius $R$ and $\\delta\\geq0$ , then $\\begin{array}{r}{\\chi_{t}(\\mu\\star\\gamma_{\\delta})\\,\\le\\,\\bigl(\\frac{R}{1+\\delta t}\\bigr)^{2}+\\frac{\\delta}{1+\\delta t}}\\end{array}$ .   \n(iii) Tensorization: $I f\\mu=\\mu_{1}\\otimes\\mu_{2}\\cdot\\cdot\\cdot\\otimes\\mu_{d}$ , then $\\chi_{t}(\\mu)=\\operatorname*{max}_{i}\\chi_{t}(\\mu_{i}).$ .   \n(iv) Uniform measure on hypercube: If $\\pi$ is uniform on the hypercube $\\mathcal{H}_{d}$ , then $\\chi_{t}(\\pi)=1$ . ", "page_idx": 6}, {"type": "text", "text": "Ising Models. As a direct consequence of Proposition 7 and Example 8 (iv), we establish a sampling guarantee for Ising models: ", "page_idx": 7}, {"type": "text", "text": "Corollary 9 (Tilted Transport for the Ising Model). Let \ud835\udf0bbe the uniform measure on the hypercube, and $Q$ such that $\\lambda_{\\operatorname*{max}}(Q)-\\lambda_{\\operatorname*{min}}(Q)<1$ . Then $\\nu_{T^{*}}$ is strongly log-concave, and therefore $\\nu=\\mathsf{T}_{Q}\\pi$ can be sampled efficiently (in continuous-time). ", "page_idx": 7}, {"type": "text", "text": "This result thus establishes that Ising models admit an efifcient continuous-time procedure for sampling provided their spectrum satisfies $\\lambda_{\\operatorname*{max}}(Q)-\\lambda_{\\operatorname*{min}}(Q)<1$ , thus precisely matching the threshold of [25, 1] achieved by Glauber dynamics, as well as the low-degree prediction from [42]. We remark though that our procedure is not (yet) algorithmic; a careful analysis of the discrete-time complexity and the approximation rates is beyond the current scope, but our next endeavor. If one specializes the previous result to the SK model, the equivalent inverse temperature that guarantees sampling is $\\beta^{*}=1/4$ , which remains below $\\beta=1$ , the threshold of the hard phase. For this threshold, dedicated AMP-based sampling succeeds [24, 14]. We also remark that, in itself, it should not come as a surprise that $\\nu$ may be sampled under these conditions, since [5] already established an LSI on $\\nu$ directly, using an entropy decomposition along the so-called Polchinski renormalization group [6] that refines our Bakry-Emery criterion. In this context, it would be interesting to explore whether this refined criterion could be applied to $\\nu_{T^{*}}$ to improve upon Proposition 7 under appropriate conditions. ", "page_idx": 7}, {"type": "text", "text": "Gaussian Mixtures. By applying Proposition 7 to Example 8 (ii), we directly obtain the following guarantee for generic comptactly supported Gaussian mixtures: ", "page_idx": 7}, {"type": "text", "text": "Corollary 10 (Tilted Transport for Gaussian Mixtures). If $\\pi=\\mu\\star\\gamma_{\\delta}$ and diam $(s u p p(\\mu))\\leq R_{*}$ , then $\\nu_{T^{*}}$ is strongly log-concave $i f$ ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\frac{(1+\\delta{\\bf S N R}^{2})(\\delta\\kappa(A)^{2}+{\\bf S N R}^{-2})}{\\kappa(A)^{2}-1}>R^{2}\\;.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "It also holds when $\\delta=0$ and the prior $\\pi$ is any distribution with a bounded support radius $R$ . ", "page_idx": 7}, {"type": "text", "text": "Figure 2 displays several contours of the condition in eq. (12) as a function of SNR and $\\kappa(A)$ . Each $U.$ -shaped contour is determined by a combination of $\\delta$ and $R$ , which uniquely characterizes the prior. For all points ((SNR), $\\kappa(A))$ outside of a contour, representing a specific inverse problem, $\\nu_{T^{*}}$ is strongly log-concave and thus easy to sample. Given an observation model where both SNR and $\\kappa(A)$ are fixed, it is straightforward to see that the condition in eq. (12) is more readily satisfied as $\\delta$ increases and $R$ decreases. Figure 2 also confirms this result since as $\\delta$ increases or $R$ decreases, the $U$ -shaped contour shrinks and the region of easy to sample expands. Now we discuss the implications in the reverse scenario where the prior is fixed and the observation model is adjusted. If we look at Figure 2 horizontally, we know that given a prior and $\\kappa(A)$ , the target posterior can be reliably sampled if the SNR is either sufifciently low or high, with the region of mid-SNR being challenging. The closer $\\kappa(A)$ is to 1, the smaller this challenging region is. When the problem is denoising such that $\\kappa(A)=1$ , the challenging region vanishes, and sampling the posterior is straightforward using the denoising oracle, as previously explained. ", "page_idx": 7}, {"type": "image", "img_path": "PhLlE8UOEv/tmp/bf85fba8a9553f1f87c1c9b2b9e0873115e88ba2cb5409fa415e0939237fbe05.jpg", "img_caption": ["Figure 2: Phase diagram for the boosted posterior $\\nu_{T^{*}}$ being strongly log-concave in Corollary 10. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Comparisons. 1. (with Langevin dynamics) As introduced above, Langevin dynamics and its discretized version, Langevin Monte Carlo (LMC) [52, 46] serve as ideal baselines for efifcient posterior sampling in high SNR regimes where the posterior becomes strongly log-concave. Proposition 7 demonstrates that our tilted transport technique enables provably efifcient sampling from a broader range of prior distributions compared to traditional Langevin dynamics without a denoising oracle. Particularly, in low SNR regimes where conventional Langevin dynamics struggle with severe non-log-concavity and slow mixing times, tilted transport can transform the sampling challenge into a tractable problem for log-concave distributions. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "2. (with Importance Sampling) In the low SNR regime with a well-conditioned $A$ , the posterior measure can be viewed as a small perturbation of the prior. As such, a natural baseline for posterior sampling is importance sampling using the prior as a proposal \u2014 for which samples can be efifciently obtained thanks to the denoising oracle and the variance of sample weights is small. However, as detailed in Appendix C.2, the sampling complexity is exponential with the SNR when the SNR is sufifciently large, assuring the failure of the importance sampling on this extreme. ", "page_idx": 8}, {"type": "text", "text": "Stability. In the numerical implementation of the boosted posterior, we often encounter specific errors. Appendix C.3 provides a stability analysis of the two-step process outlined in Corollary 6, focusing on initialization error and score error, and demonstrates that the quality of the final samples is robust with respect to these errors. ", "page_idx": 8}, {"type": "text", "text": "6 Numerical Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Our theory above demonstrates that $\\nu_{T^{*}}$ is provably easier to sample than the original posterior $\\nu$ Thus, given a baseline sampling algorithm $\\mathsf{A l g}$ , we can first sample from the boosted posterior and then apply the denoising oracle to obtain the final sample, rather than directly sampling from $\\nu$ using Alg. Algorithm 8 provides a complete description of this approach using tilted transport. In this instance, we use Euler discretization with equal time steps to transport samples from from $\\mathsf{T}_{\\mathcal{Q}_{\\tilde{T}},b_{\\tilde{T}}}\\pi_{\\tilde{T}}$ to $\\top_{Q,b}\\pi$ , though alternative time integrators and grids can also be applied. ", "page_idx": 8}, {"type": "text", "text": "Algorithm 1 Sampling Using Tilted Transport ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Require: Parameters of quadratic tilt $Q,b$ , small time shift $\\epsilon$ , baseline sampling algorithm Alg, time-dependent score $\\nabla\\log\\pi_{t}(\\cdot)$ , $\\Delta t$ for reverse SDE ", "page_idx": 8}, {"type": "text", "text": "Ensure: A sample $X_{0}$ from posterior distribution $\\mathsf{T}_{Q,b}\\pi$ ", "page_idx": 8}, {"type": "text", "text": "1: Calculate the blowup time by $\\begin{array}{r}{T^{*}:=\\frac{1}{2}\\log(1+\\lambda_{\\operatorname*{max}}^{-}(Q)^{-1})}\\end{array}$   \n2: Determine the number of reverse SDE steps by $\\begin{array}{r}{N=\\lceil\\frac{T^{*}-\\epsilon}{\\Delta t}\\rceil}\\end{array}$ and starting time ${\\tilde{T}}=N\\Delta t$   \n3: Use baseline sampling algorithm Alg to sample \ud835\udc4b\ud835\udc41from T\ud835\udc44\ud835\udc47\u02dc,\ud835\udc4f\ud835\udc47\u02dc\ud835\udf0b\ud835\udc47\u02dc   \n4: for $i=N$ to 1 do   \n5: Sample $Z_{i}\\sim{\\cal N}(0,I_{d})$   \n6: $X_{i-1}\\gets X_{i}+(X_{i}+2\\nabla\\log\\pi_{i\\Delta t}(X_{i}))\\Delta t+\\sqrt{2\\Delta t}\\ Z_{i}$   \n7: end for   \n8: return $X_{0}$ ", "page_idx": 8}, {"type": "text", "text": "We now validate our theoretical results by applying Algorithm 8 to the Gaussian mixture model in high dimensions, using LMC as the baseline algorithm. Same to the models considered in [13], the prior distribution is a mixture of 25 components with known means and variances (see Figure 1 for a 2D visualization and Appendix E for detailed settings). We examine three cases where $d=20,40$ , and 80. In each scenario, we set $d^{\\prime}=d$ , fix $\\kappa=20$ , and vary the SNR from $10^{-5}$ to $10^{-1}$ . We use the Sliced Wasserstein distance as a principled error metric, computed from samples obtained by our algorithms and samples directly from the analytically computed posterior Gaussian mixture. Figure 3 illustrates the comparison between LMC and LMC boosted by tilted transport. As analyzed earlier, LMC is effective when the SNR is high enough to render the target posterior strongly log-concave, but its error quickly increases as the SNR decreases. In contrast, the tilted transport enhances LMC to perform well in both low and high SNR regimes with small sampling errors. Its performance is weaker in the mid-SNR regime compared to the extremes, as predicted by Corollary 10. However, the tilted transport still improves upon LMC in this challenging regime by boosting effective SNR and simplifying the prior. ", "page_idx": 8}, {"type": "text", "text": "We further test tilted transport when $d^{\\prime}\\ <\\ d$ , in which $\\lambda_{\\operatorname*{min}}(Q_{t})$ remains zero but the signal corresponding to the non-zero eigenvalues still gets enhanced. Therefore, although it becomes more dififcult for $\\nu_{T^{*}}$ to be strongly log-concave, the tilted transport can still make the new posterior easier to sample even if it is not strongly log-concave yet. Detailed results are reported in Appendix E.1. ", "page_idx": 8}, {"type": "image", "img_path": "PhLlE8UOEv/tmp/849bec5d93c2d38ce144467d3ca78f3fa0379ad0e21baaeed7d64a6005ea468c.jpg", "img_caption": ["Figure 3: Comparison of Langevin and boosted Langevin for Gaussian mixture prior. We generate the prior, measurement and sample the posterior under 20 different instances in each setting. The sliced Wasserstein distances are plotted with the median in the middle, and the $25\\mathrm{th}$ and 75th percentiles indicated by the error bars. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "7 Discussion and Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we theoretically investigate posterior sampling using powerful priors provided by denoising oracles. We demonstrate that efifcient posterior sampling can be challenging even with a perfect denoising oracle for the prior. To achieve provable posterior sampling, one must either constrain the measurements or leverage the structural properties of the prior. We focus on the former, showing that well-conditioned measurements enable the proposed tilted transport technique to simplify the task significantly, providing a clear, verifiable condition for efifcient sampling, as demonstrated on the Ising model. Several questions remain open: Can this approach provably handle poorly-conditioned measurements, such as inpainting? Can it be extended from linear to nonlinear inverse problems? We show in Appendix D how to extend the tilted transport beyond the condition of Proposition 7 via \u2018iterated tilts\u2019, at the expense of introducing approximation errors. On the theory side, the key object underlying the success of the tilted transport is the spread $\\chi_{t}(\\pi)$ ; in particular, understanding when one can remove dimension dependence is an interesting question. We also aim to systematically evaluate the empirical performance of tilted transport in imaging and scientific computing. Appendix E.2 provides a proof-of-concept for various imaging tasks. We suspect that tilted transport could even improve existing posterior point estimate methods by boosting SNR and enabling proper uncertainty quantification through the reverse process. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] N. Anari, V. Jain, F. Koehler, H. T. Pham, and T.-D. Vuong. Entropic independence I: Modified log-Sobolev inequalities for fractionally log-concave distributions and high-temperature ising models. arXiv preprint arXiv:2106.04105, 2021.   \n[2] B. D. Anderson. Reverse-time diffusion equation models. Stochastic Processes and their Applications, 12(3):313\u2013326, 1982.   \n[3] D. Bakry and M. \u00c9mery. Diffusions hypercontractives. In J. Az\u00e9ma and M. Yor, editors, S\u00e9minaire de Probabilit\u00e9s XIX 1983/84, pages 177\u2013206. Springer Berlin Heidelberg, Berlin, Heidelberg, 1985.   \n[4] B. Barak, S. Hopkins, J. Kelner, P. K. Kothari, A. Moitra, and A. Potechin. A nearly tight sum-of-squares lower bound for the planted clique problem. SIAM Journal on Computing, 48 (2):687\u2013735, 2019.   \n[5] R. Bauerschmidt and T. Bodineau. A very simple proof of the LSI for high temperature spin systems. Journal of Functional Analysis, 276(8):2582\u20132588, 2019.   \n[6] R. Bauerschmidt, T. Bodineau, and B. Dagallier. Stochastic dynamics and the Polchinski equation: an introduction. arXiv preprint arXiv:2307.07619, 2023.   \n[7] J. Benton, V. De Bortoli, A. Doucet, and G. Deligiannidis. Linear convergence bounds for diffusion models via stochastic localization. arXiv preprint arXiv:2308.03686, 2023. [8] J. Benton, V. De Bortoli, A. Doucet, and G. Deligiannidis. Nearly d-linear convergence bounds for diffusion models via stochastic localization. In The Twelfth International Conference on Learning Representations, 2024.   \n[9] P. Bickel, B. Li, and T. Bengtsson. Sharp failure rates for the bootstrap particle filter in high dimensions. In Pushing the limits of contemporary statistics: Contributions in honor of Jayanta K. Ghosh, volume 3, pages 318\u2013330. Institute of Mathematical Statistics, 2008.   \n[10] S. G. Bobkov, I. Gentil, and M. Ledoux. Hypercontractivity of Hamilton\u2013Jacobi equations. Journal de Math\u00e9matiques Pures et Appliqu\u00e9es, 80(7):669\u2013696, 2001.   \n[11] H. J. Brascamp and E. H. Lieb. On extensions of the Brunn-Minkowski and Pr\u00e9kopa-Leindler theorems, including inequalities for log concave functions, and with an application to the diffusion equation. Journal of functional analysis, 22(4):366\u2013389, 1976.   \n[12] A. Cabezas, A. Corenflos, J. Lao, and R. Louf. Blackjax: Composable Bayesian inference in JAX, 2024.   \n[13] G. Cardoso, Y. Janati El idrissi, S. L. Corf,f and E. Moulines. Monte Carlo guided denoising diffusion models for Baysian linear inverse problems. In The Twelfth International Conference on Learning Representations, 2024.   \n[14] M. Celentano. Sudakov\u2013Fernique post-AMP, and a new proof of the local convexity of the TAP free energy. The Annals of Probability, 52(3):923\u2013954, 2024.   \n[15] S. Chatterjee and P. Diaconis. The sample size required in importance sampling. The Annals of Applied Probability, 28(2):1099\u20131135, 2018.   \n[16] H. Chen, H. Lee, and J. Lu. Improved analysis of score-based generative modeling: User-friendly bounds under minimal smoothness assumptions. In International Conference on Machine Learning, pages 4735\u20134763. PMLR, 2023.   \n[17] S. Chen, S. Chewi, J. Li, Y. Li, A. Salim, and A. Zhang. Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions. In The Eleventh International Conference on Learning Representations, 2023.   \n[18] S. Chen, S. Chewi, H. Lee, Y. Li, J. Lu, and A. Salim. The probability flow ODE is provably fast. Advances in Neural Information Processing Systems, 36, 2024.   \n[19] S. Chen, V. Kontonis, and K. Shah. Learning general gaussian mixtures with efifcient score matching. arXiv preprint arXiv:2404.18893, 2024.   \n[20] J. Choi, S. Kim, Y. Jeong, Y. Gwon, and S. Yoon. ILVR: Conditioning method for denoising diffusion probabilistic models. arXiv preprint arXiv:2108.02938, 2021.   \n[21] H. Chung, B. Sim, and J. C. Ye. Come-closer-diffuse-faster: Accelerating conditional diffusion models for inverse problems through stochastic contraction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12413\u201312422, 2022.   \n[22] H. Chung, J. Kim, M. T. Mccann, M. L. Klasky, and J. C. Ye. Diffusion posterior sampling for general noisy inverse problems. In The Eleventh International Conference on Learning Representations, 2023.   \n[23] Z. Dou and Y. Song. Diffusion posterior sampling for linear inverse problem solving: A flitering perspective. In The Twelfth International Conference on Learning Representations, 2024.   \n[24] A. El Alaoui, A. Montanari, and M. Sellke. Sampling from the Sherrington-Kirkpatrick Gibbs measure via algorithmic stochastic localization. In 2022 IEEE 63rd Annual Symposium on Foundations of Computer Science (FOCS), pages 323\u2013334. IEEE, 2022.   \n[25] R. Eldan, F. Koehler, and O. Zeitouni. A spectral condition for spectral gap: fast mixing in high-temperature ising models. Probability theory and related fields, 182(3):1035\u20131051, 2022.   \n[26] B. T. Feng, J. Smith, M. Rubinstein, H. Chang, K. L. Bouman, and W. T. Freeman. Score-based diffusion models as principled priors for inverse imaging. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10520\u201310531, 2023.   \n[27] A. Galanis, D. \u0160tefankovi\u010d, and E. Vigoda. Inapproximability of the partition function for the antiferromagnetic ising and hard-core models. Combinatorics, Probability and Computing, 25 (4):500\u2013559, 2016.   \n[28] M. Gelbrich. On a formula for the L2 wasserstein metric between measures on euclidean and Hilbert spaces. Mathematische Nachrichten, 147(1):185\u2013203, 1990.   \n[29] A. Graikos, N. Malkin, N. Jojic, and D. Samaras. Diffusion models as plug-and-play priors. Advances in Neural Information Processing Systems, 35:14715\u201314728, 2022.   \n[30] S. Gupta, A. Jalal, A. Parulekar, E. Price, and Z. Xun. Diffusion posterior sampling is computationally intractable. arXiv preprint arXiv:2402.12727, 2024.   \n[31] U. G. Haussmann and E. Pardoux. Time reversal of diffusions. The Annals of Probability, pages 1188\u20131205, 1986.   \n[32] R. Herbert. An empirical bayes approach to statistics. In Proceedings of the third berkeley symposium on mathematical statistics and probability, volume 1, pages 157\u2013163, 1956.   \n[33] J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851, 2020.   \n[34] M. D. Hoffman and A. Gelman. The No-U-Turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo. Journal of Machine Learning Research, 15(1):1593\u20131623, 2014.   \n[35] R. Holley and D. Stroock. Logarithmic Sobolev inequalities and stochastic ising models. Journal of Statistical Physics, 46(5\u20136):1159\u20131194, Mar. 1987.   \n[36] A. Jalal, M. Arvinte, G. Daras, E. Price, A. G. Dimakis, and J. Tamir. Robust compressed sensing mri with deep generative priors. Advances in Neural Information Processing Systems, 34:14938\u201314954, 2021.   \n[37] Y. Janati, A. Durmus, E. Moulines, and J. Olsson. Divide-and-conquer posterior sampling for denoising diffusion priors. arXiv preprint arXiv:2403.11407, 2024.   \n[38] R. Jordan, D. Kinderlehrer, and F. Otto. The variational formulation of the fokker\u2013planck equation. SIAM journal on mathematical analysis, 29(1):1\u201317, 1998.   \n[39] T. Karras, S. Laine, and T. Aila. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4401\u20134410, 2019.   \n[40] B. Kawar, G. Vaksman, and M. Elad. SNIPS: Solving noisy inverse problems stochastically. Advances in Neural Information Processing Systems, 34:21757\u201321769, 2021.   \n[41] B. Kawar, M. Elad, S. Ermon, and J. Song. Denoising diffusion restoration models. Advances in Neural Information Processing Systems, 35:23593\u201323606, 2022.   \n[42] D. Kunisky. Optimality of Glauber dynamics for general-purpose Ising model sampling and free energy approximation. Proceedings of the 2024 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 5013\u20135028, 2023.   \n[43] D. Kunisky, A. S. Wein, and A. S. Bandeira. Notes on computational hardness of hypothesis testing: Predictions using the low-degree likelihood ratio. In ISAAC Congress (International Society for Analysis, its Applications and Computation), pages 1\u201350. Springer, 2019.   \n[44] H. Lee, J. Lu, and Y. Tan. Convergence for score-based generative modeling with polynomial complexity. Advances in Neural Information Processing Systems, 35:22870\u201322882, 2022.   \n[45] H. Lee, J. Lu, and Y. Tan. Convergence of score-based generative modeling for general data distributions. In International Conference on Algorithmic Learning Theory, pages 946\u2013985. PMLR, 2023.   \n[46] Y.-A. Ma, T. Chen, and E. Fox. A complete recipe for stochastic gradient MCMC. Advances in neural information processing systems, 28, 2015.   \n[47] M. Mardani, J. Song, J. Kautz, and A. Vahdat. A variational perspective on solving inverse problems with diffusion models. arXiv preprint arXiv:2305.04391, 2023.   \n[48] X. Meng and Y. Kabashima. Diffusion model based posterior sampling for noisy linear inverse problems. arXiv preprint arXiv:2211.12343, 2022.   \n[49] A. Montanari and Y. Wu. Provably efifcient posterior sampling for sparse linear regression via measure decomposition. arXiv preprint arXiv:2406.19550, 2024.   \n[50] B. Oksendal. Stochastic differential equations: an introduction with applications. Springer Science & Business Media, 2013.   \n[51] F. Otto and C. Villani. Generalization of an inequality by Talagrand and links with the logarithmic Sobolev inequality. Journal of Functional Analysis, 173(2):361\u2013400, 2000.   \n[52] G. O. Roberts and R. L. Tweedie. Exponential convergence of langevin distributions and their discrete approximations. Bernoulli, pages 341\u2013363, 1996.   \n[53] C. Saharia, W. Chan, H. Chang, C. Lee, J. Ho, T. Salimans, D. Fleet, and M. Norouzi. Palette: Image-to-image diffusion models. In ACM SIGGRAPH 2022 conference proceedings, pages 1\u201310, 2022.   \n[54] S. Shoushtari, J. Liu, and U. S. Kamilov. Dolph: Diffusion models for phase retrieval. arXiv preprint arXiv:2211.00529, 2022.   \n[55] J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pages 2256\u20132265. PMLR, 2015.   \n[56] J. Song, A. Vahdat, M. Mardani, and J. Kautz. Pseudoinverse-guided diffusion models for inverse problems. In International Conference on Learning Representations, 2022.   \n[57] J. Song, Q. Zhang, H. Yin, M. Mardani, M.-Y. Liu, J. Kautz, Y. Chen, and A. Vahdat. Loss-guided diffusion models for plug-and-play controllable generation. In International Conference on Machine Learning, pages 32483\u201332498. PMLR, 2023.   \n[58] Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020.   \n[59] Y. Song, C. Durkan, I. Murray, and S. Ermon. Maximum likelihood training of score-based diffusion models. Advances in neural information processing systems, 34:1415\u20131428, 2021.   \n[60] Y. Song, L. Shen, L. Xing, and S. Ermon. Solving inverse problems in medical imaging with score-based generative models. In International Conference on Learning Representations, 2022.   \n[61] Y. Sun, Z. Wu, Y. Chen, B. T. Feng, and K. L. Bouman. Provable probabilistic imaging using score-based generative priors. IEEE Transactions on Computational Imaging, 2024.   \n[62] L. Wu, B. Trippe, C. Naesseth, D. Blei, and J. P. Cunningham. Practical and asymptotically exact conditional sampling in diffusion models. In Advances in Neural Information Processing Systems, volume 36, pages 31372\u201331403, 2023.   \n[63] X. Xu and Y. Chi. Provably robust score-based diffusion posterior sampling for plug-and-play image reconstruction. arXiv preprint arXiv:2403.17042, 2024. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Proof of Proposition 4 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Let $\\delta\\,>\\,0$ and $\\bar{\\pi}$ be the uniform measure in the $d$ -dimensional hypercube. Consider a Gaussian mixture prior $\\pi$ defined as $\\pi=\\bar{\\pi}\\star\\gamma_{\\delta}$ . ", "page_idx": 13}, {"type": "text", "text": "Since both $\\bar{\\pi}$ and $\\gamma_{\\delta}$ are product measures, it follows that $\\pi$ is also a product measure, and therefore its denoising oracle $\\mathrm{DO}_{\\pi}$ is explicitly given by $\\mathrm{DO}_{\\pi}(y,t)_{i}=\\psi(y_{i},t)$ , with ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\psi(\\nu,t)=\\displaystyle\\int_{\\mathbb{R}}u q_{\\nu,t}(u)\\mathrm{d}u\\,,}\\\\ &{q_{\\nu,t}(u)=Z^{-1}\\left(e^{-\\frac{1}{2}(\\delta^{-2}(u-1)^{2}+t^{-2}(\\nu-u)^{2})}+e^{-\\frac{1}{2}(\\delta^{-2}(u+1)^{2}+t^{-2}(\\nu-u)^{2})}\\right)\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Observe that $q_{\\nu,t}$ is the density of a Gaussian mixture in $\\mathbb{R}$ of the form $\\alpha\\mathcal{N}(b_{-},\\sigma)+(1-\\alpha)\\mathcal{N}(b_{+},\\sigma)$ , with parameters ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sigma^{-2}=\\delta^{-2}+t^{-2}}\\\\ &{b_{\\pm}=\\frac{\\pm\\delta^{-2}+t^{-2}\\nu}{\\sigma^{-2}}}\\\\ &{\\qquad\\alpha=\\frac{e^{\\frac{(\\delta^{-2}+t^{-2}\\nu)^{2}}{2\\sigma^{2}}}}{e^{\\frac{(\\delta^{-2}+t^{-2}\\nu)^{2}}{2\\sigma^{2}}}+e^{\\frac{(-\\delta^{-2}+t^{-2}\\nu)^{2}}{2\\sigma^{2}}}}\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "and thus $\\psi(\\nu,t)=\\alpha b_{-}+(1-\\alpha)b_{+}$ . ", "page_idx": 13}, {"type": "text", "text": "Let us now denote by $\\mu_{Q}$ the target Ising model, supported in the $d$ -dimensional hypercube, and define the approximation $\\mu_{Q}^{\\sigma}:=\\mathbb{T}_{Q^{\\pi}}$ . Suppose that there is an algorithm $\\mathcal{A}$ that leverages the denoising oracle of $\\pi$ that can efifciently sample from $\\mu_{Q}^{\\sigma}$ : its law $\\hat{\\mu}$ satisfies $\\mathrm{TV}(\\mu_{Q}^{\\sigma},\\hat{\\mu})\\,\\le\\,\\epsilon$ with runtime polynomial in $d$ and $\\log(\\epsilon^{-1})$ . ", "page_idx": 13}, {"type": "text", "text": "Let now $R(x)=\\mathrm{sign}(x)$ , and consider the sampler $R\\circ\\mathcal{A}$ , which is now supported in the hypercube. By the triangle and data-processing inequalities, we directly have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{TV}(R_{\\#}\\hat{\\mu},\\mu_{Q})\\leq\\mathrm{TV}(R_{\\#}\\hat{\\mu},R_{\\#}\\mu_{Q}^{\\delta})+\\mathrm{TV}(R_{\\#}\\mu_{Q}^{\\delta},\\mu_{Q})}\\\\ &{\\qquad\\qquad\\qquad\\leq\\mathrm{TV}(\\hat{\\mu},\\mu_{Q}^{\\delta})+\\mathrm{TV}(R_{\\#}\\mu_{Q}^{\\delta},\\mu_{Q})}\\\\ &{\\qquad\\qquad\\qquad\\leq\\epsilon+\\mathrm{TV}(R_{\\#}\\mu_{Q}^{\\delta},\\mu_{Q}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "It remains to bound the second term in the RHS. We have to compare two measures in the hypercube. For $\\sigma\\in\\mathcal{H}_{d}:=\\{\\pm1\\}^{d}$ , they are given respectively by ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\mu_{Q}(\\sigma)=\\frac{1}{Z}e^{-\\frac{1}{2}\\sigma^{\\top}Q\\sigma}\\,,}}\\\\ {{\\displaystyle R_{\\#}\\mu_{Q}^{\\delta}(\\sigma)=\\frac{1}{\\tilde{Z}}\\sum_{z\\in\\mathcal{H}_{d}}\\int_{R(x)=\\sigma}e^{-\\frac{1}{2}(x^{\\top}Q x+\\delta^{-2}\\|x-z\\|^{2})}\\mathrm{d}x\\,.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Applying the Laplace approximation into each integral we obtain, as $\\delta\\rightarrow0$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\int_{R(x)=\\sigma}e^{-{\\frac{1}{2}}(x^{\\top}Q x+\\delta^{-2}\\|x-z\\|^{2})}\\mathrm{d}x={\\left\\{\\begin{array}{l l}{\\sim C_{d,\\delta}e^{-{\\frac{1}{2}}\\sigma^{\\top}Q\\sigma}}&{{\\mathrm{~if~}}z=\\sigma\\;,}\\\\ {\\sim C_{d,\\delta}e^{-{\\frac{1}{2}}(\\sigma\\oplus z)^{\\top}Q(\\sigma\\oplus z)}e^{-{\\frac{|\\sigma-z|}{2\\delta^{2}}}}}&{{\\mathrm{~otherwise~}}\\;,}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\sigma\\oplus z$ is the XOR, and $|\\sigma-z|$ is the Hamming distance. We thus have, for any $\\sigma\\in\\mathcal{H}_{d}$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\left|\\tilde{C}R_{\\#}\\mu_{Q}^{\\delta}(\\sigma)-e^{-\\frac{1}{2}\\sigma^{\\top}Q\\sigma}\\right|\\leq2^{d}e^{d\\lambda_{\\operatorname*{min}}(Q)/2}e^{-\\frac{1}{2}\\delta^{-2}}}\\\\ &{}&{\\leq e^{-\\frac{1}{2}\\sigma^{\\top}Q\\sigma}2^{d}e^{d(\\lambda_{\\operatorname*{min}}(Q)+\\lambda_{\\operatorname*{max}}(Q))/2}e^{-\\frac{1}{2}\\delta^{-2}}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "It follows that we can write $R_{\\#}\\mu_{\\cal Q}^{\\delta}(\\sigma)$ as ", "page_idx": 13}, {"type": "equation", "text": "$$\nR_{\\#}\\mu_{Q}^{\\delta}(\\sigma)=C(e^{-\\frac{1}{2}\\sigma^{\\top}Q\\sigma}+\\eta_{\\sigma})\\;,\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "with a relative error ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{\\left\\vert\\eta_{\\sigma}\\right\\vert}{e^{-\\frac{1}{2}\\sigma^{\\top}Q\\sigma}}\\leq e^{d\\left(1+\\frac{1}{2}\\left({\\lambda_{\\operatorname*{min}}(Q)+\\lambda_{\\operatorname*{max}}(Q)}\\right)\\right)-\\delta^{-2}/2}:=\\theta\\;.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "It follows that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{TV}(R_{\\#}\\mu_{Q}^{\\delta},\\mu_{Q})=O(\\theta)\\;,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and thus if $\\begin{array}{r}{\\delta\\ll\\frac{1}{\\sqrt{d}}}\\end{array}$ , we have a negligible TV approximation. ", "page_idx": 14}, {"type": "text", "text": "B Proofs of Section 4 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Proof of Theorem 5 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proof. We denote the time-dependent score function $\\nabla\\log\\pi_{t}(x)$ by $s_{t}(x)$ . As derived in Section 2, if we initialize $X_{\\tau}$ according to density $\\rho_{\\tau}$ and run the reverse SDE eq. (5), the density of $X_{t}$ for $t\\le\\tau$ , denoted by $\\rho_{t}$ , satisfies the backward PDE: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\partial_{t}\\rho_{t}=\\nabla\\cdot\\big((\\boldsymbol{x}+2\\boldsymbol{s}_{t})\\rho_{t}\\big)-\\Delta\\rho_{t}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We need to verify that $\\nu_{t}$ satisfies the above PDE.Note that a general positive function $\\rho_{t}$ satisfies this PDE is equivalent to that $h_{t}=\\log\\rho_{t}$ satisfies the following Hamilton-Jacobi PDE ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\partial_{t}h_{t}=d+2\\nabla\\cdot s_{t}+\\nabla h_{t}\\cdot(x+2s_{t})-(\\Delta h_{t}+\\|\\nabla h_{t}\\|^{2}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "By definition, we know $h_{t}=\\log\\pi_{t}$ satisfies the above PDE, and we need to prove $h_{t}=\\log\\nu_{t}=$ $\\begin{array}{r}{\\log\\pi_{t}-\\frac{1}{2}x^{\\top}Q_{t}x+x^{\\top}b_{t}+F(t)}\\end{array}$ satisfies this PDE as well. Here $F(t)$ denotes the normalizing constant. Taking the difference between two equations, we need ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\displaystyle\\frac{1}{2}x^{\\top}\\dot{Q}_{t}x+x^{\\top}\\dot{b}_{t}+\\dot{F}=(-Q_{t}x+b_{t})\\cdot(x+2s_{t})+\\mathrm{trace}(Q_{t})+\\|s_{t}\\|^{2}-\\|s_{t}-Q_{t}x+b_{t}\\|^{2}}\\\\ {\\Leftrightarrow}&{-\\displaystyle\\frac{1}{2}x^{\\top}\\dot{Q}_{t}x+x^{\\top}\\dot{b}_{t}+\\dot{F}=x^{\\top}(-Q_{t}-Q_{t}^{\\top}Q_{t})x+x^{\\top}(b_{t}+2Q_{t}^{\\top}b_{t})+\\|b_{t}\\|^{2}+\\mathrm{trace}(Q_{t})}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which can be satisfied by the ODE dynamics (9). ", "page_idx": 14}, {"type": "text", "text": "B.2 Derivation of Solution to eq. (9) ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Sanity Check for the Motivating Example. In the denoising setting, we have $\\begin{array}{r}{Q_{0}=\\frac{1}{\\sigma^{2}}I_{d},b_{0}=\\frac{1}{\\sigma^{2}}y}\\end{array}$ . The ODE (9) has the explicit solution ", "page_idx": 14}, {"type": "equation", "text": "$$\nQ_{t}={\\frac{e^{2t}}{1+\\sigma^{2}-e^{2t}}}I_{d}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Note that this solution has a finite blow-up time when $1+\\sigma^{2}-e^{2t}\\,\\rightarrow\\,0^{+}$ , which is exactly at $T^{*}=\\textstyle{\\frac{1}{2}}\\log(1+\\sigma^{2})$ , as derived in the main text by matching the SNR. As $t\\to T^{*}$ , $Q_{t}\\to\\infty I_{d}$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\nu_{t}=\\exp\\left(f_{t}(x)-\\frac{1}{2}x^{\\top}Q_{t}x+x^{\\top}b_{t}+F(t)\\right)\\rightarrow N(Q_{t}^{-1}b_{t},Q_{t}^{-1}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "To see the limit of $\\boldsymbol{Q}_{t}^{-1}\\boldsymbol{b}_{t}$ , we only need to consider the ODE for each component since $Q$ is diagonal. So we view the above ODE as scalar ODEs. Considering the dynamics of ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t}\\frac{r}{Q}=\\frac{\\dot{r}Q-r\\dot{Q}}{Q^{2}}=-\\frac{r}{Q},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "gives $Q_{t}^{-1}b_{t}=e^{-t}Q_{0}^{-1}b_{0}$ . Therefore ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{t\\rightarrow(T^{*})^{-}}Q_{t}^{-1}b_{t}=e^{-T^{*}}Q_{0}^{-1}b_{0}=e^{-T^{*}}y,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which matches the initial condition derived in the main text for the denoising case. Furthermore, we can explicitly verify that the intermediate distribution of $X_{t}$ by running the reverse SDE from $\\nu_{T}^{*}$ is $\\nu_{t}$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\np\\left(X_{t}|X_{T^{*}}=e^{-T^{*}}y\\right)\\;\\propto\\;p(X_{t})p(X_{T^{*}}=e^{-T^{*}}y|X_{t})}\\\\ {\\;\\propto\\;\\pi_{t}(X_{t})\\exp\\left(-\\frac{1}{2}\\frac{\\|e^{-T^{*}}y-e^{-(T^{*}-t)}x\\|^{2}}{1-e^{-2(T^{*}-t)}}\\right)}\\\\ {\\;=\\pi_{t}(X_{t})\\exp\\left(-\\frac{1}{2}\\frac{\\|e^{-t}y-x\\|^{2}}{e^{2(T^{*}-t)}-1}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "To match the form of $\\nu_{t}$ , we have $\\begin{array}{r}{Q_{t}=\\frac{1}{e^{2(T^{*}-t)}-1}=\\frac{e^{2t}}{1+\\sigma^{2}-e^{2t}},Q_{t}^{-1}b_{t}=e^{-t}y.}\\end{array}$ , which are the solutions of the ODE (9). ", "page_idx": 15}, {"type": "text", "text": "Solution to eq. (9). We recall that the observation operator $A\\,\\in\\,\\mathbb{R}^{d^{\\prime}\\times d}$ has a general singular value decomposition form $A=U\\Sigma V^{\\top}$ with non-zero singular values $\\lambda_{1}\\geq\\lambda_{2}\\geq\\cdot\\cdot\\cdot\\geq\\lambda_{d^{\\prime}}>0$ . By definition, we have $Q_{0}=V\\mathrm{diag}(\\lambda_{1}^{2}/\\sigma^{2},\\cdot\\cdot\\cdot\\,,\\lambda_{d^{\\prime}}^{2}/\\sigma^{2},0,\\cdot\\cdot\\cdot\\,,0)V^{\\top}$ . By left multiplying $V^{\\top}$ and right multiplying $V$ to the first ODE in (9), we can diagonalize it to scalar equations $\\dot{q}_{t}=2(1+q_{t})q_{t}$ for each diagonal entry. Solving this ODE gives ", "page_idx": 15}, {"type": "equation", "text": "$$\nQ_{t}=V\\mathrm{diag}\\left(\\frac{e^{2t}}{1+\\sigma^{2}/\\lambda_{1}^{2}-e^{2t}},\\cdots,\\frac{e^{2t}}{1+\\sigma^{2}/\\lambda_{d^{\\prime}}^{2}-e^{2t}},0,\\cdots\\,,0\\right)V^{\\top}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Here we explain how to solve $b_{t}$ from eq. (9). We denote $V\\,=\\,\\left[\\nu_{1},\\cdots\\,,\\nu_{d}\\right]$ , in which $\\nu_{i}$ are eigenvectors of $Q$ (and $Q_{t}$ as well), and denote the eigenvalues of $Q_{t}$ $\\mid_{t}(0\\leq t<T^{*})$ by ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\tilde{\\lambda}_{i}(t)=\\left\\{\\frac{e^{2t}}{1+\\sigma^{2}/\\lambda_{i}^{2}-e^{2t}},\\quad1\\leq i\\leq d^{\\prime}\\right.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By definition, we know $\\tilde{\\lambda_{i}}$ satisfies the ODE ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\dot{\\tilde{\\lambda}}=2(1+\\tilde{\\lambda})\\tilde{\\lambda}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We rewrite the solution $\\begin{array}{r}{b_{t}=\\sum_{i}^{d}\\xi_{i}(t)\\nu_{i}}\\end{array}$ and aim to solve $\\xi_{i}(t)$ . From $\\begin{array}{r}{b_{0}=V(\\frac{1}{\\sigma^{2}}\\Sigma^{\\top}U^{\\top}y)}\\end{array}$ , we have the initial condition ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\xi_{i}(0)=\\left\\{\\frac{\\lambda_{i}}{\\sigma^{2}}(U^{\\top}y)_{i},\\begin{array}{l l}{1\\leq i\\leq d^{\\prime}}\\\\ {d^{\\prime}+1\\leq i\\leq d}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Taking the inner product between $\\nu_{i}$ and both sides of the ODE $\\dot{r}_{t}=(I+2Q_{t})b_{t}$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\dot{\\xi}_{i}(t)=(1+2\\tilde{\\lambda}_{i}(t))\\xi_{i}(t).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore, for $d^{\\prime}+1\\leq i\\leq d,\\xi_{i}(t)=0$ . For $1\\leq i\\leq d^{\\prime}$ , same to the derivation in eq. (32), we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t}\\frac{\\xi_{i}}{\\tilde{\\lambda}_{i}}=-\\frac{\\xi_{i}}{\\tilde{\\lambda}_{i}},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which gives ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\frac{\\xi_{i}(t)}{\\tilde{\\lambda}_{i}(t)}=e^{-t}\\frac{\\xi_{i}(0)}{\\tilde{\\lambda}_{i}(0)},}\\\\ &{\\Rightarrow(\\frac{e^{2t}}{1+\\sigma^{2}/\\lambda_{i}^{2}-e^{2t}})^{-1}\\xi_{i}(t)=e^{-t}\\frac{\\sigma^{2}}{\\lambda_{i}^{2}}\\frac{\\lambda_{i}}{\\sigma^{2}}(U^{\\top}y)_{i},}\\\\ &{\\Rightarrow\\xi_{i}(t)=\\frac{e^{t}}{\\lambda_{i}(1+\\sigma^{2}/\\lambda_{i}^{2}-e^{2t})}(U^{\\top}y)_{i}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof of Corollary 6. Given Theorem 5, we only need to prove that sampling from ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\nu_{t}=\\mathsf{T}_{Q_{t},b_{t}}\\pi_{t}\\ \\propto\\ \\pi_{t}(x)\\exp\\left(-\\frac{1}{2}x^{\\mathsf{T}}Q_{t}x+x^{\\mathsf{T}}b_{t}\\right)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "is equivalent to sampling from a new posterior. Taking $\\pi_{t}$ as the corresponding prior, we only need to show that the factor $\\begin{array}{r}{\\exp\\left(-\\frac{1}{2}x^{\\top}Q_{t}x+x^{\\top}b_{t}\\right)}\\end{array}$ is the likelihood of certain observation model in the form of $\\tilde{y}=A_{t}x+w$ with $w\\sim\\gamma_{d}$ . To end, we need to ensure ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\exp\\left(-\\frac{1}{2}x^{\\top}Q_{t}x+x^{\\top}b_{t}\\right)\\;\\propto\\;\\exp(-\\frac{1}{2}\\|A_{t}x-\\widetilde{y}\\|^{2})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Choosing $A_{t}$ in the standard SVD form $A_{t}=\\Sigma_{t}V^{\\top}$ where the singular values of $A_{t}$ (the diagonal entries of $\\Sigma_{t}$ ) are $\\frac{e^{t}}{(1+\\sigma^{2}/\\lambda_{i}^{2}-e^{2t})^{1/2}}$ for $1\\leq i\\leq d^{\\prime}$ , the quadratic term is matched. Matching the first order term requires $b_{t}=\\dot{A}^{\\top}\\tilde{y}=V\\Sigma_{t}^{\\top}\\tilde{y}$ . Further matching the coefifcients in the basis of $V$ requires that ", "page_idx": 16}, {"type": "equation", "text": "$$\n(\\Sigma_{t}^{\\top}\\tilde{y})_{i}=\\xi_{i}(t)=\\frac{e^{t}}{\\lambda_{i}(1+\\sigma^{2}/\\lambda_{i}^{2}-e^{2t})}(U^{\\top}y)_{i},\\quad1\\le i\\le d^{\\prime}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "It is easy to verify that $\\tilde{\\mathrm{y}}=\\Sigma_{t}^{\\prime}U^{\\top}\\mathrm{y}$ with $\\begin{array}{r}{\\Sigma_{t}^{\\prime}=\\operatorname{diag}\\left(\\frac{1}{(\\sigma^{2}+\\lambda_{1}^{2}(1-e^{2t}))^{1/2}},\\cdot\\cdot\\cdot,\\frac{1}{(\\sigma^{2}+\\lambda_{d^{\\prime}}^{2}(1-e^{2t}))^{1/2}}\\right)}\\end{array}$ satisfies the above requirement. ", "page_idx": 16}, {"type": "text", "text": "Remark 11. Also, one can directly use the backward transport equation (3) to generate samples with the probability flow ODE [58] backward in time ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{d}X_{t}^{\\leftarrow}=(-X_{t}^{\\leftarrow}-\\nabla\\log\\pi_{t}(X_{t}^{\\leftarrow}))\\mathrm{d}t\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "from $X_{T}^{\\leftarrow}\\sim\\gamma_{d}$ . Combining the fact that $\\nu_{t}$ satisfies the $P D E\\left(5\\right)$ and $\\Delta\\nu_{t}=\\boldsymbol{\\nabla}{\\cdot}\\big(\\boldsymbol{\\nabla}\\nu_{t}\\big)=\\boldsymbol{\\nabla}{\\cdot}\\big(s_{t}\\!-\\!\\boldsymbol{Q}_{t}\\boldsymbol{x}\\!+\\!\\boldsymbol{b}_{t}\\big)$ , we have that $\\nu_{t}$ also satisfies the transport equation $\\partial_{t}p_{t}=\\nabla\\!\\cdot\\!\\big((s_{t}{+}(I{+}Q_{t})x{-}b_{t})p_{t}\\big)$ . Therefore, for any $t<T^{*}$ , by initializing $X_{t}\\sim\\nu_{t}$ and run the reverse ODE $\\mathrm{d}X_{t}^{\\leftarrow}=\\big(-(I\\!+\\!\\mathcal{Q}_{t})X_{t}^{\\leftarrow}-\\nabla\\log\\pi_{t}(X_{t}^{\\leftarrow})\\!+\\!b_{t}\\big)\\mathrm{d}t$ then $X_{0}^{\\leftarrow}$ also gives the desired posterior. However, we note that unlike the reverse $S D E$ case, the corresponding transport PDE and the vector field in the reverse $O D E$ case are different from those used in the prior data generation. As discussed below, both $Q_{t}$ and $b_{t}$ are singular near $T^{*}$ . Therefore running the reverse SDE might be preferrable for better numerical stability. ", "page_idx": 16}, {"type": "text", "text": "C Proofs of Section 5 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "C.1 Proof of Proposition 7 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof of Proposition 7. By definition, we need to show ", "page_idx": 16}, {"type": "equation", "text": "$$\n-\\lambda_{\\operatorname*{min}}(Q_{T^{*}})+\\operatorname*{sup}_{x}\\lambda_{\\operatorname*{max}}(\\nabla^{2}\\log\\pi_{T^{*}}(x))<0.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "From the argument in the main text, we know $\\begin{array}{r}{T^{*}=\\frac{1}{2}\\log(1+\\lambda_{\\operatorname*{max}}(Q)^{-1})}\\end{array}$ , and thus ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\lambda_{\\operatorname*{min}}(Q_{T^{*}})=\\frac{1+\\lambda_{\\operatorname*{max}}(Q)^{-1}}{\\lambda_{\\operatorname*{min}}(Q)^{-1}-\\lambda_{\\operatorname*{max}}(Q)^{-1}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "From Corollary 13, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{x}\\lambda_{\\operatorname*{max}}(\\nabla^{2}\\log\\pi_{T^{*}}(x))\\,\\le\\,(1+\\|Q\\|)\\,\\bigl(\\|Q\\|\\chi_{\\|Q\\|}(\\pi)-1\\bigr)\\;\\;.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Let $m=\\lambda_{\\operatorname*{min}}(Q)$ . Therefore, we can guarantee that $\\nu_{T^{*}}$ is strongly log-concave if ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{1+\\|Q\\|^{-1}}{m^{-1}-\\|Q\\|^{-1}}>(1+\\|Q\\|)\\left(\\|Q\\|\\chi_{\\|Q\\|}(\\pi)-1\\right)\\;,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "or equivalently ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\chi_{\\parallel Q\\parallel}(\\pi)<\\|Q\\|^{-1}\\frac{\\kappa^{2}(A)}{\\kappa^{2}(A)-1}\\;.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Lemma 12 (Hessian of Gaussian Mixture Potential). Let $\\pi=\\mu\\star\\gamma_{\\Sigma}$ be a Gaussian mixture. Then $\\nabla^{2}\\log\\pi(x)=\\Sigma^{-1}\\left(\\operatorname{Cov}\\left[\\mathsf{T}_{\\Sigma^{-1},\\Sigma^{-1}x}\\mu\\right]\\Sigma^{-1}-I\\right)$ . ", "page_idx": 17}, {"type": "text", "text": "Proof. Let us first compute the score $\\nabla\\log\\pi$ . By definition we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla\\log\\pi(x)=-\\Sigma^{-1}\\left(x-\\frac{\\displaystyle\\int y\\mu(y)e^{-\\frac{1}{2}(x-y)^{\\top}\\Sigma^{-1}(x-y)}d y}{\\displaystyle\\int\\mu(y)e^{-\\frac{1}{2}(x-y)^{\\top}\\Sigma^{-1}(x-y)}d y}\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\quad=-\\Sigma^{-1}(x-\\mathbb{E}\\left[\\mathsf{T}_{\\Sigma^{-1},\\Sigma^{-1}x}\\mu\\right])\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and thus ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla^{2}\\log\\pi(x)=\\Sigma^{-1}\\left(\\mathbf{Cov}\\left[\\mathsf{T}_{\\Sigma^{-1},\\Sigma^{-1}x}\\mu\\right]\\Sigma^{-1}-I\\right)\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where we defined $\\operatorname{Cov}[\\mu]=\\mathbb{E}_{x\\sim\\mu}[x x^{\\top}]-(\\mathbb{E}_{x\\sim\\mu}x)(\\mathbb{E}_{x\\sim\\mu}x)^{\\top}$ . ", "page_idx": 17}, {"type": "text", "text": "Corollary 13. In particular, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{x}\\lambda_{\\operatorname*{max}}(\\nabla^{2}\\log\\pi_{T^{*}}(x))\\,\\le\\,(1+\\|Q\\|)\\,\\bigl(\\|Q\\|\\chi_{\\|Q\\|}(\\pi)-1\\bigr)\\;\\;.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. From Lemma 12 and $\\pi_{t}=\\mathsf{C}_{1-e^{-2t}}(\\mathsf{D}_{e^{t}}\\pi)=\\mathsf{D}_{e^{t}}\\pi\\star\\gamma_{1-e^{-2t}}$ , we directly have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\nabla^{2}\\log\\pi_{t}(x)=(1-e^{-2t})^{-1}\\left((1-e^{-2t})^{-1}\\mathrm{Cov}\\left[\\mathsf{T}_{(1-e^{-2t})^{-1},(1-e^{-2t})^{-1}x}(\\mathsf{D}_{e^{t}}\\pi)\\right]-I\\right)\\ .\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Now, using the commutation property between the isotropic tilt and the dilation $\\mathsf{D}_{\\alpha}\\mathsf{T}_{\\eta,\\theta}=\\mathsf{T}_{\\alpha^{2}\\eta,\\alpha\\theta}\\mathsf{D}_{\\alpha}$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Cov}\\left[\\mathsf{T}_{(1-e^{-2t})^{-1},(1-e^{-2t})^{-1}x}(\\mathsf{D}_{e^{t}}\\pi)\\right]=\\mathrm{Cov}\\left[\\mathsf{D}_{e^{t}}\\mathsf{T}_{(e^{2t}-1)^{-1},e^{-t}(1-e^{-2t})^{-1}x}\\pi\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=e^{-2t}\\mathrm{Cov}\\left[\\mathsf{T}_{(e^{2t}-1)^{-1},e^{-t}(1-e^{-2t})^{-1}x}\\pi\\right]\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and therefore ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{x}\\left\\|\\mathrm{Cov}\\left[\\mathsf{T}_{(1-e^{-2t})^{-1},(1-e^{-2t})^{-1}x}(\\mathsf{D}_{e^{t}}\\pi)\\right]\\right\\|\\leq e^{-2t}\\chi_{(e^{2t}-1)^{-1}}(\\pi)\\ .\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Using that $e^{2T^{*}}-1=\\|Q\\|^{-1}$ , we thus obtain ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{x}\\lambda_{\\operatorname*{max}}(\\nabla^{2}\\log\\pi_{T^{*}}(x))\\,\\le\\,(1+\\|Q\\|)\\,\\bigl(\\|Q\\|\\chi_{\\|Q\\|}(\\pi)-1\\bigr)\\;\\;.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Lemma 14 (Isotropic Tilt of a Gaussian Mixture). I $\\boldsymbol{\\epsilon}_{\\pi}=\\boldsymbol{\\mu}\\star\\gamma_{\\delta}$ , then ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathsf{T}_{t I,z}\\pi=\\tilde{\\mu}\\star\\gamma_{\\sigma^{2}}\\,,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\sigma^{-2}=\\delta^{-1}+t$ and ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\tilde{\\mu}(\\tilde{y})\\propto\\mu((\\sigma^{-2}\\tilde{y}-z)\\delta)e^{\\frac{1}{2}\\left(\\sigma^{-2}\\|\\tilde{y}\\|^{2}-\\delta\\|\\sigma^{-2}\\tilde{y}-z\\|^{2}\\right)}\\ .\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. By definition, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathsf{T}_{t I,z}\\pi\\propto\\int\\mathrm{d}\\mu(y)e^{-\\frac{1}{2}t\\|x\\|^{2}+x\\cdot z-\\frac{1}{2}\\delta^{-1}\\|x-y\\|^{2}}\\;.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By expressing ", "page_idx": 17}, {"type": "equation", "text": "$$\n-{\\frac{1}{2}}t\\|x\\|^{2}+x\\cdot z-{\\frac{1}{2}}\\delta^{-1}\\|x-y\\|^{2}=-{\\frac{1}{2}}\\sigma^{-2}\\|x-{\\tilde{y}}\\|^{2}+C\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\sigma}^{-2}={\\delta}^{-1}+t~,}}\\\\ {{\\displaystyle\\quad{\\tilde{y}}=\\frac{{\\delta}^{-1}y+z}{{\\delta}^{-1}+t}~,}}\\\\ {{\\displaystyle\\quad C=\\frac{1}{2}\\left[\\sigma^{-2}\\|\\tilde{y}\\|^{2}-{\\delta}^{-1}\\|y\\|^{2}\\right]~,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which gives the desired result after performing the afifne change of variables from $y$ to $\\tilde{y}$ . ", "page_idx": 17}, {"type": "text", "text": "Proof of Examples 8. The first example is immediate, after observing that $\\mathsf{T}_{t}\\gamma$ is a Gaussian of variance $(1+t\\bar{)}^{-1}$ . For the Gaussian mixture example, we observe from Lemma 14 that $\\mathsf{T}_{t}(\\mu\\star\\gamma_{\\delta})$ is a Gaussian mixture of variance $\\left(t+\\delta^{-1}\\right)^{-1}$ , where the mixture distribution is supported in a ball of radius \ud835\udc45\ud835\udeff\ud835\udeff\u22121\u22121\ud835\udc61 . Moreover, the covariance of a homogeneous mixture of the form $\\mu\\star\\gamma_{\\Sigma}$ is $\\Sigma+\\operatorname{Cov}(\\mu)$ . If $\\mu$ is a product measure, we observe that the isotropic tilt $\\mathsf{T}_{t}\\mu$ is also a product measure, and therefore its covariance is diagonal. Finally, by the previous argument, if $\\pi$ is the uniform measure on the hypercube, then $\\begin{array}{r}{\\chi_{t}(\\pi)=\\chi_{t}\\big(\\frac{1}{2}(\\delta_{-1}+\\delta_{+1})\\big)=1}\\end{array}$ . \u25a0 ", "page_idx": 18}, {"type": "text", "text": "Proof of Corollary 10. We plug the spre function $\\begin{array}{r}{\\chi_{t}(\\pi)\\;=\\;\\chi_{t}(\\mu\\star_{\\cdot}\\gamma_{\\delta})\\;\\leq\\;\\left(\\frac{R}{1+\\delta t}\\right)^{2}+\\frac{\\delta}{1+\\delta t}}\\end{array}$ from Example 8 (ii) into eq. (12) to get (we use $\\kappa$ to denote $\\kappa(A)$ for simplicity ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\|Q\\|^{-1}\\frac{\\kappa^{2}}{(\\kappa^{2}-1)}>\\left(\\cfrac{R}{1+\\delta\\|Q\\|}\\right)^{2}+\\cfrac{\\delta}{1+\\delta\\|Q\\|}}\\\\ &{\\Leftrightarrow(1+\\delta\\|Q\\|)\\left(\\cfrac{((1+\\delta\\|Q\\|))\\kappa^{2}}{\\|Q\\|(\\kappa^{2}-1)}-\\delta\\right)>R^{2}}\\\\ &{\\Leftrightarrow\\frac{((1+\\delta\\|Q\\|)(\\kappa^{2}+\\delta\\|Q\\|)}{\\|Q\\|(\\kappa^{2}-1)}>R^{2}}\\\\ &{\\Leftrightarrow\\frac{(1+\\delta\\mathrm{SNR}^{2})(\\delta\\kappa^{2}+\\mathrm{SNR}^{-2})}{\\kappa^{2}-1}>R^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "C.2 Exponential Complexity of Importance Sampling in High SNR Regime ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "As shown in the main text, the proposed boosted posterior provably works for both low SNR and high SNR regimes. In this section, we formally argue that the importance sampling method is a nature baseline for posterior sampling with a large noise (low SNR regime) , but can suffer from exponential complexity when the SNR is high. ", "page_idx": 18}, {"type": "text", "text": "In order to estimate an integral of a function $f$ with respect to the posterior measure $\\nu$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\nI(f):=\\int_{\\mathbb{R}^{d}}f(x)\\mathrm{d}\\nu(x),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "the idea of importance sampling is to independently sample $X_{1},\\ldots,X_{n}$ from the prior $\\pi$ and calculate ", "page_idx": 18}, {"type": "equation", "text": "$$\nI_{n}(f):=\\frac{\\sum_{i=1}^{n}f(X_{i})\\tau(X_{i})}{\\sum_{i=1}^{n}\\tau(X_{i})},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\tau(x)$ is the observation likelihood $\\exp(-{\\textstyle{\\frac{1}{2}}}x^{\\top}Q x+x^{\\top}r)$ . With the Denoising Oracle, we can sample from the prior efifciently. Intuitively, one can think if the posterior and prior are very similar, for example, when $\\sigma$ is large such that the ratio $\\tau$ is close to 1, $I_{n}(f)$ computed from prior samples can efifciently approximate $I(f)$ . On the contrary, if $\\sigma$ is small, $\\tau(x)$ can have very large variance and the importance sampling can be inefifcient since many prior proposals have very small weights. The work [15, Theorem 1.2] proves that, in a fairly general setting, a sample of size approximately $\\exp(\\mathrm{KL}(\\nu||\\pi))$ is necessary and sufifcient for accurate estimation by importance sampling, where $\\mathrm{KL}(\\nu||\\pi)$ is the Kullback\u2013Leibler divergence of $\\pi$ from $\\nu$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathrm{KL}(\\nu||\\pi)=\\int_{\\mathbb{R}^{d}}\\log\\left(\\frac{\\mathrm{d}\\nu}{\\mathrm{d}\\pi}\\right)\\mathrm{d}\\nu=\\int_{\\mathbb{R}^{d}}(-\\frac{1}{2}x^{\\top}Q x+x^{\\top}b)\\mathrm{d}\\nu(x).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This result confirms one part of the intuition above: if $\\sigma$ is sufifciently small, then the magnitude of $Q$ and $r$ will be sufifciently small, and so is $(\\nu||\\pi)$ and the number of samples needed in the importance sampling. Next we show that for a fairly generic prior distribution $\\pi$ , when the SNR is large, $\\mathrm{KL}(\\nu||\\pi)$ will be also large such that we need approximately $O(e^{d\\cdot\\mathrm{SNR}})$ examples to implement importance sampling, which is unachievable. ", "page_idx": 18}, {"type": "text", "text": "Without loss of generality, we assume the covariance of the prior $\\pi$ is $I_{d}$ . ", "page_idx": 18}, {"type": "text", "text": "Proposition 15 (Importance Sampling Sample Complexity Lower Bound). Assume $\\nabla\\log\\pi(x)$ is \ud835\udc3f-Lipschitz: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\nabla\\log\\pi(\\boldsymbol{x})-\\nabla\\log\\pi(\\boldsymbol{z})\\right\\|\\leq L\\|\\boldsymbol{x}-\\boldsymbol{z}\\|\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then, when $\\mathrm{SNR}>L+2$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{KL}(\\nu||\\pi)\\geq{\\cal O}(d\\cdot\\mathrm{SNR})\\;,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and therefore the sample complexity of $I S$ is exponential in dimension. ", "page_idx": 19}, {"type": "text", "text": "Proof. We wish to lower bound $\\mathrm{KL}(\\nu||\\pi)$ with tools of functional inequalities for the concentration of measure. By the celebrated work [51, 10] that the log-Sobolev inequality implies the Talagrand transport-entropy inequality, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{KL}(\\nu||\\pi)\\geq\\frac{\\mathrm{LSI}(\\nu)W(\\nu,\\pi)^{2}}{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Here $W(\\nu,\\pi)$ denotes the Wassertain distance between $\\nu$ and $\\pi$ : ", "page_idx": 19}, {"type": "equation", "text": "$$\nW(\\nu,\\pi)=\\sqrt{\\underset{\\gamma\\in\\Gamma(\\nu,\\pi)}{\\operatorname*{inf}}\\|x-y\\|^{2}\\mathrm{d}\\gamma(x,y)}\\;,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\Gamma(\\nu,\\pi)$ denotes the set of probability measures on $\\mathbb{R}^{d}\\times\\mathbb{R}^{d}$ with marginals $\\nu$ and $\\pi$ . ", "page_idx": 19}, {"type": "text", "text": "First by Equation (63), we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\nabla^{2}\\left(-\\log\\nu\\right)\\succeq\\left(\\mathrm{SNR}-L\\right)\\!\\mathrm{Id}\\,,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which gives ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{LSI}(\\nu)\\geq(\\mathrm{SNR}-L)\\;,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "by Bakry-Emery criterion [3]. Furthermore, we have the lower bound for the Wasserstain distance [28] ", "page_idx": 19}, {"type": "equation", "text": "$$\nW^{2}(\\nu,\\pi)\\geq\\|\\mathrm{mean}(\\nu)-\\mathrm{mean}(\\pi)\\|^{2}+\\mathrm{trace}(\\mathrm{Cov}(\\nu)+\\mathrm{Cov}(\\pi)-2(\\mathrm{Cov}(\\pi)^{\\frac{1}{2}}\\mathrm{Cov}(\\nu)\\mathrm{Cov}(\\pi)^{\\frac{1}{2}})^{\\frac{1}{2}})\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle\\geq\\mathrm{trace}(\\mathrm{Cov}(\\boldsymbol{\\nu})+I_{d}-2\\mathrm{Cov}(\\boldsymbol{\\nu})^{\\frac{1}{2}})}\\\\ {\\displaystyle=\\mathrm{trace}(\\mathrm{Diag}(\\mathrm{Cov}(\\boldsymbol{\\nu}))+I_{d}-2\\mathrm{Diag}(\\mathrm{Cov}(\\boldsymbol{\\nu}))^{\\frac{1}{2}})}\\\\ {\\displaystyle=\\sum_{i=1}^{d}((1-\\mathrm{Std}(\\boldsymbol{\\nu})_{i})^{2})\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the second-to-last equality comes from the fact that the trace remains unchanged under orthogonal transformation. By (66) and Brascamp-Lieb Inequality (Theorem 16), we have Std(\ud835\udf08)\ud835\udc56\u2264\u221a\ufe03SNR1\u2212\ud835\udc3f. With the condition $\\mathrm{SNR}-L>2$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\nW^{2}(\\nu,\\pi)\\geq\\left(1-\\sqrt{\\frac{1}{2}}\\right)^{2}d\\;.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Collecting eqs. (65), (67) and (72), we obtain our final estimate ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{KL}(\\nu||\\pi)\\geq{\\cal O}(d\\cdot\\mathrm{SNR})\\;.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Theorem 16 (Brascamp-Lieb Inequality, [11]). If $\\pi$ is a strongly-log-concave measure on $\\mathbb{R}^{d}$ , i.e., of the form $\\pi=e^{-f}$ with $\\mathsf{\\dot{V}}^{2}f(x)\\geq{\\dot{\\alpha}}\\mathrm{Ic}$ d for all $\\boldsymbol{x}\\in\\dot{\\mathbb{R}}^{d}$ , then $\\|\\operatorname{Cov}(\\pi)\\|\\leq\\alpha^{-1}$ ", "page_idx": 19}, {"type": "text", "text": "C.3 Stability Analysis ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In the numerical implementation of boosted posterior, we typically encounter certain errors. Especially, we may have imperfect score subject to certain $L^{2}$ errors, and we may not be able to sample the boosted posterior $\\nu_{t}$ exactly. Suppose that instead of starting from $\\nu_{t}$ at $t$ and run the exact reverse SDE (5), we start from an approximate distribution $q_{t}\\approx\\nu_{t}$ and run the reverse SDE (5) with approximating score $s_{\\theta}(x,t)\\,\\approx\\,\\nabla\\log\\pi_{t}(x)$ where $\\theta$ denote the parameters parametrizing the score. Denote the distribution of the final samples by $q_{0}$ , we have the following error estimate ", "page_idx": 20}, {"type": "text", "text": "Proposition 17. Suppose $\\nu_{t},q_{t},\\nabla\\log\\pi_{t},s_{\\theta}(x,t)$ has enough regularities such that the reverse SDEs exist, if the Novikov\u2019s condition $\\begin{array}{r}{\\mathbb{E}\\left[\\exp(\\int_{0}^{t}\\|\\nabla\\log\\pi_{\\tau}(x)-s_{\\theta}(x,\\tau)\\|^{2}{\\bf d}\\tau)\\right]<\\infty,}\\end{array}$ , then ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathrm{KL}(\\nu||q_{0})\\leq\\int_{0}^{t}\\mathbb{E}_{\\nu_{\\tau}}\\|\\nabla\\log\\pi_{\\tau}(x)-s_{\\theta}(x,\\tau)\\|^{2}\\mathrm{d}\\tau+\\mathrm{KL}(\\nu_{t}||q_{t}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The above proposition ensures that if both the initialization error and score error (over the posterior paths) are small, then the distribution of our final samples is close to the target posterior. The proof is provided in Appendix C.3. Note that we consider the reverse dynamics in continuous-time without time discretization error. There are various works [44, 45, 17, 8] analyzing the time discretization error and those techniques can be further incorporated into the above error estimate. ", "page_idx": 20}, {"type": "text", "text": "The proof is similar to that in [59]. Here we provide the proof in our posterior sampling context for completeness. ", "page_idx": 20}, {"type": "text", "text": "Proof. Consider the following two reverse dynamics needed for the error estimate: one is based on the exact score and starts from the exact boosted posterior ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathrm{d}X_{\\tau}=(-X_{\\tau}-2\\nabla\\log\\pi_{\\tau}(X_{\\tau}))\\mathrm{d}\\tau+\\sqrt{2}\\mathrm{d}W_{\\tau},\\enspace X_{t}\\sim\\nu_{t},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and another one is based on the approximate score and starts from the approximation to the boosted posterior ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathrm{d}\\tilde{X}_{\\tau}=(-\\tilde{X}_{\\tau}-2s_{\\theta}(\\tilde{X}_{\\tau},\\tau))\\mathrm{d}\\tau+\\sqrt{2}\\mathrm{d}W_{\\tau},~~\\tilde{X}_{t}\\sim q_{t},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Note that these two dynamics are defined backwardly for $\\tau\\in[0,t]$ and we drop the superscript $\\gets$ for notation simplicity. We denote the path measure of $\\{X_{\\tau}\\}_{\\tau\\in[0,t]}$ and $\\{\\tilde{X}_{\\tau}\\}_{\\tau\\in[0,t]}$ by $\\pmb{\\nu}$ and $\\pmb q$ , respectively. Then $\\nu$ and $q_{0}$ are the marginal distributions of the two path measures at $t=0$ . By data processing inequality and chain rule of KL divergence, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{KL}(\\nu||q_{0})\\leq\\mathrm{KL}(\\nu||q)}\\\\ &{\\qquad\\qquad\\leq\\mathrm{KL}(\\nu_{\\tau}||q_{\\tau})+\\mathbb{E}_{z\\sim\\nu_{\\tau}}\\mathrm{KL}(\\nu(\\cdot|X_{t}=z)||q(\\cdot|\\tilde{X}_{t}=z))}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Given the Novikov\u2019s condition, we can apply the Girsanov theorem [50] to eq. (75)76 to compute the second term above ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{z\\sim\\nu_{\\tau}}\\mathbf{KL}(\\nu(\\cdot|X_{t}=z)||q(\\cdot|\\tilde{X}_{t}=z))}\\\\ &{\\leq\\,-\\,\\mathbb{E}_{\\nu}\\left[\\log\\frac{\\mathrm{d}q}{\\mathrm{d}\\nu}\\right]}\\\\ &{=\\mathbb{E}_{\\nu}\\left[2\\int_{0}^{t}\\left(\\nabla\\log\\pi_{\\tau}(x)-s_{\\theta}(x,\\tau)\\right)\\!\\mathrm{d}W_{\\tau}+\\int_{0}^{t}\\|\\nabla\\log\\pi_{\\tau}(x)-s_{\\theta}(x,\\tau)\\|^{2}\\mathrm{d}\\tau\\right]}\\\\ &{=\\mathbb{E}_{\\nu}\\left[\\int_{0}^{t}\\|\\nabla\\log\\pi_{\\tau}(x)-s_{\\theta}(x,\\tau)\\|^{2}\\mathrm{d}\\tau\\right]}\\\\ &{=\\int_{0}^{t}\\mathbb{E}_{\\nu_{\\tau}}\\|\\nabla\\log\\pi_{\\tau}(x)-s_{\\theta}(x,\\tau)\\|^{2}\\mathrm{d}\\tau}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "D Iterated Tilted Transport ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We have shown that posterior sampling of $\\nu=\\mathsf{T}_{Q}\\pi$ can be reduced to sampling from $\\nu_{T^{*}}$ by running the reverse SDE. While $\\nu_{T^{*}}$ is easy to sample under the conditions presented in Section 5, these may ", "page_idx": 20}, {"type": "text", "text": "not be verified in several situations of interest. In this context, a natural question is whether one could still leverage the tilted transport, at the expense of introducing sampling error. This is what we address in this section. ", "page_idx": 21}, {"type": "text", "text": "Let $\\lambda_{1},...\\lambda_{d}$ be the eigenvalues of $Q$ . Let us assume for simplicity that $b=0$ and all eigenvalues have multiplicity 1, so $)\\;\\lambda_{i}>\\lambda_{i+1}$ . We define the events $T_{j}^{*}$ for $j=1\\ldots d$ given by ", "page_idx": 21}, {"type": "equation", "text": "$$\nT_{j}^{*}:=\\frac{1}{2}\\log(1+\\lambda_{j}^{-1})\\ .\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Denote by ", "page_idx": 21}, {"type": "equation", "text": "$$\n{\\bar{\\lambda}}_{j}(t)={\\left\\{\\begin{array}{l l}{\\infty}&{{\\mathrm{if~}}t\\geq T_{j}^{*}{\\mathrm{~,~}}}\\\\ {\\lambda_{j}(t)}&{{\\mathrm{~otherwise,}}}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\lambda_{j}(t)\\:=\\:e^{2t}/(1+\\lambda_{j}^{-1}-e^{2t})$ is the solution to the ODE $\\dot{q}_{t}\\,=\\,2(1+q_{t})q_{t}$ . By abusing notation, we denote by $\\bar{Q}_{t}$ , $t\\geq T^{*}$ the matrix that shares eigenvectors with $Q$ , and with eigenvalues $(\\bar{\\lambda}_{1}(t),\\ldots,\\bar{\\lambda}_{d}(t))$ . Denote by $V_{k}=\\left[\\nu_{d-k}\\,.\\,.\\,.\\,\\nu_{d}\\right]\\in\\mathbb{R}^{d\\times k}$ the orthogonal projection onto the last $k$ eigenvectors. ", "page_idx": 21}, {"type": "text", "text": "While previously we considered only the transport between $\\nu$ and $\\nu_{1}:=\\nu_{T_{1}^{*}}$ , now we can consider the sequence $\\nu_{k}:=\\mathsf{T}_{\\bar{Q}_{T_{k}^{*}}}\\pi_{T_{k}^{*}}$ for $k=1,\\dots,d$ . Observe that $\\nu_{k}$ is a measure supported on a subspace $\\Omega_{k}$ of dimension $d-k$ ; in other words, where $k$ directions are singular, corresponding to the eigenvectors associated with the $\\infty$ eigenvalues of $\\bar{Q}_{T_{k}^{*}}$ , and thus $\\Omega_{k}=\\{x\\ \\mathbf{\\bar{\\in}}\\ \\mathbb{R}^{d};V_{k}^{\\top}x=\\mathbf{y}_{k}\\}$ for some $\\mathbf{y}_{k}^{\\breve{\\mathbf{\\alpha}}}\\in\\mathbb{R}^{k}$ . ", "page_idx": 21}, {"type": "text", "text": "Now, let us consider $k^{*}\\,=\\,\\operatorname*{min}\\{k;\\nu_{k}$ is s.l.c. $\\}$ ; that is, the first $k$ such that $\\nu_{k}$ is strongly logconcave, and therefore efifciently sampleable by Langevin dynamics. Under the same assumptions as Corollary 10, and by defining $\\begin{array}{r}{\\bar{\\kappa_{k}}:=\\frac{\\bar{\\lambda_{k}}}{\\lambda_{d}}}\\end{array}$ as the condition number of the truncated $Q$ , we immediately obtain the bound ", "page_idx": 21}, {"type": "equation", "text": "$$\nk^{*}\\le\\operatorname*{min}\\left\\{k;\\frac{(1+\\delta^{2}\\mathrm{SNR})(\\delta^{2}\\kappa_{k}+\\mathrm{SNR}^{-1})}{\\kappa_{k}-1}>R^{2}\\right\\}\\ .\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For $k<k^{*}$ , assume first that one had sampling access to $\\nu_{k+1}$ . Running the reverse tilted transport for time $\\eta_{k}=T_{k+1}^{*}-T_{k}^{*}$ would produce samples from a tilted measure $\\tilde{\\nu}_{k}:=\\mathsf{T}_{\\tilde{Q}_{k}}\\pi_{T_{k}^{*}}$ , where $\\tilde{Q}_{k}$ has eigenvalues $(\\psi(\\eta_{k}),\\ldots,\\psi(\\eta_{k}),\\lambda_{k+1}(T_{k}^{*}),\\ldots,\\lambda_{d}(T_{k}^{*}))$ , where we defined $\\psi(t):=\\^{\\cdot}(e^{-2t}-1)^{-1}$ as the inverse of $\\begin{array}{r}{\\lambda\\mapsto\\frac{1}{2}\\log(1+\\lambda^{-1})}\\end{array}$ . It is thus a non-singular measure in $\\mathbb{R}^{d}$ , capturing the fact that the denoising oracle driving the reverse dynamics is isotropic, and thus oblivious to the existence of the singular support of $\\nu_{k}$ . ", "page_idx": 21}, {"type": "text", "text": "We thus need a procedure to transform samples from $\\tilde{\\nu}_{k}$ to samples of $\\nu_{k}$ . The easiest procedure is to simply marginalize the coordinates $(x_{1},\\hdots,x_{k})=V_{k}^{\\top}x\\in\\mathbb{R}^{k}$ , ie ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\bar{\\nu}_{k}(x_{k+1},\\ldots,x_{d})=\\int_{\\mathbb{R}^{k}}\\tilde{\\nu}_{k}({\\mathrm{d}}x_{1},\\ldots,{\\mathrm{d}}x_{k},x_{k+1},\\ldots,x_{d})\\in\\mathcal{P}(\\mathbb{R}^{d-k})\\;,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and then \u2018lift\u2019 this measure in the subspace $\\Omega_{k}$ , i.e., ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\hat{\\nu}_{k}({\\bf x}_{k};{\\bf x}_{-k}):=\\delta({\\bf x}_{k}-{\\bf y}_{k})\\bar{\\nu}_{k}({\\bf x}_{-k})\\;,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where we defined $\\mathbf{x}_{k}=(x_{1},\\ldots,x_{k})$ and $\\mathbf{x}_{-k}=\\left(x_{k+1},\\ldots,x_{d}\\right)$ . ", "page_idx": 21}, {"type": "text", "text": "We can then iteratively run the tilted transport backwards, from $k=k^{*}-1$ to $k=0$ , as illustrated in Algorithm 2: ", "page_idx": 21}, {"type": "table", "img_path": "PhLlE8UOEv/tmp/154231abdfc4a02080749fe9d4151ddf34e0caf7a1c7d5a1162a34154500c4a2.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "By the data-processing inequality, the TV error will accumulate linearly at each step. Denoting $\\hat{\\nu}$ the law of $X_{0}$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathrm{TV}(\\hat{\\nu},\\nu)\\leq\\sum_{0<k<k^{*}}\\mathrm{TV}(\\hat{\\nu}_{k},\\nu_{k})\\;.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "This bound can be interpreted as the accumulation of errors arising from conditioning a measure by marginalizing over its first components. To the extent that $\\psi(\\eta_{k})$ is large, these variables are nearly deterministic, so one would expect that marginalization is a good approximation of conditioning. The outstanding question is to understand conditions when this error guarantee can be quantified. ", "page_idx": 22}, {"type": "text", "text": "Inspired by [18], a natural extension of this simple iterative procedure is to apply \u2018thermalization\u2019 towards the stationary measure $\\nu_{k}$ after line 4 of Algorithm 2 above, by running Langevin dynamics in $\\Omega_{k}$ with score $\\nabla\\log\\nu_{k}$ : ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathrm{d}X_{t}=\\nabla\\log\\nu_{k}(X_{t})\\mathrm{d}t+\\sqrt{2}\\mathrm{d}W_{t}\\ ,\\ X_{0}\\sim\\hat{\\nu}_{k}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The drift of this diffusion is available, since both $Q_{T_{k}^{*}}$ and $\\nabla\\log\\pi_{T_{k}^{*}}$ are known, so is $\\nabla\\log\\nu_{k}$ . ", "page_idx": 22}, {"type": "text", "text": "Denote by $\\check{\\nu}_{k}$ the law of $X_{t}$ after time $t\\,=\\,B_{k}$ . While the time to relaxation of such Langevin dynamics is generally not quantitative (otherwise $k^{*}\\leq k$ ), even a short amount of thermalization is able to improve upon the previous method. Indeed, by the reverse transport inequality [10, Lemma 4.2], a weaker Wa\u221asserstein guarantee $W_{2}(\\hat{\\nu}_{k},\\nu_{k})$ can be \u2018upgraded\u2019 to a TV guarantee of the form $\\mathrm{T}\\bar{\\mathrm{V}}(\\check{\\nu}_{k},\\nu_{k})=O(\\sqrt{L_{k}}W_{2}(\\tilde{\\nu_{k}},\\nu_{k}))$ by running Langevin dynamics for time $B_{k}=\\Theta(1/L_{k})$ , where $L_{k}=\\operatorname*{sup}_{x}\\lambda_{\\operatorname*{max}}(\\nabla^{2}\\log\\nu_{k}(x))>0$ is the largest eigenvalue of $\\nabla^{2}\\log{\\nu_{k}}$ , which is positive by definition of $k^{*}$ and $k<k^{*}$ . In summary, the \u2018thermalized\u2019 iterated tilted transport satisfies an error bound of the form ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathrm{TV}(\\hat{\\nu},\\nu)\\lesssim\\sum_{0<k<k^{*}}\\sqrt{L_{k}}W_{2}(\\hat{\\nu}_{k},\\nu_{k})\\;.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "E Experimental Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "E.1 Gaussian mixture models ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "For a given dimension $d$ with $d\\,\\mathrm{mod}\\,2=0$ , we consider prior data a mixture of 25 Gaussian distributions, the same as considered in [13]. The Gaussian distribution has mean $(8i,8j,\\cdots,8i,8j)\\in\\mathbb{R}^{d}$ for $(i,j)\\in\\{-2,-1,0,1,2\\}_{.}^{2}$ and unit variance. Each (unnormalized) mixture weight is independently drawn according to a $\\chi^{2}$ distribution. ", "page_idx": 22}, {"type": "text", "text": "For the measurement model considered in Figure 3, we generate $A$ in the following way. We first sample a $d\\times d$ matrix with each entry sampled from the standard normal and compute its SVD to get $U$ and $V$ for $A$ . The singular value is given by $[1,\\cdot\\cdot\\cdot,1/20]$ where each component in between is independently sampled from Unif( $\\left[1/20,1\\right];$ ) such that the condition number of $A$ is 20. The observation noise is then determined by SNR. For the measurement model considered in Table 1, the matrix $U$ and $V$ for the SVD form of $A$ is the same to the above. Each singular value in $S$ is independently sampled from $\\mathrm{Unif}([0,1])$ , and $\\sigma$ is sampled from Uni $\\operatorname{f}([0.2\\operatorname*{max}S,\\operatorname*{max}S])$ ). ", "page_idx": 22}, {"type": "text", "text": "For all the experiments we run the boosted posterior from $T^{*}-0.01$ such that the ODE solution $Q,b$ is well-defined. We use BlackJAX [12] to implement the No-U-turn sampler. ", "page_idx": 22}, {"type": "text", "text": "Besides results reported in Figure 3, we further test tilted transport when $d^{\\prime}<d$ . In this setting, $\\lambda_{\\operatorname*{min}}(Q_{t})$ remains zero but the signal corresponding to the non-zero eigenvalues still gets enhanced. Therefore, although it becomes more dififcult for $\\nu_{T^{*}}$ to be strongly log-concave, the tilted transport can still make the new posterior easier to sample even if it is not strongly log-concave yet. As shown in Table 1, when $d^{\\prime}=0.9d$ , $10\\%$ percent eigenvalues of $Q_{t}$ are zero, our tilted transport technique still reduces the statistical distance of the posterior samples significantly. We also consider an even more challenging case where $d^{\\prime}=1$ such that the target posterior is still heavlily multimodal (as visualized in the 2D example in Figure 2). In this case, LMC suffers from the local maxima of the potential and thus cannot explore the multimodal distribution efifciently. We use the No-U-turn sampler[34], a Hamilton Monte Carlo (HMC) method, as the baseline method, which can move among different modes more efifciently than Langevin. We find that the tilted transport technique can still boost the performance of HMC in this challenging setting. We also verify Theorem 5 by sampling from the boosted posterior directly from its analytical formula and running the reverse SDE, and the obtained samples approximate the target posterior well, as reported in Table 1. ", "page_idx": 22}, {"type": "table", "img_path": "PhLlE8UOEv/tmp/b9a75bbf7725605bdffad5fdf2f96feb04c060d1508ae8b53a21cd69a55388cf.jpg", "table_caption": ["Table 1: Sliced Wasserstein distance for Gaussian mixture prior for degenerate case "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "E.2 Imaging Problems ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We perform four inverse tasks on the Flickr-Faces-HQ Dataset (FFHQ) [39] to demonstrate the application of the tilted transport technique on imaging data as a proof of concept. To apply the proposed tilted transport technique to these problems, we still need to select a baseline method for sampling from the boosted posterior $\\nu_{T^{*}}$ . In the case of ill-conditioned problems, sampling $\\nu_{T^{*}}$ may still be challenging for principled algorithms like LMC, and we still need to rely on heuristic methods for imaging tasks. However, as noted in the introduction, most existing heuristic methods primarily facilitate conditional generation based on the measurement, lacking principled guarantees for posterior sampling. Consequently, we lack a principled interpretation for enhancing these methods with tilted transport. Nevertheless, we can still experiment with such methods as a proof of concept. We chose Diffusion Model Based Posterior Sampling (DMPS) [48] as the baseline method for the following reasons: The main assumption of DMPS in approximating the time-dependent conditional score is that the prior $\\pi$ is uninformative (flat) with respect to $X_{t}$ , such that $p(\\mathbf{\\dot{X}}_{0}|X_{t})\\propto p(X_{t}|X_{0})$ . This assumption only holds approximately in early phases of the forward diffusion, and hopefully a higher SNR provided by tilted transport makes the effect of this approximation error smaller. ", "page_idx": 23}, {"type": "text", "text": "We conducted four tasks: (a) denoising; (b) inpainting with random masks from [53]; (c) $4\\times$ super-resolution; and (d) deblurring using a Gaussian kernel. Our algorithm was implemented using the NVIDIA codebase [47] with 1000 diffusion steps for posterior sampling, and utilized the score function from a pretrained diffusion model [20]. Similar to our Gaussian mixture model experiments where we adjusted the timing for the boosted posterior to avoid the singularity of $Q_{t}$ , we shifted 6 - 10 timesteps for setting the boosted posterior. Experiments show that the final performance is robust with respect to the number of shifted steps. Figure 4 showcases examples from the inpainting task, demonstrating how tilted transport enhances the baseline DMPS method. Additionally, we report various sample statistics including peak signal-to-noise ratio (PSNR), structural similarity index (SSIM), and Learned Perceptual Patch Similarity (LPIPS). However, it is important to note that while these statistics assess the quality of prior data generation, they may not accurately reflect the quality of posterior samples. ", "page_idx": 23}, {"type": "table", "img_path": "PhLlE8UOEv/tmp/24accd1004cd029fb1e72fd636a44fb7f2e68494c08a96d62eaf7c8fe67612b7.jpg", "table_caption": ["Table 2: Performance for tasks on FFHQ Dataset. "], "table_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "PhLlE8UOEv/tmp/7e7a6a786b5002d0ccb3bf5f59eeaf935791103c7c8dddef8a31734d899b9064.jpg", "img_caption": ["Figure 4: Examples for inpainting with random masks over FFHQ dataset "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and precede the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 25}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 25}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 25}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 25}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 25}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 25}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 25}, {"type": "text", "text": "(i) Claims ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: This work focuses on provable sampling of posterior distribution with denoising oracles and the proposed tilted transport technique. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 25}, {"type": "text", "text": "(ii) Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: See the last section. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efifciency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 26}, {"type": "text", "text": "(iii) Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: All the details of proof are provided in Appendix. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 26}, {"type": "text", "text": "(iv) Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: See Appendix E.   \nGuidelines: \u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might sufifce, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "(v) Open access to data and code ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufifcient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: Code for Gaussian mixture is uploaded in a single zip file. Our results on imaging tasks in Appendix E.2 are mainly for a proof-of-concept. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 27}, {"type": "text", "text": "\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 28}, {"type": "text", "text": "(vi) Experimental Setting/Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: See the main text, Appendix E and code. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 28}, {"type": "text", "text": "(vii) Experiment Statistical Significance ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: In each experiment, we report percentiles or standard deviation. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 28}, {"type": "text", "text": "(viii) Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufifcient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Our experiments with the Gaussian mixture model require only a few seconds per run on a laptop. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. ", "page_idx": 28}, {"type": "text", "text": "\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 29}, {"type": "text", "text": "(ix) Code Of Ethics ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: We conform the the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 29}, {"type": "text", "text": "(x) Broader Impacts ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: Our paper mainly focuses provable sampling of the posterior distribution. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efifciency and accessibility of ML). ", "page_idx": 29}, {"type": "text", "text": "(xi) Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The work poses no such risks. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 29}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 30}, {"type": "text", "text": "(xii) Licenses for existing assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: We use open-source packages. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 30}, {"type": "text", "text": "(xiii) New Assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 30}, {"type": "text", "text": "Answer: [No] ", "page_idx": 30}, {"type": "text", "text": "Justification: No new asset is released. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 30}, {"type": "text", "text": "(xiv) Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 31}, {"type": "text", "text": "(xv) Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 31}]