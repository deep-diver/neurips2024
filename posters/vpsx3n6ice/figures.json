[{"figure_path": "VPSx3n6ICE/figures/figures_1_1.jpg", "caption": "Figure 1: (a) Illustration of Hybrid HE/MPC-based private inference; (b) latency breakdown of linear layers and nonlinear layers based on Bolt's protocol; (c) latency breakdown of linear layers of the original model and SpENCNN with 50% sparsity; (d) GEMV with a circulant weight matrix.", "description": "This figure illustrates the hybrid HE/MPC framework for private inference, comparing the latency breakdown of linear and non-linear layers in different models (Bolt, SpENCNN), and showcasing the GEMV operation with a circulant weight matrix.  The hybrid approach involves both homomorphic encryption (HE) and multi-party computation (MPC) to protect data privacy during inference.  The latency breakdown highlights the significant computational cost of linear layers, which motivates the use of circulant matrices for optimization.  The circulant matrix conversion of GEMV enables more efficient computation with HE.", "section": "1 Introduction"}, {"figure_path": "VPSx3n6ICE/figures/figures_3_1.jpg", "caption": "Figure 2: Directly using coefficient or SIMD encoding to block circulant GEMMs ((d1, d2, d3,b) = (256,192,576,2)) leads to limited efficiency improvement.", "description": "This figure shows the latency comparison of different encoding methods for block circulant GEMMs. The non-circulant method has the highest latency, while the SIMD encoding method has a slightly lower latency than the coefficient encoding method. The ideal latency is 50% lower than the coefficient encoding method. This indicates that using coefficient encoding can significantly improve the efficiency of block circulant GEMMs compared to SIMD encoding.", "section": "3 PrivCirNet Framework"}, {"figure_path": "VPSx3n6ICE/figures/figures_4_1.jpg", "caption": "Figure 4: An example of CirEncode for block circulant GEMM where (d1, d2, d3, b) = (4,8,8, 4).", "description": "This figure illustrates the CirEncode process for a block circulant GEMM with dimensions (d1, d2, d3, b) = (4, 8, 8, 4). It shows how CirEncode handles both encoding within a circulant block (a) and across circulant blocks (b).  Panel (a) details how individual circulant blocks are encoded using a coefficient encoding scheme that avoids HE rotations, converting the GEMV into a HE-friendly 1D convolution. Panel (b) shows how the across-block GEMM is handled using SIMD encoding. This combined approach aims to reduce HE rotations and multiplications.", "section": "3.2 CirEncode: nested encoding for block circulant GEMMS"}, {"figure_path": "VPSx3n6ICE/figures/figures_4_2.jpg", "caption": "Figure 3: Overview of PrivCirNet.", "description": "This figure shows the overall framework of PrivCirNet.  It is divided into two main parts: Protocol Optimization and Network Optimization. Protocol Optimization focuses on the CirEncode algorithm (Section 3.2) which provides support for a latency-aware block size assignment. Network Optimization (Section 3.3) handles the layer-wise block size assignments and network-protocol co-fusion (Section 3.4).", "section": "3 PrivCirNet Framework"}, {"figure_path": "VPSx3n6ICE/figures/figures_6_1.jpg", "caption": "Figure 5: Layer-wise sensitivity and block size visualization for ViT on CIFAR-100.", "description": "This figure visualizes the sensitivity of different block sizes on the loss function for each layer in a Vision Transformer (ViT) model trained on CIFAR-100. It compares two initialization methods: one using the Frobenius norm and the other using the proposed loss-aware initialization (\u03a9\u2081). The top panels show the sensitivity of each linear layer to different block sizes (b=1, 2, 4, 8, 16), while the bottom panels show the selected block sizes for each layer. The loss-aware method shows more variability across layers and better captures the effects of varying block sizes on task loss.", "section": "3.3 Latency-aware block size assignment with loss-aware initialization"}, {"figure_path": "VPSx3n6ICE/figures/figures_6_2.jpg", "caption": "Figure 6: Network-Protocol Co-Fusion.", "description": "This figure illustrates two optimization strategies employed in PrivCirNet for improving efficiency. (a) shows Circulant ConvBN Fusion, where convolution and batch normalization layers are fused to maintain the block circulant structure and reduce latency. (b) presents IR (Inverted Residual) Fusion Protocol, which fuses consecutive linear layers in the network to reduce communication overhead.", "section": "3.4 Network-Protocol Co-Fusion"}, {"figure_path": "VPSx3n6ICE/figures/figures_7_1.jpg", "caption": "Figure 7: Latency comparison of different protocols for GEMMs and convolutions. PrivCirNet use circulant weight with block size b.", "description": "This figure compares the latency of different homomorphic encryption (HE) protocols for performing general matrix multiplications (GEMMs) and convolutions.  The protocols compared include CrypTFlow2, Cheetah, Neujeans, Bolt, and PrivCirNet.  PrivCirNet is shown with two different block sizes (b=2 and b=8), demonstrating the effect of block size on latency. The results show that PrivCirNet significantly outperforms the other methods in terms of latency, especially with the larger block size (b=8).  The x-axis shows different GEMM and convolution dimensions, representing various layer configurations in typical neural network architectures. The y-axis represents the latency in seconds.", "section": "4.2 Micro-Benchmark on Single GEMM and Convolution"}, {"figure_path": "VPSx3n6ICE/figures/figures_8_1.jpg", "caption": "Figure 8: Comparison with SpENCNN and other prior-art protocols on MobileNetV2.", "description": "This figure compares the performance of PrivCirNet with SpENCNN and other state-of-the-art private inference protocols (Bolt, Neujeans, Falcon, Cheetah, CrypTFlow2) on MobileNetV2 across four datasets (CIFAR-10, CIFAR-100, Tiny ImageNet, and ImageNet).  The Pareto front is shown, illustrating the trade-off between latency and accuracy.  PrivCirNet demonstrates significant latency reductions and accuracy improvements compared to the baselines, especially on larger datasets like ImageNet.", "section": "4.3 End-to-End Inference Evaluation"}, {"figure_path": "VPSx3n6ICE/figures/figures_8_2.jpg", "caption": "Figure 10: Ablation study of our proposed optimizations in PrivCirNet on MobileNetV2.", "description": "This ablation study shows the impact of each component of PrivCirNet on MobileNetV2, using Tiny ImageNet as the dataset.  It demonstrates that the combination of all optimizations leads to the best performance.  The individual components are:\n\n1. **Baseline MobileNetV2:** The original model without any PrivCirNet optimizations.\n2. **+ Cir. Transformation (b=2):** The model with only block circulant transformation applied (with block size b=2).\n3. **+ CirEncode (Sec. 3.2):**  Adds the CirEncode encoding method to the model. \n4. **+ Latency-aware block size assignment (Sec. 3.3):** The layer-wise block sizes are optimized for latency.\n5. **+ ConvBN Fusion (Sec. 3.4):** Convolution and batch normalization layers are fused.\n6. **+ IR Fusion (Sec. 3.4):** Inverted residual blocks are fused.\n\nThe figure shows the latency (in seconds), communication (in MB), and top-1 accuracy (%) for each stage.", "section": "4.4 Ablation Study"}, {"figure_path": "VPSx3n6ICE/figures/figures_14_1.jpg", "caption": "Figure 1: (a) Illustration of Hybrid HE/MPC-based private inference; (b) latency breakdown of linear layers and nonlinear layers based on Bolt's protocol; (c) latency breakdown of linear layers of the original model and SpENCNN with 50% sparsity; (d) GEMV with a circulant weight matrix.", "description": "This figure is composed of four subfigures. Subfigure (a) illustrates a hybrid HE/MPC framework for private inference, where linear layers are processed using HE, and non-linear layers are processed using MPC.  Subfigure (b) shows the breakdown of latency for linear and non-linear layers in Bolt's protocol. Subfigure (c) compares the latency breakdown of linear layers in a standard model and a SpENCNN model with 50% sparsity. Finally, subfigure (d) illustrates the GEMV (general matrix-vector multiplication) operation using a circulant weight matrix, which is a key concept in the proposed method.", "section": "1 Introduction"}, {"figure_path": "VPSx3n6ICE/figures/figures_15_1.jpg", "caption": "Figure 12: Illustration of our BSGS algorithm for block circulant GEMM with tiling.", "description": "This figure illustrates the authors' adaptation of the Baby-step Giant-step (BSGS) algorithm for block circulant General Matrix-Vector Multiplication (GEMM).  The BSGS algorithm reduces the number of costly homomorphic rotations. The figure shows how tiling is used to split matrices into smaller blocks whose maximum size is limited by the HE polynomial degree, and how the BSGS parameters B (number of baby steps) and G (number of giant steps) are determined to minimize the number of rotations while adhering to the constraints of the tiling and polynomial degree.", "section": "3.2 CirEncode: nested encoding for block circulant GEMMS"}, {"figure_path": "VPSx3n6ICE/figures/figures_16_1.jpg", "caption": "Figure 4: An example of CirEncode for block circulant GEMM where (d1, d2, d3, b) = (4,8,8, 4).", "description": "This figure illustrates the CirEncode process for a block circulant GEMM. Panel (a) shows CirEncode within a circulant block, detailing how a block circulant GEMV is converted into a HE-friendly 1D convolution using coefficient encoding.  Panel (b) illustrates CirEncode across circulant blocks, explaining how SIMD encoding handles GEMMs across blocks, combining the advantages of both encoding methods for enhanced efficiency. The notations used in this figure are defined within the paper.", "section": "3.2 CirEncode: nested encoding for block circulant GEMMS"}, {"figure_path": "VPSx3n6ICE/figures/figures_17_1.jpg", "caption": "Figure 12: Illustration of our BSGS algorithm for block circulant GEMM with tiling.", "description": "This figure illustrates the authors' enhanced Baby-step Giant-step (BSGS) algorithm for block circulant General Matrix-Vector Multiplication (GEMM).  It shows how tiling is used to divide the GEMM into smaller blocks that are compatible with the limitations of Homomorphic Encryption (HE) polynomial degree. The formula #Rot = (d1d2/n)(B-1) + (d1d3/n)(G-1) shows the number of rotations required, where B and G are the numbers of baby-steps and giant-steps respectively, and n is the polynomial degree. The constraint HWbd = n ensures that the tile size is within HE's limitation. This optimized approach reduces the number of rotations needed for the GEMM computation compared to the standard BSGS algorithm.", "section": "3.2 CirEncode: nested encoding for block circulant GEMMs"}, {"figure_path": "VPSx3n6ICE/figures/figures_18_1.jpg", "caption": "Figure 15: Illustration of the limitation of structured pruning in BSGS algorithm.", "description": "This figure illustrates why structured pruning is not effective with the BSGS algorithm. In BSGS, rotations are split into baby-step and giant-step rotations. To reduce rotations, diagonals across different groups must be pruned, and for tiling, diagonals across all groups for all weight matrices must be pruned. This makes it difficult to reduce the number of rotations using structured pruning.", "section": "D Why does structured pruning fail in BSGS algorithm?"}, {"figure_path": "VPSx3n6ICE/figures/figures_20_1.jpg", "caption": "Figure 1: (a) Illustration of Hybrid HE/MPC-based private inference; (b) latency breakdown of linear layers and nonlinear layers based on Bolt's protocol; (c) latency breakdown of linear layers of the original model and SpENCNN with 50% sparsity; (d) GEMV with a circulant weight matrix.", "description": "This figure illustrates the Hybrid HE/MPC framework for private inference (a), compares latency breakdowns of linear and non-linear layers in Bolt's protocol and SpENCNN (b, c), and shows a GEMV operation with a circulant weight matrix (d) which is the core concept of the proposed method.", "section": "1 Introduction"}, {"figure_path": "VPSx3n6ICE/figures/figures_20_2.jpg", "caption": "Figure 5: Layer-wise sensitivity and block size visualization for ViT on CIFAR-100.", "description": "This figure visualizes the sensitivity of each layer to block size changes (b) during the block circulant transformation of a Vision Transformer (ViT) model trained on the CIFAR-100 dataset.  It compares two initialization methods: one using the Frobenius norm and the other using the proposed loss-aware initialization (\u03a9\u2081). The x-axis represents the linear layer index, and the y-axis represents the sensitivity (\u03a9\u2081) or the Frobenius norm.  The plots show how the sensitivity to block size varies across different layers, indicating the importance of a layer-wise block size assignment strategy rather than a uniform block size for all layers.", "section": "3.3 Latency-aware block size assignment with loss-aware initialization"}, {"figure_path": "VPSx3n6ICE/figures/figures_20_3.jpg", "caption": "Figure 7: Latency comparison of different protocols for GEMMs and convolutions. PrivCirNet use circulant weight with block size b.", "description": "This figure compares the latency of various protocols (CrypTFlow2, Cheetah, Neujeans, Bolt, and PrivCirNet) for performing GEMMs (general matrix multiplications) and convolutions on different dimensions.  PrivCirNet uses circulant weight matrices with varying block sizes (b2 and b8). The results demonstrate that PrivCirNet, particularly with block size 8, significantly reduces the latency compared to other protocols.", "section": "4.2 Micro-Benchmark on Single GEMM and Convolution"}, {"figure_path": "VPSx3n6ICE/figures/figures_21_1.jpg", "caption": "Figure 8: Comparison with SpENCNN and other prior-art protocols on MobileNetV2.", "description": "This figure compares the performance of PrivCirNet with SpENCNN and other state-of-the-art protocols (Bolt, Neujeans, Falcon, Cheetah, and CrypTFlow2) on MobileNetV2 for different datasets (CIFAR-10, CIFAR-100, and Tiny ImageNet).  It shows accuracy and latency trade-offs.  PrivCirNet consistently demonstrates superior performance in terms of both latency and accuracy improvements compared to the baselines.", "section": "4.3 End-to-End Inference Evaluation"}]