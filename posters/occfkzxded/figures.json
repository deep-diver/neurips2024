[{"figure_path": "OCcfKzXded/figures/figures_1_1.jpg", "caption": "Figure 1: (1) Motivation: new inliers (outliers) tend to cluster around latent positive (negative) anchors that represent existing inliers (outliers) in the feature space, respectively. (2) Performance: pseudo-labels from INTEGER are more robust and accurate than the previous state-of-the-art EYOC[12].", "description": "This figure demonstrates the core idea and performance of the proposed INTEGER method.  The left panel shows that new inliers tend to cluster around positive anchors representing existing inliers in the feature space, while outliers cluster around negative anchors. This observation motivates the Feature-Geometry Coherence Mining (FGCM) module. The right panel compares the performance of INTEGER against the state-of-the-art EYOC method in terms of Inlier Ratio (IR%) within the pseudo-labels, showing a significant improvement of INTEGER, especially at larger distances.", "section": "1 Introduction"}, {"figure_path": "OCcfKzXded/figures/figures_3_1.jpg", "caption": "Figure 2: The Overall Pipeline. FGCM(Sec. 3.2) first adapt the teacher model to a data-specific teacher for the current mini-batch, and then mine reliable pseudo-labels. Next, MDS(Sec. 3.4) learns density-invariant features from pseudo-labels. ABCont(Sec. 3.3) is applied for adapting the teacher and transferring knowledge to the student in the feature space.", "description": "This figure illustrates the overall pipeline of the INTEGER method for unsupervised point cloud registration. It shows the two-stage training process with a teacher-student framework. The teacher model is first adapted to each mini-batch of data using Feature-Geometry Coherence Mining (FGCM) module, which produces reliable pseudo-labels. Then, the student model learns density-invariant features from these pseudo-labels using a Mixed-Density Student (MDS) module. Anchor-Based Contrastive Learning (ABCont) is used to facilitate contrastive learning with anchors, which helps to improve the robustness and accuracy of feature learning. The progressive training strategy is also shown, where the model is gradually adapted to handle pairs of point clouds with increasing distances.", "section": "3 Methodology"}, {"figure_path": "OCcfKzXded/figures/figures_3_2.jpg", "caption": "Figure 2: The Overall Pipeline. FGCM(Sec. 3.2) first adapt the teacher model to a data-specific teacher for the current mini-batch, and then mine reliable pseudo-labels. Next, MDS(Sec. 3.4) learns density-invariant features from pseudo-labels. ABCont(Sec. 3.3) is applied for adapting the teacher and transferring knowledge to the student in the feature space.", "description": "This figure illustrates the overall pipeline of the INTEGER method for unsupervised point cloud registration.  It shows a two-stage teacher-student training framework. The first stage involves FGCM (Feature-Geometry Coherence Mining) adapting the teacher model to each mini-batch of data and mining pseudo-labels.  The second stage uses these pseudo-labels to train the Mixed-Density Student (MDS) model, which learns density-invariant features.  Anchor-Based Contrastive Learning (ABCont) is applied throughout to enhance the robustness and transferability of features from teacher to student. The figure highlights the key modules and information flow within the framework.", "section": "3 Methodology"}, {"figure_path": "OCcfKzXded/figures/figures_5_1.jpg", "caption": "Figure 4: Toy Example for ABCont. Anchor-based methods introduce fewer pairwise relationships and are robust against inevitable label noise.", "description": "This figure illustrates the core concept of Anchor-Based Contrastive Learning (ABCont). The left panel shows a sample-based contrastive learning approach where many pairwise relationships are formed between positive and negative samples, making it sensitive to noise in pseudo-labels.  The right panel demonstrates ABCont, which uses anchors to represent inliers and outliers.  By leveraging anchors, ABCont reduces the number of pairwise relationships, improving robustness against noisy pseudo-labels and leading to more effective contrastive learning. The figure highlights the benefits of using anchors for better distinguishing between positive and negative samples in the presence of noise.", "section": "3.3 Anchor-Based Contrastive Learning"}, {"figure_path": "OCcfKzXded/figures/figures_8_1.jpg", "caption": "Figure 5: Before v.s. After Self-Adaption in FGCM: Point-wise Feature & Correspondence-wise Similarity Distribution indicate that the self-adaption results in more discriminative features.", "description": "This figure visualizes the impact of the Per-Batch Self-Adaptation module within the Feature-Geometry Coherence Mining (FGCM) component of the INTEGER algorithm.  It displays the distribution of features for inliers and outliers, both before and after the self-adaptation process. The top row shows the distributions before self-adaptation, revealing significant overlap between inliers and outliers in both feature space and similarity distribution. The bottom row shows that after self-adaptation, the distributions are more distinct, indicating that the process effectively enhances feature discriminability, leading to improved performance in identifying true correspondences.", "section": "4.3 Ablation Study"}, {"figure_path": "OCcfKzXded/figures/figures_14_1.jpg", "caption": "Figure 2: The Overall Pipeline. FGCM(Sec. 3.2) first adapt the teacher model to a data-specific teacher for the current mini-batch, and then mine reliable pseudo-labels. Next, MDS(Sec. 3.4) learns density-invariant features from pseudo-labels. ABCont(Sec. 3.3) is applied for adapting the teacher and transferring knowledge to the student in the feature space.", "description": "This figure illustrates the overall pipeline of the INTEGER method for unsupervised point cloud registration.  It shows the two-stage training process involving a teacher and a student network. The Feature-Geometry Coherence Mining (FGCM) module dynamically adapts the teacher to each mini-batch, mining reliable pseudo-labels. These pseudo-labels are then used to train the Mixed-Density Student (MDS), which learns density-invariant features. Anchor-Based Contrastive Learning (ABCont) facilitates contrastive learning using anchors, enabling robust feature space learning and knowledge transfer from teacher to student.", "section": "3 Methodology"}, {"figure_path": "OCcfKzXded/figures/figures_15_1.jpg", "caption": "Figure 7: Sensitivity of hyperparameters in FGCM module.", "description": "This figure shows the sensitivity analysis of two hyperparameters used in the Feature-Geometry Coherence Mining (FGCM) module: the number of putative correspondences (k) to enlarge the initial set of correspondences (C<sub>i-1</sub>) and the maximum number of iterations.  The x-axes represent different values of each hyperparameter, while the y-axes show the inlier ratio (IR) percentage in the pseudo-labels.  The plots indicate that the performance of the FGCM module is relatively stable across a range of values for both hyperparameters, suggesting robustness to hyperparameter tuning. For optimal performance, k = 0.4|C<sub>u</sub>| and max iterations = 100 were chosen.", "section": "4.2 Analysis"}, {"figure_path": "OCcfKzXded/figures/figures_16_1.jpg", "caption": "Figure 8: Visualization of Generated Pairs in the Synthetic Pretraining Stage. The synthetic pair on the right is generated from a single real scan on the right. We additionally visualize the result of periodic sampling in the top-right corner of synthetic pairs.", "description": "This figure shows examples of synthetic pairs generated for synthetic teacher pretraining.  The left side displays a real LiDAR scan. The right side shows a synthetic pair generated from that scan, illustrating how the method creates partially overlapping point cloud pairs to simulate the irregularities of real-world LiDAR data acquisition.  A zoomed-in section highlights the periodic sampling that simulates the sparsity and inconsistencies typical of real LiDAR data.", "section": "A.2 Implementation Details"}]