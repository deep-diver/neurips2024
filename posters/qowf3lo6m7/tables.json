[{"figure_path": "QoWf3lo6m7/tables/tables_3_1.jpg", "caption": "Table 1: Notations for tokens in Section 4. \"\u2192\" and \"\u2190\" denote forward and backward relationships for the reversal curse. \"\u2192\" and \"\u2192\" denote direct and indirect implication for COT. R\u2081 and R\u2082 are relationship tokens in Section 4.3. A, B, C, Ai, Bi, Ci denote tokens representing entities.", "description": "This table lists notations used for different types of tokens in Section 4 of the paper.  It clarifies the meaning of symbols representing entities, forward/backward relationships (relevant to the \"reversal curse\" phenomenon), and direct/indirect implications (relevant to Chain-of-Thought reasoning).", "section": "4 One-Layer Transformers"}, {"figure_path": "QoWf3lo6m7/tables/tables_27_1.jpg", "caption": "Table 2: Full list of hyperparameters for AdamW optimizer and training.", "description": "This table lists the hyperparameters used for training the GPT-2 model in the experiments described in the paper.  The hyperparameters include the learning rate, weight decay, beta parameters for AdamW, batch size, and number of epochs.", "section": "5 Experiments"}, {"figure_path": "QoWf3lo6m7/tables/tables_28_1.jpg", "caption": "Table 3: The list of different configurations for experiments in Appendices E.2 and E.3. Default choices are boldened for each row.", "description": "This table lists the different hyperparameter configurations used in the experiments for the reversal curse and chain-of-thought.  It shows the range of values tested for the number of layers, number of heads, vocabulary size, entity length, positional encoding type (None, Absolute, Relative), and whether token and positional embeddings were learnable or frozen. The default settings used in the main experiments are highlighted in bold.", "section": "5 Experiments"}]