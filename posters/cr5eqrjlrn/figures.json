[{"figure_path": "cr5EQRJlRn/figures/figures_1_1.jpg", "caption": "Figure 1: Left: illustration of BitDelta (Liu et al., 2024b), which employs 1-bit quantization for all the delta weights. Middle: illustration of low-rank compression (Ryu et al., 2023b), retaining the top-k singular values and the corresponding singular vectors. Right: illustration of the proposed Delta-CoMe method, which represents the singular vectors of larger singular values using high-bit vectors while compressing the singular vectors of smaller singular values into low-bit representations. This method is inspired by the long-tail distribution of singular values in delta weights.", "description": "This figure compares three different delta compression methods: BitDelta, SVD low rank, and the proposed Delta-CoMe.  Each subfigure shows the distribution of bits used to quantize singular vectors against their corresponding singular values. BitDelta uses 1 bit for all singular vectors. SVD low rank retains only the top k singular values and vectors, discarding the rest. Delta-CoMe uses a mixed-precision approach, allocating more bits to the singular vectors with larger singular values and fewer bits to those with smaller singular values, reflecting the long-tail distribution observed in delta weights.", "section": "1 Introduction"}, {"figure_path": "cr5EQRJlRn/figures/figures_3_1.jpg", "caption": "Figure 2: Illustration of Delta-CoMe, where we utilize varying bit-widths for singular vectors with different singular values. Singular vectors corresponding to larger singular values are assigned higher bit-widths. For extremely small singular values, we omit the singular vectors (i.e., 0-bit).", "description": "This figure illustrates the Delta-CoMe method. It shows how the method compresses delta weights between an aligned model and a backbone model by using different bit-widths for singular vectors based on their singular values.  Larger singular values are represented with higher bit-widths (e.g., 8-bit), while smaller singular values use lower bit-widths (e.g., 3-bit, 2-bit), and the smallest are omitted (0-bit). This mixed-precision approach aims to achieve a balance between compression and accuracy. The figure visually shows this process for three different aligned models (Code, Math, and Multi-Modal).", "section": "3 Approach"}, {"figure_path": "cr5EQRJlRn/figures/figures_8_1.jpg", "caption": "Figure 3: Inference time of the PyTorch and Triton implementation of Delta-CoMe.", "description": "This figure shows the inference time comparison between PyTorch and Triton implementations of the Delta-CoMe model. The left subplot shows how inference time changes with varying batch sizes, while the right subplot demonstrates the impact of different hidden sizes on inference time.  Both subplots show that the Triton implementation significantly outperforms the PyTorch implementation in terms of speed.", "section": "5.5 Inference Speed and Memory Cost"}, {"figure_path": "cr5EQRJlRn/figures/figures_9_1.jpg", "caption": "Figure 1: Left: illustration of BitDelta (Liu et al., 2024b), which employs 1-bit quantization for all the delta weights. Middle: illustration of low-rank compression (Ryu et al., 2023b), retaining the top-k singular values and the corresponding singular vectors. Right: illustration of the proposed Delta-CoMe method, which represents the singular vectors of larger singular values using high-bit vectors while compressing the singular vectors of smaller singular values into low-bit representations. This method is inspired by the long-tail distribution of singular values in delta weights.", "description": "This figure compares three different delta compression methods: BitDelta, low-rank compression, and the proposed Delta-CoMe.  It highlights how each method handles the singular values and vectors of the delta weights, showing BitDelta using 1-bit quantization, low-rank compression reducing the number of singular values, and Delta-CoMe using mixed-precision quantization based on the magnitude of the singular values.  The long-tail distribution of singular values motivates the mixed-precision approach in Delta-CoMe.", "section": "3 Approach"}]