[{"type": "text", "text": "OPERA: Automatic Offline Policy Evaluation with Re-weighted Aggregates of Multiple Estimators ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Allen Nie1 Yash Chandak1 Christina J. Yuan2 Anirudhan Badrinath1 Yannis Flet-Berliac3 Emma Brunskill1\u2217 ", "page_idx": 0}, {"type": "text", "text": "1Computer Science, Stanford University 2Computer Science, University of Texas, Austin 3Cohere ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Offilne policy evaluation (OPE) allows us to evaluate and estimate a new sequential decision-making policy\u2019s performance by leveraging historical interaction data collected from other policies. Evaluating a new policy online without a confident estimate of its performance can lead to costly, unsafe, or hazardous outcomes, especially in education and healthcare. Several OPE estimators have been proposed in the last decade, many of which have hyperparameters and require training. Unfortunately, choosing the best OPE algorithm for each task and domain is still unclear. In this paper, we propose a new algorithm that adaptively blends a set of OPE estimators given a dataset without relying on an explicit selection using a statistical procedure. We prove that our estimator is consistent and satisfies several desirable properties for policy evaluation. Additionally, we demonstrate that when compared to alternative approaches, our estimator can be used to select higher-performing policies in healthcare and robotics. Our work contributes to improving ease of use for a general-purpose, estimator-agnostic, off-policy evaluation framework for offline RL. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Offilne reinforcement learning (RL) involves learning better sequential decision policies from logged historical data, such as learning a personalized policy for math education software (Mandel et al., 2014; Ruan et al., 2024), providing treatment recommendations in the ICU (Komorowski et al., 2018; Luo et al., 2024) or learning new controllers for robotics(Kumar et al., 2020; Yu et al., 2020). Offline policy evaluation (OPE), in which the performance $J(\\pi_{e})$ of a new evaluation policy $\\pi_{e}$ is estimated given historical data, is a common subroutine in offilne RL for policy selection, and can be particularly important when deciding whether to deploy a new decision policy that might be unsafe or costly. Offline policy evaluation methods estimate the performance of an evaluation policy $\\pi_{e}$ given data collected by a behavior policy $\\pi_{b}$ . There are many existing OPE algorithms, including those that create importance sampling-based estimators (IS) (Precup, 2000), value-based estimators (FQE) (Le et al., 2019), model-based estimators (Paduraru, 2013; Liu et al., 2018b; Dong et al., 2023), doubly robust estimators Jiang and Li (2016); Thomas and Brunskill (2016), and minimax-style estimators (Liu et al., 2018a; Nachum et al., 2019; Yang et al., 2020). ", "page_idx": 0}, {"type": "text", "text": "This raises an important practical question: given a set of different OPE methods, each producing a particular value estimate for an evaluation policy, what value estimate should be returned? A simple approach is to avoid the problem and pick only one OPE algorithm or look at the direction of a set of OPE algorithms\u2019 scores as a coarse agreement measure. Voloshin et al. (2021) offered heuristics based on high-level domain structure (e.g., horizon length, stochasticity, or partial observability), but this does not account for instance-specific information related to the offline dataset adn policies. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this paper we seek to aggregate the results of a set of multiple off-policy RL estimators to produce a new estimand with low mean square error. This work is related to several streams of prior work: (1) multi-armed bandit and RL algorithms that combine two estimands to yield a more accurate estimate; (2) multi-armed bandit and RL algorithms that select a single estimand out of a set of estimands, and (3) stacked generalization / meta-learning / super learning methods in machine learning. ", "page_idx": 1}, {"type": "text", "text": "Research in (1) builds on doubly robust (DR) estimation in statistics to produce an estimand that combines important sampling and model-based methods (Jiang and Li, 2016; Gottesman et al., 2019; Farajtabar et al., 2018). Similarly, accounting for multiple steps, the MAGIC estimator blends between IS-based and value-based estimators within a trajectory (Thomas and Brunskill, 2016). The second line of work (2) does not combine scores but instead introduces an automatic estimator selection subroutine in the algorithm. However, such methods typically assume strong structural requirements on the input estimators. For example, Su et al. (2020); Tucker and Lee (2021) assume as input a nested set of OPE estimators, where the bias is known to strictly decrease across the set. Zhang and Jiang (2021) leveraged a set of Q-functions trained with fitted Q-evaluation (FQE) and cross-compare them in a tournament style until one Q-function emerged. None of these methods allow mix-and-match of different kinds of OPE estimators. ", "page_idx": 1}, {"type": "text", "text": "Our work is closest to a third line of more distant work, that of stacked generalization (Wolpert, 1992) / meta-learning and super learning across ensembles. There is a long history in statistics and supervised learning of combining multiple input classification or regression functions to produce a better metafunction. Perhaps surprizingly, there is little exploration of this idea to our knowledge in the context of RL or multi-armed bandits. The one exception we are aware of was for heterogeneous treatment effect estimation in a 2-action contextual bandit problem, where Nie and Wager (2021) utilized linear stacking to build a consensus treatment effect estimate using two input estimatands. ", "page_idx": 1}, {"type": "text", "text": "In this paper we introduce the meta-algorithm OPERA (Offilne Policy Evaluation with Re-weighted Aggregates of Multiple Estimators). Inspired by a linear weighted stack, OPERA combines multiple generic OPE estimates for RL in an ensemble to produce an aggregate estimate. Unlike in supervised learning where ground truth labels are available, in our setting a key choice is how to estimate the mean squared error of the resulting weighted ensemble. Under certain conditions, bootstrapping (Efron, 1992) can approximate finite sample bias and variance. We use bootstrapping to compute estimates of the mean squared error of different weightings of the underlying input estimators, which can then be optimized as a constrained convex problem. OPERA can be used with any input OPE estimands. We prove under mild conditions that OPERA produces an estimate that is consistent, and will be at least as accurate as any input estimand. We show on several common benchmark tasks that OPERA achieves more accurate offline policy evaluation than prior approaches, and we also provide a more detailed analysis of the accuracy of OPERA as a function of choices made for the meta-algorithm. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Offline policy evaluation Most commonly used offline policy estimators can be divided into a few categories depending on the algorithm. An important family of estimators focuses on using importance sampling (IS) and weighted importance sampling (WIS) to reweigh the reward from the behavior policy (Precup, 2000). These estimators are known to produce an unbiased estimate but have a high variance when the dataset size is small. For a fully observed Markov Decision Process (MDP), a model-free estimator, such as ftited Q evaluation (FQE), is proposed by Le et al. (2019), and one can also learn a model given the data to produce a model-based (MB) estimate (Pa\u02d8duraru, 2007; Fu et al., 2021; Gao et al., 2023). When the behavior policy\u2019s probability distribution over action is unknown, a minimax style optimization estimator (DualDICE) can jointly estimate the distribution ratio and the policy performance (Nachum et al., 2019). For a partial observable MDP (POMDP), many of these methods have been extended to account for unobserved confounding, such as minimax style estimation (Shi et al., 2022), value-based estimation (Tennenholtz et al., 2020; Nair and Jiang, 2021), uses sensitivity analysis to bound policy value (Kallus and Zhou, 2020; Namkoong et al., 2020; Zhang and Bareinboim, 2021), or learns useful representation over latent space (Chang et al., 2022). ", "page_idx": 1}, {"type": "text", "text": "OPE with multiple estimators Choosing the right estimators has become an issue when there are many proposals even under the same task setup and assumptions. Voloshin et al. (2021) proposed an empirical guide on estimator selection. One line of work tries to combine multiple estimators to produce a better estimate by leveraging the strengths of the underlying estimators, for example, (weighted) doubly robust (DR) method (Jiang and Li, 2016). For contextual bandit, Wang et al. (2017) proposed a switch estimator that interpolates between DM and DR estimates with an explicitly set hyperparameter. For sequential problems, MAGIC blends a model-based estimator and guided importance sampling estimator to produce a single score (Thomas and Brunskill, 2016). Another line of work tackles the many-estimator problem by reformulating multiple estimators as one estimator. Yang et al. (2020) reformulated a set of minimax estimators as a single estimator with different hyperparameter configurations. Yuan et al. (2021) constructed a spectrum of estimators where the endpoints are an IS estimator and a minimax estimator and proposed a hyperparameter to control the new estimator. This line of approaches does not leverage multiple estimators or solve the OPE selection problem because they recast the OPE selection problem as a hyperparameter selection problem. The last line of work provides an automatic selection algorithm that chooses one estimator from many, relying on an ordering of estimators (Tucker and Lee, 2021) or being able to compare the output (such as Q-values) directly Zhang and Jiang (2021). ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Bootstrapping for model selection Using bootstrap to estimate the mean-squared error for model selection was initially proposed by Hall (1990), for the application of kernel density estimation. The idea was subsequently used by others for density estimation (Delaigle and Gijbels, 2004), selecting sample fractions for tail index estimation (Danielsson et al., 2001), time-series forecasting (dos Santos and Franco, 2019) and other econometric applications (Marchetti et al., 2012). Similar ideas have been explored by Thomas et al. (2015) to construct a confidence interval for the estimator. We extend this idea to use bootstrapping to combine multiple OPE estimators to produce a single score. ", "page_idx": 2}, {"type": "text", "text": "3 Notation and Problem Setup ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We define a stochastic Decision Process $M=\\langle S,A,T,r,\\gamma\\rangle$ , where $\\boldsymbol{S}$ is a set of states; $A$ is a set of actions; $T$ is the transition dynamics; $r$ is the reward function; and $\\gamma\\,\\in\\,(0,1)$ is the discount factor. Let $D_{n}=\\{\\tau_{i}\\}_{i=1}^{n}=\\{s_{i},a_{i},s_{i}^{\\prime},r_{i}\\}_{i=1}^{n}$ be the trajectories sampled from $\\pi$ on $M$ . We denote the true performance of a policy $\\pi$ as its expected discounted return $J(\\pi)=\\mathbb{E}_{\\tau\\sim\\rho_{\\pi}}[G(\\tau)]$ where $\\begin{array}{r}{G(\\tau)=\\overleftarrow{\\sum}_{t=0}^{\\infty}\\,\\gamma^{t}r_{t}}\\end{array}$ and $\\rho_{\\pi}$ is the distribution of $\\tau$ under policy $\\pi$ . In an off-policy policy evaluation problem , we take a dataset $D_{n}$ , which can be collected by one or a group of policies which we refer to as the behavior policy $\\pi_{b}$ on the decision process $M$ . An OPE estimator takes in a policy $\\pi_{e}$ and a dataset $D_{n}$ and returns an estimate of its performance, where we mark it as $\\hat{V}:\\Pi\\times\\mathcal{D}\\rightarrow\\mathbb{R}$ . We focus on estimating the performance of a single policy $\\pi$ . We define the true performance of the policy $V^{\\pi}=J(\\pi)$ , and multiple OPE estimates of its performance as $\\hat{V}_{i}^{\\pi}(D_{n})\\stackrel{!}{=}\\hat{V}_{i}(\\pi,D_{n})$ for the $i$ -th OPE\u2019s estimate. ", "page_idx": 2}, {"type": "text", "text": "4 OPERA", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we consider combining results from multiple estimators $\\{\\hat{V}_{i}^{\\pi}\\}_{i=1}^{k}$ to obtain a better estimate for $V^{\\pi}$ . Towards this goal, given $\\{\\hat{V}_{i}^{\\pi}\\}_{i=1}^{k}$ , we propose estimating a set of weights $\\alpha_{i}^{*}\\in\\mathbb{R}$ such that $\\begin{array}{r}{\\bar{V}^{\\pi}:=\\sum_{i=1}^{k}\\alpha_{i}^{*}\\hat{V}_{i}^{\\pi}\\in\\mathbb{R}}\\end{array}$ has the lowest mean squared error (MSE) towards estimating $V^{\\pi}$ . Formally, let $\\hat{\\mathcal{V}}\\in\\mathbb{R}^{k\\times1}$ be a vector whose elements correspond to values from different estimators, and let $\\boldsymbol{\\dot{\\nu}}\\in\\mathbb{R}^{k\\times1}$ correspond to a vector where each element is the same and corresponds to $V^{\\pi}$ . Let $\\alpha^{*}\\in\\mathbb{R}^{k\\times1}$ be a vector with values of all $\\alpha_{i}^{*}$ \u2019s and let $\\alpha\\in\\mathbb{R}^{k\\times1}$ be an estimate of $\\alpha^{*}$ . For any estimator $\\hat{V}_{i}^{\\pi}$ , the mean-squared error is denoted by, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{MSE}(\\hat{V}_{i}^{\\pi}):=\\mathbb{E}_{D_{n}}\\left[\\left(\\hat{V}_{i}^{\\pi}(D_{n})-V^{\\pi}\\right)^{2}\\right]\\in\\mathbb{R},}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where we make $\\hat{V}_{i}^{\\pi}$ explicitly depend on $D_{n}$ to indicate that the expectation is over the random variables $\\hat{V}_{i}^{\\pi}$ which depend on the sampled data $D_{n}$ . With this formulation, estimating $\\alpha^{*}$ can be elicited as a solution to the following constrained optimization problem. ", "page_idx": 2}, {"type": "text", "text": "Remark 1. Let $\\textstyle\\sum_{i=1}^{k}\\alpha_{i}=1$ , then ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\alpha^{*}\\in\\underset{\\alpha\\in\\mathbb{R}^{k\\times1}}{\\arg\\operatorname*{min}}\\quad\\alpha^{\\top}A\\alpha\\quad,\\mathrm{where}\\quad A:=\\mathbb{E}\\bigg[\\Big(\\widehat{\\mathcal{V}}-\\mathcal{V}\\Big)\\Big(\\widehat{\\mathcal{V}}-\\mathcal{V}\\Big)^{\\top}\\bigg]\\in\\mathbb{R}^{k\\times k}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Using the fact that $\\textstyle\\sum_{i=1}^{k}\\alpha_{i}=1$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{MSE}\\big(\\Bar{V}^{\\pi}\\big)=\\mathbb{E}\\left[\\left(\\sum_{i=1}^{k}\\alpha_{i}\\hat{V}_{i}^{\\pi}-V^{\\pi}\\right)^{2}\\right]=\\mathbb{E}\\left[\\left(\\sum_{i=1}^{k}\\alpha_{i}\\Big(\\hat{V}_{i}^{\\pi}-V^{\\pi}\\Big)\\right)^{2}\\right].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Now re-writing the equation above equation 3 in vector form, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{MSE}\\left(\\bar{V}^{\\pi}\\right)=\\mathbb{E}\\Bigg[\\!\\left(\\left(\\hat{\\mathcal{V}}-\\mathcal{V}\\right)^{\\top}\\!\\alpha\\right)^{2}\\!\\Bigg]=\\mathbb{E}\\!\\left[\\alpha^{\\top}\\!\\left(\\hat{\\mathcal{V}}-\\mathcal{V}\\right)\\!\\left(\\hat{\\mathcal{V}}-\\mathcal{V}\\right)^{\\top}\\!\\alpha\\right]\\!.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Finally, simplifying equation 4 further ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{MSE}\\big(\\Bar{V}^{\\pi}\\big)=\\alpha^{\\top}\\mathbb{E}\\bigg[\\Big(\\hat{\\mathcal{V}}-\\mathcal{V}\\Big)\\Big(\\hat{\\mathcal{V}}-\\mathcal{V}\\Big)^{\\top}\\bigg]\\alpha=\\alpha^{\\top}A\\alpha.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Therefore, $\\alpha$ that minimizes $\\mathrm{MSE}\\big(\\Bar{V}^{\\pi}\\big)$ is equivalent to $\\alpha$ that minimizes $\\alpha^{\\top}A\\alpha$ . ", "page_idx": 3}, {"type": "text", "text": "It is worth highlighting that the optimization problem in Remark 1 is convex in $\\alpha$ with linear constraint and thus can be solved by any off-the-shelf solvers (Diamond and Boyd, 2016). ", "page_idx": 3}, {"type": "text", "text": "Estimating $A$ : An advantage of Remark 1 is that it provides the objective for estimating $\\alpha^{*}$ . Unfortunately, this objective depends on $A$ , and thus on $V^{\\pi}$ , which is not available. Further, observe that $A$ can be decomposed as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{A=\\mathbb{E}\\!\\left[\\left(\\hat{\\mathcal{V}}-\\mathbb{E}\\!\\left[\\hat{\\mathcal{V}}\\right]\\right)\\left(\\hat{\\mathcal{V}}-\\mathbb{E}\\!\\left[\\hat{\\mathcal{V}}\\right]\\right)^{\\top}\\right]+\\left[\\mathbb{E}\\!\\left[\\hat{\\mathcal{V}}-\\mathcal{V}\\right]\\left[\\hat{\\mathcal{V}}-\\mathcal{V}\\right]^{\\top}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the first term corresponds to co-variance between the estimators $\\{V_{i}^{\\pi}\\}_{i=1}^{k}$ and the second term corresponds to the outer product between their biases. One potential approach for approximating $A$ could be to ignore biases. While this could resolve the issue of not requiring access to $\\mathcal{V}$ , ignoring bias can result in severe underestimation of $A$ , especially in finite-sample settings or when function approximation is used. Further, even if we ignore the biases, it is not immediate how to compute the covariance of various OPE estimators, e.g., FQE. ", "page_idx": 3}, {"type": "text", "text": "We propose overcoming these challenges by constructing $\\hat{A}\\in\\mathbb{R}^{k\\times k}$ , an estimate of $A\\in\\mathbb{R}^{k\\times k}$ , using a statistical bootstrapping procedure (Efron and Tibshirani, 1994). Subsequently, we will use the $\\hat{A}$ as a plug-in replacement for $A$ to search for the values of $\\alpha$ as discussed in Remark 1. There is a rich literature on using bootstrap to estimate bias (Efron, 1990; Efron and Tibshirani, 1994; Hong, 1999; Shi, 2012; Mikusheva, 2013) and variance (Chen, 2017b; Gamero et al., 1998; Shao, 1990; Ghosh et al., 1984; Li and Maddala, 1999) of an estimator that can be leveraged to estimate the terms in equation 6. Instead of estimating the bias and variance individually, we directly use the bootstrap MSE estimate (Chen, 2017a; Williams, 2010; Cao, 1993; Hall, 1990) to approximate $A$ . ", "page_idx": 3}, {"type": "text", "text": "For bootstrap estimation to work, two key challenges need to be resolved. Even if provided with $V^{\\pi}$ , the regular bootstrap is not guaranteed to yield an MSE estimate which is asymptotic to the true MSE if the distribution has heavy tails (Ghosh et al., 1984). Furthermore, $V^{\\pi}$ is unknown in the first place. To address these challenges we follow the work by Hall (1990), where the first issue is resolved by using sub-sampling based bootstrap resamples of size $n_{1}<n$ , where $n_{1}$ is of a smaller order than $n$ . Therefore, we draw data $D_{n_{1}}^{*}=\\{\\tau_{1}^{*},...,\\tau_{n_{1}}^{*}\\}$ from $D_{n}=\\{\\tau_{1},...,\\tau_{n}\\}$ with replacement. To resolve the second issue, we leverage the MSE estimate by Hall (1990), and approximate equation 1 using ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\widehat{\\mathbf{MSE}}(\\hat{V}_{i}^{\\pi}):=\\mathbb{E}_{D_{n_{1}}^{*}}\\left[\\left(\\hat{V}_{i}^{\\pi}(D_{n_{1}}^{*})-\\hat{V}_{i}^{\\pi}\\right)^{2}\\bigg|D_{n}\\right].\\in\\mathbb{R}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Building upon this direction, we propose using the following estimator $\\hat{A}$ for $A$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{A}:=\\mathbb{E}_{D_{n_{1}}^{*}}\\bigg[\\Big(\\hat{\\mathcal{V}}(D_{n_{1}}^{*})-\\hat{\\mathcal{V}}\\Big)\\Big(\\hat{\\mathcal{V}}(D_{n_{1}}^{*})-\\hat{\\mathcal{V}}\\Big)^{\\top}\\bigg|D_{n}\\bigg],\\in\\mathbb{R}^{k\\times k}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "and we substitute $\\hat{\\alpha}$ for $A$ in equation 2 to obtain the weights for combining estimates $\\{\\hat{V}_{i}^{\\pi}\\}_{i=1}^{k}$ ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{\\bar{V}}^{\\pi}:=\\sum_{i=1}^{k}\\hat{\\alpha}_{i}\\hat{V}_{i}^{\\pi}\\in\\mathbb{R}\\quad\\mathrm{where},\\quad\\hat{\\alpha}\\in\\underset{\\alpha\\in\\mathbb{R}^{k\\times1}}{\\arg\\operatorname*{min}}\\quad\\alpha^{\\top}\\hat{A}\\alpha\\in\\mathbb{R}^{k\\times1}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "There are two key advantages of the proposed procedure: (1) Bias: it does not require access to the ground truth performance estimates $\\theta^{*}$ . In the supervised learning setting, a held-out/validation set can provide a way to infer approximation error, However, for the OPE setting there is no such held-out dataset that can be used to obtain reliable estimates of the ground truth performance. (2) Variance: Depending on the choice of the estimator (e.g., FQE), it might not be possible to have a closed-form estimate of the variance, especially when using rich function approximators. Using statistical bootstrapping, OPERA mitigates both these issues and thus is particularly suitable for off-policy evaluation. ", "page_idx": 4}, {"type": "text", "text": "Estimating $\\alpha^{*}$ : We now consider how error in estimating the optimal weight coefficient $\\alpha^{*}$ affects the MSE of the resulting estimator ${\\hat{\\bar{V}}}^{\\pi}$ . Without loss of generality, we consider $|\\hat{V}_{i}^{\\pi}|\\leq1$ , since we can trivially normalize each estimator\u2019s output by $|V_{\\operatorname*{max}}|$ . We now prove that under the mild assumption that the error in the estimated $\\hat{\\alpha}$ can be bounded as some function of the dataset size, that we can bound the mean squared error of the resulting value estimate: ", "page_idx": 4}, {"type": "text", "text": "Theorem 1 (Finite Sample Analysis). Assume given $n$ samples in dataset $D$ , and let $\\Delta_{c}\\ :=\\$ $\\mathbb{E}_{D_{n}}\\left[\\left({\\bar{V}}^{\\pi}-V^{\\pi}\\right)^{2}\\right]$ , there exists a $\\lambda>0$ such that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\forall i,\\quad\\mathbb{E}_{D_{n}}[|\\hat{\\alpha}_{i}-\\alpha_{i}^{*}|]\\leq n^{-\\lambda},}\\\\ &{\\mathrm{MSE}(\\hat{\\bar{V}}^{\\pi})\\leq\\frac{k^{2}}{n^{2\\lambda}}+\\Delta_{c}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The error of OPERA is divided into two terms. First note that $\\Delta_{c}$ is the approximation error: the difference between the true estimate of the policy performance $V^{\\pi}$ and the best estimand OPERA can yield when using the optimal (unknown) $\\alpha^{*}$ . If $V^{\\pi}$ can be expressed as a linear combination of the input OPE estimands \u03b8\u02c6i, then there is zero approximation error and $\\Delta_{c}=0$ . The second term in the bound comes from the estimation error due to estimating $\\alpha^{*}-$ this arises from the bootstrapping process used for estimating $A$ in equation 8. For this second term we compute an upper bound using a Cauchy-Schwartz inequality. This term decreases as the dataset size $n$ increases. The resulting error depends on the rate at which the estimated $\\hat{\\alpha}$ converges to the true $\\alpha$ as a function of the dataset size. For example, if $\\lambda=0.5$ , (a $n^{-.5}$ rate), the MSE will converge at a $n^{-1}$ rate in the first term, and if $\\lambda=0.25$ (a $n^{-.25}$ rate) the MSE will converge at a $n^{-0.5}$ rate in the first term. We provide the full proof in Appendix A.4. ", "page_idx": 4}, {"type": "text", "text": "We show a full practical implementation of OPERA in Algorithm 1, where we demonstrate how to efficiently construct $\\hat{A}$ and compute $\\hat{\\alpha}$ . ", "page_idx": 4}, {"type": "text", "text": "4.1 Properties of OPERA", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "For $\\hat{A}$ obtained from the bootstrap procedure in equation 8 to be an asymptotically accurate estimate of $A$ , (a) a consistent estimator of $\\mathcal{V}$ is required, and (b) the estimators $\\hat{\\mathcal{V}}$ need to be smooth. We discuss these points in more detail in Appendix A.3. In the following, we theoretically establish the properties of OPERA on performance improvement and consistency. We also demonstrate how OPERA allows us to interpret each estimator\u2019s quality. Further, in Section 6, we empirically study the effectiveness of OPERA even when we do not have any consistent base estimators, or $\\hat{V}_{i}^{\\pi}$ is constructed using deep neural networks. ", "page_idx": 4}, {"type": "text", "text": "Performance Improvement It would be ideal that the combined estimator $\\hat{\\bar{V}}^{\\pi}$ does not perform worse than any of the base estimators $\\{\\hat{V}_{i}^{\\pi}\\}_{i=1}^{n}$ . As OPERA optimizes for the MSE, we can directly obtain the following desired result. ", "page_idx": 4}, {"type": "text", "text": "Theorem 2 (Performance improvement). $I f\\hat{\\alpha}=\\alpha^{*},\\forall i\\in\\{1,...,k\\},\\quad M S E(\\hat{\\bar{V}}^{\\pi})\\leq M S E(\\hat{V}_{i}^{\\pi}).$ ", "page_idx": 4}, {"type": "text", "text": "However, observe that due to bootstrap approximation, $\\hat{A}$ may not be equal to $A$ , and thus $\\hat{\\alpha}$ may not be equal to $\\alpha^{*}$ . Nonetheless, as we will illustrate in Section 6, even in the non-idealized setting OPERA can often achieve MSE better than any of the base estimators $\\{V_{i}^{\\pi}\\}_{i=1}^{n}$ . ", "page_idx": 4}, {"type": "text", "text": "Consistency Some prior works that deal with multiple OPE estimators assume that there is at least one known consistent estimator (Thomas and Brunskill, 2016). Under a similar assumption that $\\exists\\hat{V}_{i}^{\\pi}:\\hat{V}_{i}^{\\pi}\\ \\xrightarrow{p}\\ J(\\pi)$ , OPERA can be made to fall back to the consistent estimator after a large $n$ , such that ${\\hat{\\bar{V}}}^{\\pi}$ is also consistent, i.e., $\\hat{\\bar{V}}^{\\pi}\\,\\xrightarrow{p}\\,J(\\pi)$ . Naturally, as ${\\bar{V}}^{\\pi}$ is a weighted combination of the base estimators $\\{\\hat{V}_{i}^{\\pi}\\}_{i=1}^{k}$ , if all the base estimators provide unreliable estimates, even in the limit of infinite data, then there is not much that can be achieved by weighted combinations of these unreliable estimators. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Interpretability With a linear weighted formulation for $\\bar{V}^{\\bar{\\pi}}$ , OPERA allows for the inspection of the assigned weights to which give further insights into the procedure. In Figure 1 we provide a synthetic example to illustrate the impact of bias and variance of the input estimators on the values of $\\alpha$ . ", "page_idx": 5}, {"type": "text", "text": "Consider a case where there are two OPE estimators $\\hat{V}_{1}^{\\pi}$ and $\\hat{V}_{2}^{\\pi}$ ) and two corresponding weights $\\alpha_{1}$ and $\\alpha_{2}$ ). Let the true unknown quantity be $V^{\\pi}\\,=\\,0$ . As we can see below, when both estimators have low bias, but one has higher variance, OPERA assigns a higher magnitude of $\\alpha_{i}$ for $V_{i}^{\\pi}$ with a lower variance (Figure1, left). When both estimators have similar variance, and their biases have opposite signs with similar magnitude, then $\\alpha_{2}\\approx\\alpha_{1}$ (Figure1, middle left). ", "page_idx": 5}, {"type": "text", "text": "Interestingly, unlike related prior work (Thomas and Brunskill, 2016), our optimization procedure in equation 2 does not require $\\alpha_{i}~\\geq~0$ . Therefore the resulting estimator ${\\bar{V}}^{\\pi}$ may assign negative weights for some of the estimators. ", "page_idx": 5}, {"type": "image", "img_path": "T6LOGZBC2m/tmp/43252d30df4336998be2c69d33a14f23d86937035f1197be036297cfbde7e641.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "This can be observed for the case when the sign of the $\\overline{{\\mathtt{B i a s}(\\hat{V}_{1}^{\\pi})}}$ and $\\mathtt{B i a s}(\\hat{V}_{1}^{\\pi})$ are the same. In such a case, using a positive and a negative weight can help cancel out the biases of the base estimators, as observed in Figure1 (middle right). When one estimator has no bias and the other has no variance, $\\alpha$ values are inversely proportional to their contributions towards the MSE (Figure1, right). ", "page_idx": 5}, {"type": "image", "img_path": "T6LOGZBC2m/tmp/fc956e3701f85c4eeaf1f41b17e869a47f80e0e0be3d7a964597002edaa9db9f.jpg", "img_caption": ["Figure 1: Interpreting weights for different estimators. X-axis shows the value of $\\hat{V}_{1}^{\\pi}$ and $\\mathrm{\\bfY}.$ axis shows the value of $\\hat{V}_{2}^{\\pi}$ . "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "5 Experiment ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We now evaluate OPERA on a number of domains commonly used for offline policy evaluation.   \nExperimental details, when omitted, are presented in the appendix. ", "page_idx": 5}, {"type": "text", "text": "5.1 Task/Domains ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Contextual Bandit. We validate the performance of OPERA on the synthetic bandit domain with a 10-dimensional feature space proposed in SLOPE (Su et al., 2020). This domain illustrates how OPERA compares to an estimator-selection algorithm (SLOPE) that assumes a special structure between the estimators. The true reward is a non-linear neural network function. The reward estimators are parametrized by kernels and the bandwidths are the main hyperparameters. As in their paper, we ran 180 configurations of this simulated environment with different parametrization of the environment. Each configuration is replicated 30 times. ", "page_idx": 5}, {"type": "text", "text": "Sepsis. This domain is based on a simulator that allows us to model learning treatment options for sepsis patients in ICU (Oberst and Sontag, 2019). There are 8 actions and a $+1/-1/0$ reward at the episode end. We experiment with two settings: Sepsis-MDP and Sepsis-POMDP, where some crucial states have been masked. We evaluate 7 different policies: an optimal policy and 6 noised suboptimal policies, which we obtain by adding uniform noise to the optimal policy. ", "page_idx": 5}, {"type": "image", "img_path": "T6LOGZBC2m/tmp/613c0259a7102b30aeaffa6ee909cfd3ebe2f43ce7e4d671b94c0b267da1acce.jpg", "img_caption": ["Figure 2: Left: Results for contextual bandits. (a) MSE of estimators when the dataset size grows. (b) CDF of normalized MSE across 180 conditions by the worst MSE of that condition. Better methods lie in the top-left quadrant. Right: (c) For an MDP domain (Sepsis), we show that as dataset sizes increase, our bootstrap estimation of MSE approaches true MSE for each OPE estimator. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Graph. Voloshin et al. (2019) introduced a ToyGraph environment with a horizon length $\\mathrm{T}$ and an absorbing state $x_{\\mathrm{{abs}}}=2T$ . Rewards can be deterministic or stochastic, with $+1$ for odd states, -1 for even states plus one based on the penultimate state.We evaluate the considered methods on a short horizon $\\scriptstyle\\mathrm{H}=4$ , varying stochasticity of the reward and transitions, and MDP/POMDP settings. ", "page_idx": 6}, {"type": "text", "text": "D4RL-Gym. D4RL (Fu et al., 2020) is an offilne RL standardized benchmark designed and commonly used to evaluate the progress of offline RL algorithms. We use 6 datasets ( $200\\mathrm{k}$ samples each) from three Gym environments: Hopper, HalfCheetah, and Walker2d. We use two datasets from each: the medium-replay dataset, which consists of samples from the experience replay buffer, and the medium dataset, which consists of samples collected by the medium-quality policy. We use conservative Q-learning (CQL) (Kumar et al., 2020), implicit Q-learning (IQL) (Kostrikov et al., 2021), and TD3 (Fujimoto et al., 2018). We train 6 policies from these three algorithms with 2 different hyperparameters for the neural network. We selected 2 FQE hyperparameters for each task and picked 2 checkpoints (one early, one late) to obtain 4 estimators to build the OPE ensemble. ", "page_idx": 6}, {"type": "text", "text": "5.2 Baseline Ensemble OPE Methods ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We compare to using single OPE estimators as well as two new baseline algorithms that combine OPE estimates together. AvgOPE: We can compute a simple average estimator that just outputs the average of all underlying OPE estimates. If an estimator in the ensemble outputs an arbitrarily bad value, this estimator has no implicit mechanism to ignore such an adversarial estimator. BestOPE: We select the OPE estimator that has the smallest estimated MSE. This estimator can be better than AvgOPE as it can ignore bad estimators. In addition, in different domains, we compare to other OPE strategies such as BVFT (Batch Value Function Tournament): making pariwise comparisons between different Q-function estimators with the BVFT-loss (Xie and Jiang, 2021; Zhang and Jiang, 2021). SLOPE: an estimator selection method that based on Lepski\u2019s method, assuming the estimators forming an order of decreasing variance and increasing bias (Yuan et al., 2021). DR (Doubly Robust): a semi-parametric estimator that combines the IS estimator and FQE estimator to have an unbiased low variance estimator (Jiang and Li, 2016; Gottesman et al., 2019; Farajtabar et al., 2018). All of these methods place explicit constraints on the type of OPE estimator to include. ", "page_idx": 6}, {"type": "table", "img_path": "T6LOGZBC2m/tmp/4a3eba670ab0f6b9805db0fab0f8e85dfbd9921904c618a37fd2bb266c2aa7ea.jpg", "table_caption": ["Table 1: We report the Mean-Squared Error (MSE) for the Sepsis domain. Each number is averaged across 20 trials. We underscore the estimator that has the lowest MSE in the ensemble. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "table", "img_path": "T6LOGZBC2m/tmp/6cfbc8d1963dcb6fe6f4f78c2273065cc854bfa6274d0422f9f759871e8ea878.jpg", "table_caption": ["Table 2: Root Mean-Squared Error (RMSE) of different OPE algorithms across D4RL tasks. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "5.3 Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Contextual Bandit We report the result in Figure 2. Figure 2a shows that as the dataset size grows, the bootstrapping procedure employed by OPERA can quickly estimate the performance each estimator and compute a weighted score that is better than a single estimator. In the ultra-small data regime, OPERA is worse than single-estimator selection style algorithms, mainly because OPERA does not explicitly reject estimators. We can add an additional procedure to reject bad estimators and then combine the rest with OPERA, using a rejection algorithm by Lee et al. (2022). ", "page_idx": 7}, {"type": "text", "text": "Sepsis We report the results in Table 1. In this domain, OPERA is able to produce an estimate, on average, across many policies with different degrees of optimality, that matches and exceeds the best estimator in the ensemble. Even though in three out of four tasks, OPERA MSE is close to the MSE of the best estimator in the ensemble, in the MDP $({\\mathrm{N}}{=}200)$ ) setup, OPERA is able to get a significantly lower MSE than any of the estimators in the ensemble, suggesting a future direction of carefully choosing a set of weak estimators to put in the ensemble to obtain a strong estimator. ", "page_idx": 7}, {"type": "text", "text": "Graph We report the graph domain result in Appendix A.9 and in Table 5. We find a similar result to the Sepsis domain. OPERA is able to outperform AvgOPE and BestOPE in different setups. ", "page_idx": 7}, {"type": "text", "text": "D4RL We report the results in Table 2. We choose this domain because, in continuous control tasks, the horizon is often very long. Many OPE estimators that rely on short-horizon or discrete actions will not be able to extend to this domain. A popular OPE choice is FQE with function approximation, but it is difficult to determine hyperparameters like early stopping, network architecture, and learning rate. We can see that even though FQE used in D4RL is not a consistent estimator and does not satisfy OPERA\u2019s theoretical assumption, we are still able to combine the estimations to reach an aggregate estimate with lower MSE. ", "page_idx": 7}, {"type": "text", "text": "6 Discussion: Different MSE Estimation Strategies ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "6.1 Estimating MSE with MAGIC ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Part of our algorithm implicitly involves estimating the MSE of each OPE estimator. In our algorithm we do this using bootstrapping but other alternatives are possible. For example, prior work by (Thomas and Brunskill, 2016) provided a way to estimate the bias and variance of an OPE estimator are computed through per-trajectory OPE scores and used this as part of their MAGIC estimator. However, this method cannot estimate the MSE of self-normalizing estimators (such as WIS) or minimax-style estimators (such as any estimator in the DICE family (Yang et al., 2020)). We denote this estimator as $\\widehat{\\mathrm{MSE}}_{\\mathrm{MAGIC}}(V^{\\pi})$ and now explore how our approach of using boostrapping compares to this method in an illustrative setting. ", "page_idx": 7}, {"type": "text", "text": "In particular, we consider estimating the MSE of the FQE and IS estimands on the Sepsis-POMDP and Sepsis-MDP domains. MAGIC estimates the bias of an OPE as the distance between the OPE ", "page_idx": 7}, {"type": "table", "img_path": "T6LOGZBC2m/tmp/dbd75c47814954f769572efec2e1d79d89a1622f845236412c6cc544f85adad9.jpg", "table_caption": ["Table 3: We compare two styles of MSE estimations and how well they can estimate the true MSE of each estimator. We report averaged results over 10 trials, with $N{=}200$ . "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "T6LOGZBC2m/tmp/a61da59b75da5a5ed170d22e13455f5c335acba95141432ee8d25d379c13865b.jpg", "table_caption": ["Table 4: We report the Mean-Squared Error (MSE) for the Sepsis domain. We additionally present two variants of OPERA where we experimented with different MSE estimation strategies. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "value and the closest upper or lower bound of a weighted importance sampling (WIS) policy estimate.   \nWe use a percentile bootstrap to construct a $50\\%$ confidence interval CI around WIS. ", "page_idx": 8}, {"type": "text", "text": "Our bootstrap $\\widehat{\\mathrm{MSE}}(V^{\\pi})$ procedure is able to provide a consistently more accurate estimate of the true MSE of the FQE estimate compared to $\\widehat{\\mathrm{MSE}}_{\\mathrm{MAGIC}}(V^{\\pi})$ and a comparable or better one for the IS estimate (see Table 3). We suspect that this is due to MAGIC\u2019s unique way of computing bias. Specifically, MAGIC computes bias by comparing two estimates (in this case, FQE and the uppper/lower bounds on WIS) which may significantly misestimate the bias in some situations. ", "page_idx": 8}, {"type": "text", "text": "6.2 Variants of OPERA with Different Strategies ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We now explore two alternative strategies to estimate the MSE of each estimator. The first strategy is, instead of using the estimator\u2019s own score as the centering variable $\\hat{\\mathcal{V}}$ , we use a consistent and unbiased estimator\u2019s score as $\\hat{\\mathcal{V}}$ . We call this OPERA-IS. Another strategy is to use the idea from (Thomas and Brunskill, 2016)\u2019s MAGIC algorithm, where the bias estimate of each estimator compares the estimand to the upper or lower confidence bound of a weighted importance sampling estimator, as above. We call this OPERA-MAGIC. These are two new variants of our OPERA algorithm that will may lead to learning different $\\hat{\\alpha}$ weights and producing different linearly stacked estimates. We use these two new methods, and compute the true MSE of the resulting stacked estimate, compared to OPERA and other baseline estimates. We use the Sepsis domains to illustrate the results and use as input IS, WIS and FQE OPE estimates. ", "page_idx": 8}, {"type": "text", "text": "The true MSE of the resulting estimates are presented in Table 4. While using an unbiased consistent estimator as the centering variable can help further improve OPERA\u2019s estimate, sometimes it also hurts the performance (MDP $\\scriptstyle\\mathrm{N}=1000$ setting). OPERA-MAGIC however almost always performs worse than the best estimator in the ensemble. This suggests that when combining OPE scores this bound on the bias, which will provide a distorted estimate of the estimator bias especially in low data regimes, can lead to learning less effective weightings of the input OPE estimands. OPERA remains a solid option across all settings presented in the table. ", "page_idx": 8}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We propose a novel offline policy evaluation algorithm, OPERA, that leverages ideas from stack generalization to combine many OPE estimators to produce a single estimate that achieves a lower MSE. Though such stacked generalization / meta-learning has been frequently used to create better estimates from ensembles of input methods in supervised learning, to our knowledge this is the first time it has been explored in offline reinforcement learning. One challenge is that unlike in supervised learning, we do not have ground truth labels for offline policy learning. OPERA uses bootstrapping to estimate the MSE for each OPE estimator in order to find a set of weights to blend each OPE\u2019s estimate. We provide a finite sample analysis of OPERA\u2019s performance under mild assumptions, and demonstrate that OPERA provides notably more accurate offilne policy evaluation estimates compared to prior methods in benchmark bandit tasks and offline RL tasks, including a Sepsis simulator and the D4RL settings. There are many interesting directions for future work, including using more complicated meta-aggregators. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The research reported in this paper was supported in part by a Stanford HAI Hoffman-Yee grant and an NSF #2112926 grant. We thank Sanath Kumar Krishnamurthy, Omer Gottesman, Yi Su, and Adith Swaminathan for the discussions. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Cao, R. (1993). Bootstrapping the mean integrated squared error. Journal of Multivariate Analysis, 45(1):137\u2013160.   \nChang, J., Wang, K., Kallus, N., and Sun, W. (2022). Learning bellman complete representations for offline policy evaluation. In International Conference on Machine Learning, pages 2938\u20132971. PMLR.   \nChen, Y.-C. (2017a). Introduction to resampling methods. lecture 5: Bootstrap. https://faculty. washington.edu/yenchic/17Sp_403/Lec5-bootstrap.pdf.   \nChen, Y.-C. (2017b). Introduction to resampling methods. lecture 9: Introduction to the bootstrap theory. https://faculty.washington.edu/yenchic/17Sp_403/Lec9_theory.pdf.   \nChernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W., and Robins, J. (2016). Double/debiased machine learning for treatment and causal parameters. arXiv preprint arXiv:1608.00060.   \nDanielsson, J., de Haan, L., Peng, L., and de Vries, C. G. (2001). Using a bootstrap method to choose the sample fraction in tail index estimation. Journal of Multivariate analysis, 76(2):226\u2013248.   \nDelaigle, A. and Gijbels, I. (2004). Bootstrap bandwidth selection in kernel density estimation from a contaminated sample. Annals of the Institute of Statistical Mathematics, 56(1):19\u201347.   \nDiamond, S. and Boyd, S. (2016). Cvxpy: A python-embedded modeling language for convex optimization. The Journal of Machine Learning Research, 17(1):2909\u20132913.   \nDong, K., Flet-Berliac, Y., Nie, A., and Brunskill, E. (2023). Model-based offilne reinforcement learning with local misspecification. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 7423\u20137431.   \ndos Santos, T. R. and Franco, G. C. (2019). Bootstrap for correcting the mean square error of prediction and smoothed estimates in structural models. Brazilian Journal of Probability and Statistics.   \nEfron, B. (1990). More efficient bootstrap computations. Journal of the American Statistical Association, 85(409):79\u201389.   \nEfron, B. (1992). Bootstrap methods: another look at the jackknife. Springer.   \nEfron, B. and Tibshirani, R. J. (1994). An introduction to the bootstrap. CRC press.   \nFarajtabar, M., Chow, Y., and Ghavamzadeh, M. (2018). More robust doubly robust off-policy evaluation. In International Conference on Machine Learning, pages 1447\u20131456. PMLR.   \nFu, J., Kumar, A., Nachum, O., Tucker, G., and Levine, S. (2020). D4rl: Datasets for deep data-driven reinforcement learning. arXiv preprint arXiv:2004.07219.   \nFu, J., Norouzi, M., Nachum, O., Tucker, G., Wang, Z., Novikov, A., Yang, M., Zhang, M. R., Chen, Y., Kumar, A., et al. (2021). Benchmarks for deep off-policy evaluation. arXiv preprint arXiv:2103.16596.   \nFujimoto, S., Hoof, H., and Meger, D. (2018). Addressing function approximation error in actor-critic methods. In International conference on machine learning, pages 1587\u20131596. PMLR.   \nGamero, M. J., Garc\u00eda, J. M., and Reyes, A. M. (1998). Bootstrapping statistical functionals. Statistics & probability letters, 39(3):229\u2013236.   \nGao, Q., Gao, G., Chi, M., and Pajic, M. (2023). Variational latent branching model for off-policy evaluation. arXiv preprint arXiv:2301.12056.   \nGhosh, M., Parr, W. C., Singh, K., and Babu, G. J. (1984). A note on bootstrapping the sample median. The Annals of Statistics, 12(3):1130\u20131135.   \nGottesman, O., Liu, Y., Sussex, S., Brunskill, E., and Doshi-Velez, F. (2019). Combining parametric and nonparametric models for off-policy evaluation. In International Conference on Machine Learning, pages 2366\u20132375. PMLR.   \nHall, P. (1990). Using the bootstrap to estimate mean squared error and select smoothing parameter in nonparametric problems. Journal of multivariate analysis, 32(2):177\u2013203.   \nHong, H. (1999). Lecture 11: Bootstrap.   \nJiang, N. and Li, L. (2016). Doubly robust off-policy value evaluation for reinforcement learning. In International Conference on Machine Learning, pages 652\u2013661. PMLR.   \nKallus, N. and Zhou, A. (2020). Confounding-robust policy evaluation in infinite-horizon reinforcement learning. Advances in neural information processing systems, 33:22293\u201322304.   \nKomorowski, M., Celi, L. A., Badawi, O., Gordon, A. C., and Faisal, A. A. (2018). The artificial intelligence clinician learns optimal treatment strategies for sepsis in intensive care. Nature medicine, 24(11):1716\u20131720.   \nKostrikov, I., Nair, A., and Levine, S. (2021). Offilne reinforcement learning with implicit q-learning. arXiv preprint arXiv:2110.06169.   \nKumar, A., Zhou, A., Tucker, G., and Levine, S. (2020). Conservative q-learning for offline reinforcement learning. Advances in Neural Information Processing Systems, 33:1179\u20131191.   \nLe, H., Voloshin, C., and Yue, Y. (2019). Batch policy learning under constraints. In International Conference on Machine Learning, pages 3703\u20133712. PMLR.   \nLee, J. N., Tucker, G., Nachum, O., Dai, B., and Brunskill, E. (2022). Oracle inequalities for model selection in offline reinforcement learning. arXiv preprint arXiv:2211.02016.   \nLi, H. and Maddala, G. (1999). Bootstrap variance estimation of nonlinear functions of parameters: an application to long-run elasticities of energy demand. Review of Economics and Statistics, 81(4):728\u2013733.   \nLiu, Q., Li, L., Tang, Z., and Zhou, D. (2018a). Breaking the curse of horizon: Infinite-horizon off-policy estimation. Advances in Neural Information Processing Systems, 31.   \nLiu, Y., Gottesman, O., Raghu, A., Komorowski, M., Faisal, A. A., Doshi-Velez, F., and Brunskill, E. (2018b). Representation balancing mdps for off-policy policy evaluation. Advances in neural information processing systems, 31.   \nLuo, Z., Pan, Y., Watkinson, P., and Zhu, T. (2024). Position: reinforcement learning in dynamic treatment regimes needs critical reexamination. Journal of Machine Learning Research.   \nMandel, T., Liu, Y.-E., Levine, S., Brunskill, E., and Popovic, Z. (2014). Offline policy evaluation across representations with applications to educational games. In AAMAS, volume 1077.   \nMarchetti, S., Tzavidis, N., and Pratesi, M. (2012). Non-parametric bootstrap mean squared error estimation for m-quantile estimators of small area averages, quantiles and poverty indicators. Computational Statistics & Data Analysis, 56(10):2889\u20132902.   \nMikusheva, A. (2013). Time series analysis. lecture 9: Bootstrap. https://ocw.mit.edu/ courses/14-384-time-series-analysis-fall-2013/resources/mit14_384f13_ lec9/.   \nNachum, O., Chow, Y., Dai, B., and Li, L. (2019). Dualdice: Behavior-agnostic estimation of discounted stationary distribution corrections. Advances in Neural Information Processing Systems, 32.   \nNair, Y. and Jiang, N. (2021). A spectral approach to off-policy evaluation for pomdps. arXiv preprint arXiv:2109.10502.   \nNamkoong, H., Keramati, R., Yadlowsky, S., and Brunskill, E. (2020). Off-policy policy evaluation for sequential decisions under unobserved confounding. Advances in Neural Information Processing Systems, 33:18819\u201318831.   \nNie, X. and Wager, S. (2021). Quasi-oracle estimation of heterogeneous treatment effects. Biometrika, 108(2):299\u2013319.   \nOberst, M. and Sontag, D. (2019). Counterfactual off-policy evaluation with gumbel-max structural causal models. In International Conference on Machine Learning, pages 4881\u20134890. PMLR.   \nPa\u02d8duraru, C. (2007). Planning with approximate and learned models of markov decision processes. PhD thesis, University of Alberta.   \nPaduraru, C. (2013). Off-policy evaluation in Markov decision processes. PhD thesis, McGill University.   \nPrecup, D. (2000). Eligibility traces for off-policy policy evaluation. Computer Science Department Faculty Publication Series, page 80.   \nRuan, S., Nie, A., Steenbergen, W., He, J., Zhang, J., Guo, M., Liu, Y., Dang Nguyen, K., Wang, C. Y., Ying, R., Landay, J., and Brunskill, E. (2024). Reinforcement learning tutor better supported lower performers in a math task. Machine Learning, pages 1\u201326.   \nShao, J. (1990). Bootstrap estimation of the asymptotic variances of statistical functionals. Annals of the Institute of Statistical Mathematics, 42:737\u2013752.   \nShi, C., Uehara, M., Huang, J., and Jiang, N. (2022). A minimax learning approach to off-policy evaluation in confounded partially observable markov decision processes. In International Conference on Machine Learning, pages 20057\u201320094. PMLR.   \nShi, X. (2012). Econ 715. lecture 10: Bootstrap. https://www.ssc.wisc.edu/\\~xshi/econ715/ Lecture_10_bootstrap.pdf.   \nSu, Y., Srinath, P., and Krishnamurthy, A. (2020). Adaptive estimator selection for off-policy evaluation. In International Conference on Machine Learning, pages 9196\u20139205. PMLR.   \nTennenholtz, G., Shalit, U., and Mannor, S. (2020). Off-policy evaluation in partially observable environments. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 10276\u201310283.   \nThomas, P. and Brunskill, E. (2016). Data-efficient off-policy policy evaluation for reinforcement learning. In International Conference on Machine Learning, pages 2139\u20132148. PMLR.   \nThomas, P., Theocharous, G., and Ghavamzadeh, M. (2015). High confidence policy improvement. In International Conference on Machine Learning, pages 2380\u20132388. PMLR.   \nTucker, G. and Lee, J. (2021). Improved estimator selection for off-policy evaluation. Workshop on Reinforcement Learning Theory at the 38th International Conference on Machine Learning.   \nVoloshin, C., Le, H. M., Jiang, N., and Yue, Y. (2019). Empirical study of off-policy policy evaluation for reinforcement learning. arXiv preprint arXiv:1911.06854.   \nVoloshin, C., Le, H. M., Jiang, N., and Yue, Y. (2021). Empirical study of off-policy policy evaluation for reinforcement learning. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1).   \nWang, Y.-X., Agarwal, A., and Dud\u0131k, M. (2017). Optimal and adaptive off-policy evaluation in contextual bandits. In International Conference on Machine Learning, pages 3589\u20133597. PMLR. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Williams, C. J. (2010). The bootstrap method for estimating mse. https://www.webpages.uidaho. edu/\\~chrisw/stat514/bootstrap1.pdf. ", "page_idx": 12}, {"type": "text", "text": "Wolpert, D. H. (1992). Stacked generalization. Neural networks, 5(2):241\u2013259.   \nXie, T. and Jiang, N. (2021). Batch value-function approximation with only realizability. In International Conference on Machine Learning, pages 11404\u201311413. PMLR.   \nYang, M., Nachum, O., Dai, B., Li, L., and Schuurmans, D. (2020). Off-policy evaluation via the regularized lagrangian. Advances in Neural Information Processing Systems, 33:6551\u20136561.   \nYu, T., Thomas, G., Yu, L., Ermon, S., Zou, J. Y., Levine, S., Finn, C., and Ma, T. (2020). Mopo: Model-based offline policy optimization. Advances in Neural Information Processing Systems, 33:14129\u201314142.   \nYuan, C., Chandak, Y., Giguere, S., Thomas, P. S., and Niekum, S. (2021). Sope: Spectrum of off-policy estimators. Advances in Neural Information Processing Systems, 34:18958\u201318969.   \nZhang, J. and Bareinboim, E. (2021). Non-parametric methods for partial identification of causal effects. Columbia CausalAI Laboratory Technical Report.   \nZhang, S. and Jiang, N. (2021). Towards hyperparameter-free policy selection for offline reinforcement learning. Advances in Neural Information Processing Systems, 34. ", "page_idx": 12}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 OPERA Diagram", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We describe the OPERA framework as a two-stage process in Figure 3. The first stage involves using black-box statistical methods (such as Bootstrap) to estimate the quality of each OPE estimator. The information is then used to estimate a weight $\\alpha$ through a convex optimization objective in $\\operatorname{Eq}9$ . At stage two, we use the learned $\\alpha$ to combine each estimator\u2019s score to output the OPERA score. ", "page_idx": 13}, {"type": "image", "img_path": "T6LOGZBC2m/tmp/9dd706fbf2800182604292201be08f80783efbd9027c8cdea26d016ed77d0ccf.jpg", "img_caption": ["Figure 3: OPERA framework as a two-stage process. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "A.2 Notation Table ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We provide a list of important notations used in this paper. ", "page_idx": 13}, {"type": "table", "img_path": "T6LOGZBC2m/tmp/ccafa34043801b2bbcc028542b17a0b32acd11aaa5159b8b757be47a288c1420.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "A.3 Bootstrap Convergence ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we provide a high-level discussion of the bootstrap procedure and its asymptotic validity. We refer the readers to the works by (Cao, 1993; Hall, 1990) for a more fine-grained analysis and convergence rates when estimating MSE using statistical bootstrap. Individual treatment of bias (Efron, 1990; Efron and Tibshirani, 1994; Hong, 1999; Shi, 2012; Mikusheva, 2013) and variance (Chen, 2017b; Gamero et al., 1998; Shao, 1990; Ghosh et al., 1984; Li and Maddala, 1999) can also be found. ", "page_idx": 14}, {"type": "text", "text": "In the following, we will discuss the consistency of $\\hat{A}$ estimated using bootstrap, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{A}_{i.j}-A_{i,j}\\xrightarrow{a.s_{\\vdots}}0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Towards this goal, we will consider the following conditions imposed on the set of the base estimators $\\{\\hat{V}_{i}^{\\pi}\\}_{i=1}^{k}$ , ", "page_idx": 14}, {"type": "text", "text": "\u2022 $\\forall i$ , $\\hat{V}_{i}^{\\pi}$ is uniformly bounded.   \n$\\begin{array}{r l}{\\bullet\\,\\,\\forall i,\\,\\,}&{{}\\hat{V}_{i}^{\\pi}\\,\\,{\\xrightarrow{a.s_{\\vdots}}}\\,c_{i}.}\\end{array}$ $\\forall i$ , $\\hat{V}_{i}^{\\pi}$ is smooth with respect to data distribution.   \n$\\begin{array}{r}{\\bullet\\ \\exists\\hat{V}_{k}^{\\pi}:\\hat{V}_{k}^{\\pi}\\ {\\xrightarrow{a.s}}\\ c_{k}=V^{\\pi}.}\\end{array}$ ", "page_idx": 14}, {"type": "text", "text": "Recall from equation 2, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A_{i,j}=\\mathbb{E}\\Big[\\Big(\\hat{V}_{i}^{\\pi}-V^{\\pi}\\Big)\\Big(\\hat{V}_{j}^{\\pi}-V^{\\pi}\\Big)\\Big]}\\\\ &{\\phantom{A_{i,j}}=\\mathbb{E}\\Big[\\Big(\\hat{V}_{i}^{\\pi}-\\mathbb{E}[\\hat{V}_{i}^{\\pi}]+\\mathbb{E}[\\hat{V}_{i}^{\\pi}]-V^{\\pi}\\Big)}\\\\ &{\\phantom{A_{i,j}}\\qquad\\qquad\\Big(\\hat{V}_{j}^{\\pi}-\\mathbb{E}[\\hat{V}_{j}^{\\pi}]+\\mathbb{E}[\\hat{V}_{j}^{\\pi}]-V^{\\pi}\\Big)\\Big]}\\\\ &{\\phantom{A_{i,j}}=\\mathbb{E}\\Big[\\Big(\\hat{V}_{i}^{\\pi}-\\mathbb{E}[\\hat{V}_{i}^{\\pi}]\\Big)\\Big(\\hat{V}_{j}^{\\pi}-\\mathbb{E}[\\hat{V}_{j}^{\\pi}]\\Big)\\Big]}\\\\ &{\\phantom{A_{i,j}}\\qquad+\\mathbb{E}\\Big[\\Big(\\mathbb{E}[\\hat{V}_{i}^{\\pi}]-V^{\\pi}\\Big)\\Big(\\mathbb{E}[\\hat{V}_{j}^{\\pi}]-V^{\\pi}\\Big)\\Big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Let $X_{n}\\ :=\\ \\left(\\hat{V}_{i}^{\\pi}-\\mathbb{E}[\\hat{V}_{i}^{\\pi}]\\right)$ and $Y_{n}\\ :=\\ \\left(\\hat{V}_{j}^{\\pi}-\\mathbb{E}[\\hat{V}_{j}^{\\pi}]\\right)$ . As $\\hat{V}_{i}^{\\pi}\\ \\xrightarrow{a.s_{\\mathrm{i}}}\\ c_{i}$ and $\\hat{V}_{i}^{\\pi}$ is uniformly bounded, using (Thomas and Brunskill, 2016, Lemma 2), we have $\\mathbb{E}[\\hat{V}_{i}^{\\pi}]\\xrightarrow{a.s_{\\mathrm{i}}}c_{i}$ . Similarly, we have $\\mathbb{E}[\\hat{V}_{j}^{\\pi}]\\ {\\xrightarrow{a.s_{\\mathrm{i}}}}\\ c_{j}$ as $\\hat{V}_{j}^{\\pi}\\xrightarrow{a.s_{\\mathrm{i}}}c_{j}$ . Then using continuous mapping theorem, ", "page_idx": 14}, {"type": "equation", "text": "$$\nX_{n}Y_{n}\\stackrel{a.s.}{\\longrightarrow}(c_{i}-c_{i})(c_{j}-c_{j})=0.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Now using (Thomas and Brunskill, 2016, Lemma 2), ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\Big[\\Big(\\hat{V}_{i}^{\\pi}-\\mathbb{E}[\\hat{V}_{i}^{\\pi}]\\Big)\\Big(\\hat{V}_{j}^{\\pi}-\\mathbb{E}[\\hat{V}_{j}^{\\pi}]\\Big)\\Big]=\\mathbb{E}[X_{n}Y_{n}]\\stackrel{a.s.}{\\longrightarrow}0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Similarly, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left(\\mathbb{E}[\\hat{V}_{i}^{\\pi}]-V^{\\pi}\\right)\\Big(\\mathbb{E}[\\hat{V}_{j}^{\\pi}]-V^{\\pi}\\Big)\\xrightarrow{a.s_{\\downarrow}}(c_{i}-V^{\\pi})(c_{j}-V^{\\pi})}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Therefore, using equation 19 and equation 20, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{A_{i,j}\\stackrel{a.s_{\\cdot}}{\\longrightarrow}0+(c_{i}-V^{\\pi})(c_{j}-V^{\\pi}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Now we consider the asymptotic property of the bootstrap estimate $\\hat{A}$ of $A$ . ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\hat{A}_{i,j}=\\mathbb{E}_{D_{n_{1}}^{*}|D_{n}}\\left[\\left(\\hat{V}_{i}^{\\pi}(D_{n_{1}}^{*})-\\hat{V}_{k}^{\\pi}\\right)\\left(\\hat{V}_{j}^{\\pi}(D_{n_{1}}^{*})-\\hat{V}_{k}^{\\pi}\\right)\\right]\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\hat{V}_{k}^{\\pi}$ is known to be a consistent estimator, i.e., $\\hat{V}_{k}^{\\pi}\\xrightarrow{a.s_{\\downarrow}}V^{\\pi}$ . Here, $\\hat{V}_{k}^{\\pi}$ could be the WIS or IS or doubly-robust estimators that are known to provide consistent estimates of $V^{\\pi}=J(\\pi)$ . For brevity, we drop the conditional notation on the subscript, and write equation 22 as, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\hat{A}_{i,j}=\\mathbb{E}_{D_{n_{1}}^{*}}\\left[\\left(\\hat{V}_{i}^{\\pi}(D_{n_{1}}^{*})-\\hat{V}_{k}^{\\pi}\\right)\\left(\\hat{V}_{j}^{\\pi}(D_{n_{1}}^{*})-\\hat{V}_{k}^{\\pi}\\right)\\right]\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Simplifying equation 23, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{A}_{i,j}=\\mathbb{E}_{D_{n_{1}}^{*}}\\Bigg[\\Big(\\hat{V}_{i}^{\\pi}(D_{n_{1}}^{*})-\\mathbb{E}_{D_{n_{1}}^{*}}\\Big[\\hat{V}_{i}^{\\pi}(D_{n_{1}}^{*})\\Big]+\\mathbb{E}_{D_{n_{1}}^{*}}\\Big[\\hat{V}_{i}^{\\pi}(D_{n_{1}}^{*})\\Big]-\\hat{V}_{k}^{\\pi}\\Big)}\\\\ &{\\qquad\\qquad\\quad\\Big(\\hat{V}_{j}^{\\pi}(D_{n_{1}}^{*})-\\mathbb{E}_{D_{n_{1}}^{*}}\\Big[\\hat{V}_{j}^{\\pi}(D_{n_{1}}^{*})\\Big]+\\mathbb{E}_{D_{n_{1}}^{*}}\\Big[\\hat{V}_{j}^{\\pi}(D_{n_{1}}^{*})\\Big]-\\hat{V}_{k}^{\\pi}\\Big)\\Bigg]}\\\\ &{=\\mathbb{E}_{D_{n_{1}}^{*}}\\Bigg[\\Big(\\hat{V}_{i}^{\\pi}(D_{n_{1}}^{*})-\\mathbb{E}_{D_{n_{1}}^{*}}\\Big[\\hat{V}_{i}^{\\pi}(D_{n_{1}}^{*})\\Big]\\Big)}\\\\ &{\\qquad\\qquad\\quad\\Big(\\hat{V}_{j}^{\\pi}(D_{n_{1}}^{*})-\\mathbb{E}_{D_{n_{1}}^{*}}\\Big[\\hat{V}_{j}^{\\pi}(D_{n_{1}}^{*})\\Big]\\Big)\\Bigg]}\\\\ &{\\qquad\\qquad+\\mathbb{E}_{D_{n_{1}}^{*}}\\Big[\\hat{V}_{i}^{\\pi}(D_{n_{1}}^{*})-\\hat{V}_{k}^{\\pi}\\Big]\\mathbb{E}_{D_{n_{1}}^{*}}\\Big[\\hat{V}_{j}^{\\pi}(D_{n_{1}}^{*})-\\hat{V}_{k}^{\\pi}\\Big]}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Let $X_{n_{1}}:=\\left(\\hat{V}_{i}^{\\pi}(D_{n_{1}}^{*})-\\mathbb{E}_{D_{n_{1}}^{*}}\\Big[\\hat{V}_{i}^{\\pi}(D_{n_{1}}^{*})\\Big]\\right)$ and $Y_{n_{1}}:=\\left(\\hat{V}_{j}^{\\pi}(D_{n_{1}}^{*})-\\mathbb{E}_{D_{n_{1}}^{*}}\\Big[\\hat{V}_{j}^{\\pi}(D_{n_{1}}^{*})\\Big]\\right)$ . As the empirical distribution $D_{n_{1}}^{*}$ converges to the population distribution, i.e., $D_{n}\\;{\\xrightarrow{a\\cdot s}}\\;D$ , the resampled distribution $D_{n_{1}}^{*}$ from $D_{n}$ also converges to the population distribution, i.e., $D_{n_{1}}^{*}\\xrightarrow{a.s_{\\downarrow}}D$ . Therefore, when the estimator $\\hat{V}_{i}^{\\pi}(D_{n_{1}}^{*})$ is smooth, using the continuous mapping theorem, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\forall i,\\qquad\\operatorname*{lim}_{n_{1}\\to\\infty}\\hat{V}_{i}^{\\pi}(D_{n_{1}}^{*})=\\hat{V}_{i}^{\\pi}\\left(\\operatorname*{lim}_{n_{1}\\to\\infty}D_{n_{1}}^{*}\\right)=\\hat{V}_{i}^{\\pi}(D)=c_{i}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore, similar to before, ", "page_idx": 15}, {"type": "equation", "text": "$$\nX_{n_{1}}Y_{n_{1}}\\stackrel{a.s.}{\\longrightarrow}(c_{i}-c_{i})(c_{j}-c_{j})=0,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and subsequently, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}_{D_{n_{1}}^{*}}[X_{n_{1}}Y_{n_{1}}]\\stackrel{a.s_{\\downarrow}}{\\longrightarrow}0.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Further, as $\\hat{V}_{k}^{\\pi}\\xrightarrow{a.s_{\\vdots}}V^{\\pi}$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{V}_{i}^{\\pi}(D_{n_{1}}^{*})-\\hat{V}_{k}^{\\pi}\\overset{a.s_{\\vdots}}{\\longrightarrow}c_{i}-V^{\\pi}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{D_{n_{1}}^{*}}\\left[\\hat{V}_{i}^{\\pi}(D_{n_{1}}^{*})-\\hat{V}_{k}^{\\pi}\\right]\\mathbb{E}_{D_{n_{1}}^{*}}\\left[\\hat{V}_{j}^{\\pi}(D_{n_{1}}^{*})-\\hat{V}_{k}^{\\pi}\\right]}\\\\ &{\\quad\\quad\\frac{a.s_{\\downarrow}}{\\omega_{i}}\\left(c_{i}-V^{\\pi}\\right)\\!(c_{j}-V^{\\pi}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Using equation 31 and equation 34 in equation 28, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\hat{A}_{i,j}\\overset{a.s.}{\\longrightarrow}0+(c_{i}-V^{\\pi})(c_{j}-V^{\\pi}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Finally, combining equation 21 and equation 35, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{A}_{i.j}-A_{i,j}\\;\\frac{a.s_{\\vdots}}{}\\,0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which gives the desired result. It is worth highlighting that, theoretically, this result relies upon assumptions that the base estimators satisfy regularity conditions and are consistent. In practice, such assumptions might not hold (for e.g., when using FQE to do policy evaluation if the function approximation is under-parameterized). Nonetheless, in Section 6 we empirically illustrate that even when these assumptions are not directly satisfied, OPERA can be effective. ", "page_idx": 15}, {"type": "text", "text": "A.4 Finite Sample Analysis of OPERA", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Without loss of generality, let $\\forall\\pi\\in\\Pi,|J(\\pi)|\\leq1$ , such that we can always consider $\\forall i$ , $|\\hat{V}_{i}^{\\pi}|\\leq1$ (this can be trivially achieved by normalizing each estimator\u2019s output by $|V_{\\operatorname*{max}}|)$ . Let ${\\bar{V}}^{\\pi}$ be a weighted sum of $\\hat{V}_{i}^{\\pi}$ with $\\alpha^{\\star}$ , where the total number of estimators in the ensemble is $k$ . ", "page_idx": 15}, {"type": "text", "text": "In the following, we show how the error in estimating the optimal weight coefficients $\\alpha^{*}$ affects the MSE of the resulting estimator $\\hat{\\bar{V}}^{\\pi}$ . Given $\\{\\hat{V}_{i}^{\\pi}\\}_{i=1}^{k}$ , we assume that $\\hat{A}$ obtained uisng the bootstrap procedure of OPERA will produce $\\hat{\\alpha}$ via Equation 9 (and a resulting estimate of ${\\hat{\\bar{V}}}^{\\pi}$ ). In contrast, using $A$ would have produced $\\alpha^{\\star}$ (and a resulting estimate of $\\bar{V}^{\\pi}$ ). To provide a finite sample characterization of OPERA\u2019s mean squared error, consider the setting where given $n$ samples in dataset $D$ , there exists $\\lambda>0$ , such that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\forall i,\\quad\\mathbb{E}_{D_{n}}[|\\hat{\\alpha}_{i}-\\alpha_{i}^{*}|]\\leq n^{-\\lambda},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the expectation is over the randomness due to data $D_{n}$ that governs the estimates $\\hat{V}_{i}^{\\pi}$ and thus also the weights $\\hat{\\alpha}$ and $\\alpha^{*}$ used to combines these estimates. We now provide a proof of Theorem 1. To bound the MSE of OPERA\u2019s estimate V\u02c6\u00af \u03c0 observe that, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{MSE}(\\hat{\\bar{V}}^{\\pi}):=\\mathbb{E}_{D_{n}}\\bigg[\\Big(\\hat{\\bar{V}}^{\\pi}-V^{\\pi}\\Big)^{2}\\bigg]}\\\\ &{\\quad\\quad\\quad\\quad=\\mathbb{E}_{D_{n}}\\bigg[\\Big(\\hat{\\bar{V}}^{\\pi}-\\bar{V}^{\\pi}+\\bar{V}^{\\pi}-V^{\\pi}\\Big)^{2}\\bigg]}\\\\ &{\\quad\\quad\\quad\\quad\\leq\\underbrace{\\mathbb{E}_{D_{n}}\\Big[(\\hat{\\bar{V}}^{\\pi}-\\bar{V}^{\\pi})^{2}\\Big]}_{=\\Delta_{\\alpha}}+\\underbrace{\\mathbb{E}_{D_{n}}\\Big[\\big(\\bar{V}^{\\pi}-V^{\\pi}\\big)^{2}\\Big]}_{=\\Delta_{\\alpha}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We isolate the error of ${\\hat{\\bar{V}}}^{\\pi}$ into two terms: $\\Delta_{\\alpha}$ and $\\Delta_{c}$ . $\\Delta_{c}$ is the gap between the best estimate OPERA can give with $\\alpha^{\\star}$ and the true estimate of the policy performance $V^{\\pi}$ . If $V^{\\pi}$ can be expressed as a linear combination of $\\hat{V}_{i}^{\\pi}$ , then $\\Delta_{c}=0$ . $\\Delta_{\\alpha}$ is the term we want to further analyze because it depends on the difference between $\\hat{\\alpha}$ and $\\alpha^{*}$ . ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta_{\\alpha}:=\\mathbb{E}_{D_{n}}\\left[\\left(\\hat{V}^{\\pi}-\\bar{V}^{\\pi}\\right)^{2}\\right]}\\\\ &{\\quad=\\mathbb{E}_{D_{n}}\\left[\\left(\\displaystyle\\sum_{i=1}^{k}\\hat{\\alpha}_{i}V_{i}^{\\pi}-\\displaystyle\\sum_{i=1}^{k}\\alpha_{i}^{*}V_{i}^{\\pi}\\right)^{2}\\right]}\\\\ &{\\quad=\\mathbb{E}_{D_{n}}\\left[\\left(\\displaystyle\\sum_{i=1}^{k}(\\hat{\\alpha}_{i}-\\alpha_{i}^{*})\\hat{V}_{i}^{\\pi}\\right)^{2}\\right]}\\\\ &{\\quad\\le\\mathbb{E}_{D_{n}}\\left[\\left(\\displaystyle\\sum_{i=1}^{k}(\\hat{\\alpha}_{i}-\\alpha_{i}^{*})^{2}\\right)\\left(\\displaystyle\\sum_{i=1}^{k}(\\hat{V}_{i}^{\\pi})^{2}\\right)\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the last inequality follows from Cauchy-Schwarz inequality. Now by using the fact that $|\\hat{\\theta}_{i}|\\leq1$ and by plugging equation 37 into equation 44: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta_{\\alpha}\\leq\\mathbb{E}_{D_{n}}\\left[k\\left(\\displaystyle\\sum_{i=1}^{k}(\\hat{\\alpha}_{i}-\\alpha_{i}^{*})^{2}\\right)\\right]}\\\\ &{\\quad=k\\displaystyle\\sum_{i=1}^{k}\\mathbb{E}_{D_{n}}\\left[(\\hat{\\alpha}_{i}-\\alpha_{i}^{*})^{2}\\right]}\\\\ &{\\quad\\leq\\displaystyle\\frac{k^{2}}{n^{2\\lambda}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, combining equation 40 and equation 47, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{MSE}(\\hat{\\bar{V}}^{\\pi})\\leq\\frac{k^{2}}{n^{2\\lambda}}+\\Delta_{c}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "This bound factors the MSE using the term $\\Delta_{c}$ , which is the best a linear combination of estimators can do. Notice that $\\Delta_{c}\\leq\\operatorname*{min}_{i}\\mathrm{MSE}(\\hat{V}_{i}^{\\pi})$ , as the best linear combination of the estimators can at least achieve the MSE of the best estimator, by assigning weight of 1 to the best estimator and 0 to the rest. Therefore, the rate of decay of $\\Delta_{c}$ is bounded above by the rate of convergence of the best estimator in our ensemble. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "The other term $k^{2}/n^{2\\lambda}$ in equation 48 results due to the error in estimating $\\alpha^{*}$ because of the bootstrapping process used for estimating $\\hat{A}$ of $A$ in equation 8. This is dependent on the number of estimators $k-\\mathrm{as}$ we include more estimators in our ensemble, the combination weights $\\alpha\\in\\mathbb{R}^{k}$ that need to be estimated becomes higher dimensional, thereby introducing more errors. However, the overall term decreases as the dataset size $n$ increases. ", "page_idx": 17}, {"type": "text", "text": "A.5 Proofs on Properties of OPERA", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "A.5.1 Invariance ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In the following, we illustrate an important property of OPERA, that the resulting combined estimate ${\\hat{\\bar{V}}}^{\\pi}$ is invariant to the addition of redundant copies of the base estimators $\\{\\hat{V}_{i}^{\\pi}\\}_{i=1}^{n}$ . Without loss of generality, let $\\hat{\\mathcal{V}}_{\\beta}\\,\\in\\,\\mathbb{R}^{(K+1)\\times1}$ be the stack of unique estimators $\\{\\hat{V}_{i}^{\\pi}\\}_{i=1}^{k}$ with $\\hat{V}_{k+1}^{\\pi}$ being a redundant copy of the $\\hat{V}_{k}^{\\pi}$ , ", "page_idx": 17}, {"type": "text", "text": "Theorem 3 (Invariance). If $\\hat{A}$ is positive definite, then $\\hat{\\bar{V}}_{\\beta}^{\\pi}=\\hat{\\bar{V}}^{\\pi}$ , where, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\hat{\\bar{V}}_{\\beta}^{\\pi}:=\\sum_{i=1}^{k+1}\\beta_{i}^{*}\\hat{V}_{i}^{\\pi}\\in\\mathbb{R},\\qquad\\qquad\\qquad w h e r e,\\,\\beta^{*}\\in\\operatorname*{arg\\,min}_{\\beta\\in\\mathbb{R}^{{(k+1)}\\times1}}\\beta^{\\top}B\\beta.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. We prove this by contradiction. Recall that $\\hat{\\alpha}\\in\\mathbb{R}^{k}$ are the weights that minimize the bootstrap estimate of MSE of ${\\hat{\\bar{V}}}^{\\pi}$ consisting of $k$ estimators. ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\widehat{\\mathrm{MSE}}(\\hat{\\alpha}_{1}\\hat{V}_{1}^{\\pi}+\\ldots+\\hat{\\alpha}_{k}\\hat{V}_{k}^{\\pi})=\\hat{\\alpha}^{\\top}\\hat{A}\\hat{\\alpha}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "As $\\hat{V}_{k+1}^{\\pi}$ is a redundant copy of $\\hat{V}_{k}^{\\pi}$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{\\sf M S E}(\\beta_{1}^{*}\\hat{V}_{1}^{\\pi}+\\ldots+\\beta_{k}^{*}\\hat{V}_{k}^{\\pi}+\\beta_{k+1}^{*}\\hat{V}_{k+1}^{\\pi})}\\\\ &{=\\widehat{\\sf M S E}(\\beta_{1}^{*}\\hat{V}_{1}^{\\pi}+\\ldots+(\\beta_{k}^{*}+\\beta_{k+1}^{*})\\hat{V}_{k}^{\\pi})}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Finally, as $\\beta^{*}\\in\\mathbb{R}^{k+1}$ is the weight that minimizes the bootstrap estimate of MSE of $\\hat{\\bar{V}}_{\\beta}^{\\pi}$ . Now, if equation $50<$ equation 52, then one could assign $\\beta_{i}^{*}:=\\hat{\\alpha}_{i}$ for $i\\in\\{1,...,k\\}$ , and $\\beta_{k+1}^{*}=0$ to make equation $52=$ equation 50. Further, notice that as both $\\hat{\\alpha}$ and $\\beta^{*}$ are within the same feasible set of solutions, the above reassignment is also within the feasible set of solutions. Similarly, if equation 50 $>$ equation 52, then one could assign $\\hat{\\alpha}_{i}:=\\beta_{i}^{*}$ for $i\\in\\{1,...,k-1\\}$ , and $\\hat{\\alpha}_{k}=\\beta_{k}^{*}+\\beta_{K+1}^{*}$ to make equation $52=$ equation 50. Hence, if equation 50 does not equal equation 52, then either $\\hat{\\alpha}$ or $\\beta^{*}$ is not optimal and that would be a contradiction. This ensures that $\\dot{\\mathrm{MSE}}(\\hat{\\bar{V}}_{\\beta}^{\\pi})=\\widehat{\\mathrm{MSE}}(\\hat{\\bar{V}}^{\\pi})$ . ", "page_idx": 17}, {"type": "text", "text": "As $\\hat{A}$ is positive definite, it implies that equation 9 is strictly convex with linear constraints. Thus the minimizer $\\hat{\\alpha}$ of equation 9 is unique, and $\\hat{\\bar{V}}_{\\beta}^{\\pi}=\\hat{\\bar{V}}^{\\pi}$ . Note that due to redundancy, $B$ will not be PD despite $\\hat{A}$ being PD. This would imply that there can be multiple values of $\\beta_{k}^{*}$ and $\\beta_{k+1}^{*}$ . Nonetheless, since $\\beta_{k}^{*}+\\beta_{k+1}^{*}=\\hat{\\alpha}_{k}$ , it implies that $\\hat{\\bar{V}}_{\\beta}^{\\pi}=\\hat{\\bar{V}}^{\\pi}$ . ", "page_idx": 17}, {"type": "text", "text": "A.5.2 Performance Improvement ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Theorem 4 (Performance improvement). If $\\hat{\\alpha}=\\alpha^{*}$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\forall i\\in\\{1,...,k\\},\\quad M S E(\\hat{\\bar{V}}^{\\pi})\\leq M S E(\\hat{V}_{i}^{\\pi}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. With a slight overload of notation2, we make the dependency of weights $\\alpha$ explicit and let $\\begin{array}{r}{\\bar{V}^{\\pi}(\\dot{\\alpha})=\\sum_{i=1}^{k}\\bar{\\alpha}_{i}\\hat{V}_{i}^{\\pi}}\\end{array}$ . Let $\\mathrm{{MSE}}(\\bar{V}^{\\pi}(\\alpha)):=\\alpha^{\\top}A\\alpha$ , where $A$ is defined as in equation 2. ", "page_idx": 17}, {"type": "text", "text": "Now from equation 1 and equation 2, we know that for $\\textstyle\\sum_{i=1}^{k}\\alpha_{i}=1$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\alpha^{*}\\in\\arg\\operatorname*{min}_{\\alpha\\in\\mathbb{R}^{k\\times1}}\\mathbf{MSE}(\\bar{V}^{\\pi}(\\alpha)).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore, for any $\\lambda\\in\\mathbb{R}^{k\\times1}$ such that $\\textstyle\\sum_{i=1}^{k}\\lambda_{i}=1$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{MSE}(\\bar{V}^{\\pi}(\\hat{\\alpha}))=\\mathbf{MSE}(\\bar{V}^{\\pi}(\\alpha^{*}))}\\\\ {\\leq\\mathbf{MSE}(\\bar{V}^{\\pi}(\\lambda)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\because{\\hat{\\alpha}}=\\alpha^{*}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Notice that for $\\boldsymbol{e}_{i}\\;:=\\;[0,0,..,1,..,0]$ , where there is a 1 in the $i^{t h}$ position and zero otherwise, $\\bar{V}^{\\pi}(e_{i})=\\hat{V}_{i}^{\\pi}$ . Therefore, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{MSE}(\\Bar{V}^{\\pi}(\\hat{\\alpha}))\\leq\\mathbf{MSE}(\\Bar{V}^{\\pi}(e_{i}))}\\\\ &{\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,=\\mathbf{MSE}(\\hat{V}_{i}^{\\pi})}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\forall i}\\\\ {\\forall i.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore, as $\\hat{\\bar{V}}^{\\pi}\\ =\\ \\bar{V}^{\\pi}(\\hat{\\alpha})$ , we have the desired result that $\\forall i~\\in~\\{1,...,k\\}$ , $\\mathrm{MSE}(\\hat{\\bar{V}}^{\\pi})\\ \\leq$ $\\mathrm{MSE}(\\hat{V}_{i}^{\\pi})$ . ", "page_idx": 18}, {"type": "text", "text": "A.6 Empirical Properties of OPERA", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In Figure 4: (a) We show that as dataset sizes increase, our bootstrap estimation of MSE approaches true MSE for each OPE estimator. (b) We show the MSE of OPERA with true and estimated $A$ matrix. Note both $\\mathrm{MSE}(\\bar{V}^{\\pi})$ and $\\widehat{\\mathrm{MSE}}(\\widehat{\\bar{V}}^{\\pi})$ are near 0 and overlap each other. (c) We show how $\\alpha$ changes between different estimators as dataset size grows. ", "page_idx": 18}, {"type": "image", "img_path": "T6LOGZBC2m/tmp/b35423a2f0ca1e40b6049e65a4e27f1c5917ad811927cac5a0d69f3bec4d0411.jpg", "img_caption": ["Figure 4: Properties of OPERA"], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "A.7 Graph Experiment ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Voloshin et al. (2019) introduced a ToyGraph environment with a horizon length $\\mathrm{T}$ and an absorbing state $x_{\\mathrm{{abs}}}=2T$ . Rewards can be deterministic or stochastic, with $+1$ for odd states, $^-1$ for even states plus one based on the penultimate state.We evaluate the considered methods on a short horizon $_{\\mathrm{H}=4}$ , varying stochasticity of the reward and transitions, and MDP/POMDP settings. We evaluate a single policy in this domain. ", "page_idx": 18}, {"type": "text", "text": "Table 5: We report the Mean-Squared Error (MSE) for the Graph domain. We conduct the experiment on Graph over 10 trials. We underscore the estimator that has the lowest MSE in the ensemble. ", "page_idx": 18}, {"type": "table", "img_path": "T6LOGZBC2m/tmp/c35d12ee039ec33dcd2a4df776d9448cc6897a80cc382bf4fc9b00879ee80d98.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "We report the results in Table 5. In three out of four setups, OPERA is able to produce an estimate that has a lower MSE compared to BestOPE and AvgOPE and is lower than the estimators used in the ensemble. In the deterministic POMDP setting, the IS estimate is significantly off, and OPERA is worse than BestOPE. This provides an important insight into the strengths and limitations of our particular procedure. In small tabular POMDPs, WIS can be quite good. Here, the error in the MSE estimation means that OPERA inaccurately balances the two estimators instead of relying only on WIS. We note that BestOPE does not match the performance of WIS in the ensemble. This is because the error in MSE estimation makes BestOPE erroneously choose the wrong estimator in some trials, resulting in a larger MSE. We note that on three other setups, the MSE estimation is fairly accurate. Therefore, OPERA is able to get a lower MSE than any of the OPEs in the ensemble. ", "page_idx": 18}, {"type": "table", "img_path": "T6LOGZBC2m/tmp/2c9f73239e4623cdefdf1a0ef12e4618824da1ab23102180d2f226e193d9ffc9.jpg", "table_caption": ["Table 6: Root Mean-Squared Error (RMSE) of the FQE estimators with different hyperparameter configurations. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "These results suggest that it is important to consider the tradeoff between bootstrap estimation error and the benefit from combined estimators. When we compare the estimation quality over many policies (Sepsis and D4RL), OPERA does well, but for a particular policy, it might not outperform other estimators. This is an interesting area for future work: if and when it is possible for such a meta-algorithm to provably match the performance of any individual estimator without further assumptions. ", "page_idx": 19}, {"type": "text", "text": "A.8 D4RL Experiment ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Setup D4RL (Fu et al., 2020) is an offline RL standardized benchmark designed and commonly used to evaluate the progress of offline RL algorithms. We use 6 datasets of different quality from three environments: Hopper, HalfCheetah, and Walker2d. We choose the medium and mediumreplay datasets. Medium dataset has 200k samples from a policy trained to approximately 1/3 the performance of a policy trained to completion with SAC. Medium-replay dataset takes the transitions stored in the experience replay buffer of policy \u2013 this dataset can be thought of as a dataset sampled by a mixture of policies. ", "page_idx": 19}, {"type": "text", "text": "Policy Training We train 6 policies from these three algorithms with 2 different hyperparameters for the neural network, Q-learning (CQL) (Kumar et al., 2020), implicit Q-learning (Kostrikov et al., 2021), and $\\mathrm{TD3+BC}$ (Fujimoto et al., 2018). We initialize all neural networks (including both actor and critics, if the algorithm uses both) with the hidden dimensions of [256, 256, 256]. We train with a batch size of 512, with Adam Optimizer. We train for 100 epochs on each dataset. We only change one important hyperparameter per algorithm. We report the discounted return of each policy in Table 11,10,12. We report these scores because they are the prediction target of the FQE algorithm. We report the un-discounted return of each policy in Table 15,16,17. ", "page_idx": 19}, {"type": "table", "img_path": "T6LOGZBC2m/tmp/23d90edaaf897808340315d8337524f72a8d1e6b73f5dfa3a285757e26acac2f.jpg", "table_caption": ["FQE Training We train Fitted Q learning for each policy. As discussed in the main text, FQE has a few hyperparameter choices. We choose 4 hyperparameters for Hopper and HalfCheetah. We "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "T6LOGZBC2m/tmp/43134d84fc8f163c577e2e4cb6ad39d32229804b662ef928b9b19032160204d2.jpg", "table_caption": [], "table_footnote": ["Table 10: Discounted perf of different policies on Hopper task. "], "page_idx": 20}, {"type": "table", "img_path": "T6LOGZBC2m/tmp/22b7f4dfd475672e07831b13118f48f5c7ca3d4939a54a3987bfeb1ed71595b5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "T6LOGZBC2m/tmp/d951e8ef1c620c305964eccc451543d0bb3c8c97082be4b70677f85b3b54a702.jpg", "table_caption": ["Table 12: Discounted perf of different policies on HalfCheetah task. ", "Table 11: Discounted perf of different policies on Walker2D task. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "choose another 4 hyperparameters for Walker2D. The reason is that we noticed the Q-value for Walker2D exploded if we used the same hyperparameters for the two other tasks. We should note that since OPERA does not require OPEs to be the same across tasks. The hyperparameter choices are around the Q-function neural network\u2019s hidden sizes and how many epochs we train each Q-function. Generally, training too long / over-training leads to exploding Q-values. ", "page_idx": 20}, {"type": "table", "img_path": "T6LOGZBC2m/tmp/0a08cefa3c280026d6aa5584c0ada8d826bb106607ae71c7d68d93487215b799.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Table 13: FQE Hyperparameters. Training epochs were chosen to be an early checkpoint and a late checkpoint (before exploding $\\mathrm{Q}-$ values). ", "page_idx": 20}, {"type": "table", "img_path": "T6LOGZBC2m/tmp/0741a16ab3fe3c1b9455a072d01ad367982a84fb04e18df0a8adeba817f65dc2.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Table 14: FQE Hyperparameters. Training epochs were chosen to be an early checkpoint and a late checkpoint (before exploding $\\mathrm{Q}-$ values). ", "page_idx": 20}, {"type": "text", "text": "A.9 Sepsis and Graph Experiment Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "A.9.1 Sepsis ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The first domain is based on the simulator and works by Oberst and Sontag (2019) and revolves around treating sepsis patients. The goal of the policy for this simulator is to discharge patients from the hospital. There are three treatments the policy can choose from antibiotics, vasopressors, and mechanical ventilation. The policy can choose multiple treatments at the same time or no treatment at all, creating 8 different unique actions. ", "page_idx": 20}, {"type": "text", "text": "The simulator models patients as a combination of four vital signs: heart rate, blood pressure, oxygen concentration and glucose levels, all with discrete states (for example, for heart rate low, normal and high). There is a latent variable called diabetes that is present with a $20\\%$ probability which drives the likelihood of fluctuating glucose levels. When a patient has at least 3 of the vital signs simultaneously out of the normal range, the patient dies. If all vital signs are within normal ranges and the treatments are all stopped, the patient is discharged. The reward function is $+1$ if a patient is discharged, $-1$ if a patient dies, and 0 otherwise. We truncate the trajectory to 20 actions $\\scriptstyle(\\mathrm{H}=20)$ ). For this simulator, early termination means we don\u2019t get to observe a positive or negative return on the patient. ", "page_idx": 20}, {"type": "table", "img_path": "T6LOGZBC2m/tmp/886bb65dc4f18d892d36ea7ec6d375a0940a988b3a61c2540bb65ab8c644c4fb.jpg", "table_caption": [], "table_footnote": ["Table 15: Undiscounted perf of different policies on Hopper task. "], "page_idx": 21}, {"type": "table", "img_path": "T6LOGZBC2m/tmp/ef2263fb935a1c0a2eec40b91bc86bb5a66e9b672f01e1f1c0d74d83a2c31196.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "T6LOGZBC2m/tmp/8c2e2a920c74d2c086cdd3b65f64a0a3e237adf1d4677dd6914841623c872d72.jpg", "table_caption": ["Table 17: Undiscounted perf of different policies on HalfCheetah task. ", "Table 16: Undiscounted perf of different policies on Walker2D task. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "We follow the process described by Oberst and Sontag (2019) to marginalize an optimal policy\u2019s action over 2 states: glucose level and whether the patient has diabetes. This creates the Sepsis-POMDP environment. We sample 200 and 1000 patients (trajectories) from Sepsis-POMDP environment with the optimal policy that has $5\\%$ chance of taking a random action. We also sample trajectories from the original MDP using the same policy; we call this the Sepsis-MDP environment. ", "page_idx": 21}, {"type": "text", "text": "FQE Training We use tabular FQE. Therefore, there is no representation mismatch. We additionally use cross-ftiting, a form of procedure commonly used in causal inference (Chernozhukov et al., 2016). Cross-ftiting is a sample-splitting procedure where we swap the roles of main and auxiliary samples to obtain multiple estimates and then average the results. The main goal of cross-fitting is to reduce overftiting. We notice significant performance improvement of our FQE estimator after using cross-fitting. We present the RMSE of each of our trained FQE estimator in Table 6. ", "page_idx": 21}, {"type": "text", "text": "A.9.2 Graph ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "For the graph environment, we set the horizon $\\scriptstyle\\mathrm{H}=4$ , with either POMDP or MDP and ablate on the stochasticity of transition and reward function. The optimal policy for the Graph domain is simply the policy that chooses action 0. All the experiments reported have 512 trajectories. ", "page_idx": 21}, {"type": "text", "text": "A.10 Compute Resource ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The training was done on a small cluster of 6 servers, each with 16GB RAM, and 4-8 GPUs of Nvidia A5000. D4RL was the most computationally expensive experiment. ", "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The contributions are clearly stated in the introduction and detailed in Section 4, and claimed improvements are validated in Section 5. Some claims are empirically verified in Section 6. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: Limitation is discussed in Section 6, and we discussed again in the last paragraph of the Appendix Section A.7. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best ", "page_idx": 22}, {"type": "text", "text": "judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 23}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Theorems are provided in Section 4. Full proofs are included in Appendix Section A.1, A.2, and A.3. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The experiments are described in Section 5 and additional details are in Appendix Section A.5, A.6, A.7, and A.8. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.   \n(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Yes. The code is submitted as part of the supplementary material. We did not use any non-open-source data or model. Additional details are in Appendix Section A.5, A.6, A.7, and A.8. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We provide all the details in Section 5 and additional details are in Appendix Section A.5, A.6, A.7, and A.8. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 24}, {"type": "text", "text": "\u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 25}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We report standard error of the mean (SEM) in Figure 2. We do not conduct any statistical test in this paper. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 25}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We report it in Appendix Section A.9. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 25}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The contributions of this paper (OPERA) do not have direct safety or security implications. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 26}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: In Sections 6 and 7, we discuss both the aspirational goals and their broader impacts. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 26}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: We are not releasing data or models that have a high risk for misuse. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 27}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: datasets used in the experiments are properly attributed. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: We are not introducing new assets. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used. ", "page_idx": 27}, {"type": "text", "text": "\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 28}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: There are no crowdsourcing or human subject studies conducted in this paper. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 28}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: he paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 28}]