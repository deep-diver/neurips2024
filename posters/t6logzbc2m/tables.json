[{"figure_path": "T6LOGZBC2m/tables/tables_6_1.jpg", "caption": "Table 1: We report the Mean-Squared Error (MSE) for the Sepsis domain. Each number is averaged across 20 trials. We underscore the estimator that has the lowest MSE in the ensemble.", "description": "This table shows the mean squared error (MSE) of different offline policy evaluation (OPE) methods on the Sepsis domain, which simulates treatment options for sepsis patients in an ICU.  The results are averaged across 20 trials for both MDP and POMDP settings with 200 and 1000 samples.  The table compares the performance of OPERA to several other individual OPE methods, such as IS, WIS and FQE, and highlights the best-performing method for each condition by underlining the lowest MSE.", "section": "5.2 Baseline Ensemble OPE Methods"}, {"figure_path": "T6LOGZBC2m/tables/tables_7_1.jpg", "caption": "Table 2: Root Mean-Squared Error (RMSE) of different OPE algorithms across D4RL tasks.", "description": "This table presents the Root Mean Squared Error (RMSE) results for various Offline Policy Evaluation (OPE) algorithms across different D4RL (Datasets for Deep Data-driven Reinforcement Learning) tasks.  The tasks are categorized by environment (Hopper, HalfCheetah, Walker2d) and dataset type (medium-replay, medium).  The table compares the performance of OPERA against several single OPE estimators and other multi-OPE estimators including BestOPE, AvgOPE, BVFT, DR, and Dual-DICE.  Lower RMSE values indicate better performance.", "section": "5.2 Baseline Ensemble OPE Methods"}, {"figure_path": "T6LOGZBC2m/tables/tables_8_1.jpg", "caption": "Table 3: We compare two styles of MSE estimations and how well they can estimate the true MSE of each estimator. We report averaged results over 10 trials, with N=200.", "description": "This table compares two methods for estimating the mean squared error (MSE) of offline policy evaluation (OPE) estimators: the proposed bootstrapping method and the MAGIC method.  It shows the estimated MSEs for two estimators (IS and FQE) under two different scenarios (Sepsis-POMDP and Sepsis-MDP) with a sample size of N=200.  The results indicate the accuracy of each MSE estimation method in approximating the true MSE values.", "section": "6.1 Estimating MSE with MAGIC"}, {"figure_path": "T6LOGZBC2m/tables/tables_8_2.jpg", "caption": "Table 1: We report the Mean-Squared Error (MSE) for the Sepsis domain. Each number is averaged across 20 trials. We underscore the estimator that has the lowest MSE in the ensemble.", "description": "This table shows the mean squared error (MSE) for different offline policy evaluation (OPE) estimators on the Sepsis domain. The MSE is calculated for four different OPE methods (IS, WIS, FQE) and three variants of the OPERA algorithm (OPERA, OPERA-IS, OPERA-MAGIC). The results are averaged over 20 trials and different dataset sizes (200 and 1000 samples). The lowest MSE for each setting is highlighted.", "section": "5.2 Baseline Ensemble OPE Methods"}, {"figure_path": "T6LOGZBC2m/tables/tables_13_1.jpg", "caption": "Table 2: Root Mean-Squared Error (RMSE) of different OPE algorithms across D4RL tasks.", "description": "This table presents the root mean squared error (RMSE) of various offline policy evaluation (OPE) algorithms across different tasks within the D4RL benchmark.  It compares the performance of OPERA against single OPE estimators (IS, WIS, FQE, MB, DR, Dual-DICE) and other ensemble methods (AvgOPE, BestOPE, BVFT). The results are categorized by environment (Hopper, HalfCheetah, Walker2D) and dataset type (medium-replay, medium).  Lower RMSE values indicate better performance.", "section": "5.2 Baseline Ensemble OPE Methods"}, {"figure_path": "T6LOGZBC2m/tables/tables_18_1.jpg", "caption": "Table 5: We report the Mean-Squared Error (MSE) for the Graph domain. We conduct the experiment on Graph over 10 trials. We underscore the estimator that has the lowest MSE in the ensemble.", "description": "This table presents the mean squared error (MSE) for different offline policy evaluation (OPE) estimators on the ToyGraph environment. The experiment was conducted 10 times with various settings for stochasticity, observability (MDP vs. POMDP), and the choice of OPE estimator (OPERA, BestOPE, AvgOPE, IS, WIS).  The lowest MSE for each setting is underscored, highlighting OPERA's performance relative to other methods.", "section": "5.1 Task/Domains"}, {"figure_path": "T6LOGZBC2m/tables/tables_19_1.jpg", "caption": "Table 2: Root Mean-Squared Error (RMSE) of different OPE algorithms across D4RL tasks.", "description": "This table presents the Root Mean Squared Error (RMSE) for different offline policy evaluation (OPE) algorithms across multiple tasks from the D4RL benchmark.  The benchmark includes tasks from various robotics domains such as Hopper, HalfCheetah, and Walker2d, with each task having 'medium-replay' and 'medium' datasets, representing different data collection scenarios. Algorithms are compared to OPERA using the 'BestOPE', 'AvgOPE', 'BVFT', 'DR', and 'Dual-DICE' methods, allowing for a comprehensive comparison of their performance across diverse scenarios and datasets.  The lower the RMSE, the better the algorithm's performance in estimating the policy's value.", "section": "5.2 Baseline Ensemble OPE Methods"}, {"figure_path": "T6LOGZBC2m/tables/tables_19_2.jpg", "caption": "Table 1: We report the Mean-Squared Error (MSE) for the Sepsis domain. Each number is averaged across 20 trials. We underscore the estimator that has the lowest MSE in the ensemble.", "description": "This table shows the mean squared error (MSE) of different offline policy evaluation (OPE) estimators on the Sepsis domain.  The results are averaged across 20 trials, and the lowest MSE for each setting is indicated by underlining.  The table compares the performance of different OPE methods, including importance sampling (IS), weighted importance sampling (WIS), fitted Q-evaluation (FQE) and OPERA, demonstrating OPERA's improved accuracy in offline policy evaluation compared to other methods.", "section": "5.2 Baseline Ensemble OPE Methods"}, {"figure_path": "T6LOGZBC2m/tables/tables_20_1.jpg", "caption": "Table 10: Discounted perf of different policies on Hopper task.", "description": "This table presents the discounted performance results for six different policies on the Hopper task, categorized by the dataset used (medium-replay and medium).  The policies evaluated include two versions each of CQL, IQL, and TD3+BC, representing different hyperparameter configurations for each algorithm.  The values likely represent an average discounted cumulative reward, reflecting the algorithm's performance in achieving a specific goal within the simulation.", "section": "5.1 Task/Domains"}, {"figure_path": "T6LOGZBC2m/tables/tables_20_2.jpg", "caption": "Table 10: Discounted perf of different policies on Hopper task.", "description": "This table presents the discounted performance results for different policies evaluated on the Hopper task.  The table shows the discounted reward obtained by each of six different policies on the Hopper task, categorized by the type of dataset used (medium-replay or medium).  These results likely represent a subset of the overall findings, used to illustrate the performance of the OPERA algorithm in a specific context.", "section": "5.1 Task/Domains"}, {"figure_path": "T6LOGZBC2m/tables/tables_20_3.jpg", "caption": "Table 12: Discounted perf of different policies on HalfCheetah task.", "description": "This table presents the discounted performance of six different policies on the HalfCheetah task from the D4RL benchmark.  The policies were trained using three different algorithms (CQL, IQL, TD3+BC) with two hyperparameter settings each.  The \"medium-replay\" and \"medium\" columns indicate the dataset used for training. Discounted reward is a common metric in reinforcement learning, reflecting the cumulative reward over time, discounted by a factor to emphasize near-term rewards.", "section": "5.2 Baseline Ensemble OPE Methods"}, {"figure_path": "T6LOGZBC2m/tables/tables_20_4.jpg", "caption": "Table 1: We report the Mean-Squared Error (MSE) for the Sepsis domain. Each number is averaged across 20 trials. We underscore the estimator that has the lowest MSE in the ensemble.", "description": "This table presents the Mean Squared Error (MSE) for different offline policy evaluation (OPE) estimators on the Sepsis domain.  The results are averaged over 20 trials for both MDP and POMDP settings with varying dataset sizes (N=200 and N=1000). The estimator with the lowest MSE in each case is underscored, highlighting OPERA's performance in comparison to other methods (IS, WIS, FQE).", "section": "5.2 Baseline Ensemble OPE Methods"}, {"figure_path": "T6LOGZBC2m/tables/tables_20_5.jpg", "caption": "Table 2: Root Mean-Squared Error (RMSE) of different OPE algorithms across D4RL tasks.", "description": "This table presents the Root Mean Squared Error (RMSE) achieved by different Offline Policy Evaluation (OPE) algorithms on various D4RL tasks.  D4RL is a standardized benchmark for offline RL, and the tasks represent different robotics control problems with varying data characteristics. The algorithms compared include OPERA (the proposed method), along with baseline methods like AvgOPE (average OPE), BestOPE (best-performing OPE), BVFT (Batch Value Function Tournament), DR (doubly robust), Dual-DICE, and MB (model-based). The results show the RMSE across different datasets ('medium' and 'medium-replay') for three environments: Hopper, HalfCheetah, and Walker2d.", "section": "5.2 Baseline Ensemble OPE Methods"}, {"figure_path": "T6LOGZBC2m/tables/tables_21_1.jpg", "caption": "Table 2: Root Mean-Squared Error (RMSE) of different OPE algorithms across D4RL tasks.", "description": "The table presents the Root Mean Squared Error (RMSE) for different offline policy evaluation (OPE) algorithms across various tasks within the D4RL benchmark.  It compares OPERA's performance against several single OPE estimators and other ensemble methods. The results show RMSE values for different D4RL environments (Hopper, HalfCheetah, Walker2d) and datasets (medium-replay, medium) providing a comprehensive comparison of the algorithms' accuracy in offline RL.", "section": "5.2 Baseline Ensemble OPE Methods"}, {"figure_path": "T6LOGZBC2m/tables/tables_21_2.jpg", "caption": "Table 2: Root Mean-Squared Error (RMSE) of different OPE algorithms across D4RL tasks.", "description": "The table presents the Root Mean Squared Error (RMSE) of different Offline Policy Evaluation (OPE) algorithms across various D4RL (Datasets for Deep Data-driven Reinforcement Learning) tasks.  It compares OPERA's performance against several baseline methods (BestOPE, AvgOPE, BVFT, DR, Dual-DICE, MB) for different environments and datasets (medium-replay and medium).  Lower RMSE values indicate better performance.", "section": "5.2 Baseline Ensemble OPE Methods"}, {"figure_path": "T6LOGZBC2m/tables/tables_21_3.jpg", "caption": "Table 17: Undiscounted perf of different policies on HalfCheetah task.", "description": "This table presents the undiscounted performance results for different policies on the HalfCheetah task, categorized by the dataset used (medium-replay-v2 and medium-v2).  Each row represents a different policy (CQL 1, CQL 2, IQL 1, IQL 2, TD3 1, and TD3 2), and the columns show the undiscounted performance metrics for each policy on the specified datasets.", "section": "5.3 Results"}]