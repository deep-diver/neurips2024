[{"type": "text", "text": "Reinforcement Learning with Lookahead Information ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Nadav Merlis FairPlay Joint Team, CREST, ENSAE Paris nadav.merlis@ensae.fr ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We study reinforcement learning (RL) problems in which agents observe the reward or transition realizations at their current state before deciding which action to take. Such observations are available in many applications, including transactions, navigation and more. When the environment is known, previous work shows that this lookahead information can drastically increase the collected reward. However, outside of specific applications, existing approaches for interacting with unknown environments are not well-adapted to these observations. In this work, we close this gap and design provably-efficient learning algorithms able to incorporate lookahead information. To achieve this, we perform planning using the empirical distribution of the reward and transition observations, in contrast to vanilla approaches that only rely on estimated expectations. We prove that our algorithms achieve tight regret versus a baseline that also has access to lookahead information - linearly increasing the amount of collected reward compared to agents that cannot handle lookahead information. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In reinforcement learning (RL), agents sequentially interact with a changing environment, aiming to collect as much reward as possible. While performing actions that yield immediate rewards is enticing, agents must also bear in mind that actions influence the state of the environment, affecting the potential reward that could be collected in future steps. When the environment is unknown, agents also need to balance reward maximization based on previous data and exploration - gathering of data that might improve future reward collection. ", "page_idx": 0}, {"type": "text", "text": "In the standard interaction model, at each timestep, agents first choose an action and only then observe its outcome on the rewards and state dynamics. As such, agents can only maximize the expected rewards, collected through the expected dynamics. Yet, in many applications, some information on the immediate outcome of actions is known before actions are performed. For example, when agents interact through transactions, prices and traded goods are usually agreed upon before performing any exchange ('reward information'). Alternatively, in navigation problems, nearby traffic information is known to the agent before choosing which path to go through ('transition information'). ", "page_idx": 0}, {"type": "text", "text": "In a recent work, Merlis et al. [2024] shows that even for agents with full statistical knowledge of the environment, such \u201clookahead\u2019 information can drastically increase the reward collected by agents -- by a multiplicative factor of up to $A H$ when immediate rewards are revealed in advance and $\\bar{A}^{H/2}$ when observing the immediate future transitions. Intuitively, agents do not only gain from instantaneously using this information - they can also adapt their planning to account for lookahead information being revealed in subsequent states, significantly increasing their future values. However, the work of Merlis et al. [2024] only tackles planning settings in which the model is known and does not provide algorithms or guarantees when interacting with unknown environments. ", "page_idx": 0}, {"type": "text", "text": "In this work, we aim to design provably-efficient agents that learn how to interact when given immediate (^one-step lookahead') reward or transition information before choosing an action, under the episodic tabular Markov Decision Process model. While such information can always be embedded into the state of the environment, the state space becomes exponential at best, and continuous at worst, rendering most theoretically-guaranteed approaches both computationally and statistically intractable. To alleviate this, we start by deriving dynamic programming ('Bellman\u2019) equations in the original state space that characterize the optimal lookahead policies. Inspired by these update rules, we present two variants to the MVP algorithm [Zhang et al., 2021b] that allow incorporating either reward or transition lookahead. In particular, we suggest a planning procedure that uses the empirical distribution of the reward/transition observations (instead of the estimated expectations), which might also be applied to other complex settings. We prove that these algorithms achieve tight regret bounds of $\\tilde{\\mathcal{O}}\\big(\\sqrt{H^{3}S A K}\\big)$ and $\\tilde{\\mathcal{O}}\\Big(\\sqrt{H^{2}S K}(\\sqrt{H}+\\sqrt{A})\\Big)$ after $K$ episodes (for reward and transition lookahead, respectively), compared to a stronger baseline that also has access to lookahead information. As such, they can collect significantly more rewards than vanilla RL algorithms. ", "page_idx": 1}, {"type": "text", "text": "Outline. We formally define RL problems with reward/transition lookahead in Section 2 and further discuss the differences between our setting and standard RL problems in Section 3. Then, we present our results in two complementary sections: Section 4 analyzes reward lookahead while Section 5 analyzes transition lookahead. We end with conclusions and future directions in Section 6. ", "page_idx": 1}, {"type": "text", "text": "Related Work. Problems with varying lookahead information have been extensively studied in control, with model predictive control [MPC, Camacho et al., 2007] as the most notable example. Conceptually, when interacting with an environment that might be too complex or hard to model, it is oftentimes convenient to use a simpler model that allows accurately predicting its behavior just in the near future. MPC uses such models to repeatedly update its policy using short-term planning. In some cases, the utilized future predictions consist of additive perturbations to the dynamics [Yu et al., 2020], while other cases involve more general future predictions on the model behavior [Li et al., 2019, Zhang et al., 2021a, Lin et al., 2021, 2022]. To the best of our knowledge, these studies focus on comparing the performance of the controller to one with full future information (and thus, linear regret is inevitable), sometimes also considering prediction errors. They do not, however, attempt to learn the predictions. In contrast, we estimate the reward/transition distributions and leverage them to better plan, thus increasing the value gained by the agent. In addition, these works focus on continuous (mostly linear) control problems, whereas we study tabular settings; results from any one of these settings cannot be directly applied to the other. ", "page_idx": 1}, {"type": "text", "text": "In RL, lookahead is mostly used as a planning tool; namely, agents test the possible outcomes after performing multiple steps to decide which actions to take or to better estimate the value [Tamar et al. 2017, Efroni et al., 2019a, 2020, Moerland et al., 2020, Rosenberg et al., 2023, E1 Shar and Jiang, 2020, Biedenkapp et al., 2021, Huang et al., 2019]. Specifically, the future value at the end of the lookahead is often estimated using rollouts, and a longer lookahead is more robust to suboptimality of the rollout policy [Bertsekas, 2023]. However, when agents actually interact with the environment, no additional lookahead information is observed. One notable exception is [Merlis et al., 2024], which analyzes the potential value increase due to multi-step reward lookahead information (and briefly mentions transition lookahead). However, they only tackle planning settings, where the model is known, and do not study learning. In this work, we continue a long line of literature on regret analysis for tabular RL [Jaksch et al., 2010, Jin et al., 2018, Dann et al., 2019, Zanette and Brunskill, 2019, Efroni et al., 2019b, 2021, Simchowitz and Jamieson, 2019, Zhang et al., 2021b, 2023]. Yet, we are not aware of any existing results on regret minimization with reward or transition lookahead information. ", "page_idx": 1}, {"type": "text", "text": "Finally, various applications that involve one-step lookahead information have been previously studied. The most notable ones are prophet problems [Correa et al., 2019], where one-step reward lookahead is obtained, and the Canadian traveler problem with resampling [Nikolova and Karger, 2008], which can be formulated through one-step transition lookahead. We discuss the relation to these problems and the relevant existing results when analyzing each type of feedback, and also discuss the relation between transition lookahead and stochastic action sets [Boutilier et al., 2018]. ", "page_idx": 1}, {"type": "text", "text": "2  Setting and Notations ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We study episodic tabular Markov Decision Processes (MDPs), defined by the tuple $\\mathcal{M}\\;=\\;$ $(S,{\\mathcal{A}},{\\bar{H_{,}}}\\,P,{\\bar{\\mathcal{R}}})$ , where $\\boldsymbol{S}$ is the state space (of size $S$ $\\boldsymbol{\\mathcal{A}}$ is the action space (of size $A$ ) and $H$ is the interaction horizon. At each timestep $h\\in\\{1,\\ldots,H\\}\\overset{\\triangle}{=}[H]$ of an episode $k\\in[K]$ , an agent, located in state $s_{h}^{k}\\in\\mathcal{S}$ chooses an action $a_{h}^{k}\\in\\mathcal{A}$ and obtainsareward $R_{h}^{k}=R_{h}(s_{h}^{\\bar{k}},\\bar{a}_{h}^{k})\\sim{\\mathcal R}_{h}(s_{h}^{k},a_{h}^{k})$ We assume that the rewards are supported by $[0,1]$ and of expectations $r_{h}(s,a)$ Afterward, the environment transitions toastate $s_{h+1}^{k}\\sim P_{h}(\\cdot|\\bar{s_{h}^{k}},a_{h}^{\\bar{k}})$ and the interaction continues until the end of the episode. We use the notation $R\\sim\\mathcal{R}_{h}(s)$ (or $s^{\\prime}\\sim P_{h}(s))$ to denote reward (next-state) samples for all actions simultaneously at step $h$ and state $s$ and assume independence between different timesteps.2 On the other hand, samples from different actions at a specific state/timestep are not necessarily independent. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Reward Lookahead.  With one-step reward lookahead at timestep $h$ and state $s$ , agents first observe the rewards for all actions $R_{h}(s)\\,\\triangleq\\,\\{R_{h}(s,a)\\}_{a\\in\\mathcal{A}}$ and only then choose an action to perform. Formally, we define the set of reward lookahead policies as $\\Pi^{R}=\\left\\{\\pi:[H]\\times\\mathcal{S}\\times[0,1]^{A}\\mapsto\\Delta_{A}\\right\\}$ where $\\Delta_{\\mathcal{A}}$ is the probability simplex, and denote $a_{h}=\\pi_{h}(s_{h},R_{h})$ . The value of a reward lookahead agent is the cumulative rewards gathered by it starting at timestep $h$ and state $s$ , denoted by ", "page_idx": 2}, {"type": "equation", "text": "$$\nV_{h}^{R,\\pi}(s)=\\mathbb{E}\\left[\\sum_{t=h}^{H}R_{t}(s_{t},\\pi_{t}(s_{t},\\pmb{R}_{t}(s_{t}))|s_{h}=s\\right].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "We also defne theotimal reward lokahead valueto be $V_{h}^{R,^{*}}(s)=\\operatorname*{max}_{\\pi\\in\\Pi^{R}}V_{h}^{R,\\pi}(s)$ When interacting with an unknown environment for episodes, agents sequentially choose reward lookahead policies $\\pi^{k}\\in\\Pi^{R}$ based on all historical information and are measured by their regret, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{Reg}^{R}(K)=\\sum_{k=1}^{K}\\Bigl(V_{1}^{R,*}(s_{1}^{k})-V_{1}^{R,\\pi^{k}}(s_{1}^{k})\\Bigr).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "We allow the initial state of each episode $s_{1}^{k}$ to be arbitrarily chosen. ", "page_idx": 2}, {"type": "text", "text": "Transition Lookahead. Denoting $s_{h+1}^{\\prime}(s,a)$ , the future tate when playing action $a$ at step $h$ and state $s$ $s_{h+1}^{\\prime}(s)\\,\\triangleq\\,\\{s_{h+1}^{\\prime}(s,a)\\}_{a\\in\\mathcal{A}}$ before acting. The set of transition lookahead agents is denoted by $\\Pi^{T}=\\left\\{\\pi:[H]\\times\\mathcal{S}\\times\\mathcal{S}^{A}\\mapsto\\dot{\\Delta}_{A}\\right\\}$ with values ", "page_idx": 2}, {"type": "equation", "text": "$$\nV_{h}^{T,\\pi}(s)=\\mathbb{E}\\left[\\sum_{t=h}^{H}R_{t}(s_{t},\\pi_{t}(s_{t},s_{t+1}^{\\prime}(s_{t})))|s_{h}=s\\right].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The optimal value is $V_{h}^{T,^{*}}(s)=\\operatorname*{max}_{\\pi\\in\\Pi^{T}}V_{h}^{T,\\pi}(s)$ and we similarly define the regret versus optimal transition lookahead agents as $\\begin{array}{r}{\\mathrm{Reg}^{T}(K)=\\sum_{k=1}^{K}\\Bigl(V_{1}^{T,*}(s_{1}^{k})-V_{1}^{T,\\pi^{k}}(s_{1}^{k})\\Bigr).}\\end{array}$ ", "page_idx": 2}, {"type": "text", "text": "When the type of lookahead is clear from the context, we sometimes denote values by $V_{h}^{\\pi}$ and $V_{h}^{*}$ Other Notations. For any $p\\in\\Delta_{n}$ and $V\\in\\mathbb{R}^{n}$ , we define $\\begin{array}{r}{\\operatorname{Var}_{p}(V)=\\sum_{i=1}^{n}p_{i}V_{i}^{2}\\!-\\!\\left(\\sum_{i=1}^{n}p_{i}V_{i}\\right)^{2}}\\end{array}$ Also, given a transition kernel $P$ and a vector $V\\in\\mathbb{R}^{S}$ , we let $\\begin{array}{r}{P V(s,a)=\\sum_{s^{\\prime}\\in\\mathcal{S}}P(s^{\\prime}|s,a)V(s^{\\prime})}\\end{array}$ and similarly define it for value or transition kernel differences. We denote by $n_{h}^{k}(s,a)$ , the number of times the pair $(s,a)$ was visited at timestep $h$ up to episode $k$ (inclusive) and similarly denote $\\begin{array}{r}{n_{h}^{k}(s)\\;=\\;\\sum_{a\\in\\mathcal{A}}n_{h}^{k}(s,a)}\\end{array}$ .We also let $\\begin{array}{r}{\\hat{r}_{h}^{k}(s,a)\\ =\\ \\frac{1}{n_{h}^{k}(s,a)}\\sum_{k^{\\prime}=1}^{k}1\\Big\\{s_{h}^{k^{\\prime}}=s,a_{h}^{k^{\\prime}}=a\\Big\\}R_{h}^{k^{\\prime}}}\\end{array}$ $\\begin{array}{r}{\\hat{P}_{h}(s^{\\prime}|s,a)\\,=\\,\\frac{1}{n_{h}^{k}(s,a)}\\sum_{k^{\\prime}=1}^{k}\\mathbb{1}\\Big\\{s_{h}^{k^{\\prime}}=s,a_{h}^{k^{\\prime}}=a,s_{h+1}^{k^{\\prime}}=s^{\\prime}\\Big\\}}\\end{array}$ be the empirical expected rewards and transition kernel at $(s_{h},a_{h})=(s,a)$ using data up to episode $k$ and assume they are initialized to be zero. Finally, we denote by $\\hat{\\mathcal{R}}_{h}^{k}(s)$ , the empirical reward distribution across all actions, and use $\\hat{P}_{h}^{k}(s)$ to denote the empirical joint next-state distribution for all actions. In particular, if $k_{i}$ is the $i^{t h}$ episode where $s$ was visited at step $h$ , to sample $R\\sim\\hat{\\mathcal{R}}_{h}^{k}(s)$ , we uniformly sample $i\\sim U\\big(\\big[n_{h}^{k}(s)\\big]\\big)$ and return $R=\\left\\{R_{h}^{k_{i}}(s,a)\\right\\}_{a\\in\\mathcal{A}}$ A sampe $\\pmb{s}^{\\prime}\\sim\\hat{P}_{h}^{k}(s)$ similaly eturns $\\pmb{s}^{\\prime}=\\bigg\\{s_{h+1}^{\\prime k_{i}}(s,a)\\bigg\\}_{a\\in\\mathcal{A}}.$ ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "When we want to indicate the distribution used to calculate an expectation, we sometimes state it in a subscript, e.g., write $E_{\\mathcal{R}_{h}(s)}[R(a)]$ to indicate that $R(a)\\sim\\mathcal{R}_{h}^{-}(s,a)$ oruse $\\mathbb{E}_{\\mathcal{M}}$ to emphasize that all distributions are according to an environment $\\mathcal{M}$ . In this paper, $\\scriptscriptstyle\\mathcal{O}$ -notation only hides absolute constants while $\\tilde{\\mathcal{O}}$ hides factors of polylog $(S,A,H,K,\\delta)$ . We also use the notation $a\\vee b=$ $\\operatorname*{max}\\{a,b\\}$ ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "3  Comparing the Values of Lookahead Agents and Vanilla RL agents ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In the classic RL formulation [e.g., Azar et al., 2017], agents only observe the reward and transition after performing an action and aim to maximize the 'no-lookahead\u2019 value, defined by ", "page_idx": 3}, {"type": "equation", "text": "$$\nV_{h}^{\\pi}(s)=\\mathbb{E}\\left[\\sum_{t=h}^{H}r_{t}(s_{t},\\pi_{t}(s_{t})|s_{h}=s\\right],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\pi\\in\\Pi^{\\mathcal{M}}=\\{\\pi:[H]\\times\\mathcal{S}\\mapsto\\Delta_{A}\\}$ is a Markovian policy. The optimal value is $V_{h}^{n o}(s)=$ $\\operatorname*{max}_{\\pi\\in\\Pi^{\\mathcal{M}}}V_{h}^{\\pi}(s)$ and the regret is classically defined as $\\begin{array}{r}{\\mathrm{Reg}(K)=\\sum_{k=1}^{K}\\Bigl(V_{1}^{n o}(s_{1}^{k})-V_{1}^{\\pi^{k}}(s_{1}^{k})\\Bigr)}\\end{array}$ ", "page_idx": 3}, {"type": "text", "text": "By definition, the set of lookahead policies also includes all Markovian policies (since agents are not obliged to use reward/transition information), so the optimal lookahead values are always larger than their no-lookahead counterpart. In other words, denoting the value gain due to lookahead information $G^{R}(s)=V_{1}^{R,*}(s)-\\bar{V_{1}^{n o}}(s)$ $G^{T}(s)=V_{1}^{T,*}(s)-V_{1}^{n o}(s)$ itholdsthat $G^{R}(s),G^{T}(s)\\geq0$ In terms of regret, for any fixed algorithm, we can also write ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{Reg}(K)=\\mathrm{Reg}^{R}(K)-\\sum_{k=1}^{K}G^{R}(s_{1}^{k})=\\mathrm{Reg}^{T}(K)-\\sum_{k=1}^{K}G^{T}(s_{1}^{k}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "As the value gains are non-negative, it directly implies that any regret bound w.r.t. the lookahead value also leads to the same bound for the standard regret. Even more so, in most cases, lookahead information leads to a strict improvement in the value, that is, $G^{R}(s),G^{T}(s)\\geq G_{0}>0$ Whenthis happens, any algorithm with sub-linear lookahead regret enjoys a negative linear standard regret: ", "page_idx": 3}, {"type": "equation", "text": "$$\nI f\\operatorname{Reg}^{R}(K)=o(K)\\;a n d\\;G^{R}(s_{1}^{k})\\geq G_{0}\\;f o r\\,a l l\\;k\\in[K],\\,t h e n\\;\\operatorname{Reg}(K)\\leq-G_{0}K+o(K).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The same also holds for transition lookahead. Conversely, any agent that suffers positive standard regret will suffer linear regret compared to the best lookahead agent, i.e., ", "page_idx": 3}, {"type": "equation", "text": "$$\nI f\\operatorname{Reg}(K)\\geq0\\;a n d\\;G^{R}(s_{1}^{k})\\geq G_{0}\\,f o r\\,a l l\\;k\\in[K],\\,t h e n\\;\\mathrm{Reg}^{R}(K)\\geq G_{0}K.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Notably, any agent that does not use lookahead information will suffer linear lookahead regret in any such environment. We now present two illustrative examples for environments where the lookahead value gain is significant, one for reward lookahead and another for transition lookahead. ", "page_idx": 3}, {"type": "image", "img_path": "wlqfOvlTQz/tmp/5f789520713f4d5c8b3522b73d88352cf1c1249317294746caebdc254792276e.jpg", "img_caption": ["Figure 1: Two-state prophet-like problem "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Reward lookahead. Consider a simple 2-state environment, depicted in Figure 1. Starting at $s_{i}$ , agents can either stay there by playing $a_{1}$ \uff0c earning no reward, or play any other action and move to the absorbing $s_{f}$ \uff0c obtaining a Bernoulli reward $B e r\\bigl({}^{1}\\!/(A\\!-\\!1)H\\bigr)$ .Actions in the terminal state $s_{f}$ yield no reward. Without observing the rewards, agents will arbitrarily move from $s_{i}$ to $s_{f}$ , obtaining a reward $V^{n o}={1}/{\\bar{(}A{-}1)H}$ in expectation. On the other hand, when agents observe the rewards before acting, they should move from $s_{i}$ to $s_{f}$ only if a reward was realized for some action (and otherwise, stay in $s_{i}$ by playing $a_{1}$ ). Such agents will have $(A\\!-\\!1)H$ opportunities to observe a unit reward across all timesteps and actions, collecting in expectation $V^{R,*}=\\left(1-1/(A{-}1)H\\right)^{(A{-}1)H}\\geq$ $1-{^1}/{e}$ . In other words, just by observing the rewards before acting, the agent's value multiplicatively increases by almost $V^{R,*}/V^{n o}\\approx{\\cal A}{\\cal H}$ ", "page_idx": 3}, {"type": "text", "text": "Moreover, the additive value gain is $G^{R}\\approx1-{}^{1/e}$ , so sub-linear lookahead regret with reward information results with a negatively-linear standard regret of $\\mathrm{Reg}(K)\\lesssim-(1-1\\bar{/}e)K$ ", "page_idx": 3}, {"type": "text", "text": "Transition lookahead. Consider a chain of $H/2$ states (also described in further detail at Appendix C.9 and depicted at Figure 2). In each state, one action deterministically keeps the agent in its current state, while all other actions move the agent one state forward w.p. $1/A$ , but lead to a terminal non-rewarding state otherwise. If the reward is located at the end of the chain, any standard RL agent can collect it only at an exponentially low probability. On the other hand, transition lookahead agents would move forward only if there is an action that allows it while staying at their current state otherwise; such agents will collect the rewards at the end of the chain with constant probability. More specifically, any no-lookahead agent can collect at most $V^{n o}={\\cal O}(H A^{-H/2})$ rewards, while transition lookahead agents can collect $\\bar{V}^{T,*}=\\Omega(H)$ ; as such, lookahead agents achieve exponential increase in value, and sublinear regret versus the best lookahead agent will yield a standard regret of $\\mathrm{Reg}(K)\\lesssim-H K$ ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "In the following sections, we will present agents that are guaranteed to always achieve sublinear regret compared to the best lookahead agent. ", "page_idx": 4}, {"type": "text", "text": "4 Planning and Learning with One-Step Reward Lookahead ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we analyze RL settings with one-step reward lookahead, in which immediate rewards are observed before choosing an action. One well-known example of this situation is the prophet problem [Correa et al., 2019], where an agent sequentially observes values from known distributions. Upon observing a value, the agent decides whether to take it as a reward and stop the interaction, or discard it and continue to observe more values. This problem has numerous applications and extensions concerning auctions and posted-price mechanisms [Correa et al., 2017]. As shown in [Merlis et al., 2024], it is critical to observe the distribution values before taking a decision; otherwise, the agent's revenue can decrease by a factor of $H$ . Notably, the example presented in Figure 1 is a small variant of the prophet problem, where the agent can either take one of $A-1$ values and finish the interaction or discard them and continue playing by staying at $s_{i}$ ; we showed that for this example, the lookahead information increases the value by a factor of $V^{R,*}/V^{n o}\\approx A H.$ ", "page_idx": 4}, {"type": "text", "text": "The most natural way to tackle this setting is to extend (augment) the state space to contain the observed rewards; this way, we transition from a state and reward observations to a new state with new reward observations and return to the vanilla MDP formulation. However, this comes at a great cost. Even for Bernoulli rewards, there are $2^{A}$ possible reward combinations at any given state, and the augmentation increases the state space by this factor - leading to an exponentially-large state space. Even worse, for continuous rewards, the augmented state space becomes continuous, and any performance guarantees that depend on the size of the state space immediately become vacuous. Hence, algorithms that naively use this reduction are expected to be both computationally and statistically intractable. We refer to Appendix B.2 for further details on one such augmentation. ", "page_idx": 4}, {"type": "text", "text": "We take a different approach and derive Bellman equations for this setting in the original state space. Proposition 1. The optimal value of one-step reward lookahead agents satisfies ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{V_{H+1}^{R,*}(s)=0,}&{\\forall s\\in S,}\\\\ &{V_{h}^{R,*}(s)={\\mathbb E}_{R\\sim\\mathcal{R}_{h}(s)}\\left[\\displaystyle\\operatorname*{max}_{a\\in\\mathcal{A}}\\left\\lbrace R_{h}(s,a)+\\displaystyle\\sum_{s^{\\prime}\\in\\mathcal{S}}P_{h}(s^{\\prime}|s,a)V_{h+1}^{R,*}(s^{\\prime})\\right\\rbrace\\right],}&{\\forall s\\in S,h\\in[H].}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Also, given reward observations $\\pmb{R}=\\{R(a)\\}_{a\\in\\mathcal{A}}$ at state s and step $h$ the optimal policy is ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\pi_{h}^{*}(s,R)\\in\\underset{a\\in\\mathcal{A}}{\\arg\\operatorname*{max}}\\Biggl\\{R(a)+\\sum_{s^{\\prime}\\in\\mathcal{S}}P_{h}(s^{\\prime}|s,a)V_{h+1}^{R,*}(s^{\\prime})\\Biggr\\}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We prove Proposition 1 in Appendix B.2, where we present an equivalent environment with extended state space in which one could apply the standard Bellman equations [Puterman, 2014] to calculate the value with reward lookahead. In contrast to the previously discussed augmentation approach, we find it more convenient to divide the augmentation into two steps - at odd steps $2h-1$ ,the augmented environment would be in a state $s_{h}\\times\\mathbf{0}$ ,while at evensteps $2h$ ,thestateis $s_{h}\\times R_{h}$ Doing so creates an overlap between the values of the original and augmented environments at odd steps, simplifying the proofs. We also use this augmentation to prove a variant of the law of total variance [LTV, e.g. Azar et al., 2017] and a value-difference lemma [e.g. Efroni et al., 2019b]. ", "page_idx": 4}, {"type": "text", "text": "We remark that calculating the exact value is not always tractable - even for $S=H=1$ (bandit problems) and Gaussian rewards, Proposition 1 requires calculating the expectation of the maximum ", "page_idx": 4}, {"type": "text", "text": "1: Require: $\\delta\\in(0,1)$ , bonuses $b_{k,h}^{r}(s),b_{k,h}^{p}(s,a)$   \n2: for $k=1,2,\\dots$ do   \n3: Initialize $\\bar{V}_{H+1}^{k}(s)=0$   \n4: for $h=H,H-1,..,1$ do   \n5: Calculate the truncated values for all $s\\in S$   \n$\\bar{V}_{h}^{k}(s)=\\operatorname*{min}\\biggl\\{\\mathbb{E}_{R\\sim\\hat{\\mathcal{R}}_{h}^{k-1}(s)}\\biggl[\\operatorname*{max}_{a\\in\\mathcal{A}}\\Bigl\\{R(a)+b_{k,h}^{p}(s,a)+\\hat{P}_{h}^{k-1}\\bar{V}_{h+1}^{k}(s,a)\\Bigr\\}\\biggr]+b_{k,h}^{r}(s),H\\biggr\\}$   \n6: end for   \n7: for $h=1,2,\\dots H$ do   \n8: Observe $s_{h}^{k}$ and $R_{h}^{k}(s_{h}^{k},a)$ for all $a\\in A$   \n9: Play an action $\\begin{array}{r}{a_{h}^{k}\\in\\arg\\operatorname*{max}_{a\\in A}\\Bigl\\{R_{h}^{k}(s_{h}^{k},a)+b_{k,h}^{p}(s_{h}^{k},a)+\\hat{P}_{h}^{k-1}\\bar{V}_{h+1}^{k}(s_{h}^{k},a)\\Bigr\\}_{*=\\!*}}\\end{array}$   \n10: Collect the reward $R_{h}^{k}(s_{h}^{k},a_{h}^{k})$ and transition to the next state $s_{h+1}^{k}\\sim P_{h}(\\cdot|s_{h}^{k},a_{h}^{k})$   \n11: end for   \n12: end for ", "page_idx": 5}, {"type": "text", "text": "of Gaussian random variables, which does not admit any simple closed-form solution. On the other hand, these equations allow approximating the value by using reward samples - in the following, we show that it can be used to achieve tight regret bounds when the environment is unknown. ", "page_idx": 5}, {"type": "text", "text": "4.1  Regret-Minimization with Reward Lookahead ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We now present a tractable algorithm that achieves tight regret bounds with one-step reward lookahead. Specifically, we modify the Monotonic Value Propagation (MVP) algorithm [Zhang et al., 2021b] to perform planning using the empirical reward distributions - instead of using the empirical reward expectations. To compensate for transition uncertainty, we add a transition bonus that uses the variance of the optimistic next-state values (w.r.t. the empirical transition kernel), designed to be monotone in the future value. Such construction permits using the variance of optimistic values for the bonus calculation while being able to later replace it with the variance of the optimal value (see discussion in Zhang et al. 2021b). A reward bonus is used for the value calculation, but does not affect the action choice in the current state. Intuitively, this is because we get the same amount of information for all the actions of a state, so they have the same level of uncertainty - there is no need for bonuses to encourage reward exploration at the action level. ", "page_idx": 5}, {"type": "text", "text": "A high-level description of the algorithm is presented in Algorithm 1, while the full algorithm and its bonuses are stated in Appendix B.3. Notice that the planning requires calculating the expected maximum using the empirical distribution, whose support always contains at most $K$ elements,so both the memory and computations are polynomial. The algorithm ensures the following guarantees: ", "page_idx": 5}, {"type": "text", "text": "Theorem 1. When running MVP-RL, with probability at least $1-\\delta$ uniformlyforall $K\\geq1,$ itholds that $\\begin{array}{r}{\\mathrm{Reg}^{R}(K)\\leq\\mathcal{O}\\Big(\\sqrt{H^{3}S A K}\\ln\\frac{S A H K}{\\delta}+H^{3}S^{2}A\\big(\\ln\\frac{S A H K}{\\delta}\\big)^{2}\\Big).}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "See proof in Appendix B.7. Remarkably, our upper bound matches the standard lower bound for episodicRL of $\\Omega\\mathopen{}\\mathclose\\bgroup\\left(\\sqrt{H^{3}S A K}\\aftergroup\\egroup\\right)$ [Domingues et al., 2021] up to log-factors; this lower bound is proved for known deterministic rewards, so in particular, it also holds for problems with reward lookahead. ", "page_idx": 5}, {"type": "text", "text": "To our knowledge, the only comparable bounds in settings with reward lookahead were proven to prophet problems; as agents observe (up to) $n$ distributions at a fixed order, it can be formulated as a deterministic chain-like MDP, with $H=n$ $S=n+1$ and $A=2$ . Agents start at the head of the chain and can either advance without collecting a reward or collect the observed reward and move to a terminal non-rewarding state (for more details, see Merlis et al. 2024). For this problem, [Gatmiry et al., 2024] proved a regret bound of $\\tilde{\\mathcal{O}}(n^{3}\\sqrt{K})$ (albeit requiring a weaker form of feedback), and [Agarwal et al., 2023] proved a bound of $\\tilde{\\mathcal{O}}(n\\sqrt{T})$ - slightly better than ours, but heavily relies on the ability to control which distributions to observe, which is a specific instance of deterministic transitions. We are unaware of any previous results that cover general Markovian dynamics. ", "page_idx": 5}, {"type": "text", "text": "4.2 Proof Concepts ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "When analyzing the regret of RL algorithms, a key step usually involves bounding the difference between the value of a policy in two different environments ('value-difference lemma'). In particular, for a given policy $\\pi^{k}$ , many algorithms maintain a confidence interval on the value $V_{h}^{\\pi^{k}}(s)\\in$ $\\left[\\underline{{V}}_{h}^{k}(s),\\bar{V}_{h}^{k}(s)\\right]$ , calculated based on optimistic and pessimistic MDPs that use the empirical model with bonuses/penalties [Dann et al., 2019, Zanette and Brunskill, 2019, Efroni et al., 2021]. Then, the instantaneous regret (without lookahead) is bounded using the optimistic values by ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{V}_{h}^{k}(s_{h})-V_{h}^{\\pi^{k}}(s_{h})=\\big(\\hat{r}_{h}^{k-1}(s_{h},a_{h})-r_{h}(s_{h},a_{h})\\big)+\\Big(\\hat{P}_{h}^{k-1}-P_{h}\\Big)\\bar{V}_{h}^{k}(s_{h},a_{h})}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\,P_{h}\\Big(\\bar{V}_{h+1}^{k}-V_{h+1}^{\\pi^{k}}\\Big)(s_{h},a_{h})+\\mathrm{bonuses},}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "while the pessimistic values are used either as part of the bonuses or while bounding them. However, when trying to perform a similar decomposition with reward lookahead, we do not have the difference of expected rewards, but rather terms of the form ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{E}_{R\\sim\\hat{\\mathcal{R}}_{h}^{k-1}(s_{h})}\\left[R(\\pi_{h}^{k}(s_{h},\\pmb{R}))\\right]-\\mathbb{E}_{\\pmb{R}\\sim\\mathcal{R}_{h}(s_{h})}\\left[R(\\pi_{h}^{k}(s_{h},\\pmb{R}))\\right]\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "(see, e.g., the last term of Lemma 4 in the appendix). As the action can be an arbitrary function of the reward realization, this term is extremely challenging to bound. For example, one could couple both distributions while trying to relate this error term to a Wasserstein distance between the empirical and real reward distribution; however, such distances exhibit much slower error rates than standard mean estimation [Fournier and Guillin, 2015]. Instead, we follow a different approach and show that uniformly for all possible expected next-state values $\\hat{P V}\\in[0,H]^{A}$ (as a function of the action at a given state), it holds w.h.p. that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\mathbb{E}_{R\\sim\\hat{R}_{h}^{k-1}(s)}\\!\\left[\\underset{a}{\\operatorname*{max}}\\!\\left\\{R(a)+\\hat{P}V(s,a)\\right\\}\\right]-\\mathbb{E}_{R\\sim\\mathcal{R}_{h}(s)}\\!\\left[\\underset{a}{\\operatorname*{max}}\\!\\left\\{R(a)+\\hat{P}V(s,a)\\right\\}\\right]\\right|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\lesssim\\sqrt{\\frac{A\\ln\\frac{1}{\\delta}}{n_{h}^{k-1}(s)\\,\\vee\\,1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Throughout the proof, whenever we face an expectation w.r.t. the empirical rewards, we reformulate the expression to fit the form of Equation (1) and use it as a ^change of measure\u2019 tool. We remark that while this confidence interval admits an extra $A$ -factor compared to standard bounds, the counts only depend on the visits to the state (and not to the state-action), which compensates for this factor. ", "page_idx": 6}, {"type": "text", "text": "The choice of MVP for the bonus is similarly motivated - unlike some other bonuses (e.g., Zanette and Brunskill 2019), MVP does not require pessimistic values - either in the bonus itself or in its analysis. In contrast to the optimistic ones, the pessimistic values are not calculated via value iteration, but rather by following the policy $\\pi^{k}$ in the pessimistic environment. As such, they cannot be easily manipulated to fit the form in Equation (1). ", "page_idx": 6}, {"type": "text", "text": "The analysis of the transitions adapts the techniques in [Efroni et al., 2021], while requiring extra care in handling the dependence of actions in the rewards. ", "page_idx": 6}, {"type": "text", "text": "5  Reinforcement Learning with One-Step Transition Lookahead ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We now move to analyzing problems with one-step transition lookahead, where the resulting next state due to playing any of the actions is revealed before deciding which action to play. For example, consider the stochastic Canadian traveler problem with resampling [Nikolova and Karger, 2008, Boutilier et al., 2018]. In this problem, an agent wants to navigate on a graph as fast as possible from a source to a target, but observes which edges at a node are available only upon reaching this node. When edge availability is stochastic and resampled every time a node is visited, this is a clear case of one-step transition lookahead, as the information on the availability of edges is given before trying to traverse them. The example in Section 3 and Appendix C.9 is one possible formulation of this problem on a chain - agents are awarded for arriving at the end of the chain as fast as possible, but trying to use a non-existing edge results with termination. We showed that in this particular instance, the lookahead value is exponentially larger than the standard value, and any lookahead agent with low regret would greatly surpass no-lookahead agents. ", "page_idx": 6}, {"type": "text", "text": "As with reward lookahead, the future states for all actions can be embedded into the state, but doing so increases the size of the state space by a factor of $S^{A}$ , again making this approach intractable (see Appendix C.2 for an example for such an extension). We once more show that this is not necessary; the transition-lookahead optimal values can be calculated using the following Bellman equations: ", "page_idx": 7}, {"type": "text", "text": "Proposition 2. The optimal value of one-step transition lookahead agents satisfies ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{V_{H+1}^{T,*}(s)=0,}&{\\forall s\\in S,}\\\\ {V_{h}^{T,*}(s)={\\mathbb E}_{s^{\\prime}\\sim P_{h}(s)}\\bigg[\\underset{a\\in\\mathcal{A}}{\\operatorname*{max}}\\Big\\lbrace r_{h}(s,a)+V_{h+1}^{T,*}(s^{\\prime}(s,a))\\Big\\rbrace\\bigg],}&{\\quad}&{\\forall s\\in S,h\\in[H].}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Also, given next-state observations $\\pmb{s}^{\\prime}=\\{s^{\\prime}(a)\\}_{a\\in\\mathcal{A}}$ at state s and step $h_{i}$ the optimal policy is ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\pi_{h}^{*}(s,s^{\\prime})\\in\\underset{a\\in\\cal{A}}{\\arg\\operatorname*{max}}\\Bigl\\{r_{h}(s,a)+V_{h+1}^{T,*}(s^{\\prime}(a))\\Bigr\\}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The proof can be found at Appendix C.2 and again relies on augmenting the state space to incorporate the transitions; this time, we divide the episode into odd steps whose extended state is $s_{h}\\times s_{0}^{\\prime}$ (for an arbitrary fixed $\\pmb{s}_{0}^{\\prime}\\in\\mathcal{S}^{A}$ and even steps with thestate $s_{h}\\times s_{h+1}^{\\prime}$ Beyond planning, this again allows proving a variant of the LTV and of a value-difference lemma. ", "page_idx": 7}, {"type": "text", "text": "One important insight is that the policy $\\pi_{h}^{*}(s,s^{\\prime})$ admits the form of a list. Namely, consider the values $V_{h}^{*}(s,s^{\\prime},a)\\,=\\,r_{h}(s,a)\\,+\\,V_{h+1}^{T,*}(s^{\\prime})$ and assume some ordering of next-state-action pairs $\\{(s_{i}^{\\prime},a_{i})\\}_{i=1}^{S A}$ such that $V_{h}^{*}(s,s_{1}^{\\prime},a_{1})\\geq\\cdots\\geq V_{h}^{*}(s,s_{S A}^{\\prime},a_{S A})$ Then, an optimal poliey wouldlok at all realized pairs $(s^{\\prime}(a),a)$ and play the action with the highest location in this list. We refer the readers to Appendix C.4 for an additional discussion on list representations in transition lookahead. ", "page_idx": 7}, {"type": "text", "text": "Similar results could be achieved through a reduction to RL problems with stochastic action sets [Boutilier et al., 2018]. There, at every round, a subset of base actions is sampled, and only these actions are available to the agent. In particular, one could sample $A$ actions of the form $(s^{\\prime},a)\\in S\\!\\times\\!A$ and impose a deterministic transition to $s^{\\prime}$ given this extended action. However, since every original action must be sampled exactly once, this sampling procedure creates a dependence between pairs even when next-states at different actions are independent, adding unnecessary complications. We show that when transitions are independent between states, the expectation in Proposition 2 can be efficiently calculated (see Appendix C.4.1 for details), and otherwise, it can be approximated through sampling, as we do in learning settings. ", "page_idx": 7}, {"type": "text", "text": "5.1Regret-Minimization with Transition Lookahead ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Relying on similar principals as with reward lookahead, we now present MVP-TL, an adaptation of MVP to settings with one-step transition lookahead (summarized in Algorithm 2; the full details can be found at Appendix C.3). This time, we estimate the empirical expected reward and add a standard Hoeffding-like reward bonus, while performing planning using samples from the empirical joint distribution of the next-state for all the actions simultaneously. A variance-based transition bonus is added to the values; though this time, the variance also incorporates the rewards, namely ", "page_idx": 7}, {"type": "equation", "text": "$$\nb_{k,h}^{p}(s)\\approx\\sqrt{\\frac{\\operatorname{Var}_{s^{\\prime}\\sim\\hat{P}_{h}^{k-1}(s)}(\\bar{V}_{h}^{k}(s,s^{\\prime}))}{n_{h}^{k-1}(s)\\vee1}},\\quad\\bar{V}_{h}^{k}(s,s^{\\prime})=\\operatorname*{max}_{a\\in A}\\Bigl\\{\\hat{r}_{h}^{k-1}(s,a)+b_{k,h}^{r}(s,a)+\\bar{V}_{h+1}^{k}(s^{\\prime}(a)\\Bigr\\}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The motivation for this modification is the technical challenges described in Section 4.2, in the context of reward lookahead. For reward lookahead, we analyzed a value term that included both the rewards and next-state values, and used concentration arguments to move from the empirical reward distribution to the real one. For transition lookahead, similar values are analyzed, but we require variance-based concentration to obtain tighter regret bounds [Azar et al., 2017], so this variance naturally arises. The bonus is again designed to be monotone, as in the original MVP algorithm, and does not affect the immediate action choice - only the optimistic lookahead value. As before, the planning relies on sampling the next-state observations at previous episodes, and so it is polynomial, even if the precise joint distribution is complex. The algorithm enjoys the following regret bounds: ", "page_idx": 7}, {"type": "text", "text": "Theorem 2. When running MVP-TL, with probability at least $1-\\delta$ uniformlyfor all $K\\geq1$ itholds that $\\begin{array}{r}{\\mathrm{Reg}^{T}(K)\\leq\\mathcal{O}\\Big(\\sqrt{H^{2}S K}\\Big(\\sqrt{H}+\\sqrt{A}\\Big)\\ln\\frac{S A H K}{\\delta}+H^{3}S^{4}A^{3}\\big(\\ln\\frac{S A H K}{\\delta}\\big)^{2}\\Big).}\\end{array}$ ", "page_idx": 7}, {"type": "text", "text": "1: Require: $\\delta\\in(0,1)$ , bonuses $b_{k,h}^{r}(s,a),b_{k,h}^{p}(s)$   \n2for $k=1,2,\\dots$ $\\bar{V}_{H+1}^{k}(s)=0$ for $h=H,H-1,..,1$ do   \n5: Calculate the truncated values for all $s\\in S$ $\\bar{V}_{h}^{k}(s)=\\operatorname*{min}\\biggl\\{\\mathbb{E}_{s^{\\prime}\\sim\\hat{P}_{h}^{k-1}(s)}\\biggl[\\operatorname*{max}_{a\\in\\mathcal{A}}\\bigl\\{\\hat{r}_{h}^{k-1}(s,a)+b_{k,h}^{r}(s,a)+\\bar{V}_{h+1}^{k}(s^{\\prime}(a))\\bigr\\}\\biggr]+b_{k,h}^{p}(s),H\\biggr\\}$   \n6: end for   \n78 $h=1,2,\\dots H$ $s_{h}^{k}$ $s_{h+1}^{\\prime k}(s_{h}^{k},a)$ foral $a\\in A$   \n9: Play an action ak E arg maxaeAk $\\begin{array}{r}{a_{h}^{k}\\in\\arg\\operatorname*{max}_{a\\in\\mathcal{A}}\\Bigl\\{\\hat{r}_{h}^{k-1}(s_{h}^{k},a)+b_{k,h}^{r}(s_{h}^{k},a)+\\bar{V}_{h+1}^{k}(s_{h+1}^{\\prime k}(s_{h}^{k},a))\\Bigr\\}}\\end{array}$   \n10: Collect the reward Rk \\~ Rh(sh, ak) and transition to the next state sh1 $s_{h+1}^{k}=s_{h+1}^{\\prime k}(s_{h}^{k},a_{h}^{k})$ ", "page_idx": 8}, {"type": "text", "text": "See proof in Appendix C.8. For transition lookahead, the regret bounds we provide exhibit two rates, both corresponding to a natural adaptation of known lower bounds to transition lookahead. ", "page_idx": 8}, {"type": "text", "text": "1.\u2018Bandit rate' $\\mathcal{O}(\\sqrt{H^{2}S A K})$ : this is the rate due to reward stochasticity. Consider a problem where at odd timesteps $2h-1$ and across all states, all actions have rewards of mean $^1/2-\\epsilon$ except for one action of mean $1/2$ .Assuming that the state-distribution is uniform, each such timestep forms a hard instance of a contextual bandit problem with $S$ contexts, exhibiting a regret of $\\Omega({\\sqrt{S A K}})$ [Auer et al., 2002, Bubeck et al., 2012]. Since there are $H/2$ odd steps and we can design each step independently, the total regret would be $\\Omega(H{\\sqrt{S A K}})$ . The even steps can be used to \u2018remove\u2019 the lookahead and create a uniform state distribution. To do so, we set that when taking an action at odd steps, we always transition to a fixed state $s_{d}$ . From this state, one action $a_{1}$ leads uniformly to all states, while the rest of the actions lead to an absorbing non-rewarding state - rendering them strictly suboptimal. Thus, no-regret agents will only play $a_{1}$ , regardless of the lookahead information, and the state distribution at odd timesteps will be uniform. ", "page_idx": 8}, {"type": "text", "text": "2.\u2018Transition learning rate' $O(\\sqrt{H^{3}S K})$ : recall that the vanilla RL lower bound designs a tree with $\\Omega(S)$ leaves, to which agents need to navigate at the right timing (with $\\Omega(H)$ options) and take the right action (out of $A$ ). While all leaves might transition agents to a rewarding state, one combination of state-action-timing has a slightly higher probability of doing so [Domingues et al., 2021]. This roughly creates a bandit problem with $S A H$ arms, constructed such that the maximal reward is $\\Omega(H)$ , yielding a total regret of $H\\sqrt{H S A K}$ . Now consider the following simple modification where in each leaf, only one action can lead to a reward (and the rest of the actions are \u2018useless\u2019 - never lead to rewards). Thus, the agent still needs to test all leaves at all timings, and so there are still $S H$ \u2018arms\u2019 with a corresponding regret of $\\sqrt{H^{3}S K}$ . Moreover, to test a leaf at a certain timing, we must navigate to it, and since the agent is going to play the single useful action at the leaf, transition lookahead does not provide any additional information. ", "page_idx": 8}, {"type": "text", "text": "As discussed before, transition lookahead can be formulated as an RL instance with stochastic action sets. While Boutilier et al. [2018] prove that with stochastic action sets, Q-learning asymptotically converges, they provide no learning algorithm nor regret bounds. Therefore, to our knowledge, our result is the first to achieve sublinear regret with transition lookahead. ", "page_idx": 8}, {"type": "text", "text": "5.2 Proof Concepts ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Transition lookahead causes similar issues as reward lookahead. Hence, it is natural to apply a similar analysis approach - first, formulate the value as the expectation w.r.t. the next-state observations of the maximum of action-observation dependent values; then use uniform concentration as a ^change of measure\u2019 tool between the empirical and real next-state distribution. In particular, if $V(s,s^{\\prime},a)$ represents the value starting from state $s$ ,performing $a$ and transitioningto $s^{\\prime}$ ,onecanshowthatfor ", "page_idx": 8}, {"type": "text", "text": "all $V(s,\\cdot,\\cdot)\\in[0,H]^{S A}$ (see Lemma 19), ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\mathbb{E}_{s^{\\prime}\\sim\\hat{P}_{h}^{k-1}(s)}\\!\\left[\\operatorname*{max}_{a}V(s,s^{\\prime}(a),a)\\right]-\\mathbb{E}_{s^{\\prime}\\sim P_{h}(s)}\\!\\left[\\operatorname*{max}_{a}V(s,s^{\\prime}(a),a)\\right]\\right|}\\\\ &{\\qquad\\qquad\\qquad\\lesssim\\sqrt{\\frac{S A\\ln\\frac1\\delta\\operatorname{Var}_{s^{\\prime}\\sim\\hat{P}_{h}^{k-1}(s)}\\operatorname*{max}_{a}V(s,s^{\\prime}(a),a)}{n_{h}^{k-1}(s)\\vee1}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "where the variance term stems from using a Bernstein-like concentration bound. However, in contrast to the reward lookahead,the $\\sqrt{S A}$ -factor propagates to the dominant term of the regret, so pursuing this approach would lead to a worse regret bound of $\\tilde{\\mathcal{O}}\\Big(\\sqrt{H^{3}S^{2}A K}\\Big)$ ", "page_idx": 9}, {"type": "text", "text": "To avoid this, we pinpoint the two locations where this change of measure is needed - the proof that $\\bar{V}_{h}^{k}$ is optimistic and the regret decomposition - and make sure to perform this change of measure only on a single value $V_{h}^{*}(\\bar{s},s^{\\prime},a)=\\bar{r_{h}}(s,a)+V_{h+1}^{*}(s^{\\prime})$ , mitigating the need to cover all possible values and removing the additional $\\sqrt{S A}$ -factor. However, doing so leaves us with a residual term. Defining $V_{h}^{*}(s,s^{\\prime})\\stackrel{-}{=}\\operatorname*{max}_{a\\in A}\\{V_{h}^{*}(s,s^{\\prime}(a),a)\\}$ and assuming a similar optimistic value $\\bar{V}_{h}^{k}(s,s^{\\prime})$ \uff0c this term is of the form ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\boldsymbol{s}^{\\prime}\\sim\\hat{P}_{h}^{k-1}(\\boldsymbol{s})}\\big[\\bar{V}_{h}^{k}(\\boldsymbol{s},\\boldsymbol{s}^{\\prime})-V_{h}^{*}(\\boldsymbol{s},\\boldsymbol{s}^{\\prime})\\big]-\\mathbb{E}_{\\boldsymbol{s}^{\\prime}\\sim P_{h}(\\boldsymbol{s})}\\big[\\bar{V}_{h}^{k}(\\boldsymbol{s},\\boldsymbol{s}^{\\prime})-V_{h}^{*}(\\boldsymbol{s},\\boldsymbol{s}^{\\prime})\\big].\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "While similar terms have been analyzed before [e.g., Zanette and Brunskill, 2019, Efroni et al., 2021], the analysis leads to a constant regret term that depends on the support of the distribution in question; in our case, it is the distribution over all possible next-states - of cardinality $S^{A}$ .Therefore, following the same derivation would lead to an exponential additive regret term. ", "page_idx": 9}, {"type": "text", "text": "We overcome it by utilizing the fact that both the optimistic policy and the optimal one decide which action to take according to a list of next-state-actions $(s^{\\prime},a)$ . In other words, instead of looking at the next-state $s^{\\prime}$ (with $S^{A}$ possible values) to determine a value, we look at the highest-ranked realized pair $(s^{\\prime},a)$ in the list that corresponds to the policy that induces the value (with $S A$ possible rankings). Since we have two values, we need to calculate the probability of being at a certain list location for both $\\pi^{k}$ and $\\pi^{*}$ , but the cardinality of this space is $\\bar{(S A)}^{2}$ : polynomial and not exponential. ", "page_idx": 9}, {"type": "text", "text": "6  Conclusions and Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we presented an RL setting in which immediate rewards or transitions are observed before actions are chosen. We showed how to design provably and computationally efficient algorithms for this setting that achieve tight regret bounds versus a strong baseline that also uses lookahead information. Our algorithms rely on estimating the distribution of the reward or transition observations, a concept that might be utilized in other settings. In particular, we believe that our techniques for transition lookahead could be extended to RL problems with stochastic action sets [Boutilier et al., 2018], but leave this for future work. ", "page_idx": 9}, {"type": "text", "text": "One natural extension to our work would be to consider multi-step lookahead information - observing the transition/rewards $L$ steps in advance. We conjecture that from a statistical point of view, a similar algorithmic approach that samples from the empirical observation distribution would be efficient. However, it is not clear how to perform efficient planning with such feedback. ", "page_idx": 9}, {"type": "text", "text": "Another possible direction would be to derive model-free algorithms [Jin et al., 2018], with the aim to improve the computation efficiency of the solutions; our model-based algorithms require at most $O(K\\dot{S}^{2}A H)$ computations per episode due to the planning stage, while model-free algorithms might potentially allow just $\\mathcal{O}(A\\bar{H})$ computations per episode. ", "page_idx": 9}, {"type": "text", "text": "On the practical side, previous works presented RL algorithms that utilize/estimate a world model with multi-step lookahead to perform planning and learning [Schrittwieser et al., 2020, Chung et al., 2024], aiming to achieve the optimal no-lookahead value. For some of these approaches, it is quite natural to replace the simulated world behavior with lookahead information on the real future realization. We leave this adaptation and evaluation to future studies. ", "page_idx": 9}, {"type": "text", "text": "Finally, the notion of lookahead could be studied in various other decision-making settings (e.g., linear MDPs Jin et al. 2020) and can also be generalized to situations where lookahead information can be queried under some budget constraints [Efroni et al., 2021] or when agents only observe noisy lookahead predictions; we leave these problems for future research. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We thank Alon Cohen and Austin Stromme for the helpful discussions. This project has received funding from the European Union's Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie grant agreement No 101034255. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Arpit Agarwal, Rohan Ghuge, and Viswanath Nagarajan. Semi-bandit learning for monotone stochastic optimization. arXiv preprint arXiv:2312.15427, 2023.   \nPeter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multiarmed bandit problem. SIAM journal on computing, 32(1):48-77, 2002.   \nMohammad Gheshlaghi Azar, Ian Osband, and Remi Munos. Minimax regret bounds for reinforcement learning. In International Conference on Machine Learning, pages 263-272. PMLR, 2017.   \nDimitri Bertsekas. A course in reinforcement learning. Athena Scientific, 2023.   \nAndre Biedenkapp, Raghu Rajan, Frank Huter, and Marius Lindauer. Temporl: Learning when to act. In International Conference on Machine Learning, pages 914-924. PMLR, 2021.   \nCraig Boutilier, Alon Cohen, Avinatan Hassidim, Yishay Mansour, Ofer Meshi, Martin Mladenov, and Dale Schuurmans. Planning and learning with stochastic action sets. In Proceedings of the 27th International Joint Conference on Artificial Intelligence, pages 4674-4682, 2018.   \nSebastien Bubeck, Nicolo Cesa-Bianchi, et al. Regret analysis of stochastic and nonstochastic multi-armed bandit problems. Foundations and Trends? in Machine Learning, 5(1):1-122, 2012.   \nEduardo F Camacho, Carlos Bordons, Eduardo F Camacho, and Carlos Bordons. Model predictive control. Springer, 2007.   \nStephen Chung, Ivan Anokhin, and David Krueger. Thinker: learning to plan and act. Advances in Neural Information Processing Systems, 36, 2024.   \nJose Correa, Patricio Foncea, Ruben Hoeksma, Tim Oosterwijk, and Tjark Vredeveld. Posted price mechanisms for a random stream of customers. In Proceedings of the 2017 ACM Conference on Economics and Computation, pages 169-186, 2017.   \nJose Correa, Patricio Foncea, Ruben Hoeksma, Tim Oosterwijk, and Tjark Vredeveld. Recent developments in prophet inequalities. ACM SIGecom Exchanges, 17(1):61-70, 2019.   \nChristoph Dann, Lihong Li, Wei Wei, and Emma Brunskill. Policy certificates: Towards accountable reinforcement learning. In International Conference on Machine Learning, pages 1507-1516, 2019.   \nOmar Darwiche Domingues, Pierre Menard, Emilie Kaufmann, and Michal Valko. Episodic reinforcement learning in finite mdps: Minimax lower bounds revisited. In Algorithmic Learning Theory, pages 578-598. PMLR, 2021.   \nYonathan Efroni, Gal Dalal, Bruno Scherrer, and Shie Mannor. How to combine tree-search methods in reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 3494-3501, 2019a.   \nYonathan Efroni, Nadav Merlis, Mohammad Ghavamzadeh, and Shie Mannor. Tight regret bounds for model-based reinforcement learning with greedy policies. In Advances in Neural Information Processing Systems, pages 12224-12234, 2019b.   \nYonathan Efroni, Mohammad Ghavamzadeh, and Shie Mannor. Online planning with lookahead policies. Advances in Neural Information Processing Systems, 33:14024-14033, 2020.   \nYonathan Efroni, Nadav Merlis, Aadirupa Saha, and Shie Mannor. Confidence-budget matching for sequential budgeted learning. In International Conference on Machine Learning, pages 2937-2947. PMLR, 2021.   \nIbrahim El Shar and Daniel Jiang. Lookahead-bounded q-learning. In International Conference on Machine Learning, pages 8665-8675. PMLR, 2020.   \nNicolas Fournier and Arnaud Guillin. On the rate of convergence in wasserstein distance of the empirical measure. Probability theory and related felds, 162(3):707-738, 2015.   \nKhashayar Gatmiry, Thomas Kesselheim, Sahil Singla, and Yifan Wang. Bandit algorithms for prophet inequality and pandora's box. In Proceedings of the 2024 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 462-500. SIAM, 2024.   \nYunhan Huang, Veeraruna Kavitha, and Quanyan Zhu. Continuous-time markov decision processes with controlled observations. In 2019 57th Annual Allerton Conference on Communication, Control, and Computing (Allerton), pages 32-39. IEEE, 2019.   \nThomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement learning. Journal of Machine Learning Research, 11(Apr):1563-1600, 2010.   \nChi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I Jordan. Is q-learning provably effcient? Advances in neural information processing systems, 31, 2018.   \nChi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement learning with linear function approximation. In Conference on learning theory, pages 2137-2143. PMLR, 2020.   \nYingying Li, Xin Chen, and Na Li. Online optimal control with linear dynamics and predictions: Algorithms and regret analysis. Advances in Neural Information Processing Systems, 32, 2019.   \nYiheng Lin, Yang Hu, Guanya Shi, Haoyuan Sun, Guannan Qu, and Adam Wierman. Perturbationbased regret analysis of predictive control in linear time varying systems. Advances in Neural Information Processing Systems, 34:5174-5185, 2021.   \nYiheng Lin, Yang Hu, Guannan Qu, Tongxin Li and Adam Wierman. Bounded-regret mpc via perturbation analysis: Prediction error,constraints, and nonlinearity.Advances in Neural Information Processing Systems, 35:36174-36187, 2022.   \nAndreas Maurer and Massimiliano Pontil. Empirical bernstein bounds and sample variance penalization. In Conference on learning theory, 2009.   \nNadav Merlis, Dorian Baudry, and Vianney Perchet. The value of reward lookahead in reinforcement learning. arXiv preprint arXiv:2403.11637, 2024.   \nThomas M Moerland, Anna Deichler, Simone Baldi, Joost Broekens, and Catholijn M Jonker. Think neither too fast nor too slow: The computational trade-off between planning and reinforcement learning. In Proceedings of the International Conference on Automated Planning and Scheduling (ICAPS), Nancy, France, pages 16-20, 2020.   \nEvdokia Nikolva and David R Karger. Route planning under uncertainty: The canadian traveller problem. In AAAI, pages 969-974, 2008.   \nMartin L Puterman. Markov decision processes: discrete stochastic dynamic programming. John Wiley & Sons, 2014.   \nAviv Rosenberg, Assaf Hallak, Shie Mannor, Gal Chechik, and Gal Dalal. Planning and learning with adaptive lookahead. In Proceedings of the AAAl Conference on Arificial Intelligence, volume 37, pages 9606-9613, 2023.   \nJulian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari, go, chess and shogi by planning with a learned model. Nature, 588(7839):604-609, 2020.   \nMax Simchowitz and Kevin G Jamieson. Non-asymptotic gap-dependent regret bounds for tabular mdps. In Advances in Neural Information Processing Systems, pages 1153-1162, 2019.   \nAviv Tamar, Garrett Thomas, Tianhao Zhang, Sergey Levine, and Pieter Abbeel. Learning from the hindsight plan\u2014episodic mpc improvement. In 2017 IEEE International Conference on Robotics and Automation (ICRA), pages 336-343. IEEE, 2017.   \nChenkai Yu, Guanya Shi, Soon-Jo Chung, Yisong Yue, and Adam Wierman. The power of predictions in online control. Advances in Neural Information Processing Systems, 33:1994-2004, 2020.   \nAndrea Zanette and Emma Brunskill. Tighter problem-dependent regret bounds in reinforcement learning without domain knowledge using value function bounds. In International Conference on Machine Learning, pages 7304-7312. PMLR, 2019.   \nRunyu Zhang, Yingying Li, and Na Li. On the regret analysis of online lqr control with predictions. In 2021 American Control Conference (ACC), pages 697-703. IEEE, 2021a.   \nZihan Zhang, Xiangyang Ji, and Simon Du. Is reinforcement learning more difficult than bandits? a near-optimal algorithm escaping the curse of horizon. In Conference on Learning Theory, pages 4528-4531. PMLR, 2021b.   \nZihan Zhang, Yuxin Chen, Jason D Lee, and Simon S Du. Settling the sample complexity of online reinforcement learning. arXiv preprint arXiv:2307.13586, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Table of Contents ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A Structure of the Appendix 14 ", "page_idx": 13}, {"type": "text", "text": "B Proofs for Reward Lookahead 15 ", "page_idx": 13}, {"type": "text", "text": "B.1 Data Generation Process . . 15   \nB.2 Extended MDP for Reward Lookahead.: 15   \nB.3Full Algorithm Description for Reward Lookahead . 20   \nB.4 The First Good Event - Concentration . . 21   \nB.5 Optimism of the Upper Confidence Value Functions 22   \nB.6 The Second Good Event - Martingale Concentration . 23   \nB.7 Regret Analysis . . . 25 ", "page_idx": 13}, {"type": "text", "text": "C Proofs for Transition Lookahead 30 ", "page_idx": 13}, {"type": "text", "text": "C.1 Data Generation Process . . 30   \nC.2 Extended MDP for Transition Lookahead . . 30   \nC.3 Full Algorithm Description for Transition Lookahead . . 34   \nC.4  Additional Notations and List Representation . . : 35   \nC.5 The First Good Event - Concentration 37   \nC.6 Optimism of the Upper Confidence Value Functions 39   \nC.7 The Second Good Event - Martingale Concentration 40   \nC.8 Regret Analysis ....... 41   \nC.9 Example: Value Gain due to Transition Lookahead . 46 ", "page_idx": 13}, {"type": "text", "text": "D  Auxiliary Lemmas 47 ", "page_idx": 13}, {"type": "text", "text": "D.1  Concentration results 47   \nD.2 Count-Related Lemmas 51   \nD.3   Analysis of Variance terms 52 ", "page_idx": 13}, {"type": "text", "text": "E Existing Results 53 ", "page_idx": 13}, {"type": "text", "text": "A Structure of the Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Both reward and transition lookahead appendices share the following structure. First, we describe our assumption on the data generation process and analyze general properties of reward and transition lookahead. This is done by looking at an extended MDP that incorporates the lookahead information into the state. Then, we present the full algorithm and describe the relevant probabilistic events that ensure the concentration of all the empirical quantities. For transition lookahead, we require some additional notions for the event definitions (including the list representation of values and policies), which are explained in a separate subsection. ", "page_idx": 13}, {"type": "text", "text": "Given the concentration-related good event, we can prove that the planning procedure in the algorithm is optimistic, which we do in the subsequent subsection. Then, we define an additional good event that allows adding and removing conditional expectations in a way that will be needed for the proof. ", "page_idx": 13}, {"type": "text", "text": "At this point, we provided all (almost all) the results required for the regret analysis, and the proof of the main theorems is stated. The proofs also require some additional analysis for the bonuses (and especially variance terms), which is located at the end of the regret analysis. ", "page_idx": 13}, {"type": "text", "text": "For transition lookahead, the appendix includes one more part that further analyzes the example presented in Section 3. ", "page_idx": 13}, {"type": "text", "text": "At the end of the appendix, we state and prove several lemmas that will be used throughout our analysis, while also stating several existing results that will be of use. ", "page_idx": 13}, {"type": "text", "text": "B Proofs for Reward Lookahead ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1  Data Generation Process ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "To simplify the proofs, we assume the following 'tabular\u2019 data-generation process: Before the game starts, a set of $K$ samples from the transition probabilities and rewards is generated for all $(s,a,h)$ Once a state $s$ at step $h$ is visited for the $i^{t h}$ time, the $i^{t h}$ sample from the reward distribution $\\mathcal{R}_{h}(s)$ is the reward realization for all action $a\\in{\\mathcal{A}}$ . When a state-action pair is visited for the $i^{t h}$ time, the $i^{t h}$ sample from the transition kernel $P_{h}(\\cdot|s,a)$ determines the next-state realization. In particular, it implies that the reward samples from the first $i$ visits to a state are i.i.d., and the same for the next-states samples and state-action visitations. Throughout this appendix, we use the notation $\\bar{\\mathbf{R}_{h}^{k}}\\,=\\,\\bigl\\{R_{h}^{k}\\bigl(s_{h}^{k},a\\bigr\\}_{a\\in\\mathcal{A}}$ to denote the reward observatio at episode $k$ and timestep $h$ fo lth e actions. ", "page_idx": 14}, {"type": "text", "text": "For the proof, we define the following three filtrations. Let ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F_{k,h}=\\sigma\\bigg(\\Big\\{s_{t}^{1},a_{t}^{1},R_{t}^{1}\\Big\\}_{t\\in[H]},\\cdots,\\Big\\{s_{t}^{k-1},a_{t}^{k-1},R_{t}^{k-1}\\Big\\}_{t\\in[H]},\\Big\\{s_{t}^{k},a_{t}^{k},R_{t}^{k}\\Big\\}_{t\\in[h]},s_{h+1}^{k}\\bigg),}\\\\ &{F_{k,h}^{R}=\\sigma\\bigg(\\Big\\{s_{t}^{1},a_{t}^{1},R_{t}^{1}\\Big\\}_{t\\in[H]},\\cdots,\\Big\\{s_{t}^{k-1},a_{t}^{k-1},R_{t}^{k-1}\\Big\\}_{t\\in[H]},\\Big\\{s_{t}^{k},a_{t}^{k},R_{t}^{k}\\Big\\}_{t\\in[h+1]}\\bigg),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "the filtrations that contains all information until episode $k$ and step $h$ , as well as the state at timestep $h+1$ , or all information of time $h+1$ , respectively. We make this distinction so that $F_{k,h-1}$ contains only $s_{h}^{k}$ while $F_{k,h-1}^{R}$ also contains $a_{h}^{k}$ We also defne ", "page_idx": 14}, {"type": "equation", "text": "$$\nF_{k}=\\sigma\\Bigg(\\big\\{s_{t}^{1},a_{t}^{1},\\pmb{R}_{t}^{1}\\big\\}_{t\\in[H]},\\pmb{\\cdot},\\-\\cdot\\,,\\Big\\{s_{t}^{k},a_{t}^{k},\\pmb{R}_{t}^{k}\\Big\\}_{t\\in[H]},s_{1}^{k+1}\\Bigg),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which contains all information up to the end of the $k^{t h}$ episode, as well as the initial state at episode $k+1$ ", "page_idx": 14}, {"type": "text", "text": "B.2Extended MDP for Reward Lookahead ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this appendix, we present an alternative formulation of the one-step reward lookahead that falls under the vanilla (no-lookahead) model and would be helpful for the analysis. ", "page_idx": 14}, {"type": "text", "text": "Throughout the section, we study the relations between MDPs with and without reward lookahead, and between different MDPs with lookahead. Therefore, for clarity, we state the concerning MDP in the value, e.g. $V^{R,\\pi}(s|\\mathcal{M})$ . Specifically in this subsection, we distinguish between values without lookahead (denoted $V^{\\pi}$ ) and values with lookahead (denoted $V^{R,\\pi}$ ). In the following subsections, unless stated otherwise, we will only consider lookahead values; for brevity, and with some abuse of notations, we will then omit the $R$ in the value notation. ", "page_idx": 14}, {"type": "text", "text": "For any MDP $\\mathcal{M}=(\\mathcal{S},\\mathcal{A},H,P,\\mathcal{R})$ , define an equivalent extended MDP $\\mathcal{M}^{R}$ of horizon $2H$ that separates the state transition and reward generation as follows: ", "page_idx": 14}, {"type": "text", "text": "1. Assume w.l.o.g. that $\\mathcal{M}$ starts at some initial state $s_{1}$ . The extended environment starts at a state $s_{1}\\times\\mathbf{0}$ , where $\\mathbf{0}\\in\\mathbb{R}^{A}$ is the zeros vector.   \n2. For any $h\\in[H]$ , at timestep $2h-1$ , the environment $\\mathcal{M}^{R}$ transitions from state $s_{h}\\times\\mathbf{0}$ to $s_{h}\\times R$ ,where $\\mathbf{\\dot{\\boldsymbol{R}}}\\sim\\mathcal{R}_{h}(s)$ is a vector containing the rewards for all actions $a\\in{\\mathcal{A}}$ . This transition occurs regardless of the action that was played. At timestep $2h$ , given an action $a_{h}$ the environment transitions from $s_{h}\\times R$ to $s_{h+1}\\times\\mathbf{0}$ , where $s_{h+1}\\sim P_{h}(\\cdot|s_{h},a_{h})$   \n3. The reward at a state $s\\times R$ when playing an action $a$ is $R(a)$ , namely, the reward is deterministic and only obtained on even timesteps. ", "page_idx": 14}, {"type": "text", "text": "We emphasize that throughout the section, we assume that $\\mathcal{M}$ and $\\mathcal{M}^{R}$ are coupled; that is, assume that under a policy $\\pi$ in $\\mathcal{M}$ , the agent visits a state $s_{h}$ , observes $\\scriptstyle R_{h}$ , plays an action $a_{h}$ and transitions to $s_{h+1}$ . Then, in $\\mathcal{M}^{R}$ , the agent starts from $s_{h}\\times\\mathbf{0}$ , transitions to $s_{h}\\times R$ (regardless of the action it played), takes the action $a_{h}$ and finally transitions to $s_{h+1}\\times\\mathbf{0}$ ", "page_idx": 14}, {"type": "text", "text": "Since the reward is embedded into the state, any state-dependent policy in $\\mathcal{M}^{R}$ is a one-step reward lookahead policy in the original MDP. Moreover, the policy at the odd steps of $\\mathcal{M}$ does not affect the value, and assuming that the policy at the even steps in $\\mathcal{M}^{R}$ is the same as the policy in $\\mathcal{M}$ ,we trivially get the following relation between the values ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{2h}^{\\pi}(s,R|\\mathcal{M}^{R})=\\mathbb{E}\\Bigg[\\underset{t=h}{\\overset{H}{\\sum}}R_{t}(s_{t},a_{t})|s_{h}=s,R_{h}(s,\\cdot)=R,\\pi\\Bigg]\\triangleq V_{h}^{R,\\pi}(s,R|\\mathcal{M}),}\\\\ &{V_{2h-1}^{\\pi}(s,\\mathbf{0}|\\mathcal{M}^{R})=\\mathbb{E}\\Bigg[\\underset{t=h}{\\overset{H}{\\sum}}R_{t}(s_{t},a_{t})|s_{h}=s,\\pi\\Bigg]=V_{h}^{R,\\pi}(s|\\mathcal{M}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "While $\\mathcal{M}^{R}$ has a continuous state space, which generally makes algorithm design impractical, this representation permits applying classic results on MDPs to environments with one-step lookahead. ", "page_idx": 15}, {"type": "text", "text": "As a remark, rewards could be directly embedded into the state without separating the state and reward updates. However, this creates unnecessary complications when analyzing the relations between similar environments. This is because we are mainly interested in the value given the state - in expectation over the realized rewards. In particular, value-difference are analyzed assuming a shared initial state, but in our case, we do not want to assume the same reward realization, but rather also account for the distance between reward distributions, which the step separation enables. For similar reasons, this representation also simplifies the proof of the law of total variance [Azar et al., 2017]. ", "page_idx": 15}, {"type": "text", "text": "Proposition 1. The optimal value of one-step reward lookahead agents satisfies ", "page_idx": 15}, {"type": "equation", "text": "$$\nV_{H+1}^{R,*}(s)=0\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\n\\forall s\\in S,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\nV_{h}^{R,*}(s)=\\mathbb{E}_{R\\sim\\mathcal{R}_{h}(s)}\\left[\\operatorname*{max}_{a\\in\\mathcal{A}}\\left\\{R_{h}(s,a)+\\sum_{s^{\\prime}\\in\\mathcal{S}}P_{h}(s^{\\prime}|s,a)V_{h+1}^{R,*}(s^{\\prime})\\right\\}\\right],\\quad\\forall s\\in\\mathcal{S},h\\in[H].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Also, given reward observations $\\pmb{R}=\\{R(a)\\}_{a\\in\\mathcal{A}}$ at state s and step $h$ the optimal policy is ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\pi_{h}^{*}(s,R)\\in\\underset{a\\in\\mathcal{A}}{\\arg\\operatorname*{max}}\\Biggl\\{R(a)+\\sum_{s^{\\prime}\\in\\mathcal{S}}P_{h}(s^{\\prime}|s,a)V_{h+1}^{R,*}(s^{\\prime})\\Biggr\\}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. We prove the result in the extended MDP $\\mathcal{M}^{R}$ and remind the reader that in this formulation, the policy only uses state information, as in the standard RL formulation. In particular, it implies that there exists a Markovian optimal policy that uniformly maximizes the value (in the extended state space), and the optimal value is given through the dynamic-programming equations [Puterman, 2014] \u4e00 ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\tilde{\\gamma}_{2H+1}^{*}(s,R|\\mathcal{M}^{R})=0,\\ }&{\\forall s\\in\\mathcal{S},R\\in\\mathbb{R}^{A},}\\\\ &{\\tilde{\\gamma}_{2h}^{*}(s,R|\\mathcal{M}^{R})=\\operatorname*{max}_{a}\\biggl\\{R(a)+\\displaystyle\\sum_{s^{\\prime}\\in\\mathcal{S}}P_{h}(s^{\\prime}|s,a)V_{2h+1}^{*}(s^{\\prime},\\mathbf{0}|\\mathcal{M}^{R})\\biggr\\},}&{\\forall h\\in[H],s\\in\\mathcal{S},R\\in\\mathbb{R}^{A},}\\\\ &{\\tilde{\\gamma}_{2h-1}^{*}(s,\\mathbf{0}|\\mathcal{M}^{R})=\\mathbb{E}_{\\mathcal{R}_{h}(s)}\\left[V_{2h}^{*}(s,R|\\mathcal{M}^{R})\\right],\\ }&{\\forall h\\in[H],s\\in\\mathcal{S}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By the equivalence between $\\mathcal{M}$ and $\\mathcal{M}^{R}$ for all policies, this is also the optimal value in $\\mathcal{M}$ Specifically, combining both recursion equations and substituting the relation between the original and extended values of Equation (3), we get the desired value recursion for any $h\\in[H]$ and $s\\in S$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{h}^{R,*}(s|\\mathcal{M})=V_{2h-1}^{*}(s,\\mathbf{0}|\\mathcal{M}^{R})}\\\\ &{=\\mathbb{E}_{\\mathcal{R}_{h}(s)}\\left[V_{2h}^{*}(s,R|\\mathcal{M}^{R})\\right]}\\\\ &{=\\mathbb{E}_{\\mathcal{R}_{h}(s)}\\left[\\underset{a}{\\operatorname*{max}}\\left\\lbrace R(a)+\\sum_{s^{\\prime}\\in\\mathcal{S}}P_{h}(s^{\\prime}|s,a)V_{2h+1}^{*}(s^{\\prime},\\mathbf{0}|\\mathcal{M}^{R})\\right\\rbrace\\right]}\\\\ &{=\\mathbb{E}_{\\mathcal{R}_{h}(s)}\\left[\\underset{a}{\\operatorname*{max}}\\left\\lbrace R(a)+\\sum_{s^{\\prime}\\in\\mathcal{S}}P_{h}(s^{\\prime}|s,a)V_{h+1}^{R,*}(s|\\mathcal{M})\\right\\rbrace\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Similarly, for any $h\\in[H]$ \uff0c $s\\in S$ and $\\pmb{R}\\in\\mathbb{R}^{A}$ , the optimal policy at the even stages of the extended MDP is ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\pi_{2h}^{*}(s,R)\\in\\underset{a\\in\\mathcal{A}}{\\arg\\operatorname*{max}}\\bigg\\{R(a)+\\sum_{s^{\\prime}\\in\\mathcal{S}}P_{h}(s^{\\prime}|s,a)V_{2h+1}^{*}(s^{\\prime},\\mathbf{0}|\\mathcal{M}^{R})\\bigg\\},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "alongside arbitrary actions at odd steps. Playing this policy in the original MDP will lead to an optimal one-step reward lookahead policy, as it achieves the optimal value of the original MDP. This policy directly translates to the optimal policy in the statement, by the equivalence between the origial and extended MDPs and therelation $V_{2h+1}^{\\ast\\,\\,\\dot{\\,}}(s^{\\prime},{\\bf0}|{\\mathcal{M}}^{R})=V_{h+1}^{\\dot{R},\\ast}(s^{\\prime}|\\dot{\\mathcal{M}})$ \u53e3 ", "page_idx": 16}, {"type": "text", "text": "Remark 1. As in Equation (4), one could also write the dynamic programming equations for any policy $\\pi\\in\\Pi^{R}$ namely ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\int_{2h}^{\\pi}(s,R|\\mathcal{M}^{R})=R(\\pi_{h}(s,R))+\\displaystyle\\sum_{s^{\\prime}\\in\\mathcal{S}}P_{h}(s^{\\prime}|s,\\pi_{h}(s,R))V_{2h+1}^{\\pi}(s^{\\prime},\\mathbf{0}|\\mathcal{M}^{R}),}&{\\forall h\\in[H],s\\in\\mathcal{S},R\\in\\mathbb{N}}\\\\ &{\\int_{2h-1}^{\\pi}(s,\\mathbf{0}|\\mathcal{M}^{R})=\\mathbb{E}_{\\mathcal{R}_{h}(s)}\\big[V_{2h}^{\\pi}(s,R|\\mathcal{M}^{R})\\big],}&{\\forall h\\in[H],s}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "In particular, following the notation of Equation (3), one can also write ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{h}^{R,\\pi}(s,R|\\mathcal{M})=R(\\pi_{h}(s,R))+\\displaystyle\\sum_{s^{\\prime}\\in S}P_{h}(s^{\\prime}|s,\\pi_{h}(s,R))V_{h+1}^{R,\\pi}(s^{\\prime}|\\mathcal{M}),\\quad a n d,}\\\\ &{V_{h}^{R,\\pi}(s|\\mathcal{M})=\\mathbb{E}_{\\mathcal{R}_{h}(s)}\\Big[V_{h}^{R,\\pi}(s,R|\\mathcal{M})\\Big]}\\\\ &{\\qquad\\qquad=\\mathbb{E}_{\\mathcal{R}_{h}(s)}\\Bigg[R(\\pi_{h}(s,R))+\\displaystyle\\sum_{s^{\\prime}\\in S}P_{h}(s^{\\prime}|s,\\pi_{h}(s,R))V_{h+1}^{R,\\pi}(s^{\\prime}|\\mathcal{M})\\Big)\\Bigg].}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We will use this notation in some of the proofs. ", "page_idx": 16}, {"type": "text", "text": "Another useful application of the extended MDP is a variation of the law of total variance (LTV), which will be useful in our analysis ", "page_idx": 16}, {"type": "text", "text": "Lemma 3. For any deterministic one-step reward lookahead policy $\\pi\\in\\Pi^{R}$ itholdsthat ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\sum_{h=1}^{H}\\mathrm{Var}_{P_{h}(\\cdot\\vert s_{h},a_{h})}(V_{h+1}^{R,\\pi}(s_{h+1}))\\vert\\pi,s_{1}\\right]\\le\\mathbb{E}\\left[\\left(\\sum_{h=1}^{H}R_{h}(s_{h},a_{h})-V_{1}^{R,\\pi}(s_{1})\\right)^{2}\\vert\\pi,s_{1}\\right].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. We apply the law of total variance (Lemma 27) in the extended MDP; there, the rewards are deterministic and equal to either O (at odd steps) or $R_{h}(s_{h},a_{h})$ (at even steps), so the total expected rewards are $\\begin{array}{r}{\\sum_{h=1}^{H}R_{h}(s_{h},a_{h})}\\end{array}$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left(\\displaystyle\\sum_{h=1}^{H}R_{h}(s_{h},a_{h})-V_{1}^{\\pi}(s_{1},0|M^{h})\\right)^{2}|\\pi,s_{1}\\right]}\\\\ &{=\\mathbb{E}\\left[\\displaystyle\\sum_{s\\geq1}^{H}\\mathrm{Var}(V_{2h}^{\\pi}(s_{h},R_{h}(s_{h})|M^{h})|(s_{h},0))+\\sum_{b=1}^{H}\\mathrm{Var}(V_{2h+1}^{\\pi}(s_{h+1},0|M^{h})|(s_{h},R_{h}(s_{h})))|\\pi\\right.}\\\\ &{\\left.\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\mathrm{~G~al~and~\\pi~}}\\\\ &{\\geq\\mathbb{E}\\left[\\displaystyle\\sum_{h=1}^{H}\\mathrm{Var}(V_{2h+1}^{\\pi}(s_{h+1},0|M^{h})|(s_{h},R_{h}(s_{h})))|\\pi,s_{1}\\right]}\\\\ &{=\\mathbb{E}\\left[\\displaystyle\\sum_{h=1}^{H}\\mathrm{Var}_{h\\in\\mathbb{F}_{+}(\\mu_{h+1},\\mu_{h})}(V_{2h+1}^{\\pi}(s_{h+1},0|M^{h}))|\\pi,s_{1}\\right]}\\\\ &{=\\mathbb{E}\\left[\\displaystyle\\sum_{h\\geq1}^{H}\\mathrm{Var}_{\\mathbb{F}_{+}(\\mu_{h+1},\\mu_{h})}(V_{h+1}^{\\pi}(s_{h+1}|M)|)|\\pi,s_{1}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Noting that ${V_{1}^{\\pi}(s_{1},\\mathbf{0}|\\mathcal{M}^{R})=V_{1}^{R,\\pi}(s_{1}|\\mathcal{M})}$ concludes the proof. ", "page_idx": 16}, {"type": "text", "text": "Finally, though not needed in our analysis, we use the extended MDP to prove the following valuedifference lemma, which could be of further use in follow-up works. While we prove decomposition just using the next-step values, one could recursively apply the formula until the end of the episode to immediately get another formula that does not depend on the next value. ", "page_idx": 16}, {"type": "text", "text": "Lemma 4 (Value-Difference Lemma with Reward Lookahead). Let $\\mathcal{M}_{1}=(\\mathcal{S},\\mathcal{A},H,P^{1},\\mathcal{R}^{1})$ and $\\mathscr{M}_{2}=(S,\\mathscr{A},H,P^{2},\\mathscr{R}^{2})$ be two environments. For any deterministic one-step reward lookahead policy $\\pi\\in\\Pi^{R}$ any $h\\in[H]$ and $s\\in S$ it holds that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{h}^{R,\\pi}(s|\\mathcal{M}_{1})-V_{h}^{R,\\pi}(s|\\mathcal{M}_{2})}\\\\ &{\\qquad=\\mathbb{E}_{M_{1}}\\Big[V_{h+1}^{R,\\pi}(s_{h+1}|\\mathcal{M}_{1})-V_{h+1}^{R,\\pi}(s_{h+1}|\\mathcal{M}_{2})|s_{h}=s\\Big]}\\\\ &{\\qquad+\\mathbb{E}_{M_{1}}\\Bigg[\\underset{s^{\\prime}\\in\\mathcal{S}}{\\sum}\\big(P_{h}^{1}(s^{\\prime}|s_{h},\\pi_{h}(s_{h},R_{h}))-P_{h}^{2}(s^{\\prime}|s_{h},\\pi_{h}(s_{h},R_{h}))\\big)V_{h+1}^{R,\\pi}(s^{\\prime}|\\mathcal{M}_{2})|s_{h}=s\\Bigg]}\\\\ &{\\qquad+\\mathbb{E}_{M_{1}}\\Big[\\mathbb{E}_{\\mathcal{R}_{h}^{1}(s)}\\Big[V_{h}^{R,\\pi}(s_{h},R|\\mathcal{M}_{2})\\Big]-\\mathbb{E}_{\\mathcal{R}_{h}^{2}(s)}\\Big[V_{h}^{R,\\pi}(s_{h},R|\\mathcal{M}_{2})\\Big]|s_{h}=s\\Big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $V_{h}^{R,\\pi}(s,R|\\mathcal{M})$ isthal dddq andgiven inRemark $^{\\,l}$ ", "page_idx": 17}, {"type": "text", "text": "Proof. We again work with the extended MDPs $\\mathcal{M}_{1}^{R},\\mathcal{M}_{2}^{R}$ : Since under the extension, both the environments and the policy are Markovian, all values obey the following Bellman equations: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\langle s,R|\\mathcal{M}^{R}\\rangle=R(\\pi_{h}(s,R))+\\displaystyle\\sum_{s^{\\prime}\\in S}P_{h}(s^{\\prime}|s,\\pi(s,R))V_{2h+1}^{\\pi}(s^{\\prime},\\mathbf{0}|\\mathcal{M}^{R}),}&{\\forall h\\in[H],s\\in S,R\\in\\mathbb{R}^{A}}\\\\ &{}&\\\\ &{-1(s,\\mathbf{0}|\\mathcal{M}^{R})=\\mathbb{E}_{\\mathcal{R}_{h}(s)}\\big[V_{2h}^{\\pi}(s,R|\\mathcal{M}^{R})\\big],}&{\\forall h\\in[H],s\\in\\mathcal{S}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Using the relation between the value of the original and extended MDP (eq. (3)) and the Bellman equations of the extended MDP, for any $h\\in[H]$ ,wehave ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{h}^{R,\\pi}(s|\\mathcal{M}_{1})-V_{h}^{R,\\pi}(s|\\mathcal{M}_{2})}\\\\ &{=V_{2h-1}^{\\pi}(s,0|\\mathcal{M}_{1}^{R})-V_{2h-1}^{\\pi}(s,0|\\mathcal{M}_{2}^{R})}\\\\ &{=\\mathbb{E}_{\\mathcal{R}_{h}^{1}(s)}\\left[V_{2h}^{\\pi}(s,R|\\mathcal{M}_{1}^{R})\\right]-\\mathbb{E}_{\\mathcal{R}_{h}^{2}(s)}\\left[V_{2h}^{\\pi}(s,R|\\mathcal{M}_{2}^{R})\\right]}\\\\ &{=\\mathbb{E}_{\\mathcal{R}_{h}^{1}(s)}\\left[V_{2h}^{\\pi}(s,R|\\mathcal{M}_{1}^{R})-V_{2h}^{\\pi}(s,R|\\mathcal{M}_{2}^{R})\\right]+\\mathbb{E}_{\\mathcal{R}_{h}^{1}(s)}\\left[V_{2h}^{\\pi}(s,R|\\mathcal{M}_{2}^{R})\\right]-\\mathbb{E}_{\\mathcal{R}_{h}^{2}(s)}\\left[V_{2h}^{\\pi}(s,R|\\mathcal{M}_{1}^{R})-V_{2h}^{\\pi}(s,R|\\mathcal{M}_{2}^{R})\\right]}\\\\ &{=\\mathbb{E}_{\\mathcal{R}_{h}^{1}(s)}\\left[V_{2h}^{\\pi}(s,R|\\mathcal{M}_{1}^{R})-V_{2h}^{\\pi}(s,R|\\mathcal{M}_{2}^{R})\\right]+\\mathbb{E}_{\\mathcal{R}_{h}^{1}(s)}\\left[V_{h}^{R,\\pi}(s,R|\\mathcal{M}_{2}^{R})\\right]-\\mathbb{E}_{\\mathcal{R}_{h}^{2}(s)}\\left[V_{h}^{R,\\pi}(s,R|\\mathcal{M}_{1}^{R})-V_{2h}^{\\pi}(s,R|\\mathcal{M}_{2}^{R})\\right]}\\\\ &{=\\mathbb{E}_{\\mathcal{M}_{1}}\\left[V_{2h}^{\\pi}(s_{h},R_{h}|\\mathcal{M}_{1}^{R})-V_{2h}^{\\pi}(s_{h},R_{h}|\\mathcal{M}_{2}^{R})|s_{h}=s\\right]}\\\\ &{\\phantom{=\\quad}+\\mathbb{E}_{\\\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We now focus on the first term. Denoting $a_{h}\\;=\\;\\pi_{h}\\bigl(s_{h},R_{h}\\bigr)$ the action taken by the agent at environment $\\mathcal{M}_{1}$ ,We have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{V_{2k}^{n}\\left(s_{k},R_{k}|M_{1}^{n}\\right)-V_{n k}^{n}(s_{k},R_{k}|M_{2}^{n})}&{\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!}\\\\ &{=\\Bigg(R_{k}(a_{k})+\\sum_{\\ell\\in{\\mathcal{S}}_{k}}^{n}\\boldsymbol{\\mathbb{D}}_{k}^{\\top}(s_{k},a_{k})\\boldsymbol{\\mathbb{V}}_{\\bar{k}+1}^{n}(\\ell,0|\\boldsymbol{\\mathcal{M}}_{1}^{n})\\Bigg)}\\\\ &{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Substituting this back into Equation (5), we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{h}^{\\pi}(s|M_{1})-V_{h}^{\\pi}(s|M_{2})}\\\\ &{\\quad=\\mathbb{E}_{M_{1}}\\left[E_{M_{1}}\\Big[V_{h+1}^{R,\\pi}(s_{h+1}|M_{1})-V_{h+1}^{R,\\pi}(s_{h+1}|M_{2})|s_{h},a_{h}\\Big]|s_{h}=s\\right]}\\\\ &{\\quad\\quad+\\mathbb{E}_{M_{1}}\\left[\\sum_{\\sigma^{\\downarrow}\\in\\mathcal{S}}(P_{h}^{1}(s^{\\prime}|s_{h},a_{h})-P_{h}^{2}(s^{\\prime}|s_{h},a_{h}))V_{h+1}^{R,\\pi}(s^{\\prime}|M_{2})|s_{h}=s\\right]}\\\\ &{\\quad\\quad+\\mathbb{E}_{\\pi_{h}^{\\pi}\\left[\\big[V_{h}^{R,\\pi}(s,R|M_{2})\\big]-\\mathbb{E}_{\\pi_{h}^{2}}(s)\\big[V_{h}^{R,\\pi}(s,R|M_{2})\\big]\\right.}\\\\ &{\\quad=\\mathbb{E}_{M_{1}}\\Big[V_{h+1}^{R,\\pi}(s_{h+1}|M_{1})-V_{h+1}^{R,\\pi}(s_{h+1}|M_{2})|s_{h}=s\\Big]}\\\\ &{\\quad\\quad+\\mathbb{E}_{M_{1}}\\left[\\sum_{\\sigma^{\\uparrow}\\in\\mathcal{S}}(P_{h}^{1}(s^{\\prime}|s_{h},\\pi_{h}(s_{h},R_{h}))-P_{h}^{2}(s^{\\prime}|s_{h},\\pi_{h}(s_{h},R_{h})))V_{h+1}^{R,\\pi}(s^{\\prime}|M_{2})|s_{h}=s\\right]}\\\\ &{\\quad\\quad+\\mathbb{E}_{M_{1}}\\Big[\\mathbb{E}_{\\pi_{h}^{\\pi}(s)}\\Big[V_{h}^{R,\\pi}(s_{h},R|M_{2})\\Big]-\\mathbb{E}_{\\pi_{h}^{2}(s)}\\Big[V_{h}^{R,\\pi}(s_{h},R|M_{2})\\Big]|s_{h}=s\\Big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Aigoritmm 5 IMionotomc vaiue Fropagauon witn Kewaru LooKaneau (Iv vr-KL)   \n1: Require: $\\delta\\in(0,1)$ , bonuses $b_{k,h}^{r}(s),b_{k,h}^{p}(s,a)$   \n2: for $k=1,2,\\dots$ do   \n3: Initialive $\\bar{V}_{H+1}^{k}(s)=0$   \n4: for $h=H,H-1,..,1$ do   \n5: for $s\\in S$ do   \n6: $n_{h}^{k-1}(s)=0$ then   \n7: $\\bar{V}_{h}^{k}(s)=H$   \n8: else   \n9: Calculate the truncated values   \n$\\bar{V}_{h}^{k}(s)=\\operatorname*{min}\\left\\{\\frac{1}{n_{h}^{k-1}(s)}\\sum_{t=1}^{n_{h}^{k-1}(s)}\\operatorname*{max}_{a\\in A}\\Bigl\\{R_{h}^{k_{h}^{t}(s)}(s,a)+b_{k,h}^{p}(s,a)+\\hat{P}_{h}^{k-1}\\bar{V}_{h+1}^{k}(s,a)\\Bigr\\}+b_{k,h}^{r}(s),H\\right\\}$   \n10: end if   \n11: For any vector $\\pmb{R}\\in\\mathbb{R}^{A}$ , define the policy $\\pi^{k}$   \n$\\pi_{h}^{k}(s,R)\\in\\underset{a\\in\\mathcal{A}}{\\arg\\operatorname*{max}}\\Bigl\\{R(a)+b_{k,h}^{p}(s,a)+\\hat{P}_{h}^{k-1}\\bar{V}_{h+1}^{k}(s,a)\\Bigr\\}$   \n12: end for   \n13: end for   \n14: for $h=1,2,\\dots H$ do   \n15: Observe $s_{h}^{k}$ and $\\mathbfit{R}_{h}^{k}=\\left\\{R_{h}^{k}(s_{h}^{k},a)\\right\\}_{a\\in\\cal{A}}$   \n16: Play an action $a_{h}^{k}=\\pi_{h}^{k}(s_{h}^{k},R_{h}^{k})$   \n17: Collecthe reward $R_{h}^{k}(s_{h}^{k},a_{h}^{k})$ and transition to th next state $s_{h+1}^{k}\\sim P_{h}(\\cdot|s_{h}^{k},a_{h}^{k})$   \n18: end for   \n19: Update the empirical estimators and counts for all visited state-actions   \n20: end for ", "page_idx": 19}, {"type": "text", "text": "We use a variant of the MVP algorithm [Zhang et al., 2021b] while adapting their proof and the one from [Efroni et al., 2021]. The algorithm is described in Algorithm 3 and uses the following bonuses: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{b_{k,h}^{r}(s)=3\\sqrt{\\frac{A L_{\\delta}^{k}}{2\\left(n_{h}^{k-1}(s)\\vee1\\right)}},}}\\\\ {{b_{k,h}^{p}(s,a)=\\operatorname*{min}\\left\\{\\displaystyle\\frac{20}{3}\\sqrt{\\frac{\\operatorname{Var}_{\\hat{P}_{h}^{k-1}(\\cdot\\vert s,a)}(\\bar{V}_{h+1}^{k})L_{\\delta}^{k}}{n_{h}^{k-1}(s,a)\\vee1}}+\\displaystyle\\frac{400}{9}\\displaystyle\\frac{H L_{\\delta}^{k}}{n_{h}^{k-1}(s,a)\\vee1},H\\right\\}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\begin{array}{r l r}{L_{\\delta}^{k}}&{{}=}&{\\ln{\\frac{144S^{2}A H^{2}k^{3}(k+1)}{\\delta}}}\\end{array}$ and fo rvity, we shorten $\\operatorname{Var}_{\\hat{P}_{h}^{k-1}(\\cdot|s,a)}(\\bar{V}_{h+1}^{k}(s^{\\prime}))$ 0 $\\operatorname{Var}_{\\hat{P}_{h}^{k-1}(\\cdot|s,a)}(\\bar{V}_{h+1}^{k})$ (omiting the state from the value). ", "page_idx": 19}, {"type": "text", "text": "For the optimistic value iteration, we use the notation $k_{h}^{t}(s)$ to represent the $t^{t h}$ episode where the state $s$ was visited at the $h^{t h}$ timestep. Thus, line 9 of Algorithm 3 is the expectation w.r.t. the empiricalreward disribution $\\hat{\\mathcal{R}}_{h}^{k-1}(s)^{\\bar{\\big}}$ (when defning is realization to be ero when $n_{h}^{k-1}(s)=0)$ Since the bonuses are larger than $H$ when $n_{h}^{k-1}(s)=0$ , one could write the update in more concisely as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\bar{V}_{h}^{k}(s)=\\operatorname*{min}\\biggl\\{\\mathbb{E}_{R\\sim\\hat{R}_{h}^{k-1}(s)}\\biggl[\\operatorname*{max}_{a\\in\\mathcal{A}}\\Bigl\\{R(a)+b_{k,h}^{p}(s,a)+\\hat{P}_{h}^{k-1}\\bar{V}_{h+1}^{k}(s,a)\\Bigr\\}\\biggr]+b_{k,h}^{r}(s),H\\biggr\\}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We will often use this representation in our analysis. ", "page_idx": 19}, {"type": "text", "text": "B.4  The First Good Event -- Concentration ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We now define the first good event, which ensures that all empirical quantities are well-concentrated. For the transitions, we require each element to concentrate well, as well as both the inner product and the variance w.r.t. the optimal value function. For the reward, we make sure that the maximum of the rewards to concentrate well (with any possible bias, that will later correspond with the next-state values). Formally, for any fixed vector $\\dot{u}\\in\\mathbb{R}^{A}$ ,denote ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{m_{h}(s,u)=\\mathbb{E}_{R\\sim\\mathcal{R}_{h}(s)}\\Big[\\underset{a}{\\operatorname*{max}}\\{R_{h}(a)+u(a)\\}\\Big],}\\\\ &{\\hat{m}_{h}^{k}(s,u)=\\mathbb{E}_{R\\sim\\hat{\\mathcal{R}}_{h}^{k}(s)}\\Big[\\underset{a}{\\operatorname*{max}}\\{R_{h}(a)+u(a)\\}\\Big]}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "with the convention that $\\hat{m}_{h}^{k}(s,u)=\\operatorname*{max}_{a}u(a)$ if $n_{h}^{k}(s)=0$ . We define the following good events: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Xi^{p}(k)=\\left\\{\\forall s,s^{\\prime},a,h:\\,|P_{h}(s^{\\prime}|s,a)-\\hat{P}_{h}^{k-1}(s^{\\prime}|s,a)|\\leq\\sqrt{\\frac{2P\\left(s^{\\prime}|s,a\\right)L_{\\delta}^{k}}{n_{h}^{k-1}(s,a)\\,\\forall\\,1}}+\\frac{L_{\\delta}^{k}}{n_{h}^{k-1}(s,a)\\,\\forall\\,1}\\right\\}}\\\\ &{\\Xi^{p v1}(k)=\\left\\{\\forall s,a,h:\\,\\left|\\left(\\hat{P}_{h}^{k-1}-P_{h}\\right)V_{h+1}^{*}(s,a)\\right|\\leq\\sqrt{\\frac{2\\mathrm{Var}_{P h}(\\cdot|s,a)(V_{h+1}^{*})L_{\\delta}^{k}}{n_{h}^{k-1}(s,a)\\,\\forall\\,1}}+\\frac{H L_{\\delta}^{k}}{n_{h}^{k-1}(s,a)\\,\\forall\\,1}\\right\\}}\\\\ &{\\Xi^{p v2}(k)=\\left\\{\\forall s,a,h:\\,\\left|\\sqrt{\\mathrm{Var}_{P h}(\\cdot|s,a)(V_{h+1}^{*})}-\\sqrt{\\mathrm{Var}_{P h}^{k-1}(\\cdot|s,a)(V_{h+1}^{*})}\\right|\\leq4H\\sqrt{\\frac{L_{\\delta}^{k}}{n_{h}^{k-1}(s,a)\\,\\forall\\,1}}\\right\\}}\\\\ &{\\Xi^{r}(k)=\\left\\{\\forall s,h,\\forall u\\in[0,2H]^{A}:\\,\\left|m_{h}(s,u)-\\hat{m}_{h}^{k-1}(s,u)\\right|\\leq3\\sqrt{\\frac{A L_{\\delta}^{k}}{2(n_{h}^{k-1}(s)\\,\\forall\\,1)}}\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where we again use $\\begin{array}{r}{L_{\\delta}^{k}=\\ln\\frac{144S^{2}A H^{2}k^{3}(k+1)}{\\delta}}\\end{array}$ Thenwes go ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{G}_{1}=\\bigcap_{k\\geq1}E^{r}(k)\\bigcap_{k\\geq1}E^{p}(k)\\bigcap_{k\\geq1}E^{p v1}(k)\\bigcap_{k\\geq1}E^{p v2}(k),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "for which, the following holds: ", "page_idx": 20}, {"type": "text", "text": "Lemma 5 (The First Good Event). The good event $\\mathbb{G}_{1}$ holds w.p. $\\mathrm{Pr}(\\mathbb{G}_{1})\\geq1-\\delta/2$ ", "page_idx": 20}, {"type": "text", "text": "Proof. The proof of the first three events uses standard concentration arguments (see, e.g., Efroni et al. 2021) and is stated for completeness. For any fixed $k\\ \\geq\\ 1,s,a,h$ and number of visits $n~\\in~[k]$ , we utilize Lemma 16 w.r.t. the transition kernel $P_{h}(\\cdot|s,a)$ , the value $V_{h+1}^{*}\\ \\in\\ [0,H]$ and probabity $\\begin{array}{r}{\\delta^{\\prime}=\\frac{\\delta}{8S A H k^{2}(k+1)}}\\end{array}$ before the game starts, given the number of visits, all samples are i.i.d., so standard concentration could be applied. By taking the union bound over all $n\\in[k]$ and slightly increasing the constants to ensure that $n\\;=\\;0$ trivially holds, we get that the events also hold for any number of visit $n_{h}^{k-1}(s,a)\\in\\{0\\ldots,k\\}$ , and taking another union bound over all $k\\geq1,s,a,h$ ensures that each of the events $\\cap_{k\\geq1}E^{p}(k),\\cap_{k\\geq1}E^{p v1}(k)$ and $\\cap_{k\\geq1}E^{p v2}(k)$ holds w,p a least $\\begin{array}{r}{1-\\frac{\\delta}{8}}\\end{array}$ ", "page_idx": 20}, {"type": "text", "text": "We now focus on bounding the probability of the event $\\cap_{k}E^{r}(k)$ . For any fixed $k,h$ and $s$ observe that the event trivially holds if $n_{h}^{k}=0$ then the event trivially holds, since for all $u\\in[0,2H]^{A}$ \uff0c $m_{h}(s,u)-\\hat{m}_{h}^{k-1}(s,u)\\big|=\\Big|\\mathbb{E}_{R\\sim\\mathcal{R}_{h}(s)}\\Big[\\!\\operatorname*{max}_{a}\\{R_{h}(s,a)+u(a)\\}\\Big]-\\operatorname*{max}_{a}\\{u(a)\\}\\Big|\\stackrel{(*)}{\\leq}1\\leq3\\sqrt{\\frac{A L_{\\delta}^{k}}{2}},$ where $(*)$ usesthe boudedness of the rewards in $[0,1]$ Next, rll th fr y d $n_{h}^{k-1}=n\\in[k]$ \uff0c the rewards samples at state $s$ and step $h$ are i.i.d. vectors on $[0,1]^{A}$ . Therefore, by Lemma 18, $\\operatorname*{Pr}\\left\\{n_{h}^{k-1}(s)=n,\\forall u\\in[0,2H]^{A}:\\ \\left|m_{h}(s,u)-\\hat{m}_{h}^{k-1}(s,u)\\right|>3\\sqrt{\\frac{A L_{\\delta}^{k}}{2(n_{h}^{k-1}(s)\\vee1)}}\\right\\}\\leq\\frac{\\delta}{8S A H k^{2}(s)}.$ +1) ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "Taking a union bound on all possible values of $n\\in[k]$ $s$ and $h$ ,weget ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\{E^{r}(k)\\}\\geq1-S A k\\cdot{\\frac{\\delta}{8S A H k^{2}(k+1)}}\\geq1-{\\frac{\\delta}{8k(k+1)}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "By summing over all $k\\geq1$ ,the event $\\cap_{k}E^{r}(k)$ holds with a probability of at least $1-\\delta/8$ Finally, taking the union bound with the other three events leads to the desired result of $\\operatorname*{Pr}(\\mathbb{G}_{1})\\geq1\\!-\\!\\delta/2$ .\u53e3 ", "page_idx": 20}, {"type": "text", "text": "B.5   Optimism of the Upper Confidence Value Functions ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this subsection, we prove that under the good event $\\mathbb{G}_{1}$ , the values $\\bar{V}^{k}$ that MVP-RL produces are optimistic. ", "page_idx": 21}, {"type": "text", "text": "Lemma 6 (Optimism). Under the first good event $\\mathbb{G}_{1}$ for all $k\\in[K]$ $h\\in[H]$ and $s\\in S_{;}$ it holds that $V_{h}^{*}(s)\\leq\\bar{V}_{h}^{k}(s)$ ", "page_idx": 21}, {"type": "text", "text": "Proof. The proof follows by backward induction on $H$ ; see that the claim trivially holds for $h=H{+}1$ where both values are defined to be zero. ", "page_idx": 21}, {"type": "text", "text": "Now assume by induction that for some $k\\,\\in\\,[K]$ and $\\textit{h}\\in[H]$ , the desired inequalities hold at timestep $h+1$ for all $s\\in S$ ; we will show that this implies that they also hold at timestep $h$ ", "page_idx": 21}, {"type": "text", "text": "At this point, we also assume w.l.o.g. that $\\bar{V}_{h}^{k}(s)<H$ , and in particular, the value is not truncated; otherwise, by the boundedness of the rewards, $V_{h}^{*}(s)\\leq H=\\bar{V}_{h}^{k}(s)$ . For similar reasons, we assume w.1.o.g. that $b_{k,h}^{p}(s,a)<H$ , so that it is also not truncated. ", "page_idx": 21}, {"type": "text", "text": "By the optimism of the value at step $h+1$ due to the induction hypothesis and the monotonicity of the bonus (Lemma 23), under the good event, we have for all $s\\in S$ and $a\\in A$ that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{P}_{h}^{k-1}\\widehat{V}_{h+1}^{k}(s,a)+\\widehat{b}_{s,h}^{k}(s,a)}\\\\ &{\\geq\\widehat{P}_{h}^{k-1}\\widehat{V}_{h+1}^{k}(s,a)+\\operatorname*{max}\\Bigg\\{\\frac{20}{3}\\sqrt{\\frac{\\mathsf{V a r}_{\\beta_{k}}(\\hat{V}_{h+1}^{k}(s,a)(\\hat{V}_{h+1}^{k})L_{\\beta}^{k}}{n_{h}^{k-1}(s,a)\\,\\nabla\\hat{V}_{h}^{k}}}\\,,\\frac{400}{9}\\frac{H L_{\\beta}^{k}}{n_{h}^{k-1}(s,a)\\,\\nabla\\hat{V}_{1}}\\Bigg\\}}\\\\ &{\\geq\\widehat{P}_{h}^{k-1}\\widehat{V}_{h+1}^{*}(s,a)+\\operatorname*{max}\\Bigg\\{\\frac{20}{3}\\sqrt{\\frac{\\mathsf{V a r}_{\\beta_{k}}(\\hat{V}_{h+1}^{k}(s,a)(\\hat{V}_{h+1}^{k})L_{\\beta}^{k}}{n_{h}^{k-1}(s,a)\\,\\nabla\\hat{V}_{1}}}\\,,\\frac{400}{9}\\frac{H L_{\\beta}^{k}}{n_{h}^{k-1}(s,a)\\,\\nabla\\hat{V}_{1}}\\Bigg\\}~,}\\\\ &{\\geq\\widehat{P}_{h}^{k-1}\\widehat{V}_{h+1}^{*}(s,a)+\\frac{10}{3}\\sqrt{\\frac{\\mathsf{V a r}_{\\beta_{k}}(\\hat{V}_{h+1}^{k}(s,a)(\\hat{V}_{h+1}^{k})L_{\\beta}^{k}}{n_{h}^{k-1}(s,a)\\,\\nabla\\hat{V}_{1}}}\\,+\\frac{200}{9}\\frac{H L_{\\beta}^{k}}{n_{h}^{k-1}(s,a)\\,\\nabla\\hat{V}_{1}}}\\\\ &{\\geq\\widehat{P}_{h}^{k-1}\\widehat{V}_{h+1}^{*}(s,a)+\\frac{10}{3}\\sqrt{\\frac{\\mathsf{V a r}_{\\beta_{k}}(\\hat{V}_{h+1}^{k}(s,a)(\\hat{V}_{h+1}^{k})L_{\\beta}^{k}}{n_{h}^{k-1}(s,a)\\,\\nabla\\hat{V}_{1 \n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Thus, under the good event and the induction hypothesis, we have that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{V}_{h}^{k}(s)=\\mathbb{E}_{R\\sim\\hat{\\mathcal{R}}_{h}(s)}\\bigg[\\underset{a\\in\\mathcal{A}}{\\operatorname*{max}}\\Big\\lbrace R(a)+b_{k,h}^{p}(s,a)+\\hat{P}_{h}^{k-1}\\bar{V}_{h+1}^{k}(s,a)\\Big\\rbrace\\bigg]+b_{k,h}^{r}(s)}\\\\ &{\\qquad\\quad\\geq\\mathbb{E}_{R\\sim\\hat{\\mathcal{R}}_{h}(s)}\\bigg[\\underset{a\\in\\mathcal{A}}{\\operatorname*{max}}\\big\\lbrace R(a)+P_{h}V_{h+1}^{*}(s,a)\\big\\rbrace\\bigg]+b_{k,h}^{r}(s).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "In particular, using Proposition 1, we get ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{V}_{h}^{k}(s)-V_{h}^{*}(s)\\geq\\mathbb{E}_{R\\sim\\hat{\\mathcal{R}}_{h}(s)}\\bigg[\\underset{a\\in\\mathcal{A}}{\\operatorname*{max}}\\big\\lbrace R(a)+P_{h}V_{h+1}^{*}(s,a)\\big\\rbrace\\bigg]+b_{k,h}^{r}(s)}\\\\ &{\\qquad\\qquad\\qquad-\\mathbb{E}_{R\\sim\\mathcal{R}_{h}(s)}\\bigg[\\underset{a\\in\\mathcal{A}}{\\operatorname*{max}}\\big\\lbrace R(a)+P_{h}V_{h+1}^{*}(s,a)\\big\\rbrace\\bigg]}\\\\ &{\\qquad\\qquad\\qquad\\geq0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the last inequality holds under the event $E^{r}(k)$ with $u(a)=P_{h}V_{h+1}^{*}(s,a)\\in[0,H]^{A}$ ", "page_idx": 21}, {"type": "text", "text": "B.6  The Second Good Event - Martingale Concentration ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this subsection, we present four good events that will allow us to replace the expectation over the randomizations inside each episode with their realization. ", "page_idx": 22}, {"type": "text", "text": "Define the following bonus-like term that will later appear in the proof due to value concentration: ", "page_idx": 22}, {"type": "equation", "text": "$$\nb_{k,h}^{p v1}(s,a)=\\operatorname*{min}\\Biggl\\{\\sqrt{\\frac{2\\mathrm{Var}_{P_{h}(\\cdot\\vert s,a)}(V_{h+1}^{*})L_{\\delta}^{k}}{n_{h}^{k-1}(s,a)\\vee1}}+\\frac{4H^{2}S L_{\\delta}^{k}}{n_{h}^{k-1}(s,a)\\vee1},H\\Biggr\\},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and let ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Y_{1,h}^{k}:=\\bar{V}_{h+1}^{k}(s_{h+1}^{k})-V_{h+1}^{\\pi^{k}}(s_{h+1}^{k}),}\\\\ &{Y_{2,h}^{k}=\\operatorname{Var}_{P_{h}(\\cdot\\vert s_{t,h},a_{t,h})}(V_{h+1}^{\\pi^{k}}),}\\\\ &{Y_{3,h}^{k}=b_{k,h}^{p}(s_{h}^{k},a_{h}^{k})+b_{k,h}^{p v1}(s_{h}^{k},a_{h}^{k}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The second good event is the intersection of the events $\\mathbb{G}_{2}=E^{\\mathrm{diff1}}\\cap E^{\\mathrm{diff2}}\\cap E^{\\mathrm{Var}}\\cap E^{b p}$ defined as follows. ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathfrak{T}^{\\mathrm{diff1}}=\\Bigg\\{\\forall h\\in[H],K\\ge1:\\displaystyle\\sum_{k=1}^{K}\\mathbb{E}[Y_{1,h}^{k}|F_{k,h-1}]\\le\\left(1+\\displaystyle\\frac{1}{2H}\\right)\\displaystyle\\sum_{k=1}^{K}Y_{1,h}^{k}+18H^{2}\\ln\\frac{8H K(K+1)}{\\delta}}\\\\ {\\displaystyle\\mathfrak{T}^{\\mathrm{diff2}}=\\Bigg\\{\\forall h\\in[H],K\\ge1:\\displaystyle\\sum_{k=1}^{K}\\mathbb{E}[Y_{1,h}^{k}|F_{k,h-1}^{R}]\\le\\left(1+\\displaystyle\\frac{1}{2H}\\right)\\displaystyle\\sum_{k=1}^{K}Y_{1,h}^{k}+18H^{2}\\ln\\frac{8H K(K+1)}{\\delta}}\\\\ {\\displaystyle\\mathfrak{T}^{\\mathrm{Var}}=\\Bigg\\{K\\ge1:\\displaystyle\\sum_{k=1}^{K}\\sum_{h=1}^{H}Y_{2,h}^{k}\\le2\\displaystyle\\sum_{k=1}^{K}\\displaystyle\\sum_{h=1}^{H}\\mathbb{E}[Y_{2,h}^{k}|F_{k-1}]+4H^{3}\\ln\\frac{8H K(K+1)}{\\delta}\\Bigg\\},}\\\\ {\\displaystyle\\mathfrak{T}^{b p}=\\Bigg\\{\\forall h\\in[H],K\\ge1:\\displaystyle\\sum_{k=1}^{K}\\mathbb{E}[Y_{3,h}^{k}|F_{k,h-1}]\\le2\\displaystyle\\sum_{k=1}^{K}Y_{3,h}^{k}+50H^{2}\\ln\\frac{8H K(K+1)}{\\delta}\\Bigg\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We define the good event $\\mathbb{G}=\\mathbb{G}_{1}\\cap\\mathbb{G}_{2}$ ", "page_idx": 22}, {"type": "text", "text": "Lemma 7. The good event $\\mathbb{G}$ holds with a probability of at least $1-\\delta$ ", "page_idx": 22}, {"type": "text", "text": "Proof. The proof follows similarly to Lemmas 15 and 21 of [Efroni et al., 2021]. ", "page_idx": 22}, {"type": "text", "text": "First, define the random process $W_{k}=\\mathbb{1}\\Big\\{\\bar{V}_{h}^{k}(s)-{V_{h}^{\\pi^{k}}(s)}\\in[0,H],\\forall h\\in[H],s\\in S\\Big\\}$ and define $\\tilde{Y}_{1,h}^{k}=W_{k}Y_{1,h}^{k}$ Wwhichisbounded in $[0,H]$ Alsobservethat $W_{k}$ .s1 $F_{k-1}$ measurable,sine oth values and policies are calculated based on data up to the episode $k-1$ , and in particular, it is measurable and $\\tilde{Y}_{1,h}^{k}$ .ss $F_{k,h}$ measurable. thus,by Lemma 25, for any $k\\in[K]$ and $h\\in[H]$ we have w.p a least $\\begin{array}{r}{1-\\frac{\\delta}{8H K(K+1)}}\\end{array}$ that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{K}\\mathbb{E}[\\tilde{Y}_{1,h}^{k}|F_{k,h-1}]\\leq\\left(1+\\frac{1}{2H}\\right)\\sum_{k=1}^{K}\\tilde{Y}_{1,h}^{k}+18H^{2}\\ln\\frac{8H K(K+1)}{\\delta}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Since $W_{k}$ is $F_{k,h-1}$ measurable, we can write the event as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{K}W_{k}\\mathbb{E}[Y_{1,h}^{k}|F_{k,h-1}]\\leq\\left(1+\\frac{1}{2H}\\right)\\sum_{k=1}^{K}W_{k}Y_{1,h}^{k}+18H^{2}\\ln\\frac{8H K(K+1)}{\\delta},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and taking the union bound over all $h\\in[H]$ and $K\\geq1$ , we get w.p. at least $\\begin{array}{r}{1-\\frac{\\delta}{8}}\\end{array}$ that the event ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\tilde{\\varsigma}^{\\mathrm{diff1}}=\\left\\{\\forall h\\in[H],K\\geq1:\\sum_{k=1}^{K}W_{k}\\mathbb{E}[Y_{1,h}^{k}|F_{k,h-1}]\\leq\\left(1+\\frac{1}{2H}\\right)\\sum_{k=1}^{K}W_{k}Y_{1,h}^{k}+18H^{2}\\ln\\frac{8H K(M)}{\\delta}\\right\\}\\sum_{k=1}^{K}W_{k}Y_{1,h}^{k},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Importantly, by optimism (Lemma 6), under $\\mathbb{G}_{1}$ , it holds that $W_{k}~=~1$ for all $k\\ \\geq\\ 1$ \uff0csowe immediately get that $\\mathbb{G}_{1}\\cap\\tilde{E}^{\\mathrm{diff1}}=\\mathbb{G}_{1}\\cap E^{\\mathrm{diff1}}$ ", "page_idx": 22}, {"type": "text", "text": "Following theact sam profjust with thlaton $F_{k,h}^{R}$ and defining the equivalent $\\tilde{E}^{\\mathrm{diff2}}$ we get that this event also holds w.p. $\\begin{array}{r}{1-\\frac{\\delta}{8}}\\end{array}$ and is the desired event when $\\mathbb{G}_{1}$ holds. ", "page_idx": 23}, {"type": "text", "text": "Next, we prove that the other two events also hold w.p. at least $\\begin{array}{r}{1-\\frac{\\delta}{8}}\\end{array}$ ", "page_idx": 23}, {"type": "text", "text": "By the assumptions of our setting, we know that $V_{h}^{\\pi^{k}}(s)\\in[0,H]$ and so ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sum_{h=1}^{H}Y_{2,h}^{k}=\\sum_{h=1}^{H}\\operatorname{Var}_{P_{h}(\\cdot|s_{t,h},a_{t,h})}(V_{h+1}^{\\pi^{k}})\\in[0,H^{3}].\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "In particular, applying Lemma 25 (w.r.t. the fltration $F_{k}$ )with $C=H^{3}$ and any fixed $K$ ,we get w.p. 8HK(K+1) that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{K}\\sum_{h=1}^{H}Y_{2,h}^{k}\\leq2\\sum_{k=1}^{K}\\sum_{h=1}^{H}\\mathbb{E}[Y_{2,h}^{k}|F_{k-1}]+4H^{3}\\ln\\frac{8H K(K+1)}{\\delta}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Takingthe union bound onallposible vlues of $K\\geq1$ proves that $E^{\\mathrm{Var}}$ holds wp a least $\\begin{array}{r}{1-\\frac{\\delta}{8}}\\end{array}$ Similaly,by defition, we havethat $Y_{3,h}^{k}\\,=\\,b_{k,h}^{p}(s_{h}^{k},a_{h}^{k})\\,+\\,b_{k,h}^{p v1}(s_{h}^{k},a_{h}^{k})\\,\\in\\,[0,2H]$ and is $F_{k,h}$ measurable. Thus, for any fixed $k\\geq1$ and $h\\in[H]$ ,using Lemma25, we have w.p. $\\begin{array}{r}{1-\\frac{\\delta}{8H K(K+1)}}\\end{array}$ that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k=1}^{K}\\mathbb{E}[Y_{3,h}^{k}|F_{k,h-1}]\\leq\\bigg(1+\\frac{1}{4H}\\bigg)\\sum_{k=1}^{K}Y_{3,h}^{k}+50H^{2}\\ln\\frac{8H K(K+1)}{\\delta}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq2\\displaystyle\\sum_{k=1}^{K}Y_{3,h}^{k}+50H^{2}\\ln\\frac{8H K(K+1)}{\\delta},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "applying the union bound on all $K\\geq1$ , the event $E^{b p}$ holds w.p. $\\begin{array}{r}{1-\\frac{\\delta}{8}}\\end{array}$ ", "page_idx": 23}, {"type": "text", "text": "To summarize, we have that the event $\\mathbb{G}_{1}$ holds w.p. $\\textstyle1-{\\frac{\\delta}{2}}$ (Lemma 5), and we proved that the events Ediff1, Edif2, EVar, Ebp hold each w.p. 1 -- $\\begin{array}{r}{1-\\frac{\\delta}{8}}\\end{array}$ , so we also have that the event ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{G}=\\mathbb{G}_{1}\\cap\\mathbb{G}_{2}}\\\\ &{\\quad=\\mathbb{G}_{1}\\cap E^{\\mathrm{diff}1}\\cap E^{\\mathrm{diff}2}\\cap E^{\\mathrm{Var}}\\cap E^{b p}}\\\\ &{\\quad=\\mathbb{G}_{1}\\cap\\tilde{E}^{\\mathrm{diff}1}\\cap\\tilde{E}^{\\mathrm{diff}2}\\cap E^{\\mathrm{Var}}\\cap E^{b p}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "holds w.p. at least $1-\\delta$ ", "page_idx": 23}, {"type": "text", "text": "B.7 Regret Analysis ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We finally analyze the regret of the algorithm ", "page_idx": 24}, {"type": "text", "text": "Theorem 1. When running MVP-RL, with probability at least $1-\\delta$ uniformlyforall $K\\geq1$ it holds that $\\begin{array}{r}{\\mathrm{Reg}^{R}(K)\\leq\\mathcal{O}\\Big(\\sqrt{H^{3}S A K}\\ln\\frac{S A H K}{\\delta}+H^{3}S^{2}A\\big(\\ln\\frac{S A H K}{\\delta}\\big)^{2}\\Big).}\\end{array}$ ", "page_idx": 24}, {"type": "text", "text": "Proof. Assume that the good events $\\mathbb{G}$ holds, which by Lemma 7, happens with probability at least $1-\\delta$ Then, by optimism (Lemma 6), for any $k\\,\\in\\,[K],\\,h\\,\\in\\,[H]$ and $s\\,\\in\\,S$ , it holds that $V_{h}^{*}(s)\\leq\\bar{V}_{h}^{k}(s)$ . Moreover, we can lower bound the value of the policy $\\pi^{k}$ as follows (see Remark 1): ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{V_{k}^{n+}(s)=\\mathbb{E}_{R-\\mathcal{R}_{k}(s)}\\bigg[R(\\pi_{k}^{n}(s,R))+P_{k}V_{n+1}^{n+}(s,\\pi_{k}^{n}(s,R))\\bigg]}&{}\\\\ {=\\mathbb{E}_{R-\\mathcal{R}_{k}(s)}\\bigg[R(\\pi_{k}^{n}(s,R))+\\hat{P}_{k}^{k-1}\\hat{V}_{n+1}^{k}(s,\\pi_{k}^{n}(s,R))+P_{k,k}^{n}(s,\\pi_{k}^{n}(s,R))\\bigg]}\\\\ &{\\quad+\\mathbb{E}_{R-\\mathcal{R}_{k}(s)}\\bigg[P_{k}V_{k+1}^{n+}(s,\\pi_{k}^{n}(s,R))-\\hat{P}_{k}^{k-1}V_{k+1}^{k}(s,\\pi_{k}^{n}(s,R))-\\hat{P}_{k,k}^{n}(s,\\pi_{k}^{n}(s,R))\\bigg]}\\\\ {\\overset{(i)}{=}\\mathbb{E}_{R-\\mathcal{R}_{k}(s)}\\bigg[\\operatorname*{max}_{\\ell}\\bigg\\{R(a)+\\hat{P}_{k}^{k-1}\\hat{V}_{k+1}^{k}(s,a)+b_{k,\\ell}^{n}(s,a)\\bigg\\}\\bigg]}\\\\ &{\\quad+\\mathbb{E}_{R-\\mathcal{R}_{k}(s)}\\bigg[\\operatorname*{max}_{\\ell}\\bigg\\{R^{n}(s,R)-\\hat{P}_{k}^{k-1}\\hat{V}_{k+1}^{k}(s,\\pi_{k}^{n}(s,R))-b_{k,\\ell}^{n}(s,\\pi_{k}^{n}(s,R))\\bigg\\}\\bigg]}\\\\ {\\overset{(i)}{\\geq}\\mathbb{E}_{R-\\mathcal{R}_{k}(s)}\\bigg[\\operatorname*{max}_{\\ell}\\bigg\\{R(a)+\\hat{P}_{k}^{k-1}\\hat{V}_{k+1}^{k}(s,a)+b_{k,\\ell}^{n}(s,\\pi_{k}^{n}(s,R))-b_{k,\\ell}^{n}(s)}\\\\ &{\\quad+\\mathbb{E}_{R-\\mathcal{R}_{k}(s)}\\bigg[P_{k}V_{k+1}^{n+}(s,\\pi_{k}^{n}(s,R))-\\hat{P}_{k}^{k-1}V_{k+1}^{k}(s,\\pi_{k}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Relation (1) is by the definition of $\\pi^{k}$ (see Algorithm 3), while (2) holds under the good event $E^{r}(k)$ $u(a)=\\hat{P}_{h}^{k-1}\\bar{V}_{h+1}^{k}(s,a)+b_{k,h}^{p}(s,a)\\in[0,2H]$ (due t the valueandboustruncation)Finaly (3) is by the definition of $\\bar{V}_{h}^{k}(s)$ , where the inequality also accounts for its possible truncation. ", "page_idx": 24}, {"type": "text", "text": "To further bound this, we need to bound ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{5k-1}{h}\\bar{V}_{h+1}^{k}(s,a)-P_{h}V_{h+1}^{\\pi^{k}}(s,a)=P_{h}\\Big(\\bar{V}_{h+1}^{k}-V_{h+1}^{\\pi^{k}}\\Big)(s,a)+\\Big(\\hat{P}_{h}^{k-1}-P_{h}\\Big)\\bar{V}_{h+1}^{k}(s,a)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=P_{h}\\Big(\\bar{V}_{h+1}^{k}-V_{h+1}^{\\pi^{k}}\\Big)(s,a)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad+\\left(\\hat{P}_{h}^{k-1}-P_{h}\\right)V_{h+1}^{*}(s,a)+\\Big(\\hat{P}_{h}^{k-1}-P_{h}\\Big)\\big(\\bar{V}_{h+1}^{k}-V_{h+1}^{*}\\big)(s,a)}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The first error term can be bounded under the good event, while the second using Lemma 24. More formally, under the good event $E^{p v1}(k)$ ,wehave ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left|\\left(\\hat{P}_{h}^{k-1}-P_{h}\\right)V_{h+1}^{*}(s,a)\\right|\\leq\\sqrt{\\frac{2\\mathrm{Var}_{P_{h}(\\cdot\\vert s,a)}(V_{h+1}^{*})L_{\\delta}^{k}}{n_{h}^{k-1}(s,a)\\vee1}}+\\frac{H L_{\\delta}^{k}}{n_{h}^{k-1}(s,a)\\vee1},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and by Lemma 24 with $\\alpha=4H$ (using and $P_{1}=P_{h}$ \uff0c $P_{2}=\\hat{P}_{h}^{k-1}$ , under $E^{p}(k),$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\hat{P}_{h}^{k-1}-P_{h}\\right)\\left(\\bar{V}_{h+1}^{k}-V_{h+1}^{*}\\right)(s,a)\\Big|\\le\\displaystyle\\frac{1}{4H}\\mathbb{E}_{P_{h}(\\cdot\\vert s,a)}\\big[\\bar{V}_{h+1}^{k}(s^{\\prime})-V_{h+1}^{*}(s^{\\prime})\\big]+\\frac{H S L_{\\delta}^{k}(1+4H\\cdot2/4)}{n_{h}^{k-1}(s,a)\\vee1}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\le\\displaystyle\\frac{1}{4H}\\mathbb{E}_{P_{h}(\\cdot\\vert s,a)}\\Big[\\bar{V}_{h+1}^{k}(s^{\\prime})-V_{h+1}^{\\pi^{k}}(s^{\\prime})\\Big]+\\frac{3H^{2}S L_{\\delta}^{k}}{n_{h}^{k-1}(s,a)\\vee1}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\frac{1}{4H}P_{h}\\Big(\\bar{V}_{h+1}^{k}-V_{h+1}^{\\pi^{k}}\\Big)(s,a)+\\frac{3H^{2}S L_{\\delta}^{k}}{n_{h}^{k-1}(s,a)\\vee1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the second inequality is since the value of $\\pi^{k}$ cannot exceed the optimal value. ", "page_idx": 25}, {"type": "text", "text": "Since under the good event by Lemma 6, we have $0\\leq V_{h+1}^{\\pi^{k}}(s^{\\prime})\\leq V_{h+1}^{*}(s^{\\prime})\\leq\\bar{V}_{h+1}^{k}(s^{\\prime})\\leq H$ we can trivially bound the error by $H$ and bound ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{\\gamma}_{h}^{k-1}\\overline{{V}}_{h+1}^{k}(s,a)-P_{h}V_{h+1}^{\\pi^{k}}(s,a)}\\\\ &{\\leq\\operatorname*{min}\\left\\{\\left(1+\\frac{1}{4H}\\right)\\underbrace{P_{h}\\Big(\\bar{V}_{h+1}^{k}-V_{h+1}^{\\pi^{k}}\\Big)(s,a)}_{\\geq0}+\\frac{3H^{2}S L_{\\delta}^{k}}{n_{h}^{k-1}(s,a)\\vee1}+\\sqrt{\\frac{2\\mathrm{Var}_{P_{h}(\\cdot\\vert s,a)}(V_{h+1}^{*})L_{\\delta}^{k}}{n_{h}^{k-1}(s,a)\\vee1}}+\\frac{1}{n_{h}^{k-1}(s,a)\\vee1}\\right.}\\\\ &{\\leq\\left(1+\\frac{1}{4H}\\right)P_{h}\\Big(\\bar{V}_{h+1}^{k}-V_{h+1}^{\\pi^{k}}\\Big)(s,a)+\\operatorname*{min}\\left\\{\\sqrt{\\frac{2\\mathrm{Var}_{P_{h}(\\cdot\\vert s,a)}(V_{h+1}^{*})L_{\\delta}^{k}}{n_{h}^{k-1}(s,a)\\vee1}}+\\frac{4H^{2}S L_{\\delta}^{k}}{n_{h}^{k-1}(s,a)\\vee1},H\\right\\}}\\\\ &{\\triangleq\\left(1+\\frac{1}{4H}\\right)P_{h}\\Big(\\bar{V}_{h+1}^{k}-V_{h+1}^{\\pi^{k}}\\Big)(s,a)+b_{k,h}^{p\\pi^{1}}(s,a).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Substituting back to Equation (6) while writing the linear operation $P_{h}V(s,a)$ as an expectation and letting the action be $a_{h}=\\pi_{h}^{k}(s,R)$ , we get under $\\mathbb{G}$ for all $k\\in[K]$ $h\\in[H]$ and $s\\in S$ that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{\\gamma}_{h}^{k}(s)-{V_{h}^{\\pi^{k}}(s)}}\\\\ &{\\leq\\mathbb{E}_{R\\sim\\mathcal{R}_{h}(s)}\\Big[\\hat{P}_{h}^{k-1}\\bar{V}_{h+1}^{k}(s,\\pi_{h}^{k}(s,R))-P_{h}{V_{h+1}^{\\pi^{k}}(s,\\pi_{h}^{k}(s,R))}+b_{k,h}^{p}(s,\\pi_{h}^{k}(s,R))\\Big]+2b_{k,h}^{r}(s)}\\\\ &{\\leq\\mathbb{E}_{R\\sim\\mathcal{R}_{h}(s)}\\Big[\\bigg(1+\\displaystyle\\frac{1}{4H}\\bigg)\\mathbb{E}\\Big[\\bar{V}_{h+1}^{k}(s_{h+1})-{V_{h+1}^{\\pi^{k}}(s_{h+1})}|s_{h}=s,a_{h}\\Big](s,a)+b_{k,h}^{p\\nu1}(s,a_{h})+b_{k,h}^{p}(s,a_{h+1})\\Big]}\\\\ &{=\\mathbb{E}\\bigg[\\bigg(1+\\displaystyle\\frac{1}{4H}\\bigg)\\Big(\\bar{V}_{h+1}^{k}(s_{h+1})-{V_{h+1}^{\\pi^{k}}(s_{h+1})}\\Big)+b_{k,h}^{p}(s_{h},a_{h})+b_{k,h}^{p\\nu1}(s_{h},a_{h})|s_{h}=s,\\pi^{k}\\bigg]+2b_{k,h}^{r}(s)}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Next, taking $s=s_{h}^{k}$ the action $a_{h}=\\pi_{h}^{k}(s,R)$ becomes $a_{h}^{k}$ and summing on all $k$ we can rewrite ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\underset{k=1}{\\overset{k}{\\sum}}\\Bigg[\\bigg(1+\\frac{1}{\\mathcal{M}}\\bigg)\\bigg(b_{++}^{k}(s_{k+1}^{k})-b_{-+}^{k+}(s_{k+1}^{k})+b_{-+}^{k}(s_{k+1}^{k})+b_{-+}^{k+}(s_{k+1}^{k})+b_{-+}^{k}(s_{k+1}^{k})\\bigg)\\mathbb{I}_{\\{k,k-1\\}}\\Bigg]+\\underset{k=1}{\\overset{k}{\\sum}}\\Bigg[\\bigg(1+\\frac{1}{\\mathcal{M}}\\bigg)\\bigg]\\Bigg[\\bigg(1+\\frac{1}{\\mathcal{M}}\\bigg)\\bigg]\\Bigg[\\frac{b_{-}^{k}}{\\sum_{i=1}^{k}\\big(s_{k+1}^{k}\\big)}-b_{-+}^{k+}(s_{k+1}^{k})\\bigg)+b_{-+}^{k}(s_{k+1}^{k})\\Bigg],}\\\\ &{\\overset{(a)}{\\leq}\\bigg(1+\\frac{1}{\\mathcal{M}}\\bigg)\\bigg(1+\\frac{1}{\\mathcal{M}}\\bigg)\\underset{k=1}{\\overset{k}{\\sum}}\\Bigg[\\bigg(b_{++}^{k}(s_{k+1}^{k})-b_{-+}^{k+}(s_{k+1}^{k})\\bigg)}\\\\ &{\\quad+\\frac{2}{\\mathcal{M}}\\bigg\\langle b_{+}^{k}(s_{k+1}^{k})\\bigg|\\mathcal{S}_{-,k}^{k}(s_{k+1}^{k})+b_{-+}^{k}(s_{k+1}^{k})+\\bigg.0\\bigg]\\bigg.\\bigg.\\times}\\\\ &{\\overset{(b)}{\\leq}\\bigg(1+\\frac{1}{\\mathcal{M}}\\bigg)\\bigg(1+\\frac{1}{\\mathcal{M}}\\bigg)\\underset{k=1}{\\overset{k}{\\sum}}\\Bigg[\\bigg(b_{++}^{k}(s_{k+1}^{k})-b_{-+}^{k+}(s_{k+1}^{k})+\\frac{1}{\\mathcal{M}}\\bigg)+\\frac{1}{4\\mathcal{M}}\\bigg(1+\\frac{1}{\\mathcal{M}}\\bigg)\\bigg]\\Bigg[\\bigg(1+\\frac{1}{\\mathcal{M}}\\bigg)\\bigg]\\Bigg[\\bigg(b_{-+}^{k}(s_{k+1}^{k})-\\frac{1}{\\mathcal{M}}\\bigg)}\\\\ &{\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Wwhere inequality (1) holds when both $E^{\\mathrm{dif1}}$ and $E^{b p}$ occur and inequality (2) is by Lemma 8. In the last inequality, we also substituted the definition of the reward bonus. Recursively applying this ", "page_idx": 25}, {"type": "text", "text": "inequality up to $h=H+1$ (where both values are zero), w.p. at least $1-\\delta$ ,weget ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\log^{R}(K)\\leq\\displaystyle\\sum_{k=1}^{K}\\left(V_{1}^{*}(s_{1}^{k})-V_{1}^{*}(s_{1}^{k})\\right)}\\\\ &{\\qquad\\leq\\displaystyle\\sum_{k=1}^{K}\\left(\\hat{V}_{1}^{k}(s_{1}^{k})-V_{1}^{*}(s_{1}^{k})\\right)}\\\\ &{\\qquad\\leq\\mathrm{i}\\,\\mathrm{Be}\\bigg(1+\\frac{1}{2H}\\bigg)^{2H}\\displaystyle\\sum_{k=1}^{K}\\frac{\\sqrt{L_{\\delta}^{k}\\mathrm{War}_{\\delta(k)}(s_{\\star}^{k})(V_{\\star1}^{k+1})}}{\\sqrt{N_{h}^{k-1}(s_{h}^{k},a_{k}^{k})}\\,\\mathrm{Vi}}+\\left(1+\\frac{1}{2H}\\right)^{2H}\\displaystyle\\sum_{k=1}^{K}\\frac{12\\mathrm{W}\\bar{U}^{2H}S_{2}^{k}k}{n_{h}^{k-1}(s_{h}^{k},a_{k}^{k})\\,\\mathrm{Vi}}}\\\\ &{\\qquad\\qquad+\\theta\\Big(1+\\frac{1}{2H}\\Big)^{2H}\\displaystyle\\sum_{k=1}^{K}\\sqrt{\\frac{L_{\\delta}^{k}}{2n_{h}^{k-1}(s_{1}^{k})}}}\\\\ &{\\qquad\\overset{(c)}{\\geq}\\displaystyle\\operatorname*{lims}\\sqrt{\\mu^{3}\\mathrm{AKL}_{{\\delta}}^{\\delta}+507\\mathrm{S}\\bar{L}\\mu^{2}(L_{\\delta}^{k})^{1.5}}}\\\\ &{\\qquad\\qquad+500H^{2}S_{2}^{k}\\,\\mathrm{S}\\,\\mathrm{A}H(2+\\ln(K))+12\\sqrt{A L_{\\delta}^{k}}\\Big(S H+2\\sqrt{S H^{2}K}\\Big)}\\\\ &{\\qquad=O\\Big(\\sqrt{H^{3}\\mathrm{AKL}_{{\\delta}}^{K}}+H^{3}S^{2}A(L_{{\\delta}}^{K})^{2}\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Relation $(*)$ is by Lemma 9 and Lemma 20. ", "page_idx": 26}, {"type": "text", "text": "B.7.1  Lemmas for Bounding Bonus Terms ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Lemma 8. Conditioned on the good event $\\mathbb{G}$ for any $h\\in[H]$ ,it holds that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{=1}^{K}\\Bigl(b_{k,h}^{p}(s_{h}^{k},a_{h}^{k})+b_{k,h}^{p v1}(s_{h}^{k},a_{h}^{k})\\Bigr)\\le\\frac{1}{8H}\\biggl(1+\\frac{1}{2H}\\biggr)\\sum_{k=1}^{K}\\Bigl(\\bar{V}_{h+1}^{k}(s_{h+1}^{k})-V_{h+1}^{\\pi^{k}}(s_{h+1}^{k})\\Bigr)}}\\\\ &{}&{\\quad\\quad+\\,9\\sum_{k=1}^{K}\\sqrt{\\frac{\\mathrm{Var}_{P_{h}(\\cdot\\vert s_{h}^{k},a_{h}^{k})}(V_{h+1}^{\\pi^{k}})L_{\\delta}^{k}}{n_{h}^{k-1}(s_{h}^{k},a_{h}^{k})\\vee1}}+\\sum_{k=1}^{K}\\frac{810H^{2}S L_{\\delta}^{k}}{n_{h}^{k-1}(s_{h}^{k},a_{h}^{k})\\vee1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Prof.e start baalyin achf te mssearatlrst weapplymawi $\\begin{array}{r}{\\alpha=\\frac{20}{3}}\\end{array}$ $32H L_{\\delta}^{k}$ , noting that under the good event (by Lemma 6), $0\\leq V_{h+1}^{\\pi^{k}}(s)\\leq V_{h+1}^{*}(s)\\leq\\bar{V}_{h+1}^{k}(s)\\leq H$ and using the event $E^{p v}$ ; doing so yields ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\frac{p}{k,h}(s,a)\\leq\\displaystyle\\frac{20}{3}\\sqrt{\\frac{\\mathrm{Var}_{\\hat{P}_{h}^{k-1}(\\cdot\\vert s,a)}(\\bar{V}_{h+1}^{k})L_{\\delta}^{k}}{n_{h}^{k-1}(s,a)\\vee1}}+\\displaystyle\\frac{400}{9}\\frac{H L_{\\delta}^{k}}{n_{h}^{k-1}(s,a)\\vee1}}\\\\ {\\leq\\displaystyle\\frac{20\\sqrt{L_{\\delta}^{k}\\mathrm{Var}_{P_{h}(\\cdot\\vert s,a)}(V_{h+1}^{\\pi^{k}})}}{3\\sqrt{n_{h}^{k-1}(s,a)\\vee1}}+\\displaystyle\\frac{1}{32H}P_{h}\\Big(\\bar{V}_{h+1}^{k}-V_{h+1}^{\\pi^{k}}\\Big)(s,a)+\\displaystyle\\frac{1}{32H}\\hat{P}_{h}^{k-1}\\Big(\\bar{V}_{h+1}^{k}-V_{h+1}^{\\pi^{k}}\\Big)(s,a)}\\\\ {\\quad+\\displaystyle\\frac{6400H^{2}L_{\\delta}^{k}}{9n_{h}^{k-1}(s,a)\\vee1}+\\displaystyle\\frac{20}{3}\\displaystyle\\frac{4H L_{\\delta}^{k}}{n_{h}^{k-1}(s,a)\\vee1}+\\displaystyle\\frac{400}{9}\\displaystyle\\frac{H L_{\\delta}^{k}}{n_{h}^{k-1}(s,a)\\vee1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Using Lemma 24 with $\\alpha=1$ , under the good event $E^{p}(k)$ and for any $s,a$ we can further bound ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{P}_{h}^{k-1}\\Big(\\Bar{V}_{h+1}^{k}-V_{h+1}^{\\pi^{k}}\\Big)(s,a)}\\\\ &{\\quad=P_{h}\\Big(\\Bar{V}_{h+1}^{k}-V_{h+1}^{\\pi^{k}}\\Big)(s,a)+\\Big(\\hat{P}_{h}^{k-1}-P_{h}\\Big)\\Big(\\Bar{V}_{h+1}^{k}(s^{\\prime})-V_{h+1}^{\\pi^{k}}\\Big)(s,a)}\\\\ &{\\quad\\le P_{h}\\Big(\\Bar{V}_{h+1}^{k}-V_{h+1}^{\\pi^{k}}\\Big)(s,a)+P_{h}\\Big(\\Bar{V}_{h+1}^{k}-V_{h+1}^{\\pi^{k}}\\Big)(s,a)+\\displaystyle{\\frac{H S L_{\\delta}^{k}(1+2\\cdot1/4)}{n_{h}^{k-1}(s,a)\\vee1}}\\quad(\\mathrm{Lemma~}24)}\\\\ &{\\quad\\le2P_{h}\\Big(\\Bar{V}_{h+1}^{k}-V_{h+1}^{\\pi^{k}}\\Big)(s,a)+\\displaystyle{\\frac{1.5H S L_{\\delta}^{k}}{n_{h}^{k-1}(s,a)\\vee1}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Thus, we get the overall bound ", "page_idx": 27}, {"type": "equation", "text": "$$\nb_{k,h}^{p}(s,a)\\leq\\frac{20\\sqrt{L_{\\delta}^{k}\\mathrm{Var}_{P_{h}(\\cdot\\vert s,a)}(V_{h+1}^{\\pi^{k}})}}{3\\sqrt{n_{h}^{k-1}(s,a)\\vee1}}+\\frac{3}{32H}P_{h}\\Big(\\bar{V}_{h+1}^{k}-V_{h+1}^{\\pi^{k}}\\Big)(s,a)+\\frac{785H^{2}S L_{\\delta}^{k}}{n_{h}^{k-1}(s,a)\\vee1}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "For the second bonus, we apply Lemma 21 w.r.t. $V_{h+1}^{\\pi^{k}}(s)\\leq V_{h+1}^{*}(s)$ and $\\alpha=32\\sqrt{2L_{\\delta}^{k}}H$ and get ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{p v1}{k,h}(s,a)\\leq\\sqrt{\\frac{2\\mathrm{Var}_{P_{h}(\\cdot\\vert s,a)}(V_{h+1}^{*})L_{\\delta}^{k}}{n_{h}^{k-1}(s,a)\\vee1}}+\\frac{4H^{2}S L_{\\delta}^{k}}{n_{h}^{k-1}(s,a)\\vee1}}\\\\ &{\\qquad\\qquad\\leq\\sqrt{\\frac{2\\mathrm{Var}_{P_{h}(\\cdot\\vert s,a)}(V_{h+1}^{\\pi k})L_{\\delta}^{k}}{n_{h}^{k-1}(s,a)\\vee1}}+\\frac{1}{32H}P_{h}\\Big(V_{h+1}^{*}-V_{h+1}^{\\pi^{k}}\\Big)(s,a)+\\frac{16H L_{\\delta}^{k}}{n_{h}^{k-1}(s,a)}+\\frac{4H^{2}S L}{n_{h}^{k-1}(s,a)}}\\\\ &{\\qquad\\qquad\\leq\\sqrt{\\frac{2\\mathrm{Var}_{P_{h}(\\cdot\\vert s,a)}(V_{h+1}^{\\pi k})L_{\\delta}^{k}}{n_{h}^{k-1}(s,a)\\vee1}}+\\frac{1}{32H}P_{h}\\Big(\\bar{V}_{h+1}^{k}-V_{h+1}^{\\pi^{k}}\\Big)(s,a)+\\frac{20H^{2}S L_{\\delta}^{k}}{n_{h}^{k-1}(s,a)\\vee1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where we again used the optimism. Combining both and summing over all $k$ ,weget ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{=1}^{K}\\biggl(b_{k,h}^{p}\\bigl(s_{h}^{k},a_{h}^{k}\\bigr)+b_{k,h}^{p v1}\\bigl(s_{h}^{k},a_{h}^{k}\\bigr)\\biggr)\\leq9\\displaystyle\\sum_{k=1}^{K}\\sqrt{\\frac{\\mathrm{Var}_{P_{h}(\\cdot\\vert s_{h}^{k},a_{h}^{k})}\\bigl(V_{h+1}^{\\pi^{k}}\\bigr)L_{\\delta}^{k}}{n_{h}^{k-1}\\bigl(s_{h}^{k},a_{h}^{k}\\bigr)\\,\\mathrm{V}\\,1}}+\\frac{1}{8H}\\displaystyle\\sum_{k=1}^{K}P_{h}\\Bigl(\\bar{V}_{h+1}^{k}-V_{h+1}^{\\pi^{k}}\\Bigr)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\displaystyle\\sum_{k=1}^{K}\\frac{805H^{2}S L_{\\delta}^{k}}{n_{h}^{k-1}\\bigl(s_{h}^{k},a_{h}^{k}\\bigr)\\,\\mathrm{V}\\,1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Finally, under the good event $E^{\\mathrm{diff2}}$ , it holds that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{=1}^{K}P_{h}\\Big(\\bar{V}_{h+1}^{k}-V_{h+1}^{\\pi^{k}}\\Big)(s_{h}^{k},a_{h}^{k})=\\displaystyle\\sum_{k=1}^{K}\\mathbb{E}\\Big[\\bar{V}_{h+1}^{k}(s_{h+1}^{k})-V_{h+1}^{\\pi^{k}}(s_{h+1}^{k})\\vert F_{k,h-1}^{R}\\Big]}&{{}}\\\\ {\\displaystyle}&{\\leq\\bigg(1+\\displaystyle\\frac{1}{2H}\\bigg)\\sum_{k=1}^{K}\\Big(\\bar{V}_{h+1}^{k}(s_{h+1}^{k})-V_{h+1}^{\\pi^{k}}(s_{h+1}^{k})\\Big)+18H^{2}\\ln\\frac{8H K(H K)}{\\delta}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Substituting this relation back concludes the proof. ", "page_idx": 28}, {"type": "text", "text": "Lemma 9. Under the event $E^{\\mathrm{Var}}$ it holds that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{K}\\sum_{h=1}^{H}\\frac{\\sqrt{\\operatorname{Var}_{P_{h}(\\cdot\\vert s_{h}^{k},a_{h}^{k})}(V_{h+1}^{\\pi^{k}})}}{\\sqrt{n_{h}^{k-1}(s_{h}^{k},a_{h}^{k})\\vee1}}\\leq2\\sqrt{H^{3}S A K L_{\\delta}^{K}}+\\sqrt{8S A}H^{2}L_{\\delta}^{K}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. Following Lemma 24 of [Efroni et al., 2021], by Cauchy-Schwartz inequality, it holds that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{K}\\sum_{h=1}^{H}\\frac{\\sqrt{\\mathrm{Var}_{P_{h}(\\cdot\\vert s_{h}^{k},a_{h}^{k})}(V_{h+1}^{\\pi^{k}})}}{\\sqrt{n_{h}^{k-1}(s_{h}^{k},a_{h}^{k})\\vee1}}\\leq\\sqrt{\\sum_{k=1}^{K}\\sum_{h=1}^{H}\\mathrm{Var}_{P_{h}(\\cdot\\vert s_{h}^{k},a_{h}^{k})}(V_{h+1}^{\\pi^{k}})}\\sqrt{\\sum_{k=1}^{K}\\sum_{h=1}^{H}\\frac{1}{n_{h}^{k-1}(s_{h}^{k},a_{h}^{k})\\vee1}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "The second term can be bounded by Lemma 20, namely, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{K}\\sum_{h=1}^{H}\\frac{1}{n_{h}^{k-1}(s_{h}^{k},a_{h}^{k})\\vee1}\\leq S A H(2+\\ln(K)).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We further focus on bounding the first term. Under $E^{\\mathrm{Var}}$ wehave ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle\\sum_{k=1}^{K}\\displaystyle\\sum_{h=1}^{H}\\mathrm{Var}_{P_{h}(\\cdot\\vert s_{h}^{k},a_{h}^{k})}(V_{h+1}^{\\pi^{k}})}\\\\ {\\displaystyle}&{\\le2\\displaystyle\\sum_{k=1}^{K}\\mathbb{E}\\left[\\displaystyle\\sum_{h=1}^{H}\\mathrm{Var}_{P_{h}(\\cdot\\vert s_{h}^{k},a_{h}^{k})}(V_{h+1}^{\\pi^{k}})\\vert F_{k-1}\\right]+4H^{3}\\ln\\frac{8H K(K+1)}\\delta}\\\\ {\\displaystyle}&{\\le2\\displaystyle\\sum_{k=1}^{K}\\mathbb{E}\\left[\\left(\\displaystyle\\sum_{h=1}^{H}R_{h}(s_{h}^{k},a_{h}^{k})-V_{1}^{\\pi^{k}}(s_{1}^{k})\\right)^{2}\\vert F_{k-1}\\right]+4H^{3}\\ln\\frac{8H K(K+1)}\\delta}&{(\\mathrm{By}}\\\\ &{\\le2H^{2}K+4H^{3}\\ln\\frac{8H K(K+1)}\\delta,}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where the last inequality is since both the values and cumulative rewards are bounded in $[0,H]$ Combining both, we get ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{s=1}^{K}\\sum_{h=1}^{H}\\frac{\\sqrt{\\operatorname{Var}_{P_{h}(\\cdot\\vert s_{h}^{k},a_{h}^{k})}(V_{h+1}^{\\pi^{k}})}}{\\sqrt{n_{h}^{k-1}(s_{h}^{k},a_{h}^{k})\\vee1}}\\leq\\sqrt{2H^{2}K+4H^{3}\\ln\\frac{8H K(K+1)}{\\delta}}\\sqrt{S A H(2+\\ln(K))}}}\\\\ &{\\leq\\sqrt{2H^{2}K+4H^{3}\\ln\\frac{8H K(K+1)}{\\delta}}\\sqrt{2S A H\\ln\\frac{8H K(K+1)}{\\delta}}}\\\\ &{\\leq2\\sqrt{H^{3}S A K L_{\\delta}^{K}}+\\sqrt{8S A H^{2}L_{\\delta}^{K}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "C Proofs for Transition Lookahead ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "C.1 Data Generation Process ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "As for the reward transition, we also assume that all data was generated before the game starts for all state-action-timesteps, and it is given to the agent when the relevant $(s,a,h)$ is visited. Thus, the rewards and next-state from the first $i^{t h}$ visits at a state (or a state-action pair) at a certain timestep are i.i.d. ", "page_idx": 29}, {"type": "text", "text": "Throughout this appendix, we use the notation $\\pmb{s}_{h+1}^{\\prime k}=\\left\\{s_{h+1}^{\\prime k}(s_{h}^{k},a)\\right\\}_{a\\in\\mathcal{A}}$ to denote the next-state observations at episode $k$ and timestep $h$ for all the actions, and use the equivalent filtrations to the ones defined at Appendix B.1, namely ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{\\tau}_{k,h}=\\sigma\\Big(\\Big\\{s_{t}^{1},a_{t}^{1},s_{t+1}^{\\prime1},R_{t}^{1}\\Big\\}_{t\\in[H]},\\cdots,\\Big\\{s_{t}^{k-1},a_{t}^{k-1},s_{t+1}^{\\prime k-1},R_{t}^{k-1}\\Big\\}_{t\\in[H]},\\Big\\{s_{t}^{k},a_{t}^{k},s_{t+1}^{\\prime k},R_{t}^{k}\\Big\\}_{t\\in[h]}\\Big),}\\\\ &{\\bar{\\tau}_{k}=\\sigma\\Big(\\Big\\{s_{t}^{1},a_{t}^{1},s_{t+1}^{\\prime1}\\Big\\}_{t\\in[H]},\\cdots,\\Big\\{s_{t}^{k},a_{t}^{k},s_{t+1}^{\\prime k},R_{t}^{k}\\Big\\}_{t\\in[H]},s_{1}^{k+1}\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "In partula,ti tat $s_{h+1}^{\\prime k}$ and $a_{h}^{k}$ are $F_{k,h}$ measurable, then sodoes $s_{h+1}^{k}$ ", "page_idx": 29}, {"type": "text", "text": "C.2 Extended MDP for Transition Lookahead ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "In this appendix, we present an equivalent extended MDP that embeds the lookahead into the state to fall under the vanilla MDP model, similarly to Appendix B.2. We use this equivalence to apply various existing results on MDPs without the need to reprove them. We follow the same conventions as Appendix B.2 while denoting transition lookahead values by $V^{T,\\pi}(s|\\mathcal{M})$ (and again, the superscript $T$ will be omitted in subsequent subsections). ", "page_idx": 29}, {"type": "text", "text": "For any MDP $\\mathcal{M}=(\\mathcal{S},\\mathcal{A},H,P,\\mathcal{R})$ ,let $\\mathcal{M}^{T}$ be an MDP of horizon $2H$ and state space $S^{A+1}$ that separates the state transition and next-state generation as follows: ", "page_idx": 29}, {"type": "text", "text": "1. Assume w.l.o.g. that $\\mathcal{M}$ starts at some initial state $s_{1}$ . The extended environment starts at a state $s_{1}\\times s_{0}^{\\prime}$ , where $s_{0}^{\\prime}\\in\\mathcal{S}^{A}$ is a vector of $A$ copies of some arbitrary state $s_{0}\\in\\mathcal S$   \n2. For any $h\\in[H]$ , at timestep $2h-1$ , the environment $\\mathcal{M}^{T}$ transitions from state $s_{h}\\times s_{0}^{\\prime}$ to Sh X Sh+1? , where $s_{h+1}^{\\prime}\\sim P_{h}(s)$ is a vector containing the next state for all actions $a\\in A$ .\uff0c this transition happens regardless of the action that the agent played. At timestep $2h$ , given an action $a_{h}$ , the environment transitions from $s_{h}\\times s_{h+1}^{\\prime}$ to $\\bar{s}_{h+1}^{\\prime}(a)\\times s_{0}^{\\prime}$   \n3. The rewards at odd steps $2h\\!-\\!1$ are zero, while the rewards at even steps $2h$ are $R_{h}(s_{h},a_{h})\\sim$ $\\mathcal{R}_{h}(s_{h},a_{h})$ of expectation $r_{h}(s_{h},a_{h})$ ", "page_idx": 29}, {"type": "text", "text": "As before, since the next state is embedded into the extended state space, any state-dependent policy in $\\mathcal{M}^{T}$ is a one-step transition lookahead policy in the original MDP. Also, the policy at even timesteps does not affect either the rewards or transitions, so it does not affect the value in any way. We again couple the two environments to have the exact same randomness, so assuming that the policy at the even steps in $\\mathcal{M}^{T}$ is the same as the policy in $\\mathcal{M}$ , we trivially get the following relation between the values ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{2h}^{\\pi}(s,s^{\\prime}|\\mathcal{M}^{T})=\\mathbb{E}\\left[\\displaystyle\\sum_{t=h}^{H}R_{t}(s_{t},a_{t})|s_{h}=s,s_{h+1}^{\\prime}(s,\\cdot)=s^{\\prime},\\pi\\right]\\triangleq V_{h}^{T,\\pi}(s,s^{\\prime}|\\mathcal{M}),}\\\\ &{V_{2h-1}^{\\pi}(s,s_{0}^{\\prime}|\\mathcal{M}^{T})=\\mathbb{E}\\left[\\displaystyle\\sum_{t=h}^{H}R_{t}(s_{t},a_{t})|s_{h}=s,\\pi\\right]=V_{h}^{T,\\pi}(s|\\mathcal{M}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "While $\\mathcal{M}^{T}$ is finite, it is exponential in size, so applying any standard algorithm in this environment would lead to exponentially-bad performance bounds. Nonetheless, as with the extended-reward environment, we use this representation to prove useful results on one-step transition lookahead. ", "page_idx": 29}, {"type": "text", "text": "Proposition 2. The optimal value of one-step transition lookahead agents satisfies ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{V_{H+1}^{T,*}(s)=0,}&{\\forall s\\in S,}\\\\ {V_{h}^{T,*}(s)={\\mathbb E}_{s^{\\prime}\\sim{P}_{h}(s)}\\bigg[\\underset{a\\in{\\mathcal A}}{\\operatorname*{max}}\\Big\\lbrace r_{h}(s,a)+V_{h+1}^{T,*}(s^{\\prime}(s,a))\\Big\\rbrace\\bigg],}&{\\quad}&{\\forall s\\in{\\mathcal S},h\\in[H].}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Also, given next-state bservations $\\pmb{s}^{\\prime}=\\{s^{\\prime}(a)\\}_{a\\in\\mathcal{A}}$ at state s and step $h_{i}$ the optimal policy is ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\pi_{h}^{*}(s,s^{\\prime})\\in\\underset{a\\in\\cal{A}}{\\arg\\operatorname*{max}}\\Big\\{r_{h}(s,a)+V_{h+1}^{T,*}(s^{\\prime}(a))\\Big\\}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof. We prove the result in the extended MDP $\\mathcal{M}^{T}$ , in which (as with reward lookahead) the optimal value can be calculated using the Bellman equations as follows [Puterman, 2014] ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{V_{2H+1}^{T}(s,s^{\\prime}|\\mathcal{M}^{T})=0,\\quad}&{\\forall s\\in S,s^{\\prime}\\in\\mathcal{S}^{A},}\\\\ &{V_{2h}^{*}(s,s^{\\prime}|\\mathcal{M}^{T})=\\underset{a}{\\operatorname*{max}}\\big\\{r_{h}(s,a)+V_{2h+1}^{*}(s^{\\prime}(a),s_{0}^{\\prime}|\\mathcal{M}^{T})\\big\\},\\quad\\forall h\\in[H],s\\in\\mathcal{S},s^{\\prime}\\in\\mathcal{S}^{A},}\\\\ &{V_{2h-1}^{*}(s,s_{0}^{\\prime}|\\mathcal{M}^{T})=\\mathbb{E}_{s^{\\prime}\\sim P_{h}(s)}\\big[V_{2h}^{*}(s,s^{\\prime}|\\mathcal{M}^{T})\\big],\\quad}&{\\forall h\\in[H],s\\in\\mathcal{S}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "By the equivalence between $\\mathcal{M}$ and $\\mathcal{M}^{T}$ for all policies, this is also the optimal value in $\\mathcal{M}$ Combining both recursion equations and substituting Equation (7) leads to the stated value calculation for all $h\\in[H]$ and $s\\in S$ ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{h}^{T,*}(s|\\mathcal{M})=V_{2h-1}^{*}(s,s_{0}^{\\prime}|\\mathcal{M}^{T})}\\\\ &{\\quad\\quad\\quad\\quad=\\mathbb{E}_{s^{\\prime}\\sim P_{h}(s)}\\left[V_{2h}^{*}(s,s_{h+1}^{\\prime}|\\mathcal{M}^{T})\\right]}\\\\ &{\\quad\\quad\\quad\\quad=\\mathbb{E}_{s^{\\prime}\\sim P_{h}(s)}\\Bigl[\\underset{a}{\\operatorname*{max}}\\bigl\\{r_{h}(s,a)+V_{2h+1}^{*}(s_{h+1}^{\\prime}(a),s_{0}^{\\prime}|\\mathcal{M}^{T})\\bigr\\}\\Bigr]}\\\\ &{\\quad\\quad\\quad\\quad=\\mathbb{E}_{s^{\\prime}\\sim P_{h}(s)}\\Bigl[\\underset{a}{\\operatorname*{max}}\\Bigl\\{r_{h}(s,a)+V_{h+1}^{T,*}(s_{h+1}^{\\prime}(a)|\\mathcal{M})\\Bigr\\}\\Bigr].}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "In addition, a given state $s$ and next-state observations $s^{\\prime}$ , the optimal policy at the even stages of the extended MDP is ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\pi_{2h}^{*}(s,s^{\\prime})\\in\\underset{a\\in\\mathcal{A}}{\\arg\\operatorname*{max}}\\big\\{r_{h}(s,a)+V_{2h+1}^{*}(s^{\\prime}(a))\\big\\},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "alongside arbitrary actions at odd steps. Playing this policy in the original MDP will lead to the optimal one-step transition lookahead policy, as it achieves the optimal value of the original MDP. By the value relations between the two environments $(V_{2h+1}^{*}(s,\\stackrel{\\cdot}{s_{0}^{\\prime}}\\!|\\mathcal{M}^{T})=V_{h+1}^{T,*}(s|\\bar{\\mathcal{M}}))$ ,thisis equivalent to the stated policy. ", "page_idx": 30}, {"type": "text", "text": "Remark 2. As in Remark $^{\\,l}$ . one could write the dynamic programming equations for any policy $\\pi\\in\\Pi^{T}$ ,and not just to the optimal one, namely ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{V_{2h}^{\\pi}(s,s^{\\prime}|\\mathcal{M}^{T})=r_{h}(s,\\pi(s,s^{\\prime}))+V_{2h+1}^{*}(s^{\\prime}(\\pi_{h}(s,s^{\\prime})),s_{0}^{\\prime}|\\mathcal{M}^{T}),}&{\\forall h\\in[H],s\\in S,s^{\\prime}\\in\\mathcal{S}^{A},}\\\\ &{V_{2h-1}^{\\pi}(s,s_{0}^{\\prime}|\\mathcal{M}^{T})=\\mathbb{E}_{s^{\\prime}\\sim P_{h}(s)}\\big[V_{2h}^{\\pi}(s,s^{\\prime}|\\mathcal{M}^{T})\\big],}&{\\forall h\\in[H],s\\in\\mathcal{S}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "In particular following the notation of Equation (7), we can write ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{h}^{T,\\pi}(s,s^{\\prime}|\\mathcal{M})=r_{h}(s,\\pi_{h}(s,s^{\\prime}))+V_{h+1}^{T,\\pi}(s^{\\prime}(\\pi_{h}(s,s^{\\prime}))|\\mathcal{M}),\\qquad a n d,}\\\\ &{V_{h}^{T,\\pi}(s|\\mathcal{M})=\\mathbb{E}_{s^{\\prime}\\sim P_{h}(s)}\\Big[V_{h}^{T,\\pi}(s,s^{\\prime}|\\mathcal{M})\\Big]}\\\\ &{\\qquad\\qquad=\\mathbb{E}_{s^{\\prime}\\sim P_{h}(s)}\\Big[r_{h}(s,\\pi_{h}(s,s^{\\prime}))+V_{h+1}^{T,\\pi}(s^{\\prime}(\\pi_{h}(s,s^{\\prime}))|\\mathcal{M})\\Big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "a notation that will be extensively used for transition lookahead. ", "page_idx": 30}, {"type": "text", "text": "We also prove a variation of the law of total variance (LTV) for transition lookahead: ", "page_idx": 31}, {"type": "text", "text": "Lemma 10. For any one-step transition lookahead policy $\\pi\\in\\Pi^{T}$ ,itholdsthat ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\sum_{h=1}^{H}\\operatorname{Var}_{s^{\\prime}\\sim P_{h}(s_{h})}(V_{h}^{T,\\pi}(s_{h},s^{\\prime}))|\\pi,s_{1}\\right]\\le\\mathbb{E}\\left[\\left(\\sum_{h=1}^{H}r_{h}(s_{h},a_{h})-V_{1}^{T,\\pi}(s_{1})\\right)^{2}|\\pi,s_{1}\\right].\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Proof. We apply the law of total variance in the extended MDP; there, the expected rewards are either O (at odd steps) or $r_{h}(s_{h},a_{h})$ (at even steps), so the total expected rewards are $\\begin{array}{r}{\\sum_{h=1}^{H}r_{h}(s_{h},a_{h})}\\end{array}$ Hence, by Lemma 27, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left(\\displaystyle\\sum_{h=1}^{H}\\gamma_{h}(s_{h},a_{h})-V_{1}^{\\pi}(s_{1},s_{0}^{\\prime}|A^{T})\\right)^{2}|\\pi,s_{1}\\right]}\\\\ &{=\\mathbb{E}\\left[\\displaystyle\\sum_{s\\geq1}^{H}\\mathrm{Var}(V_{\\widehat{J}_{h}^{\\pi}}^{\\top}(s_{h},s_{h+1}^{\\prime}|A^{T})|(s_{h},s_{0}^{\\prime}))+\\displaystyle\\sum_{s\\geq1}^{H}\\mathrm{Var}(V_{\\widehat{J}_{h}^{\\pi}+1}^{\\top}(s_{h+1},s_{0}^{\\prime}|A^{T})|(s_{h},s_{h+1}^{\\prime}))|\\pi,s_{h}\\right]}\\\\ &{\\geq\\mathbb{E}\\left[\\displaystyle\\sum_{h\\geq1}^{H}\\mathrm{Var}(V_{\\widehat{J}_{h}^{\\pi}}^{\\top}(s_{h},s_{h+1}|A^{T})|(s_{h},s_{0}^{\\prime}))|\\pi,s_{1}\\right]}\\\\ &{=\\mathbb{E}\\left[\\displaystyle\\sum_{h\\geq1}^{H}\\mathrm{Var}_{s^{\\prime}\\sim\\mathcal{P}_{h}(s_{h})}(V_{\\widehat{J}_{h}^{\\pi}}^{\\top}(s_{h},s_{h}^{\\prime}|A^{T})|)|\\pi,s_{1}\\right]}\\\\ &{=\\mathbb{E}\\left[\\displaystyle\\sum_{h\\geq1}^{H}\\mathrm{Var}_{s^{\\prime}\\sim\\mathcal{P}_{h}(s_{h})}(V_{\\widehat{J}_{h}^{\\pi}}^{\\top\\pi}(s_{h},s_{h}^{\\prime}|A^{T})|)|\\pi,s_{1}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Using again the identity $V_{1}^{\\pi}(s_{1},s_{0}^{\\prime}|\\mathcal{M}^{T})=V_{1}^{T,\\pi}(s_{1}|\\mathcal{M})$ leads to the desired result. ", "page_idx": 31}, {"type": "text", "text": "Finally, prove a value-difference lemma also for transition lookahead ", "page_idx": 31}, {"type": "text", "text": "Lemma 11 (Value-Difference Lemma with Transition Lookahead). Let $\\mathcal{M}_{1}=(\\mathcal{S},\\mathcal{A},H,P^{1},\\mathcal{R}^{1})$ and $\\mathscr{M}_{2}\\;=\\;\\stackrel{\\cdot}{(S,\\mathscr{A},H,P^{2},\\mathscr{R}^{2})}$ be two environments. For any deterministic one-step transition lookahead policy $\\pi\\in\\Pi^{T}$ ,any $h\\in[H]$ and $s\\in\\mathcal{S}$ it holds that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\gamma_{h}^{T,\\pi}(s|\\mathcal{M}_{1})-V_{h}^{T,\\pi}(s|\\mathcal{M}_{2})}\\\\ &{\\hphantom{\\gamma_{h}^{T,\\pi}(s|\\mathcal{M}_{1})-}=\\mathbb{E}_{M_{1}}\\big[r_{h}^{1}(s_{h},\\pi_{h}(s_{h},s_{h+1}^{\\prime}))-r_{h}^{2}(s_{h},\\pi_{h}(s_{h},s_{h+1}^{\\prime}))|s_{h}=s\\big]}\\\\ &{\\hphantom{\\gamma_{h}^{T,\\pi}(s|\\mathcal{M}_{1})-}+\\mathbb{E}_{M_{1}}\\Big[V_{h+1}^{T,\\pi}(s_{h+1}|\\mathcal{M}_{1})-V_{h+1}^{T,\\pi}(s_{h+1}|\\mathcal{M}_{2})|s_{h}=s\\Big]}\\\\ &{\\hphantom{\\gamma_{h}^{T,\\pi}(s|\\mathcal{M}_{1})-}+\\mathbb{E}_{M_{1}}\\Big[\\mathbb{E}_{s^{\\prime}\\sim P_{h}^{1}(s_{h})}\\Big[V_{h}^{T,\\pi}(s_{h},s^{\\prime}|\\mathcal{M}_{2})\\Big]-\\mathbb{E}_{s^{\\prime}\\sim P_{h}^{2}(s_{h})}\\Big[V_{h}^{T,\\pi}(s_{h},s^{\\prime}|\\mathcal{M}_{2})\\Big]|s_{h}=s\\Big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $V_{h}^{T,\\pi}(s,s^{\\prime}|\\mathcal{M})$ istheaeldq given in Remark 2. ", "page_idx": 31}, {"type": "text", "text": "Proof. We again work with the extended MDPs $M_{1}^{T},\\mathcal{M}_{2}^{T}$ and use their Bellman equations, namely, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{V_{2h}^{\\pi}(s,s^{\\prime}|\\mathcal{M}^{T})=r_{h}(s,\\pi(s,s^{\\prime}))+V_{2h+1}^{*}(s^{\\prime}(\\pi_{h}(s,s^{\\prime})),s_{0}^{\\prime}|\\mathcal{M}^{T}),}&{\\forall h\\in[H],s\\in S,s^{\\prime}\\in\\mathcal{S}^{A},}\\\\ &{V_{2h-1}^{\\pi}(s,s_{0}^{\\prime}|\\mathcal{M}^{T})=\\mathbb{E}_{s^{\\prime}\\sim P_{h}(s)}\\big[V_{2h}^{\\pi}(s,s^{\\prime}|\\mathcal{M}^{T})\\big],}&{\\forall h\\in[H],s\\in\\mathcal{S}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Using the relation between the value of the original and extended MDP (eq. (7)) and the Bellman equations of the extended MDP, for any $h\\in[H]$ ,wehave ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\gamma_{h}^{T,\\pi}(s|\\mathcal{M}_{1})-V_{h}^{T,\\pi}(s|\\mathcal{M}_{2})}\\\\ &{=V_{h-1}^{\\pi}(s,s_{0}^{\\prime}|\\mathcal{M}_{1}^{T})-V_{2h-1}^{\\pi}(s,s_{0}^{\\prime}|\\mathcal{M}_{2}^{T})}\\\\ &{=\\mathbb{E}_{s^{\\prime}\\sim P_{h}^{1}(s)}\\big[V_{2h}^{\\pi}(s,s^{\\prime}|\\mathcal{M}_{1}^{T})\\big]-\\mathbb{E}_{s^{\\prime}\\sim P_{h}^{2}(s)}\\big[V_{2h}^{\\pi}(s,s^{\\prime}|\\mathcal{M}_{2}^{T})\\big]}\\\\ &{=\\mathbb{E}_{s^{\\prime}\\sim P_{h}^{1}(s)}\\big[V_{2h}^{\\pi}(s,s^{\\prime}|\\mathcal{M}_{1}^{T})-V_{2h}^{\\pi}(s,s^{\\prime}|\\mathcal{M}_{2}^{T})\\big]+\\mathbb{E}_{s^{\\prime}\\sim P_{h}^{1}(s)}\\big[V_{2h}^{\\pi}(s,s^{\\prime}|\\mathcal{M}_{2}^{T})\\big]-\\mathbb{E}_{s^{\\prime}\\sim P_{h}^{2}(s)}\\big[V_{2h}^{\\pi}(s,s^{\\prime}|\\mathcal{M}_{1}^{T})\\big]}\\\\ &{=\\mathbb{E}_{s^{\\prime}\\sim P_{h}^{1}(s)}\\big[V_{2h}^{\\pi}(s,s^{\\prime}|\\mathcal{M}_{1}^{T})-V_{2h}^{\\pi}(s,s^{\\prime}|\\mathcal{M}_{2}^{T})\\big]+\\mathbb{E}_{s^{\\prime}\\sim P_{h}^{1}(s)}\\big[V_{h}^{T,\\pi}(s,s^{\\prime}|\\mathcal{M}_{2})\\big]-\\mathbb{E}_{s^{\\prime}\\sim P_{h}^{2}(s)}\\big[V_{h}^{T,\\pi}(s,s^{\\prime}|\\mathcal{M}_{1}^{T})-V_{2h}^{\\pi}(s,s^{\\prime}|\\mathcal{M}_{2}^{T})\\big]}\\\\ &{=\\mathbb{E}_{M_{1}}\\big[V_{2h}^{\\pi}(s_{h},s_{h+1}^{\\prime}|\\mathcal{\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Denoting $a_{h}=\\pi_{h}(s_{h},{\\pmb s}_{h+1}^{\\prime})$ the action taken by the agent at environment $\\mathcal{M}_{1}$ , We have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{2h}^{\\pi}(s_{h},s_{h+1}^{\\prime}|\\mathcal{M}_{1}^{T})-V_{2h}^{\\pi}(s_{h},s_{h+1}^{\\prime}|\\mathcal{M}_{2}^{T})}\\\\ &{\\quad\\quad=\\big(r_{h}^{1}(s_{h},a_{h})+V_{2h+1}^{\\pi}(s_{h+1}^{\\prime}(a_{h}),s_{0}^{\\prime}|\\mathcal{M}_{1}^{T})\\big)-\\big(r_{h}^{2}(s_{h},a_{h})+V_{2h+1}^{\\pi}(s_{h+1}^{\\prime}(a_{h}),s_{0}^{\\prime}|\\mathcal{M}_{2}^{T})\\big)}\\\\ &{\\quad\\quad=r_{h}^{1}(s_{h},a_{h})-r_{h}^{2}(s_{h},a_{h})+V_{h+1}^{T,\\pi}(s_{h+1}^{\\prime}(a_{h})|\\mathcal{M}_{1})-V_{h+1}^{T,\\pi}(s_{h+1}^{\\prime}(a_{h})|\\mathcal{M}_{2}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "when taking the expectation w.r.t. $\\mathcal{M}_{1}$ , it holds that $s_{h+1}^{\\prime}(a_{h})=s_{h+1}$ substituting this back into Equation (9), we get ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{h}^{\\pi}(s|\\mathcal{M}_{1})-V_{h}^{\\pi}(s|\\mathcal{M}_{2})}\\\\ &{\\quad=\\mathbb{E}_{M_{1}}\\Big[r_{h}^{1}(s_{h},a_{h})-r_{h}^{2}(s_{h},a_{h})+V_{h+1}^{T,\\pi}(s_{h+1}^{\\prime}(a_{h})|\\mathcal{M}_{1})-V_{h+1}^{T,\\pi}(s_{h+1}^{\\prime}(a_{h})|\\mathcal{M}_{2})\\big|s_{h}=s\\Big]}\\\\ &{\\quad\\quad+\\mathbb{E}_{s^{\\prime}\\sim P_{h}^{1}(s)}\\Big[V_{h}^{T,\\pi}(s,s^{\\prime}|\\mathcal{M}_{2})\\Big]-\\mathbb{E}_{s^{\\prime}\\sim P_{h}^{2}(s)}\\Big[V_{h}^{T,\\pi}(s,s^{\\prime}|\\mathcal{M}_{2})\\Big]}\\\\ &{\\quad=\\mathbb{E}_{M_{1}}\\big[r_{h}^{1}(s_{h},\\pi_{h}(s_{h},s_{h+1}^{\\prime}))-r_{h}^{2}(s_{h},\\pi_{h}(s_{h},s_{h+1}^{\\prime}))|s_{h}=s\\big]}\\\\ &{\\quad\\quad+\\mathbb{E}_{M_{1}}\\Big[V_{h+1}^{T,\\pi}(s_{h+1}|\\mathcal{M}_{1})-V_{h+1}^{T,\\pi}(s_{h+1}|\\mathcal{M}_{2})|s_{h}=s\\Big]}\\\\ &{\\quad\\quad+\\mathbb{E}_{M_{1}}\\Big[\\mathbb{E}_{s^{\\prime}\\sim P_{h}^{1}(s_{h})}\\Big[V_{h}^{T,\\pi}(s_{h},s^{\\prime}|\\mathcal{M}_{2})\\Big]-\\mathbb{E}_{s^{\\prime}\\sim P_{h}^{2}(s_{h})}\\Big[V_{h}^{T,\\pi}(s_{h},s^{\\prime}|\\mathcal{M}_{2})\\Big]\\Big|s_{h}=s\\Big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Algorithm 4 Monotonic Value Propagation with Transition Lookahead (MVP-TL)   \n1: Require: $\\delta\\in(0,1)$ , bonuses $b_{k,h}^{r}(s,a),b_{k,h}^{p}(s)$   \n2: for $k=1,2,\\dots$ do 3: Initialize $\\bar{V}_{H+1}^{k}(s)=0$ 4: for $h=H,H-1,..,1$ do   \n5\uff16 mi $s\\in S$ then $n_{h}^{k-1}(s)=0$ 7: $\\bar{V}_{h}^{k}(s)=H$ 8: else 9: Calculate the truncated values $\\bar{V}_{h}^{k}(s)=\\operatorname*{min}\\left\\{\\frac{1}{n_{h}^{k-1}(s)}\\sum_{t=1}^{n_{h}^{k-1}(s)}\\operatorname*{max}_{a\\in A}\\Bigl\\{\\hat{r}_{h}^{k-1}(s,a)+b_{k,h}^{r}(s,a)+\\bar{V}_{h+1}^{k}(s^{\\prime}_{h+1}^{k}(s))\\Bigr\\}+b_{k,h}^{p}(s),H\\right\\}$   \n10: end if   \n11: For any set of next-states $\\pmb{s}^{\\prime}\\in S^{A}$ , define the policy $\\pi^{k}$ $\\pi_{h}^{k}(s,s^{\\prime})\\in\\underset{a\\in\\mathcal{A}}{\\arg\\operatorname*{max}}\\bigl\\{\\hat{r}_{h}^{k-1}(s,a)+b_{k,h}^{r}(s,a)+\\bar{V}_{h+1}^{k}(s^{\\prime}(a))\\bigr\\}$   \n12: end for   \n13: end for   \n14: $h=1,2,\\dots H$ $s_{h}^{k}$ $\\pmb{s}_{h+1}^{\\prime k}=\\left\\{s_{h+1}^{\\prime k}(s_{h}^{k},a)\\right\\}_{a\\in\\mathcal{A}}$   \n16: Play an action $a_{h}^{k}=\\pi_{h}^{k}(s_{h}^{k},s_{h}^{\\prime k})$   \n17: Collect the reward $R_{h}^{k}\\sim\\mathcal{R}_{h}(s_{h}^{k},a_{h}^{k})$ and transiont the nextstate $s_{h+1}^{k}=s_{h+1}^{\\prime k}(s_{h}^{k},a_{h}^{k})$   \n18: end for   \n19: Update the empirical estimators and counts for all visited state-actions   \n20: end for ", "page_idx": 33}, {"type": "text", "text": "As with reward lookahead, we again use a variant of the MVP algorithm [Zhang et al., 2021b], described in Algorithm 4. For the bonuses, we use the notation ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\bar{V}_{h}^{k}(s,s^{\\prime})=\\operatorname*{max}_{a\\in\\mathcal{A}}\\bigl\\{\\hat{r}_{h}^{k-1}(s,a)+b_{k,h}^{r}(s,a)+\\bar{V}_{h+1}^{k}(s^{\\prime}(a)\\bigr\\}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and define the following bonuses: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle b_{k,h}^{r}(s,a)=\\operatorname*{min}\\Biggl\\{\\sqrt{\\frac{L_{\\delta}^{k}}{n_{h}^{k-1}(s,a)\\vee1}},1\\Biggr\\},}}\\\\ {{\\displaystyle b_{k,h}^{p}(s)=\\frac{20}{3}\\sqrt{\\frac{\\mathrm{Var}_{s^{\\prime}\\sim\\hat{P}_{h}^{k-1}(s)}(\\bar{V}_{h}^{k}(s,s^{\\prime}))L_{\\delta}^{k}}{n_{h}^{k-1}(s)\\vee1}}+\\frac{400}{3}\\frac{H L_{\\delta}^{k}}{n_{h}^{k-1}(s)\\vee1},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where L = In 16s3A2Hk\u00b2(b+1) and ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname{Var}_{s^{\\prime}\\sim\\hat{P}_{h}^{k-1}(s)}(\\bar{V}_{h}^{k}(s,s^{\\prime}))=\\mathbb{E}_{s^{\\prime}\\sim\\hat{P}_{h}^{k-1}(s)}\\big[\\bar{V}_{h}^{k}(s,s^{\\prime})^{2}\\big]-\\Big(\\mathbb{E}_{s^{\\prime}\\sim\\hat{P}_{h}^{k-1}(s)}\\big[\\bar{V}_{h}^{k}(s,s^{\\prime})\\big]\\Big)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "The notation $k_{h}^{t}(s)$ again represents the $t^{t h}$ episode where the state $s$ was visited at the $h^{t h}$ timestep; in particular, line 9 of the algorithm is the expectation w.r.t. the empirical reward distribution $\\hat{P_{h}^{k-1}}(s)$ $H$ when $n_{h}^{k-1}(s)=0$ we canarbiraily defne the expectation w.t. $\\hat{P}_{h}^{k-1}(s)$ when $n_{h}^{k-1}(s)=0$ to be O, and one could write the update in a more concise way as ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\bar{V}_{h}^{k}(s)=\\operatorname*{min}\\Bigl\\{\\mathbb{E}_{s^{\\prime}\\sim\\hat{P}_{h}^{k-1}(s)}\\bigl[\\bar{V}_{h}^{k}(s,s^{\\prime})\\bigr]+b_{k,h}^{p}(s),H\\Bigr\\}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "C.4  Additional Notations and List Representation ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "In this subsection, we present additional notations for both values and transition distributions that will be helpful in the analysis. In particular, we show that instead of looking at the distribution over all combinations of next state $\\pmb{s}^{\\prime}\\in S^{A}$ , we can look at a ranking of all the next-state-actions and represent important quantities using the effective distribution on these ranks - this moves the problem from being ${\\bar{S}}^{A}$ dimensional to a dimension of $S A$ ", "page_idx": 34}, {"type": "text", "text": "We start by defining the values starting from state $s\\in S$ , playing $a\\in{\\mathcal{A}}$ and transitioning to $s^{\\prime}\\in\\mathcal{S}$ denoted by ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{h}^{\\pi}(s,s^{\\prime},a)=r_{h}(s,a)+V_{h+1}^{\\pi}(s^{\\prime}),}\\\\ &{V_{h}^{*}(s,s^{\\prime},a)=r_{h}(s,a)+V_{h+1}^{*}(s^{\\prime}),}\\\\ &{\\bar{V}_{h}^{k}(s,s^{\\prime},a)=\\hat{r}_{h}^{k-1}(s,a)+b_{k,h}^{r}(s,a)+\\bar{V}_{h+1}^{k}(s^{\\prime}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We similarly define (consistently with Remark 2) ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{V_{h}^{\\pi}(s,s^{\\prime})=V_{h}^{\\pi}(s,s^{\\prime}(\\pi_{h}(s,s^{\\prime})),\\pi_{h}(s,s^{\\prime})),}\\\\ &{V_{h}^{*}(s,s^{\\prime})=\\underset{a}{\\operatorname*{max}}\\,V_{h}^{*}(s,s^{\\prime}(a),a),}&{\\mathrm{and}\\;,}\\\\ &{\\bar{V}_{h}^{k}(s,s^{\\prime})=\\underset{a}{\\operatorname*{max}}\\,\\bar{V}_{h}^{k}(s,s^{\\prime}(a),a).}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "List representation. We now move to defining lists of next-state-actions and distributions with respect to such lists. Let $\\ell$ be a list that orders all next-state-action pairs from $(s_{\\ell(1)}^{\\prime},a_{\\ell(1)})$ 0 $\\big(s_{\\ell(S A)}^{\\prime},a_{\\ell(S A)}\\big)$ and define the set of all possible lists to be $\\mathcal{L}$ (with $|{\\mathcal{L}}|=(S A)!)$ . Also, define $\\ell^{u}$ , the list induced by a function $u:S\\times A\\mapsto\\mathbb{R}$ such that $u(s_{\\ell^{u}(1)}^{\\prime},a_{\\ell^{u}(1)})\\ge\\cdots\\ge u(s_{\\ell^{u}(S A)}^{\\prime},a_{\\ell^{u}(S A)})$ , where ties are broken in any fixed arbitrary way. From this point forward, for brevity and when clear from the context, we omit the list from the indexing, e.g., write the list $\\ell$ by $(s_{1}^{\\prime},a_{1}),\\ldots,(s_{S A}^{\\prime},a_{S A})$ ", "page_idx": 34}, {"type": "text", "text": "We now define the probability of list elements. Denote by $E_{i}^{\\ell}$ the event that the highest-ranked realized element in the list is element $i$ ,namely ", "page_idx": 34}, {"type": "equation", "text": "$$\nE_{i}^{\\ell}=\\big\\{s^{\\prime}\\in S^{A}:s^{\\prime}(a_{i})=s_{i}^{\\prime}\\;{\\mathrm{~and~}}\\;\\forall j<i,s^{\\prime}(a_{j})\\neq s_{j}^{\\prime}\\big\\}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Then, for a probability measure $P$ on $S^{A}$ , define $\\mu(i|\\ell,P)=P(\\pmb{s}^{\\prime}\\in E_{i}^{\\ell})$ . Notably, when the list is induced by $u$ and element $i$ is the realized highest-ranked elements, we can write $\\begin{array}{r l}{\\operatorname*{mix}_{a}u(s^{\\prime}(a),a)=}\\end{array}$ $u(s_{i}^{\\prime},a_{i})$ , so we have that (e.g. by Lemma 17 with $f(\\pmb{\\mathscr{s}}^{\\prime})=\\operatorname*{max}_{a}u(\\pmb{\\mathscr{s}}^{\\prime}(a),a))$ ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbb{E}_{s^{\\prime}\\sim P_{h}(s)}\\Big[\\!\\operatorname*{max}_{a}\\{u(s^{\\prime}(a),a)\\}\\Big]=\\mathbb{E}_{i\\sim\\mu(\\cdot|\\ell,P_{h}(s))}[u(s_{i}^{\\prime},a_{i})]\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We also denoe y $\\begin{array}{r}{\\hat{\\mu}_{h}^{k}(i|s;\\ell)=\\frac{1}{n_{h}^{k}(s)\\vee1}\\sum_{t=1}^{K}\\mathbb{1}\\big\\{s_{h}^{t}=s,s_{h+1}^{\\prime t}\\in E_{i}^{\\ell}\\big\\}}\\end{array}$ a list location $i$ to be the highest-realized ranking according to a list $\\ell$ at state $s$ and step $h$ , based on samples up to episode $k$ ; We have by Lemma 17 that $\\hat{\\mu}_{h}^{k}(i|s;\\ell)=\\hat{P}_{h}^{k}(E_{i}^{\\ell}|s)$ and ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbb{E}_{s^{\\prime}\\sim\\hat{P}_{h}^{k-1}(s)}\\!\\left[\\!\\operatorname*{max}_{a}\\{u(s^{\\prime}(a),a)\\}\\right]=\\mathbb{E}_{i\\sim\\hat{\\mu}_{h}^{k-1}(\\cdot|s;\\ell^{u})}[u(s_{i}^{\\prime},a_{i})].\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Similarly, we will require the distribution probability w.r.t. two lists - the probability that the top element w.r.t. list $\\ell$ is $i$ and the top element w.r.t. list $\\ell^{\\prime}$ is $j$ ; we denote the real and empirical probability distributions by $\\mu(i,j|\\ell,\\bar{\\ell}^{\\prime},P)$ and $\\hat{\\mu}_{h}^{k}(i,j|s;\\ell,\\ell^{\\prime})$ , respectively. This allows, for example, using Lemma 17 to write for any $u,v:S\\times A\\mapsto\\mathbb{R}$ \uff0c ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{s^{\\prime}\\sim P_{h}(s)}\\Big[\\underset{a}{\\operatorname*{max}}\\big\\{u(s^{\\prime}(a),a)\\big\\}-\\underset{a}{\\operatorname*{max}}\\big\\{v(s^{\\prime}(a),a)\\big\\}\\Big]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\mathbb{E}_{i,j\\sim\\mu(\\cdot|\\ell^{u},\\ell^{v},P_{h}(s))}\\Big[u(s_{\\ell^{u}(i)}^{\\prime},a_{\\ell^{u}(i)})-v(s_{\\ell^{v}(j)}^{\\prime},a_{\\ell^{v}(j)})\\Big],}\\\\ &{\\mathbb{E}_{s^{\\prime}\\sim\\hat{P}_{h}^{k-1}(s)}\\Big[\\underset{a}{\\operatorname*{max}}\\big\\{u(s^{\\prime}(a),a)\\big\\}-\\underset{a}{\\operatorname*{max}}\\big\\{v(s^{\\prime}(a),a)\\big\\}\\Big]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\mathbb{E}_{i,j\\sim\\hat{\\mu}_{h}^{k}(\\cdot|s;\\ell^{u},\\ell^{v})}\\Big[u(s_{\\ell^{u}(i)}^{\\prime},a_{\\ell^{u}(i)})-v(s_{\\ell^{v}(j)}^{\\prime},a_{\\ell^{v}(j)})\\Big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Finally, we say that a policy $\\pi_{h}(s,s^{\\prime})$ is induced by lists $\\ell_{h}(s)$ if it chooses an action $a$ such that its next-state $\\bar{s^{\\prime}}(a)$ is ranked higher in $\\ell$ than all other realized next-state-action pairs. In particular, the policy $\\pi^{k}$ and the optimal policy $\\pi^{*}$ (defined in Proposition 2) are such policies w.r.t. the lists $\\bar{\\ell}_{h}^{k}(\\bar{s})$ and $\\ell_{h}^{*}(s)$ -induced by $\\vec{V}_{h}^{k}(s,\\stackrel{.}{s}^{\\prime},a)$ and $V_{h}^{*}(s,s^{\\prime},\\bar{a})$ respectivly.Asuchforany probability measure $P_{h}(s)$ , function $u:S\\times S\\times A\\mapsto\\mathbb{R}$ and a policy $\\pi$ induced by a list $\\ell$ , it holds that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{s^{\\prime}\\sim P_{h}(s)}[u(s,s^{\\prime}(\\pi(a)),\\pi(a))]=\\mathbb{E}_{i\\sim\\mu(\\cdot|\\ell_{h}(s),P_{h}(s))}[u(s,s_{i}^{\\prime},a_{i})].}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "C.4.1 Planning with Transition Lookahead ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "We have already seen the optimal policy is induced by a list $\\ell_{h}^{*}(s)$ , and in particular, we can write the dynamic programming equations of Proposition 2 as ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{h}^{*}(s)=\\mathbb{E}_{s^{\\prime}\\sim P_{h}(s)}\\bigg[\\underset{a\\in\\mathcal{A}}{\\operatorname*{max}}\\Big\\lbrace r_{h}(s,a)+V_{h}^{T,*}(s^{\\prime}(a))\\Big\\rbrace\\bigg]}\\\\ &{\\qquad\\quad=\\mathbb{E}_{i\\sim\\mu(\\cdot|\\ell_{h}^{*}(s),P_{h}(s))}\\left[r_{h}(s,a_{i})+V_{h+1}^{*}(s^{\\prime}(a_{i}))\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Therefore, one way to perform the planning is to build a list $\\ell_{h}^{*}(s)$ of $(s^{\\prime},a)$ s.t. the values ", "page_idx": 35}, {"type": "equation", "text": "$$\nV_{h}^{*}(s,s^{\\prime},a)=r_{h}(s,a)+V_{h+1}^{*}(s^{\\prime})\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "are sorted in a non-increasing order and calculate the probability of any pair in the list to be the highest-realized pair: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mu(i|\\ell,P_{h}(s))=P_{h}(E_{i}^{\\ell})=\\mathrm{Pr}\\big(s_{h+1}^{\\prime}(a_{i})=s_{i}^{\\prime}\\;\\;\\mathrm{and}\\;\\;\\forall j<i,s_{h+1}^{\\prime}(a_{j})\\neq s_{j}^{\\prime}|s_{h}=s\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "In general, calculating this distribution is intractable, and one must resort to approximating it by sampling (as done in Algorithm 4. Nonetheless, if next states are generated independently between actions, this distribution could be efficiently calculated as follows: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu(i|\\ell,P_{h}(s))=\\mathrm{Pr}\\{s_{h+1}^{\\prime}(a_{i})=s_{i}^{\\prime}\\mathrm{~and~}\\,\\forall j<i,s_{h+1}^{\\prime}(a_{j})\\neq s_{j}^{\\prime}|s_{h}=s\\}}\\\\ &{\\overset{(1)}{=}\\mathrm{Pr}\\{s^{\\prime}(a_{i})=s_{i}^{\\prime}\\mathrm{~and~}\\,\\forall j<i\\mathrm{~s.t.~}a_{j}\\neq a_{i},s^{\\prime}(a_{j})\\neq s_{j}^{\\prime}|s_{h}=s\\}}\\\\ &{\\overset{(2)}{=}\\mathrm{Pr}\\{s^{\\prime}(a_{i})=s_{i}^{\\prime}|s_{h}=s\\}\\displaystyle\\prod_{a\\neq a_{i}}\\mathrm{Pr}\\{\\forall j<i\\mathrm{~s.t.~}a_{j}=a,s^{\\prime}(a)\\neq s_{j}^{\\prime}|s_{h}=s\\}}\\\\ &{\\overset{(3)}{=}P_{h}(s_{i}^{\\prime}|s,a_{i})\\displaystyle\\prod_{a\\neq a_{i}}\\left(1-\\displaystyle\\sum_{j=1}^{i-1}1\\{a_{j}=a\\}P_{h}(s_{j}^{\\prime}|s,a)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Relation (1) holds since if $s^{\\prime}(a_{i})=s_{i}^{\\prime}$ , it cannot get any previous value of the same action in the list, so these events can be removed. Relation (2) is by the independence and (3) directly calculates the probabilities. ", "page_idx": 35}, {"type": "text", "text": "C.5 The First Good Event - Concentration ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Next, we define the events that ensure the concentration of all empirical measures. For rewards, an event handles the convergence of the empirical rewards to their mean. For the transitions, we want the Bellman operator, applied on the optimal value with the empirical model, to concentrate well, and we require the variance of values w.r.t. the empirical and real model to be close. Finally, the empirical measure $\\hat{\\mu}_{h}^{k}(i,j|s;\\ell,\\ell_{h}^{*}(s))$ must concentrate well around its mean for any list $\\ell-$ this will allow the change-of-measure argument described in the proof sketch. ", "page_idx": 36}, {"type": "text", "text": "Formally, define the following good events: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{\\boldsymbol{\\Sigma}}^{r}(\\boldsymbol{k})=\\Bigg\\{\\forall s,a,h:\\ |r_{h}(s,a)-\\hat{r}_{h}^{k-1}(s,a)|\\leq\\sqrt{\\frac{L_{\\boldsymbol{\\delta}}^{k}}{n_{h}^{k-1}(s,a)\\vee1}}\\Bigg\\}}\\\\ &{\\tilde{z}^{\\ell}(\\boldsymbol{k})=\\Big\\{\\forall s,h,\\forall\\ell\\in\\mathcal{L},\\forall i,j\\in[S A]:\\left|\\hat{\\mu}_{h}^{k-1}(i,j|s;\\ell,\\hat{\\epsilon}_{h}^{*}(s))-\\mu(i,j|\\ell,\\hat{\\epsilon}_{h}^{*}(s);P_{h}(s))\\right|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\sqrt{\\frac{4S A L_{\\boldsymbol{\\delta}}^{k}\\mu(i,j|s;\\ell,\\hat{\\epsilon}_{h}^{*}(s);P_{h}(s))}{n_{h}^{k-1}(s)\\vee1}}+\\frac{2\\cdot1}{n_{h}^{k-1}(s)\\vee1}}\\\\ &{\\tilde{z}^{p\\ell}(\\boldsymbol{k})=\\Bigg\\{\\forall s,h:\\ \\left|\\mathbb{E}_{\\boldsymbol{\\delta}^{\\prime}\\sim P_{h}(s)}\\big[V_{h}^{*}(s,s^{\\prime})\\big]-\\mathbb{E}_{\\boldsymbol{\\delta}^{\\prime}\\sim\\hat{r}_{h}^{k-1}(s)}\\big[V_{h}^{*}(s,s^{\\prime})\\big]\\right|\\leq\\sqrt{\\frac{2\\mathrm{Var}_{s^{\\prime}\\sim P_{h}(s)}(V_{h}^{*}(s,s^{\\prime}))L_{\\boldsymbol{\\delta}}^{k}}{n_{h}^{k-1}(s)\\vee1}}+}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\times\\mu\\sqrt{\\frac{L_{\\boldsymbol{\\delta}}^{k}}{n_{h}^{k-1}(s)\\vee1}}\\Bigg\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where we again use $\\begin{array}{r}{L_{\\delta}^{k}=\\ln\\frac{16S^{3}A^{2}H k^{2}(k+1)}{\\delta}}\\end{array}$ Wedefne the frst gd ent as ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathbb{G}_{1}=\\bigcap_{k\\geq1}E^{r}(k)\\bigcap_{k\\geq1}E^{\\ell}(k)\\bigcap_{k\\geq1}E^{p v1}(k)\\bigcap_{k\\geq1}E^{p v2}(k),\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "for which the following holds: ", "page_idx": 36}, {"type": "text", "text": "Lemma 12 (The First Good Event). It holds that $\\mathrm{Pr}(\\mathbb{G}_{1})\\geq1-\\delta/2$ ", "page_idx": 36}, {"type": "text", "text": "Proof. We prove that each of the events holds w.p. at least $1-\\delta/8$ . The result then directly follows by the union bound. We also remark that due to the domain of the variables and their estimators (e.g., $[0,1]$ for the rewards), all bounds trivially hold when the counts equal zero, so w.1.o.g., we only prove the results for cases in which states/state-actions were already previously visited. ", "page_idx": 36}, {"type": "text", "text": "Event $\\cap_{k\\geq1}E^{r}(k)$ . Fix $k\\geq1,s,a,h$ and visits $n\\geq1$ . Given all of these, the reward observations are i.i.d. random variables supported by $[0,1]$ . Denoting the empirical mean based on these $n$ samples by $\\hat{r}_{h}(s,a,n)$ , by Hoeffding's inequality, it holds w.p. 8SAHk2(k+1) that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\left|r_{h}(s,a)-\\hat{r}_{h}(s,a,n)\\right|\\le\\sqrt{\\frac{\\ln\\frac{16S A H k^{2}(k+1)}{\\delta}}{2n}}\\le\\sqrt{\\frac{L_{\\delta}^{k}}{n}}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Taking the union bound over all $n\\in[k]$ at timestep $k$ , we get that w.p. $\\begin{array}{r}{1-\\frac{\\delta}{8S A H k(k+1)}}\\end{array}$ ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\lvert r_{h}(s,a)-\\hat{r}_{h}^{k-1}(s,a)\\rvert\\leq\\sqrt{\\frac{L_{\\delta}^{k}}{n_{h}^{k-1}(s,a)\\vee1}},\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "and another union bound over all possible values of $s,a,h$ and $k\\geq1$ implies that $\\cap_{k\\geq1}E^{r}(k)$ holds w.p. at least $1-\\delta/8$ ", "page_idx": 36}, {"type": "text", "text": "The event $\\cap_{k\\geq1}E^{\\ell}(k)$ . For any fixed $k\\geq1,s,h$ , a list $\\ell\\in{\\mathcal{L}}$ and number of visits $n\\in[k]$ , we utilize Lemma 16 (event $E^{p}$ ) w.r.t. the distribution $\\mu(i,j|\\ell,\\ell_{h}^{*}(s),P)$ (whose support is of size $M=(S A)^{2})$ When applying the lemma, notice that given the number of visits $n\\geq1$ ,the empirical distribution $\\hat{\\mu}_{h}^{k-1}(i,j|s;\\ell,\\ell_{h}^{*}(s))$ is the average of $\\bar{n}=n_{h}^{k-1}(s)$ i.d samples, so that for ll $i,j\\in[S A]$ \uff0c ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\hat{\\mu}_{h}^{k-1}(i,j|s;\\ell,\\ell_{h}^{*}(s))-\\mu(i,j|\\ell,\\ell_{h}^{*}(s);P_{h}(s))\\right|\\leq\\sqrt{\\frac{2\\mu(i,j|\\ell,\\ell_{h}^{*}(s);P_{h}(s))\\ln\\frac{2(S A)^{2}}{\\delta^{\\prime}}}{n}}+\\frac{2\\ln\\frac{2(S A)^{2}}{\\delta^{\\prime}}}{3n}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\sqrt{\\frac{4\\mu(i,j|\\ell,\\ell_{h}^{*}(s);P_{h}(s))\\ln\\frac{2S A}{\\delta^{\\prime}}}{n}}+\\frac{2\\ln\\frac{2S A}{\\delta^{\\prime}}}{n}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "WChosing such haA\u2264SA1s(b+1) since $|{\\mathcal{L}}|\\leq(S A)^{S A})$ , while taking the union bound on all $n\\in[k]$ , all $s,h$ and all lists $\\ell\\in{\\mathcal{L}}$ implies that $\\cap_{k\\geq1}E^{\\ell}(k)$ holds w.p. at least $\\begin{array}{r}{1-\\frac{\\delta}{8}}\\end{array}$   \nEvents $\\cap_{k\\geq1}E^{p v1}(k)$ and $\\cap_{k\\geq1}E^{p v2}(k)$ . We repeat the arguments stated in Lemma 5. For any fixed $k\\ge1,s,h$ and number of visits $n\\in[k]$ , we utilize Lemma 16 w.r.t. the next-state distribution for all actions $P_{h}(s)$ ,the alue $V_{h}^{*}(s,s^{\\prime})\\in[0,H]$ and probability $\\begin{array}{r}{\\delta^{\\prime}=\\frac{\\delta}{8S H k^{2}(k+1)}}\\end{array}$ ; we ye gan remind that given the number of visits, samples are i.i.d.   \nAs before, the events $\\cap_{k\\geq1}E^{p v1}(k)$ and $\\cap_{k\\geq1}E^{p v2}(k)$ hold w.p. at least $\\begin{array}{r}{1-\\frac{\\delta}{8}}\\end{array}$ through the union bound first on $n\\in[k]$ (to get the empirical quantities) and then on $s,h$ and $k\\geq1$ . This proves that cach of the eventsin $\\mathbb{G}_{1}$ holds wp a least $\\begin{array}{r}{1-\\frac{\\delta}{8}}\\end{array}$ $\\mathbb{G}_{1}$ holds wp a last $\\textstyle1-{\\frac{\\delta}{2}}$ \u53e3 ", "page_idx": 37}, {"type": "text", "text": "C.6  Optimism of the Upper Confidence Value Functions ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "We now prove that under the event $\\mathbb{G}_{1}$ , the values that MVP-TL outputs are optimistic. ", "page_idx": 38}, {"type": "text", "text": "Lemma 13 (Optimism). Under the first good event $\\mathbb{G}_{1}$ for all $k\\,\\in\\,[K],\\;h\\,\\in\\,[H],\\;a\\,\\in\\,{\\cal A}$ and $s,s^{\\prime}\\in\\mathcal{S},$ it holds that $V_{h}^{*}(s,s^{\\prime},a)\\leq\\bar{V}_{h}^{k}(s,s^{\\prime},a)$ Moreover, for all $\\pmb{s}^{\\prime}\\in S^{A}$ \uff0c $V_{h}^{*}(s,s^{\\prime})\\leq\\bar{V}_{h}^{k}(s,s^{\\prime})$ and also $V_{h}^{*}(s)\\leq\\bar{V}_{h}^{k}(s)$ ", "page_idx": 38}, {"type": "text", "text": "Proof. The proof of all claims follows by backward induction on $H$ ; the base case naturally holds for $h=H+1$ , where all values are defined to be zero. ", "page_idx": 38}, {"type": "text", "text": "Assume by induction that for some $k\\in[K]$ and $h\\in[H]$ , the inequality $V_{h+1}^{*}(s)\\leq\\bar{V}_{h+1}^{k}(s)$ holds for all $s\\in S$ ; we will show that this implies that all stated inequalities also hold at timestep $h$ .At this point, we also assume w.l.o.g. that $\\hat{V}_{h}^{k}(s)<H$ (namely, not truncated), since otherwise, by the boundedness of the rewards, $V_{h}^{*}(s)\\leq H=\\bar{V}_{h}^{k}(s)$ . In particular, under the good event $E^{r}(k)$ , for all $s$ and $a$ , it holds that $\\hat{r}_{h}^{k-1}(s,a)+b_{k,h}^{r}(s,a)\\geq r_{h}(s,a)$ , so for all $s,a$ and $s^{\\prime}$ , we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\bar{V}_{h}^{k}(s,s^{\\prime},a)=\\hat{r}_{h}^{k-1}(s,a)+b_{k,h}^{r}(s,a)+\\bar{V}_{h+1}^{k}(s^{\\prime})\\geq r_{h}(s,a)+V_{h+1}^{*}(s^{\\prime})=V_{h}^{*}(s,s^{\\prime},a).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where the inequality also uses the induction hypothesis. This proves the first part of the lemma. Moreover, it implies that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\bar{V}_{h}^{k}(s,s^{\\prime})=\\underset{a\\in\\mathcal{A}}{\\operatorname*{max}}\\big\\{\\bar{V}_{h}^{k}(s,s^{\\prime}(a),a)\\big\\}\\geq\\underset{a\\in\\mathcal{A}}{\\operatorname*{max}}\\{V_{h}^{*}(s,s^{\\prime}(a),a)\\}=V_{h}^{*}(s,s^{\\prime}),\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "and proves the second part of the statement. ", "page_idx": 38}, {"type": "text", "text": "To prove the last claim of the lemma, we use the monotonicity of the bonus, relying on Lemma 23. This lemma can be used when applied to the empirical distribution of ll possible next-states $\\hat{P}_{h}^{k-1}(s)$ indeed, the non-truncated optimistic value can be written as ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{\\gamma}_{h}^{k}(s)=\\mathbb{E}_{s^{\\prime}\\sim\\hat{P}_{h}^{k-1}(s)}\\bigg[\\underset{a\\in\\mathcal{A}}{\\operatorname*{max}}\\big\\{\\hat{r}_{h}^{k-1}(s,a)+b_{k,h}^{r}(s,a)+\\bar{V}_{h+1}^{k}(s^{\\prime}(a))\\big\\}\\bigg]+b_{k,h}^{p}(s)}\\\\ &{\\qquad\\geq\\mathbb{E}_{s^{\\prime}\\sim\\hat{P}_{h}^{k-1}(s)}\\big[\\bar{V}_{h}^{k}(s,s^{\\prime})\\big]+\\operatorname*{max}\\Bigg\\{\\frac{20}{3}\\sqrt{\\frac{\\mathrm{Var}_{s^{\\prime}\\sim\\hat{P}_{h}^{k-1}(s)}(\\bar{V}_{h}^{k}(s,s^{\\prime}))L_{\\delta}^{k}}{n_{h}^{k-1}(s)\\vee1}},\\frac{400}{9}\\frac{3H L_{\\delta}^{k}}{n_{h}^{k-1}(s)\\vee1}\\Bigg\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "which is exactly the required form in Lemma 23, w.r.t. the distribution $\\hat{P}_{h}^{k-1}(s)$ andthevalues $\\bar{V}_{h}^{k}(s,s^{\\prime})$ (while noticing that due to the truncation of the values and bonuses, $\\bar{V}_{h}^{k}(s,s^{\\prime})\\in[0,3H])$ Thus, the lemma guarantees monotonicity in the value, so by Equation (13), ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{V}_{h}^{k}(s)\\geq\\mathbb{E}_{s^{\\prime}\\sim\\hat{P}_{h}^{k-1}(s)}\\big[V_{h}^{*}(s,s^{\\prime})\\big]+\\operatorname*{max}\\left\\{\\frac{20}{3}\\sqrt{\\frac{\\operatorname{Var}_{s^{\\prime}\\sim\\hat{P}_{h}^{k-1}(s)}(V_{h}^{*}(s,s^{\\prime}))L_{\\delta}^{k}}{n_{h}^{k-1}(s)\\vee1}},\\frac{400}{9}\\frac{3H L_{\\delta}^{k}}{n_{h}^{k-1}(s)\\vee1}\\right\\}}\\\\ &{\\qquad\\geq\\mathbb{E}_{s^{\\prime}\\sim\\hat{P}_{h}^{k-1}(s)}\\big[V_{h}^{*}(s,s^{\\prime})\\big]+\\frac{10}{3}\\sqrt{\\frac{\\operatorname{Var}_{s^{\\prime}\\sim\\hat{P}_{h}^{k-1}(s)}(V_{h}^{*}(s,s^{\\prime}))L_{\\delta}^{k}}{n_{h}^{k-1}(s)\\vee1}}+\\frac{200}{3}\\frac{H L_{\\delta}^{k}}{n_{h}^{k-1}(s)\\vee1}}\\\\ &{\\qquad\\geq\\mathbb{E}_{s^{\\prime}\\sim\\hat{P}_{h}^{k-1}(s)}\\big[V_{h}^{*}(s,s^{\\prime})\\big]+\\frac{10}{3}\\sqrt{\\frac{\\operatorname{Var}_{s^{\\prime}\\sim\\hat{P}_{h}(s)}(V_{h}^{*}(s,s^{\\prime}))L_{\\delta}^{k}}{n_{h}^{k-1}(s)\\vee1}}+\\frac{50H L_{\\delta}^{k}}{n_{h}^{k-1}(s)\\vee1}\\quad\\mathrm{(Under~}E^{p e2}(k)\\big)}\\\\ &{\\qquad\\geq\\mathbb{E}_{s^{\\prime}\\sim P_{h}(s)}\\big[V_{h}^{*}(s,s^{\\prime})\\big]}\\\\ &{\\qquad=V_{h}^{*}(s).}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "C.7   The Second Good Event - Martingale Concentration ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "In this subsection, we present three good events that allow replacing the expectation over the randomizations inside each episode by their realization. Let ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Y_{1,h}^{k}:=\\bar{V}_{h+1}^{k}(s_{h+1}^{k})-V_{h+1}^{\\pi^{k}}(s_{h+1}^{k})}\\\\ &{Y_{2,h}^{k}=\\operatorname{Var}_{s^{\\prime}\\sim P_{h}(s_{h}^{k})}(V_{h}^{\\pi^{k}}(s_{h}^{k},s^{\\prime}))}\\\\ &{Y_{3,h}^{k}=b_{k,h}^{r}(s_{h}^{k},a_{h}^{k}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "The second good event is the intersection of the events $\\mathbb{G}_{2}=E^{\\mathrm{diff}}\\cap E^{\\mathrm{Var}}\\cap E^{b r}$ defined as follows. ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{S}^{\\mathrm{diff}}=\\Bigg\\{\\forall h\\in[H],K\\ge1:\\displaystyle\\sum_{k=1}^{K}\\mathbb{E}[Y_{1,h}^{k}|F_{k,h-1}]\\le\\left(1+\\displaystyle\\frac{1}{2H}\\right)\\displaystyle\\sum_{k=1}^{K}Y_{1,h}^{k}+18H^{2}\\ln\\frac{6H K(K+1)}{\\delta}\\Bigg\\},}\\\\ &{\\mathbb{S}^{\\mathrm{var}}=\\Bigg\\{K\\ge1:\\displaystyle\\sum_{k=1}^{K}\\sum_{h=1}^{H}Y_{2,h}^{k}\\le2\\displaystyle\\sum_{k=1}^{K}\\sum_{h=1}^{H}\\mathbb{E}[Y_{2,h}^{k}|F_{k-1}]+4H^{3}\\ln\\frac{6H K(K+1)}{\\delta}\\Bigg\\},}\\\\ &{\\mathbb{S}^{b r}=\\Bigg\\{\\forall h\\in[H],K\\ge1:\\displaystyle\\sum_{k=1}^{K}\\mathbb{E}[Y_{3,h}^{k}|F_{k,h-1}]\\le2\\displaystyle\\sum_{k=1}^{K}Y_{3,h}^{k}+18\\ln\\frac{6H K(K+1)}{\\delta}\\Bigg\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "We define the good event $\\mathbb{G}=\\mathbb{G}_{1}\\cap\\mathbb{G}_{2}$ ", "page_idx": 39}, {"type": "text", "text": "Lemma 14. The good event $\\mathbb{G}$ holds with a probability of at least $1-\\delta$ ", "page_idx": 39}, {"type": "text", "text": "Proof. The analysis of the first event follows $E^{\\mathrm{diff}}$ exactly as the one of $E^{\\mathrm{dif1}}$ in Lemma 7: define $W_{k}\\,=\\,\\Im\\left\\{\\bar{V}_{h}^{k}(s)-V_{h}^{\\pi^{k}}(s)\\in[0,H],\\forall h\\in[H],s\\in{\\cal S}\\right\\}$ (which happens a.s. under $\\mathbb{G}_{1}$ due to the optimism in Lemma 13 and truncation) and $\\tilde{Y}_{1,h}^{k}=W_{k}Y_{1,h}^{k}$ which is bounded in $[0,H]$ and $F_{k,h}$ measurable. The corresponding event w.r.t.this modified variables $\\tilde{E}^{\\mathrm{diff}}$ then holds w.p. $\\textstyle1-{\\frac{\\delta}{6}}$ by Lemma 25, and as in Lemma 7, we can use the fact that $\\mathbb{G}_{1}\\cap\\tilde{E}^{\\mathrm{diff}}=\\mathbb{G}_{1}\\cap E^{\\mathrm{diff}}$ to conclude this part of the proof. ", "page_idx": 39}, {"type": "text", "text": "Moving to the secd event, i $V_{h}^{\\pi^{k}}(s,s^{\\prime})\\in[0,H]$ then Y2, E [0, H3]. Therefore, by Lemma 25 w.rt. th fltration $F_{k}$ with $C=H^{3}$ and any fixed $K$ , wege w,p. $\\begin{array}{r}{1-\\frac{\\delta}{6H K(K+1)}}\\end{array}$ that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{K}\\sum_{h=1}^{H}Y_{2,h}^{k}\\leq2\\sum_{k=1}^{K}\\sum_{h=1}^{H}\\mathbb{E}[Y_{2,h}^{k}|F_{k-1}]+4H^{3}\\ln\\frac{6H K(K+1)}{\\delta}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Taking theuninbound onall possible alus of $K\\geq1$ proves that $E^{\\mathrm{Var}}$ holds w.p. at least $\\textstyle1-{\\frac{\\delta}{6}}$ ", "page_idx": 39}, {"type": "text", "text": "Finally by $Y_{3,h}^{k}=b_{k,h}^{r}(s_{h}^{k},a_{h}^{k})\\in[0,1]$ and is $F_{k,h}$ measurable\u3002 Thus, for any fixed $k\\geq1$ and $h\\in[H]$ using Lemma 25, we have w.p. $\\begin{array}{r}{1-\\frac{\\delta}{6H K(K+1)}}\\end{array}$ that $\\sum_{i=1}^{K}\\mathbb{E}[Y_{3,h}^{k}|F_{k,h-1}]\\leq\\left(1+\\frac{1}{2}\\right)\\sum_{k=1}^{K}Y_{3,h}^{k}+18\\ln\\frac{6H K(K+1)}{\\delta}\\leq2\\sum_{k=1}^{K}Y_{3,h}^{k}+18\\ln\\frac{6H K(K+1)}{\\delta},$ ", "page_idx": 39}, {"type": "text", "text": "so that due to the union bound, $E^{b r}$ holds w.p. $\\textstyle1-{\\frac{\\delta}{6}}$ ", "page_idx": 39}, {"type": "text", "text": "To conclude, $\\mathbb{G}_{1}$ holds w.p. $1-\\frac\\delta2$ (Lemma 5) and the events $\\tilde{E}^{\\mathrm{diff}},E^{\\mathrm{Var}},E^{b r}$ each hold w.p. $1-\\frac{\\delta}{6}$ .As before, when accounting to the fact that $\\tilde{E}^{\\mathrm{diff}}$ and $E^{\\mathrm{diff}}$ are identical under $\\mathbb{G}_{1}$ , the event $G=\\mathbb{G}_{1}\\cap\\mathbb{G}_{2}$ holds w.p. at least $1-\\delta$ \u53e3 ", "page_idx": 39}, {"type": "text", "text": "C.8  Regret Analysis ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Theorem 2. When running MVP-TL, with probability at least $1-\\delta$ uniformlyfor all $K\\geq1$ itholds that $\\begin{array}{r}{\\mathrm{Reg}^{T}(K)\\leq\\mathcal{O}\\Big(\\sqrt{H^{2}S K}\\Big(\\sqrt{H}+\\sqrt{A}\\Big)\\ln\\frac{S A H K}{\\delta}+H^{3}S^{4}A^{3}\\big(\\ln\\frac{S A H K}{\\delta}\\big)^{2}\\Big).}\\end{array}$ ", "page_idx": 40}, {"type": "text", "text": "Proof. Assume that the event $\\mathbb{G}$ holds, which by Lemma 14, happens with probability at least $1\\,-\\,\\delta$ . In particular, throughout the proof, we use optimism (Lemma 13), which implies that $0\\leq V_{h}^{\\pi^{k}}(s,s^{\\prime})\\leq V_{h}^{*}(s,s^{\\prime})\\leq\\bar{V}_{h}^{k}(s,s^{\\prime})\\leq3H$ (the upper bound is also by the truncation), as well as $0\\leq V_{h}^{\\pi^{k}}(s)\\leq V_{h}^{*}(s)\\leq\\bar{V}_{h}^{k}(s)\\leq H$ ", "page_idx": 40}, {"type": "text", "text": "We first focus on lower-bounding the value of the policy $\\pi^{k}$ : by Remark 2, we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{h}^{\\pi^{k}}(s)=\\mathbb{E}_{\\sigma\\sim P_{h}(s)}\\Big[r_{h}(s,\\pi_{k}^{k}(s,s^{\\prime}))+V_{h}^{\\pi_{k}^{k}}(s^{\\prime}(\\pi_{k}^{k}(s,s^{\\prime})))\\Big]}\\\\ &{\\quad\\quad=\\mathbb{E}_{\\sigma\\sim\\mathcal{P}_{h}(s)}\\Big[r_{h}^{k-1}(s,\\pi_{k}^{k}(s,s))+V_{h+1}^{\\pi_{k}}(s^{\\prime}(\\pi_{k}^{k}(s,s^{\\prime})))+V_{h,h}^{\\pi_{k}}(s,\\pi_{k}^{k}(s,s^{\\prime}))\\Big]}\\\\ &{\\quad\\quad\\quad+\\mathbb{E}_{\\sigma\\sim\\mathcal{P}_{h}(s)}\\Big[r_{h}(s,\\pi_{k}^{k}(s,s^{\\prime}))-r_{h}^{k-1}(s,\\pi_{k}^{k}(s,s^{\\prime}))-b_{\\pi,h}^{\\pi_{k}}(s,\\pi_{k}^{k}(s,s^{\\prime}))\\Big]}\\\\ &{\\quad\\quad\\quad+\\mathbb{E}_{\\sigma\\sim\\mathcal{P}_{h}(s)}\\Big[V_{h+1}^{\\pi_{k}}(s^{\\prime}(\\pi_{k}^{k}(s,s^{\\prime})))-\\widehat{V}_{h+1}^{\\pi_{k}}(s^{\\prime}(\\pi_{k}^{k}(s,s^{\\prime})))\\Big]}\\\\ &{\\quad\\quad\\quad\\xrightarrow{(1)}\\mathbb{E}_{\\sigma\\sim\\mathcal{P}_{h}(s)}\\Big[\\operatorname*{max}_{\\bar{\\pi}^{k}}\\Big\\{r_{h}^{k-1}(s,a)+\\bar{V}_{h+1}^{\\pi_{k}}(s^{\\prime}(a))+b_{\\pi,h}^{\\pi_{k}}(s,a)\\Big\\}\\Big]}\\\\ &{\\quad\\quad\\quad+\\mathbb{E}_{\\sigma\\sim\\mathcal{P}_{h}(s)}\\Big[r_{h}(s,\\pi_{k}^{k}(s,s^{\\prime}))-\\bar{r}_{h}^{k-1}(s,\\pi_{k}^{k}(s,s))-b_{\\pi,h}^{\\pi_{k}}(s,\\pi_{k}^{k}(s,s^{\\prime}))\\Big]}\\\\ &{\\quad\\quad\\quad+\\mathbb{E}_{\\sigma\\sim\\mathcal{P}_{h}(\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where (1) is by the definition of $\\pi^{k}$ and (2) uses the reward concentration event. Thus, we can write ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\gamma_{h}^{k}(s)-V_{h}^{\\pi^{k}}(s)\\leq\\mathbb{E}_{s^{\\prime}\\sim\\hat{P}_{h}^{k-1}(s)}\\big[\\overline{{V}}_{h}^{k}(s,s^{\\prime})\\big]-\\mathbb{E}_{s^{\\prime}\\sim P_{h}(s)}\\big[\\overline{{V}}_{h}^{k}(s,s^{\\prime})\\big]+2\\mathbb{E}_{s^{\\prime}\\sim P_{h}(s)}\\big[b_{k,h}^{r}(s,\\pi_{h}^{k}(s,s^{\\prime}))\\big]}\\\\ &{\\qquad\\qquad\\qquad+\\mathbb{E}_{s^{\\prime}\\sim P_{h}(s)}\\Big[\\bar{V}_{h+1}^{k}(s^{\\prime}(\\pi_{h}^{k}(s,s^{\\prime})))-V_{h+1}^{\\pi^{k}}(s^{\\prime}(\\pi_{h}^{k}(s,s^{\\prime})))\\Big]+b_{k,h}^{p}(s)}\\\\ &{=\\underbrace{\\mathbb{E}_{s^{\\prime}\\sim\\hat{P}_{h}^{k-1}(s)}\\big[\\bar{V}_{h}^{k}(s,s^{\\prime})-V_{h}^{*}(s,s^{\\prime})\\big]-\\mathbb{E}_{s^{\\prime}\\sim P_{h}(s)}\\big[\\bar{V}_{h}^{k}(s,s^{\\prime})-V_{h}^{*}(s,s^{\\prime})\\big]+b_{k,h}^{p}(s)}_{(i)}}\\\\ &{\\qquad\\qquad+\\underbrace{\\mathbb{E}_{s^{\\prime}\\sim P_{h}(s)}\\big[V_{h}^{*}(s,s^{\\prime})\\big]-\\mathbb{E}_{s^{\\prime}\\sim\\hat{P}_{h}^{k-1}(s)}\\big[V_{h}^{*}(s,s^{\\prime})\\big]}_{(i i)}+2\\mathbb{E}_{s^{\\prime}\\sim P_{h}(s)}\\big[b_{k,h}^{r}(s,\\pi_{h}^{k}(s,s^{\\prime}))\\big]}\\\\ &{\\qquad\\qquad\\qquad+\\underbrace{\\mathbb{E}_{s^{\\prime}\\sim P_{h}(s)}\\Big[\\bar{V}_{h+1}^{k}(s^{\\prime}(\\pi_{h}^{k}(s,s^{\\prime})))-V_{h+1}^{\\pi^{k}}(s^{\\prime}(\\pi_{h}^{k}(s,s^{\\prime})))\\Big] \n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Bounding term $(i i)$ : using the concentration event $E^{p v1}(k)$ , we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{i i)\\leq\\sqrt{\\frac{2\\mathrm{Var}_{s^{\\prime}\\sim P_{h}(s)}(V_{h}^{*}(s,s^{\\prime}))L_{\\delta}^{k}}{n_{h}^{k-1}(s)\\,\\vee\\,1}}+\\frac{H L_{\\delta}^{k}}{n_{h}^{k-1}(s)\\,\\vee\\,1}}\\\\ &{\\ \\ \\ \\overset{(1)}{\\leq}\\sqrt{\\frac{2\\mathrm{Var}_{s^{\\prime}\\sim P_{h}(s)}(V_{h}^{\\pi k}(s,s^{\\prime}))L_{\\delta}^{k}}{n_{h}^{k-1}(s)\\,\\vee\\,1}}+\\frac{1}{8H}\\mathbb{E}_{s^{\\prime}\\sim P_{h}(s)}\\Big[V_{h}^{\\pi^{k}}(s,s^{\\prime})-V_{h}^{\\pi_{k}}(s,s^{\\prime})\\Big]+\\frac{4H^{2}L_{\\delta}^{k}}{n_{h}^{k-1}(s)\\,\\vee\\,1}+}\\\\ &{\\ \\ \\ \\overset{(2)}{\\leq}\\sqrt{\\frac{2\\mathrm{Var}_{s^{\\prime}\\sim P_{h}(s)}(V_{h}^{\\pi k}(s,s^{\\prime}))L_{\\delta}^{k}}{n_{h}^{k-1}(s)\\,\\vee\\,1}}+\\frac{1}{8H}\\mathbb{E}_{s^{\\prime}\\sim P_{h}(s)}\\big[\\bar{V}_{h}^{k}(s,s^{\\prime})-V_{h}^{\\pi_{k}}(s,s^{\\prime})\\big]+\\frac{5H^{2}L_{\\delta}^{k}}{n_{h}^{k-1}(s)\\,\\vee\\,1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Relation (1) uses Lemma 21 with the values $0\\leq V_{h}^{\\pi^{k}}(s,s^{\\prime})\\leq V_{h}^{*}(s,s^{\\prime})\\leq H$ with $\\alpha=8H\\cdot\\sqrt{2L_{\\delta}^{k}}$ and (2) is by optimism. ", "page_idx": 40}, {"type": "text", "text": "Bounding term $(i)$ : We first focus on the transition bonus; to bound it, we apply Lemma 22 w.r.t. $\\hat{P}_{h}^{k-1}(\\pmb{\\mathscr{s}}^{\\prime}|\\pmb{\\mathscr{s}}),P_{h}(\\pmb{\\mathscr{s}}^{\\prime}|\\pmb{\\mathscr{s}})$ , the values $0\\,\\le\\,V_{h}^{\\pi^{k}}(s,s^{\\prime})\\,\\le\\,V_{h}^{*}(s,s^{\\prime})\\,\\le\\,\\bar{V}_{h}^{k}(s,s^{\\prime})\\,\\le\\,3H$ (by optimism), under the event $E^{p v2}(k)$ and with $\\alpha=8H\\cdot\\frac{20}{3}\\sqrt{L_{\\delta}^{k}}$ ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\frac{p}{k_{k}h}(s)=\\frac{20}{3}\\sqrt{\\frac{\\operatorname{Var}_{s^{\\prime}\\sim\\hat{P}_{h}^{k-1}(s)}\\left(\\bar{V}_{h}^{k}(s,s^{\\prime})\\right)L_{\\delta}^{k}}{n_{h}^{k-1}(s)\\vee1}}+\\frac{400}{3}\\frac{H L_{\\delta}^{k}}{n_{h}^{k-1}(s)\\vee1}}}\\\\ &{\\leq\\frac{1}{8H}\\mathbb{E}_{s^{\\prime}\\sim\\hat{P}_{h}^{k-1}(s)}\\big[\\bar{V}_{h}^{k}(s,s^{\\prime})-V_{h}^{\\ast}(s,s^{\\prime})\\big]+\\frac{1}{8H}\\mathbb{E}_{s^{\\prime}\\sim P_{h}(s)}\\big[V_{h}^{\\ast}(s,s^{\\prime})-V_{h}^{\\ast}(s,s^{\\prime})\\big]}\\\\ &{\\quad+\\frac{20}{3}\\sqrt{\\frac{\\operatorname{Var}_{s^{\\prime}\\sim P_{h}(s)}\\left(V_{h}^{\\ast}(s,s^{\\prime})\\right)L_{\\delta}^{k}}{n_{h}^{k-1}(s)\\vee1}}+\\frac{1600I^{2}}{3n_{h}^{k-1}(s)\\vee1}+\\frac{20}{3}\\frac{4H L_{\\delta}^{k}}{n_{h}^{k-1}(s)\\vee1}+\\frac{400}{3}\\frac{H L_{\\delta}^{k}}{n_{h}^{k-1}(s)\\vee1}}\\\\ &{\\leq\\frac{1}{8H}\\Big(\\mathbb{E}_{s^{\\prime}\\sim\\hat{P}_{h}^{k-1}(s)}\\big[\\bar{V}_{h}^{k}(s,s^{\\prime})-V_{h}^{\\ast}(s,s^{\\prime})\\big]-E_{s^{\\prime}\\sim P_{h}(s)}\\big[\\bar{V}_{h}^{k}(s,s^{\\prime})-V_{h}^{\\ast}(s,s^{\\prime})\\big]\\Big)}\\\\ &{\\quad+\\frac{1}{8H}\\mathbb{E}_{s^{\\prime}\\sim P_{h}(s)}\\big[\\bar{V}_{h}^{k}(s,s^{\\prime})-V_{h}^{\\pi_{k}}(s,s^{\\prime})\\big]+\\frac{20}{3}\\sqrt{\\frac{\\operatorname{Var}_{s^{\\\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Substituting back to term $(i)$ , we now have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{i\\le\\left(1+\\displaystyle\\frac{1}{8H}\\right)\\left(\\mathbb{E}_{s^{\\prime}\\sim\\hat{P}_{h}^{k-1}(s)}\\left[\\bar{V}_{h}^{k}(s,s^{\\prime})-V_{h}^{*}(s,s^{\\prime})\\right]-E_{s^{\\prime}\\sim P_{h}(s)}\\left[\\bar{V}_{h}^{k}(s,s^{\\prime})-V_{h}^{*}(s,s^{\\prime})\\right]\\right)}}\\\\ {{\\qquad+\\displaystyle\\frac{1}{8H}\\mathbb{E}_{s^{\\prime}\\sim P_{h}(s)}\\left[\\bar{V}_{h}^{k}(s,s^{\\prime})-V_{h}^{\\pi_{k}}(s,s^{\\prime})\\right]+\\displaystyle\\frac{20}{3}\\sqrt{\\frac{\\mathrm{Var}_{s^{\\prime}\\sim P_{h}(s)}\\left(V_{h}^{\\pi^{k}}(s,s^{\\prime})\\right)L_{\\delta}^{k}}{n_{h}^{k-1}(s)\\vee1}}+\\displaystyle\\frac{700H^{2}L_{\\delta}^{k}}{n_{h}^{k-1}(s)\\vee1}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "The next step in the proof involves bounding the first term of $(i)$ . At this point, we remind that both values can be written as $\\bar{V}_{h}^{k}(s,s^{\\prime})=\\operatorname*{max}_{a}\\bar{V}_{h}^{k}(s,s^{\\prime}(a),a)$ and $V_{h}^{*}(s,s^{\\prime})=\\operatorname*{max}_{a}V_{h}^{*}(s,s^{\\prime}(a),a)$ inducing the lists $\\bar{\\ell}=\\bar{\\ell}_{h}^{k}(s)$ and $\\ell^{*}=\\ell_{h}^{*}(s)$ , respectively; thus the expectations can be written as (see Appendix C.4 for further details on the list representation, and in particular, Equation (11)): ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{s^{\\prime}\\sim\\hat{P}_{h}^{k-1}(s)}\\big[\\hat{V}_{h}^{k}(s,s^{\\prime})-V_{h}^{*}(s,s^{\\prime})\\big]-\\mathbb{E}_{s^{\\prime}\\sim P_{h}(s)}\\big[\\hat{V}_{h}^{k}(s,s^{\\prime})-V_{h}^{*}(s,s^{\\prime})\\big]}\\\\ &{\\overset{(1)}{=}\\mathbb{E}_{i,j\\sim\\hat{\\mu}_{h}^{k}(\\cdot\\vert s,\\cdot\\vert^{\\varepsilon})}\\Big[\\hat{V}_{h}^{k}(s,s_{\\xi(i)}^{\\prime},a_{\\tilde{\\xi}(i)})-V_{h}^{*}(s,s_{\\xi^{\\prime}(j)}^{\\prime},a_{\\tilde{\\xi}(i)})\\Big]}\\\\ &{\\quad-\\mathbb{E}_{i,j\\sim\\mu_{h}^{\\varepsilon}(\\cdot\\vert\\tilde{\\xi},\\cdot\\vert\\tilde{\\xi},\\cdot\\vert s_{\\theta})}\\Big[\\hat{V}_{h}^{k}(s,s_{\\xi(i)}^{\\prime},a_{\\tilde{\\xi}(i)})-V_{h}^{*}(s,s_{\\xi^{\\prime}(j)}^{\\prime},a_{\\tilde{\\xi}(i)})\\Big]}\\\\ &{\\overset{(2)}{\\leq}\\frac{1}{8H}\\mathbb{E}_{i,j\\sim\\mu_{\\ t}(\\cdot\\vert\\tilde{\\xi},\\cdot\\vert\\tilde{\\xi},\\cdot\\vert s_{\\theta})}\\Big[\\hat{V}_{h}^{k}(s,s_{\\xi(i)}^{\\prime},a_{\\tilde{\\xi}(i)})-V_{h}^{*}(s,s_{\\xi^{\\prime}(j)}^{\\prime},a_{\\tilde{\\xi}(i)})\\Big]+\\frac{3H(S A)^{2}L_{k}^{k}(2S A+8H}{n_{h}^{k-1}(s)\\vee1}}\\\\ &{\\overset{(1)}{\\leq}\\frac{1}{8H}\\mathbb{E}_{s^{\\prime}\\sim P_{h}(s)}\\big[\\hat{V}_{h}^{k}(s,s^{\\prime})-V_{h}^{*}(s,s^{\\prime})\\big]+\\frac{30H^{2}(S A)^{3}L_{k}^{k}}{n_{h}^{k-1}(s)\\vee1}}\\\\ &{\\leq\\frac{1}{8H}\\mathbb{E}_{s^{\\prime}\\sim P_{h}(s)\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Relations (1) formulate the expectation using the list representations and backward, as done in Equation (11). For inequality (2) we rely on Lemma 24 with $\\alpha=8H$ under the event $E^{\\ell}(k)$ and the optimism, which ensures that the value difference is bounded in $[0,3H]$ . We also remark that the support of the distributions is of size $(S A)^{2}$ ; were we to use the same result on the distributions $\\hat{P}_{h}^{k-1}(s)$ and $P_{h}(s)$ thespport would be of size $S^{A}$ whiw diald factor. And so, we finally have a bound of ", "page_idx": 41}, {"type": "equation", "text": "$$\ni)\\leq\\frac{3}{8H}\\mathbb{E}_{s^{\\prime}\\sim P_{h}(s)}\\big[\\Bar{V}_{h}^{k}(s,s^{\\prime})-V_{h}^{\\pi_{k}}(s,s^{\\prime})\\big]+\\frac{20}{3}\\sqrt{\\frac{\\mathrm{Var}_{s^{\\prime}\\sim P_{h}(s)}\\big(V_{h}^{\\pi^{k}}(s,s^{\\prime})\\big)L_{\\delta}^{k}}{n_{h}^{k-1}(s)\\vee1}+\\frac{735H^{2}(S A)^{3}L_{\\delta}^{k-1}}{n_{h}^{k-1}(s)\\vee1}}\\quad.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{-V_{h}^{\\pi^{k}}(s)\\leq\\frac{1}{2H}\\mathbb{E}_{s^{\\prime}\\sim P_{h}(s)}\\big[\\bar{V}_{h}^{k}(s,s^{\\prime})-V_{h}^{\\pi_{k}}(s,s^{\\prime})\\big]+9\\sqrt{\\frac{\\mathrm{Var}_{s^{\\prime}\\sim P_{h}(s)}(V_{h}^{\\pi^{k}}(s,s^{\\prime}))L_{\\delta}^{k}}{n_{h}^{k-1}(s)\\vee1}}+\\frac{750H^{2}(S_{h}^{\\pi_{k}}(s,s^{\\prime}))L_{\\delta}^{k}}{n_{h}^{k-1}(s)}}\\\\ &{}&{+\\,2\\mathbb{E}_{s^{\\prime}\\sim P_{h}(s)}\\big[b_{k,h}^{r}(s,\\pi_{h}^{k}(s,s^{\\prime}))\\big]+\\mathbb{E}_{s^{\\prime}\\sim P_{h}(s)}\\Big[\\bar{V}_{h+1}^{k}(s^{\\prime}(\\pi_{h}^{k}(s,s^{\\prime})))-V_{h+1}^{\\pi^{k}}(s^{\\prime}(\\pi_{h}^{k}(s,s^{\\prime})))\\Big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "and further bounding (using the concentration event $E^{r}(k)$ ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{V}_{h}^{k}(s,s^{\\prime}))-V_{h}^{\\pi^{k}}(s,s^{\\prime})=\\hat{r}_{h}^{k-1}(s,\\pi_{h}^{k}(s,s^{\\prime}))+b_{k,h}^{r}(s,\\pi_{h}^{k}(s,s^{\\prime}))+\\bar{V}_{h+1}^{k}(s^{\\prime}(\\pi_{h}^{k}(s,s^{\\prime})))}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\quad-r_{h}^{k-1}(s,\\pi_{h}^{k}(s,s^{\\prime}))-V_{h+1}^{\\pi^{k}}(s^{\\prime}(\\pi_{h}^{k}(s,s^{\\prime})))}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\bar{V}_{h+1}^{k}(s^{\\prime}(\\pi_{h}^{k}(s,s^{\\prime})))-V_{h+1}^{\\pi^{k}}(s^{\\prime}(\\pi_{h}^{k}(s,s^{\\prime})))+2b_{k,h}^{r}(s,\\pi_{h}^{k}(s,s^{\\prime})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "we finally get the decomposition ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{\\gamma}_{h}^{k}(s)-{V_{h}^{\\pi^{k}}(s)}\\le\\bigg(1+\\displaystyle\\frac{1}{2H}\\bigg)\\mathbb{E}_{s^{\\prime}\\sim P_{h}(s)}\\bigg[\\bar{V}_{h+1}^{k}\\big(s^{\\prime}(\\pi_{h}^{k}(s,s^{\\prime}))\\big)-{V_{h+1}^{\\pi^{k}}\\big(s^{\\prime}(\\pi_{h}^{k}(s,s^{\\prime}))\\big)\\Big]}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\displaystyle9\\sqrt{\\frac{\\mathrm{Var}_{s^{\\prime}\\sim P_{h}(s)}({V_{h}^{\\pi^{k}}(s,s^{\\prime})})L_{\\delta}^{k}}{n_{h}^{k-1}(s)\\vee1}}+\\displaystyle\\frac{750H^{2}(S A)^{3}L_{\\delta}^{k}}{n_{h}^{k-1}(s)\\vee1}+3\\mathbb{E}_{s^{\\prime}\\sim P_{h}(s)}\\big[b_{k,h}^{r}\\big(s,\\pi_{h}^{k}(s,s^{\\prime})\\big)\\big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Atthis point we choose to take $s=s_{h}^{k}$ and sum over all $k\\in[K]$ ; specificaly for $\\pmb{s}^{\\prime}=\\pmb{s}_{h+1}^{\\prime k}$ ,the action becomes $\\pi_{h}^{k}(s,s^{\\prime})=a_{h}^{k}$ and $s^{\\prime}(\\pi_{h}^{k}(s,s^{\\prime}))=s_{h+1}^{k}$ Formally, we can write the bound as ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{=1}^{K}\\bar{V}_{h}^{k}(s_{h}^{k})-V_{h}^{\\pi^{k}}(s_{h}^{k})\\leq\\bigg(1+\\frac{1}{2H}\\bigg)\\displaystyle\\sum_{k=1}^{K}\\mathbb{E}\\Big[\\bar{V}_{h+1}^{k}(s_{h+1}^{k})-V_{h+1}^{\\pi^{k}}(s_{h+1}^{k})\\lvert F_{k,h-1}\\Big]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+3\\displaystyle\\sum_{k=1}^{K}\\mathbb{E}\\big[b_{k,h}^{r}(s_{h}^{k},a_{h}^{k})\\lvert F_{k,h-1}\\big]+9\\displaystyle\\sum_{k=1}^{K}\\sqrt{\\frac{\\mathrm{Var}_{s^{\\prime}\\sim P_{h}(s_{h}^{k})}\\big(V_{h}^{\\pi^{k}}(s_{h}^{k},s^{\\prime})\\big)L_{\\delta}^{k}}{n_{h}^{k-1}\\big(s_{h}^{k}\\big)\\vee1}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\displaystyle\\sum_{k=1}^{K}\\frac{750H^{2}(S A)^{3}L_{\\delta}^{k}}{n_{h}^{k-1}(s_{h}^{k})\\vee1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "and, in particular, nder the events $E^{\\mathrm{diff}}$ and $E^{b r}$ , it holds that ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{=1}^{K}\\bar{V}_{h}^{k}(s_{h}^{k})-V_{h}^{\\pi^{k}}(s_{h}^{k})\\leq\\bigg(1+\\frac{1}{2H}\\bigg)^{2}\\displaystyle\\sum_{k=1}^{K}\\biggr(\\bar{V}_{h+1}^{k}(s_{h+1}^{k})\\bigr)-V_{h+1}^{\\pi^{k}}(s_{h+1}^{k})\\bigg)+36H^{2}\\ln\\frac{6H K(K+1)^{3}}{\\delta}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\displaystyle3\\sum_{k=1}^{K}b_{k,h}^{r}(s_{h}^{k},a_{h}^{k})+54\\ln\\frac{6H K(K+1)}{\\delta}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\displaystyle9\\displaystyle\\sum_{k=1}^{K}\\sqrt{\\frac{\\mathrm{Var}_{s^{\\prime}\\sim P_{h}(s_{h}^{k})}(V_{h}^{\\pi^{k}}(s_{h}^{k},s^{\\prime}))L_{\\delta}^{k}}{n_{h}^{k-1}(s_{h}^{k})\\vee1}}+\\displaystyle\\sum_{k=1}^{K}\\frac{750H^{2}(S A)^{3}L_{\\delta}^{k}}{n_{h}^{k-1}(s_{h}^{k})\\vee1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "To conclude the proof, we recursively apply this formula from $h=1$ to $h=H+1$ (where the values are zero) and use the optimism. This yields ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Re}_{\\mathrm{pr}}^{\\tau}\\tau(K)=\\underset{k=1}{\\overset{N}{\\sum}}V_{1}^{\\tau}(s_{k}^{\\tt A})-V_{1}^{\\tau,k}(s_{k}^{\\tt A})}\\\\ &{\\leq\\underset{k=1}{\\overset{N}{\\sum}}V_{1}^{\\tau_{k}}(s_{k}^{\\tt A})-V_{1}^{\\tau,k}(s_{k}^{\\tt A})}\\\\ &{\\overset{\\mathrm{QB}}{\\leq}\\left(1+\\frac{1}{2H}\\right)^{2H}\\underset{k=1}{\\overset{N}{\\sum}}\\underset{\\underset{k=1}{\\overset{N}{\\sum}}}{\\sum}\\frac{\\sqrt{N}\\pi_{k}^{\\tau}-\\underset{k=1}{\\overset{N}{\\sum}}\\zeta_{k}^{\\tau,k}(s_{k}^{\\tt A})\\overset{V_{k}^{\\tau,k}}{\\underset{k=1}{\\prod}}}{\\sqrt{\\pi_{k}^{\\tau}-\\underset{k=1}{\\overset{N}{\\sum}}\\zeta_{k}^{\\tau,k}(s_{k}^{\\tt A})\\overset{V_{k}^{\\tau,k}}{\\underset{k=1}{\\prod}}}}}\\\\ &{\\quad+3\\bigg(1+\\frac{1}{2H}\\bigg)^{2H}\\underset{k=1}{\\overset{N}{\\sum}}\\underset{\\underset{k=1}{\\overset{N}{\\sum}}}{\\sum}\\sqrt{\\frac{N}{\\underset{k=1}{2H}\\underset{k=1}{\\overset{N}{\\sum}}}{\\underset{k=1}{\\overset{N}{\\sum}}}{\\underset{k=1}{\\overset{N}{\\sum}}}{\\mathrm{T}}}1}\\\\ &{\\quad+\\bigg(1+\\frac{1}{2H}\\bigg)^{2H}\\underset{k=1}{\\overset{N}{\\sum}}\\underset{n=1}{\\overset{N}{\\sum}}{\\frac{\\underset{k=1}{\\overset{N}{\\sum}}}{\\mathrm{T}}{\\sum}}{\\frac{\\sqrt{N}H^{2}(K_{4}^{\\tt A})\\overset{T_{A}}{\\leq}}{\\mathrm{T}}}+9n H^{2}\\bigg(1+\\frac{1}{2H}\\bigg)^{2H}\\underset{k=1}{\\overset{\\theta}{\\sum}}{\\mathrm{B}}\\frac{6H K(K+1)}{\\delta}}\\\\ &{\\overset{\\mathrm{QB}}{\\leq}\\mathrm{Sm}\\overline{{W^{\\tau}}}\\mathrm{S}L_{k}^{\\tau,k}+6\\mathrm\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Relation (1) is the recursive application of the difference alongside substitution of the reward bonuses, while relation (2) is by Lemma 15 and Lemma 20. ", "page_idx": 43}, {"type": "text", "text": "C.8.1 Lemmas for Bounding Bonus Terms ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Lemma 15. Under the event $E^{\\mathrm{Var}}$ it holds that ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{K}\\sum_{h=1}^{H}\\frac{\\sqrt{\\operatorname{Var}_{s^{\\prime}\\sim P_{h}(s_{h}^{k})}\\big(V_{h}^{\\pi^{k}}(s_{h}^{k},s^{\\prime})\\big)}}{\\sqrt{n_{h}^{k-1}\\big(s_{h}^{k}\\big)\\vee1}}\\leq2\\sqrt{H^{3}S K L_{\\delta}^{K}}+\\sqrt{8S}H^{2}L_{\\delta}^{K}.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Proof. Similar to Lemma 9, we again rely on the lookahead version of the law of total variation to prove this bound. First, by Cauchy-Schwartz inequality, it holds that ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\sum_{=1}^{K}\\sum_{h=1}^{H}\\frac{\\sqrt{\\mathrm{Var}_{s^{\\prime}\\sim P_{h}(s_{h}^{k})}(V_{h}^{\\pi^{k}}(s_{h}^{k},s^{\\prime}))}}{\\sqrt{n_{h}^{k-1}(s_{h}^{k})\\vee1}}\\leq\\sqrt{\\sum_{k=1}^{K}\\sum_{h=1}^{H}\\mathrm{Var}_{s^{\\prime}\\sim P_{h}(s_{h}^{k})}(V_{h}^{\\pi^{k}}(s_{h}^{k},s^{\\prime}))}\\sqrt{\\sum_{k=1}^{K}\\sum_{h=1}^{H}\\frac{1}{n_{h}^{k-1}(s_{h}^{k})}}\\times\\frac{1}{{n_{h}}}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "We use Lemma 20 to bound the second term by ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{K}\\sum_{h=1}^{H}\\frac{1}{n_{h}^{k-1}(s_{h}^{k})\\vee1}\\le S H(2+\\ln(K))\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "and focus on bounding the first term. Under $E^{\\mathrm{Var}}$ ,wehave ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k=1}^{K}\\sum_{h=1}^{H}\\operatorname{Var}_{s^{\\prime}\\sim P_{h}(s_{h}^{k})}(V_{h}^{\\pi^{k}}(s_{h}^{k},s^{\\prime}))}\\\\ &{\\le2\\displaystyle\\sum_{k=1}^{K}\\mathbb{E}\\left[\\sum_{h=1}^{H}\\operatorname{Var}_{s^{\\prime}\\sim P_{h}(s_{h}^{k})}(V_{h}^{\\pi^{k}}(s_{h}^{k},s^{\\prime}))|F_{k-1}\\right]+4H^{3}\\ln\\frac{6H K(K+1)}{\\delta}\\qquad\\mathrm{(Under~}E^{\\mathrm{Var}})}\\\\ &{=2\\displaystyle\\sum_{k=1}^{K}\\mathbb{E}\\left[\\left(\\sum_{h=1}^{H}r_{h}(s_{h}^{k},a_{h}^{k})-V_{1}^{\\pi^{k}}(s_{1}^{k})\\right)^{2}|F_{k-1}\\right]+4H^{3}\\ln\\frac{6H K(K+1)}{\\delta}\\quad\\mathrm{(By~Lemma~10)}}\\\\ &{\\le2H^{2}K+4H^{3}\\ln\\frac{6H K(K+1)}{\\delta},}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where the last inequality is since both the values and cumulative rewards are bounded in $[0,H]$ Combining both, we get ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{\\leq\\sum_{h=1}^{K}\\frac{H}{\\sqrt{n}^{k^{\\prime}\\sim P_{h}(s_{h}^{k})}}(V_{h}^{\\pi^{k}}(s_{h}^{k},s^{\\prime}))}{\\sqrt{n_{h}^{k-1}(s_{h}^{k})\\vee1}}\\leq\\sqrt{2H^{2}K+4H^{3}\\ln\\frac{6H K(K+1)}{\\delta}}\\sqrt{S H(2+\\ln(K))}}}\\\\ &{\\leq\\sqrt{2H^{2}K+4H^{3}\\ln\\frac{6H K(K+1)}{\\delta}}\\sqrt{2S H\\ln\\frac{6H K(K+1)}{\\delta}}}\\\\ &{\\leq2\\sqrt{H^{3}S K L_{\\delta}^{K}}+\\sqrt{8S}H^{2}L_{\\delta}^{K}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "image", "img_path": "wlqfOvlTQz/tmp/97e3c8d9f5d2758c3ae119907c54ceb8c3223d71f7fa554ea8e7bde8198e8eb3.jpg", "img_caption": ["C.9Example: Value Gain due to Transition Lookahead ", "Figure 2: Random chain: agents start at the left side and must reach its right side to collect a reward. "], "img_footnote": [], "page_idx": 45}, {"type": "text", "text": "We now present in further detail the example described at Section 3. This example is inspired by the one in Appendix C.3 in [Merlis et al., 2024], greatly simplifying it and achieving similar behavior for a much smaller environment. ", "page_idx": 45}, {"type": "text", "text": "Agents start at the left side of a chain of length $H/2$ (depicted in Figure 2) and have two options: ", "page_idx": 45}, {"type": "text", "text": "1. Play a safe action $a_{1}$ that leaves the agent in the same state (in green), or, 2. play one of the $A\\!-\\!1$ risky actions $a_{2},\\dots,a_{A}$ (in red). Each of these actions moves the agent forward in the chain w.p. $\\frac{1}{A-1}$ , but leads to a terminal non-rewarding state w.p. $1-\\frac{\\breve{1}}{A-1}$ A-1\u00b7 ", "page_idx": 45}, {"type": "text", "text": "At the end of the chain, the last state is an absorbing state with a unit reward. ", "page_idx": 45}, {"type": "text", "text": "Without lookahead, all agents can do is try to randomly reach the end of the chain, succeeding with probability $(A-1)^{-H/2}$ . In particular, such agents cannot collect more than $V^{n o}\\leq H(A-1)^{-H/2}$ On the other hand, with transition lookahead, agents observe whether the risky actions allow moving forward in the chain or lead to the bad terminal state. If one action allows progressing in the chain (Wwhich hapens w,p. $\\begin{array}{r}{p=1-\\left(1-\\frac{1}{A-1}\\right)^{A-1}\\geq1-^{1}\\!/e)}\\end{array}$ , a lookahead agent would take it, and otherwise, they will use $a_{1}$ to remain in the same state. In other words, optimal lookahead agents reach the reward after $H/2-1$ successful ' progression steps\u2032 with probability $p$ each. The probability of reaching the end of the chain using less than $5H/6$ steps is at least ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left({\\mathrm{Bin}}{\\left({\\frac{5H}{6}}-1,1-{\\frac{1}{e}}\\right)}>{\\frac{H}{2}}\\right)\\geq c_{0},\\quad{\\mathrm{for~some~absolute~}}c_{0}>0.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Under this event, the agent collets $\\frac{H}{6}$ rewards stlaheadal aet $\\begin{array}{r}{V^{T,*}\\ge\\frac{c_{0}H}{6}\\!=\\!\\Omega(H)}\\end{array}$ ", "page_idx": 45}, {"type": "text", "text": "To summarize, for this example, no lookahead optimal value is at $\\mathrm{most}\\approx H A^{-H/2}$ , while transition lookahead agents can collect a value of $\\approx\\,H$ : transition lookahead increases the value by an exponential multiplicative factor. The difference between the two values is $G^{T}\\;=\\;\\Omega(H)$ ,and following the discussion in Section 3, a sublinear transition lookahead regret would imply a negatively linear standard regret of $\\mathrm{Reg}(K)\\lesssim-H K$ ", "page_idx": 45}, {"type": "text", "text": "Remark 3. The chain length was chosen to be $H/2$ for simplicity-similar conclusions canbe achievedfor alengthof $\\approx1-{}^{1/}e$ .Then,themultiplicativeincreaseinvalue due to transition lookahead would $b e\\approx(A-1)^{\\left(1-\\frac{1}{e}\\right)H}$ matching Proposition 2 in [Merlis et al., 2024]. Infact, setting the transitionfrom the last state of the chain to the terminal state(rendering it possible toearn only one unit of reward), the analysis coincides with the one in [Merlis et al., 2024]. Following their exact derivation, the valuewith lookahead information is multiplicatively larger than its no-lookahead factor by an exponential factor of $\\Theta\\Big((A-1)^{\\operatorname*{min}\\{\\left(1-\\frac{1}{e}\\right)H-1,S\\}-2}\\Big)$ This significantly improves the result in [Merlis et al., 2024], that only holds if $S\\geq A^{\\left(1-{\\frac{1}{e}}\\right)H}$ ", "page_idx": 45}, {"type": "text", "text": "D Auxiliary Lemmas ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "In this appendix, we prove various auxiliary lemma that will be used throughout our proofs. ", "page_idx": 46}, {"type": "text", "text": "D.1 Concentration results ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "We first present and reprove a set of well-known concentration results. ", "page_idx": 46}, {"type": "text", "text": "Lemma 16. Let $P$ be a distribution over a discrete set $\\mathcal{X}$ of size $|\\mathcal{X}|=M$ and let $X,X_{1},\\ldots,X_{n}$ be independent samples from this distribution.Also, let $U:\\mathcal{X}\\mapsto[0,C]$ for some $C>0$ and define the empirical distribution $\\begin{array}{r}{\\hat{P}_{n}(x)=\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{I}\\{x_{i}=x\\}}\\end{array}$ Then, for any $\\delta\\in(0,1).$ each of the following events hold $w.p$ at least $1-\\delta$ ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{E^{p}=\\Bigg\\{\\forall x\\in\\mathcal{X},|P(x)-\\hat{P}_{n}(x)|\\leq\\sqrt{\\frac{2P(x)\\ln\\frac{2M}{\\delta}}{n}}+\\frac{2\\ln\\frac{2M}{\\delta}}{3n}\\Bigg\\}}\\\\ &{E^{p v1}=\\Bigg\\{\\Bigg|\\displaystyle\\sum_{x\\in\\mathcal{X}}\\Big(\\hat{P}_{n}(x)-P(x)\\Big)U(x)\\Bigg|\\leq\\sqrt{\\frac{2\\mathrm{Var}_{P}(U(X))\\ln\\frac{2}{\\delta}}{n}}+\\frac{2C\\ln\\frac{2}{\\delta}}{3n}\\Bigg\\}}\\\\ &{E^{p v2}=\\Bigg\\{\\Big|\\sqrt{\\mathrm{Var}_{\\hat{P}_{n}}(U(X))}-\\sqrt{\\mathrm{Var}_{P}(U(X))}\\Big|\\leq4C\\sqrt{\\frac{\\ln\\frac{2}{\\delta}}{n\\,\\mathrm{V}\\,1}}\\Bigg\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "where $\\begin{array}{r}{\\operatorname{Var}_{P}(U(X))=\\sum_{x\\in\\mathcal{X}}P(x)U(x)^{2}-\\big(\\sum_{x\\in\\mathcal{X}}P(x)U(x)\\big)^{2}.}\\end{array}$ ", "page_idx": 46}, {"type": "text", "text": "Proof. All the results require standard probability arguments and are stated for completeness. ", "page_idx": 46}, {"type": "text", "text": "For the first event $E^{p}$ , notice that each of the components $\\hat{P}_{n}(x)$ is the empirical mean of independent Bernoulli random variables $X_{i}(x)$ of mean $P(x)$ . Therefore, by Bernstein's inequality, recalling that the variance of the variable $B e r(p)$ .is $p(1-p)$ , we get w.p. at least $1-\\frac{\\delta}{M}$ that ", "page_idx": 46}, {"type": "equation", "text": "$$\n|P(x)-\\hat{P}_{n}(x)|\\leq\\sqrt{\\frac{2P(x)(1-P(x))\\ln\\frac{2M}{\\delta}}{n}}+\\frac{2\\ln\\frac{2M}{\\delta}}{3n}\\leq\\sqrt{\\frac{2P(x)\\ln\\frac{2M}{\\delta}}{n}}+\\frac{2\\ln\\frac{2M}{\\delta}}{3n}.\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Taking the union bound over all $x\\in\\mathscr{X}$ implies that $E^{p}$ holds w.p. at least $1-\\delta$ ", "page_idx": 46}, {"type": "text", "text": "For the second event $E^{p v1}$ , we apply Bernstein's inequality on the variables $Y_{i}\\,=\\,U(X_{i})$ The empirical mean is given by $\\begin{array}{r}{\\hat{Y}_{n}\\,=\\,\\frac{1}{n}\\sum_{i}U(X_{i})\\,=\\,\\sum_{x\\in\\mathcal{X}}\\hat{P}_{n}(x)U(x)}\\end{array}$ and its average is $\\mathbb{E}[Y]\\,=$ $\\textstyle\\sum_{x\\in{\\mathcal{X}}}P(x)U(x)$ . Similarly, the variance o the random variables is $\\operatorname{Var}(Y)=\\operatorname{Var}_{P}(U(X))$ . Thus, by Bernstein's inequality, w.p. at least $1-\\delta$ \uff0c ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\left|{\\hat{Y}}_{n}-\\mathbb{E}[Y]\\right|\\leq{\\sqrt{\\frac{2\\mathrm{Var}(Y)\\ln{\\frac{2}{\\delta}}}{n}}}+{\\frac{2C\\ln{\\frac{2}{\\delta}}}{3n}}.\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Stating the bounds in terms of $X_{i}$ leads to the second event. ", "page_idx": 46}, {"type": "text", "text": "For the last event, we follow the analysis of [Efroni et al., 2021, Lemma 19], which in turn, relies on [Maurer and Pontil 209, Theorem 10]. Defne $\\begin{array}{r}{V_{n}=\\frac{1}{2n(n-1)}\\sum_{i,j=1}^{n}(U(X_{i})-U(X_{j}))^{2}}\\end{array}$ This is a well-known unbiased variance estimator, namely, $\\mathbb{E}[V_{n}]=\\operatorname{Var}_{P}(U(X))$ , and by [Maurer and Pontil, 2009, Theorem 10], for any $\\delta>0$ it holds w.p. at least $1-\\delta$ that ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\left|{\\sqrt{V_{n}}}-{\\sqrt{\\operatorname{Var}_{P}(U(X))}}\\right|\\leq C{\\sqrt{\\frac{2\\ln{\\frac{2}{\\delta}}}{n-1}}},\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "where we scaled the bound by $C$ to account for the values being in $[0,C]$ ", "page_idx": 46}, {"type": "text", "text": "Next, we relate $V_{n}$ to the empirical variance. By elementary algebra, we have ", "page_idx": 47}, {"type": "equation", "text": "$$\n{\\begin{array}{l}{{\\displaystyle V_{n}={\\frac{1}{2n(n-1)}}\\sum_{i,j=1}^{n}(U(X_{i})-U(X_{j}))^{2}}}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {={\\frac{1}{n}}\\sum_{i=1}^{n}U(X_{i})^{2}-{\\frac{n}{n(n-1)}}\\sum_{i\\neq j}U(X_{i})U(X_{j})}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\end{\\stackrel{=}{\\displaystyle}\\frac{1}{n}\\sum_{i=1}^{n}U(X_{i})^{2}-{\\frac{n}{(n-1)}}\\left({\\frac{1}{n}}\\sum_{i}U(X_{i})\\right)^{2}+{\\frac{1}{n(n-1)}}\\sum_{i=1}^{n}U(X_{i})^{2}}}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\displaystyle{\\sum_{x\\in\\mathcal{X}}\\hat{P}_{n}(x)U(x)^{2}-\\left(\\sum_{x\\in\\mathcal{X}}\\hat{P}_{n}(x)U(x)\\right)^{2}+{\\frac{1}{n(n-1)}}\\sum_{i=1}^{n}U(X_{i})^{2}-{\\frac{1}{n^{2}(n-1)}}\\left(\\sum_{i=1}^{n}U(X_{i})\\right)^{2}}}\\end{array}}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "The first two terms are exactly the variance w.r.t. the empirical distribution; therefore, using the inequality $\\left|{\\sqrt{a}}-{\\sqrt{b}}\\right|\\leq{\\sqrt{|a-b|}}$ for positive numbers, we have ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\sqrt{V_{n}}-\\sqrt{\\operatorname{Var}_{\\hat{P}_{n}}(U(X))}\\Big|\\leq\\sqrt{\\left|\\frac{1}{n(n-1)}\\sum_{i=1}^{n}U(X_{i})^{2}-\\frac{1}{n^{2}(n-1)}\\binom{n}{i=1}U(X_{i})\\right)^{2}\\right|}\\leq\\sqrt{\\frac{C^{2}}{n-1}}.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Combining both inequalities and recalling the trivial bound of $C$ on the difference, we get that w.p. at least $1-\\delta$ ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\sqrt{\\mathrm{Var}_{\\hat{P}_{n}}(U(X))}-\\sqrt{\\mathrm{Var}_{P}(U(X))}\\right|\\leq\\operatorname*{min}\\Biggl\\{C\\sqrt{\\frac{2\\ln\\frac{2}{\\delta}}{n-1}}+\\sqrt{\\frac{C^{2}}{n-1}},C\\Biggr\\}\\leq4C\\sqrt{\\frac{\\ln\\frac{2}{\\delta}}{n\\,\\mathrm{V}\\,1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Next, we present a short lemma that allows moving between different spaces of probabilities. ", "page_idx": 47}, {"type": "text", "text": "Lemma 17. Let $\\mathcal{X}$ be a finite set and let $X_{1},\\ldots,X_{n}\\in{\\mathcal{X}}$ Also, let $E_{1},\\ldots,E_{m}\\subseteq{\\mathcal{X}}$ be a partition of the set $\\mathcal{X}$ ,namely, for all $i\\neq j$ \uff0c $E_{i}\\cap E_{j}=\\emptyset$ and $\\cup_{i=1}^{m}E_{i}=\\mathcal X$ Finally, let $f:\\mathcal{X}\\mapsto\\mathbb{R}$ such that for all $i\\in[m]$ and $x\\in E_{i}$ , it holds that $f{\\bar{(}}x)=f(i)$ ,and define ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\hat{P}_{n}(x)=\\frac{1}{n}\\sum_{\\ell=1}^{n}\\mathbb{1}\\{X_{\\ell}=x\\},\\quad a n d,\\quad\\hat{Q}_{n}(i)=\\frac{1}{n}\\sum_{\\ell=1}^{n}\\mathbb{1}\\{X_{\\ell}\\in E_{i}\\}.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Then, the following hold: ", "page_idx": 47}, {"type": "text", "text": "Proof. For the first part, we have by definition that ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\hat{Q}}_{n}(i)=\\displaystyle\\frac{1}{n}\\sum_{\\ell=1}^{n}\\mathbb{1}\\{X_{\\ell}\\in E_{i}\\}=\\displaystyle\\sum_{x\\in\\mathcal{X}}\\displaystyle\\frac{1}{n}\\sum_{\\ell=1}^{n}\\mathbb{1}\\{X_{\\ell}=x\\}\\mathbb{1}\\{x\\in E_{i}\\}=\\displaystyle\\sum_{x\\in\\mathcal{X}}{\\hat{P}}_{n}(x)\\mathbb{1}\\{x\\in E_{i}\\}}\\\\ &{\\qquad=\\displaystyle\\sum_{x\\in E_{i}}{\\hat{P}}_{n}(x)={\\hat{P}}_{n}(E_{i}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "In particular, it holds that ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{i\\sim\\hat{Q}_{n}}[f(i)]=\\displaystyle\\sum_{i=1}^{m}\\hat{Q}_{n}(i)f(i)=\\displaystyle\\sum_{i=1}^{m}\\sum_{x\\in E_{i}}\\hat{P}_{n}(x)f(i)\\overset{(1)}{=}\\displaystyle\\sum_{i=1}^{m}\\sum_{x\\in E_{i}}\\hat{P}_{n}(x)f(x)\\overset{(2)}{=}\\sum_{x\\in\\mathcal{X}}\\hat{P}_{n}(x)f(x)}\\\\ &{\\qquad\\qquad\\qquad=\\mathbb{E}_{x\\sim\\hat{P}_{n}}[f(x)],}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "where (1) is since $f$ is constant inside $E_{i}$ and (2) is since $\\{E_{i}\\}_{i=1}^{m}$ partition $\\mathcal{X}$ ", "page_idx": 48}, {"type": "text", "text": "Forthesdat f t sta  tha stes ae $\\mathbb{E}\\Big[\\hat{P}_{n}(x)\\Big]=$ $P(x)$ , and therefore, ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\hat{Q}_{n}(i)]=\\mathbb{E}\\Bigg[\\sum_{x\\in E_{i}}\\hat{P}_{n}(x)\\Bigg]=\\sum_{x\\in E_{i}}P(x)=P(E_{i})=Q(i).\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Finally, as in the first part of the statement, it holds that ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\mathbb{E}_{i\\sim Q}[f(i)]=\\sum_{i=1}^{m}Q(i)f(i)=\\sum_{i=1}^{m}\\sum_{x\\in E_{i}}P(x)f(i)=\\sum_{i=1}^{m}\\sum_{x\\in E_{i}}P(x)f(x)=\\sum_{x\\in X}P(x)f(x)\\,{\\cal N}(x)\\,}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\mathbb{E}_{x\\sim P}[f(x)].}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Finally, we present two specialized concentration results that are needed for reward and transition lookahead, respectively. ", "page_idx": 48}, {"type": "text", "text": "Lemma 18. Let $X,X_{1},\\ldots X_{n}\\in\\mathbb{R}^{d}$ be i.i.d. random vectors over $[0,1]$ and let $C\\geq1$ be some constant. Then, for any $\\delta\\in(0,1)$ , with probability at least $1-\\delta$ ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\forall u\\in\\left[0,C\\right]^{d},\\qquad\\left|\\mathbb{E}\\left[\\operatorname*{max}_{i\\in\\left[d\\right]}\\{X(i)+u(i)\\}\\right]-\\frac{1}{n}\\sum_{\\ell=1}^{n}\\operatorname*{max}_{i\\in\\left[d\\right]}\\{X_{\\ell}(i)+u(i)\\}\\right|\\leq3\\sqrt{\\frac{d\\ln\\frac{9C n}{\\delta}}{2n}}.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Proof. Denote $\\begin{array}{r}{m(u)=\\mathbb{E}\\bigl[\\operatorname*{max}_{i\\in[d]}\\{X(i)+u(i)\\}\\bigr]}\\end{array}$ and $\\begin{array}{r}{\\hat{m}(u)=\\frac{1}{n}\\sum_{\\ell=1}^{n}\\operatorname*{max}_{i\\in[d]}\\{X_{\\ell}(i)+u(i)\\}}\\end{array}$ and fix any $u\\in[0,C]^{d}$ . Since the variables are bounded in $[0,1]$ , their maximum is bounded almost surely in $[\\operatorname*{max}_{i}\\bar{u}(i),\\operatorname*{max}_{i}u(i)+1]$ , namely, an interval of unit length. Therefore, by Hoeffding's inequality, for any $\\delta^{\\prime}\\in(0,1)$ , w.p. $\\bar{1}-\\delta^{\\prime}$ ", "page_idx": 48}, {"type": "equation", "text": "$$\n|m(u)-\\hat{m}(u)|\\leq\\sqrt{\\frac{\\ln\\frac{2}{\\delta^{\\prime}}}{2n}}.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Now, for some $\\epsilon\\,\\in\\,(0,C]$ , let $u_{\\epsilon}$ be the closest vector to $u$ on a grid $\\left\\{0,\\epsilon,2\\epsilon,\\dots,C\\right\\}^{d}$ . Then, it clearly holds that ", "page_idx": 48}, {"type": "equation", "text": "$$\n|m(u)-\\hat{m}(u)|\\leq|m(u_{\\epsilon})-\\hat{m}(u_{\\epsilon})|+2\\epsilon.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Taking the union bound over all $\\left(\\left\\lceil\\frac{C}{\\epsilon}\\right\\rceil+1\\right)^{d}$ possible choices for $u_{\\epsilon}$ and fixing $\\begin{array}{r}{\\delta^{\\prime}=\\frac{\\delta}{\\big(\\lceil\\frac{C}{\\epsilon}\\rceil+1\\big)^{d}}}\\end{array}$ (T]+1)a, we get w.p. $1-\\delta$ for all $u$ that ", "page_idx": 48}, {"type": "equation", "text": "$$\n|m(u)-\\hat{m}(u)|\\leq\\sqrt{\\frac{\\ln\\frac{2\\left(\\lceil\\frac{c}{\\epsilon}\\rceil+1\\right)^{d}}{\\delta}}{2n}}+2\\epsilon\\leq\\sqrt{\\frac{d\\ln\\frac{6C}{\\epsilon\\delta}}{2n}}+2\\epsilon.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Now, fixing $\\epsilon=\\sqrt{\\frac{d\\ln\\frac{6C}{\\delta}}{2n}}$ and noting that $\\frac{1}{\\epsilon}\\leq\\sqrt{2n}$ for $C\\geq1$ , we get ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\left\\vert m(u)-\\hat{m}(u)\\right\\vert\\leq\\sqrt{\\frac{d\\ln\\frac{6C\\sqrt{2n}}{\\delta}}{2n}}+2\\sqrt{\\frac{d\\ln\\frac{6C}{\\delta}}{2n}}\\leq\\sqrt{\\frac{d\\ln\\frac{9C n}{\\delta}}{2n}}+2\\sqrt{\\frac{d\\ln\\frac{6C}{\\delta}}{2n}}\\leq3\\sqrt{\\frac{d\\ln\\frac{9C n}{\\delta}}{2n}}.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Lemma 19. Let $X,X_{1},\\ldots X_{n}\\in\\mathbb{R}^{d}$ be i.i.d. random vectors with components supported over the discreteset $[m]$ and let $C\\geq1$ be some constant. Then, uniformly over all $u\\in[0,C]^{d m}\\:w.p.\\;1-\\delta$ ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\mathbb{E}\\!\\left[\\operatorname*{max}\\{u(X(i),i)\\}\\right]-\\frac{1}{n}\\sum_{\\ell=1}^{n}\\!\\operatorname*{max}_{i}\\{u(X_{\\ell}(i),i)\\}\\right|}\\\\ &{\\qquad\\qquad\\qquad\\leq\\sqrt{\\frac{2m d\\ln\\frac{6n}{\\delta}\\mathrm{Var}(\\operatorname*{max}_{i}\\{u(X(i),i)\\})}{n}}++\\frac{8C m d\\left(\\ln\\frac{6n}{\\delta}\\right)^{1.5}}{n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Proof. We follow a similar path to Lemma 18 and use a covering argument. Denoting $w(u)\\,=$ $\\mathbb{E}[\\operatorname*{max}_{i}\\{u(X(i),i)\\}]$ and $\\begin{array}{r}{\\hat{w}\\hat{(}u)\\,=\\,\\frac{1}{n}\\sum_{\\ell=1}^{n}\\operatorname*{max}_{i}\\{u(X_{\\ell}(i),i)\\}}\\end{array}$ byerstens qulty, a $\\delta^{\\prime}\\in(0,1)$ and fixed $u\\in[0,C]^{d m}$ , it holds w.p. $1-\\delta^{\\prime}$ that ", "page_idx": 49}, {"type": "equation", "text": "$$\n|w(u)-\\hat{w}(u)|\\leq\\sqrt{\\frac{2\\mathrm{Var}(\\operatorname*{max}_{i}\\{u(X(i),i)\\})\\ln\\frac{2}{\\delta}}{n}}+\\frac{2C\\ln\\frac{2}{\\delta}}{3n}.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Now, for some $\\epsilon\\in(0,C]$ , let $u_{\\epsilon}$ be the closest matrix to $u$ on a grid $\\{0,\\epsilon,2\\epsilon,\\hdots,C\\}^{m d}$ and denote $Z(u)=\\operatorname*{max}_{i}\\{u(X(i),i)\\}$ with samples $Z_{i}(u)$ . By the smoothness of the max function, it holds that ", "page_idx": 49}, {"type": "equation", "text": "$$\n|Z(u)-Z(u_{\\epsilon})|\\leq\\epsilon.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "In particular, we also have that ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\mathbb{E}[Z(u)^{2}]-\\mathbb{E}[Z(u_{\\epsilon})^{2}]\\right|\\leq\\epsilon^{2}+2C\\epsilon,\\qquad\\mathrm{and}\\qquad\\left|\\mathbb{E}[Z(u)]^{2}-\\mathbb{E}[Z(u_{\\epsilon})]^{2}\\right|\\leq\\epsilon^{2}+2C\\epsilon,}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "$\\operatorname{Var}\\Bigl(\\operatorname*{max}_{i}\\{u(X(i),i)\\}\\Bigr)-\\operatorname{Var}\\Bigl(\\operatorname*{max}_{i}\\{u_{\\epsilon}(X(i),i)\\}\\Bigr)\\Bigr|=|\\operatorname{Var}(Z(u))-\\operatorname{Var}(Z(u_{\\epsilon}))|\\le2\\epsilon^{2}+4C\\epsilon.$ Similarly, it holds that ", "page_idx": 49}, {"type": "equation", "text": "$$\n|w(u)-\\hat{w}(u)|\\leq|w(u_{\\epsilon})-\\hat{w}(u_{\\epsilon})|+2\\epsilon.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Taking the union bound over all $\\left(\\left\\lceil\\frac{C}{\\epsilon}\\right\\rceil+1\\right)^{m d}$ possible choices for $u_{\\epsilon}$ and fixing $\\delta^{\\prime}=\\frac{\\delta}{\\left(\\left\\lceil\\frac{C}{\\epsilon}\\right\\rceil+1\\right)^{d m}}$ we get w.p. $1-\\delta$ for all $u$ that ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|w(u)-\\bar{w}(u)|\\leq\\sqrt{\\frac{2\\mathrm{Var}\\left(\\operatorname*{max}_{i}\\{u_{\\varepsilon}(X(i),i)\\}\\right)\\ln\\frac{2\\left(\\left[\\varepsilon^{\\top}\\right]+1\\right)^{m u}}{\\delta}}}+\\frac{2C\\ln\\frac{2\\left(\\left[\\varepsilon^{\\top}\\right]+1\\right)^{m u}}{\\delta}}{3n}+2\\epsilon}\\\\ &{\\qquad\\qquad\\leq\\sqrt{\\frac{2m C\\operatorname*{max}_{i}\\{u_{\\varepsilon}(X(i),i)\\})\\ln\\frac{6C}{\\delta}}{n}}+\\frac{2C m d\\ln\\frac{6C}{\\epsilon\\delta}}{3}+2\\epsilon}\\\\ &{\\qquad\\qquad\\leq\\sqrt{\\frac{2m d\\ln\\frac{6C}{\\epsilon\\delta}\\left(\\operatorname{Var}\\left(\\operatorname*{max}_{i}\\{u(X(i),i)\\}\\right)+2\\varepsilon^{2}+4C\\epsilon\\right)}{n}}+\\frac{2C m d\\ln\\frac{6C}{\\epsilon\\delta}}{3n}+2\\epsilon}\\\\ &{\\qquad\\qquad\\leq\\sqrt{\\frac{2m d\\ln\\frac{6C}{\\epsilon\\delta}\\mathrm{Var}\\left(\\operatorname*{max}_{i}\\{u(X(i),i)\\}\\right)}{n}}+\\sqrt{\\frac{8m d C\\epsilon\\ln\\frac{6C}{\\epsilon\\delta}}{n}}+\\sqrt{\\frac{4m d\\varepsilon^{2}\\ln\\frac{6C}{\\epsilon\\delta}}{n}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad n}\\\\ &{\\qquad\\qquad\\qquad+\\ \\frac{2C m d\\ln\\frac{6C}{\\epsilon\\delta}}{3n}+2\\epsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Now, fxing e = Cln @ and noticing that $\\begin{array}{r}{\\frac{6C}{\\epsilon\\delta}\\le\\frac{6n}{\\delta}}\\end{array}$ , we get ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|w(u)-\\hat{w}(u)|\\leq\\sqrt{\\frac{2m d\\ln\\frac{6n}{\\delta}\\mathrm{Var}(\\operatorname*{max}_{i}\\{u(X(i),i)\\})}{n}}+\\frac{\\sqrt{8m d}C\\ln\\frac{6n}{\\delta}}{n}+\\frac{\\sqrt{4m d}C\\left(\\ln\\frac{6n}{\\delta}\\right)^{1.5}}{n^{1.5}}}\\\\ &{\\phantom{\\leq\\quad}+\\frac{2C m d\\ln\\frac{6n}{\\delta}}{3n}+\\frac{2C\\ln\\frac{6C}{\\delta}}{n}}\\\\ &{\\leq\\sqrt{\\frac{2m d\\ln\\frac{6n}{\\delta}\\mathrm{Var}(\\operatorname*{max}_{i}\\{u(X(i),i)\\})}{n}}+\\frac{8C m d\\left(\\ln\\frac{6n}{\\delta}\\right)^{1.5}}{n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "D.2 Count-Related Lemmas ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Lemma 20. The following bounds hold: ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\displaystyle\\sum_{\\epsilon=1}^{K}\\sum_{h=1}^{H}\\frac{1}{\\sqrt{n_{h}^{k-1}(s_{h}^{k},a_{h}^{k})\\vee1}}\\le S A H+2\\sqrt{S A H^{2}K},}&{\\displaystyle\\sum_{k=1}^{K}\\sum_{h=1}^{H}\\frac{1}{n_{h}^{k-1}(s_{h}^{k},a_{h}^{k})\\vee1}\\le S A H(2+\\ln(K+1))}\\\\ &{\\displaystyle\\sum_{\\epsilon=1}^{K}\\sum_{h=1}^{H}\\frac{1}{\\sqrt{n_{h}^{k-1}(s_{h}^{k})\\vee1}}\\le S H+2\\sqrt{S H^{2}K},}&{\\displaystyle\\sum_{k=1}^{K}\\sum_{h=1}^{H}\\frac{1}{n_{h}^{k-1}(s_{h}^{k})\\vee1}\\le S H(2+\\ln(K+1))}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Proof. Recall that every time a state (or state-action) is visited, its visitation-count is increased by 1, $n_{h}^{K-1}(s,a)$ at the last episode. therefore, we can write ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\sum_{k=1}^{K}\\displaystyle\\frac{1}{\\sqrt{n_{h}^{k-1}(s_{h}^{k},a_{h}^{k})}\\vee1}=\\displaystyle\\sum_{h=1}^{H}\\displaystyle\\sum_{\\varepsilon\\in\\mathcal{S}}\\displaystyle\\sum_{\\epsilon\\in\\mathcal{A}}\\displaystyle\\sum_{k=1}^{K}\\displaystyle\\frac{1}{\\sqrt{n_{h}^{k-1}(s,a)}\\vee1}}\\\\ {\\displaystyle=\\sum_{h=1}^{H}\\displaystyle\\sum_{\\varepsilon\\in\\mathcal{S}}\\displaystyle\\sum_{\\epsilon\\in\\mathcal{A}}\\displaystyle\\sum_{i=0}^{n_{h}^{k-1}(s,a)}\\displaystyle\\frac{1}{\\sqrt{i\\nabla{\\tau}}}}\\\\ {\\displaystyle\\leq\\displaystyle\\sum_{h=1}^{H}\\displaystyle\\sum_{\\epsilon\\in\\mathcal{S}}\\displaystyle\\sum_{\\epsilon\\in\\mathcal{A}}\\left(1+2\\sqrt{n_{h}^{K-1}(s,a)}\\right)}\\\\ {\\displaystyle\\leq A H+2\\sqrt{S A H\\sum_{h=1}^{H}\\sum_{\\epsilon\\in\\mathcal{S}}\\displaystyle\\sum_{\\epsilon\\in\\mathcal{A}}n_{h}^{K-1}(s,a)}}\\\\ {\\displaystyle<S A H+2\\sqrt{S A H^{2}K}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "where we bounded the total number of visits by the number of steps $H K$ . Similarly, we also have ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k=1}^{K}\\displaystyle\\sum_{h=1}^{H}\\frac{1}{n_{h}^{k-1}(s_{h}^{k},a_{h}^{k})\\vee1}=\\displaystyle\\sum_{h=1}^{H}\\sum_{s\\in\\mathcal{S}}\\sum_{a\\in\\mathcal{A}}\\sum_{i=0}^{n_{h}^{K-1}(s,a)}\\displaystyle\\frac{1}{i\\vee1}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\displaystyle\\sum_{h=1}^{H}\\sum_{s\\in\\mathcal{S}}\\sum_{a\\in\\mathcal{A}}(2+\\ln(n_{h}^{K-1}(s,a)\\vee1))\\leq S A H(2+\\ln(K)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "We can likewise prove the inequalities for the state counts as follows: ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{c l}{\\displaystyle\\sum_{k=1}^{K}{\\frac{H}{h\\scriptscriptstyle{h}^{-1}(s)}}\\displaystyle\\frac{1}{\\sqrt{n_{h}^{k-1}(s_{h}^{k})}\\vee1}=\\displaystyle\\sum_{h=1}^{H}{\\sum_{s\\in{\\mathcal{S}}}\\displaystyle\\sum_{k=1}^{K}{\\frac{1}{\\sqrt{n_{h}^{k-1}(s)}\\vee1}}}}\\\\ {\\displaystyle}&{\\displaystyle=\\sum_{h=1}^{H}{\\sum_{s\\in{\\mathcal{S}}}\\displaystyle\\sum_{i=0}^{n_{h}^{k-1}(s)}{\\frac{1}{\\sqrt{i\\,\\nabla{1}\\,}}}}}\\\\ {\\displaystyle}&{\\displaystyle\\leq\\displaystyle\\sum_{h=1}^{H}{\\sum_{s\\in{\\mathcal{S}}}\\left(1+2\\sqrt{n_{h}^{K-1}(s)}\\right)}}\\\\ {\\displaystyle}&{\\displaystyle\\leq{S H+2\\sqrt{S H\\sum_{h=1}^{H}{\\sum_{s\\in{\\mathcal{S}}}\\displaystyle\\sum_{k=1}^{n_{h}^{K-1}(s)}}}}}\\\\ {\\displaystyle}&{\\displaystyle<{S H+2\\sqrt{S H^{2}K}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "and ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\sum_{u=1}^{K}\\sum_{h=1}^{H}\\frac{1}{n_{h}^{k-1}(s_{h}^{k})\\vee1}=\\sum_{h=1}^{H}\\sum_{s\\in\\mathcal{S}}\\sum_{i=0}^{n_{h}^{K-1}(s)}\\frac{1}{i\\vee1}\\le\\sum_{h=1}^{H}\\sum_{s\\in\\mathcal{S}}\\bigl(2+\\ln\\bigl(n_{h}^{K-1}(s)\\vee1\\bigr)\\bigr)\\le S H(2+\\ln(K)).\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "D.3  Analysis of Variance terms ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Lemma 21. Let $P$ be a distribution over a finite set $\\mathcal{X}$ and let $X\\sim P$ Also, let $V_{1},V_{2}:\\mathcal{X}\\mapsto[0,C]$ for some $C>0$ such that $V_{1}(x)\\leq V_{2}(x)$ for all $x\\in\\mathscr{X}$ .Then, for any $\\alpha,n>0,$ it holds that ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\frac{\\sqrt{\\mathrm{Var}_{P}(V_{2}(X))}}{\\sqrt{n}}\\leq\\frac{\\sqrt{\\mathrm{Var}_{P}(V_{1}(X))}}{\\sqrt{n}}+\\frac{1}{\\alpha}\\mathbb{E}_{P}[V_{2}(X)-V_{1}(X)]+\\frac{C\\alpha}{4n}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Proof. By Lemma 26, we have ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\sqrt{\\mathrm{Var}_{P}(V_{2}(X))}-\\sqrt{\\mathrm{Var}_{P}(V_{1}(X))}\\leq\\sqrt{\\mathrm{Var}_{P}(V_{2}(X)-V_{1}(X))}}&{}\\\\ {\\leq\\sqrt{\\mathbb{E}_{P}[(V_{2}(X)-V_{1}(X))^{2}]}}&{}\\\\ {\\leq\\sqrt{C\\mathbb{E}_{P}[V_{2}(X)-V_{1}(X)]}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "where the last inequality is by the boundedness and since $V_{1}(x)\\leq V_{2}(x)$ . Thus, we can bound ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\sqrt{\\mathrm{Var}_{P}\\left(V_{2}(X)\\right)}-\\sqrt{\\mathrm{Var}_{P}\\left(V_{1}(X)\\right)}}{\\sqrt{n}}\\leq\\frac{\\sqrt{C\\mathbb{E}_{P}\\left[V_{2}(X)-V_{1}(X)\\right]}}{\\sqrt{n}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\sqrt{\\mathbb{E}_{P}\\left[V_{2}(X)-V_{1}(X)\\right]}\\cdot\\sqrt{\\frac{C}{n}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\quad\\leq\\frac{1}{\\alpha}\\mathbb{E}_{P}[V_{2}(X)-V_{1}(X)]+\\frac{C\\alpha}{4n},}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "where last inequality is due to Young's inequality $\\begin{array}{r}{(a b\\leq\\frac{1}{\\alpha}a^{2}+\\frac{\\alpha}{4}b^{2}}\\end{array}$ for all $\\alpha>0$ ", "page_idx": 51}, {"type": "text", "text": "Lemma 22. Let $P,P^{\\prime}$ be distributions over a finite set $\\mathcal{X}$ and let $X\\sim P$ .Also, let $V_{1},V_{2},V_{3}:\\mathcal{X}\\mapsto$ $[0,C]$ for some $C>0$ such that $V_{1}(x)\\leq V_{2}(x)\\leq V_{3}(x)$ for all $x\\in\\mathscr{X}$ . Finally, assume that ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\left|{\\sqrt{\\operatorname{Var}_{P}(V_{2}(X))}}-{\\sqrt{\\operatorname{Var}_{P^{\\prime}}(V_{2}(X))}}\\right|\\leq\\beta\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "for some $\\beta>0$ .Then, for any $\\alpha,n>0$ it holds that ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{\\sqrt{\\mathrm{Var}_{P}(V_{3}(X))}}{\\sqrt{n}}\\leq\\frac{\\sqrt{\\mathrm{Var}_{P}(V_{1}(X))}}{\\sqrt{n}}+\\frac{1}{\\alpha}\\mathbb{E}_{P^{\\prime}}[V_{3}(X)-V_{2}(X)]+\\frac{1}{\\alpha}\\mathbb{E}_{P}[V_{2}(X)-V_{1}(X)]+\\frac{C\\alpha}{2n}+}\\\\ {\\leq\\frac{\\sqrt{\\mathrm{Var}_{P}(V_{1}(X))}}{\\sqrt{n}}+\\frac{1}{\\alpha}\\mathbb{E}_{P^{\\prime}}[V_{3}(X)-V_{1}(X)]+\\frac{1}{\\alpha}\\mathbb{E}_{P}[V_{3}(X)-V_{1}(X)]+\\frac{C\\alpha}{2n}+}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Proof. We decompose the 1.h.s. as follows ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\sqrt{\\mathrm{Var}_{P}(V_{3}(X))}}{\\sqrt{n}}=\\frac{\\sqrt{\\mathrm{Var}_{P^{\\prime}}(V_{3}(X))}-\\sqrt{\\mathrm{Var}_{P^{\\prime}}(V_{2}(X))}}{\\sqrt{n}}+\\frac{\\sqrt{\\mathrm{Var}_{P^{\\prime}}(V_{2}(X))}-\\sqrt{\\mathrm{Var}_{P}(V_{2}(X))}}{\\sqrt{n}}}\\\\ &{\\qquad\\qquad\\qquad+\\frac{\\sqrt{\\mathrm{Var}_{P}(V_{2}(X))}-\\sqrt{\\mathrm{Var}_{P}(V_{1}(X))}}{\\sqrt{n}}+\\frac{\\sqrt{\\mathrm{Var}_{P}(V_{1}(X))}}{\\sqrt{n}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "We bound the first and third terms using Lemma 21 and bound the second term with the assumption andget ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{\\sqrt{\\mathrm{Var}_{P^{\\prime}}(V_{3}(X))}}{\\sqrt{n}}\\leq\\frac{1}{\\alpha}\\mathbb{E}_{P^{\\prime}}[V_{3}(X)-V_{2}(X)]+\\frac{C\\alpha}{4n}+\\frac{\\beta}{\\sqrt{n}}}}\\\\ &{}&{\\mathrm+\\,\\frac{1}{\\alpha}\\mathbb{E}_{P}[V_{2}(X)-V_{1}(X)]+\\frac{C\\alpha}{4n}+\\frac{\\sqrt{\\mathrm{Var}_{P}(V_{1}(X))}}{\\sqrt{n}}}\\\\ &{}&{\\mathrm=\\frac{\\sqrt{\\mathrm{Var}_{P}(V_{1}(X))}}{\\sqrt{n}}+\\frac{1}{\\alpha}\\mathbb{E}_{P^{\\prime}}[V_{3}(X)-V_{2}(X)]+\\frac{1}{\\alpha}\\mathbb{E}_{P}[V_{2}(X)-V_{1}(X)]+\\frac{C\\alpha}{2n}+\\frac{1}{\\alpha}\\mathbb{E}_{P}[V_{3}(X)-V_{1}(X)]}\\\\ &{}&{\\mathrm\\leq\\frac{\\sqrt{\\mathrm{Var}_{P}(V_{1}(X))}}{\\sqrt{n}}+\\frac{1}{\\alpha}\\mathbb{E}_{P^{\\prime}}[V_{3}(X)-V_{1}(X)]+\\frac{1}{\\alpha}\\mathbb{E}_{P}[V_{3}(X)-V_{1}(X)]+\\frac{C\\alpha}{2n}+\\frac{\\beta}{\\alpha}\\mathbb{E}_{P}[V_{4}(X)].}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "where the last inequality uses the fact that $V_{1}(x)\\,\\leq\\,V_{2}(x)\\,\\leq\\,V_{3}(x)$ for all $x\\in\\mathscr{X}$ . The last two bounds are the desired results. \u53e3 ", "page_idx": 51}, {"type": "text", "text": "E Existing Results ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Lemma 23 (Monotonic Bonuses,[Zhang et al., 2023], Appendix C.1). For any $p\\in\\Delta^{S}$ $v\\in\\mathbb{R}_{+}^{S}$ S.t. $\\|v\\|_{\\infty}\\le H,\\,\\delta^{\\prime}\\in(0,1)$ andpositiveinteger $n$ definethefunction ", "page_idx": 52}, {"type": "equation", "text": "$$\nf(p,v,n)=p^{T}v+\\operatorname*{max}\\left\\{\\frac{20}{3}\\sqrt{\\frac{\\operatorname{Var}_{p}(v)\\ln\\frac{1}{\\delta^{\\prime}}}{n}},\\frac{400}{9}\\frac{H\\ln\\frac{1}{\\delta^{\\prime}}}{n}\\right\\}.\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Then, the function $f(p,v,n)$ is non-decreasing in each entry of $v$ ", "page_idx": 52}, {"type": "text", "text": "Lemma 24 (Efroni et al. 2021, Lemma 28). Let $Y\\in\\mathbb{R}^{S}$ be a vector such that $0\\leq Y(s)\\leq H$ for all $s\\in S$ Let $P_{1}$ and $P_{2}$ bewo transionmodelsand $n\\in\\mathbb{R}_{+}^{S A}$ f ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left(\\forall(s,a,s^{\\prime})\\in\\mathcal{S}\\times A\\times\\mathcal{S},h\\in[H]:~|P_{2,h}(s^{\\prime}|s,a)-P_{1,h}(s^{\\prime}|s,a)|\\leq\\sqrt{\\frac{C_{1}L_{\\delta}^{k}P_{1,h}(s^{\\prime}|s,a)}{n(s,a)\\vee1}}+\\frac{C_{2}}{n(s,a)}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "for some $C_{1},C_{2}>0,$ then, for any $\\alpha>0$ ", "page_idx": 52}, {"type": "equation", "text": "$$\n|(P_{1,h}-P_{2,h})Y(s,a)|\\leq\\frac{1}{\\alpha}\\mathbb{E}_{s^{\\prime}\\sim P_{1,h}(\\cdot\\vert s,a)}[Y(s^{\\prime})]+\\frac{H S L_{\\delta}^{k}(C_{2}+\\alpha C_{1}/4)}{n(s,a)\\vee1},\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Lemma 25 (Efroni et al. 2021, Lemma 27). Let $\\{Y_{t}\\}_{t\\ge1}$ be a real-valued sequence of random variables adapted to a filtration $\\{\\boldsymbol{F}_{t}\\}_{t\\ge0}$ . Assume that for all $t\\geq1$ it holds that $0\\leq Y_{t}\\leq C$ a.s., andlet $T\\in\\mathbb N$ .Then each of the following inequalities holds with probability greater than $1-\\delta$ ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}[Y_{t}|F_{t-1}]\\leq\\left(1+\\frac{1}{2C}\\right)\\sum_{t=1}^{T}Y_{t}+2(2C+1)^{2}\\ln\\frac{1}{\\delta},}\\\\ &{\\displaystyle\\sum_{t=1}^{T}Y_{t}\\leq2\\sum_{t=1}^{T}\\mathbb{E}[Y_{t}|F_{t-1}]+4C\\ln\\frac{1}{\\delta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Lemma 26 (Standard Deviation Differences, e.g., Zanette and Brunskill 2019, lines 48-51). Let $P\\in\\Delta_{d}$ besomedistributionover $[d]$ andlet $V_{1},\\bar{V}_{2}\\in\\mathbb{R}^{d}$ Then,itholdsthat ", "page_idx": 52}, {"type": "equation", "text": "$$\n{\\sqrt{\\operatorname{Var}_{P}(V_{1})}}-{\\sqrt{\\operatorname{Var}_{P}(V_{2})}}\\leq{\\sqrt{\\operatorname{Var}_{P}(V_{1}-V_{2})}}.\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Lemma 27 (Law of Total Variance, e.g., Zanette and Brunskill 2019, Lemma 15). For any nolookaheadpolicy $\\pi$ itholdsthat ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\sum_{h=1}^{H}\\mathrm{Var}(V_{h+1}^{\\pi}(s_{h+1})|s_{h})|\\pi,s_{1}\\right]=\\mathbb{E}\\left[\\left(\\sum_{h=1}^{H}r_{h}(s_{h},a_{h})-V_{1}^{\\pi}(s_{1})\\right)^{2}|\\pi,s_{1}\\right],\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Wwhere $\\operatorname{Var}(V_{h+1}^{\\pi}(s_{h+1})|s_{h})$ is the variance of the value at step $s_{h+1}$ givenstate $s_{h}$ and under the policy $\\pi$ , due to the policy randomization and next-state transition probabilities. ", "page_idx": 52}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 53}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 53}, {"type": "text", "text": "Justification: In the abstract, we accurately present the setting and its motivation, as well as a summary of the results,all of which are proved in the appendix. ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 53}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 53}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Justification: The main limitations in this work are a result of the studied setup - some possibleextensions and improvement arediscussedin thefutureworksection. ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should refect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 53}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 53}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 54}, {"type": "text", "text": "Justification: Proofs for all the stated results are provided in the appendix. ", "page_idx": 54}, {"type": "text", "text": "Guidelines: ", "page_idx": 54}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 54}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 54}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 54}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 54}, {"type": "text", "text": "Guidelines: ", "page_idx": 54}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 54}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 55}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 55}, {"type": "text", "text": "Justification: The paper does not include experiments. Guidelines: ", "page_idx": 55}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 55}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 55}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 55}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 55}, {"type": "text", "text": "Guidelines: ", "page_idx": 55}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 55}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 55}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 55}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 55}, {"type": "text", "text": "Guidelines: ", "page_idx": 55}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative errorrates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 55}, {"type": "text", "text": "", "page_idx": 56}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 56}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 56}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 56}, {"type": "text", "text": "Guidelines: ", "page_idx": 56}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 56}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 56}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 56}, {"type": "text", "text": "Justification: The paper is purely theoretical and studies a fundamental decision-making model; any ethical issue that might arise would be a core issue in the ethics of applying machinelearning,and not tiedspecificallytothiswork. ", "page_idx": 56}, {"type": "text", "text": "Guidelines: ", "page_idx": 56}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 56}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 56}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 56}, {"type": "text", "text": "Justification: Due to the theoretical nature of the paper and the generality of the model, it is nodirectsocietalimpact. ", "page_idx": 56}, {"type": "text", "text": "Guidelines: ", "page_idx": 56}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed. \u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 56}, {"type": "text", "text": "\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 57}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 57}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 57}, {"type": "text", "text": "Justification: No data or models are released with this paper. ", "page_idx": 57}, {"type": "text", "text": "Guidelines: ", "page_idx": 57}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safetyfilters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 57}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 57}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 57}, {"type": "text", "text": "Justification: The paper does not use existing assets. ", "page_idx": 57}, {"type": "text", "text": "Guidelines: ", "page_idx": 57}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 57}, {"type": "text", "text": "", "page_idx": 58}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 58}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 58}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 58}, {"type": "text", "text": "Guidelines: ", "page_idx": 58}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 58}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 58}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 58}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 58}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 58}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 58}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 58}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 58}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 58}]