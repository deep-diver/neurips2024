[{"figure_path": "wlqfOvlTQz/figures/figures_3_1.jpg", "caption": "Figure 1: Two-state prophet-like problem", "description": "This figure depicts a two-state Markov Decision Process (MDP) to illustrate the benefit of reward lookahead. The agent starts in state *s<sub>i</sub>*. Action *a<sub>1</sub>* keeps the agent in *s<sub>i</sub>* with zero reward.  Other actions move the agent to a terminal state *s<sub>f</sub>* with a reward sampled from a Bernoulli distribution with a probability of success equal to 1/((A-1)H). The terminal state yields zero reward. The key insight is that with reward lookahead, the agent observes the reward distribution for each action before making a decision; this drastically increases its expected reward, highlighting the advantages of using lookahead information.", "section": "Comparing the Values of Lookahead Agents and Vanilla RL agents"}, {"figure_path": "wlqfOvlTQz/figures/figures_45_1.jpg", "caption": "Figure 2: Random chain: agents start at the left side and must reach its right side to collect a reward.", "description": "This figure shows a Markov Decision Process (MDP) represented as a chain of states.  The agent starts at the leftmost state and aims to reach the rightmost state, which yields a reward of 1.  At each state, the agent can choose between a safe action (green arrow), which keeps the agent in the same state, and A-1 risky actions (red arrows).  Each risky action has a probability of 1/(A-1) of moving the agent one step to the right and a probability of (A-2)/(A-1) of sending the agent to a terminal state with no reward. This setup illustrates the benefit of having transition lookahead information, enabling the agent to make more informed decisions compared to scenarios without such information.", "section": "C.9 Example: Value Gain due to Transition Lookahead"}]