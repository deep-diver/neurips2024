[{"type": "text", "text": "How DNNs break the Curse of Dimensionality: Compositionality and Symmetry Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 We show that deep neural networks (DNNs) can efficiently learn any composition   \n2 of functions with bounded $F_{1}$ -norm, which allows DNNs to break the curse of   \n3 dimensionality in ways that shallow networks cannot. More specifically, we   \n4 derive a generalization bound that combines a covering number argument for   \n5 compositionality, and the $F_{1}$ -norm (or the related Barron norm) for large width   \n6 adaptivity. We show that the global minimizer of the regularized loss of DNNs can   \n7 fit for example the composition of two functions $f^{*}=h\\circ g$ from a small number   \n8 of observations, assuming $g$ is smooth/regular and reduces the dimensionality (e.g.   \n9 $g$ could be the modulo map of the symmetries of $f^{*}$ ), so that $h$ can be learned in   \n0 spite of its low regularity. The measures of regularity we consider is the Sobolev   \n1 norm with different levels of differentiability, which is well adapted to the $F_{1}$ norm.   \n2 We compute scaling laws empirically and observe phase transitions depending on   \n3 whether $g$ or $h$ is harder to learn, as predicted by our theory. ", "page_idx": 0}, {"type": "text", "text": "14 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "15   One of the fundamental features of DNNs is their ability to generalize even when the number of   \n16 neurons (and of parameters) is so large that the network could fit almost any function [46]. Actually   \n17 DNNs have been observed to generalize best when the number of neurons is infinite [8, 21, 20].   \n18 The now quite generally accepted explanation to this phenomenon is that DNNs have an implicit   \n19 bias coming from the training dynamic where properties of the training algorithm lead to networks   \n20 that generalize well. This implicit bias is quite well understood in shallow networks [11, 36], in   \n21 linear networks [24, 30], or in the NTK regime [28], but it remains ill-understood in the general deep   \n22 nonlinear case.   \n23  In both shallow networks and linear networks, one observes a bias towards small parameter norm   \n24 (either implicit [12] or explicit in the presence of weight decay [42]). Thanks to tools such as the   \n25 $F_{1}$ -norm [5], or the related Barron norm [44], or more generally the representation cost [14], it is   \n26 possible to describe the family of functions that can be represented by shallow networks or linear   \n27 networks with a finite parameter norm. This was then leveraged to prove uniform generalization   \n28 bounds (based on Rademacher complexity) over these sets [5], which depend only on the parameter   \n29  norm, but not on the number of neurons or parameters.   \n30  Similar bounds have been proposed for DNNs [7, 6, 39, 33, 25, 40], relying on different types of   \n31 norms on the parameters of the network. But it seems pretty clear that we have not yet identified   \n32 the \u2018right' complexity measure for deep networks, as there remains many issues: these bounds are   \n33typically orders of magnitude too large [29, 23], and they tend to explode as the depth $L$ grows[40].   \n34  Two families of bounds are particularly relevant to our analysis: bounds based on covering numbers   \n35which rely on the fact that one can obtain a covering of the composition of two function classes from   \n36  covering of the individual classes [7, 25], and path-norm bounds which extend the techniques behind   \n37 the $F_{1}$ -norm bound from shallow networks to the deep case [32, 6, 23].   \n38  Another issue is the lack of approximation results to accompany these generalization bounds: many   \n39different complexity measures $R(\\theta)$ on the parameters $\\theta$ of DNNs have been proposed along with   \n40 guarantees that the generalization gap will be small as long as $R(\\theta)$ is bounded, but there are often   \n41 little to no result describing families of functions that can be approximated with a bounded $R(\\theta)$   \n42 norm. The situation is much clearer in shallow networks, where we know that certain Sobolev spaces   \n43can be approximated with bounded $F_{1}$ -norm [5].   \n44  We will focus on approximating composition of Sobolev functions, and obtaining close to optimal   \n45 rates. This is quite similar to the family of tasks considered [39], though the complexity measure we   \n46 consider is quite different, and does not require sparsity of the parameters. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "471.1\u3000 Contribution", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "48  We consider Accordion Networks (AccNets), which are the composition of multiple shallow   \n49networks $f_{L;1}=f_{L}\\circ\\cdot\\cdot\\circ f_{1}$ , we prove a uniform generalization bound $\\mathcal{L}(f_{L:1})-\\bar{\\tilde{\\mathcal{L}}}_{N}(f_{L:1})\\lesssim$   \n50 $\\begin{array}{r}{R(f_{1},\\ldots,f_{L}){\\frac{\\log N}{\\sqrt{N}}}}\\end{array}$ , for a complexity measure ", "page_idx": 1}, {"type": "equation", "text": "$$\nR(f_{1},\\ldots,f_{L})=\\prod_{\\ell=1}^{L}L i p(f_{\\ell})\\sum_{\\ell=1}^{L}\\frac{\\Vert f_{\\ell}\\Vert_{F_{1}}}{L i p(f_{\\ell})}\\sqrt{d_{\\ell}+d_{\\ell-1}}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "51  that depends on the $F_{1}$ -norms $\\|f_{\\ell}\\|_{F_{1}}$ and Lipschitz constanst $L i p(f_{\\ell})$ of the subnetworks, and the   \n52  intermediate dimensions $d_{0},\\ldots,d_{L}$ . This use of the $F_{1}$ -norms makes this bound independent of the   \n53  widths $w_{1},\\dots,w_{L}$ of the subnetworks, though it does depend on the depth $L$ (it typically grows   \n5  linearly in $L$ which is still better than the exponential growth often observed).   \n55  Any traditional DNN can be mapped to an AccNet (and vice versa), by spliting the middle weight   \n56  matrices $W_{\\ell}$ with SVD $U S V^{T}$ into two matrices $U\\sqrt{S}$ and $\\sqrt{S}V^{T}$ to obtain an AccNet with   \n57dimensions $d_{\\ell}=\\mathrm{Rank}W_{\\ell}$ , so that the bound can be applied to traditional DNNs with bounded rank.   \n58  We then show an approximation result: any composition of Sobolev functions $f^{*}=f_{L^{*}}^{*}\\circ\\cdots\\circ f_{1}^{*}$   \n59  can be approximated with a network with either a bounded complexity $R(\\theta)$ or a slowly growing one.   \n60 Thus under certain assumptions one can show that DNNs can learn general compositions of Sobolev   \n61 functions. This ability can be interpreted as DNNs being able to learn symmetries, allowing them to   \n62 avoid the curse of dimensionality in settings where kernel methods or even shallow networks suffer   \n63 heavily from it.   \n64  Empirically, we observe a good match between the scaling laws of learning and our theory, as well as   \n65 qualitative features such as transitions between regimes depending on whether it is harder to learn the   \n66 symmetries of a task, or to learn the task given its symmetries. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "67 2  Accordion Neural Networks and ResNets ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "68 Our analysis is most natural for a slight variation on the traditional fully-connected neural networks   \n69 (FCNNs), which we call Accordion Networks, which we define here. Nevertheless, all of our results   \n70can easily be adapted to FCNNs.   \n71  Accordion Networks (AccNets) are simply the composition of $L$ shallow networks, that is $f_{L;1}=$   \n72 $f_{L}\\circ\\cdot\\cdot\\circ f_{1}$ where $f_{\\ell}(z)=W_{\\ell}\\sigma(V_{\\ell}z+b_{\\ell})$ for the nonlinearity $\\sigma:\\mathbb{R}\\rightarrow\\mathbb{R}$ ,the $d_{\\ell}\\times w_{\\ell}$ matrix   \n73 $W_{\\ell}$ $w_{\\ell}\\times d_{\\ell-1}$ matrix $V_{\\ell}$ , and $w_{\\ell}$ -dim. vector $b_{\\ell}$ , and for the widths $w_{1},\\dots,w_{L}$ and dimensions   \n74 $d_{0},\\ldots,d_{L}$ . We will focus on the ReLU $\\sigma(x)=\\operatorname*{max}\\{0,x\\}$ for the nonlinearity. The parameters $\\theta$ are   \n75  made up of the concatenation of all $(W_{\\ell},V_{\\ell},b_{\\ell})$ . More generally, we denote $f_{\\ell_{2}:\\ell_{1}}=f_{\\ell_{2}}\\circ\\cdot\\cdot\\circ f_{\\ell_{1}}$   \n76 for any $1\\leq\\ell_{1}\\leq\\ell_{2}\\leq L$   \n77 We will typically be interested in settings where the widths $w_{\\ell}$ is large (or even infinitely large), while   \n78  the dimensions $d_{\\ell}$ remain finite or much smaller in comparison, hence the name accordion.   \n79  If we add residual connections, i.e. $f_{1:L}^{r e s}=(f_{L}+i d)\\circ\\cdot\\cdot\\circ(f_{1}+i d)$ for the same shallow nets ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "80 $f_{1},\\ldots,f_{L}$ we recover the typical ResNets. ", "page_idx": 1}, {"type": "text", "text": "81Remark. The only difference between AccNets and FCNNs is that each weight matrix $M_{\\ell}$ of the   \n82 FCNN is replaced by a product of two matrices $M_{\\ell}=V_{\\ell}W_{\\ell-1}$ in the middle of the network (such a   \n83 structure has already been proposed [34]). Given an AccNet one can recover an equivalent FCNN by   \n84 choosing $M_{\\ell}=V_{\\ell}W_{\\ell-1}$ \uff0c $M_{0}=V_{0}$ and $M_{L+1}=W_{L}$ . In the other direction there could be multiple   \n85 ways to split $M_{\\ell}$ into the product of two matrices, but we will focus on taking $V_{\\ell}\\,=\\,U\\sqrt{S}$ and   \n86 $W_{\\ell-1}=\\sqrt{S}V^{T}$ for the SVD decomposition $M_{\\ell}=U S V^{T}$ , along with the choice $d_{\\ell}=\\mathrm{Rank}M_{\\ell}$ ", "page_idx": 2}, {"type": "text", "text": "87 2.1 Learning Setup ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "8 We consider a traditional learning setup, where we want to find a function $f:\\Omega\\subset\\mathbb{R}^{d_{i n}}\\rightarrow\\mathbb{R}^{d_{o u t}}$   \n89  that minimizes the population loss $\\mathcal{L}(f)\\,=\\,\\mathbb{E}_{x\\sim\\pi}\\left[\\ell(x,f(x))\\right]$ for an input distribution $\\pi$ and a   \n90 $\\rho$ Lipschitz and $\\rho$ bounded loss function $\\ell(x,y)\\in[0,B]$ Given a training set $x_{1},\\ldots,x_{N}$ of size $N$   \n91 weaproximate th population lossby thempirical ss $\\begin{array}{r}{\\tilde{\\mathcal{L}}_{N}(f)=\\frac{1}{N}\\sum_{i=1}^{N}\\ell(x_{i},f(x_{i}))}\\end{array}$ that can be   \n92minimized.   \n93 To ensure that the empirical loss remains representative of the population loss, we will prove high   \n94  probability bounds on the generalization gap $\\tilde{\\mathcal{L}}_{N}(f)-\\mathcal{L}(f)$ uniformly over certain functions families   \n95 $f\\in\\mathcal F$   \n96 For regression tasks, we assume the existence of a true function $f^{*}$ and try to minimize the distance   \n97 $\\ell(x,y)^{\\flat}=\\|f^{*}(x)-y\\|^{p}$ for $p\\geq1$ . If we assume that $f^{*}(x)$ and $y$ are uniformly bounded then one   \n98 can easily show that $\\ell(x,y)$ is bounded and Lipschitz. We are particularly interested in the cases   \n99 $p\\in\\{1,2\\}$ , with $p=2$ representing the classical MSE, and $p=1$ representing a $L_{1}$ distance. The   \n100 $p=2$ case is amenable to fast rates' which take advantage of the fact that the loss increases very   \n101 slowly around the optimal solution $f^{*}$ , We do not prove such fast rates (even though it might be   \n102  possible) so we focus on the $p=1$ case.   \n103  For classification tasks on $k$ classes, we assume the existence of a \u2018true class? function $f^{*}:\\Omega\\rightarrow$   \n104 $\\{1,\\ldots,k\\}$ and want to learn a function $f:\\Omega\\to\\mathbb{R}^{k}$ such that the largest entry of $f(x)$ is the $f^{*}(k)$ -th   \n105 entry. One can consider the hinge cost $\\ell(x,y)=\\mathrm{max}\\{0,1-\\big(y_{f^{*}(k)}-\\mathrm{max}_{i\\neq f^{*}(x)}\\,y_{i}\\big)\\}$ ,which is   \n106  zero whenever the margin $y_{f^{*}(k)}-\\operatorname*{max}_{i\\neq f^{*}(x)}y_{i}$ is larger than 1 and otherwise equals 1 minus the   \n107  margin. The hinge loss is Lipschitz and bounded if we assume bounded outputs $y\\,=\\,f(x)$ . The   \n108 cross-entropy loss also fts our setup. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "1093  Generalization Bound for DNNs ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "110The reason we focus on accordion networks is that there exists generalization bounds for shallow   \n111 networks [5, 44], that are (to our knowledge) widely considered to be tight, which is in contrast to the   \n112 deep case, where many bounds exist but no clear optimal bound has been identified. Our strategy   \n113 is to extend the results for shallow nets to the composition of multiple shallow nets, i.e. AccNets.   \n114 Roughly speaking, we will show that the complexity of an AccNet $f_{\\theta}$ is bounded by the sum of the   \n115 complexities of the shallow nets $f_{1},\\ldots,f_{L}$ it is made of.   \n116 We will therefore first review (and slightly adapt) the existing generalization bounds for shallow   \n117 networks in terms of their so-called $F_{1}$ -norm [5], and then prove a generalization bound for deep   \n118AccNets. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "1193.1 Shallow Networks ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "120  The complexity of a shallow net $f(x)~=~W\\sigma(V x\\,+\\,b)$ , with weights $W\\;\\in\\;\\mathbb{R}^{w\\times d_{o u t}}$ and   \n121 $V\\!\\!\\!\\!\\!\\!\\!\\!\\in\\!\\!\\!\\!\\!\\!\\!\\!\\!\\ensuremath{\\mathbb{R}}^{d_{i n}\\times w}$ , can be bounded in terms of the quantity $\\begin{array}{r c l}{C}&{=}&{\\sum_{i=1}^{w}\\left\\|{W..}\\right\\|\\sqrt{\\left\\|{V_{i\\cdot}}\\right\\|^{2}+b_{i}^{2}}}\\end{array}$   \n122 First note that the rescaled function $\\scriptstyle{\\frac{1}{C}}f$ can be written as a convex combination $\\scriptstyle{\\frac{1}{C}}f(x)\\ =$   \n123 W+W(V+b\uff09 forW.=W\uff0cV= $\\begin{array}{r}{\\bar{W}_{\\cdot i}=\\frac{W_{\\cdot i}}{\\Vert W_{\\cdot i}\\Vert},\\bar{V}_{i\\cdot}=\\frac{V_{i}.}{\\sqrt{\\Vert V_{i\\cdot}\\Vert^{2}+b_{i}^{2}}}}\\end{array}$ $\\begin{array}{r}{\\bar{b}_{i}=\\frac{{b}_{i}}{\\sqrt{\\left\\Vert{V_{i}.\\Vert^{2}+b_{i}^{2}}\\right.}}}\\end{array}$   \n124 since the coefficients $\\frac{\\|W._{i}\\|\\sqrt{\\|V_{i\\cdot}\\|^{2}\\!+\\!b_{i}^{2}}}{C}$ are positive and sum up to 1. Thus $f$ belongs to $C$ times the   \n125 convex hull ", "page_idx": 2}, {"type": "equation", "text": "$$\nB_{F_{1}}=\\operatorname{Conv}\\left\\{x\\mapsto w\\sigma({v^{T}}x+b):\\left\\|w\\right\\|^{2}=\\left\\|v\\right\\|^{2}+b^{2}=1\\right\\}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "126  We call this the $F_{1}$ -ball since it can be thought of as the unit ball w.r.t. the $F_{1}$ -norm $\\|f\\|_{F_{1}}$ which we   \n127  define as the smallest positive scalar $s$ such that $\\textstyle{\\frac{1}{s}}f\\in B_{F_{1}}$ . For more details in the single output   \n128 case, see [5]. ", "page_idx": 3}, {"type": "text", "text": "129  The generalization gap over any $F_{1}$ -ball can be uniformly bounded with high probability: ", "page_idx": 3}, {"type": "text", "text": "130  Theorem 1. For any input distribution $\\pi$ supported on the $L_{2}$ ball $B(0,b)$ with radius $b$ we have   \n131 with probability $1-\\delta$ over the training samples $x_{1},\\ldots,x_{N}$ , that for all $f\\in B_{F_{1}}(0,R)=R\\cdot B_{F_{1}}$ ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}(f)-\\tilde{\\mathcal{L}}_{N}(f)\\leq\\rho b R\\sqrt{d_{i n}+d_{o u t}}\\frac{\\log N}{\\sqrt{N}}+c_{0}\\sqrt{\\frac{2\\log2/\\delta}{N}}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "132  This theorem is a slight variation of the one found in [5]: we simply generalize it to multiple outputs,   \n133 and also prove it using a covering number argument instead of a direct computation of the Rademacher   \n134 complexity, which will be key to obtaining a generalization bound for the deep case. But due to this   \n135 change of strategy we pay a $\\log N$ cost here and in our later results. We know that the $\\log N$ term   \n136 can be removed in Theorem 1 by switching to a Rademacher argument, but we do not know whether   \n137 it can be removed in deep nets.   \n138 Notice how this bound does not depend on the width $w$ ,becausethe $F_{1}$ -norm (andthe $F_{1}$ -ball)   \n139  themselves do not depend on the width. This matches with empirical evidence that shows that   \n140  increasing the width does not hurt generalization [8, 21, 20].   \n141 To use Theorem 1 effectively we need to be able to guarantee that the learned function will have a   \n142 small enough $F_{1}$ -norm. The $F_{1}$ -norm is hard to compute exactly, but it is bounded by the parameter   \n143 norm: if $f(x)=W\\sigma(V x+b)$ , then $\\begin{array}{r}{\\|f\\|_{F_{1}}\\leq\\frac{1}{2}\\left(\\|W\\|_{F}^{2}+\\|V\\|_{F}^{2}+\\|b\\|^{2}\\right)}\\end{array}$ , and this bound is tight   \n144  if the width $w$ is large enough and the parameters are chosen optimally. Adding weight decayl $L_{2}$   \n145 regularization to the cost then leads to bias towards learning with small $F_{1}$ norm. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "146 3.2  Deep Networks ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "147 Since an AccNet is simply the composition of multiple shallow nets, the functions represented by an   \n148 AccNet is included in the set of composition of $F_{1}$ balls. More precisely, if $\\left\\|W_{\\ell}\\right\\|^{2}+\\left\\|\\bar{V}_{\\ell}\\right\\|^{2}+\\left\\|b_{\\ell}\\right\\|^{2}\\leq$   \n149 $2R_{\\ell}$ then $f_{L:1}$ belongs to the set $\\left\\{g_{L}\\circ\\cdot\\cdot\\cdot\\circ g_{1}:g_{\\ell}\\in B_{F_{1}}(\\bar{0},R_{\\ell})\\right\\}$ for some $R_{\\ell}$ , which is width   \n150 agnostic.   \n151  As already noticed in [7], the covering number number is well-behaved under composition, this   \n152 allows us to bound the complexity of AccNets in terms of the individual shallow nets it is made up of:   \n153  Theorem 2. Consider an accordion net of depth $L$ and widths $d_{L},\\ldots,d_{0}$ with corresponding set of   \n154   functions $\\mathcal{F}=\\{f_{L:1}:\\Vert f_{\\ell}\\Vert_{F_{1}}\\leq R_{\\ell},L i p(f_{\\ell})\\}\\leq\\rho_{\\ell}\\}$ With probability $1-\\delta$ over the sampling of the   \n155  training set $X$ from the distribution $\\pi$ supported in $B(0,b)$ we have for all $f\\in\\mathcal F$ ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}(f)-\\tilde{\\mathcal{L}}_{N}(f)\\leq C\\rho b\\rho_{L:1}\\sum_{\\ell=1}^{L}\\frac{R_{\\ell}}{\\rho_{\\ell}}\\sqrt{d_{\\ell}+d_{\\ell-1}\\frac{\\log N}{\\sqrt{N}}}(1+o(1))+c_{0}\\sqrt{\\frac{2\\log^{2}/\\delta}{N}}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "156  Theorem 2 can be extended to ResNets $\\left(f_{L}+i d\\right)\\circ\\cdot\\cdot\\cdot\\circ\\left(f_{1}+i d\\right)$ by simply replacing the Lipschitz   \n157constant $L i p(f_{\\ell})$ by $L i p(f_{\\ell}+i d)$   \n158   The Lipschitz constants $L i p(f_{\\ell})$ are difficult to compute exactly, so it is easiest to simply bound it   \n159 by the product of the operator norms $L i p(f_{\\ell})\\leq\\|W_{\\ell}\\|_{o p}\\,\\|V_{\\ell}\\|_{o p}$ but this bound can often be quite   \n160 loose. The fact that our bound depends on the Lipschitz constants rather than the operator norms   \n161 $\\|W_{\\ell}\\|_{o p}\\,,\\|V_{\\ell}\\|_{o p}$ is thus a significant advantage.   \n162 This bound can be applied to a FCNNs with weight matrices $M_{1},\\dots,M_{L+1}$ , by replacing the middle   \n163 $M_{\\ell}$ with SVD decomposition $U S V^{T}$ in the middle by two matrices $W_{\\ell-1}=\\sqrt{S}V^{T}$ and $V_{\\ell}=U\\sqrt{S}$   \n164  so that the dimensions can be chosen as the rank $d_{\\ell}=\\mathrm{Rank}M_{\\ell+1}$ . The Frobenius norm of the new   \n165  matrices equal the nuclear norm of the original one $\\left\\|W_{\\ell-1}\\right\\|_{F}^{2}=\\left\\|V_{\\ell}\\right\\|_{F}^{2}=\\left\\|M_{\\ell}\\right\\|_{*}$ . Some bounds   \n166 assuming rank sparsity of the weight matrices also appear in [41]. And several recent results have   \n167 shown that weight-decay leads to a low-rank bias on the weight matrices of the network [27, 26, 19]   \n168 and replacing the Frobenius norm regularization with a nuclear norm regularization (according to the   \n169 above mentioned equivalence) will only increase this low-rank bias.   \n170 We compute in Figure 1 the upper bound of Theorem 2 for both AccNets and DNNs, and even though   \n171 we observe a very large gap (roughly of order $10^{3}$ ), we do observe that it captures rate/scaling of the   \n172 test eror (the log-log slope) well. So this generalization bound could be well adapted to predicting   \n173  rates, which is what we will do in the next section.   \n174 Remark. Note that if one wants to compute this upper bound in practical setting, it is important to   \n175train with $L_{2}$ regularization until the parameter norm also converges (this often happens after the   \n176 train and test loss have converged). The intuition is that at initialization, the weights are initialized   \n177 randomly, and they contribute a lot to the parameter norm, and thus lead to a larger generalization   \n178 bound. During training with weight decay, these random initial weights slowly vanish, thus leading   \n179 to a smaller parameter norm and better generalization bound. It might be possible to improve our   \n180 generalization bounds to take into account the randomness at initialization to obtain better bounds   \n181 throughout training, but we leave this to future work. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "image", "img_path": "zuWgB7GerW/tmp/0d0780ad50b84fa14bc1d919135206e13ce69cf2acedd515a9ed95bd813ab27e.jpg", "img_caption": ["Figure 1: Visualization of scaling laws. We observe that deep networks (either AccNets or DNNs) achieve better scaling laws than kernel methods or shallow networks on certain compositional tasks, in agreement with our theory. We also see that our new generalization bounds approximately recover the right saling laws (even though they are orders of magnitude too large overall). We consider a compositional true function $f^{*}\\,=\\,h\\circ g$ where $g$ maps from dimension 15 to 3 while h maps from 3 to 20, and we denote $\\nu_{g},\\nu_{h}$ for the number of times $g,h$ are differentiable. In the first plot $\\nu_{g}=8,\\nu_{h}=1$ so that $g$ is easy to learn while $h$ is hard, whereas in the second plot $\\nu_{g}=9,\\nu_{h}=9$ so both $g$ and $h$ are relatively easier. The third plot presents the decay in test error and generalization bounds for networks evaluated using the real-world dataset, WESAD [37]. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "182 4   Breaking the Curse of Dimensionality with Compositionality ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "183 In this section we study a large family of functions spaces, obtained by taking compositions of   \n184  Sobolev balls. We focus on this family of tasks because they are well adapted to the complexity   \n185  measure we have identified, and because kernel methods and even shallow networks do suffer from   \n186 the curse of dimensionality on such tasks, whereas deep networks avoid it (e.g. Figure 1).   \n187 More precisely, we will show that these sets of functions can be approximated by a AccNets with   \n188 bounded (or in some cases slowly growing) complexity measure ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\nR(f_{1},\\dots,f_{L})=\\prod_{\\ell=1}^{L}L i p(f_{\\ell})\\sum_{\\ell=1}^{L}\\frac{\\|f_{\\ell}\\|_{F_{1}}}{L i p(f_{\\ell})}\\sqrt{d_{\\ell}+d_{\\ell-1}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "189 This will then allow us show that AccNets can (assuming global convergence) avoid the curse of   \n190 dimensionality, even in settings that should suffer from the curse of dimensionality, when the input   \n191 dimension is large and the function is not very smooth (only a few times differentiable). ", "page_idx": 4}, {"type": "image", "img_path": "zuWgB7GerW/tmp/2d0d61d2329f9985e1f1684dfa93d4e47b2161de21ef595ecf2f8cc180ee9d78.jpg", "img_caption": ["Figure 2: A comparison of empirical and theoretical error rates. The first plot illustrates the log decay rate of the test error with respect to the dataset size $N$ based on our empirical simulations. $\\begin{array}{r}{-\\operatorname*{min}\\lbrace\\frac{1}{2},\\frac{\\nu_{g}^{\\star}}{d_{i n}},\\frac{\\nu_{h}}{d_{m i d}}\\rbrace}\\end{array}$ $g$ $h$ $h$ learn than $g$ , and the lower right region where both $f$ and $g$ are easy. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "1924.1 Composition of Sobolev Balls ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "193 The family of Sobolev norms capture some notion of regularity of a function, as it measures the size   \n194  of its derivatives. The Sobolev norm of a function $f:\\mathbb{R}^{\\overline{{d}}_{i n}}\\rightarrow\\mathbf{\\bar{R}}$ is defined in terms of its derivatives   \n195 $\\partial_{x}^{\\alpha}f$ for some $d_{i n}$ -multi-index $\\alpha$ , namely the $W^{\\nu,p}(\\pi)$ -Sobolev norm with integer $\\nu$ and $p\\geq1$ is   \n196defined as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\|f\\|_{W^{\\nu,p}(\\pi)}^{p}=\\sum_{|\\alpha|\\leq\\nu}\\|\\partial_{x}^{\\alpha}f\\|_{L_{p}(\\pi)}^{p}\\,.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "197 Note that the derivative $\\partial_{x}^{\\alpha}f$ only needs to be defined in the \u2018weak' sense, which means that even   \n198 non-differentiable functions such as the ReLU functions can actually have finite Sobolev norm.   \n199 The Sobolev balls $B_{W^{\\nu,p}(\\pi)}(0,R)=\\{f:\\|f\\|_{W^{\\nu,p}(\\pi)}\\leq R\\}$ are a family of function spaces with a   \n200 range of regularity (the larger $\\nu$ , the more regular). This regularity makes these spaces of functions   \n201 learnable purely from the fact that they enforce the function $f$ to vary slowly as the input changes.   \n202 Indeed we can prove the following generalization bound:   \n203 Proposition 3. Given a distribution $\\pi$ withsupport the $L_{2}$ ballwithradius $^b$ wehave that with   \n204  probability $1-\\delta$ for all functions $f\\in\\mathcal{F}=\\{f:\\|f\\|_{W^{\\nu,2}}\\leq R,\\|f\\|_{\\infty}\\leq R\\}$ ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}(f)-\\tilde{\\mathcal{L}}_{N}(f)\\leq2\\rho C_{1}R E_{\\nu/d}(N)+c_{0}\\sqrt{\\frac{2\\log^{2}\\!/\\delta}{N}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "205   where $E_{r}(N)=N^{-\\frac{1}{2}}\\;i f\\,r>\\frac{1}{2}$ \uff0c $\\begin{array}{r}{E_{r}(N)=N^{-\\frac{1}{2}}\\log N\\,i f r=\\frac{1}{2}}\\end{array}$ and $\\begin{array}{r}{E_{r}(N)=N^{-r}\\;i f\\,r<\\frac{1}{2}}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "206 But this result also illustrates the curse of dimensionality: the differentiability $\\nu$ needs to scale with   \n207 the input dimension $d_{i n}$ to obtain a reasonable rate. If instead $\\nu$ is constant and $d_{i n}$ grows, then the   \n208  number of datapoints $N$ needed to guarantee a generalization gap of at most $\\epsilon$ scales exponentially in   \n209 $d_{i n}$ , i.e. N \\~ e- \\* . One way to interpret this issue is that regularity becomes less and less useful the   \n210 larger the dimension: knowing that similar inputs have similar outputs is useless in high dimension   \n211  where the closest training point $x_{i}$ to a test point $x$ is typically very far away. ", "page_idx": 5}, {"type": "text", "text": "212 4.1.1  Breaking the Curse of Dimensionality with Compositionality ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "213 To break the curse of dimensionality, we need to assume some additional structure on the data or task   \n214  which introduces an \u2018'intrinsic dimension\u2019 that can be much lower than the input dimension $d_{i n}$   \n215  Manifold hypothesis: If the input distribution lies on a $d_{s u r f}$ -dimensional manifold, the error rates   \n216 typically depends on $d_{s u r f}$ instead of $d_{i n}$ [38, 10].   \n217 Known Symmetries: If $f^{*}(g\\cdot x)=f^{*}(x)$ for a group action $\\cdot$ w.r.t. a group $G$ , then $f^{*}$ can be   \n218 written as the composition of a modulo map $g^{\\ast}:\\mathbb{R}^{d_{i n}}\\rightarrow\\mathbb{R}^{d_{i n}}/G$ which maps pairs of inputs which   \n219 are equivalent up to symmetries to the same value (pairs $x,y$ s.t. $y=g\\cdot x$ for some $g\\in G$ ), and then   \n220 a second function $h^{\\ast}:\\mathbb{R}^{d_{i n}}/G\\to\\mathbb{R}^{d_{o u t}}$ , then the complexity of the task will depend on the dimension   \n221 of the modulo space $\\mathbb{R}^{d_{i n}}/G$ which can be much lower. If the symmetry is known, then one can for   \n222 example fix $g^{\\ast}$ and only learn $h^{*}$ (though other techniques exist, such as designing kernels or features   \n223 that respect the same symmetries) [31].   \n224 Symmetry Learning: However if the symmetry is not known then both $g^{*}$ and $h^{*}$ have to be learned,   \n225  and this is where we require feature learning and/or compositionality. Shallow networks are able   \n226 to learn translation symmetries, since they can learn so-called low-index functions which satisfy   \n227 $f^{*}(x)=f^{*}(P x)$ for some projection $P$ (with a statistical complexity that depends on the dimension   \n228 of the space one projects into, not the full dimension [5, 2]). Low-index functions correspond exactly   \n229 to the set of functions that are invariant under translation along the kernel $\\ker P$ . To learn general   \n230 symmetries, one needs to learn both $h^{*}$ and the modulo map $g^{*}$ simultaneously, hence the importance   \n231  of feature learning.   \n232For $g^{*}$ to be learnable efficiently, it needs to be regular enough to not suffer from the curse of   \n233 dimensionality, but many traditional symmetries actually have smooth modulo maps, for example   \n234the modulo map $g^{*}(x)=\\|x\\|^{2}$ for rotation invariance. This can be understood as a special case of   \n235 composition of Sobolev functions, whose generalization gap can be bounded: ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "image", "img_path": "zuWgB7GerW/tmp/23368e74c7da4bca695d5bcb590bab283ad5cb8f3986f16f1a4f47506934e736.jpg", "img_caption": ["Figure 3: Comparing error rates for shallow and AccNets: shallow nets vs. AccNets, and kernel methods vs. AccNets. The left two graphs shows the empirical decay rate of test error with respect to dataset size (N) for both shallow nets and kernel methods. In contrast to our earlier empirical findings for AccNets, both shallow nets and kernel methods exhibit a slower decay rate in test error. The right two graphs present the differences in log decay rates between shallow nets and AccNets, as well as between kernel methods and AccNets. AccNets almost always obtain better rates, with a particularly large advantage at the bottom and middle-left. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "236  Theorem  4.  Consider  the function set $\\begin{array}{r l r}{\\mathcal{F}}&{{}=}&{\\mathcal{F}_{L}}\\end{array}$ ${\\mathcal{F}}_{1}$ where $\\mathcal{F}_{\\ell}^{\\mathrm{~\\,~\\,~}}=$ $\\left\\{f_{\\ell}:\\mathbb{R}^{d_{\\ell-1}}\\to\\mathbb{R}^{d_{\\ell}}$ $\\|f_{\\ell}\\|_{W^{\\nu_{\\ell},2}}\\leq R_{\\ell},\\|f_{\\ell}\\|_{\\infty}\\leq b_{\\ell},L i p(f_{\\ell})\\leq\\rho_{\\ell}\\}$ and let $r_{m i n}=\\operatorname*{min}_{\\ell}r_{\\ell}$ for 238re =d then with probabiliry  we haveforall fEfe ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}(f)-\\tilde{\\mathcal{L}}_{N}(f)\\leq\\rho C_{0}\\left(\\sum_{\\ell=1}^{L}{(C_{\\ell}\\rho_{L:\\ell+1}R_{\\ell})^{\\frac{1}{r_{m i n}+1}}}\\right)^{r_{m i n}+1}E_{r_{m i n}}(N)+c_{0}\\sqrt{\\frac{2\\log^{2}/\\delta}{N}},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "239  where $C_{\\ell}$ depends only on $d_{\\ell-1},d_{\\ell},\\nu_{\\ell},b_{\\ell-1}$ ", "page_idx": 6}, {"type": "text", "text": "240 We see that only the smallest ratio $r_{m i n}$ matters when it comes to the rate of learning. And actually   \n241 the above result could be slightly improved to show that the sum over all layers could be replaced by   \n242  a sum over only the layers where the ratio $r_{\\ell}$ leads to the worst rate $E_{r_{\\ell}}(N)=E_{r_{m i n}}(N)$ (and the   \n243 other layers contribute an asymptotically subdominant amount).   \n244 Coming back to the symmetry learning example, we see that the hardness of learning a function of   \n245  the type $f^{*}=h\\circ g$ with inner dimension $d_{m i d}$ and regularities $\\nu_{g}$ and $\\nu_{h}$ , the error rate will be (up   \n246 to logterms) $\\begin{array}{r}{N^{-\\operatorname*{min}\\{\\frac{1}{2},\\frac{\\nu_{g}}{d_{i n}},\\frac{\\nu_{h}}{d_{m i d}}\\}}}\\end{array}$ hsugtsfin   \n247 term attains the minimum: a regime where both $g$ and $h$ are easy to learn and we have $N^{-\\frac{1}{2}}$ learning,   \n248  a regime $g$ is hard, and a regime where $h$ is hard. The last two regimes differentiate between tasks   \n249 where learning the symmetry is hard and those where learning the function knowing its symmetries is   \n250hard.   \n251 In contrast, without taking advantage of the compositional structure, we expect $f^{*}$ to be only   \n252 $\\operatorname*{min}\\{\\nu_{g},\\nu_{h}\\}$ times differentiable, so trying to learn it as a single Sobolev function would lead to an   \n253 errorate of N-min  mi(a) $\\begin{array}{r}{N^{-\\operatorname*{min}\\{\\frac{1}{2},\\frac{\\operatorname*{min}\\{\\nu_{g},\\nu_{h}\\}}{d_{i n}}\\}}=N^{-\\operatorname*{min}\\{\\frac{1}{2},\\frac{\\nu_{g}}{d_{i n}},\\frac{\\nu_{h}}{d_{i n}}\\}}}\\end{array}$ which is no beter than the compositional   \n254 rate, and is stritly worse whenever $\\nu_{h}<\\nu_{g}$ and $\\begin{array}{r}{\\frac{\\nu_{h}}{d_{i n}}<\\frac{1}{2}}\\end{array}$ (wecan always assume $d_{m i d}\\leq d_{i n}$ since   \n255one could always choose $d=i d$   \n256  Furthermore, since multiple compositions are possible, one can imagine a hierarchy of symmetries   \n257 that slowly reduce the dimensionality with less and less regular modulo maps. For example one could   \n258 imagine a composition $f_{L}\\circ\\cdot\\cdot\\circ f_{1}$ with dimensions $d_{\\ell}=d_{0}2^{-\\ell}$ and regularities $\\nu_{\\ell}=\\^{\\star}d_{0}2^{-\\ell}$ so that   \n259 the ratios remain constant $\\begin{array}{r}{r_{\\ell}=\\frac{d_{0}2^{-\\ell}}{d_{0}2^{-\\ell+1}}=\\frac{1}{2}}\\end{array}$ leadin toa almost parametri rateof $N^{-\\frac{1}{2}}\\log N$   \n260 even though the function may only be $d_{0}2^{-L}$ times differentiable. Without compositionality, the rate   \n261 would only be N-2-L   \n262Remark. In the case of a single Sobolev function, one can show that the rate $E_{\\nu_{/d}}(N)$ is in some   \n263 sense optimal, by giving an information theoretic lower bound with matching rate. A naive argument   \n264 suggests that the rateof $E_{\\operatorname*{min}\\{r_{1},\\ldots,r_{L}\\}}(N)$ should similarly be optimal: assume that the minimum   \n265 $r_{\\ell}$ is attained at a layer $\\ell$ , then one can consider the subset of functions such that the image   \n266 $f_{\\ell-1:1}(B(0,r))$ contains a ball $B(z,r^{\\prime})\\subset\\mathbb{R}^{d_{\\ell-1}}$ and that the function $f_{L;\\ell+1}$ .s $\\beta$ -non-contracting   \n267 $\\|f_{L:\\ell+1}(x)-f_{L:\\ell+1}(y)\\|\\geq\\beta\\,\\|x-y\\|$ , then learning $f_{L:1}$ should be as hard as learning $f_{\\ell}$ over the   \n268  ball $B(z,r^{\\prime})$ (more rigorously this could be argued from the fact that any $\\epsilon_{}$ -covering of $f_{L;1}$ can be   \n269   mapped to an $\\epsilon/\\beta$ -covering of $f_{\\ell})$ ,thus forcing a rate of at least $E_{r_{\\ell}}(N)=E_{\\mathrm{min}\\{r_{1},\\dots,r_{L}\\}}(N)$ ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "270 An analysis of minimax rates in a similar setting has been done in [22] ", "page_idx": 7}, {"type": "text", "text": "271 4.2  Breaking the Curse of Dimensionality with AccNets ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "272 Now that we know that composition of Sobolev functions can be easily learnable, even in settings   \n273where the curse of dimensionality should make it hard to learn them, we need to find a model that can   \n274  achieve those rates. Though many models are possible 2, we focus on DNNs, in particular AccNets.   \n275 Assuming convergence to a global minimum of the loss of sufficiently wide AccNets with two types   \n276  of regularization, one can guarantee close to optimal rates:   \n277 Theorem 5. Given a true function $f_{L^{*};1}^{*}=f_{L^{*}}^{*}\\circ\\cdot\\cdot\\circ f_{1}^{*}$ going through the dimensions $d_{0}^{*},\\ldots,d_{L^{*}}^{*}$ \uff0c   \n278 along with a continuous input distribution $\\pi_{0}$ supported in $B(0,b_{0})$ ,such that the distributions $\\pi_{\\ell}$   \n279 of $f_{\\ell}^{*}(x)$ (for $x\\sim\\pi_{0}$ )are continuous too and supported inside $B(0,b_{\\ell})\\subset\\mathbb{R}^{d_{\\ell}^{*}}$ .Further assume   \n280that therearedifferentiabilies $\\nu_{\\ell}$ and radi $R_{\\ell}$ such that $\\|f_{\\ell}^{*}\\|_{W^{\\nu_{\\ell},2}(B(0,b_{\\ell}))}\\leq R_{\\ell}$ and pe such that   \n281 $L i p(f_{\\ell}^{*})\\le\\rho_{\\ell}$ For an infinite width AccNet with $L\\geq L^{*}$ and dimensions $d_{\\ell}\\geq d_{1}^{*},\\ldots,d_{L^{*}-1}^{*}$ \uff0c $w e$   \n282 have for the ratios re = a+3: ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "283 ", "page_idx": 7}, {"type": "text", "text": "284 ", "page_idx": 7}, {"type": "text", "text": "285 ", "page_idx": 7}, {"type": "text", "text": "286 ", "page_idx": 7}, {"type": "text", "text": "287 There are a number of limitations to this result. First we assume that one is able to recover the global   \n288 minimizer of the regularized loss, which should be hard in general3 (we already know from [5] that   \n289 this is NP-hard for shallow networks and a simple $F_{1}$ -regularization). Note that it is sufficient to   \n290recover a network $f_{L:1}$ whose regularized loss is within a constant of the global minimum, which   \n291 might be easier to guarantee, but should still be hard in general. The typical method of training with   \n292 GD on the regularized loss is a greedy approach, which might fail in general but could recover almost   \n293 optimal parameters under the right conditions (some results suggest that training relies on first order   \n294 correlations to guide the network in the right direction [2, 1, 35]).   \nthari affau a tua daaff.   \n296  First regularization: The first regularization term leads to almost optimal rates, up to the change   \n2971 from re =  to re = a+3 which is negligible for large dimensions $d_{\\ell}$ and differentiabilities $\\nu_{\\ell}$ The   \n298 first problem is that it requires an infinite width at the moment, because we were not able to prove   \n299 that a function with bounded $F_{1}$ -norm and Lipschitz constant can be approximated by a sufficiently   \n300 wide shallow networks with the same (or close) $F_{1}$ -norm and Lipschitz constant (we know from [5]   \n301 that it is possible without preserving the Lipschitzness). We are quite hopeful that this condition   \n302 might be removed in future work.   \n303  The second and more significant problem is that the Lipschitz constants $L i p(f_{\\ell})$ are difficult to   \n304 optimize over. For finite width networks it is in theory possible to take the max over all linear regions,   \n305 but the complexity might be unreasonable. It might be more reasonable to leverage an implicit bias   \n306 instead, such as a large learning rate, because a large Lipschitz constant implies that the nework is   \n307 sensible to small changes in its parameters, so GD with a large learning rate should only converge to   \n308 minima with a small Lipschitz constant (such a bias is described in [26]). It might also be possible to   \n309 replace the Lipschitz constant in our generalization bounds, possibly along the lines of [43].   \n310 Second regularization: The second regularization term actually does not require an infinite width,   \n311 only a sufficiently large one. Also its regularization term is equivalent to $\\prod(\\|\\dot{W_{\\ell}}\\|^{2}+\\|V_{\\ell}\\|^{2}+\\|b_{\\ell}\\|^{2})$   \n312 which is much closer to the traditional $L_{2}$ -regularization (and actually one could prove the same   \n313 or very similar rates for $L_{2}$ -regularization). The issue is that it lead to rates that could be far from   \n314optimal depending on the ratios $\\tilde{r}_{\\ell}$ : it recovers the same rate as the first regularization term if no   \n315more than one ratio $\\tilde{r}_{\\ell}$ is smallerthan $\\frac{1}{2}$ uimayftrati $\\frac{1}{2}$ t can be arbitrarily   \n316  smaller.   \n317 In Figure 2, we compare the empirical rates (by doing a linear fit on a log-log plot of test error as a   \n318  function of $N$ ) and the predicted optimal rates $\\begin{array}{r}{\\operatorname*{min}\\{\\frac{\\tilde{1}}{2},\\frac{\\nu_{g}}{d_{i n}},\\frac{\\nu_{h}}{d_{m i d}}\\}}\\end{array}$ and observe a pretty good match.   \n319 Though surprisingly, it appears the the empirical rates tend to be slightly better than the theoretical   \n320ones.   \n321 Remark. As can be seen in the proof of Theorem5, when the depth $L$ is strictly larger than the true   \n322depth $L^{*}$ , one needs to add identity layers, leading to a so-called Bottleneck structure, which was   \n323 proven to be optimal and observed empirically in [27, 26, 45]. These identity layers add a term   \n324  that scales linearly in the additional depth t thefrst ulaization, andanexpenl   \n325  prefactor $(2d_{m i n}^{*})^{L-L^{*}}$ to the second. It might be possible to remove these factors by leveraging the   \n326 bottleneck structure, or simply by switching to ResNets. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "327 5\u3000Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "328 We have given a generalization bound for Accordion Networks and as an extension Fully-Connected   \n329networks. It depends on $F_{1}$ -norms and Lipschitz constants of its shallow subnetworks. This allows us   \n330 to prove under certain assumptions that AccNets can learn general compositions of Sobolev functions   \n331 efficiently, making them able to break the curse of dimensionality in certain settings, such as in the   \n332 presence of unknown symmetries. ", "page_idx": 8}, {"type": "text", "text": "333 References ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "334 [1]  Emmanuel Abbe, Enric Boix Adsera, and Theodor Misiakiewicz. The merged-staircase property:   \n335 a necessary and nearly sufficient condition for sgd learning of sparse functions on two-layer   \n336 neural networks. In Conference on Learning Theory, pages 4782-4887. PMLR, 2022.   \n337 [2] Emmanuel Abbe, Enric Boix-Adsera, Matthew Stewart Brennan, Guy Bresler, and   \n338 Dheeraj Mysore Nagaraj. The staircase property: How hierarchical structure can guide deep   \n339 learning. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advances   \n340 in Neural Information Processing Systems, 2021.   \n341 [3]  Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via   \n342 over-parameterization. pages 242-252, 2019.   \n343 [4]  Kendall Atkinson and Weimin Han. Spherical harmonics and approximations on the unit   \n344 sphere: an introduction, volume 2044. Springer Science & Business Media, 2012.   \n345 [5]  Francis Bach. Breaking the curse of dimensionality with convex neural networks. The Journal   \n346 of Machine Learning Research, 18(1):629-681, 2017.   \n347 [6]  Andrew R Barron and Jason M Klusowski. Complexity, statistical risk, and metric entropy of   \n348 deep nets using total path variation. stat, 1050:6, 2019.   \n349 [7]  Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds   \n350 for neural networks. Advances in neural information processing systems, 30, 2017.   \n351 [8]  Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine  \n352 learning practice and the classical bias-variance trade-off. Proceedings of the National Academy   \n353 of Sciences, 116(32):15849-15854, 2019.   \n354 [9] M. S. Birman and M. Z. Solomjak. Piecewise-polynomial approximations of functions of the   \n355 classes $W_{p}^{\\alpha}$ . Mathematics of The USSR-Sbornik, 2:295-317, 1967.   \n356  [10] Minshuo Chen, Haoming Jiang, Wenjing Liao, and Tuo Zhao. Nonparametric regression on   \n357 low-dimensional manifolds using deep relu networks: Function approximation and statistical   \n358 recovery. Information and Inference: A Journal of the IMA, 11(4):1203-1253, 2022.   \n359[11] L\u00e9naic Chizat and Francis Bach. On the Global Convergence of Gradient Descent for Over  \n360 parameterized Models using Optimal Transport. In Advances in Neural Information Processing   \n361 Systems 31, pages 3040-3050. Curran Associates, Inc., 2018.   \n362  [12] L\u00e9naic Chizat and Francis Bach. Implicit bias of gradient descent for wide two-layer neural   \n363 networks trained with the logistic loss. In Jacob Abernethy and Shivani Agarwal, editors,   \n364 Proceedings of Thirty Third Conference on Learning Theory, volume 125 of Proceedings of   \n365 Machine Learning Research, pages 1305-1338. PMLR, 09-i2 Jul 2020.   \n366[13] Feng Dai. Approximation theory and harmonic analysis on spheres and balls. Springer, 2013.   \n367  [14] Zhen Dai, Mina Karzand, and Nathan Srebro. Representation costs of linear neural networks:   \n368 Analysis and design. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors,   \n369 Advances in Neural Information Processing Systems, 2021.   \n370 [15] Andreas Damianou and Neil D. Lawrence. Deep Gaussian processes. In Carlos M. Carvalho   \n371 and Pradeep Ravikumar, editors, Proceedings of the Sixteenth International Conference on   \n372 Artificial Intelligence and Statistics, volume 31 of Proceedings of Machine Learning Research,   \n373 pages 207-215, Scottsdale, Arizona, USA, 29 Apr-01 May 2013. PMLR.   \n374  [16] Simon S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably   \n375 optimizes over-parameterized neural networks. In International Conference on Learning   \n376 Representations, 2019.   \n377 [17] I. Dumer, M.S. Pinsker, and V.V. Prelov. On coverings of ellipsoids in euclidean spaces. IEEE   \n378 Transactions on Information Theory, 50(10):2348-2356, 2004.   \n379[18] Lawrence C Evans. Partial differential equations, volume 19. American Mathematical Society,   \n380 2022.   \n381 [19]  Tomer Galanti, Zachary S Siegel, Aparna Gupte, and Tomaso Poggio. Sgd and weight decay   \n382 provably induce a low-rank bias in neural networks. arXiv preprint arXiv:2206.05794, 2022.   \n383 [20] Mario Geiger, Arthur Jacot, Stefano Spigler, Franck Gabriel, Levent Sagun, Stephane d'Ascoli,   \n384 Giulio Biroli, Cl\u00e9ment Hongler, and Matthieu Wyart. Scaling description of generalization   \n385 with number of parameters in deep learning. Journal of Statistical Mechanics: Theory and   \n386 Experiment, 2020(2):023401, 2020.   \n387 [21] Mario Geiger, Stefano Spigler, Stephane d\u2019Ascoli, Levent Sagun, Marco Baity-Jesi, Giulio   \n388 Biroli, and Matthieu Wyart. Jamming transition as a paradigm to understand the loss landcape   \n389 of deep neural networks. Physical Review E, 100(1):012115, 2019.   \n390 [22] Matteo Giordano, Kolyan Ray, and Johannes Schmidt-Hieber. On the inability of gaussian   \n391 process regression to optimally learn compositional functions. In Alice H. Oh, Alekh Agarwal,   \n392 Daniell Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing   \n393 Systems, 2022.   \n394  [23] Antoine Gonon, Nicolas Brisebarre, Elisa Riccietti, and Remi Gribonval. A path-norm toolkit   \n395 for modern networks: consequences, promises and challenges. In The Twelfth International   \n396 Conference on Learning Representations, 2023.   \n397 [24] Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characterizing implicit bias   \n398 in terms of optimization geometry. In Jennifer Dy and Andreas Krause, editors, Proceedings of   \n399 the35thInternationalConferenceonMachineearning,volume0ofProceedings ofMachine   \n400 Learning Research, pages 1832-1841. PMLR, 10-15 Jul 2018.   \n401 [25] Daniel Hsu, Ziwei Ji, Matus Telgarsky, and Lan Wang. Generalization bounds via distiltion.   \n402 In International Conference on Learning Representations, 2021.   \n403  [26] Arthur Jacot. Bottleneck structure i learned features: Low-dimension vs regularity tradeoff n   \n404 A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in   \n405 Neural Information Processing Systems, volume 36, pages 23607-23629. Curran Associates,   \n406 Inc., 2023.   \n407  [27]  Arthur Jacot. Implicit bias of large depth networks: a notion of rank for nonlinear functions. In   \n408 The Eleventh International Conference on Learning Representations, 2023.   \n409  [28] Arthur Jacot, Franck Gabriel, and Cl\u00e9ment Hongler. Neural Tangent Kernel: Convergence and   \n410 Generalization in Neural Networks. In Advances in Neural Information Processing Systems 31,   \n411 pages 8580-8589. Curran Associates, Inc., 2018.   \n412 [29] Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio.Fantastic   \n413 generalization measures and where to find them. arXiv preprint arXiv: 1912.02178, 2019.   \n414  [30] Zhiyuan Li, Yuping Luo, and Kaifeng Lyu. Towards resolving the implicit bias of gradient   \n415 descent for matrix factorization: Greedy low-rank learning. In International Conference on   \n416 Learning Representations, 2020.   \n417 [31] Stephane Mallat. Group invariant scattering. Communications on Pure and Applied Mathematics,   \n418 65(10):1331-1398, 2012.   \n419  [32] Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural   \n420 networks. In Conference on learning theory, pages 1376-1401. PMLR, 2015.   \n421 [3]  Atsushi Nitanda and Taiji Suzuki. Optimal rates for averaged stochastic gradient descent under   \n422 neural tangent kernel regime. In International Conference on Learning Representations, 2020.   \n423  [34] Greg Ongie and Rebecca Willett. The role of linear layers in nonlinear interpolating networks.   \n424 arXiv preprint arXiv:2202.00856, 2022.   \n425 [35] Leonardo Petrini, Francesco Cagnetta, Umberto M Tomasini, Alessandro Favero, and Matthieu   \n426 Wyart. How deep neural networks learn compositional data: The random hierarchy model.   \n427 arXiv preprint arXiv:2307.02129, 2023.   \n428  [36] Grant Rotskoff and Eric Vanden-Eijnden. Parameters as interacting particles: long time   \n429 convergence and asymptotic error scaling of neural networks. In Advances in Neural Information   \n430 Processing Systems $3l$ , pages 7146-7155. Curran Associates, Inc., 2018.   \n431 [37] Philip Schmidt, Attila Reiss, Robert Duerichen, Claus Marberger, and Kristof Van Laerhoven.   \n432 Introducing wesad, a multimodal dataset for wearable stress and affect detection. In Proceedings   \n433 of the 20th ACM International Conference on Multimodal Interaction, ICMI '18, page 400-408,   \n434 New York, NY, USA, 2018. Association for Computing Machinery.   \n435[38]  Johannes Schmidt-Hieber. Deep relu network approximation of functions on a manifold. arXiv   \n436 preprint arXiv: 1908.00695, 2019.   \n437 [39] Johannes Schmidt-Hieber. Nonparametric regression using deep neural networks with ReLU   \n438 activation function. The Annals of Statistics, 48(4):1875 - 1897, 2020.   \n439 [40] Mark Sellke. On size-independent sample complexity of relu networks. Information Processing   \n440 Letters, page 106482, 2024.   \n441 [41] Taiji Suzuki, Hiroshi Abe, and Tomoaki Nishimura. Compression based bound for non  \n442 compressed network: unified generalization error analysis of large compressible deep neural   \n443 network. In International Conference on Learning Representations, 2020.   \n444 [42] Zihan Wang and Arthur Jacot.  Implicit bias of SGD in $l_{2}$ -regularized linear DNNs: One  \n445 way jumps from high to low rank. In The Twelfth International Conference on Learning   \n446 Representations, 2024.   \n447 [43] Colin Wei and Tengyu Ma. Data-dependent sample complexity of deep neural networks via   \n448 lipschitz augmentation. Advances in Neural Information Processing Systems, 32, 2019.   \n449  [44] E Weinan, Chao Ma, and Lei Wu. Barron spaces and the compositional function spaces for   \n450 neural network models. arXiv preprint arXiv: 1906.08039, 2019.   \n451[45]  Yuxiao Wen and Arthur Jacot. Which frequencies do cnns need? emergent bottleneck structure   \n452 in feature learning. to appear at ICML, 2024.   \n453  [46] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding   \n454 deep learning requires rethinking generalization. ICLR 2017 proceedings, Feb 2017. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "455 The Appendix is structured as follows: ", "page_idx": 12}, {"type": "text", "text": "456 1. In Section A, we describe the experimental setup and provide a few additional experiments.   \n457 2. In Section B, we prove Theorems 1 and 2 from the main.   \n458 3. In Section C, we prove Proposition 3 and Theorem 4.   \n459 4. In Section D, we prove Theorem 5 and other approximation results concerning Sobolev   \n460 functions.   \n461 5. In Section E, we prove a few technical results on the covering number. ", "page_idx": 12}, {"type": "text", "text": "462 A  Experimental Setup ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "463  In this section, we review our numerical experiments and their setup both on synthetic and real-world   \n464  datasets in order to address theoretical results more clearly and intuitively. ", "page_idx": 12}, {"type": "text", "text": "465 A.1 Dataset ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "466 A.1.1  Emperical Dataset ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "467 The Mat\u00e9rn kernel is considered a generalization of the radial basis function (RBF) kernel. It   \n468 controls the differentiability, or smoothness, of the kernel through the parameter $\\nu$ .As $\\nu\\to\\infty$ , the   \n469 Mat\u00e9rn kernel converges to the RBF kernel, and as $\\nu\\to0$ , it converges to the Laplacian kernel, a   \n470 O-differentiable kernel. In this study, we utilized the Mat\u00e9rn kernel to generate Gaussian Process (GP)   \n471 samples based on the composition of two Mat\u00e9rn kernels, $K_{g}$ and $K_{h}$ , with varying differentiability   \n472 in the range $[0.5,10]\\!\\times\\![0.5,10]$ . The input dimension $(d_{i n})$ of the kernel, the bottleneck mid-dimension   \n473 $(d_{m i d})$ , and the output dimension $(d_{o u t})$ are 15, 3, and 20, respectively. ", "page_idx": 12}, {"type": "text", "text": "474  This outlines the general procedure of our sampling method for synthetic data: ", "page_idx": 12}, {"type": "text", "text": "475 1. Sample the training dataset $X\\in\\mathbb{R}^{D\\times d_{i n}}$   \n476 2. From X, compute the $D\\times D$ kernel $K_{g}$ with given $\\nu_{g}$   \n477 3. From $K_{g}$ , sample $Z\\in\\mathbb{R}^{D\\times d_{m i d}}$ with columns sampled from the Gaussian $\\mathcal{N}(0,K_{g})$   \n478 4. From $Z$ , compute $K_{g}$ with given $\\nu_{h}$   \n479 5. From $K_{h}$ , sample the test dataset $Y\\in\\mathbb{R}^{D\\times d_{o u t}}$ with columns sampled from the Gaussian   \n480 ${\\mathcal{N}}(0,K_{h})$   \n481 We utilized four AMD Opteron 6136 processors (2.4 GHz, 32 cores) and $128\\,\\mathrm{GB}$ ofRAM togenerate   \n482  our synthetic dataset. The maximum possible dataset size for 128 GB of RAM is approximately   \n483 52,500; however, we opted for a synthetic dataset size of 22,000 due to the computational expense   \n484  associated with sampling the Mat\u00e9rn kernel. This decision was made considering the time complexity   \n485of ${\\mathcal{O}}(n^{3})$ and the space complexity of ${\\mathcal{O}}(n^{2})$ involved. Out of the 22,000 dataset points, 20,000 were   \n486 allocated for training data, and 2,0o0 were used for the test dataset ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "487A.1.2 Real-world dataset: WESAD ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "488 In our study, we utilized the Wearable Stress and Affect Detection (WESAD) dataset to train our   \n489 AccNets for binary classification. The WESAD dataset, which is publicly accessible, provides   \n490 multimodal physiological and motion data collected from 15 subjects using devices worn on the wrist   \n491 and chest. For the purpose of our experiment, we specifically employed the Empatica E4 wrist device   \n492 to distinguish between non-stress (baseline) and stress conditions, simplifying the classification task   \n493to these two categories.   \n494  After preprocessing, the dataset comprised a total of 136,482 instances. We implemented a train-test   \n495  split ratio of approximately 75:25, resulting in 100,000 instances for the training set and 36,482   \n496 instances for the test set. The overall hyperparameters and architecture of the AccNets model applied   \n497 to the WESAD dataset were largely consistent with those used for our synthetic data. The primary   \n498  differences were the use of 100 epochs for each iteration of Ni from Ns, and a learning rate set to   \n4991e-5. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "image", "img_path": "zuWgB7GerW/tmp/3c1bbc2d8a363d86c92ef29653dfe588ef6519c3e23a03efaf69a3f7a7980663.jpg", "img_caption": ["Figure 4: A comparison: singular values of the weight matrices for DNN and AccNets models. The first two plots represent cases where $N=10000$ while the right two plots correspond to $N=$ 200.The number of outliers at the top of each plot signifies the rank of each network. The plots with $N=10000$ datasets demonstrate a clearer capture of the true rank compared to those with $N=200$ indicating that a higher dataset count provides more accurate rank determination "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "500 A.2 Model setups ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "501 To investigate the scaling law of test error for our synthetic data, we trained models using $N_{i}$   \n502 datapoints from our training data, where $N=[100,200,500,1000,2000,5000,10000,20000].$ The   \n503 models employed for this analysis included the kernel method, shallow networks, fully connected   \n504 deep neural networks (FC DNN), and AccNets. For FC DNN and AccNets, we configured the   \n505 network depth to 12 layers, with the layer widths set as $[d_{i n},500,500,...,500,d_{o u t}]$ for DNNs, and   \n506 $[d_{i}n,900,100,900,...,100,900,d_{o u t}]$ forAccNets.   \n507 To ensure a comparable number of neurons, the width for the shallow networks was set to 50,000,   \n508 resulting in dimensions of $[d_{i n},50000,d_{o u t}]$   \n509 We utilized ReLU as the activation function and $L^{1}$ -norm as the cost function, with the Adam   \n510 optimizer. The total number of batch was set to 5, and the training process was conducted over 3600   \n511 epochs, divided into three phases. The detailed optimizer parameters are as follows:   \n1. For the first 1200 epochs: learning rate $(l r)=1.5*0.001$ , weight decay $=0$   \n2. For the second 1200 epochs: $l r=0.4*0.001$ , weight decay $=0.002$   \n3. For the final 1200 epochs: $l r=0.1*0.001$ , weight decay $=0.005$   \n515  We conducted experiments utilizing 12 NVIDIA V100 GPUs (each with 32GB of memory) over a   \n516  period of 6.3 days to train the synthetic dataset. In contrast, training the WESAD dataset required   \n517only one hour on a single V100 GPU. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "518A.3 Additional experiments ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "519 B AccNet Generalization Bounds ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "520 The proof of generalization for shallow networks (Theorem 1) is the special case $L=1$ of theproof   \n521 of Theorem 2, so we only prove the second:   \n522Theorem 6. Consider an accordion net of depth $L$ and widths $d_{L},\\ldots,d_{0}$ with corresponding set   \n523  of functions $\\mathcal{F}=\\{f_{L:1}\\,:\\,\\|f_{\\ell}\\|_{F_{1}}\\,\\le\\,R_{\\ell},L i p(f_{\\ell})\\,\\le\\,\\rho_{\\ell}\\}$ with input space $\\Omega\\,=\\,B(0,r)$ . For any   \n524 $\\rho$ -Lipschitz loss function $\\ell(x,f(x))$ with $|\\ell(x,y)|\\leq c_{0}$ , we know that with probability $1-\\delta$ over the   \n525 sampling of the training set $X$ from the distribution $\\pi$ we have for all $f\\in\\mathcal F$ ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathcal{L}(f)-\\tilde{\\mathcal{L}}_{N}(f)\\leq C\\rho_{L:1}r\\sum_{\\ell^{\\prime}=1}^{L}\\frac{R_{\\ell^{\\prime}}}{\\rho_{\\ell^{\\prime}}}\\sqrt{d_{\\ell^{\\prime}}+d_{\\ell^{\\prime}-1}}\\frac{\\log N}{\\sqrt{N}}(1+o(1))+c_{0}\\sqrt{\\frac{2\\log^{2}/\\delta}{N}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "526  Proof. The strategy is: (1) prove a covering number bound on $\\mathcal{F}$ (2) use it to obtain a Rademacher   \n527 complexity bound, (3) use the Rademacher complexity to bound the generalization error.   \n528  (1) We define $f_{\\ell}=V_{\\ell}\\circ\\sigma\\circ W_{\\ell}$ so that $f_{\\theta}=f_{L:1}=f_{L}\\circ\\cdot\\cdot\\circ f_{1}$ . First notice that we can write each   \n529 $f_{\\ell}$ as convex combination of its neurons: ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "equation", "text": "$$\nf_{\\ell}(\\boldsymbol x)=\\sum_{i=1}^{w_{\\ell}}v_{\\ell,i}\\sigma(w_{\\ell,i}^{T}\\boldsymbol x)=R_{\\ell}\\sum_{i=1}^{w_{\\ell}}c_{\\ell,i}\\bar{v}_{\\ell,i}\\sigma(\\bar{w}_{\\ell,i}^{T}\\boldsymbol x)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "530 for we,i = $\\begin{array}{r}{\\bar{w}_{\\ell,i}=\\frac{w_{\\ell,i}}{\\|w_{\\ell,i}\\|}}\\end{array}$ \uff0c $\\begin{array}{r}{\\iota_{\\ell,i}=\\frac{v_{\\ell,i}}{\\|v_{\\ell,i}\\|},R_{\\ell}=\\sum_{i=1}^{\\ell}\\|v_{\\ell,i}\\|\\,\\|w_{\\ell,i}\\|\\mathrm{~and~}c_{\\ell,i}=\\frac{1}{R_{\\ell}}\\,\\|v_{\\ell,i}\\|\\,\\|w_{\\ell,i}\\|.}\\end{array}$ ", "page_idx": 14}, {"type": "text", "text": "53  Lt us ow conside sequene $\\epsilon_{k}=2^{-k}$ $k=0,\\ldots,K$ and define $\\tilde{v}_{\\ell,i}^{(k)},\\tilde{w}_{\\ell,i}^{(k)}$ to be the $\\epsilon_{k}$ -covers   \n532 of $\\bar{v}_{\\ell,i},\\bar{w}_{\\ell,i}$ furthermore we may choose $\\tilde{v}_{\\ell,i}^{(0)}=\\tilde{w}_{\\ell,i}^{(0)}=0$ since every unit vctoris within a $\\epsilon_{0}=1$   \n53 distance of the origin. We will now show that on can approximate $f_{\\theta}$ by approximating each of the $f_{\\ell}$   \n534by functions of the form ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\tilde{f}_{\\ell}(\\boldsymbol{x})=R_{\\ell}\\sum_{k=1}^{K_{\\ell}}\\frac{1}{M_{k,\\ell}}\\sum_{m=1}^{M_{k,\\ell}}\\tilde{v}_{\\ell,i_{\\ell,m}^{(k)}}^{(k)}\\sigma(\\tilde{w}_{\\ell,i_{\\ell,m}^{(k)}}^{(k)T}\\boldsymbol{x})-\\tilde{v}_{\\ell,i_{\\ell,m}^{(k)}}^{(k-1)}\\sigma(\\tilde{w}_{\\ell,i_{\\ell,m}^{(k)}}^{(k-1)T}\\boldsymbol{x})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "535 for indices 2,m $i_{\\ell,m}^{(k)}=1,\\ldots,w_{\\ell}$ choosen adequately. Notice that the number of functions of this type   \n536  equals the number of $M_{k,\\ell}$ quadruples $(\\tilde{v}_{\\ell,i_{\\ell,m}^{(k)}}^{(k)},\\tilde{w}_{\\ell,i_{\\ell,m}^{(k)}}^{(k)T},\\tilde{v}_{\\ell,i_{\\ell,m}^{(k)}}^{(k-1)},\\tilde{w}_{\\ell,i_{\\ell,m}^{(k)}}^{(k-1)T})$ where thesevctors belong   \n537   to the $\\epsilon_{k^{\\,\\cdot}}$ resp. $\\epsilon_{k-1}$ -coverings of the $d_{i n}$ -resp. $d_{o u t}$ -dimensional unit sphere. Thus the number of   \n538  such functions is bounded by ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\prod_{k=1}^{K_{\\ell}}\\big(\\mathcal{N}_{2}\\big(\\mathbb{S}^{d_{i n}-1},\\epsilon_{k}\\big)\\mathcal{N}_{2}\\big(\\mathbb{S}^{d_{o u t}-1},\\epsilon_{k}\\big)\\mathcal{N}_{2}\\big(\\mathbb{S}^{d_{i n}-1},\\epsilon_{k-1}\\big)\\mathcal{N}_{2}\\big(\\mathbb{S}^{d_{o u t}-1},\\epsilon_{k-1}\\big)\\big)^{M_{k,\\ell}}\\,,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "539  and we have this choice for all $\\ell=1,\\ldots,L$ . We will show that with sufficiently large $M_{k,\\ell}$ this set   \n540 of functions $\\epsilon$ -covers $\\mathcal{F}$ which then implies that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\log\\mathcal{N}_{2}(\\mathcal{F},\\epsilon)\\le2\\sum_{\\ell=1}^{L}\\sum_{k=1}^{K_{\\ell}}M_{k,\\ell}\\left(\\log\\mathcal{N}_{2}(\\mathbb{S}^{d_{i n}-1},\\epsilon_{k-1})+\\log\\mathcal{N}_{2}(\\mathbb{S}^{d_{i n}-1},\\epsilon_{k-1})\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "$i_{\\ell,m}^{(k)}$ to aprximatea funcion $f_{\\ell}=$ 542 $\\begin{array}{r}{R_{\\ell}\\sum_{i=1}^{w_{\\ell}}c_{\\ell,i}\\bar{v}_{\\ell,i}\\sigma(\\bar{w}_{\\ell,i}^{T}x)}\\end{array}$ with a function $\\tilde{f_{\\ell}}$ . We take all $i_{\\ell,m}^{(k)}$ to be i.i.d. equal to the index $i=$ 543 $1,\\cdots,w_{\\ell}$ with probability $c_{\\ell,i}$ , so that in expectation ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{E}\\tilde{f}_{\\ell}(\\boldsymbol{x})=R_{\\ell}\\sum_{k=1}^{K_{\\ell}}\\sum_{i=1}^{w_{\\ell}}c_{\\ell,i}\\left(\\tilde{v}_{\\ell,i}^{(k)}\\sigma(\\tilde{w}_{\\ell,i}^{(k)T}\\boldsymbol{x})-\\tilde{v}_{\\ell,i}^{(k-1)}\\sigma(\\tilde{w}_{\\ell,i}^{(k-1)T}\\boldsymbol{x})\\right)}}\\\\ &{}&{=R_{\\ell}\\sum_{i=1}^{w_{\\ell}}c_{\\ell,i}\\tilde{v}_{\\ell,i}^{(K)}\\sigma(\\tilde{w}_{\\ell,i}^{(K)T}\\boldsymbol{x}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "544   We will show that this expectation is $O(\\epsilon_{K_{\\ell}})$ -close to $f_{\\ell}$ and that the variance of $\\tilde{f_{\\ell}}$ goes to zero as   \n545 the $M_{\\ell,k}\\mathbf{s}$ grow, allowing us to bound the expected error $\\mathbb{E}\\left\\|f_{L:1}-\\tilde{f}_{L:1}\\right\\|_{\\pi}^{2}\\leq\\epsilon^{2}$ which then implies   \n546 that there must be at least one choice of indices $i_{\\ell,m}^{(k)}$ such that $\\left\\|f_{L:1}-\\tilde{f}_{L:1}\\right\\|_{\\pi}\\leq\\epsilon$ ", "page_idx": 14}, {"type": "text", "text": "547 Let us first bound the distance ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|f_{\\ell}(x)-\\mathbb{E}\\tilde{f}_{\\ell}(x)\\right|\\Big|=R_{\\ell}\\left|\\displaystyle\\left|\\sum_{i=1}^{w_{\\ell}}c_{\\ell,i}\\left(\\bar{v}_{\\ell,i}\\sigma(\\bar{w}_{\\ell,i}^{T}x)-\\bar{v}_{\\ell,i}^{(K)}\\sigma(\\bar{w}_{\\ell,i}^{(K)T}x)\\right)\\right|\\right|}\\\\ &{\\qquad\\qquad\\qquad\\leq R_{\\ell}\\displaystyle\\sum_{i=1}^{w_{\\ell}}c_{\\ell,i}\\left(\\left\\|\\left(\\bar{v}_{\\ell,i}-\\bar{v}_{\\ell,i}^{(K)}\\right)\\sigma(\\bar{w}_{\\ell,i}^{T}x)\\right\\|+\\left\\|\\bar{v}_{\\ell,i}^{(K)}\\left(\\sigma(\\bar{w}_{\\ell,i}^{T}x)-\\sigma(\\bar{w}_{\\ell,i}^{(K)T}x)\\right)\\right\\|\\right)}\\\\ &{\\qquad\\qquad\\leq R_{\\ell}\\displaystyle\\sum_{i=1}^{w_{\\ell}}c_{\\ell,i}\\left(\\left\\|\\bar{v}_{\\ell,i}-\\bar{v}_{\\ell,i}^{(K)}\\right\\|\\left\\|\\bar{w}_{\\ell,i}^{T}x\\right\\|+\\left\\|\\bar{v}_{\\ell,i}^{(K)}\\right\\|\\left\\|\\bar{w}_{\\ell,i}^{T}x-\\bar{w}_{\\ell,i}^{(K)T}x\\right\\|\\right)}\\\\ &{\\qquad\\qquad\\leq R_{\\ell}\\displaystyle\\sum_{i=1}^{w_{\\ell}}c_{\\ell,i}\\epsilon_{K,\\ell}\\left\\|x\\right\\|}\\\\ &{\\qquad=2R_{\\ell}\\epsilon_{K,\\ell}\\left\\|x\\right\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "548  Then we bound the trace of the covariance of $\\tilde{f_{\\ell}}$ which equals the expected square distance between   \n549 $\\tilde{f_{\\ell}}$ and its expectation: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left\\|\\tilde{f}_{i}(x)-\\mathbb{E}\\hat{f}_{i}(x)\\right\\|^{2}}\\\\ &{=\\underset{\\bar{\\lambda}=1}{\\overset{K}{\\sum}}\\frac{\\mathbb{E}_{\\hat{f}_{i}}^{2}M_{\\hat{f}_{i}}}{M_{\\hat{f}_{i}}^{2}}\\underset{\\underset{\\ell=1}{\\overset{N}{\\sum}}}{\\sum}\\left\\|\\phi_{i,\\ell_{i}}^{(k)}\\frac{\\sigma(k)^{2}\\Gamma}{(\\ell_{i},\\ell_{i}^{(k)})}-\\hat{\\eta}_{\\ell,i}^{(k-1)}\\sigma(\\bar{u}_{\\ell,i}^{(k-1)\\tau})-\\mathbb{E}\\left[\\hat{\\eta}_{\\ell,i}^{(k)},\\sigma(\\bar{u}_{\\ell,i}^{(k)\\tau})-\\bar{\\eta}_{\\ell,i}^{(k-1)}\\sigma(\\bar{u}_{\\ell,i}^{(k-1)})\\right]\\right.}\\\\ &{\\left.\\leq\\underset{\\bar{\\lambda}=1}{\\overset{K}{\\sum}}\\frac{R_{\\hat{f}_{i}}^{2}M_{\\hat{f}_{i}}}{M_{\\hat{f}_{i}}^{2}}\\underset{\\underset{\\ell=1}{\\overset{N}{\\sum}}}{\\sum}\\mathbb{E}\\left\\|\\phi_{i,\\ell_{i}}^{(k)}(w_{i,\\ell_{i}}^{(k)\\tau})-\\hat{\\eta}_{\\ell,i}^{(k-1)}\\sigma(\\bar{u}_{\\ell,i}^{(k-1)\\tau})\\right\\|^{2}}\\\\ &{\\underset{\\bar{\\lambda}=1}{\\overset{K}{\\sum}}\\frac{R_{\\hat{f}_{i}}^{2}M_{\\hat{f}_{i}}}{M_{\\hat{f}_{i}}\\underset{\\ell=1}{\\overset{N}{\\sum}}}\\subset\\left\\|\\frac{\\hat{\\eta}_{\\ell}^{(k)}}{\\ell_{i}!}\\sigma\\left(\\bar{u}_{\\ell,i}^{(k)\\tau}\\right)-\\hat{\\eta}_{\\ell,i}^{(k-1)}\\sigma\\left(\\bar{u}_{\\ell,i}^{(k-1)\\tau}\\right)\\right\\|^{2}}\\\\ &{\\underset{\\bar{\\lambda}=1}{\\overset{K}{\\sum}}\\frac{\\mathcal{L}_{\\hat{f}_{i}}^{2}M_{\\hat{f}_{i}}}{M_{\\hat{f}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "550 Putting both together, we obtain ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left\\|f_{\\ell}(x)-\\tilde{f}_{\\ell}(x)\\right\\|^{2}\\leq4R_{\\ell}^{2}\\epsilon_{K_{\\ell}}^{2}\\left\\|x\\right\\|^{2}+\\displaystyle\\sum_{k=1}^{K_{\\ell}}\\frac{36R_{\\ell}^{2}\\left\\|x\\right\\|^{2}}{M_{k,\\ell}}\\epsilon_{k}^{2}}\\\\ {=4R_{\\ell}^{2}\\left\\|x\\right\\|^{2}\\left(\\epsilon_{K_{\\ell}}^{2}+9\\displaystyle\\sum_{k=1}^{K_{\\ell}}\\frac{\\epsilon_{k}^{2}}{M_{k,\\ell}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "551We will now use this bound, together with the Lipschitzness of $f_{\\ell}$ to  bound  the error   \n52 $\\mathbb{E}\\left\\|f_{L:1}(x)-\\tilde{f}_{L:1}(x)\\right\\|^{2}$ . We will do this by induction on the distances $\\mathbb{E}\\left\\|f_{\\ell:1}(x)-\\tilde{f}_{\\ell:1}(x)\\right\\|^{2}$   \n553We start by ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left\\Vert f_{1}(x)-\\tilde{f}_{1}(x)\\right\\Vert^{2}\\leq4R_{1}^{2}\\left\\Vert x\\right\\Vert^{2}\\left(\\epsilon_{K_{\\ell}}^{2}+9\\sum_{k=1}^{K_{\\ell}}\\frac{\\epsilon_{k}^{2}}{M_{k,1}}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{r}_{\\ell:1}(x)-\\widetilde{f}_{\\ell:1}(x)\\Big\\Vert^{2}=\\mathbb{E}\\left[\\mathbb{E}\\left[\\left\\Vert f_{\\ell:1}(x)-\\widetilde{f}_{\\ell:1}(x)\\right\\Vert^{2}|\\widetilde{f}_{\\ell-1:1}\\right]\\right]}\\\\ &{\\qquad\\qquad=\\mathbb{E}\\left\\Vert f_{\\ell:1}(x)-\\mathbb{E}\\left[\\widetilde{f}_{\\ell:1}(x)|\\widetilde{f}_{\\ell-1:1}\\right]\\right\\Vert^{2}+\\mathbb{E}\\mathbb{E}\\left[\\left\\Vert\\widetilde{f}_{\\ell:1}(x)-\\mathbb{E}\\left[\\widetilde{f}_{\\ell:1}(x)|\\widetilde{f}_{\\ell-1:1}\\right]\\right\\Vert^{2}|\\widetilde{f}_{\\ell}\\right.}\\\\ &{\\qquad\\qquad=\\mathbb{E}\\left\\Vert f_{\\ell:1}(x)-f_{\\ell}(\\widetilde{f}_{\\ell-1:1}(x))\\right\\Vert^{2}+\\mathbb{E}\\mathbb{E}\\left[\\left\\Vert\\widetilde{f}_{\\ell:1}(x)-f_{\\ell}(\\widetilde{f}_{\\ell-1:1}(x))\\right\\Vert^{2}|\\widetilde{f}_{\\ell-1:1}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\left.\\leq\\rho_{\\ell}^{2}\\mathbb{E}\\left\\Vert f_{\\ell-1:1}(x)-\\widetilde{f}_{\\ell-1:1}(x)\\right\\Vert^{2}+4R_{\\ell}^{2}\\mathbb{E}\\left\\Vert\\widetilde{f}_{\\ell-1:1}(x)\\right\\Vert^{2}\\left(\\epsilon_{K_{\\ell}}^{2}+9\\displaystyle\\sum_{k=1}^{K_{\\ell}}\\frac{\\epsilon_{k}^{2}}{M_{k,\\ell}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "555 Now since ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left\\|\\tilde{f}_{\\ell-1:1}(x)\\right\\|^{2}\\leq\\left\\|f_{\\ell-1:1}(x)\\right\\|^{2}+\\mathbb{E}\\left\\|f_{\\ell-1:1}(x)-\\tilde{f}_{\\ell-1:1}(x)\\right\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\rho_{\\ell-1}^{2}\\cdot\\cdot\\cdot\\rho_{1}^{2}\\left\\|x\\right\\|^{2}+\\mathbb{E}\\left\\|f_{\\ell-1:1}(x)-\\tilde{f}_{\\ell-1:1}(x)\\right\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "556 we obtain that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left\\|f_{\\ell\\cdot1}(x)-\\tilde{f}_{\\ell\\cdot1}(x)\\right\\|^{2}\\leq\\left(\\rho_{\\ell}^{2}+4R_{\\ell}^{2}\\left(\\epsilon_{K_{\\ell}}^{2}+9\\sum_{k=1}^{K_{\\ell}}\\frac{\\epsilon_{k}^{2}}{M_{k,\\ell}}\\right)\\right)\\mathbb{E}\\left\\|f_{\\ell-1:1}(x)-\\tilde{f}_{\\ell-1:1}(x)\\right\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\left.4R_{\\ell}^{2}\\rho_{\\ell-1}^{2}\\cdot\\cdot\\cdot\\rho_{1}^{2}\\left\\|x\\right\\|^{2}\\left(\\epsilon_{K_{\\ell}}^{2}+9\\sum_{k=1}^{K_{\\ell}}\\frac{\\epsilon_{k}^{2}}{M_{k,\\ell}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "557 Wedene=\u00b2[1+4(k+9) and obtain ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left\\Vert f_{L:1}(x)-\\tilde{f}_{L:1}(x)\\right\\Vert^{2}\\leq4\\sum_{\\ell=1}^{L}\\tilde{\\rho}_{L:\\ell+1}^{2}R_{\\ell}^{2}\\rho_{\\ell-1:1}^{2}\\left\\Vert x\\right\\Vert^{2}\\left(\\epsilon_{K_{\\ell}}^{2}+9\\sum_{k=1}^{K_{\\ell}}\\frac{\\epsilon_{k}^{2}}{M_{k,\\ell}}\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "558Thus for any distribution $\\pi$ over the ball $B(0,r)$ there is a choice of indices $i_{\\ell,m}^{(k)}$ such that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left\\Vert f_{L:1}-\\tilde{f}_{L:1}\\right\\Vert_{\\pi}^{2}\\leq4\\sum_{\\ell=1}^{L}\\tilde{\\rho}_{L:\\ell+1}^{2}R_{\\ell}^{2}\\rho_{\\ell-1:1}^{2}r^{2}\\left(\\epsilon_{K_{\\ell}}^{2}+9\\sum_{k=1}^{K_{\\ell}}\\frac{\\epsilon_{k}^{2}}{M_{k,\\ell}}\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "559 We now simply need to choose $K_{\\ell}$ and $M_{k,\\ell}$ adequately. To reach an error of $2\\epsilon$ we choose ", "page_idx": 16}, {"type": "equation", "text": "$$\nK_{\\ell}=\\left\\lceil-\\log\\epsilon+\\frac{1}{2}\\log\\left[4\\rho_{L:1}^{2}r^{2}\\left(\\sum_{\\ell^{\\prime}=1}^{L}\\frac{R_{\\ell^{\\prime}}}{\\rho_{\\ell^{\\prime}}}\\sqrt{d_{\\ell^{\\prime}}+d_{\\ell^{\\prime}-1}}\\right)\\frac{R_{\\ell}}{\\rho_{\\ell}\\sqrt{d_{\\ell}+d_{\\ell-1}}}\\right]\\right\\rceil\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "560 where $\\begin{array}{r}{\\rho_{L:1}=\\prod_{\\ell=1}^{L}\\rho_{\\ell}}\\end{array}$ Notice that that $\\begin{array}{r}{\\epsilon_{K_{\\ell}}^{2}\\leq\\frac{1}{4\\rho_{L:1}^{2}r^{2}\\left(\\sum_{\\ell^{\\prime}=1}^{L}\\frac{R_{\\ell^{\\prime}}}{\\rho_{\\ell^{\\prime}}}\\sqrt{d_{\\ell^{\\prime}}+d_{\\ell^{\\prime}-1}}\\right)}\\frac{\\rho_{\\ell}\\sqrt{d_{\\ell}+d_{\\ell-1}}}{R_{\\ell}}\\epsilon^{2}.}\\end{array}$ ", "page_idx": 16}, {"type": "text", "text": "561 Given $\\begin{array}{r}{s_{0}=\\sum_{k=1}^{\\infty}{\\sqrt{k}}2^{-k}\\approx1.3473<\\infty}\\end{array}$ , we define ", "page_idx": 16}, {"type": "equation", "text": "$$\nM_{k,\\ell}=\\left\\lceil36\\rho_{L:1}^{2}r^{2}s_{0}\\left(\\sum_{\\ell^{\\prime}=1}^{L}\\frac{R_{\\ell^{\\prime}}}{\\rho_{\\ell^{\\prime}}}\\sqrt{d_{\\ell^{\\prime}}+d_{\\ell^{\\prime}-1}}\\right)\\frac{R_{\\ell}}{\\rho_{\\ell}\\sqrt{d_{\\ell}+d_{\\ell-1}}}\\frac{2^{-k}}{\\sqrt{k}}\\frac{1}{\\epsilon^{2}}\\right\\rceil.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "562 So that for all $\\ell$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle4\\frac{R_{\\ell}^{2}}{\\rho_{\\ell}^{2}}\\left(\\epsilon_{K_{\\ell}}^{2}+9\\sum_{k=1}^{K_{\\ell}}\\frac{\\epsilon_{k}^{2}}{M_{k,\\ell}}\\right)\\le\\frac{\\frac{R_{\\ell}}{\\rho_{\\ell}}\\sqrt{d_{\\ell}+d_{\\ell-1}}}{\\rho_{L;1}^{2}r^{2}\\left(\\sum_{\\ell^{\\prime}=1}^{L}\\frac{R_{\\ell}}{\\rho_{\\ell}}\\sqrt{d_{\\ell}+d_{\\ell-1}}\\right)}\\epsilon^{2}}\\\\ {\\displaystyle+\\,\\frac{\\frac{R_{\\ell}}{\\rho_{\\ell}}\\sqrt{d_{\\ell}+d_{\\ell-1}}}{\\rho_{L;1}^{2}r^{2}\\left(\\sum_{\\ell^{\\prime}=1}^{L}\\frac{R_{\\ell}}{\\rho_{\\ell}}\\sqrt{d_{\\ell}+d_{\\ell-1}}\\right)}\\epsilon^{2\\frac{\\sum_{k^{\\prime}=1}^{K_{\\ell}}\\sqrt{k^{\\prime}}2^{-k^{\\prime}}}{s_{0}}}}\\\\ {\\displaystyle}&{\\le2\\frac{\\frac{R_{\\ell}}{\\rho_{\\ell}}\\sqrt{d_{\\ell}+d_{\\ell-1}}}{\\rho_{L;1}^{2}r^{2}\\left(\\sum_{\\ell^{\\prime}=1}^{L}\\frac{R_{\\ell}}{\\rho_{\\ell}}\\sqrt{d_{\\ell}+d_{\\ell-1}}\\right)}\\epsilon^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "563  Now this also implies that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\tilde{\\rho}_{\\ell}\\leq\\rho_{\\ell}\\exp\\left(2\\frac{\\frac{R_{\\ell}}{\\rho_{\\ell}}\\sqrt{d_{\\ell}+d_{\\ell-1}}}{\\rho_{L:1}^{2}r^{2}\\left(\\sum_{\\ell^{\\prime}=1}^{L}\\frac{R_{\\ell}}{\\rho_{\\ell}}\\sqrt{d_{\\ell}+d_{\\ell-1}}\\right)}\\epsilon^{2}\\right)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "564 and thus ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\tilde{\\rho}_{L:\\ell+1}\\leq\\rho_{L:\\ell+1}\\exp\\left(2\\frac{\\sum_{\\ell^{\\prime}=\\ell+1}^{L}\\frac{R_{\\ell}}{\\rho_{\\ell}}\\sqrt{d_{\\ell}+d_{\\ell-1}}}{\\rho_{L:1}^{2}r^{2}\\left(\\sum_{\\ell^{\\prime}=1}^{L}\\frac{R_{\\ell}}{\\rho_{\\ell}}\\sqrt{d_{\\ell}+d_{\\ell-1}}\\right)}\\epsilon^{2}\\right)\\leq\\rho_{L:\\ell+1}\\exp\\left(\\frac{2}{\\rho_{L:1}^{2}r^{2}}\\epsilon^{2}\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "565 Putting it all together, we obtain that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Big\\|f_{L:1}-\\tilde{f}_{L:1}\\Big\\|_{\\boldsymbol\\pi}^{2}\\leq4\\displaystyle\\sum_{\\ell=1}^{L}\\tilde{\\rho}_{L:\\ell+1}^{2}R_{\\ell}^{2}\\rho_{\\ell-1:1}^{2}r^{2}\\left(\\epsilon_{K_{\\ell}}^{2}+9\\displaystyle\\sum_{k=1}^{K_{\\ell}}\\frac{\\epsilon_{k}^{2}}{M_{k,\\ell}}\\right)}&{}\\\\ {\\leq\\exp\\left(\\displaystyle\\frac{2}{\\rho_{L:1}^{2}r^{2}}\\epsilon^{2}\\right)\\rho_{L:1}^{2}r^{2}\\displaystyle\\sum_{\\ell=1}^{L}4\\frac{R_{\\ell}^{2}}{\\rho_{\\ell}^{2}}\\left(\\epsilon_{K_{\\ell}}^{2}+9\\displaystyle\\sum_{k=1}^{K_{\\ell}}\\frac{\\epsilon_{k}^{2}}{M_{k,\\ell}}\\right)}&{}\\\\ {\\leq2\\exp\\left(\\displaystyle\\frac{2}{\\rho_{L:1}^{2}r^{2}}\\epsilon^{2}\\right)\\epsilon^{2}}&{}\\\\ {=2\\epsilon^{2}+O(\\epsilon^{4}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "566  Now since $\\begin{array}{r}{\\log\\mathcal{N}_{2}(\\mathbb{S}^{d_{\\ell}-1},\\epsilon)=d_{\\ell}\\log\\left(\\frac{1}{\\epsilon}+1\\right)}\\end{array}$ and ", "page_idx": 17}, {"type": "equation", "text": "$$\nM_{k,\\ell}\\le36\\rho_{L:1}^{2}r^{2}s_{0}\\left(\\sum_{\\ell^{\\prime}=1}^{L}\\frac{R_{\\ell^{\\prime}}}{\\rho_{\\ell^{\\prime}}}\\sqrt{d_{\\ell^{\\prime}}+d_{\\ell^{\\prime}-1}}\\right)\\frac{R_{\\ell}}{\\rho_{\\ell}\\sqrt{d_{\\ell}+d_{\\ell-1}}}\\frac{2^{-k}}{\\sqrt{k}}\\frac{1}{\\epsilon^{2}}+1,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "567 we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{F},\\sqrt{2}\\exp\\left(\\frac{\\varepsilon^{2}}{\\rho_{L+1}^{2}r^{2}}\\right)\\epsilon)\\leq2\\sum_{t=1}^{L}M_{k,\\varepsilon}\\left(\\log\\Lambda_{2}^{\\varepsilon}(\\mathbb{S}^{d_{\\varepsilon-1}},\\epsilon_{k-1})+\\log\\Lambda_{2}^{\\varepsilon}(\\mathbb{S}^{d_{\\varepsilon-1}-1},\\epsilon_{k-1})\\right)}\\\\ &{\\qquad\\qquad\\qquad\\leq2\\sum_{t=1}^{L}\\sum_{k=1}^{K}M_{k,\\varepsilon}\\left(d_{\\varepsilon}+d_{\\varepsilon-1}\\right)\\log(\\frac{1}{\\epsilon_{k-1}}+1)}\\\\ &{\\qquad\\qquad\\leq72\\alpha\\rho_{L+1}^{2}r^{2}\\left(\\sqrt{\\underset{\\varepsilon=1}{\\sum}}\\frac{R_{\\varepsilon}}{\\rho_{\\varepsilon}}\\sqrt{d_{\\varepsilon}\\prime}+d_{\\varepsilon-1}\\right)\\underset{\\varepsilon=1}{\\sum}\\frac{R_{\\varepsilon}}{\\rho_{\\varepsilon}}\\sqrt{d_{\\varepsilon}\\prime}\\sqrt{d_{\\varepsilon}\\prime+d_{\\varepsilon-1}}\\underset{k=1}{\\overset{K}{\\sum}}\\frac{2^{-k}\\log(\\frac{1}{\\varepsilon})}{\\mathrm{Kn}}}\\\\ &{\\qquad\\qquad+2\\sum_{t=1}^{L}\\left(d_{\\varepsilon}+d_{\\varepsilon-1}\\right)\\underset{k=1}{\\overset{K}{\\sum}}\\log(\\frac{1}{\\epsilon_{k-1}}+1)}\\\\ &{\\qquad\\qquad\\leq72\\delta_{\\sigma}^{2}\\rho_{L+1}^{2}r^{2}\\left(\\underset{\\varepsilon=1}{\\overset{L}{\\sum}}\\frac{R_{\\varepsilon}}{\\rho_{\\varepsilon}}\\sqrt{d_{\\varepsilon}\\prime+d_{\\varepsilon-1}}\\right)^{2}\\frac{1}{\\varepsilon^{2}}+o(\\epsilon^{-2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "568  The diameter of $\\mathcal{F}$ is smaller than $\\rho_{L:1}r$ , so for all $\\delta\\geq\\rho_{L:1}r$ $\\log\\mathcal{N}_{2}(\\mathcal{F},\\delta)=0$ . For all $\\delta\\leq\\rho_{L:1}r$   \n569  we choose $\\begin{array}{r}{\\epsilon=\\frac{\\delta}{\\sqrt{2e}}}\\end{array}$ so that $\\begin{array}{r}{\\sqrt{2}\\exp\\left(\\frac{\\epsilon^{2}}{\\rho_{L:1}^{2}r^{2}}\\right)\\epsilon\\leq\\delta}\\end{array}$ and herefore ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\log\\mathcal{N}_{2}(\\mathcal{F},\\delta)\\leq144s_{0}^{2}e\\rho_{L;1}^{2}r^{2}\\left(\\sum_{\\ell^{\\prime}=1}^{L}\\frac{R_{\\ell^{\\prime}}}{\\rho_{\\ell^{\\prime}}}\\sqrt{d_{\\ell^{\\prime}}+d_{\\ell^{\\prime}-1}}\\right)^{2}\\frac{1}{\\delta^{2}}+o(\\delta^{-2}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "570(2) Our goal now is to use chaining $/$ Dudley's theorem to bound the Rademacher complexity   \n571 $R({\\mathcal{F}}(X))$ evaluated on a set $X$ ofsize $N$ (e.g. Lemma 27.4 in [Understanding Machine Learning])   \n572of our set: ", "page_idx": 18}, {"type": "text", "text": "573 Lemma 7. Let $\\begin{array}{r}{c=\\operatorname*{max}_{f\\in\\mathcal{F}}\\frac{1}{\\sqrt{N}}\\left\\|f(X)\\right\\|}\\end{array}$ , then for any integer $M>0$ ", "page_idx": 18}, {"type": "equation", "text": "$$\nR(\\mathcal{F}(X))\\leq c2^{-M}+\\frac{6c}{\\sqrt{N}}\\sum_{k=1}^{M}2^{-k}\\sqrt{\\log\\mathcal{N}(\\mathcal{F},c2^{-k})}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "574  To apply it to our setting, first note that for all $x\\,\\in\\,B(0,r),\\;\\|f_{L:1}(x)\\|\\,\\leq\\,\\rho_{L:1}r$ so that $c=$   \n575 $\\begin{array}{r}{\\operatorname*{max}_{f\\in\\mathcal{F}}\\frac{1}{\\sqrt{N}}\\|f(X)\\|\\leq\\bar{\\rho}_{L:1}r}\\end{array}$ we then have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle R(\\mathcal{F}(X))\\leq c2^{-M}+\\frac{6c}{\\sqrt{N}}\\sum_{k=1}^{M}2^{-k}12s_{0}\\sqrt{e}\\rho_{L;1}r\\sum_{\\ell^{\\prime}=1}^{L}\\frac{R_{\\ell^{\\prime}}}{\\rho_{\\ell^{\\prime}}}\\sqrt{d_{\\ell^{\\prime}}+d_{\\ell^{\\prime}-1}}c^{-1}2^{k}(1+o(1))}}\\\\ {{\\displaystyle=c2^{-M}+\\frac{72}{\\sqrt{N}}M s_{0}\\sqrt{e}\\rho_{L;1}r\\sum_{\\ell^{\\prime}=1}^{L}\\frac{R_{\\ell^{\\prime}}}{\\rho_{\\ell^{\\prime}}}\\sqrt{d_{\\ell^{\\prime}}+d_{\\ell^{\\prime}-1}}(1+o(1)).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "576 Taking $\\begin{array}{r}{M=\\left\\lceil-\\log_{2}\\left(\\frac{72}{\\sqrt{N}}s_{0}\\sqrt{e}\\sum_{\\ell^{\\prime}=1}^{L}\\frac{R_{\\ell^{\\prime}}}{\\rho_{\\ell^{\\prime}}}\\sqrt{d_{\\ell^{\\prime}}+d_{\\ell^{\\prime}-1}}\\right)\\right\\rceil}\\end{array}$ we obtain ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{I}(\\mathcal{F}(X))\\leq\\displaystyle\\frac{72}{\\sqrt{N}}M s_{0}\\sqrt{e}\\rho_{L\\bot1}r\\displaystyle\\sum_{\\ell^{\\prime}=1}^{L}\\frac{R_{\\ell^{\\prime}}}{\\rho_{\\ell^{\\prime}}}\\sqrt{d_{\\ell^{\\prime}}+d_{\\ell^{\\prime}-1}}(1+M(1+o(1)))}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\frac{144}{\\sqrt{N}}M s_{0}\\sqrt{e}\\rho_{L\\bot1}r\\displaystyle\\sum_{\\ell^{\\prime}=1}^{L}\\frac{R_{\\ell^{\\prime}}}{\\rho_{\\ell^{\\prime}}}\\sqrt{d_{\\ell^{\\prime}}+d_{\\ell^{\\prime}-1}}\\left[-\\log_{2}\\left(\\displaystyle\\frac{72}{\\sqrt{N}}s_{0}\\sqrt{e}\\displaystyle\\sum_{\\ell^{\\prime}=1}^{L}\\frac{R_{\\ell^{\\prime}}}{\\rho_{\\ell^{\\prime}}}\\sqrt{d_{\\ell^{\\prime}}+d_{\\ell^{\\prime}-1}}\\right)\\right.}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\left.\\leq C\\rho_{L\\bot1}r\\displaystyle\\sum_{\\ell^{\\prime}=1}^{L}\\frac{R_{\\ell^{\\prime}}}{\\rho_{\\ell^{\\prime}}}\\sqrt{d_{\\ell^{\\prime}}+d_{\\ell^{\\prime}-1}}\\frac{\\log N}{\\sqrt{N}}(1+o(1)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "577 (3) For any $\\rho$ -Lipschitz loss function $\\ell(x,f(x))$ with $|\\ell(x,y)|\\leq c_{0}$ , we know that with probability   \n578 $1-\\delta$ over the sampling of the training set $X$ from the distribution $\\pi$ , we have for all $f\\in\\mathcal F$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{L}_{x\\sim\\pi}\\left[\\ell(x,f(x))\\right]-\\displaystyle\\frac{1}{N}\\sum_{i=1}^{N}\\ell(x_{i},f(x_{i}))\\le2\\mathbb{E}_{X^{\\prime}}\\left[R(\\ell\\circ\\mathcal{F}(X^{\\prime}))\\right]+c_{0}\\sqrt{\\frac{2\\log2/\\delta}{N}}}\\\\ &{\\phantom{\\sum_{i=1}^{N}\\ell(x,f(x))\\right]-c_{0}\\sqrt{\\frac{2\\pi}{\\ell^{\\prime}}}}\\underline{{R_{\\ell^{\\prime}}}}\\sqrt{d_{\\ell^{\\prime}}+d_{\\ell^{\\prime}-1}}\\frac{\\log N}{\\sqrt{N}}(1+o(1))+c_{0}\\sqrt{\\frac{2\\pi}{N}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "579 ", "page_idx": 18}, {"type": "text", "text": "580 C  Composition of Sobolev Balls ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "581 Proposition 8 (Proposition 3 from the main.). Given a distribution $\\pi$ withsupport in $B(0,r)$ ,we   \n582 have that with probability $1-\\delta$ forallfunctions $f\\in\\mathcal{F}=\\{f:\\|f\\|_{W^{\\nu,2}}\\leq R,\\|f\\|_{\\infty}\\leq R\\}$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{L}(f)-\\tilde{\\mathcal{L}}_{N}(f)\\leq2C_{1}R E_{\\nu/d}(N)+c_{0}\\sqrt{\\frac{2\\log^{2}\\!/\\delta}{N}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "58  where $E_{r}(N)=N^{-\\frac{1}{2}}\\;i f\\,r>\\frac{1}{2}$ \uff0c $\\begin{array}{r}{E_{r}(N)=N^{-\\frac{1}{2}}\\log N\\,i f r=\\frac{1}{2}}\\end{array}$ and $\\begin{array}{r}{E_{r}(N)=N^{-r}\\;i f\\,r<\\frac{1}{2}}\\end{array}$ ", "page_idx": 18}, {"type": "text", "text": "584Proof. (1) We know from Theorem 5.2 of [9] that the Sobolev ball $B_{W^{\\nu,2}}(0,R)$ overany $d$   \n585dimensional hypercube $\\Omega$ satisfies ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\log\\mathcal{N}_{2}(B_{W^{\\nu,2}}(0,R),\\pi,\\epsilon)\\leq C_{0}\\left(\\frac{R}{\\epsilon}\\right)^{\\frac{d}{\\nu}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "586  for a constant $c$ and any measure $\\pi$ supported in the hypercube. ", "page_idx": 19}, {"type": "text", "text": "587 (2) By Dudley's theorem we can bound the Rademacher complexity of our function class $B(X)$   \n588evaluated on any training set $X$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{{\\cal R}(B(X))\\le{\\cal R}2^{-M}+\\displaystyle\\frac{6R}{\\sqrt{N}}\\sum_{k=1}^{M}2^{-k}\\sqrt{C_{0}\\left(\\displaystyle\\frac{R}{R2^{-k}}\\right)^{\\frac{d}{\\nu}}}}}\\\\ {{\\displaystyle={\\cal R}2^{-M}+\\displaystyle\\frac{6R}{\\sqrt{N}}\\sqrt{C_{0}}\\sum_{k=1}^{M}2^{k(\\frac{d}{2\\nu}-1)}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "589If $2\\nu=d$ , we take $M={\\textstyle\\frac{1}{2}}\\log N$ and obtain the bound ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{R}{\\sqrt{N}}+\\frac{6R}{\\sqrt{N}}\\sqrt{C_{0}}\\frac{1}{2}\\log N\\leq C_{1}R\\frac{\\log N}{\\sqrt{N}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "590If $2\\nu>d$ ,we take $M=\\infty$ and obtain the bound ", "page_idx": 19}, {"type": "equation", "text": "$$\n{\\frac{6R}{\\sqrt{N}}}\\sqrt{C_{0}}\\left({\\frac{2^{\\frac{d}{2\\nu}-1}}{1-2^{\\frac{d}{2\\nu}-1}}}\\right)\\leq C_{1}R{\\frac{1}{\\sqrt{N}}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "591If $2\\nu<d$ ,we take $\\begin{array}{r}{M=\\frac{\\nu}{d}\\log N}\\end{array}$ and obtain the bound ", "page_idx": 19}, {"type": "equation", "text": "$$\nR2^{-M}+\\frac{6R}{\\sqrt{N}}\\sqrt{C_{0}}2^{\\frac{d}{2\\nu}-1}\\left(\\frac{2^{M(\\frac{d}{2\\nu}-1)}-1}{2^{\\frac{d}{2\\nu}-1}-1}\\right)\\le C_{1}R N^{-\\frac{\\nu}{d}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "592  Putting it all together, we obtain that $R(B(X))\\leq C_{1}E_{\\nu/d}(N)$ ", "page_idx": 19}, {"type": "text", "text": "593 (3) For any $\\rho$ -Lipschitz loss function $\\ell(x,f(x))$ with $|\\ell(x,y)|\\leq c_{0}$ , we know that with probability   \n594 $1-\\delta$ over the sampling of the training set $X$ from the distribution $\\pi$ , we have for all $f\\in\\mathcal F$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{E}_{\\boldsymbol{x}\\sim\\pi}\\left[\\ell(\\boldsymbol{x},f(\\boldsymbol{x}))\\right]-\\frac{1}{N}\\sum_{i=1}^{N}\\ell(\\boldsymbol{x}_{i},f(\\boldsymbol{x}_{i}))\\le2\\mathbb{E}_{X^{\\prime}}\\left[R(\\ell\\circ\\mathcal{F}(X^{\\prime}))\\right]+c_{0}\\sqrt{\\frac{2\\log^{2}\\!\\beta}{N}}}}\\\\ &{}&{\\le2C_{1}E_{\\nu/d}(N)+c_{0}\\sqrt{\\frac{2\\log^{2}\\!/\\delta}{N}}.\\quad\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "595 ", "page_idx": 19}, {"type": "text", "text": "596  Proposition 9. Let $\\mathcal{F}_{1},\\ldots,\\mathcal{F}_{L}$ be set of functions mapping through the sets $\\Omega_{0},\\ldots,\\Omega_{L}$ then if all   \n597 functions in $\\mathcal{F}_{\\ell}$ are $\\rho_{\\ell}$ -Lipschitz, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\log\\mathcal{N}_{2}(\\mathcal{F}_{L}\\circ\\dots\\circ\\mathcal{F}_{1},\\sum_{\\ell=1}^{L}\\rho_{L:\\ell+1}\\epsilon_{\\ell})\\leq\\sum_{\\ell=1}^{L}\\log\\mathcal{N}_{2}(\\mathcal{F}_{\\ell},\\epsilon_{\\ell}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "598 Proof. For any distribution $\\pi_{0}$ on $\\Omega$ there is a $\\epsilon_{1}$ -covering $\\tilde{\\mathcal{F}}_{1}$ of ${\\mathcal{F}}_{1}$ with $\\left|\\tilde{\\mathcal{F}}_{1}\\right|\\leq\\mathcal{N}_{2}(\\mathcal{F}_{1},\\epsilon_{1})$ then   \n599  for any $\\tilde{f}_{1}\\in\\tilde{\\mathcal{F}}_{1}$ we choose a $\\epsilon_{2}$ -covering ${\\tilde{\\mathcal{F}}}_{2}$ W.r.t. the measure $\\pi_{1}$ which is the measure of $f_{1}(x)$ if   \n600 $x\\sim\\pi_{0}$ of ${\\mathcal{F}}_{2}$ with $\\left|\\tilde{\\mathcal{F}}_{2}\\right|\\le\\sqrt{\\L_{2}(\\mathcal{F}_{2},\\epsilon)}$ , and so on until we obtain coverings for all $\\ell$ . Then the set   \n601 $\\tilde{\\mathcal{F}}=\\left\\{\\tilde{f}_{L}\\circ\\cdot\\cdot\\cdot\\circ\\tilde{f}_{1}:\\tilde{f}_{1}\\in\\tilde{\\mathcal{F}}_{1},...,\\tilde{f}_{L}\\in\\tilde{\\mathcal{F}}_{L}\\right\\}$ is a $\\sum_{\\ell=1}^{L}\\rho_{L:\\ell+1}\\epsilon_{\\ell}$ -covering of $\\mathcal{F}=\\mathcal{F}_{L}\\circ\\cdot\\cdot\\circ\\mathcal{F}_{1}$   \n602   indeed for any $f=f_{L;1}$ we choose $\\tilde{f}_{1}\\in\\tilde{\\mathcal{F}}_{1},\\ldots,\\tilde{f}_{L}\\in\\tilde{\\mathcal{F}}_{L}$ that cover $f_{1},\\ldots,f_{L}$ , then $\\tilde{f}_{L:1}$ covers   \n603 $f_{L;1}$ ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\left\\|f_{L:1}-\\widetilde{f}_{L:1}\\right\\|_{\\boldsymbol\\pi}\\leq\\displaystyle\\sum_{\\ell=1}^{L}\\left\\|f_{L:\\ell}\\circ\\widetilde{f}_{\\ell-1:1}-f_{L:\\ell+1}\\circ\\widetilde{f}_{\\ell:1}\\right\\|_{\\boldsymbol\\pi}}\\\\ {\\displaystyle\\leq\\sum_{\\ell=1}^{L}\\left\\|f_{L:\\ell}-f_{L:\\ell+1}\\circ\\widetilde{f}_{\\ell}\\right\\|_{\\boldsymbol\\pi_{\\ell-1}}}\\\\ {\\displaystyle\\leq\\sum_{\\ell=1}^{L}\\rho_{L:\\ell+1}\\epsilon_{\\ell},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "604  and log cardinality of the set $\\tilde{\\mathcal F}$ is bounded $\\begin{array}{r}{\\sum_{\\ell=1}^{L}\\log\\mathcal{N}_{2}(\\mathcal{F}_{\\ell},\\epsilon_{\\ell})}\\end{array}$ ", "page_idx": 20}, {"type": "text", "text": "605 Theorem10.\uff0e Let $\\begin{array}{r l r l r l r}{\\mathcal{F}}&{{}}&{}&{{}=}&{{}}&{\\mathcal{F}_{L}}&{}\\end{array}$ ${\\mathcal{F}}_{1}$ where $\\mathcal{F}_{\\ell}$ $=$ $\\{f_{\\ell}:\\mathbb{R}^{d_{\\ell-1}}\\rightarrow\\mathbb{R}^{d_{\\ell}}$ $\\|f_{\\ell}\\|_{W^{\\nu_{\\ell},2}}\\leq R_{\\ell},\\|f_{\\ell}\\|_{\\infty}\\leq b_{\\ell},L i p(f_{\\ell})\\leq\\rho_{\\ell}\\}$ $r^{*}\\;\\;\\;=\\;\\;\\operatorname*{min}_{\\ell}\\,r_{\\ell}$ $\\begin{array}{r}{r_{\\ell}=\\frac{\\nu_{\\ell}}{d_{\\ell-1}}}\\end{array}$ $1-\\delta$ $f\\in\\mathcal F$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{L}(f)-\\tilde{\\mathcal{L}}_{N}(f)\\leq\\rho C_{0}\\left(\\sum_{\\ell=1}^{L}\\left(C_{\\ell}\\rho_{L:\\ell+1}R_{\\ell}\\right)^{\\frac{1}{r^{*}+1}}\\right)^{r^{*}+1}E_{r^{*}}(N)+c_{0}\\sqrt{\\frac{2\\log^{2}\\!/\\delta}{N}},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "608  where $C_{\\ell}$ depends only on $d_{\\ell-1},d_{\\ell},\\nu_{\\ell},b_{\\ell-1}$ ", "page_idx": 20}, {"type": "text", "text": "609 Proof. (1) We know from Theorem 5.2 of [9] that the Sobolev ball $B_{W^{\\nu_{\\ell}},2}(0,R_{\\ell})$ over any $d_{\\ell}$   \n610 dimensional hypercube $\\Omega$ satisfies ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\log\\mathcal{N}_{2}(B_{W^{\\nu,2}}(0,R_{\\ell}),\\pi_{\\ell-1},\\epsilon_{\\ell})\\leq\\left(C_{\\ell}\\frac{R_{\\ell}}{\\epsilon_{\\ell}}\\right)^{\\frac{1}{r_{\\ell}}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "611 for a constant $C_{\\ell}$ that depends on the size of hypercube and the dimension $d_{\\ell}$ and the regularity $\\nu_{\\ell}$   \n612  and any measure $\\pi_{\\ell-1}$ supported in the hypercube. ", "page_idx": 20}, {"type": "text", "text": "613 Thus Proposition 9 tells us that the composition of the Sobolev balls satisfies ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\log\\mathcal{N}_{2}(\\mathcal{F}_{L}\\circ\\dots\\circ\\mathcal{F}_{1},\\sum_{\\ell=1}^{L}\\rho_{L:\\ell+1}\\epsilon_{\\ell})\\leq\\sum_{\\ell=1}^{L}\\left(C_{\\ell}\\frac{R_{\\ell}}{\\epsilon_{\\ell}}\\right)^{\\frac{1}{r_{\\ell}}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "614 Given $r^{*}\\ \\ =\\ \\operatorname*{min}_{\\ell}r_{\\ell}$ , we can bound it by $\\begin{array}{r}{\\sum_{\\ell=1}^{L}\\left(C_{\\ell}\\frac{R_{\\ell}}{\\epsilon_{\\ell}}\\right)^{\\frac{1}{r^{*}}}}\\end{array}$ and by then choosing $\\epsilon_{\\ell}=$   \n615 $\\begin{array}{r l}&{\\frac{\\rho_{L:\\ell+1}^{-1}\\left(\\rho_{L:\\ell+1}C_{\\ell}R_{\\ell}\\right)^{\\frac{1}{r^{*}+1}}}{\\sum_{\\ell}\\left(\\rho_{L:\\ell+1}C_{\\ell}R_{\\ell}\\right)^{\\frac{1}{r^{*}+1}}}\\epsilon}\\end{array}$ , we obtain that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\log\\mathcal{N}_{2}(\\mathcal{F}_{L}\\circ\\dots\\circ\\mathcal{F}_{1},\\epsilon)\\leq\\left(\\sum_{\\ell=1}^{L}\\left(\\rho_{L:\\ell+1}C_{\\ell}R_{\\ell}\\right)^{\\frac{1}{r^{*}+1}}\\right)^{r^{*}+1}\\epsilon^{-\\frac{1}{r^{*}}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "616  (2,3) It the follows by a similar argument as in points $(2,3)$ of the proof of Proposition 8 that there is   \n617 a constant $C_{0}$ such that with probability $1-\\delta$ for all $f\\in\\mathcal F$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{L}(f)-\\tilde{\\mathcal{L}}_{N}(f)\\leq C_{0}\\left(\\sum_{\\ell=1}^{L}\\left(\\rho_{L:\\ell+1}C_{\\ell}R_{\\ell}\\right)^{\\frac{1}{r^{*}+1}}\\right)^{r^{*}+1}E_{r^{*}}(N)+c_{0}\\sqrt{\\frac{2\\log^{2}\\!/\\delta}{N}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "618 ", "page_idx": 20}, {"type": "text", "text": "619 D  Generalization at the Regularized Global Minimum ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "620 In this section, we first give the proof of Theorem 5 and then present detailed proofs of lemmas used   \n621 in the proof. The lemmas are largely inspired by [5] and may be of independent interest.   \n623 Theorem 11 (Theorem 5 in the main). Given a true function $f_{L^{*};1}^{*}=f_{L^{*}}^{*}\\circ\\cdot\\cdot\\circ f_{1}^{*}$ going through the   \n624  dimensions $d_{0}^{*},\\ldots,d_{L^{*}}^{*}$ , along with a continuous input distribution $\\pi_{0}$ supported in $B(0,b_{0})$ such   \n625 that the distributions $\\pi_{\\ell}$ of $f_{\\ell}^{*}(x)$ (for $x\\sim\\pi_{0}.$ )are continuous too and supported inside $B(0,b_{\\ell})\\subset$   \n62RdFuhasu that thereardiffentabiltieve adadi Re schthatf(Bbe)   \n627 $R_{\\ell}$ , and pe such that $L i p(f_{\\ell}^{*})\\le\\rho_{\\ell}$ .For a infinite width AccNet with $L\\geq L^{*}$ and constant width   \n628 $d\\geq d_{1}^{*},\\ldots,d_{L^{*}-1}^{*}$ we have fortheratos $\\begin{array}{r}{\\tilde{r}_{\\ell}=\\frac{\\nu_{\\ell}}{d_{\\ell}^{*}+3}}\\end{array}$ ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "629 ", "page_idx": 21}, {"type": "text", "text": "630 ", "page_idx": 21}, {"type": "text", "text": "631 ", "page_idx": 21}, {"type": "text", "text": "632 ", "page_idx": 21}, {"type": "text", "text": "633 Proof. If $f^{*}\\,=\\,f_{L^{*}}^{*}\\,\\circ\\cdots\\circ f_{1}^{*}$ with $L^{*}\\leq L$ , intermediate dimensions $d_{0}^{*},\\ldots,d_{L^{*}}^{*}$ , along with a   \n634  continuous input distribution $\\pi_{0}$ supported in $B(0,b_{0})$ , such that the distributions $\\pi_{\\ell}$ of $f_{\\ell}^{*}(x)$ (for   \n635 $x\\sim\\pi_{0}$ ) are continuous too and supported inside $B(0,b_{\\ell})\\,\\subset\\,\\mathbb{R}^{d_{\\ell}^{*}}$ . Further assume that there are   \n636 differentiabilities $\\nu_{\\ell}^{*}$ and radi $R_{\\ell}$ such that $\\|f_{\\ell}^{*}\\|_{W^{\\nu_{\\ell}^{*},2}(B(0,b_{\\ell}))}\\leq R_{\\ell}$ ", "page_idx": 21}, {"type": "text", "text": "637  We first focus on the $L=L^{*}$ case and then extend to the $L>L^{*}$ case. ", "page_idx": 21}, {"type": "text", "text": "638Each $f_{\\ell}^{*}$ can be approximated by another function $\\tilde{f_{\\ell}}$ with bounded $F_{1}$ -norm and Lipschitz constant.   \n639  Actually if $2\\nu_{\\ell}^{*}\\geq d_{\\ell-1}^{*}+3$ one can choose $\\tilde{f}_{\\ell}=f_{\\ell}^{*}$ since $\\|f_{\\ell}^{*}\\|_{F_{1}}\\le C_{\\ell}R_{\\ell}$ by Lemma 14, and by   \n640 assumption $L i p(\\tilde{f}_{\\ell})\\le\\rho_{\\ell}$ .If $2\\nu_{\\ell}^{*}<d_{\\ell-1}^{*}+3$ , then by Lemma 13 we know that there is a $\\tilde{f_{\\ell}}$ with   \n641 l\u2264CeRe and $L i p(\\tilde{f}_{\\ell})\\leq C_{\\ell}L i p(f_{\\ell}^{*})\\leq C_{\\ell}\\rho_{\\ell}$ and error ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left\\|f_{\\ell}^{*}-\\tilde{f}_{\\ell}\\right\\|_{L_{2}(\\pi_{\\ell-1})}\\le c_{\\ell}\\left\\|f^{*}-\\tilde{f}_{\\ell}\\right\\|_{L_{2}(B(0,b_{\\ell}))}\\le c_{\\ell}\\epsilon_{\\ell}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "642   Therefore the composition $\\tilde{f}_{L:1}$ satisfies ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\|f_{L:1}^{*}-\\hat{f}_{L:1}\\right\\|_{L_{2}\\left(\\pi_{\\ell-1}\\right)}\\le\\displaystyle\\sum_{\\ell=1}^{L}\\left\\|\\tilde{f}_{L:\\ell+1}\\circ f_{\\ell:1}^{*}-\\tilde{f}_{L:\\ell}\\circ f_{\\ell-1:1}^{*}\\right\\|_{L_{2}\\left(\\pi\\right)}}&{}\\\\ {\\le\\displaystyle\\sum_{\\ell=1}^{L}L i p(\\tilde{f}_{L:\\ell+1})c_{\\ell}\\epsilon_{\\ell}}&{}\\\\ {\\le\\displaystyle\\sum_{\\ell=1}^{L}\\rho_{L:\\ell+1}C_{L:\\ell+1}c_{\\ell}\\epsilon_{\\ell}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "643 For any $L\\geq L^{*}$ , dimensions $d_{\\ell}\\geq d_{\\ell}^{\\ast}$ and widths $w_{\\ell}\\geq N$ , we can build an AccNet that fits eactly   \n644 $\\tilde{f}_{L:1}$ , by simply adding zero weights along the additional dimensions and widths, and by adding   \n645 identity layers if $L>L^{*}$ , since it is possible to represent the identity on $\\mathbb{R}^{d}$ with a shallow network   \n646  with $2d$ neurons and $F_{1}$ -norm $2d$ (by having two neurons $e_{i}\\sigma(e_{i}^{T}\\cdot)$ and $-e_{i}\\sigma\\big(-e_{i}^{T}\\cdot\\big)$ for each basis   \n647 $e_{i})$ . Since the cost in parameter norm of representing the identity scales with the dimension, it is   \n648 best to ad those identity layers at the minimal dimension $\\operatorname*{min}\\{d_{0}^{*},\\ldots,d_{L^{*}}^{*}\\}$ We therefore end up   \n649  with a AccNet with $L-L^{*}$ identity layers (with $F_{1}$ norm $2\\operatorname*{min}\\{d_{0}^{*},\\ldots,d_{L^{*}}^{*}\\})$ and $L^{*}$ layers that   \n650 approximate each of the $f_{\\ell}^{*}$ with a bounded $F_{1}$ -norm function $\\tilde{f_{\\ell}}$   \n651 Since $f_{L;1}^{*}$ has zero population loss, the population loss of the AccNet $\\tilde{f}_{L:1}$ is bounded by   \n652 $\\begin{array}{r}{\\rho\\sum_{\\ell=1}^{L}\\rho_{L:\\ell+1}C_{L:\\ell+1}c_{\\ell}\\epsilon_{\\ell}}\\end{array}$ . By McDiarmid's inequality, we know that with probability $1-\\delta$ over the   \n65amifran etransy $\\begin{array}{r}{\\rho\\sum_{\\ell=1}^{L}\\rho_{L:\\ell+1}C_{L:\\ell+1}c_{\\ell}\\epsilon_{\\ell}+B\\sqrt{\\frac{2\\log2/\\delta}{N}}}\\end{array}$   \n654  (1) The global minimizer $\\hat{f}_{L:1}=\\hat{f}_{L}\\circ\\cdot\\cdot\\circ\\hat{f}_{1}$ of the regularized loss (with the first regularization ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "655  term) is therefore bounded by ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{\\ell=1}^{L}\\rho_{L;\\ell+1}C_{L;\\ell+1}c_{\\ell}\\epsilon_{\\ell}+B\\sqrt{\\frac{2\\log2/\\delta}{N}}}\\\\ &{+\\lambda\\sqrt{2d}\\left[\\displaystyle\\prod_{\\ell=1}^{L^{*}}C_{\\ell}\\rho_{\\ell}\\displaystyle\\sum_{\\ell=1}^{L^{*}}\\frac{1}{C_{\\ell}\\rho_{\\ell}}\\left\\{C_{\\ell}R_{\\ell}\\epsilon_{\\ell}^{-\\frac{1}{2\\gamma_{\\ell}}+1}}&{2\\nu_{\\ell}^{*}\\geq d_{\\ell-1}^{*}+3+2(L-L^{*})\\operatorname*{min}\\{d_{0}^{*},\\ldots,d_{L^{*}}^{*}\\}\\right\\}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "656  Taking $\\epsilon_{\\ell}=E_{\\tilde{r}_{m i n}}(N)$ and $\\lambda=N^{-\\frac{1}{2}}\\log N$ , this is upper bounded by ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\rho\\sum_{\\ell=1}^{L}\\rho_{L;\\ell+1}C_{L;\\ell+1}c_{\\ell}+C\\sqrt{2d}r\\prod_{\\ell=1}^{L^{*}}C_{\\ell}\\rho_{\\ell}\\sum_{\\ell=1}^{L^{*}}\\frac{R_{\\ell}}{\\rho_{\\ell}}+2(L-L^{*})\\operatorname*{min}\\{d_{0}^{*},\\ldots,d_{L^{*}}^{*}\\}\\Bigg]E_{\\tilde{r}_{m i n}}(N)+B\\sqrt{\\sum_{\\ell=1}^{L^{*}}C_{\\ell}\\rho_{\\ell}\\sum_{\\ell=1}^{L^{*}}\\frac{R_{\\ell}}{\\rho_{\\ell}}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "657 which implies that at the globla minimizer of the regularized loss, the (unregularized) train loss is of   \n658order $E_{\\tilde{r}_{m i n}}(N)$ and the complexity measure $R(\\hat{f_{1}},\\bar{\\mathbf{\\Omega}}...\\,,\\hat{f_{L}})$ is of order $\\begin{array}{r}{{\\frac{1}{N}}E_{\\tilde{r}_{m i n}}(N)}\\end{array}$ which implies   \n659 that the test error will be of order ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Sigma(f)\\leq\\left[2\\rho\\displaystyle\\sum_{\\ell=1}^{L}\\rho_{L:\\ell+1}C_{L:\\ell+1}c_{\\ell}+2C\\sqrt{2d}r\\displaystyle\\prod_{\\ell=1}^{L^{*}}C_{\\ell}\\rho_{\\ell}\\sum_{\\ell=1}^{L^{*}}\\frac{R_{\\ell}}{\\rho_{\\ell}}+2(L-L^{*})\\operatorname*{min}\\{d_{0}^{*},\\ldots,d_{L^{*}}^{*}\\}\\right]E_{\\tilde{r}_{m}}}\\\\ &{\\quad\\quad+\\left(2B+c_{0}\\right)\\sqrt{\\frac{2\\log2/\\delta}{N}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "660 (2) Let us now consider adding the closer to traditional $L_{2}$ -regularization $\\mathcal{L}_{\\lambda}(f_{L:1})\\lambda=\\mathcal{L}(f_{L:1})+$   \n661 $\\lambda\\prod_{\\ell=1}^{L}\\left\\|f_{\\ell}\\right\\|_{F_{1}}$ . ,We see that the global minimizer $\\hat{f}_{L:1}$ of the $L_{2}$ -regularized loss is upper bounded   \n662by ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sum_{\\ell=1}^{L}\\rho_{L:\\ell+1}C_{L:\\ell+1}c_{\\ell}\\epsilon_{\\ell}+B\\sqrt{\\frac{2\\log2/\\delta}{N}}+\\lambda\\left[\\prod_{\\ell=1}^{L^{*}}\\left\\{C_{\\ell}R_{\\ell}\\epsilon_{\\ell}^{-\\frac{1}{2r_{\\ell}}+1}\\right.\\right.\\left.\\left.2\\nu_{\\ell}^{*}\\geq d_{\\ell-1}^{*}+3\\right\\}\\left(2\\operatorname*{min}\\{d_{0}^{*},\\ldots,d_{\\ell}\\}\\right)\\right]\\leq\\frac{2\\ln2}{N},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "663  Which for $\\epsilon_{\\ell}={\\cal E}_{\\tilde{r}_{m i n}}(N)$ and $\\lambda=N^{-\\frac{1}{N}}$ is upper bounded by ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sum_{\\ell=1}^{L}\\rho_{L;\\ell+1}C_{L;\\ell+1}c_{\\ell}E_{\\tilde{r}_{m i n}}(N)+B\\sqrt{\\frac{2\\log^{2}\\!/\\delta}{N}}+N^{-\\frac{1}{2}}\\left[\\prod_{\\ell=1}^{L^{\\star}}C_{\\ell}R_{\\ell}\\sqrt{N}E_{\\tilde{r}_{m i n}}(N)\\right](2\\operatorname*{min}\\{d_{0}^{\\ast},\\dots,d_{L}^{\\ast}\\})+N^{\\frac{1}{2}}\\sum_{\\ell=1}^{N}\\rho_{\\ell}^{\\star},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "664 Which implies that both the train error is of order $\\begin{array}{r}{N^{-\\frac12}\\prod_{\\ell=1}^{L^{*}}\\sqrt{N}E_{\\tilde{r}_{m i n}}(N)}\\end{array}$ and the product of the   \n65 $F_{1}$ -norms is of order $\\begin{array}{r}{\\prod_{\\ell=1}^{L^{*}}\\sqrt{N}E_{\\tilde{r}_{m i n}}(N)}\\end{array}$   \n66  Now note that the product of the $F_{1}$ -norms bounds the complexity measure up to a constant since   \n667  Lip(f)\u2264IIf F ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "equation", "text": "$$\nR(f_{1},\\dots,f_{L})=r\\prod_{\\ell=1}^{L}L i p(f_{\\ell})\\sum_{\\ell=1}^{L}\\frac{||f_{\\ell}||_{F_{1}}}{L i p(f_{\\ell})}\\sqrt{d_{\\ell-1}+d_{\\ell}}\\le L\\sqrt{2d}\\prod_{\\ell=1}^{L}\\|f\\|_{F_{1}}\\,.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "668 And since at the global minimum the product of the $F_{1}$ -norms isof order $\\begin{array}{r}{\\prod_{\\ell=1}^{L^{*}}\\sqrt{N}E_{\\tilde{r}_{m i n}}(N)}\\end{array}$ the   \n669 test error will of order $\\begin{array}{r}{\\left(\\prod_{\\ell=1}^{L^{*}}\\sqrt{N}E_{\\tilde{r}_{\\ell}}(N)\\right)\\frac{\\log N}{\\sqrt{N}}}\\end{array}$   \n670 Note that if there is at a most one $\\ell$ where $\\tilde{r}_{\\ell}\\,>\\,{\\textstyle\\frac{1}{2}}$ then the rate is up to log term the same as   \n671 $E_{\\tilde{r}_{m i n}}(N)$ \u53e3 ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "672 D.2 Lemmas on approximating Sobolev functions ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "673  Now we present the lemmas used in this proof above that concern the approximation errors and   \n674 Lipschitz constants of Sobolev functions and compositions of them. We will bound the $F_{2}$ -normand   \n675note that the $F_{2}$ -norm is larger than the $F_{1}$ -norm, cf. [5, Section 3.1].   \n676 Lemma 12 (Approximation for Sobolev function with bounded error and Lipschitz constant).   \n677 Suppose $g:\\mathbb{S}_{d}\\rightarrow\\mathbb{R}$ is anevenfunctionwithboundedSobolevnorm $\\|g\\|_{W^{\\nu,2}}^{2}\\le\\bar{R_{\\!\\scriptscriptstyle{W}}}$ with $2\\nu\\leq d+2$   \n678 withinputsontheunit -dimensional sphere.Thenforevery $\\epsilon\\,>\\,0$ there is $\\hat{g}\\in\\mathcal{G}_{2}$ with small   \n679 approximation error $\\|g-\\hat{g}\\|_{L_{2}(\\mathbb{S}_{d})}=C(d,\\nu,R)\\epsilon$ boundedLipschitzness $\\mathrm{Lip}({\\hat{g}})\\leq C^{\\prime}(d)\\mathrm{Lip}(g)$   \n680 and bounded norm ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\hat{g}\\|_{F_{2}}\\le C^{\\prime\\prime}(d,\\nu,R)\\epsilon^{-\\frac{d+3-2\\nu}{2\\nu}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "681 Prof. Given our assumptions on the target function $g$ we may decompose $\\begin{array}{r}{g(x)=\\sum_{k=0}^{\\infty}g_{k}(x)}\\end{array}$   \n682  along the basis of spherical harmonics with $\\begin{array}{r}{g_{0}(x)=\\int_{\\mathbb{S}_{d}}g(y)\\mathrm{d}\\tau_{d}(y)}\\end{array}$ being the mean of $g(x)$ over the   \n683  uniform distribution $\\tau_{d}$ over $\\mathbb{S}_{d}$ . The $k$ -th component can be written as ", "page_idx": 23}, {"type": "equation", "text": "$$\ng_{k}(x)=N(d,k)\\int_{\\mathbb{S}_{d}}g(y)P_{k}(x^{T}y)\\mathrm{d}\\tau_{d}(y)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "684  with N(d, k) = 2k+d-1 (k+d and a Gegenbauer polynomial of degree $k$ and dimension $d+1$ ", "page_idx": 23}, {"type": "equation", "text": "$$\nP_{k}(t)=(-1/2)^{k}\\frac{\\Gamma(d/2)}{\\Gamma(k+d/2)}(1-t^{2})^{(2-d)/2}\\frac{d^{k}}{d t^{k}}(1-t^{2})^{k+(d-2)/2},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "685  known as Rodrigues\u2019 formula. Given the assumption that the Sobolev norm $\\|g\\|_{W^{\\nu,2}}^{2}$ is upper   \n686 bounded, we have $\\|f\\|_{L_{2}(\\mathbb{S}_{d})}^{2}\\le C_{0}(d,\\nu)R$ for $f=\\Delta^{\\nu/2}g$ where $\\Delta$ is the Laplacian on $\\mathbb{S}_{d}$ [18, 5].   \n687 Note that $g_{k}$ are eigenfunctions of the Laplacian with eigenvalues $k(k+d-1)$ [4], thus ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|g_{k}\\|_{L_{2}(\\mathbb{S}_{d})}^{2}=\\|f_{k}\\|_{L_{2}(\\mathbb{S}_{d})}^{2}(k(k+d-1))^{-\\nu}\\leq\\|f_{k}\\|_{L_{2}(\\mathbb{S}_{d})}^{2}k^{-2\\nu}\\leq C_{1}(d,\\nu,R)k^{-2\\nu-1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "688  where the last inequality holds because $\\begin{array}{r}{\\|f\\|_{L_{2}(\\mathbb{S}_{d})}^{2}=\\sum_{k\\geq0}\\|f_{k}\\|_{L_{2}(\\mathbb{S}_{d})}^{2}}\\end{array}$ converges. Note using the   \n689 Hecke-Funk formula, we can also write $g_{k}$ as scaled $p_{k}$ for the underlying density $p$ of the $F_{1}$ and   \n690 $F_{2}$ -norms: ", "page_idx": 23}, {"type": "equation", "text": "$$\ng_{k}(x)=\\lambda_{k}p_{k}(x)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "691  where $\\begin{array}{r}{\\lambda_{k}\\,=\\,\\frac{\\omega_{d-1}}{\\omega_{d}}\\int_{-1}^{1}\\sigma(t)P_{k}(t)(1-t^{2})^{(d-2)/2}\\mathrm{d}t\\,=\\,\\Omega(k^{-(d+3)/2})}\\end{array}$ [5, Appendix D.2] and $\\omega_{d}$   \n692  denotes the surface area of $\\mathbb{S}_{d}$ . Then by definition of $\\Vert\\cdot\\Vert_{F_{2}}$ , for some probability density $p$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\|g\\|_{F_{2}}^{2}=\\int_{\\mathbb{S}_{d}}|p|^{2}\\mathrm{d}\\tau(v)=\\|p\\|_{L_{2}(\\mathbb{S}_{d})}^{2}=\\sum_{0\\leq k}\\|p_{k}\\|_{L_{2}(\\mathbb{S}_{d})}^{2}=\\sum_{0\\leq k}\\lambda_{k}^{-2}\\|g_{k}\\|_{L_{2}(\\mathbb{S}_{d})}^{2}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "693 Now to approximate $g$ , consider function $\\hat{g}$ defined by truncating the \u201chigh frequencies\u201d of $g$ ,i.e.   \n694  setting $\\hat{g}_{k}=\\mathbb{1}[k\\leq m]g_{k}$ for some $m>0$ we specify later. Then we can bound the norm with ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\hat{g}\\|_{F_{2}}^{2}=\\displaystyle\\sum_{0\\leq k:\\lambda_{k}\\neq0}\\lambda_{k}^{-2}\\|\\hat{g}_{k}\\|_{L_{2}(\\mathbb{S}_{d})}^{2}=\\displaystyle\\sum_{0\\leq k\\leq m}\\lambda_{k}^{-2}\\|g_{k}\\|_{L_{2}(\\mathbb{S}_{d})}^{2}}\\\\ &{\\quad\\quad\\overset{\\mathrm{(a)}}{\\leq}C_{2}(d,\\nu,R)\\displaystyle\\sum_{0\\leq k\\leq m}k^{d+2-2\\nu}}\\\\ &{\\quad\\quad\\overset{\\mathrm{(b)}}{\\leq}C_{3}(d,\\nu,R)m^{d+3-2\\nu}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "695  where (a) uses $\\operatorname{Eq}\\,1$ and $\\lambda_{k}=\\Omega(k^{-(d+3)/2})$ ; (b) approximates by integral ", "page_idx": 23}, {"type": "text", "text": "696  To bound the approximation error, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\|g-\\hat{g}\\|_{L_{2}(\\mathbb{S}_{d})}^{2}=\\left\\|\\displaystyle\\sum_{k>m}g_{k}\\right\\|_{L_{2}(\\mathbb{S}_{d})}^{2}\\le\\displaystyle\\sum_{k>m}\\|g_{k}\\|_{L_{2}(\\mathbb{S}_{d})}^{2}}}\\\\ &{\\le C_{4}(d,\\nu,R)\\displaystyle\\sum_{k>m}k^{-2\\nu-1}}\\\\ &{\\le C_{5}(d,\\nu,R)m^{-2\\nu}~~~\\mathrm{by~integral~app}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "697  Finally, choosing $m=\\epsilon^{-\\frac{1}{\\nu}}$ , we obtain $\\|g-\\hat{g}\\|_{L_{2}(\\mathbb{S}_{d})}\\le C(d,\\nu,R)\\epsilon$ and ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\hat{g}\\|_{F_{2}}\\le C^{\\prime}(d,\\nu,R)\\epsilon^{-\\frac{d+3-2\\nu}{2\\nu}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "698 Then it remains to bound $\\operatorname{Lip}({\\hat{g}})$ for our constructed approximation. By construction and by [13,   \n699 Theorem 2.1.3], we have $\\hat{g}=g*h$ withnow ", "page_idx": 24}, {"type": "equation", "text": "$$\nh(t)=\\sum_{k=0}^{m}h_{k}P_{k}(t),\\quad t\\in[-1,1]\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "700 by orthogonality of the Gegenbauer polynomial $P_{k}$ 's and the convolution is defined as ", "page_idx": 24}, {"type": "equation", "text": "$$\n(g*h)(x):=\\frac{1}{\\omega_{d}}\\int_{\\mathbb{S}_{d}}g(y)h(\\langle x,y\\rangle)\\mathrm{d}y.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "701 The coefficients for $0\\leq k\\leq m$ given by [13, Theorem 2.1.3] are ", "page_idx": 24}, {"type": "equation", "text": "$$\nh_{k}\\triangleq{\\frac{\\omega_{d+1}}{\\omega_{d}}}{\\frac{\\Gamma(d-1)}{\\Gamma(d-1+k)}}P_{k}(1){\\frac{k!(k+(d-1)/2)\\Gamma((d-1)/2)^{2}}{\\pi2^{2-d}\\Gamma(d-1+k)}}\\overset{(\\mathrm{b})}{=}O\\left({\\frac{k}{\\Gamma(d-1+k)}}\\right)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "702 where (a) follows from the (inverse of) weighted $L_{2}$ norm of $P_{k}$ ; (b) plugs in the unit constant   \n703 $\\begin{array}{r}{P_{k}(1)\\,=\\,\\frac{\\Gamma(k+d-1)}{\\Gamma(d-1)k!}}\\end{array}$ and suppresses the dependence on d. Note that the constant factor   \n704 comes from the difference in the definitions of the Gegenbauer polynomials here and in [13]. Then   \n705 we can bound ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\nabla\\bar{g}(x)|_{\\mathcal{H}}^{2}=\\int_{\\mathbb{R}}\\|\\nabla\\bar{g}(y)\\|_{\\mathcal{H}}\\|\\bar{D}(x,\\bar{y})\\|d y}\\\\ &{\\qquad\\leq\\operatorname*{lim}_{\\mathcal{H}}\\bigg\\|\\int_{\\mathbb{R}}\\bigg\\|\\bar{g}(x,y)\\bigg\\|_{\\mathcal{H}}^{2}}\\\\ &{\\qquad\\leq\\sqrt{\\|\\bar{g}(y)\\|_{\\mathcal{H}}\\big(\\int_{\\mathbb{R}}\\big(\\mathcal{R},\\bar{y}\\big)\\big)^{\\perp}\\|\\mathcal{H}}^{2}}\\\\ &{\\qquad=\\sqrt{\\|\\bar{g}(y)\\|_{\\mathcal{H}}\\bigg(\\int_{\\mathbb{R}}\\int_{\\mathbb{R}}\\int_{\\mathbb{R}}h_{\\mathcal{H}}\\big(P_{\\||\\mathcal{H}}(x,y)\\big)P_{\\||\\mathcal{H}}(y,\\bar{y})\\bigg)^{\\perp/2}}}\\\\ &{\\qquad=\\sqrt{\\|\\bar{g}(y)\\|_{\\mathcal{H}}\\bigg(\\sum_{\\ell=1}^{\\mathcal{r}}\\int_{\\mathbb{R}}h_{\\ell}P_{\\ell}(y)(P_{\\ell}(1-\\ell)^{\\perp}y,\\bar{y})\\bigg)^{\\perp/2}}}\\\\ &{\\qquad=\\sqrt{\\|\\bar{g}(y)\\|_{\\mathcal{H}}\\bigg(\\sum_{\\ell=1}^{\\mathcal{r}}\\int_{\\mathbb{R}}h_{\\ell}P_{\\ell}(y)(P_{\\ell}(1-\\ell)^{\\perp}y,\\bar{y})\\bigg)^{\\perp/2}}}\\\\ &{\\qquad=\\sqrt{\\|\\bar{g}(y)\\|_{\\mathcal{H}}\\bigg(\\sum_{\\ell=1}^{\\mathcal{r}}\\int_{\\mathbb{R}}\\int_{\\mathbb{R}}\\|\\bar{D}(y)(P_{\\ell}(1-\\ell)^{\\perp}y,\\bar{y})\\|^{2}}}\\\\ &{\\qquad=\\sqrt{\\|\\bar{g}(y)\\|_{\\mathcal{H}}\\bigg(\\sum_{\\ell=1}^{\\mathcal{r}}\\int_{\\mathbb{R}}\\|\\bar{D}(y)(P_{\\ell}(1-\\ell)^{\\perp}y,\\bar{y})\\|^{2}}}\\\\ &{\\qquad=\\sqrt{\\|\\bar{g}(y)\\|_{\\mathcal{H}}\\bigg(\\sum_{\\ell=1}^{\\mathcal{r}}\\int_{\\mathbb{R}}\\|\\bar{D}(y)\\|_{\\mathcal{H}}(y)\\bigg)^{\\perp/2}}}\\\\ &{ \n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "706 for some constant $C(d)$ that only depends on $d$ .Hence $\\mathrm{Lip}({\\hat{g}})=C^{\\prime}(d)\\mathrm{Lip}(g)$ ", "page_idx": 24}, {"type": "text", "text": "707 The next lemma adapts Lemma 12 to inputs on balls instead of spheres following the construction in   \n708  [5, Proposition 5].   \n709Lemma13.Suppose $f:B(0,b)\\rightarrow\\mathbb{R}$ hasboundedSobolevnorm $\\|f\\|_{W^{\\nu,2}}^{2}\\leq R\\,w i t h\\,\\nu\\leq(d\\!+\\!2)/2$   \n710 even, where $B(0,b)=\\{x\\in\\mathbb{R}^{d}:\\|x\\|_{2}\\leq b\\}$ is the radius- $b$ ball.Thenforevery $\\epsilon>0$ there exists   \n711 $f_{\\epsilon}\\in\\mathcal{F}_{2}$ such tha $t\\,\\|f-\\!\\!\\!\\!\\!\\int_{\\epsilon}\\!\\left\\|_{L_{2}(B(0,b))}\\!\\!\\!\\!\\!\\right\\rangle\\stackrel{}{=}C(\\bar{d},\\nu,b,R)\\epsilon,\\mathrm{Lip}(f_{\\epsilon})\\leq C^{\\prime}(b,d)\\mathrm{Lip}(f)$ and ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "equation", "text": "$$\n\\|f_{\\epsilon}\\|_{F_{2}}\\le C^{\\prime\\prime}(d,\\nu,b,R)\\epsilon^{-\\frac{d+3-2\\nu}{2\\nu}}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "712  Proof. Define $\\begin{array}{r}{g(z,a)\\:=\\:f\\left(\\frac{2b z}{a}\\right)a}\\end{array}$ on $(z,a)\\,\\in\\,\\mathbb{S}_{d}$ with $z~\\in~\\mathbb{R}^{d}$ and ${\\scriptstyle{\\frac{1}{\\sqrt{2}}}}\\ \\leq\\ a\\ \\in\\ \\mathbb{R}$ . One may   \n713  verify that unit-norm $(z,a)$ with $\\textstyle a\\,\\geq\\,{\\frac{1}{\\sqrt{2}}}$ is sufficient to cover $B(0,b)$ by setting $\\textstyle x\\,=\\,{\\frac{b z}{a}}$ b and   \n714   solve for $(z,a)$ . Then we have bounded $\\|g\\|_{W^{\\nu,2}}^{2}\\le b^{\\nu}R$ and may apply Lemma 12 to get $\\hat{g}$ with   \n715 $\\|g-\\hat{g}\\|_{L_{2}(\\mathbb{S}_{d})}\\le C(d,\\nu,b,R)\\epsilon$ Letting $\\begin{array}{r}{f_{\\epsilon}(x)=\\hat{g}\\left(\\frac{a x}{b},a\\right)a^{-1}}\\end{array}$ for the corresponding $\\textstyle\\left({\\frac{a x}{b}},a\\right)\\in\\mathbb S_{d}$   \n716  gives the desired upper bounds.   \n717 Lemma 14. Suppose $f:B(0,b)\\rightarrow\\mathbb{R}$ has bounded Sobolev norm $\\|f\\|_{W^{\\nu,2}}^{2}\\leq R$ with $\\nu\\geq(d\\!+\\!3)/2$   \n718 even. Then $f\\in\\mathcal{F}_{2}$ and $\\|f\\|_{F_{2}}\\leq C(d,\\nu)b^{\\nu}R$ ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "719  In particular, $W^{\\nu,2}\\subseteq\\mathcal{F}_{2}$ for $\\nu\\geq(d+3)/2$ even. ", "page_idx": 25}, {"type": "text", "text": "720   Proof. This lemma reproduces [5, Proposition 5] to functions with bounded Sobolev $L_{2}$ norm instead   \n721of $L_{\\infty}$ norm. The proof follows that of Lemma 12 and Lemma 13 and noticing that by Eq 1, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|g\\|_{F_{2}}^{2}=\\displaystyle\\sum_{0\\leq k:\\lambda_{k}\\neq0}\\lambda_{k}^{-2}\\|g_{k}\\|_{L_{2}(\\mathbb{S}_{d})}^{2}}\\\\ &{\\qquad\\leq\\displaystyle\\sum_{0\\leq k}k^{d+3-2\\nu}\\|(\\Delta^{\\nu/2}g)_{k}\\|_{L_{2}(\\mathbb{S}_{d})}^{2}}\\\\ &{\\qquad\\leq\\|\\Delta^{\\nu/2}g\\|_{L_{2}(\\mathbb{S}_{d})}^{2}}\\\\ &{\\qquad\\leq C_{1}(d,\\nu)\\|g\\|_{W^{\\nu,2}}^{2}}\\\\ &{\\qquad\\leq C_{1}(d,\\nu)R.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "722 ", "page_idx": 25}, {"type": "text", "text": "723 Finally, we remark that the above lemmas extend straightforward to functions $f:B(0,b)\\rightarrow\\mathbb{R}^{d^{\\prime}}$   \n724  with multi-dimensional outputs, where the constants then depend on the output dimension $d^{\\prime}$ too. ", "page_idx": 25}, {"type": "text", "text": "725 D.3  Lemma on approximating compositions of Sobolev functions ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "726 With the lemmas given above and the fact that the $F_{2}$ -norm upperbounds the $F_{1}$ -norm, we can find   \n727 infinite-width DNN approximations for compositions of Sobolev functions, which is also pointed out   \n728 in the proof of Theorem 5. ", "page_idx": 25}, {"type": "text", "text": "729 Lemma 15. Assume the target function $f:\\Omega\\to\\mathbb{R}^{d_{o u t}}$ with $\\Omega\\subseteq B(0,b)\\subseteq\\mathbb{R}^{d_{i n}}$ satisfies: ", "page_idx": 25}, {"type": "text", "text": "731 ", "page_idx": 25}, {"type": "text", "text": "$f=g_{k}\\circ\\cdots\\circ g_{1}$ a composition of $k$ Sobolev functions $g_{i}:\\mathbb{R}^{d_{i}}\\rightarrow\\mathbb{R}^{d_{i+1}}$ with bounded   \nnorms $\\|g_{i}\\|_{W^{\\nu_{i},2}}^{2}\\leq R$ for $i=1,\\ldots,k$ with $d_{1}=d_{i n}$ \uff0c   \n$f$ is Lipschitz, i.e. $\\mathrm{Lip}(g_{i})<\\infty$ for $i=1,\\ldots,k$ ", "page_idx": 25}, {"type": "text", "text": "732 ", "page_idx": 25}, {"type": "text", "text": "733f $\\dot{^\\prime}\\nu_{i}\\leq(d_{i}+2)/2$ for any $i$ , i.e. less smooth than needed, for depth $L\\geq k$ and any $\\epsilon>0$ there is an   \n734 infinite-width DNN $\\tilde{f}$ such that ", "page_idx": 25}, {"type": "text", "text": "736 ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bullet\\ \\mathrm{Lip}(\\tilde{f})\\leq C_{1}\\prod_{i=1}^{k}\\mathrm{Lip}(g_{i});}\\\\ &{\\bullet\\ \\|\\tilde{f}-f\\|_{L_{2}}\\leq C_{2}\\epsilon;}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "737the constants $C_{1}$ depends on all of the input dimensions $d_{i}$ (to $g_{i}$ \uff09and $d_{o u t}$ and $C_{2}$ dependson   \n738 $d_{i},d_{o u t},\\nu_{i},b,R,k,$ and $\\mathrm{Lip}(g_{i})$ for all $i$   \n739 If otherwise $\\nu_{i}\\geq(d_{i}+3)/2$ for all $i$ , we can have ${\\tilde{f}}=f$ where each layer has a parameter norm   \n740 bounded by $C_{3}R_{\\mathrm{:}}$ with $C_{3}$ depending on $l_{i},d_{o u t},\\nu_{i}$ ,and $b$ ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "741 Proof. Note that by Lipschitzness, ", "page_idx": 25}, {"type": "equation", "text": "$$\n(g_{i}\\circ\\cdots\\circ g_{1})(\\Omega)\\subseteq B\\left(0,b\\prod_{j=1}^{i}\\mathrm{Lip}(g_{j})\\right),\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "742 i.e. the pre-image of each component lies in a ball. By Lemma 12, for each $g_{i}$ ,if $\\nu_{i}\\leq(d_{i}+2)/2$   \n743 we have an approximation $\\hat{g}_{i}$ on a slightly larger ball $\\begin{array}{r}{b_{i}^{\\prime}=b\\prod_{j=1}^{i-1}C^{\\prime\\prime}(d_{j},d_{j+1})\\mathrm{Lip}(g_{j})}\\end{array}$ such that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bullet\\ \\|g_{i}-\\widehat{g}_{i}\\|_{L_{2}}\\leq C(d_{i},d_{i+1},\\nu_{i},b_{i}^{\\prime},R)\\epsilon;}\\\\ &{\\bullet\\ \\|\\widehat{g}_{i}\\|_{F_{2}}\\leq C^{\\prime}(d_{i},d_{i+1},\\nu_{i},b_{i}^{\\prime},R)\\epsilon^{\\frac{d_{i}+3-2\\nu_{i}}{2\\nu_{i}}};}\\\\ &{\\bullet\\ \\mathrm{Lip}(\\widehat{g}_{i})\\leq C^{\\prime\\prime}(d_{i},d_{i+1})\\mathrm{Lip}(g_{i});}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "747  where $d_{i}$ is the input dimension of $g_{i}$ . Write the constants as $C_{i},C_{i}^{\\prime}$ , and $C_{i}^{\\prime\\prime}$ for notation simplicity.   \n748 Note that the Lipschitzness of the approximations $\\hat{g}_{i}$ 's guarantees that, when they are composed,   \n749 $(\\hat{g}_{i-1}\\circ\\cdot\\cdot\\circ\\hat{g}_{1})(\\Omega)$ lies in a ballo radius $\\begin{array}{r}{b_{i}^{\\prime}=b\\prod_{j=1}^{i-1}C_{j}^{\\prime\\prime}\\mathrm{Lip}(g_{j})}\\end{array}$ hence the approximation error   \n750 remains bounded while propagating. While each $\\hat{g}_{i}$ is a (infinite-width) layer, for the other $L-k$   \n751 layers, we may have identity layers?. ", "page_idx": 26}, {"type": "text", "text": "752 Let $\\tilde{f}$ be the composed DNN of these layers. Then we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathrm{Lip}({\\widetilde{f}})\\leq\\prod_{i=1}^{k}C_{i}^{\\prime\\prime}\\mathrm{Lip}(g_{i})=C^{\\prime\\prime}(d_{1},\\ldots,d_{k},d_{o u t})\\prod_{i=1}^{k}\\mathrm{Lip}(g_{i})\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "753 and approximation error ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\|\\tilde{f}-f\\|_{L_{2}}\\le\\sum_{i=1}^{k}C_{i}\\epsilon\\prod_{j>i}C_{j}^{\\prime\\prime}\\mathrm{Lip}(g_{j})=O(\\epsilon)\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "754  where the last equality suppresses the dependence on $d_{i},d_{o u t},\\nu_{i},b,R,k$ .and $\\operatorname{Lip}(g_{i})$ for $i\\,=$   \n755 $1,\\dots,k$   \n756  In particular, by Lemma 14, if $\\nu_{i}\\,\\geq\\,(d_{i}+3)/2$ for any $i=1,\\dots,k$ , we can take ${\\hat{g}}_{i}=g_{i}$ . If this   \n757   holds for all $i$ , then we can have $\\tilde{f}=f$ while each layer has a $F_{2}$ -norm bounded by $O(R)$ \u518f\u53e3 ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "758 E Technical results ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "759  Here we show a number of technical results regarding the covering number. ", "page_idx": 26}, {"type": "text", "text": "760 First, here is a bound for the covering number of Ellipsoids, which is a simple reformulation of   \n761Theorem 2 of [17]:   \n762  Theorem 16. The $d$ -dimensional ellipsoid $E=\\{x:x^{T}K^{-1}x\\leq1\\}$ with radii $\\sqrt{\\lambda_{i}}$ for $\\lambda_{i}$ the i-th   \n763  eigenvalue of $K$ satisfies $\\log\\mathcal{N}_{2}\\left(E,\\bar{\\epsilon}\\right)=M_{\\epsilon}\\left(1+o(1)\\right)$ for ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "equation", "text": "$$\nM_{\\epsilon}=\\sum_{i:\\sqrt{\\lambda_{i}}\\geq\\epsilon}\\log\\frac{\\sqrt{\\lambda_{i}}}{\\epsilon}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "764  if one has log $\\begin{array}{r}{\\mathrm{~\\boldmath~\\int~}\\frac{\\sqrt{\\lambda_{1}}}{\\epsilon}=o\\left(\\frac{M_{\\epsilon}^{2}}{k_{\\epsilon}\\log d}\\right)}\\end{array}$ for $k_{\\epsilon}=|\\{i:\\sqrt{\\lambda_{i}}\\geq\\epsilon\\}|$ ", "page_idx": 26}, {"type": "text", "text": "765 For our purpose, we will want to cover a unit ball $B=\\{w:\\|w\\|\\leq1\\}$ w.r.t. to a non-isotropic norm   \n766 $\\|w\\|_{K}^{2}=w^{T}K w$ , but this is equivalent to covering $E$ with an isotropic norm:   \n767 Corollary 17. The covering number of the ball $B=\\{w:\\|w\\|\\leq1\\}$ w.rt. the norm $\\|w\\|_{K}^{2}=w^{T}K w$   \n76   satisfies $\\log\\mathcal{N}\\left(B,\\left\\|\\cdot\\right\\|_{K},\\bar{\\epsilon}\\right)=M_{\\epsilon}\\left(\\bar{1}+o(1)\\right)$ for the same $M_{\\epsilon}$ as in Theorem $^ \u1e0a l6 \u1e0c$ and under the same   \n769 condition. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\log\\mathcal{N}\\left(B,\\left\\|\\cdot\\right\\|_{K},\\epsilon\\right)\\leq\\frac{\\mathrm{Tr}K}{2\\epsilon^{2}}\\left(1+o(1)\\right)a s\\;l o n g\\;a s\\log d=o\\left(\\frac{\\sqrt{\\mathrm{Tr}K}}{\\epsilon}\\left(\\log\\frac{\\sqrt{\\mathrm{Tr}K}}{\\epsilon}\\right)^{-1}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "771 Proof. If $\\tilde{E}$ is an $\\epsilon$ -covering of $E$ w.r.t. to the $L_{2}$ -norm, then $\\tilde{B}=K^{-\\frac{1}{2}}\\tilde{E}$ is an $\\epsilon$ -covering of $B$   \n772 w.r.t. the norm $\\left\\|\\cdot\\right\\|_{K}$ ,because if $w\\;\\in\\;B$ , then $\\sqrt{K}w\\,\\in\\,E$ and so there is an $\\tilde{x}\\,\\in\\,\\tilde{E}$ such that   \n773 $\\left\\|x-{\\sqrt{K}}w\\right\\|\\leq\\epsilon,$ but then $\\tilde{w}=\\sqrt{K}^{-1}x$ covers $w$ since $\\left\\|\\tilde{w}-w\\right\\|_{K}=\\left\\|x-\\sqrt{K}w\\right\\|_{K}\\leq\\epsilon.$   \n774 ince $\\lambda_{i}\\leq\\frac{\\mathrm{Tr}K}{i}$ $K\\leq\\bar{K}$ $\\bar{K}$ thematxbdbyan $i$ th igeavle $\\lambda_{i}$ d   \n775 b $\\textstyle{\\frac{\\mathrm{Tr}K}{i}}$ , and therefore $\\mathcal{N}\\left(B,\\left\\Vert\\cdot\\right\\Vert_{K},\\epsilon\\right)\\leq\\mathcal{N}\\left(B,\\left\\Vert\\cdot\\right\\Vert_{\\bar{K}},\\epsilon\\right)$ since $\\|\\cdot\\|_{K}\\leq\\|\\cdot\\|_{\\bar{K}}$ We now have the   \n776  a[proximation $\\log\\mathcal{N}\\left(B,\\|\\cdot\\|_{\\bar{K}}\\,,\\epsilon\\right)=\\bar{M}_{\\epsilon}\\left(1+o(1)\\right)$ for ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{M}_{\\epsilon}=\\displaystyle\\sum_{i=1}^{\\bar{k}_{\\epsilon}}\\log\\frac{\\sqrt{\\mathrm{Tr}K}}{\\sqrt{i}\\epsilon}}\\\\ &{\\bar{k}_{\\epsilon}=\\left\\lfloor\\frac{\\mathrm{Tr}K}{\\epsilon^{2}}\\right\\rfloor.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "777 We now have the simplification ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\bar{M}_{\\epsilon}=\\sum_{i=1}^{k_{\\epsilon}}\\log\\frac{\\sqrt{\\mathrm{Tr}K}}{\\sqrt{i}\\epsilon}=\\frac{1}{2}\\sum_{i=1}^{\\bar{k}_{\\epsilon}}\\log\\frac{\\bar{k}_{\\epsilon}}{i}=\\frac{\\bar{k}_{\\epsilon}}{2}(\\int_{0}^{1}\\log\\frac{1}{x}d x+o(1))=\\frac{\\bar{k}_{\\epsilon}}{2}(1+o(1))\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "778 where the $o(1)$ term vanishes as $\\epsilon\\searrow0$ . Furthermore, this allows us to check that as long as   \n779 $\\begin{array}{r}{\\log d=o\\left(\\frac{\\sqrt{\\mathrm{Tr}K}}{4\\epsilon\\log\\frac{\\sqrt{\\mathrm{Tr}K}}{\\epsilon}}\\right)}\\end{array}$ , the condition is satisfied ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\log\\frac{\\sqrt{\\mathrm{Tr}K}}{\\epsilon}=o\\left(\\frac{\\bar{k}_{\\epsilon}}{4\\log d}\\right)=o\\left(\\frac{\\bar{M}_{\\epsilon}^{2}}{\\bar{k}_{\\epsilon}\\log d}\\right).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "780 ", "page_idx": 27}, {"type": "text", "text": "781 Second we prove how to obtain the covering number of the convex hull of a function set $\\mathcal{F}$   \n782Theorem 18.Let $\\mathcal{F}$ beaset of $B$ -uniformlybounded functions, thenfor all $\\epsilon_{K}=B2^{-K}$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\sqrt{\\log\\mathcal{N}_{2}(\\mathrm{Conv}\\mathcal{F},2\\epsilon_{K})}\\le\\sqrt{18}\\sum_{k=1}^{K}2^{K-k}\\sqrt{\\log\\mathcal{N}_{2}(\\mathcal{F},B2^{-k})}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "783  Proof. Define $\\epsilon_{k}=B2^{-k}$ and the corresponding $\\epsilon_{k}$ -coverings $\\tilde{\\mathcal{F}}_{k}$ (w.r.t. some measure $\\pi$ ). For any   \n784 $f$ , we write $\\tilde{f}_{k}[f]$ for the function $\\bar{f}_{k}[f]\\in\\tilde{\\mathcal{F}}_{k}$ that covers $f$ . Then for any functions $f$ in $\\operatorname{Conv}\\!{\\mathcal{F}}$ ,we   \n785 have ", "page_idx": 27}, {"type": "equation", "text": "$$\nf=\\sum_{i=1}^{m}\\beta_{i}f_{i}=\\sum_{i=1}^{m}\\beta_{i}\\left(f_{i}-\\tilde{f}_{K}[f_{i}]\\right)+\\sum_{k=1}^{K}\\sum_{i=1}^{m}\\beta_{i}\\left(\\tilde{f}_{k}[f_{i}]-\\tilde{f}_{k-1}[f_{i}]\\right)+\\tilde{f}_{0}[f_{i}].\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "786   We may assume that $\\tilde{f}_{0}[f_{i}]=0$ since the zero function $\\epsilon_{0}$ -covers the whole $\\mathcal{F}$ since $\\epsilon_{0}=B$ ", "page_idx": 27}, {"type": "text", "text": "787  We will now use the probabilistic method to show that the sums $\\begin{array}{r}{\\sum_{i=1}^{m}\\beta_{i}\\left(\\tilde{f}_{k}[f_{i}]-\\tilde{f}_{k-1}[f_{i}]\\right)}\\end{array}$   \n788 an be approximated by finite averages. Consider the random functions $\\tilde{g}_{1}^{(k)},\\ldots,\\tilde{g}_{m_{k}}^{(k)}$   \n789 sampled id with $\\mathbb{P}\\left[\\tilde{g}_{j}^{(k)}\\right]\\;=\\;\\left(\\tilde{f}_{k}[f_{i}]-\\tilde{f}_{k-1}[f_{i}]\\right)$ with probability $\\beta_{i}$ . We have $\\mathbb{E}[\\tilde{g}_{j}^{(k)}]\\;=\\;$   \n790 $\\begin{array}{r}{\\sum_{i=1}^{m}\\beta_{i}\\left(\\tilde{f}_{k}[f_{i}]-\\tilde{f}_{k-1}\\tilde{[f_{i}]}\\right)}\\end{array}$ and ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{E}\\left\\|\\displaystyle\\sum_{k=1}^{K}\\displaystyle\\frac{1}{m_{k}}\\sum_{j=1}^{m_{k}}\\tilde{g}_{j}^{(k)}-\\sum_{k=1}^{K}\\sum_{i=1}^{m}\\beta_{i}\\left(\\tilde{f}_{k}[f_{i}]-\\tilde{f}_{k-1}[f_{i}]\\right)\\right\\|_{L_{p}(\\pi)}^{p}\\leq\\displaystyle\\sum_{k=1}^{K}\\displaystyle\\frac{1}{m_{k}^{p}}\\sum_{j=1}^{m_{k}}\\mathbb{E}\\left\\|\\tilde{g}_{j}^{(k)}\\right\\|_{L_{p}(\\pi)}^{p}}}\\\\ &{}&{=\\displaystyle\\sum_{k=1}^{K}\\displaystyle\\frac{1}{m_{k}}\\sum_{i=1}^{m}\\beta_{i}\\left\\|\\tilde{f}_{k}[f_{i}]-\\tilde{f}_{k-1}[f_{i}]\\right\\|_{L_{p}}^{p}}\\\\ &{}&{\\leq\\displaystyle\\sum_{k=1}^{K}\\displaystyle\\frac{3^{2}\\epsilon_{k}^{2}}{m_{k}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "791  Thus i we take $\\begin{array}{r}{m_{k}=\\frac{1}{a_{k}}(\\frac{3\\epsilon_{k}}{\\epsilon_{K}})^{2}}\\end{array}$ with $\\textstyle\\sum a_{k}=1$ we know that there must exist a choice of $\\tilde{g}_{j}^{(k)}\\mathrm{s}$ such   \n792that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left\\|\\sum_{k=1}^{K}\\frac{1}{m_{k}}\\sum_{j=1}^{m_{k}}\\tilde{g}_{j}^{(k)}-\\sum_{k=1}^{K}\\sum_{i=1}^{m}\\beta_{i}\\left(\\tilde{f}_{k}[f_{i}]-\\tilde{f}_{k-1}[f_{i}]\\right)\\right\\|_{L_{p}(\\pi)}\\le\\epsilon_{K}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "7 This imles ta fite he set $\\begin{array}{r}{\\tilde{\\mathcal{C}}=\\left\\{\\sum_{k=1}^{K}\\frac{1}{m_{k}}\\sum_{j=1}^{m_{k}}\\tilde{g}_{j}^{(k)}:\\tilde{g}_{j}^{(k)}\\in\\tilde{\\mathcal{F}}_{k}-\\tilde{\\mathcal{F}}_{k-1}\\right\\}}\\end{array}$ isan $2\\epsilon_{K}$ covring   \n794 of $\\mathcal{C}=\\mathrm{Conv}\\mathcal{F}$ since we know that for all $\\textstyle f=\\sum_{i=1}^{m}\\beta_{i}f_{i}$ there are $\\tilde{g}_{j}^{(k)}$ such that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\displaystyle\\sum_{k=1}^{K}\\frac{1}{m_{k}}\\displaystyle\\sum_{j=1}^{m_{k}}\\tilde{g}_{j}^{(k)}-\\displaystyle\\sum_{i=1}^{m}\\beta_{i}f_{i}\\right\\|_{L_{p}(\\pi)}\\le\\left\\|\\displaystyle\\sum_{i=1}^{m}\\beta_{i}\\left(f_{i}-\\tilde{f}_{K}[f_{i}]\\right)\\right\\|_{L_{p}(\\pi)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\displaystyle\\sum_{k=1}^{K}\\left\\|\\displaystyle\\frac{1}{m_{k}}\\displaystyle\\sum_{j=1}^{m_{k}}\\tilde{g}_{j}^{(k)}-\\displaystyle\\sum_{i=1}^{m}\\beta_{i}\\left(\\tilde{f}_{k}[f_{i}]-\\tilde{f}_{k-1}[f_{i}]\\right)\\right\\|_{L_{p}(\\pi)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\le2\\epsilon_{K}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "795 Since $\\begin{array}{r}{\\left|{\\tilde{c}}\\right|=\\prod_{k=1}^{K}\\left|{\\tilde{\\mathcal{F}}}_{k}\\right|^{m_{k}}\\left|{\\tilde{\\mathcal{F}}}_{k-1}\\right|^{m_{k}}}\\end{array}$ , we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\log\\mathcal{N}_{p}(\\mathcal{C},2\\epsilon_{K})\\leq\\displaystyle\\sum_{k=1}^{K}\\frac{1}{a_{k}}(\\frac{3\\epsilon_{k}}{\\epsilon_{K}})^{2}\\left(\\log\\mathcal{N}_{p}(\\mathcal{F},\\epsilon_{k})+\\log\\mathcal{N}_{p}(\\mathcal{F},\\epsilon_{k-1})\\right)}\\\\ &{\\qquad\\qquad\\qquad\\leq18\\displaystyle\\sum_{k=1}^{K}\\frac{1}{a_{k}}2^{2(K-k)}\\log\\mathcal{N}_{2}(\\mathcal{F},\\epsilon_{k}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "796  This is minimized for the choice ", "page_idx": 28}, {"type": "equation", "text": "$$\na_{k}=\\frac{2^{(K-k)}\\sqrt{\\log\\mathcal{N}_{2}(\\mathcal{F},\\epsilon_{k})}}{\\sum2^{(K-k)}\\sqrt{\\log\\mathcal{N}_{2}(\\mathcal{F},\\epsilon_{k})}},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "797 which yields the bound ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\sqrt{\\log\\mathcal{N}_{p}(\\mathcal{C},2\\epsilon_{K})}\\le\\sqrt{18}\\sum_{k=1}^{K}2^{K-k}\\sqrt{\\log\\mathcal{N}_{2}(\\mathcal{F},\\epsilon_{k})}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "798 ", "page_idx": 28}, {"type": "text", "text": "799 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Claims ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The contribution section accurately describes our contributions, and all theorems are proven in the appendix. ", "page_idx": 28}, {"type": "text", "text": "800   \n801   \n802   \n803   \n804   \n805   \n806   \n807   \n808   \n809   \n810   \n811   \n812   \n813 ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper. \u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. \u00b7 The claims made should match theoretical and experimental results, and refect how much the results can be expected to generalize to other settings. ", "page_idx": 28}, {"type": "text", "text": "\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors?   \n3 Answer: [Yes]   \n\uff0c Justification: We discuss limitations of our Theorems after we state them. Guidelines: \u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. \u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \nF \u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should refect on how these assumptions might be violated in practice and what the implications would be.   \n\uff0c \u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n\uff09 only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\uff1a \u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. \u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. \u00b7 If applicable, the authors should discuss possible limitations of their approach to   \n\uff09 address problems of privacy and fairness. \u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 29}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: All assumptions are either stated in the Theorem statements, except for a few recurring assumptions that are stated in the setup section. ", "page_idx": 29}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 29}, {"type": "text", "text": "864 4. Experimental Result Reproducibility ", "page_idx": 29}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or ", "page_idx": 30}, {"type": "text", "text": "conclusions of the paper (regardless of whether the code and data are provided or not)?   \nAnswer: [Yes]   \nJustification: The experimental setup is described in the Appendix.   \nGuidelines: \u00b7 The answer NA means that the paper does not include experiments. \u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. \u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. \u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. \u00b7 While NeurIPs does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 30}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: We use openly available data or synthetic data, with a description of how to build this synthetic data. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "29 6. Experimental Setting/Details ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "30 Question: Does the paper specify all the training and test details (e.g., data splits,   \n31 hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand   \n32 the results? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "34 Justification: In the experimental setup section in the Appendix.   \n35 Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 31}, {"type": "text", "text": "341 7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [No] ", "page_idx": 31}, {"type": "text", "text": "Justification: The numerical experiments are mostly there as a visualization of the theoretical results, our main goal is therefore clarity, which would be hurt by putting error bars everywhere. ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 31}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 31}, {"type": "text", "text": "973 Answer: [Yes]   \n974 Justification: In the experimental setup section of the Appendix   \n975 Guidelines:   \n976 \u00b7 The answer NA means that the paper does not include experiments.   \n977 \u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n978 or cloud provider, including relevant memory and storage.   \n979 \u00b7The paper should provide the amount of compute required for each of the individual   \n980 experimental runs as well as estimate the total compute.   \n981 \u00b7 The paper should disclose whether the full research project required more compute   \n982 than the experiments reported in the paper (e, preliminary or faild experiments that   \n983 didn't make it into the paper).   \n984 9. Code Of Ethics   \n985 Question: Does the research conducted in the paper conform, in every respect, with the   \n986 NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines?   \n987 Answer: [Yes]   \n988 Justification: We have read the Code of Ethics and see no issue.   \n989 Guidelines:   \n990 \u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n991 \u00b7 If the authors answer No, they should explain the special circumstances that require a   \n992 deviation from the Code of Ethics.   \n993 \u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special   \n994 consideration due to laws or regulations in their jurisdiction).   \n995 10. Broader Impacts   \n996 Question: Does the paper discuss both potential positive societal impacts and negative   \n997 societal impacts of the work performed?   \n998 Answer: [NA]   \n999 Justification: The paper is theoretical in nature, so it has no direct societal impact that can   \n1000 be meaningfully discussed.   \n1001 Guidelines:   \n1002 \u00b7 The answer NA means that there is no societal impact of the work performed.   \n1003 \u00b7 If the authors answer NA or No, they should explain why their work has no societal   \n1004 impact or why the paper does not address societal impact.   \n1005 \u00b7 Examples of negative societal impacts include potential malicious or unintended uses   \n1006 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n1007 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n1008 groups), privacy considerations, and security considerations.   \n1009 \u00b7 The conference expects that many papers willbe foundational research and not tied   \n1010 to particular applications, let alone deployments. However, if there is a direct path to   \n1011 any negative applications, the authors should point it out. For example, it is legitimate   \n1012 to point out that an improvement in the quality of generative models could be used to   \n1013 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n1014 that a generic algorithm for optimizing neural networks could enable people to train   \n1015 models that generate Deepfakes faster.   \n1016 \u00b7 The authors should consider possible harms that could arise when the technology is   \n1017 being used as intended and functioning correctly, harms that could arise when the   \n1018 technology is being used as intended but gives incorrect results, and harms following   \n1019 from (intentional or unintentional) misuse of the technology.   \n1020 \u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation   \n1021 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n1022 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n1023 feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "1024 11. Safeguards ", "page_idx": 32}, {"type": "text", "text": "1025 Question: Does the paper describe safeguards that have been put in place for responsible   \n1026 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n1027 image generators, or scraped datasets)?   \n1028 Answer: [NA]   \n1029 Justification: Not relevant to our paper.   \n1030 Guidelines:   \n1031 \u00b7 The answer NA means that the paper poses no such risks.   \n1032 \u00b7 Released models that have a high risk for misuse or dual-use should be released with   \n1033 necessary safeguards to allow for controlled use of the model, for example by requiring   \n1034 that users adhere to usage guidelines or restrictions to access the model or implementing   \n1035 safety filters.   \n1036 \u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n1037 should describe how they avoided releasing unsafe images.   \n1038 \u00b7We recognize that providing effective safeguards is challenging, and many papers do   \n1039 not require this, but we ncourage authors to take this into account and make a best   \n1040 faith effort.   \n1041 12. Licenses for existing assets   \n1042 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n1043 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n1044 properly respected?   \n1045 Answer: [Yes]   \n1046 Justification: In the experimental setup section of the Appendix.   \n1047 Guidelines:   \n1048 \u00b7 The answer NA means that the paper does not use existing assets.   \n1049 \u00b7 The authors should cite the original paper that produced the code package or dataset.   \n1050 \u00b7 The authors should state which version of the asset is used and, if possible, include a   \n1051 URL.   \n1052 \u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n1053 \u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of   \n1054 service of that source should be provided.   \n1055 If assets arereleased, tlise,cpyright fation, and tms of e t   \n1056 package should be provided. For popular datasets, paperswithcode.com/datasets   \n1057 has curated licenses for some datasets. Their licensing guide can help determine the   \n1058 license of a dataset.   \n1059 \u00b7 For existing datasets that are re-packaged, both the original license and the license of   \n1060 the derived asset if it has changed) should be provided.   \n1061 \u00b7 If this information is not available online,the authors are encouraged to reach out to   \n1062 the asset's creators.   \n1063 13. New Assets   \n1064 Question: Are new assets introduced in the paper well documented and is the documentation   \n1065 provided alongside the assets?   \n1066 Answer: [NA]   \n1067 Justification: We do not release any new assets.   \n1068 Guidelines:   \n1069 \u00b7 The answer NA means that the paper does not release new assets.   \n1070 \u00b7 Researchers should communicate the details of the dataset/code/model as part of their   \n1071 submissions via structured templates. This includes details about training, license,   \n1072 limitations, etc.   \n1073 \u00b7 The paper should discuss whether and how consent was obtained from people whose   \n1074 asset is used.   \n1075 \u00b7 At submission time, remember to anonymize your asets (if applicable). You can either   \n1076 create an anonymized URL or include an anonymized zip file. ", "page_idx": 33}, {"type": "text", "text": "uuiuellies: \u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. \u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. -Atcuiumg tu ut 1veuin o Luue u1 Lumes, wuintis mvuiveu m uata cunetuun, cuauun, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 34}, {"type": "text", "text": "9   \n0   \n1   \n2 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n3 Subjects   \n4 Question: Does the paper describe potential risks incurred by study participants, whether   \n5 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n6 approvals (or an equivalent approval/review based on the requirements of your country or   \n7 institution) were obtained?   \n8 Answer: [NA]   \n9 Justification: Not relevant to this paper.   \n0 Guidelines:   \n1 \u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with   \n2 human subjects.   \n3 \u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n4 may be required for any human subjects research. If you obtained IRB approval, you   \n5 should clearly state this in the paper.   \n6 \u00b7 We recognize that the procedures for this may vary significantly between institutions   \n7 and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the   \n8 guidelines for their institution.   \n9 \u00b7 For initial submissions, do not include any information that would break anonymity (if   \n0 applicable), such as the institution conducting the review. ", "page_idx": 34}]