[{"figure_path": "zuWgB7GerW/figures/figures_4_1.jpg", "caption": "Figure 1: Visualization of scaling laws. We observe that deep networks (either AccNets or DNNs) achieve better scaling laws than kernel methods or shallow networks on certain compositional tasks, in agreement with our theory. We also see that our new generalization bounds approximately recover the right scaling laws (even though they are orders of magnitude too large overall). We consider a compositional true function f* = hog where g maps from dimension 15 to 3 while h maps from 3 to 20, and we denote vg, vh for the number of times g, h are differentiable. In the first plot vg = 8, vh = 1 so that g is easy to learn while h is hard, whereas in the second plot vg = 9, vh = 9, so both g and h are relatively easier. The third plot presents the decay in test error and generalization bounds for networks evaluated using the real-world dataset, WESAD [37].", "description": "This figure visualizes the scaling laws of different network architectures (AccNets, DNNs, shallow networks, and kernel methods) on compositional tasks.  It shows how deep networks generalize better than shallower models, especially when one part of the composite function is much harder to learn than the other.  The three subplots represent different scenarios of function complexity and dataset used (synthetic and WESAD).  The results support the theoretical findings about compositionality and the generalization bounds derived in the paper, although these bounds are still orders of magnitude from the empirical results.", "section": "Breaking the Curse of Dimensionality with Compositionality"}, {"figure_path": "zuWgB7GerW/figures/figures_5_1.jpg", "caption": "Figure 2: A comparison of empirical and theoretical error rates. The first plot illustrates the log decay rate of the test error with respect to the dataset size N based on our empirical simulations. The second plot depicts the theoretical decay rate of the test error as discussed in Section 4.1, min{, h}. The final plot on the right displays the difference between the two. The lower left region represents the area where g is easier to learn than h, the upper right where h is easier to learn than g, and the lower right region where both f and g are easy.", "description": "This figure compares empirical and theoretical error rates for a composition of two functions, f* = h o g. The leftmost plot shows the empirically observed log decay rate of the test error as the dataset size (N) increases. The middle plot depicts the theoretically predicted log decay rate based on the model's complexity and the smoothness of the functions h and g. The rightmost plot highlights the difference between empirical and theoretical rates, revealing three distinct regions: one where function g is easier to learn, another where function h is easier to learn, and a third where both are relatively easy to learn.", "section": "4.1 Composition of Sobolev Balls"}, {"figure_path": "zuWgB7GerW/figures/figures_6_1.jpg", "caption": "Figure 3: Comparing error rates for shallow and AccNets: shallow nets vs. AccNets, and kernel methods vs. AccNets. The left two graphs shows the empirical decay rate of test error with respect to dataset size (N) for both shallow nets and kernel methods. In contrast to our earlier empirical findings for AccNets, both shallow nets and kernel methods exhibit a slower decay rate in test error. The right two graphs present the differences in log decay rates between shallow nets and AccNets, as well as between kernel methods and AccNets. AccNets almost always obtain better rates, with a particularly large advantage at the bottom and middle-left.", "description": "This figure compares the test error decay rates for shallow networks, kernel methods, and Accordion Networks (AccNets) across various dataset sizes.  The left two heatmaps show the error rates for shallow nets and kernel methods respectively. The right two heatmaps illustrate the performance difference between AccNets and the other two methods.  AccNets consistently demonstrate superior performance, particularly noticeable in regions of lower complexity.", "section": "4 Symmetry Learning"}, {"figure_path": "zuWgB7GerW/figures/figures_13_1.jpg", "caption": "Figure 1: Visualization of scaling laws. We observe that deep networks (either AccNets or DNNs) achieve better scaling laws than kernel methods or shallow networks on certain compositional tasks, in agreement with our theory. We also see that our new generalization bounds approximately recover the right scaling laws (even though they are orders of magnitude too large overall). We consider a compositional true function f* = hog where g maps from dimension 15 to 3 while h maps from 3 to 20, and we denote vg, vh for the number of times g, h are differentiable. In the first plot vg = 8, vh = 1 so that g is easy to learn while h is hard, whereas in the second plot vg = 9, vh = 9, so both g and h are relatively easier. The third plot presents the decay in test error and generalization bounds for networks evaluated using the real-world dataset, WESAD [37].", "description": "This figure visualizes the scaling laws of different network architectures (AccNets, DNNs, shallow networks, and kernel methods) on compositional tasks.  It demonstrates that deep networks exhibit superior scaling compared to other methods.  The plots illustrate the impact of varying differentiability (vg, vh) of component functions (g, h) in a compositional function (f* = h o g), and the results using a real-world dataset (WESAD).", "section": "Breaking the Curse of Dimensionality with Compositionality"}]