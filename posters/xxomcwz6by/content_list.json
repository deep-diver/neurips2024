[{"type": "text", "text": "Optimus-1 : Hybrid Multimodal Memory Empowered Agents Excel in Long-Horizon Tasks ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Zaijing Li1 2, Yuquan Xie1, Rui Shao1,\u2217 Gongwei Chen1, Dongmei Jiang2, Liqiang Nie1\u2217 1Harbin Institute of Technology, Shenzhen 2Peng Cheng Laboratory {lzj14011,xieyuquan20016,rshaojimmy,nieliqiang}@gmail.com ", "page_idx": 0}, {"type": "image", "img_path": "XXOMCwZ6by/tmp/23bb2098aad054204723be8c0e4fa6ebf0063b60e8245710da987f5934c9ef2f.jpg", "img_caption": ["Figure 1: An illustration of Optimus-1 performing long-horizon tasks in Minecraft. Given the task \u201cCraft stone sword\u201d, Knowledge-Guided Planner incorporates knowledge from Hierarchical Directed Knowledge Graph into planning, then Action Controller executes these planning sequences step-bystep. During the execution of the task, the Experience-Driven Reflector is periodically activated and retrieve experience from Abstracted Multimodal Experience Pool to make reflection. "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Building a general-purpose agent is a long-standing vision in the field of artificial intelligence. Existing agents have made remarkable progress in many domains, yet they still struggle to complete long-horizon tasks in an open world. We attribute this to the lack of necessary world knowledge and multimodal experience that can guide agents through a variety of long-horizon tasks. In this paper, we propose a Hybrid Multimodal Memory module to address the above challenges. It 1) transforms knowledge into Hierarchical Directed Knowledge Graph that allows agents to explicitly represent and learn world knowledge, and 2) summarises historical information into Abstracted Multimodal Experience Pool that provide agents with rich references for in-context learning. On top of the Hybrid Multimodal Memory module, a multimodal agent, Optimus-1, is constructed with dedicated Knowledge-guided Planner and Experience-Driven Reflector, contributing to a better planning and reflection in the face of long-horizon tasks in Minecraft. Extensive experimental results show that Optimus-1 significantly outperforms all existing agents on challenging long-horizon task benchmarks, and exhibits near humanlevel performance on many tasks. In addition, we introduce various Multimodal Large Language Models (MLLMs) as the backbone of Optimus-1. Experimental results show that Optimus-1 exhibits strong generalization with the help of the Hybrid Multimodal Memory module, outperforming the GPT-4V baseline on various tasks. Please see the project page at https://cybertronagent.github.io/Optimus1.github.io/. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Optimus Prime faces complex tasks alongside humans in Transformers to protect the peace of the planet. Creating an agent [44, 13] like Optimus that can perceive, plan, reflect, and complete long-horizon tasks in an open world has been a longstanding aspiration in the field of artificial intelligence [22, 36, 37, 27, 58]. Early research developed simple policy through reinforcement learning [7] or imitation learning [1, 25]. A lot of work [47, 50] have utilized Large Language Models (LLMs) as action planners for agents, generating executable sub-goal sequences for low-level action controllers. Further, recent studies [52, 33] employed Multimodal Large Language Models (MLLMs) [4, 39, 56] as planner and reflector. Leveraging the powerful instruction-following and logical reasoning capabilities of (Multimodal) LLMs [24], LLM-based agents have achieved remarkable success across multiple domains [14, 9, 10, 55]. Nevertheless, the ability of these agents to complete long-horizon tasks still falls significantly short of human-level performance. ", "page_idx": 1}, {"type": "text", "text": "According to relevant studies [28, 42, 46], the human ability to complete long-horizon tasks in an open world relies on long-term memory storage, which is divided into knowledge and experience. The storage and utilization of knowledge and experience play a crucial role in guiding human behavior and enabling humans to adapt flexibly to their environments in order to accomplish long-horizon tasks. Inspired by this theory, we summarize the challenges faced by current agents as follows: ", "page_idx": 1}, {"type": "text", "text": "Insufficient Exploration of Structured Knowledge: Structured knowledge, encompassing open world rules, object relationships, and interaction methods with the environment, is essential for agents to complete complex tasks [34, 44]. However, MLLMs such as GPT-4V \\* lack sufficient knowledge in Minecraft. Existing agents [1, 25, 7] only learn dispersed knowledge from video data and are unable to efficiently represent and learn this structured knowledge, rendering them incapable of performing complex tasks. ", "page_idx": 1}, {"type": "text", "text": "Lack of Multimodal Experience: Humans derive successful strategies and lessons from information on historical experience [8, 32], which assists them in tackling current complex tasks. In a similar manner, agents can benefit from in-context learning with experience demonstrations [43, 54]. However, existing agents [47, 51, 33] only consider unimodal information, which prevents them from learning from multimodal experience as humans do. ", "page_idx": 1}, {"type": "text", "text": "To address the aforementioned challenges, we propose Hybrid Multimodal Memory module that consists of Hierarchical Directed Knowledge Graph (HDKG) and Abstracted Multimodal Experience Pool (AMEP). For HDKG, we map the logical relationships between objects into a directed graph structure, thereby transforming knowledge into high-level semantic representations. HDKG efficiently provides the agent with the necessary knowledge for task execution, without requiring any parameter updates. For AMEP, we dynamically summarize and store the multimodal information (e.g., environment, agent state, task plan, video frames, etc.) from the agent\u2019s task execution process, ensuring that historical information contains both a global overview and local details. Different from the method of directly storing successful cases as experience [52], AMEP considers both successful and failed cases as references. This innovative approach of incorporating failure cases into in-context learning significantly enhances the performance of the agent. ", "page_idx": 1}, {"type": "text", "text": "On top of the Hybrid Multimodal Memory module, we construct a multimodal composable agent, Optimus-1. As shown in Figure 1, Optimus-1 consists of Knowledge-Guided Planner, ExperienceDriven Reflector, and Action Controller. To enhance the ability of agents to cope with complex environments and long-horizon tasks, Knowledge-Guided Planner incorporates visual observation into the planning phase, leveraging HDKG to capture the knowledge needed. This allows the agent to efficiently transform tasks into executable sub-goals. Action Controller takes the sub-goal and the current observation as inputs and generates low-level actions, interacting with the game environment to update the agent\u2019s state. In open-world complex environments, agents are prone to be erroneous when performing long-horizon tasks. To address this, we propose Experience-Driven Reflector, which is periodically activated to retrieve relevant multimodal experiences from AMEP. This encourages the agent to reflect on its current actions and refine the plan. ", "page_idx": 2}, {"type": "text", "text": "We validate the performance of Optimus-1 in Minecraft, a popular open-world game environment. Experimental results show that Optimus-1 exhibits remarkable performance on long-horizon tasks, representing up to $30\\%$ improvement over existing agents. Moreover, we introduce various Multimodal Large Language Models (MLLMs) as the backbone of Optimus-1. Experimental results show that Optimus-1 has a 2 to 6 times performance improvement with the help of Hybrid Multimodal Memory, outperforming powerful GPT-4V baseline on lots of long-horizon tasks. Additionally, we verified that the plug-and-play Hybrid Multimodal Memory can drive Optimus-1 to incrementally improve its performance in a self-evolution manner. The extensive experimental results show that Optimus-1 makes a major step toward a general agent with a human-like level of performance. Main contributions of our paper: ", "page_idx": 2}, {"type": "text", "text": "\u2022 We propose Hybrid Multimodal Memory module which is composed of HDKG and AMEP. HDKG helps the agent make the planning of long-horizon tasks efficiently. AMEP provides refined historical experience and guides the agent to reason about the current situation state effectively. \u2022 On top of the Hybrid Multimodal Memory module, we construct Optimus-1, which consists of Knowledge-Guided Planner, Experience-Driven Reflector, and Action Controller. Optimus-1 outperforms all baseline agents on long-horizon task benchmarks, and exhibits capabilities close to the level of human players. \u2022 Driven by Hybrid Multimodal Memory, various MLLM-based Optimus-1 have demonstrated 2 to 6 times performance improvement, demonstrating the generalization of Hybrid Multimodal Memory. ", "page_idx": 2}, {"type": "text", "text": "2 Optimus-1 ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we first elaborate on how to implement the Hybrid Multimodal Memory in Sec 2.1. As a core innovation, it plays a crucial role in enabling Optimus-1 to execute long-horizon tasks. Next, we give an overview of Optimus-1 framework (Sec 2.2), which consists of Hybrid Multimodal Memory, Knowledge-Guided Planner, Experience-Driven Reflector, and Action Controller. Finally, we introduce a non-parametric learning approach to expand the hybrid multimodal memory (Sec 2.3), thereby enhancing the success rate of task execution for Optimus-1. ", "page_idx": 2}, {"type": "text", "text": "2.1 Hybrid Multimodal Memory ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In order to endow agent with a long-term memory storage mechanism [28, 46], we propose the Hybrid Multimodal Memory module, which consists of Abstracted Multimodal Experience Pool (AMEP) and Hierarchical Directed Knowledge Graph (HDKG). ", "page_idx": 2}, {"type": "text", "text": "2.1.1 Abstracted Multimodal Experience Pool ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Relevant studies [23, 29, 17, 15] highlight the importance of historical information for agents completing long-horizon tasks. Minedojo [7] and Voyager [47] employed unimodal storage of historical information. Jarvis-1 [52] used a multimodal experience mechanism that stores task planning and visual information without summarization, posing challenges to storage capacity and retrieval speed. To address this issue, we propose AMEP, which aims to dynamically summarize all multimodal information during task execution. It preserves the integrity of long-horizon data while enhancing storage and retrieval efficiency. ", "page_idx": 2}, {"type": "image", "img_path": "XXOMCwZ6by/tmp/d001a0a44b0bae631067234e0a05010fb63ba51df04a67bed5cadf201c849244.jpg", "img_caption": ["Figure 2: (a) Extraction process of multimodal experience. The frames are filtered through video buffer and image buffer, then MineCLIP [7] is employed to compute the visual and sub-goal similarities and finally they are stored in Abstracted Multimodal Experience Pool. (b) Overview of Hierarchical Directed Knowledge Graph. Knowledge is stored as a directed graph, where its nodes represent objects, and directed edges point to materials that can be crafted by this object. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Specifically, as depicted in Figure 2, to conduct the static visual information abstraction, the video stream captured by Optimus-1 during task execution is first input to a video buffer, filtering the stream at a fixed frequency of 1 frame per second. Based on the filtered video frames, to further perform a dynamic visual information abstraction, these frames are then fed into an image buffer with a window size of 16, where the image similarity is dynamically computed and final abstracted frames are adaptively updated. To align such abstracted visual information with the corresponding textual sub-goal, we then utilize MineCLIP [7], a pre-trained video-text alignment model, to calculate their multimodal correlation. When this correlation exceeds a threshold, the corresponding image buffer and textual sub-goal are saved as multimodal experience into a pool. Finally, we further incorporate environment information, agent initial state, and plan generated by Knowledge-Guided Planner, into such a pool, which forms the AMEP. In this way, we consider the multimodal information of each sub-goal, and summarise it to finally compose the multimodal experience of the given task. ", "page_idx": 3}, {"type": "text", "text": "2.1.2 Hierarchical Directed Knowledge Graph ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In Minecraft, mining and crafting represent a complex knowledge network crucial for effective task planning. For instance, crafting a diamond sword $\\mathcal{A}$ requires two diamonds $\\circleddash$ and one wooden stick $\\nearrow$ , while mining diamonds requires an iron pickaxe $\\mathbf{\\hat{\\Pi}}$ , which involving further materials and steps. Such knowledge is essential for an agent\u2019s ability to perform long-horizon complex tasks. Instead of implicit learning through fine-tuning [33, 60], we propose HDKG, which transforms knowledge into a graph representation. It enables the agent to perform explicit learning by retrieving information from the knowledge graph. ", "page_idx": 3}, {"type": "text", "text": "As shown in the Figure 2, we transform knowledge into a graph $\\mathcal{D}(\\boldsymbol{\\upnu},\\boldsymbol{\\mathcal{E}})$ , where nodes set $\\nu$ represent objects, and directed edges set $\\mathcal{E}$ point to nodes that can be crafted by this object. An edge $e\\in\\mathcal{E}$ in the $\\mathcal{D}$ can be represented as $\\boldsymbol{e}=(u,v)$ , where $u,v\\in\\mathcal{V}$ . The directed graph efficiently stores and updates knowledge. For a given object $x$ , retrieving the corresponding node allows extraction of a sub-graph $\\mathcal{D}_{j}(\\mathcal{V}_{j},\\mathcal{E}_{j})\\in\\mathcal{D}$ , where nodes set $\\nu_{j}$ and edges set $\\mathscr{E}_{j}$ can be formulated as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n{\\mathcal V}_{j}=\\left\\{v\\in{\\mathcal V}\\mid x\\right\\},\\qquad{\\mathcal E}_{j}=\\left\\{e=\\left(u,v\\right)\\in{\\mathcal V}\\mid u\\in{\\mathcal V}_{j}\\cup v\\in{\\mathcal V}_{j}\\right\\},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Then by topological sorting, we can get all the materials and their relationships needed to complete the task. This knowledge is provided to the Knowledge-Guided Planner as a way to generate a more ", "page_idx": 3}, {"type": "image", "img_path": "XXOMCwZ6by/tmp/6997da8154a57abf1aef87ad9c529d2da1330a9fc60ebc1117d71a7a717304e7.jpg", "img_caption": ["Figure 3: Overview framework of our Optimus-1. Optimus-1 consists of Knowledge-Guided Planner, Experience-Driven Reflector, Action Controller, and Hybrid Multimodal Memory architecture. Given the task \u201ccraft stone sword\u201d, Optimus-1 incorporates the knowledge from HDKG into KnowledgeGuided Planning, then Action Controller generates low-level actions. Experience-Driven Reflector is periodically activated to introduce multimodal experience from AMEP to determine if the current task can be executed successfully. If not, it will ask the Knowledge-Guided Planner to refine the plan. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "reasonable sequence of sub-goals. With HDKG, we can significantly enhance the world knowledge of the agent in a train-free manner. ", "page_idx": 4}, {"type": "text", "text": "2.2 Optimus-1: Framework ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Relevant studies indicate that the human brain is essential for planning and reflection, while the cerebellum controls low-level actions, both crucial for complex tasks [40, 41]. Inspired by this, we divide the structure of Optimus-1 into Knowledge-Guided Planner, Experience-Driven Reflector, and Action Controller. In a given game environment with a long-horizon task, the Knowledge-Guided Planner senses the environment, retrieves knowledge from HDKG, and decomposes the task into executable sub-goals. The action controller then sequentially executes these sub-goals. During execution, the Experience-Driven Reflector is activated periodically, leveraging historical experience from AMEP to assess whether Optimus-1 can complete the current sub-goal. If not, it instructs the Knowledge-Guided Planner to revise its plan. Through iterative interaction with the environment, Optimus-1 ultimately completes the task. ", "page_idx": 4}, {"type": "text", "text": "Knowledge-Guided Planner. Open-world environments vary greatly, affecting task execution. Previous approaches [51] using LLMs for task planning failed to consider the environment, leading to the failure of tasks. For example, an agent in a cave aims to catch fish. It lacks visual information to plan conditions on the current situation, such as \u201cleave the cave and find a river\u201d. Therefore, we integrate environmental information into the planning stage. Unlike Jarvis-1 [52] and MP5 [33], which convert observation to textual descriptions, Optimus-1 directly employs observation as visual conditions to generate environment-related plans, i.e., sub-goal sequences. This results in more comprehensive and reasonable planning. More importantly, Knowledge-Guided Planner retrieves the knowledge needed to complete the task from HDKG, allowing task planning to be done once, rather than generating the next step in each iteration. Given the task $t$ , observation $o$ , the sub-goals ", "page_idx": 4}, {"type": "text", "text": "sequence $g_{1},g_{2},g_{3},...,g_{n}$ can be formulated as: ", "page_idx": 5}, {"type": "equation", "text": "$$\ng_{1},g_{2},g_{3},...,g_{n}=p_{\\theta}(o,t,p_{\\eta}(t)),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $n$ is the number of sub-goals, $p_{\\eta}$ denotes sub-graph retrieved from HDKG, $p_{\\theta}$ denotes MLLM. In this paper, we employ OpenAI\u2019s GPT-4V as Knowledge-Guided Planner and Experience-Driven Reflector. We also evaluate other alternatives of GPT-4V, such as open-source models like DeepseekVL [26] and InternLM-XComposer2-VL [6] in Section 3.4. ", "page_idx": 5}, {"type": "text", "text": "Action Controller. It takes the sub-goal and the current observation as inputs and then generates low-level actions, which are control signals for the mouse and keyboard. Thus, it can interact with the game environment to update the agent\u2019s state and the observation. The formulation is as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\na_{k}=p_{\\pi}(o,g_{i}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $a_{k}$ denotes low-level action at time $k$ , $p_{\\pi}$ denotes action controller. Unlike generating code [47, 33, 50], generating control actions for the mouse and keyboard [1, 25, 52, 3] more closely resembles human behavior. In this paper, we employ STEVE-1 [25] as our Action Controller. ", "page_idx": 5}, {"type": "text", "text": "Experience-Driven Reflector. The sub-goals generated by Knowledge-Guided Planner are interdependent. The failure of any sub-goal halts the execution of subsequent ones, leading to overall task failure. Therefore, a reflection module is essential to identify and rectify errors promptly. During task execution, the Experience-Driven Reflector activates at regular intervals, retrieving historical experience from AMEP, and then analyzing the current state of Optimus-1. The reflection results of Optimus-1 are categorized as COMPLETE, CONTINUE, or REPLAN. COMPLETE indicates successful execution, prompting the action controller to proceed to the next sub-goal. CONTINUE signifies ongoing execution without additional feedback. REPLAN denotes failure, requiring the KnowledgeGuided Planner to revise the plan. The reflection $r$ generated by Experience-Driven Reflector can be formulated as: ", "page_idx": 5}, {"type": "equation", "text": "$$\nr=p_{\\theta}(o,g_{i},p_{\\epsilon}(t)),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $p_{\\epsilon}$ denotes multimodal experience retrieved from AMEP. Experimental results in Section 3.3 demonstrate that the Experience-Driven Reflector significantly enhances the success rate of long-horizon tasks. ", "page_idx": 5}, {"type": "text", "text": "During task execution, even in cases where task failure necessitates REPLAN, multimodal experiences are stored in AMEP. Thus, during the reflection phase, Optimus-1 can retrieve the most relevant cases from each of the three scenarios COMPLETE, CONTINUE, and REPLAN from AMEP as references. Experimental Results in Section 3.3 demonstrate the effectiveness of this innovative method of incorporating failure cases into in-context learning. ", "page_idx": 5}, {"type": "text", "text": "2.3 Non-parametric Learning of Hybrid Multimodal Memory ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To implement the Hybrid Multimodal Memory and enhance Optimus-1\u2019s capacity, we propose a nonparametric learning method named \u201cfree exploration-teacher guidance\u201d. In the free exploration phase, Optimus-1\u2019s equipment and tasks are randomly initialized, and it explores random environments, acquiring world knowledge through environmental feedback. For example, it learns that \u201ca stone sword $\\mathcal{A}$ can be crafted with a wooden stick $\\nearrow$ and two cobblestones \u201d, storing this in the HDKG. Additionally, successful and failed cases are stored in the AMEP, providing reference experience for the reflection phase. We initialize multiple Optimus-1, and they share the same HDKG and AMEP. Thus the memory is fliled up efficiently. After free exploration, Optimus-1 has basic world knowledge and multimodal experience. In the teacher guidance phase, Optimus-1 needs to learn a small number of long-horizon tasks based on extra knowledge. For example, it learns \u201ca diamond sword $\\mathcal{A}$ is obtained by a stick $\\nearrow$ and two diamonds $\\circleddash$ \u201d from the teacher, then perform the task \u201ccraft diamond sword\u201d. During the teacher guidance phase, Optimus-1\u2019s memory is further expanded and it gains the experience of executing complete long-horizon tasks. ", "page_idx": 5}, {"type": "text", "text": "Unlike fine-tuning, this method enhances Optimus-1 incrementally without updating parameters, in a self-evolution manner. Starting with an empty Hybrid Multimodal Memory, Optimus-1 iterates between \u201cfree exploration-teacher guidance\u201d learning and unseen task inference. With each iteration, its memory capacity grows, enabling mastery of tasks from easy to hard. ", "page_idx": 5}, {"type": "table", "img_path": "XXOMCwZ6by/tmp/2b563a73bd654e5989338c1cca4dd5c5a0c3e2984865e9bd6e3035d7eadf64a8.jpg", "table_caption": ["Table 1: Main Result of Optimus-1 on long-horizon tasks benchmark. We report the average success rate (SR), average number of steps (AS), and average time (AT) on each task group, the results of each task can be found in the Appendix F. Lower AS and AT metrics mean that the agent is more efficient at completing the task, while $+\\infty$ indicates that the agent is unable to complete the task. Overall represents the average result on the five groups of Iron, Gold, Diamond, Redstone, and Armor. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "3 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "3.1 Experiments Setting ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Environment. To ensure realistic gameplay like human players, we employ MineRL [11] with Minecraft 1.16.5 as our simulation environment. The agent operates at a fixed speed of 20 frames per second and only interacts with the environment via low-level action control signals of the mouse and keyboard. For more information about the detailed descriptions of the observation and action spaces, please refer to the Appendix B. ", "page_idx": 6}, {"type": "text", "text": "Benchmark. We constructed a benchmark of 67 tasks to evaluate the Optimus-1\u2019s ability to complete long-horizon tasks. As illustrated in Table 5, we divide the 67 Minecraft tasks into 7 groups according to recommended categories in Minecraft. Please refer to Appendix D for more details. ", "page_idx": 6}, {"type": "text", "text": "Baseline. We compare Optimus-1 with various agents, including GPT-3.5 2, GPT-4V, DEPS [51], and Jarvis-1 [52] on the challenging long-horizon tasks benchmark. In addition, we employed 10 volunteers to perform the same task on the benchmark, and their average performance served as a human-level baseline. Please refer to Appendix D.2 for more details about human-level baseline. For a more comprehensive comparison, we also report Optimus-1\u2019s performances on the benchmark used by Voyager [47], MP5 [33], and DEPS [51] in the Appendix F.2. Note that we initialize Optimus-1 with an empty inventory, while DEPS [51] and Jarvis-1 [52] have tools in their initial state. This makes it more challenging for Optimus-1 to perform the same tasks. ", "page_idx": 6}, {"type": "text", "text": "Table 2: Ablation study results. We report averageTable 3: Ablation study on AMEP. We report success rate (SR) on each task group. P., R., K.,the average success rate (SR) on each task group. E. represent Planning, Reflection, Knowledge, andZero, Suc., and Fail. represent retrieving from Experience, respectively. AMEP without getting the case, getting the success case, and getting the failure case, respectively. ", "page_idx": 7}, {"type": "table", "img_path": "XXOMCwZ6by/tmp/d3296c224f22718f15d693fdf65f288267b768791c29c09595660b8e2c9039eb.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "XXOMCwZ6by/tmp/ac3625c4ccf749adb5c02588aba95061270401e69c82434f995a52b8ae2af1f0.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "XXOMCwZ6by/tmp/b30dada4439354995a3b97af8196a8faf7f0427d04a82edf9851f63a9d9c11a4.jpg", "img_caption": ["Figure 4: Illustration of the role of reflection mechanism. Without the help of reflective mechanisms, STEVE-1 [25] often gets into trouble and fails to complete the task. While Optimus-1, with the help of the Experience-Driven Reflector, leverages the AMEP to retrieve relevant experience, reflect current situation and correct errors. This improves Optimus-1\u2019s success rate on long-horizon tasks. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Evaluation Metrics. The agent always starts in survival mode, with an empty inventory. We conducted at least 30 times for each task using different world seeds and reported the average success rate to ensure fair and thorough evaluation. Additionally, we add the average steps and average time of completing the task as evaluation metrics. ", "page_idx": 7}, {"type": "text", "text": "3.2 Experimental Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The overall experimental results on benchmark are shown in Table 1, see the accuracy for each task in Appendix F. Optimus-1 has a success rate near $100\\%$ on the Wood Group $\\textcircled{10}$ . Compared with Jarvis-1, Optimus-1 has $29.28\\%$ and $53.40\\%$ improvement on the Diamond Group $\\circleddash$ and Redstone Group $\\oplus$ , respectively. Optimus-1 achieves the best performance and the shortest elapsed time among all task groups. It reveals the effectiveness and efficiency of our proposed Optimus-1 framework. Moreover, compared with all baselines, Optimus-1 performance was closer (average $5.37\\%$ improvement) to human levels on long-horizon task groups. ", "page_idx": 7}, {"type": "text", "text": "3.3 Ablation Study ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We conduct extensive ablation experiments on 18 tasks, experiment setting can be found in Table 6. As shown in Table 2, we first remove Knowledge-Driven Planner and Experience-Driven Reflector, the performance of Optimus-1 on all task groups drops dramatically. It demonstrates the necessity of Knowledge-Guided Planner and Experience-Driven Reflector modules for performing long-horizon tasks. As for Hybrid Multimodal Memory, we remove HDKG from Optimus-1. Without the help of world knowledge, the performance of Optimus-1 decreased by an average of $20\\%$ across all task groups. We then removed AMEP, this resulted in the performance of Optimus-1 decreased by an average of $12\\%$ . Finally, we performed ablation experiments on the way of retrieving cases from AMEP. As shown in Table 3, without retrieving cases from AMEP, the success rate shows an average of $10\\%$ decrease across all groups. It reveals that this reflection mechanism, which considers both success and failure cases, has a significant impact on the performance of Optimus-1. To illustrate the role of the reflection mechanism, we have shown some cases in Figure 4. ", "page_idx": 7}, {"type": "image", "img_path": "XXOMCwZ6by/tmp/82835d5d4681055e9f6b5477ccc8f55d0232ada31f965a0a05ea044dee8b7b95.jpg", "img_caption": ["Figure 5: (a) With the help of Hybrid Multimodal Memory, various MLLM-based Optimus-1 have demonstrated 2 to 6 times performance improvement. (b) Illustration of the change in Optimus-1 success rate on the unseen task over 4 epochs. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "3.4 Generalization Ability ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we explore an interesting issue: whether generic MLLMs can effectively perform various long-horizon complex tasks in Minecraft using Hybrid Multimodal Memory. As shown in Figure 5, We employ Deepseek-VL [26] and InternLM-XComposer2-VL [6] as Knowledge-Guided Planner and Experience-Driven Reflector. The experimental results show that the original MLLM has low performance on long-horizon tasks due to the lack of knowledge and experience of Minecraft. With the assistance of Hybrid Multimodal Memory, the performance of MLLMs has improved by 2 to 6 times across various task groups, outperforming the GPT-4V baseline. This encouraging result demonstrates the generalization of the proposed Hybrid Multimodal Memory. ", "page_idx": 8}, {"type": "text", "text": "3.5 Self-Evolution via Hybrid Multimodal Memory ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "As shown in Section 2.3, we randomly initialize the Hybrid Multimodal Memory of Optimus-1, then update it multiple times by using the \u201cfree exploration-teacher guidance\u201d learning method. We set the epoch to 4, and the number of learning tasks to 160. At each period, Optimus-1 performs free exploration on 150 tasks and teacher guidance learning on the remaining 10 tasks, we then evaluate Optimus-1\u2019s learning ability on the task groups same as ablation study. Experimental results are shown in Figure 5. It reveals that Optimus-1 keeps getting stronger through the continuous expansion of memory during the learning process of multiple periods. Moreover, it demonstrates that MLLM with Hybrid Multimodal Memory can incarnate an expert agent in a self-evolution manner [45]. ", "page_idx": 8}, {"type": "text", "text": "4 Related Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "4.1 Agents in Minecraft ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We summarise the differences of existing Minecraft agents in the Appendix D.3. Earlier work [30, 57, 2, 3] introduced policy models for agents to perform simple tasks in Minecraft. MineCLIP [7] used text-video data to train a contrastive video-language model as a reward model for policy, while VPT [1] pre-trained on unlabelled videos but lacked instruction as input. Building on VPT and MineCLIP, STEVE-1 [25] added text input to generate low-level action sequences from human instructions and images. However, these agents struggle with complex tasks due to limitations in instruction comprehension and planning. Recent work [50, 47, 61] incorporated LLMs as planning and reflection modules, but lacked visual information integration for adaptive planning. MP5 [33], MineDreamer [60], and Jarvis-1 [52] enhanced situation-aware planning by obtaining textual descriptions of visual information, yet lacked detailed visual data. Optimus-1 addresses these issues by directly using observation as situation-aware conditions in the planning phase, enabling more rational, visually informed planning. Additionally, unlike other agents requiring multiple queries for task refinement, Optimus-1 generates a complete and effective plan in one step with the help of HDKG. This makes Optimus-1 planning more efficient. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "4.2 Memory in Agents ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In the agent-environment interaction process, memory is key to achieving experience accumulation [21], environment exploration [16], and knowledge abstraction [59]. There are two forms to represent memory content in LLM-based agents: textual form [17, 15, 31] and parametric form [5, 29, 48, 20]. In textual form, the information is explicitly retained and recalled by natural languages. In parametric form, the memory information [38] is encoded into parameters and implicitly influences the agent\u2019s actions. Recent work [49, 53, 12] has explored the long-term visual information storage [18, 19] and summarisation in MLLM. Our proposed hybrid multimodal memory module is plug-and-play and can provide world knowledge and multimodal experience for Optimus-1 efficiently. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we propose Hybrid Multimodal Memory module, which consists of two parts: HDKG and AMEP. HDKG provides the necessary world knowledge for the planning phase of the agent, and AMEP provides the refined historical experience for the reflection phase of the agent. On top of the Hybrid Multimodal Memory, we construct the multimodal composable agent, Optimus-1, in Minecraft. Extensive experimental results show that Optimus-1 outperforms all existing agents on long-horizon tasks. Moreover, we validate that general-purpose MLLMs, based on Hybrid Multimodal Memory and without additional parameter updates, can exceed the powerful GPT-4V baseline. The extensive experimental results show that Optimus-1 makes a major step toward a general agent with a human-like level of performance. ", "page_idx": 9}, {"type": "text", "text": "6 Limitation and Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In the framework of Optimus-1, we are dedicated to leverage proposed Hierarchical Directed Knowledge Graph and Abstracted Multimodal Experience Pool can be used to enhance the agent\u2019s ability to plan and reflect. For Action Controller, we directly introduce STEVE-1 [25] as a generator of low-level actions. However, limited by STEVE-1\u2019s ability to follow instructions and execute complex actions, Optimus-1 is weak in completing challenging tasks such as \u201cbeat ender dragon\u201d and \u201cbuild a house\u201d. Therefore, a potential future research direction is to enhance the instruction following and action generation capabilities of action controller. ", "page_idx": 9}, {"type": "text", "text": "In addition, most of the work, including Optimus-1, utilize a multimodal large language model for planning and reflection, which then drives an action controller to perform the task. Building an end-to-end vision-language-action agent will be future work. ", "page_idx": 9}, {"type": "text", "text": "7 Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This study is supported by National Natural Science Foundation of China (Grant No. 62236003 and 62306090), Shenzhen College Stability Support Plan (Grant No. GXWD20220817144428005), Natural Science Foundation of Guangdong Province of China (Grant No. 2024A1515010147), and Major Key Project of Peng Cheng Laboratory (Grant No. PCL2023A08). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Bowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon Houghton, Raul Sampedro, and Jeff Clune. Video pretraining (vpt): Learning to act by watching unlabeled online videos. Advances in Neural Information Processing Systems, 35:24639\u201324654, 2022. [2] Shaofei Cai, Zihao Wang, Xiaojian Ma, Anji Liu, and Yitao Liang. Open-world multi-task control through goal-aware representation learning and adaptive horizon prediction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13734\u201313744, 2023.   \n[3] Shaofei Cai, Bowei Zhang, Zihao Wang, Xiaojian Ma, Anji Liu, and Yitao Liang. Groot: Learning to follow instructions by watching gameplay videos. In The Twelfth International Conference on Learning Representations, 2023. [4] Gongwei Chen, Leyang Shen, Rui Shao, Xiang Deng, and Liqiang Nie. Lion: Empowering multimodal large language model with dual-level visual knowledge. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024.   \n[5] Nicola De Cao, Wilker Aziz, and Ivan Titov. Editing factual knowledge in language models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6491\u20136506, 2021. [6] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong Duan, Maosong Cao, et al. Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model. arXiv preprint arXiv:2401.16420, 2024.   \n[7] Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied agents with internet-scale knowledge. Advances in Neural Information Processing Systems, 35:18343\u201318362, 2022.   \n[8] Mariel K Goddu and Alison Gopnik. The development of human causal learning and reasoning. Nature Reviews Psychology, pages 1\u201321, 2024.   \n[9] Maitrey Gramopadhye and Daniel Szafir. Generating executable action plans with environmentally-aware language models. In 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 3568\u20133575. IEEE, 2023.   \n[10] Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, and Aleksandra Faust. A real-world webagent with planning, long context understanding, and program synthesis. arXiv preprint arXiv:2307.12856, 2023.   \n[11] William H Guss, Brandon Houghton, Nicholay Topin, Phillip Wang, Cayden Codel, Manuela Veloso, and Ruslan Salakhutdinov. Minerl: A large-scale dataset of minecraft demonstrations. arXiv preprint arXiv:1907.13440, 2019.   \n[12] Bo He, Hengduo Li, Young Kyun Jang, Menglin Jia, Xuefei Cao, Ashish Shah, Abhinav Shrivastava, and Ser-Nam Lim. Ma-lmm: Memory-augmented large multimodal model for long-term video understanding. arXiv preprint arXiv:2404.05726, 2024.   \n[13] Jiangyong Huang, Silong Yong, Xiaojian Ma, Xiongkun Linghu, Puhao Li, Yan Wang, Qing Li, Song-Chun Zhu, Baoxiong Jia, and Siyuan Huang. An embodied generalist agent in 3d world. arXiv preprint arXiv:2311.12871, 2023.   \n[14] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In International Conference on Machine Learning, pages 9118\u20139147. PMLR, 2022.   \n[15] Ziheng Huang, Sebastian Gutierrez, Hemanth Kamana, and Stephen MacNeil. Memory sandbox: Transparent and interactive memory management for conversational agents. In Adjunct Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology, pages 1\u20133, 2023.   \n[16] Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang, Yongqiang Dou, Yanjun Chen, Li Fei-Fei, Anima Anandkumar, Yuke Zhu, and Linxi Fan. Vima: General robot manipulation with multimodal prompts. In NeurIPS 2022 Foundation Models for Decision Making Workshop, 2022.   \n[17] Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang. How long can context length of open-source llms truly promise? In NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following, 2023.   \n[18] Xiaojie Li, Shaowei He, Jianlong Wu, Yue Yu, Liqiang Nie, and Min Zhang. Mask again: Masked knowledge distillation for masked video modeling. In Proceedings of the ACM International Conference on Multimedia, page 2221\u20132232. ACM, 2023.   \n[19] Xiaojie Li, Jianlong Wu, Shaowei He, Shuo Kang, Yue Yu, Liqiang Nie, and Min Zhang. Finegrained key-value memory enhanced predictor for video representation learning. In Proceedings of the ACM International Conference on Multimedia, page 2264\u20132274. ACM, 2023.   \n[20] Xiaojie Li, Yibo Yang, Xiangtai Li, Jianlong Wu, Yue Yu, Bernard Ghanem, and Min Zhang. Genview: Enhancing view quality with pretrained generative model for self-supervised learning. In Proceedings of the European Conference on Computer Vision. Springer, 2024.   \n[21] Xiaojie Li, Yibo Yang, Jianlong Wu, Bernard Ghanem, Liqiang Nie, and Min Zhang. Mambafscil: Dynamic adaptation with selective state space model for few-shot class-incremental learning. arXiv preprint arXiv:2407.06136, 2024.   \n[22] Zaijing Li, Fengxiao Tang, Ming Zhao, and Yusen Zhu. Emocaps: Emotion capsule based model for conversational emotion recognition. In Findings of the Association for Computational Linguistics: ACL 2022, pages 1610\u20131618, 2022.   \n[23] Zaijing Li, Ting-En Lin, Yuchuan Wu, Meng Liu, Fengxiao Tang, Ming Zhao, and Yongbin Li. Unisa: Unified generative framework for sentiment analysis. In Proceedings of the 31st ACM International Conference on Multimedia, pages 6132\u20136142, 2023.   \n[24] Zaijing Li, Gongwei Chen, Rui Shao, Dongmei Jiang, and Liqiang Nie. Enhancing the emotional generation capability of large language models via emotional chain-of-thought. arXiv preprint arXiv:2401.06836, 2024.   \n[25] Shalev Lifshitz, Keiran Paster, Harris Chan, Jimmy Ba, and Sheila McIlraith. Steve-1: A generative model for text-to-behavior in minecraft. Advances in Neural Information Processing Systems, 2023.   \n[26] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Yaofeng Sun, et al. Deepseek-vl: towards real-world vision-language understanding. arXiv preprint arXiv:2403.05525, 2024.   \n[27] Qi Lv, Xiang Deng, Gongwei Chen, Michael Y Wang, and Liqiang Nie. Decision mamba: A multi-grained state space model with self-evolution regularization for offline rl. In NeurIPS, 2024.   \n[28] Simon Makin. The amyloid hypothesis on trial. Nature, 559(7715):S4\u2013S4, 2018.   \n[29] Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher D Manning. Fast model editing at scale. In International Conference on Learning Representations, 2021.   \n[30] Junhyuk Oh, Satinder Singh, Honglak Lee, and Pushmeet Kohli. Zero-shot task generalization with multi-task deep reinforcement learning. In Proceedings of the 34th International Conference on Machine Learning, pages 2661\u20132670. PMLR, 2017.   \n[31] Arka Pal, Deep Karkhanis, Manley Roberts, Samuel Dooley, Arvind Sundararajan, and Siddartha Naidu. Giraffe: Adventures in expanding context lengths in llms. arXiv preprint arXiv:2308.10882, 2023.   \n[32] Eileen Parkes. Scientific progress is built on failure. Nature, 10, 2019.   \n[33] Yiran Qin, Enshen Zhou, Qichang Liu, Zhenfei Yin, Lu Sheng, Ruimao Zhang, Yu Qiao, and Jing Shao. Mp5: A multi-modal open-ended embodied system in minecraft via active perception. arXiv preprint arXiv:2312.07472, 2023.   \n[34] Maria Abi Raad, Arun Ahuja, Catarina Barros, Frederic Besse, Andrew Bolt, Adrian Bolton, Bethanie Brownfield, Gavin Buttimore, Max Cant, Sarah Chakera, et al. Scaling instructable agents across many simulated worlds. arXiv preprint arXiv:2404.10179, 2024.   \n[35] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[36] Rui Shao, Xiangyuan Lan, Jiawei Li, and Pong C Yuen. Multi-adversarial discriminative deep domain generalization for face presentation attack detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10023\u201310031, 2019.   \n[37] Rui Shao, Tianxing Wu, and Ziwei Liu. Detecting and grounding multi-modal media manipulation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6904\u20136913, 2023.   \n[38] Rui Shao, Tianxing Wu, Jianlong Wu, Liqiang Nie, and Ziwei Liu. Detecting and grounding multi-modal media manipulation and beyond. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024.   \n[39] Leyang Shen, Gongwei Chen, Rui Shao, Weili Guan, and Liqiang Nie. Mome: Mixture of multimodal experts for generalist multimodal large language models. In NeurIPS, 2024.   \n[40] Shan H Siddiqi, Konrad P Kording, Josef Parvizi, and Michael D Fox. Causal mapping of human brain function. Nature reviews neuroscience, pages 361\u2013375, 2022.   \n[41] JF Stein. Role of the cerebellum in the visual guidance of movement. Nature, pages 217\u2013221, 1986.   \n[42] Tim Stuart, Andrew Butler, Paul Hoffman, Christoph Hafemeister, Efthymia Papalexi, William M Mauck, Yuhan Hao, Marlon Stoeckius, Peter Smibert, and Rahul Satija. Comprehensive integration of single-cell data. cell, 177(7):1888\u20131902, 2019.   \n[43] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, et al. Generative multimodal models are in-context learners. arXiv preprint arXiv:2312.13286, 2023.   \n[44] Weihao Tan, Ziluo Ding, Wentao Zhang, Boyu Li, Bohan Zhou, Junpeng Yue, Haochong Xia, Jiechuan Jiang, Longtao Zheng, Xinrun Xu, et al. Towards general computer control: A multimodal agent for red dead redemption ii as a case study. arXiv preprint arXiv:2403.03186, 2024.   \n[45] Zhengwei Tao, Ting-En Lin, Xiancai Chen, Hangyu Li, Yuchuan Wu, Yongbin Li, Zhi Jin, Fei Huang, Dacheng Tao, and Jingren Zhou. A survey on self-evolution of large language models. arXiv preprint arXiv:2404.14387, 2024.   \n[46] Deniz Vatansever, Jonathan Smallwood, and Elizabeth Jefferies. Varying demands for cognitive control reveals shared neural processes supporting semantic and episodic memory retrieval. Nature communications, 12(1):2134, 2021.   \n[47] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023.   \n[48] Mengru Wang, Ningyu Zhang, Ziwen Xu, Zekun Xi, Shumin Deng, Yunzhi Yao, Qishen Zhang, Linyi Yang, Jindong Wang, and Huajun Chen. Detoxifying large language models via knowledge editing. arXiv preprint arXiv:2403.14472, 2024.   \n[49] Shihao Wang, Zhiding Yu, Xiaohui Jiang, Shiyi Lan, Min Shi, Nadine Chang, Jan Kautz, Ying Li, and Jose M Alvarez. Omnidrive: A holistic llm-agent framework for autonomous driving with 3d perception, reasoning and planning. arXiv preprint arXiv:2405.01533, 2024.   \n[50] Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. arXiv preprint arXiv:2302.01560, 2023.   \n[51] Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. arXiv preprint arXiv:2302.01560, 2023.   \n[52] Zihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jinbing Hou, Bowei Zhang, Haowei Lin, Zhaofeng He, Zilong Zheng, Yaodong Yang, et al. Jarvis-1: Open-world multi-task agents with memory-augmented multimodal language models. arXiv preprint arXiv:2311.05997, 2023.   \n[53] Yuetian Weng, Mingfei Han, Haoyu He, Xiaojun Chang, and Bohan Zhuang. Longvlm: Efficient long video understanding via large language models. arXiv preprint arXiv:2404.03384, 2024.   \n[54] Xu Yang, Yongliang Wu, Mingzhuo Yang, Haokun Chen, and Xin Geng. Exploring diverse in-context configurations for image captioning. Advances in Neural Information Processing Systems, 36, 2024.   \n[55] Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. Appagent: Multimodal agents as smartphone users. arXiv preprint arXiv:2312.13771, 2023.   \n[56] Qilang Ye, Zitong Yu, Rui Shao, Xinyu Xie, Philip Torr, and Xiaochun Cao. Cat: Enhancing multimodal large language model to answer questions in dynamic audio-visual scenarios. In ECCV, 2024.   \n[57] Haoqi Yuan, Chi Zhang, Hongcheng Wang, Feiyang Xie, Penglin Cai, Hao Dong, and Zongqing Lu. Skill reinforcement learning and planning for open-world long-horizon tasks. arXiv preprint arXiv:2303.16563, 2023.   \n[58] Haoyu Zhang, Meng Liu, Zixin Liu, Xuemeng Song, Yaowei Wang, and Liqiang Nie. Multifactor adaptive vision selection for egocentric video question answering. In Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 59310\u201359328. PMLR, 2024.   \n[59] Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong, and Ji-Rong Wen. A survey on the memory mechanism of large language model based agents. arXiv preprint arXiv:2404.13501, 2024.   \n[60] Enshen Zhou, Yiran Qin, Zhenfei Yin, Yuzhou Huang, Ruimao Zhang, Lu Sheng, Yu Qiao, and Jing Shao. Minedreamer: Learning to follow instructions via chain-of-imagination for simulated-world control. arXiv preprint arXiv:2403.12037, 2024.   \n[61] Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, et al. Ghost in the minecraft: Generally capable agents for open-world enviroments via large language models with text-based knowledge and memory. arXiv preprint arXiv:2305.17144, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Broader Impact ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "With the increasing capability level of Multimodal Large Language Models (MLLM) comes many potential benefits and also risks. On the positive side, we anticipate that the techniques that used to create Optimus-1 could be applied to the creation of helpful agents in robotics, video games, and the web. This plug-and-play architecture that we have created can be quickly adapted to different MLLMs, and the proposed methods also provide a viable solution for other application areas in the agent domain. However, on the negative side, it is imperative to acknowledge the inherent stochastic nature of MLLMs in text generation. If not addressed carefully, this could lead to devastating consequences for society. Prior to deploying MLLMs in conjunction with the Hybrid Multimodal Memory methodology, a comprehensive assessment of their potential risks must be undertaken. We hope that while the stakes are low, works such as ours can improve access to safety research on instruction-following models in multimodal agents domains. ", "page_idx": 14}, {"type": "text", "text": "B Minecraft ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Minecraft is an extremely popular sandbox video game developed by Mojang Studios 3. It allows players to explore a blockly, procedurally generated 3D world with infinite terrain, discover and extract raw materials, craft tools and items, and build structures or earthworks (shown in Figure 6). Minecraft is a valuable and representative environment for evaluating long-horizon tasks, offering greater diversity and complexity compared to other environments. Unlike web/app navigation [55] and embodied manipulation [16], Minecraft is an open world with a complex and dynamic environment (79 biomes, including ocean, plains, forest, desert, etc.). To complete long-horizon tasks, agents must achieve multiple sub-goals (e.g., 15 sub-goals to craft a diamond sword), making the construction of a Minecraft agent quite challenging. Many studies [47, 33, 52] have chosen Minecraft as the environment for validating performance on long-horizon tasks. Extensive experimental results in the paper show that Optimus-1 outperforms all baselines. Therefore, we chose Minecraft as open-world environment to evaluate the ability of agents to perform long-horizon tasks. ", "page_idx": 14}, {"type": "text", "text": "B.1 Basic Rules ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Biomes. The Minecraft world is divided into different areas called \u201cbiomes\u201d. Different biomes contain different blocks and plants and change how the land is shaped. There are 79 biomes in Minecraft 1.16.5, including ocean, plains, forest, desert, etc. Diverse environments have high requirements for the generalization of agents. ", "page_idx": 14}, {"type": "text", "text": "Time. Time passes within this world, and a game day lasts for 20 real-world minutes. Nighttime is much more dangerous than daytime: the game starts at dawn, and agents have 10 minutes of game time before nightfall. Hostile or neutral mobs spawn when night falls, and most of these mobs are dangerous, trying to attack agents. How to survive in such a dangerous world is an open problem for Minecraft agents research. ", "page_idx": 14}, {"type": "text", "text": "Item. In Minecraft 1.16.5, there are 975 items can be obtained, such as wooden pickaxe $\\barwedge$ , iron sword $\\mathcal{A}$ . Item can be obtained by crafting or destroying blocks or attacking entities. For example, agent can attack cows $\\textcircled{49}$ to obtain leather $\\approx$ and beef . Agent also can use 1 stick $\\diagup$ and 2 diamonds $\\circleddash$ to craft diamond sword $\\mathcal{A}$ . ", "page_idx": 14}, {"type": "text", "text": "Gameplay progress. Progression primarily involves discovering and utilizing various materials and resources, each of which unlocks new capabilities and options. For instance, crafting a wooden pickaxe $\\bar{\\curvearrowright}$ enables the player to mine stone $\\circledcirc$ , which can be used to create a stone pickaxe $\\bar{\\bf\\Phi}$ and a furnace $\\otimes$ ; these, in turn, allow for the mining and smelting of iron ore . Subsequently, an iron pickaxe $\\hat{\\rho}$ permits the extraction of diamonds $\\circleddash$ , and a diamond pickaxe $\\bar{\\bf\\Phi}$ can mine virtually any block in the game. Similarly, cultivating different crops allows for the breeding of various animals, each providing distinct resources beyond mere sustenance. Enemy drops also have specific applications, with some being more beneficial than others. By integrating resources from mining, farming, and breeding, players can enchant their equipment. The collection and crafting of materials also facilitate construction, enabling players to build diverse structures. Beyond practical considerations such as secure bases and farms, the creative aspect of building personalized structures constitutes a significant part of the Minecraft experience. ", "page_idx": 14}, {"type": "image", "img_path": "XXOMCwZ6by/tmp/c3770f8e72f352b645a2384a000b1e66f71594e0b1a110968c2f1c2250e8d95a.jpg", "img_caption": ["Figure 6: The screenshots in Minecraft. (a). The world has different complex terrains, including plains, river, forest and mine. (b). The agent can use crafting table to craft tools and items with recipes. (c). The agent can use the furnace to smelt ore to obtain precious ingot. (d). The agent can grow wheat near the river. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Freedom. In Minecraft, player can do anything they can imagine. Player can craft tools, smelt ore, brew potions, trade with villagers and wandering traders, attack mobs, grow crops, raise animals in captivity, etc. Player even can use redstone $\\Lsh$ to build a computer. This is a world of freedom and infinite possibilities. ", "page_idx": 15}, {"type": "text", "text": "More Challenge than Diamond . Progression beyond the Overworld is fairly limited: Eventually, you can build a nether portal to reach the Nether, where you can get materials for more complex crafting, the resources to brew potions, and the top tier of tools and armor. The Nether materials also let you reach the End dimension, where you must defeat the Ender Dragon to unlock the outer End Islands, where you can get an elytra that lets you fly, and shulker boxes for more storage. ", "page_idx": 15}, {"type": "text", "text": "B.2 Observation and Action Spaces ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Observation. Our observation space is completely consistent with human players. The agent only receives an RGB image with dimensions of $640\\times360$ during the gameplay process, including the hotbar, health indicators, food saturation, and animations of the player\u2019s hands. It is worth helping the agent see more clearly in extremely dark environments, we have added a night vision effect for the agent, which increases the brightness of the environment during the night. ", "page_idx": 15}, {"type": "text", "text": "Action Spaces. Our action space is almost similar to human players, except for craft and smelt actions. It consists of two parts: the mouse and the keyboard. The keypresses are responsible for controlling the movement of agents, such as jumping, forward, back, etc. The mouse movements are responsible for controlling the perspective of agents and the cursor movements when the GUI is opened. The left and right buttons of the mouse are responsible for attacking and using or placing items. In Minecraft, precise mouse movements are important when completing complex tasks that need open inventory or crafting table. In order to achieve both the same action space with MineDojo [7], we abstract the craft and the smelt action into action space. The detailed action space is described in Table 4. ", "page_idx": 15}, {"type": "table", "img_path": "XXOMCwZ6by/tmp/7c7065bde4e21a406555547bbf124af3bcdb9acca8b7c082309d57d3a8bc7f3b.jpg", "table_caption": ["Table 4: Our action space. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "B.3 Long-horizon Tasks ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Long-horizon Tasks are complex tasks that require world knowledge to solve and consist of multiple indispensable subtask sequences. In Minecraft, technology has six levels, including wood $\\oslash$ , stone $\\otimes$ , iron $\\vartriangle$ , golden $\\Rightarrow$ , diamond $\\circleddash$ , and netherite $\\Leftrightarrow$ . Wooden tools can mine stone-level blocks, but can\u2019t mine iron-level and upper-level blocks. Stone tools can mine iron-level blocks, but can\u2019t mine diamond-level and upper-level blocks. Iron-level tools can mine diamond-level blocks, but can\u2019t mine netherite-level blocks. Diamond-level tools can mine any level blocks. ", "page_idx": 16}, {"type": "text", "text": "For example, the agent now wants to complete the task \u201cCraft iron sword $\\mathcal{A}^{\\bullet}$ . The agent needs to craft wood-level tools to mine stone $\\textcircled{+}$ , and craft stone-level tools to mine iron ore $\\otimes$ . In order to craft tools, the agent needs a crafting table $\\textcircled{+1}$ . To smelt iron ore $\\textcircled{-1}$ into iron ingot $\\Rightarrow$ , the agent needs a furnace . Moreover, craft crafting table needs 4 planks, and craft furnace needs 8 cobblestone. In summary, the agent needs to obtain many raw materials, wood-level and stone-level tools, 1 crafting table, 1 furnace, and most importantly, 2 iron ingots. The process of this task is shown in Figure 7. ", "page_idx": 16}, {"type": "text", "text": "C Theory ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we briefly introduce the relevant theory of cognitive science. For more details, please refer to the original articles. ", "page_idx": 16}, {"type": "text", "text": "Our ability to understand and predict the world around us depends on our long-term memory stores, which have historically been divided into two distinct systems [28, 42, 46]. The semantic memory system provides a conceptual framework for describing the similar meanings of words and objects as they are encountered in different contexts (e.g., a bee is a flying insect with yellow and black stripes that produces honey), whereas the episodic memory system records our personal experiences characterized by the co-occurrence of words and objects at different times and places (e.g., being stung by a bee while eating honey at a picnic last weekend). These information stores and the interactions between them play a crucial role in guiding our behaviour and giving us the flexibility to adapt to the various demands of our environment. ", "page_idx": 16}, {"type": "text", "text": "In this paper, inspired by the above theory, we divide the agent memory module into two parts: knowledge and experience. Based on this, we propose Hierarchical Directed Knowledge Graph and Abstracted Multimodal Experience Pool to enable the agent to acquire, store, and utilize knowledge and experience during the execution of tasks. Extensive experimental results demonstrate the effectiveness of the proposed methodology ", "page_idx": 16}, {"type": "image", "img_path": "XXOMCwZ6by/tmp/b04b46772b811bfaea1e7b410e786574bb513b733d40823f32c2aa375df5409f.jpg", "img_caption": ["Figure 7: Processing of task \"Craft 1 iron sword\". Optimus-1 needs thousands of steps to complete this task. To craft and smelt precisely, the mouse movements action can\u2019t have any error. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "D Benchmark Suite ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "D.1 Benchmark ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We constructed a benchmark of 67 tasks to evaluate Optimus-1\u2019s ability to complete long-horizon tasks in Minecraft. According to recommended categories in Minecraft, we have classified these tasks into 7 groups: Wood , Stone $\\otimes$ , Iron $\\Rightarrow$ , Gold $\\bigcirc.$ , Diamond $\\circleddash$ , Redstone $\\Lsh$ , Armor . The statistics for benchmark are shown in Table 5. Due to the varying complexity of these tasks, we adopt different maximum gameplay steps (Max. Steps) for each task. The maximum steps are determined by the average steps that human players need to complete the task. Due to the randomness of Minecraft, the world and initial spawn point of the agent could vary a lot. In our benchmark setting, We initialize the agent with an empty inventory, which makes it necessary for the agent to complete a series of sub-goals (mining materials, crafting tools) in order to perform any tasks. This makes every task challenging, even for human players. ", "page_idx": 17}, {"type": "text", "text": "Note that Diamonds are a very rare item that only spawns in levels 2 to 16 and have a $0.0846\\%$ chance of spawning in Minecraft 1.16.5. Diamonds are usually found near level 9, or in man-made or natural mines no higher than level 16. In order to reduce the huge impact that diamond generation probability has on agent\u2019s likelihood of completing a task, we have adjusted the diamond generation probability to $20\\%$ , spawns in levels 2 to 16. This setting applies to human players as well. ", "page_idx": 17}, {"type": "table", "img_path": "XXOMCwZ6by/tmp/32ae8552d9017164685a88167e7ac02abfaf2c71fd54ed2a776f722afdc858dc.jpg", "table_caption": ["Table 5: Setting of 7 groups encompassing 67 Minecraft long-horizon tasks. "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "XXOMCwZ6by/tmp/c1ed3023cc2d5d2fc2ce678e6defd2d6dc9b9889f7333976bb67fd2960f042fe.jpg", "table_caption": ["Table 6: We evaluate Optimus-1 on these tasks in ablation study which are the subset of our benchmark. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "In the ablation study, we select the subset of our benchmark as the test set (shown in Table 6). The environment setting is the same as the benchmark. ", "page_idx": 18}, {"type": "text", "text": "D.2 Baselines ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Existing Baseline. On the one hand, we employ GPT-3.5 and GPT-4V as baseline, which are evaluated without integrating hybrid multimodal memory modules. During the planning phase, they generate a plan for the action controller based on task prompt (and observation). During the reflection phase, they generate reflection results in a zero-shot manner. On the other hand, we compare existing SOTA Agents [51, 52] in Minecraft. ", "page_idx": 18}, {"type": "text", "text": "Human-level Baseline. To better demonstrate agent\u2019s performance level in Minecraft, we hired 10 volunteers to play the game as a human-level baseline. The volunteers played the game with the same environment and settings, and every volunteer asked to perform the each task on the benchmark 10 times. Ultimately, we used the average scores of 10 volunteers as the human-level baseline. The results of the human-level baseline are shown in Table 1. To ensure the validity of the experiment, we ensured that each volunteer had at least 20 hours of Minecraft gameplay before conducting the experiment. For each volunteer, we pay $\\mathbb{S}25$ as reward. ", "page_idx": 18}, {"type": "table", "img_path": "XXOMCwZ6by/tmp/fdcc8e26bb5ea690ae7e80d64ec2812668fcacc63a32e47446128a68938b7523.jpg", "table_caption": ["Table 7: Statistics for various Minecraft agents. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "D.3 Minecraft Agents ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we summarise the differences between existing Minecraft agents. As shown in the Table 7, earlier work [1, 7, 25, 3] constructed Transformer-based policy network as agent. Recent work [47, 51, 52, 33] introduces the Multimodal Large Language Model, which empowers the agent to complete long-horizon tasks by exploiting the powerful language comprehension and planning capabilities of LLM. ", "page_idx": 19}, {"type": "text", "text": "In the Mineflayer and Minedojo environments, agents [7, 47, 51, 33] can accomplish sub-goals by calling APIs (in the form of codes), which is a different behavioral pattern from humans. In MineRL [11], agents [1, 25, 3, 52] must generate low-level actions to perform tasks, which is more challenging to accomplish long-horizon tasks. ", "page_idx": 19}, {"type": "text", "text": "Moreover, existing agents lack knowledge and experience, and their performance in Minecraft is still vastly gapped from the human level. In this paper, we introduce Hybrid Multimodal Memory, which empowers Optimus-1 with hierarchical knowledge and multimodal experience. This makes Optimus-1 significantly outperform all existing agents on challenging long-horizon tasks benchmark, and exhibits near human-level performance on many tasks. ", "page_idx": 19}, {"type": "text", "text": "E Implementation Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "E.1 Hybrid Multimodal Memory ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "E.1.1 Abstracted Multimodal Experience Pool ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Relevant studies [5, 29, 17, 15] have demonstrated the importance of memory for agents to complete long-horizon tasks. To implement the memory mechanism, Minedojo [7] and Voyager [47] only considered unimodal storage of historical information. Jarvis-1 [52] considered a multimodal memory mechanism to store task planning and visual information as experience, but it stores all historical information without summarisation. This approach stores all visual images, which poses a huge challenge in storage size and retrieval efficiency. To solve the problem, we propose the Abstracted Multimodal Experience Pool structure, which summarizes all historical information during the agent\u2019s execution of the task, which maintains the integrity of long sequential information and greatly improves the storage and retrieval efficiency of the experience. ", "page_idx": 19}, {"type": "text", "text": "As shown in Figure 2, we first input the visual image stream to the video buffer, which filters the image stream at a fixed frequency. It makes the length of the image stream substantially shorter. Empirically, we set the frequency of flitering to 1 second/frame, meaning that the video buffer takes one frame per second from the original image stream to compose the filtered image stream. We found that above this frequency makes the visual information redundant (too much similarity between images), and below this frequency does not preserve enough complete visual information. ", "page_idx": 19}, {"type": "text", "text": "Then, we feed the filtered frames into an image buffer with a window size of 16. We dynamically compute the similarity between images in the image buffer, when a new image comes in, we compute the similarity between the new image and the most recent image, and then we remove the image with the highest similarity in order to keep the image buffer\u2019s window size to 16. ", "page_idx": 19}, {"type": "text", "text": "Subsequently, we introduce MineCLIP [7], a pre-trained model of video-text alignment with a structure similar to CLIP [35], as our visual summariser. For a given sub-goal, it calculates the correlation between the visual content within the current memory bank and the sub-goal, and when this correlation exceeds a pre-set threshold, the frames within the memory bank are saved as the visual memories corresponding to that sub-goal. Finally, we store the visual memories with the sub goal\u2019s textual description into the Abstracted Multimodal Experience Pool. In addition, we incorporate the environment information, agent initial state, plan from Knowledge-Guided Planner, etc. into the experience memory of the given task. In this way, we consider the history information of each sub-goal and summaries and summarise it to finally compose the multimodal experience of the given task. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "Note that we also store these visual memories as failure cases when the feedback from the reflection phase is REPLAN. Therefore, when Optimus-1 executes a long-horizon task, it can retrieve past successes and failures as references and update memory after the task is finished. In the reflection phase, Optimus-1 retrieve the most relevant cases from Abstracted Multimodal Experience Pool, which contains the three scenarios COMPLETE, CONTINUE, and REPLAN, to help the agent better assess which state the current situation belongs to. This approach of considering both successful and failed cases for in-context learning is inspired by related research [8, 32], and its effectiveness is validated in Section 3.3. ", "page_idx": 20}, {"type": "text", "text": "E.1.2 Hierarchical Directed Knowledge Graph ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "As shown in the Figure 2, crafting a diamond sword $\\mathcal{A}$ requires two diamonds $\\circleddash$ and a wooden stick $\\diagup$ , while mining diamonds requires an iron pickaxe $\\hat{\\nearrowright}$ , which in turn requires additional raw materials and crafting steps. We transform this mine and craft knowledge into a graph structure, where the nodes of the graph are objects, and the nodes point to objects that can be crafted or completed by that object. With directed graph, we show that connections between objects are established, and that this knowledge can be stored and updated efficiently. For a given object, we only need to retrieve the corresponding node to extract the corresponding subgraph from the knowledge graph. Then by topological sorting, we can get the antecedents and required materials for the object, and this information is provided to the Knowledge-Guided Planner as a way to generate a more reasonable sequence of sub-goals. With Hierarchical Directed Knowledge Graph, we can significantly enhance the world knowledge of the agent in a train-free manner, as shown in the experimental results in Section 3.3. ", "page_idx": 20}, {"type": "text", "text": "Our HDKG can be efficiently updated and expanded. When adding new nodes, the HDKG can be updated by simply merging the nodes and relationships into the graph. This method involves local linear modifications to the graph rather than altering the entire graph, making the process efficient and time-saving. For example, when M new nodes and N edges are added, the HDKG can be updated with $\\mathbf{M}{+}\\mathbf{N}$ times of operations. Moreover, an HDKG containing 851 objects (nodes) requires less than 1 MB of memory. Thus, the HDKG can be efficiently updated and maintained. ", "page_idx": 20}, {"type": "text", "text": "E.2 Hybrid Multimodal Memory Driven Optimus-1 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In order to implement the proposed Hybrid Multimodal Memory and to progressively increase the capacity of Optimus-1 in a self-evolution manner, we propose a non-parametric learning method named \u201cfree exploration-teacher guidance\u201d. ", "page_idx": 20}, {"type": "text", "text": "In the free exploration phase, we randomly initialize the environment, materials, and tasks. For the task \u201ccraft a wooden pickaxe\u201d, we provide initial materials (three planks, two sticks), and then Optimus-1 (only the action controller activated) attempts to complete the task. If the environment feedback indicates the task is successful, the knowledge {3 planks, 2 sticks $\\rightarrow$ wooden pickaxe} is added to the HDKG. Note that we randomly initialize materials and their quantities, which means that the task may not always succeed. As a result, each free exploration may not acquire the corresponding knowledge, but it can record the relevant experience (whether successful or fail). In the free exploration phase, Optimus-1 learns simple atomic operations, such as crafting sticks in the Wooden Group and mining diamonds in the Diamond Group. ", "page_idx": 20}, {"type": "text", "text": "In the teacher guidance phase, Optimus-1 need to learn a small number of long-horizon tasks based on extra knowledge. For example, during the free exploration phase, Optimus-1 mastered crafting stick $\\nearrow$ and mining diamond $\\circleddash$ , but did not know that \u201ca diamond sword $\\mathcal{A}$ is obtained by a stick $\\nearrow$ and two diamonds $\\circleddash\\infty$ . So we provide some task plans, which will serve as extra knowledge to guide ", "page_idx": 20}, {"type": "text", "text": "Optimus-1 to complete the task of \u201ccraft diamond sword\u201d. We built the following automated process to get the task plan needed for \u201cfree exploration\u201d: ", "page_idx": 21}, {"type": "text", "text": "\u2022 We randomly select 5 tasks for each Group (7 groups in total) that are not included in the benchmark.   \n\u2022 For each selected task, we use a script to automatically obtain the crafting relationships from the Minecraft Wiki 4. Taking the task \u201ccraft a wooden sword\u201d as an example, we use the script to automatically obtain the crafting relationships: 1 wooden stick, 2 planks, 1 crafting table $\\rightarrow1$ wooden sword, $1\\;\\mathrm{log}\\rightarrow4$ planks, 2 planks $\\to4$ sticks, 4 planks $\\rightarrow1$ crafting table.   \n\u2022 These relationships are converted into a directed acyclic graph through an automated script. By performing a topological sort, the graph can be converted into tuples of materials and their quantities: (wooden sword, 1), (crafting table, 1), (wooden stick, 1) (planks, 8), (log, 2).   \n\u2022 We prompt GPT-4 to construct a plan in order from basic materials to advanced materials.   \n\u2022 Finally, we get the plan: 1. Get two logs 2. Craft eight planks 3. Craft a crafting table 4. Craft a wooden stick 5. Craft a wooden sword ", "page_idx": 21}, {"type": "text", "text": "During the teacher guidance phase, Optimus-1\u2019s memory is further expanded and it gains the experience of executing complete long-horizon tasks. Teacher guidance phase allows Optimus-1 to acquire advanced knowledge and learn multimodal experiences through complete long-horizon tasks. ", "page_idx": 21}, {"type": "text", "text": "E.3 Backbone of Optimus-1 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Optimus-1 consists of Knowledge-Guided Planner, Experience-Driven Reflector, and Action Controller. In this paper, we employ OpenAI\u2019s GPT-4V (gpt-4-turbo) 5 as Knowledge-Guided Planner and Experience-Driven Reflector, and STEVE-1 [25] as Action Controller. We also employ opensource models like Deepseek-VL [26] and InternLM-XComposer2-VL [6] as Knowledge-Guided Planner and Experience-Driven Reflector. ", "page_idx": 21}, {"type": "text", "text": "All experiments were implemented on $4\\mathbf{x}$ NVIDIA A100 GPUs. We employ multiple Optimus-1 to perform different tasks at the same time, and this parallelized inference greatly improves our experimental efficiency. In the free exploration and teacher guidance phases, there is no need to access OpenAI\u2019s API, and the learning process takes approximately 16 hours on 4x A100 80G GPUs. During the inference phase, it takes about 20 hours on 4x A100 80G GPUs. ", "page_idx": 21}, {"type": "text", "text": "Throughout the experiment, we spent about $85,000$ to access the GPT-4V API. However, we also offer more cost-effective solutions. As shown in Figure 5, if we employ Deepseek-VL [26] or InternLM-XComposer2-VL [6] as Optimus-1\u2019s backbone, we can get comparable performance with low-cost! ", "page_idx": 21}, {"type": "text", "text": "E.4 Prompt for Optimus-1 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We show the prompt templates for Experience-Driven Reflector and Action Controller as follows. ", "page_idx": 21}, {"type": "text", "text": "System: You are a MineCraft game expert and you can guide agents to complete complex tasks.   \nUser: For a given game screen and task, you need to complete <goal inference> and <visual inference>.   \n<goal inference>: According to the task, you need to infer the weapons, equipment, or materials required to complete the task.   \n<visual inference>: According to the game screen, you need to infer the following aspects: health bar, food bar, hotbar, environment.   \nI will give you an example as follow:   \n[Example]   \n<task>: craft a stone sword.   \n<goal inference>: stone sword   \n<visual inference>   \nhealth bar: full   \nfood bar: full hotbar: empty   \nenvironment: forest   \nHere is a game screen and task, you MUST output in example format. <task>: {task}.   \n<game screen>: {image} ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "Assistant: ", "page_idx": 22}, {"type": "text", "text": "==== ========   \nUser: Now you need to make a plan with the help of <visual info> and <craft graph>.   \n<visual info>: Consists of the following aspects: health bar, food bar, hotbar, environment. Based on the current visual information, you need to consider whether prequel steps needed to ensure that agent can complete the task.   \n<craft graph>: a top-down list of all the tools and materials needed to complete the task.   \nI will give you an example of planning under specific visual conditions as follow:   \n[Example]   \n{example}   \nHere is a game screen and task, you MUST output in example format. Remember <task planning> MUST output in example format. <task>: {task}   \n<game screen>: {image}   \n<craft graph>: {graph}   \nAssistant: ", "page_idx": 22}, {"type": "text", "text": "Listing 1: Prompt for Knowledge-Guided Planner. ", "page_idx": 22}, {"type": "text", "text": "System: You are a MineCraft game expert and you can guide agents to complete complex tasks. Agent is executing the task: {task}.   \nGiven two images about agent\u2019s state before executing the task and its current state, you should first detection the environment (forest, cave, ocean, etc.,) in which the agent is located, then determine whether the agent\u2019s current situation is done, continue, or replan.   \n<done>: Comparing the image before the task was performed, the current image reveals that the task is complete.   \n<continue>: Current image reveals that the task is NOT complete, but agent is in good state ( good health, not hungry) with high likelihood to complete task.   \n<replan>: Current image reveals that the task is NOT complete, and agent is in bad state (bad health, or hungry) or situation (in danger, or in trouble), need for replanning. For replan, you need to further determine whether the agent\u2019s predicament is \"drop_down\" or \"in_water\". \"drop_down\" means that the agent has fallen into a cave or is trapped in a mountain or river, while \"in_water\" means that the agent is in the ocean and needs to return to land immediately.   \nUser: I\u2019ll give you some examples to illustrate the different situations. Each example consists of two images, where the first image is the state of the agent before performing the task and the second image is the current state of the agent. ", "page_idx": 22}, {"type": "text", "text": "[Examples] <done>: {image1},{image2} <continue>: {image1},{image2} <replan>: {image1},{image2} ", "page_idx": 22}, {"type": "text", "text": "Now given two images about agent\u2019s state before executing the task and its current state, you MUST and ONLY output in following format:   \nEnviroment: <environment>   \nSituation: <situation>   \n(if situation is replan) Predicament: <predicament> ", "page_idx": 22}, {"type": "text", "text": "Listing 2: Prompt for Experience-Driven Reflector. ", "page_idx": 22}, {"type": "text", "text": "F Additional Experimental Results ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "F.1 Full Results on Our Benchmark ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We list the results of each task on the benchmark below, with details including task name, sub-goal numbers, success rate (SR), average number of steps (AS), average time (AT), and eval times. All tasks are evaluated in Minecraft 1.16.5 Survival Mode. Note that each time Optimus-1 performs a task, we initial it with an empty initial inventory and a random start point. This makes it challenging for Optimus-1 to perform each task. ", "page_idx": 22}, {"type": "table", "img_path": "XXOMCwZ6by/tmp/7848ff12cb456e1fa582c6a3c5c3bc9411bb8ea12544efb430c38f0c789ed6f7.jpg", "table_caption": ["Table 8: The results of Optimus-1 on various tasks in the Wood group. SR, AS, AT denote success rate, average number of steps, and average time (seconds), respectively. "], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "XXOMCwZ6by/tmp/d73c0aa629b3d235a70c130fcda69330c5979e7997466c1c3c4dc8ea2586acf7.jpg", "table_caption": ["Table 9: The results of Optimus-1 on various tasks in the Stone group. SR, AS, AT denote success rate, average number of steps, and average time (seconds), respectively. "], "table_footnote": ["Moreover, in MineRL [11] environment, \u2019steps\u2019 refers to the number of interactions between the agent and the environment, occurring at a frequency of 20 times per second. For example, if an agent takes 2 seconds to complete the task \u201cchop a tree\u201d, it interacts with the environment 40 times, resulting in a recorded steps number of 40. Experimental results show that Optimus-1\u2019s average task completion step (AS) is significantly lower than other baselines. "], "page_idx": 23}, {"type": "text", "text": "F.2 Results on Other Benchmark ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "For a more comprehensive comparison with current Minecraft Agents, we also report Optimus-1\u2019s performances on the benchmark used by Voyager [47], MP5 [33], and DEPS [51] below. Due to the different environments and settings, agents perform tasks with varying degrees of difficulty. For example, Optimus-1 requires low-level action to perform any task in MineRL [11], and we initialize its inventory to be empty. While Voyager [47] performs tasks in Mineflayer 6 environment only through encapsulated code, MP5 [33] performs tasks in MineDOJO [7] environment only needs a specific control signal to craft tools, no low-level actions (mouse movement and click) are needed. ", "page_idx": 23}, {"type": "text", "text": "Optimus-1\u2019s success rate in completing tasks with these baselines is shown in the Table 15 and Table 16, and Optimus-1\u2019s efficiency in unlocking the tech tree in Minecraft is shown in the Figure 8. These results reveal that Optimus-1 outperforms a variety of powerful baseline agents, even in challenging environmental settings! ", "page_idx": 23}, {"type": "table", "img_path": "XXOMCwZ6by/tmp/014440a4b1a3f8f5fa0586e36387f7162c89c67dc62aaec433836986f4820d91.jpg", "table_caption": ["Table 10: The results of Optimus-1 on various tasks in the Iron group. SR, AS, AT denote success rate, average number of steps, and average time (seconds), respectively. "], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "XXOMCwZ6by/tmp/5552707ceccce57a11f9cca863f5a1e4c168248b8fe8dc4aebc50b0905a18f01.jpg", "table_caption": ["Table 11: The results of Optimus-1 on various tasks in the Gold group. SR, AS, AT denote success rate, average number of steps, and average time (seconds), respectively. "], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "XXOMCwZ6by/tmp/be6a7b82b816608bc0e9ae0596c3e6d89019bd2868fdc1c508d594507183ace4.jpg", "table_caption": ["Table 12: The results of Optimus-1 on various tasks in the Diamond group. SR, AS, AT denote success rate, average number of steps, and average time (seconds), respectively. "], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "XXOMCwZ6by/tmp/f7dd3e55d0528386732771496dd1ecfb5dc22d096bab446b8f3b097d06d6ac2b.jpg", "table_caption": ["Table 13: The results of Optimus-1 on various tasks in the Redstone group. SR, AS, AT denote success rate, average number of steps, and average time (seconds), respectively. "], "table_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "XXOMCwZ6by/tmp/104e3d4e48c94d756e2aa7f64bb196694daebe5159dde568414306d0f6a413ba.jpg", "table_caption": ["Table 14: The results of Optimus-1 on various tasks in the Armor group. SR, AS, AT denote success rate, average number of steps, and average time (seconds), respectively. "], "table_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "XXOMCwZ6by/tmp/852aab4122c2e66005ae8fbec1988deb34c05c924d44f572a3e22ef0654c1590.jpg", "img_caption": ["Figure 8: An illustration of Optimus-1 unlocking the tech tree in Minecraft. "], "img_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "XXOMCwZ6by/tmp/7983341fc40228027f5ca8f0fe03229fb106e1544adce85a566ca08339fd057b.jpg", "table_caption": ["Table 15: Result on Process-Dependent Tasks compared with MP5 [33]. SR, AS, AT denote success rate, average number of steps, and average time (seconds), respectively. "], "table_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "XXOMCwZ6by/tmp/2469aae42216b4faddf11206baf50adfa57b19b85f6f456e7b0b5acf10df21e7.jpg", "table_caption": ["Table 16: Results (success rate) on 8 META TASK groups compared with DEPS [50]. "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "G Case Study ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "This section introduces several cases to comprehensively demonstrate Optimus-1\u2019s capabilities. ", "page_idx": 28}, {"type": "text", "text": "Figures 9, 10, and 11 demonstrate the superiority of our reflection mechanism, which dynamically adjusts the plan based on the current game progress. ", "page_idx": 28}, {"type": "text", "text": "\u2022 Figure 9 illustrates Optimus-1\u2019s replanning ability. When Optimus-1 realizes it cannot complete a task (such as a craft failure shown in the figure), it will replan the current task and continue execution.   \n\u2022 Figures 10 and 11 showcase Optimus-1\u2019s ability to make judgments based on visual signals. When Optimus-1 determines that it has completed a task (such as \u201ckill a cow $\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$ in Figure 10), it will finish the current task and move on to the next one. If Optimus-1 discovers that it has not yet completed the task and the task has not failed(as shown in Figure 11), it will continue executing the task. ", "page_idx": 28}, {"type": "text", "text": "Figures 12 and 13 illustrate the advantages of planning with knowledge. With the Hierarchical Directed Knowledge Graph, we can generate a high-quality plan in one step and dynamically adjust the plan based on current visual signals. ", "page_idx": 28}, {"type": "text", "text": "\u2022 Figure 12 demonstrates the importance of knowledge. For a long-horizon task such as \u201cMine 1 diamond $\\circledcirc$ ,\u201d Optimus-1 first generates a plan based on the Hierarchical Directed Knowledge Graph. However, this plan needs to be adjusted based on the current visual signals. For example, in this figure, Optimus-1 appears in a cave, so the primary task is not to \u201cchop a tree\u201d but to \u201cleave the cave\u201d first. Only after exiting the cave can Optimus-1 proceed with the initial plan. \u2022 Figure 13 demonstrates the high efficiency of our method. Agents like MP5 [33] and Voyager [47] use an iterative planning approach, which is very time-consuming, generating the final plan step by step. During this process, agent does not take any action. As shown in Figure 13, a zombie is gradually approaching the agent, but the agent is still iterating on its plan. Optimus-1, however, generates the plan in one step based on the Hierarchical Directed Knowledge Graph and makes reasonable plans based on the current visual signals. ", "page_idx": 28}, {"type": "image", "img_path": "XXOMCwZ6by/tmp/3d67dcc76246620d9433c5630c90ad18fe526900a34422172f805ce2ccdfb69c.jpg", "img_caption": ["Figure 9: The process of completing the task \"Craft 1 wooden pickaxe\". Optimus-1 gives wrong planning. When Optimus-1 realizes it cannot complete the task, it will replan the current task. "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "XXOMCwZ6by/tmp/90f4e3279fd5a5430bb1da3cc75de1eb77d37d04fe41d8fec48f247e1f5b1c14.jpg", "img_caption": ["Figure 10: The process of completing the task \"Find a cow and kill it\". Hierarchical Directed Knowledge Graph indicates that having a wooden sword will make the task easier to complete. Therefore, Optimus-1 first crafts a wooden sword and then proceeds to find and kill a cow. "], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "XXOMCwZ6by/tmp/bdd105da6e23a77991667b88fdf7114294080e6fd32066dd440edaf6b1c97fb4.jpg", "img_caption": ["Figure 11: The process of completing the task \"Chop tree to obtain $10\\,\\mathrm{logs^{\\prime\\prime}}$ . Hierarchical Directed Knowledge graph indicates that no tools are needed to complete this goal. After finding a tree, Optimus-1 starts chopping it down. The task requires a substantial amount of wood, so midway through, Optimus-1 performs a reflection. The task is not yet complete but is progressing smoothly, and the result of the reflection is to continue. "], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "XXOMCwZ6by/tmp/3169a7b5c5e9f997f14cce5b681af81fbdd78984aa4a288f0d288dce7a6f2077.jpg", "img_caption": [], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "Figure 12: The process of completing the task \"Mine 1 diamond $\\ @^{\\prime\\prime}$ . Mining diamonds $\\circleddash$ is a highly complex task. Diamonds can only be mined with an iron pickaxe $\\hat{\\nearrowright}$ , so an iron pickaxe $\\hat{\\nearrowright}$ must be crafted first. Crafting an iron pickaxe $\\mathbf{\\hat{\\Pi}}$ requires iron ingots $\\Rightarrow$ , which are smelted from iron ore $\\textcircled{+}$ . Mining iron ore $\\textcircled{<}$ requires a stone pickaxe $\\leftrightharpoons$ . Crafting a stone pickaxe $\\leftrightharpoons$ requires stone $\\ntrianglelefteq$ , which in turn must be mined with a wooden pickaxe $\\Finv$ . Crafting a wooden pickaxe $\\bar{\\nearrowright}$ requires wooden planks and sticks $\\nearrow$ . All these crafting processes require a crafting table $\\textcircled{+1}$ , and smelting requires a furnace $\\ntrianglelefteq$ . In this case, the agent spawns at a cave, so Optimus-1 must leave the cave to chop logs. ", "page_idx": 30}, {"type": "image", "img_path": "XXOMCwZ6by/tmp/668f70276a6ea748038c9861f7d2b77a29986163f39fa8cec4d1c8a5c2e3b011.jpg", "img_caption": ["Figure 13: In this example, a zombie is slowly approaching the agent. Agents like MP5 [33] and Voyager [47] uses an iterative planning strategy to generate the plan, which consumes a great deal of time and puts the agent in danger. While Optimus-1 directly generates a plan in one step based on the knowledge graph. Using the current visual information, it makes a plan to \"run to a sunny place,\" allowing the agent to avoid danger then begin to achieve sub-goals. "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We clearly state the claims and contributions of this paper in the abstract and introduction. ", "page_idx": 31}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We clearly state the limitations of this paper in the Section 6. ", "page_idx": 31}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 31}, {"type": "text", "text": "Answer:[NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: In this paper, we do not propose new theories or principles. ", "page_idx": 31}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We describe the proposed methods and experimental setup in detail in the Section 2 and Section 3, and the implementation details in Appendix F. ", "page_idx": 31}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We release our code and project in: https://cybertronagent.github.io/Optimus1.github.io/. ", "page_idx": 31}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Our method requires no parameter training. But we detail the benchmark and environment settings in Section 3 and Appendix E. ", "page_idx": 31}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: For each evaluation task, we ran it at least 30 times and calculated the average accuracy to minimise random errors and uncertainty factors. ", "page_idx": 31}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We report the computer resource in Appendix F.3. ", "page_idx": 32}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The research conducted in the paper complies with the NeurIPS Code of Ethics in all respects. ", "page_idx": 32}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We state the Broader Impacts of the paper in the Appendix A ", "page_idx": 32}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We describe the safeguards in place to responsibly release models with a high risk in Appendix A. ", "page_idx": 32}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We state the models used in Section 2 and comply with all licences. ", "page_idx": 32}, {"type": "text", "text": "13. New Assets ", "page_idx": 32}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 32}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: We hired volunteers to play Minecraft and presented the details in the Appendix E.2. ", "page_idx": 32}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: We hired volunteers to play Minecraft as human-level baseline. We only record the games results, and volunteers are not exposed to any risks. ", "page_idx": 32}]