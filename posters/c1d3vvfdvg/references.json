{"references": [{"fullname_first_author": "Jared Kaplan", "paper_title": "Scaling laws for neural language models", "publication_date": "2020-01-08", "reason": "This paper establishes the scaling laws that govern the performance of large language models, a foundation for understanding the resource requirements and limitations of such models, which is directly relevant to the current paper's focus on efficient scaling with Mixture-of-Experts (MoE) models."}, {"fullname_first_author": "OpenAI", "paper_title": "GPT-4 Technical Report", "publication_date": "2023-03-01", "reason": "This report details the development of GPT-4, a state-of-the-art large language model, offering insights into the architectural choices and performance benchmarks relevant to the current paper's exploration of scaling models efficiently using MoE techniques."}, {"fullname_first_author": "Machel Reid", "paper_title": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context", "publication_date": "2024-03-01", "reason": "This paper introduces Gemini 1.5, a multimodal large language model emphasizing its ability to process extensive amounts of context, directly relevant to the current work because multimodal understanding requires efficient scaling techniques, such as MoE."}, {"fullname_first_author": "Noam Shazeer", "paper_title": "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer", "publication_date": "2017-04-24", "reason": "This seminal paper introduces the Mixture-of-Experts (MoE) layer, a crucial component of the current paper's proposed method; it provides the foundational understanding of MoE's capabilities and its application in large-scale language models."}, {"fullname_first_author": "Yanqi Zhou", "paper_title": "Mixture-of-experts with expert choice routing", "publication_date": "2022-00-00", "reason": "This paper details a specific MoE routing mechanism, 'expert choice routing,' which is relevant to the current paper's discussion of routing strategies and their impact on the overall performance of MoE models."}]}