[{"type": "text", "text": "MiniCache: KV Cache Compression in Depth Dimension for Large Language Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Akide Liu1 Jing Liu1 Zizheng Pan1 Yefei He2 Gholamreza Haffari1 Bohan Zhuang1,2\u2020 ", "page_idx": 0}, {"type": "text", "text": "1ZIP Lab, Monash University, Australia   \n2ZIP Lab, Zhejiang University, China ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "A critical approach for efficiently deploying computationally demanding large language models (LLMs) is Key-Value (KV) caching. The KV cache stores key-value states of previously generated tokens, significantly reducing the need for repetitive computations and thereby lowering latency in autoregressive generation. However, the size of the KV cache grows linearly with sequence length, posing challenges for applications requiring long context input and extensive sequence generation. In this paper, we present a simple yet effective approach, called MiniCache, to compress the KV cache across layers from a novel depth perspective, significantly reducing the memory footprint for LLM inference. Our approach is based on the observation that KV cache states exhibit high similarity between the adjacent layers in the middle-to-deep portion of LLMs. To facilitate merging, we propose disentangling the states into the magnitude and direction components, interpolating the directions of the state vectors while preserving their lengths unchanged. Furthermore, we introduce a token retention strategy to keep highly distinct state pairs unmerged, thus preserving the information with minimal additional storage overhead. Our MiniCache is training-free and general, complementing existing KV cache compression strategies, such as quantization and sparsity. We conduct a comprehensive evaluation of MiniCache utilizing various models including LLaMA-2, LLaMA-3, Phi-3, Mistral, and Mixtral across multiple benchmarks, demonstrating its exceptional performance in achieving superior compression ratios and high throughput. On the ShareGPT dataset, LLaMA-2-7B with cross-layer merging achieves a compression ratio of $1.53\\times$ . Additionally, since MiniCache is orthogonal to existing quantization techniques, it can achieve a compression ratio of up to $5.02\\times$ when combined with the 4-bit quantization technique, enhancing inference throughput by approximately $5\\times$ and reducing the memory footprint by $41\\%$ compared to the FP16 full cache baseline, all while maintaining near-lossless performance. Project is available at https://minicache.vmv.re . ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large Language Models (LLMs), exemplified by the GPT series [1, 2, 3] and the LLaMA series [4, 5, 6], have emerged as pivotal innovations within the artificial general intelligence, significantly enhancing the capabilities of natural language processing. However, these models are meticulously trained using extensive computational resources [7] and massive datasets [8], which enables them to ", "page_idx": 0}, {"type": "image", "img_path": "sgVOjDqUMT/tmp/6751ff89582e83b65c4b23c231a61f2f87ac930e10431c46adaec2862e5df4b7.jpg", "img_caption": ["(a) Cross-layer KV cache similarity ", "(b) Merged layers vs EM score on GSM8K "], "img_footnote": [], "page_idx": 1}, {"type": "image", "img_path": "sgVOjDqUMT/tmp/995757a09a4fd8be52b38cd364fdac92c8caae5c3da8a1ecd7d212d7d4f261bb.jpg", "img_caption": ["(c) Comparison between MiniCache and previous methods "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Overview of our MiniCache strategy and example results: (a) shows the observation that the KV cache states between two adjacent layers are highly similar, particularly across the middle to deep layers. The x-axis uses index/2 to represent the similarities for each pair of layers. (b) compares the performance of MiniCache, and the mean baseline, which simply averages the KV caches of two layers, using the LLaMA-3-70B model [6] on the GSM8K dataset [10]. MiniCache, which begins merging from the half-layer depth, achieves near-lossless performance. (c) highlights the primary difference between MiniCache and previous approaches. MiniCache investigates the inter-layer redundancy of KV caches along the depth dimension of LLMs, an aspect overlooked by intra-layer-based methods. Here, $T$ refers to the last timestamp of pre-filling, and $T+1$ des to the first timestamp of decoding. ", "page_idx": 1}, {"type": "text", "text": "produce text that effectively mimics human writing styles and conducts complex reasoning analysis, yet raises the challenge of efficient deployment and serving. Within the inference framework of LLMs, KV caches [9] are crucial for storing pre-computed keys and values, thus avoiding repeated calculations over the preceding context and substantially enhancing LLMs\u2019 deployment efficiency. However, the increasing demand for longer sequence lengths results in voluminous cached states, leading to significant memory consumption during generation. For instance, a 175B GPT-3 model [2], with a batch size of 64 and a sequence length of 4,096 tokens (both prefliled and generated), requires approximately 1,208GB of GPU memory. This requirement is $3.45\\times$ greater than the memory used to store the model\u2019s weights. In this context, KV cache compression is of paramount importance due to its clear beneftis: 1) it largely reduces the memory footprint allowing for faster generation and larger batch serving; 2) it significantly lowers the cost per token, demonstrating substantial commercial benefits. ", "page_idx": 1}, {"type": "text", "text": "Existing KV cache compression efforts can be roughly categorized into two types, namely quantization and sparsity. The quantization approaches [11, 12] propose storing the KV states in low-bit numerical values. Typically, FlexGen [13] demonstrates that 4-bit KV cache quantization can achieve lossless performance. In contrast, sparsity-driven methods aim to retain only the salient tokens while evicting the rest, either heuristically [14, 15] or adaptively [16]. Some approaches [11] explore the intersection of these two types, by assigning high-bit to salient tokens and extremely low-bit to the rest of the tokens, achieving more aggressive memory gain. Despite these innovations, existing literature merely consider the intra-layer redundancy, while neglecting another important complementary direction \u2013 the inter-layer redundancy, as illustrated in the Figure 1(c). ", "page_idx": 1}, {"type": "text", "text": "Our analysis begins by exploring the redundancy of KV caches along the depth dimension, as shown in Figure 1(a). We observe that KV cache states exhibit high similarity between neighbouring layers in the middle-to-deep portions of LLMs. This intriguing property suggests that states paired by position between adjacent layers can be accurately merged into a single state space with a strong performance guarantee, as illustrated in Figure 1(b). This approach significantly reduces the memory footprint without the need to retain individual states for each attention layer. Note that these observations are pertinent to dynamic inference strategies such as mixture-of-depths [17] and layer-wise early exiting [18, 19], which optimize computational paths by skipping non-critical layers to enhance training and inference efficiency. Furthermore, layer pruning methods [20] highlight considerable redundancy in deeper layers. However, despite these advancements, the redundancy of KV caches along the depth dimension has largely been overlooked. ", "page_idx": 2}, {"type": "text", "text": "In this paper, we propose MiniCache, a simple yet effective cross-layer KV cache compression method aimed at advancing the inference efficiency of LLMs. MiniCache consists of two essential components. Firstly, we introduce an accurate cache merging strategy, employing a reparameterization of state vectors that decompose them into the magnitude and direction components, akin to weight normalization [21]. This approach allows for effective interpolation of the directional component in polar coordinates while preserving the original state norms to retain as much information as possible. This interpolation refers to the cross-layer merging as shown in the Figure 1(c). Secondly, we recognize that a small subset of state pairs, characterized by low similarities but carrying largely distinct semantic meanings, are unsuitable for inter-layer merging. To address this, we propose a token retention strategy to minimize performance degradation, which involves separately retaining these outlier pairs. Our framework is notably memory-efficient, requiring storage for only a single high-dimensional directional component, along with minimal extra memory overhead. The overhead consists of a few unmergeable tokens and their corresponding indexes, as well as token-wise magnitudes to accurately restore the original states. ", "page_idx": 2}, {"type": "text", "text": "We conduct extensive experiments with representative LLMs, including Mixtral-8x7B [22], Phi3-Mini [23], and LLaMA-3 [6] 8B and 70B, respectively. Our method is benchmarked across a diverse range of question answering and generation datasets [24, 25, 26, 27, 28, 29, 30, 31] using the lm-eval-harness [32]. Additionally, we evaluate our results on LongBench [33] for long-sequence generation. The results demonstrate that MiniCache can reduce the memory footprint required for LLM inference by up to $41\\%$ , while simultaneously enhancing throughput by approximately $5\\times$ compared to fully cached baseline, clearly surpassing existing methods [11, 12, 14, 15]. ", "page_idx": 2}, {"type": "text", "text": "Our contributions are summarized as follows: ", "page_idx": 2}, {"type": "text", "text": "\u2022 We introduce MiniCache, a simple yet highly effective framework for KV cache compression. MiniCache pioneers the exploration of KV cache compression along the depth dimension, thereby significantly expanding its capabilities.   \n\u2022 We observe a fascinating characteristic of cross-layer KV cache states: high similarity between adjacent layers in the middle to later stages of LLMs. Additionally, we find that not all state pairs are suitable for merging.   \n\u2022 We propose an accurate and memory-efficient method for cross-layer cache merging, comprising a reparameterization strategy and a token retention mechanism. Our method complements existing KV cache compression approaches, further enhancing LLM serving efficiency.   \n\u2022 Our MiniCache performs favourably against the state-of-the-art methods. Notably, our 4-bit MiniCache achieves a strong compression ratio of up to $5.02\\times$ , $5\\times$ higher inference throughput, and $41\\%$ memory reduction compared to the FP16 full cache baseline with near-lossless performance. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Efficient inference for LLMs. Large Language Models (LLMs) are constrained by considerable computational and memory requirements during inference, particularly in resource-constrained environments. To mitigate these challenges, a variety of efficient inference techniques have been developed. For instance, dynamic inference methods [18, 34, 35, 36, 37, 38], represented by mixtureof-experts (MoE) [39, 40, 41, 42, 43], adaptively select specific sub-structures of the model during the inference process based on the input data, significantly improving inference efficiency while keeping model capacity. Techniques like Multi-Query Attention [44, 45], Kernel-driven attentions [46, 47, 48, 49], and low-rank attentions [43, 50, 51, 52] approximate the functionality of traditional attention mechanisms but with more efficient implementations. Quantization strategies [53, 54, 55, 56] involve converting the model\u2019s weights and activations into a low bit-width format, thereby reducing memory footprint and computational intensity. Sparsification approaches [14, 15, 57, 58] eliminate unnecessary elements in both model weights and token representations, further enhancing efficiency. Some closely related works, such as MoD [17] and LayerSkips [19], considered the dynamic inference nature to ignore unimportant layers according to input [59] . However, these methods require an additional fine-tuning process or carefully designed pre-training stages, which reduces the adaptivity of these methods. MiniCache relies on inter-layer similarity observations to perform cross-layer merging, significantly reducing memory demand. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Model merging. Merging compression involves the aggregation of a model\u2019s parameters and activations at various granularities. This process enhances the efficiency of inference in large models and facilitates huge redundancy [60]. Linear Mode Connectivity (LMC) [61] enables the fine-tuning of models from a shared pre-trained base. Commonly, weight averaging [62] is employed as an efficient technique to perform merge compression. Notably, Model Soup [63] utilizes linear averaging in this context. Advanced methods like TIES Merging [64], Model Breadcrumbs [65], and DARE [66] further enhance this process by sparsifying and combining model parameters, enabling the merging of models without sacrificing performance capabilities. Additionally, Spherical Linear intERPolation (SLERP) [67] extends beyond simple weight averaging by interpolating between model parameters. The Fisher information matrix [68] and RegMean-based methods [69] further optimize merges to produce ideal weights, minimizing the $\\ell_{2}$ distance to generation outputs while preserving the privacy of the training data. However, most existing works focus on merging model parameters, with the concept of depth-wise mergeability not being thoroughly explored in prior research. MiniCache focuses on the KV cache token merging in the depth dimensional of LLMs. ", "page_idx": 3}, {"type": "text", "text": "3 Motivation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In the below, we present our new observations in a novel cross-layer perspective. ", "page_idx": 3}, {"type": "image", "img_path": "sgVOjDqUMT/tmp/9b4019a87ee28f2eba4accae67f1dcd544cd73062603df7e89be3f776a6d3b44.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 2: Overall of our explorations and observations $:$ (a) shows the strong baseline by performing average merging on the KV cache. (b) shows the pairwise similarity of cache states between adjacent layers. (c) compares the MiniCache, simple average, and full cache baseline across five different datasets. ", "page_idx": 3}, {"type": "text", "text": "3.1 Cross-Layer Redundancy in KV Cache ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Prior studies have revealed the ineffectiveness of middle-to-deep layers in LLMs [20]. Thus, layerwise early exiting in this case can effectively avoid the redundant computation with minor effect on LLM performance [19, 70]. Inspired by this, we explore a layer-wise merging of KV cache in LLMs, starting with a simple baseline by averaging full tokens across adjacent layers. We provide our key observations as follows. ", "page_idx": 3}, {"type": "text", "text": "Observation 1: KV cache shares a high similarity between adjacent layers. Based on LLaMA-3- 70B [6], we conduct zero-shot inference on the validation sets of three widely recognized benchmarks: COQA [71], GSM8K [10] and TruthfulQA [72]. In general, we find that KV cache in the shallow layers exhibits low similarity, whereas those in the middle-to-deep layers are very similar to one another based on angular distance, as shown in Figure 1(a). Next, we merge the KV cache across adjacent layers by conducting five-shot inference with LLaMA-2-7B, LLaMA-2-13B [5], LLaMA3-8B [6], and Mixtral-8x7B [22] on GSM8K [10]. Specifically, starting from the middle layer of each model, we merge the KV cache in the adjacent two layers. As shown in Figure 2(a), we observe a favourable performance for different LLMs, which reveals the huge potential for efficiency improvement by sharing the KV cache across adjacent layers during LLM decoding. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Observation 2: Not all tokens are equally important to merge, a few distinct pairs require retention. Recent works [15, 16] in KV cache compression have found that keeping a few salient tokens at each layer, which contribute the majority of attention scores, could be sufficient to maintain LLM performance. In our case, we speculate that certain token pairs in adjacent layers also exhibit outlier behaviours, showing strong semantic differences that make them unsuitable for merging. Based on COQA [71] and LLaMA-2-7B [5], we investigate the similarities at the level of token pairs. As shown in Figure 2(b), we find a significant portion of token pairs share high similarities across adjacent layers. However, we observe a few outlier pairs, such as indices 0 and 15, with large margins of difference. We consider these tokens non-mergeable due to their significant differences. We also show that merging these distinct tokens results in performance degradation, corresponding to $\\gamma=0$ row in Table 2. Thus, while cross-layer merging is a promising strategy for reducing memory burdens, it must be implemented with careful consideration of token-level similarities to ensure optimal performance, as shown in Figure 2(c). ", "page_idx": 4}, {"type": "text", "text": "4 Method ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we introduce our MiniCache, a simple yet effective method aimed at trimming the KV cache redundancy in the depth dimension. This framework exploits the high similarity of KV cache states between adjacent layers and consists of two primary components: a reparameterization-based merging strategy and a token retention mechanism. The merging strategy compresses the KV cache states in adjacent layers to aggregate them into a single shared memory space, beginning from the middle of the model. The token retention mechanism mitigates information lost by retaining the highly distinct state pairs with minimal additional memory cost. With the merged cache, retention tokens, and magnitudes, we can accurately restore the original cache states for token decoding. ", "page_idx": 4}, {"type": "text", "text": "4.1 Cross-Layer Compression ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Our method commences with the identification of an optimal starting layer $S$ . Observations in Section 3.1 indicate that the KV cache from middle-to-deep layers consistently exhibits patterns of high similarity across adjacent layers. Consequently, we select the starting layer from the middle of the LLM, specifically $S=L/2$ . From this layer onward, the KV pairs are assumed to be sufficiently similar across adjacent layers to warrant their consolidation. Central to this approach is a merge function, $F$ , which is designed to integrate the KV caches of consecutive layers into a single, unified cache. We define $\\textbf{\\em x}$ as the vectorized cache state of a single token, where the superscript indicates the layer index and the subscripts $k$ and $v$ denote the keys and values, respectively. Specifically, for a pair of key/value tokens at the same position in layers $l$ and $l-1$ , the merged cache is computed as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{c}_{k}^{l,l-1}=F(\\pmb{x}_{k}^{l},\\pmb{x}_{k}^{l-1}),}\\\\ {\\pmb{c}_{v}^{l,l-1}=F(\\pmb{x}_{v}^{l},\\pmb{x}_{v}^{l-1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This consolidation process effectively eliminates the need to store and process the original memoryintensive keys and values in each layer independently. Instead, it approximates a shared cache across the adjacent layers. ", "page_idx": 4}, {"type": "text", "text": "4.2 KV Cache Merging and Restoration ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Reparameterization-based cache merging. To perform the pairwise merging, one solution is to directly average a pair of KV tokens, analogous to model merging [63, 64]. However, we observe that direct averaging can cause significant information loss. We conjecture that the distance between activations can be larger than that of weights due to the presence of outlier activation channels with extremely large magnitudes in LLMs [73, 74], while weights typically have relatively quite small magnitudes. A potential method to compensate for this information loss is to project from $^c$ to $\\pmb{x}^{l-1}$ and $\\mathbf{\\boldsymbol{x}}^{l}$ , then rescale the projected vectors based on their relative magnitudes to exactly restore the original states. However, this approach requires extensive additional storage and computations; for example, restoring $\\pmb{x}^{l-1}$ needs both $^c$ and $\\mathbf{\\dot{x}}^{l}$ , which undermines the benefits of cache merging. To efficiently merge token pairs, we draw inspiration from weight normalization [21], which disentangles model parameters into the magnitude and direction components to accelerate the convergence of stochastic gradient descent. Additionally, we take cues from DoRA [75], which employs a similar way to resemble the learning behavior of parameter-efficient fine-tuning compared to full fine-tuning. In our case, the reparameterization can be formulated as follows: ", "page_idx": 4}, {"type": "image", "img_path": "sgVOjDqUMT/tmp/47ff25a4069df96e0d7a159bf9a897a422b0cc8ba01bb376c179865d0f4c4124.jpg", "img_caption": ["Figure 3: The illustration of the proposed method MiniCache. (a) depicts the cross-layer compression process. We fetch the KV caches, from layers $l$ and $l-1$ , and merge them into shared states via Eq. (3). Additionally, we compute the $\\ell_{2}$ norm for the caches to obtain their magnitudes. Furthermore, we select unmergable tokens for retention, then store merged cache, retention tokens, and magnitudes at layer $l$ in $_{C}$ . (b) illustrates the restoration process for layers $l$ and $l-1$ , which includes magnitude rescaling in Eq. (2) and retention token recovery. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\pmb{x}}^{l}=e^{l,l-1}\\cdot\\frac{\\|\\pmb{x}^{l}\\|}{\\|e^{l,l-1}\\|},\\;\\hat{\\pmb{x}}^{l-1}=e^{l,l-1}\\cdot\\frac{\\|\\pmb{x}^{l-1}\\|}{\\|e^{l,l-1}\\|},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $^e$ is the directional vector. This decomposition ensures that $\\frac{e^{l,l-1}}{\\|e^{l,l-1}\\|}$ is a unit vector, and allows the restored states to match the $\\ell_{2}$ norm of the original states, thereby preserving the cache\u2019s information as much as possible. The restoration is shown as Figure 3(b). For brevity, we omit the subscripts $k$ and $v$ , as keys and values are decomposed in the same way. For estimating the directional component $e^{l,l-1}$ , we follow SLERP [67], which adaptively handles the interpolation, which often resembles rotation-like transformations. The choice of SLERP as the merging function is strategic, as it facilitates interpolation along the shortest path on the unit sphere between two high-dimensional vectors, thereby preserving their geometric integrity, which refers to merge operation in Figure 3(a). This is crucial for maintaining the semantic and syntactic properties of the KV caches. The formula for SLERP in our context is: ", "page_idx": 5}, {"type": "equation", "text": "$$\ne^{l,l-1}=\\frac{\\sin((1-t)\\Omega^{l,l-1})}{\\sin(\\Omega^{l,l-1})}\\cdot\\frac{x^{l-1}}{\\|x^{l-1}\\|}+\\frac{\\sin(t\\Omega^{l,l-1})}{\\sin(\\Omega^{l,l-1})}\\cdot\\frac{x^{l}}{\\|x^{l}\\|},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\begin{array}{r}{\\Omega^{l,l-1}=\\operatorname{arccos}\\left(\\frac{\\pmb{x}^{l}\\cdot\\pmb{x}^{l-1}}{\\|\\pmb{x}^{l}\\|\\|\\pmb{x}^{l-1}\\|}\\right)}\\end{array}$ \u2225xxl\u2225\u00b7\u2225xxl\u22121\u2225 represents the angle between vectors xl and xl\u22121, and sin(\u00b7) is the sine function. $t$ is an interpolation hyperparameter that adjusts the relative influence of each vector on the resulting direction, tailored to the layer depth and specific characteristics of the KV pairs. Note that when we set $t=0.5$ , it will become an average merging along the geometry surface, which we consider special cases in Eq. (A). The merged cache for each token pair is then a concatenation of the directional vector, magnitude and $\\Omega^{l,l-1}$ , denoting as $\\pmb{c}^{l,l-1}=[\\pmb{e}^{l,l-1},\\lVert\\pmb{x}^{l-1}\\rVert,\\lVert\\pmb{x}^{l}\\rVert,\\Omega^{l,l-1}]$ , cached components as shown in Figure 3(a). Note that apart from storing the merged directional vector, we only need to store additional token-wise magnitude and angle scalars, which is memory efficient. In this way, we achieve substantial memory efficiencies through reduced redundancy while ensuring the retention of the critical functional characteristics of the original KV pairs across transformer layers. ", "page_idx": 5}, {"type": "text", "text": "Unmergeable token retention. Highly distinct pairs are sensitive to merging operations, leading us to propose unmergeable token retention, as shown in Figure 3(a). Despite the high similarity between ", "page_idx": 5}, {"type": "text", "text": "KV cache states across neighbouring layers, a few sensitive distinct pairs remain that are significantly difficult to merge and share. Aligned with previous studies, these distinct tokens carry substantial semantic meanings [15, 16]. We observe that merging sensitive tokens, which results in the loss of layer-specific information, can lead to significant performance degradation. Therefore, it is crucial to properly disentangle the shared and unique information between adjacent layers. To address this issue, we designed a token retention strategy to selectively retain tokens that cannot be merged based on their angular distance, defined as: $d(\\breve{\\pmb{x}^{l}},\\pmb{x}^{l-1})\\,=\\,\\textstyle{\\frac{1}{\\pi}}\\dot{\\Omega}$ . For the KV caches, the minimum and maximum angular distances are determined to identify the unmergeable tokens. ", "page_idx": 6}, {"type": "text", "text": "The set of required token indices to keep, $\\mathbb{I}$ , is obtained by: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{I}=\\{i\\mid d_{i}<d_{\\operatorname*{min}}+(d_{\\operatorname*{max}}-d_{\\operatorname*{min}})\\cdot\\gamma\\},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\gamma$ is a predefined hyperparameter that controls the retention threshold. The tokens with indices in I are retained and not compressed during the merge, which ensures that performance does not decline by preventing the loss of unmergeable tokens. ", "page_idx": 6}, {"type": "text", "text": "Next, let $\\boldsymbol{X}\\in\\mathbb{R}^{n\\times h}$ be either the key or value cache at one attention layer, where $n$ denotes the number of tokens and $h$ is the number of hidden dimensions, and $\\pmb{E}\\in\\mathbb{R}^{n\\times\\check{h}}$ be the shared KV cache states. For each pair of neighbouring two layers, the unmergeable tokens are selected along with the token dimension by $R^{l}=X^{l}[\\mathbb{I}]$ , $\\mathbf{\\hat{R}}^{l-1}=\\mathbf{X}^{l-1}[\\mathbb{I}]$ , then restoring to our compressed caches by $\\hat{\\mathbf{X}}^{l}[\\mathbb{I}]=R^{l}$ , $\\hat{\\pmb X}^{l-1}[\\mathbb{I}]=R^{l-1}$ , as shown in Figure 3(b). Overall, we share the final cache for the two layers as $\\pmb{C}^{l,l-1}=[\\pmb{E}^{l,l-1},\\pmb{R}^{l},\\pmb{R}^{l-1},\\|\\pmb{X}^{l-1}\\|,\\|\\pmb{X}^{l}\\|,\\mathbb{I}]$ . This cache includes the shared KV cache states, retention of unmerged tokens, magnitude vectors for each layer, and the token-keeping index, respectively. These additional components are quite lightweight. Thus compared to full-layer caches, our method remains memory-efficient, as discussed in Sec. 4.3. ", "page_idx": 6}, {"type": "text", "text": "Cache restoration. After obtaining the shared cache $C^{l,l-1}$ , we further need to approximately restore the original cache states for the current token decoding, as shown in Fig. 3(b). Specifically, to restore $X^{l}$ , we first rescale the directional shared states with the corresponding magnitude vector along the token dimension, denoted as $E^{l,l-1}\\|X^{l}\\|$ . Subsequently, we perform retention token recovery by placing the sensitive tokens according to their token indices. ", "page_idx": 6}, {"type": "text", "text": "4.3 Efficiency Discussion ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Compression efficiency. We primarily analyze our memory efficiency in terms of the number of tokens used. Next, let $r$ be the number of layers and and $b$ is the batch size, $s$ and $n$ are input and output sequence length respectively. We consider the FP16 storage for KV cache. The full cache memory usage is given by $4b r h(s+n)$ . In our study, we begin merging layers from the middle to the deeper layers, consolidating the KV cache states of every two layers into a single shared state space. As a result, we effectively reduce the GPU memory usage in the decoding inference to $3b r h(s+n)$ , demonstrating a significant compression rate. ", "page_idx": 6}, {"type": "text", "text": "Restoration efficiency. We then analyze the additional memory cost incurred during the restoration process, which During the magnitude rescaling phase, we save an additional norm vector for the corresponding layers in the KV cache. It is important to note that the norm vector is in the shape of $\\mathbb{R}^{b\\times s\\times1}$ , which has a single channel dimension compared to the fully ranked original KV states. Additionally, we suppose that the retention threshold can be set to 0.05. Therefore, we have $b r h(0.05(s+n))$ tokens retained without compression. Finally, our overall memory requirement is given by $(3.1h+2)b r(s+n)$ . The detailed derivation is shown in the Appendix E. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We demonstrated that our MiniCache can perform merging compression on the latter half of the layers of LLMs with minimal performance degradation. ", "page_idx": 6}, {"type": "text", "text": "Implementation details. Our experiments are based on representative model families of LLMs, including a compact LLM Phi-3-Mini [23] and an MoE LLM Mixtral-8x7B [22]. Additionally, we adopt LLaMA-3 [6] 8B and 70B models to explore how our method generalizes to larger LLMs. We sample ten tasks from lm-eval-harness [32], including COPA [24], MathQA [25], OpenBookQA [26], PIQA [27], RTE [28], WinoGrande [29], XSUM [30], and CNN/Daily Mail [31]. We also evaluate long-sequence generation on LongBench [33]. We compare our method with a fully cached baseline, and other methods such as round-to-nearest quantization (RTN) [76], SmoothQuant [73] and KIVI [11]. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "For the proposed MiniCache, we set the interpolation parameter $t$ to 0.6, indicating that the merged results have a smaller rotation angle to the next layer. Furthermore, we set the token retention threshold $\\gamma$ to 0.05, according to the statistics of unmergeable tokens across multiple datasets. In addition to our merging method, we also consider a strong baseline of average merging. For sequential loading of large models, we utilize NVIDIA 4 A100 80GB GPUs, more details refers to Appendix D. ", "page_idx": 7}, {"type": "text", "text": "Main results. We evaluate MiniCache by merging KV caches across all layers on GSM8K, COQA, and TruthfulQA. The results are shown in Figure 4. In general, we demonstrate the general effectiveness of merging KV caches from middle-to-deep layers across different sized LLMs. Moreover, the proposed MiniCache demonstrates a consistent and significant advantage over the averaging baseline. We also illustrate the performance of merging KV caches across half of the layers with the blue lines, where MiniCache still maintains a robust performance and achieves the best compression ratio. Besides, we find that our method is even more effective for larger LLMs. For instance, based on LLaMA-3-70B, MiniCache shows nearly zero performance drop even with the KV cache in $87.5\\%$ of the layers merged on the COQA dataset. This highlights the adaptability and efficiency of our approach in handling large-scale models while ensuring minimal performance degradation. ", "page_idx": 7}, {"type": "text", "text": "LongBench. We also conduct experiments to evaluate performance and quality in long-sequence generation using the LongBench dataset [33], as shown in Table 1. Our experiments applied MiniCache over several models: LLaMA-2-7B-Chat, LLaMA-2-13B-Chat, Mistral-7B, and Mistral-7B-Instruct. It is important to note that our MiniCache method maintains orthogonality with all existing quantization and sparsity (refers to Table A) methods at both the model and token-wise levels. When combined with KIVI-4bit KV cache quantization, our approach achieves a compression rate of $5.02\\times$ , with minimal impact on accuracy across various challenging long-context generation tasks. The combination of MiniCache and KIVI-4bit KV cache quantization demonstrates significant memory savings without compromising the model\u2019s ability to handle long sequences effectively. This highlights the potential of our method to optimize large language models for tasks requiring extensive context, making them more efficient and scalable for real-world applications. ", "page_idx": 7}, {"type": "image", "img_path": "sgVOjDqUMT/tmp/f944c0d81fffdb4bedf18f975f4e631fecdc3b7fa2d9d87dac8fc2e1000f5c6e.jpg", "img_caption": ["Figure 4: Performance comparisons between our proposed MiniCache with the \u201caveraging baseline\u201d and the \u201cunmerged full cache baseline\u201d on multiple datasets with Phi3-Mini, Mixtral- $\\mathbf{\\cdot8x7B}$ , LLaMA3-8B, and LLaMA-3-70B. More result details are shown in Appendix F. The $\\mathbf{X}$ -axis indicates the number of layers merged. As more layers are merged, a greater reduction in memory usage is achieved. "], "img_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "sgVOjDqUMT/tmp/fc61074251d6b7f32bc046d9f3633bde0b68bf14f2fbd93acf4f741ea5331dc2.jpg", "table_caption": ["Table 1: Evaluation of different KV cache compression methods on LongBench. MiniCache builds on top of 4-bit KIVI [11] and achieves the best performance with the strongest compression rate. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Efficiency analysis. To assess the acceleration capabilities of MiniCache, we conduct evaluations based on the methodologies employed in vLLM [77] and KIVI [11]. We generate synthetic workloads derived from ShareGPT, which include real input and output texts from LLM services. The dataset features an average input prompt length of 161 tokens and an output length of 338 tokens. Using the LLaMA-2-7B model on a single 80GB NVIDIA A100 GPU, we benchmark our method in a batch-serving scenario, comparing peak memory usage and throughput among 2-bit KIVI, 4-bit MiniCache, and an FP16 baseline. As illustrated in Figure 5, with a batch size of 128, MiniCache reduces memory usage by 25GB, achieving a ${\\bf41\\%}$ memory saving. In terms of throughput, MiniCache outperforms the FP16 baseline by approximately $\\mathsf{\\Sigma}^{5\\times}$ . Additionally, despite utilizing 4-bit quantization, MiniCache benefits from merging and sharing KV caches across adjacent layers, resulting in a $1.29\\times$ higher throughput compared to the 2-bit KIVI. These results demonstrate that MiniCache offers a state-of-the-art trade-off between efficiency and performance. ", "page_idx": 8}, {"type": "image", "img_path": "sgVOjDqUMT/tmp/e0cbd1f94e8fede7bc7e8455de64ae22f33c29919977333b1381ab4c871a5101.jpg", "img_caption": ["(a) BS. vs. Peak Memory Usage (b) BS. vs. Decoding Throughput "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "experiment on the GSM8K [10]. Figure 5: Memory usage and throughput comparison between The right axis is the normalized our 4-bit MiniCache, 2-bit KIVI, and 16-bit Baseline. MiniCache frequency of the relative magnican achieve higher throughput by enabling a larger batch size tude ratio. Optional $t$ shows a while reducing memory footprints via LLaMA-2-7B [5]. strong correlation with frequency. ", "page_idx": 8}, {"type": "text", "text": "6 Ablation Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The effect of interpretation parameter $t,$ . We explore the effects of the interpretation parameter $t$ on performance, particularly in relation to the relative magnitude ratio of adjacent layers, as shown in Figure 6. We maintain all settings constant, starting from layer $S=16$ (halfway through the layers of LLaMA-3-8B), and vary the interpretation parameter $t$ from 0.3 to 0.7. Our findings reveal several key points. When $t=0.5$ , the process resembles average merging, which is less effective for cross-layer merging. In contrast, when $t=0.6$ is optimal, the merged representation exhibits the most robust performance, while indicating that more information is derived from the second term $(\\pmb{x}^{l})$ of the SLERP. ", "page_idx": 8}, {"type": "table", "img_path": "sgVOjDqUMT/tmp/5531e6e12bf850c780cabe470588647c03def7a1450ae6a9a1933ecad0af72b4.jpg", "table_caption": ["Table 2: Comparisons of various token retention thresholds $\\gamma$ by LLaMA-2-7B [5] on three benchmarks. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "The frequency results also indicate that the high frequencies are clustered around 0.4 and 0.6, corroborating our optimal $t$ . Moreover, there is a strong correlation between the optimal $t$ and the high frequency of the relative magnitude ratio of adjacent layers. This finding provides an opportunity to utilize the relative magnitude ratio to dynamically determine the interpretation parameter $t$ . Dynamic $t$ allows for more flexible weight control in SLERP merging for each layer-wise operation, thereby showing potential for further exploration. ", "page_idx": 9}, {"type": "text", "text": "The effect of token retention threshold $\\gamma$ . We investigate the impact of the token retention threshold $\\gamma$ on model performance across the three datasets, as shown in Table 2. A larger $t$ generally means retaining more tokens for improved performance, but this comes at the cost of increased memory demand. The results suggest that setting $\\gamma$ to 0.05 achieves the best balance between performance and efficiency. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion and Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper presents a pioneering exploration of KV cache compression in the depth dimension, addressing a significant memory bottleneck in LLMs. Our proposed MiniCache offers a simple, effective, and training-free approach to compressing KV caches by leveraging the notable high similarities between KV caches in neighboring layers, starting from the midpoint of LLMs. We have demonstrated that MiniCache can significantly reduce the memory footprint required for LLM inference by up to $41\\%$ , while simultaneously enhancing throughput by approximately five times compared to the FP16 baseline. In conclusion, MiniCache significantly advances the field of KV cache compression, offering a state-of-the-art balance between efficiency and performance. Future work will focus on enhancing the compression ratio by cross-multiple-layer merging, developing advanced merging algorithms such as Spherical Cubic Interpolation [78], and further optimizing memory usage for large-scale deployments in diverse application scenarios. ", "page_idx": 9}, {"type": "text", "text": "8 Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This research is partially supported by the ARC Future Fellowship (FT190100039) to G.H. Additional support was partially provided by the DARPA Assured Neuro-Symbolic Learning and Reasoning (ANSR) program under award number FA8750-23-2-1016. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al., \u201cLanguage models are few-shot learners,\u201d in NeurIPS, vol. 33, pp. 1877\u20131901, 2020.   \n[2] OpenAI, J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, and OTHERS, \u201cGpt-4 technical report,\u201d 2023.   \n[3] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al., \u201cTraining language models to follow instructions with human feedback,\u201d in NeurIPS, vol. 35, pp. 27730\u201327744, 2022.   \n[4] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi\u00e8re, N. Goyal, E. Hambro, F. Azhar, et al., \u201cLlama: Open and efficient foundation language models,\u201d arXiv preprint arXiv:2302.13971, 2023.   \n[5] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al., \u201cLlama 2: Open foundation and fine-tuned chat models,\u201d arXiv preprint arXiv:2307.09288, 2023. [6] \u201cIntroducing meta llama 3: The most capable openly available llm to date.\u201d https://ai.meta. com/blog/meta-llama-3/, 2024. Accessed: 2024-05-04. [7] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei, \u201cScaling laws for neural language models,\u201d arXiv preprint arXiv:2001.08361, 2020.   \n[8] G. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, et al., \u201cGemini: a family of highly capable multimodal models,\u201d arXiv preprint arXiv:2312.11805, 2023. [9] R. Pope, S. Douglas, A. Chowdhery, J. Devlin, J. Bradbury, J. Heek, K. Xiao, S. Agrawal, and J. Dean, \u201cEfficiently scaling transformer inference,\u201d Proceedings of Machine Learning and Systems, vol. 5, 2023.   \n[10] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, et al., \u201cTraining verifiers to solve math word problems,\u201d arXiv preprint arXiv:2110.14168, 2021.   \n[11] Z. Liu, J. Yuan, H. Jin, S. Zhong, Z. Xu, V. Braverman, B. Chen, and X. Hu, \u201cKivi: Plugand-play 2bit kv cache quantization with streaming asymmetric quantization,\u201d arXiv preprint arXiv:2402.02750, 2024.   \n[12] H. Kang, Q. Zhang, S. Kundu, G. Jeong, Z. Liu, T. Krishna, and T. Zhao, \u201cGear: An efficient kv cache compression recipefor near-lossless generative inference of llm,\u201d arXiv preprint arXiv:2403.05527, 2024.   \n[13] Y. Sheng, L. Zheng, B. Yuan, Z. Li, M. Ryabinin, B. Chen, P. Liang, C. R\u00e9, I. Stoica, and C. Zhang, \u201cFlexgen: High-throughput generative inference of large language models with a single gpu,\u201d in ICML, pp. 31094\u201331116, PMLR, 2023.   \n[14] Z. Zhang, Y. Sheng, T. Zhou, T. Chen, L. Zheng, R. Cai, Z. Song, Y. Tian, C. R\u00e9, C. Barrett, et al., \u201cH2o: Heavy-hitter oracle for efficient generative inference of large language models,\u201d in NeurIPS, vol. 36, 2024.   \n[15] G. Xiao, Y. Tian, B. Chen, S. Han, and M. Lewis, \u201cEfficient streaming language models with attention sinks,\u201d in ICLR, 2024.   \n[16] S. Ge, Y. Zhang, L. Liu, M. Zhang, J. Han, and J. Gao, \u201cModel tells you what to discard: Adaptive kv cache compression for llms,\u201d ICLR, 2024.   \n[17] D. Raposo, S. Ritter, B. Richards, T. Lillicrap, P. C. Humphreys, and A. Santoro, \u201cMixture-ofdepths: Dynamically allocating compute in transformer-based language models,\u201d arXiv preprint arXiv:2404.02258, 2024.   \n[18] W. Zhou, C. Xu, T. Ge, J. McAuley, K. Xu, and F. Wei, \u201cBert loses patience: Fast and robust inference with early exit,\u201d in NeurIPS, vol. 33, pp. 18330\u201318341, 2020.   \n[19] M. Elhoushi, A. Shrivastava, D. Liskovich, B. Hosmer, B. Wasti, L. Lai, A. Mahmoud, B. Acun, S. Agarwal, A. Roman, et al., \u201cLayer skip: Enabling early exit inference and self-speculative decoding,\u201d arXiv preprint arXiv:2404.16710, 2024.   \n[20] A. Gromov, K. Tirumala, H. Shapourian, P. Glorioso, and D. A. Roberts, \u201cThe unreasonable ineffectiveness of the deeper layers,\u201d arXiv preprint arXiv:2403.17887, 2024.   \n[21] T. Salimans and D. P. Kingma, \u201cWeight normalization: A simple reparameterization to accelerate training of deep neural networks,\u201d in NeurIPS, vol. 29, 2016.   \n[22] A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford, D. S. Chaplot, D. d. l. Casas, E. B. Hanna, F. Bressand, et al., \u201cMixtral of experts,\u201d arXiv preprint arXiv:2401.04088, 2024.   \n[23] M. Abdin, S. A. Jacobs, A. A. Awan, J. Aneja, A. Awadallah, H. Awadalla, N. Bach, A. Bahree, A. Bakhtiari, H. Behl, et al., \u201cPhi-3 technical report: A highly capable language model locally on your phone,\u201d arXiv preprint arXiv:2404.14219, 2024.   \n[24] M. Roemmele, C. A. Bejan, and A. S. Gordon, \u201cChoice of plausible alternatives: An evaluation of commonsense causal reasoning.,\u201d in AAAI spring symposium: logical formalizations of commonsense reasoning, pp. 90\u201395, 2011.   \n[25] A. Amini, S. Gabriel, S. Lin, R. Koncel-Kedziorski, Y. Choi, and H. Hajishirzi, \u201cMathqa: Towards interpretable math word problem solving with operation-based formalisms,\u201d in NAACL, pp. 2357\u20132367, 2019.   \n[26] T. Mihaylov, P. Clark, T. Khot, and A. Sabharwal, \u201cCan a suit of armor conduct electricity? a new dataset for open book question answering,\u201d in EMNLP, 2018.   \n[27] Y. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi, \u201cPiqa: Reasoning about physical commonsense in natural language,\u201d in AAAI, 2020.   \n[28] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman, \u201cGlue: A multitask benchmark and analysis platform for natural language understanding,\u201d arXiv preprint arXiv:1804.07461, 2018.   \n[29] K. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y. Choi, \u201cWinogrande: An adversarial winograd schema challenge at scale,\u201d Communications of the ACM, vol. 64, no. 9, pp. 99\u2013106, 2021.   \n[30] S. Narayan, S. B. Cohen, and M. Lapata, \u201cDon\u2019t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization,\u201d arXiv preprint arXiv:1808.08745, 2018.   \n[31] R. Nallapati, B. Zhou, C. Gulcehre, B. Xiang, et al., \u201cAbstractive text summarization using sequence-to-sequence rnns and beyond,\u201d arXiv preprint arXiv:1602.06023, 2016.   \n[32] L. Gao, J. Tow, B. Abbasi, S. Biderman, S. Black, A. DiPof,i C. Foster, L. Golding, J. Hsu, A. Le Noac\u2019h, H. Li, K. McDonell, N. Muennighoff, C. Ociepa, J. Phang, L. Reynolds, H. Schoelkopf, A. Skowron, L. Sutawika, E. Tang, A. Thite, B. Wang, K. Wang, and A. Zou, \u201cA framework for few-shot language model evaluation,\u201d 12 2023.   \n[33] Y. Bai, X. Lv, J. Zhang, H. Lyu, J. Tang, Z. Huang, Z. Du, X. Liu, A. Zeng, L. Hou, et al., \u201cLongbench: A bilingual, multitask benchmark for long context understanding,\u201d arXiv preprint arXiv:2308.14508, 2023.   \n[34] L. Del Corro, A. Del Giorno, S. Agarwal, B. Yu, A. Awadallah, and S. Mukherjee, \u201cSkipdecode: Autoregressive skip decoding with batching and caching for efficient llm inference,\u201d arXiv preprint arXiv:2307.02628, 2023.   \n[35] T. Schuster, A. Fisch, J. Gupta, M. Dehghani, D. Bahri, V. Tran, Y. Tay, and D. Metzler, \u201cConfident adaptive language modeling,\u201d in NeurIPS, vol. 35, pp. 17456\u201317472, 2022.   \n[36] H. Wu and K. Tu, \u201cLayer-condensed kv cache for efficient inference of large language models,\u201d 2024.   \n[37] J.-H. Kim, J. Yeom, S. Yun, and H. O. Song, \u201cCompressed context memory for online language model interaction,\u201d arXiv preprint arXiv:2312.03414, 2023.   \n[38] A. Bulatov, Y. Kuratov, and M. Burtsev, \u201cRecurrent memory transformer,\u201d Advances in Neural Information Processing Systems, vol. 35, pp. 11079\u201311091, 2022.   \n[39] W. Fedus, B. Zoph, and N. Shazeer, \u201cSwitch transformers: Scaling to trillion parameter models with simple and efficient sparsity,\u201d Journal of Machine Learning Research, vol. 23, no. 120, pp. 1\u201339, 2022.   \n[40] D. Lepikhin, H. Lee, Y. Xu, D. Chen, O. Firat, Y. Huang, M. Krikun, N. Shazeer, and Z. Chen, \u201cGshard: Scaling giant models with conditional computation and automatic sharding,\u201d arXiv preprint arXiv:2006.16668, 2020.   \n[41] D. Dai, C. Deng, C. Zhao, R. Xu, H. Gao, D. Chen, J. Li, W. Zeng, X. Yu, Y. Wu, et al., \u201cDeepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models,\u201d arXiv preprint arXiv:2401.06066, 2024.   \n[42] C. Hwang, W. Cui, Y. Xiong, Z. Yang, Z. Liu, H. Hu, Z. Wang, R. Salas, J. Jose, P. Ram, et al., \u201cTutel: Adaptive mixture-of-experts at scale,\u201d Proceedings of Machine Learning and Systems, vol. 5, 2023.   \n[43] DeepSeek-AI, \u201cDeepseek-v2: A strong, economical, and efficient mixture-of-experts language model,\u201d arXiv preprint arXiv:2405.04434, 2024.   \n[44] J. Ainslie, J. Lee-Thorp, M. de Jong, Y. Zemlyanskiy, F. Lebr\u00f3n, and S. Sanghai, \u201cGqa: Training generalized multi-query transformer models from multi-head checkpoints,\u201d arXiv preprint arXiv:2305.13245, 2023.   \n[45] N. Shazeer, \u201cFast transformer decoding: One write-head is all you need,\u201d arXiv preprint arXiv:1911.02150, 2019.   \n[46] K. Choromanski, V. Likhosherstov, D. Dohan, X. Song, A. Gane, T. Sarlos, P. Hawkins, J. Davis, A. Mohiuddin, L. Kaiser, et al., \u201cRethinking attention with performers,\u201d arXiv preprint arXiv:2009.14794, 2020.   \n[47] H. Peng, N. Pappas, D. Yogatama, R. Schwartz, N. A. Smith, and L. Kong, \u201cRandom feature attention,\u201d in ICLR, 2021.   \n[48] T. Dao, D. Fu, S. Ermon, A. Rudra, and C. R\u00e9, \u201cFlashattention: Fast and memory-efficient exact attention with io-awareness,\u201d in NeurIPS, vol. 35, pp. 16344\u201316359, 2022.   \n[49] T. Dao, \u201cFlashattention-2: Faster attention with better parallelism and work partitioning,\u201d arXiv preprint arXiv:2307.08691, 2023.   \n[50] S. Wang, B. Z. Li, M. Khabsa, H. Fang, and H. Ma, \u201cLinformer: Self-attention with linear complexity,\u201d arXiv preprint arXiv:2006.04768, 2020.   \n[51] X. Ma, X. Kong, S. Wang, C. Zhou, J. May, H. Ma, and L. Zettlemoyer, \u201cLuna: Linear unified nested attention,\u201d in NeurIPS, vol. 34, pp. 2441\u20132453, 2021.   \n[52] J. Lee, Y. Lee, J. Kim, A. Kosiorek, S. Choi, and Y. W. Teh, \u201cSet transformer: A framework for attention-based permutation-invariant neural networks,\u201d in ICML, pp. 3744\u20133753, PMLR, 2019.   \n[53] J. Lin, J. Tang, H. Tang, S. Yang, X. Dang, and S. Han, \u201cAwq: Activation-aware weight quantization for llm compression and acceleration,\u201d arXiv preprint arXiv:2306.00978, 2023.   \n[54] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, \u201cQlora: Efficient finetuning of quantized llms,\u201d in NeurIPS, vol. 36, 2023.   \n[55] T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer, \u201cLlm.int8(): 8-bit matrix multiplication for transformers at scale,\u201d in NeurIPS, vol. 35, pp. 30318\u201330332, 2022.   \n[56] J. Liu, R. Gong, X. Wei, Z. Dong, J. Cai, and B. Zhuang, \u201cQllm: Accurate and efficient low-bitwidth quantization for large language models,\u201d in ICLR, 2024.   \n[57] M. Zhang, C. Shen, Z. Yang, L. Ou, X. Yu, B. Zhuang, et al., \u201cLoraprune: Pruning meets low-rank parameter-efficient fine-tuning,\u201d in ACL findings, 2024.   \n[58] N. Kitaev, \u0141. Kaiser, and A. Levskaya, \u201cReformer: The efficient transformer,\u201d arXiv preprint arXiv:2001.04451, 2020.   \n[59] S. Bae, J. Ko, H. Song, and S.-Y. Yun, \u201cFast and robust early-exiting framework for autoregressive language models with synchronized parallel decoding,\u201d arXiv preprint arXiv:2310.05424, 2023.   \n[60] S. K. Ainsworth, J. Hayase, and S. Srinivasa, \u201cGit re-basin: Merging models modulo permutation symmetries,\u201d in ICLR, 2023.   \n[61] R. Entezari, H. Sedghi, O. Saukh, and B. Neyshabur, \u201cThe role of permutation invariance in linear mode connectivity of neural networks,\u201d arXiv preprint arXiv:2110.06296, 2021.   \n[62] T. Garipov, P. Izmailov, D. Podoprikhin, D. P. Vetrov, and A. G. Wilson, \u201cLoss surfaces, mode connectivity, and fast ensembling of dnns,\u201d in NeurIPS, vol. 31, 2018.   \n[63] M. Wortsman, G. Ilharco, S. Y. Gadre, R. Roelofs, R. Gontijo-Lopes, A. S. Morcos, H. Namkoong, A. Farhadi, Y. Carmon, S. Kornblith, et al., \u201cModel soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time,\u201d in ICML, pp. 23965\u201323998, PMLR, 2022.   \n[64] P. Yadav, D. Tam, L. Choshen, C. A. Raffel, and M. Bansal, \u201cTies-merging: Resolving interference when merging models,\u201d in NeurIPS, vol. 36, 2023.   \n[65] M. Davari and E. Belilovsky, \u201cModel breadcrumbs: Scaling multi-task model merging with sparse masks,\u201d arXiv preprint arXiv:2312.06795, 2023.   \n[66] L. Yu, B. Yu, H. Yu, F. Huang, and Y. Li, \u201cLanguage models are super mario: Absorbing abilities from homologous models as a free lunch,\u201d arXiv preprint arXiv:2311.03099, 2023.   \n[67] K. Shoemake, \u201cAnimating rotation with quaternion curves,\u201d in Proceedings of the 12th annual conference on Computer graphics and interactive techniques, pp. 245\u2013254, 1985.   \n[68] M. S. Matena and C. A. Raffel, \u201cMerging models with fisher-weighted averaging,\u201d in NeurIPS, vol. 35, pp. 17703\u201317716, 2022.   \n[69] X. Jin, X. Ren, D. Preotiuc-Pietro, and P. Cheng, \u201cDataless knowledge fusion by merging weights of language models,\u201d arXiv preprint arXiv:2212.09849, 2022.   \n[70] Y. Chen, X. Pan, Y. Li, B. Ding, and J. Zhou, \u201cEe-llm: Large-scale training and inference of early-exit large language models with 3d parallelism,\u201d arXiv preprint arXiv:2312.04916, 2023.   \n[71] S. Reddy, D. Chen, and C. D. Manning, \u201cCoqa: A conversational question answering challenge,\u201d Transactions of the Association for Computational Linguistics, vol. 7, pp. 249\u2013266, 2019.   \n[72] S. Lin, J. Hilton, and O. Evans, \u201cTruthfulqa: Measuring how models mimic human falsehoods,\u201d arXiv preprint arXiv:2109.07958, 2021.   \n[73] G. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, and S. Han, \u201cSmoothquant: Accurate and efficient post-training quantization for large language models,\u201d in International Conference on Machine Learning, pp. 38087\u201338099, PMLR, 2023.   \n[74] J. Liu, R. Gong, X. Wei, Z. Dong, J. Cai, and B. Zhuang, \u201cQllm: Accurate and efficient low-bitwidth quantization for large language models,\u201d in ICLR, 2024.   \n[75] S.-Y. Liu, C.-Y. Wang, H. Yin, P. Molchanov, Y.-C. F. Wang, K.-T. Cheng, and M.-H. Chen, \u201cDora: Weight-decomposed low-rank adaptation,\u201d arXiv preprint arXiv:2402.09353, 2024.   \n[76] M. Nagel, R. A. Amjad, M. Van Baalen, C. Louizos, and T. Blankevoort, \u201cUp or down? adaptive rounding for post-training quantization,\u201d in International Conference on Machine Learning, pp. 7197\u20137206, PMLR, 2020.   \n[77] W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. Gonzalez, H. Zhang, and I. Stoica, \u201cEfficient memory management for large language model serving with pagedattention,\u201d in Proceedings of the 29th Symposium on Operating Systems Principles, pp. 611\u2013626, 2023.   \n[78] D. H. Eberly, \u201cQuaternion algebra and calculus,\u201d 2002.   \n[79] G. Xiao, Y. Tian, B. Chen, S. Han, and M. Lewis, \u201cEfficient streaming language models with attention sinks,\u201d arXiv preprint arXiv:2309.17453, 2023.   \n[80] \u201cStanford crfm.\u201d https://crfm.stanford.edu/2023/10/12/flashdecoding.html, 2024. Accessed: 2024-05-04.   \n[81] Y. Liu, H. Li, K. Du, J. Yao, Y. Cheng, Y. Huang, S. Lu, M. Maire, H. Hoffmann, A. Holtzman, et al., \u201cCachegen: Fast context loading for language model applications,\u201d arXiv preprint arXiv:2310.07240, 2023.   \n[82] L. Chen, H. Zhao, T. Liu, S. Bai, J. Lin, C. Zhou, and B. Chang, \u201cAn image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration for large vision-language models,\u201d arXiv preprint arXiv:2403.06764, 2024.   \n[83] S. Wei, T. Ye, S. Zhang, Y. Tang, and J. Liang, \u201cJoint token pruning and squeezing towards more aggressive compression of vision transformers,\u201d in CVPR, pp. 2092\u20132101, 2023.   \n[84] L. Gao, J. Tow, S. Biderman, S. Black, A. DiPof,i C. Foster, L. Golding, J. Hsu, K. McDonell, N. Muennighoff, J. Phang, L. Reynolds, E. Tang, A. Thite, B. Wang, K. Wang, and A. Zou, \u201cA framework for few-shot language model evaluation,\u201d 2021.   \n[85] W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. Gonzalez, H. Zhang, and I. Stoica, \u201cEfficient memory management for large language model serving with pagedattention,\u201d in Proceedings of the 29th Symposium on Operating Systems Principles, pp. 611\u2013626, 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A Additional Experiment Results ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Comparisons with token sparsity methods. We also compare MiniCache with the sparsity-based method H2O [14]. Our method outperforms H2O in most tasks on LongBench. Notably, our approach focuses on reducing inter-layer redundancy, whereas H2O focuses on intra-layer redundancy. Our approaches are orthogonal to sparsity-based methods. ", "page_idx": 15}, {"type": "table", "img_path": "sgVOjDqUMT/tmp/95c83091fad24c72d62341908ecfd2b6e1bd843f11d5ad5191e3f0a3df529eae.jpg", "table_caption": ["Table A: Comparison between MiniCache with token sparsity based method H2O, using mistral-7Binstruct on LongBench dataset. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Execution time comparison for difference sequence length. We benchmark the latency of LlaMA2-7B on an NVIDIA A100 GPU using different sequence lengths ranging from 1024 to 4096 with a batch size of 16, as shown in table B. We compare it with H2O, which requires calculating full attention scores to estimate token importance. In contrast, MiniCache performs reparameterizationbased merging and token restoration using simple matrix manipulations, resulting in more lightweight computations and lower latency. Specifically, when the sequence length is 4096, MiniCache shows a $36.83\\%$ reduction in latency compared to H2O. ", "page_idx": 15}, {"type": "text", "text": "Table B: Execution time comparison for different sequence lengths. We benchmarked the latency of LLaMA-2-7B on an NVIDIA A100 GPU using sequence lengths ranging from 1024 to 4096 with a batch size of 16. ", "page_idx": 15}, {"type": "table", "img_path": "sgVOjDqUMT/tmp/bda2a8ecba582fce6ece082ad8921abd3eb06ecb14fe2cf572b5ff4688c4464a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Trade-off between accuracy and efficiency. We conducted experiments by varying the ratio of retention tokens, as shown in table C. We observe that accuracy improves up to a certain point before plateauing as the token retention ratio increases. Retaining $20\\%$ of the tokens is necessary to ensure all salient tokens are preserved without performance degradation. In contrast, we only need to retain the top $5\\%$ of the salient tokens using a dynamic distance-based threshold, as proposed in our paper. Note that efficiency decreases as more tokens are retained. This demonstrates that our distance-based approach better balances performance and efficiency than the fixed retention ratio counterpart. ", "page_idx": 15}, {"type": "text", "text": "Ablation for relations between performance and ratio of merged tokens. We are adding another column in terms of compression ratio to represent the efficiency trade-offs, as shown in table D. The results suggest that setting $\\gamma$ to 0.05 achieves the best balance between performance and efficiency. ", "page_idx": 15}, {"type": "table", "img_path": "sgVOjDqUMT/tmp/e97aeaa06c369cc4ae31504cab710a4b8079ca997e36f5cceb5c15847b6a1e71.jpg", "table_caption": ["Table C: Performance comparison of LLaMA2-7B under different retention ratios of overall tokens. "], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "sgVOjDqUMT/tmp/db209015c58e8036119211b1a448efd84b496e246cf03a6ad5a4e7c154e730ea.jpg", "table_caption": ["Table D: Performance and compression ratios of LLaMA-2-7B at various retention thresholds $\\gamma$ in Eq. (4). "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Benchmark of the computational overhead of different components. As shown in Table table E, the reparametrization process, including the computation of magnitude and direction, takes $0.093~\\mathrm{ms}$ $(0.031\\;\\mathrm{ms}+0.062\\;\\mathrm{ms})$ ), and the restoration process, including the computation of the distance matrix and token replacement, takes $0.116~\\mathrm{ms}$ $\\mathrm{0.061~ms+0.055~ms})$ ). These times indicate a negligible computational overhead compared to the overall attention computation, which takes $9.756~\\mathrm{ms}$ . The running time is measured in milliseconds per attention layer with a batch size of 1 on LLaMA-2-7B. ", "page_idx": 16}, {"type": "table", "img_path": "sgVOjDqUMT/tmp/7bad43a2d2cec73fca8d1f9c68d689ab92a3e123f33777dd585cdb701da4ef5d.jpg", "table_caption": ["Table E: We benchmark the different components in the reparameterization and restoration stages using LLaMA-2-7B. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Efficiency comparison without quantization. table F demonstrates that MiniCache\u2019s cross-layer merging without quantization achieves almost lossless performance compared to the FP16 baseline, with a compression ratio of 1.53. In contrast, 2-bit quantization causes performance degradation. For instance, on the GSM8K dataset, performance drops from 0.159 to 0.127, but the compression ratio improves to 3.95. Our MiniCache method, focusing on the depth dimension, can complement any quantization and existing KV cache compression methods. The results indicate that combining cross-layer merging with 4-bit quantization achieves the optimal balance between performance and efficiency. ", "page_idx": 16}, {"type": "table", "img_path": "sgVOjDqUMT/tmp/2e8803851e4b737dc8fa8bea0a5050a88c22ff8952f5a4b3d3b40c86ee561386.jpg", "table_caption": ["Table F: Performance comparison and compression ratios across different methods. MiniCache is orthogonal to existing quantization techniques. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "B Additional Related Work ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Preliminaries of KV cache in LLMs. LLM inference has been significantly improved in recent works [9, 13, 77] by optimizing the KV cache management. Overall, this line of research is typically done in two steps: 1) First, at the prefilling stage, LLM calculates the initial KV caches at each attention layer based on the input sequence and decodes the first token. Second, 2) at the decoding stage, LLM autoregressively predicts the next token, where its KV cache at each attention layer is added to the overall caches. Existing works have compressed KV cache in different aspects (e.g., quantization [11, 12], token pruning [14, 16] ). ", "page_idx": 16}, {"type": "text", "text": "KV cache compression. In the prior study, various strategies for enhancing efficient transformer architectures are discussed, covering a spectrum of techniques aimed at optimizing performance and managing resource constraints. These methods include attention optimization [48, 49, 80], grouping queries [44, 45], sparse KV caching [16, 81, 82], shrinking tokens [15, 83], and improving long-context generation. Significant contributions come from projects such as H2O [15], GEAR [15], and KIVI [11]. Additionally, efforts to alleviate KV cache bottlenecks include strategies like multi-query attention [44] and multi-group attention [45], which propose reducing the number of heads in the KV cache. However, these methods often necessitate retraining or fine-tuning models. Other approaches focus on diminishing the size of the KV cache by selectively evicting less important tokens [14] and enhancing the system architecture through technologies like offloading the KV cache [84] or integrating techniques such as virtual memory and paging [85] into the attention mechanism. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "C Discussions and Limitations ", "text_level": 1, "page_idx": 17}, {"type": "table", "img_path": "sgVOjDqUMT/tmp/06dbf73b2dea0adbe1918bca58a6ea0c47cd1b18640d1a89f655eca0cf3b1755.jpg", "table_caption": ["Table G: Comparison of performance using different cross-layer merging strategies. The experiment shows that SLERP has the best performance across three datasets. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Alternative merging function. During our preliminary exploration, we initially considered an alternative, simpler merge function for cross-layer compression: maximum norm-preserving interpolation. This function is designed to maintain the maximum norm of the vectors involved, ensuring that the most significant features are preserved during the merging process. The maximum norm-preserving interpolation in terms of $F_{\\mathrm{max}}$ can be defined as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\nF_{\\operatorname*{max}}(x^{l},x^{l-1})=\\frac{\\bar{x}^{l,l-1}}{\\|\\bar{x}^{l,l-1}\\|}\\cdot\\mathrm{Max}(\\|x^{l}\\|,\\|x^{l-1}\\|).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Here $\\bar{\\pmb{x}}^{l,l-1}$ represents the average vector between $\\mathbf{\\boldsymbol{x}}^{l}$ and $\\pmb{x}^{l-1}$ . The function $F_{\\mathrm{max}}$ ensures that the merged vector preserves the direction of the average vector while scaling it to the maximum norm of the original KV states. Compared to the SLERP-based merge function, $F_{\\mathrm{max}}$ has less computational overhead and lower memory consumption. However, it is less accurate than SLERP. The choice between using $F_{\\mathrm{SLERP}}$ or $F_{\\mathrm{max}}$ depends on the specific requirements of the application. In our study, we primarily use SLERP to maximize performance, as also shown in Table G. ", "page_idx": 17}, {"type": "text", "text": "Societal impact. Our work shows a preliminary exploration of KV cache Compression in the depth dimension, a relatively unexplored yet critically bottlenecked area in large language models (LLMs). The proposed MiniCache provides a solution to improve the efficiency of LLM generation and is adaptable to existing intra-layer-based KV cache pruning and quantization technologies. Additionally, we proposed a simple yet effective approach that merges similar KV cache states in a cross-layer manner and effectively restores performance through a novel restoration technique. Our observations indicate that cross-layer similarity and mergeability can be applied in broad-efficient applications for post-training optimization in low-resource scenarios, such as deployment on mobile devices. Moreover, with effective optimization of KV cache memory demand, MiniCache further enhances long-context generation, which is a crucial paradigm for real-world applications, such as understanding concepts in textbooks. We aim for our work to advance the boundaries of two key challenges in the LLM industry and research: batch inference and long-context generation. ", "page_idx": 17}, {"type": "text", "text": "Furthermore, in addition to the significant contributions of MiniCache for KV cache Compression, several challenges remain that are common to LLMs. Issues such as the truthfulness and security of LLMs are still unresolved. Ensuring the accuracy and reliability of generated content is critical, as LLMs can sometimes produce plausible but incorrect or misleading information. Additionally, safeguarding against security vulnerabilities, such as adversarial attacks or data leakage, is paramount to maintaining the integrity and confidentiality of user interactions. Addressing these challenges requires ongoing research and development to enhance the robustness and trustworthiness of LLMs. This effort must proceed alongside advancements in computational efficiency and performance, as exemplified by innovations like MiniCache. ", "page_idx": 17}, {"type": "text", "text": "Limitations. The current merging algorithm based on Spherical Linear Interpolation (SLERP) has its limitations. SLERP is suitable only for merging two vectors and uses an interpolation approach that restricts our algorithm from merging multiple layers simultaneously and maximizing the compression ratio in further states. This limitation impacts the overall efficiency of KV cache compression and underscores the need for advanced techniques capable of handling more complex merging scenarios. ", "page_idx": 17}, {"type": "text", "text": "Future research should focus on developing more sophisticated algorithms that can overcome these constraints, thereby enhancing the compression capabilities and overall performance of LLMs. ", "page_idx": 18}, {"type": "text", "text": "D Additional Implementation Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Overview of the inference algorithm. The MiniCache inference implementation, as shown in Alg. 1, involves several key steps. When the interface starts, In the prefilling phase, we define the merging starting layer $S$ . Before reaching layer $S$ , the inference uses the original attention and cache logic. From layer $S$ onward, we implement our merging algorithm, which operates in a cross-layer manner and is applied only at odd-numbered layers. During merging, we fetch the KV cache from the previous layer and save the merged shared states into the current layer\u2019s KV cache. To reduce memory usage, we then remove the cache of the previous layer. Additionally, we store the magnitudes vector and retention-sensitive tokens for each layer. During the decoding phase, two scenarios arise after layer $S$ . For even-numbered layers (first round), since the KV cache has been removed during the prefilling phase, we refer to the next layer $(l+1)$ to fetch the shared KV cache states. We then perform approximated scale restoration and retention token recovery. The new KV states from this phase are stored for use in the next round. In the second round, which involves odd-numbered layers, we use the new KV tokens from both the previous and current layers. After the restoration phase, we perform the merge operations and update the shared KV cache states in the stack. ", "page_idx": 18}, {"type": "text", "text": "Cross-Layer merging and restoration algorithm. As outlined in Alg. 2, MiniCache algorithm involves several crucial steps to ensure efficient memory usage while maintaining the integrity of the Key-Value (KV) Cache states. Initially, given the KV cache $E_{k,v}^{l}$ , norm values $\\|X_{k,v}^{l}\\|$ unmerged tokens $R_{k,v}^{l}$ , retention indices $\\mathbb{I}_{k,v}$ , and the next tokens $t^{l}$ , $t^{l-1}$ , the algorithm proceeds by rescaling the magnitude of the KV pairs. Specifically, $\\hat{X}_{k}^{l}$ and $\\hat{X}_{v}^{l}$ are computed by multiplying the normalized KV pairs $E_{k,v}^{l}$ with their respective magnitude norms $\\|X_{k,v}^{l}\\|$ . Following this, the algorithm restores unmerged tokens using the retention indices, updating $\\hat{X}_{k}^{l}$ and $\\hat{X}_{v}^{l}$ accordingly. Next, the new tokens $\\pmb{t}_{k}$ and $\\scriptstyle t_{v}$ are concatenated to the rescaled KV pairs along the token dimension. This augmented KV cache undergoes a softmax attention mechanism where the attention scores $\\pmb{A}$ are computed by taking the dot product of the query token $\\pmb{t}_{q}$ with the transposed keys $(\\hat{X}_{k}^{l})^{\\top}$ . The output token $\\scriptstyle t_{O}$ is then obtained by multiplying the attention scores $\\pmb{A}$ with the values $\\hat{X}_{v}^{l}$ . In cases where the previous token $t^{l-1}$ exists, the algorithm performs a compression step. It concatenates the existing KV cache $E_{k,v}^{l}$ with the merged tokens resulting from the current and previous layers, effectively reducing redundancy and optimizing memory. If $\\pmb{t}^{l-1}$ is not available, the KV cache is updated by simply concatenating $E_{k,v}^{l}$ with the new tokens $\\pmb{t}_{k,v}^{l}$ , deferring the compression until the next iteration. The final output token $\\scriptstyle t_{O}$ is then returned, concluding the decoding process. In the merging function, the algorithm normalizes the KV pairs from both the current and previous layers. It calculates the angular distance $\\Omega$ between the normalized vectors, ensuring that the interpolation occurs along the shortest path on the unit sphere. The merged KV cache is then obtained by weighted interpolation of the normalized vectors, preserving the geometric and semantic integrity of the original states. This comprehensive process allows MiniCache to achieve substantial memory efficiencies while maintaining the functional characteristics of the KV pairs across transformer layers. ", "page_idx": 18}, {"type": "text", "text": "MiniCache execution flow. Figure A delineates the pre-fliling and decoding logic for the MiniCache framework, which incorporates cross-layer merging and error suppression to achieve memory efficiency and maintain functional integrity. Initially, in Step 1, the KV cache is fetched from the previous layer (Layer $L-1$ ) during the pre-fliling phase. In Step 2, the fetched KV pairs from the current layer $\\chi^{\\check{L}}$ are merged with the KV pairs from the preceding layer $\\chi^{L-1}$ , reducing redundancy through a merge function. Subsequently, in Step 3, the merged KV pairs are cached for future use, representing a consolidated data set from multiple layers. During the decoding phase, Step 4 involves deleting unnecessary or redundant KV pairs to optimize memory usage. In Step 5, the decoder fetches the required KV pairs from the cache for output generation. Step 6 applies error suppression mechanisms to the fetched KV pairs, including rescaling and retention recovery, to minimize errors introduced during the merging and compression processes. Finally, in Step 7, the cache is updated with the final KV pairs post-error suppression and adjustments, ensuring the cache contains the most accurate and efficient representation of the KV pairs for subsequent layers. This comprehensive approach guarantees substantial memory efficiencies while preserving the critical functional characteristics of the original KV pairs across transformer layers. ", "page_idx": 18}, {"type": "image", "img_path": "sgVOjDqUMT/tmp/f38bb43a469daffe26ef8963959be33ff617b8da4d1250ae53672fd5f45f37fa.jpg", "img_caption": ["Figure A: Overall prefilling and decoding logic for MiniCache involves performing cross-layer merging and recovery within our framework. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "E Detailed Efficiency Derivation ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we provide a detailed derivation of the memory efficiency improvements outlined in Section 4.3. ", "page_idx": 19}, {"type": "text", "text": "First, we consider the original KV cache memory usage, which is given by: ", "page_idx": 19}, {"type": "equation", "text": "$$\n4b r h(s+n).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Here, $r$ is the number of layers, $b$ is the batch size, $h$ is the hidden size, $s$ is the input sequence length, and $n$ is the output sequence length. To improve efficiency, we begin merging layers starting from the midpoint, $\\textstyle S^{\\ast}={\\frac{1}{2}}r$ , by consolidating the KV cache states of every two layers into a single shared state space. The memory usage derivation proceeds as follows: for the unmerged part of the cache (from layer 1 to $S$ ): ", "page_idx": 19}, {"type": "image", "img_path": "sgVOjDqUMT/tmp/8337f46fc64e693f3645f0adf32550e788d0fc7db917110209647ff1c4e0fafe.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "equation", "text": "$$\n4b r h(s+n)\\cdot{\\frac{1}{2}}=2b r h(s+n).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For the merged part of the cache (from layer $S+1$ to $r$ ): ", "page_idx": 20}, {"type": "equation", "text": "$$\n4b r h(s+n)\\cdot{\\frac{1}{2}}\\cdot{\\frac{1}{2}}=b r h(s+n).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Combining these two parts, the total memory usage is: ", "page_idx": 20}, {"type": "equation", "text": "$$\n2b r h(s+n)+b r h(s+n)=3b r h(s+n).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Next, we consider the additional memory cost incurred during the restoration process. During this phase, we save additional normalized vectors for the layers in the KV cache. These vectors are of shape $\\mathbb{R}^{b\\times s\\times1}$ , which means they have a single channel dimension compared to the fully ranked original KV states. ", "page_idx": 20}, {"type": "text", "text": "The additional normalized vectors for layers from $S$ onwards are given by: ", "page_idx": 20}, {"type": "text", "text": "Algorithm 2: The MiniCache Prefill & Decoding Compression Algorithm ", "page_idx": 21}, {"type": "text", "text": "Hyperparameter: number of layers $L$ , merging beginning layer $S$ 1 procedure MiniCache Prefill: Input: KV cache from current layer $l$ : $X_{k,v}^{l}\\in\\mathbb{R}^{2\\times t_{\\mathrm{prompt}}\\times d}$ , KV cache from previous layer $l-1$ : $X_{k,v}^{l-1}\\in\\mathbb{R}^{2\\times t_{\\mathrm{prompt}}\\times d}$ , token retention threshold: $\\gamma$ 2 $\\pmb{E}_{k}^{l},\\|\\pmb{X}_{k}^{l}\\|,\\|\\pmb{X}_{k}^{l-1}\\|,\\Omega_{k}\\leftarrow\\operatorname{Merge}(\\pmb{X}_{k}^{l},\\pmb{X}_{k}^{l-1})$ ; 3 $\\pmb{E}_{v}^{l},\\lVert\\pmb{X}_{v}^{l}\\rVert,\\lVert\\pmb{X}_{v}^{l-1}\\rVert,\\Omega_{v}\\leftarrow\\operatorname{Merge}(\\pmb{X}_{v}^{l},\\pmb{X}_{v}^{l-1})$ ; 4 $\\mathbf{\\deltaE}_{k,v}^{l}\\in\\mathbb{R}^{t_{\\mathrm{prompt}}\\times d},\\|\\mathbf{X}_{k,v}^{l,l-1}\\|\\in\\mathbb{R}^{4\\times t_{\\mathrm{prompt}}\\times1}\\mathrm{~};$ ; compress i on output norms fo r  rescaling 5 $\\begin{array}{r}{d(\\mathbf{X}^{l},\\mathbf{X}^{l-1})_{k,v}=\\frac{1}{\\pi}\\cdot\\Omega_{k,v}\\mathbf{\\Omega}/\\prime}\\end{array}$ distance metrics 6 $\\mathbb{I}_{k,v}=\\{i\\mid d_{i}<d_{\\operatorname*{min}}+\\left(d_{\\operatorname*{max}}-d_{\\operatorname*{min}}\\right)\\cdot\\gamma\\}$ // retention indices 7 ${\\pmb R}_{k}^{l,l-1}\\gets{\\pmb X}_{k}^{l,l-1}[{\\mathbb I}_{k}]$ , ${\\pmb{R}}_{v}^{l,l-1}\\gets{\\pmb{X}}_{v}^{l,l-1}[\\mathbb{I}_{v}]\\mathrm{~}//\\$ unmerged tokens 8 return $\\pmb{E}_{k,v}^{l},\\|\\pmb{X}_{k,v}^{l,l-1}\\|,\\pmb{R}_{k}^{l,l-1},\\pmb{R}_{v}^{l,l-1},\\mathbb{I}_{k,v}$ 9 procedure MiniCache Decoding: Input: KV cache: $E_{k,v}^{l}\\in\\mathbb{R}^{2\\times t_{\\mathrm{prompt}}\\times d}$ , Norm: $\\|\\boldsymbol{X}_{k,v}^{l}\\|\\in\\mathbb{R}^{2\\times t_{\\mathrm{prompt}}\\times1}$ , Unmerged Tokens: $R_{k,v}^{l}\\in\\mathbb{R}^{2\\times\\gamma\\cdot t_{\\mathrm{prompt}}\\times d}$ , Retention indices: $\\mathbb{I}_{k,v}\\in\\mathbb{R}^{2\\times\\gamma\\cdot t_{\\mathrm{prompt}}\\times1}$ , Next Token: $\\pmb{t}^{l}\\in\\mathbb{R}^{1\\times d}$ , $\\pmb{t}^{l-1}\\in\\mathbb{R}^{1\\times d}$ 0 $\\begin{array}{r}{\\hat{X}_{k}^{l}\\leftarrow E_{k}^{l}\\cdot\\frac{\\Vert{X}_{k}^{l}\\Vert}{\\Vert E_{k}^{l}\\Vert}\\,\\hat{X}_{v}^{l}\\leftarrow E_{v}^{l}\\cdot\\frac{\\Vert{X}_{v}^{l}\\Vert}{\\Vert E_{v}^{l}\\Vert}\\,//}\\end{array}$ magnitude rescale 1 $\\hat{\\pmb X}_{k}^{l}[\\mathbb{I}_{k}]=\\pmb R_{k}^{l}\\ \\hat{\\pmb X}_{v}^{l}[\\mathbb{I}_{v}]=\\pmb R_{v}^{l}\\ /$ / token restoration 12 $\\hat{X}_{k}^{l}\\gets\\mathrm{Concat}(\\hat{X}_{k}^{l},t_{k},\\mathrm{dim}{=}\\mathrm{tok}$ en) $\\hat{\\mathbf{X}}_{v}^{l}\\gets\\mathrm{Concat}(\\hat{X}_{v}^{l},t_{v},\\mathrm{dim}{=}\\mathrm{token})$ $\\boldsymbol{A}\\gets\\operatorname{Softmax}(\\boldsymbol{t}_{\\boldsymbol{q}}\\cdot(\\hat{\\mathbf{X}}_{\\boldsymbol{k}}^{l})^{\\top})\\:\\boldsymbol{t}_{\\mathcal{O}}\\gets\\boldsymbol{A}\\cdot\\hat{\\mathbf{X}}_{\\boldsymbol{v}}^{l}$ if $t^{l-1}$ exists then 3 $\\textsc{K V c a c h e}\\leftarrow\\mathrm{Concat}(E_{k,v}^{l},\\mathrm{Merge}(t_{k,v}^{l},t_{k,v}^{l-1}),$ , dim=token) $//$ perform compression 4 else 5 $\\lfloor\\mathrm{~\\tiny~K~V~cache}\\leftarrow\\mathrm{Concat}(E_{k,v}^{l},t_{k,v}^{l},$ dim=token) // wait for compression 6 return $\\scriptstyle t_{O}$ 7 function MiniCache $\\mathsf{M e r g e}(X^{l},X^{l-1},t)\\colon$ 18 $\\begin{array}{r}{\\vec{X}^{l}\\leftarrow\\frac{\\mathbf{X}^{l}}{\\|\\mathbf{X}^{l}\\|}}\\end{array}$ 19 $\\begin{array}{r}{\\vec{X}^{l-1}\\leftarrow\\frac{\\mathbf{X}^{l-1}}{\\lVert\\mathbf{X}^{l-1}\\rVert}}\\end{array}$ 20 $\\begin{array}{r}{\\Omega\\leftarrow\\operatorname{arccos}\\left(\\frac{\\mathbf{X}_{T}^{l}\\cdot\\mathbf{X}_{T}^{l-1}}{\\|\\mathbf{X}_{T}^{l}\\|\\|\\mathbf{X}_{T}^{l-1}\\|}\\right)}\\end{array}$ 21 $\\begin{array}{r}{E\\gets\\frac{\\sin((1-t)\\Omega)}{\\sin(\\Omega)}\\vec{X}^{\\,l}+\\frac{\\sin(t\\Omega)}{\\sin(\\Omega)}\\vec{X}^{\\,l-1}}\\end{array}$ 22 return $E,\\|X^{l}\\|,\\|X^{l-1}\\|,\\Omega$ ", "page_idx": 21}, {"type": "equation", "text": "$$\nb r(s+n)\\cdot2=2b r(s+n).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We also introduce a retention threshold, which we set to 0.05. This means that $5\\%$ of the KV cache tokens are retained without compression: ", "page_idx": 21}, {"type": "equation", "text": "$$\nb r h(0.05(s+n)).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Combining these terms, the total additional memory for the restoration process is: ", "page_idx": 21}, {"type": "equation", "text": "$$\n2b r(s+n)+0.1b r h(s+n).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Finally, summing the compressed memory usage and the restoration memory cost, the overall memory requirement is: ", "page_idx": 21}, {"type": "equation", "text": "$$\n3b r h(s+n)+2b r(s+n)+0.1b r h(s+n).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "This can be simplified by grouping the common factors: ", "page_idx": 22}, {"type": "equation", "text": "$$\nb r(s+n)\\left(3h+2+0.1h\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Simplifying the expression inside the parentheses, we get: ", "page_idx": 22}, {"type": "equation", "text": "$$\nb r(s+n)\\left(3.1h+2\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Therefore, the total memory cost for the KV cache in the MiniCache Framework is: ", "page_idx": 22}, {"type": "equation", "text": "$$\nb r(s+n)(3.1h+2).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "This detailed derivation confirms the efficiency improvements discussed in Section 4.3, highlighting the significant reduction in memory usage achieved through our layer merging and restoration strategies. ", "page_idx": 22}, {"type": "text", "text": "F Detailed Experiment Results ", "text_level": 1, "page_idx": 22}, {"type": "table", "img_path": "sgVOjDqUMT/tmp/5894b20d7b60f8509d46984a39778ced24975f86a8d5b39e7c193b47a791c35b.jpg", "table_caption": ["Table H: Detailed performance comparison on GSM8K dataset with LLaMA-3-70B. "], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "sgVOjDqUMT/tmp/8cd37e5cfe01424de1b419da09abfe50382efa61c633d8aee8687aee6a8360f1.jpg", "table_caption": ["Table I: Detailed performance comparison on COQA dataset with LLaMA-3-70B. "], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "sgVOjDqUMT/tmp/abdb287c4674df40f5b4b3f0ea074606a1961bb851422d7162bec4ec8af4c3ad.jpg", "table_caption": ["Table J: Detailed performance comparison on TruthfulQA dataset with LLaMA-3-70B. "], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "sgVOjDqUMT/tmp/e71b3eeab98784c991d08b93a00479ce37f5a175c80cb70ca6023b07fbf202b4.jpg", "table_caption": ["Table K: Detailed performance comparison on GSM8K dataset with LLaMA-3-8B. "], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "sgVOjDqUMT/tmp/92db5af0e56a8a261ec3cecd43069c5555163d351daf4c309efd3f3fcc0ad8f8.jpg", "table_caption": ["Table L: Detailed performance comparison on COQA dataset with LLaMA-3-8B. "], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "sgVOjDqUMT/tmp/9c9898291d0535e4bf43ef795aad6d5931b159fb1b2e42aa9f8bbee983f5face.jpg", "table_caption": ["Table M: Detailed performance comparison on TruthfulQA dataset with LLaMA-3-8B. "], "table_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "sgVOjDqUMT/tmp/d322d665d991a01c5261d75dfb31bd11eb3dcd7c56cb31b80881acedebd17b06.jpg", "table_caption": ["Table N: Detailed performance comparison on GSM8K dataset with Mixtral-8x7B. "], "table_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "sgVOjDqUMT/tmp/bf2bd24c3cf8d8483a578fe1bea6ec9db26ad6a4a61d380114eccfc5c3dd7647.jpg", "table_caption": ["Table O: Detailed performance comparison on COQA dataset with Mixtral-8x7B. "], "table_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "sgVOjDqUMT/tmp/20648d3ff109ed7ed2af1f2a6535bf2660133e5595b45231df302ff557f89eb7.jpg", "table_caption": ["Table P: Detailed performance comparison on TruthfulQA dataset with Mixtral-8x7B. "], "table_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "sgVOjDqUMT/tmp/c0c27b9d9b67028d5ec8fc1cb1f9fec32a076bad50164065572399b6f7ada971.jpg", "table_caption": ["Table Q: Detailed performance comparison on GSM8K dataset with Phi-3-Mini. "], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "sgVOjDqUMT/tmp/bff954513e1a42f4a5b51a286cb1eac7de9f61bba86101016ce429ab12ac3961.jpg", "table_caption": ["Table R: Detailed performance comparison on COQA dataset with Phi-3-Mini. "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "Table S: Detailed performance comparison on TruthfulQA dataset with Phi-3-Mini. ", "page_idx": 28}, {"type": "table", "img_path": "sgVOjDqUMT/tmp/b0c5d5f82587678dbc190ac0d601a58fabca913c3cca1dce371b534960019603.jpg", "table_caption": [], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We have properly made the main claims in the abstract and introduction. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 28}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: Yes, we include the limitations in the Appendix. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper. \u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. ", "page_idx": 28}, {"type": "text", "text": "\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 29}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: See sections in methodology and appendixes. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 29}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: We provide detailed experiments setup on Experiment sections, we also have detailed algorithm in appendix for Reproducibility. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often ", "page_idx": 29}, {"type": "text", "text": "one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. ", "page_idx": 30}, {"type": "text", "text": "\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 30}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We provide code after acceptance. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 30}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We provide implementation details. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 31}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [No] ", "page_idx": 31}, {"type": "text", "text": "Justification: We verify our results on huge size of language models, repeat computation are expensive. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 31}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We give the competing requirements in the experiment setup. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 31}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Yes we do. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 32}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: We have included the broader impacts in the Conclusion. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 32}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: Our methodology not release new data or models. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 32}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: Yes, we give proper citations and reference for used resources. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 33}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: Our method is training free, can accelerate the LLMs inference . ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 33}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: This paper does not study human subjects. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 33}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: We are not subject to the approval of IRB ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 34}]