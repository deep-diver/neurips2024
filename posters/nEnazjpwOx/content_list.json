[{"type": "text", "text": "Diffusion Models Meet Contextual Bandits with Large Action Spaces ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Efficient exploration in contextual bandits is crucial due to their large action space,   \n2 where uninformed exploration can lead to computational and statistical inefficien  \n3 cies. However, the rewards of actions are often correlated, which can be leveraged   \n4 for more efficient exploration. In this work, we use pre-trained diffusion model pri  \n5 ors to capture these correlations and develop diffusion Thompson sampling (dTS).   \n6 We establish both theoretical and algorithmic foundations for dTS. Specifically,   \n7 we derive efficient posterior approximations (required by dTS) under a diffusion   \n8 model prior, which are of independent interest beyond bandits and reinforcement   \n9 learning. We analyze dTS in linear instances and provide a Bayes regret bound   \n10 highlighting the benefits of using diffusion models as priors. Our experiments   \n11 validate our theory and demonstrate dTS\u2019s favorable performance. ", "page_idx": 0}, {"type": "text", "text": "12 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "13 A contextual bandit is a popular and practical framework for online learning under uncertainty [Li   \n14 et al., 2010]. In each round, an agent observes a context, takes an action, and receives a reward based   \n15 on the context and action. The goal is to maximize the expected cumulative reward over $n$ rounds,   \n16 striking a balance between exploiting actions with high estimated rewards from available data and   \n17 exploring other actions to improve current estimates. This trade-off is often addressed using either   \n18 upper confidence bound (UCB) [Auer et al., 2002] or Thompson sampling (TS) [Scott, 2010].   \n19 The action space in contextual bandits is often large, resulting in less-than-optimal performance   \n20 with standard exploration strategies. Luckily, actions usually exhibit correlations, making efficient   \n21 exploration possible as one action may inform the agent about other actions. In particular, Thompson   \n22 sampling offers remarkable flexibility, allowing its integration with informative priors [Hong et al.,   \n23 2022b] that capture these correlations. Inspired by the achievements of diffusion models [Sohl  \n24 Dickstein et al., 2015, Ho et al., 2020], which effectively approximate complex distributions [Dhariwal   \n25 and Nichol, 2021, Rombach et al., 2022], this work captures action correlations by employing   \n26 diffusion models as priors in contextual Thompson sampling.   \n27 We illustrate the idea using video streaming. The objective is to optimize watch time for a user $j$   \n28 by selecting a video $i$ from a catalog of $K$ videos. Users $j$ and videos $i$ are associated with context   \n29 vectors $x_{j}$ and unknown video parameters $\\theta_{i}$ , respectively. User $j$ \u2019s expected watch time for video $i$   \n30 is linear as $x_{j}^{\\top}\\theta_{i}$ . Then, a natural strategy is to independently learn video parameters $\\theta_{i}$ using LinTS   \n31 or LinUCB [Agrawal and Goyal, 2013a, Abbasi-Yadkori et al., 2011], but this proves statistically   \n32 inefficient for larger $K$ . Fortunately, the reward when recommending a movie can provide informative   \n33 insights into other movies. To capture this, we leverage offilne estimates of video parameters denoted   \n34 by ${\\hat{\\theta}}_{i}$ and build a diffusion model on them. This diffusion model approximates the video parameter   \n35 distribution, capturing their dependencies. This model enriches contextual Thompson sampling as a   \n36 prior, effectively capturing complex video dependencies while ensuring computational efficiency.   \n37 We introduce a framework for contextual bandits with diffusion model priors, upon which we develop   \n38 diffusion Thompson sampling (dTS) that is both computationally and statistically efficient. dTS   \n39 requires fast updates of the posterior and fast sampling from the posterior, both of which are achieved   \n40 through our novel efficient posterior approximations. These approximations become exact when   \n41 both the diffusion model and likelihood are linear. We establish a bound on dTS\u2019s Bayes regret for   \n42 this specific case, highlighting the advantages of using diffusion models as priors. Our empirical   \n43 evaluations validate our theory and demonstrate dTS\u2019s strong performance across various settings.   \n44 Diffusion models were applied in offilne decision-making [Ajay et al., 2022, Janner et al., 2022, Wang   \n45 et al., 2022], but their use in online learning was only recently explored by Hsieh et al. [2023], who   \n46 focused on multi-armed bandits without theoretical guarantees. Our work extends Hsieh et al. [2023]   \n47 in two ways. First, we apply the concept to the broader contextual bandit, which is more practical and   \n48 realistic. Second, we demonstrate that with diffusion models parametrized by linear score functions   \n49 and linear rewards, we can derive exact closed-form posteriors without approximations. These exact   \n50 posteriors are valuable as they enable theoretical analysis (unlike Hsieh et al. [2023], who did not   \n51 provide theoretical guarantees) and motivate efficient approximations for non-linear score functions   \n52 in contextual bandits, addressing gaps in Hsieh et al. [2023]\u2019s focus on multi-armed bandits.   \n53 A key contribution, beyond applying diffusion models in contextual bandits, is the efficient com  \n54 putation and sampling of the posterior distribution of a $d\\!.$ -dimensional parameter $\\theta\\mid H_{t}$ , with $H_{t}$   \n55 representing the data, when using a diffusion model prior on $\\theta$ . This is relevant not only to bandits   \n56 and reinforcement learning but also to a broader range of applications [Chung et al., 2022]. To   \n57 motivate our approximations, we start with exact closed-form solutions for cases where both the   \n58 score functions of the diffusion model and the likelihood are linear. These solutions form the basis for   \n59 our approximations for non-linear score functions, demonstrating both strong empirical performance   \n60 and computational efficiency. Our approach avoids the computational burden of heavy approximate   \n6 sampling algorithms required for each latent parameter. For a detailed comparison with existing   \n62 studies, see Appendix A, where we discuss diffusion models in decision-making, structured bandits,   \n63 approximate posteriors, and more. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "64 2 Setting ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "65 The agent interacts with a contextual bandit over $n$ rounds. In round $t\\in[n]$ , the agent observes a   \n66 context $X_{t}\\in\\mathcal{X}$ , where $\\mathcal{X}\\subseteq\\mathbb{R}^{d}$ is a context space, it takes an action $A_{t}\\in[K]$ , and then receives a   \n67 stochastic reward $Y_{t}\\in\\mathbb{R}$ that depends on both the context $X_{t}$ and the taken action $A_{t}$ . Each action   \n68 $i\\in[K]$ is associated with an unknown action parameter $\\theta_{\\ast,i}\\in\\mathbb{R}^{d}$ , so that the reward received in   \n69 round $t$ is $Y_{t}\\sim P(\\cdot\\mid X_{t};\\theta_{*,A_{t}})$ , where $P(\\cdot\\mid x;\\theta_{*,i})$ is the reward distribution of action $i$ in context   \n70 $x$ . Throughout the paper, we assume that the reward distribution is parametrized as a generalized   \n71 linear model (GLM) [McCullagh and Nelder, 1989]. That is, for any $x\\in\\mathscr{X}$ , $P(\\cdot\\mid x;\\theta_{*,i})$ is an   \n72 exponential-family distribution with mean $g(x^{\\top}\\theta_{*,i})$ , where $g$ is the mean function. For example, we   \n73 recover linear bandits when $P(\\cdot\\mid x;\\theta_{*,i})=N(\\cdot;x^{\\top}\\theta_{*,i},\\sigma^{2})$ where $\\sigma>0$ is the observation noise.   \n74 Similarly, we recover logistic bandits [Filippi et al., 2010] if we let $g(u)=(1+\\exp(-u))^{-1}$ and   \n75 $P(\\cdot\\mid x;\\theta_{*,i})=\\operatorname{Ber}(g(x^{\\top}\\theta_{*,i}))$ , where $\\mathrm{Ber}(p)$ be the Bernoulli distribution with mean $p$ .   \n76 We consider the Bayesian bandit setting [Russo and Van Roy, 2014, Hong et al., 2022b], where the   \n77 action parameters $\\theta_{*,i}$ are assumed to be sampled from a known prior distribution. We proceed to   \n78 define this prior distribution using a diffusion model. The correlations between the action parameters   \n79 $\\theta_{*,i}$ are captured through a diffusion model, where they share a set of $L$ consecutive unknown latent   \n80 parameters $\\psi_{\\ast,\\ell}\\in\\mathbb{R}^{d}$ for $\\ell\\in[L]$ . Precisely, the action parameter $\\theta_{*,i}$ depends on the $L$ -th latent   \n81 parameter $\\psi_{*,L}$ as $\\theta_{*,i}\\mid\\psi_{*,1}\\sim\\mathcal{N}(f_{1}(\\psi_{*,1}),\\Sigma_{1})$ , where the score function $f_{1}:\\mathbb{R}^{d}\\to\\mathbb{R}^{d}$ is known.   \n82 Also, the $\\ell\\!-\\!1$ -th latent parameter $\\psi_{*,\\ell-1}$ depends on the $\\ell$ -th latent parameter $\\psi_{*,\\ell}$ as $\\psi_{*,\\ell-1}\\mid\\psi_{*,\\ell}\\sim$   \n83 $\\mathcal{N}(f_{\\ell}(\\psi_{*,\\ell}),\\Sigma_{\\ell})$ , where the score function $f_{\\ell}:\\mathbb{R}^{d}\\to\\mathbb{R}^{d}$ is known. Finally, the $L$ -th latent parameter   \n84 $\\psi_{*,L}$ is sampled as $\\psi_{*,L}\\sim\\mathcal{N}(0,\\Sigma_{L+1})$ . We summarize this model in (1) and its graph in Fig. 1.   \n85 ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\psi_{*,L}\\sim{\\cal N}(0,\\Sigma_{L+1})\\,,}\\\\ {\\psi_{*,\\ell-1}\\;|\\;\\psi_{*,\\ell}\\sim{\\cal N}(f_{\\ell}(\\psi_{*,\\ell}),\\Sigma_{\\ell})\\,,\\quad\\forall\\ell\\in[L]/\\{1\\}\\,,}&{\\quad{\\cal N}(0,\\Sigma_{L+1})\\smallskip\\to\\middle(\\psi_{*,\\ell})\\to\\cdots\\brace{\\widehat{(\\psi_{*,1})}\\middle\\to\\bigwedge}^{\\infty}\\middle\\to\\bigwedge_{\\displaystyle\\begin{array}{c}{{1\\hfill\\downarrow}}\\\\ {{1\\hfill\\psi_{*,1}\\sim\\cal N}(f_{1}(\\psi_{*,1}),\\Sigma_{1})\\,,}\\\\ {{1\\hfill\\qquad\\theta_{*,4}\\cdots\\cal N}(f_{1}(\\psi_{*,4}),\\Sigma_{1})\\,,}\\end{array}}\\quad\\psi i\\in[K]\\,,}\\\\ &{\\quad Y_{t}\\;|\\;X_{t},\\theta_{*,A_{t}}\\sim{\\cal P}(\\cdot\\;|\\;X_{t};\\theta_{*,A_{t}})\\,,\\qquad\\quad\\forall t\\in[n]\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "86 The model in (1) represents a Bayesian bandit, where the agent interacts with a bandit instance   \n87 defined by $\\theta_{*,i}$ over $n$ rounds (4-th line in (1)). These action parameters $\\theta_{*,i}$ are drawn from the   \n88 generative process in the first 3 lines of (1). In practice, (1) can be built by pre-training a diffusion   \n89 model on offline estimates of the action parameters $\\theta_{*,i}$ [Hsieh et al., 2023].   \n90 A natural goal for the agent in this Bayesian framework is to minimize its Bayes regret [Russo and Van   \n91 Roy, 2014] that measures the expected performance across multiple bandit instances $\\theta_{*}=(\\theta_{*,i})_{i\\in[K]}$ , ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{B}\\mathcal{R}(n)=\\mathbb{E}\\Big[\\sum_{t=1}^{n}r(X_{t},A_{t,*};\\theta_{*})-r(X_{t},A_{t};\\theta_{*})\\Big]\\,,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "92 where the expectation in (2) is taken over all random variables in (1). Here   \n93 $r(x,i;\\theta_{*})\\ =\\ \\bar{\\mathbb{E}}_{Y\\sim P(\\cdot|x;\\theta_{*,i})}\\left[Y\\right]$ is the expected reward of action $i$ in context $x$ and $A_{t,*}\\;=\\;$   \n94 $\\arg\\operatorname*{max}_{i\\in[K]}r(X_{t},i;\\theta_{*})$ is the optimal action in round $t$ . The Bayes regret is known to capture the   \n95 benefits of using informative priors, and hence it is suitable for our problem. ", "page_idx": 2}, {"type": "text", "text": "96 3 Diffusion contextual Thompson sampling ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "97 We design Thompson sampling that samples the latent and action parameters hierarchically [Lindley   \n98 and Smith, 1972]. Precisely, let $H_{t}=(X_{k},A_{k},Y_{k})_{k\\in[t-1]}$ be the history of all interactions up to   \n99 round $t$ and let $H_{t,i}=(X_{k},A_{k},Y_{k})_{\\{k\\in[t-1];A_{k}=i\\}}$ be the history of interactions with action $i$ up to   \n100 round $t$ . To motivate our algorithm, we decompose the posterior $\\mathbb{P}\\left(\\theta_{*,i}=\\theta\\,|\\,H_{t}\\right)$ recursively as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\theta_{*,i}=\\theta\\mid H_{t}\\right)=\\int_{\\psi_{1:L}}Q_{t,L}(\\psi_{L})\\prod_{\\ell=2}^{L}Q_{t,\\ell-1}(\\psi_{\\ell-1}\\mid\\psi_{\\ell})P_{t,i}(\\theta\\mid\\psi_{1})\\,\\mathrm{d}\\psi_{1:L}\\,,\\quad\\mathrm{where~}\\quad\\psi_{1:L}=\\psi_{1:L}\\prod_{\\ell=2}^{L}Q_{t,\\ell}(\\theta_{1:L})\\,\\mathrm{d}\\psi_{1:L}\\,,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "101 $Q_{t,L}(\\psi_{L})\\,=\\,\\mathbb{P}\\left(\\psi_{*,L}=\\psi_{L}\\,|\\,H_{t}\\right)$ is the latent-posterior density of $\\psi_{*,L}\\ |\\ H_{t}$ . Moreover, for any   \n102 $\\ell\\,\\in\\,[2\\,:\\,L]$ , $Q_{t,\\ell-1}(\\psi_{\\ell-1}\\mid\\psi_{\\ell})\\,=\\,\\mathbb{P}\\left(\\psi_{*,\\ell-1}=\\psi_{\\ell-1}\\,|\\,H_{t},\\dot{\\psi_{*,\\ell}}=\\psi_{\\ell}\\right)$ is the conditional latent  \n103 posterior density of $\\psi_{*,\\ell-1}~\\mid~H_{t},\\psi_{*,\\ell}\\,=\\,\\psi_{\\ell}$ . Finally, for any action $i\\ \\in\\ [K]$ , $P_{t,i}(\\theta\\mid\\psi_{1})\\;=$   \n104 $\\operatorname{\\mathbb{P}}\\left(\\theta_{*,i}=\\theta\\,|\\,H_{t,i},\\psi_{*,1}=\\psi_{1}\\right)$ is the conditional action-posterior density of $\\theta_{*,i}\\mid H_{t,i},\\psi_{*,1}=\\psi_{1}$ .   \n105 The decomposition in (3) inspires hierarchical sampling. In round $t$ , we initially sample the $L$ -th   \n106 latent parameter as $\\psi_{t,L}\\sim Q_{t,L}(\\cdot)$ . Then, for $\\ell\\in[L]/\\{1\\}$ , we sample the $\\ell-1$ -th latent parameter   \n107 given that $\\psi_{*,\\ell}\\,=\\,\\psi_{t,\\ell}$ , as $\\psi_{t,\\ell-1}\\sim Q_{t,\\ell-1}(\\cdot\\mid\\bar{\\psi_{t,\\ell}})$ . Lastly, given that $\\psi_{*,1}\\,=\\,\\psi_{t,1}$ , each action   \n108 parameter is sampled individually as $\\theta_{t,i}\\sim P_{t,i}(\\theta\\mid\\psi_{t,1})$ . This is possible because action parameters   \n109 $\\theta_{*,i}$ are conditionally independent given $\\psi_{*,1}$ . This leads to Algorithm 1, named diffusion Thompson   \n110 Sampling (dTS). dTS requires sampling from the $K+L$ posteriors $P_{t,i}$ and $Q_{t,\\ell}$ . Thus we start by   \n111 providing an efficient recursive scheme to express these posteriors using known quantities. We note   \n112 that these expressions do not necessarily lead to closed-form posteriors and approximation might be   \n113 needed. First, the conditional action-posterior $P_{t,i}(\\cdot\\mid\\psi_{1})$ can be written as ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\nP_{t,i}(\\boldsymbol{\\theta}\\mid\\psi_{1})\\propto\\prod_{k\\in S_{t,i}}P(Y_{k}\\mid X_{k};\\boldsymbol{\\theta})\\mathcal{N}(\\boldsymbol{\\theta};f_{1}(\\psi_{1}),\\Sigma_{1})\\,,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "114 where $S_{t,i}\\,=\\,\\{\\ell\\,\\in\\,[t-1],A_{\\ell}\\,=\\,i\\}$ are the rounds where the agent takes action $i$ up to round $t$   \n115 Moreover, let $\\mathcal{L}_{\\ell}(\\psi_{\\ell})=\\mathbb{P}\\left(H_{t}\\,|\\,\\psi_{*,\\ell}=\\psi_{\\ell}\\right)$ be the likelihood of observations up to round $t$ given that   \n116 $\\psi_{*,\\ell}=\\psi_{\\ell}$ . Then, for any $\\ell\\in[L]/\\{1\\}$ , the $\\ell-1$ -th conditional latent-posterior $Q_{t,\\ell-1}(\\cdot\\mid\\psi_{\\ell})$ is ", "page_idx": 2}, {"type": "equation", "text": "$$\nQ_{t,\\ell-1}(\\psi_{\\ell-1}\\mid\\psi_{\\ell})\\propto\\mathcal{L}_{\\ell-1}(\\psi_{\\ell-1})\\mathcal{N}(\\psi_{\\ell-1},f_{\\ell}(\\psi_{\\ell}),\\Sigma_{\\ell})\\,,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "117 and $Q_{t,L}(\\psi_{L})\\propto\\mathcal{L}_{L}(\\psi_{L})\\mathcal{N}(\\psi_{L},0,\\Sigma_{L+1})$ . All the terms above are known, except the likelihoods   \n118 $\\mathcal{L}_{\\ell}(\\psi_{\\ell})$ for $\\ell\\in[L]$ . These are computed recursively as follows. First, the basis of the recursion is ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{L}_{1}(\\psi_{1})=\\prod_{i=1}^{K}\\int_{\\theta_{i}}\\prod_{k\\in S_{t,i}}P(Y_{k}\\mid X_{k};\\theta_{i})\\mathcal{N}(\\theta_{i};f_{1}(\\psi_{1}),\\Sigma_{1})\\,\\mathrm{d}\\theta_{i}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "119 Then for $\\ell\\in[L]/\\{1\\}$ , the recursive step is $\\begin{array}{r}{\\mathcal{L}_{\\ell}(\\psi_{\\ell})=\\int_{\\psi_{\\ell-1}}\\mathcal{L}_{\\ell-1}(\\psi_{\\ell-1})\\mathcal{N}(\\psi_{\\ell-1};f_{\\ell}(\\psi_{\\ell}),\\Sigma_{\\ell})\\,\\mathrm{d}\\psi_{\\ell-1}.}\\end{array}$ . ", "page_idx": 2}, {"type": "text", "text": "120 All posterior expressions above use known quantities $(f_{\\ell},\\Sigma_{\\ell},P(y\\mid x;\\theta))$ . However, these expres  \n121 sions typically need to be approximated, except when the score functions $f_{\\ell}$ are linear and the reward   \n122 distribution $P(\\cdot\\mid x;\\theta)$ is linear-Gaussian, where closed-form solutions can be obtained with careful   \n123 derivations. These approximations are not trivial, and prior studies often rely on computationally   \n124 intensive approximate sampling algorithms. In the following sections, we explain how we derive our   \n125 efficient approximations which are motivated by the closed-form solutions of linear instances. ", "page_idx": 2}, {"type": "text", "text": "Input: Prior: $f_{\\ell},\\ell\\in[L],\\Sigma_{\\ell},\\ell\\in[L+1]$ , and $P$ .   \nfor $t=1,\\dots,n$ do Sample $\\psi_{t,L}\\sim Q_{t,L}$ (requires fast approximate posterior update and sampling) for $\\ell=L,\\ldots,2$ do Sample $\\psi_{t,\\ell-1}\\sim Q_{t,\\ell-1}(\\cdot\\mid\\psi_{t,\\ell})$ (requires fast approximate posterior update and sampling) for $i=1,\\ldots,K$ do Sample $\\theta_{t,i}\\sim P_{t,i}(\\cdot\\mid\\psi_{t,1})$ (requires fast approximate posterior update and sampling) Take action $A_{t}=\\mathrm{argmax}_{i\\in[K]}r(X_{t},i;\\theta_{t})$ , where $\\theta_{t}=(\\theta_{t,i})_{i\\in[K]}$ Receive reward $Y_{t}\\sim P(\\cdot\\mid X_{t};\\theta_{*,A_{t}})$ and update posteriors $Q_{t+1,\\ell}$ and $P_{t+1,i}$ . ", "page_idx": 3}, {"type": "text", "text": "126 3.1 Linear diffusion model ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "127 Assume the score functions $f_{\\ell}$ are linear such as $f_{\\ell}(\\psi_{*,\\ell})=\\mathrm{W}_{\\ell}\\psi_{*,\\ell}$ for $\\ell\\in[L]$ , where $\\mathrm{W}_{\\ell}\\in\\mathbb{R}^{d\\times d}$   \n128 are known mixing matrices. Then, (1) becomes a linear Gaussian system (LGS) [Bishop, 2006] in   \n129 this case. This model is important, both in theory and practice. For theory, it leads to closed-form   \n130 posteriors when the reward distribution is linear-Gaussian as $P(\\cdot\\mid x;\\theta_{*,i})\\stackrel{\\cdot}{=}N(\\cdot;x^{\\top}\\theta_{*,i},\\sigma^{2})$ . This   \n131 allows bounding the Bayes regret of dTS. For practice, the posterior expressions are used to motivate   \n132 efficient approximations for the general case in (1) as we show in Section 3.2.   \n133 The reward distribution is parameterized as a generalized linear model (GLM) [McCullagh and   \n134 Nelder, 1989], allowing for non-linear rewards. Thus, we need posterior approximation despite   \n135 linearity in score functions. Since this non-linearity arises solely from the reward distribution, we   \n136 approximate it by a Gaussian and propagate this approximation to the latent parameters. This results   \n137 in efficient posterior approximations that are exact when the reward function is Gaussian (a special   \n138 case of the GLM model). Specifically, the reward distribution $P(\\cdot\\mid x;\\theta)$ is an exponential family   \n139 distribution with a mean function denoted by $g$ . Then, we approximate the corresponding likelihood   \n140 as $\\mathbb{P}\\left(H_{t,i}\\,|\\,\\theta_{*,i}=\\theta\\right)\\approx\\mathcal{N}\\!\\left(\\theta;\\hat{B}_{t,i},\\hat{G}_{t,i}^{-1}\\right)$ , where $\\hat{B}_{t,i}$ and $\\hat{G}_{t,i}$ are the maximum likelihood estimate   \n141 (MLE) and the Hessian of the negative log-likelihood, respectively, and they are defined as ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{B}_{t,i}=\\arg\\operatorname*{max}_{\\theta\\in\\mathbb{R}^{d}}\\log\\mathbb{P}\\left(H_{t,i}\\,|\\,\\theta_{*,i}=\\theta\\right)\\,,\\quad\\quad\\;\\;\\hat{G}_{t,i}=\\sum_{k\\in S_{t,i}}\\dot{g}\\big(X_{k}^{\\top}\\hat{B}_{t,i}\\big)X_{k}X_{k}^{\\top}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "142 where $S_{t,i}\\,=\\,\\{\\ell\\,\\in\\,[t-1]\\,:\\,A_{\\ell}\\,=\\,i\\}$ represents the rounds where the agent takes action $i$ up to   \n143 round $t$ . This simple approximation makes all posteriors Gaussian. Specifically, the conditional   \n144 action-posterior is Gaussian and is given by $P_{t,i}(\\cdot\\mid\\psi_{1})=\\mathcal{N}(\\cdot;\\hat{\\mu}_{t,i},\\hat{\\Sigma}_{t,i})$ , where $\\hat{\\mu}_{t,i}$ and $\\hat{\\Sigma}_{t,i}$ are   \n145 computed using $\\hat{B}_{t,i}$ and $\\hat{G}_{t,i}$ in (7). Moreover, for $\\ell\\in[L-1]$ , the $\\ell$ -th conditional latent-posterior is   \n146 also Gaussian, $Q_{t,\\ell}(\\cdot\\mid\\psi_{\\ell+1})=\\mathcal{N}(\\cdot;\\bar{\\mu}_{t,\\ell},\\bar{\\Sigma}_{t,\\ell})$ , where $\\bar{\\mu}_{t,\\ell}$ and $\\bar{\\Sigma}_{t,\\ell}$ are computed recursively. The   \n147 recursion starts with $\\bar{\\mu}_{t,1}$ and $\\bar{\\Sigma}_{t,1}$ , which are calculated using $\\hat{B}_{t,i}$ and $\\hat{G}_{t,i}$ in (7). Full expressions are   \n148 provided in Appendix B.1. The only approximation made is $\\mathbb{P}\\left(H_{t,i}\\,|\\,\\theta_{*,i}=\\theta\\right)\\approx\\mathcal{N}\\big(\\boldsymbol{\\theta};\\hat{B}_{t,i},\\hat{G}_{t,i}^{-1}\\big)$   \n149 and we propagated it to latent posteriors. Thus, these posterior approximations become exact when   \n150 the reward distribution follows a linear-Gaussian model, $P(\\cdot\\mid x;\\dot{\\theta}_{*,a})=N(\\cdot;x^{\\top}\\theta_{*,a},\\sigma^{2})$ . ", "page_idx": 3}, {"type": "text", "text": "151 3.2 Non-linear diffusion model ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "152 After deriving the posteriors for linear score functions, we return to the general model in (1).   \n153 Approximation is needed since both the score functions and rewards can be non-linear. To avoid   \n154 computational challenges, we use a simple and intuitive approximation, where all posteriors $P_{t,i}$   \n155 and $Q_{t,\\ell}$ are approximated by Gaussians that are computed recursively. First, the conditional action  \n156 posterior is approximated by a Gaussian distribution as $P_{t,i}(\\cdot\\mid\\psi_{1})=\\mathcal{N}(\\cdot;\\hat{\\mu}_{t,i},\\hat{\\Sigma}_{t,i})$ , where ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\Sigma}_{t,i}^{-1}=\\Sigma_{1}^{-1}+\\hat{G}_{t,i}\\qquad\\qquad\\qquad\\hat{\\mu}_{t,i}=\\hat{\\Sigma}_{t,i}\\big(\\Sigma_{1}^{-1}f_{1}(\\psi_{1})+\\hat{G}_{t,i}\\hat{B}_{t,i}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "157 In the absence of samples, $G_{t,i}\\,=\\,0_{d\\times d}$ . Thus, the approximate action posterior in (8) matches   \n158 precisely the term $\\mathcal{N}(f_{1}(\\psi_{1}),\\dot{\\Sigma}_{1})$ in the diffusion prior (1). Moreover, as more data is accumulated,   \n159 $G_{t,i}$ increases, and the influence of the prior diminishes as $\\hat{G}_{t,i}\\hat{B}_{t,i}$ will dominate the prior term   \n160 $\\Sigma_{1}^{-1}f_{1}(\\psi_{1})$ . Similarly, for $\\ell\\in[L]/\\{1\\}$ , the $\\ell-1$ -th conditional latent-posterior is approximated by ", "page_idx": 3}, {"type": "text", "text": "161 a Gaussian distribution as $Q_{t,\\ell-1}(\\cdot\\mid\\psi_{\\ell})=\\mathcal{N}(\\bar{\\mu}_{t,\\ell-1},\\bar{\\Sigma}_{t,\\ell-1})$ , where ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bar{\\Sigma}_{t,\\ell-1}^{-1}=\\Sigma_{\\ell}^{-1}+\\bar{G}_{t,\\ell-1}\\,,\\qquad\\qquad\\bar{\\mu}_{t,\\ell-1}=\\bar{\\Sigma}_{t,\\ell-1}\\!\\left(\\Sigma_{\\ell}^{-1}f_{\\ell}(\\psi_{\\ell})+\\bar{B}_{t,\\ell-1}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "162 and the $L$ -th latent-posterior is $Q_{t,L}(\\cdot)=\\mathcal{N}(\\bar{\\mu}_{t,L},\\bar{\\Sigma}_{t,L})$ , ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\bar{\\Sigma}_{t,L}^{-1}=\\Sigma_{L+1}^{-1}+\\bar{G}_{t,L}\\,,\\qquad\\qquad\\qquad\\qquad\\bar{\\mu}_{t,L}=\\bar{\\Sigma}_{t,L}\\bar{B}_{t,L}\\,.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "163 Here, $\\bar{G}_{t,\\ell}$ and $\\bar{B}_{t,\\ell}$ for $\\ell\\in[L]$ are computed recursively. The basis of the recursion are ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bar{G}_{t,1}=\\sum_{i=1}^{K}\\left(\\Sigma_{1}^{-1}-\\Sigma_{1}^{-1}\\hat{\\Sigma}_{t,i}\\Sigma_{1}^{-1}\\right),\\qquad\\qquad\\bar{B}_{t,1}=\\Sigma_{1}^{-1}\\sum_{i=1}^{K}\\hat{\\Sigma}_{t,i}\\hat{G}_{t,i}\\hat{B}_{t,i}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "164 Then, the recursive step for $\\ell\\in[L]/\\{1\\}$ is, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bar{G}_{t,\\ell}=\\Sigma_{\\ell}^{-1}-\\Sigma_{\\ell}^{-1}\\bar{\\Sigma}_{t,\\ell-1}\\Sigma_{\\ell}^{-1}\\,,\\qquad\\qquad\\bar{B}_{t,\\ell}=\\Sigma_{\\ell}^{-1}\\bar{\\Sigma}_{t,\\ell-1}\\bar{B}_{t,\\ell-1}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "165 Similarly, in the absence of samples, $Q_{t,\\ell-1}$ in (9) precisely matches the term $\\mathcal{N}(f_{\\ell}(\\psi_{1}),\\Sigma_{\\ell})$ in the   \n166 diffusion prior (1). As more data is accumulated, the influence of this prior diminishes. Therefore,   \n167 this approximation retains a key attribute of exact posteriors: they match the prior when there is no   \n168 data, and the prior\u2019s effect diminishes as data accumulates. ", "page_idx": 4}, {"type": "text", "text": "169 4 Analysis ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "170 We analyze dTS under the linear diffusion model in Section 3.1 with linear rewards $P(\\cdot\\mid x;\\theta_{*,a})=$   \n171 $\\mathcal{N}(\\cdot;x^{\\top}\\theta_{*,a},\\sigma^{2})$ . This assumption leads to a structure with $L$ layers of linear Gaussian relationships,   \n172 allowing for theory inspired by linear bandits [Agrawal and Goyal, 2013a, Abbasi-Yadkori et al.,   \n173 2011]. However, proofs are not the same, and technical challenges remain (explained in Appendix D).   \n174 Although our result holds for milder assumptions, we make some simplifications for clarity and   \n175 interpretability. We assume that (A1) Contexts satisfy $\\|X_{t}\\|_{2}^{2}=1$ for any $t\\,\\in\\,[n]$ . (A2) Mixing   \n176 matrices and covariances satisfy $\\lambda_{1}(\\mathrm{W}_{\\ell}^{\\top}\\mathrm{W}_{\\ell})=1$ for any $\\ell\\in[L]$ and $\\Sigma_{\\ell}=\\sigma_{\\ell}^{2}I_{d}$ for any $\\ell\\in[L+1]$ .   \n177 Note that (A1) can be relaxed to any contexts $X_{t}$ with bounded norms $\\|X_{t}\\|_{2}$ . Also, (A2) can be   \n178 relaxed to positive definite covariances $\\Sigma_{\\ell}$ and arbitrary mixing matrices $\\mathrm{W}_{\\ell}$ . In this section, we   \n179 write $\\tilde{\\mathcal{O}}$ for the big-O notation up to polylogarithmic factors. We start by stating our bound for dTS.   \n180 Theorem 4.1. Let $\\begin{array}{r}{\\sigma_{\\mathrm{MAX}}^{2}=\\operatorname*{max}_{\\ell\\in[L+1]}1+\\frac{\\sigma_{\\ell}^{2}}{\\sigma^{2}}}\\end{array}$ . For any $\\delta\\in(0,1)$ , the Bayes regret of dTS under   \n181 Section 3.1 with linear rewards, (A1) and (A2) is bounded as ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{3\\mathcal{R}(n)\\leq\\sqrt{2n\\big(\\mathcal{R}^{\\mathrm{acr}}(n)+\\sum_{\\ell=1}^{L}\\mathcal{R}_{\\ell}^{\\mathrm{LAT}}\\big)\\log(1/\\delta)}\\bigg)+c n\\delta\\,,\\,w i t h\\,c>0\\,\\,i s\\,\\,c o n s t a n t\\,a n d,\\qquad(13)}\\\\ &{9^{\\mathrm{acr}}(n)=c_{0}d K\\log\\big(1+\\frac{n\\sigma_{1}^{2}}{d}\\big),\\,\\,c_{0}=\\frac{\\sigma_{1}^{2}}{\\log\\left(1+\\sigma_{1}^{2}\\right)}\\,,\\quad\\mathcal{R}_{\\ell}^{\\mathrm{LAT}}=c_{\\ell}d\\log\\big(1+\\frac{\\sigma_{\\ell+1}^{2}}{\\sigma_{\\ell}^{2}}\\big)\\,,\\,c_{\\ell}=\\frac{\\sigma_{\\ell+1}^{2}\\sigma_{\\mathrm{ax}}^{2\\ell}}{\\log\\left(1+\\sigma_{\\ell+1}^{2}\\right)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "182 (13) holds for any $\\delta\\,\\in\\,(0,1)$ . In particular, the term cn\u03b4 is constant when $\\delta\\,=\\,1/n$ . Then, the   \n183 bound is ${\\tilde{\\mathcal{O}}}({\\sqrt{n}})$ , and this dependence on the horizon $n$ aligns with prior Bayes regret bounds. The   \n184 bound comprises $L+1$ main terms, $\\mathcal{R}^{\\mathrm{ACT}}(n)$ and $\\mathcal{R}_{\\ell}^{\\mathrm{LAT}}$ for $\\ell\\in[L]$ . First, $\\mathcal{R}^{\\mathrm{ACT}}(n)$ relates to action   \n185 parameters learning, conforming to a standard form [Lu and Van Roy, 2019]. Similarly, $\\mathcal{R}_{\\ell}^{\\mathrm{LAT}}$ is   \n186 associated with learning the $\\ell_{}$ -th latent parameter. Roughly speaking, our bound captures that our   \n187 problem can be seen as $L+1$ sequential linear bandit instances stacked upon each other.   \n188 Technical contributions. dTS uses hierarchical sampling. Thus the marginal posterior distribution of   \n189 $\\theta_{*,i}\\mid H_{t}$ is not explicitly defined. The first contribution is deriving $\\theta_{*,i}\\mid H_{t}$ using the total covariance   \n190 decomposition combined with an induction proof, as our posteriors in Section 3.1 were derived   \n191 recursively. Unlike standard analyses where the posterior distribution of $\\theta_{*,i}\\mid H_{t}$ is predetermined   \n192 due to the absence of latent parameters, our method necessitates this recursive total covariance   \n193 decomposition. Moreover, in standard proofs, we need to quantify the increase in posterior precision   \n194 for the action taken $A_{t}$ in each round $t\\in[n]$ . However, in dTS, our analysis extends beyond this.   \n195 We not only quantify the posterior information gain for the taken action but also for every latent   \n196 parameter, since they are also learned. To elaborate, we use the recursive formulas in Section 3.1 that   \n197 connect the posterior covariance of each latent parameter $\\psi_{*,\\ell}$ with the covariance of the posterior   \n198 action parameters $\\theta_{*,i}$ . This allows us to propagate the information gain associated with the action   \n199 taken in round $A_{t}$ to all latent parameters $\\psi_{*,\\ell}$ , for $\\ell\\in[L]$ by induction. Finally, we carefully bound   \n200 the resulting terms so that the constants reflect the parameters of the linear diffusion model. More   \n201 technical details are provided in Appendix D.   \n202 To include more structure, we propose the sparsity assumption (A3) $\\mathrm{W}_{\\ell}=\\left(\\bar{\\mathrm{W}}_{\\ell},0_{d,d-d_{\\ell}}\\right)$ , where   \n203 $\\bar{\\mathrm{W}}_{\\ell}\\,\\in\\,\\mathbb{R}^{d\\times d_{\\ell}}$ for any $\\ell\\in[L]$ . Note that (A3) is not an assumption when $d_{\\ell}=d$ for any $\\ell\\in[L]$ .   \n204 Notably, (A3) incorporates a plausible structural characteristic that a diffusion model could capture.   \n205 Proposition 4.2 (Sparsity). Let $\\begin{array}{r}{\\sigma_{\\mathrm{MAX}}^{2}=\\operatorname*{max}_{\\ell\\in[L+1]}1+\\frac{\\sigma_{\\ell}^{2}}{\\sigma^{2}}}\\end{array}$ . For any $\\delta\\in(0,1)$ , the Bayes regret of   \n206 dTS under Section 3.1 with linear rewards, (A1), (A2) and (A3) is bounded as ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{3\\mathcal{R}(n)\\leq\\sqrt{2n\\big(\\mathcal{R}^{\\mathrm{acr}}(n)+\\sum_{\\ell=1}^{L}\\tilde{\\mathcal{R}}_{\\ell}^{\\mathrm{LAT}}\\big)\\log(1/\\delta)}\\,\\Big)+c n\\delta\\,,\\,w i t h\\,c>0\\,\\,i s\\,\\,c o n s t a n t,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,(14)}\\\\ &{}&{\\mathsf{2}^{\\mathrm{acr}}(n)=c_{0}d K\\log\\big(1+\\frac{n\\sigma_{1}^{2}}{d}\\big),c_{0}=\\frac{\\sigma_{1}^{2}}{\\log\\left(1+\\sigma_{1}^{2}\\right)}\\,,\\quad\\tilde{\\mathcal{R}}_{\\ell}^{\\mathrm{LAT}}=c_{\\ell}d_{\\ell}\\log\\big(1+\\frac{\\sigma_{\\ell+1}^{2}}{\\sigma_{\\ell}^{2}}\\big)\\,,c_{\\ell}=\\frac{\\sigma_{\\ell+1}^{2}\\sigma_{\\mathsf{A n}}^{2\\ell}}{\\log\\left(1+\\sigma_{\\ell+1}^{2}\\right)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "207 From Proposition 4.2, our bounds scales as $\\begin{array}{r}{\\mathcal{B}\\mathcal{R}(n)=\\tilde{\\mathcal{O}}\\Big(\\sqrt{n(d K\\sigma_{1}^{2}+\\sum_{\\ell=1}^{L}d_{\\ell}\\sigma_{\\ell+1}^{2}\\sigma_{\\mathrm{MAX}}^{2\\ell})}\\Big)}\\end{array}$ The   \n208 Bayes regret bound has a clear interpretation: if the true environment parameters are drawn from   \n209 the prior, then the expected regret of an algorithm stays below that bound. Consequently, a less   \n210 informative prior (such as high variance) leads to a more challenging problem and thus a higher   \n211 bound. Then, smaller values of $K$ , $L$ , $d$ or $d_{\\ell}$ translate to fewer parameters to learn, leading to lower   \n212 regret. The regret also decreases when the initial variances $\\sigma_{\\ell}^{\\dot{2}}$ decrease. These dependencies are   \n213 common in Bayesian analysis, and empirical results match them. The reader might question the   \n214 dependence of our bound on both $L$ and $K$ . We will address this next.   \n215 Why the bound increases with $K?$ This arises due to our conditional learning of $\\theta_{*,i}$ given   \n216 $\\psi_{*,1}$ . Rather than assuming deterministic linearity, $\\theta_{*,i}=\\mathrm{W}_{1}\\psi_{*,1}$ , we account for stochasticity by   \n217 modeling $\\theta_{*,i}\\sim\\mathcal{N}(\\mathrm{W}_{1}\\psi_{*,1},\\sigma_{1}^{2}I_{d})$ . This makes dTS robust to misspecification scenarios where $\\theta_{*,i}$   \n218 is not perfectly linear with respect to $\\psi_{*,1}$ , at the cost of additional learning of $\\theta_{*,i}\\mid\\psi_{*,1}$ . If we were   \n219 to assume deterministic linearity $(\\sigma_{1}=0)$ ), our regret bound would scale with $L$ only.   \n220 Why the bound increases with $L?$ This is because increasing the number of layers $L$ adds more   \n221 initial uncertainty due to the additional covariance introduced by the extra layers. However, this does   \n222 not imply that we should always use $L=1$ (the minimum possible $L$ ). While a higher $L$ complicates   \n223 online learning and increases regret bound, it also enables the capture of a more complex prior   \n224 distribution through offline pre-training of the diffusion model. Thus, a trade-off exists in practice.   \n225 A smaller $L$ results in faster computation and easier learning for dTS, but the learned prior might   \n226 deviate from reality, potentially violating the \"true prior assumption\" used to derive the regret bound.   \n227 On the other hand, a larger $L$ allows for better modeling of complex action distributions, producing a   \n228 prior that more accurately reflects reality and strengthens the validity of the bound. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "229 4.1 Discussion ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "230 Computational benefits. Action correlations prompt an intuitive approach: marginalize all latent   \n231 parameters and maintain a joint posterior of $(\\bar{\\theta_{*,i}})_{i\\in[K]}\\mid H_{t}$ . Unfortunately, this is computationally   \n232 inefficient for large action spaces. To illustrate, suppose that all posteriors are multivariate Gaussians   \n233 (Section 3.1). Then maintaining the joint posterior $(\\theta_{*,i})_{i\\in[K]}\\ \\mid\\,H_{t}$ necessitates converting and   \n234 storing its $d K\\times d K$ -dimensional covariance matrix. Then the time and space complexities are   \n235 $O(K^{3}d^{3})$ and $O(K^{2}d^{2})$ . In contrast, the time and space complexities of dTS are $\\mathcal{O}\\big(\\dot{\\big(L}+K\\big)d^{3}\\big)$   \n236 and $\\mathcal{O}\\big((L+K)d^{2}\\big)$ . This is because dTS requires converting and storing $L+K$ covariance matrices,   \n237 each being $d\\times d$ -dimensional. The improvement is huge when $K\\gg L$ , which is common in   \n238 practice. Certainly, a more straightforward way to enhance computational efficiency is to discard   \n239 latent parameters and maintain $K$ individual posteriors, each relating to an action parameter $\\theta_{\\ast,i}\\in\\mathbb{R}^{d}$   \n240 (LinTS). This improves time and space complexity to $O\\big(K d^{3}\\big)$ and $O\\big(K d^{2}\\big)$ , respectively. However,   \n241 LinTS maintains independent posteriors and fails to capture the correlations among actions; it only   \n242 models $\\theta_{*,i}\\mid H_{t,i}$ rather than $\\theta_{*,i}\\mid H_{t}$ as done by dTS. Consequently, LinTS incurs higher regret   \n243 due to the information loss caused by unused interactions of similar actions. Our regret bound and   \n244 empirical results reflect this aspect.   \n245 Statistical benefits. We do not provide a matching lower bound. The only Bayesian lower bound   \n246 that we know of is $\\Omega(\\log^{2}(n))$ for a much simpler $K$ -armed bandit [Lai, 1987, Theorem 3]. All   \n247 seminal works on Bayesian bandits do not match it and providing such lower bounds on Bayes regret   \n248 is still relatively unexplored (even in standard settings) compared to the frequentist one. Therefore,   \n249 we argue that our bound reflects the overall structure of the problem by comparing dTS to algorithms   \n250 that only partially use the structure or do not use it at all as follows.   \n251 The linear diffusion model in Section 3.1 can be transformed into a Bayesian linear model (LinTS)   \n252 by marginalizing out the latent parameters; in which case the prior on action parameters becomes   \n253 $\\bar{\\theta_{*,i}}\\sim\\bar{\\mathcal{N}}(0,\\Sigma)$ , with the $\\theta_{*,i}$ being not necessarily independent, and $\\Sigma$ is the marginal initial   \n254 covariance of action parameters and it writes $\\begin{array}{r}{\\Sigma=\\sigma_{1}^{2}I_{d}+\\sum_{\\ell=1}^{L}{\\sigma_{\\ell+1}^{2}\\mathrm{B}_{\\ell}\\mathrm{B}_{\\ell}^{\\top}}}\\end{array}$ with $\\begin{array}{r}{\\mathrm{B}\\ell=\\prod_{k=1}^{\\ell}\\mathrm{W}_{k}}\\end{array}$ .   \n255 Then, it is tempting to directly apply LinTS to solve our problem. This approach will induce   \n256 higher regret because the additional uncertainty of the latent parameters is accounted for in $\\Sigma$   \n257 258 conditional action uncertainty despite integrating them. This causes the marginal action uncertainty $\\sigma_{1}^{2}I_{d}$ in (3.1), since we have $\\begin{array}{r}{\\Sigma=\\sigma_{1}^{2}I_{d}+\\sum_{\\ell=1}^{L}\\sigma_{\\ell+1}^{2}\\mathrm{B}_{\\ell}\\mathrm{B}_{\\ell}^{\\top}\\succcurlyeq\\sigma_{1}^{2}I_{d}}\\end{array}$ $\\Sigma$ to be much higher than the   \n259 This discrepancy leads to higher regret, especially when is large. This is due to LinTS needing to   \n260 learn $K$ independent $d$ -dimensional parameters, each with a considerably higher initial covariance $\\Sigma$ .   \n261 This is also reflected by our regret bound. To simply comparisons, suppose that $\\sigma\\geq\\operatorname*{max}_{\\ell\\in[L+1]}\\sigma_{\\ell}$   \n262 so that $\\sigma_{\\mathrm{MAX}}^{2}\\leq2$ . Then the regret bounds of dTS (where we bound $\\sigma_{\\mathrm{MAX}}^{2\\ell}$ by $2^{\\ell}$ ) and LinTS read ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{d}\\mathrm{TS}:\\tilde{\\mathcal{O}}\\big(\\sqrt{n(d K\\sigma_{1}^{2}+\\sum_{\\ell=1}^{L}d_{\\ell}\\sigma_{\\ell+1}^{2}2^{\\ell})}\\big)\\,,\\qquad\\mathrm{LinTS}:\\tilde{\\mathcal{O}}\\big(\\sqrt{n d K(\\sigma_{1}^{2}+\\sum_{\\ell=1}^{L}\\sigma_{\\ell+1}^{2})}\\big)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "263 Then regret improvements are captured by the variances $\\sigma_{\\ell}$ and the sparsity dimensions $d_{\\ell}$ , and we   \n264 proceed to illustrate this through the following scenarios. ", "page_idx": 6}, {"type": "text", "text": "265 (I) Decreasing variances. Assume that $\\sigma_{\\ell}=2^{\\ell}$ for any $\\ell\\in[L+1]$ . Then, the regrets become ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{d}\\mathrm{TS}:\\tilde{\\mathcal{O}}\\big(\\sqrt{n(d K+\\textstyle\\sum_{\\ell=1}^{L}d_{\\ell}4^{\\ell}))}\\big)\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathrm{LinTS:}\\;\\tilde{\\mathcal{O}}\\big(\\sqrt{n d K2^{L})}\\big)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "266 Now to see the order of gain, assume the problem is high-dimensional $[d\\gg1]$ ), and set $L=\\log_{2}(d)$   \n267 and $\\begin{array}{r}{d\\ell=\\left\\lfloor\\frac{d}{2^{\\ell}}\\right\\rfloor}\\end{array}$ . Then the regret of dTS becomes $\\tilde{\\mathcal{O}}\\big(\\sqrt{n d(K+L)}\\big)\\big)$ , and hence the multiplicative   \n268 factor $2^{L}$ in LinTS is removed and replaced with a smaller additive factor $L$ . ", "page_idx": 6}, {"type": "text", "text": "269 $(\\mathbf{II})$ Constant variances. Assume that $\\sigma_{\\ell}=1$ for any $\\ell\\in[L+1]$ . Then, the regrets become ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{TS}:\\tilde{\\mathcal{O}}\\big(\\sqrt{n(d K+\\sum_{\\ell=1}^{L}d_{\\ell}2^{\\ell}))}\\big)\\,,\\qquad\\qquad}&{{}\\mathrm{LinTS}:\\tilde{\\mathcal{O}}\\big(\\sqrt{n d K L}\\big)\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "270 Similarly, let $L\\,=\\,\\log_{2}(d)$ , and $\\begin{array}{r}{d\\ell\\,=\\,\\left\\lfloor\\frac{d}{2^{\\ell}}\\right\\rfloor}\\end{array}$ . Then dTS\u2019s regret is $\\tilde{\\mathcal{O}}\\big(\\sqrt{n d(K+L)}\\big)$ . Thus the   \n271 multiplicative factor $L$ in LinTS is removed and replaced with the additive factor $L$ . By comparing   \n272 this to $\\mathbf{\\delta}(\\mathbf{I})$ , the gain with decreasing variances is greater than with constant ones. In general, diffusion   \n273 models use decreasing variances [Ho et al., 2020] and hence we expect great gains in practice.   \n274 All observed improvements in this section could become even more pronounced when employing   \n275 non-linear diffusion models. In our current analysis, we used linear diffusion models, and yet we can   \n276 already discern substantial differences. Moreover, under non-linear diffusion (1), the latent parameters   \n277 cannot be analytically marginalized, making LinTS with exact marginalization inapplicable. Finally,   \n278 Appendix D.7 provide an additional comparison and connection to hierarchies with two levels.   \n279 Large action space aspect. dTS\u2019s regret bound scales with $K\\sigma_{1}^{2}$ instead of $K\\sum_{\\ell}\\sigma_{\\ell}^{2}$ , particularly   \n280 beneficial when $\\sigma_{1}$ is small, as often seen in diffusion models. Our regret bou nd and experiments   \n281 show that dTS outperforms LinTS more distinctly when the action space becomes larger. Prior   \n282 studies [Foster et al., 2020, $\\mathrm{\\DeltaXu}$ and Zeevi, 2020, Zhu et al., 2022] proposed bandit algorithms that   \n283 do not scale with $K$ . However, our setting differs significantly from theirs, explaining our inherent   \n284 dependency on $K$ when $\\sigma_{1}>0$ . Precisely, they assume a reward function of $\\bar{r(x,i;\\theta_{*})}\\,\\bar{=}\\,\\phi(x,i)^{\\top}\\theta_{*}$ ,   \n285 with a shared $\\boldsymbol{\\theta}_{\\ast}\\in\\mathbb{R}^{d}$ and a known mapping $\\phi$ . In contrast, we consider $r(x,i;\\theta_{*})=x^{\\top}\\theta_{*,i}$ , with   \n286 $\\theta_{*}\\,=\\,(\\theta_{*,i})_{i\\in[K]}\\,\\in\\,\\mathbb{R}^{d K}$ , requiring the learning of $K$ separate $d$ -dimensional action parameters.   \n287 In their setting, with the availability of $\\phi$ , the regret of dTS would similarly be independent of   \n288 $K$ . However, obtaining such a mapping $\\phi$ can be challenging as it needs to encapsulate complex   \n289 context-action dependencies. Notably, our setting reflects a common practical scenario, such as in   \n290 recommendation systems where each product is often represented by its unique embedding. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "291 5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "292 We evaluate dTS using synthetic data, to validate our theory and test dTS in large action spaces. We   \n293 omit semi-synthetic data [Riquelme et al., 2018] as they often result in small action spaces. This   \n294 choice is further justified by the fact that Hsieh et al. [2023] has already demonstrated the advantages   \n295 of diffusion models in multi-armed bandits using such data, without theoretical guarantees. ", "page_idx": 6}, {"type": "image", "img_path": "nEnazjpwOx/tmp/6e5b07f86786856eaf683ab4875084ac219e2c6c4941e6fcd9182467e4d80fd8.jpg", "img_caption": ["Figure 2: Regret of dTS with varying diffusion and reward models and varying parameters $d,K,L$ . "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "296 5.1 Settings and baselines ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "297 We run 50 random simulations and plot the average regret with its standard error. We consider both   \n298 linear and non-linear rewards. The distribution of linear rewards is $P(\\cdot\\mid x;\\theta_{a})=\\mathcal{N}(x^{\\top}\\theta_{a},\\sigma^{2})$ with   \n299 $\\sigma=1$ . The non-linear rewards are binary and generated from $P(\\cdot\\mid x;\\theta_{a})=\\operatorname{Ber}(g(x^{\\top}\\theta_{a})))$ , where   \n300 $g$ is the sigmoid function. The covariances are $\\Sigma_{\\ell}=I_{d}$ , and the context $X_{t}$ is uniformly drawn from   \n301 $[-1,1]^{d}$ . We vary $d\\in\\{5,20\\}$ , $L\\in\\{2,4\\}$ and $K\\in\\{10^{2},10^{4}\\}$ . We set the horizon $n=5000$ .   \n302 Linear diffusion. We consider the linear diffusion model in (3.1) where score functions are linear as   \n303 $f_{\\ell}(\\psi)=\\mathrm{W}_{\\ell}\\psi$ where $\\mathrm{W}_{\\ell}$ are uniformly drawn from $[-1,1]^{d\\times d}$ . To introduce sparsity, we zero out   \n304 the last $d_{\\ell}$ columns of $\\mathrm{W}_{\\ell}$ , resulting in $\\mathrm{W}_{\\ell}=(\\bar{\\mathrm{W}}_{\\ell},\\bar{0}_{d,d-\\bar{d}_{\\ell}})$ , where $(d_{1},d_{2})\\bar{=}(5,\\bar{2})$ when $d=5$   \n305 and $L=2$ and $(d_{1},d_{2},d_{3},d_{4})=(20,10,5,2)$ when $d=20$ and $L=4$ .   \n306 Non-linear diffusion. We consider the general diffusion model in (1) with score functions $f_{\\ell}$ defined   \n307 by two-layer neural networks with random weights in $[-1,1]$ , ReLU activation, and a hidden layer   \n308 dimension of $h=20$ when $d=5$ and $h=60$ when $d=20$ .   \n309 Baselines. When rewards are linear, we use LinUCB [Abbasi-Yadkori et al., 2011], LinTS [Agrawal   \n310 and Goyal, 2013a], and HierTS [Hong et al., 2022b] that marginalizes out all latent parameters   \n311 except $\\psi_{*,L}$ . This corresponds to HierTS-1 in Appendix D.7. When rewards are non-linear, we   \n312 include UCB-GLM [Li et al., 2017], and GLM-TS [Chapelle and Li, 2012]. GLM-UCB [Filippi et al.,   \n313 2010] induced high regret while HierTS was designed for linear rewards only and thus both are not   \n314 included. We name dTS for each setting as dTS-dr, where the suffix d indicates the type of diffusion;   \n315 L for linear and N for non-linear. The suffix $\\mathbf{\\Deltar}$ indicates the type of rewards; L for linear and N for   \n316 non-linear. For instance, dTS-LL signifies dTS in linear diffusion (Section 3.1) with linear rewards. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "317 5.2 Results and interpretations ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "318 Results are shown in Fig. 2 and we make the following observations: ", "page_idx": 7}, {"type": "text", "text": "319 1) dTS has better performance. dTS outperforms the baselines. First, when both the diffusion and   \n320 rewards are linear, dTS-LL consistently outperforms all baselines that disregard the latent structure   \n321 (LinTS and LinUCB) or incorporate it only partially (HierTS). Second, when the diffusion is linear   \n322 and rewards are non-linear, dTS-LN surpasses all baselines. Third, when the diffusion is non-linear   \n323 and rewards are linear, dTS-NL demonstrates significant performance gains compared to both LinTS   \n324 and LinUCB. With non-linear diffusion and rewards, dTS-NN surpasses both GLM-TS and UCB-GLM.   \n325 2) Latent diffusion structure may be more important than the reward distribution. When   \n326 rewards are non-linear (second and fourth columns in Fig. 2), we included variants of dTS that use   \n327 the correct diffusion prior but the wrong reward distribution, employing linear-Gaussian instead of   \n328 logistic-Bernoulli (dTS-LL in the second column and dTS-NL in the fourth column). In both cases,   \n329 despite the misspecification of the reward distribution, these variants outperform models that use the   \n330 correct reward distribution but neglect the latent diffusion structure, such as GLM-TS and UCB-GLM.   \n331 This underscores the significance of accounting for the latent structure, which can sometimes be more   \n332 crucial than having an accurate reward distribution. Also, the performance gap between dTS-NL   \n333 (non-linear diffusion) and GLM-TS and UCB-GLM is even more pronounced compared to the gap   \n334 between dTS-LL (linear diffusion) and these baselines, possibly due to the increased complexity of   \n335 the latent structure, in the non-linear diffusion, overshadowing the impact of the reward model itself.   \n336 3) Prior misspecification (Fig. 3). We consider a scenario   \n337 where the prior used by dTS does not match the true prior.   \n338 To simulate this, we use our setting with linear diffusion   \n339 and rewards above, but the true parameters $\\mathrm{W}_{\\ell}$ and $\\Sigma_{\\ell}$ are   \n340 replaced by misspecified parameters $\\mathrm{W}_{\\ell}+\\epsilon_{1}$ and $\\Sigma_{\\ell}+\\epsilon_{2}$ .   \n341 Here, $\\epsilon_{1}$ and $\\epsilon_{2}$ are sampled uniformly from $[v,v{+}0.5]^{d\\times d}$ ,   \n342 with $v$ controlling the level of misspecification. The higher   \n343 the value of $v$ , the greater the misspecification. We vary   \n344 $v\\in\\{0.5,1,1.5\\}$ and analyze its impact on dTS\u2019s perfor  \n345 mance. For comparison, we include the well-specified   \n346 dTS-LL and the most competitive baseline, HierTS. Re  \n347 sults are shown in Fig. 3. As expected, dTS\u2019s performance   \n348 decreases with increasing misspecification. However, even   \n349 with misspecification, dTS outperforms the most competitive baseline, except when $v=1.5$ , where   \n350 their performances are comparable. Note that the entries of the true parameters $\\mathrm{W}_{\\ell}$ and $\\Sigma_{\\ell}$ are smaller   \n351 than 1, so values of $v\\,\\in\\,\\{0.5,1,1.5\\}$ can lead to significant parameter misspecification. Yet, the   \n352 performance of dTS with misspecified prior parameters remains favorable, suggesting that even an   \n353 imperfect pre-trained diffusion model can be beneficial when used as prior.   \n354 4) Regret scaling with $K$ , $d$ and $L$ matches our theory   \n355 (Fig. 4). We verify the impact of the number of actions   \n356 $K$ , the context dimension $d$ , and the diffusion depth $L$   \n357 on the regret of dTS. We maintain the same experimental   \n358 setup with linear diffusion and rewards, for which we have   \n359 derived a Bayes regret upper bound. In Fig. 4, we plot   \n360 the regret of dTS-LL across varying values of these pa  \n361 rameters: $K\\in\\{10,100,500,1000\\}$ , $d\\in\\{5,10,15,2\\bar{0}\\}$ ,   \n362 and $L\\in\\{2,4,5,6\\}$ . As anticipated and aligned with our   \n363 theory, the empirical regret increases as the values of $K,d,$ ,   \n364 or $L$ grow. This trend arises because larger values of $K$ , $d$ ,   \n365 or $L$ result in problem instances that are more challenging   \n366 to learn, consequently leading to higher regret.   \n367 5) Performance gap between dTS and LinTS widens   \n368 as $K$ increases (Fig. 5). To showcase dTS\u2019s improved   \n369 scalability to larger action spaces, we examine its perfor  \n370 mance across a range of $K$ values, from 10 to 50, 000,   \n371 in our setting with linear diffusion and rewards. Fig. 5   \n372 reports the final cumulative regret for varying values of $K$   \n373 for both dTS-LL and LinTS, observing that the gap in the   \n374 performance becomes larger as $K$ increases. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "image", "img_path": "nEnazjpwOx/tmp/3693944938fa3f3d09e6025f3fab35e2149ea19dc48f140c9f17a5fa3cbe76a3.jpg", "img_caption": ["Figure 3: Prior misspecification effect. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "image", "img_path": "nEnazjpwOx/tmp/38ec3549d237be86ab9509e1d9d88d4319bcd2c66fbe8d9de2c2eb2effa78be3.jpg", "img_caption": ["Figure 4: dTS-LL\u2019s regret scaling. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "375 6 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "image", "img_path": "nEnazjpwOx/tmp/66ec362fe18e836e30d7dca95a736eb0c8271f1f6ae3dee77470035a3e517ffa.jpg", "img_caption": ["Figure 5: Regret of dTS-LL and LinTS with varying $K$ . "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "376 Grappling with large action spaces in contextual bandits is challenging. Recognizing this, we focused   \n377 on structured problems where action parameters are sampled from a diffusion model; upon which we   \n378 built diffusion Thompson sampling (dTS). We developed both theoretical and algorithmic foundations   \n379 for dTS in numerous practical settings. We identified several directions for future work. Exploring   \n380 other approximations for non-linear diffusion models, both empirically and theoretically. From a   \n381 theoretical perspective, future research could explore the advantages of non-linear diffusion models   \n382 by deriving their Bayes regret bounds, akin to our analysis in Section 4. Empirically, investigating   \n383 our and other approximations in complex tasks would be interesting. Additionally, exploring the   \n384 extension of this work to offline (or off-policy) learning in contextual bandits [Swaminathan and   \n385 Joachims, 2015, Aouali et al., 2023a] represents a promising avenue for future research. ", "page_idx": 8}, {"type": "text", "text": "386 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "387 Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Improved algorithms for linear stochastic   \n388 bandits. In Advances in Neural Information Processing Systems 24, pages 2312\u20132320, 2011.   \n389 Marc Abeille and Alessandro Lazaric. Linear Thompson sampling revisited. In Proceedings of the   \n390 20th International Conference on Artificial Intelligence and Statistics, 2017.   \n391 Shipra Agrawal and Navin Goyal. Thompson sampling for contextual bandits with linear payoffs. In   \n392 Proceedings of the 30th International Conference on Machine Learning, pages 127\u2013135, 2013a.   \n393 Shipra Agrawal and Navin Goyal. Further optimal regret bounds for thompson sampling. In   \n394 Proceedings of the Sixteenth International Conference on Artificial Intelligence and Statistics,   \n395 pages 99\u2013107, 2013b.   \n396 Shipra Agrawal and Navin Goyal. Near-optimal regret bounds for thompson sampling. Journal of   \n397 the ACM (JACM), 64(5):1\u201324, 2017.   \n398 Anurag Ajay, Yilun Du, Abhi Gupta, Joshua Tenenbaum, Tommi Jaakkola, and Pulkit Agrawal. Is con  \n399 ditional generative modeling all you need for decision-making? arXiv preprint arXiv:2211.15657,   \n400 2022.   \n401 Imad Aouali, Victor-Emmanuel Brunel, David Rohde, and Anna Korba. Exponential smoothing for   \n402 off-policy learning. In International Conference on Machine Learning, pages 984\u20131017. PMLR,   \n403 2023a.   \n404 Imad Aouali, Branislav Kveton, and Sumeet Katariya. Mixed-effect thompson sampling. In Interna  \n405 tional Conference on Artificial Intelligence and Statistics, pages 2087\u20132115. PMLR, 2023b.   \n406 Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit   \n407 problem. Machine Learning, 47:235\u2013256, 2002.   \n408 Mohammad Gheshlaghi Azar, Alessandro Lazaric, and Emma Brunskill. Sequential transfer in   \n409 multi-armed bandit with finite set of models. In Advances in Neural Information Processing   \n410 Systems 26, pages 2220\u20132228, 2013.   \n411 Hamsa Bastani, David Simchi-Levi, and Ruihao Zhu. Meta dynamic pricing: Transfer learning across   \n412 experiments. CoRR, abs/1902.10918, 2019. URL https://arxiv.org/abs/1902.10918.   \n413 Soumya Basu, Branislav Kveton, Manzil Zaheer, and Csaba Szepesvari. No regrets for learning the   \n414 prior in bandits. In Advances in Neural Information Processing Systems 34, 2021.   \n415 Christopher M Bishop. Pattern Recognition and Machine Learning, volume 4 of Information science   \n416 and statistics. Springer, 2006.   \n417 Leonardo Cella, Alessandro Lazaric, and Massimiliano Pontil. Meta-learning with stochastic linear   \n418 bandits. In Proceedings of the 37th International Conference on Machine Learning, 2020.   \n419 Leonardo Cella, Karim Lounici, and Massimiliano Pontil. Multi-task representation learning with   \n420 stochastic linear bandits. arXiv preprint arXiv:2202.10066, 2022.   \n421 Olivier Chapelle and Lihong Li. An empirical evaluation of Thompson sampling. In Advances in   \n422 Neural Information Processing Systems 24, pages 2249\u20132257, 2012.   \n423 Hyungjin Chung, Jeongsol Kim, Michael T Mccann, Marc L Klasky, and Jong Chul Ye. Diffusion   \n424 posterior sampling for general noisy inverse problems. arXiv preprint arXiv:2209.14687, 2022.   \n425 Aniket Anand Deshmukh, Urun Dogan, and Clayton Scott. Multi-task learning for contextual bandits.   \n426 In Advances in Neural Information Processing Systems 30, pages 4848\u20134856, 2017.   \n427 Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances   \n428 in neural information processing systems, 34:8780\u20138794, 2021.   \n429 Sarah Filippi, Olivier Cappe, Aurelien Garivier, and Csaba Szepesvari. Parametric bandits: The   \n430 generalized linear case. In Advances in Neural Information Processing Systems 23, pages 586\u2013594,   \n431 2010.   \n432 Dylan J Foster, Claudio Gentile, Mehryar Mohri, and Julian Zimmert. Adapting to misspecification in   \n433 contextual bandits. Advances in Neural Information Processing Systems, 33:11478\u201311489, 2020.   \n434 Claudio Gentile, Shuai Li, and Giovanni Zappella. Online clustering of bandits. In Proceedings of   \n435 the 31st International Conference on Machine Learning, pages 757\u2013765, 2014.   \n436 Aditya Gopalan, Shie Mannor, and Yishay Mansour. Thompson sampling for complex online   \n437 problems. In Proceedings of the 31st International Conference on Machine Learning, pages   \n438 100\u2013108, 2014.   \n439 Samarth Gupta, Shreyas Chaudhari, Subhojyoti Mukherjee, Gauri Joshi, and Osman Yagan. A   \n440 unified approach to translate classical bandit algorithms to the structured bandit setting. CoRR,   \n441 abs/1810.08164, 2018. URL https://arxiv.org/abs/1810.08164.   \n442 Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in   \n443 neural information processing systems, 33:6840\u20136851, 2020.   \n444 Joey Hong, Branislav Kveton, Manzil Zaheer, Yinlam Chow, Amr Ahmed, and Craig Boutilier. Latent   \n445 bandits revisited. In Advances in Neural Information Processing Systems 33, 2020.   \n446 Joey Hong, Branislav Kveton, Sumeet Katariya, Manzil Zaheer, and Mohammad Ghavamzadeh.   \n447 Deep hierarchy in bandits. In International Conference on Machine Learning, pages 8833\u20138851.   \n448 PMLR, 2022a.   \n449 Joey Hong, Branislav Kveton, Manzil Zaheer, and Mohammad Ghavamzadeh. Hierarchical Bayesian   \n450 bandits. In Proceedings of the 25th International Conference on Artificial Intelligence and Statistics,   \n451 2022b.   \n452 Yu-Guan Hsieh, Shiva Prasad Kasiviswanathan, Branislav Kveton, and Patrick Bl\u00f6baum. Thompson   \n453 sampling with diffusion generative prior. arXiv preprint arXiv:2301.05182, 2023.   \n454 Jiachen Hu, Xiaoyu Chen, Chi Jin, Lihong Li, and Liwei Wang. Near-optimal representation learning   \n455 for linear bandits and linear rl. In International Conference on Machine Learning, pages 4349\u20134358.   \n456 PMLR, 2021.   \n457 Michael Janner, Yilun Du, Joshua B Tenenbaum, and Sergey Levine. Planning with diffusion for   \n458 flexible behavior synthesis. arXiv preprint arXiv:2205.09991, 2022.   \n459 Emilie Kaufmann, Nathaniel Korda, and R\u00e9mi Munos. Thompson sampling: An asymptotically   \n460 optimal finite-time analysis. In International conference on algorithmic learning theory, pages   \n461 199\u2013213. Springer, 2012.   \n462 Daphne Koller and Nir Friedman. Probabilistic Graphical Models: Principles and Techniques. MIT   \n463 Press, Cambridge, MA, 2009.   \n464 Nathaniel Korda, Emilie Kaufmann, and Remi Munos. Thompson sampling for 1-dimensional   \n465 exponential family bandits. Advances in neural information processing systems, 26, 2013.   \n466 John K Kruschke. Bayesian data analysis. Wiley Interdisciplinary Reviews: Cognitive Science, 1(5):   \n467 658\u2013676, 2010.   \n468 Branislav Kveton, Manzil Zaheer, Csaba Szepesvari, Lihong Li, Mohammad Ghavamzadeh, and   \n469 Craig Boutilier. Randomized exploration in generalized linear bandits. In International Conference   \n470 on Artificial Intelligence and Statistics, pages 2066\u20132076. PMLR, 2020.   \n471 Branislav Kveton, Mikhail Konobeev, Manzil Zaheer, Chih-Wei Hsu, Martin Mladenov, Craig   \n472 Boutilier, and Csaba Szepesvari. Meta-Thompson sampling. In Proceedings of the 38th Interna  \n473 tional Conference on Machine Learning, 2021.   \n474 Tze Leung Lai. Adaptive treatment allocation and the multi-armed bandit problem. The Annals of   \n475 Statistics, 15(3):1091\u20131114, 1987.   \n476 Tor Lattimore and Remi Munos. Bounded regret for finite-armed structured bandits. In Advances in   \n477 Neural Information Processing Systems 27, pages 550\u2013558, 2014.   \n478 Lihong Li, Wei Chu, John Langford, and Robert Schapire. A contextual-bandit approach to personal  \n479 ized news article recommendation. In Proceedings of the 19th International Conference on World   \n480 Wide Web, 2010.   \n481 Lihong Li, Yu Lu, and Dengyong Zhou. Provably optimal algorithms for generalized linear contextual   \n482 bandits. In Proceedings of the 34th International Conference on Machine Learning, pages 2071\u2013   \n483 2080, 2017.   \n484 Dennis Lindley and Adrian Smith. Bayes estimates for the linear model. Journal of the Royal   \n485 Statistical Society: Series B (Methodological), 34(1):1\u201318, 1972.   \n486 Xiuyuan Lu and Benjamin Van Roy. Information-theoretic confidence bounds for reinforcement   \n487 learning. In Advances in Neural Information Processing Systems 32, 2019.   \n488 Odalric-Ambrym Maillard and Shie Mannor. Latent bandits. In Proceedings of the 31st International   \n489 Conference on Machine Learning, pages 136\u2013144, 2014.   \n490 P. McCullagh and J. A. Nelder. Generalized Linear Models. Chapman & Hall, 1989.   \n491 Amit Peleg, Naama Pearl, and Ron Meirr. Metalearning linear bandits by prior update. In Proceedings   \n492 of the 25th International Conference on Artificial Intelligence and Statistics, 2022.   \n493 Carlos Riquelme, George Tucker, and Jasper Snoek. Deep bayesian bandits showdown: An empirical   \n494 comparison of bayesian deep networks for thompson sampling. arXiv preprint arXiv:1802.09127,   \n495 2018.   \n496 Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High  \n497 resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF confer  \n498 ence on computer vision and pattern recognition, pages 10684\u201310695, 2022.   \n499 Daniel Russo and Benjamin Van Roy. Learning to optimize via posterior sampling. Mathematics of   \n500 Operations Research, 39(4):1221\u20131243, 2014.   \n501 Steven Scott. A modern bayesian look at the multi-armed bandit. Applied Stochastic Models in   \n502 Business and Industry, 26:639 \u2013 658, 2010.   \n503 Max Simchowitz, Christopher Tosh, Akshay Krishnamurthy, Daniel Hsu, Thodoris Lykouris, Miro   \n504 Dudik, and Robert Schapire. Bayesian decision-making under misspecified priors with applications   \n505 to meta-learning. In Advances in Neural Information Processing Systems 34, 2021.   \n506 Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised   \n507 learning using nonequilibrium thermodynamics. In International conference on machine learning,   \n508 pages 2256\u20132265. PMLR, 2015.   \n509 Adith Swaminathan and Thorsten Joachims. Counterfactual risk minimization: Learning from logged   \n510 bandit feedback. In International Conference on Machine Learning, pages 814\u2013823. PMLR, 2015.   \n511 Runzhe Wan, Lin Ge, and Rui Song. Metadata-based multi-task bandits with Bayesian hierarchical   \n512 models. In Advances in Neural Information Processing Systems 34, 2021.   \n513 Runzhe Wan, Lin Ge, and Rui Song. Towards scalable and robust structured bandits: A meta-learning   \n514 framework. CoRR, abs/2202.13227, 2022. URL https://arxiv.org/abs/2202.13227.   \n515 Zhendong Wang, Jonathan J Hunt, and Mingyuan Zhou. Diffusion policies as an expressive policy   \n516 class for offline reinforcement learning. arXiv preprint arXiv:2208.06193, 2022.   \n517 Neil Weiss. A Course in Probability. Addison-Wesley, 2005.   \n518 Yunbei Xu and Assaf Zeevi. Upper counterfactual confidence bounds: a new optimism principle for   \n519 contextual bandits. arXiv preprint arXiv:2007.07876, 2020.   \n520 Jiaqi Yang, Wei Hu, Jason D Lee, and Simon S Du. Impact of representation learning in linear bandits.   \n521 arXiv preprint arXiv:2010.06531, 2020.   \n522 Tong Yu, Branislav Kveton, Zheng Wen, Ruiyi Zhang, and Ole Mengshoel. Graphical models meet   \n523 bandits: A variational Thompson sampling approach. In Proceedings of the 37th International   \n524 Conference on Machine Learning, 2020.   \n525 Yinglun Zhu, Dylan J Foster, John Langford, and Paul Mineiro. Contextual bandits with large action   \n526 spaces: Made practical. In International Conference on Machine Learning, pages 27428\u201327453.   \n527 PMLR, 2022. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "528 Supplementary materials ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "529 Notation. For any positive integer $n$ , we define $[n]=\\{1,2,...,n\\}$ . Let $v_{1},\\ldots,v_{n}\\in\\mathbb{R}^{d}$ be $n$ vectors,   \n530 $(v_{i})_{i\\in[n]}\\in\\mathbb{R}^{n d}$ is the $^{n d}$ -dimensional vector obtained by concatenating $v_{1},\\ldots,v_{n}$ . For any matrix   \n531 $\\mathrm{A}\\in\\mathbb{R}^{d\\times d}$ , $\\lambda_{1}(\\mathrm{A})$ and $\\lambda_{d}(\\mathrm{A})$ denote the maximum and minimum eigenvalues of A, respectively.   \n532 Finally, we write $\\tilde{\\mathcal{O}}$ for the big-O notation up to polylogarithmic factors. ", "page_idx": 13}, {"type": "text", "text": "533 A Extended related work ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "534 Thompson sampling (TS) operates within the Bayesian framework and it involves specifying a   \n535 prior/likelihood model. In each round, the agent samples unknown model parameters from the   \n536 current posterior distribution. The chosen action is the one that maximizes the resulting reward. TS   \n537 is naturally randomized, particularly simple to implement, and has highly competitive empirical   \n538 performance in both simulated and real-world problems [Russo and Van Roy, 2014, Chapelle and Li,   \n539 2012]. Regret guarantees for the TS heuristic remained open for decades even for simple models.   \n540 Recently, however, significant progress has been made. For standard multi-armed bandits, TS is   \n541 optimal in the Beta-Bernoulli model [Kaufmann et al., 2012, Agrawal and Goyal, 2013b], Gaussian  \n542 Gaussian model [Agrawal and Goyal, 2013b], and in the exponential family using Jeffrey\u2019s prior   \n543 [Korda et al., 2013]. For linear bandits, TS is nearly-optimal [Russo and Van Roy, 2014, Agrawal and   \n544 Goyal, 2017, Abeille and Lazaric, 2017]. In this work, we build TS upon complex diffusion priors   \n545 and analyze the resulting Bayes regret [Russo and Van Roy, 2014] in the linear contextual bandit   \n546 setting.   \n547 Decision-making with diffusion models gained attention recently, especially in offline learning   \n548 [Ajay et al., 2022, Janner et al., 2022, Wang et al., 2022]. However, their application in online   \n549 learning was only examined by Hsieh et al. [2023], which focused on meta-learning in multi-armed   \n550 bandits without theoretical guarantees. In this work, we expand the scope of Hsieh et al. [2023] to   \n551 encompass the broader contextual bandit framework. In particular, we provide theoretical analysis for   \n552 linear instances, effectively capturing the advantages of using diffusion models as priors in contextual   \n553 Thompson sampling. These linear cases are particularly captivating due to closed-form posteriors,   \n554 enabling both theoretical analysis and computational efficiency; an important practical consideration.   \n555 Hierarchical Bayesian bandits [Bastani et al., 2019, Kveton et al., 2021, Basu et al., 2021, Sim  \n556 chowitz et al., 2021, Wan et al., 2021, Hong et al., 2022b, Peleg et al., 2022, Wan et al., 2022, Aouali   \n557 et al., 2023b] applied TS to simple graphical models, wherein action parameters are generally sampled   \n558 from a Gaussian distribution centered at a single latent parameter. These works mostly span meta  \n559 and multi-task learning for multi-armed bandits, except in cases such as Aouali et al. [2023b], Hong   \n560 et al. [2022a] that consider the contextual bandit setting. Precisely, Aouali et al. [2023b] assume that   \n561 action parameters are sampled from a Gaussian distribution centered at a linear mixture of multiple   \n562 latent parameters. On the other hand, Hong et al. [2022a] applied TS to a graphical model represented   \n563 by a tree. Our work can be seen as an extension of all these works to much more complex graphical   \n564 models, for which both theoretical and algorithmic foundations are developed. Note that the settings   \n565 in most of these works can be recovered with specific choices of the diffusion depth $L$ and functions   \n566 $f_{\\ell}$ . This attests to the modeling power of dTS.   \n567 Approximate Thompson sampling is a major problem in the Bayesian inference literature. This is   \n568 because most posterior distributions are intractable, and thus practitioners must resort to sophisti  \n569 cated computational techniques such as Markov chain Monte Carlo [Kruschke, 2010]. Prior works   \n570 [Riquelme et al., 2018, Chapelle and Li, 2012, Kveton et al., 2020] highlight the favorable empirical   \n571 performance of approximate Thompson sampling. Particularly, [Kveton et al., 2020] provide the  \n572 oretical guarantees for Thompson sampling when using the Laplace approximation in generalized   \n573 linear bandits (GLB). In our context, we incorporate approximate sampling when the reward exhibits   \n574 non-linearity. While our approximation does not come with formal guarantees, it enjoys strong   \n575 practical performance. An in-depth analysis of this approximation is left as a direction for future   \n576 works. Similarly, approximating the posterior distribution when the diffusion model is non-linear as   \n577 well as analyzing it is an interesting direction of future works.   \n578 Bandits with underlying structure also align with our work, where we assume a structured relation  \n579 ship among actions, captured by a diffusion model. In latent bandits [Maillard and Mannor, 2014,   \n580 Hong et al., 2020], a single latent variable indexes multiple candidate models. Within structured   \n581 finite-armed bandits [Lattimore and Munos, 2014, Gupta et al., 2018], each action is linked to a known   \n582 mean function parameterized by a common latent parameter. This latent parameter is learned. TS   \n583 was also applied to complex structures [Yu et al., 2020, Gopalan et al., 2014]. However, simultaneous   \n584 computational and statistical efficiencies aren\u2019t guaranteed. Meta- and multi-task learning with   \n585 upper confidence bound (UCB) approaches have a long history in bandits [Azar et al., 2013, Gentile   \n586 et al., 2014, Deshmukh et al., 2017, Cella et al., 2020]. These, however, often adopt a frequentist   \n587 perspective, analyze a stronger form of regret, and sometimes result in conservative algorithms.   \n588 In contrast, our approach is Bayesian, with analysis centered on Bayes regret. Remarkably, our   \n589 algorithm, dTS, performs well as analyzed without necessitating additional tuning. Finally, Low-rank   \n590 bandits [Hu et al., 2021, Cella et al., 2022, Yang et al., 2020] also relate to our linear diffusion model   \n591 when $L=1$ . Broadly, there exist two key distinctions between these prior works and the special   \n592 case of our model (linear diffusion model with $L=1$ ). First, they assume $\\theta_{*,i}=\\mathrm{W}_{1}\\psi_{*,1}$ , whereas   \n593 we incorporate additional uncertainty in the covariance $\\Sigma_{1}$ to account for possible misspecification   \n594 as $\\theta_{*,i}=\\mathcal{N}(\\mathrm{W}_{1}\\psi_{*,1},\\Sigma_{1})$ . Consequently, these algorithms might suffer linear regret due to model   \n595 misalignment. Second, we assume that the mixing matrix $\\mathrm{W_{1}}$ is available and pre-learned offline,   \n596 whereas they learn it online. While this is more general, it leads to computationally expensive   \n597 methods that are difficult to employ in a real-world online setting.   \n598 Large action spaces. Roughly speaking, the regret bound of dTS scales with $K\\sigma_{1}^{2}$ rather than   \n599 $K\\breve{\\sum_{\\ell}\\sigma_{\\ell}^{2}}$ . This is particularly beneficial when $\\sigma_{1}$ is small, a common scenario in diffusion models   \n600 with decreasing variances. A notable case is when $\\sigma_{1}=0$ , where the regret becomes independent of   \n601 $K$ . Also, our analysis (Section 4.1) indicates that the gap in performance between dTS and LinTS   \n602 becomes more pronounced when the number of action increases, highlighting dTS\u2019s suitability for   \n603 large action spaces. Note that some prior works [Foster et al., 2020, $\\mathrm{Xu}$ and Zeevi, 2020, Zhu et al.,   \n604 2022] proposed bandit algorithms that do not scale with $K$ . However, our setting differs significantly   \n605 from theirs, explaining our inherent dependency on $K$ when $\\sigma_{1}\\,>\\,0$ . Precisely, they assume a   \n606 reward function of $r(x,\\stackrel{\\bullet}{\\iota})=\\phi(x,i)^{\\top}\\theta_{*}$ , with a shared $\\theta_{\\ast}\\in\\mathbb{R}^{d}$ across actions and a known mapping   \n607 $\\phi$ . In contrast, we consider $r(x,i)=x^{\\top}\\theta_{*,i}$ , requiring the learning of $K$ separate $d$ -dimensional   \n608 action parameters. In their setting, with the availability of $\\phi$ , the regret of dTS would similarly be   \n609 independent of $K$ . However, obtaining such a mapping $\\phi$ can be challenging as it needs to encapsulate   \n610 complex context-action dependencies. Notably, our setting reflects a common practical scenario,   \n611 such as in recommendation systems where each product is often represented by its embedding. In   \n612 summary, the dependency on $K$ is more related to our setting than the method itself, and dTS would   \n613 scale with $d$ only in their setting. Note that dTS is both computationally and statistically efficient   \n614 (Section 4.1). This becomes particularly notable in large action spaces. Our empirical results in   \n615 Fig. 2, notably with $K=10^{4}$ , demonstrate that dTS significantly outperforms the baselines. More   \n616 importantly, the performance gap between dTS and these baselines is larger when the number of   \n617 actions $(K)$ increases, highlighting the improved scalability of dTS to large action spaces. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "618 B Posterior derivations for linear diffusion models ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "619 Here, we assume the score functions $f_{\\ell}$ are linear such as $f_{\\ell}(\\psi_{*,\\ell})=\\mathrm{W}_{\\ell}\\psi_{*,\\ell}$ for $\\ell\\in[L]$ , where   \n620 $\\mathrm{W}_{\\ell}\\in\\mathbb{R}^{d\\times d}$ are known mixing matrices. Then, (1) becomes a linear Gaussian system (LGS) [Bishop,   \n621 2006] and can be summarized as follows ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\psi_{*,L}\\sim\\mathcal{N}(0,\\Sigma_{L+1})\\,,\\quad}\\\\ &{}&{\\psi_{*,\\ell-1}\\mid\\psi_{*,\\ell}\\sim\\mathcal{N}(\\mathrm{W}_{\\ell}\\psi_{*,\\ell},\\Sigma_{\\ell})\\,,}\\\\ &{}&{\\theta_{*,i}\\mid\\psi_{*,1}\\sim\\mathcal{N}(\\mathrm{W}_{1}\\psi_{*,1},\\Sigma_{1})\\,,}\\\\ &{}&{Y_{t}\\mid X_{t},\\theta_{*,A_{t}}\\sim P(\\cdot\\mid X_{t};\\theta_{*,A_{t}})\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "622 In this section, we derive the $K+L$ posteriors $P_{t,i}$ and $Q_{t,\\ell}$ , for which we provide the full expressions   \n623 in Appendix B.1. In our proofs, $p(x)\\propto f(x)$ means that the probability density $p$ satisfies $p(x)=$   \n624 $\\frac{f(x)}{Z}$ for any $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ , where $Z$ is a normalization constant. In particular, we extensively use that if   \n625 $\\begin{array}{r}{p(\\bar{x})\\propto\\exp[-\\frac{1}{2}x^{\\top}\\Lambda x+x^{\\top}m]}\\end{array}$ , where $\\Lambda$ is positive definite. Then $p$ is the multivariate Gaussian   \n626 density with covariance $\\Sigma=\\Lambda^{-1}$ and mean $\\mu=\\Sigma m$ . These are standard notations and techniques   \n627 to manipulate Gaussian distributions [Koller and Friedman, 2009, Chapter 7]. ", "page_idx": 14}, {"type": "text", "text": "628 B.1 Posterior expressions for linear diffusion models ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "629 Recall that we posit that the reward distribution is parameterized as a generalized linear model (GLM)   \n630 [McCullagh and Nelder, 1989], allowing for non-linear rewards. As a result, despite linearity in   \n631 score functions, the non-linearity in rewards makes it challenging to obtain closed-form posteriors.   \n632 However, since this non-linearity arises solely from the reward distribution, we approximate it using   \n633 a Gaussian distribution. This leads to efficient posterior approximations that are exact in cases where   \n634 the reward function is indeed Gaussian (a special case of the GLM model). Precisely, the reward   \n635 distribution $P(\\cdot\\mid x;\\theta)$ is an exponential-family distribution. Therefore, the log-likelihoods write   \n636 $\\begin{array}{r}{\\log\\mathbb{P}\\left(H_{t,i}\\,|\\,\\theta_{*,i}=\\theta\\right)=\\sum_{k\\in S_{t,i}}Y_{k}X_{k}^{\\top}\\theta-A\\dot{(X_{k}^{\\top}\\theta)}+C(Y_{k})}\\end{array}$ , where $C$ is a real function, and $A$   \n637 is a twice continuously differentiable function whose derivative is the mean function, ${\\dot{A}}=g$ . Now   \n638 we let $\\hat{B}_{t,i}$ and $\\hat{G}_{t,i}$ be the maximum likelihood estimate (MLE) and the Hessian of the negative   \n639 log-likelihood, respectively, defined as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\hat{B}_{t,i}=\\underset{\\theta\\in\\mathbb{R}^{d}}{\\arg\\operatorname*{max}}\\log\\mathbb{P}\\left(H_{t,i}\\,|\\,\\theta_{*,i}=\\theta\\right)\\,,\\qquad\\quad\\hat{G}_{t,i}=\\sum_{k\\in S_{t,i}}\\dot{g}\\big(X_{k}^{\\top}\\hat{B}_{t,i}\\big)X_{k}X_{k}^{\\top}\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "640 where $S_{t,i}=\\{\\ell\\in[t-1]:A_{\\ell}=i\\}$ are the rounds where the agent takes action $i$ up to round $t$ .   \n641 Then we approximation the respective likelihood as $\\mathbb{P}\\left(H_{t,i}\\,|\\,\\theta_{*,i}=\\theta\\right)\\approx\\mathcal{N}\\big(\\theta;\\hat{B}_{t,i},\\hat{G}_{t,i}^{-1}\\big)$ . This   \n642 approximation makes all posteriors Gaussian. First, the conditional action-posterior reads $P_{t,i}(\\cdot\\mid$   \n643 $\\psi_{1}\\rangle=\\mathcal{N}(\\cdot;\\hat{\\mu}_{t,i},\\hat{\\Sigma}_{t,i})$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\Sigma}_{t,i}^{-1}=\\Sigma_{1}^{-1}+\\hat{G}_{t,i}\\qquad\\qquad\\qquad\\hat{\\mu}_{t,i}=\\hat{\\Sigma}_{t,i}\\big(\\Sigma_{1}^{-1}\\mathrm{W}_{1}\\psi_{1}+\\hat{G}_{t,i}\\hat{B}_{t,i}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "644 For $\\ell\\in[L]/\\{1\\}$ , the $\\ell-1$ -th conditional latent-posterior is $Q_{t,\\ell-1}(\\cdot\\mid\\psi_{\\ell})=\\mathcal{N}(\\bar{\\mu}_{t,\\ell-1},\\bar{\\Sigma}_{t,\\ell-1}),$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bar{\\Sigma}_{t,\\ell-1}^{-1}=\\Sigma_{\\ell}^{-1}+\\bar{G}_{t,\\ell-1}\\,,\\qquad\\qquad\\bar{\\mu}_{t,\\ell-1}=\\bar{\\Sigma}_{t,\\ell-1}\\!\\left(\\Sigma_{\\ell}^{-1}\\mathrm{W}_{\\ell}\\psi_{\\ell}+\\bar{B}_{t,\\ell-1}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "645 and the $L$ -th latent-posterior is $Q_{t,L}(\\cdot)=\\mathcal{N}(\\bar{\\mu}_{t,L},\\bar{\\Sigma}_{t,L})$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\bar{\\Sigma}_{t,L}^{-1}=\\Sigma_{L+1}^{-1}+\\bar{G}_{t,L}\\,,\\qquad\\qquad\\qquad\\qquad\\bar{\\mu}_{t,L}=\\bar{\\Sigma}_{t,L}\\bar{B}_{t,L}\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "646 Finally, $\\bar{G}_{t,\\ell}$ and $\\bar{B}_{t,\\ell}$ for $\\ell\\in[L]$ are computed recursively. The basis of the recursion are ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\bar{G}_{t,1}=\\mathbf{W}_{1}^{\\top}\\sum_{i=1}^{K}\\left(\\Sigma_{1}^{-1}-\\Sigma_{1}^{-1}\\hat{\\Sigma}_{t,i}\\Sigma_{1}^{-1}\\right)\\mathbf{W}_{1}\\,,\\qquad\\bar{B}_{t,1}=\\mathbf{W}_{1}^{\\top}\\Sigma_{1}^{-1}\\sum_{i=1}^{K}\\hat{\\Sigma}_{t,i}\\hat{G}_{t,i}\\hat{B}_{t,i}\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "647 Then, the recursive step for $\\ell\\in[L]/\\{1\\}$ is, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\bar{G}_{t,\\ell}=\\mathrm{W}_{\\ell}^{\\top}\\,\\big(\\Sigma_{\\ell}^{-1}-\\Sigma_{\\ell}^{-1}\\bar{\\Sigma}_{t,\\ell-1}\\Sigma_{\\ell}^{-1}\\big)\\mathrm{W}_{\\ell}\\,,\\qquad\\qquad\\bar{B}_{t,\\ell}=\\mathrm{W}_{\\ell}^{\\top}\\,\\Sigma_{\\ell}^{-1}\\bar{\\Sigma}_{t,\\ell-1}\\bar{B}_{t,\\ell-1}\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "648 This concludes the derivation of our posterior approximation. Note that these approximations are exact   \n649 when the reward distribution follows a linear-Gaussian model, $P(\\cdot\\mid x;\\theta_{*,a})^{\\stackrel{\\cdot}{}}{\\stackrel{\\cdot}{=}}N(\\cdot;x^{\\top}\\theta_{*,a},\\sigma^{2})$ . ", "page_idx": 15}, {"type": "text", "text": "650 B.2 Derivation of Action-Posteriors for Linear Diffusion Models ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "651 To simplify derivations, we consider the case where the reward distribution is indeed linear  \n652 Gaussian as $P(\\cdot\\mid X_{t};\\theta_{*,A_{t}})=\\mathcal{N}\\left(X_{t}^{\\top}\\theta_{*,A_{t}},\\sigma^{2}\\right)$ , but the same derivations can be applied when   \n653 the rewards are non-linear. In this case, the likelihood approximation in (16) becomes exact as   \n654 we have that $\\mathbb{P}\\left(H_{t,i}\\,|\\,\\theta_{*,i}=\\theta\\right)\\,\\propto\\,\\mathcal{N}\\big(\\theta;\\hat{B}_{t,i},\\hat{G}_{t,i}^{-1}\\big)$ , where $\\hat{B}_{t,i}$ is the corresponding MLE and   \n655 $\\begin{array}{r}{\\hat{G}_{t,i}=\\sigma^{-2}\\sum_{k\\in S_{t,i}}X_{k}X_{k}^{\\top}}\\end{array}$ in this case. Our derivations rely on the fact that the MLE $\\hat{B}_{t,i}$ in this   \n656 linear-Gaussian case satisfies: $\\begin{array}{r}{\\hat{G}_{t,i}\\hat{B}_{t,i}=v\\sum_{k\\in S_{t,i}}X_{k}Y_{k}^{\\top}}\\end{array}$ . ", "page_idx": 15}, {"type": "text", "text": "657 Proposition B.1. Consider the following model, which corresponds to the last two layers in Eq. (15) ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\theta_{*,i}\\mid\\psi_{*,1}\\sim\\mathcal{N}\\left(\\mathrm{W}_{1}\\psi_{*,1},\\Sigma_{1}\\right)\\,,\\ }\\\\ {Y_{t}\\mid X_{t},\\theta_{*,A_{t}}\\sim\\mathcal{N}\\left(X_{t}^{\\top}\\theta_{*,A_{t}},\\sigma^{2}\\right)\\,,\\ }\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\n\\forall t\\in[n]\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "658 Then we have that for any $t\\,\\in\\,[n]$ and $i\\,\\in\\,[K],\\,P_{t,i}(\\theta\\mid\\psi_{1})\\,=\\mathbb{P}\\,(\\theta_{*,i}=\\theta\\,|\\,\\psi_{*,1}=\\psi_{1},H_{t,i})\\,=$   \n659 $\\mathcal{N}(\\theta;\\hat{\\mu}_{t,i},\\hat{\\Sigma}_{t,i})$ , where ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\hat{\\Sigma}_{t,i}^{-1}=\\hat{G}_{t,i}+\\Sigma_{1}^{-1}\\,,\\qquad\\qquad\\hat{\\mu}_{t,i}=\\hat{\\Sigma}_{t,i}\\left(\\hat{G}_{t,i}\\hat{B}_{t,i}+\\Sigma_{1}^{-1}\\mathrm{W}_{1}\\psi_{1}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "660 Proof. Let $v=\\sigma^{-2}\\,,\\quad\\Lambda_{1}=\\Sigma_{1}^{-1}$ . Then the action-posterior decomposes as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad-\\underset{k\\in\\mathcal{S}_{t,i}}{\\1\\sim}\\dots\\underset{\\mathcal{S}_{k}}{\\1\\sim}\\dots\\underset{\\mathcal{S}_{k}}{\\1\\sim}\\dots\\underset{\\mathcal{S}_{k}}{\\1\\sim}\\dots\\underset{\\mathcal{S}_{k}}{\\1\\sim}\\dots\\underset{\\mathcal{S}_{k}}{\\1\\sim}\\dots\\underset{\\mathcal{S}_{k}}{\\1\\sim}\\dots\\underset{\\mathcal{S}_{k}}{\\1\\sim}\\dots\\underset{\\mathcal{S}_{k}}{1\\sim}\\dots\\underset{\\mathcal{S}_{k}}{1\\sim}\\dots\\underset{\\mathcal{S}_{k}}{1\\sim}\\dots\\underset{\\mathcal{S}_{k}}{1\\sim}\\dots\\underset{\\mathcal{S}_{k}}{1\\sim}\\dots\\underset{\\mathcal{S}_{k}}{1\\sim}\\dots}\\\\ &{=\\exp\\Big[-\\frac{1}{2}\\Big(\\underset{k\\in\\mathcal{S}_{t,i}}{\\sum}\\underset{\\mathcal{S}_{k}}{\\sum}(Y_{k}^{2}-2Y_{k}X_{k}^{\\top}\\theta+(X_{k}^{\\top}\\theta)^{2})+\\theta^{\\top}\\Lambda_{1}\\theta-2\\theta^{\\top}\\Lambda_{1}\\mathrm{W}_{1}\\psi_{1}}\\\\ &{\\qquad\\qquad\\quad+\\left(\\mathrm{W}_{1}\\psi_{1}\\right)^{\\top}\\Lambda_{1}\\big(\\mathrm{W}_{1}\\psi_{1}\\big)\\Big)\\Big]\\,,}\\\\ &{\\qquad\\qquad\\times\\exp\\Big[{-\\frac{1}{2}\\Big(\\theta^{\\top}(v\\underset{k\\in\\mathcal{S}_{t,i}}{\\sum}X_{k}X_{k}^{\\top}+\\Lambda_{1})\\theta-2\\theta^{\\top}\\Big(v\\underset{k\\in\\mathcal{S}_{t,i}}{\\sum}X_{k}Y_{k}+\\Lambda_{1}\\mathrm{W}_{1}\\psi_{1}\\Big)\\Big)}\\Big]\\,,}\\\\ &{\\underset{\\times}{\\times}\\Lambda\\big(\\theta;\\hat{\\mu}_{t,i},\\hat{\\Lambda}_{t,i}^{-1}\\big)\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "661 with $\\begin{array}{r}{\\hat{\\Lambda}_{t,i}=v\\sum_{k\\in S_{t,i}}X_{k}X_{k}^{\\top}+\\Lambda_{1}}\\end{array}$ , $\\begin{array}{r}{\\hat{\\Lambda}_{t,i}\\hat{\\mu}_{t,i}=v\\sum_{k\\in S_{t,i}}X_{k}Y_{k}+\\Lambda_{1}\\mathrm{W}_{1}\\psi_{1}}\\end{array}$ . Using that, in this   \n662 linear-Gaussian case, $\\begin{array}{r}{\\hat{G}_{t,i}\\;=\\;v\\sum_{k\\in S_{t,i}}X_{k}X_{k}^{\\top}}\\end{array}$ and $\\begin{array}{r}{\\hat{G}_{t,i}\\hat{B}_{t,i}\\;=\\;v\\sum_{k\\in S_{t,i}}X_{k}Y_{k}}\\end{array}$ concludes the   \n663 proof. \u53e3   \n664 The same proof applies when the reward distribution is not linear-Gaussian, with the approximation   \n665 $\\mathbb{P}\\left(H_{t,i}\\,|\\,\\theta_{*,i}^{-}=\\theta\\right)\\approx\\mathcal{N}\\big(\\theta;\\hat{B}_{t,i},\\hat{G}_{t,i}^{-1}\\big)$ . Using this approximation in the derivations above leads to   \n666 the same results. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "667 B.3 Derivation of recursive latent-posteriors for linear diffusion models ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "668 Again, to simplify derivations, we consider the case where the reward distribution is indeed linear  \n669 Gaussian as $\\Dot{P}(\\cdot\\mid X_{t};\\theta_{*,A_{t}})=\\mathcal{N}\\left(X_{t}^{\\top}\\theta_{*,A_{t}},\\sigma^{2}\\right)$ , but the same derivations can be applied when the   \n670 rewards are non-linear.   \n671 Proposition B.2. For any $\\ell\\in[L]/\\{1\\}$ , the $\\ell-1$ -th conditional latent-posterior reads $Q_{t,\\ell-1}(\\cdot\\ |$   \n672 $\\psi_{\\ell})=\\mathcal{N}(\\bar{\\mu}_{t,\\ell-1},\\bar{\\Sigma}_{t,\\ell-1})$ , with ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bar{\\Sigma}_{t,\\ell-1}^{-1}=\\Sigma_{\\ell}^{-1}+\\bar{G}_{t,\\ell-1}\\,,\\qquad\\qquad\\bar{\\mu}_{t,\\ell-1}=\\bar{\\Sigma}_{t,\\ell-1}\\!\\left(\\Sigma_{\\ell}^{-1}\\mathrm{W}_{\\ell}\\psi_{\\ell}+\\bar{B}_{t,\\ell-1}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "673 and the $L$ -th latent-posterior reads $Q_{t,L}(\\cdot)=\\mathcal{N}(\\bar{\\mu}_{t,L},\\bar{\\Sigma}_{t,L})$ , with ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\bar{\\Sigma}_{t,L}^{-1}=\\Sigma_{L+1}^{-1}+\\bar{G}_{t,L}\\,,\\qquad\\qquad\\qquad\\qquad\\bar{\\mu}_{t,L}=\\bar{\\Sigma}_{t,L}\\bar{B}_{t,L}\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "674 Proof. Let $\\ell\\in[L]/\\{1\\}$ . Then, Bayes rule yields that ", "page_idx": 16}, {"type": "equation", "text": "$$\nQ_{t,\\ell-1}(\\psi_{\\ell-1}\\mid\\psi_{\\ell})\\propto\\mathbb{P}\\left(H_{t}\\,|\\,\\psi_{*,\\ell-1}=\\psi_{\\ell-1}\\right)\\!\\mathcal{N}(\\psi_{\\ell-1},\\mathrm{W}_{\\ell}\\psi_{\\ell},\\Sigma_{\\ell})\\,,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "675 But from Lemma B.3, we know that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(H_{t}\\,|\\,\\psi_{*,\\ell-1}=\\psi_{\\ell-1}\\right)\\propto\\exp\\Big[-\\frac{1}{2}\\psi_{\\ell-1}^{\\top}\\bar{G}_{t,\\ell-1}\\psi_{\\ell-1}+\\psi_{\\ell-1}^{\\top}\\bar{B}_{t,\\ell-1}\\Big]\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "676 Therefore, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q_{t,\\ell-1}(\\psi_{\\ell-1}\\mid\\psi_{\\ell})\\propto\\exp\\Big[-\\frac{1}{2}\\psi_{\\ell-1}^{\\top}\\bar{G}_{t,\\ell-1}\\psi_{\\ell-1}+\\psi_{\\ell-1}^{\\top}\\bar{B}_{t,\\ell-1}\\Big]N(\\psi_{\\ell-1},\\mathrm{W}_{\\ell}\\psi_{\\ell},\\Sigma_{\\ell})\\,,}\\\\ &{\\qquad\\qquad\\qquad\\propto\\exp\\Big[-\\frac{1}{2}\\psi_{\\ell-1}^{\\top}\\bar{G}_{t,\\ell-1}\\psi_{\\ell-1}+\\psi_{\\ell-1}^{\\top}\\bar{B}_{t,\\ell-1}}\\\\ &{\\qquad\\qquad\\qquad\\quad-\\frac{1}{2}(\\psi_{\\ell-1}-\\mathrm{W}_{\\ell}\\psi_{\\ell})^{\\top}\\Sigma_{\\ell}^{-1}(\\psi_{\\ell-1}-\\mathrm{W}_{\\ell}\\psi_{\\ell}))\\Big]\\,,}\\\\ &{\\overset{(i)}{\\propto}\\exp\\Big[-\\frac{1}{2}\\psi_{\\ell-1}^{\\top}\\big(\\bar{G}_{t,\\ell-1}+\\Sigma_{\\ell}^{-1}\\big)\\psi_{\\ell-1}+\\psi_{\\ell-1}^{\\top}\\big(\\bar{B}_{t,\\ell-1}+\\Sigma_{\\ell}^{-1}\\mathrm{W}_{\\ell}\\psi_{\\ell}\\big)\\Big]\\,,}\\\\ &{\\overset{(i i)}{\\propto}\\mathcal{N}(\\psi_{\\ell-1};\\bar{\\mu}_{t,\\ell-1},\\bar{\\Sigma}_{t,\\ell-1})\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "677 with $\\bar{\\Sigma}_{t,\\ell-1}^{-1}=\\Sigma_{\\ell}^{-1}+\\bar{G}_{t,\\ell-1}$ and $\\bar{\\mu}_{t,\\ell-1}=\\bar{\\Sigma}_{t,\\ell-1}\\big(\\Sigma_{\\ell}^{-1}\\mathrm{W}_{\\ell}\\psi_{\\ell}+\\bar{B}_{t,\\ell-1}\\big)$ . In $(i)$ , we omit terms that   \n678 are constant in $\\psi_{\\ell-1}$ . In $(i i)$ , we complete the square. This concludes the proof for $\\ell\\in[L]/\\{1\\}$ . For   \n679 $Q_{t,L}$ , we use Bayes rule to get ", "page_idx": 17}, {"type": "equation", "text": "$$\nQ_{t,L}(\\psi_{L})\\propto\\mathbb{P}\\left(H_{t}\\,|\\,\\psi_{*,L}=\\psi_{L}\\right)\\mathcal{N}(\\psi_{L},0,\\Sigma_{L+1})\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "680 Then from Lemma B.3, we know that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(H_{t}\\,|\\,\\psi_{*,L}=\\psi_{L}\\right)\\propto\\exp\\Big[-\\frac{1}{2}\\psi_{L}^{\\top}\\bar{G}_{t,L}\\psi_{L}+\\psi_{L}^{\\top}\\bar{B}_{t,L}\\Big]\\,,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "681 We then use the same derivations above to compute the product $\\exp\\Big[-\\textstyle\\frac{1}{2}\\psi_{L}^{\\top}\\bar{G}_{t,L}\\psi_{L}+\\psi_{L}^{\\top}\\bar{B}_{t,L}\\Big]\\;\\times$   \n682 $\\mathcal{N}(\\psi_{L},0,\\Sigma_{L+1})$ , which concludes the proof. ", "page_idx": 17}, {"type": "text", "text": "683 Lemma B.3. The following holds for any $t\\in[n]$ and $\\ell\\in[L]$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(H_{t}\\,|\\,\\psi_{*,\\ell}=\\psi_{\\ell}\\right)\\propto\\exp\\Big[-\\frac{1}{2}\\psi_{\\ell}^{\\top}\\bar{G}_{t,\\ell}\\psi_{\\ell}+\\psi_{\\ell}^{\\top}\\bar{B}_{t,\\ell}\\Big]\\,,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "684 where $\\bar{G}_{t,\\ell}$ and $\\bar{B}_{t,\\ell}$ are defined by recursion in Section 3.1. ", "page_idx": 17}, {"type": "text", "text": "685 Proof. We prove this result by induction. To reduce clutter, we let $v=\\sigma^{-2}$ , and $\\Lambda_{1}=\\Sigma_{1}^{-1}$ . We   \n686 start with the base case of the induction when $\\ell=1$ .   \n687 (I) Base case. Here we want to show that $\\begin{array}{r}{\\mathbb{P}\\left(H_{t}\\,|\\,\\psi_{*,1}=\\psi_{1}\\right)\\propto\\exp\\Big[-\\frac{1}{2}\\psi_{1}^{\\top}\\bar{G}_{t,1}\\psi_{1}+\\psi_{1}^{\\top}\\bar{B}_{t,1}\\Big)\\Big],}\\end{array}$   \n688 where $\\bar{G}_{t,1}$ and ${\\bar{B}}_{t,1}$ are given in Eq. (20). First, we have that ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\mathbb{P}\\left(H_{t}\\,|\\,\\psi_{*,1}=\\psi_{1}\\right)\\overset{(i)}{=}\\prod_{i\\in[K]}\\mathbb{P}\\left(H_{t,i}\\,|\\,\\psi_{*,1}=\\psi_{1}\\right)=\\displaystyle\\prod_{i\\in[K]}\\int_{\\theta}\\mathbb{P}\\left(H_{t,i},\\theta_{*,i}=\\theta\\,|\\,\\psi_{*,1}=\\psi_{1}\\right)\\,\\mathrm{d}\\theta\\,,}&{}\\\\ {=\\displaystyle\\prod_{i\\in[K]}\\int_{\\theta}\\mathbb{P}\\left(H_{t,i}\\,|\\,\\theta_{*,i}=\\theta\\right)\\mathcal{N}(\\theta;\\mathbb{W}_{1}\\psi_{1},\\Sigma_{1})\\,\\mathrm{d}\\theta\\,,}&{}\\\\ {=\\displaystyle\\prod_{i\\in[K]}\\int_{\\theta}\\Big(\\prod_{k\\in\\mathcal{S}_{t,i}}\\mathcal{N}(Y_{k};X_{k}^{\\top}\\theta,\\sigma^{2})\\Big)\\mathcal{N}(\\theta;\\mathbb{W}_{1}\\psi_{1},\\Sigma_{1})\\,\\mathrm{d}\\theta\\,,}&{}\\\\ {=\\displaystyle\\prod_{i\\in[K]}h_{i}(\\psi_{1})\\,,}&{\\ h_{i}(\\psi_{1})\\,~~~~~~~~~~~~~~~~~~}\\,}&{(1-\\psi_{1})\\,~~~~~}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "689 where $(i)$ follows from the fact that $\\theta_{*,i}$ for $\\begin{array}{r l r}{i}&{{}\\in}&{[K]}\\end{array}$ are conditionally independent given   \n690 $\\psi_{*,1}~=~\\psi_{1}$ and that given $\\theta_{*,i},\\ H_{t,i}$ is independent of $\\psi_{*,1}$ . Now we compute $h_{i}(\\Bar{\\psi_{1}})\\;=\\;$   \n691 $\\begin{array}{r}{\\int_{\\theta}\\left(\\prod_{k\\in S_{t,i}}\\mathcal{N}(Y_{k};X_{k}^{\\top}\\theta,\\sigma^{2})\\right)\\mathcal{N}\\left(\\theta;\\mathrm{W}_{1}\\psi_{1},\\Sigma_{1}\\right)\\mathrm{d}\\theta}\\end{array}$ as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{h_{i}(\\psi_{1})=\\displaystyle\\int_{\\theta}\\Big(\\prod_{k\\in S_{i,i}}\\mathcal{N}(Y_{k};X_{k}^{\\top}\\theta,\\sigma^{2})\\Big)\\mathcal{N}(\\theta;\\mathrm{W}_{1}\\psi_{1},\\Sigma_{1})\\,{\\mathrm{d}}\\theta\\,,}\\\\ &{\\propto\\displaystyle\\int_{\\theta}\\exp\\Big[-\\frac{1}{2}v\\sum_{k\\in S_{i,i}}(Y_{k}-X_{k}^{\\top}\\theta)^{2}-\\frac{1}{2}(\\theta-\\mathrm{W}_{1}\\psi_{1})^{\\top}\\Lambda_{1}(\\theta-\\mathrm{W}_{1}\\psi_{1})\\Big]\\,{\\mathrm{d}}\\theta\\,,}\\\\ &{=\\displaystyle\\int_{\\theta}\\exp\\Big[-\\frac{1}{2}\\Big(v\\sum_{k\\in S_{i,i}}(Y_{k}^{2}-2Y_{k}\\theta^{\\top}X_{k}+(\\theta^{\\top}X_{k})^{2})+\\theta^{\\top}\\Lambda_{1}\\theta-2\\theta^{\\top}\\Lambda_{1}\\mathrm{W}_{1}\\psi_{1}}\\\\ &{\\qquad\\qquad\\quad+(\\mathrm{W}_{1}\\psi_{1})^{\\top}\\Lambda_{1}(\\mathrm{W}_{1}\\psi_{1})\\Big)\\Big]\\,{\\mathrm{d}}\\theta\\,,}\\\\ &{\\propto\\displaystyle\\int_{\\theta}\\exp\\Big[-\\frac{1}{2}\\Big(\\theta^{\\top}\\Big(v\\sum_{k\\in S_{i,i}}X_{k}^{\\top}+\\Lambda_{1}\\Big)\\theta-2\\theta^{\\top}\\Big(v\\sum_{k\\in S_{i,i}}Y_{k}X_{k}}\\\\ &{\\qquad\\qquad\\quad+\\Lambda_{1}\\mathrm{W}_{1}\\psi_{1}\\Big)+(\\mathrm{W}_{1}\\psi_{1})^{\\top}\\Lambda_{1}(\\mathrm{W}_{1}\\psi_{1})\\Big)\\Big]\\,{\\mathrm{d}}\\theta\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "692 But we know that $\\begin{array}{r}{\\hat{G}_{t,i}=v\\sum_{k\\in S_{t,i}}X_{k}X_{k}^{\\top}}\\end{array}$ , and $\\begin{array}{r}{\\hat{G}_{t,i}\\hat{B}_{t,i}=v\\sum_{k\\in S_{t,i}}Y_{k}X_{k}}\\end{array}$ (because we assumed   \n693 linear-Gaussian likelihood). To further simplify expressions, we also let ", "page_idx": 18}, {"type": "equation", "text": "$$\nV=\\left(\\hat{G}_{t,i}+\\Lambda_{1}\\right)^{-1},\\quad U=V^{-1}\\,,\\quad\\beta=V\\big(\\hat{G}_{t,i}\\hat{B}_{t,i}+\\Lambda_{1}{\\mathrm{W}}_{1}\\psi_{1}\\big)\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "694 We have that $U V=V U=I_{d}$ , and thus ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathrm{\\partial}_{\\mathbb{t}}(\\psi_{1})\\propto\\int_{\\mathbb{R}^{3}}\\mathrm{e}^{\\mathrm{i}\\left(\\mathbf{\\ell}_{2}\\times\\int_{0}^{\\infty}\\mathrm{i}\\left(\\partial^{\\tau}U\\theta-2\\theta^{\\top}U V\\left(\\hat{G}_{t_{i}}\\hat{B}_{t_{i}}+\\Lambda_{1}\\mathbf{W}_{1}\\psi_{1}\\right)+(\\mathbf{W}_{1}\\psi_{1})^{\\top}\\Lambda_{1}(\\mathbf{W}_{1}\\psi_{1})\\right)\\right)\\mathrm{d}\\theta},}}\\\\ &{=\\int_{\\theta}\\mathrm{e}^{\\mathrm{i}\\phi}\\left[-\\frac{1}{2}\\left(\\theta^{\\top}U\\theta-2\\theta^{\\top}U\\beta+(\\mathbf{W}_{1}\\psi_{1})^{\\top}\\Lambda_{1}(\\mathbf{W}_{1}\\psi_{1})\\right)\\right]\\mathrm{d}\\theta,}\\\\ &{=\\int_{\\theta}\\mathrm{e}^{\\mathrm{i}\\phi}\\left[-\\frac{1}{2}\\left((\\theta-\\beta)^{\\top}U(\\theta-\\beta)-\\beta^{\\top}U\\beta+(\\mathbf{W}_{1}\\psi_{1})^{\\top}\\Lambda_{1}(\\mathbf{W}_{1}\\psi_{1})\\right)\\right]\\mathrm{d}\\theta,}\\\\ &{\\propto\\mathrm{e}\\exp\\left[-\\frac{1}{2}\\left(-\\beta^{\\top}U\\beta+(\\mathbf{W}_{1}\\psi_{1})^{\\top}\\Lambda_{1}(\\mathbf{W}_{1}\\psi_{1})\\right)\\right],}\\\\ &{=\\exp\\left[-\\frac{1}{2}\\left(-\\left(\\hat{G}_{t_{i}}\\hat{B}_{t_{i}}+\\Lambda_{1}\\mathbf{W}_{1}\\psi_{1}\\right)^{\\top}V\\left(\\hat{G}_{t_{i}}\\hat{B}_{t_{i}}+\\Lambda_{1}\\mathbf{W}_{1}\\psi_{1}\\right)+(\\mathbf{W}_{1}\\psi_{1})^{\\top}\\Lambda_{1}(\\mathbf{W}_{1}\\psi_{1})\\right)\\right.}\\\\ &{\\times\\exp\\left[-\\frac{1}{2}\\left(\\psi_{1}^{\\top}\\mathbf{W}_{1}^{\\top}(\\Lambda_{1}-\\Lambda_{1}V\\Lambda_{1})\\mathbf{W}_{1}\\psi_{1}-2\\mathrm{e}^{\\top}\\left(\\mathbf{W}_{1}^{\\top}\\mathbf{W \n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "695 where ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Omega_{i}=\\mathrm{W}_{1}^{\\top}\\left(\\Lambda_{1}-\\Lambda_{1}V\\Lambda_{1}\\right)\\mathrm{W}_{1}=\\mathrm{W}_{1}^{\\top}\\left(\\Lambda_{1}-\\Lambda_{1}(\\hat{G}_{t,i}+\\Lambda_{1})^{-1}\\Lambda_{1}\\right)\\mathrm{W}_{1}\\,,}\\\\ &{m_{i}=\\mathrm{W}_{1}^{\\top}\\Lambda_{1}V\\hat{G}_{t,i}\\hat{B}_{t,i}=\\mathrm{W}_{1}^{\\top}\\Lambda_{1}(\\hat{G}_{t,i}+\\Lambda_{1})^{-1}\\hat{G}_{t,i}\\hat{B}_{t,i}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "696 But notice that $V=(\\hat{G}_{t,i}+\\Lambda_{1})^{-1}=\\hat{\\Sigma}_{t,i}$ and thus ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Omega_{i}=\\mathrm{W}_{1}^{\\top}\\big(\\Lambda_{1}-\\Lambda_{1}\\hat{\\Sigma}_{t,i}\\Lambda_{1}\\big)\\mathrm{W}_{1}\\,,\\qquad\\qquad\\quad m_{i}=\\mathrm{W}_{1}^{\\top}\\Lambda_{1}\\hat{\\Sigma}_{t,i}\\hat{G}_{t,i}\\hat{B}_{t,i}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "697 Finally, we plug this result in Eq. (24) to get ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\mathbb P\\left(H_{t}\\,|\\,\\psi_{*,1}=\\psi_{1}\\right)=\\prod_{i\\in[K]}\\,h_{i}(\\psi_{1})\\propto\\prod_{i\\in[K]}\\,\\exp\\left[-\\frac12\\psi_{1}^{\\top}\\Omega_{i}\\psi_{1}+\\psi_{1}^{\\top}m_{i}\\right]\\,,}}\\\\ {{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\exp\\left[-\\frac12\\psi_{1}^{\\top}\\sum_{i\\in[K]}\\Omega_{i}\\psi_{1}+\\psi_{1}^{\\top}\\sum_{i\\in[K]}m_{i}\\right]\\,,}}\\\\ {{\\displaystyle\\qquad\\qquad\\qquad\\qquad=\\exp\\left[-\\frac12\\psi_{1}^{\\top}\\bar{G}_{t,1}\\psi_{1}+\\psi_{1}^{\\top}\\bar{B}_{t,1}\\right]\\,,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "698 where ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{{\\displaystyle{\\bar{G}}_{t,1}=\\sum_{i=1}^{K}\\Omega_{i}=\\sum_{i=1}^{K}\\mathrm{W}_{1}^{\\top}\\big(\\Lambda_{1}-\\Lambda_{1}\\hat{\\Sigma}_{t,i}\\Lambda_{1}\\big)\\mathrm{W}_{1}=\\mathrm{W}_{1}^{\\top}\\sum_{i=1}^{K}\\big(\\Sigma_{1}^{-1}-\\Sigma_{1}^{-1}\\hat{\\Sigma}_{t,i}\\Sigma_{1}^{-1}\\big)\\mathrm{W}_{1}\\,,}}\\\\ {{\\displaystyle{\\bar{B}}_{t,1}=\\sum_{i=1}^{K}m_{i}=\\sum_{i=1}^{K}\\hat{\\Sigma}_{t,i}\\hat{G}_{t,i}\\hat{B}_{t,i}=\\mathrm{W}_{1}^{\\top}\\Sigma_{1}^{-1}\\sum_{i=1}^{K}\\hat{\\Sigma}_{t,i}\\hat{G}_{t,i}\\hat{B}_{t,i}\\,.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "699 This concludes the proof of the base case. ", "page_idx": 18}, {"type": "text", "text": "700 $(\\mathbf{II})$ Induction step. Let $\\ell\\in[L]/\\{1\\}$ . Suppose that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(H_{t}\\,|\\,\\psi_{*,\\ell-1}=\\psi_{\\ell-1}\\right)\\propto\\exp\\left[-\\frac{1}{2}\\psi_{\\ell-1}^{\\top}\\bar{G}_{t,\\ell-1}\\psi_{\\ell-1}+\\psi_{\\ell-1}^{\\top}\\bar{B}_{t,\\ell-1}\\right]\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(H_{t}\\,|\\,\\psi_{*,\\ell}=\\psi_{\\ell}\\right)\\propto\\exp\\left[-\\frac{1}{2}\\psi_{\\ell}^{\\top}\\bar{G}_{t,\\ell}\\psi_{\\ell}+\\psi_{\\ell}^{\\top}\\bar{B}_{t,\\ell}\\right]\\,,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "702 where ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{G}_{t,\\ell}=\\mathrm{W}_{\\ell}^{\\top}\\left(\\Sigma_{\\ell}^{-1}-\\Sigma_{\\ell}^{-1}\\bar{\\Sigma}_{t,\\ell-1}\\Sigma_{\\ell}^{-1}\\right)\\mathrm{W}_{\\ell}=\\mathrm{W}_{\\ell}^{\\top}\\left(\\Sigma_{\\ell}^{-1}-\\Sigma_{\\ell}^{-1}(\\Sigma_{\\ell}^{-1}+\\bar{G}_{t,\\ell-1})^{-1}\\Sigma_{\\ell}^{-1}\\right)\\mathrm{W}_{\\ell}\\,,}\\\\ &{\\bar{B}_{t,\\ell}=\\mathrm{W}_{\\ell}^{\\top}\\Sigma_{\\ell}^{-1}\\bar{\\Sigma}_{t,\\ell-1}\\bar{B}_{t,\\ell-1}=\\mathrm{W}_{\\ell}^{\\top}\\Sigma_{\\ell}^{-1}(\\Sigma_{\\ell}^{-1}+\\bar{G}_{t,\\ell-1})^{-1}\\bar{B}_{t,\\ell-1}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "703 To achieve this, we start by expressing $\\mathbb{P}\\left(H_{t}\\,|\\,\\psi_{*,\\ell}=\\psi_{\\ell}\\right)$ in terms of $\\mathbb{P}\\left(H_{t}\\,|\\,\\psi_{*,\\ell-1}=\\psi_{\\ell-1}\\right)$ as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\big(H_{t}\\,|\\,\\psi_{*},\\varepsilon=\\psi_{t}\\big)=\\displaystyle\\int_{\\psi_{t-1}}\\mathbb{P}\\big(H_{t},\\psi_{*},\\varepsilon_{-1}=\\psi_{t-1}\\,|\\,\\psi_{*,\\varepsilon}=\\psi_{t}\\big)\\,\\mathrm{d}\\psi_{t-1}\\,,}\\\\ &{=\\displaystyle\\int_{\\psi_{t-1}}\\mathbb{P}\\big(H_{t}\\,|\\,\\psi_{*,\\varepsilon-1}=\\psi_{\\varepsilon-1},\\psi_{*,\\varepsilon}=\\psi_{t}\\big)\\mathcal{N}(\\psi_{\\varepsilon-1};\\mathrm{W}_{\\varepsilon}\\psi_{\\varepsilon},\\Sigma_{\\varepsilon})\\,\\mathrm{d}\\psi_{\\varepsilon-1}\\,,}\\\\ &{=\\displaystyle\\int_{\\psi_{t-1}}\\mathbb{P}\\big(H_{t}\\,|\\,\\psi_{*,\\varepsilon-1}=\\psi_{\\varepsilon-1}\\big)\\mathcal{N}(\\psi_{\\varepsilon-1};\\mathrm{W}_{\\varepsilon}\\psi_{\\varepsilon},\\Sigma_{\\varepsilon})\\,\\mathrm{d}\\psi_{\\varepsilon-1}\\,,}\\\\ &{\\propto\\displaystyle\\int_{\\psi_{t-1}}\\exp\\Big[-\\frac{1}{2}\\psi_{t-1}^{\\top}\\bar{G}_{t,\\varepsilon-1}\\psi_{\\varepsilon-1}+\\psi_{t-1}^{\\top}\\bar{B}_{t,\\varepsilon-1}\\Big]\\mathcal{N}(\\psi_{\\varepsilon-1};\\mathrm{W}_{\\varepsilon}\\psi_{\\varepsilon},\\Sigma_{\\varepsilon})\\,\\mathrm{d}\\psi_{\\varepsilon-1}\\,,}\\\\ &{\\propto\\displaystyle\\int_{\\psi_{t-1}}\\exp\\Big[-\\frac{1}{2}\\psi_{t-1}^{\\top}\\bar{G}_{t,\\varepsilon-1}\\psi_{\\varepsilon-1}+\\psi_{t-1}^{\\top}\\bar{B}_{t,\\varepsilon-1}}\\\\ &{\\qquad\\qquad\\qquad+\\left(\\psi_{t-1}-\\mathrm{W}_{\\varepsilon}\\psi_{\\varepsilon}\\right)^{\\top}\\Lambda_{\\varepsilon}(\\psi_{\\varepsilon-1}-\\mathrm{W}_{\\varepsilon}\\psi_{\\varepsilon})\\Big]\\,\\mathrm{d}\\psi_{t-1}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "704 Now let $S=\\bar{G}_{t,\\ell-1}+\\Lambda_{\\ell}$ and $V=\\bar{B}_{t,\\ell-1}+\\Lambda_{\\ell}\\mathrm{W}_{\\ell}\\psi_{\\ell}$ . Then we have that, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\big(H_{t}\\mid\\psi_{s},\\epsilon=\\psi_{t}\\big)}\\\\ &{\\propto\\int_{\\psi_{t-1}}\\exp\\left[-\\frac{1}{2}\\psi_{t-1}^{T}\\bar{G}_{t}\\ell_{t-1}\\psi_{t-1}+\\psi_{t-1}^{T}\\bar{B}_{t}\\ell_{t-1}\\right.}\\\\ &{\\qquad\\qquad\\left.+\\left(\\psi_{t-1}-\\mathrm{N}_{t}\\psi_{t}\\psi_{t}\\right)^{T}\\Lambda_{t}(\\psi_{t-1}-\\mathrm{N}_{t}\\psi_{t}\\psi_{t})\\right]\\mathrm{d}\\psi_{t-1}\\,,}\\\\ &{\\propto\\int_{\\psi_{t-1}}\\exp\\left[-\\frac{1}{2}\\left(\\psi_{t-1}^{T}S\\psi_{t-1}-2\\psi_{t-1}^{T}\\left(\\bar{B}_{t},\\ell_{t-1}+\\Lambda_{t}\\mathrm{N}_{t}\\psi_{t}\\psi_{t}\\right)+\\psi_{t}^{T}\\mathrm{W}_{t}^{T}\\Lambda_{t}\\mathrm{W}_{t}\\psi_{t}\\psi_{t}\\right)\\right]\\mathrm{d}\\psi_{t-1}\\,,}\\\\ &{=\\int_{\\psi_{t-1}}\\exp\\left[-\\frac{1}{2}\\left(\\psi_{t-1}^{T}S(\\psi_{t-1}-2S^{-1}V)+\\psi_{t}^{T}\\mathrm{W}_{t}^{T}\\Lambda_{t}\\mathrm{W}_{t}\\psi_{t}\\psi_{t}\\right)\\right]\\mathrm{d}\\psi_{t-1}\\,,}\\\\ &{=\\int_{\\psi_{t-1}}\\exp\\left[-\\frac{1}{2}\\left((\\psi_{t-1}-S^{-1}V)^{T}S(\\psi_{t-1}-S^{-1}V)\\right.\\right.}\\\\ &{\\qquad\\qquad\\qquad\\left.\\left.+\\psi_{t}^{T}\\mathrm{W}_{t}^{T}\\Lambda_{t}\\mathrm{W}_{t}\\psi_{t}-V^{T}S^{-1}V\\right)\\right]\\mathrm{d}\\psi_{t-1}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "705 In the second step, we omit constants in $\\psi_{\\ell}$ and $\\psi_{\\ell-1}$ . Thus ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(H_{t}\\,|\\,\\psi_{*,\\ell}=\\psi_{\\ell}\\right)}\\\\ &{\\propto\\displaystyle\\int_{\\psi_{\\ell-1}}\\exp\\left[-\\frac{1}{2}\\left((\\psi_{\\ell-1}-S^{-1}V)^{\\top}S(\\psi_{\\ell-1}-S^{-1}V)+\\psi_{\\ell}^{\\top}\\mathbf{W}_{\\ell}^{\\top}\\Lambda_{\\ell}\\mathbf{W}_{\\ell}\\psi_{\\ell}-V^{\\top}S^{-1}V\\right)\\right]\\mathrm{d}\\psi_{\\ell-1},}\\\\ &{\\propto\\exp\\left[-\\frac{1}{2}\\left(\\psi_{\\ell}^{\\top}\\mathbf{W}_{\\ell}^{\\top}\\Lambda_{\\ell}\\mathbf{W}_{\\ell}\\psi_{\\ell}-V^{\\top}S^{-1}V\\right)\\right]\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "706 It follows that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(H_{t}\\,|\\,\\psi_{*,\\ell}=\\psi_{\\ell}\\right)}\\\\ &{\\propto\\exp\\left[-\\frac{1}{2}\\left(\\psi_{\\ell}^{\\top}\\mathbf{W}_{\\ell}^{\\top}\\mathbf{\\Lambda}_{\\ell}\\mathbf{W}_{\\ell}\\psi_{\\ell}-V^{\\top}S^{-1}V\\right)\\right]\\,,}\\\\ &{=\\exp\\left[-\\frac{1}{2}\\left(\\psi_{\\ell}^{\\top}\\mathbf{W}_{\\ell}^{\\top}\\mathbf{\\Lambda}_{\\ell}\\mathbf{W}_{\\ell}\\psi_{\\ell}-\\left(\\bar{B}_{t,\\ell-1}+\\Lambda_{\\ell}\\mathbf{W}_{\\ell}\\psi_{\\ell}\\right)^{\\top}S^{-1}\\left(\\bar{B}_{t,\\ell-1}+\\Lambda_{\\ell}\\mathbf{W}_{\\ell}\\psi_{\\ell}\\right)\\right)\\right]}\\\\ &{\\propto\\exp\\left[-\\frac{1}{2}\\left(\\psi_{\\ell}^{\\top}\\left(\\mathbf{W}_{\\ell}^{\\top}\\mathbf{\\Lambda}_{\\ell}\\mathbf{W}_{\\ell}-\\mathbf{W}_{\\ell}^{\\top}\\mathbf{\\Lambda}_{\\ell}S^{-1}\\Lambda_{\\ell}\\mathbf{W}_{\\ell}\\right)\\psi_{\\ell}-2\\psi_{\\ell}^{\\top}\\mathbf{W}_{\\ell}^{\\top}\\mathbf{\\Lambda}_{\\ell}S^{-1}\\bar{B}_{t,\\ell-1}\\right)\\right]\\,,}\\\\ &{=\\exp\\left[-\\frac{1}{2}\\psi_{\\ell}^{\\top}\\bar{G}_{t,\\ell}\\psi_{\\ell}+\\psi_{\\ell}^{\\top}\\bar{B}_{t,\\ell}\\right]\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "707 In the last step, we omit constants in $\\psi_{\\ell}$ and we set ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{G}_{t,\\ell}=\\mathrm{W}_{\\ell}^{\\top}\\,\\left(\\Lambda_{\\ell}-\\Lambda_{\\ell}S^{-1}\\Lambda_{\\ell}\\right)\\mathrm{W}_{\\ell}=\\mathrm{W}_{\\ell}^{\\top}\\,\\left(\\Lambda_{\\ell}-\\Lambda_{\\ell}\\big(\\Lambda_{\\ell}+\\bar{G}_{t,\\ell-1}\\big)^{-1}\\Sigma_{\\ell}^{-1}\\Lambda_{\\ell}\\right)\\mathrm{W}_{\\ell}\\,,}\\\\ &{\\bar{B}_{t,\\ell}=\\mathrm{W}_{\\ell}^{\\top}\\Lambda_{\\ell}S^{-1}\\bar{B}_{t,\\ell-1}=\\mathrm{W}_{\\ell}^{\\top}\\Lambda_{\\ell}\\big(\\Lambda_{\\ell}+\\bar{G}_{t,\\ell-1}\\big)^{-1}\\bar{B}_{t,\\ell-1}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "708 This completes the proof. ", "page_idx": 20}, {"type": "text", "text": "709 Similarly, this same proof applies when the reward distribution is not linear-Gaussian, with the   \n710 approximation $\\mathbb{P}\\left(H_{t,i}\\,|\\,\\theta_{*,i}=\\theta\\right)\\approx\\mathcal{N}\\big(\\theta;\\hat{B}_{t,i},\\hat{G}_{t,i}^{-1}\\big)$ . Using this approximation in the derivations   \n711 above leads to the same results. ", "page_idx": 20}, {"type": "text", "text": "712 C Posterior derivations for non-linear diffusion models ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "713 After deriving the posteriors for linear score functions $f_{\\ell}$ , we now get back to the general case in (1),   \n714 where the score functions are potentially non-linear. Approximation is needed since both the score   \n715 functions and rewards can be non-linear. To avoid any computational challenges, we use a simple   \n716 and intuitive approximation, where all posteriors $P_{t,i}$ and $Q_{t,\\ell}$ are approximated by the Gaussian   \n717 distributions in Appendix B.1, with few changes. First, the terms $\\mathrm{W}_{\\ell}\\psi_{\\ell}$ in (18) are replaced by $f_{\\ell}(\\psi_{\\ell})$ .   \n718 This accounts for the fact that the prior mean is now $f_{\\ell}(\\psi_{\\ell})$ rather than $\\mathrm{W}_{\\ell}\\psi_{\\ell}$ , and this is the main   \n719 difference between the linear diffusion model in (15) and the general, potentially non-linear, diffusion   \n720 model in (1). Second, the matrix multiplications that involve the matrices $\\mathrm{W}_{\\ell}$ in (20) and (21) are   \n721 simply removed. Despite being simple, this approximation is efficient and avoids the computational   \n722 burden of heavy approximate sampling algorithms required for each latent parameter. This is why   \n723 deriving the exact posterior for linear score functions was key beyond enabling theoretical analyses.   \n724 Moreover, this approximation retains some key attributes of exact posteriors. Specifically, in the   \n725 absence of data, it recovers precisely the prior in (1), and as more data is accumulated, the influence   \n726 of the prior diminishes. ", "page_idx": 20}, {"type": "text", "text": "727 D Regret proof and additional discussions ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "728 D.1 Sketch of the proof ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "729 We start with the following standard lemma upon which we build our analysis [Aouali et al., 2023b]. ", "page_idx": 20}, {"type": "text", "text": "730 Lemma D.1. Assume that $\\mathbb{P}\\left(\\theta_{*,i}=\\theta\\,\\vert\\,H_{t}\\right)=\\mathcal{N}(\\theta;\\tilde{\\mu}_{t,i},\\check{\\Sigma}_{t,i})$ for any $i\\in[K]$ , then for any $\\delta\\in$   \n731 $(0,1)$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{B}\\mathcal{R}(n)\\leq\\sqrt{2n\\log(1/\\delta)}\\sqrt{\\mathbb{E}\\left[\\sum_{t=1}^{n}\\|X_{t}\\|_{\\bar{\\Sigma}_{t,A_{t}}}^{2}\\right]}+c n\\delta\\,,\\qquad w h e r e\\,\\,c>0\\,\\,i s\\,a\\,c o n s t a n t\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "732 Applying Lemma D.1 requires proving that the marginal action-posteriors $\\mathbb{P}\\left(\\theta_{*,i}=\\theta\\,|\\,H_{t}\\right)$ in Eq. (3)   \n733 are Gaussian and computing their covariances, while we only know the conditional action-posteriors   \n734 $P_{t,i}$ and latent-posteriors $Q_{t,\\ell}$ . This is achieved by leveraging the preservation properties of the   \n735 family of Gaussian distributions [Koller and Friedman, 2009] and the total covariance decomposition   \n736 [Weiss, 2005] which leads to the next lemma. ", "page_idx": 20}, {"type": "text", "text": "737 Lemma D.2. Let $t\\in[n]$ and $i\\in[K],$ , then the marginal covariance matrix $\\check{\\Sigma}_{t,i}$ reads ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\check{\\Sigma}_{t,i}=\\hat{\\Sigma}_{t,i}+\\sum_{\\ell\\in[L]}\\mathrm{P}_{i,\\ell}\\check{\\Sigma}_{t,\\ell}\\mathrm{P}_{i,\\ell}^{\\top}\\,,\\quad w h e r e~\\mathrm{P}_{i,\\ell}=\\hat{\\Sigma}_{t,i}\\Sigma_{1}^{-1}\\mathrm{W}_{1}\\prod_{k=1}^{\\ell-1}\\bar{\\Sigma}_{t,k}\\Sigma_{k+1}^{-1}\\mathrm{W}_{k+1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "738 The marginal covariance matrix $\\check{\\Sigma}_{t,i}$ in Eq. (29) decomposes into $L+1$ terms. The first term   \n739 corresponds to the posterior uncertainty of $\\theta_{*,i}\\mid\\psi_{*,1}$ . The remaining $L$ terms capture the posterior   \n740 uncertainties of $\\psi_{*,L}$ and $\\psi_{*,\\ell-1}\\mid\\psi_{*,\\ell}$ for $\\ell\\in[L]/\\{1\\}$ . These are then used to quantify the posterior   \n741 information gain of latent parameters after one round as follows. ", "page_idx": 21}, {"type": "text", "text": "742 Lemma D.3 (Posterior information gain). Let $t\\in[n]$ and $\\ell\\in[L]$ , then ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bar{\\Sigma}_{t+1,\\ell}^{-1}-\\bar{\\Sigma}_{t,\\ell}^{-1}\\succeq\\sigma^{-2}\\sigma_{\\mathrm{MAX}}^{-2\\ell}\\mathrm{P}_{A_{t},\\ell}^{\\top}X_{t}X_{t}^{\\top}\\mathrm{P}_{A_{t},\\ell}\\,,\\qquad w h e r e~\\sigma_{\\mathrm{MAX}}^{2}=\\operatorname*{max}_{\\ell\\in[L+1]}1+\\frac{\\sigma_{\\ell}^{2}}{\\sigma^{2}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "743 Finally, Lemma D.2 is used to decompose $\\|X_{t}\\|_{\\check{\\Sigma}_{t,A_{t}}}^{2}$ in Eq. (28) into $L+1$ terms. Each term is   \n744 bounded thanks to Lemma D.3. This results in the Bayes regret bound in Theorem 4.1. ", "page_idx": 21}, {"type": "text", "text": "745 D.2 Technical contributions ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "746 Our main technical contributions are the following. ", "page_idx": 21}, {"type": "text", "text": "747 Lemma D.2. In dTS, sampling is done hierarchically, meaning the marginal posterior distribution of   \n748 $\\theta_{*,i}|H_{t}$ is not explicitly defined. Instead, we use the conditional posterior distribution of $\\theta_{*,i}|H_{t},\\psi_{*,1}$ .   \n749 9 The first contribution was deriving $\\theta_{*,i}|H_{t}$ using the total covariance decomposition combined with   \n750 an induction proof, as our posteriors in Section 3.1 were derived recursively. Unlike in Bayes   \n751 regret analysis for standard Thompson sampling, where the posterior distribution of $\\theta_{*,i}|H_{t}$ is   \n752 predetermined due to the absence of latent parameters, our method necessitates this recursive total   \n753 covariance decomposition, marking a first difference from the standard Bayesian proofs of Thompson   \n754 sampling. Note that HierTS, which is developed for multi-task linear bandits, also employs total   \n755 covariance decomposition, but it does so under the assumption of a single latent parameter; on which   \n756 action parameters are centered. Our extension significantly differs as it is tailored for contextual   \n757 bandits with multiple, successive levels of latent parameters, moving away from HierTS\u2019s assumption   \n758 of a 1-level structure. Roughly speaking, HierTS when applied to contextual would consider a single  \n759 level hierarchy, where $\\theta_{*,i}|\\psi_{*,1}\\sim\\mathcal{N}(\\psi_{*,1},\\Sigma_{1})$ with $L\\,=\\,1$ . In contrast, our model proposes a   \n760 multi-level hierarchy, where the first level is $\\theta_{*,i}|\\psi_{*,1}\\sim\\mathcal{N}(W_{1}\\psi_{*,1},\\Sigma_{1})$ . This also introduces a new   \n761 aspect to our approach \u2013 the use of a linear function $W_{1}\\psi_{*,1}$ , as opposed to HierTS\u2019s assumption   \n762 where action parameters are centered directly on the latent parameter. Thus, while HierTS also   \n763 uses the total covariance decomposition, our generalize it to multi-level hierarchies under $L$ linear   \n764 functions $W_{\\ell}\\psi_{*,\\ell}$ , instead of a single-level hierarchy under a single identity function $\\psi_{*,1}$ .   \n765 Lemma D.3. In Bayes regret proofs for standard Thompson sampling, we often quantify the posterior   \n766 information gain. This is achieved by monitoring the increase in posterior precision for the action   \n767 taken $A_{t}$ in each round $t\\in[n]$ . However, in dTS, our analysis extends beyond this. We not only   \n768 quantify the posterior information gain for the taken action but also for every latent parameter, since   \n769 they are also learned. This lemma addresses this aspect. To elaborate, we use the recursive formulas   \n770 in Section 3.1 that connect the posterior covariance of each latent parameter $\\psi_{*,\\ell}$ with the covariance   \n771 of the posterior action parameters $\\theta_{*,i}$ . This allows us to propagate the information gain associated   \n772 with the action taken in round $A_{t}$ to all latent parameters $\\psi_{*,\\ell}$ , for $\\ell\\in[L]$ by induction. This is a   \n773 novel contribution, as it is not a feature of Bayes regret analyses in standard Thompson sampling.   \n774 Proposition 4.2. Building upon the insights of Theorem 4.1, we introduce the sparsity assumption   \n775 (A3). Under this assumption, we demonstrate that the Bayes regret outlined in Theorem 4.1 can be   \n776 significantly refined. Specifically, the regret becomes contingent on dimensions $d\\varrho\\leq d$ , as opposed   \n777 to relying on the entire dimension $d$ . This sparsity assumption is both a novel and a key technical   \n778 contribution to our work. Its underlying principle is straightforward: the Bayes regret is influenced   \n779 by the quantity of parameters that require learning. With the sparsity assumption, this number is   \n780 reduced to less than $d$ for each latent parameter. To substantiate this claim, we revisit the proof of   \n781 Theorem 4.1 and modify a crucial equality. This adjustment results in a more precise representation by   \n782 partitioning the covariance matrix of each latent parameter $\\psi_{*,\\ell}$ into blocks. These blocks comprise   \n783 a $d_{\\ell}\\times d_{\\ell}$ segment corresponding to the learnable $d_{\\ell}$ parameters of $\\psi_{*,\\ell}$ , and another block of size   \n784 $(d-d\\ell)\\times(d-d\\ell)$ that does not necessitate learning. This decomposition allows us to conclude that   \n785 the final regret is solely dependent on $d_{\\ell}$ , marking a significant refinement from the original theorem. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "786 D.3 Proof of lemma D.2 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "787 In this proof, we heavily rely on the total covariance decomposition [Weiss, 2005]. Also, refer to   \n788 [Hong et al., 2022b, Section 5.2] for a brief introduction to this decomposition. Now, from Eq. (17),   \n789 we have that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{cov}\\left[\\theta_{*,i}\\,|\\,H_{t},\\psi_{*,1}\\right]=\\hat{\\Sigma}_{t,i}=\\left(\\hat{G}_{t,i}+\\Sigma_{1}^{-1}\\right)^{-1}\\,,}\\\\ &{\\quad\\mathbb{E}\\left[\\theta_{*,i}\\,|\\,H_{t},\\psi_{*,1}\\right]=\\hat{\\mu}_{t,i}=\\hat{\\Sigma}_{t,i}\\left(\\hat{G}_{t,i}\\hat{B}_{t,i}+\\Sigma_{1}^{-1}\\mathrm{W}_{1}\\psi_{*,1}\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "790 First, given $H_{t}$ , cov $\\left[\\theta_{*,i}\\,|\\,H_{t},\\psi_{*,1}\\right]=\\left(\\hat{G}_{t,i}+\\Sigma_{1}^{-1}\\right)^{-1}$ is constant. Thus ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\operatorname{cov}\\left[\\theta_{*,i}\\,\\big|\\,H_{t},\\psi_{*,1}\\right]\\big|\\,H_{t}\\right]=\\operatorname{cov}\\left[\\theta_{*,i}\\,\\big|\\,H_{t},\\psi_{*,1}\\right]=\\left(\\hat{G}_{t,i}+\\Sigma_{1}^{-1}\\right)^{-1}=\\hat{\\Sigma}_{t,i}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "791 In addition, given $H_{t},\\,\\hat{\\Sigma}_{t,i},\\hat{G}_{t,i}$ and $\\hat{B}_{t,i}$ are constant. Thus ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{cov}\\left[\\mathbb E\\left[\\theta_{*,i}\\left|\\,H_{t},\\psi_{*,1}\\right.\\right]\\big|H_{t}\\right]=\\mathrm{cov}\\left[\\hat{\\Sigma}_{t,i}\\left(\\hat{G}_{t,i}\\hat{B}_{t,i}+\\Sigma_{1}^{-1}\\mathrm{W}_{1}\\psi_{*,1}\\right)\\Big|\\,H_{t}\\right]\\,,}\\\\ &{\\hphantom{\\mathrm{cov}\\left[\\hat{\\Sigma}_{t,i}\\sum_{1}^{-1}\\mathrm{W}_{1}\\psi_{*,1}\\right.}\\,}\\\\ &{\\hphantom{\\mathrm{cov}\\left[\\hat{\\Sigma}_{t,i}\\Sigma_{1}^{-1}\\mathrm{W}_{1}\\psi_{*,1}\\right.}H_{t}\\Big]\\,,}\\\\ &{\\hphantom{\\mathrm{cov}\\left[\\hat{\\Sigma}_{t,i}\\Sigma_{1}^{-1}\\mathrm{W}_{1}\\mathrm{cov}\\left[\\psi_{*,1}\\left|\\,H_{t}\\right]\\mathrm{W}_{1}^{\\top}\\Sigma_{1}^{-1}\\hat{\\Sigma}_{t,i}\\right.}}\\\\ &{\\hphantom{\\mathrm{cov}\\left[\\hat{\\Sigma}_{t,i}\\Sigma_{1}^{-1}\\mathrm{W}_{1}\\bar{\\Sigma}_{t,1}\\mathrm{W}_{1}^{\\top}\\Sigma_{1}^{-1}\\hat{\\Sigma}_{t,i}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "792 where $\\bar{\\bar{\\Sigma}}_{t,1}=\\operatorname{cov}\\left[\\psi_{*,1}\\,\\vert\\,H_{t}\\right]$ is the marginal posterior covariance of $\\psi_{*,1}$ . Finally, the total covariance   \n793 decomposition [Weiss, 2005, Hong et al., 2022b] yields that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\check{\\boldsymbol{\\Sigma}}_{t,i}=\\operatorname{cov}\\left[\\theta_{*,i}\\left|\\right.\\boldsymbol{H}_{t}\\right]=\\boldsymbol{\\mathbb{E}}\\left[\\operatorname{cov}\\left[\\theta_{*,i}\\left|\\right.\\boldsymbol{H}_{t},\\psi_{*,1}\\right|\\left|\\right.\\boldsymbol{H}_{t}\\right]+\\operatorname{cov}\\left[\\mathbb{E}\\left[\\theta_{*,i}\\left|\\right.\\boldsymbol{H}_{t},\\psi_{*,1}\\right|\\left|\\right.\\boldsymbol{H}_{t}\\right]\\,,}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\hat{\\boldsymbol{\\Sigma}}_{t,i}+\\hat{\\boldsymbol{\\Sigma}}_{t,i}\\boldsymbol{\\Sigma}_{1}^{-1}\\boldsymbol{\\mathrm{W}}_{1}\\bar{\\boldsymbol{\\Sigma}}_{t,1}\\boldsymbol{\\mathrm{W}}_{1}^{\\top}\\boldsymbol{\\Sigma}_{1}^{-1}\\hat{\\boldsymbol{\\Sigma}}_{t,i}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "794 However, $\\bar{\\bar{\\Sigma}}_{t,1}=\\operatorname{cov}\\left[\\psi_{*,1}\\,\\vert\\,H_{t}\\right]$ is different from $\\bar{\\Sigma}_{t,1\\_}=\\mathrm{cov}\\left[\\psi_{*,1}\\right.\\vert\\left.H_{t},\\psi_{*,2}\\right]$ that we already derived   \n795 in Eq. (18). Thus we do not know the expression of $\\bar{\\bar{\\Sigma}}_{t,1}$ . But we can use the same total covariance   \n796 decomposition trick to find it. Precisely, let $\\bar{\\bar{\\Sigma}}_{t,\\ell}=\\mathrm{cov}\\left[\\psi_{*,\\ell}\\mid H_{t}\\right]$ for any $\\ell\\in[L]$ . Then we have that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{\\Sigma}_{t,1}=\\mathrm{cov}\\left[\\psi_{*,1}\\,|\\,H_{t},\\psi_{*,2}\\right]=\\left(\\Sigma_{2}^{-1}+\\bar{G}_{t,1}\\right)^{-1},}\\\\ &{\\quad\\bar{\\mu}_{t,1}=\\mathbb{E}\\left[\\psi_{*,1}\\,|\\,H_{t},\\psi_{*,2}\\right]=\\bar{\\Sigma}_{t,1}\\!\\left(\\Sigma_{2}^{-1}\\mathrm{W}_{2}\\psi_{*,2}+\\bar{B}_{t,1}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "797 First, given $H_{t},\\,\\mathrm{cov}\\left[\\psi_{\\ast,1}\\,|\\,H_{t},\\psi_{\\ast,2}\\right]=\\left(\\Sigma_{2}^{-1}+{\\bar{G}}_{t,1}\\right)^{-1}$ is constant. Thus ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\mathrm{cov}\\left[\\psi_{*,1}\\,|\\,H_{t},\\psi_{*,2}\\right]|\\,H_{t}\\right]=\\mathrm{cov}\\left[\\psi_{*,1}\\,|\\,H_{t},\\psi_{*,2}\\right]=\\bar{\\Sigma}_{t,1}\\,.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "798 In addition, given $H_{t},\\bar{\\Sigma}_{t,1},\\tilde{\\Sigma}_{t,1}$ and ${\\bar{B}}_{t,1}$ are constant. Thus ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{cov}\\left[\\mathbb{E}\\left[\\psi_{*,1}\\,|\\,H_{t},\\psi_{*,2}\\right]\\,|\\,H_{t}\\right]=\\mathrm{cov}\\left[\\bar{\\Sigma}_{t,1}\\!\\left(\\Sigma_{2}^{-1}\\mathrm{W}_{2}\\psi_{*,2}+\\bar{B}_{t,1}\\right)\\Big|\\,H_{t}\\right]\\,,}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\mathrm{cov}\\left[\\bar{\\Sigma}_{t,1}\\Sigma_{2}^{-1}\\mathrm{W}_{2}\\psi_{*,2}\\,\\big|\\,H_{t}\\right]\\,,}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\bar{\\Sigma}_{t,1}\\!\\!\\sum_{2}^{-1}\\!\\!\\mathrm{W}_{2}\\mathrm{cov}\\left[\\psi_{*,2}\\,|\\,H_{t}\\right]\\mathrm{W}_{2}^{\\top}\\Sigma_{2}^{-1}\\bar{\\Sigma}_{t,1}\\,,}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\bar{\\Sigma}_{t,1}\\!\\Sigma_{2}^{-1}\\mathrm{W}_{2}\\bar{\\Sigma}_{t,2}\\mathrm{W}_{2}^{\\top}\\Sigma_{2}^{-1}\\bar{\\Sigma}_{t,1}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "799 Finally, total covariance decomposition [Weiss, 2005, Hong et al., 2022b] leads to ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{\\Sigma}_{t,1}=\\mathrm{cov}\\left[\\psi_{*,1}\\,|\\,H_{t}\\right]=\\mathbb{E}\\left[\\mathrm{cov}\\left[\\psi_{*,1}\\,|\\,H_{t},\\psi_{*,2}\\right]\\,|\\,H_{t}\\right]+\\mathrm{cov}\\left[\\mathbb{E}\\left[\\psi_{*,1}\\,|\\,H_{t},\\psi_{*,2}\\right]\\,|\\,H_{t}\\right]\\,,}\\\\ &{\\qquad\\qquad\\qquad=\\bar{\\Sigma}_{t,1}+\\bar{\\Sigma}_{t,1}\\Sigma_{2}^{-1}\\mathrm{W}_{2}\\bar{\\Sigma}_{t,2}\\mathrm{W}_{2}^{\\top}\\Sigma_{2}^{-1}\\bar{\\Sigma}_{t,1}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "800 Now using the techniques, this can be generalized using the same technique as above to ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bar{\\Sigma}_{t,\\ell}=\\bar{\\Sigma}_{t,\\ell}+\\bar{\\Sigma}_{t,\\ell}\\Sigma_{\\ell+1}^{-1}\\mathrm{W}_{\\ell+1}\\bar{\\Sigma}_{t,\\ell+1}\\mathrm{W}_{\\ell+1}^{\\top}\\Sigma_{\\ell+1}^{-1}\\bar{\\Sigma}_{t,\\ell}\\,,\\qquad\\qquad\\forall\\ell\\in[L-1]\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "801 Then, by induction, we get that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\bar{\\bar{\\Sigma}}_{t,1}=\\sum_{\\ell\\in[L]}\\bar{\\mathrm P}_{\\ell}\\bar{\\Sigma}_{t,\\ell}\\bar{\\mathrm P}_{\\ell}^{\\top}\\,,\\qquad\\qquad\\qquad\\qquad\\forall\\ell\\in[L-1]\\,,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "802 where we use that by definition $\\bar{\\bar{\\Sigma}}_{t,L}\\;=\\;\\mathrm{cov}\\left[\\psi_{*,L}\\;\\vert\\;H_{t}\\right]\\;=\\;\\bar{\\Sigma}_{t,L}$ and set $\\bar{\\mathrm{P}}_{1}\\ =\\ I_{d}$ and $\\bar{\\mathrm{~P~}}_{\\ell}\\;=\\;$   \n803 $\\begin{array}{r}{\\prod_{k=1}^{\\ell-1}\\bar{\\Sigma}_{t,k}\\Sigma_{k+1}^{-1}\\mathrm{W}_{k+1}}\\end{array}$ for any $\\ell\\in[L]/\\{1\\}$ . Plugging this in Eq. (31) leads to ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\check{\\Sigma}_{t,i}=\\hat{\\Sigma}_{t,i}+\\displaystyle\\sum_{\\ell\\in[L]}\\hat{\\Sigma}_{t,i}\\Sigma_{1}^{-1}\\mathrm{W}_{1}\\bar{\\ P}_{\\ell}\\bar{\\Sigma}_{t,\\ell}\\bar{\\ P}_{\\ell}^{\\top}\\mathrm{W}_{1}^{\\top}\\Sigma_{1}^{-1}\\hat{\\Sigma}_{t,i}\\,,}\\\\ &{\\qquad=\\hat{\\Sigma}_{t,i}+\\displaystyle\\sum_{\\ell\\in[L]}\\hat{\\Sigma}_{t,i}\\Sigma_{1}^{-1}\\mathrm{W}_{1}\\bar{\\ P}_{\\ell}\\bar{\\Sigma}_{t,\\ell}\\big(\\hat{\\Sigma}_{t,i}\\Sigma_{1}^{-1}\\mathrm{W}_{1}\\big)^{\\top}\\,,}\\\\ &{\\qquad=\\hat{\\Sigma}_{t,i}+\\displaystyle\\sum_{\\ell\\in[L]}\\mathrm{P}_{i,\\ell}\\bar{\\Sigma}_{t,\\ell}\\mathrm{P}_{i,\\ell}^{\\top}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$\\begin{array}{r}{\\mathrm{P}_{i,\\ell}=\\hat{\\Sigma}_{t,i}\\Sigma_{1}^{-1}\\mathrm{W}_{1}\\bar{\\mathrm{P}}_{\\ell}=\\hat{\\Sigma}_{t,i}\\Sigma_{1}^{-1}\\mathrm{W}_{1}\\prod_{k=1}^{\\ell-1}\\bar{\\Sigma}_{t,k}\\Sigma_{k+1}^{-1}\\mathrm{W}_{k+1}.}\\end{array}$ ", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "805 D.4 Proof of lemma D.3 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "806 We prove this result by induction. We start with the base case when $\\ell=1$ . ", "page_idx": 23}, {"type": "text", "text": "807 (I) Base case. Let u = \u03c3\u22121 \u03a3\u02c6t2,A $u=\\sigma^{-1}\\hat{\\Sigma}_{t,A_{t}}^{\\frac{1}{2}}X_{t}$ From the expression of $\\bar{\\Sigma}_{t,1}$ in Eq. (18), we have that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{\\Sigma}_{t+1,1}^{-1}-\\bar{\\Sigma}_{t,1}^{-1}=\\mathbb{W}_{1}^{\\top}\\left(\\Sigma_{1}^{-1}-\\Sigma_{1}^{-1}(\\hat{\\Sigma}_{t,A_{t}}^{-1}+\\sigma^{-2}X_{t}X_{t}^{\\top})^{-1}\\Sigma_{1}^{-1}-\\left(\\Sigma_{1}^{-1}-\\Sigma_{1}^{-1}\\hat{\\Sigma}_{t,A_{t}}\\Sigma_{1}^{-1}\\right)\\right)\\mathbb{W}_{1}\\,,}\\\\ &{\\phantom{\\sum}=\\mathbb{W}_{1}^{\\top}\\left(\\Sigma_{1}^{-1}(\\hat{\\Sigma}_{t,A_{t}}-(\\hat{\\Sigma}_{t,A_{t}}^{-1}+\\sigma^{-2}X_{t}X_{t}^{\\top})^{-1})\\Sigma_{1}^{-1}\\right)\\mathbb{W}_{1}\\,,}\\\\ &{\\phantom{\\sum}=\\mathbb{W}_{1}^{\\top}\\left(\\Sigma_{1}^{-1}\\hat{\\Sigma}_{t,A_{t}}^{\\frac{1}{2}}(I_{d}-(I_{d}+\\sigma^{-2}\\hat{\\Sigma}_{t,A_{t}}^{\\frac{1}{2}}X_{t}X_{t}^{\\top}\\hat{\\Sigma}_{t,A_{t}}^{\\frac{1}{2}})^{-1})\\hat{\\Sigma}_{t,A_{t}}^{\\frac{1}{2}}\\Sigma_{1}^{-1}\\right)\\mathbb{W}_{1}\\,,}\\\\ &{\\phantom{\\sum}=\\mathbb{W}_{1}^{\\top}\\left(\\Sigma_{1}^{-1}\\hat{\\Sigma}_{t,A_{t}}^{\\frac{1}{2}}(I_{d}-(I_{d}+u\\bar{u}^{\\top})^{-1})\\hat{\\Sigma}_{t,A_{t}}^{\\frac{1}{2}}\\Sigma_{1}^{-1}\\right)\\mathbb{W}_{1}\\,,}\\\\ &{\\phantom{\\sum}\\frac{(i)}{\\alpha}\\mathbb{W}_{1}^{\\top}\\left(\\Sigma_{1}^{-1}\\hat{\\Sigma}_{t,A_{t}}^{\\frac{1}{2}}\\frac{u\\alpha^{\\top}}{1+u^{\\top}u}\\hat{\\Sigma}_{t,A_{t}}^{\\frac{1}{2}}\\Sigma_{1}^{-1}\\\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "808 In $(i)$ we use the Sherman-Morrison formula. Note that $(i i)$ says that $\\bar{\\Sigma}_{t+1,1}^{-1}-\\bar{\\Sigma}_{t,1}^{-1}$ is one-rank   \n809 which we will also need in induction step. Now, we have that $\\|X_{t}\\|^{2}=1$ . Therefore, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{1+u^{\\top}u=1+\\sigma^{-2}X_{t}^{\\top}\\hat{\\Sigma}_{t,A_{t}}X_{t}\\leq1+\\sigma^{-2}\\lambda_{1}(\\Sigma_{1})\\|X_{t}\\|^{2}=1+\\sigma^{-2}\\sigma_{1}^{2}\\leq\\sigma_{\\mathrm{MAX}}^{2}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "810 where we use that by definition of $\\sigma_{\\mathrm{MAX}}^{2}$ in Lem\u22122ma D.3, we have that $\\sigma_{\\mathrm{MAX}}^{2}\\geq1+\\sigma^{-2}\\sigma_{1}^{2}$ . Therefore,   \n811 by taking the inverse, we get that1+u1\u22a4u \u2265 $\\begin{array}{r}{\\frac{\\mathrm{1}}{1+u^{\\top}u}\\geq\\sigma_{\\mathrm{MAX}}^{-2}}\\end{array}$ . Combining this with Eq. (32) leads to ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bar{\\Sigma}_{t+1,1}^{-1}-\\bar{\\Sigma}_{t,1}^{-1}\\succeq\\sigma^{-2}\\sigma_{\\mathrm{MAX}}^{-2}\\mathrm{W}_{1}^{\\top}\\Sigma_{1}^{-1}\\hat{\\Sigma}_{t,A_{t}}X_{t}X_{t}^{\\top}\\hat{\\Sigma}_{t,A_{t}}\\Sigma_{1}^{-1}\\mathrm{W}_{1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "812 Noticing that $\\mathrm{P}_{A_{t},1}=\\hat{\\Sigma}_{t,A_{t}}\\Sigma_{1}^{-1}\\mathrm{W}_{1}$ concludes the proof of the base case when $\\ell=1$ . ", "page_idx": 23}, {"type": "text", "text": "813 $(\\mathbf{II})$ Induction step. Let $\\ell\\in[L]/\\{1\\}$ and suppose that $\\bar{\\Sigma}_{t+1,\\ell-1}^{-1}-\\bar{\\Sigma}_{t,\\ell-1}^{-1}$ is one-rank and that it   \n814 holds for $\\ell-1$ that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\bar{\\Sigma}_{t+1,\\ell-1}^{-1}-\\bar{\\Sigma}_{t,\\ell-1}^{-1}\\succeq\\sigma^{-2}\\sigma_{\\mathrm{MAX}}^{-2(\\ell-1)}\\mathbf{P}_{A_{t},\\ell-1}^{\\top}X_{t}X_{t}^{\\top}\\mathbf{P}_{A_{t},\\ell-1}\\,,\\quad\\mathrm{where}~\\,\\sigma_{\\mathrm{MAX}}^{-2}=\\operatorname*{max}_{\\ell\\in[L]}1+\\sigma^{-2}\\sigma_{\\ell}^{2}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "815 Then, we want to show that $\\bar{\\Sigma}_{t+1,\\ell}^{-1}-\\bar{\\Sigma}_{t,\\ell}^{-1}$ is also one-rank and that it holds that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\bar{\\Sigma}_{t+1,\\ell}^{-1}-\\bar{\\Sigma}_{t,\\ell}^{-1}\\succeq\\sigma^{-2}\\sigma_{\\mathrm{MAX}}^{-2\\ell}\\mathrm{P}_{A_{t},\\ell}^{\\top}X_{t}X_{t}^{\\top}\\mathrm{P}_{A_{t},\\ell}\\,,\\qquad\\mathrm{~where~}\\sigma_{\\mathrm{MAX}}^{-2}=\\operatorname*{max}_{\\ell\\in[L]}1+\\sigma^{-2}\\sigma_{\\ell}^{2}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "816 This is achieved as follows. First, we notice that by the induction hypothesis, we have that $\\tilde{\\Sigma}_{t+1,\\ell-1}^{-1}-$ 881178 $\\bar{G}_{t,\\ell-1}=\\bar{\\Sigma}_{t+1,\\ell-1}^{-1}-\\bar{\\Sigma}_{t,\\ell-1}^{-1}$ Iwnh aerded .e  Tmhaetnr,i xs iism iploarslityi vtoe  tsheem bi-adsee fcinaistee,.  wTeh uhsa vwee $\\tilde{\\Sigma}_{t+1,\\ell-1}^{-1}-\\bar{G}_{t,\\ell-1}=u u^{\\top}$ $u\\in\\mathbb{R}^{d}$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\sum_{t=1,t}^{n}-\\sum_{t=1,t}^{2}\\sum_{t=1,t}^{-1}-\\sum_{t=1}^{2}t\\sum_{t=1}^{1},}&{}\\\\ {=\\mathbf{W}_{t}^{\\top}\\Big(\\sum_{t=t}^{1}+\\sum_{t=1,t=1}^{1}\\Big)^{-1}\\mathbf{W}_{t}-\\mathbf{W}_{t}^{\\top}\\big(\\sum_{t=1}^{t}+\\sum_{t,t=1}^{1}\\big)^{-1}\\mathbf{W}_{t},}\\\\ &{=\\mathbf{W}_{t}^{\\top}\\Big[(\\sum_{t=1}^{t}+\\sum_{t=1,t=1}^{1})^{-1}-\\big(\\sum_{t^{\\prime}}+\\sum_{t=1,t=1}^{1}\\big)^{-1}\\Big]\\mathbf{W}_{t},}\\\\ &{=\\mathbf{W}_{t}^{\\top}\\sum_{t=1}^{\\infty}\\Big[\\big(\\sum_{t^{\\prime}=1}^{1}+\\widehat G_{t,t-1}\\big)^{-1}-\\big(\\sum_{t^{\\prime}=1}^{1}+\\widehat\\widetilde{\\Sigma}_{t+1,t-1}^{-1}\\big)^{-1}\\Big]\\mathbf{E}_{t}^{-1}\\mathbf{W}_{t},}\\\\ &{=\\mathbf{W}_{t}^{\\top}\\Sigma_{t}^{-1}\\Big[\\big(\\sum_{t^{\\prime}=1}^{1}+\\widehat G_{t,t-1}\\big)^{-1}-\\big(\\sum_{t^{\\prime}=1}^{1}+\\widehat G_{t,t-1}+\\sum_{t^{\\prime}=1,t-1}^{1}-\\widehat G_{t,t-1}\\big)^{-1}\\Big]\\mathbf{S}_{t}^{-1}\\mathbf{W}_{t},}\\\\ &{=\\mathbf{W}_{t}^{\\top}\\Sigma_{t^{\\prime}}^{-1}\\Big[\\big(\\Sigma_{t}^{1}+\\widehat G_{t,t-1}\\big)^{-1}-\\big(\\Sigma_{t}^{-1}+\\widehat G_{t,t-1}+\\operatorname*{m}^{\\top}\\big)^{-1}\\Big]\\mathbf{S}_{t}^{-1}\\mathbf{W}_{t},}\\\\ &{=\\mathbf{W}_{t}^{\\top}\\Sigma_{t^{\\prime}}^{-1}\\Big[\\Sigma_{t,t-1}^{1}-\\big(\\sum_{t=1}^{\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "881290 .t hTehsiesr etfhoart $u u^{\\top}=\\tilde{\\Sigma}_{t+1,\\ell-1}^{-1}-\\bar{G}_{t,\\ell-1}=\\bar{\\Sigma}_{t+1,\\ell-1}^{-1}-$ $\\bar{\\Sigma}_{t,\\ell-1}^{-1}\\succeq\\sigma^{-2}\\sigma_{\\mathrm{MAX}}^{-2(\\ell-1)}\\mathrm{P}_{A_{t},\\ell-1}^{\\top}X_{t}X_{t}^{\\top}\\mathrm{P}_{A_{t},\\ell-1}$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{\\Sigma}_{t+1,\\ell}^{-1}-\\bar{\\Sigma}_{t,\\ell}^{-1}=\\mathrm{W}_{\\ell}^{\\top}\\,\\Sigma_{\\ell}^{-1}\\bar{\\Sigma}_{t,\\ell-1}\\frac{u u^{\\top}}{1+u^{\\top}\\Sigma_{t,\\ell-1}u}\\bar{\\Sigma}_{t,\\ell-1}\\Sigma_{\\ell}^{-1}\\mathrm{W}_{\\ell}\\,,}\\\\ &{\\phantom{\\bar{\\Sigma}_{t+1,\\ell}}\\,\\succeq\\mathrm{W}_{\\ell}^{\\top}\\,\\Sigma_{\\ell}^{-1}\\bar{\\Sigma}_{t,\\ell-1}\\frac{\\sigma^{-2}\\sigma_{\\mathrm{MX}}^{-2}\\,\\Gamma_{1}^{\\top}}{1+u^{\\top}\\Sigma_{t,\\ell-1}u}\\mathrm{P}_{\\ell}^{\\top}\\Sigma_{t,\\ell-1}\\bar{\\Sigma}_{t,\\ell-1}\\bar{\\Sigma}_{\\ell}^{-1}\\mathrm{W}_{\\ell}\\,,}\\\\ &{\\phantom{\\bar{\\Sigma}_{t+1,\\ell}}=\\frac{\\sigma^{-2}\\sigma_{\\mathrm{MX}}^{-2}}{1+u^{\\top}\\Sigma_{t,\\ell-1}u}\\mathrm{W}_{\\ell}^{\\top}\\Sigma_{\\ell}^{-1}\\bar{\\Sigma}_{t,\\ell-1}\\mathrm{P}_{A_{t},\\ell-1}^{\\top}X_{t}X_{t}^{\\top}\\mathrm{P}_{A_{t},\\ell-1}\\bar{\\Sigma}_{t,\\ell-1}\\Sigma_{\\ell}^{-1}\\mathrm{W}_{\\ell}\\,,}\\\\ &{\\phantom{\\bar{\\Sigma}_{t+1,\\ell}}=\\frac{\\sigma^{-2}\\sigma_{\\mathrm{MX}}^{-2}}{1+\\,u^{\\top}\\Sigma_{t,\\ell-1}u}\\mathrm{P}_{\\ell}^{\\top}\\Sigma_{\\ell}^{-1}\\bar{\\Sigma}_{t,\\ell}X_{t}^{\\top}\\mathrm{P}_{A_{t},\\ell-1}X_{t}X_{t}^{\\top}\\mathrm{P}_{A_{t},\\ell-1}\\bar{\\Sigma}_{t,\\ell-1}\\Sigma_{\\ell}^{-1}\\mathrm{W}_{\\ell}\\,,}\\\\ &{\\phantom{\\bar{\\Sigma}_{t+1,\\ell}}=\\frac{\\sigma^{-2}\\sigma_{\\mathrm{MX}}^{-2}}{1+\\,u^{\\top}\\Sigma_{t,\\ell-1}u}\\mathrm{P}_ \n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "821 Finally, we use that $1+u^{\\top}\\bar{\\Sigma}_{t,\\ell-1}u\\,\\le\\,1+\\|u\\|_{2}\\lambda_{1}\\bigl(\\bar{\\Sigma}_{t,\\ell-1}\\bigr)\\,\\le\\,1+\\sigma^{-2}\\sigma_{\\ell}^{2}$ . Here we use that   \n822 $\\lVert u\\rVert_{2}\\leq\\sigma^{-2}$ , which can also be proven by induction, and that $\\lambda_{1}(\\bar{\\Sigma}_{t,\\ell-1})\\leq\\sigma_{\\ell}^{2}$ , which follows from   \n823 the expression of $\\bar{\\Sigma}_{t,\\ell-1}$ in Section 3.1. Therefore, we have that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{\\Sigma}_{t+1,\\ell}^{-1}-\\bar{\\Sigma}_{t,\\ell}^{-1}\\succeq\\frac{\\sigma^{-2}\\sigma_{\\mathrm{MAX}}^{-2(\\ell-1)}}{1+u^{\\top}\\bar{\\Sigma}_{t,\\ell-1}u}\\mathrm{P}_{A_{t},\\ell}^{\\top}X_{t}X_{t}^{\\top}\\mathrm{P}_{A_{t},\\ell}\\,,}\\\\ &{\\phantom{\\sum_{t+1,\\ell}}\\succeq\\frac{\\sigma^{-2}\\sigma_{\\mathrm{MAX}}^{-2(\\ell-1)}}{1+\\sigma^{-2}\\sigma_{\\ell}^{2}}\\mathrm{P}_{A_{t},\\ell}^{\\top}X_{t}X_{t}^{\\top}\\mathrm{P}_{A_{t},\\ell}\\,,}\\\\ &{\\phantom{\\sum_{t+1,\\ell}}\\succeq\\sigma^{-2}\\sigma_{\\mathrm{MAX}}^{-2\\ell}\\mathrm{P}_{A_{t},\\ell}^{\\top}X_{t}X_{t}^{\\top}\\mathrm{P}_{A_{t},\\ell}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "824 where the last inequality follows from the definition of $\\sigma_{\\mathrm{MAX}}^{2}=\\operatorname*{max}_{\\ell\\in[L]}1+\\sigma^{-2}\\sigma_{\\ell}^{2}$ . This concludes   \n825 the proof. ", "page_idx": 24}, {"type": "text", "text": "826 D.5 Proof of theorem 4.1 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "827 We start with the following standard result which we borrow from [Hong et al., 2022a, Aouali et al.,   \n828 2023b], ", "page_idx": 24}, {"type": "equation", "text": "$$\nB\\mathcal{R}(n)\\leq\\sqrt{2n\\log(1/\\delta)}\\sqrt{\\mathbb{E}\\left[\\sum_{t=1}^{n}\\|X_{t}\\|_{\\Sigma_{t,A_{t}}}^{2}\\right]}+c n\\delta\\,,\\qquad\\mathrm{where}~c>0\\,\\mathrm{is~a~constant}\\,.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "829 Then we use Lemma D.2 and express the marginal covariance $\\check{\\Sigma}_{t,A_{t}}$ as ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\check{\\Sigma}_{t,i}=\\hat{\\Sigma}_{t,i}+\\sum_{\\ell\\in[L]}\\mathrm{P}_{i,\\ell}\\bar{\\Sigma}_{t,\\ell}\\mathrm{P}_{i,\\ell}^{\\top}\\,,\\qquad\\mathrm{where}~\\mathrm{P}_{i,\\ell}=\\hat{\\Sigma}_{t,i}\\Sigma_{1}^{-1}\\mathrm{W}_{1}\\prod_{k=1}^{\\ell-1}\\bar{\\Sigma}_{t,k}\\Sigma_{k+1}^{-1}\\mathrm{W}_{k+1}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "830 Therefore, we can decompose $\\|X_{t}\\|_{\\tilde{\\Sigma}_{t,A_{t}}}^{2}$ as ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|X_{t}\\|_{\\dot{\\Sigma}_{t,A_{t}}}^{2}=\\sigma^{2}\\frac{X_{t}^{\\top}\\check{\\Sigma}_{t,A_{t}}X_{t}}{\\sigma^{2}}\\overset{(i)}{=}\\sigma^{2}\\big(\\sigma^{-2}X_{t}^{\\top}\\hat{\\Sigma}_{t,A_{t}}X_{t}+\\sigma^{-2}\\displaystyle\\sum_{\\ell\\in[L]}X_{t}^{\\top}\\mathrm{P}_{A_{t},\\ell}\\bar{\\Sigma}_{t,\\ell}\\mathrm{P}_{A_{t},\\ell}^{\\top}X_{t}\\big)\\,,}\\\\ {\\displaystyle\\overset{(i i)}{\\leq}c_{0}\\log(1+\\sigma^{-2}X_{t}^{\\top}\\hat{\\Sigma}_{t,A_{t}}X_{t})+\\displaystyle\\sum_{\\ell\\in[L]}c_{\\ell}\\log(1+\\sigma^{-2}X_{t}^{\\top}\\mathrm{P}_{A_{t},\\ell}\\bar{\\Sigma}_{t,\\ell}\\mathrm{P}_{A_{t},\\ell}^{\\top}X_{t})\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "831 where $(i)$ follows from Eq. (34), and we use the following inequality in $(i i)$ ", "page_idx": 25}, {"type": "equation", "text": "$$\nx=\\frac{x}{\\log(1+x)}\\log(1+x)\\leq\\left(\\operatorname*{max}_{x\\in[0,u]}\\frac{x}{\\log(1+x)}\\right)\\log(1+x)=\\frac{u}{\\log(1+u)}\\log(1+x)\\,,\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "832 which holds for any $x\\in[0,u]$ , where constants $c_{0}$ and $c_{\\ell}$ are derived as ", "page_idx": 25}, {"type": "equation", "text": "$$\nc_{0}=\\frac{\\sigma_{1}^{2}}{\\log(1+\\frac{\\sigma_{1}^{2}}{\\sigma^{2}})}\\;,\\quad c_{\\ell}=\\frac{\\sigma_{\\ell+1}^{2}}{\\log(1+\\frac{\\sigma_{\\ell+1}^{2}}{\\sigma^{2}})}\\;,\\mathrm{with\\;the\\;convention\\;that}\\;\\sigma_{L+1}=1\\,.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "833 The derivation of $c_{\\mathrm{0}}$ uses that ", "page_idx": 25}, {"type": "equation", "text": "$$\nX_{t}^{\\top}\\hat{\\Sigma}_{t,A_{t}}X_{t}\\leq\\lambda_{1}(\\hat{\\Sigma}_{t,A_{t}})\\|X_{t}\\|^{2}\\leq\\lambda_{d}^{-1}(\\Sigma_{1}^{-1}+G_{t,A_{t}})\\leq\\lambda_{d}^{-1}(\\Sigma_{1}^{-1})=\\lambda_{1}(\\Sigma_{1})=\\sigma_{1}^{2}\\,.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "834 The derivation of $c_{\\ell}$ follows from ", "page_idx": 25}, {"type": "equation", "text": "$$\nX_{t}^{\\top}\\mathrm{P}_{A_{t},\\ell}\\bar{\\Sigma}_{t,\\ell}\\mathrm{P}_{A_{t},\\ell}^{\\top}X_{t}\\leq\\lambda_{1}\\big(\\mathrm{P}_{A_{t},\\ell}\\mathrm{P}_{A_{t},\\ell}^{\\top}\\big)\\lambda_{1}\\big(\\bar{\\Sigma}_{t,\\ell}\\big)\\|X_{t}\\|^{2}\\leq\\sigma_{\\ell+1}^{2}\\,.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "835 Therefore, from Eq. (35) and Eq. (33), we get that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{B}\\mathcal{R}(n)\\leq\\sqrt{2n\\log(1/\\delta)}\\Big(\\mathbb{E}\\Big[c_{0}\\displaystyle\\sum_{t=1}^{n}\\log(1+\\sigma^{-2}X_{t}^{\\top}\\hat{\\Sigma}_{t,A_{t}}X_{t})}\\\\ &{\\qquad\\mathrm+\\displaystyle\\sum_{\\ell\\in[L]}c_{\\ell}\\displaystyle\\sum_{t=1}^{n}\\log(1+\\sigma^{-2}X_{t}^{\\top}\\mathrm{P}_{A_{t},\\ell}\\bar{\\Sigma}_{t,\\ell}\\mathrm{P}_{A_{t},\\ell}^{\\top}X_{t})\\Big]\\Big)^{\\frac{1}{2}}+c n\\delta}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "836 Now we focus on bounding the logarithmic terms in Eq. (36). ", "page_idx": 25}, {"type": "text", "text": "837 (I) First term in Eq. (36) We first rewrite this term as ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\log(1+\\sigma^{-2}X_{t}^{\\top}\\hat{\\Sigma}_{t,A_{t}}X_{t})\\stackrel{(i)}{=}\\log\\operatorname*{det}(I_{d}+\\sigma^{-2}\\hat{\\Sigma}_{t,A_{t}}^{\\frac{1}{2}}X_{t}X_{t}^{\\top}\\hat{\\Sigma}_{t,A_{t}}^{\\frac{1}{2}})\\,,}\\\\ &{\\qquad=\\log\\operatorname*{det}(\\hat{\\Sigma}_{t,A_{t}}^{-1}+\\sigma^{-2}X_{t}X_{t}^{\\top})-\\log\\operatorname*{det}(\\hat{\\Sigma}_{t,A_{t}}^{-1})=\\log\\operatorname*{det}(\\hat{\\Sigma}_{t+1,A_{t}}^{-1})-\\log\\operatorname*{det}(\\hat{\\Sigma}_{t,A_{t}}^{-1})\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "838 where $(i)$ follows from the Weinstein\u2013Aronszajn identity. Then we sum over all rounds $t\\in[n]$ , and   \n839 get a telescoping ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{t=1}^{n}\\log\\operatorname*{det}(I_{d}+\\sigma^{-2}\\hat{\\Sigma}_{t,A_{t}}^{\\frac{1}{2}}X_{t}X_{t}^{\\top}\\hat{\\Sigma}_{t,A_{t}}^{\\frac{1}{2}})=\\displaystyle\\sum_{t=1}^{n}\\log\\operatorname*{det}(\\hat{\\Sigma}_{t+1,A_{t}}^{-1})-\\log\\operatorname*{det}(\\hat{\\Sigma}_{t,A_{t}}^{-1})\\,,}\\\\ {\\displaystyle\\qquad=\\sum_{t=1}^{n}\\sum_{i=1}^{K}\\log\\operatorname*{det}(\\hat{\\Sigma}_{t+1,i}^{-1})-\\log\\operatorname*{det}(\\hat{\\Sigma}_{t,i}^{-1})=\\displaystyle\\sum_{i=1}^{K}\\sum_{t=1}^{n}\\log\\operatorname*{det}(\\hat{\\Sigma}_{t+1,i}^{-1})-\\log\\operatorname*{det}(\\hat{\\Sigma}_{t,i}^{-1})\\,,}\\\\ {\\displaystyle\\qquad=\\sum_{i=1}^{K}\\log\\operatorname*{det}(\\hat{\\Sigma}_{n+1,i}^{-1})-\\log\\operatorname*{det}(\\hat{\\Sigma}_{1,i}^{-1})\\overset{(i)}{=}\\displaystyle\\sum_{i=1}^{K}\\log\\operatorname*{det}(\\Sigma_{1}^{\\frac{1}{2}}\\hat{\\Sigma}_{n+1,i}^{-1}\\Sigma_{1}^{\\frac{1}{2}})\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "840 where $(i)$ follows from the fact that $\\hat{\\Sigma}_{1,i}\\,=\\,\\Sigma_{1}$ . Now we use the inequality of arithmetic and   \n841 geometric means and get ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{t=1}^{n}\\log\\operatorname*{det}(I_{d}+\\sigma^{-2}\\hat{\\Sigma}_{t,A_{t}}^{\\frac{1}{2}}X_{t}X_{t}^{\\top}\\hat{\\Sigma}_{t,A_{t}}^{\\frac{1}{2}})=\\displaystyle\\sum_{i=1}^{K}\\log\\operatorname*{det}(\\Sigma_{1}^{\\frac{1}{2}}\\hat{\\Sigma}_{n+1,i}^{-1}\\Sigma_{1}^{\\frac{1}{2}})\\~,}\\\\ {\\displaystyle\\leq\\sum_{i=1}^{K}d\\log\\left(\\frac{1}{d}\\operatorname{Tr}(\\Sigma_{1}^{\\frac{1}{2}}\\hat{\\Sigma}_{n+1,i}^{-1}\\Sigma_{1}^{\\frac{1}{2}})\\right)~,~~~~~~~~~~~~~~~~~~}\\\\ {\\displaystyle\\leq\\sum_{i=1}^{K}d\\log\\left(1+\\frac{n}{d}\\frac{\\sigma_{1}^{2}}{\\sigma^{2}}\\right)=K d\\log\\left(1+\\frac{n}{d}\\frac{\\sigma_{1}^{2}}{\\sigma^{2}}\\right)~.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "842 $(\\mathbf{II})$ Remaining terms in Eq. (36) Let $\\ell\\in[L]$ . Then we have that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\log(1+\\sigma^{-2}X_{t}^{\\top}\\mathrm{P}_{A_{t},\\ell}\\bar{\\Sigma}_{t,\\ell}\\mathrm{P}_{A_{t},\\ell}^{\\top}X_{t})=\\sigma_{\\mathrm{Max}}^{2\\ell}\\sigma_{\\mathrm{Max}}^{-2\\ell}\\log(1+\\sigma^{-2}X_{t}^{\\top}\\mathrm{P}_{A_{t},\\ell}\\bar{\\Sigma}_{t,\\ell}\\mathrm{P}_{A_{t},\\ell}^{\\top}X_{t})\\,,}\\\\ &{\\qquad\\leq\\sigma_{\\mathrm{Max}}^{2\\ell}\\log(1+\\sigma^{-2}\\sigma_{\\mathrm{MaX}}^{-2\\ell}X_{t}^{\\top}\\mathrm{P}_{A_{t},\\ell}\\bar{\\Sigma}_{t,\\ell}\\mathrm{P}_{A_{t},\\ell}^{\\top}X_{t})\\,,}\\\\ &{\\qquad\\overset{(i)}{=}\\sigma_{\\mathrm{Max}}^{2\\ell}\\log\\operatorname*{det}(I_{d}+\\sigma^{-2}\\sigma_{\\mathrm{Max}}^{-2\\ell}\\bar{\\Sigma}_{t,\\ell}^{\\frac{1}{2}}\\mathrm{P}_{A_{t},\\ell}^{\\top}X_{t}X_{t}^{\\top}\\mathrm{P}_{A_{t},\\ell}\\bar{\\Sigma}_{t,\\ell}^{\\frac{1}{2}})\\,,}\\\\ &{\\qquad=\\sigma_{\\mathrm{Max}}^{2\\ell}\\Bigl(\\log\\operatorname*{det}(\\bar{\\Sigma}_{t,\\ell}^{-1}+\\sigma^{-2}\\sigma_{\\mathrm{MaX}}^{-2\\ell}\\mathrm{P}_{A_{t},\\ell}^{\\top}X_{t}X_{t}^{\\top}\\mathrm{P}_{A_{t},\\ell})-\\log\\operatorname*{det}(\\bar{\\Sigma}_{t,\\ell}^{-1})\\Bigr)\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "884434 fwohlleorew iwnge  iunseeq tuhael itWy ehinosltdesi $\\sigma^{-2}\\sigma_{\\mathrm{MAX}}^{-2\\ell}\\mathrm{P}_{A_{t},\\ell}^{\\top}X_{t}X_{t}^{\\top}\\mathrm{P}_{A_{t},\\ell}\\,\\preceq\\,\\bar{\\Sigma}_{t+1,\\ell}^{-1}-\\bar{\\Sigma}_{t,\\ell}^{-1}$ $(i)$ .r oAms  La eremsmulat , Dw.3e  tgheatt  tthhaet   \n845 $\\bar{\\Sigma}_{t,\\ell}^{-1}+\\sigma^{-2}\\sigma_{\\mathrm{MAX}}^{-2\\ell}\\mathrm{P}_{A_{t},\\ell}^{\\top}X_{t}X_{t}^{\\top}\\mathrm{P}_{A_{t},\\ell}\\preceq\\bar{\\Sigma}_{t+1,\\ell}^{-1}$ . Thus, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\log(1+\\sigma^{-2}X_{t}^{\\top}\\mathrm{P}_{A_{t},\\ell}\\bar{\\Sigma}_{t,\\ell}\\mathrm{P}_{A_{t},\\ell}^{\\top}X_{t})\\leq\\sigma_{\\mathrm{Max}}^{2\\ell}\\Bigl(\\log\\operatorname*{det}(\\bar{\\Sigma}_{t+1,\\ell}^{-1})-\\log\\operatorname*{det}(\\bar{\\Sigma}_{t,\\ell}^{-1})\\Bigr)\\,,\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "846 Then we sum over all rounds $t\\in[n]$ , and get a telescoping ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{n}\\log(1+\\sigma^{-2}X_{t}^{\\top}\\mathrm{P}_{A_{t},\\ell}\\bar{\\Sigma}_{t,\\ell}\\mathrm{P}_{A_{t},\\ell}^{\\top}X_{t})\\leq\\sigma_{\\operatorname*{Max}}^{2\\ell}\\sum_{t=1}^{n}\\log\\operatorname*{det}(\\bar{\\Sigma}_{t+1,\\ell}^{-1})-\\log\\operatorname*{det}(\\bar{\\Sigma}_{t,\\ell}^{-1})\\,,}\\\\ {=\\sigma_{\\operatorname*{Max}}^{2\\ell}\\Big(\\log\\operatorname*{det}(\\bar{\\Sigma}_{n+1,\\ell}^{-1})-\\log\\operatorname*{det}(\\bar{\\Sigma}_{1,\\ell}^{-1})\\Big)\\,,}\\\\ {\\overset{(i)}{=}\\sigma_{\\operatorname*{Max}}^{2\\ell}\\Big(\\log\\operatorname*{det}(\\bar{\\Sigma}_{n+1,\\ell}^{-1})-\\log\\operatorname*{det}(\\Sigma_{\\ell+1}^{-1})\\Big)\\,,}\\\\ {=\\sigma_{\\operatorname*{Max}}^{2\\ell}\\Big(\\log\\operatorname*{det}(\\Sigma_{\\ell+1}^{\\frac{1}{2}}\\bar{\\Sigma}_{n+1,\\ell}^{-1}\\Sigma_{\\ell+1}^{\\frac{1}{2}})\\Big)\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "847 where we use that $\\bar{\\Sigma}_{1,\\ell}=\\Sigma_{\\ell+1}$ in $(i)$ . Finally, we use the inequality of arithmetic and geometric   \n848 means and get that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{t=1}^{n}\\log(1+\\sigma^{-2}X_{t}^{\\top}\\mathrm{P}_{A_{t},\\ell}\\bar{\\Sigma}_{t,\\ell}\\mathrm{P}_{A_{t},\\ell}^{\\top}X_{t})\\le\\sigma_{\\mathrm{Max}}^{2\\ell}\\bigg(\\log\\operatorname*{det}(\\Sigma_{\\ell+1}^{\\frac{1}{2}}\\bar{\\Sigma}_{n+1,\\ell}^{-1}\\Sigma_{\\ell+1}^{\\frac{1}{2}})\\bigg)\\;,}}\\\\ &{}&{\\le d\\sigma_{\\mathrm{Max}}^{2\\ell}\\log\\left(\\displaystyle\\frac{1}{d}\\,\\mathrm{Tr}(\\Sigma_{\\ell+1}^{\\frac{1}{2}}\\bar{\\Sigma}_{n+1,\\ell}^{-1}\\Sigma_{\\ell+1}^{\\frac{1}{2}})\\right)\\,,}\\\\ &{}&{\\le d\\sigma_{\\mathrm{Max}}^{2\\ell}\\log\\left(1+\\frac{\\sigma_{\\ell+1}^{2}}{\\sigma_{\\ell}^{2}}\\right)\\,,\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "849 The last inequality follows from the expression of $\\bar{\\Sigma}_{n+1,\\ell}^{-1}$ in Eq. (18) that leads to ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Sigma_{\\ell+1}^{\\frac{1}{2}}\\bar{\\Sigma}_{n+1,\\ell}^{-1}\\Sigma_{\\ell+1}^{\\frac{1}{2}}=I_{d}+\\Sigma_{\\ell+1}^{\\frac{1}{2}}\\bar{G}_{t,\\ell}\\Sigma_{\\ell+1}^{\\frac{1}{2}}\\,,}\\\\ &{\\qquad\\qquad\\qquad\\qquad=I_{d}+\\Sigma_{\\ell+1}^{\\frac{1}{2}}\\mathrm{W}_{\\ell}^{\\top}\\left(\\Sigma_{\\ell}^{-1}-\\Sigma_{\\ell}^{-1}\\bar{\\Sigma}_{t,\\ell-1}\\Sigma_{\\ell}^{-1}\\right)\\mathrm{W}_{\\ell}\\Sigma_{\\ell+1}^{\\frac{1}{2}}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "850 since $\\bar{G}_{t,\\ell}=\\mathrm{W}_{\\ell}^{\\top}\\big(\\Sigma_{\\ell}^{-1}-\\Sigma_{\\ell}^{-1}\\bar{\\Sigma}_{t,\\ell-1}\\Sigma_{\\ell}^{-1}\\big)\\mathrm{W}_{\\ell}$ . This allows us to bound $\\textstyle{\\frac{1}{d}}\\operatorname{Tr}(\\Sigma_{\\ell+1}^{\\frac{1}{2}}\\bar{\\Sigma}_{n+1,\\ell}^{-1}\\Sigma_{\\ell+1}^{\\frac{1}{2}})$ as ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\frac{1}{d}\\operatorname{Tr}(\\Sigma_{t+1}^{\\frac{1}{\\gamma}}\\Sigma_{n+1,\\tau}^{-1}\\Sigma_{t+1}^{\\frac{1}{\\gamma}})=\\frac{1}{d}\\operatorname{Tr}(I_{d}+\\Sigma_{t+1}^{\\frac{1}{\\gamma}}\\mathbb{W}_{\\ell}^{\\top}(\\Sigma_{\\ell}^{-1}-\\Sigma_{\\ell}^{-1}\\Sigma_{\\ell,\\ell-1}\\Sigma_{\\ell}^{-1})\\mathbb{W}_{\\ell}\\Sigma_{\\ell+1}^{\\frac{1}{\\gamma}})\\,,}\\\\ &{=\\frac{1}{d}(d+\\operatorname{Tr}(\\Sigma_{t+1}^{\\frac{1}{\\gamma}}\\mathbb{W}_{\\ell}^{\\top}(\\Sigma_{\\ell}^{-1}-\\Sigma_{\\ell}^{-1}\\Sigma_{\\ell,\\ell-1}\\Sigma_{\\ell}^{-1})\\mathbb{W}_{\\ell}\\Sigma_{\\ell+1}^{\\frac{1}{\\gamma}})\\,,}\\\\ &{\\leq1+\\frac{1}{d}\\displaystyle\\sum_{k=1}^{d}\\lambda_{1}(\\Sigma_{t+1}^{\\frac{1}{\\gamma}}\\mathbb{W}_{\\ell}^{\\top}(\\Sigma_{\\ell}^{-1}-\\Sigma_{\\ell}^{-1}\\Sigma_{\\ell,\\ell-1}\\Sigma_{\\ell}^{-1})\\mathbb{W}_{\\ell}\\Sigma_{\\ell+1}^{\\frac{1}{\\gamma}}\\,,}\\\\ &{\\leq1+\\displaystyle\\frac{1}{d}\\displaystyle\\sum_{k=1}^{d}\\lambda_{1}(\\Sigma_{\\ell+1})\\lambda_{1}(\\mathbb{W}_{\\ell}^{\\top}\\mathbb{W}_{\\ell})\\lambda_{1}(\\Sigma_{\\ell}^{-1}-\\Sigma_{\\ell}^{-1}\\Sigma_{\\ell,\\ell-1}\\Sigma_{\\ell}^{-1})\\,,}\\\\ &{\\leq1+\\frac{1}{d}\\displaystyle\\sum_{k=1}^{d}\\lambda_{1}(\\Sigma_{t+1})\\lambda_{1}(\\mathbb{W}_{\\ell}^{\\top}\\mathbb{W}_{\\ell})\\lambda_{1}(\\Sigma_{\\ell}^{-1})\\,,}\\\\ &{\\leq1+\\frac{1}{d}\\displaystyle\\sum_{k=1}^{d}\\lambda_{1}(\\Sigma_{t+1})\\lambda_{1}(\\mathbb{W}_{\\ell}^{\\top}\\mathbb{W}_{\\ell})\\lambda_{1}(\\Sigma_{\\ell}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "851 where we use the assumption that $\\lambda_{1}(\\mathrm{W}_{\\ell}^{\\top}\\mathrm{W}_{\\ell})=1$ (A2) and that $\\lambda_{1}(\\Sigma_{\\ell+1})=\\sigma_{\\ell+1}^{2}$ and $\\lambda_{1}(\\Sigma_{\\ell}^{-1})=$   \n852 $1/\\sigma_{\\ell}^{2}$ . This is because $\\Sigma_{\\ell}=\\sigma_{\\ell}^{2}I_{d}$ for any $\\ell\\in[L+1]$ . Finally, plugging Eqs. (37) and (38) in Eq. (36)   \n853 concludes the proof. ", "page_idx": 27}, {"type": "text", "text": "854 D.6 Proof of proposition 4.2 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "855 We use exactly the same proof in Appendix D.5, with one change to account for the sparsity   \n856 assumption (A3). The change corresponds to Eq. (38). First, recall that Eq. (38) writes ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{n}\\log(1+\\sigma^{-2}X_{t}^{\\top}\\mathrm{P}_{A_{t},\\ell}\\bar{\\Sigma}_{t,\\ell}\\mathrm{P}_{A_{t},\\ell}^{\\top}X_{t})\\leq\\sigma_{\\mathrm{Max}}^{2\\ell}\\Bigl(\\log\\operatorname*{det}(\\Sigma_{\\ell+1}^{\\frac{1}{2}}\\bar{\\Sigma}_{n+1,\\ell}^{-1}\\Sigma_{\\ell+1}^{\\frac{1}{2}})\\Bigr)\\,,\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "857 where ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Sigma_{\\ell+1}^{\\frac{1}{2}}\\bar{\\Sigma}_{n+1,\\ell}^{-1}\\Sigma_{\\ell+1}^{\\frac{1}{2}}=I_{d}+\\Sigma_{\\ell+1}^{\\frac{1}{2}}\\mathrm{W}_{\\ell}^{\\top}\\big(\\Sigma_{\\ell}^{-1}-\\Sigma_{\\ell}^{-1}\\bar{\\Sigma}_{t,\\ell-1}\\Sigma_{\\ell}^{-1}\\big)\\mathrm{W}_{\\ell}\\Sigma_{\\ell+1}^{\\frac{1}{2}}\\,,}\\\\ &{\\qquad\\qquad\\qquad\\qquad=I_{d}+\\sigma_{\\ell+1}^{2}\\mathrm{W}_{\\ell}^{\\top}\\big(\\Sigma_{\\ell}^{-1}-\\Sigma_{\\ell}^{-1}\\bar{\\Sigma}_{t,\\ell-1}\\Sigma_{\\ell}^{-1}\\big)\\mathrm{W}_{\\ell}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "858 where the second equality follows from the assumption that $\\Sigma_{\\ell+1}\\,=\\,\\sigma_{\\ell+1}^{2}I_{d}$ . But notice that in   \n859 our assumption, (A3), we assume that $\\mathrm{W}_{\\ell}=\\left(\\bar{\\mathrm{W}}_{\\ell},0_{d,d-d_{\\ell}}\\right)$ , where $\\bar{\\mathrm{W}}_{\\ell}\\in\\mathbb{R}^{d\\times d_{\\ell}}$ for any $\\ell\\in[L]$ .   \n860 Therefore, we have that for any $d\\,\\times\\,d$ matrix $\\mathrm{~B~}\\in\\ \\mathbb{R}^{d d\\times d}$ , the following holds, $\\mathrm{W}_{\\ell}^{\\top}\\mathrm{BW}_{\\ell}\\;=\\;$   \n861 0W\u2113B W\u2113 00d\u2113,d\u2212d\u2113  . In particular, we have that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{W}_{\\ell}^{\\top}\\left(\\Sigma_{\\ell}^{-1}-\\Sigma_{\\ell}^{-1}\\bar{\\Sigma}_{t,\\ell-1}\\Sigma_{\\ell}^{-1}\\right)\\mathrm{W}_{\\ell}=\\left(\\bar{\\mathrm{W}}_{\\ell}^{\\top}\\left(\\Sigma_{\\ell}^{-1}-\\Sigma_{\\ell}^{-1}\\bar{\\Sigma}_{t,\\ell-1}\\Sigma_{\\ell}^{-1}\\right)\\bar{\\mathrm{W}}_{\\ell}\\right.\\quad\\left.0_{d_{\\ell},d-d_{\\ell}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "862 Therefore, plugging this in Eq. (41) yields that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Sigma_{\\ell+1}^{\\frac{1}{2}}\\bar{\\Sigma}_{n+1,\\ell}^{-1}\\Sigma_{\\ell+1}^{\\frac{1}{2}}=\\left({\\cal I}_{d_{\\ell}}+\\sigma_{\\ell+1}^{2}\\bar{\\bf W}_{\\ell}^{\\top}\\left(\\Sigma_{\\ell}^{-1}-\\Sigma_{\\ell}^{-1}\\bar{\\Sigma}_{t,\\ell-1}\\Sigma_{\\ell}^{-1}\\right)\\bar{\\bf W}_{\\ell}\\quad0_{d_{\\ell},d-d_{\\ell}}\\right).}\\\\ {0_{d-d_{\\ell},d_{\\ell}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "863 As a result, $\\begin{array}{r}{\\operatorname*{det}(\\Sigma_{\\ell+1}^{\\frac{1}{2}}\\bar{\\Sigma}_{n+1,\\ell}^{-1}\\Sigma_{\\ell+1}^{\\frac{1}{2}})=\\operatorname*{det}(I_{d_{\\ell}}+\\sigma_{\\ell+1}^{2}\\bar{\\mathrm{W}}_{\\ell}^{\\top}\\left(\\Sigma_{\\ell}^{-1}-\\Sigma_{\\ell}^{-1}\\bar{\\Sigma}_{t,\\ell-1}\\Sigma_{\\ell}^{-1}\\right)\\bar{\\mathrm{W}}_{\\ell})}\\end{array}$ . This allows   \n864 us to move the problem from a $d$ -dimensional one to a $d_{\\ell}$ -dimensional one. Then we use the inequality ", "page_idx": 27}, {"type": "text", "text": "865 of arithmetic and geometric means and get that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{n}\\log(1+\\sigma^{-2}X_{t}^{\\top}\\mathrm{P}_{A_{t},\\ell}\\bar{\\Sigma}_{t,\\ell}\\mathrm{P}_{A_{t},\\ell}^{\\top}X_{t})\\le\\sigma_{\\mathrm{Max}}^{2\\ell}\\bigg(\\log\\operatorname*{det}(\\Sigma_{\\ell+1}^{\\frac{1}{2}}\\bar{\\Sigma}_{n+1,\\ell}^{-1}\\Sigma_{\\ell+1}^{\\frac{1}{2}})\\bigg)\\,,}\\\\ &{\\qquad\\qquad\\qquad=\\sigma_{\\mathrm{Max}}^{2\\ell}\\log\\operatorname*{det}(I_{d_{\\ell}}+\\sigma_{\\ell+1}^{2}\\bar{\\mathrm{W}}_{\\ell}^{\\top}\\big(\\Sigma_{\\ell}^{-1}-\\Sigma_{\\ell}^{-1}\\bar{\\Sigma}_{t,\\ell-1}\\Sigma_{\\ell}^{-1}\\big)\\bar{\\mathrm{W}}_{\\ell})\\,,}\\\\ &{\\qquad\\qquad\\le d_{\\ell}\\sigma_{\\mathrm{Max}}^{2\\ell}\\log\\left(\\displaystyle\\frac{1}{d_{\\ell}}\\,\\mathrm{Tr}(I_{d_{\\ell}}+\\sigma_{\\ell+1}^{2}\\bar{\\mathrm{W}}_{\\ell}^{\\top}\\big(\\Sigma_{\\ell}^{-1}-\\Sigma_{\\ell}^{-1}\\bar{\\Sigma}_{t,\\ell-1}\\Sigma_{\\ell}^{-1}\\big)\\bar{\\mathrm{W}}_{\\ell})\\right)\\,,}\\\\ &{\\qquad\\qquad\\le d_{\\ell}\\sigma_{\\mathrm{Max}}^{2\\ell}\\log\\left(1+\\frac{\\sigma_{\\ell+1}^{2}}{\\sigma_{\\ell}^{2}}\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "866 To get the last inequality, we use derivations similar to the ones we used in Eq. (40). Finally, the   \n867 desired result in obtained by replacing Eq. (38) by Eq. (44) in the previous proof in Appendix D.5. ", "page_idx": 28}, {"type": "text", "text": "868 D.7 Additional discussion: link to two-level hierarchies ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "869 The linear diffusion (15) can be marginalized into a 2-level hierarchy using two different strategies.   \n870 The first one yields, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\psi_{*,L}\\sim\\mathcal{N}(0,\\sigma_{L+1}^{2}\\mathrm{B}_{L}\\mathrm{B}_{L}^{\\top})\\,,\\ }\\\\ {\\theta_{*,i}\\mid\\psi_{*,L}\\sim\\mathcal{N}(\\psi_{*,L},\\,\\Omega_{1})\\,,\\ }\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "equation", "text": "$$\n\\forall i\\in[K]\\,,\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "871 with $\\begin{array}{r}{\\Omega_{1}=\\sigma_{1}^{2}I_{d}+\\sum_{\\ell=1}^{L-1}{\\sigma_{\\ell+1}^{2}\\mathrm{B}_{\\ell}\\mathrm{B}_{\\ell}^{\\top}}}\\end{array}$ and $\\begin{array}{r}{\\mathrm{B}\\ell=\\prod_{k=1}^{\\ell}\\mathrm{W}_{k}}\\end{array}$ . The second strategy yields, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\psi_{*,1}\\sim{\\cal N}(0,\\Omega_{2})\\,,\\qquad\\qquad\\qquad\\qquad\\qquad}\\\\ {\\theta_{*,i}\\mid\\psi_{*,1}\\sim{\\cal N}(\\psi_{*,1},\\,\\sigma_{1}^{2}I_{d})\\,,\\qquad\\qquad\\qquad\\qquad\\forall i\\in[K]\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "872 where $\\begin{array}{r}{\\Omega_{2}\\,=\\,\\sum_{\\ell=1}^{L}\\sigma_{\\ell+1}^{2}\\mathrm{B}_{\\ell}\\mathrm{B}_{\\ell}^{\\top}}\\end{array}$ . Recently, HierTS [Hong et al., 2022b] was developed for such   \n873 two-level gra phical models, and we call HierTS under (45) by HierTS-1 and HierTS under (46)   \n874 by HierTS-2. Then, we start by highlighting the differences between these two variants of HierTS.   \n875 First, their regret bounds scale as ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{HierTS-1:~}\\tilde{\\mathcal{O}}\\big(\\sqrt{n d(K\\sum_{\\ell=1}^{L}\\sigma_{\\ell}^{2}+L\\sigma_{L+1}^{2})}\\,\\,\\big)\\,\\,,\\quad\\mathrm{HierTS-2:~}\\tilde{\\mathcal{O}}\\big(\\sqrt{n d(K\\sigma_{1}^{2}+\\sum_{\\ell=1}^{L}\\sigma_{\\ell+1}^{2})}\\big)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "876 When $K\\approx L$ , the regret bounds of HierTS-1 and HierTS-2 are similar. However, when $K>L$ ,   \n877 HierTS-2 outperforms HierTS-1. This is because HierTS-2 puts more uncertainty on a single   \n878 $d$ -dimensional latent parameter $\\psi_{*,1}$ , rather than $K$ individual $d$ -dimensional action parameters   \n879 $\\theta_{*,i}$ . More importantly, HierTS-1 implicitly assumes that action parameters $\\theta_{*,i}$ are conditionally   \n880 independent given $\\psi_{*,L}$ , which is not true. Consequently, HierTS-2 outperforms HierTS-1. Note   \n881 that, under the linear diffusion model (15), dTS and HierTS-2 have roughly similar regret bounds.   \n882 Specifically, their regret bounds dependency on $K$ is identical, where both methods involve mul  \n883 tiplying $K$ by $\\sigma_{1}^{2}$ , and both enjoy improved performance compared to HierTS-1. That said, note   \n884 that Theorem 4.1 and Proposition 4.2 provide an understanding of how dTS\u2019s regret scales under   \n885 linear score functions $f_{\\ell}$ , and do not say that using dTS is better than using HierTS when the score   \n886 functions $f_{\\ell}$ are linear since the latter can be obtained by a proper marginalization of latent parameters   \n887 (i.e., HierTS-2 instead of HierTS-1). While such a comparison is not the goal of this work, we still   \n888 provide it for completeness next.   \n889 When the mixing matrices $\\mathrm{W}_{\\ell}$ are dense (i.e., assumption (A3) is not applicable), dTS and HierTS-2   \n890 have comparable regret bounds and computational efficiency. However, under the sparsity assumption   \n891 (A3) and with mixing matrices that allow for conditional independence of $\\psi_{*,1}$ coordinates given   \n892 $\\psi_{*,2}$ , dTS enjoys a computational advantage over HierTS-2. This advantage explains why works   \n893 focusing on multi-level hierarchies typically benchmark their algorithms against two-level structures   \n894 akin to HierTS-1, rather than the more competitive HierTS-2. This is also consistent with prior   \n895 works in Bayesian bandits using multi-level hierarchies, such as Tree-based priors [Hong et al.,   \n896 2022a], which compared their method to HierTS-1. In line with this, we also compared dTS with   \n897 HierTS-1 in our experiments. But this is only given for completeness as this is not the aim of   \n898 Theorem 4.1 and Proposition 4.2. More importantly, HierTS is inapplicable in the general case in (1)   \n899 with non-linear score functions since the latent parameters cannot be analytically marginalized. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "900 E Broader impact ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "901 This work contributes to the development and analysis of practical algorithms for online learning to   \n902 act under uncertainty. While our generic setting and algorithms have broad potential applications,   \n903 the specific downstream social impacts are inherently dependent on the chosen application domain.   \n904 Nevertheless, we acknowledge the crucial need to consider potential biases that may be present in   \n905 pre-trained diffusion models, given that our method relies on them. ", "page_idx": 29}, {"type": "text", "text": "906 F Limitations ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "907 Our work investigated contextual bandits, laying the groundwork for future exploration into reinforce  \n908 ment learning. This exploration can be done from both practical (empirical) and theoretical angles.   \n909 While our method, which approximates rewards using a Gaussian distribution, worked well for linear   \n910 rewards and those following a generalized linear model, its effectiveness in real-world, complex   \n911 scenarios needs further testing. Another interesting direction for future research is pre-training the   \n912 diffusion model prior. Hsieh et al. [2023] proposed a method for this in multi-armed bandits, but its   \n913 application to contextual bandits remains unexplored. ", "page_idx": 29}, {"type": "text", "text": "914 G Amount of computation required ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "915 Our experiments were conducted on internal machines with 30 CPUs and thus they required a moder  \n916 ate amount of computation. These experiments are also reproducible with minimal computational   \n917 resources. ", "page_idx": 29}, {"type": "text", "text": "918 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "20 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n21 paper\u2019s contributions and scope?   \n22 Answer: [Yes]   \n23 Justification: All claims are supported by the theory in Section 4 (with proofs provided in   \n24 the appendix) and experiments in Section 5.   \n25 Guidelines:   \n26 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n27 made in the paper.   \n28 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n29 contributions made in the paper and important assumptions and limitations. A No or   \n30 NA answer to this question will not be perceived well by the reviewers.   \n31 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n32 much the results can be expected to generalize to other settings.   \n33 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n34 are not attained by the paper. ", "page_idx": 30}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Limitations were discussed in Section 6 and Appendix F. ", "page_idx": 30}, {"type": "text", "text": "39 Guidelines:   \n40 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n41 the paper has limitations, but those are not discussed in the paper.   \n2 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n43 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n44 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n45 model well-specification, asymptotic approximations only holding locally). The authors   \n6 should reflect on how these assumptions might be violated in practice and what the   \n47 implications would be.   \n48 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n9 only tested on a few datasets or with a few runs. In general, empirical results often   \ndepend on implicit assumptions, which should be articulated.   \n1 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n52 For example, a facial recognition algorithm may perform poorly when image resolution   \n3 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n54 used reliably to provide closed captions for online lectures because it fails to handle   \n5 technical jargon.   \n56 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n7 and how they scale with dataset size.   \n58 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n59 address problems of privacy and fairness.   \n0 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n61 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n2 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n63 judgment and recognize that individual actions in favor of transparency play an impor  \n4 tant role in developing norms that preserve the integrity of the community. Reviewers   \n65 will be specifically instructed to not penalize honesty concerning limitations.   \n66 3. Theory Assumptions and Proofs   \n967 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n968 a complete (and correct) proof? ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Assumptions are mentioned in the main text. Complete proofs are provided in the appendix. ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 31}, {"type": "text", "text": "983 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "4 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n85 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n86 of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Information needed to reproduce the main experimental results of the paper is described in Section 5. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 31}, {"type": "text", "text": "1022 5. Open access to data and code ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "tions to faithfully reproduce the main experimental results, as described in supplemental   \nmaterial?   \nAnswer: [Yes]   \nJustification: The code for the main experiments is shared in the supplementary material.   \nGuidelines: \u2022 The answer NA means that paper does not include experiments requiring code. \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. \u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). \u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. \u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. \u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. \u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). \u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 32}, {"type": "text", "text": "1023   \n1024   \n1025   \n1026   \n1027   \n1028   \n1029   \n1030   \n1031   \n1032   \n1033   \n1034   \n1035   \n1036   \n1037   \n1038   \n1039   \n1040   \n1041   \n1042   \n1043   \n1044   \n1045   \n1046   \n1047   \n1048   \n1049   \n1050   \n1051   \n1052   \n1053   \n1054   \n1055   \n1056   \n1057   \n1058   \n1059   \n1060   \n1061   \n1062   \n1063   \n1064   \n1065   \n1066   \n1067   \n1068   \n1069   \n1070   \n1071   \n1072   \n1073   \n1074 ", "page_idx": 32}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] Justification: All experimental details are described in Section 5. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 32}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Standard error bars are included in the figures. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) ", "page_idx": 32}, {"type": "text", "text": "1075 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n1076 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n1077 of the mean.   \n1078 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n1079 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n1080 of Normality of errors is not verified.   \n1081 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n1082 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n1083 error rates).   \n1084 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n1085 they were calculated and reference the corresponding figures or tables in the text.   \n1086 8. Experiments Compute Resources   \n1087 Question: For each experiment, does the paper provide sufficient information on the com  \n1088 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n1089 the experiments?   \n1090 Answer: [Yes]   \n1091 Justification: As mentioned in Appendix G, our experiments were conducted on internal   \n1092 machines with 30 CPUs and thus they required a moderate amount of computation. These   \n1093 experiments are also reproducible with minimal computational resources.   \n1094 Guidelines:   \n1095 \u2022 The answer NA means that the paper does not include experiments.   \n1096 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n1097 or cloud provider, including relevant memory and storage.   \n1098 \u2022 The paper should provide the amount of compute required for each of the individual   \n1099 experimental runs as well as estimate the total compute.   \n1100 \u2022 The paper should disclose whether the full research project required more compute   \n1101 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n1102 didn\u2019t make it into the paper).   \n1103 9. Code Of Ethics   \n1104 Question: Does the research conducted in the paper conform, in every respect, with the   \n1105 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n1106 Answer: [Yes]   \n1107 Justification: This work contributes to the development and theoretical analysis of online   \n1108 learning to act under uncertainty and it adheres to the Neurips Code Of Ethics.   \n1109 Guidelines:   \n1110 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n1111 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n1112 deviation from the Code of Ethics.   \n1113 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n1114 eration due to laws or regulations in their jurisdiction).   \n1115 10. Broader Impacts   \n1116 Question: Does the paper discuss both potential positive societal impacts and negative   \n1117 societal impacts of the work performed?   \n1118 Answer: [Yes]   \n1119 Justification: Broader Impacts are discussed in Appendix E.   \n1120 Guidelines:   \n1121 \u2022 The answer NA means that there is no societal impact of the work performed.   \n1122 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n1123 impact or why the paper does not address societal impact.   \n1124 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n1125 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n1126 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n1127 groups), privacy considerations, and security considerations.   \n1128 \u2022 The conference expects that many papers will be foundational research and not tied   \n1129 to particular applications, let alone deployments. However, if there is a direct path to   \n1130 any negative applications, the authors should point it out. For example, it is legitimate   \n1131 to point out that an improvement in the quality of generative models could be used to   \n1132 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n1133 that a generic algorithm for optimizing neural networks could enable people to train   \n1134 models that generate Deepfakes faster.   \n1135 \u2022 The authors should consider possible harms that could arise when the technology is   \n1136 being used as intended and functioning correctly, harms that could arise when the   \n1137 technology is being used as intended but gives incorrect results, and harms following   \n1138 from (intentional or unintentional) misuse of the technology.   \n1139 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n1140 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n1141 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n1142 feedback over time, improving the efficiency and accessibility of ML).   \n1143 11. Safeguards   \n1144 Question: Does the paper describe safeguards that have been put in place for responsible   \n1145 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n1146 image generators, or scraped datasets)?   \n1147 Answer: [NA]   \n1148 Justification: Our paper is mainly theoretical and the used data is simulated. Thus, we   \n1149 believe that our work poses no such risks.   \n1150 Guidelines:   \n1151 \u2022 The answer NA means that the paper poses no such risks.   \n1152 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n1153 necessary safeguards to allow for controlled use of the model, for example by requiring   \n1154 that users adhere to usage guidelines or restrictions to access the model or implementing   \n1155 safety filters.   \n1156 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n1157 should describe how they avoided releasing unsafe images.   \n1158 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n1159 not require this, but we encourage authors to take this into account and make a best   \n1160 faith effort.   \n1161 12. Licenses for existing assets   \n1162 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n1163 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n1164 properly respected?   \n1165 Answer: [Yes]   \n1166 Justification: To the best of our knowledge, all relevant and used papers were cited.   \n1167 Guidelines:   \n1168 \u2022 The answer NA means that the paper does not use existing assets.   \n1169 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n1170 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n1171 URL.   \n1172 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n1173 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n1174 service of that source should be provided.   \n1175 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n1176 package should be provided. For popular datasets, paperswithcode.com/datasets   \n1177 has curated licenses for some datasets. Their licensing guide can help determine the   \n1178 license of a dataset.   \n1179 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n1180 the derived asset (if it has changed) should be provided.   \n1181 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n1182 the asset\u2019s creators.   \n1183 13. New Assets   \n1184 Question: Are new assets introduced in the paper well documented and is the documentation   \n1185 provided alongside the assets?   \n1186 Answer: [Yes]   \n1187 Justification: We include our code as supplementary material, with all details needed for   \n1188 reproducibility given in Section 5.   \n1189 Guidelines:   \n1190 \u2022 The answer NA means that the paper does not release new assets.   \n1191 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n1192 submissions via structured templates. This includes details about training, license,   \n1193 limitations, etc.   \n1194 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n1195 asset is used.   \n1196 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n1197 create an anonymized URL or include an anonymized zip file.   \n1198 14. Crowdsourcing and Research with Human Subjects   \n1199 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n1200 include the full text of instructions given to participants and screenshots, if applicable, as   \n1201 well as details about compensation (if any)?   \n1202 Answer: [NA]   \n1203 Justification: This work does not involve crowdsourcing nor research with human subjects.   \n1204 Guidelines:   \n1205 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n1206 human subjects.   \n1207 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n1208 tion of the paper involves human subjects, then as much detail as possible should be   \n1209 included in the main paper.   \n1210 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n1211 or other labor should be paid at least the minimum wage in the country of the data   \n1212 collector.   \n1213 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n1214 Subjects   \n1215 Question: Does the paper describe potential risks incurred by study participants, whether   \n1216 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n1217 approvals (or an equivalent approval/review based on the requirements of your country or   \n1218 institution) were obtained?   \n1219 Answer: [NA]   \n1220 Justification: This work does not involve crowdsourcing nor research with human subjects.   \n1221 Guidelines:   \n1222 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n1223 human subjects.   \n1224 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n1225 may be required for any human subjects research. If you obtained IRB approval, you   \n1226 should clearly state this in the paper. ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 36}]