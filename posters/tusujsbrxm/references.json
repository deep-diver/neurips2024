{"references": [{"fullname_first_author": "Y. Abbasi-Yadkori", "paper_title": "Improved algorithms for linear stochastic bandits", "publication_date": "2011", "reason": "This paper provides improved algorithms for linear stochastic bandits, which are foundational to the development of efficient offline RL algorithms."}, {"fullname_first_author": "D. J. Foster", "paper_title": "Offline reinforcement learning: Fundamental barriers for value function approximation", "publication_date": "2021-11-19", "reason": "This paper establishes fundamental limitations of offline RL with function approximation, providing crucial context for the current work's positive result."}, {"fullname_first_author": "G. Weisz", "paper_title": "Online RL in linearly Q-realizable MDPs is as easy as in linear MDPs if you learn what to ignore", "publication_date": "2023", "reason": "This paper demonstrates that linear MDPs can approximate linearly Q-realizable MDPs, a key technique used in the current work to achieve statistical efficiency."}, {"fullname_first_author": "A. Zanette", "paper_title": "Learning near optimal policies with low inherent Bellman error", "publication_date": "2020", "reason": "This paper introduces the ELEANOR algorithm for efficient online RL, which is adapted in the current work for the offline setting."}, {"fullname_first_author": "T. Xie", "paper_title": "Batch value-function approximation with only realizability", "publication_date": "2021", "reason": "This paper studies batch value-function approximation under realizability, providing related work and background for the current work's approach."}]}