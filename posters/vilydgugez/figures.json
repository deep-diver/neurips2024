[{"figure_path": "VIlyDguGEz/figures/figures_3_1.jpg", "caption": "Figure 1: System diagram of the proposed model editing method.", "description": "The figure illustrates the proposed model editing method for Vision Transformers (ViTs). It shows the workflow of locate-then-edit approach. First, the where-to-edit challenge is addressed by meta-learning a hypernetwork on CutMix-augmented data. This hypernetwork generates binary masks which identify a sparse subset of structured model parameters. Then, the how-to-edit problem is solved by fine-tuning the identified parameters using gradient descent.  The ViT is used as a feature extractor and a base model. The hypernetwork takes the output of the ViT's classification head as input and generates the binary mask. The binary mask is then used to select the parameters that will be fine-tuned. The fine-tuned parameters are then used to update the base model.", "section": "3 Learning Where to Edit ViTs"}, {"figure_path": "VIlyDguGEz/figures/figures_5_1.jpg", "caption": "Figure 1: System diagram of the proposed model editing method.", "description": "This figure shows a detailed flowchart of the proposed model editing method.  It illustrates the process of using a hypernetwork trained on CutMix-augmented data to generate binary masks indicating which parameters in the Vision Transformer (ViT) should be modified. The system uses a locate-then-edit approach; the hypernetwork identifies the location ('where') and then simple fine-tuning adjusts the identified parameters ('how'). The resulting edited ViT model is expected to improve prediction accuracy while maintaining generalization and locality.", "section": "3 Learning Where to Edit ViTs"}, {"figure_path": "VIlyDguGEz/figures/figures_5_2.jpg", "caption": "Figure 2: The left subfigure shows representative editing examples, highlighting the predictive errors of the base ViT when predicting volleyball as basketball. The right subfigure depicts the generalization and locality trade-offs when editing different groups of FFNs or MSAs in the base ViT. It is evident that editing the 8-th to 10-th FFNs achieves the optimal Pareto front.", "description": "This figure shows the results of editing experiments conducted to determine the optimal location for editing within the ViT model.  The left side displays examples of the model's errors in classifying images, particularly confusing volleyball and basketball. The right side presents a Pareto-optimal curve.  This curve illustrates the trade-off between the generalization ability of the model (ability to correctly classify similar images after editing) and locality (how much the edits affect unrelated parts of the model). The results suggest that editing the 8th to 10th feed-forward networks (FFNs) offers the best balance between these two objectives.", "section": "3 Model Editing at Training Time: Where-to-edit"}, {"figure_path": "VIlyDguGEz/figures/figures_6_1.jpg", "caption": "Figure 3: Visual examples seen by the base ViT/B-16 during pre-training, contrasted with visual examples in the proposed editing benchmark as predictive errors of the base ViT/B-16.", "description": "This figure shows examples of images used during pre-training and those misclassified by the base ViT model in the proposed editing benchmark. The pre-training examples show volleyball and basketball players, and shovels and paddles. The misclassified examples from the benchmark show instances where the ViT model incorrectly classified volleyball as basketball and shovels as paddles.  This highlights the subpopulation shifts introduced in the benchmark, showcasing where the pre-trained model fails.", "section": "Editing Benchmark with Subpopulation Shifts"}, {"figure_path": "VIlyDguGEz/figures/figures_7_1.jpg", "caption": "Figure 4: Editing results for ViT/B-16 on the proposed benchmark.", "description": "This figure presents the Pareto front between generalization and locality achieved by different model editing methods on the proposed benchmark using ViT-B-16. The x-axis represents the locality rate (LR), and the y-axis represents the generalization rate (GR). Each curve represents a different method, showing the trade-off between generalization and locality. The proposed method achieves the best Pareto front, indicating superior performance in both generalization and locality compared to other methods.", "section": "5 Experiments"}, {"figure_path": "VIlyDguGEz/figures/figures_8_1.jpg", "caption": "Figure 5: Ablation results of the hypernetwork for ViT/B-16.", "description": "This figure demonstrates the ablation study on the hypernetwork for ViT/B-16.  Specifically, it compares the performance of the proposed method against fine-tuning with L1 and L2 regularization, and random masking, all at various sparsity levels. It shows that the proposed method achieves significantly better results than random masking, particularly at lower sparsity levels, indicating the importance of effective parameter localization.  The second graph shows the effect of using multiple samples for editing. ", "section": "3.3 Optimization Challenges"}, {"figure_path": "VIlyDguGEz/figures/figures_9_1.jpg", "caption": "Figure 1: System diagram of the proposed model editing method.", "description": "This figure shows a flowchart of the model editing process. It starts with CutMix data augmentation, which generates pseudo-samples for training a hypernetwork. The hypernetwork meta-learns to generate binary masks that identify a sparse subset of structured model parameters, responsive to real-world failure samples. Then, the identified parameters are fine-tuned using gradient descent to achieve successful edits.  The process is composed of two phases: \"where-to-edit\" and \"how-to-edit\", where the \"where-to-edit\" phase is focused on using a hypernetwork to learn to select important parameters and \"how-to-edit\" phase uses gradient descent to fine-tune these parameters.", "section": "3 Learning Where to Edit ViTs"}, {"figure_path": "VIlyDguGEz/figures/figures_9_2.jpg", "caption": "Figure 6: Mask specificity results.", "description": "This figure shows the specificity of the parameters identified by the hypernetwork for different editing samples.  It visualizes the Intersection over Union (IoU) of the corresponding binary masks at the 0.95 sparsity level for samples within and outside the same groups in the natural image subset.  High IoU values within the same group indicate that the hypernetwork identifies parameters relevant to specific error corrections, while low IoU values between groups demonstrate its ability to avoid affecting unrelated parts of the model. This effectively balances generalization and locality.", "section": "3.3 Optimization Challenges"}, {"figure_path": "VIlyDguGEz/figures/figures_15_1.jpg", "caption": "Figure A: Visual examples in each group of the natural image subset. Part 1/2.", "description": "This figure shows visual examples of the natural image subset used in the editing benchmark.  The images are grouped by the prediction discrepancies of two different classifiers. Each group shows images where the base ViT model (Vision Transformer) makes errors, categorized by the stronger model's prediction and the base ViT's incorrect prediction.  This illustrates the types of subpopulation shifts used to create the benchmark. Part 1 of 2.", "section": "4 Editing Benchmark with Subpopulation Shifts"}, {"figure_path": "VIlyDguGEz/figures/figures_16_1.jpg", "caption": "Figure 3: Visual examples seen by the base ViT/B-16 during pre-training, contrasted with visual examples in the proposed editing benchmark as predictive errors of the base ViT/B-16.", "description": "This figure shows a comparison of images used for pre-training the ViT model versus examples where the model failed in the proposed editing benchmark.  The pre-training images show diverse, well-represented object categories, while the benchmark images highlight subpopulation shifts (underrepresented natural images and AI-generated images) that cause the pre-trained model to make incorrect predictions. This demonstrates the limitations of pre-trained ViTs and motivates the need for the model editing technique proposed in the paper.", "section": "4 Editing Benchmark with Subpopulation Shifts"}, {"figure_path": "VIlyDguGEz/figures/figures_17_1.jpg", "caption": "Figure C: Visual examples of the AI-generated oil painting images.", "description": "This figure shows visual examples of images generated by an AI model with an oil painting style.  The images are part of the AI-generated image subset in the editing benchmark, used to test the model's ability to generalize to this specific style. Each image displays four versions, showing variations within the same class.", "section": "4 Editing Benchmark with Subpopulation Shifts"}, {"figure_path": "VIlyDguGEz/figures/figures_17_2.jpg", "caption": "Figure D: Visual examples of the AI-generated stage light images.", "description": "This figure shows visual examples of AI-generated images with a lighting condition shift (i.e., stage light) produced by PUG. The lighting condition shift is one of the subpopulation shifts in the editing benchmark for pre-trained vision transformers proposed in the paper.  The images are generated by text-to-image generative models and used to reveal the limitations of pre-trained ViTs for object recognition.", "section": "4 Editing Benchmark with Subpopulation Shifts"}, {"figure_path": "VIlyDguGEz/figures/figures_18_1.jpg", "caption": "Figure 1: System diagram of the proposed model editing method.", "description": "This figure shows a detailed illustration of the proposed model editing method, which is broken down into two subproblems: where-to-edit and how-to-edit.  The diagram illustrates the workflow, starting with CutMix augmentation on input images, and proceeding through the hypernetwork, feature extraction with ViT, the inner-loop optimization, the outer-loop optimization, a binarization step, and finally, the classification head. The parameters identified by the hypernetwork and updated in the model are clearly highlighted in the diagram.", "section": "3 Learning Where to Edit ViTs"}, {"figure_path": "VIlyDguGEz/figures/figures_19_1.jpg", "caption": "Figure 4: Editing results for ViT/B-16 on the proposed benchmark.", "description": "This figure displays the Pareto front between generalization and locality achieved by different model editing methods applied to the Vision Transformer (ViT/B-16) on the proposed benchmark. The benchmark comprises subpopulation shifts towards natural underrepresented images and AI-generated images.  The results showcase the superior performance of the proposed method, achieving the best trade-off between generalization (extending edits to similar samples) and locality (minimizing unintended effects on unrelated samples).", "section": "5 Experiments"}, {"figure_path": "VIlyDguGEz/figures/figures_19_2.jpg", "caption": "Figure 4: Editing results for ViT/B-16 on the proposed benchmark.", "description": "This figure presents the generalization-locality trade-off of various model editing methods applied to Vision Transformer (ViT) model ViT/B-16.  The x-axis represents the locality rate (LR), indicating the model's performance on unrelated samples after editing, and the y-axis shows the generalization rate (GR), assessing the model's performance on neighboring samples.  Each curve represents a different model editing method, highlighting the balance they achieve between generalization and locality. The Pareto front illustrates the best trade-off between these two criteria. The results are obtained from a proposed benchmark that evaluates editing performance on two subpopulation shifts: natural underrepresented images and AI-generated images.", "section": "5 Experiments"}, {"figure_path": "VIlyDguGEz/figures/figures_20_1.jpg", "caption": "Figure 4: Editing results for ViT/B-16 on the proposed benchmark.", "description": "This figure presents the generalization-locality trade-off curves for various model editing methods applied to the ViT-B/16 model on the proposed benchmark.  The benchmark evaluates performance on three subsets: natural images, AI-generated oil paintings, and AI-generated images with stage lighting. The curves show the relationship between generalization rate (GR) and locality rate (LR) for each method, illustrating the balance between editing successfully to neighboring samples and minimizing unwanted impact on other samples.  The results highlight that the proposed method achieves the best Pareto front (optimal balance between generalization and locality).", "section": "5 Experiments"}, {"figure_path": "VIlyDguGEz/figures/figures_20_2.jpg", "caption": "Figure H: Training curves of the hypernetwork.", "description": "The figure shows the training curves of the hypernetwork, plotting the mask sparsity and the outer-loop KL divergence against the number of iterations.  It illustrates how the mask sparsity increases rapidly at the beginning of training before stabilizing, while the outer-loop KL divergence decreases, suggesting that the hypernetwork effectively learns to locate key parameters for successful edits.", "section": "C.4 Ablation Studies"}, {"figure_path": "VIlyDguGEz/figures/figures_20_3.jpg", "caption": "Figure 6: Mask specificity results.", "description": "This figure demonstrates the specificity of the parameters identified by the hypernetwork for different editing samples.  (a) shows six representative editing examples from three different groups. (b) shows the Intersection over Union (IoU) of the corresponding binary masks at the 0.95 sparsity level for samples within and outside the same groups in the natural image subset.  The higher IoU values within groups indicate that the hypernetwork successfully identifies key parameters needed to correct specific errors, while excluding parameters related to unrelated samples. This demonstrates that the method effectively balances generalization and locality.", "section": "3.3 Optimization Challenges"}, {"figure_path": "VIlyDguGEz/figures/figures_21_1.jpg", "caption": "Figure 4: Editing results for ViT/B-16 on the proposed benchmark.", "description": "This figure shows the Pareto front between generalization and locality achieved by different model editing methods on the proposed benchmark using ViT-B-16. The x-axis represents the locality rate (LR), and the y-axis represents the generalization rate (GR). Each curve represents a different method, with the proposed method achieving the best Pareto front.", "section": "5 Experiments"}, {"figure_path": "VIlyDguGEz/figures/figures_21_2.jpg", "caption": "Figure 5: Ablation results of the hypernetwork for ViT/B-16.", "description": "This figure shows the ablation study results on the hypernetwork for ViT-B-16. It compares the performance of the proposed method with random masking, one-sample, two-samples, and three-samples editing. The left graph (a) shows the localization effectiveness, which compares the generalization (GR) and locality (LR) of the different methods.  The right graph (b) shows how the performance changes with more editing samples.", "section": "5 Experiments"}, {"figure_path": "VIlyDguGEz/figures/figures_21_3.jpg", "caption": "Figure F: Editing results for ViT/S-16 on the proposed benchmark.", "description": "The figure shows the generalization (GR) and locality (LR) trade-off curves for different model editing methods applied to ViT/S-16. It demonstrates the performance of various methods on the three subsets of the proposed editing benchmark: natural image subset, AI oil painting subset, and AI stage light subset. The results show that the proposed method achieves the best Pareto front between generalization and locality, outperforming other existing methods.", "section": "5 Experiments"}, {"figure_path": "VIlyDguGEz/figures/figures_21_4.jpg", "caption": "Figure 4: Editing results for ViT/B-16 on the proposed benchmark.", "description": "This figure shows the Pareto front between generalization and locality for different model editing methods applied to ViT/B-16 on the proposed benchmark. Each curve represents a different method, and the x-axis represents the locality rate (LR), while the y-axis represents the generalization rate (GR). The Pareto front is the set of points such that no other point has a better generalization rate and locality rate simultaneously. The figure shows that the proposed method achieves the best Pareto front among all methods, indicating that it achieves the best trade-off between generalization and locality.", "section": "5 Experiments"}, {"figure_path": "VIlyDguGEz/figures/figures_22_1.jpg", "caption": "Figure 5: Ablation results of the hypernetwork for ViT/B-16.", "description": "This figure shows the ablation study on the hypernetwork's performance by comparing the generalization and locality trade-offs when using the proposed method against using random masking and different numbers of training samples.  The results highlight the importance of the learned hypernetwork in achieving a good balance between generalization and locality in model editing for Vision Transformers.", "section": "5 Experiments"}]