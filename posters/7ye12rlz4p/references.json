{"references": [{"fullname_first_author": "Geoffrey E Hinton", "paper_title": "How to represent part-whole hierarchies in a neural network", "publication_date": "2023-03-01", "reason": "This paper is foundational to the core architecture of APM, providing the theoretical basis for its asynchronous patch processing and semantic clustering."}, {"fullname_first_author": "Yoshua Bengio", "paper_title": "The consciousness prior", "publication_date": "2017-09-01", "reason": "This philosophical paper offers a high-level perspective on consciousness and AI, which is relevant to the paper's broader implications regarding efficient learning and perception."}, {"fullname_first_author": "Alexey Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2020-10-01", "reason": "This work is a significant contribution to the field of computer vision, influencing APM's use of transformers and the approach to handling high-dimensional image data."}, {"fullname_first_author": "Yossi Gandelsman", "paper_title": "Test-time training with masked autoencoders", "publication_date": "2022-12-01", "reason": "This work directly addresses the problem of test-time training, a key focus of APM, making it an important comparative benchmark for evaluating APM's performance."}, {"fullname_first_author": "Yu Sun", "paper_title": "Test-time training with self-supervision for generalization under distribution shifts", "publication_date": "2020-06-01", "reason": "This work focuses on test-time training under distribution shifts, addressing a crucial challenge also tackled by APM.  It provides a crucial comparative baseline for evaluation."}]}