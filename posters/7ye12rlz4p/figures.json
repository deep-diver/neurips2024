[{"figure_path": "7Ye12RLZ4P/figures/figures_1_1.jpg", "caption": "Figure 1: (i) Asynchronous Perception Machine (APM): An image I passes through a column module and routes to a trigger column Ti. Ti then unfolds and generates h \u00d7 w location-specific queries. These queries are i.i.d and can be parallelized across cores of a gpu [53]. Each query Ti is passed through a shared MLP and yields the vector fry and frgb. MLP is queried iteratively until whole grid f' comes into existence. Classification then involves comparing the averaged representation f with class-specific textual representations in the contrastive space. (ii) Folded State: The parameters which the net learns are parameters of T and MLP. (iii) Unfolded State: T expands to yield h \u00d7 w queries \u2018on-the-fly'. Learning involves oscillating this net between folded and unfolded states. This net can be trained via backpropogation [84, 33].", "description": "This figure illustrates the architecture of the Asynchronous Perception Machine (APM).  Panel (i) shows the overall process, where an input image is processed sequentially through a column module and MLP to generate location-specific feature vectors. These vectors are then averaged and compared to textual representations for classification. Panel (ii) highlights the folded state of the network (parameters of T and MLP), while panel (iii) shows the unfolded state (h x w location-specific queries).  The APM alternates between these folded and unfolded states during training and inference.", "section": "3 Asynchronous Perception Machine (APM)"}, {"figure_path": "7Ye12RLZ4P/figures/figures_5_1.jpg", "caption": "Figure 2: APM's analysis with variable number of patches: (left) Gflops of CLIP VIT-B/16 and APM as a function of number of processed patches. (right) Feed-forward time vs number of patches.", "description": "This figure demonstrates APM's computational efficiency by comparing it against CLIP VIT-B/16 across varying numbers of image patches. The left panel shows the total number of GFLOPs (floating-point operations) required for processing, highlighting how APM's patch-based processing significantly reduces computational cost compared to CLIP VIT-B/16's parallel processing.  The right panel illustrates the inference time for each method. Although APM might take slightly longer for a smaller number of patches, it significantly outperforms CLIP VIT-B/16 when processing many patches, emphasizing its scalability.", "section": "4 Experimenting with APM"}, {"figure_path": "7Ye12RLZ4P/figures/figures_5_2.jpg", "caption": "Figure 3: Overfitting on a single distilled token representation leads to islands of agreement[34]: APM is overfit on a test-sample's representation distilled from a teacher. We plot t-sne clustering of output features over 250ttt iterations. L2 loss between predicted features and distilled sample falls from le-3 to le-12. Moving left to right shows that wholes break into smaller parts.", "description": "This figure visualizes the process of overfitting on a single distilled token representation using t-SNE clustering.  It shows how, over 250 test-time training iterations, the initially holistic representation of the input image breaks down into smaller, more distinct parts, which are interpreted as islands of agreement. The L2 loss between the predicted and distilled features decreases significantly over these iterations, indicating successful overfitting.", "section": "3 Asynchronous Perception Machine (APM)"}, {"figure_path": "7Ye12RLZ4P/figures/figures_6_1.jpg", "caption": "Figure 4: RGB Decoding in APM: Input trigger column Tij is concatenated with predicted feature fij and fed to downstream RGB head. This decodes RGB logit at location (i,j) for any 2D input Xk. (ii) Input xk sampled from Coco-val set. RGBout: reconstructed RGB, fout: Predicted feature grid.", "description": "This figure illustrates how the Asynchronous Perception Machine (APM) reconstructs RGB values from an input image. It shows that the trigger column (Tij), containing an image's identity, is combined with predicted features (fij) and fed to an RGB head to produce reconstructed RGB values (RGBout) and a feature grid (fout).  The input image (xk) is from COCO-val dataset. This demonstrates APM's ability to reconstruct RGB values directly from its internal representation, showcasing its efficiency and capacity for low-level perceptual tasks. The skip connection between the trigger column and the output layer helps break symmetry and improve RGB reconstruction.", "section": "5.1 APM can do RGB reconstruction for any 2D input."}, {"figure_path": "7Ye12RLZ4P/figures/figures_7_1.jpg", "caption": "Figure 5: APM feature Analysis: (i) TTT iterations on an input image leads to semantically aware clustering. top: 2D t-sNE. bottom: 3D t-sNE. [70, 34]. (ii) APM is trained via self-supervision using DINOv2-Teacher. (from left) Input, Dinov2 grid, APM grid. APM's grid closely approximates Dinov2 grid evident from black regions in error map. Note that APM does asynchronous patch-based processing whereas Dinov2 does parallel perception. (iii) Cifar-10 samples feed-forwarded through SSL-trained APM yields features of significant semantic quality.[34]", "description": "This figure shows the results of applying three different methods (TTT, SSL-trained, and inference) to image data using the Asynchronous Perception Machine (APM).  The top row demonstrates that test-time training (TTT) with APM leads to semantically meaningful clustering of features in the image. The middle row compares the features extracted by APM to those extracted by DINOv2 after self-supervised training, showing a close approximation. The bottom row shows the semantically-aware features obtained through inference with a self-supervised trained APM.", "section": "APM feature Analysis"}, {"figure_path": "7Ye12RLZ4P/figures/figures_7_2.jpg", "caption": "Figure 6: APM is a step towards validating GLOM's insight [34]: input percept is a field. An interpolation between any two images in the wild. This field arises in APM's MLP consisting of 5 layers. Trigger column T acts as a key which retrieves an image from the APM's memory. T resides in a continuous embedding space, not discrete addressing space.", "description": "This figure demonstrates the ability of APM to interpolate between two images.  The trigger column, T, acts as a key to retrieve images from APM's internal representation, a continuous embedding space rather than discrete addressing. This interpolation highlights the concept of the input percept as a field, a core tenet of GLOM.", "section": "5.3 APM is a step towards validating GLOM's insight: input percept is a field[41]"}, {"figure_path": "7Ye12RLZ4P/figures/figures_18_1.jpg", "caption": "Figure 1: (i) Asynchronous Perception Machine (APM): An image I passes through a column module and routes to a trigger column T<sub>i</sub>. T<sub>i</sub> then unfolds and generates h \u00d7 w location-specific queries. These queries are i.i.d and can be parallelized across cores of a gpu [53]. Each query T<sub>i</sub> is passed through a shared MLP and yields the vector f<sub>xy</sub> and f<sub>rgb</sub>. MLP is queried iteratively until whole grid f' comes into existence. Classification then involves comparing the averaged representation f with class-specific textual representations in the contrastive space. (ii) Folded State: The parameters which the net learns are parameters of T and MLP. (iii) Unfolded State: T expands to yield h \u00d7 w queries \u2018on-the-fly\u2019. Learning involves oscillating this net between folded and unfolded states. This net can be trained via backpropogation [84, 33].", "description": "This figure illustrates the architecture of the Asynchronous Perception Machine (APM).  Panel (i) shows the overall process: an image is processed sequentially by a column module, which generates location-specific queries. These queries are processed through a shared MLP, resulting in a feature grid.  Finally, a classifier compares the averaged feature representation with textual representations for classification. Panel (ii) depicts the 'folded' state of the network, showing only the learned parameters (T and MLP weights). Panel (iii) shows the 'unfolded' state, illustrating the generation of multiple queries from a single column during operation. The network's training involves switching between these folded and unfolded states using backpropagation.", "section": "3 Asynchronous Perception Machine (APM)"}, {"figure_path": "7Ye12RLZ4P/figures/figures_23_1.jpg", "caption": "Figure 7: Cifar 10 islands: Individual part-wholes are clearly observed in APM features. These features are used for downstream classification. We leverage the visualization mechanism by [70]. These islands are not been hand-picked.[34]", "description": "This figure visualizes the results of applying the proposed Asynchronous Perception Machine (APM) model on CIFAR-10 dataset. The image shows a collection of 20 image patches extracted from the dataset, with each patch accompanied by the corresponding feature map generated by APM.  The visualization technique used is t-SNE, which projects the high-dimensional feature maps into a 2D space for visualization. The resulting visualization reveals distinct clusters of features corresponding to different parts and wholes of objects present in the image patches.  The caption highlights that these clusters or \"islands\" emerge organically without any manual selection or pre-processing of the data, demonstrating the model's ability to capture meaningful, semantically-aware feature representations.", "section": "5.3 APM is a step towards validating GLOM's insight: input percept is a field[41]"}, {"figure_path": "7Ye12RLZ4P/figures/figures_24_1.jpg", "caption": "Figure 1: (i) Asynchronous Perception Machine (APM): An image I passes through a column module and routes to a trigger column T<sub>i</sub>. T<sub>i</sub> then unfolds and generates h \u00d7 w location-specific queries. These queries are i.i.d and can be parallelized across cores of a gpu [53]. Each query T<sub>i</sub> is passed through a shared MLP and yields the vector f<sub>xy</sub> and f<sub>rgb</sub>. MLP is queried iteratively until whole grid f\u2019 comes into existence. Classification then involves comparing the averaged representation f with class-specific textual representations in the contrastive space. (ii) Folded State: The parameters which the net learns are parameters of T and MLP. (iii) Unfolded State: T expands to yield h \u00d7 w queries \u2018on-the-fly\u2019. Learning involves oscillating this net between folded and unfolded states. This net can be trained via backpropogation [84, 33].", "description": "This figure shows the architecture of Asynchronous Perception Machine (APM). It is composed of three parts: (i) APM architecture, which shows how an image is processed asynchronously through a column module and MLPs to generate location-specific queries; (ii) Folded state, showing that the learnable parameters are those of the trigger column and MLP; and (iii) Unfolded state, illustrating the expansion of the trigger column into multiple location-aware queries. The process involves oscillating between folded and unfolded states for training using backpropagation.", "section": "3 Asynchronous Perception Machine (APM)"}, {"figure_path": "7Ye12RLZ4P/figures/figures_26_1.jpg", "caption": "Figure 1: (i) Asynchronous Perception Machine (APM): An image I passes through a column module and routes to a trigger column T<sub>i</sub>. T<sub>i</sub> then unfolds and generates h \u00d7 w location-specific queries. These queries are i.i.d and can be parallelized across cores of a gpu [53]. Each query T<sub>i</sub> is passed through a shared MLP and yields the vector f<sub>xy</sub> and f<sub>rgb</sub>. MLP is queried iteratively until whole grid f' comes into existence. Classification then involves comparing the averaged representation f with class-specific textual representations in the contrastive space. (ii) Folded State: The parameters which the net learns are parameters of T and MLP. (iii) Unfolded State: T expands to yield h \u00d7 w queries \u2018on-the-fly\u2019. Learning involves oscillating this net between folded and unfolded states. This net can be trained via backpropogation [84, 33].", "description": "This figure shows the architecture of the Asynchronous Perception Machine (APM).  Panel (i) illustrates the overall process, where an image is processed asynchronously patch by patch via a column module and MLP. Panel (ii) shows the 'folded state' of the network (its learnable parameters), while panel (iii) displays the 'unfolded state' representing the expanded set of queries processed during training.", "section": "3 Asynchronous Perception Machine (APM)"}, {"figure_path": "7Ye12RLZ4P/figures/figures_27_1.jpg", "caption": "Figure 7: Cifar 10 islands: Individual part-wholes are clearly observed in APM features. These features are used for downstream classification. We leverage the visualization mechanism by [70]. These islands are not been hand-picked[34]", "description": "This figure visualizes features extracted from the APM model trained on CIFAR-10.  Each image shows an input image alongside its corresponding feature representation generated by the APM. The feature maps are visualized using a t-SNE dimensionality reduction technique, which reveals distinct clusters (islands) of features corresponding to different parts of the objects in the input images.  The visualization highlights the model's ability to capture semantically meaningful features by clustering related parts of objects together, demonstrating the model's capacity for part-whole representation learning.  Importantly, these clusters were not manually selected, indicating that APM's feature representation naturally forms these semantically relevant groupings.", "section": "5.1 APM can do RGB reconstruction for any 2D input."}, {"figure_path": "7Ye12RLZ4P/figures/figures_28_1.jpg", "caption": "Figure 7: Cifar 10 islands: Individual part-wholes are clearly observed in APM features. These features are used for downstream classification. We leverage the visualization mechanism by [70]. These islands are not been hand-picked.[34]", "description": "This figure visualizes the features learned by the Asynchronous Perception Machine (APM) on the CIFAR-10 dataset.  Each image shows a different object from CIFAR-10, with its corresponding feature representation generated by APM.  The color patterns within the feature representations highlight distinct parts and sub-parts within the objects, demonstrating the model's ability to learn and represent part-whole relationships.  The caption emphasizes that these representations, referred to as \"islands\", are not hand-picked but emerge naturally from the model's learning process.  This visualization uses a technique from [70] to project the high-dimensional features into a 2D space while preserving spatial relationships.", "section": "5.1 APM can do RGB reconstruction for any 2D input."}, {"figure_path": "7Ye12RLZ4P/figures/figures_29_1.jpg", "caption": "Figure 1: (i) Asynchronous Perception Machine (APM): An image I passes through a column module and routes to a trigger column T<sub>i</sub>. T<sub>i</sub> then unfolds and generates h \u00d7 w location-specific queries. These queries are i.i.d and can be parallelized across cores of a gpu [53]. Each query T<sub>i</sub> is passed through a shared MLP and yields the vector f<sub>xy</sub> and f<sub>rgb</sub>. MLP is queried iteratively until whole grid f' comes into existence. Classification then involves comparing the averaged representation f with class-specific textual representations in the contrastive space. (ii) Folded State: The parameters which the net learns are parameters of T and MLP. (iii) Unfolded State: T expands to yield h \u00d7 w queries \u2018on-the-fly\u2019. Learning involves oscillating this net between folded and unfolded states. This net can be trained via backpropogation [84, 33].", "description": "This figure illustrates the architecture of the Asynchronous Perception Machine (APM), showing how it processes an image asynchronously, patch by patch, using a column module and MLPs.  It highlights the folded and unfolded states of the network, crucial to the training process, and the final classification stage which compares averaged representations to textual representations.", "section": "3 Asynchronous Perception Machine (APM)"}, {"figure_path": "7Ye12RLZ4P/figures/figures_30_1.jpg", "caption": "Figure 1: (i) Asynchronous Perception Machine (APM): An image I passes through a column module and routes to a trigger column T<sub>i</sub>. T<sub>i</sub> then unfolds and generates h \u00d7 w location-specific queries. These queries are i.i.d and can be parallelized across cores of a gpu [53]. Each query T<sub>i</sub> is passed through a shared MLP and yields the vector f<sub>xy</sub> and f<sub>rgb</sub>. MLP is queried iteratively until whole grid f' comes into existence. Classification then involves comparing the averaged representation f with class-specific textual representations in the contrastive space. (ii) Folded State: The parameters which the net learns are parameters of T and MLP. (iii) Unfolded State: T expands to yield h \u00d7 w queries \u2018on-the-fly\u2019. Learning involves oscillating this net between folded and unfolded states. This net can be trained via backpropogation [84, 33].", "description": "This figure shows the architecture of the Asynchronous Perception Machine (APM).  It illustrates how an image is processed asynchronously, one patch at a time, using a column module and MLPs. The figure highlights the folded and unfolded states of the network during the training process.  It also visually represents the process of querying the MLP iteratively to generate a complete feature grid.  Finally, it shows how classification is performed by comparing the averaged representation to textual representations.", "section": "3 Asynchronous Perception Machine (APM)"}, {"figure_path": "7Ye12RLZ4P/figures/figures_31_1.jpg", "caption": "Figure 1: (i) Asynchronous Perception Machine (APM): An image I passes through a column module and routes to a trigger column T<sub>i</sub>. T<sub>i</sub> then unfolds and generates h \u00d7 w location-specific queries. These queries are i.i.d and can be parallelized across cores of a gpu [53]. Each query T<sub>i</sub> is passed through a shared MLP and yields the vector f<sub>xy</sub> and f<sub>rgb</sub>. MLP is queried iteratively until whole grid f' comes into existence. Classification then involves comparing the averaged representation f with class-specific textual representations in the contrastive space. (ii) Folded State: The parameters which the net learns are parameters of T and MLP. (iii) Unfolded State: T expands to yield h \u00d7 w queries \u2018on-the-fly'. Learning involves oscillating this net between folded and unfolded states. This net can be trained via backpropogation [84, 33].", "description": "This figure illustrates the architecture of the Asynchronous Perception Machine (APM).  Panel (i) shows the overall process flow, highlighting the sequential querying of MLPs to process image patches asynchronously. The system alternates between a 'folded' state (ii), where parameters are learned, and an 'unfolded' state (iii), where h x w location-specific queries are generated on-the-fly. This dynamic unfolding and folding process allows for efficient test-time training.", "section": "3 Asynchronous Perception Machine (APM)"}]