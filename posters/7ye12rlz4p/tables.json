[{"figure_path": "7Ye12RLZ4P/tables/tables_4_1.jpg", "caption": "Table 1: APM's Robustness to Natural Distribution Shifts. CoOp and CoCoOp are tuned on ImageNet using 16-shot training data per category. Baseline CLIP, prompt ensemble, TPT and our APM do not require training data. A \u2713 in P means that method leveraged pre-trained weights on clean variant of train set aka, Image-net and downstream-ttt on corrupted version.", "description": "This table presents the robustness of different methods, including the proposed Asynchronous Perception Machine (APM), against natural distribution shifts.  It compares the top-1 accuracy of various methods across several ImageNet datasets (ImageNet, ImageNet-A, ImageNet-V2, ImageNet-R, ImageNet-Sketch), which represent different types of distribution shifts (adversarial, corrupted, real-world, sketches). The table indicates whether each method used pre-trained weights and shows that APM performs competitively with or even better than other methods, even without pre-training or specific data augmentation techniques.", "section": "4 Experimenting with APM"}, {"figure_path": "7Ye12RLZ4P/tables/tables_4_2.jpg", "caption": "Table 2: APM's performance on ImageNet-C, level 5. The first three rows are fixed models without test-time training. The third row, ViT probing, is the baseline used in [17]. A \u2713 in P means that method leveraged pre-trained weights on clean variant of train set aka, Image-net and downstream-ttt on corrupted version.", "description": "This table presents the performance of APM and several baseline models on ImageNet-C, a dataset with 15 types of image corruptions at 5 severity levels.  The baseline models are not using test time training.  It shows the robustness of the different methods to the image corruptions.  APM's performance is compared to others both with and without pre-trained weights.", "section": "4 Experimenting with APM"}, {"figure_path": "7Ye12RLZ4P/tables/tables_5_1.jpg", "caption": "Table 1: APM's Robustness to Natural Distribution Shifts. CoOp and CoCoOp are tuned on ImageNet using 16-shot training data per category. Baseline CLIP, prompt ensemble, TPT and our APM do not require training data. A \u2713 in P means that method leveraged pre-trained weights on clean variant of train set aka, Image-net and downstream-ttt on corrupted version.", "description": "This table presents a comparison of the performance of various methods, including the proposed Asynchronous Perception Machine (APM), on ImageNet and several out-of-distribution (OOD) datasets.  It highlights APM's ability to achieve competitive performance without requiring dataset-specific pre-training or augmentation. The 'P' column indicates whether a method used pre-trained weights on a clean ImageNet dataset before being tested on the OOD datasets. The table showcases APM's robustness to various types of image corruptions and distribution shifts.", "section": "4 Experimenting with APM"}, {"figure_path": "7Ye12RLZ4P/tables/tables_8_1.jpg", "caption": "Table 5: Ablations on APM. All nets except CLIP VIT-L/14 use random weights b) Tc: trigger column contains convolutions. Tvit: Trigger column contains a routed VIT representation. C-10: CIFAR-10, C-100: CIFAR-100. Accuracy is reported.", "description": "This ablation study investigates the impact of different design choices on APM's performance. It compares the accuracy of various models (including baselines like CLIP ViT-L/14, MLP, ResNet18, and ResNet34) on CIFAR-10 and CIFAR-100 datasets using different combinations of loss functions (Lgrid, Lcls, Lrgb) and trigger column configurations (Tc, Tvit). The results demonstrate the effectiveness of the proposed APM architecture and specific design choices in achieving high accuracy.", "section": "6 Ablations on APM"}, {"figure_path": "7Ye12RLZ4P/tables/tables_17_1.jpg", "caption": "Table 1: APM's Robustness to Natural Distribution Shifts. CoOp and CoCoOp are tuned on ImageNet using 16-shot training data per category. Baseline CLIP, prompt ensemble, TPT and our APM do not require training data. A \u2713 in P means that method leveraged pre-trained weights on clean variant of train set aka, Image-net and downstream-ttt on corrupted version.", "description": "This table compares the performance of APM against several other methods on various image classification datasets with natural distribution shifts.  The datasets include ImageNet, ImageNet-A, ImageNet-V2, ImageNet-R, and ImageNet-Sketch.  The table highlights APM's robustness by showing competitive results even without dataset-specific pre-training, augmentation or any pretext task, unlike methods that leverage pre-trained weights.", "section": "4 Experimenting with APM"}, {"figure_path": "7Ye12RLZ4P/tables/tables_19_1.jpg", "caption": "Table 1: APM's Robustness to Natural Distribution Shifts. CoOp and CoCoOp are tuned on ImageNet using 16-shot training data per category. Baseline CLIP, prompt ensemble, TPT and our APM do not require training data. A \u2713 in P means that method leveraged pre-trained weights on clean variant of train set aka, Image-net and downstream-ttt on corrupted version.", "description": "This table presents a comparison of the performance of various methods on different ImageNet datasets with varying levels of corruption or distribution shifts.  It highlights APM's robustness to these shifts, even without dataset-specific pre-training, augmentation, or pretext tasks, and its competitive performance against existing state-of-the-art methods. The 'P' column indicates whether pre-trained weights were used.", "section": "4 Experimenting with APM"}, {"figure_path": "7Ye12RLZ4P/tables/tables_19_2.jpg", "caption": "Table 1: APM's Robustness to Natural Distribution Shifts. CoOp and CoCoOp are tuned on ImageNet using 16-shot training data per category. Baseline CLIP, prompt ensemble, TPT and our APM do not require training data. A \u2713 in P means that method leveraged pre-trained weights on clean variant of train set aka, Image-net and downstream-ttt on corrupted version.", "description": "This table presents the performance comparison of different methods on various image classification tasks with natural distribution shifts. It includes baseline methods such as CLIP, prompt ensemble, and TPT, along with methods that leverage pre-trained weights. The table shows the top-1 accuracy for each method on ImageNet, ImageNet-A, ImageNet-V2, ImageNet-R, and ImageNet-Sketch.  The results demonstrate APM's robustness in handling distribution shifts, showing competitive performance to the state-of-the-art methods, while not requiring additional training data.", "section": "4 Experimenting with APM"}, {"figure_path": "7Ye12RLZ4P/tables/tables_20_1.jpg", "caption": "Table 2: APM's performance on ImageNet-C, level 5. The first three rows are fixed models without test-time training. The third row, ViT probing, is the baseline used in [17]. A \u2713 in P means that method leveraged pre-trained weights on clean variant of train set aka, Image-net and downstream-ttt on corrupted version. CLIP VIT-L/14 is generally more robust. APM does better on 11/15 noises with an average accuracy score of 50.3.", "description": "This table presents a comparison of different models' performance on ImageNet-C, a dataset with 15 types of image corruptions at level 5 severity.  The models include fixed models without test-time training (TTT), a ViT probing baseline, and the proposed APM model.  The table shows the accuracy achieved by each model for various noise types, highlighting APM's superior performance on 11 out of 15 noise types, with an overall average accuracy of 50.3%. The 'P' column indicates whether pre-trained weights were used.", "section": "4 Experimenting with APM"}, {"figure_path": "7Ye12RLZ4P/tables/tables_21_1.jpg", "caption": "Table 9: Ablation on APM parameter count on DTD dataset: Increasing the number of parameters to 53M improves APM's performance to 49.1 beyond which it starts to drop. Top 1 classification accuracy is being reported.", "description": "This table shows the results of an ablation study on the APM model, specifically focusing on the impact of varying the number of parameters in its linear layers. The experiment was conducted on the DTD dataset, and the results show that increasing the number of parameters from 7M to 53M leads to a gradual improvement in the top-1 classification accuracy, reaching a peak of 49.1%. However, further increases in the number of parameters beyond 53M result in a decrease in accuracy, indicating that the model starts to overfit.  The table provides a quantitative analysis of the optimal parameter count for APM on the DTD dataset.", "section": "4 Experimenting with APM"}, {"figure_path": "7Ye12RLZ4P/tables/tables_21_2.jpg", "caption": "Table 2: APM's performance on ImageNet-C, level 5. The first three rows are fixed models without test-time training. The third row, ViT probing, is the baseline used in [17]. A \u2713 in P means that method leveraged pre-trained weights on clean variant of train set aka, Image-net and downstream-ttt on corrupted version. CLIP VIT-L/14 is generally more robust. APM does better on 11/15 noises with an average accuracy score of 50.3.", "description": "This table presents a comparison of the performance of various methods, including APM, on ImageNet-C, which is a dataset with 15 types of image corruptions at level 5 severity.  The first three rows show results from models without test-time training.  The table highlights APM's superior performance compared to other methods, especially on a majority of the noise types, with an overall average accuracy of 50.3%.", "section": "Results and Analysis"}, {"figure_path": "7Ye12RLZ4P/tables/tables_22_1.jpg", "caption": "Table 2: APM's performance on ImageNet-C, level 5. The first three rows are fixed models without test-time training. The third row, ViT probing, is the baseline used in [17]. A \u2713 in P means that method leveraged pre-trained weights on clean variant of train set aka, Image-net and downstream-ttt on corrupted version. CLIP VIT-L/14 is generally more robust. APM does better on 11/15 noises with an average accuracy score of 50.3.", "description": "This table presents the performance comparison of APM against other methods on ImageNet-C dataset with corruption level 5.  It shows the accuracy of each method for each type of noise.  The table highlights APM's superior performance on 11 out of 15 noise types, demonstrating its robustness and achieving an average accuracy score of 50.3%.", "section": "4 Experimenting with APM"}, {"figure_path": "7Ye12RLZ4P/tables/tables_22_2.jpg", "caption": "Table 2: APM's performance on ImageNet-C, level 5. The first three rows are fixed models without test-time training. The third row, ViT probing, is the baseline used in [17]. A \u2713 in P means that method leveraged pre-trained weights on clean variant of train set aka, Image-net and downstream-ttt on corrupted version. CLIP VIT-L/14 is generally more robust. APM does better on 11/15 noises with an average accuracy score of 50.3.", "description": "This table presents a comparison of the performance of different models on ImageNet-C, specifically at level 5 corruption.  It shows the accuracy of several models (including the proposed APM) on various types of image noise.  The baseline model uses pre-trained weights. The results demonstrate APM's ability to outperform existing methods on a majority of noise types.", "section": "4 Experimenting with APM"}, {"figure_path": "7Ye12RLZ4P/tables/tables_22_3.jpg", "caption": "Table 2: APM's performance on ImageNet-C, level 5. The first three rows are fixed models without test-time training. The third row, ViT probing, is the baseline used in [17]. A \u2713 in P means that method leveraged pre-trained weights on clean variant of train set aka, Image-net and downstream-ttt on corrupted version. CLIP VIT-L/14 is generally more robust. APM does better on 11/15 noises with an average accuracy score of 50.3.", "description": "This table compares the performance of APM against other methods on ImageNet-C, level 5, a dataset with 15 types of image corruptions.  The results show the top-1 accuracy for each corruption type.  It highlights that while some methods use pre-trained weights, APM does not, yet still achieves competitive or superior performance.  The average accuracy across all corruptions demonstrates APM's robustness.", "section": "Results and Analysis"}, {"figure_path": "7Ye12RLZ4P/tables/tables_22_4.jpg", "caption": "Table 2: APM's performance on ImageNet-C, level 5. The first three rows are fixed models without test-time training. The third row, ViT probing, is the baseline used in [17]. A \u2713 in P means that method leveraged pre-trained weights on clean variant of train set aka, Image-net and downstream-ttt on corrupted version. CLIP VIT-L/14 is generally more robust. APM does better on 11/15 noises with an average accuracy score of 50.3.", "description": "This table compares the performance of APM against other methods on ImageNet-C, level 5, a dataset with 15 types of image corruptions.  It shows that APM outperforms several baselines on 11 out of 15 noise types.  The table highlights APM's robustness to image corruptions without relying on pre-trained weights or data augmentation.", "section": "4 Experimenting with APM"}]