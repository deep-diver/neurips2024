[{"figure_path": "0AwMciNShl/figures/figures_1_1.jpg", "caption": "Figure 1: (a) An illustration of our human evaluation protocol. (b) The annotation interface, wherein annotators choose the superior video based on provided evaluation metrics. (c) Instruction and examples to guide used to the \"Video Quality\" evaluation.", "description": "This figure provides a visual representation of the proposed human evaluation protocol, showing the workflow, annotation interface, and instructions/examples. Panel (a) illustrates the overall protocol, which consists of pre-annotation sorting, static annotation, and dynamic annotation modules, driven by an automatic scorer and refined by trained annotators. Panel (b) displays the annotation interface, where evaluators use provided metrics to compare video pairs and select a superior video, and panel (c) shows example questions and detailed analysis steps to aid the evaluators, illustrating how the process works.", "section": "1 Introduction"}, {"figure_path": "0AwMciNShl/figures/figures_8_1.jpg", "caption": "Figure 2: Scores and rankings of models across various dimensions for pre-training LRAs, AMT Annotators, and Post-training LRAs. Post-training LRAs (Dyn) refers to the annotation results of Post-training LRAs using the dynamic evaluation component.", "description": "This figure presents a comparison of the scores and rankings of five different text-to-video (T2V) models across six evaluation dimensions: Video Quality, Temporal Quality, Motion Quality, Text Alignment, Ethical Robustness, and Human Preference.  The scores are shown for three different groups of annotators: pre-training laboratory-recruited annotators (LRAs), Amazon Mechanical Turk (AMT) annotators, and post-training LRAs.  A fourth set of results is included showing the impact of the dynamic evaluation module on the post-training LRAs' evaluations.  The radar charts allow for a visual comparison of the relative strengths and weaknesses of each model across the different evaluation dimensions and annotator groups.", "section": "5 Human evaluation of existing models"}, {"figure_path": "0AwMciNShl/figures/figures_9_1.jpg", "caption": "Figure 3: The left figure shows how the number of annotations required for different protocols. The right figure represents model score estimations across different metrics. Each boxplot illustrates the median, interquartile range, and 95% confidence intervals of the estimates.", "description": "This figure compares the number of annotations needed for traditional pairwise comparison protocols versus the proposed protocol with a dynamic evaluation module.  The left panel shows a plot demonstrating the drastically reduced annotation requirements of the dynamic approach as the number of models increases. The right panel provides box plots showing score estimations across six different video quality metrics (Video Quality, Temporal Quality, Motion Quality, Text Alignment, Ethical Robustness, Human Preference) for five different models.  The box plots illustrate the median, interquartile range, and 95% confidence intervals of the estimates, showing the relative performance of each model across the different metrics.", "section": "5.3 Module validation"}, {"figure_path": "0AwMciNShl/figures/figures_27_1.jpg", "caption": "Figure 1: (a) An illustration of our human evaluation protocol. (b) The annotation interface, wherein annotators choose the superior video based on provided evaluation metrics. (c) Instruction and examples to guide used to the \"Video Quality\" evaluation.", "description": "This figure illustrates the proposed human evaluation protocol. (a) shows the overall workflow of the protocol, including pre-annotation sorting, static annotation, and dynamic annotation stages. (b) shows the annotation interface where human annotators compare video pairs and select the better one based on provided metrics. (c) provides detailed instructions and examples for annotators to assess \"Video Quality\".", "section": "4 Our protocol for text-to-vedio models"}, {"figure_path": "0AwMciNShl/figures/figures_28_1.jpg", "caption": "Figure 1: (a) An illustration of our human evaluation protocol. (b) The annotation interface, wherein annotators choose the superior video based on provided evaluation metrics. (c) Instruction and examples to guide used to the \"Video Quality\" evaluation.", "description": "This figure provides a visual overview of the proposed Text-to-Video Human Evaluation (T2VHE) protocol.  Panel (a) shows a flowchart illustrating the protocol's workflow: pre-annotation sorting using automated scores, static annotation, dynamic annotation (where only videos with significant score differences are annotated), and the final automatic scoring. Panel (b) displays the annotation interface, showing video pairs to compare and buttons for annotator selection. Panel (c) shows example prompts and detailed instructions/evaluation criteria to guide the annotators when evaluating \"Video Quality\" and other metrics.", "section": "4 Our protocol for text-to-video models"}, {"figure_path": "0AwMciNShl/figures/figures_29_1.jpg", "caption": "Figure 1: (a) An illustration of our human evaluation protocol. (b) The annotation interface, wherein annotators choose the superior video based on provided evaluation metrics. (c) Instruction and examples to guide used to the \"Video Quality\" evaluation.", "description": "This figure shows three aspects of the Text-to-Video Human Evaluation (T2VHE) protocol.  (a) illustrates the overall workflow, including pre-annotation sorting, static annotation, and dynamic annotation components. (b) displays the annotation interface that annotators use.  This interface presents pairs of videos and allows annotators to select the superior video according to the provided metrics. (c) gives examples of instructions and examples to guide annotators in using the protocol, specifically in relation to \"Video Quality\" evaluation.  These instructions emphasize how to assess realism and aesthetic appeal.", "section": "4 Our protocol for text-to-vedio models"}, {"figure_path": "0AwMciNShl/figures/figures_29_2.jpg", "caption": "Figure 1: (a) An illustration of our human evaluation protocol. (b) The annotation interface, wherein annotators choose the superior video based on provided evaluation metrics. (c) Instruction and examples to guide used to the \"Video Quality\" evaluation.", "description": "This figure provides a visual representation of the Text-to-Video Human Evaluation (T2VHE) protocol. Panel (a) shows a flowchart of the protocol's workflow. Panel (b) displays the interface used by human evaluators to compare videos based on various metrics. Panel (c) shows detailed instructions and examples provided to help evaluators make consistent and reliable judgments of video quality.", "section": "4 Our protocol for text-to-vedio models"}, {"figure_path": "0AwMciNShl/figures/figures_30_1.jpg", "caption": "Figure 1: (a) An illustration of our human evaluation protocol. (b) The annotation interface, wherein annotators choose the superior video based on provided evaluation metrics. (c) Instruction and examples to guide used to the \"Video Quality\" evaluation.", "description": "This figure provides a visual representation of the Text-to-Video Human Evaluation (T2VHE) protocol.  (a) shows the overall workflow, highlighting pre-annotation sorting, static and dynamic annotation stages, and the automatic scorer. (b) displays the user interface where annotators compare video pairs and make judgments based on provided metrics (Video Quality, Temporal Quality, Motion Quality, Text Alignment, Ethical Robustness, Human Preference). (c) gives specific instructions and example prompts to guide annotators in evaluating \"Video Quality\".  The figure illustrates the protocol's structured approach to ensure reliable and consistent human evaluations.", "section": "4 Our protocol for text-to-vedio models"}, {"figure_path": "0AwMciNShl/figures/figures_30_2.jpg", "caption": "Figure 9: The number of prompts corresponding to each category for different locations at the sorted video pairs.", "description": "This figure shows the cumulative number of prompts for each category at different points in the sorting process.  The x-axis represents the overall number of video pairs considered, while the y-axis shows the count of prompts from each category. The line graph illustrates how the number of prompts for each category increases as the sorting process progresses, reflecting their relative importance and difficulty in automated evaluation.  Categories with steeper curves indicate a higher frequency of occurrence earlier in the sorting process, suggesting they were more easily differentiated by the algorithm.", "section": "C Further Discussions"}]