[{"figure_path": "S93hrwT8u9/tables/tables_8_1.jpg", "caption": "Table 1: Experimental results on ImageNet-1k. \u201c#Layers\u201d refers to the number of fine-tuned convolutional layers (counted from the end of the model). Activation memory consumption is shown in MegaBytes (MB).", "description": "This table presents the classification performance and memory consumption for MobileNetV2, ResNet18, and ResNet34 models.  It shows the top-1 validation accuracy and peak/mean activation memory (in MB) for different fine-tuning setups (varying the number of fine-tuned layers): Vanilla training, Gradient Filter (with different patch sizes), SVD (with \u03b5 = 0.8 and \u03b5 = 0.9), and HOSVD (with \u03b5 = 0.8 and \u03b5 = 0.9).  The results demonstrate the trade-off between accuracy and memory consumption for different methods and the impact of fine-tuning different numbers of layers.", "section": "4.3 Main Results"}, {"figure_path": "S93hrwT8u9/tables/tables_9_1.jpg", "caption": "Table 2: Experimental results for semantic segmentation. mIoU is the mean Intersection over Union, and mAcc is the micro averaged accuracy.", "description": "This table presents the results of semantic segmentation experiments using different methods (Vanilla training, gradient filtering, SVD, and HOSVD) on various models (PSPNet, PSPNet-M, DLV3, DLV3-M, FCN, and UPerNet).  For each model and method, the mean Intersection over Union (mIoU), micro-averaged accuracy (mAcc), peak memory usage, and mean memory usage are reported for different numbers of fine-tuned layers (5 and 10). The results show the trade-off between model accuracy and memory consumption achieved using different compression techniques.", "section": "4.3 Main Results"}, {"figure_path": "S93hrwT8u9/tables/tables_17_1.jpg", "caption": "Table 4: Segmentation with DeepLabV3 on different random seeds.", "description": "This table presents the results of semantic segmentation experiments using the DeepLabV3 model with different random seeds. It shows the mean intersection over union (mIoU) and micro-averaged accuracy (mAcc) for different numbers of layers (5 and 10) and different methods (HOSVD and SVD with \u03b5 = 0.8). The standard deviations are also provided for each metric.", "section": "4.2 Explained Variance Evolution"}, {"figure_path": "S93hrwT8u9/tables/tables_20_1.jpg", "caption": "Table 1: Experimental results on ImageNet-1k. \u201c#Layers\u201d refers to the number of fine-tuned convolutional layers (counted from the end of the model). Activation memory consumption is shown in MegaBytes (MB).", "description": "This table shows the experimental results of fine-tuning different numbers of layers on ImageNet-1k using various methods including vanilla training, gradient filtering, SVD, and HOSVD.  The table presents the top-1 validation accuracy and activation memory (peak and mean) in MB for MobileNetV2, ResNet18, ResNet34, and SwinT models.", "section": "4.3 Main Results"}]