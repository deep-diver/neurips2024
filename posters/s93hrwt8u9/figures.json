[{"figure_path": "S93hrwT8u9/figures/figures_0_1.jpg", "caption": "Figure 1: We compress the activations that will be later employed for backpropagation.", "description": "This figure illustrates the core concept of the proposed method: compressing activation maps to reduce memory usage during backpropagation.  The left side shows the standard forward and backward passes, where activation maps (A<sub>i</sub> and A<sub>i+1</sub>) are stored in memory. The right side shows how the proposed method compresses these activation maps before storing them, reducing memory usage while allowing for efficient reconstruction when needed for backpropagation. The compression process is represented by the symbol of a double arrow with a smaller width than the other arrows to indicate that it is using a lossy compression technique. The gears represent the computation process.", "section": "1 Introduction"}, {"figure_path": "S93hrwT8u9/figures/figures_5_1.jpg", "caption": "Figure 2: For a single convolutional layer with minibatch size B, (a) and (b) illustrate the predicted changes in compression rate Rc and speedup ratios Rs as functions of Kj, when comparing HOSVD with vanilla training, respectively. (c) shows the evolution of the SNR with retained variance \u03b5.", "description": "This figure shows the predicted performance of HOSVD compared to vanilla training for a single convolutional layer.  Subfigure (a) shows the compression rate (Rc) as a function of the number of principal components (Kj) kept after applying HOSVD. Subfigure (b) displays the speedup ratio (Rs) achieved by using HOSVD versus vanilla training, also as a function of Kj.  Subfigure (c) illustrates the relationship between the signal-to-noise ratio (SNR) and the retained variance (\u03b5) when applying HOSVD. This figure demonstrates that HOSVD offers significant compression and speedup advantages while maintaining acceptable SNR.", "section": "3.4 Complexity and Error Analyis"}, {"figure_path": "S93hrwT8u9/figures/figures_6_1.jpg", "caption": "Figure 3: Explained variance \u03b5 for the first two dimensions of the activation map in the 4th last layer when fine-tuning the last four layers of MCUNet using HOSVD on CIFAR-10, following setup A. ", "description": "This figure shows the explained variance for the first two dimensions of the activation map in the 4th last layer of a MCUNet model when fine-tuning the last four layers using HOSVD on the CIFAR-10 dataset.  It illustrates how many principal components (K1 and K2) are needed to retain a specific fraction (\u03b5=0.8) of the explained variance in each dimension. The plot indicates that a relatively small number of principal components can capture a significant portion of the variance.", "section": "4.2 Explained Variance Evolution"}, {"figure_path": "S93hrwT8u9/figures/figures_7_1.jpg", "caption": "Figure 5: Performance curves of an MCUNet pretrained on ImageNet and finetuned on CIFAR-10 with different activation compression strategies.", "description": "This figure shows the performance of different activation compression techniques on a MCUNet model. The x-axis represents the peak activation memory in kilobytes, while the y-axis represents the top-1 validation accuracy.  The plot compares vanilla training to several compression methods, including HOSVD and SVD with different variance thresholds, and gradient filtering with different patch sizes. The results demonstrate that HOSVD achieves higher accuracy with significantly lower memory compared to other methods, highlighting its effectiveness for memory-constrained environments.  Specific values are highlighted showing the improvement of HOSVD over vanilla training in accuracy and memory reduction.", "section": "4.3 Main Results"}, {"figure_path": "S93hrwT8u9/figures/figures_7_2.jpg", "caption": "Figure 5: Performance curves of an MCUNet pretrained on ImageNet and finetuned on CIFAR-10 with different activation compression strategies.", "description": "This figure shows the performance of different activation compression strategies when fine-tuning a MCUNet model. The model is first pre-trained on ImageNet and then fine-tuned on CIFAR-10.  The x-axis represents the peak activation memory in kilobytes (kB), while the y-axis represents the top-1 validation accuracy.  Multiple lines represent different strategies: vanilla training (no compression), gradient filtering with different patch sizes (R2, R4, R7), SVD (singular value decomposition), and HOSVD (higher-order singular value decomposition) with an explained variance threshold of 0.8.  The graph illustrates the trade-off between memory usage and accuracy for each method.  HOSVD demonstrates superior performance in terms of achieving higher accuracy with significantly less memory compared to other methods, highlighting its effectiveness for memory-constrained environments.", "section": "4.3 Main Results"}, {"figure_path": "S93hrwT8u9/figures/figures_16_1.jpg", "caption": "Figure 6: Predicted evolution of FLOPs for the forward pass of both vanilla training and HOSVD.", "description": "This figure shows the predicted FLOPs (floating point operations) for the forward pass of a convolutional layer during training. The blue line represents vanilla training, while the orange dashed line represents the proposed HOSVD method.  The x-axis shows the increasing size of the activation map (B=C=H=W=C'). The plot demonstrates that vanilla training's FLOPs increase more slowly than HOSVD's with increasing activation map size.  The HOSVD method has significantly higher FLOPs than Vanilla at smaller activation map sizes but eventually the difference becomes relatively small.", "section": "A.4 Details of Overhead, Computational Speedup and Space Complexity"}, {"figure_path": "S93hrwT8u9/figures/figures_18_1.jpg", "caption": "Figure 2: For a single convolutional layer with minibatch size B, (a) and (b) illustrate the predicted changes in compression rate Rc and speedup ratios Rs as functions of Kj, when comparing HOSVD with vanilla training, respectively. (c) shows the evolution of the SNR with retained variance \u03b5.", "description": "This figure shows the predicted effects of using Higher Order Singular Value Decomposition (HOSVD) for activation map compression on a single convolutional layer.  Subfigure (a) illustrates the predicted compression rate (Rc) as a function of the number of principal components (Kj) kept in each mode of the tensor decomposition, comparing HOSVD to the standard training method (vanilla). Subfigure (b) shows the predicted speedup ratio (Rs) achieved by HOSVD over the standard training method.  Finally, subfigure (c) shows the Signal-to-Noise Ratio (SNR) in relation to retained variance (\u03b5), indicating how well the variance is captured with compression.", "section": "3.4 Complexity and Error Analysis"}, {"figure_path": "S93hrwT8u9/figures/figures_19_1.jpg", "caption": "Figure 2: For a single convolutional layer with minibatch size B, (a) and (b) illustrate the predicted changes in compression rate Rc and speedup ratios Rs as functions of Kj, when comparing HOSVD with vanilla training, respectively. (c) shows the evolution of the SNR with retained variance \u03b5.", "description": "This figure shows the predicted changes in compression rate and speedup ratios when using HOSVD compared to vanilla training for a single convolutional layer.  Different values of Kj (the number of principal components kept in each mode of the tensor decomposition) are used to demonstrate how the compression rate and speedup vary.  The graphs show that higher compression rates and speedup are achieved with smaller values of Kj.  Finally, another graph illustrates the relationship between the Signal-to-Noise Ratio (SNR) and retained variance \u03b5, showing that the SNR increases quadratically with retained variance.", "section": "3.4 Complexity and Error Analyis"}, {"figure_path": "S93hrwT8u9/figures/figures_20_1.jpg", "caption": "Figure 5: Performance curves of an MCUNet pretrained on ImageNet and finetuned on CIFAR-10 with different activation compression strategies.", "description": "This figure illustrates the trade-off between peak activation memory (in kilobytes) and top-1 validation accuracy (%) for different activation compression methods on the MCUNet model.  The model was pre-trained on ImageNet and fine-tuned on CIFAR-10.  The plot compares vanilla training with various compression techniques including HOSVD (Higher-Order Singular Value Decomposition), SVD (Singular Value Decomposition), and Gradient Filter with different compression ratios. The results show that HOSVD achieves a better balance between high accuracy and low memory compared to other methods.", "section": "4.3 Main Results"}]