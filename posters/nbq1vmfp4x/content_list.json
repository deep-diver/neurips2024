[{"type": "text", "text": "The Power of Hard Attention Transformers on Data Sequences: A Formal Language Theoretic Perspective ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Pascal Bergstr\u00e4\u00dfer RPTU Kaiserslautern-Landau 67663 Kaiserslautern, Germany bergstraesser@cs.uni-kl.de ", "page_idx": 0}, {"type": "text", "text": "Chris K\u00f6cher MPI-SWS 67663 Kaiserslautern, Germany ckoecher@mpi-sws.org ", "page_idx": 0}, {"type": "text", "text": "Anthony Widjaja Lin MPI-SWS and RPTU Kaiserslautern-Landau 67663 Kaiserslautern, Germany awlin@mpi-sws.org ", "page_idx": 0}, {"type": "text", "text": "Georg Zetzsche MPI-SWS 67663 Kaiserslautern, Germany georg@mpi-sws.org ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Formal language theory has recently been successfully employed to unravel the power of transformer encoders. This setting is primarily applicable in Natural Language Processing (NLP), as a token embedding function (where a bounded number of tokens is admitted) is first applied before feeding the input to the transformer. On certain kinds of data (e.g. time series), we want our transformers to be able to handle arbitrary input sequences of numbers (or tuples thereof) without a priori limiting the values of these numbers. In this paper, we initiate the study of the expressive power of transformer encoders on sequences of data (i.e. tuples of numbers). Our results indicate an increase in expressive power of hard attention transformers over data sequences, in stark contrast to the case of strings. In particular, we prove that Unique Hard Attention Transformers (UHAT) over inputs as data sequences no longer lie within the circuit complexity class AC0 (even without positional encodings), unlike the case of string inputs, but are still within the complexity class TC0 (even with positional encodings). Over strings, UHAT without positional encodings capture only regular languages. In contrast, we show that over data sequences UHAT can capture non-regular properties. Finally, we show that UHAT capture languages definable in an extension of linear temporal logic with unary numeric predicates and arithmetics. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recent years have witnessed the success of transformers [42] in different applications, including natural language processing [12], computer vision [14], speech recognition [13], and time series analysis [45, 46]. In the quest to better understand the ability and limitation of transformers, theoretical investigations have actively been undertaken in the last few years. Among others, formal language theory has been successfully applied to reveal deep insights into the expressive power of transformers, e.g., see the recent survey [40] and [2, 3, 8, 15, 20, 21, 30, 32, 38, 39]. In particular, relevant questions pertain to the power of various attention mechanisms, bounded/unbounded precision, positional encoding functions, and interplay between encoders and decoders, among many others. ", "page_idx": 0}, {"type": "text", "text": "One common assumption in the formal language theoretic approach to transformers is that the input sequence ranges over a finite set $\\Sigma$ (called alphabet), which is then to be fed into a transformer after applying a token embedding function of the form $f:\\Sigma\\to\\mathbb{R}^{d}$ . As a by-product, the number of tokens is finite. In certain applications (e.g. time series forecasting [28]), we want our transformers to be able to handle arbitrary input sequences of numbers (or tuples thereof) without a priori limiting the values of these numbers. Moreover, numbers could be compared using arithmetic and (in)equality, which is not the case for elements of alphabets considered in formal language theory. For this reason, we propose to investigate the expressive power of transformers over data sequences, which takes us to the setting of formal language theory over the alphabet $\\Sigma=\\mathbb{Q}^{d}$ , for some $d\\in\\mathbb{Z}_{>0}$ , e.g., see [4, 11]. That is, what properties of a sequence of (tuples of) numbers can be recognized by transformers? ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Connections to circuit complexity. Existing work has revealed intimate connections between transformers and circuit complexity. In particular, let us consider the following class of transformer encoders that has been the main focus of many recent papers: Unique Hard Attention Transformers (UHAT). Among others, UHAT allows arbitrary positional encoding and an attention mechanism that picks a vector at a unique minimum position in the sequence that maximizes the attention score. It is known that the class of formal languages recognized by UHAT is a strict subset of the circuit complexity class $\\mathbf{A}\\mathbf{C}^{0}$ (cf. [3, 20, 21]), i.e., each UHAT can be simulated by a family of boolean circuits of constant depth. More concretely, this entails among others that UHAT cannot compute the parity (even/oddness) of the number of occurrences of any given letter $a$ in the input string (for strings over an alphabet containing at least two letters). ", "page_idx": 1}, {"type": "text", "text": "Our first contribution is that UHAT over data sequences (even without positional encodings) is no longer contained in $\\mathbf{A}\\mathbf{C}^{0}$ , unlike the case of finite number of tokens. Instead, we show that UHAT can be captured by the circuit complexity class $\\mathrm{T}\\mathbf{C}^{0}$ , which extends $\\mathbf{A}\\mathbf{C}^{0}$ circuits with majority gates. ", "page_idx": 1}, {"type": "text", "text": "Theorem 1. UHAT with positional encoding over data sequences is in $T C^{0}$ but not in $A C^{0}$ ", "page_idx": 1}, {"type": "text", "text": "This complexity upper bound allows us to deduce the expressive power of UHAT over data sequences by using complexity theory. For example, since UHAT accepts only $\\mathrm{T}\\mathrm{C}^{0}$ languages, successfully constructing a UHAT (e.g. through learning) for ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathsf{S Q R T S U M}:=\\left\\{(a_{1},b_{1}),\\dotsc,(a_{n},b_{n})\\left|\\sum_{i=1}^{n}\\sqrt{a_{i}}\\leq\\sum_{i=1}^{n}\\sqrt{b_{i}},\\,\\mathrm{and}\\;\\mathsf{e a c h}\\left(a_{i},b_{i}\\right)\\in\\mathbb{Z}_{>0}\\times\\mathbb{Z}_{>0},\\right.\\right\\},\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "would constitute a major breakthrough in complexity theory (cf. [1, 18]), i.e., showing that SQRTSUM is in the complexity class $\\mathrm{TC^{0}\\subseteq P/p o l y}$ . A byproduct of our proof is that for each length, the set of accepted sequences is a semialgebraic set. This implies, e.g., that the graph $\\{(\\stackrel{\\smile}{x,e^{x}})\\mid x\\in\\mathbb{R}\\}\\subseteq\\mathbb{R}^{\\dot{2}}$ of $x\\mapsto e^{x}$ (viewed as a set of length-1 sequences) is not accepted by UHAT. ", "page_idx": 1}, {"type": "text", "text": "Connection to regular languages over data sequences. Recent results have revealed surprising connections between regular languages and formal languages recognizable by transformer encoders. In particular, it was proven (cf. [2]) that languages recognizable by UHAT (even with no positional encodings) form a strict subset of regular languages, namely those that are \u201cstar-free\u201d or, equivalently, definable in First-Order Logic (FO), or Linear Temporal Logic (LTL). With positional encodings, similar connections hold, by extending the logics with unary numerical predicates (cf. [2, 3]). ", "page_idx": 1}, {"type": "text", "text": "To investigate whether such connections extend to data sequences, we bring forth formal languages theory over infinite alphabets (cf., [4, 11]), which has been an active research field in the last decade or so with applications to programming languages and databases (to name a few), e.g., see [11, 26, 43]. Our second contribution is a language over data sequences recognizable by UHAT without positional encodings that lies beyond existing formal models over infinite alphabets (in particular, \u201cregular\u201d ones). This shows the strength of UHAT over data sequences even without positional encodings, in stark contrast to the case of finite alphabets. ", "page_idx": 1}, {"type": "text", "text": "Theorem 2. There is a non-regular language over $\\Sigma=\\mathbb{Q}^{d}$ that is accepted by masked UHAT with no positional encoding. ", "page_idx": 1}, {"type": "text", "text": "Finally, to better understand languages over data sequences recognizable by UHAT, our third contribution is to show how UHAT can recognize languages definable by the so-called Locally Testable LTL $\\mathrm{[LT)^{2}L)}$ ), which extends LTL with unary numerical predicates and local arithmetic tests for fixed-size windows over the input sequence. Using $(\\mathrm{LT})^{2}\\dot{\\mathrm{L}}$ , we can see at a glance what can be expressed by UHAT over data sequences. For one, this includes the well-known Simple Moving Averages. As another example, using $\\mathrm{(LT)^{2}L}$ it can be easily shown that UHAT can capture linear recurrence sequences considered in the famous Skolem problem and discrete linear dynamical systems [25, 27, 29], i.e., sequences of the form $\\mathbf{x},A\\mathbf{x},\\ldots,A^{n}\\mathbf{x}$ such that $n\\geq0$ is minimal with ${\\pmb y}A^{n}{\\pmb x}=0$ where $\\pmb{y}\\in\\mathbb{Q}^{1\\times d}$ and $A\\in\\mathbb{Q}^{d\\times d}$ are fixed and $\\pmb{x}\\in\\mathbb{Q}^{d}$ . ", "page_idx": 1}, {"type": "text", "text": "Technical challenges. Obtaining our results poses several challenges. First, for the $\\mathbf{T}\\mathbf{C}^{0}$ upper bound, we need to use Boolean (constant depth) circuits to simulate UHATs, in which real constants can occur (in affine transformations or positional encodings). While in $\\mathbf{T}\\mathbf{C}^{0}$ , it is known that majority gates can be used to perform multiplication of rationals [7], arithmetic with reals requires infinite precision and cannot be done with Boolean circuits. To this end, we compute rational approximations of reals accurate enough to preserve the acceptance condition for inputs up to a particular length $n$ . ", "page_idx": 2}, {"type": "text", "text": "Here, a naive attempt would be to replace each real occurring in the UHAT in affine transformations and the positional encoding by some rational approximation. However, this is not possible, meaning any rational approximation would change the behavior on input sequences of length $n=3$ , even for low-dimensional vectors with entries in $\\{0,1\\}$ . Indeed, there is a UHAT involving real numbers $\\alpha,\\beta$ that accepts a simple sequence of $\\{0,1\\}$ -vectors if and only if $\\alpha\\beta=2$ and $\\alpha=\\beta$ , i.e. $\\alpha=\\beta={\\sqrt{2}}$ Thus, $\\alpha$ and $\\beta$ cannot be replaced by rationals, even for very short inputs (see Appendix A for details). ", "page_idx": 2}, {"type": "text", "text": "Instead, we show that a UHAT can be translated into a small Boolean combination of polynomial inequalities. This format has the advantage that\u2014as we show using convex geometry\u2014the real coefficients of those polynomials can be replaced by suitably chosen rational numbers. In turn, the layer-by-layer construction of these polynomial inequalities requires a carefully chosen data structure to encode the function computed by a sequence of transformer layers. For example, we show that the resulting Boolean combinations of polynomial inequalities have a bound on the number of alternations between conjunctions and disjunctions, which is crucial for constructing $\\mathbf{T}\\mathbf{C}^{0}$ -circuits. ", "page_idx": 2}, {"type": "text", "text": "Another key challenge occurs in the translation from $(\\mathrm{LT})^{2}\\mathrm{L}$ to UHATs: In the inductive construction, we need to represent truth values using reals in $[0,1]$ . To implement negation, we use a UHAT gadget (with positional encodings) that can normalize these truth values to $\\{\\bar{0},1\\}$ . ", "page_idx": 2}, {"type": "text", "text": "Notation. In the sequel, we assume some background from computational complexity, in particular circuit complexity (see the book [44]). In particular, we use the circuit complexity class $\\mathbf{A}\\mathbf{C}^{0}$ , which defines a class of problems that are computable by a nonuniform family of constant-depth boolean circuits, where each gate permits an unbounded fan-in (i.e. arbitrary many inputs). Similarly, the complexity class $\\mathbf{T}\\mathbf{C}^{\\bar{0}}$ is an extension of $\\mathbf{A}\\mathbf{C}^{0}$ , where majority gates are allowed. It is well-known that ${\\mathrm{\\bar{AC}}}^{0}\\ {\\bar{\\subset}}\\ {\\mathrm{TC}}^{0}$ . Assuming uniformity, both $\\mathbf{A}\\mathbf{C}^{0}$ and $\\mathrm{T}\\mathbf{C}^{0}$ are contained in the class of problems solvable in polynomial-time. For nonuniformity, these classes are contained in the complexity class P/poly, which admits (nonuniform) polynomial-size circuits. It is a long-standing open problem whether numerical analysis (e.g. square-root-sum) is in P/poly, e.g., see [1]. ", "page_idx": 2}, {"type": "text", "text": "2 Transformer encoders and their languages ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In the following, we adapt the setting of formal language theory (see [3, 21, 40]) to data sequences. For a vector $\\pmb{a}=(a_{1},\\ldots,a_{d})$ we write $\\pmb{a}[i,j]:=(a_{i},\\dots,a_{j})$ for all $1\\leq i\\leq j\\leq d$ and if $i=j$ , we simply write $a[i]$ . For a set $S$ we denote the set of (potentially empty) sequences of elements from $S$ by $S^{*}$ . We write $S^{+}$ for the restriction to non-empty sequences. We consider languages $L$ over the infinite alphabet $\\Sigma=\\mathbb{Q}^{d}$ , for some integer $d>0$ . That is, $L$ is a set of sequences of $d$ -tuples over rational numbers. We will define a UHAT (similarly as in previous papers that study formal language theoretic perspectives) as a length preserving map $(\\mathbb{Q}^{d})^{\\ast}\\,\\dot{\\to}\\,(\\mathbb{R}^{e})^{\\ast}$ . ", "page_idx": 2}, {"type": "text", "text": "Standard encoder layer with unique hard attention. A standard encoder layer is defined by three affine transformations $A,B\\colon\\bar{\\mathbb{R}^{d}}\\to\\mathbb{R}^{d}$ and $C\\colon\\ensuremath{\\mathbb{R}}^{2d}\\to\\ensuremath{\\mathbb{R}}^{e}$ . For a sequence $\\pmb{v}_{1},\\ldots,\\pmb{v}_{n}\\in\\mathbb{R}^{\\b{d}}$ with $n~\\ge~1$ we define the attention vector at position $i\\ \\in\\ [1,n]$ as $\\mathbf{\\Delta}a_{i}\\mathbf{\\Sigma}:=\\mathbf{\\Delta}v_{j}$ with $j~\\in~[1,n]$ minimal such that the attention score $\\langle A{\\pmb v}_{i},B{\\pmb v}_{j}\\rangle$ is maximized. The layer outputs the sequence $C(v_{1},a_{1}),\\ldots,C(v_{n},\\mathbf{{a}}_{n})$ . ", "page_idx": 2}, {"type": "text", "text": "ReLU encoder layer. A ReLU layer for some $k\\,\\in\\,[1,d]$ on input $\\pmb{v}_{1},\\ldots,\\pmb{v}_{n}\\,\\in\\,\\mathbb{R}^{d}$ applies the ReLU function to the $k$ -th coordinate of each $\\pmb{v}_{i}$ , i.e. it outputs the sequence $\\pmb{v}_{1}^{\\prime},\\ldots,\\pmb{v}_{n}^{\\prime}$ where $\\boldsymbol{v}_{i}^{\\prime}:=\\,(v_{i}[1,k-1],\\operatorname*{max}\\{0,v_{i}[k]\\},v_{i}[k+1,n])$ . [Equivalently, one could instead allow a feedforward network at the end of an encoder layer (see [3, 21, 40]).] ", "page_idx": 2}, {"type": "text", "text": "Transformer encoder. A unique hard attention transformer encoder (UHAT) is a repeated application of standard encoder layers with unique hard attention and ReLU encoder layers. Clearly, using an alternation of standard layers and ReLU layers, we can assume that the output of a UHAT layer is an arbitrary composition of affine transformations and component-wise ReLU application. In particular, these compositions may use the functions max and min. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Languages accepted by UHATs. The notion of \u201clanguages\u201d accepted by a UHAT (i.e. a set of accepted sequences) can be defined, depending on whether a positional encoding is permitted. If it is permitted, a language $L\\subseteq(\\mathbb{Q}^{d})^{+}$ is accepted by a UHAT $T$ if and only if there exists a positional encoding function $p\\colon\\mathbb{N}\\times\\mathbb{N}\\rightarrow\\mathbb{R}^{s}$ and an acceptance vector $\\pmb{t}\\in\\mathbb{R}^{e}$ such that on every sequence ", "page_idx": 3}, {"type": "equation", "text": "$$\n(p(1,n+1),\\pmb{v}_{1}),\\dots,(p(n,n+1),\\pmb{v}_{n}),(p(n+1,n+1),\\pmb{0})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "$T$ outputs a sequence $\\mathbf{\\boldsymbol{v}}_{1}^{\\prime},\\ldots,\\mathbf{\\boldsymbol{v}}_{n+1}^{\\prime}\\in\\mathbb{R}^{e}$ with $\\langle t,v_{1}^{\\prime}\\rangle>0$ if and only if $(\\pmb{v}_{1},\\dots,\\pmb{v}_{n})\\in L$ . Note that if $T_{1}$ and $T_{2}$ are UHATs with positional encoding that realize functions $f_{1}\\colon(\\mathbb{Q}^{d})^{*}\\to(\\mathbb{Q}^{e})^{*}$ and $f_{2}\\colon(\\mathbb{Q}^{e})^{*}\\to(\\mathbb{Q}^{r})^{*}$ , then there is a UHAT $T_{2}\\circ T_{1}$ with positional encoding that realizes the composition $f_{2}\\circ f_{1}$ by using a positional encoding that combines the positional encodings of $T_{1},T_{2}$ . ", "page_idx": 3}, {"type": "text", "text": "In the above definition we appended an additional zero vector to the end of the input. Over finite alphabets it is often assumed that the input sequence is extended with a special unique end-of-input marker (e.g. see [21, 40]). When the input is a sequence of (tuples of) numbers, if we allow positional encoding, then the zero vector at the end of the input can be turned into a unique vector marking the end of the input (see Section 5). Without positional encoding, however, we have to explicitly make the zero vector at the end of the input unique. That is, a UHAT without positional encoding is initialized with the sequence $(1,\\pmb{v}_{1}),\\dots,\\bar{(1,\\pmb{v}_{n})},\\pmb{0}\\in\\mathbb{Q}^{d+1}$ instead; this ensures, among others, that the end-of-input marker does not appear in the actual input. ", "page_idx": 3}, {"type": "text", "text": "In the definition of a standard encoder layer the attention vector at position $i\\in[1,n]$ can be any vector in the sequence $\\pmb{v}_{1},\\dots,\\pmb{v}_{n}$ . Using masking, one can restrict the attention vector to vectors of certain positions. A UHAT with past masking restricts the attention vector $\\mathbf{a}_{i}$ at position $1\\leq i<n$ to be contained in the subsequence $\\pmb{v}_{i+1},\\ldots,\\pmb{v}_{n}$ and at position $n$ to $\\mathbf{\\Delta}\\mathbf{a}_{n}:=\\pmb{v}_{n}$ . ", "page_idx": 3}, {"type": "text", "text": "3 UHAT and TC0 ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we prove Theorem 1. First, we show that all languages of UHAT (even with positional encoding) belong to the class $\\mathbf{T}\\mathbf{C}^{0}$ . Then, we show that there is a UHAT (even without positional encoding) whose language is $\\mathrm{T}\\mathrm{C}^{0}$ -hard under $\\mathbf{A}\\mathbf{C}^{0}$ -reductions. We begin with the proof that all UHAT languages belong to $\\mathrm{T}\\mathrm{C}^{0}$ . ", "page_idx": 3}, {"type": "text", "text": "Input encoding A language $L\\subseteq\\Sigma^{*}$ over a finite alphabet $\\Sigma$ belongs to $\\mathrm{T}\\mathbf{C}^{0}$ if for every input length $n$ , there is a circuit of size polynomial $n$ , such that the circuit consists of input gates (for each input position $i$ , and each letter $a\\in\\Sigma$ , there is a gate that evaluates to \u201ctrue\u201d if and only if position $i$ of the input words carries an $a$ ), Boolean gates (computing the AND, OR, or NOT function) and majority gates (evaluating to true if more than half of their input wires carry true). Here, AND, OR, and majority gates can have arbitrary fan-in. In order to define what it means that a language $L\\subseteq(\\mathbb{Q}^{d})^{+}$ belongs to $\\mathrm{T}\\mathbf{C}^{0}$ , we need to specify an encoding as finite-alphabet words. To this end, we encode a sequence $\\pmb{u}_{1},\\dots,\\pmb{u}_{n}$ with $\\bar{\\b u_{i}}\\in\\bar{\\mathbb{Q}}^{d}$ as a string $v_{1}\\#\\cdots\\#v_{n}$ , where $v_{i}=p_{1}/q_{1}\\square\\cdot\\cdot\\cdot\\square p_{d}/q_{d}$ with $p_{j},q_{j}\\in\\{-,0,1\\}^{*}$ . Here, $v_{i}\\in\\{-,/,\\boxed{\\Omega},0,1\\}^{*}$ represents the vector $\\mathbf{u}_{i}\\in\\mathbb{Q}^{d}$ such that $\\begin{array}{r}{{\\pmb u}_{i}[j]=\\frac{a_{j}}{b_{j}}}\\end{array}$ , $a_{j},b_{j}\\in\\mathbb{Z}$ , and $p_{j},q_{j}\\in\\{-,0,1\\}^{*}$ are the binary expansions of $a_{j}$ and $b_{j}$ . ", "page_idx": 3}, {"type": "text", "text": "Remark 4. The main challenge in proving Theorem 1 is that the constants appearing in a UHAT can be real numbers. These can in general not be avoided: There are UHAT languages over $\\Sigma=\\mathbb{Q}$ (even without positional encoding) that cannot be accepted by UHAT with rational constants (even with positional encoding). For example, for every real number $r>0$ , one can1 construct a UHAT for the language $L_{r}$ of all sequences over $\\Sigma=\\mathbb{Q}$ where the first letter is $>r$ . Note that $L_{r}\\neq L_{s}$ for any $r,s>0$ , $r\\neq s$ . However, there are clearly only countably many languages over $\\Sigma=\\mathbb{Q}$ accepted by UHAT with rational constants where membership only depends on the first letter2. Thus, there are uncountably many real $r>0$ such that $L_{r}$ is not accepted by a UHAT with rational constants. ", "page_idx": 3}, {"type": "text", "text": "The construction of $\\mathbf{T}\\mathbf{C}^{0}$ circuits comprises three steps. In Step I, we show that the set of accepted length- $\\cdot n$ sequences can be represented by a Boolean combination of polynomial inequalities. Importantly, (i) this representation, called \u201cpolynomial constraints\u201d is polynomial-sized in $n$ , and (ii) the number of alternations between conjunction and disjunction is bounded (i.e. independent of $n$ ). The polynomials in this representations can still contain real coefficients. In Step $\\mathrm{II}$ , we show that if we restrict the input not only to length- ${\\mathbf{\\nabla}}n$ sequences, but to rational numbers of size $\\leq m$ , then we can replace all real coefficients of our polynomials by rationals of size polynomial in $m$ and $n$ , without changing the language (among vectors of size $\\leq m$ ). In Step III, we implement a $\\mathbf{T}\\mathbf{C}^{0}$ circuit. Here, it is important that the number of alternations between conjunctions and disjunctions in our polynomial constraints is bounded, because the depth of the circuit is proportional to this number of alternations. ", "page_idx": 4}, {"type": "text", "text": "Step I: UHAT as polynomials We first consider a formalism to describe a set of sequences over $\\mathbb{Q}^{d}$ . We consider such sequences $(x_{1},\\ldots,x_{n})$ of length $n$ , where $\\pmb{x}_{i}\\in\\mathbb{Q}^{d}$ for each $i$ . In this case, we also abbreviate $\\bar{\\pmb{x}}=(\\pmb{x}_{1},\\dots,\\pmb{x}_{n})$ . A polynomial constraint (PC) is a positive Boolean combination (i.e., without negation) of constraints of the form $p(\\bar{\\pmb{x}})>0$ or $p(\\bar{\\pmb{x}})\\ge0$ , where $p\\in\\mathbb{R}[X_{1},\\ldots,X_{d\\cdot n}]$ is a polynomial with real coefficients. Here, plugging $\\bar{\\pmb{x}}\\in(\\mathbb{Q}^{d})^{n}$ into $p$ is defined by assigning the $d\\cdot n$ rational numbers in $\\bar{\\pmb{x}}$ to the $d\\cdot n$ variables $X_{1},\\ldots,X_{d\\cdot n}$ . The PC $\\alpha$ accepts a sequence of vectors $\\bar{\\pmb{x}}\\in(\\mathbb{Q}^{d})^{n}$ , if the Boolean formula evaluates to true when plugging $\\bar{\\pmb{x}}$ into the polynomials $p$ in $\\alpha$ . The set of accepted sequences is denoted by $[\\alpha]$ . Now let $a\\in\\mathbb N$ . A PC has $a$ alternations if the positive Boolean combination has $a$ alternation s  be tween disjunctions and conjunctions. ", "page_idx": 4}, {"type": "text", "text": "In the following, a constrained polynomial representation $(C P R)$ can be used to compute from a sequence of inputs $(x_{1},\\ldots,x_{n})$ with $\\mathbf{\\boldsymbol{x}}_{1},\\ldots,\\mathbf{\\boldsymbol{x}}_{n}\\,\\in\\,\\mathbb{R}^{d^{\\prime}}$ a new sequence of outputs $(y_{1},\\ldots,y_{n})$ with $\\pmb{y}_{1},\\dots,\\pmb{y}_{n}\\,\\,\\in\\,\\,\\mathbb{R}^{d}$ . Formally, a CPR comprises for each $i\\;\\;\\in\\;\\;\\{1,\\ldots,n\\}$ a sequence $(\\varphi_{1},D_{1}),\\ldots,(\\varphi_{s_{i}},D_{s_{i}})$ of pairs $(\\varphi_{j},D_{j})$ , where each pair $(\\varphi_{j},D_{j})$ is a \u201cconditional assignment\u201d: each $(\\varphi_{j},D_{j})$ tells us that if the condition $\\varphi_{j}$ is satisfied, then we return $D_{j}(\\bar{\\pmb{x}})$ . More precisely: (i) each $\\varphi_{j}$ is a polynomial constraint where all polynomials have degree $\\leq2$ , (ii) for any $j\\neq m$ , the constraints $\\varphi_{j}$ and $\\varphi_{m}$ are mutually exclusive, and (iii) each $D_{j}\\colon\\ensuremath{\\mathbb{R}}^{d^{\\prime}\\cdot n}\\,\\to\\,\\ensuremath{\\mathbb{R}}^{d}$ is an affine transformation. Because of their role as conditional assignments, we also write $\\varphi_{j}\\rightarrow D_{j}$ for such pairs. For $a\\in\\mathbb N$ , we say that the CPR is $a$ -alternation-bounded if each of the formulas $\\varphi_{j}$ has at most $a$ alternations. A CPR as above computes a function $\\mathbb{R}^{d^{\\prime}\\cdot n}\\rightarrow\\mathbb{R}^{d\\cdot n}$ : Given $\\bar{\\pmb{x}}=(\\pmb{x}_{1},\\pmb{\\ldots},\\pmb{x}_{n})$ with $\\ensuremath{\\mathbf{x}}_{1},\\ldots,\\ensuremath{\\mathbf{x}}_{n}\\in\\mathbb{R}^{d^{\\prime}}$ , it computes the sequence $(y_{1},\\ldots,y_{n})$ if for every $i\\in\\{1,\\ldots,n\\}$ , we have ${\\pmb y}_{i}=D_{j}({\\bar{\\pmb x}})$ , provided that $j$ is the (in case of existence uniquely determined) index for which $\\varphi_{j}(\\bar{\\pmb{x}})$ is satisfied. The size of PCs and CPRs are their bit lengths (see Appendix B for details). ", "page_idx": 4}, {"type": "text", "text": "Proposition 5. Fix a UHAT with positional encoding and $\\ell$ layers. For any given sequence length $n$ , there exists a polynomial-sized $P C\\,\\alpha$ with $O(\\ell)$ alternations such that $[\\alpha]$ equals the set of accepted sequences of length $n$ . ", "page_idx": 4}, {"type": "text", "text": "Note that Proposition 5 implies that the set of sequences of each length $n$ is a semialgebraic set [31]. The proof is by induction on the number of layers, which requires a slight strengthening: ", "page_idx": 4}, {"type": "text", "text": "Lemma 6. Fix a UHAT with positional encoding and $\\ell$ layers. For any given sequence length $n$ , one can construct in polynomial time an $O(\\ell)$ -alternation-bounded CPR computing the function $\\mathbb{R}^{d\\cdot(n+1)}\\to\\mathbb{R}^{e\\cdot(n+1)}$ computed by the UHAT. ", "page_idx": 4}, {"type": "text", "text": "Proof. We prove the statement by induction on the number of layers. First, we consider the positional encoding $p\\colon\\mathbb{N}\\times\\mathbb{N}\\rightarrow\\mathbb{R}^{d}$ as some affine transformations $P_{i}\\colon\\mathbb{R}^{d\\cdot(n+1)}\\to\\mathbb{R}^{d}$ mapping the input sequence $\\bar{\\pmb{x}}$ to $\\pmb{x}_{i}+p(i,n+1)$ . Then we obtain a CPR with $\\top\\rightarrow P_{i}$ for each $1\\leq\\,i\\,\\leq\\,n+1$ . Now, suppose the statement is shown for $\\ell$ layers and consider a UHAT with $\\ell+1$ layers. Suppose that the first $\\ell$ layers of our UHAT compute a function $\\mathbb{R}^{d^{\\prime}\\cdot(n+1)}\\rightarrow\\mathbb{R}^{d\\cdot(n+1)}$ , and the last layer computes a function $\\mathbb{R}^{d\\cdot(n+1)}\\to\\mathbb{R}^{e\\cdot(n+1)}$ . By induction, we have a polynomial size CPR consisting of conditional assignments $\\varphi_{i,k}\\rightarrow D_{i,k}$ for every $i\\in\\{1,...,n+1\\}$ and $1\\le k\\le s_{i}$ . Here, each $D_{i,j}$ is an affine transformation $\\mathbb{R}^{d^{\\prime}\\cdot(n+1)}\\to\\mathbb{R}^{d}$ . ", "page_idx": 4}, {"type": "text", "text": "Let us first consider the case that the last layer of our UHAT is a standard encoder layer. For each $(i,I,j,J)\\in\\{1,\\dots,n+1\\}^{4}$ , we build the conditional assignment using the formula $\\psi_{i,I,j,J}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\bigwedge_{m=1}^{j-1}\\left(\\bigvee_{M=1}^{s_{m}}\\varphi_{m,M}\\wedge p_{i,I,j,J,m,M}(\\bar{x})>0\\right)\\wedge\\bigwedge_{m=j+1}^{n+1}\\left(\\bigvee_{M=1}^{s_{m}}\\varphi_{m,M}\\wedge p_{i,I,j,J,m,M}(\\bar{x})\\geq0\\right)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $p_{i,I,j,J,m,M}(\\bar{\\pmb{x}})$ is the polynomial $\\langle A D_{i,I}\\bar{\\pmb{x}},\\ B D_{j,J}\\bar{\\pmb{x}}-B D_{m,M}\\bar{\\pmb{x}}\\rangle$ . Then, the conditional assignment is $\\varphi_{i,I}\\wedge\\varphi_{j,J}\\wedge\\psi_{i,I,j,J}\\to C(D_{i,I}\\bar{x},D_{j,J}\\bar{x})$ . Here, the idea is that (i) $\\varphi_{i,I}$ expresses that the $I$ -th conditional assignment was used to produce the $i$ -th cell in the previous layer, (ii) $\\varphi_{j,J}$ says the $J$ -th conditional assignment was used to produce the $j$ -th vector in the previous layer, and (iii) $\\psi_{i,I,j,J}$ says the vector $\\pmb{x}_{j}$ yields the maximal attention score for the input $\\pmb{x}_{i}$ , meaning (iii-a) for all positions $m<j$ , $\\pmb{x}_{m}$ has a lower score than $\\pmb{x}_{j}$ (left parenthesis), and (iii-b) for all positions $m\\:>\\:j$ , ${\\pmb x}_{m}$ has at most the score of $\\pmb{x}_{j}$ (right parenthesis). In (iii-a) and (iii-b), we first find the index $M$ of the conditional assignment used to produce the $m$ -th cell in the previous layer. Note that then indeed, all the PCs $\\psi_{i,I,j,I}$ are mutually exclusive. Moreover, the polynomials $\\langle\\dot{A}D_{i,I}\\bar{\\pmb x},\\,B D_{j,J}\\bar{\\pmb x}-B D_{m,M}\\bar{\\pmb x}\\rangle$ have indeed degree 2 and are of size polynomial in $n$ . Furthermore, if the assignments $\\varphi_{i,k}\\rightarrow D_{i,k}$ had at most $a$ alternations, then the new assignments have at most $a+3$ alternations. Finally, the case of ReLU layers is straightforward (see Appendix B). \u53e3 ", "page_idx": 5}, {"type": "text", "text": "Finally, the proof of Proposition 5 is straightforward: from the constructed CPR in Lemma 6 we obtain the polynomial constraint $\\vee_{J=1}^{s_{1}}\\varphi_{1,J}\\bar{\\wedge}\\left\\langle t,D_{1,J}\\bar{\\pmb{x}}\\right\\rangle>0$ with a bounded number of alternations. ", "page_idx": 5}, {"type": "text", "text": "Step II: Replace real coefficients by rationals In our proof, the key step is to replace the real coefficients in the PC by rational coefficients so that the rational PC will define the same set of rational sequences, up to some given size. Let us make this precise. We denote by $\\mathbb{Q}_{\\leq m}=\\{a\\in\\mathbb{Q}\\mid\\;$ $\\lVert a\\rVert_{2}\\leq m\\bar{\\right}$ the set of all rational numbers of size at most $m$ . A polynomial constraint is rational if all the polynomials occurring in it have rational coefficients. ", "page_idx": 5}, {"type": "text", "text": "Proposition 7. For every $m\\in\\mathbb{N}$ and every $P C\\,\\alpha$ with polynomials having $n$ variables, there exists a rational $P C\\,\\alpha^{\\prime}$ of polynomial size such that $[\\![\\alpha]\\cap\\mathbb{Q}_{\\leq m}^{n}=[\\![\\alpha^{\\prime}]\\!]\\cap\\mathbb{Q}_{\\leq m}^{n}$ . ", "page_idx": 5}, {"type": "text", "text": "Proving Proposition 7 requires the following technical lemma, for which we introduce some notation. For two vectors $\\mathbf{u},v\\in\\bar{\\mathbb{R}}^{t}$ and $m\\in\\mathbb{N}$ , we write $\\pmb{u}\\sim_{m}\\pmb{v}$ if for every $\\pmb{w}\\in\\mathbb{Q}^{t}$ and every $z\\in\\mathbb{Q}$ with $\\|\\pmb{w}\\|_{2},\\|z\\|_{2}\\leq m$ , we have (i) $\\langle\\boldsymbol{w},\\boldsymbol{u}\\rangle\\ge z$ if and only if $\\langle\\boldsymbol{w},\\boldsymbol{v}\\rangle\\geq z$ and (ii) $\\langle{\\pmb w},{\\pmb u}\\rangle>z$ if and only if $\\langle{\\pmb w},{\\pmb v}\\rangle>z$ . In other words, we have $\\pmb{u}\\sim_{m}\\pmb{v}$ if and only if $\\textbf{\\em u}$ and $\\pmb{v}$ satisfy exactly the same inequalities with rational coefficients of size at most $m$ . ", "page_idx": 5}, {"type": "text", "text": "Lemma 8. For every $\\boldsymbol{c}\\in\\mathbb{R}^{t}$ and $m\\in\\mathbb{N}$ , there is a $\\pmb{c}^{\\prime}\\in\\mathbb{Q}^{t}$ with $\\|\\pmb{c}^{\\prime}\\|_{2}\\leq(m t)^{O(1)}$ and $c\\sim_{m}c^{\\prime}$ . ", "page_idx": 5}, {"type": "text", "text": "Remark 9. For proving Lemma 8, it is not sufficient to pick a rational $c^{\\prime}$ with $\\|c^{\\prime}-c\\|<\\varepsilon$ for some small enough $\\varepsilon$ . For example, note that if in some coordinate, c contains a rational number of size $\\leq m$ , then in this coordinate, $c^{\\prime}$ and $^c$ must agree exactly for $c\\sim_{m}c^{\\prime}$ to hold. ", "page_idx": 5}, {"type": "text", "text": "Before we prove Lemma 8, let us see how to deduce Proposition 7: in a PC $\\alpha$ we understand each polynomial $p(X_{1},\\ldots,X_{n})$ as a scalar product $\\langle{\\pmb w},{\\pmb u}\\rangle$ where $\\pmb{w}$ constains only variables and $\\textbf{\\em u}$ consists of all coefficients. Then Lemma 8 yields a vector $\\pmb{v}\\sim_{2m}\\pmb{u}$ containing only rationals with the same behavior as $\\textbf{\\em u}$ . From this we finally obtain polynomials having only rational coefficients, which also proves Proposition 7. A detailed proof of Proposition 7 can be found in Appendix B. ", "page_idx": 5}, {"type": "text", "text": "In the proof of Lemma 8, we use the following fact about solution sizes to systems of inequalities. ", "page_idx": 5}, {"type": "text", "text": "Lemma 10. Let $A\\in\\mathbb{Q}^{k\\times n}$ , $A^{\\prime}\\in\\mathbb{Q}^{\\ell\\times n}$ , $z\\in\\mathbb{Q}^{k}$ , $z^{\\prime}\\in\\mathbb{Q}^{\\ell}$ with $\\|A\\|_{2},\\|A^{\\prime}\\|_{2},\\|z\\|_{2},\\|z^{\\prime}\\|_{2}\\leq m.$ If the inequalities $A x\\gg z$ and $A^{\\prime}{\\pmb x}\\geq z^{\\prime}$ have a solution in $\\mathbb{R}^{n}$ , then they have a solution $r\\in\\mathbb{Q}^{n}$ with $\\lVert\\dot{\\boldsymbol{r}}\\rVert_{2}\\leq(m n)^{O(1)}$ . ", "page_idx": 5}, {"type": "text", "text": "We prove Lemma 10 in the appendix. The proof idea is the following. By standard results about polyhedra, the set of vectors $\\textbf{\\em x}$ satisfying $A x\\ge z$ and $A^{\\prime}{\\pmb x}\\geq z^{\\prime}$ can be written as the convex hull of some finite set $X=\\{x_{1},\\ldots,x_{s}\\}$ , plus the cone generated by some finite set $\\{y_{1},\\ldots,y_{t}\\}$ . Here, the vectors in $X\\cup Y$ are all rational and of polynomial size. By the Carath\u00e9odory Theorem, the real solution $s\\in\\mathbb{R}^{n}$ to $A s\\gg z$ and $A^{\\prime}s\\geq\\bar{z}^{\\prime}$ belongs to the convex hull of $n$ elements of $X$ , plus a conic combination of $n$ elements of $Y$ . We then argue that by taking the barycenter of those $n$ elements of $X$ , plus the sum of the $n$ elements of $Y$ gives a rational vector $r\\in\\mathbb{Q}^{n}$ with $A r\\gg z$ and $A^{\\prime}\\pmb{r}\\geq z^{\\prime}$ . The full proof of Lemma 10 is in Appendix B.4. To prove Lemma 8, given $c\\in\\mathbb{R}^{n}$ , we set up a system of (exponentially many) inequalities of polynomial size so that the solutions are exactly the vectors $^d$ with $d\\sim_{m}c$ . The solution provided by Lemma 10 is the desired $c^{\\prime}$ (see Appendix B.5). ", "page_idx": 5}, {"type": "text", "text": "Step III: Constructing $T C^{0}$ circuits It is now straightforward to translate a polynomial-sized CPR with rational coefficients and bounded alternations into a $\\mathrm{T}\\mathbf{C}^{0}$ circuit: ", "page_idx": 5}, {"type": "text", "text": "Proposition 11. Every language accepted by a UHAT with positional encoding is recognized by a family of circuits in $T\\dot{C}^{0}$ . ", "page_idx": 6}, {"type": "text", "text": "We now show that the $\\mathbf{T}\\mathbf{C}^{0}$ upper bound is tight: There is a UHAT whose language is $\\mathbf{T}\\mathbf{C}^{0}$ -hard under $\\mathbf{A}\\mathbf{C}^{0}$ reductions. In particular, this language is not in $\\mathbf{A}\\mathbf{C}^{0}$ , since $\\mathbf{A}\\mathbf{C}^{0}$ is strictly included in $\\mathrm{T}\\mathrm{C}^{0}$ . ", "page_idx": 6}, {"type": "text", "text": "Proposition 12. There is a $T C^{0}$ -complete language that is accepted by a UHAT, even without positional encoding and masking, but is not recognized by any family of circuits in $A C^{0}$ . ", "page_idx": 6}, {"type": "text", "text": "Proof. As shown by Buss [6, Corollary 3], the problem of deciding whether $a b=c$ for given binary encoded integers $a,b,c\\in\\mathbb{Z}$ is $\\mathbf{T}\\mathbf{C}^{0}$ -complete under $\\mathbf{A}\\mathbf{C}^{0}$ -reductions. Since $a b=c$ if and only if $a b>c-1$ and $-a b>-(c+1)$ , the problem of deciding $a b>c$ is also $\\mathbf{T}\\mathbf{C}^{0}$ -complete. We exibit a UHAT such that the problem of deciding $a b>c$ can be $\\mathbf{A}\\mathbf{C}^{0}$ -reduced to membership in the language. ", "page_idx": 6}, {"type": "text", "text": "It suffices to define a UHAT $T$ that accepts a language $L\\subseteq(\\mathbb{Q}^{2})^{+}$ such that for all $r,s\\in\\mathbb{Q}$ we have that $(r,s)\\in L$ if and only if $r>s$ . Then we can reduce the test $a b>c$ to checking whether $(a,\\frac{c}{b})\\in L$ . Note that formally, $\\frac{c}{b}$ is represented as a string containing the binary encodings of $c$ and $b$ separated by a special symbol. The UHAT $T$ is by definition initialized with the sequence $(1,r,s),\\bar{(0,0,0)}\\in\\mathbb{Q}^{3}$ since we only have to consider the accepted language restricted to sequences of length 1. It can directly check that $r-s>0$ using the acceptance vector $\\pmb{t}:=(0,1,-1)$ . \u53e3 ", "page_idx": 6}, {"type": "text", "text": "4 UHAT and regular languages over infinite alphabets ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "It was shown by Angluin et al. [2] that UHATs with no positional encoding on binary input strings accept only regular languages, even if masking is allowed. We show that UHATs with masking over data sequences can recognize \u201cnon-regular\u201d languages over infinite alphabet (Theorem 2). More precisely, a standard notion of regularity over the alphabet $\\Sigma=\\mathbb{Q}^{d}$ is that of symbolic automata (see the CACM article [11]), since it extends and shares all nice closure and algorithmic properties of finite automata over finite alphabets, while at the same time permitting arithmetics. Intuitively, a transition rule in a symbolic automaton is of the form $p\\rightarrow_{\\varphi}q$ , where $\\varphi$ represents the (potentially infinite) set $S\\subseteq\\mathbb{Q}^{d}$ of solutions to an arithmetic constraint $\\varphi$ (e.g. $2x=y$ represents $\\{(n,2n):n\\in\\mathbb{Q}\\}$ . The meaning of such a transition rule is: move from state $p$ to state $q$ by reading any $a\\in S$ . ", "page_idx": 6}, {"type": "text", "text": "To prove Theorem 2, we define the language ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathfrak{s}:=\\{(r_{1},\\ldots,r_{n})\\in\\mathbb{Q}^{n}\\mid n\\geq1\\mathrm{~and~}2r_{i}<r_{i+1}\\mathrm{~for~all~}1\\leq i<n\\}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "of sequences of rational numbers where each number is more than double the preceding number. ", "page_idx": 6}, {"type": "text", "text": "Lemma 13. UHAT with past masking and without positional encoding can recognize Double. ", "page_idx": 6}, {"type": "text", "text": "Proof. Given an input sequence $\\mathbf{\\boldsymbol{u}}_{1},\\ldots,\\mathbf{\\boldsymbol{u}}_{n+1}=(1,r_{1}),\\ldots,(1,r_{n}),(0,0)\\in\\mathbb{Q}^{2}$ , we need to check that for all pairs $1\\leq i<j<n+1$ , we have $2\\cdot r_{i}<r_{j}$ . To this end, a first standard encoder layer uses the differences $2r_{i}-r_{j}$ as attention scores\u2014except for $j=n+1$ , where the attention score will be 0. This is achieved by setting the attention score for positions $i,j$ to $2\\pmb{u}_{i}[2]\\cdot\\pmb{u}_{j}[1]-\\pmb{u}_{j}$ [2]. Indeed, this evaluates to $2r_{i}-r_{j}$ for $i<j<n+1$ , and to 0 for $i<j=n+1$ . In particular, for a position $i\\in[1,n]$ , the attention score is maximized at $j=n+1$ if and only if $2r_{i}<r_{j}$ for all $j\\in[\\bar{i}+1,n]$ . ", "page_idx": 6}, {"type": "text", "text": "The output vector $\\pmb{v}_{i}$ at position $i$ is then set to $a_{i}[1]$ , where $\\mathbf{a}_{i}$ is the attention vector at position $i$ . Thus, the output vector has dimension 1, and for each $i\\in[1,n+1]$ , we have ${\\pmb v}_{i}=0$ if and only if $2\\cdot r_{i}<r_{j}$ holds for all $j\\in[i+1,n]$ . ", "page_idx": 6}, {"type": "text", "text": "In a second standard encoder layer we now check whether all $\\pmb{v}_{i}$ have value 0. To this end, we choose for $i\\,<\\,j\\,\\leq\\,n+1$ the attention score $\\pmb{v}_{j}$ . Let $b_{i}$ denote the attention vector at position $i$ . Then $b_{i}=0$ iff $\\pmb{v}_{i+1},\\ldots,\\pmb{v}_{n}$ are all 0. We then output $\\pmb{w}_{i}\\,=\\,1\\,-\\,(\\pmb{v}_{i}+\\pmb{b}_{i})$ , which is positive if and only if $\\pmb{v}_{i}=\\cdot\\cdot\\cdot=\\pmb{v}_{n+1}=0$ . Finally, with the acceptance vector $\\pmb{t}=1$ we accept if and only if $w_{1}>0$ , which is equivalent to $\\pmb{v}_{1}=\\cdot\\cdot\\cdot=\\pmb{v}_{n+1}=0$ . As we saw above, the latter holds if and only if $2r_{i}<r_{j}$ for all $i,j$ with $1\\leq i<j<n+1$ . \u53e3 ", "page_idx": 6}, {"type": "text", "text": "The proof of non-regularity of Double is easy (see Appendix C). One could also easily show that Double cannot be recognized by other existing models in the literature of formal language theory over infinite alphabets, e.g., register automata [4, 9, 24, 36, 41], variable/parametric automata [16, 19, 23], and data automata variants [5, 17]. For example, for register automata over $\\left(\\mathbb{Q};<\\right)$ (see the book [4]), one could use the result therein that data sequences accepted by such an automaton are closed under any order-preserving map of the elements in the sequence (e.g., if $1,2,3$ is accepted, then so is 10, 11, 20), which is not satisfied by Double. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "5 Logical languages accepted by UHAT ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section we show that an extension of linear temporal logic (LTL) with linear rational arithmetic (LRA) and unary numerical predicates is expressible in UHAT over data sequences (Theorem 3). A formula of dimension $d>0$ in locally testable $L T L$ ( $\\mathrm{[LT)^{2}L})$ ) has the following syntax: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\varphi::=\\psi_{k}(\\pmb{x}_{1},\\pmb{\\mathscr{s}}_{1},\\pmb{\\mathscr{s}}_{k+1})\\mid\\Theta\\mid\\neg\\varphi\\mid\\varphi\\vee\\varphi\\mid X\\varphi\\mid\\varphi U\\varphi\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Here, $\\psi_{k}$ for $k\\geq0$ is an atom in LRA over the $d$ -dimensional vectors of variables $\\pmb{x}_{i}$ of the form $\\langle a,(x_{1},\\ldots,x_{k+1})\\rangle+b>0$ where $\\pmb{a}\\in\\mathbb{Q}^{d(k+1)}$ and $b\\in\\mathbb{Q}$ . Intuitively, $\\psi_{k}$ allows one to check the values in the sequence with $k$ \u201clookaheads\u201d. Furthermore, $\\Theta$ is a unary numerical predicate, i.e. a family of functions $\\theta_{n}\\colon\\{1,\\ldots,n\\}\\rightarrow\\{0,1\\}$ for all $n\\geq1$ . We define the satisfaction of an $\\mathrm{(LT)^{2}L}$ formula $\\varphi$ over a sequence $\\bar{\\b{v}}=(\\pmb{v}_{1},\\dots,\\pmb{v}_{n})$ of vectors in $\\mathbb{Q}^{d}$ at position $i\\in[1,n]$ , written $({\\bar{\\pmb v}},i)\\,\\vDash\\,\\varphi$ , inductively as follows (omitting negation and disjunction): ", "page_idx": 7}, {"type": "text", "text": "\u2022 $({\\bar{v}},i)\\left|=\\psi_{k}({\\pmb x}_{1},\\dots,{\\pmb x}_{k+1})\\right.$ iff $i\\le n-k$ and $\\psi_{k}(\\pmb{v}_{i},\\dots,\\pmb{v}_{i+k})$   \n\u2022 $(\\bar{\\pmb{v}},i)\\,\\v{\\in}\\,\\Theta$ iff $\\theta_{n}(i)=1$   \n\u2022 $({\\bar{\\pmb{v}}},i)\\ \\!\\left|=X\\varphi\\right.$ iff $i<n$ and $({\\bar{v}},i+1)\\vDash\\varphi$   \n\u2022 $(\\bar{\\boldsymbol{v}},i)\\models\\varphi\\boldsymbol{U}\\boldsymbol{\\psi}$ iff there is $j\\in[i,n]$ with $(\\bar{\\pmb{v}},j)\\models\\psi$ and $(\\bar{\\pmb{v}},\\boldsymbol{k})\\models\\varphi$ for all $k\\in[i,j-1]$ ", "page_idx": 7}, {"type": "text", "text": "We write $L(\\varphi):=\\{\\bar{v}\\in(\\mathbb{Q}^{d})^{+}\\mid(\\bar{v},1)\\mid=\\varphi\\}$ for the language of $\\varphi$ . ", "page_idx": 7}, {"type": "text", "text": "Example 14. Consider sequences of the form ${\\pmb x},A{\\pmb x},A^{2}{\\pmb x},\\ldots,A^{n}{\\pmb x}$ such that ${\\pmb y}A^{n}{\\pmb x}\\,=\\,0$ and $n~\\ge~0$ is minimal with this property where $\\pmb{y}\\,\\in\\,\\mathbb{Q}^{1\\times d}$ and $A\\,\\in\\,\\mathbb{Q}^{d\\times d}$ are fixed and $\\pmb{x}\\in\\mathbb{Q}^{d}$ . Theorem 3 implies that this language is accepted by a UHAT since it is defined by the $(L T)^{2}L$ formula $G[(-L a s t\\to({\\pmb y}{\\pmb x}_{1}\\neq0\\land A{\\pmb x}_{1}={\\pmb x}_{2})$ )) \u2227 $(L a s t\\to y x_{1}=0)$ ], where $L a s t:=\\neg X^{\\top}$ . ", "page_idx": 7}, {"type": "text", "text": "Example 15. Take the standard notion of 7-day Simple Moving Average (7-SMA); this can be generalized to larger sliding windows of 50-days, or 100 days, which are often used in finance. Using $(L T)^{2}L,$ it is easy to show that the following notion of \u201cuptrend\u201d can be captured using UHAT: sequences of numbers such that the value at each time $t$ is greater than the 7-SMA value at time $t$ . The formula for this is: ", "page_idx": 7}, {"type": "equation", "text": "$$\nG(X^{7}{\\top}\\rightarrow\\varphi(x_{1},\\ldots,x_{7}))\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\varphi({\\bar{x}})$ is the formula $\\textstyle7x_{7}>\\sum_{i=1}^{7}x_{i}$ . Note here that $G\\psi$ means (as usual for LTL) \u201cglobally\u201d $\\psi$ , which can be written as $\\neg(\\top U\\neg\\psi)$ . Similarly, $X^{i}$ means that $X$ is repeated i times. ", "page_idx": 7}, {"type": "text", "text": "We assume UHATs with positional encoding and a zero vector at the end of the input sequence (see Section 2). In the following we always assume that the components from the positional encoding are implicitly given and are not changed by any UHAT. So we write the sequence in Eq. (1) as $\\pmb{v}_{1},\\dots,\\pmb{v}_{n},\\mathbf{0}$ . We use the following results from [3] that also hold for UHATs over data sequences. ", "page_idx": 7}, {"type": "text", "text": "Lemma 16. Let $d>0$ and $\\ell\\in[1,d]$ . ", "page_idx": 7}, {"type": "text", "text": "1) For every $b~\\in~\\{0,1\\}$ there is a UHAT with positional encoding that on every sequence $\\pmb{v}_{1},\\ldots,\\pmb{v}_{n}\\ \\in\\ \\mathbb{Q}^{d}$ with $\\upsilon_{i}[\\ell]~\\in~\\{0,1\\}$ for all $i\\ \\in\\ [1,n]$ outputs the sequence $\\displaystyle v_{1},\\ldots,v_{n-1},(v_{n}[1,\\ell-1],b,v_{n}[\\ell+1,d])$ .   \n2) There is a UHAT layer with positional encoding that on every sequence ${\\pmb v}_{1},\\dots,{\\pmb v}_{n}\\in\\mathbb{Q}^{d}$ and for every $i\\in[1,n-1]$ picks attention vector $\\pmb{a}_{i}=\\pmb{v}_{i+1}$ .   \n3) There is a UHAT layer with positional encoding that on every sequence $v_{1},\\ldots,v_{n}\\in\\mathbb{Q}^{d}$ , for every $\\ell\\in[1,d]$ with $v_{1}[\\ell],\\ldots,v_{n-1}[\\ell]\\in\\{0,1\\}$ and ${\\pmb v}_{n}[{\\boldsymbol\\ell}]=0$ , and for every $i\\in[1,n]$ picks attention vector $\\mathbf{\\Delta}\\mathbf{a}_{i}=\\mathbf{v}_{j}$ with minimal $j\\in[i,n]$ such that ${\\pmb v}_{j}[{\\boldsymbol\\ell}]=0$ . ", "page_idx": 7}, {"type": "text", "text": "Here, 2) and 3) directly follow from [3]. For 1) we remark that in [3] only the case $b\\,=\\,0$ is shown. On input $\\pmb{v}_{1},\\dots,\\pmb{v}_{n}$ as in 1), the UHAT uses positional encoding function $p(i,n):=(i,n)$ ", "page_idx": 7}, {"type": "text", "text": "and a composition of affine transformations and ReLU to output at position $i\\in[1,n]$ the vector $({\\pmb v}_{i}[1,\\ell-1],b_{i},{\\pmb v}_{i}[\\ell+1,d])$ where $b_{i}:=\\operatorname*{min}\\{{\\pmb v}_{i}[\\ell],n-i\\}$ if $b=0$ and $b_{i}:=\\operatorname*{max}\\{\\bar{v}_{i}[\\ell],i-n+1\\}$ if $b=1$ . ", "page_idx": 8}, {"type": "text", "text": "Using Lemma 16, we show that a UHAT can transform rational values $>0$ to 1 and values $\\leq0$ to 0.   \nThis will be used to evaluate inequalities by outputting 1 for true and 0 for false. ", "page_idx": 8}, {"type": "text", "text": "Lemma 17. Let $d>0$ and $\\ell\\,\\in\\,[1,d]$ . There is a UHAT with positional encoding that on every sequence $\\mathbf{v}_{1},\\ldots,\\mathbf{v}_{n+1}\\in\\mathbb{Q}^{d}$ outputs $\\mathbf{\\overline{{v}}}_{1}^{\\prime},\\dots,\\mathbf{v}_{n+1}^{\\prime}\\in\\mathbb{Q}^{d}$ with $\\pmb{v}_{i}^{\\prime}:=(\\pmb{v}_{i}[1,\\ell-1],b_{i},\\pmb{v}_{i}[\\ell+1,d])$ for all $i\\in[1,n+1]$ where $b_{i}:=1$ if ${\\pmb v}_{i}[{\\boldsymbol\\ell}]>0$ and $b_{i}:=0$ otherwise. ", "page_idx": 8}, {"type": "text", "text": "Proof. On input $\\mathbf{\\psi}_{1},\\,.\\,.\\,.\\,,\\mathbf{\\psi}_{v_{n+1}}\\in\\,\\mathbb{Q}^{d}$ , the first layer outputs at position $i\\in[1,n+1]$ the vector $\\pmb{w}_{i}\\,:=\\,(\\pmb{v}_{i}[1,\\ell-1],r_{i},\\pmb{v}_{i}[\\ell+1,d])$ where $r_{i}:=\\,\\operatorname*{max}\\{\\pmb{v}_{i}[\\ell],0\\}$ . Thus, $r_{i}\\,=\\,0$ if ${\\pmb v}_{i}[{\\pmb\\ell}]\\,\\leq\\,0$ and $r_{i}>0$ otherwise. The second layer turns the sequence $w_{1},\\ldots,w_{n+1}$ into $(0,{\\pmb w}_{1}),\\dots,(0,{\\pmb w}_{n+1})$ . We then apply 1) of Lemma 16 to obtain the sequence $(0,{\\pmb w}_{1}),\\dots,(0,{\\pmb w}_{n}),(1,{\\pmb w}_{n+1})$ , i.e. the last vector has first component 1, and all other vectors have first component 0. Let $u_{1},\\ldots,u_{n+1}\\in$ $\\mathbb{Q}^{d+1}$ be the resulting sequence. The final layer uses attention score $\\langle A\\pmb{u}_{i},B\\pmb{u}_{j}\\rangle$ for all $1\\ \\leq$ $i,j\\leq n+1$ where the affine transformations $A,B\\colon\\mathbb{Q}^{d+1}\\to\\mathbb{Q}^{d+1}$ yield $A\\pmb{u}_{i}=(\\pmb{u}_{i}[\\ell],0,\\ldots,0)$ and $B\\pmb{u}_{j}\\,=\\,(\\pmb{u}_{j}[1],0,\\ldots,0)$ . Let $\\mathbf{}a_{i}$ be the attention vector of position $i\\,\\in\\,[1,n+1]$ . Since ${\\pmb u}_{i}[{\\boldsymbol\\ell}]\\,\\geq\\,0$ , we have $a_{i}[1]\\;=\\;0$ if ${\\pmb u}_{i}[{\\boldsymbol\\ell}]\\;=\\;0$ and $a_{i}[1]\\;=\\;1$ if $\\mathbf{\\bar{\\mu}}\\mathbf{\\mu}\\mathbf{\\bar{\\mu}}\\mathbf{\\Lambda}\\geq\\mathbf{\\mu}0$ . The layer outputs $\\pmb{v}_{i}^{\\prime}:=(\\pmb{u}_{i}[2,\\ell],\\pmb{a}_{i}[1]$ , ${\\pmb u}_{i}[\\ell+2,d+1])$ at position $i\\in[1,n+1]$ . \u53e3 ", "page_idx": 8}, {"type": "text", "text": "We now prove Theorem 3. We claim that for every $\\mathrm{(LT)^{2}L}$ formula $\\varphi$ of dimension $d$ and every $m\\geq d$ there exists a UHAT $T_{\\varphi,m}$ with positional encoding that on every sequence $\\pmb{w}_{1},\\dots,\\pmb{w}_{n},\\mathbf{0}\\in\\mathbb{Q}^{m}$ outputs a sequence $\\pmb{w}_{1}^{\\prime},\\dots,\\pmb{w}_{n}^{\\prime},\\pmb{0}\\in\\mathbb{Q}^{m+1}$ such that for all $i\\in[1,n]$ we have $\\pmb{w}_{i}^{\\prime}[1,m]=\\pmb{w}_{i}$ and $\\pmb{w}_{i}^{\\prime}[m+1]=1$ if $({\\bar{\\pmb v}},i)\\,\\vDash\\,\\varphi$ and $\\pmb{w}_{i}^{\\prime}[m+1]=0$ otherwise, where $\\bar{\\pmb{v}}:=(\\pmb{w}_{1}[1,d],\\dots,\\pmb{w}_{n}[1,d])$ . Then the theorem follows since for every $(\\mathrm{LT})^{2}\\mathrm{L}$ formula $\\varphi$ of dimension $d$ the UHAT $T_{\\varphi,d}$ outputs on every sequence $\\bar{\\b{v}}=(\\pmb{v}_{1},\\dots,\\pmb{v}_{n})$ of vectors in $\\mathbb{Q}^{d}$ extended with the vector $\\mathbf{0}\\in\\mathbb{Q}^{d}$ a sequence $\\ensuremath{\\mathbf{{v}}}_{1}^{\\prime},\\ldots,\\ensuremath{\\mathbf{{v}}}_{n}^{\\prime},\\ensuremath{\\mathbf{{0}}}\\in\\mathbb{Q}^{d+1}$ such that $\\pmb{v}_{1}^{\\prime}[d+1]>0$ if and only if $(\\bar{\\pmb{v}},1)\\,\\v{=}\\,\\varphi$ . Thus, $T_{\\varphi,d}$ accepts $L(\\varphi)$ by taking the acceptance vector $\\pmb{t}:=(0,\\dots,0,1)\\in\\mathbb{Q}^{d+1}$ . ", "page_idx": 8}, {"type": "text", "text": "We prove the claim by induction on the structure of $(\\mathrm{LT})^{2}\\mathrm{L}$ formulas. If the formula is a unary numerical predicate $\\Theta$ , then we can use the positional encoding $p(i,n+1):=\\theta_{n}(i)$ for all $i\\in$ $[1,n]$ and $p(n+1,n+1):=0$ to output on every sequence $\\pmb{w}_{1},\\dots,\\pmb{w}_{n},\\mathbf{0}\\in\\mathbb{Q}^{m}$ the sequence $({\\pmb w}_{1},p(1,n+1)),\\dots,({\\pmb w}_{n},p(n,n+1)),({\\pmb0},p(n+1))$ + 1, n + 1)) \u2208Qm+1. ", "page_idx": 8}, {"type": "text", "text": "If the formula is an atom $\\psi_{k}(\\pmb{x}_{1},\\dots,\\pmb{x}_{k+1})$ of the form $\\langle a,(x_{1},\\ldots,x_{k+1})\\rangle+b>0$ , the UHAT $T_{\\psi_{k},m}$ adds in its first layer a component that is set to 1 to the top of every vector, outputting on every sequence $w_{1},\\dots,w_{n}$ , $\\mathbf{0}\\in\\mathbb{Q}^{m}$ the sequence $(1,{\\pmb w}_{1}),\\dots,({\\bar{1}},{\\pmb w}_{n}),({\\bar{1}},{\\pmb0})$ . Then we apply 1) of Lemma 16 to turn this sequence into $(1,{\\pmb w}_{1}),\\dots,(1,{\\pmb w}_{n}),{\\bf0}$ . Next, $T_{\\psi_{k},m}$ uses $k$ layers to allow each position to gather the first $d+1$ components of its $k$ right neighbors. More precisely, the $\\ell_{}$ -th layer, for $\\ell\\in[1,k]$ , on sequence $u_{1},\\ldots,u_{n+1}$ uses 2) of Lemma 16 to get for every position $i\\in[1,n]$ the attention vector $\\pmb{a}_{i}=\\pmb{u}_{i+1}$ and the attention vector $\\pmb{a}_{n+1}$ is arbitrary. Note that if $\\ell=1$ , then $\\textstyle\\mathbf{a}_{n}=\\mathbf{0}$ . Then it applies an affine transformation to output at position $i\\in[1,n]$ the vector $(\\pmb{a}_{i}[1,d+1],\\pmb{u}_{i})$ and using 1) at position $n+1$ the vector $(0,a_{n+1}[2,d+1],{\\pmb u}_{n+1})$ ). Let $u_{1},\\ldots,u_{n+1}\\in\\mathbb{Q}^{k(d+1)+m+1}$ be the output of the $k$ -th of those layers. We add another layer that using a composition of ReLU and affine functions outputs at every position $i\\in[1,n+1]$ the vector $\\pmb{u}_{i}^{\\prime}:=(\\pmb{u}_{i}[\\bar{k(d+1)}+2,k(d+1)+m+1],r_{i})\\in\\mathbb{Q}^{\\bar{m}+1}$ where $r_{i}:=\\operatorname*{min}\\{u_{i}[1],\\langle a,\\hat{\\mathbf{u}}_{i}\\rangle+b\\}$ and ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{u}_{i}:=(u_{i}[2,d+1],u_{i}[d+3,2(d+1)],\\dots,u_{i}[k d+k+2,(k+1)(d+1)]),}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "which contains the first $d$ components of the initial input vector and its $k$ right neighbors. That is, for all $i\\in[1,n]$ we have that $r_{i}=0$ if $\\langle\\pmb{a},\\hat{\\pmb{u}}_{i}\\rangle+b\\leq0$ or $i+k>n$ since $\\pmb{u}_{i}[1]$ can only be 0 if it was gathered from the vector at position $n+1$ using attention. Furthermore, $r_{i}>0$ if $\\langle\\pmb{a},\\hat{\\pmb{u}}_{i}\\rangle+b>0$ and $i+k\\le n$ . Note that $u_{i}^{\\prime}[1,m]$ is equal to the input vector $\\boldsymbol{w}_{i}$ from the beginning if $i\\in[1,n]$ and 0 if $i=n+1$ . Finally, we apply Lemma 17 followed by 1) to output at position $i\\in[1,n]$ the vector $\\pmb{w}_{i}^{\\prime}:=(\\pmb{u}_{i}^{\\prime}[1,m],r_{i}^{\\prime})$ where $r_{i}^{\\prime}:=1$ if $r_{i}>0$ and $r_{i}^{\\prime}:=0$ otherwise and at position $n+1$ the vector $\\pmb{w}_{n+1}^{\\prime}:=(\\pmb{u}_{n+1}^{\\prime}[1,m],0)=\\mathbf{0}$ . Thus, for all $i\\in[1,n]$ we have that $\\pmb{w}_{i}^{\\prime}[\\stackrel{\\bar{\\ldots}}{m}+1]=1$ if $({\\bar{\\pmb{v}}},i)\\mid=\\psi_{k}$ and $\\pmb{w}_{i}^{\\prime}[m+1]=0$ otherwise, where $\\bar{\\pmb{v}}:=(\\pmb{w}_{1}[1,d],\\bar{\\dots}...,\\pmb{w}_{n}[1,d])$ . ", "page_idx": 8}, {"type": "text", "text": "Let us now continue with the inductive step where we assume that $\\varphi$ and $\\psi$ are $\\mathrm{(LT)^{2}L}$ formulas of dimension $d$ such that for all $m\\geq d$ we already showed existence of the UHATs $T_{\\varphi,m}$ and $T_{\\psi,m+1}$ . ", "page_idx": 8}, {"type": "text", "text": "For the cases $\\neg\\varphi,\\varphi\\vee\\psi$ , and $X\\varphi$ we refer to Appendix D. For $\\varphi U\\psi$ define the UHAT $T_{\\varphi U\\psi,m}$ that first applies $T_{\\psi,m+1}\\circ T_{\\varphi,m}$ outputting a sequence ${\\mathbf{\\{}}u_{1},\\dots,\\mathbf{}}{\\mathbf{}}u_{n},{\\mathbf{0}}\\in\\mathbb{Q}^{m+2}$ . Observe that $(\\bar{\\pmb{v}},i)\\ \\vert=\\varphi U\\psi$ for $i\\;\\in\\;[1,n]$ and $\\overrightharpoon{\\pmb v}\\ :=\\ (\\mathbf{\\bar{u}}_{1}[1,\\overbar{d}],\\dots,\\mathbf{\\bar{u}}_{n}[1,d])$ if and only if for the minimal $j~\\in~[i,n]$ with $(\\bar{\\boldsymbol{v}},\\boldsymbol{j})\\models\\neg\\varphi\\lor\\psi$ we have $(\\bar{\\pmb{v}},j)\\models\\psi$ and such a $j$ exists. Equivalently, for the minimal $j\\in[i,\\bar{n}+1]$ with $\\mathrm{min}\\{{\\pmb u}_{j}[m\\!+\\!1],1\\!-\\!{\\pmb u}_{j}[m\\!+\\!2]\\}=0$ it holds that $\\pmb{u}_{j}[m{+}2]=1$ . To check this, we first add a layer that outputs at position $i\\in[1,n\\!+\\!1]$ the vector $\\pmb{u}_{i}^{\\prime}:=(\\pmb{u}_{i},\\operatorname*{min}\\{\\pmb{u}_{i}[m+1],1-\\pmb{u}_{i}[m+2]\\})\\in\\mathbb{Q}^{m+3}$ . Finally, we add a layer that uses 3) of Lemma 16 to get attention vector $\\pmb{a}_{i}=\\pmb{u}_{j}^{\\prime}$ with $j\\in[i,n+1]$ minimal such that ${\\pmb u}_{j}^{\\prime}[m+3]\\,=\\,0$ . The layer then outputs at position $i\\,\\in\\,[1,n+1]$ the vector $\\pmb{w}_{i}^{\\prime}:=(\\pmb{u}_{i}^{\\prime}[1,m],\\pmb{a}_{i}[\\bar{m}+2])\\in\\mathbb{Q}^{m+1}$ . ", "page_idx": 9}, {"type": "text", "text": "6 Concluding remarks ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We initiated the study of the expressive power of transformers, when the input is a sequence of (tuples of) numbers, which is the setting for applications like time series analysis/forecasting. Our results indicate an increased expressiveness of transformers on such input data, in comparison to the previous formal language theoretic setting (see survey [40]), i.e., when a token embedding function (with a bounded number of tokens) is first applied before feeding the input to a transformer. More precisely, this represents for Unique Hard Attention Transformers (UHAT) a jump from the complexity class $\\mathbf{A}\\mathbf{C}^{0}$ to $\\mathrm{T}\\mathrm{C}^{0}$ (since ${\\mathrm{AC}}^{0}\\,\\mathsf{\\Sigma}\\subsetneq\\mathrm{TC}^{0}\\mathrm{)}$ , and the jump from regular to non-regular languages (when position encoding is not allowed). On the positive side, we successfully developed an expressive class of logical languages recognized by UHAT in terms of a logic called locally testable LTL, which extends previously identified logic for UHAT for strings over finite alphabets [2, 3]. ", "page_idx": 9}, {"type": "text", "text": "Limitations. While we follow the standard formalization of transformer encoders in Formal Languages and Neural Networks (e.g. [3, 20, 21, 40]), limitations of the models are known (see [40] for a thorough discussion). For example, used real numbers could be of unbounded precision, which allow one to precisely represent values of sin and cos functions (actually used in practice for positional encoding). In addition, the positional encoding used by the model could be uncomputable. Three answers can be given. First, an upper bound complexity on the model with unbounded precision and arbitrary positional encodings (e.g. in $\\mathbf{T}\\mathbf{C}^{0}$ ) still applies in the case of bounded precision. Second, limiting the power of UHAT (e.g. allow only rational numbers, and assuming efficient (i.e. uniform $\\mathbf{T}\\mathbf{C}^{0}$ ) computability of the positional encoding $p:\\mathbb{N}\\times\\mathbb{N}\\rightarrow\\mathbb{Q}^{d},$ ), our proof in fact yields uniformity of our $\\mathbf{T}\\mathbf{C}^{\\bar{0}}$ upper bound. Third, our lower bound for non-regularity of UHAT (cf. Theorem 2) holds even with only rational numbers and no positional encodings. Finally, to alleviate these issues, we have always made an explicit distinction between UHAT with and without positional encodings. ", "page_idx": 9}, {"type": "text", "text": "Future directions. Our paper opens up a plethora of research avenues on the expressive power of transformers on data sequences. In particular, one could consider other transformer encoder models that have been considered in the formal language theoretic setting to transformers (see the survey [40]). For example, instead of unique hard attention mechanism, we could consider the expressive power of transformers on data sequences using average hard attention mechanism. Similar question could be asked if we use a softmax function instead of a hard attention, which begs the question of which numerical functions could be computed in different circuit complexity classes like $\\mathbf{T}\\mathbf{C}^{0}$ . Another important question concerns a logical characterization for UHAT over sequences of numbers. This is actually still an open question even for the case of finite alphabets. Barcelo et al. [3] showed that first-order logic (equivalently, LTL) with monadic numerical predicates (called LTL(Mon)) is subsumed in UHAT with arbitrary position encodings, i.e., the transformer model that we are generalizing in this paper to sequences of numbers. There are UHAT languages (e.g. the set of palindromes) that are not captured by this. As remarked in [3], LTL(Mon) can be extended with arbitrary linear orders on the positions (parameterized by lengths), which then can define the palindromes. [An analogous extension for $(\\mathrm{L}\\dot{\\mathrm{T}})^{2}\\mathrm{L}$ can define palindromes over an infinite alphabet.] However, it is possible to show that the resulting logic is still not expressive enough to capture the full generality of UHAT. That said, although our logic does not capture the full UHAT, it can still be used to see at a glance what languages can be recognized by UHAT (e.g. Simple Moving Averages). There could perhaps be a hope of obtaining a precise logical characterization if we restrict the model of UHAT. The recent paper [2] showed that LTL(Mon) captures precisely the languages of masked UHAT with position encodings with \u201cfinite image\u201d. It is interesting to study similar restrictions for UHAT in the case of sequences of numbers. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments Funded by the European Union (ERC, LASD, 101089343 and FINABIS, 101077902). Views and opinions expressed are however those of the authors only and do not necessarily reflect those of the European Union or the European Research Council Executive Agency. Neither the European Union nor the granting authority can be held responsible for them. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Allender, E., B\u00fcrgisser, P., Kjeldgaard-Pedersen, J., and Miltersen, P. B. (2006). On the complexity of numerical analysis. In 21st Annual IEEE Conference on Computational Complexity (CCC 2006), 16-20 July 2006, Prague, Czech Republic, pages 331\u2013339. IEEE Computer Society.   \n[2] Angluin, D., Chiang, D., and Yang, A. (2023). Masked hard-attention transformers and boolean RASP recognize exactly the star-free languages. CoRR, abs/2310.13897.   \n[3] Barcelo, P., Kozachinskiy, A., Lin, A. W., and Podolskii, V. (2024). Logical languages accepted by transformer encoders with hard attention. In ICLR.   \n[4] Boja\u00b4nczyk, M. (2019). Slightly Infinite Sets. See https://www.mimuw.edu.pl/\\~bojan/ upload/main-10.pdf.   \n[5] Bojan\u00b4czyk, M., David, C., Muscholl, A., Schwentick, T., and Segoufin, L. (2011). Two-variable logic on data words. ACM Trans. Comput. Log., 12(4):27:1\u201327:26.   \n[6] Buss, S. R. (1992). The graph of multiplication is equivalent to counting. Inf. Process. Lett., 41(4):199\u2013201.   \n[7] Chandra, A. K., Stockmeyer, L., and Vishkin, U. (1984). Constant depth reducibility. SIAM Journal on Computing, 13(2):423\u2013439.   \n[8] Chiang, D., Cholak, P., and Pillay, A. (2023). Tighter bounds on the expressivity of transformer encoders. In ICML, volume 202, pages 5544\u20135562. PMLR.   \n[9] D\u2019Antoni, L., Ferreira, T., Sammartino, M., and Silva, A. (2019). Symbolic register automata. In Dillig, I. and Tasiran, S., editors, Computer Aided Verification - 31st International Conference, CAV 2019, New York City, NY, USA, July 15-18, 2019, Proceedings, Part I, volume 11561 of Lecture Notes in Computer Science, pages 3\u201321. Springer.   \n[10] D\u2019Antoni, L. and Veanes, M. (2017). The power of symbolic automata and transducers. In Majumdar, R. and Kuncak, V., editors, Computer Aided Verification - 29th International Conference, CAV 2017, Heidelberg, Germany, July 24-28, 2017, Proceedings, Part I, volume 10426 of Lecture Notes in Computer Science, pages 47\u201367. Springer.   \n[11] D\u2019Antoni, L. and Veanes, M. (2021). Automata modulo theories. Commun. ACM, 64(5):86\u201395.   \n[12] Devlin, J., Chang, M., Lee, K., and Toutanova, K. (2019). BERT: pre-training of deep bidirectional transformers for language understanding. In Burstein, J., Doran, C., and Solorio, T., editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 4171\u20134186. Association for Computational Linguistics.   \n[13] Dong, L., Xu, S., and Xu, B. (2018). Speech-transformer: A no-recurrence sequence-tosequence model for speech recognition. In 2018 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2018, Calgary, AB, Canada, April 15-20, 2018, pages 5884\u20135888. IEEE.   \n[14] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. (2021). An image is worth 16x16 words: Transformers for image recognition at scale. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.   \n[15] DuSell, B. and Chiang, D. (2024). Stack attention: Improving the ability of transformers to model hierarchical patterns. In Proc. ICLR, Vienna, Austria.   \n[16] Faran, R. and Kupferman, O. (2018). LTL with arithmetic and its applications in reasoning about hierarchical systems. In Barthe, G., Sutcliffe, G., and Veanes, M., editors, LPAR-22. 22nd International Conference on Logic for Programming, Artificial Intelligence and Reasoning, Awassa, Ethiopia, 16-21 November 2018, volume 57 of EPiC Series in Computing, pages 343\u2013362. EasyChair.   \n[17] Figueira, D. and Lin, A. W. (2022). Reasoning on data words over numeric domains. In LICS, pages 37:1\u201337:13. ACM.   \n[18] Garey, M. R., Graham, R. L., and Johnson, D. S. (1976). Some NP-complete geometric problems. In Chandra, A. K., Wotschke, D., Friedman, E. P., and Harrison, M. A., editors, Proceedings of the 8th Annual ACM Symposium on Theory of Computing, May 3-5, 1976, Hershey, Pennsylvania, USA, pages 10\u201322. ACM.   \n[19] Grumberg, O., Kupferman, O., and Sheinvald, S. (2010). Variable automata over infinite alphabets. In Dediu, A., Fernau, H., and Mart\u00edn-Vide, C., editors, Language and Automata Theory and Applications, 4th International Conference, LATA 2010, Trier, Germany, May 24-28, 2010. Proceedings, volume 6031 of Lecture Notes in Computer Science, pages 561\u2013572. Springer.   \n[20] Hahn, M. (2020). Theoretical limitations of self-attention in neural sequence models. Trans. Assoc. Comput. Linguistics, 8:156\u2013171.   \n[21] Hao, Y., Angluin, D., and Frank, R. (2022). Formal language recognition by hard attention transformers: Perspectives from circuit complexity. Trans. Assoc. Comput. Linguistics, 10:800\u2013 810.   \n[22] Hesse, W., Allender, E., and Barrington, D. A. M. (2002). Uniform constant-depth threshold circuits for division and iterated multiplication. J. Comput. Syst. Sci., 65(4):695\u2013716.   \n[23] Jez, A., Lin, A. W., Markgraf, O., and R\u00fcmmer, P. (2023). Decision procedures for sequence theories. In CAV (2), volume 13965 of Lecture Notes in Computer Science, pages 18\u201340. Springer.   \n[24] Kaminski, M. and Francez, N. (1994). Finite-memory automata. Theor. Comput. Sci., 134(2):329\u2013363.   \n[25] Karimov, T., Kelmendi, E., Ouaknine, J., and Worrell, J. (2022). What\u2019s decidable about discrete linear dynamical systems? In Raskin, J., Chatterjee, K., Doyen, L., and Majumdar, R., editors, Principles of Systems Design - Essays Dedicated to Thomas A. Henzinger on the Occasion of His 60th Birthday, volume 13660 of Lecture Notes in Computer Science, pages 21\u201338. Springer.   \n[26] Libkin, L., Martens, W., and Vrgoc, D. (2016). Querying graphs with data. J. ACM, 63(2):14:1\u2013 14:53.   \n[27] Lipton, R., Luca, F., Nieuwveld, J., Ouaknine, J., Purser, D., and Worrell, J. (2022). On the Skolem problem and the Skolem conjecture. In Baier, C. and Fisman, D., editors, LICS \u201922: 37th Annual ACM/IEEE Symposium on Logic in Computer Science, Haifa, Israel, August 2 - 5, 2022, pages 5:1\u20135:9. ACM.   \n[28] Liu, Y., Hu, T., Zhang, H., Wu, H., Wang, S., Ma, L., and Long, M. (2023). itransformer: Inverted transformers are effective for time series forecasting. CoRR, abs/2310.06625. To appear in ICLR\u201924.   \n[29] Luca, F., Maynard, J., Noubissie, A., Ouaknine, J., and Worrell, J. (2023). Skolem meets Bateman-Horn. CoRR, abs/2308.01152.   \n[30] Merrill, W., Sabharwal, A., and Smith, N. A. (2022). Saturated transformers are constant-depth threshold circuits. Trans. Assoc. Comput. Linguistics, 10:843\u2013856.   \n[31] Mishra, B. (1993). Algorithmic Algebra. Texts and Monographs in Computer Science. Springer.   \n[32] P\u00e9rez, J., Barcel\u00f3, P., and Marinkovic, J. (2021). Attention is turing-complete. J. Mach. Learn. Res., 22:75:1\u201375:35.   \n[33] Reif, J. H. (1987). On threshold circuits and polynomial computation. In Proceedings of the Second Annual Conference on Structure in Complexity Theory, Cornell University, Ithaca, New York, USA, June 16-19, 1987, pages 118\u2013123. IEEE Computer Society.   \n[34] Reif, J. H. and Tate, S. R. (1992). On threshold circuits and polynomial computation. SIAM J. Comput., 21(5):896\u2013908.   \n[35] Schrijver, A. (1986). Theory of linear and integer programming. John Wiley & Sons.   \n[36] Segoufin, L. and Torun\u00b4czyk, S. (2011). Automata based verification over linearly ordered data domains. In Schwentick, T. and D\u00fcrr, C., editors, 28th International Symposium on Theoretical Aspects of Computer Science, STACS 2011, March 10-12, 2011, Dortmund, Germany, volume 9 of LIPIcs, pages 81\u201392. Schloss Dagstuhl - Leibniz-Zentrum f\u00fcr Informatik.   \n[37] Sipser, M. (1997). Introduction to the Theory of Computation. PWS Publishing Company.   \n[38] Strobl, L. (2023). Average-hard attention transformers are constant-depth uniform threshold circuits. CoRR, abs/2308.03212.   \n[39] Strobl, L., Angluin, D., Chiang, D., Rawski, J., and Sabharwal, A. (2024). Transformers as transducers. CoRR, abs/2404.02040.   \n[40] Strobl, L., Merrill, W., Weiss, G., Chiang, D., and Angluin, D. (2023). Transformers as recognizers of formal languages: A survey on expressivity.   \n[41] Toru\u00b4nczyk, S. and Zeume, T. (2022). Register automata with extrema constraints, and an application to two-variable logic. Log. Methods Comput. Sci., 18(1).   \n[42] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. (2017). Attention is all you need. In NeurIPS, pages 5998\u20136008.   \n[43] Veanes, M., Hooimeijer, P., Livshits, B., Molnar, D., and Bj\u00f8rner, N. S. (2012). Symbolic finite state transducers: algorithms and applications. In Field, J. and Hicks, M., editors, Proceedings of the 39th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, POPL 2012, Philadelphia, Pennsylvania, USA, January 22-28, 2012, pages 137\u2013150. ACM.   \n[44] Vollmer, H. (1999). Introduction to Circuit Complexity. Springer.   \n[45] Wen, Q., Zhou, T., Zhang, C., Chen, W., Ma, Z., Yan, J., and Sun, L. (2023). Transformers in time series: A survey. In IJCAI, pages 6778\u20136786. ijcai.org.   \n[46] Zhou, H., Zhang, S., Peng, J., Zhang, S., Li, J., Xiong, H., and Zhang, W. (2021). Informer: Beyond efficient transformer for long sequence time-series forecasting. In Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021, pages 11106\u201311115. AAAI Press. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Example UHAT with real parameters ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We present here an example UHAT in which two real numbers $\\alpha$ and $\\beta$ occur (each occurs once in a matrix associated to a particular layer) such that the simple sequence ", "page_idx": 13}, {"type": "equation", "text": "$$\n(1,0,{e}_{1})(1,0,{e}_{2})(1,0,{e}_{3}),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $e_{i}\\in\\mathbb{R}^{3}$ is the $i$ -th unit vector, is accepted if and only if $\\alpha\\beta=2$ and $\\alpha=\\beta$ . This shows that even if we restrict the input sequence to a particular number $B$ of bits (e.g. the number of bits to represent the input sequence Eq. (2)), it is not possible to replace the real constants $\\alpha$ and $\\beta$ by rational numbers without c\u221ahanging the accepted sequences of up to $B$ bits: The sequence Eq. (2) is only accepted if $\\alpha=\\beta={\\sqrt{2}}$ . And if we change $\\alpha$ or $\\beta$ in any way, the sequence Eq. (2) will not be accepted anymore. ", "page_idx": 13}, {"type": "text", "text": "1. Input layer: ", "page_idx": 13}, {"type": "equation", "text": "$$\n(1,0,e_{1})(1,0,e_{2})(1,0,e_{3}).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "2. Using a standard encoding layer, we multiply the first component in each position by $\\alpha$ . Result: ", "page_idx": 13}, {"type": "equation", "text": "$$\n(\\alpha,0,{e}_{1})(\\alpha,0,{e}_{2})(\\alpha,0,{e}_{3}).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "3. Using attention, we can apply distinct affine transformations to the first three positions. We choose the following affine transformations. The first position is unchanged. The second position is mapped to $(1,0,e_{2})$ , and the third position is mapped to $\\left(0,\\alpha,e_{3}\\right)$ , using a matrix that flips the first and second component. Result: ", "page_idx": 13}, {"type": "equation", "text": "$$\n(\\alpha,0,{e}_{1})(1,0,{e}_{2})(0,\\alpha,{e}_{3}).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "4. Using a standard encoding layer, we multiply the first component in each position with $\\beta$ . Result: ", "page_idx": 13}, {"type": "equation", "text": "$$\n(\\alpha\\beta,0,e_{1})(\\beta,0,e_{2})(0,\\alpha,e_{3}).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "5. Finally, by using our result on $(\\mathrm{LT})^{2}\\mathrm{L}$ , we can build further layers so that we accept if and only if (i) the first component of the first position equals 2 and (ii) the first component of the second position equals the second component of the third position. Thus, we accept our original input if and only if $\\alpha\\beta=2$ and $\\alpha=\\beta$ . ", "page_idx": 13}, {"type": "text", "text": "B Omitted definitions and proofs in Section 3 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "B.1 Descriptional size ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Let $x\\in\\mathbb R$ be some real number. The (descriptional) size of $x$ is $\\mathrm{size}(x)=1+\\lceil\\log_{2}(|p|+1)\\rceil+$ $\\lceil\\log_{2}(|q|+1)\\rceil$ if $x\\,=\\,{\\frac{p}{q}}$ is a rational number (where $p$ and $q$ are relatively prime integers) and $\\mathrm{size}(x)=1$ if $x$ is an irrational number. Note that in the latter case we use the number 1 as some placeholder, since we do not have to represent irrational numbers in any algorithm. However, for analysis of the sizes in our constructions we still need some value. ", "page_idx": 13}, {"type": "text", "text": "Let $\\pmb{v}\\in\\mathbb{R}^{d}$ be a vector. The size of $\\pmb{v}$ is $\\begin{array}{r}{\\mathrm{size}(v)=n+\\sum_{i=1}^{d}\\mathrm{size}(v_{i})}\\end{array}$ where $\\pmb{v}=(v_{1},\\ldots,v_{d})^{T}$ . ", "page_idx": 13}, {"type": "text", "text": "Let $M\\in\\mathbb{R}^{m\\times n}$ be a matrix. The size of $M$ is $\\begin{array}{r}{\\mathrm{size}(M)=m n+\\sum_{1\\leq i\\leq m,1\\leq j\\leq n}\\mathrm{size}(a_{i j})}\\end{array}$ where M = (aij)1\u2264i\u2264m,1\u2264j\u2264n. ", "page_idx": 13}, {"type": "text", "text": "Let $A\\colon\\ensuremath{\\mathbb{R}}^{d}\\to\\ensuremath{\\mathbb{R}}^{e}$ be an affine transformation, i.e., we have $A(\\pmb{x})=B\\pmb{x}+\\pmb{c}$ with $B\\in\\mathbb{R}^{d\\times e}$ and $c\\in\\mathbb{R}^{e}$ . Then the size of $A$ is $\\operatorname{size}(A)=\\operatorname{size}(B)+\\operatorname{size}(c)+1$ . ", "page_idx": 13}, {"type": "text", "text": "Now, let $\\begin{array}{r l r}{p}&{{}\\in}&{\\mathbb{R}[X_{1},\\ldots,X_{n}]}\\end{array}$ be a polynomial. Then we have $\\begin{array}{r l}{p(X_{1},\\ldots,X_{n})}&{{}=}\\end{array}$ $\\begin{array}{r}{\\sum_{0\\leq r_{1},\\dots,r_{n}\\leq k}c_{r_{1},\\dots,r_{n}}X_{1}^{r_{1}}\\cdot\\cdot\\cdot X_{n}^{r_{n}}}\\end{array}$ for some numbers $k\\,\\in\\,\\mathbb{N}$ and $c_{r_{1},\\ldots,r_{n}}\\,\\in\\,\\mathbb{R}$ . The size of $p$ is ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathrm{size}(p)=\\sum_{0\\leq r_{1},\\ldots,r_{n}\\leq k}\\mathrm{size}(c_{r_{1},\\ldots,r_{n}})+\\mathrm{size}(r_{1})+\\cdots+\\mathrm{size}(r_{n})+n\\,.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Let $\\alpha$ be a polynomial constraint. We define the size of $\\alpha$ inductively on the structure of the formula as follows: ", "page_idx": 13}, {"type": "text", "text": "\u2022 if $\\alpha=(p(X_{1},\\ldots,X_{n})\\sim0)$ with $\\sim\\in\\{>,\\geq\\}$ is an atom. Then ${\\mathrm{size}}(\\alpha)={\\mathrm{size}}(p)+1$ . \u2022 if $\\alpha=\\wedge_{1\\leq i\\leq k}\\beta_{i}$ or $\\alpha=\\bigvee_{1\\leq i\\leq k}\\beta_{i}$ is a formula with PCs $\\beta_{1},\\ldots,\\beta_{k}$ , then $\\mathrm{size}(\\alpha)=$ $\\begin{array}{r}{k+\\sum_{1\\leq i\\leq k}\\overline{{\\mathrm{size}(()\\beta_{i})}}}\\end{array}$ . ", "page_idx": 14}, {"type": "text", "text": "Let $R\\ =\\ (\\phi_{1},D_{1}),\\ldots,(\\phi_{k},D_{k})$ be a CPR. Then the size of this CPR is $\\mathrm{size}(R)~=~k~+$ $\\begin{array}{r}{\\sum_{i=1}^{k}\\mathrm{size}(\\phi_{i})+\\mathrm{size}(D_{k})}\\end{array}$ . ", "page_idx": 14}, {"type": "text", "text": "B.2 ReLU case in Lemma 6 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Let us now consider a ReLU layer. Assume that we compute the ReLU-value for the $j$ -th component, i.e., we compute $\\operatorname*{max}\\{0,x_{i,j}\\}$ for each $i\\in\\{1,\\ldots,n^{+}+1\\}$ where $x_{i,j}$ is the $j$ -th component of $\\pmb{x}_{i}$ . From each conditional assignment $\\varphi_{i,k}\\rightarrow D_{i,k}$ with $i\\in\\{1,\\ldots,\\bar{n}+1\\}$ and $1\\le k\\le s_{i}$ we construct two new conditional assignments: ", "page_idx": 14}, {"type": "text", "text": "1. $\\langle e_{(i-1)\\cdot n+j},D_{i,k}\\pmb{\\bar{x}}\\rangle\\geq0\\rightarrow D_{i,k}$ where $e_{h}$ is the $h$ -th unit vector.   \n2. $\\langle-e_{(i-1)\\cdot n+j},D_{i,k}\\pmb{\\bar{x}}\\rangle>0\\rightarrow M D_{i,k}$ where $M=(m_{g h})_{1\\leq g,h\\leq d^{\\prime}\\cdot(n+1)}$ is the matrix with $m_{g h}=1$ if $g=h\\neq(i-1)\\cdot n+j$ and $m_{g h}=0$ otherwise (i.e., $M$ is the unit matrix except for the $(i-1)\\cdot j$ -th entry). ", "page_idx": 14}, {"type": "text", "text": "Now, if the $j$ -th component of $\\pmb{x}_{i}$ is non-negative, only the first conditional assignment is satisfied and the value of this component is left untouched. Otherwise, the $j$ -th component is negative. But then the second conditional assignment is satisfied and the value of this component is set to 0 (while the others stay unchanged). So, we obtain again some polynomial sized CPR with the same number of alternations as before. ", "page_idx": 14}, {"type": "text", "text": "B.3 Omitted proofs ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proof of Proposition 5. Let $\\pmb{t}\\in\\mathbb{R}^{e}$ be the acceptance criterion of the UHAT and $f\\colon\\mathbb{R}^{d\\cdot(n+1)}\\ \\rightarrow$ $\\mathbb{R}^{e\\cdot(n+1)}$ be the computed function for inputs of length $n$ . By Lemma 6, we can construct in polynomial time an $O(\\ell)$ -alternation-bounded CPR computing $f$ . So, let $\\varphi_{i,k}\\ \\to\\ D_{i,k}$ be the conditional assignments in this CPR (for $1\\leq i\\leq n+1$ and $1\\leq k\\leq s_{i}$ ). Then we obtain a PC from this CPR as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\bigvee_{J=1}^{s_{1}}\\varphi_{1,J}\\wedge\\langle\\pmb{t},D_{1,J}\\bar{\\pmb{x}}\\rangle>0\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Note that this PC has still polynomial size, accepts an input sequence $(x_{1},\\ldots,x_{n},\\mathbf{0})$ if, and only if, the UHAT accepts $(x_{1},\\ldots,x_{n})$ , and \u2014 if the CPR is $a$ -alternation-bounded \u2014 then it has at most $a+2$ alternations of disjunctions and conjunctions. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "Proof of Proposition 7. Consider a constraint $p(X_{1},\\ldots,X_{n})\\;>\\;0$ $\\mathrm{~\\bf~or~}\\ge\\mathrm{~\\bf~0~}$ resp.) in $\\alpha$ . Then $p\\in\\mathbb{R}[X_{1},\\ldots,X_{n}]$ is a polynomial of degree at most 2, i.e., there are real numbers $c_{i,j,r,s}\\in\\mathbb{R}$ such that ", "page_idx": 14}, {"type": "equation", "text": "$$\np(X_{1},\\ldots,X_{n})=\\sum_{0\\leq r+s\\leq2}\\sum_{1\\leq i\\leq j\\leq n}c_{i,j,r,s}X_{i}^{r}X_{j}^{s}\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Now, construct two vectors $\\textbf{\\em u}$ and $\\mathbf{\\nabla}w$ with a component for each tuple $(i,j,r,s)$ : $u_{i,j,r,s}=c_{i,j,r,s}$ and $w=i,j,r,s=X_{i}^{r}X_{j}^{s}$ . Then it is clear that $p(X_{1},\\ldots,X_{n})=\\langle{\\pmb u},{\\pmb w}(X_{1},\\ldots,X_{n}^{\\setminus})\\rangle$ holds. ", "page_idx": 14}, {"type": "text", "text": "Application of Lemma 8 yields a vector $\\pmb{v}\\in\\mathbb{Q}^{t}$ with $\\|\\pmb{v}\\|_{2}\\leq(2m t)^{O(1)}$ and $\\pmb{u}\\sim_{2m}\\pmb{v}$ . Note that we need to consider rational numbers up to size $2m$ due to the fact that substitution of the variables in $\\mathbf{\\nabla}w$ by rational numbers $\\pmb{x}\\in\\mathbb{Q}_{\\leq m}^{n}$ yields a rational vector ${\\pmb w}({\\pmb x})\\in Q_{\\leq2m}^{t}$ . Let $p^{\\prime}(X_{1},\\ldots,X_{n})$ be the polynomial obtained from $p$ be replacing the coefficients $c_{i,j,r,s}\\in\\mathbb{R}$ by $v_{i,j,r,s}\\in\\mathbb{Q}$ . Then for each $\\pmb{x}\\in\\mathbb{Q}_{\\leq m}^{n}$ we have $p(\\pmb{x})=\\langle\\pmb{u},\\pmb{\\dot{w}}(\\pmb{x})\\rangle>0$ (resp. $\\ge0$ ) if, and only if, $p^{\\prime}(\\tilde{\\mathbf{x})}=\\langle\\pmb{v},\\pmb{w}(\\pmb{x})\\rangle>0$ (resp. $\\ge0$ ). ", "page_idx": 14}, {"type": "text", "text": "Replacing each real polynomial $p$ in $\\alpha$ by the constructed rational polynomial $p^{\\prime}$ results in a rational PC $\\alpha^{\\prime}$ with $[\\![\\alpha]\\!]\\cap\\mathbb{Q}_{\\leq m}^{n}=[\\![\\alpha]\\!]\\cap\\mathbb{Q}_{\\leq m}^{n}$ . \u53e3 ", "page_idx": 14}, {"type": "text", "text": "B.4 Proof of Lemma 10 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The rest of this subsection is devoted to proving Lemma 8, for which we rely on results from convex geometry, which requires some terminology. For a set $S\\subseteq\\mathbb{R}^{n}$ , we define the convex hull of $S$ as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{S=\\{\\lambda_{1}s_{1}+\\cdot\\cdot\\cdot+\\lambda_{m}s_{m}\\mid m>0,\\ s_{1},\\ldots,s_{m}\\in S,\\hfill}}\\\\ {{\\lambda_{1},\\ldots,\\lambda_{m}\\in[0,1],\\ \\lambda_{1}+\\cdot\\cdot\\cdot+\\lambda_{m}=1\\}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and the cone generated by $S$ as ", "page_idx": 15}, {"type": "equation", "text": "$$\nS~=~\\{\\lambda_{1}s_{1}~+~\\cdot~\\cdot~+~\\lambda_{m}s_{m}~~|~~m~>~0,~~s_{1},\\ldots,s_{m}~\\in~S,~\\;\\lambda_{1},\\ldots,\\lambda_{m}~\\geq~0\\}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We will also rely on Carath\u00e9odory\u2019s theorem, which says that a points in cones and convex sets can be obtained from at most $n$ points. See, for example, [35, Theorems 7.1i and 7.1j]. ", "page_idx": 15}, {"type": "text", "text": "Theorem 18 (Carath\u00e9odory\u2019s Theorem). Let $S\\,\\subseteq\\,\\mathbb{R}^{n}$ . For every $_x\\in$ conv.hull $S$ , there are $x_{1},\\ldots,x_{n}\\,\\in\\,S$ with $_x\\in$ conv.hull $\\{\\pmb{x}_{1},\\cdot\\cdot\\cdot,\\pmb{x}_{n}\\}$ . Moreover, for every $\\textit{\\textbf{y}}\\in$ cone $S$ , there are $y_{1},\\ldots,y_{n}\\in S$ with $\\pmb{y}\\in$ cone $\\{y_{1},\\ldots,y_{n}\\}$ . ", "page_idx": 15}, {"type": "text", "text": "A polyhedron is a set of the form $\\{\\pmb{x}\\in\\mathbb{R}^{n}\\mid A\\pmb{x}\\geq\\pmb{b}\\}$ , where $A\\in\\mathbb{R}^{m\\times n}$ and $\\pmb{b}\\in\\mathbb{R}^{m}$ for some $m\\,\\in\\,\\mathbb{N}$ . If the matrix $A$ and the vector $^{b}$ are rational, then the polyhedron is called a rational polyhedron. It is a standard result about polyhedra that if $A$ and $^{b}$ are rational of size at most $m$ , then the polyhedron $P=\\{{\\pmb x}\\in\\mathbb{R}^{n}\\mid A{\\pmb x}\\geq{\\pmb\\bar{b}}\\}$ can be written as ", "page_idx": 15}, {"type": "equation", "text": "$$\nP={\\mathsf{c o n v.h u l l}}\\left\\{x_{1},\\ldots,x_{s}\\right\\}+{\\mathsf{c o n e}}\\left\\{y_{1},\\ldots,y_{t}\\right\\},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "with rational vectors $x_{1},\\ldots,x_{s},y_{1},\\ldots,y_{t}$ , where each vector has size polynomial in mn. See, for example, [35, Theorem 10.2]. ", "page_idx": 15}, {"type": "text", "text": "Proof. We define the matrix $B\\in\\mathbb{Q}^{(k+\\ell)\\times n}$ and the vector $\\pmb{b}\\in\\mathbb{Q}^{k+\\ell}$ as ", "page_idx": 15}, {"type": "equation", "text": "$$\nB={\\binom{A}{A^{\\prime}}}\\qquad\\qquad\\qquad\\qquad b={\\binom{z}{z^{\\prime}}}\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and consider the polyhedron $P=\\{{\\pmb x}\\in\\mathbb{R}^{n}\\mid B{\\pmb x}\\geq{\\pmb b}\\}$ . As mentioned above, we can write ", "page_idx": 15}, {"type": "equation", "text": "$$\nP={\\mathsf{c o n v.h u l l}}\\left\\{x_{1},\\ldots,x_{s}\\right\\}+{\\mathsf{c o n e}}\\left\\{y_{1},\\ldots,y_{t}\\right\\},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $x_{1},\\ldots,x_{s},y_{1},\\ldots,y_{t}$ are rational vectors of size polynomial in mn. By our assumption, there exists an $s\\in\\mathbb{R}^{n}$ with $A s\\gg z$ and $A^{\\prime}s\\geq z^{\\prime}$ . By Carath\u00e9odory\u2019s Theorem, wlog, $\\pmb{s}$ belongs to the smaller polyhedron ", "page_idx": 15}, {"type": "equation", "text": "$$\nQ={\\mathsf{c o n v.h u l l}}\\;\\{x_{1},\\ldots,x_{n}\\}+{\\mathsf{c o n e}}\\;\\{y_{1},\\ldots,y_{n}\\}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Now note that we have $A u\\ge z$ and $A^{\\prime}\\pmb{u}\\geq z^{\\prime}$ for every $\\pmb{u}\\in U:=\\{\\pmb{x}_{1},\\pmb{\\dots},\\pmb{x}_{n},\\pmb{y}_{1},\\pmb{\\dots},\\pmb{y}_{n}\\}$ . We claim that the vector ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{r}=\\frac{1}{n}(\\pmb{x}_{1}+\\cdot\\cdot\\cdot+\\pmb{x}_{n})+\\pmb{y}_{1}+\\cdot\\cdot\\cdot+\\pmb{y}_{n}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "satisfies $A r\\gg z$ and $A^{\\prime}\\pmb{r}\\geq z^{\\prime}$ . Indeed, it clearly belongs to $Q\\subseteq P$ and thus satisfies $A^{\\prime}\\pmb{r}\\geq z^{\\prime}$ . Moreover, for every row $\\pmb{a}^{\\top}\\pmb{x}>z$ of $A x\\gg z$ , there must be a $u\\in U$ with $\\mathbf{\\omega}\\mathbf{\\top}\\mathbf{\\mu}\\mathbf{\\Sigma}\\mathbf{a}^{\\top}\\mathbf{u}>z\\cdot$ \u2014otherwise, we would would have $\\pmb{a}^{\\top}\\pmb{u}\\;=\\;\\boldsymbol{z}$ for every $\\textbf{\\em u}\\in\\~U$ and thus $\\pmb{a}^{\\top}\\pmb{s}\\,=\\,z$ . In particular, we have $A r\\gg z$ . Finally, the vector $\\pmb{r}$ has size at most $\\|\\pmb{x}_{1}\\|_{2}+\\cdot\\cdot\\cdot+\\|\\pmb{x}_{n}\\|_{2}+\\|\\pmb{y}_{1}\\|_{2}+\\cdot\\cdot\\cdot+\\|\\pmb{y}_{n}\\|_{2}$ , which is polynomial in $m n$ , since each $\\pmb{x}_{i}$ and each $\\pmb{y}_{i}$ has size polynomial in mn. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "B.5 Proof of Lemma 8 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof of Lemma 8. Collect the set of all inequalities $\\langle{\\pmb w},{\\pmb u}\\rangle>z$ or $\\langle\\boldsymbol{w},\\boldsymbol{u}\\rangle\\ge\\ z$ with $\\pmb{w}\\in\\mathbb{Q}_{\\leq m}^{t}$ and $z\\in\\mathbb{Q}_{\\leq m}$ that are satisfied for $\\textbf{\\em u}$ . This results in two large matrices $A\\in\\mathbb{Q}_{\\leq m}^{k\\times t}$ and $A^{\\prime}\\in\\mathbb{Q}_{\\leq m}^{\\ell\\times t}$ and vectors $z\\in\\mathbb{Q}_{\\leq m}^{k}$ and $z^{\\prime}\\in\\mathbb{Q}_{\\leq m}^{\\ell}$ such that we have $\\pmb{u}\\sim_{m}\\pmb{v}$ if and only if $A v\\gg z$ and $A^{\\prime}\\pmb{v}\\geq z^{\\prime}$ . Thus, we can construct $c^{\\prime}$ using Lemma 10. Observe that the bound from Lemma 10 does not depend on the (exponentially large) $k$ and $\\ell$ . \u53e3 ", "page_idx": 15}, {"type": "text", "text": "Proof of Proposition $I I$ . Let $T$ be some UHAT with positional encoding and $n,m\\,\\in\\,\\mathbb{N}$ be two natural numbers. In the following, we consider input sequences of $T$ having the length $n$ and size $m$ . By Propositions 5 and 7 there is polynomial sized, $O(\\ell)$ -alternation-bounded, and rational PC $\\alpha$ (where $\\ell$ is the number of layers in $T$ ) such that the set of sequences of size $m$ accepted by the UHAT $T$ equals $\\mathbb{[}\\alpha\\mathbb{J}\\cap\\mathbb{Q}_{\\leq m}^{n\\cdot d}$ . ", "page_idx": 16}, {"type": "text", "text": "We finally show that the PC $\\alpha$ can be realized as a circuit of constant depth and polynomial size. So, consider a constraint of the form $p(\\bar{\\pmb{x}})\\sim0$ where $\\sim\\in\\{\\geq,>\\}$ , $p$ is a polynomial of degree at most 2 and $\\bar{\\pmb{x}}$ represents the input sequence. Since addition and multiplication of rational numbers are realizable in $\\mathbf{T}\\mathbf{C}^{0}$ [7], it is clear that the computation of the value $p(\\bar{{\\pmb x}})$ is also realizable. Additionally, checking whether this value is $\\ge0$ $\\mathrm{\\Delta}^{\\mathrm{or}}>0$ , resp.) is a simple check of the bit representing the signum (and checking that the numerator has at least one non-zero bit). ", "page_idx": 16}, {"type": "text", "text": "Finally, we have to connect all the atoms of the form $p(\\bar{\\pmb{x}})\\sim0$ to the Boolean formula $\\alpha$ . Since $\\alpha$ alternates only a bounded number of times between disjunctions and conjunctions, we can realize the complete formula $\\alpha$ in a circuit of constant depth and with polynomial size. Since $\\alpha$ is equivalent (up to the input size $m$ ), the UHAT $T$ is realizable in $\\mathrm{T}\\mathbf{C}^{0}$ . Note that here, we need to perform iterated addition of rational numbers, which requires iterated multiplication of integers represented in binary. The latter is well-known to be possible in $\\mathrm{T}\\mathbf{C}^{0}$ [33, 34] (even uniformly [22], but this is not needed in our setting). \u53e3 ", "page_idx": 16}, {"type": "text", "text": "C Proof of non-regularity in Section 4 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Recall that ", "page_idx": 16}, {"type": "text", "text": "We next define the notion of symbolic automata [10, 11, 43]. A symbolic automaton is a tuple $(Q,\\delta,q_{0},F)$ , where $Q$ is a finite set of states, $q_{0}\\in Q$ is an initial state, $F\\subseteq Q$ is a set of final states, and $\\delta$ is a set of transition rules of the form $(p,S,q)$ , where $S\\subseteq\\mathbb{Q}$ . For $a\\in\\mathbb{Q}$ , we write $p\\rightarrow_{a}q$ (read \u201cthere is a transition from $p$ to $q$ reading $a$ ) if there is a transition rule $(p,S,q)$ such that $a\\in S$ . Slightly abusing notation, for a set $S\\subseteq\\mathbb{Q}$ , we also write $p\\rightarrow_{S}q$ to mean that $(p,S,q)$ is a transition rule in $\\delta$ . The notion of a run, and an accepting run can then be defined in exactly the same way as for finite automata (e.g. see [37]); namely, it is a sequence of transitions $q_{0}\\rightarrow_{a_{1}}\\cdot\\cdot\\cdot\\rightarrow_{a_{n}}q_{n}$ , where $q_{n}\\in F$ , reading the sequence $w=a_{1}\\cdot\\cdot\\cdot a_{n}$ over $\\mathbb{Q}$ . ", "page_idx": 16}, {"type": "text", "text": "To prove that there is no symbolic automaton recognizing Double, let us assume to the contrary that such an automaton $A$ exists, say, with $n$ states. Consider a sufficiently long $w=a_{1}\\cdot\\cdot\\cdot a_{m}\\in[$ Double (i.e. of length at least $n$ ), and an accepting run of $A$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\nq_{0}\\rightarrow_{S_{1}}\\cdot\\cdot\\cdot\\rightarrow_{S_{m}}q_{m}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where each $a_{i}\\in S_{i}$ and $q_{m}\\in F$ . Since $m+1>n$ , by pigeonhole principle, it must be the case that $q_{r}=q_{s}$ for some $r<s$ . Thus, also the sequence ", "page_idx": 16}, {"type": "equation", "text": "$$\na_{1}\\cdot\\cdot\\cdot a_{r}(a_{r+1}\\cdot\\cdot\\cdot a_{s})^{2}a_{s+1}\\cdot\\cdot\\cdot a_{m}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "is accepted by $A$ and therefore contained in Double. This is a contradiction since Double imposes $a_{s}<a_{s}$ . ", "page_idx": 16}, {"type": "text", "text": "D Omitted cases in proof of Theorem 3 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "For $\\neg\\varphi$ the UHAT $T_{\\neg\\varphi,m}$ first applies $T_{\\varphi,m}$ and on the obtained sequence ${\\mathbf{\\{}}u_{1},\\dots,\\mathbf{}}{\\mathbf{}}u_{n},{\\mathbf{0}}\\in\\mathbb{Q}^{m+1}$ uses a further layer followed by 1) to output $\\pmb{w}_{i}^{\\prime}:=(\\pmb{u}_{i}[1,m],1-\\pmb{\\bar{u_{i}}}[m+1])$ at position $i\\in[1,n]$ and $\\mathbf{0}\\in\\mathbb{Q}^{m+1}$ at position $n+1$ . ", "page_idx": 16}, {"type": "text", "text": "For $\\varphi\\vee\\psi$ we define the UHAT $T_{\\varphi\\lor\\psi,m}$ that first applies $T_{\\psi,m+1}\\circ T_{\\varphi,m}$ followed by a layer that on sequence $\\ensuremath{\\mathbf{u}}_{1},\\ensuremath{\\mathbf{\\phi}}.\\ensuremath{\\mathbf{\\phi}}.\\ensuremath{\\mathbf{\\phi}},\\ensuremath{\\mathbf{u}}_{n+1}\\in\\mathbb{Q}^{m+2}$ outputs $\\pmb{w}_{1}^{\\prime},\\dots,\\pmb{w}_{n+1}^{\\prime}\\in\\mathbb{Q}^{m+1}$ with ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\pmb{w}_{i}^{\\prime}:=(\\pmb{u}_{i}[1,m],\\operatorname*{max}\\{\\pmb{u}_{i}[m+1],\\pmb{u}_{i}[m+2]\\})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "for all $i\\in[1,n+1]$ . Note that if ${\\pmb u}_{n+1}={\\bf0}$ , then also ${\\pmb w}_{n+1}^{\\prime}={\\bf0}$ . ", "page_idx": 16}, {"type": "text", "text": "For $X\\varphi$ the UHAT $T_{X\\varphi,m}$ first applies $T_{\\varphi,m}$ to output a sequence $\\mathbf{\\psi}_{u_{1},\\,.\\,.\\,.\\,,\\,}\\mathbf{u}_{n+1}\\,\\in\\,\\mathbb{Q}^{m+1}$ with ${\\pmb u}_{n+1}={\\bf0}$ . With an additional layer that uses 2) to get attention vector $\\pmb{a}_{i}=\\pmb{u}_{i+1}$ for all $i\\in[1,n]$ it then outputs at position $i\\in[1,\\dot{n}]$ the vector $\\pmb{w}_{i}^{\\prime}:=(\\pmb{u}_{i}[1,m],\\pmb{a}_{i}[m+1])$ and at position $n+1$ the vector $\\pmb{w}_{n+1}^{\\prime}:=(\\pmb{u}_{n+1}[1,m],0)$ after applying 1). ", "page_idx": 17}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: This paper contains theoretical results on the expressive power of transformers on data sequences. As standard in theoretical results, we try to mathematically formulate the results as as precisely as they can be. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 18}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: We have discussed assumptions we made in this paper (e.g., regarding the formal model of transformers) and what consequences they have (see Limitations in Concluding Remarks). ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 18}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: Since this is a theory paper, we try to be explicit in all the assumptions that we made for our results. We have also included all the proofs for the claims we made in the paper. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 19}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: The paper contains no experiments. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 19}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: The paper contains no experiments and no code. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 20}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: The paper contains no experiments. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 20}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: The paper contains no experiments. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper contains no experiments. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 21}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: This is a theory paper with no reference to human subjects and datasets. Our paper does not have neither direct societal impact nor potential harmful consequences. We have also carefully checked the impact mitigation measures, which do not directly apply to our paper, which involves neither experiments nor code. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 21}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: This is a theory paper with no direct societal impacts. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 21}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 22}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: This is a theory paper with neither experiments, codes, nor the use of datasets. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 22}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: This is a theory paper that uses no code, data, and models. We do not use assets of others in any form. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: We do not release new assets. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 23}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: This is a theory paper with no crowdsourcing and no human subjects. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 23}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: This is a theory paper with no crowdsourcing and no human subjects. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}]