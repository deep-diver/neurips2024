{"references": [{"fullname_first_author": "Angluin, D.", "paper_title": "Masked hard-attention transformers and boolean RASP recognize exactly the star-free languages", "publication_date": "2023-10-13", "reason": "This paper provides a precise characterization of the expressive power of masked hard-attention transformers, a significant contribution to the understanding of transformer capabilities."}, {"fullname_first_author": "Barcel\u00f3, P.", "paper_title": "Logical languages accepted by transformer encoders with hard attention", "publication_date": "2024", "reason": "This paper establishes a connection between the expressive power of transformer encoders and logical languages, offering a formal framework for analyzing their capabilities."}, {"fullname_first_author": "Chiang, D.", "paper_title": "Tighter bounds on the expressivity of transformer encoders", "publication_date": "2023", "reason": "This paper refines the understanding of transformer expressiveness by providing tighter bounds on their computational power, improving the precision of previous analyses."}, {"fullname_first_author": "Hao, Y.", "paper_title": "Formal language recognition by hard attention transformers: Perspectives from circuit complexity", "publication_date": "2022", "reason": "This paper connects the study of transformers to circuit complexity, providing a new lens for analyzing their computational power and limitations."}, {"fullname_first_author": "Merrill, W.", "paper_title": "Saturated transformers are constant-depth threshold circuits", "publication_date": "2022", "reason": "This paper shows an equivalence between saturated transformers and a specific class of circuits, establishing a direct link between neural network architectures and well-studied computational models."}]}