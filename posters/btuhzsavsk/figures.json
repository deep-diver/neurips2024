[{"figure_path": "btuHzsAVsK/figures/figures_0_1.jpg", "caption": "Figure 1: Humans excel at biological motion perception (BMP) tasks with zero training, while current AI models struggle with poor generalization performance. AI models are trained to recognize actions from natural RGB videos and tested using BMP stimuli on point-light displays, which come in two forms: Joint videos, which display only the detected joints of actors in white dots, and Sequential position actor videos (SP), where light points in white are randomly positioned between joints and reallocated to other random positions on the limb in subsequent frames (Sec. 3.1). Note that skeletons, shown in gray in the example video, are not visible to humans or AI models during testing. The generalization performance of both humans and models is assessed after varying five properties in temporal and visual dimensions. See Appendix, Sec. A.1 for example videos.", "description": "This figure illustrates the core idea of the paper by comparing human and AI performance on biological motion perception (BMP) tasks. Humans easily perform these tasks even without prior training, unlike current AI models.  The figure shows how AI models are trained using RGB videos but tested using more challenging point-light displays (Joint and Sequential Position videos).  Five properties (temporal order, temporal resolution, amount/lifetime of visual information, and invariance to camera view) are manipulated to assess generalization performance across different levels of visual information and temporal dynamics.", "section": "1 Introduction"}, {"figure_path": "btuHzsAVsK/figures/figures_2_1.jpg", "caption": "Figure 2: Architecture of our proposed Motion Perceiver (MP) model. Given a reference patch (yellow or green example patches), MP computes its patch-level optical flow (red arrows, Sec. 2.1) on the feature maps extracted from DINO [10]. Subsequently, these flows are processed through flow snapshot neurons (Sec. 2.2) and motion invariant neurons (Sec. 2.3) in two pathways. Activations from both groups of neurons are then integrated for action classification (Sec. 2.4). Time embeddings (T Emb.) are used in the feature fusion process.", "description": "The figure illustrates the architecture of the Motion Perceiver (MP) model.  It shows how patch-level optical flow is computed from video frames using DINO.  The optical flow is then processed by flow snapshot neurons and motion invariant neurons, which produce outputs that are fused for action classification. The process uses time embeddings for feature fusion.", "section": "Our Proposed Motion Perceiver (MP)"}, {"figure_path": "btuHzsAVsK/figures/figures_6_1.jpg", "caption": "Figure 3: Temporal orders and resolutions matter in generalization performance on RGB and Joint videos. Stimuli encompass RGB and Joint (J) videos. Short forms include R (reversal), S (shuffle), F (frames), and P (points) in Sec. 3.1. Error bars indicate the standard error of the top-1 accuracy across different action classes.", "description": "This figure displays the top-1 accuracy of human subjects and the proposed MP model on action recognition tasks using RGB and Joint videos. The videos are manipulated by varying their temporal orders (reversed, shuffled), resolutions (number of frames: 3, 4, or 32), and number of points (6) in point-light displays. Error bars represent the standard error of the top-1 accuracy across different action classes, demonstrating the impact of temporal properties on model generalization performance.", "section": "4 Results"}, {"figure_path": "btuHzsAVsK/figures/figures_6_2.jpg", "caption": "Figure 4: Our model demonstrates human-like robustness under reduced visual information. Top-1 action recognition accuracy is a function of the number of points (P) in Joint (J) videos. Results from RGB test videos are at the leftmost. The colored shaded region represents the standard error across all action classes.", "description": "The figure shows the top-1 accuracy of human subjects and several AI models on the task of action recognition using joint videos with varying numbers of points, ranging from 5 to 26 points.  The x-axis represents the number of points in the joint videos, while the y-axis represents the accuracy. The results show that both human performance and the performance of the proposed Motion Perceiver (MP) model are relatively robust to a reduction in the number of points. In contrast, other AI models show a significant drop in accuracy with fewer points. The colored shaded region around the data points represents the standard error across all action classes, indicating the variability in the results.", "section": "4 Results"}, {"figure_path": "btuHzsAVsK/figures/figures_8_1.jpg", "caption": "Figure 5: Both humans and our model can recognize actions in SP videos without local motions. Performance varies depending on the persistence of visual information, with stimuli having 4 and 8 points (P) of the actors (Sec. 3.1).", "description": "This figure compares the performance of both humans and the proposed Motion Perceiver (MP) model on the task of recognizing actions from Sequential Position actor videos (SP videos).  SP videos are a type of point-light display where light points are randomly positioned between joints and reallocated to other random positions on the limb in subsequent frames.  The experiment manipulates two variables:\n\n*   **Number of points (P):** The number of light points used to represent the actor's movement (4 or 8 points).\n*   **Lifetime (frames):** The duration for which each point remains in a specific location before being reassigned (1, 2, or 4 frames).\n\nThe results show that both humans and MP can successfully recognize actions even when local motion signals are minimized, which is a hallmark of biological motion perception. Furthermore, the performance is not greatly affected by changes in lifetime (number of frames each point remains in one place), however, increasing the number of points used improves the accuracy of both humans and the model.", "section": "4 Results"}, {"figure_path": "btuHzsAVsK/figures/figures_8_2.jpg", "caption": "Figure 6: Our MP model shows a significantly stronger correlation with human performance compared to all baselines. The correlation between the model and human performance across all BMP conditions is presented.", "description": "This figure shows a bar chart comparing the correlation coefficients between different AI models (including the proposed MP model) and human performance across various biological motion perception (BMP) tasks.  The higher the bar, the stronger the correlation between the AI model's predictions and human judgments.  The MP model demonstrates a substantially stronger correlation with human performance than all other AI models, indicating better alignment with human behavior in these tasks.", "section": "4 Results"}, {"figure_path": "btuHzsAVsK/figures/figures_8_3.jpg", "caption": "Figure 7: Humans and AI models show minimal difference in generalization across camera views on J-6P videos (Sec. 3.1). Error bars indicate the standard error.", "description": "This figure compares the performance of humans and various AI models (including the proposed Motion Perceiver) on a biological motion perception (BMP) task, specifically focusing on how the models generalize across different camera viewpoints. The results indicate that both humans and the Motion Perceiver model demonstrate minimal differences in accuracy across frontal, 45\u00b0, and 90\u00b0 views. The error bars represent the standard error of the measurements, indicating the variability in performance.", "section": "4 Results"}, {"figure_path": "btuHzsAVsK/figures/figures_18_1.jpg", "caption": "Figure 1: Humans excel at biological motion perception (BMP) tasks with zero training, while current AI models struggle with poor generalization performance. AI models are trained to recognize actions from natural RGB videos and tested using BMP stimuli on point-light displays, which come in two forms: Joint videos, which display only the detected joints of actors in white dots, and Sequential position actor videos (SP), where light points in white are randomly positioned between joints and reallocated to other random positions on the limb in subsequent frames (Sec. 3.1). Note that skeletons, shown in gray in the example video, are not visible to humans or AI models during testing. The generalization performance of both humans and models is assessed after varying five properties in temporal and visual dimensions. See Appendix, Sec. A.1 for example videos.", "description": "This figure illustrates the superior performance of humans compared to current AI models in biological motion perception tasks.  Humans can easily recognize actions from minimal motion cues (like point-light displays), even without prior training. AI models, however, struggle with poor generalization when tested on similar data after being trained on natural RGB videos. The figure demonstrates different types of BMP stimuli used for testing (Joint videos, Sequential position actor videos) and highlights five key properties (temporal and visual) that are varied to assess generalization performance.", "section": "1 Introduction"}, {"figure_path": "btuHzsAVsK/figures/figures_20_1.jpg", "caption": "Figure S2: Human performance across camera views within different action classes on J-6P videos in BMP dataset. The labels on the x-axis are the 10 action classes in the BMP dataset (Appendix, Sec. A.2).", "description": "This figure shows the human performance on J-6P (Joint videos with 6 points) videos from the BMP dataset for 10 different action classes across three different camera views (frontal, 45\u00b0, and 90\u00b0). The x-axis represents the 10 action classes, and the y-axis represents the top-1 accuracy (%). The three bars for each action class represent the accuracy for each camera view.  The figure illustrates how human accuracy varies with both the type of action and the camera viewpoint.", "section": "H Alignment Between Models and Humans"}, {"figure_path": "btuHzsAVsK/figures/figures_21_1.jpg", "caption": "Figure 3: Temporal orders and resolutions matter in generalization performance on RGB and Joint videos. Stimuli encompass RGB and Joint (J) videos. Short forms include R (reversal), S (shuffle), F (frames), and P (points) in Sec. 3.1. Error bars indicate the standard error of the top-1 accuracy across different action classes.", "description": "This figure compares the performance of humans and the MP model on RGB and Joint videos under various conditions. The conditions varied are temporal order (reversed or shuffled), temporal resolution (number of frames), and number of points in the Joint videos.  The results show how these factors affect the models' ability to generalize across different conditions, highlighting the difficulty of generalizing in Joint videos due to limited visual information.", "section": "4 Results"}, {"figure_path": "btuHzsAVsK/figures/figures_21_2.jpg", "caption": "Figure S3: Our MP and En-MP significantly surpass all existing models on both Joint and SP videos. A correlation plot between model and human performances is provided, with accuracy averaged over conditions within each property on the same type of stimulus (Sec. 3.1). The black dash diagonal serves as a reference.", "description": "This figure compares the performance of the proposed Motion Perceiver (MP) and its enhanced version (En-MP) against other state-of-the-art models on biological motion perception tasks using Joint and SP videos. It visually demonstrates the superior performance of MP and En-MP by showing their higher accuracy compared to other models across various conditions and visual information levels. It also provides a correlation plot to highlight the stronger alignment of MP and En-MP's performance with that of humans.", "section": "Results"}, {"figure_path": "btuHzsAVsK/figures/figures_22_1.jpg", "caption": "Figure S5: Visualization of patch-level optical flow in vector field plots across a sequence of example video frames from \"stand up\" action class in the BMP dataset. Optical flow from (a) Frame 1; (b) Frame 8; (c) Frame 16; (d) Frame 31 to Frame 32.", "description": "This figure visualizes the patch-level optical flow in a \"stand up\" action from the BMP dataset.  It shows vector field plots across four frames (1, 8, 16, and 31-32) illustrating how the model captures motion primarily in the moving parts (the person) rather than being affected by noise in static areas.", "section": "J Visualization of Patch-Level Optical Flow"}, {"figure_path": "btuHzsAVsK/figures/figures_24_1.jpg", "caption": "Figure S6: Frame analysis for the \"pick-up\" action class in one example video from the BMP dataset. The color in the heat map indicates the video frames which would impact the action recognition accuracy of the model the most. Blue colors represent higher importance.", "description": "This figure shows a heatmap visualizing the importance of each frame in a \"pick-up\" action video for the model's accuracy. The color intensity represents the impact of removing that frame; darker blue indicates more significant impact on the model's accuracy.  This analysis helps understand which frames contain the most crucial information for action recognition.", "section": "K More Ablation Studies"}]