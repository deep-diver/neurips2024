[{"figure_path": "qL4nN6Ew7U/tables/tables_5_1.jpg", "caption": "Table 1: Evaluation of diffusion (upper) and transformer (down) models on HPSv2. We underline the highest value and color the first above Fantasy in blue.", "description": "This table presents a comparison of different diffusion and transformer-based models on the HPSv2 benchmark.  The HPSv2 benchmark evaluates the quality of generated images across five categories: Animation, Concept-art, Painting, Photo, and DrawBench. The table shows the performance of each model in terms of mean and standard deviation for each category.  The highest score in each category is underlined, and the first model that outperforms Fantasy is highlighted in blue. This allows for a direct comparison of Fantasy's performance against state-of-the-art models in text-to-image generation.", "section": "3.2 Performance Comparisons and Analysis"}, {"figure_path": "qL4nN6Ew7U/tables/tables_5_2.jpg", "caption": "Table 2: Comparison with recent T2I models. 'Trained' indicates the model develops a text encoder from scratch, foregoing a pre-trained one.", "description": "This table compares Fantasy's performance against other state-of-the-art text-to-image (T2I) models.  It lists each model's type (diffusion or transformer), text encoder used, number of parameters, number of training images, and FID-30K score. The FID-30K score is a lower-is-better metric evaluating the quality of generated images. The 'Trained' column indicates whether the model trained its own text encoder or used a pre-trained one.", "section": "3.2 Performance Comparisons and Analysis"}, {"figure_path": "qL4nN6Ew7U/tables/tables_7_1.jpg", "caption": "Table 3: Ablation study on two stages with the best bolded. 'Base' indicates the model after the pre-training stage.", "description": "This table presents the ablation study results on two training stages. The first row, \"Base\", shows the performance of the model after the pre-training stage using only masked image modeling (MIM). The second row, \"Fantasy\", shows the performance after fine-tuning with MIM and Phi-2, a lightweight decoder-only LLM, demonstrating the impact of the proposed two-stage training strategy. The metrics used are HPSv2 scores across five categories: Animation, Concept-art, Painting, Photo, and DrawBench.", "section": "4 Ablation Study"}, {"figure_path": "qL4nN6Ew7U/tables/tables_8_1.jpg", "caption": "Table 4: Ablation study on models at different scales with the best bolded. DB. represents DrawBench [36].", "description": "This ablation study investigates the impact of varying the size of the image generator on the model's performance.  Three different model sizes are evaluated, distinguished by the number of transformer layers (6, 12, and 22) and the resulting number of parameters (257M, 421M, and 611M). The performance is assessed across five categories: Animation, Concept-art, Painting, Photo, and DrawBench, using the HPSv2 benchmark. The table highlights the best performance achieved within each category, demonstrating the effect of model scale on image generation quality.", "section": "4.2 Scale of Image Generator"}, {"figure_path": "qL4nN6Ew7U/tables/tables_8_2.jpg", "caption": "Table 5: Training cost for Fantasy at 3 different scales. BS. denotes batch size and LR. denotes learning rate.", "description": "This table shows the training cost (steps, batch size, learning rate) for Fantasy at three different scales (6, 12, and 22 layers). It provides a detailed breakdown of training hyperparameters, allowing for reproducibility and comparison across different model sizes.", "section": "4.2 Scale of Image Generator"}]