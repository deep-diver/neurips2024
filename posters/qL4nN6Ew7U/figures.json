[{"figure_path": "qL4nN6Ew7U/figures/figures_0_1.jpg", "caption": "Figure 1: Comparison of data usage, training time and image quality. Colors from dark to light represent parameters increasing in size, and circles from small to large indicate improvements in image quality.", "description": "This figure compares several text-to-image models based on three key aspects: the amount of training data used (in millions), the training time (in A100 GPU days), and the resulting image quality.  Image quality is represented visually by the size and color of the circles; larger, lighter circles indicate better image quality.  The dashed line connects models, illustrating a trend where greater data usage and longer training times generally correspond to better image quality.  Specific data usage and time percentages are shown for each model relative to a baseline.", "section": "1 Introduction"}, {"figure_path": "qL4nN6Ew7U/figures/figures_1_1.jpg", "caption": "Figure 2: Samples produced by Fantasy (512 \u00d7 512). Each image, generated in 1.26 seconds (without super-resolution models), is accompanied by a descriptive caption showcasing diverse styles and comprehension.", "description": "This figure showcases example images generated by the Fantasy model, each with a corresponding caption.  The images demonstrate the model's ability to generate a wide variety of styles and its comprehension of complex prompts.  The speed of generation (1.26 seconds) is also highlighted.", "section": "1 Introduction"}, {"figure_path": "qL4nN6Ew7U/figures/figures_2_1.jpg", "caption": "Figure 3: (Up) Overview of Fantasy featuring text encoder, VQGAN (encoder & and decoder D), masked image generator G, and super-resolution model. (Down) Our training pipeline involves two stages. The red parts are trainable and the blue parts are frozen; the yellow part is optionally utilized during inference.", "description": "The figure illustrates the architecture of the Fantasy model and its training process. The upper part shows the model's components: a text encoder (Phi-2), a VQGAN (encoder and decoder), a masked image generator (G), and a super-resolution model.  The lower part details the two-stage training pipeline. Stage 1 is large-scale concept alignment pre-training, where only the masked image generator is trained, and the text encoder and VQGAN are frozen. Stage 2 is instruction fine-tuning, involving the fine-tuning of both the masked image generator and the text encoder with high-quality image-text data.", "section": "2 Method"}, {"figure_path": "qL4nN6Ew7U/figures/figures_6_1.jpg", "caption": "Figure 4: User study on prompts with different length. VC., CV2., FT., SD., and PA. refer to VQGAN+CLIP [8], CogView2 [12], our Fantasy, Stable Diffusion v2.0 [33], and Pixart-\u03b1 [5].", "description": "This figure presents the results of a user study comparing the performance of Fantasy against other text-to-image models.  Two sets of prompts were used: long prompts (more than 30 words) and short prompts (10-30 words). The study evaluated two metrics: visual appeal and text-image alignment.  For long prompts, Fantasy showed significantly higher preference in terms of both visual appeal and text-image alignment compared to other models, especially VQGAN+CLIP and CogView2.  However, for short prompts, while Fantasy still performed well,  the results showed less of a performance gap between models, suggesting that diffusion models and models not using decoder-only LLMs may have an advantage for shorter and less specific prompts.", "section": "3.3 Results on Human Evaluation"}, {"figure_path": "qL4nN6Ew7U/figures/figures_6_2.jpg", "caption": "Figure 6: Visual Comparison with ParaDiffusion [43]: Red markings and boxes highlight text misalignments in images generated by ParaDiffusion.", "description": "This figure shows a comparison of image generation results between Fantasy and ParaDiffusion, highlighting the superior text-image alignment of Fantasy.  Two example prompts are given, and for each, the images generated by both models are shown. Red boxes highlight areas in the ParaDiffusion images where the text description does not match the generated image, illustrating a failure in text-image alignment. The results demonstrate that Fantasy generates images which are more faithful to the given text descriptions, showcasing its superior ability in text-image alignment.", "section": "3.4 Case Study"}, {"figure_path": "qL4nN6Ew7U/figures/figures_7_1.jpg", "caption": "Figure 5: Visual comparison with existing T2I models. (a) A hamster resembling a horse. (b) A frontal portrait of a anime girl with chin length pink hair wearing sunglasses and a white T-shirt smiling. (c) A colorful illustration of a suburban neighborhood on an ancient post-apocalyptic planet featuring creatures made by Jim Henson's workshop. (d) A blue-haired girl with soft features stares directly at the camera in an extreme close-up Instagram picture. (e) A building in a landscape by Ivan Aivazovsky. (f) Aoshima's masterpiece depicts a forest illuminated by morning light. (g) The image is a highly detailed portrait of an oak in GTA V, created using Unreal Engine and featuring fantasy artwork by various artists.", "description": "This figure presents a visual comparison of the image generation results from several different text-to-image (T2I) models, including VQGAN+CLIP, Cogview2, SD XL, Pixart-\u03b1, and Fantasy.  Each model was given the same text prompts, and the resulting images are displayed side-by-side to highlight the differences in style, detail, and overall quality. The figure showcases the diversity and quality of image generation across various models.", "section": "3.4 Case Study"}, {"figure_path": "qL4nN6Ew7U/figures/figures_8_1.jpg", "caption": "Figure 7: Examples generated by models at different scales: 1st column for 6 layers, 2nd column for 12 layers and 3rd column for 22 layers.", "description": "This figure shows the image generation results of three models with different numbers of transformer layers (6, 12, and 22).  The prompt used for all three model variations was identical. By comparing the generated images, we can observe how the quality of image generation increases with the increase of transformer layers. The images are visually compared to show how increasing the number of layers impacts detail, clarity, and overall visual fidelity.  The model with 22 layers appears to perform best.", "section": "4.2 Scale of Image Generator"}]