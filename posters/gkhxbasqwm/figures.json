[{"figure_path": "GkHXBasQwm/figures/figures_1_1.jpg", "caption": "Figure 1: We present HOI-Swap that seamlessly swaps the in-contact object in videos using a reference object image, producing precise video edits with natural hand-object interactions (HOI). Notice how the generated hand needs to adjust to the shapes of the swapped-in objects (A,B,C) and how the reference object may require automatic re-posing to fit the video context (A).", "description": "This figure demonstrates the results of HOI-Swap, a novel video editing framework that swaps objects in videos while maintaining hand-object interaction awareness.  Three examples are shown, each replacing a kettle with a different object (A water bottle, a bowl, and a different kettle). The figure highlights the model's ability to seamlessly integrate the new objects into the videos, naturally adjusting hand poses to match the new objects' shapes and positions. This illustrates the key capabilities of the HOI-Swap model: precise object swapping and realistic HOI.", "section": "1 Introduction"}, {"figure_path": "GkHXBasQwm/figures/figures_2_1.jpg", "caption": "Figure 2: We highlight three challenges for the in-contact object swapping problem: (a) HOI awareness, where the model needs to adapt to interactions, such as changing the grasp to realistically accommodate the different shapes of the kettle vs. the bowl; (b) spatial alignment with source, requiring the model to automatically reorient objects, such as aligning the blue kettle from the reference image to match the hand position in the source; (c) temporal alignment with source, necessitating controllable motion guidance capability, essential when swapping objects like a trash can with a differently shaped and functioning reference, where not all original motions are transferable or desirable. In (a) and (b), we compare HOI-Swap's edited images with Paint by Example (PBE) [57], AnyDoor [7], and Affordance Diffusion (Afford Diff) [62].", "description": "This figure highlights three key challenges in swapping in-contact objects in videos:  HOI (Hand-Object Interaction) awareness (adapting hand grasp to object shape changes), spatial alignment (correctly positioning the swapped object relative to the hand and scene), and temporal alignment (controlling the motion of the swapped object to match the video's context). It compares HOI-Swap's results to three other methods (PBE, AnyDoor, Afford Diff), showcasing its superior performance in addressing these challenges.", "section": "2 Related Work"}, {"figure_path": "GkHXBasQwm/figures/figures_4_1.jpg", "caption": "Figure 3: HOI-Swap involves two stages, each trained separately in a self-supervised manner. In stage I, an image diffusion model \u03b8\u2081 is trained to inpaint the masked object region with a strongly augmented version of the original object image. In stage II, one frame is selected from the video to serve as the anchor. The remaining video is then warped using this anchor frame, several points sampled within it, and optical flow extracted from the video. A video diffusion model \u03b8\u2082 is trained to reconstruct the full video sequence from the warped sequence. During inference, the stage-I model swaps the object in one frame. This edited frame then serves as the anchor for warping a new video sequence, which is subsequently taken as input for the stage-II model to generate the complete video. reference object is realistically interacting with human hands and accurately positioned within the scene context of the source frame.", "description": "This figure illustrates the two-stage framework of HOI-Swap. Stage I focuses on single-frame object swapping with HOI awareness, while stage II extends this edit across the whole video sequence, achieving controllable motion alignment. The self-supervised training process is also visualized.", "section": "3 Method"}, {"figure_path": "GkHXBasQwm/figures/figures_7_1.jpg", "caption": "Figure 4: Qualitative results of HOI-Swap. We compare HOI-Swap with image (left) and video editing approaches (right). The reference object image is shown in the upper left corner of the source image. For image editing, HOI-Swap demonstrates the ability to seamlessly swap in-contact objects with HOI awareness, even in cluttered scenes. For video editing, HOI-Swap effectively propagates the one-frame edit across the entire video sequence while accurately following the source video's motion, achieving the highest overall quality among all methods. We highly encourage readers to check Supp. C.1 and the project page video for more comparisons.", "description": "This figure shows qualitative comparisons of HOI-Swap's image and video editing results against several baselines.  It highlights HOI-Swap's ability to seamlessly replace objects in contact with hands while maintaining realistic hand interactions and motion consistency in videos. The results showcase HOI-Swap's superior performance in both image and video editing tasks compared to other state-of-the-art methods.", "section": "4.2 Editing Results"}, {"figure_path": "GkHXBasQwm/figures/figures_8_1.jpg", "caption": "Figure 5: Ablation study on sampled motion points, comparing no to full motion points sampling. Left: we visualize Vwarp, used as conditional guidance for the stage-II model. Note that row 1 displays warp based on the source frame and is for illustration only, not provided to the model. Right: HOI-Swap exhibits controllable motion alignment: with no sampled points, the generated video diverges from the source video's motion; with full motion points, it closely mimics the source.", "description": "This figure shows an ablation study on the impact of using different numbers of sampled motion points as input to the second stage of the HOI-Swap model.  The left side visualizes the warped video sequences (Vwarp) that serve as input to the second stage's video diffusion model.  The right side shows the final generated videos resulting from using no sampled points, all sampled points, or an intermediate number of points. It demonstrates the model's ability to control the degree of motion alignment with the source video by adjusting the number of sampled points, which is particularly useful when swapping objects with significantly different shapes or functionalities.  When no points are sampled, the generated video diverges from the original video's motion. As the number of sampled points increases, the generated motion more closely follows the motion of the source video.", "section": "4.2 Editing Results"}, {"figure_path": "GkHXBasQwm/figures/figures_9_1.jpg", "caption": "Figure 6: Qualitative and quantitative comparisons between a one-stage baseline [1] and our two-stage HOI-Swap. The one-stage model struggles with preserving the new object's identify and fails to generate accurate interaction patterns, yielding inferior quantitative performance.", "description": "This figure compares a one-stage and a two-stage approach for swapping objects in videos. The results show that the two-stage approach (HOI-Swap) outperforms the one-stage baseline in terms of both qualitative and quantitative metrics.  The one-stage model struggles to maintain the identity of the swapped object and doesn't accurately model the hand-object interactions, while the two-stage model produces better results.", "section": "4.2 Editing Results"}, {"figure_path": "GkHXBasQwm/figures/figures_15_1.jpg", "caption": "Figure 7: Human evaluation interface for image editing part. We provide a source frame for editing alongside an image of the reference object. Users are asked to evaluate and select their favorite edited results based on various image editing criteria.", "description": "This figure shows the user interface for the human evaluation of image editing results.  Participants are presented with a source image and a reference object image. They are then shown four edited images: three from baseline methods and one from HOI-Swap (randomly ordered).  Participants are asked to rate the images based on several criteria including how well the reference object's identity is preserved, the realism of the hand-object interaction, and the overall quality of the edit.  These criteria help to assess the HOI-awareness, spatial alignment, and overall effectiveness of the different methods.", "section": "4.1 Experimental Setup"}, {"figure_path": "GkHXBasQwm/figures/figures_16_1.jpg", "caption": "Figure 3: HOI-Swap involves two stages, each trained separately in a self-supervised manner. In stage I, an image diffusion model \u03b81 is trained to inpaint the masked object region with a strongly augmented version of the original object image. In stage II, one frame is selected from the video to serve as the anchor. The remaining video is then warped using this anchor frame, several points sampled within it, and optical flow extracted from the video. A video diffusion model \u03b82 is trained to reconstruct the full video sequence from the warped sequence. During inference, the stage-I model swaps the object in one frame. This edited frame then serves as the anchor for warping a new video sequence, which is subsequently taken as input for the stage-II model to generate the complete video.", "description": "This figure illustrates the two-stage framework of the HOI-Swap model.  Stage I focuses on single-frame object swapping with hand-object interaction (HOI) awareness, using an image diffusion model.  Stage II extends this single-frame edit across the video sequence using warping based on optical flow and sampled points, then a video diffusion model reconstructs the video.  The self-supervised training process for each stage is also shown.", "section": "3 Method"}, {"figure_path": "GkHXBasQwm/figures/figures_18_1.jpg", "caption": "Figure 4: Qualitative results of HOI-Swap. We compare HOI-Swap with image (left) and video editing approaches (right). The reference object image is shown in the upper left corner of the source image. For image editing, HOI-Swap demonstrates the ability to seamlessly swap in-contact objects with HOI awareness, even in cluttered scenes. For video editing, HOI-Swap effectively propagates the one-frame edit across the entire video sequence while accurately following the source video's motion, achieving the highest overall quality among all methods. We highly encourage readers to check Supp. C.1 and the project page video for more comparisons.", "description": "This figure showcases a qualitative comparison of HOI-Swap with other image and video editing methods.  It highlights HOI-Swap's ability to seamlessly replace objects in images and videos while maintaining realistic hand-object interactions, even in complex scenes.  The results demonstrate its superior performance compared to other methods in terms of natural interactions and motion alignment.", "section": "4.2 Editing Results"}, {"figure_path": "GkHXBasQwm/figures/figures_19_1.jpg", "caption": "Figure 10: Qualitative image editing results of HOI-Swap on challenging in-the-wild scenarios, with source images from EgoExo4D and EPIC-Kitchens.", "description": "This figure shows a comparison of HOI-Swap with other image editing baselines on challenging in-the-wild scenarios.  The first column shows the source image with the region to be inpainted indicated by a dotted box.  The second column displays the reference object. The subsequent columns show the results from using PBE, AnyDoor, Afford Diff, and HOI-Swap.  The images demonstrate HOI-Swap's ability to seamlessly swap objects in complex scenes even when the background is cluttered, resulting in more natural and realistic-looking hand-object interactions compared to the baselines.", "section": "Editing Results"}, {"figure_path": "GkHXBasQwm/figures/figures_21_1.jpg", "caption": "Figure 1: We present HOI-Swap that seamlessly swaps the in-contact object in videos using a reference object image, producing precise video edits with natural hand-object interactions (HOI). Notice how the generated hand needs to adjust to the shapes of the swapped-in objects (A,B,C) and how the reference object may require automatic re-posing to fit the video context (A).", "description": "This figure showcases the results of HOI-Swap, a novel video editing framework.  It demonstrates the ability to seamlessly swap objects within videos while maintaining realistic hand-object interactions.  The results show several examples of objects being swapped, highlighting the model's ability to adapt the hand's grasp and pose to match the new object, even requiring re-posing to fit the video context. The reference image is shown for comparison.", "section": "1 Introduction"}, {"figure_path": "GkHXBasQwm/figures/figures_22_1.jpg", "caption": "Figure 1: We present HOI-Swap that seamlessly swaps the in-contact object in videos using a reference object image, producing precise video edits with natural hand-object interactions (HOI). Notice how the generated hand needs to adjust to the shapes of the swapped-in objects (A,B,C) and how the reference object may require automatic re-posing to fit the video context (A).", "description": "This figure showcases the main results of HOI-Swap, highlighting its ability to seamlessly replace objects in videos while maintaining realistic hand-object interactions.  It demonstrates the model's adaptability to different object shapes and its capacity to adjust hand poses accordingly.  The example shows three variations (A, B, and C) of the same video with different objects substituted for the original object.  This illustrates the nuanced changes in hand-object interaction the system accounts for. ", "section": "1 Introduction"}, {"figure_path": "GkHXBasQwm/figures/figures_23_1.jpg", "caption": "Figure 13: Ablation study of sampled motion points sparsity. The left figure illustrates a scenario where only partial motion transfer is desired, due to differences between the original and new object. The right figure showcases a scenario where full motion transfer is beneficial, owing to the similarities between the objects. We invite readers to view these examples in our project page.", "description": "This figure shows an ablation study on the impact of sampled motion points sparsity on the video generation results. The left column shows a scenario where the original and new objects differ significantly in shape and function, so only partial motion transfer is desired; the right column shows a scenario where the original and new objects are quite similar, so full motion transfer is preferred. In each column, the top row shows the source video frames, and the subsequent rows show the generated video frames using different numbers of sampled motion points (0%, 50%, 100%). The results demonstrate that HOI-Swap can generate videos with different degrees of motion alignment with the original video, demonstrating its flexibility and adaptability.", "section": "C.5 Discussion on sampling region"}, {"figure_path": "GkHXBasQwm/figures/figures_24_1.jpg", "caption": "Figure 5: Ablation study on sampled motion points, comparing no to full motion points sampling. Left: we visualize Vwarp, used as conditional guidance for the stage-II model. Note that row 1 displays warp based on the source frame and is for illustration only, not provided to the model. Right: HOI-Swap exhibits controllable motion alignment: with no sampled points, the generated video diverges from the source video's motion; with full motion points, it closely mimics the source.", "description": "This figure shows an ablation study on the impact of the number of sampled motion points used in the second stage of the HOI-Swap model. The left side visualizes the warped video sequence (Vwarp) which is used as conditional guidance for the video generation process. The right side shows the generated videos with different numbers of sampled points. It demonstrates that HOI-Swap exhibits controllable motion alignment, allowing the model to generate videos that either closely mimic or diverge from the source video's motion depending on the number of points sampled.", "section": "4.2 Editing Results"}, {"figure_path": "GkHXBasQwm/figures/figures_24_2.jpg", "caption": "Figure 15: Comparison of HOI-Swap with text-guided diffusion models: Pika [30], Runway [10], and Rerender-a-video (Rerender) [59]. To evaluate these latest models on the object swapping task, we describe the reference object in text and prompt the models to replace the original object in the video. These approaches are unable to alter the shape of the bowl and fail to swap the original bowl with a kettle as required.", "description": "This figure compares HOI-Swap's performance against three other text-guided diffusion models (Pika, Runway, and Rerender) in a video object-swapping task.  The results show that HOI-Swap significantly outperforms the others by successfully swapping the object while maintaining natural hand-object interactions, unlike the other models which fail to accurately replace or reshape the object.", "section": "C.6 Comparison with text-guided editing approaches"}, {"figure_path": "GkHXBasQwm/figures/figures_25_1.jpg", "caption": "Figure 13: Ablation study of sampled motion points sparsity. The left figure illustrates a scenario where only partial motion transfer is desired, due to differences between the original and new object. The right figure showcases a scenario where full motion transfer is beneficial, owing to the similarities between the objects. We invite readers to view these examples in our project page.", "description": "This figure shows an ablation study on the impact of the number of sampled motion points on the generated video's motion.  The left side demonstrates a scenario where the objects differ significantly, and only a partial transfer of motion from the source video to the generated video is desirable; using all points leads to incorrect motion. The right side shows a scenario where the objects are similar, and full motion transfer is desired. This figure highlights HOI-Swap's flexibility in controlling the degree of motion alignment.", "section": "C.4 Ablation study"}]