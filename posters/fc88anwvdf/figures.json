[{"figure_path": "fc88ANWvdF/figures/figures_1_1.jpg", "caption": "Figure 1: We use Einsums to parameterize a wide range of structured matrices and search for the most efficient structure for compute-optimal training. Left: A diagrammatic representation of a general two-factor Einsum. We parameterize the space of Einsums through a real-valued vector \u03b8 = (\u03b8\u03b1, \u03b8\u03b2, \u03b8\u03b7, \u03b8\u03b4, \u03b8\u03b5, \u03b8\u03c6, \u03b8\u03c1) \u2208 [0, 1]7. This space captures many well-known structures through specific values of 0. Middle: Example of well-known structures with their values. Any omitted line implies the value of the entry in the vector is 0. Right: Compute-optimal scaling laws of example structures for GPT-2 on OpenWebText when substituting its dense layers (see details in Section 4).", "description": "This figure demonstrates how the authors use Einstein summation (Einsum) to parameterize a wide range of structured matrices and search for the most efficient one for compute-optimal training.  The left panel shows a diagram of a general two-factor Einsum. The middle panel lists examples of well-known matrix structures and their corresponding parameters within the Einsum framework. Finally, the right panel displays the compute-optimal scaling laws for various structures when applied to GPT-2 language model training, showcasing the performance differences compared to dense layers.", "section": "1 Introduction"}, {"figure_path": "fc88ANWvdF/figures/figures_3_1.jpg", "caption": "Figure 2: Illustrating the Einsum taxonomy. The 3D graph represents relevant quantities of the Einsum structure such as the amount of parameter sharing \u03c9 (x-axis), its rank \u03c8 (y-axis), and its compute intensity \u03bd (z-axis). The structures on the left of the figure appear as dots on the graph based on their coordinates \u03b8. We highlight two key subspaces. (a) The BTT subspace, characterized by no parameter sharing \u03c9 = 0, learning the maximum number of parameters per FLOP. (b) The full-rank BTT subspace where \u03c9 = 0 and \u03c8 = 1. In Section 4 we show that the full-rank BTT subspace contains the most performant structures across multiple tasks.", "description": "This figure illustrates a taxonomy of Einsum linear structures, visualizing their properties in a 3D space defined by parameter sharing (\u03c9), rank (\u03c8), and compute intensity (\u03bd).  The plot shows how various known matrix structures (Monarch, Dense, Tensor-Train, Kronecker, BTT, Low-rank) are positioned within this 3D space, highlighting two key subspaces: the BTT subspace (\u03c9=0, maximizing parameters per FLOP) and the full-rank BTT subspace (\u03c9=0, \u03c8=1).  The figure demonstrates that full-rank BTT structures without parameter sharing generally perform best across multiple tasks.", "section": "A Taxonomy of the Space of Einsum Linear Structures"}, {"figure_path": "fc88ANWvdF/figures/figures_3_2.jpg", "caption": "Figure 4: The taxonomy parameters (\u03c9, \u03c8) explain differences in the scaling laws. (Left): parameter sharing (\u03c9 > 0) leads to worse scaling. (Middle): among structures without parameter sharing (\u03c9 = 0), full-rank structures (\u03c8 = 1) scale better than low-rank structures (\u03c8 < 1). (Right): in the (\u03c9 = 0, \u03c8 = 1) subspace, various structures have nearly indistinguishable scaling laws compared to dense matrices, suggesting that not implementing parameter sharing and being full-rank are the necessary and sufficient conditions for a compute-efficient linear layer for GPT-2.", "description": "This figure shows how the three key scalar quantities (\u03c9, \u03c8, \u03bd) characterizing the space of Einsums affect the compute-optimal scaling laws.  The left panel demonstrates that parameter sharing (\u03c9 > 0) negatively impacts scaling performance. The middle panel illustrates that, for structures without parameter sharing (\u03c9 = 0), full-rank structures (\u03c8 = 1) exhibit better scaling than low-rank ones (\u03c8 < 1).  Finally, the right panel indicates that, within the subspace of full-rank structures without parameter sharing (\u03c9 = 0, \u03c8 = 1), various structures demonstrate near-identical scaling laws compared to dense matrices, suggesting that the absence of parameter sharing and full rank are key factors for efficient linear layers.", "section": "Analyzing the Compute-Optimal Scaling Laws"}, {"figure_path": "fc88ANWvdF/figures/figures_5_1.jpg", "caption": "Figure 4: The taxonomy parameters (\u03c9, \u03c8, \u03bd) explain differences in the scaling laws. (Left): parameter sharing (\u03c9 > 0) leads to worse scaling. (Middle): among structures without parameter sharing (\u03c9 = 0), full-rank structures (\u03c8 = 1) scale better than low-rank structures (\u03c8 < 1). (Right): in the (\u03c9 = 0, \u03c8 = 1) subspace, various structures have nearly indistinguishable scaling laws compared to dense matrices, suggesting that not implementing parameter sharing and being full-rank are the necessary and sufficient conditions for a compute-efficient linear layer for GPT-2.", "description": "This figure shows the impact of the three key scalar quantities (\u03c9, \u03c8, \u03bd) on the scaling laws of different Einsum linear layers in the context of GPT-2 training.  The left panel demonstrates that parameter sharing (\u03c9 > 0) negatively affects scaling performance. The middle panel illustrates that, within the parameter-sharing-free subspace (\u03c9 = 0), full-rank structures (\u03c8 = 1) exhibit superior scaling compared to low-rank ones (\u03c8 < 1). The right panel reveals that, within the subspace where \u03c9 = 0 and \u03c8 = 1, various structures demonstrate very similar scaling behavior to dense matrices.  This suggests that the absence of parameter sharing and full-rank nature are crucial for achieving efficient linear layers.", "section": "Analyzing the Compute-Optimal Scaling Laws"}, {"figure_path": "fc88ANWvdF/figures/figures_6_1.jpg", "caption": "Figure 4: The taxonomy parameters (\u03c9, \u03c8, \u03bd) explain differences in the scaling laws. (Left): parameter sharing (\u03c9 > 0) leads to worse scaling. (Middle): among structures without parameter sharing (\u03c9 = 0), full-rank structures (\u03c8 = 1) scale better than low-rank structures (\u03c8 < 1). (Right): in the (\u03c9 = 0, \u03c8 = 1) subspace, various structures have nearly indistinguishable scaling laws compared to dense matrices, suggesting that not implementing parameter sharing and being full-rank are the necessary and sufficient conditions for a compute-efficient linear layer for GPT-2.", "description": "This figure shows the relationship between the taxonomy parameters (\u03c9, \u03c8, \u03bd) and the compute-optimal scaling laws of different Einsum linear layers in the GPT-2 language model. The left panel shows that parameter sharing (\u03c9 > 0) leads to worse scaling laws. The middle panel shows that among structures without parameter sharing (\u03c9 = 0), full-rank structures (\u03c8 = 1) exhibit better scaling than low-rank structures (\u03c8 < 1). The right panel demonstrates that within the subspace of structures with no parameter sharing (\u03c9 = 0) and full rank (\u03c8 = 1), various structures exhibit nearly identical scaling laws to dense matrices, indicating that the absence of parameter sharing and full rank are crucial for efficient linear layers in GPT-2.", "section": "4.2 Analyzing the Compute-Optimal Scaling Laws"}, {"figure_path": "fc88ANWvdF/figures/figures_7_1.jpg", "caption": "Figure 6: BTT Mixture-of-Experts has significantly better compute-optimal scaling laws than dense GPT-2 and its standard MoE variant. (Left): Compute-optimal frontier with 8. (Middle): 8 experts compute multiplier of BTT-MoE and standard MoE relative to dense as a function of FLOPs required by the dense model to achieve the same loss. (Right): Increasing the number of experts improves computational savings. Mean and standard deviation of the compute multiplier over all compute observations for 8 and 16 experts.", "description": "This figure shows the compute-optimal scaling laws for three different models: dense GPT-2, standard MoE, and BTT-MoE, demonstrating that BTT-MoE has significantly better scaling laws. The left panel shows the compute-optimal frontier for 8 experts. The middle panel shows the compute multiplier of BTT-MoE and standard MoE relative to dense as a function of FLOPs. The right panel shows how increasing the number of experts improves computational savings.", "section": "5 Structured Mixture of Experts"}, {"figure_path": "fc88ANWvdF/figures/figures_8_1.jpg", "caption": "Figure 7: Mean and std dev of compute multipliers for structured MoE. BTT is better than low rank or dense.", "description": "This bar chart compares the compute efficiency of different Mixture-of-Experts (MoE) architectures using various base structures: BTT-MoE (Block Tensor Train), Std. MoE (standard MoE), Dense-MoE (dense MoE), and Low-Rank-MoE (low-rank MoE).  The compute multiplier represents the compute efficiency gain over dense models. A higher multiplier indicates greater compute savings. The chart shows that BTT-MoE achieves the highest compute multiplier, demonstrating its superior compute efficiency compared to other MoE architectures.", "section": "5.3 Effect of Structures"}, {"figure_path": "fc88ANWvdF/figures/figures_8_2.jpg", "caption": "Figure 4: The taxonomy parameters (\u03c9, \u03c8, \u03bd) explain differences in the scaling laws. (Left): parameter sharing (\u03c9 > 0) leads to worse scaling. (Middle): among structures without parameter sharing (\u03c9 = 0), full-rank structures (\u03c8 = 1) scale better than low-rank structures (\u03c8 < 1). (Right): in the (\u03c9 = 0, \u03c8 = 1) subspace, various structures have nearly indistinguishable scaling laws compared to dense matrices, suggesting that not implementing parameter sharing and being full-rank are the necessary and sufficient conditions for a compute-efficient linear layer for GPT-2.", "description": "This figure shows the impact of three key parameters (\u03c9, \u03c8, \u03bd) on the compute-optimal scaling laws of different Einsum linear layers in a GPT-2 language model.  It demonstrates that parameter sharing (\u03c9) negatively affects scaling, while full-rank structures (\u03c8 = 1) exhibit better scaling than low-rank structures.  Interestingly, the compute intensity (\u03bd) has minimal impact on scaling within the subset of structures without parameter sharing and with full rank.", "section": "Analyzing the Compute-Optimal Scaling Laws"}, {"figure_path": "fc88ANWvdF/figures/figures_16_1.jpg", "caption": "Figure 4: The taxonomy parameters (\u03c9, \u03c8, \u03bd) explain differences in the scaling laws. (Left): parameter sharing (\u03c9 > 0) leads to worse scaling. (Middle): among structures without parameter sharing (\u03c9 = 0), full-rank structures (\u03c8 = 1) scale better than low-rank structures (\u03c8 < 1). (Right): in the (\u03c9 = 0, \u03c8 = 1) subspace, various structures have nearly indistinguishable scaling laws compared to dense matrices, suggesting that not implementing parameter sharing and being full-rank are the necessary and sufficient conditions for a compute-efficient linear layer for GPT-2.", "description": "This figure shows the impact of the taxonomy parameters (\u03c9, \u03c8, \u03bd) on the scaling laws of various Einsum structures when used in GPT-2.  The left panel demonstrates that parameter sharing (\u03c9 > 0) negatively affects scaling. The middle panel highlights that, for structures without parameter sharing (\u03c9 = 0), full-rank structures (\u03c8 = 1) exhibit superior scaling compared to low-rank structures (\u03c8 < 1). The right panel indicates that within the subspace where \u03c9 = 0 and \u03c8 = 1, numerous structures demonstrate comparable scaling laws to dense matrices, implying that the absence of parameter sharing and full-rank nature are crucial for computational efficiency.", "section": "Analyzing the Compute-Optimal Scaling Laws"}, {"figure_path": "fc88ANWvdF/figures/figures_16_2.jpg", "caption": "Figure 10: Exploiting attention head structure improves compute-efficiency by an average of 17%.", "description": "This figure shows the impact of incorporating the attention head structure into the BTT (Block Tensor Train) model. By aligning the BTT output axes with the attention head structure, the model achieves a 17% improvement in compute efficiency compared to a naive replacement of all attention and FFN (feed-forward network) matrices with BTT.", "section": "5 Structured Mixture of Experts"}]