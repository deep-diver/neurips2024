{"references": [{"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper introduces GPT-3, a foundational model that motivates the study of efficient linear layers for large neural networks."}, {"fullname_first_author": "Jared Kaplan", "paper_title": "Scaling laws for neural language models", "publication_date": "2020-01-01", "reason": "This paper establishes the scaling laws which the authors aim to leverage and improve upon with the proposed mixture-of-experts architecture."}, {"fullname_first_author": "Tri Dao", "paper_title": "Monarch: Expressive Structured Matrices for Efficient and Accurate Training", "publication_date": "2022-07-01", "reason": "This paper introduces the Monarch structure, which is one of the structures analyzed and improved upon in the presented work."}, {"fullname_first_author": "Shikai Qiu", "paper_title": "Compute better spent: Replacing dense layers with structured matrices", "publication_date": "2024-07-01", "reason": "This paper is highly relevant as it is a previous work by some of the same authors that directly inspires and informs the current work."}, {"fullname_first_author": "Andres Potapczynski", "paper_title": "CoLA: Exploiting Compositional Structure for Automatic and Efficient Numerical Linear Algebra", "publication_date": "2023-12-01", "reason": "This paper introduces the CoLA framework, which provides the computational infrastructure for efficiently implementing and evaluating the Einstein summation-based linear layers."}]}