DiTFastAttn: Attention Compression for Diffusion Transformer Models