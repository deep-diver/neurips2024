{"references": [{"fullname_first_author": "Joshua Ainslie", "paper_title": "GQA: Training generalized multi-query transformer models from multi-head checkpoints", "publication_date": "2023-05-13", "reason": "This paper introduces Group Query Attention (GQA), a key technique used in DiTFastAttn to improve attention efficiency and is directly compared to in the results section."}, {"fullname_first_author": "Tim Brooks", "paper_title": "Video generation models as world simulators", "publication_date": "2024-00-00", "reason": "This paper introduces OpenSora, a key video generation model used to evaluate DiTFastAttn's performance on video generation tasks."}, {"fullname_first_author": "Junsong Chen", "paper_title": "PixArt-Sigma: Weak-to-strong training of diffusion transformer for 4k text-to-image generation", "publication_date": "2024-03-04", "reason": "This paper introduces PixArt-Sigma, a major image generation model used for evaluating DiTFastAttn's image generation capabilities and is extensively analyzed in the results."}, {"fullname_first_author": "Tri Dao", "paper_title": "FlashAttention-2: Faster attention with better parallelism and work partitioning", "publication_date": "2023-00-00", "reason": "This paper introduces FlashAttention-2, a crucial component in DiTFastAttn that is implemented to improve the speed of attention computation and is discussed in the methodology section."}, {"fullname_first_author": "Jonathan Ho", "paper_title": "Denoising diffusion probabilistic models", "publication_date": "2020-00-00", "reason": "This foundational paper on denoising diffusion probabilistic models provides the theoretical underpinning for diffusion models, which DiTFastAttn aims to improve the efficiency of"}]}