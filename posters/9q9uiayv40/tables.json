[{"figure_path": "9Q9UiAyV40/tables/tables_5_1.jpg", "caption": "Table 1: ImageNet-1K Top-1 accuracy across 28\u00d728 to 448\u00d7448 resolutions: Our method was only trained for 5 epochs, while ResFormer [16] was trained for 200 epochs, all methods based on the same well-trained model.", "description": "This table presents the ImageNet-1K Top-1 accuracy results for various resolutions (from 28x28 to 448x448) using different methods: Vanilla ViT, ResFormer, FlexiViT, and the proposed MSPE.  It highlights that MSPE, despite training only for 5 epochs (compared to 200 epochs for ResFormer), achieves superior or comparable performance across resolutions.", "section": "4.1 Image Classification"}, {"figure_path": "9Q9UiAyV40/tables/tables_6_1.jpg", "caption": "Table 2: Comparative results of semantic segmentation on ADE20K and Cityscapes, using well-trained SETR Naive [34] as the segmentation model (ViT-L backbone), evaluated by mIOU, MACC, and F1-score.", "description": "This table presents a comparison of semantic segmentation results on the ADE20K and Cityscapes datasets using three different methods: Vanilla ViT, FlexiViT, and the proposed MSPE method.  The evaluation metrics used are mean Intersection over Union (mIOU), mean Accuracy (mACC), and F1-score.  The results are shown for different input image resolutions (128x128, 192x192, 256x256, 384x384, 512x512, 768x768).  The table demonstrates how MSPE improves performance at various resolutions compared to the baseline Vanilla ViT and FlexiViT methods.", "section": "4.2 Semantic Segmentation"}, {"figure_path": "9Q9UiAyV40/tables/tables_6_2.jpg", "caption": "Table 3: Comparative results of object detection and instance segmentation on COCO2017, employing well-trained ViTDeT [32] as the detection model (ViT-B backbone), pre-trained on ImageNet-1K via MAE [36].", "description": "This table presents the comparative results of object detection and instance segmentation on the COCO2017 dataset.  The results are obtained using the well-trained ViTDeT [32] model, which has a ViT-B backbone pre-trained on ImageNet-1K using MAE [36]. The table compares different methods (Vanilla, FlexiViT, and MSPE) across various metrics, including different Average Precision (AP) scores at different Intersection over Union (IoU) thresholds (APb, AP50, AP75, APs, APm, API).  This allows for a comprehensive comparison of the performance of each method on object detection and instance segmentation tasks. ", "section": "4 Experiments"}, {"figure_path": "9Q9UiAyV40/tables/tables_8_1.jpg", "caption": "Table 1: ImageNet-1K Top-1 accuracy across 28\u00d728 to 448\u00d7448 resolutions: Our method was only trained for 5 epochs, while ResFormer [16] was trained for 200 epochs, all methods based on the same well-trained model.", "description": "This table presents the ImageNet-1K Top-1 accuracy results for various resolutions (from 28x28 to 448x448) comparing four different methods: Vanilla ViT, ResFormer, FlexiViT, and MSPE.  It highlights the performance of MSPE, which was trained only for 5 epochs, in contrast to ResFormer's 200 epochs of training, while all methods use the same pre-trained model.  The table allows for a direct comparison of the methods across various image sizes.", "section": "4.1 Image Classification"}, {"figure_path": "9Q9UiAyV40/tables/tables_13_1.jpg", "caption": "Table 6: Comparison results of higher resolution.", "description": "This table compares the performance of Vanilla ViT and MSPE models on ImageNet-1K using high-resolution images (from 224x224 to 4032x4032).  It demonstrates MSPE's ability to maintain high accuracy even at very high resolutions, unlike the Vanilla ViT model which experiences a significant drop in accuracy as resolution increases.  The results show that MSPE effectively handles both non-overlapping and overlapping patch embedding types, indicating its broad applicability.", "section": "D.2 Results on Higher Resolution"}, {"figure_path": "9Q9UiAyV40/tables/tables_13_2.jpg", "caption": "Table 1: ImageNet-1K Top-1 accuracy across 28\u00d728 to 448\u00d7448 resolutions: Our method was only trained for 5 epochs, while ResFormer [16] was trained for 200 epochs, all methods based on the same well-trained model.", "description": "This table presents the ImageNet-1K Top-1 accuracy results for different resolutions (from 28x28 to 448x448) using various methods: Vanilla ViT, ResFormer, FlexiViT, and the proposed MSPE method.  It highlights that MSPE achieves comparable or superior performance to other state-of-the-art methods with significantly less training (5 epochs vs. 200 epochs for ResFormer).", "section": "4.1 Image Classification"}]