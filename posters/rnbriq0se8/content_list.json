[{"type": "text", "text": "Ada-MSHyper: Adaptive Multi-Scale Hypergraph Transformer for Time Series Forecasting ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Zongjiang Shang, Ling Chen,\u2217 Binqing Wu, Dongliang Cui ", "page_idx": 0}, {"type": "text", "text": "State Key Laboratory of Blockchain and Data Security College of Computer Science and Technology Zhejiang University {zongjiangshang, lingchen, binqingwu, runnercdl}@cs.zju.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Although transformer-based methods have achieved great success in multi-scale temporal pattern interaction modeling, two key challenges limit their further development: (1) Individual time points contain less semantic information, and leveraging attention to model pair-wise interactions may cause the information utilization bottleneck. (2) Multiple inherent temporal variations (e.g., rising, falling, and fluctuating) entangled in temporal patterns. To this end, we propose Adaptive Multi-Scale Hypergraph Transformer (Ada-MSHyper) for time series forecasting. Specifically, an adaptive hypergraph learning module is designed to provide foundations for modeling group-wise interactions, then a multi-scale interaction module is introduced to promote more comprehensive pattern interactions at different scales. In addition, a node and hyperedge constraint mechanism is introduced to cluster nodes with similar semantic information and differentiate the temporal variations within each scales. Extensive experiments on 11 real-world datasets demonstrate that Ada-MSHyper achieves state-of-the-art performance, reducing prediction errors by an average of $4.56\\%$ , $10.38\\%$ , and $4.97\\%$ in MSE for longrange, short-range, and ultra-long-range time series forecasting, respectively. Code is available at https://github.com/shangzongjiang/Ada-MSHyper. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Time series forecasting has demonstrated its wide applications across many fields [30, 37], e.g., energy consumption planning, traffic and economics prediction, and disease propagation forecasting. In these real-world applications, the observed time series often demonstrate complex and diverse temporal patterns at different scales[6, 9, 28]. For example, due to periodic human activities, traffic occupation and electricity consumption show clear daily patterns (e.g., afternoon or evening), weekly patterns (e.g., weekday or weekend), and even monthly patterns (e.g., summer or winter). ", "page_idx": 0}, {"type": "text", "text": "Recently, deep models have achieved great success in time series forecasting. To tackle intricate temporal patterns and their interactions at different scales, numerous foundational backbones have emerged, including recurrent neural networks (RNNs) [7, 9, 10], convolutional neural networks (CNNs) [1, 25], graph neural networks (GNNs) [4, 8], and transformers [20, 39]. Particularly, due to the capabilities of depicting pair-wise interactions and extracting multi-scale representations in sequences, transformers are widely used in time series forecasting. However, some recent studies show that even simple multi-scale MLP [11, 35] or na\u00efve series decomposition methods [3, 15] can outperform transformer-based methods on various benchmarks. We argue the challenges that limit the effectiveness of transformers in time series forecasting are as follows. ", "page_idx": 0}, {"type": "text", "text": "The first one is semantic information sparsity. Different from natural language processing (NLP) and computer vision (CV), individual time point in time series contains less semantic information [5, 29]. Compared to pair-wise interactions, group-wise interactions among time points with similar semantic information (e.g., neighboring time points or distant but strongly correlated time points) are more emphasized in time series forecasting. To address the problem of semantic information sparsity, some recent works employ patch-based approaches [12, 23] and hypergraph structures [26] to enhance locality and capture group-wise interactions. However, simple partitioning of patches and predefined hypergraph structures may introduce a large amount of noise and be hard to discover implicit interactions. ", "page_idx": 1}, {"type": "text", "text": "The second one is temporal variations entanglement. Due to the complexity and non-stationary of real-world time series, the temporal patterns of observed time series often contain a large number of inherent variations (e.g., rising, falling, and fluctuating), which may mix and overlap with each other. Especially when there are distinct temporal patterns at different scales, multiple temporal variations are deeply entangled, bringing extreme challenges for time series forecasting. To tackle the problem of temporal variations entanglement, recent studies employ series decomposition [30, 39] and multiperiodicity analysis [27, 29] to differentiate temporal variations at different scales. However, existing methods lack the ability to differentiate temporal variations within each scale, making temporal variations within each scale overlap and become entangled with redundant information. ", "page_idx": 1}, {"type": "text", "text": "Motivated by the above, we propose Ada-MSHyper, an Adaptive Multi-Scale Hypergraph Transformer for time series forecasting. Specifically, Ada-MSHyper map the input sequence into multiscale feature representations, then by treating the multi-scale feature representations as nodes, an adaptive multi-scale hypergraph structure is introduced to discover the abundant and implicit groupwise node interactions at different scales. To the best of our knowledge, Ada-MSHyper is the first work that incorporates adaptive hypergraph modeling into time series forecasting. The main contributions are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We design an adaptive hypergraph learning (AHL) module to model the abundant and implicit group-wise node interactions at different scales and a multi-scale interaction module to perform hypergraph convolution attention, which empower transformers with the ability to model group-wise pattern interactions at different scales. \u2022 We introduce a node and hyperedge constraint (NHC) mechanism during hypergraph learning phase, which utilizes semantic similarity to cluster nodes with similar semantic information and leverages distance similarity to differentiate the temporal variations within each scales. \u2022 We conduct extensive experiments on 11 real-world datasets. The experimental results demonstrate that Ada-MSHyper achieves state-of-the-art (SOTA) performance, reducing error by an average of $4.56\\%$ , $10.38\\%$ , and $4.97\\%$ in MSE for long-range, short-range, and ultra-long-range time series forecasting, respectively, compared to the best baseline. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Deep Models for Time Series Forecasting. Deep models have shown promising results in time series forecasting. To model temporal patterns at different scales and their interactions, a large number of specially designed backbones have emerged. TAMS-RNNs [10] captures periodic temporal dependencies through multi-scale recurrent structures with different update frequencies. TimesNet [29] extends the 1D time series into the 2D space, and models multi-scale temporal pattern interactions through 2D convolution inception blocks. Benefiting from the attention mechanism, transformers have gone beyond contemporaneous RNN- and CNN-based methods and achieved promising results in time series forecasting. FEDformer [39] combines mixture of expert and frequency attention to capture multi-scale temporal dependencies. Pyraformer [20] extends the input sequence into multi-scale representations and models the interactions between nodes at different scales through pyramidal attention. Nevertheless, with the rapid emergence of linear forecasters [22, 35, 11], the effectiveness of transformers in this direction is being questioned. ", "page_idx": 1}, {"type": "text", "text": "Recently, some methods have attempted to fully utilize transformers and paid attention to the inherent properties of time series. Some of these methods are dedicated to addressing the problem of semantic information sparsity in time series forecasting. PatchTST [23] segments the input sequence into subseries-level patches to enhance locality and capture group-wise interactions. MSHyper [26] models group-wise interactions through multi-scale hypergraph structures and introduces $k$ -hop connections to aggregate information from different range of neighbors. However, constrained by the fixed windows and predefined rules, these methods cannot discover implicit interactions. Others emphasize on addressing the problem of temporal variations entanglement in time series forecasting. FilM [38] differentiates temporal variations at different scales by decomposing the input series into different period lengths. iTransformer [21] combines inverted structures with transformer to learn entangled global temporal variations. However, these methods cannot differentiate temporal variations within each scale, making temporal variations within each scale overlap and become entangled with redundant information. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Hypergraph Neural Networks. As a generalized form of GNNs, hypergraph neural networks (HGNNs) have been applied in different fields, e.g., video object segmentation [17], stock selection [24], multi-agent trajectory prediction [31], and time series forecasting [26]. HyperGCN [32] is the first work that incorporates convolution operation into hypergraphs, which demonstrates the superiority of HGNNs over ordinary GNNs in capture group-wise interactions. Recent studies [2, 33] show that HGNNs are promising to model group-wise pattern interactions. LBSN2Vec $^{++}$ [34] uses hypergraphs for location-based social networks, which leverages heterogeneous hypergraph embeddings to capture mobility and social relationship pattern interactions. GroupNet [31] utilizes multi-scale hypergraph for trajectory prediction, which combines relational reasoning with hypergraph structures to capture group-wise pattern interactions among multiple agents. ", "page_idx": 2}, {"type": "text", "text": "Considering the capability of HGNNs in modeling group-wise interactions, in this work, an adaptive multi-scale hypergraph transformer framework is proposed to model the group-wise pattern interactions at different scales. Specifically, an AHL model is designed to model the abundant and implicit group-wise node interactions. In addition, a NHC mechanism is introduced to cluster nodes with similar semantic information and differentiate temporal variations within each scale, respectively. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Hypergraph. A hypergraph is defined as $\\mathcal{G}=\\{\\mathcal{V},\\mathcal{E}\\}$ , where $\\mathcal{E}\\,=\\,\\{e_{1},\\boldsymbol{\\cdot}\\boldsymbol{\\cdot}\\boldsymbol{\\cdot},e_{m},\\boldsymbol{\\cdot}\\boldsymbol{\\cdot},e_{M}\\}$ is the hyperedge set and $\\mathcal{V}=\\{v_{1},\\ldots,v_{n},\\ldots,v_{N}\\}$ is the node set. Each hyperedge represents group-wise interactions by connecting a set of nodes $\\{v_{1},v_{2},\\ldots,v_{n}\\}\\subseteq\\mathcal{V}.$ The topology of hypergraph can be represented as an incidence matrix $\\mathbf{H}\\in\\mathbb{R}^{N\\times M}$ , with entries $\\mathbf{H}_{n m}$ defined as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{H}_{n m}=\\left\\{\\begin{array}{l l}{1,\\quad}&{v_{n}\\in e_{m}}\\\\ {0,}&{v_{n}\\not\\in e_{m}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The degree of the $n$ th node is defined as $\\begin{array}{r}{d(v_{n})=\\sum_{m=1}^{M}\\mathbf{H}_{n m}}\\end{array}$ and the degree of the mth hyperedge is defined as $\\begin{array}{r}{d(v_{m})=\\sum_{n=1}^{N}\\mathbf{H}_{n m}}\\end{array}$ . Further, the  node degrees and hyperedge degrees are sorted in diagonal matirces ${\\bf D}_{\\mathrm{v}}\\in\\mathbb{R}^{N\\times N}$ and $\\mathbf{D}_{\\mathrm{e}}\\in\\mathbb{R}^{M\\times M}$ , respectively. ", "page_idx": 2}, {"type": "text", "text": "Problem Formulation. Given the input sequence $\\mathbf{X}_{1:T}^{\\mathrm{I}}=\\left\\{\\mathbf{x}_{t}\\mid\\mathbf{x}_{t}\\in\\mathbb{R}^{D},t\\in[1,T]\\right\\}$ , where $\\pmb{x}_{t}$ represents the values at time step $t,T$ is the input length, and $\\dot{D}$ is the feature dimension. The task of time series forecasting is to predict the future $H$ steps, which can be formulated as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\widehat{\\mathbf{X}}_{T+1:T+H}^{0}=\\mathcal{F}\\left(\\mathbf{X}_{1:T}^{\\mathrm{I}};\\boldsymbol{\\theta}\\right)\\in\\mathbb{R}^{H\\times D},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "twhhe elreea $\\widehat{\\mathbf{X}}_{T+1:T+H}^{0}$ mdeetnerost eosf e.  fTohree cdaesstcirnigp trieosnu lotfs ,t $\\mathcal{F}$ dkeeyn ontoetsa ttihoen sm aarpep ignivge fnu innc tAiopnp,e anndid $\\theta$ Ad.enotes $\\mathcal{F}$ ", "page_idx": 2}, {"type": "text", "text": "4 Ada-MSHyper ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "As previously mentioned, the core of Ada-MSHyper is to promote more comprehensive pattern interactions at different scales. To accomplish this goal, we first map the input sequence into subsequences at different scales through the multi-scale feature extraction (MFE) module. Then, by treating multi-scale feature representations as nodes, the AHL module is introduced to model the abundant and implicit group-wise node interactions at different scales. Finally, the multi-scale interaction module is introduced to model group-wise pattern interactions at different scales. Notably, during the hypergraph learning phase, an NHC mechanism is introduced to cluster nodes with similar semantic information and differentiate temporal variations within each scale. The overall framework of Ada-MSHyper is shown in Figure 1. ", "page_idx": 2}, {"type": "image", "img_path": "RNbrIQ0se8/tmp/6b8de0c47afb5a0dbf75304ac80ce8e4b3dacd09df29c3bdb5ef66f6bf9cb469.jpg", "img_caption": ["Figure 1: The framework of Ada-MSHyper. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "4.1 Multi-Scale Feature Extraction (MFE) Module ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The MFE module is designed to get the feature representations at different scales. As shown in Figure 1(a), suppose $\\mathbf{X}^{s}\\breve{=}\\left\\{\\pmb{x}_{t}^{s}|\\pmb{x}_{t}^{\\breve{s}}\\in\\mathbb{R}^{D},t\\in[1,\\hat{N^{s}}]\\right\\}$ denotes the sub-sequence at scale $s$ , where $s=1,...,S$ denotes the scale index and $S$ is the total number of scales. $\\begin{array}{r}{N^{s}=\\left\\lfloor\\frac{N^{s-1}}{l^{s-1}}\\right\\rfloor}\\end{array}$ Nls \u22121 is the number of nodes at scale $s$ and $l^{s-1}$ denotes the size of the aggregation window at scale $s-1$ . $\\mathbf{X}^{1}=\\mathbf{X}_{1:T}^{\\mathrm{I}}$ is the raw input sequence and the aggregation process can be formulated as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{X}^{s}=A g g(\\mathbf{X}^{s-1};\\theta^{s-1})\\in\\mathbb{R}^{N^{s}\\times D},s\\geq2,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $A g g$ is the aggregation function, e.g., 1D convolution or average pooling, and $\\theta^{s-1}$ denotes the learnable parameters of the aggregation function at scale $s-1$ . ", "page_idx": 3}, {"type": "text", "text": "4.2 Adaptive Hypergraph Learning (AHL) Module ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The AHL module automatically generates incidence matrices to model implicit group-wise node interactions at different scales. As shown in Figure 1(b), we first initialize two kinds of parameters, i.e., node  embeddings $E_{\\mathrm{node}}^{s}\\in\\mathbb{R}^{N^{s}\\times D}$ and hyperedge embeddings $E_{\\mathrm{hyper}}^{s}\\in\\mathbb{R}^{M^{s}\\times\\bar{D}}$ at scale $s$ , where is hyperparameters, representing the number of hyperedges at scale $s$ . Then, we can obtain the scale-specific incidence matrix $\\mathbf{H}^{s}$ by similarity calculation, which can be formulated as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{H}^{s}=S o f t M a x(R e L U(\\mathbf{E}_{\\mathrm{node}}^{s}(\\mathbf{E}_{\\mathrm{hyper}}^{s})^{T})),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the $R e L U$ activation function is used to eliminate weak connections and the SoftMax function is applied to normalize the value of $\\mathbf{H}^{s}$ . In order to reduce subsequent computational costs and noise interference, the following strategy is designed to sparsify the incidence matrix: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{H}_{n m}^{s}=\\{\\mathbf{H}_{n m}^{s},\\quad\\mathbf{H}_{n m}^{s}\\in T o p K(\\mathbf{H}_{n*}^{s},\\boldsymbol{\\eta})}\\\\ {0,\\quad\\mathbf{H}_{n m}^{s}\\not\\in T o p K(\\mathbf{H}_{n*}^{s},\\boldsymbol{\\eta})}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\eta$ is the threshold of $T o p K$ function and denotes the max number of neighboring hyperedges connected to a node. The final values of $\\mathbf{H}_{n m}^{s}$ can be obtained as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{H}_{n m}^{s}=\\bigl\\{1,\\quad\\mathbf{H}_{n m}^{s}\\!>\\!\\beta\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\beta$ denotes the threshold, and the final scale-specific incidence matrices can be represented as $\\{\\mathbf{H}^{1},\\cdot\\cdot\\cdot,\\mathbf{H}^{s},\\cdot\\cdot\\cdot,\\mathbf{H}^{S}\\}$ . Compared to previous methods, our adaptive hypergraph learning is novel from two aspects. Firstly, our methods can capture group-wise interactions at different scales, while most previous methods [5, 21] can only model pair-wise interactions at a single scale. Secondly, our methods can model abundant and implicit interactions, while many previous methods [23, 26] depend on fixed windows and predefined rules. ", "page_idx": 3}, {"type": "image", "img_path": "RNbrIQ0se8/tmp/e5f724d3725feb8e3bc69b962ac04a67006b924e8c06cde885ca1c8f7c014915.jpg", "img_caption": ["Figure 2: The node and hyperedge constraint mechanism. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "4.3 Node and Hyperedge Constraint (NHC) Mechanism ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Although the AHL module can help discover implicit group-wise node interactions at different scales, we argue that the pure data-driven approach faces two limitations, i.e., unable to efficiently cluster nodes with similar semantic information and differentiate temporal variations within each scale. To tackle the above dilemmas, we introduce the NHC mechanism during hypergraph learning phase. ", "page_idx": 4}, {"type": "text", "text": "Given the multi-scale feature representations $\\{\\mathbf{X}^{1},\\cdot\\cdot\\cdot,\\mathbf{X}^{s},\\cdot\\cdot\\cdot,\\mathbf{X}^{S}\\}$ generated from the MFE module, and the scale-specific incidence matrices $\\{\\mathbf{H}^{1},\\cdot\\cdot\\cdot,\\mathbf{H}^{s},\\cdot\\cdot\\cdot,\\mathbf{H}^{s}\\}$ generated from the AHL module, we first get the initialized node feature representations $\\pmb{\\upnu}^{s}=f(\\mathbf{X}^{s})$ at scale $s$ , where $f$ can be implemented by the multi-layer perceptron (MLP). As shown in Figure 2(a), the initialized hyperedge feature representations can be obtained by the aggregation operation based on $\\mathbf{H}^{s}$ . Specifically, for the $i$ th hyperedge $e_{i}^{s}$ at scale $s$ , its feature representations $e_{i}^{s}$ can be formulated as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\pmb{e}_{i}^{s}=a v g(\\sum_{v_{j}^{s}\\in\\mathcal{N}(e_{i}^{s})}\\pmb{v}_{j}^{s})\\in\\mathbb{R}^{D},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where avg is the average operation, $\\mathcal{N}(e_{i})$ represents the neighboring nodes connected by $e_{i}^{s}$ at scale $s$ , and $v_{j}^{s}\\in\\mathcal{V}^{s}$ is the $j$ th node feature representations at scale $s$ . The initialized hyperedge feature representations at different scales can be represented as $\\{\\pmb{\\mathcal{E}}^{1},\\cdot\\cdot\\cdot,\\pmb{\\mathcal{E}}^{s},\\cdot\\cdot\\cdot,\\pmb{\\mathcal{E}}^{S}\\}$ . Then, based on semantic similarity and distance similarity, we introduce node constraint to cluster nodes with similar semantic information and leverage hyperedge constraint to differentiate the temporal variations of temporal patterns. ", "page_idx": 4}, {"type": "text", "text": "Node Constraint. In the data-driven hypergraph, we observe that some nodes connected by the same hyperedge contain distinct semantic information. To cluster nodes with similar semantic information and reduce irrelevant noise interference, we introduce node constraint based on the semantic similarity between nodes and their corresponding hyperedges. As shown in Figure $2({\\mathfrak{b}})$ , for the $j$ th node at scale $s$ , we first obtain its semantic similarity difference $\\widetilde{\\pmb{v}_{j}^{s}}$ with its corresponding hyperedges: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\widetilde{\\pmb{v}_{j}^{s}}=\\{a b s(\\pmb{v}_{j}^{s}-\\pmb{e}_{i}^{s})|\\pmb{v}_{j}^{s}\\in\\mathcal{N}(\\pmb{e}_{i}^{s})\\},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $a b s$ refers to the operation of calculating the absolute value. The node loss $L_{n o d e}^{s}$ at scale $s$ based on node constraint can be formulated as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\nL_{n o d e}^{s}=\\frac{1}{N^{s}}\\sum_{i=1}^{N^{s}}\\widetilde{v_{j}^{s}},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $N^{s}$ is the number of nodes at scale $s$ . Empowered by the node constraint, our method can enjoy more advantageous group-wise semantic information than pure data-driven hypergraph. Further experimental results and visualization analysis in Section 5.3 and Appendix H demonstrate the effectiveness of the node constraint in clustering nodes with similar semantic information. ", "page_idx": 4}, {"type": "text", "text": "Hyperedge Constraint. Since time series is a collection of data points arranged in chronological order, some recent works [23, 26] show that connecting multiple nodes sequentially through patches or hyperedges can represent specific temporal variations. Therefore, to deal with the problem of temporal variations entanglement, we introduce hyperedge constraint based on distance similarity. As shown in Figure 2(c), we first compute the cosine similarity to reflect the correlation of any two hyperedge representations at scale $s$ , which can be formulated as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\alpha_{i,j}=\\frac{e_{i}^{s}(e_{j}^{s})^{T}}{\\|e_{i}^{s}\\|_{2}\\left\\|e_{j}^{s}\\right\\|_{2}},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\alpha_{i,j}$ represents the correlation weight. $e_{i}^{s}$ and $e_{j}^{s}$ are the $i$ th and $j$ th hyperedge representation at scale $s$ , respectively. Then, we use Euclidean distance $D_{i,j}$ to measure the differentiation magnitude between any two hyperedge representations, which can be formulated as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\nD_{i,j}=\\left\\|e_{i}^{s}-e_{j}^{s}\\right\\|_{2}=\\sqrt{\\sum_{d=1}^{D}((e_{i}^{s})^{d}-(e_{j}^{s})^{d})^{2}},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The hyperedge loss $L_{h y p e r}^{s}$ at scale $s$ based on the correlation weight and Euclidean distance can be formulated as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\nL_{h y p e r}^{s}=\\frac{1}{(M^{s})^{2}}\\sum_{i=1}^{M^{s}}\\sum_{j=1}^{M^{s}}\\left(\\alpha_{i,j}D_{i,j}+(1-\\alpha_{i,j})m a x(\\gamma-D_{i,j},0)\\right),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\gamma>0$ denotes the threshold. Notably, when $\\alpha_{i,j}=1$ , indicating that $e_{i}^{s}$ and $e_{k}^{s}$ are deemed similar, the hyperedge loss turns to $\\begin{array}{r}{L_{h y p e r}\\;=\\;\\frac{1}{(M^{s})^{2}}\\sum_{i=1}^{M^{s}}\\sum_{j=1}^{M^{s}}\\alpha_{i,j}D_{i,j}}\\end{array}$ (M 1s)2 i= 1 j= 1 \u03b1i,jDi,j, where the loss will increase if $D_{i,j}$ becomes large. Conversely, when $\\alpha_{i,j}\\,=\\,0$ , meaning $e_{i}$ and $e_{k}$ are regarded as dissimilar, the hyperedge loss turns to $\\begin{array}{r}{L_{h y p e r}\\,=\\,\\frac{1}{(M^{s})^{2}}\\sum_{i=1}^{M^{s}}\\sum_{j=1}^{M^{s}}(1-\\alpha_{i,j})m a x(s-D_{i,j},0),}\\end{array}$ where the loss will increase if $D_{i,j}$ falls below the threshold and turns smaller. Other cases lie between the above circumstances. We further provide the visualization results in Section 5.3 and appendix $\\mathrm{H}$ to verify that our constraint loss can differentiate temporary variations of temporary patterns within each scale and promote forecasting performance. The final constraint loss $L_{c o n s t}$ based on node constraint and hyperedge constraint can be formulated as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\nL_{c o n s t}=\\lambda\\sum_{s=1}^{S}L_{n o d e}^{s}+(1-\\lambda)\\sum_{s=1}^{S}L_{h y p e r}^{s},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "wherer $\\lambda$ denotes the hyperparameter controlling the balance between node loss and hyperedge loss. ", "page_idx": 5}, {"type": "text", "text": "4.4 Multi-Scale Interaction Module ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To promote more comprehensive pattern interactions at different scales, a direct way is to mix multi-scale node feature representations at different scales. However, we argue that intra-scale interactions and inter-scale interactions reflect different aspects of pattern interactions, where intrascale interactions mainly depict detailed interactions between nodes with similar semantic information and inter-scale interactions highlight macroscopic variations interactions[9, 27]. Therefore, instead of directly mixing multi-scale pattern information as a whole, we introduce the multi-scale interaction module to perform inter-scale interactions and intra-scale interactions. ", "page_idx": 5}, {"type": "text", "text": "Intra-Scale Interaction Module. Due to the semantic information sparsity of time series, traditional pair-wise attention may may cause the information utilization bottleneck [5]. In contrast, some recent studies [23, 26] show that group-wise interactions can provide more informative insights in time series forecasting. To capture group-wise interactions among nodes with similar semantic information within each scale, we introduce hypergraph convolution attention within the intra-scale interaction module. Specifically, given $\\mathbf{H}^{s}$ , we first use attention mechanism to capture the interaction strength of each node $v_{i}^{s}\\in\\pmb{\\nu}^{s}$ and its related hyperedges at scale $s$ , which can be formulated as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{H}_{i j}^{s}=\\frac{\\exp(\\sigma(f_{t}[\\boldsymbol{v}_{i}^{s},\\boldsymbol{e}_{j}^{s}]))}{\\sum_{\\boldsymbol{e}_{k}^{s}\\in\\mathcal{N}(\\boldsymbol{v}_{i}^{s})}e x p(\\sigma(f_{t}[\\boldsymbol{v}_{i}^{s},\\boldsymbol{e}_{k}^{s}])},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $[.,.]$ denotes the concatenation operation of the $i$ th node and its related hyperedges. $f_{t}$ is a trainable MLP, and $\\mathcal{N}(v_{i}^{s})$ is the neighboring hyperedges connected to $v_{i}^{s}$ , which can be accessed using $\\mathbf{H}^{s}$ . Then, considering the symmetric normalized hypergraph Laplacian convolution $\\Delta=$ ${\\mathbf D}_{v}^{-1/2}{\\mathbf H}{\\mathbf W}{\\mathbf D}_{e}^{-1}{\\mathbf H}^{T}{\\mathbf D}_{v}^{-1/2}$ used in HGNN [13], the multi-head hypergraph convolution attention can be formulated as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\widetilde{\\boldsymbol{\\mathcal{V}}}^{s}=\\bigoplus_{\\jmath=1}^{\\mathcal{I}}(\\sigma(\\mathbf{D}_{v^{s}}^{-1/2}\\mathcal{H}_{\\jmath}^{s}\\mathbf{D}_{e^{s}}^{-1}(\\mathcal{H}_{\\jmath}^{s})^{\\mathrm{T}}\\mathbf{D}_{v^{s}}^{-1/2}\\boldsymbol{\\mathcal{V}}^{s}\\mathbf{P}_{\\jmath}^{s}))\\in\\mathbb{R}^{N^{s}\\times D},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\tilde{\\nu}^{s}$ is the updated node feature representations at scale $s$ , $\\oplus$ is the aggregation function used for co mbing the outputs of multi-head, e.g., concatenation or a verage pooling. $\\sigma$ is the activation function, e.g., LeakyReLU and ELU. $\\mathcal{H}_{\\boldsymbol{j}}^{s}$ and $\\mathbf{P}_{\\mathcal{I}}^{s}$ are the enriched incidence matrix and the learnable weight matrix of the \u0237th head at scale $s$ , respectively. $\\mathcal{I}$ is the number of heads. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Inter-Scale Interaction Module. The inter-scale interaction module is introduced to capture pattern interactions at different scales. To achieve this goal, a direct way is to model group-wise node interactions across all scales. However, detailed group-wise node interactions across all scales can introduce redundant information and increase computation complexity. Therefore, we adopt a hyperedge attention within the inter-scale interaction module to capture macroscopic variations interactions at different scales. Technically, based on the hyperedge representations $\\pmb{\\mathcal{E}}=\\{\\pmb{\\mathcal{E}}^{1},\\cdot\\cdot\\cdot,\\pmb{\\mathcal{E}}^{s},\\cdot\\cdot\\cdot,\\pmb{\\mathcal{E}}^{S}\\}$ , we first adopt linear projections to get queries, keys, and values $\\mathbf{Q}$ , K, $\\mathbf{V}\\in\\mathbb{R}^{M\\times D}$ . Then the hyperedge attention can be formulated as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\tilde{\\mathbf{V}}=s o f t m a x(\\frac{\\mathbf{Q}\\mathbf{K}^{\\mathrm{T}}}{\\sqrt{D_{K}}})\\mathbf{V},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\tilde{\\textbf{V}}$ is the updated hyperedge feature representations. ", "page_idx": 6}, {"type": "text", "text": "4.5 Prediction Module & Loss Function ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "After obtaining the updated node and hyperedge feature representations, we concatenate them and feed them into a linear layer for prediction. We choose Mean Squared Error (MSE) as our forecasting loss, which can be formulated as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\nL_{m s e}=\\frac{1}{H}\\left\\|\\widehat{\\mathbf{X}}_{T+1:T+H}^{0}-\\mathbf{X}_{T+1:T+H}^{0}\\right\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\mathbf{X}_{T+1:T+H}^{0}$ and $\\widehat{\\mathbf{X}}_{T+1:T+H}^{0}$ are ground truth and forecasting results, respectively. Notably, during training phase, $L_{m s e}$ is used to regulate the overall learning process, while is only used to constrain hypergraph learning process. ", "page_idx": 6}, {"type": "text", "text": "4.6 Complexity Analysis ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "For the MFE module, the time complexity is $\\mathcal{O}(N l)$ , where $N$ is the number of nodes at the finest scale and $N$ is equal to the input length $T,\\,l$ is the aggregation window size at the finest scale. For the AHL module, the time complexity is $\\mathcal{O}(M N+\\bar{M}^{2})$ , where $M$ is the number of hypergraphs at the finest scale. For the intra-scale interaction module, since $\\mathbf{D}_{v}$ and ${\\bf D}_{e}$ are diagonal matrices, the time complexity is $\\mathcal{O}(M N)$ . For the inter-scale interaction module, the time complexity is $M^{2}$ . In practical operation, $M$ and $l$ is the hyperparameter and is much smaller than $N$ . As a result, the total time complexity of Ada-MSHyper is bounded by $\\mathcal{O}(N)$ . ", "page_idx": 6}, {"type": "text", "text": "5 Experiment ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "5.1 Experimental Setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Datasets. For long-range time series forecasting, we conduct experiments on 7 commonly used benchmarks, including ETT (ETTh1, ETTh2, ETTm1, and ETTm2), Traffic, Electricity, and Weather datasets following [30, 21, 26]. For short-range time series forecasting, we adopt 4 benchmarks from PEMS (PEMS03, PEMS04, PEMS07, and PEMS08) following [21, 27]. For ultra", "page_idx": 6}, {"type": "table", "img_path": "RNbrIQ0se8/tmp/b5c3d70c1f23065bcdad8ac361a992cead958a5509d12066b26225f5f69e8d3e.jpg", "table_caption": ["Table 1: Dataset statistics. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "long-range time series forecasting, we adopt ETT datasets following [18]. Table 1 gives the dataset statistics. In addition, the forecastability is derived from one minus the entropy of the Fourier decomposition of a time series[27, 14]. Higher values mean greater forecastability. ", "page_idx": 6}, {"type": "text", "text": "Baselines. We compare Ada-MSHyper with 15 competitive baselines, i.e., iTransformer [21], MSHyper [26], PatchTST [23], TimeMixer [27], MSGNet [4], CrossGNN [16], TimesNet [29], WITRAN [18], SCINet [19], Crossformer [36], FiLM [38], DLinear [35], FEDformer [39], Pyraformer [20], and Autoformer [30]. ", "page_idx": 6}, {"type": "text", "text": "Experimental Settings. Ada-MSHyper is trained/tested on a single NVIDIA Geforce RTX 3090 GPU. MSE and MAE are used as evaluation metrics and lower values mean better performance. Adam is set as the optimizer with the initial learning rate of $10^{-4}$ . It is notable that the above mentioned baseline results cannot be used directly due to different input and output lengths. For a fair comparison, we set the commonly used input length $T=96$ and output lengths $H\\in\\{96,192,336,720\\}$ for long-range forecasting, $H\\in\\{12,24,48\\}$ for short-range forecasting, and $H\\in\\{1080,1440,1800,2160\\}$ for ultra-long-range forecasting. More descriptions about datasets, baselines, and experimental settings are given in Appendix B, C, and $\\mathrm{D}$ , respectively. ", "page_idx": 7}, {"type": "text", "text": "5.2 Main Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Long-Range Forecasting. Table 2 shows the results of long-range time series forecasting under multivariate settings. We can observe that: (1) Ada-MSHyper achieves the SOTA results in all datasets, with an average error reduction of $4.56\\%$ and $3.47\\%$ compared to the best baseline in MSE and MAE, respectively. (2) FEDformer and Autoformer exhibit relatively poor predictive performance. This may be that vanilla attention and simplistic decomposition techniques are insufficient in capturing multiscale pattern interactions. (3) By considering multi-scale pattern interactions, TimeMixer achieves competitive results. However, its performance deteriorates on the datasets with low forecastability (e.g., ETTh1 and ETTh2 datasets). In contrast, Ada-MSHyper still maintains superiority on low forecastability datasets by modeling group-wise pattern interactions. Notably, despite modeling group-wise pattern interactions, the performance of MSHyper and PatchTST still lags behind that of Ada-MSHyper, indicating that predefined rules may overlook implicit interactions and introduce noise interference for forecasting. Moreover, for long-range time series forecasting under univariate settings, Ada-MSHyper gives an average error reduction of $7.57\\%$ and $4.65\\%$ compared to the best baseline in MSE and MAE, respectively. The univariate results are given in Appendix E. ", "page_idx": 7}, {"type": "table", "img_path": "RNbrIQ0se8/tmp/0306e4922c8c87c5d70f93fbcf677f2dabb9974bbcca6b5ffb52f0f8c14d0e8b.jpg", "table_caption": ["Table 2: Results of long-range time series forecasting under multivariate settings. The best results are bolded and the second best results are underlined. Results are averaged from all prediction lengths. Full results are listed in Appendix E. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Short-Range Forecasting. Table 3 summarizes the results of short-range time series forecasting under multivariate settings. It is notable that the PEMS datasets record multiple time series of citywide traffic networks and show complex spatial-temporal correlations among multiple variates. We adopt the same settings as iTransformer [26] and TimeMixer [20]. Ada-MSHyper still achieves the best performance in PEMS datasets, verifying its effectiveness in handling complex multivariate short-range time series forecasting. Specifically, Ada-MSHyper gives an average error reduction of $10.38\\%$ and $3.82\\%$ compared to the best baseline in terms of MSE and MAE, respectively. ", "page_idx": 7}, {"type": "table", "img_path": "RNbrIQ0se8/tmp/ed2d315546a1d266b0f2568f22ec3b3b2507f55ab1a5a8766d588fbdb4d3d4c8.jpg", "table_caption": ["Table 3: Results of short-range time series forecasting under multivariate settings. Results are averaged from all prediction lengths. Full results are listed in Appendix E. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Ultra-Long-Range Forecasting. Table 4 summarizes the results of ultra-long-range time series forecasting under multivariate settings. We can see that: (1) Ada-MSHyper achieves SOTA results in almost all benchmarks, with an average error reduction of $4.97\\%$ and $2.21\\%$ compared to the best baseline in MSE and MAE, respectively. (2) Compared with other baselines, PatchTST and MSHyper achieve competitive results. The reason may be that group-wise interactions can help mitigate the issue of semantic information sparsity. (3) Compared to PatchTST and MSHyper, Ada-MSHyper achieves superior performance, the reason may be that the inter-scale interaction module can help capture macroscopic variations interactions, especially for the ultra-long-rang time series. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "table", "img_path": "RNbrIQ0se8/tmp/27fd3f606e45b2e50e437a3341ec244129831778b2572206b8278e7523787170.jpg", "table_caption": ["Table 4: Results of ultra-long-range time series forecasting under multivariate settings. Results are averaged from all prediction lengths. Full results are listed in Appendix E. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.3 Ablation Studies ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "AHL Module. To investigate the effectiveness of the AHL model, we conduct ablation studies by designing the following three variations: (1) Replacing the AHL module with adaptive graph learning module (-AGL). (2) Replacing the AHL model with one incidence matrix to capture group-wise node interactions at different scales (-one). (3) Replacing the AHL module with predefined multi-scale hypergraphs (-PH), i.e., each hyperedge connected a fixed number of nodes (4 in our experiment) in chronological order. The experimental results on ETTh1 dataset are shown in Table 5. We can observe that -AGL gets the worst forecasting results, indicating the importance of modeling group-wise interactions. In addition, -PH and -one perform worse than Ada-MSHyper, showing the effectiveness of adaptive hypergraph and multi-scale hypergraph, respectively. ", "page_idx": 8}, {"type": "table", "img_path": "RNbrIQ0se8/tmp/5eff3e49915f39912e4fcf45bc7c4c9fb3e2dd8b643b30cd796e6464a18d1c84.jpg", "table_caption": ["Table 5: Results of different adaptive hypergraph learning methods and constraint mechanisms. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "NHC Mechanism. To investigate the effectiveness of the NHC mechanism, we conduct ablation studies by designing the following three variations: (1) Removing the node constraint (-w/o NC). (2) Removing the hyperedge constraint (-w/o HC). (3) Removing the NHC mechanism (-w/o NHC). The experimental results on ETTh1 dataset are shown in Table 5. We can observe that Ada-MSHyper performs better than -w/o NC and -w/o HC, showing the effectiveness of node constraint and hyperedge constraint, respectively. In addition, -w/o NHC gets the worst forecasting results, which demonstrates the superiority of the NHC mechanism in adaptive hypergraph learning. More results about ablation studies are shown in Appendix F. ", "page_idx": 8}, {"type": "text", "text": "To further demonstrate the effectiveness of the node constraint in clustering nodes with similar semantic information, we present case visualization with -w/o NHC and -w/o HC on Electricity dataset. We randomly select one sample and plot the node values at the finest scale. We categorize the nodes into four groups based on the node values. Nodes with the same color indicate that they may have similar semantic information. As shown in Figure 3a, for the target node, nodes of other colors may be considered as noise. We drew the nodes related to the target node in black color based on incidence matrix $\\mathbf{H}^{1}$ . As shown in Figure 3b, due to the lack of node constraint, -w/o NHC can only capture the interactions among the target node and neighboring nodes and cannot distinguish nuanced noise information. In Figure 3c, with the node constraint, -w/o HC can cluster neighboring and distant but still strongly correlated nodes. In Figure 3d, with the NHC mechanism, Ada-MSHyper cannot only cluster nodes with similar semantic information but can differentiate temporal variations. The full visualization results are shown in Appendix H. ", "page_idx": 8}, {"type": "text", "text": "5.4 Parameter Studies ", "text_level": 1, "page_idx": 8}, {"type": "image", "img_path": "RNbrIQ0se8/tmp/88e3f0923e8ba0941808959d4d98cbf0b1b88c5cc55d3f64a7cba1bb35c77db0.jpg", "img_caption": ["Figure 3: Visualization the node constraint effect on Electricity dataset. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "We perform parameter studies to measure the impact of the number of scales (#scales) and the max number of hyperedges connected to a node (#hyperedges). The experimental results on ETTh1 dataset are shown in Figure 4, we can see that: (1) the best performance can be obtained when #scales is 3. The reason is that smaller #scales cannot provide sufficient pattern information and larger #scales may introduce excessive parameters and result in overfitting ", "page_idx": 9}, {"type": "image", "img_path": "RNbrIQ0se8/tmp/c76de0a9a1ababe8db8428916e1ee44cf9d7a4f3a380d9240b25c3635f080e48.jpg", "img_caption": ["Figure 4: The impact of hyperparameters. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "problems. (2) The optimal #hyperedges is 5. The reason is that smaller #hyperedges cannot capture group-wise interactions sufficiently and larger #hyperedges may introduce noise. More results about parameter studies are shown in Appendix G. ", "page_idx": 9}, {"type": "text", "text": "5.5 Computational Cost ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We compare Ada-MSHyper with the two latest transformer-based methods, i.e., iTransformer and PatchTST, on traffic datasets with the output length of 96. The experimental results are shown in Table 6. Although we have a larger number of parameters, we achieve lower training time and lower GPU occupation due to the matrix sparsity operation in the model and the optimization of hypergraph computation provided by torch_geometry[2]. Considering the forecasting performance and the computation cost, Ada-MSHyper demonstrates its superiority over existing methods. ", "page_idx": 9}, {"type": "table", "img_path": "RNbrIQ0se8/tmp/bf71106de9a43cd657c2f64198bb5af89959520d10b5d088beb7a115d673f532.jpg", "table_caption": ["Table 6: Computation cost. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "6 Conclusions and Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we propose Ada-MSHyper with an adaptive multi-scale hypergraph for time series forecasting. Empowered by the AHL module and multi-scale interaction module, Ada-MSHyper can promote more comprehensive multi-scale group-wise pattern interactions, addressing the problem of semantic information sparsity. Experimentally, Ada-MSHyper achieves the SOTA performance, reducing prediction errors by an average of $4.56\\%$ , $10.38\\%$ , and $4.97\\%$ in MSE for long-range, short-range, and ultra-long-range time series forecasting, respectively. In addition, the visualization analysis and the ablation studies demonstrate the effectiveness of NHC mechanism in clustering nodes with similar semantic information and in addressing the issue of temporal variations entanglement. ", "page_idx": 9}, {"type": "text", "text": "In the future, this work can be extended in the following two aspects. First, since 2D spectrogram data may offer a better representation for time series forecasting, we will adapt our framework to the 2D spectrogram data in time-frequency domain. Second, since the features extracted by the MFE module may contain redundant information, we will design a disentangled multi-scale feature extraction module to extract more independent and representative features. ", "page_idx": 9}, {"type": "text", "text": "7 Acknowledgement ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was supported by the Science Foundation of Donghai Laboratory (Grant No. DH2022ZY0013). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Shaojie Bai, J Zico Kolter, and Vladlen Koltun. An empirical evaluation of generic convolutional and recurrent networks for sequence modeling. arXiv preprint arXiv:1803.01271, 2018.   \n[2] Song Bai, Feihu Zhang, and Philip HS Torr. Hypergraph convolution and hypergraph attention. Pattern Recognition, 110:107637, 2021.   \n[3] George EP Box and Gwilym M Jenkins. Some recent advances in forecasting and control. Journal of the Royal Statistical Society. Series C (Applied Statistics), 17(2):91\u2013109, 1968.   \n[4] Wanlin Cai, Yuxuan Liang, Xianggen Liu, Jianshuai Feng, and Yuankai Wu. MSGNet: Learning multi-scale inter-series correlations for multivariate time series forecasting. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 11141\u201311149, 2024.   \n[5] Haizhou Cao, Zhenhao Huang, Tiechui Yao, Jue Wang, Hui He, and Yangang Wang. InParformer: Evolutionary decomposition transformers with interactive parallel attention for long-term time series forecasting. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 6906\u20136915, 2023.   \n[6] Donghui Chen, Ling Chen, Zongjiang Shang, Youdong Zhang, Bo Wen, and Chenghu Yang. Scale-aware neural architecture search for multivariate time series forecasting. ACM Transactions on Knowledge Discovery from Data, 2024.   \n[7] Donghui Chen, Ling Chen, Youdong Zhang, Bo Wen, and Chenghu Yang. A multiscale interactive recurrent network for time-series forecasting. IEEE Transactions on Cybernetics, 52(9):8793\u20138803, 2021.   \n[8] Ling Chen, Donghui Chen, Zongjiang Shang, Binqing Wu, Cen Zheng, Bo Wen, and Wei Zhang. Multi-scale adaptive graph neural network for multivariate time series forecasting. IEEE Transactions on Knowledge and Data Engineering, pages 10748\u201310761, 2023.   \n[9] Ling Chen and Jiahua Cui. TPRNN: A top-down pyramidal recurrent neural network for time series forecasting. arXiv preprint arXiv:2312.06328, 2023.   \n[10] Zipeng Chen, Qianli Ma, and Zhenxi Lin. Time-aware multi-scale RNNs for time series modeling. In IJCAI, pages 2285\u20132291, 2021.   \n[11] Abhimanyu Das, Weihao Kong, Andrew Leach, Shaan K Mathur, Rajat Sen, and Rose Yu. Longterm forecasting with TiDE: Time-series dense encoder. Transactions on Machine Learning Research, 2023.   \n[12] Vijay Ekambaram, Arindam Jati, Nam Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. Tsmixer: Lightweight mlp-mixer model for multivariate time series forecasting. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 459\u2013469, 2023.   \n[13] Yifan Feng, Haoxuan You, Zizhao Zhang, Rongrong Ji, and Yue Gao. Hypergraph neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 3558\u20133565, 2019.   \n[14] Georg Goerg. Forecastable component analysis. In International conference on machine learning, pages 64\u201372. PMLR, 2013.   \n[15] Hansika Hewamalage, Klaus Ackermann, and Christoph Bergmeir. Forecast evaluation for data scientists: common pitfalls and best practices. Data Mining and Knowledge Discovery, 37(2):788\u2013832, 2023.   \n[16] Qihe Huang, Lei Shen, Ruixin Zhang, Shouhong Ding, Binwu Wang, Zhengyang Zhou, and Yang Wang. CrossGNN: Confronting noisy multivariate time series via cross interaction refinement. Advances in Neural Information Processing Systems, 36:46885\u201346902, 2023.   \n[17] Yuchi Huang, Qingshan Liu, and Dimitris Metaxas. ] video object segmentation by hypergraph cut. In 2009 IEEE conference on computer vision and pattern recognition, pages 1738\u20131745. IEEE, 2009.   \n[18] Yuxin Jia, Youfang Lin, Xinyan Hao, Yan Lin, Shengnan Guo, and Huaiyu Wan. WITRAN: Water-wave information transmission and recurrent acceleration network for long-range time series forecasting. Advances in Neural Information Processing Systems, 36, 2024.   \n[19] Minhao Liu, Ailing Zeng, Muxi Chen, Zhijian Xu, Qiuxia Lai, Lingna Ma, and Qiang Xu. SCINet: Time series modeling and forecasting with sample convolution and interaction. Advances in Neural Information Processing Systems, 35:5816\u20135828, 2022.   \n[20] Shizhan Liu, Hang Yu, Cong Liao, Jianguo Li, Weiyao Lin, Alex X Liu, and Schahram Dustdar. Pyraformer: Low-complexity pyramidal attention for long-range time series modeling and forecasting. In Proceedings of the International Conference on Learning Representations, 2021.   \n[21] Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, and Mingsheng Long. iTransformer: Inverted transformers are effective for time series forecasting. In The Twelfth International Conference on Learning Representations, 2023.   \n[22] Yong Liu, Chenyu Li, Jianmin Wang, and Mingsheng Long. Koopa: Learning non-stationary time series dynamics with koopman predictors. In A. Oh, T. Neumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 12271\u201312290. Curran Associates, Inc., 2023.   \n[23] Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. A time series is worth 64 words: Long-term forecasting with transformers. In The Eleventh International Conference on Learning Representations, 2022.   \n[24] Ramit Sawhney, Shivam Agarwal, Arnav Wadhwa, Tyler Derr, and Rajiv Ratn Shah. Stock selection via spatiotemporal hypergraph attention network: A learning to rank approach. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 497\u2013504, 2021.   \n[25] Rajat Sen, Hsiang-Fu Yu, and Inderjit S Dhillon. Think globally, act locally: A deep neural network approach to high-dimensional time series forecasting. Advances in neural information processing systems, 32, 2019.   \n[26] Zongjiang Shang and Ling Chen. MSHyper: Multi-scale hypergraph transformer for long-range time series forecasting. arXiv preprint arXiv:2401.09261, 2024.   \n[27] Shiyu Wang, Haixu Wu, Xiaoming Shi, Tengge Hu, Huakun Luo, Lintao Ma, James Y Zhang, and JUN ZHOU. TimeMixer: Decomposable multiscale mixing for time series forecasting. In The Twelfth International Conference on Learning Representations, 2023.   \n[28] Binqing Wu, Weiqi Chen, Wengwei Wang, Bingqing Peng, Liang Sun, and Ling Chen. WeatherGNN: Exploiting meteo- and spatial-dependencies for local numerical weather prediction bias-correction. In Proceedings of the International Joint Conference on Artificial Intelligence, pages 2433\u20132441, 2024.   \n[29] Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long. TimesNet: Temporal 2d-variation modeling for general time series analysis. In The eleventh international conference on learning representations, 2022.   \n[30] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting. Advances in Neural Information Processing Systems, pages 22419\u201322430, 2021.   \n[31] Chenxin Xu, Maosen Li, Zhenyang Ni, Ya Zhang, and Siheng Chen. GroupNet: Multiscale hypergraph neural networks for trajectory prediction with relational reasoning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6498\u20136507, 2022.   \n[32] Naganand Yadati, Madhav Nimishakavi, Prateek Yadav, Vikram Nitin, Anand Louis, and Partha Talukdar. HyperGCN: A new method for training graph convolutional networks on hypergraphs. Advances in Neural Information Processing Systems, 32, 2019.   \n[33] Yichao Yan, Jie Qin, Jiaxin Chen, Li Liu, Fan Zhu, Ying Tai, and Ling Shao. Learning multigranular hypergraphs for video-based person re-identification. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2899\u20132908, 2020.   \n[34] Dingqi Yang, Bingqing Qu, Jie Yang, and Philippe Cudr\u00e9-Mauroux. LBSN2Vec $^{++}$ : Heterogeneous hypergraph embedding for location-based social networks. IEEE Transactions on Knowledge and Data Engineering, 34(4):1843\u20131855, 2020.   \n[35] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers effective for time series forecasting? arXiv preprint arXiv:2205.13504, 2022.   \n[36] Yunhao Zhang and Junchi Yan. Crossformer: Transformer utilizing cross-dimension dependency for multivariate time series forecasting. In Proceedings of the International Conference on Learning Representations, 2023.   \n[37] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 11106\u201311115, 2021.   \n[38] Tian Zhou, Ziqing Ma, Qingsong Wen, Liang Sun, Tao Yao, Wotao Yin, Rong Jin, et al. FiLM: Frequency improved legendre memory model for long-term time series forecasting. Advances in Neural Information Processing Systems, 35:12677\u201312690, 2022.   \n[39] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. FEDformer: Frequency enhanced decomposed transformer for long-term series forecasting. In Proceedings of the International Conference on Machine Learning, pages 27268\u201327286, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Descriptions of Notations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "To help understand the symbols used throughout the paper, we provide a detailed list of the key notations in Table 7. ", "page_idx": 13}, {"type": "table", "img_path": "RNbrIQ0se8/tmp/eda0a4e6eb8c74fb3727d48f450a160a1a215739fa9e29e7d364c6dfdb50e956.jpg", "table_caption": ["Table 7: Description of the key notations. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "B Descriptions of Datasets ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Datasets. For long-range time series forecasting, we conduct experiments on 7 commonly used benchmarks, including Electricity Transformers Temperature (ETT), Traffic2, Electricity3, and Weather4 datasets following [30, 21, 26]. ETT datasets include data from two counties in the same Chinese province, each data point comprising seven variables: the target variable \"oil temperature\" and six power load features. The datasets vary in granularity, with \"h\" indicating hourly data and \"m\" indicating 15-minute intervals. Weather dataset contains 21 weather indicators collected every 10 minutes from a weather station in Germany. Electricity dataset records hourly electricity consumption of 321 clients. Traffic dataset provides hourly road occupancy rates from 821 freeway sensors. For short-range time series forecasting, we use four benchmarks from PEMS (PEMS03, PEMS04, PEMS07, and PEMS08), as referenced in [27, 19]. These datasets capture 5-minute traffic flow data from freeway sensors. For ultra-long-range time series forecasting, we adopt ETTh1, ETTh2, ETTm1, and ETTm2 following [18]. Table 8 gives the detailed dataset statistics. In addition, the forecastability is derived from one minus the entropy of the Fourier decomposition of a time series[27, 14]. Higher values mean greater forecastability. ", "page_idx": 14}, {"type": "table", "img_path": "RNbrIQ0se8/tmp/8cc901bf9e27c5e92020c62cda22c60f27fe9fd4722fd351975e91e928454947.jpg", "table_caption": ["Table 8: Detailed dataset statistics. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "We adopt the same data processing and train-validation-test split protocol as in existing works [26, 21, 23]. We split each dataset into training, validation, and test sets based on chronological order. For PEMS (PEMS03, PEMS04, PEMS07, and PEMS08) dataset and ETT (ETTh1, ETTh2, ETTm1, and ETTm2) dataset, the train-validation-test split ratio is 6:2:2. For Weather, Traffic, and Electricity dataset, the train-validation-test split ratio is 7:2:1. ", "page_idx": 14}, {"type": "text", "text": "Metric details. Following existing methods [26, 21], we employ Mean Squared Error (MSE) and Mean Absolute Error (MAE) as our evaluation metrics, which can be formulated as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\nL_{m s e}=\\frac{1}{H}\\left\\|\\widehat{\\mathbf{X}}_{T+1:T+H}^{0}-\\mathbf{X}_{T+1:T+H}^{0}\\right\\|_{2}^{2}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\nL_{m a e}=\\frac{1}{H}\\big|\\widehat{\\mathbf{X}}_{T+1:T+H}^{0}-\\mathbf{X}_{T+1:T+H}^{0}\\big|,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $T$ and $H$ are the input and output lengths, $\\widehat{\\mathbf{X}}_{T+1:T+H}^{0}$ and ${\\bf X}_{T+1:T+H}^{0}$ are the predicted results and ground truth. ", "page_idx": 14}, {"type": "text", "text": "C Descriptions of Baselines ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We compare Ada-MSHyper with 15 competitive baselines. Below are brief descriptions of the baselines: (1) iTransformer [21]: Applies the attention and feed-forward network on the inverted dimensions, i.e., the time points of individual series are embedded into variate tokens, and the feedforward network is applied for each variate token to learn nonlinear representations. (2) MSHyper [26]: Utilizes rule-based multi-scale hypergraphs to model high-order pattern interactions in univariate time series. (3) PatchTST [23]: Uses channel-independent techniques and treats subseries-level patches as input tokens to a Transformer, facilitating semantic extraction of multiple time steps in time series. (4) TimesMixer [27]: Employs a fully MLP-based architecture with past-decomposable-mixing and future-multipredictor-mixing blocks to leverage disentangled multiscale series. (5) MSGNet [4]: Leverages frequency domain analysis to extract periodic patterns and combines an attention mechanism with adaptive graph convolution to capture multi-scale pattern interactions. (6) CrossGNN [16]: Uses an adaptive multi-scale identifier to construct multi-scale representations and utilizes a cross-scale GNN to capture multi-scale pattern interactions. (7) TimesNet [29]: Conducts multiperiodicity analysis by extending 1D time series into a set of 2D tensors, modeling complex temporal variations from a 2D perspective. (8) WITRAN [18]: Proposes an RNN-based architecture that handles univariate input sequences from a 2D space perspective, maintaining a fixed scale throughout the processing. (9) SCINet [19]: Uses a recursive downsample-convolve-interact architecture to extract temporal features from downsampled sub-sequences or features. (10) Crossformer [36]: Adopts cross-dimension attention to capture inter-series dependencies for multivariate time series forecasting. (11) FiLM [38]: Applies Legendre polynomial projections to approximate historical information, uses Fourier projections to remove noise, and adds a low-rank approximation to speed up computation. (12) DLinear [35]: Decomposes time series into two different components and uses a single linear layer for each component to model temporal dependencies. (13) FEDformer [39]: Utilizes a seasonal-trend decomposition method to capture the global profile of time series and a frequency-enhanced Transformer to capture more detailed structures. (14) Pyraformer [20]: Utilizes a pyramidal attention module to extract inter-scale features at different resolutions and intra-scale features at different ranges with linear complexity. (15) Autoformer [30]: Uses an auto-correlation mechanism based on series periodicity to capture features at the sub-series level. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "D Experimental Settings ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We repeat all experiments 3 times and use the mean of the metrics as the final results. The training process is early stopped when there is no improvement within 5 epochs. Following existing works [21, 27, 35], we use instance normalization to normalize all datasets. The max number of scale $S$ is set to 3. We use 1D convolution as our aggregation function. For other hyperparameters, we use Neural Network Intelligence $\\left(\\mathrm{NNI}\\right)^{5}$ toolkit to automatically search the best hyperparameters, which can greatly reduce computation cost compared to the grid search approach. The detailed search space of hyperparameters is given in Table 9. The source code of Ada-MSHyper is released on GitHub 6. ", "page_idx": 15}, {"type": "table", "img_path": "RNbrIQ0se8/tmp/ec5ecf1f0f2a5fe568724489bc5fb008f7714cd92d0a01107fbde307bf418b39.jpg", "table_caption": ["Table 9: The search space of hyperparameters. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "E Full Results ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We compare Ada-MSHyper with 13 baselines across four tasks: long-range forecasting for multivariate time series, long-range forecasting for univariate time series, ultra-long-range forecasting for multivariate time series, and short-range forecasting for multivariate time series. For a fair comparison, we evaluate Ada-MSHyper and baselines under unified experimental settings of each task. The average results from all prediction lengths are presented in tables, where the best results are bolded and the second best results are underlined. \\* indicates that some baselines do not meet our settings, thus we rerun these baselines using their official code and fine-tune their key hyperparameters. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "Long-Range Time Series Forecasting Under Multivariate Settings. Table 10 summarizes the results of long-range time series forecasting under multivariate settings, where the results of baselines without \\* are cited from iTransformer [21]. We can see from Table 10 that Ada-MSHyper achieves the SOTA results on all datasets. Specifically, Ada-MSHyper gives an average error reduction of $4.56\\%$ and $3.47\\%$ compared to the best baseline in MSE and MAE, respectively. ", "page_idx": 16}, {"type": "table", "img_path": "RNbrIQ0se8/tmp/d311aeaa671006bda6f57368df302f77a7a08d290bed63e559919713351448f7.jpg", "table_caption": ["Table 10: Full results of long-range time series forecasting under multivariate settings. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Long-Range Time Series Forecasting Under Univariate Settings. Table 11 and Table 12 summarize the average results and full results of long-range time series forecasting under univariate settings, where the results of baselines without \\* are cited from DLinear [35]. Following existing works [39, 26, 23, 30], we set the univariate forecasting on ETT as only predicting a target variate \"oil temperature\" given inputs from all variables. We can see from Table 11 that Ada-MSHyper achieves the SOTA results on all datasets. Specifically, Ada-MSHyper gives an average error reduction of $7.57\\%$ and $4.65\\%$ compared to the best baseline in terms of MSE and MAE, respectively. ", "page_idx": 16}, {"type": "table", "img_path": "RNbrIQ0se8/tmp/d9fb481ab611c6b9a76c07effe1678e3780090be514a93e05f1a3df978911e1d.jpg", "table_caption": ["Table 11: Long-range time series forecasting results under univariate settings. "], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "RNbrIQ0se8/tmp/f982e14e7be8f8a88a0afecb1783390fe9b789abbd3546b544aca7a9846772a7.jpg", "table_caption": ["Table 12: Full results of long-range time series forecasting under univariate settings "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Short-Range Time Series Forecasting Under Multivariate Settings. Table 13 summarizes the results of short-range time series forecasting under multivariate settings, where the results of baselines without \\* are cited from iTransformer [21]. We can see from Table 13 that Ada-MSHyper achieves the SOTA results on all datasets. Specifically, Ada-MSHyper gives an average error reduction of $10.38\\%$ and $3.82\\%$ compared to the best baseline in terms of MSE and MAE, respectively. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "table", "img_path": "RNbrIQ0se8/tmp/847d46082a050484cb9e5f3b0224a057ceb5173498db62a87121b45df95685da.jpg", "table_caption": ["Table 13: Full results of short-range time series forecasting under multivariate settings. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Ultra-Long-Range Time Series Forecasting Under Multivariate Settings. We conduct ultra-longrange time series forecasting by taking fixed input length $\\boldsymbol{T}=96\\$ ) to predict ultra-long lengths $(H\\dot{=}\\{1080,1440,1800,21\\dot{6}0\\}\\$ ). We run all results by ourselves. Table 14 summarizes the results of ultra-long-range time series forecasting under multivariate settings, where - - indicates that the method fails to produce any results on that prediction length due to the out-of-memory problems. We can see from Table 14 that Ada-MSHyper achieves SOTA results on almost all datasets. Specifically, Ada-MSHyper gives an average error reduction of $4.97\\%$ and $2.21\\%$ compared to the best baseline in terms of MSE and MAE, respectively. ", "page_idx": 17}, {"type": "table", "img_path": "RNbrIQ0se8/tmp/a2a489c08b09c6f212874087b157eb09a451857889f13e5a26b512e2b20260e4.jpg", "table_caption": ["Table 14: Full results of ultra-long-range time series forecasting results under multivariate settings. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "F Ablation Studies ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "To investigate the performance of Ada-MSHyper on longer prediction lengths, we compare the forecasting results of Ada-MSHyper with those of six variations (i.e., AGL, one, PH, -w/o NC, -w/o HC, and -w/o NHC) on ETTh1 dataset. The experimental results are shown in Table 15. We can observe that for longer prediction lengths, -w/o NC has smaller performance degradation than other variations. The reason may be that when the prediction length increases, the model tends to focus more on macroscopic variation interactions and diminishes its emphasis on fine-grained node constraint. In addition, Ada-MSHyper performs better than other six variations even with longer prediction length, showing the effectiveness of our AHL module and NHC mechanism. ", "page_idx": 17}, {"type": "table", "img_path": "RNbrIQ0se8/tmp/042b1e3cab9999ff8267bc64b52eb55bb83ec63c99b33a5198d4522ab15a8790.jpg", "table_caption": ["Table 15: Results of different adaptive hypergraph learning methods and constraint mechanisms. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "To investigate the impact of node and hypergraph constraints mechanism on the adaptive hypergraph learning (AHL) module, we design two variants: (1) Removing the NHC mechanism (-w/o NHC). ", "page_idx": 17}, {"type": "text", "text": "(2) Only optimizing the hypergraph learning module (-OH). We illustrate these two variants for better understanding in Figure 5. ", "page_idx": 18}, {"type": "image", "img_path": "RNbrIQ0se8/tmp/19d668c3ab2cbbe52623aeace21fb473f41da634a93d24df9478ddb1072fd3a2.jpg", "img_caption": ["Figure 5: Different optimization strategies for AHL. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "To investigate the impact of the multi-scale feature extraction (MFE) module, we design two variants: (1) Replacing the aggregation function in the MFE module with average pooling (-avg). (2) Replacing the aggregation function in the MFE module with max pooling (-max). ", "page_idx": 18}, {"type": "text", "text": "The results for the four variants are shown in Table 16. We can observe that: (1) -w/o NHC gets the worst results. This may be because lacking the constraints makes the model fail to capture implicit semantic features of the clustered nodes and learned hyperedges. (2) Compared to -OH, Ada-MSHyper yields slightly better results. The reason may be that the NHC mechanism facilitates the MFE model to more effectively aggregate similar features during multi-scale feature extraction and reduce the interference of noise. (3) -avg and -max get relatively worse performance. This may be because the lack of parameters leads to the reduction of the representative ability of Ada-MSHyper. ", "page_idx": 18}, {"type": "table", "img_path": "RNbrIQ0se8/tmp/61bc13922e239369105eca959633261650b2eacbbf2f1bfbc8e7da842977bc67.jpg", "table_caption": ["Table 16: Results of different AHL, MFE and multi-scale interaction methods. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "To investigate the effectiveness of hypergraph convolution attention, we design one variant: Replacing the hypergraph convolution attention with the attention mechanism used in the inter-scale interaction module to update node features $\\mathrm{(-r/att)}$ . The experimental results on ETTh1 dataset are shown in Table 16. We can observe that Ada-MSHyper performs better than $-\\mathrm{{r/}}$ att, which demonstrates the effectiveness of the hyperedge convolution attention used in the intra-scale interaction module. ", "page_idx": 18}, {"type": "text", "text": "G Parameter Studies ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We perform parameter studies to measure the impact of the threshold $\\eta$ , which influences the effectiveness of the sparsity strategy. The experimental results on ETTh1 dataset are shown in Table 17. We can see that the best performance can be obtained when $\\eta$ is 3. The reason is that a small $\\eta$ may filter out useful information and a large $\\eta$ would introduce noise interference. ", "page_idx": 18}, {"type": "table", "img_path": "RNbrIQ0se8/tmp/fc20516e65d1a5ec73ae7c117e5ddf76b06a5a2a8924760cac24b16a87e24f64.jpg", "table_caption": ["Table 17: Results of Ada-MSHyper with different $\\eta$ "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "H Visualization ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Visualization of Node Constraint. As shown in Figure 6a, each time step is denoted a node of the hypergraph at the finest scale. We categorize the nodes into four groups based on the node values of original inputs, and draw them using different colors. For a target node, nodes of the same color may be regarded as those sharing similar semantic information with the target node, while nodes of other colors may be regarded as noise. Then, we draw the nodes related to the target node based on the incidence matrix $\\mathbf{\\breve{H}}^{1}$ of the learned hypergraph in the black color. ", "page_idx": 18}, {"type": "image", "img_path": "RNbrIQ0se8/tmp/68ec2f8aa21aef0b3b2195a79ed4d304e20ee29872f9d2b2a20bd29e836316ff.jpg", "img_caption": ["Figure 6: Visualization the node constrain effect on Electricity dataset. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "We random select samples at the same time step from three variants, i.e., without node constraint (-w/o NC), without hyperedge constraint (-w/o HC), and without node and hyperedges constraints (-w/o NHC), and plot these three samples with samples from original inputs and Ada-MHyper. ", "page_idx": 19}, {"type": "text", "text": "We can observe that: (1) In Figure 6b and Figure 6c, the related nodes of the target node are almost neighboring nodes. However, some noise, plotted as orange, is included as well. The reason may be that -w/o NHC and -w/o NC cannot distinguish noise information without node constraint, i.e., cannot consider the semantic similarity to cluster nodes. (2) In Figure 6d and Figure 6e, since -w/o HC and the proposed Ada-MSHyper have node constraint, both of them can cluster neighboring and distant but strongly correlated nodes, and they can also mitigate the interference of noise, indicating the effectiveness of node constraint. ", "page_idx": 19}, {"type": "text", "text": "Visualization of Hyperedge Constraint. We use the samples which are used in the visualization of node constraint. We visualize the sequentially connecting nodes that belong to the same hyperedges whose indices are $\\{4,8,12\\}$ . Figure 7 shows three types of temporal variations learned by -w/o NC, -w/o HC, -w/o NHC, and Ada-MSHyper, respectively. We can observe that: (1) -w/o HC and -w/o NHC exhibit irregular temporal variations in comparison to -w/o NC and Ada-MSHyper. The reason may be that without the hyperedge constraint, these methods are unable to adequately differentiate temporal variations entangled in temporal patterns. (2) Compared to -w/o NC, Ada-MSHyper exhibits relatively simple temporal variations. The reason may be that influenced by node constraint, the temporal variations extracted by Ada-MSHyper contain less noise. ", "page_idx": 19}, {"type": "image", "img_path": "RNbrIQ0se8/tmp/510f6d181554a6a2af88e66ee006b5944fcb51be8f37ea61ef99865739f6e6c7.jpg", "img_caption": ["Figure 7: Different temporal variations learned by different methods. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "We also matched the temporal variations extracted by Ada-MSHyper to the sample sequences. As shown in Figure 8b, we can observe that these variations can represent inherent changes. We speculate that by introducing hyperedge constraint, the model will treat temporal variations with different shapes as distinct positive and negative examples. In addition, the differentiated temporal variations are like a kind of Shapelet, akin to those used in NLP and CV, enabling a better representation of temporal patterns within time series. ", "page_idx": 20}, {"type": "image", "img_path": "RNbrIQ0se8/tmp/0514056f7c3aafa0fa84def932fb665327ff4f5c2233c409405f1b2d6af85465.jpg", "img_caption": ["Figure 8: Visualization the hyperedge constraint effect on Electricity dataset. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "I Limitations and Future Works ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In the future, we will extend our work in the following directions. Firstly, due to our NHC mechanism can cluster nodes with similar semantic information and differentiate temporal variations within each scales, It is interesting to correlate the inherent temporal variations with corpora used in natural language processing, and leverage large language models to investigate deeper correlations between corpora and TS data. Secondly, compared to natural language processing and computer vision, time series analysis has access to fewer datasets, which may limit the expressive power of the models. Therefore, in the future, we plan to compile larger datasets to validate the generalization capabilities of our models on more extensive data. ", "page_idx": 20}, {"type": "text", "text": "J Broader Impacts ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this paper, we propose Ada-MSHyper for time series forecasting. Extensive experimental results demonstrate the effectiveness of Ada-MSHyper. Our paper mainly focuses on scientific research and has no obvious negative social impact. ", "page_idx": 20}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The main claims made in the abstract and introduction accurately reflect the contributions and scope of the paper (see Abstract and Introduction) ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: We discuss the limitations of our work in AppendixI. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper provides corresponding experimental validation in Section5.2 and AppendixE, provides ablation studies in Section 5.3 and Appendix F, and provides visualization analysis in Appendix H to support the claimed capabilities of the model. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 22}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We have provided the details regarding computational platforms, dataset descriptions, network architectures, hyper-parameter settings, and the training process of our method in Section B in the main paper and Appendix B, C, and D. In addition, we provide source codes on anonymous Github as stated in the abstract. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 22}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Our codes are released at anonymous Github as stated in the abstract. The download links of the public datasets are provided in the project homepage and pre-processing functions are included in the codes. The hyper-parameter settings are given in Appendix D. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 23}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We have provided the details regarding computational platforms, dataset descriptions, network architectures, hyper-parameter settings, and the training process of our method in Section B in the main paper and Appendix B, C, and D. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 23}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We repeat all experiments 3 times and use the mean of the metrics as the final results as illustrated in Appedix D. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 23}, {"type": "text", "text": "\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 24}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We have stated the experimental platforms (i.e., GPUs and CPUs) and software (the version of Pytorch) used in our paper in Section 5.1 and Appendix D, and we have analyzed the computational efficiency in Appendix ??. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: We have reviewed the NeurIPS Code of Ethics and we have ensured to preserve anonymity. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 24}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We discuss both potential positive societal impacts and negative societal impacts of our work in Appendix J. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 25}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: Our paper poses no such risks, as we focus on theoretical analysis and general research areas, and conduct our experiments on public datasets. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 25}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We make sure to cite the original papers (or URLs) of the code packages or datasets that are used in our paper. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Our codes are provided on the project homepage at anonymous Github. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 26}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: No crowdsourcing nor research with human subjects is involved. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 26}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: No crowdsourcing nor research with human subjects is involved. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}]