[{"figure_path": "bkLetzd97M/figures/figures_3_1.jpg", "caption": "Figure 1: Context-aware Feature Augmentation (CFA) module. CFA takes as input a video clip Ck of length w, augments it with temporal information captured in an adaptive memory bank Mk, and outputs an enhanced clip feature \u010dk. I is the number of iterations of SA, TransDecoder, and CA.", "description": "This figure illustrates the Context-aware Feature Augmentation (CFA) module, a key component of the proposed online temporal action segmentation framework.  The CFA module takes a video clip (Ck) as input and enhances its features by incorporating contextual information from an adaptive memory bank (Mk). This is achieved through an iterative process involving self-attention (SA), a transformer decoder, and cross-attention (CA), repeated I times. The output is a context-enhanced clip representation (\u010dk). The adaptive memory bank dynamically updates with short-term and long-term context information, enabling the CFA module to capture temporal dependencies effectively.", "section": "3.2 Context-aware Feature Augmentation"}, {"figure_path": "bkLetzd97M/figures/figures_4_1.jpg", "caption": "Figure 2: Two inference types. a) Online inference samples clips with stride \u03b4 = 1 and only preserves the last frame prediction, while b) Semi-online inference samples non-overlapping clips with stride \u03b4 = w and all predictions are preserved.", "description": "This figure illustrates two different inference approaches used in the OnlineTAS model.  The left panel (a) shows online inference where clips are processed one frame at a time (stride \u03b4=1), with only the last frame prediction retained.  This approach prioritizes responsiveness. The right panel (b) depicts semi-online inference, utilizing non-overlapping clips (stride \u03b4=w) and retaining all predictions within each clip. This method balances responsiveness and accuracy by considering a wider temporal context.  The visual representation uses circles to signify predictions and boxes for input clips.", "section": "3.4 Training and Inference"}, {"figure_path": "bkLetzd97M/figures/figures_8_1.jpg", "caption": "Figure 3: Visualization of segmentation outputs for sequence \"rgb-01-1\" from 50Salads [38].", "description": "This figure visualizes the segmentation results of different methods on a sample sequence from the 50Salads dataset. It compares the ground truth (GT) segmentation with the results obtained using LSTR [44], the proposed online and semi-online methods with and without post-processing, and the offline MS-TCN [11] method.  The visualization clearly highlights the over-segmentation problem frequently observed in online methods and how the post-processing step in the proposed method helps to mitigate this issue.  Different colors represent different action classes.", "section": "4.2 Comparison with State-of-the-Art Methods"}, {"figure_path": "bkLetzd97M/figures/figures_14_1.jpg", "caption": "Figure 4: Standard vs. Causal Convolution", "description": "This figure illustrates the difference between standard and causal convolutions.  Standard convolutions consider both past and future inputs within their kernel (receptive field), while causal convolutions only use past and present inputs. This difference is crucial for online tasks, where future information is unavailable. The figure visually represents this by showing how the output node (dark teal) connects to different input nodes (light teal) for each convolution type.", "section": "A.1 Standard vs. Causal Convolution"}]