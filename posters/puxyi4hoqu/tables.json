[{"figure_path": "PuXYI4HOQU/tables/tables_1_1.jpg", "caption": "Table 1: Fundamental convergence properties of smooth optimization methods", "description": "This table lists five fundamental convergence properties for smooth optimization methods.  The properties describe the convergence behavior of iterates (x<sup>k</sup>), gradients (\u2207f(x<sup>k</sup>)), and function values (f(x<sup>k</sup>)) to a stationary point.  They range from the weakest condition (lim inf ||\u2207f(x<sup>k</sup>)||=0) to the strongest (convergence of the sequence of iterates to a stationary point).  Understanding these properties helps in comparing and analyzing the performance of different optimization algorithms.", "section": "1.1 Lack of convergence properties for SAM due to constant stepsize"}, {"figure_path": "PuXYI4HOQU/tables/tables_2_1.jpg", "caption": "Table 2: Convergence properties of SAM for convex functions in Theorem 3.2", "description": "This table summarizes the convergence properties of the Sharpness-Aware Minimization (SAM) algorithm for convex functions, as proven in Theorem 3.2 of the paper.  It shows the relationship between different classes of convex functions (general setting, bounded minimizer set, unique minimizer) and the resulting convergence properties of the SAM algorithm.  The convergence properties include the limit inferior of the gradient norm, the existence of a stationary accumulation point, and the convergence of the iterates themselves.", "section": "Convergence of SAM and normalized variants"}, {"figure_path": "PuXYI4HOQU/tables/tables_2_2.jpg", "caption": "Table 3: Convergence properties of SAM together with normalized variants (Corollary 3.5, Appendix D), and USAM together with unnormalized variants (Theorem 4.2)", "description": "This table summarizes the convergence properties of SAM and its normalized variants, as well as USAM and its unnormalized variants.  It shows the convergence results (e.g., limit of the gradient norm, convergence of function values, convergence of iterates) under different assumptions on the function (general setting, KL property, Lipschitz gradient).  The table highlights the different convergence results achieved for convex vs nonconvex functions and also highlights the specific theorems/corollaries which provide the proofs of each result.", "section": "Convergence of SAM and normalized variants"}, {"figure_path": "PuXYI4HOQU/tables/tables_5_1.jpg", "caption": "Table 1: Fundamental convergence properties of smooth optimization methods", "description": "This table presents five fundamental convergence properties frequently analyzed in smooth optimization methods. These properties describe the convergence behavior of the iterates (x<sup>k</sup>), gradients (\u2207f(x<sup>k</sup>)), and function values (f(x<sup>k</sup>)) as the algorithm progresses.  Property (1) states that the gradient norms approach zero in the limit inferior sense. Property (2) indicates that every accumulation point is a stationary point. Property (3) signifies that the gradient norms converge to zero. Property (4) states that the function values converge to the optimal value at some stationary point.  Property (5) shows that the iterates converge to a stationary point. The relationships between these properties are also illustrated.", "section": "Convergence of SAM and normalized variants"}, {"figure_path": "PuXYI4HOQU/tables/tables_6_1.jpg", "caption": "Table 2: Convergence properties of SAM for convex functions in Theorem 3.2", "description": "This table presents the convergence properties of the Sharpness-Aware Minimization (SAM) algorithm for convex functions, as proven in Theorem 3.2 of the paper.  It shows the convergence results under different assumptions about the function's minimizer set. Specifically, it shows what can be concluded about the limit inferior of the gradient norm, whether a stationary accumulation point exists, and whether the sequence of iterates converges to a solution.  The conditions under which these conclusions are valid are that the gradient is Lipschitz continuous and that either there is a bounded minimizer set or a unique minimizer.", "section": "Convergence of SAM and normalized variants"}, {"figure_path": "PuXYI4HOQU/tables/tables_7_1.jpg", "caption": "Table 2: Convergence properties of SAM for convex functions in Theorem 3.2", "description": "This table summarizes the convergence properties of the Sharpness-Aware Minimization (SAM) algorithm for convex functions, as proven in Theorem 3.2 of the paper.  It shows the relationship between different convergence properties under different assumptions about the function's minimizer set (general setting, bounded minimizer set, unique minimizer). The table shows that under increasingly restrictive assumptions on the minimizer set, increasingly stronger convergence properties can be guaranteed for SAM.", "section": "Convergence of SAM and normalized variants"}, {"figure_path": "PuXYI4HOQU/tables/tables_7_2.jpg", "caption": "Table 2: Convergence properties of SAM for convex functions in Theorem 3.2", "description": "This table summarizes the convergence properties of the Sharpness-Aware Minimization (SAM) algorithm for convex functions, as proven in Theorem 3.2 of the paper.  It shows the relationship between different convergence properties under varying conditions. For instance, it shows that if the function has a bounded minimizer set, SAM achieves a stationary accumulation point. If there is a unique minimizer, the sequence of iterates converges to that solution.", "section": "Convergence of SAM and normalized variants"}, {"figure_path": "PuXYI4HOQU/tables/tables_7_3.jpg", "caption": "Table 1: Fundamental convergence properties of smooth optimization methods", "description": "This table summarizes the fundamental convergence properties that are typically studied for smooth optimization methods.  These properties include the convergence of the gradient norms to zero, the existence of stationary accumulation points, the convergence of function values to the optimal value, and the convergence of iterates to an optimal solution. The table provides a concise overview of these properties and their relationships, which serves as a benchmark for comparing the convergence behavior of the Sharpness-Aware Minimization (SAM) method against other optimization methods.", "section": "1 Introduction"}, {"figure_path": "PuXYI4HOQU/tables/tables_8_1.jpg", "caption": "Table 4: Test accuracy on CIFAR-10 and CIFAR-100", "description": "This table presents the test accuracy results on CIFAR-10 and CIFAR-100 datasets for different deep learning models (ResNet18, ResNet34, and WideResNet28-10) and different stepsize strategies (constant and diminishing).  The results show the mean accuracy and the 95% confidence interval across three independent runs for each model and stepsize combination.  It allows for a comparison of the performance of SAM with different stepsize schemes on popular image classification tasks.", "section": "Numerical Experiments"}, {"figure_path": "PuXYI4HOQU/tables/tables_23_1.jpg", "caption": "Table 2: Convergence properties of SAM for convex functions in Theorem 3.2", "description": "This table summarizes the convergence properties of the Sharpness-Aware Minimization (SAM) algorithm for convex functions, as established in Theorem 3.2 of the paper.  It shows the relationship between different convergence properties, such as the limit inferior of the gradient norm going to zero, the existence of a stationary accumulation point, and the convergence of the iterates to a minimizer under different assumptions on the minimizer set (general, bounded, or unique).", "section": "Convergence of SAM and normalized variants"}, {"figure_path": "PuXYI4HOQU/tables/tables_26_1.jpg", "caption": "Table 4: Test accuracy on CIFAR-10 and CIFAR-100", "description": "This table presents the test accuracy results for different deep neural network models trained on CIFAR-10 and CIFAR-100 datasets using SAM with various stepsize strategies.  The models include ResNet18, ResNet34, and WideResNet28-10.  The stepsize strategies are 'Constant', 'Diminish 1', 'Diminish 2', and 'Diminish 3', representing different diminishing stepsize schemes.  The table shows the mean test accuracy and its 95% confidence interval across three independent runs for each model and stepsize strategy on each dataset.  The highest accuracy for each model and dataset is highlighted in bold, allowing comparison of the performance across the different stepsize approaches.", "section": "Numerical Experiments"}, {"figure_path": "PuXYI4HOQU/tables/tables_26_2.jpg", "caption": "Table 6: Additional Numerical Results on Tiny ImageNet [Le and Yang, 2015] for SAM with and without momentum", "description": "This table presents the test accuracy results on the Tiny ImageNet dataset using different stepsize strategies for the SAM optimization algorithm.  It compares the performance of SAM with and without momentum.  The results show the test accuracy for four different deep neural network models (ResNet18, ResNet34, WideResNet28-10) under four different stepsize schemes (Constant, Diminish 1, Diminish 2, Diminish 3).  The highest accuracy for each model and stepsize scheme is bolded.", "section": "Numerical Experiments"}]