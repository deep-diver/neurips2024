[{"type": "text", "text": "Efficient Multi-Task Reinforcement Learning with Cross-Task Policy Guidance ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jinmin $\\mathbf{H}\\mathbf{e}^{1,2}$ , Kai $\\mathbf{Li}^{1,2}$ ,\u2217 Yifan Zang1,2, Haobo $\\mathbf{Fu}^{5}$ , Qiang $\\mathbf{Fu}^{5}$ , Junliang $\\mathbf{Xing^{4}}$ ,\u2217 Jian Cheng1,3 ", "page_idx": 0}, {"type": "text", "text": "1Institute of Automation, Chinese Academy of Sciences 2School of Artificial Intelligence, University of Chinese Academy of Sciences 3AiRiA 4Tsinghua University 5Tencent AI Lab {hejinmin2021,kai.li,zangyifan2019,jian.cheng}@ia.ac.cn, {haobofu,leonfu}@tencent.com, jlxing@tsinghua.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Multi-task reinforcement learning endeavors to efficiently leverage shared information across various tasks, facilitating the simultaneous learning of multiple tasks. Existing approaches primarily focus on parameter sharing with carefully designed network structures or tailored optimization procedures. However, they overlook a direct and complementary way to exploit cross-task similarities: the control policies of tasks already proficient in some skills can provide explicit guidance for unmastered tasks to accelerate skills acquisition. To this end, we present a novel framework called Cross-Task Policy Guidance (CTPG), which trains a guide policy for each task to select the behavior policy interacting with the environment from all tasks\u2019 control policies, generating better training trajectories. In addition, we propose two gating mechanisms to improve the learning efficiency of CTPG: one gate filters out control policies that are not beneficial for guidance, while the other gate blocks tasks that do not necessitate guidance. CTPG is a general framework adaptable to existing parameter sharing approaches. Empirical evaluations demonstrate that incorporating CTPG with these approaches significantly enhances performance in manipulation and locomotion benchmarks. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Deep reinforcement learning (RL) has undergone remarkable progress over the past decades, showcasing its efficacy across various domains, such as game playing [16, 28] and robotic control [12, 13]. However, most of these deep RL methods primarily focus on learning different tasks in isolation, making it challenging to utilize shared information between tasks to develop a generalized policy. Multi-task reinforcement learning (MTRL) aims to master a set of RL tasks effectively. By leveraging the potential information sharing among different tasks, joint multi-task learning typically exhibits higher sample efficiency than training each task individually [27]. ", "page_idx": 0}, {"type": "text", "text": "A significant challenge in MTRL lies in determining what information should be shared and how to share it effectively. Recent studies have proposed various approaches to tackle this challenge via network parameter sharing with carefully designed network structures [10, 22, 27] or tailored optimization procedures [4, 14, 30]. We summarize such methods as implicit knowledge sharing in Section 2. Despite these unremitting efforts, another largely overlooked way exists to exploit cross-task similarities to improve the learning efficiency of multiple tasks. Intuitively, humans can effortlessly discern which skills can be shared from other tasks while learning a specific task. For instance, someone who can ride a bicycle can quickly learn to ride a motorcycle by referring to ", "page_idx": 0}, {"type": "image", "img_path": "3qUks3wrnH/tmp/1403bfff4269464609d2942b2a42a165aff30af99735ab7609e2e52a49ca39aa.jpg", "img_caption": ["(a) Button-Press v.s. Drawer-Close ", "(b) Door-Open v.s. Drawer-Open "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Full or partial policy sharing in the manipulation environment. (a): Task Button-Press and Drawer-Open share almost the same policy, where the robotic arm needs to reach a specified position (button or handle) and then push the target object. (b): Task Door-Open and Drawer-Open share the policy of grabbing the handle in the first phase, but they are required to open the target object by different movements (rotation or translation). ", "page_idx": 1}, {"type": "text", "text": "related skills, such as operating controls, maintaining balance, and executing turns. Likewise, a motorcyclist adept in these skills can also quickly learn to ride a bicycle. This ability allows humans to efficiently master multiple tasks by selectively referring to skills previously learned. As shown in Figure 1, similar full or partial policy sharing is also evident in robotic arm manipulation tasks. These cross-task similarities enable policy guidance, i.e., control policies of tasks already proficient in specific skills can generate valuable training data for unmastered tasks. Compared to the common practice, which blindly generates training trajectories for each task solely with its own control policy, generating training trajectories using a control policy from other tasks that perform better in the current situation can better facilitate the learning procedure. Moreover, this explicit policy sharing approach significantly reduces unnecessary exploration of similar contexts in different tasks. ", "page_idx": 1}, {"type": "text", "text": "The key challenge encountered in this approach to MTRL is discerning beneficial sharing control policies for each task adaptively. To address this challenge, [32] uses a Q-filter to identify singlestep shareable behaviors without ensuring optimality for long-term policy sharing. In contrast, we propose a simple yet effective framework called Cross-Task Policy Guidance (CTPG) for more robust long-term policy guidance. Initially, we group the control policies of all tasks into a candidate set. Subsequently, for each task, we train a guide policy to identify useful sharing control policies, and then the chosen control policy generates better training trajectories to achieve policy guidance. Furthermore, we design two gating mechanisms to avoid unfavorable policy guidance interfering with learning. The first, policy-filter gate, leverages the value function to refine the candidate set by masking out control policies that are not beneficial for guidance. The second, guide-block gate, withholds extra guidance for the mastered easy tasks, allowing the focus to be on further solidifying the skills already acquired. With the incorporation of the above two gates, CTPG greatly improves the quality of the policy guidance, thereby fostering enhanced exploration and learning efficiency. ", "page_idx": 1}, {"type": "text", "text": "CTPG is a generalized MTRL framework that can be combined with various existing parameter sharing methods. Among these, we choose several classical approaches and integrate them with CTPG, achieving significant improvement in sample efficiency and final performance on both manipulation [31] and locomotion [11] MTRL benchmarks. Furthermore, we conduct detailed ablation studies to gain insights into how each component of CTPG contributes to its final performance. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Multi-task learning is a training paradigm that enhances generalization by leveraging the information inherent in potentially related tasks [3, 19, 34]. Multi-task reinforcement learning extends this concept to reinforcement learning, expecting that information shared across tasks will be uncovered by simultaneously learning multiple RL tasks [25]. In this study, we distinguish between information sharing as implicit knowledge sharing and explicit policy sharing. ", "page_idx": 1}, {"type": "text", "text": "Implicit Knowledge Sharing. Implicit knowledge sharing primarily focuses on sharing parameters or representations, but it encounters the challenge of negative knowledge transfer due to simultaneous updates within the same network. [14, 30] regard MTRL as a multi-objective optimization problem aimed at managing confilcting gradients resulting from different task losses during training. [22, 27] partition the network into distinct modules and combine these modules to form different sub-policies for different tasks. [5, 21] endeavor to choose or learn better representations as more effective taskconditioned information for policy training. [7, 23] employ distillation and regularization to fuse separate task-specific policies into a unified policy for diverse tasks. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Explicit Policy Sharing. Explicit policy sharing is expressed as the direct sharing of behaviors or policies between different tasks. [20] employs a hierarchical policy that decides when to directly use a previously learned policy and when to acquire a new one. Nonetheless, instead of learning multiple tasks simultaneously, it adopts a sequential task-learning approach, necessitating a manually welldefined curriculum of tasks. [32] uses a Q-filter to identify shareable behaviors. During exploration, each control policy proposes a candidate action, and the policy that suggests the maximum Q-value action on the source task is executed for the following timesteps. However, maximizing Q-value in a single timestep does not guarantee the optimality of this policy across continuous timesteps. CTPG is a new explicit policy sharing method that learns a guide policy for long-term policy guidance. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Multi-Task Reinforcement Learning. We aim to simultaneously learn $N$ tasks, where each task $i\\in\\mathbb{T}$ is represented as a Markov decision process (MDP) [1, 18]. Each MDP is defined by the tuple $\\langle S,A,P_{i},R_{i},\\gamma\\rangle$ , where $S$ denotes the state space, $A$ the action space, $P_{i}:S\\times A\\to S$ the environment transition function, $R_{i}:S\\times A\\to\\mathbb{R}$ the reward function, and $\\gamma\\in[0,1)$ the discount factor. In the scope of this work, different tasks share the same state and action spaces, distinguished by different transition and reward functions. The goal of the MTRL agent is to maximize the average expected return across all tasks, which are uniformly sampled during training. ", "page_idx": 2}, {"type": "text", "text": "Soft Actor-Critic. In this work, we use the Soft Actor-Critic (SAC) [9] algorithm, an off-policy actor-critic method under the maximum entropy framework. The critic network $Q_{\\theta}{\\big(}s_{t},a_{t}{\\big)}$ parameterized by $\\theta$ , representing a soft Q-function [8], aims to minimize the soft Bellman residual: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{J_{Q}(\\theta)=\\mathbb{E}_{(s_{t},a_{t},r_{t})\\sim\\mathcal{D}}\\left[\\frac{1}{2}\\left(Q_{\\theta}(s_{t},a_{t})-\\left(r_{t}+\\gamma\\mathbb{E}_{s_{t+1}\\sim P}\\left[V_{\\bar{\\theta}}(s_{t+1})\\right]\\right)\\right)^{2}\\right],}\\\\ &{\\quad\\quad\\quad\\quad V_{\\bar{\\theta}}(s_{t})=\\mathbb{E}_{a_{t}\\sim\\pi_{\\hat{\\theta}}}\\left[Q_{\\bar{\\theta}}(s_{t},a_{t})-\\alpha\\log\\pi_{\\phi}(a_{t}|s_{t})\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathcal{D}$ represents the data in the replay buffer, and $\\bar{\\theta}$ is the target critic network parameter. The actor network $\\pi_{\\phi}\\big(a_{t}|s_{t}\\big)$ is parameterized by $\\phi$ , and the objective of policy optimization is: ", "page_idx": 2}, {"type": "equation", "text": "$$\nJ_{\\pi}(\\phi)=\\mathbb{E}_{s_{t}\\sim\\mathcal{D}}\\left[\\mathbb{E}_{a_{t}\\sim\\pi_{\\phi}}\\left[\\alpha\\log\\pi_{\\phi}(a_{t}|s_{t})-Q_{\\theta}(s_{t},a_{t})\\right]\\right],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\alpha$ is a learnable temperature parameter to penalize entropy as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\nJ(\\alpha)=\\mathbb{E}_{a_{t}\\sim\\pi_{\\phi}}\\left[-\\alpha\\log\\pi_{\\phi}(a_{t}|s_{t})-\\alpha\\bar{\\mathcal{H}}\\right],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\bar{\\mathcal{H}}$ is a desired minimum expected entropy. If the optimization leads to an increase in $\\pi_{\\phi}\\big(a_{t}|s_{t}\\big)$ with a decrease in the entropy, the temperature $\\alpha$ will accordingly increase. In the following sections, we use subscripts to signify the networks specific to each task. Specifically, the control policy of task $i$ is represented as $\\pi_{i}$ , and the corresponding Q-value function is denoted as $Q_{i}$ . ", "page_idx": 2}, {"type": "text", "text": "4 Cross-Task Policy Guidance ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Explicit policy sharing offers a direct and efficient way to master multiple tasks. If a task is already mastered, its control policy can be fully or partially shared with other tasks to guide tasks requiring similar skills to be quickly learned. Instead of each task generating trajectories constantly by its corresponding control policy, as in most existing MTRL algorithms, we consider using control policies of other tasks to generate training data for the current task when appropriate. To achieve this goal, we propose a novel framework called Cross-Task Policy Guidance (CTPG), which extra learns a guide policy for each task to identify beneficial policies for guidance. We illustrate the trajectory generation process of Task 1 in Figure 2. For this task, its guide policy $\\Pi_{1}^{g}$ selects a policy $\\pi^{\\prime}$ from the candidate set of all control policies $\\{\\pi_{i}\\}_{i=1}^{N}$ every fixed $K$ timesteps. It then uses $\\pi^{\\prime}$ as the behavior policy to interact with the environment and collect data for the next $K$ timesteps. ", "page_idx": 2}, {"type": "text", "text": "The CTPG framework alters only the data collection process, guiding the control policy training through better exploration trajectories. In Section 4.1, we introduce the guide policy in detail and propose a hindsight off-policy correction mechanism for its training. In addition, we propose two gating mechanisms to enhance the efficiency of CTPG: the policy-filter gate discussed in Section 4.2 and the guide-block gate detailed in Section 4.3. ", "page_idx": 3}, {"type": "image", "img_path": "3qUks3wrnH/tmp/0729288910028fe619ddfcb9fa05113f73f858002dfe911edd91ecef6f474244.jpg", "img_caption": ["Figure 2: Overview of the CTPG framework. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "4.1 Guide Policy ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The control policy $\\pi_{i}\\!\\left(a_{t}|s_{t}\\right)$ of task $i$ maps the state $s_{t}$ to the environment action $a_{t}$ , and its corresponding Q-value function $Q_{i}(s_{t},a_{t})$ estimates the expected return. The guide policy $\\Pi_{i}^{g}(j_{t}|s_{t})$ of task $i$ outputs a task index $j_{t}\\in\\mathbb{T}$ , and the corresponding control policy $\\pi_{j_{t}}$ of task $j_{t}$ serves as the behavior policy. The guide Q-value function $Q_{i}^{g}(\\bar{s}_{t},j_{t})$ estimates the expected return of using $\\Pi_{i}^{g}$ to select different control policies for every $K$ timesteps, with its Bellman equation defined as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{B}^{\\Pi_{i}^{g}}Q_{i}^{g}(s_{t},j_{t})\\triangleq R_{i}^{g}(s_{t},j_{t})+\\gamma^{K}\\mathbb{E}_{j_{t+K}\\sim\\Pi_{i}^{g},s_{t+K}\\sim P_{i}}\\left[Q_{i}^{g}(s_{t+K},j_{t+K})\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $B^{\\Pi_{i}^{g}}$ is the Bellman operator of the guide policy $\\Pi_{i}^{g}$ , and the corresponding reward function $R_{i}^{g}$ is defined as the expected cumulative discount rewards for behavior policy $\\pi_{j_{t}}$ over $K$ timesteps: ", "page_idx": 3}, {"type": "equation", "text": "$$\nR_{i}^{g}(s_{t},j_{t})=\\mathbb{E}_{a_{t^{\\prime}}\\sim\\pi_{j_{t}},s_{t^{\\prime}+1}\\sim P_{i}}\\left[\\sum_{t^{\\prime}=t}^{t+K-1}\\gamma^{t^{\\prime}-t}R_{i}(s_{t^{\\prime}},a_{t^{\\prime}})\\right].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "For each task $i$ , during trajectory generation, CTPG first utilizes its guide policy $\\Pi_{i}^{g}$ to sample a behavior policy $\\pi_{j_{t}}$ from the candidate set of all tasks\u2019 control policies: ", "page_idx": 3}, {"type": "equation", "text": "$$\nj_{t}\\sim\\Pi_{i}^{g}(\\cdot|s_{t}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "and then samples actions using the behavior policy $\\pi_{j_{t}}$ for the next $K$ timesteps: ", "page_idx": 3}, {"type": "equation", "text": "$$\na_{t^{\\prime}}\\sim\\pi_{j_{t}}(\\cdot|s_{t^{\\prime}}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $t^{\\prime}\\in\\{t,t+1,\\ldots,t+K-1\\}$ . After each timestep, we obtain the reward $r_{t^{\\prime}}=R_{i}(s_{t^{\\prime}},a_{t^{\\prime}})$ and the next state $s_{t^{\\prime}+1}$ . The transition $\\langle i,s_{t^{\\prime}},a_{t^{\\prime}},j_{t},r_{t^{\\prime}},s_{t^{\\prime}+1}\\rangle$ is stored in the replay buffer. ", "page_idx": 3}, {"type": "text", "text": "The guide policy is trained to maximize the expected return of the current task by choosing appropriate control policies in certain states. If the control policies of some tasks already proficient in specific skills can be shared with the current task in similar states, the guide policy can quickly learn to use these control policies in those states to generate better training data for the current task. The guide policy and the control policy are trained simultaneously. We can use any off-policy RL algorithms for control policy training and any on/off-policy RL algorithms for guide policy training. In this work, we use SAC [9] for the control policy training and the discrete action space variant of SAC [6] for the guide policy training. Given that the guide policy acts every $K$ timesteps, its training frequency is $1/K$ that of the control policy. The detailed pseudo-codes for the control policy and guide policy training are provided in Appendix A.1 (Algorithms 1 and 2). ", "page_idx": 3}, {"type": "text", "text": "Hindsight Off-Policy Correction. During off-policy training, the guide policy faces a nonstationary challenge. Since the control policies are continually updated during the training of the guide policies, the actions chosen by the behavior policies during data collection may no longer align with the improved corresponding control policies, thereby compromising the validity of the training experience. We address this concern by implementing a hindsight off-policy correction mechanism that reassigns the action $j_{t}$ sampled by the past guide policy to a new one $j_{t}^{\\prime}$ , whose control policy $\\pi_{j_{t}^{\\prime}}$ is more likely to output the historical action sequence $\\{a_{t^{\\prime}}\\}_{t^{\\prime}=t}^{t+K-1}$ . Specifically, we utilize maximum likelihood estimation following: ", "page_idx": 3}, {"type": "equation", "text": "$$\nj_{t}^{\\prime}=\\arg\\operatorname*{max}_{j}\\prod_{t^{\\prime}=t}^{t+K-1}\\pi_{j}(a_{t^{\\prime}}|s_{t^{\\prime}})=\\arg\\operatorname*{max}_{j}\\sum_{t^{\\prime}=t}^{t+K-1}\\log\\pi_{j}(a_{t^{\\prime}}|s_{t^{\\prime}}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In this way, we can leverage past experiences effectively to train the guide policy. The workings of the hindsight off-policy correction mechanism in SAC are detailed in Appendix $\\mathbf{C}$ . ", "page_idx": 3}, {"type": "text", "text": "4.2 Not All Policies Are Beneficial for Guidance ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In Section 4.1, we set the guide policy\u2019s action space as the set of all control policies $\\{\\pi_{i}\\}_{i=1}^{N}$ . However, not all control policies within the action space of $\\Pi_{i}^{g}$ are beneficial for task $i$ in state $s_{t}$ . Some control policies perform even worse than the current task\u2019s own control policy $\\pi_{i}$ , rendering them ineffective for guidance. To address this issue, we design a policy-filter gate to refine the action space of the guide policy by adaptively filtering out unfavorable control policies in state $s_{t}$ . The trajectory generation process solely using the current task\u2019s control policy $\\pi_{i}$ can be regarded as equipped with a special guide policy $\\Pi_{i}^{\\tilde{g}}$ that exclusively selects $\\pi_{i}$ as the behavior policy, i.e., $\\Pi_{i}^{\\tilde{g}}(i|s_{t})=1$ for any $s_{t}$ . The guide Q-value $Q_{i}^{\\tilde{g}}$ of $\\Pi_{i}^{\\tilde{g}}$ , defined by Equation 5, is: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{Q_{i}^{\\tilde{g}}\\bigl(s_{t},i\\bigr)=R_{i}^{g}\\bigl(s_{t},i\\bigr)+\\gamma^{K}\\mathbb{E}_{s_{t+K}\\sim P_{i}}\\left[Q_{i}^{\\tilde{g}}\\bigl(s_{t+K},i\\bigr)\\right]}}\\\\ &{=\\mathbb{E}_{a_{t^{\\prime}}\\sim\\pi_{i},s_{t^{\\prime}+N}P_{i}}\\left[\\displaystyle\\sum_{t^{\\prime}=t}^{t+K-1}\\gamma^{t^{\\prime}-t}R_{i}\\bigl(s_{t^{\\prime}},a_{t^{\\prime}}\\bigr)+\\gamma^{K}Q_{i}^{\\tilde{g}}\\bigl(s_{t+K},i\\bigr)\\right]}\\\\ &{=\\cdots}\\\\ &{=\\mathbb{E}_{a_{t^{\\prime}}\\sim\\pi_{i},s_{t^{\\prime}+1}\\sim P_{i}}\\left[\\displaystyle\\sum_{t^{\\prime}=t}^{\\infty}\\gamma^{t^{\\prime}-t}R_{i}\\bigl(s_{t^{\\prime}},a_{t^{\\prime}}\\bigr)\\right]}\\\\ &{=V_{i}(s_{t}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where we repeatedly expand $Q_{i}^{\\tilde{g}}$ to find that $Q_{i}^{\\tilde{g}}(s_{t},i)$ is equal to $V_{i}(s_{t})$ , the state value function of task $i$ \u2019s control policy. Because the value function can serve as a filter for high-quality training data [17, 29, 33], it becomes intuitive to judge the quality of the behavior policy $\\pi_{j_{t}}$ following guide policy and the current task\u2019s control policy $\\pi_{i}$ by directly comparing $Q_{i}^{g}(s_{t},j_{t})$ and $V_{i}(s_{t})$ . In our implementation, we estimate $V_{i}(s_{t})$ via Monte Carlo sampling of $Q_{i}(s_{t},a_{t}^{\\bigstar})$ with $a_{t}\\sim\\pi_{i}(a_{t}|s_{t})$ [15]. Relying on this mechanism, the policy-filter gate serves as a mask vector $m(s_{t})$ to indicate whether each control policy is beneficial for guidance in state $s_{t}$ . Specifically, each element of $m{\\left(s_{t}\\right)}$ is: ", "page_idx": 4}, {"type": "equation", "text": "$$\nm_{j}(s_{t})=\\left\\{\\!\\!\\begin{array}{l l}{1,}&{Q_{i}^{g}(s_{t},j)\\geq V_{i}(s_{t}),}\\\\ {0,}&{Q_{i}^{g}(s_{t},j)<V_{i}(s_{t}),}\\end{array}\\right.\\;\\mathrm{for}\\;j\\in\\{1,2,\\ldots,N\\},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $j$ indicates the element index and task index. Then, the behavior policy $\\pi_{j_{t}}$ is sampled by: ", "page_idx": 4}, {"type": "equation", "text": "$$\nj_{t}\\sim\\mathrm{Normalize}\\left(\\Pi_{i}^{g}(\\cdot|s_{t})\\cdot m(s_{t})\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "If none of the control policies are beneficial for guidance, i.e., $m(s_{t})=\\mathbf{0}$ , it indicates that the current task\u2019s control policy $\\pi_{i}$ is the most proficient within the current state, rendering other control policies unnecessary for enhancing trajectory generation. ", "page_idx": 4}, {"type": "text", "text": "Comparable Guide Q-Value. Typically, RL algorithms estimate $Q\\cdot$ -values to approximate the expected return of the current state-action pair, allowing for the calculation of the policy-fliter gate in Equation 11 through a direct comparison of $V_{i}(s_{t})$ and $\\bar{Q}_{i}^{g}(s_{t},j_{t})$ . However, in maximum entropy RL algorithms such as SAC, Q-value estimation incorporates the maximum entropy objective, leading to the incomparability of two policies with different entropy objectives. Therefore, we learn another comparable guide Q-value ${\\hat{\\hat{Q}}}_{i}^{g}$ with discounted entropy of $\\pi_{i}$ following: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{Q}_{i}^{g}(s_{t},j_{t})=\\mathbb{E}_{a_{t^{\\prime}}\\sim\\pi_{j_{t}},s_{t^{\\prime}+1}\\sim P_{i}}\\left[\\sum_{t^{\\prime}=t}^{t+K-1}\\gamma^{t^{\\prime}-t}\\left(R_{i}(s_{t^{\\prime}},a_{t^{\\prime}})+\\alpha_{i}\\mathcal{H}(\\pi_{i}(\\cdot|s_{t^{\\prime}}))\\right)\\right]}\\\\ &{\\qquad\\qquad+\\,\\gamma^{K}\\mathbb{E}_{j_{t+K}\\sim\\Pi_{i}^{g},s_{t+K}\\sim P_{i}}\\left[\\hat{Q}_{i}^{g}(s_{t+K},j_{t+K}^{g})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Since both $\\hat{Q}_{i}^{g}(s_{t},j_{t})$ and $V_{i}(s_{t})$ estimate the return with the entropy of the current task\u2019s control policy, they can be directly compared to assess whether control policies are beneficial for guidance. A detailed comparability analysis of this comparable guide Q-value in SAC is provided in Appendix B. ", "page_idx": 4}, {"type": "text", "text": "4.3 Not All Tasks Need Guidance ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "When simultaneously learning multiple tasks, easy tasks converge faster than difficult ones. The control policies of easy tasks allow for the quick acquisition of some effective skills, which may ", "page_idx": 4}, {"type": "image", "img_path": "3qUks3wrnH/tmp/a5d326f40918d80ab4838c78fd044ec713c981596d98881a2ae891413998f63e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 3: Illustration of the comprehensive CTPG framework. Initially, the guide-block gate selectively provides guidance on tasks $i\\in\\mathbb{T}^{g}$ . Subsequently, the policy-filter gate generates a mask $m$ to sift through the beneficial policies. Finally, the policy chosen by the guide policy or the control policy of the current task itself interacts with the environment over $K$ timesteps to collect training data. ", "page_idx": 5}, {"type": "text", "text": "be helpful in exploring other tasks. However, these mastered or easy tasks do not need additional guidance from other policies; instead, they focus on further solidifying their already acquired skills. Therefore, not all tasks require assistance from the guide policy. ", "page_idx": 5}, {"type": "text", "text": "Based on the above analysis, we design another guide-block gate to prevent the guide policy from engaging in tasks that do not necessitate guidance. This mechanism is directly related to SAC\u2019s temperature coefficient $\\alpha_{i}$ . For difficult tasks $i_{\\mathrm{diff}}$ , their control policy entropies $\\mathcal{H}\\left(\\pi_{i_{\\mathrm{diff}}}(\\cdot|s_{t})\\right)$ tend to be high, and the corresponding temperature parameters $\\alpha_{i_{\\mathrm{diff}}}$ decrease according to Equation 4. Conversely, the temperature parameters $\\alpha_{i_{\\mathrm{casy}}}$ increase for easy tasks $i_{\\mathrm{easy}}$ . Therefore, $\\alpha_{i}$ is a metric reflecting the relative difficulty and mastery of different tasks. We form the tasks that require guidance into a subset $\\mathbb{T}^{g}$ following: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{T}^{g}=\\left\\{i|\\log\\alpha_{i}\\leq\\frac{1}{N}\\sum_{j=1}^{N}\\log\\alpha_{j}\\right\\},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "which selects the tasks by comparing the difficulty of the current task versus the average of all tasks. For tasks $i\\notin\\mathbb{T}^{g}$ , the guide-block gate restricts them from using the guide policy, so they only use their own control policies to interact with the environment. Consequently, the guide policy can stop training with samples from the tasks $i\\notin\\mathbb{T}^{g}$ and focus on learning the guidance for unmastered tasks. ", "page_idx": 5}, {"type": "text", "text": "The comprehensive CTPG framework, with two special gating mechanisms, is summarized in Figure 3. The complete pseudo-code for CTPG is described in Appendix A.2 (Algorithm 3). ", "page_idx": 5}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The experiments are designed to answer the following research questions: Q1: Can implicit knowledge sharing approaches be combined with CTPG to further improve performance? Q2: Does the guide policy in CTPG learn useful sharing policies? Q3: How does each component within CTPG contribute to the final performance? Q4: How does CTPG perform without implicit knowledge sharing approaches? Q5: Can CTPG expedite the exploration of new tasks effectively? ", "page_idx": 5}, {"type": "text", "text": "5.1 Environments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We conduct experiments on MetaWorld manipulation and HalfCheetah locomotion MTRL benchmarks, selecting two setups for evaluation within each benchmark. ", "page_idx": 5}, {"type": "text", "text": "MetaWorld Manipulation Benchmark. The MetaWorld benchmark [31] consists of 50 robotics manipulation tasks employing a sawyer arm in the MuJoCo environment [24]. It provides two setups: MetaWorld-MT10, comprising a suite of 10 tasks, and MetaWorld-MT50, comprising a suite of 50 tasks. Following the settings in [9], the goal position is randomly reset at the start of every episode. We use the mean success rate as our evaluation metric, which is clearly defined in the environment. ", "page_idx": 5}, {"type": "text", "text": "HalfCheetah Locomotion Benchmark. The HalfCheetah is a 6-DoF walking robot consisting of 9 links and 8 joints connecting them in the MuJoCo environment [24]. The multi-task benchmark HalfCheetah Task Group [11] contains different HalfCheetah robots. HalfCheetah-MT5 includes 5 tasks under various scales of simulated earth-like gravity, ranging from one-half to one-and-a-half of the normal gravity level. HalfCheetah-MT8 includes 8 tasks with various morphology of a specific robot body part. We use the episode return as our evaluation metric. ", "page_idx": 5}, {"type": "table", "img_path": "3qUks3wrnH/tmp/8b1b378df6d38bdf0f3ff5c3a0fe6e38df0a98a323f238630d724f8903e64cd0.jpg", "table_caption": ["Table 1: Quantitative result of five classical implicit knowledge sharing approaches combined with different explicit policy sharing methods. The two HalfCheetah locomotion environments are measured on episode return, and the two MetaWorld manipulation environments are measured on success rate. We highlight the best-performing explicit policy sharing method in bold and annotate the best combination of two information sharing methods with boxes. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Further information regarding environmental setups is provided in Appendix D. ", "page_idx": 6}, {"type": "text", "text": "5.2 Performance Improvement on Implicit Knowledge Sharing Approaches ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "CTPG is a generalized MTRL framework adaptable to various implicit knowledge sharing approaches, wherein both the control policy and the guide policy use a unified network. Specifically, in our implementation, the unified control policy employs the same network structure and update procedure as the implicit knowledge sharing approaches, and the unified guide policy utilizes a straightforward multi-head structure for parameter sharing. ", "page_idx": 6}, {"type": "text", "text": "To answer Q1, we choose five classical implicit knowledge sharing approaches: (1) MTSAC extends SAC for MTRL by employing one-hot encoding for task representation. (2) MHSAC utilizes a shared network backbone apart from independent heads for each task. (3) PCGrad [30] resolves issues arising from conflicting gradients among tasks through gradient manipulation. (4) SM [27] trains a routing network to propose weights for soft module combinations. (5) PaCo [22] learns a compositional policy where task-shared parameters combine with task-specific parameters to form task policies. Combined with these implicit knowledge sharing approaches, we compare CTPG against: (i) Base represents the source version of these approaches. (ii) QMP [32] employs a one-step Q-value filter to identify shareable behaviors. ", "page_idx": 6}, {"type": "text", "text": "We train all combinations with 0.8 million samples per task in the HalfCheetah locomotion benchmark and 1.5 million samples per task in the MetaWorld manipulation benchmark. Each combination is trained 5 times with different seeds. We evaluate the final policy over 100 episodes per task and report the mean performance and standard deviation across different seeds in Table 1. ", "page_idx": 6}, {"type": "text", "text": "Based on the experimental findings, it becomes evident that, except for the PaCo algorithm in HalfCheetah-MT8, the combination with CTPG leads to a notable enhancement in performance across all scenarios. QMP does not perform as well as CTPG because QMP only offers single-step behavior guidance, whereas CTPG\u2019s guide policy learns long-term policy guidance. Notably, as the number of tasks increases, CTPG exhibits superior performance due to the higher probability of explicit policy sharing between tasks, facilitating more effective policy exploration. Besides the final performance evaluation, comprehensive training curves are presented in Appendix E.1. ", "page_idx": 6}, {"type": "image", "img_path": "3qUks3wrnH/tmp/dac6bb6a9abd2abf858dcf9ef09e33b42606b8926999330e76057e4def846973.jpg", "img_caption": ["Figure 4: We display the state of task Pick-Place at every 10 timesteps, along with the corresponding output probability of the guide policy and the actual sampled behavior policy. Except for employing the Pick-Place task\u2019s control policy during timesteps 20 to 30, the guide policy selects control policies of other tasks for the remaining timesteps, successfully accomplishing the task. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "3qUks3wrnH/tmp/9777c80246be2ee5011e2719645707a2522b886af02177c864ae8120d34549e5.jpg", "img_caption": ["Figure 5: Three distinct ablation studies of MHSAC $w/$ CTPG on MetaWorld-MT10. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "5.3 Guidance Learned by Guide Policy ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To answer Q2, we visualize the task Pick-Place with guidance on MetaWorld-MT10 to show the specific role of the guide policy. Since the guide policy is only used in trajectory generation during training, we visualize one of the sampled trajectories in Figure 4. We only present the initial 50 timesteps of this trajectory, in which the task has essentially been completed. ", "page_idx": 7}, {"type": "text", "text": "To better understand policy sharing between tasks, we first explain the relevant tasks. Pick-Place: Pick and place a puck to a goal. Peg-Insert-Side: Insert a peg sideways. Push: Push the puck to a goal. Button-Press-Topdown: Press a button from the top. Drawer-Close: Push and close a drawer. Reach: Reach a goal position. The visualizations of these tasks are provided in Figure 8 (Appendix D.1). ", "page_idx": 7}, {"type": "text", "text": "The initial and final 20 timesteps showcase that the guide policy learns useful guidance to successfully complete the source task. In the initial 20 timesteps, Pick-Place and Peg-Insert-Side employ a shared policy directing the robotic arm toward the target object. In the final 20 timesteps, the task is executed using Button-Press-Topdown to raise the gripper and then Drawer-Close to move forward. Interestingly, although the task in the final 20 timesteps intuitively aligns with Reach, the guide policy opts not to use it because the learned Reach\u2019s control policy always opens the gripper, causing the puck to fall. In the middle 10 timesteps, the probability of Pick-Place is notably high due to the absence of alternative shared policies at this stage. Specifically, the most similar Peg-Insert-Side grabs the end of the peg for insertion, while Pick-Place requires gripping the puck centrally. ", "page_idx": 7}, {"type": "text", "text": "5.4 Ablation Studies ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To answer Q3, we conduct detailed ablation studies to analyze the impact of each component in CTPG on performance enhancement. CTPG contains three key components: (1) the policy-filter gate, (2) the guide-block gate, and (3) the hindsight off-policy correction mechanism. We study the influence of each component using the MHSAC implicit knowledge sharing approach on MetaWorld-MT10. ", "page_idx": 7}, {"type": "text", "text": "(1) Figure 5(a) shows that the policy-filter gate plays a significant role within CTPG, leveraging the value function to constrain the direction of guided exploration. (2) Given the precise definition of binary-valued success signal in the MetaWorld benchmark, we compare the SAC temperature metric, described in Section 4.3, with another success rate metric to determine if tasks need guidance. ", "page_idx": 7}, {"type": "text", "text": "Specifically, we block guidance for tasks with success rates exceeding $80\\%$ during evaluation. Figure 5(b) illustrates that both metrics showcase competitive performance, exhibiting clear performance gains over no guide-block gate. However, since the success rate is a human-defined metric and difficult to define for some tasks (e.g., HalfCheetah), the SAC temperature metric is more general across most environments. (3) We ablate the hindsight off-policy correction mechanism used in guide policy training in Figure 5(c), elucidating its improvement in training efficiency and stability. Further ablation studies of SM with CTPG on MetaWorld-MT50 are provided in Appendix E.2. ", "page_idx": 8}, {"type": "text", "text": "5.5 CTPG without Implicit Knowledge Sharing ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To answer Q4, we train $N$ control and guide policies independently on $N$ tasks without implicit knowledge sharing. We perform experiments on HalfCheetah-MT8 and MetaWorldMT10, employing the same environment configuration as in Section 5.2. As illustrated in Figure 6, CTPG significantly improves the performance of Single-Task SAC. Notably, the impact of CTPG on MetaWorld-MT10 is more pronounced. The potential rationale is that the tasks within HalfCheetah-MT8 exhibit minimal variance in difficulty, resulting in a narrow gap in ", "page_idx": 8}, {"type": "image", "img_path": "3qUks3wrnH/tmp/211075a0b304510b5f58fb4924d4404a4c372b6064959d07bc18d6f6c1f89e5e.jpg", "img_caption": ["Figure 6: CTPG also improves performance in the absence of implicit knowledge sharing approaches. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "learning progress. Conversely, MetaWorld-MT10 presents a broader disparity in task difficulty, where CTPG facilitates the guidance from simpler to more challenging tasks, thus appearing more efficient. ", "page_idx": 8}, {"type": "text", "text": "5.6 Exploration of New Tasks with CTPG ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To answer Q5, we split the original task set in half, pre-training expert policies on the one half $\\mathbb{T}^{e}$ . For the other half, we compare direct learning with CTPG, where the agent leverages guidance from both the control policies being learned and the expert policies. Specifically, we use MTSAC on HalfCheetah-MT8, where $\\mathbb{T}^{e}$ includes all tasks that enlarge the size of body parts. On the other hand, we use SM on MetaWorld-MT10, with $\\mathbb{T}^{e}$ containing tasks indexed from 0 to 4. Figure 7 indicates that, compared to learning the ", "page_idx": 8}, {"type": "image", "img_path": "3qUks3wrnH/tmp/ff530447df5a290d7b8e61c1453a46155bff1e618185701410c6ee7634463dbf.jpg", "img_caption": ["Figure 7: CTPG with expert policies can expedite the exploration of new tasks effectively. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "new set of tasks from scratch, CTPG can reasonably utilize experts to explore new tasks rapidly. Notably, in environments where task similarity is high, such as HalfCheetah-MT8, CTPG can quickly transfer the expert\u2019s abilities to the control policy being learned with the guide policy. ", "page_idx": 8}, {"type": "text", "text": "6 Conclusion and Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "This paper proposes the Cross-Task Policy Guidance (CTPG) framework for the MTRL explicit policy sharing. CTPG contains a guide policy and two special gates to identify beneficial sharing policies from the set of all task control policies and choose the most proficient one to generate high-quality trajectories for the current task as policy guidance. In addition, CTPG is a generalized framework adaptable to diverse implicit knowledge sharing approaches. Empirical evidence showcases that these approaches combined with CTPG further improve sample efficiency and final performance. ", "page_idx": 8}, {"type": "text", "text": "Limitations and Future Works. One limitation of CTPG is its reliance on a predetermined guide step $K$ , necessitating hyperparameter tuning for $K$ across different environments. Moreover, the fixed guide step setting lacks flexibility, as the duration of shared skills execution timesteps varies inconsistently among different tasks. Consequently, exploring methods to automate the selection of the guide step $K$ presents an intriguing avenue for future research. Additionally, irrespective of the human-defined win rate and the unique temperature parameter of SAC, investigating alternative metrics for the guide-block gate emerges as another important direction for future endeavors. ", "page_idx": 8}, {"type": "text", "text": "7 Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work is supported in part by the National Science and Technology Major Project (2022ZD0116401), the Natural Science Foundation of China (Grant Nos. 62076238, 62222606, and 61902402), the Key Research and Development Program of Jiangsu Province (Grant No. BE2023016), and the China Computer Federation (CCF)-Tencent Open Fund. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Richard Bellman. Dynamic programming. Science, 153(3731):34\u201337, 1966.   \n[2] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. OpenAI Gym. CoRR, abs/1606.01540, 2016.   \n[3] Rich Caruana. Multitask learning. Machine Learning, 28:41\u201375, 1997.   \n[4] Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and Andrew Rabinovich. Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks. In Proceedings of International Conference on Machine Learning, pages 794\u2013803, 2018.   \n[5] Myungsik Cho, Whiyoung Jung, and Youngchul Sung. Multi-task reinforcement learning with task representation method. In ICLR Workshop on Generalizable Policy Learning in Physical World, pages 1\u201311, 2022.   \n[6] Petros Christodoulou. Soft actor-critic for discrete action settings. CoRR, abs/1910.07207, 2019.   \n[7] Dibya Ghosh, Avi Singh, Aravind Rajeswaran, Vikash Kumar, and Sergey Levine. Divideand-conquer reinforcement learning. In Proceedings of International Conference on Learning Representations, pages 1\u201310, 2018.   \n[8] Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with deep energy-based policies. In Proceedings of International Conference on Machine Learning, pages 1352\u20131361, 2017.   \n[9] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In Proceedings of International Conference on Machine Learning, pages 1856\u20131865, 2018.   \n[10] Jinmin He, Kai Li, Yifan Zang, Haobo Fu, Qiang Fu, Junliang Xing, and Jian Cheng. Not all tasks are equally difficult: Multi-task deep reinforcement learning with dynamic depth routing. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 12376\u201312384, 2024.   \n[11] Peter Henderson, Wei-Di Chang, Florian Shkurti, Johanna Hansen, David Meger, and Gregory Dudek. Benchmark environments for multitask learning in continuous domains. In ICML Workshop on Lifelong Learning: A Reinforcement Learning Approach, pages 1\u20136, 2017.   \n[12] Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuomotor policies. The Journal of Machine Learning Research, 17(1):1334\u20131373, 2016.   \n[13] Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In Proceedings of International Conference on Learning Representations, pages 1\u201310, 2016.   \n[14] Bo Liu, Xingchao Liu, Xiaojie Jin, Peter Stone, and Qiang Liu. Conflict-averse gradient descent for multi-task learning. In Advances in Neural Information Processing Systems, pages 18878\u201318890, 2021.   \n[15] Samuel Lobel, Sreehari Rammohan, Bowen He, Shangqun Yu, and George Konidaris. Qfunctionals for value-based continuous control. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 8932\u20138939, 2023.   \n[16] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin A. Riedmiller, Andreas Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature, 518(7540):529\u2013533, 2015.   \n[17] Ashvin Nair, Bob McGrew, Marcin Andrychowicz, Wojciech Zaremba, and Pieter Abbeel. Overcoming exploration in reinforcement learning with demonstrations. In Proceedings of International Conference on Robotics and Automation, pages 6292\u20136299, 2018.   \n[18] Martin L Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley & Sons, 2014.   \n[19] Sebastian Ruder. An overview of multi-task learning in deep neural networks. CoRR, abs/1706.05098, 2017.   \n[20] Tianmin Shu, Caiming Xiong, and Richard Socher. Hierarchical and interpretable skill acquisition in multi-task reinforcement learning. In Proceedings of International Conference on Learning Representations, pages 1\u201311, 2018.   \n[21] Shagun Sodhani, Amy Zhang, and Joelle Pineau. Multi-task reinforcement learning with contextbased representations. In Proceedings of International Conference on Machine Learning, pages 9767\u20139779, 2021.   \n[22] Lingfeng Sun, Haichao Zhang, Wei Xu, and Masayoshi Tomizuka. PaCo: Parametercompositional multi-task reinforcement learning. In Advances in Neural Information Processing Systems, pages 21495\u201321507, 2022.   \n[23] Yee Teh, Victor Bapst, Wojciech M. Czarnecki, John Quan, James Kirkpatrick, Raia Hadsell, Nicolas Heess, and Razvan Pascanu. Distral: Robust multitask reinforcement learning. In Advances in Neural Information Processing Systems, pages 4496\u20134506, 2017.   \n[24] Emanuel Todorov, Tom Erez, and Yuval Tassa. MuJoCo: A physics engine for model-based control. In Proceedings of IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026\u20135033, 2012.   \n[25] Nelson Vithayathil Varghese and Qusay H Mahmoud. A survey of multi-task deep reinforcement learning. Electronics, 9(9):1363, 2020.   \n[26] Maciej Wolczyk, Micha\u0142 Zaj a\u02dbc, Razvan Pascanu, \u0141ukasz Kucin\u00b4ski, and Piotr Mi\u0142os\u00b4. Disentangling transfer in continual reinforcement learning. Advances in Neural Information Processing Systems, 35:6304\u20136317, 2022.   \n[27] Ruihan Yang, Huazhe Xu, Yi Wu, and Xiaolong Wang. Multi-task reinforcement learning with soft modularization. In Advances in Neural Information Processing Systems, pages 4767\u20134777, 2020.   \n[28] Deheng Ye, Zhao Liu, Mingfei Sun, Bei Shi, Peilin Zhao, Hao Wu, Hongsheng Yu, Shaojie Yang, Xipeng Wu, Qingwei Guo, Qiaobo Chen, Yinyuting Yin, Hao Zhang, Tengfei Shi, Liang Wang, Qiang Fu, Wei Yang, and Lanxiao Huang. Mastering complex control in MOBA games with deep reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 6672\u20136679, 2020.   \n[29] Tianhe Yu, Aviral Kumar, Yevgen Chebotar, Karol Hausman, Sergey Levine, and Chelsea Finn. Conservative data sharing for multi-task offline reinforcement learning. In Advances in Neural Information Processing Systems, pages 11501\u201311516, 2021.   \n[30] Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn. Gradient surgery for multi-task learning. In Advances in Neural Information Processing Systems, pages 5824\u20135836, 2020.   \n[31] Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey Levine. Meta-World: A benchmark and evaluation for multi-task and meta reinforcement learning. In Proceedings of the Conference on Robot Learning, pages 1094\u20131100, 2020.   \n[32] Grace Zhang, Ayush Jain, Injune Hwang, Shao-Hua Sun, and Joseph J Lim. Efficient multi-task reinforcement learning via selective behavior sharing. CoRR, abs/2302.00671, 2023.   \n[33] Jin Zhang, Siyuan Li, and Chongjie Zhang. CUP: Critic-guided policy reuse. In Advances in Neural Information Processing Systems, pages 27537\u201327548, 2022.   \n[34] Yu Zhang and Qiang Yang. A survey on multi-task learning. IEEE Transactions on Knowledge and Data Engineering, 34(12):5586\u20135609, 2021. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Pseudo Code ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Without implicit knowledge sharing approaches, each task $i$ has its control policy $\\pi_{i}$ and guide policy $\\Pi_{i}^{g}$ . With implicit knowledge sharing approaches, there is a unified control policy $\\pi$ and guide policy $\\Pi^{g}$ with the input of task representation $z_{i}$ . In both cases, we uniformly represent tasks as subscripts and omit the network parameters for a clear presentation in pseudo-code. Specifically, ", "page_idx": 12}, {"type": "text", "text": "\u2022 The actor network with parameter $\\phi$ for task $i$ \u2019s control policy is denoted as $\\pi_{i}(a_{t}|s_{t})$ .   \n\u2022 The critic network with parameter $\\theta$ for task $i$ \u2019s control policy is denoted as $Q_{i}(s_{t},a_{t})$ .   \n\u2022 The temperature parameter for task $i$ \u2019s control policy is denoted as $\\alpha_{i}$ .   \n\u2022 The guide actor network with parameter $\\phi^{g}$ for task $i$ \u2019s guide policy is denoted as $\\Pi_{i}^{g}(j_{t}|s_{t})$ .   \n\u2022 The guide critic network with parameter $\\theta^{g}$ for task $i$ \u2019s guide policy is denoted as $Q_{i}^{g}(s_{t},j_{t})$ .   \n\u2022 The guide temperature parameter for task $i$ \u2019s guide policy is denoted as $\\alpha_{i}^{g}$ .   \n\u2022 The comparable guide critic network with parameter ${\\hat{\\theta}}^{g}$ for task $i$ is denoted as $\\hat{Q}_{i}^{g}(s_{t},j_{t})$ . ", "page_idx": 12}, {"type": "text", "text": "Algorithm 1 Control Policy\u2019s Training Step ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Input: minibatch $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ contains $\\langle i,s_{t},a_{t},r_{t},s_{t+1}\\rangle$ Initialization: actor network $\\pi$ , critic network $Q$ , SAC temperature $\\alpha$ 1: Optimize $\\theta$ with SAC\u2019s critic loss in Equation 1 ", "page_idx": 12}, {"type": "equation", "text": "$$\nJ_{Q}(\\theta)=\\mathbb{E}_{(i,s_{t},a_{t},r_{t},s_{t+1})\\sim{B}}\\left[\\frac{1}{2}\\left(Q_{i}(s_{t},a_{t})-\\left(r_{t}+\\gamma V_{i}(s_{t+1})\\right)\\right)^{2}\\right]\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "2: Optimize $\\phi$ with SAC\u2019s actor loss in Equation 3 ", "page_idx": 12}, {"type": "equation", "text": "$$\nJ_{\\pi}(\\phi)=\\mathbb{E}_{(i,s_{t})\\sim\\mathcal{B}}\\left[\\mathbb{E}_{a_{t}\\sim\\pi_{i}}\\left[\\alpha_{i}\\log\\pi_{i}(a_{t}|s_{t})-Q_{i}(s_{t},a_{t})\\right]\\right]\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "3: Optimize $\\alpha$ with SAC\u2019s alpha loss in Equation 4 ", "page_idx": 12}, {"type": "equation", "text": "$$\nJ(\\alpha)=\\mathbb{E}_{i\\sim\\mathcal{B}}\\left[\\mathbb{E}_{a_{t}\\sim\\pi_{i}}\\left[-\\alpha_{i}\\log\\pi_{i}(a_{t}|s_{t})-\\alpha_{i}\\bar{\\mathcal{H}}\\right]\\right]\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Algorithm 2 Guide Policy\u2019s Training Step ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "IInniptiuatl: iazcattoiro nn:e tgwuoirdke $\\pi$ ,c tSorA Cn ettewmoprek r, eg $\\alpha$ i, dme icnriibtiact cnh $B^{g}$ ocrok n, sg $\\langle\\left\\{s_{t^{\\prime}},a_{t^{\\prime}},r_{t^{\\prime}}\\right\\}_{t^{\\prime}=t}^{t+K-1},i,j_{t},s_{t+K}\\rangle$ $\\Pi^{g}$ $Q^{g}$ $\\alpha^{g}$ ", "page_idx": 12}, {"type": "text", "text": "1: Hindsight off-policy correct $j_{t}$ into $j_{t}^{\\prime}$ following Equation 9 ", "page_idx": 12}, {"type": "equation", "text": "$$\nj_{t}^{\\prime}=\\arg\\operatorname*{max}_{j}\\sum_{t^{\\prime}=t}^{t+K-1}\\log\\pi_{j}(a_{t^{\\prime}}|s_{t^{\\prime}})\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "2: Calculate the guide reward $r_{t}^{g}$ following Equation 6 ", "page_idx": 12}, {"type": "equation", "text": "$$\nr_{t}^{g}=\\sum_{t^{\\prime}=t}^{t+K-1}\\gamma^{t^{\\prime}-t}r_{t^{\\prime}}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "3: Optimize $\\theta^{g}$ with SAC\u2019s critic loss for guide policy ", "page_idx": 12}, {"type": "equation", "text": "$$\nJ_{Q^{g}}(\\theta^{g})=\\mathbb{E}_{(i,s_{t},j_{t}^{\\prime},r_{t}^{g},s_{t+K})\\sim{B^{g}}}\\left[\\frac{1}{2}\\left(Q_{i}^{g}(s_{t},j_{t}^{\\prime})-\\left(r_{t}^{g}+\\gamma^{K}V_{i}^{g}(s_{t+K})\\right)\\right)^{2}\\right]\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "4: Optimize $\\phi^{g}$ with SAC\u2019s actor loss for guide policy ", "page_idx": 12}, {"type": "equation", "text": "$$\nJ_{\\Pi^{g}}(\\phi^{g})=\\mathbb{E}_{(i,s_{t})\\sim\\mathcal{B}^{g}}\\left[\\mathbb{E}_{j_{t}\\sim\\Pi_{i}^{g}}\\left[\\alpha_{i}^{g}\\log\\Pi_{i}^{g}(j_{t}|s_{t})-Q_{i}^{g}(s_{t},j_{t})\\right]\\right]\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "5: Optimize $\\alpha^{g}$ with SAC\u2019s alpha loss for guide policy ", "page_idx": 12}, {"type": "equation", "text": "$$\nJ(\\alpha^{g})=\\mathbb{E}_{i\\sim\\mathcal{B}^{g}}\\left[\\mathbb{E}_{j_{t}\\sim\\Pi_{i}^{g}}\\left[-\\alpha_{i}^{g}\\log\\Pi_{i}^{g}(j_{t}|s_{t})-\\alpha_{i}^{g}\\bar{\\mathcal{H}}^{g}\\right]\\right]\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "A.1 Pseudo Code of Policy Training ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Algorithm 1 describes a training step for the control policy given a minibatch of data $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ , which contains a batch of tasks $i$ , states $s_{t}$ , actions $a_{t}$ , rewards $r_{t}$ , and next states $s_{t+1}$ . ", "page_idx": 13}, {"type": "text", "text": "In addition, Algorithm 2 describes a training step for the guide policy given a minibatch data $\\,_{\\!\\mathscr{B}^{g}}$ . eti dmaetsat $B^{g}$ naohte aodn lsyt acteosn ab buta taclhs oo fc toanstkais $i$ ,  rsetawtaers $s_{t}$ $j_{t}$ taackcernu ebdy  ogvueird otilimcey,s taenpds $K$ $s_{t+K}$ $\\{r_{t^{\\prime}}\\}_{t^{\\prime}=t}^{t+K-1}$ $K$ M  Bg  {st\u2032}tt\u2032+=Kt\u2212 tfaokr ecna lbcyu lcaotinntrg otlh pe ogliuciyd eo rveerw tahred. toirmeoesvteer,p ri nhcilnuddseisg hstt aotfefs- catinodn .actions $\\scriptstyle\\{a_{t^{\\prime}}\\}_{t^{\\prime}=t}^{t+K-1}$ $K$ ", "page_idx": 13}, {"type": "text", "text": "A.2 Pseudo Code of Comprehensive CTPG ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The full pseudo-code of CTPG is described in Algorithm 3. ", "page_idx": 13}, {"type": "text", "text": "Algorithm 3 Cross-Task Policy Guidance ", "text_level": 1, "page_idx": 13}, {"type": "table", "img_path": "3qUks3wrnH/tmp/b2c46b46ddb2f7d1dfe5a340d260756e5c706871646efd290b72ba8a1dd69c32.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "B Details on Comparable Guide Q-Value ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We expand the value function of SAC to obtain: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{V_{i}(s_{t})=\\mathbb{E}_{a_{t}\\sim\\pi_{i}}\\left[Q_{i}(s_{t},a_{t})-\\alpha_{i}\\log\\pi_{i}(a_{t}|s_{t})\\right]}}\\\\ &{=\\mathbb{E}_{a_{t}\\sim\\pi_{i}}\\left[Q_{i}(s_{t},a_{t})+\\alpha_{i}\\mathcal{H}(\\pi_{i}(\\cdot|s_{t}))\\right]}\\\\ &{=\\mathbb{E}_{a_{t}\\sim\\pi_{i}}\\left[R_{i}(s_{t},a_{t})+\\gamma\\mathbb{E}_{s_{t+1}\\sim P_{i}}\\left[V_{i}(s_{t+1})\\right]+\\alpha_{i}\\mathcal{H}(\\pi_{i}(\\cdot|s_{t}))\\right]}\\\\ &{=\\mathbb{E}_{a_{t}\\sim\\pi_{i}}\\left[R_{i}(s_{t},a_{t})+\\alpha_{i}\\mathcal{H}(\\pi_{i}(\\cdot|s_{t}))+\\gamma\\mathbb{E}_{s_{t+1}\\sim P_{i}}\\left[V_{i}(s_{t+1})\\right]\\right]}\\\\ &{=\\dots}\\\\ &{=\\mathbb{E}_{a_{t}\\sim\\pi_{i},s_{t^{\\prime}+1}\\sim P_{i}}\\left[\\displaystyle\\sum_{t^{\\prime}=t}^{\\infty}\\gamma^{t^{\\prime}-t}\\left(R_{i}(s_{t^{\\prime}},a_{t^{\\prime}})+\\alpha_{i}\\mathcal{H}(\\pi_{i}(\\cdot|s_{t^{\\prime}}))\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "By repeatedly expanding $V_{i}$ , we get the final form, which is actually the optimization objective under the maximum entropy reinforcement learning framework. ", "page_idx": 14}, {"type": "text", "text": "Then, we consider that the trajectory generation process solely using the current task\u2019s control policy $\\pi_{i}$ can be regarded as equipped with a special guide policy $\\Pi_{i}^{\\tilde{g}}$ in SAC. The comparable guide $Q\\cdot$ -value in Equation 13 of this special guide policy $\\Pi_{i}^{\\tilde{g}}$ is: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{Q}_{i}^{\\hat{\\eta}}(s_{t},i)=\\mathbb{E}_{a_{t^{\\prime}}\\sim\\pi t_{i},s_{t^{\\prime}+1}\\sim P_{i}}\\left[\\displaystyle\\sum_{t^{\\prime}=t}^{t+K-1}\\gamma^{t^{\\prime}-t}\\left(R_{i}(s_{t^{\\prime}},a_{t^{\\prime}})+\\alpha_{i}\\mathcal{H}(\\pi_{i}(\\cdot|s_{t^{\\prime}}))\\right)\\right]+\\gamma^{K}\\mathbb{E}_{s_{t+K}\\sim P_{i}}\\left[\\hat{Q}_{i}^{\\hat{\\eta}}(s_{t+K},i)\\right]}\\\\ &{\\quad\\quad=\\mathbb{E}_{a_{t^{\\prime}}\\sim\\pi t_{i},s_{t^{\\prime}+1}\\sim P_{i}}\\left[\\displaystyle\\sum_{t^{\\prime}=t}^{t+K-1}\\gamma^{t^{\\prime}-t}\\left(R_{i}(s_{t^{\\prime}},a_{t^{\\prime}})+\\alpha_{i}\\mathcal{H}(\\pi_{i}(\\cdot|s_{t^{\\prime}}))\\right)+\\gamma^{K}\\hat{Q}_{i}^{\\hat{\\eta}}(s_{t+K},i)\\right]}\\\\ &{\\quad\\quad=\\dots}\\\\ &{\\displaystyle=\\mathbb{E}_{a_{t^{\\prime}}\\sim\\pi t_{i},s_{t^{\\prime}+1}\\sim P_{i}}\\left[\\displaystyle\\sum_{t^{\\prime}=t}^{\\infty}\\gamma^{t^{\\prime}-t}\\left(R_{i}(s_{t^{\\prime}},a_{t^{\\prime}})+\\alpha_{i}\\mathcal{H}(\\pi_{i}(\\cdot|s_{t^{\\prime}}))\\right)\\right]}\\\\ &{\\quad\\quad=V_{i}(s_{t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The ellipsis part is the repeated expansion of $\\hat{Q}_{i}^{\\tilde{g}}$ . Therefore, in SAC, the policy-fliter gate can refine the action space of the guide policy by directly comparing $\\hat{Q}_{i}^{g}(s_{t},j_{t})$ and $V_{i}(s_{t})$ . ", "page_idx": 14}, {"type": "text", "text": "C Details on Hindsight Off-Policy Correction for SAC ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The hindsight off-policy correction mechanism is proposed to mitigate the non-stationarity challenge of the guide policy update process. It reassigns the action $j_{t}$ sampled by the past guide policy to a new one $j_{t}^{\\prime}$ , whose control policy $\\pi_{j_{t}^{\\prime}}$ is more likely to output the historical action sequence $\\bar{\\{a_{t^{\\prime}}\\}}_{t^{\\prime}=t}^{t+K-1}$ During the guide policy update process, we first get the reassigned action $j_{t}^{\\prime}$ following the maximum likelihood estimation in Equation 9. For SAC\u2019s critic update, the critic loss of the guide policy is, ", "page_idx": 14}, {"type": "equation", "text": "$$\nJ_{Q^{g}}(\\theta^{g})=\\mathbb{E}_{(i,s_{t},j_{t}^{\\prime},r_{t}^{g},s_{t+K})\\sim{B^{g}}}\\left[\\frac{1}{2}\\left(Q_{i}^{g}(s_{t},j_{t}^{\\prime})-\\left(r_{t}^{g}+\\gamma^{K}V_{i}^{g}(s_{t+K})\\right)\\right)^{2}\\right],\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the hindsight correction explicitly modifies the update procedure by shifting the Q-value estimation from $\\bar{Q}_{i}^{g}(s_{t},j_{t})$ to $Q_{i}^{g}(s_{t}^{-},j_{t}^{\\prime})$ . For SAC\u2019s actor update, the actor loss of the guide policy is, ", "page_idx": 14}, {"type": "equation", "text": "$$\nJ_{\\Pi^{g}}(\\phi^{g})=\\mathbb{E}_{(i,s_{t})\\sim\\mathcal{B}^{g}}\\left[\\mathbb{E}_{j_{t}\\sim\\Pi_{i}^{g}}\\left[\\alpha_{i}^{g}\\log\\Pi_{i}^{g}(j_{t}|s_{t})-Q_{i}^{g}(s_{t},j_{t})\\right]\\right],\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which is not explicitly different from the original function in SAC. However, it indirectly affects actor learning due to SAC\u2019s unique actor update method [9]. Specifically, SAC\u2019s actor loss is derived from its optimization objective, and the policy is updated according to, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\pi_{\\mathrm{new}}=\\arg\\operatorname*{min}_{\\pi^{\\prime}\\in\\Pi}D_{K L}\\left(\\pi^{\\prime}(\\cdot|s_{t})\\left\\|\\frac{\\exp\\left(\\frac{1}{\\alpha}Q^{\\pi_{\\mathrm{old}}}(s_{t},\\cdot)\\right)}{Z^{\\pi_{\\mathrm{old}}}(s_{t})}\\right),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the partition function $Z^{\\pi_{\\mathrm{old}}}\\!\\left(s_{t}\\right)$ normalizes the distribution. In essence, SAC\u2019s policy is updated by fitting the distribution of the SoftMax function of Q-value with temperature $\\alpha$ . Therefore, modifying the update of $Q_{i}^{g}(s_{t},j_{t})$ to $Q_{i}^{g}(s_{t},j_{t}^{\\prime})$ using hindsight correction leads to a different guide policy actor optimization objective, thus affecting the training of the guide actor network. ", "page_idx": 14}, {"type": "text", "text": "D Environment Details ", "text_level": 1, "page_idx": 15}, {"type": "image", "img_path": "3qUks3wrnH/tmp/454f1ec8b383134ecd0654e96bf864df8fd1be209aa1562be4a2e67ca47c684d.jpg", "img_caption": ["D.1 MetaWorld Manipulation Benchmark ", "Figure 8: Visualizations of robotic manipulation tasks on MetaWorld-MT10. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "The MetaWorld manipulation benchmark [31] consists of 50 robotics manipulation tasks employing a sawyer arm to acquire diverse manipulation skills. MetaWorld-MT50 encompasses all 50 manipulation tasks, and MetaWorld-MT10 encompasses 10 tasks (a subset of MT50), as illustrated in Figure 8. In MetaWorld, the source tasks are configured with fixed goals, limiting the policy\u2019s ability to generalize to tasks of the same type with varying goals. Following a similar setup as [27], we extend all the tasks to a random-goal setting, where both items and goals reset randomly at each episode\u2019s onset. In addition, unlike [27, 30], we conduct our experiments on the MetaWorld-V2 benchmark 2. ", "page_idx": 15}, {"type": "text", "text": "In addition, we extend the episode length to 200 timesteps, different from the previous setting of 150. We evaluate the rule-based policies provided by the MetaWorld benchmark across 100 sample episodes. Under the initial episode length setting of 150 timesteps, 42 tasks achieve a success rate exceeding $90\\%$ . However, among the remaining 8 tasks, the task Disassemble exhibits a mere $50\\%$ success rate. In contrast, upon adjusting the episode length setting to 200 timesteps, success rates for all 50 tasks exceed $90\\%$ , with the task Disassemble achieving an improved success rate of $91\\%$ . ", "page_idx": 15}, {"type": "text", "text": "D.2 HalfCheetah Locomotion Benchmark ", "text_level": 1, "page_idx": 15}, {"type": "image", "img_path": "3qUks3wrnH/tmp/8770d30b888618beb4f66f6fb60b2074d8733303e3e3bb0728f94af80dcd73e4.jpg", "img_caption": ["Figure 9: Visualizations of robotic locomotion tasks on HalfCheetah-MT8. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "The HalfCheetah locomotion benchmark originates from the gym-extensions framework [11] 3, which focuses on continuous control multi-task reinforcement learning tasks, including several task groups of the standard gym environments [2]. We run our experiments with the HalfCheetah agent, which is a 2-dimensional robot consisting of 9 links and 8 joints connecting them (including two paws). Each episode contains 1000 timesteps. ", "page_idx": 16}, {"type": "text", "text": "HalfCheetah-MT5 consists of 5 locomotion tasks. The HalfCheetah agent is tasked with running in environments with various scales of simulated earth-like gravity, ranging from one-half to one-and-a-half of the normal gravity level. The detailed gravity value of each task is illustrated in Table 2. ", "page_idx": 16}, {"type": "text", "text": "HalfCheetah-MT8 consists of 8 locomotion tasks, as depicted in Figure 9. The HalfCheetah agent must run with variations in the morphol", "page_idx": 16}, {"type": "table", "img_path": "3qUks3wrnH/tmp/f2ec6e19d88c71f5dd1f16364c88d173267b824114eeb94d6857e8c31764d3dc.jpg", "table_caption": ["Table 2: The gravity setup of HalfCheetah-MT5. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "ogy of a specific body part, such as the torso, thigh, leg, and foot. The Big body part involves scaling the mass and width of the limb by 1.25, and the Small body part involves scaling by 0.75. ", "page_idx": 16}, {"type": "image", "img_path": "3qUks3wrnH/tmp/319d27f26b6b1eb9ac57a0831b0a63785018abc26fed70adfe021ebb766b1c1d.jpg", "img_caption": ["Figure 10: Training curves of experiment with implicit knowledge sharing approaches. Beyond the ultimate performance improvement, CTPG also enhances the sample efficiency. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "E Additional Experimental Results ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "E.1 Training Curves of Experiment with Implicit Knowledge Sharing Approaches ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "This section complements the result in Section 5.2, which only presents the final performance of the experiment. Here, we show the training curves for different combinations of explicit policy sharing methods and implicit knowledge sharing approaches across four environments in Figure 10. Each row of the figure represents a distinct implicit knowledge sharing approach, while each column represents a different environment. Within each subfigure, the three curves represent the base one without any explicit policy sharing method and two variations using different explicit policy sharing methods. We evaluate the training policy every 10K samples per task with 32 episodes and report the mean episode return or success rate, along with standard deviation, across 5 different seeds. The result shows that beyond the ultimate performance improvement, CTPG also enhances the sample efficiency. ", "page_idx": 17}, {"type": "text", "text": "E.2 Additional Results of Ablation Studies ", "text_level": 1, "page_idx": 17}, {"type": "image", "img_path": "3qUks3wrnH/tmp/86bfebb90afa4db2775e68c34e0b6d97a78d68bf1b4e43f8236ba5813068ca0e.jpg", "img_caption": ["Figure 11: Three distinct ablation studies of $\\mathbf{S}\\mathbf{M}\\mathbf{\\Sigma}w/\\mathbf{\\Sigma}$ CTPG on MetaWorld-MT50. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "In addition to the ablation experiment of MHSAC with CTPG on MetaWorld-MT10 in Section 5.4, we also conduct ablation studies using SM implicit knowledge sharing approach on MetaWorld-MT50. Figure 11 shows similar results to those in Section 5.4. The policy-fliter gate is crucial within CTPG. The guide-block gates with two metrics demonstrate competitive performance and show improvement compared to the absence of the guide-block gate. Additionally, the hindsight off-policy correction mechanism significantly enhances training efficiency and stability. ", "page_idx": 17}, {"type": "text", "text": "E.3 Ablation Study on Guide Step ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Within CTPG, the guide policy $\\Pi_{i}^{g}$ selects a policy $\\pi_{j_{t}}$ from the candidate set of all control policies $\\{\\pi_{j}\\}_{j=1}^{N}$ every fixed $K$ timesteps. The selected policy $\\pi_{j_{t}}$ is then used as the behavior policy to interact with the environment and collect data for the next $K$ timesteps. ", "page_idx": 17}, {"type": "image", "img_path": "3qUks3wrnH/tmp/ced5e4145f78dce7d79604b89c0cf4f5673c9d1787a4ed3a46fb7979646f10fe.jpg", "img_caption": ["Figure 12: MHSAC $w/$ CTPG with different guide steps $K$ . "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "The guide step $K$ is a predefined hyper-parameter. We conduct ablation experiments on the guide step $K$ in the two setups: HalfCheetah-MT8 and MetaWorld-MT10. In HalfCheetah-MT8, an entire episode contains 1000 timesteps, so we set $K\\in\\{1,5,10,20,50\\}$ . In MetaWorld-MT10, an entire episode contains 200 timesteps, so we set $K\\,\\in\\,\\{1,3,5,10,20\\}$ . The result, shown in Figure 12, indicates that both short and long guide steps lead to decreased performance and increased variance. ", "page_idx": 17}, {"type": "text", "text": "Overall, CTPG performs well in both environments when $K=10$ . Furthermore, automating the selection of the guide step $K$ is part of our future work. ", "page_idx": 18}, {"type": "text", "text": "E.4 Ablation Study on Monte Carlo Sampling ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In Section 4.2, we design a policy-fliter gate to refine the action space of the guide policy by adaptively filtering out control policies that are not beneficial for guidance. Specifically, the restriction of the action space is implemented by a mask, which is generated by comparing the control policy\u2019s V-value $V_{i}(s_{t})$ and the guide policy\u2019s Q-value $Q_{i}^{g}(s_{t},j_{t})$ . ", "page_idx": 18}, {"type": "image", "img_path": "3qUks3wrnH/tmp/abfe9d7e508184e894dcfe551a00c2357a81ac64aa967fd73e19848f187ceb92.jpg", "img_caption": ["Figure 13: MHSAC $w/$ CTPG with various Monte Carlo sampling times $H$ to estimate $\\mathrm{V}.$ -value. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "In SAC, the $\\mathrm{V}.$ -value is the expectation of the $\\mathrm{{Q}}.$ -value with the entropy of policy. It is formulated as: ", "page_idx": 18}, {"type": "equation", "text": "$$\nV_{i}(s_{t})=\\mathbb{E}_{a_{t}\\sim\\pi_{i}}\\left[Q_{i}(s_{t},a_{t})-\\alpha_{i}\\log\\pi_{i}(a_{t}|s_{t})\\right]\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "To avoid introducing an additional network to estimate the $\\mathrm{V}.$ -value $V_{i}(s_{t})$ , we estimate it via Monte Carlo sampling of the $\\mathrm{^Q}$ -values $Q_{i}(s_{t},a_{t})$ with $a_{t}\\sim\\pi_{i}(a_{t}|s_{t})$ [15]. We conduct ablation experiments varying the Monte Carlo sampling times $H$ , as depicted in Figure 13. It is observed that the performance remains largely unaffected by the number of samples, except in cases where $H$ equals 1, which leads to performance degradation and heightened variance due to excessive randomness. ", "page_idx": 18}, {"type": "text", "text": "E.5 Additional Comparison with BPT ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "CTPG learns a flexible mixture policy combining different control policies. To verify the advantages of the mixture strategy, we compare CTPG against another baseline that incorporates a guide strategy from [26], which solely learns from another control policy with the highest performance on the task. Since [26] focuses on continual RL, which requires a predefined task sequence and does not align with the MTRL experimental setting. Therefore, we adapt the core idea, implementing a version that selects the best-performing policy for the current task to guide exploration and generate trajectory data. Specifically, after each $H$ rounds of data collection, we evaluate all policies across all tasks, selecting the top-performing policy for each task to guide data collection in the subsequent rounds. We refer to this baseline as BPT (Best Performance Transfer) and set $H$ to 50 episodes in our implementation. We use MTSAC in HalfCheetah-MT8 and MHSAC in MetaWorld-MT10. ", "page_idx": 18}, {"type": "image", "img_path": "3qUks3wrnH/tmp/91b4949f588d5d83bfccb3695f6545922dec27299c19dc39f340a693aec2746a.jpg", "img_caption": ["Figure 14: The result of additional comparison with BPT. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "The result, shown in Figure 14, demonstrates that CTPG outperforms BPT. CTPG\u2019s guide policy learns a more flexible strategy: it not only can learn to share a single policy within a complete trajectory (like BPT), but also can develop a mixture policy by combining different control policies, making it more transferable between tasks. Notably, BPT performs better in HalfCheetah-MT8 than in MetaWorld-MT10 because the tasks in HalfCheetah-MT8 share more significant similarities, whereas the policy transfer and guidance in MetaWorld-MT10 require combination strategies. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "E.6 Adaptability of Other RL Algorithms to CTPG ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "CTPG is a general MTRL framework that can be adapted with other RL algorithms, even allowing the control and guide policy to use different RL algorithms. Since SAC is a widely used algorithm in continuous control and serves as the base algorithm for all baselines, we also choose SAC as the base algorithm in this work. In addition, we explore the adaptation of TD3 with the CTPG and also employ different RL algorithms for the control and guide policies. Specifically, the control policy uses TD3, and the guide policy uses DQN. We evaluate MTTD3 (the original TD3 using one-hot encoding for task representation) on HalfCheetah-MT8 and MHTD3 (utilizing a multi-head network for different tasks) on MetaWorld-MT10. The result shown in Figure 15 demonstrates that CTPG can enhance performance and sample efficiency when combined with other backbone RL algorithms. ", "page_idx": 19}, {"type": "image", "img_path": "3qUks3wrnH/tmp/74ced09a8581cea2678714c5f22533b83f064ae18c48f07deca32e906c6881c0.jpg", "img_caption": ["Figure 15: The result of CTPG based on TD3 RL algorithm. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "F Implementation Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We implement all experiments using the MTRL codebase [21] 4 and access the HalfCheetah locomotion environment to this framework. One significant change we make involves removing the reward normalizer wrapper, which leads to worse results. ", "page_idx": 19}, {"type": "text", "text": "F.1 Details on Computational Resources ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We use AMD EPYC 7742 64-Core Processor with NVIDIA Geforce RTX 3090 GPU for training. Each method is trained 5 times with different seeds. Using MetaWorld-MT10 as an example, the training time required for a full training run varies from 16 hours (MTSAC) to 52 hours (PcGrad) for the baseline implicit knowledge sharing approaches. For each approach with CTPG, the training times range from 20 hours (MTSAC $w/$ CTPG) to 56 hours (PcGrad w/ CTPG). The guide policy, using a simple multi-head network architecture, acts every $K$ step ( $K=10$ in our implementation), so its training frequency is set as $1/K$ . Consequently, CTPG does not significantly increase the training time. For a detailed hyper-parameter of the guide policy, please refer to Appendix F.3. ", "page_idx": 19}, {"type": "text", "text": "F.2 Additional MTRL Training Setups ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "For a fair comparison, we use the following common training setups on all methods: ", "page_idx": 19}, {"type": "text", "text": "Disentangled SAC temperature parameters. This setup is standard established in MTRL. Herein, distinct tasks utilize varying SAC temperature parameters, as described in Equations 2 and 3, which are also optimized by independent losses in Equation 4. Therefore, this setup facilitates the individual adaptation of exploration and exploitation balances for each task, effectively accommodating the diverse learning dynamics across different tasks during training. ", "page_idx": 19}, {"type": "text", "text": "Loss maskout of extreme tasks. This setup, utilized by [21, 22], involves the selective masking of the potentially destabilized loss $J_{i}$ of task $i$ from the total loss $J$ , aiming to mitigate its adverse effects on other tasks. Specifically, when the task loss $J_{i}$ surpasses a predefined threshold $\\epsilon$ (set as $3e3$ , the same as [22]), that task $i$ is excluded from the total training loss. ", "page_idx": 20}, {"type": "text", "text": "Multi-task loss rescaling. In recognition of the inherent discrepancy in convergence rates between tasks, where easy tasks usually converge faster, [27] propose an optimization objective weight of task $i$ , formulated as: ", "page_idx": 20}, {"type": "equation", "text": "$$\nw_{i}=\\frac{\\exp(-\\alpha_{i})}{\\sum_{j=1}^{N}\\exp(-\\alpha_{j})},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\alpha_{i}$ is the SAC temperature parameter of task $i$ , and $N$ is the total number of tasks. Consequently, the total loss is adjusted to $J=\\mathbb{E}_{i}[w_{i}\\cdot J_{i}]$ , ensuring a balanced training process across different tasks. ", "page_idx": 20}, {"type": "text", "text": "F.3 Hyper-Parameters of All Method ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "This section provides the hyper-parameters of each method in our experiment. General hyperparameters shared by all methods are illustrated in Table 3. Table 4 to Table 8 show the additional hyper-parameters specific to each implicit knowledge sharing approach. In addition, the guide policy can also be trained using implicit knowledge sharing methods. In our implementation, we use the simple multi-head network structure for the guide policy in CTPG, and the relevant additional hyper-parameters are displayed in Table 10. ", "page_idx": 20}, {"type": "table", "img_path": "3qUks3wrnH/tmp/3558be14b30e4a9f867bf7f4e032674590669dfac7286db108794a4b12e44a0f.jpg", "table_caption": ["Table 3: General hyper-parameters of all methods. "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "3qUks3wrnH/tmp/98d42e340b57741061030be32b57159d19eca5e7c237568a1d1ce0c4aeffc672.jpg", "table_caption": ["Table 4: Additional hyper-parameters of MTSAC. "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "3qUks3wrnH/tmp/721e721846c6960e31761e33d19ecf6e0bc62a72c6c0b4b1eef85ac2661c0cfb.jpg", "table_caption": ["Table 5: Additional hyper-parameters of MHSAC. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Table 6: Additional hyper-parameters of PCGrad. ", "page_idx": 21}, {"type": "table", "img_path": "3qUks3wrnH/tmp/a378d7cd75523189231c426a165469de72475466df13c5a8757307d8dea6a57e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "3qUks3wrnH/tmp/be37c1ab19b1f74b9d2af32bd685a55cc47b2f6106ae7871d2ac03a3d89f308a.jpg", "table_caption": ["Table 7: Additional hyper-parameters of SM. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "Table 8: Additional hyper-parameters of Paco. ", "page_idx": 21}, {"type": "table", "img_path": "3qUks3wrnH/tmp/da652dd66a49c42efcc297568cec18e5b669616401668aa04a359ee03707487b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "Table 9: Additional hyper-parameters of QMP. ", "page_idx": 21}, {"type": "table", "img_path": "3qUks3wrnH/tmp/ec6349d9ba7a627e32d6099201db349a0cfae2932df875e2ce572a314375d568.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "Table 10: Additional hyper-parameters of guide policy in CTPG. ", "page_idx": 21}, {"type": "table", "img_path": "3qUks3wrnH/tmp/dc5bd9e0aae8737945160d25d9cd6aa2e2d15f014f0508f923601384c93f356b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "G Broader Impacts ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "This paper presents work that aims to advance the field of Multi-Task Reinforcement Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here. ", "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: The paper discusses the limitations of the work in Section 6. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: This paper does not include theoretical results. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The full code is provided in supplemental material and detailed settings are available in Appendix E. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 23}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The full code is provided in supplemental material. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: All the training and test details are provided in Appendix E. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 24}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: The statistical significance of the experiments is provided in Section 5. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The experiments compute resources information is provided in Appendix E.1. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 25}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The research conduct in the paper conform, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 25}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper discusses broader impacts in Appendix F. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 26}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The code this paper uses is properly mentioned in Appendix with footnotes. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 26}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 27}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 27}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}]