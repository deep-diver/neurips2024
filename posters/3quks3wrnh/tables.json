[{"figure_path": "3qUks3wrnH/tables/tables_6_1.jpg", "caption": "Table 1: Quantitative result of five classical implicit knowledge sharing approaches combined with different explicit policy sharing methods. The two HalfCheetah locomotion environments are measured on episode return, and the two MetaWorld manipulation environments are measured on success rate. We highlight the best-performing explicit policy sharing method in bold and annotate the best combination of two information sharing methods with boxes.", "description": "This table presents the quantitative results of experiments comparing five different implicit knowledge sharing approaches (MTSAC, MHSAC, SM, PaCo, PCGrad) combined with three different explicit policy sharing methods (Base, w/ QMP, w/ CTPG).  The table shows performance metrics (episode return or success rate) on two HalfCheetah locomotion tasks (MT5 and MT8) and two MetaWorld manipulation tasks (MT10 and MT50). The best-performing explicit policy sharing method for each implicit knowledge sharing approach is highlighted in bold, and the best overall combination (implicit and explicit) is indicated by a box.", "section": "5.2 Performance Improvement on Implicit Knowledge Sharing Approaches"}, {"figure_path": "3qUks3wrnH/tables/tables_13_1.jpg", "caption": "Table 1: Quantitative result of five classical implicit knowledge sharing approaches combined with different explicit policy sharing methods. The two HalfCheetah locomotion environments are measured on episode return, and the two MetaWorld manipulation environments are measured on success rate. We highlight the best-performing explicit policy sharing method in bold and annotate the best combination of two information sharing methods with boxes.", "description": "This table presents a quantitative comparison of five different implicit knowledge sharing methods (MTSAC, MHSAC, SM, PaCo, PCGrad) when combined with two explicit policy sharing methods (QMP and CTPG).  The performance is evaluated on two locomotion tasks (HalfCheetah MT5 and MT8) using episode return as the metric, and on two manipulation tasks (MetaWorld MT10 and MT50) using success rate.  The table highlights the best performing explicit policy sharing method for each implicit method and also indicates the best overall combination of implicit and explicit methods.", "section": "5.2 Performance Improvement on Implicit Knowledge Sharing Approaches"}, {"figure_path": "3qUks3wrnH/tables/tables_16_1.jpg", "caption": "Table 1: Quantitative result of five classical implicit knowledge sharing approaches combined with different explicit policy sharing methods. The two HalfCheetah locomotion environments are measured on episode return, and the two MetaWorld manipulation environments are measured on success rate. We highlight the best-performing explicit policy sharing method in bold and annotate the best combination of two information sharing methods with boxes.", "description": "This table presents the quantitative results of experiments comparing five different implicit knowledge sharing methods (MTSAC, MHSAC, SM, PaCo, PCGrad) combined with two different explicit policy sharing methods (QMP, CTPG) across four different benchmark environments (HalfCheetah MT5, HalfCheetah MT8, MetaWorld MT10, and MetaWorld MT50).  The performance metrics are episode return for the HalfCheetah tasks and success rate for the MetaWorld tasks. The table highlights the best-performing explicit policy sharing method (QMP or CTPG) for each benchmark and also highlights the best overall combination of implicit and explicit policy sharing methods. ", "section": "5.2 Performance Improvement on Implicit Knowledge Sharing Approaches"}, {"figure_path": "3qUks3wrnH/tables/tables_20_1.jpg", "caption": "Table 1: Quantitative result of five classical implicit knowledge sharing approaches combined with different explicit policy sharing methods. The two HalfCheetah locomotion environments are measured on episode return, and the two MetaWorld manipulation environments are measured on success rate. We highlight the best-performing explicit policy sharing method in bold and annotate the best combination of two information sharing methods with boxes.", "description": "This table presents the quantitative results of combining five classical implicit knowledge sharing approaches (MTSAC, MHSAC, SM, PaCo, PCGrad) with two explicit policy sharing methods (QMP and CTPG).  It shows the performance on four benchmark environments: HalfCheetah MT5, HalfCheetah MT8, MetaWorld MT10, and MetaWorld MT50.  The performance metrics used are episode return (for HalfCheetah) and success rate (for MetaWorld). The table highlights the best-performing explicit policy sharing method for each benchmark and the best overall combination of implicit and explicit methods.", "section": "5.2 Performance Improvement on Implicit Knowledge Sharing Approaches"}, {"figure_path": "3qUks3wrnH/tables/tables_20_2.jpg", "caption": "Table 1: Quantitative result of five classical implicit knowledge sharing approaches combined with different explicit policy sharing methods. The two HalfCheetah locomotion environments are measured on episode return, and the two MetaWorld manipulation environments are measured on success rate. We highlight the best-performing explicit policy sharing method in bold and annotate the best combination of two information sharing methods with boxes.", "description": "This table presents the quantitative results of experiments comparing five different implicit knowledge sharing methods (MTSAC, MHSAC, SM, PaCo, PCGrad) combined with two different explicit policy sharing methods (QMP, CTPG). The performance is evaluated on two HalfCheetah locomotion environments (episode return) and two MetaWorld manipulation environments (success rate).  The table highlights the best performing explicit policy sharing method for each implicit method and shows the best combination of the two approaches.", "section": "5.2 Performance Improvement on Implicit Knowledge Sharing Approaches"}, {"figure_path": "3qUks3wrnH/tables/tables_20_3.jpg", "caption": "Table 1: Quantitative result of five classical implicit knowledge sharing approaches combined with different explicit policy sharing methods. The two HalfCheetah locomotion environments are measured on episode return, and the two MetaWorld manipulation environments are measured on success rate. We highlight the best-performing explicit policy sharing method in bold and annotate the best combination of two information sharing methods with boxes.", "description": "This table presents the quantitative results of experiments comparing five different implicit knowledge sharing approaches (MTSAC, MHSAC, SM, PaCo, PCGrad) combined with three different explicit policy sharing methods (Base, w/QMP, w/CTPG).  The performance is measured on two HalfCheetah locomotion environments (episode return) and two MetaWorld manipulation environments (success rate). The table highlights the best-performing explicit policy sharing method for each task and indicates the best overall combinations.", "section": "5.2 Performance Improvement on Implicit Knowledge Sharing Approaches"}, {"figure_path": "3qUks3wrnH/tables/tables_21_1.jpg", "caption": "Table 1: Quantitative result of five classical implicit knowledge sharing approaches combined with different explicit policy sharing methods. The two HalfCheetah locomotion environments are measured on episode return, and the two MetaWorld manipulation environments are measured on success rate. We highlight the best-performing explicit policy sharing method in bold and annotate the best combination of two information sharing methods with boxes.", "description": "This table presents the quantitative results of experiments comparing five different implicit knowledge sharing approaches combined with two explicit policy sharing methods (QMP and CTPG).  The experiments were conducted on two HalfCheetah locomotion environments (measured by episode return) and two MetaWorld manipulation environments (measured by success rate). The table highlights the best-performing explicit policy sharing method for each setup and also indicates the best combination of implicit and explicit methods.", "section": "5.2 Performance Improvement on Implicit Knowledge Sharing Approaches"}, {"figure_path": "3qUks3wrnH/tables/tables_21_2.jpg", "caption": "Table 1: Quantitative result of five classical implicit knowledge sharing approaches combined with different explicit policy sharing methods. The two HalfCheetah locomotion environments are measured on episode return, and the two MetaWorld manipulation environments are measured on success rate. We highlight the best-performing explicit policy sharing method in bold and annotate the best combination of two information sharing methods with boxes.", "description": "This table presents the quantitative results of experiments comparing different combinations of implicit and explicit knowledge sharing methods in multi-task reinforcement learning.  Five classical implicit knowledge sharing approaches are tested alongside two explicit policy sharing methods (QMP and CTPG).  The results are reported for two locomotion tasks (HalfCheetah-MT5 and HalfCheetah-MT8), and two manipulation tasks (MetaWorld-MT10 and MetaWorld-MT50).  The table highlights the best performing explicit policy sharing method for each setup and also indicates the best overall combination of implicit and explicit methods.", "section": "5.2 Performance Improvement on Implicit Knowledge Sharing Approaches"}, {"figure_path": "3qUks3wrnH/tables/tables_21_3.jpg", "caption": "Table 1: Quantitative result of five classical implicit knowledge sharing approaches combined with different explicit policy sharing methods. The two HalfCheetah locomotion environments are measured on episode return, and the two MetaWorld manipulation environments are measured on success rate. We highlight the best-performing explicit policy sharing method in bold and annotate the best combination of two information sharing methods with boxes.", "description": "This table presents the quantitative results of experiments comparing five different implicit knowledge sharing approaches (MTSAC, MHSAC, SM, PaCo, PCGrad) combined with three different explicit policy sharing methods (Base, w/ QMP, w/ CTPG).  The comparison is performed on two HalfCheetah locomotion tasks (MT5 and MT8), and two MetaWorld manipulation tasks (MT10 and MT50).  The metrics used are episode return (for HalfCheetah) and success rate (for MetaWorld).  The table highlights the best-performing explicit policy sharing method for each task and also indicates the best combination of implicit and explicit methods.", "section": "5.2 Performance Improvement on Implicit Knowledge Sharing Approaches"}, {"figure_path": "3qUks3wrnH/tables/tables_21_4.jpg", "caption": "Table 1: Quantitative result of five classical implicit knowledge sharing approaches combined with different explicit policy sharing methods. The two HalfCheetah locomotion environments are measured on episode return, and the two MetaWorld manipulation environments are measured on success rate. We highlight the best-performing explicit policy sharing method in bold and annotate the best combination of two information sharing methods with boxes.", "description": "This table presents the quantitative results of experiments that combine five classical implicit knowledge sharing approaches with two different explicit policy sharing methods (QMP and CTPG).  The experiments were conducted on two HalfCheetah locomotion environments (evaluated by episode return) and two MetaWorld manipulation environments (evaluated by success rate).  The table highlights the best-performing explicit policy sharing method for each scenario and also indicates the best overall combination of implicit and explicit methods.", "section": "5.2 Performance Improvement on Implicit Knowledge Sharing Approaches"}, {"figure_path": "3qUks3wrnH/tables/tables_21_5.jpg", "caption": "Table 1: Quantitative result of five classical implicit knowledge sharing approaches combined with different explicit policy sharing methods. The two HalfCheetah locomotion environments are measured on episode return, and the two MetaWorld manipulation environments are measured on success rate. We highlight the best-performing explicit policy sharing method in bold and annotate the best combination of two information sharing methods with boxes.", "description": "This table presents the quantitative results of experiments comparing five different implicit knowledge sharing methods (MTSAC, MHSAC, SM, PaCo, PCGrad) combined with two different explicit policy sharing methods (QMP and CTPG).  The table shows the performance of each combination on four different benchmark environments (HalfCheetah MT5, HalfCheetah MT8, MetaWorld MT10, and MetaWorld MT50).  The performance metrics used are episode return (for HalfCheetah) and success rate (for MetaWorld). The best-performing explicit policy sharing method for each implicit method is highlighted in bold, and the best overall combinations are indicated by boxes.", "section": "5.2 Performance Improvement on Implicit Knowledge Sharing Approaches"}]