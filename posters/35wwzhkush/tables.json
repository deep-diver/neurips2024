[{"figure_path": "35WwZhkush/tables/tables_3_1.jpg", "caption": "Table 1: Performance comparison between feed-forward and diffusion-based MDE. MFFD and MDM correspond to feed-forward and diffusion-based architectures, respectively. Dsyn and Dreal denote synthetic and real datasets, respectively. X(M, D) is the output distribution with a selected model M and training set D. Our goal is to approach the ideal distribution X(Mideal, Dideal) and achieve zero-shot MDE with precise details.", "description": "This table compares the performance of feed-forward and diffusion-based monocular depth estimation (MDE) models.  It highlights the trade-offs between detail extraction and zero-shot generalizability.  Feed-forward models (MFFD) trained on both synthetic (Dsyn) and real (Dreal) data achieve robust zero-shot performance but lack fine details. Diffusion-based models (MDM) trained on synthetic data (Dsyn) excel at extracting fine details but struggle with zero-shot generalization. The ideal model (Mideal) would combine the strengths of both, achieving both high detail and robust zero-shot performance with diverse data (Dideal). The table uses X(M,D) to represent the output distribution as a function of the model (M) and training data (D).", "section": "3 Problem Formulation"}, {"figure_path": "35WwZhkush/tables/tables_7_1.jpg", "caption": "Table 2: Quantitative evaluation of zero-shot performance with state-of-the-art affine-invariant MDE methods. #Train is the amount of training data. FFD and DM correspond to feed-forward and diffusion models. Metrics are shown in percentage with best and second-best results marked. The average rank cannot be computed for DepthFM due to missing metrics on ETH3D and ScanNet.", "description": "This table compares the zero-shot performance of BetterDepth against other state-of-the-art methods across five datasets (NYUv2, KITTI, ETH3D, ScanNet, DIODE).  It evaluates performance using AbsRel and 81 metrics, showing the relative error and percentage of accurate depth predictions.  The table highlights BetterDepth's superior performance, especially when trained on a smaller dataset.", "section": "4.2 Benchmarking"}, {"figure_path": "35WwZhkush/tables/tables_7_2.jpg", "caption": "Table 3: Plug-and-play experiments. BetterDepth directly works with CNN-based (MiDaS [31]) and transformer-based MDE models (DPT [30]), improving their results without re-training.", "description": "This table presents the results of plug-and-play experiments using BetterDepth with two different pre-trained models: MiDaS (CNN-based) and DPT (transformer-based).  It demonstrates that BetterDepth can improve the performance of these existing models without requiring further re-training, showcasing its plug-and-play functionality and versatility.", "section": "4.2 Benchmarking"}, {"figure_path": "35WwZhkush/tables/tables_7_3.jpg", "caption": "Table 4: Quantitative evaluation of detail extraction on Middlebury 2014 [41]. Edge-based metrics, i.e., the completeness and accuracy of depth boundaries (DBE_comp and DBE_acc) [20] and the edge precision and recall (EP and ER) [14], are also shown to evaluate performance specifically on high-frequency details. The best and second-best results are marked.", "description": "This table compares the performance of different depth estimation methods on the Middlebury 2014 dataset, focusing on detail extraction capabilities.  It uses standard metrics (AbsRel, \u03b41) along with edge-based metrics (DBE_comp, DBE_acc, EP, ER) to evaluate the accuracy and completeness of depth boundaries and fine details. The results highlight the ability of BetterDepth to accurately reconstruct fine-grained details compared to other methods.", "section": "4.2 Benchmarking"}, {"figure_path": "35WwZhkush/tables/tables_8_1.jpg", "caption": "Table 5: Ablation study. All variants are trained on the full 74K training pairs for 5K iterations. The best and second-best results are marked.", "description": "This table presents the ablation study results for the BetterDepth model. It shows the impact of each component of the model: depth conditioning, global pre-alignment, and local patch masking.  The table compares the performance (AbsRel and 81) on the NYUv2 and KITTI datasets for four different model variants, each excluding one of the components.  Variant #4 includes all three components and achieves the best performance, showcasing the contribution of each element to the overall results.", "section": "4.3 Ablation Study"}, {"figure_path": "35WwZhkush/tables/tables_14_1.jpg", "caption": "Table 2: Quantitative evaluation of zero-shot performance with state-of-the-art affine-invariant MDE methods. #Train is the amount of training data. FFD and DM correspond to feed-forward and diffusion models. Metrics are shown in percentage with best and second-best results marked. The average rank cannot be computed for DepthFM due to missing metrics on ETH3D and ScanNet.", "description": "This table compares the zero-shot performance of BetterDepth against other state-of-the-art monocular depth estimation (MDE) methods across five datasets (NYUv2, KITTI, ETH3D, ScanNet, DIODE).  It shows metrics (AbsRel and 81) for each method and dataset, indicating the amount of training data used for each method. The best and second-best results are highlighted for each metric.  It helps to understand the relative performance of BetterDepth in comparison to other MDE models in zero-shot settings. Note that DepthFM is excluded from average rank calculations due to missing data.", "section": "4.2 Benchmarking"}, {"figure_path": "35WwZhkush/tables/tables_14_2.jpg", "caption": "Table 4: Quantitative evaluation of detail extraction on Middlebury 2014 [41]. Edge-based metrics, i.e., the completeness and accuracy of depth boundaries (DBE_comp and DBE_acc) [20] and the edge precision and recall (EP and ER) [14], are also shown to evaluate performance specifically on high-frequency details. The best and second-best results are marked.", "description": "This table presents a quantitative comparison of detail extraction performance on the Middlebury 2014 dataset.  It compares several methods, including BetterDepth, using metrics such as absolute relative error (AbsRel), percentage of values within 1.25x of ground truth (\u03b41),  and edge-based metrics (DBE_comp, DBE_acc, EP, ER) to evaluate accuracy of details.  The best and second-best results are highlighted.", "section": "4.2 Benchmarking"}, {"figure_path": "35WwZhkush/tables/tables_15_1.jpg", "caption": "Table A3: Contribution of the geometric prior and the image prior in BetterDepth, where geometric and image priors correspond to the knowledge gained from the pre-trained depth model, i.e., Depth Anything [49], and the Stable Diffusion model [34]. The model without geometric prior uses the same network and fine-tuning method as Marigold [17] but estimates inverse depth (following Depth Anything [49]) instead of relative depth. For the model without image prior, we follow Stable Diffusion [34] to train the latent UNet from scratch and keep the pre-trained VAE unchanged. Metrics are shown in percentage terms, where the best and second-best results are marked.", "description": "This table shows the ablation study results on the contribution of geometric and image priors in BetterDepth.  It compares the performance of BetterDepth with and without each prior, demonstrating their individual and combined impact on depth estimation accuracy across several metrics. The results highlight the importance of incorporating both types of prior knowledge for optimal performance.", "section": "More BetterDepth Variants"}, {"figure_path": "35WwZhkush/tables/tables_15_2.jpg", "caption": "Table 2: Quantitative evaluation of zero-shot performance with state-of-the-art affine-invariant MDE methods. #Train is the amount of training data. FFD and DM correspond to feed-forward and diffusion models. Metrics are shown in percentage with best and second-best results marked. The average rank cannot be computed for DepthFM due to missing metrics on ETH3D and ScanNet.", "description": "This table compares the zero-shot performance of BetterDepth against other state-of-the-art methods on five datasets (NYUv2, KITTI, ETH3D, ScanNet, DIODE).  It shows quantitative metrics (AbsRel\u2193 and 81\u2191) for different models, categorized as feed-forward (FFD) or diffusion (DM) models.  The table highlights BetterDepth's superior performance, especially with limited training data, by showing its rank compared to other methods.", "section": "4.2 Benchmarking"}, {"figure_path": "35WwZhkush/tables/tables_16_1.jpg", "caption": "Table 2: Quantitative evaluation of zero-shot performance with state-of-the-art affine-invariant MDE methods. #Train is the amount of training data. FFD and DM correspond to feed-forward and diffusion models. Metrics are shown in percentage with best and second-best results marked. The average rank cannot be computed for DepthFM due to missing metrics on ETH3D and ScanNet.", "description": "This table compares the zero-shot performance of BetterDepth with other state-of-the-art monocular depth estimation methods on five benchmark datasets (NYUv2, KITTI, ETH3D, ScanNet, DIODE).  It evaluates both feed-forward (FFD) and diffusion (DM) models, considering the amount of training data used.  Metrics such as AbsRel and \u03b41 are presented, with the best and second-best results highlighted.  The table indicates BetterDepth's superior performance across various datasets, even with less training data.", "section": "4.2 Benchmarking"}]