[{"figure_path": "dqdffX3BS5/figures/figures_2_1.jpg", "caption": "Figure 1: Overview of the Mecoin framework for GFSCIL. (a)Graph neural network: Consists of a GNN encoder and a classifier(MLP) pre-trained by GNN. In GFSCIL tasks, the encoder parameters are frozen. (b)Structured Memory Unit: Constructs class prototypes through MeCs and stores them in SMU. (c)Memory Representation Adaptive Module: Facilitates adaptive knowledge interaction with the GNN model.", "description": "This figure shows the architecture of the Mecoin framework for graph few-shot class-incremental learning (GFSCIL). It consists of three main modules: a graph neural network (GNN) encoder and classifier, a structured memory unit (SMU), and a memory representation adaptive module (MRaM).  The GNN processes the graph data, while the SMU maintains and updates class prototypes using the Memory Construction Module (MeCs). The MRaM facilitates adaptive knowledge interaction with the GNN through the Graph Knowledge Interaction Module (GKIM), allowing the model to learn from limited samples and retain prior knowledge.", "section": "Efficient Memory Construction and Interaction Module"}, {"figure_path": "dqdffX3BS5/figures/figures_6_1.jpg", "caption": "Figure 1: Overview of the Mecoin framework for GFSCIL. (a)Graph neural network: Consists of a GNN encoder and a classifier(MLP) pre-trained by GNN. In GFSCIL tasks, the encoder parameters are frozen. (b)Structured Memory Unit: Constructs class prototypes through MeCs and stores them in SMU. (c)Memory Representation Adaptive Module: Facilitates adaptive knowledge interaction with the GNN model.", "description": "This figure illustrates the architecture of the Mecoin framework for Graph Few-Shot Class-Incremental Learning (GFSCIL). It shows the interaction between a Graph Neural Network (GNN), a Structured Memory Unit (SMU), and a Memory Representation Adaptive Module (MRaM). The GNN is pre-trained and its encoder parameters are frozen during GFSCIL. The SMU uses a Memory Construction Module (MeCs) to create and store class prototypes. The MRaM facilitates the interaction between the GNN and the SMU, enabling adaptive knowledge transfer.", "section": "Efficient Memory Construction and Interaction Module"}, {"figure_path": "dqdffX3BS5/figures/figures_8_1.jpg", "caption": "Figure 3: The outcomes of GKIM when conducting the few-shot continuous learning task on the CoraFull, Computers and CS datasets. The results are presented sequentially from left to right: GKIM with full capabilities, GKIM where node features do not interact with class prototypes in the SMU, GKIM without GraphInfo and GKIM without MeCs. The experimental results for CoraFull are shown in the above figure, the results for Computers are in the middle and the results for CS are in the figure below.", "description": "This figure shows the results of ablation experiments on GKIM. Four different scenarios are compared:\n1. GKIM with all features enabled.\n2. GKIM without node feature interaction with class prototypes in the SMU.\n3. GKIM without GraphInfo.\n4. GKIM without the MeCs module.\nThe results are shown separately for three different datasets: CoraFull, Computers, and CS. The plots visualize the performance in terms of accuracy and forgetting rate.", "section": "4.3 GKIM for Memory (Q3)"}, {"figure_path": "dqdffX3BS5/figures/figures_9_1.jpg", "caption": "Figure 4: Left 2 columns: Line charts depict the performance of models across various sessions on the CoraFull and CS datasets when using different distillation methods; Right 2 columns: Histograms illustrate the forgetting rates of different distillation methods on these two datasets.", "description": "This figure shows the performance and forgetting rates of different methods across multiple sessions on two datasets. The left two columns display line charts illustrating the accuracy of each method over time, while the right two columns present histograms depicting their forgetting rates.  The comparison helps to visualize how effectively each method maintains past knowledge while learning new information.", "section": "4.3 GKIM for Memory (Q3)"}, {"figure_path": "dqdffX3BS5/figures/figures_12_1.jpg", "caption": "Figure 5: From left to right are the results of GKIM, without using GraphInfo, node features not interacting with class prototypes in SMU and without using MeCs, when performing the graph small-sample continuous learning task in the Computers dataset, four randomly selected categories from session1 and 400 randomly selected samples from the four categories are clustered at the class center of the class prototypes bit class centers obtained from the learning during the training process.", "description": "This figure shows the results of an ablation study on the GKIM model for graph few-shot continual learning. Four different versions of the model are tested on the Computers dataset: the full GKIM model, a version without GraphInfo, a version where node features do not interact with class prototypes, and a version without MeCs.  The results show that all components contribute to the model's performance.  The visualization displays the clustering of 400 samples from four randomly selected categories, highlighting the differences in prototype representation and class boundary separation among the various model versions.", "section": "A Ablation Experiments"}, {"figure_path": "dqdffX3BS5/figures/figures_13_1.jpg", "caption": "Figure 3: The outcomes of GKIM when conducting the few-shot continuous learning task on the CoraFull, Computers and CS datasets. The results are presented sequentially from left to right: GKIM with full capabilities, GKIM where node features do not interact with class prototypes in the SMU, GKIM without GraphInfo and GKIM without MeCs. The experimental results for CoraFull are shown in the above figure, the results for Computers are in the middle and the results for CS are in the figure below.", "description": "This figure shows the ablation study results on the performance of GKIM under different configurations.  The leftmost column shows the results of GKIM with all features enabled (MeCs, GraphInfo, interaction between node features and class prototypes).  Subsequent columns remove one component at a time: no interaction between node features and class prototypes, no GraphInfo, and finally no MeCs.  Each row represents results on a different dataset (CoraFull, Computers, CS). The plots visualize the resulting accuracy and forgetting rate (PD) for each session.", "section": "4.3 GKIM for Memory (Q3)"}, {"figure_path": "dqdffX3BS5/figures/figures_23_1.jpg", "caption": "Figure 3: The outcomes of GKIM when conducting the few-shot continuous learning task on the CoraFull, Computers and CS datasets. The results are presented sequentially from left to right: GKIM with full capabilities, GKIM where node features do not interact with class prototypes in the SMU, GKIM without GraphInfo and GKIM without MeCs. The experimental results for CoraFull are shown in the above figure, the results for Computers are in the middle and the results for CS are in the figure below.", "description": "This figure shows the ablation study results of the GKIM module in the Mecoin framework.  It presents four variations of the GKIM's performance across three datasets (CoraFull, Computers, CS) under different conditions. The four conditions tested are: GKIM with all features enabled, GKIM without interaction between node features and class prototypes (No Inter), GKIM without local graph structure information (No GraphInfo), and GKIM without the Memory Construction Module (MeCs, which is renamed from MeCo in the original paper). The results are visualized in a two-dimensional space, illustrating differences in accuracy and forgetting rate across these conditions.", "section": "4.3 GKIM for Memory (Q3)"}, {"figure_path": "dqdffX3BS5/figures/figures_23_2.jpg", "caption": "Figure 3: The outcomes of GKIM when conducting the few-shot continuous learning task on the CoraFull, Computers and CS datasets. The results are presented sequentially from left to right: GKIM with full capabilities, GKIM where node features do not interact with class prototypes in the SMU, GKIM without GraphInfo and GKIM without MeCs. The experimental results for CoraFull are shown in the above figure, the results for Computers are in the middle and the results for CS are in the figure below.", "description": "This figure shows the results of an ablation study on the GKIM component of the Mecoin framework. Four variations of GKIM are tested:  (1) GKIM with all features enabled, (2) GKIM where node features do not interact with class prototypes, (3) GKIM without local graph structure information, and (4) GKIM without the MeCs module. The results are shown for three datasets: CoraFull, Computers, and CS.  Each plot displays the results for a specific dataset and GKIM configuration, illustrating the model's performance and highlighting the contribution of each component of GKIM.", "section": "4.3 GKIM for Memory (Q3)"}, {"figure_path": "dqdffX3BS5/figures/figures_24_1.jpg", "caption": "Figure 1: Overview of the Mecoin framework for GFSCIL. (a)Graph neural network: Consists of a GNN encoder and a classifier(MLP) pre-trained by GNN. In GFSCIL tasks, the encoder parameters are frozen. (b)Structured Memory Unit: Constructs class prototypes through MeCs and stores them in SMU. (c)Memory Representation Adaptive Module: Facilitates adaptive knowledge interaction with the GNN model.", "description": "This figure provides a high-level overview of the Mecoin framework, illustrating its key components for graph few-shot class-incremental learning (GFSCIL).  It shows how a pre-trained graph neural network (GNN) encoder interacts with the Mecoin's modules. The Structured Memory Unit (SMU) constructs and stores class prototypes using a Memory Construction Module (MeCs), and the Memory Representation Adaptive Module (MRaM) dynamically manages knowledge interaction with the GNN to avoid catastrophic forgetting.", "section": "Efficient Memory Construction and Interaction Module"}]