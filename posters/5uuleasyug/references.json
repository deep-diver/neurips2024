{"references": [{"fullname_first_author": "Ao, T.", "paper_title": "Rhythmic gesticulator: Rhythm-aware co-speech gesture synthesis with hierarchical neural embeddings", "publication_date": "2022-07-01", "reason": "This paper is highly relevant due to its focus on rhythm-aware co-speech gesture synthesis, a central theme of the current research."}, {"fullname_first_author": "Baevski, A.", "paper_title": "wav2vec 2.0: A framework for self-supervised learning of speech representations", "publication_date": "2020-12-01", "reason": "This is a foundational paper in self-supervised speech representation learning, a crucial aspect of the current work's audio processing."}, {"fullname_first_author": "Gu, A.", "paper_title": "Mamba: Linear-time sequence modeling with selective state spaces", "publication_date": "2023-12-01", "reason": "The Mamba model is the core of the proposed method, making this a primary source for the current work's theoretical foundation."}, {"fullname_first_author": "Liu, H.", "paper_title": "Towards unified holistic co-speech gesture generation via masked audio gesture modeling", "publication_date": "2024-01-01", "reason": "The EMAGE model is a state-of-the-art holistic gesture generation model, serving as a major benchmark and comparison point for the presented work."}, {"fullname_first_author": "Yoon, Y.", "paper_title": "Speech gesture generation from the trimodal context of text, audio, and speaker identity", "publication_date": "2020-12-01", "reason": "This paper offers a valuable comparative point, representing a significant advancement in co-speech gesture generation using RNNs."}]}