[{"figure_path": "JkqrZqBO7d/tables/tables_4_1.jpg", "caption": "Table 1: Comparisons with other methods in an ideal setting for one stage. We compare several methods to compute a gradient estimate in a model parallel setting. Here, J is the total number of stages while j is the stage index. For the sake of simplicity, we assume that a backward pass requires approximately 2 times more FLOPs than a forward pass. Full Graph indicates that it is required to store the full computational graph of a local forward pass. With a limited increase in communication volume and FLOPs, PETRA requires the least storage of all methods while being linearly faster than backpropagation. We assume that the forward and backward passes can be executed in parallel for PETRA or delayed gradients, making the backward pass responsible for most of the computation time in parallelizable approaches.", "description": "This table compares the memory usage, communication overhead, computational cost (FLOPs), and average time per batch for various gradient computation methods (backpropagation, reversible backpropagation, delayed gradients with checkpointing, and PETRA) in a model-parallel setting.  It highlights PETRA's efficiency in terms of reduced memory requirements and faster computation time compared to other methods.", "section": "3.3 A parallelizable approach: PETRA"}, {"figure_path": "JkqrZqBO7d/tables/tables_7_1.jpg", "caption": "Table 2: Classification accuracies using our PETRA method with RevNets, compared to standard backpropagation on ResNets and RevNets on CIFAR-10, ImageNet32, and ImageNet. Our method delivers competitive results with backpropagation, even on ImageNet.", "description": "This table compares the classification accuracy of the PETRA method using reversible residual networks (RevNets) against standard backpropagation using both ResNets and RevNets. The comparison is made across three benchmark datasets: CIFAR-10, ImageNet32, and ImageNet.  The table shows that PETRA achieves competitive accuracy compared to backpropagation, even on the large ImageNet dataset.  For each model and dataset, the table lists the number of parameters, and the accuracy achieved by each method.", "section": "4 Numerical experiments"}, {"figure_path": "JkqrZqBO7d/tables/tables_8_1.jpg", "caption": "Table 3: Memory savings for RevNet50 on ImageNet with our method for different configurations. We indicate the use of memory buffers for inputs or parameters. The savings are computed with respect to the first configuration, where inputs and buffers are stored. Our method achieves 54.3% memory reduction over the base configuration of Delayed Gradients.", "description": "This table compares memory usage (in GB) and savings (%) for different configurations of memory buffers used in training RevNet50 on the ImageNet dataset.  The configurations vary in whether input and parameter buffers are used. The baseline is using both input and parameter buffers, showing the memory savings achieved by reducing or removing these buffers.  The results demonstrate significant memory savings when using PETRA.", "section": "4.2 Technical details"}]