[{"figure_path": "ZfBuhzE556/tables/tables_7_1.jpg", "caption": "Table 1: Win rate comparison of Pythia-410M, -1.4B, and -2.8B models on the Anthropic HH dataset, evaluated using GPT-4.", "description": "This table presents the win rates achieved by different methods on the Anthropic HH dataset using GPT-4 for evaluation.  The methods compared are standard DPO, DPO with dynamic beta, DPO with data filtering, and the proposed B-DPO method. The win rates are shown for three different sizes of Pythia models (410M, 1.4B, and 2.8B). Percentage improvements compared to the standard DPO are also indicated for each of the other methods.  The table highlights the superior performance of B-DPO across all model sizes.", "section": "5.1 Empirical Evaluation of B-DPO on Dialogue Generation and Summarization"}, {"figure_path": "ZfBuhzE556/tables/tables_9_1.jpg", "caption": "Table 2: Comparison of win rates across varying mixture ratios on the Anthropic HH dataset, with each ratio indicating the proportion of high-gap to low-gap datasets, e.g., a 40% mixture ratio reflects a blend of 40% high-gap and 60% low-gap.", "description": "This table presents the win rates achieved by three different DPO methods across different ratios of high-gap and low-gap data in the Anthropic HH dataset.  The \"Vanilla DPO\" row shows the performance of the standard DPO method.  The next row shows the performance when instance-level beta calibration is used, and the final row shows the performance when batch-level beta calibration is applied.  The percentage changes compared to the Vanilla DPO are shown in parentheses, with positive values representing improvements and negative values representing decreases in performance.", "section": "5.3 Necessity of Batch-Level Dynamic \u03b2 Calibration"}, {"figure_path": "ZfBuhzE556/tables/tables_9_2.jpg", "caption": "Table 1: Win rate comparison of Pythia-410M, -1.4B, and -2.8B models on the Anthropic HH dataset, evaluated using GPT-4.", "description": "This table presents the win rates achieved by four different methods (DPO, DPO with dynamic beta, DPO with data filtering, and beta-DPO) on three different sized language models (Pythia-410M, Pythia-1.4B, and Pythia-2.8B) when evaluated using GPT-4 on the Anthropic HH dataset.  The win rate represents the percentage of times the model's response was preferred to the baseline response by GPT-4.", "section": "5.1 Empirical Evaluation of \u03b2-DPO on Dialogue Generation and Summarization"}, {"figure_path": "ZfBuhzE556/tables/tables_13_1.jpg", "caption": "Table 1: Win rate comparison of Pythia-410M, -1.4B, and -2.8B models on the Anthropic HH dataset, evaluated using GPT-4.", "description": "This table presents the win rates achieved by different methods on the Anthropic HH dataset using the GPT-4 model for evaluation.  The methods compared include the standard DPO, DPO with dynamic beta, DPO with data filtering, and the proposed B-DPO. Win rate is a metric indicating how often GPT-4 prefers the model's response over the default response. The table shows the win rates for three different sizes of Pythia models (410M, 1.4B, and 2.8B parameters) to demonstrate performance across different model scales.", "section": "5.1 Empirical Evaluation of B-DPO on Dialogue Generation and Summarization"}, {"figure_path": "ZfBuhzE556/tables/tables_15_1.jpg", "caption": "Table 4: Comparison of win rates across varying Mo in \u03b2-DPO.", "description": "This table presents the win rates achieved by the \u03b2-DPO method across different values of the hyperparameter Mo.  Mo is a threshold used in the dynamic \u03b2 calibration strategy.  The results show that using a moving average to update Mo (last column) leads to the highest win rate, indicating that this approach is superior to using a fixed value of Mo.", "section": "5.2 Adaptations of \u03b2-DPO"}, {"figure_path": "ZfBuhzE556/tables/tables_15_2.jpg", "caption": "Table 1: Win rate comparison of Pythia-410M, -1.4B, and -2.8B models on the Anthropic HH dataset, evaluated using GPT-4.", "description": "This table presents the win rates achieved by different methods (DPO, DPO with dynamic beta, DPO with data filtering, and Beta-DPO) on the Anthropic HH dataset using three different sizes of Pythia models (410M, 1.4B, and 2.8B).  The win rate is a measure of how often GPT-4 prefers the response generated by the model over the default chosen response.  The table shows the improvement in win rate achieved by Beta-DPO compared to the other methods across different model sizes. ", "section": "5.1 Empirical Evaluation of B-DPO on Dialogue Generation and Summarization"}]