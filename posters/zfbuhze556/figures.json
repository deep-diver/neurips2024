[{"figure_path": "ZfBuhzE556/figures/figures_1_1.jpg", "caption": "Figure 1: (1a) Pairwise Data: Low vs. High Gap: \u201cLow gap\u201d denotes cases where the chosen and rejected examples are closely similar, typically indicating high-quality, informative pairs. \u201cHigh gap\u201d signifies pairs with larger differences, implying lower-quality data. (1b) Influence of Data Quality on \u03b2 Selection: Pythia-1.4B\u2019s performance on the HH dataset reveals a distinct trend: for \u201cLow gap\u201d, a higher \u03b2 reduces win rate, whereas for \u201cHigh gap\u201d, an increased \u03b2 improves it.", "description": "This figure shows the impact of data quality and the hyperparameter \u03b2 on the performance of Direct Preference Optimization (DPO).  Part (a) illustrates two types of data pairs: 'low gap' (similar chosen and rejected responses) and 'high gap' (dissimilar chosen and rejected responses). Part (b) displays the win rate (model performance) for different values of \u03b2 under both low and high gap conditions, demonstrating that optimal \u03b2 values depend on the informativeness of the data.", "section": "4.1 Motivation: The Impact of Pairwise Data Quality on \u03b2 Selection"}, {"figure_path": "ZfBuhzE556/figures/figures_4_1.jpg", "caption": "Figure 2: Win rate performance of DPO across different \u03b2 settings on the low gap, mixed gap, and high gap datasets.", "description": "This figure shows the results of an experiment evaluating the effect of different beta values on the win rate of Direct Preference Optimization (DPO) across three different types of datasets: low gap, mixed gap, and high gap.  The low gap dataset contains pairs of similar responses where a subtle difference is present, whereas the high gap contains pairs of greatly dissimilar responses. The mixed gap is a combination of both.  Each line represents the win rate of a specific DPO model (Pythia-410M, Pythia-1.4B, and Pythia-2.8B) at various beta values.  The results show that the optimal beta value depends on the type of dataset, highlighting the importance of adapting beta based on data quality.", "section": "4.1 Motivation: The Impact of Pairwise Data Quality on \u03b2 Selection"}, {"figure_path": "ZfBuhzE556/figures/figures_4_2.jpg", "caption": "Figure 3: The distribution of individual reward discrepancy (r(yw; x(i)) - r(y\u03b9; x(i))) on the training dataset of HH.", "description": "This figure shows the distribution of reward discrepancies for each data point in the Anthropic HH dataset.  The reward discrepancy is calculated as the difference between the reward for the preferred response (yw) and the reward for the rejected response (y\u03b9) for a given prompt (x). The distribution is shown as a violin plot, with the central black bar representing the median and quartiles, and the shape of the violin showing the probability density of the discrepancies.  The plot also includes dashed lines indicating the mean and the 10th and 90th percentiles of the distribution.  This visualization highlights the presence of outliers in the dataset, indicated by the tails of the distribution extending beyond the 10th and 90th percentiles, suggesting some samples have significantly higher or lower reward discrepancies than most.", "section": "4.1 Motivation: The Impact of Pairwise Data Quality on \u03b2 Selection"}, {"figure_path": "ZfBuhzE556/figures/figures_6_1.jpg", "caption": "Figure 4: Left. The win rates computed by GPT-4 evaluations for the Anthropic-HH one-step dialogue; \u03b2-DPO consistently outperforms across all sampling temperatures. Right. In the comparison of TL;DR summarization win rates versus chosen summaries with GPT-4 as the evaluator, \u03b2-DPO is distinguished as the only strategy achieving a win rate over 50% across different sampling temperatures.", "description": "This figure displays the results of comparing different DPO methods across two tasks: Anthropic HH dialogue and TL;DR summarization. The left panel shows that \u03b2-DPO consistently achieves higher win rates compared to other methods for the dialogue task across various sampling temperatures. The right panel shows that \u03b2-DPO outperforms other methods, being the only method with a win rate above 50% for the summarization task across various sampling temperatures.", "section": "5.1 Empirical Evaluation of \u03b2-DPO on Dialogue Generation and Summarization"}, {"figure_path": "ZfBuhzE556/figures/figures_8_1.jpg", "caption": "Figure 5: Left: Win rates from GPT-4 evaluations on Anthropic-HH single-turn dialogues, showcasing  \u03b2-DPO\u2019s adaptability to diverse filtering strategies. Middle: Win rates of \u03b2-DPO across various DPO variants as evaluated by GPT-4. Right: Distribution of individual reward discrepancies following fine-tuning through batch-level and instance-level calibration.", "description": "This figure presents a comparison of the performance of the proposed \u03b2-DPO method against several baselines across three different aspects: filtering strategies, DPO variants, and calibration methods.  The left panel shows win rates for different filtering methods (Filter Tail, Filter Head, Filter Tail & Head). The middle panel shows win rates for different DPO variants (DPO, IPO, KTO, SPPO) with and without the dynamic \u03b2 adjustment. The right panel displays the distribution of reward discrepancies obtained using batch-level and instance-level calibration, highlighting the impact of outlier handling.", "section": "5.2 Adaptations of \u03b2-DPO"}, {"figure_path": "ZfBuhzE556/figures/figures_14_1.jpg", "caption": "Figure 6: Distribution of individual reward discrepancies following the Pythia-2.8B model.", "description": "This violin plot shows the distribution of individual reward discrepancies for different mixture ratios of low-gap and high-gap datasets using the Pythia-2.8B model.  The x-axis represents the mixture ratio (proportion of high-gap data), ranging from 0.1 to 1.0. The y-axis represents the individual reward discrepancy, which reflects the difference in reward scores between the preferred and dispreferred responses in a pair.  The plot reveals that as the mixture ratio increases (more high-gap data), the distribution of reward discrepancies becomes wider, indicating greater variability in the data quality. Conversely, a lower mixture ratio results in a more concentrated distribution, suggesting higher quality and more consistent data.", "section": "4.1 Motivation: The Impact of Pairwise Data Quality on \u03b2 Selection"}, {"figure_path": "ZfBuhzE556/figures/figures_14_2.jpg", "caption": "Figure 7: Performance comparison across different \u03b2 values and \u03c1 values for three different model sizes (Pythia-2.8B, Pythia-1.4B, and Pythia-410M) on the Anthropic HH dataset using GPT-4 as the evaluator. Each subplot represents the win rate for varying parameters \u03b2 = 0.1, 0.3, and 0.9 with exponential smoothing.", "description": "This figure displays the win rates achieved by three different sized language models (Pythia 410M, Pythia 1.4B, and Pythia 2.8B) on the Anthropic HH dataset when varying two hyperparameters: \u03b2 and \u03c1.  The plots show how the win rate changes depending on the values of \u03b2 (a trade-off parameter in the DPO algorithm) and \u03c1 (the filtering ratio). Each subplot shows the performance for a specific model size. Exponential smoothing is applied to the win rates.", "section": "4.3 Method: Dynamic \u03b2 Calibration in DPO"}]