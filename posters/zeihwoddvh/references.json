{"references": [{"fullname_first_author": "T. Gu", "paper_title": "Badnets: Identifying vulnerabilities in the machine learning model supply chain", "publication_date": "2017-08-00", "reason": "This paper is foundational for the field of backdoor attacks, introducing the concept and providing a baseline for future research."}, {"fullname_first_author": "J. Geiping", "paper_title": "Witches' brew: Industrial scale data poisoning via gradient matching", "publication_date": "2021-00-00", "reason": "This paper introduces a novel and highly effective data poisoning attack, Gradient Matching, which poses a significant challenge to existing defense mechanisms."}, {"fullname_first_author": "Y. Zeng", "paper_title": "Narcissus: A practical clean-label backdoor attack with limited information", "publication_date": "2022-04-00", "reason": "This paper presents a state-of-the-art clean-label backdoor attack, Narcissus, which is particularly challenging due to its minimal reliance on prior knowledge."}, {"fullname_first_author": "H. Aghakhani", "paper_title": "Bullseye polytope: A scalable clean-label poisoning attack with improved transferability", "publication_date": "2021-00-00", "reason": "This paper introduces a new clean-label poisoning attack, Bullseye Polytope, that exhibits high transferability across different datasets and models."}, {"fullname_first_author": "C.-H. Yuan", "paper_title": "Neural tangent generalization attacks", "publication_date": "2021-07-18", "reason": "This paper introduces a novel data poisoning attack that leverages neural tangent kernels, posing a unique challenge to conventional defense methods."}]}