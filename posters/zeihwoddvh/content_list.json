[{"type": "text", "text": "PureGen: Universal Data Purification for Train-Time Poison Defense via Generative Model Dynamics ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Sunay Bhat \u2217 Jeffrey Jiang sunaybhat1@ucla.edu jeffrey.jiang@ucla.edu ", "page_idx": 0}, {"type": "text", "text": "Omead Pooladzandi Alexander Branch Gregory Pottie opooladz@caltech.edu alexrbranch@ucla.edu pottie@ee.ucla.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Train-time data poisoning attacks threaten machine learning models by introducing adversarial examples during training, leading to misclassification. Current defense methods often reduce generalization performance, are attack-specific, and impose significant training overhead. To address this, we introduce a set of universal data purification methods using a stochastic transform, $\\Psi(x)$ , realized via iterative Langevin dynamics of Energy-Based Models (EBMs), Denoising Diffusion Probabilistic Models (DDPMs), or both. These approaches purify poisoned data with minimal impact on classifier generalization. Our specially trained EBMs and DDPMs provide state-of-the-art defense against various attacks (including Narcissus, Bullseye Polytope, Gradient Matching) on CIFAR-10, Tiny-ImageNet, and CINIC-10, without needing attack or classifier-specific information. We discuss performance trade-offs and show that our methods remain highly effective even with poisoned or distributionally shifted generative model training data. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large datasets enable modern deep learning models but are vulnerable to data poisoning, where adversaries inject imperceptible poisoned images to manipulate model behavior at test time. Poisons can be created with or without knowledge of the model\u2019s architecture or training settings. As deep learning models grow in capability and usage, securing them against such attacks while preserving accuracy is critical. ", "page_idx": 0}, {"type": "text", "text": "Numerous methods of poisoning deep learning systems to create backdoors have been proposed in recent years. These disruptive techniques typically fall into two distinct categories: explicit backdoor, triggered data poisoning, or triggerless poisoning attacks. Triggered attacks conceal an imperceptible trigger pattern in the samples of the training data leading to the misclassification of test-time samples containing the hidden trigger [1, 2, 3, 4]. In contrast, triggerless poisoning attacks involve introducing slight, bounded perturbations to individual images that align them with target images of another class within the feature or gradient space resulting in the misclassification of specific instances without necessitating further modification during inference [5, 6, 7, 8, 9]. Alternatively, data availability attacks pose a training challenge by preventing model learning at train time, but do not introduce any latent backdoors that can be exploited at inference time [10, 11, 12]. In all these scenarios, poisoned examples often appear benign and correctly labeled making them challenging for observers or algorithms to detect. ", "page_idx": 0}, {"type": "image", "img_path": "ZeihWodDVh/tmp/b9ad8d7b1a96052fcf0d4d146874de309fe6c031286e4d1ad07973128c88e3b7.jpg", "img_caption": ["Figure 1: Top The full PUREGEN pipeline is shown where we apply our method as a preprocessing step with no further downstream changes to the classifier training or inference. Poisoned images are moderately exaggerated to show visually. Bottom Left Energy distributions of clean, poisoned, and PUREGEN purified images. Our methods push poisoned images via purification into the natural,clean image energy manifold. Bottom Right The removal of poison artifacts and the similarity of clean and poisoned images after purification using PUREGEN EBM and DDPM dynamics. The purified dataset results in SoTA defense and high classifier natural accuracy. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Current defense strategies against data poisoning exhibit significant limitations. While some methods rely on anomaly detection through techniques such as nearest neighbor analysis, training loss minimization, singular-value decomposition, feature activation or gradient clustering [13, 14, 15, 16, 17, 18, 19], others resort to robust training strategies including data augmentation, randomized smoothing, ensembling, adversarial training and maximal noise augmentation [20, 21, 22, 23, 24, 25, 26]. However, these approaches either undermine the model\u2019s generalization performance [27, 18], offer protection only against specific attack types [27, 17, 15], or prove computationally prohibitive for standard deep learning workflows [22, 16, 28, 18, 27, 17, 26]. There remains a critical need for more effective and practical defense mechanisms in the realm of deep learning security. ", "page_idx": 1}, {"type": "text", "text": "Generative models have been used for robust/adversarial training, but not for train-time backdoor attacks, to the best of our knowledge. Recent works have demonstrated the effectiveness of both EBM dynamics and Diffusion models to purify datasets against inference or availability attacks [29, 30, 31], but train-time backdoor attacks present additional challenges in both evaluation and application, requiring training using Public Out-of-Distribution (POOD) datasets and methods to avoid cumbersome computation or setup for classifier training. ", "page_idx": 1}, {"type": "text", "text": "We propose PUREGEN, a set of powerful stochastic preprocessing defense techniques, $\\Psi_{T}(x)$ , against train-time poisoning attacks. PUREGEN-EBM uses EBM-guided Markov Chain Monte Carlo (MCMC) sampling to purify poisoned images, while PUREGEN-DDPM uses a limited forward/reverse diffusion process, specifically for purification. Training DDPM models on a subset of the noise schedule improves purification by dedicating more model capacity to \u2018restoration\u2019 rather than generation. We further find that the energy of poisoned images is significantly higher than the baseline images, for a trained EBM, and PUREGEN techniques move poisoned samples to a lowerenergy, natural data manifold with minimal accuracy loss. The PUREGEN pipeline, sample energy distributions, and purification on a sample image can be seen in Figure 1.PUREGEN significantly outperforms current defenses in all tested scenarios. Our key contributions in this work are as follows. ", "page_idx": 1}, {"type": "text", "text": "\u2022 A set of state-of-the-art (SoTA) stochastic preprocessing defenses $\\Psi(x)$ against adversarial poisons using MCMC dynamics of EBMs and DDPMs trained specifically for purification ", "page_idx": 1}, {"type": "text", "text": "named PUREGEN-EBM and PUREGEN-DDPM with analysis providing further intuition on effectiveness   \n\u2022 Experimental results showing the broad application of $\\Psi(x)$ with minimal tuning and no prior knowledge needed of the poison type and classification model   \n\u2022 Results showing SoTA performance can be maintained even when PUREGEN models\u2019 training data includes poisons or is from a significantly different distribution than the classifier/attacked train data distribution   \n\u2022 Results showing even further performance gains from combinations of PUREGEN-EBM and PUREGEN-DDPM and robustness to defense-aware poisons ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Targeted Data Poisoning Attack ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Poisoning of a dataset occurs when an attacker injects small adversarial perturbations $\\delta$ (where $\\|\\delta\\|_{\\infty}\\leq\\bar{\\xi}$ and typically $\\xi\\,=\\,8$ or 16/255) into a small fraction, $\\alpha$ , of training images, making poisoning incredibly difficult to detect. These train-time attacks introduce local sharp regions with a considerably higher training loss [26]. A successful attack occurs when, after SGD optimizes the cross-entropy training objective on these poisoned datasets, invisible backdoor vulnerabilities are baked into a classifier, without a noticeable change in overall test accuracy. This is in contrast to inference-time or other adversarial scenarios where an attacker might be defense or model-aware. The goal in train-time attacks is \"stealth\" via minimal impact to the dataset and training and testing curves while creating backdoors to exploit at deployment. ", "page_idx": 2}, {"type": "text", "text": "In the realm of deep network poison security, we encounter two primary categories of attacks: triggered and triggerless attacks. Triggered attacks, often referred to as backdoor attacks, involve contaminating a limited number of training data samples with a specific trigger (often a patch) $\\rho$ (similarly constrained $\\|\\rho\\|_{\\infty}\\leq\\xi)$ ) that corresponds to a target label, $\\bar{y}^{\\mathrm{adv}}$ . After training, a successful backdoor attack misclassifies when the perturbation $\\rho$ is added: ", "page_idx": 2}, {"type": "equation", "text": "$$\nF(x)={\\binom{y}{y^{\\mathrm{adv}}}}\\quad x\\in\\{x:(x,y)\\in{\\mathcal{D}}_{t e s t}\\}}\\\\ {y^{\\mathrm{adv}}}&{x\\in\\{x+\\rho:(x,y)\\in{\\mathcal{D}}_{t e s t},y\\neq y^{\\mathrm{adv}}\\}}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Early backdoor attacks were characterized by their use of non-clean labels [32, 1, 33, 3], but more recent iterations of backdoor attacks have evolved to produce poisoned examples that lack a visible trigger [2, 34, 4]. ", "page_idx": 2}, {"type": "text", "text": "On the other hand, triggerless poisoning attacks involve the addition of subtle adversarial perturbations to base images $\\|\\epsilon\\|_{\\infty}\\leq\\xi$ , aiming to align their feature representations or gradients with those of target images of another class, causing target misclassification [5, 6, 7, 8, 9]. These poisoned images are virtually undetectable by external observers. Remarkably, they do not necessitate any alterations to the target images or labels during the inference stage. For a poison targeting a group of target images $\\bar{\\Pi}\\,=\\,\\{(\\bar{x^{\\pi}},y^{\\pi})\\}$ to be misclassified as $y^{\\mathrm{adv}}$ , an ideal triggerless attack would produce a resultant function: ", "page_idx": 2}, {"type": "equation", "text": "$$\n{\\cal F}(x)=\\left\\{y\\atop{\\scriptstyle y^{\\mathrm{adv}}}\\right.\\,\\,\\left.x\\in\\{x:(x,y)\\in{\\mathcal{D}}_{t e s t}\\setminus\\Pi\\right\\}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Background for data availability attacks can be found in [35]. We include results for one leading data availability attack Neural Tangent Gradient Attack (NTGA) [12], but we do not focus on such attacks since they are realized in model results during training. They do not pose a latent security risk in deployed models, and arguably have ethical applications within data privacy and content creator protections as discussed in App. 6. ", "page_idx": 2}, {"type": "text", "text": "The current leading poisoning attacks that we assess our defense against are listed below. More details about their generation can be found in App. A.1. ", "page_idx": 2}, {"type": "text", "text": "\u2022 Bullseye Polytope (BP): BP crafts poisoned samples that position the target near the center of their convex hull in a feature space [9].   \n\u2022 Gradient Matching (GM): GM generates poisoned data by approximating a bi-level objective by aligning the gradients of clean-label poisoned data with those of the adversariallylabeled target [8]. This attack has shown effectiveness against data augmentation and differential privacy. ", "page_idx": 2}, {"type": "text", "text": "\u2022 Narcissus (NS): NS is a clean-label backdoor attack that operates with minimal knowledge of the training set, instead using a larger natural dataset, evading state-of-the-art defenses by synthesizing persistent trigger features for a given target class. [4]. \u2022 Neural Tangent Generalization Attacks (NTGA): NTGA is a clean-label, black-box data availability attack that can collapse model test accuracy [12]. ", "page_idx": 3}, {"type": "text", "text": "2.2 Train-Time Poison Defense Strategies ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Poison defense categories broadly take two primary approaches: filtering and robust training techniques. Filtering methods identify outliers in the feature space through methods such as thresholding [14], nearest neighbor analysis [17], activation space inspection [16], or by examining the covariance matrix of features [15]. These defenses often assume that only a small subset of the data is poisoned, making them vulnerable to attacks involving a higher concentration of poisoned points. Furthermore, these methods substantially increase training time, as they require training with poisoned data, followed by computationally expensive filtering and model retraining [16, 17, 14, 15]. ", "page_idx": 3}, {"type": "text", "text": "On the other hand, robust training methods involve techniques like randomized smoothing [20], extensive data augmentation [36], model ensembling [21], gradient magnitude and direction constraints [37], poison detection through gradient ascent [24], and adversarial training [27, 28, 25]. Additionally, differentially private (DP) training methods have been explored as a defense against data poisoning [22, 38]. Robust training techniques often require a trade-off between generalization and poison success rate [22, 37, 24, 28, 25, 26] and can be computationally intensive [27, 28]. Some methods use optimized noise constructed via Generative Adversarial Networks (GANs) or Stochastic Gradient Descent methods to make noise that defends against attacks [39, 26]. ", "page_idx": 3}, {"type": "text", "text": "Recently Yang et al. [2022] proposed EPIC, a coreset selection method that rejects poisoned images that are isolated in the gradient space while training, and Liu et al. [2023] proposed FRIENDS, a per-image preprocessing transformation that solves a min-max problem to stochastically add $\\ell_{\\infty}$ norm $\\zeta$ -bound \u2018friendly noise\u2019 (typically 16/255) to combat adversarial perturbations (of 8/255) [18, 26]. ", "page_idx": 3}, {"type": "text", "text": "These two methods are the previous SoTA and will serve as a benchmark for our PUREGEN methods in the experimental results. Finally, simple compression JPEG has been shown to defend against a variety of other adversarial attacks, and we apply it as a baseline defense in train-time poison attacks here as well, finding that it often outperforms previous SoTA methods [40]. ", "page_idx": 3}, {"type": "text", "text": "3 PUREGEN: Purifying Generative Dynamics against Poisoning Attacks ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "3.1 Energy-Based Models and PUREGEN-EBM ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "An Energy-Based Model (EBM) is formulated as a Gibbs-Boltzmann density, as introduced in [41]. This model can be mathematically represented as: ", "page_idx": 3}, {"type": "equation", "text": "$$\np_{\\theta}(x)=\\frac{1}{Z(\\theta)}\\exp(-\\mathcal{G}_{\\theta}(x))q(x),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\boldsymbol{x}\\in\\mathcal{X}\\subset\\mathbb{R}^{D}$ denotes an image signal, and $q(x)$ is a reference measure, often a uniform or standard normal distribution. Here, $g_{\\theta}$ signifies the energy potential, parameterized by a ConvNet with parameters $\\theta$ . ", "page_idx": 3}, {"type": "text", "text": "The EBM $\\mathcal{G}_{\\theta}(x)$ can be interpreted as an unnormalized probability of how natural the image is to the dataset. Thus, we can use $\\mathcal{G}_{\\theta}(x)$ to filter images based on their likelihood of being poisoned. Furthermore, the EBM can be used as a generator. Given a starting clear or purified image $x_{\\tau}$ , we use Markov Chain Monte Carlo (MCMC) Langevin dynamics to iteratively generate more natural images via Equation 4. ", "page_idx": 3}, {"type": "equation", "text": "$$\nx_{\\tau+\\Delta\\tau}=x_{\\tau}-\\Delta\\tau\\nabla_{x_{\\tau}}\\mathcal{G}_{\\theta}(x_{\\tau})+\\sqrt{2\\Delta\\tau}\\varepsilon_{\\tau},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\varepsilon_{k}\\sim\\mathcal{N}(0;\\mathbf{I}_{D})$ , $\\tau$ indexes the time step of the Langevin dynamics, and $\\Delta\\tau$ is the discretization of time [41]. $\\dot{\\nabla}_{x}\\mathcal G_{\\theta}(x)=\\partial\\mathcal G_{\\theta}(x)/\\partial x$ can be obtained by back-propagation. Intuitively, the EBM informs a noisy stochastic gradient descent toward natural images. More details on the convergent contrastive learning mechanism of the EBM and mid-run generative dynamics that makes purification possible can be found in App. A.2.1. Ultimately, the training modifications of using realistic images to initialize the MCMC runs of negative samples produces mid-run, meta-stable EBM dynamics which can be leveraged for better purification. Further intuition is in Section 3.4. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "3.2 Diffusion Models and PUREGEN-DDPM ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Denoising Diffusion Probabilistic Models (DDPMs) are a class of generative models proposed by [Ho et al., 2020] where the key idea is to define a forward diffusion process that adds noise until the image reaches a noise prior and then learn a reverse process that removes noise to generate samples as discussed further in App. A.3 [42]. For purification, we are interested in the stochastic \u201crestoration\u201d of the reverse process, where the forward process can degrade the image enough to remove adversarial perturbations. We find that only training the DDPM with a subset of the standard $\\beta_{t}$ schedule, where the original image never reaches the prior, sacrifices generative capabilities for slightly improved poison defense while reducing training costs. Thus we introduce PUREGEN-DDPM which makes the simple adjustment of only training DDPMs for an initial portion of the standard forward process, improving purification capabilities. For our experiments, we find models trained up to 250 steps outperformed models in terms of poison purification than those trained on higher steps, up to the standard 1000 steps. We show visualizations and empirical evidence of this in Figure 2 below. In App. E.2.2 we show that pre-trained, standard DDPMs can offer comparable defense performance, but with added training cost. ", "page_idx": 4}, {"type": "image", "img_path": "ZeihWodDVh/tmp/1abfa9b42add6c1176138ae973be58b187f9877bbd36a3c84317e6be9502b0fa.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 2: Top We compare PUREGEN-DDPM forward steps with the standard DDPM where 250 steps degrades images for purification but does not reach a noise prior. Note that all model are trained with the same linear $\\beta$ schedule. Bottom Left Generated images from models with 250, 750, and 1000 (Standard) train forward steps where it is clear 250 steps does not generate realistic images Bottom Right Significantly improved poison defense performance of PUREGEN-DDPM with 250 train steps indicating a trade-off between data purification and generative capabilities. ", "page_idx": 4}, {"type": "text", "text": "3.3 Classification with Stochastic Transformation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Let $\\Psi_{T}:\\mathbb{R}^{D}\\rightarrow\\mathbb{R}^{D}$ be a stochastic pre-processing transformation. In this work, $\\Psi_{T}(x)$ , is the random variable of a fixed image $x$ , and we define $T\\stackrel{\\cdot}{=}(T_{\\mathrm{EBM}},T_{\\mathrm{DDPM}},T_{\\mathrm{Reps}})\\in\\mathbb{R}^{3}$ , hyperparameters specifying the number of EBM MCMC steps, the number of diffusion steps, and the number of times these steps are repeated, respectively. Then, $T_{\\mathrm{PUREGEN-EBM}}\\,=\\,(T_{\\mathrm{EBM}},0,1)$ and ${\\cal T}_{\\mathrm{PUREGEN-DDPM}}=$ $(0,T_{\\mathrm{DDPM}},1)$ . ", "page_idx": 4}, {"type": "text", "text": "We compose a stochastic transformation $\\Psi_{T,k}(x)$ with a randomly initialized deterministic classifier $f_{\\phi}(x)\\in\\mathbb{R}^{J}$ (for us, a naturally trained classifier) to define a new deterministic classifier $F_{\\phi}(x)\\in\\mathbb{R}^{J}$ as ", "page_idx": 5}, {"type": "equation", "text": "$$\nF_{\\phi}(x)=\\mathbb{E}_{\\Psi_{T,k}(x)}[f_{\\phi_{0}}(\\Psi_{T,k}(x))]\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "which is trained with cross-entropy loss via SGD to realize $F_{\\phi}(x)$ . As this is computationally infeasible we take $f_{\\phi}(\\Psi_{T,k}(x))$ as the point estimate of $F_{\\phi}(x)$ , which is valid because $\\bar{\\Psi}_{T,k}(x)$ has low variance. ", "page_idx": 5}, {"type": "text", "text": "3.4 Erasing Poison Signals via Mid-Run MCMC ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The stochastic transform $\\Psi_{T}(x)$ is an iterative process. PUREGEN-EBM is akin to a noisy gradient descent over the unconditional energy landscape of a learned data distribution. This is more implicit in the PUREGEN-DDPM dynamics. As $T$ increases, poisoned images move from their initial higher energy towards more realistic lower-energy samples that lack poison perturbations. As shown in Figure 1, the energy distributions of poisoned images are much higher, pushing the poisons away from the likely manifold of natural images. By using Langevin dynamics of EBMs and DDPMs, we transport poisoned images back toward the center of the energy basin. ", "page_idx": 5}, {"type": "image", "img_path": "ZeihWodDVh/tmp/6afb1fa9f9311e6a0c798f8d7b74f965bd4e4b82e7fea75f1267e6b2e7333306.jpg", "img_caption": ["Figure 3: Plot of $\\ell_{2}$ distances for PUREGEN-EBM (Left) and PUREGEN-DDPM (Right) between clean images and clean purified (blue), clean images and poisoned purified (green), and poisoned images and poisoned purified images (orange) at points on the Langevin dynamics trajectory. Purifying poisoned images for less than 250 steps moves a poisoned image closer to its clean image with a minimum around 150, preserving the natural image while removing the adversarial features. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "In from-scratch $\\epsilon=8$ poison scenarios, 150 EBM Langevin steps or 75 DDPM steps fully purifies the majority of the dataset with minimal feature loss to the original image. In Figure 3, we explore the Langevin trajectory\u2019s impacts on $\\ell_{2}$ distance of both purified clean and poisoned images from the initial clean image $(\\|\\boldsymbol{x}-\\boldsymbol{\\Psi}_{T}(\\boldsymbol{x})\\|_{2}$ and $\\|x-\\Psi_{T}(x+\\bar{\\delta})\\|_{2}),$ , and the purified poisoned image\u2019s trajectory away from its poisoned starting point $(\\Vert(x+\\delta)-\\Psi_{T}(x+\\delta)\\Vert_{2})$ . Both poisoned and clean distance trajectories converge to similar distances away from the original clean image $(\\mathrm{lim}_{T\\rightarrow\\infty}\\parallel x-$ $\\begin{array}{r}{\\Psi_{T}(x)\\|_{2}=\\operatorname*{lim}_{T\\rightarrow\\infty}\\|x-\\Psi_{T}(x+\\delta)\\|_{2})}\\end{array}$ , and the intersection where $\\|(x+\\delta)-\\Psi_{T}(x+\\delta)\\|_{2}>$ $\\|\\boldsymbol{x}-\\boldsymbol{\\Psi}_{T}(\\boldsymbol{x}+\\boldsymbol{\\delta})\\|_{2}$ (indicated by the dotted red line), occurs at $\\mathord{\\sim}150$ EBM and 75 DDPM steps, indicating when purification has moved the poisoned image closer to the original clean image than the poisoned version of the image. ", "page_idx": 5}, {"type": "text", "text": "These dynamics provide a concrete intuition for choosing step counts that best balance poison defense with natural accuracy (given a poison $\\epsilon$ ), hence why we use 150-1000 EBM steps of 75-125 (specifically 150 EBM, 75 DDPM steps in from-scratch scenarios) shown in App. D.2. Further, PUREGEN-EBM dynamics stay closer to the original images, while PUREGEN-DDPM moves further away as we increase the steps as the EBM has explicitly learned a probable data distribution, while the DDPM restoration is highly dependent on the conditional information in the degraded image. More experiments comparing the two are shown in App. G.2. These dynamics align with empirical results showing that EBMs better maintain natural accuracy and poison defense with smaller perturbations and across larger distributional shifts, but DDPM dynamics are better suited for larger poison perturbations. Finally, we note the purify times in the $x$ -axes of Fig. 3, where PUREGEN-EBM is much faster for the same step counts to highlight the computational differences for the two methods, which we further explore Section 4.5. ", "page_idx": 5}, {"type": "text", "text": "Ultimately, one can think of PureGen as sampling from a \u201cclose\u201d region in the pixel space around the original image where proximity is determined by a stochastic process that is initialized at the image and remains \u201cclose\u201d due to an explicit (EBM) or implicit (DDPM) energy gradient - all but assuring the original poison is mitigated in the process. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1 Experimental Details ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Table 1: Poison success and natural accuracy in all ResNet poison training scenarios. We report the mean and the standard deviations (as subscripts) of $100\\,\\mathrm{GM}$ experiments, 50 BP experiments, and NS triggers over 10 classes. ", "page_idx": 6}, {"type": "table", "img_path": "ZeihWodDVh/tmp/030b87700b4a1078d0e4d8e33f04e172ad9f2449794401afd46d727976f0625d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "We evaluate PUREGEN-EBM and PUREGEN-DDPM against state-of-the-art defenses EPIC and FRIENDS, and baseline defense JPEG, on leading poisons Narcissus (NS), Gradient Matching (GM), and Bullseye Polytope (BP). Using ResNet18 and CIFAR-10, we measure poison success, natural accuracy, and max poison success across classes for triggered NS attacks. All poisons and poison scenario settings come from previous baseline attack and defense works, and additional details on poison sources, poison crafting, definitions of poison success, and training hyperparameters can be found in App. D. Poisons were chosen for their availability or ease of generation from the poisoncrafting research community, which is why there are no GM results on CINIC-10 and no Narcissus results on Tiny-ImageNet. And we note that certain poison successes (GM and BP) are for moving a single image to a target class per 50-100 classifier scenarios and, hence, lack a standard deviation. Athough, we show the results are low variance using different seeds on a subset of scenarios in App. E.2.3. ", "page_idx": 6}, {"type": "text", "text": "Our EBMs and DDPMs are trained on the ImageNet (70k) portion of the CINIC-10 dataset, CIFAR-10, and CalTech-256 for poisons scenarios using CIFAR-10, CINIC-10, and Tiny-ImageNet respectively, to ensure no overlap of PUREGEN train and attacked classifier train datasets [43, 44, 45]. ", "page_idx": 6}, {"type": "text", "text": "4.2 Benchmark Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Table 1 shows our primary results using ResNet18 (34 for Tiny-IN) in which PUREGEN achieves state-of-the-art (SoTA) poison defense and natural accuracy in all poison scenarios. Both PUREGEN methods show large improvements over baselines in triggered NS attacks (PUREGEN matches or exceeds previous SoTA with a $1\\!-\\!6\\%$ poison defense reduction and $0.5\u20131.5\\%$ less degradation in natural accuracy), while maintaining perfect or near-perfect defense with improved natural accuracy in triggerless BP and GM scenarios. Note that PUREGEN-EBM does a better job maintaining natural accuracy in the 100 class scenarios (BP-WhiteBox and Tiny-IN), while PUREGEN-DDPM tends to get much better poison defense when the PUREGEN-EBM is not already low. ", "page_idx": 6}, {"type": "text", "text": "Table 2 shows selected results for additional models MobileNetV2, DenseNet121, and Hyperlight Benchmark (HLB), which is a unique case study with a residual-less network architecture, unique initialization scheme, and super-convergence training method that recently held the world record of achieving $94\\%$ test accuracy on CIFAR-10 with just 10 epochs [46]. Due to the fact that PUREGEN is a preprocessing step, it can be readily applied to novel training paradigms with no modifications unlike previous baselines EPIC and FRIENDS. In all results, PUREGEN is again SoTA, except for NTGA data-availability attack, where PUREGEN is just below SoTA method AVATAR (which is also a diffusion based approach). But we again emphasize data-availability attacks are not the focus of PUREGEN which secures against latent attacks. ", "page_idx": 6}, {"type": "table", "img_path": "ZeihWodDVh/tmp/f5664e3be6af813d08c287a81961ee9c2422f8edefc7b45533d034d1bdb00cbe.jpg", "table_caption": ["Table 2: Results for additional models (MobileNetV2, DenseNet121, and HLB) and the NTGA data-availability attack. PUREGEN remains state-of-the-art for all train-time latent attacks, while NTGA defense shows near SoTA performance. \\*All NTGA baselines pulled from [30]. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "The complete results for all models and all versions of each baseline can be found in App. E. ", "page_idx": 7}, {"type": "image", "img_path": "ZeihWodDVh/tmp/9db97e9a23bfe35742d208c3ddc97019efaddc9b1f7059adcbf0109a08d9d1c8.jpg", "img_caption": ["4.3 PUREGEN Robustness to Train Data Shifts, Poisoning, and Defense-Aware Poisons ", "Figure 4: PUREGEN-EBM vs. PUREGEN-DDPM with increasingly Out-of-Distribution training data (for generative model training) and purifying target/attacked distribution CIFAR-10. PUREGENEBM is much more robust to distributional shift for natural accuracy while both PUREGEN-EBM and PUREGEN-DDPM maintain SoTA poison defense across all train distributions \\*CIFAR-10 is a \u201ccheating\u201d baseline as clean versions of poisoned images are present in training data. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "An important consideration for PUREGEN is the distributional shift between the data used to train the generative models and the target dataset to be purified. Figure 4 explores this by training PUREGEN-EBM and PUREGEN-DDPM on increasingly out-of-distribution (OOD) datasets while purifying the CIFAR-10 dataset (NS attack). We quantify the distributional shift using the Fr\u00e9chet Inception Distance (FID) [47] between the original CIFAR-10 training images and the OOD datasets. Notably, both methods maintain SoTA or near SoTA poison defense across all training distributions, highlighting their effectiveness even under distributional shift. The results show that PUREGENEBM is more robust to distributional shift in terms of maintaining natural accuracy, with only a slight drop in performance even when trained on highly OOD datasets like Flowers-102 and LFW people. In contrast, PUREGEN-DDPM experiences a more significant drop in natural accuracy as the distributional shift increases. Note that the CIFAR-10 is a \u201ccheating\u201d baseline, as clean versions of the poisoned images are present in the generative model training data, but it provides an upper bound on the performance that can be achieved when the generative models are trained on perfectly in-distribution data. ", "page_idx": 7}, {"type": "table", "img_path": "ZeihWodDVh/tmp/d2a1d495a7ec5b7e04ebe3fb7c77a116d9948bb82821a39c2f4c378ce0b23458.jpg", "table_caption": ["Table 3: Both PUREGEN-EBM and PUREGEN-DDPM are robust to NS attack even when fully poisoned (all classes at once) during model training except for NS Eps-16 for PUREGEN-EBM "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Another important consideration is the robustness of PUREGEN when the generative models themselves are trained on poisoned data. Table 3 shows the performance of PUREGEN-EBM and PUREGEN-DDPM when their training data is fully poisoned with the Narcissus (NS) attack, meaning that all classes are poisoned simultaneously. The results demonstrate that both PUREGEN-EBM and PUREGEN-DDPM are highly robust to poisoning during model training, maintaining SoTA poison defense and natural accuracy with only exception being PUREGEN-EBM\u2019s performance on the more challenging NS $\\epsilon=16$ attack when poisoned with the same perturbations. While it is unlikely an attacker would have access to both the the generative model and classifier train datasets, these findings highlight the inherent robustness of PUREGEN, as the generative models can effectively learn the underlying clean data distribution even in the presence of poisoned samples during training. This is a key advantage of PUREGEN compared to other defenses, especially when there is no secure dataset. ", "page_idx": 8}, {"type": "text", "text": "Finally, in App. E.2.1, we show results where we integrate an EBM into the Narcissus crafting pipeline, done by taking a gradient through the EBM MCMC dynamics in three ways based on the specifics of crafting. In all cases, PUREGEN shows almost no defense degradation, and we actually show we can generate more effective poisons over baseline this way. These results further validate the effectiveness of PUREGEN even with defense-aware crafting, which is not a typical assumption in train-time attacks. ", "page_idx": 8}, {"type": "text", "text": "4.4 PUREGEN Extensions on Higher Power Attacks ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To leverage the strengths of both PUREGEN-EBM and PUREGEN-DDPM, we propose PUREGEN combinations that utilize either both EMB and DPPM back-to-back (PUREGEN-NAIVE), EBM and DDPM with multiple repetitions of smaller steps (PUREGEN-REPS), and finally EBM as a filter to then use EBM/DDPM on only the $\\mathbf{k}$ highest energy samples as described in 3.3. For additional description, see C, and note that these extensions required extensive hyperparameter search with performance sweeps shown in App F, as there was little intuition for the amount of reps $(T_{R e p s})$ or the flitering threshold $(k)$ needed. Thus, we do not include these methods in our core results, but we do show the added performance gains on higher power poisons in Table 4, both in terms of increased perturbation size $\\epsilon=16$ and increased poison $\\%$ (and both together). ", "page_idx": 8}, {"type": "table", "img_path": "ZeihWodDVh/tmp/f8bcff5466d2a870372a2f40aab8371114e04431871364852b29f749f50b8a53.jpg", "table_caption": ["Table 4: PUREGEN-NAIVE, PUREGEN-REPS, and PUREGEN-FILT results showing further performance gains on increased poison power scenarios "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.5 PUREGEN Timing and Limitations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Table 5 presents the training times for using PUREGEN-EBM and PUREGEN-DDPM (923 and 4181 seconds respectively to purify) on CIFAR-10 using a TPU V3. Although these times may seem significant, PUREGEN is a universal defense applied once per dataset, making its cost negligible when reused across multiple tasks and poison scenarios. To highlight this, we also present the purification times amortized over the 10 and 100 NS and GM poison scenarios, demonstrating that the cost becomes negligible when the purified dataset is used multiple times relative to baselines like FRIENDS which require retraining for each specific task and poison scenario (while still utilizing the full dataset unlike EPIC). PUREGEN-EBM generally has lower purification times compared to PUREGEN-DDPM, making it more suitable for subtle and rare perturbations. Conversely, PUREGENDDPM can handle more severe perturbations but at a higher computational cost and potential reduction in natural accuracy. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "table", "img_path": "ZeihWodDVh/tmp/30ff2888736620038cb868aa3d0fb884f47bdc8aebc234b90a06fb160ecad6d6.jpg", "table_caption": ["Table 5: PUREGEN and baseline Timing Analysis on TPU V3 "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Training the generative models for PUREGEN involves substantial computational cost and data requirements. However, as shown in Table 3 and Figure 4, these models remain effective even when trained on poisoned or out-of-distribution data. This universal applicability justifies the initial training cost, as the models can defend against diverse poisoning scenarios. So while JPEG is a fairly effective baseline, the added beneftis of PUREGEN start to outweigh the compute as the use cases of the dataset increase. While PUREGEN combinations (PUREGEN-REPS and PUREGEN-FILT) show enhanced performance on higher power attacks (Table 4), further research is needed to fully exploit the strengths of both PUREGEN-EBM and PUREGEN-DDPM. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Poisoning has the potential to become one of the greatest attack vectors to AI models, decreasing model security and eroding public trust. In this work, we introduce PUREGEN, a suite of universal data purification methods that leverage the stochastic dynamics of Energy-Based Models (EBMs) and Denoising Diffusion Probabilistic Models (DDPMs) to defend against train-time data poisoning attacks. PUREGEN-EBM and PUREGEN-DDPM effectively purify poisoned datasets by iteratively transforming poisoned samples into the natural data manifold, thus mitigating adversarial perturbations. Our extensive experiments demonstrate that these methods achieve state-of-the-art performance against a range of leading poisoning attacks and can maintain SoTA performance in the face of poisoned or distributionally shifted generative model training data. These versatile and efficient methods set a new standard in protecting machine learning models against evolving data poisoning threats, potentially inspiring greater trust in AI applications. ", "page_idx": 9}, {"type": "text", "text": "6 Potential Social Impacts ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Poisoning represents one of the greatest emerging threats to AI systems, particularly as foundation models increasingly rely on large, diverse datasets without rigorous quality control against imperceptible perturbations. This vulnerability is especially concerning in high-stakes domains like healthcare, security, and autonomous vehicles, where model integrity is crucial and erroneous outputs could have catastrophic consequences. Our research provides a universal defense method that can be implemented with minimal impact to existing training infrastructure, enabling practitioners to preemptively secure their datasets against state-of-the-art poisoning attacks. ", "page_idx": 9}, {"type": "text", "text": "While we acknowledge that the poison defense space can promote an \u2018arms race\u2019 of increasingly sophisticated attacks and defenses, our approach\u2019s universality poses a fundamentally harder challenge for attackers, even when using defense-aware crafting E.2.1. We specifically focus on defending against latent backdoor vulnerabilities rather than data availability attacks, as the latter can serve legitimate purposes in protecting content creators\u2019 rights. By providing robust defense against malicious poisoning while preserving natural model performance, our method helps build trust in AI systems for increasingly consequential real-world applications. ", "page_idx": 9}, {"type": "text", "text": "7 Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work is supported with Cloud TPUs from Google\u2019s Tensorflow Research Cloud (TFRC). We would like to acknowledge Jonathan Mitchell, Mitch Hill, Yuan Du and Kathrine Abreu for support and discussion on base EBM and Diffusion code, and Yunzheng Zhu for his help in crafting poisons. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] T. Gu, B. Dolan-Gavitt, and S. Garg, \u201cBadnets: Identifying vulnerabilities in the machine learning model supply chain,\u201d arXiv preprint arXiv:1708.06733, 2017.   \n[2] A. Turner, D. Tsipras, and A. Madry, \u201cClean-label backdoor attacks,\u201d 2018.   \n[3] H. Souri, M. Goldblum, L. Fowl, R. Chellappa, and T. Goldstein, \u201cSleeper agent: Scalable hidden trigger backdoors for neural networks trained from scratch,\u201d arXiv preprint arXiv:2106.08970, 2021.   \n[4] Y. Zeng, M. Pan, H. A. Just, L. Lyu, M. Qiu, and R. Jia, \u201cNarcissus: A practical clean-label backdoor attack with limited information,\u201d arXiv preprint arXiv:2204.05255, 2022.   \n[5] A. Shafahi, W. R. Huang, M. Najibi, O. Suciu, C. Studer, T. Dumitras, and T. Goldstein, \u201cPoison frogs! targeted clean-label poisoning attacks on neural networks,\u201d 2018.   \n[6] C. Zhu, W. R. Huang, H. Li, G. Taylor, C. Studer, and T. Goldstein, \u201cTransferable clean-label poisoning attacks on deep neural nets,\u201d in International Conference on Machine Learning, 2019, pp. 7614\u20137623.   \n[7] W. R. Huang, J. Geiping, L. Fowl, G. Taylor, and T. Goldstein, \u201cMetapoison: Practical generalpurpose clean-label data poisoning,\u201d Advances in Neural Information Processing Systems, vol. 33, 2020.   \n[8] J. Geiping, L. H. Fowl, W. R. Huang, W. Czaja, G. Taylor, M. Moeller, and T. Goldstein, \u201cWitches\u2019 brew: Industrial scale data poisoning via gradient matching,\u201d in International Conference on Learning Representations, 2021. [Online]. Available: https://openreview.net/forum?id=01olnfLIbD   \n[9] H. Aghakhani, D. Meng, Y.-X. Wang, C. Kruegel, and G. Vigna, \u201cBullseye polytope: A scalable clean-label poisoning attack with improved transferability,\u201d in 2021 IEEE European Symposium on Security and Privacy (EuroS&P). IEEE, 2021, pp. 159\u2013178.   \n[10] J. Feng, Q.-Z. Cai, and Z.-H. Zhou, \u201cLearning to confuse: Generating training time adversarial data with auto-encoder,\u201d 2019.   \n[11] H. Huang, X. Ma, S. M. Erfani, J. Bailey, and Y. Wang, \u201cUnlearnable examples: Making personal data unexploitable,\u201d 2021.   \n[12] C.-H. Yuan and S.-H. Wu, \u201cNeural tangent generalization attacks,\u201d in Proceedings of the 38th International Conference on Machine Learning, ser. Proceedings of Machine Learning Research, M. Meila and T. Zhang, Eds., vol. 139. PMLR, 18\u201324 Jul 2021, pp. 12 230\u201312 240. [Online]. Available: https://proceedings.mlr.press/v139/yuan21b.html   \n[13] G. F. Cretu, A. Stavrou, M. E. Locasto, S. J. Stolfo, and A. D. Keromytis, \u201cCasting out demons: Sanitizing training data for anomaly sensors,\u201d in 2008 IEEE Symposium on Security and Privacy (sp 2008). IEEE, 2008, pp. 81\u201395.   \n[14] J. Steinhardt, P. W. Koh, and P. Liang, \u201cCertified defenses for data poisoning attacks,\u201d 2017.   \n[15] B. Tran, J. Li, and A. Madry, \u201cSpectral signatures in backdoor attacks,\u201d in Advances in Neural Information Processing Systems, 2018, pp. 8000\u20138010.   \n[16] B. Chen, W. Carvalho, N. Baracaldo, H. Ludwig, B. Edwards, T. Lee, I. Molloy, and B. Srivastava, \u201cDetecting backdoor attacks on deep neural networks by activation clustering,\u201d in SafeAI@ AAAI, 2019.   \n[17] N. Peri, N. Gupta, W. R. Huang, L. Fowl, C. Zhu, S. Feizi, T. Goldstein, and J. P. Dickerson, \u201cDeep k-nn defense against clean-label data poisoning attacks,\u201d in European Conference on Computer Vision. Springer, 2020, pp. 55\u201370.   \n[18] Y. Yang, T. Y. Liu, and B. Mirzasoleiman, \u201cNot all poisons are created equal: Robust training against data poisoning,\u201d 2022.   \n[19] O. Pooladzandi, D. Davini, and B. Mirzasoleiman, \u201cAdaptive second order coresets for dataefficient machine learning,\u201d 2022.   \n[20] M. Weber, X. Xu, B. Karla\u0161, C. Zhang, and B. Li, \u201cRab: Provable robustness against backdoor attacks,\u201d arXiv preprint arXiv:2003.08904, 2020.   \n[21] A. Levine and S. Feizi, \u201cDeep partition aggregation: Provable defenses against general poisoning attacks,\u201d in International Conference on Learning Representations, 2020.   \n[22] M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar, and L. Zhang, \u201cDeep learning with differential privacy,\u201d in Proceedings of the 2016 ACM SIGSAC conference on computer and communications security, 2016, pp. 308\u2013318.   \n[23] Y. Ma, X. Z. Zhu, and J. Hsu, \u201cData poisoning against differentially-private learners: Attacks and defenses,\u201d in International Joint Conference on Artificial Intelligence, 2019.   \n[24] Y. Li, X. Lyu, N. Koren, L. Lyu, B. Li, and X. Ma, \u201cAnti-backdoor learning: Training clean models on poisoned data,\u201d Advances in Neural Information Processing Systems, vol. 34, 2021.   \n[25] L. Tao, L. Feng, J. Yi, S.-J. Huang, and S. Chen, \u201cBetter safe than sorry: Preventing delusive adversaries with adversarial training,\u201d Advances in Neural Information Processing Systems, vol. 34, 2021.   \n[26] T. Y. Liu, Y. Yang, and B. Mirzasoleiman, \u201cFriendly noise against adversarial noise: A powerful defense against data poisoning attacks,\u201d 2023.   \n[27] J. Geiping, L. Fowl, G. Somepalli, M. Goldblum, M. Moeller, and T. Goldstein, \u201cWhat doesn\u2019t kill you makes you robust (er): Adversarial training against poisons and backdoors,\u201d arXiv preprint arXiv:2102.13624, 2021.   \n[28] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, \u201cTowards deep learning models resistant to adversarial attacks,\u201d in International Conference on Learning Representations, 2018.   \n[29] M. Hill, J. C. Mitchell, and S.-C. Zhu, \u201cStochastic security: Adversarial defense using long-run dynamics of energy-based models,\u201d in International Conference on Learning Representations, 2021. [Online]. Available: https://openreview.net/forum?id $\\equiv$ gwFTuzxJW0   \n[30] H. M. Dolatabadi, S. Erfani, and C. Leckie, \u201cThe devil\u2019s advocate: Shattering the illusion of unexploitable data using diffusion models,\u201d 2024.   \n[31] W. Nie, B. Guo, Y. Huang, C. Xiao, A. Vahdat, and A. Anandkumar, \u201cDiffusion models for adversarial purification,\u201d arXiv preprint arXiv:2205.07460, 2022.   \n[32] X. Chen, C. Liu, B. Li, K. Lu, and D. Song, \u201cTargeted backdoor attacks on deep learning systems using data poisoning,\u201d arXiv preprint arXiv:1712.05526, 2017.   \n[33] Y. Liu, S. Ma, Y. Aafer, W.-C. Lee, J. Zhai, W. Wang, and X. Zhang, \u201cTrojaning attack on neural networks,\u201d 2017.   \n[34] A. Saha, A. Subramanya, and H. Pirsiavash, \u201cHidden trigger backdoor attacks,\u201d 2019.   \n[35] D. Yu, H. Zhang, W. Chen, J. Yin, and T.-Y. Liu, \u201cAvailability attacks create shortcuts,\u201d in Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, ser. KDD \u201922. ACM, Aug. 2022. [Online]. Available: http://dx.doi.org/10.1145/3534678.3539241   \n[36] E. Borgnia, V. Cherepanova, L. Fowl, A. Ghiasi, J. Geiping, M. Goldblum, T. Goldstein, and A. Gupta, \u201cStrong data augmentation sanitizes poisoning and backdoor attacks without an accuracy tradeoff,\u201d in ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021, pp. 3855\u20133859.   \n[37] S. Hong, V. Chandrasekaran, Y. Kaya, T. Dumitra\u00b8s, and N. Papernot, \u201cOn the effectiveness of mitigating data poisoning attacks with gradient shaping,\u201d arXiv preprint arXiv:2002.11497, 2020.   \n[38] B. Jayaraman and D. Evans, \u201cEvaluating differentially private machine learning in practice,\u201d in 28th {USENIX} Security Symposium ({USENIX} Security 19), 2019, pp. 1895\u20131912.   \n[39] D. Madaan, J. Shin, and S. J. Hwang, \u201cLearning to generate noise for multi-attack robustness,\u201d 2021.   \n[40] N. Das, M. Shanbhogue, S.-T. Chen, F. Hohman, S. Li, L. Chen, M. E. Kounavis, and D. H. Chau, \u201cShield: Fast, practical defense and vaccination for deep learning using jpeg compression,\u201d 2018.   \n[41] J. Xie, Y. Lu, S.-C. Zhu, and Y. Wu, \u201cA theory of generative convnet,\u201d in Proceedings of the 33rd International Conference on Machine Learning, 2016, pp. 2635\u20132644.   \n[42] J. Ho, A. Jain, and P. Abbeel, \u201cDenoising diffusion probabilistic models,\u201d 2020.   \n[43] A. Krizhevsky, V. Nair, and G. Hinton, \u201cCifar-10 (canadian institute for advanced research).\u201d [Online]. Available: http://www.cs.toronto.edu/\\~kriz/cifar.html   \n[44] L. N. Darlow, E. J. Crowley, A. Antoniou, and A. J. Storkey, \u201cCinic-10 is not imagenet or cifar-10,\u201d 2018.   \n[45] G. Griffin, A. Holub, and P. Perona, \u201cCaltech 256,\u201d Apr 2022.   \n[46] T. Balsam, \u201chlb-cifar10,\u201d 2023, released on 2023-02-12. [Online]. Available: https: //github.com/tysam-code/hlb-CIFAR10   \n[47] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter, \u201cGans trained by a two time-scale update rule converge to a local nash equilibrium,\u201d 2018.   \n[48] E. Nijkamp, M. Hill, T. Han, S.-C. Zhu, and Y. N. Wu, \u201cOn the anatomy of MCMC-based maximum likelihood learning of energy-based models,\u201d in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 34, 2020.   \n[49] T. Miyato, T. Kataoka, M. Koyama, and Y. Yoshida, \u201cSpectral normalization for generative adversarial networks,\u201d arXiv preprint arXiv:1802.05957, 2018.   \n[50] A. Schwarzschild, M. Goldblum, A. Gupta, J. P. Dickerson, and T. Goldstein, \u201cJust how toxic is data poisoning? a unified benchmark for backdoor and data poisoning attacks,\u201d 2021.   \n[51] J. Whitaker, \u201cHugging face ddpm butterflies model,\u201d https://huggingface.co/johnowhitaker/ ddpm-butterflies-32px, accessed: 2024-08-05.   \n[52] N. Onzo, \u201cHugging face ddpm anime model,\u201d https://huggingface.co/onragi/ anime-ddpm-32-res2-v3, accessed: 2024-08-05.   \n[53] N. Kokhlikyan, V. Miglani, M. Martin, E. Wang, B. Alsallakh, J. Reynolds, A. Melnikov, N. Kliushkina, C. Araya, S. Yan, and O. Reblitz-Richardson, \u201cCaptum: A unified and generic model interpretability library for pytorch,\u201d 2020. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Further Background ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Poisons ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The goal of adding train-time poisons is to change the prediction of a set of target examples $\\Pi=$ $\\{(x^{\\overline{{\\pi}}},y^{\\pi})\\}\\subset\\mathcal{D}_{t e s t}$ or triggered examples $\\{(x+\\bar{\\rho},y):\\bar{(x,y)}\\in\\mathcal{D}_{t e s t}\\}$ to an adversarial label $y^{\\mathrm{adv}}$ . ", "page_idx": 13}, {"type": "text", "text": "Targeted clean-label data poisoning attacks can be formulated as the following bi-level optimization problem: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{\\delta_{i}\\in\\mathcal{C}_{\\delta},\\rho\\in\\mathcal{C}_{\\rho}}{\\mathrm{argmin}}\\:\\sum_{\\substack{(x^{\\pi},y^{\\pi})\\in\\Pi}}\\mathcal{L}\\left(F(x^{\\pi}+\\rho;\\phi(\\delta)),y^{\\mathrm{adv}}\\right)}&{}\\\\ {\\sum_{i=0}^{n}\\mathbb{1}_{\\delta_{i}\\not=0}\\le\\alpha n^{(x^{\\pi},y^{\\pi})}\\mathrm{erm}}&{}\\\\ {s.t.\\quad\\phi(\\delta)\\!=\\!\\underset{\\phi}{\\mathrm{argmin}}\\:\\sum_{(x,y)\\in\\mathcal{D}}\\mathcal{L}\\left(F(x\\!+\\!\\delta_{i};\\phi),y\\right)}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "For a triggerless poison, we solve for the ideal perturbations $\\delta_{i}$ to minimize the adversarial loss on the target images, where $\\mathcal{C}_{\\delta}=\\mathcal{C}$ , $\\mathcal{C}_{\\rho}=\\{\\mathbf{0}\\in\\mathbb{R}^{D}\\}$ , and $\\mathcal{D}=\\mathcal{D}_{t r a i n}$ . To address the above optimization problem, powerful poisoning attacks such as Meta Poison (MP) [7], Gradient Matching (GM) [8], and Bullseye Polytope (BP) [9] craft the poisons to mimic the gradient of the adversarially labeled target, i.e., ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\nabla\\mathcal{L}\\left(F_{\\phi}\\left(x^{\\pi}\\right),y^{\\mathrm{adv}}\\right)\\propto\\sum_{i:\\delta_{i}\\neq\\mathbf{0}}\\nabla\\mathcal{L}\\left(F_{\\phi}(x_{i}+\\delta_{i}),y_{i}\\right)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Minimizing the training loss on RHS of Equation 7 also minimizes the adversarial loss objective of Equation 6. ", "page_idx": 13}, {"type": "text", "text": "For the triggered poison, Narcissus (NS), we find the most representative patch $\\rho$ for class $\\pi$ given $\\mathcal{C}$ , defining Equation 6 with $\\mathcal{C}_{\\delta}=\\{\\mathbf{0}\\in\\mathbb{R}^{D}\\}$ , $\\mathcal{C}_{\\rho}\\!=\\!\\mathcal{C}$ , $\\Pi=\\bar{D_{t r a i n}^{\\pi}},y^{\\mathrm{adv}}=\\bar{y}^{\\pi}$ , and $\\mathcal{D}=\\mathcal{D}_{P O O D}\\cup$ $\\mathcal{D}_{t r a i n}^{\\pi}$ . In particular, this patch uses a public out-of-distribution dataset $\\mathcal{D}_{P O O D}$ and only the targeted class $\\mathcal{D}_{t r a i n}^{\\pi}$ . As finding this patch comes from another natural dataset and does not depend on other train classes, NS has been more flexible to model architecture, dataset, and training regime [4]. ", "page_idx": 13}, {"type": "text", "text": "Background for data availability attacks can be found in [35]. The goal for data availability attacks (sometimes referred to as \u201cunlearnable\u201d attacks is to collapse the test accuracy, and hence the model\u2019s ability to generalize, or learn useful representations, from the train dataset. As we discuss in the main paper, such attacks are immediately obvious when training a model, or rather create a region of poor performance in the model. These attacks do not create latent vulnerabilities that can then be exploited by an adversary. Thus we do not focus on, or investigate our methods with any detail for availability attacks. Further, we discuss in the Societal Impacts section how such attacks have many ethical uses for privacy and data protection 6. ", "page_idx": 13}, {"type": "text", "text": "A.2 Further EBM Discussions ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Recalling the Gibbs-Boltzmann density from Equation 3, ", "page_idx": 13}, {"type": "equation", "text": "$$\np_{\\theta}(x)=\\frac{1}{Z(\\theta)}\\exp(-\\mathcal{G}_{\\theta}(x))q(x),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\boldsymbol{x}\\in\\mathcal{X}\\subset\\mathbb{R}^{D}$ denotes an image signal, and $q(x)$ is a reference measure, often a uniform or standard normal distribution. Here, $g_{\\theta}$ signifies the energy potential, parameterized by a ConvNet with parameters $\\theta$ . ", "page_idx": 13}, {"type": "text", "text": "The normalizing constant, or the partition function, $\\begin{array}{r c l}{{{\\cal Z}(\\theta)}}&{{=}}&{{\\int\\exp\\{-{\\mathcal G}_{\\theta}(x)\\}q(x)d x\\;\\;=}}\\end{array}$ $\\mathbb{E}_{q}[\\exp(-\\mathcal{G}_{\\theta}(x))\\bar{]}$ , while essential, is generally analytically intractable. In practice, $Z(\\theta)$ is not computed explicitly, as $\\mathcal{G}_{\\theta}(x)$ sufficiently informs the Markov Chain Monte Carlo (MCMC) sampling process. ", "page_idx": 13}, {"type": "text", "text": "As which $\\alpha$ of the images are poisoned is unknown, we treat them all the same for a universal defense. Considering i.i.d. samples $x_{i}\\sim\\mathbb{P}$ for $i=1,\\hdots,n$ , with $n$ sufficiently large, the sample average over $x_{i}$ converges to the expectation under $\\mathbb{P}$ and one can learn a parameter $\\theta^{*}$ such that $p_{\\theta^{*}}(x)\\approx\\mathbb{P}(x)$ . For notational simplicity, we equate the sample average with the expectation. ", "page_idx": 13}, {"type": "text", "text": "The objective is to minimize the expected negative log-likelihood, formulated as: ", "page_idx": 14}, {"type": "equation", "text": "$$\n{\\mathcal{L}}(\\theta)={\\frac{1}{n}}\\sum_{i=1}^{n}\\log p_{\\theta}(x_{i})\\doteq\\mathbb{E}_{\\mathbb{P}}[\\log p_{\\theta}(x)].\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The derivative of this log-likelihood, crucial for parameter updates, is given by: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\nabla_{\\theta}\\mathcal{L}(\\theta)=\\mathbb{E}_{\\mathbb{P}}\\left[\\nabla_{\\theta}\\mathcal{G}_{\\theta}(x)\\right]-\\mathbb{E}_{p_{\\theta}}\\left[\\nabla_{\\theta}\\mathcal{G}_{\\theta}(x)\\right]}}\\\\ {{\\displaystyle\\doteq\\frac{1}{n}\\sum_{i=1}^{n}\\!\\nabla_{\\theta}\\mathcal{G}_{\\theta}(x_{i}^{+})-\\frac{1}{k}\\sum_{i=1}^{k}\\nabla_{\\theta}\\mathcal{G}_{\\theta}(x_{i}^{-}),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where solving for the critical points results in the average gradient of a batch of real images $(x_{i}^{+})$ should be equal to the average gradient of a synthesized batch of examples from the real images $x_{i}^{-}\\sim p_{\\theta}(x)$ . The parameters are then updated as $\\theta_{t+1}=\\theta_{t}+\\eta_{t}\\nabla\\mathcal{L}(\\theta_{t}\\bar{)}$ , where $\\eta_{t}$ is the learning rate. ", "page_idx": 14}, {"type": "text", "text": "In this work, to obtain the synthesized samples $x_{i}^{-}$ from the current distribution $p_{\\theta}(x)$ we use the iterative application of the Langevin update as the Monte Carlo Markov Chain (MCMC) method, first introduced in Equation 4: ", "page_idx": 14}, {"type": "equation", "text": "$$\nx_{\\tau+\\Delta\\tau}=x_{\\tau}-\\Delta\\tau\\nabla_{x_{\\tau}}\\mathcal{G}_{\\theta}(x_{\\tau})+\\sqrt{2\\Delta\\tau}\\epsilon_{\\tau},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\epsilon_{\\tau}\\sim\\mathrm{N}(0,I_{D})$ , $\\tau$ indexes the time step of the Langevin dynamics, and $\\Delta\\tau$ is the discretization of time [41]. $\\dot{\\nabla}_{x}\\mathcal G_{\\theta}(x)=\\partial\\mathcal G_{\\theta}(x)/\\partial x$ can be obtained by back-propagation. If the gradient term dominates the diffusion noise term, the Langevin dynamics behave like gradient descent. We implement EBM training following [48], see App. A.2.1 for details. ", "page_idx": 14}, {"type": "text", "text": "Algorithm 1 Data Preprocessing with PUREGEN-EBM: $\\Psi_{T}(x)$ ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Require: Trained ConvNet potential $\\mathcal{G}_{\\theta}(x)$ , training images $x\\ \\in\\ X$ , Langevin steps $T$ , Time discretization $\\Delta\\tau$ for $\\tau$ in $1\\ldots T$ do Langevin Step: draw $\\epsilon_{\\tau}\\sim\\mathrm{N}(0,I_{D})$ ", "page_idx": 14}, {"type": "equation", "text": "$$\nx_{\\tau+1}=x_{\\tau}-\\Delta\\tau\\nabla_{x_{\\tau}}\\mathcal{G}_{\\theta}(x_{\\tau})+\\sqrt{2\\Delta\\tau}\\epsilon_{\\tau}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "end for Return: Purified set $\\tilde{X}$ from final Langevin updates ", "page_idx": 14}, {"type": "text", "text": "A.2.1 EBM Training ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Algorithm 2 is pseudo-code for the training procedure of a data-initialized convergent EBM. We use the generator architecture of the SNGAN [49] for our EBM as our network architecture. Further intuiton can be found in App. B.1. ", "page_idx": 14}, {"type": "text", "text": "A.3 DDPM Background ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The forward process successively adds noise over a sequence of time steps, eventually resulting in values that follow a prior distribution, typically a standard Gaussian as in: ", "page_idx": 14}, {"type": "equation", "text": "$$\nq(\\pmb{x}_{t}|\\pmb{x}_{t-1})=\\mathcal{N}(\\pmb{x}_{t};\\sqrt{1-\\beta_{t}}\\pmb{x}_{t-1},\\beta_{t}\\mathbf{I})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $x_{0}\\sim q(x_{0})$ be a clean image sampled from the data distribution. The forward process is defined by a fixed Markov chain with Gaussian transitions for a sequence of timesteps $t=1,\\dots,T$ : ", "page_idx": 14}, {"type": "text", "text": "The reverse process is defined as the conditional distribution of the previous variable at a timestep, given the current one: ", "page_idx": 14}, {"type": "equation", "text": "$$\np_{\\theta}(\\mathbf{\\boldsymbol{x}}_{t-1}|\\mathbf{\\boldsymbol{x}}_{t})=\\mathcal{N}(\\mathbf{\\boldsymbol{x}}_{t-1};\\mu_{\\theta}(\\mathbf{\\boldsymbol{x}}_{t},t),\\Sigma_{\\theta}(\\mathbf{\\boldsymbol{x}}_{t},t))\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Algorithm 2 ML with SGD for Convergent Learning of EBM (3) ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Require: ConvNet potential $\\mathcal{G}_{\\theta}(x)$ , number of training steps $J=150000$ , initial weight $\\theta_{1}$ , training images $\\{x_{i}^{+}\\}_{i=1}^{N_{\\mathrm{data}}}$ , data perturbation $\\tau_{\\mathrm{data}}=0.02$ , step size $\\tau=0.01$ , Langevin steps $T=100$ SGD learning rate . ", "page_idx": 15}, {"type": "text", "text": "Ensure: Weights $\\theta_{J+1}$ for energy ${\\mathcal{G}}_{\\theta}(x)$ . ", "page_idx": 15}, {"type": "text", "text": "Set optimizer $g\\gets\\mathrm{SGD}(\\gamma_{\\mathrm{SGD}})$ . Initialize persistent image bank as $N_{\\mathrm{data}}$ uniform noise images. for $j{=}1{:}(J{+}1)$ do ", "page_idx": 15}, {"type": "text", "text": "1. Draw batch images $\\{x_{(i)}^{+}\\}_{i=1}^{m}$ from training set, where $(i)$ indicates a randomly selected index for sample $i$ , and get samples $X_{i}^{+}=x_{(i)}+\\tau_{\\mathrm{data}}\\epsilon_{i}$ , where i.i.d. $\\epsilon_{i}\\sim\\mathsf{N}(0,I_{D})$ . ", "page_idx": 15}, {"type": "text", "text": "2. Draw initial negative samples $\\{Y_{i}^{(0)}\\}_{i=1}^{m}$ from persistent image bank. Update $\\{Y_{i}^{(0)}\\}_{i=1}^{m}$ with the Langevin equation ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\boldsymbol{Y}_{i}^{(k)}=\\boldsymbol{Y}_{i}^{(k-1)}-\\Delta\\tau\\nabla_{Y_{\\tau}}f_{\\theta_{j}}(Y_{i}^{\\tau-1})+\\sqrt{2\\Delta\\tau}\\boldsymbol{\\epsilon}_{i,k},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\epsilon_{i,k}\\sim\\mathbf{N}(0,I_{D})$ i.i.d., for $K$ steps to obtain samples $\\{X_{i}^{-}\\}_{i=1}^{m}=\\{Y_{i}^{(K)}\\}_{i=1}^{m}$ . Update persistent image bank with images $\\{Y_{i}^{(K)}\\}_{i=1}^{m}$ ", "page_idx": 15}, {"type": "text", "text": "3. Update the weights by $\\theta_{j+1}=\\theta_{j}-g(\\Delta\\theta_{j})$ , where $g$ is the optimizer and ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\Delta\\theta_{j}=\\frac{\\partial}{\\partial\\theta}\\left(\\frac{1}{n}\\sum_{i=1}^{n}f_{\\theta_{j}}(X_{i}^{+})-\\frac{1}{m}\\sum_{i=1}^{m}f_{\\theta_{j}}(X_{i}^{-})\\right)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "is the ML gradient approximation. ", "page_idx": 15}, {"type": "text", "text": "end for ", "page_idx": 15}, {"type": "text", "text": "where $\\beta_{t}\\in(0,1)$ is a variance schedule. After $T$ steps, $x_{T}$ is nearly an isotropic Gaussian distribution. This reverse process is parameterized by a neural network and trained to de-noise a variable from the prior to match the real data distribution. Generating from a standard DDPM involves drawing samples from the prior, and then running the learned de-noising process to gradually remove noise and yield a final sample. ", "page_idx": 15}, {"type": "text", "text": "B PUREGEN Further Intuition ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "B.1 Why EBM Langevin Dynamics Purify ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The theoretical basis for eliminating adversarial signals using MCMC sampling is rooted in the established steady-state convergence characteristic of Markov chains. The Langevin update, as specified in Equation (4), converges to the distribution $p_{\\theta}(x)$ learned from unlabeled data after an infinite number of Langevin steps. The memoryless nature of a steady-state sampler guarantees that after enough steps, all adversarial signals will be removed from an input sample image. Full mixing between the modes of an EBM will undermine the original natural image class features, making classification impossible [29]. Nijkamp et al. [2020] reveals that without proper tuning, EBM learning heavily gravitates towards non-convergent $M L$ where short-run MCMC samples have a realistic appearance and long-run MCMC samples have unrealistic ones. In this work, we use image initialized convergent learning. $p_{\\theta}(x)$ is described further by Algorithm 1 [48]. ", "page_idx": 15}, {"type": "text", "text": "The metastable nature of EBM models exhibits characteristics that permit the removal of adversarial signals while maintaining the natural image\u2019s class and appearance [29]. Metastability guarantees that over a short number of steps, the EBM will sample in a local mode, before mixing between modes. Thus, it will sample from the initial class and not bring class features from other classes in its learned distribution. Consider, for instance, an image of a horse that has been subjected to an adversarial $\\ell_{\\infty}$ perturbation, intended to deceive a classifier into misidentifying it as a dog. The perturbation, constrained by the $\\ell_{\\infty}$ -norm ball, is insufficient to shift the EBM\u2019s recognition of the image away from the horse category. Consequently, during the brief sampling process, the EBM actively replaces the adversarially induced \u2018dog\u2019 features with characteristics more typical of horses, as per its learned distribution resulting in an output image resembling a horse more closely than a dog. It is important to note, however, that while the output image aligns more closely with the general characteristics of a horse, it does not precisely replicate the specific horse from the original, unperturbed image. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "We use mid-run chains for our EBM defense to remove adversarial signals while maintaining image features needed for accurate classification. The steady-state convergence property ensures adversarial signals will eventually vanish, while metastable behaviors preserve features of the initial state. We can see PUREGEN-EBM sample purification examples in Fig. 5 below and how clean and poisoned sampled converge and poison perturbations are removed in the mid-run region (100-2000 steps for us). ", "page_idx": 16}, {"type": "image", "img_path": "ZeihWodDVh/tmp/87613faede2c1f041def880b598615019a372d12596d15e36978ef929e72dd35.jpg", "img_caption": ["Figure 5: PUREGEN-EBM purification with various MCMC steps "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Our experiments show that the mid-run trajectories (100-1000 MCMC steps) we use to preprocess the dataset $\\mathcal{X}$ capitalize on these metastable properties by effectively purifying poisons while retaining high natural accuracy on $F_{\\phi}(x)$ with no training modification needed. Intuitively, one can think of the MCMC process as a directed random walk toward a low-energy, more probable natural image version of the original image. The mid-range dynamics stay close to the original image, but in a lower energy manifold which removes the majority of poisoned perturbations. ", "page_idx": 16}, {"type": "text", "text": "B.2 PUREGEN Additional L2 Dynamics Images ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Additional L2 Dynamics specifically for Narcissus $\\epsilon=16$ are shown in Figures 6 and 7. ", "page_idx": 16}, {"type": "image", "img_path": "ZeihWodDVh/tmp/466926573ca18ba68fd5a56c16e6c7e120275d1bdaee164d747c8a4259bf1920.jpg", "img_caption": ["Figure 6: L2 Dynamics for Narcissus $\\epsilon=16$ "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "ZeihWodDVh/tmp/91ffb1414ee8b1bf39b1d2f39e7f2519abe727084f4613d9ae24eae98f4df006.jpg", "img_caption": ["Figure 7: Energy distributions with Narcissus $\\epsilon=16$ poison. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "C PUREGEN Extensions on Higher Power Attacks ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We continue with the notation introduced in 3.3, where $\\Psi_{T}(x)$ is the random variable of a fixed image $x$ , and we define $T=(T_{\\mathrm{EBM}},T_{\\mathrm{DDPM}},T_{\\mathrm{Reps}})\\in\\mathbb{R}^{3}$ , where $T_{\\mathrm{EBM}}$ represents the number of EBM MCMC steps, $T_{\\mathrm{DDPM}}$ represents the number of diffusion steps, and $T_{\\mathrm{REPS}}$ represents the number of times these steps are repeated. ", "page_idx": 17}, {"type": "text", "text": "To incorporate EBM filtering, we order $\\bar{D_{m a x}^{(k)}}\\cup\\bar{D_{m i n}^{(1-k)}}$ , where $\\mathcal{D}_{m a x}^{(k)}$ co $k|\\mathcal{D}|$ $\\mathcal{D}$ by datapoints with the maximal energy (where ${\\mathcal{G}}_{\\theta}(x)$ and partition the ordering based on $k\\,=\\,1$ $k$ into results in purifying everything and $k=0$ is traditional training). Then, with some abuse of notation, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\Psi_{T,k}(\\mathcal{D})=\\Psi_{T}(\\mathcal{D}_{m a x}^{(k)})\\cup\\mathcal{D}_{m i n}^{(1-k)}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "To leverage the strengths of both PUREGEN-EBM and PUREGEN-DDPM, we propose PUREGEN combinations: ", "page_idx": 17}, {"type": "text", "text": "1. PUREGEN-NAIVE $(\\Psi_{T|T_{E B M}>0,T_{D D P M}>0,T_{R e p s}=1})$ : Apply a fixed number of PUREGENEBM steps followed by PUREGEN-DDPM steps. While this approach does improve the purification results compared to using either method alone, it does not fully exploit the synergy between the two techniques.   \n2. PUREGEN-REPS $(\\Psi_{T|T_{E B M}>0,T_{D D P M}>0,T_{R e p s}>1})$ : To better leverage the strengths of both methods, we propose a repetitive combination, where we alternate between a smaller number of PUREGEN-EBM and PUREGEN-DDPM steps for multiple iterations.   \n3. PUREGEN-FILT $(\\Psi_{T,k|T_{E B M}\\geq0,T_{D D P M}\\geq0,0<k<1})$ : In this combination, we first use PUREGEN-EBM to identify a percentage of the highest energy points in the dataset, which are more likely to be samples with poisoned perturbations as shown in Fig. 1. We then selectively apply PUREGEN-EBM or PUREGEN-DDPM purification to these high-energy points. ", "page_idx": 17}, {"type": "text", "text": "We note that methods 2 and 3 require extensive hyperparameter search with performance sweeps using the HLB model in App F, as there was little intuition for the amount of reps $(T_{R e p s})$ or the flitering threshold $(k)$ needed. Thus, we do not include these methods in our core results, but instead show the added performance gains on higher power poisons in Table 4, both in terms of increased perturbation size $\\epsilon=16$ and increased poison $\\%$ (and both together). We note that $10\\%$ would mean the adversary has poisoned the entire class in CIFAR-10 with an NS trigger, and $\\epsilon=16$ is starting to approach visible perturbations, but both are still highly challenging scenarios worth considering for purification. ", "page_idx": 17}, {"type": "text", "text": "D Poison Sourcing and Experiment Implementation Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Triggerless attacks GM and BP poison success refers to the number of single-image targets successfully flipped to a target class (with 50 or 100 target image scenarios) while the natural accuracy is averaged across all target image training runs. Triggered attack Narcissus poison success is measured as the number of non-class samples from the test dataset shifted to the trigger class when the trigger is applied, averaged across all 10 classes, while the natural accuracy is averaged across the 10 classes on the un-triggered test data. We include the worst-defended class poison success. The Poison Success Rate for a single experiment can be defined for triggerless $P S R_{n o t r}$ and triggered $P S R_{t r}$ poisons as: ", "page_idx": 18}, {"type": "equation", "text": "$$\nP S R_{n o t r}(F,i)=\\mathbb{1}_{F(x_{i}^{\\pi})=y_{i}^{\\mathrm{adv}}}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\nP S R_{t r}(F,y^{\\pi})=\\frac{\\sum_{(x,y)\\in{\\mathcal{D}}_{t e s t}\\backslash{\\mathcal{D}}_{t e s t}^{\\pi}}\\mathbb{1}_{F(x+\\rho^{\\pi})=y^{\\pi}}}{|{\\mathcal{D}}_{t e s t}\\backslash\\mathcal{D}_{t e s t}^{\\pi}|}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Note that all results except for Poison Success Rate for GM and BP attacks have a standard deviation, since those attacks are based on a single image class flip for a single classifiers training run. We do provide a subset with results for a single poison paradigm of BP and GM in App. E.2.3 where we used 3 different seeds for the training 3 classifiers for each of the 50 and 100 runs respectively to get a standard deviation, showing these results are low variance relative to the difference in results between baselines and our method. The compute required to collect such results across all scenarios would be extensive. . ", "page_idx": 18}, {"type": "text", "text": "D.1 Poison Sourcing ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "D.1.1 Bullseye Polytope ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The Bullseye Polytope (BP) poisons are sourced from two distinct sets of authors. From the original authors of BP [9], we obtain poisons crafted specifically for a black-box scenario targeting ResNet18 and DenseNet121 architectures, and grey-box scenario for MobileNet (used in poison crafting). These poisons vary in the percentage of data poisoned, spanning $1\\%$ , $2\\%$ , $5\\%$ and $10\\%$ for the linear-transfer mode and a single $1\\%$ fine-tune mode for all models over a 500 image transfer dataset. Each of these scenarios has 50 datasets that specify a single target sample in the test-data. We also use a benchmark paper that provides a pre-trained white-box scenario on CIFAR-100 [50]. This dataset includes 100 target samples with strong poison success, but the undefended natural accuracy baseline is much lower. ", "page_idx": 18}, {"type": "text", "text": "D.1.2 Gradient Matching ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "For GM, we use 100 publicly available datasets provided by [8]. Each dataset specifies a single target image corresponding to 500 poisoned images in a target class. The goal of GM is for the poisons to move the target image into the target class, without changing too much of the remaining test dataset using gradient alignment. Therefore, each individual dataset training gives us a single datapoint of whether the target was correctly moved into the poisoned target class and the attack success rate is across all 100 datasets provided. ", "page_idx": 18}, {"type": "text", "text": "D.1.3 Narcissus ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "For Narcissus triggered attack, we use the same generating process as described in the Narcissus paper, we apply the poison with a slight change to more closely match with the baseline provided by [50]. We learn a patch with $\\epsilon=8/255$ on the entire 32-by-32 size of the image, per class, using the Narcissus generation method. We keep the number of poisoned samples comparable to GM for from-scratch experiment, where we apply the patch to 500 images ( ${}[\\%$ of the dataset) and test on the patched dataset without the multiplier. In the fine-tune scenarios, we vary the poison $\\%$ over $1\\%$ , $2.5\\%$ , and $10\\%$ , by modifying either the number of poisoned images or the transfer dataset size (specifically 20/2000, 50/2000, 50/500 poison/train samples). ", "page_idx": 18}, {"type": "text", "text": "D.1.4 Neural Tangent Availability Attacks ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "For Neural Tangent Availability Attack, the full NTGA dataset (all samples poisoned) is sourced from the authors of the original NTGA attack paper [12]. Baseline defenses are pull from AVATAR [30]. ", "page_idx": 19}, {"type": "text", "text": "D.2 Training Parameters ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We follow the training hyperparameters given by [18, 4, 9, 50] for GM, NS, BP Black/Gray-Box, and BP White-Box respectively as closely as we can, with moderate modifications to align poison scenarios. HyperlightBench training followed the original creators settings and we only substituted in a poisoned dataloader [46]. ", "page_idx": 19}, {"type": "table", "img_path": "ZeihWodDVh/tmp/a4e064c38b6c17dee932887bb0fd5abfc00d62c3e4423935de5ad0e3915c8119.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "D.3 Core Results Compute ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Training compute for core result only which is in Table 1 on a TPU V3. ", "page_idx": 19}, {"type": "table", "img_path": "ZeihWodDVh/tmp/7c72b47fda66e341595df795a568d4bcff274fb4486679b9e2a0bcf9b153c7cb.jpg", "table_caption": ["Table 6: Compute Hours TPU V3 "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "E Additional Results ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "E.1 Extended Core Results ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "E.1.1 Full Results Primary Experiments ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Results on all primary poison scenarios with ResNet18 classifier including all EPIC versions (various subset sizes and selection frequency), FRIENDS versions (bernouilli or gaussian added noise trasnform), and all natural JPEG versions (compression ratios). Green highlight indicates a baseline defense that was selected for the main paper results table chosen by the best poison defense performance that did not result in significant natural accuracy degradaion. For both TinyImageNet and CINIC-10 from-scratch results, best performing baseline settings were used from respective poison scenarios in CIFAR-10 for compute reasons (so there are no additonal results and hence they are removed from this table). ", "page_idx": 19}, {"type": "table", "img_path": "ZeihWodDVh/tmp/414a4aa8f4e32d9f3b38737b87cbabf1b3ad79b14203021722dd1b46c1e3d179.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "ZeihWodDVh/tmp/ba628a9a9055380f2f1ff148ca0f7ef2bbfd4f051e7a4265422e43b5918a2a62.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "E.1.2 From Scratch 80 Epochs Experiments ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Baseline FRIENDS [26] includes an 80-epoch from-scratch scenario to show poison defense on a faster training schedule. None of these results are included in the main paper, but we show again SoTA or near SoTA for PUREGEN against all baselines (and JPEG is again introduced as a baseline). ", "page_idx": 20}, {"type": "table", "img_path": "ZeihWodDVh/tmp/a3f0bf28e3eb680442dbc50a39f1cafc996976fb80bfdfbf9fb5d85b435fd060.jpg", "table_caption": ["Table 7: From-Scratch 80-Epochs Results (ResNet-18, CIFAR-10) "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "E.1.3 Full Results for MobileNetV2 and DenseNet121 ", "text_level": 1, "page_idx": 21}, {"type": "table", "img_path": "ZeihWodDVh/tmp/512b3e8b2f6fd223355b3769aec9a04b14a3504cabf5d94f2de7ec82c5c54718.jpg", "table_caption": ["Table 8: MobileNetV2 Full Results "], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "ZeihWodDVh/tmp/0ff8f3dcf53ae88fd8eedb393ccb1d099d1283cabe61a14ab62ca7ac0b409f25.jpg", "table_caption": ["Table 9: DenseNet121 Full Results "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "E.1.4 PUREGEN Combos on Increased Poison Power ", "text_level": 1, "page_idx": 21}, {"type": "table", "img_path": "ZeihWodDVh/tmp/10538f97fcdad724cbd784ad5ba534e71e140cd18a4f1787e16893e19f843ccf.jpg", "table_caption": ["Table 10: PUREGEN Combos with Narcissus Increased Poison $\\%$ and $\\epsilon$ "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "E.2 Additional Experiment Results ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "E.2.1 Defense/EBM-Aware Narcissus Experiments ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "To further demonstrate the robustness of PUREGEN, we conduct an additional experiment simulating a scenario where an adversary is aware that PUREGEN is in use. Specifically, we incorporate the Energy-Based Model (EBM) within the Narcissus poison generation framework. Narcissus generates poisons by training a surrogate model, then refining the poison through SGD on frozen surrogate outputs (further details in [4]). For simplicity, we assume that the adversary possesses knowledge of the EBM defense mechanism but is unaware of the specific image instances ultimately used during training. ", "page_idx": 21}, {"type": "text", "text": "In this experiment, we include our pre-trained EBM as a preprocessing layer for the ResNet-based surrogate model, using 50 EBM steps\u2014\u2013a choice that balances computational efficiency with reliable performance. When training the EBM-aware surrogate, we freeze the EBM to allow the classifier to train on a diverse set of stochastic EBM outputs. After a brief warmup phase, the entire surrogate model is frozen, and we propagate gradients to optimize poison effectiveness. We test variations of when the surrogate and poison generation do or do not have EBM information, applied to 3 of the 10 CIFAR-10 classes. The results are recorded in Table 11. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "Our results show that PUREGEN consistently defends against all new EBM-aware Narcissus poisons, demonstrating the robustness of our approach. Interestingly, we observe that when both the surrogate and poison-generating models include the EBM, the Narcissus method struggles to produce effective poisons, even in an undefended model context. Conversely, with a traditionally trained surrogate model but EBM augmentation in the poison generation process, the new poisons slightly improve efficacy on an undefended model. Overall, these findings reinforce the reliability of PUREGEN as a robust defense mechanism against adaptive poisoning attacks. ", "page_idx": 22}, {"type": "table", "img_path": "ZeihWodDVh/tmp/ea099794253c2f21f0ebc423516b266c4646ddd9781b744ecceb43ae231593b4.jpg", "table_caption": ["Table 11: Success Rates of EBM-Aware Narcissus Poisons Against PUREGEN Defense on classes 0,1,2. This table shows that even with the knowledge of the EBM, Narcissus is unable to generate effective poisons against PUREGEN. Furthermore, when the EBM is taken in the entire Narcissus process, Narcissus struggles to find a poison that works for undefended models. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "E.2.2 Pre-Trained Public DDPM Comparison ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We include results using two pre-trained diffusion models from HuggingFace [51, 52]. The results show that these models can achieve defense performance similar to that of some of our POOD in-house trained models. The table below includes 4 baseline PUREGEN models and the two Hugging Face models trained on butterflies and anime datasets, showing both are comparable for poison defense and natural accuracy to some POOD datasets in performance. These results how that, for PUREGEN-DDPM pre-trained models could be adequate, but come with the risks of using a model with unknown data security. We reiterate our primary contribution for PUREGEN-DDPM was in reducing training and improving performance for a given architecture and dataset if one needs to train a diffusion model and if purification is the known use-case. ", "page_idx": 22}, {"type": "table", "img_path": "ZeihWodDVh/tmp/015d082dd39b854a0ac9191596511233a576dbaadee41e14eb0c5fefca114e7b.jpg", "table_caption": ["Table 12: Two pre-trained diffusion models from HuggingFace, showing similar results to our POOD DDPM results on Narcissus From-Scratch attack [51, 52]. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "E.2.3 GM and BP Poison Success Standard Deviation ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Table 13: Core results poison success for one GM and one BP scenario where we compute poison success across 3 different seeds to show the relatively low variance of these results where our method is still SoTA. ", "page_idx": 23}, {"type": "table", "img_path": "ZeihWodDVh/tmp/fd47ea3faa6dcd5e67aad7143146efc88535c35256f8d98ebd9e191d67393d4b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "ZeihWodDVh/tmp/dc720ed4d018971ab961ca3d8971e039ccee24e0827ebfb8ec17d4bb0dc22660.jpg", "img_caption": ["F PUREGEN Extensions T Sweeps ", "Figure 8: PUREGEN-REPS Sweeps with HLB Model on Narcissus "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "ZeihWodDVh/tmp/fcc2ca81ecbfd9b095c6a5dfe14cfa1471f7f86d23cd81b7b54041534cf7ec44.jpg", "img_caption": ["Figure 9: PUREGEN-NAIVE Sweeps with HLB Model on Narcissus "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "ZeihWodDVh/tmp/c6853ab7b266669f35bcccc31052b084b44cc481676513ce2e3f84ca844fd159.jpg", "img_caption": ["Figure 10: PUREGEN-FILT Sweeps with HLB Model on Narcissus "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "G Interpreting PUREGEN Results ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "G.1 Model Interpretability ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Using the Captum interpretability library, in Figure 11, we compare a clean model with clean data to various defense techniques on a sample image poisoned with the NS Class 5 trigger $\\rho$ [53]. Only the clean model and the model that uses PUREGEN-EBM correctly classify the sample as a horse, and the regions most important to prediction, via occlusion analysis, most resemble the shape of a horse in the clean and PUREGEN-EBM images. Integrated Gradient plots show how PUREGEN-EBM actually enhances interpretability of relevant features in the gradient space for prediction compared to even the clean NN. ", "page_idx": 25}, {"type": "image", "img_path": "ZeihWodDVh/tmp/9c3136d0d3a06e6e83f609795a70ddc598ef7384b72fd8addad9b9abbb91061f.jpg", "img_caption": ["Figure 11: Defense Interpretability: Model using PUREGEN-EBM focuses on the outline of the horse in the occlusions analysis and to a higher degree on the primary features in the gradient space than even the clean model on clean data. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "G.2 Differences between PUREGEN-EBM and PUREGEN-DDPM ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In this section we visualize a Narcissus $\\epsilon=64$ trigger patch to better see the PUREGEN-EBM and PUREGEN-DDPM samples on a visible perturbation. In Figure 12 we see again how the EBM struggles to purify larger perturbations but better preserves the image content, while DDPM can degrade such perturbations better at the cost of degrading the image content as well. ", "page_idx": 27}, {"type": "image", "img_path": "ZeihWodDVh/tmp/66b33a4a0dbaac207c5245aa98217a6d2a6704860f3117da06924f3a5325fa6e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Figure 12: Narcissus $\\epsilon=64$ trigger patch purification samples Top Left: Original Poisoned. Top Right: PUREGEN-EBM 500 Steps. Top Left: PUREGEN-DDPM 75 Steps. ", "page_idx": 27}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 28}, {"type": "text", "text": "Answer:[Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: The claims in the abstract and introduction are supported by the detailed methodologies and comprehensive experimental results presented in Sections 3 and 4, respectively. These sections demonstrate the effectiveness and universality of PUREGEN methods in defending against a variety of train-time poisoning attacks. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 28}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 28}, {"type": "text", "text": "Answer:[Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: The limitations are discussed in Section 4.5, where we address the computational cost, data requirements for training the generative models, and the trade-offs between purification times and performance. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 28}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: The theoretical results, assumptions, and proofs are detailed in Sections 3.1 and 3.2, with additional mathematical formulations provided in the appendices. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 29}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper includes a commitment to release code and models upon publication (along with an anonymous codebase attached for submission). Detailed instructions for reproducing the experiments are provided in the Appendix. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in ", "page_idx": 29}, {"type": "text", "text": "some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 30}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: All relevant training and testing details, including data splits, hyperparameters, and optimizer settings, are thoroughly described in Section 4.1 and Appendix D. The paper includes a commitment to release code and models upon publication (along with an anonymous codebase attached for submission). ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 30}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: Training and test details are, to the best extent of the authors, following previous benchmarks provided by previous poisons authors [50, 18, 4, 9]. Otherwise, hyperparameter explanations specific to PUREGEN are given in Section 3.4. The details of all hyperparameters used are listed in App. D.2. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 30}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: The results include error bars in all experiential results in Section 4. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 31}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Information on the compute resources used, including the type of hardware (TPU V3), memory, and execution time, is provided in Section 4.5 and Table 5. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 31}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The research adheres to the NeurIPS Code of Ethics, ensuring responsible use of AI technologies and addressing potential ethical concerns related to data poisoning and model security, as discussed in the \u201cPotential Social Impacts\u201d Section 6. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 31}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The social impacts section in App. 6 provides a balanced discussion of both positive and negative societal impacts of the research, including ethical considerations and the potential for misuse. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 32}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: The research does not involve the release of high-risk models or datasets that require specific safeguards, but broader risks of the research in general is discussed in Section 6. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 32}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: All datasets and models used are properly cited in Section 4 or in Appendix D. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 33}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: Code and models will be released upon acceptance. Anonymous code is attached with submission. No additional assets. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 33}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: No Human Subjects ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 33}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: No Human Subjects ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 34}]