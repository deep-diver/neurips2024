[{"figure_path": "UJ9k3j93MD/figures/figures_8_1.jpg", "caption": "Figure 1: Test losses of FNN and DEQ networks with various width W and depth L. (a) and (d) apply Sawtooth function I and II with 25 and 210 linear regions, respectively. (b) and (e) apply function g(x) defined in Eq. (5) with d = 2-10 and \u03b4 = 2-20, respectively. (c) and (f) show the train loss and the GOTU error of FNN and DEQ on the boolean function f1, f2 with unseen domain given by Eq. (14) and Eq. (15).", "description": "This figure compares the performance of Feedforward Neural Networks (FNN) and Deep Equilibrium models (DEQ) on six different tasks. The first two are the sawtooth functions with varying numbers of linear regions, showcasing expressivity.  The next two involve approximating a steep function, highlighting DEQ's ability to handle such functions.  The final two demonstrate the out-of-distribution (OOD) generalization performance on Boolean functions, comparing training loss and GOTU error.  Different network sizes and depths (width W and depth L) are tested to demonstrate the parameter efficiency of DEQ.", "section": "6 Experiments"}, {"figure_path": "UJ9k3j93MD/figures/figures_9_1.jpg", "caption": "Figure 2: The heatmaps of diagonal DEQ, vanilla DEQ and FNN. We dispaly the magnitude of feature z of DEQ and the magnitude of feature before the fully-connented layer of FNN. The x-axis represents features 1-20 and darker colors indicate smaller features.", "description": "This figure compares the feature distributions learned by three different models: a diagonal linear DEQ, a vanilla DEQ, and an FNN.  Each heatmap shows the magnitude of 20 features; darker colors indicate smaller magnitudes. The purpose is to visually demonstrate a difference in the implicit bias of DEQs, showing they tend to learn features with relatively similar magnitudes compared to FNNs.", "section": "5 The Bias on Learning Dynamics of DEQ"}, {"figure_path": "UJ9k3j93MD/figures/figures_26_1.jpg", "caption": "Figure 3: Test loss of FNN and DEQ trained on sawtooth functions with 21, 23, 215 linear regions.", "description": "This figure displays the test loss curves for both Feedforward Neural Networks (FNN) and Deep Equilibrium models (DEQ) trained on sawtooth functions with varying numbers of linear regions (2\u00b9, 2\u00b3, 2\u00b9\u2075).  The results demonstrate the superior performance of DEQs in approximating functions with a large number of linear regions, a key finding supporting the paper's claim about DEQ's enhanced expressiveness.", "section": "6 Experiments"}, {"figure_path": "UJ9k3j93MD/figures/figures_26_2.jpg", "caption": "Figure 4: Results on steep target functions applying DEQ and FNN with comparable performance on loss. We plot the infinity norm of the weights of DEQ and FNN with d = 6, 8, 10, 12, 14, 16, 18, 20.", "description": "This figure compares the infinity norm of the weights of DEQ and FNN models trained on steep target functions with different input dimensions (d). The results show that DEQ maintains relatively small weights even when the input dimension increases, while FNN's weights grow significantly.", "section": "Experiments"}, {"figure_path": "UJ9k3j93MD/figures/figures_27_1.jpg", "caption": "Figure 1: Test losses of FNN and DEQ networks with various width W and depth L. (a) and (d) apply Sawtooth function I and II with 25 and 210 linear regions, respectively. (b) and (e) apply function g(x) defined in Eq. (5) with d = 2-10 and \u03b4 = 2-20, respectively. (c) and (f) show the train loss and the GOTU error of FNN and DEQ on the boolean function f1, f2 with unseen domain given by Eq. (14) and Eq. (15).", "description": "This figure compares the performance of Feedforward Neural Networks (FNN) and Deep Equilibrium Models (DEQ) on six different tasks.  The tasks are designed to highlight the relative strengths of each model architecture, testing expressivity (sawtooth functions and steep functions) and generalization (out-of-distribution, or OOD, tasks on boolean functions). The results show that DEQ is more parameter-efficient and demonstrates better generalization performance for certain tasks, consistent with the paper's theoretical findings.", "section": "6 Experiments"}, {"figure_path": "UJ9k3j93MD/figures/figures_28_1.jpg", "caption": "Figure 6: The saliency map of Multiscale DEQ (MDEQ) and ResNet.", "description": "This figure presents a comparison of saliency maps generated by Multiscale DEQ (MDEQ) and ResNet-50.  Saliency maps highlight the image regions most influential in a model's prediction.  The images show that MDEQ attends to a broader range of features (e.g., fences, trees) compared to ResNet-50, which focuses more narrowly on the horses. This supports the paper's claim that DEQs learn 'denser' features.", "section": "Experiments"}, {"figure_path": "UJ9k3j93MD/figures/figures_28_2.jpg", "caption": "Figure 6: The saliency map of Multiscale DEQ (MDEQ) and ResNet.", "description": "The figure shows the saliency map generated by Grad-CAM for both MDEQ and ResNet-50 on images of horses and dogs. The saliency map highlights image regions that contribute the most to the model's prediction.  The comparison aims to illustrate that MDEQ produces a more distributed set of salient features compared to ResNet-50, which tends to focus on a smaller number of regions. This supports the paper's argument that DEQs are biased towards learning dense features.", "section": "Experiments"}, {"figure_path": "UJ9k3j93MD/figures/figures_28_3.jpg", "caption": "Figure 1: Test losses of FNN and DEQ networks with various width W and depth L. (a) and (d) apply Sawtooth function I and II with 25 and 210 linear regions, respectively. (b) and (e) apply function g(x) defined in Eq. (5) with d = 2-10 and 8 = 2-20, respectively. (c) and (f) show the train loss and the GOTU error of FNN and DEQ on the boolean function f1, f2 with unseen domain given by Eq. (14) and Eq. (15).", "description": "This figure compares the performance of Feedforward Neural Networks (FNN) and Deep Equilibrium Models (DEQ) on six different tasks, showcasing DEQ's potential advantages in specific scenarios.  The tasks involve approximating functions with varying complexities, including those with many linear regions (sawtooth functions) and steep functions (g(x)).  Additionally, the figure illustrates the performance on out-of-distribution (OOD) tasks, where the models are evaluated on data unseen during training.  The results suggest that DEQ outperforms FNN in several cases, particularly when approximating functions with high complexity or when handling OOD data.", "section": "6 Experiments"}]