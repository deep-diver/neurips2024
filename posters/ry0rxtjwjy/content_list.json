[{"type": "text", "text": "Learning to Balance Altruism and Self-interest Based on Empathy in Mixed-Motive Games ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Fanqi Kong2,1 Yizhe Huang2,1 Song-Chun Zhu1,2,3 Siyuan Qi1 Xue Feng  1 ", "page_idx": 0}, {"type": "text", "text": "1State Key Laboratory of General Artificial Intelligence, BIGAI 2Institute for Artificial Intelligence, Peking University 3Department of Automation, Tsinghua University kfq20@stu.pku.edu.cn, fengxue@bigai.ai ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Real-world multi-agent scenarios often involve mixed motives, demanding altruistic agents capable of self-protection against potential exploitation. However, existing approaches often struggle to achieve both objectives. In this paper, based on that empathic responses are modulated by inferred social relationships between agents, we propose LASE (Learning to balance Altruism and Self-interest based on Empathy), a distributed multi-agent reinforcement learning algorithm that fosters altruistic cooperation through gifting while avoiding exploitation by other agents in mixed-motive games. LASE allocates a portion of its rewards to co-players as gifts, with this allocation adapting dynamically based on the social relationship \u2014 a metric evaluating the friendliness of co-players estimated by counterfactual reasoning. In particular, social relationship measures each co-player by comparing the estimated $Q$ -function of current joint action to a counterfactual baseline which marginalizes the co-player\u2019s action, with its action distribution inferred by a perspective-taking module. Comprehensive experiments are performed in spatially and temporally extended mixed-motive games, demonstrating LASE\u2019s ability to promote group collaboration without compromising fairness and its capacity to adapt policies to various types of interactive co-players. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Multi-agent reinforcement learning (MARL) has exhibited impressive performance in numerous collaborative tasks and zero-sum games such as MPE, StarCraft, and Google Research Football [19, 25, 38]. These environments involve a predefined competitive or cooperative relationship between agents. Besides, mixed-motive games are prevalent, in which the relationships between agents are non-deterministic and dynamic. That is, agents could cooperate with some co-players and simultaneously compete with someone else. Furthermore, along with interactions, friends may turn into foes, and vice versa. In such games, to maximize self-interest, agents need to cooperate altruistically in some relationships while keep self-interested to avoid being exploited in some others. Consequently, in mixed-motive environments, the ability to balance altruism and self-interest according to social relationships is crucial for agent performance. ", "page_idx": 0}, {"type": "text", "text": "The commonly used CTDE (Centralized Training and Decentralized Execution) methods [31, 29] in MARL focus on global optimization goals and necessitate individual information sharing with centralized controllers, which is impractical for self-interest agents in mixed-motive games. On the other hand, simply training self-interest agents in a decentralized way may converge to local optima, failing to maximize individual interests. For example, in Iterated Prisoner\u2019s Dilemma (IPD), decentralized A2C agents converge to defection, getting the minimal reward 0 (see details in Fig. 4). ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Within the framework of decentralized learning, the gifting mechanism has been used to address the decision-making problems in mixed-motive games by enabling agents to transfer a portion of their rewards to others [20, 4]. Agents independently select gift recipients and determine gift amount, which complies with the decentralized requirement. Gifting can potentially shape co-players\u2019 policies and even incentivize them to behave more altruistically by influencing co-players\u2019 reward structure. Previous work has studied handcrafted gifting scheme [20, 34] and has learned end-to-end neural networks to determine the reward transfer scheme [36, 37]. However, a consideration of the correlation between social relationships and response strategies is absent, which is crucial for decision-making in mixed-motive games. ", "page_idx": 1}, {"type": "text", "text": "To balance the dilemma between altruism and self-interest caused by the non-deterministic and dynamic relationship between agents, it is a feasible way that adaptively modulating the gift amount to others according to the social relationships. This process is called (cognitive) empathy in developmental psychology[28]. What\u2019s more, previous studies and human behavioral experiments have shown that empathy can promote the emergence of altruism among self-interested individuals [1, 2, 30]. To the best of our knowledge, there has been a lack of computational models of empathy and the study of empathy-based decision-making. In this work, we propose a computational model of empathy, in which social relationship is measured by a continuous variable, capturing the influence of co-players\u2019 behavior on the focal agent\u2019s reward and guiding gift scheme. On that basis, we provide a distributed MARL algorithm LASE (Learning to balance Altruism and Self-interest based on Empathy) to address the dilemma between altruism and self-interest in mixed-motive games. ", "page_idx": 1}, {"type": "text", "text": "LASE uses counterfactual reasoning to infer the different social relationships with different coplayers separately by comparing the estimated $Q.$ -value for the joint action to a counterfactual baseline established for each other agent. The counterfactual baseline marginalizes a single agent\u2019s action while keeping the other agents\u2019 actions fixed. This computational approach to social relationships enables LASE to explicitly decompose the value contributions of other agents to it, thereby providing clearer guidance for gift allocation. That is, gift more to the co-players who contribute more. Additionally, to deal with the challenge of inferring co-players in partially observable and decentralized environments, LASE is equipped with a perspectivec taking module to predict others\u2019 policies by converting LASE\u2019s local observation to a simulated observation of others. ", "page_idx": 1}, {"type": "text", "text": "To verify the effectiveness of LASE, we theoretically analyze its dynamics of decision-making in iterated mixed-motive games and conduct comprehensive experiments in spatially and temporally extended mixed-motive games. The results demonstrate LASE\u2019s ability to promote group cooperation without compromising fairness and its capability of self-protection against potential exploitation. ", "page_idx": 1}, {"type": "text", "text": "This paper makes three main contributions. (1) To our best knowledge, we are the first to computationally model empathy which modulates the response based on inferred social relationships. (2) We present LASE, a decentralized MARL algorithm that balances altruism and self-interest in mixed-motive games. It flexibly adapts strategy to promote cooperation while mitigating exploitation by others. (3) We provide a theoretical analysis of decision dynamics in iterated matrix games and experimentally verify that LASE outperforms baselines in a variety of sequential social dilemmas. ", "page_idx": 1}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In multi-agent learning, the game-theoretic notion of social dilemmas has been generalized from the classic two-player matrix-form games Tab. 1 to sequential social dilemmas, a spatial-temporallyextended complex behavior learning setting [15, 4]. Various approaches have been proposed to foster cooperative behavior among agents to advance societal welfare. One approach incorporates the rewards of others as intrinsic rewards into one\u2019s own optimization objectives, with the ratio of intrinsic to extrinsic rewards determined by different methods, such as pre-defined social value orientations like altruistic or prosocial [21, 23], introducing the concept of inequity aversion [11], or learning in a model-free way [33]. However, this approach depends on direct access to others\u2019 reward functions, which may not be feasible in realistic mixed-motive games. Another line of work dispenses with this assumption, allowing agents to model others and influence them through their actions [6, 17, 13, 8, 10]. Here, we employ a more direct form of opponent shaping named gifting. ", "page_idx": 1}, {"type": "text", "text": "As a peer rewarding mechanism that allows agents to reward other agents as a part of their action space [20], gifting can be viewed as a process of redistributing rewards among agents [12, 9, 7], but the key difference is that in our work, gifting does not require a powerful centralized controller to decide on the allocation. Instead, individuals make their own decisions about gifting, which is more in line with the setting of decentralized training. [34, 35] study the theoretical basis of how gifting promotes cooperative behavior in simple social dilemmas, while LIO [36] independently learns an incentive function to gift others. However, the rewards used for gifting in LIO are determined by the incentive function rather than split from its own reward. LIO doesn\u2019t adhere to zero-sum conditions, which to some extent alters the original game-theoretic nature. LToS [37] models the optimization problem of gifting (sharing) weights with the zero-sum setting as a bi-level problem and uses an end-to-end approach to train weights and policies jointly. MOCA [3] introduces contracts to restrict gifting recipients to the agents that fulfill certain behavioral patterns. ", "page_idx": 2}, {"type": "text", "text": "Our perspective taking module simulates others\u2019 behavior by adopting their perspective, a technique akin to Self-Other Modeling (SOM) [24]. The difference is that instead of inferring the agent\u2019s goal which may not necessarily be well defined in some environments, we directly imagine their observations, and decouple the agent\u2019s network for inferring others\u2019 actions from its own real policy network utilized for execution. In MARL, some prior studies have employed counterfactual reasoning to deduce the impact of individual actions on others [13] or the whole team [5]. In contrast, our focus lies on assessing others\u2019 influence on the focal agent. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Partially observable Markov games ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We consider an $N$ -player partially observable Markov game (POMG) [27, 18], $\\mathcal{M}\\quad=$ $\\left\\langle N,S,\\left\\{\\mathcal{O}^{i}\\right\\},\\left\\{A^{i}\\right\\},\\dot{\\mathcal{P}},\\left\\{\\mathcal{R}^{i}\\right\\}\\right\\rangle$ , where $N$ represents the number of agents, $s\\ \\in{\\cal S}$ represents the state of the environment. In the partially observable setting, agent $i$ only obtains the local observation $o^{i}\\in\\mathcal{O}^{i}$ based on the current state $s$ . Each agent $i$ learns an independent policy $\\pi^{i}(a^{i}|o^{i})$ to select actions and form a joint action $\\mathbf{\\boldsymbol{a}}=(a^{1},...,\\mathsf{\\check{a}}^{N})\\in\\mathcal{A}^{1}\\times\\cdot\\cdot\\cdot\\times\\mathbf{\\boldsymbol{\\dot{A}}}^{N}$ , resulting in the state change from $s$ to $s^{\\prime}$ according to the transition function $\\mathcal{P}:\\mathcal{S}\\times\\mathcal{A}^{1}\\times\\cdot\\cdot\\cdot\\times\\mathcal{A}^{N}\\,\\dot{\\rightarrow}\\,\\Delta(\\mathcal{S})$ , where $\\Delta(S)$ represents a probability distribution over the set $\\boldsymbol{S}$ . Agent $i$ receives an individual extrinsic reward $r^{i^{\\mathbf{\\lambda}}}\\!\\!=\\!\\mathcal{R}^{i}(s,a^{\\mathbf{\\lambda}},\\cdots\\:,a^{N})$ and tries to maximize a long-term return: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{V_{i}^{\\pi}(s_{0})=\\mathbb{E}_{a_{t}\\sim\\pi,s_{t+1}\\sim P(s_{t},a_{t})}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}\\mathcal{R}^{i}(s_{t},a_{t})\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where the variables in bold represent the joint information of all agents, and $\\gamma$ is the discount factor. ", "page_idx": 2}, {"type": "text", "text": "3.2 Policy Gradient Learning ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In decentralized MARL, each self-interest agent $i$ learns an independent policy $\\pi^{i}$ parameterized by $\\theta^{i}$ . The optimization objective is to maximize the expected return in Eq. 1. We use the policybased Actor-Critic method as the learning algorithm for our agents. The gradient for actor is $\\begin{array}{r}{\\nabla_{\\theta}J(\\theta)=\\mathbb{E}_{\\pi_{\\theta}}[\\sum_{t=0}^{T}\\psi_{t}\\nabla_{\\theta}}\\end{array}$ liosg $\\pi_{\\theta}\\big(a_{t}|o_{t}\\big)\\big]$ $\\psi_{t}$ e.valuation of the actor. $\\psi_{t}$ $\\bar{\\psi}_{t}=r_{t}+\\gamma\\bar{V^{\\pi_{\\theta}}}\\!\\left(s_{t+1}\\right)-V^{\\pi_{\\theta}}\\!\\left(s_{t}\\right)$ ", "page_idx": 2}, {"type": "text", "text": "4 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "To balance altruism and self-interest in mixed-motive games, we propose a distributed MARL algorithm LASE which empathically shares rewards with co-players based on inferred social relationships. The architecture of LASE is illustrated in Fig. 1, composed of two main modules: Social Relationship Inference (SRI) and Gifting. SRI conducts counterfactual reasoning to get the social relationships with co-players, which reflects the impact of co-players\u2019 actions on LASE\u2019s return. SRI compares the $Q$ -value (estimated by the SR value network) of the current joint action to a counterfactual baseline which marginalizes the co-player\u2019s action, with its action distribution inferred by a perspective-taking (PT) module. PT is provided to address the challenge of predicting co-players\u2019 policies in partially observable and decentralized environments. In particular, PT consists of an observation conversion network, simulating the co-player $j$ \u2019s observation ${\\hat{o}}^{j}$ from the local observation $o^{i}$ , and an SR policy network, learning a function from $\\hat{o}^{j}$ to the inferred $j$ \u2019s policy. ", "page_idx": 2}, {"type": "text", "text": "The Gifting module, according to the inferred social relationships, determines the amount of reward sharing with others. Meanwhile, agents receive others\u2019 gifts and get the final reward $r^{i,\\mathrm{tot}}$ , which serves as an optimization target to guide the training of policy $\\pi^{i}$ . Section 4.1 and Section 4.2 will introduce our algorithm in detail. LASE\u2019s pseudocode is given as Algorithm 1. ", "page_idx": 2}, {"type": "image", "img_path": "ry0RXTJwjy/tmp/ff44ba5fb752b92e564646baec50a79eb1358902f92dee89d51f906e06765957.jpg", "img_caption": ["Figure 1: Architecture of LASE. It consists of Social Relationships Inference (SRI) and Gifting. SRI conducts counterfactual reasoning to get the social relationships with co-players. The social relationship is measured by comparing the $Q$ -value (estimated by the SR value network) of the current joint action to a counterfactual baseline which marginalizes the co-player\u2019s action, with its action distribution inferred by a perspective-taking (PT) module. PT is provided to address the challenge of predicting co-players\u2019 policies in partially observable and decentralized environments. The Gifting module, according to the inferred social relationships, determines the amount of reward to share. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "4.1 Zero-Sum Gifting ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Here, we use the zero-sum gifting mechanism which adheres to the principle of \u201cwhat I give is what I lose\u201d [20], indicating that the overall rewards for the group remain constant. Notably, gifting is an autonomous decision, wherein any agent $i$ holds a gifting weight vector at time step $t$ : $\\bar{\\boldsymbol{w_{t}^{i}}}=[\\boldsymbol{w_{t}^{\\bar{i}j}}]_{j=1}^{N}$ , where wtij \u2208[0, 1] and jN=1 $\\textstyle\\sum_{j=1}^{N}w_{t}^{i j}=1$ . $w_{t}^{i j}$ is the fraction of agent $i$ \u2019s reward to gift $j$ . It is exactly the social relationship computed as Eq. 3. For an $N$ -player group using the zero-sum gifting mechanism, $r_{t}$ denotes the extrinsic rewards vector obtained through interactions with the environment at timestep $t$ . Agent $i$ \u2019s total reward is computed by ", "page_idx": 3}, {"type": "equation", "text": "$$\nr_{t}^{i,\\mathrm{tot}}({\\bf{w}}_{t},r_{t})=\\sum_{j=1}^{N}w_{t}^{j i}r_{t}^{j}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The policy $\\pi^{i}(a_{t}^{i}|o_{t}^{i})$ is trained to maxmize $\\mathbb{E}_{\\pi^{i}}[\\sum_{t=0}^{T}\\gamma^{t}r_{t}^{i,\\mathrm{tot}}]$ using the TD-error introduced in Section 3.2 by replacing $r_{t}$ with $r_{t}^{\\mathrm{tot}}$ . ", "page_idx": 3}, {"type": "text", "text": "4.2 Social Relationships Inference ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The social relationship $w^{i j}$ , modeled as a continuous variable, measures $i$ \u2019s inference of $j$ \u2019s friendliness to him. Based on counterfactual reasoning, $w^{i j}$ is inferred as: ", "page_idx": 3}, {"type": "equation", "text": "$$\nw^{i j}=\\frac{Q_{\\mathrm{SR}}^{i}(o_{t}^{i},{a}_{t})-\\sum_{a_{t}^{j^{\\prime}}}\\pi_{\\mathrm{SR}}^{i}(a_{t}^{j^{\\prime}}|\\hat{o}_{t}^{j})Q_{\\mathrm{SR}}^{i}(o_{t}^{i},({a}_{t}^{-j},{a}_{t}^{j^{\\prime}}))}{\\mathcal{M}},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $Q_{\\mathrm{SR}}^{i}\\big(o_{t}^{i},a_{t}\\big)$ , estimated by the SR value network, is $i$ \u2019s $Q$ -value of its local observation and the joint action. Eq. 3 compares $Q_{\\mathrm{SR}}^{i}\\big(o_{t}^{i},\\pmb{a}_{t}\\big)$ with a counterfactual baseline, which is the weighted sum of $Q$ -values, with $j$ taking all possible actions $a_{t}^{j^{\\prime}}$ while the other agents\u2019 actions ${\\pmb{a}}^{-j}$ fixed. The weight $\\pi_{\\mathrm{SR}}^{i}(a_{t}^{j^{\\prime}}|\\hat{o}_{t}^{j})$ is $j$ \u2019s policy inferred by $i$ . The denominator is for normalization, $\\begin{array}{r}{\\mathcal{M}=(N-1)(\\operatorname*{max}_{a_{t}^{j^{\\prime}}}Q_{\\mathrm{SR}}^{i}(o_{t}^{i},(\\mathbf{a}_{t}^{-j},a_{t}^{j^{\\prime}})-\\operatorname*{min}_{a_{t}^{j^{\\prime}}}Q_{\\mathrm{SR}}^{i}(o_{t}^{i},(\\mathbf{a}_{t}^{-j},a_{t}^{j^{\\prime}})))}\\end{array}$ , where $(N-1)$ ensures $w^{i j}\\leq\\frac{1}{N-1}$ . After giftting, LASE keeps the remainitng reward for itself, $\\begin{array}{r}{w^{i i}=1-\\sum_{j=1,j\\neq i}^{N}w^{i j}}\\end{array}$ . ", "page_idx": 3}, {"type": "text", "text": "Due to partial observability and decentralized learning, $i$ is unable to accurately obtain $j$ \u2019s policy $\\pi^{j}$ and its observation $o_{t}^{j}$ . So we utilize PT module to estimate $j$ \u2019s policy, denoted as $\\pi_{\\mathrm{SR}}^{i}(a_{t}^{j^{\\prime}}|\\hat{o}_{t}^{j})$ . $\\pi_{\\mathrm{SR}}^{i}$ is the SR policy network parameterized by $\\mu^{i}$ which predicts $j$ \u2019s actions conditioned on the simulated obervation ${\\hat{o}}_{t}^{j}$ learned by the observation conversion network. The observation conversion network, parameterized by $\\eta^{i}$ , enables LASE to adopt the perspective of others and generate a simulated observation of them. At timestep $t$ , LASE processes its own observation $o_{t}^{i}$ along with another agent $j$ \u2019s ID (represented as a one-hot vector), yielding the output ${\\hat{o}}_{t}^{j}$ which is of the same size as $o_{t}^{i}$ . To update $\\eta^{i}$ and get a more accurate ${\\hat{o}}_{t}^{j}$ , the loss function is ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\eta^{i})=\\sum_{t}\\sum_{j=1,j\\neq i}^{N}(1-\\delta)C E(\\mathbb{I}\\{a_{t}^{j}\\},\\pi_{\\mathrm{SR}}^{i}(a_{t}^{j}|\\hat{\\sigma}_{t}^{j}))+\\delta\\|\\hat{\\sigma}_{t}^{j}-o_{t}^{i}\\|_{1}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The first term aims to ensure that the predicted policy $\\pi_{\\mathrm{SR}}^{i}(a_{t}^{j}|\\hat{o}_{t}^{j})$ aligns with co-player $j$ \u2019s actual actions. The second term aims to minimize the deviation of the simulated observation ${\\hat{O}}_{t}^{j}$ from $i$ \u2019s true observation $o_{t}^{i}$ , so that some common features in the environment can be reconstructed. The hyperparameter $\\delta\\in[0,1]$ balances the two goals. ", "page_idx": 4}, {"type": "text", "text": "Since SR policy network predicts actions from an agent\u2019s self-perspective and SR value network estimates the agent\u2019s own returns, we integrated the training processes of the two networks within an actor-critic framework. In training, $\\pi_{\\mathrm{SR}}^{\\bar{i}}$ takes one agent\u2019s observation as input and outputs a probability distribution over his action space. $Q_{\\mathrm{SR}}^{i}$ computes the $Q$ -value of the joint action under the current observation. Both networks are updated based on the individual extrinsic rewards obtained in the environment, and the TD-error defined as $\\delta_{t}^{i}=r_{t}^{i}{+}\\gamma Q_{\\mathrm{SR}}^{i}(o_{t+1}^{i},a_{t+1}){-}Q_{\\mathrm{SR}}^{i}(o_{t}^{i},a_{t})$ . Specifically, the SR policy network and SR value network, parameterized by $\\mu^{i}$ and $\\phi^{i}$ , are updated by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mu^{i}=\\mu^{i}+\\alpha_{\\mu^{i}}\\sum_{t}\\delta_{t}^{i}\\nabla_{\\mu^{i}}\\log\\pi_{\\mathrm{SR}}^{i}(a_{t}^{i}|\\sigma_{t}^{i}),\\quad\\phi^{i}=\\phi^{i}+\\alpha_{\\phi^{i}}\\sum_{t}\\delta_{t}^{i}\\nabla_{\\phi^{i}}Q_{\\mathrm{SR}}^{i}(o_{t}^{i},a_{t}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We can further intuitively comprehend Eq. 3: when $w_{t}^{i j}$ attains the maximum value of $1/(N-1).$ , $j$ has taken the best action that maximized $\\overline{{Q}}_{\\mathrm{SR}}^{i}$ and $i$ predicts with a probability of 1 that $j$ will select the action that minimizes $Q_{\\mathrm{SR}}^{i}$ . At this point, the value $j$ brings to $i$ far exceeds $i$ \u2019s psychological expectation, which corresponds to the real-world scenario where people feel particularly happy when they are helped by someone they might have thought was unkind to them. So the amount of $i\\,\\mathrm{>}\\,$ gifting to $j$ reaches the maximum. It is worth noting that Eq. 3 may yield a negative value, indicating that the agent is required to acquire rewards from other agents. Dealing with this scenario becomes intricate, particularly when the other agent is uncooperative. As this type of competition is not the primary focus of our research, we assign $w^{i j}=0$ when $w^{i j}\\leq0$ . ", "page_idx": 4}, {"type": "text", "text": "4.3 Analysis in Iterated Matrix Game ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We use iterated matrix games to theoretically analyze LASE\u2019s learning process. The iterative matrix game is to play multiple rounds of a single game with the payoff matrix shown in Tab. 1, where both players get a payoff of $R$ by mutual cooperation (C) and $P$ by mutual defection (D). If one player defects and the other cooperates, the defector receives a reward of $T$ , while the cooperator ", "page_idx": 4}, {"type": "text", "text": "Table 1: Matrix-form game ", "page_idx": 4}, {"type": "text", "text": "P1/P2 C D C $\\overline{{(R,R)}}$ $(S,T)$ D $(T,S)$ $(P,P)$ ", "page_idx": 4}, {"type": "text", "text": "receives a reward of $S$ . We normalize $R$ to 1 and $S$ to 0, and let $0\\,\\leq\\,T\\,\\leq\\,2,-1\\,\\leq\\,S\\,\\leq\\,1$ which is shown to be sufficient to characterize the three typical kinds of dilemmas in Tab. 3 [26]. ", "page_idx": 4}, {"type": "text", "text": "We carry out a closed-form gradient descent analysis on LASE in the two-player iterated matrix games and derive the policy update rule Eq. 15 and Eq. 16, where each agent $i$ optimizes the reward after gifting $\\bar{r}^{i,\\mathrm{tot}}$ . The detailed deduction is provided in Appendix A. Then we simulate the policy update iteratively with random initial value and plot LASE\u2019s cooperation probability after convergence under various game parameters as illustrated in Fig. 2. ", "page_idx": 4}, {"type": "text", "text": "The results demonstrate that LASE converges to pure cooperation in Harmony and SH. In the more intense games, SG and PD, LASE stabilizes at cooperating with a probability greater than 0.5, successfully escaping from non-efficient Nash equilibria. ", "page_idx": 4}, {"type": "text", "text": "5 Experimental setup ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "5.1 Environments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Iterated Prisoner\u2019s Dilemma (IPD). Here, we use iterated prisoner\u2019s dilemma (IPD) as an illustration to validate the theoretical analysis of LASE conducted in Section 4.3 and Appendix A. The specific game parameters are set as $[R,S,T,P]\\ =\\ [1,-0.2,1.2,0]$ , represented ", "page_idx": 4}, {"type": "image", "img_path": "ry0RXTJwjy/tmp/90eb8e28d57f04afebe64cd30305940e6dfd537061ab8f31dff08bb0c62e83e2.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 2: The cooperation probability of LASE agents after convergence under different matrix-game parameters. The X-axis and Y-axis represent two parameters $T$ and $S$ respectively, where $T~\\in~[0~:~0.02~:~2]$ and $S\\in[-1:0.02:1]$ . ", "page_idx": 4}, {"type": "text", "text": "as the red dot in Fig. 2. We employ the memory-1 IPD introduced in [6], with the state $s=$ $[\\mathrm{CC},\\mathrm{CD},\\mathrm{DC},\\mathrm{DD},s_{0}\\bar{]}$ comprising the joint action in the previous round and the initial state $s_{0}$ . The action space consists of two discrete actions - cooperation (C) and defection (D). Each episode lasts for 100 timesteps. ", "page_idx": 5}, {"type": "text", "text": "To evaluate the ability of LASE to address more complex environments, we study its performance in partially observable SSDs. SSDs extend matrix-form games in terms of space, time, and number of agents. Here, we study four specific SSDs: Coingame, Cleanup, Sequential Stag-Hunt (SSH), and Sequential Snowdrift Game (SSG) (Fig. 3). Schelling diagrams (see Fig. 10) of the four environments validate that they are appropriate extensions of representative game paradigms (a detailed analysis is given in Appendix B). Below is a detailed description of the four environments. ", "page_idx": 5}, {"type": "image", "img_path": "ry0RXTJwjy/tmp/d48a34c6647dc176ff4d768730fdcb9392b8f2c48fe28978a0e6d81560828a7f.jpg", "img_caption": ["Figure 3: Graphic representations of four SSDs: (a) Coingame $(5\\!\\times\\!5\\;\\mathrm{map})$ ), (b) Cleanup $\\leftmoon8\\!\\times\\!8\\;\\mathrm{map}\\right]$ ), (c) SSH ( $8\\!\\times\\!8$ map), (d) SSG $8\\!\\times\\!8$ map). "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Coingame. The Coingame was originally introduced in [16] as a higher dimensional alternative to the IPD with multi-step actions. In this environment, two agents, \u2018red\u2019 and \u2018blue\u2019, collect coins in a $5\\times5$ map. A coin is either red or blue and there is only one coin on the map at any timestep. Agents will get a reward of 1 by picking up a coin of any color. However, when an agent collects a coin of a different color, the other agent will lose 2 points. After a pickup, a new coin with a random color and random location appears immediately. Therefore, if each agent greedily picks up all the coins, the sum of their expected scores will be 0. ", "page_idx": 5}, {"type": "text", "text": "Cleanup. In Cleanup, the goal of the agent is to gather as many apples as possible, with each apple carrying a reward of $+1$ . However, the accumulation of waste in the river steadily approaches a depletion threshold, causing a linear decline in the apple growth rate to 0. At the beginning of each episode, the waste level exceeds the threshold and there are no apples in the map. This places the agent in a social dilemma: while individually focusing on collecting apples under the map leads to higher rewards, if all agents opt to refuse waste cleanup, no rewards are obtained. To maintain consistency with other environments, we partially modify the setting of cleanup from [11]. We eliminate the actions of firing beams (cleaning and zapping) and require the agent to move to the waste\u2019s position to clean it. This does not alter the nature of the dilemma but makes it more challenging because the cleaning beam could have helped the agent clean from a distance. ", "page_idx": 5}, {"type": "text", "text": "Sequential Stag-Hunt (SSH). This environment is inspired by Markov Stag Hunt in [23]. Each agent can get a reward of $+1$ by hunting a hare while hunting stags is more challenging and requires two or more agents. Each stag can bring a reward of $+10$ , which is divided equally among the agents that jointly hunted it. The agent is immediately removed from the environment after successfully hunting once. Therefore, if an agent chooses to hunt stags, it must contend with the risk of no one cooperating with it. In contrast, hunting rabbits is a safer choice. ", "page_idx": 5}, {"type": "text", "text": "Sequential Snowdrift Game (SSG). In SSG, there are 6 piles of snowdrifts which can be removed by the agent. A pile of removed snowdrifts brings a $^{+6}$ reward for each agent, but the remover incurs a cost of 4. So the agent waiting for others to remove the snowdrift (free-rider) can obtain a higher return. However, if no one chooses to remove it, no rewards can be obtained by anyone. ", "page_idx": 5}, {"type": "text", "text": "5.2 Implementations ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We employ fully decentralized training and execution for the agents, where all network parameters are independent. The policy structure of the agent comprises 2 convolutional layers for encoding observations, an LSTM layer to capture temporal information, and several fully connected layers activated by ReLU. The input is a multi-channel binary tensor, with the specific number of channels determined by the characteristics of different environments. For example, in Cleanup, 7 channels are incorporated, wherein the first four channels denote the positions of the four agents, the fifth and sixth channels signify waste and apples, and the last channel distinguishes between the inside and outside of the map through masking. To ensure adequate exploration, we let $\\widetilde{\\pi}(a|o)=(1-\\epsilon)\\pi(a|o)+\\epsilon/|A|$ , with $\\epsilon$ decaying linearly from $\\epsilon_{\\mathrm{start}}$ to $\\epsilon_{\\mathrm{end}}$ over $\\epsilon_{\\mathrm{div}}$ episodes. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "The SR policy network and SR value network of the SRI module share two convolutional layers, followed by their own linear layers activated by ReLU function. The difference is that the inputs of the linear layers in SR value network include an additional concatenated one-hot vector of the joint action. The observation conversion network uses a single convolutional layer to encode observation, which is then concatenated with a one-hot vector representing the ID of other agents. This concatenated input is then processed through two linear layers, with the resulting output normalized using the sigmoid function. ", "page_idx": 6}, {"type": "text", "text": "In IPD, we modify the implementation by removing convolutional layers, reducing the parameters of FC layers, and appropriately increasing the learning rate to adapt the algorithm to this environment. See Appendix C for more details about the implementations of LASE. ", "page_idx": 6}, {"type": "image", "img_path": "ry0RXTJwjy/tmp/f5844cd66ec98b709982315bcbd0709bfc2c9d79edb3b56701131066089649cf.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 4: Results in IPD. (a) The learning path of two LASE agents. They start from the lower left and converge to the upper right of the phase diagram, where both agents cooperate with a probability around 0.93. (b) The collective reward of LASE and baselines. Five seeds are randomly selected for the experiment. The solid line represents the mean performance, while the shaded area indicates the standard deviation. ", "page_idx": 6}, {"type": "text", "text": "5.3 Baselines ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Independent advantage actor-critic labeled A2C [22] is a classical gradient-based RL algorithm. LIO [36] learns an incentive function through the learning updates of reward recipients. LOLA [6] considers the learning process of other agents when updating its own policy parameters. IA [11, 32] modifies the individual reward function by introducing inequity aversion. SI [13, 32] achieves coordination by rewarding agents for having causal influence over other agents\u2019 actions. We also show the approximate upper bound on performance by training the group optimal (GO) agents to maximize the collective reward. And we conduct ablation experiments LASE w/o, by removing the observation conversion network and replacing $\\pi_{\\mathrm{SR}}^{i}(a_{t}^{j^{\\prime}}|\\hat{o}_{t}^{j})$ in Eq. 3 with $1/|A^{j}|$ . ", "page_idx": 6}, {"type": "text", "text": "6 Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "6.1 LASE promotes cooperation in social dilemmas ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In IPD, as shown in Fig. 4, LASE successfully escapes the dilemma of non-efficient Nash Equilibrium (D, D). Both LASE agents converge to cooperate with a high probability, around 0.93 (see Fig. 4a), accompanied by a high collective reward. This is consistent with the theoretical results shown in Fig. 2, validating the effectiveness of LASE in dealing with social dilemmas. LASE is better at convergence speed and stability than LOLA, which also achieves a high collective reward. Unsurprisingly, GO, aiming to maximize group reward, reaches the upper bound. On the other hand, A2C, optimizing for one\u2019s own return, easily falls into the Nash equilibrium, where everyone defects and the group reward reduces to the minimum. ", "page_idx": 6}, {"type": "text", "text": "In SSH and SSG, LASE nearly reaches the upper bound of the total rewards as shown in Fig. 5a and Fig. 5b: a total reward of 20 for successfully hunting two stags in SSH and a total reward of 120 in SSG for removing all the six snowdrifts. In Coingame (Fig. 5c) and Cleanup (Fig. 5d), LASE exhibits commendable performance by effectively avoiding the sub-optimal equilibrium, where everyone defects. GO outperforms LASE in SSG, Coingame, and Cleanup, because its optimization objective of maximizing the collective reward of all the agents and a strong assumption of the accessibility of everyone\u2019s reward function can help it avoid the dilemma. ", "page_idx": 6}, {"type": "text", "text": "On the other hand, it\u2019s worth emphasizing that the architecture of GO also gets it into the lazy problem [31], shown as its underperformance in SSH (Fig. 5a). In SSH, early hunting leads to moving out of the environment and failing to obtain the group rewards of others\u2019 later hunting. Thus, GO may not hunt until the last few steps, and likely misses the the opportunity of cooperating to hunt stags. ", "page_idx": 6}, {"type": "image", "img_path": "ry0RXTJwjy/tmp/7114d6ba828f43865d5c7eec36ce51b3bf49244613b42b64d62739d736b5ba04.jpg", "img_caption": ["Figure 5: Learning curves in four SSDs. Shown is the collective reward. All the curves are plotted using 5 training runs with different random seeds, where the solid line is the mean and the shadowed area indicates the standard deviation. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "A2C and SI hardly improve the collective return in Coingame and Cleanup, and fail to outperform LASE in SSH and SSG, where the dilemma is less intense. Although IA can directly access the rewards of others and adjust its own intrinsic rewards accordingly, it is still unable to get a higher collective return than LASE. Also as a gifting algorithm, LIO struggles to perform well in the four environments. Meanwhile, the need to manually specify the hyperparameters used to scale the amount of gifting also creates challenges for LIO to apply to different environments. In contrast, LASE breaks the limitations and achieves significantly better performance than LIO. ", "page_idx": 7}, {"type": "text", "text": "LASE w/o\u2019s convergence speed and performance are affected to a certain extent but not significantly. This is because LASE w/o differs from LASE by replacing the policy predictions of others with uniform policy to compute counterfactual baselines. It means that the gifting will continue as long as the first term of the numerator is substantial in Eq. 3. On the other hand, LASE builds a counterfactual baseline dynamically through perspective taking, and only gifts when the co-player\u2019s real action is superior to the baseline. Tab. 2 shows the mean of the gifting weights to other agents over the last 1e4 episodes of training, showing that LASE\u2019s gifting weights are lower than those of LASE w/o. Regarding gifting as a form of communication, LASE is valuable in reducing communication costs. Furthermore, to test LASE\u2019s scalability, we implement LASE in the extended Cleanup and SSG with 8 agents and a larger map. Results show that LASE is able to deal with more complex environments (see details in Section D.1). ", "page_idx": 7}, {"type": "table", "img_path": "ry0RXTJwjy/tmp/760a949c1c934f9473a2730cb96b78c7d8a37f21248906a6bfb7a0cba02ffa60.jpg", "table_caption": ["Table 2: Mean of the gifting weights for LASE and LASE w/o during the last 10000 episodes of self-play training. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "6.2 LASE promotes fairness ", "text_level": 1, "page_idx": 7}, {"type": "image", "img_path": "ry0RXTJwjy/tmp/f35ba0206a2d03e26699ecc28a825cfe286974a9201dfc14966f5f97cdeffa7d.jpg", "img_caption": ["Figure 6: Learning curves of each agent in Cleanup. Since the division of labor for different random seeds is not the same, only the results under one seed are shown to distinguish individual performance. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Considering the fact that the division of labor in Cleanup is more pronounced than in other environments and that cooperation in Cleanup is purely altruistic, making the dilemma more challenging, we take Cleanup as an example to show LASE\u2019s ability to promote fairness. Fig. 6a and Fig. 6b show the extrinsic rewards of each agent and the amount of waste cleaned by each in Cleanup. We find Agent 4 is the only one to clean and does not receive any extrinsic reward. ", "page_idx": 7}, {"type": "text", "text": "Fig. 6c illustrates the gifting weights each agent received from the other three agents. Agent 4 gets the most gifts, indicating that its cleaning contribution to the team is recognized and rewarded by the other three agents. Fig. 6d shows reward curves after gifting, where the reward gap between Agent 4 and the other three agents shrinks, implying fairness within the group is improved. ", "page_idx": 8}, {"type": "text", "text": "Agents 1-3 always collect apples without cleaning, but this is not a freerider behavior. Both cleaning and collecting are indispensable to get rewards in Cleanup. Although Agent 4 does not obtain any reward directly from the environment, the acquired reward is redistributed through gifting, narrowing the gap of reward between the cleaner and collectors. In contrast, although GO obtains higher collective returns, it sacrifices the individual interests of Agent 1 and Agent 2, while only Agent 3 and Agent 4 can get rewards Fig. 7. ", "page_idx": 8}, {"type": "text", "text": "We use Equality $(E)$ given by $\\begin{array}{r}{E=1-\\frac{\\sum_{i=1}^{N}\\sum_{j=1}^{N}|R_{i}-R_{j}|}{2N\\sum_{i=1}^{N}R_{i}}}\\end{array}$ [20] to quantify the fairness, where the second term is the Gini inequality index. The greater the value of $E$ , the fairer it is. We can get $E(\\mathrm{LASE})\\,\\approx\\,0.802$ , $E(\\mathrm{GO})\\approx0.496$ , showing that LASE achieves a higher level of fairness. ", "page_idx": 8}, {"type": "image", "img_path": "ry0RXTJwjy/tmp/64783ce9310a35a1b52430621e48ebc28f468fd190a07f2061e3e74036380546.jpg", "img_caption": ["Figure 7: Four GOs\u2019 rewards in Cleanup. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "6.3 LASE distinguishes co-players and responds adaptively ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To evaluate LASE\u2019s adaptation ability to interact with various types of agents, we conduct an experiment in which a focal LASE agent interacts with three rule-based agents: cooperator (always clean up waste), defector (always try to collect apples), and a random agent. The gifting weights of LASE to the other agents are shown in Fig. 8. LASE can explicitly distinguish between different types of co-players. Moreover, it responds in a manner that aligns with human values: preferring to share rewards with cooperators rather than defectors. ", "page_idx": 8}, {"type": "image", "img_path": "ry0RXTJwjy/tmp/054e20b8d93ec2719e0ea2c91705701e4be01c433b7019a129cdee133efc4578.jpg", "img_caption": ["Figure 8: LASE\u2019s gifting weights to three rulebased co-players. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "To study how LASE responds dynamically and how it affects collective behavior, we conduct an experiment where one focal LASE agent interacts with three A2C agents (Background agents, Bgs). A GO agent is trained in the same way for comparison with LASE. Fig. 9a displays each agent\u2019s rewards after training for 30k episodes, whereas the LASE group shows the reward after gifting. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "LASE and GO improve the group reward to a similar level, while four A2C agents will converge to the equilibrium of defection and gain almost no reward (see Fig. 5d). With gifting, LASE incentives $B g3$ to clean as shown in Fig. 9b. Thus, LASE and the other two $B g\\mathrm{s}$ can get rewards by gathering apples. But for GO, the focal agent sacrifices itself to undertake all the cleaning tasks and cannot get any reward. ", "page_idx": 8}, {"type": "text", "text": "GO and LASE represent two different methods to foster cooperation. GO sacrifices its own interest to promote collective reward. LASE attempts to alleviate social dilemmas by incentivizing others to cooperate. When such an incentive mechanism fails, LASE will no longer gift those agents who constantly exploit it, like the defector in Fig. 8. Overall, we believe that LASE is a more efficient and secure policy for ", "page_idx": 8}, {"type": "image", "img_path": "ry0RXTJwjy/tmp/49bbe3910bf4d526ee817fe6bc282f0ff2767042afe9f8859d4f1d6c949f0be1.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 9: An LASE (or GO) agent interacts with three A2C agents in Cleanup. (a) The average reward for LASE and GO groups after training $30\\mathrm{k}$ episodes. The LASE group shows the reward after gifting. The first two bars show whole groups\u2019 rewards. The remaining bars show the average reward for each agent. (b) The amount of the waste cleaned by each agent in the LASE group. ", "page_idx": 8}, {"type": "text", "text": "SSDs, as it can promote cooperation as well as avoid potential exploitation by others. ", "page_idx": 8}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We introduce LASE, a decentralized MARL algorithm that fosters cooperation through gifting while safeguarding individual interests in mixed-motive games. LASE uses counterfactual reasoning to infer the social relationships with others which captures the influences of others\u2019 actions on LASE and modulates the gifting strategy empathetically. In particular, to empower LASE with the ability to infer others\u2019 policies in partially observable and decentralized environments, we establish a perspective taking module for LASE. Both theoretical analyses in matrix-form games and experimental results across diverse SSDs show that LASE can effectively promote cooperative behavior while ensuring relative fairness within the group. Furthermore, LASE is also able to recognize various types of coplayers and adjust its gifting strategy adaptively to avoid being exploited, enabling broad applicability in complex real-world multi-agent interactions, such as automated negotiations in E-commerce and decision-making in autonomous driving. Whilst LASE exhibits superior abilities, there are some limitations of our method. What would be the consequence of giving agents the ability to refuse gifts? How to extend the algorithm LASE to continuous action space? These problems will illuminate our future work. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work is supported by the National Science and Technology Major Project (No. 2022ZD0114904). ", "page_idx": 9}, {"type": "text", "text": "This work is also supported by the project \u201cWuhan East Lake High-Tech Development Zone, National Comprehensive Experimental Base for Governance of Intelligent Society\u201d. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] C. D. Batson, B. D. Duncan, P. Ackerman, T. Buckley, and K. Birch. Is empathic emotion a source of altruistic motivation? Journal of personality and Social Psychology, 40(2):290, 1981. [2] C. D. Batson, N. Ahmad, D. A. Lishner, J. Tsang, C. Snyder, and S. Lopez. Empathy and altruism. The Oxford handbook of hypo-egoic phenomena, pages 161\u2013174, 2002.   \n[3] P. J. Christoffersen, A. A. Haupt, and D. Hadfield-Menell. Get it in writing: Formal contracts mitigate social dilemmas in multi-agent rl. arXiv preprint arXiv:2208.10469, 2022.   \n[4] Y. Du, J. Z. Leibo, U. Islam, R. Willis, and P. Sunehag. A review of cooperation in multi-agent learning. arXiv preprint arXiv:2312.05162, 2023. [5] J. Foerster, G. Farquhar, T. Afouras, N. Nardelli, and S. Whiteson. Counterfactual multi-agent policy gradients. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018.   \n[6] J. N. Foerster, R. Y. Chen, M. Al-Shedivat, S. Whiteson, P. Abbeel, and I. Mordatch. Learning with opponent-learning awareness. arXiv preprint arXiv:1709.04326, 2017.   \n[7] I. Gemp, K. R. McKee, R. Everett, E. A. Du\u00e9\u00f1ez-Guzm\u00e1n, Y. Bachrach, D. Balduzzi, and A. Tacchetti. D3c: Reducing the price of anarchy in multi-agent learning. arXiv preprint arXiv:2010.00575, 2020.   \n[8] H. Heemskerk. Social curiosity in deep multi-agent reinforcement learning. Master\u2019s thesis, 2020.   \n[9] Y. Hua, S. Gao, W. Li, B. Jin, X. Wang, and H. Zha. Learning optimal\" pigovian tax\" in sequential social dilemmas. arXiv preprint arXiv:2305.06227, 2023.   \n[10] Y. Huang, A. Liu, F. Kong, Y. Yang, S.-C. Zhu, and X. Feng. Efficient adaptation in mixed-motive environments via hierarchical opponent modeling and planning. arXiv preprint arXiv:2406.08002, 2024.   \n[11] E. Hughes, J. Z. Leibo, M. Phillips, K. Tuyls, E. Due\u00f1ez-Guzman, A. Garc\u00eda Casta\u00f1eda, I. Dunning, T. Zhu, K. McKee, R. Koster, et al. Inequity aversion improves cooperation in intertemporal social dilemmas. Advances in neural information processing systems, 31, 2018.   \n[12] A. Ibrahim, A. Jitani, D. Piracha, and D. Precup. Reward redistribution mechanisms in multiagent reinforcement learning. In Adaptive Learning Agents Workshop at the International Conference on Autonomous Agents and Multiagent Systems, 2020.   \n[13] N. Jaques, A. Lazaridou, E. Hughes, C. Gulcehre, P. Ortega, D. Strouse, J. Z. Leibo, and N. De Freitas. Social influence as intrinsic motivation for multi-agent deep reinforcement learning. In International conference on machine learning, pages 3040\u20133049. PMLR, 2019.   \n[14] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \n[15] J. Z. Leibo, V. Zambaldi, M. Lanctot, J. Marecki, and T. Graepel. Multi-agent reinforcement learning in sequential social dilemmas. arXiv preprint arXiv:1702.03037, 2017.   \n[16] A. Lerer and A. Peysakhovich. Maintaining cooperation in complex social dilemmas using deep reinforcement learning. arXiv preprint arXiv:1707.01068, 2017.   \n[17] A. Letcher, J. Foerster, D. Balduzzi, T. Rockt\u00e4schel, and S. Whiteson. Stable opponent shaping in differentiable games. arXiv preprint arXiv:1811.08469, 2018.   \n[18] M. L. Littman. Markov games as a framework for multi-agent reinforcement learning. In Machine learning proceedings 1994, pages 157\u2013163. Elsevier, 1994.   \n[19] R. Lowe, Y. I. Wu, A. Tamar, J. Harb, O. Pieter Abbeel, and I. Mordatch. Multi-agent actor-critic for mixed cooperative-competitive environments. Advances in neural information processing systems, 30, 2017.   \n[20] A. Lupu and D. Precup. Gifting in multi-agent reinforcement learning. In Proceedings of the 19th International Conference on autonomous agents and multiagent systems, pages 789\u2013797, 2020.   \n[21] K. R. McKee, I. Gemp, B. McWilliams, E. A. Du\u00e9\u00f1ez-Guzm\u00e1n, E. Hughes, and J. Z. Leibo. Social diversity and social preferences in mixed-motive reinforcement learning. arXiv preprint arXiv:2002.02325, 2020.   \n[22] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International conference on machine learning, pages 1928\u20131937. PMLR, 2016.   \n[23] A. Peysakhovich and A. Lerer. Prosocial learning agents solve generalized stag hunts better than selfish ones. arXiv preprint arXiv:1709.02865, 2017.   \n[24] R. Raileanu, E. Denton, A. Szlam, and R. Fergus. Modeling others using oneself in multi-agent reinforcement learning. In International conference on machine learning, pages 4257\u20134266. PMLR, 2018.   \n[25] T. Rashid, M. Samvelyan, C. S. De Witt, G. Farquhar, J. Foerster, and S. Whiteson. Monotonic value function factorisation for deep multi-agent reinforcement learning. The Journal of Machine Learning Research, 21(1):7234\u20137284, 2020.   \n[26] F. C. Santos, J. M. Pacheco, and T. Lenaerts. Evolutionary dynamics of social dilemmas in structured heterogeneous populations. Proceedings of the National Academy of Sciences, 103 (9):3490\u20133494, 2006.   \n[27] L. S. Shapley. Stochastic games. Proceedings of the national academy of sciences, 39(10): 1095\u20131100, 1953.   \n[28] T. Singer, B. Seymour, J. P. O\u2019Doherty, K. E. Stephan, R. J. Dolan, and C. D. Frith. Empathic neural responses are modulated by the perceived fairness of others. Nature, 439(7075):466\u2013469, 2006.   \n[29] K. Son, D. Kim, W. J. Kang, D. E. Hostallero, and Y. Yi. Qtran: Learning to factorize with transformation for cooperative multi-agent reinforcement learning. In International conference on machine learning, pages 5887\u20135896. PMLR, 2019.   \n[30] E. L. Stocks, D. A. Lishner, and S. K. Decker. Altruism or psychological escape: Why does empathy promote prosocial behavior? European journal of social psychology, 39(5):649\u2013665, 2009.   \n[31] P. Sunehag, G. Lever, A. Gruslys, W. M. Czarnecki, V. Zambaldi, M. Jaderberg, M. Lanctot, N. Sonnerat, J. Z. Leibo, K. Tuyls, et al. Value-decomposition networks for cooperative multi-agent learning. arXiv preprint arXiv:1706.05296, 2017.   \n[32] E. Vinitsky, N. Jaques, J. Leibo, A. Castenada, and E. Hughes. An open source implementation of sequential social dilemma games. https://github.com/eugenevinitsky/ sequential_social_dilemma_games/issues/182, 2019. GitHub repository.   \n[33] J. X. Wang, E. Hughes, C. Fernando, W. M. Czarnecki, E. A. Du\u00e9\u00f1ez-Guzm\u00e1n, and J. Z. Leibo. Evolving intrinsic motivations for altruistic behavior. arXiv preprint arXiv:1811.05931, 2018.   \n[34] W. Z. Wang, M. Beliaev, E. B\u0131y\u0131k, D. A. Lazar, R. Pedarsani, and D. Sadigh. Emergent prosociality in multi-agent games through gifting. arXiv preprint arXiv:2105.06593, 2021.   \n[35] R. Willis and M. Luck. Resolving social dilemmas through reward transfer commitments. In Proc. Adaptive and Learning Agents Workshop (ALA2023) 29\u201330 May 2023, London, UK, 2023.   \n[36] J. Yang, A. Li, M. Farajtabar, P. Sunehag, E. Hughes, and H. Zha. Learning to incentivize other learning agents. Advances in Neural Information Processing Systems, 33:15208\u201315219, 2020.   \n[37] Y. Yi, G. Li, Y. Wang, and Z. Lu. Learning to share in multi-agent reinforcement learning. arXiv preprint arXiv:2112.08702, 2021.   \n[38] C. Yu, A. Velu, E. Vinitsky, J. Gao, Y. Wang, A. Bayen, and Y. Wu. The surprising effectiveness of ppo in cooperative multi-agent games. Advances in Neural Information Processing Systems, 35:24611\u201324624, 2022. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Analysis in Iterated Matrix Games ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "For a general iterated matrix game whose payoff matrix is computed according to Tab. 1 at each step, we can let $\\theta^{i}$ for $i\\in\\{1,2\\}$ denote each agent\u2019s probability of taking the cooperative action, and let $\\ensuremath{\\hat{\\theta}}^{j}$ for $j\\in\\{2,1\\}$ denote each agent\u2019s prediction of the other\u2019s policy. To avoid the difficulties caused by the coupled update of reinforcement learning for the theoretical analysis, we make the simplifying assumption that the three networks in SRI module have been fully trained. We further assume that this two-player game is fully observable, and we can make the following approximation to Eq. 3: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{Q^{i}(o_{t}^{i},a_{t})-\\displaystyle\\sum_{a_{t}^{j^{\\prime}}}\\pi_{\\mathrm{sc}}^{i}(a_{t}^{j^{\\prime}}|\\hat{\\sigma}_{t}^{j})Q^{i}(o_{t}^{i},(a_{t}^{-j},a_{t}^{j^{\\prime}}))}\\\\ {w_{t}^{i j}=\\displaystyle\\frac{1}{N-1}\\frac{a_{t}^{j^{\\prime}}(o_{t}^{i},(a_{t}^{-j},a_{t}^{j^{\\prime}})-\\displaystyle\\operatorname*{min}_{a_{t}^{j^{\\prime}}}Q^{i}(o_{t}^{i},(a_{t}^{-j},a_{t}^{j^{\\prime}}))}{a_{t}^{j^{\\prime}}}}\\\\ {\\approx\\displaystyle\\frac{1}{N-1}\\frac{r^{i}(a^{i},a^{j})-(\\hat{\\theta}^{j}r^{i}(a^{i},C)+(1-\\hat{\\theta}^{j})r^{i}(a^{i},D))}{r^{i}(a^{i},C)-r^{i}(a^{i},D)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Here, $r^{i}(a^{i},a^{j})$ represents player $i$ \u2019s reward determined by the payoff matrix which only relies on the two players\u2019 actions without state or observation. The update rule Eq. 5 states that it is feasible to replace ${\\bar{Q}}^{i}$ by $r^{i}$ . Then we can calculate the gifting weights under the four combinations of actions: CC, CD, DC, and DD. We take w12for example: ", "page_idx": 12}, {"type": "equation", "text": "$$\nw_{C C}^{12}=\\frac{r^{1}(C,C)-(\\hat{\\theta}^{2}r^{1}(C,C)+(1-\\hat{\\theta}^{2})r^{1}(C,D))}{r^{1}(C,C)-r^{1}(C,D)}=\\frac{R-(\\hat{\\theta}^{2}\\cdot R+(1-\\hat{\\theta}^{2}\\cdot S))}{R-S}=1-\\hat{\\theta}^{2}-\\frac{(\\hat{\\theta}^{2}\\cdot R+(1-\\hat{\\theta}^{2}\\cdot R))}{R-S},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "equation", "text": "$$\nw_{C D}^{12}=\\frac{r^{1}(C,D)-(\\hat{\\theta}^{2}r^{1}(C,C)+(1-\\hat{\\theta}^{2})r^{1}(C,D))}{r^{1}(C,C)-r^{1}(C,D)}=\\frac{S-(\\hat{\\theta}^{2}\\cdot R+(1-\\hat{\\theta}^{2}\\cdot S))}{R-S}=-\\hat{\\theta}^{2}\\cdot R\\cdot\\hat{\\theta}^{2}\\cdot R\\cdot\\hat{\\theta}^{2}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "equation", "text": "$$\nv_{D C}^{12}=\\frac{r^{1}(D,C)-(\\hat{\\theta}^{2}r^{1}(D,C)+(1-\\hat{\\theta}^{2})r^{1}(D,D))}{r^{1}(D,C)-r^{1}(D,D)}=\\frac{T-(\\hat{\\theta}^{2}\\cdot T+(1-\\hat{\\theta}^{2}\\cdot P))}{T-P}=1-\\hat{\\theta}^{2}\\cdot D\\cdot D\\cdot D.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "equation", "text": "$$\nw_{D D}^{12}=\\frac{r^{1}(D,D)-(\\hat{\\theta}^{2}r^{1}(D,C)+(1-\\hat{\\theta}^{2})r^{1}(D,D))}{r^{1}(D,C)-r^{1}(D,D)}=\\frac{P-(\\hat{\\theta}^{2}\\cdot T+(1-\\hat{\\theta}^{2}\\cdot P))}{T-P}=-\\hat{\\theta}^{2}\\cdot\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Similarly as Section 4, we set $w^{12}$ less than 0 to 0. So agent 1\u2019s reward distribution scheme is: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{r_{C C}^{12}=w_{C C}^{12}R=(1-\\hat{\\theta}^{2})R,r_{C C}^{11}=R-r_{C C}^{12}=\\hat{\\theta}^{2}R,}\\\\ &{r_{D C}^{12}=w_{D C}^{12}T=(1-\\hat{\\theta}^{2})T,r_{D C}^{11}=T-r_{D C}^{12}=\\hat{\\theta}^{2}T,}\\\\ &{r_{C D}^{12}=r_{D D}^{12}=0,r_{C D}^{11}=S,r_{D D}^{11}=P}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Agent 2\u2019s computation is symmetric. The total reward received by each agent is ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{r^{1,\\mathrm{tot}}=[\\hat{\\theta}^{2}R+(1-\\hat{\\theta}^{1})R,S+(1-\\hat{\\theta}^{1})T,\\hat{\\theta}^{2},P]}\\\\ &{r^{2,\\mathrm{tot}}=[\\hat{\\theta}^{1}R+(1-\\hat{\\theta}^{2})R,\\hat{\\theta}^{1},S+(1-\\hat{\\theta}^{2})T,P]}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "The value function for each agent is defined by ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle V^{i}(\\theta^{1},\\theta^{2})=\\sum_{t=0}^{\\infty}\\gamma^{t}p^{T}r^{i,\\mathrm{tot}}},}\\\\ {{\\mathrm{where~}p=[\\theta^{1}\\theta^{2},\\theta^{1}(1-\\theta^{2}),(1-\\theta^{1})\\theta^{2},(1-\\theta^{1})(1-\\theta^{2})]}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Agent 2 updates its policy by ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\theta^{2}=\\theta^{2}+\\alpha\\nabla_{\\theta^{2}}V^{2}(\\theta^{1},\\theta^{2})}}\\\\ {{\\displaystyle\\quad=\\theta^{2}+\\frac{\\alpha}{1-\\gamma}\\nabla_{\\theta^{2}}\\left\\{\\theta^{1}\\theta^{2}\\left[\\hat{\\theta}^{1}R+(1-\\hat{\\theta}^{2})r\\right]\\right.}}\\\\ {{\\displaystyle\\left.+\\theta^{1}(1-\\theta^{2})\\hat{\\theta}^{1}T+\\theta^{2}(1-\\theta^{1})\\left[S+(1-\\hat{\\theta}^{2})\\right]+(1-\\theta^{1})(1-\\theta^{2})P\\right\\}}}\\\\ {{\\displaystyle=\\theta^{2}+\\frac{\\alpha}{1-\\gamma}\\left\\{\\left[\\hat{\\theta}^{1}R+(1-\\hat{\\theta}^{2})R-\\hat{\\theta}^{1}T\\right]\\theta^{1}+\\left[S+(1-\\hat{\\theta}^{2})T-P\\right](1-\\theta^{1})\\right\\}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "By symmetry, agent 1 updates its policy by ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\theta^{1}=\\theta^{1}+\\frac{\\alpha}{1-\\gamma}\\left\\{\\left[\\hat{\\theta}^{2}R+(1-\\hat{\\theta}^{1})R-\\hat{\\theta}^{2}T\\right]\\theta^{2}+\\left[S+(1-\\hat{\\theta}^{1})T-P\\right](1-\\theta^{2})\\right\\}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We assume that LASE\u2019s prediction to the others is accurate when simulating in Section 4.3, i.e $\\hat{\\theta}^{1}=\\theta^{1}$ , $\\hat{\\theta^{2}}=\\theta^{2}$ . And we let $\\alpha=10^{-3}$ , $\\gamma=0.99$ . ", "page_idx": 13}, {"type": "text", "text": "B Environments ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "B.1 Validating the environments ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we will show that our four environments are all sequential social dilemmas defined in [11]: An $N$ -player sequential social dilemma is a tuple $(\\mathcal{M},\\Pi=\\Pi_{c}\\sqcup\\Pi_{d})$ of a Markov game and two disjoint sets of policies, said to implement cooperation and defection respectively, satisfying the following properties. Consider the strategy profile $\\big(\\pi_{c}^{1},\\dots,\\pi_{c}^{l},\\pi_{d}^{1},\\dots,\\pi_{d}^{\\dot{m}}\\big)\\in\\Pi_{c}^{l^{\\prime}}\\times\\Pi_{d}^{m}$ with $l+m=N$ . We shall denote the average payoff for the cooperating policies by $R_{c}(l)$ and for the defecting policies by $R_{d}(l)$ . $(\\mathcal{M},\\Pi)$ is a sequential social dilemma iff the following hold: ", "page_idx": 13}, {"type": "text", "text": "1. Mutual cooperation is preferred over mutual defection: $R_{c}(N)>R_{d}(0)$ .   \n2. Mutual cooperation is preferred to being exploited by defectors: $R_{c}(N)>R_{c}(0)$ .   \n3. Either the fear property, the greed property, or both: \u2022 Fear: mutual defection is preferred to being exploited. $R_{d}(i)>R_{c}(i)$ for sufficiently small $i$ . \u2022 Greed: exploiting a cooperator is preferred to mutual cooperation. $R_{d}(i)>R_{c}(i)$ for sufficiently large $i$ . ", "page_idx": 13}, {"type": "text", "text": "A Schelling diagram is a game representation that highlights interdependencies between agents, showing how the choices of others shape one\u2019s own incentives. It plots the curves $R_{c}(l+1)$ and $R_{d}(l)$ as shown in Fig. 10. All the environments satisfy the first two properties of sequential social dilemmas: $R_{c}(N)>\\,\\^{\\prime}R_{d}(0)$ and $R_{c}(N)>R_{d}(0)$ . In SSH, fear promotes defection: $\\bar{R_{d}}(0)>R_{c}(0)$ . In Cleanup and SSG, the problem is greed: $R_{d}(3)>R_{c}(3)$ . Coingame suffers from both temptations to defect. This indicates that our experimental environments include all three different types of sequential social dilemmas corresponding to Tab. 3. ", "page_idx": 13}, {"type": "image", "img_path": "ry0RXTJwjy/tmp/59b4999214b937f6cc0d5adbae713aee5c1d3883f33f33b96c5c1f9341635734.jpg", "img_caption": ["Figure 10: The Schelling diagram of Cleanup, Coingame, Sequential Stag-Hunt (SSH) and Sequential Snowdrift Game (SSG). The dotted line shows the overall average return where the individual chooses defection. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "Table 3: Classification of social dilemmas. In all dilemmas, mutual cooperation yields higher payoffs than mutual defection, yet each dilemma provides an incentive for defection. In the Snowdrift game, one player can gain a higher payoff by defecting when the other cooperates, while in the Stag Hunt game, a player can achieve higher payoffs by defecting when the other defects. The Prisoner\u2019s Dilemma encompasses both types of incentives. ", "page_idx": 13}, {"type": "table", "img_path": "ry0RXTJwjy/tmp/98076bd1ec2cc6d9e79b1336b58ad4ee577f377b694057b8621b8ba91d8c175a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "B.2 Environment details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "IPD. Each agent makes decisions based on the actions of the two players in the previous step, so this is a fully observable environment unlike other environments. We trained for 10k episodes, each with 100 steps. ", "page_idx": 14}, {"type": "text", "text": "Coingame. Map size is $5\\times5$ . Agent\u2019s action space is ${\\mathcal{A}}=\\{{\\mathrm{up}}$ , down, left, right}. The state is represented as a $5\\times5\\times5$ binary tensor. The five channels are {blue agent, red agent, blue coin, red coin, mask}, where the first two channels encode the location of each agent, the last channel distinguishes between parts within and beyond the boundary, and the other two channels encode the location of the coin if any exist. If two agents walk on the coin at the same time, one of them is randomly selected to successfully pick it up. Cooperative agents will only try to collect coins with their own color, while self-interested agents tend to greetingly collect all coins. Each episode lasts for 100 steps. ", "page_idx": 14}, {"type": "text", "text": "Cleanup. Map size is $8\\times8$ . ${\\mathcal{A}}=\\{{\\mathfrak{u p}}$ , down, left, right, stay, clean, pick}, where the last two actions require the agent to be in the same position as the waste or the apple. The seven channels are {agent 1, ..., agent 4, waste, apple, mask}, The parameters with the same meaning as the open-source implementation about Cleanup [32] are shown in Tab. 4. To achieve cooperation, agents need to take the initiative to undertake part of the cleaning task to help improve the group\u2019s revenue. A defecting agent will just keep waiting for the apple to grow and gather it. Each episode lasts for 100 steps. ", "page_idx": 14}, {"type": "table", "img_path": "ry0RXTJwjy/tmp/0686db7762bb697f0fa4bc5192adefab9ff475e0b541810064dd031dbb851f52.jpg", "table_caption": ["Table 4: Environment parameters in Cleanup "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "SSH. Map size is $8\\times8$ . ${\\mathcal{A}}=\\{{\\mathrm{up}}$ , down, left, right, stay, hunt hare, hunt stag}. Agents must be in the same position as the prey to hunt and the prey doesn\u2019t respawn after being hunted. The seven channels are {agent 1, ..., agent 4, hare, stag, mask}. Cooperative agents are happy to hunt deer with others, while defectors only hunt rabbits to avoid the risk of not getting a payoff. Each episode lasts for 30 steps. ", "page_idx": 14}, {"type": "text", "text": "SSG. Map size is $8\\times8$ . ${\\mathcal{A}}=\\{{\\mathrm{up}}$ , down, left, right, stay, remove snowdrift $\\}$ . Agents must be in the same position as the snowdrift to remove it and the removed snowdrift doesn\u2019t respawn. The six channels are {agent 1, ..., agent 4, snowdrift, mask}. Cooperators will proactively remove snowdrifts to bring high rewards to the team, while defectors will just wait for others to remove them. Each episode lasts for 50 steps. ", "page_idx": 14}, {"type": "text", "text": "C Implementation ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The pseudocode for the LASE algorithm is shown in Algorithm 1. ", "page_idx": 14}, {"type": "text", "text": "SSDs. The actor-critic model interacting with environments utilizes two cascaded CNNs to process input data, with a kernel size of 3, stride of size 1 and 16 / 32 output channels. This is connected to one fully connected layer of size 128 activated by ReLU, and an LSTM with 128 cells. The actor head and critic head are two separate fully-connected layers that output the softmax normalized action policy and a scalar value respectively. The implementation of the intrinsic policy network and value network in OM is basically the same, but due to the need to further judge the actions of others, joint action space is added to the value network input dimension. In the observation transformation network, the input data is passed through a CNN with a kernel of size 3, stride of size 1 and 16 output channels, and is concatenated with a one-hot vector representing the agent index to input in two FC ", "page_idx": 14}, {"type": "text", "text": "Initialize action policy $\\pi^{i}$ , SR policy network $\\pi_{\\mathrm{SR}}^{i}$ , SR value newtork $Q_{\\mathrm{SR}}^{i}$ and observation conver  \nsion network parameterized by $\\theta^{i},\\mu^{i},\\phi^{i},\\eta^{i}$ and trajectory buffer $B^{i}$ , for each agent $i$   \nfor $e p s=1$ to max_episodes do awnitdh s ttohree se nitv iirn ment for many steps, each agent $i$ gets a trajectory $\\tau^{i}=$ $(o_{t}^{i},\\pmb{a}_{t},r_{t}^{i,\\mathrm{env}},o_{t+1}^{i})$ $B^{i}$ for each agent $i$ do for each other agent $j$ do get $j$ \u2019s simulated observation ${\\hat{o}}^{j}$ by $i$ \u2019s observation conversion network estimate $j$ \u2019s policy $\\pi_{\\mathrm{SR}}^{i}(a^{j^{\\prime}}|\\hat{o}^{j})$ with SR policy network for $j$ \u2019s all possible actions $a^{j^{\\prime}}$ do compute $i$ \u2019s $Q.$ -value $Q_{\\mathrm{SR}}^{i}\\big(o_{t}^{i},(a_{t}^{-j},a_{t}^{j^{\\prime}})\\big)$ , with $j$ taking all possible actions $a_{t}^{j^{\\prime}}$ while the other agents\u2019 actions ${\\pmb{a}}^{-j}$ fixed end for compute the counterfactual baseline $\\begin{array}{r}{\\sum_{a_{t}^{j^{\\prime}}}\\pi_{\\mathrm{SR}}^{i}(a_{t}^{j^{\\prime}}|\\hat{o}_{t}^{j})Q_{\\mathrm{SR}}^{i}(o_{t}^{i},(a_{t}^{-j},a_{t}^{j^{\\prime}}))}\\end{array}$ compute gifting weights $w^{i j}$ by Eq. 3 egentd $w^{i i}$ r by $\\begin{array}{r}{w^{i i}=1-\\sum_{j=1,j\\neq i}^{N}w^{i j}}\\end{array}$ get $r^{\\mathrm{{tot}}}\\gets r$ and update policy $\\pi^{i}$ to maximize the accumulated $r^{\\mathrm{tot}}$ if eps mod update_frequency $=0$ then for each agent $i$ do sample a minibatch from $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ , update $\\mu^{i},\\phi^{i},\\eta^{i}$ by Eq. 4, Eq. 5 end for end if   \nend for ", "page_idx": 15}, {"type": "text", "text": "layers of size 128. The output is reshaped to be the same size as the input observations. We use Adam optimizer [14] for all modules\u2019 training. ", "page_idx": 15}, {"type": "text", "text": "IPD. All the CNNs in IPD are removed. The size of FC layer and the cell number in LSTM are scaled down to 32. ", "page_idx": 15}, {"type": "table", "img_path": "ry0RXTJwjy/tmp/a4961eb6523b499419160ecb4e60b9400b7612c04267639a8cfc7725c4eb5460.jpg", "table_caption": ["Table 5: Hyperparameters (a) Hyperparameters in SSDs (b) Hyperparameters in IPD "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Experiments Compute Resources ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "CPU: 128 Intel(R) Xeon(R) Platinum 8369B CPU $\\textcircled{a}2.90\\mathrm{GHz}$ ; Total memory: 263729336 kB GPU: 8 NVIDIA GeForce RTX 3090; Memory per GPU: 24576 MiB The main experiments are as shown in Fig. 5. An experiment takes about 2000 MiB of the GPU and takes about 1.5 days to run. About 5-7 experiments can be run simultaneously on the machine. Due to the need to debug and adjust parameters, approximately double the amount of computation is required. ", "page_idx": 15}, {"type": "text", "text": "D Additional Results ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "D.1 Scalability of LASE ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "To test LASE\u2019s scalability, we have extended Cleanup and Snowdrift as Tab. 6. ", "page_idx": 16}, {"type": "table", "img_path": "ry0RXTJwjy/tmp/f0cac0d69a14221253035991bfdc414094fc46ec95601281b84fe97c71916827.jpg", "table_caption": ["Table 6: Environmental parameters of extended Cleanup and SSG "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Tab. 7 shows the self-play results of LASE and baselines. LASE outperforms baselines in the two more complex environments. ", "page_idx": 16}, {"type": "table", "img_path": "ry0RXTJwjy/tmp/858524fdb887c1b3ea4f6a4df275a4a81cab0cc1084b2f3284bb1081da244854.jpg", "table_caption": ["Table 7: Self-play results (total reward) in extended Cleanup and SSG "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "D.2 Estimate the uncertainty of their social relationship ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We select the $w^{i j}$ data from the last $10^{6}$ timesteps of training to calculate their mean value and standard deviation, which estimates the uncertainty of social relationships. The calculation method is as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\overline{{w}}^{i j}=\\frac{\\sum_{t=T_{\\mathrm{max}}}^{T_{\\mathrm{max}}-10^{6}}w_{t}^{i j}}{10^{6}},\\overline{{w}}=\\sum_{i}^{n}\\sum_{j,j\\neq i}^{n}\\overline{{w}}^{i j},s=\\frac{\\sum_{i}^{n}\\sum_{j,j\\neq i}^{n}\\sqrt{\\frac{\\sum_{t=T_{\\mathrm{max}}}^{T_{\\mathrm{max}}-1e\\theta}(w_{t}^{i j}-\\overline{{w}}^{i j})^{2}}{1e6-1}}}{n\\times(n-1)}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We conduct a comparative experiment to replace the input of SR policy network ${\\hat{o}}^{j}$ with $j$ \u2019s real observation $o^{j}$ . Here is the results: ", "page_idx": 16}, {"type": "text", "text": "The results show that the mean value of LASE\u2019s inferred social relationships closely match actual observations when available, although partial observability significantly increases uncertainty. Considering that the social relationships between people in real life tend to be relatively stable and do not change drastically, we think that a possible solution to handle the uncertainty of social relationships is to introduce some smoothing techniques for $w^{i j}$ to reduce the variance of social relationships over time. This approach will be explored in our future work. ", "page_idx": 16}, {"type": "text", "text": "Meanwhile, It is important to note that Figure 8 and corresponding analysis show that LASE is able to correctly infer the relationships with different co-players and respond properly. Specifically, in the experiments conducted in Section 6.3, one LASE interacts with three rule-based co-players: cooperator, defector and random. The results show the $w^{i j}$ given to cooperative co-player is the largest, significantly higher than that given to the other two co-players. The $w^{i j}$ given to random coplayer comes second, and the smallest is given to defector. These results demonstrate the consistency between LASE\u2019s estimates of the relations and the ground truth. ", "page_idx": 16}, {"type": "table", "img_path": "ry0RXTJwjy/tmp/5f76841ccc6966feff0d0004dae6755b048b73ea306dc2cb94b3dabe1506c6f2.jpg", "table_caption": ["Table 8: The uncertainty of inferred social relationships "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "D.3 Compare the Equality with other baselines ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "As an evaluation metric, fairness should be evaluated alongside reward to measure algorithm performance effectively. Some algorithms may fail to address decision-making issues in mixed-motive games where each agent receives a small reward, but the reward disparity between agents is minimal, resulting in high fairness. Clearly, these methods are not effective. An effective method should both maximize group reward and ensure intra-group equity. Here, we include the fairness results of other baselines: ", "page_idx": 17}, {"type": "table", "img_path": "ry0RXTJwjy/tmp/2e6d9e84ab2da5ce6bad226cd1871b45fc3df60ae192ab2b22613441861fe29c.jpg", "table_caption": ["Table 9: All Equality results "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "E Broader Impact ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The rapid advancement of AI technology has brought about an explosion in the number of agents, making it unfeasible to rely on a centralized controller to achieve coordinated collective behavior. The question of how to design independent agents that excel in specific tasks and can demonstrate adequate social behavior when interacting with humans or other agents remains an open challenge. We take a tentative step towards this problem by introducing the mechanism of gifting and the theory of empathy in human society. We believe our work will have a positive impact on the interaction of multi-agent systems, the alignment of AI with human values, and the development of AI safety. ", "page_idx": 17}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: We clearly set out the contribution of our work in the abstract and introduction, and support our claims with both theory and experiments. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 18}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: We discuss the limitations of our work and future directions for improvement at the conclusion section of the paper. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 18}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We provide a complete proof of the theoretical results in Section 4.3 in Appendix A. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 19}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We provide the model structure and parameters required for the experiment in detail in Section 5.2 and Appendix C, and describe the environment in which the experiment is run in detail in Section 5.1 and Section B.2. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 19}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 20}, {"type": "text", "text": "Answer: [No] ", "page_idx": 20}, {"type": "text", "text": "Justification: We are still sorting out the code for future open source. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 20}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We provide the model structure and parameters required for the experiment in detail in Section 5.2 and Appendix C, and describe the environment in which the experiment is run in detail in Section 5.1 and Section B.2. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 20}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We run five random seeds for the experiments. In Fig. 4, Fig. 5 and Fig. 8, the solid line represents the mean performance, while the shaded area indicates the range between the mean minus the standard deviation and the mean plus the standard deviation. In some figures like Fig. 6, we only show the results under one seed for clarity due to the different division of agents under different seeds. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 20}, {"type": "text", "text": "\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 21}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: As introduced in Appendix C. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 21}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We have reviewed the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 21}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: As shown in Appendix E. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 22}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: The paper poses no such risks Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 22}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The open-source codes used in the paper are cited. The license is MIT license. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 23}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 23}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}]