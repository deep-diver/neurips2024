[{"figure_path": "ry0RXTJwjy/figures/figures_3_1.jpg", "caption": "Figure 1: Architecture of LASE. It consists of Social Relationships Inference (SRI) and Gifting. SRI conducts counterfactual reasoning to get the social relationships with co-players. The social relationship is measured by comparing the Q-value (estimated by the SR value network) of the current joint action to a counterfactual baseline which marginalizes the co-player's action, with its action distribution inferred by a perspective-taking (PT) module. PT is provided to address the challenge of predicting co-players' policies in partially observable and decentralized environments. The Gifting module, according to the inferred social relationships, determines the amount of reward to share.", "description": "The figure illustrates the architecture of the LASE algorithm.  LASE comprises two main modules: Social Relationship Inference (SRI) and Gifting. The SRI module uses counterfactual reasoning and a perspective-taking module to infer the social relationships between agents. These relationships are then used by the Gifting module to determine how much reward to share with other agents.  The figure shows the flow of information between the environment, the agent, and the two modules.", "section": "4 Methodology"}, {"figure_path": "ry0RXTJwjy/figures/figures_4_1.jpg", "caption": "Figure 2: The cooperation probability of LASE agents after convergence under different matrix-game parameters. The X-axis and Y-axis represent two parameters T and S respectively, where T\u2208 [0 : 0.02 : 2] and S\u2208 [-1:0.02: 1].", "description": "This figure shows the results of a theoretical analysis of LASE's learning process in iterated matrix games.  The surface plot displays the probability of LASE agents cooperating (vertical axis) as a function of two game parameters: T (temptation to defect) on the horizontal axis and S (sucker's payoff) on the depth axis.  The plot shows that LASE converges to higher cooperation probability as the temptation to defect (T) decreases and the sucker's payoff (S) increases.  The dashed lines and shaded regions divide the parameter space into four well-known game types (Prisoner's Dilemma (PD), Stag Hunt (SH), Snowdrift (SG), and Harmony). The red dot represents the specific game parameters used in the IPD experiments presented in the paper.", "section": "Analysis in Iterated Matrix Game"}, {"figure_path": "ry0RXTJwjy/figures/figures_5_1.jpg", "caption": "Figure 3: Graphic representations of four SSDs: (a) Coingame (5\u00d75 map), (b) Cleanup (8\u00d78 map), (c) SSH (8\u00d78 map), (d) SSG (8\u00d78 map).", "description": "This figure shows four different spatially extended social dilemmas (SSDs): Coingame, Cleanup, Sequential Stag-Hunt (SSH), and Sequential Snowdrift Game (SSG). Each subfigure displays a simplified visual representation of the game's environment, illustrating the agents' positions, resources, and potential interactions.  The map sizes are indicated in the caption, showing the scale of each game. These diverse SSDs serve to evaluate the proposed LASE algorithm in more complex and dynamic environments than simple matrix games.", "section": "5.1 Environments"}, {"figure_path": "ry0RXTJwjy/figures/figures_6_1.jpg", "caption": "Figure 4: Results in IPD. (a) The learning path of two LASE agents. They start from the lower left and converge to the upper right of the phase diagram, where both agents cooperate with a probability around 0.93. (b) The collective reward of LASE and baselines. Five seeds are randomly selected for the experiment. The solid line represents the mean performance, while the shaded area indicates the standard deviation.", "description": "This figure shows the results of the Iterated Prisoner's Dilemma (IPD) experiment.  Panel (a) is a scatter plot visualizing the learning paths of two LASE agents, showing their cooperation probabilities over time. The agents begin with low cooperation probabilities but gradually converge towards high cooperation (around 0.93).  Panel (b) compares the collective reward obtained by LASE with other baselines (GO, LOLA, A2C, and random) over training steps. The plot displays the mean collective reward and standard deviation across five random seeds.  LASE demonstrates superior performance in achieving a higher collective reward and consistent cooperation compared to the other baselines.", "section": "6 Results"}, {"figure_path": "ry0RXTJwjy/figures/figures_7_1.jpg", "caption": "Figure 5: Learning curves in four SSDs. Shown is the collective reward. All the curves are plotted using 5 training runs with different random seeds, where the solid line is the mean and the shadowed area indicates the standard deviation.", "description": "This figure presents the learning curves for four different spatially extended social dilemmas (SSDs) using LASE and other baseline algorithms.  The x-axis represents the number of steps in training, and the y-axis displays the collective reward achieved by the agents.  Each line shows the average performance over five runs with different random seeds, with a shaded region indicating the standard deviation.  The figure demonstrates how LASE compares to other methods like A2C, LIO, IA, SI, and GO in different scenarios.  It provides a visual representation of the comparative effectiveness of LASE for promoting cooperation across different types of mixed-motive environments.", "section": "6 Results"}, {"figure_path": "ry0RXTJwjy/figures/figures_7_2.jpg", "caption": "Figure 6: Learning curves of each agent in Cleanup. Since the division of labor for different random seeds is not the same, only the results under one seed are shown to distinguish individual performance.", "description": "This figure shows the learning curves for four agents in the Cleanup environment.  It displays the extrinsic rewards earned by each agent, the amount of waste cleaned by each agent, the average gifting weights each agent received from the other agents, and the total rewards for each agent over time. The figure highlights the impact of LASE's gifting mechanism in promoting fairness and mitigating exploitation. Note that since the division of labor varies with different random seeds, only results from one specific seed are displayed for easier analysis.", "section": "6.2 LASE promotes fairness"}, {"figure_path": "ry0RXTJwjy/figures/figures_8_1.jpg", "caption": "Figure 6: Learning curves of each agent in Cleanup. Since the division of labor for different random seeds is not the same, only the results under one seed are shown to distinguish individual performance.", "description": "This figure shows the learning curves for four agents in the Cleanup environment.  The top graph displays the extrinsic rewards earned by each agent over training steps, showing a significant disparity in the rewards earned. The second graph illustrates the amount of waste each agent cleans up. Agent 4 is the only one that cleans waste, and, thus receives no direct rewards from cleaning. The third graph shows the gifting weights from other agents to each agent. Agent 4 receives substantially more gifting weight compared to the other agents, suggesting a recognition of its contribution. The final graph shows the total rewards (extrinsic + gifting) received by each agent, illustrating that while Agent 4 receives no direct rewards from cleaning, it receives a high total reward through the gifting mechanism, balancing the inequality of the extrinsic rewards.", "section": "6.2 LASE promotes fairness"}, {"figure_path": "ry0RXTJwjy/figures/figures_8_2.jpg", "caption": "Figure 8: LASE's gifting weights to three rule-based co-players.", "description": "This figure shows how LASE's gifting weights change over time when interacting with three different types of agents: a cooperator (always performs the helpful action), a defector (always performs the selfish action), and a random agent.  The shaded areas represent the standard deviation across multiple runs. The results demonstrate LASE's ability to dynamically adapt its gifting strategy based on the observed behavior of its co-players, rewarding cooperators more and defectors less.", "section": "6.3 LASE distinguishes co-players and responds adaptively"}, {"figure_path": "ry0RXTJwjy/figures/figures_8_3.jpg", "caption": "Figure 9: An LASE (or GO) agent interacts with three A2C agents in Cleanup. (a) The average reward for LASE and GO groups after training 30k episodes. The LASE group shows the reward after gifting. The first two bars show whole groups' rewards. The remaining bars show the average reward for each agent. (b) The amount of the waste cleaned by each agent in the LASE group.", "description": "This figure shows the results of an experiment where one LASE agent (or GO agent for comparison) interacts with three A2C agents in the Cleanup environment.  Part (a) presents a bar chart comparing the average reward obtained by the LASE group and the GO group after 30,000 training episodes.  The LASE group's reward is shown after the gifting mechanism is applied. The chart breaks down the rewards for the whole group and each individual agent (LASE, three A2C agents). Part (b) is a line graph illustrating the amount of waste cleaned by each agent (LASE and three A2C agents) over 3,000,000 steps.", "section": "6.3 LASE distinguishes co-players and responds adaptively"}, {"figure_path": "ry0RXTJwjy/figures/figures_13_1.jpg", "caption": "Figure 10: The Schelling diagram of Cleanup, Coingame, Sequential Stag-Hunt (SSH) and Sequential Snowdrift Game (SSG). The dotted line shows the overall average return where the individual chooses defection.", "description": "This figure presents Schelling diagrams for four sequential social dilemmas: Cleanup, Coingame, Sequential Stag-Hunt (SSH), and Sequential Snowdrift Game (SSG).  A Schelling diagram illustrates the interdependencies between agents, showing how the choices of others influence an agent's incentives. The x-axis represents the number of other cooperators, and the y-axis shows the average payoff for either choosing cooperation or defection. The dotted line indicates the overall average return when an agent chooses to defect. These diagrams visually demonstrate the nature of the social dilemma in each game, highlighting the conditions under which cooperation or defection is preferred.", "section": "B Environments"}]