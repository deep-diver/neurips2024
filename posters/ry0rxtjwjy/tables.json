[{"figure_path": "ry0RXTJwjy/tables/tables_7_1.jpg", "caption": "Table 2: Mean of the gifting weights for LASE and LASE w/o during the last 10000 episodes of self-play training.", "description": "This table presents the average gifting weights assigned by LASE and its variant (LASE w/o) to other agents during the final 10,000 episodes of their self-play training across four different social dilemma environments: Coingame, Cleanup, SSG, and SSH.  It shows how the gifting strategy changes when the perspective-taking module is removed (LASE w/o), providing insight into its impact on the distribution of rewards.", "section": "6.1 LASE promotes cooperation in social dilemmas"}, {"figure_path": "ry0RXTJwjy/tables/tables_13_1.jpg", "caption": "Table 3: Classification of social dilemmas. In all dilemmas, mutual cooperation yields higher payoffs than mutual defection, yet each dilemma provides an incentive for defection. In the Snowdrift game, one player can gain a higher payoff by defecting when the other cooperates, while in the Stag Hunt game, a player can achieve higher payoffs by defecting when the other defects. The Prisoner's Dilemma encompasses both types of incentives.", "description": "This table classifies three types of social dilemmas based on their payoff matrices, highlighting the incentives for cooperation and defection in each scenario.  It shows the conditions under which each game is characterized by a preference for mutual cooperation, defection, or a mix of both, reflecting the differing dynamics of these social dilemmas.", "section": "B Environments"}, {"figure_path": "ry0RXTJwjy/tables/tables_14_1.jpg", "caption": "Table 4: Environment parameters in Cleanup", "description": "This table lists the hyperparameters used in the Cleanup environment, including the map size, probabilities of apple and waste respawning, depletion and restoration thresholds, agent view size, and the maximum number of steps per episode.", "section": "5.1 Environments"}, {"figure_path": "ry0RXTJwjy/tables/tables_15_1.jpg", "caption": "Table 5: Hyperparameters", "description": "This table presents the hyperparameters used in the experiments.  It's divided into two parts: (a) shows the hyperparameters used for the Sequential Social Dilemmas (SSDs) experiments, and (b) shows the hyperparameters used for the Iterated Prisoner's Dilemma (IPD) experiments. Each section lists parameters such as exploration rate decay parameters (\u03b5start, \u03b5div, \u03b5end), discount factors (\u03b3sc, \u03b3), weighting factor (\u03b4), learning rates (\u03b1\u03b8, \u03b1\u03bc, \u03b1\u03c6, \u03b1\u03b7), and batch size. Note the difference in learning rates between the SSDs and IPD settings.", "section": "5.2 Implementations"}, {"figure_path": "ry0RXTJwjy/tables/tables_16_1.jpg", "caption": "Table 6: Environmental parameters of extended Cleanup and SSG", "description": "This table presents the variations in parameters for the extended versions of the Cleanup and Snowdrift games.  It shows how the map size, number of players, observation size, initial amount of waste/snowdrifts, and episode length were modified to assess the scalability of the LASE algorithm in more complex scenarios.", "section": "D.1 Scalability of LASE"}, {"figure_path": "ry0RXTJwjy/tables/tables_16_2.jpg", "caption": "Table 7: Self-play results (total reward) in extended Cleanup and SSG", "description": "This table presents the total reward achieved by different multi-agent reinforcement learning algorithms in two extended social dilemma games: Cleanup.Extn and SSG.Extn. These extended versions involve a larger number of agents and a more complex environment compared to the original Cleanup and SSG games. The algorithms compared are LASE (the proposed method in the paper), IA (Inequity Aversion), LIO (Learning Incentive Optimization), SI (Social Influence), and A2C (Advantage Actor-Critic). The results show the total reward obtained by each algorithm in the extended games.  This demonstrates how well the algorithms perform in more challenging scenarios, testing their ability to cooperate and achieve higher collective rewards.", "section": "D.1 Scalability of LASE"}, {"figure_path": "ry0RXTJwjy/tables/tables_16_3.jpg", "caption": "Table 8: The uncertainty of inferred social relationships", "description": "This table presents the mean and standard deviation of the inferred social relationships (wij) calculated from the last 100,000 timesteps of training for LASE with and without the perspective-taking module in different game environments (SSH, Coingame, and Cleanup). The standard deviation estimates the uncertainty of the social relationships.  It shows that LASE's inferred social relationships are less uncertain when perspective taking is used.", "section": "D.2 Estimate the uncertainty of their social relationship"}, {"figure_path": "ry0RXTJwjy/tables/tables_17_1.jpg", "caption": "Table 9: All Equality results", "description": "This table presents the fairness results for all algorithms across various environments (SSH, SSG, Coingame, Cleanup). Fairness is measured using the Equality metric (E), which quantifies the evenness of reward distribution among agents. Higher E values represent greater fairness.  The table compares the fairness achieved by LASE (proposed algorithm) and other baselines (LASE w/o, GO, IA, LIO, SI, A2C). This provides a comparative analysis of reward distribution fairness across different methods. ", "section": "6.3 LASE distinguishes co-players and responds adaptively"}]