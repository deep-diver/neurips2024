[{"figure_path": "mlm3nUwOeQ/tables/tables_2_1.jpg", "caption": "Table 1: Comparison of results. This work is the first to address all three generalities with an \u00d5(\u221aT) optimal regret: (Agarwal et al., 2019a) addressed perturbation + loss, (Cassel and Koren, 2020) addressed feedback + loss, (Suggala et al., 2024) addressed perturbation + feedback. (Cassel and Koren, 2020) also obtained a sub-optimal \u00d5(T\u00b2/\u00b3) regret for perturbation + feedback + loss.", "description": "This table compares the results of this paper with previous works on online non-stochastic control.  It shows the achieved regret bound, the type of perturbation (adversarial, stochastic, semi-adversarial), the feedback model (full or bandit), and the type of loss function used (convex, strongly-convex, smooth, quadratic). The table highlights that this paper is the first to achieve an optimal regret bound of \u00d5(\u221aT) while addressing all three challenges: adversarial perturbations, bandit feedback, and strongly-convex smooth loss functions.", "section": "1.1 Technical Overview"}]