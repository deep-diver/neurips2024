[{"figure_path": "TWeVQ5meMW/figures/figures_1_1.jpg", "caption": "Figure 1: We illustrate the \u03bb-Harmonic reward function applied to the subject-driven generation task. Leveraging preference labels produced by the \u03bb-Harmonic reward function, alongside a few reference images, our preference-based algorithm efficiently generates unseen scenes that are both faithful to the reference images and the textual prompts.", "description": "This figure demonstrates the \u03bb-Harmonic reward function used in subject-driven text-to-image generation.  It shows how preference labels, generated using the \u03bb-Harmonic function and a few reference images, guide the generation of new images.  The generated images maintain the identity of the subject from the reference image while incorporating the details described in the accompanying text prompt.  This illustrates the effectiveness of the method for producing images that are both faithful to the original subject and consistent with the text prompt.", "section": "1 Introduction"}, {"figure_path": "TWeVQ5meMW/figures/figures_5_1.jpg", "caption": "Figure 2: Overview of the finetuning phase for RPO. First, the base diffusion model generates a few images based on novel training prompts. Second, we compute the rewards for both reference and generated images using Equation (5). Then, preference labels are sampled according to the preference distribution, as defined in Equation (6). Finally, the diffusion model is trained by minimizing both the similarity loss (Equation (7)) and preference loss (Equation (8)).", "description": "This figure illustrates the training phase of the Reward Preference Optimization (RPO) method.  It shows how the base diffusion model generates images using new training prompts.  The \u03bb-Harmonic reward function then calculates rewards for both the generated and reference images.  These rewards are used with the Bradley-Terry model to sample preference labels.  Finally, the diffusion model is fine-tuned by minimizing both similarity and preference losses.", "section": "4 Method"}, {"figure_path": "TWeVQ5meMW/figures/figures_7_1.jpg", "caption": "Figure 3: Qualitative comparison with other subject-driven text-to-image methods, adapted from [7]", "description": "This figure compares the image generation results of four different methods: SuTI, Re-Imagen, DreamBooth, and the proposed RPO method.  The methods are applied to three separate subject-driven image generation tasks. For each task, example reference images are displayed and followed by the generated images for each of the four methods.  The purpose of the figure is to qualitatively showcase the comparative performance of RPO against state-of-the-art methods in terms of subject fidelity, text prompt adherence, and overall image quality. The results suggest that RPO produces images that are more faithful to both the reference images and text prompts than other methods, especially when dealing with more challenging prompt variations.", "section": "5.2 Results"}, {"figure_path": "TWeVQ5meMW/figures/figures_8_1.jpg", "caption": "Figure 4: Changes in the \u03bb-Harmonic reward value during RPO training process.", "description": "This figure shows how the \u03bb-Harmonic reward changes during the training process for two different subjects (backpack and cat).  Each row represents a subject and displays the generated images and the corresponding \u03bb-Harmonic reward at various gradient steps (GS) during fine-tuning. The goal is to illustrate how the reward function helps guide the training process and regularize against overfitting by monitoring the image quality and its alignment with textual prompts. The reward values indicate the model's performance, with higher values suggesting better image quality and alignment.  The figure demonstrates that the \u03bb-Harmonic reward function provides informative feedback for early stopping, allowing for more efficient training.", "section": "5.3 Ablation Study and Method Analysis"}, {"figure_path": "TWeVQ5meMW/figures/figures_9_1.jpg", "caption": "Figure 5: Different \u03bbval\u2019s will lead to different results. A small \u03bbval assigns a higher weight for text-to-image alignment and leads to diverse generation. A large \u03bbval may also cause overfitting.", "description": "This figure shows the impact of the \u03bbval hyperparameter on the generated images.  \u03bbval controls the balance between faithfulness to the reference image and alignment with the text prompt.  A lower \u03bbval prioritizes text alignment, resulting in more diverse outputs but potentially sacrificing image fidelity. Conversely, a higher \u03bbval prioritizes image similarity, leading to more consistent results but at the risk of overfitting to the reference image.", "section": "5.3 Ablation Study and Method Analysis"}, {"figure_path": "TWeVQ5meMW/figures/figures_14_1.jpg", "caption": "Figure 6: Training prompts for objects and live subjects.", "description": "The figure shows two lists of prompts used for training the RPO model.  The \"Object prompts\" list contains prompts describing inanimate objects in various contexts and artistic styles.  The \"Live subject prompts\" list contains prompts about living subjects (presumably animals, based on the paper's focus), again in varied settings and styles.  These prompts are designed to test the model's ability to generate images that accurately reflect both the subject and the described scene, focusing on subject identity and contextualization. The use of placeholders like {unique_token} and {subject_token} suggests a systematic approach to generating many training examples with varied descriptions for each object/subject.", "section": "4 Method"}, {"figure_path": "TWeVQ5meMW/figures/figures_16_1.jpg", "caption": "Figure 3: Qualitative comparison with other subject-driven text-to-image methods, adapted from [7]", "description": "This figure compares the qualitative results of several subject-driven text-to-image generation methods, including DreamBooth, SuTI, Dream-SuTI, and the proposed RPO method.  For three different prompts and associated reference images, the generated images show how each model handles the tasks of preserving the subject's identity and aligning with the textual descriptions. The comparison highlights the strengths and weaknesses of each method in terms of faithfulness to the reference image and adherence to the text prompt.", "section": "5.2 Results"}, {"figure_path": "TWeVQ5meMW/figures/figures_16_2.jpg", "caption": "Figure 5: Different \u03bbval\u2019s will lead to different results. A small \u03bbval assigns a higher weight for text-to-image alignment and leads to diverse generation. A large \u03bbval may also cause overfitting.", "description": "This figure shows the impact of the \u03bbval hyperparameter on the generated images.  Three different \u03bbval values (0.3, 0.5, and 0.7) are tested. Lower \u03bbval values emphasize text-to-image alignment, resulting in more diverse generations. Conversely, higher \u03bbval values prioritize image-to-image similarity, which can cause overfitting and reduce diversity.", "section": "5.3 Ablation Study and Method Analysis"}, {"figure_path": "TWeVQ5meMW/figures/figures_18_1.jpg", "caption": "Figure 9: RPO's failure modes include: (1) The context and the appearance of the subject becoming entangled. (2) The trained model failing to generate an image with respect to the input prompt. (3) The trained model still overfitting to the training set.", "description": "This figure shows three failure cases of the Reward Preference Optimization (RPO) model. The first case demonstrates context-appearance entanglement, where the generated image is influenced by both the context and the appearance of the subject, resulting in an unrealistic combination. The second case illustrates incorrect contextual integration, where the model fails to accurately incorporate the contextual information into the generated image, leading to an inaccurate representation. The third case shows overfitting, where the model overfits to the training data, resulting in generated images that closely resemble the training images but lack generalization to unseen prompts.", "section": "A.4 Limitations"}, {"figure_path": "TWeVQ5meMW/figures/figures_18_2.jpg", "caption": "Figure 9: RPO's failure modes include: (1) The context and the appearance of the subject becoming entangled. (2) The trained model failing to generate an image with respect to the input prompt. (3) The trained model still overfitting to the training set.", "description": "This figure shows three examples of failure cases of the Reward Preference Optimization (RPO) model. The first failure mode demonstrates the entanglement between the context and the appearance of the subject, where the context influences the subject's appearance rather than only the background. The second failure mode illustrates the model's failure to generate an image that aligns with the given prompt. The third failure mode shows that the model is still overfitting to the training set.", "section": "A.4 Limitations"}, {"figure_path": "TWeVQ5meMW/figures/figures_19_1.jpg", "caption": "Figure 1: We illustrate the A-Harmonic reward function applied to the subject-driven generation task. Leveraging preference labels produced by the A-Harmonic reward function, alongside a few reference images, our preference-based algorithm efficiently generates unseen scenes that are both faithful to the reference images and the textual prompts.", "description": "This figure shows example results of subject-driven text-to-image generation using the proposed A-Harmonic reward function. It demonstrates the ability of the method to generate images that are consistent with both a given text prompt and a set of reference images. The top row showcases images generated for the text prompt \u201cduck toy\u201d using various styles and contexts. The bottom row displays similar results for the text prompt \u201cdog\u201d with diverse background settings.", "section": "1 Introduction"}, {"figure_path": "TWeVQ5meMW/figures/figures_19_2.jpg", "caption": "Figure 1: We illustrate the A-Harmonic reward function applied to the subject-driven generation task. Leveraging preference labels produced by the A-Harmonic reward function, alongside a few reference images, our preference-based algorithm efficiently generates unseen scenes that are both faithful to the reference images and the textual prompts.", "description": "The figure shows examples of subject-driven text-to-image generation using the A-Harmonic reward function.  It demonstrates how the proposed method uses a few reference images and preference labels to generate new images that are both consistent with the reference images and accurately reflect the text prompt. For instance, images of a dog are generated in various artistic styles and settings, successfully integrating both the textual description and visual characteristics of the reference images.", "section": "1 Introduction"}, {"figure_path": "TWeVQ5meMW/figures/figures_19_3.jpg", "caption": "Figure 12: Expression modification samples from the RPO algorithm. RPO can integrate subject with various unseen expressions in the reference images. We also notice the pose of generated images, e.g., a [V] sleepy dog, were not displayed in the training set.", "description": "This figure shows the results of applying the Reward Preference Optimization (RPO) algorithm to modify the expressions of a subject in generated images.  The algorithm takes a set of reference images showing the subject with different expressions and uses them to generate new images with novel expressions (depressed, joyous, sleepy, screaming) that were not present in the training data. The results demonstrate RPO's ability to generate diverse and faithful images with unseen expressions.", "section": "5.2 Results"}, {"figure_path": "TWeVQ5meMW/figures/figures_20_1.jpg", "caption": "Figure 13: Accessories samples from the RPO algorithm. Conditioned the prompts, \u201ca [V] chow chow wearing a [target outfit]\u201d, the generated images retains the unique features of the reference images, e.g., the hair color and breed of the subject dog. The interaction between subject dog and the outfits is realistic.", "description": "This figure shows several images generated by the Reward Preference Optimization (RPO) algorithm.  Each image depicts a chow chow dog wearing different outfits (chef, nurse, police, Superman, witch, Ironman, angel wings, firefighter).  The caption highlights that RPO successfully maintains the dog's breed and hair color characteristics while realistically integrating it with the various outfits.  This demonstrates RPO's ability to control subject identity while adhering to textual prompts.", "section": "5.2 Results"}, {"figure_path": "TWeVQ5meMW/figures/figures_20_2.jpg", "caption": "Figure 1: We illustrate the \u03bb-Harmonic reward function applied to the subject-driven generation task. Leveraging preference labels produced by the \u03bb-Harmonic reward function, alongside a few reference images, our preference-based algorithm efficiently generates unseen scenes that are both faithful to the reference images and the textual prompts.", "description": "The figure shows examples of subject-driven text-to-image generation using the \u03bb-Harmonic reward function.  The top row shows images of duck toys generated with various styles and locations described in text prompts. The bottom row shows images of a dog in different settings, again based on text prompts and a reference image.  The A-Harmonic reward function helps the model select for images that are both faithful to the reference image and consistent with the textual description.", "section": "1 Introduction"}, {"figure_path": "TWeVQ5meMW/figures/figures_20_3.jpg", "caption": "Figure 15: Multi-view samples from the RPO algorithm. We generate images from specified viewpoints of the subject. For the top and bottom views, we use the prompts \u201ca [V] cat looking up/down at the camera\u201d. For the back and side views, we apply the prompts \u201ca back/side view of a [V] cat\u201d.", "description": "This figure shows the results of using the Reward Preference Optimization (RPO) algorithm to generate images of a cat from multiple viewpoints.  The prompts used specified the desired view (top, bottom, back, side), and the algorithm successfully generated images that accurately reflect those views while maintaining the identity of the cat.", "section": "5.2 Results"}, {"figure_path": "TWeVQ5meMW/figures/figures_21_1.jpg", "caption": "Figure 16: Novel hybrid synthesis samples from the RPO algorithm. We apply the prompts \u201ca cross of a [V] chow chow and [target species]\u201d to the RPO-trained model to generate these images. We highlight that RPO can synthesize new hybrids that retain the identity of the subject chow chow and perform property modifications.", "description": "This figure shows examples of novel hybrid animal synthesis generated by the RPO algorithm.  The algorithm successfully combined the features of a Chow Chow with those of other animals (bear, koala, lion, panda) while maintaining the core identity of the Chow Chow. This demonstrates the algorithm's ability to generate novel and imaginative variations of subjects.", "section": "5.2 Results"}, {"figure_path": "TWeVQ5meMW/figures/figures_21_2.jpg", "caption": "Figure 1: We illustrate the A-Harmonic reward function applied to the subject-driven generation task. Leveraging preference labels produced by the A-Harmonic reward function, alongside a few reference images, our preference-based algorithm efficiently generates unseen scenes that are both faithful to the reference images and the textual prompts.", "description": "This figure shows examples of images generated using the A-Harmonic reward function for subject-driven text-to-image generation.  The top row shows images generated from a textual prompt about duck toys, demonstrating the model's ability to maintain fidelity to both the text prompt and provided reference image. The bottom row shows similar results for images of a dog, highlighting the algorithm's capacity to create novel scenes while preserving subject identity.", "section": "1 Introduction"}]