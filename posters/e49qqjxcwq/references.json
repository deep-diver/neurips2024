{"references": [{"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-01", "reason": "This paper introduces CLIP, a highly influential model for learning visual representations from natural language supervision, which is directly relevant to the methodology of PLIP."}, {"fullname_first_author": "Kaiming He", "paper_title": "Masked autoencoders are scalable vision learners", "publication_date": "2022-06-01", "reason": "This paper introduces MAE, a highly influential self-supervised learning method for visual representation learning, which is relevant to the pretext tasks used in PLIP."}, {"fullname_first_author": "Junnan Li", "paper_title": "BLIP: Bootstrapping language-image pre-training for unified vision-language understanding and generation", "publication_date": "2022-07-01", "reason": "This paper introduces BLIP, a prominent vision-language pre-training framework, providing a foundation for PLIP's approach to learning person representations."}, {"fullname_first_author": "Dengpan Fu", "paper_title": "Unsupervised pre-training for person re-identification", "publication_date": "2021-06-01", "reason": "This paper presents a pioneering effort in unsupervised pre-training for person re-identification, providing a context for PLIP's focus on person-centric representation learning."}, {"fullname_first_author": "Weihua Chen", "paper_title": "Beyond appearance: a semantic controllable self-supervised learning framework for human-centric visual tasks", "publication_date": "2023-06-01", "reason": "This paper explores self-supervised learning for human-centric tasks, offering insights into the design choices and challenges addressed in PLIP."}]}