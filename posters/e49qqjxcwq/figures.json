[{"figure_path": "e49QqJxCwq/figures/figures_1_1.jpg", "caption": "Figure 1: Illumination of our framework. Based on the constructed dataset, we pre-train a language-image model by three pretext tasks and transfer the model to some downstream person-centric tasks.", "description": "The figure illustrates the overall framework of the proposed PLIP model.  It begins with the creation of a large-scale person dataset (SYNTH-PEDES). This dataset is used to pre-train a language-image model using three pretext tasks: Text-guided Image Colorization, Image-guided Attributes Prediction, and Identity-based Vision-Language Contrast.  The resulting pre-trained model is then fine-tuned on various person-centric downstream tasks, including image-based and text-based person re-identification, person attribute recognition, person search, and human parsing.", "section": "1 Introduction"}, {"figure_path": "e49QqJxCwq/figures/figures_3_1.jpg", "caption": "Figure 2: Overview of our proposed framework incorporating a text-guided image colorization task, an image-guided attributes prediction task and an identity-based vision-language contrast task.", "description": "This figure illustrates the PLIP framework, which incorporates three pretext tasks: text-guided image colorization (TIC), image-guided attributes prediction (IAP), and identity-based vision-language contrast (IVLC).  The framework uses a dual-branch encoder structure (visual and textual encoders) to learn generic person representations.  TIC aims to restore color to a grayscale person image using textual descriptions, helping the model learn relationships between image regions and textual phrases. IAP predicts masked attribute phrases in a description using color images, enabling understanding of key areas and semantic concepts.  IVLC associates representations at the identity level, rather than the instance level, which is crucial for distinguishing different people. The whole process uses a combined loss to train the model, explicitly learning fine-grained and meaningful cross-modal associations.", "section": "2 PLIP: Representation Learning Framework"}, {"figure_path": "e49QqJxCwq/figures/figures_5_1.jpg", "caption": "Figure 3: Visualization of some examples in our SYNTH-PEDES dataset.", "description": "This figure shows three sets of examples from the SYNTH-PEDES dataset. Each set contains three images of the same person with three different captions describing the person's attributes and attire.  The captions demonstrate the variety and detail of the automatically generated descriptions in SYNTH-PEDES.", "section": "3 SYNTH-PEDES: A Large-scale Image-text Person Dataset"}, {"figure_path": "e49QqJxCwq/figures/figures_8_1.jpg", "caption": "Figure 2: Overview of our proposed framework incorporating a text-guided image colorization task, an image-guided attributes prediction task and an identity-based vision-language contrast task.", "description": "This figure illustrates the overall architecture of the PLIP framework. It shows three main pretext tasks: Text-guided Image Colorization (TIC), Image-guided Attributes Prediction (IAP), and Identity-based Vision-Language Contrast (IVLC).  The visual and textual encoders are shown, along with the specific processing steps for each task.  TIC aims to restore color to a grayscale image guided by text. IAP predicts masked attribute phrases in a textual description using the corresponding image. IVLC links visual and textual representations at the identity level, rather than instance level. The figure demonstrates the dual-branch encoder structure and how the three tasks work together to learn generic and discriminative person representations.", "section": "2 PLIP: Representation Learning Framework"}, {"figure_path": "e49QqJxCwq/figures/figures_17_1.jpg", "caption": "Figure 5: Visualization of gray-scale person image colorization results by changing the color words in textual descriptions.", "description": "This figure shows the results of a text-guided image colorization task.  Multiple rows display the same grayscale person image, but each column represents a different colorization result based on changing a single color word in the corresponding textual description. This demonstrates how the model interprets and applies color information from the text to different image regions.", "section": "A.4 Altering Color Word Affects Image Colorization: Visualization"}, {"figure_path": "e49QqJxCwq/figures/figures_18_1.jpg", "caption": "Figure 1: Illumination of our framework. Based on the constructed dataset, we pre-train a language-image model by three pretext tasks and transfer the model to some downstream person-centric tasks.", "description": "This figure illustrates the overall framework of the proposed PLIP model.  The framework begins with constructing a large-scale dataset of image-text pairs. This dataset is then used to pre-train a language-image model using three different pretext tasks: Text-guided Image Colorization, Image-guided Attributes Prediction, and Identity-based Vision-Language Contrast. Finally, the pre-trained model is transferred to various downstream person-centric tasks such as image-based and text-based re-identification, attribute recognition, person search, and human parsing.", "section": "1 Introduction"}, {"figure_path": "e49QqJxCwq/figures/figures_19_1.jpg", "caption": "Figure 1: Illumination of our framework. Based on the constructed dataset, we pre-train a language-image model by three pretext tasks and transfer the model to some downstream person-centric tasks.", "description": "This figure illustrates the overall framework of the proposed PLIP model. It consists of three stages: 1) a large-scale dataset construction stage, where a large-scale person dataset with image-text pairs is created; 2) a language-image pre-training stage, where the language-image model is pre-trained using three pretext tasks; and 3) a downstream task transfer stage, where the pre-trained model is transferred to several person-centric downstream tasks for evaluation. The three pretext tasks are: Text-guided Image Colorization, Image-guided Attributes Prediction, and Identity-based Vision-Language Contrast.", "section": "1 Introduction"}, {"figure_path": "e49QqJxCwq/figures/figures_25_1.jpg", "caption": "Figure 2: Overview of our proposed framework incorporating a text-guided image colorization task, an image-guided attributes prediction task and an identity-based vision-language contrast task.", "description": "This figure shows the overall architecture of the PLIP framework, which incorporates three pretext tasks: text-guided image colorization (TIC), image-guided attributes prediction (IAP), and identity-based vision-language contrast (IVLC).  The visual and textual encoders process the input image and text respectively.  The three pretext tasks work together to learn fine-grained and meaningful cross-modal associations for person representation learning.  Each task's processing steps are shown within the framework diagram.", "section": "2 PLIP: Representation Learning Framework"}, {"figure_path": "e49QqJxCwq/figures/figures_25_2.jpg", "caption": "Figure 2: Overview of our proposed framework incorporating a text-guided image colorization task, an image-guided attributes prediction task and an identity-based vision-language contrast task.", "description": "This figure illustrates the PLIP framework, a novel language-image pre-training framework for person representation learning.  It shows the three pretext tasks used: Text-guided Image Colorization (TIC), Image-guided Attributes Prediction (IAP), and Identity-based Vision-Language Contrast (IVLC). The framework uses a dual-branch encoder structure (visual and textual encoders) to learn generic person representations by jointly training on these three pretext tasks. The figure also highlights the process of feature extraction, fusion, and prediction for each task.", "section": "2 PLIP: Representation Learning Framework"}, {"figure_path": "e49QqJxCwq/figures/figures_26_1.jpg", "caption": "Figure 2: Overview of our proposed framework incorporating a text-guided image colorization task, an image-guided attributes prediction task and an identity-based vision-language contrast task.", "description": "This figure illustrates the PLIP framework, showing the three pretext tasks used for pre-training: Text-guided Image Colorization (TIC), Image-guided Attributes Prediction (IAP), and Identity-based Vision-Language Contrast (IVLC).  It shows the dual-branch encoder structure (visual and textual) and how the three tasks are integrated.  The figure highlights the flow of information through the encoders and the fusion of visual and textual features for each task.  The overall architecture is designed to learn fine-grained and meaningful cross-modal associations for person representation learning.", "section": "2 PLIP: Representation Learning Framework"}, {"figure_path": "e49QqJxCwq/figures/figures_26_2.jpg", "caption": "Figure 2: Overview of our proposed framework incorporating a text-guided image colorization task, an image-guided attributes prediction task and an identity-based vision-language contrast task.", "description": "The figure provides a visual representation of the PLIP framework, highlighting its three main pretext tasks: text-guided image colorization (TIC), image-guided attribute prediction (IAP), and identity-based vision-language contrast (IVLC).  It shows the dual-branch encoder structure of the model, how each pretext task works, and the flow of information between the visual and textual encoders.  TIC focuses on restoring color information to grayscale images using textual descriptions. IAP focuses on predicting masked attribute words in the description based on the image. IVLC focuses on contrasting representations at the identity level instead of the instance level. The figure illustrates the overall architecture and the interconnections of the three pretext tasks.", "section": "2 PLIP: Representation Learning Framework"}, {"figure_path": "e49QqJxCwq/figures/figures_27_1.jpg", "caption": "Figure 2: Overview of our proposed framework incorporating a text-guided image colorization task, an image-guided attributes prediction task and an identity-based vision-language contrast task.", "description": "The figure illustrates the PLIP framework, which incorporates three pretext tasks: text-guided image colorization (TIC), image-guided attributes prediction (IAP), and identity-based vision-language contrast (IVLC).  TIC aims to restore color information in a grayscale image guided by text. IAP predicts masked attribute phrases in text descriptions given the corresponding image.  IVLC aligns visual and textual representations at the identity level to learn more meaningful cross-modal associations. The framework consists of dual-branch encoders (visual and textual) and three pretext task modules.  The output of the encoders, representing visual and textual features, is fused for cross-modal learning and downstream tasks.", "section": "2 PLIP: Representation Learning Framework"}]