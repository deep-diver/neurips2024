[{"type": "text", "text": "Improving Neural Network Surface Processing with Principal Curvatures ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Josquin Harrison James Benn Inria Inria Sophia Antipolis Sophia Antipolis josquin.harrison@inria.fr james.benn@inria.fr ", "page_idx": 0}, {"type": "text", "text": "Maxime SermesantInriaSophia Antipolismaxime.sermesant@inria.fr", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The modern study and use of surfaces is a research topic grounded in centuries of mathematical and empirical inquiry. From a mathematical point of view, curvature is an invariant that characterises the intrinsic geometry and the extrinsic shape of a surface. Yet, in modern applications the focus has shifted away from finding expressive representations of surfaces, and towards the design of efficient neural network architectures to process them. The literature suggests a tendency to either overlook the representation of the processed surface, or use overcomplicated representations whose ability to capture the essential features of a surface is opaque. We propose using curvature as the input of neural network architectures for surface processing, and explore this proposition through experiments making use of the shape operator. Our results show that using curvature as input leads to significant a increase in performance on segmentation and classification tasks, while allowing far less computational overhead than current methods. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Surfaces are a natural representation for many real world objects ranging from organs and organisms to archaeological artefacts. They are also a central tool in virtual environments such as computer games, or computer-aided design. This ubiquity has resulted in a large body of work dedicated to mathematical methods developed for the efficient use of surfaces, as well as their analysis. ", "page_idx": 0}, {"type": "text", "text": "The goal of traditional computational surface analysis is to find a representation of a surface that is expressive enough to capture details relevant for the problem or task at hand, while being computationally light-weight. However, the effectiveness of Convolutional Neural Network (CNN) in image processing opened new doors to surface processing. The design of efficient convolution-like operations to adapt neural networks (NN) to surfaces alleviated the need for complex and detailed representations, to the point where most state of the art architectures use extrinsic vertex coordinates as input, letting the NN models learn the surface structure at multiple scales. While some attempts were made to use well known representations as inputs, yielding some increase in performance [38, 35], the general consensus is that the model should be able to learn it by itself [25]. While it is true that neural networks are efficient at capturing surface features at multiple scales, the use of a local surface representation that is more expressive and more natural to interpret than extrinsic coordinates should naturally improve the performance of the network. The optimal choice of representation should be somewhere between coarse extrinsic vertex coordinates, and more complex representations. ", "page_idx": 0}, {"type": "text", "text": "In Riemannian geometry, the shape operator is the main tool linking the intrinsic geometry of a surface with its bending and curving in ambient space. Its eigenvalues are the principal curvatures. Their product and evenly divided sum give precisely the Gauss and mean curvature of the surface, respectively. We hypothesize that the optimal choice for a local surface representation that meets the requirements of surface processing is the set of principal curvatures: they characterize the surface up to isometry (location and orientation in space); they are purely local, which allows the neural network to decide on more general surface features; and they are lightweight, leaving very little computational overhead in any scenario. This work tests our hypothesis against two widely used representations of surfaces in three state of the art NN architectures. ", "page_idx": 1}, {"type": "text", "text": "In the next section we give an overview of surface processing, and introduce shape representations and learning methods. In section 3 we give an introduction to the Shape Operator, although we should remark now that this is not the first appearance of the shape operator in the surface processing literature. The shape operator has already become an efficient tool in surface processing and is, among other things, used to define local tangent frames and compute surface features like creases [31]. Following these introductions, we then conduct extensive experiments in section 4, comparing principal curvature with three other representations in conjunction with three different NN architectures, on two segmentation datasets and one classification dataset, that shows how principal curvature enhances any state of the art model in different tasks. In addition to outperforming other methods, we show that this more concise representation is faster to compute, leaving minimal computational overhead when added to a pipeline. ", "page_idx": 1}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "The first step to surface processing is usually its discretisation as a mesh or point cloud, which is particularly useful for visualisation or rendering. From this starting point, novel representations have been derived in an effort to provide tools for different surface related tasks. These tasks include surface matching, semantic segmentation, classification, or even shape retrieval. ", "page_idx": 1}, {"type": "text", "text": "Historically, the general trend has been to find compact descriptors of a shape which could be then compared within a dataset. A long list of such descriptors exist, among them signaturebased descriptors, such as Heat Kernel Signature (HKS) [42] or wave kernel signature [4], proved to be particularly efficient. Closely related are histogram methods, which are often combined with signatures to provide expressive representations such as the SHOT [37] or the Echo [26] descriptors. Geometric measure theory has also been a source of inspiration for developing efficient representations, such as geometric currents [5] leveraging on finite elements, or kernel-based currents [43] and varifolds [10] tailored for shape deformation. Such representations can be used in conjunction with classical statistical analysis tools, e.g [26, 37, 5], although they are often building blocks for specialised methods on surfaces such as LDDMM [49], functional maps [29] or spectral-based analysis [45]. ", "page_idx": 1}, {"type": "text", "text": "With the advent of deep learning, many methods previously stated were re-written with the help of neural networks resulting in Deep functional maps [20], and ResNet-LDDMM [2], to name but a few. Representations of surfaces themselves were proposed as neural networks, such as DeepSDF [32] or DeepCurrents [30]. As convolutions proved particularly effective when learning on images, i.e structured grids, some work proposed voxel-based solutions to the study of surfaces [23]. Others suggested representing surfaces as geometric images [40], on which convolutions can be applied. A second generation of geometric deep learning has focused on building network architecture specifically tailored to work directly on surfaces, i.e meshes or point clouds. From a point cloud perspective, Point Net [34] and its extension Point $\\mathrm{Net}++$ [35] consider the surface as a set of points by applying set operations on them. Among others, DGCNN [46] applies a convolution-like operation on dynamic graphs constructed layer-wise. MeshCNN [16] on the other hand fully leverages the mesh structure to develop operation unique to triangulations. Transformer based architectures have also appeared for the specific purpose of surface processing [18]. Among them, and in a similar vein as before, GaTr [8] proposes to represent geometric data in an algebra of choice and designs an architecture with operations belonging to this algebra. An effort to have efficient generalisation of convolutions on surfaces was proposed by [22], although the lack of global coordinates creates ambiguity in local operations. To alleviate this problem, a large body of work has proposed rotation equivariant operations, namely GemCNN [12], augmenting graph NNs, or field convolutions [25]. Finally, recent models propose to bypass the problem of generalising convolutions by focusing on well defined operations on surfaces, such as discrete exterior calculus in Hodge Net [41], heat diffusion in Diffusion Net [38] or a suit of known operators in Delta Net [48]. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "As model architectures include more and more knowledge of shapes, the need for a better representation of the input to these models has decreased. Outside of models that contain operations proper to the structure of choice (e.g [16, 8]), most models naturally accept as input the coordinates at every point. Some papers propose to augment the model by inputing higher dimensional descriptors initially designed for a more direct analysis, such as the ones previously mentioned (e.g HKS, WKS, SHOT). Such proposals can be seen in [38], where HKS interacts well with the diffusion part of the architecture, or [35] were they combine HKS, WKS, Gaussian curvature through concatenation and PCA. However, recent work has dismissed this idea [25], citing the results of Diffusion Net [38] which show no great improvement when moving from coordinates to HKS. ", "page_idx": 2}, {"type": "text", "text": "As methods for learning on surfaces have evolved, we suggest that a better input representation is a simpler one, yet is more expressive than coordinates. We suggest that we can scale back to the simplest differentiable invariant of a surface: its curvature. ", "page_idx": 2}, {"type": "text", "text": "3 The Shape Operator ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Here we give a conceptual introduction to the Shape Operator and describe how its eigenvalues completely characterize the surface to which it belongs (section 3.1). In section 3.2 we describe an explicit calculation of the Shape Operator which igl\u2019s implementation of the principal curvatures is based on \u2013 this is the implementation we use in our experiments (see section 4.1). Those already familiar with the differential geometry of curves and surfaces may skip ahead to section 4; for others this section serves as a concise introduction \u2013 although, we do rely on a basic understanding of functions of several variables and their derivatives, and surfaces and their tangent spaces. ", "page_idx": 2}, {"type": "text", "text": "Surfaces in $\\mathbb{R}^{3}$ will be denoted by $S$ and $\\overline{S}$ , points in surfaces by $p$ \u2019s and $q$ \u2019s, and the tangent space to $S$ at a point $p\\in S$ by $T_{p}S$ . Maps from $\\mathbb{R}^{3}$ to itself will be denoted by $F:\\mathbb{R}^{3}\\rightarrow\\mathbb{R}^{3}$ , and their derivatives at a point $p$ by ${\\bar{D}}F_{p}$ \u2013 the Jacobian matrix. A parameterisation $X$ of a smooth surface $S$ is a diffeomorphism between an open set $U\\subset\\mathbb{R}^{2}$ and an open set $V\\subset S$ , and provides a mathematical description of $S$ as it lies in $\\mathbb{R}^{3}$ . The standard Euclidean inner product on $\\mathbb{R}^{3}$ will be signified by $\\langle\\cdot,\\cdot\\rangle$ , and it\u2019s restriction to a surface $S$ and its tangent bundle $\\begin{array}{r}{\\bar{T5}=\\coprod_{p\\in S}T_{p}S}\\end{array}$ by $g_{S}\\left(\\cdot,\\cdot\\right)$ , which we call the induced metric. A normal vector to $S$ at $p$ is one which is orthogonal to every vector $v$ in $T_{p}S$ (measured in $\\langle\\cdot,\\cdot\\rangle_{,}$ ) and will be denoted by $N_{p}$ ; if we have a field of normal vectors in an open set around $p$ then this field will be denoted simply by $N$ . ", "page_idx": 2}, {"type": "text", "text": "The Shape Operator of a surface $S$ at a point $p\\in S$ measures the rate at which surface normal vectors $N$ separate around $p$ , which is precisely the bending of the surface in space: ", "page_idx": 2}, {"type": "text", "text": "Definition 1 Given a point $p$ on a surface $S\\subset\\mathbb{R}^{3}$ , and the unit normal vector $N$ defined on a neighbourhood $U$ of $p_{:}$ , the shape operator is the linear map ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{S}_{p}:T_{p}S\\longrightarrow T_{p}S\\qquad}\\\\ {v\\longmapsto-\\nabla_{v}N,}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $T_{p}S$ denotes the tangent space of $S$ at point $p$ ", "page_idx": 2}, {"type": "text", "text": "In other words, the shape operator $\\mathbf{S}_{p}$ tells us how the normal vector changes as we move in $S$ , in the direction of $v$ from $p$ . One possible way to visualise the shape operator is through the Gauss map, which identifies each point $\\bar{p}\\in S\\subset\\mathbb{R}^{3}$ with its unit normal vector $N_{p}$ , now thought of as a point in $\\mathbb{S}^{2}$ . The shape operator is then the differential of the Gauss map at $p$ and is a tangent vector to $\\mathbb{S}^{2}$ at the image $N_{p}$ of $p$ , as illustrated in figure 1. ", "page_idx": 2}, {"type": "text", "text": "The operator $\\mathbf{S}_{p}$ is linear for each $p\\in S$ , and self-adjoint in the Euclidean inner product $\\langle\\cdot,\\cdot\\rangle$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\langle\\mathbf{S}_{p}(v),w\\rangle=\\langle v,\\mathbf{S}_{p}(w)\\rangle.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "It can therefore be represented by a symmetric $2\\times2$ matrix $[\\mathbf{S}_{p}]:\\,T_{p}S\\to T_{p}S$ at each point $p\\in S$ . It is well-known that symmetric matrices admit a complete system of orthonormal eigenvectors $(e_{1},e_{2})$ spanning the space on which they act. The matrix representation $\\big[\\mathbf{S}_{p}\\big]$ with respect to the basis $(e_{1},e_{2})$ has the simple form: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{[{\\bf S}_{p}]=\\left(\\!\\!\\begin{array}{c c}{\\kappa_{1}}&{0}\\\\ {0}&{\\kappa_{2}}\\end{array}\\!\\!\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "image", "img_path": "8koaqRdRYH/tmp/df617aafb1a9cee78335f57aeca4d6a4ad63b31ee6582236c9cbb87c5184821c.jpg", "img_caption": ["Figure 1: The shape operator may be visualised via the Gauss map. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Where $\\kappa_{1}$ and $\\kappa_{2}$ are the eigenvalues of $S_{p}$ . ", "page_idx": 3}, {"type": "text", "text": "Definition 2 Let $S$ be a surface in $\\mathbb{R}^{3}$ , $p$ a point in $S$ , $\\mathbf{S}_{p}$ the shape operator at $p$ and $\\left[{\\bf S}_{p}\\right]$ its matrix representation. ", "page_idx": 3}, {"type": "text", "text": "1. The eigenvalues $\\kappa_{1}(p)$ and $\\kappa_{2}(p)$ of $\\left[\\mathbf{S}_{p}\\right]$ at $p$ are the principal curvatures of $S$ at $p$ , and their corresponding eigenvectors $e_{1}$ and $e_{2}$ are the principal directions; 2. The Gauss curvature $\\kappa$ of $S$ at $p$ is the product $\\kappa_{1}(p)\\cdot\\kappa_{2}(p)$ of the principal curvatures; 3. The mean curvature $H_{p}$ is the averag e \u03ba1(p)+2\u03ba2(p)of the principal curvatures ", "page_idx": 3}, {"type": "text", "text": "The Gauss and mean curvatures can be equivalently interpreted as the determinant and half the trace of $\\left[\\mathbf{S}_{p}\\right]$ , respectively. ", "page_idx": 3}, {"type": "text", "text": "The importance of these quantities is two-fold: (1) two surfaces differ only in location and orientation in space if and only if they have the same principal curvatures (Theorems 9.1 and 9.2 in [28]) \u2013 that is, the shape operator completely characterizes the shape of a surface; and (2) the Gauss and mean curvature generate all possible differential invariants of a surface (see Guggenheim [15], Olver [27]) \u2013 in particular, Gauss and mean curvature are fundamental characteristics of the shape of a surface, and the inclusion of the higher order invariants they generate into a representation could even improve the results shown here. ", "page_idx": 3}, {"type": "text", "text": "3.1 Congruence ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To explain how Gauss and mean curvature completely describe the shape of a surface we need a few more definitions. ", "page_idx": 3}, {"type": "text", "text": "An isometry of $\\mathbb{R}^{3}$ is a map $F\\colon\\mathbb{R}^{3}\\rightarrow\\mathbb{R}^{3}$ whose differential preserves the angles between tangent vectors at every point of $\\mathbb{R}^{n}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\langle v,w\\rangle_{p}=\\langle D F_{p}\\cdot v,D F_{p}\\cdot w\\rangle_{p},\\quad\\forall v,\\,w\\in T_{p}\\mathbb{R}^{3}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "If $g_{S}$ is the Riemannian inner product induced on $T S$ by the Euclidean inner product $\\langle\\cdot,\\cdot\\rangle$ then an isometry between two surfaces is a map $\\eta:\\,S\\to{\\overline{{S}}}$ whose differential preserves the angles between tangent vectors to $S$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\ng_{S}(v,w)=g_{\\overline{{S}}}(D\\eta\\cdot v,D\\eta\\cdot w).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Every isometry $F$ of $\\mathbb{R}^{3}$ restricts to an isometry of surfaces $F|_{S}=\\eta:S\\to F(S)$ , but the converse need not be true, unless an additional hypothesis on the shape operators is satisfied. ", "page_idx": 3}, {"type": "text", "text": "Two surfaces $S$ and $\\overline{S}$ are congruent if there exists an isometry $F:\\mathbb{R}^{3}\\rightarrow\\mathbb{R}^{3}$ such that $F(S)={\\overline{{S}}}$ ; that is, congruent surfaces are surfaces which differ only in their location and orientation in space. It is clear that the shape operators $\\mathbf{S}$ and $\\overline{{\\mathbf{S}}}$ of two congruent surfaces are related by ", "page_idx": 3}, {"type": "equation", "text": "$$\nD F_{p}\\cdot{\\bf S}_{p}(v)=\\overline{{{\\bf S}}}_{F(p)}\\left(D F_{p}\\cdot v\\right),\\quad\\forall v\\in T_{p}S;\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "in particular, the matrices $\\left[\\mathbf{S}_{p}\\right]$ and $[\\overline{{\\mathbf{S}}}_{p}]$ are conjugate to one another via $[D F_{p}]$ . As per Theorem 9.2 of [28], if there exists an isometry $\\eta:S\\rightarrow{\\overline{{S}}}$ such that ", "page_idx": 3}, {"type": "equation", "text": "$$\nD\\eta_{p}\\cdot\\mathbf{S}_{p}(v)=\\overline{{\\mathbf{S}}}_{\\eta(p)}\\left(D\\eta_{p}\\cdot v\\right),\\quad\\forall v\\in T_{p}S,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "i.e. such that the matrices $\\left[\\mathbf{S}_{p}\\right]$ and $[\\overline{{\\mathbf{S}}}_{p}]$ are conjugate to one another via the matrix representation $[D\\eta_{p}]$ , then there exists an isometry $F:\\,\\mathbb{R}^{3}\\,\\rightarrow\\,\\mathbb{R}^{3}$ such that $F|_{S}(S)\\,=\\,\\eta(S)\\,=\\,\\overline{{S}}$ , and the two surfaces are congruent. The conclusion of this brief mathematical digression is that two congruent surfaces have the same intrinsic geometry and shape in space, and two surfaces with the same intrinsic geometry and shape in space are congruent. This is what is sought after when representing shapes with intrinsic quantities. ", "page_idx": 4}, {"type": "text", "text": "3.2 Discrete curvature ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "As well as being an important theoretical tool, curvature is a central notion in mesh processing. A large body of work has been dedicated to estimating its discrete counterpart. Among them, many methods propose to infer Gaussian curvature directly, such as in [24], or involve the use of geometric measure theory [13], as in [11, 17]. Interestingly, many efficient methods propose to first discretize the shape operator in order to compute the Gaussian curvature from it. This is done either directly on the mesh triangles, such as in [36], or by first locally ftiting a function to the surface, and then computing explicitly the shape operator. To get a better feel for why this is a natural construction of the shape operator, consider a surface $S$ , given a point $p\\in S$ . Then the surface around $p$ can be parameterized as $X(u,v)$ with $(u,v)\\in\\mathbb{R}^{2}$ . The inner product at $T_{p}S$ , also called the first fundamental form, is then given for any two tangent vectors $v,w$ by: ", "page_idx": 4}, {"type": "equation", "text": "$$\n(v,w)_{p}=v^{T}\\left({\\boldsymbol{E}}\\quad{\\boldsymbol{F}}\\right)w,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $E=\\langle\\partial_{u}X,\\partial_{u}X\\rangle$ , $F=\\langle\\partial_{u}X,\\partial_{v}X\\rangle$ and $G=\\langle\\partial_{v}X,\\partial_{v}X\\rangle$ . And the surface normal at $p$ can be defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\nn=\\frac{\\partial_{u}X(u,v)\\times\\partial_{v}X(u,v)}{|\\partial_{u}X(u,v)\\times\\partial_{v}X(u,v)|}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We can now define the second partial derivatives of $X$ in the normal direction $n$ , a quantity called the second fundamental form, noted $\\mathbb{I}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{I}={\\binom{L}{M}}\\subseteqq N{\\Bigr)}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $L=\\langle\\partial_{u u}X,n\\rangle$ , $M=\\langle\\partial_{u v}X,n\\rangle$ , and $N=\\langle\\partial_{v v}X,n\\rangle$ . The partial derivatives of the surface normal can then be expressed via the Weingarten equations, in terms of the components of the first and second fundamental form: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\partial_{u}n=\\frac{F M-G L}{E G-F^{2}}\\partial_{u}X+\\frac{F L-E M}{E G-F^{2}}\\partial_{v}X}\\\\ {\\partial_{v}n=\\frac{F N-G M}{E G-F^{2}}\\partial_{u}X+\\frac{F M-E N}{E G-F^{2}}\\partial_{v}X.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This enables us to write the matrix form of the shape operator at $p$ as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n[\\mathbf{S}_{p}]=(E G-F^{2})^{-1}\\left({\\overset{L G-M F}{M E}}\\quad M E-L F\\right)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "From these derivations, it becomes interesting to find good local parametrisation of surfaces, that is, a bi-variate scalar function $f$ such that: ", "page_idx": 4}, {"type": "equation", "text": "$$\nX(u,v)=(u,v,f(u,v))\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The shape operator can then be easily derived from the first and second derivatives of $f$ . An efficient way to find such functions is via osculating jets, proposed in [9]. For the following experiments, we use a multi-scale version of this, proposed in [31], in which the shape operator is computed by using neighbourhoods of varying size around a point, yielding a robust method for estimating curvature on a mesh. ", "page_idx": 4}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We test the representation of surfaces by the principal curvatures $\\kappa_{1},\\kappa_{2}$ and Gaussian curvature $\\kappa$ against the three most commonly used representations: the HKS [42], the SHOT descriptor [37], and the extrinsic coordinates. The HKS is a purely intrinsic representation derived from the Laplace operator, and constitutes the most widely used signature-based method to represent shapes. The SHOT representation is a descriptor mixing signature and histogram-based methods to describe shapes, and is therefore an extrinsic representation. As they belong to two different classes of surface representations we believe they are the most adequate for benchmarking our proposed curvature representation. ", "page_idx": 4}, {"type": "image", "img_path": "8koaqRdRYH/tmp/1d4ece302ab78315d94a1f4d6e10abc879bd006744f896274a29aa31e36e0c9f.jpg", "img_caption": ["Figure 2: Principal curvature visualisation of a Louis XIV statue. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "All representations are tested with three different architectures. We regard Diffusion Net [38] as the state of the art in NN architectures, as it shows the most promising results on general benchmark tasks. In addition, it shows very little difference in performance when changing the input from coordinates to HKS, making it the hardest test for our representation. Point Net $++\\left[35\\right]$ has been designed as a general method to process shapes arising in many situations, including controlled environments \u2013 as in our case \u2013 but also from segmented images encountered in the autonomous driving field [35]. As such Point $\\mathrm{\\bfNet++}$ uses the least geometric structure to describe a surface: all one needs is a point set. We believe that in this case, using better surface information for the input will greatly enhance the performance of the model. The authors of PointNet+ $^+$ have already touched on this subject, recommending a linear combination of HKS, WKS and Gaussian curvature, followed by a PCA projection, leading to a 64 dimensional feature per point. We aim to show that a 2d (or even 1d) input of curvature information is more relevant for a smaller computational cost. Delta Net [48] proposes an architecture intrinsic to surfaces by design, by combining four operators defined on the surface: Laplacian, divergence, curl and norm. Most papers that propose other surface descriptors rather than coordinates as input, do it solely to have an intrinsic representation of the surface. Curvature gives isometry invariance (section 3), and we further believe it is also more robust, numerically. Better performance from a curvature based representation in this architecture would support this belief. ", "page_idx": 5}, {"type": "text", "text": "Finally, we pick three tasks of varying complexity to measure the impact of each method: human segmentation [21], molecular segmentation [6], and shape classification [19]. Examples from each dataset are shown in figure 3. ", "page_idx": 5}, {"type": "text", "text": "4.1 Implementation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The performance of each representation is strongly dependent on the chosen implementation. We have tried to be as fair as possible by not developing our own implementations of existing work and instead using implementations which have already been tried, tested, and validated in the literature. For calculating the discrete principal curvatures via quadratic surface fitting, we have used igl\u2019s implementation with a fixed neighbourhood radius of 5; the Gaussian curvature $\\kappa$ is then computed directly as the product of $\\kappa_{1},\\kappa_{2}$ . HKS depends on the Laplacian, and we have used the method implemented in robust-laplacian based on [39] - this is also consistent with what is used in Diffusion Net. The eigendecomposition of the Laplacian is then performed with scipy. For the ", "page_idx": 5}, {"type": "image", "img_path": "8koaqRdRYH/tmp/aef777a226e2ab34845b086b4d01b0286d069010e54e0cc4663d0c99860c2add.jpg", "img_caption": ["Figure 3: Samples of the segmentation and classification datasets used for experiments. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "SHOT representation, we use the implementation in the pcl library, which computes 352 features per vertex, in this case we normalise all shapes and use a ball of radius .1; all other parameters are left untouched. ", "page_idx": 6}, {"type": "text", "text": "Regarding the neural networks, we use the implementations made publicly available by the authors, modifying only when needed to accommodate more than just coordinates as input. We also use the same parameters proposed in each paper when they are known, which we detail for every task below. We make all our code and experiments available at https://github.com/Inria-Asclepios/ shape-nets ", "page_idx": 6}, {"type": "text", "text": "4.2 Time Complexity ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "As a first experiment, we compute1 for each representation method, the computation time as a function of the number of points in a surface. The performances are reported in figure 4. HKS and curvature are both efficient for meshes with up to $100\\mathrm{k}$ points. However, curvature is consistently faster, even for larger meshes (up to $500\\mathrm{k}$ points), displaying the very small overhead incurred by the use of curvature in surface models. ", "page_idx": 6}, {"type": "image", "img_path": "8koaqRdRYH/tmp/0d585ee228326b12315534ca634f892a4d7d792c3256fb203c9069e5a63855d8.jpg", "img_caption": ["Figure 4: Time of computation for each representation with respect to the number of points in a mesh. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "4.3 Human Anatomy segmentation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We first segment the human parts from the composite dataset proposed in [21], containing samples from other human dataset, namely FAUST [7], SCAPE [3], Adobe [1], MIT [44], and SHREC07 [14]. As in the original paper, we use the SHREC07 dataset as test set. Similar to [47], we differ from [21] by evaluating on vertices rather than faces. For Point $\\mathrm{Net}++$ and Delta Net we resample each shape to 1024 points, and we leave the meshes untouched for Diffusion Net, as per the experiments conducted in each paper. We optimise the negative log-likelihood for 100 epochs, with the ADAM optimiser and a scheduler step every 20 epochs. We ran the experiment 5 times and have reported the mean test accuracy in table 1. ", "page_idx": 7}, {"type": "table", "img_path": "8koaqRdRYH/tmp/debc83e01cef1d2ef6b91f296474ca5ae13fe85c84add768098d500d0c8e8a63.jpg", "table_caption": [], "table_footnote": ["Table 1: Test accuracies $(\\%)$ on the Human part segmentation task. "], "page_idx": 7}, {"type": "text", "text": "The results highlight the assumption that better representations lead to better performance. PointNet++ showed the greatest improvement when moving away from coordinates: this is due to its architecture having the least amount of geometric information at baseline. The better results come from the principal curvatures, and show how expressive this representation is. The effects of the principal curvatures are even more pronounced in the Delta Net experiment, where $\\kappa_{1},\\kappa_{2}$ greatly outperform all other methods. It\u2019s interesting to note that in this experiment the coordinate representation performs better than the other more complex representations. Diffusion Net may show that it is more robust to the type of input, as long as it loosely describes the shape, however the improvement brought by the principal curvature is still significant. To further demonstrate the impact of a good representation, even in the case of Diffusion Net, we show in 5 the worst cases for xyz and $\\kappa_{1},\\kappa_{2}$ inputs. The clear improvement in this case may be even more important than general accuracy in some cases, e.g with human-in-the-loop type corrections. ", "page_idx": 7}, {"type": "image", "img_path": "8koaqRdRYH/tmp/c4fdbf2c98ae6165e8ed5165830e531cc92fe614c241c25801628b45178e1f33.jpg", "img_caption": ["Figure 5: Human part segmentation with Diffusion Net. Worst cases for different representations, blue shows the correct prediction, red the error. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.4 Molecular segmentation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The molecular dataset made available by [6] and first proposed in [33], can be considered a harder segmentation task then the Human part dataset: it proposes a wider range of shapes in the form of RNA molecules, and a 260-way part segmentation task. We resample all meshes to 2048 points, except in the case of Diffusion Net where we kept the original discretisation. We evaluate all our baselines on 5 random splits with a train-test ratio of 80-20. We run the models for 200 epochs, and report the mean test accuracies in table 2. ", "page_idx": 7}, {"type": "table", "img_path": "8koaqRdRYH/tmp/937d6997f59ac7acc059a335caada37784d64feadcd690ff8d784507fda6766f.jpg", "table_caption": [], "table_footnote": ["Table 2: Test accuracies $\\overline{{(\\%)}}$ on the Molecular segmentation task. "], "page_idx": 8}, {"type": "text", "text": "Again, we see a significant improvement when using a better representation of the surface in the case of PointNet++, going from failing in the case of coordinates to outperforming Delta Net \u2013 with principal curvatures giving the best performances. Diffusion Net shows a non-negligable jump in performance as well. Although the SHOT descriptor outperforms other representations in the case of Delta Net, the general performance of this architecture is underwhelming. We believe this is due to the accumulation of errors in the discretisation of surface operators used. Indeed, one layer computes a chain of 6 operators on the surface: since the RNA shapes are very irregular, the error for each operator could be significant. ", "page_idx": 8}, {"type": "text", "text": "4.5 Classification ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In addition to segmentation tasks, we propose to compare representations in the context of classification. This experiment should show whether or not geometrically informative inputs interact well with pooling-type operations. We choose the widely adopted baseline Shrec11, proposed in [19]. It is a 30-way classification dataset with 20 shapes per category. We choose the simplified mesh dataset and the harder version of training, using only 10 samples per class and evaluating on the test. We perform our experiments on 5 random splits. We train our baselines for 100 epochs with a scheduler step at epoch 50 and optimise the cross-entropy loss with a label smoothing factor of 0.2. Resulting mean test accuracies are shown in table 3. ", "page_idx": 8}, {"type": "table", "img_path": "8koaqRdRYH/tmp/271d6895380f088a6e9657b25cb19ca2e5fe23e9adf01790f9b4cf5adcb58008.jpg", "table_caption": [], "table_footnote": ["Table 3: Test accuracies $(\\%)$ on the Shrec11 classification task. "], "page_idx": 8}, {"type": "text", "text": "Yet again we observe a significant improvement when turning to better representations, even more so when using Gaussian curvature $\\kappa$ . Additionally, figure 6 shows that all geometric representations yield less variability across each folds. In addition, HKS, Gaussian curvature $\\kappa$ , and principal curvature $\\kappa_{1},\\kappa_{2}$ converge much faster than all others. ", "page_idx": 8}, {"type": "image", "img_path": "8koaqRdRYH/tmp/8831d934fb215ba494685b8ca0dc11e492fd0e7e9a28c0293509b6e560ec7e8f.jpg", "img_caption": ["Figure 6: Evolution of the test accuracy with $95\\%$ confidence interval by epochs per representations across folds, for the Shrec07 dataset using Diffusion Net. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "The fact that gaussian curvature, closely followed by HKS outperform principal curvature in this classification task seems to indicate that Gaussian curvature interacts better with pooling operations present in classification architectures. Interestingly, all three architectures tested here have different ways of performing the pooling operation. Although it is hard to give any analytical reasoning to this behavior, we believe it is simply the fact that gaussian curvature is already an aggregation of the principal curvatures, that it shows better performance in classification tasks. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "For each experiment, additional metrics can be found in Appendix A.1. ", "page_idx": 9}, {"type": "text", "text": "4.6 Noisy data ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We propose one final experiment to highlight the robustness of input features to noisy data. We focus on three representations: HKS, known to be robust to noise as it computes a representation at multiple scale; extrinsic coordinates that are directly impacted by the noise; and principal curvatures, known to be less robust to noise as a purely local descriptor. To compare these representations we pick the diffusion net trained on the human pose dataset, and we add noise to the dataset at inference time. Specifically, we add gaussian noise with a standard deviation of $1\\%$ $\\mathbf{\\chi}_{0}^{\\prime},3\\%,5\\%,$ $7\\%$ and $10\\%$ of the diagonal length of the bounding box of each shape. Examples of the noisy data can be seen in Appendix A.2. Results show that the accuracy for all three features worsen at the same rate, as shown in figure 7, showing that principal curvature can be a viable choice even in the presence of noisy data. ", "page_idx": 9}, {"type": "image", "img_path": "8koaqRdRYH/tmp/c31e608163ed9b0dfbfc593f8fcafa4fae620a736eec3d02d6c585bf2c8ebbdd.jpg", "img_caption": ["Figure 7: Evolution of the test accuracy on the human pose segmentation task for inputs $(k_{1},k_{2})$ , HKS and the extrinsic coordinates when noise is added to the shapes. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work we have shown that curvature should be the representation of choice when it comes to processing surfaces with neural networks. In almost all experiments the principal and Gaussian curvatures performed better than any other choice of input, both qualitatively and quantitatively. In particular, this representation can be obtained with minimal computational overhead. Its combination with PointNet++, the architecture that has the least prior information about the surface, showed that it can help the network better understand the surface structure. When combined with Delta Net, which contains only intrinsic operations, the improvement indicates that curvature gives more than just a rigid transformation invariance. Even in the case of Diffusion Net, where the diffusion operation seems to interact nicely with any representation, curvature as input showed significant amelioration. For these reasons, we believe curvature should become the standard practice when using models to learn on surfaces. Finally, although experiments have shown that gaussian curvature outperforms principal curvatures on classification tasks, we would like to further define those guidelines in future work, as well as compare representations in a wider range of tasks and architectures. ", "page_idx": 9}, {"type": "text", "text": "6 Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This Work has been funded by PARIS - ERACoSysMed grant number 15087, by G-Statistics - ERC grant number 786854 and has been supported by the French government through the National Research Agency (ANR) Investments in the 3IA C\u00f4te d\u2019Azur (ANR-19-P3IA-000). The authors are grateful to the OPAL infrastructure from Universit\u00e9 C\u00f4te d\u2019Azur for providing computing resources and support. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Adobe. Adobe mixamo 3d characters. Retrieved from Mixano.com, 2016.   \n[2] Boulbaba Ben Amor, Sylvain Arguill\u00e8re, and Ling Shao. Resnet-lddmm: advancing the lddmm framework using deep residual networks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(3):3707\u2013 3720, 2022.   \n[3] Dragomir Anguelov, Praveen Srinivasan, Daphne Koller, Sebastian Thrun, Jim Rodgers, and James Davis. Scape: shape completion and animation of people. In ACM SIGGRAPH 2005 Papers, pages 408\u2013416. 2005.   \n[4] Mathieu Aubry, Ulrich Schlickewei, and Daniel Cremers. The wave kernel signature: A quantum mechanical approach to shape analysis. In 2011 IEEE international conference on computer vision workshops (ICCV workshops), pages 1626\u20131633. IEEE, 2011.   \n[5] James Benn, Stephen Marsland, Robert I McLachlan, Klas Modin, and Olivier Verdier. Currents and finite elements as tools for shape space. Journal of Mathematical Imaging and Vision, 61:1197\u20131220, 2019.   \n[6] Helen M Berman, John Westbrook, Zukang Feng, Gary Gilliland, Talapady N Bhat, Helge Weissig, Ilya N Shindyalov, and Philip E Bourne. The protein data bank. Nucleic acids research, 28(1):235\u2013242, 2000.   \n[7] Federica Bogo, Javier Romero, Matthew Loper, and Michael J Black. Faust: Dataset and evaluation for 3d mesh registration. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3794\u20133801, 2014.   \n[8] Johann Brehmer, Pim De Haan, S\u00f6nke Behrends, and Taco S Cohen. Geometric algebra transformer. Advances in Neural Information Processing Systems, 36, 2024.   \n[9] Fr\u00e9d\u00e9ric Cazals and Marc Pouget. Estimating differential quantities using polynomial ftiting of osculating jets. Computer aided geometric design, 22(2):121\u2013146, 2005.   \n[10] Nicolas Charon and Alain Trouv\u00e9. The varifold representation of nonoriented shapes for diffeomorphic registration. SIAM journal on Imaging Sciences, 6(4):2547\u20132580, 2013.   \n[11] David Cohen-Steiner and Jean-Marie Morvan. Restricted delaunay triangulations and normal cycle. In Proceedings of the nineteenth annual symposium on Computational geometry, pages 312\u2013321, 2003.   \n[12] Pim De Haan, Maurice Weiler, Taco Cohen, and Max Welling. Gauge equivariant mesh cnns: Anisotropic convolutions on geometric graphs. arXiv preprint arXiv:2003.05425, 2020.   \n[13] Herbert Federer. Curvature measures. Transactions of the American Mathematical Society, 93(3):418\u2013491, 1959.   \n[14] Daniela Giorgi, Silvia Biasotti, and Laura Paraboschi. Shape retrieval contest 2007: Watertight models track. SHREC competition, 8(7):7, 2007.   \n[15] Heinrich W Guggenheimer. Differential geometry. Courier Corporation, 2012.   \n[16] Rana Hanocka, Amir Hertz, Noa Fish, Raja Giryes, Shachar Fleishman, and Daniel Cohen-Or. Meshcnn: a network with an edge. ACM Transactions on Graphics (ToG), 38(4):1\u201312, 2019.   \n[17] J-O Lachaud, Pascal Romon, Boris Thibert, and David Coeurjolly. Interpolated corrected curvature measures for polygonal surfaces. In Computer Graphics Forum, volume 39, pages 41\u201354. Wiley Online Library, 2020.   \n[18] Jean Lahoud, Jiale Cao, Fahad Shahbaz Khan, Hisham Cholakkal, Rao Muhammad Anwer, Salman Khan, and Ming-Hsuan Yang. 3d vision with transformers: A survey. arXiv preprint arXiv:2208.04309, 2022.   \n[19] Z Lian, A Godil, B Bustos, M Daoudi, J Hermans, S Kawamura, Y Kurita, G Lavoua, P Dp Suetens, et al. Shape retrieval on non-rigid 3d watertight meshes. In Eurographics workshop on 3d object retrieval (3DOR). Citeseer, 2011.   \n[20] Or Litany, Tal Remez, Emanuele Rodola, Alex Bronstein, and Michael Bronstein. Deep functional maps: Structured prediction for dense shape correspondence. In Proceedings of the IEEE international conference on computer vision, pages 5659\u20135667, 2017.   \n[21] Haggai Maron, Meirav Galun, Noam Aigerman, Miri Trope, Nadav Dym, Ersin Yumer, Vladimir G Kim, and Yaron Lipman. Convolutional neural networks on surfaces via seamless toric covers. ACM Trans. Graph., 36(4):71\u20131, 2017.   \n[22] Jonathan Masci, Davide Boscaini, Michael Bronstein, and Pierre Vandergheynst. Geodesic convolutional neural networks on riemannian manifolds. In Proceedings of the IEEE international conference on computer vision workshops, pages 37\u201345, 2015.   \n[23] Daniel Maturana and Sebastian Scherer. Voxnet: A 3d convolutional neural network for real-time object recognition. In 2015 IEEE/RSJ international conference on intelligent robots and systems (IROS), pages 922\u2013928. IEEE, 2015.   \n[24] Mark Meyer, Mathieu Desbrun, Peter Schr\u00f6der, and Alan H Barr. Discrete differential-geometry operators for triangulated 2-manifolds. In Visualization and mathematics III, pages 35\u201357. Springer, 2003.   \n[25] Thomas W Mitchel, Vladimir G Kim, and Michael Kazhdan. Field convolutions for surface cnns. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10001\u201310011, 2021.   \n[26] Thomas W Mitchel, Szymon Rusinkiewicz, Gregory S Chirikjian, and Michael Kazhdan. Echo: Extended convolution histogram of orientations for local surface description. In Computer Graphics Forum, volume 40, pages 180\u2013194. Wiley Online Library, 2021.   \n[27] Peter J Olver. Differential invariants of surfaces. Differential Geometry and Its Applications, 27(2):230\u2013239, 2009.   \n[28] Barrett O\u2019neill. Elementary differential geometry. Elsevier, 2006.   \n[29] Maks Ovsjanikov, Mirela Ben-Chen, Justin Solomon, Adrian Butscher, and Leonidas Guibas. Functional maps: a flexible representation of maps between shapes. ACM Transactions on Graphics (ToG), 31(4):1\u201311, 2012.   \n[30] David Palmer, Dmitriy Smirnov, Stephanie Wang, Albert Chern, and Justin Solomon. Deepcurrents: Learning implicit representations of shapes with boundaries. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18665\u201318675, 2022.   \n[31] Daniele Panozzo, Enrico Puppo, and Luigi Rocca. Efficient multi-scale curvature and crease estimation. Proceedings of Computer Graphics, Computer Vision and Mathematics (Brno, Czech Rapubic, 1(6), 2010.   \n[32] Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. Deepsdf: Learning continuous signed distance functions for shape representation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 165\u2013174, 2019.   \n[33] Adrien Poulenard, Marie-Julie Rakotosaona, Yann Ponty, and Maks Ovsjanikov. Effective rotation-invariant point cnn with spherical harmonics kernels. In 2019 International Conference on 3D Vision (3DV), pages 47\u201356. IEEE, 2019.   \n[34] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 652\u2013660, 2017.   \n[35] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. Advances in neural information processing systems, 30, 2017.   \n[36] Szymon Rusinkiewicz. Estimating curvatures and their derivatives on triangle meshes. In Proceedings. 2nd International Symposium on 3D Data Processing, Visualization and Transmission, 2004. 3DPVT 2004., pages 486\u2013493. IEEE, 2004.   \n[37] Samuele Salti, Federico Tombari, and Luigi Di Stefano. Shot: Unique signatures of histograms for surface and texture description. Computer Vision and Image Understanding, 125:251\u2013264, 2014.   \n[38] Nicholas Sharp, Souhaib Attaiki, Keenan Crane, and Maks Ovsjanikov. Diffusionnet: Discretization agnostic learning on surfaces. ACM Transactions on Graphics (TOG), 41(3):1\u201316, 2022.   \n[39] Nicholas Sharp and Keenan Crane. A Laplacian for Nonmanifold Triangle Meshes. Computer Graphics Forum (SGP), 39(5), 2020.   \n[40] Ayan Sinha, Jing Bai, and Karthik Ramani. Deep learning 3d shape surfaces using geometry images. In Computer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VI 14, pages 223\u2013240. Springer, 2016.   \n[41] Dmitriy Smirnov and Justin Solomon. Hodgenet: Learning spectral geometry on triangle meshes. ACM Transactions on Graphics (TOG), 40(4):1\u201311, 2021.   \n[42] Jian Sun, Maks Ovsjanikov, and Leonidas Guibas. A concise and provably informative multi-scale signature based on heat diffusion. In Computer graphics forum, volume 28, pages 1383\u20131392. Wiley Online Library, 2009.   \n[43] Marc Vaillant and Joan Glaunes. Surface matching via currents. In Biennial international conference on information processing in medical imaging, pages 381\u2013392. Springer, 2005.   \n[44] Daniel Vlasic, Ilya Baran, Wojciech Matusik, and Jovan Popovi\u00b4c. Articulated mesh animation from multi-view silhouettes. In Acm Siggraph 2008 papers, pages 1\u20139. 2008.   \n[45] Yu Wang and Justin Solomon. Intrinsic and extrinsic operators for shape analysis. In Handbook of numerical analysis, volume 20, pages 41\u2013115. Elsevier, 2019.   \n[46] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma, Michael M Bronstein, and Justin M Solomon. Dynamic graph cnn for learning on point clouds. ACM Transactions on Graphics (tog), 38(5):1\u201312, 2019.   \n[47] Ruben Wiersma, Elmar Eisemann, and Klaus Hildebrandt. Cnns on surfaces using rotation-equivariant features. ACM Transactions on Graphics (ToG), 39(4):92\u20131, 2020.   \n[48] Ruben Wiersma, Ahmad Nasikun, Elmar Eisemann, and Klaus Hildebrandt. Deltaconv: anisotropic operators for geometric deep learning on point clouds. ACM Transactions on Graphics (TOG), 41(4):1\u201310, 2022.   \n[49] Laurent Younes. Shapes and diffeomorphisms, volume 171. Springer, 2010. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Segmentation and classification detailed results ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We present below the complete results for each experiment. For each dataset, and each neural network architecture, we show the Accuracy, Balanced accuracy, F1 score, and Specificity. They were all measured using 5-fold cross validation, and we give the results on the test sets in the form of mean $\\pm$ standard deviation. ", "page_idx": 13}, {"type": "table", "img_path": "8koaqRdRYH/tmp/4c03f1b37635b26a741f34785f1476c1cf619cd2e3a996f139a43501163b4c10.jpg", "table_caption": [], "table_footnote": ["Table 4: Human pose segmentation - Point Net ++ results. "], "page_idx": 13}, {"type": "table", "img_path": "8koaqRdRYH/tmp/2661fc8467d44c223c9abec125af6956ab2ae8302f3601fec43e60d9258a01fa.jpg", "table_caption": [], "table_footnote": ["Table 5: Human pose segmentation - Delta Net results. "], "page_idx": 13}, {"type": "table", "img_path": "8koaqRdRYH/tmp/bc129054727bd6176fcb14ae95f8865f18327d35b7daeac2e53a4fd1f9b9129a.jpg", "table_caption": [], "table_footnote": ["Table 6: Human pose segmentation - Diffusion Net results. "], "page_idx": 13}, {"type": "table", "img_path": "8koaqRdRYH/tmp/1b9bb7f7a2087d4797f54703e79b7e64d0810c11f55ed77116c8793885fe8111.jpg", "table_caption": [], "table_footnote": ["Table 7: RNA molecules segmentation - PointNet++ results. "], "page_idx": 13}, {"type": "table", "img_path": "8koaqRdRYH/tmp/07aec134cd0ab9f12926aa88ed6d2a4732b1818d161cb827727117e776d937da.jpg", "table_caption": [], "table_footnote": ["Table 8: RNA molecules segmentation - Delta Net results. "], "page_idx": 13}, {"type": "table", "img_path": "8koaqRdRYH/tmp/322168c72e3ef28678da0305f608a4cf36e2b76fa4ff2b883abb2dba6d8d89cc.jpg", "table_caption": [], "table_footnote": ["Table 9: RNA molecules segmentation - Diffusion Net results. "], "page_idx": 14}, {"type": "table", "img_path": "8koaqRdRYH/tmp/7013465f1ff9284e5945c5b77a213a32b169343a1ace69393f6a92261d9fc96c.jpg", "table_caption": [], "table_footnote": ["Table 10: Shrec classification - Point Net ++ results. "], "page_idx": 14}, {"type": "table", "img_path": "8koaqRdRYH/tmp/a80d06ed3bd15180d6b73f538dc1e7ad8c4ffd26c4765829bc223dd9681750a0.jpg", "table_caption": [], "table_footnote": ["Table 11: Shrec classification - Delta Net results. "], "page_idx": 14}, {"type": "table", "img_path": "8koaqRdRYH/tmp/0556e30c10dc9def273e2a022b64f74afa32a5ef67b61fa188245a75dd9092b5.jpg", "table_caption": [], "table_footnote": ["Table 12: Shrec classification - Diffusion Net results. "], "page_idx": 14}, {"type": "image", "img_path": "8koaqRdRYH/tmp/270e0f7ccbc39edf1f45a2f53623d7a777585223410f7fb85a2fd49ae5172867.jpg", "img_caption": ["Figure 8: Different quantity of noise added to a shape from the human pose dataset, from $1\\%$ to $10\\%$ of the diagonal of the bounding box of the shape. "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "8koaqRdRYH/tmp/c35efd8f8ae74eb86dce7f588c0d7a10ef38856e64fe4b77db4e36609649962c.jpg", "img_caption": ["Figure 9: Different quantity of noise added to a shape, from $1\\%$ to $10\\%$ of the diagonal of the bounding box of the shape. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Claims ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: Throughout the paper. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 15}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: We perform a comparative study and experimentally highlight what we believe to be the best methodology. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 16}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 16}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 16}, {"type": "text", "text": "Justification: There are no theoretical results. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 16}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: See section 4. Moreover, a repository with all the code and experiments will be made available after the review process. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 17}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Justification: Link will be available after the review process. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: See section 4 ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 18}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: See Appendix A.1. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 18}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 18}, {"type": "text", "text": "Answer: [No] ", "page_idx": 18}, {"type": "text", "text": "Justification: Not completely. Section 4.2 shows the time complexity of each representation. We do not provide the computational cost of other experiments. However, we point out that they take very similar time to the one indicated in papers describing each architecture. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 19}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We have reviewed the NeurIPS Code of Ethics and made sure our paper complies to it. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 19}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: There is no societal impact to discuss. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 19}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: This work poses no such risk. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 20}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: See section 4. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 20}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: No new assets are released. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used. ", "page_idx": 20}, {"type": "text", "text": "\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 21}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: We do not conduct experiments involving crowdsourcing or human subjects. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 21}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: We do not involve research with human subjects or crowdfunding. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 21}]