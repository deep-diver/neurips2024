[{"type": "text", "text": "Learning Variational Temporal Abstraction Embeddings in Option-Induced MDPs ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 The option framework in hierarchical reinforcement learning has notably advanced   \n2 the automatic discovery of temporally-extended actions from long-horizon tasks.   \n3 However, existing methods often struggle with ineffective exploration and unstable   \n4 updates when learning action and option policies simultaneously. Addressing these   \n5 challenges, we introduce the Variational Markovian Option Critic (VMOC), an   \n6 off-policy algorithm with provable convergence that employs variational inference   \n7 to stabilize updates. VMOC naturally integrates maximum entropy as intrinsic re  \n8 wards to promote the exploration of diverse and effective options. Furthermore, we   \n9 adopt low-cost option embeddings instead of traditional, computationally expensive   \n10 option triples, enhancing scalability and expressiveness. Extensive experiments in   \n11 challenging Mujoco environments validate VMOC\u2019s superior performance over ex  \n12 isting on-policy and off-policy methods, demonstrating its effectiveness in learning   \n13 coherent and diverse option sets suitable for complex tasks. ", "page_idx": 0}, {"type": "text", "text": "14 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "15 Recent advancements in deep reinforcement learning (DRL) have demonstrated significant successes   \n16 across a variety of complex domains, such as mastering the human level of atari [36] and Go [44]   \n17 games. These achievements underscore the potential of combining reinforcement learning (RL)   \n18 with powerful function approximators like neural networks [5] to tackle intricate tasks that require   \n19 nuanced control over extended periods. Despite these breakthroughs, Deep RL still faces substantial   \n20 challenges, such as insufficient exploration in dynamic environments [18, 13, 42], inefficient learning   \n21 associated with temporally extended actions [6, 9] and long horizon tasks [30, 4], and vast amounts   \n22 of samples required for training proficient behaviors [16, 40, 15].   \n23 One promising area for addressing these challenges is the utilization of hierarchical reinforcement   \n24 learning (HRL) [11, 2, 12], a diverse set of strategies that decompose complex tasks into simpler, hier  \n25 archical structures for more manageable learning. Among these strategies, the option framework [47],   \n26 developed on the Semi-Markov Decision Process (SMDP), is particularly effective at segmenting   \n27 non-stationary task stages into temporally-extended actions known as options. Options are typically   \n28 learned through a maximum likelihood approach that aims to maximize the expected rewards across   \n29 trajectories. In this framework, options act as temporally abstracted actions executed over variable   \n30 time steps, controlled by a master policy that decides when each option should execute and terminate.   \n31 This structuring not only simplifies the management of complex environments but also enables the   \n32 systematic discovery and execution of temporal abstractions over long-horizon tasks [24, 23].   \n33 However, the underlying SMDP framework is frequently undermined by three key challenges:   \n34 1) Insufficient exploration and degradation [20, 37, 23]. As options are unevenly updated using   \n35 conventional maximum likelihood methods [4, 10, 45, 25, 26], the policy is quickly saturated with   \n36 early rewarding observations. This typically results in focusing on only low-entropy options that lead   \n37 to local optima rewards, causing a single option to either dominate the entire policy or switch every   \n38 timestep. Such premature convergence limits option diversity significantly. 2) Sample Inefficiency.   \n39 The semi-Markovian nature inherently leads to sample inefficiency [47, 29]: each policy update   \n40 at the master level extends over multiple time steps, thus consuming a considerable volume of   \n41 experience samples with relatively low informational gain. This inefficiency is further exacerbated   \n42 by the prevalence of on-policy option learning algorithms [4, 52], which require new samples to be   \n43 collected simultaneously from both high-level master policies and low-level action policies at each   \n44 gradient step, and thus sample expensive. 3) Computationally expensive. Options are conventionally   \n45 defined as triples [4] with intra-option policies and termination functions, often modeled using neural   \n46 networks which are expensive to optimize. These challenges collectively limit the broader adoption   \n47 and effectiveness of the option framework in real-world scenarios, particularly in complex continuous   \n48 environments where scalability and stability are critical [14, 34, 26].   \n49 To address these challenges, we introduce the Variational Markovian Option Critic (VMOC), a   \n50 novel off-policy algorithm that integrates the variational inference framework on option-induced   \n51 MDPs [35]. We first formulate the optimal option-induced SMDP trajectory as a probabilistic   \n52 inference problem, presenting a theoretical convergence proof of the variational distribution under   \n53 the soft policy iteration framework [19]. Similar to prior variational methods [31], policy entropy   \n54 terms naturally arise as intrinsic rewards during the inference procedure. As a result, VMOC not   \n55 only seeks high-reward options but also maximizes entropy across the space, promoting extensive   \n56 exploration and maintaining high diversity. We implements this inference procedure as an off-policy   \n57 soft actor critic [19] algorithm, which allows reusing samples from replay buffer and enhances sample   \n58 efficiency. Furthermore, to address the computational inefficiencies associated with conventional   \n59 option triples, we follow [35] and employ low-cost option embeddings rather than complex neural   \n60 network models. This not only simplifies the training process but also enhances the expressiveness of   \n61 the model by allowing the agent to capture a more diverse set of environmental dynamics. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "62 Our contributions can be summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "63 \u2022 We propose a variational inference approach within the maximum entropy framework to   \n64 enhance diverse and robust exploration of options.   \n65 \u2022 We implement an off-policy algorithm that improves sample efficiency.   \n66 \u2022 We introduce option embeddings into latent variable policies and enhance expressiveness   \n67 and computational cost-effectiveness of option representations.   \n68 \u2022 We conduct extensive experiments in OpenAI Gym Mujoco [49] environments, demonstrat  \n69 ing that VMOC significantly outperforms other option-based variants in terms of exploration   \n70 capabilities, sample efficiency, and computational efficiency. ", "page_idx": 1}, {"type": "text", "text": "71 2 Preliminary ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "72 2.1 Control as Structured Variational Inference ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "73 Conventionally, the control as inference framework [19, 31, 19, 53] is derived using the maximum   \n74 entropy objective. In this section, we present an alternative derivation from the perspective of   \n75 structured variational inference. We demonstrate that this approach provides a more concise and   \n76 intuitive pathway to the same theoretical results, where the maximum entropy principle naturally   \n77 emerges through the direct application of variational inference techniques.   \n78 Traditional control methods focus on directly maximizing rewards, often resulting in suboptimal trade  \n79 offs between exploration and exploitation. By reinterpreting the control problem as a probabilistic   \n80 inference problem, the control as inference framework incorporates both the reward structure and   \n81 environmental uncertainty into decision-making, providing a more robust and flexible approach   \n82 to policy optimization. In this framework, optimality is represented by a binary random variable   \n83 $\\mathcal{E}\\,\\doteq\\,\\{0,1\\}^{1}$ . The probability of optimality given a state-action pair $(\\mathbf{s},\\mathbf{a})$ is denoted as $P(\\mathcal{E}\\,=$   \n84 $1\\mid|\\mathbf{\\Deltas,\\dot{a}}\\rangle\\,=\\,\\exp(r(\\mathbf{s,a}))$ , which is an exponential function of the conventional reward function   \n85 $r(\\mathbf{s},\\mathbf{a})$ that measures the desirability of an action in a specific state. Focusing on $\\mathcal{E}=1$ captures the   \n86 occurrence of optimal events. For simplicity, we will use $\\mathcal{E}$ instead of $\\mathcal E=1$ in the following text   \n87 to avoid cluttered notations. The joint distribution over trajectories $\\boldsymbol{\\tau}=(\\mathbf{s}_{1},\\mathbf{a}_{1},\\dots,\\mathbf{s}_{T},\\mathbf{a}_{T})$ given   \n88 optimality is expressed as: ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\nP(\\tau|\\mathcal{E}_{1:T})\\propto P(\\tau,\\mathcal{E}_{1:T})=P(\\mathbf{s}_{1})\\prod_{t=1}^{T-1}P(\\mathbf{s}_{t+1}|\\mathbf{s}_{t},\\mathbf{a}_{t})P(\\mathcal{E}_{t}|\\mathbf{s}_{t},\\mathbf{a}_{t})\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "89 where $P(\\mathbf{s}_{1})$ is the initial state distribution, $P(\\mathbf{s}_{t+1}|\\mathbf{s}_{t},\\mathbf{a}_{t})$ is the dynamics model. As explained   \n90 in [19, 31], direct optimization of $P(\\tau\\mid\\mathcal{E}_{1:T})$ can result in an optimistic policy that assumes a degree   \n91 of control over the dynamics. One way to correct this risk-seeking behavior [31] is through structured   \n92 variational inference. In our case, the goal is to approximate the optimal trajectory $P(\\tau)$ with the   \n93 variational distribution: ", "page_idx": 2}, {"type": "equation", "text": "$$\nq(\\tau)=P(\\mathbf{s}_{1})\\prod_{t=1}^{T-1}P(\\mathbf{s}_{t+1}\\mid\\mathbf{s}_{t},\\mathbf{a}_{t})q(\\mathbf{a}_{t}\\mid\\mathbf{s}_{t})\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "94 where the initial distribution $P(\\mathbf{s}_{1})$ and transition distribution $P(\\mathbf{s}_{t+1}\\mid\\mathbf{s}_{t},\\mathbf{a}_{t})$ is set to be the true   \n95 environment dynamics from $P(\\tau)$ . The only variational term is the variational policy $q(\\mathbf{a}_{t}\\mid\\mathbf{s}_{t})$ ,   \n96 which is used to approximate the optimal policy $P(\\mathbf{a}_{t}\\mid\\mathbf{s}_{t},\\mathcal{E}_{1:T})$ . Under this setting, the environment   \n97 dynamics will be canceled out from the optimization objective between $P(\\tau\\mid\\mathcal{E})$ and $q(\\tau)$ , thus   \n98 explicitly disallowing the agent to influence its dynamics and correcting the risk-seeking behavior.   \n99 With the variational distribution at hand, the conventional maximum entropy framework can be   \n100 recovered through a direct application of standard structural variational inference [28]: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\log P(\\mathcal{E}_{1:T})=\\mathcal{L}(q(\\tau),P(\\tau,\\mathcal{E}_{1:T}))+D_{\\mathrm{KL}}(q(\\tau)\\parallel P(\\tau|\\mathcal{E}_{1:T}))}\\\\ &{\\qquad\\qquad\\qquad=\\underbrace{\\mathbb{E}_{\\tau\\sim q(\\tau)}[\\displaystyle\\sum_{t}r(\\mathbf{s}_{t},\\mathbf{a}_{t})+\\mathcal{H}(q(\\cdot|\\mathbf{s}_{t}))]}_{t}+D_{\\mathrm{KL}}(q(\\mathbf{a}_{t}|\\mathbf{s}_{t})\\parallel P(\\mathbf{a}_{t}|\\mathbf{s}_{t},\\mathcal{E}_{1:T}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "101 where $\\begin{array}{r}{\\mathcal{L}(q,P)=\\mathbb{E}_{q}[\\log\\frac{P}{q}]}\\end{array}$ is the Evidence Lower Bound (ELBO) [28]. The maximum entropy   \n102 objective arises naturally as the environment dynamics in $P(\\tau,\\mathcal{E})$ and $q(\\tau)$ cancel out. Under this   \n103 formulation, the soft policy iteration theorem [19] has an elegant Expectation-Maximization (EM)   \n104 algorithm [28] interpretation: the $\\boldsymbol{\\mathrm E}$ -step corresponds to the policy evaluation of the maximum   \n105 entropy objective $\\bar{\\mathcal{L}}(\\bar{q}^{[k]},P)$ ; while the M-step corresponds to the policy improvement of the $D_{\\mathrm{KL}}$   \n106 term $q^{[k+1]}=\\arg\\operatorname*{max}_{q}D_{\\mathrm{KL}}(q^{[k]}(\\tau)\\parallel P(\\tau\\mid\\mathcal{E}))$ . Thus, soft policy iteration is an exact inference if   \n107 both EM steps can be performed exactly.   \n108 Theorem 1 (Convergence Theorem for Soft Policy Iteration). Let $\\tau$ be the latent variable and $\\mathcal{E}$   \n109 be the observed variable. Define the variational distribution $q(\\tau)$ and the log-likelihood $\\log{P(\\mathcal{E})}$ .   \n110 Let $M:q^{[k]}\\,\\rightarrow\\,q^{[k+1]}$ represent the mapping defined by the EM steps inference update, so that   \n111 $q^{[k+1]}=\\operatorname{\\dot{M}}(q^{[k]})$ . The likelihood function increases at each iteration of the variational inference   \n112 algorithm until convergence conditions are satisfied. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "114 2.2 The Option Framework ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "115 In conventional SMDP-based Option Framework [47], an option is a triple $(\\mathbb{I}_{o},\\pi_{o},\\beta_{o})\\in\\mathcal{O}$ , where $\\scriptscriptstyle\\mathcal{O}$   \n116 denotes the option set; $o\\in\\mathbb{O}=\\{1,2,\\dots,K\\}$ is a positive integer index which denotes the $o$ -th triple   \n117 where $K$ is the number of options; $\\mathbb{I}_{o}$ is an initiation set indicating where the option can be initiated;   \n118 $\\pi_{o}=P_{o}(\\mathbf{a}|\\mathbf{s}):\\mathbb{A}\\times\\mathbb{S}\\rightarrow[0,1]$ is the action policy of the oth option; $\\beta_{o}=P_{o}(\\mathbf{\\bar{b}}=1|\\mathbf{s}):\\mathbb{S}\\rightarrow[0,1]$   \n119 where ${\\bf b}\\in\\{0,1\\}$ is a termination function. For clarity, we use $P_{o}(\\mathbf{b}=1|\\mathbf{s})$ instead of $\\beta_{o}$ which is   \n120 widely used in previous option literatures (e.g., Sutton et al. [47], Bacon et al. [4]). A master policy   \n121 $\\pi(\\mathbf{o}|\\mathbf{\\bar{s}})\\,=\\,P(\\mathbf{o}|\\mathbf{\\bar{s}})$ where $\\mathbf o\\in\\mathbb{O}$ is used to sample which option will be executed. Therefore, the   \n122 dynamics (stochastic process) of the option framework is written as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P(\\tau)=P(\\mathbf{s}_{0},\\mathbf{o}_{0})\\displaystyle\\prod_{t=1}^{\\infty}P(\\mathbf{s}_{t}|\\mathbf{s}_{t-1},\\mathbf{a}_{t-1})P_{o_{t}}(\\mathbf{a}_{t}|\\mathbf{s}_{t})}\\\\ &{\\qquad\\qquad\\qquad\\qquad[P_{o_{t-1}}(\\mathbf{b}_{t}=0|\\mathbf{s}_{t})\\mathbf{1}_{\\mathbf{o}_{t}=o_{t-1}}+P_{o_{t-1}}(\\mathbf{b}_{t}=1|\\mathbf{s}_{t})P(\\mathbf{o}_{t}|\\mathbf{s}_{t})],}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "123 where ${\\boldsymbol\\tau}\\;=\\;\\{{\\bf s}_{0},{\\bf o}_{0},{\\bf a}_{0},{\\bf s}_{1},{\\bf o}_{1},{\\bf a}_{1},.\\;.\\;.\\}$ denotes the trajectory of the option framework. 1 is an   \n124 indicator function and is only true when $\\mathbf{o}_{t}\\,=\\,o_{t-1}$ (notice that $O_{t-1}$ is the realization at $\\mathbf o_{t-1}$ ).   \n125 Therefore, under this formulation the option framework is defined as a Semi-Markov process since   \n126 the dependency on an activated option $o$ can cross a variable amount of time [47]. Due to the nature   \n127 of SMDP assumption, conventional option framework is unstable and computationally expensive to   \n128 optimize. Li et al. [34, 35] proposed the Hidden Temporal Markovian Decision Process (HiT-MDP): ", "page_idx": 3}, {"type": "equation", "text": "$$\nP(\\tau)=P(\\mathbf{s}_{0},\\mathbf{o}_{0})\\prod_{t=1}^{\\infty}P(\\mathbf{s}_{t}|\\mathbf{s}_{t-1},\\mathbf{a}_{t-1})P(\\mathbf{a}_{t}|\\mathbf{s}_{t},\\mathbf{o}_{t})P(\\mathbf{o}_{t}|\\mathbf{s}_{t},\\mathbf{o}_{t-1})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "129 and theoretically proved that the option-induced HiT-MDP is homomorphically equivalent to the   \n130 conventional SMDP-based option framework. Following RL conventions, we use $\\stackrel{\\cdot}{\\pi^{A}}=P(\\mathbf{a}_{t}|\\mathbf{s}_{t},\\mathbf{o}_{t})$   \n131 to denote the action policy and $\\pi^{O}=P(\\mathbf{o}_{t}|\\mathbf{s}_{t},\\mathbf{o}_{t-1})$ to denote the option policy respectively. In   \n132 HiT-MDPs, options can be viewed as latent variables with a temporal structure $P(\\mathbf{o}_{t}|\\mathbf{s}_{t},\\mathbf{o}_{t-1})$ ,   \n133 enabling options to be represented as dense latent embeddings rather than traditional option triples.   \n134 They demonstrated that learning options as embeddings on HiT-MDPs offers significant advantages   \n135 in performance, scalability, and stability by reducing variance. However, their work only derived an   \n136 on-policy policy gradient algorithm for learning options on HiT-MDPs. In this work, we extend their   \n137 approach to an off-policy algorithm under the variational inference framework, enhancing exploration   \n138 and sample efficiency. ", "page_idx": 3}, {"type": "text", "text": "139 3 Methodology ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "140 In this section, we introduce the Variational Markovian Option Critic (VMOC) algorithm by extending   \n141 the variational policy iteration (Theorem 1) to the option framework. In Section 3.1, we reformulate   \n142 the optimal option trajectory and the variational distribution as probabilistic graphical models (PGMs),   \n143 propose the corresponding variational objective, and present a provable exact inference procedure for   \n144 these objectives in tabular settings. Section 3.2 extends this result by introducing VMOC, a practical   \n145 off-policy option learning algorithm that uses neural networks as function approximators and proves   \n146 the convergence of VMOC under approximate inference settings. Our approach differs from previous   \n147 works [19, 33, 34] by leveraging structured variational inference directly, providing a more concise   \n148 pathway to both theoretical results and practical algorithms. ", "page_idx": 3}, {"type": "text", "text": "149 3.1 PGM Formulations of The Option Framework ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "150 Formulating complex problems as probabilistic graphical models (PGMs) offers a consistent and   \n151 flexible framework for deriving principled objectives, analyzing convergence, and devising practical   \n152 algorithms. In this section, we first formulate the optimal trajectory of the conventional SMDP-based   \n153 option framework (Eq. 1) as a PGM. We then use the HiT-MDPs as the variational distribution to   \n154 approximate this optimal trajectory. With these PGMs, we can straightforwardly derive the variational   \n155 objective, where maximum entropy terms arise naturally. This approach allows us to develop a stable   \nalgorithm for learning diversified options and preventing degeneracy. Specifically, we follow [31, 28]   \n156   \n157 by introducing the concept of \"Optimality\" [48] into the conventional SMDP-based option framework   \n158 (Equation equation 1). This allows us to define the probability of an option trajectory being optimal ", "page_idx": 3}, {"type": "image", "img_path": "eM3Wzs6Unt/tmp/7f91113bf207b719a1c2c530f586a4a17fadd7ca6879ac5055d7b2e9f0d9101f.jpg", "img_caption": ["Figure 1: PGMs of the option framework. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "159 as a probabilistic graphical model (PGM), as illustrated in Figure 1 (a): ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{^{>}(\\tau,\\xi_{1:T}^{A},\\xi_{1:T}^{O})=P(\\mathbf{s}_{0},\\mathbf{o}_{0})\\prod_{t=1}^{T}P(\\mathbf{s}_{t+1}|\\mathbf{s}_{t},\\mathbf{a}_{t})P(\\xi_{t}^{A}=1|\\mathbf{s}_{t},\\mathbf{a}_{t})P(\\xi_{t}^{O}=1|\\mathbf{s}_{t},\\mathbf{a}_{t},\\mathbf{o}_{t},\\mathbf{o}_{t-1})P(\\mathbf{o}_{t})P(\\xi_{t}^{O}=\\xi_{1:T}^{O},\\mathbf{o}_{t})}}\\\\ &{}&{\\propto P(\\mathbf{s}_{0})\\prod_{t=1}^{T}P(\\mathbf{s}_{t+1}|\\mathbf{s}_{t},\\mathbf{a}_{t})\\prod_{t=1}^{T}P(\\xi_{t}^{A}=1|\\mathbf{s}_{t},\\mathbf{a}_{t})P(\\xi_{t}^{O}=1|\\mathbf{s}_{t},\\mathbf{a}_{t},\\mathbf{o}_{t},\\mathbf{o}_{t-1}),\\quad\\mathrm{~(3)~}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "160 where ${\\mathcal{E}}\\in\\{0,1\\}$ are observable binary \u201coptimal random variables\u201d [31], $\\tau=\\{\\mathbf{s}_{0},\\mathbf{o}_{0},\\mathbf{a}_{0},\\mathbf{s}_{1}\\ldots\\cdot\\}$   \n161 denotes the trajectory of the option framework. The agent is optimal at time step $t$ when $P(\\mathcal{E}_{t}^{A}=$   \n162 ${1}|{\\mathbf{s}}_{t},{\\mathbf{a}}_{t})$ and $\\bar{P}(\\mathcal{E}_{t}^{O}=1|{\\bf s}_{t},{\\bf a}_{t},{\\bf o}_{t},{\\bf o}_{t-1})$ . We will use $\\mathcal{E}$ instead of $\\mathcal{E}=1$ in the following text to   \n163 avoid cluttered notations. To simplify the derivation, priors $P(\\mathbf{o})$ and $P(\\mathbf{a})$ can be assumed to be   \n164 uniform distributions without loss of generality [31]. Note that Eq. 3 shares the same environment   \n165 dynamics with Eq. 1 and Eq. 2. With the optimal random variables ${\\mathcal{E}}^{O}$ and $\\mathcal{E}^{A}$ , the likelihood of a   \n166 state-action $\\{\\mathbf{s}_{t},\\mathbf{a}_{t}\\}$ pair that is optimal is defined as: ", "page_idx": 4}, {"type": "equation", "text": "$$\nP(\\mathcal{E}_{t}^{A}|\\mathbf{s}_{t},\\mathbf{a}_{t})=\\exp(r(\\mathbf{s}_{t},\\mathbf{a}_{t})),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "167 as this specific design facilitates recovering the value function at the latter structural variational infer  \n168 ence stage. Based on the same motivation, the likelihood of an option-state-action $\\{\\mathbf{o}_{t},\\mathbf{s}_{t},\\mathbf{a}_{t},\\mathbf{o}_{t-1}\\}$   \n169 pair that is optimal is defined as, ", "page_idx": 4}, {"type": "equation", "text": "$$\nP(\\mathcal{E}_{t}^{O}|\\mathbf{s}_{t},\\mathbf{a}_{t},\\mathbf{o}_{t},\\mathbf{o}_{t-1})=\\exp(f(\\mathbf{o}_{t},\\mathbf{s}_{t},\\mathbf{a}_{t},\\mathbf{o}_{t-1})),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "170 where $f(\\cdot)$ is an arbitrary non-positive function which measures the preferable of selecting an option   \n171 given state-action pair $[\\mathbf{s}_{t},\\mathbf{a}_{t}]$ and the previous executed option $\\mathbf o_{t-1}$ . In this work, we choose $f$ to   \n172 be the mutual-information $f\\dot{=}\\,I[\\mathbf{o}_{t}|\\mathbf{s}_{t},\\mathbf{a}_{t},\\mathbf{o}_{t-1}]$ as a fact that when the uniform prior assumption of   \n173 $P(\\mathbf{o})$ is relaxed the optimization introduces a mutual-information as a regularizer [35].   \n174 As explained in Section 2.1, direct optimization of Eq. 3 results in optimistic policies that assumes a   \n175 degree of control over the dynamics. We correct this risk-seeking behavior [31] through approximating   \n176 the optimal trajectory $P(\\tau)$ with the variational distribution: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\nq(\\tau)=P(\\mathbf{s}_{0},\\mathbf{o}_{0})\\prod_{t=1}^{T-1}P(\\mathbf{s}_{t+1}|\\mathbf{s}_{t},\\mathbf{a}_{t})q(\\mathbf{a}_{t}|\\mathbf{s}_{t},\\mathbf{o}_{t})q(\\mathbf{o}_{t}|\\mathbf{s}_{t},\\mathbf{o}_{t-1})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "177 where the initial distribution $P(\\mathbf{s}_{0},\\mathbf{o}_{0})$ and transition distribution $P(\\mathbf{s}_{t+1}\\mid\\mathbf{s}_{t},\\mathbf{a}_{t})$ is set to be the true   \n178 environment dynamics from $P(\\tau)$ . The variational distribution turns out to be the HiT-MDP, where   \n179 the action policy $q\\big(\\mathbf{a}_{t_{\\partial}}\\big|\\ \\mathbf{s}_{t}\\big)$ and the option policy $q\\big(\\mathbf{o}_{t}\\big|\\mathbf{s}_{t},\\mathbf{o}_{t-1}\\big)$ are used to approximate the optimal   \n180 policy $P(\\mathbf{a}_{t}|\\mathbf{s}_{t},\\mathbf{o}_{t},\\mathcal{E}_{1:T}^{A})$ and $P(\\mathbf{o}_{t}|\\bar{\\mathbf{s}_{t}},\\mathbf{o}_{t-1},\\mathcal{E}_{1:T}^{O})$ . The Evidence Lower Bound (ELBO) [28] of the   \n181 log-likelihood optimal trajectory (Eq. 3) can be derived as (see Appendix A.3): ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}(q(\\tau),P(\\tau,\\mathcal{E}_{1:T}^{A},\\mathcal{E}_{1:T}^{O}))=\\mathbb{E}_{q(\\tau)}[\\log P(\\tau,\\mathcal{E}_{1:T}^{A},\\mathcal{E}_{1:T}^{O})-\\log q(\\tau)]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\mathbb{E}_{q(\\tau)}[r(\\mathbf{s}_{t},\\mathbf{a}_{t})+f(\\cdot)-\\log q(\\mathbf{a}_{t}|\\mathbf{s}_{t},\\mathbf{o}_{t})-\\log q(\\mathbf{o}_{t}|\\mathbf{s}_{t},\\mathbf{o}_{t-1})]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\mathbb{E}_{q(\\tau)}\\left[r(\\mathbf{s}_{t},\\mathbf{a}_{t})+f(\\cdot)+\\mathcal{H}[\\pi^{A}]+\\mathcal{H}[\\pi^{O}]\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "182 where line 2 is substituting Eq. 3 and Eq. 6 into the ELBO. As a result, the maximum entropy   \n183 objective naturally arises in Eq. 7. Optimizing the ELBO not only seeks high-reward options but also   \n184 maximizes entropy across the space, promoting extensive exploration and maintaining high diversity.   \n185 Given the ELBO, we now define soft value functions of the option framework following the Bellman   \n186 Backup Functions along the trajectory $q(\\tau)$ as bellow: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad Q_{O}^{s o f t}[{\\bf s}_{t},{\\bf o}_{t}]=f(\\cdot)+\\mathbb{E}_{{\\bf a}_{t}\\sim\\pi^{A}}\\left[Q_{A}^{s o f t}[{\\bf s}_{t},{\\bf o}_{t},{\\bf a}_{t}]\\right]+H[\\pi^{A}],}\\\\ &{Q_{A}^{s o f t}[{\\bf s}_{t},{\\bf o}_{t},{\\bf a}_{t}]=r(s,a)+\\mathbb{E}_{{\\bf s}_{t+1}\\sim P({\\bf s}_{t+1}\\mid{\\bf s}_{t},{\\bf a}_{t})}\\left[\\mathbb{E}_{{\\bf o}_{t+1}\\sim\\pi^{O}}\\left[Q_{O}^{s o f t}[{\\bf s}_{t+1},{\\bf o}_{t+1}]\\right]+H[\\pi^{O}]\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "187 Assuming policies $\\pi^{A},\\pi^{O}\\in\\Pi$ where $\\Pi$ is an arbitrary feasible set, under a tabular setting where the   \n188 inference on $\\mathcal{L}$ can be done exactly, we have the following theorem holds:   \n189 Theorem 2 (Soft Option Policy Iteration Theorem). Repeated optimizing $\\mathcal{L}$ and $D_{\\mathrm{KL}}$ defined in   \n190 Eq. 10 from any $\\pi_{0}^{A},\\pi_{0}^{O}\\in\\Pi$ converges to optimal policies $\\pi^{A*},\\pi^{O*}$ such that $Q_{O}^{s o f t*}[\\mathbf{s}_{t},\\mathbf{o}_{t}]\\geq$   \n191 $Q_{O}^{s o f t}[\\mathbf{s}_{t},\\mathbf{o}_{t}]$ and $Q_{A}^{s o f t*}[\\mathbf{s}_{t},\\mathbf{o}_{t},\\mathbf{a}_{t}]\\;\\geq\\;Q_{A}^{s o f t}[\\mathbf{s}_{t},\\mathbf{o}_{t},\\mathbf{a}_{t}],$ , for all $\\pi_{0}^{A},\\pi_{0}^{O}\\;\\in\\;\\Pi$ and $(\\mathbf{s}_{t},\\mathbf{a}_{t},\\mathbf{o}_{t})\\;\\in$   \n192 $s\\times A\\times O$ , assuming under tabular settings where $|{\\cal S}|<\\infty$ , $|\\mathcal{O}|<\\infty$ , $|{\\mathcal{A}}|<\\infty$ . ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "193 Proof. See Appendix A.2. ", "page_idx": 5}, {"type": "text", "text": "194 Theorem 2 guarantees finding the optimal solution only when the inference can be done exactly   \n195 under tabular settings. However, real-world applications often involve large continuous domains and   \n196 employ neural networks as function approximators. In these cases, inference procedures can only be   \n197 done approximately. This necessitate a practical approximation algorithm which we present below. ", "page_idx": 5}, {"type": "text", "text": "198 3.2 Variational Markovian Option Critic Algorithm ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "199 Formulating complex problems as probabilistic graphical models (PGMs) allowing us to leverage   \n200 established methods from PGM literature to address the associated inference and learning challenges   \n201 in real-world applications. To this end, we utilizes the structured variational inference treatment for   \n202 optimizing the log-likelihood of optimal trajectory and prove its convergence under approximate   \n203 inference settings. Specifically, using the variational distribution $q(\\tau)$ (Eq. 6) as an approximator, the   \n204 ELBO can be derived as (see Appendix A.3): ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}(q(\\tau),P(\\tau,\\mathcal{E}_{1:T}^{A},\\mathcal{E}_{1:T}^{O}))=-D_{\\mathrm{KL}}(q(\\tau)||P(\\tau|\\mathcal{E}_{1:T}^{A},\\mathcal{E}_{1:T}^{O}))+\\log P(\\mathcal{E}_{1:T}^{A},\\mathcal{E}_{1:T}^{O})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "205 where $D_{\\mathrm{KL}}$ is the KL-Divergence between the trajectory following variational policies $q(\\tau)$ and   \n206 optimal policies $P(\\tau|\\mathcal{E}_{1:T}^{A},\\check{\\mathcal{E}_{1:T}^{O}})$ . Under the structural variational inference [28] perspective, con  \n207 vergence to the optimal policy can be achieved by optimizing the ELBO with respect to the the   \n208 variational policy repeatedly:   \n209 Theorem 3 (Convergence Theorem for Variational Markovian Option Policy Iteration). Let $\\tau$ be   \n210 the latent variable and $\\mathcal{E}^{A},\\mathcal{E}^{O}$ be the ground-truth optimality variables. Define the variational   \n211 distribution $q(\\tau)$ and the true log-likelihood of optimality $\\log{\\dot{P}(\\mathcal{E}^{A},\\mathcal{E}^{O})}$ . iterates according to the   \n212 update rule $q^{k+1}=\\arg\\operatorname*{max}_{q}\\mathcal{L}(q(\\tau),P(\\tau,\\mathcal{E}_{1:T}^{A},\\mathcal{E}_{1:T}^{O}))$ converges to the maximum value bounded   \n213 by the true log-likelihood of optimality. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "214 Proof. See Appendix A.4. ", "page_idx": 5}, {"type": "text", "text": "215 We further implements a practical algorithm, the Variational Markovian Option Critic (VMOC)   \n216 algorithm, which is suitable for complex continuous domains. Specifically, we employ parameterized   \n217 neural networks as function approximators for both the Q-functions $(Q_{\\psi^{A}}^{s o f t},Q_{\\psi^{O}}^{s o f t})$ , Qs\u03c8oOf t) and the policies   \n218 $(\\pi_{\\theta^{A}},\\pi_{\\theta^{O}})$ . Instead of running evaluation and improvement to full convergence using Theorem 2, we   \n219 can optimize the variational distribution by taking stochastic gradient descent following Theorem 3   \n220 with respect to the ELBO (Eq. 7) directly. Share the same motivation with Haarnoja et al. [19]   \n221 of reducing the variance during the optimization procedure, we derive an option critic framework   \n222 by optimizing the maximum entropy objectives between the action Eq. 9 and the option Eq. 8   \n223 alternatively. The Bellman residual for the action critic is: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{J_{Q^{A}}(\\psi_{i}^{A})=\\mathbb{E}_{(\\mathbf{s}_{t},\\mathbf{o}_{t},\\mathbf{a}_{t},\\mathbf{s}_{t+1})\\sim D}\\bigg[\\bigg(\\operatorname*{min}_{i=1,2}Q_{\\psi_{i}^{A}}(\\mathbf{s}_{t},\\mathbf{o}_{t},\\mathbf{a}_{t})-}&{{}}\\\\ {\\big(r(\\mathbf{s}_{t},\\mathbf{a}_{t})+\\mathbb{E}_{\\mathbf{o}_{t+1}\\sim\\pi^{\\sigma}}\\left[Q_{O}^{s o f t}[\\mathbf{s}_{t+1},\\mathbf{o}_{t+1}]\\right]+\\alpha^{O}H[\\pi^{O}]\\big)\\bigg)^{2}\\bigg]}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "224 where $\\alpha^{O}$ is the temperature hyper-parameter and the expectation over option random variable   \n225 $\\mathbb{E}_{\\mathbf{o}_{t+1}\\sim\\pi^{O}}$ can be evaluated exactly since $\\pi^{O}$ is a discrete distribution. The Bellman residual for the   \n226 option critic is: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{J_{Q^{o}}(\\psi_{i}^{O})=\\mathbb{E}_{(\\mathbf{s}_{t},\\mathbf{o}_{t})\\sim{D}}\\bigg[\\bigg(\\underset{i=1,2}{\\operatorname*{min}}Q_{\\psi_{i}^{o}}^{O}(\\mathbf{s}_{t},\\mathbf{o}_{t})-}\\\\ &{\\qquad\\qquad\\qquad\\quad\\left(f(\\cdot)+\\mathbb{E}_{\\mathbf{a}_{t}\\sim\\pi^{A}}\\left[Q_{A}^{s o f t}[\\mathbf{s}_{t},\\mathbf{o}_{t},\\mathbf{a}_{t}]-\\alpha^{A}\\log q(\\mathbf{a}_{t}|\\mathbf{s}_{t},\\mathbf{o}_{t})\\right]\\right)\\bigg)^{2}\\bigg]}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "227 $\\alpha^{A}$ is the temperature hyper-parameter. Unlike $\\mathbb{E}_{\\mathbf{o}_{t+1}\\sim\\pi^{O}}$ can be trivially evaluated, evaluating   \n228 $\\mathbb{E}_{\\mathbf{a}_{t}\\sim\\pi^{A}}$ is typically intractable. Therefore, in implementation we use ${\\bf a}_{t}$ sampled from the replay   \n229 buffer to estimate the expectation over $\\pi^{A}$ .   \n230 Following Theorem 3, the policy gradients can be derived by directly taking gradient with respect to   \n231 the ELBOs defined for the action Eq. 9 and the option Eq. 8 policies respectively. The action policy   \n232 objective is given by: ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\nJ_{\\pi^{A}}(\\theta^{A})=-\\mathbb{E}_{(\\mathbf{s}_{t},\\mathbf{o}_{t})\\sim D}\\left[\\operatorname*{min}_{i=1,2}Q_{\\psi_{i}^{A}}(\\mathbf{s}_{t},\\mathbf{o}_{t},\\tilde{\\mathbf{a}}_{t})-\\alpha^{A}\\log q(\\tilde{\\mathbf{a}}_{t}|\\mathbf{s}_{t},\\mathbf{o}_{t})\\right],\\;\\tilde{\\mathbf{a}}_{t}\\sim q(\\cdot|\\mathbf{s}_{t},\\mathbf{o}_{t})\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "233 where in practice the action policy is often sampled by using the re-parameterization trick introduced   \n234 in [19]. The option objective is given by: ", "page_idx": 6}, {"type": "equation", "text": "$$\nJ_{\\pi^{O}}(\\theta^{O})=-\\mathbb{E}_{(\\mathbf{s}_{t},\\mathbf{o}_{t-1})\\sim D}\\left[\\operatorname*{min}_{i=1,2}Q_{\\psi_{i}^{O}}(\\mathbf{s}_{t},\\mathbf{o}_{t})+\\alpha^{O}\\mathcal{H}[\\pi^{O}]\\right]\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "235 The variational distribution $q(\\tau)$ defined in Eq. 6 allows us to learn options as embeddings [34, 35]   \n236 with a learnable embedding matrix $\\mathbf{W}\\in\\mathbb{R}^{\\mathrm{num\\_options}\\times}$ embedding_dim. Under this setting, the embedding   \n237 matrix W can be absorbed into the parameter vector $\\theta^{O}$ . This integration into VMOC ensures that   \n238 options are represented as embeddings without any additional complications, thereby enhancing the   \n239 expressiveness and scalability of the model. ", "page_idx": 6}, {"type": "text", "text": "240 The temperature hyper-parameters can also be adjusted by minimizing the following objective: ", "page_idx": 6}, {"type": "equation", "text": "$$\nJ(\\alpha^{A})=-\\mathbb{E}_{\\tilde{\\mathbf{a}}_{t}\\sim\\pi^{A}}\\left[\\alpha^{A}(\\log\\pi^{A}(\\tilde{\\mathbf{a}}_{t}\\mid\\mathbf{s}_{t},\\mathbf{o}_{t})+\\overline{{\\mathcal{H}}})\\right]\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "241 for the action policy temperature $\\alpha^{A}$ , where $\\overline{{\\mathcal{H}}}$ is a target entropy. Similarly, the option policy   \n242 temperature $\\alpha^{\\mathcal{O}}$ can be adjusted by: ", "page_idx": 6}, {"type": "equation", "text": "$$\nJ(\\alpha^{O})=-\\mathbb{E}_{\\mathbf{o}_{t}\\sim\\pi^{O}}\\left[\\alpha^{O}(\\log\\pi^{O}(\\mathbf{o}_{t}\\mid\\mathbf{s}_{t},\\mathbf{o}_{t-1})+\\overline{{\\mathcal{H}}})\\right]\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "243 where $\\overline{{\\mathcal{H}}}$ is also a target entropy for the option policy. In both cases, the temperatures $\\alpha^{A}$ and $\\alpha^{O}$   \n244 are updated using gradient descent, ensuring that the entropy regularization terms dynamically adapt   \n245 to maintain a desired level of exploration. This approach aligns with the methodology proposed   \n246 in SAC [19]. By adjusting the temperature parameters, the VMOC algorithm ensures a balanced   \n247 trade-off between exploration and exploitation, which is crucial for achieving optimal performance in   \n248 complex continuous control tasks. We summarize the VMOC algorithm in Appendix B. ", "page_idx": 6}, {"type": "text", "text": "249 4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "250 In this section, we design experiments on the challenging single task OpenAI Gym MuJoCo [7]   \n251 environments (10 environments) to test Variational Markovian Option Critic (VMOC)\u2019s performance   \n252 over other option variants and non-option baselines.   \n253 For VMOC in all environments, we fix the temperature rate for both $\\alpha^{O}$ and $\\alpha^{A}$ to 0.05; we add an   \n254 exploration noise $\\mathcal{N}(\\mu=0,\\sigma=0.2)$ during exploration. For all baselines, we follow DAC [52]\u2019s   \n255 open source implementations and compare our algorithm with six baselines, five of which are option   \n256 variants, i.e., MOPG [35], $\\scriptstyle\\mathrm{DAC}+\\mathrm{PPO}$ , AHP $^+$ PPO [32], IOPG [45], PPOC [27], OC [4] and PPO   \n257 [41]. All baselines\u2019 parameters used by DAC remain unchanged over 1 million environment steps   \n258 to converge. Figures are plotted following DAC\u2019s style: curves are averaged over 10 independent   \n259 runs and smoothed by a sliding window of size 20. Shaded regions indicate standard deviations.   \n260 All experiments are run on an Inte $\\textsuperscript{\\textregistered}$ Core\u2122i9-9900X CPU $\\textcircled{a}3.50\\mathrm{GHz}$ with a single thread and   \n261 process. Our implementation details are summarized in Appendix C. For a fair comparison, we follow   \n262 option literature conventions and use four options in all implementations. Our code is available in   \n263 supplemental materials. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "264 5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "265 We evaluate the performance of VMOC against six option-based baselines (MOPG [35],   \n266 DAC+PPO [52], AHP $+.$ PPO [32], IOPG [45], PPOC [27], and OC [4]) as well as the hierarchy-free   \n267 PPO algorithm [41]. Previous studies [27, 45, 20, 52] have suggested that option-based algorithms   \n268 do not exhibit significant advantages over hierarchy-free algorithms in single-task environments.   \n269 Nonetheless, our results demonstrate that VMOC significantly outperforms all baselines in terms   \n270 of episodic return, convergence speed, step variance, and variance across 10 runs, as illustrated in   \n271 Figure 2. The only exception is the relatively simple InvertedDoublePendulum environment, which   \n273 Notably, VMOC exhibits superior performance on the Humanoid-v2 and HumanoidStandup-v2   \n274 environments. These environments are characterized by a large state space $\\ensuremath{\\mathcal{S}}\\in\\mathbb{R}^{376}$ ) and action   \n275 space $(A\\in\\mathbb{R}^{17})$ ), whereas other environments typically have state dimensions less than 20 and   \n276 action dimensions less than 5. The enhanced performance of VMOC in these environments can be   \n277 attributed to its maximum entropy capability: in large state-action spaces, the agent must maximize   \n278 rewards while exploring a diverse set of state-action pairs. Maximum likelihood methods tend to   \n279 quickly saturate with early rewarding observations, leading to the selection of low-entropy options   \n280 that converge to local optima.   \n281 A particularly relevant comparison is with the Markovian Option Policy Gradient (MOPG) [35],   \n282 as both VMOC and MOPG are developed based on HiT-MDPs and employ option embeddings.   \n283 Despite being derived under the maximum entropy framework, MOPG utilizes an on-policy gradient   \n284 descent approach. Our experimental results show that VMOC\u2019s performance surpasses that of MOPG,   \n285 highlighting the limitations of on-policy methods, which suffer from shortsighted rollout lengths   \n286 and quickly saturate to early high-reward observations. In contrast, VMOC\u2019s variational off-policy   \n287 approach effectively utilizes the maximum entropy framework by ensuring better exploration and   \n288 stability across the learning process. Additionally, the off-policy nature of VMOC allows it to reuse   \n289 samples from a replay buffer, enhancing sample efficiency and promoting greater diversity in the   \n290 learned policies. This capability leads to more robust learning, as the algorithm can leverage a broader   \n291 range of experiences to improve policy optimization. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "image", "img_path": "eM3Wzs6Unt/tmp/74014d3ded981934daaffd0dac7e0f14c69096fc7f99819bd8376b2409f696a1.jpg", "img_caption": ["Figure 2: Experiments on Mujoco Environments. Curves are averaged over 10 independent runs with different random seeds and smoothed by a sliding window of size 20. Shaded regions indicate standard deviations. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "292 6 Related Work ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "293 The VMOC incorporates three key ingredients: the option framework, a structural variational in  \n294 ference based off-policy algorithm and latent variable policies. We review prior works that draw   \n295 on some of these ideas in this section. The options framework [47] offers a promising approach   \n296 for discovering and reusing temporal abstractions, with options representing temporally abstract   \n297 skills. Conventional option frameworks [39], typically developed under the maximum likelihood   \n298 (MLE) framework with few constraints on options behavior, often suffer from the option degra  \n299 dation problem [32, 4]. This problem occurs when options quickly saturate with early rewarding   \n300 observations, causing a single option to dominate the entire policy, or when options switch every   \n301 timestep, maximizing policy at the expense of skill reuse across tasks. On-policy option learning   \n302 algorithms [4, 3, 52, 34, 35] aim to maximize expected return by adjusting policy parameters to in  \n303 crease the likelihood of high-reward option trajectories, which often leads to focusing on low-entropy   \n304 options. Several techniques [20, 21, 23] have been proposed to enhance on-policy algorithms with   \n305 entropy-like extrinsic rewards as regularizers, but these often result in biased optimal trajectories. In   \n306 contrast, the maximum entropy term in VMOC arises naturally within the variational framework and   \n307 provably converges to the optimal trajectory.   \n308 Although several off-policy option learning algorithms have been proposed [10, 43, 45, 50], these   \n309 typically focus on improving sample efficiency by leveraging the control as inference framework.   \n310 Recent works [45] aim to enhance sample efficiency by inferring and marginalizing over options,   \n311 allowing all options to be learned simultaneously. Wulfmeier et al. [50] propose off-policy learning of   \n312 all options across every experience in hindsight, further boosting sample efficiency. However, these   \n313 approaches generally lack constraints on options behavior. A closely related work [33] also derives   \n314 a variational approach under the option framework; however, it is based on probabilistic graphical   \n315 model that we believe are incorrect, potentially leading to convergence issues. Additionally, our   \n316 algorithm enables learning options as latent embeddings, a feature not present in their approach.   \n317 Recently, several studies have extended the maximum entropy reinforcement learning framework to   \n318 discover skills by incorporating additional latent variables. One class of methods [22, 17] maintains   \n319 latent variables constant over the duration of an episode, providing a time-correlated exploration   \n320 signal. Other works [19, 51] focus on discovering multi-level action abstractions that are suitable for   \n321 repurposing by promoting skill distinguishability, but they do not incorporate temporal abstractions.   \n322 Studies such as [38, 1, 8] aim to discover temporally abstract skills essential for exploration, but they   \n323 predefine their temporal resolution. In contrast, VMOC learns temporal abstractions as embeddings   \n324 in an end-to-end data-driven approach with minimal prior knowledge encoded in the framework. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "325 7 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "326 In this paper, we have introduced the Variational Markovian Option Critic (VMOC), a novel off-policy   \n327 algorithm designed to address the challenges of ineffective exploration, sample inefficiency, and com  \n328 putational complexity inherent in the conventional option framework for hierarchical reinforcement   \n329 learning. By integrating a variational inference framework, VMOC leverages maximum entropy   \n330 as intrinsic rewards to promote the discovery of diverse and effective options. Additionally, by   \n331 employing low-cost option embeddings instead of traditional, computationally expensive option   \n332 triples, VMOC enhances both scalability and expressiveness. Extensive experiments in challenging   \n333 Mujoco environments demonstrate that VMOC significantly outperforms existing on-policy and   \n334 off-policy option variants, validating its effectiveness in learning coherent and diverse option sets   \n335 suitable for complex tasks. This work advances the field of hierarchical reinforcement learning by   \n336 providing a robust, scalable, and efficient method for learning temporally extended actions. ", "page_idx": 8}, {"type": "text", "text": "337 8 Limitations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "338 Due to limited computing resources, we did not conduct an ablation study of VMOC. Additionally,   \n339 the temperature parameter was fixed in our experiments, whereas an automatically tuned parameter   \n340 could potentially enhance performance (see SAC [19]). While our baselines focus on option variants,   \n341 a thorough comparison to other off-policy algorithms is also worth investigating. It is particularly   \n342 important to explore whether VMOC exhibits performance improvements in scalability when the   \n343 number of option embeddings is significantly increased. These investigations are left for future work. ", "page_idx": 8}, {"type": "text", "text": "344 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "345 [1] Ajay, A., Kumar, A., Agrawal, P., Levine, S., and Nachum, O. Opal: Offilne primitive discovery   \n346 for accelerating offline reinforcement learning. arXiv preprint arXiv:2010.13611, 2020.   \n347 [2] Araujo, E. G. and Grupen, R. A. Learning control composition in a complex environment. In   \n348 Proceedings of the Fourth International Conference on Simulation of Adaptive Behavior, pp.   \n349 333\u2013342, 1996.   \n350 [3] Bacon, P.-L. Temporal Representation Learning. PhD thesis, McGill University Libraries, 2018.   \n351 [4] Bacon, P.-L., Harb, J., and Precup, D. The option-critic architecture. In Thirty-First AAAI   \n352 Conference on Artificial Intelligence, 2017.   \n353 [5] Bertsekas, D. and Tsitsiklis, J. N. Neuro-dynamic programming. Athena Scientific, 1996.   \n354 [6] Brockett, R. W. Hybrid models for motion control systems. In Essays on Control: Perspectives   \n355 in the Theory and its Applications, pp. 29\u201353. Springer, 1993.   \n356 [7] Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., and Zaremba,   \n357 W. Openai gym. arXiv preprint arXiv:1606.01540, 2016.   \n358 [8] Co-Reyes, J., Liu, Y., Gupta, A., Eysenbach, B., Abbeel, P., and Levine, S. Self-consistent   \n359 trajectory autoencoder: Hierarchical reinforcement learning with trajectory embeddings. In   \n360 International conference on machine learning, pp. 1009\u20131018. PMLR, 2018.   \n361 [9] Colombetti, M., Dorigo, M., and Borghi, G. Behavior analysis and training-a methodology   \n362 for behavior engineering. IEEE Transactions on Systems, Man, and Cybernetics, Part B   \n363 (Cybernetics), 26(3):365\u2013380, 1996.   \n364 [10] Daniel, C., Van Hoof, H., Peters, J., and Neumann, G. Probabilistic inference for determining   \n365 options in reinforcement learning. Machine Learning, 104(2-3):337\u2013357, 2016.   \n366 [11] Dayan, P. and Hinton, G. E. Feudal reinforcement learning. Advances in Neural Information   \n367 Processing Systems, pp. 271\u2013278, 1993.   \n368 [12] Dietterich, T. G. Hierarchical reinforcement learning with the maxq value function decomposi  \n369 tion. Journal of Artificial Intelligence Research, 13:227\u2013303, 2000.   \n370 [13] Eysenbach, B., Gupta, A., Ibarz, J., and Levine, S. Diversity is all you need: Learning skills   \n371 without a reward function. arXiv preprint arXiv:1802.06070, 2018.   \n372 [14] Fujimoto, S., Van Hoof, H., and Meger, D. Addressing function approximation error in   \n373 actor-critic methods. arXiv preprint arXiv:1802.09477, 2018.   \n374 [15] Goyal, A., Islam, R., Strouse, D., Ahmed, Z., Botvinick, M., Larochelle, H., Bengio, Y., and   \n375 Levine, S. Infobot: Transfer and exploration via the information bottleneck. arXiv preprint   \n376 arXiv:1901.10902, 2019.   \n377 [16] Guo, Z., Thomas, P. S., and Brunskill, E. Using options and covariance testing for long   \n378 horizon off-policy policy evaluation. In Advances in Neural Information Processing Systems,   \n379 pp. 2492\u20132501, 2017.   \n380 [17] Gupta, A., Kumar, V., Lynch, C., Levine, S., and Hausman, K. Relay policy learning: Solving   \n381 long-horizon tasks via imitation and reinforcement learning. arXiv preprint arXiv:1910.11956,   \n382 2019.   \n383 [18] Haarnoja, T., Tang, H., Abbeel, P., and Levine, S. Reinforcement learning with deep energy  \n384 based policies. In International Conference on Machine Learning, pp. 1352\u20131361. PMLR,   \n385 2017.   \n386 [19] Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. Soft actor-critic: Off-policy maximum   \n387 entropy deep reinforcement learning with a stochastic actor. arXiv preprint arXiv:1801.01290,   \n388 2018.   \n389 [20] Harb, J., Bacon, P.-L., Klissarov, M., and Precup, D. When waiting is not an option: Learning   \n390 options with a deliberation cost. In Thirty-Second AAAI Conference on Artificial Intelligence,   \n391 2018.   \n392 [21] Harutyunyan, A., Dabney, W., Borsa, D., Heess, N., Munos, R., and Precup, D. The termination   \n393 critic. arXiv preprint arXiv:1902.09996, 2019.   \n394 [22] Hausman, K., Springenberg, J. T., Wang, Z., Heess, N., and Riedmiller, M. Learning an   \n395 embedding space for transferable robot skills. In International Conference on Learning Repre  \n396 sentations, 2018.   \n397 [23] Kamat, A. and Precup, D. Diversity-enriched option-critic. arXiv, 2020.   \n398 [24] Khetarpal, K. and Precup, D. Learning options with interest functions. In Proceedings of the   \n399 32nd AAAI Conference on Artificial Intelligence, pp. 1\u20132, 2019.   \n400 [25] Khetarpal, K., Klissarov, M., Chevalier-Boisvert, M., Bacon, P.-L., and Precup, D. Options of   \n401 interest: Temporal abstraction with interest functions. In Proceedings of the AAAI Conference   \n402 on Artificial Intelligence, volume 34, pp. 4,444\u20134,451, 2020.   \n403 [26] Klissarov, M. and Precup, D. Flexible option learning. In Ranzato, M., Beygelzimer, A.,   \n404 Dauphin, Y., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing   \n405 Systems, volume 34, pp. 4632\u20134646. Curran Associates, 2021.   \n406 [27] Klissarov, M., Bacon, P.-L., Harb, J., and Precup, D. Learnings options end-to-end for continu  \n407 ous action tasks. arXiv preprint arXiv:1712.00004, 2017.   \n408 [28] Koller, D. and Friedman, N. Probabilistic graphical models: principles and techniques. MIT   \n409 press, 2009.   \n410 [29] Kolobov, A., Weld, D. S., et al. Discovering hidden structure in factored mdps. Artificial   \n411 Intelligence, 189:19\u201347, 2012.   \n412 [30] Konidaris, G. and Barto, A. G. Skill discovery in continuous reinforcement learning domains   \n413 using skill chaining. In Advances in neural information processing systems, pp. 1015\u20131023,   \n414 2009.   \n415 [31] Levine, S. Reinforcement learning and control as probabilistic inference: Tutorial and review.   \n416 arXiv preprint arXiv:1805.00909, 2018.   \n417 [32] Levy, K. Y. and Shimkin, N. Unified inter and intra options learning using policy gradient   \n418 methods. In European Workshop on Reinforcement Learning, pp. 153\u2013164. Springer, 2011.   \n419 [33] Li, C., Ma, X., Zhang, C., Yang, J., Xia, L., and Zhao, Q. Soac: The soft option actor-critic   \n420 architecture. arXiv preprint arXiv:2006.14363, 2020.   \n421 [34] Li, C., Song, D., and Tao, D. The skill-action architecture: Learning abstract action embeddings   \n422 for reinforcement learning. 2020.   \n423 [35] Li, C., Song, D., and Tao, D. Hit-mdp: learning the smdp option framework on mdps with hidden   \n424 temporal embeddings. In The Eleventh International Conference on Learning Representations,   \n425 2022.   \n426 [36] Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves,   \n427 A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., et al. Human-level control through deep   \n428 reinforcement learning. Nature, 518(7540):529\u2013533, 2015.   \n429 [37] Osa, T., Tangkaratt, V., and Sugiyama, M. Hierarchical reinforcement learning via advantage  \n430 weighted information maximization. arXiv preprint arXiv:1901.01365, 2019.   \n431 [38] Pertsch, K., Rybkin, O., Ebert, F., Finn, C., Jayaraman, D., and Levine, S. Long-horizon visual   \n432 planning with goal-conditioned hierarchical predictors. NeurIPS, 2020.   \n433 [39] Precup, D. Temporal abstraction in reinforcement learning. University of Massachusetts   \n434 Amherst, 2000.   \n435 [40] Schulman, J., Chen, X., and Abbeel, P. Equivalence between policy gradients and soft q-learning.   \n436 arXiv preprint arXiv:1704.06440, 2017.   \n437 [41] Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimiza  \n438 tion algorithms. arXiv preprint arXiv:1707.06347, 2017.   \n439 [42] Sharma, A., Gu, S., Levine, S., Kumar, V., and Hausman, K. Dynamics-aware unsupervised   \n440 discovery of skills. arXiv preprint arXiv:1907.01657, 2019.   \n441 [43] Shiarlis, K., Wulfmeier, M., Salter, S., Whiteson, S., and Posner, I. Taco: Learning task   \n442 decomposition via temporal alignment for control. In International Conference on Machine   \n443 Learning, pp. 4654\u20134663. PMLR, 2018.   \n444 [44] Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., Schrittwieser,   \n445 J., Antonoglou, I., Panneershelvam, V., Lanctot, M., et al. Mastering the game of go with deep   \n446 neural networks and tree search. Nature, 529(7587):484\u2013489, 2016.   \n447 [45] Smith, M., Hoof, H., and Pineau, J. An inference-based policy gradient method for learning   \n448 options. In International Conference on Machine Learning, pp. 4,703\u20134,712, 2018.   \n449 [46] Sutton, R. S. and Barto, A. G. Reinforcement learning: An introduction. MIT press, 2018.   \n450 [47] Sutton, R. S., Precup, D., and Singh, S. Between mdps and semi-mdps: A framework for   \n451 temporal abstraction in reinforcement learning. Artificial Intelligence, 112(1-2):181\u2013211, 1999.   \n452 [48] Todorov, E. Linearly-solvable markov decision problems. Advances in neural information   \n453 processing systems, 19, 2006.   \n454 [49] Todorov, E., Erez, T., and Tassa, Y. Mujoco: A physics engine for model-based control. In 2012   \n455 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026\u20135033. IEEE,   \n456 2012.   \n457 [50] Wulfmeier, M., Rao, D., Hafner, R., Lampe, T., Abdolmaleki, A., Hertweck, T., Neunert, M.,   \n458 Tirumala, D., Siegel, N., Heess, N., et al. Data-efficient hindsight off-policy option learning.   \n459 arXiv preprint arXiv:2007.15588, 2020.   \n460 [51] Zhang, D., Courville, A., Bengio, Y., Zheng, Q., Zhang, A., and Chen, R. T. Latent   \n461 state marginalization as a low-cost approach for improving exploration. arXiv preprint   \n462 arXiv:2210.00999, 2022.   \n463 [52] Zhang, S. and Whiteson, S. DAC: The double actor-critic architecture for learning options. In   \n464 Advances in Neural Information Processing Systems, pp. 2,012\u20132,022, 2019.   \n465 [53] Ziebart, B. D., Bagnell, J. A., and Dey, A. K. Modeling interaction via the principle of maximum   \n466 causal entropy. In ICML, 2010. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "467 A Proofs ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "468 A.1 Theorem 1 ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "469 Theorem 1 (Convergence Theorem for Structured Variational Policy Iteration). Let $\\tau$ be the   \n470 latent variable and $\\mathcal{E}$ be the observed variable. Define the variational distribution $q(\\tau)$ and the   \n471 log-likelihood $\\log{P(\\mathcal{E})}$ . Let $M:q^{[k]}\\,\\rightarrow\\,q^{[k+1]}$ represent the mapping defined by the EM steps   \n472 inference update, so that $q^{[k+1]}=M(q^{[k]})$ . The likelihood function increases at each iteration of the   \n473 variational inference algorithm until the conditions for equality are satisfied and a fixed point of the   \n474 iteration is reached: ", "page_idx": 11}, {"type": "text", "text": "475 ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\mathcal{L}(q^{[k+1]},P)=\\mathcal{L}(q^{[k]},P)\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "476 and ", "page_idx": 11}, {"type": "equation", "text": "$$\nD_{K L}(q^{[k+1]}(\\tau)\\parallel P(\\tau\\mid{\\mathcal{E}}))=D_{K L}(q^{[k]}(\\tau)\\parallel P(\\tau\\mid{\\mathcal{E}})).\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "477 Proof. Let $\\tau$ be the latent variable and $\\mathcal{E}$ be the observed variable. Define the evidence lower bound   \n478 (ELBO) as $\\mathcal{L}(q,P)$ and the Kullback-Leibler divergence as $\\operatorname{D}_{\\mathrm{KL}}(q\\parallel P)$ , where $q(\\tau)$ approximates   \n479 the posterior distribution and $P(\\mathcal{E}\\mid\\tau)$ is the likelihood. ", "page_idx": 12}, {"type": "text", "text": "480 The log-likelihood function $\\log{P(\\mathcal{E})}$ can be decomposed as: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\log P(\\mathcal{E})=\\mathcal{L}(q,P)+\\mathrm{D}_{\\mathrm{KL}}(q(\\tau)\\parallel P(\\tau\\mid\\mathcal{E})),\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "481 where ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\boldsymbol{q},\\boldsymbol{P})=\\mathbb{E}_{\\boldsymbol{q}(\\tau)}\\left[\\log{P(\\mathcal{E},\\tau)}-\\log{q(\\tau)}\\right]\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "482 and ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\operatorname{D}_{\\mathrm{KL}}(q(\\tau)\\parallel P(\\tau\\mid{\\mathcal{E}}))=\\mathbb{E}_{q(\\tau)}\\left[\\log{\\frac{q(\\tau)}{P(\\tau\\mid{\\mathcal{E}})}}\\right].\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "483 Let $M:q^{[k]}\\to q^{[k+1]}$ represent the mapping defined by the variational inference update, so that   \n484 $q^{[k+1]}\\,=\\,M(q^{[k]})$ . If $q^{*}$ is a variational distribution that maximizes the ELBO, so that $\\log{P(\\mathcal{E}\\mid}$   \n485 $q^{*})\\ \\geq\\ \\log P(\\mathcal{E}\\ |\\ \\ q)$ for all $q$ , then $\\log P(\\mathcal{E}\\;\\mid\\;M(q^{*}))\\;=\\;\\log P(\\mathcal{E}\\;\\mid\\;q^{*})$ . In other words, the   \n486 maximizing distributions are fixed points of the variational inference algorithm. Since the likelihood   \n487 function is bounded (for distributions of practical interest), the sequence of variational distributions   \n488 $q^{[0]},q^{[1]},\\dotsc,q^{[k]}$ yields a bounded nondecreasing sequence $\\log\\bar{P}(\\mathcal{E}\\mid q^{[0]})\\,\\leq\\,\\log P(\\mathcal{E}\\mid q^{[1]})\\,\\leq$   \n489 $\\cdot\\cdot\\cdot\\leq\\log P({\\mathcal{E}}\\mid q^{[k]})\\leq\\log P({\\mathcal{E}}\\mid q^{[k]})$ which must converge as $k\\rightarrow\\infty$ . ", "page_idx": 12}, {"type": "text", "text": "490 ", "page_idx": 12}, {"type": "text", "text": "491 A.2 Theorem 2 ", "page_idx": 12}, {"type": "text", "text": "492 Theorem 2 (Soft Option Policy Iteration Theorem). Repeated optimizing $\\mathcal{L}$ and $D_{\\mathrm{KL}}$ defined in   \n493 Eq. 10 from any $\\pi_{0}^{A},\\pi_{0}^{O}\\in\\Pi$ converges to optimal policies $\\pi^{A*},\\pi^{O*}$ such that $Q_{O}^{s o f t*}[\\mathbf{s}_{t},\\mathbf{o}_{t}]\\geq$   \n494 $Q_{O}^{s o f t}[\\mathbf{s}_{t},\\mathbf{o}_{t}]$ and $Q_{A}^{s o f t*}[\\mathbf{s}_{t},\\mathbf{o}_{t},\\mathbf{a}_{t}]\\;\\geq\\;Q_{A}^{s o f t}[\\mathbf{s}_{t},\\mathbf{o}_{t},\\mathbf{a}_{t}],$ , for all $\\pi_{0}^{A},\\pi_{0}^{O}\\;\\in\\;\\Pi$ and $(\\mathbf{s}_{t},\\mathbf{a}_{t},\\mathbf{o}_{t})\\;\\in$   \n495 $s\\times A\\times O$ , assuming $\\cdot\\left|S\\right|<\\infty,\\;\\left|\\mathcal{O}\\right|<\\infty,\\;\\left|A\\right|<\\infty.$   \n496 Proof. Define the entropy augmented reward as $r^{s o f t}(\\mathbf{s}_{t},\\mathbf{a}_{t})\\;\\;=\\;\\;r(\\mathbf{s}_{t},\\mathbf{a}_{t})\\;+\\;\\mathcal{H}[\\pi^{A}]$ and   \n497 $f^{s o f t}\\big(\\mathbf{o}_{t},\\mathbf{s}_{t},\\mathbf{a}_{t},\\mathbf{o}_{t-1}\\big)=f\\big(\\mathbf{o}_{t},\\mathbf{s}_{t},\\mathbf{a}_{t},\\mathbf{o}_{t-1}\\big)+\\mathcal{H}[\\pi^{O}]$ and rewrite Bellman Backup functions as, ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ \\ \\ \\ {Q_{O}}[{\\bf s}_{t},{\\bf o}_{t}]=f^{s o f t}(\\cdot)+{\\mathbb{E}}_{{\\bf a}_{t}\\sim\\pi^{A}}\\left[{Q_{A}}[{\\bf s}_{t},{\\bf o}_{t},{\\bf a}_{t}]\\right],}\\\\ &{{Q_{A}}[{\\bf s}_{t},{\\bf o}_{t},{\\bf a}_{t}]=r^{s o f t}(s,a)+{\\mathbb{E}}_{{\\bf s}_{t+1}\\sim P({\\bf s}_{t+1}|{\\bf s}_{t},{\\bf a}_{t})}\\left[{\\mathbb{E}}_{{\\bf o}_{t+1}\\sim\\pi^{O}}\\left[{Q_{O}}[{\\bf s}_{t+1},{\\bf o}_{t+1}]\\right]\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "498 We start with proving the convergence of soft option policy evaluation. As with the standard Q  \n499 function and value function, we can relate the Q-function at a future state via a Bellman Operator   \n500 ${\\mathcal{T}}^{s o f t}$ . The option-action value function satisfies the Bellman Operator $\\mathcal{T}^{s o f t}$ ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{T}^{s o f t}Q_{A}[\\mathbf{s}_{t},\\mathbf{o}_{t},\\mathbf{a}_{t}]=\\mathbb{E}[G_{t}|\\mathbf{s}_{t},\\mathbf{o}_{t},\\mathbf{a}_{t}]}\\\\ &{\\qquad\\qquad\\qquad\\qquad={r}^{s o f t}(s,a)+\\gamma\\displaystyle\\sum_{\\mathbf{s}_{t+1}}P(\\mathbf{s}_{t+1}|\\mathbf{s}_{t},\\mathbf{a}_{t})Q_{O}[\\mathbf{s}_{t+1},\\mathbf{o}_{t}],}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "501 As with the standard convergence results for policy evaluation [46], by the definition of $\\mathcal{T}^{s o f t}$ (Eq. 11)   \n502 the option-action value function $Q_{A}^{\\pi_{A}}$ is a fixed point. ", "page_idx": 12}, {"type": "text", "text": "503 To prove the ${\\mathcal{T}}^{s o f t}$ is a contraction, define a norm on $V$ -values functions $V$ and $U$ ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\|V-U\\|_{\\infty}\\triangleq\\operatorname*{max}_{\\bar{s}\\in\\bar{S}}|V(\\bar{s})-U(\\bar{s})|.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "504 where $\\bar{s}=\\{s,o\\}$ . ", "page_idx": 12}, {"type": "text", "text": "505 By recurssively apply the Hidden Temporal Bellman Operator ${\\mathcal{T}}^{s o f t}$ , we have: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{\\mathcal{O}}[\\mathbf{s}_{t},\\mathbf{o}_{t-1}]=\\mathbb{E}[G_{t}|\\mathbf{s}_{t},\\mathbf{o}_{t-1}]=\\sum_{\\alpha_{t}}P(\\mathbf{o}_{t}|\\mathbf{s}_{t},\\mathbf{o}_{t-1})Q_{\\mathcal{O}}[\\mathbf{s}_{t},\\mathbf{o}_{t}]}}\\\\ &{=\\sum_{\\alpha_{t}}P(\\mathbf{o}_{t}|\\mathbf{s}_{t},\\mathbf{o}_{t-1})\\sum_{\\mathbf{n}_{t}}P(\\mathbf{a}_{t}|\\mathbf{s}_{t},\\mathbf{o}_{t})\\biggl[r(s,\\mathbf{a})+\\gamma\\sum_{\\alpha_{t+1}}P(\\mathbf{s}_{t+1}|\\mathbf{s}_{t},\\mathbf{a}_{t})Q_{\\mathcal{O}}[\\mathbf{s}_{t+1},\\mathbf{o}_{t}]\\biggr]}\\\\ &{=r(s,a)+\\gamma\\sum_{\\alpha_{t}}P(\\mathbf{o}_{t}|\\mathbf{s}_{t},\\mathbf{o}_{t-1})\\sum_{\\mathbf{n}_{t}}P(\\mathbf{a}_{t}|\\mathbf{s}_{t},\\mathbf{o}_{t})\\sum_{\\mathbf{s}_{t+1}}P(\\mathbf{s}_{t+1}|\\mathbf{s}_{t},\\mathbf{a}_{t})Q_{\\mathcal{O}}[\\mathbf{s}_{t+1},\\mathbf{o}_{t}]}\\\\ &{=r(s,a)+\\gamma\\sum_{\\alpha_{t},\\mathbf{s}_{t+1}}P(\\mathbf{s}_{t+1},\\mathbf{o}_{t}|\\mathbf{s}_{t},\\mathbf{o}_{t-1})Q_{\\mathcal{O}}[\\mathbf{s}_{t+1},\\mathbf{o}_{t}]}\\\\ &{=r(s,a)+\\gamma E_{s+1,\\alpha_{t}}\\biggl[Q_{\\mathcal{O}}[\\mathbf{s}_{t+1},\\mathbf{o}_{t}]\\biggr]}\\end{array}~\\qquad\\qquad\\mathrm{(12)}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "506 Therefore, by applying Eq. 12 to $V$ and $U$ we have: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|T^{\\pi}V-T^{\\pi}U\\|_{\\infty}}\\\\ &{\\quad=\\underset{\\Tilde{s}\\in S}{\\operatorname*{max}}\\left\\vert\\gamma E_{s_{t+1},0_{t}}\\bigg[Q_{\\mathcal{O}}\\big[\\mathbf{s}_{t+1},\\mathbf{o}_{t}\\big]\\right]-\\gamma E_{s_{t+1},0_{t}}\\bigg[U\\big[\\mathbf{s}_{t+1},\\mathbf{o}_{t}\\big]\\bigg]}\\\\ &{\\quad=\\gamma\\underset{\\Tilde{s}\\in S}{\\operatorname*{max}}E_{s_{t+1},0_{t}}\\bigg[\\bigg\\vert Q_{\\mathcal{O}}\\big[\\mathbf{s}_{t+1},\\mathbf{o}_{t}\\big]-U\\big[\\mathbf{s}_{t+1},\\mathbf{o}_{t}\\big]\\bigg\\vert\\bigg]}\\\\ &{\\quad\\le\\gamma\\underset{\\Tilde{s}\\in S}{\\operatorname*{max}}E_{s_{t+1},0_{t}}\\bigg[\\underset{\\Tilde{s}\\in S}{\\operatorname*{max}}\\left\\vert Q_{\\mathcal{O}}\\big[\\mathbf{s}_{t+1},\\mathbf{o}_{t}\\big]-U\\big[\\mathbf{s}_{t+1},\\mathbf{o}_{t}\\big]\\bigg\\vert\\right]}\\\\ &{\\quad\\le\\gamma\\underset{\\Tilde{s}\\in S}{\\operatorname*{max}}\\left\\vert V\\big[\\Tilde{s}\\big]-U\\big[\\Tilde{s}\\big]\\right\\vert}\\\\ &{\\quad=\\gamma\\|V-U\\|_{\\infty}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "507 Therefore, $\\mathcal{T}^{s o f t}$ is a contraction. By the fixed point theorem, assuming that throughout our computa  \n508 tion the $Q_{A}[\\cdot,\\cdot]$ and $Q_{O}[\\cdot]$ are bounded and $\\mathbb{A}<\\infty$ , the sequence $Q_{A}^{k}$ defined by $\\begin{array}{r}{\\bar{Q}_{A}^{k+1}=\\mathcal{T}^{s o f t}Q_{A}^{k}}\\end{array}$   \n509 will converge to the option-action value function $Q_{A}^{\\pi_{A}}$ as $k\\rightarrow\\infty$ .   \n510 The convergence results of and the Soft Option Policy Improvement Theorem then follows conven  \n511 tional Soft Policy Improvement Theorem Theorem 1. Consequently, the Soft Option Policy Iteration   \n512 Theorem follows directly from these results. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "513 ", "page_idx": 13}, {"type": "text", "text": "514 A.3 Derivation of Eq. 10 ", "text_level": 1, "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}(q(\\tau),P(\\tau,\\mathcal{E}_{1:T}^{A},\\mathcal{E}_{1:T}^{O}))=\\mathbb{E}_{q(\\tau)}[\\log P(\\tau,\\mathcal{E}_{1:T}^{A},\\mathcal{E}_{1:T}^{O})-\\log q(\\tau)]}\\\\ &{\\hphantom{=}\\mathbb{E}_{q(\\tau)}[\\log P(\\tau|\\mathcal{E}_{1:T}^{A},\\mathcal{E}_{1:T}^{O})+\\log P(\\mathcal{E}_{1:T}^{A},\\mathcal{E}_{1:T}^{O})-\\log q(\\tau)]}\\\\ &{\\hphantom{=}=\\mathbb{E}_{q(\\tau)}[\\log P(\\tau|\\mathcal{E}_{1:T}^{A},\\mathcal{E}_{1:T}^{O})-\\log q(\\tau)]+\\mathbb{E}_{q(\\tau)}\\log P(\\mathcal{E}_{1:T}^{A},\\mathcal{E}_{1:T}^{O})}\\\\ &{\\hphantom{=}=\\mathbb{E}_{q(\\tau)}[\\frac{\\log P(\\tau|\\mathcal{E}_{1:T}^{A},\\mathcal{E}_{1:T}^{O})}{\\log q(\\tau)}]+\\log P(\\mathcal{E}_{1:T}^{A},\\mathcal{E}_{1:T}^{O})}\\\\ &{\\hphantom{=}=-D_{\\mathrm{KL}}(\\log q(\\tau)\\parallel\\log P(\\tau|\\mathcal{E}_{1:T}^{A},\\mathcal{E}_{1:T}^{O}))+\\log P(\\mathcal{E}_{1:T}^{A},\\mathcal{E}_{1:T}^{O})}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "515 A.4 Theorem 3 ", "page_idx": 13}, {"type": "text", "text": "516 Theorem 3 (Convergence Theorem for Variational Markovian Option Policy Iteration). Let $\\tau$ be   \n517 the latent variable and $\\mathcal{E}^{A},\\mathcal{E}^{O}$ be the ground-truth optimality variables. Define the variational   \n518 distribution $q(\\tau)$ and the true log-likelihood of optimality $\\log{\\dot{P}(\\mathcal{E}^{A},\\mathcal{E}^{O})}$ . iterates according to the   \n519 update rule $q^{k+1}=\\arg\\operatorname*{max}_{q}\\mathcal{L}(q(\\tau),P(\\tau,\\mathcal{E}_{1:T}^{A},\\mathcal{E}_{1:T}^{O}))$ converges to the maximum value bounded   \n520 by the data log-likelihood.   \n521 Proof. The objective is to maximize the ELBO with respect to the policy $q$ . Formally, this can be   \n522 written as: ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "equation", "text": "$$\nq^{k+1}=\\arg\\operatorname*{max}_{q}\\mathcal{L}(q,P).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "523 Suppose we $q$ is a neural network function approximator, assuming the continuity and differentiability   \n524 of $q$ with respect to its parameters. Using stochastic gradient descent (SGD) to optimize the parameters   \n525 guarantees that the ELBO increases, such that $\\mathcal{L}(q^{\\check{k}+1},P)\\ge\\mathcal{L}(q^{k},P)$ . ", "page_idx": 14}, {"type": "text", "text": "526 Rearranging Eq. 10 we get: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D_{\\mathrm{KL}}(q^{k+1}(\\tau)||P(\\tau|\\mathcal{E}_{1:T}^{A},\\mathcal{E}_{1:T}^{O}))=-L(q^{k+1}(\\tau),P(\\tau,\\mathcal{E}_{1:T}^{A},\\mathcal{E}_{1:T}^{O}))+\\log P(\\mathcal{E}_{1:T}^{A},\\mathcal{E}_{1:T}^{O})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq-L(q^{k}(\\tau),P(\\tau,\\mathcal{E}_{1:T}^{A},\\mathcal{E}_{1:T}^{O}))+\\log P(\\mathcal{E}_{1:T}^{A},\\mathcal{E}_{1:T}^{O})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=D_{\\mathrm{KL}}(q^{k}(\\tau)||P(\\tau|\\mathcal{E}_{1:T}^{A},\\mathcal{E}_{1:T}^{O}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "527 Thus, each SGD update not only potentially increases the ELBO but also decreases the KL divergence,   \n528 moving $q$ closer to $P$ . Given the properties of SGD and assuming appropriate learning rates and   \n529 sufficiently expressive neural network architectures, the sequence $\\bar{\\{q^{k}\\}}$ converges to a policy $q^{*}$ that   \n530 minimizes the KL divergence to the true posterior. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "531 B VMOC Algorithm ", "text_level": 1, "page_idx": 14}, {"type": "table", "img_path": "eM3Wzs6Unt/tmp/1c0b8d8f614f3d34e9a202688cb05879302940e7fdd3663bf14399112cae9623.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "532 C Implementation Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "533 C.1 Hyperparameters ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "534 In this section we summarize our implementation details. For a fair comparison, all baselines:   \n535 MOPG [35], DAC $^+$ PPO [52], AHP+PPO [32], PPOC [27], OC [4] and PPO [41] are from DAC\u2019s   \n536 open source Github repo: https://github.com/ShangtongZhang/DeepRL/tree/DAC. Hyper  \n537 parameters used in DAC [52] for all these baselines are kept unchanged.   \n538 VMOC Network Architecture: We use Pytorch to build neural networks. Specifically, for option   \n539 embeddings, we use an embedding matrix $W_{S}\\,\\in\\,\\mathbb{R}^{4\\times40}$ which has 4 options (4 rows) and an   \n540 embedding size of 40 (40 columns). For layer normalization we use Pytorch\u2019s built-in function   \n541 LayerNorm 2. For Feed Forward Networks (FNN), we use a 2 layer FNN with ReLu function as   \n542 activation function with input size of state-size, hidden size of [256, 256], and output size of action  \n543 dim neurons. For Linear layer, we use built-in Linear function3to map FFN\u2019s outputs to 4 dimension.   \n544 Each dimension acts like a logit for each skill and is used as density in Categorical distribution4. For   \n545 both action policy and critic module, FFNs are of the same size as the one used in the skill policy. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "546 Preprocessing: States are normalized by a running estimation of mean and std. ", "page_idx": 15}, {"type": "text", "text": "547 Hyperparameters for all on-policy option variants: For a fair comparison, we use exactly the same   \n548 parameters of PPO as DAC . Specifically:   \n549 \u2022 Optimizer: Adam with $\\epsilon=10^{-5}$ and an initial learning rate $3\\times10^{-4}$   \n550 \u2022 Discount ratio $\\gamma$ : 0.99   \n551 \u2022 GAE coefficient: 0.95   \n552 \u2022 Gradient clip by norm: 0.5   \n553 \u2022 Rollout length: 2048 environment steps   \n554 \u2022 Optimization epochs: 10   \n555 \u2022 Optimization batch size: 64   \n556 \u2022 Action probability ratio clip: 0.2   \n557 Computing Infrastructure: We conducted our experiments on an Inte $^\\mathrm{\\textregistered}$ Core\u2122i9-9900X CPU @   \n558 3.50GHz with a single thread and process with PyTorch. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "559 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "561 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n562 paper\u2019s contributions and scope?   \n563 Answer: [Yes]   \n564 Justification: The abstract and introduction accurately reflect the claims and findings of the   \n565 paper.   \n566 Guidelines:   \n567 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n568 made in the paper.   \n569 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n570 contributions made in the paper and important assumptions and limitations. A No or   \n571 NA answer to this question will not be perceived well by the reviewers.   \n572 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n573 much the results can be expected to generalize to other settings.   \n574 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n575 are not attained by the paper. ", "page_idx": 16}, {"type": "text", "text": "576 2. Limitations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: Limitations of the study are discussed in the discussion section. ", "page_idx": 16}, {"type": "text", "text": "0 Guidelines:   \n81 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n2 the paper has limitations, but those are not discussed in the paper.   \n83 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n84 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n85 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n6 model well-specification, asymptotic approximations only holding locally). The authors   \n87 should reflect on how these assumptions might be violated in practice and what the   \n8 implications would be.   \n89 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n0 only tested on a few datasets or with a few runs. In general, empirical results often   \n91 depend on implicit assumptions, which should be articulated.   \n92 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n3 For example, a facial recognition algorithm may perform poorly when image resolution   \n4 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n95 used reliably to provide closed captions for online lectures because it fails to handle   \n96 technical jargon.   \n97 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n98 and how they scale with dataset size.   \n99 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n0 address problems of privacy and fairness.   \n01 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n02 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n03 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n04 judgment and recognize that individual actions in favor of transparency play an impor  \n05 tant role in developing norms that preserve the integrity of the community. Reviewers   \n06 will be specifically instructed to not penalize honesty concerning limitations.   \n3. Theory Assumptions and Proofs ", "page_idx": 16}, {"type": "text", "text": "607 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "608 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n609 a complete (and correct) proof? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: The paper provides a full derivation of assumptions and proofs of the theoretical result (convergence of the evidence lower bound) ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 17}, {"type": "text", "text": "624 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "25 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n26 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n27 of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: Yes. Our code is provided in supplementary materials. Full details of the experimental setup, model architectures are provided. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 17}, {"type": "text", "text": "663 5. Open access to data and code ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "664 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n665 tions to faithfully reproduce the main experimental results, as described in supplemental   \n666 material?   \n667 Answer: [Yes]   \n668 Justification: The paper provides open access to the code and data.   \n669 Guidelines:   \n670 \u2022 The answer NA means that paper does not include experiments requiring code.   \n671 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n672 public/guides/CodeSubmissionPolicy) for more details.   \n673 \u2022 While we encourage the release of code and data, we understand that this might not be   \n674 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n675 including code, unless this is central to the contribution (e.g., for a new open-source   \n676 benchmark).   \n677 \u2022 The instructions should contain the exact command and environment needed to run to   \n678 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n679 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n680 \u2022 The authors should provide instructions on data access and preparation, including how   \n681 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n682 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n683 proposed method and baselines. If only a subset of experiments are reproducible, they   \n684 should state which ones are omitted from the script and why.   \n685 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n686 versions (if applicable).   \n687 \u2022 Providing as much information as possible in supplemental material (appended to the   \n688 paper) is recommended, but including URLs to data and code is permitted.   \n689 6. Experimental Setting/Details   \n690 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n691 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n692 results?   \n693 Answer: [Yes]   \n694 Justification: Justification: All details are provided in the main content and the appendix.   \n695 Guidelines:   \n696 \u2022 The answer NA means that the paper does not include experiments.   \n697 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n698 that is necessary to appreciate the results and make sense of them.   \n699 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n700 material.   \n701 7. Experiment Statistical Significance   \n702 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n703 information about the statistical significance of the experiments?   \n704 Answer: [Yes]   \n705 Justification: All gym env experiments are run with 10 different random seeds. Performance   \n706 are reported by 1 sigma shaded area over all 10 runs.   \n707 Guidelines:   \n708 \u2022 The answer NA means that the paper does not include experiments.   \n709 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n710 dence intervals, or statistical significance tests, at least for the experiments that support   \n711 the main claims of the paper.   \n712 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n713 example, train/test split, initialization, random drawing of some parameter, or overall   \n714 run with given experimental conditions).   \n715 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n716 call to a library function, bootstrap, etc.)   \n717 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n718 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n719 of the mean.   \n720 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n721 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n722 of Normality of errors is not verified.   \n723 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n724 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n725 error rates).   \n726 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n727 they were calculated and reference the corresponding figures or tables in the text.   \n728 8. Experiments Compute Resources   \n729 Question: For each experiment, does the paper provide sufficient information on the com  \n730 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n731 the experiments?   \n732 Answer: [Yes]   \n733 Justification: Computational details are provided in the Appendix.   \n734 Guidelines:   \n735 \u2022 The answer NA means that the paper does not include experiments.   \n736 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n737 or cloud provider, including relevant memory and storage.   \n738 \u2022 The paper should provide the amount of compute required for each of the individual   \n739 experimental runs as well as estimate the total compute.   \n740 \u2022 The paper should disclose whether the full research project required more compute   \n741 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n742 didn\u2019t make it into the paper).   \n743 9. Code Of Ethics   \n744 Question: Does the research conducted in the paper conform, in every respect, with the   \n745 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n746 Answer: [Yes]   \n747 Justification: The research was conducted in accordance with the NeurIPs Code of Ethics.   \n748 Guidelines:   \n749 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n750 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n751 deviation from the Code of Ethics.   \n752 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n753 eration due to laws or regulations in their jurisdiction).   \n754 10. Broader Impacts   \n755 Question: Does the paper discuss both potential positive societal impacts and negative   \n756 societal impacts of the work performed?   \n757 Answer: [NA]   \n758 Justification: The work in the paper has no potential for societal impacts.   \n759 Guidelines:   \n760 \u2022 The answer NA means that there is no societal impact of the work performed.   \n761 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n762 impact or why the paper does not address societal impact.   \n763 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n764 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n765 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n766 groups), privacy considerations, and security considerations.   \n767 \u2022 The conference expects that many papers will be foundational research and not tied   \n768 to particular applications, let alone deployments. However, if there is a direct path to   \n769 any negative applications, the authors should point it out. For example, it is legitimate   \n770 to point out that an improvement in the quality of generative models could be used to   \n771 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n772 that a generic algorithm for optimizing neural networks could enable people to train   \n773 models that generate Deepfakes faster.   \n774 \u2022 The authors should consider possible harms that could arise when the technology is   \n775 being used as intended and functioning correctly, harms that could arise when the   \n776 technology is being used as intended but gives incorrect results, and harms following   \n777 from (intentional or unintentional) misuse of the technology.   \n778 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n779 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n780 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n781 feedback over time, improving the efficiency and accessibility of ML).   \n782 11. Safeguards   \n783 Question: Does the paper describe safeguards that have been put in place for responsible   \n784 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n785 image generators, or scraped datasets)?   \n786 Answer: [NA]   \n787 Justification: The paper poses no such risks.   \n788 Guidelines:   \n789 \u2022 The answer NA means that the paper poses no such risks.   \n790 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n791 necessary safeguards to allow for controlled use of the model, for example by requiring   \n792 that users adhere to usage guidelines or restrictions to access the model or implementing   \n793 safety filters.   \n794 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n795 should describe how they avoided releasing unsafe images.   \n796 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n797 not require this, but we encourage authors to take this into account and make a best   \n798 faith effort.   \n799 12. Licenses for existing assets   \n800 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n801 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n802 properly respected?   \n803 Answer: [Yes]   \n804 Justification: The only applicable assets are the code which are credited and distributed   \n805 under a Creative Commons Attribution License.   \n806 Guidelines:   \n807 \u2022 The answer NA means that the paper does not use existing assets.   \n808 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n809 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n810 URL.   \n811 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n812 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n813 service of that source should be provided.   \n814 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n815 package should be provided. For popular datasets, paperswithcode.com/datasets   \n816 has curated licenses for some datasets. Their licensing guide can help determine the   \n817 license of a dataset.   \n818 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n819 the derived asset (if it has changed) should be provided.   \n820 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n821 the asset\u2019s creators.   \n822 13. New Assets   \n823 Question: Are new assets introduced in the paper well documented and is the documentation   \n824 provided alongside the assets?   \n825 Answer: [Yes]   \n826 Justification: New assets include the code required to run the experiments described in the   \n827 paper. Documentation is provided along with the code.   \n828 Guidelines:   \n829 \u2022 The answer NA means that the paper does not release new assets.   \n830 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n831 submissions via structured templates. This includes details about training, license,   \n832 limitations, etc.   \n833 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n834 asset is used.   \n835 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n836 create an anonymized URL or include an anonymized zip file.   \n837 14. Crowdsourcing and Research with Human Subjects   \n838 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n839 include the full text of instructions given to participants and screenshots, if applicable, as   \n840 well as details about compensation (if any)?   \n841 Answer: [NA]   \n842 Justification: The paper does not involve crowdsourcing nor research with human subjects.   \n843 Guidelines:   \n844 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n845 human subjects.   \n846 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n847 tion of the paper involves human subjects, then as much detail as possible should be   \n848 included in the main paper.   \n849 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n850 or other labor should be paid at least the minimum wage in the country of the data   \n851 collector.   \n852 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n853 Subjects   \n854 Question: Does the paper describe potential risks incurred by study participants, whether   \n855 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n856 approvals (or an equivalent approval/review based on the requirements of your country or   \n857 institution) were obtained?   \n858 Answer: [NA]   \n859 Justification: The paper does not involve crowdsourcing nor research with human subjects.   \n860 Guidelines:   \n861 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n862 human subjects.   \n863 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n864 may be required for any human subjects research. If you obtained IRB approval, you   \n865 should clearly state this in the paper.   \n866 \u2022 We recognize that the procedures for this may vary significantly between institutions   \n867 and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the   \n868 guidelines for their institution.   \n869 \u2022 For initial submissions, do not include any information that would break anonymity (if   \n870 applicable), such as the institution conducting the review. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}]