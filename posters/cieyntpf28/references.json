{"references": [{"fullname_first_author": "Marc G Bellemare", "paper_title": "A distributional perspective on reinforcement learning", "publication_date": "2017-00-00", "reason": "This paper introduced the foundational concept of distributional reinforcement learning, which is central to the current work."}, {"fullname_first_author": "Will Dabney", "paper_title": "Distributional reinforcement learning with quantile regression", "publication_date": "2018-00-00", "reason": "This paper proposed a widely adopted approach for distributional RL using quantile regression, which is compared and contrasted with the current work."}, {"fullname_first_author": "Martin Arjovsky", "paper_title": "Wasserstein generative adversarial networks", "publication_date": "2017-00-00", "reason": "This paper introduced Wasserstein distance as a powerful tool for comparing probability distributions, which is fundamental to the understanding of the current work's use of Sinkhorn divergence."}, {"fullname_first_author": "Aude Genevay", "paper_title": "Learning generative models with Sinkhorn divergences", "publication_date": "2018-00-00", "reason": "This paper demonstrated the efficacy of Sinkhorn divergences in machine learning, which is directly applied in the current work."}, {"fullname_first_author": "Will Dabney", "paper_title": "Implicit quantile networks for distributional reinforcement learning", "publication_date": "2018-00-00", "reason": "This paper introduced the Implicit Quantile Networks (IQN), a sophisticated technique for approximating the quantile function, which is compared with the current work's approach."}]}