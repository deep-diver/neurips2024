[{"figure_path": "CiEynTpF28/figures/figures_7_1.jpg", "caption": "Figure 1: Mean (left), Median (middle), and IQM (5%) (right) of Human-Normalized Scores (HNS) summarized over 55 Atari games. We run 3 seeds for each algorithm.", "description": "This figure presents a comparison of the performance of five distributional reinforcement learning algorithms (DQN, C51, QR-DQN, MMD-DQN, and SinkhornDRL) across 55 Atari games.  The performance is measured using three different metrics: Mean, Median, and Interquartile Mean (IQM) of Human-Normalized Scores (HNS).  The x-axis represents millions of training frames, and the y-axis represents the HNS.  The figure shows learning curves for each algorithm, averaged over three independent runs (seeds).  The shaded areas around the curves represent the standard deviation.", "section": "5.1 Performance of SinkhornDRL"}, {"figure_path": "CiEynTpF28/figures/figures_7_2.jpg", "caption": "Figure 2: Ratio improvement of return for SinkhornDRL over QR-DQN (left) and MMD-DQN (right) averaged over 3 seeds. The ratio improvement is calculated by (SinkhornDRL - QR-DQN) / QR-DQN in (a) and (SinkhornDRL - MMD-DQN) / MMD-DQN in (b), respectively.", "description": "This figure shows the performance improvement of SinkhornDRL compared to QR-DQN and MMD-DQN across 55 Atari games.  The ratio improvement is calculated for each game and visualized as a bar chart. Positive values indicate that SinkhornDRL outperformed the respective baseline, while negative values indicate that the baseline performed better. The figure helps to understand which games benefit most from using SinkhornDRL.", "section": "5.1 Performance of SinkhornDRL"}, {"figure_path": "CiEynTpF28/figures/figures_7_3.jpg", "caption": "Figure 2: Ratio improvement of return for SinkhornDRL over QR-DQN (left) and MMD-DQN (right) averaged over 3 seeds. The ratio improvement is calculated by (SinkhornDRL - QR-DQN) / QR-DQN in (a) and (SinkhornDRL - MMD-DQN) / MMD-DQN in (b), respectively.", "description": "This figure shows the improvement ratio of SinkhornDRL compared to QR-DQN and MMD-DQN across 55 Atari games. The improvement is calculated as the percentage increase in the return of SinkhornDRL over each of the other algorithms, averaged over three separate runs. Positive values indicate that SinkhornDRL performed better; negative values indicate it performed worse. The figure highlights which games benefited most from the use of SinkhornDRL compared to the other algorithms.", "section": "5.1 Performance of SinkhornDRL"}, {"figure_path": "CiEynTpF28/figures/figures_8_1.jpg", "caption": "Figure 3: Sensitivity analysis of SinkhornDRL on Breakout and Seaquest in terms of \u025b, number of samples, and number of iteration L. Learning curves are reported over three seeds.", "description": "This figure shows the sensitivity analysis of the SinkhornDRL algorithm on two Atari games, Breakout and Seaquest.  It illustrates how the algorithm's performance changes as key hyperparameters are varied:  the entropic regularization strength (\u03b5), the number of generated samples (N), and the number of Sinkhorn iterations (L).  Learning curves (average return vs. millions of frames) are shown for different values of each hyperparameter, with three independent runs plotted for each configuration to show variability.", "section": "5.2 Sensitivity Analysis and Computational Cost"}, {"figure_path": "CiEynTpF28/figures/figures_8_2.jpg", "caption": "Figure 4: Performance of SinkhornDRL on six Atari games with multi-dimensional reward functions.", "description": "This figure compares the performance of SinkhornDRL and MMD-DQN on six Atari games that have been modified to include multiple reward sources.  The x-axis represents millions of frames of training, while the y-axis represents average return.  Each subplot shows a different game, and each line shows the performance of either SinkhornDRL (red) or MMD-DQN (green). The shaded regions represent standard deviations across multiple runs. The results demonstrate that SinkhornDRL generally outperforms MMD-DQN in these multi-dimensional reward settings.", "section": "5.3 Modeling Joint Return Distribution for Multi-Dimensional Reward Functions"}, {"figure_path": "CiEynTpF28/figures/figures_14_1.jpg", "caption": "Figure 5: Optimal transport plans for via Sinkhorn Iterations in SinkhornDRL on three Atari games. The first row denotes the (two-dimensional) spatial transport plans across different data points, while the second row represents the heat map of the obtained transport plan (optimal coupling).", "description": "This figure visualizes optimal transport plans obtained using Sinkhorn iterations with varying regularization strengths (epsilon) in three Atari games: Enduro, Qbert, and Seaquest.  It demonstrates how increasing epsilon leads to smoother, less concentrated, and more uniformly distributed transport plans, reflecting the effect of regularization on the transport plan.", "section": "A Smoother Transport Plan via Sinkhorn Divergence by Increasing \u03b5"}, {"figure_path": "CiEynTpF28/figures/figures_24_1.jpg", "caption": "Figure 1: Mean (left), Median (middle), and IQM (5%) (right) of Human-Normalized Scores (HNS) summarized over 55 Atari games. We run 3 seeds for each algorithm.", "description": "This figure presents a comparison of the learning curves for three different metrics (Mean, Median, and Interquartile Mean) across various distributional reinforcement learning algorithms.  The results are averaged across 55 Atari games and three different seeds (runs) for each algorithm, providing a robust and comprehensive evaluation of algorithm performance. The x-axis represents training time (in millions of frames), while the y-axis displays the Human-Normalized Scores (HNS). This visualization allows for a detailed comparison of the convergence speed and overall performance of each algorithm across different performance metrics.", "section": "5.1 Performance of SinkhornDRL"}, {"figure_path": "CiEynTpF28/figures/figures_27_1.jpg", "caption": "Figure 7: (a) Sensitivity analysis w.r.t. a small level of  SinkhornDRL to compare with QR-DQN that approximates Wasserstein distance on Breakout. (b) Sensitivity analysis w.r.t. a large level of  SinkhornDRL algorithm to compare with MMD-DQN on Breakout. All learning curves are reported over 2 seeds. (c) and (d) are results for a general  on Breakout and Seaquest, respectively.", "description": "This figure shows the sensitivity analysis of the SinkhornDRL algorithm's performance with respect to the hyperparameter epsilon (\u03b5).  The left two subfigures (a and b) compare the performance of SinkhornDRL against QR-DQN (small \u03b5) and MMD-DQN (large \u03b5) on the Breakout game. The right two subfigures (c and d) show the effect of varying \u03b5 on the Breakout and Seaquest games, respectively.  The results demonstrate that a well-tuned value for \u03b5 is crucial for optimal performance, avoiding numerical instability issues that arise when \u03b5 is too small or too large.", "section": "5.2 Sensitivity Analysis and Computational Cost"}, {"figure_path": "CiEynTpF28/figures/figures_27_2.jpg", "caption": "Figure 3: Sensitivity analysis of SinkhornDRL on Breakout and Seaquest in terms of \u025b, number of samples, and number of iteration L. Learning curves are reported over three seeds.", "description": "This figure shows the sensitivity analysis of the SinkhornDRL algorithm on two Atari games, Breakout and Seaquest.  It explores how different hyperparameters affect the algorithm's performance. Specifically, it investigates the impact of the regularization parameter (\u03b5), the number of samples used to approximate the distribution (Samples), and the number of Sinkhorn iterations (L).  Learning curves, averaged over three seeds, are presented for each hyperparameter setting, visualizing the average return over millions of frames during training.", "section": "5.2 Sensitivity Analysis and Computational Cost"}, {"figure_path": "CiEynTpF28/figures/figures_28_1.jpg", "caption": "Figure 7: (a) Sensitivity analysis w.r.t. a small level of \u03b5 SinkhornDRL to compare with QR-DQN that approximates Wasserstein distance on Breakout. (b) Sensitivity analysis w.r.t. a large level of \u03b5 SinkhornDRL algorithm to compare with MMD-DQN on Breakout. All learning curves are reported over 2 seeds. (c) and (d) are results for a general \u03b5 on Breakout and Seaquest, respectively.", "description": "This figure presents sensitivity analysis of the SinkhornDRL algorithm's performance with respect to the hyperparameter epsilon (\u03b5).  Panels (a) and (b) show comparisons against QR-DQN (small \u03b5) and MMD-DQN (large \u03b5) on the Breakout game. Panels (c) and (d) show the sensitivity of \u03b5 on Breakout and Seaquest, respectively. The results highlight the algorithm's robustness to changes in \u03b5 and reveal its interpolation behavior between Wasserstein and MMD distances.", "section": "5.2 Sensitivity Analysis and Computational Cost"}, {"figure_path": "CiEynTpF28/figures/figures_28_2.jpg", "caption": "Figure 1: Mean (left), Median (middle), and IQM (5%) (right) of Human-Normalized Scores (HNS) summarized over 55 Atari games. We run 3 seeds for each algorithm.", "description": "The figure shows the learning curves for three different metrics (Mean, Median, and IQM(5%)) of human-normalized scores (HNS) across 55 Atari games.  The curves compare the performance of five different distributional reinforcement learning algorithms (DQN, C51, QR-DQN, MMD-DQN, and SinkhornDRL). Each curve represents the average performance over three separate runs (seeds) of the algorithm.  The shading around each curve illustrates the standard deviation between the runs, indicating the variability of the algorithm\u2019s performance. This allows for a visual comparison of the stability and overall performance of the algorithms on the Atari benchmark.", "section": "5.1 Performance of SinkhornDRL"}, {"figure_path": "CiEynTpF28/figures/figures_28_3.jpg", "caption": "Figure 10: Average computational cost per 10,000 iterations of all considered distributional RL algorithm, where we select  = 10, L = 10 and the number of samples N = 200 in SinkhornDRL algorithm.", "description": "This figure compares the average computational cost per 10,000 iterations for different distributional reinforcement learning (RL) algorithms across two Atari games, Breakout and Qbert.  The algorithms compared are DQN, QR-DQN, C51, MMD-DQN, and the proposed SinkhornDRL.  For SinkhornDRL, the impact of varying the number of samples (N) is also shown, ranging from 20 to 500.  The figure illustrates the relative computational overhead of SinkhornDRL compared to the other algorithms and highlights the effect of the number of samples on its computational cost.", "section": "5.2 Sensitivity Analysis and Computational Cost"}]