[{"type": "text", "text": "Catastrophic Goodhart: regularizing RLHF with KL divergence does not mitigate heavy-tailed reward misspecification ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Thomas Kwa Drake Thomas\u2217 Adri\u00e0 Garriga-Alonso Independent / FAR Labs Anthropic FAR AI kwathomas0@gmail.com drake@anthropic.com adria@far.ai ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "When applying reinforcement learning from human feedback (RLHF), the reward is learned from data and, therefore, always has some error. It is common to mitigate this by regularizing the policy with KL divergence from a base model, with the hope that balancing reward with regularization will achieve desirable outcomes despite this reward misspecification. We show that when the reward function has light-tailed error, optimal policies under less restrictive KL penalties achieve arbitrarily high utility. However, if error is heavy-tailed, some policies obtain arbitrarily high reward despite achieving no more utility than the base model\u2013a phenomenon we call catastrophic Goodhart. We adapt a discrete optimization method to measure the tails of reward models, finding that they are consistent with light-tailed error. However, the pervasiveness of heavy-tailed distributions in many real-world applications indicates that future sources of RL reward could have heavy-tailed error, increasing the likelihood of reward hacking even with KL regularization. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Kullback-Leibler (KL) divergence constraints in reinforcement learning (RL) are employed to stay in regimes where the objective is accurate enough. Some on-policy (Schulman et al., 2015, 2017) and many off-policy (Abdolmaleki et al., 2018; Jaques et al., 2019) policy gradient algorithms employ KL constraints or penalties during optimization to prevent the policy from deviating too much from the data collection distribution. This ensures that estimates of each action\u2019s advantage are reliable enough to update the policy in a helpful way. ", "page_idx": 0}, {"type": "text", "text": "Reinforcement learning from human feedback (Christiano et al., 2017; Ziegler et al., 2020, RLHF) is a very popular method to induce desirable behavior in language models. RLHF starts with a base pre-trained model, then learns a reward function from human annotator data. Next, it trains an RL policy to maximize this reward, while penalizing high KL divergence from the policy to the base model. RLHF uses an on-policy algorithm and has accurate advantages, but the reward function is always somewhat misspecified compared to desired behavior, due to insufficient data, human biases, and other factors. ", "page_idx": 0}, {"type": "text", "text": "The main purpose of the KL penalty in RLHF is to limit the consequence of reward modeling errors by keeping the policy within a distribution similar to that on which it was trained. Ideally, in the low-KL regime the reward model\u2019s errors are small enough that it provides correct updates to the base model. Gao et al. (2023) empirically supports this view: if the KL divergence in RLHF is allowed to grow too much, with a misspecified reward, the model\u2019s performance on the true utility starts to decrease. ", "page_idx": 0}, {"type": "text", "text": "We ask: can we obtain good outcomes from misspecified reward in RLHF by controlling the KL divergence? That is, if there is some error between the true reward $V$ and the proxy reward $U$ , can the KL help us to still optimize $V?$ Using mathematical proof, we answer the question in the negative for heavy-tailed errors: there exist policies which have infinite proxy reward $U$ , but whose KL with the base model vanishes (these have undetermined $V$ ). We term this phenomenon \u201ccatastrophic Goodhart\u201d, after Goodhart\u2019s law. ", "page_idx": 1}, {"type": "text", "text": "If the misspecification errors are independent and light-tailed, the KL divergence does suffice to guarantee good outcomes. There may also be guarantees under weaker assumptions, but assumptions that intuitively seem sufficient are often not (see Section 5). ", "page_idx": 1}, {"type": "text", "text": "Possibly, other regularization schemes would guarantee good outcomes for heavy-tailed errors, but this is not just a problem of KL. We show that optimizing by conditioning on large reward $U$ has similar outcomes in light- and heavy-tailed regimes. ", "page_idx": 1}, {"type": "text", "text": "Empirically, open-source language reward models seem to be light-tailed, which does not imply light-tailed errors but suggests it (Section 4.1). However, the errors are likely not independent and, given the prevalence of heavy-tailed distributions in the real world, error in future reward models may also be heavy-tailed. In any case, the present success of RLHF with misspecified rewards cannot be explained solely by the KL regularization in its objective. ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 KL divergence and KL regularization ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Recall that KL divergence between two distributions $\\mathbf{P}$ and $\\mathrm{Q}$ is defined as ", "page_idx": 1}, {"type": "equation", "text": "$$\nD_{\\mathrm{KL}}(P\\|Q)=\\sum_{x\\in{\\mathcal{X}}}P(x)\\log\\left({\\frac{P(x)}{Q(x)}}\\right).\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "If we have two policies $\\pi,\\pi_{0}$ , we define $D_{K L}(\\pi\\lVert\\pi_{0})$ as the KL divergence between the distributions of actions taken on the states in trajectories reached by $\\pi$ . That is, if $T r(\\pi)$ is the distribution of trajectories taken by $\\pi$ , we penalize $D_{K L}(\\pi\\|\\pi_{0})\\triangleq\\mathbb{E}_{s\\in T,T\\sim T r(\\pi)}[D_{K L}(\\pi(s)\\|\\pi_{0}(s))].$ . ", "page_idx": 1}, {"type": "text", "text": "In RLHF, it is common to use the regularization term $\\beta D_{K L}\\left(\\pi\\|\\pi_{0}\\right)$ to prevent the learned policy from deviating too much from the base policy, which can prevent unstable behavior or overftiting to the reward model. If our reward model gives reward $U$ , then the optimal policy for RLHF with a KL penalty is ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\arg\\operatorname*{max}_{\\boldsymbol{\\pi}}\\mathbb{E}[U(\\boldsymbol{\\pi})]-\\beta D_{K L}\\left(\\boldsymbol{\\pi}\\Vert\\pi_{0}\\right).\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Often the regularization parameter $\\beta$ is dynamically adjusted to keep the $D_{K L}$ near some target value (Ziegler et al., 2020). ", "page_idx": 1}, {"type": "text", "text": "2.2 Heavy-tailed distributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "A distribution $P$ over $\\mathbb{R}$ with cumulative distribution function (CDF) $F_{P}$ is heavy-tailed if its tail function $\\bar{F}_{P}(x)\\triangleq1-F_{P}(x)$ satisfies ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{x\\to\\infty}e^{t x}{\\bar{F}}(x)=\\infty\\qquad{\\mathrm{for~all}}t>0.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Heavy-tailed distributions are well-known in statistics to have a higher probability of producing a single extreme value. For example, if the sum of two independent variables from heavy-tailed distributions is large, it is most likely due to one extreme sample rather than two equally large samples. (Wierman, 2013) ", "page_idx": 1}, {"type": "text", "text": "2.3 Reward misspecification and Goodhart\u2019s Law ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Reward misspecification has caused low-utility outcomes in practice; for example, in (Clark and Amodei, 2016), an RL agent trained to play a racing videogame according to a misspecified reward function achieves a high score while failing to complete the course. ", "page_idx": 1}, {"type": "text", "text": "Gao et al. (2023) introduce the concept of \u201coveroptimization\u201d: optimizing for a proxy objective decreases performance according to the true objective. This raises the question: in general, when RLHF reward is misspecified, when does the optimal policy produce high utility? ", "page_idx": 2}, {"type": "text", "text": "By applying the proxy reward and true reward functions to a distribution over text (generated by an LLM), we get two scalar random variables, which we call $U$ for proxy reward and $V$ for true reward $/$ utility. Then we can define the error in the proxy reward as $X\\triangleq U-V$ , so that $U=X+V$ . Framed this way, optimization for a proxy reward $U$ is a mix of desirable optimization for $V$ and undesirable optimization for $X$ . The joint distribution of $V$ and $X$ determines the limiting value of $V$ as we apply more optimization. When we say that reward misspecification can have negative effects, we mean that too much variance in $X$ can \"redirect\" the optimization pressure from $V$ to $X$ , and prevent utility gain from optimization. ", "page_idx": 2}, {"type": "text", "text": "Reward misspecification is also studied by (Lambert and Calandra, 2024), (Laidlaw et al., 2024), and others. Laidlaw et al show that a KL penalty between action distributions can be ineffective, and propose instead regularizing state occupancy measure. Our results show an inherent weakness of KL divergence, including when applied to state occupancy measure. ", "page_idx": 2}, {"type": "text", "text": "We prove that in many cases, $V\\rightarrow0$ in the limit of optimization for some proxy $U$ . We call this phenomenon \u201ccatastrophic Goodhart\u201d, after Goodhart\u2019s law: \u201cwhen a measure becomes a target, it ceases to be a good measure\u201d (Strathern, 1997). In these cases, the end result of optimizing for a proxy of $V$ is no better than not optimizing at all. However, in other cases, $V\\rightarrow\\infty$ despite some reward misspecification; in these cases the reward misspecification is not severe enough to prevent good outcomes. ", "page_idx": 2}, {"type": "text", "text": "3 Theoretical results ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "When applying KL regularization, the trained model is regularized towards some base policy $\\pi_{0}$ . One would hope that a KL penalty can produce good outcomes even in the case of reward misspecification; that is, if the reward $U$ is the sum of true utility $V$ and an error term $X$ , we would hope that optimal policies under a KL penalty achieve high $V$ even if the magnitude of $X$ is large. We show that this is not always the case: Corollary 1 of Theorems 1, 3, and 2 establishes that when $X(\\pi_{0})$ is heavy-tailed, there are arbitrarily well-performing policies $\\pi$ with $\\mathbb{E}_{\\pi}[V]\\approx\\mathbb{E}_{\\pi_{0}}[V]$ . However, Theorem 4 shows that when error is light-tailed and independent of $V$ , the optimal policy under a KL penalty results in $V>0$ , and $V$ can be made arbitrarily large. Thus, the tails of the error distribution are crucial in determining how much utility will result from optimization towards an imperfect proxy. ", "page_idx": 2}, {"type": "text", "text": "Theorems 5 and 6 (Section 3.4) show that the relationship of catastrophic Goodhart to heavy-tailed error is not just a quirk of KL divergence by using a different model of optimization based on conditioning on high reward values. Under this model (and given additional regularity conditions), it is also true that heavy-tailed error results in catastrophic Goodhart and light-tailed error plus independence results in arbitrarily large utility. All proofs are in the appendix. ", "page_idx": 2}, {"type": "text", "text": "3.1 KL divergence on heavy- and light-tailed distributions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Theorem 1. Given any heavy-tailed reference distribution $Q$ over $\\mathbb{R}$ with mean $\\mu_{Q}$ , and any $M,\\epsilon>0$ , there is a distribution $P$ with mean $\\mu_{P}>M$ and $D_{K L}(P\\|Q)<\\epsilon$ . ", "page_idx": 2}, {"type": "text", "text": "Outline of proof (see appendix for full proof): WLOG take $\\mu_{Q}=0$ . If we set $P_{t}$ to upweight the probability mass of $P\\overline{{r_{P_{t}}}}(X>t)$ to $c/t$ for some $c,t$ , then the mean of $P_{t}$ will be approximately at least $c$ . As $t\\to\\infty$ , the KL divergence $D_{K L}(P_{t}||Q)$ will shrink to zero. ", "page_idx": 2}, {"type": "text", "text": "Intuitively, in a heavy-tailed distribution, events with extremely high $x$ are not very rare, so you don\u2019t pay much of a KL penalty to upweight them so they happen about $1/x$ of the time. ", "page_idx": 2}, {"type": "text", "text": "Theorem 2. However, if the distribution $Q$ is light-tailed and $d=D_{K L}(P||Q)$ is bounded, then $\\mu_{P}$ is bounded, and $\\mu_{P}-\\mu_{Q}\\to0$ as $d\\to0$ . ", "page_idx": 2}, {"type": "text", "text": "3.2 RLHF with KL penalty under heavy-tailed return distribution ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We now adapt our result to the case where the policy is a language model and we are training it using RLHF. We are now applying KL divergence over the policies rather than the return distributions. ", "page_idx": 2}, {"type": "text", "text": "We first formally define the properties of RLHF on language models that cause the result to hold: namely, when when considered as a Markov decision process (MDP), environmental transitions are deterministic and return depends only on the final state reached. ", "page_idx": 3}, {"type": "text", "text": "Definition: A deterministic-transition MDP with Markovian returns (DMRMDP) is an MDP $(S,{\\mathcal{A}},P,R)$ such that: ", "page_idx": 3}, {"type": "text", "text": "\u2022 The transition function $P:S\\times A\\to S$ is deterministic, i.e., for each state $s\\in S$ and action $a\\in{\\mathcal{A}}$ , there exists a unique state $s^{\\prime}\\in\\mathcal{S}$ such that $P(s^{\\prime}|s,a)=1$ . In RLHF: the transition is appending the generated token $a$ to the context $s$ .   \n\u2022 There is a set of sink states $E\\subseteq S$ that terminate every trajectory, which is disjoint from the set of start states. In RLHF: The sink states are sequences ending in $<\\tt E O S>$ or above a certain length.   \n\u2022 Returns are Markovian; that is, for any two trajectories $\\tau\\ =\\ (s_{1},a_{1},\\dots,s_{n}),\\tau^{\\prime}\\ =$ $(s_{1}^{\\prime},a_{1}^{\\prime},\\ldots,s_{n}^{\\prime})$ , if $s_{n}\\;=\\;s_{n}^{\\prime}$ , then $\\tau$ and $\\tau^{\\prime}$ have identical return distributions. Equivalently, for the trajectory random variable $T=(S_{1},A_{1},\\dots)$ distributed according to any policy, with return $G,\\dot{G}\\bot\\!\\!\\!\\perp(S_{<i},A_{<i})\\mid S_{i}$ for any $i\\geq1$ . In RLHF: the return only depends on the full generated string, which is the final state. ", "page_idx": 3}, {"type": "text", "text": "The language model stochastically outputs the next token $a$ given $s$ , and corresponds to the policy. A DMRMDP is therefore a good model of RLHF. ", "page_idx": 3}, {"type": "text", "text": "Theorem 3. Let $W\\,=\\,({\\cal S},{\\mathcal A},P,R)$ be a deterministic-transition MDP with Markovian returns. Given $W$ we define the function that takes policies to trajectories $T r:(S\\rightarrow\\Delta A)\\rightarrow\\Delta(S\\times A)^{*}$ , and the average return function $g:(S\\times A)^{*}\\to\\mathbb{R}_{}$ , which induces a function $G:\\Delta(S\\times A)^{*}\\rightarrow\\Delta\\mathbb{R}$ . Let $\\pi_{0}\\;:\\;{\\cal S}\\;\\rightarrow\\;\\Delta{\\cal A}$ be some base policy. If $G\\circ T r(\\pi_{0})$ is heavy-tailed with finite mean $\\mu_{Q}$ , then for any $M,\\epsilon\\,>\\,0$ , there is a policy $\\pi$ with mean return $\\mathbb{E}[U|U\\,\\sim\\,G\\circ T\\dot{r}(\\pi)]\\,>\\,M$ and $\\begin{array}{r}{\\mathbb{E}_{s\\in T,T\\sim T r(\\pi)}[D_{K L}(\\pi(s)\\|\\pi_{0}(s))]<\\epsilon.}\\end{array}$ . ", "page_idx": 3}, {"type": "text", "text": "Corollary 1. Theorems 2 and 3 imply that when utility is light-tailed, reward modeling errors make the proxy reward heavy-tailed, and a policy $\\pi$ is regularized severely enough to have $K L$ divergence values approaching zero, the reward $\\mathbb{E}[U(\\pi)]$ can go to infinity while utility $\\mathbb{E}[V(\\pi)]$ approaches a value no higher than the base policy. ", "page_idx": 3}, {"type": "text", "text": "3.3 Light-tailed $^+$ independence imply $\\mathbb{E}[V]\\to\\infty$ ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Theorem 4. If $U=X{+}V$ with $X$ and $V$ both light-tailed and $V$ unbounded, and the distribution of $U$ is continuous, and $\\pi^{*}(\\beta)\\triangleq\\arg\\operatorname*{max}_{\\pi}\\mathbb{E}[U(\\pi)]-\\beta D_{K L}(\\pi,\\pi_{0}),$ , then $\\begin{array}{r}{\\operatorname*{lim}_{\\beta\\to0^{+}}\\mathbb{E}[V(\\pi^{*}(\\beta))]=\\infty}\\end{array}$ . ", "page_idx": 3}, {"type": "text", "text": "3.4 Conditioning as alternate model of optimization ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Although we think a KL divergence penalty or cap is the most realistic setting for RLHF, it is not the only model of optimization where heavy-tailedness of the error determines whether catastrophic Goodhart occurs. Consider another model of optimization where $U\\,=\\,X+V$ as before, but we simply condition on $U$ being higher than some threshold $t$ .2 Then we are interested in the quantity $\\begin{array}{r}{\\operatorname*{lim}_{t\\to\\infty}\\mathbb{E}[V|X+V\\geq\\bar{t}]}\\end{array}$ . If we slightly strengthen the heavy-tailedness and light-tailedness assumptions, heavy-tailed error results in catastrophic Goodhart, while light-tailed error results in arbitrarily high expected utility. ", "page_idx": 3}, {"type": "text", "text": "3.4.1 Conditioning with heavy-tailed error produces catastrophic Goodhart ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Theorem 5. Let $X$ and $V$ be two independent random variables with CDFs $F_{X}$ and $F_{V}$ and tail functions $\\bar{F}_{V}\\triangleq1-F_{V}$ , ${\\bar{F}}_{X}\\triangleq1-F_{X}$ such that ", "page_idx": 3}, {"type": "text", "text": "\u2022 $V$ has a finite mean. ", "page_idx": 3}, {"type": "text", "text": "\u2022 $X$ is subexponential; that is, Pr(PXr1(X+X>2x)>x) = 2 if X1, X2 are two independent samples from $X$ . This is a slightly stronger property than being heavy-tailed. ", "page_idx": 3}, {"type": "text", "text": "\u2022 The tail of $V$ is sufficiently lighter than the tail of $X$ that $\\begin{array}{r}{\\operatorname*{lim}_{t\\to\\infty}\\frac{t^{p}\\bar{F}_{V}(t)}{\\bar{F}_{X}(t)}=0}\\end{array}$ t\u00afF\u00afV (t) = 0 for some $p>1$ . ", "page_idx": 4}, {"type": "text", "text": "Then $\\begin{array}{r}{\\operatorname*{lim}_{t\\to\\infty}\\mathbb{E}[V|X+V\\,\\geq\\,t]\\,=\\,\\mathbb{E}[V],}\\end{array}$ ; that is, catastrophic Goodhart occurs in the limit of optimization for $U=X+V$ . ", "page_idx": 4}, {"type": "text", "text": "The proof is included in the appendix. It requires expressing the conditional expectation in question as $\\frac{\\int_{-\\infty}^{\\infty}v f_{V}(v)\\mathrm{Pr}(X\\!>\\!t\\!-\\!v)}{\\int_{-\\infty}^{\\infty}f_{V}(v)\\mathrm{Pr}(X\\!>\\!t\\!-\\!v)}$ , then partitioning the interval $(-\\infty,\\infty)$ into four regions and bounding the integrand in the numerator above by a different quantity in each region. ", "page_idx": 4}, {"type": "text", "text": "3.4.2 Conditioning with light-tailed error produces arbitrarily high utility ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Theorem 6. Let $X,V$ be independent random variables such that $\\begin{array}{r}{\\operatorname*{lim}_{t\\to\\infty}\\frac{\\bar{F}_{X}\\left(t+1\\right)}{\\bar{F}_{X}\\left(t\\right)}\\,=\\,0.}\\end{array}$ ) = 0. (This implies that $X$ has tails that are dominated by $e^{-c x}$ for any $c$ , though it\u2019s a slightly stronger claim because it requires that $X$ not have large jumps in the decay of its tails.) Then for any $V$ with a finite mean which has no upper bound, $\\begin{array}{r}{\\operatorname*{lim}_{t\\rightarrow\\infty}\\mathbb{E}[V|X+V>t]=\\infty}\\end{array}$ . ", "page_idx": 4}, {"type": "text", "text": "Theorem 6 generalizes a consequence of the \"Regressional Goodhart Identity\" in (Gao et al., 2023). ", "page_idx": 4}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Our theoretical results now raise the question of whether the error in reward models is heavy-tailed or light-tailed in practice. 3 If we observe the reward distribution to be light-tailed, this is a strong indication that error is light-tailed. 4 ", "page_idx": 4}, {"type": "text", "text": "To empirically test whether the reward is heavy-tailed, we consider two lines of evidence: examining the distributions directly through random sampling and temperature-1 sampling, and finding adversarial token sequences that get high rewards. We examine one small and one medium reward model that performed reasonably well on RewardBench (Lambert et al., 2023). The small model is an OpenAssistant model based on Pythia 1.4B, and the medium model is Starling 7B-alpha (Zhu et al., 2023)5. ", "page_idx": 4}, {"type": "text", "text": "For random sampling, we sample 30000 length-1024 sequences of uniformly random tokens and observe the distribution of rewards assigned by both Pythia 1.4B and Llama 7B-chat. We also use Llama 7B-chat to generate 16000 length-133 sequences at temperature 1 and observe the distribution of rewards assigned by Starling 7B-alpha. ", "page_idx": 4}, {"type": "text", "text": "Because sampling is inefficient at probing the extreme tail, we also find token sequences that optimize Starling 7B-alpha for reward. We considered Greedy Coordinate Gradient (GCG) from (Zou et al., 2023), a method used to find adversarial suffixes that circumvent jailbreaking, but decided on a faster version of GCG called Accelerated Coordinate Gradient (ACG) from (Haize Labs, 2024). See Table 4 for ACG hyperparameters. ", "page_idx": 4}, {"type": "text", "text": "Generating plots took about 5 GPU-hours on 1x Nvidia H100, and running ACG took a further 8 hours. ", "page_idx": 4}, {"type": "text", "text": "4.1 Results ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "When sampling token sequences, both the Pythia model on random inputs (Figure B.1) and Starling 7B-alpha on Llama-generated inputs (Figure 2) appear approximately normal and, therefore, lighttailed. Starling on random inputs (Figure 1 is ambiguous, with the exponential Q-Q plot having an outlier that could indicate a heavy-tailed distribution, but the Hill estimator is consistent with a light-tailed distribution. Because Llama-7B-chat is a more reasonable base model than a completely random policy, we believe that Starling 7B-alpha is more likely to be light-tailed for the purposes of our theoretical results. ", "page_idx": 4}, {"type": "table", "img_path": "UXuBzWoZGK/tmp/5248ff18ac33ca5b0534c4df23dd2b2d081904959456294a43c353e467160e45.jpg", "table_caption": [], "table_footnote": ["Table 1: Hyperparameters for ACG "], "page_idx": 5}, {"type": "image", "img_path": "UXuBzWoZGK/tmp/847399716f9de46484882760906a88f76d8eecbffdda13961b6273f0d588ef15.jpg", "img_caption": ["Plots of reward from 30000 random token sequences to Starling 7B alpha reward model ", "Figure 1: Plots of the distribution of reward from 30000 random length-1024 token sequences to Starling 7B-alpha. Clockwise from top left: The histogram shows a unimodal distribution with a slight right skew. The normal probability plot indicates the data are heavier-tailed than normal. The Hill estimator (error bars are standard error) appears to be 0.20 for higher values but fluctuates for lower values. The exponential probability plot of the right half of the distribution is consistent with either light or heavy tails (under heavy tails, the slope would go to infinity). "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "The ACG results need some interpretation. The KL divergence between two distributions $P$ and $Q$ if $P$ is the same as $Q$ a fraction $1-\\alpha$ of the time, but is some value $x$ a fraction $\\alpha$ of the time is given $\\begin{array}{r}{D_{K L}(P||Q)=[(1-\\alpha)q(x)+\\alpha]\\log\\Big(\\frac{(1-\\alpha)q(x)+\\alpha}{q(x)}\\Big)+(1-\\alpha)\\log(1-\\alpha)(1-q(x)).}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "When $\\alpha$ is small but much larger than $q(x)$ , we approximate this to first order as $D_{K L}(P\\|Q)\\approx$ $\\textstyle\\alpha\\log\\left({\\frac{\\alpha}{q(x)}}\\right)$ . In Theorems 1 and 3, we prove that when the error is sufficiently heavy-tailed, a policy that gets extremely large reward a small fraction of the time will achieve high expected reward with low KL divergence. This is not the case here because the rewards achieved through ACG were small and the log-probabilities extremely negative. For example, a policy that matches Llama 2-chat\u2019s base reward $99\\%$ of the time and uses the highest-reward input generated by ACG $\\alpha=\\!1\\%$ of the time will have KL divergence from Llama 2-chat of $\\alpha(\\log(\\alpha)-1339.70)=\\mathrm{{13.35}}$ nats, but reward only about $\\alpha*(2.2377-0.3329)=0.02571$ greater than the base model, far less than can be obtained with the same KL divergence by conditioning. ", "page_idx": 5}, {"type": "image", "img_path": "UXuBzWoZGK/tmp/1454d9d8c121583f23842222728e4688c9b38430746142c2384ed35aed1c7a85.jpg", "img_caption": ["Figure 2: Plots of the reward distribution from 16000 token sequences generated by Llama 7B-chat of length $\\leq133$ , starting with five random tokens. Clockwise from top left: A histogram shows the reward distribution has a left skew. The normal probability plot suggests reward is approximately normal and thus light-tailed. The Hill estimator plot should stabilize if the distribution is heavy-tailed, but it does not; thus, there is no evidence the distribution is heavy-tailed. The exponential probability plot also indicates light tails, because the curve is bending downwards. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "5 Discussion and Limitations ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "5.1 How likely is catastrophic Goodhart? ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The low-KL policies that result in catastrophic Goodhart are not a unique optimal policy, just one family of high-performing policies. When optimizing $\\mathbb{E}[U(\\pi)]\\!-\\!\\beta D_{K L}\\left(\\bar{\\pi},\\pi_{0}\\right)$ , the outcome depends on RL training dynamics; it could be that $D_{K L}\\rightarrow0$ causing catastrophic Goodhart, but more likely both terms will go to infinity, potentially allowing $V\\rightarrow\\infty$ . Catastrophic Goodhart can be prevented by using a light-tailed or bounded reward function. ", "page_idx": 6}, {"type": "text", "text": "Even so, catastrophic Goodhart is likely to occur in many scenarios where KL regularization is naively employed in an attempt to avoid Goodhart\u2019s Law: ", "page_idx": 7}, {"type": "text", "text": "\u2022 If we maximize $\\sigma({\\mathbb E}[U])+D_{K L}(T r(\\pi)\\|T r(\\pi_{0}))$ , where $\\sigma$ is a bounded function (e.g. sigmoid), all near-optimal policies will have $V\\approx0$ . Since we can only obtain so much reward from $\\sigma(\\mathbb{E}[U]{\\bar{\\big)}}$ , it pays to make the KL (and thus V) go to zero. \u2022 If we cap KL to a finite value (or dynamically adjust the KL penalty to target a finite KL, as done in Ziegler et al. (2020), then $\\mathbb{E}[V]$ is also upper bounded by a finite value (see Theorem 2), and we think it is likely that $\\dot{\\mathbb{E}}[V]\\approx0$ . Consider a toy model where an AI can adjust three parameters: true quality $V$ of responses, frequency of reward hacking (producing actions with extremely high X), and severity of hacking (value of X on those actions). Adjusting the policy to increase $\\mathbb{E}[U]$ without increasing KL increase the severity of hacking while decreasing either frequency of hacking or quality of responses. When $E[U]$ is already large, decreasing quality has much better returns than decreasing frequency. This is similar to Theorems 5, 6 about hard-threshold optimization. \u2022 Any way we maximize $\\mathbb{E}[U(\\pi)]-\\beta D_{K L}\\left(\\pi,\\pi_{0}\\right)$ results in very large values of $\\mathbb{E}[U(\\pi)]$ , and there are a number of arguments that extreme optimization for an imperfect proxy can result in decreased utility due to tradeoffs between $X$ and $V$ ; e.g., the constrained resource scenario in (Zhuang and Hadfield-Menell, 2021). ", "page_idx": 7}, {"type": "text", "text": "5.2 Independence assumptions ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Theorems 1-3 do not require any independence assumption, but Theorems 4, 5, and 6 require that error $X$ and utility $V$ are independent, which seems to be violated in practice. Future work could weaken this assumption, although intuitively obvious ways to weaken it result in the statement being false. 6 ", "page_idx": 7}, {"type": "text", "text": "5.3 Stronger optimization methods ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We did not search the entire space of token sequences, so we cannot rule out that the reward is heavy-tailed enough to cause catastrophic Goodhart in some situations. While it is intractable to search the more than $10^{2000}$ possible token sequences, future work could get more evidence through more powerful optimization methods. ", "page_idx": 7}, {"type": "text", "text": "5.4 Reparameterizing reward ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In some cases, a heavy-tailed reward can be reparameterized to make it light-tailed and avoid catastrophic Goodhart; however, in settings where the true reward is heavy-tailed, making reward artificially light-tailed or bounded can result in unintended behavior. ", "page_idx": 7}, {"type": "text", "text": "For example, a stock-trading agent should be rewarded by profit, but financial returns are known to be heavy-tailed. If we clip or otherwise transform rewards into a bounded interval, it will have no incentive to take into account huge gains or losses. Since RLHF rewards as implemented in Ziegler et al are unbounded, clipping or transforming rewards could itself cause reward misspecification. ", "page_idx": 7}, {"type": "text", "text": "In some cases, e.g. when the reward is not the true intended one, it is possible to reparameterize the reward without adverse effects. In the RL literature for Atari games, rewards are changes in score clipped to $[-1,1]$ (Machado et al., 2018). ", "page_idx": 7}, {"type": "text", "text": "5.5 Relation to previous overoptimization work ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Gao et al. (2023) found that optimizing the reward of small reward models causes overoptimization: a decrease in utility with increasing optimization. However, we observed that reward models are lighttailed, and (Theorem 4) that independence combined with light-tailed error prevents overoptimization. We think this discrepancy is explained by dependence between error and utility. Policies optimized for high error may activate features in the proxy reward models that are undesirable according to the true utility function.7 More research is needed to understand why high-error completions have low utility and to design reward models that do not suffer from this problem; perhaps it is possible to construct reward models whose errors are in directions orthogonal to human preferences, so that the large-reward completions do not have lower utility. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We have argued that the purpose of the KL divergence regularization in RLHF is to mitigate reward misspecification. However, we have also proven that when errors in the reward function are heavytailed, it cannot serve this purpose: even with zero KL divergence, there are policies that achieve very high misspecified reward and no actual reward. ", "page_idx": 8}, {"type": "text", "text": "When errors are light-tailed and independent, the KL divergence can mitigate misspecification, but when they are dependent, this may not be possible. Thus, we must look to places other than the KL objective to explain the current success of RLHF and ensure its continued success in the future. ", "page_idx": 8}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "This work was supported by the Long-Term Future Fund (LTFF). We also thank the anonymous reviewers for their valuable feedback and constructive suggestions. ", "page_idx": 8}, {"type": "text", "text": "Impact Statement ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "As this work aims to improve the safety of future ML systems by characterizing a possible failure mode of reward misspecification in RLHF, we hope the social impact is positive. We see no particular ethical issues to discuss. ", "page_idx": 8}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Abdolmaleki, A., Springenberg, J. T., Tassa, Y., Munos, R., Heess, N., and Riedmiller, M. (2018). Maximum a posteriori policy optimisation. CoRR.   \nChristiano, P. F., Leike, J., Brown, T., Martic, M., Legg, S., and Amodei, D. (2017). Deep reinforcement learning from human preferences. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R., editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.   \nClark, J. and Amodei, D. (2016). Faulty reward functions in the wild. Accessed: 2024-07-07.   \nFoss, S., Korshunov, D., and Zachary, S. (2013). An Introduction to Heavy-Tailed and Subexponential Distributions. Springer, 2 edition.   \nGao, L., Schulman, J., and Hilton, J. (2023). Scaling laws for reward model overoptimization. In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J., editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 10835\u201310866. PMLR.   \nHaize Labs (2024). Making a sota adversarial attack on llms $38\\mathrm{x}$ faster. https://blog.haizelabs.com/ posts/acg/. Accessed: 2024-05-22.   \nHu, J., Wu, X., Wang, W., Xianyu, Zhang, D., and Cao, Y. (2024). Openrlhf: An easy-to-use, scalable and high-performance rlhf framework. arXiv preprint arXiv:2405.11143.   \nJaques, N., Ghandeharioun, A., Shen, J. H., Ferguson, C., Lapedriza, A., Jones, N., Gu, S., and Picard, R. (2019). Way off-policy batch deep reinforcement learning of implicit human preferences in dialog. arXiv preprint arXiv:1907.00456. ", "page_idx": 8}, {"type": "image", "img_path": "UXuBzWoZGK/tmp/252fbf706f88cb684d610e2e09429fb1654a447519d303e93e67a16583c02bce.jpg", "img_caption": ["Figure A.1: As $t\\,\\rightarrow\\,\\infty$ , the mean of $X$ (blue bar) grows without bound while KL divergence $\\bar{D_{K L}}(P_{t}\\parallel Q)$ (orange bar) goes to 0. The base distribution Q is a Student t-distribution with $d f=3$ In this case, high values of X are upweighted to $1/t^{0.8}$ ; upweighting them to $1/t$ would cause $\\mathbb{E}[X]$ to converge to 1 while KL divergence goes to zero faster. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Laidlaw, C., Singhal, S., and Dragan, A. (2024). Preventing reward hacking with occupancy measure regularization. arXiv. Accessed: 2024-07-07. ", "page_idx": 9}, {"type": "text", "text": "Lambert, N. and Calandra, R. (2024). The alignment ceiling: Objective mismatch in reinforcement learning from human feedback. ", "page_idx": 9}, {"type": "text", "text": "Lambert, N., Pyatkin, V., Morrison, J., Miranda, L., Lin, B. Y., Chandu, K., Dziri, N., Kumar, S., Zick, T., Choi, Y., Smith, N. A., and Hajishirzi, H. (2023). Rewardbench: Evaluating reward models for language modeling. 40 pages, 19 figures, 12 tables. ", "page_idx": 9}, {"type": "text", "text": "Machado, M. C., Bellemare, M. G., Talvitie, E., Veness, J., Hausknecht, M., and Bowling, M. (2018). Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents. Journal of Artificial Intelligence Research, 61:523\u2013562. ", "page_idx": 9}, {"type": "text", "text": "Schulman, J., Levine, S., Moritz, P., Jordan, M. I., and Abbeel, P. (2015). Trust region policy optimization. CoRR. ", "page_idx": 9}, {"type": "text", "text": "Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. (2017). Proximal policy optimization algorithms. CoRR. ", "page_idx": 9}, {"type": "text", "text": "Strathern, M. (1997). \u2018improving ratings\u2019: audit in the british university system. European Review, 5(3):305\u2013321. ", "page_idx": 9}, {"type": "text", "text": "Wierman, A. (2013). Catastrophes, conspiracies, and subexponential distributions (part ii). https://rigorandrelevance.wordpress.com/2013/12/17/ catastrophes-conspiracies-and-subexponential-distributions-part-ii/. Accessed: 2024-06-26. ", "page_idx": 9}, {"type": "text", "text": "Zhu, B., Frick, E., Wu, T., Zhu, H., and Jiao, J. (2023). Starling-7B: Improving llm helpfulness & harmlessness with rlaif. ", "page_idx": 9}, {"type": "text", "text": "Zhuang, S. and Hadfield-Menell, D. (2021). Consequences of misaligned AI. Advances in Neural Information Processing Systems, 33:15762\u201315773. arXiv:2102.03896v1 [cs.AI]. ", "page_idx": 9}, {"type": "text", "text": "Ziegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Radford, A., Amodei, D., Christiano, P., and Irving, G. (2020). Fine-tuning language models from human preferences. arXiv:1909.08593v2 [cs.CL]. ", "page_idx": 9}, {"type": "text", "text": "Zou, A., Wang, Z., Kolter, J. Z., and Fredrikson, M. (2023). Universal and transferable adversarial attacks on aligned language models. ", "page_idx": 9}, {"type": "text", "text": "A Proofs ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "A.1 Theorem 1 ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Restatement of Theorem 1. Given any heavy-tailed reference distribution $Q$ over $\\mathbb{R}$ with mean $\\mu_{Q}$ , and any $M,\\epsilon>0,$ , there is a distribution $P$ with mean $\\mu_{P}>M$ and $D_{K L}(P||Q)<\\epsilon$ . ", "page_idx": 9}, {"type": "text", "text": "Intuitively, in a heavy-tailed distribution, events with extremely high $x$ are not very rare, so you don\u2019t pay much of a $\\mathrm{KL}$ penalty to upweight them so they happen about $1/x$ of the time. This is visually illustrated in Figure A.1. ", "page_idx": 9}, {"type": "text", "text": "Proof. WLOG let $\\mu_{Q}=0$ . We construct a sequence of distributions $\\{P_{t}\\}$ such that $\\begin{array}{r}{\\operatorname*{lim}_{t\\to\\infty}\\mathbb{E}_{P_{t}}[X]\\geq c}\\end{array}$ for any constant $c$ , and $\\begin{array}{r}{\\operatorname*{lim}_{t\\to\\infty}D_{K L}(P_{t}\\|Q)=0}\\end{array}$ . We define $P_{t}$ for any $t>c$ thusly. Writing $F_{P_{t}}(x)$ for the CDF $P r_{X\\sim P_{t}}(X\\leq x)$ and $\\bar{F}_{P_{t}}(x)$ for $1-F_{P_{t}}(x)$ , we let ", "page_idx": 10}, {"type": "equation", "text": "$$\n\\bar{F}_{-}\\{P_{-}t\\}(x)=\\left\\{\\!\\!\\begin{array}{l l}{{1-\\frac{1-c/t}{F_{Q}(t)}F_{Q}(x)}}&{{x\\leq t}}\\\\ {{\\frac{c/t}{\\bar{F}_{Q}(t)}\\bar{F}_{Q}(x)}}&{{x>t}}\\end{array}\\!\\right.\n$$", "text_format": "latex", "page_idx": 10}, {"type": "text", "text": "Intuitively, we rescale the part of the distribution to the right of $t$ evenly to have total probability $c/t$ , which is less than 1 because $t>c$ . ", "page_idx": 10}, {"type": "text", "text": "We must check that $\\operatorname*{lim}_{t\\to\\infty}\\mathbb{E}_{P_{t}}[X]=c$ . We can write ", "page_idx": 10}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{P_{t}}[X]=F_{P_{t}}(t)\\mathbb{E}_{P_{t}}[X|X\\leq t]+\\bar{F}_{P_{t}}(t)\\mathbb{E}_{P_{t}}[X|X>t]}\\\\ &{\\quad\\quad\\quad=F_{P_{t}}(t)\\mathbb{E}_{Q}[X|X\\leq t]+\\bar{F}_{P_{t}}(t)\\mathbb{E}_{Q}[X|X>t]}\\\\ &{\\quad\\quad\\quad=F_{Q}(t)\\mathbb{E}_{Q}[X|X\\leq t]+\\bar{F}_{Q}(t)\\mathbb{E}_{Q}[X|X>t]+}\\\\ &{\\quad\\quad\\quad\\quad\\quad(F_{P_{t}}(t)-F_{Q}(t))E_{Q}[X|X\\leq t]+(\\bar{F}_{P_{t}}(t)-\\bar{F}_{Q}(t))E_{Q}[X|X>t]}\\\\ &{\\quad\\quad\\quad=\\mathbb{E}_{Q}[X]+(\\bar{F}_{P_{t}}(t)\\!-\\!\\bar{F}_{Q}(t))(E_{Q}[X|X>t]-E_{Q}[X|X\\leq t])}\\end{array}\n$$", "text_format": "latex", "page_idx": 10}, {"type": "text", "text": "We know that $\\mathbb{E}_{Q}[X|X>t]>t$ because it is an integral of values strictly greater than t. Because $\\mathbb{E}_{Q}[X]=0$ is a weighted average of $\\mathbb{E}_{Q}[X|X>t]$ and $E_{Q}[X|X\\leq t]$ , and $\\mathbb{E}_{Q}[X|X>t]>0$ , we know $E_{Q}[X|X\\leq t]<0$ . So $E_{Q}[X|X>t]-E_{Q}[X|X\\leq t]>t$ . We also know that for sufficiently large $t$ , $\\left(F_{P_{t}}(t)-F_{Q}(t)\\right)>0$ . Intuitively, starting from $Q$ , which has mean $_0$ , $P_{t}$ moves a probability mass approaching $\\scriptstyle{\\frac{c}{t}}$ from mean ${<}0$ to mean ${\\sf{>}}{\\sf{t}}$ . ", "page_idx": 10}, {"type": "text", "text": "Now we can say ", "page_idx": 10}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{t\\to\\infty}\\mathbb{E}_{P_{t}}[X]>\\operatorname*{lim}_{t\\to\\infty}\\left[\\mathbb{E}_{Q}[X]+(\\bar{F}_{P_{t}}(t)-\\bar{F}_{Q}(t))(t-0)\\right]\\,=\\operatorname*{lim}_{t\\to\\infty}\\left(\\frac{c}{t}-\\bar{F}_{Q}(t)\\right)t=\\operatorname*{lim}_{t\\to\\infty}c-t\\bar{F}_{Q}(t)\n$$", "text_format": "latex", "page_idx": 10}, {"type": "text", "text": "Because $Q$ has a finite mean, $\\begin{array}{r}{\\operatorname*{lim}_{t\\to\\infty}t\\bar{F}_{Q}(t)=0}\\end{array}$ , and so $\\begin{array}{r}{\\operatorname*{lim}_{t\\to\\infty}\\mathbb{E}_{P_{t}}[X]\\geq c}\\end{array}$ . ", "page_idx": 10}, {"type": "text", "text": "Now we check that $\\begin{array}{r}{\\operatorname*{lim}_{t\\to\\infty}D_{K L}(P_{t}||Q)=0}\\end{array}$ ", "page_idx": 10}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}{D_{K L}(P_{t}||Q)=\\displaystyle\\int_{\\mathbb{R}}\\log{\\frac{P_{t}(d x)}{Q(d x)}}\\,P_{t}(d x)}\\\\ {=\\displaystyle\\int_{x\\leq t}\\log{\\frac{P_{t}(d x)}{Q(d x)}}\\,P_{t}(d x)+\\int_{x>t}\\log{\\frac{P_{t}(d x)}{Q(d x)}}\\,P_{t}(d x)}\\\\ {=F_{P_{t}}(t)\\log{\\frac{F_{P_{t}}(t)}{F_{Q}(t)}}+{\\bar{F}}_{P_{t}}(t)\\log{\\frac{{\\bar{F}}_{P_{t}}(t)}{{\\bar{F}}_{Q}(t)}}\\quad{\\mathrm{since~both~ratios~are~constant}}}\\\\ {=F_{P_{t}}(t)\\log{\\frac{1-c/t}{F_{Q}(t)}}+{\\bar{F}}_{P_{t}}(t)\\log{\\frac{{\\bar{F}}_{P_{t}}(t)}{{\\bar{F}}_{Q}(t)}}}\\end{array}}\n$$", "text_format": "latex", "page_idx": 10}, {"type": "text", "text": "Since both $1-c/t$ and $F_{Q}(t)$ go to $1$ as $t\\to\\infty$ , the left term goes to $0$ , and so ", "page_idx": 10}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\operatorname*{lim}_{t\\to\\infty}D_{K L}(P_{t}\\|Q)\\leq0+\\displaystyle\\operatorname*{lim}_{t\\to\\infty}\\bar{F}_{P_{t}}(t)\\log\\frac{\\bar{F}_{P_{t}}(t)}{\\bar{F}_{Q}(t)}}&{}\\\\ {\\displaystyle=\\operatorname*{lim}_{t\\to\\infty}\\frac{c}{t}\\log\\frac{c}{t\\bar{F}_{Q}(t)}\\leq\\displaystyle\\operatorname*{lim}_{t\\to\\infty}\\frac{c}{t}\\log\\frac{1}{\\bar{F}_{Q}(t)}}&{}\\\\ {\\displaystyle=\\operatorname*{lim}_{t\\to\\infty}-\\frac{c}{t}\\log\\bar{F}_{Q}(t)}&{\\mathrm{since~t\\mathrm{sc}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 10}, {"type": "text", "text": "$Q$ is heavy-tailed, so by definition $\\begin{array}{r}{\\operatorname*{lim}_{t\\to\\infty}e^{a t}\\bar{F}_{Q}(\\underline{{t}})=\\infty}\\end{array}$ for all $a>0$ . This implies that for every $a>0$ there is a sufficiently large $t_{c}$ so that for all $t>t_{c}$ , $\\overleftarrow{F}_{Q}(x)>e^{-a t}$ , which means that $\\log\\bar{F}_{Q}(t)>-a\\dot{t}$ . ", "page_idx": 10}, {"type": "text", "text": "Therefore for every $a>0$ $\\begin{array}{r}{0,\\operatorname*{lim}_{t\\to\\infty}D_{K L}(P_{t}||Q)\\leq\\operatorname*{lim}_{t\\to\\infty}-c/t\\log\\bar{F}_{Q}(t)<\\operatorname*{lim}_{t\\to\\infty}-\\frac{-a c t}{t}=a c}\\end{array}$ , which since KL divergence is nonnegative means thatl $\\mathrm{im}_{t\\rightarrow\\infty}\\,D_{K L}(P_{t}\\|Q)=0$ as desired. \u25a0 ", "page_idx": 10}, {"type": "text", "text": "A.2 Theorem 2 ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "Restatement of Theorem 2. If $V$ is light-tailed, $\\mathbb{E}_{Q}[V]$ is zero, and $d=D_{K L}(P||Q)$ is bounded, then $\\mathbb{E}_{P}[V]$ is bounded, and $\\mathbb{E}_{P}[V]\\rightarrow0$ as $d\\to0$ . ", "page_idx": 11}, {"type": "text", "text": "Proof. Using Lagrange multipliers, we find that when KL divergence is minimized, we have $\\begin{array}{r}{P(V)[\\lambda_{1}\\log{\\frac{P(V)}{Q(V)}}+}\\end{array}$ $\\lambda_{2}-X]=0$ for some constants $\\lambda_{1},\\lambda_{2}$ , so ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\log\\frac{P(V)}{Q(V)}=\\frac{V-\\lambda_{2}}{\\lambda_{1}}}\\\\ {P(V)=Q(V)\\exp\\left(\\frac{V-\\lambda_{2}}{\\lambda_{1}}\\right)=Q(V)}\\\\ {e^{V/\\lambda_{-}1}e^{-\\lambda_{-}2/\\lambda_{-}1}=C Q(V)e^{V/\\lambda_{-}1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "That is, the new PDF is an exponential tilting of the old PDF. Now, what is $\\mathbb{E}_{P}[V]?$ It\u2019s just $\\textstyle\\int_{-\\infty}^{\\infty}C V e^{V/\\lambda_{1}}Q(X)\\,d V$ . If the distribution of $\\mathrm{v}$ is heavy-tailed distribution, this is $\\infty$ ; if it is light-tailed, this is some finite value. ", "page_idx": 11}, {"type": "text", "text": "When $d=0$ , $P$ and $Q$ are identical, and $\\mathbb{E}[V]=0$ . So by a continuity argument, $\\mathbb{E}_{P}[V]\\rightarrow0$ as $d\\to0$ . \u25a0 ", "page_idx": 11}, {"type": "text", "text": "A.3 Theorem 3 ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "Restatement of Theorem 3. Let $W=(S,{\\mathcal{A}},P,R)$ be a deterministic-transition MDP with Markovian returns. Given $W$ , we define the function that takes policies to trajectories $T r:(S\\rightarrow\\Delta A)\\rightarrow\\Delta(S\\times A)^{*}$ , and the average return function $g:(S\\times A)^{*}\\to{\\mathbb R}$ which induces a function $G:\\Delta(S\\times A)^{*}\\to\\Delta\\mathbb{R}$ . Let $\\pi_{0}:{\\mathcal{S}}\\rightarrow\\Delta{\\mathcal{A}}$ be some base policy. If $G\\circ T r(\\pi_{0})$ is heavy-tailed with finite mean $\\mu_{Q}$ , then for any $M,\\epsilon>0$ , there is a policy $\\pi$ with mean return $\\mathbb{E}[U|U\\sim G\\circ T r(\\pi)]>M$ and $\\begin{array}{r}{\\mathbb{E}_{s\\in T,T\\sim T r(\\pi)}[D_{K L}(\\pi(s)\\|\\pi_{0}(s))]<\\epsilon.}\\end{array}$ . ", "page_idx": 11}, {"type": "text", "text": "Proof: We will exhibit a distribution of trajectories $\\rho$ such that $D_{K L}(\\rho||T r(\\pi_{0}))<\\epsilon$ and $\\mathbb{E}[G(\\rho)]>M$ , and then construct a policy $\\pi$ with $T r(\\pi)=\\rho$ . Note that this proof applies for continuous action spaces if trajectories are replaced with measurable sets, but this would make it harder to read. ", "page_idx": 11}, {"type": "text", "text": "Let $\\rho_{\\pi_{0}}\\,=\\,T r(\\pi_{0})$ . We have a heavy-tailed distribution of return $Q\\,\\triangleq\\,G(\\rho_{\\pi_{0}})$ over $\\mathbb{R}$ , so we can apply Theorem 1. But to define $\\rho$ , we can construct $P_{t}$ in the proof of Theorem 1 in a particular way. For any $t>c$ , we need a $P_{t}$ that uniformly upweights values of mean return such that $\\bar{F}_{P_{t}}(t)\\stackrel{\\cdot}{=}c/t$ . We can define $\\rho_{t}$ such that any trajectory $\\tau$ is upweighted by a factor depending only on its mean return: ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\rho_{t}(\\tau)=\\left\\{\\frac{1\\!-\\!c/t}{F_{Q}(t)}\\rho_{\\pi_{0}}(\\tau)\\quad g(\\tau)\\leq t\\right.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "Then we can let $P_{t}\\triangleq G\\circ\\rho_{t}$ and the rest of the proof of Theorem 1 applies. Therefore, applying the theorem, we can let $\\rho=\\rho_{t}$ for sufficiently large $t$ , and then $\\mu_{G o\\rho}>M$ and $D_{K L}\\big(G\\circ\\rho,G\\circ\\rho_{\\pi_{0}}\\big)<\\epsilon.$ By the chain rule for KL divergence, $D_{K L}(\\rho,\\rho_{\\pi_{0}})=D_{K L}(G\\circ\\rho,G\\circ\\rho_{\\pi_{0}})+\\mathbb{E}_{\\gamma\\sim G\\circ\\rho}[D_{K L}(\\rho_{K L}(\\rho_{K L}),G\\circ\\rho_{\\pi_{0}})].$ $\\\"\\gamma\\sim_{G\\circ\\rho}[D_{K L}(\\rho(T)|G(T)=\\gamma\\parallel\\rho_{\\pi_{0}}(T)|G(T)=\\gamma)]$ . Since we constructed $\\rho$ so that the probabilities of each $\\tau$ conditional on its return being $\\gamma$ are equal, the second term is zero, and we also have $\\bar{D_{K L}}(\\rho,\\rho_{\\pi_{0}})<\\epsilon$ . ", "page_idx": 11}, {"type": "text", "text": "Finally, since the KL divergence between trajectory distributions is the sum of $\\mathrm{KL}$ divergence between policies at each action in the trajectory, and each trajectory has at least one action, $\\begin{array}{r l}{\\lefteqn{\\mathbb{E}_{s\\in T,T\\sim T r(\\pi)}[D_{K L}(\\pi(s)\\|\\pi_{0}\\bar{(}s))]\\le}}\\end{array}$ $\\begin{array}{r}{\\mathbb{E}_{T\\sim T r(\\pi)}\\sum_{s\\in T}[D_{K}\\bar{\\mathbf{\\alpha}}(\\pi(s)||\\pi_{0}(s))]=\\bar{D}_{K L}\\(\\rho||\\rho_{\\pi_{0}})<\\epsilon}\\end{array}$ as desired. ", "page_idx": 11}, {"type": "text", "text": "To define $\\pi$ such that $T r(\\pi)=\\rho$ , we let $\\pi(s,a)=P r(a_{i}=a|\\tau=(...,s,a_{i},...)\\sim\\rho).$ . ", "page_idx": 11}, {"type": "text", "text": "Then, the probability that any trajectory $\\tau=(s_{1},a_{1},\\ldots,a_{n})$ is sampled is: ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle T r(\\pi)(\\tau)=\\prod_{i=1}^{n}\\pi(s_{i},a_{i})}}\\\\ {{\\displaystyle=\\prod_{i=1}^{n}P r(a_{i}=a_{i}^{\\prime}|\\tau^{\\prime}=(...,s,a_{i}^{\\prime},...)\\sim\\rho)}}\\\\ {{\\displaystyle=\\prod_{i=1}^{n}P r(a_{i}=a_{i}^{\\prime}|\\tau^{\\prime}=(s_{1}^{\\prime},a_{1}^{\\prime},...,s,a_{i}^{\\prime},...)\\sim\\rho,s_{<i}=s_{<i}^{\\prime},a_{<i}=a_{<i}^{\\prime})}}\\\\ {{\\displaystyle=\\rho(\\tau)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "In (2), returns are Markovian, so all trajectory prefixes ending in state $s$ have the same distribution of returns under any policy. In the construction of $\\rho$ , all trajectories with the same mean return have equal measure. Therefore, conditioning on earlier states and actions of $\\tau$ does not change the measure, so we can write (3). So $T r(\\pi)=\\rho$ as desired. \u25a0 ", "page_idx": 12}, {"type": "text", "text": "A.4 Theorem 4 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Restatement of Theorem 4. If $U=X+V$ with $X$ and $V$ both light-tailed and $V$ unbounded, and the distribution of $U$ is continuous, and $\\pi^{*}(\\beta)\\triangleq\\arg\\operatorname*{max}_{\\pi}\\mathbb{E}[U(\\pi)]-\\beta D_{K L}(\\pi,\\pi_{0}).$ , then $\\begin{array}{r}{\\operatorname*{lim}_{\\beta\\to0^{+}}\\mathbb{E}[V(\\pi^{*}(\\beta))]=\\infty}\\end{array}$ . ", "page_idx": 12}, {"type": "text", "text": "Proof. Fix some $\\beta$ . Using Lagrange multipliers, we find that for any event $S$ , $\\mathrm{Pr}_{\\pi}(S)=\\mathrm{Pr}_{\\pi_{0}}(S)e^{\\lambda U(S)}$ . Let $c(\\beta)$ be the median value of $U$ under the policy $\\pi^{*}(\\beta)$ ; that is, $\\begin{array}{r}{P r(U>c(\\beta)|U\\sim G\\circ T r(\\pi^{*}(\\beta)))=\\frac{1}{2}}\\end{array}$ . This exists because $U$ has a continuous distribution. Then: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\cal E}[V|\\pi]=\\frac{1}{2}{\\cal E}[V|\\pi,U<c]+\\frac{1}{2}{\\cal E}[V|\\pi,U\\geq c]}}\\\\ {{\\displaystyle\\geq\\frac{1}{2}{\\cal E}[V|\\pi,U<c]+\\frac{1}{2}{\\cal E}[V|\\pi]}}\\\\ {{\\displaystyle\\operatorname*{lim}_{\\beta\\rightarrow0^{+}}{\\cal E}[V|\\pi]\\geq\\operatorname*{lim}_{\\beta\\rightarrow0^{+}}\\frac{1}{2}{\\cal E}[V|\\pi,U<c]+\\operatorname*{lim}_{\\beta\\rightarrow0^{+}}\\frac{1}{2}{\\cal E}[V|\\pi]}}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "The left term is $c$ , while the right term is $\\infty$ , so the overall limit is $\\infty$ . ", "page_idx": 12}, {"type": "text", "text": "A.5 Theorem 5 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Restatement of theorem 5. Let $X$ and $V$ be two independent random variables with CDFs $F_{X}$ and $F_{V}$ and tail functions $\\tilde{F}_{V}\\triangleq1-F_{V}$ , ${\\bar{F}}_{X}\\triangleq1-F_{X}$ such that ", "page_idx": 12}, {"type": "text", "text": "\u2022 $V$ has a finite mean. ", "page_idx": 12}, {"type": "text", "text": "\u2022 $X$ is subexponential; that is, Pr(PXr1(X+X>2x)>x) = 2 if X1, X2 are two independent samples from $X$ . This is a slightly stronger property than being heavy-tailed. \u2022 The tail of $V$ is sufficiently lighter than the tail of $X$ that $\\begin{array}{r}{\\operatorname*{lim}_{t\\to\\infty}\\frac{t^{p}\\bar{F}_{V}(t)}{\\bar{F}_{X}(t)}=0.}\\end{array}$ for some $p>1$ . ", "page_idx": 12}, {"type": "text", "text": "Then $\\begin{array}{r}{\\operatorname*{lim}_{t\\to\\infty}\\mathbb{E}[V|X+V\\geq t]=\\mathbb{E}[V]}\\end{array}$ ; that is, catastrophic Goodhart occurs in the limit of optimization for $U=X+V$ . ", "page_idx": 12}, {"type": "text", "text": "The proof requires expressing the conditional expectation in question as \u2212\u2212\u221e\u221e\u221efVV  (v)Pr(X>t\u2212v) , then partitioning the interval $(-\\infty,\\infty)$ into four regions and bounding the integrand in the numerator above by a different quantity in each region. ", "page_idx": 12}, {"type": "text", "text": "In addition to the works cited in the main paper, we make reference to the textbook (Foss et al., 2013) throughout the proof. Many similar results about random variables are present in the textbook. ", "page_idx": 12}, {"type": "table", "img_path": "UXuBzWoZGK/tmp/67a2b41908dcdb91899d0ff17cde66fefaaa315b178edfc179ccc8a606d3c2f7.jpg", "table_caption": [], "table_footnote": ["Table A.1: A summary of the proof strategy for Theorem 5. "], "page_idx": 13}, {"type": "image", "img_path": "UXuBzWoZGK/tmp/1d09681754a978a950e29a38e34eaea59140fbfc719da141a406cd84c30a062d.jpg", "img_caption": ["Figure A.2: A diagram showing the region boundaries at $-h(t),h(t)$ , and $t-h(t)$ in an example where $t=25$ and $h(t)=4$ , along with a negative log plot of the relevant distribution: "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "A.5.1 Proof sketch and intuitions ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The conditional expectation E[V |X + V > t] is given by  \u2212\u2212\u221e\u221e\u221e\u221evffVV  ((vv))PPrr((XX>>tt\u2212\u2212vv)) , 8 and we divide the integral in that of the corresponding region in the unconditional expectation $\\mathbb{E}[V]$ . ", "page_idx": 13}, {"type": "text", "text": "The regions are defined in terms of a slow-growing function $h(t):\\mathbb{R}\\rightarrow\\mathbb{R}{\\geq}0$ such that the fiddly bounds on different pieces of the proof work out. Roughly, we want it to go to infinity so that $|V|$ is likely to be less than $h(t)$ in the limit, but grow slowly enough that the shape of $V$ \u2019s distribution within the interval $[-h(t),h(t)]$ doesn\u2019t change much after conditioning. ", "page_idx": 13}, {"type": "text", "text": "In Table A.5.1, we abbreviate the condition $X+V>t$ as $c$ . ", "page_idx": 13}, {"type": "text", "text": "Note that up to a constant vertical shift of normalization, the green curve is the pointwise sum of the blue and orange curves. ", "page_idx": 13}, {"type": "text", "text": "A.5.2 Definitions ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "To be more precise, we\u2019re going to make the following definitions and assumptions: ", "page_idx": 13}, {"type": "text", "text": "Let $f_{V}(v)$ be the PDF of $V$ at the value $v$ . We assume for convenience that $f_{V}$ exists, is integrable, etc, though we suspect that this isn\u2019t necessary, and that one could work through a similar proof just referring to the tails of $V$ . We won\u2019t make this assumption for $X$ . Let $F_{X}(x)=\\operatorname*{Pr}(X\\leq x)$ and ${\\bar{F}}_{X}{\\hat{(}}x)=\\operatorname*{Pr}(X>x{\\bar{)}}$ , similarly for $F_{V}$ and $\\bar{F}_{V}$ . Assume that ", "page_idx": 14}, {"type": "text", "text": "\u2022 $V$ has a finite mean: $\\begin{array}{r}{\\int_{-\\infty}^{\\infty}v f_{V}(v)\\,d v}\\end{array}$ converges absolutely.   \n\u2022 $X$ is subexponential. ", "page_idx": 14}, {"type": "text", "text": "Formally, this means that Pr(PXr1(X+X>2x)>x) = 2. This occurs roughly whenever X has tails that are heavier than $e^{-c x}$ for any $c$ and is reasonably well-behaved; counterexamples to the claim \"long-tailed implies subexponential\" exist, but they\u2019re nontrivial to exhibit. Examples of subexponential distributions include lognormal distributions, anything that decays like a power law, the Pareto distribution,and distributions with tails asymptotic to $e^{-x^{a}}$ for any $0<a<1$ . ", "page_idx": 14}, {"type": "text", "text": "We require for $V$ that its tail function is substantially lighter than X\u2019s, namely that $\\begin{array}{r}{\\operatorname*{lim}_{t\\to\\infty}\\frac{t^{p}\\bar{F}_{V}(t)}{\\bar{F}_{X}(t)}=0}\\end{array}$ for some $p>1$ . (This implies that $\\bar{F}_{V}(t)=O(\\bar{F}_{X}(t)/t).$ .) ", "page_idx": 14}, {"type": "text", "text": "With these definitions and assumptions, we can move on to the proof. ", "page_idx": 14}, {"type": "text", "text": "The unnormalized PDF of $V$ conditioned on $X+V\\geq t$ is given by $f_{V}(v)\\bar{F}_{X}(t-v)$ . Its expectation is given by $\\frac{\\int_{-\\infty}^{\\infty}\\,v\\,f_{V}\\left(v\\right)\\bar{F}_{X}\\left(t-v\\right)}{\\int_{-\\infty}^{\\infty}\\,f_{V}\\left(v\\right)\\bar{F}_{X}\\left(t-v\\right)}$ . ", "page_idx": 14}, {"type": "text", "text": "Meanwhile, the unconditional expectation of $\\mathrm{v}$ is given by $\\scriptstyle\\int_{-\\infty}^{\\infty}v f_{V}(v)$ . ", "page_idx": 14}, {"type": "text", "text": "We\u2019d like to show that these two expectations are equal in the limit for large $t$ . To do this, we\u2019ll introduce $\\begin{array}{r}{Q(v)=\\frac{\\bar{F}_{X}\\left(t-v\\right)}{\\bar{F}_{X}\\left(t\\right)}}\\end{array}$ = F\u00afFX\u00afX(t(\u2212t)v ). (More pedantically, this should really be Qt(v), which we\u2019ll occasionally use where it\u2019s helpful to remember that this is a function of $t$ .) ", "page_idx": 14}, {"type": "text", "text": "For a given value of $t$ , $Q(v)$ is just a scaled version of $\\bar{F}_{X}(t-v)$ , so the conditional expectation of $V$ is given by $\\frac{\\int_{-\\infty}^{\\infty}v\\,f_{V}(v)Q(v)}{\\int_{-\\infty}^{\\infty}f_{V}(v)Q(v)}$ . But because $Q(0)=1$ , the numerator and denominator of this fraction are (for small $v$ ) close to the unconditional expectation and 1, respectively. ", "page_idx": 14}, {"type": "text", "text": "We\u2019ll aim to show that for all $\\epsilon>0$ , we have for sufficiently large $t$ that $\\begin{array}{r}{\\left|\\int_{-\\infty}^{\\infty}v f_{V}(v)Q_{t}(v)-\\int_{-\\infty}^{\\infty}v f_{V}(v)\\right|<}\\end{array}$ $\\epsilon$ and $\\begin{array}{r}{\\int_{-\\infty}^{\\infty}f_{V}(v)Q_{t}(v)\\,\\in\\,[1-\\epsilon,1+\\epsilon]}\\end{array}$ , which implies (exercise) that the two expectations have limiting difference zero. But first we need some lemmas. ", "page_idx": 14}, {"type": "text", "text": "A.5.3 Lemmas ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Lemma 1. There is $h(t)$ depending on $F_{X}$ such that: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{)~\\operatorname*{lim}_{x\\rightarrow\\infty}h(t)=\\infty}\\\\ &{)~\\operatorname*{lim}_{t\\rightarrow\\infty}t-h(t)=\\infty}\\\\ &{)~\\operatorname*{lim}_{t\\rightarrow\\infty}\\frac{\\bar{F}_{X}(t-h(t))}{\\bar{F}_{X}(t)}=1}\\\\ &{)~\\operatorname*{lim}_{t\\rightarrow\\infty}\\operatorname*{sup}_{|v|\\leq h(t)}|Q(v,t)-1|=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. Lemma 2.19 from (Foss et al., 2013) implies that if $X$ is long-tailed (which it is, because subexponential implies long-tailed), then there is $h(t)$ such that condition (a) holds and $\\bar{F}_{X}$ is $h$ -insensitive; by Proposition 2.20 we can take $h$ such that $h(t)\\leq t/2$ for sufficiently large $t$ , implying condition (b). Conditions (c) and (d) follow from being $h$ -insensitive. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "Lemma 2. Suppose that $F_{X}$ is whole-line subexponential and $h$ is chosen as in Lemma 1. Also suppose that $\\bar{F}_{V}(t)=O(\\bar{F_{X}}\\dot{(t)}/t)$ . Then $P r[X+V>t$ , $V>h(t)$ , $X>h(t)]=o(\\bar{F}_{X}(t)/t)$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. This is a slight variation on lemma 3.8 from (Foss et al., 2013), and follows from the proof of Lemma 2.37. Lemma 2.37 states that ", "page_idx": 14}, {"type": "text", "text": "Lemma 2.37. Let $h$ be any increasing function on $\\mathbb{R}^{+}$ such that $h(x)\\rightarrow\\infty$ . Then, for any distributions $F_{1},F_{2},G_{1}$ ,  xa,n \u03bed1 $G_{2}$ oh(n $\\mathbb{R}$ ,, \u03b71 > h(x)} $\\operatorname*{lim}_{x\\to\\infty}\\frac{\\mathbb{P}\\left\\{\\xi_{1}+\\eta_{1}>x,\\xi_{1}>h(x),\\eta_{1}>h(x)\\right\\}}{\\mathbb{P}\\left\\{\\xi_{2}+\\eta_{2}>x,\\xi_{2}>h(x),\\eta_{2}>h(x)\\right\\}}\\leq\\operatorname*{lim}_{x\\to\\infty}\\frac{\\overline{{F_{1}}}(x)}{\\overline{{F_{2}}}(x)}\\cdot\\operatorname*{lim}_{x\\to\\infty}\\frac{\\overline{{G_{1}}}(x)}{\\overline{{G_{2}}}(x)},$ where $\\xi_{1},\\xi_{2},\\eta_{1}$ , and $\\eta_{2}$ are independent random variables with respective distributions $F_{1},F_{2},G_{1}$ and $G_{2}$ . ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "but it is actually proved that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left\\{\\xi_{1}+\\eta_{1}>x,\\xi_{1}>h(x),\\eta_{1}>h(x)\\right\\}\\leq}\\\\ {\\underset{z>h(x)}{\\operatorname*{sup}}\\frac{\\overline{{F_{1}}}(z)}{\\overline{{F_{2}}}(z)}\\cdot\\underset{z>h(x)}{\\operatorname*{sup}}\\frac{\\overline{{G_{1}}}(z)}{\\overline{{G_{2}}}(z)}\\cdot\\mathbb{P}\\left\\{\\xi_{2}+\\eta_{2}>x,\\xi_{2}>h(x),\\eta_{2}>h(x)\\right\\}.}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "If we let $F_{1}=F_{V},F_{2}=G_{1}=G_{2}=F_{X}$ , then we get ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left\\{X+V>t,X>h(t),V>h(t)\\right\\}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\underset{z>h(t)}{\\operatorname*{sup}}\\frac{\\bar{F}_{V}(z)}{\\bar{F}_{X}(z)}\\underset{z>h(t)}{\\operatorname*{sup}}\\frac{\\bar{F}_{X}(z)}{\\bar{F}_{X}(z)}\\mathbb{P}\\left\\{X+X^{\\prime}>t,X>h(t),X^{\\prime}>h(t)\\right\\}}\\\\ &{\\qquad\\qquad\\qquad=\\underset{z>h(t)}{\\operatorname*{sup}}\\frac{\\bar{F}_{V}(z)}{\\bar{F}_{X}(z)}\\mathbb{P}\\left\\{X+X^{\\prime}>t,X>h(t),X^{\\prime}>h(t)\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $X,X^{\\prime}\\sim F_{X}$ . Multiplying by $t$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{t\\mathbb P\\left\\{X+V>t,X>h(t),V>h(t)\\right\\}}&{{}\\leq}&{\\underset{z>h(t)}{\\operatorname*{sup}}\\frac{t\\bar{F}_{V}(z)}{\\bar{F}_{X}(z)}\\mathbb P\\left\\{X+X^{\\prime}>t,X>h(t),X^{\\prime}>h(t)\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and because $h(t)~\\rightarrow~\\infty$ as $t~\\rightarrow~\\infty$ and $\\bar{F}_{V}(t)~=~O(\\bar{F}_{X}(t)/t)$ , we can say that for some $c~<~\\infty$ , $\\begin{array}{r}{\\operatorname*{lim}_{t\\rightarrow\\infty}\\operatorname*{sup}_{z>h(t)}\\frac{t\\bar{F}_{V}(z)}{\\bar{F}_{X}(z)}\\;<\\;c}\\end{array}$ )tF \u00afF\u00afXV ((zz)) < c. Therefore for sufficiently large t P {X + V > t, X > h(t), V > h(t)} \u2264 $\\scriptstyle{\\frac{c}{t}}\\mathbb{P}\\left\\{X+X^{\\prime}>t,X>h(t),X^{\\prime}>h(t)\\right\\}$ . ", "page_idx": 15}, {"type": "text", "text": "By Theorem 3.6, ${\\mathbb P}\\left\\{X\\!+\\!X^{\\prime}>t,X\\!>\\!h(t),X^{\\prime}\\!>\\!h(t)\\right\\}$ is $o(\\bar{F}_{X}(t))$ , so the LHS is $o(\\bar{F}_{X}(t)/t)$ as desired. ", "page_idx": 15}, {"type": "text", "text": "A.5.4 Bounds on the numerator ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We want to show, for arbitrary $\\epsilon>0$ , that $\\begin{array}{r}{\\left|\\int_{-\\infty}^{\\infty}v f_{V}(v)Q(v)-\\int_{-\\infty}^{\\infty}v f_{V}(v)\\right|\\,<\\,\\epsilon}\\end{array}$ in the limit as $t\\,\\rightarrow\\,\\infty$ . Since $\\begin{array}{r}{\\left|\\int_{-\\infty}^{\\infty}v f_{V}(v)Q(v)-\\int_{-\\infty}^{\\infty}v f_{V}(v)\\right|\\leq\\int_{-\\infty}^{\\infty}|v f_{V}(v)(Q(v)-1)|=\\int_{-\\infty}^{\\infty}|v|\\cdot f_{V}(v)\\cdot|Q(v)-1|}\\end{array}$ it will suffice to show that the latter quantity is less than $\\epsilon$ for large $t$ . ", "page_idx": 15}, {"type": "text", "text": "We\u2019re going to show that $\\textstyle\\int_{-\\infty}^{\\infty}|v|\\cdot f_{V}(v)\\cdot|Q(v)-1|$ is small by showing that the integral gets arbitrarily small on each of four pieces: $(-\\infty,-h(t)],(-h(t),h(t)),[h(t),t-h(t)]$ , and $(t-h(t),\\infty)$ . We\u2019ll handle these case by case (they\u2019ll get monotonically trickier). ", "page_idx": 15}, {"type": "text", "text": "Region 1: $\\left(-\\infty,-h(t)\\right]$ Since $\\scriptstyle\\int_{-\\infty}^{\\infty}v f_{V}(v)$ is absolutely convergent, for sufficiently large $t$ we will have $\\begin{array}{r}{\\int_{-\\infty}^{-h(t)}|v|f_{V}(v)<\\epsilon}\\end{array}$ , since $h(t)$ goes to infinity by Lemma 1(a). Since $Q(v)$ is monotonically increasing and $Q(0)=1$ , we know that in this interval $|Q(v)-1|=1-Q(v)$ . So we have $\\begin{array}{r}{\\int_{-\\infty}^{-h(t)}|v|\\cdot f_{V}(v)\\cdot|Q(v)-1|=\\int_{-\\infty}^{-h(t)}|v|f_{V}(v)(1-Q(v))<\\int_{-\\infty}^{-h(t)}|v|f_{V}(v)<\\epsilon}\\end{array}$ as desired. Region 2: (\u2212h(t), h(t)) By lemma 1(d), h is such that for sufficiently large t, |Q(v) \u22121| <  \u2212\u221e\u221e|v\u03f5|fV (v) on the interval $[-h(t),h(t)]$ . (Note that the value of this upper bound depends only on $V$ and $\\epsilon$ , not on $t$ or $h$ .) So we have $\\begin{array}{r}{\\int_{-h(t)}^{h(t)}|v|f_{V}(v)|Q(v)-1|<\\frac{\\epsilon}{\\int_{-\\infty}^{\\infty}|v|f_{V}(v)}\\int_{-h(t)}^{h(t)}|v|f_{V}(v)<\\frac{\\epsilon}{\\int_{-\\infty}^{\\infty}|v|f_{V}(v)}\\int_{-\\infty}^{\\infty}|v|f_{V}(v)=\\epsilon,}\\end{array}$ . ", "page_idx": 15}, {"type": "text", "text": "Region 3: $[h(t),t-h(t)]$ For the third part, we\u2019d like to show that $\\begin{array}{r}{\\int_{h(t)}^{t-h(t)}v f_{V}(v)(Q(v)-1)<\\epsilon}\\end{array}$ . Since $\\begin{array}{r}{\\int_{h(t)}^{t-h(t)}v f_{V}(v)(Q(v)-1)<\\int_{h(t)}^{t-h(t)}t f_{V}(v)Q(v)=\\frac{t}{F_{X}(t)}\\int_{h(t)}^{t-h(t)}f_{V}(v)\\bar{F}_{X}(t-v)\\,d v.}\\end{array}$ it would suffice to show that the latter expression becomes less than $\\epsilon$ for large $t$ , or equivalently that $\\begin{array}{r}{\\int_{h(t)}^{t-h(t)}f_{V}(v)\\bar{F}_{X}(t-v)\\,=\\,}\\end{array}$ $o\\left(\\frac{\\bar{F}_{X}\\left(t\\right)}{t}\\right)$ . ", "page_idx": 15}, {"type": "text", "text": "The LHS in this expression is the unconditional probability that $X+V>t$ and $h(t)<V<t-h(t)$ , but this event implies $X+V>t,V>h(t)$ , and $X>\\bar{h}(t)$ . So we can write ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\int_{h(t)}^{t-h(t)}f_{V}(v)\\bar{F}_{X}(t-v)=P r[X+V>t,\\;h(t)<V<t-h(t)]}}\\\\ &{}&{<P r[X+V>t,\\;V>h(t),\\;X>h(t)]=o(\\bar{F}_{X}(t)/t)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "by Lemma 2. ", "page_idx": 16}, {"type": "text", "text": "Region 4: $(t-h(t),\\infty)$ For the fourth part, we\u2019d like to show that $\\begin{array}{r}{\\int_{t-h(t)}^{\\infty}v f_{V}(v)Q(v)\\rightarrow0}\\end{array}$ forlarge $t$ . Since $\\begin{array}{r}{Q(v)=\\frac{\\bar{F}_{X}\\left(t-v\\right)}{\\bar{F}_{X}\\left(t\\right)}<\\frac{1}{\\bar{F}_{X}\\left(t\\right)}}\\end{array}$ , it would suffice to show $\\begin{array}{r}{\\int_{t-h(t)}^{\\infty}v f_{V}(v)=o(\\bar{F}_{X}(t))}\\end{array}$ . But note that since limt\u2192\u221e F\u00afXF(\u00aftX\u2212(th)(t))= 1 by Lemma 1(c), this is equivalent to t\u221e\u2212h(t) vfV (v) = o( F\u00afX(t \u2212h(t))), which (by Lemma 1(b)) is equivalent to $\\begin{array}{r}{\\int_{t}^{\\infty}v f_{V}(v)=o(\\bar{F}_{X}(t))}\\end{array}$ .   \nNote that $\\begin{array}{r}{\\int_{t}^{\\infty}v f_{V}(v)=t\\int_{t}^{\\infty}f_{V}(v)+\\int_{t}^{\\infty}(v-t)f_{V}(v)=t\\bar{F}_{V}(t)+\\int_{t}^{\\infty}\\bar{F}_{V}(v),}\\end{array}$ , so it will suffice to show that both terms in this sum are $o(\\bar{F}_{X}(t))$ .   \nThe first term t F\u00afV (t) is o( F\u00afX(t)) because we assumed limt\u2192\u221etpF\u00af F\u00afXV( t()t) for some $p>1$ .   \nFor the second term, we have for the same reason $\\begin{array}{r}{\\int_{t}^{\\infty}\\bar{F}_{V}(v)<\\int_{t}^{\\infty}\\frac{\\bar{F}_{X}(v)}{v^{p}}=\\bar{F}_{X}(t)\\int_{t}^{\\infty}v^{-p}=\\frac{t^{1-p}}{p-1}\\bar{F}_{X}(t)=}\\end{array}$ $o(\\bar{F}_{X}(t))$ . ", "page_idx": 16}, {"type": "text", "text": "A.5.5 Bounds on the denominator ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "For the denominator, we want to show that $\\begin{array}{r}{\\operatorname*{lim}_{t\\to\\infty}\\int_{-\\infty}^{\\infty}f_{V}(v)Q_{t}(v)=1=\\int_{-\\infty}^{\\infty}f_{V}(v)}\\end{array}$ , so it\u2019ll suffice to show $\\begin{array}{r}{|\\int_{-\\infty}^{\\infty}f_{V}(v)(Q_{t}(v)-1)|=o(1)}\\end{array}$ as $t\\to\\infty$ . Again, we\u2019ll break up this integral into pieces, though they\u2019ll be more straightforward than last time. We\u2019ll look at $(-\\infty,-h(t)),[-h(t),h(t)]$ , and $(h(t),\\infty)$ . ", "page_idx": 16}, {"type": "text", "text": "$\\begin{array}{r}{\\bullet\\ |\\int_{-\\infty}^{-h(t)}f_{V}(v)(Q(v)-1)|=\\int_{-\\infty}^{-h(t)}f_{V}(v)(1-Q(v))<\\int_{-\\infty}^{-h(t)}f_{V}(v).}\\end{array}$ \u2013 But since $h(t)$ goes to infinity, this left tail of the integral will contain less and less of $V$ \u2019s probability mass as $t$ increases.   \n$\\begin{array}{r l}&{\\bullet\\ |\\int_{-h(t)}^{h(t)}f_{V}(v)(Q(v)-1)|\\leq\\int_{-h(t)}^{h(t)}f_{V}(v)|Q(v)-1|}\\\\ &{\\bullet\\leq\\operatorname*{sup}_{|v|\\leq h(t)}|Q(v,t)-1|\\int_{-h(t)}^{h(t)}f_{V}(v)\\leq\\operatorname*{sup}_{|v|\\leq h(t)}|Q(v,t)-1|}\\end{array}$ \u2013 By Lemma 1(d) we know that this goes to zero for large $t$ .   \n$\\begin{array}{r}{\\bullet\\ |\\int_{h(t)}^{\\infty}f_{V}(v)(Q(v)-1)|=\\int_{h(t)}^{\\infty}f_{V}(v)(Q(v)-1)<\\int_{h(t)}^{\\infty}f_{V}(v)Q(v).}\\end{array}$ ", "page_idx": 16}, {"type": "text", "text": "But for sufficiently large $t$ we have $h(t)~>~1$ , so we obtain $\\begin{array}{r}{\\int_{h(t)}^{\\infty}f_{V}(v)Q(v)\\ <\\ \\int_{h(t)}^{\\infty}v f_{V}(v)Q(v)\\ <}\\end{array}$ $\\begin{array}{r}{\\int_{-\\infty}^{\\infty}v f_{V}(v)Q(v)=o(1)}\\end{array}$ by the results of the previous section. This completes the proof. ", "page_idx": 16}, {"type": "text", "text": "A.6 Theorem 6 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Restatement of theorem 6. Let $X,V$ be independent random variables such that $\\begin{array}{r}{\\operatorname*{lim}_{t\\to\\infty}\\frac{\\bar{F}_{X}\\left(t+1\\right)}{\\bar{F}_{X}\\left(t\\right)}=0.}\\end{array}$ implies that $X$ has tails that are dominated by $e^{-c x}$ for any $c$ , though it\u2019s a slightly stronger claim because it requires that $X$ not have large jumps in the decay of its tails.) Then for any $V$ with a finite mean which has no upper bound, $\\begin{array}{r}{\\operatorname*{lim}_{t\\to\\infty}\\mathbb{E}[V|X+V>t]=\\infty}\\end{array}$ . ", "page_idx": 16}, {"type": "text", "text": "Theorem 6 generalizes a consequence of the \"Regressional Goodhart Identity\" in (Gao et al., 2023). ", "page_idx": 16}, {"type": "text", "text": "Proof. Let $\\operatorname*{Pr}(V>c+1)=p>0$ , which exists by our assumption that $V$ is unbounded. ", "page_idx": 16}, {"type": "text", "text": "Let $\\mathbb{E}[V|V<c]=q$ . (If this is undefined because the conditional has probability 0, we\u2019ll have the desired result anyway since then $V$ would always be at least $c.$ ) ", "page_idx": 16}, {"type": "text", "text": "Observe that for all $t$ , $\\mathbb{E}[V|V\\,<\\,c,X+V\\,>\\,t]\\,\\ge\\,q$ (assuming it is defined), because we\u2019re conditioning $(V|V<c)$ on an event which is more likely for larger $v$ (since $X$ and $V$ are independent). ", "page_idx": 16}, {"type": "text", "text": "First, let\u2019s see that limt\u2192\u221eP P( V( V> c<+c|1X|X++V V\u2265 t\u2265)t) . This ratio of probabilities is equal to ", "page_idx": 16}, {"type": "image", "img_path": "UXuBzWoZGK/tmp/269eb30b5dc43e7a47b28bc628402a9f37138b903ad88cf34d0e64eb34086a22.jpg", "img_caption": ["Figure B.1: Histogram and normal probability plot of reward assigned by Pythia RM to random length-1024 token sequences. The Q-Q plot suggests the distribution is approximately normal, which is much lighter-tailed than exponential. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "UXuBzWoZGK/tmp/521b98ccef7c1cf066782a9a18a261166384689b22fd6e60271b2fe13cd22000.jpg", "img_caption": ["Figure B.2: Reward and log-probability for ACG-optimized inputs to Starling 7B-alpha. "], "img_footnote": [], "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\int_{-\\infty}^{c}f_{V}(v)\\bar{F}_{X}(t-v)}{\\int_{c+1}^{\\infty}f_{V}(v)\\bar{F}_{X}(t-v)}\\leq\\frac{\\int_{-\\infty}^{c}f_{V}(v)\\bar{F}_{X}(t-c)}{\\int_{c+1}^{\\infty}f_{V}(v)\\bar{F}_{X}(t-c-1)}=\\frac{\\bar{F}_{X}(t-c)}{\\bar{F}_{X}(t-c-1)}\\cdot\\frac{\\int_{-\\infty}^{c}f_{V}(v)}{\\int_{c+1}^{\\infty}f_{V}(v)}}\\\\ &{=\\frac{\\bar{F}_{X}(t-c)}{\\bar{F}_{X}(t-c-1)}\\cdot\\frac{\\operatorname*{Pr}(V<c)}{\\operatorname*{Pr}(V>c+1)}\\leq\\frac{\\bar{F}_{X}(t-c)}{\\bar{F}_{X}(t-c-1)}\\cdot\\frac{1}{p}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which, by our assumption that $\\begin{array}{r}{\\operatorname*{lim}_{t\\to\\infty}\\frac{\\bar{F}_{X}\\left(t+1\\right)}{\\bar{F}_{X}\\left(t\\right)}=0}\\end{array}$ , will get arbitrarily small as $t$ increases for any positive $p$ ", "page_idx": 17}, {"type": "text", "text": "Now, consider $\\mathbb{E}[V|X+V\\geq t]$ . We can break this up as the sum across outcomes $Z$ of $\\mathbb{E}[V|Z,X+V\\ge$ $t\\!\\!\\!\\mid\\,\\cdot\\,\\operatorname*{Pr}(Z\\!\\!\\mid\\!X+V\\!\\!\\!\\mid\\,\\geq\\,t)$ for the three disjoint outcomes $V<c$ , $c\\,\\leq\\,V\\,\\leq\\,c+1$ , and $V\\,>\\,c+1$ . Note that we can lower bound these expectations by $q,c,c+1$ respectively. But then once $t$ is large enough that PrP(rV( V> c<+c|1X|X++V V\u2265 t\u2265)t) < c\u22121q , this weighted sum of conditional expectations will add to more than c. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "B Additional experiments and figures ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Figures B.1, B.2 relate to experiments mentioned in the main paper. In response to reviewer feedback, we added two further experiments to demonstrate the catastrophic Goodhart phenemonon with artificially heavy-tailed reward, one using best-of-N on synthetic distributions and one with PPO on Pythia 1B. ", "page_idx": 17}, {"type": "text", "text": "B.1 Best-of-N experiment ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We created a synthetic experiment by letting reward $U=X+V$ , where $X$ and $V$ are independent and sampled from different probability distributions, consistent with our theoretical assumptions. We vary $N$ from 1 to 65536, do 100 trials of taking the best-of- $N$ sample with highest $U$ , and note whether $V$ goes towards 0 (overoptimization) or not. ", "page_idx": 17}, {"type": "text", "text": "Possible distributions for V are normal and t-distribution with $\\mathrm{df}{=}10$ . Possible distributions for $\\Chi$ are normal, t with $\\mathrm{df}{=}3$ , t with $\\mathrm{df}{=}5$ , lognormal, and Levy. (All of these heavy-tailed except for the normal distribution.) V is scaled to a standard deviation of 2 and $\\Chi$ has s.d. of 1 (except for the Levy distribution, which has infinite variance), representing that in ordinary regimes most of the variance comes from utility rather than error. ", "page_idx": 17}, {"type": "image", "img_path": "UXuBzWoZGK/tmp/82bfdf7bb09c28bc3d610dfd16d12b57ad4534ed85e4e714b61e54633d8424f7.jpg", "img_caption": [], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Figure B.3: When the error $X$ is normal and thus light-tailed, $V$ increases monotonically with $N$ , consistent with our Theorem 6. However, when both $X$ and $V$ are heavy-tailed, we see results consistent with theorem 5. In 5 of 6 cases when $X$ is lognormal or student-t, $V$ first increases then starts to decline around $N=10^{2}$ or $10^{3}$ . When $X$ is $(\\mathrm{t},\\,\\mathrm{df}\\mathrm{=}5)$ and $\\mathrm{v}$ is (t, $,\\mathrm{{df}=10}$ ), $V$ instead peaks around $N=10^{5}$ (but declines afterwards). Finally, when X is Levy-distributed, utility never goes significantly above zero (optimization completely fails) because the Levy distribution is too heavy-tailed. ", "page_idx": 18}, {"type": "text", "text": "The results are shown in Figure B.3. Briefly, the results are consistent with the asymptotic results in theorems 5 and 6, showing that overoptimization ", "page_idx": 18}, {"type": "text", "text": "B.2 PPO experiment ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this experiment, we examined PPO with artificially heavy-tailed rewards to see if catastrophic Goodhart could be observed. ", "page_idx": 18}, {"type": "text", "text": "OpenRLHF (Hu et al., 2024) was used to train Pythia 1B with a reward model derived also from Pythia 1B, on the default OpenRLHF prompt dataset. We used the reward model to represent true utility, and a heavy-tailed error term based on the number of \"the\" tokens was added to get the proxy reward. The kl_target $=0.5$ option was used to dynamically adjust KL penalty, as we mention is done in Ziegler et al. (2020). Rewards were not clipped. (Reward clipping can be useful to prevent overoptimization, but is not always used in PPO.) Response length was limited to 256. ", "page_idx": 18}, {"type": "text", "text": "Two example completions are in figure B.4. Midway through training, the model generates reasonable text with a higher number of \"the\" tokens than usual, but as training progresses the model starts to generate completions with a very high number of \"the\" tokens while proxy reward reaches values $>10^{9}$ , demonstrating that the basic pattern of catastrophic Goodhart can occur in RLHF under conditions of heavy-tailed error. ", "page_idx": 18}, {"type": "text", "text": "The preparation of polyarylethylene which is either employed for the purpose of applications in the petroleum refining or in the production of the well-known automobile oil lubes is carried out by the so-called preparation by the salt bath method described... in 1980 by D. W. Perkins in the United States. This method presupposes the usage of the salt bath as the medium for the preparation of the polyarylediene\u2013the method of preparation of the crude oil base by the process which consists in letting it be saturated up with a particular the ", "page_idx": 19}, {"type": "text", "text": "A mirror, the essence of the understanding of the given the conclusion that the manifestation in the form of the gender and the identification the solutions related to different fields of the modern utilization of the the functionality of the the the the modification of the the the the the the state of the the the the the the the the the the the employment and the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the and the the the the the the the the the the the the the the the the the the the the by the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the and the the the the the the the the the the the the the the the the the ", "page_idx": 19}, {"type": "text", "text": "Figure B.4: PPO sample generations. Top: early in training, the model generates reasonable completions. Bottom: later in training, the artifically heavy-tailed reward dominates and the model generates completions with a very high number of \"the\" tokens. ", "page_idx": 19}, {"type": "text", "text": "The result depends on hyperparameters (e.g. reward clipping would prevent this), so our observation should not be taken as a claim that catastrophic Goodhart is inevitable in all RLHF settings with heavy-tailed error. ", "page_idx": 19}, {"type": "text", "text": "C Assets ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We use three models for our experiments: Starling 7B-alpha, Llama 2 7B-chat, and Pythia-1.4B. Starling was developed by Berkeley, and Pythia by EleutherAI. Starling and Pythia models are licensed under Apache-2.0.1 11 Llama 2 models were developed by Meta and licensed under a license published by Meta.12 ", "page_idx": 19}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 20}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 20}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 20}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 20}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 20}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 20}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 20}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: The abstract lists the important claims: the relationship between Goodhart\u2019s Law and whether the error in a misspecified reward is heavy-tailed. The main limitation of independence assumptions is clearly stated in the introduction. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 20}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: Section 7 lists the limitations, which we have combined with the discussion section due to heavy overlap. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. ", "page_idx": 20}, {"type": "text", "text": "\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: All proofs are given inline or in the appendix, except for Theorem 5 which appears in the supplemental material. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 21}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: Sampling rewards requires no hyperparameters, and hyperparameters are provided for ACG. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but ", "page_idx": 21}, {"type": "text", "text": "reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. ", "page_idx": 22}, {"type": "text", "text": "While NeurIPS does not require releasing code, the conference does require all submissions   \nto provide some reasonable avenue for reproducibility, which may depend on the nature of the   \ncontribution. For example   \n(a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.   \n(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 22}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: Code will be provided in the supplemental material ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 22}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Other than hyperparameters, there are no details required to understand the results. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.   \n\u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 22}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The only error bars are inter-run variability of ACG. The standard deviation was reported rather than error bars due to the small number of runs. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 23}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: The experiments took minimal compute resources except H100 hours for ACG, and we report the number of GPU-hours used. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 23}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: The research conforms to all data-related concerns. No human subjects were involved, and we think the risk of harmful societal impact is minimal. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 23}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: The immediate societal impacts are limited, but we discuss some potential applications to long-term safety. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 24}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: We have created no such artifacts. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 24}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: The assets we use are Starling 7B-alpha, Llama 2 7B-chat, and Pythia-1.4B. Starling and Pythia models are licensed under Apache-2.0.13 14 Llama 2 models are licensed under a license published by Meta.15 We are in compliance with all licenses and terms of use. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: We do not release new assets. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 25}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: No human subjects are involved. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 25}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: No human subjects are involved. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 25}]