[{"figure_path": "UXuBzWoZGK/figures/figures_5_1.jpg", "caption": "Figure 1: Plots of the distribution of reward from 30000 random length-1024 token sequences to Starling 7B-alpha. Clockwise from top left: The histogram shows a unimodal distribution with a slight right skew. The normal probability plot indicates the data are heavier-tailed than normal. The Hill estimator (error bars are standard error) appears to be 0.20 for higher values but fluctuates for lower values. The exponential probability plot of the right half of the distribution is consistent with either light or heavy tails (under heavy tails, the slope would go to infinity).", "description": "This figure presents four different visualizations of the reward distribution obtained from sampling 30000 random sequences of length 1024 tokens and evaluating them with the Starling 7B-alpha reward model.  The histogram provides a visual representation of the distribution's shape. A normal probability plot helps assess the normality of the distribution and identifies deviations, suggesting potential heavy tails.  An exponential probability plot analyzes the distribution's tail behavior. Finally, the Hill estimator, with error bars, quantifies the heaviness of the tail by estimating the tail index. The overall analysis aims to determine if the reward distribution is light-tailed or heavy-tailed, a key aspect of the research.", "section": "4.1 Results"}, {"figure_path": "UXuBzWoZGK/figures/figures_6_1.jpg", "caption": "Figure B.1: Plots of the distribution of reward from 30000 random length-1024 token sequences to Starling 7B-alpha. Clockwise from top left: The histogram shows a unimodal distribution with a slight right skew. The normal probability plot indicates the data are heavier-tailed than normal. The Hill estimator (error bars are standard error) appears to be 0.20 for higher values but fluctuates for lower values. The exponential probability plot of the right half of the distribution is consistent with either light or heavy tails (under heavy tails, the slope would go to infinity).", "description": "This figure presents four different plots that visually represent the distribution of rewards obtained from 30,000 randomly generated token sequences using the Starling 7B-alpha reward model.  The histogram displays the frequency distribution of the rewards, showing a unimodal pattern with a slight right skew. The normal probability plot assesses normality, revealing heavier tails than a normal distribution.  A Hill estimator is used to estimate the tail index of the distribution, indicating a value around 0.20 for higher-order statistics but fluctuating for lower values. Lastly, an exponential probability plot analyzes the right half of the data, suggesting the possibility of both light or heavy tails.  Collectively, these plots help ascertain the nature of the reward distribution, which is important for understanding potential over-optimization or catastrophic Goodhart effects.", "section": "4.1 Results"}, {"figure_path": "UXuBzWoZGK/figures/figures_9_1.jpg", "caption": "Figure B.3: When the error X is normal and thus light-tailed, V increases monotonically with N, consistent with our Theorem 6. However, when both X and V are heavy-tailed, we see results consistent with theorem 5. In 5 of 6 cases when X is lognormal or student-t, V first increases then starts to decline around N = 102 or 103. When X is (t, df=5) and V is (t, df=10), V instead peaks around N = 105 (but declines afterwards). Finally, when X is Levy-distributed, utility never goes significantly above zero (optimization completely fails) because the Levy distribution is too heavy-tailed.", "description": "This figure displays the results of a best-of-N experiment. It demonstrates how the utility (V) changes with increasing N (number of samples) under different conditions of heavy-tailed and light-tailed errors (X). It supports the theoretical findings of Theorems 5 and 6 by showcasing the impact of error distribution on the optimization outcome.", "section": "B.1 Best-of-N experiment"}, {"figure_path": "UXuBzWoZGK/figures/figures_13_1.jpg", "caption": "Figure A.2: A diagram showing the region boundaries at \u2212h(t), h(t), and t \u2013 h(t) in an example where t = 25 and h(t) = 4, along with a negative log plot of the relevant distribution:", "description": "This figure is used to illustrate the proof strategy for Theorem 5 in the paper. It shows four regions (-\u221e, -h(t)], (-h(t), h(t)), [h(t), t-h(t)], and (t-h(t), \u221e) defined to show that the effect of each region on E[V|c] is small in the limit. The figure shows the conditional distribution of V across these four regions, along with a plot of the negative logarithm of the distribution.", "section": "A.5 Theorem 5"}, {"figure_path": "UXuBzWoZGK/figures/figures_17_1.jpg", "caption": "Figure B.1: Histogram and normal probability plot of reward assigned by Pythia RM to random length-1024 token sequences. The Q-Q plot suggests the distribution is approximately normal, which is much lighter-tailed than exponential.", "description": "This figure displays two plots showing the distribution of rewards assigned by the Pythia RM to randomly generated sequences of 1024 tokens. The histogram visually represents the frequency of different reward values. The normal probability plot compares the observed reward distribution to a theoretical normal distribution. The close alignment of data points to the diagonal line in the Q-Q plot suggests that the reward distribution is well-approximated by a normal distribution, which has relatively light tails compared to heavier-tailed distributions like the exponential distribution.", "section": "B.1 Best-of-N experiment"}, {"figure_path": "UXuBzWoZGK/figures/figures_17_2.jpg", "caption": "Figure B.2: Reward and log-probability for ACG-optimized inputs to Starling 7B-alpha.", "description": "This figure shows the relationship between reward and log-probability for sequences optimized using the Accelerated Coordinate Gradient (ACG) method on the Starling 7B-alpha reward model.  It visually represents the distribution of rewards obtained by ACG and their corresponding probabilities, providing insights into the effectiveness and efficiency of the method in finding high-reward sequences.  The x-axis represents the reward, and the y-axis represents the log-probability.", "section": "B.1 Best-of-N experiment"}, {"figure_path": "UXuBzWoZGK/figures/figures_18_1.jpg", "caption": "Figure B.3: When the error X is normal and thus light-tailed, V increases monotonically with N, consistent with our Theorem 6. However, when both X and V are heavy-tailed, we see results consistent with theorem 5. In 5 of 6 cases when X is lognormal or student-t, V first increases then starts to decline around N = 102 or 103. When X is (t, df=5) and V is (t, df=10), V instead peaks around N = 105 (but declines afterwards). Finally, when X is Levy-distributed, utility never goes significantly above zero (optimization completely fails) because the Levy distribution is too heavy-tailed.", "description": "This figure shows the results of a best-of-N experiment on synthetic datasets with various combinations of heavy-tailed and light-tailed distributions for error (X) and utility (V).  As N increases (number of samples), the results show that when the error is light-tailed, utility increases monotonically. However, with heavy-tailed error, utility initially increases but then declines, demonstrating catastrophic Goodhart.  The Levy distribution, being extremely heavy-tailed, results in optimization failure.", "section": "B.1 Best-of-N experiment"}]