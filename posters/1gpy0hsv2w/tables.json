[{"figure_path": "1GpY0hsv2w/tables/tables_6_1.jpg", "caption": "Table 1: Ablation study on minimal trajectory length for T-Diff training in Gibson (val). When the trajectory length is 1 (rows 2-6), T-Diff is trained to directly predict a waypoint. i-th denotes that the training ground truth is the point at the i-th step from the current position on the optimal trajectory.", "description": "This table presents an ablation study on the impact of minimal trajectory length used for training the Trajectory Diffusion model (T-Diff). It compares the performance of T-Diff when trained to predict a single waypoint (length 1) versus predicting a sequence of waypoints (lengths 4 and 32).  The results show the effect of this choice on MSE, Success Rate (SR), Success weighted by Path Length (SPL), and Distance To Goal (DTS) in the Gibson environment. It demonstrates that using longer trajectory sequences improves performance.", "section": "Evaluations of T-Diff variants"}, {"figure_path": "1GpY0hsv2w/tables/tables_7_1.jpg", "caption": "Table 1: Ablation study on minimal trajectory length for T-Diff training in Gibson (val). When the trajectory length is 1 (rows 2-6), T-Diff is trained to directly predict a waypoint. i-th denotes that the training ground truth is the point at the i-th step from the current position on the optimal trajectory.", "description": "This table presents an ablation study on the impact of the minimal trajectory length used for training the Trajectory Diffusion (T-Diff) model.  It shows the results on the Gibson validation set. The study compares the performance of T-Diff when trained to predict a single waypoint (trajectory length 1) versus longer trajectories.  Different rows show results using different offsets from the current position for the ground truth waypoint, highlighting that using longer training trajectories (32 steps) significantly improves the model's performance compared to using single-step waypoints.", "section": "Evaluations of T-Diff variants"}, {"figure_path": "1GpY0hsv2w/tables/tables_7_2.jpg", "caption": "Table 2: Comparison with different variants of T-Diff on Gibson (val). It means using RGB information as condition while mt refers to employing semantic map as condition. Here, LP denotes local policy, and FBE corresponds to the area potential function proposed by [31].", "description": "This table compares different variants of the Trajectory Diffusion (T-Diff) model on the Gibson dataset for validation.  It contrasts the performance of T-Diff using only visual input (It), semantic map (mt), and both semantic map and goal information (mt+Goal).  A baseline method (FBE+LP) without T-Diff is also included for comparison, showing the impact of the proposed sequence planning method on various metrics such as success rate (SR), success weighted by path length (SPL), and distance to goal (DTS). MSE (Mean Squared Error) is included to evaluate the trajectory generation accuracy.", "section": "Evaluations of T-Diff variants"}, {"figure_path": "1GpY0hsv2w/tables/tables_8_1.jpg", "caption": "Table 3: Comparisons with simpler model for trajectory generation. MSE measures the quality of generated trajectories, while SR, SPL, and DTS indicate navigation performance.", "description": "This table compares the performance of the proposed trajectory diffusion model (T-diff) against a simpler model for trajectory generation.  The simpler model uses a similar Transformer-based architecture but is trained with MSE loss instead of the diffusion process used by T-diff. The comparison highlights the effectiveness of the diffusion model in generating high-quality trajectories that lead to improved navigation performance, as measured by Success Rate (SR), Success weighted by Path Length (SPL), and Distance To Goal (DTS).", "section": "Comparisons with simpler model"}, {"figure_path": "1GpY0hsv2w/tables/tables_8_2.jpg", "caption": "Table 4: Comparisons of navigation performance across different training and testing simulators.", "description": "This table compares the performance of the proposed Trajectory Diffusion (T-Diff) model and the PONI model across different training and testing simulators.  It demonstrates the scalability of the T-Diff model by showing that its performance is less affected when training and testing are done on different simulators compared to PONI.", "section": "5.2 Evaluation Results"}, {"figure_path": "1GpY0hsv2w/tables/tables_9_1.jpg", "caption": "Table 5: Comparing ObjectNav performance on Gibson and MP3D of related studies. Note that Red-Rabbit [53] utilizes auxiliary tasks for training, while THDA [28] and Habitat-Web [34] incorporate additional training data. Results of SemExp [4], L2M [12] and Stubborn [27] are reported from [60]. Results marked with * indicate our implementation.", "description": "This table compares the performance of the proposed Trajectory Diffusion model (T-Diff) against other state-of-the-art methods on two standard ObjectNav datasets: Gibson and Matterport3D (MP3D).  It presents success rate (SR), success weighted by path length (SPL), and distance to goal (DTS) metrics for each method.  Noteworthy is the inclusion of methods that utilize auxiliary tasks or additional training data, highlighting the superior performance of T-Diff even in comparison to those approaches.  The asterisk (*) denotes results that were reproduced by the authors.", "section": "5 Experiments"}, {"figure_path": "1GpY0hsv2w/tables/tables_15_1.jpg", "caption": "Table 6: Chosen object categories in Gibson [47] and MP3D [3].", "description": "This table presents the object categories used in the Gibson and Matterport3D (MP3D) datasets for both training and evaluation phases of the ObjectNav task.  For Gibson, a larger set of categories is used for training, while a smaller subset is used for evaluation. Similarly, MP3D uses a comprehensive set of object categories for training and a subset for evaluation. This reflects a common practice in machine learning where a broader range of data is used to train the model, and then a more focused subset is used for testing the model's generalization ability.", "section": "5.1 Experimental Setup"}, {"figure_path": "1GpY0hsv2w/tables/tables_15_2.jpg", "caption": "Table 7: FLOPs(Floating Point Operations) of SemExp [4], PONI [31], 3D-aware [60] and our T-Diff.", "description": "This table compares the computational complexity of four different methods for object goal navigation: SemExp, PONI, 3D-Aware, and the proposed T-Diff. The complexity is measured in terms of FLOPs (floating point operations), which represents the number of floating-point operations required to perform the computations of each method.  Lower FLOPs generally indicate higher computational efficiency. The table shows that the proposed T-Diff method has a significantly lower computational cost compared to PONI, demonstrating enhanced efficiency.", "section": "A.2 Computation Complexity"}]