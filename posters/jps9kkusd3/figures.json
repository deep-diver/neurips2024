[{"figure_path": "jps9KkuSD3/figures/figures_1_1.jpg", "caption": "Figure 1: Overview of the proposed approach. Left: calibrating an estimated error threshold to separate low/high true errors. Right: sequentially tracking production data exceeding the threshold and raising an alarm upon a significant increase.", "description": "This figure illustrates the two-stage process of the proposed approach for detecting harmful distribution shifts. The left panel shows the calibration stage, where a secondary error estimator model is used with labeled data to establish a threshold that effectively separates observations with low and high true errors. The right panel depicts the online monitoring stage. In this stage, the error estimator is applied to unlabeled production data, and the proportion of observations exceeding the calibrated threshold is continuously tracked. An alarm is triggered when this proportion significantly increases beyond what is expected under the null hypothesis of no harmful shift.", "section": "3 SHSD with Production Labels"}, {"figure_path": "jps9KkuSD3/figures/figures_3_1.jpg", "caption": "Figure 2: Calibration toy example. Left: threshold grid created by sweeping p \u2208 [0.5, 0.95] at increments of 0.05 and p\u0302 \u2208 [0.1, 0.9] at increments of 0.1. Middle: FDP of selector for each (p, p\u0302) pair. Black outline indicates pairs for which FDP < 0.2. Right: selector power for each (p, p\u0302) pair. Green dotted outline indicates the pair that maximises power subject to the FDP < 0.2 limit. Corresponding thresholds (q, q\u0302) shown as thick lines in left plot.", "description": "This figure illustrates the calibration process for selecting optimal thresholds to balance statistical power and false discovery proportion (FDP). The left panel shows a grid search over different quantiles of true and estimated errors, represented as (p, p\u0302). The middle panel displays the FDP for each (p, p\u0302) pair, highlighting those with FDP below 0.2. The right panel shows the power for each pair, with the optimal pair maximizing power while keeping FDP below 0.2 indicated. The optimal thresholds (q, q\u0302) are visually highlighted on the left panel.", "section": "4.1 Fitting and Calibrating the Error Estimator"}, {"figure_path": "jps9KkuSD3/figures/figures_4_1.jpg", "caption": "Figure 2: Calibration toy example. Left: threshold grid created by sweeping p \u2208 [0.5, 0.95] at increments of 0.05 and p\u0302 \u2208 [0.1, 0.9] at increments of 0.1. Middle: FDP of selector for each (p, p\u0302) pair. Black outline indicates pairs for which FDP < 0.2. Right: selector power for each (p, p\u0302) pair. Green dotted outline indicates the pair that maximises power subject to the FDP < 0.2 limit. Corresponding thresholds (q, q\u0302) shown as thick lines in left plot.", "description": "This figure illustrates the calibration process for selecting optimal thresholds to balance statistical power and false discovery proportion (FDP).  The left panel shows a grid search over different quantiles of true and estimated errors. The middle panel displays the FDP for each threshold pair, highlighting pairs with FDP below 0.2. The right panel shows the power for each threshold pair. The optimal threshold pair, maximizing power while keeping FDP below 0.2, is indicated.  This process ensures that the selected thresholds effectively distinguish between low and high-error observations, which are then used to track harmful shifts in the production data.", "section": "4.1 Fitting and Calibrating the Error Estimator"}, {"figure_path": "jps9KkuSD3/figures/figures_6_1.jpg", "caption": "Figure 4: Evolution of bounds in production for mean detector (left) and quantile detector (right).", "description": "This figure shows the evolution of upper and lower bounds for both the mean and quantile detectors over time in a production setting where a shift occurs. The gray lines represent the lower bounds calculated using the true errors, while the blue lines represent the lower bounds calculated using the estimated errors from the error estimators. The red line indicates the upper bound that must be exceeded for an alarm to be triggered. The pink line in the right panel shows the upper bound of the first statistic for the quantile detector. The figure illustrates the relative performance of the two methods and highlights the difference in detection time due to the reliance on estimated errors in the plug-in approach.", "section": "5.1 Illustrative Example on an Image Dataset"}, {"figure_path": "jps9KkuSD3/figures/figures_7_1.jpg", "caption": "Figure 5: Left: Power/FDP when  for all datasets. Middle: Absolute detection time difference vs. the methods using true errors. Right: Power values for different harmfulness thresholds ( ).", "description": "This figure presents a comparison of the quantile and mean detection methods across various datasets and harmfulness thresholds. The left panel shows the power and false discovery proportion (FDP) when the harmfulness threshold is set to zero.  The middle panel displays the absolute difference in detection time between the methods using estimated versus true errors. The right panel illustrates the power of each method as the harmfulness threshold varies.", "section": "5.2 Synthetic Shifts on Tabular Datasets"}, {"figure_path": "jps9KkuSD3/figures/figures_8_1.jpg", "caption": "Figure 6: Power and FDP by error across all datasets.", "description": "This figure shows the relationship between the power and false discovery proportion (FDP) of both the quantile detector and the mean detector, across different ranges of R-squared values from the error estimator. The R-squared values represent the accuracy of the error estimator, which ranges from 0.1 to 0.55 across all datasets.  For each R-squared range, the power and FDP are averaged over all datasets and all the different shift types for both methods.  It is used to illustrate how the performance of the error estimator affects both methods. Overall, the figure indicates that the quantile detector maintains lower FDP compared to the mean detector across all datasets and error estimator ranges, while exhibiting comparable or even better power.", "section": "5.2 Synthetic Shifts on Tabular Datasets"}, {"figure_path": "jps9KkuSD3/figures/figures_8_2.jpg", "caption": "Figure 5: Left: Power/FDP when  for all datasets. Middle: Absolute detection time difference vs. the methods using true errors. Right: Power values for different harmfulness thresholds ( ).", "description": "This figure summarizes the results of a large-scale experiment evaluating the effectiveness of both mean and quantile detection methods in detecting harmful shifts while controlling false alarms.  It shows the performance of both methods across various datasets and synthetic distribution shifts. The left panel displays the aggregated power and false discovery proportion (FDP) across datasets. The middle panel visualizes the absolute difference in detection time between the methods using estimated versus true errors.  The right panel analyzes how the power changes with varying harmfulness threshold, showing the consistent superiority of the quantile detector.", "section": "5.2 Synthetic Shifts on Tabular Datasets"}, {"figure_path": "jps9KkuSD3/figures/figures_12_1.jpg", "caption": "Figure 8: Distribution of \u03b4 across the different shifts and datasets of Section 5.2 (a) and the natural distribution shifts of Section 5.3 (b)", "description": "This figure shows the distribution of delta (\u03b4), which represents the difference between the empirical distribution of false positives in production data and that in the source data.  Subfigure (a) presents the distribution across synthetic shifts and datasets from Section 5.2, while subfigure (b) displays the distribution across natural shifts from Section 5.3. The box plots illustrate the median, quartiles, and range of \u03b4 values, providing insights into the validity of Assumption 4.1 which states that the rate of false positives in production should not exceed that in the source data by much.  The figure suggests that Assumption 4.1 holds approximately half the time and when it doesn't the difference is small.", "section": "A Discussion of Assumption 4.1"}, {"figure_path": "jps9KkuSD3/figures/figures_14_1.jpg", "caption": "Figure 4: Evolution of bounds in production for mean detector (left) and quantile detector (right).", "description": "This figure displays the evolution of the upper and lower bounds for both the mean and quantile detectors over time. The upper bound (red line) represents the threshold that must be exceeded to trigger an alarm. The lower bound (blue line) represents the estimated lower bound of the error parameter with access to only estimated errors, and the grey line is based on calculations using the true error. The pink line displays an additional upper bound for the quantile detector, and this line must also be exceeded to raise an alarm for the second test. The figure shows that in both mean and quantile scenarios, the detector using true errors raised alarms earlier than the plug-in version. The quantile detector is closer to the true error detector than the mean detector, especially when the estimator is imperfect.", "section": "5.1 Illustrative Example on an Image Dataset"}, {"figure_path": "jps9KkuSD3/figures/figures_15_1.jpg", "caption": "Figure 10: Left: Power/FDP when  \u03b5<sub>tol</sub> = 0 for all datasets. Middle: Absolute detection time difference vs. the methods using true errors. Right: Power values for different harmfulness thresholds (\u03b5<sub>tol</sub>).", "description": "This figure compares the performance of the first and second versions of the quantile detector with the mean detector using three metrics: Power, FDP (False Discovery Proportion), and absolute detection time difference.  The left panel shows the power and FDP trade-off for all datasets when the harmfulness threshold is zero. The middle panel displays box plots of the absolute difference in detection time between the methods using estimated errors and the same methods with access to true errors. The right panel illustrates how the power of each method varies across datasets as the harmfulness threshold increases.", "section": "E Comparison Between \u0424\u2081 and \u0424\u2082"}, {"figure_path": "jps9KkuSD3/figures/figures_15_2.jpg", "caption": "Figure 10: Left: Power/FDP when  for all datasets. Middle: Absolute detection time difference vs. the methods using true errors. Right: Power values for different harmfulness thresholds ( ).", "description": "This figure compares the performance of two different quantile detectors (v1 and v2) and a mean detector. The left panel shows the power and false discovery proportion (FDP) when the harmfulness threshold is zero. The middle panel shows the difference in detection time between the methods using estimated and true errors. The right panel shows how power varies across different harmfulness thresholds. The quantile detectors generally exhibit better power-FDP trade-off compared to the mean detector.  However, the first quantile detector (v1) performs poorly in terms of power. ", "section": "E Comparison Between  and "}, {"figure_path": "jps9KkuSD3/figures/figures_16_1.jpg", "caption": "Figure 12: Illustration of a Disagreement-Based Detector Failure Case", "description": "The figure illustrates a scenario where a disagreement-based detector, like Detectron, might fail.  Even though the data has shifted (in a benign way), the base classifier (a perfect classifier in this example) still correctly classifies the new data.  However, a disagreement classifier might learn to mimic the base classifier's performance on the training data but disagree on the new data, leading to a false alarm. This highlights the sensitivity of disagreement-based methods to the base model's performance, the complexity of the disagreement classifier, and the properties of the data.", "section": "F.1 Limitations of Disagreement-based Detectors"}]