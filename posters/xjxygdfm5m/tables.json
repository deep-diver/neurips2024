[{"figure_path": "xjXYgdFM5M/tables/tables_4_1.jpg", "caption": "Table 2: Sequencial edits on GPT and Llama with ZsRE dataset. The best results are in bold and underline means the suboptimal.", "description": "This table presents the results of sequential editing experiments performed on GPT and Llama language models using the ZsRE dataset.  Multiple editing methods (ROME, MEMIT, PMET, GRACE, and the proposed D4S) are compared across different numbers of edits (100, 500, and 1000). The table shows the performance of each method on three evaluation metrics (Efficacy, Paraphrase, and Specificity) along with the average performance.  The best result for each metric and edit number is highlighted in bold, while the suboptimal result is underlined.", "section": "5.1 Performance Comparison of Sequence Editing"}, {"figure_path": "xjXYgdFM5M/tables/tables_9_1.jpg", "caption": "Table 2: Sequencial edits on GPT and Llama with ZsRE dataset. The best results are in bold and underline means the suboptimal.", "description": "This table presents the performance comparison of different sequence editing methods on two large language models (LLMs): GPT and Llama. The evaluation is done on the factual triplet dataset zsRE, and the metrics used are efficacy (Eff.), paraphrase accuracy (Par.), specificity (Spe.), and their average (Avg.). Three different numbers of edits are tested: 100, 500, and 1000.  The results show the performance of each method under different editing intensities.  The best performance for each metric and average under each edit number are highlighted in bold. ", "section": "5.1 Performance Comparison of Sequence Editing"}, {"figure_path": "xjXYgdFM5M/tables/tables_13_1.jpg", "caption": "Table 3: Sequencial edits on GPT and Llama with Counterfact and Mquake dataset. The best results are in bold and underline means the suboptimal. Since GRACE is not suitable for the data structure of Mquake, we did not include it in the comparison.", "description": "This table presents the results of sequential edits on GPT and Llama language models using different knowledge editing methods (FT, ROME, MEMIT, GRACE, and D4S) and two different datasets (Counterfact and MQuake).  It shows the efficacy, paraphrase accuracy, specificity, and average performance of each method on each dataset.  The best-performing methods for each metric are highlighted in bold, while suboptimal results are underlined. The GRACE method is excluded from the MQuake dataset results due to incompatibility.", "section": "Appendix /Additional Experiments"}, {"figure_path": "xjXYgdFM5M/tables/tables_13_2.jpg", "caption": "Table 4: Downstream performance of llama after 10,000 edits.", "description": "This table presents the downstream task performance of the Llama model after 10,000 sequence edits.  It shows the performance metrics (presumably accuracy) on six different downstream tasks (arc_challenge, hellaswag, mmlu, truthfulqa_mc, winogrande) before any edits (Edit Num = 0) and after 10,000 edits. The values in parentheses represent the change in performance after the edits, showing whether the model improved or degraded.  The table helps assess the impact of the extensive sequence editing on the model's generalization abilities across various tasks.", "section": "5.2 Performance of Edited Model"}]