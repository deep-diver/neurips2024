[{"figure_path": "6LVxO1C819/figures/figures_1_1.jpg", "caption": "Figure 1: Overview of attack amplification through knowledge distillation. a) In the benign setting, KD reduces drift and brings benign local models closer to the benign global model. b) In the malicious setting, KD unknowingly reduces drift between benign local models and the poisoned global model.", "description": "This figure illustrates how knowledge distillation (KD) can exacerbate the effects of model poisoning attacks in federated learning (FL).  In a benign setting (a), KD helps align benign local models with the global model, reducing the effect of data heterogeneity. However, in a malicious setting (b), where a poisoned global model is present, KD inadvertently aligns benign local models with this poisoned model, amplifying the attack's impact and degrading overall performance. The arrows represent the drift of local models, and their convergence towards a global model shows the effect of KD.", "section": "3 Attack Amplification through Knowledge Distillation"}, {"figure_path": "6LVxO1C819/figures/figures_3_1.jpg", "caption": "Figure 2: Impact of increasing KL-divergence loss for Fed-NTD and contrastive loss for MOON on accuracy.", "description": "This figure shows the impact of increasing the KL-divergence loss (for FedNTD) and contrastive loss (for MOON) on accuracy in both benign and adversarial settings.  It presents three key results: benign accuracy (blue), post-attack accuracy (orange), and the accuracy drop (green).  The results demonstrate that increasing the loss improves global model accuracy in benign settings but simultaneously increases the accuracy drop after a model poisoning attack, highlighting the trade-off between performance in benign and adversarial conditions. The x-axis represents the hyperparameter values (\u03b2 for FedNTD and \u03bc for MOON), and the y-axis represents the accuracy.", "section": "3 Attack Amplification through Knowledge Distillation"}, {"figure_path": "6LVxO1C819/figures/figures_4_1.jpg", "caption": "Figure 3: Impact of the heterogeneity parameter, \u03b1 in benign and adversarial settings. We use the Dirichlet distribution where a higher \u03b1 means lower heterogeneity.", "description": "This figure compares the performance of FedNTD, MOON, and FedAvg under different levels of data heterogeneity (\u03b1 values) in both benign (no attack) and adversarial (model poisoning attack) settings.  The results show the tradeoff between accuracy in benign settings and robustness to attacks.  FedNTD and MOON initially exhibit higher accuracy than FedAvg under benign conditions, particularly at lower \u03b1 values (higher heterogeneity). However, in the presence of attacks, their performance degrades significantly more than FedAvg.  The higher the heterogeneity (lower \u03b1), the more the accuracy drops under attacks for both FedNTD and MOON, highlighting the vulnerability of these KD-based FL techniques to model poisoning attacks when heterogeneity is high.", "section": "Impact of Heterogeneity"}, {"figure_path": "6LVxO1C819/figures/figures_5_1.jpg", "caption": "Figure 4: HYDRA-FL framework: we refine client model training by reducing the final layer's KD-loss and incorporating shallow KD-loss at an earlier shallow layer via an auxiliary classifier.", "description": "The figure illustrates the HYDRA-FL framework, a hybrid knowledge distillation method for robust and accurate federated learning.  It shows how client model training is refined by reducing the knowledge distillation (KD) loss at the final layer and incorporating a shallow KD loss at an earlier layer using an auxiliary classifier. This approach helps mitigate the amplification of poisoning attacks while maintaining good performance in benign settings. The diagram details the interaction between clients, an auxiliary classifier, and the server during the model training process.", "section": "4 HYDRA-FL: Hybrid Knowledge Distillation for Robust and Accurate FL"}, {"figure_path": "6LVxO1C819/figures/figures_7_1.jpg", "caption": "Figure 5: HYDRA-FL vs. MOON and FedAvg when auxiliary classifiers are placed at different shallow layers.", "description": "This figure compares the performance of HYDRA-FL with different placements of auxiliary classifiers (HYDRA-FL-S1 and HYDRA-FL-S2), MOON, and FedAvg under both benign (no attack) and malicious (attack) conditions.  The x-axis represents the Dirichlet distribution parameter (\u03b1), indicating the level of data heterogeneity, with higher values indicating lower heterogeneity.  The y-axis represents the test accuracy.  The plot reveals how HYDRA-FL consistently maintains comparable performance to FedAvg and MOON in benign scenarios while significantly outperforming both algorithms in attack settings across various heterogeneity levels. The placement of the auxiliary classifier impacts performance, suggesting optimal positioning for robust defense against model poisoning attacks.", "section": "5.2 Shallow Not-True Distillation and 5.3 Shallow MOON"}, {"figure_path": "6LVxO1C819/figures/figures_8_1.jpg", "caption": "Figure 1: Overview of attack amplification through knowledge distillation. a) In the benign setting, KD reduces drift and brings benign local models closer to the benign global model. b) In the malicious setting, KD unknowingly reduces drift between benign local models and the poisoned global model.", "description": "This figure illustrates the effect of knowledge distillation (KD) on federated learning (FL) models under benign and malicious (attack) conditions.  In the benign setting (Figure 1a), KD successfully reduces the differences between the local models trained on diverse data and the global model, thus improving model performance.  In contrast, in the malicious setting (Figure 1b), where some malicious local models are introduced, KD causes the benign local models to align with the poisoned global model (the poisoned global model is not depicted explicitly in the figure). This phenomenon, known as \"attack amplification\", results in a significant performance degradation.  The figure highlights the vulnerability of KD-based FL techniques to model poisoning attacks.", "section": "3 Attack Amplification through Knowledge Distillation"}, {"figure_path": "6LVxO1C819/figures/figures_9_1.jpg", "caption": "Figure 7: Comparison of performance of FedNTD-S with different values of \u03b2", "description": "This figure shows the impact of varying the diminishing factor (\u03b2) on the post-attack accuracy of FedNTD with shallow distillation (FedNTD-S) and HYDRA-FL across different levels of data heterogeneity (\u03b1).  It demonstrates that reducing \u03b2, which effectively reduces the weight of the knowledge distillation loss at the output layer, significantly improves the post-attack accuracy, especially at high heterogeneity levels (small \u03b1).  This highlights the effectiveness of HYDRA-FL's approach in mitigating the attack amplification problem caused by over-reliance on final-layer alignment in knowledge distillation.", "section": "6.3 Ablation Study"}]