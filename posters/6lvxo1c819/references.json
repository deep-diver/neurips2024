{"references": [{"fullname_first_author": "H. Brendan McMahan", "paper_title": "Communication-efficient learning of deep networks from decentralized data", "publication_date": "2017-00-00", "reason": "This paper introduces federated learning (FL), the core concept the current paper builds upon and analyzes."}, {"fullname_first_author": "Geoffrey Hinton", "paper_title": "Distilling the knowledge in a neural network", "publication_date": "2015-00-00", "reason": "This paper introduces knowledge distillation (KD), a key technique that is analyzed and improved in the current paper."}, {"fullname_first_author": "Minghong Fang", "paper_title": "Local Model Poisoning Attacks to Byzantine-Robust Federated Learning", "publication_date": "2020-00-00", "reason": "This paper identifies model poisoning attacks, a critical security vulnerability in FL that the current paper addresses."}, {"fullname_first_author": "Gihun Lee", "paper_title": "Preservation of the global knowledge by not-true distillation in federated learning", "publication_date": "2022-00-00", "reason": "This paper proposes FedNTD, a specific KD-based FL algorithm that is used as a baseline and improved in the current paper."}, {"fullname_first_author": "Qinbin Li", "paper_title": "Model-contrastive federated learning", "publication_date": "2021-00-00", "reason": "This paper proposes MOON, another KD-based FL algorithm used as a baseline and improved by the current paper."}]}