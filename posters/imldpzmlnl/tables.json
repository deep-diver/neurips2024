[{"figure_path": "IMlDpZmLnL/tables/tables_4_1.jpg", "caption": "Table 1: Learning curve in the over-parameterized regime (p > n): n is the sample size, a, r > 0 define the eigen-decay rates of the kernel and target function, b > 0 controls the decay rate of the ridge regularization parameter (Assumptions (PE) and (EE)), \u03c3\u00b2 def. E [\u03b5\u00b2] is the noise level, and s > 0 is a technical parameter often determined by a and r (e.g. under Assumption (SC)). Here \u0161 def min{s, 2}. Results in blue indicate either previously unstudied regimes or improvements in available rates in a studied regime. See Table 6 for more comparisons and Subsection 2.2 for details on various settings.", "description": "This table summarizes the results of the over-parameterized regime (p>n) of kernel ridge regression.  It shows the asymptotic upper bounds for the bias and variance terms in the test error, expressed as functions of the sample size (n). The table considers various settings defined by combinations of polynomial/exponential eigen-decay, strong/weak ridge regularization, independent/generic features, and source coefficient (s).  Improvements and novel results are highlighted in blue.", "section": "3 Main result"}, {"figure_path": "IMlDpZmLnL/tables/tables_7_1.jpg", "caption": "Table 2: The table shows whether the lower bound is matching the upper bound deduced in this paper.", "description": "This table summarizes whether the lower bounds derived in the paper match the upper bounds for bias (B) and variance (V) in different settings of kernel ridge regression.  The settings include strong vs. weak ridge regularization and independent vs. generic features. The table shows whether a matching lower bound could be proven for each setting, indicating the tightness of the upper bounds established in the paper.", "section": "Main result"}, {"figure_path": "IMlDpZmLnL/tables/tables_30_1.jpg", "caption": "Table 1: Learning curve in the over-parameterized regime (p > n): n is the sample size, a, r > 0 define the eigen-decay rates of the kernel and target function, b > 0 controls the decay rate of the ridge regularization parameter (Assumptions (PE) and (EE)), \u03c3\u00b2 def. E[\u03f5\u00b2] is the noise level, and s > 0 is a technical parameter often determined by a and r (e.g. under Assumption (SC)). Here s def min{s, 2}. Results in blue indicate either previously unstudied regimes or improvements in available rates in a studied regime. See Table 6 for more comparisons and Subsection 2.2 for details on various settings.", "description": "This table summarizes the main results of the paper regarding the learning curve in the over-parameterized regime (when the number of features p is greater than the sample size n). It shows how the bias (B) and variance (V) terms of the test error decompose, depending on various factors: the type of eigen-decay (polynomial or exponential), the strength of the ridge regularization, whether the features are independent or generic, and the source condition (SC). The table also highlights novel bounds and improvements over existing results.", "section": "3 Main result"}, {"figure_path": "IMlDpZmLnL/tables/tables_38_1.jpg", "caption": "Table 1: Learning curve in the over-parameterized regime (p > n): n is the sample size, a, r > 0 define the eigen-decay rates of the kernel and target function, b > 0 controls the decay rate of the ridge regularization parameter (Assumptions (PE) and (EE)), \u03c3\u00b2 def. E [\u03b5\u00b2] is the noise level, and s > 0 is a technical parameter often determined by a and r (e.g. under Assumption (SC)). Here s def min{s, 2}. Results in blue indicate either previously unstudied regimes or improvements in available rates in a studied regime. See Table 6 for more comparisons and Subsection 2.2 for details on various settings.", "description": "This table summarizes the main results of the paper regarding the learning curve of kernel ridge regression in the over-parameterized regime (p>n). It shows the asymptotic bounds (in terms of the sample size n) for the bias (B) and variance (V) terms of the test error under various combinations of assumptions regarding the kernel eigen-decay, ridge regularization, noise level, target function smoothness, and feature vector properties (independent or generic features). The table highlights improvements over existing bounds and identifies scenarios where the Gaussian Equivalence Property holds. The table also distinguishes between the strong and weak ridge regimes. ", "section": "3 Main result"}, {"figure_path": "IMlDpZmLnL/tables/tables_57_1.jpg", "caption": "Table 1: Learning curve in the over-parameterized regime (p > n): n is the sample size, a, r > 0 define the eigen-decay rates of the kernel and target function, b > 0 controls the decay rate of the ridge regularization parameter (Assumptions (PE) and (EE)), \u03c3\u00b2 def. E[\u03f5\u00b2] is the noise level, and s > 0 is a technical parameter often determined by a and r (e.g. under Assumption (SC)). Here \u0161 def min{s, 2}. Results in blue indicate either previously unstudied regimes or improvements in available rates in a studied regime. See Table 6 for more comparisons and Subsection 2.2 for details on various settings.", "description": "This table summarizes the main results of the paper regarding the learning curve in the over-parameterized regime (where the number of features p is greater than the sample size n). It shows how the bias (B) and variance (V) terms of the test error decompose, depending on various factors such as the type of eigen-decay (polynomial or exponential), the type of ridge regularization (strong or weak), whether the features are independent or generic, and the source condition (SC).  The table provides asymptotic upper bounds (and in some cases matching lower bounds) on the test error expressed as a function of the sample size n and other relevant parameters.  Results that improve upon or extend previous results are highlighted in blue.", "section": "3 Main result"}, {"figure_path": "IMlDpZmLnL/tables/tables_57_2.jpg", "caption": "Table 1: Learning curve in the over-parameterized regime (p > n): n is the sample size, a, r > 0 define the eigen-decay rates of the kernel and target function, b > 0 controls the decay rate of the ridge regularization parameter (Assumptions (PE) and (EE)), \u03c3\u00b2 def. E [\u03b5\u00b2] is the noise level, and s > 0 is a technical parameter often determined by a and r (e.g. under Assumption (SC)). Here \u0161 def min{s, 2}. Results in blue indicate either previously unstudied regimes or improvements in available rates in a studied regime. See Table 6 for more comparisons and Subsection 2.2 for details on various settings.", "description": "This table summarizes the results of the analysis of the learning curve in the over-parameterized regime (p>n) under various combinations of assumptions and settings. It shows the asymptotic bounds of the bias (B) and variance (V) terms of the test error for both polynomial and exponential eigen-decay rates.  The table also highlights the difference in bounds between independent and generic features, and how the results differ under strong and weak ridge regularization.  Results in blue represent either previously unstudied cases or improvements over existing bounds.", "section": "3 Main result"}]