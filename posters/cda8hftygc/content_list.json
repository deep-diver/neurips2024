[{"type": "text", "text": "Truthfulness of Calibration Measures ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Nika Haghtalab, Mingda Qiao, Kunhe Yang, and Eric Zhao ", "page_idx": 0}, {"type": "text", "text": "University of California, Berkeley [nika,mingda.qiao,kunheyang,eric.zh?@berkeley.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We study calibration measures in a sequential prediction setup. In addition to rewarding accurate predictions (completeness) and penalizing incorrect ones (soundness), an important desideratum of calibration measures is truthfulness, a minimal condition for the forecaster not to be incentivized to exploit the system. Formally, a calibration measure is truthful if the forecaster (approximately) minimizes the expected penalty by predicting the conditional expectation of the next outcome, given the prior distribution of outcomes. We conduct a taxonomy of existing calibration measures. Perhaps surprisingly, all of them are far from being truthful. We introduce a new calibration measure termed the Subsampled Smooth Calibration Error (SSCE), which is complete and sound, and under which truthful prediction is optimal up to a constant multiplicative factor. In contrast, under existing calibration measures, there are simple distributions on which a polylogarithmic (or even zero) penalty is achievable, while truthful prediction leads to a polynomial penalty. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Probability forecasting is a central prediction task to a wide range of domains and applications, such as finance, meteorology, and medicine [MW84, DF83, WM68, JOKOM12, KSB21, VCV15, $\\mathbf{B}\\mathbf{F}^{+}02$ CAT16]. For forecasts to be useful, a common minimum requirement is that they are calibrated, i.e., the predictions are unbiased conditioned on the predicted value. Formally, for a sequence of $T$ binary events, a forecaster who predicts probabilities in [O, 1] is perfectly calibrated if for every $\\alpha\\in[0,1]$ among the time steps on which $\\alpha$ is predicted, an $\\alpha$ fraction of the outcomes is indeed 1. Since perfectly calibrated forecasts are often unachievable, calibration measures have been introduced to quantify some form of deviation from perfectly calibrated forecasts. Common examples of these measures include the expected calibration error (ECE) [FV98], the smooth calibration error [KF08], and the distance from calibration [BGHN23]. ", "page_idx": 0}, {"type": "text", "text": "As these calibration measures are commonly used to evaluate the performance of forecasters, it is important that their use encourages forecasters to incorporate the highest quality information available to them (e.g., via their expert knowledge or side information) about the next outcome. This desideratum, formally referred to as truthfulness, requires that a calibration measure incentivizes the forecasters to predict truthfully when the true distribution of the next outcome is known to them. Lack of truthfulness can have severe consequences: it serves as a poor measure of quality of forecasts, tempts forecasters to make deliberately biased predictions in order to game the system, and erodes trust in predictions provided by third-party forecasters. Given the importance of truthfulness, we set outtoidentifycalibrationmeasuresthatdemonstratetruthfulness. ", "page_idx": 0}, {"type": "text", "text": "While truthfulness of calibration measures has not been systematically investigated to date, evidence of the lack of truthfulness of some calibration measures has emerged in recent literature. For example, [FH21, QV21] noted that a forecaster can lower their ECE by predicting according to the past. This observation was applied in the algorithm of [FH21] and motivated the \u201csidestepping\u201d technique in the lower bound proof of [QV21]. More recently, [QZ24] highlighted a large gap in the truthfulness of a recently proposed calibration measure (called the distance from calibration [BGHN23]) by showing that in a simple setup of predicting i.i.d. outcomes, the truthful forecaster incurs a distance of $\\Omega({\\sqrt{T}})$ from calibration but there is a forecasting algorithm that achieves polylog $(T)$ distance from calibration. We call this a $\\mathrm{polylog}(T)\\ \u2013\\Omega(\\sqrt{T})$ truthfulness gap. On the other hand, we say that a calibration measure is $(\\alpha,\\beta)$ -truthful if predicting the next outcome according to its conditional distribution incurs a measure that is no more than $\\alpha{\\mathsf{O P T}}+\\beta$ ,where $\\mathsf{O P T}$ is the minimum value of the calibration measure achievable by any forecaster. Faced with evidence that some calibration measures suffer from large truthfulness gaps, we will systematically examine the truthfulness (or a gap thereof) of a wide range of calibration measures. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "For a truthful calibration measure to also be useful it must distinguish accurate predictions from inaccurate ones. After all, a measure that is uniformly O regardless of the quality of predictions is perfectly truthful (formally $(1,0)$ -truthful) but provides no insights into the quality of the predictions. We formalize the minimum requirement for a measure to be useful by its completeness and soundness when predicting i.i.d. Bernoulli outcomes. The former requires that predicting the outcomes according to the correct parameter of the generating Bernoulli distribution incurs no or $o(T)$ penalty, whereas the latter requires the penalty to be $\\Omega(T)$ when predictions systematically deviate from the correct parameter. An equally important feature of a calibration measure is that it defines an ideal that could be asymptotically achieved for all prediction tasks. This is formalized by the existence of forecasting algorithms with an $o(T)$ penalty in the adversarial sequential prediction setting [FV98], where the sequence of outcomes is produced by an adaptive adversary. ", "page_idx": 1}, {"type": "text", "text": "With these desiderata in place (namely truthfulness, soundness, completeness, and asymptotic calibration), we ask whether there are calibration measures that simultaneously satisfy all these criteria? We answer this question in three parts: ", "page_idx": 1}, {"type": "text", "text": "Part I: We show that existing calibration measures do not simultaneously meet these criteria. We conduct a taxonomy of several existing calibration measures in terms of their completeness, soundness and truthfulness (formally defined in Section 2). We show that almost all of them have large truthfulness gaps: There are simple distributions on which an $O(1)$ (or even zero) penalty is achievable, while truthful predictions lead to a $\\mathrm{poly}(T)$ penalty; see Table 1 for details. ", "page_idx": 1}, {"type": "text", "text": "Indeed, this lack of truthfulness is not limited to specific or contrived distributions. In the next theorem which we will prove in Appendix B, we strengthen these findings by showing that a commonly used notion of calibration systematically suffers large truthfulness gaps in most forecasting instances. ", "page_idx": 1}, {"type": "text", "text": "Theorem 1.1 (Informal). For every product distribution with marginals bounded away from 0 and 1, thetruthfulforecasterincurs $\\Omega({\\sqrt{T}})$ smooth calibration error but there exists a forecastingalgorithm thatincursonlypolylog $(T)$ smoothcalibrationerror. ", "page_idx": 1}, {"type": "text", "text": "A notable exception in Table 1 is the class of calibration measures induced by proper scoring rules, i.e., loss functions for probabilistic predictions that are optimized by truthful forecasts. By definition, these calibration measures are $(1,0)$ -truthful. However, none of them is complete: as we show in Appendix A, even on i.i.d. Bernoulli trials, the optimal and truthful predictions incur an $\\Omega(T)$ penalty. ", "page_idx": 1}, {"type": "text", "text": "Part II: We introduce a new calibration measure, called SSCE, that is sound, complete, and approximately truthful. We do this using a simple adjustment to an existing notion of calibration measure: we subsample a subset of the time steps and evaluate the smooth calibration error [KF08] on this sampled set only. We call this the Subsampled Smooth Calibration Error (SSCE) and formally define it in Section 2. Our main result is that SSCE is $(O(1),0)$ -truthful. ", "page_idx": 1}, {"type": "text", "text": "Theorem 1.2 (Main Theorem). There exists a universal constant $c\\,>\\,0$ such that theSSCE is $(c,0)$ -truthful. Furthermore, the SSCE is complete and sound. ", "page_idx": 1}, {"type": "text", "text": "As shown in Table 1, to the best of our knowledge, SSCE is the first calibration measure that simultaneously achieves completeness, soundness, and non-trivial truthfulness. ", "page_idx": 1}, {"type": "text", "text": "While our methodology for constructing this calibration measure is simple, the analytical steps required to establish the $(O(1),0)$ -truthfulness guarantee are far from simple. We dedicate most of the main body of this paper to illustrating the proof ideas in a series of warmups to Theorem 1.2. ", "page_idx": 1}, {"type": "text", "text": "Part II: There is a forecasting algorithm that achieves $O({\\sqrt{T}})$ SsCE even in the adversarial setting.  While our study of truthfulness of calibration measures is necessarily focused on when the forecaster knows the conditional distribution of the next outcome, it is important to ensure that, even in the adversarial setting, a sublinear penalty can be achieved for this calibration measure. For this, we study the sequential calibration setting (e.g., [FV98]) where the outcome at time $t$ ischosenby an adaptive adversary who has observed the sequence of earlier outcomes and predictions. We show that an $O({\\sqrt{T}})$ SSCE is achievable. ", "page_idx": 1}, {"type": "table", "img_path": "cDa8hfTyGc/tmp/18fd6c0df8a3e8d2109a4506b26d1d31e7943ad74e998549e3057ef8a10cdf44.jpg", "table_caption": [], "table_footnote": ["Table 1: Evaluation of existing calibration measures along with SSCE, in terms of completeness, soundness and truthfulness (Definitions 2.2 and 2.5). An $\\alpha{-}\\beta$ truthfulness gap means that there is a prediction instance on which forecasting according to the true conditional distribution of the next outcome incurs more than $\\beta$ penalty, but there is a forecasting strategy that incurs at most $\\alpha$ penalty. See Appendix A for more details. "], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Theorem 1.3. In the adversarial sequential calibration setting, there is a deterministic strategy for theforecaster that achievesan $O({\\sqrt{T}})$ SSCE. ", "page_idx": 2}, {"type": "text", "text": "An interesting and important feature of this result is that it achieves an $O({\\sqrt{T}})$ ratewhereas an $O({\\sqrt{T}})$ rate for the expected calibration error is known to be impossible to achieve [QV21]. Together our Theorems 1.2 and 1.3 establish that SSCE is a truthful and useful calibration measure. ", "page_idx": 2}, {"type": "text", "text": "1.1 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "There is a large body of work on calibration, a notion that dates back to the 1950s [Bri50, Daw82, Daw85] and has been applied to game theory [FV97, HPY23], machine learning [GPSW17], and algorithmic fairness [KMR17, $\\bar{\\mathrm{PRW}}^{+}17$ , HJKRR18, HJZ23]. We will restrict our discussion to sequential calibration and the systematic study of calibration measures, which are the closest to this Work. ", "page_idx": 2}, {"type": "text", "text": "Sequential calibration. Foster and Vohra [FV98] first proved that one can achieve asymptotic calibration on arbitrary and adaptive outcomes. Formally, they gave a forecasting algorithm with an $O(T^{2/3})$ ECE in expectation, when predicting $T$ binary outcomes chosen by an adaptive adversary. Subsequent work gave alternative and simpler proofs of the result [FL99, Fos99, Har22], extended the result to other calibration measures [KF08, FH18, FH21, QZ24], and proved lower bounds on the optimal ECE [QV21]. Most closely related to our approach is the work of [FRST11], who studied a stronger notion that requires calibration on a family of checking rules, where each checking rule specifies a subset of the time horizon. Despite the apparent similarity, their notion is qualitatively different from the SSCE, since we take an expectation over the subsampled horizon, whereas they take the maximum. In particular, no forecaster can be calibrated in their definition if the checking rule family contains all subsets of $[T]$ , since there always exists a checking rule that strongly correlates with the outcomes. ", "page_idx": 2}, {"type": "text", "text": "Calibration measures. The recent work of Blasiok, Gopalan, Hu and Nakkiran [BGHN23] initiated the rigorous study of calibration measures. Their work focused on the offline setup, where there is a known marginal distribution over the feature space, and each predictor maps the feature space to $[0,1]$ . They proposed to use the distance from calibration\u2014the $\\ell_{1}$ distance from the predictor to the closest predictor that is perfectly calibrated\u2014-as the ground truth, and studied whether existing calibration measures are consistent with it. Note that completeness and soundness are defined differently in [BGHN23]: a calibration measure is called complete (resp., sound) if it is upper (resp., lower) bounded by a polynomial of the distance from calibration. Since the distance from calibration is far from being truthful in the online setup (as shown by [QZ24]), our definition of completeness and soundness set up minimal conditions for an error metric to be regarded as measuring calibration, rather than enforcing closeness to the distance from calibration. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Subsampling.  Our new calibration measure is derived from subsampling the time horizon. This simple idea has been shown to be effective in various different contexts, including privacy amplification in differential privacy (e.g.,[Ste22, Section 6]), handling adversarial corruptions [BLMT22], as well as adaptive data analysis [Bla23]. ", "page_idx": 3}, {"type": "text", "text": "Proper scoring rules.  Proper scoring rules [WM68] are error metrics for probabilistic forecasts that are optimized when the forecaster predicts according to the true distribution. While the error metrics induced by proper scoring rules are (perfectly) truthful by definition, as we show in Appendix A, they are qualitatively different from the usual calibration measures and, in particular, do not meet the completeness criterion. We note that a recent line of work [CY21, NNW21, LHSW22, PW22, HSLW23] studied the optimization of scoring rules, namely, finding the proper scoring rule that maximally incentivizes the forecaster to exert effort to obtain additional information. ", "page_idx": 3}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Sequential prediction.We consider the following prediction setup: First, a sequence $x\\in\\{0,1\\}^{T}$ is sampled from distribution $\\mathcal{D}$ . At each step $t\\in[T]$ , the forecaster makes a prediction $p_{t}\\in[0,1]$ , after which $x_{t}$ is revealed. Formally, a deterministic forecaster is a function $\\mathcal{A}:\\bigcup_{t=1}^{T}\\{0,1\\}^{t-\\dot{1}}\\rightarrow[0,1]$ where $\\mathcal{A}(b_{1},b_{2},\\ldots,b_{t-1})$ specifies the forecaster's prediction at step $t$ if the first $t-1$ observations match $b_{1:(t-1)}$ . Distribution $\\mathcal{D}$ and forecaster $\\boldsymbol{\\mathcal{A}}$ naturally induce a joint distribution of $(x,p)\\in$ $\\{0,1\\}^{T}\\times[0,1]^{T}$ via sampling $x\\sim\\mathcal{D}$ and predicting $p_{t}={\\mathcal{A}}(x_{1},x_{2},.~.~.~,x_{t-1})$ ", "page_idx": 3}, {"type": "text", "text": "Note that we could have defined the forecaster as a function of both the outcomes $x_{1:(t-1)}$ and the predictions $p_{1:(t-1)}$ in the past. This altenative dfnition s equivalent to ours, since $p_{1:(t-1)}$ would be uniquely determined by $x_{1:(t-1)}$ .We could also have considered randomized forecasters, which are specified by distributions over deterministic forecasters. However, as we will see later, restricting our attention to deterministic forecasters does not affect the subsequent definitions. ", "page_idx": 3}, {"type": "text", "text": "Calibration measures.  The quality of the forecaster's predictions in the setting above is quantified by calibration measures. Formally, a calibration measure ${\\mathsf{C M}}$ is a family of functions $\\{\\mathsf{C}\\mathsf{M}_{T}:T\\in\\mathbb{N}\\}$ where each $\\mathsf{C M}_{T}$ maps $\\{0,1\\}^{T}\\times[0,1]^{T}$ to $[0,T]$ . We will frequently omit the subscript $T$ , since it is usually clear from the context. With respect to calibration measure CM, the expected penalty incurred by forecaster $\\boldsymbol{\\mathcal{A}}$ on distribution $\\mathcal{D}$ is defined as $\\mathsf{e r r}_{\\mathsf{C M}}(\\mathscr{D},\\mathscr{A}):=\\mathbb{E}_{(x,p)\\sim(\\overline{{\\mathscr{D}}},\\mathscr{A})}\\left[\\mathsf{C M}(x,p)\\right]$ where $(x,p)\\sim(D,A)$ denotes sampling a sequence $x$ and predictions $p$ from the joint distribution induced by $\\mathcal{D}$ and $\\boldsymbol{\\mathcal{A}}$ ", "page_idx": 3}, {"type": "text", "text": "One example of calibration measures is the smooth calibration error introduced by [KFo8] that is defined as $\\begin{array}{r}{\\mathsf{s m C E}(x,p):=\\operatorname*{sup}_{f\\in{\\mathcal F}}\\sum_{t=1}^{T}f(p_{t})(x_{t}-p_{t})}\\end{array}$ , where $\\mathcal{F}$ is the family of 1-Lipschitz functions from $[0,1]$ to $[-1,1]$ . In this work, we introduce a new calibration measure called Subsampled Smooth Calibration Error (SSCE) that is defined by subsampling a subset of the time horizon, and evaluating the smooth calibration error on it. We will formally define this measure next. In the following. $\\mathsf{U n i f}(S)$ denotes the uniform distribution over a finite set $S$ . For a $T$ -dimensional vector $x$ and $S\\subseteq[T],x|_{S}$ denotes the $|S|$ -dimensional vector formed by the entries of $x$ indexed by $S$ ", "page_idx": 3}, {"type": "text", "text": "Definition 2.1 (Subsampled Smooth Calibration Error). For a sequence of outcomes $x\\in\\{0,1\\}^{T}$ andpredictions $\\boldsymbol{p}\\in[0,\\dot{1}]^{T}$ , the Subsampled Smooth Calibration Error (SSCE) is defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathsf{S S C E}(x,p):=\\underset{S\\sim\\mathsf{U n i f}(2^{(T)})}{\\mathbb{E}}[\\mathsf{s m C E}(x|s,p|s)]=\\underset{y\\sim\\mathsf{U n i f}(\\{0,1\\}^{T})}{\\mathbb{E}}\\left[\\underset{f\\in\\mathcal F}{\\operatorname*{sup}}\\sum_{t=1}^{T}y_{t}\\cdot f(p_{t})\\cdot\\big(x_{t}-p_{t}\\big)\\right].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Completeness and soundness. We give minimal conditions for a calibration measure to be regarded as complete (intuitively \u201caccurate\u201d predictions have a small penalty) and sound (intuitively \u201cinaccurate' predictions have a large penalty). ", "page_idx": 3}, {"type": "text", "text": "Definition 2.2 (Completeness and soundness). A calibration measure CM is complete if: $(I)$ For any $x\\in\\{0,1\\}^{T}$ \uff0c $\\mathsf{C}\\mathsf{M}_{T}(x,x)=0$ :(2) For any $\\alpha\\in[0,1]$ $\\left[\\mathsf{C}\\mathsf{M}_{T}(x,\\alpha\\cdot\\vec{1}_{T})\\right]=$ $o_{\\alpha}(T)$ Thecalibrationmeasureissound if: $(I)$ For any $x\\in\\{0,1\\}^{T}$ \uff0c ${\\mathsf{C M}}_{T}(x,\\vec{1}_{T}-x)=\\Omega(T),$ (2) For any $\\alpha,\\beta\\in[0,1]$ such that $\\alpha\\neq\\beta$ $\\mathbb{E}_{x_{1},\\dots,x_{T}\\sim\\mathsf{B e r n o u l l i}(\\alpha)}\\left[\\mathsf{C M}_{T}(x,\\beta\\cdot\\vec{1}_{T})\\right]=\\Omega_{\\alpha,\\beta}(T)$ Here, $o_{\\alpha}(\\cdot)$ and $\\Omega_{\\alpha,\\beta}(\\cdot)$ may hide constant factors that depend on the parameters in the subscript. ", "page_idx": 4}, {"type": "text", "text": "Truthfulness. To define the truthfulness of a calibration measure, we introduce the truthful forecaster and the optimal error for a distribution $\\mathcal{D}$ ", "page_idx": 4}, {"type": "text", "text": "Definition 2.3 (Truthful forecaster). With respect to distribution $\\mathcal{D}\\,\\in\\,\\Delta(\\{0,1\\}^{T})$ \uff0cthetruthful forecasteris defined as $\\begin{array}{r}{A^{\\mathrm{truthful}}({\\mathcal D})(b_{1},b_{2},\\ldots,b_{t-1}):=\\operatorname*{Pr}_{x\\sim{\\mathcal D}}\\left[x_{t}=1\\;\\middle|\\;x_{1:(t-1)}=b_{1:(t-1)}\\right]}\\end{array}$ ", "page_idx": 4}, {"type": "text", "text": "Arguably, $A^{\\mathrm{truthful}}({\\mathcal{D}})$ is the only forecaster that makes the \u201cright\u2019 predictions on distribution $\\mathcal{D}$ ", "page_idx": 4}, {"type": "text", "text": "Definition 2.4 (Optimal error). The optimal error on distribution $\\mathcal{D}\\in\\Delta(\\{0,1\\}^{T})$ withrespectto calibration measure CM is defined as ${\\mathsf{O P T}}_{\\mathsf{C M}}({\\mathcal{D}}):=\\operatorname*{inf}_{A}{\\mathsf{e r r}}_{\\mathsf{C M}}({\\mathcal{D}},{\\mathcal{A}})$ where $\\boldsymbol{\\mathcal{A}}$ rangesover all deterministicforecasters. ", "page_idx": 4}, {"type": "text", "text": "Note that by an averaging argument, the definition of ${\\mathsf{O P T}}_{\\mathsf{C M}}({\\mathcal{D}})$ is unchanged if we take an infimum over randomized forecasters. ", "page_idx": 4}, {"type": "text", "text": "A calibration measure is truthful if, on every distribution, the truthful forecaster is near-optimal. ", "page_idx": 4}, {"type": "text", "text": "Definition 2.5 (Truthfulness of calibration measures). A calibration measure CM is $(\\alpha,\\beta)$ -truthful $i f,$ for every $\\mathcal{D}\\;\\in\\;\\Delta(\\{0,1\\}^{T})$ \uff0c $\\mathsf{e r r}_{\\mathsf{C M}}\\big(\\mathcal{D},\\mathcal{A}^{\\mathrm{truthful}}(\\mathcal{D})\\big)\\;\\leq\\;\\alpha\\,\\cdot\\,\\mathsf{O P T}_{\\mathsf{C M}}(\\mathcal{D})\\,+\\,\\beta.$ Conversely, CM is said to have an $\\alpha{-}\\beta$ truthfulnessgap $i f,$ for some distribution $\\mathcal{D}$ $\\mathsf{O P T}_{\\mathsf{C M}}(D)\\,\\leq\\,\\alpha$ and $\\mathsf{e r r}_{\\mathsf{C M}}(\\mathscr{D},\\mathscr{A}^{\\mathrm{truthful}}(\\mathscr{D}))\\geq\\beta$ ", "page_idx": 4}, {"type": "text", "text": "3 Technical Overview ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we briefly discuss the main technical ideas and challenges behind the proofs of Theorems 1.1, 1.2, and 1.3. We provide more details on our main result, i.e., that SSCE is $(O(1),0)$ -truthful, in Sections 4 through 6. Theorem 1.3 follows from a recent result of [ACRS24] on minimizing the distance from calibration in the adversarial setup, along with a new result connecting SSCE to distance from calibration, and is proved in Section 7. We defer the proof of Theorem 1.1 to AppendixB. ", "page_idx": 4}, {"type": "text", "text": "A simple distribution that witnesses truthfulness gaps. Inspired by [QV21, Example 2], we consider the distribution $\\mathcal{D}$ specified as follows: The time horizon is divided into $T/3$ blocks of length 3, each with a uniformly random bit, followed by a zero and a one. Within each block, the truthful forecaster predicts $1/2$ , 0 and 1 in order. Then, among the steps on which $1/2$ is predicted, the frequency of ones is typically $1/2\\pm\\Theta(1/\\sqrt{T})$ . This deviation results in a $\\Theta({\\sqrt{T}})$ penalty under most calibration measures (concretely, all calibration measures in the first two rows of Table 1). ", "page_idx": 4}, {"type": "text", "text": "However, there is a different strategy that ensures perfect calibration, and thus a zero penalty under most calibration measures. Within each block, the forecaster predicts $1/2$ on the first step. If the bit turns out to be 1, the forecaster maintains perfect calibration by predicting $1/2$ on the second step, on which the outcome is known to be O; otherwise, the forecaster accomplishes the same by predicting $1/2$ on the third step. Therefore, the distribution $\\mathcal{D}$ witnesses a $0{-}\\Omega({\\sqrt{T}})$ truthfulness gap for every calibration measure in the first two rows of Table 1. ", "page_idx": 4}, {"type": "text", "text": "The importance of subsampling in the SSCE becomes apparent in light of the example above. On distribution $\\mathcal{D}$ , the truthful forecaster has to pay a $\\Theta({\\sqrt{T}})$ cost for the mild deviation from the expectation, while a strategic forecaster avoids this deviation by correlating the predictions with the biases in the past. With the subsampling, however, the forecaster is no longer sure about the biases that factor into the penalty. This ensures that, compared to truth-telling, the benefit from predicting strategically is marginal, and thus makes the truthfulness guarantee in Theorem 1.2 possible. ", "page_idx": 4}, {"type": "text", "text": "Establishing truthfulness via martingale inequalities. We prove that the SSCE is $(O(1),0)$ truthful in three steps: (1) Define a complexity measure $\\sigma(\\mathcal{D})$ of distribution $\\mathcal{D}$ ; (2) Show that errss $\\mathsf{c}\\mathsf{E}\\big(\\mathcal{D},\\mathcal{A}^{\\mathrm{truthful}}(\\mathcal{D})\\big)=O(\\sigma(\\mathcal{D}))$ (3) Show that $\\mathsf{O P T}_{\\mathsf{S S C E}}({\\mathcal{D}})=\\Omega(\\sigma({\\mathcal{D}}))$ ", "page_idx": 5}, {"type": "text", "text": "As we elaborate in Section 5, the crux of Step (2) is to control the expected deviation of a martingale $(M_{t})_{0\\leq t\\leq T}$ with respect to filtraion $\\left(\\mathbb{F}_{t}\\right)$ by the its realized variance $\\begin{array}{r}{\\mathrm{Var}_{t}:=\\sum_{s=1}^{t}\\mathrm{Var}\\left[M_{s}|\\vec{\\mathbb{F}_{s-1}}\\right]}\\end{array}$ which is highly non-trivial as the two processes $\\left(M_{t}\\right)$ and $\\left(\\mathrm{Var}_{t}\\right)$ are correlated. In more detail, the filtration $\\left(\\mathbb{F}_{t}\\right)$ corresponds to the randomness in $x\\sim\\mathcal{D}$ , while $\\left(M_{t}\\right)$ tracks the biases in the predictions (on a subset of the time horizon) tested by a Lipschitz function. We note that such a bound would easily follow from \u201coff-the-shelf\u201d concentration inequalities for martingales (e.g., Freedman's inequality [Fre75]), if the total realized variance $\\mathrm{Var}_{T}$ were uniformly bounded. However, in general, $\\mathrm{Var}_{T}$ may vary drastically, and directly applying these concentration inequalities would introduce an extra super-constant factor. Our workaround is a \u201cdoubling trick\" that divides the time horizon into epochs, the realized variances in which grow exponentially. We then apply Freedman's inequality to each epoch separately. In Section 5, we formulate a toy random walk problem that highlights this challenge and demonstrates our solution to it, which is of independent interest. ", "page_idx": 5}, {"type": "text", "text": "Similarly, as we show in Section 6, the crux of Step (3) is to establish another martingale inequality. We first show that for fixed $x$ and $p$ , we have ${\\sf S S C E}(x,p)~=~\\Omega(\\sqrt{N_{T}})$ , where $\\begin{array}{r}{N_{t}:=\\sum_{s=1}^{t}\\mathbb{1}\\left[|x_{s}-p_{s}|\\geq1/2\\right]}\\end{array}$ $x\\sim\\mathcal{D}$ theralized varance process $\\left(\\mathrm{Var}_{t}\\right)$ defined above is shown to lower bound $\\left(N_{t}\\right)$ , i.e., $\\left(N_{t}-\\mathrm{Var}_{t}\\right)$ is a sub-martingale. However, the desired result requires the lower bound $\\mathbb{E}\\left[\\sqrt{N_{T}}\\right]\\geq\\Omega(1)\\cdot\\mathbb{E}\\left[\\sqrt{\\mathrm{Var}_{T}}\\right]$ , which does not follow from E $\\mathrm{~\\boldmath~\\dot{~}{~\\left[N_{T}-V a r_{T}\\right]~}}\\geq0$ in general. This challenge necessitates a more careful analysis tailored to the specific properties of the processes $\\left(N_{t}\\right)$ and $\\left(\\mathrm{Var}_{t}\\right)$ ", "page_idx": 5}, {"type": "text", "text": "Deterministic forecasting strategy via reduction to $\\mathsf{s m C E}$ .We build on the result of [ACRS24] showing the existence of a deterministic forecasting strategy guaranteeing an $O({\\sqrt{T}})$ bound on $\\mathsf{s m C E}$ . In particular, we show via a standard chaining argument that SSCE is upper bounded by $\\mathsf{s m C E}$ plus a variance term that can be upper bounded by $O({\\sqrt{T}})$ . The result of [ACRS24] then implies a deterministic forecasting algorithm achieving an $O({\\sqrt{T}})$ SSCE. ", "page_idx": 5}, {"type": "text", "text": "4Warmup: The Product Distribution Case ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "As a warmup, in this section, we start by showing that SSCE is $(O(1),O(\\log T))$ -truthful for product distributions. This is a weaker version of Theorem 1.2 in terms of both the truthfulness parameters of SSCE and the restriction to product distributions. In Sections 5 and 6, we outline how we will remove these restrictions and improve the analysis of truthfulness. ", "page_idx": 5}, {"type": "text", "text": "For distribution $\\begin{array}{r}{\\mathcal{D}=\\prod_{t=1}^{T}\\mathsf{B e r n o u}\\lVert\\mathsf{i}(p_{t}^{\\star})}\\end{array}$ take $\\begin{array}{r}{\\sigma^{2}:=\\operatorname{Var}_{x\\sim\\mathcal{D}}\\left[\\sum_{t=1}^{T}x_{t}\\right]=\\sum_{t=1}^{T}p_{t}^{\\star}(1-p_{t}^{\\star})}\\end{array}$ as a complexity measure of the distribution of outcomes. We will show that e $\\mathsf{\\bar{r}r}_{\\mathsf{S S C E}}(\\mathcal{D},\\mathcal{A}^{\\mathrm{truthful}}(\\mathcal{D}))=$ $O(\\bar{\\sigma^{}}+\\log T)$ and ${\\mathsf{O P T}}_{\\mathsf{S S C E}}({\\mathcal{D}})={\\Omega}(\\sigma)-O(1)$ ", "page_idx": 5}, {"type": "text", "text": "4.1 Upper Bound the SSCE of the Truthful Forecaster ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We first show that the truthful forecaster for $\\mathcal{D}$ , which predicts $p_{t}\\,=\\,p_{t}^{\\star}$ at every step $t$ ,gives $\\mathbb{E}_{x\\sim\\mathcal{D}}\\left[55\\mathsf{C E}(x,p^{\\star})\\right]=O(\\sigma+\\log T)$ . For this purpose, it suffices to prove ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}\\left[{\\sf s m C E}(x,p^{\\star})\\right]=O(\\sigma+\\log T),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "since for each fixed $\\begin{array}{r}{S\\subseteq\\;[T]}\\end{array}$ ,applying (1) to $x|_{S}$ and $p^{\\star}|_{S}$ gives $\\mathbb{E}_{x\\sim>D}\\left[{\\sf s m C E}(x|_{S},p^{\\star}|_{S})\\right]\\ \\le$ $O(\\sigma+\\log T)$ , and taking an expectation over $S\\sim\\mathsf{U n i f}(2^{[T]})$ gives the desired bound on SSCE. Recall that $\\begin{array}{r}{\\mathbb{E}\\left[\\mathsf{s m C E}(x,p^{\\star})\\right]=\\mathbb{E}\\left[\\operatorname*{sup}_{f\\in\\mathcal{F}}\\sum_{t=1}^{T}f(p_{t}^{\\star})\\cdot(x_{t}-p_{t}^{\\star})\\right]\\!.}\\end{array}$ If we replace $\\mathcal{F}$ with the family of constant functions from $[0,1]$ to $[-1,1]$ , the right-hand side would reduce to ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname{\\mathbb{E}}_{x\\sim\\mathcal{D}}\\left[\\left|\\sum_{t=1}^{T}(x_{t}-p_{t}^{\\star})\\right|\\right]\\leq\\sqrt{\\operatorname{\\mathbb{E}}_{x\\sim\\mathcal{D}}\\left[\\left(\\sum_{t=1}^{T}(x_{t}-p_{t}^{\\star})\\right)^{2}\\right]}=\\sqrt{\\operatorname{Var}_{x\\sim\\mathcal{D}}\\left[\\sum_{t=1}^{T}x_{t}\\right]}=\\sigma.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Therefore, to prove the upper bound in (1), we need to show that the family of one-dimensional Lipschitz functions is not significantly richer than constant functions. ", "page_idx": 6}, {"type": "text", "text": "At a high level, this is done by taking finite coverings of Lipschitz functions and using Dudley's chaining technique [Dud87] to upper bound the value of this stochastic process. In more detail, let ${\\mathcal{F}}_{\\delta}$ be the smallest $\\delta$ -covering of $\\mathcal{F}$ in the uniform norm, i.e., for each $f\\in\\mathcal F$ , there exists $f_{\\delta}\\in\\mathcal{F}_{\\delta}$ such that $\\|f-f_{\\delta}\\|_{\\infty}\\leq\\delta$ Itis wll-known that $|{\\mathcal{F}}_{\\delta}|=e^{O(1/\\delta)}$ , and a chaining argument givs ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}\\left[\\underset{f\\in\\mathcal{F}}{\\operatorname*{sup}}\\sum_{t=1}^{T}f(p_{t}^{\\star})\\cdot(x_{t}-p_{t}^{\\star})\\right]\\le1+\\sum_{k=0}^{O(\\log T)}\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}\\left[\\underset{g\\in\\mathcal{G}_{2^{-k}}}{\\operatorname*{max}}\\sum_{t=1}^{T}g(p_{t}^{\\star})\\cdot(x_{t}-p_{t}^{\\star})\\right],\n$$", "text_format": "latex", "page_idx": 6}, {"type": "equation", "text": "$\\mathcal{G}_{\\delta}:=\\{f_{\\delta}-f_{\\delta/2}:f_{\\delta}\\in\\mathcal{F}_{\\delta},f_{\\delta/2}\\in\\mathcal{F}_{\\delta/2},\\|f_{\\delta}-f_{\\delta/2}\\|_{\\infty}\\le3\\delta/2\\}.$ ", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "It remains to bound the second term of (2). Note that for a fixed $g$ ,because of the independence of $x_{t}\\mathbf{s}$ $g(p_{t}^{\\star})\\cdot(x_{t}-p_{t}^{\\star})$ is independent across $t\\in[T]$ . Therefore, we can control the tail probability of $\\begin{array}{r}{\\sum_{t=1}^{T}g(p_{t}^{\\star})\\cdot(x_{t}-p_{t}^{\\star})}\\end{array}$ by Bernstein inequalities. For each fixed $\\delta$ using a Bernstein tail bound, taking a union bound over $g\\in\\mathcal{G}_{\\delta}$ , and noting that $|{\\mathcal G}_{\\delta}|\\le|{\\mathcal F}_{\\delta}|\\cdot|{\\mathcal F}_{\\delta/2}|=e^{O(1/\\delta)}$ , we have ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}\\left[\\operatorname*{max}_{g\\in\\mathcal{G}_{\\delta}}\\sum_{t=1}^{T}g(p_{t}^{\\star})\\cdot(x_{t}-p_{t}^{\\star})\\right]\\le O(\\delta)\\cdot O\\left(\\sqrt{\\sigma^{2}\\log|\\mathcal{G}_{\\delta}|}+\\log|\\mathcal{G}_{\\delta}|\\right)=O(\\sigma\\sqrt{\\delta}+1).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Plugging this into (2) proves (1) and thus the desired bound $\\mathbb{E}_{x\\sim\\mathcal{D}}\\left[{\\sf S S C E}(x,p^{\\star})\\right]=O(\\sigma+\\log T).$ ", "page_idx": 6}, {"type": "text", "text": "4.2  Lower Bound the Optimal SSCE ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Next, we lower bound ${\\mathsf{O P T}}_{\\mathsf{S S C E}}({\\mathcal{D}})$ by showing that every forecasting strategy must incur an $\\Omega(\\sigma)$ SSCE on $\\mathcal{D}$ . Recall that ${\\mathsf{S S C E}}(x,p)$ is given by ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\underset{y\\sim\\mathsf{U n i f}(\\{0,1\\}^{T})}{\\mathbb{E}}\\left[\\underset{f\\in\\mathcal{F}}{\\operatorname*{sup}}\\sum_{t=1}^{T}y_{t}\\cdot f(p_{t})\\cdot(x_{t}-p_{t})\\right]\\geq\\underset{y\\sim\\mathsf{U n i f}(\\{0,1\\}^{T})}{\\mathbb{E}}\\left[\\left|\\sum_{t=1}^{T}y_{t}\\cdot(x_{t}-p_{t})\\right|\\right],\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where we use the fact that $\\mathcal{F}$ contains the constant functions 1 and $-1$ ", "page_idx": 6}, {"type": "text", "text": "Fix $x\\,\\in\\,\\{0,1\\}^{T}$ \uff0c $p\\,\\in\\,[0,1]^{T}$ and let $\\begin{array}{r}{N\\;:=\\;\\sum_{t=1}^{T}\\mathbb{1}\\left[|x_{t}-p_{t}|\\geq1/2\\right]}\\end{array}$ . Over the randomness in $y\\sim\\mathsf{U n i f}(\\{0,1\\}^{T})$ , the quantity $\\scriptstyle\\sum_{t=1}^{T}y_{t}\\cdot(x_{t}-p_{t})$ by the centrl limit theorem,is approximately distributed as a normal distribution with variance $\\begin{array}{r}{\\sum_{t=1}^{T}\\frac{1}{4}(x_{t}-p_{t})^{2}\\ge\\sum_{t=1}^{T}\\frac{1}{16}\\mathbb{1}\\left[|x_{t}-p_{t}|\\ge1/2\\right]=}\\end{array}$ $\\Omega(N)$ ,so its expected absolutevalue is $\\Omega({\\sqrt{N}})$ ", "page_idx": 6}, {"type": "text", "text": "Now it remains to lower bound the expectation of $\\sqrt{N}$ induced by an arbitrary forecaster. Conditioning on $x_{1:(t-1)}$ $x_{t}$ always follows Bernoulli $\\left(p_{t}^{\\star}\\right)$ . Thus, regardless of the choice of $p_{t}\\in[0,1]$ , the condition $|\\dot{\\boldsymbol{x}}_{t}-\\boldsymbol{p}_{t}|\\geq1/2$ holds with probability at least $\\operatorname*{min}\\{p_{t}^{\\star},1-p_{t}^{\\star}\\}\\geq p_{t}^{\\star}(1-p_{t}^{\\star})$ . Then, over the $T$ steps, we expect that $\\begin{array}{r}{N\\ge\\Omega(\\sum_{t=1}^{T}p_{t}^{\\star}(1-p_{t}^{\\star}))=\\Omega(\\sigma^{2})}\\end{array}$ holds with probability $\\Omega(1)$ , as long as $\\sigma=\\Omega(1)$ . This gives the desired lower bound $\\mathbb{E}\\left[{\\sf S S C E}(x,p)\\right]\\gtrsim\\mathbb{E}\\left[\\sqrt{N}\\right]=\\Omega(\\sigma)-O(1).$ ", "page_idx": 6}, {"type": "text", "text": "5   Upper Bound the SSCE of the Truthful Forecaster ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To extend the proof strategy sketched in Section 4 to non-product distributions, the first challenge is to define an appropriate complexity measure of a general distribution $\\mathcal{D}$ . Consider the stochastic process $(\\mathrm{Var}_{t})_{0\\leq t\\leq T}$ defined as $\\begin{array}{r}{\\mathrm{Var}_{t}\\;:=\\;\\sum_{s=1}^{t}p_{s}^{\\star}(1\\,-\\,p_{s}^{\\star})}\\end{array}$ where $x\\,\\sim\\,\\mathcal{D}$ and $p_{t}^{\\star}:=\\mathbb{E}_{x^{\\prime}\\sim\\mathcal{D}}\\left[x_{t}^{\\prime}\\middle|x_{1:(t-1)}^{\\prime}=x_{1:(t-1)}\\right]$ is now random variablethat denotes theconditoal expectation of $x_{t}$ after observing $x_{1:(t-1)}$ The \u201cright'? definition turns out to be roughly $\\sigma(\\mathcal{D}):=\\mathbb{E}\\left[\\sqrt{\\mathrm{Var}_{T}}\\right]$ In this section, we prove the following weaker upper bound on the SSCE incurred by the truthful forecaster. We provide a stronger bound (Theorem C.1) in Appendix C. ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{D}\\in\\Delta(\\{0,1\\}^{T}),\\,\\mathrm{err}_{\\mathsf{S S C E}}(\\mathcal{D},\\mathcal{A}^{\\mathrm{truthful}}(\\mathcal{D}))=O(\\mathbb{E}\\left[\\sqrt{\\mathrm{Var}_{T}}\\right]+\\log^{2}T).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Proof sketch. We begin by repeating the chaining argument in Section 4. Recall that, for any $\\delta>0$ there is a $\\delta$ covering ${\\mathcal{F}}_{\\delta}$ $\\mathcal{F}$ inthe $\\infty$ norm that has size $e^{O(1/\\delta)}$ Letting $\\pi_{\\delta}(f)$ denotethemaping ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathsf{S C E}({\\boldsymbol{x}},{\\boldsymbol{p}})\\leq2^{-M}\\cdot T+\\underset{y\\sim\\mathsf{U n i f}(\\{0,1\\}^{T})}{\\mathbb{E}}\\left[\\overset{M}{\\underset{k=0}{\\sum}}\\underset{\\underset{t=1}{\\underbrace{\\forall}}}{\\underbrace{\\operatorname*{sup}\\sum_{t=1}^{T}\\boldsymbol{y}_{t}\\cdot\\big(\\pi_{2^{-k}}(f)(p_{t})-\\pi_{2^{1-k}}(f)(p_{t})\\big)\\cdot(x_{t}-p_{t})}}\\right]\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "To control the expectation of each $W_{k}$ , we note that the set $\\mathcal{G}_{k}:=\\left\\{\\pi_{2^{-k}}(f)-\\pi_{2^{1-k}}(f):f\\in\\mathcal{F}\\right\\}$ is of size at most $|\\bar{\\mathcal{F}}_{2^{-k}}|\\cdot|\\mathcal{F}_{2^{1-k}}|$ . Furthermore, every function $g\\in{\\mathcal{G}}_{k}$ satisfies ", "page_idx": 7}, {"type": "equation", "text": "$$\n||g||_{\\infty}=||\\pi_{2^{-k}}(f)-\\pi_{2^{1-k}}(f)||_{\\infty}\\le||\\pi_{2^{-k}}(f)-f||_{\\infty}+||f-\\pi_{2^{1-k}}(f)||_{\\infty}=O(2^{-k})\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "for some $f\\in\\mathcal F$ . We apply the following technical lemma, which we prove in Appendix C. ", "page_idx": 7}, {"type": "text", "text": "Lemma5.2.Givena function $f\\,:\\,[0,1]\\,\\rightarrow\\,[-1,1]$ and $y\\,\\in\\,\\{0,1\\}^{T}$ considerthemartingale $\\begin{array}{r}{M_{t}(f,y):=\\sum_{s=1}^{t}y_{s}\\cdot f(p_{s}^{\\star})\\cdot(x_{s}-p_{s}^{\\star})}\\end{array}$ where $x\\sim\\mathcal{D}$ Then,for any finitefamily $\\mathcal{G}$ of functions from $[0,1]\\;t o\\;[-1,1]$ and any ${\\boldsymbol{y}}\\in\\{0,1\\}^{T}$ , we have ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbb{K}_{\\times\\sim\\mathcal{D}}\\left[\\operatorname*{max}_{f\\in\\mathcal{G}}M_{T}(f,y)\\right]\\le O\\left(\\log\\vert\\mathcal{G}\\vert\\cdot\\log T+\\sqrt{\\log\\vert\\mathcal{G}\\vert}\\cdot\\operatorname*{\\mathbb{E}}_{x\\sim\\mathcal{D}}\\left[\\sqrt{\\mathrm{Var}_{T}}\\right]\\right).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Applying Lemma 5.2 to each $\\mathcal{G}_{k}$ scaled up by a $\\Theta(2^{k})$ factor and noting that $\\log|\\mathcal{G}_{k}|\\leq\\log|\\mathcal{F}_{2^{-k}}|+$ $\\log|\\mathcal{F}_{2^{1-k}}|=O(2^{k})$ gives ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{e r r}_{\\mathsf{S S C E}}({\\mathcal D},{\\mathcal A}^{\\mathrm{truthful}}({\\mathcal D}))\\leq2^{-M}\\cdot T+\\displaystyle\\sum_{k=0}^{M}O(2^{-k})\\cdot O\\left(2^{k}\\log T+2^{k/2}\\operatorname*{\\mathbb{E}}_{x\\sim{\\mathcal D}}\\left[\\sqrt{\\mathrm{Var}_{T}}\\right]\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq2^{-M}\\cdot T+\\displaystyle\\sum_{k=0}^{M}O\\left(\\log T+2^{-k/2}\\operatorname*{\\mathbb{E}}_{x\\sim{\\mathcal D}}\\left[\\sqrt{\\mathrm{Var}_{T}}\\right]\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq2^{-M}\\cdot T+O\\left(M\\log T+\\operatorname*{\\mathbb{E}}_{x\\sim{\\mathcal D}}\\left[\\sqrt{\\mathrm{Var}_{T}}\\right]\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Choosing $M=\\Theta(\\log T)$ proves the theorem. ", "page_idx": 7}, {"type": "text", "text": "We remark that the proof of Lemma 5.2 is highly non-trivial. As mentioned in Section 3, such an upper bound would follow from Freedman's inequality, if $\\mathrm{Var}_{T}$ were always bounded by $O\\left(\\left(\\mathbb{E}\\left[{\\sqrt{\\operatorname{Var}_{T}}}\\right]\\right)^{2}\\right)$ . However, in general, applying Freedman's inequality to each $M_{T}(f,y)$ necessarily requires an additional union bound over possible values of $\\mathrm{Var}_{T}$ , and introduces a superconstant multiplicative factor. ", "page_idx": 7}, {"type": "text", "text": "The challenge in dealing with the randomness in $\\mathrm{Var}_{T}$ is captured by the following toy problem: ", "page_idx": 7}, {"type": "text", "text": "Random walk with early stopping: Let $(X_{t})_{0\\leq t\\leq T}$ be the random walk such that $X_{0}=0$ and each $X_{t}-X_{t-1}$ independently follows $\\mathsf{U n i f}(\\{\\pm1\\})$ . Let $\\tau$ be an arbitrary stopping time with respect to $\\left(X_{t}\\right)$ . Prove that E $[|X_{\\tau}|]\\leq\\dot{O}(1)\\cdot\\mathbb{E}\\left[\\sqrt{\\tau}\\right]$ ", "page_idx": 7}, {"type": "text", "text": "Indeed, the above corresponds to a special case of Lemma 5.2 in which: (1) the sequence $p^{\\star}$ starts with entry $1/2$ , and may switch to entry O at any point, depending on the realization of $x_{t}\\mathbf{s}$ ; (2) the family $\\mathcal{G}$ consists of two constant functions 1 and $-1$ ", "page_idx": 7}, {"type": "text", "text": "One might be tempted to prove I $\\mathfrak{T}\\left[|X_{\\tau}|\\right]\\leq O(1)\\cdot\\mathbb{E}\\left[\\sqrt{\\tau}\\right]$ by first proving $\\mathbb{E}\\left[|X_{\\tau}||\\tau=t\\right]=O(\\sqrt{t})$ for all $t\\in[T]$ , and then applying the law of total expectation. Such an approach is doomed to fail, because the stopping time $\\tau$ might significantly bias the conditional expectation of $\\left|X_{\\tau}\\right|$ on some event $\\tau=t_{0}$ , e.g., by stopping at time $t_{0}$ only if $\\lvert X_{t_{0}}\\rvert\\gg\\sqrt{t_{0}}$ ", "page_idx": 7}, {"type": "text", "text": "Our workaround is inspired by the standard doubling trick in online learning. We break the time horizon into epochs of geometrically increasing lengths: the $k$ -thepochcontains $2^{k}$ steps.Webreak $\\left|X_{\\tau}\\right|$ into the displacements accumulated in different epochs; their sum clearly upper bounds $\\left|X_{\\tau}\\right|$ ", "page_idx": 7}, {"type": "text", "text": "Furthermore, we can show that, conditioning on reaching epoch $k$ , the displacement within the epoch $O({\\sqrt{2^{k}}})$ . This allows us to establish the desired inequality via ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left|X_{\\tau}\\right|\\right]\\leq O(1)\\cdot\\sum_{k=1}^{O(\\log T)}\\operatorname*{Pr}\\left[\\tau{\\mathrm{~reaches~epoch~}}k\\right]\\cdot{\\sqrt{2^{k}}}\\leq O(1)\\cdot\\mathbb{E}\\left[{\\sqrt{\\tau}}\\right].\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "To prove Lemma 5.2, we extend this technique to a general martingale $M_{T}(f,y)$ by dividing the time horizon into epochs according to the doubling of $\\mathrm{Var}_{t}$ , and then applying Freedman's inequality to each epoch. ", "page_idx": 8}, {"type": "text", "text": "Towards a stronger upper bound. In our actual proof, we use a slightly different complexity measure $\\sigma_{\\gamma}(\\mathcal{D}):=\\mathbb{E}\\left[\\bar{\\gamma}\\bar{(}\\mathrm{Var}_{T}\\right)\\right]$ , where $\\gamma(x)=x$ if $x<1$ and $\\gamma(x)\\doteq{\\sqrt{x}}$ otherwise. Roughly speaking, this definition accounts for the fact that a sum of independent Bernoulli random variables behaves quite differently when its mean is close to O0. To remove the extra $\\log^{2}T$ term in Theorem 5.1, our actual proof also uses a variant of Lemma 5.2, Lemma C.9, which involves a more careful application of Freedman's inequality tailored to specific coverings of Lipschitz functions. ", "page_idx": 8}, {"type": "text", "text": "6 Lower Bound the Optimal SSCE ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we outline a weaker lower bound on the optimal SSCE achievable on a distribution. ", "page_idx": 8}, {"type": "text", "text": "Theorem 6.1. For any $\\mathcal{D}\\in\\Delta(\\{0,1\\}^{T}),\\,\\mathsf{O P T}_{\\mathsf{S S C E}}(\\mathcal{D})=\\Omega(\\mathbb{E}\\left[\\sqrt{\\mathrm{Var}_{T}}\\right])-O(1).$ ", "page_idx": 8}, {"type": "text", "text": "Similar to the product distribution case (Section 4), the key quantity in the proof is the stochastic process $(N_{t})_{0\\leq t\\leq T}$ defined as $\\begin{array}{r}{N_{t}:=\\sum_{s=1}^{t}n_{s}}\\end{array}$ and $n_{t}:=\\mathbb{1}\\left[|x_{t}-p_{t}|\\geq1/2\\right]$ . This is formalized by the following lemma, which applies to any realization of $x,p$ and $\\begin{array}{r}{N_{T}=\\sum_{t=1}^{T}\\mathbb{1}\\left[|x_{t}-p_{t}|\\geq1/2\\right]}\\end{array}$ ", "page_idx": 8}, {"type": "text", "text": "Lemma 6.2. For any $x\\in\\{0,1\\}^{T}$ and $p\\in[0,1]^{T}$ we have ${\\sf S S C E}(x,p)\\geq{\\Omega}\\left(\\sqrt{N_{T}}\\right)$ ", "page_idx": 8}, {"type": "text", "text": "It remains to lower bound the quantity $\\mathbb{E}\\ [\\sqrt{N_{T}}]$ induced by an arbitrary forecaster. As argued earlier, conditioning on $x_{1:(t-1)}$ , we always have ${\\bar{\\operatorname*{Pr}}}\\left[n_{t}=1\\right]\\geq p_{t}^{\\star}(1-p_{t}^{\\star})=\\operatorname{Var}_{t}-\\operatorname{Var}_{t-1}$ where $p_{t}^{\\star}$ and $\\mathrm{Var}_{t}$ are defined as in Section 5. Thus, $\\left(N_{t}\\mathrm{~-~}\\mathrm{Var}_{t}\\right)$ is a sub-martingale, which implies $\\mathbb{E}\\left[N_{T}\\right]\\geq\\mathbb{E}\\left[\\mathrm{Var}_{T}\\right]$ . However, this does not imply that I $\\mathbb{E}\\left[\\sqrt{N_{T}}\\right]\\geq\\Omega(\\mathbb{E}\\left[\\sqrt{\\mathrm{Var}_{T}}\\right])$ . In fact, such an inequality does not hold in general: When $p_{1}^{\\star}=\\varepsilon\\ll1$ and $p_{t}^{\\star}=0$ for all $t\\geq2,\\bar{\\mathbb{E}}\\left[\\sqrt{N_{T}}\\right]$ could be $O(\\varepsilon)$ , yet $\\mathbb{E}\\left[{\\sqrt{\\mathrm{Var}_{T}}}\\right]=\\Omega({\\sqrt{\\varepsilon}})\\gg O(\\varepsilon)$ ", "page_idx": 8}, {"type": "text", "text": "The following technical lemma circumvents this counterexample by subtracting a constant term from the right-hand side: ", "page_idx": 8}, {"type": "text", "text": "Lemma 6.3. The stochastic process $(N_{t})_{t\\in[T]}$ satisfies I $\\mathfrak{T}\\left[\\sqrt{N_{T}}\\right]\\geq\\Omega(\\mathbb{E}\\left[\\sqrt{\\mathrm{Var}_{T}}\\right])-O(1).$ ", "page_idx": 8}, {"type": "text", "text": "Note that Theorem 6.1 directly follows from Lemmas 6.2 and 6.3, which we prove in Appendix D.1. To avoid the extra $-O(1)$ term in the lower bound, our actual proof (deferred to Appendix D.3) works with the slightly different complexity measure $\\sigma_{\\gamma}(D):=\\mathbb{E}\\left[\\bar{\\gamma}(\\mathrm{Var}_{T})\\right]$ defined in Section 5. ", "page_idx": 8}, {"type": "text", "text": "7   Forecasting with $O(\\sqrt{T})$ SSCE ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we prove Theorem 1.3, which states the existence of a deterministic forecaster that incurs an $O(\\sqrt{T})$ SSCE against all adaptive adversaries. Recall the definition of the smooth calibrationerror $\\scriptstyle({\\mathsf{s m C E}})$ from Section 2. Using standard chaining arguments, we can show the following relation between SSCE and smCE, whose proof we defer to Appendix F. ", "page_idx": 8}, {"type": "text", "text": "Lemma 7.1. For any $x\\in\\{0,1\\}^{T}$ and $p\\in[0,1]^{T}$ \uff0c ", "page_idx": 8}, {"type": "equation", "text": "$$\n{\\mathsf{S S C E}}(x,p)\\leq{\\frac{1}{2}}{\\mathsf{s m C E}}(x,p)+O({\\sqrt{T}}),\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "wherethe $O(\\cdot)$ notation hides a universal constant that does not depend on $T$ Cor $p$ ", "page_idx": 8}, {"type": "text", "text": "Theorem 1.3 follows from the lemma above and a recent result of [ACRS24] ", "page_idx": 8}, {"type": "text", "text": "Proof of Theorem 1.3. It was shown by [ACRS24] that there exists a deterministic forecaster with an $O({\\sqrt{T}})$ distance from calibration $(\\mathsf{C a l D i s t}(x,p))$ against every adaptive adversary in the adversarial sequential calibration setup. Lemma 7.1 together with the inequality ${\\frac{1}{2}}\\mathsf{s m C E}(x,\\dot{p})\\leq\\mathsf{C a l D i s t}(x,p)$ from [BGHN23, Lemma 5.4 and Theorem 7.3] implies that ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\mathsf{S S C E}(x,p)\\leq\\mathsf{C a l D i s t}(x,p)+O(\\sqrt{T}),\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "so the same forecaster incurs an SSCE of $O({\\sqrt{T}})$ as well. ", "page_idx": 9}, {"type": "text", "text": "8 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We formulate three natural desiderata of calibration measures that evaluate the quality of probabilistic forecasts: truthfulness, completeness, and soundness. They serve as minimal requirements for an error metric to be considered as measuring calibration and not to create a significant incentive for forecasters to predict untruthfully. While existing calibration measures fail to simultaneously meet all these criteria, we propose the new calibration measure (SSCE) that is shown to be approximately truthful via a non-trivial analysis. In the following, we discuss two natural directions of future work. ", "page_idx": 9}, {"type": "text", "text": "Inherent trade-offs among different desiderata? As shown in Table 1, the SSCE and the error metrics induced by proper scoring rules give a trade-off between truthfulness and completeness: The former is complete and approximately truthful, while the latter is perfectly truthful but not complete. Is there a calibration measure that achieves the best of both worlds? Taking a step back, while our definition of truthfulness seems natural, the completeness and soundness criteria, as defined, only serve as minimal requirements. It still remains to explore ways to formally quantify the latter two, and investigate the inherent quantitative trade-offs among truthfulness, completeness and soundness. ", "page_idx": 9}, {"type": "text", "text": "Truthfulness against adaptive adversaries? One may wonder whether the truthfulness guarantee of SSCE can be extended to handle adaptive adversaries as well. Assuming that the forecaster is given an adversary's (randomized) strategy for choosing $x_{t}$ based on $x_{1:(t-1)}$ and $p_{1:(t-1)}$ , is it sill approximately optimal to always predict the conditional probability? Here, \u201cadaptive\u201d emphasizes that $x_{t}$ may depend on both $x_{1:(t-1)}$ and $p_{1:(t-1)}$ ; the formulation in Section 2 is equivalent to that $x_{t}$ Only depends on $x_{1:(t-1)}$ ", "page_idx": 9}, {"type": "text", "text": "Unfortunately, as we show in Appendix G, such a guarantee does not hold for SSCE, and is unlikely to hold for any natural calibration measure: An adversary can \u201cforce\u201d the forecaster to predict untruthfully by \u201c\"threatening\u201d to increase the variance of the subsequent bits. However, this adversary is highly contrived and unrealistic for practical scenarios. We may thus identify reasonable restrictions on the adaptive adversary to sidestep this counterexample. ", "page_idx": 9}, {"type": "text", "text": "9 Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work is supported in part by the National Science Foundation under grants CCF-2145898 and the Graduate Research Fellowship Program under grant DGE 2146752, the Office of Naval Research under grant N00014-24-1-2159, an Alfred P. Sloan fellowship, a Schmidt Sciences AI2050 fellowship, and a Google Research Scholars award. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the funding agencies. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[ACRS24] Eshwar Ram Arunachaleswaran, Natalie Collina, Aaron Roth, and Mirah Shi. An elementary predictor obtaining $2\\sqrt{T}$ distance to calibration.  arXiv preprint arXiv:2402.11410, 2024. $[\\mathbf{B}\\mathbf{F}^{+}02]$ Henri Berestycki, Igor Florent, et al. Asymptotics and calibration of local volatility models. Quantitative finance, 2(1):61, 2002.   \nBGHN23] Jarostaw Btasiok, Parikshit Gopalan, Lunjia Hu, and Preetum Nakkiran. A unifying theory of distance from calibration. In Symposium on Theory of Computing (STOC), pages 1727-1740, 2023. [Bla23] Guy Blanc. Subsampling sufices for adaptive data analysis. In Symposium on Theory of Computing (STOC), pages 999-1012, 2023.   \nBLMT22] Guy Blanc, Jane Lange, Ali Malik, and Li-Yang Tan. On the power of adaptivity in statistical adversaries. In Conference on Learning Theory (COLT), pages 5030-5061, 2022. [Bri50] Glenn W. Brier. Verification of forecasts expressed in terms of probability. Monthly Weather Review, 78(1): 1-3, 1950.   \n[CAT16] Cynthia S Crowson, Elizabeth J Atkinson, and Terry M Therneau. Assessing calibration of prognostic risk scores. Statistical methods in medical research, 25(4):1692-1706, 2016. [CY21] Yiling Chen and Fang-Yi Yu.  Optimal scoring rule design. arXiv preprint arXiv:2107.07420, 2021.   \n[Daw82] A. P. Dawid. The well-calibrated bayesian.  Journal of the American Statistical Association, 77(379):605-610, 1982.   \n[Daw85] A. P Dawid. Calibration-based empirical probability. The Annals of Statistics, 13(4):1251-1274, 1985. [DF83] Morris H DeGroot and Stephen E Fienberg. The comparison and evaluation of forecasters. Journal of the Royal Statistical Society: Series D (The Statistician), 32(1-2):12-22, 1983. [Dud87] R. M. Dudley. Universal donsker classes and metric entropy. The Annals of Probability, 15(4):1306-1326, 1987. [FH18] Dean P. Foster and Sergiu Hart. Smooth calibration, leaky forecasts, finite recall, and nash dynamics. Games and Economic Behavior, 109:271-293, 2018. [FH21]  Dean P. Foster and Sergiu Hart. Forecast hedging and calibration. Journal of Political Economy, 129(12):3447-3490, 2021. [FL99] Drew Fudenberg and David K. Levine. An easier way to calibrate. Games and Economic Behavior, 29(1-2):131-137, 1999. [Fos99] Dean P Foster. A proof of calibration via blackwell's approachability theorem. Games and Economic Behavior, 29(1-2):73-78, 1999. [Fre75] David A. Freedman. On tail probabilities for martingales. The Annals of Probability, 3(1):100-118, 1975.   \n[FRST11] Dean P. Foster, Alexander Rakhlin, Karthik Sridharan, and Ambuj Tewari. Complexitybased approach to calibration with checking rules. In Conference on Learning Theory (COLT), pages 293-314, 2011. [FV97] Dean P. Foster and Rakesh V. Vohra. Calibrated learning and correlated equilibrium. Games and Economic Behavior, 21(1-2):40-55, 1997. ", "page_idx": 10}, {"type": "text", "text": "[FV98] Dean P. Foster and Rakesh V. Vohra. Asymptotic calibration. Biometrika, 85(2):379- ", "page_idx": 11}, {"type": "text", "text": "390, 1998.   \n[GPSW17] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural networks. In International Conference on Machine Learning (ICML), pages 1321-1330, 2017. [Har22] Sergiu Hart. Calibrated forecasts: The minimax proof. arXiv preprint arXiv:2209.05863, 2022.   \n[HJKRR18] Ursula Hebert-Johnson, Michael Kim, Omer Reingold, and Guy Rothblum. Multicalibration: Calibration for the (computationally-identifiable) masses. In International Conference on Machine Learning (ICML), pages 1939-1948, 2018. [HJZ23] Nika Haghtalab, Michael Jordan, and Eric Zhao. A unifying perspective on multicalibration: Game dynamics for multi-objective learning. In Advances in Neural Information Processing Systems (NeurIPS), pages 72464-72506, 2023. [HPY23] Nika Haghtalab, Chara Podimata, and Kunhe Yang. Calibrated Stackelberg games: Learning optimal commitments against calibrated agents. In Advances in Neural Information Processing Systems (NeurIPS), pages 61645-61677, 2023.   \n[HSLW23] Jason D. Hartline, Liren Shan, Yingkai Li, and Yifan Wu. Optimal scoring rules for multi-dimensional effort. In Conference on Learning Theory (COLT), pages 2624-2650, 2023. [HW24] Lunjia Hu and Yifan Wu. Predict to minimize swap regret for all payoff-bounded tasks. arXiv preprint arXiv:2404.13503, 2024.   \nJOKOM12] Xiaoqian Jiang, Melanie Osl, Jihoon Kim, and Lucila Ohno-Machado. Calibrating predictive model estimates to support personalized medicine. Journal of the American Medical Informatics Association, 19(2):263-274, 2012. [KFO8] Sham M. Kakade and Dean P. Foster. Deterministic calibration and Nash equilibrium. Journal of Computer and System Sciences, 74(1):115-130, 2008. [KLST23] Robert Kleinberg, Renato Paes Leme, Jon Schneider, and Yifeng Teng. U-calibration: Forecasting for an unknown agent. In Conference on Learning Theory (COLT), pages 5143-5145, 2023. [KMR17] Jon Kleinberg, Sendhil Mullainathan, and Manish Raghavan. Inherent trade-offs in the fair determination of risk scores. In Innovations in Theoretical Computer Science (ITCS), pages 43:1-43:23, 2017. [KSB21] Benjamin Kompa, Jasper Snoek, and Andrew L Beam. Second opinion needed: communicating uncertainty in medical machine learning. NPJ Digital Medicine, 4(1):4, 2021. [KSJ18] Aviral Kumar, Sunita Sarawagi, and Ujwal Jain.  Trainable calibration measures for neural networks from kernel mean embeddings. In International Conference on Machine Learning (ICML), pages 2805-2814, 2018.   \n[LHSW22] Yingkai Li, Jason D. Hartline, Liren Shan, and Yifan Wu. Optimization of scoring rules. In Economics and Computation (EC), pages 988-989, 2022. [MW84]  Allan H Murphy and Robert L Winkler. Probability forecasting in meteorology. Journal of the American Statistical Association, 79(387):489-500, 1984. [NNW21] Eric Neyman, Georgy Noarov, and S. Matthew Weinberg. Binary scoring rules that incentivize precision. In Economics and Computation (EC), pages 718-733, 2021.   \n[NRRX23] Georgy Noarov, Ramya Ramalingam, Aaron Roth, and Stephan Xie. High-dimensional unbiased prediction for sequential decision making. In OPT 2023: Optimization for Machine Learning, 2023.   \n?RW+ 17] Geoff Pleiss, Manish Raghavan, Felix Wu, Jon Kleinberg, and Kilian Q. Weinberger. On fairness and calibration. Advances in Neural Information Processing Systems (NIPS), pages 5680-5689, 2017. [PW22] Maneesha Papireddygari and Bo Waggoner. Contracts with information acquisition, via scoring rules. In Economics and Computation (EC), pages 703-704, 2022. [QV21] Mingda Qiao and Gregory Valiant. Stronger calibration lower bounds via sidestepping. In Symposium on Theory of Computing (STOC), pages 456-466, 2021. [QZ24]  Mingda Qiao and Letian Zheng. On the distance from calibration in sequential prediction. In Conference on Learning Theory (COLT), pages 4307-4357, 2024. [RS24]  Aaron Roth and Mirah Shi. Forecasting for swap regret for all downstream agents. arXiv preprint arXiv:2402.08753, 2024. [She10] I1. G. Shevtsova. An improvement of convergence rate estimates in the Lyapunov theorem. Doklady Mathematics, 82(3):862-864, 2010. [Ste22]  Thomas Steinke. Composition of differential privacy & privacy amplification by subsampling. arXiv preprint arXiv:2210.00597, 2022.   \n[VCV15] Ben Van Calster and Andrew J Vickers. Calibration of risk prediction models: impact on decision-analytic performance. Medical decision making, 35(2):162-169, 2015.   \n[WM68] Robert L. Winkler and Allan H. Murphy. \u201c\"Good\" probability assessors. Journal of Applied Meteorology and Climatology, 7(5):751-758, 1968. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "table", "img_path": "cDa8hfTyGc/tmp/e17c776e3295bf6e197fb44ec2d9b4e543072efd78c796167427a3fb58c02984.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "Table 2: Evaluation of previous calibration measures along with SSCE, in terms of completeness, soundness and truthfulness (Definitions 2.2 and 2.5). Every calibration measure, except SSCE, either lacks completeness or has a significant truthfulness gap. ", "page_idx": 13}, {"type": "text", "text": "A Taxonomy of Existing Calibration Measures ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we prove that the existing calibration measures in Table 2 either have a large truthfulness gap or lack completeness. ", "page_idx": 13}, {"type": "text", "text": "In these proofs, the biases induced by specific outcomes and predictions will be frequently used: Withrespect to outcomes $x\\in\\{0,1\\}^{\\check{T}}$ and predictions $\\mathit{p}\\in[0,1]^{T}$ , the bias associated with value $\\alpha\\in[0,1]$ is defined as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\Delta_{\\alpha}:=\\sum_{t=1}^{T}(x_{t}-p_{t})\\cdot\\mathbb{1}\\left[p_{t}=\\alpha\\right].\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "A.1 Existing Calibration Measures ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The expected calibration error.  A common calibration measure is the sum of $L_{1}$ errors of each level set, known as the $L_{1}$ calibration error or the Expected Calibration Error $(E C E)$ : On $x\\in\\{0,1\\}^{T}$ and $p\\in[0,1]^{T}$ , the expected calibration error is defined as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathsf{E C E}(x,p):=\\sum_{\\alpha\\in[0,1]}\\left|\\sum_{t=1}^{T}(x_{t}-p_{t})\\cdot\\mathbb{1}[p_{t}=\\alpha]\\right|=\\sum_{\\alpha\\in[0,1]}|\\Delta_{\\alpha}|.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Note that the summand $|\\Delta_{\\alpha}|$ is non-zero only if $\\alpha\\in\\{p_{1},p_{2},\\dots,p_{T}\\}$ , so the summations above are essentially finite and well-defined. ", "page_idx": 13}, {"type": "text", "text": "The smooth calibration error. The smooth calibration error [KFO8] is defined as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathsf{s m C E}(x,p):=\\operatorname*{sup}_{f\\in\\mathcal{F}}\\sum_{t=1}^{T}f(p_{t})\\cdot(x_{t}-p_{t})=\\operatorname*{sup}_{f\\in\\mathcal{F}}\\sum_{\\alpha\\in[0,1]}f(\\alpha)\\cdot\\Delta_{\\alpha},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\mathcal{F}$ is the family of 1-Lipschitz functions from $[0,1]$ to $[-1,1]$ .Again, since $\\Delta_{\\alpha}\\neq0$ holds only if $\\alpha\\in\\{p_{1},p_{2},\\ldots,p_{T}\\}$ , the summation above is finite and well-defined. ", "page_idx": 13}, {"type": "text", "text": "The distance from calibration.  The distance from calibration, introduced by [BGHN23] and extended to the sequential setup by [QZ24], is defined as: ", "page_idx": 14}, {"type": "equation", "text": "$$\n{\\mathsf{C a l D i s t}}(x,p):=\\operatorname*{min}_{q\\in{\\mathcal{C}}(x)}\\|p-q\\|_{1},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{C}(\\boldsymbol{x}):=\\left\\{p\\in[0,1]^{T}:\\forall\\boldsymbol{a}\\in[0,1],\\sum_{t=1}^{T}(\\boldsymbol{x}_{t}-p_{t})\\cdot\\mathbb{1}[p_{t}=\\alpha]=0\\right\\}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "is the set of predictions that are perfectly calibrated for $x$ ", "page_idx": 14}, {"type": "text", "text": "Interval calibration.  The interval calibration error of [BGHN23] relaxes the ECE to a binned version while penalizing the use of long intervals. Formally, an interval partition $\\mathcal{T}$ is a finite collection of intervals $\\{\\bar{I}_{1},I_{2},\\ldots,I_{|\\mathcal{Z}|}\\}$ that form a partition of $[0,1]$ . The interval calibration error is defined as: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathsf{i n t C E}(x,p):=\\operatorname*{inf}_{\\mathcal{I}}\\,\\left[\\sum_{i=1}^{|\\mathcal{I}|}\\left|\\sum_{t=1}^{T}(x_{t}-p_{t})\\cdot\\mathbb{1}\\left[p_{t}\\in I_{i}\\right]\\right|+\\sum_{t=1}^{T}\\sum_{i=1}^{|\\mathcal{Z}|}\\mathsf{l e n}(I_{i})\\cdot\\mathbb{1}\\left[p_{t}\\in I_{i}\\right]\\right],\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the infimum is over all interval partitions $\\mathcal{T}$ ,and $|\\mathsf{e n}(I)$ denotes the length of interval $I$ .Note that the first summation inside the infimum is analogous to the ECE, except that the biases associated with all values within the same interval are added together. The second summation gives the total lengths of the intervals into which the $T$ predictions fall. ", "page_idx": 14}, {"type": "text", "text": "Laplace-kernel calibration. The Laplace-kernel calibration error [BGHN23] is a special case of the maximum mean calibration error introduced by [KSJ18]. It can be viewed as a variant of the smooth calibration error, in which the family $\\mathcal{F}$ of Lipschitz functions is replaced by ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\widetilde{\\mathcal{F}}:=\\left\\{f:\\mathbb{R}\\rightarrow\\mathbb{R}:\\|f\\|_{2}^{2}+\\|f^{\\prime}\\|_{2}^{2}\\leq1\\right\\},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\|\\cdot\\|_{2}$ denotes the $\\ell_{2}$ norm of functions, and $f^{\\prime}$ is the derivative of $f$ . Namely, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathsf{k C E}^{\\mathsf{L a p}}(x,p):=\\operatorname*{sup}_{f\\in\\tilde{\\mathcal{F}}}\\sum_{t=1}^{T}f(p_{t})\\cdot(x_{t}-p_{t}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "U-calibration. The definition of the $U$ -calibration error [KLST23] is based on proper scoring rules. A (bounded) scoring rule is a function $S:\\{0,1\\}\\times[0,1]\\rightarrow[-1,1]$ . A scoring rule is proper if it holds for every $\\alpha\\in[0,1]$ that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\alpha\\in\\underset{\\beta\\in[0,1]}{\\mathrm{arg}\\,\\mathrm{min}}\\,\\underset{x\\sim\\mathsf{B e r n o u l l i}(\\alpha)}{\\mathbb{E}}\\left[S(x,\\beta)\\right].\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "In other words, when the outcome $x$ is drawn from follow Bernoulli $(\\alpha)$ , predicting the true parameter $\\alpha$ minimizes the expected loss. The U-calibration error is then defined as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathsf{U C a l}(x,p):=\\operatorname*{sup}_{S}\\left[\\sum_{t=1}^{T}S(x_{t},p_{t})-\\operatorname*{inf}_{\\alpha\\in[0,1]}\\sum_{t=1}^{T}S(x_{t},\\alpha)\\right],\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the supremum is over all proper scoring rules. Note that for each fixed $S$ , the expression inside the supremum is exactly the external regret of the forecaster, i.e., the excess loss compared to the best fixed prediction in hindsight. ", "page_idx": 14}, {"type": "text", "text": "Maximum swap regret.  A recent line of work [NRRX23, RS24, HW24] considers a strengthening of U-calibration, in which the external regret is replaced with the swap regret. In particular, [HW24] showed that the resulting calibration measure, termed the Maximum Swap Regret (MSR), is polynomially related to the ECE after scaling by a factor of $1/T$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left[{\\frac{\\mathsf{E C E}(x,p)}{T}}\\right]^{2}\\leq{\\frac{\\mathsf{M S R}(x,p)}{T}}\\leq{\\frac{2\\mathsf{E C E}(x,p)}{T}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "A.2 $0{\\bullet}\\Omega(T)$ Truthfulness Gaps ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We first prove the $0{-}\\Omega(T)$ truthfulness gaps of the ECE and the MSR. ", "page_idx": 15}, {"type": "text", "text": "Proposition A.1. Both the expected calibration error and the maximum swap regret have a $0{-}\\Omega(T)$ truthfulnessgap. ", "page_idx": 15}, {"type": "text", "text": "To establish Proposition A.1, we follow a similar argument to the one in Section 3: We divide the time horizon into $T/3$ triples, each containing a random bit followed by a zero and a one. The truthful forecaster would predict the true probabilities for the $T/3$ random bits, which are designed to be close to $1/2$ but distinct. This leads to a linear ECE. On the other hand, a strategic forecaster may alwayspredict $1/2$ on the random bit. Then, based on the realization of the random bit, they use the subsequent deterministic bits to offset the bias. The resulting predictions are perfectly calibrated, and thus have a zero ECE. Finally, the relation between the ECE and the MSR gives the same truthfulness gap for the MSR. ", "page_idx": 15}, {"type": "text", "text": "Proof of Proposition A.1. Consider the distribution $\\mathcal{D}$ defined as follows: ", "page_idx": 15}, {"type": "text", "text": "\u00b7 Let $\\varepsilon_{1},\\varepsilon_{2},\\ldots,\\varepsilon_{T/3}$ be distinct values in $[-1/4,1/4]$ chosen arbitrarily. \u00b7 For each $k\\in[T/3]$ , set $(p_{3k-2}^{\\star},p_{3k-1}^{\\star},p_{3k}^{\\star})=(1/2+\\varepsilon_{k},0,1).$ $\\mathcal{D}$ is the product distribution $\\begin{array}{r l}{\\prod_{t=1}^{T}\\mathsf{B e r n o u l l i}(p_{t}^{\\star})}&{{}}\\end{array}$ ", "page_idx": 15}, {"type": "text", "text": "By definition, the predictions made by the truthful forecaster are exactly given by. $p^{\\star}$ .Then, for each $k\\in[T/3]$ and $\\alpha=1/2+\\varepsilon_{k}\\in[1/4,3/4]$ , we have $|\\Delta_{\\alpha}|=|x_{3k-2}-\\alpha|\\geq1/4$ .This shows $\\mathsf{e r r}_{\\mathsf{E C E}}({\\mathcal{D}},{\\mathcal{A}}^{\\mathrm{truthful}}({\\mathcal{D}}))\\geq(T/3)\\cdot(1/4)=\\Omega(T)$ By th inaguaity $\\begin{array}{r}{\\frac{\\mathsf{M S R}(x,p)}{T}\\geq\\left[\\frac{\\mathsf{E C E}(x,p)}{T}\\right]^{2}}\\end{array}$ also have $\\mathsf{e r r}_{\\mathsf{M S R}}({\\mathcal{D}},{\\mathcal{A}}^{\\mathrm{truthful}}({\\mathcal{D}}))=\\Omega(T).$ ", "page_idx": 15}, {"type": "text", "text": "On the other hand, consider the following alternative forecaster for $\\mathcal{D}$ ", "page_idx": 15}, {"type": "text", "text": "\u00b7 For each $k\\in[T/3]$ , predict $p_{3k-2}=1/2$   \n\u00b7If $x_{3k-2}\\,=\\,0$ , predict $p_{3k-1}\\,=\\,0$ and $p_{3k}\\,=\\,1/2$ ; otherwise, predict $p_{3k-1}\\,=\\,1/2$ and P3k = 1. ", "page_idx": 15}, {"type": "text", "text": "Clearly, for each $k\\in[T/3]$ , the steps $t\\in\\{3k-2,3k-1,3k\\}$ have zero contribution to $\\Delta_{0},\\,\\Delta_{1}$ and $\\Delta_{1/2}$ . Therefore, this forecaster achieves a zero ECE on $\\mathcal{D}$ . This proves $\\mathsf{O P T}_{\\mathsf{E C E}}(\\mathcal{D})=0$ and establishes the O-(T) truthulness gap for the ECE. Finally, the inequality MSR(c2EE(2.) implies that the same forecaster achieves a zero MSR, which establishes $\\mathsf{O P T}_{\\mathsf{M S R}}(\\mathcal{D})=0$ and the $0{-}\\Omega(T)$ truthfulness gap for the MSR. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "A.3 $0{\\bullet}\\Omega({\\sqrt{T}})$ Truthfulness Gaps ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Next, we prove the $0{-}\\Omega({\\sqrt{T}})$ truthfulness gap for several calibration measures. The proof follows the argument outlined in Section 3. ", "page_idx": 15}, {"type": "text", "text": "Proposition A.2. The smooth calibration error, the distance from calibration, the interval calibration error, and the Laplace-kernel calibration error all have a $0{-}\\Omega({\\sqrt{T}})$ truthfulnessgap. ", "page_idx": 15}, {"type": "text", "text": "Proof. The truthfulness gaps of the four calibration measures are witnessed by the same product distribution $\\begin{array}{r}{\\mathcal{D}=\\prod_{t=1}^{T}}\\end{array}$ Bernoulli $\\left(p_{t}^{\\star}\\right)$ where $(p_{3k-2}^{\\star},p_{3k-1}^{\\star},p_{3k}^{\\star})=(1/2,0,1)$ forevery $k\\in[T/3]$ ", "page_idx": 15}, {"type": "text", "text": "Truthful forecaster has an $\\Omega({\\sqrt{T}})$ penalty. The truthful forecaster makes predictions that are identical to $p^{\\star}$ . As a result, we have $\\Delta_{0}\\,=\\,\\Delta_{1}\\,=\\,0$ , while $\\Delta_{1/2}$ is distributed as the difference between a sample from Binomial $(T/3,1/2)$ and its mean $T/6$ It then follows that $|\\Delta_{1/2}|\\geq\\Omega(\\sqrt{T})$ holds with probability $\\Omega(1)$ . We will show that all four calibration measures evaluate to $\\Omega({\\sqrt{T}})$ in expectation. ", "page_idx": 15}, {"type": "text", "text": "For the smooth calibration error, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathsf{e r r}_{\\mathsf{s m C E}}(\\mathscr{D},\\mathscr{A}^{\\mathrm{truthful}}(\\mathscr{D}))=\\underset{x\\sim\\mathscr{D}}{\\mathbb{E}}\\left[|\\Delta_{1/2}|\\right]=\\underset{X\\sim\\mathsf{B i n o m i a l}(T/3,1/2)}{\\mathbb{E}}\\left[|X-T/6|\\right]=\\Omega(\\sqrt{T}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For the distance from calibration, by [BGHN23, Lemma 5.4 and Theorem 7.3], we have the inequality ${\\scriptstyle{\\frac{1}{2}}}\\mathsf{s m C E}(x,p)\\leq\\mathsf{C a l D i s t}(x,p)$ for any $x\\in\\{0,1\\}^{T}$ and $p\\in[0,1]^{T}$ , so the truthful forecaster also gives $\\mathsf{e r r}_{\\mathsf{C a l D i s t}}({\\mathcal{D}},{\\mathcal{A}}^{\\mathrm{truthful}}({\\mathcal{D}}))=\\Omega(\\sqrt{T}).$ ", "page_idx": 16}, {"type": "text", "text": "For interval calibration, let $\\mathcal{T}$ be an arbitrary interval partition, and $I\\in{\\mathcal{T}}$ be the interval that contains $1/2$ $I$ $\\mathtt{l e n}(I)\\,\\geq\\,1/2$ $\\begin{array}{r}{\\sum_{t=1}^{T}\\sum_{i=1}^{|\\mathcal{Z}|}\\mathsf{l e n}(I_{i})~.}\\end{array}$ $\\mathrm{\\Delta}[p_{t}\\in I_{i}]$ $2T/3\\,\\cdot\\,1/2\\;=\\;\\Omega(T)$ $I$   \n$\\begin{array}{r}{\\sum_{t=1}^{T}(x_{t}-p_{t})\\cdot\\mathbb{1}\\left[p_{t}\\in I\\right]}\\end{array}$ will be exactly $\\Delta_{1/2}$ , and the first term in the definition willbe at least $|\\Delta_{1/2}|$ . It follows that intCE $(x,p)\\geq\\Omega({\\sqrt{T}})$ with probability $\\Omega(1)$ , so we have the lower bound $\\mathsf{e r r}_{\\mathrm{intCE}}(\\mathcal{D},\\mathcal{A}^{\\mathrm{truthful}}(\\mathcal{D}))=\\Omega(\\sqrt{T})$ ", "page_idx": 16}, {"type": "text", "text": "For Laplace-kernel calibration, let $f_{0}$ be an arbitrary function in $\\widetilde{\\mathcal F}$ such that $f_{0}(1/2)>0$ , e.g., we can take $f_{0}(x)=c e^{-x^{2}}$ for a sufficiently small constant $c>0$ . Then, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathsf{k C E}^{\\mathsf{L a p}}(x,p)\\geq\\operatorname*{sup}_{f\\in\\{f_{0},-f_{0}\\}}\\sum_{\\alpha\\in[0,1]}f(\\alpha)\\cdot\\Delta_{\\alpha}\\geq\\Omega(1)\\cdot|\\Delta_{1/2}|.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "It follows that $\\mathsf{e r r}_{\\mathsf{k C E}^{\\mathsf{L a p}}}\\big(\\mathscr{D},\\mathcal{A}^{\\mathrm{truthful}}(\\mathscr{D})\\big)\\geq\\Omega(1)\\cdot\\mathbb{E}\\left[|\\Delta_{1/2}|\\right]=\\Omega(\\sqrt{T}).$ ", "page_idx": 16}, {"type": "text", "text": "Strategic forecaster has a zero penalty.   Consider the same strategic forecaster as in the proof of Proposition A.1: For each $k\\in[T/3]$ ", "page_idx": 16}, {"type": "text", "text": "Clearly, this guarantees that $\\Delta_{\\alpha}=0$ holds for all $\\alpha\\in[0,1]$ . By definition, we have $\\mathsf{O P T}_{\\mathsf{s m C E}}(\\mathcal{D})=$ $\\mathsf{O P T}_{\\mathsf{C a l D i s t}}({\\cal D})\\,=\\,0$ . It also easily follows that both intCE and $k C E^{\\mathsf{L a p}}$ evaluate to O. For intCE, we consider the interval partition ${\\mathcal Z}~=~\\{\\{0\\},(0,1/2),\\{1/2\\},(1/2,1),\\{1\\}\\}$ \uff0cwhich witnesses int ${\\mathsf{C E}}(x,p)\\;=\\;0$ For $k C E^{\\mathsf{L a p}}$ , the summation $\\begin{array}{r}{\\sum_{t=1}^{T}f(p_{t})\\,\\cdot\\,(x_{t}\\,-\\,p_{t})\\;=\\;\\sum_{\\alpha\\in[0,1]}f(\\alpha)\\,\\cdot\\,\\Delta_{\\alpha}}\\end{array}$ evaluates to O for all $f\\in\\widetilde{\\mathcal F}$ . This proves ${\\mathsf{O P T}}_{\\mathsf{i n t C E}}({\\mathcal{D}})={\\mathsf{O P T}}_{\\mathsf{k C E}^{\\mathsf{L a p}}}({\\mathcal{D}})=0$ \u53e3 ", "page_idx": 16}, {"type": "text", "text": "A.4 $O(1){\\bullet}\\Omega({\\sqrt{T}})$ Truthfulness Gap of U-Calibration ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "For the U-calibration error, we prove a slightly smaller truthfulness gap of $O(1){\\mathfrak{\u2013}}\\Omega({\\sqrt{T}})$ , via amore involved analysis. ", "page_idx": 16}, {"type": "text", "text": "Proposition A.3. The $U$ -calibration error has an $O(1){\\cdot}\\Omega({\\sqrt{T}})$ truthfulness gap. ", "page_idx": 16}, {"type": "text", "text": "Proof. We use a slightly different construction: the product distribution $\\begin{array}{r}{\\mathcal{D}=\\prod_{t=1}^{T}}\\end{array}$ Bernoulli $\\left(p_{t}^{\\star}\\right)$ where $p_{t}^{\\star}=1/2$ for $t\\leq T/2$ and $p_{t}^{\\star}=1$ for $t>T/2$ ", "page_idx": 16}, {"type": "text", "text": "Truthful forecaster has an $\\Omega({\\sqrt{T}})$ penalty. We first show that the truthful forecaster has an $\\Omega({\\sqrt{T}})$ Ucalibraton rrrLranmriable $\\begin{array}{r}{X:=\\sum_{t=1}^{T/2}x_{t}}\\end{array}$ denote the number of ones among the first $T/2$ random bits. Note that $X$ follows Binomia $\\bar{|(T/2,1/2)}$ . Consider the scoring rule defined as: ", "page_idx": 16}, {"type": "equation", "text": "$$\nS(0,\\alpha)=\\operatorname{sgn}(\\alpha-1/2)\\quad{\\mathrm{and}}\\quad S(1,\\alpha)=\\operatorname{sgn}(1/2-\\alpha).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Note that $S$ is proper, since for any $\\alpha\\in[0,1]$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\underset{x\\sim\\mathrm{Bernoulif}(\\alpha)}{\\mathbb{E}}[S(x,\\beta)]=(1-\\alpha)\\cdot\\mathrm{sgn}(\\beta-1/2)+\\alpha\\cdot\\mathrm{sgn}(1/2-\\beta)=(1-2\\alpha)\\cdot\\mathrm{sgn}(\\beta-1/2).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which is always minimized at $\\beta=\\alpha$ ", "page_idx": 16}, {"type": "text", "text": "The total loss (w.r.t. $S$ ) incurred by the forecaster is then ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}S(x_{t},p_{t})=X\\cdot S(1,1/2)+(T/2-X)\\cdot S(0,1/2)+T/2\\cdot S(1,1)}\\\\ &{\\quad\\quad\\quad\\quad\\quad=X\\cdot0+(T/2-X)\\cdot0+T/2\\cdot(-1)}\\\\ &{\\quad\\quad\\quad\\quad\\quad=-T/2.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "On the other hand, the total loss incurred by a fixed prediction $\\beta\\in[0,1]$ is given by: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{t=1}^{T}S(x_{t},\\beta)=(T/2+X)\\cdot S(1,\\beta)+(T/2-X)\\cdot S(0,\\beta)}}\\\\ &{=(T/2+X)\\cdot\\mathrm{sgn}(1/2-\\beta)+(T/2-X)\\cdot\\mathrm{sgn}(\\beta-1/2)}\\\\ &{=2X\\cdot\\mathrm{sgn}(1/2-\\beta).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By choosing $\\beta=1$ , we can obtain a total loss of $-2X$ . Therefore, whenever $X\\geq T/4$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathsf{U C a l}(x,p)\\geq-T/2-(-2X)=2(X-T/4).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "When $X<T/4$ , we always have $\\mathsf{U C a l}(x,p)\\geq0$ , since the trivial scoring rule $S\\equiv0$ is proper. This shows that the truthful forecaster gives ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathsf{e r r}_{\\mathsf{U C a l}}({\\mathcal D},\\mathcal{A}^{\\mathrm{ruthful}}({\\mathcal D}))\\geq\\underset{X\\sim\\mathrm{Binomial}(T/2,1/2)}{\\mathbb{E}}\\left[\\operatorname*{max}\\{2(X-T/4),0\\}\\right]=\\Omega(\\sqrt{T}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Strategic forecaster with an $O(1)$ penalty. We consider an alternative forecaster $\\boldsymbol{\\mathcal{A}}$ , which is slightly more involved: ", "page_idx": 17}, {"type": "text", "text": "\u00b7 At every step $t\\leq T/2$ , predict $p_{t}=5/8$   \n\u00b7 For $t=T/2+1,T/2+2,.\\,.\\,.\\,,T$ predict $p_{t}=5/8$ until $|\\Delta_{5/8}|\\leq1$ at some time $t$ .After that step, predict $p_{t}=1$ ", "page_idx": 17}, {"type": "text", "text": "We frst argue that the condition $|\\Delta_{5/8}|\\leq1$ must hold at some point. Reallthat $\\textstyle X=\\sum_{t=1}^{T/2}x_{t}$ By a Chernoff bound, $X$ falls into $[T/8,5T/16]$ except with probability $e^{-\\Omega(T)}$ .Assuming this, we have $\\Delta_{5/8}=X-(T/2)\\cdot(5/8)\\leq0$ at time $t=T/2$ . Furthermore, if we hypothetically predict $5/8$ for each of the last $T/2$ steps, we would have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\Delta_{5/8}=(X+T/2)-T\\cdot(5/8)\\geq T/8+T/2-5T/8=0\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "after all the $T$ steps. Since $\\Delta_{5/8}$ changes by at most 1 at each step, we must hit the condition $|\\Delta_{5/8}|\\leq1$ at some point. ", "page_idx": 17}, {"type": "text", "text": "Therefore, except with an $e^{-\\Omega(T)}$ probability, we end up with $\\Delta_{5/8}\\in[-1,1]$ . Furthermore, we predict at most two different values: $5/8$ and 1. For every fixed proper scoring rule $S:\\{0,1\\}\\times$ $\\left[0,1\\right]\\rightarrow\\left[-1,1\\right]$ ,we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}S(x_{t},p_{t})-\\operatorname*{inf}_{\\beta\\in[0,1]}\\sum_{t=1}^{T}S(x_{t},\\beta)}\\\\ &{\\le\\displaystyle\\sum_{\\alpha\\in\\{5/8,1\\}}\\left[\\sum_{t=1}^{T}S(x_{t},p_{t})\\cdot\\mathbb{1}\\left[p_{t}=\\alpha\\right]-\\operatorname*{inf}_{\\beta\\in[0,1]}\\sum_{t=1}^{T}S(x_{t},\\beta)\\cdot\\mathbb{1}\\left[p_{t}=\\alpha\\right]\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "In the above, we divide the time horizon $[T]$ into two parts, based on whether $5/8$ or 1 is predicted. The inequality holds since the right-hand side allows different values of $\\beta$ for different parts. Clearly, the term corresponding to $\\alpha=1$ has zero contribution, since it reduces to $S(1,1)-\\operatorname*{inf}_{\\beta\\in[0,1]}S(1,\\beta)$ times the number of times 1 is predicted, which evaluates to 0 by definition of proper scoring rules. ", "page_idx": 17}, {"type": "text", "text": "The term corresponding to $\\alpha=5/8$ , on the other hand, is given by ", "page_idx": 17}, {"type": "equation", "text": "$$\nN_{0}\\cdot S(0,5/8)+N_{1}\\cdot S(1,5/8)-\\operatorname*{inf}_{\\beta\\in[0,1]}[N_{0}\\cdot S(0,\\beta)+N_{1}\\cdot S(1,\\beta)],\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where each $N_{b}$ denotes the number of steps on which $5/8$ is predicted and the outcome is $b\\in\\{0,1\\}$ By definition of proper scoring rules, the infimum is achieved by $\\begin{array}{r}{\\beta^{\\star}=\\frac{N_{1}}{N_{0}+N_{1}}}\\end{array}$ No+N , and the above can be further simplified into ", "page_idx": 18}, {"type": "equation", "text": "$$\n(N_{0}+N_{1})\\cdot[S\\left(\\beta^{\\star},5/8\\right)-S\\left(\\beta^{\\star},\\beta^{\\star}\\right)],\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $S(\\alpha,\\beta):=\\alpha\\cdot S(1,\\beta)+(1-\\alpha)\\cdot S(0,\\beta)$ is the linear extension of $S$ to $[0,1]^{2}$ ", "page_idx": 18}, {"type": "text", "text": "Let $\\ell(\\alpha):=S(\\alpha,\\alpha)$ denote the uni-variate form of $S$ . The following is a standard fact about proper scoring rules (see e.g., [KLST23, Lemma 1 and Corollary 2]). ", "page_idx": 18}, {"type": "text", "text": "Lemma A.4. For any proper scoring rule $S:[0,1]^{2}\\rightarrow[-1,1]$ and its uni-variate form $\\ell:[0,1]\\to$ $[-1,1]$ , it holds for all $\\alpha,\\beta\\in[0,1]$ that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bullet\\;S(\\alpha,\\beta)=\\ell(\\beta)+(\\alpha-\\beta)\\cdot\\ell^{\\prime}(\\beta)}\\\\ &{\\bullet\\;|\\ell^{\\prime}(\\alpha)|\\leq2\\,f\\!o r\\,a l l\\,\\alpha\\in[0,1].}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "In particular, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n|S({\\beta}^{\\star},5/8)-S(5/8,5/8)|=|{\\beta}^{\\star}-5/8|\\cdot{\\ell}^{\\prime}(5/8)\\leq2|{\\beta}^{\\star}-5/8|\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and ", "page_idx": 18}, {"type": "equation", "text": "$$\n|S(5/8,5/8)-S(\\beta^{\\star},\\beta^{\\star})|=|\\ell(5/8)-\\ell(\\beta^{\\star})|\\le2|\\beta^{\\star}-5/8|.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "It follows that, assuming $X\\in[T/8,5T/16]$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathsf{U C a l}(x,p)\\leq4(N_{0}+N_{1})|\\beta^{\\star}-5/8|=4\\left|N_{1}-\\frac{5}{8}(N_{0}+N_{1})\\right|=4|\\Delta_{5/8}|\\leq4.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "When $X\\in[T/8,5T/16]$ does not hold (which happens with probability $e^{-\\Omega(T)}$ ), the U-calibration error is trivially upper bounded by ${\\cal O}(T)$ . It follows that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathsf{O P T}_{\\mathsf{U C a l}}({\\mathcal D})\\leq\\mathsf{e r r}_{\\mathsf{U C a l}}({\\mathcal D},{\\cal A})\\leq4+O(T)\\cdot e^{-\\Omega(T)}=O(1).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "A.5 Lack of Completeness ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Every scoring rule $S\\,:\\,\\{0,1\\}\\,\\times\\,[0,1]\\;\\to\\;[0,1]$ induces a calibration measure $\\mathsf{C M}^{(S)}(x,p)\\,:=$ $\\textstyle\\sum_{t=1}^{T}S(x_{t},p_{t})$ When $S$ is proper, it is easy to show that the resulting ${\\mathsf{C M}}^{(S)}$ is perfectly truthful, i.e., $(1,0)$ -truthful. ", "page_idx": 18}, {"type": "text", "text": "A drawback of such calibration measures is that they all lack completeness. Concretely, consider the squared loss $S(x,p):=(x-p)^{2}$ . When the outcomes $x_{1},x_{2},\\ldots,x_{T}$ are independent and uniformly random bits, the \u201cright\u201d\u2019 prediction $p_{t}\\equiv1/2$ gives a total penalty of $T/4$ , which is only a constant factor away from the maximum possible penalty of $T$ . This violates the completeness property in Definition 2.2. In contrast, as shown in Table 2, almost all the other calibration measures would evaluate to $\\ll T$ in this case. Such an asymptotic gap better justifies the intuition that $p_{t}\\equiv1/2$ is a much better prediction than, say, $p_{t}\\equiv0$ ", "page_idx": 18}, {"type": "text", "text": "More generally, unless the proper scoring rule $S$ is trivial, we may find $(x_{0},p_{0})\\in\\{0,1\\}\\times(0,1)$ such that $S(x_{0},p_{0})>0$ . Then, on a sequence of independent samples from Bernoulli $\\left(p_{0}\\right)$ ,wehave ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad\\underset{x_{1},\\ldots,x_{T}\\sim\\mathsf{B e r n o u l i}(p_{0})}{\\mathbb{E}}\\left[\\mathsf{C M}_{T}^{(S)}(x,p_{0}\\cdot\\vec{1})\\right]\\ge T\\cdot S(x_{0},p_{0})\\cdot\\underset{X\\sim\\mathsf{B e r n o u l i}(p_{0})}{\\mathbb{P}}[X=x_{0}]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\ge T\\cdot S(x_{0},p_{0})\\cdot\\operatorname*{min}\\{p_{0},1-p_{0}\\}=\\Omega(T),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which violates the completeness condition in Definition 2.2 ", "page_idx": 18}, {"type": "text", "text": "We also note that $\\mathsf{s m C E}(x,p)+\\sqrt{T}$ gives a calibration measure that is trivially truthful: Implicit in the proof of [QZ24, Theorem 3], the truthful forecaster gives an $O({\\sqrt{T}})$ smooth calibration error on every distribution $\\mathcal{D}\\in\\Delta(\\{0,1\\}^{T})$ , so it immediately gives a constant approximation of the optimal error, which is at least $\\sqrt{T}$ . However, this metric is not complete in the sense of Definition 2.2, since it evaluates to $\\sqrt{T}$ instead of O when $p=x$ (i.e., the predictions are binary and perfect). While SSCE also discourages the forecaster from \u201cover-optimizing\u201d the metric by introducing some additional noise, the subsampling procedure is arguably more \u201corganic\u201d and better-justified than adding a $\\sqrt{T}$ term. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "B Proof of Theorem 1.1 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We prove Theorem 1.1, which we formally restate below. ", "page_idx": 19}, {"type": "text", "text": "Theorem B.1 (Formal version of Theorem 1.1). For every $p^{\\star}\\in[0,1]^{T}$ on theproduct distribution $\\begin{array}{r}{\\mathcal{D}=\\prod_{t=1}^{T}}\\end{array}$ Bernoulli $\\left(p_{t}^{\\star}\\right)$ . there is a forecaster that achieves an $O(\\log^{3/2}T)$ smooth calibration error and distance from calibration. Moreover, assuming that $p^{\\star}\\in[\\delta,1-\\delta]^{T}$ for afixed constant $\\delta\\in(0,1/2]$ both $\\mathsf{O P T}_{\\mathsf{s m C E}}(\\mathcal{D})$ and $\\mathsf{O P T}_{\\mathsf{C a l D i s t}}(\\mathcal{D})$ are $\\Omega({\\sqrt{T}})$ ", "page_idx": 19}, {"type": "text", "text": "B.1   The Upper Bound Part ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We start by proving the upper bound part of Theorem B.1 by designing a forecasting algorithm. ", "page_idx": 19}, {"type": "text", "text": "The forecasting algorithm. Our proof is based on an algorithm of [QZ24] that works for the special case that $p_{t}^{\\star}\\equiv1/2$ . Their algorithm starts by predicting $1/2$ on the first $T/2$ steps. Depending on the realization of these $T/2$ random bits, it predicts a slightly biased value for the next $T/2$ steps, until the total bias (i.e., the partial sum of $x_{t}-p_{t})$ becomes close to O at some point. If there is still time left, the algorithm repeats the above strategy for the remainder of the time horizon. ", "page_idx": 19}, {"type": "text", "text": "Roughly speaking, [QZ24] shows that a polylog $(T)$ distance from calibration can be achieved by designing a sub-routine with the following three properties: ", "page_idx": 19}, {"type": "text", "text": "\u00b7 Small bias: With high probability, the total bias is $O(1)$ in magnitude at some time $t\\in[T/2,T]$   \n\u00b7 Proximity of predictions: During the sub-routine, the values being predicted lie in a short interval of length $\\mathrm{polylog}(T)/\\sqrt{T}$   \n\u00b7 Sparsity of predictions: During the sub-routine, only $O(1)$ different values are predicted. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "To handle the general case that $p^{\\star}\\in[0,1]^{T}$ is arbitrary, we design an alternative sub-routine, the behavior of which depends on whether the sequence $p^{\\star}$ is \u201csufficiently stationary\u201d in some sense. Let $\\begin{array}{r}{\\mu_{\\mathrm{first}}:=\\frac{1}{T/2}\\sum_{t=1}^{T/2}p_{t}^{\\star}}\\end{array}$ and $\\begin{array}{r}{\\mu_{\\mathrm{second}}:=\\frac{1}{T/2}\\sum_{t=T/2+1}^{T}p_{t}^{\\star}}\\end{array}$ be the averages of the frst and the second halves of the sequence, respectively. Let $\\mu=(\\mu_{\\mathrm{first}}+\\mu_{\\mathrm{second}})/2$ be the overall average. ", "page_idx": 19}, {"type": "text", "text": "\u00b7 Case 1: $|\\mu_{\\mathrm{first}}\\,-\\,\\mu|\\,>\\,\\mathrm{polylog}(T)/\\sqrt{T}$ .When $\\mu_{\\mathrm{first}}$ and $\\mu$ are far away, we predict $\\begin{array}{r}{\\alpha:=\\frac{\\mu_{\\mathrm{first}}+\\mu}{2}}\\end{array}$ at every step. Without loss of generality, suppose that $\\mu_{\\mathrm{first}}<\\mu$ , in which case wehave ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mu_{\\mathrm{first}}<\\alpha<\\mu,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where both inequalities hold with a margin $>\\mathrm{polylog}(T)/\\sqrt{T}$ . Then, with high probability the following two events happen: (1) The total bias is negative at time $T/2$ , i.e., it holds that $\\begin{array}{r}{\\sum_{t=1}^{T/2}x_{t}<\\alpha\\cdot(T/2);}\\end{array}$ $\\alpha$ for the second haf, the bias wil be positive in the ed with hnigh probability .e,T t=1 t > \u03b1 T. Therefore, with high probability, the bias must be close to O at some point in $[\\bar{T}/2,T]$ . In this case, this sub-routine has all the desired properties. ", "page_idx": 19}, {"type": "text", "text": "\u00b7 Case 2: $|\\mu_{\\mathrm{first}}-\\mu|\\leq\\mathrm{polylog}(T)/\\sqrt{T}$ When $\\mu_{\\mathrm{first}}$ and $\\mu$ are close, we use a strategy that is more similar to the algorithm of [QZ24]. For the first half of the sequence, we predict $\\alpha:=\\mu_{\\mathrm{first}}$ . Let $\\begin{array}{r}{\\Delta_{\\mathrm{first}}:=\\sum_{t=1}^{T/2}(x_{t}-\\alpha)}\\end{array}$ denote the total bias at time $T/2$ .Say that $\\Delta_{\\mathrm{first}}\\:\\geq\\:0$ Then, we ill rediet $\\begin{array}{r}{\\beta:=\\mu_{\\mathrm{second}}+\\frac{\\Delta_{\\mathrm{first}}}{T/2}+\\frac{\\mathrm{polylog}(T)}{\\sqrt{T}}}\\end{array}$ in the second half of the sequence. The value of $\\beta$ is chosen such that we can offset the bias incurred in the first half (i.e., the $\\Delta_{\\mathrm{first}}/(T/2)$ term). We also introduce some additional bias (i.e., thepolylog $(T)/\\sqrt{T}$ term), so that we can return to a zero bias with high probability. In this case, our sub-routine predicts two different values ( $\\dot{\\alpha}$ and $\\beta$ ), and they only differ by $\\mathrm{polylog}(T)/\\sqrt{T}$ with high probability. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "Formally, our algorithm is given in Algorithm 1. The actual algorithm is significantly more involved than the outline above. The complication is due to the constraint that all predictions must lie in $[0,1]$ while our choice of $\\beta$ in Case 2 above might be invalid. We circumvent this issue by noting that $\\beta$ can be invalid only if $\\mu_{\\mathrm{first}}$ is too close to either O or 1. In that case, we will choose a different value of $\\alpha$ (i.e., the prediction for the first half), so that the sign of the bias at time $T/2$ is more predictable, and the resulting choice of $\\beta$ will likely be valid. ", "page_idx": 20}, {"type": "text", "text": "Algorithm 1: Forecaster for Product Distributions ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Input: Parameters $p_{1}^{\\star},p_{2}^{\\star},\\ldots,p_{T}^{\\star}$ . Outcomes $x_{1},x_{2},\\ldots,x_{T}$ observed sequentially.   \nOutput: Predictions $p_{1},p_{2},...,p_{T}$   \n$t\\leftarrow0$ $r\\gets0$ .\uff0c   \nwhile $t<T$ do $r\\gets r+1$ $1;T^{(r)}\\gets T-t;H^{(r)}\\gets\\lfloor T^{(r)}/2\\rfloor;$ $T^{(r)}=1$ then predict $p_{T}=0$ and break; $\\begin{array}{r}{\\mu_{\\mathrm{first}}^{(r)}\\leftarrow\\frac{1}{H^{(r)}}\\sum_{s=t+1}^{t+H^{(r)}}p_{s}^{\\star};\\mu_{\\mathrm{second}}^{(r)}\\leftarrow\\frac{1}{H^{(r)}}\\sum_{s=t+H^{(r)}+1}^{t+2H^{(r)}}p_{s}^{\\star};}\\end{array}$ $\\mu^{(r)}\\gets[\\mu_{\\mathrm{first}}^{(r)}+\\mu_{\\mathrm{second}}^{(r)}]/2;\\Delta^{(r)}\\gets0;$ $\\begin{array}{r}{\\cdot|\\mu_{\\mathrm{first}}^{(r)}-\\mu^{(r)}|\\geq\\sqrt{\\frac{2\\ln T^{(r)}}{H^{(r)}}}}\\end{array}$ then $\\alpha^{(r)}\\gets[\\mu_{\\mathrm{first}}^{(r)}+\\mu^{(r)}]/2$ for $i=1,2,\\dots,2H^{(r)}\\;{\\bf d o}$ $t\\gets t+1$ ; Predict $p_{t}\\gets\\alpha^{(r)}$ .\uff0c Observe $x_{t}$ $\\Delta^{(r)}\\gets\\Delta^{(r)}+(x_{t}-p_{t})$ $i>H^{(r)}$ and $|\\Delta^{(r)}|\\leq1$ then break; end else $\\mu_{\\mathrm{first}}^{(r)}\\leq1/2$ then $\\begin{array}{r}{\\tilde{\\mu}_{\\mathrm{first}}^{(r)}\\geq10\\sqrt{\\frac{\\ln{T^{(r)}}}{H^{(r)}}}}\\end{array}$ then a(r) \u2190 \u03bcfrst ; else $\\begin{array}{r}{\\alpha^{(r)}\\leftarrow\\operatorname*{max}\\left\\{\\mu_{\\mathrm{first}}^{(r)}-\\sqrt{\\frac{2\\mu_{\\mathrm{first}}^{(r)}\\ln T^{(r)}}{H^{(r)}}},0\\right\\}\\,;}\\end{array}$ else $\\begin{array}{r}{1-\\mu_{\\mathrm{first}}^{(r)}\\geq10\\sqrt{\\frac{\\ln T^{(r)}}{H^{(r)}}}}\\end{array}$ then \u03b1(r) \u2190 (r) \u03bcfirst else $\\begin{array}{r}{\\alpha^{(r)}\\gets\\operatorname*{min}\\left\\{\\mu_{\\mathrm{first}}^{(r)}+\\sqrt{\\frac{2[1-\\mu_{\\mathrm{first}}^{(r)}]\\ln T^{(r)}}{H^{(r)}}},1\\right\\}\\,;}\\end{array}$ for $i=1,2,\\ldots,H^{(r)}$ do $t\\gets t+1$ ; Predict $p_{t}\\gets\\alpha^{(r)}$ .\uff0c Observe $x_{t}$ $\\Delta^{(r)}\\gets\\Delta^{(r)}+(x_{t}-p_{t})$ end i $\\begin{array}{r l}&{\\mathrm{\\boldmath~\\lambda~}^{(r)}\\ge0\\mathrm{\\then~}\\,\\beta^{(r)}\\leftarrow\\operatorname*{min}\\bigg\\{\\mu_{\\mathrm{second}}^{(r)}+\\Delta^{(r)}/H^{(r)}+\\sqrt{\\frac{\\ln T^{(r)}}{2H^{(r)}}},1\\bigg\\}\\,;}\\\\ &{\\mathrm{\\boldmath~\\lambda~}_{\\mathrm{\\boldmath~\\displaystyle~\\beta~}}^{(r)}\\leftarrow\\operatorname*{max}\\bigg\\{\\mu_{\\mathrm{second}}^{(r)}+\\Delta^{(r)}/H^{(r)}-\\sqrt{\\frac{\\ln T^{(r)}}{2H^{(r)}}},0\\bigg\\}\\,;}\\end{array}$ e for $i=1,2,\\ldots,H^{(r)}$ do $t\\gets t+1$ ; Predict $p_{t}\\gets\\beta^{(r)}$ .\uff0c Observe $x_{t}$ $\\Delta^{(r)}\\gets\\Delta^{(r)}+(x_{t}-p_{t})$ .\uff0c if $|\\Delta^{(r)}|\\leq1$ then break; end   \nend ", "page_idx": 20}, {"type": "text", "text": "1   \n2   \n3   \n4   \n5   \n6   \n7   \n8   \n9   \n10   \n11   \n12   \n13   \n14   \n15   \n16   \n17   \n18   \n19   \n20   \n21   \n22   \n23   \n24   \n25   \n26   \n27   \n28   \n29   \n30   \n31   \n32 ", "page_idx": 20}, {"type": "text", "text": "The analysis. We analyze Algorithm 1 and prove the upper bound in Theorem B.1 in the following three steps: ", "page_idx": 21}, {"type": "text", "text": "\u00b7 First, we break the execution of Algorithm 1 into different rounds of the while-loop, and show that each round brings a polylog $(T)$ smooth calibration error in expectation.   \n\u00b7 Then, using the simple observation that the smooth calibration error is sub-additive, we obtain an upper bound on the overall smooth calibration error.   \n\u00b7 Finally, we use a relation between $\\mathsf{s m C E}({\\boldsymbol{x}},{\\boldsymbol{p}})$ and ${\\mathsf{C a l D i s t}}(x,p)$ when $p$ only contains a few different values (shown by [QZ24]) to translate the upper bound to one on the distance from calibration. ", "page_idx": 21}, {"type": "text", "text": "The first step is the most technical. We fix $r$ and condition on the value of $t$ (equivalently, the value of $T^{(r)}$ ) at the beginning of the $r$ -th round. Note that the event $t=t_{0}$ is solely determined by the realization of $x_{1},x_{2},\\ldots,x_{t_{0}}$ , so conditioning on the value of $t$ , the subsequent bits $x_{t+1}$ through $x_{T}$ are still distributed according to $\\mathcal{D}$ . Let sequences $x^{(r)}$ and $\\boldsymbol{p}^{(r)}$ denote the outcomes and predictions made in the $r$ -th round. Note that the two sequences are of the same length, though the length might vary. ", "page_idx": 21}, {"type": "text", "text": "We classify the rounds into three different types as follows: ", "page_idx": 21}, {"type": "text", "text": "\u00b7 Type 1: The condition $\\begin{array}{r}{|\\mu_{\\mathrm{first}}^{(r)}-\\mu^{(r)}|\\geq\\sqrt{\\frac{2\\ln T^{(r)}}{H^{(r)}}}}\\end{array}$ holds in the if-statement on Line 7. \u00b7 Type 2: $\\begin{array}{r}{|\\mu_{\\mathrm{first}}^{(r)}-\\mu^{(r)}|<\\sqrt{\\frac{2\\ln T^{(r)}}{H^{(r)}}}}\\end{array}$ , and a(r) is set to \u03bcfrst on either Line 16 or Line 19. \u00b7 Type 3: $\\begin{array}{r}{|\\mu_{\\mathrm{first}}^{(r)}-\\mu^{(r)}|<\\sqrt{\\frac{2\\ln T^{(r)}}{H^{(r)}}}}\\end{array}$ and $\\alpha^{(r)}$ is not set to $\\mu_{\\mathrm{first}}^{(r)}$ ", "page_idx": 21}, {"type": "text", "text": "Note that for fixed $p^{\\star}$ , the type of a round is deterministic given $r$ and $T^{(r)}$ ", "page_idx": 21}, {"type": "text", "text": "The three lemmas below give high-probability bounds on the smooth calibration error incurred during eachround. ", "page_idx": 21}, {"type": "text", "text": "Lemma B.2. Conditioning on the value of $T^{(r)}$ ,ifthe $r$ -th round is Type $^{\\,I}$ , it holds with probability $1-O(1/T^{(r)})$ that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathsf{s m C E}(x^{(r)},p^{(r)})\\leq1.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Lemma B.3. Conditioning on the value of $T^{(r)}$ ,if the $r$ -th round is Type 2, it holds with probability $1-O(1/T^{(r)})$ that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathsf{s m C E}(x^{(r)},p^{(r)})\\leq1+O\\left(\\frac{1}{T^{(r)}}\\right)\\cdot\\left[\\Delta_{\\mathrm{first}}^{(r)}\\right]^{2}+O\\left(\\sqrt{\\frac{\\log T^{(r)}}{T^{(r)}}}\\right)\\cdot\\left|\\Delta_{\\mathrm{first}}^{(r)}\\right|,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where \u25b3(r)st denotes the value of $\\Delta^{(r)}$ at the end of the first for-loop (on Line 25) ", "page_idx": 21}, {"type": "text", "text": "Lemma B.4. Conditioning on the value of $T^{(r)}$ ,if the $r$ -th round is Type 3, it holds with probability $1-O(1/T^{(r)})$ that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathsf{s m C E}(x^{(r)},p^{(r)})\\leq1+O\\left(\\frac{1}{T^{(r)}}\\right)\\cdot\\left[\\Delta_{\\mathrm{first}}^{(r)}\\right]^{2}+O\\left(\\sqrt{\\frac{\\log T^{(r)}}{T^{(r)}}}\\right)\\cdot\\left|\\Delta_{\\mathrm{first}}^{(r)}\\right|,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Where \u25b3(rst denotes the value of $\\Delta^{(r)}$ at the end of the first for-loop (on Line 25) ", "page_idx": 21}, {"type": "text", "text": "We first prove the upper bound part of Theorem B.1 using the lemmas above. ", "page_idx": 21}, {"type": "text", "text": "Proof of Theorem B.1, the upper bound part. By Lemmas B.2 through B.4, regardless of the type of the $r$ -th round, it holds with probability $1-O\\left(1/T^{\\left(r\\right)}\\right)$ that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathsf{s m C E}(x^{(r)},p^{(r)})\\leq1+O\\left(\\frac{1}{T^{(r)}}\\right)\\cdot\\left[\\Delta_{\\mathrm{first}}^{(r)}\\right]^{2}+O\\left(\\sqrt{\\frac{\\log T^{(r)}}{T^{(r)}}}\\right)\\cdot\\left|\\Delta_{\\mathrm{first}}^{(r)}\\right|,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\Delta_{\\mathrm{first}}^{(r)}$ is regarded as O if the $r$ throudisT .eaythat tudflithsup on smCE does not hold. Conditioning on that $T^{(r)}\\,=\\,L$ , we always have $\\mathsf{s m C E}(x^{(r)},p^{(r)})\\leq L$ since there are at most $L$ steps in the $r$ -th round. Therefore, we have the inequality ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathsf{s m C E}(x^{(r)},p^{(r)})\\leq1+O\\left(\\frac{1}{L}\\right)\\cdot\\left[\\Delta_{\\mathrm{first}}^{(r)}\\right]^{2}+O\\left(\\sqrt{\\frac{\\log L}{L}}\\right)\\cdot\\left|\\Delta_{\\mathrm{first}}^{(r)}\\right|+L\\cdot\\mathbb{1}\\left[\\mathrm{round}\\ r\\ \\mathrm{fails}\\right].\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We will upper bound the value of $\\mathtt{E}\\left[\\mathsf{s m C E}(x^{(r)},p^{(r)})\\right]$ by taking an expectation over both sides of the above. Therefore, we examine the expectation of $\\lvert\\Delta_{\\mathrm{first}}^{(r)}\\rvert$ and $[\\Delta_{\\mathrm{first}}^{(r)}]^{2}$ conditioning on $T^{(r)}=L$ Wheth $\\Delta_{\\mathrm{first}}^{(r)}$ between $\\begin{array}{r}{X_{\\mathrm{first}}=\\sum_{s=t+1}^{t+H}x_{s}}\\end{array}$ and its men $\\mu_{\\mathrm{{first}}}H$ Since thevaranceof $X_{\\mathrm{{first}}}$ $O(L)$ w have $\\mathbb{E}\\left[\\left|\\Delta_{\\mathrm{first}}^{(r)}\\right|\\right]=O(\\sqrt{L})$ and $\\begin{array}{r}{\\dot{\\left[\\left[\\Delta_{\\mathrm{first}}^{(r)}\\right]^{2}\\right]}=O(L).}\\end{array}$ ", "page_idx": 22}, {"type": "text", "text": "Type 3 rounds are trickier. We assume that $\\mu_{\\mathrm{first}}\\leq1/2$ ; this is without loss of generality since the $\\mu_{\\mathrm{first}}>1/2$ case can be handled by a completely symmetric argument. Then, firs is the diference between $\\begin{array}{r}{X_{\\mathrm{first}}=\\sum_{s=t+1}^{t+H}x_{s}}\\end{array}$ and $\\alpha H$ , and $\\alpha$ may differ from $\\mu_{\\mathrm{first}}$ by at most $\\sqrt{\\frac{2\\mu_{\\mathrm{first}}\\ln{\\cal L}}{H}}$ . This gives ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left[\\Delta_{\\mathrm{first}}^{(r)}\\right]^{2}\\right]=\\mathbb{E}\\left[\\left(X_{\\mathrm{first}}-\\mu_{\\mathrm{first}}H\\right)^{2}\\right]+(\\mu_{\\mathrm{first}}H-\\alpha H)^{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq O(L)+O(\\mu_{\\mathrm{first}}H\\ln L).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Now we use the fact that when $\\mu_{\\mathrm{first}}\\leq1/2$ , the round is Type 3 only if $\\mu_{\\mathrm{first}}<10\\sqrt{\\frac{\\ln T^{(r)}}{H}}$ . This implies ", "page_idx": 22}, {"type": "equation", "text": "$$\nO(\\mu_{\\mathrm{first}}H\\ln L)\\le O(\\sqrt{L}\\cdot\\log^{3/2}L),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which is dominated by the $O(L)$ term. It then follows from Jensen's inequality that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\vert\\Delta_{\\mathrm{frst}}^{(r)}\\right\\vert\\right]\\leq\\sqrt{\\mathbb{E}\\left[\\left[\\Delta_{\\mathrm{first}}^{(r)}\\right]^{2}\\right]}=O(\\sqrt{L}).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Put everything together. Therefore, we have the upper bound ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\left[\\mathsf{s m C E}(x^{(r)},p^{(r)})\\Big|T^{(r)}=L\\right]}\\\\ &{\\leq1+\\mathbb{E}\\left[O\\left(\\frac{1}{L}\\right)\\cdot[\\Delta_{\\mathrm{first}}^{(r)}]^{2}+O\\left(\\sqrt{\\frac{\\log L}{L}}\\right)\\cdot|\\Delta_{\\mathrm{first}}^{(r)}|\\Bigg|T^{(r)}=L\\right]+L\\cdot\\mathrm{Pr}\\left[\\mathrm{round}\\ r\\;\\mathrm{fails}\\Big|T^{(r)}=L\\right]}\\\\ &{\\leq1+O(\\sqrt{\\log L})+L\\cdot O(1/L)=O(\\sqrt{\\log T}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The second step aplies our earlie conlusion that $\\mathbb{E}\\left[\\left|\\Delta_{\\mathrm{first}}^{(r)}\\right|\\right]=O(\\sqrt{L})$ and $\\mathbb{E}\\left[[\\Delta_{\\mathrm{first}}^{(r)}]^{2}\\right]=O(L)$ conditioning on $T^{(r)}\\,=\\,L$ .Taking another expectation over the randomness in $T^{(r)}$ shows that ${\\mathsf{s m C E}}(x^{(r)},\\bar{p}^{(r)})=O({\\sqrt{\\log T}})$ for every $r$ . Note that we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{s m C E}(x,p)=\\displaystyle\\operatorname*{sup}_{f\\in\\mathcal{F}}\\displaystyle\\sum_{t=1}^{T}f(p_{t})\\cdot(x_{t}-p_{t})}\\\\ &{\\phantom{s m C E}=\\displaystyle\\operatorname*{sup}_{f\\in\\mathcal{F}}\\sum_{r\\ v\\ n}f(p_{t}^{(r)})\\cdot(x_{t}^{(r)}-p_{t}^{(r)})}\\\\ &{\\phantom{s m c E}\\leq\\displaystyle\\sum_{r}\\operatorname*{sup}_{f\\in\\mathcal{F}}\\displaystyle\\sum_{t}f(p_{t}^{(r)})\\cdot(x_{t}^{(r)}-p_{t}^{(r)})}\\\\ &{=\\displaystyle\\sum_{r}\\mathsf{s m C E}(x^{(r)},p^{(r)}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Furthermore, there are at most ${\\cal O}(\\log T)$ rounds. It follows that $\\mathbb{E}\\left[\\mathsf{s m C E}(x,p)\\right]=O(\\log^{3/2}T)$ ", "page_idx": 23}, {"type": "text", "text": "Finally, we note that in each round of the while-loop, the forecaster predicts at most 2 different values (namely, $\\alpha^{(r)}$ and $\\beta^{(r)}$ ). Therefore,the predictions $p_{1},p_{2},\\dots,p_{T}$ contain at most ${\\cal O}(\\log T)$ different values. By [QZ24, Theorem 2], we conclude that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\mathsf{C a}|\\mathsf{D i s t}(x,p)\\right]\\leq O(1)\\cdot\\mathbb{E}\\left[\\mathsf{s m C E}(x,p)+|\\{p_{1},p_{2},\\dots,p_{T}\\}|\\right]=O(\\log^{3/2}T).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Now we prove Lemmas B.2 through B.4. In the proofs below, we frequently drop the superscript $(r)$ since we only refer to the $r$ -th round. ", "page_idx": 23}, {"type": "text", "text": "Proof of Lemma B.2. Recall that a Type 1 round is one in which the condition $\\left|\\mu_{\\mathrm{first}}\\textrm{--}\\mu\\right|\\ \\geq$ $\\sqrt{\\frac{2\\ln T^{(r)}}{H}}$ holds in the ifstatement. We say that the round succeeds, if we exit the for-loopusing the \u201cbreak\u201d' statement on Line 12, i.e., the condition $i>H$ and $|\\Delta^{(r)}|\\leq1$ holds at some point (including in the last iteration where $i=2H$ ); otherwise, the round fails. ", "page_idx": 23}, {"type": "text", "text": "Note that only one value (namely, $\\alpha^{(r)}$ ) is predicted within the round. Thus, if the round succeeds, wehave ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathsf{s m C E}(x^{(r)},p^{(r)})=\\left|\\Delta_{\\alpha^{(r)}}\\right|=\\left|\\Delta^{(r)}\\right|\\leq1.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "It remains to control the probability for a Type 1 round to fail. Consider random variables ", "page_idx": 23}, {"type": "equation", "text": "$$\nX_{\\mathrm{first}}:=\\sum_{s=t+1}^{t+H}x_{s}\\quad\\mathrm{and}\\quad X:=\\sum_{s=t+1}^{t+2H}x_{s}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Note that both are sums of independent Bernoulli random variables, with I $\\mathfrak{L}\\left[X_{\\mathrm{first}}\\right]=\\mu_{\\mathrm{first}}H$ and $\\mathbb{E}\\left[X\\right]=2\\mu H$ . Also note that since $\\alpha=(\\mu_{\\mathrm{first}}+\\mu)/2$ wehave ", "page_idx": 23}, {"type": "equation", "text": "$$\n|\\mu_{\\mathrm{first}}-\\alpha|=|\\mu-\\alpha|={\\frac{1}{2}}\\left|\\mu_{\\mathrm{first}}-\\mu\\right|\\geq{\\sqrt{\\frac{\\ln{T^{(r)}}}{2H}}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Without loss of generality, suppose that $\\mu_{\\mathrm{first}}\\leq\\mu$ . By an additive Chernoff bound, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathrm{Pr}\\left[X_{\\mathrm{first}}/H\\geq\\alpha\\right]\\leq\\exp\\left(-2H\\left(\\alpha-\\mu_{\\mathrm{first}}\\right)^{2}\\right)\\leq{\\frac{1}{T^{(r)}}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[X/(2H)\\leq\\alpha\\right]\\leq\\exp\\left(-4H\\left(\\alpha-\\mu\\right)^{2}\\right)\\leq{\\frac{1}{T^{(r)}}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Therefore, except with probability ${\\cal O}(1/T^{(r)})$ , we have both $X_{\\mathrm{first}}<\\alpha H$ and $X>2\\alpha H$ In other words, if the for-loop (hypothetically) runs all the $2H$ iterations, we would have $\\Delta^{(r)}<0$ at the end of the $H$ -th iteration, and $\\Delta^{(r)}>0$ at the end of the $2H$ -th iteration. Since $\\Delta^{(r)}$ changes by $|x_{t}-p_{t}|\\leq1$ within each iteration, there must be an iteration $i\\in\\{H+1,H+2,\\ldots,2H\\}$ at the end of which $\\Delta^{(r)}$ falls into $[0,1]$ . By definition of Algorithm 1, we exit the for-loop at that time, and the $r$ -th round succeeds. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "Proof of Lemma B.3. Recallthat in a Type 2 round, we have $\\begin{array}{r}{\\left|\\mu_{\\mathrm{first}}-\\mu\\right|<\\sqrt{\\frac{2\\ln T^{(r)}}{H}}}\\end{array}$ and \u03b1 = \u03bcfrst\u00b7 Without loss of generality, suppose that $\\mu_{\\mathrm{first}}\\,\\leq\\,1/2$ ; the case that $\\mu_{\\mathrm{first}}\\,>\\,1/2$ follows from a completely symmetric argument. We say that a Type 2 round succeeds if both conditions below are satisfied: ", "page_idx": 23}, {"type": "text", "text": "\u00b7When $\\beta$ is chosen, the clipping (i.e., taking the minimum with 1 or taking the maximum with O) is not effective. ", "page_idx": 23}, {"type": "text", "text": "\u00b7 We exit the second for-loop through the break statement on Line 30. ", "page_idx": 23}, {"type": "text", "text": "Otherwise, the round fails. ", "page_idx": 24}, {"type": "text", "text": "Again, we first upper bound the smooth calibration error incurred within a successful round, and then control the probability for a round to fail. Since only $\\alpha$ and $\\beta$ are predicted in this round, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathsf{s m C E}(x^{(r)},p^{(r)})=\\operatorname*{sup}_{f\\in\\mathcal{F}}[f(\\alpha)\\cdot\\Delta_{\\alpha}+f(\\beta)\\cdot\\Delta_{\\beta}],\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\textstyle\\Delta_{\\alpha}$ and $\\Delta_{\\beta}$ are defined with respect to $x^{(r)}$ and $\\boldsymbol{p}^{(r)}$ . The above is further given by ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~~\\underset{f\\in\\mathcal{F}}{\\operatorname*{sup}}[f(\\beta)\\cdot(\\Delta_{\\alpha}+\\Delta_{\\beta})+[f(\\alpha)-f(\\beta)]\\cdot\\Delta_{\\alpha}]}\\\\ &{\\leq\\underset{f\\in\\mathcal{F}}{\\operatorname*{sup}}[f(\\beta)\\cdot(\\Delta_{\\alpha}+\\Delta_{\\beta})]+\\underset{f\\in\\mathcal{F}}{\\operatorname*{sup}}[(f(\\alpha)-f(\\beta))\\cdot\\Delta_{\\alpha}]}\\\\ &{=|\\Delta_{\\alpha}+\\Delta_{\\beta}|+|\\alpha-\\beta|\\cdot|\\Delta_{\\alpha}|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Note that $\\Delta_{\\alpha}+\\Delta_{\\beta}$ is exactly the value of $\\Delta^{(r)}$ at the end of the second for-loop, while $\\Delta_{\\alpha}$ is its value after the frst for-loop, i.e., rst Then, assuming that the round succeeds, we have $|\\Delta_{\\alpha}+\\Delta_{\\beta}|\\leq1$ and ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\alpha-\\beta|=|\\mu_{\\mathrm{first}}-\\beta|\\leq|\\mu_{\\mathrm{first}}-\\mu_{\\mathrm{second}}|+|\\mu_{\\mathrm{second}}-\\beta|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\sqrt{\\frac{2\\ln T^{(r)}}{H}}+\\left(\\frac{|\\Delta_{\\mathrm{first}}^{(r)}|}{H}+\\sqrt{\\frac{\\ln T^{(r)}}{2H}}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad=O\\left(\\frac{1}{T^{(r)}}\\right)\\cdot|\\Delta_{\\mathrm{first}}^{(r)}|+O\\left(\\sqrt{\\frac{\\log T^{(r)}}{T^{(r)}}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Plugging the above back into the upper bound on $\\mathsf{s m C E}(x^{(r)},p^{(r)})$ shows that in a successful Type 2 round, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathsf{s m C E}(x^{(r)},p^{(r)})\\le1+O\\left(\\frac{1}{T^{(r)}}\\right)\\cdot[\\Delta_{\\mathrm{first}}^{(r)}]^{2}+O\\left(\\sqrt{\\frac{\\log T^{(r)}}{T^{(r)}}}\\right)\\cdot|\\Delta_{\\mathrm{first}}^{(r)}|.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "In the following, we show that a Type 2 round succeeds with probability $1\\mathrm{~-~}O(1/T^{(r)})$ . Let $\\begin{array}{r}{X_{\\mathrm{first}}:=\\sum_{s=t+1}^{t+H}x_{s}}\\end{array}$ Note that $X_{\\mathrm{{first}}}$ is a sum of $H$ independent Bermnoull random variables and $\\mathbb{E}\\left[X_{\\mathrm{first}}\\right]\\,=\\,\\mu_{\\mathrm{first}}H$ . Furthermore, we have $\\Delta_{\\mathrm{first}}^{(r)}\\,=\\,X_{\\mathrm{first}}\\,-\\,\\mu_{\\mathrm{first}}H$ . By an additive Chernoff bound, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[|\\Delta_{\\mathrm{first}}^{(r)}|\\leq\\sqrt{\\frac{H\\ln T^{(r)}}{2}}\\right]=\\operatorname*{Pr}\\left[|X_{\\mathrm{first}}/H-\\mu_{\\mathrm{first}}|\\leq\\sqrt{\\frac{\\ln T^{(r)}}{2H}}\\right]\\geq1-\\frac{2}{T^{(r)}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Recall that we need to argue that no clipping is applied when $\\beta$ is chosen. We analyze the following twocases: ", "page_idx": 24}, {"type": "text", "text": "\u00b7 Case 1. $\\Delta_{\\mathrm{first}}^{(r)}\\geq0$ . In this case, we need to show that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mu_{\\mathrm{second}}+\\frac{\\Delta_{\\mathrm{first}}^{(r)}}{H}+\\sqrt{\\frac{\\ln{T^{(r)}}}{2H}}\\leq1.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Recallthat we assumed $\\mu_{\\mathrm{first}}\\leq1/2$ and $\\begin{array}{r}{\\left|\\mu_{\\mathrm{first}}-\\mu\\right|<\\sqrt{\\frac{2\\ln T^{(r)}}{H}}}\\end{array}$ . The latter further implies $\\begin{array}{r}{|\\mu_{\\mathrm{first}}-\\mu_{\\mathrm{second}}|=2|\\mu_{\\mathrm{first}}-\\mu|<\\sqrt{\\frac{8\\ln T^{(r)}}{H}}}\\end{array}$ Thus, it suffces to prove that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sqrt{\\frac{8\\ln T^{(r)}}{H}}+\\frac{|\\Delta_{\\mathrm{first}}^{(r)}|}{H}+\\sqrt{\\frac{\\ln T^{(r)}}{2H}}\\leq\\frac{1}{2}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "When $\\begin{array}{r}{\\lvert\\Delta_{\\mathrm{first}}^{(r)}\\rvert\\leq\\sqrt{\\frac{H\\ln T^{(r)}}{2}}}\\end{array}$ (i.e., the event in Equation (3) holds), the left-hand side above is $\\begin{array}{r}{O\\left(\\sqrt{\\frac{\\log T^{(r)}}{T^{(r)}}}\\right)}\\end{array}$ , which is below $1/2$ as long as $T^{(r)}$ exceeds some universal constant $T_{0}$ Therefore, the probability that a clipping is applied is at most ${\\cal O}(1/T^{(r)})$ , where we absorb the constraint $T^{(r)}\\geq T_{0}$ into the hidden constant in $O(\\cdot)$ ", "page_idx": 24}, {"type": "text", "text": "\u00b7 Case 2. \u25b3(r)f $\\Delta_{\\mathrm{first}}^{(r)}<0$ . In this case, we need to show that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mu_{\\mathrm{second}}+\\frac{\\Delta_{\\mathrm{first}}^{(r)}}{H}-\\sqrt{\\frac{\\ln{T^{(r)}}}{2H}}\\geq0.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "$\\begin{array}{r}{\\mu_{\\mathrm{first}}\\geq10\\sqrt{\\frac{\\ln T^{(r)}}{H}}}\\end{array}$ prove that ", "page_idx": 25}, {"type": "equation", "text": "$$\n10\\sqrt{\\frac{\\ln T^{(r)}}{H}}-\\sqrt{\\frac{2\\ln T^{(r)}}{H}}-\\frac{|\\Delta_{\\mathrm{frst}}^{(r)}|}{H}-\\sqrt{\\frac{\\ln T^{(r)}}{2H}}\\ge0.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The above holds whenever the event in Equation (3) happens, since $10-\\sqrt{2}-1/\\sqrt{2}-$ $1/\\sqrt{2}>0$ ", "page_idx": 25}, {"type": "text", "text": "Finally, we argue that, with high probability, we exit the second for-loop via the break statement. Let $\\begin{array}{r}{X_{\\mathrm{second}}:=\\sum_{s=t+H+1}^{t+2H}x_{s}}\\end{array}$ denote the total outcome in the second haf By symetry, we only deal with the case that \u25b3rst $\\Delta_{\\mathrm{first}}^{(r)}\\geq0$ wherewe have $\\begin{array}{r}{\\beta=\\mu_{\\mathrm{second}}+\\Delta_{\\mathrm{first}}^{(r)}/H+\\sqrt{\\frac{\\ln{T^{(r)}}}{2H}}}\\end{array}$ /mT( . If the second for-loop runs all the $H$ iterations in full, at the end of it, the value of $\\Delta^{(r)}$ will be given by ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\Delta_{\\mathrm{first}}^{(r)}+X_{\\mathrm{second}}-\\beta H=X_{\\mathrm{second}}-\\mu_{\\mathrm{second}}H-{\\sqrt{\\frac{H\\ln T^{(r)}}{2}}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Note that the above is non-negative only if $\\begin{array}{r}{X_{\\mathrm{second}}\\leq\\mu_{\\mathrm{second}}H+{\\sqrt{\\frac{H\\ln T^{(r)}}{2}}}}\\end{array}$ , which, by an additive Chernoff bound, holds with probability at most $1/T^{(r)}$ . Therefore, with probability $1-1/T^{(r)}$ , the value of $\\Delta^{(r)}$ must fll into $[-1,0]$ during the second for-loop, and we will take the break statement accordingly. \u53e3 ", "page_idx": 25}, {"type": "text", "text": "Proof of Lemma B.4. Again, without loss of generality, suppose that $\\mu_{\\mathrm{first}}\\,\\leq\\,1/2$ ; the other case follows from a completely symmetric argument. In contrast to Type 1 and Type 2 rounds, we say that a Type 3 round succeeds if all the following conditions hold simultaneously: ", "page_idx": 25}, {"type": "text", "text": "$\\Delta_{\\mathrm{first}}^{(r)}\\geq0$ i.e., $\\Delta^{(r)}\\geq0$ holds at the end of the first for-loop (on Line 25).   \n\u00b7When $\\beta$ is chosen, the clipping (i.e., taking the minimum with 1) is not effective.   \n\u00b7 We exit the second for-loop through the break statement on Line 30. ", "page_idx": 25}, {"type": "text", "text": "Otherwise, the round fails. ", "page_idx": 25}, {"type": "text", "text": "By the same argument as in the proof of Lemma B.3, in a successful Type 3 round, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathsf{s m C E}(x^{(r)},p^{(r)})\\le1+O\\left(\\frac{1}{T^{(r)}}\\right)\\cdot[\\Delta_{\\mathrm{first}}^{(r)}]^{2}+O\\left(\\sqrt{\\frac{\\log T^{(r)}}{T^{(r)}}}\\right)\\cdot|\\Delta_{\\mathrm{first}}^{(r)}|.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The only change in the argument is the upper bound on $\\left|\\alpha-\\beta\\right|$ since $\\alpha$ is no longer equal to $\\mu_{\\mathrm{first}}$ Nevertheless, we still have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\alpha-\\beta|\\le|\\alpha-\\mu_{\\mathrm{frst}}|+|\\mu_{\\mathrm{first}}-\\mu_{\\mathrm{second}}|+|\\mu_{\\mathrm{second}}-\\beta|}\\\\ &{\\qquad\\le\\sqrt{\\frac{2\\mu_{\\mathrm{first}}\\ln T^{(r)}}{H}}+\\sqrt{\\frac{2\\ln T^{(r)}}{H}}+\\left(\\frac{|\\Delta_{\\mathrm{first}}^{(r)}|}{H}+\\sqrt{\\frac{\\ln T^{(r)}}{2H}}\\right)}\\\\ &{\\qquad=O\\left(\\frac{1}{T^{(r)}}\\right)\\cdot|\\Delta_{\\mathrm{first}}^{(r)}|+O\\left(\\sqrt{\\frac{\\log T^{(r)}}{T^{(r)}}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and the rest of the analysis goes through. ", "page_idx": 25}, {"type": "text", "text": "Thus,it remains to show that a Type 3 round succeeds with probability $1-O(1/T^{(r)})$ . Let $X_{\\mathrm{\\scriptsize{first}}}:=$ $\\sum_{s=t+1}^{t+H}x_{s}$ . Note that $X_{\\mathrm{{first}}}$ is a sum of independent Bernoulli random variables and $\\mathbb{E}\\left[X_{\\mathrm{first}}\\right]=$ $\\mu_{\\mathrm{{first}}}H$ . By a multiplicative Chernoff bound, for any $\\delta\\geq0$ ,wehave ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathrm{Pr}\\left[X_{\\mathrm{first}}/H\\leq(1-\\delta)\\mu_{\\mathrm{first}}\\right]\\leq\\exp\\left(-\\delta^{2}\\mu_{\\mathrm{first}}H/2\\right).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "In particular, plugging $\\begin{array}{r}{\\delta=\\sqrt{\\frac{2\\ln T^{(r)}}{\\mu_{\\mathrm{first}}H}}}\\end{array}$ into the above gives ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[X_{\\mathrm{first}}/H\\le\\mu_{\\mathrm{first}}-\\sqrt{\\frac{2\\mu_{\\mathrm{first}}\\ln T^{(r)}}{H}}\\right]\\le\\frac{1}{T^{(r)}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Reall that $\\alpha$ is chosen as the maximum between $\\mu_{\\mathrm{first}}-\\sqrt{\\frac{2\\mu_{\\mathrm{first}}\\ln T^{(r)}}{H}}$ and 0. Thus, with probability at least $1-1/T^{(r)}$ , we have $X_{\\mathrm{first}}/H\\geq\\alpha$ , which is equivalent to $\\Delta^{(r)}\\geq0$ at the end of the first for-loop. ", "page_idx": 26}, {"type": "text", "text": "Then, we need to argue that when $\\beta$ is chosen, we have $\\begin{array}{r}{\\mu_{\\mathrm{second}}+\\Delta^{(r)}/H+\\sqrt{\\frac{\\ln T^{(r)}}{2H}}\\leq1}\\end{array}$ Wewill show the equivalent inequality: ", "page_idx": 26}, {"type": "equation", "text": "$$\n(\\mu_{\\mathrm{second}}-1/2)+\\Delta^{(r)}/H+\\sqrt{\\frac{\\ln{T^{(r)}}}{2H}}\\le1/2.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "For the fist term, we note that since $\\mu=(\\mu_{\\mathrm{first}}\\!+\\!\\mu_{\\mathrm{second}})/2$ , the assumption $\\begin{array}{r}{|\\mu_{\\mathrm{first}}\\!-\\!\\mu|<\\sqrt{\\frac{2\\ln T^{(r)}}{H}}}\\end{array}$ implies $\\begin{array}{r}{\\left|\\mu_{\\mathrm{first}}-\\mu_{\\mathrm{second}}\\right|=O\\left(\\sqrt{\\frac{\\log T^{(r)}}{T^{(r)}}}\\right)}\\end{array}$ . With the additional assumption that $\\mu_{\\mathrm{first}}\\leq1/2$ we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mu_{\\mathrm{second}}-1/2\\leq(\\mu_{\\mathrm{first}}-1/2)+|\\mu_{\\mathrm{first}}-\\mu_{\\mathrm{second}}|\\leq O\\left(\\sqrt{\\frac{\\log T^{(r)}}{T^{(r)}}}\\right).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "For the second term, we note that, at the end of the first for-loop, $\\Delta^{(r)}/H$ is givenby ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\frac{X_{\\mathrm{first}}-\\alpha H}{H}=\\bigg(\\frac{X_{\\mathrm{first}}}{H}-\\mu_{\\mathrm{first}}\\bigg)+(\\mu_{\\mathrm{first}}-\\alpha).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "By an additive Chernoff bound, $\\begin{array}{r}{\\frac{X_{\\mathrm{first}}}{H}\\!-\\!\\mu_{\\mathrm{first}}\\leq O\\left(\\sqrt{\\frac{\\log T^{(r)}}{T^{(r)}}}\\right)}\\end{array}$ holds with probability $1{-}O(1/T^{(r)})$ By our choice of $\\alpha$ $,\\ \\mu\\mathrm{{first}}\\ -\\ \\alpha$ is always $\\begin{array}{r}{O\\left(\\sqrt{\\frac{\\log T^{(r)}}{T^{(r)}}}\\right)}\\end{array}$ Finally, the last term is clearly $\\begin{array}{r}{O\\left(\\sqrt{\\frac{\\log T^{(r)}}{T^{(r)}}}\\right)}\\end{array}$ Therefore, as long as $T^{(r)}$ is larger than a universal constant $T_{0}$ , the total $\\begin{array}{r}{O\\left(\\sqrt{\\frac{\\log T^{(r)}}{T^{(r)}}}\\right)}\\end{array}$ term is upper bounded by $1/2$ . Again, we can absorb the condition $T^{(r)}\\,\\geq\\,T_{0}$ into the big- $O$ notation, so the second condition (that $\\beta$ is not clipped) is satisfied with probability $1-O(1/T^{(r)})$ ", "page_idx": 26}, {"type": "text", "text": "Finally, we argue that we exit the second for-loop via the break statement with high probability. Let $\\begin{array}{r}{X_{\\mathrm{second}}:=\\dot{\\sum_{s=t+H+1}^{t+2H}x_{s}}}\\end{array}$ denote the tal outcome in the second alf. Recll that we have $\\Delta^{(r)}\\geq0$ at the end of the frst for-loop, and that $\\begin{array}{r}{\\beta=\\mu_{\\mathrm{second}}+\\Delta^{(r)}/H+\\sqrt{\\frac{\\ln{T^{(r)}}}{2H}}}\\end{array}$ T If the second for-loop runs all the $H$ iterations in full, at the end of it, the value of $\\Delta^{(r)}$ will be given by ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\Delta_{\\mathrm{first}}^{(r)}+X_{\\mathrm{second}}-\\beta H=X_{\\mathrm{second}}-\\mu_{\\mathrm{second}}H-{\\sqrt{\\frac{H\\ln T^{(r)}}{2}}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Note that the above is non-negative only if $\\begin{array}{r}{X_{\\mathrm{second}}\\leq\\mu_{\\mathrm{second}}H+{\\sqrt{\\frac{H\\ln T^{(r)}}{2}}}}\\end{array}$ , which, by an aditive Chernoff bound, holds with probability at most $1/T^{(r)}$ . Therefore, with probability $1-1/T^{(r)}$ , the value of $\\Delta^{(r)}$ must fll into $[-1,0]$ during the second for-loop, and we willtake the break statement accordingly. \u53e3 ", "page_idx": 26}, {"type": "text", "text": "B.2 The Lower Bound Part ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We prove the lower bound part of Theorem B.1 via a central limit theorem. ", "page_idx": 26}, {"type": "text", "text": "Proof of Theorem B.1, the lower bound part. On the product distribution $\\begin{array}{r}{\\mathcal{D}=\\prod_{t=1}^{T}}\\end{array}$ Bernoulli $\\left(p_{t}^{\\star}\\right)$ the truthful forecaster predicts $p_{t}=p_{t}^{\\star}$ at every step $t$ .Then, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\underset{x\\sim D}{\\mathbb{E}}[\\mathsf{s m C E}({x},{p^{\\star}})]=\\underset{x\\sim D}{\\mathbb{E}}\\left[\\underset{f\\in\\mathcal{F}}{\\operatorname*{sup}}\\sum_{t=1}^{T}f({p_{t}^{\\star}})\\cdot({x}_{t}-{p_{t}^{\\star}})\\right]\\geq\\underset{x\\sim D}{\\mathbb{E}}\\left[\\left|\\sum_{t=1}^{T}({x}_{t}-{p_{t}^{\\star}})\\right|\\right],\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where we use the fact that $\\mathcal{F}$ contains the constant functions $f\\equiv1$ and $f\\equiv-1$ ", "page_idx": 27}, {"type": "text", "text": "Applying the Berry-Esseen theorem [She10] to the random variable $\\begin{array}{r}{X:=\\sum_{t=1}^{T}(x_{t}-p_{t}^{\\star})}\\end{array}$ gives: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\forall x\\in\\mathbb{R},\\;\\left|\\operatorname*{Pr}\\left[X\\leq x\\cdot\\sigma_{0}\\right]-\\Phi(x)\\right|\\leq C_{0}\\cdot\\sigma_{0}^{-1}\\cdot\\rho_{0},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $\\Phi(x)$ is CDF of the standard normal distribution, $C_{0}\\leq0.56$ is a universal constant, and ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sigma_{0}=\\sqrt{\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}\\left[(x_{t}-p_{t}^{\\star})^{2}\\right]}=\\sqrt{\\displaystyle\\sum_{t=1}^{T}p_{t}^{\\star}(1-p_{t}^{\\star})}\\geq\\sqrt{T\\delta(1-\\delta)};}\\\\ &{\\rho_{0}=\\displaystyle\\operatorname*{max}_{t\\in[T]}\\frac{\\mathbb{E}\\left[|x_{t}-p_{t}^{\\star}|^{3}\\right]}{\\mathbb{E}\\left[|x_{t}-p_{t}^{\\star}|^{2}\\right]}=\\displaystyle\\operatorname*{max}_{t\\in[T]}\\frac{p_{t}^{\\star}(1-p_{t}^{\\star})\\cdot[(p_{t}^{\\star})^{2}+(1-p_{t}^{\\star})^{2}]}{p_{t}^{\\star}(1-p_{t}^{\\star})}\\leq1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "In particular, taking $x=-1$ gives: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{Pr}\\left[X\\leq-\\sigma_{0}\\right]\\geq\\Phi(-1)-C_{0}\\cdot\\sigma_{0}^{-1}\\cdot\\rho_{0}=\\Omega(1)-O(1/\\sqrt{T}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "For all sufficiently large $T$ the $O(1/\\sqrt{T})$ term is dominated by the $\\Omega(1)$ term, in which case we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}[\\mathsf{s m C E}(x,p^{\\star})]\\geq\\mathbb{E}\\left[\\left|X\\right|\\right]\\geq\\sigma_{0}\\cdot\\operatorname*{Pr}\\left[X\\leq-\\sigma_{0}\\right]=\\Omega(\\sqrt{T}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Finally, by the inequality ${\\scriptstyle{\\frac{1}{2}}}\\mathsf{s m C E}(x,p)\\leq\\mathsf{C a l D i s t}(x,p)$ [BGHN23, Lemma 5.4 and Theorem 7.3], the distance from calibration incurred by the truthful forecaster is also $\\Omega({\\sqrt{T}})$ \u53e3 ", "page_idx": 27}, {"type": "text", "text": "C Supplemental Materials for Section 5 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "The following is a tighter version of Theorem 5.1. ", "page_idx": 27}, {"type": "text", "text": "Theorem C.1. For any $\\mathcal{D}\\,\\in\\,\\Delta(\\{0,1\\}^{T})$ $\\mathsf{e r r}_{\\mathsf{S S C E}}(\\mathcal{D},\\mathcal{A}^{\\mathrm{truthful}}(\\mathcal{D}))\\,=\\,O(\\mathbb{E}\\left[\\gamma(\\mathrm{Var}_{T})\\right])$ \uff0cwhere $\\gamma(x):={\\binom{x,\\quad\\ x<1,}{\\sqrt{x},\\quad x\\geq1.}}$ ", "page_idx": 27}, {"type": "text", "text": "Proof. Given a function $f:[0,1]\\rightarrow[-1,1]$ and binary vector ${\\boldsymbol{y}}\\in\\left\\{0,1\\right\\}^{T}$ , we define the martingale $\\begin{array}{r}{M_{t}(f,y)\\;:=\\;\\sum_{s=1}^{t}y_{s}\\,\\cdot\\,f(\\bar{p_{s}^{\\star}})\\,\\cdot\\,\\bar{(x_{s}\\,-\\,p_{s}^{\\star})}^{\\!}}\\end{array}$ where $x\\,\\sim\\,\\mathcal{D}$ and we use $\\mathbb{F}_{t}$ to denote the filtration describing the randomness of $M_{T}(f,y)$ up to time $t$ and $p_{t}^{\\star}:=\\mathbb{E}\\left[x_{t}\\,|\\mathbb{F}_{t-1}\\right]$ . Note that, conditioned on $\\mathbb{F}_{t-1}$ \uff0c $x_{t}$ is distributed as a Bernoulli with parameter $p_{t}^{\\star}$ ", "page_idx": 27}, {"type": "text", "text": "We can write the SSCE of a truthful forecaster in terms of $M_{T}(f,y)$ as ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathsf{S S C E}(x,p^{\\star}):=\\underset{y\\sim\\mathrm{Unif}(\\{0,1\\}^{T})}{\\mathbb{E}}\\left[\\operatorname*{sup}_{f\\in\\mathcal{F}}M_{T}(f,y)\\right].\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We now proceed via chaining and define the dyadic scale $\\varepsilon_{k}=2^{1-k}$ for $k=0,1,2,\\ldots$ To cover the set of Lipschtz functions $\\mathcal{F}$ we willusethe sets of piecewiseconstant functions $\\{\\mathcal{F}_{\\delta}\\}_{\\delta>0}$ described in Lemma C.2. For each function $f\\in\\mathcal F$ , let $\\pi_{k}(f)$ be a close function in $\\mathcal{F}_{\\varepsilon_{k}}$ such that $d(f,\\pi_{k}(f))\\leq2\\varepsilon_{k}$ Observe that the covering $\\mathcal{F}_{\\varepsilon_{0}}$ is a singleton and that $\\pi_{k}(f)$ always exists as $\\mathcal{F}_{\\varepsilon_{k}}$ is a $2\\varepsilon_{k}$ -covering of $\\mathcal{F}$ Telescoping then gives ", "page_idx": 27}, {"type": "equation", "text": "$$\nf(x)={(f(x)-\\pi_{M}(f)(x))+\\pi_{0}(f)(x)+\\sum_{i=1}^{M}\\left[\\pi_{i}(f)(x)-\\pi_{i-1}(f)(x)\\right]}\\,,\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "meaning that we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathsf{S S C E}(x,p^{\\star})\\leq\\underset{y\\sim\\mathrm{Unif}(\\{0,1\\}^{T})}{\\mathbb{E}}\\left[\\underset{f\\in\\mathcal{F}}{\\operatorname*{sup}}\\sum_{t=1}^{T}y_{t}\\cdot\\big(f(p_{t}^{\\star})-\\pi_{M}(f)(p_{t}^{\\star})\\big)\\cdot\\big(x_{t}-p_{t}^{\\star}\\big)\\right]\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "First, we can use that $d(f(p_{t}^{\\star})-\\pi_{M}(f)(p_{t}^{\\star}))\\leq2^{2-M}$ to deterministically bound Term A by ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathrm{>Unif}(\\{0,1\\}^{T})}\\left[\\operatorname*{sup}_{f\\in\\mathcal{F}}\\sum_{t=1}^{T}y_{t}\\cdot\\big(f(p_{t}^{\\star})-\\pi_{M}(f)(p_{t}^{\\star})\\big)\\cdot(x_{t}-p_{t}^{\\star})\\right]\\le2^{2-M}\\cdot T.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Second, we can observe that the image of $\\pi_{0}(f)$ is a singleton: $\\mid\\{\\pi_{0}(f)\\mid f\\in\\mathcal{F}\\}\\mid=1$ ; let this unique function be denoted by $f^{\\star}$ . Then, Term B reduces to $\\mathbb{E}_{y\\sim\\mathsf{U n i f}(\\{0,1\\}^{T})}\\left[M_{T}(f^{\\star},y)\\right]$ , which evaluates toOafertaking an expectation ver $x\\sim\\mathcal{D}$ since for every ${\\boldsymbol{y}}\\in\\{0,1\\}^{T}$ \uff0c $(M_{t}(f^{\\star},y))_{0\\leq t\\leq T}$ forms a martingale. Third, we can observe that $\\pi_{i}(f)\\,-\\,\\pi_{i-1}(f)$ is a function from $\\lbrack0,1\\rbrack\\rightarrow$ $\\left\\{-2^{1-i},0,2^{1-i}\\right\\}$ that takeconstant valalong te segments $[(j\\!-\\!1)2^{1-i},j2^{1-i})$ for all $j\\in[2^{i-1}]$ Thus, we can bound the summands of Term C by ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{y\\sim\\mathrm{Unif}(\\{0,1\\}^{T})}{\\mathbb{E}}\\left[\\underset{j\\in\\mathcal{F}_{t}^{\\top}\\tplus\\mathbf{l}}{\\operatorname*{sup}}\\sum_{t\\in\\{\\pi_{i}(f)(p_{t}^{\\star})-\\pi_{i-1}(f)(p_{t}^{\\star})\\}\\cdot(x_{t}-p_{t}^{\\star})\\right]}\\Bigg[\\underset{\\Tilde{y}\\sim\\mathrm{Unif}(\\{0,1\\}^{T})}{\\underbrace{\\sum_{t\\in\\mathbf{l}}^{t-1}}}\\left[\\underset{v\\in\\{0,\\pm2^{-1}\\}}{\\operatorname*{sup}}\\sum_{t=1}^{T}y_{t}\\cdot v\\cdot(x_{t}-p_{t}^{\\star})\\cdot\\mathbf{l}\\left[j2^{1-i}\\leq p_{t}^{\\star}<(j+1)2^{1-i}\\right]\\right]}\\\\ &{\\leq\\underset{j=0}{\\overset{2^{i-1}}{\\sum}}g^{1-i}\\underset{y\\sim\\mathrm{Unif}(\\{0,1\\}^{T})}{\\mathbb{E}}\\left[\\underset{v\\in\\{0,\\pm2^{1-i}\\}}{\\operatorname*{sup}}\\sum_{t=1}^{T}y_{t}\\cdot v\\cdot(x_{t}-p_{t}^{\\star})\\cdot\\mathbf{l}\\left[j2^{1-i}\\leq p_{t}^{\\star}<(j+1)2^{1-i}\\right]\\right]}\\\\ &{\\leq\\underset{j=0}{\\overset{2^{i-1}}{\\sum}}g^{1-i}\\underset{y\\sim\\mathrm{Unif}(\\{0,1\\}^{T})}{\\mathbb{E}}\\left[\\underset{v\\in\\{\\pm1\\}}{\\operatorname*{sup}}\\sum_{t=1}^{T}y_{t}\\cdot v\\cdot(x_{t}-p_{t}^{\\star})\\cdot\\mathbf{l}\\left[j2^{1-i}\\leq p_{t}^{\\star}<(j+1)2^{1-i}\\right]\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Invoking Lemma C.9 with $\\mathcal{G}=\\{x\\mapsto1,x\\mapsto-1\\}$ and $\\mathcal{Z}=\\left[j2^{1-i},(j+1)2^{1-i}\\right)$ , we have that for all $i\\in[M],j\\in\\{0,1,\\dots,2^{i-1}\\}$ , and ${\\boldsymbol{y}}\\in\\left\\{0,1\\right\\}^{T}$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\underset{r\\sim P}{\\mathbb{E}}\\left[\\operatorname*{sup}_{v\\in\\{\\pm1\\}}M_{T}(v,y,i,j)\\right]\\leq(48+8\\ln2)\\underset{x\\sim D}{\\mathbb{E}}\\left[\\gamma\\left(\\sum_{t=1}^{T}p_{t}^{\\star}(1-p_{t}^{\\star})\\mathbb{1}\\left[j2^{1-i}\\leq p_{t}^{\\star}<(j+1)2^{1-i}\\right]\\right)\\right]\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Plugging this into Term C, we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\left[\\mathrm{Term~C}\\right]\\leq\\left(48+8\\ln2\\right)\\underset{i=1}{\\overset{M}{\\sum}}2^{1-i}\\underset{j=0}{\\overset{\\mathbb{Z}}{\\sum}}\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}\\left[\\gamma\\left(\\underset{t=1}{\\overset{T}{\\sum}}p_{t}^{\\star}(1-p_{t}^{\\star})\\mathbb{1}\\left[j2^{1-i}\\leq p_{t}^{\\star}<(j+1)2^{1-i}\\right]\\right)\\right]}\\\\ &{}&{=\\left(48+8\\ln2\\right)\\underset{i=1}{\\overset{M}{\\sum}}2^{1-i}\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}\\left[\\underset{j=0}{\\overset{\\mathcal{2}}{\\sum}}\\gamma\\left(\\underset{t=1}{\\overset{T}{\\sum}}p_{t}^{\\star}(1-p_{t}^{\\star})\\mathbb{1}\\left[j2^{1-i}\\leq p_{t}^{\\star}<(j+1)2^{1-i}\\right]\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{[Term~C]}\\leq\\left(48+8\\ln2\\right)\\displaystyle\\sum_{i=1}^{M}2^{1-i}\\cdot\\sqrt{2^{i-1}+1}\\displaystyle\\mathbb{E}_{x\\sim D}\\left[\\gamma\\left(\\displaystyle\\sum_{j=0}^{2^{i-1}}\\sum_{t=1}^{T}p_{t}^{\\star}(1-p_{t}^{\\star})\\mathbb{1}\\left[j2^{1-i}\\leq p_{t}^{\\star}<(j+1)\\right]\\right)\\right]}\\\\ &{\\qquad\\qquad\\leq\\left(48+8\\ln2\\right)\\displaystyle\\sum_{i=1}^{M}2^{1-i/2}\\displaystyle\\frac{\\mathbb{E}}{x\\sim D}\\left[\\gamma\\left(\\displaystyle\\sum_{t=1}^{T}p_{t}^{\\star}(1-p_{t}^{\\star})\\right)\\right]}\\\\ &{\\qquad\\qquad=\\left(48+8\\ln2\\right)\\cdot\\left(2+2\\sqrt{2}\\right)\\cdot\\displaystyle\\frac{\\mathbb{E}}{x\\sim D}\\left[\\gamma\\left(\\mathrm{Var}_{T}\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Plugging this into (4) and observing that we can choose $M$ to be arbitrarily large, we have as desired ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}\\left[\\mathsf{S S C E}(x,p^{\\star})\\right]\\leq\\underset{M\\in\\mathbb{N}}{\\operatorname*{inf}}\\left[2^{2-M}\\cdot T+\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}\\left[\\mathsf{T e r m}\\,\\mathbf{C}\\right]\\right]}\\\\ &{\\qquad\\qquad\\qquad\\leq(48+8\\ln2)\\cdot(2+2\\sqrt{2})\\cdot\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}\\left[\\gamma\\left(\\mathrm{Var}_{T}\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "C.1 Auxillary Lemmas ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Covering Lipschitz functions. Let us first recall a standard covering of the class of Lipschitz functions ${\\mathcal{F}}\\subseteq[-1,1]^{[0,1]}$ . We will work with the metric $d$ on the functions $\\mathcal{F}$ induced by the $\\infty$ -norm; that is, for any $\\begin{array}{r}{f,g\\in\\mathcal{F},d(f,g):=\\operatorname*{sup}_{x\\in[0,1]}|f(x)-g(x)|}\\end{array}$ . In this section, for $\\delta>0$ and $b~>~a$ where $\\smash{\\frac{b-a}{\\delta}}\\in\\mathbb{Z}$ we will use the shorthand $[a,b]_{\\delta}\\;:=\\;\\{a,a+\\delta,\\ldots,b\\}$ to denote endpoints of partitioning of $[a,b]$ into segments of length $\\delta$ We will also use the shorthand $\\lfloor x\\rfloor_{\\delta}:=$ max $\\{i\\delta\\mid i\\delta\\stackrel{\\cdot}{\\leq}x,i\\in\\mathbb{Z}\\}$ to denote rounding down to the nearest multiple of $\\delta$ ", "page_idx": 29}, {"type": "text", "text": "Lemma C.2. For $\\delta\\,>\\,0$ where $\\textstyle{\\frac{1}{\\delta}}\\,\\in\\,\\mathbb{Z}.$ . consider all functions $f:[0,1]\\,\\rightarrow\\,[-1,1]$ that satisfy conditions ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\forall x\\in[0,1]_{\\delta}:f(x)\\in[-1,1]_{\\delta}}\\\\ &{\\forall x\\in[0,1]_{\\delta}\\setminus\\{1\\}:|f(x+\\delta)-f(x)|\\leq\\delta}\\\\ &{\\forall x\\in[0,1]:f(x)=f(\\lfloor x\\rfloor_{\\delta}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "This set of functions, which we will denote by $\\mathcal{F}_{\\delta}$ is a $2\\delta$ -covering of the set of $^{\\,l}$ -Lipschitz.functions $\\mathcal{F}:[0,1]\\rightarrow[-1,1]$ in the metric $d$ ", "page_idx": 29}, {"type": "text", "text": "Proof. Fix a 1-Lipschitz function $f\\in\\mathcal F$ Let $f^{\\prime}\\in\\mathcal{F}_{\\delta}$ be the function in our covering where, for all $x\\in[0,1]_{\\delta}$ \uff0c $f^{\\prime}(x)\\bar{=}\\,\\lfloor f(x)\\rfloor_{\\delta}$ . Note that $f^{\\prime}$ is unique because the elements of ${\\mathcal{F}}_{\\delta}$ can be identified by their image on $[0,1]_{\\delta}$ . For any $x\\in[0,1]$ , we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|f(x)-f^{\\prime}(x)|\\leq|f(x)-f(\\lfloor x\\rfloor_{\\delta})|+|f^{\\prime}(x)-f^{\\prime}(\\lfloor x\\rfloor_{\\delta})|+|f(\\lfloor x\\rfloor_{\\delta})-f^{\\prime}(\\lfloor x\\rfloor_{\\delta})|}\\\\ &{\\qquad\\qquad\\qquad\\leq|x-\\lfloor x\\rfloor_{\\delta}|+0+|f(\\lfloor x\\rfloor_{\\delta})-f^{\\prime}(\\lfloor x\\rfloor_{\\delta})|}\\\\ &{\\qquad\\qquad\\leq2\\delta,}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where the first inequality is the triangle inequality, the second inequality uses the 1-Lipschitzness of $f$ and that $f^{\\prime}(x)=\\bar{f^{\\prime}}(\\lfloor x\\rfloor_{\\delta})$ and the third inequality uses the fact that $|f(z)-f^{\\prime}(\\bar{z})|\\leq\\delta$ and $|z-\\lfloor z\\rfloor_{\\delta}|\\leq\\delta$ for all $z\\in[0,1]$ \u53e3 ", "page_idx": 29}, {"type": "text", "text": "Bounding sums of $\\gamma$ . Conside the pecewise function $\\gamma(x):={\\binom{x,\\quad\\ x<1,}{\\sqrt{x},\\quad x\\geq1.}}$ Lemma C.3. For all values $x_{1},\\ldots,x_{n}\\geq0$ we can upper bound $\\begin{array}{r}{\\sum_{i=1}^{n}\\gamma(x_{i})\\leq\\sqrt{n}\\cdot\\gamma(\\sum_{i=1}^{n}x_{i})}\\end{array}$ ", "page_idx": 29}, {"type": "text", "text": "Proof. First, suppose that $\\textstyle\\sum_{i=1}^{n}x_{i}\\leq1$ Then, $\\textstyle\\gamma(\\sum_{i=1}^{n}x_{i})=\\sum_{i=1}^{n}x_{i}$ and $x_{i}\\leq1$ for all $i\\in[n]$ The claim is therefore equivalent to the trivial statement $\\begin{array}{r}{\\sum_{i=1}^{n}x_{i}\\le\\sqrt{n}\\cdot\\sum_{i=1}^{n}x_{i}}\\end{array}$ ", "page_idx": 29}, {"type": "text", "text": "Now suppose that $\\textstyle\\sum_{i=1}^{n}x_{i}>1$ The Cauchy-Schwarz inequality gives ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}{\\sqrt{x_{i}}}\\leq{\\sqrt{n}}{\\sqrt{\\sum_{i=1}^{n}x_{i}}}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "By our assumption that $\\textstyle\\sum_{i=1}^{n}x_{i}>1$ , we have $\\textstyle\\gamma(\\sum_{i=1}^{n}x_{i})={\\sqrt{\\sum_{i=1}^{n}x_{i}}}$ We separately have that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}\\gamma(x_{i})\\leq\\sum_{i=1}^{n}{\\sqrt{x_{i}}},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "because $\\gamma(x)=x\\leq\\sqrt{x}$ for $x\\in[0,1]$ and $\\gamma(x)=\\sqrt{x}=\\sqrt{x}$ for $x\\geq1$ . Thus, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}\\gamma(x_{i})\\leq\\sum_{i=1}^{n}{\\sqrt{x_{i}}}\\leq{\\sqrt{n}}{\\sqrt{\\sum_{i=1}^{n}x_{i}}}={\\sqrt{n}}\\cdot\\gamma\\left(\\sum_{i=1}^{n}x_{i}\\right).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "C.2 Epochs of Doubling Realized Variance ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Definition C.4. For ${\\mathcal{T}}\\subseteq[0,1],$ consider the stochastic process $(\\mathrm{Var}_{t}(\\mathcal{T}))_{0\\leq t\\leq T}$ defined as ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\operatorname{Var}_{t}(\\mathcal{Z}):=\\sum_{s=1}^{t}p_{s}^{\\star}(1-p_{s}^{\\star})\\cdot\\mathbb{1}\\left[p_{s}^{\\star}\\in\\mathcal{Z}\\right],\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $x\\sim\\mathcal{D}$ and $p_{t}^{\\star}:=\\operatorname*{Pr}_{x^{\\prime}\\sim\\mathcal{D}}\\left[x_{t}^{\\prime}=1|x_{1:(t-1)}^{\\prime}=x_{1:(t-1)}\\right]$ We define the epochs withrespect to $\\mathcal{T}$ as the sequence $\\tau_{0},\\tau_{1},\\cdot\\cdot\\cdot\\in\\mathbb{N}$ where $\\tau_{0}=0$ and, for each $^{\\,k}\\in[\\lceil\\log_{2}(T)\\rceil+2],$ ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\tau_{k}:=\\operatorname*{min}\\left\\{t\\in[\\tau_{k-1}+1,T]\\mid\\operatorname{Var}_{t}({\\mathbb{Z}})-\\operatorname{Var}_{\\tau_{k-1}}({\\mathbb{Z}})\\geq2^{k-1}\\right\\}\\cup\\{\\infty\\}\\,.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "The epochs $\\tau_{0},\\tau_{1},\\ldots.$ : defined in Definition C.4 partition the $T$ time steps of a martingale into epochs such that the realized variance $\\operatorname{Var}_{t}(\\mathbb{Z})$ increases by approximately $2^{\\hat{k}-1}$ within the $k$ -th epoch. In particular, we can understand $\\tau_{k}$ as pointing to the last time step of the $k$ th epoch. The definition of $\\tau$ ensures that: ", "page_idx": 30}, {"type": "text", "text": "\u00b7 Epoch 1 starts from time step 1, and ends at the earliest time step $t$ such that $\\operatorname{Var}_{t}(\\mathbb{Z})\\geq1=$ $2^{\\dot{0}}$ \u00b7 For $k\\geq2$ , Epoch $k$ starts from the time step after the last step of Epoch $k-1$ and ends at the earliest time step such that the total variance within the epoch reaches $2^{k-1}$ ", "page_idx": 30}, {"type": "text", "text": "We have the following technical facts about the epochs $\\tau$ ", "page_idx": 30}, {"type": "text", "text": "Fact C.5. The ( $\\left[\\log_{2}(T)\\right]+2)$ -thepoch is nevercomlete . $\\tau_{\\lceil\\log_{2}(T)\\rceil+2}=\\infty$ ", "page_idx": 30}, {"type": "text", "text": "Proof. Our definition of $\\operatorname{Var}_{t}(\\mathbb{Z})$ clearly guarantees $\\mathrm{Var}_{T}(\\mathbb{Z})\\leq T$ , which implies ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\operatorname{Var}_{T}({\\mathcal{Z}})-\\operatorname{Var}_{\\tau_{\\lceil\\log_{2}(T)\\rceil+1}}({\\mathcal{Z}})\\leq T<2^{\\lceil\\log_{2}(T)\\rceil+1},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "and therefore, $\\tau_{\\lceil\\log_{2}(T)\\rceil+2}=\\infty$ ", "page_idx": 30}, {"type": "text", "text": "Fact C.6. For every epoch $k\\,\\in\\,[\\lceil\\log_{2}(T)\\rceil+2]$ . the change in realized variance in epoch $k$ is deterministically upper bounded by $\\operatorname{Var}_{\\tau_{k}}(\\bar{\\mathcal{Z}})-\\bar{\\operatorname{Var}}_{\\tau_{k-1}}(\\mathcal{Z})<2^{k-1}+1$ ", "page_idx": 30}, {"type": "text", "text": "Proof. By definition, $\\operatorname{Var}_{\\tau_{k}-1}(\\mathcal{Z})-\\operatorname{Var}_{\\tau_{k-1}}(\\mathcal{Z})<2^{k-1}$ . Because $p_{t}^{\\star}\\in[0,1]$ for all $t\\in[T]$ the realized varianccreasesbyatost $p_{t}^{\\star}(1\\!-\\!p_{t}^{\\star})\\leq1$ in each timestep, i.e. $\\operatorname{Var}_{\\tau_{k}}(\\mathcal{Z})-\\operatorname{Var}_{\\tau_{k}-1}(\\mathcal{Z})\\leq$ 1. The fact follows by summing the two inequalities. ", "page_idx": 30}, {"type": "text", "text": "Fact C.7. For any epoch $k\\in[\\lceil\\log_{2}(T)\\rceil+2]$ the probability that the kthepoch ends is atmst ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{Pr}\\left[\\tau_{k}<\\infty\\right]\\le\\operatorname*{min}\\left\\{\\frac{\\mathbb{E}\\left[\\mathbb{1}\\left[\\operatorname{Var}_{T}(\\mathbb{Z})\\ge\\mathbb{1}\\right]\\cdot\\sqrt{\\operatorname{Var}_{T}(\\mathbb{Z})}\\right]}{\\sqrt{2^{k-1}}},1\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof. The sequence of realized variances $\\operatorname{Var}_{1}({\\mathcal{Z}}),\\ldots,\\operatorname{Var}_{T}({\\mathcal{Z}})$ is deterministically non-decreasing. Thus, for every epoch $k\\in[\\lceil\\log_{2}(T)\\rceil+2]$ \uff0c ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{Pr}\\left[\\tau_{k}<\\infty\\right]\\leq\\operatorname*{Pr}\\left[\\operatorname{Var}_{T}(\\mathbb{Z})-\\operatorname{Var}_{\\tau_{k-1}}\\geq2^{k-1}\\wedge\\operatorname{Var}_{T}(\\mathbb{Z})\\geq1\\right]}\\\\ &{\\qquad\\qquad\\leq\\operatorname*{Pr}\\left[\\mathbb{1}\\left[\\operatorname{Var}_{T}(\\mathbb{Z})\\geq1\\right]\\cdot\\operatorname{Var}_{T}(\\mathbb{Z})\\geq2^{k-1}\\right]}\\\\ &{\\qquad\\qquad=\\operatorname*{Pr}\\left[\\mathbb{1}\\left[\\operatorname{Var}_{T}(\\mathbb{Z})\\geq1\\right]\\cdot\\sqrt{\\operatorname{Var}_{T}(\\mathbb{Z})}\\geq\\sqrt{2^{k-1}}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "with the second inequality following as $\\tau_{1}<\\infty$ implies $\\mathrm{Var}_{T}(\\mathcal{T})\\geq1$ We can next invoke Markov's inequality $\\begin{array}{r}{\\operatorname*{Pr}\\left[X\\geq a\\right]\\leq\\frac{\\mathbb{E}[X]}{a}}\\end{array}$ With $a={\\sqrt{2^{k-1}}}$ and $X=\\mathbb{1}\\left[\\mathrm{Var}_{T}(\\mathbb{Z})\\geq1\\right]\\cdot\\sqrt{\\mathrm{Var}_{T}(\\mathbb{Z})}$ to recover ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{Pr}\\left[\\mathbb{1}\\left[\\operatorname{Var}_{T}(Z)\\geq1\\right]\\cdot\\sqrt{\\operatorname{Var}_{T}}\\geq\\sqrt{2^{k-1}}\\right]\\leq\\operatorname*{min}\\left\\{\\frac{\\mathbb{E}\\left[\\mathbb{1}\\left[\\operatorname{Var}_{T}(Z)\\geq1\\right]\\cdot\\sqrt{\\operatorname{Var}_{T}(Z)}\\right]}{\\sqrt{2^{k-1}}},1\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Fact C.8. The exponentially weighted sum of probabilities that each epoch ends is at most ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\sum_{k=2}^{\\lceil\\log_{2}(T)\\rceil+2}\\sqrt{2^{k-1}}\\operatorname*{Pr}[\\tau_{k-1}<\\infty]\\leq(2\\sqrt{2}+2)\\,\\mathbb{E}\\left[\\mathbb{1}\\left[\\operatorname{Var}_{T}(\\mathbb{Z})\\geq\\mathbb{1}\\right]\\cdot\\sqrt{\\operatorname{Var}_{T}(\\mathbb{Z})}\\right].\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Proof. We will prove the deterministic inequality ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\sum_{k=2}^{\\lceil\\log_{2}(T)\\rceil+2}\\sqrt{2^{k-1}}\\cdot\\mathbb{1}\\left[\\tau_{k-1}<\\infty\\right]\\leq(2\\sqrt{2}+2)\\mathbb{1}\\left[\\mathrm{Var}_{T}(\\mathbb{Z})\\geq1\\right]\\cdot\\sqrt{\\mathrm{Var}_{T}(\\mathbb{Z})};\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "the fact follows from taking an expectation on both sides. ", "page_idx": 31}, {"type": "text", "text": "Let $K=\\operatorname*{max}\\left\\{k\\mid\\tau_{k}<\\infty\\right\\}$ be the number of completed epochs. When $K=0$ wehave $\\mathrm{Var}_{T}(\\mathcal{T})<$ 1, and both sides of the above reduce to 0. Now, suppose that $K\\ge1$ , in which case we have $\\mathrm{Var}_{T}(\\mathcal{T})\\geq1$ . By telescoping, we can lower bound the realized variance by ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\operatorname{Var}_{T}(\\mathcal{Z})\\geq\\sum_{k=1}^{K}2^{k-1}\\geq2^{K-1}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Separately, by definition of $K$ , we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{k=2}^{\\mathrm{\\tiny~log}_{2}(T)\\mid+2}\\sqrt{2^{k-1}}\\cdot\\mathbb{1}\\left[\\tau_{k-1}<\\infty\\right]=\\displaystyle\\sum_{k=2}^{K+1}\\sqrt{2^{k-1}}}&{}\\\\ {\\displaystyle=\\mathbb{1}\\left[\\mathrm{Var}_{T}(\\mathbb{Z})\\geq1\\right]\\displaystyle\\sum_{k=2}^{K+1}\\sqrt{2^{k-1}}}&{}\\\\ {\\displaystyle}&{\\leq\\mathbb{1}\\left[\\mathrm{Var}_{T}(\\mathbb{Z})\\geq1\\right]\\sqrt{2^{K}}(\\sqrt{2}+2)}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "with the second equality following from $\\mathrm{Var}_{T}(\\mathcal{Z})\\geq1$ . Combining the previous two inequalities gives the desired inequality ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\sum_{k=2}^{\\lceil\\log_{2}(T)\\rceil+2}\\sqrt{2^{k-1}}\\cdot\\mathbb{1}\\left[\\tau_{k-1}<\\infty\\right]\\leq(2\\sqrt{2}+2)\\mathbb{1}\\left[\\mathrm{Var}_{T}(\\mathbb{Z})\\geq1\\right]\\cdot\\sqrt{\\mathrm{Var}_{T}(\\mathbb{Z})}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "C.3 Random Walks with Early Stopping ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "We now prove a technical result that the magnitude of a random walk with random variance can be upper bounded by its (expected) standard deviation. Compared to Lemma 5.2, the lemma below gives a bound that depends on $\\bar{\\gamma}(\\mathrm{Var}_{T}(\\mathcal{T}))$ (rather than the square root), and avoids the extra 1 $\\operatorname{og}|{\\mathcal{G}}|\\cdot\\operatorname{log}T$ term. While the leading factor $(\\approx\\log|\\mathcal{G}|)$ is larger than the one in Lemma 5.2 $(\\approx\\sqrt{\\log|\\mathcal G|})$ , we will only apply the bound to the case that $|\\mathcal{G}|=O(1)$ , where the difference between the two is only a constant factor. ", "page_idx": 31}, {"type": "text", "text": "Lemma C.9. Given a function $f:[0,1]\\,\\to\\,[-1,1],$ $y\\,\\in\\,\\{0,1\\}^{T}$ , and set ${\\mathcal{T}}\\subseteq[0,1]$ consider the martingale $\\begin{array}{r}{M_{t}(f,y,\\mathbb{Z}):=\\sum_{s=1}^{t}y_{t}\\cdot f(p_{s}^{\\star})\\cdot(x_{s}-p_{s}^{\\star})\\cdot\\mathbb{1}\\left[p_{s}^{\\star}\\in\\mathbb{Z}\\right]}\\end{array}$ , where $x\\sim\\mathcal{D}$ and $p_{t}^{\\star}=$ $\\operatorname*{Pr}_{x^{\\prime}\\sim\\mathcal{D}}\\left[x_{t}^{\\prime}=1|x_{1:(t-1)}^{\\prime}=x_{1:(t-1)}\\right]$ Then,for any fnite family $\\mathcal{G}$ of functions from $[0,1]\\;t o\\;[-1,1],$ any ${\\boldsymbol{y}}\\in\\{0,1\\}^{T}$ , and any ${\\mathcal{T}}\\subseteq[0,1]$ , we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbb{E}_{x\\sim\\mathcal{D}}\\left[\\operatorname*{max}_{f\\in\\mathcal{G}}M_{T}(f,y,\\mathcal{Z})\\right]\\leq8\\big(6+\\log(|\\mathcal{G}|)\\big)\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}\\left[\\gamma(\\mathrm{Var}_{T}(\\mathcal{Z}))\\right].\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $\\begin{array}{r}{\\mathrm{Var}_{t}(\\mathbb{Z}):=\\sum_{s=1}^{t}p_{s}^{\\star}(1-p_{s}^{\\star})\\cdot\\mathbb{1}\\left[p_{s}^{\\star}\\in\\mathbb{Z}\\right]}\\end{array}$ is the realized variance restricted to subset $\\mathcal{T}$ and $\\gamma(x):={\\binom{x,\\quad\\ x<1,}{\\sqrt{x},\\quad x\\geq1.}}$ ", "page_idx": 32}, {"type": "text", "text": "Proof. Let us decompose the horizon into epochs of doubling realized variance with respect to the subset $\\mathcal{T}$ as per Definition C.4. Using $\\tau$ as defined in (5), we will write $I_{k}:=[\\tau_{k-1}+1:\\operatorname*{min}\\left\\{T,\\tau_{k}\\right\\}]$ to denote the time steps composing epoch $k$ and write $K:=\\operatorname*{max}\\left\\{k\\mid\\tau_{k}<\\dot{\\infty}\\right\\}$ to denote the number of completed epochs. ", "page_idx": 32}, {"type": "text", "text": "We will separately handle the contributions of epoch 1 and those of later epochs ", "page_idx": 32}, {"type": "text", "text": "First epoch._Since $y_{t}\\in\\{0,1\\}$ and $\\|f\\|_{\\infty}\\leq1$ holds for every $f\\in\\mathcal G$ , we can bound the expected contribution from the first epoch as follows: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}\\left[\\operatorname*{max}_{f\\in\\mathcal{G}}\\sum_{t=1}^{\\tau_{1}}y_{t}\\cdot f(p_{t}^{\\star})\\cdot(x_{t}-p_{t}^{\\star})\\cdot\\mathbb{1}\\left[p_{t}^{\\star}\\in\\mathbb{Z}\\right]\\right]\\le\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}\\left[\\sum_{t=1}^{\\tau_{1}}|x_{t}-p_{t}^{\\star}|\\cdot\\mathbb{1}\\left[p_{t}^{\\star}\\in\\mathbb{Z}\\right]\\right].\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Note that for any $p\\in[0,1]$ and Bernoulli random variable $x\\sim$ Bernoulli $(p)$ ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left|x-p\\right|\\right]=\\operatorname*{Pr}\\left[x=0\\right]\\cdot\\left|0-p\\right|+\\operatorname*{Pr}\\left[x=1\\right]\\cdot\\left|1-p\\right|=2p(1-p).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "It thus follows that the process $(X_{t})_{0\\leq t\\leq T}$ where ", "page_idx": 32}, {"type": "equation", "text": "$$\nX_{t}:=\\sum_{s=1}^{t}\\left[|x_{s}-p_{s}^{\\star}|-2p_{s}^{\\star}(1-p_{s}^{\\star})\\right]\\cdot\\mathbb{1}\\left[p_{s}^{\\star}\\in\\mathbb{Z}\\right]\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "is a martingale, as conditioning on any realization of $x_{1:(t-1)}$ ,wehave ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}\\left[\\left|x_{t}-p_{t}^{\\star}\\right|-2p_{t}^{\\star}(1-p_{t}^{\\star})\\ |\\ x_{1:t-1}\\right]=\\underset{x\\sim\\mathsf{B e r n o u l i}(p_{t}^{\\star})}{\\mathbb{E}}\\left[\\left|x-p_{t}^{\\star}\\right|\\right]-2p_{t}^{\\star}(1-p_{t}^{\\star})=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "By the optional stopping theorem, we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\underset{x\\sim D}{\\mathbb{E}}\\left[\\sum_{t=1}^{\\tau_{1}}\\left[\\left|x_{t}-p_{t}^{\\star}\\right|-2p_{t}^{\\star}(1-p_{t}^{\\star})\\right]\\cdot\\mathbb{1}\\left[p_{t}^{\\star}\\in\\mathbb{Z}\\right]\\right]=\\mathbb{E}\\left[X_{\\tau_{1}}\\right]=0.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Plugging this identity into (6) gives ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}\\left[\\underset{f\\in\\mathcal{G}}{\\operatorname*{max}}\\sum_{t=1}^{\\tau_{1}}{y}_{t}\\cdot f(p_{t}^{\\star})\\cdot(x_{t}-p_{t}^{\\star})\\cdot\\mathbb{1}\\left[p_{t}^{\\star}\\in\\mathcal{I}\\right]\\right]\\leq\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}\\left[\\underset{t=1}{\\overset{\\tau_{1}}{\\sum}}2p_{t}^{\\star}(1-p_{t}^{\\star})\\cdot\\mathbb{1}\\left[p_{t}^{\\star}\\in\\mathcal{I}\\right]\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=2\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}\\left[\\mathrm{Var}_{\\tau_{1}}(\\mathcal{Z})\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq2\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}\\left[\\operatorname*{min}\\left\\{2,\\mathrm{Var}_{T}(\\mathcal{Z})\\right\\}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq4\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}\\left[\\operatorname*{min}\\left\\{1,\\mathrm{Var}_{T}(\\mathcal{Z})\\right\\}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where the third step applies Fact C.6 with $k=1$ ", "page_idx": 32}, {"type": "text", "text": "Later epochs. Applying a triangle inequality and the law of total expectation gives ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}\\left[\\operatorname*{max}_{t\\rightarrow t}\\frac{Y}{t\\rightarrow t}\\quad f(p_{t}^{*})\\cdot\\left(x_{t}-p_{t}^{*})\\cdot1[p_{t}^{*}\\in\\mathcal{T}]\\right]}\\\\ &{=\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}\\left[\\operatorname*{max}_{t\\rightarrow0}^{K+1}\\sum_{y\\in\\mathcal{T}_{t}}y_{t}\\cdot f(p_{t}^{*})\\cdot\\left(x_{t}-p_{t}^{*}\\right)\\cdot1[p_{t}^{*}\\in\\mathcal{T}]\\right]}\\\\ &{\\leq\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}\\left[\\frac{\\int_{y\\in\\mathcal{T}}^{K+1}\\operatorname*{max}_{t\\rightarrow0}^{\\infty}\\sum_{y\\in\\mathcal{T}_{t}}y_{t}\\cdot f(p_{t}^{*})\\cdot\\left(x_{t}-p_{t}^{*}\\right)\\cdot1[p_{t}^{*}\\in\\mathcal{T}]}{2\\mathrm{e}^{\\xi_{0}\\xi}\\mathrm{e}^{\\xi_{0}}}\\right]}\\\\ &{=\\underset{y\\in\\mathcal{T}_{0}}{\\underbrace{\\sum_{k=2}^{K}}}\\frac{\\mathbb{E}}{\\nu\\times\\mathcal{D}}\\left[\\operatorname*{max}_{t\\in\\mathcal{T}_{t}}^{T}\\operatorname*{max}_{t\\rightarrow0}^{T}\\cdot f(p_{t}^{*})\\cdot\\left(x_{t}-p_{t}^{*}\\right)\\cdot1[p_{t}^{*}\\in\\mathcal{T}\\wedge t\\in I_{k}\\right]}\\right]}\\\\ &{=\\underset{k=2}{\\overset{\\big[\\mathrm{K}\\otimes_{\\xi\\in\\mathcal{T}}(T)\\big]+2}{\\sum}}\\underset{y\\in\\mathcal{T}_{0}}{\\mathbb{E}}\\left[\\operatorname*{max}_{t\\rightarrow1}^{T}\\operatorname*{max}_{t\\rightarrow1}^{T}\\left(p_{t}^{*}\\right)\\cdot\\left(x_{t}-p_{t}^{*}\\right)\\cdot1[p_{t}^{*}\\in\\mathcal{T}\\wedge t\\in I_{k}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where we define the process ", "page_idx": 33}, {"type": "equation", "text": "$$\nM_{T}^{k,f}:=\\sum_{t=1}^{T}y_{t}\\cdot f(p_{t}^{\\star})\\cdot(x_{t}-p_{t}^{\\star})\\cdot\\mathbb{1}\\left[p_{t}^{\\star}\\in\\mathbb{Z}\\wedge t\\in I_{k}\\right].\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "In the above, the third stp uses FactC.5, namely that $\\tau_{\\lceil\\log_{2}(T)\\rceil+2}=\\infty$ We can use Freedman's inequalt tobtainamaximalinequait foreachofthese $M_{T}^{k,f}$ proceses ", "page_idx": 33}, {"type": "text", "text": "Fact C.10. For every ${\\boldsymbol{y}}\\in\\left\\{0,1\\right\\}^{T}$ and $k\\geq2$ we can uniformly bound the process $M_{T}^{k,f}$ defined in (9) over a finite class $\\mathcal{G}$ of functions from $[0,1]\\;t o\\;[-1,1]\\;.$ ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbb{K}_{\\times\\sim\\mathcal{D}}\\left[\\operatorname*{max}_{f\\in\\mathcal{G}}M_{T}^{k,f}\\mid\\tau_{k-1}<\\infty\\right]\\le\\sqrt{2^{k-1}}\\big(2+2\\sqrt{\\log|\\mathcal{G}|}\\big)+2+2\\log|\\mathcal{G}|\\,.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Applying Fact C.10 to each of the martingales $M_{T}^{k,f}$ in (8) gives us ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}\\left[\\underset{f\\in\\mathcal{G}}{\\operatorname*{max}}\\sum_{t=\\tau_{1}+1}^{T}y_{t}\\cdot f(p_{t}^{\\star})\\cdot(x_{t}-p_{t}^{\\star})\\cdot\\mathbb{1}\\left[p_{t}^{\\star}\\in\\mathbb{Z}\\right]\\right]}\\\\ &{\\leq\\underset{k=2}{\\overset{\\lceil\\log_{2}(T)\\rceil+2}{\\sum}}\\operatorname*{Pr}\\left[\\tau_{k-1}<\\infty\\right](\\sqrt{2^{k-1}}(2+2\\sqrt{\\log|\\mathcal{G}|})+2+2\\log|\\mathcal{G}|).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "To upper bound the right-hand side above, we use Fact C.8 to bound ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\sum_{k=2}^{\\lceil\\log_{2}(T)\\rceil+2}\\operatorname*{Pr}\\left[\\tau_{k-1}<\\infty\\right]\\sqrt{2^{k-1}}\\leq\\mathbb{E}\\left[\\mathbb{I}\\left[\\operatorname{Var}_{T}(Z)\\geq1\\right]\\cdot\\sqrt{\\operatorname{Var}_{T}(Z)}\\right](2+2\\sqrt{2}),\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and use Fact C.7 to bound ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{k=2}^{\\lceil\\log_{2}(T)\\rceil+2}\\operatorname*{Pr}\\left[\\tau_{k-1}<\\infty\\right]\\leq\\sum_{k=2}^{\\lceil\\log_{2}(T)\\rceil+2}\\operatorname*{min}\\left\\{1,\\frac{\\mathbb{E}\\left[\\mathbb{1}\\left[\\mathrm{Var}_{T}(\\mathbb{Z})\\geq1\\right]\\cdot\\sqrt{\\mathrm{Var}_{T}(\\mathbb{Z})}\\right]}{2^{(k-2)/2}}\\right\\}}}\\\\ &{\\leq\\mathbb{E}\\left[\\mathbb{1}\\left[\\mathrm{Var}_{T}(\\mathbb{Z})\\geq1\\right]\\cdot\\sqrt{\\mathrm{Var}_{T}(\\mathbb{Z})}\\right](2+\\sqrt{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Plugging these into (10) gives ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\left[\\underset{f\\in\\mathcal{G}}{\\operatorname*{max}}\\left[M_{T}(f,y,\\mathbb{Z})-M_{\\tau_{1}}(f,y,\\mathbb{Z})\\right]\\right]}\\\\ &{\\leq\\mathbb{E}\\left[\\mathbb{1}\\left[\\mathrm{Var}_{T}(\\mathbb{Z})\\geq1\\right]\\cdot\\sqrt{\\mathrm{Var}_{T}(\\mathbb{Z})}\\right](2+2\\sqrt{2})(2+2\\sqrt{\\log|\\mathcal{G}|}+\\sqrt{2}+\\sqrt{2}\\log|\\mathcal{G}|)}\\\\ &{\\leq\\mathbb{E}\\left[\\mathbb{1}\\left[\\mathrm{Var}_{T}(\\mathbb{Z})\\geq1\\right]\\cdot\\sqrt{\\mathrm{Var}_{T}(\\mathbb{Z})}\\right]8\\big(5+\\log|\\mathcal{G}|\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Combine bounds. Combining (7) and (11) and recalling the definition of $\\gamma$ we recover our main claim ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~~\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}\\left[\\operatorname*{max}_{I}(f,y,\\mathcal{Z})\\right]}\\\\ &{\\le\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}\\left[\\operatorname*{max}_{I}\\boldsymbol{M}_{\\cap}(f,y,\\mathcal{Z})\\right]+\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}\\left[\\operatorname*{max}_{I}(f,y,\\mathcal{Z})-M_{\\tau_{1}}(f,y,\\mathcal{Z})\\right]}\\\\ &{\\le4\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}\\left[\\operatorname*{min}\\left\\lbrace1,\\mathrm{Var}_{T}(\\mathcal{Z})\\right\\rbrace\\right]+8\\big(5+\\log|\\mathcal{G}|\\big)\\cdot\\mathbb{E}\\left[\\mathbb{I}\\left[\\mathrm{Var}_{T}(\\mathcal{Z})\\ge1\\right]\\sqrt{\\mathrm{Var}_{T}(\\mathcal{Z})}\\right]}\\\\ &{\\le4\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}\\left[\\gamma(\\mathrm{Var}_{T}(\\mathcal{Z}))\\right]+8\\big(5+\\log|\\mathcal{G}|\\big)\\cdot\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}\\left[\\gamma(\\mathrm{Var}_{T}(\\mathcal{Z}))\\right]}\\\\ &{\\le8\\cdot(6+\\log|\\mathcal{G}|)\\cdot\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}\\left[\\gamma(\\mathrm{Var}_{T}(\\mathcal{Z}))\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "The second step above applies Inequalities (7) and (11). The third holds since $\\operatorname*{min}\\{1,x\\}\\leq\\gamma(x)$ and $\\mathbb{1}\\left[x\\geq1\\right]\\sqrt{x}\\leq\\gamma(x)$ hold for all $x\\geq0$ \u53e3 ", "page_idx": 34}, {"type": "text", "text": "Let us recall Freedman's inequality [Fre75] ", "page_idx": 34}, {"type": "text", "text": "Lemma C.11. Consider a martingale $M_{n}\\sim\\mathcal{D}$ with filtration $\\left(\\mathbb{F}_{t}\\right)$ where $|M_{t}-M_{t-1}|\\leq1$ for all $t\\in[n]$ . For all $x,y>0$ , we have the following high-probability bound on $M_{n}$ ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[\\exists n,M_{n}\\ge x~\\land~\\sum_{t=1}^{n}\\mathbb{E}\\left[\\left(M_{t}-M_{t-1}\\right)^{2}\\left|\\mathbb{F}_{t-1}\\right.\\right]\\le y\\right]\\le\\exp\\left(-\\frac{x^{2}}{2(x+y)}\\right).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We now prove Fact C.10. ", "page_idx": 34}, {"type": "text", "text": "Fact C.10. For every ${\\boldsymbol{y}}\\in\\left\\{0,1\\right\\}^{T}$ and $k\\geq2$ wecanuniformlybound thprocess $M_{T}^{k,f}$ defined in (9) over a finite class $\\mathcal{G}$ of functions from $[0,1]\\:t o\\left[-1,1\\right]b$ ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbb{K}_{\\times\\sim\\mathcal{D}}\\left[\\operatorname*{max}_{f\\in\\mathcal{G}}M_{T}^{k,f}\\mid\\tau_{k-1}<\\infty\\right]\\le\\sqrt{2^{k-1}}\\big(2+2\\sqrt{\\log|\\mathcal{G}|}\\big)+2+2\\log|\\mathcal{G}|\\,.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Proof. Fix any $f\\in\\mathcal G$ . For $t\\notin I_{k}$ , we have trivially that for any $x_{1:t-1}\\in\\left\\{0,1\\right\\}^{t-1}$ ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\underset{x^{\\prime}\\sim\\mathcal{D}}{\\mathbb{E}}\\left[y_{t}\\cdot f(p_{t}^{\\star})\\cdot(x_{t}^{\\prime}-p_{t}^{\\star})\\cdot\\mathbb{1}\\left[t\\in I_{k}\\wedge p_{t}^{\\star}\\in\\mathbb{Z}\\right]\\mid x_{1:t-1}^{\\prime}=x_{1:t-1}\\right]=0.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "For $t\\in I_{k}$ , since $\\mathbb{1}\\left[\\tau_{k-1}<\\infty\\right]$ and $p_{t}^{\\star}$ is measurable by $x_{1:t-1}$ , we again have that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\underset{x^{\\prime}\\sim\\mathcal{D}}{\\mathbb{E}}\\left[y_{t}\\cdot f(p_{t}^{\\star})\\cdot\\left(x_{t}-p_{t}^{\\star}\\right)\\cdot\\mathbb{1}\\left[t\\in I_{k}\\wedge p_{t}^{\\star}\\in\\mathbb{Z}\\right]\\mid x_{1:t-1}^{\\prime}=x_{1:t-1}\\right]}\\\\ &{=y_{t}\\cdot f(p_{t}^{\\star})\\cdot\\mathbb{1}\\left[t\\in I_{k}\\wedge p_{t}^{\\star}\\in\\mathbb{Z}\\right]\\cdot\\big(\\underset{x^{\\prime}\\sim\\mathcal{D}}{\\mathbb{E}}\\left[x_{t}^{\\prime}\\mid x_{1:t-1}^{\\prime}=x_{1:t-1}\\right]-p_{t}^{\\star}\\big)}\\\\ &{=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "This means that $M_{T}^{k,f}$ is a martingale even conditioned on the event that $\\tau_{k-1}<\\infty$ ", "page_idx": 34}, {"type": "text", "text": "Our constrution of epch $k$ i5d $M_{T}^{k,f}$ is deterministically upper bounded by $\\operatorname{Var}_{\\tau_{k}}(\\mathcal{Z})-\\operatorname{Var}_{\\tau_{k-1}}(\\mathcal{Z})\\leq2^{k-1}+1$ (Fact C.6). Thus, ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{2^{k-1}+1\\ge\\displaystyle\\sum_{t=1}^{T}p_{t}^{*}(1-p_{t}^{*})\\cdot1\\left[t\\in I_{k}\\wedge p_{t}^{*}\\in\\mathbb{Z}\\right]}&{}\\\\ &{\\qquad=\\displaystyle\\sum_{t=1}^{T}\\underline{{\\mathbb{E}}}_{\\otimes M_{\\theta}}[(x-p_{t}^{*})^{2}]\\cdot1\\left[t\\in I_{k}\\wedge p_{t}^{*}\\in\\mathbb{Z}\\right]}\\\\ &{\\qquad=\\displaystyle\\sum_{t=1}^{T}\\underline{{\\mathbb{E}}}_{\\tau}[(x_{t}^{\\prime}-p_{t}^{*})^{2}\\ |\\ x_{1:t-1}^{\\prime}=x_{1:t-1}]\\cdot1\\left[t\\in I_{k}\\wedge p_{t}^{*}\\in\\mathbb{Z}\\right]^{2}}\\\\ &{\\qquad=\\displaystyle\\sum_{t=1}^{T}\\underline{{\\mathbb{E}}}_{\\tau}^{\\prime}\\cdot f(p_{t}^{*})^{2}\\cdot\\underline{{\\mathbb{E}}}_{\\tau}\\mathbb{E}_{\\tau}\\left[(x_{t}^{\\prime}-p_{t}^{*})^{2}\\ |\\ x_{1:t-1}^{\\prime}=x_{1:t-1}\\right]\\cdot1\\left[t\\in I_{k}\\wedge p_{t}^{*}\\in\\mathbb{Z}\\right]^{2}}\\\\ &{\\qquad=\\displaystyle\\sum_{t=1}^{T}\\underline{{\\mathbb{E}}}_{\\tau}\\mathbb{E}_{\\tau}\\left[(M_{t}^{k,f}-M_{t-1}^{k,f})^{2}\\ |\\ x_{1:t-1}^{\\prime}=x_{1:t-1}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where the first equality uses the definition of a Bernoulli's variance; the second equality uses that, conditioned on $\\vec{\\mathrm{:}}_{t-1},x_{t}\\sim\\mathrm{Bernoulli}(p_{t}^{\\star})$ ; and the second inequality uses that $y_{t}^{2}\\leq\\dot{1}$ and $|f(x)|\\leq1$ for all $x\\in[0,1]$ ", "page_idx": 35}, {"type": "text", "text": "We can thus use Freedman's inequality to bound the deviation of each martingale $M_{T}^{k,f}$ .First, observe that the quadratic formula gives the inequality exp (- 2(+g)\uff09 if $x\\,\\geq\\,\\log(1/p)\\,+$ $\\sqrt{\\log^{2}(p)+2y\\log(1/p)}$ We can therefore invoke Lemma C.11 with $y=2^{k-1}+1$ and ", "page_idx": 35}, {"type": "equation", "text": "$$\nx=2\\log(1/p)+{\\sqrt{2y\\log(1/p)}}\\geq\\log(1/p)+{\\sqrt{\\log^{2}(p)+2y\\log(1/p)}}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "to show that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\geqslant\\operatorname*{Pr}\\left[M_{T}^{k,f}\\geq x\\;\\wedge\\;\\sum_{t=1}^{T}{_x}_{t}^{\\sharp}\\sim_{\\mathcal{D}_{t}}\\left[(M_{t}^{k,f}-M_{t-1}^{k,f})^{2}\\;|\\;x_{1:t-1}^{\\prime}=x_{1:t-1}\\right]\\leq2^{k-1}+1\\;|\\;{_{\\mathcal{T}_{k-1}}}<\\infty\\right].\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Applying (12), we can simplify this to ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p\\geq\\operatorname*{Pr}\\left[M_{T}^{k,f}\\geq\\sqrt{2(2^{k-1}+1)\\log(1/p)}+2\\log(1/p)\\mid\\tau_{k-1}<\\infty\\right]}\\\\ &{\\quad\\geq\\operatorname*{Pr}\\left[M_{T}^{k,f}\\geq\\sqrt{2^{k+1}\\log(1/p)}+2\\log(1/p)\\mid\\tau_{k-1}<\\infty\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "We can then take a union bound over $\\mathcal{G}$ for ", "page_idx": 35}, {"type": "equation", "text": "$$\np\\geq\\operatorname*{Pr}\\left[\\operatorname*{max}_{f\\in\\mathcal{G}}{M_{T}^{k,f}}\\geq\\sqrt{2^{k+1}\\log(|\\mathcal{G}|\\,/p)}+2\\log(|\\mathcal{G}|\\,/p)\\mid\\tau_{k-1}<\\infty\\right].\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Using the layer cake representation of expectation, we can convert this high-probability bound into the expectation bound through a change of variables ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}\\left[\\operatorname*{max}_{f\\in\\mathcal{G}}M_{T}^{k,f}\\mid\\tau_{k-1}<\\infty\\right]=\\int_{0}^{\\infty}\\operatorname*{Pr}\\left[\\operatorname*{max}_{f\\in\\mathcal{G}}M_{T}^{k,f}\\ge t\\mid\\tau_{k-1}<\\infty\\right]\\;\\mathrm{d}t}\\\\ {\\displaystyle=\\int_{0}^{1}\\sqrt{2^{k+1}\\log(|\\mathcal{G}|\\//p)}+2\\log(|\\mathcal{G}|\\//p)\\:\\mathrm{d}p}\\\\ {\\displaystyle=\\sqrt{2^{k+1}}(\\frac{|\\mathcal{G}|}{2}\\sqrt{\\pi}\\cdot\\mathrm{erfc}(\\sqrt{\\log|\\mathcal{G}|})+\\sqrt{\\log|\\mathcal{G}|})+2+2\\log|\\mathcal{G}|\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where the last equality follows by Fact C.12. When $|\\mathcal G|>1$ , we can compute the integral to be ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\underset{f\\in\\mathcal{G}}{\\operatorname*{max}}\\,M_{T}^{k,f}\\mid\\tau_{k-1}<\\infty\\right]\\leq\\sqrt{2^{k+1}}(\\frac{\\lvert\\mathcal{G}\\rvert}{2\\sqrt{\\log\\lvert\\mathcal{G}\\rvert}}\\exp(-\\log\\lvert\\mathcal{G}\\rvert)+\\sqrt{\\log\\lvert\\mathcal{G}\\rvert})+2+2\\log\\lvert\\mathcal{G}\\rvert}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\sqrt{2^{k-1}}(2+2\\sqrt{\\log\\lvert\\mathcal{G}\\rvert})+2+2\\log\\lvert\\mathcal{G}\\rvert\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where the frst equaltyuss h $\\begin{array}{r}{\\operatorname{erfc}(z)<\\frac{\\exp(-z^{2})}{z\\sqrt{\\pi}}}\\end{array}$ When $|\\mathcal{G}|=1$ we again have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{E}\\left[\\operatorname*{max}_{f\\in\\mathcal{G}}M_{T}^{k,f}\\mid\\tau_{k-1}<\\infty\\right]\\leq\\sqrt{\\pi}\\sqrt{2^{k-1}}+2}}\\\\ &{}&{\\leq\\sqrt{2^{k-1}}(2+2\\sqrt{\\log|\\mathcal{G}|})+2+2\\log|\\mathcal{G}|\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Fact C.12. For $k,n\\in\\mathbb{Z}_{+}$ , the following integral equality holds ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\int_{0}^{1}{\\sqrt{2^{k+1}\\log(n/p)}}+2\\log(n/p)\\;\\mathrm{d}p={\\sqrt{2^{k+1}}}{\\bigl(}{\\frac{n}{2}}{\\sqrt{\\pi}}\\cdot\\mathrm{erfc}({\\sqrt{\\log n}})+{\\sqrt{\\log n}}{\\bigr)}+2+2\\log n\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where erfc denotes the complementary error function. ", "page_idx": 35}, {"type": "text", "text": "Proof. Let us first separate the integral into two parts: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\int_{0}^{1}{\\sqrt{2^{k+1}\\log(n/p)}}+2\\log(n/p)\\;\\mathrm{d}p=\\int_{0}^{1}{\\sqrt{2^{k+1}\\log(n/p)}}\\;\\mathrm{d}p+\\int_{0}^{1}2\\log(n/p)\\;\\mathrm{d}p.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "We can bound the second integral easily. Since $\\log(n/p)=\\log n-\\log p.$ ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\int_{0}^{1}2\\log(n/p)~\\mathrm{d}p=\\int_{0}^{1}2(\\log n-\\log p)~\\mathrm{d}p}\\\\ {\\displaystyle\\qquad\\qquad\\qquad=2\\log n\\int_{0}^{1}\\mathrm{d}p-2\\int_{0}^{1}\\log p~\\mathrm{d}p}\\\\ {\\displaystyle\\qquad\\qquad\\qquad=2\\log n+2}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Now we consider the first integral. Let $u=\\log(n/p)$ . Then $p=n e^{-u}$ and $\\mathrm{d}p=-n e^{-{\\boldsymbol{u}}}\\mathrm{~c~}$ Hu. When $p=1$ $u=\\log n$ . When $p=0$ $u$ goes to $\\infty$ . Thus, the integral becomes: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\int_{0}^{1}\\sqrt{2^{k+1}\\log(n/p)}~\\mathrm{d}p=\\int_{\\infty}^{\\log n}\\sqrt{2^{k+1}u}\\cdot(-n e^{-u})~\\mathrm{d}u}}\\\\ &{}&{=n\\sqrt{2^{k+1}}\\int_{\\log n}^{\\infty}\\sqrt{u}\\,e^{-u}~\\mathrm{d}u.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "The integral involving the error function $\\operatorname{erfc}(x)$ can be recognized: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\int_{\\log n}^{\\infty}\\sqrt{u}\\,e^{-u}\\,\\operatorname{d}\\!u=\\displaystyle-\\sqrt{u}\\,e^{-u}\\Big|_{\\log n}^{\\infty}+\\displaystyle\\int_{\\log n}^{\\infty}\\frac{1}{2\\sqrt{u}}\\,e^{-u}\\,\\operatorname{d}\\!u}&{}\\\\ {\\displaystyle=\\operatorname*{lim}_{u\\to\\infty}\\Big(\\!-\\!\\sqrt{u}\\,e^{-u}\\Big)-\\Big(\\!-\\!\\sqrt{\\log n}\\,e^{-\\log n}\\Big)+\\displaystyle\\int_{\\log n}^{\\infty}\\frac{1}{2\\sqrt{u}}\\,e^{-u}\\,\\operatorname{d}\\!u}&{}\\\\ {\\displaystyle=\\sqrt{\\log n}\\,e^{-\\log n}+\\displaystyle\\int_{\\log n}^{\\infty}\\frac{1}{2\\sqrt{u}}\\,e^{-u}\\,\\operatorname{d}\\!u}&{}\\\\ {\\displaystyle=\\sqrt{\\log n}\\,e^{-\\log n}+\\displaystyle\\int_{\\sqrt{\\log n}}^{\\infty}e^{-t^{2}}\\,\\operatorname{d}\\!t}&{}\\\\ {\\displaystyle=\\frac{\\sqrt{\\log n}}{n}+\\frac{\\sqrt{\\pi}}{2}\\mathrm{erfc}(\\sqrt{\\log n}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Thus, the integral $\\textstyle\\int_{0}^{1}{\\sqrt{2^{k+1}\\log(n/p)}}\\ \\mathrm{d}p$ is given by ", "page_idx": 36}, {"type": "equation", "text": "$$\nn{\\sqrt{2^{k+1}}}\\left({\\frac{\\sqrt{\\pi}}{2}}\\mathrm{erfc}({\\sqrt{\\log n}})+{\\frac{\\sqrt{\\log n}}{n}}\\right)={\\sqrt{2^{k+1}}}\\left({\\frac{n{\\sqrt{\\pi}}}{2}}\\mathrm{erfc}({\\sqrt{\\log n}})+{\\sqrt{\\log n}}\\right).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Summing (13) and (14) gives the claim. ", "page_idx": 36}, {"type": "text", "text": "C.4 Proof of Lemma 5.2 ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Lemma 5.2. Given a function $f\\,:\\,[0,1]\\,\\rightarrow\\,[-1,1]$ and $y\\,\\in\\,\\{0,1\\}^{T}$ ,consider the martingale $\\begin{array}{r}{M_{t}(f,y):=\\sum_{s=1}^{t}y_{s}\\cdot f(p_{s}^{\\star})\\cdot(x_{s}-p_{s}^{\\star})}\\end{array}$ where $x\\sim\\mathcal{D}$ Then, for any fnite fomily $\\mathcal{G}$ of functions from $[0,1]\\;t o\\;[-1,1]$ and any ${\\boldsymbol{y}}\\in\\{0,1\\}^{T}$ , we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathbb{K}_{\\times\\sim\\mathcal{D}}\\left[\\operatorname*{max}_{f\\in\\mathcal{G}}M_{T}(f,y)\\right]\\le O\\left(\\log\\vert\\mathcal{G}\\vert\\cdot\\log T+\\sqrt{\\log\\vert\\mathcal{G}\\vert}\\cdot\\operatorname*{\\mathbb{E}}_{x\\sim\\mathcal{D}}\\left[\\sqrt{\\mathrm{Var}_{T}}\\right]\\right).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Proof. Let us decompose the martingale $M_{T}(f,y)$ into epochs of doubling realized variance with respect to $\\mathcal{T}=[0,1]$ as per Definition C.4. Using $\\tau$ as defined in (5), we will write $I_{k}:=[\\tau_{k-1}+$ $1,\\operatorname*{min}\\left\\{T,\\tau_{k}\\right\\}]$ to denote the time steps composing epoch $k$ and write $K:=\\operatorname*{max}\\left\\{k\\mid\\tau_{k}<\\infty\\right\\}$ to denote the number of completed epochs. ", "page_idx": 36}, {"type": "text", "text": "Applying a triangle inequality and the law of total expectation gives ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}\\left[\\operatorname*{max}\\sum_{i=1}^{r}y_{i}\\cdot f(p_{t}^{*})\\cdot(x_{t}-p_{t}^{*})\\right]}\\\\ &{=\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}\\left[\\operatorname*{max}\\sum_{i=0}^{K+1}{y}_{i}\\cdot f(p_{t}^{*})\\cdot(x_{t}-p_{t}^{*})\\right]}\\\\ &{\\leq\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}\\left[\\frac{\\displaystyle{\\operatorname*{max}\\sum_{i=1}^{K+1}\\operatorname*{max}}}{\\displaystyle{\\operatorname*{max}\\sum_{i=0}^{K+1}\\operatorname*{max}}}\\sum_{j\\in\\mathcal{E}_{i}}f(p_{t}^{*})\\cdot(x_{t}-p_{t}^{*})\\right]}\\\\ &{=\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}\\left[\\log_{(1/4)}+2\\right.}\\\\ &{=\\left.\\underset{k=1}{\\overset{\\lceil\\log_{(2)}(1/4)}{\\sum}}\\frac{\\operatorname*{max}}{\\displaystyle{\\operatorname*{max}}}\\sum_{\\ell\\in\\mathcal{E}_{i}}f_{\\ell}(p_{t}^{*})\\cdot(x_{t}-p_{t}^{*})\\cdot1\\left[\\ell\\in I_{k}\\right]\\right]}\\\\ &{=\\underset{k=1}{\\overset{\\lceil\\log_{(2)}(1/4)}{\\sum}}2^{K}[\\tau_{k-1}<\\infty]\\cdot\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}\\left[\\operatorname*{max}\\sum_{j\\in\\mathcal{E}}M_{j}^{k,f}\\mid\\tau_{k-1}<\\infty\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Where we define the proces $\\begin{array}{r}{M_{T}^{k,f}:=\\sum_{t=1}^{T}y_{t}\\cdot f(p_{t}^{\\star})\\cdot(x_{t}-p_{t}^{\\star})\\cdot\\mathbb{1}\\left[t\\in I_{k}\\right]}\\end{array}$ I the above, the second equality sesFact C5 namly that $\\tau_{\\lceil\\log_{2}(T)\\rceil+2}=\\infty$ . Applying Fact C.10 to each of the martingales $M_{T}^{k,f}$ in (15) gives us ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}\\left[\\underset{f\\in\\mathcal{G}}{\\operatorname*{max}}\\sum_{t=1}^{T}y_{t}\\cdot f(p_{t}^{\\star})\\cdot(x_{t}-p_{t}^{\\star})\\right]}\\\\ &{\\leq\\underset{k=1}{\\overset{\\lceil\\log_{2}(T)\\rceil+2}{\\sum}}\\mathtt{P r}\\left[\\tau_{k-1}<\\infty\\right]\\cdot\\left[\\sqrt{2^{k-1}}(2+2\\sqrt{\\log|\\mathcal{G}|})+2+2\\log|\\mathcal{G}|\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "We can upper bound some of the summands in the right-hand side by using Fact C.8 to bound ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\sum_{k=2}^{\\lceil\\log_{2}(T)\\rceil+2}\\operatorname*{Pr}\\left[\\tau_{k-1}<\\infty\\right]\\sqrt{2^{k-1}}\\leq\\mathbb{E}\\left[\\sqrt{\\mathrm{Var}}_{T}\\right](2+2\\sqrt{2}).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "This gives that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\operatorname*{lim}_{x\\sim\\mathcal{D}}\\left[\\operatorname*{max}_{f\\in\\mathcal{G}}\\sum_{t=1}^{T}y_{t}\\cdot f(p_{t}^{\\star})\\cdot(x_{t}-p_{t}^{\\star})\\right]}}\\\\ &{}&{\\leq(2+2\\log|\\mathcal{G}|)(\\lceil\\log_{2}(T)\\rceil+2)+\\mathbb{E}\\left[\\sqrt{\\mathrm{Var}}_{T}\\right](2+2\\sqrt{2})(2+2\\sqrt{\\log|\\mathcal{G}|}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "D  Supplemental Materials for Section 6 ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Notation.  For all stochastic processes $\\left(X_{t}\\right)$ we use $X_{t_{1}:t_{2}}\\,=\\,X_{\\operatorname*{min}\\{t_{2},T\\}}\\,-\\,X_{t_{1}}$ to denote the increment within the time interval $(t_{1},t_{2}]$ (with $X_{0}=0$ by default). ", "page_idx": 37}, {"type": "text", "text": "D.1Proof of the Weaker Lower Bound ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "We restate and prove Lemmas 6.2 and 6.3. ", "page_idx": 37}, {"type": "text", "text": "Lemma 6.2. For any $x\\in\\{0,1\\}^{T}$ and $p\\in[0,1]^{T}$ wehave ${\\sf S S C E}(x,p)\\geq{\\Omega}\\left(\\sqrt{N_{T}}\\right)$ ", "page_idx": 37}, {"type": "text", "text": "Proof. Recall that SSCE is defined using smCE, which is in turn a supremum over the family $\\mathcal{F}$ of Lipschitz functions. Since both $f\\equiv1$ and $f\\equiv-1$ are included in $\\mathcal{F}$ ,for any realized sequences $x$ and $p$ , we can lower bound SSCE $(x,p)$ as follows: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathsf{S S C E}(x,p)\\geq\\underset{y\\sim\\mathsf{U n i f}(\\{0,1\\}^{T})}{\\mathbb{E}}\\left[\\left|\\sum_{t=1}^{T}y_{t}\\cdot(x_{t}-p_{t})\\right|\\right]=\\underset{y}{\\mathbb{E}}\\left[\\left|\\sum_{t=1}^{T}z_{t}+\\mu\\right|\\right],\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where we have defined $z_{t}:=(y_{t}-0.5)(x_{t}-p_{t})$ to be zero-mean independent random variables, and $\\begin{array}{r}{\\mu:=\\sum_{t=1}^{T}0.5(x_{t}-p_{t})}\\end{array}$ . Now we partition $[T]$ into $T_{1}$ and $T_{2}$ , where $T_{1}$ includes the al time steps such that $\\left|x_{t}-p_{t}\\right|\\geq\\frac{1}{2}$ : and $T_{2}=T\\setminus T_{1}$ contains the remaining time steps. From the definition of $N_{T}$ , it immediately follows that $\\begin{array}{r}{N_{T}=|T_{1}|}\\end{array}$ . Letting $\\textstyle Z_{1}:={\\bar{\\sum}}_{t\\in T_{1}}\\,z_{t}$ and $\\textstyle Z_{2}:=\\sum_{t\\in T_{2}}z_{t}$ , it remains to lower bound E $\\left[|Z_{1}+Z_{2}+\\mu|\\right]$ by $\\Omega(\\sqrt{N_{T}})$ ", "page_idx": 38}, {"type": "text", "text": "We will first prove that E $[|Z_{1}|]\\ge C\\sqrt{N_{T}}$ for a universal constant $C>0$ . From the Berry-Esseen theorem (e.g. from [She10]), the CDF of $Z_{1}$ can be approximated by the CDF of the standard normal distribution as follows: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\forall x\\in\\mathbb{R},\\;\\left|\\operatorname*{Pr}\\left[Z_{1}\\leq x\\cdot\\sigma_{0}\\right]-\\Phi(x)\\right|\\leq C_{0}\\cdot\\sigma_{0}^{-1}\\cdot\\rho_{0},\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where $\\Phi(x)$ is the standard Gaussian CDF, $C_{0}$ is a universal constant no larger than 0.56, and ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sigma_{0}=\\sqrt{\\displaystyle\\sum_{t\\in T_{1}}\\mathbb{E}\\left[z_{t}^{2}\\right]}=\\sqrt{\\frac{1}{4}\\sum_{t\\in T_{1}}(x_{t}-p_{t})^{2}}\\geq\\frac{1}{4}\\sqrt{N_{T}};}\\\\ &{\\rho_{0}=\\displaystyle\\operatorname*{max}_{t\\in T_{1}}\\frac{\\mathbb{E}\\left[|z_{t}|^{3}\\right]}{\\mathbb{E}\\left[|z_{t}|^{2}\\right]}=\\displaystyle\\operatorname*{max}_{t\\in T_{1}}\\frac{|x_{t}-p_{t}|^{3}/8}{|x_{t}-p_{t}|^{2}/4}\\leq\\frac{1}{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "As a result, we can lower bound the probability of $|Z_{1}|\\ge0.05\\sqrt{N_{T}}$ as follows: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\operatorname*{Pr}\\left[|Z_{1}|\\geq0.05\\sqrt{N_{T}}\\right]\\geq\\operatorname*{Pr}\\left[|Z_{1}|>0.2\\cdot\\sigma_{0}\\right]}&{}&{(\\sigma_{0}\\geq\\frac{1}{4}\\sqrt{N_{T}})}\\\\ {=2\\left(1-\\operatorname*{Pr}\\left[Z_{1}\\leq0.2\\cdot\\sigma_{0}\\right]\\right)}&{}&{(Z_{1}\\mathrm{~is~symmetric})}\\\\ {\\geq2\\left(1-\\Phi(0.2)-2C_{0}/\\sqrt{N_{T}}\\right).}&{}&{\\mathrm{(Berry-Esseen~theorem)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Since $C_{0}~\\leq~0.56$ and $\\Phi(0.2)\\,\\leq\\,0.58$ , we can guarantee $\\operatorname*{Pr}\\left[|Z_{1}|\\geq0.05\\sqrt{N_{T}}\\right]\\,\\geq\\,\\Omega(1)$ for all $N_{T}\\geq8$ When $N_{T}\\leq7$ , we have $|Z_{1}|=N_{T}/2\\ge0.05\\sqrt{N_{T}}$ when all $\\{z_{t}\\mid t\\in T_{1}\\}$ are positive, which happens with probability $2^{-N_{T}}\\stackrel{.}{\\geq}2^{-7}=\\Omega(1)$ . Therefore, we can always conclude that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[|Z_{1}|\\right]\\geq0.05\\sqrt{N_{T}}\\cdot\\mathrm{Pr}\\left[|Z_{1}|\\geq0.05\\sqrt{N_{T}}\\right]\\geq C\\sqrt{N_{T}}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "for some universal constant $C>0$ ", "page_idx": 38}, {"type": "text", "text": "Finally, we consider the randomness of $Z_{2}$ and show that $\\begin{array}{r}{\\mathbb{E}\\left[|Z_{1}+Z_{2}+\\mu|\\right]\\geq\\frac{C}{2}\\sqrt{N_{T}}}\\end{array}$ .Applying the tower property of expectations, we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[|Z_{1}+Z_{2}+\\mu|\\right]=\\mathbb{E}\\left[\\mathbb{E}\\left[|Z_{1}+Z_{2}+\\mu|\\ |\\ Z_{2}\\right]\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Consider the following two cases for the conditional expectation inside: ", "page_idx": 38}, {"type": "text", "text": "\u00b7 When $\\begin{array}{r l r}{|Z_{2}\\,+\\,\\mu|}&{{}\\!\\!\\ge}&{\\frac{C}{2}\\sqrt{N_{T}}}\\end{array}$ , we use Jensen's inequality and $\\mathbb{E}\\left[Z_{1}\\right]\\mathrm{~=~}\\mathrm{~0~}$ to obtain $\\begin{array}{r}{\\mathfrak{L}\\left[\\left|Z_{1}+Z_{2}+\\mu\\right|\\;\\middle|\\;Z_{2}\\right]\\geq\\left|\\;\\mathbb{E}\\left[Z_{1}+Z_{2}+\\mu\\;\\middle|\\;Z_{2}\\right]\\right|=\\left|Z_{2}+\\mu\\right|\\geq\\frac{C}{2}\\sqrt{N_{T}}.}\\end{array}$ \u00b7When $\\begin{array}{r}{|Z_{2}+\\mu|<\\frac{C}{2}\\sqrt{N_{T}}}\\end{array}$ , we apply the triangle inequality and have $\\begin{array}{r}{\\mathbb{E}\\left[\\left|Z_{1}+Z_{2}+\\mu\\right|\\;|\\;Z_{2}\\right]\\ge\\mathbb{E}\\left[\\left|Z_{1}\\right|\\right]-\\left|Z_{2}+\\mu\\right|>C\\sqrt{N_{T}}-\\frac{C}{2}\\sqrt{N_{T}}=\\frac{C}{2}\\sqrt{N_{T}}.}\\end{array}$ ", "page_idx": 38}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Therefore, regardless of the realization of $Z_{2}$ , we always have $\\begin{array}{r}{\\mathbb{E}\\left[|Z_{1}+Z_{2}+\\mu_{0}|\\mid Z_{2}\\right]\\geq\\frac{C}{2}\\sqrt{N_{T}}}\\end{array}$ Taking an expectation over the randomness of $Z_{2}$ gives the desired bound $\\begin{array}{r}{\\mathsf{S S C E}(x,p)\\geq\\frac{C}{2}\\sqrt{N_{T}}}\\end{array}$ \uff1a\u53e3 ", "page_idx": 38}, {"type": "text", "text": "Lemma 6.3. The stochastic process $(N_{t})_{t\\in[T]}\\;s a t i s f i e s\\;\\mathbb{E}\\left[\\sqrt{N_{T}}\\right]\\geq\\Omega(\\mathbb{E}\\left[\\sqrt{\\mathrm{Var}_{T}}\\right])-O(1).$ ", "page_idx": 38}, {"type": "text", "text": "Proof. Since $N_{T}\\ge\\mathrm{Var}_{T}/16$ implies $\\sqrt{N_{T}}\\ge\\sqrt{\\mathrm{Var}_{T}}/4$ , we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\sqrt{N_{T}}\\ge\\frac{\\sqrt{\\mathrm{Var}_{T}}}{4}\\cdot\\mathbb{1}\\left[N_{T}\\ge\\frac{\\mathrm{Var}_{T}}{16}\\right]=\\frac{\\sqrt{\\mathrm{Var}_{T}}}{4}-\\frac{\\sqrt{\\mathrm{Var}_{T}}}{4}\\cdot\\mathbb{1}\\left[N_{T}<\\frac{\\mathrm{Var}_{T}}{16}\\right].\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Therefore, to establish the inequality $\\mathbb{E}\\left[{\\sqrt{N_{T}}}\\right]\\geq\\Omega(\\mathbb{E}\\left[{\\sqrt{\\operatorname{Var}_{T}}}\\right])-O(1)$ , it sufices to prove that the expectation of the second term\u2014which we denote with $M$ -is upper bounded by $O(1)$ , i.e., ", "page_idx": 39}, {"type": "equation", "text": "$$\nM:=\\mathbb{E}\\left[\\sqrt{\\operatorname{Var}_{T}}\\cdot\\mathbb{1}\\left[N_{T}<\\operatorname{Var}_{T}/16\\right]\\right]\\leq O(1).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "We proceed by partitioning the range of $\\mathrm{Var}_{T}$ into subintervals of geometrically increasing length and enumerating all possibilities for which subinterval $\\mathrm{Var}_{T}$ falls into. If $\\mathrm{Var}_{T}\\leq1$ , its contribution to $M$ is clearly $O(1)$ . Otherwise, we must have $\\mathrm{Var}_{T}\\in[2^{l},2^{l+1})$ for some $l\\in\\mathbb N$ , which implies that $N_{T}<\\mathrm{Var}_{T}/16<2^{l-3}$ . Therefore, we bound $M$ by taking a union bound over all such $l`_{\\mathbf{S}}$ ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{M\\leq O(1)+\\displaystyle\\sum_{l\\in\\mathbb{N}}\\mathbb{E}\\left[\\sqrt{\\mathrm{Var}_{T}}\\cdot\\mathbb{1}\\left[N_{T}<2^{l-3}\\,\\wedge\\,\\mathrm{Var}_{T}\\in[2^{l},2^{l+1})\\right]\\right]}\\\\ &{\\quad\\leq O(1)+\\displaystyle\\sum_{l\\in\\mathbb{N}}\\sqrt{2^{l+1}}\\cdot\\mathrm{Pr}\\left[N_{T}<2^{l-3}\\,\\wedge\\,\\mathrm{Var}_{T}\\geq2^{l}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Now we bound $\\operatorname*{Pr}\\left[N_{T}<k/8\\,\\land\\,\\operatorname{Var}_{T}\\geq k\\right]$ for any fixed value of $k$ (that plays the role of $2^{l}$ by constructing a sub-martingale. We start by partitioning the time horizon $[T]$ intoblocksbasedonthe realized variance $\\mathrm{Var}_{t}$ , such that each block $B_{j}:=(\\bar{b_{j-1}},\\;b_{j}]$ terminates upon the realized variance $\\mathrm{Var}_{B_{j}}$ first exceeds 1. Formally, using notation $X_{t_{1}:t_{2}}:=X_{\\operatorname*{min}\\{t_{2},T\\}}-X_{t_{1}}$ to denote the increment of any process $\\left(X_{t}\\right)$ in $(t_{1},t_{2}]$ (with $X_{0}=0$ by default), the endpoints $b_{j}$ are defined recursively as: ", "page_idx": 39}, {"type": "equation", "text": "$$\nb_{0}:=0,\\;b_{j}:=\\operatorname*{min}\\left\\{\\infty\\right\\}\\cup\\left\\{t\\in\\left[b_{j-1}+1,T\\right]|\\;\\mathrm{Var}_{b_{j-1}:t}\\geq1\\right\\},\\;\\forall j\\geq1.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "We show in the following lemma that for each block $B_{j}$ , the expected increment $N_{B_{j}}$ within $B_{j}$ is lower bounded by a constant as long as $B_{j}$ terminates before $T$ ", "page_idx": 39}, {"type": "text", "text": "Lemma D.1. For the constant $c=1-1/e,\\,\\mathbb{E}\\left[\\mathbb{1}\\left[N_{B_{j}}\\geq1\\right]-c\\cdot\\mathbb{1}\\left[b_{j}<\\infty\\right]\\,\\Big|\\,\\mathbb{F}_{b_{j-1}}\\right]\\geq0.$ ", "page_idx": 39}, {"type": "text", "text": "We prove Lemma D.1 in Appendix D.2. This lemma justifies that if we define $A_{j}$ as ", "page_idx": 39}, {"type": "equation", "text": "$$\nA_{0}:=0,\\quad A_{j}-A_{j-1}:=\\mathbb{1}\\left[N_{B_{j}}\\geq1\\right]-c\\cdot\\mathbb{1}\\left[b_{j}<\\infty\\right](j\\geq1),\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "then $(A_{j})_{j\\geq0}$ forms a sub-martingale of bounded increment $|A_{j}-A_{j-1}|\\leq1$ , making it unlikely for any $A_{j}$ to deviate significantly below 0. However, if $N_{T}<\\bar{k}/8$ and $\\operatorname{Var}_{T}\\geq k$ , then $A_{k/2}$ must witness a large deviation: on the one hand, block $B_{k/2}$ should terminate properly because the variance in each block cannot exceed 2; on the other hand, $N_{T}<k/8$ implies that at most $k/8$ of these blocks can have a nonzero increment $N_{B_{j}}$ . As a result, ", "page_idx": 39}, {"type": "equation", "text": "$$\nA_{k/2}=\\sum_{j=1}^{k}\\mathbb{1}\\left[N_{B_{j}}\\geq1\\right]-c\\cdot\\sum_{j=1}^{k}\\mathbb{1}\\left[b_{j}<\\infty\\right]\\leq N_{T}-c\\cdot(k/2)<-k/8.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "By applying the Azuma-Hoeffding inequality for submartingales, we can quantitatively bound the probability of such a large deviation by ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[N_{T}<k/8\\,\\wedge\\,\\operatorname{Var}_{T}\\geq k\\right]\\leq\\operatorname*{Pr}\\left[A_{k/2}\\leq-k/8\\right]\\leq e^{-k/64}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Finally, plugging the above bound back into equation (17) gives us ", "page_idx": 39}, {"type": "equation", "text": "$$\nM\\leq O(1)+\\sum_{l\\in\\mathbb{N}}\\sqrt{2^{l+1}}\\cdot e^{-2^{l-6}}\\leq O(1).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "We have thus established the inequality (16), which in turn proves the lemma. ", "page_idx": 39}, {"type": "text", "text": "D.2  Proof of Lemma D.1 ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Now we prove Lemma D.1, which we restate below. ", "page_idx": 39}, {"type": "text", "text": "Lemma D.1. For the constant $c=1-1/e,\\,\\mathbb{E}\\left[\\mathbb{1}\\left[N_{B_{j}}\\geq1\\right]-c\\cdot\\mathbb{1}\\left[b_{j}<\\infty\\right]\\,\\Big|\\,\\mathbb{F}_{b_{j-1}}\\right]\\geq0.$ ", "page_idx": 39}, {"type": "text", "text": "Proof. We first show that for all $t\\,\\in\\,[T]$ , we have $\\operatorname*{Pr}\\left[n_{t}=1\\mid\\mathbb{F}_{t-1}\\right]\\ge p_{t}^{\\star}(1-p_{t}^{\\star})$ , where $\\mathbb{F}_{t-1}$ denotes the filtration generated by all the randomness up to time $t-1$ . Note that conditioning on $\\mathbb{F}_{t-1}$ \uff0c $x_{t}$ is distributed according to Bernoulli $\\left(p_{t}^{\\star}\\right)$ . If the forecaster chooses $p_{t}\\,\\geq\\,{\\frac{1}{2}}$ , the condition ", "page_idx": 39}, {"type": "text", "text": "$\\left|x_{t}-p_{t}\\right|\\geq{\\frac{1}{2}}$ holds when $x_{t}=0$ which happens with probability $1-p_{t}^{\\star}$ ; otherwise it holds when $x_{t}=1$ , which happens with probability $p_{t}^{\\star}$ . Therefore, regardless of the choice of $p_{t}$ , we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[n_{t}=1\\mid\\mathbb{F}_{t-1}\\right]=\\operatorname*{Pr}_{x_{t}\\sim\\mathsf{B e r n o u l i t}(p_{t}^{\\star})}\\left[\\left|x_{t}-p_{t}\\right|\\geq1/2\\right]\\geq\\operatorname*{min}\\{p_{t}^{\\star},1-p_{t}^{\\star}\\}\\geq p_{t}^{\\star}(1-p_{t}^{\\star}).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "This allows us to invoke Lemma D.5 with $q_{t}:=n_{t}$ $r_{t}:=p_{t}^{\\star}(1-p_{t}^{\\star})$ , and $\\theta=1$ , where we only consider the random process inside block $B_{j}$ . In this context, the stopping time $\\tau_{1}$ corresponds to the end of the block, i.e., $b_{j}$ . Therefore, by applying Lemma D.5 at time step $b_{j-1}$ , we obtain ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle A_{b_{j-1}}=\\operatorname*{Pr}\\left[N_{B_{j}}\\geq1\\;\\middle|\\;\\mathbb{F}_{b_{j-1}}\\right]-\\left(1-e^{-1}\\right)\\cdot\\operatorname*{Pr}\\left[b_{j}<\\infty\\;\\middle|\\;\\mathbb{F}_{b_{j-1}}\\right]\\geq0}\\\\ {\\iff\\mathbb{E}\\left[\\mathbb{I}\\left[N_{B_{j}}\\geq1\\right]-c\\cdot\\mathbb{I}\\left[b_{j}<\\infty\\right]\\;\\middle|\\;\\mathbb{F}_{b_{j-1}}\\right]\\geq0,\\;\\;\\mathrm{where}\\;c=1-\\displaystyle\\frac{1}{e}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "D.3 A Stronger Lower Bound ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "In this section, we state and prove the stronger SSCE lower bound for all forecasters. ", "page_idx": 40}, {"type": "text", "text": "Theorem D.2. For any $\\mathcal{D}\\in\\Delta(\\{0,1\\}^{T})$ \uff0c $\\mathsf{O P T}_{\\mathsf{S S C E}}(\\mathcal D)=\\Omega(\\mathbb E\\left[\\gamma(\\mathrm{Var}_{T})\\right])$ , where the function $\\gamma$ .s defined as $\\gamma(x):=x\\cdot\\mathbb{1}\\left[0\\leq x<1\\right]+\\sqrt{x}\\cdot\\mathbb{1}\\left[x\\geq1\\right]$ ", "page_idx": 40}, {"type": "text", "text": "ProofofTheorem $D.2$ . The theorem holds by combining Lemma 6.2, which lower bounds the SSCE by $\\Omega(\\bar{\\sqrt{N_{T}}})$ , and the stronger lower bound on E $[\\sqrt{N_{T}}]$ shown in Lemma D.3. \u53e3 ", "page_idx": 40}, {"type": "text", "text": "Lemma D.3. There exists a universal constant $C>0$ such that E $\\left[{\\sqrt{N_{T}}}\\right]\\geq C\\cdot\\mathbb{E}\\left[\\gamma({\\mathrm{Var}}_{T})\\right]$ where the function $\\gamma$ is defined as $\\gamma(x):=x\\cdot\\mathbb{1}\\left[0\\leq x<1\\right]+\\sqrt{x}\\cdot\\mathbb{1}\\left[\\bar{x^{'}}\\geq1\\right]$ ", "page_idx": 40}, {"type": "text", "text": "Proof of Lemma D.3. The proof is also based on partitioning the time horizon into blocks $B_{j}\\,=$ $(b_{j-1},b_{j}]$ each with approximately unit variance\u2014similar to the approach used in proving Lemma 6.3. However, this proof involves a more careful analysis of the growth of $\\sqrt{N_{t}}$ byfurther grouping blocks into \u201cepochs\"\u201d and giving special treatment to the first epoch, where the cumulative variance is very small. ", "page_idx": 40}, {"type": "text", "text": "Specifically, consider the blocks $B_{j}=(b_{j-1},b_{j}]$ defined by ", "page_idx": 40}, {"type": "equation", "text": "$$\nb_{0}:=0,\\;b_{j}:=\\operatorname*{min}\\left\\{\\infty\\right\\}\\cup\\left\\{t\\in\\left[b_{j-1}+1,T\\right]|\\;\\mathrm{Var}_{b_{j-1}:t}\\geq1\\right\\},\\;\\forall j\\geq1.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Recall that the increment of $\\mathrm{Var}_{t}$ satisfies $\\mathrm{Var}_{t}-\\mathrm{Var}_{t-1}=p_{t}^{\\star}(1-p_{t}^{\\star})\\leq1/4$ Thus, every block $j$ satisfies $\\operatorname{Var}_{B_{j}}=\\operatorname{Var}_{b_{j}}-\\operatorname{Var}_{b_{j-1}}=(\\operatorname{Var}_{b_{j}-1}-\\operatorname{Var}_{b_{j-1}})+(\\operatorname{Var}_{b_{j}}-\\operatorname{Var}_{b_{j}-1})\\leq1+1/4=5/4.$ We further group blocks into epochs such that the $k$ -th epoch $\\mathcal{T}_{k}:=(\\tau_{k-1},\\tau_{k}]$ contains $\\approx2^{k}$ blocks: ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\mathcal{T}_{0}:=B_{1},\\quad\\mathcal{T}_{k}:=\\bigcup_{j\\in(2^{k-1},2^{k}]}B_{j},\\;\\forall k\\geq1\\quad(\\mathrm{or}\\;\\mathrm{equivalently},\\,\\tau_{k}:=b_{2^{k}}).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "In addition, we define ${\\widetilde{N}}_{t}$ as the sum of $n_{s}$ capped by 1 in each block: ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\widetilde{N}_{t}:=\\sum_{j:b_{j}\\leq t}\\operatorname*{min}\\{N_{B_{j}},1\\}=\\sum_{j:b_{j}\\leq t}\\mathbb{1}\\left[N_{B_{j}}\\geq1\\right].\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Clearly, for all the realized sequences we have $N_{T}\\geq\\widetilde{N}_{T}$ and $\\widetilde{N}_{\\tau_{k}}\\le2^{k}$ , where the latter is because each block contributes at most 1 to ${\\widetilde{N}}_{t}$ . In the following, we will first analyze the growth of $\\sqrt{\\widetilde{N}_{t}}$ in epochs $k\\geq1$ , then provide a different analysis for the zeroth epoch. ", "page_idx": 40}, {"type": "text", "text": "In each epoch $\\mathcal{T}_{k}$ With $k\\geq1$ : We start by establishing the following lemma, which extends the characterization of Lemma D.1 into epochs. ", "page_idx": 40}, {"type": "text", "text": "Lemma D.4 (Lower bound on $\\widetilde{N}_{\\mathcal{T}_{k}}$ ). For any $k\\geq1$ we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\widetilde{N}_{\\mathcal{T}_{k}}\\right]\\geq2^{k-2}\\cdot\\operatorname*{Pr}\\left[\\tau_{k}<\\infty\\right].\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Proof of Lemma D.4. According to Lemma D.1, we have that in each block $B_{j}=(b_{j-1},b_{j}]$ ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\mathbb{1}\\left[N_{B_{j}}\\geq1\\right]-c\\cdot\\mathbb{1}\\left[b_{j}<\\infty\\right]\\right]=\\mathbb{E}\\left[\\mathbb{E}\\left[\\mathbb{1}\\left[N_{B_{j}}\\geq1\\right]-c\\cdot\\mathbb{1}\\left[b_{j}<\\infty\\right]\\;\\Big|\\,\\mathbb{F}_{b_{j-1}}\\right]\\right]\\geq0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where the first step uses the tower property of expectations and $\\begin{array}{r}{c=1-\\frac{1}{e}\\ge\\frac{1}{2}}\\end{array}$ Summing over all the blocks in epoch $\\mathcal{T}_{k}$ , we obtain ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\widetilde{N}_{\\widetilde{T}_{k}}\\right]=\\underset{j=2^{k-1}+1}{\\overset{2^{k}}{\\sum}}\\mathbb{E}\\left[\\widetilde{N}_{B_{j}}\\right]=\\underset{j=2^{k-1}+1}{\\overset{2^{k}}{\\sum}}\\mathbb{E}\\left[\\mathbb{I}\\left[N_{B_{j}}\\geq1\\right]\\right]}\\\\ &{\\qquad\\qquad\\geq c\\cdot\\mathbb{E}\\left[\\underset{j=2^{k-1}+1}{\\overset{2^{k}}{\\sum}}1\\left[b_{j}<\\infty\\right]\\right]}\\\\ &{\\qquad\\qquad\\geq c\\cdot\\mathbb{E}\\left[\\underset{j=2^{k-1}+1}{\\overset{2^{k}}{\\sum}}1\\left[\\tau_{k}<\\infty\\right]\\right]}\\\\ &{\\qquad\\qquad\\geq2^{k-2}\\cdot\\operatorname*{Pr}\\left[\\tau_{k}<\\infty\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "equation", "text": "$$\n(c\\geq1/2)\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "We have thus established Lemma D.4. ", "page_idx": 41}, {"type": "text", "text": "With Lemma D4, we obtain alowerbound bylinearizing the incremet of $\\sqrt{\\widetilde{N}_{t}}$ in each block. ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\left[\\sqrt{\\widetilde{N}_{\\tau_{k}}}-\\sqrt{\\widetilde{N}_{\\tau_{k-1}}}\\right]}\\\\ &{\\geq\\mathbb{E}\\left[\\frac{1}{2}\\left(\\widetilde{N}_{\\tau_{k}}\\right)^{-\\frac{1}{2}}\\cdot\\left(\\widetilde{N}_{\\tau_{k}}-\\widetilde{N}_{\\tau_{k-1}}\\right)\\right]}\\\\ &{\\geq2^{-\\frac{k}{2}-1}\\cdot\\mathbb{E}\\left[\\widetilde{N}_{\\tau_{k}}-\\widetilde{N}_{\\tau_{k-1}}\\right]=2^{-\\frac{k}{2}-1}\\cdot\\mathbb{E}\\left[\\widetilde{N}_{T_{k}}\\right]}\\\\ &{\\geq2^{\\frac{k}{2}-3}\\cdot\\operatorname*{Pr}\\left[\\tau_{k}<\\infty\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "equation", "text": "$$\n(\\widetilde{N}_{\\tau_{k}}\\leq2^{k})\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "The first step above can be alternatively justified by $\\begin{array}{r}{\\sqrt{a}-\\sqrt{b}=\\frac{a-b}{\\sqrt{a}+\\sqrt{b}}\\geq\\frac{a-b}{2\\sqrt{a}}}\\end{array}$ V+ \u2265 , which holds for all $a\\ge b\\ge0$ ", "page_idx": 41}, {"type": "text", "text": "In epoch $\\mathcal{T}_{0}$ .We now analyze $\\sqrt{\\widetilde{N}_{T_{0}}}$ in epoch 0. Note that the $\\mathcal{T}_{0}$ contains only the first block $B_{1}$ , so this value is either O or 1, depending on whether there exists a $t\\ \\in\\ B_{1}$ such that $n_{t}=$ \u2161 $\\begin{array}{r}{\\left[\\!\\!\\left|x_{t}-p_{t}\\right|\\geq\\frac{1}{2}\\right]=1}\\end{array}$ ", "page_idx": 41}, {"type": "text", "text": "Recall that in the proof of Lemma D.1, we have shown that regardless of the choice of $p_{t}$ ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[n_{t}=1\\mid\\mathbb{F}_{t-1}\\right]=\\operatorname*{Pr}_{\\substack{x_{t}\\sim\\mathsf{B e r n o u l l i}(p_{t}^{\\star})}}\\left[\\left|x_{t}-p_{t}\\right|\\geq1/2\\right]\\geq p_{t}^{\\star}(1-p_{t}^{\\star})\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Therefore, in the special case of product distributions (i.e., the sequence $\\left(p_{t}^{\\star}\\right)$ is deterministic and eachoutcome $x_{t}\\sim p_{t}^{\\star}$ is independent of other time steps), we can directly bound the probability that $\\sqrt{\\widetilde{N}_{70}}=1$ as follows: ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\operatorname*{Pr}\\left[\\sqrt{\\widetilde{N}_{T_{0}}}=1\\right]=1-\\prod_{t=1}^{T_{1}}\\operatorname*{Pr}\\big[n_{t}=0\\big]\\ge1-\\prod_{t=1}^{T_{1}}[1-p_{t}^{\\star}(1-p_{t}^{\\star})]}}\\\\ &{}&{\\ge1-\\exp\\left(-\\displaystyle\\sum_{t=1}^{\\tau_{1}}p_{t}^{\\star}(1-p_{t}^{\\star})\\right)=1-\\exp(-\\mathrm{Var}_{B_{1}})\\ge\\frac{1}{2}\\mathrm{Var}_{B_{1}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where the last step follows from the inequality $1-e^{-x}\\geq x/2$ when $0\\le x\\le5/4$ , and the fact that $\\mathrm{Var}_{B_{1}}\\leq5/4$ ", "page_idx": 41}, {"type": "text", "text": "However, in the general case where the sequence $\\left(p_{t}^{\\star}\\right)$ is itself random and depends on the history of $x_{t}$ 's, such a direct argument fails. Instead, we use Lemma D.6 that extends the above analysis to this more general setting. Lemma D.6 is itself a similar but more general statement than Lemma D.5, as it is applicable even when the cumulative variance is smaller than the hard threshold $\\theta$ . Invoking Lemma D.6 with $q_{t}:=n_{t}$ \uff0c $r_{t}:=p_{t}^{\\star}(1-p_{t}^{\\star})$ , and the stopping time $\\tau$ as the earlier time step between the end ofblock $B_{1}$ and the first time where $n_{t}=1$ ,we have ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[N_{\\tau}\\geq1\\right]\\geq1-\\mathbb{E}\\left[e^{-\\mathrm{Var}_{\\tau}}\\right]\\geq\\frac{1}{2}\\,\\mathbb{E}\\left[\\mathrm{Var}_{\\tau}\\right],\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where the last step again uses $1-e^{-x}\\geq x/2$ for $x\\in[0,5/4]$ . Moreover, since $\\begin{array}{r l}{\\frac{5}{4}\\cdot\\mathbb{1}\\left[N_{\\tau}\\geq1\\right]\\geq}&{{}}\\end{array}$ $\\mathrm{Var}_{\\tau:b_{1}}$ , we also have ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[N_{\\tau}\\geq1\\right]\\geq\\frac45\\operatorname{\\mathbb{E}}\\left[\\operatorname{Var}_{\\tau:b_{1}}\\right]\\geq\\frac12\\operatorname{\\mathbb{E}}\\left[\\operatorname{Var}_{\\tau:b_{1}}\\right].\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Combining the two inequalities, we obtain ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{E}\\left[\\sqrt{\\widetilde{N}_{T_{0}}}\\right]=\\operatorname*{Pr}\\left[N_{B_{1}}\\ge1\\right]\\ge\\operatorname*{Pr}\\left[N_{\\tau}\\ge1\\right]}}\\\\ &{}&{\\ge\\frac{1}{4}\\mathbb{E}\\left[\\mathrm{Var}_{\\tau}+\\mathrm{Var}_{\\tau:b_{1}}\\right]=\\frac{1}{4}\\mathbb{E}\\left[\\mathrm{Var}_{B_{1}}\\right]}\\\\ &{}&{\\ge\\frac{1}{4}\\mathbb{E}\\left[\\mathrm{Var}_{T}\\cdot\\mathbb{I}\\left[\\tau_{1}=\\infty\\right]\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Putting everything together.  Combining the lower bounds for epoch O and epochs $k\\geq1$ we obtain ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\left[\\sqrt{\\widetilde{N}_{T}}\\right]=\\mathbb{E}\\left[\\sqrt{\\widetilde{N}_{T_{0}}}\\right]+\\underset{k\\ge1}{\\sum}\\mathbb{E}\\left[\\sqrt{\\widetilde{N}_{T_{k}}}-\\sqrt{\\widetilde{N}_{T_{k-1}}}\\right]}\\\\ &{\\ge\\frac{1}{4}\\mathbb{E}\\left[\\mathrm{Var}_{T}\\cdot\\mathbf{1}\\left[\\tau_{1}=\\infty\\right]+\\underset{k\\ge1}{\\sum}2^{\\frac{k}{2}-3}\\cdot\\mathbf{Pr}\\left[\\tau_{k}<\\infty\\right]}\\\\ &{=\\frac{1}{4}\\mathbb{E}\\left[\\mathrm{Var}_{T}\\cdot\\mathbf{1}\\left[\\tau_{1}=\\infty\\right]+\\underset{k\\ge1}{\\sum}\\mathrm{Pr}\\left[\\tau_{k-1}<\\infty,\\tau_{k}=\\infty\\right]\\underset{k^{\\prime}<k}{\\sum}2^{\\frac{k^{\\prime}-3}{2}}}\\\\ &{\\ge\\frac{1}{8\\sqrt{2}}\\mathbb{E}\\left[\\mathrm{Var}_{T}\\cdot\\mathbf{1}\\left[\\tau_{1}=\\infty\\right]+\\underset{k\\ge1}{\\sum}\\mathbf{1}\\left[\\tau_{k-1}<\\infty,\\tau_{k}=\\infty\\right]\\cdot2^{\\frac{k}{2}}\\right]}\\\\ &{\\ge\\frac{1}{16}\\mathbb{E}\\left[\\mathrm{Var}_{T}\\cdot\\mathbf{1}\\left[\\tau_{1}=\\infty\\right]+\\underset{k\\ge1}{\\sum}\\mathbf{1}\\left[\\tau_{k-1}<\\infty,\\tau_{k}=\\infty\\right]\\cdot\\sqrt{\\mathrm{Var}_{T}}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where the last step follows from the observation that the cumulative variance in each block cannot exceed 2, so $\\tau_{k}=\\infty$ implies that $\\mathrm{Var}_{T}<2^{k+1}$ , i.e., $2^{k/2}\\geq\\sqrt{\\mathrm{Var}_{T}}/\\sqrt{2}$ Finally, since $\\tau_{1}=\\infty$ is equivalent to $\\mathrm{Var}_{T}<1$ ,we have established that ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\sqrt{\\tilde{N}_{T}}\\right]\\geq\\frac{1}{16}\\mathbb{E}\\left[\\mathrm{Var}_{T}\\cdot\\mathbb{1}\\left[\\mathrm{Var}_{T}<1\\right]+\\mathbb{1}\\left[\\mathrm{Var}_{T}\\geq1\\right]\\cdot\\sqrt{\\mathrm{Var}_{T}}\\right]=\\frac{1}{16}\\,\\mathbb{E}\\left[\\gamma(\\mathrm{Var}_{T})\\right].\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "The lemma follows from the fact that $N_{T}~\\ge~\\widetilde{N}_{T}$ always holds, which implies $\\mathbb{E}\\left[\\sqrt{N_{T}}\\right]\\:\\geq\\:$ $\\begin{array}{r}{\\mathbb{E}\\left[\\sqrt{\\widetilde{N}_{T}}\\right]\\ge\\frac{1}{16}\\,\\mathbb{E}\\left[\\gamma(\\mathrm{Var}_{T})\\right]\\!.}\\end{array}$ ", "page_idx": 42}, {"type": "text", "text": "D.4  Auxiliary Lemmas ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Lemma D.5. Let $\\begin{array}{r}{Q_{t}=\\sum_{s\\leq t}q_{s},R_{t}=\\sum_{s\\leq t}r_{s}}\\end{array}$ be two (coupled) stochastic processes such that $q_{t}\\in\\{0,1\\}$ \uff0c $r_{t}\\in[0,1]$ for all $t\\in[T]$ . Let $\\mathbb{F}_{t}$ denote the filtration generated by all the randomness up to time $t$ .Suppose $r_{t}$ is a deterministic function on $\\mathbb{F}_{t-1}$ ,and $s_{t}^{\\bar{}}:=\\operatorname*{Pr}\\left[q_{t}=1\\mid\\mathbb{F}_{t-1}\\right]\\geq r_{t}$ ", "page_idx": 42}, {"type": "text", "text": "For any constant $\\theta>0$ define $\\tau_{\\theta}$ to be a stopping time chosen as the first time that $R_{t}$ reaches $\\theta_{i}$ i.e., ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\tau_{\\theta}:=\\operatorname*{min}\\{\\infty\\}\\cup\\{t\\in[T]\\mid R_{t}\\geq\\theta\\}.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Let $Q_{t}^{+}:=Q_{t:\\tau_{\\theta}}$ be the sum of $q_{s}$ in the future until the stopping time $\\tau_{\\theta}$ .If $t>\\tau_{\\theta}$ , then we let $Q_{t}^{+}:=0$ Consider random variables $A_{t}$ 's defined on the filtration $\\mathbb{F}_{t}$ as follows: ", "page_idx": 43}, {"type": "equation", "text": "$$\nA_{t}:=\\operatorname*{Pr}\\left[Q_{t}^{+}\\geq1\\;\\Big|\\;\\mathbb{F}_{t}\\right]-\\left(1-e^{-(\\theta-R_{t})}\\right)\\cdot\\operatorname*{Pr}\\left[\\tau_{\\theta}<\\infty\\;\\vert\\;\\mathbb{F}_{t}\\right].\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Then we have $A_{t}\\geq0$ for every $t\\leq T$ and every event in $\\mathbb{F}_{t}$ ", "page_idx": 43}, {"type": "text", "text": "Proof of Lemma D.5. It suffices to prove the inequality conditioning on events in $\\mathcal{F}_{t}$ that are \u201catomic\" in the sense that they uniquely determine the values of $q_{1:t}$ and $r_{1:t}$ . The general case would follow from the law of total probability. In particular, in the following proof, we may view the value of $R_{t}$ as fixed when we analyze the quantity $A_{t}$ ", "page_idx": 43}, {"type": "text", "text": "We perform a backwards induction from $t=T$ to $t=0$ . Consider the base case of $t=T$ If $R_{T}\\geq\\theta$ wehave ", "page_idx": 43}, {"type": "equation", "text": "$$\nA_{T}=\\underbrace{\\mathrm{Pr}\\left[Q_{T}^{+}\\geq1\\;\\Big|\\;\\mathbb{F}_{T}\\right]}_{=0}-\\underbrace{\\left(1-e^{-(\\theta-R_{T})}\\right)}_{\\leq0}\\cdot\\mathrm{Pr}\\left[\\tau_{\\theta}<\\infty\\;\\big|\\;\\mathbb{F}_{T}\\right]\\geq0.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Otherwise when $R_{T}<\\theta$ , we have $\\operatorname*{Pr}\\left[\\tau_{\\theta}<\\infty\\mid\\mathbb{F}_{T}\\right]=0$ , which also implies $A_{T}=0\\ge0$ ", "page_idx": 43}, {"type": "text", "text": "We then assume $A_{t}~\\geq~0$ , and show that the same holds for $A_{t-1}$ , where $t\\,\\leq\\,T$ . If $R_{t-1}\\,\\geq\\,\\theta$ we clearly have $A_{t-1}\\geq0$ , as the factor - $-\\left(1-e^{-\\left(\\theta-R_{t-1}\\right)}\\right)$ would be non-negative. Therefore, it suffices to consider the case that $R_{t-1}<\\dot{\\theta}$ . In this case, the stopping time $\\tau_{\\theta}$ should be $\\geq t$ we have $Q_{t-1}^{+}=q_{t}+Q_{t}^{+}$ . We bound $A_{t-1}$ by breaking the event $\\bar{Q}_{t-1}^{+}\\geq1$ into two cases: either $q_{t}=1$ ,or $q_{t}=0$ but $Q_{t}^{+}\\geq1$ . We have ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{Pr}\\left[Q_{t-1}^{+}\\geq1\\;\\Big|\\;\\mathbb{F}_{t-1}\\right]=\\operatorname*{Pr}\\left[q_{t}=1\\;\\big|\\;\\mathbb{F}_{t-1}\\right]+\\operatorname*{Pr}\\left[q_{t}=0\\;\\big|\\;\\mathbb{F}_{t-1}\\right]\\cdot\\operatorname*{Pr}\\left[Q_{t}^{+}\\geq1\\;\\big|\\;\\mathbb{F}_{t-1},q_{t}=0\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=s_{t}+(1-s_{t})\\,\\mathbb{E}\\left[\\operatorname*{Pr}\\left[Q_{t}^{+}\\geq1\\;\\big|\\;\\mathbb{F}_{t}\\right]\\;\\Big|\\;\\mathbb{F}_{t-1},q_{t}=0\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "For the second term, we apply the induction hypothesis of $A_{t}\\geq0$ and get ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\operatorname*{Pr}\\left[Q_{t}^{+}\\geq1\\mid\\mathbb{F}_{t}\\right]\\;\\Big|\\;\\mathbb{F}_{t-1},q_{t}=0\\right]\\geq\\mathbb{E}\\left[\\left(1-e^{-(\\theta-R_{t})}\\right)\\cdot\\operatorname*{Pr}\\left[\\tau_{\\theta}<\\infty\\mid\\mathbb{F}_{t}\\right]\\;\\Big|\\;\\mathbb{F}_{t-1},q_{t}=0\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\left(1-e^{-(\\theta-R_{t-1}-r_{t})}\\right)\\cdot\\operatorname*{Pr}\\left[\\tau_{\\theta}<\\infty\\mid\\mathbb{F}_{t-1},q_{t}=0\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where the second step uses the fact that conditioning on $\\mathcal{F}_{t-1}$ $R_{t}=R_{t-1}+r_{t}$ . As a result, we obtain ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[Q_{t-1}^{+}\\geq1\\;\\Big|\\;\\mathbb{F}_{t-1}\\right]\\geq s_{t}+(1-s_{t})\\left(1-e^{-(\\theta-R_{t-1}-r_{t})}\\right)\\cdot\\operatorname*{Pr}\\left[\\tau_{\\theta}<\\infty\\;\\big|\\;\\mathbb{F}_{t-1},q_{t}=0\\right].\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "We also expand the conditional probability $\\operatorname*{Pr}\\left[\\tau_{\\theta}<\\infty\\mid\\mathbb{F}_{t-1}\\right]$ as follows: ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{Pr}\\left[\\tau_{\\theta}<\\infty\\mid\\mathbb{F}_{t-1}\\right]=s_{t}\\cdot\\operatorname*{Pr}\\left[\\tau_{\\theta}<\\infty\\mid\\mathbb{F}_{t-1},q_{t}=1\\right]+(1-s_{t})\\operatorname*{Pr}\\left[\\tau_{\\theta}<\\infty\\mid\\mathbb{F}_{t-1},q_{t}=0\\right]}\\\\ &{\\qquad\\qquad\\qquad\\le s_{t}+(1-s_{t})\\operatorname*{Pr}\\left[\\tau_{\\theta}<\\infty\\mid\\mathbb{F}_{t-1},q_{t}=0\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Combining the bounds in (18) and (19), we obtain ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{A_{t-1}\\geq s_{t}+(1-s_{t})\\left(1-e^{-(\\theta-R_{t-1}-r_{t})}\\right)\\cdot\\operatorname*{Pr}\\left[\\tau_{\\theta}<\\infty\\mid\\mathbb{F}_{t-1},q_{t}=0\\right]}}\\\\ &{}&{-\\left(1-e^{-(\\theta-R_{t-1})}\\right)\\cdot\\left(s_{t}+(1-s_{t})\\operatorname*{Pr}\\left[\\tau_{\\theta}<\\infty\\mid\\mathbb{F}_{t-1},q_{t}=0\\right]\\right)}\\\\ &{}&{=s_{t}\\cdot e^{-(\\theta-R_{t-1})}+(1-s_{t})\\cdot\\left(e^{-(\\theta-R_{t-1})}-e^{-(\\theta-R_{t-1}-r_{t})}\\right)\\cdot\\operatorname*{Pr}\\left[\\tau_{\\theta}<\\infty\\mid\\mathbb{F}_{t-1},q_{t}=0\\right]}\\\\ &{}&{\\geq e^{-(\\theta-R_{t-1})}\\cdot\\left(s_{t}\\cdot e^{r_{t}}+1-e^{r_{t}}\\right)}\\\\ &{}&{\\geq e^{-(\\theta-R_{t-1})}\\cdot\\left(r_{t}\\cdot e^{r_{t}}+1-e^{r_{t}}\\right)}\\\\ &{}&{=e^{-(\\theta-R_{t-1})+r_{t}}\\cdot\\left(r_{t}+e^{-r_{t}}-1\\right)\\geq0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "We have thus proved that the claim also holds for $t-1$ . This completes the induction. ", "page_idx": 43}, {"type": "text", "text": "Lemma D.6.Let $\\begin{array}{r}{Q_{t}=\\sum_{s\\leq t}q_{s},R_{t}=\\sum_{s\\leq t}r_{s}}\\end{array}$ be two (coupled) stochastic processes such that $q_{t}\\in\\{0,1\\}$ \uff0c $r_{t}\\in[0,1]$ for all $t\\in[T]$ . Let $\\mathbb{F}_{t}$ denote thefiltrationgeneratedby all therandomnes up to time $t$ .Suppose $r_{t}$ is adeterministicfunctionon $\\mathbb{F}_{t-1}$ ,and $s_{t}^{\\bar{}}:=\\operatorname*{Pr}\\left[q_{t}=1\\mid\\mathbb{F}_{t-1}\\right]\\geq r_{t}$ ", "page_idx": 44}, {"type": "text", "text": "For any constant $\\theta>0$ define $\\tau$ to be a stopping time chosen as the first time that either $R_{t}$ reaches 1 or $q_{t}=1$ i.e., ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\tau:=\\operatorname*{min}\\{\\infty\\}\\cup\\{t\\in[T]\\mid R_{t}\\geq1\\}\\cup\\{t\\in[T]\\mid q_{t}=1\\}.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Let $Q_{t}^{+}:=Q_{t:\\tau}$ and $R_{t}^{+}:=Q_{t:\\tau}$ be the sum of $q_{s}$ and $r_{s}$ in the future until the stopping time $\\tau$ \uff0c respectively. We also let $Q_{t}^{+}=R_{t}^{+}=0$ when $t>\\tau$ . Consider random variables $A_{t}$ 's defined on the filtration $\\mathbb{F}_{t}$ as follows: ", "page_idx": 44}, {"type": "equation", "text": "$$\nA_{t}:=\\operatorname*{Pr}\\left[Q_{t}^{+}\\geq1\\;\\Big|\\;\\mathbb{F}_{t}\\right]-\\mathbb{E}\\left[1-e^{-R_{t}^{+}}\\;\\Big|\\;\\mathbb{F}_{t}\\right].\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Then we have $A_{t}\\geq0$ for every $t\\leq T$ and every event in $\\mathbb{F}_{t}$ ", "page_idx": 44}, {"type": "text", "text": "Proof of Lemma $D.6$ Using a similar approach to that for Lemma D.5, we prove this claim via a backwards induction from $t=T$ to $t=0$ . Again, we only consider the \u201catomic\u201d\u2019 events in $\\mathcal{F}_{t}$ that uniquely determines the values of $q_{1:t}$ and $r_{1:t}$ , and thus whether $\\tau\\leq t$ ; the general case follows from the law of total probability. ", "page_idx": 44}, {"type": "text", "text": "For the base case of $t=T$ , we have ", "page_idx": 44}, {"type": "equation", "text": "$$\nA_{T}=\\underbrace{\\mathrm{Pr}\\left[Q_{T}^{+}\\geq1\\;\\Big|\\;\\mathbb{F}_{T}\\right]}_{=0\\;\\mathrm{as}\\;Q_{T}^{+}\\equiv0}+\\underbrace{\\mathbb{E}\\left[e^{-R_{T}^{+}}\\;\\Big|\\;\\mathbb{F}_{T}\\right]}_{=1\\;\\mathrm{as}\\;R_{T}^{+}\\equiv0}-1=0.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Now for $t\\leq T$ , we assume the claim holds for $t$ and analyze $A_{t-1}$ .If $\\tau\\leq t-1$ , we immediately obtain $A_{t-1}\\geq0$ since $Q_{t-1}^{+}=R_{t-1}^{+}=0$ It remains to consider the case of $\\tau\\geq t$ . For the first term of $A_{t-1}$ (the conditional probability), we have ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{Pr}\\left[Q_{t-1}^{+}\\geq1\\;\\Big|\\;\\mathbb{F}_{t-1}\\right]=\\operatorname*{Pr}\\left[q_{t}=1\\;\\Big|\\;\\mathbb{F}_{t-1}\\right]+\\operatorname*{Pr}\\left[q_{t}=0\\;\\big|\\;\\mathbb{F}_{t-1}\\right]\\cdot\\operatorname*{Pr}\\left[Q_{t}^{+}\\geq1\\;\\big|\\;\\mathbb{F}_{t-1},q_{t}=0\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=s_{t}+(1-s_{t})\\operatorname*{Pr}\\left[Q_{t}^{+}\\geq1\\;\\Big|\\;\\mathbb{F}_{t-1},q_{t}=0\\right]}\\\\ &{\\qquad\\qquad\\qquad\\geq s_{t}+(1-s_{t})\\cdot\\mathbb{E}\\left[1-e^{-R_{t}^{+}}\\;\\Big|\\;\\mathbb{F}_{t-1},q_{t}=0\\right]}\\\\ &{\\qquad\\qquad\\qquad=1-(1-s_{t})\\cdot\\mathbb{E}\\left[e^{-R_{t}^{+}}\\;\\Big|\\;\\mathbb{F}_{t-1},q_{t}=0\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where the inequality step follows from the induction hypothesis $A_{t}\\geq0$ ", "page_idx": 44}, {"type": "text", "text": "On the other hand, the second term of $A_{t-1}$ (the conditional expectation) can be bounded as ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\mathbb{E}\\left[1-e^{-R_{t-1}^{+}}\\left|~\\mathbb{F}_{t-1}\\right]\\right.}\\\\ &{=\\mathrm{Pr}\\left[q_{t}=1\\;\\Big|\\;\\mathbb{F}_{t-1}\\right]\\cdot\\left(1-e^{-r_{t}}\\right)}&{(q_{t}=1\\;\\mathrm{implies}\\;\\tau=t\\;\\mathrm{and}\\;R_{t-1}^{+}=r_{t})}\\\\ &{\\quad\\quad\\quad+\\mathrm{Pr}\\left[q_{t}=0\\;\\Big|\\;\\mathbb{F}_{t-1}\\right]\\cdot\\mathbb{E}\\left[1-e^{-r_{t}-R_{t}^{+}}\\;\\Big|\\;\\mathbb{F}_{t-1},q_{t}=0\\right]}&\\\\ &{=1-s_{t}\\cdot e^{-r_{t}}-(1-s_{t})\\cdot\\mathbb{E}\\left[e^{-r_{t}-R_{t}^{+}}\\;\\Big|\\;\\mathbb{F}_{t-1},q_{t}=0\\right]}&\\\\ &{\\geq1-\\mathbb{E}\\left[e^{-r_{t}-(1-s_{t})R_{t}^{+}}\\;\\Big|\\;\\mathbb{F}_{t-1},q_{t}=0\\right].}&{(\\mathrm{fensen's~inequality~for~the~convex~function}\\;e^{-x})}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Finally, combining the bounds for both terms of $A_{t-1}$ ,weobtain ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A_{t-1}=\\operatorname*{Pr}\\left[Q_{t-1}^{+}\\geq1\\;\\middle|\\;\\mathbb{F}_{t-1}\\right]-\\mathbb{E}\\left[1-e^{-R_{t-1}^{+}}\\;\\middle|\\;\\mathbb{F}_{t-1}\\right]}\\\\ &{\\qquad\\geq\\mathbb{E}\\left[e^{-r_{t}-(1-s_{t})R_{t}^{+}}-(1-s_{t})\\cdot e^{-R_{t}^{+}}\\;\\middle|\\;\\mathbb{F}_{t-1},q_{t}=0\\right]}\\\\ &{\\qquad\\geq\\mathbb{E}\\left[e^{-R_{t}^{+}}\\cdot\\left(e^{-r_{t}}-(1-s_{t})\\right)\\;\\middle|\\;\\mathbb{F}_{t-1},q_{t}=0\\right]}\\\\ &{\\qquad\\geq\\mathbb{E}\\left[e^{-R_{t}^{+}}\\cdot\\left(e^{-s_{t}}-(1-s_{t})\\right)\\;\\middle|\\;\\mathbb{F}_{t-1},q_{t}=0\\right]}\\\\ &{\\qquad>0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "equation", "text": "$$\n(e^{s_{t}}R_{t}^{+}\\geq1)\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "We have proved that $A_{t-1}\\geq0$ . As a result, $A_{t}\\geq0$ for all $t\\leq T$ and all events in $\\mathcal{F}_{t}$ ", "page_idx": 44}, {"type": "text", "text": "EProof of Theorem 1.2 ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "In this section, we prove our main theorem (Theorem 1.2) by combining the theorems established in the previous sections, and then verifying the completeness and soundness of the SsCE. ", "page_idx": 45}, {"type": "text", "text": "Proof of Theorem 1.2. Let $\\mathcal{D}\\,\\in\\,\\Delta(\\{0,1\\}^{T})$ be an arbitrary distribution and define the random variable ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\mathrm{Var}_{T}:=\\sum_{t=1}^{T}p_{t}^{\\star}(1-p_{t}^{\\star}),\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "over $x\\sim\\mathcal{D}$ ,where $p_{t}^{\\star}:=\\operatorname*{Pr}_{x^{\\prime}\\sim\\mathcal{D}}\\left[x_{t}^{\\prime}=1\\mid x_{1:(t-1)}^{\\prime}=x_{1:(t-1)}\\right]$ By Theorems C.1 and D.2, the truthful forecaster gives ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\mathsf{e r r}_{\\mathsf{S S C E}}(\\mathcal D,\\mathcal A^{\\mathrm{truthful}}(\\mathcal D))=O\\left(\\underset{x\\sim\\mathcal D}{\\mathbb{E}}\\left[\\gamma(\\mathrm{Var}_{T})\\right]\\right),\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "whereas ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\mathsf{O P T}_{\\mathtt{S S C E}}(\\mathcal D)=\\Omega\\left(\\underset{x\\sim\\mathcal D}{\\mathbb{E}}\\left[\\gamma(\\mathrm{Var}_{T})\\right]\\right).\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "In the above, the $O(\\cdot)$ and $\\Omega(\\cdot)$ notations hide universal constants that do not depend on $\\mathcal{D}$ . Therefore, there exists a universal constant $c>0$ such that the SSCE is $(c,0)$ -truthful. ", "page_idx": 45}, {"type": "text", "text": "Completeness. Now we verify that the SSCE is complete. For any $x\\in\\{0,1\\}^{T}$ ,wehave ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\mathsf{S S C E}(x,x)=\\underset{y\\sim\\mathsf{U n i f}(\\{0,1\\}^{T})}{\\mathbb{E}}\\left[\\underset{f\\in\\mathcal{F}}{\\operatorname*{sup}}\\sum_{t=1}^{T}y_{T}\\cdot f(x_{t})\\cdot(x_{t}-x_{t})\\right]=0.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "For any $\\alpha\\in[0,1]$ , the upper bound ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\underset{x_{1},\\ldots,x_{T}\\sim\\mathsf{B e r n o u l i}(\\alpha)}{\\mathbb{E}}\\left[\\mathsf{S S C E}(x,\\alpha\\cdot\\vec{1}_{T})\\right]=O(\\sqrt{T\\cdot\\alpha\\cdot(1-\\alpha)})=o_{\\alpha}(T)\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "follows from applying Theorem C.1 to the product distribution $\\begin{array}{r}{\\mathcal{D}=\\prod_{t=1}^{T}}\\end{array}$ Bernoulli $(\\alpha)$ and the fact that $\\gamma(x)\\leq{\\sqrt{x}}$ for all $x\\geq0$ ", "page_idx": 45}, {"type": "text", "text": "Soundness. To show that the SSCE is sound, we first consider the case that $x\\in\\{0,1\\}^{T}$ is arbitrary and the predictions are $p={\\vec{1}}_{T}-x$ . Noting that the function $x\\mapsto1/2-x$ is in the family $\\mathcal{F}$ of 1-Lipschitz functions from [0, 1] to $[-1,1]$ , we have ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{S S C E}(x,\\vec{1}_{T}-x)=\\underset{y\\sim\\mathsf{U n i f}(\\{0,1\\}^{T})}{\\mathbb{E}}\\left[\\underset{f\\in\\mathcal{F}}{\\operatorname*{sup}}\\sum_{t=1}^{T}y_{t}\\cdot f(1-x_{t})\\cdot\\left(x_{t}-(1-x_{t})\\right)\\right]}\\\\ &{\\qquad\\qquad\\geq\\underset{y\\sim\\mathsf{U n i f}(\\{0,1\\}^{T})}{\\mathbb{E}}\\left[\\displaystyle\\sum_{t=1}^{T}y_{t}\\cdot\\left(x_{t}-1/2\\right)\\cdot\\left(2x_{t}-1\\right)\\right]}\\\\ &{\\qquad=\\underset{y\\sim\\mathsf{U n i f}(\\{0,1\\}^{T})}{\\mathbb{E}}\\left[\\frac{1}{2}\\displaystyle\\sum_{t=1}^{T}y_{t}\\right]=\\frac{T}{4}=\\Omega(T),}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where the third step holds since $(x-1/2)\\cdot(2x-1)=1/2$ holds for every $x\\in\\{0,1\\}$ ", "page_idx": 45}, {"type": "text", "text": "Finally, we fix $\\alpha,\\beta\\in[0,1]$ such that $\\alpha\\neq\\beta$ . For fixed $x,y\\in\\{0,1\\}^{T}$ , we have ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{f\\in\\mathcal{F}}\\sum_{t=1}^{T}y_{t}\\cdot f(\\beta)\\cdot\\left(x_{t}-\\beta\\right)=\\left|\\sum_{t=1}^{T}y_{t}\\cdot\\left(x_{t}-\\beta\\right)\\right|.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Taking an expectation over $x_{1},\\ldots,x_{T}\\sim\\mathsf{B e r n o u l l i}(\\alpha)$ and $y\\sim\\mathsf{U n i f}(\\{0,1\\}^{T})$ gives ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{x_{1},\\ldots,x_{T}\\sim\\mathbf{B}\\mathbf{ernoilit}(\\alpha)}{\\mathbb{E}}\\left[\\mathbf{S}\\mathbf{S}\\mathbf{C}\\mathbf{E}(x,\\beta\\cdot\\frac{\\mathbb{T}}{\\mathbb{T}})\\right]=\\underset{x_{\\mathcal{N}}}{\\mathbb{E}}\\left[\\underset{f\\in\\mathcal{F}_{t}=1}{\\operatorname*{sup}}\\,\\sum_{\\ell}^{T}\\cdot f(\\beta)\\cdot\\left(x_{t}-\\beta\\right)\\right]}\\\\ {=\\underset{x_{\\mathcal{N}}}{\\mathbb{E}}\\left[\\left|\\underset{t=1}{\\overset{T}{\\sum}}y_{t}\\cdot(x_{t}-\\beta)\\right|\\right]}\\\\ {\\geq\\left|\\underset{x_{\\mathcal{N}}}{\\mathbb{E}}\\left[\\underset{t=1}{\\overset{T}{\\sum}}y_{t}\\cdot(x_{t}-\\beta)\\right]\\right|}\\\\ {=\\left|\\frac{\\alpha-\\beta}{2}\\cdot T\\right|=\\Omega_{\\alpha,\\beta}(T),}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "where the third step follows from Jensen's inequality $\\mathbb{E}\\left[\\left|X\\right|\\right]\\geq\\left|\\mathbb{E}\\left[X\\right]\\right|$ ", "page_idx": 46}, {"type": "text", "text": "F Proof of Lemma 7.1 ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Lemma 7.1. For any $x\\in\\{0,1\\}^{T}$ and $p\\in[0,1]^{T}$ ", "page_idx": 46}, {"type": "equation", "text": "$$\n{\\mathsf{S S C E}}(x,p)\\leq{\\frac{1}{2}}{\\mathsf{s m C E}}(x,p)+O({\\sqrt{T}}),\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "wherethe $O(\\cdot)$ notation hides a universal constant that does not depend on $T$ Cor $p$ ", "page_idx": 46}, {"type": "text", "text": "We prove Lemma 7.1 via a standard chaining argument. ", "page_idx": 46}, {"type": "text", "text": "Proof of Lemma 7.1. We decompose the SSCE as follows: ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathsf{S S C E}(x,p)}\\\\ &{=\\underbrace{\\mathbb{E}}_{y\\sim\\mathsf{U n i f}(\\{0,1\\}^{T})}\\left[\\displaystyle\\operatorname*{sup}_{f\\in\\mathcal{F}}\\sum_{t=1}^{T}y_{t}\\cdot f(p_{t})\\cdot(x_{t}-p_{t})\\right]}\\\\ &{\\le\\underbrace{\\mathbb{E}}_{y\\sim\\mathsf{U n i f}(\\{0,1\\}^{T})}\\left[\\displaystyle\\operatorname*{sup}_{f\\in\\mathcal{F}}\\sum_{t=1}^{T}\\left(y_{t}-\\frac{1}{2}\\right)\\cdot f(p_{t})\\cdot(x_{t}-p_{t})\\right]+\\frac{1}{2}\\displaystyle\\operatorname*{sup}_{f\\in\\mathcal{F}}\\sum_{t=1}^{T}f(p_{t})\\cdot(x_{t}-p_{t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Note that the second term is exactly ${\\scriptstyle{\\frac{1}{2}}}\\mathsf{s m C E}(x,p)$ , so it suffices to bound the frst term by $O(\\sqrt{T})$ ", "page_idx": 46}, {"type": "text", "text": "$\\begin{array}{r}{\\boldsymbol{M}_{T}^{(f)}:=\\sum_{t=1}^{T}\\left(\\boldsymbol{y}_{t}-\\frac{1}{2}\\right)\\cdot\\boldsymbol{f}(\\boldsymbol{p}_{t})\\cdot\\left(\\boldsymbol{x}_{t}-\\boldsymbol{p}_{t}\\right)}\\end{array}$ $f\\in\\mathcal F$ $N\\geq1$ $f_{1},f_{2},\\ldots,f_{N}$ $[0,1]$ $[-1,1]$ ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\substack{y\\sim\\mathsf{U n i f}\\,\\left(\\left\\{0,1\\right\\}^{T}\\right)}}\\left[\\operatorname*{sup}_{i\\in[N]}M_{T}^{\\left(f_{i}\\right)}\\right]\\le O(\\sqrt{T\\log N}).\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Assuming Inequality (20), applying Dudley's chaining technique [Dud87] to the $\\delta$ -covering $\\mathcal{F}_{\\delta}$ defined in Lemma C.2would give ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{E}_{\\mathbb{\\theta}\\sim\\mathbb{U}_{\\mathbb{N}}^{\\star}(\\{0,1\\}^{T})}\\left[\\operatorname*{sup}_{f\\in\\mathcal{F}}M_{T}^{(f)}\\right]\\lesssim\\int_{0}^{1}\\sqrt{T\\log|\\mathcal{F}_{\\delta}|}\\,\\,\\mathrm{d}\\delta}}&{}&{\\mathrm{(chaining)}}\\\\ &{}&{\\lesssim\\sqrt{T}\\cdot\\int_{0}^{1}\\delta^{-\\frac{1}{2}}\\,\\,\\mathrm{d}\\delta}&{\\mathrm{(log\\,}|\\mathcal{F}_{\\delta}|\\leq O(1/\\delta)\\mathrm{~from~Lemma~C.2})}\\\\ &{}&{\\leq O(\\sqrt{T}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "which implies the lemma. ", "page_idx": 46}, {"type": "text", "text": "Therefore, it remains to establish Inequality (20). We prove this using Hoeffding's inequality and a union bound. For each $i\\in[N]$ andevery $\\varepsilon>0$ ,wehave ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[\\operatorname*{sup}_{i\\in[N]}M_{T}^{(f_{i})}\\geq\\varepsilon\\right]\\leq\\sum_{i=1}^{N}\\operatorname*{Pr}\\left[M_{T}^{(f_{i})}\\geq\\varepsilon\\right]\\qquad\\qquad\\qquad\\mathrm{(union~bound)}}\\\\ {\\leq\\sum_{i=1}^{N}\\exp\\left(-\\frac{2\\varepsilon^{2}}{\\sum_{t=1}^{T}(x_{t}-p_{t})^{2}f_{i}(p_{t})^{2}}\\right)\\qquad\\mathrm{(Hoeffding's~inequality)}}\\\\ {\\leq N\\cdot\\exp\\left(-\\frac{2\\varepsilon^{2}}{T}\\right).\\qquad\\qquad\\qquad\\qquad\\mathrm{(}||f_{i}||_{\\infty}\\leq1,\\ \\forall i\\in[N])}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Finally, the bound (20) holds by taking an integral over $\\varepsilon>0$ shorthanding $\\begin{array}{r}{X:=\\operatorname*{sup}_{i\\in[N]}M_{T}^{(f_{i})}}\\end{array}$ wehave ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[X\\right]\\leq\\int_{0}^{+\\infty}\\operatorname*{Pr}\\left[X\\geq\\tau\\right]~\\mathrm{d}\\tau\\leq\\int_{0}^{+\\infty}\\operatorname*{min}\\{N\\cdot e^{-2\\tau^{2}/T},1\\}~\\mathrm{d}\\tau=O(\\sqrt{T\\log N}).\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "This completes the proof. ", "page_idx": 47}, {"type": "text", "text": "G  Supplemental Materials for Section 8 ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "We justify the claim in Section 8 that it is impossible for the SSCE (and most natural calibration measures) to incentivize truthful prediction against all adaptive adversaries. ", "page_idx": 47}, {"type": "text", "text": "Suppose that the adversary draws $x_{1}$ from Bernoulli $(1/2)$ . If the forecaster predicts $p_{1}\\,=\\,0$ , all the subsequent bits are zeros; otherwise, the adversary keeps producing independent samples from Bernoulli $(1/2)$ ", "page_idx": 47}, {"type": "text", "text": "Clearly, the truthful forecaster predicts $p_{t}=1/2$ at every step $t\\in[T]$ , and the resulting outcome sequence $x$ is uniform over $\\{0,1\\}^{T}$ . The resulting SSCE is then $\\Theta(T^{1/2})$ in expectation. If the forecaster keeps predicting $p_{t}=0$ instead, the expectation of ${\\mathsf{S S C E}}(x,p)$ is only $O(1)$ . Note that this impossibility holds for any calibration measure CM that satisfies ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\substack{x\\,1,\\ldots,x_{T}\\sim\\mathsf{B e r n o u l l i}(1/2)}}\\left[\\mathsf{C M}_{T}(x,\\vec{1}_{T}/2)\\right]=\\omega(1)}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "and ", "page_idx": 47}, {"type": "equation", "text": "$$\n{\\underset{x_{1}\\sim\\mathsf{B e r n o u l l i}(1/2)}{\\mathbb{E}}}\\left[\\mathsf{C M}_{T}(x_{1}\\circ\\vec{0}_{T-1},\\vec{0}_{T})\\right]=O(1),\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "where $\\circ$ denotes concatenation. ", "page_idx": 47}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Justification: The claims match the theoretical results that we prove in the paper. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 48}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Justification: This is a theoretical work, so the results hold for the specific problem setups and formulations, which we formally state in Section 2. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should refect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should refect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 48}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Justification: All the theorems and claims are formally proved, either in the main paper or in the appendix. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 49}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 49}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 49}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. () If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 49}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 49}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 50}, {"type": "text", "text": "Justification: The paper does not include experiments requiring code. Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : / /nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 50}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 50}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 50}, {"type": "text", "text": "Justification: The paper does not include experiments. Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 50}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 50}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 50}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative errorrates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 50}, {"type": "text", "text": "", "page_idx": 51}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce theexperiments? ", "page_idx": 51}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 51}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 51}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 51}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 51}, {"type": "text", "text": "Justification: The research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 51}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 51}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 51}, {"type": "text", "text": "Justification: This is a theoretical work, and there is no societal impact of the work performed to the best of our knowledge. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 51}, {"type": "text", "text": "", "page_idx": 52}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 52}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 52}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 52}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properlyrespected? ", "page_idx": 52}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 52}, {"type": "text", "text": "Justification: The paper does not use existing assets. ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 52}, {"type": "text", "text": "\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 53}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 53}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 53}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 53}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 53}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 53}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 53}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 53}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 53}]