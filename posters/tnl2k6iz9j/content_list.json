[{"type": "text", "text": "Dynamic Service Fee Pricing under Strategic Behavior: Actions as Instruments and Phase Transition ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Rui Ai David Simchi-Levi MIT MIT ruiai@mit.edu dslevi@mit.edu Feng Zhu MIT fengzhu@mit.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We study a dynamic pricing problem for third-party platform service fees under strategic, far-sighted customers. In each time period, the platform sets a service fee based on historical data, observes the resulting transaction quantities, and collects revenue. The platform also monitors equilibrium prices influenced by both demand and supply. The objective is to maximize total revenue over a time horizon $T$ . Our problem incorporates three practical challenges: (a) initially, the platform lacks knowledge of the demand side beforehand, necessitating a balance between exploring (learning the demand curve) and exploiting (maximizing revenue) simultaneously; (b) since only equilibrium prices and quantities are observable, traditional Ordinary Least Squares (OLS) estimators would be biased and inconsistent; (c) buyers are rational and strategic, seeking to maximize their consumer surplus and potentially misrepresenting their preferences. To address these challenges, we propose novel algorithmic solutions. Our approach involves: (i) a carefully designed active randomness injection to balance exploration and exploitation effectively; (ii) using non-i.i.d. actions as instrumental variables (IV) to consistently estimate demand; (iii) a low-switching cost design that promotes nearly truthful buyer behavior. We show an expected regret bound of $\\widetilde{\\mathcal{O}}(\\sqrt{T}\\wedge\\sigma_{S}^{-2})$ and demonstrate its optimality, up to logarithmic factors, with respect to both the time horizon $T$ and the randomness in supply $\\sigma_{S}$ . Despite its simplicity, our model offers valuable insights into the use of actions as estimation instruments, the benefits of low-switching pricing policies in mitigating strategic buyer behavior, and the role of supply randomness in facilitating exploration which leads to a phase transition of policy performance. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "A large number of transactions nowadays take place on third-party platforms, such as shopping on Amazon, taking rides on Uber, and ordering takeout food on DoorDash [33, 39]. For these platforms, deciding how to set the service fee is an important issue [35, 40, 49] that not only affects the platform\u2019s short-term revenue but also impacts long-term user retention. Therefore, understanding buyers\u2019 demand curve is of significant importance for platforms aiming to maximize their revenue. ", "page_idx": 0}, {"type": "text", "text": "Example 1.1. As an illustrative example, as a ride-hailing platform, Uber possesses information on the supply side (drivers), specifically the number of drivers available on the road at any given moment. Simultaneously, when buyers (passengers) request a ride, it matches drivers with passengers and charges a certain fee. Uber charges a booking fee for each reservation. Over a certain time period, ", "page_idx": 0}, {"type": "text", "text": "Uber\u2019s booking fee remains approximately the same, as shown in Figure 1. However, over a longer period of time, an exploratory dynamic pricing strategy can be beneficial to acquire more demand information. ", "page_idx": 1}, {"type": "text", "text": "Meanwhile, consumers often refrain from taking rides when prices are high even if these prices are within their willingness to pay, aiming at inducing Uber to reduce the price or offer them coupons (which can be understood as a negative service fee) to reduce future purchase expenses. This phenomenon is common in both the psychology [6] and the economics [27, 42] literature. ", "page_idx": 1}, {"type": "image", "img_path": "Tnl2K6Iz9j/tmp/9523f0653906e3e6850f3aa6131da55b3afb46a1daab7512e72f10f09737a1e6.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Uber. Above are two Uber rides from the same city in February 2024. The price of the two rides differs by nearly double, yet the booking fee remains at $\\mathbb{S}1.27$ for both, suggesting that a fixed booking fee mechanism might be employed in February. However, over a longer period of time, Uber may switch to a dynamic pricing strategy. ", "page_idx": 1}, {"type": "text", "text": "For third-party platforms, pricing service fees can face three challenges: ", "page_idx": 1}, {"type": "text", "text": ". Demand information needs to be learned. Typically platforms can observe the number and quoted prices of sellers on the platform, i.e., the supply curve, but the preference of buyers, or the demand curve, is not observable. At the same time, due to legal restrictions [58], platforms in many cases cannot personalize pricing for different buyers but can only set a uniform price for a group of buyers. Therefore, it is crucial to learn about the buyer group\u2019s willingness to pay. ", "page_idx": 1}, {"type": "text", "text": "2. Only equilibria can be observed. Regarding buyer information, platforms can only observe the equilibrium price and quantity, say $P^{e}$ and $Q^{e}$ respectively, which depend on the service fee set by the platform itself and the changing supply curve. Thus, due to changes in the demand curve, $(P^{e},Q^{e})$ may only reveal partial information about the demand curve and thus can fail to recover the full demand curve. In the absence of randomness in the supply curve, $(P^{e},Q^{e})$ could even form an upward-sloping curve, far from the characteristics of a demand curve. ", "page_idx": 1}, {"type": "text", "text": "3. Buyers may exhibit strategic behavior. When buyers interact with the platform over an extended period, they may present a false demand curve to the platform, hoping to gain more benefits in future purchases, as shown in Example 1.1. The strategic behavior of buyers increases the difficulty for the platform to accurately estimate demand, and at the same time, causes the service fee to deviate from its optimal value, resulting in revenue loss. Pessimistically, Amin et al. [2] demonstrated that when the buyer possesses patience comparable to that of the seller, no learning algorithm can achieve sub-linear regret (cf. Appendix F.6). ", "page_idx": 1}, {"type": "text", "text": "In this paper, we address the above challenges by establishing the first set of theories for pricing service fees on third-party platforms. We summarize our contributions in the following. ", "page_idx": 1}, {"type": "text", "text": "\u2022 To the best of our knowledge, we are the first to attempt applying non-i.i.d. actions as instrumental variables in the problem of online pricing. In the traditional econometric framework [67], researchers often seek external instrumental variables to estimate the demand curve. However, this method has significant limitations because good instrumental variables are hard to find. We demonstrate that even when actions are not independent and identically distributed (i.i.d.) random variables, or even possess strong correlations with one another, they still lead to excellent estimate of the demand curve. In Theorems 3.1, 3.2 and 4.3, we show our algorithms\u2019 optimal regret bounds. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "\u2022 We discover that the randomness in supply can effectively assist us in learning the demand curve. This counterintuitive fact reveals why, in Theorem 3.2, we can achieve a regret of $\\widetilde O(1)$ , but in the case where there is no noise in supply, in Theorem 3.1, we can only expect a regret of $\\widetilde{\\mathcal{O}}(\\sqrt{T})$ . We explain in Section 4 via lower bound results (Theorem 4.1 and 4.2) that these  orders of magnitude differences are fundamental and unavoidable. Additionally, we detailed the phase transition points of regret bounds regarding supply randomness. \u2022 We investigate robust pricing in the absence of prior knowledge of the buyer\u2019s discount rate. Our AaI and AAaI algorithms don\u2019t require the input of a discount rate to initiate, but can instead universally motivate buyers whose time has value to nearly truthfully report the demand curve. Specifically, our algorithm is also applicable in scenarios where the discount rate varies. The robustness of our algorithm is benign both in theory and applications. ", "page_idx": 2}, {"type": "text", "text": "1.1 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "This work is closely related to instruments in machine learning [4, 5, 12, 56, 73, 47, 24, 69, 23], pricing with strategic buyers [55, 2, 29, 37, 1, 38], and demand learning under uncertainty [14, 26, 46, 21, 7, 48, 20, 50, 28]. Due to space constraints, additional references are provided in Appendix A for readers\u2019 reference. Detailed discussion of the relation and comparison between our work and previous work is also presented in Appendix A. ", "page_idx": 2}, {"type": "text", "text": "Notations. For any positive integer $n$ , we let $[n]$ denote the set $\\{1,...,n\\}$ and $n_{1}:n_{2}$ represent $\\{n_{1},...,n_{2}-1\\}$ . We use $N(\\mu,\\sigma^{2})$ to represent a Gaussian random variable with mean $\\mu$ and variance $\\sigma^{2}$ . Moreover, we use $O(\\cdot)$ when ignoring constant terms while $\\widetilde O(\\cdot)$ when ignoring constants and logarithmic terms. Similarly, we have $\\Omega(\\cdot),\\widetilde\\Omega(\\cdot),\\Theta(\\cdot)$ and $\\widetilde{\\Theta}(\\cdot)$ . ", "page_idx": 2}, {"type": "text", "text": "2 Model and Assumptions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We consider an online dynamic pricing problem faced by a platform interacting with a representative buyer for $T$ rounds. The platform aims to maximize its revenue against the rational and strategic buyer by dynamically adjusting the service fees. ", "page_idx": 2}, {"type": "text", "text": "Information structure of platform pricing. In each round $t$ , multiple sellers pose different portfolios on the platform and thus form the cumulative supply curve, denoted by $P_{S t}=P_{S t}(Q)$ as a function of $Q$ . For example, Uber can observe in real-time how many different types of drivers are available on the platform, such as UberX, UberXL, Black SUV, etc. Similarly, Ticketmaster knows how many tickets are still available in each area. Then, the platform sets a service fee $a_{t}\\,\\in\\,\\mathbb{R}_{\\geq0}$ . Here we assume the sellers are not strategic: as the platform formulates the fee-charging plan after observing $P_{S t}(Q)$ , there is no issue concerning trustfulness for sellers. In each round $t$ , the representative buyer may receive a private signal, such as a shock on income, and form a time-dependent demand curve, denoted by $P_{D t}=P_{D t}(Q)$ . However, the buyer may behave as if her demand is $P_{D t}^{\\prime}$ rather than $P_{D t}$ because of her forward-looking strategic behavior, elaborating below. Together with service fee $a_{t}$ , the platform observes an equilibrium price and equilibrium quantity in the market, denoted by $(P_{t}^{e},Q_{t}^{e})$ , satisfying ", "page_idx": 2}, {"type": "equation", "text": "$$\nP_{t}^{e}:=P_{S t}(Q_{t}^{e})+a_{t}=P_{D t}^{\\prime}(Q_{t}^{e}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Then, the platform receives revenue $\\Pi_{t}=a_{t}\\cdot Q_{t}^{e}(P_{S t},P_{D t}^{\\prime},a_{t})$ in this round. As a result, across the time horizon of $T$ , the platform has a cumulative revenue: $\\begin{array}{r}{\\sum_{t=1}^{T}a_{t}\\cdot Q_{t}^{e}(P_{S t},P_{D t}^{\\prime},a_{t})}\\end{array}$ . As a concrete setting, we study the case when both the supply and de mand curves have linear forms following canonical literature of both demand learning [19, 61, 44, 74] and instrumental variable models [56]. ", "page_idx": 2}, {"type": "text", "text": "Assumption 2.1. Assume that the supply curve in round $t$ has the form of ", "page_idx": 2}, {"type": "equation", "text": "$$\nP_{S t}(Q)=\\alpha_{0}+\\alpha_{1}Q+\\epsilon_{S t}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "and the demand curve is ", "page_idx": 2}, {"type": "equation", "text": "$$\nP_{D t}(Q)=\\beta_{0}+\\beta_{1}Q+\\epsilon_{D t}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The independent noise terms $\\epsilon_{S t}$ and $\\epsilon_{D t}$ follow normal distributions, $\\mathcal{N}(0,\\sigma_{S}^{2})$ and $\\mathcal{N}(0,\\sigma_{D}^{2})$ , respectively. ", "page_idx": 2}, {"type": "text", "text": "To avoid nuisance from market feasibility, we assume that $\\beta_{0}>\\alpha_{0}\\geq0$ , $\\alpha_{1}\\geq0>\\beta_{1}$ , and that all equilibrium prices and quantities are positive and bounded. We note from Equation (1) that if $Q_{t}^{e}$ is negative, then optimal $Q_{t}^{e}$ shall be truncated as 0. For simplicity of analysis, we will not make truncation throughout the paper. This shall not influence the validity of our analysis and results. ", "page_idx": 3}, {"type": "text", "text": "Utility-maximizing buyer. We assume the representative buyer fully knows the learning policy used by the platform to set service fees [37]. Note that the buyer is only aware of the policy beforehand. If the policy involves randomization, the buyer knows in advance the policy but not the realization of the policy. In fact, previous behavior-based pricing literature [41, 64, 10, 11] has suggested that committing to a pricing strategy can help the platform earn more revenue. ", "page_idx": 3}, {"type": "text", "text": "In each time period $t$ , the buyer receives a surplus, say surplus $_t(P_{S t},P_{D t},P_{t}^{e},Q_{t}^{e},a_{t})$ , which depends on the supply curve, demand curve, equilibrium price, equilibrium quantity, and the service fee. For brevity, we will write $\\textstyle\\operatorname{Sur}_{t}$ as an abbreviation of surplus $_t(P_{S t},P_{D t},P_{t}^{e},Q_{t}^{e},a_{t})$ . We utilize the Ramsey model [62], which originates from the economic literature, to calculate the surplus. More rigorously, we employ Assumption 2.2. We postpone the discussion of the economic intuition behind Assumption 2.2 and its implications for calculating the surplus to Appendix $\\mathbf{B}$ for interested readers. ", "page_idx": 3}, {"type": "text", "text": "Assumption 2.2. We assume a private market where all sellers are civilian-run enterprises. ", "page_idx": 3}, {"type": "text", "text": "The representative buyer has a discount rate $\\gamma\\in[0,1)$ . When $\\gamma=0$ , the buyer is myopic and only considers her surplus in the current round $t$ . She truthfully purchases the optimal quantity of items she needs. That is, her realized demand curve $P_{D t}^{\\prime}$ coincides with $P_{D t}$ . However, when $\\dot{\\gamma}\\in(0,1)$ , i.e., the buyer is far-sighted [8, 72], she may choose to misreport her demand curve $P_{D t}^{\\prime}\\neq P_{D t}$ in exchange for increasing long-term expected cumulative utility: $\\mathbb{E}[\\sum_{s=t}^{T}\\gamma^{s-t}\\mathrm{Sur}_{s}]$ , where again $\\mathrm{Sur}_{s}$ is the surplus gained at time $s$ . The expectation is taken over the randomness for all time $s>t$ : supply randomness $\\epsilon_{S s}$ , demand randomness $\\epsilon_{D s}$ , and potential randomness from the platform pricing policy. Here, we use the general economic term \u201cmisreport\u201d to represent that the equilibrium price and quantity are not aligned with the buyer\u2019s true demand. They can be either larger or smaller. Note that the buyer won\u2019t show her demand curve $P_{D t}(Q)$ to the platform at all, and all the information the platform can learn is through observing equilibrium prices and quantities. Thus, it is important for the platform to design a careful service charging mechanism to motivate a truthful disclosure of the demand curve from the buyer. ", "page_idx": 3}, {"type": "text", "text": "Performance metric. With the help of the revelation principle [63], there is an incentive-compatibledirect mechanism for pricing to achieve the highest revenue. So, we can define optimal service fee from Equation (1) by ", "page_idx": 3}, {"type": "equation", "text": "$$\na_{t}^{*}=\\underset{a_{t}\\geq0}{\\operatorname{argmax}}\\,\\mathbb{E}\\big[a_{t}\\cdot Q_{t}^{e}(P_{S t},P_{D t},a_{t})\\big].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The expectation is taken over $P_{D t}$ since the platform needs to set $a_{t}$ before the realization of $P_{D t}$ . Define the suboptimality at time $t$ as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{SubOpt}_{t}(a_{t})=\\mathbb{E}[a_{t}^{*}\\cdot Q_{t}^{e}(P_{S t},P_{D t},a_{t}^{*})-a_{t}\\cdot Q_{t}^{e}(P_{S t},P_{D t}^{\\prime},a_{t})].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Our evaluation metric is then the revenue regret against a clairvoyant policy attained over the whole $T$ rounds, namely, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname{Regret}(T)=\\sum_{t=1}^{T}\\operatorname{SubOpt}_{t}(a_{t}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Remarks on the model. We would like to provide some further remarks on the model as follows. ", "page_idx": 3}, {"type": "text", "text": "1. Platform is patient while buyer\u2019s discount rate is $\\gamma\\in[0,1)$ . In practice, platforms are usually less time-sensitive compared with individuals, for instance, in a sponsored search auction, where the platform usually auctions off large numbers of ad slots each time while buyers usually urgently need advertisement and value future rewards less. On the other hand, the platform is not especially concerned with slight decreases in immediate rewards and maybe more on user stickiness. Readers can refer to Drutsa [30], Golrezaei et al. [37] for more information on different time values for different market forces. Additionally, from a theoretical standpoint, achieving sub-linear regret is impossible when the buyer discount rate is one [2]. Therefore, our paper focuses on strategic buyers whose discount rates fall within the $[0,1)$ range, accommodating both real-world scenarios and theoretical constraints. ", "page_idx": 3}, {"type": "text", "text": "2. On and beyond the linear model. In this paper, we focus on linear models. In practical scenarios, particularly when considering a localized segment of the market to avoid eliciting competitive reactions, there often emerges a pattern of linearity (which also serves as the cornerstone for the application of the so-called \u201cDelta method\u201d [45]). Further, Besbes and Zeevi [15] also suggests that in certain circumstances, misspecification stemmed from assuming a linear model is less detrimental than anticipated. We note that although we explicitly assume the linearity of the demand curve and the supply curve in the model, it should not be difficult to extend to some variations of nonlinear models (e.g. log-log, semi-log model in hedonic pricing [66] and logistic model in click prediction [51]) like Ban and Keskin [13] with some extra assumption on non-linear factors, by utilizing the generalized method of moments (GMM). Our technique also has the potential to work in high-dimensional cases and even non-parametric Reproducing Kernel Hilbert Space (RKHS). Despite such extensions, we will adopt the linear model throughout this work for brevity and clarity. We leave further generalizations for future work. ", "page_idx": 4}, {"type": "text", "text": "3 Regret Upper Bounds: Actions as Instruments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we present our main algorithm AaI (Action-as-Instruments) and demonstrate its theoretical guarantees. We assume that $T$ and $\\sigma_{S}\\geq0$ are both known $\\sigma_{D}$ need not be known a priori). This assumption will be further relaxed in the next section. ", "page_idx": 4}, {"type": "table", "img_path": "Tnl2K6Iz9j/tmp/f19c800dcca2a1715b855ba00618c573e7ec0dd1ab86ef660424d13b23a90b08.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "table", "img_path": "Tnl2K6Iz9j/tmp/2e57c819732b3c572fdb6a4813e30f4ca129c18767c6631a9877202abc67bbe0.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "We now elaborate on the design of AaI and how it overcomes the main challenges mentioned in Section 1. We first demonstrate the design of our low-switching regime. In Algorithm 1, to deal with the far-sighted buyer, we update our policy when $t$ is a power of 2. The intuition behind the low-switching cost update is that since the buyer has a discount rate strictly lower than 1, long-run beneftis will be hard to compensate for short-run loss. Under a low-switching regime, the gap between the buyer\u2019s behavior $P_{D}^{\\prime}$ and the true demand curve $P_{D}$ becomes small. In the existing literature, Ai et al. [1] utilizes explicit \u201cbuffer\u201d periods against far-sighted buyers while other works such as Golrezaei et al. [37, 38] use implicit methods to motivate truthfulness. In Algorithm 1, we adopt an implicit way to punish untruthful behavior so that we don\u2019t need information about the buyer\u2019s discount rate, thus enhancing the robustness and universality of our algorithm. We note that the use of low-switching cost algorithms may not be necessary when facing a myopic buyer, though indispensable for a far-sighted buyer [34, 71]. ", "page_idx": 4}, {"type": "text", "text": "Second, the proof of Theorem 3.1 and 3.2 relies on solving the challenge that the platform can only observe equilibrium prices and quantities. The novel approach we take here is to utilize the service fee price as an instrumental variable (reflected in Algorithm 4). Despite the fact that the service fees as actions are correlated, we prove they provide valid estimates of the true coefficients $(\\beta_{0},\\beta_{1})$ , which is rigorously shown in Lemma 3.1 (in the lemma only $\\beta_{1}$ is analyzed; a similar result also holds for $\\beta_{0}$ ). ", "page_idx": 5}, {"type": "text", "text": "Lemma 3.1. Let ${\\widehat{\\beta}}_{1t}$ be the estimated slope after round $t$ . Then, under Assumptions 2.1 and 2.2, with high probability,  it holds that ", "page_idx": 5}, {"type": "equation", "text": "$$\n|\\widehat{\\beta}_{1t}-\\beta_{1}|\\lesssim\\mathcal{O}(\\frac{\\sqrt{\\log\\log T}}{t^{\\frac{1}{4}}}+\\frac{1}{(1-\\gamma)\\sqrt{t}})\\,f o r\\,a l l\\,t\\in[T]\\,\\,w h e n\\,\\sigma_{S}=0,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "and ", "page_idx": 5}, {"type": "equation", "text": "$$\n|\\widehat{\\beta}_{1t}-\\beta_{1}|\\lesssim\\mathcal{O}(\\frac{\\sqrt{\\log\\log T}}{\\sigma_{S}\\sqrt{t}}+\\frac{1}{\\sigma_{S}^{2}(1-\\gamma)t})\\,f o r\\,a l l\\,t\\in[T]\\,\\,w h e n\\,\\sigma_{S}>0.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Now let\u2019s discuss the magnitude of the artificially added noise (see Algorithm 2) \u2014 which serves as an exploration step to learn demand information. Combined with our previous discussion, we will present our main theorems in this section. We differentiate between two cases: $\\sigma_{S}=0$ and $\\sigma_{S}>0$ . ", "page_idx": 5}, {"type": "text", "text": "3.1 $\\sigma_{S}=0{\\mathrel{:}}\\,{\\tilde{\\mathcal{O}}}({\\sqrt{T}})\\,\\mathbf{Regret}$ ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We first consider the case when there is no supply randomness. At the beginning of each episode $m$ , we add a $\\mathcal{O}(1)$ noise to the service fee. Then, we decay the magnitude of noise variance at an inverse square root rate. There are two key points when implementing the algorithm. ", "page_idx": 5}, {"type": "text", "text": "First, there is a trade-off between utilizing early data and recent data \u2014 the more data we use, the better estimate we will have, but the early data may have bad data-generating processes which may cause extra nuisances. Therefore, we choose only to use data from the last episode, whose length is roughly half of all available data and we reset the magnitude of noise as long as we start a new episode to ensure sufficient exploration. ", "page_idx": 5}, {"type": "text", "text": "Second, there is a trade-off between adding larger and smaller randomness \u2014 the more randomness we add to the service fee, the more accurate estimate we will obtain, whereas higher noise leads to larger revenue loss. In our design, in episode $m$ , the variance level of noise added is designed to be $\\tilde{\\mathcal{O}(\\frac{1}{\\sqrt{2^{m}}})}$ on average where $2^{m}$ is the length of the episode. It decays at a relatively slow rate, aiming at exploration without losing much exploitation. We note that a special case of $\\sigma_{S}=0$ is when the supply curve coincides with the $Q$ -axis $(P_{S t}(Q)=0)$ ) and the buyer has no strategic behavior $(\\gamma=0)$ ). This degenerates to the standard dynamic pricing problem [44] where a $\\widetilde{\\mathcal{O}}(\\sqrt{T})$ regret holds. The following theorem sho\u221aws that with the additional features in our model (equ ilibrium observation, strategic behavior), an $\\widetilde{\\mathcal{O}}(\\sqrt{T})$ regret bound still holds. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.1 $(\\sigma_{S}=0)$ ). Under Assumptions 2.1 and 2.2, for any fix\u221aed failure probability $\\iota\\in(0,1)$ , with probability at least $1\\,-\\,\\iota_{!}$ , Algorithm $^{\\,l}$ achieves at most $\\begin{array}{r}{\\mathcal{O}(\\sqrt{T}\\log(\\frac{\\log T}{\\iota})+\\frac{\\log T}{(1-\\gamma)^{2}})}\\end{array}$ (1\u2212\u03b3)2 ) regret against any buyer whose discount rate $\\gamma\\in[0,1)$ when supply doesn\u2019t have noise, i.e. $\\sigma_{S}^{2}=0$ . Here, $O(\\cdot)$ hides only absolute constants. ", "page_idx": 5}, {"type": "text", "text": "3.2 $\\sigma_{S}>0$ : Noise Helps Learning ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We then consider the case when there is supply randomness. In contrast to Section 3.1, here no random noise is injected into the empirical optimal action $\\widehat{a}^{*}$ (see Algorithm 2). The following theorem shows that noise helps learning \u2014 an $\\widetilde{\\mathcal{O}}(1)$ regret is obtainable. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.2 $(\\sigma_{S}>0)$ ). Under Assumptions 2.1 and 2.2, for any fixed failure probability $\\iota\\in(0,1)$ , with probability at least 1 \u2212\u03b9, Algorithm 1 achieves at most O( log T lo\u03c3g2S( log\u03b9 T) $\\begin{array}{r}{{\\mathcal O}(\\frac{\\log T\\log(\\frac{\\log T}{\\iota})}{\\sigma_{S}^{2}}+\\frac{\\log T}{\\sigma_{S}^{2}(1-\\gamma)})}\\end{array}$ regret against any buyer whose discount rate $\\gamma\\in[0,1)$ . Here, $O(\\cdot)$ hides only absolute constants. ", "page_idx": 5}, {"type": "text", "text": "The regret upper bound in Theorem 3.2 contains two parts. The first ${\\mathcal{O}}(\\log T)$ term is the main regret incurred by learning the demand curve \u2014 which illustrates how noise helps learning. Although the platform can only observe equilibrium prices and quantities, the extra randomness in the supply curve automatically generates exploration and pushes the empirical optimal action $\\widehat{a}^{*}$ from Algorithm 2 to vibrate aroun\u221ad some intrinsic number. As a result, the estimation of $\\beta$ gene r ates a fast convergence rate of $\\mathcal{O}(1/\\sqrt{t})$ (see Lemma 3.1) even if no active exploration is presented. As a comparison, in Section 3.1 when the supply curve is deterministic, the problem becomes \u201cdegenerate\u201d to some extent. The equilibrium prices on the supplier side and equilibrium quantities lie on one line (the fixed sup\u221aply curve) in the 2-dimensional space, which forces us to do active exploration that leads to a $\\mathcal{O}(\\sqrt{T})$ regret. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "The second term is the extra regret due to the strategic behavior of the buyer. Golrezaei et al. [37] achieves an extra (lolgo2g(1/T\u03b3)) regret in repeated auction pricing when the discount rate is approaching one. Since $\\log(1/\\gamma)\\leq2(1-\\gamma)$ for $\\gamma\\in[0,1)$ , we obtain better results in both the order of $\\log T$ and order of $\\textstyle{\\frac{1}{1-\\gamma}}$ . We note that since the implementation of Algorithm 1 doesn\u2019t depend on the buyer\u2019s discount rate $\\gamma$ , it can achieve ${\\mathcal{O}}(\\log T\\log\\log T)$ regret even if the discount rates are changing in different rounds \u2014 as long as the discount rate $\\gamma_{t}$ has a uniform upper bound $\\bar{\\gamma}$ and $\\bar{\\gamma}<1$ , which is widely observed in the real-world market, especially online advertising [31], Algorithm 1 remains good permanence. ", "page_idx": 6}, {"type": "text", "text": "From Theorems 3.1 and 3.2, we know the key to achieve $\\widetilde O(1)$ regret is the internal randomness of the supply curve. Therefore, we have the following coroll ary. ", "page_idx": 6}, {"type": "text", "text": "Corollary 3.2. If the platform needs to set a uniform service fee in multi-markets, such as Order Processing Fee on Ticketmaster [17], the demand curve goes to ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\vec{P}_{D t}=\\vec{\\beta}_{0}+\\vec{\\beta}_{1}\\otimes\\vec{Q}+\\vec{\\epsilon}_{D t},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\otimes$ is the Kronecker product of two matrices. Then, as long as in one market, the corresponding supply has internal randomness, Algorithm $^{\\,l}$ can achieve $\\widetilde O(1)$ regret by utilizing $a_{t}$ as an instrument for all markets. Otherwise, it will suffer $\\widetilde{\\mathcal{O}}(\\sqrt{T})$ regret. ", "page_idx": 6}, {"type": "text", "text": "4 Regret Lower Bounds: Phase Transition ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we present results on regret lower bounds. We will focus on the case when $\\gamma=0$ and investigate how the regret intrinsically scales as a function of $T$ as well as $\\sigma_{S}$ . We will show that there exhibits a phase transition phenomenon with respect to $T$ and an inverse square law with respect to $\\sigma_{S}$ . We first present a result when there is no supply randomness. ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.1. Given any time h\u221aorizon $T$ , when the supply doesn\u2019t have noise, the worst-case expected regret is lower bounded by $\\Omega({\\sqrt{T}})$ . In $\\Omega(\\cdot)$ we are hiding a constant term irrelevant with $\\sigma_{S}$ and $T$ . ", "page_idx": 6}, {"type": "text", "text": "Next, we provide a lower bound when $\\sigma_{S}$ can take any general positive number. ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.2. Given any time horizon $T$ and the supply noise level $\\sigma_{S}$ , the worst-case expected regret is lower bounded by ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\Omega\\left(\\sqrt T\\wedge\\frac{\\log T}{\\sigma_{S}^{2}(1+\\log_{+}(1/\\sigma_{S}))}\\right),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\log_{+}(\\cdot)=\\operatorname*{max}\\{0,\\log(\\cdot)\\}$ . In $\\Omega(\\cdot)$ we are hiding a constant term irrelevant with $\\sigma_{S}$ and $T$ . ", "page_idx": 6}, {"type": "text", "text": "The proof of Theorem 4.1 depends on constructing two hard-to-differentiate instances such that the demand curves deviate with each other while the supply curves remain the same. The proof of Theorem 4.2 relies on the multivariate Van Trees inequality [36, 44]. An important step in both of the proof is when considering the magnitude of demand noise $\\sigma_{D}$ , it should be dependent on $\\alpha_{1}$ and $\\beta_{1}$ . The reason is that if the noise magnitude is irrelevant with the slope parameters, the standard deviation of equilibrium prices and quantities observed by the platform can provide additional information on the true parameters \u2014 this will make it possible for a policy to learn more quickly by \u201ccheating\u201d. Only when the noise magnitude posits a delicate dependence on the true parameters, the equilibrium prices/quantities exhibit constant deviation across all instances without information leakage. ", "page_idx": 6}, {"type": "text", "text": "Now we give some remarks for the theorems. Theorem 4.1 states that the regret order in Theorem 3.1 is tight. It also implicitly provides some intuition for our choice of the magnitude of the artificially introduced noise in our algorithm design. The total variance of noise we add is proportional to $\\sqrt{T}-$ this shall be the largest noise magnitude we shall use for sufficient exploration to match the $\\Omega({\\sqrt{T}})$ lower bound. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Theorem 4.2 shows that if $\\sigma_{S}>0$ is a constant irrelevant with $T$ , then our regret upper bound in Theorem 3.2 is tight up to a $\\log T$ factor. Moreo\u221aver, Theorem 4.2 gives a valid lower bou\u221and if $\\sigma_{S}$ is entangled with $T$ . To be\u221a precise, if $\\sigma_{S}^{2}\\lesssim\\mathcal{O}(1/\\sqrt{T})$ , then the best we can\u221a hope is an $\\widetilde{\\mathcal{O}}(\\sqrt{T})$ regret. Only when $\\sigma_{S}^{2}\\gtrsim\\omega(1/\\sqrt{T})$ can we expect a regret bound better than $\\widetilde{\\mathcal{O}}(\\sqrt{T})$ . ", "page_idx": 7}, {"type": "text", "text": "Readers may immediately notice that there is a gap between our re\u221agret upper bounds (see Theorem 3.2) and the regret lower bound (see Theorem 4.2) when $\\sigma_{S}^{2}\\lesssim O(1/\\sqrt{T})$ . It raises the following question whether we can achieve the same regret upper bounds without knowing $\\sigma_{S}$ in advance. The answer is YES! We have the following theorem. ", "page_idx": 7}, {"type": "text", "text": "Theorem 4.3. Under Assump\u221ations 2.1 and 2.2, there exists an algorithm, e.g., Algorithm 3, whose expected regret is at most $\\widetilde{\\mathcal{O}}(\\sqrt{T}\\wedge\\sigma_{S}^{-2})$ against any buyer with discount rate $\\gamma\\in[0,1)$ . Here, $\\widetilde O(\\cdot)$ hides only constants and l ogarithmic terms. ", "page_idx": 7}, {"type": "text", "text": "Algorithm 3 AAaI (Adaptive Action-as-Instruments) Algorithm. Input: $T$ . for $t=1$ to $T_{0}\\approx\\Theta(\\log T)$ do Observe market randomness $\\epsilon_{S t}$ or supply intercept $\\alpha_{0}+\\epsilon_{S t}$ . end for Hypothesis Test: $\\mathbb H_{0}:\\sigma_{S}^{2}\\lesssim\\mathcal O(\\frac{1}{\\sqrt{T}})$ and $\\mathbb{H}_{1}:\\sigma_{S}^{2}\\gtrsim\\Omega(\\frac{1}{\\sqrt{T}})$ , denoting the result as $\\mathbb{H}$ . Conduct $\\mathtt{i a I}(T-T_{0})$ with $\\mathsf{A c t-H T}(\\mathbb{H},\\cdot,\\cdot)$ (Algorithm 5 in Appendix C.2) replacing $\\mathtt{A c t}(\\cdot,\\cdot)$ . ", "page_idx": 7}, {"type": "text", "text": "In Algorithm 3, Act-HT is essentially a generalized version of Act: if $\\mathbb{H}_{0}$ holds, we treat $\\sigma_{S}$ as if it is 0; if not, we treat $\\sigma_{S}$ as it is. When $T$ is unknown or infinite, we can leverage the well-known doubling trick [9, 16] to achieve the same order of regret. From Theorem 4.3, we know that when the supply randomness is small enough, namely, $\\sigma_{S}^{2}\\lesssim O(1/\\sqrt{T})$ , the expected regr\u221aet has a fixed rate $\\widetilde\\Theta(\\sqrt{T})$ , whereas when the supply randomness is large enough, namely $\\sigma_{S}^{2}\\gtrsim\\Omega(1/\\sqrt{T})$ , the expected regret is inversely proportional with respect to $\\sigma_{S}^{2}$ \u2014 an inverse square law ignoring con\u221astants and logarithmic terms. Therefore, there exists an essential phase transition when $\\sigma_{S}^{2}\\approx\\Theta(1/\\sqrt{T})$ (see Figure 4) \u2014 this tells us that we should take market randomness into consideration whenever solving pricing problems. ", "page_idx": 7}, {"type": "text", "text": "5 Numerical Study ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we conduct two simulation experiments. The goal is to test the performance of our algorithms as well as numerically demonstrate the phase transition phenomenon. We provide detailed experimental descriptions in Appendix G. ", "page_idx": 7}, {"type": "text", "text": "In the first experiment, we consider regret attained in Algorithm 1 under two scenarios. Here, we set $T=10^{5}$ . We replicate 10 times in each setting and draw the average regrets and their $95\\%$ confidence regions. We consider both constant $\\sigma_{S}^{2}$ and zero randomness in Figure 2. It\u2019s obvious that when $\\sigma_{S}^{2}\\approx\\bar{\\Theta}(1)$ , the growth rate of regret is significantly smaller than the growth rate when $\\sigma_{S}^{2}=0$ . The first regret is slightly lar\u221ager than 200 and the second one is more than 1200, validating the respective ${\\mathcal{O}}(\\log^{2}T)$ and $\\mathcal{O}(\\bar{\\sqrt{T}}\\log T)$ expected regrets. Numerically, when $\\sigma_{S}^{2}=1$ , we find that the regrets when $T=20000$ , 40000, 60000, 80000, 100000 $(\\log T=9.90,10.60,\\bar{1}1.00,11.29,11.51)$ are 220, 230, 234, 236, and 237, respectively. These points are even slightly sublinear. So, the actual performance is even better than the theoretical bound. In addition, when there is no randomness in the supply, i.e., $\\sigma_{S}^{2}=0$ , we have $\\log({\\mathrm{Regret}})=6.43$ , 6.75, 6.95, 7.10, 7.17, respectively. The estimated slope by linear regression is 0.47, testifying our regret bound. ", "page_idx": 7}, {"type": "text", "text": "Moreover, we increase the number of trajectories to 100 and observe that the bandwidth of the corresponding confidence region significantly decreases (cf. Figure 3 (top, right)). Additionally, we test the regret under different $\\sigma_{S}^{2}$ values, say 0.5, 1, 1.5 and 2. We notice that the larger the $\\sigma_{S}^{\\bar{2}}$ , the smaller the regret, which confirms our theoretical results (cf. Figure 3). ", "page_idx": 7}, {"type": "image", "img_path": "Tnl2K6Iz9j/tmp/e0fd3f0f935017b9b7ac5ae82257ae04a510f0f930b9648a1b65c15c0c13b4fb.jpg", "img_caption": ["Figure 2: $95\\%$ confidence region of regret of Algorithm 1 over 10 trajectories: $\\sigma_{S}^{2}=1$ (left) and $\\sigma_{S}^{2}=0$ (right). "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "Tnl2K6Iz9j/tmp/0453aff50b987150f831010efcb030d6c615f239efad27e602eb0626023dbc5f.jpg", "img_caption": ["Figure 3: $95\\%$ confidence region of regret of Algorithm 1 over 100 trajectories: $\\sigma_{S}^{2}=0.5,1,1.5,2$ (top to bottom, left to right). "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Then, we conduct the second experiment examining the dependency of regret on supply randomness $\\sigma_{S}^{2}$ . We set $T=10^{4}$ and range $\\overline{{\\sigma_{S}^{2}}}$ from 0.001 to 1. We replicate the simulation 20 times and use quantile statistics to enhance the robustness (see raw results in Figure 5, i.e., blue points). There are around 100 points cho\u221aosing $\\mathbb{H}_{0}$ and we can observe the phase \u221atransition when $\\sigma_{S}^{2}\\approx0.1$ . Notice that Algorithm 3 has a $\\mathcal{O}(\\sqrt{T}\\log T)$ expected regret for $\\mathbb{H}_{0}$ but ${\\mathcal{O}}({\\sqrt{T}}\\log^{2}T)$ for $\\mathbb{H}_{1}$ when $\\sigma_{S}^{2}\\approx\\Theta(\\frac{1}{\\sqrt{T}})$ (cf. Appendix F.5). This additional $\\log T$ factor explains why there is increasing fluctuation in regret near the phase transition point, as Algorithm 3 engages in a mixture of applying $\\mathbb{H}_{0}$ and $\\mathbb{H}_{1}$ . Finally, we employ locally weighted scatterplot smoothing (LOWESS) [25] to approximate regret for each level of randomness $\\sigma_{S}^{2}$ , with the fitting depicted by a red line. We notice that regret first reaches a plateau and keeps nearly constant and then gradually decreases, testifying Theorem 4.3. ", "page_idx": 8}, {"type": "image", "img_path": "Tnl2K6Iz9j/tmp/40d2f78f5462e6342991606f33832e308ec96c4170c46ecd505feb4d9b33c623.jpg", "img_caption": ["Figure 4: Phase transition with supply randomness $\\sigma_{S}^{2}$ . Aggregating Theorems 4.1 to 4.3. "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "Tnl2K6Iz9j/tmp/9883ee2fc81d78ab8ee03db606b8770b024db08820158cb2851abd0a07e948b6.jpg", "img_caption": ["Figure 5: Phase transition in Algorithm 3 and its non-parametric local fitting. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we study the dynamic pricing problem for third-party platforms. Our model incorporates practical challenges involving lack of demand knowledge, limited observation of equilibria, and buyer strategic behavior. We design an effective policy that obtains optimal performance guarantees to address the challenges by injecting carefully chosen randomness, using non-i.i.d. actions as instruments, and forcing a low-switching design. Specifically, in the case of supply fluctuations\u221a, we achieve a regret upper bound of $\\widetilde O(1)$ , and when supply is fixed, we achieve a regret bound of $\\widetilde{\\mathcal{O}}(\\sqrt{T})$ , both of which match the information-theoretical lower bounds. Additionally, we demonstrate the relationship between regret and supply randomness, and provide their optimal dependency and phase transition points. ", "page_idx": 9}, {"type": "text", "text": "Questions arise for future explorations. What is the dependence of regret on the discount rate $\\gamma?$ Our conjecture is that a discount rate strictly less than 1 will introduce inevitable $\\textstyle\\Omega\\bigl(\\frac{1}{1-\\gamma}\\bigr)$ regret universally, meaning the regret upper bound in Theorem 3.2 is optimal with respect to $\\gamma$ but the bound in Theorem 3.1 may not. Unfortunately, the analysis can be very challenging, which we leave as an open question. Furthermore, one future work is to extend the linear model to more complex models and investigate whether the insights in this paper (e.g., phase transition) still hold. Despite the simplicity of our model, we hope our results offer valuable insights into solving general service fee pricing problems for third-party platforms. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The authors would like to thank the reviewers for their valuable comments and suggestions, which have greatly improved the article. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Rui Ai, Boxiang Lyu, Zhaoran Wang, Zhuoran Yang, and Michael I Jordan. A reinforcement learning approach in multi-phase second-price auction design. arXiv preprint arXiv:2210.10278, 2022.   \n[2] Kareem Amin, Afshin Rostamizadeh, and Umar Syed. Learning prices for repeated auctions with strategic buyers. Advances in neural information processing systems, 26, 2013.   \n[3] Kareem Amin, Afshin Rostamizadeh, and Umar Syed. Repeated contextual auctions with strategic buyers. Advances in Neural Information Processing Systems, 27, 2014.   \n[4] Joshua D Angrist and Guido W Imbens. Two-stage least squares estimation of average causal effects in models with variable treatment intensity. Journal of the American statistical Association, 90(430):431\u2013442, 1995. [5] Joshua D Angrist and Alan B Krueger. Instrumental variables and the search for identification: From supply and demand to natural experiments. Journal of Economic perspectives, 15(4): 69\u201385, 2001. [6] Bradley M Appelhans, Christy C Tangney, Simone A French, Melissa M Crane, and Yamin Wang. Delay discounting and household food purchasing decisions: The shopper study. Health Psychology, 38(4):334, 2019.   \n[7] Victor F Araman and Ren\u00e9 Caldentey. Dynamic pricing for nonperishable products with demand learning. Operations research, 57(5):1169\u20131188, 2009. [8] William Ascher. Long-term strategy for sustainable development: strategies to promote farsighted action. Sustainability Science, 1:15\u201322, 2006. [9] Peter Auer, Nicolo Cesa-Bianchi, and Claudio Gentile. Adaptive and self-confident on-line learning algorithms. Journal of Computer and System Sciences, 64(1):48\u201375, 2002.   \n[10] Yossi Aviv and Amit Pazgal. Optimal pricing of seasonal products in the presence of forwardlooking consumers. Manufacturing & service operations management, 10(3):339\u2013359, 2008.   \n[11] Yossi Aviv, Mike Mingcheng Wei, and Fuqiang Zhang. Responsive pricing of fashion products: The effects of demand learning and strategic consumer behavior. Management Science, 65(7): 2982\u20133000, 2019.   \n[12] Badi H Baltagi and Dan Levin. Estimating dynamic demand for cigarettes using panel data: the effects of bootlegging, taxation and advertising reconsidered. The Review of Economics and Statistics, pages 148\u2013155, 1986.   \n[13] Gah-Yi Ban and N Bora Keskin. Personalized dynamic pricing with machine learning: Highdimensional features and heterogeneous elasticity. Management Science, 67(9):5549\u20135568, 2021.   \n[14] Omar Besbes and Assaf Zeevi. On the minimax complexity of pricing in a changing environment. Operations research, 59(1):66\u201379, 2011.   \n[15] Omar Besbes and Assaf Zeevi. On the (surprising) sufficiency of linear models for dynamic pricing with demand learning. Management Science, 61(4):723\u2013739, 2015.   \n[16] Lilian Besson and Emilie Kaufmann. What doubling tricks can and can\u2019t do for multi-armed bandits. arXiv preprint arXiv:1803.06971, 2018.   \n[17] Milica Bosnjak. \u201cget your tickets!\u201d from a legitimate source: Primary and secondary ticketing markets in nevada. UNLV Gaming Law Journal, 11(2):6, 2021.   \n[18] Josef Broder and Paat Rusmevichientong. Dynamic pricing under a general parametric choice model. Operations Research, 60(4):965\u2013980, 2012.   \n[19] Jinzhi Bu, David Simchi-Levi, and Yunzong Xu. Online pricing with offline data: Phase transition and inverse square law. In International Conference on Machine Learning, pages 1202\u20131210. PMLR, 2020.   \n[20] Felipe Caro and J\u00e9r\u00e9mie Gallien. Dynamic assortment with demand learning for seasonal consumer goods. Management science, 53(2):276\u2013292, 2007.   \n[21] Alexandre X Carvalho and Martin L Puterman. Learning and pricing in an internet environment with binomial demands. Journal of Revenue and Pricing Management, 3:320\u2013336, 2005.   \n[22] G. Casella and R.L. Berger. Statistical Inference. Duxbury advanced series in statistics and decision sciences. Thomson Learning, 2002. ISBN 9780534243128. URL https://books. google.com/books?id=0x_vAAAAMAAJ.   \n[23] Shuxiao Chen and Bo Zhang. Estimating and improving dynamic treatment regimes with a time-varying instrumental variable. Journal of the Royal Statistical Society Series B: Statistical Methodology, 85(2):427\u2013453, 2023.   \n[24] Xiaohong Chen and Zhengling Qi. On well-posedness and minimax optimal rates of nonparametric $\\mathbf{q}$ -function estimation in off-policy evaluation. In International Conference on Machine Learning, pages 3558\u20133582. PMLR, 2022.   \n[25] William S Cleveland. Robust locally weighted regression and smoothing scatterplots. Journal of the American statistical association, 74(368):829\u2013836, 1979.   \n[26] Eric Cope. Bayesian strategies for dynamic pricing in e-commerce. Naval Research Logistics (NRL), 54(3):265\u2013281, 2007.   \n[27] John G Cross. A theory of the bargaining process. The American Economic Review, 55(1/2): 67\u201394, 1965.   \n[28] Arnoud V Den Boer and Bert Zwart. Simultaneously learning and optimizing using controlled variance pricing. Management science, 60(3):770\u2013783, 2014.   \n[29] Yuan Deng, S\u00e9bastien Lahaie, and Vahab Mirrokni. Robust pricing in dynamic mechanism design. In International Conference on Machine Learning, pages 2494\u20132503. PMLR, 2020.   \n[30] Alexey Drutsa. Horizon-independent optimal pricing in repeated auctions with truthful and strategic buyers. In Proceedings of the 26th International Conference on World Wide Web, pages 33\u201342, 2017.   \n[31] Alexey Drutsa. Reserve pricing in repeated second-price auctions with strategic bidders. In International Conference on Machine Learning, pages 2678\u20132689. PMLR, 2020.   \n[32] Alessandro Epasto, Mohammad Mahdian, Vahab Mirrokni, and Song Zuo. Incentive-aware learning for large markets. In Proceedings of the 2018 World Wide Web Conference, pages 1369\u20131378, 2018.   \n[33] Pnina Feldman, Andrew E Frazelle, and Robert Swinney. Service delivery platforms: Pricing and revenue implications. Available at SSRN, 3258739, 2018.   \n[34] Minbo Gao, Tianle Xie, Simon S Du, and Lin F Yang. A provably efficient algorithm for linear markov decision process with low switching cost. arXiv preprint arXiv:2101.00494, 2021.   \n[35] Ming Gao. Platform pricing in mixed two-sided markets. International Economic Review, 59 (3):1103\u20131129, 2018.   \n[36] Richard D Gill and Boris Y Levit. Applications of the van trees inequality: a bayesian cram\u00e9r-rao bound. Bernoulli, pages 59\u201379, 1995.   \n[37] Negin Golrezaei, Adel Javanmard, and Vahab Mirrokni. Dynamic incentive-aware learning: Robust pricing in contextual auctions. Advances in Neural Information Processing Systems, 32, 2019.   \n[38] Negin Golrezaei, Patrick Jaillet, and Jason Cheuk Nam Liang. Incentive-aware contextual pricing with non-parametric market noise. In International Conference on Artificial Intelligence and Statistics, pages 9331\u20139361. PMLR, 2023.   \n[39] Harish Guda and Upender Subramanian. Your uber is arriving: Managing on-demand workers through surge pricing, forecast communication, and worker incentives. Management Science, 65(5):1995\u20132014, 2019.   \n[40] Andrei Hagiu. Two-sided platforms: Product variety and pricing structures. Journal of Economics & Management Strategy, 18(4):1011\u20131043, 2009.   \n[41] Oliver D Hart and Jean Tirole. Contract renegotiation and coasian dynamics. The Review of Economic Studies, 55(4):509\u2013540, 1988.   \n[42] Conghui Jin. Research on consumers\u2019 delayed purchase. In 2018 2nd International Conference on Education Science and Economic Management (ICESEM 2018), pages 912\u2013915. Atlantis Press, 2018.   \n[43] Yash Kanoria and Hamid Nazerzadeh. Dynamic reserve prices for repeated auctions: Learning from bids. arXiv preprint arXiv:2002.07331, 2020.   \n[44] N Bora Keskin and Assaf Zeevi. Dynamic pricing with an unknown demand model: Asymptotically optimal semi-myopic policies. Operations research, 62(5):1142\u20131167, 2014.   \n[45] L.R. Klein. A Textbook of Econometrics. Row, Peterson, 1953. URL https://books.google. com/books?id $=$ uzwiAAAAMAAJ.   \n[46] Robert Kleinberg and Tom Leighton. The value of knowing a demand curve: Bounds on regret for online posted-price auctions. In 44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings., pages 594\u2013605. IEEE, 2003.   \n[47] Luofeng Liao, Zuyue Fu, Zhuoran Yang, Yixin Wang, Mladen Kolar, and Zhaoran Wang. Instrumental variable value iteration for causal offline reinforcement learning. arXiv preprint arXiv:2102.09907, 2021.   \n[48] Kyle Y Lin. Dynamic pricing with real-time demand learning. European Journal of Operational Research, 174(1):522\u2013538, 2006.   \n[49] Mei Lin, Xiajun Amy Pan, and Quan Zheng. Platform pricing with strategic buyers: The impact of future production cost. Production and Operations Management, 29(5):1122\u20131144, 2020.   \n[50] Miguel Sousa Lobo and Stephen Boyd. Pricing and learning with uncertain demand. In INFORMS revenue management conference, pages 63\u201364. Citeseer, 2003.   \n[51] H Brendan McMahan, Gary Holt, David Sculley, Michael Young, Dietmar Ebner, Julian Grady, Lan Nie, Todd Phillips, Eugene Davydov, Daniel Golovin, et al. Ad click prediction: a view from the trenches. In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 1222\u20131230, 2013.   \n[52] Franco Modigliani and Shi Larry Cao. The chinese saving puzzle and the life-cycle hypothesis. Journal of economic literature, 42(1):145\u2013170, 2004.   \n[53] Mehryar Mohri and Andres Munoz Medina. Learning theory and algorithms for revenue optimization in second price auctions with reserve. In International conference on machine learning, pages 262\u2013270. PMLR, 2014.   \n[54] Mehryar Mohri and Andres Munoz. Revenue optimization against strategic buyers. Advances in Neural Information Processing Systems, 28, 2015.   \n[55] Roger B Myerson. Optimal auction design. Mathematics of operations research, 6(1):58\u201373, 1981.   \n[56] Mila Nambiar, David Simchi-Levi, and He Wang. Dynamic learning and pricing with model misspecification. Management Science, 65(11):4980\u20135000, 2019.   \n[57] Maurice Obstfeld and Kenneth Rogoff. Foundations of international macroeconomics. MIT press, 1996.   \n[58] ROTT Peter et al. Personalised pricing (at a glance-study in focus). 2022.   \n[59] Amil Petrin and Kenneth Train. A control function approach to endogeneity in consumer choice models. Journal of marketing research, 47(1):3\u201313, 2010.   \n[60] Robert Phillips, A Serdar \u00b8Sim\u00b8sek, and Garrett Van Ryzin. The effectiveness of field price discretion: Empirical evidence from auto lending. Management Science, 61(8):1741\u20131759, 2015.   \n[61] Sheng Qiang and Mohsen Bayati. Dynamic pricing with demand covariates. arXiv preprint arXiv:1604.07463, 2016.   \n[62] Frank Plumpton Ramsey. A mathematical theory of saving. The economic journal, 38(152): 543\u2013559, 1928. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "[63] Tim Roughgarden. Algorithmic game theory. Communications of the ACM, 53(7):78\u201386, 2010. ", "page_idx": 13}, {"type": "text", "text": "[64] Stephen W Salant. When is inducing self-selection suboptimal for a monopolist? The Quarterly Journal of Economics, 104(2):391\u2013397, 1989.   \n[65] Rahul Singh. Kernel methods for unobserved confounding: Negative controls, proxies, and instruments. arXiv preprint arXiv:2012.10315, 2020.   \n[66] Stacy Sirmans, David Macpherson, and Emily Zietz. The composition of hedonic pricing models. Journal of real estate literature, 13(1):1\u201344, 2005.   \n[67] James H Stock and Mark W Watson. Introduction to econometrics. Pearson, 2020.   \n[68] Alexandre B Tsybakov. Introduction to nonparametric estimation, 2009. URL https://doi. org/10.1007/b13794. Revised and extended from the, 9(10), 2004.   \n[69] Masatoshi Uehara, Haruka Kiyohara, Andrew Bennett, Victor Chernozhukov, Nan Jiang, Nathan Kallus, Chengchun Shi, and Wen Sun. Future-dependent value-based off-policy evaluation in pomdps. Advances in Neural Information Processing Systems, 36, 2024.   \n[70] Martin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cambridge university press, 2019.   \n[71] Tianhao Wang, Dongruo Zhou, and Quanquan Gu. Provably efficient reinforcement learning with linear function approximation under adaptivity constraints. Advances in Neural Information Processing Systems, 34:13524\u201313536, 2021.   \n[72] Yinfei Xu, Yafei Zu, and Hui Zhang. Optimal inter-organization control of collaborative advertising with myopic and far-sighted behaviors. Entropy, 23(9):1144, 2021.   \n[73] Mengxin Yu, Zhuoran Yang, and Jianqing Fan. Strategic decision-making in the presence of information asymmetry: Provably efficient rl with algorithmic instruments. arXiv preprint arXiv:2208.11040, 2022.   \n[74] Feng Zhu and Zeyu Zheng. When demands evolve larger and noisier: Learning and earning in a growing environment. In International conference on machine learning, pages 11629\u201311638. PMLR, 2020. ", "page_idx": 13}, {"type": "text", "text": "A Literature Review ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "This work contributes to the theory of econometrics, demand learning, and pricing with strategic buyers. We summarize below three lines of existing literature pertinent to our work. ", "page_idx": 14}, {"type": "text", "text": "Instruments in Machine Learning. Instrumental variable is a powerful tool in econometrics [4, 5, 12]. Nambiar et al. [56] uses IV to learn the demand curve under model misspecification, inspired by Petrin and Train [59], Phillips et al. [60]. In recent years, a series of articles have emerged that use the IV method to estimate machine learning parameters. Yu et al. [73], Liao et al. [47] utilize instruments to learn a nearly-optimal policy via offline data. Chen and Qi [24], Uehara et al. [69] adopt a non-parametric instrumental variable framework to do off-policy evaluation. Chen and Zhang [23] considers a time-varying instrument and Singh [65] extends the kernel methods to scenarios with confounders. ", "page_idx": 14}, {"type": "text", "text": "Unlike existing literature, our paper first leverages endogenous proactive actions as IV in online learning problems, especially in pricing. Unlike offline data, the distribution of our instrument variable is not only non-identical but also even non-independent. Meanwhile, as we face a revenuemaximizing problem, we need to deal with the famous exploration-exploitation tradeoff. More efficient instruments may decrease the estimation error leading to higher future revenue, but usually suffer a larger short-term loss. Thus, prior research does not encompass our model and we examine the robustness of IV to market randomness with novelty. ", "page_idx": 14}, {"type": "text", "text": "Pricing with Strategic Buyers. There is a burgeoning amount of literature on pricing with strategic buyers. Amin et al. [2] first proves that no algorithm can achieve sublinear regret when buyers are as patient as sellers. Deng et al. [29] considers pricing in dynamic mechanism design with less patient buyers. It obtains sublinear regret in contextual auctions. Golrezaei et al. [37] study optimal reserve design problem original from Myerson [55] facing strategic bidders while Mohri and Munoz [54] considers a corresponding revenue optimization problem. Golrezaei et al. [38] considers pricing with strategic buyers under non-parametric market noise and Ai et al. [1] extends to Markov decision process (MDP) pricing models. Mohri and Medina [53], Kanoria and Nazerzadeh [43], Epasto et al. [32], Amin et al. [3] also study such issues under different information structures and depict different scenarios in real markets. ", "page_idx": 14}, {"type": "text", "text": "The difficulty in our model is that we can only observe equilibrium prices and quantities. Worst yet, there are confounders behind the information feedback. Therefore, the methods from previous literature cannot be directly applied to our model. Consequently, we explore a robust pricing framework originating from econometrics to address this issue. ", "page_idx": 14}, {"type": "text", "text": "Demand Learning under Uncertainty. Demand learning is a hot topic in microeconomics, management science, and operations research [14, 26, 46, 21, 7, 48, 20]. Lobo and Boyd [50] considers a linear demand model and invents a \u201cprice-dithering\u201d policy to add perturbation. Den Boer and Zwart [28] invents the controlled variance pricing idea and Broder and Rusmevichientong [18] scales exploration by adding $t^{-1/4}$ which has similar connotations to our approach. ", "page_idx": 14}, {"type": "text", "text": "Nambiar et al. [56] considers confounders in demand learning but doesn\u2019t involve strategic buyers. We extend high-level econometrics ideas behind it and design recipes against non-truthful buyers. We construct a robust pricing framework free of prior knowledge of both discount info and market randomness and attain optimal regret bounds across all settings. ", "page_idx": 14}, {"type": "text", "text": "B Discussion on Assumption 2.2 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We clarify the definition of $\\textstyle\\operatorname{Sur}_{t}$ first. Similar to the Ramsey model [62], there are three different types of sellers, namely, civilian-run enterprises, state-owned enterprises, and mixed-ownership enterprises [57]. For civilian-run enterprises, $100\\%$ of the profits belong to individuals, namely the representative buyer, so she aims to maximize the sum of producer surplus and consumer surplus. Then, it holds that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{Sur}_{t}=\\mathrm{Consumer~surplus~at~time~}t+\\mathrm{Producer~surplus~at~time~}t.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Meanwhile, for state-owned enterprises, their goal is to maximize their own scale. This is a common form of business organization, especially in developing countries, such as China (see Modigliani and Cao [52] for more information). Because the buyer cannot receive dividends from companies, she ", "page_idx": 14}, {"type": "text", "text": "will only maximize their consumer surplus, namely, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname{Sur}_{t}=\\operatorname{Consumer}{\\mathrm{surplus~at~time~}}t.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Finally, for mixed-ownership enterprises, a portion of the profits will be distributed to the buyer in the form of dividends. We use $\\alpha\\in(0,1)$ to denote the proportion. Therefore, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathrm{Sur}_{t}=\\mathrm{Consumer~surplus~at~time~}t+\\alpha*\\mathrm{Producer~surplus~at~time~}t.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Hence, we make Assumption 2.2 in our paper, all sellers are civilian-run enterprises. We point out that this assumption won\u2019t affect our results and regrets with the same order can be obtained under state-owned enterprises and mixed-ownership enterprises. Here, we use the definition of $\\textstyle\\operatorname{Sur}_{t}$ to detail Assumption 2.2 as Assumption B.1. ", "page_idx": 15}, {"type": "text", "text": "Assumption B.1. Without loss of generality, we assume that all sellers are civilian-run enterprises. Then, the strategic representative buyer aims to maximize her cumulative surplus aligned with the sum of the corresponding consumer surplus and provider surplus. In other words, $\\textstyle\\operatorname{Sur}_{t}$ has the form of ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathrm{Sur}_{t}=\\mathrm{Consumer~surplus~at~time~}t+\\mathrm{Producer~surplus~at~time~}t.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "C Omitted Details of Algorithm Implementations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "C.1 Omitted Details of Algorithm 1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we present the subroutine used in Algorithm 1. Specifically, Algorithm 4 details the coefficient estimation process. ", "page_idx": 15}, {"type": "table", "img_path": "Tnl2K6Iz9j/tmp/e9e6fe95eea1160d2091a72f0c75e2ef7c70f500631c2cd9e1cd7830cb0842f6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "C.2 Omitted Details of Algorithm 3 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We fill in the details of Algorithm 3 below. ", "page_idx": 15}, {"type": "table", "img_path": "Tnl2K6Iz9j/tmp/0cca6fa378b96d7f7e16f51e1380e28c81169b6db6090fa3dc06fb073f20e689.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "D Concentration Inequalities ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We now recall some inequalities [70] which bound the accumulated difference of a martingale and are extensively used in our proofs. ", "page_idx": 15}, {"type": "text", "text": "Lemma D.1 (Bernstein-type bound for a martingale difference sequence). Let $\\{(D_{k},\\mathcal{F}_{k})\\}_{k=1}^{\\infty}$ be a martingale difference sequence, and suppose that $\\mathbb{E}\\left[e^{\\lambda D_{k}}\\mid\\mathcal{F}_{k-1}\\right]\\leq e^{\\lambda^{2}v_{k}^{2}/2}$ almost surely for any $|\\lambda|<1/\\alpha_{k}$ . Then the following hold: ", "page_idx": 16}, {"type": "text", "text": "(a) The sum $\\sum_{k=1}^{n}D_{k}$ is sub-exponential with parameter tuple $\\left(\\sqrt{\\sum_{k=1}^{n}v_{k}^{2}},\\alpha_{*}\\right)$ , where $\\alpha_{*}:=$ $\\operatorname*{max}_{k=1,\\ldots,n}\\alpha_{k}$ . ", "page_idx": 16}, {"type": "text", "text": "$(b)$ The sum satisfies the concentration inequality ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\left|\\sum_{k=1}^{n}D_{k}\\right|\\geq t\\right]\\leq\\left\\{2e^{-\\frac{t^{2}}{2\\sum_{k=1}^{n}v_{k}^{2}}}\\;\\;\\;\\;\\;i f0\\leq t\\leq\\frac{\\sum_{k=1}^{n}v_{k}^{2}}{\\alpha_{*}}\\right.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Lemma D.2 (Hoeffding bound). Suppose that the variables $X_{i},i=1,\\ldots,n,$ , are independent, and $X_{i}$ has mean $\\mu_{i}$ and sub-Gaussian parameter $\\sigma_{i}$ . Then for all $t\\geq0$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\left|\\sum_{i=1}^{n}\\left(X_{i}-\\mu_{i}\\right)\\right|\\geq t\\right]\\leq2\\exp\\left\\{-\\frac{t^{2}}{2\\sum_{i=1}^{n}\\sigma_{i}^{2}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "E Omitted Proof in Section 3 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We first prove results in Section 3.2 and then prove results in Section 3.1 in leverage of some lemmas derived from the proof of Theorem 3.2. ", "page_idx": 16}, {"type": "text", "text": "E.1 Omitted Proof in Section 3.2 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Since we assume that all equilibrium prices and quantities are bounded, reflecting the buyer\u2019s budget constraint and sellers\u2019 production capacity, we use $\\bar{P}$ and $\\bar{Q}$ to represent their upper bounds respectively. Moreover, recall that the buyer is myopic, so the demand curve behaved $P_{D}^{\\prime}$ is the same as the real demand $P_{D}$ . ", "page_idx": 16}, {"type": "text", "text": "We first consider the situation in which the buyer is myopic as a warm-up. We have the following theorem. ", "page_idx": 16}, {"type": "text", "text": "Theorem E.1. Under Assumption 2.1, for any fixed failure probability $\\iota\\in(0,1)$ , with probability at least $1-\\iota$ , Algorithm 1 achieves at most $\\mathcal{O}\\big(\\frac{\\log T\\log(\\frac{\\log T}{\\iota})}{\\sigma_{S}^{2}}\\big)$ regret, where $O(\\cdot)$ hides only absolute constants when facing a myopic buyer. ", "page_idx": 16}, {"type": "text", "text": "E.1.1 Useful Facts for Proving Theorem E.1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Now, we state the following lemma for the planning problem first. ", "page_idx": 16}, {"type": "text", "text": "Lemma E.1. If $\\beta_{0}$ and $\\beta_{1}$ are common knowledge, the optimal service fee is $\\begin{array}{r}{a^{*}\\,=\\,\\frac{\\beta_{0}-{\\alpha_{0}}-\\epsilon_{S}}{2}}\\end{array}$ Meanwhile, the corresponding equilibrium quantity is $\\begin{array}{r}{Q^{e}=\\frac{\\beta_{0}-\\alpha_{0}-\\epsilon_{S}+2\\epsilon_{D}}{2\\left(\\alpha_{1}-\\beta_{1}\\right)}}\\end{array}$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. Since $P_{S}=\\alpha_{0}+\\alpha_{1}Q+\\epsilon_{S}$ and $P_{D}=\\beta_{0}+\\beta_{1}Q_{D}+\\epsilon_{D}$ , applying $P_{S}+a=P_{D}$ leads to $\\begin{array}{r}{Q^{e}=\\frac{\\beta_{0}-\\alpha_{0}-a+\\epsilon_{D}-\\epsilon_{S}}{\\alpha_{1}-\\beta_{1}}}\\end{array}$ . As the platform cannot observe $\\epsilon_{D}$ when setting the service fee $a$ , i.e. $\\epsilon_{D}$ is realized ex post, the expected revenue associated with $a$ is $\\begin{array}{r}{\\mathbb{E}a*Q^{e}=\\frac{(\\beta_{0}-\\alpha_{0}-a-\\epsilon_{S})a}{\\alpha_{1}-\\beta_{1}}}\\end{array}$ . It\u2019s maximized when ", "page_idx": 16}, {"type": "equation", "text": "$$\na^{*}=\\frac{\\beta_{0}-\\alpha_{0}-\\epsilon_{S}}{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "With some calculation, we know that the equilibrium quantity is ", "page_idx": 16}, {"type": "equation", "text": "$$\nQ^{e}=\\frac{\\beta_{0}-\\alpha_{0}-\\epsilon_{S}+2\\epsilon_{D}}{2(\\alpha_{1}-\\beta_{1})}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The reason why $\\epsilon_{S}$ appears explicitly is that the shock on the supple curve is ex-ante and observed by the platform. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "Thereafter, we\u2019d like to bound the estimation of $\\beta_{0}$ and $\\beta_{1}$ in Algorithm 4. We have the following proposition. We ignore the flooring in $\\lfloor\\log_{2}T\\rfloor$ throughout this section to avoid notation clutter. ", "page_idx": 17}, {"type": "text", "text": "Proposition E.2. Let ${\\widehat{\\beta}}_{1}$ be the estimated slope of Algorithm $^{4}$ after the m-th episode. Then, under Assumption 2.1, there  exists a constant $C_{1}$ such that with probability at least $1-6\\delta$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n|\\widehat{\\beta}_{1}-\\beta_{1}|\\leq\\frac{C_{1}\\sqrt{\\log\\log T}}{\\sigma_{S}\\sqrt{2^{m}}}\\,f o r\\,a l l\\,m\\in[\\log_{2}T],\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "then ", "page_idx": 17}, {"type": "equation", "text": "$$\n|\\widehat{\\beta}_{1t}-\\beta_{1}|\\lesssim\\mathcal{O}(\\frac{\\sqrt{\\log\\log T}}{\\sigma_{S}\\sqrt{t}})\\,f o r\\,a l l\\,t\\in[T],\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where subscript t represents which round. ", "page_idx": 17}, {"type": "text", "text": "Proof. After the $m$ -th episode, we know that we have data sets $\\mathcal{D}$ and $\\boldsymbol{\\mathcal{A}}$ with ${\\mathrm{Size}}(A)=2^{m}:=n$ .   \nThen, we need to prove that there exists a uniform constant C1 such that |\u03b2 1 \u2212\u03b21| \u2264C1\u03c3lSog\u221a lnog T. ", "page_idx": 17}, {"type": "text", "text": "Since during episode $m$ , we choose the service fee as $\\frac{\\widehat{\\beta}_{0}\\!-\\!\\alpha_{0}\\!-\\!\\epsilon_{S}}{2}$ , where ${\\widehat{\\beta}}_{0}$ is the latest estimate of $\\beta_{0}$ , i.e. the estimate after $m-1$ -th episode. With a little abuse of notation, we omit the subscript of it with ${\\widehat{\\beta}}_{0}$ along the road of proof without confusion. ", "page_idx": 17}, {"type": "text", "text": "When $\\mathrm{Size}(A)=n$ , it means that there are $n$ samples in the data set, that is to say, a trajectory of length $n$ . We use $\\mathbb{E}_{n}$ to represent the sample mean and $\\mathbb{E}$ to represent the population mean. With simple algebra, we know that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\widehat{\\beta}_{1}=\\frac{\\mathbb{E}_{n}[P^{e}(a-\\mathbb{E}_{n}a)]}{\\mathbb{E}_{n}[Q^{e}(a-\\mathbb{E}_{n}a)]}=\\frac{\\frac{1}{n}\\sum_{i=1}^{n}P_{i}^{e}(a_{i}-\\mathbb{E}_{n}a)}{\\frac{1}{n}\\sum_{i=1}^{n}Q_{i}^{e}(a_{i}-\\mathbb{E}_{n}a)}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Since $P^{e}=\\beta_{0}+\\beta_{1}Q^{e}+\\epsilon_{D}$ as $(P^{e},Q^{e})$ is on the demand curve, it holds that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{\\beta}_{1}=\\beta_{1}+\\frac{\\frac{1}{n}\\sum_{i=1}^{n}\\epsilon_{D i}(a_{i}-\\mathbb{E}_{n}a)}{\\frac{1}{n}\\sum_{i=1}^{n}Q_{i}^{e}(a_{i}-\\mathbb{E}_{n}a)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "First, we bound the numerator in=1 \u03f5Di(ai \u2212Ena) = \u221221n in=1 \u03f5Di\u03f5Si + 12En\u03f5SEn\u03f5D. We use $\\textstyle\\sum_{i}x_{i}$ to denote $\\sum_{i}\\epsilon_{D i}\\epsilon_{S i}$ for simplicity. ", "page_idx": 17}, {"type": "text", "text": "For $\\textstyle{\\frac{1}{n}}\\sum_{i}x_{i}$ , we leverage Lemma D.1 to give a Bernstein-type bound for the corresponding martingale difference sequence. ", "page_idx": 17}, {"type": "text", "text": "Lemma E.3. There exists constants $n_{1}$ and $\\nu$ such that with probability at least $1-\\delta$ , it holds that for any $n\\geq n_{1}$ where $n=2^{m}$ and $m\\in[\\log_{2}T]$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n|\\frac{1}{n}\\sum_{i=1}^{n}x_{i}|\\leq\\sqrt{\\frac{2\\nu^{2}\\log(\\frac{2\\log_{2}T}{\\delta})}{n}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. In our case, we know that $a_{i}$ depends on not only previous $\\epsilon_{D}$ and $\\epsilon_{S}$ , but also $\\epsilon_{S i}$ . However, it doesn\u2019t rely on $\\epsilon_{D i}$ . This conditional independence motivates us to use actions as nearly valid instruments. Similarly, $x_{i}$ has the same property. Therefore, by utilizing this independence, we know that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}[x_{i}\\,|\\,x_{1},...,x_{i-1}]=\\mathbb{E}[\\epsilon_{D i}]*\\mathbb{E}[\\epsilon_{S i}\\,|\\,x_{1},...,x_{i-1}]=0.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "As a result, $\\{x_{i}\\}_{i=1}^{n}$ becomes a martingale difference sequence and we use $\\mathcal{F}$ to denote associated filtration. Then, let\u2019s compute the moment-generating function (MGF). ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[e^{\\lambda x_{i}}\\,\\vert\\,\\mathcal{F}_{i-1}]=\\int\\exp(\\lambda\\epsilon_{S}\\epsilon_{D})\\frac{1}{\\sqrt{2\\pi}\\sigma_{S}}e^{-\\frac{\\epsilon_{S}^{2}}{2\\sigma_{S}^{2}}}\\frac{1}{\\sqrt{2\\pi}\\sigma_{D}}e^{-\\frac{\\epsilon_{D}^{2}}{2\\sigma_{D}^{2}}}d\\epsilon_{S}d\\epsilon_{D}}\\\\ &{\\qquad\\qquad=\\frac{1}{\\sqrt{1-\\lambda^{2}\\sigma_{S}^{2}\\sigma_{D}^{2}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "as long as |\u03bb| \u2264\u03c3S1\u03c3D . The first equation comes from the definition of $\\epsilon_{D}$ and $\\epsilon_{S}$ while the second equation comes from simple algebra. ", "page_idx": 18}, {"type": "text", "text": "Therefore, assuming $\\alpha=2\\sigma_{S}\\sigma_{D}$ , we know that for any $\\begin{array}{r}{|\\lambda|\\leq\\frac{1}{\\alpha}}\\end{array}$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}[e^{\\lambda x_{i}}\\,|\\,\\mathcal{F}_{i-1}]\\le\\frac{1}{\\sqrt{1-\\lambda^{2}\\sigma_{S}^{2}\\sigma_{D}^{2}}}\\le e^{\\lambda^{2}\\sigma_{S}^{2}\\sigma_{D}^{2}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then, assuming $\\nu=\\sqrt{2\\sigma_{S}^{2}\\sigma_{D}^{2}}\\,=\\sqrt{2}\\sigma_{S}\\sigma_{D}$ , it holds that $x_{i}$ is sub-exponential with parameters $(\\nu,\\alpha)$ . ", "page_idx": 18}, {"type": "text", "text": "With the help of Lemma D.1, we know that as long as n \u2265 2\u03b12 log(\u03bd 22 log\u03b42 T) , with probability at least $\\begin{array}{r}{1-\\frac{\\delta}{\\log_{2}T},|\\frac{1}{n}\\sum_{i=1}^{n}x_{i}|\\leq\\sqrt{\\frac{2\\nu^{2}\\log(\\frac{2\\log_{2}T}{\\delta})}{n}}}\\end{array}$ . Since there are at most $\\log_{2}T$ kinds of different values for $n$ , the total sum of probabilities of bad events is less than $\\begin{array}{r}{\\frac{\\delta}{\\log_{2}T}*\\log_{2}T=\\delta}\\end{array}$ , which ends the proof. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "For $\\mathbb{E}_{n}\\epsilon_{S}$ , we have the following lemma therein to bound it. ", "page_idx": 18}, {"type": "text", "text": "Lemma E.4. With probability at least $1-\\delta$ , it holds that for any $n=2^{m}$ and $m\\in[\\log_{2}T]$ that ", "page_idx": 18}, {"type": "equation", "text": "$$\n|\\frac{1}{n}\\sum_{i=1}^{n}\\epsilon_{S i}|\\leq\\sqrt{\\frac{2\\sigma_{S}^{2}\\log(\\frac{2\\log_{2}T}{\\delta})}{n}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. For any $n\\,=\\,2^{m}$ where $m\\,\\in\\,[\\log_{2}T]$ , we know that $\\epsilon_{S i}$ is a sub-Gaussian variable with parameter $\\sigma_{S}$ because it follows distribution $\\dot{N}(0,\\sigma_{S}^{2})$ . ", "page_idx": 18}, {"type": "text", "text": "By applying Lemma D.2, namely Hoeffding\u2019s inequality, it holds that with probability $1-\\frac{\\delta}{\\log_{2}T}$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n|\\frac{1}{n}\\sum_{i=1}^{n}\\epsilon_{S i}|\\leq\\sqrt{\\frac{2\\sigma_{S}^{2}\\log(\\frac{2\\log_{2}T}{\\delta})}{n}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Combining all nuisance leads to the probability of all inequalities holding, that is, $1-\\delta$ , which finishes our proof. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "Moreover, for $\\mathbb{E}_{n}\\epsilon_{D}$ , we have a similar result whence giving an upper bound for it. ", "page_idx": 18}, {"type": "text", "text": "Lemma E.5. With probability at least $1-\\delta$ , it holds that for any $n=2^{m}$ and $m\\in[\\log_{2}T]$ that ", "page_idx": 18}, {"type": "equation", "text": "$$\n|\\frac{1}{n}\\sum_{i=1}^{n}\\epsilon_{D i}|\\leq\\sqrt{\\frac{2\\sigma_{D}^{2}\\log(\\frac{2\\log_{2}T}{\\delta})}{n}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. The proof is the same as the one for Lemma E.4 with replacement of $\\epsilon_{S i}$ by $\\epsilon_{D i}$ . ", "page_idx": 18}, {"type": "text", "text": "Second, let\u2019s give some concentration bounds for the denominator $\\begin{array}{r}{\\frac{1}{n}\\sum_{i=1}^{n}Q_{i}^{e}(a_{i}-\\mathbb{E}_{n}a)}\\end{array}$ By applying Lemma E.1, we have the following decomposition of the denominator that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\frac{1}{n}\\sum_{i=1}^{n}Q_{i}^{e}(a_{i}-\\mathbb{E}_{n}a)}}\\\\ &{=\\frac{1}{2n(\\alpha_{1}-\\beta_{1})}\\left[\\sum_{i}2(a_{i}-\\mathbb{E}_{n}a)\\epsilon_{D i}-\\sum_{i}(a_{i}-\\mathbb{E}_{n}a)\\epsilon_{S i}+\\sum_{i}(a_{i}-\\mathbb{E}_{n}a)(2\\beta_{0}-\\widehat{\\beta}_{0}-\\alpha_{0})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We use $\\textstyle\\sum_{i}x_{i},\\sum_{i}y_{i}$ and $\\sum_{i}z_{i}$ to denote for shorthand $\\begin{array}{r}{2\\sum_{i}(a_{i}-\\mathbb{E}_{n}a)\\epsilon_{D i},\\sum_{i}(a_{i}-\\mathbb{E}_{n}a)\\epsilon_{S i}}\\end{array}$ and $\\begin{array}{r}{\\sum_{i}(a_{i}-\\mathbb{E}_{n}a)(2\\beta_{0}-\\widehat{\\beta}_{0}-\\alpha_{0})}\\end{array}$ respectively. ", "page_idx": 18}, {"type": "text", "text": "From Lemmas E.3 to E.5, we know that with probability at least $1-3\\delta$ , it holds that ", "page_idx": 19}, {"type": "equation", "text": "$$\n|\\frac{1}{n}\\sum_{i}x_{i}|\\leq\\sqrt{\\frac{2\\nu^{2}\\log(\\frac{2\\log_{2}T}{\\delta})}{n}}+\\frac{2\\sigma_{S}\\sigma_{D}\\log(\\frac{2\\log_{2}T}{\\delta})}{n},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\nu$ are defined in Lemma E.3. ", "page_idx": 19}, {"type": "text", "text": "For $\\textstyle{\\frac{1}{n}}\\sum_{i}y_{i}$ , we have the following lemma to illustrate its property. ", "page_idx": 19}, {"type": "text", "text": "Lemma E.6. There exists constant $n_{2}$ such that with probability at least 1 \u22123\u03b4, it holds that for any $n\\geq n_{2}$ where $n=2^{m}$ and $m\\in[\\log_{2}T],$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\sum_{i=1}^{n}y_{i}+\\frac{\\sigma_{S}^{2}}{2}|\\leq\\frac{3\\bar{P}}{2}\\sqrt{\\frac{2\\sigma_{S}^{2}\\log(\\frac{2\\log_{2}T}{\\delta})}{n}}+\\sqrt{\\frac{8\\log(\\frac{2\\log_{2}T}{\\delta})}{n}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. Let\u2019s decompose $y_{i}$ as $\\begin{array}{r}{-\\mathbb{E}_{n}a*\\epsilon_{S i}+\\frac{\\widehat{\\beta}_{0}-\\alpha_{0}}{2}\\epsilon_{S i}-\\frac{\\epsilon_{S i}^{2}}{2}.}\\end{array}$ ", "page_idx": 19}, {"type": "text", "text": "For $-\\mathbb{E}_{n}a*\\epsilon_{S i}$ , similar to Lemma E.4, it holds that with probability at least $1-\\delta$ , for all $n=2^{m}$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\Big|\\frac{1}{n}\\sum_{i}-\\mathbb{E}_{n}a*\\epsilon_{S i}\\Big|=\\Big|\\frac{1}{n}\\sum_{i}\\epsilon_{S i}\\Big|*\\Big|\\mathbb{E}_{n}a\\Big|}&{}\\\\ {\\displaystyle\\leq\\bar{P}\\Big|\\frac{1}{n}\\sum_{i}\\epsilon_{S i}\\Big|}&{}\\\\ {\\displaystyle}&{\\leq\\bar{P}\\sqrt{\\frac{2\\sigma_{S}^{2}\\log(\\frac{2\\log_{2}T}{\\delta})}{n}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The first equation holds due to simple algebra while the first inequality holds because the service fee has a trivial upper bound $\\bar{P}$ . The last inequality holds due to Lemma D.2. ", "page_idx": 19}, {"type": "text", "text": "As for term $\\frac{\\widehat{\\beta}_{0}-\\alpha_{0}}{2_{.}}\\epsilon_{S i}$ , we use $t_{i}$ to denote it. Like the proof of Lemma E.3, we know that ${\\widehat{\\beta}}_{0}$ only depends on previous $\\epsilon_{D}$ and $\\epsilon_{S}$ but don\u2019t $\\epsilon_{S i}$ . So, it holds that $\\mathbb{E}[t_{i}\\,|\\,t_{1},...,t_{i-1}]=0$ and we u se to denote the corresponding filtration. Since we have the following MGF ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[e^{\\lambda t_{i}}\\,|\\,\\mathcal{F}_{i-1}]=e^{\\frac{\\lambda^{2}\\sigma_{S}^{2}(\\hat{\\beta}_{0}-\\alpha_{0})}{32}}\\le e^{\\frac{\\lambda^{2}\\sigma_{S}^{2}\\bar{P}^{2}}{32}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "we know that $t_{i}$ is a sub-Gaussian random variable with a parameter $\\frac{\\sigma_{S}{\\bar{P}}}{4}$ , which is also subexponential with parameters $\\left(\\frac{\\sigma_{S}\\bar{P}}{4},0\\right)$ . With Lemma D.1, we know that with probability $1-\\delta$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n|\\frac{1}{n}\\sum_{i}\\frac{\\widehat{\\beta}_{0}-\\alpha_{0}}{2}\\epsilon_{S i}|\\leq\\bar{P}\\sqrt{\\frac{\\sigma_{S}^{2}\\log(\\frac{2\\log_{2}T}{\\delta})}{2n}}\\mathrm{~for~all~}m\\in[\\log_{2}T]\\mathrm{~and~}n=2^{m}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For the term $\\frac{\\sigma_{S i}^{2}}{2}$ , since $\\begin{array}{r}{\\frac{\\epsilon_{S i}^{2}}{\\sigma_{S}^{2}}\\sim\\chi_{1}^{2}}\\end{array}$ and $\\chi_{1}^{2}$ is sub-exponential with parameters $(2,4)$ , we know that with probability $1-\\delta$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n|\\frac{1}{n}\\sum_{i}\\frac{\\epsilon_{S i}^{2}}{2}-\\frac{\\sigma_{S}^{2}}{2}|\\leq\\sqrt{\\frac{8\\sigma_{S}^{4}\\log(\\frac{2\\log_{2}T}{\\delta})}{n}},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "as long as $\\begin{array}{r}{n\\geq8\\log(\\frac{2\\log_{2}T}{\\delta}):=n_{2}}\\end{array}$ ", "page_idx": 19}, {"type": "text", "text": "Accordingly, combining the above three terms leads to the needed lemma. ", "page_idx": 19}, {"type": "text", "text": "For $\\textstyle{\\frac{1}{n}}\\sum_{i}z_{i}$ , we notice that it\u2019s indeed zero. It holds that ", "page_idx": 19}, {"type": "equation", "text": "$$\n|\\frac{1}{n}\\sum_{i}z_{i}|=|\\frac{1}{n}\\sum_{i}(a_{i}-\\mathbb{E}_{n}a)(2\\beta_{0}-\\widehat{\\beta}_{0}-\\alpha_{0})|=0,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "due to the definition of $\\mathbb{E}_{n}a$ . ", "page_idx": 19}, {"type": "text", "text": "With the bounds for both nominator and denominator, it holds that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq\\frac{2\\left(\\alpha_{1}-\\beta_{1}\\right)\\left(\\frac{1}{2}\\sqrt{\\frac{2\\nu^{2}\\log(\\frac{2\\log_{2}T}{\\delta})}{n}}+\\frac{\\sigma_{S}\\sigma_{D}\\log(\\frac{2\\log_{2}T}{\\delta})}{n}\\right)}{\\frac{\\sigma_{S}^{2}}{2}-\\sqrt{\\frac{2\\nu^{2}\\log(\\frac{2\\log_{2}T}{\\delta})}{n}}-\\frac{2\\sigma_{S}\\sigma_{D}\\log(\\frac{2\\log_{2}T}{\\delta})}{n}-\\frac{3\\bar{P}}{2}\\sqrt{\\frac{2\\sigma_{S}^{2}\\log(\\frac{2\\log_{2}T}{\\delta})}{n}}-\\sqrt{\\frac{8\\sigma_{S}^{4}\\log(\\frac{2\\log_{2}T}{\\delta})}{n}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then, there exists constants $C_{2},\\,C_{3}$ and $C_{4}$ such that when $n\\,\\geq\\,\\operatorname*{max}\\{n_{1},n_{2},C_{3}\\log\\log T\\}\\,=$ $\\begin{array}{r}{C_{4}\\log\\log T,\\vert\\widehat\\beta_{1}-\\beta_{1}\\vert\\leq\\frac{C_{2}\\sqrt{\\log\\log T}}{\\sigma_{S}\\sqrt{n}}}\\end{array}$ due to $\\nu=\\sqrt{2}\\sigma_{S}\\sigma_{D}$ . Note that $\\begin{array}{r}{C_{4}\\lesssim{\\mathcal O}(\\frac{1}{\\sigma_{S}^{2}})}\\end{array}$ from previous definitions and process of proof. ", "page_idx": 20}, {"type": "text", "text": "When $n\\leq C_{4}\\log\\log T$ , we can choose large enough constant $C_{5}$ such that $\\frac{C_{5}}{\\sigma_{S}\\sqrt{C_{4}}}$ is greater than a trivial bound for $\\beta_{1}$ . Since we care about the situation when $\\sigma_{S}$ is small, the probable appearance of it in the denominator won\u2019t cause trouble for us. Afterwards, by setting $C_{1}=\\bar{\\operatorname*{max}}\\{C_{2},\\bar{C}_{5}\\}$ , we find a uniform constant such that ", "page_idx": 20}, {"type": "equation", "text": "$$\n|\\widehat{\\beta}_{1}-\\beta_{1}|\\leq\\frac{C_{1}\\sqrt{\\log\\log T}}{\\sigma_{S}\\sqrt{2^{m}}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Moreover, we know that after the $m$ -th episode, i.e. in the $m+1$ -th episode, $t\\,\\in\\,\\{2^{m+1}\\,-\\,1\\,:$ $2^{m+2}-1\\}$ , then $t\\approx\\Theta(2^{m})$ . Therefore, from the above, we know ", "page_idx": 20}, {"type": "equation", "text": "$$\n|\\widehat{\\beta}_{1t}-\\beta_{1}|\\lesssim\\mathcal{O}(\\frac{\\sqrt{\\log\\log T}}{\\sigma_{S}\\sqrt{t}})\\;\\mathrm{for}\\;\\mathrm{all}\\;t\\in[T].\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Whereas when calculating the probability of bad events, there is some redundancy, after calibration, the union bound on the probability of bad events is $6\\delta$ which ends the proof. \u53e3 ", "page_idx": 20}, {"type": "text", "text": "Since we can estimate $\\beta_{1}$ with high accuracy, we similarly have the following proposition about the estimates of $\\beta_{0}$ when Proposition E.2 holds. ", "page_idx": 20}, {"type": "text", "text": "Proposition E.7. Let ${\\widehat{\\beta}}_{0}$ be the estimated intercept of Algorithm 4 after the m-th episode. Then, under Assumption 2.1 and conditional on the event that Proposition $E.2$ holds, there exists a constant $D_{1}$ such that with probability at least $1-\\delta$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n|\\widehat{\\beta}_{0}-\\beta_{0}|\\leq\\frac{D_{1}\\sqrt{\\log\\log T}}{\\sigma_{S}\\sqrt{2^{m}}}\\,f o r\\,a l l\\,m\\in[\\log_{2}T],\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "then ", "page_idx": 20}, {"type": "equation", "text": "$$\n|\\widehat{\\beta}_{0t}-\\beta_{0}|\\lesssim\\mathcal{O}(\\frac{\\sqrt{\\log\\log T}}{\\sigma_{S}\\sqrt{t}})\\,f o r\\,a l l\\,t\\in[T],\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where subscript $t$ represents which round. ", "page_idx": 20}, {"type": "text", "text": "Proof. It holds that for any $n=2^{m}$ , with probability at least $1-\\delta$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\widehat{\\beta}_{0}-\\beta_{0}|=|\\beta_{1}\\mathbb{E}_{n}Q^{e}-\\widehat{\\beta}_{1}\\mathbb{E}_{n}Q^{e}+\\mathbb{E}_{n}\\epsilon_{D}|}\\\\ &{\\phantom{\\beta_{0}=}\\;\\leq|\\beta_{1}\\mathbb{E}_{n}Q^{e}-\\widehat{\\beta}_{1}\\mathbb{E}_{n}Q^{e}|+|\\mathbb{E}_{n}\\epsilon_{D}|}\\\\ &{\\phantom{\\beta_{0}=}=|\\mathbb{E}_{n}Q^{e}|*|\\beta_{1}-\\widehat{\\beta}_{1}|+|\\mathbb{E}_{n}\\epsilon_{D}|}\\\\ &{\\phantom{\\beta_{0}=}\\leq\\frac{C_{1}\\bar{Q}\\sqrt{\\log\\log T}}{\\sigma_{S}\\sqrt{n}}+\\sqrt{\\frac{2\\sigma_{D}^{2}\\log\\left(\\frac{2\\log_{2}T}{\\delta}\\right)}{n}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The first equation comes from Algorithm 4. The first inequality holds due to the triangle inequality while the second inequality holds due to Proposition E.2 and Lemma E.5. ", "page_idx": 20}, {"type": "text", "text": "Then, for any n = 2m, there exists a D1 such that |\u03b2 0 \u2212\u03b20| \u2264 D1\u03c3log\u221a lnog T. Similarly, since $t\\approx\\Theta(n)$ , it holds that ", "page_idx": 20}, {"type": "equation", "text": "$$\n|\\widehat{\\beta}_{0t}-\\beta_{0}|\\lesssim\\mathcal{O}(\\frac{\\sqrt{\\log\\log T}}{\\sigma_{S}\\sqrt{t}}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Finally, bad events coming from $\\mathbb{E}_{n}\\epsilon_{D}$ have a cumulative probability of $\\delta$ which closes the proof. ", "page_idx": 20}, {"type": "text", "text": "Up to now, we provide proof of Propositions E.2 and E.7 showing that even when IV is not i.i.d. with respect to time, we can obtain good estimation results, demonstrating the effectiveness of our Algorithm 4. ", "page_idx": 21}, {"type": "text", "text": "E.1.2 Proof of Theorem E.1 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Proof. With Appendix E.1.1 in hand, we are ready to give a high probability upper bound for the total regret. ", "page_idx": 21}, {"type": "text", "text": "We use $f(a)$ to denote $\\begin{array}{r}{\\mathbb{E}a*Q^{e}(P_{S},P_{D},a)=\\frac{\\beta_{0}-\\alpha_{0}-a-\\epsilon_{S}}{\\alpha_{1}-\\beta_{1}}a}\\end{array}$ . Then, with an ordinary Taylor-series expansion and the mean value theorem, it holds that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{SubOpt}_{t}(a_{t})=\\frac{d f(a_{t}^{*})}{d a}(a_{t}^{*}-a_{t})+\\frac{1}{2}\\frac{d^{2}f(\\alpha a_{t}^{*}+(1-\\alpha)a_{t})}{d a^{2}}(a_{t}^{*}-a_{t})^{2},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\alpha\\in[0,1]$ ", "page_idx": 21}, {"type": "text", "text": "From Lemma E.1, we know that dfd(aat ) . Besides, it holds that $\\begin{array}{r}{(a_{t}^{\\ast}-a_{t})^{2}\\lesssim\\mathcal{O}(|\\widehat{\\beta}_{0t}-\\beta_{0}|^{2})\\lesssim}\\end{array}$ $O\\big(\\frac{\\log\\log T}{\\sigma_{S}^{2}t}\\big)$ due to Proposition E.7. Then, since $\\textstyle\\left|{\\frac{d^{2}f}{d a^{2}}}\\right|$ is uniformly bounded by a constant, namely \u03b1 2\u2212\u03b2 , we have that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{SubOpt}_{t}(a_{t})\\lesssim\\mathcal{O}(\\frac{\\log\\log T}{\\sigma_{S}^{2}t}).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "As a result, it holds that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{Regret}(T)=\\sum_{t=1}^{T}\\mathrm{SubOpt}_{t}(a_{t})\\lesssim\\sum_{t=1}^{T}\\mathcal{O}(\\frac{\\log\\log T}{\\sigma_{S}^{2}t})\\lesssim\\mathcal{O}(\\frac{\\log T\\log\\log T}{\\sigma_{S}^{2}}).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The last inequality holds due to $\\begin{array}{r}{\\sum_{i=1}^{n}\\frac{1}{i}\\leq1+\\log n}\\end{array}$ . ", "page_idx": 21}, {"type": "text", "text": "The total odds of some inequalities not holding is $7\\delta$ . Among them, $6\\delta$ derives from Proposition E.2 and $\\delta$ originates from Proposition E.7. Note that $\\delta$ only appears in $\\log\\log T$ terms. Setting $\\iota=7\\delta$ ends our proof. \u53e3 ", "page_idx": 21}, {"type": "text", "text": "We then turn to prove Theorem 3.2. We begin this section with some preparing lemmas. Then, we will prove Theorem 3.2 in leverage of them. Finally, we will give a brief proof of the statement on time-varying discount. ", "page_idx": 21}, {"type": "text", "text": "E.1.3 Useful Facts for Proving Theorem 3.2 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Since the buyer may behave as having a different demand curve $P_{D}^{\\prime}$ rather than the true one $P_{D}$ , we first give the following proposition of depicting such untruthfulness.   \nProposition E.8. For any behavior $P_{D}^{\\prime}$ , we can fully characterize it with a drift parameter $\\eta_{D}$ . Consequently, without loss of generality, we can assume that ", "page_idx": 21}, {"type": "equation", "text": "$$\nP_{D t}^{\\prime}=\\beta_{0}+\\beta_{1}Q+\\epsilon_{D t}+\\eta_{D t}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. Since the platform can only observe achieved equilibrium price $P_{t}^{e}$ and quantity $Q_{t}^{e}$ each round, we only need to find an associated $\\eta_{D t}$ leading to the same results. ", "page_idx": 21}, {"type": "text", "text": "By setting $\\eta_{D t}=P_{t}^{e}-\\beta_{0}-\\beta_{1}Q_{t}^{e}-\\epsilon_{D t}$ , we find that $(P_{t}^{e},Q_{t}^{e})$ will satisfy Equation (1). Since two lines in Euclidean space have at most one intersection point, we know that $\\eta_{D t}$ fully describes $P_{D t}^{\\prime}$ and this correspondence is unique. Hence, we can use $\\eta_{D}$ to represent untruthful behavior $P_{D}^{\\prime}$ and it finishes our proof. \u53e3 ", "page_idx": 21}, {"type": "text", "text": "Using the equivalence in Proposition E.8, we present the relationship between equilibrium price and quantity and the service fee. ", "page_idx": 21}, {"type": "text", "text": "Lemma E.9. For any valid service fee a and tuple $(P_{S},P_{D}^{\\prime})$ , the equilibrium price and quantity follow ", "page_idx": 21}, {"type": "equation", "text": "$$\nQ^{e}=\\frac{\\beta_{0}-\\alpha_{0}-a+\\epsilon_{D}-\\epsilon_{S}+\\eta_{D}}{\\alpha_{1}-\\beta_{1}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and ", "page_idx": 22}, {"type": "equation", "text": "$$\nP^{e}=\\frac{\\alpha_{1}\\beta_{0}-\\alpha_{0}\\beta_{1}-\\beta_{1}a+\\alpha_{1}\\epsilon_{D}-\\beta_{1}\\epsilon_{S}+\\alpha_{1}\\eta_{D}}{\\alpha_{1}-\\beta_{1}},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "respectively. Then, the change coming from $\\eta_{D}$ leads to ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\Delta Q^{e}=\\frac{\\eta_{D}}{\\alpha_{1}-\\beta_{1}}\\,a n d\\,\\Delta P^{e}=\\frac{\\alpha_{1}\\eta_{D}}{\\alpha_{1}-\\beta_{1}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. From Proposition E.8 and Equation (1), we can have the formulas for $Q^{e}$ and $P^{e}$ with some simple algebra. Furthermore, comparing them and the ones without $\\eta_{D}$ leads to the formulas about $\\Delta Q^{e}$ and $\\Delta P^{e}$ , which finishes the proof. \u53e3 ", "page_idx": 22}, {"type": "text", "text": "With these preparations, we are ready to give some properties of $\\eta_{D t}$ and show good performance of Algorithm 1. ", "page_idx": 22}, {"type": "text", "text": "Lemma E.10. Considering the m-th episode of Algorithm $^{\\,l}$ , in the $i$ -th round of this episode, we use $\\eta_{D i}$ to denote $\\eta_{D(2^{m}+i-2)}$ though a little abuse of notations. Then, it holds that for any rational buyer, ", "page_idx": 22}, {"type": "equation", "text": "$$\n|\\eta_{D i}|\\leq\\frac{(\\alpha_{1}-\\beta_{1})\\bar{Q}}{\\sqrt{1-\\gamma}}\\gamma^{\\frac{n-i+1}{2}},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the m-th episode contains $n=2^{m}$ rounds. More precisely, it holds that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}\\gamma^{i-n-1}\\eta_{D i}^{2}\\leq\\frac{(\\alpha_{1}-\\beta_{1})^{2}\\bar{Q}^{2}}{1-\\gamma}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. From Assumption 2.2, we know that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathrm{Sur}=\\int_{0}^{Q^{e}}(P_{D}-P_{S}-a)d Q.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Therefore, we know that in the $i$ -th round of the $m$ -th episode, the loss of surplus is ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\Delta\\mathrm{Sur}_{i}=\\frac{1}{2}(\\Delta Q_{i}^{e})^{2}(\\beta_{1}-\\alpha_{1})=-\\frac{1}{2}\\frac{\\eta_{D i}^{2}}{\\alpha_{1}-\\beta_{1}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Since the update of the estimation of $(\\beta_{0},\\beta_{1})$ happens in $n-i$ rounds, the maximum gain from misreport is bounded by ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sum_{j=n-i}^{T}\\gamma^{j+1}\\frac{1}{2}(\\alpha_{1}-\\beta_{1})\\bar{Q}^{2}\\leq\\sum_{j=n-i}^{\\infty}\\gamma^{j+1}\\frac{1}{2}(\\alpha_{1}-\\beta_{1})\\bar{Q}^{2}=\\frac{\\gamma^{n-i+1}(\\alpha_{1}-\\beta_{1})\\bar{Q}^{2}}{2(1-\\gamma)},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "because the surplus is bounded by a trivial bound $\\frac{1}{2}\\big(\\alpha_{1}-\\beta_{1}\\big)\\bar{Q}^{2}$ in each round. The inequality holds because every term is positive. With some calculations, we have the final equation. ", "page_idx": 22}, {"type": "text", "text": "Hence, we know that if the buyer is rational, she will guarantee that ", "page_idx": 22}, {"type": "equation", "text": "$$\n-\\frac{1}{2}\\frac{\\eta_{D i}^{2}}{\\alpha_{1}-\\beta_{1}}+\\frac{\\gamma^{n-i+1}(\\alpha_{1}-\\beta_{1})\\bar{Q}^{2}}{2(1-\\gamma)}\\geq0,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "ending with ", "page_idx": 22}, {"type": "equation", "text": "$$\n|\\eta_{D i}|\\leq\\frac{(\\alpha_{1}-\\beta_{1})\\bar{Q}}{\\sqrt{1-\\gamma}}\\gamma^{\\frac{n-i+1}{2}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "When we consider all $n$ rounds in the $m$ -th episode as a whole, we know the total loss of surplus at the point of the last round in the m \u22121-th episode is in=1 \u03b3i 12\u03b11\u03b7\u2212Di\u03b21 . The maximum surplus increment is less than $\\gamma^{n+1}\\frac{(\\alpha\\!-\\!\\beta_{1})\\bar{Q}^{2}}{2(1\\!-\\!\\gamma)}$ . For a rational and strategic buyer, it holds that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}\\gamma^{i}\\frac{1}{2}\\frac{\\eta_{D i}^{2}}{\\alpha_{1}-\\beta_{1}}\\le\\gamma^{n+1}\\frac{(\\alpha-\\beta_{1})\\bar{Q}^{2}}{2(1-\\gamma)},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "leading to ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}\\gamma^{i-n-1}\\eta_{D i}^{2}\\leq\\frac{(\\alpha_{1}-\\beta_{1})^{2}\\bar{Q}^{2}}{1-\\gamma},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "which finishes our proof. ", "page_idx": 23}, {"type": "text", "text": "We notice that since the buyer owns strictly less than 1 discount rate, if we inverse time, the bound of misreport will exponentially decay. Therefore, we have reason to believe that these misreports will not significantly affect the estimation of parameters. Consequently, we have the following propositions. ", "page_idx": 23}, {"type": "text", "text": "Proposition E.11. Let ${\\widehat{\\beta}}_{1}$ be the estimated slope of Algorithm 4 after the $m$ -th episode. Then, under Assumptions 2.1 and 2 .2, with probability at least $1-6\\delta$ , it holds that when $\\begin{array}{r}{{2^{m}}\\gtrsim{\\cal O}(\\frac{\\log\\log T}{{\\sigma_{S}^{2}}}+}\\end{array}$ $\\frac{1}{\\sigma_{S}^{2}(1-\\gamma)})$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n|\\widehat{\\beta}_{1}-\\beta_{1}|\\lesssim\\mathcal{O}(\\frac{\\sqrt{\\log\\log T}}{\\sigma_{S}\\sqrt{2^{m}}}+\\frac{1}{2^{m}\\sigma_{S}^{2}(1-\\gamma)})\\,f o r\\,a l l\\,m\\in[\\log_{2}T],\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "then ", "page_idx": 23}, {"type": "equation", "text": "$$\n|\\widehat{\\beta}_{1t}-\\beta_{1}|\\lesssim\\mathcal{O}(\\frac{\\sqrt{\\log\\log T}}{\\sigma_{S}\\sqrt{t}}+\\frac{1}{\\sigma_{S}^{2}(1-\\gamma)t})\\,f o r\\,a l l\\,t\\in[T],\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where subscript $t$ represents which round. ", "page_idx": 23}, {"type": "text", "text": "Proof. Similar to Proposition E.2, it holds that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\widehat{\\beta}_{1}=\\beta_{1}+\\frac{\\frac{1}{n}\\sum_{i=1}^{n}\\epsilon_{D i}(a_{i}-\\mathbb{E}_{n}a)}{\\frac{1}{n}\\sum_{i=1}^{n}Q_{i}^{e}(a_{i}-\\mathbb{E}_{n}a)}+\\frac{\\frac{1}{n}\\sum_{i=1}^{n}\\eta_{D i}(a_{i}-\\mathbb{E}_{n}a)}{\\frac{1}{n}\\sum_{i=1}^{n}Q_{i}^{e}(a_{i}-\\mathbb{E}_{n}a)}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "For the numerator, we know that with probability at least $1-3\\delta$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n|\\frac{1}{n}\\sum_{i=1}^{n}\\epsilon_{D i}(a_{i}-\\mathbb{E}_{n}a)|\\leq\\frac{1}{2}\\sqrt{\\frac{2\\nu^{2}\\log(\\frac{2\\log_{2}T}{\\delta})}{n}}+\\frac{\\sigma_{S}\\sigma_{D}\\log(\\frac{2\\log_{2}T}{\\delta})}{n},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "due to Lemmas E.3 to E.5. ", "page_idx": 23}, {"type": "text", "text": "Moreover, we bound $\\begin{array}{r}{\\frac{1}{n}\\sum_{i=1}^{n}\\eta_{D i}(a_{i}-\\mathbb{E}_{n}a)}\\end{array}$ directly in leverage of Lemma E.10. It holds that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\eta_{D i}(a_{i}-\\mathbb{E}_{n}a)|\\leq\\frac{2\\bar{P}}{n}\\displaystyle\\sum_{i=1}^{n}|\\eta_{D i}|}\\\\ {\\leq\\frac{2\\bar{P}}{n}\\sqrt{\\displaystyle\\sum_{i=1}^{n}\\gamma^{i-n-1}\\eta_{D i}^{2}}\\sqrt{\\displaystyle\\sum_{i=1}^{n}\\gamma^{n+1-i}}}\\\\ {\\displaystyle}&{\\leq\\frac{2(\\alpha_{1}-\\beta_{1})\\bar{P}\\bar{Q}}{n\\sqrt{1-\\gamma}}\\sqrt{\\displaystyle\\frac{\\gamma}{1-\\gamma}}}\\\\ {\\displaystyle}&{=\\frac{2(\\alpha_{1}-\\beta_{1})\\bar{P}\\bar{Q}\\sqrt{\\gamma}}{n(1-\\gamma)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The first inequality holds due to $a_{i}\\leq\\bar{P}$ while the second inequality holds due to Lemma E.10 and Cauchy\u2013Schwarz inequality. The third inequality holds because every term in the summation is positive and the geometric series sum formula. ", "page_idx": 23}, {"type": "text", "text": "For the denominator, since $\\Delta Q^{e}\\;=\\;\\eta_{D}\\big/(\\alpha_{1}\\,-\\,\\beta_{1})$ from Lemma E.9, we only need to bound $\\begin{array}{r}{\\frac{1}{n}\\sum_{i=1}^{n}\\frac{\\eta_{D}}{\\alpha_{1}-\\beta_{1}}(a_{i}-\\mathbb{E}_{n}a)}\\end{array}$ . It holds that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\sum_{i=1}^{n}\\frac{\\eta_{D}}{\\alpha_{1}-\\beta_{1}}(a_{i}-\\mathbb{E}_{n}a)\\leq\\frac{2\\bar{P}\\bar{Q}\\sqrt{\\gamma}}{n(1-\\gamma)}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "due to the same process as bounding $\\begin{array}{r}{|{\\frac{1}{n}}\\sum_{i=1}^{n}\\eta_{D i}(a_{i}-\\mathbb{E}_{n}a)|}\\end{array}$ above. ", "page_idx": 23}, {"type": "text", "text": "Therefore, we notice that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}Q_{i}^{e}(a_{i}-\\mathbb{E}_{n}a)|\\geq\\!\\frac{1}{2(\\alpha_{1}-\\beta_{1})}\\!\\left(\\frac{\\sigma_{S}^{2}}{2}-\\sqrt{\\frac{2\\nu^{2}\\log(\\frac{2\\log_{2}T}{\\delta})}{n}}-\\frac{2\\sigma_{S}\\sigma_{D}\\log(\\frac{2\\log_{2}T}{\\delta})}{n}\\right.}\\\\ {-\\left.\\frac{3\\bar{P}}{2}\\sqrt{\\frac{2\\sigma_{S}^{2}\\log(\\frac{2\\log_{2}T}{\\delta})}{n}}-\\sqrt{\\frac{8\\sigma_{S}^{4}\\log(\\frac{2\\log_{2}T}{\\delta})}{n}}\\right)-\\frac{2\\bar{P}\\bar{Q}\\sqrt{\\gamma}}{n(1-\\gamma)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "with probability at least $1-6\\delta$ due to Lemma E.6. ", "page_idx": 24}, {"type": "text", "text": "Combining all these terms, it holds that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\widehat{\\beta}_{1}-\\beta_{1}|\\leq\\Big|\\frac{1}{\\frac{1}{n}\\sum_{i=1}^{n}\\Theta_{i}(a_{i}-\\mathbb{E}_{n}a)}\\Big|+\\Big|\\frac{\\frac{1}{n}\\sum_{i=1}^{n}\\eta_{D i}(a_{i}-\\mathbb{E}_{n}a)}{\\frac{1}{n}\\sum_{i=1}^{n}Q_{i}^{*}(a_{i}-\\mathbb{E}_{n}a)}\\Big|}\\\\ &{\\qquad\\lesssim\\frac{\\mathcal{O}(\\sigma_{S}\\frac{\\sqrt{\\log\\log T}}{\\sqrt{n}})}{\\mathcal{O}(\\sigma_{S}^{2})-\\mathcal{O}(\\sigma_{S}\\frac{\\sqrt{\\log\\log T}}{\\sqrt{n}})-\\mathcal{O}(\\frac{1}{n(1-\\gamma)})}+\\frac{\\mathcal{O}(\\frac{1}{n(1-\\gamma)})}{\\mathcal{O}(\\sigma_{S}^{2})-\\mathcal{O}(\\sigma_{S}\\frac{\\sqrt{\\log T}}{\\sqrt{n}})-\\mathcal{O}(\\frac{1}{n(1-\\gamma)})}}\\\\ &{\\qquad\\lesssim\\mathcal{O}(\\frac{\\sqrt{\\log\\log T}}{\\sigma_{S}\\sqrt{n}}+\\frac{1}{n\\sigma_{S}^{2}(1-\\gamma)})}\\\\ &{\\qquad\\lesssim\\mathcal{O}(\\frac{\\sqrt{\\log\\log T}}{\\sigma_{S}\\sqrt{2^{m}}}+\\frac{1}{2^{m}\\sigma_{S}^{2}(1-\\gamma)}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The first inequality holds due to the triangle inequality while the second inequality holds due to previously presented concentration bounds. The third inequality holds due to simple algebra while the last one holds due to $n=2^{m}$ . Besides, taking into account repeated bad events, the probability of all inequalities holding is at least $1-6\\delta$ . Here, we need $\\begin{array}{r}{n\\ge\\mathcal{O}\\bar{(\\frac{\\log\\log T}{\\sigma_{S}^{2}}+\\frac{1}{\\sigma_{S}^{2}(1-\\gamma)})}}\\end{array}$ . ", "page_idx": 24}, {"type": "text", "text": "Considering that for any round in the $m+1$ -th episode, we have $t\\in\\{2^{m+1}-1:2^{m+2}-1\\}$ , it then holds that ", "page_idx": 24}, {"type": "equation", "text": "$$\n|\\widehat{\\beta}_{1t}-\\beta_{1}|\\lesssim\\mathcal{O}(\\frac{\\sqrt{\\log\\log T}}{\\sigma_{S}\\sqrt{t}}+\\frac{1}{\\sigma_{S}^{2}t(1-\\gamma)})\\mathrm{~for~all~}t\\in[T],\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "which ends the proof. ", "page_idx": 24}, {"type": "text", "text": "Conditional on having a good estimation of $\\beta_{1}$ , we have the following proposition, saying that we can also estimate $\\beta_{0}$ precisely. ", "page_idx": 24}, {"type": "text", "text": "Proposition E.12. Let ${\\widehat{\\beta}}_{0}$ be the estimated intercept of Algorithm 4 after the m-th episode. Then, under Assumptions 2.1 and 2.2 and conditional on the event that Proposition E.11 holds, we have with probability at least $1-\\delta$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n|\\widehat{\\beta}_{0}-\\beta_{0}|\\lesssim\\mathcal{O}(\\frac{\\sqrt{\\log\\log T}}{\\sigma_{S}\\sqrt{2^{m}}}+\\frac{1}{2^{m}\\sigma_{S}^{2}(1-\\gamma)})\\,f o r\\,a l l\\,m\\in[\\log_{2}T],\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "then ", "page_idx": 24}, {"type": "equation", "text": "$$\n|\\widehat{\\beta}_{0t}-\\beta_{0}|\\lesssim\\mathcal{O}(\\frac{\\sqrt{\\log\\log T}}{\\sigma_{S}\\sqrt{t}}+\\frac{1}{\\sigma_{S}^{2}(1-\\gamma)t})\\,f o r\\,a l l\\,t\\in[T],\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where subscript $t$ represents which round. ", "page_idx": 24}, {"type": "text", "text": "Proof. We know that $\\widehat{\\beta}_{0}=\\mathbb{E}_{n}P^{e}-\\widehat{\\beta}_{1}\\mathbb{E}_{n}Q^{e}$ . Thus, it holds that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{|\\widehat{\\beta}_{0}-\\beta_{0}|\\leq|\\mathbb{E}_{n}P^{\\epsilon}-\\widehat{\\beta}_{1}\\mathbb{E}_{n}Q^{\\epsilon}-\\beta_{0}|}}\\\\ &{\\leq|\\beta_{1}\\mathbb{E}_{n}Q^{\\epsilon}+\\mathbb{E}_{n}\\epsilon_{D}+\\mathbb{E}_{n}\\eta_{D}-\\widehat{\\beta}_{1}\\mathbb{E}_{n}Q^{\\epsilon}|}\\\\ &{\\leq|\\widehat{\\beta}_{1}-\\beta_{1}|*|\\mathbb{E}_{n}Q^{\\epsilon}|+|\\mathbb{E}_{n}\\epsilon_{D}|+|\\mathbb{E}_{n}\\eta_{D}|}\\\\ &{\\leq|\\widehat{\\beta}_{1}-\\beta_{1}|*\\tilde{Q}+|\\mathbb{E}_{n}\\epsilon_{D}|+\\mathbb{E}_{n}|\\eta_{D}|}\\\\ &{\\lesssim\\bar{Q}*\\mathcal{O}(\\frac{\\sqrt{\\log\\log T}}{\\sigma_{S}\\sqrt{n}}+\\frac{1}{n\\sigma_{S}^{2}(1-\\gamma)})+\\sqrt{\\frac{2\\sigma_{D}^{2}\\log(\\frac{2\\log2T}{\\delta})}{n}}+\\frac{(\\alpha_{1}-\\beta_{1})\\bar{Q}\\sqrt{\\gamma}}{n(1-\\gamma)}}\\\\ &{\\lesssim\\mathcal{O}(\\frac{\\sqrt{\\log\\log T}}{\\sigma_{S}\\sqrt{n}}+\\frac{1}{n\\sigma_{S}^{2}(1-\\gamma)}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The first inequality holds due to the definition of ${\\widehat{\\beta}}_{0}$ while the second inequality holds due to Proposition E.8. The third inequality holds due to the triangle inequality while the fourth inequality holds due to Jensen\u2019s inequality and trivial upper bound $\\bar{Q}$ . The fifth inequality holds with probability at least $1-\\delta$ because of the results achieved in the proof of Proposition E.11 while the last inequality holds due to simple algebra. ", "page_idx": 25}, {"type": "text", "text": "Similarly, we have that ", "page_idx": 25}, {"type": "equation", "text": "$$\n|\\widehat{\\beta}_{0t}-\\beta_{0}|\\lesssim\\mathcal{O}(\\frac{\\sqrt{\\log\\log T}}{\\sigma_{S}\\sqrt{t}}+\\frac{1}{\\sigma_{S}^{2}t(1-\\gamma)})\\mathrm{~for~all~}t\\in[T],\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "which ends the proof. ", "page_idx": 25}, {"type": "text", "text": "Propositions E.11 and E.12 highlight the statistical efficiency of Algorithm 1 even when the buyer is far-sighted, showing the robustness of our algorithm against non-myopic strategic agents. ", "page_idx": 25}, {"type": "text", "text": "E.1.4 Proof of Theorem 3.2 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Proof. We recall the definition of $\\operatorname{SubOpt}_{t}$ in round $t$ first, that is, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathrm{SubOpt}_{t}(a_{t})=\\mathbb{E}[a_{t}^{*}*Q^{e}(P_{S t},P_{D t},a_{t}^{*})-a_{t}*Q^{e}(P_{S t},P_{D t}^{\\prime},a_{t})].\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Hence, we can use the triangle inequality to gain ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ \\ \\mathrm{SubOpt}_{t}(a_{t})}\\\\ &{\\le\\mathbb{E}|a_{t}^{*}*Q^{e}(P_{S t},P_{D t},a_{t}^{*})-a_{t}*Q^{e}(P_{S t},P_{D t},a_{t})|+}\\\\ &{\\ \\ \\ \\ \\mathbb{E}|a_{t}*Q^{e}(P_{S t},P_{D t},a_{t})-a_{t}*Q^{e}(P_{S t},P_{D t}^{\\prime},a_{t})|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "For the first term, it holds that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}|a_{t}^{*}\\ast Q^{e}(P_{S t},P_{D t},a_{t}^{*})-a_{t}\\ast Q^{e}(P_{S t},P_{D t},a_{t})|\\lesssim\\mathcal{O}(\\frac{\\log\\log T}{\\sigma_{S}^{2}t}+\\frac{1}{\\sigma_{S}^{2}t(1-\\gamma)})\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the first term is similar to the proof of Theorem E.1. For the second term, since $\\begin{array}{r}{|\\frac{d f}{d a}|\\leq\\frac{2\\bar{P}}{(\\alpha_{1}-\\beta_{1})}}\\end{array}$ , we use the first-order approximation to obtain it. We thereafter know that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathbb{E}|a_{t}^{*}*Q^{e}(P_{S t},P_{D t},a_{t}^{*})-a_{t}*Q^{e}(P_{S t},P_{D t},a_{t})|\\lesssim\\mathcal{O}(\\frac{\\log T\\log\\log T}{\\sigma_{S}^{2}}+\\frac{\\log T}{\\sigma_{S}^{2}(1-\\gamma)}).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "For the second term, it holds that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}|a_{t}Q^{e}(P_{S t},P_{D t},a_{t})-a_{t}*Q^{e}(P_{S t},P_{D t}^{\\prime},a_{t})|\\leq\\mathbb{E}|a_{t}|*|Q^{e}(P_{S t},P_{D t},a_{t})-Q^{e}(P_{S t},P_{D t}^{\\prime},a_{t})|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\bar{P}*\\mathbb{E}|Q^{e}(P_{S t},P_{D t},a_{t})-Q^{e}(P_{S t},P_{D t}^{\\prime},a_{t})|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\frac{\\bar{P}}{\\alpha_{1}-\\beta_{1}}|\\eta_{D t}|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The first inequality holds due to simple algebra while the second inequality holds because the service fee is always no larger than $\\bar{P}$ . The last inequality holds due to Lemma E.9. ", "page_idx": 26}, {"type": "text", "text": "Then, during the $m$ -th episode, there are $n=2^{m}$ rounds and we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\frac{\\bar{P}}{\\alpha_{1}-\\beta_{1}}|\\eta_{D i}|\\leq\\frac{\\bar{P}\\bar{Q}}{\\sqrt{1-\\gamma}}(\\sqrt{\\gamma})^{n-i+1},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}\\gamma^{i-n-1}\\eta_{D i}^{2}\\leq\\frac{(\\alpha_{1}-\\beta_{1})^{2}\\bar{Q}^{2}}{1-\\gamma}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Furthermore, it holds that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{T}\\frac{\\bar{P}}{\\alpha_{1}-\\beta_{1}}|\\eta_{D t}|=\\sum_{m=1}^{\\log_{2}T}\\sum_{i=1}^{2^{m}}\\frac{\\bar{P}}{\\alpha_{1}-\\beta_{1}}|\\eta_{D i}|\\leq\\sum_{m=1}^{\\log_{2}T}\\frac{\\bar{P}}{\\alpha_{1}-\\beta_{1}}\\frac{(\\alpha_{1}-\\beta_{1})\\bar{Q}\\sqrt{\\gamma}}{1-\\gamma}=\\frac{\\bar{P}\\bar{Q}\\sqrt{\\gamma}\\log_{2}T}{1-\\gamma}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The inequality holds due to the same process when proving Proposition E.11. It comes from Lemma E.10 and Cauchy-Schwartz inequality. ", "page_idx": 26}, {"type": "text", "text": "Consequently, we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{R e g r e t}(T)=\\displaystyle\\sum_{t=1}^{T}\\mathsf{S u b O p t}_{t}(a_{t})}\\\\ &{\\qquad\\qquad\\lesssim\\mathcal{O}(\\frac{\\log T\\log\\log T}{\\sigma_{S}^{2}}+\\frac{\\log T}{\\sigma_{S}^{2}(1-\\gamma)})+\\frac{\\bar{P}\\bar{Q}\\sqrt{\\gamma}\\log_{2}T}{1-\\gamma}+\\mathcal{O}(\\frac{\\log\\log T}{\\sigma_{S}^{2}}+\\frac{1}{\\sigma_{S}^{2}(1-\\gamma)})}\\\\ &{\\qquad\\qquad\\lesssim\\mathcal{O}(\\frac{\\log T\\log\\log T}{\\sigma_{S}^{2}}+\\frac{\\log T}{\\sigma_{S}^{2}(1-\\gamma)}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "with probability at least $1-7\\delta$ . For the first inequality, the first term comes from suboptimal service fees while the second term comes from misreport. The last term is needed because of the requirement $\\begin{array}{r}{n\\ge\\mathcal{O}(\\frac{\\log\\log T}{\\sigma_{S}^{2}}+\\frac{1}{\\sigma_{S}^{2}(1-\\gamma)})}\\end{array}$ . ", "page_idx": 26}, {"type": "text", "text": "Finally, as $\\delta$ merely hides within $\\log\\log T$ terms, setting $\\iota=7\\delta$ leads to the desired result and finishes our proof. \u53e3 ", "page_idx": 26}, {"type": "text", "text": "E.1.5 When Facing Time-varying Discount ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "When the buyer has a time-varying discount rate $\\gamma_{t}$ , we can replace it with its uniform upper confidence bound $\\bar{\\gamma}$ in the proofs of Lemma E.10, propositions E.11 and E.12, and theorem 3.2. Since we use upper bound $\\bar{\\gamma}$ for every spot discount rate, all estimators become more conservative. Intuitively, the higher the discount rate, the greater the buyer\u2019s motivation to misreport. So, all inequalities will hold with corresponding probabilities. ", "page_idx": 26}, {"type": "text", "text": "Therefore, when the discount rates are changing in different rounds but with a uniform upper bound $\\bar{\\gamma}$ , Algorithm 1 still achieves ${\\mathcal{O}}(\\log T\\log\\log T)$ regret with high probability which ends the proof. ", "page_idx": 26}, {"type": "text", "text": "E.2 Omitted Proof in Section 3.1 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "So as to prove Theorem 3.1, we state some auxiliary lemmas at first. ", "page_idx": 26}, {"type": "text", "text": "E.2.1 Useful Facts for Proving Theorem 3.1 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Recall that we use $\\epsilon$ to represent added noise which follows ,\u221aSize1(A)+1) and \u03b7D to represent the distance between $P_{D}^{\\prime}$ and $P_{D}$ thanks to Lemma E.9. We have the following lemma to depict the equilibrium price and quantity in each round. ", "page_idx": 26}, {"type": "text", "text": "Lemma E.13. For any estimation ${\\widehat{\\beta}}_{0}$ and tuple $(P_{S},P_{D}^{\\prime})$ , the equilibrium price follows ", "page_idx": 26}, {"type": "equation", "text": "$$\nQ^{e}=\\frac{2\\beta_{0}-\\widehat{\\beta}_{0}-\\alpha_{0}+2\\epsilon_{D}+2\\eta_{D}-2\\epsilon}{2(\\alpha_{1}-\\beta_{1})}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Then, the change coming from $\\eta_{D}$ leads to ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\Delta Q^{e}=\\frac{\\eta_{D}}{\\alpha_{1}-\\beta_{1}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. We get the formula of $Q^{e}$ from Equation (1) with $\\epsilon_{S}=0$ and the implementation of Algorithm 2. Moreover, due to Lemma E.1, we know the optimal ex-ante service fee $\\begin{array}{r}{a^{*}=\\frac{\\beta_{0}-\\alpha_{0}}{2}}\\end{array}$ \u03b20\u2212\u03b10without untruthful behavior and extra noise term \u03f5. \u53e3 ", "page_idx": 27}, {"type": "text", "text": "Since the choice of $\\epsilon$ is independent of $\\eta_{D}$ , we know that Lemma E.10 still holds, that is, in the $i$ -th round of $m$ -th episode, the quantity of misreport $\\eta_{D i}$ follows that for the rational and strategic buyer, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}\\gamma^{i-n-1}\\eta_{D i}^{2}\\leq\\frac{(\\alpha_{1}-\\beta_{1})^{2}\\bar{Q}^{2}}{1-\\gamma},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where the $m$ -th episode contains $n=2^{m}$ rounds. ", "page_idx": 27}, {"type": "text", "text": "Subsequently, we are ready to bound the distance between the estimates of $\\beta_{0}$ and $\\beta_{1}$ and their true values. ", "page_idx": 27}, {"type": "text", "text": "Proposition E.14. Let ${\\widehat{\\beta}}_{1}$ be the estimated slope of Algorithm 4 after the m-th episode. Then, under Assumptions 2.1 and 2.2 , with probability at least $1\\!-\\!8\\delta$ , it holds that when $2^{m}\\gtrsim\\mathcal{O}(\\log T\\log\\log T+$ (1\u2212\u03b3)2 ), ", "page_idx": 27}, {"type": "equation", "text": "$$\n|\\widehat{\\beta}_{1}-\\beta_{1}|\\lesssim\\mathcal{O}(\\frac{\\sqrt{\\log\\log T}}{2^{\\frac{m}{4}}}+\\frac{1}{2^{\\frac{m}{2}}(1-\\gamma)})\\,f o r\\,a l l\\,m\\in[\\log_{2}T],\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "then ", "page_idx": 27}, {"type": "equation", "text": "$$\n|\\widehat{\\beta}_{1t}-\\beta_{1}|\\lesssim\\mathcal{O}(\\frac{\\sqrt{\\log\\log T}}{t^{\\frac{1}{4}}}+\\frac{1}{(1-\\gamma)\\sqrt{t}})\\,f o r\\,a l l\\,t\\in[T],\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where subscript $t$ represents which round. ", "page_idx": 27}, {"type": "text", "text": "Proof. Let\u2019s first decompose the error terms of estimating $\\beta_{1}$ . Assume that $n=2^{m}$ as usual. From the implementation of Algorithm 4, we know that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\widehat{\\beta}_{1}=\\frac{\\mathbb{E}_{n}[(a-\\mathbb{E}_{n}a)P^{e}]}{\\mathbb{E}_{n}[(a-\\mathbb{E}_{n}a)Q^{e}]}=\\beta_{1}+\\left|\\frac{\\frac{1}{n}\\sum_{i=1}^{n}\\epsilon_{D i}(a_{i}-\\mathbb{E}_{n}a)}{\\frac{1}{n}\\sum_{i=1}^{n}Q_{i}^{e}(a_{i}-\\mathbb{E}_{n}a)}\\right|+\\left|\\frac{\\frac{1}{n}\\sum_{i=1}^{n}\\eta_{D i}(a_{i}-\\mathbb{E}_{n}a)}{\\frac{1}{n}\\sum_{i=1}^{n}Q_{i}^{e}(a_{i}-\\mathbb{E}_{n}a)}\\right|.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Since $\\begin{array}{r}{a=\\frac{\\widehat{\\beta}_{0}-\\alpha_{0}}{2}+\\epsilon}\\end{array}$ , it holds that $\\begin{array}{r}{\\frac{1}{n}\\sum_{i=1}^{n}\\epsilon_{D i}\\big(a_{i}-\\mathbb{E}_{n}a\\big)=\\frac{1}{n}\\sum_{i=1}^{n}\\epsilon_{i}\\ast\\epsilon_{D i}-\\mathbb{E}_{n}\\epsilon\\ast\\mathbb{E}_{n}\\epsilon_{D}}\\end{array}$ . Let\u2019s bound them one by one. ", "page_idx": 27}, {"type": "text", "text": "First, let\u2019s give a high probability upper bound for $\\underline{{1}}\\sum_{i=1.}^{n}\\epsilon_{i}*\\epsilon_{D i}$ . We note that it\u2019s a martingale difference sequence because has mean zero conditional on previous eve\u221ants. From the proof of Lemma E.3, we know that $\\epsilon_{i}*\\epsilon_{D i}$ is sub-exponential with parameters $\\begin{array}{r}{\\nu_{i}=\\sqrt{2}\\frac{\\sigma_{D}}{i^{1/4}}}\\end{array}$ and $\\begin{array}{r}{\\alpha_{i}=2\\frac{\\sigma_{D}}{i^{1/4}}}\\end{array}$ because $\\begin{array}{r}{\\mathrm{Var}(\\epsilon_{i})=\\frac{1}{\\sqrt{i}}}\\end{array}$ . Then, it holds that due to Lemma D.1, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}(|\\displaystyle\\sum_{i=1}^{n}\\epsilon_{i}*\\epsilon_{D i}|\\geq n t)\\leq2\\exp(-\\frac{n^{2}t^{2}}{4\\sigma_{D}^{2}\\sum_{i=1}^{n}\\sigma_{i}^{2}})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq2\\exp(-\\frac{n^{2}t^{2}}{8\\sigma_{D}^{2}\\sqrt{n}})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=2\\exp(-\\frac{n^{3/2}t^{2}}{8\\sigma_{D}^{2}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The first inequality holds due to Lemma D.1 while the second inequality holds due to $\\sqrt{n}\\,\\leq$ $\\textstyle\\sum_{i=1}^{n}{\\frac{1}{\\sqrt{i}}}\\,\\leq\\,2{\\sqrt{n}}$ . Hence, by setting $\\begin{array}{r}{t\\,=\\,\\frac{\\sqrt{8\\sigma_{D}^{2}\\log(\\frac{2\\log_{2}T}{\\delta})}}{n^{3/4}}}\\end{array}$ , it holds that $\\begin{array}{r}{\\mathbb{P}(|\\frac{1}{n}\\sum_{i=1}^{n}\\epsilon_{i}\\ast\\epsilon_{D i}|\\ \\geq}\\end{array}$ $\\begin{array}{r}{t)\\leq\\frac{\\delta}{\\log_{2}T}}\\end{array}$ . Besides, we need $\\begin{array}{r}{n t\\leq\\frac{\\sum_{i=1}^{n}\\nu_{i}^{2}}{\\operatorname*{max}_{i}\\alpha_{i}}}\\end{array}$ . A sufficient condition is $\\begin{array}{r}{n\\geq(8\\log(\\frac{2\\log_{2}T}{\\delta}))^{2}}\\end{array}$ , then $n\\gtrsim\\mathcal{O}((\\log\\log T)^{2})$ . Finally, we have that with probability at least $1-\\delta$ , for any $m\\in[\\log_{2}T]$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n|\\frac{1}{n}\\sum_{i=1}^{n}\\epsilon_{D i}\\epsilon_{i}|\\leq\\frac{\\sqrt{8\\sigma_{D}^{2}\\log(\\frac{2\\log_{2}T}{\\delta})}}{n^{3/4}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "For the term $\\mathbb{E}_{n}\\epsilon_{D i}$ , we have the following result from Lemma E.5 ", "page_idx": 28}, {"type": "equation", "text": "$$\n|\\mathbb{E}_{n}\\epsilon_{D i}|\\leq\\sqrt{\\frac{2\\sigma_{D}^{2}\\log(\\frac{2\\log_{2}T}{\\delta})}{n}}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "with probability at least $1-\\delta$ . ", "page_idx": 28}, {"type": "text", "text": "For the term $\\mathbb{E}_{n}\\epsilon_{i}$ , we know that $\\epsilon_{i}$ is sub-Gaussian with parameter $\\begin{array}{r}{\\sigma_{i}=\\frac{1}{i^{1/4}}}\\end{array}$ . Consequently, it holds that ", "page_idx": 28}, {"type": "equation", "text": "$$\n|\\mathbb{E}_{n}\\epsilon_{i}|\\leq\\frac{\\sqrt{4\\log(\\frac{2\\log_{2}T}{\\delta})}}{n^{3/4}},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "with cumulative probability at least $1-\\delta$ due to Lemma D.2. ", "page_idx": 28}, {"type": "text", "text": "We thereafter bound the term $\\begin{array}{r}{\\frac{1}{n}\\sum_{i=1}^{n}\\eta_{\\underline{{D}}i}\\big({a}_{i}-\\mathbb{E}_{n}{a}\\big)}\\end{array}$ . From Equation (2), we can obtain the following bound directly inherited from the proof of Proposition E.11 that ", "page_idx": 28}, {"type": "equation", "text": "$$\n|\\frac{1}{n}\\sum_{i=1}^{n}\\eta_{D i}(a_{i}-\\mathbb{E}_{n}a)|\\leq\\frac{2(\\alpha_{1}-\\beta_{1})\\bar{P}\\bar{Q}\\sqrt{\\gamma}}{n(1-\\gamma)}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "After that, we need to investigate the property of $\\begin{array}{r}{\\sum_{i=1}^{n}Q_{i}^{e}(a_{i}-\\mathbb{E}_{n}a)}\\end{array}$ . We divide it into three terms. It holds that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}Q_{i}^{e}(a_{i}-\\mathbb{E}_{n}a)}\\\\ &{=\\displaystyle\\frac{1}{\\alpha_{1}-\\beta_{1}}[\\underbrace{\\frac{1}{n}\\sum_{i=1}^{n}\\epsilon_{D i}\\big(a_{i}-\\mathbb{E}_{n}a\\big)}_{q_{1}}+\\underbrace{\\frac{1}{n}\\sum_{i=1}^{n}\\eta_{D i}\\big(a_{i}-\\mathbb{E}_{n}a\\big)}_{q_{2}}+\\underbrace{\\frac{1}{n}\\sum_{i=1}^{n}(-\\epsilon_{i})\\big(a_{i}-\\mathbb{E}_{n}a\\big)}_{q_{3}}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "From previous results, we know that with probability at least $1-3\\delta$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left\\vert q_{1}\\right\\vert\\leq\\frac{\\sqrt{8\\sigma_{D}^{2}\\log(\\frac{2\\log_{2}T}{\\delta})}}{n^{3/4}}+\\frac{\\sqrt{8\\sigma_{D}^{2}\\log^{2}(\\frac{2\\log_{2}T}{\\delta})}}{n^{5/4}},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "and ", "page_idx": 28}, {"type": "equation", "text": "$$\n|q_{2}|\\leq{\\frac{2(\\alpha_{1}-\\beta_{1})\\bar{P}\\bar{Q}{\\sqrt{\\gamma}}}{n(1-\\gamma)}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "For the last term $q_{3}$ , it holds that ", "page_idx": 28}, {"type": "equation", "text": "$$\n-q_{3}=\\frac{1}{n}\\sum_{i=1}^{n}\\epsilon_{i}^{2}-(\\frac{1}{n}\\sum_{i=1}^{n}\\epsilon_{i})^{2}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Besides, we know that with probability at least $1-\\delta$ , it holds that ", "page_idx": 28}, {"type": "equation", "text": "$$\n(\\frac{1}{n}\\sum_{i=1}^{n}\\epsilon_{i})^{2}\\leq\\frac{4\\log(\\frac{2\\log_{2}T}{\\delta})}{n^{3/2}},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "due to Lemma D.2. ", "page_idx": 28}, {"type": "text", "text": "Finally, for $\\textstyle{\\frac{1}{n}}\\sum_{i=1}^{n}\\epsilon_{i}^{2}$ , we know that $\\begin{array}{r}{\\epsilon_{i}\\sim\\mathcal{N}(0,\\frac{1}{\\sqrt{i}})}\\end{array}$ and they are independent. Therefore, $\\epsilon_{i}^{2}$ is sub-exponential random variable with $\\begin{array}{r}{\\nu_{i}=\\frac{2}{\\sqrt{i}}}\\end{array}$ and $\\begin{array}{r}{\\dot{\\alpha}_{i}=\\frac{4}{\\sqrt{i}}}\\end{array}$ with scaling of $\\chi_{1}^{2}$ random variable. So, from Lemma D.1, it holds that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{P}(|\\sum_{i=1}^{n}\\epsilon_{i}^{2}-\\sum_{i=1}^{n}\\frac{1}{\\sqrt{i}}|\\geq n t)\\leq\\operatorname*{max}\\left\\{2\\exp(-\\frac{n^{2}t^{2}}{8\\sum_{i=1}^{n}\\frac{1}{i}}),2\\exp(-\\frac{n t}{2\\alpha^{*}})\\right\\},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\alpha^{*}\\,=\\,\\operatorname*{max}_{i}\\alpha_{i}\\,=\\,4$ . By choosing $\\begin{array}{r}{t\\,=\\,\\operatorname*{max}\\Bigg\\{\\frac{\\sqrt{8(1+\\log T)\\log(\\frac{2\\log_{2}T}{\\delta})}}{n},\\frac{8\\log(\\frac{2\\log_{2}T}{\\delta})}{n}\\Bigg\\},}\\end{array}$ we know that with probability at least $1-\\delta$ , we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{n}|\\displaystyle\\sum_{i=1}^{n}\\epsilon_{i}^{2}-\\displaystyle\\sum_{i=1}^{n}\\frac{1}{\\sqrt{i}}|}\\\\ &{\\leq\\operatorname*{max}\\left\\{\\frac{\\sqrt{8(1+\\log T)\\log(\\frac{2\\log_{2}T}{\\delta})}}{n},\\frac{8\\log(\\frac{2\\log_{2}T}{\\delta})}{n}\\right\\}\\lesssim\\mathcal{O}(\\frac{\\sqrt{\\log T\\log\\log T}}{n}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "for any $n=2^{m}$ . Here, we use facts $\\begin{array}{r}{\\log n\\leq\\sum_{i=1}^{n}\\frac{1}{i}\\leq1+\\log n}\\end{array}$ and $n\\leq T$ . So, with probability at least $1-2\\delta$ , we know that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|q_{3}|\\geq\\frac{1}{\\sqrt{n}}-\\operatorname*{max}\\left\\{\\frac{\\sqrt{8(1+\\log T)\\log(\\frac{2\\log_{2}T}{\\delta})}}{n},\\frac{8\\log\\left(\\frac{2\\log_{2}T}{\\delta}\\right)}{n}\\right\\}-\\frac{4\\log\\left(\\frac{2\\log_{2}T}{\\delta}\\right)}{n^{3/2}}}\\\\ &{\\qquad\\gtrsim\\frac{1}{\\sqrt{n}}-\\mathcal{O}(\\frac{\\sqrt{\\log T\\log\\log T}}{n}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Combing all these terms, we know that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\beta_{1}-\\widehat{\\beta}_{1}|\\leq\\frac{\\big(\\alpha_{1}-\\beta_{1}\\big)\\,\\left(\\frac{\\sqrt{8\\sigma_{D}^{2}\\log(\\frac{2\\log2\\,T}{\\delta})}}{n^{3/4}}+\\frac{\\sqrt{8\\sigma_{D}^{2}\\log^{2}(\\frac{2\\log2\\,T}{\\delta})}}{n^{5/4}}+\\frac{2\\big(\\alpha_{1}-\\beta_{1}\\big)\\bar{P}\\bar{Q}\\sqrt{\\gamma}}{n(1-\\gamma)}\\right)}{\\frac{1}{\\sqrt{n}}-\\mathcal{O}(\\frac{\\sqrt{\\log T\\log\\log T}}{n})-\\frac{\\sqrt{8\\sigma_{D}^{2}\\log(\\frac{2\\log2\\,T}{\\delta})}}{n^{3/4}}-\\frac{\\sqrt{8\\sigma_{D}^{2}\\log^{2}(\\frac{2\\log2\\,T}{\\delta})}}{n^{5/4}}-\\frac{2\\big(\\alpha_{1}-\\beta_{1}\\big)\\bar{P}\\bar{Q}\\sqrt{\\gamma}}{n(1-\\gamma)}}}\\\\ &{\\qquad\\qquad\\lesssim\\mathcal{O}(\\frac{\\sqrt{\\log\\log T}}{n^{1/4}}+\\frac{1}{\\sqrt{n}(1-\\gamma)}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "as long as $\\begin{array}{r}{n\\gtrsim\\mathcal{O}(\\log T\\log\\log T+\\frac{1}{(1-\\gamma)^{2}})}\\end{array}$ . ", "page_idx": 29}, {"type": "text", "text": "Since we know that $t\\approx\\Theta(n)$ , it holds that with probability at least $1-8\\delta$ , ", "page_idx": 29}, {"type": "equation", "text": "$$\n|\\widehat{\\beta}_{1t}-\\beta_{1}|\\lesssim\\mathcal{O}(\\frac{\\sqrt{\\log\\log T}}{t^{\\frac{1}{4}}}+\\frac{1}{\\sqrt{t}(1-\\gamma)}),\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "which ends the proof. ", "page_idx": 29}, {"type": "text", "text": "Now, we have the following proposition to bound the estimation error of $\\beta_{0}$ in a similar manner. ", "page_idx": 29}, {"type": "text", "text": "Proposition E.15. Let ${\\widehat{\\beta}}_{0}$ be the estimated intercept of Algorithm 4 after the $m$ -th episode. Then, under Assumptions 2.1  and 2.2 and conditional on the event that Proposition E.14 holds, we have with probability at least $1-\\delta$ , ", "page_idx": 29}, {"type": "equation", "text": "$$\n|\\widehat{\\beta}_{0}-\\beta_{0}|\\lesssim\\mathcal{O}(\\frac{\\sqrt{\\log\\log T}}{2^{\\frac{m}{4}}}+\\frac{1}{2^{\\frac{m}{2}}(1-\\gamma)})\\,f o r\\,a l l\\,m\\in[\\log_{2}T],\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "then ", "page_idx": 29}, {"type": "equation", "text": "$$\n|\\widehat{\\beta}_{0t}-\\beta_{0}|\\lesssim\\mathcal{O}(\\frac{\\sqrt{\\log\\log T}}{t^{\\frac{1}{4}}}+\\frac{1}{(1-\\gamma)\\sqrt{t}})\\,f o r\\,a l l\\,t\\in[T],\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where subscript $t$ represents which round. ", "page_idx": 29}, {"type": "text", "text": "Proof. Similar to the proof of Proposition E.12, it holds that $\\widehat{\\beta}_{0}=\\mathbb{E}_{n}P^{e}-\\widehat{\\beta}_{1}\\mathbb{E}_{n}Q^{e}$ . Consequently, we have with probability at least $1-\\delta$ , ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{|\\widehat{\\beta}_{0}-\\beta_{0}|\\leq|\\mathbb{E}_{n}P^{\\epsilon}-\\widehat{\\beta}_{1}\\mathbb{E}_{n}Q^{\\epsilon}-\\beta_{0}|}}\\\\ &{\\leq|\\beta_{1}\\mathbb{E}_{n}Q^{\\epsilon}+\\mathbb{E}_{n}\\epsilon_{D}+\\mathbb{E}_{n}\\eta_{D}-\\widehat{\\beta}_{1}\\mathbb{E}_{n}Q^{\\epsilon}|}\\\\ &{\\leq|\\widehat{\\beta}_{1}-\\beta_{1}|*|\\mathbb{E}_{n}Q^{\\epsilon}|+|\\mathbb{E}_{n}\\epsilon_{D}|+|\\mathbb{E}_{n}\\eta_{D}|}\\\\ &{\\leq|\\widehat{\\beta}_{1}-\\beta_{1}|*\\bar{Q}+|\\mathbb{E}_{n}\\epsilon_{D}|+\\mathbb{E}_{n}|\\eta_{D}|}\\\\ &{\\lesssim\\bar{Q}\\ast\\mathcal{O}(\\frac{\\sqrt{\\log\\log T}}{n^{1/4}}+\\frac{1}{\\sqrt{n}(1-\\gamma)})+\\sqrt{\\frac{2\\sigma_{D}^{2}\\log(\\frac{2\\log2T}{\\delta})}{n}}+\\frac{(\\alpha_{1}-\\beta_{1})\\bar{Q}\\sqrt{\\gamma}}{n(1-\\gamma)}}\\\\ &{\\lesssim\\mathcal{O}(\\frac{\\sqrt{\\log\\log T}}{n^{1/4}}+\\frac{1}{\\sqrt{n}(1-\\gamma)}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "The first four inequalities are the same as the one in the proof of Proposition E.12. Moreover, the fifth inequality comes from Proposition E.14 and the last inequality comes from simple order of magnitude analysis. ", "page_idx": 30}, {"type": "text", "text": "Similarly, we have that ", "page_idx": 30}, {"type": "equation", "text": "$$\n|\\widehat{\\beta}_{0t}-\\beta_{0}|\\lesssim\\mathcal{O}(\\frac{\\sqrt{\\log\\log T}}{t^{\\frac{1}{4}}}+\\frac{1}{\\sqrt{t}(1-\\gamma)})\\mathrm{~for~all~}t\\in[T],\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "which ends the proof. ", "page_idx": 30}, {"type": "text", "text": "Propositions E.14 and E.15 show that although we add approaching zero noise to explore the unknown environment, we can asymptotically estimate true parameters $\\beta_{0}$ and $\\beta_{1}$ accurately. Nevertheless, comparing with Propositions E.2, E.7, E.11 and E.12, the convergence rates have decreased to a certain extent, ultimately resulting in a larger regret. ", "page_idx": 30}, {"type": "text", "text": "E.2.2 Proof of Theorem 3.1 ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Proof. Basically, similar to the proof of Theorem 3.2, we decompose the $\\operatorname{SubOpt}(\\cdot)$ into two parts. ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathrm{SubOpt}_{t}(a_{t})}\\\\ &{\\le\\mathbb{E}|a_{t}^{*}*Q^{e}(P_{S t},P_{D t},a_{t}^{*})-a_{t}*Q^{e}(P_{S t},P_{D t},a_{t})|+}\\\\ &{\\quad\\quad\\mathbb{E}|a_{t}*Q^{e}(P_{S t},P_{D t},a_{t})-a_{t}*Q^{e}(P_{S t},P_{D t}^{\\prime},a_{t})|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We know that the optimal service fee at round $t$ is $\\begin{array}{r}{a_{t}^{*}=\\,\\frac{\\beta_{0}-\\alpha_{0}}{2}}\\end{array}$ while the service fee we set is \u03b2 0t2\u2212\u03b10+ \u03f5t. Therefore, considering the Taylor\u2019s expansion, it holds that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{E}|a_{t}^{*}\\ast Q^{e}(P_{S t},P_{D t},a_{t}^{*})-a_{t}\\ast Q^{e}(P_{S t},P_{D t},a_{t})|\\lesssim\\mathcal{O}(\\frac{\\log\\log T}{\\sqrt{t}}+\\frac{1}{t(1-\\gamma)^{2}}+\\epsilon_{t}^{2}),\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where the expectation is taken over $\\epsilon_{D t}$ due to Proposition E.15 and $(a+b+c)^{2}\\leq3(a^{2}+b^{2}+c^{2})$ ", "page_idx": 30}, {"type": "text", "text": "Considering the $m$ -th episode, we know that there are $n=2^{m}$ rounds in it. Since we already have that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\frac{1}{n}|\\sum_{i=1}^{n}\\epsilon_{i}^{2}-\\sum_{i=1}^{n}\\frac{1}{\\sqrt{i}}|\\lesssim\\mathcal{O}(\\frac{\\sqrt{\\log T\\log\\log T}}{n}),\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "with probability at least $1-{\\frac{\\delta}{\\log_{2}T}}$ , it holds that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}\\epsilon_{i}^{2}\\lesssim\\mathcal{O}(\\sqrt{n}+\\sqrt{\\log T\\log\\log T}).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "This will lead to the following result that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\epsilon_{t}^{2}\\lesssim\\mathcal{O}(\\sqrt{T})\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "because of the geometric series summation formula. ", "page_idx": 31}, {"type": "text", "text": "As a result, we get ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathbb{E}|a_{t}^{*}*Q^{e}(P_{S t},P_{D t},a_{t}^{*})-a_{t}*Q^{e}(P_{S t},P_{D t},a_{t})|\\lesssim\\mathcal{O}(\\sqrt{T}\\log\\log T+\\frac{\\log T}{(1-\\gamma)^{2}}),\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "with probability at least $1-\\delta$ . ", "page_idx": 31}, {"type": "text", "text": "As for the second term $\\mathbb{E}|a_{t}*Q^{e}(P_{S t},P_{D t},a_{t})-a_{t}*Q^{e}(P_{S t},P_{D t}^{\\prime},a_{t})|$ , similarly, we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathbb{E}|a_{t}*Q^{e}(P_{S t},P_{D t},a_{t})-a_{t}*Q^{e}(P_{S t},P_{D t}^{\\prime},a_{t})|\\leq\\frac{\\bar{P}\\bar{Q}\\sqrt{\\gamma}\\log_{2}T}{1-\\gamma}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "due to Equation (2). ", "page_idx": 31}, {"type": "text", "text": "Recall that we also need that $\\begin{array}{r}{n\\,\\gtrsim\\,\\mathcal{O}(\\log T\\log\\log T+\\frac{1}{(1-\\gamma)^{2}})}\\end{array}$ to make Proposition E.14 hold. Combining all these terms, it holds that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathrm{Regret}(T)=\\sum_{t=1}^{T}\\mathrm{SubOpt}_{t}(a_{t})}}\\\\ &{\\lesssim\\mathcal{O}(\\sqrt{T}\\log{\\log T}+\\frac{\\log T}{(1-\\gamma)^{2}})+\\frac{\\bar{P}\\bar{Q}\\sqrt{\\gamma}\\log_{2}T}{1-\\gamma}+\\mathcal{O}(\\log T\\log\\log{T}+\\frac{1}{(1-\\gamma)^{2}})}\\\\ &{\\lesssim\\mathcal{O}(\\sqrt{T}\\log\\log{T}+\\frac{\\log T}{(1-\\gamma)^{2}}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "with probability at least $1-10\\delta$ . ", "page_idx": 31}, {"type": "text", "text": "Recall that $\\delta$ is exclusively concealed within $\\log\\log T$ terms. By assigning $\\iota=10\\delta$ , we have reached the following conclusion that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathrm{Regret}(T)\\lesssim\\mathcal{O}(\\sqrt{T}\\log(\\frac{\\log T}{\\iota})+\\frac{\\log T}{(1-\\gamma)^{2}}),\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "which finishes our proof. ", "page_idx": 31}, {"type": "text", "text": "F Omitted Proof in Section 4 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "F.1 Useful Facts for Proving Theorem 4.1 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "First, we introduce some auxiliary lemmas on Kullback\u2013Leibler (KL) divergence. ", "page_idx": 31}, {"type": "text", "text": "Lemma F.1. For two independent normal distributions $p=\\mathcal N(\\mu_{1},\\sigma_{1}^{2})$ and $q=\\mathcal{N}(\\mu_{2},\\sigma_{2}^{2})$ , the $K L$ divergence between them is ", "page_idx": 31}, {"type": "equation", "text": "$$\nK L(p||q)=\\frac{\\sigma_{1}^{2}+(\\mu_{1}-\\mu_{2})^{2}}{2\\sigma_{2}^{2}}+\\log(\\frac{\\sigma_{2}}{\\sigma_{1}})-\\frac{1}{2}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Proof. Recall that $\\begin{array}{r}{K L(p||q)\\,=\\,\\int_{x}p(x)\\log(\\frac{p(x)}{q(x)})d x}\\end{array}$ . It leads to the result with the expression of normal distribution and some calculations. \u53e3 ", "page_idx": 31}, {"type": "text", "text": "Lemma F.2 ([68]). For two probability distributions $p$ , q over space $(\\Omega,{\\mathcal{F}})$ , it holds that for any $A\\in{\\mathcal{F}}$ ", "page_idx": 31}, {"type": "equation", "text": "$$\np(A)+q(A^{c})\\geq{\\frac{1}{2}}e^{-K L(p\\|q)}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "F.2 Proof of Theorem 4.1 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Proof. We only prove the lower bound against myopic buyer which is the loosest one. Let\u2019s consider the following case. We assume the supply curve is $P_{S}=Q_{S}$ and the family of the demand curve parameterized by $\\epsilon$ is $\\{P_{\\epsilon D}=20+5\\epsilon-(1+\\epsilon)Q_{D}+\\epsilon_{\\epsilon D}\\}$ where $\\begin{array}{r}{\\epsilon_{\\epsilon D}\\sim\\dot{\\mathcal N}(0,(\\frac{2+\\epsilon}{2})^{2})}\\end{array}$ . ", "page_idx": 31}, {"type": "text", "text": "The system is depicted completely by $(a_{t},P_{\\epsilon t}^{e})$ , where $Q_{\\epsilon t}^{e}=P_{\\epsilon t}^{e}-a_{t}$ . Recall that $a_{t}$ depends on history up to $t-1$ , namely, $\\mathcal{H}_{t=1}=(a_{1},P_{\\epsilon1}^{e},...,a_{t-1},P_{\\epsilon(t-1)}^{e})$ . From Lemma E.1, we know that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{Q_{\\epsilon}^{e}=\\frac{20+5\\epsilon-a+\\epsilon_{\\epsilon D}}{2+\\epsilon},}\\\\ {P_{\\epsilon}^{e}=\\frac{20+5\\epsilon+(1+\\epsilon)a+\\epsilon_{\\epsilon D}}{2+\\epsilon}.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Then, we know that ( 20+5\u03f52++(\u03f51+\u03f5)a, 14) due to the definition of \u03f5\u03f5D. Moreover, the ex-ante optimal service fee is a\u2217= 202+5\u03f5. ", "page_idx": 32}, {"type": "text", "text": "We now consider the situation when $\\epsilon\\,=\\,0$ and $\\epsilon\\,=\\,T^{-\\frac{1}{4}}$ and we use $\\mathbb{P}_{\\epsilon}$ to denote associated probability measure. With the help of Lemma F.1, it holds that ", "page_idx": 32}, {"type": "equation", "text": "$$\nK L(\\mathbb{P}_{0}||\\mathbb{P}_{\\epsilon})=\\mathbb{E}_{a}[K L(\\mathbb{P}_{0}(\\cdot\\,|\\,a)||\\mathbb{P}_{\\epsilon}(\\cdot\\,|\\,a))\\,|\\,a]=\\mathbb{E}_{a}[\\frac{\\epsilon^{2}(10-a)^{2}}{2(2+\\epsilon)^{2}}],\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where we use the law of iterated expectations. ", "page_idx": 32}, {"type": "text", "text": "During the whole $T$ round, we calculate the number of rounds with $|10-a_{t}|\\geq5\\epsilon$ denoted by $T_{0}$ . With a slight abuse of notation, we use $[T_{0}]$ to denote the corresponding index set. Let\u2019s consider achieved regret with $T_{0}$ case by case. ", "page_idx": 32}, {"type": "text", "text": "On the one hand, when $\\begin{array}{l}{T_{0}\\ \\geq\\ \\frac{T}{2}}\\end{array}$ , it holds that one-round suboptimlality for $\\epsilon\\;=\\;0$ is at least SubOpt0t(at) = (10\u22122a) when $t\\in[T_{0}]$ . Then, we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{Regret}_{0}(T)+\\mathrm{Regret}_{\\epsilon}(T)\\geq\\mathrm{Regret}_{0}(T)}&{}\\\\ {\\geq\\displaystyle\\sum_{t\\in[T_{0}]}\\mathrm{SubOpt}_{0t}(a_{t})}\\\\ {\\geq T_{0}\\frac{25\\epsilon^{2}}{2}}\\\\ {\\geq\\displaystyle\\frac{T}{2}\\frac{25}{2\\sqrt{T}}}\\\\ {\\gtrsim\\Omega(\\sqrt{T}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "The first inequality holds due to the positivity of regret while the second inequality holds due to the positivity of $\\operatorname{SubOpt}(\\cdot)$ . The third inequality holds because when $t$ belongs to $[T_{0}]$ , we have that $|10-a|\\geq5\\epsilon$ while the fourth inequality holds due to the assumption of $T_{0}$ . ", "page_idx": 32}, {"type": "text", "text": "On the other hand, when $\\begin{array}{r}{T_{0}<\\frac{T}{2}}\\end{array}$ , there exists more than $\\textstyle{\\frac{T}{2}}$ rounds such that $|10-a|\\,\\leq\\,5\\epsilon$ . In these rounds, denoted by $[T-T_{0}]$ , it holds that $\\begin{array}{r}{K L(\\mathbb{P}_{0}||\\mathbb{P}_{\\epsilon})\\stackrel{}{\\le}\\frac{25\\epsilon^{4}}{2(2+\\epsilon)^{2}}\\,\\le\\,\\frac{25\\epsilon^{4}}{18}}\\end{array}$ as $\\epsilon\\leq1$ . The first inequality holds with no need to consider the distribution of $a$ because we use a union bound over all $a$ . Then, the total $K L$ among these $T-T_{0}$ rounds is no larger than $\\begin{array}{r}{(T-T_{0})\\frac{25\\epsilon^{4}}{18}\\leq\\frac{25}{18}\\lesssim\\mathcal{O}(1)}\\end{array}$ as $\\epsilon=T^{-1/4}$ in leverage of properties of KL divergence. ", "page_idx": 32}, {"type": "text", "text": "We define an event $A$ that in more than $\\textstyle{\\frac{T}{4}}$ rounds, $\\begin{array}{r}{a_{t}\\geq\\frac{40+5\\epsilon}{4}}\\end{array}$ . Therefore, $A^{c}$ contains at least $\\textstyle{\\frac{T}{4}}$ rounds that $\\textstyle a_{t}<{\\frac{40+5\\epsilon}{4}}$ < 404+5\u03f5. Therefore, it holds that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{[\\gamma_{0}<\\frac{\\tau}{2}]}\\left[\\mathrm{Regret}_{0}(T)+\\mathrm{Regret}_{\\epsilon}(T)\\right]\\ge\\mathbb{E}_{[\\gamma_{0}<\\frac{T}{2}]}\\left(\\displaystyle\\sum_{t\\in[T-T_{0}]}\\mathrm{SubOpt}_{0}(a_{t})+\\mathrm{SubOpt}_{\\epsilon}(a_{t})\\right)}\\\\ &{\\qquad\\qquad\\qquad\\ge\\mathbb{P}_{0}(A\\,|\\,T_{0}<\\frac{T}{2})\\frac{T}{4}\\frac{25\\epsilon^{2}}{32}+\\mathbb{P}_{\\epsilon}(A^{c}\\,|\\,T_{0}<\\frac{T}{2})\\frac{T}{4}\\frac{25\\epsilon^{2}}{16(2+\\epsilon)}}\\\\ &{\\qquad\\qquad\\qquad\\ge\\frac{25\\epsilon^{2}T}{192}(\\mathbb{P}_{0}(A\\,|\\,T_{0}<\\frac{T}{2})+\\mathbb{P}_{\\epsilon}(A^{c}\\,|\\,T_{0}<\\frac{T}{2}))}\\\\ &{\\qquad\\qquad\\qquad\\ge\\frac{25\\sqrt{T}}{192}\\frac{1}{2}e^{-\\frac{25}{18}}}\\\\ &{\\qquad\\qquad\\qquad\\ge\\Omega(\\sqrt{T}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "The first inequality holds due to the positivity of $\\operatorname{SubOpt}(\\cdot)$ while the second inequality holds because the optimal service fee is $\\textstyle{\\frac{20+5\\epsilon}{2}}$ and one-round loss is $\\frac{[a-(20+5\\epsilon)/2]^{2}}{2+\\epsilon}$ . The third inequality holds due to $\\epsilon\\leq1$ while the fourth inequality holds due to Lemma F.2. ", "page_idx": 33}, {"type": "text", "text": "Therefore, it holds that by combining both two cases ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{max}\\{\\mathbb{E}\\mathrm{Regret}_{0}(T),\\mathbb{E}\\mathrm{Regret}_{\\epsilon}(T)\\}}\\\\ &{\\geq\\frac{1}{2}(\\mathbb{E}\\mathrm{Regret}_{0}(T)+\\mathbb{E}\\mathrm{Regret}_{\\epsilon}(T))}\\\\ &{\\ge\\operatorname*{min}\\{\\mathbb{E}_{[\\Gamma_{1},\\Gamma_{0}\\geq\\frac{T}{2}]}\\frac{\\mathrm{Regret}_{0}(T)+\\mathrm{Regret}_{\\epsilon}(T)}{2},\\mathbb{E}_{[\\cdot|T_{0}<\\frac{T}{2}]}\\frac{\\mathrm{Regret}_{0}(T)+\\mathrm{Regret}_{\\epsilon}(T)}{2}\\}}\\\\ &{\\geq\\frac{1}{2}\\operatorname*{min}\\{\\frac{25\\sqrt{T}}{4},\\frac{25\\sqrt{T}}{384}e^{-\\frac{25}{18}}\\}}\\\\ &{\\gtrsim\\Omega(\\sqrt{T}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "due to $\\epsilon=T^{-1/4}$ . \u221aTherefore, for any algorithm against $\\epsilon=0$ and $\\epsilon=T^{-1/4}$ , at least one regret is no smaller than $\\Omega({\\sqrt{T}})$ , which ends the proof. \u53e3 ", "page_idx": 33}, {"type": "text", "text": "There are two key points when constructing such a hard-to-learn instance. When $\\epsilon=0$ , the optimal service fee is 10 and the expected equilibrium is $(P^{e},Q^{e})=(15,5)$ . Then, we choose a special family of demand curves which are all through $(P^{e},Q^{e})=(15,5)$ in expectation. Therefore, when the platform chooses $a=10$ , it cannot infer the value of $\\epsilon$ which ends with large regret for non-zero $\\epsilon$ However, if the platform sets service fee deviated from 10, it will suffer a high loss in the case when $\\epsilon=0$ , ending with large total regret. This introduces an internal exploration-exploitation tradeoff leading to $\\Omega({\\sqrt{T}})$ regret loss bound. However, when $\\sigma_{S}^{2}\\,>\\,0$ , even if the platform sets optimal service fee for $\\epsilon=0$ , the noise $\\epsilon_{S}$ plays the role in exploring the environment. The platform can utilize such information to do casual inference which heavily reduces the regret to $\\widetilde O(1)$ . So, we answer the question that noise helps learning essentially. ", "page_idx": 33}, {"type": "text", "text": "Secondly, from a technical perspective, we adaptively adjust the noise variance in the demand. For the tuned hyperparameter $\\bar{\\epsilon_{\\mathrm{~\\rightmoon~}}}\\bar{T}^{-\\frac{1}{4}}$ and baseline parameter $\\epsilon\\,=\\,0$ , we shrink $\\sigma_{\\epsilon D}^{2}$ from $\\textstyle{\\big(}{\\frac{2+\\epsilon}{2}}{\\big)}^{2}$ to 1. Since the randomness of demand introduces noise into the equilibrium retrospectively, the equilibrium price and quantity appear nebula-like. The shrinkage of variance results in greater overlap between the two nebulae, which complicates the learning process. Mathematically, it causes the equilibrium price to have the same variance regardless of the value of $\\epsilon$ , entailing smaller KL divergence between different choices of demand. To sum up, these two crucial aspects make up this delicate and non-trivial example. ", "page_idx": 33}, {"type": "text", "text": "F.3 A Weaker Version of Theorem 4.2 ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Note that there is a gap between the proposed upper bounds and corresponding lower bounds when $\\sigma_{S}^{2}\\lesssim\\mathcal{O}(\\frac{1}{\\sqrt{T}})$ , indicating essential hardness in this interval. Therefore, we first give the following lemma which is a little bit weaker than the original Theorem 4.2 but depicts the characteristics in this area. Here, we use a constructive proof and then we give a complete information-theoretic proof of Theorem 4.2 in Appendix F.4. ", "page_idx": 33}, {"type": "table", "img_path": "Tnl2K6Iz9j/tmp/fd2a29e4d06a8bb6317e321bebd68d7be3872e4d7712c11b517b50f9e1b1e73a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 33}, {"type": "text", "text": "Lemma F.3. It\u2019s impossible to find an algorithm such that there exists some $\\sigma_{S}^{2}\\lesssim o\\big(\\frac{1}{\\sqrt{T}}\\big)$ and the corresponding expectation of regret belongs to $o(\\sqrt{T})$ , namely, ERegre $\\mathbf{\\nabla}:(T)\\lesssim o(\\sqrt{T})$ . ", "page_idx": 34}, {"type": "text", "text": "Proof. We have proof by contradiction to demonstrate this lemma. We assume that when the supply randomness is $\\begin{array}{r}{\\bar{\\sigma_{S}^{2}}\\lesssim o\\bar{(\\frac{1}{\\sqrt{T}})}}\\end{array}$ , there exists an algorithm denoted by Oracle whose expected regret belongs to $o(\\sqrt{T})$ . We use $H_{t}$ to represent history until round $t$ , namely, $\\begin{array}{r}{H_{t}=\\{a_{\\tau},P_{\\tau}^{e},Q_{\\tau}^{e}\\}_{\\tau=1}^{t}}\\end{array}$ . Therefore, $0\\mathtt{r a c l e}(\\cdot,\\cdot)$ is a mapping from $H_{t-1}\\times P_{S t}$ to $\\mathbb{R}$ . Now, we are going to show tha\u221at if existing such Oracle, we can find a variant of it called Oracle-Variant which achieves $o(\\sqrt{T})$ without supply randomness, contradicting Theorem 4.1. We assume $\\gamma\\,=\\,0$ which is enough to construct a counterexample. We present the detail of Oracle-Variant in Algorithm 6. The input is the number of rounds $T$ and the supply randomness $\\sigma_{S}^{2}\\lesssim o\\big(\\frac{1}{\\sqrt{T}}\\big)$ associated with Oracle. ", "page_idx": 34}, {"type": "text", "text": "We use $\\widetilde{a_{t}}^{*}$ to denote the optimal service fee when there exists $\\epsilon_{S t}\\sim\\mathcal{N}(0,\\sigma_{S}^{2})$ and $a_{t}^{*}$ to denote the one without randomness. Considering the following counterfactual case, if we adopt $\\widetilde{a_{t}}$ when there exists $\\epsilon_{S t}$ , the equilibrium price and quantity will be ", "page_idx": 34}, {"type": "equation", "text": "$$\n(\\widetilde{P_{t}^{e}},\\widetilde{Q_{t}^{e}})=(\\frac{\\beta_{0}-\\alpha_{0}-\\widetilde{a}_{t}+\\epsilon_{D t}-\\epsilon_{S t}}{\\alpha_{1}-\\beta_{1}},\\frac{\\alpha_{1}\\beta_{0}-\\alpha_{0}\\beta_{1}-\\beta_{1}\\widetilde{a}_{t}+\\alpha_{1}\\epsilon_{D t}-\\beta_{1}\\epsilon_{S t}}{\\alpha_{1}-\\beta_{1}})\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "according to Lemma E.9. Fortunately, choosing $a_{t}=\\widetilde{a_{t}}+\\epsilon_{S t}$ leads to the same equilibrium price and quantity without randomness, that is to say, $(P_{t}^{e},Q_{t}^{e})=(\\widetilde{P_{t}^{e}},\\widetilde{Q_{t}^{e}})$ . ", "page_idx": 34}, {"type": "text", "text": "Since we use Oracle, the regret when baseline is $\\widetilde{a_{t}}^{*}$ is $o(\\sqrt{T})$ . Therefore, we know that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\sum_{t=1}^{T}(\\widetilde{a_{t}}-\\widetilde{a_{t}}^{*})^{2}]\\lesssim o(\\sqrt{T}),\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "according to the proof of Theorem E.1. Now, let\u2019s give a decomposition of $(a_{t}-a_{t}^{\\ast})^{2}$ . It holds that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(a_{t}-a_{t}^{*})^{2}\\lesssim\\mathcal{O}((a_{t}-\\widetilde{a_{t}})^{2})+\\mathcal{O}((\\widetilde{a_{t}}-\\widetilde{a_{t}}^{*})^{2})+\\mathcal{O}((a_{t}^{*}-\\widetilde{a_{t}}^{*})^{2}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "according to simple algebra. For the first term, we know that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\sum_{t=1}^{T}\\mathcal{O}((a_{t}-\\widetilde{a}_{t})^{2})]\\lesssim\\mathbb{E}[\\mathcal{O}(\\sum_{t=1}^{T}\\epsilon_{S t}^{2})]\\lesssim\\mathcal{O}(T\\sigma_{S}^{2}),\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "from the implementation of Algorithm 6. For the second term, it holds that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\sum_{t=1}^{T}\\mathcal{O}((\\widetilde{a_{t}}-\\widetilde{a_{t}}^{*})^{2})]\\lesssim o(\\sqrt{T}).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "For the last term, we know that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\sum_{t=1}^{T}\\mathcal{O}((a_{t}^{*}-\\widetilde{a_{t}}^{*})^{2})]\\lesssim\\mathbb{E}[\\sum_{t=1}^{T}\\mathcal{O}(\\epsilon_{S t}^{2})]\\lesssim\\mathcal{O}(T\\sigma_{S}^{2}),\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "because $\\begin{array}{r}{|a_{t}^{*}-\\widetilde{a_{t}}^{*}|=|\\frac{\\epsilon_{S t}}{2}|}\\end{array}$ according to Lemma E.1. ", "page_idx": 34}, {"type": "text", "text": "Therefore, we know that without supply randomness, the expected regret of Algorithm 6 satisfying ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathrm{Regret}(T)\\stackrel{}{\\sim}o(\\sqrt{T})+\\mathcal{O}(T\\sigma_{S}^{2})\\stackrel{}{\\sim}o(\\sqrt{T}).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "The last inequality holds due to $\\begin{array}{l}{\\sigma_{S}^{2}~\\lesssim~o\\big(\\frac{1}{\\sqrt{T}}\\big)}\\end{array}$ . However, from Theorem 4.1, we know that ERegret $(T)\\,\\gtrsim\\,\\Omega(\\sqrt{T})$ . It then causes a contradiction. We subsequently conclude that the existence of Oracle is untenable, which ends the proof. \u53e3 ", "page_idx": 34}, {"type": "text", "text": "F.4 Proof of Theorem 4.2 ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Proof. Let $\\beta=(\\beta_{0},\\beta_{1})$ and $\\epsilon_{D t}\\sim\\mathcal{N}(0,\\sigma_{D}^{2})$ with $\\sigma_{D}=(\\alpha_{1}-\\beta_{1})\\sigma$ for some $\\sigma>0$ . To express concisely, we use subscript $\\beta$ and superscript $\\pi$ to denote parameter and policy, respectively. We know that ", "page_idx": 34}, {"type": "equation", "text": "$$\na_{t}^{*}(\\beta)=\\frac{\\beta_{0}-\\alpha_{0}-\\epsilon_{S t}}{2}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Also, when the decision maker sets any $a_{t}$ , we observe ", "page_idx": 35}, {"type": "equation", "text": "$$\nQ_{t}^{e}=\\frac{\\beta_{0}+\\epsilon_{D t}-a_{t}-\\alpha_{0}-\\epsilon_{S t}}{\\alpha_{1}-\\beta_{1}}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Note that the only unknown randomness comes from $\\epsilon_{D t}$ since $\\epsilon_{S t}$ is always observable. Meanwhile, upon deciding $a_{t}$ and observing $Q_{t}^{e}$ and $\\epsilon_{S t}$ , the price from both the demand and supply side can be uniquely decided. Therefore, the log-likelihood prior to time $t$ can be calculated as: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathcal{L}_{t-1}(\\beta)=\\sum_{i=1}^{t-1}-\\frac{1}{2\\sigma^{2}}\\left(Q_{i}^{e}-\\frac{\\beta_{0}-a_{i}-\\alpha_{0}-\\epsilon_{S i}}{\\alpha_{1}-\\beta_{1}}\\right)^{2}+C,\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where $C$ is a constant only dependent on $\\{a_{i},Q_{i}^{e}\\}_{i=1}^{t-1}$ and not dependent on $\\beta$ . The fisher information matrix prior to time $t$ can be calculated as ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathcal{F}_{t-1}^{\\pi}(\\beta)=\\mathbb{E}_{\\beta}^{\\pi}\\left[-\\partial^{2}\\mathcal{L}_{t-1}(\\beta)/\\partial\\beta^{2}\\right]=\\frac{1}{\\sigma^{2}}\\mathbb{E}_{\\beta}^{\\pi}\\left[\\sum_{i=1}^{t-1}\\left[\\frac{\\frac{1}{(\\alpha_{1}-\\beta_{1})^{2}}}{-\\frac{\\beta_{0}-a_{t}-\\alpha_{0}-\\epsilon_{S t}}{(\\alpha_{1}-\\beta_{1})^{2}}}\\right.\\right.\\left.-\\frac{\\beta_{0}-a_{t}-\\alpha_{0}-\\epsilon_{S t}}{(\\alpha_{1}-\\beta_{1})^{2}}\\right]\\right].\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Let $\\lambda$ be an absolutely continuous density on $\\Theta$ , taking positive values on the interior of $\\Theta$ and zero on its boundary (see, e.g., Keskin and Zeevi [44]). Then the multivariate Van Trees inequality [36] implies that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\lambda}\\left[\\mathbb{E}_{\\beta}^{\\pi}\\left[(a_{t}-a_{t}^{*}(\\beta))^{2}\\right]\\right]\\ge\\frac{\\left(\\mathbb{E}_{\\lambda}\\left[C(\\beta)(\\partial a_{t}^{*}(\\beta)/\\partial\\beta)^{\\top}\\right]\\right)^{2}}{\\mathbb{E}_{\\lambda}\\left[C(\\beta)\\mathcal{F}_{t-1}^{\\pi}(\\beta)C(\\beta)^{\\top}\\right]+\\widetilde{\\mathcal{F}}(\\lambda)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Let $C(\\beta)=[(\\beta_{0}-\\alpha_{0})/2$ 1]. Then we have ", "page_idx": 35}, {"type": "equation", "text": "$$\nC(\\beta)(\\partial a_{t}^{*}(\\beta)/\\partial\\beta)^{\\top}=[(\\beta_{0}-\\alpha_{0})/2\\quad1][1/2\\quad0]^{\\top}=(\\beta_{0}-\\alpha_{0})/4=\\Omega(1)\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "and ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{C(\\beta)\\mathcal{F}_{t-1}^{\\pi}(\\beta)C(\\beta)^{\\top}=\\frac{1}{\\sigma^{2}\\left(\\alpha_{1}-\\beta_{1}\\right)^{2}}\\mathbb{E}_{\\beta}^{\\pi}\\left[\\displaystyle\\sum_{i=1}^{t-1}\\left(\\frac{\\beta_{0}-\\alpha_{0}}{2}-(\\beta_{0}-a_{t}-\\alpha_{0}-\\epsilon_{S t})\\right)^{2}\\right]}\\\\ &{\\qquad\\qquad\\qquad=\\frac{1}{\\sigma^{2}\\left(\\alpha_{1}-\\beta_{1}\\right)^{2}}\\displaystyle\\sum_{i=1}^{t-1}\\mathbb{E}_{\\beta}^{\\pi}\\left[\\left(a_{i}-\\frac{\\beta_{0}-\\alpha_{0}-\\epsilon_{S i}}{2}+\\frac{\\epsilon_{S i}}{2}\\right)^{2}\\right]}\\\\ &{\\qquad\\qquad\\qquad=\\mathcal{O}\\left(\\displaystyle\\sum_{i=1}^{t-1}\\left(\\mathbb{E}_{\\beta}^{\\pi}\\left[(a_{i}-a_{i}^{*}(\\beta))^{2}\\right]+\\sigma_{S}^{2}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Plugging the inequalities above into Inequality (3), we know that there exists a positive constant $c$ such that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\lambda}\\left[\\mathbb{E}_{\\beta}^{\\pi}\\left[(a_{t}-a_{t}^{*}(\\beta))^{2}\\right]\\right]\\ge\\frac{2c}{1+\\sum_{i=1}^{t-1}\\mathbb{E}_{\\lambda}\\left[\\mathbb{E}_{\\beta}^{\\pi}\\left[(a_{i}-a_{i}^{*}(\\beta))^{2}\\right]\\right]+(t-1)\\sigma_{S}^{2}}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "From now on, for notation simplicity, we denote $\\begin{array}{r}{\\Delta^{[t,T]}=\\sum_{i=t}^{T}\\mathbb{E}_{\\lambda}\\left[\\mathbb{E}_{\\beta}^{\\pi}\\left[(a_{i}-a_{i}^{*}(\\beta))^{2}\\right]\\right]}\\end{array}$ . Summing the formula above from $t$ to $T$ , we know that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\Delta^{[t,T]}\\ge\\sum_{i=t-1}^{T-1}\\frac{2c}{1+\\Delta^{[1,i]}+\\sigma_{S}^{2}\\cdot i}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Then $\\operatorname*{sup}_{\\beta}\\operatorname{Regret}_{\\beta}^{\\pi}(T)=\\Theta(\\Delta^{[1,T]})$ . We show the lower bound in two cases. ", "page_idx": 35}, {"type": "text", "text": "(a). $T\\leq1+1/\\sigma_{S}^{4}$ . Then from Inequality (4) we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\Delta^{[1,T]}\\ge\\sum_{i=1}^{T-1}\\frac{2c}{1+\\Delta^{[1,i]}+\\sigma_{S}^{2}\\cdot i}\\ge\\frac{2c(T-1)}{1+\\Delta^{[1,T]}+\\sqrt{T}}\\ge\\frac{c T}{\\Delta^{[1,T]}+2\\sqrt{T}}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Therefore, ", "page_idx": 36}, {"type": "equation", "text": "$$\n(\\Delta^{[1,T]}+\\sqrt{T})^{2}\\geq(1+c)T,\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "which indicates that there exists a constant $c_{0}=\\sqrt{1+c}-1>0$ such that $\\Delta^{[1,T]}\\ge c_{0}\\sqrt{T}$ . (b). $T>1+1/\\sigma_{S}^{4}$ . Let $K$ be the smallest positive integer such that $2^{K}\\geq1+1/\\sigma_{S}^{4}$ . Then $2^{K-1}<$ $1+1/\\sigma_{S}^{4}$ , which indicate that ", "page_idx": 36}, {"type": "equation", "text": "$$\nK+1=K-1+2<\\log_{2}(1+1/\\sigma_{S}^{4})+2\\leq6(1+\\log_{+}(1/\\sigma_{S})).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "From (a) we know that for $t=2^{K}$ , we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\Delta^{[1,t]}\\geq c_{0}\\sqrt{1+1/\\sigma_{S}^{4}}\\geq c_{0}\\frac{1}{\\sigma_{S}^{2}}\\geq\\frac{c_{1}}{1+\\log_{+}(1/\\sigma_{S})}\\frac{K}{\\sigma_{S}^{2}},\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where $c_{1}$ is a small positive constant irrelevant with $K$ and $\\sigma_{S}$ such that $c_{1}(2+6c_{1})\\leq c$ . Now we use induction. Suppose we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\Delta^{[1,2^{k}]}\\geq\\frac{c_{1}}{1+\\log_{+}(1/\\sigma_{S})}\\frac{k}{\\sigma_{S}^{2}}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "for some $k\\geq K$ . From Inequality (4) we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\Delta^{[2^{k}+1,2^{k+1}]}\\ge\\frac{(2^{k+1}-2)c}{1+\\Delta^{[1,2^{k+1}]}+\\sigma_{S}^{2}\\cdot(2^{k+1}-1)}\\ge\\frac{c}{2^{-k}+\\frac{\\Delta^{[1,2^{k+1}]}}{2^{k}}+\\frac{3}{2}\\sigma_{S}^{2}}\\ge\\frac{c}{\\frac{\\Delta^{[1,2^{k+1}]}}{2^{k}}+2\\sigma_{S}^{2}}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Note that we have utilized the following inequality: ", "page_idx": 36}, {"type": "equation", "text": "$$\n2^{-k}\\leq\\frac{\\sigma_{S}^{4}}{1+\\sigma_{S}^{4}}\\leq\\frac{\\sigma_{S}^{4}}{2\\sigma_{S}^{2}}=\\sigma_{S}^{2}/2.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "As a result, ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\Delta^{[1,2^{k+1}]}\\geq\\frac{c_{1}}{1+\\log_{+}(1/\\sigma_{S})}\\frac{k}{\\sigma_{S}^{2}}+\\frac{c}{\\frac{\\Delta^{[1,2^{k+1}]}}{2^{k}}+2\\sigma_{S}^{2}}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "If \u2206[1,2k+1] <1+log+c1(1/\u03c3S)k\u03c3+2S1 , then ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{c}{\\frac{\\Delta^{[1,2^{k+1}]}}{2^{k}}+2\\sigma_{S}^{2}}>\\frac{c}{\\frac{1}{1+\\log_{+}(1/\\sigma)s}\\frac{k+1}{2^{k}\\sigma_{S}^{2}}+2\\sigma_{S}^{2}}}\\\\ {\\geq\\frac{c}{\\frac{c_{1}}{1+\\log_{+}(1/\\sigma_{S})}\\frac{K+1}{2^{k}\\sigma_{S}^{2}}+2\\sigma_{S}^{2}}}\\\\ {\\geq\\frac{c}{(2+6c_{1})\\sigma_{S}^{2}}}\\\\ {\\geq\\frac{c_{1}}{\\sigma_{S}^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "This causes a contradiction. Therefore, for any $k\\ \\geq\\ K$ , Inequality (5) holds. Now select any $T>1+1/\\sigma_{S}^{4}$ . If $T<2^{K}$ , then ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\Delta^{[1,T]}\\geq c_{0}\\sqrt{1+1/\\sigma_{S}^{4}}\\geq\\frac{c_{1}}{1+\\log_{+}(1/\\sigma_{S})}\\frac{K}{\\sigma_{S}^{2}}\\geq\\frac{c_{1}}{1+\\log_{+}(1/\\sigma_{S})}\\frac{\\log T}{\\sigma_{S}^{2}}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "If $T\\geq2^{K}$ , choose $k=\\lfloor\\log_{2}T\\rfloor$ , then ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\Delta^{[1,T]}\\geq\\Delta^{[1,2^{k}]}\\geq\\frac{c_{1}}{1+\\log_{+}(1/\\sigma_{S})}\\frac{k}{\\sigma_{S}^{2}}\\geq\\frac{c_{1}/2}{1+\\log_{+}(1/\\sigma_{S})}\\frac{\\log T}{\\sigma_{S}^{2}}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "F.5 Proof of Theorem 4.3 ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Proof. Note that the buyer\u2019s strategic behavior only causes extra $\\mathcal{O}(\\frac{\\log T}{\\sigma_{S}^{2}})$ in Theorem 3.2 and ${\\mathcal{O}}(\\log T)$ in Theorem 3.1. Since terms from estimation errors dominate them, we without loss of generality assume that the buyer is myopic i.e., $\\gamma=0$ for simplicity without weakening our results. From traditional results of hypothesis test [22], we know we need $\\begin{array}{r}{n\\,\\approx\\,\\Theta((\\frac{z_{\\alpha}\\sigma}{E})^{2})}\\end{array}$ samples to distinguish $\\mathbb{H}_{0}$ and $\\mathbb{H}_{1}$ , where $z_{\\alpha}$ , $\\sigma$ and $E$ are the $z$ -score, the population standard deviation and the acceptable margin of error, respectively. Here, \u221awe know that $\\dot{\\sigma}\\overset{\\cdot}{\\sim}\\Theta(\\sigma_{S}^{2})$ since $\\mathrm{Var}(\\epsilon_{S t}^{2})\\approx\\Theta(\\sigma_{S}^{4})$ . Besides, we set $\\begin{array}{r}{\\alpha\\approx\\Theta(\\frac{1}{T})}\\end{array}$ and then $z_{\\alpha}\\approx\\Theta(\\sqrt{\\log T})$ due to Lemmas D.1 and D.2. The largest tolerance of error is $\\begin{array}{r}{E\\,\\stackrel{{}}{\\sim}\\,\\Theta(\\frac{1}{\\sqrt{T}})}\\end{array}$ . Therefore, by setting $T_{0}\\,\\approx\\,\\Theta(\\log T)$ with some accurately designed constants, we can distinguish $\\mathbb{H}_{0}$ and $\\mathbb{H}_{1}$ with probability at least $1-\\Theta\\big(\\frac{1}{T}\\big)$ . ", "page_idx": 37}, {"type": "text", "text": "Then, in the case of $\\mathbb{H}_{0}$ , we will add some human-made noise referring to Algorithm 5. Then, the randomness of $a_{t}$ comes from two parts. One is from original $\\epsilon_{S t}$ . The other one comes from artificially added noise. Therefore, the variance of $a_{t}$ is no smaller than the one in the proof of Theorem 3.1 and the estimation of parameters will be more accurate. Subsequently, we can bound three parts of regret which construct the total $\\mathrm{Regret}(T)$ . The first one comes from the gap between $a_{t}$ and $a_{t}^{*}$ . It will lead to an $\\widetilde{\\mathcal{O}}(\\sqrt{T})$ regret. The second part comes from the fact that the probability that some inequalities don\u2019t hold is at most $\\mathcal{O}(\\frac{1}{T})$ . The last part comes from the first $T_{0}$ rounds. Therefore, it holds that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathbb{E R e g r e t}(T)\\lesssim\\tilde{\\mathcal{O}}(\\sqrt{T})+T*\\mathcal{O}(\\frac{1}{T})+\\mathcal{O}(T_{0})\\lesssim\\tilde{\\mathcal{O}}(\\sqrt{T}).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Recall that we will suffer st most extra ${\\mathcal{O}}(\\log T)$ from a strategic buyer which is indeed a subdominant term. It covers cases when \u03c32S \u2272o( \u221a1 ). ", "page_idx": 37}, {"type": "text", "text": "In the case of $\\mathbb{H}_{1}$ , it will reduce to Theorem E.1 directly. The regret consists of three parts as well. The first part is $\\widetilde{\\mathcal{O}}(\\frac{1}{\\sigma_{S}^{2}})$ inherited from Theorem E.1. The second term comes from the $\\mathcal{O}(\\frac{1}{T})$ probability of some inequalities being violated. The last term is from the first $T_{0}$ rounds, yielding ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathbb{E R e g r e t}(T)\\lesssim\\tilde{\\mathcal{O}}(\\frac{1}{\\sigma_{S}^{2}})+T\\ast\\mathcal{O}(\\frac{1}{T})+\\mathcal{O}(T_{0})\\lesssim\\tilde{\\mathcal{O}}(\\frac{1}{\\sigma_{S}^{2}}).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Note that we will suffer an extra $\\mathcal{O}(\\frac{\\log T}{\\sigma_{S}^{2}})$ regret facing a far-sighted buyer. This case contains all situations when $\\sigma_{S}^{2}\\gtrsim\\omega(\\frac{1}{\\sqrt{T}})$ . To be specific, from the proof of Theorems 3.1 and 3.2, we know we hide ${\\mathcal{O}}(\\log^{2}T)$ for $\\mathbb{H}_{1}$ but only ${\\mathcal{O}}(\\log T)$ for $\\mathbb{H}_{0}$ . ", "page_idx": 37}, {"type": "text", "text": "Finally, when $\\sigma_{S}^{2}\\approx\\Theta(\\frac{1}{\\sqrt{T}})$ , the policy we use might be a mixture of the one under $\\mathbb{H}_{0}$ and the one under $\\mathbb{H}_{1}$ . Luckily, both of them will lead to an at most $\\widetilde{\\mathcal{O}}(\\sqrt{T})$ expected regret. ", "page_idx": 37}, {"type": "text", "text": "To sum up, we know that the expected regret of Algorithm 3 satisfies ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathbb{E R e g r e t}(T)\\lesssim\\left\\{{\\tilde{\\mathcal{O}}}({\\sqrt{T}})\\quad{\\mathrm{~when~}}\\sigma_{S}^{2}\\lesssim{\\mathcal{O}}({\\frac{1}{\\sqrt{T}}})\\right.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Let\u2019s now turn to the tightness of lower bounds. From Theorems 4.1 and 4.2, we have that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathbb{E R e g r e t}(T)\\gtrsim\\left\\{\\begin{array}{l l}{\\Omega(\\sqrt{T})}&{\\quad\\mathrm{~when~}\\sigma_{S}^{2}=0}\\\\ {\\Omega(\\sqrt{T})}&{\\quad\\mathrm{when~}\\sigma_{S}^{2}\\lesssim\\mathcal{O}(\\frac{1}{\\sqrt{T}})}\\\\ {\\Omega(\\frac{1}{\\sigma_{S}^{2}})}&{\\quad\\mathrm{when~}\\sigma_{S}^{2}\\gtrsim\\Omega(\\frac{1}{\\sqrt{T}}).}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Therefore, we conclude that our regret lower bounds are tight and Algorithm 3 is optimal regardless of constants and logarithmic terms. ", "page_idx": 37}, {"type": "text", "text": "Moreover, we instantly obtain the phase transition of regret and it finishes our proof. ", "page_idx": 37}, {"type": "text", "text": "F.6 A Lower Bound When $\\gamma=1$ ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "We close off this section by giving an easy reduction to the $\\Omega(T)$ lower bound in Amin et al. [2] when the buyer is patient, i.e., equipped with discount rate 1. To determine the optimal dependency on $\\gamma$ is s beyond the scope of the current work and we leave it as an interesting future direction. ", "page_idx": 37}, {"type": "text", "text": "For each $\\beta_{0}$ , we have a unique optimal service fee $a^{*}$ corresponding to it. In the meanwhile, $a^{*}$ is the marginal willingness to pay and we focus on the pricing problem for those with value $a^{*}$ . From Theorem 3 in Amin et al. [2], we know there exists a value $v$ , i.e., $a^{*}$ here, an $\\Omega(T)$ regret is inevitable for any algorithm when the buyer is patient, namely, $\\gamma=1$ . Through the bijection between $\\beta_{0}$ and $a^{*}$ , we know there exists some $\\beta_{0}$ that makes this pricing problem unlearnable. ", "page_idx": 38}, {"type": "text", "text": "G Details of Numerical Experiments ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "We choose parameters $\\gamma\\,=\\,0$ , $\\alpha_{0}\\,=\\,1$ , $\\alpha_{1}\\,=\\,1$ , $\\beta_{0}\\,=\\,5$ , $\\beta_{1}\\,=\\,-1$ and $\\epsilon_{D}\\sim\\mathcal{N}(0,1)$ in both experiments. ", "page_idx": 38}, {"type": "text", "text": "In Section 5, we notice that the increment of regret has several stages. At the beginning of each stage, the regret will increase relatively rapidly, and then the rate decrease gradually. The reason behind this phenomenon is the way we implement $\\mathtt{A c t}(\\cdot,\\cdot)$ . We actually update our pricing policy at the start of each stage. Therefore, the noise we add has a variance that starts from 1 and decreases according to the inverse square root law. ", "page_idx": 38}, {"type": "text", "text": "For the second experiment, there are some hyper-parameters when implementing Algorithm 3. First, sample m0 ean of \u03f52S i2s  smaller than\u221a10T and H1 otherwise. Among the 1000 choices  oHf \u03c32S , tHh0e re are about 100 cases where $\\mathbb{H}=\\mathbb{H}_{0}$ , nearly to say, when $\\sigma_{S}^{2}\\leq0.1$ . These choices of hyper-parameters are not essential and not well-tuned though enough for our experiments. When using LOWESS, we set the hyperparameter \u201cfraction\u201d as 0.15. It means that we use $15\\%$ nearest points when fitting the needed function value. Similarly, it is not carefully selected but rather folklore. Besides, note that $\\log T\\approx9.21$ so regret fluctuating between 300 and 600 around $\\sigma_{S}^{2}=0.1$ should be acceptable. ", "page_idx": 38}, {"type": "text", "text": "In conclusion, the first experiment testifies regret upper bounds in Theorems 3.1 and 3.2 while the second experiment validates results about the tightness of our lower bounds and the existence of phase transition in Theorem 4.3 in a concrete manner. ", "page_idx": 38}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: The main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 39}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Justification: The paper discusses the limitations of the work performed by the authors. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 39}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: The paper provides the full set of assumptions and a complete (and correct) proof for each theoretical result. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 40}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: The paper fully discloses all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and conclusions of the paper. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 40}, {"type": "text", "text": "5. Open access to data and code ", "page_idx": 40}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: The paper provides open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material. Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 41}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: The paper specifies all the training and test details necessary to understand the results. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 41}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: The paper reports error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 41}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 42}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: The paper provides sufficient information on the computer resources needed to reproduce the experiments. All experiments can be conducted on a personal computer. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 42}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: The research conducted in the paper conforms, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 42}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: The paper discusses both potential positive societal impacts and negative societal impacts of the work performed. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 43}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 43}, {"type": "text", "text": "Justification: The paper poses no such risks. Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 43}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 43}, {"type": "text", "text": "Justification: The paper does not use existing assets. Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 43}, {"type": "text", "text": "", "page_idx": 44}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 44}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 44}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 44}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 44}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 44}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 44}, {"type": "text", "text": "", "page_idx": 45}]