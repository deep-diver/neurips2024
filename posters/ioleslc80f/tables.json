[{"figure_path": "iOleSlC80F/tables/tables_7_1.jpg", "caption": "Table 1: 3D detection results on ScanNet V2 [7] and SUN RGB-D [39]. We compare 3DET-Mamba against open-source methods that directly process point clouds, using PointNet++[33] (PN) and Transformer[40] (Tran.) as backbones. 3DET-Mamba employs the same number of key points and queries as 3DETR [26]. 3DET-Mamba\u2020 doubles the number of key points and queries to model longer sequences of point clouds. Compared to 3DETR [26], 3DET-Mamba achieves superior performance.", "description": "This table presents a comparison of 3D object detection results on the ScanNet V2 and SUN RGB-D datasets for several methods, including the proposed 3DET-Mamba.  It highlights the performance improvements achieved by 3DET-Mamba compared to existing state-of-the-art methods, specifically 3DETR.  The table also shows the impact of increasing the number of key points and queries on the performance of 3DET-Mamba.", "section": "4.2 Comparisons on 3D Object Detection"}, {"figure_path": "iOleSlC80F/tables/tables_7_2.jpg", "caption": "Table 2: Effect of our designed point cloud encoder which contains Inner and Dual Mamba blocks. We compare the performance of encoder combinations using Pointnet++ [33] and Transformer [26] with Inner and Dual Mamba blocks, with results showing that the proposed Inner Mamba & Dual Mamba achieves the best performance.", "description": "This table presents an ablation study comparing different combinations of encoders in the 3DET-Mamba model.  It shows the impact of using either PointNet++ or Inner Mamba for local feature extraction, combined with either a Transformer or Dual Mamba for global context modeling. The results demonstrate that the combination of Inner Mamba and Dual Mamba yields the best performance, measured by mAP@0.25 and mAP@0.5.", "section": "4.3 Ablation Studies"}, {"figure_path": "iOleSlC80F/tables/tables_8_1.jpg", "caption": "Table 3: Effect of Dual Mamba block. We compare it with the original Mamba block [10] and the Bi-directional Mamba block [60, 22].", "description": "This table presents the ablation study on the Dual Mamba block. The performance of using only the original Mamba, the bidirectional Mamba, and the proposed Dual Mamba is compared using mAP@0.25 and mAP@0.5 metrics. The result shows that the Dual Mamba block outperforms other methods.", "section": "4.3 Ablation Studies"}, {"figure_path": "iOleSlC80F/tables/tables_8_2.jpg", "caption": "Table 4: Effect of Query-aware Mamba. We compare it with the 3DETR [26] decoder and an implementation based on the original Mamba.", "description": "This table presents the ablation study on the decoder part of the 3DET-Mamba model. It compares the performance of three different decoder designs: a standard Transformer decoder, a decoder using the original Mamba model, and the proposed Query-aware Mamba decoder. The results show a significant improvement in mAP@0.25 and mAP@0.5 when using the Query-aware Mamba decoder, highlighting its effectiveness in effectively modeling the relationship between learnable queries and scene features.", "section": "4.3 Ablation Studies"}]