[{"type": "text", "text": "Aggregating Quantitative Relative Judgments: From Social Choice to Ranking Prediction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yixuan Even Xu Carnegie Mellon University yixuanx@cs.cmu.edu ", "page_idx": 0}, {"type": "text", "text": "Hanrui Zhang Chinese University of Hong Kong hanrui@cse.cuhk.edu.hk ", "page_idx": 0}, {"type": "text", "text": "Yu Cheng Brown University yu_cheng@brown.edu ", "page_idx": 0}, {"type": "text", "text": "Vincent Conitzer Carnegie Mellon University conitzer@cs.cmu.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Quantitative Relative Judgment Aggregation (QRJA) is a new research topic in (computational) social choice. In the QRJA model, agents provide judgments on the relative quality of different candidates, and the goal is to aggregate these judgments across all agents. In this work, our main conceptual contribution is to explore the interplay between QRJA in a social choice context and its application to ranking prediction. We observe that in QRJA, judges do not have to be people with subjective opinions; for example, a race can be viewed as a \u201cjudgment\u201d on the contestants\u2019 relative abilities. This allows us to aggregate results from multiple races to evaluate the contestants\u2019 true qualities. At a technical level, we introduce new aggregation rules for QRJA and study their structural and computational properties. We evaluate the proposed methods on data from various real races and show that QRJA-based methods offer effective and interpretable ranking predictions. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In voting theory, each voter ranks a set of candidates, and a voting rule maps the vector of rankings to either a winning candidate or an aggregate ranking of all the candidates. There has been significant interaction between computer scientists interested in voting theory and the learning-to-rank community. The learning-to-rank community is interested in problems such as ranking webpages in response to a search query, or ranking recommendations to a user (see, e.g., Liu [2009]). Another problem of interest is to aggregate multiple rankings into a single one, for example combining the ranking results from different algorithms (\u201cvoters\u201d) into a single meta-ranking. While the interests of the communities may differ, e.g., the learning-to-rank community is less concerned about strategic aspects of voting, a natural intersection point for these two communities is a model where there is a latent \u201ctrue\u201d ranking of the candidates, of which all the votes are just noisy observations. Consequently, it is natural to try to estimate the true ranking based on the received rankings, and such an estimation procedure corresponds to a voting rule. (See, e.g., Young [1995]; Conitzer and Sandholm [2005]; Meila et al. [2007]; Conitzer et al. [2009]; Caragiannis et al. [2013]; Soufiani et al. [2014]; Xia [2016], and Elkind and Slinko [2015] for an overview.) ", "page_idx": 0}, {"type": "text", "text": "Voting rules are just one type of mechanism in the broader field of social choice, which studies the broader problem of making decisions based on the opinions and preferences of multiple agents. Such opinions are not necessarily represented as rankings. For example, in judgment aggregation (see Endriss [2015] for an overview), judges assess whether certain propositions are true or false, and the goal is to aggregate these judgments into logically consistent statements. The observation that other types of input are aggregated in social choice prompts the natural question of whether analogous problems exist in statistics and machine learning (as is the case with ranking aggregation). ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this paper, we focus on a relatively new model in social choice, the quantitative judgment aggregation problem [Conitzer et al., 2015, 2016]. In this problem, the goal is to aggregate relative quantitative judgments: for example, one agent may value the life of a 20-year-old at 2 times the life of a 50-year-old (say in the context of self-driving cars making decisions) [Noothigattu et al., 2018]; another example could be that an agent judges that \u201cusing 1 unit of gasoline is as bad as creating 3 units of landflil trash\u201d (in a societal tradeoff context) [Conitzer et al., 2016]. Quantitative judgment aggregation has been considered in the area of automated moral decision-making, where an AI system may choose a course of action based on data about human judgments in similar scenarios. ", "page_idx": 1}, {"type": "text", "text": "An important conceptual difference between this work and previous studies on quantitative judgment aggregation is that we observe that relative \u201cjudgments\u201d can be produced by a process other than a subjective agent reporting them, which is the standard assumption in social choice. To illustrate, consider a race in which contestant A finishes at 20:00 and contestant B at 30:00. In this race, the \u201cjudgment\u201d is that A is 10:00 faster than B. This key observation allows us to bring the social choice community and the learning-to-rank community closer together, by applying existing social choice formulations of quantitative judgment aggregation to the problem of ranking prediction. ", "page_idx": 1}, {"type": "text", "text": "Under this new perspective, the formulation of quantitative judgment aggregation can be applied a set of new scenarios, like ranking contestants using \u201cjudgments\u201d from past races, or ranking products based on \u201cjudgments\u201d from their sales data. We are interested in aggregating such \u201cjudgments\u201d from past data, and using them to predict future rankings. Given the different motivations, some important aspects in a social choice context are less important in our setting. For example, social choice is often concerned with agents strategically misreporting, but this is less relevant in our setting because the \u201cjudgments\u201d considered in our setting are not strategic. ", "page_idx": 1}, {"type": "text", "text": "Our Contributions. We summarize our main contributions below: (1) Conceptually, we apply social-choice-motivated solution concepts to the problem of ranking prediction, which creates a bridge between research typically done in the social choice and the learning-to-rank communities. (2) We pose and study the problem of quantitative relative judgment aggregation (QRJA) in Section 3, which generalizes models from previous work [Conitzer et al., 2015, 2016]. (3) Theoretically, we focus on $\\ell_{p}$ QRJA, an important subclass of QRJA problems. We (almost) settle the computational complexity of $\\ell_{p}$ QRJA in Section 4, proving that $\\ell_{p}$ QRJA is solvable in almost-linear time when $p\\geq1$ , and is NP-hard when $p<1$ . (4) Empirically, we focus on $\\ell_{1}$ and $\\ell_{2}$ QRJA. We conduct extensive experiments on a wide range of real-world datasets in Section 5 to compare the performance of QRJA with several other commonly used methods, showing the effectiveness of QRJA in practice. ", "page_idx": 1}, {"type": "text", "text": "2 Motivating Examples ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "To better motivate our study and help readers understand the problem, we first consider simple mean/median approaches for aggregating quantitative judgments and illustrate their limitations through three examples. ", "page_idx": 1}, {"type": "text", "text": "Example 1. When each race has some common \u201cdifficulty\u201d factor (e.g. how hilly a marathon route is), if a contestant only participates in the \u201ceasy\u201d races (or only the \u201chard\u201d races), simply taking the median or mean of historical performance will return biased estimates, as illustrated in Figure 1. ", "page_idx": 1}, {"type": "table", "img_path": "37CyA1K0vV/tmp/d9e91515bb4984da039f9fec66b40065d7a8837b38c54fa1a3f8af16ccdbe4c4.jpg", "table_caption": [], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Bob finishes earlier than Charlie in the Chicago race, which suggests that Bob runs marathons faster than Charlie. However, if we simply calculate the mean or median of all available data, Charlie\u2019s mean/median finishing time will be faster than Bob\u2019s. This is because, Charlie participated only in the Chicago race, where conditions were more favorable. ", "page_idx": 1}, {"type": "text", "text": "Example 2. Suppose past data shows that Alice has beaten Bob in some race, and Bob has beaten Charlie in another race. If we have never seen Alice and Charlie competing in the same race, we may want to predict that Alice runs faster than Charlie (see Figure 2). However, when comparing Alice and Charlie, simple measures like median and mean effectively ignore the data on Bob, even though Bob\u2019s data can provide useful information for this comparison. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "table", "img_path": "37CyA1K0vV/tmp/858f5310400c7a9797ae8642df6d2e69acff6fb66dd6e66b11cb81d79f63afe4.jpg", "table_caption": [], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 2: The same results as in Figure 1, but with some data missing. If we only look at the data on Alice and Charlie, it is difficult to judge who is the faster runner. If anything, Charlie appears to be slightly faster. However, if we know Bob\u2019s results in these races, then transitivity suggests that Alice runs faster than Charlie. ", "page_idx": 2}, {"type": "table", "img_path": "37CyA1K0vV/tmp/936e83dffd05b006d646e2b81bc2d6e4005cc7e3adf510e5af77dcd5b973d905.jpg", "table_caption": ["Example 3. When the variance of the races\u2019 difficulty is much higher than the variance in the contestants\u2019 performance, taking the median will essentially focus on the result of a single race (with median difficulty) and may throw away useful information as shown in Figure 3. "], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 3: In this example, the races\u2019 difficulty has high variance, and everyone\u2019s median time is in Boston. Based on this, we would predict Charlie to be faster than Bob. However, if we consider the other two races, overall it seems that Bob runs faster than Charlie. ", "page_idx": 2}, {"type": "text", "text": "QRJA addresses the above issues by considering relative performance instead of absolute performance. More specifically, each race provides a judgment of the form \u201cA runs faster than B by Y minutes\u201d for every pair of contestants $(A,B)$ that participated in this race. ", "page_idx": 2}, {"type": "text", "text": "3 Problem Formulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we formally define the Quantitative Relative Judgment Aggregation (QRJA) problem.   \nWe start with the definition of its input. ", "page_idx": 2}, {"type": "text", "text": "Definition 1 (Quantitative Relative Judgment). For a set of n candidates $N\\;=\\;\\{1,\\ldots,n\\}$ , $a$ quantitative relative judgment is a tuple $\\bar{\\boldsymbol{J}}=(a,b,y)$ , denoting a judgment that candidate $a\\in N$ is better than candidate $b\\in N$ by $y\\in\\mathbb R$ units. ", "page_idx": 2}, {"type": "text", "text": "The input of QRJA is a set of quantitative relative judgments to be aggregated. We model the aggregation result as a vector $\\mathbf{x}\\in\\mathbb{R}^{n}$ , where $x_{i}$ is the single-dimensional evaluation of candidate $i$ The aggregation result should be consistent with the input judgments as much as possible, i.e., for a quantitative relative judgment $(a,b,y)$ , we want $|x_{a}-x_{b}-y|$ to be small. We use a loss function $f(|x_{a}-x_{b}-y|)$ to measure the inconsistency between the aggregation result and the input judgments. The aggregation result should minimize the weighted total loss. Formally, we define QRJA as follows. ", "page_idx": 2}, {"type": "text", "text": "Definition 2 (Quantitative Relative Judgment Aggregation (QRJA)). Consider n candidates $N=$ $\\{1,\\ldots,n\\}$ and m quantitative relative judgments $\\mathbf{J}=(J_{1},\\dots,J_{m})$ with weights $\\mathbf{w}=(w_{1},\\hdots,w_{m})$ where $J_{i}=(a_{i},b_{i},y_{i})$ . The quantitative relative judgment aggregation problem with loss function $f:\\mathbb{R}_{\\geq0}\\rightarrow\\mathbb{R}_{\\geq0}$ asks for a vector $\\mathbf{x}\\in\\mathbb{R}^{n}$ that minimizes $\\begin{array}{r}{\\sum_{i=1}^{\\bar{m}^{-}}{\\bar{w_{i}}f(|x_{a_{i}}-x_{b_{i}}-y_{i}|)}}\\end{array}$ . ", "page_idx": 2}, {"type": "text", "text": "Previous work [Conitzer et al., 2015, 2016; Zhang et al., 2019] studied a special case of QRJA where $f(t)=t$ . In this work, we broaden the scope and study QRJA with more general loss functions. We first note that when the loss function $f$ is convex, QRJA can be formulated as a convex optimization problem. Consequently, one can use standard convex optimization methods like gradient descent or the ellipsoid method to solve QRJA in polynomial time. ", "page_idx": 2}, {"type": "text", "text": "However, general-purpose convex optimization methods are often very slow when the numbers of candidates $n$ and judgments $m$ are large. For this reason, we focus on $\\ell_{p}$ QRJA, an important subclass of QRJA problems with loss function $f(t)=t^{p}$ . Our theoretical analysis (almost) settles the computational complexity of $\\ell_{p}$ QRJA for all $p\\,>\\,0$ . We show that $\\ell_{p}$ QRJA is solvable in almost-linear time when $p\\geq1$ , and is NP-hard when $p<1$ . Our experiments focus on comparing $\\ell_{1}$ and $\\ell_{2}$ QRJA with various baselines in social choice and machine learning. We conduct extensive experiments on a wide range of real-world data sets. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "4 Theoretical Aspects of $\\ell_{p}$ QRJA ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we study the theoretical aspects of $\\ell_{p}$ QRJA, providing a clean and (almost) tight characterization of the computational complexity of $\\ell_{p}$ QRJA for different values of $p$ . Recall that $n$ is the number of candidates and $m$ is the number of judgments. Note that $n\\leq2m$ . ", "page_idx": 3}, {"type": "text", "text": "In Section 4.1, we prove that for all $p\\geq1$ , $\\ell_{p}$ QRJA can be solved in almost-linear time $O(m^{1+o(1)})$ . In Section 4.2, we show that when $p<1$ , $\\ell_{p}$ QRJA is NP-hard and there is no FPTAS 1 unless $\\mathbf{P}=$ NP. Additionally, in Appendix A, we show that if $1\\le p\\le2$ and $m\\gg n$ , we can reduce $m$ to ${\\widetilde{O}}(n)$ while incurring a small error. 2 ", "page_idx": 3}, {"type": "text", "text": "4.1 $\\ell_{p}$ QRJA in Almost-Linear Time When $p\\geq1$ ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We first show that when $p\\geq1,\\ell_{p}$ QRJA can be solved in $O(m^{1+o(1)})$ time, i.e., in time almost linear in the size of the input. Note that to solve $\\ell_{p}$ QRJA with $p\\geq1$ in polynomial time, one can formulate the problem as an $\\ell_{p}$ regression problem and apply general-purpose techniques for $\\ell_{p}$ regression, e.g., [Bubeck et al., 2018; Adil et al., 2024]. However, these methods would result in a running time that is $\\Omega(m+n^{\\omega})$ , where $\\omega\\ge2$ is the matrix multiplication exponent. This is significantly slower than almost-linear time. Our approach leverages the additional structure of the QRJA problem, and utilizes the recent advancements in faster algorithms for (directed) maximum flow [Chen et al., 2022]. ", "page_idx": 3}, {"type": "text", "text": "Theorem 1. Let $p\\geq1$ be an absolute constant. Consider $\\ell_{p}$ QRJA in Definition 2 with loss function $f(t)=t^{p}$ . Assume all input numbers are polynomially bounded in $m$ . We can solve $\\ell_{p}$ QRJA in time $O(m^{1+o(1)})$ with $\\exp(-\\log^{c}m)$ additive error for any constant $c>0$ . ", "page_idx": 3}, {"type": "text", "text": "Proof of Theorem 1: We first prove the theorem for $p>1$ . We will prove the $p\\,=\\,1$ case in Appendix B.1. Let $S_{\\mathrm{input}}=(n,m,\\bar{,}(w_{i})_{i=1}^{m},(y_{i})_{i=1}^{m})$ . We assume $m$ is sufficiently large, and that $c$ is a sufficiently large constant such that $\\forall v\\in S_{\\mathrm{input}}$ , either $v=0$ or $1/m^{c}<|v|<m^{c}$ . ", "page_idx": 3}, {"type": "text", "text": "Consider an $\\ell_{p}$ QRJA instance $(N,\\mathbf{J},\\mathbf{w})$ where $\\mathbf{J}=(J_{1},\\dots,J_{m})$ and $J_{i}=(a_{i},b_{i},y_{i})$ , we construct a matrix $\\mathbf{A}\\in\\mathbb{R}^{m\\times n}$ and a vector $\\mathbf{z}\\in\\mathbb{R}^{m}$ as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\nA_{i,j}=\\left\\{\\begin{array}{l l}{\\!\\!\\sqrt[j w_{i}}&{\\mathrm{if}\\ j=a_{i}}\\\\ {\\!\\!-\\sqrt[j w_{i}}&{\\mathrm{if}\\ j=b_{i}}\\\\ {0}&{\\mathrm{otherwise}}\\end{array}\\right.,\\quad z_{i}=\\sqrt[j w_{i}}y_{i}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Given $\\mathbf{A}$ and $\\mathbf{z}$ , the $\\ell_{p}$ QRJA problem can be formulated as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{x}\\in\\mathbb{R}^{n}}\\sum_{i=1}^{m}w_{i}|x_{a_{i}}-x_{b_{i}}-y_{i}|^{p}=\\operatorname*{min}_{\\mathbf{x}\\in\\mathbb{R}^{n}}\\|\\mathbf{A}\\mathbf{x}-\\mathbf{z}\\|_{p}^{p}\\,,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We will show how to find $\\mathbf{x}$ in time $O(m^{1+o(1)})$ such that ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\left\\|\\mathbf{Ax}-\\mathbf{z}\\right\\|_{p}\\leq\\operatorname*{min}_{\\mathbf{x}^{*}}\\left\\|\\mathbf{Ax}^{*}-\\mathbf{z}\\right\\|_{p}+\\exp(-\\log^{2c}m).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We first write the optimization as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{x}\\in\\mathbb{R}^{n}}\\left\\|\\mathbf{A}\\mathbf{x}-\\mathbf{z}\\right\\|_{p}=\\operatorname*{min}_{\\substack{\\mathbf{x}\\in\\mathbb{R}^{n},\\mathbf{s}\\in\\mathbb{R}^{m},\\mathbf{s}=\\mathbf{A}\\mathbf{x}-\\mathbf{z}}}\\left\\|\\mathbf{s}\\right\\|_{p}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The Lagrangian dual of (2) is ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{x}\\in\\mathbb{R}^{n},\\mathbf{s}\\in\\mathbb{R}^{m}}\\operatorname*{max}_{\\mathbf{f}\\in\\mathbb{R}^{m}}\\left(\\left\\|\\mathbf{s}\\right\\|_{p}+\\mathbf{f}^{\\top}(\\mathbf{s}-(\\mathbf{A}\\mathbf{x}-\\mathbf{z}))\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "1Fully Polynomial-Time Approximation Scheme.   \n2The $\\widetilde O(\\cdot)$ notation hides logarithmic factors in its argument. ", "page_idx": 3}, {"type": "text", "text": "Note that $\\mathbf{s}=\\mathbf{A}\\mathbf{x}-\\mathbf{z}$ is enforced; otherwise the inner maximization problem is unbounded. Let $\\left\\lVert\\cdot\\right\\rVert_{q}$ be the dual norm of $\\left\\|\\cdot\\right\\|_{p}$ , i.e., $\\textstyle{\\frac{1}{p}}+{\\frac{1}{q}}=1$ . (So $q>1.$ ) By strong duality, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\mathbf{f}\\in\\mathbb{R}^{m}}{\\operatorname*{max}}\\underset{\\mathbf{x}\\in\\mathbb{R}^{n},\\mathbf{s}\\in\\mathbb{R}^{m}}{\\operatorname*{min}}\\left(\\left\\lvert\\mathbf{s}\\right\\rvert\\right\\rvert_{p}+\\mathbf{f}^{\\top}(\\mathbf{s}-\\left(\\mathbf{A}\\mathbf{x}-\\mathbf{z}\\right))\\right)}\\\\ &{=\\underset{\\mathbf{f}\\in\\mathbb{R}^{m}}{\\operatorname*{max}}\\left[\\mathbf{f}^{\\top}\\mathbf{z}+\\underset{\\mathbf{s}\\in\\mathbb{R}^{m}}{\\operatorname*{min}}\\left(\\left\\lvert\\mathbf{s}\\right\\rvert\\right\\rvert_{p}+\\mathbf{f}^{\\top}\\mathbf{s}\\right)-\\underset{\\mathbf{x}\\in\\mathbb{R}^{n}}{\\operatorname*{max}}\\mathbf{f}^{\\top}\\mathbf{A}\\mathbf{x}\\right]}\\\\ &{=\\underset{\\mathbf{f}\\in\\mathbb{R}^{m},\\mathbf{A}^{\\top}\\mathbf{f}=\\mathbf{0},\\left\\lVert\\mathbf{f}\\right\\rVert_{q}\\leq1}{\\operatorname*{max}}\\mathbf{f}^{\\top}\\mathbf{z}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The last step follows from the fact that the value of $\\mathrm{(min}_{\\mathbf{s}\\in\\mathbb{R}^{m}}\\left\\lVert\\mathbf{s}\\right\\rVert_{p}+\\mathbf{f}^{\\top}\\mathbf{s})$ is $0$ if $\\|\\mathbf{f}\\|_{q}\\leq1$ and $-\\infty$ otherwise, and that $\\operatorname{max}_{\\mathbf{x}\\in\\mathbb{R}^{n}}$ $\\mathbf{f}^{\\top}\\mathbf{Ax}$ is unbounded if $\\mathbf A^{\\top}\\mathbf f\\neq\\mathbf0$ . ", "page_idx": 4}, {"type": "text", "text": "We will show that the dual program (3) can be solved near-optimally in almost-linear time (Lemma 1), and given a near-optimal dual solution $\\mathbf{f}\\in\\mathbb{R}^{m}$ , a good primal solution $\\mathbf{x}\\in\\mathbb{R}^{n}$ can be computed in linear time (Lemma 2). Theorem 1 follows directly from Lemmas 1 and 2. ", "page_idx": 4}, {"type": "text", "text": "Lemma 1. We can find a feasible solution $\\mathbf{f}\\in\\mathbb{R}^{m}$ of (3) in time $O(m^{1+o(1)})$ with additive error $\\exp(-\\log^{6c}m)$ . ", "page_idx": 4}, {"type": "text", "text": "Proof of Lemma 1: Consider the following problem, which moves the norm constraint of (3) into the objective: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\mathbf{f}\\in\\mathbb{R}^{m},\\mathbf{A}^{\\top}\\mathbf{f}=\\mathbf{0}}\\mathbf{f}^{\\top}\\mathbf{z}-\\|\\mathbf{f}\\|_{q}^{q}\\,.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "(4) is closely related to $\\ell_{p}$ norm mincost flow. Recent breakthrough in mincost flow [Chen et al., 2022] showed that a feasible solution $\\mathbf{f}^{\\dagger}$ of (4) within error $\\exp(-\\log^{13c}m)$ can be computed in $O(m^{1+o(1)})$ time. ", "page_idx": 4}, {"type": "text", "text": "Suppose $\\left\\|\\mathbf{f}^{\\dagger}\\right\\|_{q}\\geq\\exp(-\\log^{7c}m)$ , which we prove later. Notice that $\\mathbf{f}^{\\dagger}$ is a solution within error $\\exp(-\\log^{13c}m)$ of ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\mathbf{f}\\in\\mathbb{R}^{m},\\mathbf{A}^{\\top}\\mathbf{f}=\\mathbf{0},\\|\\mathbf{f}\\|_{q}=\\|\\mathbf{f}^{\\dagger}\\|_{q}}\\mathbf{f}^{\\top}\\mathbf{z}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Choosing $\\mathbf{f}=\\mathbf{f}^{\\dagger}/\\left\\lVert\\mathbf{f}^{\\dagger}\\right\\rVert_{q}$ satisfies Lemma 1. ", "page_idx": 4}, {"type": "text", "text": "To lower bound $\\left\\|\\mathbf{f}^{\\dagger}\\right\\|_{q}$ , let $\\mathbf{f}^{*}$ be the optimal solution of (3). When $\\mathbf{f}^{\\ast}\\mathbf{\\Sigma}^{\\top}\\mathbf{z}\\geq3$ , because the optimal value of (4) is at least $\\mathbf{f}^{\\ast}{}^{\\top}\\mathbf{z}-1$ and $\\mathbf{f}^{\\dagger}$ is near-optimal for (4), we have f \u2020 $\\mathbf{\\widetilde{z}}\\geq\\mathbf{f^{*}}^{\\top}\\mathbf{z}-2$ and thus $\\left\\|\\mathbf{f}^{\\dagger}\\right\\|_{q}\\geq1/3$ . When f $\\mathbf{\\nabla}*\\mathsf{T}_{\\mathbf{Z}}<3$ , we will show $\\mathbb{f}^{\\dagger}{}^{\\top}\\mathbf{z}\\geq\\exp(-\\log^{6c}m)$ , so $\\left\\|\\mathbf{f}^{\\dagger}\\right\\|_{q}\\geq\\exp(-\\log^{7c}m)$ . To show f $\\dagger^{\\top}\\mathbf{z}\\,\\geq\\,\\exp(-\\log^{6c}m)$ , we only need to show that the optimal value of (4) is at least $\\exp(-\\log^{5c}m)$ . We can assume w.l.o.g. that $\\mathbf{f}^{*}\\,^{\\top}\\mathbf{z}>\\exp(-\\log^{3c}m)$ , otherwise there is a primal solution $\\mathbf{x}$ almost consistent with all judgments, which is easy to approximate. Note that when scaling down $\\mathbf{f}^{*}$ , $\\|\\mathbf{f}^{*}\\|_{q}^{q}$ scales faster than $\\mathbf{\\bar{f}^{\\ast}}^{\\top}\\mathbf{z}$ . Let $\\mathbf{f}^{\\prime}=k\\mathbf{f}^{*}$ with $k=\\exp(-\\log^{4c}m)$ . We have $\\mathbf{f^{\\prime}}^{\\top}\\mathbf{z}-\\|\\mathbf{f^{\\prime}}\\|_{q}^{q}=k(\\mathbf{f^{*}}^{\\top}\\mathbf{z})-k^{q}>\\exp(-\\log^{5c}m)$ , where the last step assumes that $m$ is sufficiently large, in particular $\\begin{array}{r}{\\log^{c}m>\\operatorname*{max}\\{\\frac{2}{q-1},q+1\\}}\\end{array}$ . \u25a0 ", "page_idx": 4}, {"type": "text", "text": "Lemma 2. Given a solution f of (3) that satisfies Lemma $^{\\,l}$ , we can compute a vector $\\mathbf{x}\\in\\mathbb{R}^{n}$ in time $O(m)$ such that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\left\\|\\mathbf{Ax}-\\mathbf{z}\\right\\|_{p}\\leq\\operatorname*{min}_{\\mathbf{x}^{*}}\\left\\|\\mathbf{Ax}^{*}-\\mathbf{z}\\right\\|_{p}+\\exp(-\\log^{2c}m).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Proof of Lemma 2: We assume w.l.o.g. that $\\|\\mathbf{f}\\|_{q}=1$ . ", "page_idx": 4}, {"type": "text", "text": "Let $v=\\mathbf{f}^{\\top}\\mathbf{z}$ and consider ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\mathbf{f}^{\\prime}\\in\\mathbb{R}^{m},\\mathbf{A}^{\\top}\\mathbf{f^{\\prime}}=\\mathbf{0}}\\Phi(\\mathbf{f}^{\\prime})\\,\\mathrm{~where~}\\,\\Phi(\\mathbf{f}^{\\prime})=\\mathbf{f}^{\\prime}{}^{\\top}\\mathbf{z}-\\frac{v}{q}\\left\\|\\mathbf{f}^{\\prime}\\right\\|_{q}^{q}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Because f is a solution of (3) within error $\\exp(-\\log^{6c}m)$ , and $\\begin{array}{r}{\\operatorname*{max}_{\\|\\mathbf{f}\\|_{q}}v\\left\\|\\mathbf{f}\\right\\|_{q}-\\frac{v}{q}\\left\\|\\mathbf{f}\\right\\|_{q}^{q}}\\end{array}$ is achieved when $\\left\\|\\mathbf{f}\\right\\|_{q}=1$ , we know that f is a solution of (5) within error $\\exp(-\\log^{5c}m)$ . ", "page_idx": 5}, {"type": "text", "text": "The first-order optimality condition of (5) guarantees that $\\nabla\\Phi(\\mathbf{f})$ is very close to a potential flow. That is, we can find in $O(m)$ time a vector $\\mathbf{x}\\in\\mathbb{R}^{n}$ , such that $\\|\\mathbf{Ax}-\\nabla\\Phi(\\mathbf{f})\\|_{\\infty}\\leq\\exp(-\\log^{3c}m)$ . For this $\\mathbf{x}$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|\\mathbf{Ax}-\\mathbf{z}\\|_{p}\\leq\\|\\nabla\\Phi(\\mathbf{f})-\\mathbf{z}\\|_{p}+\\|\\mathbf{Ax}-\\nabla\\Phi(\\mathbf{f})\\|_{p}}&{}\\\\ {=v+\\|\\mathbf{Ax}-\\nabla\\Phi(\\mathbf{f})\\|_{p}}&{}\\\\ {\\leq v+m\\,\\|\\mathbf{Ax}-\\nabla\\Phi(\\mathbf{f})\\|_{\\infty}}&{}\\\\ {\\leq v+\\exp(-\\log^{2c}m)}&{}\\\\ {\\leq\\underset{\\mathbf{x}^{*}\\in\\mathbb{R}^{n}}{\\operatorname*{min}}\\,\\|\\mathbf{Ax}^{*}-\\mathbf{z}\\|_{p}+\\exp(-\\log^{2c}m).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The last inequality uses that $v=\\mathbf{f}^{\\top}\\mathbf{z}$ is a lower bound on the optimal value because f is a feasible dual solution. ", "page_idx": 5}, {"type": "text", "text": "4.2 NP-Hardness of $\\ell_{p}$ QRJA When $p<1$ ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we show that $\\ell_{p}$ QRJA is NP-hard when $p<1$ by reducing from Max-Cut. Note that in this case, the loss function ${\\dot{f}}(t)=t^{p}$ is no longer convex. ", "page_idx": 5}, {"type": "text", "text": "Definition 3 (Max-Cut). For an undirected graph $G=(V,E)$ , Max-Cut asks for a partition of $V$ into two sets $S$ and $T$ that the number of edges between $S$ and $T$ is maximized. ", "page_idx": 5}, {"type": "text", "text": "Reduction from Max-Cut to $\\ell_{p}$ QRJA. Given a Max-Cut instance on an undirected graph $G=$ (V, E), let n = |V |, m = |E|, w2 =12\u2212np , and $w_{1}=n w_{2}+1$ . ", "page_idx": 5}, {"type": "text", "text": "We will construct an $\\ell_{p}$ QRJA instance with $n+2$ candidates $V\\cup\\{v^{(s)},v^{(t)}\\}$ and $O(n+m)$ quantitative relative judgments. Specifically, we add the following judgments: ", "page_idx": 5}, {"type": "text", "text": "\u2022 $(v^{(t)},v^{(s)},1)$ with weight $w_{1}$ .   \n\u2022 $(\\boldsymbol{v}^{(s)},\\boldsymbol{u},0)$ with weight $w_{2}$ for each $u\\in V$ .   \n\u2022 $(\\boldsymbol{v}^{(t)},\\boldsymbol{u},0)$ with weight $w_{2}$ for each $u\\in V$ .   \n\u2022 $(u,v,1),(v,u,1)$ with weight 1 for each $(u,v)\\in E$ . ", "page_idx": 5}, {"type": "text", "text": "In Appendix B.2, we will prove that the Max-Cut instance has a cut of size at least $k$ if and only if the constructed $\\ell_{p}$ QRJA instance has a solution with loss at most $n w_{2}+2(m-k)+k2^{p}$ , which implies the following hardness result. ", "page_idx": 5}, {"type": "text", "text": "Theorem 2. For any $p<1$ , there exists a constant $c>0$ such that it is NP-hard to approximate $\\ell_{p}$ QRJA within a multiplicative factor of $\\textstyle\\left(1+{\\frac{c}{n^{2}}}\\right)$ . ", "page_idx": 5}, {"type": "text", "text": "Theorem 2 implies that there is no (multiplicative) FPTAS for $\\ell_{p}$ QJA when $p<1$ unless $\\mathbf{P}=\\mathbf{NP}.$ This is because if a $(1+\\varepsilon)$ solution can be computed in poly $(m,1/\\varepsilon)$ time, then choosing $\\textstyle\\varepsilon={\\frac{c}{n^{2}}}$ gives a poly-time algorithm for Max-Cut. ", "page_idx": 5}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We conduct experiments on real-world datasets to compare the performance of $\\ell_{1}$ and $\\ell_{2}$ QRJA with existing methods. We focus on $\\ell_{1}$ and $\\ell_{2}$ QRJA because the almost-linear time algorithm for general values of $p\\geq1$ relies on very complicated galactic algorithms for $\\ell_{p}$ norm mincost flow [Chen et al., 2022]. Although general-purpose convex optimization methods can also be used to solve $\\ell_{p}$ QRJA, they are not efficient enough for some of the large-scale datasets we use. All experiments are done on a server with 56 CPU cores and 504G RAM. The experiments in Section 5 and Appendices A and C take around 2 weeks in total to run on this server. No GPU is used. All source code is available at https://github.com/YixuanEvenXu/quantitative-judgment-aggregation. ", "page_idx": 5}, {"type": "text", "text": "5.1 Experiments Setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Datasets. We consider types of contests where events are reasonably frequent (so it makes sense to predict future events based on past ones), and contest results contain numerical scores in addition to rankings. Specifically, we use the four datasets listed below. We include additional experiments on three more datasets in Appendix C, and the copyright information of the datasets in Appendix E. ", "page_idx": 6}, {"type": "text", "text": "\u2022 Chess. This dataset contains the results of the Tata Steel Chess Tournament (https: //tatasteelchess.com/, also historically known as the Hoogovens Tournament or the Corus Chess Tournament) from 1983 to $2023^{\\ 3}$ . Each contest is typically a round-robin tournament among 10 to 14 contestants. A contestant\u2019s numerical score is the contestant\u2019s number of wins in the tournament. There are 80 contests and 408 contestants in this dataset.   \n\u2022 F1. This dataset contains the results of Formula 1 races (https://www.formula1.com/) from 1950 to 2023. In each contest, we take all contestants who complete the whole race. There are around 7 such contestants in each contest. A contestant\u2019s numerical score is the negative of his/her finishing time (in seconds). There are 878 contests and 261 contestants in this dataset.   \n\u2022 Marathon. This dataset contains the results of the Boston and New York Marathons from 2000 to 2023. We use the data from https://www.marathonguide.com/, which publishes results of all major marathon events. Each contest usually involves more than 20000 contestants. We take the 100 top-ranked contestants in each contest as our dataset. A contestant\u2019s numerical score is the negative of that contestant\u2019s finishing time (in seconds). There are 44 contests and 2984 contestants.   \n\u2022 Codeforces. This dataset contains the results of Codeforces (https://codeforces.com), a website hosting frequent online programming contests, from 2010 to 2023 (Codeforces Round 875). We consider only Division 1 contests, where only more skilled contestants can participate. Each contest involves around 700 contestants. We take the 100 top-ranked contestants in each contest as our dataset. A contestant\u2019s numerical score is that contestant\u2019s points in that contest. There are 327 contests and 5338 contestants in total in this dataset. ", "page_idx": 6}, {"type": "text", "text": "Evaluation Metrics. For all the datasets we use, contests are naturally ordered chronologically. We use the results of the first $i-1$ contests to predict the results of the $i$ -th contest. We apply the following two metrics to evaluate the prediction performance of different algorithms. ", "page_idx": 6}, {"type": "text", "text": "\u2022 Ordinal Accuracy. This metric measures the percentage of correct relative ordinal predictions. For each contest, we predict the ordinal results of all pairs of contestants that (i) have both appeared before and (ii) have different numerical scores in the current contest. We compute the percentage of correct predictions. ", "page_idx": 6}, {"type": "text", "text": "\u2022 Quantitative Loss. This metric measures the average absolute error 4 of relative quantitative predictions. For each contest, we predict the difference in numerical scores of all pairs of contestants that have both appeared before. We then compute the quantitative loss as the average absolute error of the predictions. We normalize this number by the quantitative loss of the trivial prediction that always predicts 0 for all pairs. ", "page_idx": 6}, {"type": "text", "text": "Implementation. We have implemented both $\\ell_{1}$ and $\\ell_{2}$ QRJA in Python. We use Gurobi Gurobi Optimization, LLC [2023] and NetworkX Hagberg et al. [2008] to implement $\\ell_{1}$ QRJA and the least-square regression implementation in SciPy [Jones et al., 2014] to implement $\\ell_{2}$ QRJA. To transform the contest standings into a QRJA instance, we construct a quantitative relative judgment $\\boldsymbol{J}=(a,b,y)$ for each contest and each pair of contestants $(a,b)$ with $y$ being the score difference between $a$ and $b$ in that contest. We set all weights to 1 to ensure fair comparison with benchmarks. ", "page_idx": 6}, {"type": "text", "text": "Benchmarks. We evaluate $\\ell_{1}$ and $\\ell_{2}$ QRJA against several benchmark algorithms. Specifically, we consider the natural one-dimensional aggregation methods Mean and Median, social choice methods Borda and Kemeny-Young, and a common method for prediction, matrix factorization. We describe how we apply these methods to our setting below. ", "page_idx": 6}, {"type": "image", "img_path": "37CyA1K0vV/tmp/0cbaadea6a701ae9d3c75c51d7775a3b986e17bec17d038a4026b6f277692d92.jpg", "img_caption": ["Figure 4: Ordinal accuracy and quantitative loss of the algorithms on all four datasets. Error bars are not shown here as the algorithms are deterministic. The results show that both versions of QRJA perform consistently well across the tested datasets. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "\u2022 Mean and Median. For every contestant in the training set, we take the mean or median of that contestant\u2019s scores in training contests. We then make predictions based on differences between these mean or median scores. In one-dimensional environments like ours, means and medians are considered to be among the best imputation methods for various tasks (see, e.g., Engels and Diehr, 2003, Shrive et al., 2006). ", "page_idx": 7}, {"type": "text", "text": "\u2022 The Borda rule. The Borda rule is a voting rule that takes rankings as input and produces a ranking as output. We use a normalized version of the Borda rule. The $i$ -th ranked contestant in contest $j$ receives $\\textstyle1-{\\frac{2(i-1)}{n_{j}-1}}$ 2n(ij\u2212\u221211) points, where nj is the number of contestants in the contest. The aggregated ranking result is obtained by sorting the contestants by their total number of points. ", "page_idx": 7}, {"type": "text", "text": "\u2022 The Kemeny-Young rule. [Kemeny, 1959; Young and Levenglick, 1978; Young, 1988]. The Kemeny-Young rule is a voting rule that takes multiple (partial) rankings of the contestants as input and produces a ranking as output. Specifically, it outputs a ranking that minimizes the number of disagreements on pairs of contestants with the input rankings. Finding the optimal Kemeny-Young ranking is known to be NP-hard Bartholdi et al. [1989]. In our experiments, we use Gurobi to solve the mixed-integer program formulation of the Kemeny-Young rule given in Conitzer et al. [2006]. As this method is still computationally expensive and can only scale to hundreds of contestants, for each contest we predict, we only keep the contestants within that specific contest and discard all other contestants to run Kemeny-Young. ", "page_idx": 7}, {"type": "text", "text": "\u2022 Matrix Factorization (MF). Matrix factorization takes as input a matrix with missing entries and outputs a prediction of the whole matrix. Every row is a contestant and every column is a race. The score of a contestant in a race is the entry in the corresponding row and column. We implement several variants of MF and report results for one variant (Koren et al. [2009]), as other variants have comparable or worse performance. For implementation details and other variants, see Appendix C.4. ", "page_idx": 7}, {"type": "text", "text": "Many other, related approaches deserve mention in this context. But we do not include them in the benchmarks because they do not exactly fit our setting or motivation. For example, the seminal Elo rating system Elo [1978] as well as many other methods Maher [1982]; Karlis and Ntzoufras [2008]; Guo et al. [2012]; Hunter and others [2004] can all predict the results of pairwise matches in, e.g., chess and football. However, they are not originally designed for predicting the results of contests with more than two contestants. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "5.2 Experiment Results ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The complete experimental results of all algorithms on the four datasets are shown in Fig. 4. Note that Borda and Kemeny-Young do not make quantitative predictions, so they are not included in Figs. 4b, 4d, 4f and 4h. ", "page_idx": 8}, {"type": "text", "text": "The performance of QRJA. As shown in Fig. 4, both versions of QRJA perform consistently well across the tested datasets. They are always among the best algorithms in terms of both ordinal accuracy and quantitative loss. ", "page_idx": 8}, {"type": "text", "text": "The performance of Mean and Median. In terms of ordinal accuracy, Mean and Median do well on Marathon, but are not among the best algorithms on other datasets, especially on F1 (for both) and Codeforces (for Median). Moreover, for quantitative loss, they are never among the best algorithms. ", "page_idx": 8}, {"type": "text", "text": "The performance of Borda and Kemeny-Young. Borda and Kemeny-Young do not make quantitative predictions, so we only compare them with other algorithms in terms of ordinal accuracy. As shown in Fig. 4, Borda and Kemeny-Young perform very well on F1, but are not among the best algorithms on other datasets. By only using rankings as input, Borda and Kemeny-Young are more robust on datasets where contestants\u2019 performance varies a lot. However, they fail to utilize the quantitative information on other datasets. ", "page_idx": 8}, {"type": "text", "text": "The performance of Matrix Factorization (MF). MF works well across the tested datasets in terms of both metrics. In all of our four datasets, it has performance comparable to QRJA. The advantage of QRJA over MF is the interpretability of its model. The variables in QRJA have clear meanings - they can be interpreted as the strength of each contestant - in contrast to the latent factors and features in MF, which are harder to interpret. Additionally, we observe in Appendix C.2 that $\\ell_{1}$ QRJA is more robust to large variance in contestants\u2019 performance than MF. ", "page_idx": 8}, {"type": "text", "text": "Summary of experimental results. In summary, both MF and QRJA are never significantly worse than the best-performing algorithm on any of the tested datasets, unlike the other benchmark methods. QRJA additionally offers an interpretable model. This shows that QRJA is an effective method for making predictions on contest results. ", "page_idx": 8}, {"type": "text", "text": "6 Related Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Random utility models. Random utility models (Fahandar et al. [2017]; Zhao et al. [2018]) explicitly reason about the contestants being numerically different from each other, e.g., one contestant is generally 1.1 times as fast as another. However, they are still designed for settings in which the only input data we have is ranking data, rather than numerical data such as finishing times. Moreover, random utility models generally do not model common factors, such as a given race being tough and therefore resulting in higher finishing times for everyone. ", "page_idx": 8}, {"type": "text", "text": "Matrix completion. Richer models considered in recommendation systems appear too general for the scenarios we have in mind. Matrix completion Rennie and Srebro [2005]; Cand\u00e8s and Recht [2009] is a popular approach in collaborative filtering, where the goal is to recover missing entries given a partially-observed low-rank matrix. While using higher ranks may lead to better predictions, we want to model contestants in a single-dimensional way, which is necessary for interpretability purposes (the single parameter being interpreted as the \u201cquality\u201d of the contestant). ", "page_idx": 8}, {"type": "text", "text": "Preference learning. In preference learning, we train on a subset of items that have preferences toward labels and predict the preferences for all items (see, e.g., Pahikkala et al. [2009]). One high-level difference is that preference learning tends to use existing methodologies in machine learning to learn rankings. In contrast, our methods (as well as those in previous work Conitzer et al. [2015, 2016]) are social-choice-theoretically well motivated. In addition, our methods are designed for quantitative predictions, while the main objective of preference learning is to learn ordinal predictions. ", "page_idx": 8}, {"type": "text", "text": "Elo and TrueSkill. Empirical methods, such as the Elo rating system Elo [1978] and Microsoft\u2019s TrueSkill Herbrich et al. [2006], have been developed to maintain rankings of players in various forms of games. Unlike QRJA, these methods focus more on the online aspects of the problem, i.e., how to properly update scores after each game. While under specific statistical assumptions, these methods can in principle predict the outcome of a future game, they are not designed for making ordinal or quantitative predictions in their nature. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we conduct a thorough investigation of QRJA (Quantitative Relative Judgment Aggregation). We pose and study QRJA and focus on an important subclass of problems, $\\ell_{p}$ QRJA. Our theoretical analysis shows that $\\ell_{p}$ QRJA can be solved in almost-linear time when $p\\geq1$ , and is NP-hard when $p<1$ . Empirically, we conduct experiments on real-world datasets to show that QRJA-based methods are effective for predicting contest results. As mentioned before, the almostlinear time algorithm for general values of $p\\neq1,2$ relies on very complicated galactic algorithms. An interesting avenue for future work would be to develop fast (e.g., nearly-linear time) algorithms for $\\ell_{p}$ QRJA with $p\\neq1,2$ that are more practical, and evaluate their empirical performance. ", "page_idx": 9}, {"type": "text", "text": "Broader Impacts. We expect our work to have a mostly positive social impact by providing an effective and interpretable method for aggregating quantitative relative judgments that can be used in applications such as predicting contest results. While for specific applications, certain desiderata may be not met by QRJA, we allow users (e.g., contest organizers) to set different weights for different judgments, which can be used to reflect the importance of different contests. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Zhang and Conitzer are supported by NSF IIS-1814056, the Center for Emerging Risk Research, and the Cooperative AI Foundation. Cheng is supported in part by NSF Award CCF-2307106. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Deeksha Adil, Rasmus Kyng, Richard Peng, and Sushant Sachdeva. Fast algorithms for $\\ell_{p}$ -regression. J. ACM, August 2024.   \nJohn Bartholdi, Craig A Tovey, and Michael A Trick. Voting schemes for which it can be difficult to tell who won the election. Social Choice and welfare, 6:157\u2013165, 1989.   \nAaron Bernstein, Danupon Nanongkai, and Christian Wulff-Nilsen. Negative-weight single-source shortest paths in near-linear time. In 2022 IEEE 63rd Annual Symposium on Foundations of Computer Science (FOCS), pages 600\u2013611. IEEE, 2022.   \nS\u00e9bastien Bubeck, Michael B. Cohen, Yin Tat Lee, and Yuanzhi Li. An homotopy method for $\\ell_{p}$ regression provably beyond self-concordance and in input-sparsity time. In Proceedings of the 45th annual ACM Symposium on Theory of Computing (STOC), pages 1130\u20131137. ACM, 2018.   \nEmmanuel J. Cand\u00e8s and Benjamin Recht. Exact matrix completion via convex optimization. Foundations of Computational Mathematics, 9(6):717\u2013772, 2009.   \nIoannis Caragiannis, Ariel D Procaccia, and Nisarg Shah. When do noisy votes reveal the truth? In Proceedings of the fourteenth ACM conference on Electronic commerce, pages 143\u2013160. ACM, 2013.   \nLi Chen, Rasmus Kyng, Yang P Liu, Richard Peng, Maximilian Probst Gutenberg, and Sushant Sachdeva. Maximum flow and minimum-cost flow in almost-linear time. In 2022 IEEE 63rd Annual Symposium on Foundations of Computer Science (FOCS), pages 612\u2013623. IEEE, 2022.   \nMichael B. Cohen and Richard Peng. $\\ell_{p}$ row sampling by lewis weights. In Proceedings of the 47th annual ACM Symposium on Theory of Computing (STOC), pages 183\u2013192. ACM, 2015.   \nVincent Conitzer and Tuomas Sandholm. Common voting rules as maximum likelihood estimators. In Proceedings of the 21st Annual Conference on Uncertainty in Artificial Intelligence (UAI), pages 145\u2013152, Edinburgh, UK, 2005.   \nVincent Conitzer, Andrew Davenport, and Jayant Kalagnanam. Improved bounds for computing kemeny rankings. In AAAI, volume 6, pages 620\u2013626, 2006.   \nVincent Conitzer, Matthew Rognlie, and Lirong Xia. Preference functions that score rankings and maximum likelihood estimation. In Proceedings of the Twenty-First International Joint Conference on Artificial Intelligence (IJCAI), pages 109\u2013115, Pasadena, CA, USA, 2009.   \nVincent Conitzer, Markus Brill, and Rupert Freeman. Crowdsourcing societal tradeoffs. In Proceedings of the Fourteenth International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS), pages 1213\u20131217, Istanbul, Turkey, 2015.   \nVincent Conitzer, Rupert Freeman, Markus Brill, and Yuqian Li. Rules for choosing societal tradeoffs. In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, pages 460\u2013467, Phoenix, AZ, USA, 2016.   \nEdith Elkind and Arkadii Slinko. Rationalizations of voting rules. In F. Brandt, V. Conitzer, U. Endriss, J. Lang, and A. D. Procaccia, editors, Handbook of Computational Social Choice, chapter 8. Cambridge University Press, 2015.   \nArpad E Elo. The rating of chessplayers, past and present. Arco Pub., 1978.   \nUlle Endriss. Judgment aggregation. In F. Brandt, V. Conitzer, U. Endriss, J. Lang, and A. D. Procaccia, editors, Handbook of Computational Social Choice, chapter 17. Cambridge University Press, 2015.   \nJean Mundahl Engels and Paula Diehr. Imputation of missing longitudinal data: a comparison of methods. Journal of clinical epidemiology, 56(10):968\u2013976, 2003.   \nMohsen Ahmadi Fahandar, Eyke H\u00fcllermeier, and In\u00e9s Couso. Statistical inference for incomplete ranking data: the case of rank-dependent coarsening. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 1078\u20131087. JMLR. org, 2017. ", "page_idx": 10}, {"type": "text", "text": "Shengbo Guo, Scott Sanner, Thore Graepel, and Wray Buntine. Score-based bayesian skill learning. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 106\u2013121. Springer, 2012. ", "page_idx": 11}, {"type": "text", "text": "Gurobi Optimization, LLC. Gurobi Optimizer Reference Manual, 2023. ", "page_idx": 11}, {"type": "text", "text": "Aric Hagberg, Pieter Swart, and Daniel S Chult. Exploring network structure, dynamics, and function using networkx. Technical report, Los Alamos National Lab.(LANL), Los Alamos, NM (United States), 2008. ", "page_idx": 11}, {"type": "text", "text": "Ralf Herbrich, Tom Minka, and Thore Graepel. Trueskilltm: A bayesian skill rating system. In Proceedings of the Twentieth Annual Conference on Neural Information Processing Systems, pages 569\u2013576, 2006. ", "page_idx": 11}, {"type": "text", "text": "David R Hunter et al. Mm algorithms for generalized bradley-terry models. The annals of statistics, 32(1):384\u2013406, 2004. ", "page_idx": 11}, {"type": "text", "text": "Eric Jones, Travis Oliphant, and Pearu Peterson. Scipy: open source scientific tools for python, 2014. ", "page_idx": 11}, {"type": "text", "text": "Dimitris Karlis and Ioannis Ntzoufras. Bayesian modelling of football outcomes: using the skellam\u2019s distribution for the goal difference. IMA Journal of Management Mathematics, 20(2):133\u2013145, 2008. ", "page_idx": 11}, {"type": "text", "text": "John Kemeny. Mathematics without numbers. Daedalus, 88:575\u2013591, 1959. ", "page_idx": 11}, {"type": "text", "text": "Yehuda Koren, Robert Bell, and Chris Volinsky. Matrix factorization techniques for recommender systems. Computer, 42(8):30\u201337, 2009. ", "page_idx": 11}, {"type": "text", "text": "D. Lewis. Finite dimensional subspaces of $l_{p}$ . Studia Mathematica, 63(2):207\u2013212, 1978. ", "page_idx": 11}, {"type": "text", "text": "Tie-Yan Liu. Learning to rank for information retrieval. Foundations and Trends in Information Retrieval, 3(3):225\u2013231, 2009. ", "page_idx": 11}, {"type": "text", "text": "Michael J Maher. Modelling association football scores. Statistica Neerlandica, 36(3):109\u2013118, 1982. ", "page_idx": 11}, {"type": "text", "text": "Marina Meila, Kapil Phadnis, Arthur Patterson, and Jeff Bilmes. Consensus ranking under the exponential model. In Proceedings of the 23rd Annual Conference on Uncertainty in Artificial Intelligence (UAI), pages 285\u2013294, Vancouver, BC, Canada, 2007. ", "page_idx": 11}, {"type": "text", "text": "Ritesh Noothigattu, Snehalkumar Gaikwad, Edmond Awad, Sohan Dsouza, Iyad Rahwan, Pradeep Ravikumar, and Ariel Procaccia. A voting-based system for ethical decision making. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018. ", "page_idx": 11}, {"type": "text", "text": "Tapio Pahikkala, Evgeni Tsivtsivadze, Antti Airola, Jouni J\u00e4rvinen, and Jorma Boberg. An efficient algorithm for learning to rank from preference graphs. Machine Learning, 75(1):129\u2013165, 2009. ", "page_idx": 11}, {"type": "text", "text": "Jason D. M. Rennie and Nathan Srebro. Fast maximum margin matrix factorization for collaborative prediction. In Proceedings of the 22nd International Conference on Machine Learning, pages 713\u2013719, 2005. ", "page_idx": 11}, {"type": "text", "text": "Fiona M Shrive, Heather Stuart, Hude Quan, and William A Ghali. Dealing with missing data in a multi-question depression scale: a comparison of imputation methods. BMC medical research methodology, 6(1):57, 2006. ", "page_idx": 11}, {"type": "text", "text": "H. Peyton Young. Condorcet\u2019s theory of voting. American Political Science Review, 82:1231\u20131244, 1988.   \nH. Peyton Young. Optimal voting rules. Journal of Economic Perspectives, 9(1):51\u201364, 1995.   \nHanrui Zhang, Yu Cheng, and Vincent Conitzer. A better algorithm for societal tradeoffs. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 2229\u20132236, 2019.   \nZhibing Zhao, Tristan Villamil, and Lirong Xia. Learning mixtures of random utility models. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018. ", "page_idx": 12}, {"type": "text", "text": "A Subsampling Judgments ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Subsampling Judgments When $p\\in[1,2]$ ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we show that for $p\\in[1,2]$ , we can reduce the number of judgments while incurring a small approximation error by subsampling the input judgments. ", "page_idx": 13}, {"type": "text", "text": "Algorithm 1 Subsampling Judgments   \nInput: $\\ell_{p}$ QRJA instance $(N,\\mathbf{J},\\mathbf{w})$ , subsample count $M\\in\\mathbb{N}$ , and subsampling weights $\\mathbf{s}\\in\\mathbb{R}^{m}$ . Output: $\\ell_{p}$ QRJA instance $(N,\\mathbf{J}^{\\prime},\\mathbf{w}^{\\prime})$ .   \n1: Let qi \u2190 $\\begin{array}{r}{q_{i}\\leftarrow\\frac{s_{i}}{\\sum_{j=1}^{m}s_{j}}}\\end{array}$ for each $i\\in\\{1,2,\\dots,m\\}$ .   \n2: for $i\\in\\{1,2,\\dots,M\\}$ do   \n3: Sample $x\\in\\{1,2,\\ldots,m\\}$ with probability $q_{x}$ .   \n4: Let Ji\u2032 \u2190Jx and wi\u2032 \u2190Mw\u00b7xqx .   \n5: end for   \n6: return $(N,\\mathbf{J}^{\\prime},\\mathbf{w}^{\\prime})$ . ", "page_idx": 13}, {"type": "text", "text": "Algorithm 1 takes as input an $\\ell_{p}$ QRJA instance, a parameter $M$ , and a vector $\\textbf{s}\\in\\mathbb{R}^{m}$ . It then samples $M$ judgments from the input instance (with replacements) with probability proportional to s, and outputs a new $\\ell_{p}$ QRJA instance with the sampled judgments. The weight of any judgment in the output instance is divided by its expected number of occurrences in the output instance, so that the expected total weight of any judgment is preserved after subsampling. ", "page_idx": 13}, {"type": "text", "text": "Theorem 3. Fix absolute constants $p\\in[1,2]$ and $\\varepsilon>0$ . Given any $\\ell_{p}$ QRJA instance $(N,\\mathbf{J},\\mathbf{w})$ , we can compute subsampling weights $\\textbf{s}\\in\\mathbb{R}^{m}$ in time $O(m+n^{\\omega+o(1)})$ , where $\\omega$ is the matrix multiplication exponent. For these weights s and $M={\\tilde{O}}(n)$ , Algorithm 1 with high probability outputs an $\\ell_{p}$ QRJA instance $(N,\\mathbf{J}^{\\prime},\\mathbf{w}^{\\prime})$ whose optimal solution is an $(1+\\varepsilon)$ -approximate solution of the original instance. ", "page_idx": 13}, {"type": "text", "text": "To obtain the theoretical guarantee of Algorithm 1, we use the Lewis weights mentioned in (Cohen and Peng [2015]) as vector s. Empirically, we also find that simply setting s as an all-ones vector works well in many real-world datasets (see Appendix A.2). ", "page_idx": 13}, {"type": "text", "text": "Proof of Theorem 3: For an $\\ell_{p}$ QRJA instance $(N,\\mathbf{J},\\mathbf{w})$ , define matrix $\\mathbf{A}\\in\\mathbb{R}^{m\\times(n+1)}$ ", "page_idx": 13}, {"type": "equation", "text": "$$\nA_{i,j}=\\left\\{{\\begin{array}{l l}{\\displaystyle{\\sqrt[{9}]{w_{i}}}}&{{\\mathrm{if~}}j=a_{i}}\\\\ {\\displaystyle-{\\sqrt[{9}]{w_{i}}}}&{{\\mathrm{if~}}j=b_{i}}\\\\ {\\displaystyle-{\\sqrt[{9}]{w_{i}}}y_{i}}&{{\\mathrm{if~}}j=n+1}\\\\ {\\displaystyle0}&{{\\mathrm{otherwise}}.}\\end{array}}\\right.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The Lewis weights for this $\\ell_{p}$ QRJA instance is defined as the unique vector $\\mathbf{s}\\in\\mathbb{R}^{m}$ such that for each $i\\in\\{1,2,\\dots,m\\}$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbf{a}_{i}\\left(\\mathbf{A}^{\\top}\\mathbf{S}^{1-\\frac{2}{p}}\\mathbf{A}\\right){}^{-1}\\mathbf{a}_{i}^{\\top}=s_{i}^{2/p},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\mathbf{S}=\\mathrm{diag}(\\mathbf{s})$ and ${\\bf a}_{i}$ is the $i$ -th row of $\\mathbf{A}$ . ", "page_idx": 13}, {"type": "text", "text": "The existence and uniqueness of such weights are first shown in Lewis [1978]. In Cohen and Peng [2015], the authors show that for $p\\in[1,2]$ , the Lewis weights can be computed in $O(\\mathrm{nnz}(\\mathbf{A})+$ $n^{\\omega+o(1)})=O(m+n^{\\omega+o(1)})$ time. ", "page_idx": 13}, {"type": "text", "text": "For $\\mathbf{x}\\in\\mathbb{R}^{n}$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\left\\|\\mathbf{A}\\left[\\mathbf{\\hat{x}}\\right]\\right\\|_{p}^{p}=\\sum_{i=1}^{m}w_{i}|x_{a_{i}}-x_{b_{i}}-y_{i}|^{p}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Thus the $\\ell_{p}$ QRJA loss is always equal to $\\|\\mathbf{Ax}\\|_{p}^{p}$ for some $\\mathbf{x}\\in\\mathbb{R}^{n+1}$ . The theorem then follows from the $\\ell_{p}$ Matrix Concentration Bounds in Cohen and Peng [2015]. ", "page_idx": 13}, {"type": "text", "text": "A.2 Subsampling Experiments ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We also conduct experiments to test the performance of our subsampling algorithm (Algorithm 1), which speeds up the (approximate) computation of QRJA on large datasets. In the experiments, we specify the subsample rate $\\alpha$ , let $M=\\lfloor\\alpha m\\rfloor$ and s be an all-ones vector in Algorithm 1. ", "page_idx": 14}, {"type": "text", "text": "Experiment setup. We run $\\ell_{1}$ and $\\ell_{2}$ QRJA with instances subsampled by Algorithm 1 on the datasets. For each $\\alpha=\\{0.1,0.2,\\ldots,1.0\\}$ , we run $\\ell_{1}$ and $\\ell_{2}$ QRJA 10 times and report their average performance on both metrics with error bars. Due to the space constraints, we only show the results on Chess in Fig. 5 in this section. The results on other datasets are deferred to Appendix C.3. ", "page_idx": 14}, {"type": "image", "img_path": "37CyA1K0vV/tmp/eb3edcf2b3cdab9baa2bc9f9f1afa3fbcc97028b9b2bb0ba14abf4c660541dda.jpg", "img_caption": ["(a) $\\ell_{1}$ and $\\ell_{2}$ QRJA\u2019s ordinal accuracy on Chess "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "37CyA1K0vV/tmp/859f5d8e36e2d88f84980255ff2dbe5d1383a6786b40bd9f85ca9039c4c5f46c.jpg", "img_caption": ["(b) $\\ell_{1}$ and $\\ell_{2}$ QRJA\u2019s quantitative loss on Chess "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "Figure 5: The performance of $\\ell_{1}$ and $\\ell_{2}$ QRJA on Chess after subsampling judgments using Algorithm 1 with equal weights for all judgments. The subsample rate $\\alpha$ means $M\\,=\\,\\left\\lfloor\\alpha m\\right\\rfloor$ in Algorithm 1. Error bars indicate the standard deviation. The results show that Algorithm 1 can reduce the number of judgments to a factor of 0.4 with a minor performance loss on Chess. ", "page_idx": 14}, {"type": "text", "text": "Experiment results. As is shown in Fig. 5, with equal weights for all judgments, Algorithm 1 can reduce the number of judgments without significantly hurting the performance of $\\ell_{1}$ and $\\ell_{2}$ QRJA as long as the sampling rate $\\alpha$ is not too small $(\\ge0.4$ for Chess). This shows that Algorithm 1 is a practical algorithm for subsampling judgments in QRJA. We also note that as the experiments show, $\\ell_{2}$ QRJA is more robust to subsampling than $\\ell_{1}$ QRJA. ", "page_idx": 14}, {"type": "text", "text": "B Missing Proofs in Section 4 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Proof of Theorem 1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Theorem 1. Let $p\\geq1$ be an absolute constant. Consider $\\ell_{p}$ QRJA in Definition 2 with loss function $f(t)=t^{p}$ . Assume all input numbers are polynomially bounded in $m$ . We can solve $\\ell_{p}$ QRJA in time $O(m^{1+o(1)})$ with $\\exp(-\\log^{c}m)$ additive error for any constant $c>0$ . ", "page_idx": 14}, {"type": "text", "text": "Proof of Theorem 1 (when $p=1$ ): We proved Theorem 1 for $p>1$ in Section 4.1. It remains to consider $p=1$ . ", "page_idx": 14}, {"type": "text", "text": "When $p=1$ , the overall loss function of QRJA is a sum of absolute values of some linear terms. We can therefore formulate $\\ell_{1}$ QRJA as the following linear program (LP), as observed in [Zhang et al., 2019]: ", "page_idx": 14}, {"type": "image", "img_path": "37CyA1K0vV/tmp/49e98da7f781da45989a1874b4f1bd15162cf26bb4c1497f704e2f3b934e4bd8.jpg", "img_caption": [], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "For this LP, Zhang et al. [2019] gave a faster algorithm than using general-purpose LP solvers. ", "page_idx": 14}, {"type": "text", "text": "Lemma 3 (Zhang et al. 2019). There is a reduction from $\\ell_{1}$ QRJA to Minimum Cost Flow with $O(n)$ vertices and $O(\\bar{m})$ edges in $O(T_{\\mathrm{SSSP}}(n,m,W))$ time, where $T_{\\mathrm{SSSP}}(n,m,W)$ is the time required to solve Single-Source Shortest Path with negative weights on a graph with n vertices, m edges, and maximum absolute distance W. ", "page_idx": 14}, {"type": "text", "text": "Using this reduction (Lemma 3) together with the SSSP algorithm in Bernstein et al. [2022] and the minimum cost flow algorithm in Chen et al. [2022], we have an algorithm for $\\ell_{1}$ QRJA that runs in time $O(m^{1+o(1)})$ . \u25a0 ", "page_idx": 14}, {"type": "text", "text": "B.2 Proof of Theorem 2 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Theorem 2. For any $p<1$ , there exists a constant $c>0$ such that it is NP-hard to approximate $\\ell_{p}$ QRJA within a multiplicative factor of $\\textstyle\\left(1+{\\frac{c}{n^{2}}}\\right)$ . ", "page_idx": 15}, {"type": "text", "text": "Recall the reduction from Max-Cut to $\\ell_{p}$ QRJA: Given an instance of Max-Cut with an undirected graph $G=(V,E)$ , let $n=|V|,m=|E|$ and let 12\u2212np + 1, w1 = nw2 + 1. We construct an instance of $\\ell_{p}$ QRJA with $n+2$ candidates $V\\cup\\{v^{(s)},v^{(t)}\\}$ and $O(n+m)$ quantitative relative judgments. Specifically, we construct the followings judgments: ", "page_idx": 15}, {"type": "text", "text": "$(v^{(t)},v^{(s)},1)$ with weight $w_{1}$ .   \n\u2022 $(\\boldsymbol{v}^{(s)},\\boldsymbol{u},0)$ with weight $w_{2}$ for each $u\\in V$ .   \n\u2022 $(\\boldsymbol{v}^{(t)},\\boldsymbol{u},0)$ with weight $w_{2}$ for each $u\\in V$ .   \n\u2022 $(u,v,1),(v,u,1)$ with weight 1 for each $(u,v)\\in E$ . ", "page_idx": 15}, {"type": "text", "text": "To show validity of the reduction above, we will first establish integrality of any optimal solution. ", "page_idx": 15}, {"type": "text", "text": "Lemma 4. Any optimal solution of the $\\ell_{p}$ QRJA instance described in the above reduction is integral.   \nMoreover, all variables must be either 0 or 1 up to a global constant shift. ", "page_idx": 15}, {"type": "text", "text": "We need an inequality for the proof of Lemma 4. ", "page_idx": 15}, {"type": "text", "text": "Lemma 5. For any $d\\in(0,{\\frac{1}{2}}]$ , $p\\in(0,1)$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n1-(1-d)^{p}\\leq p d^{p}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof of Lemma 5: Fix $p\\in(0,1)$ . Let $f(d)=p d^{p}-1+(1-d)^{p}$ . We have ", "page_idx": 15}, {"type": "equation", "text": "$$\nf^{\\prime}(d)=p(p d^{p-1}-(1-d)^{p-1}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Note that $f^{\\prime}$ is decreasing for $d\\in(0,1)$ . In other words, $f$ is single peaked on $\\left(0,{\\frac{1}{2}}\\right]$ and continuous at 0. Now we only have to check that $f(0)\\geq0$ , which is trivial, and $f\\left({\\frac{1}{2}}\\right)\\geq0$ . For the latter, let ", "page_idx": 15}, {"type": "equation", "text": "$$\ng(p)=(p+1)0.5^{p}-1.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "$g(p)\\geq0$ for $p\\in[0,1]$ since $g(p)$ is concave on [0, 1] and $g(0)=g(1)=0$ . The lemma then follows. ", "page_idx": 15}, {"type": "text", "text": "We then proceed to prove Lemma 4. ", "page_idx": 15}, {"type": "text", "text": "Proof of Lemma 4: Let $x_{a}$ be the potential of candidate $a$ in $\\ell_{p}$ QRJA. W.l.o.g. assume that in any solution, $x_{v^{(s)}}=0$ . We first show that if $x_{v^{(t)}}\\neq1$ , then moving it to 1 strictly improves the solution. Suppose $|x_{v^{(t)}}-1|=d$ . By moving $x_{v}(t)$ to 1, we decrease the loss on the judgment $(v^{(t)},v^{(s)},1)$ by $w_{1}d^{p}$ . For other judgments $(v^{(t)},u)$ incident on $\\boldsymbol{v}^{(t)}$ , the loss increase by no more than $w_{2}d^{p}$ , since ", "page_idx": 15}, {"type": "equation", "text": "$$\n|(x_{v^{(t)}}\\pm d)-x_{u}|^{p}\\leq|x_{v^{(t)}}-x_{u}|^{p}+d^{p}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Overall, the cost decreases by at least ", "page_idx": 15}, {"type": "equation", "text": "$$\nw_{1}d^{p}-n w_{2}d^{p}=d^{p}>0.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Now we show moving any fractional $x_{u}$ to the closest value in $\\{0,1\\}$ strictly improves the solution. There are two cases: ", "page_idx": 15}, {"type": "text", "text": "\u2022 $x_{u}\\in(0,1)$ . W.l.o.g. $\\textstyle x_{u}\\in(1,{\\frac{1}{2}}]$ and we try to move it to 0 by a displacement of $d=x_{u}$ . The total loss on $(\\boldsymbol{v}^{(s)},\\boldsymbol{u},0)$ and $(\\boldsymbol{v}^{(t)},\\boldsymbol{u},0)$ decreases by $w_{2}(d^{p}+(1-d)^{p}-1)$ , while the total cost on judgments of form $(u,v,1)$ and $(v,u,1)$ can increase by no more than $n\\dot{(}d^{p}+(2+d)^{p}-2^{p})$ . With Lemma 5, we see that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{w_{2}(d^{p}+(1-d)^{p}-1)\\geq w_{2}(d^{p}-p d^{p})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad>2n d^{p}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\quad\\geq n(d^{p}+(2+d)^{p}-2^{p}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "So, there is a positive improvement from rounding $x_{u}$ . ", "page_idx": 15}, {"type": "text", "text": "\u2022 $x_{u}\\notin[0,1]$ . W.l.o.g. $x_{u}<0$ and we try to move it to 0 by a displacement of $d=-x_{u}$ . The total loss on $(v^{(s)},u,0)$ and $(\\boldsymbol{v}^{(t)},\\boldsymbol{u},0)$ decreases by $w_{2}(d^{p}+(1+d)^{p}-1)$ , while the total cost on edges of form $(u,v,1)$ and $(v,u,1)$ can increase by no more than $n(d^{p}+(2+d)^{p}-2^{p})$ . And ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{w_{2}(d^{p}+(1+d)^{p}-1)\\geq w_{2}d^{p}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad>2n d^{p}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\quad\\geq n(d^{p}+(2+d)^{p}-2^{p}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We conclude that in any optimal solution, $x_{v^{(s)}}=0$ , $x_{v^{(t)}}=1$ , and for any $u\\in V$ , $x_{u}\\in\\{0,1\\}$ . ", "page_idx": 16}, {"type": "text", "text": "Next, we present a lemma that shows the connection between solutions in the Max-Cut instance and those in the constructed $\\ell_{p}$ QRJA instance. ", "page_idx": 16}, {"type": "text", "text": "Lemma 6. A Max-Cut instance has a solution of size at least $k$ iff its corresponding $\\ell_{p}$ QRJA instance has a solution of loss at most $n w_{2}+2(m-k)+k2^{p}$ . Moreover, with such a solution to the $\\ell_{p}$ QRJA instance, one can construct a Max-Cut solution of the claimed size. ", "page_idx": 16}, {"type": "text", "text": "Proof of Lemma 6: Given a Max-Cut solution $(S,T)$ of size at least $k$ , setting the potentials of the vertices in $S$ and $T$ to be 0 and 1 respectively gives an $\\ell_{p}$ QRJA solution with loss at most $n w_{2}+2(m-k)+k2^{p}$ . ", "page_idx": 16}, {"type": "text", "text": "Given a $\\ell_{p}$ QRJA solution of loss at most $n w_{1}+2(m-k)+k2^{p}$ , we first round the solution to the form stated in Lemma 4. This improves the solution. The two vertex sets $U=\\{u\\in V\\mid x(u)=0\\}$ and $V=\\{v\\in V\\mid x(v)=1\\}$ then form a Max-Cut solution of size at least $k$ . ", "page_idx": 16}, {"type": "text", "text": "We are now ready to prove Theorem 2. ", "page_idx": 16}, {"type": "text", "text": "Proof of Theorem 2: According to Lemma 6, any approximation with an additive error less than $2-2^{p}$ of the constructed $\\ell_{p}$ QRJA instance can be rounded to produce an optimal solution to Max-Cut. Since Max-Cut is NP-Hard and the constructed $\\ell_{p}$ QRJA instance\u2019s optimal solution has loss $\\Theta(n^{2}+m)$ , the theorem follows. \u25a0 ", "page_idx": 16}, {"type": "text", "text": "C Additional Experiments ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "C.1 L2 Variant of Quantitative Loss ", "text_level": 1, "page_idx": 16}, {"type": "image", "img_path": "37CyA1K0vV/tmp/ad1e66a755df838fed782e33bde94c2732cd0f1e06f0da8e8d0b5bee8ee91047.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Figure 6: L2 quantitative loss of the algorithms on all four datasets used in Section 5. Error bars are not shown here as the algorithms are deterministic. Similar to Fig. 4, the results show that both versions of QRJA perform consistently well across the tested datasets. ", "page_idx": 16}, {"type": "text", "text": "We include in this subsection experiment results using average squared error as the quantitative metric. We call this metric L2 quantitative loss. Specifically, for each contest, we predict the difference in numerical scores of all pairs of contestants that have both appeared before. We then compute the L2 quantitative loss as the average squared error of the predictions, and normalize it by the L2 quantitative loss of the trivial prediction that always predicts 0 for all pairs. ", "page_idx": 16}, {"type": "text", "text": "The results are shown in Fig. 6. We observe that both versions of QRJA still perform consistently well compared to other algorithms across the tested datasets. This is consistent with the results using the (L1) quantitative loss in Section 5. ", "page_idx": 17}, {"type": "text", "text": "Additionally, $\\ell_{2}$ QRJA performs slightly better than $\\ell_{1}$ QRJA on this metric. This is expected because this metric is more aligned with the $\\ell_{2}$ QRJA\u2019s loss function. ", "page_idx": 17}, {"type": "text", "text": "C.2 Performance Experiments on More Datasets ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We include in this subsection the performance experiments on three more datasets. The new datasets are listed below. ", "page_idx": 17}, {"type": "text", "text": "\u2022 Cross-Tables. This dataset contains the results of cross-tables (a crossword-style word game) tournaments (https://www.cross-tables.com/) from 2000 to 2023. Each contest is a round-robin tournament involving around 8 contestants. A contestant\u2019s numerical score is his/her number of wins in the tournament. There are 1215 contests and 1912 contestants in this dataset. ", "page_idx": 17}, {"type": "text", "text": "\u2022 F1-Full. This dataset is an alternative version of F1. In F1-Full, we choose to additionally include contestants who do not complete the whole race. Now the contestants are ranked first by the number of laps they finish, and then their finishing time. A contestant\u2019s numerical score is the negative of the contestant\u2019s finishing time (in seconds). If the contestant does not finish all laps, we add a large penalty (1000 seconds) for each lap the contestant fails to finish. There are 878 contests and 606 contestants in this dataset.   \n\u2022 Codeforces-Core. This dataset is a modified version of Codeforces. We only keep contestants who have participated in at least half of the contests in this dataset. We test on this modified dataset because all other datasets we use in the experiments are sparse datasets (i.e., contestants participate in a small fraction of the contests on average), so we want to see what happens on dense ones. There are 327 contests and 17 contestants in total. ", "page_idx": 17}, {"type": "text", "text": "We evaluate $\\ell_{1}$ and $\\ell_{2}$ QRJA using the same metrics against the same set of benchmarks as in Section 5 on these three datasets. The results are shown in Fig. 7. We highlight a few extra observations below. ", "page_idx": 17}, {"type": "text", "text": "Extra observations on Cross-Tables. In terms of ordinal accuracy, Median performs the best among the tested algorithms on Cross-Tables. However, in terms of quantitative loss, Median is the worst algorithm among the tested ones. Moreover, it mostly performs suboptimally on other datasets as shown in Figs. 4 and 7. This shows that although Median is occasionally good in performance, it fails in other cases. ", "page_idx": 17}, {"type": "text", "text": "Extra observations on F1-Full. On F1-Full, both MF and $\\ell_{2}$ QRJA and perform considerably worse than $\\ell_{1}$ QRJA. This is not seen in other datasets. We believe this is because our score calculation results in a large variance in contestants\u2019 scores on F1-Full, which makes it harder for these methods to make good predictions. This also shows that $\\ell_{1}$ QRJA is more robust to datasets with large variances in contestants\u2019 performance than these methods. We also notice that Borda and Kemeny-Young perform well on F1-Full, which is consistent with their good performance on F1. ", "page_idx": 17}, {"type": "text", "text": "Extra observations on Codeforces-Core. In terms of ordinal accuracy, all tested algorithms except Borda perform well. In terms of quantitative loss, MF and Median are worse than the other ones. This shows that on a dense dataset like Codeforces-Core, most algorithms can make good predictions. Moreover, MF does not have a clear advantage over other algorithms in our problem even if the dataset is dense. ", "page_idx": 17}, {"type": "text", "text": "C.3 Subsampling Experiments on More Datasets ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We also conduct the subsampling experiments in Appendix A.2 on all other 5 datasets. The results are shown in Fig. 8. ", "page_idx": 17}, {"type": "text", "text": "Experiment results. The message here is the same as that in Appendix A.2. In particular, Algorithm 1 can reduce the number of judgments with only a minor loss in performance as long as the subsample rate $\\alpha$ is not too small. Note that in some of the figures, like Fig. 8c, the errors seem to be large visually. This is because of the small scale of the y-axis (only $0.6\\%$ for Fig. 8c). The actual errors are small. Moreover, we observe that the performance of $\\ell_{2}$ QRJA is slightly more robust to subsampling than that of $\\ell_{1}$ QRJA. This is consistent with the results in Appendix A.2. ", "page_idx": 17}, {"type": "image", "img_path": "37CyA1K0vV/tmp/8cf0e4d557837521eda55e53bb3fe83c40c61d19c555d5dfe6f11722f748d94d.jpg", "img_caption": ["Figure 7: The performance of the algorithms on Cross-Tables, F1-Full, and Codeforces-Core. Error bars are not shown as the algorithms are deterministic. The results show that $\\ell_{1}$ QRJA still performs consistently well across the tested datasets. However, $\\ell_{2}$ QRJA performs considerably worse than $\\ell_{1}$ QRJA on F1-Full. This is not seen in other datasets. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "C.4 Experiments about Matrix Factorization ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Recall that in Section 5, we only show results of one version of Matrix Factorization (MF). We include in this subsection the experiments involving different variants of Matrix Factorization as well as their implementation details. ", "page_idx": 18}, {"type": "text", "text": "Implementation details. We have implemented two variants of MF: Low-Rank MF and Additive MF. The MF algorithm used in Section 5 is Low-Rank MF with rank $r\\,=\\,1$ . We describe the implementation details below. ", "page_idx": 18}, {"type": "text", "text": "\u2022 Low-Rank MF. Recall that in the context of our experiments, we can view each contestant as a row and each contest as a column. The score of a contestant in a contest is the entry in the corresponding row and column. A classical model of MF Koren et al. [2009] is factorizing $\\mathbf{A}\\in\\bar{\\mathbb{R}}^{n\\times m}$ as the product of two low-rank matrices $\\mathbf{U}\\mathbf{V}^{\\top}$ , where $\\mathbf{U}\\,\\in\\,\\mathbb{R}^{n\\times r}$ , $\\mathbf{V}\\,\\in\\,\\mathbb{R}^{m\\times\\bar{r}}$ for some small $r$ . Note that in our experiments, the algorithm is required to predict a new column of A with no known entries. Therefore, we cannot directly apply this method since the corresponding row of $\\mathbf{V}$ will remain unchanged after initialization. To solve this problem, we instead predict every column with known entries in $\\mathbf{A}$ and then take the average of the predictions as the prediction for the new column. We use the standard loss function that sums up the squared errors of all observed entries. We implement this method with SciPy [Jones et al., 2014] and use gradient descent for a fixed number of epochs on a deterministic initialization to keep the results deterministic. We test $r=1,2,5$ in this subsection. ", "page_idx": 18}, {"type": "text", "text": "\u2022 Additive MF. We also consider an additive variant of MF. For $\\mathbf{x}\\in\\mathbb{R}^{n},\\mathbf{y}\\in\\mathbb{R}^{m}$ , this method predicts $A_{i,j}\\,=\\,x_{i}+y_{j}$ . Here, $x_{i}$ can be viewed as contestant $i$ \u2019s skill level, and $y_{j}$ can be interpreted as the (inversed) difficulty of contest $j$ . We then use the vector $\\mathbf{x}$ to make predictions. Note that this version of MF resembles QRJA in that for each of these two methods, the loss function is 0 if $A_{i,j}=x_{i}+y_{j}$ holds for the known entries. We also use the standard sum of the squared loss function and use gradient descent for a fixed number of epochs on a deterministic initialization to keep it deterministic. ", "page_idx": 18}, {"type": "text", "text": "Performance experiments. We first evaluate these variants of MF using the same metrics as in Section 5 on all datasets. The results are shown in Fig. 9. We can see that R1 MF and Additive MF ", "page_idx": 18}, {"type": "image", "img_path": "37CyA1K0vV/tmp/dc4b01a4721e237fc1acdf72b6fffcd896987dea6aed1fd6fc05ee29e8936f96.jpg", "img_caption": ["(a) QRJA\u2019s ordinal accuracy on F1 "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "37CyA1K0vV/tmp/8c110134536109c8f93b9e7051c73b97d96d3b20059b3a507f5a449b2d9731ef.jpg", "img_caption": ["(c) QRJA\u2019s ordinal accuracy on Marathon "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "37CyA1K0vV/tmp/51b23fa7d0ebb0e6cfd81f5c697c5ad90f1e76393f52aabc5e74a22b6212f554.jpg", "img_caption": ["(e) QRJA\u2019s ordinal accuracy on Codeforces "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "37CyA1K0vV/tmp/432b980c06ce2f82591ad908608d4a21faba6eccbb53bb98c4865449007427ba.jpg", "img_caption": ["(g) QRJA\u2019s ordinal accuracy on Cross-Tables "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "37CyA1K0vV/tmp/f4a673e20180ffa7b8f0d77f686cce1d89b2e44f119147fd381100ba3a5b402e.jpg", "img_caption": ["(i) QRJA\u2019s ordinal accuracy on F1-Full "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "37CyA1K0vV/tmp/6049fbe20a54e9bfc9344c48dc13759c910acf3968f56caa2f8453cf1966b0c7.jpg", "img_caption": ["(k) QRJA\u2019s ordinal accuracy on Codeforces-Core "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "37CyA1K0vV/tmp/dd016e170890c9209a5939f9c751cdbadb55b577d1c5d4bc49f70b9ca6f2e3f3.jpg", "img_caption": ["(l) QRJA\u2019s quantitative loss on Codeforces-Core "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 8: The performance of $\\ell_{1}$ and $\\ell_{2}$ QRJA after subsampling judgments using Algorithm 1 with equal weights for all judgments. The subsample rate $\\alpha$ means $M=\\lfloor\\alpha m\\rfloor$ in Algorithm 1. Error bars indicate the standard deviation. The results show that Algorithm 1 can reduce the number of judgments to a factor less than 1.0 with a minor loss in performance in the used datasets. Note that errors in some figures appear large because of the small scale of the y-axis. The actual errors are small. ", "page_idx": 19}, {"type": "image", "img_path": "37CyA1K0vV/tmp/f5515057ff2ca188acd1c400f031f0a04ff87f4e4c60e1b4e75b6ff23c7588d3.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "", "img_caption": ["(m) MF\u2019s ordinal accuracy on Codeforces-Core "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "", "img_caption": ["Figure 9: The performance of different variants of Matrix Factorization. The results show that R1 MF and Additive MF generally have similar performance. In contrast, R2 and R5 MF perform worse than the former. ", "(n) MF\u2019s quantitative loss on Codeforces-Core "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "37CyA1K0vV/tmp/f52681028e7d479b20addc1c0563c03de71efae330318c7648ac6edcbd0b7cbe.jpg", "img_caption": ["${(\\mathfrak{m})}$ MF\u2019s ordinal accuracy on Codeforces-Core "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "37CyA1K0vV/tmp/524edec79e58b6e38808f17062cd6e0b13feab3130dab3f368d842666acdbb44.jpg", "img_caption": ["(n) MF\u2019s quantitative loss on Codeforces-Core "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure 10: The performance of Matrix Factorization with different numbers of training epochs on all datasets. The results generally show that R1 MF outperforms R2 and $\\mathrm{R}5\\ \\mathrm{MF}.$ Moreover, on some datasets, R2 and R5 MF\u2019s performance worsens as the number of training epochs increases. In contrast, R1 MF\u2019s performance improves as the number of training epochs increases. ", "page_idx": 21}, {"type": "image", "img_path": "37CyA1K0vV/tmp/70923b07910826bce12b15efc68c9d2ab15959c334288d8107366a2b23c12100.jpg", "img_caption": [""], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "", "img_caption": ["Figure 11: Entrywise L1 and L2 loss of Matrix Factorization, Mean, and Median. The results show that on most datasets, R1 MF outperforms R2 and R5 MF. The exceptions are F1-Full and Codeforces-Core. Moreover, Matrix Factorization does not have a clear advantage over Mean and Median on any dataset in terms of entrywise metrics. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "generally have similar performance. In contrast, R2 and R5 MF perform worse than the former. We therefore choose to present R1 MF in Section 5. ", "page_idx": 23}, {"type": "text", "text": "Low-Rank MF\u2019s performance over training. The observation that R2 and R5 MF perform worse than R1 MF is surprising to us. To confirm this observation, we plot the performance of these variants of MF with different numbers of training epochs on all datasets. The results are shown in Fig. 10. We can see that R1 MF generally outperforms R2 and R5 MF in terms of both ordinal accuracy and quantitative loss when trained for long enough. Moreover, R1 MF\u2019s performance on both metrics generally improves as the number of training epochs increases (the only exception is quantitative loss on F1-Full). In contrast, R2 and R5 MF\u2019s performance in terms of both metrics worsens as the number of training epochs increases on Chess, F1, and Codeforces. These observed phenomena suggest that R2 and R5 MF tend to overfit the data. The problem for R1 MF is less severe. ", "page_idx": 23}, {"type": "text", "text": "Experiment results on entrywise metrics. As the metrics in Section 5 are defined in a pairwise fashion and might not be well-suited for MF, we also evaluate the performance of MF in terms of entrywise L1 and L2 loss (i.e., the average absolute and squared error of the predictions on each contestant\u2019s actual score in each contest). We also normalize each of these losses by the corresponding loss of the trivial all-zero prediction. The results are shown in Fig. 11. Note that QRJA and Additive MF are not included, because their predictions can be shifted by an arbitrary constant, and thus entrywise losses do not apply to them. We can see that in terms of entrywise L1 and L2 loss, R1 MF outperforms R2 and R5 MF on most datasets. The exceptions are F1-Full and Codeforces-Core. These two datasets are different from the other ones in that F1-Full\u2019s scores are calculated with two numbers (the number of laps finished and the finishing time) and Codeforces-Core is a dense dataset constructed from Codeforces. Therefore, on these datasets, MF with higher ranks might be more suitable than R1 MF, while on the other datasets, they tend to overfti the training data. Moreover, we note that on entrywise metrics, MF generally performs worse than Mean and Median. ", "page_idx": 23}, {"type": "text", "text": "Summary of experiment results. In summary, experiments in this subsection show that on our datasets, R1 MF and Additive MF, which are similar in performance, generally perform better than R2 and R5 MF. Therefore, we choose to include only the results of R1 MF in Section 5. ", "page_idx": 23}, {"type": "text", "text": "D Axiomatic Characterization of $\\ell_{p}$ QRJA ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We characterize $\\ell_{p}$ QRJA by giving a set of axioms for the family of transformation functions $f$ of pairwise loss that we consider. We show that those transformation functions considered in $\\ell_{p}$ QRJA are essentially the minimum set of functions satisfying these axioms. ", "page_idx": 23}, {"type": "text", "text": "Recall that for each judgment about $a$ and $b$ where $a$ is better $b$ by $y$ units, the absolute error of the prediction vector $\\mathbf{x}$ on this pair is $|x_{a}-x_{b}-y|$ . Using this as the loss function, we obtain the $\\ell_{1}$ QRJA rule, which has been characterized using axioms in the context of social choice theory Conitzer et al. [2016]. Below we extend this characterization to $\\ell_{p}$ QRJA for any positive rational number $p\\in\\mathbb{Q}_{+}$ . Note that restricting $p$ to be rational is without loss of generality, since the output of $\\ell_{p}$ QRJA is continuous in $p$ . ", "page_idx": 23}, {"type": "text", "text": "We consider transforming the absolute error by a transformation function $f$ to obtain the actual pairwise loss, which is $f(\\bar{|}x_{a}-x_{b}-y|)$ . For $\\ell_{p}$ QRJA, the transformation function is $f(t)=t^{p}$ . To characterize QRJA as a family of rules (for different $p\\in\\mathbb{Q}_{+})$ , we give axioms for the corresponding family of transformation functions, i.e., $t^{p}$ for $p\\in\\mathbb{Q}_{+}$ . Let $\\mathcal{F}$ be a family of transformation functions. ", "page_idx": 23}, {"type": "text", "text": "Below are the axioms we consider: ", "page_idx": 23}, {"type": "text", "text": "\u2022 Identity. There is an identity transformation $f_{0}\\in\\mathcal{F}$ , such that $f_{0}(t)=t$ for any $t\\geq0$ .   \n\u2022 Invertibility. For each $f_{1}\\in\\mathcal{F}$ , there is an $f_{2}\\in\\mathcal{F}$ such that $f_{1}$ composed with $f_{2}$ is identity, i.e., for any $t\\geq0$ , $f_{1}(f_{2}(t))=t.$   \n\u2022 Closedness under multiplication. For any $f_{1},f_{2}\\in{\\mathcal{F}}$ , there exists $f_{3}\\in\\mathcal{F}$ such that for any $t\\geq0$ , $f_{1}(t)\\cdot f_{2}(t)=f_{3}(t).$ ", "page_idx": 23}, {"type": "text", "text": "We show below that the family of transformation functions corresponding to the $\\ell_{p}$ QRJA rules is the minimum family of functions $\\mathcal{F}^{*}$ satisfying the above axioms. By the first axiom, the identity transformation $f_{0}$ where $f_{0}(t)=t$ is in $\\mathcal{F}^{*}$ . (This corresponds to $\\ell_{1}$ QRJA.) Then by the third axiom, for any $k\\in\\mathbb{Z}_{+}$ , $f_{0}^{k}$ is also in ${\\mathcal{F}}^{*}$ , where $f_{0}^{k}(t)=t^{k}$ . And by the second axiom, for any $k\\in\\mathbb{Z}_{+}$ , $f_{0}^{1/k}$ is also in ${\\mathcal{F}}^{*}$ , where $f_{0}^{1/k}(t)=t^{1/k}$ . This is because $f_{0}^{1/k}(f_{0}^{k}(t))=t$ . Finally, for any $r\\in\\mathbb{Q}_{+}$ where $r=p/q$ for $p,q\\in\\mathbb{Z}_{+}$ , by the third axiom, $f_{0}^{r}=(f_{0}^{1/q})^{p}$ is in $\\mathcal{F}^{*}$ , where $f_{0}^{r}(t)=t^{r}$ . ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "Note that the above argument establishes that $\\mathcal{F}^{*}$ contains all transformation functions corresponding to QRJA, i.e., ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\{t^{r}\\mid r\\in\\mathbb{Q}_{+}\\}\\subseteq\\mathcal{F}^{*}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Below we show the other direction, i.e., $\\{t^{r}\\mid r\\in\\mathbb{Q}_{+}\\}$ satisfy the 3 axioms, and as a result, ", "page_idx": 24}, {"type": "equation", "text": "$$\n{\\mathcal{F}}^{*}\\subseteq\\{t^{r}\\mid r\\in\\mathbb{Q}_{+}\\}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "For $f_{1}(t)=t^{r_{1}},f_{2}(t)=t^{r_{2}}$ where $r_{1},r_{2}\\in\\mathbb{Q}_{+}$ , we have ", "page_idx": 24}, {"type": "equation", "text": "$$\nf_{1}(t)\\cdot f_{2}(t)=t^{r_{1}+r_{2}},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $r_{1}+r_{2}\\in\\mathbb{Q}_{+}$ , and ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f_{1}(f_{2}(t))=(t^{r_{2}})^{r_{1}}=t^{r_{1}\\cdot r_{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $r_{1}\\cdot r_{2}\\in\\mathbb{Q}_{+}$ . This implies ${\\mathcal{F}}^{*}\\subseteq\\{t^{r}\\mid r\\in\\mathbb{Q}_{+}\\}$ . Thus ${\\mathcal{F}}^{*}=\\{t^{r}\\mid r\\in\\mathbb{Q}_{+}\\}$ as desired. ", "page_idx": 24}, {"type": "text", "text": "E Copyright Information for Datasets Used ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "The datasets used in this paper are collected from publicly available websites either manually or through an API. We provide the following information about these datasets. ", "page_idx": 24}, {"type": "text", "text": "\u2022 Chess. Copyright: $\\copyright$ 2023 - Tata Steel Chess Tournament. Data collected is subject to the website\u2019s Terms of Conditions, available at https://tatasteelchess.com/ terms-and-conditions/.   \n\u2022 F1. Copyright: $\\copyright$ 2003-2024 Formula One World Championship Limited. Data collected is subject to the website\u2019s Terms of Use, available at https://account.formula1.com/#/ en/terms-of-use.   \n\u2022 Marathon. Copyright: $\\copyright$ 2000-2024, All Rights Reserved by MarathonGuide.com LLC. Data collected is subject to the website\u2019s Policy, available at https://www. marathonguide.com/Policy.cfm.   \n\u2022 Codeforces. Copyright: \u00a9 2010-2024 Mike Mirzayanov. Data collected is subject to the website\u2019s Terms and Conditions, available at https://codeforces.com/terms.   \n\u2022 Cross-Tables. Copyright: $\\copyright$ 2005-2024 Seth Lipkin and Keith Smith. Data collected is subject to the website\u2019s Policy, available at https://www.cross-tables.com/privacy. html. ", "page_idx": 24}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The main contributions are summarized at the end of the introduction. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 25}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: We briefly discuss the limitations of our work in Section 7. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 25}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The theoretical results are stated with the full set of assumptions and their proofs are provided either in Section 4 or in Appendices A and B. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 26}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The code and data are provided in the supplemental materials, including an automated test script to reproduce the experimental results stated in the paper. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 26}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The code and data are provided in the supplemental materials, including an automated test script to reproduce the experimental results stated in the paper. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 27}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: The experiment settings in Section 5 and Appendices A and C aim to provide necessary details to understand the results. The full details are provided with the code. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 27}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: We state in the caption of the figures that \u201cerror bars are not shown here as the algorithms are deterministic\u201d, which is appropriate information about statistical significance. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 27}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: It is stated in Section 5 that \u201cAll experiments are done on a server with 56 CPU cores and 504G RAM. The experiments in Section 5 and Appendices A and C take around 2 weeks in total to run on this server. No GPU is used.\u201d ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 28}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: We have reviewed the Code of Ethics and believe that our paper conforms to it. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 28}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We briefly discuss the boarder impacts of our work in Section 7. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 28}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 29}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper does not release data or models that have a high risk for misuse. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Any existing code package used in the paper is properly cited in Section 5. The datasets used in the paper are publicly available and their copyright information are explicitly mentioned in Appendix E. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: The uploaded code is accompanied by a README file that documents the overall usage of it, and for each individual source flie, comments are provided to explain the purpose of the file and the functions defined in it. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 30}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 30}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 30}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 31}]