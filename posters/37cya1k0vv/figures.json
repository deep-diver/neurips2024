[{"figure_path": "37CyA1K0vV/figures/figures_7_1.jpg", "caption": "Figure 4: Ordinal accuracy and quantitative loss of the algorithms on all four datasets. Error bars are not shown here as the algorithms are deterministic. The results show that both versions of QRJA perform consistently well across the tested datasets.", "description": "This figure presents the performance comparison of various ranking algorithms on four different datasets.  The algorithms being compared are L1QRJA, L2QRJA, Median, Mean, Borda, Kemeny-Young, and Matrix Factorization (MF). Two metrics are used for evaluation: ordinal accuracy (the percentage of correctly predicted relative rankings) and quantitative loss (the average absolute error in predicting the difference in numerical scores between contestants). The results consistently demonstrate the effectiveness of both L1QRJA and L2QRJA across all four datasets, showcasing their superior performance in comparison to other methods.", "section": "5 Experiments"}, {"figure_path": "37CyA1K0vV/figures/figures_14_1.jpg", "caption": "Figure 8: The performance of l1 and l2 QRJA after subsampling judgments using Algorithm 1 with equal weights for all judgments. The subsample rate \u03b1 means M = [\u03b1m] in Algorithm 1. Error bars indicate the standard deviation. The results show that Algorithm 1 can reduce the number of judgments to a factor less than 1.0 with a minor loss in performance in the used datasets. Note that errors in some figures appear large because of the small scale of the y-axis. The actual errors are small.", "description": "This figure displays the results of experiments on the effect of subsampling judgments using Algorithm 1 on the performance of l1 and l2 QRJA.  It shows that subsampling can significantly reduce the number of judgments required, while maintaining relatively good performance, across multiple datasets. Note that some error bars seem large due to the scale of the y-axis.", "section": "Experiments"}, {"figure_path": "37CyA1K0vV/figures/figures_14_2.jpg", "caption": "Figure 8: The performance of l1 and l2 QRJA after subsampling judgments using Algorithm 1 with equal weights for all judgments. The subsample rate \u03b1 means M = [\u03b1m] in Algorithm 1. Error bars indicate the standard deviation. The results show that Algorithm 1 can reduce the number of judgments to a factor less than 1.0 with a minor loss in performance in the used datasets. Note that errors in some figures appear large because of the small scale of the y-axis. The actual errors are small.", "description": "The figure shows the performance of L1 and L2 QRJA algorithms after applying a subsampling method (Algorithm 1) on several datasets.  The x-axis represents the subsample rate (\u03b1), which determines the number of samples used. The y-axis shows both the ordinal accuracy and quantitative loss. The results indicate that Algorithm 1 effectively reduces the number of judgments needed while maintaining relatively good performance.  Error bars showing standard deviation are included.", "section": "Experiments"}, {"figure_path": "37CyA1K0vV/figures/figures_14_3.jpg", "caption": "Figure 4: Ordinal accuracy and quantitative loss of the algorithms on all four datasets. Error bars are not shown here as the algorithms are deterministic. The results show that both versions of QRJA perform consistently well across the tested datasets.", "description": "The figure shows the performance comparison of several algorithms, including two versions of QRJA (Quantitative Relative Judgment Aggregation), on four different datasets using two metrics: ordinal accuracy and quantitative loss.  The algorithms are compared in terms of their ability to predict the ranking of contestants in various contests. The results demonstrate that QRJA consistently performs well across all datasets.", "section": "5 Experiments"}, {"figure_path": "37CyA1K0vV/figures/figures_16_1.jpg", "caption": "Figure 4: Ordinal accuracy and quantitative loss of the algorithms on all four datasets. Error bars are not shown here as the algorithms are deterministic. The results show that both versions of QRJA perform consistently well across the tested datasets.", "description": "This figure presents the performance of different algorithms on four datasets (Chess, F1, Marathon, Codeforces) using two metrics: ordinal accuracy and quantitative loss.  The results demonstrate that both versions of QRJA (l1 and l2) consistently achieve high performance across all datasets, showcasing their effectiveness in predicting contest outcomes.", "section": "5 Experiments"}, {"figure_path": "37CyA1K0vV/figures/figures_18_1.jpg", "caption": "Figure 4: Ordinal accuracy and quantitative loss of the algorithms on all four datasets. Error bars are not shown here as the algorithms are deterministic. The results show that both versions of QRJA perform consistently well across the tested datasets.", "description": "This figure compares the performance of various ranking algorithms, including two versions of the proposed QRJA method (l1 and l2), against baselines like Mean, Median, Borda, Kemeny-Young, and Matrix Factorization on four real-world datasets.  The results visualize both ordinal accuracy (percentage of correct relative ranking predictions) and quantitative loss (average absolute error of relative quantitative predictions, normalized).  The key observation is that QRJA consistently performs well across all datasets and metrics.", "section": "5 Experiments"}, {"figure_path": "37CyA1K0vV/figures/figures_19_1.jpg", "caption": "Figure 8: The performance of l1 and l2 QRJA after subsampling judgments using Algorithm 1 with equal weights for all judgments. The subsample rate \u03b1 means M = [\u03b1m] in Algorithm 1. Error bars indicate the standard deviation. The results show that Algorithm 1 can reduce the number of judgments to a factor less than 1.0 with a minor loss in performance in the used datasets. Note that errors in some figures appear large because of the small scale of the y-axis. The actual errors are small.", "description": "This figure shows the performance of L1 and L2 QRJA algorithms after applying a subsampling technique (Algorithm 1). The x-axis represents the subsample rate (\u03b1), ranging from 0.1 to 1.0, which determines the number of sampled judgments (M=[\u03b1m]). The y-axis shows the ordinal accuracy of the algorithms. The figure demonstrates that Algorithm 1 effectively reduces the number of judgments with only minor impact on the accuracy, particularly when \u03b1 is greater than 0.4.  Error bars are included to illustrate the variability of the results.", "section": "Experiments"}, {"figure_path": "37CyA1K0vV/figures/figures_19_2.jpg", "caption": "Figure 8: The performance of l1 and l2 QRJA after subsampling judgments using Algorithm 1 with equal weights for all judgments. The subsample rate \u03b1 means M = [\u03b1m] in Algorithm 1. Error bars indicate the standard deviation. The results show that Algorithm 1 can reduce the number of judgments to a factor less than 1.0 with a minor loss in performance in the used datasets. Note that errors in some figures appear large because of the small scale of the y-axis. The actual errors are small.", "description": "This figure shows the results of experiments evaluating the performance of L1 and L2 QRJA algorithms after applying a subsampling technique (Algorithm 1).  The x-axis represents the subsampling rate (\u03b1), and the y-axis displays the accuracy. The results demonstrate that subsampling can significantly reduce the number of judgments required while maintaining relatively high accuracy, especially for L2 QRJA, indicating that the subsampling approach is efficient for large-scale datasets.", "section": "Experiments"}, {"figure_path": "37CyA1K0vV/figures/figures_19_3.jpg", "caption": "Figure 5: The performance of l1 and l2 QRJA on Chess after subsampling judgments using Algorithm 1 with equal weights for all judgments. The subsample rate \u03b1 means M = [\u03b1m] in Algorithm 1. Error bars indicate the standard deviation. The results show that Algorithm 1 can reduce the number of judgments to a factor of 0.4 with a minor performance loss on Chess.", "description": "This figure shows the performance of L1 and L2 QRJA algorithms on the Chess dataset after applying a subsampling technique (Algorithm 1). The x-axis represents the subsample rate (\u03b1), indicating the proportion of judgments used. The y-axis displays both ordinal accuracy and quantitative loss. The results demonstrate that subsampling can reduce the number of judgments significantly while incurring minimal loss in performance. Error bars show standard deviation across multiple runs.", "section": "A.2 Subsampling Experiments"}, {"figure_path": "37CyA1K0vV/figures/figures_19_4.jpg", "caption": "Figure 8: The performance of l1 and l2 QRJA after subsampling judgments using Algorithm 1 with equal weights for all judgments. The subsample rate \u03b1 means M = [\u03b1m] in Algorithm 1. Error bars indicate the standard deviation. The results show that Algorithm 1 can reduce the number of judgments to a factor less than 1.0 with a minor loss in performance in the used datasets. Note that errors in some figures appear large because of the small scale of the y-axis. The actual errors are small.", "description": "The figure shows the performance of L1 and L2 QRJA algorithms on multiple datasets after applying a subsampling technique (Algorithm 1).  The x-axis represents the subsample rate (\u03b1), indicating the fraction of judgments kept. The y-axis shows the ordinal accuracy. The results demonstrate that subsampling reduces computation time with minimal impact on accuracy.", "section": "Subsampling Experiments"}, {"figure_path": "37CyA1K0vV/figures/figures_19_5.jpg", "caption": "Figure 8: The performance of l1 and l2 QRJA after subsampling judgments using Algorithm 1 with equal weights for all judgments. The subsample rate \u03b1 means M = [\u03b1m] in Algorithm 1. Error bars indicate the standard deviation. The results show that Algorithm 1 can reduce the number of judgments to a factor less than 1.0 with a minor loss in performance in the used datasets. Note that errors in some figures appear large because of the small scale of the y-axis. The actual errors are small.", "description": "This figure shows the performance of L1 and L2 QRJA algorithms after applying a subsampling technique (Algorithm 1).  The x-axis represents the subsample rate (\u03b1), indicating the fraction of judgments used. The y-axis shows both ordinal accuracy (top row) and quantitative loss (bottom row) for each algorithm across several datasets.  The results demonstrate that subsampling, using Algorithm 1, reduces the number of judgements while maintaining accuracy.  The error bars represent standard deviation.", "section": "Subsampling Experiments"}, {"figure_path": "37CyA1K0vV/figures/figures_19_6.jpg", "caption": "Figure 8: The performance of l1 and l2 QRJA after subsampling judgments using Algorithm 1 with equal weights for all judgments. The subsample rate \u03b1 means M = [\u03b1m] in Algorithm 1. Error bars indicate the standard deviation. The results show that Algorithm 1 can reduce the number of judgments to a factor less than 1.0 with a minor loss in performance in the used datasets. Note that errors in some figures appear large because of the small scale of the y-axis. The actual errors are small.", "description": "This figure shows the performance of L1 and L2 QRJA algorithms after applying a subsampling technique (Algorithm 1).  The x-axis represents the subsample rate (\u03b1), ranging from 0.1 to 1.0,  indicating the proportion of the original judgments used. The y-axis shows the ordinal accuracy of the algorithm. The results demonstrate that the subsampling method reduces the number of judgments without significant performance degradation, especially when \u03b1 is greater than or equal to 0.4. The error bars represent the standard deviation across multiple runs. Note that the visual magnitude of errors can be misleading due to the y-axis scale.", "section": "Subsampling Experiments"}, {"figure_path": "37CyA1K0vV/figures/figures_19_7.jpg", "caption": "Figure 8: The performance of l1 and l2 QRJA after subsampling judgments using Algorithm 1 with equal weights for all judgments. The subsample rate \u03b1 means M = [\u03b1m] in Algorithm 1. Error bars indicate the standard deviation. The results show that Algorithm 1 can reduce the number of judgments to a factor less than 1.0 with a minor loss in performance in the used datasets. Note that errors in some figures appear large because of the small scale of the y-axis. The actual errors are small.", "description": "This figure shows the performance of L1 and L2 QRJA algorithms after subsampling the judgments using Algorithm 1.  The x-axis represents the subsample rate (\u03b1), ranging from 0.1 to 1.0,  while the y-axis shows both ordinal accuracy and quantitative loss for four datasets. The results demonstrate that subsampling reduces computational cost with minimal impact on performance, even when reducing the dataset to 40% of its original size.", "section": "Subsampling Experiments"}, {"figure_path": "37CyA1K0vV/figures/figures_20_1.jpg", "caption": "Figure 4: Ordinal accuracy and quantitative loss of the algorithms on all four datasets. Error bars are not shown here as the algorithms are deterministic. The results show that both versions of QRJA perform consistently well across the tested datasets.", "description": "This figure presents the performance of different ranking prediction algorithms on four datasets: Chess, F1, Marathon, and Codeforces.  Two metrics are used for evaluation: ordinal accuracy (percentage of correct ordinal predictions) and quantitative loss (average absolute error of quantitative predictions, normalized by the trivial prediction). The results show that both versions of QRJA (Quantitative Relative Judgment Aggregation) consistently perform well, achieving high ordinal accuracy and low quantitative loss across all datasets.  The figure showcases the relative performance of QRJA compared to several baseline algorithms (Mean, Median, Borda, Kemeny-Young, and Matrix Factorization).", "section": "5 Experiments"}, {"figure_path": "37CyA1K0vV/figures/figures_21_1.jpg", "caption": "Figure 10: The performance of Matrix Factorization with different numbers of training epochs on all datasets. The results generally show that R1 MF outperforms R2 and R5 MF. Moreover, on some datasets, R2 and R5 MF's performance worsens as the number of training epochs increases. In contrast, R1 MF's performance improves as the number of training epochs increases.", "description": "This figure presents the results of experiments comparing three variants of matrix factorization (MF) with different numbers of training epochs.  The performance is measured across various datasets, using ordinal accuracy as the metric. The results highlight that the R1 MF variant generally outperforms R2 and R5 MF, with R1 MF's performance improving as the number of training epochs increases, while R2 and R5 MF can show performance degradation with increased training.", "section": "Experiments"}, {"figure_path": "37CyA1K0vV/figures/figures_21_2.jpg", "caption": "Figure 10: The performance of Matrix Factorization with different numbers of training epochs on\nall datasets. The results generally show that R1 MF outperforms R2 and R5 MF. Moreover, on\nsome datasets, R2 and R5 MF\u2019s performance worsens as the number of training epochs increases. In\ncontrast, R1 MF\u2019s performance improves as the number of training epochs increases.", "description": "This figure shows the results of the Matrix Factorization method on different datasets for varying numbers of training epochs. It indicates that the R1 MF model generally performs better than R2 and R5 MF, and its performance tends to improve with more training epochs, unlike R2 and R5 MF which may worsen with more epochs on certain datasets.", "section": "Experiments"}, {"figure_path": "37CyA1K0vV/figures/figures_22_1.jpg", "caption": "Figure 4: Ordinal accuracy and quantitative loss of the algorithms on all four datasets. Error bars are not shown here as the algorithms are deterministic. The results show that both versions of QRJA perform consistently well across the tested datasets.", "description": "This figure compares the performance of various ranking prediction algorithms on four datasets using two metrics: ordinal accuracy (percentage of correct relative ordinal predictions) and quantitative loss (average absolute error of relative quantitative predictions, normalized).  The algorithms are compared on Chess, F1, Marathon, and Codeforces datasets.  The results show that both l1 and l2 QRJA consistently perform well across all datasets.", "section": "5 Experiments"}]