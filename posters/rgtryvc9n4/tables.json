[{"figure_path": "rgtrYVC9n4/tables/tables_4_1.jpg", "caption": "Table 1: Some operations in our search space. Full operations are in Appendix D.", "description": "This table lists a subset of the operations used in the search space of the DSA framework.  It shows the operation ID, name, and mathematical expression for each operation. These operations are categorized into pre-processing, reduction, transformation, and post-processing steps, which are combined to create allocation functions that map element-wise scores to sparsity ratios in LLMs.", "section": "4 Allocation Function Search Space"}, {"figure_path": "rgtrYVC9n4/tables/tables_6_1.jpg", "caption": "Table 2: Mean accuracies (%) of our DSA at 50% sparsity rate on 7 zero-shot tasks.", "description": "This table presents the mean accuracy results achieved by the proposed DSA method on seven zero-shot tasks.  The results are shown for several large language models (LLMs) including LLaMA-1, LLaMA-2, LLaMA-3 and OPT at 50% sparsity. The performance of DSA is compared to baseline methods (Magnitude, Wanda, and SparseGPT) to highlight its improvements.", "section": "7.1 Experiments on Zero-shot Tasks"}, {"figure_path": "rgtrYVC9n4/tables/tables_7_1.jpg", "caption": "Table 3: Mean accuracies (%) of our DSA on 7 zero-shot tasks at 60 & 70% sparsity rates.", "description": "This table presents the mean accuracies achieved by the DSA method on seven zero-shot tasks.  The results are shown for different LLMs (LLaMA-2-7B, LLaMA-2-13B, LLaMA-3-70B) at two different sparsity rates (60% and 70%).  Comparisons are provided against three baseline methods: Magnitude pruning, Wanda, and SparseGPT, highlighting the performance improvement achieved by integrating DSA with each of these methods. The \"Gain\" row indicates the improvement in accuracy achieved by using DSA in comparison to the respective baseline method.", "section": "7.1 Experiments on Zero-shot Tasks"}, {"figure_path": "rgtrYVC9n4/tables/tables_8_1.jpg", "caption": "Table 4: WikiText-2 perplexity (\u2193) performance of various allocation methods with the Wanda metric for sparse LLaMA-1-7B at varying high sparsity rates (65%~80%).", "description": "This table presents the WikiText-2 perplexity results for different sparsity allocation methods applied to the LLaMA-1-7B model at high sparsity ratios (65% to 80%).  The methods compared are Global, ER-plus, ER, Uniform, BESA, OWL, and the proposed DSA method.  Lower perplexity indicates better performance. The table showcases DSA's superior performance, especially at higher sparsity rates, demonstrating its ability to effectively allocate sparsity across layers.", "section": "7.1 Experiments on Zero-shot Tasks"}, {"figure_path": "rgtrYVC9n4/tables/tables_8_2.jpg", "caption": "Table 7: Results (%) on 7B LLaVA-1.5.", "description": "This table presents the results of the experiments conducted on the 7B LLaVA-1.5 model.  It shows the performance of different methods (Dense, Magnitude, SparseGPT, Wanda, and the proposed method 'Ours') on various multimodal tasks (VQAv2, SQA, VQA). The results are presented as percentages, indicating the accuracy or performance achieved by each method on each task. This table highlights the improvements achieved by the proposed method compared to existing methods in multimodal tasks.", "section": "7.3 Experiments on Multimodal Tasks"}, {"figure_path": "rgtrYVC9n4/tables/tables_8_3.jpg", "caption": "Table 2: Mean accuracies (%) of our DSA at 50% sparsity rate on 7 zero-shot tasks.", "description": "This table presents the mean accuracies achieved by the proposed DSA method on seven zero-shot tasks, using different large language models (LLMs) at a 50% sparsity rate.  It compares the performance of DSA against baseline methods (Magnitude, Wanda, and SparseGPT) to demonstrate the improvement in accuracy obtained by incorporating DSA's adaptive sparsity allocation.  The results show the accuracy gain for each LLM and method.", "section": "7.1 Experiments on Zero-shot Tasks"}, {"figure_path": "rgtrYVC9n4/tables/tables_15_1.jpg", "caption": "Table 9: Comparison of Method Characteristics of our DSA and Pruner-Zero.", "description": "This table compares the characteristics of the proposed DSA method and the baseline Pruner-Zero method.  The comparison covers the types of sparsity (uniform vs. non-uniform), the task addressed (symbolic pruning metric), the search space used to find the optimal sparsity allocation, the input and output data used in the allocation function, and the strategy used to obtain the sparsity allocation (symbolic regression vs. evolutionary algorithm).  It highlights the key differences in the approaches taken by DSA and Pruner-Zero in achieving sparsity.", "section": "A More Discussion"}, {"figure_path": "rgtrYVC9n4/tables/tables_16_1.jpg", "caption": "Table 10: WikiText-2 perplexity (\u2193) performance of our searched allocation function with different initial seeds with the Wanda metric for 70% sparse LLaMA-1-8B.", "description": "This table presents the WikiText-2 perplexity results obtained using the best allocation functions found through evolutionary search with five different random initial seeds.  The experiments used the Wanda metric and a 70% sparsity level on the LLaMA-1-8B model. The table demonstrates the robustness of the evolutionary search, as similar perplexity scores were achieved across different initializations despite resulting in distinct allocation functions.", "section": "C More Experiment Details"}, {"figure_path": "rgtrYVC9n4/tables/tables_17_1.jpg", "caption": "Table 11: WikiText perplexity of pruned LLaVA-1.5(Vicuna-7B) models with different sparsity ratios.", "description": "This table displays the WikiText perplexity scores achieved by different pruning methods (Magnitude, SparseGPT, Wanda) and the proposed DSA method integrated with Wanda on the LLaVA-1.5 model using Vicuna-7B at various sparsity levels (10%, 20%, 30%, 40%, 50%). Lower perplexity indicates better performance.  The results demonstrate the impact of the proposed DSA method in improving the model's performance under varying sparsity conditions.", "section": "7.1 Experiments on Zero-shot Tasks"}, {"figure_path": "rgtrYVC9n4/tables/tables_17_2.jpg", "caption": "Table 12: WikiText perplexity of pruned LLaVA-1.5(Vicuna-13B) models with different sparsity ratios.", "description": "This table shows the WikiText perplexity results for different sparsity ratios (10%, 20%, 30%, 40%, 50%) when using four different pruning methods: Magnitude, SparseGPT, Wanda, and Wanda with DSA.  It demonstrates the performance of the  pruning methods on the Vicuna-13B model within the LLaVA-1.5 framework. The values indicate the model's perplexity, a measure of how well the model predicts the next word in a sequence. Lower perplexity scores indicate better performance.", "section": "7.1 Experiments on Zero-shot Tasks"}]