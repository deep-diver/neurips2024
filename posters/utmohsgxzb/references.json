{"references": [{"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-18", "reason": "This paper introduces CLIP, a foundational vision-language model that is extensively used and analyzed in the target paper."}, {"fullname_first_author": "Junnan Li", "paper_title": "BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation", "publication_date": "2022-07-18", "reason": "This paper introduces BLIP, another significant vision-language model, whose biases are also investigated in the target paper."}, {"fullname_first_author": "Haotian Liu", "paper_title": "Visual instruction tuning", "publication_date": "2024-07-18", "reason": "This paper introduces LLaVA, a vision-language model that is also a key subject of bias analysis in the target paper."}, {"fullname_first_author": "Ching-Yao Chuang", "paper_title": "Debiasing vision-language models via biased prompts", "publication_date": "2023-02-01", "reason": "This paper presents a debiasing technique for vision-language models, which is compared against in the target paper."}, {"fullname_first_author": "Hugo Berg", "paper_title": "A prompt array keeps the bias away: Debiasing vision-language models with adversarial learning", "publication_date": "2022-11-15", "reason": "This paper offers another debiasing method compared in the target paper, providing a different approach to bias mitigation."}]}