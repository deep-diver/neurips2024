[{"figure_path": "VcPtU8e6yK/tables/tables_5_1.jpg", "caption": "Table 1: Statistics of the datasets used in the image compositing stage.", "description": "This table lists the datasets used in the image compositing stage of the Bifr\u00f6st model.  It shows the type of data (video or image), the number of samples in each dataset, and whether the dataset contains variations in pose or viewpoint of the objects. This is important because the model needs to learn how to generate images with different object poses and views to create high-fidelity compositions.", "section": "3 Method"}, {"figure_path": "VcPtU8e6yK/tables/tables_6_1.jpg", "caption": "Table 2: Quantitative evaluation results on the accuracy of the MLLM's prediction of Bifr\u00f6st. Note: MiniGPTv2 and LLaVA (baseline) do not support depth prediction.", "description": "This table presents a quantitative comparison of the accuracy of the proposed method's multi-modal large language model (MLLM) in predicting the 2.5D location of objects for image composition.  It compares the performance of Bifr\u00f6st against two baseline models, MiniGPTv2 and LLaVA, using Mean Squared Error (MSE) for bounding box (BBox) prediction and Intersection over Union (IoU) for bounding box accuracy. Notably, Bifr\u00f6st is the only model that predicts depth, with the accuracy measured using MSE.", "section": "4.2 Quantitative Evaluation"}, {"figure_path": "VcPtU8e6yK/tables/tables_7_1.jpg", "caption": "Table 3: Quantitive evaluation results on the performance of image compositing. Bifr\u00f6st outperforms all other methods across all metrics.", "description": "This table presents a quantitative comparison of Bifr\u00f6st's image compositing performance against several other state-of-the-art methods.  Three metrics are used for evaluation: DINO-score (higher is better), CLIP-score (higher is better), and FID (Fr\u00e9chet Inception Distance, lower is better).  The results demonstrate that Bifr\u00f6st significantly outperforms all other methods across all three metrics, indicating its superior performance in generating high-fidelity and realistic composited images.", "section": "4 Experiment"}, {"figure_path": "VcPtU8e6yK/tables/tables_7_2.jpg", "caption": "Table 4: User study on the comparison between our Bifr\u00f6st and existing alternatives. \u201cQuality\u201d, \u201cFidelity\u201d, \u201cDiversity\u201d, and \u201c3D Awareness\u201d measure synthesis quality, object identity preservation, object local variation (i.e., across four proposals), and spatial relation awareness (i.e., occlusion) respectively. Each metric is rated from 1 (worst) to 5 (best).", "description": "This table presents the results of a user study comparing Bifr\u00f6st with four other image compositing methods.  Users rated the generated images on four criteria: Quality (overall visual quality), Fidelity (how well the generated object matched the reference), Diversity (variation in generated poses), and 3D Awareness (handling of spatial relationships, including occlusion).  Higher scores indicate better performance.", "section": "4 Experiment"}, {"figure_path": "VcPtU8e6yK/tables/tables_9_1.jpg", "caption": "Table 5: Quantitative ablation studies on the core components of the image compositing model of Bifr\u00f6st. Note that: + indicates adding one component based on the previous model.", "description": "This table presents the quantitative ablation study results for the core components of the Bifr\u00f6st image compositing model. It shows the impact of adding video data, classifier-free guidance (CFG), high-frequency filter (HF Filter), and depth information to the baseline model. The metrics used are DINO-score (higher is better), CLIP-score (higher is better), and FID (lower is better).  The results demonstrate the incremental improvements in performance when these components are added sequentially.", "section": "4.4 Ablation Study"}]