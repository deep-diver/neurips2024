[{"type": "text", "text": "Non-Euclidean Mixture Model for Social Network Embedding ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Roshni G. Iyer ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yewen Wang ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Computer Science Department University of California, Los Angeles Los Angeles, California, USA roshnigiyer@cs.ucla.edu ", "page_idx": 0}, {"type": "text", "text": "Computer Science Department University of California, Los Angeles Los Angeles, California, USA wyw10804@gmail.com ", "page_idx": 0}, {"type": "text", "text": "", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Wei Wang Computer Science Department University of California, Los Angeles Los Angeles, California, USA weiwang@cs.ucla.edu ", "page_idx": 0}, {"type": "text", "text": "Yizhou Sun Computer Science Department University of California, Los Angeles Los Angeles, California, USA yzsun@cs.ucla.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "It is largely agreed that social network links are formed due to either homophily or social influence. Inspired by this, we aim at understanding the generation of links via providing a novel embedding-based graph formation model. Different from existing graph representation learning, where link generation probabilities are defined as a simple function of the corresponding node embeddings, we model the link generation as a mixture model of the two factors. In addition, we model the homophily factor in spherical space and the influence factor in hyperbolic space to accommodate the fact that (1) homophily results in cycles and (2) influence results in hierarchies in networks. We also design a special projection to align these two spaces. We call this model Non-Euclidean Mixture Model, i.e., NMM. We further integrate NMM with our non-Euclidean graph variational autoencoder (VAE) framework, NMM-GNN. NMM-GNN learns embeddings through a unified framework which uses non-Euclidean GNN encoders, non-Euclidean Gaussian priors, a non-Euclidean decoder, and a novel space unification loss component to unify distinct non-Euclidean geometric spaces. Experiments on public datasets show NMM-GNN significantly outperforms state-of-the-art baselines on social network generation and classification tasks, demonstrating its ability to better explain how the social network is formed. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Social networks are omnipresent because they are used for modeling interactions among users on social platforms. Social network analysis plays a key role in several applications, including detecting underlying communities among users [1], classifying people into meaningful social classes [2], and predicting user connectivity [3]. Most existing embedding models are designed based on the homophily aspect of social networks [4, 5]. They utilize the intuition that associated nodes in a social network imply feature similarity, and an edge is usually generated between similar nodes. Prior works have used shallow embedding models to represent homophily, like matrix factorization and random-walk (Section 2), which are parameter intensive and do not employ message passing. As an improvement, graph neural network (GNN) models (Section 2) have been proposed to more effectively capture homophily by representing a node through its local neighborhood context. ", "page_idx": 0}, {"type": "text", "text": "However, research of RaRE [6] and work of [7] show homophily is insufficient, and social influence is also critical in forming connections. This is due to popular nodes having direct influence in forming links [8]. For example, in Twitter network, users tend to follow celebrities in addition to users who share similar interests [9]. Though RaRE jointly models both factors, it has limitations in modeling capabilities. Specifically, it is parameter intensive as each node embedding is fully parameterized through a Bayesian framework. Further, RaRE assumes graphs are transductive, limiting its performance in the practical inductive setting where new links not seen during training must be predicted. Moreover, nearly all works embedding social networks utilize a single zero-curvature Euclidean space, when in reality, network factors may create different topologies. Specifically, edges generated by homophily tend to form cycles [10], while edges generated by social influence tend to form tree structures [11, 12]. From Riemannian geometry, people have found that networks with cycles are best represented by spherical space embeddings [13], while tree structured networks are best represented by hyperbolic space embeddings [14]. Thus, an end-to-end model to bridge social network embeddings of distinct non-Euclidean geometric spaces is a promising direction. ", "page_idx": 1}, {"type": "text", "text": "Our motivation is two-fold: (1) We aim to understand how the social network is generated e.g., which factors affect node connectivity and what topological patterns emerge in the network as a result. (2) Using our learning from (1), we aim to design a more realistic deep learning model to explain how the network is generated (inferring new connections). We summarize our contributions as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose Graph-based Non-Euclidean Mixture Model (NMM) to explain social network generation. NMM represents nodes via joint influence by homophily (modeled in spherical space) and social influence (modeled in hyperbolic space), while seamlessly unifying embeddings via our space unification loss.   \n\u2022 To our knowledge, we are also the first to couple NMM with a graph-based VAE learning framework, NMM-GNN. Specifically, we introduce a novel non-Euclidean VAE framework where node embeddings are learned with a powerful encoder of GNNs using spherical and hyperbolic spaces, non-Euclidean Gaussian priors, and unified non-Euclidean optimization.   \n\u2022 Extensive experiments on several real-world datasets on large-scale social networks, Wikipedia networks, and attributed graphs demonstrate effectiveness of NMM-GNN in social network generation and classification, which outperforms state-of-the-art (SOTA) network embedding models. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminary and Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We provide an overview of social network embedding models and discuss advancements in nonEuclidean graph learning. ", "page_idx": 1}, {"type": "text", "text": "2.1 Social Network Embedding ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Several works that embed social networks merely model homophily [15], capturing node-node similarity, without also considering a node\u2019s social influence or node popularity e.g., a celebrity. Homophily-based models include shallow embedding models and GNN embedding models. Most shallow embedding models are either based on matrix factorization [16] or random-walk [17]. Though GNN models [18] effectively learn on large networks and in inductive settings, they still fail to model the social influence factor in the network. Further, even the model that captures both homophily and social influence, RaRE [6], has limitations in that its fully-parameterized node embeddings require large parameter size to be learned, and it models all nodes in Euclidean space, an approach not effective in capturing different topologies (e.g., cycles and hierarchy) in the social network. ", "page_idx": 1}, {"type": "text", "text": "2.2 Non-Euclidean Geometry for Graphs ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Non-Euclidean geometric spaces, commonly used to model surfaces in mathematics and physics, are curved geometries that include spherical spaces with positive curvature, and hyperbolic spaces with negative curvature [19]. Works have recently found Euclidean space modeling to be insufficient for non-Euclidean graph-structured data [20]. Namely, spherical spaces have been shown to effectively embed graphs with cyclic structure due to their positive curvature [21, 13], while hyperbolic spaces have been shown to effectively embed graphs with hierarchical structure due to their negative curvature and exponential or \"tree-like\" growth of the space [22, 23]. HGCN [24] and [25] are critical works exploring GNNs in non-Euclidean spaces. Both these hyperbolic GNN methods have shown significant improvement on benchmark datasets by preserving hierarchical structure of graphs. In knowledge graphs, [26] has achieved notable performance by modeling graphs in non-Euclidean spaces of various curvatures, using both hyperbolic and spherical space. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "image", "img_path": "nuZv2iTlvn/tmp/c5299feb9bd39c30ca4450013fe90f51a7379fb0b45870a202c1bf8ea69a70c8.jpg", "img_caption": ["3 Methodology "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 1: Model Architecture. (a) Architecture overview of NMM-GNN, a non-Euclidean Mixture Model with non-Euclidean VAE framework. (b) Illustration of space unification loss of NMM-GNN . $z_{i}^{H}$ from the hyperbolic space is projected to its corresponding point in the spherical space, ziS , such that its geodesic distance is ensured to be close to $v_{i}$ \u2019s existing spherical space representation, $z_{i}^{S}$ . ", "page_idx": 2}, {"type": "text", "text": "A social network $G=(V,A)$ consists of a set of vertices $V=\\{v_{i}\\}_{i=1}^{N}$ , and associated adjacency matrix $e_{i j}\\in A$ , where $e_{i j}$ is an edge from $v_{i}$ to $v_{j}$ . We aim to design a model to jointly learn both node homophily and social influence representation, denoted $z_{i}^{S}$ and $z_{i}^{H}$ respectively, that can best explain the social network in terms of link reconstruction. In this section, we describe our architecture. First, we introduce a novel non-Euclidean mixture model, called NMM, to model the probability of a new link. Second, we add a non-Euclidean GNN encoder to enrich NMM, called NMM-GNN, which is connected to the variational autoencoder framework. Source code is in the Appendix. For details on future directions, the reader is also referred to the Limitations section in the Appendix. ", "page_idx": 2}, {"type": "text", "text": "3.1 Overview ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "To model both factors of homophily and social influence that affect graph connectivity, we model each node $v_{i}$ with homophily regulated representation, $z_{i}^{S}$ , and social influence regulated representation, $z_{i}^{H}$ 1. A unified framework is designed such that $z_{i}^{S}$ and $z_{i}^{H}$ influence each other bi-directionally and are seamlessly merged through NMM. NMM utilizes non-Euclidean geometric spaces to better represent homophily and social influence components which produce curved structures like cycles and trees. To improve NMM, we enrich its encoder by GNNs, $q_{\\psi}(z_{i}^{H}|G)$ and $q_{\\phi}(z_{i}^{S}|G)$ . To ensure generated embeddings are in spherical and hyperbolic spaces respectively, non-Euclidean GNNs are adopted. The enhanced model is NMM-GNN, which has a clear connection to the VAE framework. ", "page_idx": 2}, {"type": "text", "text": "Figure 1(a) illustrates the architecture overview of NMM-GNN. Specifically, the encoder component maps nodes into homophily based embedding, $z^{S}$ , in spherical space (for homophily generated cycles) and social influence based embedding, ${\\bar{z}}^{H}$ , in hyperbolic space (for social influence generated trees), which follow non-Euclidean prior distributions. The embeddings are passed into our mixture model decoder, which models the probability of a link as a mixture of a homophily based distribution component and a social influence based distribution component. The objective is to maximize the likelihood to observe the links, or equivalently to minimize the link reconstruction loss. In addition, the two geometric spaces are ensured to be aligned together via a space unification regularization term, to make sure the two embeddings of the same node are corresponding to each other. ", "page_idx": 2}, {"type": "text", "text": "3.2 Modeling via Non-Euclidean Mixture Model (NMM) ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Geometrically, the space of NMM is a spherical surface inside a unit Poincar\u00e9 ball. Each node $v_{i}$ corresponds to (1) a point in the Poincar\u00e9 ball, $z_{i}^{H}$ , and (2) a point on the spherical surface, $z_{i}^{S}$ , where both spaces are embedded inside a Euclidean space. These two points are aligned by enforcing projection of $z_{i}^{H}$ onto the spherical surface to be close to $z_{i}^{S}$ , as projecting a star onto earth\u2019s atmosphere, shown in Fig. 1(b). We note that the space alignment does not mean embeddings are enforced to be the same in two spaces2. Rather, we require the projection of $z_{i}^{H}$ is close to $i$ \u2019s embedding in spherical space $z_{i}^{S}$ . In this case, the two distances of spherical and hyperbolic spaces are also different from each other (norm space difference vs. spherical geodesic distance). Without the space alignment, $z_{i}^{H}$ has too much degree of freedom, which can move freely as long its norm is kept the same. Lastly, the probability of generating a link is a mixture of the probability of generating the link in each non-Euclidean space. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "3.2.1 Modeling for homophily ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Representation of homophily regulated nodes. We embed homophily based representation of node $v_{i}$ , or $z_{i}^{S}$ , on the surface of the spherical ball in spherical space, $\\bar{\\mathbb{S}^{d}}$ . Formally, $\\mathbb{S}^{d}=\\{z_{i}^{S}\\in$ $\\mathbb{R}^{d}|\\|z_{i}^{S}\\|=w^{S}\\}$ is the $d_{\\cdot}$ -dimensional $w^{S}$ -norm ball, where $\\left\\Vert\\cdot\\right\\Vert$ is the Euclidean norm and $w^{S}\\in[0,1)$ is a constant to ensure the spherical surface is inside the unit Poincar\u00e9 ball. To better capture homophily distribution, $p(z_{i}^{\\hat{S}})$ , we use spherical Gaussian distribution $G_{S}(\\cdot)$ as spherical prior, described in Table 1. $\\beta$ is the axis or direction of the lobe controlling where the lobe is located on the sphere, and points towards the center of the lobe; $\\lambda$ is the sharpness of the lobe such that as this value increases, the lobe will become narrower in width; and $a$ is the amplitude or intensity of the lobe, corresponding to the height of the lobe at its peak. ", "page_idx": 3}, {"type": "text", "text": "Link prediction using homophily based distribution. The probability of link $e_{i j}\\,=\\,1$ between nodes $v_{i}$ and $v_{j}$ , also described in Table 1, is determined by the geodesic distance between $z_{i}^{S}$ and $z_{j}^{S}$ . $\\mathrm{dist_{s}}(z_{i}^{S},z_{j}^{S})=\\mathrm{arccos}(\\langle z_{i}^{S},z_{j}^{S}\\rangle)$ is the geodesic distance between $z_{i}^{S}$ and $z_{j}^{S}$ ; $J>0$ and $B\\geq0$ are model parameters; and $\\langle\\cdot,\\cdot\\rangle$ denotes the inner product. The probabilistic model is designed so nodes greatly dissimilar (exhibiting low homophily), or $\\mathrm{dist_{s}}(z_{i}^{S},z_{j}^{S})\\rightarrow+\\infty$ , have low probability of a link being generated, or $p_{\\mathrm{hom}}(e_{i j}=1)\\to0$ . In the case of nodes exhibiting high homophily, they either (1) may be connected due to highly similar characteristics, or (2) may not be connected simply because the individuals do not know each other. Our distribution models both scenarios. Note when $\\mathrm{dist_{s}}(z_{i}^{S},z_{j}^{S})\\rightarrow0$ , $\\begin{array}{r}{p_{\\mathrm{hom}}(e_{i j}=1)\\to\\frac{1}{1+e^{B}}}\\end{array}$ , which can be interpreted as a factor to control sparsity of the network. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{\\Delta}p(z_{i}^{S})=G_{S}(z_{i}^{S};\\beta,\\lambda,a)\\qquad\\qquad}&{{}\\mathrm{\\Delta}p_{\\mathrm{hom}}(e_{i j}=1)=p(e_{i j}=1|\\mathrm{dist}_{s}(z_{i}^{S},z_{j}^{S}))}\\\\ {=a e^{\\lambda(\\beta\\cdot z_{i}^{S}-1)}\\qquad\\qquad}&{{}=\\cfrac{1}{1+e^{J\\times\\mathrm{dist}_{s}(z_{i}^{S},z_{j}^{S})+B}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "3.2.2 Modeling for social influence ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Representation of social influence regulated nodes. We embed social influence based representations of node $v_{i}$ , or $z_{i}^{H}$ , on the Poincar\u00e9 ball from hyperbolic space, $\\mathbb{H}^{d+1}$ , to better capture resulting hierarchical structures. Social influence regulated nodes are represented as points, $z_{i}^{H}$ , belonging inside the Poincar\u00e9 (open) ball in $\\mathbb{H}^{d+1}$ . Formally, $\\mathbb{H}^{d+1}=\\{z_{i}^{H}\\in\\mathbb{R}^{d+1}|\\|z_{i}^{H}\\|=w_{z_{i}}^{\\check{H}}\\},w_{z_{i}}^{H}\\in[0,1)$ , is $(d\\!+\\!1)$ -dimensional $w_{z_{i}}^{H}$ -norm ball, where $\\lVert\\cdot\\rVert$ is Euclidean norm. We assume center of the Poincar\u00e9 ball is aligned with center of the sphere, and is one dimension larger than the sphere to ensure the spherical surface is inside the Poincar\u00e9 ball. To better capture a social influence regulated distribution, $\\bar{p(z_{i}^{H})}$ , we use Hyperbolic Gaussian distribution $G_{H}(\\cdot)$ as non-Euclidean Gaussian prior, described in Table 2. $\\overline{{z_{i}^{H}}}$ is the origin of $(r,\\omega)$ for radius $r$ and angle $\\omega$ in polar coordinates. $\\overline{{z_{i}^{H}}}$ is the center of mass and $\\zeta>0$ is the dispersion parameter, where the dispersion dependent normalization constant $Z(\\zeta)$ accounts for the underlying non-Euclidean geometry. $Z(\\zeta)$ is partitioned into angular $\\omega$ and radial $r$ components. $\\Gamma(\\cdot)$ is Euler\u2019s gamma function, and $\\mathrm{{dist}_{H}(\\cdot)}$ is the hyperbolic distance between two hyperbolic space node embeddings, $\\textbf{\\em x}$ and $\\textit{\\textbf{y}}$ , where $\\lVert\\cdot\\rVert$ denotes Euclidean norm. ", "page_idx": 3}, {"type": "text", "text": "Link prediction using social influence based distribution. We model existence of an edge, $e_{i j}=1$ between nodes $v_{i}$ and $v_{j}$ , also described in Table 2, as a function of norm space difference, $\\mathrm{{dist}_{h}(\\cdot)}$ , between two hyperbolic space node embeddings, $z_{i}^{H}$ and $z_{j}^{H}$ . We utilize norm space difference as opposed to hyperbolic distance, since nodes of similar social influence status may have large distance in the Poincar\u00e9 (open) ball due to nodes possibly being placed towards the ball\u2019s boundary. This is because, at the boundary of the ball, nodes become infinitely distanced apart. Thus, to allow for numerical stability and to capture social influence difference (in which the higher the social influence of nodes indicated by large in-degree and smaller out-degree, the closer they are embedded towards the center of the ball), we utilize norm space as indicator. Consistent with the notation of [6], a node with higher social influence is associated with a smaller norm value. $C$ and $D$ are learned model parameters and norm space of a vector is $\\mathrm{norm}(\\pmb{x})\\b{=}\\|\\pmb{x}\\|$ . $\\mathrm{dist}_{\\mathrm{h}}(z_{i}^{H},z_{j}^{H})\\,=\\,|\\mathrm{norm}(z_{i}^{H})-\\mathrm{norm}(z_{j}^{H})|$ is the norm difference between $z_{i}^{H}$ and $z_{j}^{H}$ ; $C>0$ and $D\\geq0$ are model parameters; and $\\operatorname{norm}(\\cdot)$ ddiefnfeorteesn tt hien  Lp1o pnuolramriatlyi z(aetxiohin bfituinncgt iohing. hT shoe cpiralo bianbfiluliesnticce )m, oodre $\\mathrm{dist_{h}}(z_{i}^{\\overline{{H}}},z_{j}^{H})\\,\\to\\,+\\infty$ o, dheasv lea rhgieglhy probability of a link being generated, or $p_{\\mathrm{rank}}(e_{i j}=1)\\to1$ . In the case both nodes exhibit low social influence (such as having similar social rank), they either (1) may be connected due to highly similar characteristics, or (2) may not be connected simply because individuals do not know each other. Our distribution models both scenarios. When $\\mathrm{dist}_{\\mathrm{h}}(z_{i}^{H},z_{j}^{H})\\to0$ H ) \u21920, prank(eij = 1) \u21921e+eD , which can be interpreted as another factor to control sparsity of the network. ", "page_idx": 4}, {"type": "image", "img_path": "nuZv2iTlvn/tmp/d19ccbfeae247d50f87049ad133654b43f1d8db51468fdb635de5c895027e254.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "3.2.3 Non-Euclidean Mixture Model ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Link Prediction Using Mixed Space Distribution. Since both homophily and social influence affect the connectivity structure of social networks, we model existence of a new link between nodes, $p_{\\theta}(e_{i j}=1)$ , as a weighted combination distribution of these factors. Specifically, our non-Euclidean mixture model is a weighted combination of homophily based distribution, $p_{\\mathrm{hom}}(e_{i j}=1)$ , and social influence based distribution, $p_{\\mathrm{rank}}(e_{i j}=1)$ , with learned weight $\\gamma$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p_{\\theta}(e_{i j}=1)=p_{\\theta}(e_{i j}=1|z_{i}^{S},z_{j}^{S},z_{i}^{H},z_{j}^{H})}\\\\ &{\\qquad\\qquad\\qquad=\\gamma\\cdot p_{\\mathrm{hom}}(e_{i j}=1)+(1-\\gamma)\\cdot p_{\\mathrm{rank}}(e_{i j}=1)}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $p_{\\mathrm{hom}}(e_{i j}=1)$ is modeled in positively curved spherical space since homophily based links may form cycles due to similarity connections between node clusters. $p_{\\mathrm{rank}}(e_{i j}=1)$ is modeled in negatively curved hyperbolic space since social influence based links may form tree-like structures due to popularity-based social hierarchy between node clusters. We would like to highlight that the link between nodes $i$ and $j$ is a mixture model because each link is a weighted combination of influence from both spherical and hyperbolic spaces (not one or the other) as evidenced in Equation 6. As shown in Figure 1b, the same node has two representations \u2013 one in the spherical space and one in the hyperbolic space, and because they represent the same underlying node, they need to be aligned. Therefore, these two network factors do not contradict each other, but rather work together to explain how links are formed between users. ", "page_idx": 4}, {"type": "text", "text": "3.3 Modeling via Non-Euclidean VAE on Graphs ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We enrich the encoder of NMM to generate better embeddings. To do so, we explore GNN methods which have been shown to be more effective than shallow embedding methods (see Experiments and Ablation Studies). We refer to this enriched NMM model as NMM-GNN. NMM-GNN uses non-Euclidean VAE as its learning framework. The framework integrates a mixture of different nonEuclidean geometric spaces e.g., hyperbolic and spherical spaces, for learning of encoder, decoder, and node prior distributions, and ensures geometric spaces are unified during training. ", "page_idx": 5}, {"type": "text", "text": "Encoder model. The encoder learns two corresponding embedding representations per node $v_{i}$ to produce spherical embedding $z_{i}^{S}$ and hyperbolic embedding $z_{i}^{H}$ . For homophily regulated nodes in spherical space, $\\mathbb{S}^{d}$ , any spherical space GNN (SGNN) can be applied, and for social influence regulated nodes in hyperbolic space, $\\mathbb{H}^{d+1}$ , any hyperbolic space GNN (HGNN) can be applied. The general framework for SGNN and HGNN are shown in Tables 3 and 4. $\\boldsymbol{z}_{i}^{S^{(l)}}\\in\\mathbb{R}^{d^{(l)}}$ and $\\boldsymbol{z}_{i}^{H^{(l)}}\\in\\mathbb{R}^{d+1^{(l)}}$ are spherical and hyperbolic feature representations of node $v_{i}$ at layer $l$ , with dimensionality $d$ and $(d+1)$ respectively. $f$ is a message-specific neural network function of incoming messages to $v_{i}$ from neighborhood context $N_{i}$ , and activation function $\\sigma(\\cdot)$ , typically $\\mathrm{ReLU}(\\cdot)$ for all layers but the last one being softmax $(\\cdot)$ . ", "page_idx": 5}, {"type": "image", "img_path": "nuZv2iTlvn/tmp/4bfd33ac9f2fb34e9ea3079447033fb8c6db369f1cf41fda3b0b0226962a6cf3.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Example Non-Euclidean Encoder models. We illustrate Non-Euclidean Graph Convolutional Neural Network (Non-Euclidean GCN) as an example GNN. Tables 3 and 4 describe the model architectures for both Spherical GCN and Hyperbolic GCN respectively. Node features are initialized by sampling each embedding dimension from a distribution uniformly at random for all nodes where $z_{i}^{\\overline{{S^{(0)}}}}\\in\\mathbb{R}^{\\bar{d^{(l)}}}\\leftarrow\\mathrm{Unif}([0,1\\bar{)})^{d}$ and $\\boldsymbol{z}_{i}^{H^{(0)}}\\in\\mathbb{R}^{d+1^{(l)}}\\gets\\mathrm{Unif}([0,1))^{\\bar{d}+1}$ respectively. The retraction operator, $\\mathcal{R}(\\cdot)$ , involves mapping between spaces. For non-Euclidean spaces, retraction is performed between non-Euclidean space and approximate tangent Euclidean space using logarithmic and exponential map functions. Specifically, $\\log_{0}^{H}(z_{i}^{H})=\\operatorname{tanh}^{-1}(\\mathrm{i}\\cdot\\|z_{i}^{H}\\|)\\frac{z_{i}^{H}}{\\mathrm{i}\\cdot\\|z_{i}^{H}\\|}$ is a logarithmic map $\\begin{array}{r}{\\exp_{0}^{H}(z_{i}^{H})=\\operatorname{tanh}(\\mathrm{i}\\cdot\\|z_{i}^{H}\\|)\\frac{z_{i}^{H}}{\\mathrm{i}\\cdot\\|z_{i}^{H}\\|}}\\end{array}$ is an exponential map at center 0 from Euclidean tangent space to hyperbolic space. $\\log_{0}^{S}(z_{i}^{S})=$ $\\textstyle\\operatorname{tanh}^{-1}(\\|z_{i}^{S}\\|)\\frac{z_{i}^{S}}{\\|z_{i}^{S}\\|}$ is a logarithmic map at center 0 from spherical space to Euclidean tangent space and $\\begin{array}{r}{\\exp_{0}^{S}(z_{i}^{S})=\\operatorname{tanh}(\\|z_{i}^{S}\\|)\\frac{z_{i}^{S}}{\\|z_{i}^{S}\\|}}\\end{array}$ is an exponential map at center 0 from Euclidean tangent space to spherical space. where $z_{i}^{S^{(l)}},z_{i}^{H^{(l)}}$ are embeddings of node $v_{i}$ at layer $l\\in[0,L)$ , $L=2$ ; $W_{l}$ is a layer-specific learnable weight matrix; $N_{i}$ is the set of nodes in the neighborhood context of $v_{i}$ ; $e_{j,i}$ is the edge-weight between nodes $v_{j}\\to v_{i}$ , with default edge weight being 1.0 if an edge exists. $m_{i},m_{j}$ are entries of the degree matrix, with $\\begin{array}{r}{m_{i}=1+\\sum_{j\\in N_{i}}e_{j,i}}\\end{array}$ . ", "page_idx": 5}, {"type": "text", "text": "Decoder model. NMM can be considered as a probabilistic decoder for link generation, which maps embeddings of two nodes into the probablity to generate a link between them. ", "page_idx": 5}, {"type": "text", "text": "Joint loss function. The training loss involves components of reconstruction loss (to ensure the generated graph is consistent with the original graph), KL divergence loss (to ensure predicted embeddings $z_{i}^{S}$ and $z_{i}^{H}$ closely match their non-Euclidean Gaussian distributions), and space unification loss (to ensure $z_{i}^{S}$ and $z_{i}^{H}$ map to the same node $v_{i}$ ). ", "page_idx": 6}, {"type": "text", "text": "Reconstruction Loss. Table 5 shows reconstruction loss, which minimizes the upper bound on the negative log-likelihood. $\\lambda_{A}$ is a hyperparameter; $A^{\\prime}=X A X^{T}$ , given $X\\in0,1^{k\\times d}$ where $X_{a,i}=1$ only if node $a\\in{\\tilde{G}}$ is assigned to $i\\in G$ and $X_{a,i}=0$ otherwise, where $\\tilde{G}$ is the predicted graph. ", "page_idx": 6}, {"type": "table", "img_path": "nuZv2iTlvn/tmp/d287d8d313039da5f009ea3b78cd2dd4517caa8b3214516f6eef19850e4739c4.jpg", "table_caption": ["Table 5: Description for Reconstruction Loss. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Space Unification Loss. The space unification loss is formed by minimizing the equations described in Table 6. Minimizing the space unification loss ensures that the hyperbolic space representation of node $v_{i}$ in spherical space, pro ${\\mid}_{S}(z_{i}^{H})$ , is close to the corresponding learned representation of node $v_{i}$ in the spherical space, $z_{i}^{S}$ . Note that using a normalized hyperbolic disk is not a substitute for the projection operator from the social influence hyperbolic space onto the homophily spherical space. The projection operation solely projects out-of-sphere nodes onto the $w^{S}$ norm space, or the norm at the surface of the spherical ball. A normalization operator would instead change the embedding values of all nodes. More importantly, the space unification loss component ensures minimal spherical geodesic distance between $z_{i}^{H}$ \u2019s representation on the spherical ball and $z_{i}^{S}$ , illustrated in Figure 1(b). ", "page_idx": 6}, {"type": "table", "img_path": "nuZv2iTlvn/tmp/a99d7b9788d932cc1458366420e532f47333f91f15bfab78663ef9b12dbc633b.jpg", "table_caption": ["Table 6: Description of $\\mathrm{KL}$ Divergence Loss and Space Unification Loss. ) KL Divergence Loss (b) Space Unification Loss "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Total Loss. The overall loss function for homophily regulated and social influence regulated nodes respectively is then a summation of the above loss components given by: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{L^{S}=L_{\\mathrm{recon}}^{S}(\\phi,\\theta;G)+L_{\\mathrm{KL}}^{S}(\\phi;G)+L_{\\mathrm{unify}}(G)}\\\\ {L^{H}=L_{\\mathrm{recon}}^{S}(\\psi,\\theta;G)+L_{\\mathrm{KL}}^{H}(\\psi;G)+L_{\\mathrm{unify}}(G)}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "3.4 Training ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "This section details NMM-GNN\u2019s training framework, using non-Euclidean VAE, for representing social networks. We describe the optimization method for variables from Sections 3.2 and 3.3. ", "page_idx": 6}, {"type": "text", "text": "Embedding Initialization. We randomly initialize all embeddings of $z_{i}^{S}$ and $z_{i}^{H}$ . For homophily   \nregulated nodes, we choose a value for norm $w^{S}$ , that is sampled from uniform distribution: $w^{S}$ :   \n$w^{\\widecheck{S}}\\in[0,1)\\to\\operatorname{Unif}([0,1))$ $w_{z_{i}}^{H}$ f loalr nnmd ormre  ssppoeerc cintailov dienel.yf .lu WeWene c sleee tr aecvgueru tlvhaatet eundro en nv-otadrlieuvsei,s a lw opef r socphbhloeeormsie c oaafl $K_{S}=1$ $K_{H}=-1$   \nlearning optimal curvatures as future work. ", "page_idx": 6}, {"type": "text", "text": "Training procedure for homophily regulated nodes. Parameter optimization for learning node embeddings is performed using Riemannian stochastic gradient descent (RSGD) for the spherical space as shown in Table 7. To ensure the updated node embeddings remain in norm- $\\cdot w^{S}$ space, we perform a rescaling operation, $\\mathrm{proj}_{S}$ , to project out-of-boundary embeddings back to the surface of the $w^{S}$ -ball. We further update scalar parameters $J$ and $B$ (from the homophily regulated distribution) and $\\beta,\\lambda,a$ (from the spherical gaussian prior) through stochastic gradient descent (SGD) as defined below via $\\mathrm{SGD}^{S}(J),\\dot{\\mathrm{SGD}}^{S}(\\bar{B_{)}},\\mathrm{SGD}^{S}(\\beta),\\mathrm{SGD}^{S}\\bar{(\\lambda)},\\mathrm{SGD}^{S}(a)$ . ", "page_idx": 6}, {"type": "image", "img_path": "nuZv2iTlvn/tmp/b44a55308247126855f373b26425499323a3ec6faa1d7a3375514b4dac83091e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "nuZv2iTlvn/tmp/6be102b54dc469c775a11f5edb83ee1adc19c4884b01e1e8afe78afb6ec08e25.jpg", "table_caption": ["Table 7: Training Procedure for homophily regulated nodes and social influence regulated nodes. (a) Homophily Regulated Nodes (b) Social Influence Regulated Nodes ", "Table 8: Dataset statistics for evaluation datasets. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Training procedure for social influence regulated nodes. Parameter optimization for learning node embeddings is performed using RSGD for the hyperbolic space as shown in Table 7. The corresponding norm space, $w_{z_{i}}^{H}$ , is also learned through RSGD by updating embeddings of $z_{i}^{H}$ . We further update scalar parameters and (from the social influence regulated distribution) and $\\zeta$ (from the hyperbolic gaussian prior) through SGD as defined below via $\\mathrm{SGD}^{H}(C),\\mathrm{SGD}^{H}(D),\\mathrm{SGD}^{H}(\\zeta)$ . ", "page_idx": 7}, {"type": "text", "text": "Training procedure for NMM-GNN weights. Parameter optimization for $\\gamma$ uses SGD as follows, where $\\bar{L_{\\mathrm{total}}}=L^{S}+L^{H}$ : ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\gamma_{t+1}\\leftarrow\\gamma_{t}-\\eta_{t}\\nabla L_{\\mathrm{{total}}}(\\gamma_{t})\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We comprehensively evaluate NMM-GNN on social network generation for popular large-scale social networks through multi-label classification and link prediction tasks against competitive SOTA baselines in various categories. The Appendix section further details Ablation Studies where we test quality of using (1) a mixture model, (2) distinct non-Euclidean geometric spaces, (3) non-Euclidean GNN-based encoders and non-Euclidean GraphVAE framework (through the inductive setting), and (4) space unification loss component. All experiments and each of the ablation studies consistently show NMM-GNN outperforms baseline network embedding models on all metrics for all datasets. ", "page_idx": 7}, {"type": "text", "text": "4.1 Datasets ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "For comprehensive evaluation, we assess our models on real-world datasets from well-known social media venues: BlogCatalog (BC) [27], LiveJournal $(L J)$ [28], and Friendster $(F)$ [29] which are friendship networks among bloggers. Table 8 provides statistics of the datasets. In the Appendix, we also include experiments for Wikipedia datasets, to show that our model can also benefit other networks. Our research goal is to design an embedding model to better explain how the social network is formed. Thus, to separate model contribution from the learned representation, we focus on the setting of featureless graphs since quality of node features can be a confounding factor in determining quality of our model. However, since our model can handle feature graphs, we also provide these experiments in the Appendix, with our model outperforming all baselines on attributed networks. ", "page_idx": 7}, {"type": "text", "text": "4.2 Models ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Baselines. We compare NMM-GNN to SOTA network embedding models, in Table 9, and report results in Table 10. We omit comparison to prior models e.g., LINE [30] due to lower performance. We also highlight that unlike prior works like $\\kappa$ -GCN, our model overcomes limitations of the product space, e.g., where the entire model belongs to a Cartesian product of non-Euclidean geometric spaces by default. Our work is in a category called mixed space model that uses a multi-geometric space framework where different portions of the graph may possibly belong to different spaces (based on the amount of impact each of homophily and social influence has for that personalized pair of nodes). In the extreme case (Case 1) where only social influence is at play, e.g., weight of homophily representation is learned close to 0, the hyperbolic space will be used. On the other hand if only homophily is at play (Case 2), e.g., weight of homophily representation is learned close to 0, the spherical space will be used. In the normal case of both factors at play (Case 3), then both spaces will be used and can be jointly aligned with our space alignment mechanism. When using product space, Cases 1, 2, and 3 will all not be distinguished from each other as all cases will be modeled by one complex non-Euclidean geometric space as a Cartesian product of spherical and hyperbolic spaces. ", "page_idx": 7}, {"type": "table", "img_path": "nuZv2iTlvn/tmp/d22e78b54f4e1e5df6fde492e9515276e3277937acca7e62a54a64bc9eaa336c.jpg", "table_caption": ["Table 9: Category and description of baseline models. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Time Complexity of NMM-GNN. Regarding our model, NMM is highly efficient, with time complexity $O(e d+n d)$ , where $n$ is number of nodes, $e$ is number of edges, and $d$ is dimension size. In comparison, the time complexity analyses for the remaining baseline models are as follows: the mixture model of RaRE is $\\bar{O}(e d+n d)$ which is comparable to the NMM mixture model, and the GNN embedding models of GCN, GAT (with one-head attention), and $\\kappa$ -GCN (for $\\kappa=0$ ) are $O(e d+n d^{2})$ . $\\kappa$ -GCN (for $\\kappa\\neq0$ ) and HGCN\u2019s time complexities are $O(e d+a\\cdot n d^{2})$ , where $a$ is the fliter length, and NMM-GNN is $O(e d+n d^{2})$ which is comparable to GNN embedding models. As our work focuses on improving accuracy of learned embeddings, we further use GraphVAE training with NMM to achieve SOTA performance. We would like to point out that GraphVAE (of NMM-GNN) training is also designed to be highly parallelizable, which allows for scalability. Moreover, our model is capable of learning on real-world, highly large-scale graphs on the order of millions of nodes and billion of edges, e.g., Friendster, while achieving the best performance, which attests to its practical value to the network science community. ", "page_idx": 8}, {"type": "text", "text": "4.3 Evaluation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We detail our evaluation procedure for multi-label classification and link prediction. For all experiments, for fairness of comparison to baselines, we utilize the experiment procedure of [6]. Specifically, $90\\%$ of links are randomly sampled as training data. We do not perform cross-validation, since it may cause overftiting to occur as our framework uses learnable parameters e.g., $\\mathbf{z}^{S},\\mathbf{z}^{H},J,B,C,D,\\gamma,\\dot{\\beta}$ , $\\lambda,\\alpha,\\mathbf{W}_{l}$ , and $\\zeta$ which is a function of ${\\bf z}^{H}$ equivalently interpreted as mean square error. Per dataset, we choose hyperparameter values for $\\lambda_{A}$ in reconstruction loss: $\\{0,1,2,4,8,16,32,64\\}$ , step sizes $\\eta_{t}$ : {0.005, 0.001, 0.01, 0.05, 0.1}, and experiments are performed on AWS cluster (8 Nvidia GPUs). ", "page_idx": 8}, {"type": "text", "text": "4.3.1 Classification ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Evaluation results are in Table 10. We observe that mixture models (homophily and social influence), achieve better performance on all datasets for all metrics. Specifically, it improves over structural embedding models (GraphWave), GNNs (GAT, HGCN), and homophily-based models (GELTOR, NRP). We also see learning embeddings in non-Euclidean geometric spaces helps better represent structures in social networks (HGCN vs. GAT, RaRE vs. NMM). We further observe using GNNbased encoders with GraphVAE learning yields additional improvement (NMM vs. NMM-GNN). ", "page_idx": 8}, {"type": "text", "text": "4.3.2 Link Prediction ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "As from [6], we measure quality of link prediction by sorting probability scores of every pair of nodes per model and evaluating them using area under the ROC curve (AUC) score. Specifically, $10\\%$ of existing edges and non-existing edges are hidden from training set, and probabilities are examined by the model. Further, $10\\%$ of non-training edges are used for validation. For fairness against baselines on undirected networks, we treat all directed networks as undirected. Table 10 shows evaluation results for AUC score. Comparing relative score differences between best performing homophily embedding model (NRP) to RaRE, we observe that LiveJournal and Friendster datasets contain relatively more node social influence than BlogCatalog, which homophily-based models do not capture. Due to the above observation, it is likely that LiveJournal and Friendster (which are also larger datasets) show more realistic heterogeneity in network structure compared to BlogCatalog dataset e.g., cyclic structures produced by homophily based nodes and tree-like structures produced by social influence based nodes. Thus, modeling these structures in non-Euclidean spaces (RaRE vs. NMM) also shows more improvement. Moreover, in our NMM variant models, every node is influenced by both factors of homophily and social influence through our non-Euclidean mixture model. It is a weighted combination personalized per node of these factors that influence the links formed, rather than being generated solely through homophily vs. social influence. ", "page_idx": 8}, {"type": "table", "img_path": "nuZv2iTlvn/tmp/f73e1545adf7faaf2b6c1671f2215b4963fa63fb6ebc69bec2157af73f8925cb.jpg", "table_caption": ["Table 10: Results of social network classification and link prediction for Jaccard Index (JI), Hamming Loss (HL), F1 Score (F1), and AUC in $\\%$ using embedding dimension 64. Our NMM and its variants are in gray shading. For each group of models, the best results are bold-faced. The overall best results on each dataset are underscored. \u2020Ablation study variant models using distinct non-Euclidean geometric spaces for NMM (homophily/social influence) where E, S, and $\\mathbb{H}$ denote Euclidean, Spherical, and Hyperbolic spaces. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "5 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We are among the first to explore a Graph-based non-Euclidean mixture model for social networks. As social networks are influenced by homophily and social influence, we design a model to represent both factors jointly for nodes. Further, we model resulting unique network topologies (cycles and trees) using distinct non-Euclidean geometric spaces and introduce a GNN-based non-Euclidean variational autoencoder framework for our model, to effectively learn embeddings. The resulting model, NMM-GNN, significantly outperforms various state-of-the-art models for social networks. As future work, we hope to explore alternatives to graph-based VAE methods for improved learning. ", "page_idx": 9}, {"type": "text", "text": "6 Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was partially supported by DARPA HR0011-24-9-0370; NSF 1937599, 2106859, 2119643, 2200274, 2211557, 2303037, 2312501; NIH U54HG012517, U24DK097771; Optum AI; NASA; SRC JUMP 2.0 Center; Amazon Research Awards; and Snapchat Gifts. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Huiwen Xiang, Mingjing Xie, and Yiyi Fang. Study on the architecture space-social network characteristics based on social network analysis: A case study of anshun tunpu settlement. Elsevier, 2024.   \n[2] Farzana Afridi, Amrita Dhillon, and Swati Sharma. The ties that bind us: Social networks and productivity in the factory. Elsevier, 2024.   \n[3] Austin P. Logan, Phillip M. LaCasse, and Brian J. Lunday. Social network analysis of twitter interactions: a directed multilayer network approach. Springer, 2023.   \n[4] Wei Jiang, Xinyi Gao, Guandong Xu, Tong Chen, and Hongzhi Yin. Challenging low homophily in social recommendation. WWW, pages 3476\u20133484, 2024.   \n[5] Kazi Khanam, Gautam Srivastava, and Vijay Mago. The homophily principle in social network analysis: A survey. Springer, 2023.   \n[6] Yupeng Gu, Yizhou Sun, Yanen Li, and Yang Yang. Rare: Social rank regulated large-scale network embedding. In WWW, pages 359\u2013368, 2018.   \n[7] Liye Ma, Ramayya Krishnan, and Alan L. Montgomery. Latent homphily or social influence? an empirical analysis of purchase within a social network. In Management Science INFORMS, pages 454\u2013473, 2015.   \n[8] A.L. Barabasi, H. Jeong, Z. Neda, E. Ravasz, A. Schubert, and T. Vicsek. Evolution of the social network of scientific collaborations. Elsevier Science, 2002.   \n[9] Julian McAuley and Jure Leskovec. Learning to discover social circles in ego networks. NeurIPS, 2012.   \n[10] Li Sun, Mengjie Li, Yong Yang, Xiao Li, Lin Liu, Pengfei Zhang, and Haohua Du. Rcoco: contrastive collective link prediction across multiplex network in riemannian space. Springer, 2024.   \n[11] Xuelian Ni, Fei Xiong, Yu Zheng, and Liang Wang. Graph contrastive learning with kernel dependence maximization for social recommendation. WWW, pages 481\u2013492, 2024.   \n[12] Zitai Qiu, Congbo Ma, Jia Wu, and Jian Yang. An efficient automatic meta-path selection for social event detection via hyperbolic space. WWW, pages 2519\u20132529, 2024.   \n[13] Cosimo Gregucci, Mojtaba Nayyeri, Daniel Hern\u00e1ndez, and Steffen Staab. Link prediction with attention applied on multiple knowledge graph embedding models. WWW, pages 2600\u20132610, 2023.   \n[14] Min Zhou, Menglin Yang, Bo Xiong, Hui Xiong, and Irwin King. Hyperbolic graph neural networks: A tutorial on methods and applications. KDD, pages 5843\u20135844, 2023.   \n[15] Azad Noori, Mohammad Ali Balafar, Asgarali Bouyer, and Khosro Salmani. Review of heterogeneous graph embedding methods based on deep learning techniques and comparing their efficiency in node classification. Springer, 2024.   \n[16] Xin Liu, Tsuyoshi Murata, Kyoung-Sook Kim, Chatchawan Kotarasu, and Chenyi Zhuang. A general view for network embedding as matrix factorization. In WSDM, pages 375\u2013383, 2019.   \n[17] Masoud R. Hamedani, Jin-Su Ryu, and Sang-Wook Kim. Geltor: A graph embedding method based on listwise learning to rank. WWW, pages 6\u201316, 2023.   \n[18] Guohao Li, Chenxin Xiong, Ali Thabet, and Bernard Ghanem. Deepergcn: All you need to train deeper gcns. IEEE TPAMI, 2023.   \n[19] Peter Petersen. Riemannian geometry, volume 171. Springer, 2006.   \n[20] Gabriel Moreira, Manuel Marques, Jo\u00e3o P. Costeira, and Alexander Hauptmann. Hyperbolic vs euclidean embeddings in few-shot learning: Two sides of the same coin. IEEE WACV, pages 2082\u20132090, 2024.   \n[21] Yansong Ning, Hao Liu, Hao Wang, Zhenyu Zeng, and Hui Xiong. Uukg: Unified urban knowledge graph dataset for urban spatiotemporal prediction. NeurIPS, 2024.   \n[22] Jongmin Park, Seunghoon Han, Soohwan Jeong, and Sungsu Lim. Hyperbolic heterogeneous graph attention networks. WWW, pages 561\u2013564, 2024.   \n[23] Fragkiskos Papadopoulos, Maksim Kitsak, M. \u00c1ngeles Serrano, Mari\u00e1n Bogu\u00f1\u00e1, and Dmitri Krioukov. Popularity versus similarity in growing networks. Nature, pages 537\u2013540, 2012.   \n[24] Ines Chami, Zhitao Ying, Christopher R\u00e9, and Jure Leskovec. Hyperbolic graph convolutional neural networks. NeurIPS, 32, 2019.   \n[25] Qi Liu, Maximilian Nickel, and Douwe Kiela. Hyperbolic graph neural networks. NeurIPS, 32, 2019.   \n[26] Roshni G. Iyer, Yunsheng Bai, Wei Wang, and Yizhou Sun. Dual-geometric space embedding model for two-view knowledge graphs. In KDD, pages 676\u2013686, 2022.   \n[27] Renchi Yang, Jieming Shi, Xiaokui Xiao, Yin Yang, and Sourav S. Bhowmick. Homogeneous network embedding for massive graphs via reweighted personalized pagerank. VLDB, pages 670\u2013683, 2020.   \n[28] Feng Xia, Lei Wang, Tao Tang, Xin Chen, Xiangjie Kong, Giles Oatley, and Irwin King. Cengcn: Centralized convolutional networks with vertex imbalance for scale-free graphs. IEEE TKDE, pages 4555\u20134569, 2022.   \n[29] Danah M. Boyd. Friendster and publicly articulated social networking. ACM CHI, 2004.   \n[30] Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. Line: Largescale information network embedding. In WWW, pages 1067\u20131077, 2015.   \n[31] Shaosheng Cao, Wei Lu, and Qiongkai Xu. Grarep: Learning graph representations with global structural information. CIKM, pages 891\u2013900, 2015.   \n[32] Keith Henderson, Brian Gallagher, Tina Eliassi-Rad, Hanghan Tong, Sugato Basu, Leman Akoglu, Danai Koutra, Christos Faloutsos, and Lei Li. Rolx: structural role extraction & mining in large graphs. KDD, pages 1231\u20131239, 2012.   \n[33] Claire Donnat, Marinka Zitnik, David Hallac, and Jure Leskovec. Learning structural node embeddings via diffusion wavelets. KDD, pages 1320\u20131329, 2018.   \n[34] William L. Hamilton, Rex Ying, and Jure Lescovec. Inductive representation learning on large graphs. NeurIPS, 30, 2017.   \n[35] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. ICLR, 2017.   \n[36] Petar Velic\u02c7kovic\u00b4, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. ICLR, 2018.   \n[37] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? ICLR, 2019.   \n[38] Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. Graph contrastive learning with augmentations. NeurIPS, 2020.   \n[39] Gregor Bachmann, Gary B\u00e9cigneul, and Octavian Ganea. Constant curvature graph convolutional networks. In ICML, pages 486\u2013496. PMLR, 2020.   \n[40] Jiaxuan You, Rex Ying, Xiang Ren, William L. Hamilton, and Jure Lescovek. Graphrnn: Generating realistic graphs with deep auto-regressive models. ICML, 2018.   \n[41] Renchi Yang, Jieming Shi, Xiaokui Xiao, Yin Yang, Sourav S. Bhowmick, and Juncheng Liu. Scaling attributed network embedding to massive graphs. VLDBJ, pages 37\u201349, 2023.   \n[42] Qiaoyu Tan, Xin Zhang, Xiao Huang, Hao Chen, Jundong Li, and Xia Hu. Collaborative graph neural networks for attributed network embedding. IEEE TKDE, 2023.   \n[43] Jure Leskovec, Kevin J. Lang, Anirban Dasgupta, and Michael W. Mahoney. Community structure in large networks: Natural cluster sizes and the absence of large well-defined clusters. Internet Mathematics, 2009. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A Limitations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "NMM is highly efficient, with time complexity $O(e d+n d)$ , where $n$ is number of nodes, $e$ is number of edges, and $d$ is dimension size. As our work focuses on improving accuracy of learned embeddings, we further use GraphVAE training with NMM to achieve SOTA performance, called NMM-GNN. Using the GraphVAE training pipeline may be seen as a limitation as it is less efficient compared to GNNs for training time. However, in the worst case scenario, NMM-GNN still achieves efficiency comparable to popular heterogeneous graph models including GraphRNN [40], and training is designed to be highly parallelizable, which allows for scalability. Moreover, our model is capable of learning on real-world, highly large-scale graphs on the order of millions of nodes and billion of edges, e.g., Friendster, while achieving the SOTA performance, which attests to its practical value to the network science community. ", "page_idx": 13}, {"type": "text", "text": "Our model also makes the assumption the homophily regulated nodes lie on the surface of the spherical ball and the social influence regulated nodes lie on the open Poincare ball, which are both natural representations for modeling nodes. People have observed that more popular celebrity nodes tend to have smaller norm space and are embedded towards the center of the ball (similar to higher order concepts in the knowledge graph space), while less popular nodes (containing less social influence) are embedded towards the boundary of the ball (similar to entities in the knowledge graph space). To ensure that we can compute an intersection space (for the space unification component to unify the homophily and social influence representation for the same node), we enforce that the spherical surface norm is in the Poincare ball e.g., any learned soft value norm space between 0 and 1. In compute, while we evaluate on a cluster of 8 GPUs, at least one GPU is necessary for running our experiments (which may been seen as compute limitation). ", "page_idx": 13}, {"type": "text", "text": "Regarding ethical considerations and fairness, privacy and fairness are often topics of focus regarding social networks. Our model uses information like node popularity based on graph structure (in degree vs. out degree ratio), as well as attributed features if available. Further it also infers links through neighborhood context by looking at connected nodes. In general, a network embedding model uses personal information from the user which may impinge on their privacy. However, we make every attempt in our model to allow the user to select their granular choice of model personalization (e.g., we provide the option to choose from whether or not users want to enable their attributed information like profile interest and history being learned). ", "page_idx": 13}, {"type": "text", "text": "B A Note on Graph-Level Learning ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Our work can be generalized to the graph-level because our method learns to represent social science network factors based on topologies in the graph on clusters of nodes and edges. Thus, if the cluster of nodes and edges comprised of the entire graph and we subsequently applied graph pooling per node embedding (that we currently learn), we can reduce the embedding from node level to graph level. In this way, homophily and social influence can be modeled at the graph level. That said, it is unclear whether graph-level modeling would be specifically useful or interpretable for social network embedding models as compared to node-level modeling. This is due to the social network setting requiring links to be generated that are per node and not at the graph level, because a user (modeled as a node) is recommended to another specific user in the practical social network setting. This is different from other network domains like molecular classification where the entire graph represents one molecule e.g., atoms form individual nodes and chemical bonds form the edges. For this reason, node-level learning is in fact consistent with the recent state-of-the-art NN methods in the network science community though NMM-GNN can still be generalized to learning at the graph-level. ", "page_idx": 13}, {"type": "text", "text": "C Additional Experiments ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we provide experiment results on Wikipedia networks and on attributed graphs. As in the case for social networks, we also evaluate on multi-label classification and link prediction for Wikipedia networks and attributed graphs. For all experiments, for fairness of comparison to baselines, we utilize experiment procedure of [6]. Specifically, $90\\%$ of links are randomly sampled as training data. We do not perform cross-validation, since it may cause overfitting to occur as our framework uses learnable parameters e.g. $,\\,\\mathbf{z}^{S},\\,\\mathbf{z}^{H},\\,J,\\,B,\\,C,\\,D,\\,\\gamma,\\,\\beta,\\,\\lambda,\\,\\alpha,\\,\\mathbf{W}_{l}.$ , and $\\zeta$ which is a function of ${\\bf z}^{H}$ equivalently interpreted as mean square error. Per dataset, we choose hyperparameter values for $\\lambda_{A}$ in reconstruction loss: $\\{0,1,2,4,8,16,32,64\\}$ , and step sizes $\\eta_{t}\\colon\\{0.005,0.001,0.01,$ , 0.05, 0.1}. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Evaluation on Wikipedia networks. In Table 12, we additionally present multi-label classification and link prediction experiments on Wikipedia datasets to show the benefits of our model on other networks. These include Wikipedia Clickstream [6], which contains counts of (referrer, resource) pairs extracted from the request logs of Wikipedia, and Wikipedia Hyperlink [6], which contains edges as hyperlinks from one page to another. Dataset statistics for the Wikipedia datasets are summarized in Table 11. Our NMM model variants consistently achieves the best performance over all the competitive baseline models belonging to categories of (1) structural embedding models, (2) GNN embedding models (Euclidean space), (3) homophily-based embedding models, (4) GNN embedding models (non-Euclidean space), and (5) mixture models. This shows that NMM\u2019s model is generalizable and widely applicable because it can learn effective representations on online information networks that go beyond social networks. ", "page_idx": 14}, {"type": "table", "img_path": "nuZv2iTlvn/tmp/14c6460ef18dd9d154cca4c408e9e1f523553844b205f7ef338df603c2340bc4.jpg", "table_caption": ["Table 11: Dataset statistics for evaluation datasets. "], "table_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "nuZv2iTlvn/tmp/f4cb4648b7d4dbcaf840575f46c54b6f83dc263baeaf7e75bd7a56c26911c4e5.jpg", "table_caption": ["Table 12: Results of social network classification and link prediction for Jaccard Index $\\left(\\mathbf{J}\\mathbf{I}\\right)$ , Hamming Loss (HL), F1 Score (F1), and AUC in $\\%$ using embedding dimension 64. Our NMM and variants are in gray shading. For each group of models, best results are bold-faced. The overall best results on each dataset are underscored.\u2020Ablation study variant models using distinct non-Euclidean geometric spaces for NMM (homophily/social influence) where E, $\\mathbb{S}.$ , and $\\mathbb{H}$ denote Euclidean, Spherical, and Hyperbolic spaces. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "Evaluation on attributed graphs. As our model can also handle attributed graphs, we provide experiments against SOTA attributed network embedding models that are summarized below. Since these models require the presence of network attributes, we do not include them in our experiments on featureless graphs for fairness of comparison. We conduct evaluation on well-known largescale attributed graph datasets which include Facebook [9] and $G o o g l e+$ [9] social networks where consistent with [27], we treat each ego-network as a label and extract attributes from their user profiles. Dataset statistics are in Table 13, and results are reported in Table 14. It can be seen that our NMM-GNN model consistently achieves the best performance in both the large-scale graph datasets, indicating that our model\u2019s inherent learning ability is effective to learn graph structure and topology, which goes beyond simply exploiting information from node and edge attributes. Below is a summary of the baseline attributed network embedding models: ", "page_idx": 14}, {"type": "image", "img_path": "nuZv2iTlvn/tmp/47f04892674dfb81ab5db73fe67b73c7aa71a8afed03cd1808fd695b43c42439.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Figure 2: Ablation studies. (a) Quality of mixture model, where $\\mathbf{NMM_{hom}}$ and $\\mathbf{NMM}_{\\mathbf{rank}}$ are homophilyonly and social influence-only deconstructed NMM components. (b) Inductive reasoning for NMM-GNN and RaRE on LiveJournal. $\\%$ nodes $\\{10,30,50,70,90\\}$ are sampled ensuring no overlap with test. (c) Ablation study on quality of using space unification loss (SUL) component. ", "page_idx": 15}, {"type": "text", "text": "\u2022 PANE [41], random-walk based attirubted network embedding (ANE) model \u2022 NRP [27], described in Table 9 of the main paper \u2022 CONN [42], GNN for attributed networks via collaborative aggregation on bipartite graphs ", "page_idx": 15}, {"type": "table", "img_path": "nuZv2iTlvn/tmp/2bcb72e75efab7af28cafbbefdd31fd8cbb0f7ee90fdfd90a39c32fade0b5267.jpg", "table_caption": ["Table 13: Dataset statistics for evaluation datasets. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Table 14: Results of social network classification and link prediction for Jaccard Index (JI), Hamming Loss (HL), F1 Score (F1), and AUC in $\\%$ using embedding dimension 64. Our NMM and variants are in gray shading. The overall best results on each dataset are bold-faced. ", "page_idx": 15}, {"type": "table", "img_path": "nuZv2iTlvn/tmp/bc9b70f4f6f829a3b66b5be879be01f2e487077797c0425cc76dd5cafef51804.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "The source code and datasets for our work can be found at: https://github.com/roshnigiyer/nmm . ", "page_idx": 15}, {"type": "text", "text": "D Ablation Studies ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "This section details ablation studies where we test quality of using (1) a mixture model, (2) distinct nonEuclidean geometric spaces, (3) non-Euclidean GNN-based encoders and non-Euclidean GraphVAE framework (through the inductive setting), and (4) space unification loss component. Each of the ablation studies provides insight to motivate our architecture design choices, as well as consistently shows NMM-GNN outperforms baseline network embedding models. ", "page_idx": 15}, {"type": "text", "text": "Quality of using a mixture model architecture. We deconstruct our mixture model of NMM, to observe the effect different network factors have on learning embeddings, where $\\alpha$ and $\\beta$ are learnable. We report results on embedding dimension 64, evaluated on AUC score, with the models summarized below: ", "page_idx": 15}, {"type": "text", "text": "\u2022 $\\mathbf{NMM_{hom}}$ , deconstructed homophily component: $p_{\\theta}(e_{i j}=1)=\\alpha\\cdot p_{\\mathrm{hom}}(e_{i j}=1)$ \u2022 $\\mathbf{NMM}_{\\mathbf{rank}}$ , deconstructed social influence component: $p_{\\theta}(e_{i j}=1)=\\beta\\cdot p_{\\mathrm{rank}}(e_{i j}=1)$ \u2022 NMM, which is our mixture model defined by Eqn. 6 ", "page_idx": 15}, {"type": "text", "text": "As shown in Figure 2(a), the mixture model of NMM outperforms that of its subcomponents $\\mathbf{NMM_{hom}}$ and $\\mathbf{NMM}_{\\mathbf{rank}}$ on all datasets for link prediction. This validates the effectiveness of using a mixture model architecture for modeling both homophily and social influence factors jointly. ", "page_idx": 15}, {"type": "text", "text": "Quality of using distinct non-Euclidean geometric spaces. We study combinations of geometric spaces to model NMM (homophily/social influence) to observe the effect it has on learning topological structure, denoted with $\\mathbf{NMM}(\\cdot)^{\\dagger}$ in Table 10 (main paper). As shown, the choice of modeling homophily based nodes in spherical space and modeling social influence based nodes in hyperbolic space leads to the best performance. Further, NMM outperforms its Euclidean space counterpart showing that the social network exhibits structures (cycles and hierarchy) that need to be appropriately represented in curved geometric spaces. There is also evidence of non-Euclidean topologies in the datasets. For example, the average node on LiveJournal has in-degree of 17 but outdegree of 25, showing several hierarchical structures present, and $17.7\\%$ of LiveJournal data contains cycles [43]. ", "page_idx": 16}, {"type": "text", "text": "Link prediction on unseen nodes (inductive task). We study ability of NMM-GNN to learn on the inductive setting (in addition to the standard transductive setting of Table 10 of the main paper). To do so, we randomly sample $\\%$ of nodes being $\\{10,30,50,70,9\\bar{0}\\}$ and their corresponding links as our training set. Test nodes are ensured to have no overlap with training nodes, to allow for link prediction on unseen graphs. Figure 2(b) reports results on NMM-GNN and RaRE for LiveJournal on embedding dimension of 64 for AUC score. NMM-GNN outperforms RaRE on all settings of training nodes. This shows the effectiveness of NMM-GNN in using non-Euclidean GNNbased encoders and a non-Euclidean GraphVAE training framework during the learning process, two components that RaRE lacks. Further, as less training nodes are observed, NMM-GNN outperforms RaRE by larger margins (e.g., $10\\%$ vs. $70\\%$ training nodes), showing NMM-GNN better generalizes to unseen graphs. ", "page_idx": 16}, {"type": "text", "text": "Quality of using space unification loss. We test quality of using our proposed space unification loss (SUL) in our NMM-GNN, by conducting experiments for with the loss component (NMM-GNN) and without it (NMM-GNN-no-SUL). We report results on embedding dimension 64, evaluated on AUC score in Figure 2(c), for datasets Wikipedia Clickstream (WC), BlogCatalog (BC), LiveJournal $(L J)$ , Wikipedia Hyperlink (WH), and Friendster $(F)$ . As shown, using SUL improves performance on all datasets, indicating importance of bridging together representations of distinct non-Euclidean spaces (spherical and hyperbolic space) at node level. ", "page_idx": 16}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: The abstract summarizes the main paper claims and we also provide a contribution summary with bullet points in the Introduction. Further, all these main claims of the paper are supported through Section 3 (detailing our model architecture), Section 4 on Experiments, as well as our (four) ablation studies in the Appendix. ", "page_idx": 17}, {"type": "text", "text": "Our model novelly represents both homophily and social influence factors when modeling the social network, an advancement over the recent network embedding methods. Further, the novel utilization of non-Euclidean geometric spaces to model the resulting topologies due to network factors through appropriate positive and negative curvatures naturually exibiting properties of cycles and hierarchy (the observed topological heterogeneity), tremendously improves the representation capability of network embedding modes. This is evidenced by the empirical results as ablation studies for our work. Moreover, NMM-GNN not only improves against SOTA models in the performance metrics (Jaccard Index, Hamming Loss, , F1 score, AUC Score), but is also applicable to the Inductive Setting, other large-scale information networks like Wikipedia networks, and attributed networks (Facebook and Google+), while consistently achieving the best performance indicating model generalizability. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 17}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: We have an entire section on Limitations in the Appendix (Section A) that addresses all the guidelines below. Our model also makes the assumption the homophily regulated nodes lie on the surface of the spherical ball and the social influence regulated nodes lie on the open Poincare ball, which are both natural representations for modeling nodes. People have observed that more popular celebrity nodes tend to have smaller norm space and are embedded towards the center of the ball (similar to higher order concepts in the knowledge graph space), while less popular nodes (containing less social influence) are embedded towards the boundary of the ball (similar to entities in the knowledge graph space). To ensure that we can compute an intersection space (for the space unification component to unify the homophily and social influence representation for the same node), we enforce that the spherical surface norm is in the Poincare ball e.g., any learned soft value norm space between 0 and 1. In compute, while we evaluate on a cluster of 8 GPUs, at least one GPU is necessary for running our experiments (which may been seen as compute limitation). ", "page_idx": 17}, {"type": "text", "text": "Regarding ethical considerations and fairness, privacy and fairness are often topics of focus regarding social networks. Our model uses information like node popularity based on graph structure (in degree vs. out degree ratio), as well as attributed features if available. Further it also infers links through neighborhood context by looking at connected nodes. In general, a network embedding model uses personal information from the user which may impinge on their privacy. However, we make every attempt in our model to allow the user to select their granular choice of model personalization (e.g., we provide the option to choose from whether or not users want to enable their attributed information like profile interest and history being learned). ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 18}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: The assumptions about the social network graph as well as our model are comprehensively listed in Section 3 which contains the \"Methodology\" and \"Overview\" assumptions as well as subsections that each list the associated assumptions for that particular model component. Moreover, Section 4 (Experiments) and Appendix (Additional Experiments $^+$ Ablation Studies) detail the entire experiment evaluation procedure and design as well as all the corresponding graph assumptions. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 18}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The full details of experiments for train and test sampling and evaluation etc. as well as the comprehensive experiment design procedure are provided in Section 4 (Experiments). We also provide all hyperparameter setting details. Further, we provide all the details for each of the baseline models evaluated for both Experiments as well as Ablation Studies. Moreover, we release all source code and datasets used in the paper for enhanced reproducibility. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 19}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: We provide open access to all our source code as well as for the datasets in the Supplemental Material that has been uploaded. As part of this, we also include a README file to details the setup/requirement as well as commands instructions. We also refer to this in Section 3 (Methodology) and Section 4 (Experiments). In addition, we detail all the hyperparameter settings in the main paper (Section 4). ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code. \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details. ", "page_idx": 19}, {"type": "text", "text": "\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 20}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We have an entire training procedure section and loss function information inside Section 3 (Methodology) which comprehensively describes our model architecture. All optimizer and loss function details are presented here. Our Section 4 (Experiments) and Appendix (which contains our four Ablation Studies) detail the experiment evaluation for the corresponding experiments and detail all the training and test details as well as any additional information needed to understand the results. We provide detailed information about hyperparameter settings (Section 4), which were selected based on performance on the validation set (train/validation/test split details also in this section). Further, the use of all evaluation metrics from the experiments are clearly defined (paper resource references pointed) and justified. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 20}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: We provide all details about distribution priors, random embedding initialization etc., and evaluate our model on standard experiment metrics important to the social network community for determining model quality. Error bars are not applicable in this scenario but our paper details all experiment conditions. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper has a section devoted to analyzing the memory complexity of the model (Section 4: Experiments), as well as also provide compute and hyperparameter details in this section and Section 3 (model architecture). Additionally, we release all of our source code and datasets for maximal reproducibility (Supplement Material). ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 21}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: All paper guidelines have been carefully followed regarding page limit, formatting, content, research topic and relevance, writing, figures etc. ", "page_idx": 21}, {"type": "text", "text": "All privacy and data security concerns are also addressed as we utilize public benchmark datasets that do not contain any sensitive user information. Our Limitations section further provides details regarding ethical considerations. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 21}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper provides clear motivation of the importance of the research problem of investigation and further identifies key challenges in the network science community in state-of-the-art embedding models, which thus motivates our model architecture. Further, our paper provides clear intuition and justification for each design choice decision in our model components. Additionally, impact of our model is discussed with respect to the social network community and more broadly for other online information networks e.g., Wikipedia datasets and attributed networks (in the Appendix section on Ablation studeies). Lastly, we also discuss Limitations of our model in Appendix Section A. ", "page_idx": 22}, {"type": "text", "text": "This model effectively also learns without needed data attributes, which could beneficially influence privacy considerations in social networks (that users do not need to have their profile information and history being stored). Of course, any network science model could have potential for misuse in areas like surveillance or manipulation of online communities, so litigation and action should be taken to safeguard against misuse. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 22}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: We have been extremely careful on the type of data and model we use and to consider all ethical aspects respectively. No human subjects were used in our research study (N/A), and all of our datasets for evaluation are large-scale public benchmark popular social media datasets evaluated on numerous papers in the recent years. As such, these datasets are highly suitable for direct comparisons against various SOTA models as they have been widely tested on. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. ", "page_idx": 22}, {"type": "text", "text": "\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 23}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: All source codes are attributed to the respective authors and we even identify in our paper the training procedures if used from prior work for fairness of comparison e.g., RaRE [6] that we indicate in Section 4 (Experiments). We also release all our source code and datasets (in addition to referencing each of them in our main paper for the source information) for reproducibility under MIT and CC-by licences. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 23}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper is about improved model development for enhancing representation learning of nodes in the social network to better explain how links in the social network are formed. No new assets or datasets etc. are released as part of this work. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 23}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: As mentioned in previous questions this is not applicable for this paper and all datasets evaluated are popular large-scale networks from social media that have been widely evaluated on in recent years in the network science community. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 24}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: This paper does not involve any research or testing of human subjects. All training and testing procedures are solely conducted on publically released benchmark datasets that do not involve intervention from human subjects. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}]