[{"figure_path": "qLnXPVvwLx/figures/figures_1_1.jpg", "caption": "Figure 1: Prism Framework Architecture. Prism framework takes image-query pairs as input. An instruction (can be query-agnostic or query-aware) and the image are first fed into the VLM to extract visual information. Then, an LLM is used to generate the answer based on the reformatted query which combines the original question and visual information in textual form.", "description": "The figure illustrates the architecture of the Prism framework, which decouples the perception and reasoning stages of visual question answering.  Image-query pairs are input. The perception stage uses a Vision Language Model (VLM) to extract visual information from the image guided by an instruction (either query-agnostic or query-aware). This information is then combined with the original question to form a reformatted query for the reasoning stage. The reasoning stage employs a Large Language Model (LLM) to generate the final answer based on the textual visual information and reformatted query.  The figure shows various options for instructions and lists several example VLMs and LLMs.", "section": "2.1 Prism Architecture"}, {"figure_path": "qLnXPVvwLx/figures/figures_5_1.jpg", "caption": "Figure 2: Comparing End-to-End Performance and Perception Capability on MMStar. We display model accuracies in end-to-end VQA and the Prism perception test with query-specific instructions. Most small-scale (7B, 13B, etc.) VLMs achieve better performance within Prism.", "description": "This figure compares the overall performance of various vision-language models (VLMs) in a standard end-to-end visual question answering (VQA) task versus a two-stage approach using the Prism framework. The two-stage approach separates perception (extracting visual information) and reasoning (answering the question based on visual information).  The graph shows that while large proprietary VLMs like GPT-40 generally perform better overall, smaller, open-source models often show improved performance when using Prism's two-stage approach, suggesting that these models' limitations may primarily lie in their reasoning capabilities rather than perception.", "section": "3 Evaluation Results"}, {"figure_path": "qLnXPVvwLx/figures/figures_7_1.jpg", "caption": "Figure 1: Prism Framework Architecture. Prism framework takes image-query pairs as input. An instruction (can be query-agnostic or query-aware) and the image are first fed into the VLM to extract visual information. Then, an LLM is used to generate the answer based on the reformatted query which combines the original question and visual information in textual form.", "description": "The figure illustrates the architecture of the Prism framework, which is a modular system designed to decouple the perception and reasoning processes involved in visual question answering. It takes image-query pairs as input and consists of two stages: a perception stage where a VLM extracts visual information from an image based on an instruction (which can be either query-agnostic or query-aware) and a reasoning stage where an LLM generates an answer using the reformatted query containing textual information from both the original query and the visual information extracted by the VLM. This modular design allows for systematic comparison and assessment of both VLMs and LLMs.", "section": "2.1 Prism Architecture"}, {"figure_path": "qLnXPVvwLx/figures/figures_8_1.jpg", "caption": "Figure 2: Comparing End-to-End Performance and Perception Capability on MMStar. We display model accuracies in end-to-end VQA and the Prism perception test with query-specific instructions. Most small-scale (7B, 13B, etc.) VLMs achieve better performance within Prism.", "description": "The figure compares the performance of various vision-language models (VLMs) on the MMStar benchmark in two settings: end-to-end visual question answering (VQA) and a decoupled approach using the Prism framework. The Prism approach separates perception (visual information extraction) and reasoning (answer generation). The graph shows that most small-scale VLMs (those with 7B or 13B parameters) perform better when using the Prism framework compared to the end-to-end approach.  This suggests that the limitations of these smaller models are primarily in reasoning rather than perception.", "section": "3 Evaluation Results"}, {"figure_path": "qLnXPVvwLx/figures/figures_16_1.jpg", "caption": "Figure 1: Prism Framework Architecture. Prism framework takes image-query pairs as input. An instruction (can be query-agnostic or query-aware) and the image are first fed into the VLM to extract visual information. Then, an LLM is used to generate the answer based on the reformatted query which combines the original question and visual information in textual form.", "description": "This figure illustrates the architecture of the Prism framework, which is a two-stage framework for decoupling and assessing the capabilities of Vision-Language Models (VLMs). The first stage involves a VLM that extracts visual information from an image, based on an instruction (query-agnostic or query-aware).  The second stage utilizes a Large Language Model (LLM) to generate an answer based on the extracted visual information and the original question. This modular design allows for separate assessment of perception and reasoning capabilities.", "section": "2.1 Prism Architecture"}, {"figure_path": "qLnXPVvwLx/figures/figures_19_1.jpg", "caption": "Figure 1: Prism Framework Architecture. Prism framework takes image-query pairs as input. An instruction (can be query-agnostic or query-aware) and the image are first fed into the VLM to extract visual information. Then, an LLM is used to generate the answer based on the reformatted query which combines the original question and visual information in textual form.", "description": "The figure shows the architecture of the Prism framework.  Prism consists of two stages: a perception stage and a reasoning stage.  In the perception stage, an image and an instruction (which can be either query-agnostic or query-aware) are fed into a Vision Language Model (VLM) to extract visual information and articulate this information in textual form. In the reasoning stage, this textual visual information and the original question are fed into a Large Language Model (LLM) to generate an answer. The VLM and LLM can be replaced flexibly to enable various combinations to assess their perception and reasoning capabilities separately.", "section": "2.1 Prism Architecture"}, {"figure_path": "qLnXPVvwLx/figures/figures_20_1.jpg", "caption": "Figure 1: Prism Framework Architecture. Prism framework takes image-query pairs as input. An instruction (can be query-agnostic or query-aware) and the image are first fed into the VLM to extract visual information. Then, an LLM is used to generate the answer based on the reformatted query which combines the original question and visual information in textual form.", "description": "This figure illustrates the architecture of the Prism framework.  It shows two stages: a perception stage and a reasoning stage. In the perception stage, an image and an instruction (either query-agnostic or query-aware) are input to a Vision Language Model (VLM) to extract visual information. This information is then formatted as text. In the reasoning stage, a Large Language Model (LLM) receives the textual visual information and the original question to produce the final answer.", "section": "2.1 Prism Architecture"}, {"figure_path": "qLnXPVvwLx/figures/figures_21_1.jpg", "caption": "Figure 1: Prism Framework Architecture. Prism framework takes image-query pairs as input. An instruction (can be query-agnostic or query-aware) and the image are first fed into the VLM to extract visual information. Then, an LLM is used to generate the answer based on the reformatted query which combines the original question and visual information in textual form.", "description": "The figure shows the architecture of the Prism framework, which consists of two stages: a perception stage and a reasoning stage.  In the perception stage, a Vision Language Model (VLM) receives an image and a query (which can be either query-agnostic or query-aware). The VLM processes the image and query to extract relevant visual information, which is then converted into text. In the reasoning stage, a Large Language Model (LLM) receives the textual visual information and the original query. The LLM processes this information to generate a final answer.", "section": "2.1 Prism Architecture"}, {"figure_path": "qLnXPVvwLx/figures/figures_22_1.jpg", "caption": "Figure 1: Prism Framework Architecture. Prism framework takes image-query pairs as input. An instruction (can be query-agnostic or query-aware) and the image are first fed into the VLM to extract visual information. Then, an LLM is used to generate the answer based on the reformatted query which combines the original question and visual information in textual form.", "description": "The figure shows the architecture of the Prism framework, which consists of two stages: a perception stage and a reasoning stage. In the perception stage, a Vision Language Model (VLM) extracts visual information from an image and reformats it into a textual form, based on a given instruction (which can be query-agnostic or query-aware).  In the reasoning stage, a Large Language Model (LLM) uses this textual information along with the original query to generate a final answer.  Different VLMs and LLMs can be used in each stage, allowing for flexible experimentation and evaluation.", "section": "2.1 Prism Architecture"}, {"figure_path": "qLnXPVvwLx/figures/figures_23_1.jpg", "caption": "Figure 1: Prism Framework Architecture. Prism framework takes image-query pairs as input. An instruction (can be query-agnostic or query-aware) and the image are first fed into the VLM to extract visual information. Then, an LLM is used to generate the answer based on the reformatted query which combines the original question and visual information in textual form.", "description": "The figure shows the architecture of the Prism framework, which consists of two stages: a perception stage and a reasoning stage. In the perception stage, a Vision Language Model (VLM) extracts visual information from an image and reformats it into a textual form. In the reasoning stage, a Large Language Model (LLM) generates an answer based on the reformatted query and the original question. The framework is modular and flexible, allowing for different VLMs and LLMs to be used. This modular design allows for decoupling and assessment of the perception and reasoning capabilities of VLMs.", "section": "2.1 Prism Architecture"}, {"figure_path": "qLnXPVvwLx/figures/figures_24_1.jpg", "caption": "Figure 1: Prism Framework Architecture. Prism framework takes image-query pairs as input. An instruction (can be query-agnostic or query-aware) and the image are first fed into the VLM to extract visual information. Then, an LLM is used to generate the answer based on the reformatted query which combines the original question and visual information in textual form.", "description": "The figure illustrates the architecture of the Prism framework, which is a two-stage system for decoupling and assessing the capabilities of Vision-Language Models (VLMs). In the first stage, a VLM extracts visual information from an image and a query, which is then fed to an LLM in the second stage. The LLM generates an answer based on the combined visual and textual information. The modular design allows for flexible combination of VLMs and LLMs for evaluating their relative capabilities.", "section": "2.1 Prism Architecture"}, {"figure_path": "qLnXPVvwLx/figures/figures_25_1.jpg", "caption": "Figure 1: Prism Framework Architecture. Prism framework takes image-query pairs as input. An instruction (can be query-agnostic or query-aware) and the image are first fed into the VLM to extract visual information. Then, an LLM is used to generate the answer based on the reformatted query which combines the original question and visual information in textual form.", "description": "The figure shows the architecture of the Prism framework, which is composed of two stages: perception and reasoning. In the perception stage, a VLM takes an image and a query as input and extracts visual information. This information is then passed to the reasoning stage, where an LLM generates an answer based on the extracted information and the original query. The framework is designed to decouple perception and reasoning processes, which is important for assessing the capabilities of VLMs.", "section": "2.1 Prism Architecture"}, {"figure_path": "qLnXPVvwLx/figures/figures_27_1.jpg", "caption": "Figure 1: Prism Framework Architecture. Prism framework takes image-query pairs as input. An instruction (can be query-agnostic or query-aware) and the image are first fed into the VLM to extract visual information. Then, an LLM is used to generate the answer based on the reformatted query which combines the original question and visual information in textual form.", "description": "The figure shows the architecture of the Prism framework. It consists of two main stages: a perception stage and a reasoning stage. In the perception stage, a Vision Language Model (VLM) takes an image and a query as input and extracts visual information from the image. This information is then passed to the reasoning stage, where a Large Language Model (LLM) uses the extracted visual information along with the original query to generate a final answer.", "section": "2.1 Prism Architecture"}, {"figure_path": "qLnXPVvwLx/figures/figures_28_1.jpg", "caption": "Figure 1: Prism Framework Architecture. Prism framework takes image-query pairs as input. An instruction (can be query-agnostic or query-aware) and the image are first fed into the VLM to extract visual information. Then, an LLM is used to generate the answer based on the reformatted query which combines the original question and visual information in textual form.", "description": "The figure shows the architecture of the Prism framework, which consists of two stages: a perception stage and a reasoning stage. In the perception stage, a Vision Language Model (VLM) processes an image and a query to extract visual information. Then, in the reasoning stage, a Large Language Model (LLM) takes the reformatted query (combining the original query and the extracted visual information) as input to generate the final answer. The figure shows the different components of the two stages, including the instruction options, the VLM zoo, the LLM zoo, and the output.", "section": "2.1 Prism Architecture"}]