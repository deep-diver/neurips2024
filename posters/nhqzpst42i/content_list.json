[{"type": "text", "text": "Understanding Visual Feature Reliance through the Lens of Complexity ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Thomas Fel \u2217\u2020 Louis B\u00e9thune \u2217\u2021 Andrew Kyle Lampinen   \nGoogle DeepMind Universit\u00e9 de Toulouse Google DeepMind   \nBrown University Thomas Serre Katherine Hermann Brown University Google DeepMind ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recent studies suggest that deep learning models\u2019 inductive bias towards favoring simpler features may be one of the sources of shortcut learning. Yet, there has been limited focus on understanding the complexity of the myriad features that models learn. In this work, we introduce a new metric for quantifying feature complexity, based on $\\nu$ -information and capturing whether a feature requires complex computational transformations to be extracted. Using this $\\nu$ -information metric, we analyze the complexities of 10,000 features\u2014represented as directions in the penultimate layer\u2014that were extracted from a standard ImageNet-trained vision model. Our study addresses four key questions: First, we ask what features look like as a function of complexity and find a spectrum of simple-to-complex features present within the model. Second, we ask when features are learned during training. We find that simpler features dominate early in training, and more complex features emerge gradually. Third, we investigate where within the network simple and complex features \u201cflow\u201d, and find that simpler features tend to bypass the visual hierarchy via residual connections. Fourth, we explore the connection between features\u2019 complexity and their importance in driving the network\u2019s decision. We find that complex features tend to be less important. Surprisingly, important features become accessible at earlier layers during training, like a \u201csedimentation process,\u201d allowing the model to build upon these foundational elements. ", "page_idx": 0}, {"type": "text", "text": "\u201cIt is necessary to have on hand a method of measuring the complexity of calculating devices which in turn can be done if one has a theory of the complexity of functions, some partial results on this problem have been obtained by Shannon.\u201d ", "page_idx": 0}, {"type": "text", "text": "Darmouth Workshop proposal [70] ", "page_idx": 0}, {"type": "text", "text": "Measuring complexity is one of the core problems described by Shannon & McCarty in the famous 1956 proposal of the Dartmouth workshop. This problem\u2014and the question, \u201cHow can a set of (hypothetical) neurons be arranged to form concepts?\u201d[70]\u2014encapsulate what we investigate: how do neural networks form features and concepts [53], and how can their complexity be quantified? ", "page_idx": 0}, {"type": "text", "text": "Recent studies [97, 48] reveal that models often favor simpler features, which may contribute to shortcut learning [6, 71, 36, 100]. For example, CNNs privilege texture over object shape [10, 37, 46] and single diagnostic pixels over semantic content [69]. Moreover, models tend to prefer input features that are linearly rather than nonlinearly related to task labels [47, 97]. However, there ", "page_idx": 0}, {"type": "image", "img_path": "NhqZpst42I/tmp/1920bdcf0d1352ecff7ba9c3d3db3df559651e4dcda63a904b5dd3205d7e3df0.jpg", "img_caption": ["Maximally activating images Feature visualization of $z_{3}$ at Block 10 "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: A) Simple vs. Complex Features. Shown is an example of three features extracted using an overcomplete dictionary on the penultimate layer of a ResNet50 trained on ImageNet. Although all three features can be extracted from the final layer of a ResNet50, some features, such as $z_{1}$ , seem to respond to color, which can be linearly extractable directly from the input. In contrast, $z_{2},z_{3}$ visualization appear more \u201cComplex\u201d, responding to more diverse stimuli. In this work, we seek to study the complexity of features. We start by introducing a computationally inspired complexity metric. Using this metric, we inspect both simple and complex features of a ResNet50. B) Feature Evolution Across Layers. Each row illustrates how a feature from the penultimate layer $(z_{1},z_{2},z_{3})$ evolves as we decode it using linear probing at the outputs of blocks 1, 5, and 10 of the ResNet50. Simpler features, like color, are decodable throughout the network. The feature in the middle shows similar visualization at block 10 and the penultimate layer, whereas the most complex feature is only decodable at the end. Our complexity metric, based on $\\nu$ -information [115], measures how easily a model extracts a feature across its layers. ", "page_idx": 1}, {"type": "text", "text": "has not been a comprehensive quantitative framework for assessing the complexities of features learned by large-scale, natural image-trained vision models used in practice. Leveraging recent observations and advances in feature (also called \u201cconcept\u201d) extraction from the area of Explainable AI [83, 8, 25, 29, 32, 77, 19, 32, 42], we extract a large set of features from an ImageNet-trained model [45], and analyze their complexity. ", "page_idx": 1}, {"type": "text", "text": "Our contributions are as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We build upon $\\nu$ -information [115]\u2014which measures the mutual information between two variables, considering computational constraints\u2014to introduce a measure of feature complexity. We use this measure to quantify the complexity of over 10,000 features in an ImageNet model [45] at each epoch of training.   \n\u2022 We visualize the differences between simple and complex features on a spectrum to understand which features are readily available to our model and which ones require more computation and transformation to retrieve.   \n\u2022 We investigate where sensitivity to simple versus complex features emerges during a forward pass through the model. Our findings suggest that residual connections \u201cteleport\u201d simple features, computed in early layers, to the final layer. The main branch naturally facilitates the layer-wise construction of more complex features.   \n\u2022 We examine feature learning dynamics, revealing when different concepts emerge over the course of training, and find that complex concepts tend to appear later than simpler ones.   \n\u2022 We explore the link between complexity and importance in driving the model\u2019s decisions. We find a preference for simpler features over more complex ones. This simplicity bias emerges during training, and, surprisingly, the model simplifies its most important features over time. ", "page_idx": 1}, {"type": "text", "text": "1 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Feature analysis. Large vision models learn a diversity of features [77, 84] to support performance on the training task and can exhibit preferences for certain features over others, for example textures over shapes [10, 37, 46]. These preferences can be related to their use of shortcuts [36, 100] which compromise generalization capabilities [74, 73]. Hermann et al. [48] suggest that a full account of a model\u2019s feature preferences should consider both the predictivity and availability of features, and identify image properties that induce a shortcut bias. Relatedly, work shows that models often prefer features that are computationally simpler to extract\u2014a \u201csimplicity bias\u201d [107, 87, 91, 7, 97, 47]. ", "page_idx": 2}, {"type": "text", "text": "Explainability. Attribution methods [99, 117, 9, 33, 88, 80, 41, 102, 106] seek to attribute model predictions to specific input parts and to visualize the most important area of an image for a given prediction. In response to the many limitations of these methods [2, 38, 101, 27, 54, 79], Feature Visualization [78, 84, 44] methods have sought to allow for the generation of images that maximize certain structures in the model \u2013 e.g., a single neuron, entire channel, or direction, providing a clearer view features learned early [82, 96], as well as circuits present in the models [83, 62]. Recently, work has scaled these methods to deeper models [31]. Another approach, complementary to feature visualization, is automated concept extraction [39, 119, 30, 32, 1, 112], which identifies a wide range of concepts \u2013 directions in activations space \u2013 learned by models, inspired by recent works that suggest that the number of learned features often exceeds the neuron count [29]. This move towards over-complete dictionary learning for more comprehensive feature analysis represents a critical advancement. ", "page_idx": 2}, {"type": "text", "text": "Complexity. On a theoretical level, the complexity of functions in deep learning has long been a subject of interest, with traditional frameworks like VC-dimension falling short of adequacy with current results. In particular, deep learning models often have the capacity to memorize the entire dataset, yet still generalize [118]; the reason is often suggested to be a positive benefit of simplicity bias [3, 51, 110]. Measures of the complexity of neural network functions are hard to make tractable [93]. Recent work has proposed various methods to evaluate this complexity. For instance, [17] proposed a score of non-linearity propagation, while [52] introduced a measure of local complexity based on spline partitioning. Additionally, [111] demonstrated that models tend to learn functions with low sensitivity to random changes in the input. The role of optimizers in complexity has also been explored. It has been shown that different optimizers impact the features learned by models; for example, [105] found that sharpness-aware minimization (SAM) [34] learns more diverse features, both simple and hard, whereas stochastic gradient descent (SGD) models tend to rely on simpler features. Furthermore, [24] utilized category theory to propose a metric based on redundancy, which consist in merging neurons until a distance gap is too large, with this distance gap acting as a hyperparameter. Concurrent work by Lampinen et al. [58] studies representations induced by input features of different complexities when datasets are carefully controlled and manipulated. Finally, Okawa et al. [81], Park et al. [85] investigated the development of concepts during the training process on toy datasets and revealed that the sequence in which they appear, related to their complexity, can be attributed to the multiplicative emergence of compositional skills. ", "page_idx": 2}, {"type": "text", "text": "Concerning algorithmic complexity, Kolmogorov complexity [103, 56, 21], later expanded by Levin [63] to include a computational time component, offers a measure for evaluating the shortest programs capable of generating specific outputs on a Turing machine [22, 43]. This notion of complexity is at the roots of Solomonoff induction [104], which is often understood as the formal expression of Occam\u2019s razor and has received some attention in deep learning community [94, 95, 16]. Further developing these concepts, $\\mathcal{V}$ -information [115] introduces computational constraints on mutual information measures, extending Shannon\u2019s legacy. This methodology enables the assessment of a feature\u2019s availability or the simplicity with which it can be decoded from a data source. We will formally introduce this concept in Section 2. ", "page_idx": 2}, {"type": "image", "img_path": "NhqZpst42I/tmp/4b6a1ce43faaf99dc96fa2a10ce078e9c47bafb7bb1fc7360db7f454c6aa2802.jpg", "img_caption": ["Figure 2: Qualitative Analysis of \u201cMeta-feature\u201d (cluster of features) Complexity. (Left) A 2D UMAP projection displaying the 10,000 extracted features. The features are organized into 150 clusters using K-means clustering applied to the feature dictionary $\\mathbf{D}^{\\star}$ . 30 clusters were selected for analysis of features at different complexity levels. (Right) For each Meta-feature cluster, we compute the average complexity score. This allows us to classify the features based on their complexity according to the model. Notably, simple features are often akin to color detectors (e.g., grass, sky) and detectors for low-frequency patterns (e.g., bokeh detector) or lines. In contrast, complex features encompass parts or structured objects, as well as features resembling shapes (such as ears or curve detectors). Visualizations of individual Meta-features are presented in Appendix B. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "2 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Before we measure feature complexity, we define what is meant by features, explain how they are extracted, and then introduce the complexity metric. ", "page_idx": 3}, {"type": "text", "text": "Model Setup. We study feature complexity within an ImageNet-trained ResNet50 [45]. We train the model for 90 epochs with an initial learning rate of 0.7, adjusted down by a factor of 10 at epochs 30, 60, and 80, achieving a $78.9\\%$ accuracy on the ImageNet validation set, which is on par with reported accuracy in similar studies [45, 114]. Focusing on one model reduces architectural variables, creating a controlled environment to analyze feature complexities and provide insights for broader model hypotheses. ", "page_idx": 3}, {"type": "text", "text": "Feature Extraction. We operate within a classical supervised machine learning setting on $\\textstyle(\\Omega,{\\mathcal{F}},\\mathbb{P})$ \u2013 the underlying probability space \u2013 where $\\Omega$ is the sample space, $\\mathcal{F}$ is a $\\sigma$ -algebra on $\\Omega$ , and $\\mathbb{P}$ is a probability measure on $\\mathcal{F}$ . The input space is denoted $\\dot{\\boldsymbol{x}}\\subseteq\\mathbb{R}^{d}$ . Let the input data $x:\\Omega\\to\\mathcal{X}$ be random variables with distributions $P_{x}$ . We will explore how, from $_{x}$ and using a neural network, we extract a series of $k$ features. We will assume a classical vision neural network that admits a series of $n$ intermediate spaces, such that: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{f}_{\\ell}:\\mathcal{X}\\rightarrow\\mathcal{A}_{\\ell}\\rightarrow\\mathrm{with}~~\\ell\\in\\{1,\\dots,n\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Initially, one might suggest that a feature is a dimension of the model, meaning, for example, that a feature could be a neuron in the last layer of the model $z=f_{n}(\\mathbf{x})_{i},i\\in\\left\\{1,\\ldots,|A_{n}|\\right\\}$ , thus each of the neurons would be a feature. However, several recent studies [83, 8, 25, 29, 32] have shown that our models actually learn a multitude of features, far more than the number of neurons, which explains, for example, why they are not mono-semantic [77, 19], which could also hinder our study of features. Therefore, we use a recent explainability method, Craft [30], to extract more features than neurons and avoid this problem of superposition \u2013 or feature collapse. With $\\pmb{f}_{n}(\\pmb{x})$ being the penultimate layer, we extract a large number of features, five times more than the number of neurons, using an over-complete dictionary of concepts $\\mathbf{D}^{\\star}\\in\\mathbb{R}^{k\\times|A_{n}|}$ , with $k\\gg|\\mathcal{A}_{n}|$ . This dictionary is obtained by optimization over the entire training set and contains a total of $k=10,000$ features. Thus, for a new point $_{x}$ , we obtain the value of the $k$ features \u2013 we recall that the number of features is greater than the number of neurons, $k\\gg|\\mathcal{A}_{n}|-\\mathsf{b}\\mathsf{y}$ solving the following optimization problem: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\nz=\\underset{z\\geq0}{\\arg\\operatorname*{min}}||f_{n}(\\mathbf{x})-z\\mathbf{D}^{\\star}||_{F}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "With $z\\in\\mathbb{R}^{k}$ being the value for each feature of the image $_{x}$ , in particular, and from now on for ease of notation, we consider $z_{i}\\in\\mathbb{R},i\\in\\left\\{1,\\ldots,k\\right\\}$ that we will simply denote $z$ for the rest of the paper, as a specific feature for which we want to compute a complexity score. Thus, in our work, a feature refers to a random scalar value extracted by a dictionary learning method on the activations. More details and full derivation regarding the training of $\\mathbf{D}^{\\star}$ are available in the Appendix A. ", "page_idx": 4}, {"type": "text", "text": "Complexity through the Lens of Computation. To formalize this, still on $\\textstyle(\\Omega,{\\mathcal{F}},\\mathbb{P})$ , we denote the output space $\\mathcal{Z}$ , and $z:\\Omega\\to\\mathcal{Z}$ are random variables of a feature of interest with distributions $P_{z}$ . The joint random vector $(\\mathbf{x},z)$ representing an image $_{x}$ and the value of its feature $z$ on $(\\Omega,{\\mathcal{F}})$ has a joint distribution $P$ defined over the product space $\\mathcal X\\times\\mathcal Z$ . Furthermore, $\\mathcal{P}(\\mathcal{Z})$ denotes the set of all probability measures on $\\mathcal{Z}$ . We can now associate, for an $_{x}$ which we recall is a real-valued random variable, a corresponding feature $z$ , another real-valued random variable, and we seek to correctly evaluate the complexity of the mapping from $_{x}$ to $z$ . For this, we turn to the $\\nu$ -Information [115] that generalizes and extends the classical mutual information $\\mathbb{Z}(\\cdot,\\cdot)$ from Shannon\u2019s theory by overcoming its inability to take into account the computational capabilities of the decoder. Indeed, for two (not necessarily independent) random variables $_{x}$ and $z$ , and for any bijective mapping $\\gamma:\\mathcal{X}\\rightarrow\\mathcal{X}$ Shannon\u2019s mutual information remains unchanged: $\\mathcal{T}(\\mathbf{x},z)=\\mathcal{T}(\\gamma(\\mathbf{x}),z)$ . ", "page_idx": 4}, {"type": "text", "text": "Consider, for instance, $\\gamma$ as a cryptographic function that encrypts an image $_{x}$ using a bijective key-based algorithm (e.g., the AES encryption algorithm). If $_{x}$ represents the original image, and $\\gamma(\\pmb{x})$ represents the cipherimage, the mutual information between $_{x}$ and $z$ remains unchanged. This is because the encryption is a bijective process, and the information content is preserved. However, in practice, the encrypted images would be much harder to decode and use for training a model compared to the original one, without access to the decryption key. Another example we may think of is $\\gamma$ as a pixel shuffilng operation. The information carried by $_{x}$ does not disappear after processing by $\\gamma$ . However, it may be harder to extract in practice. ", "page_idx": 4}, {"type": "text", "text": "This demonstrates the practical importance of $\\nu$ -Information, as it considers the computational effort required to decode the information, highlighting the difference between theoretical and practical accessibility of information. Specifically, the $\\mathcal{V}$ -information proposes taking into account the computational constraint of the decoder by assuming it can only extract information using a predictive family $\\mathcal{V}\\subseteq\\mathfrak{F}=\\{\\eta:\\mathcal{X}\\cup\\{\\alpha\\}\\rightarrow\\dot{\\mathcal{P}}(\\mathcal{Z})\\}$ . The authors [115] then define the $\\nu$ -entropy and the $\\nu$ -conditional entropy as follo ws: ", "page_idx": 4}, {"type": "equation", "text": "$$\nH_{\\mathcal{V}}(z)=\\operatorname*{inf}_{\\eta\\in\\mathcal{V}}\\mathbb{E}_{P_{z}}(-\\log\\eta(\\mathcal{D};z)),~~~~~H_{\\mathcal{V}}(z|x)=\\operatorname*{inf}_{\\eta\\in\\mathcal{V}}\\mathbb{E}_{P}(-\\log\\eta(x;z)).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Where $\\eta(\\cdot;\\cdot)$ is a function from $\\mathcal{X}\\cup\\{\\mathcal{A}\\}\\rightarrow\\mathcal{P}(\\mathcal{Z})$ that returns a probability density $\\eta(\\mathbf{\\boldsymbol{x}};\\cdot)$ on $\\mathcal{Z}$ using side information $_{x}$ , or without side information $\\mathcal{X}$ . The predictive family $\\nu$ summarizes the computational capabilities of the decoder. When $\\nu$ cont a ins all possible functions, $\\mathcal{V}=\\mathfrak{F}$ , it recovers Shannon\u2019s entropy as a special case. Intuitively, we seek the best possible prediction for $z$ knowing $_{x}$ by maximizing the log-likelihood. Continuing, we naturally introduce the $\\mathcal{V}$ -information: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{Z}_{\\mathcal{V}}(\\mathbf{x}\\rightarrow z)=H_{\\mathcal{V}}(z)-H_{\\mathcal{V}}(z|\\mathbf{x}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The complexity of the mapping from $_{x}$ to $z$ can now be assessed by examining a hierarchy of predictive families $\\mathcal{V}_{1}\\subset...\\subset\\mathcal{V}_{n}$ of increasing expressiveness, like explored in [61]. Each predictive family $\\nu_{\\ell}$ corresponds to a partial forward up to depth $\\ell$ , followed by a decoding step. This involves determining at which point we can decode or make the information from $_{x}$ to $z$ available. Formally, we define the complexity of the feature as dependent of the cumulative $\\mathcal{V}$ -information across layers: ", "page_idx": 4}, {"type": "image", "img_path": "NhqZpst42I/tmp/232b9ac0630a4b1f56f32fc8e25b7756e255d832fb5b2c5e702137ea617e3e6b.jpg", "img_caption": ["Figure 3: Visualization of Meta-features, sorted by Complexity. We use Feature visualization [84, 31] to visualize the Meta-features found after concept extraction. The entire visualization for each Meta-feature can be found in Appendix B. "], "img_footnote": [], "page_idx": 5}, {"type": "equation", "text": "$$\nK(z,\\pmb{x})=1-\\frac{1}{n}\\sum_{\\ell}^{n}\\mathcal{Z}_{\\mathcal{V}}(\\pmb{f}_{\\ell}(\\pmb{x})\\rightarrow z).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Here, we define the predictive family $\\mathcal{V}$ as a class of linear probes with Gaussian prior. Under this hypothesis, the associated $\\mathcal{V}$ -information of this class possesses a closed-form solution (see Appendix C), which serves as the basis for our evaluation. A higher score implies that the feature $z$ is readily accessible and persists throughout the model\u2019s layers. Conversely, a lower score suggests that the feature $z$ is unveiled only at the very end of the model, if at all. ", "page_idx": 5}, {"type": "text", "text": "Assumption. Crucially, the correctness of the computation of $Z_{\\nu}(f_{\\ell}({\\pmb x})\\rightarrow z)$ relies on the hypothesis that each layer $f_{\\ell}$ provides the optimal representation for the downstream linear probe $\\eta$ . In other words, we assume that $\\mathcal{T}_{\\mathcal{V}}(\\boldsymbol{f}_{\\ell}(\\boldsymbol{\\mathsf{x}})\\bar{\\to}\\,z)=\\bar{\\mathcal{T}}_{\\mathcal{V}_{\\ell}}(\\boldsymbol{\\mathsf{x}}\\to z)$ , or again that $\\eta_{\\ell}^{*}=\\eta^{*}\\circ f_{\\ell}$ . This hypothesis is reasonable, since a neural network is essentially \u201clinearizing\u201d the training set\u2014projecting the training set into a space in which it is linearly separable. Thus, it makes sense to assume that each layer attempts to make the feature linearly decodable as efficiently as possible. If this condition is violated, the complexity measure may overestimate the true complexity of a feature (since we can only underestimate the $\\nu$ -information). For example, this may happen if the optimal path to calculate a feature requires deviating from the linear decoding to make it easier to decode later. While some recent works have motivated a slightly different complexity metric based on redundancy [24], we show in Appendix E that our complexity measure is inherently linked to redundancy. ", "page_idx": 5}, {"type": "text", "text": "3 What Do Complex Features Look Like? A Qualitative Analysis ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "This section presents a qualitative investigation of relatively simple versus more complex features. Drawing from critical insights of recent studies, which indicate a tendency of neural networks to prefer input features that are both predictive and not overly complex [48], this analysis aims to better understand the nature of features that are easily processed by models versus those that pose more significant challenges. Indeed, understanding the types of features that are too complex for our model can help us anticipate the types of shortcuts the model might rely on and, on the other hand, design methods to simplify the learning of complex features. This section of the manuscript is intentionally qualitative and aims to be exploratory. We applied our complexity metric to 10,000 features extracted from a fully trained ResNet50. For each feature, we computed the complexity score $K(z,x)$ using a subset of 20,000 images from the validation set. Recognizing the impracticality of manually examining each of the 10,000 features, we employed a strategy to aggregate these features into a more manageable number of groups that we called Meta-features. ", "page_idx": 5}, {"type": "text", "text": "Method for Aggregating Features into Meta-features. To condense the vast array of features into a reduced number of similar features, we applied ${\\bf K}$ -means clustering to the feature dictionary $\\mathbf{D}^{\\star}$ , resulting in 150 distinct clusters. These clusters represent collections of features, referred to as Meta-features $\\mathcal{C}=\\{v_{1},\\ldots,v_{|\\mathcal{C}|}\\}$ ; we then computed an average complexity score for each group. By selecting a diverse range of 30 clusters, chosen to cover a spectrum of complexity levels from the simplest to the most complex features, we aimed to provide a comprehensive overview of the diversity of feature complexity within the model. We propose to visualize the distance matrix in $\\mathbf{D}^{\\star}$ , showing feature complexity in Figure 2. This approach offers preliminary insights into features seen as simple or complex by the model. ", "page_idx": 6}, {"type": "text", "text": "Simple Features. Among the simpler features, we find elements primarily based on color, such as $s k y$ and sea, as well as simple pattern detectors like line detectors and low-frequency detectors exemplified by bokeh. Interestingly, features geared towards text detection, such as watermark, are also included in this group. These findings align with previous studies [117, 96, 12, 82], which have shown that neural networks tend to identify color and simple geometric patterns in the early layers as well as low-frequency detectors. This suggests that these features are relatively easy for neural networks to process and recognize. Furthermore, our findings detailed in Appendix $11$ corroborate the theoretical work posited in [11, 72]: robust learning possibly induces the learning of shortcuts or reliance on \u201ceasy\u201d features within the model. ", "page_idx": 6}, {"type": "text", "text": "Medium Complexity Features. Features with medium complexity reveal more nuanced and sometimes unexpected characteristics. We find, for example, low-quality detectors sensitive to lowresolution images. Additionally, a significant number of concepts related to human elements were observed despite the absence of a dedicated human class in ImageNet. Trademark-related features, distinct from simpler watermark detectors, also reside within this intermediate complexity bracket. ", "page_idx": 6}, {"type": "text", "text": "Complex Features. Among the most complex features, we find several Meta-features that exhibit a notable degree of structural coherence, including categories such as insect legs, curves, and ears. These patterns represent structured configurations that are ostensibly more challenging for models to process than more localized features, echoing the ongoing discussion about texture bias in current models [10, 37, 46]. Intriguingly, the most complex Meta-features identified, namely whiskers and insect legs, embody types of fliament-like structures. Interestingly, we note that those types of features are known to be challenging for current models to identify accurately [60], aligning with documented difficulties in path-tracking tasks [66]. Such tasks have revealed current models\u2019 limitations in tracing paths, which parallels challenges in connectomics [89], particularly in filament segmentation\u2014a domain recognized for its complexity within deep learning research. ", "page_idx": 6}, {"type": "text", "text": "Now that we\u2019ve browsed simple and complex features, another question arises: how does the model build these features during the forward pass? For instance, where within the model does the formation of a watermark detector feature occur? And for more complex features that require greater structure, in which block of computation are these features formed within the model? ", "page_idx": 6}, {"type": "text", "text": "4 Where do Complex Features Emerge ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "As suggested by previous work, simple features, like color detectors and low-frequency detectors, may already exist within the early layers of the model. An intriguing question arises: how does the model ensure the propagation of these features to the final latent space $\\pmb{f}_{n}$ , where features are extracted? A key component to consider in addressing this question is the role of residual connections within the ResNet [45] architecture. The formulation of a residual connection in ResNet blocks is mathematically represented as: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f_{\\ell+1}({\\boldsymbol{\\mathsf{x}}})=\\underbrace{f_{\\ell}({\\boldsymbol{\\mathsf{x}}})}_{\\mathrm{\\cdot\\cdotResidual\"branch}}+\\underbrace{(\\pmb{g}_{\\ell}\\circ f_{\\ell})({\\boldsymbol{\\mathsf{x}}})}_{\\mathrm{\\cdot\\cdotMain\"branch}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "This equation highlights two distinct paths: the \u201cResidual\u201d branch, which facilitates the direct ", "page_idx": 6}, {"type": "image", "img_path": "NhqZpst42I/tmp/93f1fc9999fb2388b18ace6e194996a37ce6b734219d47cdc078ad7c39d8d6e2.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 4: Simple Features Teleported by Residuals. (Left) CKA between residual branch activations $\\scriptstyle f_{\\ell}$ and final concept value $z$ . For simple concepts, beyond a certain layer (block 3), the residual already carries nearly all the information, effectively teleporting it to the last layer. (Right) Conversely, for complex features, both the main and residual branches gradually construct the features during the forward pass. ", "page_idx": 6}, {"type": "text", "text": "transfer of features from $\\scriptstyle f_{\\ell}$ to the subsequent layer $\\ell+1$ , and the \u201cMain\u201d branch, which in", "page_idx": 7}, {"type": "text", "text": "troduces additional transformations to $\\scriptstyle f_{\\ell}$ through additional computation $_{g_{\\ell}}$ to enhance its representational capacity. We aim to investigate the flow of simple and complex features through these branches. In our analysis, we examine two subsets of features: 100 features of the highest complexity (top-1 percentile) and 100 features of the lowest complexity (bottom-1 percentile). We measure the Centered Kernel Alignment (CKA) [57] between the final concept values $z$ and the activations from (A) the \u201cResidual\u201d branch $\\pmb{f}_{\\ell}$ , and (B) the \u201cMain\u201d branch $(\\pmb{g}_{\\ell}\\circ f_{\\ell})$ , at each residual block, as a proxy for concept information contained in each branch. The findings, illustrated in Figure 4, reveal that simple features are efficiently \u201cteleported\u201d to later layers through the residual branches \u2013 in other words, once computed, they are passed forward with little subsequent modification. In contrast, complex concepts are incrementally built up through an interactive process involving the \u201cmain\u201d and \u201cresidual\u201d branches. This understanding of feature evolution within network architectures emphasizes the importance of residual connections. This insight, though expected, clarifies a common conception by showing that simple features utilize the residual branch. The next step is to examine the temporal dynamics of feature development, specifically investigating when complex and simple concepts emerge during model training. ", "page_idx": 7}, {"type": "image", "img_path": "NhqZpst42I/tmp/2a1d9bf0ed560a2cfee4d4ca546cc64ca5a4f458fe4a5c99be3d718b33afe74f.jpg", "img_caption": ["Figure 5: A) Complex features emerge later in training. There is a strong correlation between the complexity of a feature and the requisite temporal span for its decoding. The temporal decoding score, $\\Lambda$ , is derived as the mean $\\nu$ -information across epochs, with $\\nu$ representing the class encompassing linear models. A low score indicates a feature is accessible earlier during the training continuum, whereas a high score implies its tardy availability. The correlation between these scores suggests that complex features tend to emerge later in training. B) Important features are being compressed by the neural network: Levin Machine hypothesis. The average complexity of 10,000 features extracted independently at each epoch increases rapidly before stabilizing (the black curve shows the average). However, among the top- $1\\%$ of features in terms of importance, complexity decreases over time, as if the model is self-compressing or simplifying, akin to a sedimentation process. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "5 When do Complex Features Arise ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Figure 1 raises an important question: Does the complexity of a feature influence the time it takes to develop during training? To explore this, we refer to the 10,000 features extracted at the final epoch of our model as $\\pmb{f}_{n}^{(e)}$ , and we use $\\pmb{f}_{n}^{(i)}$ to represent the penultimate layer of the model at any given epoch $i$ , where $i\\in\\{1,\\ldots,e\\}$ and $e$ represents the total number of epochs. We aim to determine how early each feature can be detected in previous epochs $\\pmb{f}_{n}^{(i)}$ for $i<e$ . This involves calculating a specific decoding score; in our scenario, we define this score as $\\mathcal{T}_{\\mathcal{V}}$ \u2014the measure of $\\nu$ -information between the model\u2019s penultimate activations across epochs and an ultimate feature values, where $\\nu$ is the set of linear models. This metric helps us assess whether a feature was \u201creadily available\u201d at a certain epoch $i$ . The cumulative score $\\Lambda$ is calculated by averaging this measure across all epochs, leading to our score: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\Lambda({\\bf x},z)=1-\\frac{1}{e}\\sum_{i}^{e}\\mathcal{Z}_{\\nu}(f_{n}^{(i)}({\\bf x})\\rightarrow z).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The results, as illustrated in Figure 5A, showcase the complexity of a feature $(K)$ with it\u2019s Time to Decode $(\\Lambda)$ score. An observed correlation coefficient nearing 0.5 intimates that features of heightened complexity are generally decoded later during the training epoch. This finding suggests a nuanced interrelation between the layer for which a feature is available and the epoch of discovery: a feature decoded later in the forward pass trajectory also came online later in training. This naturally leads us to the question of the dynamics of model training. Can we get a deeper understanding of how precisely complex concepts are formed within the model? Does the model develop complex features solely upon necessity, thereby suggesting a correlation between the complexity of a feature and its importance? ", "page_idx": 8}, {"type": "text", "text": "6 Complexity and Importance: A Subtle Tango ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Numerous studies have proposed hypotheses regarding the relationship between the importance and complexity of features within neural networks. A particularly notable hypothesis is the simplicity bias [3, 51, 110], which suggests that models leverage simpler features more frequently. This section aims to quantitatively validate these claims using our complexity metric paired with the importance of each feature. Because features are extracted from the penultimate layer, a closed-form relationship between features and logits can be derived due to the linear nature of this relationship. By analyzing this relationship over training for features of different complexity, we identify a surprising novel perspective: models appear to reduce the complexity of their important features. This process is analogous to sedimentation and mirrors the operation of a Levin Universal Search [63]. The model incrementally shifts significant features to earlier layers, taking time to identify simpler algorithms in the process. ", "page_idx": 8}, {"type": "image", "img_path": "NhqZpst42I/tmp/219b473ae4bd94c7cef88868231cfcf06275cd10e9efb894b75980cc01e37fd3.jpg", "img_caption": ["Figure 6: Simplicity bias appears during training. Complexity vs. Importance of 10,000 features extracted from a ResNet50 at Epochs 1 and 90 of training. In Epoch 1, important features are not necessarily simple and seem uniformly distributed. In contrast, by the end of training, there is a clear simplicity bias, consistent with numerous studies: the model prefers to rely on simpler features. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Importance Measure. The feature extraction framework outlined in Section 2 offers a structured approach to estimating the importance of a feature within the network. Specifically, the feature vector $z\\in\\mathbb{R}^{k}$ is linearly related to the model\u2019s decision-making process, exemplified by a logit calculation $\\mathbf{\\tau}_{\\mathbb{Y}}=z\\mathbf{D}^{\\star}W\\in\\mathbb{R}$ , where $W\\in\\mathbb{R}^{|A_{n}|}$ represents the weights of the penultimate layer for the class-specific logit. The contribution of the $i$ -th feature, $z_{i}$ , to the logit $\\mathfrak{y}$ can be precisely measured by leveraging the gradient-input formulation, which is optimal for fidelity metrics within a linear context [5, 32]. This optimality and the closed-form expression are feasible primarily because the analysis is confined to the penultimate layer of the network. Formally, the importance of a feature $z_{i}$ is defined as: $\\begin{array}{r}{\\mathbf{T}(z_{i})=\\mathbb{E}_{\\mathbb{P}_{z}}\\bigg(||\\frac{\\partial\\mathbf{y}}{\\partial z_{i}}\\cdot z_{i}||\\bigg)}\\end{array}$ . In essence, the importance measure $\\Gamma(z_{i})$ quantifies the average contribution of the $i$ -th feature to the class-specific logit \u2013 essentially, the average score that each feature brings to the decision logit. More details on importance measures and the effect of inhibition features are available in Appendix G. ", "page_idx": 8}, {"type": "text", "text": "Models Prefer Simple Features. The analysis, supported by Figure 6 (right), demonstrates a clear trend indicating the model\u2019s simplicity bias. Among the 10,000 features extracted in the final epoch, more complex features\u2014characterized by higher $K(\\mathbf{x},z)$ values\u2014are generally assigned lower importance $(\\Gamma(z))$ . In contrast, simpler features predominantly influence the model\u2019s decisions. The plot on the left showcases the complexity and importance of 10,000 concepts extracted at the end of the first epoch; we observe that the model does not exhibit this simplicity bias at the end of the first epoch. More detail and study on the role of complex concept is proposed in Appendix D. This observation raises the question of the dynamic interplay between feature complexity and importance. To further investigate, we did a detailed analysis of the evolution of feature complexity and importance throughout the training process. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Model as Levin\u2019s Machine: Simplifying the Complexity of Important Features. A closer examination of the evolution of feature importance over time reveals an interesting phenomenon in Figure 5B: the emergence of two distinct phases during training. Initially, there is a global increase in feature complexity, with the model beginning its training with relatively simple features. Surprisingly, this is followed by a phase where the model actively reduces its overall complexity, specifically targeting and simplifying its most important features. The model appears to be \u201cshortening\u201d the computational \u201cprograms\u201d responsible for generating these significant features. This observation suggests that the ResNet50 under study, like a Levin Machine, develops simpler computational paths for crucial features. Put simply, our complexity metric shows that important features are extracted at earlier layers, resembling sedimentation with foundational elements near the network\u2019s input. ", "page_idx": 9}, {"type": "text", "text": "This behavior presents a novel perspective on how neural networks might be intrinsically driven to generalize by simplifying the computation graph of their important features. However, at least at the early stages of learning, it also challenges our assumption that each layer is optimized to provide a linearly-separable representation for the downstream linear probe \u2013 early in learning, this assumption is clearly violated since some complex features could be represented more simply than they are initially. Thus, future work will be needed to fully disentangle the interaction of complexity and importance over training. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We introduced a complexity metric for neural network features, identifying both simple and complex types. We have shown where simple features flow \u2013 through residual connections \u2013 as opposed to complex ones that develop via collaboration with main branches. Our study further revealed that complex features are learned later in training than simple ones. We have concluded by exploring the relationship between feature complexity and importance, and discovered that the simplicity bias found in neural networks becomes more pronounced as training progresses. Surprisingly, we found that important features simplify over time, suggesting a sedimentation process within neural networks that compresses important features to be accessible earlier in the network. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We thank Amal Rannen-Triki and Noah Fiedel for feedback on the manuscript, and Amal, Robert Geirhos, Olivia Wiles, and Mike Mozer for interesting discussions. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Achtibat, R., Dreyer, M., Eisenbraun, I., Bosse, S., Wiegand, T., Samek, W., and Lapuschkin, S. (2023). From attribution maps to human-understandable explanations through concept relevance propagation. Nature Machine Intelligence. [2] Adebayo, J., Gilmer, J., Muelly, M., Goodfellow, I., Hardt, M., and Kim, B. (2018). Sanity checks for saliency maps. In Advances in Neural Information Processing Systems (NIPS). [3] Advani, M. S., Saxe, A. M., and Sompolinsky, H. (2020). High-dimensional dynamics of generalization error in neural networks. Neural Networks, 132:428\u2013446. [4] Allender, E. (1992). Applications of time-bounded kolmogorov complexity in complexity theory. In Kolmogorov complexity and computational complexity, pages 4\u201322. Springer. ", "page_idx": 9}, {"type": "text", "text": "[5] Ancona, M., Ceolini, E., \u00d6ztireli, C., and Gross, M. (2018). Towards better understanding of gradient-based attribution methods for deep neural networks. In Proceedings of the International Conference on Learning Representations (ICLR).   \n[6] Arjovsky, M., Bottou, L., Gulrajani, I., and Lopez-Paz, D. (2019). Invariant risk minimization. arXiv preprint arXiv:1907.02893.   \n[7] Arora, S., Cohen, N., Hu, W., and Luo, Y. (2019). Implicit regularization in deep matrix factorization. Advances in Neural Information Processing Systems, 32.   \n[8] Arora, S., Li, Y., Liang, Y., Ma, T., and Risteski, A. (2018). Linear algebraic structure of word senses, with applications to polysemy. Transactions of the Association for Computational Linguistics, 6:483\u2013495.   \n[9] Bach, S., Binder, A., Montavon, G., Klauschen, F., M\u00fcller, K.-R., and Samek, W. (2015). On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation. Public Library of Science (PloS One).   \n[10] Baker, N., Lu, H., Erlikhman, G., and Kellman, P. J. (2018). Deep convolutional networks do not classify based on global object shape. PLoS computational biology, 14(12):e1006613.   \n[11] Banerjee, D., Singh, A., and Singh, G. (2023). Dissecting neural network robustness proofs. In The Twelfth International Conference on Learning Representations.   \n[12] Bau, D., Zhou, B., Khosla, A., Oliva, A., and Torralba, A. (2017). Network dissection: Quantifying interpretability of deep visual representations. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).   \n[13] Bauwens, B., Makhlin, A., Vereshchagin, N., and Zimand, M. (2018). Short lists with short programs in short time. computational complexity, 27:31\u201361.   \n[14] Beyer, L., Izmailov, P., Kolesnikov, A., Caron, M., Kornblith, S., Zhai, X., Minderer, M., Tschannen, M., Alabdulmohsin, I., and Pavetic, F. (2022). Flexivit: One model for all patch sizes. arXiv preprint arXiv:2212.08013.   \n[15] Bhatt, U., Weller, A., and Moura, J. M. F. (2020). Evaluating and aggregating feature-based model explanations. In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI).   \n[16] Blier, L. and Ollivier, Y. (2018). The description length of deep learning models. Advances in Neural Information Processing Systems, 31.   \n[17] Bouniot, Q., Redko, I., Mallasto, A., Laclau, C., Arndt, K., Struckmeier, O., Heinonen, M., Kyrki, V., and Kaski, S. (2023). Understanding deep neural networks through the lens of their non-linearity. arXiv preprint arXiv:2310.11439.   \n[18] Bricken, T., Templeton, A., Batson, J., Chen, B., Jermyn, A., Conerly, T., Turner, N., Anil, C., Denison, C., Askell, A., et al. (2023a). Towards monosemanticity: Decomposing language models with dictionary learning. Transformer Circuits Thread, 2.   \n[19] Bricken, T., Templeton, A., Batson, J., Chen, B., Jermyn, A., Conerly, T., Turner, N., Anil, C., Denison, C., Askell, A., Lasenby, R., Wu, Y., Kravec, S., Schiefer, N., Maxwell, T., Joseph, N., Hatfield-Dodds, Z., Tamkin, A., Nguyen, K., McLean, B., Burke, J. E., Hume, T., Carter, S., Henighan, T., and Olah, C. (2023b). Towards monosemanticity: Decomposing language models with dictionary learning. Transformer Circuits Thread. https://transformercircuits.pub/2023/monosemantic-features/index.html.   \n[20] Cantor, G. (1890). Ueber eine elementare frage der mannigfaltigketislehre. Jahresbericht der Deutschen Mathematiker-Vereinigung, 1:72\u201378.   \n[21] Chaitin, G. J. (1969). On the length of programs for computing finite binary sequences: statistical considerations. Journal of the ACM (JACM), pages 145\u2013159.   \n[22] Chaitin, G. J. (1977). Algorithmic information theory. IBM journal of research and development. ", "page_idx": 10}, {"type": "text", "text": "[23] Chattopadhay, A., Sarkar, A., Howlader, P., and Balasubramanian, V. N. (2018). Grad-cam $^{++}$ : Generalized gradient-based visual explanations for deep convolutional networks. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV). ", "page_idx": 11}, {"type": "text", "text": "[24] Chen, Y., Zhou, Z., and Yan, J. (2024). Going beyond neural network feature similarity: The network feature complexity and its interpretation using category theory. Proceedings of the International Conference on Learning Representations (ICLR). ", "page_idx": 11}, {"type": "text", "text": "[25] Cheung, B., Terekhov, A., Chen, Y., Agrawal, P., and Olshausen, B. (2019). Superposition of many models into one. Advances in Neural Information Processing Systems (NeurIPS). ", "page_idx": 11}, {"type": "text", "text": "[26] Chollet, F. et al. (2015). Keras. https://keras.io. ", "page_idx": 11}, {"type": "text", "text": "[27] Colin, J., Fel, T., Cad\u00e8ne, R., and Serre, T. (2021). What i cannot predict, i do not understand: A human-centered evaluation framework for explainability methods. Advances in Neural Information Processing Systems (NeurIPS). ", "page_idx": 11}, {"type": "text", "text": "[28] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. (2020). An image is worth 16x16 words: Transformers for image recognition at scale. In Proceedings of the International Conference on Learning Representations (ICLR). ", "page_idx": 11}, {"type": "text", "text": "[29] Elhage, N., Hume, T., Olsson, C., Schiefer, N., Henighan, T., Kravec, S., Hatfield-Dodds, Z., Lasenby, R., Drain, D., Chen, C., Grosse, R., McCandlish, S., Kaplan, J., Amodei, D., Wattenberg, M., and Olah, C. (2022). Toy models of superposition. Transformer Circuits Thread. ", "page_idx": 11}, {"type": "text", "text": "[30] Fel, T., Agustin, P., Louis, B., Thibaut, B., David, V., Julien, C., R\u00e9mi, C., and Thomas, S. (2023a). Craft: Concept recursive activation factorization for explainability. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). ", "page_idx": 11}, {"type": "text", "text": "[31] Fel, T., Boissin, T., Boutin, V., Picard, A., Novello, P., Colin, J., Linsley, D., Rousseau, T., Cad\u00e8ne, R., and Laurent Gardes, T. S. (2023b). Unlocking feature visualization for deeper networks with magnitude constrained optimization. ", "page_idx": 11}, {"type": "text", "text": "[32] Fel, T., Boutin, V., Moayeri, M., Cad\u00e8ne, R., Bethune, L., Chalvidal, M., Serre, T., et al. (2023c). A holistic approach to unifying automatic concept extraction and concept importance estimation. ", "page_idx": 11}, {"type": "text", "text": "[33] Fong, R. C. and Vedaldi, A. (2017). Interpretable explanations of black boxes by meaningful perturbation. In Proceedings of the IEEE International Conference on Computer Vision (ICCV). ", "page_idx": 11}, {"type": "text", "text": "[34] Foret, P., Kleiner, A., Mobahi, H., and Neyshabur, B. (2020). Sharpness-aware minimization for efficiently improving generalization. arXiv preprint arXiv:2010.01412. ", "page_idx": 11}, {"type": "text", "text": "[35] Gao, L., la Tour, T. D., Tillman, H., Goh, G., Troll, R., Radford, A., Sutskever, I., Leike, J., and Wu, J. (2024). Scaling and evaluating sparse autoencoders. arXiv preprint arXiv:2406.04093. ", "page_idx": 11}, {"type": "text", "text": "[36] Geirhos, R., Jacobsen, J.-H., Michaelis, C., Zemel, R., Brendel, W., Bethge, M., and Wichmann, F. A. (2020). Shortcut learning in deep neural networks. Nature Machine Intelligence. ", "page_idx": 11}, {"type": "text", "text": "[37] Geirhos, R., Rubisch, P., Michaelis, C., Bethge, M., Wichmann, F. A., and Brendel, W. (2019). Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness. Proceedings of the International Conference on Learning Representations (ICLR). ", "page_idx": 11}, {"type": "text", "text": "[38] Ghorbani, A., Abid, A., and Zou, J. (2017). Interpretation of neural networks is fragile. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI). ", "page_idx": 11}, {"type": "text", "text": "[39] Ghorbani, A., Wexler, J., Zou, J. Y., and Kim, B. (2019). Towards automatic concept-based explanations. In Advances in Neural Information Processing Systems (NeurIPS). ", "page_idx": 11}, {"type": "text", "text": "[40] Goldblum, M., Finzi, M., Rowan, K., and Wilson, A. G. (2023). The no free lunch theorem, kolmogorov complexity, and the role of inductive biases in machine learning. arXiv preprint arXiv:2304.05366. ", "page_idx": 11}, {"type": "text", "text": "[41] Grabska-Barwinska, A., Rannen-Triki, A., Rivasplata, O., and Gy\u00f6rgy, A. (2021). Towards better visual explanations for deep image classifiers. In eXplainable AI approaches for debugging and diagnosis.   \n[42] Graziani, M., Nguyen, A.-p., O\u2019Mahony, L., M\u00fcller, H., and Andrearczyk, V. (2023). Concept discovery and dataset exploration with singular value decomposition. In WorkshopProceedings of the International Conference on Learning Representations (ICLR).   \n[43] Gr\u00fcnwald, P. D., Vit\u00e1nyi, P. M., et al. (2008). Algorithmic information theory. Handbook of the Philosophy of Information.   \n[44] Hamblin, C., Fel, T., Saha, S., Konkle, T., and Alvarez, G. (2024). Feature accentuation: Revealing\u2019what\u2019features respond to in natural images. arXiv preprint arXiv:2402.10039.   \n[45] He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).   \n[46] Hermann, K., Chen, T., and Kornblith, S. (2020). The origins and prevalence of texture bias in convolutional neural networks. Advances in Neural Information Processing Systems (NeurIPS).   \n[47] Hermann, K. and Lampinen, A. (2020). What shapes feature representations? exploring datasets, architectures, and training. Advances in Neural Information Processing Systems (NeurIPS).   \n[48] Hermann, K. L., Mobahi, H., Fel, T., and Mozer, M. C. (2023). On the foundations of shortcut learning. In International Conference on Learning Representations.   \n[49] Hochreiter, S. and Schmidhuber, J. (1994). Simplifying neural nets by discovering flat minima. Advances in neural information processing systems, 7.   \n[50] Hooker, S., Courville, A., Clark, G., Dauphin, Y., and Frome, A. (2019). What do compressed deep neural networks forget? arXiv preprint arXiv:1911.05248.   \n[51] Huh, M., Mobahi, H., Zhang, R., Cheung, B., Agrawal, P., and Isola, P. (2021). The low-rank simplicity bias in deep networks. arXiv preprint arXiv:2103.10427.   \n[52] Humayun, A. I., Balestriero, R., and Baraniuk, R. (2024). Deep networks always grok and here is why. arXiv preprint arXiv:2402.15555.   \n[53] Kim, B., Wattenberg, M., Gilmer, J., Cai, C., Wexler, J., Viegas, F., et al. (2018). Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav). In International conference on machine learning. Proceedings of the International Conference on Machine Learning (ICML).   \n[54] Kim, S. S. Y., Meister, N., Ramaswamy, V. V., Fong, R., and Russakovsky, O. (2022). HIVE: Evaluating the human interpretability of visual explanations. In Proceedings of the IEEE European Conference on Computer Vision (ECCV).   \n[55] Kolmogorov, A. N. (1963). On tables of random numbers. Sankhy\u00afa: The Indian Journal of Statistics, Series A, pages 369\u2013376.   \n[56] Kolmogorov, A. N. (1965). Three approaches to the quantitative definition of information. Problems of information transmission.   \n[57] Kornblith, S., Norouzi, M., Lee, H., and Hinton, G. (2019). Similarity of neural network representations revisited. In International conference on machine learning, pages 3519\u20133529. PMLR.   \n[58] Lampinen, A. K., Chan, S. C., and Hermann, K. (2024). Learned feature representations are biased by complexity, learning order, position, and more. arXiv preprint arXiv:2405.05847.   \n[59] Lee, D. D. and Seung, H. S. (1999). Learning the parts of objects by non-negative matrix factorization. nature, 401(6755):788\u2013791.   \n[60] Lee, K., Zung, J., Li, P., Jain, V., and Seung, H. S. (2017). Superhuman accuracy on the snemi3d connectomics challenge. arXiv preprint arXiv:1706.00120.   \n[61] Lee, Y., Finn, C., and Ermon, S. (2022). Relaxing the kolmogorov structure function for realistic computational constraints. In NeurIPS 2022 Workshop on Information-Theoretic Principles in Cognitive Systems.   \n[62] Lepori, M. A., Pavlick, E., and Serre, T. (2023). Neurosurgeon: A toolkit for subnetwork analysis. arXiv preprint arXiv:2309.00244.   \n[63] Levin, L. A. (1973). Universal sequential search problems. Problemy peredachi informatsii, 9(3):115\u2013116.   \n[64] Levin, L. A. (1984). Randomness conservation inequalities; information and independence in mathematical theories. Information and Control, 61(1):15\u201337.   \n[65] Li, M., Vit\u00e1nyi, P., et al. (2008). An introduction to Kolmogorov complexity and its applications, volume 3. Springer.   \n[66] Linsley, D., Malik, G., Kim, J., Govindarajan, L. N., Mingolla, E., and Serre, T. (2021). Tracking without re-recognition in humans and machines. Advances in Neural Information Processing Systems (NeurIPS).   \n[67] Liu, Z., Mao, H., Wu, C.-Y., Feichtenhofer, C., Darrell, T., and Xie, S. (2022). A convnet for the 2020s. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).   \n[68] Lu, Z. and Oliveira, I. C. (2022). Theory and applications of probabilistic kolmogorov complexity. arXiv preprint arXiv:2205.14718.   \n[69] Malhotra, G., Evans, B. D., and Bowers, J. S. (2020). Hiding a plane with a pixel: examining shape-bias in cnns and the benefti of building in biological constraints. Vision Research, 174:57\u201368.   \n[70] McCarthy, J., Minsky, M. L., Rochester, N., and Shannon, C. E. (1956). A proposal for the dartmouth summer research project on artificial intelligence, august 31, 1955.   \n[71] McCoy, R. T., Pavlick, E., and Linzen, T. (2019). Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. arXiv preprint arXiv:1902.01007.   \n[72] Moayeri, M., Banihashem, K., and Feizi, S. (2022a). Explicit tradeoffs between adversarial and natural distributional robustness. In Advances in Neural Information Processing Systems (NeurIPS).   \n[73] Moayeri, M., Pope, P., Balaji, Y., and Feizi, S. (2022b). A comprehensive study of image classification model sensitivity to foregrounds, backgrounds, and visual attributes. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).   \n[74] Moayeri, M., Singla, S., and Feizi, S. (2022c). Hard imagenet: Segmentations for objects with strong spurious cues. In Advances in Neural Information Processing Systems (NeurIPS).   \n[75] Nakkiran, P. (2021). Turing-universal learners with optimal scaling laws. arXiv preprint arXiv:2111.05321.   \n[76] Nanda, V., Speicher, T., Dickerson, J., Gummadi, K., Feizi, S., and Weller, A. (2024). Diffused redundancy in pre-trained representations. Advances in Neural Information Processing Systems, 36.   \n[77] Nguyen, A., Yosinski, J., and Clune, J. (2016). Multifaceted feature visualization: Uncovering the different types of features learned by each neuron in deep neural networks. Visualization for Deep Learning workshop, Proceedings of the International Conference on Machine Learning (ICML).   \n[78] Nguyen, A., Yosinski, J., and Clune, J. (2019). Understanding neural networks via feature visualization: A survey. arXiv preprint arXiv:1904.08939.   \n[79] Nguyen, G., Kim, D., and Nguyen, A. (2021). The effectiveness of feature attribution methods and its correlation with automatic evaluation scores. Advances in Neural Information Processing Systems (NeurIPS).   \n[80] Novello, P., Fel, T., and Vigouroux, D. (2022). Making sense of dependence: Efficient black-box explanations using dependence measure. In Advances in Neural Information Processing Systems (NeurIPS).   \n[81] Okawa, M., Lubana, E. S., Dick, R., and Tanaka, H. (2024). Compositional abilities emerge multiplicatively: Exploring diffusion models on a synthetic task. Advances in Neural Information Processing Systems (NeurIPS).   \n[82] Olah, C., Cammarata, N., Schubert, L., Goh, G., Petrov, M., and Carter, S. (2020a). An overview of early vision in inceptionv1. Distill.   \n[83] Olah, C., Cammarata, N., Schubert, L., Goh, G., Petrov, M., and Carter, S. (2020b). Zoom in: An introduction to circuits. Distill. https://distill.pub/2020/circuits/zoom-in.   \n[84] Olah, C., Mordvintsev, A., and Schubert, L. (2017). Feature visualization. Distill.   \n[85] Park, C. F., Okawa, M., Lee, A., Lubana, E. S., and Tanaka, H. (2024). Emergence of hidden capabilities: Exploring learning dynamics in concept space. arXiv preprint arXiv:2406.19370.   \n[86] Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., et al. (2011). Scikit-learn: Machine learning in python. the Journal of machine Learning research, 12:2825\u20132830.   \n[87] P\u00e9rez, G. V., Camargo, C. Q., and Louis, A. A. (2018). Deep learning generalizes because the parameter-function map is biased towards simple functions. stat, 1050:23.   \n[88] Petsiuk, V., Das, A., and Saenko, K. (2018). Rise: Randomized input sampling for explanation of black-box models. In Proceedings of the British Machine Vision Conference (BMVC).   \n[89] Plaza, S. M. and Funke, J. (2018). Analyzing image segmentation for connectomics. Frontiers in neural circuits.   \n[90] Raghu, M., Unterthiner, T., Kornblith, S., Zhang, C., and Dosovitskiy, A. (2021). Do vision transformers see like convolutional neural networks? Advances in neural information processing systems.   \n[91] Rahaman, N., Baratin, A., Arpit, D., Draxler, F., Lin, M., Hamprecht, F., Bengio, Y., and Courville, A. (2019). On the spectral bias of neural networks. In International conference on machine learning. PMLR.   \n[92] Rajamanoharan, S., Lieberum, T., Sonnerat, N., Conmy, A., Varma, V., Kram\u00e1r, J., and Nanda, N. (2024). Jumping ahead: Improving reconstruction fidelity with jumprelu sparse autoencoders. arXiv preprint arXiv:2407.14435.   \n[93] Rannen-Triki, A., Berman, M., Kolmogorov, V., and Blaschko, M. B. (2019). Function norms for neural networks. In Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops.   \n[94] Schmidhuber, J. (1995). Discovering solutions with low kolmogorov complexity and high generalization capability. In Machine Learning Proceedings 1995, pages 488\u2013496. Elsevier.   \n[95] Schmidhuber, J. (1997). Discovering neural nets with low kolmogorov complexity and high generalization capability. Neural Networks, 10(5):857\u2013873.   \n[96] Schubert, L., Voss, C., Cammarata, N., Goh, G., and Olah, C. (2021). High-low frequency detectors. Distill. https://distill.pub/2020/circuits/frequency-edges.   \n[97] Shah, H., Tamuly, K., Raghunathan, A., Jain, P., and Netrapalli, P. (2020). The pitfalls of simplicity bias in neural networks. Advances in Neural Information Processing Systems (NeurIPS), 33.   \n[98] Shen, A., Uspensky, V. A., and Vereshchagin, N. (2022). Kolmogorov complexity and algorithmic randomness, volume 220. American Mathematical Society.   \n[99] Simonyan, K., Vedaldi, A., and Zisserman, A. (2013). Deep inside convolutional networks: Visualising image classification models and saliency maps. In Workshop, Proceedings of the International Conference on Learning Representations (ICLR).   \n[100] Singla, S. and Feizi, S. (2022). Salient imagenet: How to discover spurious features in deep learning? In Proceedings of the International Conference on Learning Representations (ICLR).   \n[101] Slack, D., Hilgard, A., Lakkaraju, H., and Singh, S. (2021). Counterfactual explanations can be manipulated. Advances in Neural Information Processing Systems (NeurIPS).   \n[102] Smilkov, D., Thorat, N., Kim, B., Vi\u00e9gas, F., and Wattenberg, M. (2017). Smoothgrad: removing noise by adding noise. In Workshop on Visualization for Deep Learning, Proceedings of the International Conference on Machine Learning (ICML).   \n[103] Solomonoff, R. J. (1964). A formal theory of inductive inference. part i. Information and control.   \n[104] Solomonoff, R. J. (2009). Algorithmic probability: Theory and applications. Information theory and statistical learning, pages 1\u201323.   \n[105] Springer, J. M., Nagarajan, V., and Raghunathan, A. (2024). Sharpness-aware minimization enhances feature quality via balanced learning. In The Twelfth International Conference on Learning Representations.   \n[106] Sundararajan, M., Taly, A., and Yan, Q. (2017). Axiomatic attribution for deep networks. In Proceedings of the International Conference on Machine Learning (ICML).   \n[107] Tachet, R., Pezeshki, M., Shabanian, S., Courville, A., and Bengio, Y. (2018). On the learning dynamics of deep neural networks. arXiv preprint arXiv:1809.06848.   \n[108] Teutsch, J. (2014). Short lists for shortest descriptions in short time. computational complexity, 23:565\u2013583.   \n[109] Trockman, A. and Kolter, J. Z. (2023). Patches are all you need? Transactions on Machine Learning Research.   \n[110] Valle-Perez, G., Camargo, C. Q., and Louis, A. A. (2018). Deep learning generalizes because the parameter-function map is biased towards simple functions. In International Conference on Learning Representations.   \n[111] Vasudeva, B., Fu, D., Zhou, T., Kau, E., Huang, Y., and Sharan, V. (2024). Simplicity bias of transformers to learn low sensitivity functions. arXiv preprint arXiv:2403.06925.   \n[112] Vielhaben, J., Bl\u00fccher, S., and Strodthoff, N. (2023). Multi-dimensional concept discovery (mcd): A unifying framework with completeness guarantees.   \n[113] Wang, Y.-X. and Zhang, Y.-J. (2012). Nonnegative matrix factorization: A comprehensive review. IEEE Transactions on knowledge and data engineering, 25(6):1336\u20131353.   \n[114] Wightman, R., Touvron, H., and J\u00e9gou, H. (2021). Resnet strikes back: An improved training procedure in timm. arXiv preprint arXiv:2110.00476.   \n[115] Xu, Y., Zhao, S., Song, J., Stewart, R., and Ermon, S. (2019). A theory of usable information under computational constraints. In International Conference on Learning Representations.   \n[116] Yuan, L., Chen, Y., Wang, T., Yu, W., Shi, Y., Jiang, Z.-H., Tay, F. E., Feng, J., and Yan, S. (2021). Tokens-to-token vit: Training vision transformers from scratch on imagenet. In Proceedings of the IEEE/CVF international conference on computer vision.   \n[117] Zeiler, M. D. and Fergus, R. (2014). Visualizing and understanding convolutional networks. In Proceedings of the IEEE European Conference on Computer Vision (ECCV).   \n[118] Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O. (2021a). Understanding deep learning (still) requires rethinking generalization. Communications of the ACM, 64(3):107\u2013115.   \n[119] Zhang, R., Madumal, P., Miller, T., Ehinger, K. A., and Rubinstein, B. I. (2021b). Invertible concept-based explanations for cnn models with non-negative concept activation vectors. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI). ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "A Feature Extraction ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Dictionary Learning. To comprehensively analyze the complexity of features extracted from a deep learning model, we employed a detailed feature extraction process using dictionary learning, specifically utilizing an over-complete dictionary. This approach allows each activation $\\pmb{f}_{n}(\\pmb{x})\\in\\mathcal{A}_{\\ell}$ to be expressed as a linear combination of multiple basis elements (direction, also called atoms) $\\pmb{d}\\in\\mathcal{A}_{\\ell}$ from the dictionary $\\mathbf{D}^{\\star}\\,=\\,\\{\\pmb{d}_{1},\\dots,\\pmb{d}_{k}\\}$ coupled with some sparse coefficient $z\\,\\in\\,\\mathbb{R}^{k}$ associated to each atoms. ", "page_idx": 16}, {"type": "text", "text": "The over-completness of $\\mathbf{D}^{\\star}$ means that the dimension of the dictionnary $(k)$ is larger than the dimension of the activations space $k>>|\\mathcal{A}_{\\ell}|$ . This property allow us to overcome the superposition problem [29] essentially stating that there be more feature than neurons. ", "page_idx": 16}, {"type": "text", "text": "Mathematically, given an activation function ${\\pmb f}_{n}(x)$ , it can be represented as a linear combination of atoms from the dictionary $\\mathbf{D}$ , expressed as: ", "page_idx": 16}, {"type": "equation", "text": "$$\n{\\displaystyle{\\pmb f}_{n}(\\mathbf{x})\\approx z\\mathbf{D}^{\\star}=\\sum_{i}^{k}z_{i}\\pmb{d}_{i}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $z_{i}$ are the coefficients indicating the contribution of each atom $\\pmb{d}_{i}$ from the dictionary. ", "page_idx": 16}, {"type": "text", "text": "Implementation. Our implementation was inspired by Craft [30], leveraging the properties of ReLU activations in ResNet50. Given that ReLUs induce non-negativity of the activation, we employed Non-Negative Matrix Factorization (NMF) [59, 113] for the reconstruction, as it naturally aligns with the sparsity and non-negativity constraints of ReLU activations. Unlike PCA, which cannot produce overcomplete dictionaries and may result in non-positive activations, NMF can create overcomplete dictionaries in this context. ", "page_idx": 16}, {"type": "text", "text": "The dictionary $\\mathbf{D}^{\\star}$ was trained to reconstruct the activations ${\\pmb f}_{n}({\\pmb x})$ using the entire ImageNet training dataset, comprising 1.2 million images. Formally, for the set of images $\\mathbf{\\deltaX}$ and their corresponding activations ${f_{n}(X)}$ , the objective was to minimize the reconstruction error: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\|f_{n}(\\boldsymbol{X})-Z\\mathbf{D}^{\\star}\\|_{F},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "ensuring that ${f_{n}(X)}$ can be closely approximated by $Z\\mathbf{D}^{\\star}$ . Additionally, the NMF framework enforces non-negativity constraints on the dictionary matrix $\\mathbf{D}^{\\star}\\geq0$ and the coefficients $Z\\ge0$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\n(Z,{\\mathbf D}^{\\star})=\\operatorname*{\\arg\\operatorname*{min}}_{Z\\geq0,{\\mathbf D}^{\\star}\\geq0}||f_{n}(X)-Z{\\mathbf D}^{\\star}||_{F}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The dictionary $\\mathbf{D}^{\\star}$ was designed to encapsulate 10 concepts per class, resulting in a total of 10,000 concepts. To augment the training samples for NMF, we exploited the spatial dimensions of the last layer of ResNet50, which has 2048 channels with a spatial resolution of $7\\mathrm{x}7$ . By training the NMF independently on each of the 49 spatial dimensions, we effectively increased the number of training samples to approximately 58 million artificial samples (channel activations). ", "page_idx": 16}, {"type": "text", "text": "We utilized the block coordinate descent solver from Scikit-learn [86] to solve the NMF problem. This algorithm decomposes the problem into smaller subproblems, making it more tractable. The optimization process continued until convergence was achieved with a tolerance of $\\varepsilon\\:=\\:10^{-4}$ , ensuring the dictionary was sufficiently optimized for accurate feature extraction. Post-training, the reconstructed activations $Z\\mathbf{D}^{\\star}$ retained over $99\\%$ accuracy in common predictions compared to the original activations ${f_{n}(X)}$ . ", "page_idx": 16}, {"type": "text", "text": "Extracting Features for New Data Points. Once the dictionary $\\mathbf{D}^{\\star}$ was trained, it was fixed. For any new input $_{x}$ , the corresponding feature $_{z}$ was extracted by solving a Non-Negative Least Squares (NNLS) problem. This mapping of new input activations $\\pmb{f}_{n}(\\pmb{x})$ to the learned feature space was performed by minimizing the following objective: ", "page_idx": 16}, {"type": "equation", "text": "$$\nz=\\underset{z\\geq0}{\\arg\\operatorname*{min}}||f_{n}(\\mathbf{x})-z\\mathbf{D}^{\\star}||_{F}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "This optimization problem is convex, ensuring computational feasibility and robust feature extraction for new data points. ", "page_idx": 16}, {"type": "text", "text": "B Visualization of Meta-features ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "To visualize each feature, we used feature visualization methods [84]. Specifically, a recent improvement [31] re-parameterizes the original feature visualization optimization problem\u2014i.e., finding an image $_{x}$ that maximizes a direction, channel, or neuron\u2014by optimizing only the phase of a Fourier buffer while fixing the image magnitude. This approach produces more realistic images and prevents high-frequency leakage that results in adversarial positives. ", "page_idx": 17}, {"type": "image", "img_path": "NhqZpst42I/tmp/b117060b9ec5e63c8c23e63af7b2b13b8fcdbc9e39221a22c45be6a625a35f60.jpg", "img_caption": ["Figure 7: Visualization of Meta-Features, sorted by Complexity. Feature visualization using MACO [31] for the most simple (1-15) of the 30 Meta-features found on the 10,000 features extracted. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "NhqZpst42I/tmp/5e2820a66038fb1c195f4f49bfd2dc12130dfc8608cd96077cdd07af68b37603.jpg", "img_caption": ["Figure 8: Visualization of Meta-Features, sorted by Complexity. Feature visualization using MACO [31] for for the most complex (15-30) of the 30 Meta-features found on the 10,000 features extracted. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "C Complexity measure ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we detail the closed-form expression of the $\\nu$ -information when the predictive family $\\nu$ consists of linear classifiers with Gaussian posteriors. Specifically, $\\mathcal{V}$ is defined as follows: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{V}=\\left\\{\\eta:x\\rightarrow\\mathcal{N}(\\psi(\\mathbf{x}),\\sigma^{2}),\\;\\mathrm{with}\\;x\\in\\mathcal{X}\\mathrm{~and~}\\psi\\in\\Psi;\\right.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\Psi=\\{\\pmb{x}\\mapsto M\\pmb{x}\\mid M\\in\\mathbb{R}^{d}\\}$ is a set of linear predictors. This setting corresponds to the linear decoding we apply during the computation of $\\nu$ -information. In this context, a closed-form solution is available (see [115]): ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{\\mathcal{V}}(\\mathbf{x}\\rightarrow2)=H_{\\mathcal{V}}(z)\\simeq H_{\\mathcal{V}}(z\\mid\\mathbf{x})}\\\\ &{\\qquad\\qquad=\\operatorname*{inf}\\mathbb{E}_{z\\sim P_{\\mathbf{z}}}\\left[-\\log{\\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}}e^{-\\frac{(z-w)^{2}}{2\\sigma^{2}}}\\right]-\\operatorname*{inf}\\mathbb{E}_{\\mathbf{x},z\\sim P}\\left[-\\log{\\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}}e^{-\\frac{(z-w)(z)^{2}}{2\\sigma^{2}}}\\right]}\\\\ &{\\qquad\\qquad=\\operatorname*{inf}\\mathbb{E}_{z\\sim P_{\\mathbf{z}}}\\left[\\frac{(z-\\mu)^{2}}{2\\sigma^{2}}\\right]-\\operatorname*{inf}\\mathbb{E}_{\\mathbf{x},z\\sim P}\\left[\\frac{(z-\\psi)(\\mathbf{x})^{2}}{2\\sigma^{2}}\\right]}\\\\ &{\\qquad=\\frac{1}{2\\sigma^{2}}\\left(\\operatorname*{inf}\\mathbb{E}_{z\\sim P_{\\mathbf{z}}}\\left[(z-\\mu)^{2}\\right]-\\operatorname*{inf}\\mathbb{E}_{\\mathbf{x},z\\sim P}\\left[(z-\\psi(\\mathbf{x}))^{2}\\right]\\right)}\\\\ &{\\qquad=\\frac{\\mathrm{Var}(z)}{2\\sigma^{2}}\\left(1-\\operatorname*{inf}_{\\psi\\in\\Psi}\\mathbb{E}_{\\mathbf{x},z\\sim P}\\left[(z-\\psi(\\mathbf{x}))^{2}\\right]\\right)}\\\\ &{\\qquad=\\frac{\\mathrm{Var}(z)}{2\\sigma^{2}}\\left(1-\\frac{\\operatorname*{inf}_{\\psi\\in\\Psi}\\mathbb{E}_{\\mathbf{x},z\\sim P}\\left[(z-\\psi(\\mathbf{x}))^{2}\\right]}{\\mathrm{Var}(z)}\\right)}\\\\ &{\\qquad=\\frac{\\mathrm{Var}(z)}{2\\sigma^{2}}R^{2}}\\\\ &{\\qquad=\\mathrm{Var}(z)R^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Here, $R^{2}$ is the coefficient of determination. Therefore, the following inequalities hold: ", "page_idx": 19}, {"type": "equation", "text": "$$\n0\\leq\\mathcal{T}_{\\mathcal{V}}(\\mathbf{x}\\rightarrow z)\\leq\\mathrm{Var}(z).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Given that the input data are centered and scaled, we typically have $\\operatorname{Var}(z)$ around 1 (or less in case of Layer normalization). Furthermore, residual connections and batch normalization tend to preserve this scaling in deeper layers. We note, however, that our score $K(z,x)$ is not strictly bounded. Indeed, we define complexity as the opposite of the average $\\mathcal{V}$ -information across layers: complex features are those that are harder to decode. We add a shift of 1 for the ease of plotting. Empirically, we observed that this adjustment yields $K(z,x)$ in the range [0, 1], with 1 indicating a complex feature that is not available and 0 indicating a simple feature that is fully available. ", "page_idx": 19}, {"type": "text", "text": "D Feature Support Theory ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "While simplifying important features underscores a trend toward computational efficiency, the role of complex features within the model deserves a closer examination. Despite these features often being deemed less important directly, they contribute significantly to the model\u2019s overall performance, a paradox that can lead us to introduce the concept of \u201csupport features.\u201d These are a set of features that may not carry substantial importance individually, but that collectively play a crucial role in the model\u2019s decision-making process. ", "page_idx": 20}, {"type": "image", "img_path": "NhqZpst42I/tmp/a6e995c3eda8921bf6c33536b0bd807adcc72bc1c9333c811117c183e734ded5.jpg", "img_caption": ["Figure 9: \u201cSupport Features\u201d hypothesis. The majority of complex features are not very important, but play a non-negligible role and contribute to significant performance gains. This paradox is referred to as the \u201csupport features,\u201d a large ensemble of features individually of little to very little importance to the model but collectively holding a significant role. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "The presence of numerous complex features, whose importance on average is less pronounced, poses a conundrum. However, these features are far from redundant. Experiments conducted by progressively removing the most complex concepts from the model demonstrate a noticeable impact on performance, as illustrated in Figure 9. This empirical evidence supports the theory that, while individually, these complex features may not be pivotal, their collective presence contributes indispensably to the robustness and adaptability of the model. These results are reminiscent of prior findings that low-importance model components that are removed in pruning may nevertheless contribute to model accuracy on rare items [50]. ", "page_idx": 20}, {"type": "text", "text": "This observation aligns with the broader understanding of neural network functionality, where diversity in feature representation\u2014spanning from simple to complex\u2014enhances the model\u2019s ability to generalize and perform across varied datasets and tasks. Therefore, the \u201cFeature Support Theory\u201d underscores an essential aspect of neural network design and training: integrating and preserving a wide spectrum of features, regardless of their individual perceived importance, are vital for achieving high levels of performance and robustness. ", "page_idx": 20}, {"type": "text", "text": "E Complexity and Redundancy ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "To further understand the link between feature complexity and redundancy, we utilized the redundancy measure from [76]. Our findings indicate that complex features tend to be less redundant, as depicted in Figure 10. This observation aligns with the strong correlation between our complexity measure and the redundancy-based complexity measure proposed by [24]. ", "page_idx": 21}, {"type": "image", "img_path": "NhqZpst42I/tmp/539f213ad573232acb53cce404284c2e0733cd61127546a546e833264de692f7.jpg", "img_caption": ["Figure 10: Complex features are less redundant. Using the redundancy measure from [76], we show that our complex features tend to be less redundant. This result also confirms a link between our complexity measure and the one recently proposed by [24], which is also based on redundancy. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "To quantify redundancy, [76] employed a modified version of Centered Kernel Alignment (CKA) [57], a measure of similarity between two sets of activation features. We briefly recall that CKA between two set of activations $A,B$ in $\\mathbb{R}^{d}$ is defined as follows: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname{CKA}(A,B)=\\frac{\\|K_{A}K_{B}\\|_{F}^{2}}{\\|K_{A}K_{A}\\|_{F}\\|K_{B}K_{B}\\|_{F}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $K_{A}$ and $K_{B}$ are the Gram matrices of the feature activations $\\pmb{A}$ and $_B$ , respectively, and $\\|\\cdot\\|_{F}$ denotes the Frobenius norm. ", "page_idx": 21}, {"type": "text", "text": "In our analysis, we calculated the CKA measure between a feature $z$ and the activations for a set of 2,000 images from the validation set ${f_{n}}(X)$ . Subsequently, we compared this with the CKA measure when a portion of the activation is masked using a binary mask $m\\in\\{0,1\\}^{|A_{\\ell}|}$ , denoted as $\\mathrm{CKA}(z,f_{n}(\\bar{X^{}})\\odot m)$ , where $\\odot$ represents element-wise multiplication (Hadamard product). This comparison enabled us to assess whether masking a subset of neurons impacts the decoding of the features. Specifically, to evaluate redundancy, we employed a progressive masking strategy, successively masking $10\\%$ , $50\\%$ , and $90\\%$ of the activation. If the masked activations retain a high CKA with $z$ , it indicates that the information remains largely intact, suggesting that the feature is redundantly stored across multiple neurons, sign of a redundant encoding mechanism within the network. Conversely, if masking results in a substantial decrease in CKA, it implies that the information was predominantly localized on a specific neuron. In this scenario, the feature is not redundantly encoded but rather concentrated in specific neurons. This concentration indicates a lower degree of redundancy, as the loss of these specific neurons (throught the masking) leads to a significant reduction in the CKA score. ", "page_idx": 21}, {"type": "text", "text": "The final score of redundancy is then the average CKA difference between the original activation and the masked activations: ", "page_idx": 22}, {"type": "equation", "text": "$$\n{\\mathrm{Redundancy}}={\\frac{\\operatorname{\\mathbb{E}}_{m}\\bigl({\\mathrm{CKA}}(f_{n}(X)\\odot m,z)\\bigr)}{{\\mathrm{CKA}}(f_{n}(X),z)}}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "And averaged across the different level of masking. A high score (1) indicating a high redundancy \u2013 i.e. the CKA between the masked activation and with the original activation is similar \u2013 while a low score indicate a more localized and thus a lower degree of redundancy. ", "page_idx": 22}, {"type": "text", "text": "In summary, our results, as depicted in Figure 10 support the idea that complex features exhibit lower redundancy. ", "page_idx": 22}, {"type": "image", "img_path": "NhqZpst42I/tmp/2675e4e4d93875694ac1fa470a0bb88fd765f4d02e319ee23d3e499baac7c720.jpg", "img_caption": ["Figure 11: Complex features are less robust. This figure illustrates the relationship between feature complexity and robustness, quantified as the variance of the feature value when the image is perturbed with Gaussian noise. The results indicate that more complex features tend to exhibit lower robustness. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "To measure robustness, we evaluate the stability of feature responses under perturbations. For each input point $_{x}$ , we add isotropic Gaussian noise with varying levels of standard deviation $\\sigma$ . The robustness score is determined by measuring the variance in the feature response due to the noise. Formally, let $z(\\pmb{x})$ represent the feature response for input $_{x}$ . We define the perturbed input as: $\\tilde{\\mathbf{x}}=\\mathbf{x}+\\mathcal{N}(0,\\partial^{2}\\mathbf{I})$ where $\\mathcal{N}(0,\\sigma^{2}\\mathbf{I})$ represents Gaussian noise with mean 0 and variance \u03c32. The sensitivity score Sensitivity $(z)$ for a feature $z$ is given by: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathrm{Sensitivity}(z)=\\mathrm{Var}(z({\\tilde{\\mathbf{x}}}))\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Specifically, we sample 100 random noise and repeat this for 3 levels of noise $\\sigma\\in\\{0.01,0.1,0.5\\}$ to compute the variance in feature response for each input from 2,000 samples from the Validation set of ImageNet to get a distribution of feature value. We also consider other metrics such as the range (min-max) of the feature response, but all methods consistently indicate that more complex features are less robust. ", "page_idx": 23}, {"type": "text", "text": "In summary, our results, as shown in Figure 11, demonstrate that complex features exhibit lower robustness. This indicates that features with higher complexity are more sensitive to perturbations and noise, resulting in greater variability in their responses. ", "page_idx": 23}, {"type": "image", "img_path": "NhqZpst42I/tmp/17544d97685c0427b77baf660dadb3cd186177d41462604f50172ed98ec866f0.jpg", "img_caption": ["Figure 12: Inhibiting and non-inhibiting features vs complexity. Important features can be significant either by inhibition, i.e., removing information from a class, or by adding information for a given class. Each point represents a feature, and violet-colored features generally act as inhibitors $(\\Gamma(z_{i})<0)$ . "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "The problem of estimating feature importance is closely related to attribution methods [117, 33, 88, 80, 15, 102, 106, 23], which aim to identify the important pixels for a decision. A recent study has shown that all attribution methods can be extended in the space of concepts [32]. In our case, the features are extracted from the penultimate layer, where the relationship between feature values and logits is linear. We will elaborate on this and demonstrate that the notion of importance in the linear case is easier and optimal methods to estimate importance exist. ", "page_idx": 24}, {"type": "text", "text": "Setup. Recall that for a point $_{x}$ , we can obtain its $k$ feature values by solving an NNLS problem $z=\\mathrm{\\bar{\\arg\\operatorname*{min}}}\\,\\vert\\vert\\,f_{n}(\\mathbf{x})-z\\bar{\\mathbf{D}}^{\\star}\\vert\\vert_{F}$ . The vector $\\boldsymbol{z}$ contains the $k$ features in $\\mathbb{R}^{k}$ . We can replace the activation of the penultimate layer $\\pmb{f}_{n}(\\pmb{x})$ with its feature representation in the over-complete basis $z\\mathbf{D}^{\\star}\\approx f_{n}(\\mathbf{x})$ . Since we are in the penultimate layer, the model\u2019s decision, i.e., the logit $\\mathfrak{y}\\in\\mathbb{R}$ for the predicted class, is linearly related to each feature $z$ by the last weight matrix, denoted as $W$ , as follows: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{y}=f_{n}(\\mathbf{x})W}\\\\ &{\\quad\\approx z\\mathbf{D}^{\\star}W\\quad\\mathrm{with}\\ \\ z=\\arg\\operatorname*{min}||f_{n}(\\mathbf{x})-z\\mathbf{D}^{\\star}||_{F}}\\\\ &{\\quad=z W^{\\prime}\\ \\ \\mathrm{with}\\ \\ W^{\\prime}=\\mathbf{D}^{\\star}W\\in\\mathbb{R}^{k}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Thus, the energy contributed to the logit by feature $i$ can be directly measured by $W_{i}^{\\prime}z_{i}$ and $y=$ $\\sum_{i}^{k}W_{i}^{\\prime}z_{i}$ . Consequently, the contribution of a feature $z_{i}$ can be measured using gradient-input, $\\left(\\nabla_{z}\\ y\\right)\\odot z$ . Several studies [5, 32] have detailed the linear case and shown the optimality of gradientinput with respect to fidelity metrics. They also demonstrated that many methods in the linear case boil down to Gradient-Input, including Occlusion [117], Rise[88], and Integrated Gradient[106]. ", "page_idx": 24}, {"type": "text", "text": "In our case, we measured the importance by taking the absolute value of the importance vector, i.e., $\\begin{array}{r}{\\mathbf{T}(z_{i})=\\mathbb{E}_{\\mathbb{P}_{z}}\\!\\left(||\\frac{\\partial\\mathbf{y}}{\\partial z_{i}}\\cdot\\boldsymbol{z}_{i}||\\right)}\\end{array}$ . It is natural to question whether this approach might overlook important features due to their inhibitory effects. Indeed, as depicted in Figure 12, numerous features may be important not because they add positive energy to the logits, but by inhibition, i.e., by suppressing class information. Although this does not alter the implications of our previous observations, it is noteworthy that the majority of inhibitory features are also simple features. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "Prevalence and Importance. Another property of importance is its close relationship with prevalence [32], which indicates that a frequently occurring feature will, on average, be more important given the same importance coefficient $(\\nabla_{z}\\ y)$ . In our study, this implies that if the most important features are reduced, these important features are also potentially more frequently present. Consequently, the prevalence of a feature can be a factor explaining this sedimentation process. We refer the reader to a concurrent study that proposed to investigate more deeply this phenomena using a controlled dataset [58]. ", "page_idx": 25}, {"type": "text", "text": "H Features Clustering ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "The visualization in Figure 2 prompts an important question: are features in our model clustered based on their complexity? Specifically, are there regions in the feature space that are generally more \u201ccomplex\u201d than others, or that respond primarily to more complex stimuli? Our access to 10,000 features via overcomplete decomposition enables a more detailed analysis compared to traditional neuron-wise studies. We aim to explore three main hypotheses: ", "page_idx": 26}, {"type": "text", "text": "\u2022 Hypothesis 1: Features cluster by super-class. This hypothesis posits that features corresponding to semantically related categories are spatially grouped within the feature space\u2014e.g., concepts related to the dog class are closer to those of the cat class than to unrelated classes like furniture.   \n\u2022 Hypothesis 2: Features cluster by complexity. We suggest that features may organize themselves based on their complexity, with simpler features forming distinct clusters separate from more complex ones.   \n\u2022 Hypothesis 3: Features cluster by importance. This hypothesis explores whether features with similar predictive importance tend to group together within the feature space. ", "page_idx": 26}, {"type": "text", "text": "It is important to emphasize that these hypotheses are mutually independent. To test them, we propose two methodologies: (1) visualizing feature embeddings using UMAP and (2) clustering the features followed by dendrogram analysis to examine whether the resulting clusters are homogeneous (also called \"pure\") in terms of super-class, complexity, or importance. ", "page_idx": 26}, {"type": "image", "img_path": "NhqZpst42I/tmp/00ed146904354f1379ef19ca7b7cccd4c8e23cabe59672595837799073eda41f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Figure 13: Feature Similarity vs Super-Class. Each point represents a concept, with its color indicating the associated super-class. Some super-classes such as birds, reptiles, dogs & other mammals form well-defined, tight clusters, suggesting that features belonging to them are close in the feature space. Others, such as device, clothes appear more dispersed. By comparing this figure with Figure 2, we can identify which meta-features are \"pure\" (belonging to a single super-class) and which are \"impure\" (spanning multiple super-classes). Interestingly, the \u201cimpurity\u201d region seems to cover low-complexity and mid-complexity concepts such that Grass, Waves, Trees, Low-pixel quality detector which are not class-specific. ", "page_idx": 27}, {"type": "text", "text": "Feature Similarity vs Super-Class. Figure 13 illustrates the organization of features by their super-class. Each point is a feature, colored according to its super-class label. We observe that certain super-classes, like those associated with birds, dogs, reptiles form distinct and cohesive clusters, indicating a strong grouping within the feature space. Other region have features that encompassing a wider range of super-class, such that grass, waves, low-pixel quality detector. This reveals that some meta-features are predominantly associated with a single super-class (\"pure\"), while others span multiple super-classes (\"impure\"), reflecting the shared visual characteristics or multi-functional nature of those features. ", "page_idx": 27}, {"type": "image", "img_path": "NhqZpst42I/tmp/cd59948bafeed2512162b45d4f919d2e7dbeac8cbcd2c89fd136176144df30e9.jpg", "img_caption": ["Complexity of concepts ", "Figure 14: Feature Similarity by Complexity. A) Each point is a feature, colored by its complexity score. Distinct areas of the graph correspond to varying levels of complexity, suggesting a nonrandom distribution of feature complexity. For instance, animal-related features tend to have higher complexity, one could hypothesize that the fine-grained classification required for these categories are responsible for this complexity. B) A four-level dendrogram where each level further segments clusters and calculates the average complexity for each sub-cluster. A clear split by complexity appears at the first level and intensifies with depth, supporting the idea that some regions of the feature space are inherently more complex than others. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "Clustering by Complexity. Figure 14 explores how features are organized based on their complexity. Panel A shows a UMAP visualization with points colored according to their complexity scores. The visualization reveals distinct regions of varying complexity, indicating that the distribution is structured. For instance, features related to animals display higher complexity, likely because these require fine-grained and precise detectors. In contrast, simpler features, such as color detectors or low-frequency patterns, cluster together in less complex regions. Panel B displays a dendrogram with four hierarchical levels, where each level introduces additional splits, and the mean complexity is calculated for each sub-cluster. The pronounced division in complexity at the first level, which sharpens as we delve deeper, suggests that the feature space is compartmentalized based on complexity. A promising direction for future research would be to align these complexity clusters with known visual cortical areas to explore potential correspondences. ", "page_idx": 28}, {"type": "image", "img_path": "NhqZpst42I/tmp/ff4cfa1c3c37f6970ac7674567a64779e84891c33fda7acf5aea46c7b0fc0f6a.jpg", "img_caption": ["Importance of concepts ", "Figure 15: Feature Similarity by Importance. A) Each point represents a feature, with color indicating its importance. Distinct regions of the graph contain features of varying importance, particularly with more important features clustering at the top. B) A four-level dendrogram with subclusters evaluated by their average importance. We observe that from the first level, the dendrogram effectively splits features into groups of varying importance, corresponding to the upper and lower parts of the UMAP graph in Panel A. "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "Clustering by Importance. Finally, we investigate the third hypothesis: the presence of regions within the feature space that contain features with higher predictive importance. Figure 15 Panel A depicts a UMAP visualization where features are colored by their importance. The graph shows a clear structure, with highly important features grouping in specific regions (e.g., the upper part of the graph), while less important features are distributed in other regions. Panel B provides the corresponding dendrogram, revealing that even at the first level of the hierarchy, features segregate into clusters of varying importance. Notably, low-complexity \u201csupport\u201d features\u2014such as grass, waves, and low-pixel-quality detectors\u2014tend to form cohesive clusters. Meanwhile, more predictive features, like animal-related features that drive classification, group together in another distinct region. This raises a crucial question: is this clustering merely correlated (i.e., based on shared visual aspects like context or background), or does it reflect a causal relationship in the model\u2019s predictive structure? This question remains an open avenue for future investigation. ", "page_idx": 29}, {"type": "text", "text": "I Local vs Distributed Encoding ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Neural networks exhibit a diversity in how features are encoded, ranging from local to distributed representations [25, 29]. In the local encoding scenario, a single neuron is primarily responsible for encoding a feature. On the other hand, in a distributed encoding scheme, features are represented by the coordinated activity of multiple neurons, often deviating from canonical axis-aligned directions. More specifically, features could be distributed across many neurons\u2014sometimes densely, or in a pseudo-distributed fashion\u2014rather than being localized to a specific neuron. This theory is one of the main motivations behind employing overcomplete concept extraction methods [18, 32, 92, 35]. ", "page_idx": 30}, {"type": "text", "text": "The distinction between local and distributed encoding is critical, particularly when extracting overcomplete features from a large-scale dictionary. In our analysis of 10,000 features, we assess whether these features are encoded in a local or distributed manner. A local encoding would imply that the feature vector $^d$ is aligned with a canonical vector, i.e., $d\\in\\{e_{1},...,\\bar{e}_{n}\\}$ , where $n\\,=\\,|A_{\\ell}|$ is the dimensionality of the activation space and $e_{i}$ represents the one-hot canonical vector for the ith neuron. In contrast, a fully distributed feature would be characterized by non-zero values across all neurons, with no alignment to any single axis. ", "page_idx": 30}, {"type": "text", "text": "To quantify the extent of this local or distributed encoding, we use the Hoyer score. This score, which ranges between 0 and 1, captures the sparsity of a vector by comparing its $\\ell_{1}$ -norm to its ", "page_idx": 30}, {"type": "image", "img_path": "NhqZpst42I/tmp/de4feed5bf105713e4bc72700d4e71fd7235b2694d0c31d0f765dd43259c8c0b.jpg", "img_caption": ["Hoyer score of concepts "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "Figure 16: Local vs Distributed Encoding. Each point represents a feature, with color indicating its Hoyer score. Higher scores suggest a more \"local\" representation, where a feature is primarily encoded by a single neuron. Lower scores indicate a distributed representation across a population of neurons. Interestingly, some features have scores near 1, implying near-complete localization, while others are more distributed. This variation highlights the diversity in encoding across features. ", "page_idx": 30}, {"type": "text", "text": "$\\ell_{2}$ -norm, with a correction term for normalization. Formally, the Hoyer score is given by: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathrm{Hoyer}(\\pmb{d})=\\frac{\\sqrt{n}-\\vert\\vert\\pmb{d}\\vert\\vert_{1}/\\vert\\vert\\pmb{d}\\vert\\vert_{2}}{\\sqrt{n}-1}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "For each feature in our dictionary $\\mathbf{D}=\\{d_{1},...,\\,d_{10,000}\\}$ , we compute the Hoyer score to determine whether the feature is locally encoded (score near 1) or distributed (score near 0). ", "page_idx": 30}, {"type": "text", "text": "Figure 16 illustrates the results of this analysis, revealing significant variability in the degree of distribution among features. Some regions of the feature space show highly distributed encoding, while others exhibit more local representations. This analysis was conducted on a ResNet50 model, and it is possible that the distribution of encoding strategies varies across different architectures. ", "page_idx": 30}, {"type": "image", "img_path": "NhqZpst42I/tmp/30714ae99abf8ce762d91bf896551d033fb19d08e523b2687c22c9431c94ad2f.jpg", "img_caption": ["Figure 17: Feature Complexity vs. Distributed Encoding. We show that there is no clear relationship between feature complexity and the degree of distributed encoding. Whether a feature is encoded by a single neuron or distributed across multiple neurons does not seems to be determined by its complexity. "], "img_footnote": [], "page_idx": 31}, {"type": "image", "img_path": "NhqZpst42I/tmp/8adedded028e168f297baaeaec43cdf34989a2fcb27dbf31e75a4dbf89f8b719.jpg", "img_caption": ["Figure 18: Replication of Figure 4 Using $\\nu$ -Information. As described in Section 4, we replicate the analysis of feature flow and complexity using $\\nu$ -information as a measure. "], "img_footnote": [], "page_idx": 32}, {"type": "text", "text": "A legitimate consideration arises when revisiting Section 4, where we introduced the hypothesis that simpler features are primarily carried through the residual connections of a network, while more complex features are progressively constructed through interactions between the main branch and residual connections. The measure we originally used to support this hypothesis was CKA, which serves as a proxy to assess the similarity between the activations at a certain stage of the model and the final state of a concept. However, one might wonder why not use $\\mathcal{V}.$ -information directly. ", "page_idx": 32}, {"type": "text", "text": "Figure 18 presents the results of this replication. Although the scale of the values differs from those in Section 4, the overall trend remains consistent. The left panel shows the dynamics of simpler features. Early in the network, the main branch carries a significant portion of the $\\mathcal{V}.$ - information, which diminishes as the simpler features are gradually \u201cpassed along\u201d through the residual connections. Notably, even as the information content decreases, the absolute quantity of $\\nu$ -information remains higher for simple features compared to complex ones. This indicates that while simpler features are transported through the residual connections, they are not entirely depleted of their information content. The right panel of Figure 18 demonstrates the progressive construction of more complex features. Here, we see that both the main branch and residual connections contribute to the gradual accumulation of information necessary for these complex features. This supports our initial hypothesis: complex features do not traverse the network intact but are built up in a cumulative process, drawing on multiple layers and branches to form intricate representations. ", "page_idx": 32}, {"type": "text", "text": "For compute consideration, this replication was conducted using a different experimental setup than that of Section 4. For this analysis, we employed the validation set of ImageNet to build the dictionary instead of the train set. Both the dictionary and the model were different from those used in the main body of the paper. Specifically, the model used here was the ResNet50 implementation from the Keras library [26]. Despite these differences in experimental conditions, the overarching trends observed in our original CKA-based analysis are preserved, bolstering the validity of the flow hypothesis. ", "page_idx": 32}, {"type": "text", "text": "K Kolmogorov, Levin and $\\nu$ -information ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "In this section we recall some of the most important complexity measures like Kolmogorov complexity, its computationally tractable counterpart the Levin complexity, and finally we underline the epistemic similarity between these concepts in deep learning. ", "page_idx": 32}, {"type": "text", "text": "Kolmogorov complexity [55] is a measure of the complexity of an object. The objects (image, video, text, pdf, etc.) can be ecnoded as a sequence $\\left(u_{n}\\right)^{\\mathrm{~\\,~}}\\in\\Sigma^{\\mathbb{N}}$ of symbols over a finite alphabet $\\Sigma$ . A program is a finite sequence $P\\in L$ written in language $L\\subset\\Sigma^{*}$ (e.g a Python source file). Kolmogorov complexity $K_{L}^{(\\stackrel{-}{\\infty})}(u_{n})$ is the length of the shortest program $P:\\mathbb{N}\\to\\Sigma^{*}$ that produces ", "page_idx": 32}, {"type": "text", "text": "the $n$ -th first terms of the sequence $\\left(u_{n}\\right)$ : ", "page_idx": 33}, {"type": "equation", "text": "$$\nK_{L}^{(\\infty)}(u_{n})\\stackrel{\\mathrm{def}}{=}\\operatorname*{min}_{P(n)=u_{n}}|P|.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Intuitively, if the sequence is highly compressible, the program will be short. For example, the sequences $[1,2,3,4,\\ldots]$ or $[2,4,8,16,32,...]$ are few lines of code in most programming languages. Conversely, if the sequence is purely random, then no finite-length program exists. The digits of $\\pi$ , seemingly without structure, are not Kolmogorov random since there exist short programs computing them. The famous Cantor\u2019s diagonal argument [20] shows that most sequences are random, since no bijection exists between $\\Sigma^{*}$ (countably infinite) and $\\Sigma^{\\mathbb{N}}$ (cardinality of the continuum). The definition implicitly assumes a specific computation model (Python interpreter, $C++$ compiler, Turing machine) to describe the language. However, by definition Turing-complete models can simulate each other, which implies there exist a universal constant $\\mathcal{C}(\\mathrm{Python}|\\mathrm{C}\\!+\\!+\\!)$ such that for all sequences $u_{n}$ we have $K_{\\mathrm{Python}}^{\\mathrm{(\\infty)}}(u_{n})\\leq K_{\\mathrm{C++}}^{\\mathrm{(\\infty)}}(u_{n})+{\\mathcal{C}}(\\mathrm{Python}|{\\mathrm{C}}++)$ . This constant corresponds to the length of a Python interpreter written in $C++$ for example. In general, this holds for any other pair of languages. Therefore, if K(L\u221e)(un) \u2192+\u221eas n \u2192\u221efor some language L, then it is true in every other language: intrinsic randomness is universal in this sense [98]. ", "page_idx": 33}, {"type": "text", "text": "Levin complexity. Kolmogorov\u2019s complexity suffers from an important drawback: it is not Turingcomputable. Put another way, there exists no algorithm that computes $K^{(\\infty)}$ . Fortunately, by regularizing $K^{(\\infty)}$ appropriately it is possible to make it computable. Levin [64] proposed to regularize the cost with the runtime $T(P,n)$ of program $P$ on input $n$ . This is the Levin complexity: ", "page_idx": 33}, {"type": "equation", "text": "$$\nK_{L}^{(T)}(u_{n})\\stackrel{\\mathrm{def}}{=}\\operatorname*{min}_{P(n)=u_{n}}|P|+\\log_{|\\Sigma|}T(P,n).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "This modification makes $K^{(T)}(u_{n},L)$ computable with the Levin Universal Search algorithm (see Alg. 1). Informally, instead of looking for $a$ shortest program, this algorithm seeks algorithms that run fast among those who are shorts. It is obtained by iterating over lengths $i\\in\\mathbb N$ , and by running exactly one step of computation of all these programs in parallel. The first program $P$ that halts on $u_{n}$ minimizes $\\bar{K}^{(T)}$ . This is a central property of Levin\u2019s universal search: the first programs found are the simplest and the ones requiring the lesser compute [4, 108, 13]. ", "page_idx": 33}, {"type": "table", "img_path": "NhqZpst42I/tmp/74b999fedf92ef7fcaae2049689b6cd02ec0bf589c6637d40e375203bbf4c40c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 33}, {"type": "text", "text": "Deep learning and simplicity bias. The links between algorithmic information theory and deep learning have been a recurring although spurious interest throughout the years [94, 95, 75, 61, 68, 40]. Neural networks are a special kind of program, composed of the source files required for inference, and the weights embedded in the network. Therefore, results related to the complexity of sequences apply transparently. Program length (Kolmogorov) and program runtime (Levin) are tightly linked since deeper and wider networks also consume more FLOPS during inference. Smaller networks implement simpler programs. Similarly, features that can be decoded \u201cearly\u201d in the network are simpler than those requiring all the layers. The idea is often coined as Minimum Description Length (MDL) principle [16], Occam\u2019s Razzor, or even simplicity bias [49]. ", "page_idx": 33}, {"type": "text", "text": "L Limitations ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Task. Here, we have studied a specific CNN architecture, ResNet50. In future experiments, it will be useful to investigate whether other model families exhibit similar feature learning, including in domains beyond vision. ", "page_idx": 34}, {"type": "text", "text": "Architecture. The residual connections of the ResNet are shared by other architectures like ConvNext [67] or Vision Transformers (ViT) [28, 14]. The works of [109] indicate that findings from convolutional models may transfer to ViTs. Furthermore, the work of [116] suggest that features in convolutional networks and ViT are of similar nature. However, [90] found significant quantitative differences between the layers of ResNets and ViTs, highlighting the need for further empirical testing. ", "page_idx": 34}, {"type": "text", "text": "Training Dynamics The observed dynamics of feature learning, including the emergence of complex features and the reduction in the complexity of important features later in training, are based on a specific training schedule and set of hyperparameters. To accurately attribute these findings, a more comprehensive study is required to evaluate the role of various factors such as the learning rate scheduler and weight decay. Future research should systematically investigate how these and other training parameters influence feature complexity and importance. ", "page_idx": 34}, {"type": "text", "text": "Nested predictive families. Our complexity metrics rely on the hypothesis that the different predictive families associated to the network up to depth ${\\pmb f}_{\\ell}({\\pmb x})$ are nested, i.e. that stacking more layers strictly increases expressiveness. This is highlighted in the relevant assumption in section 2. If this hypothesis is violated, the true complexity of a feature may be overestimated in deeper layers. This is typically the case at the early stages of training. ", "page_idx": 34}, {"type": "text", "text": "Dictionary of features. Regarding the building of the dictionary using NMF, a previous study [32] has shown that the specific dictionary learning method yielded a favorable tradeoff between several criterions such as sparsity, reconstruction error, or stability. Other dictionary learning methods (like sparse-PCA, K-Means or sparse auto-encoder) may yield a bank of concepts with different properties. ", "page_idx": 34}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We support every claim with the corresponding experiment on which we compute well-defined metrics, correlations, and p-values. ", "page_idx": 35}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: the paper relies on a large empirical study, that was specifically performed on the ResNet50 architecture and the Imagenet dataset. This limitation is clearly specified in the Section 2. Furthermore, the relevance of $\\nu$ -Information to analyse the expressiveness of the network at difference depth assumes that expressiveness monotically increases with depth. This assumption is formalized in paragraph Assumption page 5. Finally, a last \"Limitations\" section is placed in Appendix $\\mathrm{L}$ . ", "page_idx": 35}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: No new theoretical results are claimed. ", "page_idx": 35}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: we rely on existing protocols to train the models, as specified in Section 2, which includes all pre-processing, learning rates, architectures, and other common hyperparameters. Furthermore, the metrics introduced are existing metrics of literature for which standard implementations are available. ", "page_idx": 35}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 35}, {"type": "text", "text": "Answer: [No] ", "page_idx": 35}, {"type": "text", "text": "Justification: the dataset ImageNet can be accessed through https://www.image-net.   \norg. However the code is not available for review at time of submission. ", "page_idx": 35}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: see answers above. ", "page_idx": 35}, {"type": "text", "text": "7. Experiment Statistical Significance ", "page_idx": 35}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We compute correlation coefficients between metrics, and we also report the corresponding p-value for statistical significance in every case. ", "page_idx": 35}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 36}, {"type": "text", "text": "Answer: [No] ", "page_idx": 36}, {"type": "text", "text": "Justification: the cost of re-training the model can be estimated in the relevant literature. Computing the complexity scores involves solving a (high dimensional) linear regression at every depth and every epoch of interest for each of the features. ", "page_idx": 36}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: after close examination of the Code of Ethics, there are no concerns associated to our study. ", "page_idx": 36}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: the goal of this empirical study is to understand phenomenons arising during the training of deep neural networks. No new algorithm nor dataset is proposed, that could have had direct societal impact. ", "page_idx": 36}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: we rely exclusively on the ImageNet dataset, more specifically the subset of the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) whose license can be accessed at this URL: https://www.kaggle.com/competitions/ imagenet-object-localization-challenge/rules#7-competition-data. ", "page_idx": 36}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}]