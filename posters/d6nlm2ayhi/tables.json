[{"figure_path": "D6nlm2AYHi/tables/tables_13_1.jpg", "caption": "Table 1: Average returns of compared algorithms in Pac-Men, SMAC, and SMACv2. \u00b1 denotes the standard deviation over five random seeds.", "description": "This table presents a comparison of the average returns achieved by different multi-agent reinforcement learning algorithms across three benchmark environments: Pac-Men (a custom grid-world environment), SMAC (StarCraft Multi-Agent Challenge), and SMACv2 (an upgraded version of SMAC).  The algorithms compared include QMIX, MAPPO, MAVEN, EOI, QTRAN, SCDS, LIPO, FOX, and the proposed CTR method integrated with both QMIX and MAPPO.  The results are averaged over five random seeds, with standard deviations reported to indicate variability. The table provides quantitative comparisons of algorithm performance across different levels of task difficulty (easy, hard, super hard).", "section": "4 Experiments"}, {"figure_path": "D6nlm2AYHi/tables/tables_14_1.jpg", "caption": "Table 1: Average returns of compared algorithms in Pac-Men, SMAC, and SMACv2. \u00b1 denotes the standard deviation over five random seeds.", "description": "This table presents the average returns achieved by different multi-agent reinforcement learning algorithms across three environments: Pac-Men, SMAC, and SMACv2.  The results show the mean performance and standard deviation for each algorithm across five independent runs with different random seeds.  Pac-Men is a simpler grid-world environment designed for this paper, while SMAC (StarCraft Multi-Agent Challenge) and SMACv2 are well-known benchmarks for cooperative MARL. The table allows readers to directly compare the performance of the proposed CTR method (CTR+QMIX and CTR+MAPPO) against several state-of-the-art baselines across a variety of task complexities.", "section": "4 Experiments"}, {"figure_path": "D6nlm2AYHi/tables/tables_14_2.jpg", "caption": "Table 2: Performance of our method and QMIX in homogeneous scenarios.", "description": "This table compares the performance of CTR+QMIX and QMIX in four homogeneous StarCraft II scenarios where agents benefit from using similar policies (focus fire).  The scenarios vary in the number of units on each team, demonstrating the effectiveness of CTR even when homogeneous strategies are optimal.  The results show CTR+QMIX consistently outperforms QMIX in these scenarios.", "section": "E Evaluations of CTR in scenarios requiring homogeneous behavior"}, {"figure_path": "D6nlm2AYHi/tables/tables_14_3.jpg", "caption": "Table 3: Performance of our method and QMIX in scenarios of SMACv2 with different number of agents", "description": "This table presents the performance comparison between CTR+QMIX and QMIX in four SMACv2 scenarios with varying numbers of agents (5 vs 5, 10 vs 10, 15 vs 15, and 20 vs 20).  It demonstrates the scalability of the CTR method, showing that it maintains high performance even with a significant increase in the number of agents, whereas QMIX's performance declines dramatically.", "section": "4 Experiments"}, {"figure_path": "D6nlm2AYHi/tables/tables_15_1.jpg", "caption": "Table 4: Hyperparameters", "description": "This table lists the hyperparameters used in the experiments for the Pac-Men, SMAC, and SMACv2 environments.  It shows the settings for hidden dimension, learning rate, optimizer, target update frequency, batch size, and the alpha (\u03b1) values for the contrastive learning loss in both CTR+QMIX and CTR+MAPPO.  The epsilon annealing time is also specified for each environment.", "section": "H Visualization"}]