{"references": [{"fullname_first_author": "Jonathan Ho", "paper_title": "Denoising diffusion probabilistic models", "publication_date": "2019-12-01", "reason": "This paper introduces the foundational concepts and framework of diffusion models, which are central to the core methodology of DITS."}, {"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-01", "reason": "CLIP, introduced in this paper, provides the backbone for the multi-modal embedding space that DITS operates within."}, {"fullname_first_author": "Satya Krishna Gorti", "paper_title": "X-pool: Cross-modal language-video attention for text-video retrieval", "publication_date": "2022-06-01", "reason": "This paper is used as a baseline for the text-video retrieval task and sets the stage for comparison with DITS performance."}, {"fullname_first_author": "Yang Liu", "paper_title": "Adaptive cross-modal prototypes for cross-domain visual-language retrieval", "publication_date": "2021-06-01", "reason": "This paper's work on cross-modal alignment and the handling of modality gaps provides relevant context and a basis for comparison to the DITS approach."}, {"fullname_first_author": "Jiamian Wang", "paper_title": "Text is mass: Modeling as stochastic embedding for text-video retrieval", "publication_date": "2024-06-01", "reason": "This paper, also authored by some of the same authors, presents another closely related method in the same domain, enabling a deeper understanding of the research context and contributions."}]}