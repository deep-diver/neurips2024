[{"figure_path": "SrQua0ATRZ/figures/figures_1_1.jpg", "caption": "Figure 1: Multi-modality gap alignment for the text-video retrieval. (a) Considering the many-to-one correspondence of text-to-video, we define the modality gap stemming from the text (d = v \u2013 t) for uniqueness. (b) Fixed prior alignment posits a Gaussian distribution, which is inflexible. (c) Diffusion-based alignment upon L2 loss, which does not fit the retrieval task. The generation is heavily affected by the random samples from N(0, I). (d) The proposed diffusion-inspired truncated sampler (DITS) aligns from t to v and gradually models the gap, guided by the contrastive loss.", "description": "This figure illustrates different approaches to multi-modality gap alignment in text-video retrieval.  (a) Defines the modality gap as the difference between video and text embeddings. (b) Shows a fixed prior approach using a Gaussian distribution, which lacks flexibility. (c) Depicts the limitations of using vanilla diffusion models with L2 loss for retrieval tasks, highlighting the impact of random sampling. (d) Introduces the proposed DITS method, which progressively aligns text and video embeddings using a truncated diffusion process and contrastive loss for improved control and discriminative power.", "section": "3 Method"}, {"figure_path": "SrQua0ATRZ/figures/figures_4_1.jpg", "caption": "Figure 2: Overview of DITS. Given the video embedding v from Fe(\u00b7) and the text embedding t from G(), DITS performs a T'-steps sampling by starting from t, to gradually approach v (as exampled by a relevant pair t(1), v(1)) with the aligned embedding v\u00b9 at the timestamp t = 0. Meanwhile the progressive alignment, DITS intrinsically approximates the distribution of the modality gap \u03b4 ~ Pt at each timestamp t = T', ..., 1. The contrastive loss is adopted to guide the alignment and modeling, with parameters: \u0398 = {\u03b8, \u03c6, \u03b3}. We devise \u03b5\u03b3(\u00b7) upon DiT [Peebles and Xie, 2023].", "description": "This figure illustrates the architecture of the proposed Diffusion-Inspired Truncated Sampler (DITS).  It starts with pre-trained text and video embeddings, focusing on the progressive alignment of text and video embeddings by gradually modeling the modality gap (the difference between text and video embeddings). The process uses a truncated diffusion process starting from the text embedding, guiding the alignment towards the video embedding. The process is controlled by the contrastive loss, which simultaneously considers relevant and irrelevant text-video pairs, making the embedding space discriminative.  The figure visually demonstrates the steps of the truncated diffusion process and its integration with the contrastive learning mechanism to accomplish text-video alignment, highlighting its key advantages over existing methods.", "section": "3 Method"}, {"figure_path": "SrQua0ATRZ/figures/figures_5_1.jpg", "caption": "Figure 3: Modality gap distribution for the relevant pairs before and after the Diffusion model alignment.", "description": "This figure shows the distribution of the modality gap (the difference between text and video embeddings) for relevant text-video pairs before and after applying a diffusion model for alignment.  The x-axis represents the L2-norm of the modality gap, and the y-axis represents the count of pairs with that gap. Two distributions are presented: one for the pairs before diffusion model alignment and one for the pairs after alignment. The figure visually demonstrates how the diffusion model helps reduce the modality gap, improving the alignment of text and video embeddings in the joint space. The reduction in the spread of the distribution after the alignment is also evident.", "section": "3.3 Proposed Alignment by DITS"}, {"figure_path": "SrQua0ATRZ/figures/figures_8_1.jpg", "caption": "Figure 4: DITS reduces the modality gap for the relevant pairs, serving as a tool to improve the space by guiding the CLIP's learning.", "description": "This figure shows the distribution of the modality gap (the L2-norm of the difference between video and text embeddings) for relevant pairs before and after applying DITS.  The results indicate that DITS successfully reduces the modality gap, aligning the embeddings of text and video more closely.  The comparison between \"DITS (Joint Train)\" and \"DITS (Fix CLIP)\" further highlights the effectiveness of jointly training the DITS model and CLIP embedding space, leading to a more significant reduction in the modality gap than simply using a pre-trained CLIP space.", "section": "4.4 Discussion on DITS"}, {"figure_path": "SrQua0ATRZ/figures/figures_17_1.jpg", "caption": "Figure 5: Similarity value subtraction between DITS and \u201cDITS fix CLIP\u201d (relevant pairs). DITS enables higher similarity values, yielding positive-valued histogram shown above. By comparison, DITS can effectively bridge the modality gap by aligning the CLIP embedding space.", "description": "This figure shows the distribution of the difference in similarity scores between two methods: DITS and DITS with a fixed CLIP embedding space. The positive values in the histogram indicate that DITS improves the similarity between text and video embeddings, particularly for relevant pairs.  This aligns with the paper's central claim that DITS effectively reduces the modality gap by aligning the embeddings in the CLIP space.", "section": "4.4 Discussion on DITS"}, {"figure_path": "SrQua0ATRZ/figures/figures_17_2.jpg", "caption": "Figure 2: Overview of DITS. Given the video embedding v from Fe(\u00b7) and the text embedding t from G(), DITS performs a T'-steps sampling by starting from t, to gradually approach v (as exampled by a relevant pair t(1), v(1)) with the aligned embedding v\u00b9 at the timestamp t = 0. Meanwhile the progressive alignment, DITS intrinsically approximates the distribution of the modality gap \u03b4 ~ Pt at each timestamp t = T', ..., 1. The contrastive loss is adopted to guide the alignment and modeling, with parameters: \u0398 = {\u03b8, \u03c6, \u03b3}. We devise \u03b5\u03b3(\u00b7) upon DiT [Peebles and Xie, 2023].", "description": "This figure illustrates the Diffusion-Inspired Truncated Sampler (DITS) process for text-video retrieval. Starting from the text embedding, DITS progressively aligns it with the video embedding using a truncated diffusion process guided by a contrastive loss. The figure depicts the different components involved in the process, including the video and text encoders, the alignment network, and the contrastive loss calculation.  It also shows how DITS models the modality gap between text and video embeddings over time.", "section": "3 Method"}]