[{"figure_path": "SrQua0ATRZ/tables/tables_6_1.jpg", "caption": "Table 1: Text-to-video retrieval performance on MSRVTT [Xu et al., 2016] and LSMDC [Rohrbach et al., 2015]. Bold denotes the best performance. \"-\" denotes that the result is unavailable.", "description": "This table presents a comparison of various text-to-video retrieval methods on two benchmark datasets: MSRVTT and LSMDC.  The metrics used for comparison include Recall at ranks 1, 5, and 10 (R@1, R@5, R@10), Median Rank (MdR), and Mean Rank (MnR).  The table shows the performance of different methods using two different CLIP backbones (ViT-B/32 and ViT-B/16).  Bold values indicate the best performing method for each metric and dataset/backbone combination.  A '-' indicates that the result is not available for a specific method.", "section": "4 Experiment"}, {"figure_path": "SrQua0ATRZ/tables/tables_6_2.jpg", "caption": "Table 2: Text-to-video retrieval performance on DiDeMo [Anne Hendricks et al., 2017] and VATEX [Wang et al., 2019]. Bold denotes the best performance. \"-\" denotes that the result is unavailable.", "description": "This table presents the performance comparison of different text-to-video retrieval methods on two benchmark datasets: DiDeMo and VATEX.  The metrics used for comparison include Recall@1, Recall@5, Recall@10, Median Rank (MdR), and Mean Rank (MnR).  The best performance for each metric is highlighted in bold.  The '-' symbol indicates that results were unavailable for a specific method and dataset.", "section": "4 Experiment"}, {"figure_path": "SrQua0ATRZ/tables/tables_7_1.jpg", "caption": "Table 3: Text-to-video comparisons on Charades [Sigurdsson et al., 2016]. Bold denotes the best.", "description": "This table presents the results of text-to-video retrieval experiments conducted on the Charades dataset.  The table compares several methods, showing their performance using the metrics R@1, R@5, R@10, MdR, and MnR.  The best performance for each metric is highlighted in bold.  The results are broken down by the CLIP model variant used (CLIP-ViT-B/32 and CLIP-ViT-B/16).", "section": "4.2 Performance Comparison"}, {"figure_path": "SrQua0ATRZ/tables/tables_7_2.jpg", "caption": "Table 4: Ablation study of the proposed DITS on MSRVTT-1k. We adopt X-Pool [Gorti et al., 2022] as the \"Baseline\". \"Diffusion\" denotes the Diffusion-based alignment method in Section 3.2.", "description": "This ablation study analyzes different components of the proposed Diffusion-Inspired Truncated Sampler (DITS) method on the MSRVTT-1k dataset. It compares the performance of DITS against a baseline (X-Pool) and explores variations in the alignment method (fixed prior vs. diffusion), the loss function (L2 vs. contrastive loss), and the use of truncation. The results highlight the contribution of each component to the overall performance gains achieved by DITS.", "section": "4.3 Ablation Study"}, {"figure_path": "SrQua0ATRZ/tables/tables_8_1.jpg", "caption": "Table 5: Discussion on DITS. Highlighted settings are adopted for the benchmark comparison. (a) Discussion on the timestamps (T') on MSRVTT. (b) The effect of DITS on CLIP. We study the DITS alignment in the fixed CLIP space and upon a learnable CLIP model. By aligning the embedding, DITS guides the CLIP learning and improves the space.", "description": "This table presents an ablation study of the proposed DITS method on the MSRVTT dataset.  It shows the impact of different numbers of timestamps (T') on the model's performance, highlighting the optimal value for T'.  Additionally, it compares the performance of DITS when used to align embeddings within a fixed CLIP embedding space versus when it is jointly trained with CLIP.  The results show that jointly training DITS with CLIP leads to the best performance and improves the structure of the CLIP embedding space.", "section": "4.4 Discussion on DITS"}, {"figure_path": "SrQua0ATRZ/tables/tables_9_1.jpg", "caption": "Table 6: Discussion on DITS. Highlighted settings are adopted for the benchmark comparison. (a) Different model conditions for DITS on MSRVTT. (b) Different types of the modality gap \u03b4 on MSRVTT.", "description": "This table shows the ablation study of different model conditions (with text, video, both, or no condition) and different modality gap definitions (\u03b4=v-t and \u03b4=t-v) on the performance of the Diffusion-Inspired Truncated Sampler (DITS) method on the MSRVTT dataset.  The results demonstrate that using text as both the starting point and condition in DITS leads to the best performance and using v-t as the modality gap results in significantly better performance compared to t-v.", "section": "4.4 Discussion on DITS"}, {"figure_path": "SrQua0ATRZ/tables/tables_14_1.jpg", "caption": "Table 4: Ablation study of the proposed DITS on MSRVTT-1k. We adopt X-Pool [Gorti et al., 2022] as the \"Baseline\". \"Diffusion\" denotes the Diffusion-based alignment method in Section 3.2.", "description": "This ablation study shows the performance of different variations of the proposed method, DITS, on the MSRVTT-1k dataset.  The baseline is X-Pool. It compares the baseline against methods using fixed priors (with different variance settings), a pretrained diffusion model (with fine-tuning and without), and the full DITS approach.  The metrics used are R@1, R@5, R@10, MdR, and MnR, which are standard retrieval metrics.", "section": "4.3 Ablation Study"}, {"figure_path": "SrQua0ATRZ/tables/tables_15_1.jpg", "caption": "Table 5a: Discussion on DITS. Highlighted settings are adopted for the benchmark comparison. (a) Discussion on the timestamps (T') on MSRVTT.", "description": "This table shows the impact of varying the number of truncated timestamps (T') in the Diffusion-Inspired Truncated Sampler (DITS) on the text-to-video retrieval performance.  The results are presented using several metrics, including Recall@1, Recall@5, Recall@10, Median Rank (MdR), and Mean Rank (MnR), evaluated on the MSRVTT dataset.  It demonstrates how the choice of T' significantly affects performance; there is an optimal value for T' that balances the accuracy of alignment with the computational efficiency of the model.", "section": "4.4 Discussion on DITS"}, {"figure_path": "SrQua0ATRZ/tables/tables_15_2.jpg", "caption": "Table 9: Modality gap (measured by L\u2081-norm), similarity, and performance change discussion.", "description": "This table presents a comparison between two approaches: DITS fix CLIP and DITS (Ours).  The comparison is based on three metrics: Averaged Modality Gap (lower is better), Averaged Similarity (higher is better), and R@1 (Recall at rank 1, higher is better). The results show that DITS (Ours) significantly outperforms DITS fix CLIP across all three metrics, indicating a more effective modality gap reduction and improved retrieval performance.", "section": "4.4 Discussion on DITS"}, {"figure_path": "SrQua0ATRZ/tables/tables_16_1.jpg", "caption": "Table 10: Training resource usage comparison with on MSRVTT dataset.", "description": "This table compares the training resource usage, including GPU memory (MB), GPU request, and training time (h), of different methods on the MSRVTT dataset.  It provides a comparison of the computational efficiency and resource demands of various approaches to text-video retrieval, allowing for a relative assessment of their scalability and feasibility for different computational settings.", "section": "4.1 Experimental Settings"}, {"figure_path": "SrQua0ATRZ/tables/tables_16_2.jpg", "caption": "Table 11: Inference time efficiency and GPU usage comparison on MSRVTT dataset.", "description": "This table compares the inference time and GPU memory usage of different text-video retrieval methods on the MSRVTT dataset.  It shows that DITS, while having comparable GPU memory usage to other top-performing methods, demonstrates faster inference time.", "section": "4 Experiment"}]