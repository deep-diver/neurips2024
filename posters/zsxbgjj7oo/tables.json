[{"figure_path": "zsXbGJJ7Oo/tables/tables_6_1.jpg", "caption": "Table 1: Results of semantic segmentation and object detection. Best results are highlighted in bold, with '-' denoting mAP values < 1%. Methods with * use disease-level annotations. '/' indicates object detection not deployable with encoder-decoder architecture. The MedKLIP results in this table differ from the original work [5] because MedKLIP fine-tuned the encoder in their original study, whereas other methods froze the encoder. To ensure fairness, we reimplemented MedKLIP with the frozen encoder for comparison in this table. Additionally, for a fair comparison specifically with MedKLIP, we compare G2D with MedKLIP under its original configuration in Tab 7 and Sec A.5.", "description": "This table compares the performance of G2D against other state-of-the-art (SOTA) medical VLP approaches on semantic segmentation and object detection tasks.  It shows Dice scores for semantic segmentation and mAP for object detection across three datasets (SIIM, RSNA, Object CXR) and various training data percentages (1%, 10%, 100%).  Note that some methods could not perform object detection due to their architecture.  The table also addresses differences in how MedKLIP fine-tuned its models compared to the others, aiming for a fair comparison.", "section": "4.2 Performance on Visual Localisation Tasks"}, {"figure_path": "zsXbGJJ7Oo/tables/tables_7_1.jpg", "caption": "Table 2: Comparison between G2D (ours) and various other medical VLP methods in vision-language understanding tasks, with the best results emphasized in bold. Methods marked with * utilize extra annotated data during pre-training. '/' indicates that the original work did not report the results. Notably, KAD [21] does not report ACC for the CheXpert dataset.\n(a) Results of zero-shot visual grounding task.\n(b) Results of zero-shot image classification task.", "description": "This table compares the performance of the proposed G2D model against other state-of-the-art (SOTA) medical vision-language pre-training (VLP) approaches on two vision-language understanding tasks: zero-shot visual grounding and zero-shot image classification.  The results are shown for several datasets (SIIM, RSNA, CXR14, CheXpert).  Note that some methods used extra annotated data during pre-training, which is indicated with an asterisk (*).  The table highlights the superior performance of G2D across these tasks and datasets.", "section": "4.3 Performance on Vision-Language Understanding"}, {"figure_path": "zsXbGJJ7Oo/tables/tables_7_2.jpg", "caption": "Table 2: Comparison between G2D (ours) and various other medical VLP methods in vision-language understanding tasks, with the best results emphasized in bold. Methods marked with * utilize extra annotated data during pre-training. '/' indicates that the original work did not report the results. Notably, KAD [21] does not report ACC for the CheXpert dataset.\n(a) Results of zero-shot visual grounding task.\n(b) Results of zero-shot image classification task.", "description": "This table compares the performance of the proposed G2D model against other state-of-the-art (SOTA) medical vision-language pre-training (VLP) approaches on two vision-language understanding tasks: zero-shot visual grounding and zero-shot image classification.  The results are shown for several datasets (SIIM, RSNA, CXR14, and CheXpert).  Note that some methods used extra annotated data during pre-training, and some results were not reported in the original papers.", "section": "4.3 Performance on Vision-Language Understanding"}, {"figure_path": "zsXbGJJ7Oo/tables/tables_7_3.jpg", "caption": "Table 1: Results of semantic segmentation and object detection. Best results are highlighted in bold, with '-' denoting mAP values <1%. Methods with * use disease-level annotations. '/' indicates object detection not deployable with encoder-decoder architecture. The MedKLIP results in this table differ from the original work [5] because MedKLIP fine-tuned the encoder in their original study, whereas other methods froze the encoder. To ensure fairness, we reimplemented MedKLIP with the frozen encoder for comparison in this table. Additionally, for a fair comparison specifically with MedKLIP, we compare G2D with MedKLIP under its original configuration in Tab 7 and Sec A.5.", "description": "This table compares the performance of G2D against other state-of-the-art (SOTA) methods on semantic segmentation and object detection tasks using three different percentages of training data (1%, 10%, and 100%).  It shows the Dice score for segmentation and mean Average Precision (mAP) for object detection.  Important notes in the caption address differences in training methodology between G2D and the compared methods (especially MedKLIP), ensuring a fair comparison.", "section": "4.2 Performance on Visual Localisation Tasks"}, {"figure_path": "zsXbGJJ7Oo/tables/tables_8_1.jpg", "caption": "Table 4: Linear classification results for CheXpert, RSNA, and COVIDx datasets with 1%, 10%, and 100% training data. The best results are highlighted in bold. Methods with * leverage disease-level annotations for pre-training. The evaluation metric follows [9].", "description": "This table presents the results of linear image classification on three datasets (CheXpert, RSNA, and COVIDx) using different training data percentages (1%, 10%, and 100%).  It compares the performance of G2D against other state-of-the-art methods, highlighting the superior performance of G2D, especially with limited training data.  Note that some methods use extra disease-level annotations during pre-training.", "section": "4.4 Performance on Visual Recognition Tasks"}, {"figure_path": "zsXbGJJ7Oo/tables/tables_8_2.jpg", "caption": "Table 5: Results of various ablation experiments. The best results are bolded.", "description": "This table presents the results of ablation studies conducted to analyze the impact of different design choices in the G2D model.  Specifically, it examines the effect of different decoder loss functions (None, Reconstruction, Pseudo Seg), various thresholds for pseudo mask generation, the number of dimensions in the projectors, the aggregation method for multi-head attention maps, and the refinement steps applied to the pseudo segmentation masks.  The results highlight the optimal configurations for each component of the G2D model that yield the best performance across three downstream tasks (SIIM Dice, RSNA mAP, CXR14 AUC).", "section": "4.5 Ablation Studies"}, {"figure_path": "zsXbGJJ7Oo/tables/tables_13_1.jpg", "caption": "Table 6: Details on Data Split: The symbol '/' denotes that training/validation data is not required for the zero-shot tasks.", "description": "This table shows the data split for various downstream tasks used in the paper's experiments.  It includes the dataset used, the references for the data split, and the number of samples allocated for training, validation, and testing. Note that for zero-shot tasks, the training and validation sets are not applicable, hence the '/'.", "section": "4 Experiments and Analysis"}, {"figure_path": "zsXbGJJ7Oo/tables/tables_14_1.jpg", "caption": "Table 7: Performance of CXR14 Classification Fine-Tuning and Segmentation Results on SIIM and RSNA using the MedKLIP Setting [5].", "description": "This table compares the performance of MedKLIP and G2D on three downstream tasks: CXR14 classification (AUC), RSNA segmentation (Dice), and SIIM segmentation (Dice).  The results are broken down by the percentage of training data used (1%, 10%, and 100%).  The comparison is specifically done using the configuration and data splits as defined in the MedKLIP paper [5], to ensure a fair and direct comparison under identical conditions.", "section": "A.5 Comparison under MedKLIP Configuration"}, {"figure_path": "zsXbGJJ7Oo/tables/tables_15_1.jpg", "caption": "Table 8: Perturbation on Pseudo Masks.", "description": "This table presents the ablation study results of using pseudo masks with and without semantic meaning in the pretext task.  The results demonstrate the importance of the semantic information provided by pseudo masks generated using the attention mechanism for improving the performance in visual localization and recognition tasks.  The model using pseudo masks with semantic meaning significantly outperforms the model using pseudo masks without semantic meaning (shuffled).", "section": "4.5 Ablation Studies"}]