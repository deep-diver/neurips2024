[{"type": "text", "text": "DECRL: A Deep Evolutionary Clustering Jointed Temporal Knowledge Graph Representation Learning Approach ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Qian Chen, Ling Chen\u2217 State Key Laboratory of Blockchain and Data Security College of Computer Science and Technology Zhejiang University {qianchencs,lingchen}@cs.zju.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Temporal Knowledge Graph (TKG) representation learning aims to map temporal evolving entities and relations to embedded representations in a continuous low-dimensional vector space. However, existing approaches cannot capture the temporal evolution of high-order correlations in TKGs. To this end, we propose a Deep Evolutionary Clustering jointed temporal knowledge graph Representation Learning approach (DECRL). Specifically, a deep evolutionary clustering module is proposed to capture the temporal evolution of high-order correlations among entities. Furthermore, a cluster-aware unsupervised alignment mechanism is introduced to ensure the precise one-to-one alignment of soft overlapping clusters across timestamps, thereby maintaining the temporal smoothness of clusters. In addition, an implicit correlation encoder is introduced to capture latent correlations between any pair of clusters under the guidance of a global graph. Extensive experiments on seven real-world datasets demonstrate that DECRL achieves the state-of-the-art performances, outperforming the best baseline by an average of $9.53\\%$ , $12.98\\%$ , $10.42\\%$ , and $14.68\\%$ in MRR, Hits $@1$ , Hits $@3$ , and Hits $@10$ , respectively. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Temporal Knowledge Graphs (TKGs) are collections of human temporal evolving knowledge [35], which are widely utilized in various fields, e.g., information retrieval [19], natural language understanding [3], and recommendation systems [32]. A TKG represents events in the form of quadruples $(s,r,o,t)$ , where $s$ and $o$ denote the subject and object entities, respectively, $r$ denotes the relation between $s$ and $o$ , and $t$ represents the timestamp [35]. Event prediction in TKGs is an important task that predicts future events according to historical events [17]. TKG representation learning aims to map temporal evolving entities and relations to embedded representations in a continuous low-dimensional vector space. Due to the complex temporal dynamics and multi-relations within TKGs, TKG representation learning poses great challenges to the research community. ", "page_idx": 0}, {"type": "text", "text": "In recent years, many TKG representation learning approaches have used graph neural networks [33] (GNNs) to model pairwise entity correlations [14, 1]. Furthermore, some researchers have leveraged derived structures, e.g., communities [39], entity groups [28], and hypergraphs [26, 29], to model high-order correlations among entities, i.e., the simultaneous correlations among three or more entities. ", "page_idx": 0}, {"type": "text", "text": "These approaches, however, lack the capability to capture the temporal evolution of high-order correlations in TKGs. As a kind of dynamic graph, TKGs inherently feature the temporal evolution of high-order correlations. For example, within TKGs focused solely on countries, a country entity may be affiliated with different international political organizations at different timestamps, with these organizations experiencing membership adjustment over time. In addition, the membership adjustment in international political organizations does not shift suddenly. Typically, numerous events occur before the formal adjustment of their membership, gradually pushing memberships away from or drawing them closer to international political organizations. For instance, the UK\u2019s official departure from the European Union (EU) was preceded by numerous events that incrementally estranged it from the EU. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To address the aforementioned deficiencies, a Deep Evolutionary Clustering jointed temporal knowledge graph Representation Learning approach (DECRL) is proposed for event prediction in TKGs. To the best of our knowledge, DECRL is the first work that integrates deep evolutionary clustering approaches into TKGs, which jointly optimizes TKG representation learning with evolutionary clustering to capture the temporal evolution of high-order correlations. Our main contributions are outlined as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose a deep evolutionary clustering module to capture the temporal evolution of high-order correlations among entities, where clusters represent the high-order correlations between multiple entities. Furthermore, a cluster-aware unsupervised alignment mechanism is introduced to ensure precise one-to-one alignment of soft overlapping clusters across timestamps, maintaining the temporal smoothness of clusters over successive timestamps. \u2022 We propose an implicit correlation encoder to capture latent correlations between any pair of clusters, which defines the interaction intensities between clusters to form a cluster graph. In addition, a global graph, constructed from all events of training set, is introduced to guide the assignment of different interaction intensities to different cluster pairs. \u2022 We evaluate DECRL on seven real-world datasets. The experimental results demonstrate that DECRL achieves the state-of-the-art (SOTA) performance. It outperforms the best baseline by an average of $9.53\\%$ , $12.98\\%$ , $10.42\\%$ , and $14.68\\%$ in MRR, Hits $@1$ , Hits $@3$ , and Hits $@10$ , respectively. ", "page_idx": 1}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 TKG representation learning approach ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "GNNs and variants of recurrent neural networks (RNNs) are commonly integrated to model graph structural information and temporal dependency within TKGs, respectively [1, 34, 16, 14]. For example, TeMP [34] and RE-GCN [16] both utilize relation-aware GCN [24] to model the influence of neighbor entities and apply GRUs to model the temporal dependency. TiRGN [14] employs multirelational GNNs to model graph structural information, and utilizes GRUs to capture the temporal dependency across sequential timestamps. However, these approaches often resort to stacking multiple layers to model the influence of distant neighbors, which may lead to the over-smoothing problem. ", "page_idx": 1}, {"type": "text", "text": "Recent research advancements have introduced paths [15, 8, 18] to enhance the modeling capability of the latent pairwise correlations between entities. For example, xERTE [8] utilizes a time-aware graph attention mechanism to obtain and exploit local subgraphs. TLogic [18] employs a timebased random walk strategy to acquire logical paths. Furthermore, some researchers have leveraged derived structures, e.g., communities [39], entity groups [28], and hypergraphs [29], to model highorder correlations among entities. For example, EvoExplore [39] utilizes a temporal point process enhanced by a hierarchical attention mechanism to establish dynamic communities for modeling latent correlations among entities. DHyper [29] utilizes hypergraph neural networks to model high-order correlations among entities and among relations. ", "page_idx": 1}, {"type": "text", "text": "While the promising results of introducing derived structures, existing approaches lack the capability to capture the temporal evolution of high-order correlations in TKGs. To address this gap, we propose DECRL, which is the first work to integrate deep evolutionary clustering approaches into TKGs. DECRL jointly optimizes TKG representation learning with evolutionary clustering to capture the temporal evolution of high-order correlations. ", "page_idx": 1}, {"type": "text", "text": "2.2 Deep evolutionary clustering approach ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Existing deep evolutionary clustering approaches employ different strategies for discovering stable clusters [7, 21, 36, 20, 38, 37]. For example, DYNMOGA [7] employs a deep evolutionary clustering approach designed to identify evolving communities within dynamic networks, which treats the process as a multi-objective optimization problem aimed at cluster stability. sE-NMF [21] implements linear fusion of adjacency matrices across timestamps to enhance the temporal stability of clusters. Both DNETC [36] and CDNE [20] use constructed temporal smoothness loss functions to ensure a consistent and stable evolution of clustering results through successive timestamps. TRNNGCN [37] introduces a decay matrix to assess the influence of historical clustering results, thereby stabilizing the current clustering configuration. ", "page_idx": 2}, {"type": "text", "text": "These approaches straightforwardly incorporate prior clustering results with current timestamp results over time under the presumption that clustering results are relatively stable across different timestamps. However, the nuances between similar clusters are difficult to recognize in this way. In TKG representation learning, it is necessary to discern the subtle differences in high-order correlations. To bridge this gap, in our work, a cluster-aware unsupervised alignment mechanism is introduced, which ensures the precise one-to-one alignment of soft overlapping clusters across timestamps, thereby maintaining the temporal smoothness of clusters over successive timestamps. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Definition 1 (TKG). A TKG is a set of events formalized as $\\mathcal{G}=\\{(s,r,o,t)\\mid s,o\\in\\mathcal{E},r\\in\\mathcal{R},t\\in\\mathcal{T}\\}$ , where $\\mathcal{E},\\ \\mathcal{R}$ , and $\\tau$ denote the set of entities, relations, and timestamps, respectively. $\\mathcal{G}^{t}$ denotes the set of events at timestamp t. ", "page_idx": 2}, {"type": "text", "text": "Definition 2 (Entity Graph). The entity graph is constructed based on $\\mathcal{G}^{t}$ , which is a multi-relational graph, and can be denoted as $G_{\\mathrm{e}}^{t}=(V_{\\mathrm{e}}^{\\dot{t}},\\dot{E}_{\\mathrm{e}}^{t})$ , where $V_{\\mathrm{e}}^{t}$ and $E_{\\mathrm{e}}^{t}$ denote the set of nodes and edges of the entity graph at timestamp $t$ , respectively. The nodes in $V_{\\mathrm{e}}^{t}$ represent entities, while the edges in $E_{\\mathrm{e}}^{t}$ represent relations between entities at timestamp $t$ . ", "page_idx": 2}, {"type": "text", "text": "Definition 3 (Cluster Graph). The cluster graph is constructed based on $G_{\\mathrm{e}}^{t}$ , which is a fully connected graph, and can be denoted as $G_{\\mathrm{c}}^{t}=(\\bar{V}_{\\mathrm{c}}^{t},E_{\\mathrm{c}}^{t})$ , where $V_{\\mathrm{c}}^{t}$ and $E_{\\mathrm{c}}^{t}$ denote the set of nodes and edges of the cluster graph at timestamp $t$ , respectively. The nodes and edges of cluster graphs represent clusters and the latent correlations between them, respectively, where each cluster represents the high-order correlations among entities. ", "page_idx": 2}, {"type": "text", "text": "Task (Event Prediction). Given a query $(s,?,o,t)$ , the event prediction task in TKGs aims to predict the conditional probability of all relations with the subject entity $s$ , the object entity $o$ , and the historical event sets $\\mathcal{G}^{1:T-\\tilde{1}}$ , which can be denoted as $p(\\hat{\\pmb{r}}|\\bar{s},o,\\mathcal{G}^{1:T-1})$ , where $T$ denotes the number of historical timestamps. ", "page_idx": 2}, {"type": "text", "text": "4 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "4.1 Overview ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The proposed DECRL approach is illustrated in Figure 1. At each timestamp, entity and relation representations updated by the cluster graph message passing module are merged with input representations from the previous timestamp using a time residual gate, serving as the input for the current timestamp. The multi-relational interactions among entities are modeled by the relation-aware GCN. The deep evolutionary clustering module captures the temporal evolution of high-order correlations among entities, maintaining the temporal smoothness of clusters over successive timestamps through an unsupervised alignment mechanism. The implicit correlation encoder captures the latent correlations between any pair of clusters, enabling message passing within the cluster graph to update entity representations. Finally, the attentive temporal encoder captures temporal dependencies among entity and relation representations across timestamps, integrating them with temporal information for future event prediction. The computational complexity of DECRL can be found in Appendix A.1. ", "page_idx": 2}, {"type": "text", "text": "4.2 Evolutionary clustering module ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The entity graph $G_{\\mathrm{e}}^{t}=(V_{\\mathrm{e}}^{t},E_{\\mathrm{e}}^{t})$ is constructed based on the historical events at timestamp $t$ , which can be used to capture the structural dependency among concurrent events. Since the entity graph ", "page_idx": 2}, {"type": "image", "img_path": "V42zfM2GXw/tmp/62af72ed8a4c76d21d5ade0be3b76e97eb29ab1c1ef7e5f02ddca3a077870a90.jpg", "img_caption": ["Figure 1: The framework of DECRL. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "is a multi-relational graph, relation-aware GCN [24] is utilized to model the structural dependency, which is formulated as: ", "page_idx": 3}, {"type": "equation", "text": "$$\nh_{0}^{t,l+1}=\\operatorname{RReLU}\\left(\\frac{1}{d_{0}}\\sum_{(s,r),\\exists(s,r,o)\\in\\mathcal{G}^{t}}W_{1}(h_{s}^{t,l}+h_{\\mathrm{r}}^{t,l})+W_{2}h_{0}^{t,l}\\right)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $h_{\\mathrm{s}}^{t,l},\\,h_{\\mathrm{o}}^{t,l}$ , and $h_{\\mathrm{r}}^{t,l}$ denote the $l^{t h}$ layer representations of subject entity $s$ , object entity $o$ and relation $r$ at timestamp $t$ , respectively. $W_{1}$ and $W_{2}$ are learnable parameters for aggregating neighbor entity representations and self-loop, respectively. $d_{\\mathrm{o}}$ denotes the in-degree of object entity $o$ Initially, entities are assigned random representations based on their in-degree, such that entities with the same in-degree have the same initial representations, thereby efficiently accelerating the update process of entity representations. Relations are given with the random initial representations. For simplicity, the subscript $l$ of variables is omitted in the following sections without causing ambiguity. ", "page_idx": 3}, {"type": "text", "text": "The representation of the relation $r$ at timestamp $t$ is influenced by the $r$ -related entity representations at timestamp $t$ and its representation at timestamp $t-1$ . The updating of the relation representation is formulated as $h_{\\mathrm{r}}^{t}=\\mathrm{pooling}[e_{\\gamma_{\\mathrm{r}}^{t}}^{t};h_{\\mathrm{r}}^{t-1}]$ , where $[;]$ denotes the concatenation operation. pooling denotes the mean pooling operation. $e_{\\mathcal{V}_{\\mathrm{r}}^{t}}^{t}$ denotes $r$ -related entity representations at timestamp $t$ where $\\mathcal{V}_{\\mathrm{r}}^{t}=\\{i|(i,r,o,t)\\,o r\\,(s,r,i,t)\\in\\dot{\\mathcal{G}}^{t}\\}$ . ", "page_idx": 3}, {"type": "text", "text": "A country entity may be affiliated with different international political organizations at different timestamps within TKGs. Thus, the fuzzy c-means clustering method [23] is utilized to obtain the soft overlapping clusters, which outputs the membership matrix between entities and clusters by optimizing the object function as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\nJ=\\sum_{i=1}^{N_{\\mathrm{e}}}\\sum_{j=1}^{N_{\\mathrm{c}}}u_{i,j}^{t,m}\\left(1-\\frac{\\langle e_{i}^{t},\\mu_{j}^{t}\\rangle}{\\|e_{i}^{t}\\|\\|\\mu_{j}^{t}\\|}\\right),s.t.\\sum_{j=1}^{N_{\\mathrm{c}}}u_{i,j}^{t,m}=1,0<\\sum_{i=1}^{N_{\\mathrm{c}}}u_{i,j}^{t,m}<N_{\\mathrm{e}}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $N_{\\mathrm{e}}$ and $N_{\\mathrm{c}}$ are the number of entities and clusters, respectively. $u_{i,j}^{t}$ denotes the membership degree of the entity representation $e_{i}^{t}$ to the cluster centroid $\\mu_{j}^{t}$ at timestamp $t,\\,m$ denotes the fuzzy ", "page_idx": 3}, {"type": "text", "text": "smoothing hyper-parameter, which is typically set to a value greater than 1. $\\langle\\cdot\\rangle$ denotes the dot product. $\\Vert\\cdot\\Vert$ denotes the norm of a vector. Then, the representation of clusters is formulated as: ", "page_idx": 4}, {"type": "equation", "text": "$$\nc_{j}^{t}=\\sum_{e_{i}^{t}\\in V_{\\mathrm{e}}^{t}}u_{i,j}^{t}e_{i}^{t},\\quad u_{i,j}^{t}=\\frac{\\left(\\frac{\\langle e_{i}^{t},\\mu_{j}^{t}\\rangle}{\\|e_{i}^{t}\\|\\|\\mu_{j}^{t}\\|}\\right)^{\\frac{1}{m-1}}}{\\displaystyle\\sum_{k=1}^{N_{\\mathrm{c}}}\\left(\\frac{\\langle e_{i}^{t},\\mu_{k}^{t}\\rangle}{\\|e_{i}^{t}\\|\\|\\mu_{k}^{t}\\|}\\right)^{\\frac{1}{m-1}}}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\pmb{c}_{j}^{t}$ denotes the representation of cluster $j$ at timestamp $t$ , weighted by the membership degree of different entities to the cluster. ", "page_idx": 4}, {"type": "text", "text": "The maximum weight matching can be found in polynomial time using the Hungarian algorithm [11]. Herein, a cluster-aware Hungarian matching algorithm is introduced to ensure a smooth one-to-one alignment of clusters across timestamps. The cluster-aware Hungarian matching algorithm quantifies the similarity between clusters of consecutive timestamps, i.e., $t-1$ and $t$ , which creates an affinity matrix $A$ , where each element $a_{j,k}$ represents the similarity between the cluster $j$ at timestamp $t-1$ and the cluster $k$ at timestamp $t$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\na_{j,k}=\\cos(\\pmb{c}_{j}^{t-1},\\pmb{c}_{k}^{t})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "With the affinity matrix established, the cluster-aware Hungarian algorithm seeks an optimal assignment that maximizes the sum of similarities across matched clusters. It is formulated as an optimization problem: $\\begin{array}{r}{\\operatorname*{max}_{\\pi}\\sum_{j=1}^{N_{\\mathrm{c}}}a_{j,\\pi(j)}}\\end{array}$ , where $\\pi$ is the permutation function representing the one-to-one alignment between clusters of consecutive timestamps. Therefore, $\\pi(j)$ denotes the index of the cluster at timestamp $t$ matched with the cluster $j$ at timestamp $t-1$ . ", "page_idx": 4}, {"type": "text", "text": "Subsequently, the fused cluster representations are computed by taking a weighted combination of the cluster representations at consecutive timestamps, which is formulated as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\pmb{c}_{j}^{t}=\\beta\\pmb{c}_{j}^{t-1}+(1-\\beta)\\pmb{c}_{\\pi(j)}^{t}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\beta$ is the fusion weight, which indicates the relative contribution of each cluster representation at consecutive timestamps to the fused cluster representation. ", "page_idx": 4}, {"type": "text", "text": "What\u2019s more, it is essential to ensure the temporal smoothness of these cluster representations across consecutive timestamps. To this end, a temporal smoothness loss is introduced to penalize significant deviations of cluster representations across consecutive timestamps, which is formulated as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{temporal}}=\\sum_{t=1}^{T-1}\\sum_{j=1}^{N_{\\mathrm{c}}}\\left(1-\\frac{\\langle c_{j}^{t-1},c_{j}^{t}\\rangle}{\\|c_{j}^{t-1}\\|\\|c_{j}^{t}\\|}\\right)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "4.3 Cluster graph message passing module ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Due to the intricate relationships and alliances formed among international political organizations, it is crucial to capture the latent correlations between clusters for event prediction. Herein, an implicit correlation encoder is proposed to capture the latent correlations between any pair of clusters. The cluster graph is designed to be a fully connected graph to avoid missing any relevant information between clusters. The representations of the latent correlations are formulated as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\pmb{s}_{i,j}^{t}=\\operatorname{ReLU}\\left(\\varphi(\\pmb{c}_{i}^{t},\\pmb{c}_{j}^{t})\\right)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\boldsymbol{c}_{i}^{t}$ and $c_{j}^{t}$ are representations of the cluster $i$ and $j$ at timestamp $t$ , respectively. $\\varphi$ is the transformation function, which is implemented by the multi-layer perceptron. ", "page_idx": 4}, {"type": "text", "text": "It is worth noting that clusters exhibit varying degrees of interaction, characterized by different intensities of latent correlations. Thus, we quantify the intensity of latent correlations between clusters, which is formulated as: ", "page_idx": 4}, {"type": "equation", "text": "$$\nq_{i,j}^{t}=\\sigma\\left(\\mathrm{Conv}(s_{i,j}^{t})\\right)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\sigma$ is the sigmoid function, which ensures the resulting correlation intensity bounded between 0 and 1. $\\mathrm{Conv}(\\cdot)$ represents the convolution operation. ", "page_idx": 4}, {"type": "text", "text": "The intensity of the latent correlations between clusters varies in the short term and intensifies over time; a higher frequency of interactions indicates a stronger latent correlation. To this end, we construct a global graph, which is a static graph composed of all events from the training set. The spectral clustering approach is then applied to this global graph to obtain global clusters cglobal. The similarity between global clusters is used to enhance the intensity of latent correlations between cluster pairs, which is formulated as: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\widehat{q}_{i,j}^{t}=q_{i,j}^{t}\\times m_{i,j}^{t},\\ \\ m_{i,j}^{t}=\\frac{\\langle c_{\\pi^{t}(i)}^{\\mathrm{global}},c_{\\pi^{t}(j)}^{\\mathrm{global}}\\rangle}{\\lVert c_{\\pi^{t}(i)}^{\\mathrm{global}}\\rVert\\lVert c_{\\pi^{t}(j)}^{\\mathrm{global}}\\rVert}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $c_{\\pi^{t}(i)}^{\\mathrm{global}}$ nd cglobal a are global clusters matched with the cluster $i$ and $j$ at timestamp $t$ , respectively. $m_{i,j}^{t}$ is the similarity between the global cluster $i$ and $j$ at timestamp $t$ . $\\widehat{q}_{i,j}^{t}$ is the enhanced intensity of latent correlations, which also integrates structural information from the global graph. ", "page_idx": 5}, {"type": "text", "text": "In the message passing process of the cluster graph, the stronger the intensity of the latent correlation, the more critical the latent correlation is, and vice versa. The aggregation operation is formulated as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\pmb{v}_{c_{i}}^{t}=\\sum_{i\\neq j}\\widetilde{q}_{i,j}^{t}\\pmb{s}_{i,j}^{t}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\widehat{q}_{i,j}^{t}$ and $\\pmb{s}_{i,j}^{t}$ denote the enhanced intensity and the representation of latent correlation calculated by Equation 9 and Equation 7, respectively. Then, the representation of clusters is updated through: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\widehat{\\mathbfit{c}}_{i}^{t}=\\varphi(\\mathbf{v}_{c_{i}}^{t},\\mathbf{c}_{i}^{t})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "To implement information transfer from clusters to individual entities, we use the membership matrix from Section 4.2. Entity representations are updated based on their associated cluster representations, which are formulated as $\\begin{array}{r}{e_{i}^{\\prime{t}}=\\sum_{j=1}^{N_{\\mathrm{c}}}{\\pmb{u}}_{i,j}^{t}\\widehat{\\pmb{c}}_{j}^{t}}\\end{array}$ , where $N_{\\mathrm{c}}$ denotes the number of clusters. ", "page_idx": 5}, {"type": "text", "text": "4.4 Time residual gate ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The time residual gate is introduced to combine updated entity and relation representations with input representations of the current timestamp through a weighted mechanism. This approach preserves inherent characteristics while capturing the temporal evolution of entities and relations. For simplicity, the subscripts $i$ and $j$ of variables are omitted in the following sections without causing ambiguity. The final updated representations at timestamp $t$ are formulated as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\pmb{H}^{t}=\\pmb{X}^{t}\\otimes\\pmb{H}_{\\theta}^{t}+(1-\\pmb{X}^{t})\\otimes\\pmb{H}^{t-1},\\ \\ \\pmb{X}^{t}=\\sigma(\\pmb{W}_{3}\\pmb{H}^{t-1}+\\pmb{b})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $H_{\\boldsymbol{\\theta}}^{t}$ denotes the entity or relation representations updated by the cluster graph message passing module at timestamp $t$ . $H^{t-1}$ denotes the output of entity or relation representations at timestamp $t-1$ , and also is the input of timestamp $t$ . The time residual gate, i.e., $\\pmb{X}^{t}$ , determines the inherent characteristics to be preserved, where $\\sigma$ is the sigmoid function. $W_{3}$ and $^{b}$ are learnable parameters. ", "page_idx": 5}, {"type": "text", "text": "4.5 Attentive temporal encoder ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In the context of sequence modeling, the attentive temporal encoder is introduced to capture the temporal dependency among the final updated representations across timestamps. The positionenhanced representations of entity and relation are formulated as: ", "page_idx": 5}, {"type": "equation", "text": "$$\nz^{t}=[H^{t};\\Phi(t)],\\quad\\Phi(t)=\\sqrt{\\frac{1}{d}}[\\cos(\\omega_{1}\\tau^{t}),\\cos(\\omega_{2}\\tau^{t}),...,\\cos(\\omega_{d}\\tau^{t})]\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $H^{t}$ is the final updated representation of entity or relation at timestamp $t.\\ \\ [;]$ denotes the concatenation operation. $\\Phi(t)$ is the time position encoder. $d$ denotes the value of the representation dimension. $\\omega_{1}$ to $\\omega_{d}$ are learnable parameters. $\\tau^{t}$ is the timestamp. Then, the temporal dependency is captured by a position-enhanced self-attention mechanism based on representation sequence $z^{1:T^{\\frac{.}{-1}}}=\\{z^{\\tilde{1}},z^{2},...,z^{T-1}\\}$ . For simplicity, the subscript $t$ of variables is omitted in the following sections. The integrated entity or relation representation is formulated as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\beta_{m,n}=\\frac{\\langle W_{q}z_{m},W_{k}z_{n}\\rangle}{\\sqrt{d}},\\ \\ \\alpha_{m,n}=\\frac{\\exp(\\beta_{m,n})}{\\underset{i=1}{\\overset{r-1}{\\sum}}\\exp(\\beta_{m,i})},\\ \\ \\ \\ \\ \\overline{{h}}_{m}=\\sum_{n=1}^{T-1}\\alpha_{m,n}z_{m,n}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $W_{q}$ and $\\boldsymbol{W}_{k}$ are learnable parameters. $\\alpha_{m,n}$ is the attention weight. $\\overline{{h}}_{m}$ is the integrated representation of entity or relation, which integrates the temporal information. ", "page_idx": 5}, {"type": "text", "text": "4.6 Event prediction ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "ConvTransE [25] is employed as the decoder of DECRL, which predicts the probability of each relation between an entity pair, which is formulated as: ", "page_idx": 6}, {"type": "equation", "text": "$$\np(\\hat{\\boldsymbol{r}}|\\boldsymbol{s},\\boldsymbol{o},\\mathcal{G}^{1:T-1})=\\sigma(H_{\\mathrm{r}}\\mathrm{ConvTransE}(\\bar{\\boldsymbol{e}}_{\\mathrm{s}},\\bar{\\boldsymbol{e}}_{\\mathrm{o}}))\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\hat{\\pmb{r}}$ is the probability vector of relations. $\\sigma$ is the sigmoid function. $\\textstyle H_{\\mathrm{r}}$ is the relation representation matrix, each row of which corresponds to an integrated relation representation $\\overline{{\\pmb{r}}}$ . ConvTransE $(\\cdot)$ contains a one-dimensional convolution layer and a fully connected layer. ", "page_idx": 6}, {"type": "text", "text": "The event prediction training objective is minimizing the cross-entropy loss, which is formulated as: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{TKG}}=-\\frac{1}{S}\\sum_{i=1}^{S}\\sum_{j=1}^{N_{\\mathrm{r}}}\\{y_{i,j}\\log p_{i,j}+(1-y_{i,j})\\log(1-p_{i,j})\\}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $S$ and $N_{\\mathrm{r}}$ denote the numbers of samples in the training set and relations, respectively. $y_{i,j}$   \ndenotes the label of relation $j$ for sample $i$ , of which the element is 1 if the event occurs, otherwise $0$ .   \n$p_{i,j}$ is the predicted probability of relation $j$ for sample $i$ , calculated by Equation 15. ", "page_idx": 6}, {"type": "text", "text": "Finally, the total loss for DECRL is formulated as: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}=(1-\\lambda)\\mathcal{L}_{\\mathrm{TKG}}+\\lambda\\mathcal{L}_{\\mathrm{temporal}}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\lambda$ is a hyper-parameter that controls the trade-off between event prediction and temporal smoothness, which is bounded between 0 and 1. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we evaluate the performance of DECRL through comprehensive experiments. We first describe the datasets and experimental settings, followed by a comparison with 12 SOTA TKG representation learning approaches. Next, we present an ablation study and hyper-parameter sensitivity analysis (see Appendix A.5). Finally, we offer case studies showcasing the effectiveness of DECRL. ", "page_idx": 6}, {"type": "text", "text": "5.1 Datasets and experimental settings ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We evaluate DECRL on seven real-world datasets: ICEWS14 [30], ICEWS18 [9], ICEWS14C [29], ICEWS18C [29], GDELT [13], WIKI [12], and YAGO [9]. The ICEWS14 and ICEWS18 datasets span January 1, 2014, to December 31, 2014, and January 1, 2018, to October 31, 2018, respectively. We derive ICEWS14C and ICEWS18C datasets by flitering the original datasets to focus exclusively on events involving countries. The GDELT dataset spans January 1, 2018, to January 31, 2018. The WIKI and YAGO datasets are subsets of the Wikipedia history and YAGO3 [22], respectively. Dataset statistics and experimental settings are detailed in Appendices A.2 and A.3. ", "page_idx": 6}, {"type": "text", "text": "5.2 Comparison with baselines ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "DECRL is compared against 12 SOTA TKG representation learning approaches, categorized into shallow encoder-based approaches, i.e., TTransE [12] and HyTE [5], DNN-based approaches, i.e., RE-NET [9], Glean [6], TeMP [34], RE-GCN [16], DACHA [4], and TiRGN [14], and derived structure-based approaches, i.e., TITer [27], EvoExplore [39], GTRL [28], and DHyper [29]. All baselines are evaluated following consistent experimental settings with well-tuned hyper-parameters. The detailed descriptions of compared baselines can be found in Appendix A.4. ", "page_idx": 6}, {"type": "text", "text": "Tables 1, 2, and 3 display the MRR and Hits $@1/3/10$ results of event prediction on ICEWS14, ICEWS18, ICEWS14C, ICEWS18C, and GDELT datasets. \u201c\\*\u201d denotes the statistical superiority of DECRL over compared approaches based on pairwise t-tests at a $95\\%$ confidence level. The best results are highlighted in bold, while the second-best results are underlined. Notably, DECRL consistently exhibits superior performance, yielding average improvements of $13.24\\%$ , $12.98\\%$ , $10.42\\%$ , and $14.68\\%$ in MRR and Hits $@1/3/10$ metrics, respectively. These findings affirm the efficacy of integrating deep evolutionary clustering for capturing the temporal evolution of high-order correlations among entities. Furthermore, it is evident that approaches leveraging derived structures ", "page_idx": 6}, {"type": "table", "img_path": "V42zfM2GXw/tmp/0b79f5f8cdf2dd7d61c33fa0890290163dfebb5ff99293cb9c79c07da73acf88.jpg", "table_caption": ["Table 1: The performance of DECRL and the compared approaches on ICEWS14 and ICEWS14C "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "V42zfM2GXw/tmp/3b3d33dd3730e0588f45126ecec1cba04fd4c3ff0f407d41790973abd09e37ca.jpg", "table_caption": ["Table 2: The performance of DECRL and the compared approaches on ICEWS18 and ICEWS18C "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "V42zfM2GXw/tmp/efcc6c8c6b00ff14237d3faad16174e5cea4d20e838c52804b0e1cffcd94a4d0.jpg", "table_caption": ["Table 3: The performance of DECRL and the compared approaches on GDELT. \u201cOOM\u201d and \u201cTLE\u201d indicate out of memory and a single epoch exceeded 24 hours "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 4: The performance of DECRL and the compared approaches on WIKI and YAGO with MRR generally outshine those relying on DNNs, which in turn surpass approaches employing shallow encoders. ", "page_idx": 7}, {"type": "table", "img_path": "V42zfM2GXw/tmp/e64e9df28ef3b32248596d84a7554b199009391fd5d77332f3c5bbe3f4bec40f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "V42zfM2GXw/tmp/4ca3a379d9d32604bf306f584e06883d9d5b83b06ce757ddf19a37794d529ee2.jpg", "table_caption": ["Table 5: The performance of DECRL and the variants on ICEWS14 "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Since most approaches do not report event prediction results on WIKI and YAGO datasets, we compare DECRL with TiRGN and DHyper, both of which provide results on these datasets, as shown in Table 4. Furthermore, TiRGN and DHyper are the SOTA DNN-based approach and the SOTA derived structure-based approach, respectively. As a result, DECRL shows improvements of $0.29\\%$ and $0.25\\%$ over the runner-up, DHyper, on WIKI and YAGO datasets, respectively, underscoring its effectiveness. Detailed entity prediction results are presented in Appendix A.6 due to space limitation. ", "page_idx": 8}, {"type": "text", "text": "5.3 Ablation study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To dissect the contributions of each module within DECRL, we conduct the ablation study on ICEWS14. The detailed descriptions of variants are as follows: ", "page_idx": 8}, {"type": "text", "text": "\u2022 DECRL-w/o-alignment: Removing unsupervised alignment mechanism.   \n\u2022 DECRL-w/o-fusion: Removing fusion operation between clusters across timestamps.   \n\u2022 DECRL-w/o-ICE: Removing implicit correlation encoder, resulting in a fully connected graph where all edges are assigned uniform weights.   \n\u2022 DECRL-w/o-global-graph: Removing the guidance of the global graph on the assignment of different interaction intensities to different cluster pairs.   \n\u2022 DECRL-w/o- $\\mathcal{L}_{\\mathrm{temporal}}$ : Removing the temporal smoothness loss term. ", "page_idx": 8}, {"type": "text", "text": "Table 5 displays the MRR and Hits $\\mathcal{Q}\\,1/3/10$ results of DECRL and the variants on ICEWS14. From the ablation study, several conclusions can be drawn: Firstly, the most substantial performance degradation is observed in the DECRL-w/o-fusion variant, which indicates the effectiveness of capturing the temporal evolution of high-order correlations among entities. Moreover, the performance degradation of the DECRL-w/o-global-graph variant justifies the effectiveness of leveraging structural information from the global graph for guiding the assignment of different interaction intensities to different cluster pairs. Lastly, the performance degradation of DECRL-w/o-alignment and DECRL-w/o- $\\mathcal{L}_{\\mathrm{temporal}}$ variants shows the importance of preserving the temporal smoothness of clusters over successive timestamps. ", "page_idx": 8}, {"type": "text", "text": "5.4 Case study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To demonstrate the efficacy of DECRL, we conducted a case study with two variants: DECRL-w/oalignment and DECRL-w/o-fusion, both of which are elaborated in Sub-section 5.3. In Figure 2, we utilize t-SNE [31] for the visualization of entity representations on ICEWS14C, where red dots represent individual entities in the TKGs, with their groupings indicating entity clusters. Notably, the entities marked in Figure 2, i.e., China, Thailand, Vietnam, Laos, Malaysia, Philippines, and Cambodia, are countries in East Asia participating in the Belt and Road Initiative. \u201cMiddle\u201d and \u201cFinal\u201d denote the entity representations obtained after training at the penultimate epoch and the final epoch, respectively, for different variants. ", "page_idx": 8}, {"type": "text", "text": "By comparing the visualizations in the first row of sub-figures in Figure 2, we observe that DECRL exhibits the best entity clustering phenomena during the mid-training phase. By comparing the visualizations in the second row of sub-figures in Figure 2, Figure 2d (Final DECRL) showcases pronounced clustering phenomena, and inter-cluster distances indicate significant differences in clusters, i.e., international political organizations. Figure 2e (Final DECRL-w/o-alignment) demonstrates moderate clustering phenomena among countries, but inter-cluster distances are not significant enough. This observation implies that the lack of precise one-to-one alignment may lead to proximity between different clusters. Figure 2f (Final DECRL-w/o-fusion, which only models high-order correlations without temporal evolution) exhibits increased inter-cluster distances, with less distinct clustering phenomena. The comparison between Figure 2d (Final DECRL) and Figure 2f (Final DECRL-w/o-fusion) shows that capturing temporal evolution leads to better entity representations, as indicated by larger inter-cluster distances and tighter intra-cluster groupings. Comparing the first and third rows of Figure 2 further highlights the training progression, where modeling temporal evolution gradually enhances cluster separation and tightens the grouping of entities within clusters. This demonstrates that the capability of DECRL to model the temporal evolution of high-order correlations significantly enhances its ability to capture more nuanced cluster representations. Additional case study details are provided in Appendix A.7. ", "page_idx": 8}, {"type": "image", "img_path": "V42zfM2GXw/tmp/0cfdc4b7d5e3ae451af2607dcd76b3666c11cd7de293a3409a8af96d40bf9641.jpg", "img_caption": ["Figure 2: The visualization of entity representations on ICEWS14C. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "6 Conclusions and limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, a Deep Evolutionary Clustering jointed temporal knowledge graph Representation Learning approach (DECRL) is proposed for event prediction in TKGs, which jointly optimizes TKG representation learning with evolutionary clustering to capture the temporal evolution of high-order correlations. Comprehensive experiments are conducted on seven real-world datasets, including the comparison with baselines, ablation study, hyper-parameter sensitivity analysis, and case studies, which demonstrate the superior performance of DECRL. However, this study overlooks the continuous temporal evolution of diverse high-order correlations. Currently, the implicit correlation encoder assumes uniform correlations across all clusters, which may not reflect the complexity of real-world interactions between political organizations. In future work, we will address this issue by developing a multi relation-aware inter-cluster correlation encoder. Furthermore, DECRL currently models the temporal evolution of high-order correlations by considering only the previous and current timestamps. Future work will focus on explicitly modeling the continuous influence of high-order correlations from different past timestamps on the current timestamp. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was supported by the Science Foundation of Donghai Laboratory (Grant No. DH2022ZY0013). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Luyi Bai, Mingcheng Zhang, Han Zhang, and Heng Zhang. FTMF: Few-shot temporal knowledge graph completion based on meta-optimization and fault-tolerant mechanism. World Wide Web, 26(3):1243\u20131270, 2023.   \n[2] James Bergstra, Brent Komer, Chris Eliasmith, Dan Yamins, and David D Cox. Hyperopt: A python library for model selection and hyperparameter optimization. Computational Science & Discovery, 8(1):014008, 2015.   \n[3] Jiaao Chen, Jianshu Chen, and Zhou Yu. Incorporating structured commonsense knowledge in story completion. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 6244\u20136251, 2019.   \n[4] Ling Chen, Xing Tang, Weiqi Chen, Yuntao Qian, Yansheng Li, and Yongjun Zhang. DACHA: A dual graph convolution based temporal knowledge graph representation learning method using historical relation. ACM Transactions on Knowledge Discovery from Data, 16(3):1\u201318, 2021.   \n[5] Shib Sankar Dasgupta, Swayambhu Nath Ray, and Partha Talukdar. HyTE: Hyperplane-based temporally aware knowledge graph embedding. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 2001\u20132011, 2018.   \n[6] Songgaojun Deng, Huzefa Rangwala, and Yue Ning. Dynamic knowledge graph based multievent forecasting. In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 1585\u20131595, 2020.   \n[7] Francesco Folino and Clara Pizzuti. An evolutionary multiobjective approach for community discovery in dynamic networks. IEEE Transactions on Knowledge and Data Engineering, 26(8):1838\u20131852, 2013.   \n[8] Zhen Han, Peng Chen, Yunpu Ma, and Volker Tresp. Explainable subgraph reasoning for forecasting on temporal knowledge graphs. In Proceedings of the International Conference on Learning Representations, pages 1\u201324, 2020.   \n[9] Woojeong Jin, Meng Qu, Xisen Jin, and Xiang Ren. Recurrent event network: Autoregressive structure inference over temporal knowledge graphs. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 6669\u20136683, 2020.   \n[10] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \n[11] Harold W Kuhn. The hungarian method for the assignment problem. Naval Research Logistics Quarterly, 2(1-2):83\u201397, 1955.   \n[12] Julien Leblay and Melisachew Wudage Chekol. Deriving validity time in knowledge graph. In Proceedings of the Web Conference, pages 1771\u20131776, 2018.   \n[13] Kalev Leetaru and Philip A Schrodt. GDELT: Global data on events, location, and tone, 1979\u20132012. In Proceedings of ISA Annual Convention, volume 2, pages 1\u201349, 2013.   \n[14] Yujia Li, Shiliang Sun, and Jing Zhao. TiRGN: Time-guided recurrent graph network with local-global historical patterns for temporal knowledge graph reasoning. In Proceedings of the International Joint Conference on Artificial Intelligence, pages 2152\u20132158, 2022.   \n[15] Zixuan Li, Xiaolong Jin, Saiping Guan, Wei Li, Jiafeng Guo, Yuanzhuo Wang, and Xueqi Cheng. Search from history and reason for future: Two-stage reasoning on temporal knowledge graphs. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, pages 4732\u20134743, 2021.   \n[16] Zixuan Li, Xiaolong Jin, Wei Li, Saiping Guan, Jiafeng Guo, Huawei Shen, Yuanzhuo Wang, and Xueqi Cheng. Temporal knowledge graph reasoning based on evolutional representation learning. In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 408\u2013417, 2021.   \n[17] Kangzheng Liu, Feng Zhao, Hongxu Chen, Yicong Li, Guandong Xu, and Hai Jin. Da-Net: Distributed attention network for temporal knowledge graph reasoning. In Proceedings of the ACM International Conference on Information & Knowledge Management, pages 1289\u20131298, 2022.   \n[18] Yushan Liu, Yunpu Ma, Marcel Hildebrandt, Mitchell Joblin, and Volker Tresp. TLogic: Temporal logical rules for explainable link forecasting on temporal knowledge graphs. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 4120\u20134127, 2022.   \n[19] Zhenghao Liu, Chenyan Xiong, Maosong Sun, and Zhiyuan Liu. Entity-duet neural ranking: Understanding the role of knowledge graph semantics in neural information retrieval. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, pages 2395\u20132405, 2018.   \n[20] Lijia Ma, Yutao Zhang, Jianqiang Li, Qiuzhen Lin, Qing Bao, Shanfeng Wang, and Maoguo Gong. Community-aware dynamic network embedding by using deep autoencoder. Information Sciences, 519:22\u201342, 2020.   \n[21] Xiaoke Ma and Di Dong. Evolutionary nonnegative matrix factorization algorithms for community detection in dynamic networks. IEEE Transactions on Knowledge and Data Engineering, 29(5):1045\u20131058, 2017.   \n[22] Farzaneh Mahdisoltani, Joanna Biega, and Fabian M Suchanek. Yago3: A knowledge base from multilingual wikipedias. In Proceedings of Conference on Innovative Data Systems Research, 2013.   \n[23] Yue Pu, Wenbin Yao, and Xiaoyong Li. EM-IFCM: Fuzzy c-means clustering algorithm based on edge modification for imbalanced data. Information Sciences, 659:120029, 2024.   \n[24] Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne Van Den Berg, Ivan Titov, and Max Welling. Modeling relational data with graph convolutional networks. In Proceedings of the European Semantic Web Conference, pages 593\u2013607, 2018.   \n[25] Chao Shang, Yun Tang, Jing Huang, Jinbo Bi, Xiaodong He, and Bowen Zhou. End-to-end structure-aware convolutional networks for knowledge base completion. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 3060\u20133067, 2019.   \n[26] Zongjiang Shang and Ling Chen. MSHyper: Multi-scale hypergraph transformer for long-range time series forecasting. arXiv preprint arXiv:2401.09261, 2024.   \n[27] Haohai Sun, Jialun Zhong, Yunpu Ma, Zhen Han, and Kun He. TimeTraveler: Reinforcement learning for temporal knowledge graph forecasting. arXiv preprint arXiv:2109.04101, 2021.   \n[28] Xing Tang and Ling Chen. GTRL: An entity group-aware temporal knowledge graph representation learning method. IEEE Transactions on Knowledge and Data Engineering, pages 1\u201316, 2023.   \n[29] Xing Tang, Ling Chen, Hongyu Shi, and Dandan Lyu. DHyper: A recurrent dual hypergraph neural network for event prediction in temporal knowledge graphs. ACM Transactions on Information Systems, pages 1\u201325, 2024.   \n[30] Rakshit Trivedi, Hanjun Dai, Yichen Wang, and Le Song. Know-Evolve: Deep temporal reasoning for dynamic knowledge graphs. In Proceedings of International Conference on Machine Learning, pages 3462\u20133471, 2017.   \n[31] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. Journal of Machine Learning Research, 9(11):2579\u20132605, 2021.   \n[32] Xiang Wang, Xiangnan He, Yixin Cao, Meng Liu, and Tat-Seng Chua. KGAT: Knowledge graph attention network for recommendation. In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 950\u2013958, 2019.   \n[33] Binqing Wu, Weiqi Chen, Wengwei Wang, Bingqing Peng, Liang Sun, and Ling Chen. WeatherGNN: Exploiting meteo- and spatial-dependencies for local numerical weather prediction bias-correction. In Proceedings of the International Joint Conference on Artificial Intelligence, page 2433\u20132441, 2024.   \n[34] Jiapeng Wu, Meng Cao, Jackie Chi Kit Cheung, and William L Hamilton. TeMP: Temporal message passing for temporal knowledge graph completion. arXiv preprint arXiv:2010.03526, 2020.   \n[35] Yi Xu, Junjie Ou, Hui Xu, and Luoyi Fu. Temporal knowledge graph reasoning with historical contrastive learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 4765\u20134773, 2023.   \n[36] Min Yang, Xiaoliang Chen, Baiyang Chen, Peng Lu, and Yajun Du. DNETC: Dynamic network embedding preserving both triadic closure evolution and community structures. Knowledge and Information Systems, 65(3):1129\u20131157, 2023.   \n[37] Yuhang Yao and Carlee Joe-Wong. Interpretable clustering on dynamic graphs with recurrent graph neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 4608\u20134616, 2021.   \n[38] Jingyi You, Chenlong Hu, Hidetaka Kamigaito, Kotaro Funakoshi, and Manabu Okumura. Robust dynamic clustering for temporal networks. In Proceedings of the ACM International Conference on Information & Knowledge Management, pages 2424\u20132433, 2021.   \n[39] Jiasheng Zhang, Shuang Liang, Yongpan Sheng, and Jie Shao. Temporal knowledge graph representation learning with local and global evolutions. Knowledge-Based Systems, 251:109234, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "table", "img_path": "V42zfM2GXw/tmp/d232ada1af72f5d20b9b13ed3c14962005f60e9aa4edf78224da996a02331ff0.jpg", "table_caption": ["Table 6: The statistics of datasets "], "table_footnote": [], "page_idx": 13}, {"type": "table", "img_path": "V42zfM2GXw/tmp/837276c31381ccab40b39ffee717fb9c656bb887276c923f2590092f4d0dbee2.jpg", "table_caption": ["Table 7: The final choices of hyper-parameter values "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Complexity analysis ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The time complexity of the relation-aware GCN is $O(N_{\\mathrm{e}}N_{\\mathrm{r}}D^{2})$ , where $N_{\\mathrm{e}}$ and $N_{\\mathrm{r}}$ are the numbers of entities and relations, respectively. $D$ is the dimension of representations. The time complexity of the evolutionary clustering module is $O(N_{\\mathrm{e}}N_{\\mathrm{c}}D+N_{\\mathrm{c}}^{3}+N_{\\mathrm{c}}D)$ , where $N_{\\mathrm{c}}$ is the number of clusters. The time complexity of the cluster graph message passing module is $O(N_{\\mathrm{c}}^{2}D^{2})$ . The time complexity of the time residual gate is $O(D^{2})$ . For the attentive temporal encoder, the time complexity is $\\bar{O}(T^{2}D)$ , where $T$ is the length of the history. For the event prediction module, the time complexity is $O(D)$ . The total complexity of DECRL is $O(N_{\\mathrm{e}}N_{\\mathrm{r}}D^{2}+\\dot{N}_{\\mathrm{c}}^{3}+N_{\\mathrm{c}}^{2}D^{2}+T^{2}D)$ . ", "page_idx": 13}, {"type": "text", "text": "A.2 Statistics of datasets ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "All datasets are split into training $(80\\%)$ , validation $(10\\%)$ , and test $(10\\%)$ sets following [16]. The statistics of these datasets are summarized in Table 6. ", "page_idx": 13}, {"type": "text", "text": "A.3 Experimental settings ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "DECRL is implemented in Python using PyTorch and trained on one NVIDIA RTX 3080 GPU with 10GB memory. The source code is available on $\\mathrm{{GitHub}}^{2}$ . We leverage the Neural Network Intelligence (NNI) toolkit3 to automatically identify the optimal hyper-parameter values. The search space for the number of clusters $N_{\\mathrm{c}}$ , the number of DECRL layers $N_{\\mathrm{DECRL\\,layer}}$ , the length of historical windows $N_{\\mathrm{historical\\;window}}$ , and the value of $\\lambda$ range from 1 to 20 with the step of 2, 1 to 5 with the step of 1, 1 to 14 with the step of 1, and 0.1 to 0.5 with the step of 0.1, respectively. The final hyper-parameter values are presented in Table 7. For NNI configurations, the maximum number of trials is set to 30, and the optimization algorithm used is the Tree-structured Parzen Estimator [2]. ", "page_idx": 13}, {"type": "text", "text": "We utilize the Adam [10] optimizer with an initial learning rate of 0.01. The batch size is set to 16. The representation dimension is set to 200. The hidden sizes for the time residual gate and the attentive temporal encoder are both set to 200. The results reported are the averages across five independent runs. The evaluation metrics used in this paper include Mean Reciprocal Rank (MRR) and Hits $@1/3/10$ , which represent the proportion of correct predictions ranked within the top 1, 3, and 10 positions, respectively, all expressed as percentages. Higher Hits $@_{\\mathrm{k}}$ and MRR scores indicate better performance. ", "page_idx": 13}, {"type": "text", "text": "A.4 Description of baselines ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Shallow encoder-based approaches: ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "\u2022 TTransE [12] extends the TransE by incorporating timestamps as corresponding representations.   \n\u2022 HyTE [5] models timestamps as corresponding hyperplanes. ", "page_idx": 14}, {"type": "text", "text": "DNN-based approaches: ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "\u2022 RE-NET [9] leverages GCNs to model the influence of neighbor entities and uses RNNs to capture temporal dependencies among events.   \n\u2022 Glean [6] employs CompGCN to model the influence of neighbor entities and utilizes GRUs to capture temporal dependencies among representations.   \n\u2022 TeMP [34] utilizes relation-aware GCN to model the influence of neighbor entities and employs a frequency-based gating GRU to capture temporal dependencies among inactive events.   \n\u2022 RE-GCN [16] uses relation-aware GCN to model the influence of neighbor entities and employs an autoregressive GRU to capture temporal dependencies among events.   \n\u2022 DACHA [4] introduces a dual graph convolution network to obtain entity representations and uses a self-attentive encoder to model temporal dependencies among relations.   \n\u2022 TiRGN [14] is the SOTA approach, using a multi-relational GCN to capture graph structure information and a double recurrent mechanism to model temporal dependencies. ", "page_idx": 14}, {"type": "text", "text": "Derived structure-based approaches: ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "\u2022 TITer [27] incorporates temporal agent-based reinforcement learning to search paths and obtain entity representations via the inductive mean.   \n\u2022 EvoExplore [39] establishes dynamic communities to model the influence of neighbor entities.   \n\u2022 GTRL [28] uses entity group modeling to capture the influence of distant and unreachable entities and employs GRUs to model temporal dependencies among representations.   \n\u2022 DHyper [29] is the SOTA approach, which utilizes hypergraph neural networks to model high-order correlations among entities and among relations. ", "page_idx": 14}, {"type": "text", "text": "A.5 Hyper-parameter sensitivity analysis ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this sub-section, we study the hyper-parameter sensitivity of DECRL on ICEWS18C, including the length of historical windows $N_{\\mathrm{{hi}}}$ storical window, the number of clusters $N_{\\mathrm{c}}$ , the number of DECRL layers $N_{\\mathrm{DECRL\\,layer}}$ , and the value of $\\lambda$ . We show the performance changes by varying the hyper-parameter values in Figure 3. ", "page_idx": 14}, {"type": "text", "text": "We present the impact of Nhistorical window in Figure 3a. The results indicate that as Nhistorical window increases, the performance of DECRL gradually improves, peaking at the window length of 10. Beyond this point, performance declines rapidly, likely due to the inclusion of excessive irrelevant information in longer historical windows, which negatively impacts the performance of DECRL. ", "page_idx": 14}, {"type": "text", "text": "In Figure 3b, we present the effect of $N_{\\mathrm{c}}$ . The performance of DECRL remains relatively stable across most metrics as $N_{\\mathrm{c}}$ increases. Hits $@1$ shows a slight initial increase until $N_{\\mathrm{c}}=8$ and then stabilizes. MRR, Hits $@3$ , and Hits $@10$ remain consistent, indicating that the performance of DECRL is not substantially affected by variations of $N_{\\mathrm{c}}$ . ", "page_idx": 14}, {"type": "text", "text": "We show the impact of $N_{\\mathrm{DECRL}}$ layer and $\\lambda$ in Figures 3c and 3d, respectively. In Figure 3c, performance metrics reach their peak when $N_{\\mathrm{DECRL\\,layer}}\\,=\\,2$ . Beyond this point, the performance of DECRL declines, likely due to increased model complexity, which raises the risk of over-ftiting. Similarly, in Figure 3d, the performance metrics peak at $\\lambda=0.2$ , which suggests the need to set the appropriate trade-off between event prediction loss and temporal smoothness loss. ", "page_idx": 14}, {"type": "image", "img_path": "V42zfM2GXw/tmp/2a893530a4a3f55e022fc1d35835f56e4b227117e91e14e9a44d14fb55954c1a.jpg", "img_caption": ["(a) Hyper-parameter Nhistorical window on ICEWS18C "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "V42zfM2GXw/tmp/a59ea538ba4ae90cf0a526f300ae7938fa36abc5025d8636c0425975959b16ae.jpg", "img_caption": ["(c) Hyper-parameter $N_{\\mathrm{DECRL\\,layer}}$ on ICEWS18C ", "Figure 3: Results of hyper-parameters changes of DECRL on ICEWS18C. "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "V42zfM2GXw/tmp/5f0fcdd09fb33cfb39ec3e9cc8020cbc5fce170341a9a96f08ba29706926e0f2.jpg", "img_caption": ["(b) Hyper-parameter $N_{\\mathrm{c}}$ on ICEWS18C "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "V42zfM2GXw/tmp/17ff05d8e4de77cc3b09e2a4ab00ab9d103a5e28c48845184f0daacdad171828.jpg", "img_caption": ["(d) Hyper-parameter $\\lambda$ on ICEWS18C "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Table 8: The entity prediction performance of DECRL and the compared approaches on GDELT. \u201cOOM\u201d and \u201cTLE\u201d indicate out of memory and a single epoch exceeded 24 hours. \\* indicates that DECRL is statistically superior to the compared approaches according to pairwise t-test at a $95\\%$ significance level. The best results are in bold and the second best results are underlined ", "page_idx": 15}, {"type": "table", "img_path": "V42zfM2GXw/tmp/a3d2aead47224faaf89fb20a225bdb8051a7e680e6b9526d9ac9a025d0062137.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "V42zfM2GXw/tmp/716e8adfc66a230214fe96b37b85259d6061c16507bb6b1a960df726fd21d92d.jpg", "table_caption": ["Table 9: Top 5 relations predicted by TiRGN, DHyper, and DECRL "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "A.6 Entity prediction performance ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We have conducted additional experiments to evaluate the performance of DECRL on the future entity prediction task, as shown in Table 8. Although DECRL does not achieve the SOTA performance in terms of MRR and Hits $@1$ metrics, it achieves the best results in Hits $@3$ and Hits $@10$ metrics. It is important to note that DECRL is not specifically designed for entity prediction tasks. Nevertheless, these results demonstrate the effectiveness and robustness of DECRL, particularly in capturing a broader range of relevant entities. ", "page_idx": 16}, {"type": "text", "text": "A.7 Case study ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Table 9 presents the top 5 relations predicted by the SOTA DNN-based approach TiRGN, the SOTA derived structure-based approach DHyper, and our proposed approach DECRL for two test samples on ICEWS14C. The test samples pertain to the conflict between Russia and Ukraine that began in February 2014. During this period, Russia deployed military forces to the Crimean region of Ukraine, which resulted in widespread condemnations and sanctions from several countries, including the United States and various European nations. Correct predictions are underlined in Table 9. Compared to TiRGN and DHyper, DECRL predicts more correct relations and ranks the correct predictions higher. The results indicate that by modeling the temporal evolution of the high-order correlations among entities, DECRL can achieve more accurate prediction results. ", "page_idx": 16}, {"type": "text", "text": "A.8 Societal impacts ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this paper, a Deep Evolutionary Clustering jointed temporal knowledge graph Representation Learning approach (DECRL) is proposed for event prediction in temporal knowledge graphs, which offers more accurate and credible results. We summarize the positive and possible negative societal impacts as follows: ", "page_idx": 16}, {"type": "text", "text": "Positive societal impacts: ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "\u2022 Optimizing social governance: Temporal knowledge graph event prediction can help governments and relevant institutions foresee potential future events, enabling them to take preemptive measures and optimize social governance.   \n\u2022 Supporting business decisions: Companies can use the prediction results for market analysis and business decisions, enhancing their competitiveness.   \n\u2022 Enhancing public safety: By predicting potential threats and dangerous events, relevant departments can allocate resources in advance to ensure public safety.   \n\u2022 Advancing academic research: This research can promote progress in the fields of temporal knowledge graphs and event prediction, providing new directions and methods for academic studies. ", "page_idx": 16}, {"type": "text", "text": "Negative societal impacts: ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "\u2022 Misuse risks: Accurate predictive technology might be exploited by malicious actors to forecast and manipulate events. ", "page_idx": 16}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Justification: The main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope (see Abstract and Introduction). ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 17}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Justification: The paper discusses the limitations of the work (see Conclusions and limitations). ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \u201cLimitations\u201d section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 17}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: The paper provides the full set of assumptions and a complete (and correct) proof (see Section 4). ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 18}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: The paper fully discloses all the information needed to reproduce the main experimental results of the paper (see Appendix A.3). ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 18}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We upload source code and datasets on Anonymous GitHub (see Appendix A.3). ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 19}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The paper specifies all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results (see Appendices A.2 and A.3). ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 19}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: The paper reports appropriate information about the statistical significance of the experiments (see Sub-section 5.2). ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \u201cYes\u201d if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 19}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 20}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The paper provides sufficient information on the computer resources and time complexity analysis (see Appendices A.1 and A.3). ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 20}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The research conducted in the paper conforms, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 20}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The paper discusses both potential positive societal impacts and negative societal impacts of the work performed (see Appendices A.8). ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 21}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: The datasets used in this work are public and out of risk. Our approach poses no such risks, not involving pretrained language models or image generators. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 21}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The creators or original owners of assets (e.g., code, data, models), used in the paper, are properly credited, and the license and terms of use are explicitly mentioned and properly respected. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. \u2022 The authors should cite the original paper that produced the code package or dataset. \u2022 The authors should state which version of the asset is used and, if possible, include a URL. ", "page_idx": 21}, {"type": "text", "text": "\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 22}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The instruction of dataset/code/model is provided on the project homepage at the anonymous Github. The URL of the anonymous repository please see Abstract. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 22}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 22}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 22}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 23}]