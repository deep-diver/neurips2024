[{"type": "text", "text": "Stochastic contextual bandits with graph feedback: from independence number to MAS number ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yuxiao Wen Yanjun Han Zhengyuan Zhou New York University New York University New York University yuxiaowen@nyu.edu yanjunhan@nyu.edu zzhou@stern.nyu.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We consider contextual bandits with graph feedback, a class of interactive learning problems with richer structures than vanilla contextual bandits, where taking an action reveals the rewards for all neighboring actions in the feedback graph under all contexts. Unlike the multi-armed bandits setting where a growing literature has painted a near-complete understanding of graph feedback, much remains unexplored in the contextual bandits counterpart. In this paper, we make inroads into this inquiry by establishing a regret lower bound $\\Omega(\\sqrt{\\beta_{M}(G)T})$ , where $M$ is the number of contexts, $G$ is the feedback graph, and $\\beta_{M}(G)$ is our proposed graphtheoretic quantity that characterizes the fundamental learning limit for this class of problems. Interestingly, $\\beta_{M}(G)$ interpolates between $\\alpha(G)$ (the independence number of the graph) and $\\mathsf{m}(G)$ (the maximum acyclic subgraph (MAS) number of the graph) as the number of contexts $M$ varies. We also provide algorithms that achieve near-optimal regret for important classes of context sequences and/or feedback graphs, such as transitively closed graphs that find applications in auctions and inventory control. In particular, with many contexts, our results show that the MAS number essentially characterizes the statistical complexity for contextual bandits, as opposed to the independence number in multi-armed bandits. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Contextual bandits encode a rich class of sequential decision making problems in reality, including clinical trials, personalized healthcare, dynamic pricing, recommendation systems (Bouneffouf et al., 2020). However, due to the exploration-exploitation trade-off and a potentially large context space, the pace of learning for contextual bandits could be slow, and the statistical complexity of learning could be costly for application scenarios with bandit feedback (Agarwal et al., 2012). There are two common approaches to alleviate the burden of sample complexity, either by exploiting the function class structure for the reward (Zhu and Mineiro, 2022), or by utilizing additional feedback available during exploration. ", "page_idx": 0}, {"type": "text", "text": "In this paper we focus on the second approach, and aim to exploit the feedback structure efficiently in contextual bandits. The framework of formulating the feedback structure as feedback graphs in bandits has a long history (Mannor and Shamir, 2011; Alon et al., 2015, 2017; Lykouris et al., 2020), where a direct edge between two actions indicates choosing one action provides the reward information for the other. Such settings have also been generalized to contextual cases (Balseiro et al., 2019; Dann et al., 2020; Han et al., 2024), where counterfactual rewards could be available under different contexts. Typical results in these settings are that, the statistical complexity of bandits with feedback graphs is characterized by some graph-theoretic quantities, such as the independence number or the maximum acyclic subgraph (MAS) number of the feedback graph. ", "page_idx": 0}, {"type": "text", "text": "To understand the influence of the presence of contexts on the statistical complexity of learning and to compare with multi-armed bandits, we focus on the tabular setting where the contexts are treated as general variables determining the rewards. A widely studied alternative is the structured setting that leverages certain structures in the dependence on the context. Examples of the latter include linear contextual bandits (Auer, 2002; Agrawal and Goyal, 2013), which assume a linear reward function on the contexts, and their variants (Chu et al., 2011; Li et al., 2017; Agrawal and Devanur, 2016). ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Despite the existing results, especially in multi-armed bandits where a near-complete characterization of the optimal regret is available (Alon et al., 2015; Koc\u00e1k and Carpentier, 2023; Eldowa et al., 2024), the statistical complexity of contextual bandits with feedback graphs is much less understood. For example, consider the case where there is a feedback graph $G$ across the actions and a complete feedback graph across the contexts (termed as complete cross-learning in (Balseiro et al., 2019)). In this case, for a long time horizon $T$ , the optimal regret scales as $\\widetilde{\\Theta}(\\sqrt{\\alpha(G)T})$ when there is only one context (Alon et al., 2015), but only an upper bound $\\widetilde{O}(\\sqrt{\\mathsf{m}(G)T})$ is known regardless of the number of contexts (Dann et al., 2020). Here $\\alpha(G)$ and $\\mathsf{m}(G)$ denote the independence number and the MAS number of the graph $G$ , respectively; we refer to Section 1.1 for the precise definitions. While $\\alpha(G)={\\mathfrak{m}}(G)$ for all undirected graphs, for directed graphs their gap could be significant. It is open if the change from $\\alpha(G)$ to $\\mathsf{m}(G)$ is essential with the increasing number of contexts, not to mention the precise dependence on the number of contexts. ", "page_idx": 1}, {"type": "text", "text": "1.1 Notations ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "For $n\\in\\mathbb N$ , let $[n]:=\\{1,\\cdot\\cdot\\cdot,n\\}$ . For two probability measures $P$ and $Q$ over the same space, let $\\begin{array}{r}{\\mathsf{T V}(P,Q)=\\int\\bar{|\\mathrm{d}P\\!-\\!\\mathrm{d}Q|}/2}\\end{array}$ be the total variation (TV) distance, and $\\begin{array}{r}{{\\mathsf{K L}}(P\\|Q)=\\int\\mathrm{d}P\\log(\\mathrm{\\bar{d}}P/\\mathrm{d}Q)}\\end{array}$ be the Kullback-Leibler (KL) divergence. We use the standard asymptotic notations $O,\\Omega,\\Theta$ , as well as $\\widetilde{O},\\widetilde{\\Omega},\\widetilde{\\Theta}$ to denote respective meanings within polylogarithmic factors. ", "page_idx": 1}, {"type": "text", "text": "We use the following graph-theoretic notations. For a directed graph $G=(V,E)$ , let $u\\rightarrow v$ denote that $(u,v)\\in E$ . For $u\\,\\in\\,V$ , let $N_{\\mathrm{out}}(u)\\,=\\,\\{v\\,\\in\\,V\\,:\\,u\\,\\to\\,{\\bar{v}}\\}$ be the set of out-neighbors of $u$ (including $u$ itself). We will also use $N_{\\mathrm{out}}(A)=\\cup_{v\\in A}N_{\\mathrm{out}}(v)$ to denote the set of all out-neighbors of vertices in $A$ . The independence number $\\alpha(G)$ , dominating number $\\delta(G)$ , and maximum acyclic subgraph (MAS) number $\\mathsf{m}(G)$ are defined as ", "page_idx": 1}, {"type": "text", "text": "$\\alpha(G)=\\operatorname*{max}\\{|I|:I\\subseteq V$ is an independent set, i.e. $u\\nrightarrow v,\\forall u\\neq v\\in I\\}$ , $\\delta(G)=\\operatorname*{min}\\{|J|:J\\subseteq V$ is a dominating set, i.e. $N_{\\mathrm{out}}(J)=V\\}$ , $\\mathsf{m}(G)=\\operatorname*{max}\\{|D|:D\\subseteq V$ induces an acyclic subgraph of $G\\}$ , ", "page_idx": 1}, {"type": "text", "text": "respectively. It is easy to show that $\\operatorname*{max}\\{\\alpha(G),\\delta(G)\\}\\leq\\mathsf{m}(G)$ , with a possibly unbounded gap, and a probabilistic argument also shows that $\\delta(G)=O(\\alpha(G)\\log|V|)$ (cf. Lemma A.1). ", "page_idx": 1}, {"type": "text", "text": "1.2 Our results ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this paper we focus on contextual bandits with both feedback graphs across actions and complete cross-learning across contexts. This setting was proposed in (Han et al., 2024), with applications to bidding in first-price auctions. As opposed to an arbitrary feedback graph across all context-action pairs in (Dann et al., 2020), we assume a complete cross-learning because of two reasons. First, in many scenarios the contexts encode different states which only play roles in the reward function; in other words, the counterfactual rewards for all contexts can be observed by plugging different contexts into the reward function. Such examples include bidding in auctions (Balseiro et al., 2019; Han et al., 2024) and sleeping bandits modeled in (Schneider and Zimmert, 2024). Second, this scenario is representative and sufficient to reflect the main ideas and findings of this paper. Discussion and results under more general settings is left to Section 4.1. ", "page_idx": 1}, {"type": "text", "text": "Throughout this paper we consider the following stochastic contextual bandits. At the beginning of each round $t\\in[T]$ during the time horizon $T$ , an oblivious adversary chooses a context $c_{t}\\in[M]$ and reveals it to the learner, and the learner chooses an action $\\boldsymbol{a}_{t}\\dot{\\in}\\ [K]$ . There is a strongly observable1 directed feedback graph $G\\;=\\;([K],E)$ across the actions such that all rewards in $(r_{t,c,a})_{c\\in[M],(a_{t},a)\\in E}$ are observable, where we assume no structure in the rewards except that $r_{t,c,a}\\,\\in\\,[0,1]$ . In our stochastic environment, the mean reward $\\mathbb{E}[r_{t,c,a}]\\,=\\,\\mu_{c,a}$ is unknown but invariant with time. We are interested in the characterization of the minimax regret achieved by the ", "page_idx": 1}, {"type": "text", "text": "learner: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{R}_{T}^{\\star}(G,M)=\\operatorname*{inf}_{\\pi^{T}}\\mathsf{R}_{T}(\\pi^{T};G,M)}\\\\ &{\\qquad\\qquad=\\displaystyle\\operatorname*{inf}_{\\pi^{T}}\\operatorname*{sup}_{\\varepsilon^{T}}\\operatorname*{sup}_{\\mu\\in[0,1]^{K\\times M}}\\mathbb{E}\\left[\\sum_{t=1}^{T}\\left(\\operatorname*{max}_{a^{\\star}(c_{t})\\in[K]}\\mu_{c_{t},a^{\\star}(c_{t})}-\\mu_{c_{t},\\pi_{t}(c_{t})}\\right)\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where the infimum is over all admissible policies based on the available observations. In the sequel we might also constrain the class of context sequences to $c^{T}\\in\\mathcal{C}$ , and we will use $\\mathsf{R}_{T}^{\\star}(G,M,\\mathcal{C})$ and $\\mathsf{R}_{T}(\\pi^{T};G,M,\\mathcal{C})$ to denote the respective meanings by taking the supremum over $c^{T}\\in\\mathcal{C}$ . ", "page_idx": 2}, {"type": "text", "text": "Our first result concerns a new lower bound on the minimax regret. ", "page_idx": 2}, {"type": "text", "text": "Theorem 1.1 (Minimax lower bound). For $T\\geq\\beta_{M}(G)^{3}$ , it holds that $\\mathsf{R}_{T}^{\\star}(G,M)=\\Omega(\\sqrt{\\beta_{M}(G)T})$ , where the graph-theoretic quantity $\\beta_{M}(G)$ is given by ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\beta_{M}(G)=\\operatorname*{max}\\left\\{\\sum_{c=1}^{M}|I_{c}|:I_{1},\\cdots,\\,I_{M}\\ d i s j o i n t\\,i n d e p e n d e n t\\,s u b s e t s\\;o f[K],\\;a n d\\,I_{i}\\neq I_{j}\\,f o r\\,i<j,\\ldots\\right\\}\\,.\n$$$i<j\\Biggr\\},$ ", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "and $I_{i}\\neq I_{j}$ means that $u\\not\\to v$ whenever $u\\in I_{i}$ and $v\\in I_{j}$ . ", "page_idx": 2}, {"type": "text", "text": "Theorem 1.1 provides a minimax lower bound on the optimal regret, depending on both the number of contexts $M$ and the feedback graph $G$ . Note that the independent subsets $I_{1},\\ldots,I_{M}$ are allowed to be empty if needed. It is clear that $\\beta_{1}(G)=\\alpha(G)$ is the independence number, and $\\beta_{M}(G)={\\mathsf{m}}(G)$ whenever $M\\geq{\\mathsf{m}}(G)$ . This leads to the following corollary. ", "page_idx": 2}, {"type": "text", "text": "Corollary 1.2 (Tightness of MAS number). For any graph $G$ , i $f M\\geq{\\mathsf{m}}(G)$ and $T\\geq\\mathsf{m}(G)^{3}$ , one has $\\mathsf{R}_{T}^{\\star}(G,M)=\\Omega(\\sqrt{\\mathsf{m}(G)T})$ . ", "page_idx": 2}, {"type": "text", "text": "Corollary 1.2 shows that, the regret change from $\\widetilde{\\Theta}(\\sqrt{\\alpha(G)T})$ in multi-armed bandits to $\\widetilde{O}(\\sqrt{\\mathsf{m}(G)T})$ in contextual bandits (Dann et al., 2020) is in fact not superfluous when there are many contexts. In other words, although the independence number determines the statistical complexity of multi-armed bandits with graph feedback, the statistical complexity in contextual bandits with many contexts is completely characterized by the MAS number. ", "page_idx": 2}, {"type": "text", "text": "For intermediate values of $M\\in(1,\\mathsf{m}(G))$ , the next result shows that the quantity $\\beta_{M}(G)$ is tight for a special class $\\mathcal{C}_{\\sf S A}$ of context sequence called self-avoiding contexts. A context sequence $(c_{1},\\cdot\\cdot\\cdot\\,,c_{T})$ is called self-avoiding iff $c_{s}=c_{t}$ for $s<t$ implies $c_{s}=c_{s+1}=\\cdot\\cdot\\cdot=c_{t}$ (or in other words, contexts do not jump back). For example, 113222 is self-avoiding, but 12231 is not. This assumption is reasonable when contexts model a nonstationary environment changing slowly, e.g. the environment changes from season to season. ", "page_idx": 2}, {"type": "text", "text": "Theorem 1.3 (Upper bound for self-avoiding contexts). For self-avoiding contexts, there is a policy $\\pi$ achieving $\\mathsf{R}_{T}(\\pi;\\dot{G},M,\\mathcal{C}_{\\mathsf{S A}})=\\widetilde O(\\sqrt{\\beta_{M}(\\dot{G})T})$ . This policy can be implemented in polynomial-time, and does not need to know the context sequence in advance. ", "page_idx": 2}, {"type": "text", "text": "As the minimax lower bound in Theorem 1.1 is actually shown under $\\mathcal{C}_{\\sf S A}$ , for large $T$ , Theorem 1.3 establishes a tight regret bound for stochastic contextual bandits with graph feedback and selfavoiding contexts. The policy used in Theorem 1.3 is based on arm elimination, where a central step of exploration is to solve a sequential game in general graphs which has minimax value $\\widetilde\\Theta(\\beta_{M}(G))$ and could be of independent interest. ", "page_idx": 2}, {"type": "text", "text": "For general context sequences, we have a different sequential game in which we do not have a tight characterization of the minimax value in general. Instead, we have the following upper bound, which exhibits a gap compared with Theorem 1.1. ", "page_idx": 2}, {"type": "text", "text": "Theorem 1.4 (Upper bound for general contexts). For general contexts, there is a policy $\\pi$ achieving ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathsf{R}_{T}(\\pi;G,M)=\\widetilde{O}\\left(\\sqrt{\\operatorname*{min}\\left\\{\\overline{{\\beta}}_{M}(G),\\boldsymbol{\\mathsf{m}}(G)\\right\\}T}\\right),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\overline{{\\beta}}_{M}(G)=\\operatorname*{max}\\left\\{\\sum_{c=1}^{M}|I_{c}|:I_{1},\\cdot\\cdot\\cdot\\cdot,I_{M}\\ a r e\\ d i s j o i n t\\ i n d e p e n d e n t\\ s u b s e t s\\ o f[K]\\right\\}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Fortunately, additional assumptions on the feedback graph $G$ can be leveraged to recover the tight regret bound: ", "page_idx": 3}, {"type": "text", "text": "Corollary 1.5 (Upper bound for transtively closed or undirected feedback). For any undirected or transitively closed graph $G$ , the policy $\\pi$ in Theorem 1.4 achieves a near-optimal regret $\\mathsf{R}_{T}(\\pi;G,M)=\\widetilde{O}(\\sqrt{\\beta_{M}(\\vec{G})T})$ . ", "page_idx": 3}, {"type": "text", "text": "A directed graph $G$ is called transitively closed if $u\\rightarrow v$ and $v\\rightarrow w$ imply that $u\\rightarrow w$ . In reality directed feedback graphs are often transitively closed, for a directed structure of the feedback typically indicates a partial order over the actions. Examples include bidding in auctions (Zhao and Chen, 2019; Han et al., 2024) and inventory control (Huh and Rusmevichientong, 2009), both of which exhibit the one-sided feedback structure $i\\rightarrow j$ for $i\\leq j$ . For general graphs, Theorem 1.4 gives another graph-theoretic quantity ${\\overline{{\\beta}}}_{M}(G)$ . Note that ${\\overline{{\\beta}}}_{M}(G)\\geq\\beta_{M}(G)$ as there is no acyclic requirement between $I_{c}$ \u2019s in (3), which in turn is due to a technical difficulty of non-self-avoiding contexts. Further discussions on this gap are deferred to Section 4.3. ", "page_idx": 3}, {"type": "text", "text": "Interestingly, the upper bound quantities $\\beta_{M}(G)$ and ${\\overline{{\\beta}}}_{M}(G)$ are not explicitly linear in $M$ and are always no larger than $\\alpha(G)M$ . Hence our results partially answer an open problem in (Hao et al., 2022, Remark 5.11) that if the dependence of regret bound $O(\\sqrt{\\alpha(G)M T})$ on $M$ can be improved. ", "page_idx": 3}, {"type": "text", "text": "1.3 Related work ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The study of bandits with feedback graphs has a long history dating back to (Mannor and Shamir, 2011). For (both adversarial and stochastic) multi-armed bandits, a celebrated result in (Alon et al., 2015, 2017) shows that the optimal regret scales as $\\widetilde{\\Theta}(\\sqrt{\\alpha(G)T})$ if $T\\geq\\alpha(G)^{3}$ ; the case of smaller $T$ was settled in (Koc\u00e1k and Carpentier, 2023), where the optimal regret is a mixture of $\\sqrt{T}$ and $T^{2/3}$ rates. For stochastic bandits, simpler algorithms based on arm elimination or upper confidence bound (UCB) are also proposed (Lykouris et al., 2020; Han et al., 2024), while the UCB algorithm is only known to achieve an upper bound of $\\widetilde{O}(\\sqrt{\\mathsf{m}(G)T})$ .2 In addition to strongly observable graphs we primarily focus on, weakly observable graphs have also drawn vast interest (Alon et al., 2015; Chen et al., 2021) where the optimal regret is characterized by the dominating number $\\delta(G)$ . There exploration plays a more significant role due to weaker observability of certain nodes, leading to an optimal regret $\\widetilde\\Theta(\\delta(G)^{1/3}T^{2/3})$ . We will briefly discuss the regret characterization of our contextual setting with weakly observable graphs in Section 4.1 and 4.2. ", "page_idx": 3}, {"type": "text", "text": "Recently, the graph feedback was also extended to contextual bandits under the name of \u201ccrosslearning\u201d (Balseiro et al., 2019; Schneider and Zimmert, 2024). The work (Balseiro et al., 2019) considered both complete and partial cross-lea\u221arning, and showed that the optimal regret for stochastic bandits with complete cross learning is $\\widetilde{\\Theta}(\\sqrt{K T})$ . Motivated by bidding in first-price auctions, (Han et al., 2024) generalized the settin g to general graph feedback across actions and complete cross-learning across contexts, a setting used in the current paper. The finding in (Han et al., 2024) is that the effects of graph feedback and cross-learning could be \u201cdecoupled\u201d: a regret upper bound $\\widetilde{O}(\\sqrt{\\operatorname*{min}\\{\\alpha(G)M,K\\}T})$ is shown, which is tight only for a special choice of the feedback graph $G$ The work (Dann et al., 2020) considered a tabular reinforcement learning setting with adversarial initial states, so that their setting with episode length $H=1$ coincides with our problem with a general feedback graph $G$ across all context-action pairs. They showed that the UCB algorithm achieves a regret upper bound $\\widetilde{O}(\\sqrt{\\mathsf{m}(G)T})$ ; however, their lower bound was only $\\Omega({\\sqrt{\\alpha(G)T}})$ when $T\\geq\\alpha(G)^{3}$ . Therefore, tight lower bounds that work for general graphs $G$ are still underexplored in the literature, and our regret upper bounds in Theorems 1.3 and 1.4 also improve or generalize the existing results. ", "page_idx": 3}, {"type": "text", "text": "The problem of bandits with feedback is also closely related to partial monitoring games (Bart\u00f3k et al., 2014). Although this is a more general setting which subsumes bandits with graph feedback, the results in the literature (Bart\u00f3k et al., 2014; Lattimore, 2022; Foster et al., 2023a) typically have tight dependence on $T$ , but often not on other parameters such as the dimensionality. Similar issues also applied to the recent line of work (Foster et al., 2021, 2023b) aiming to provide a unified complexity measure based on the decision-estimation coefficient (DEC); the nature of the two-point lower bound used there often leaves a gap. We also point to some recent work (Zhang et al., 2024) which adopted the DEC framework and established regret bounds for contextual bandits with graph feedback, but no cross-learning across contexts, based on regression oracles. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "2 Hard instance and the regret lower bound ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section we sketch the proof of the minimax lower bound $\\mathsf{R}_{T}^{\\star}(G,M,\\mathcal{C}_{\\mathsf{S A}})=\\Omega(\\sqrt{\\beta_{M}(G)T})$ for $T\\geq\\beta_{M}(G)^{3}$ and general $(G,M)$ , implying Theorem 1.1. We first identify a hard instance that corresponds to the graph-theoretic quantity $\\beta_{M}(G)$ , and then present the core exploration-exploitation tradeoff in the proof to arrive at the fundamental limit of learning under this instance. This approach has been widely adopted in the bandit literature. The complete proof is deferred to Appendix B. ", "page_idx": 4}, {"type": "text", "text": "The proof uses the definition (2) of $\\beta_{M}(G)$ to construct $M$ independent sets $I_{1},\\cdot\\cdot\\cdot,I_{M}$ such that $I_{i}\\ne I_{j}$ for $i<j$ ; by definition, the independent sets $I_{1},\\cdot\\cdot\\cdot,I_{M}$ are disjoint. We then construct a hard instance where the best action under context $c\\in[M]$ is distributed uniformly over $I_{c}$ ; since $I_{c}$ is an independent set, this ensures that the learner must essentially explore all actions in $I_{c}$ under context $c$ . Moreover, the context sequence $c^{T}$ is set to be $11\\cdot\\cdot\\cdot122\\cdot\\cdot\\cdot2\\cdot\\cdot\\cdot M$ , i.e. never goes back to previous contexts. This order ensures that the exploration in $I_{c_{1}}$ during earlier rounds provides no information to the exploration in $I_{c_{2}}$ during later rounds, whenever $c_{1}<c_{2}$ . Na\u00efvely, if the learner only explores in each $I_{c}$ under context $c$ , then learning under each context $c$ becomes a multi-armed bandit problem (because $I_{c}$ is itself an independent set), and we can show lower bound $\\sqrt{T\\sum_{c=1}^{M}\\left|I_{c}\\right|}$ with appropriate context sequence $c^{T}$ . Maximizing over all possible constructions gives the desired result. ", "page_idx": 4}, {"type": "text", "text": "It is possible, however, for the learner to choose actions outside $I_{c}$ to obtain information for the later rounds. To address this challenge, we use a delicate exploration-exploitation tradeoff argument to show that this pure exploration must incur a too large regret to be informative when $T\\geq\\bar{\\beta}_{M}(G)^{3}$ . Specifically, consider the regret incurred by this pure exploration: ", "page_idx": 4}, {"type": "equation", "text": "$$\n{\\mathsf{R e x p l o r e}}=\\sum_{c=1}^{M}\\sum_{t\\in T_{c}}\\mathbb{E}[\\mathbb{1}(a_{t}\\not\\in I_{c})]\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $T_{c}=\\{t\\in[T]:c_{t}=c\\}$ . Then for some absolute constants $c_{1}$ and $c_{2}$ , the tradeoff can be formulated as two lower bounds of the regret $\\mathsf{R}_{T}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathsf{R}_{T}\\geq c_{1}\\sqrt{\\beta_{M}(G)T}\\,\\mathrm{exp}(-\\beta_{M}(G)\\mathsf{R}_{\\mathrm{explore}}/T)\\quad\\mathrm{and}\\quad\\mathsf{R}_{T}\\geq c_{2}\\mathsf{R}_{\\mathrm{explore}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The first bound is decreasing in the amount of pure exploration, while the second one is increasing. Balancing this tradeoff gives the desired lower bound $\\mathsf{R}_{T}=\\Omega\\Big(\\sqrt{\\beta_{M}(G)T}\\Big)$ for $T\\geq\\beta_{M}(G)^{3}$ . In summary, the key structure used in the proof is that $I_{i}\\neq I_{j}$ for $i<j$ ; we remark that this does not preclude the possibility that $I_{j}\\to I_{i}$ for $j>i$ , which underlies the change from $\\alpha(G)$ to $\\mathsf{m}(G)$ as the number of context increases. ", "page_idx": 4}, {"type": "text", "text": "3 Algorithms achieving the regret upper bounds ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "This section provides algorithms that achieve the claimed regret upper bounds in Theorems 1.3 and 1.4. The crux of these algorithms is to exploit the structure of the feedback graph and choose a small number of actions to explore. Depending on whether the context sequence is self-avoiding or not, the above problem can be reduced to two different kinds of sequential games on the feedback graph. Given solutions to the sequential games, Sections 3.2 and 3.3 will rely on the layering technique to use these solutions on each layer, and propose the final learning algorithms via arm elimination. ", "page_idx": 4}, {"type": "text", "text": "3.1 Two sequential games on graphs ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section we introduce two sequential games on graphs which are purely combinatorial and independent of the learning process. We begin with the first sequential game. ", "page_idx": 4}, {"type": "text", "text": "Definition 1 (Sequential game I). Given a directed graph $G=(V,E)$ and a positive integer $M$ , the sequential game consists of $M$ steps, where at each step $c=1,\\cdot\\cdot\\cdot,M$ : ", "page_idx": 5}, {"type": "text", "text": "1. the adversary chooses a strongly observable subset $A_{c}\\subseteq V$ disjoint from $N_{\\mathrm{out}}(\\cup_{c^{\\prime}<c}D_{c^{\\prime}})$ ; ", "page_idx": 5}, {"type": "text", "text": "The learner\u2019s goal is to minimize the total size $\\sum_{c=1}^{M}|D_{c}|$ of the sets $D_{c}$ . ", "page_idx": 5}, {"type": "text", "text": "The above sequential game is motivated by bandit learning under self-avoiding contexts. Consider a self-avoiding context sequence in the order of $1,2,\\cdots,M$ . For $c\\in[M]$ , the set $A_{c}$ represents the \u201cactive set\u201d of actions, i.e. the set of all probably good actions, yet to be explored when context $c$ first occurs. Thanks to the self-avoiding structure, \u201cyet to be explored\u201d means that $A_{c}$ must be disjoint from $N_{\\mathrm{out}}\\big(\\cup_{c^{\\prime}<c}D_{c^{\\prime}}\\big)$ . The learner then plays a set of actions $D_{c}\\subseteq A_{c}$ to ensure that all actions in $A_{c}$ have been explored at least once; we note that a good choice of $D_{c}$ not only aims to observe all of $A_{c}$ , but also tries to observe as many actions as possible outside $A_{c}$ and make the complement of $N_{\\mathrm{out}}\\big(\\cup_{c^{\\prime}\\le c}D_{c^{\\prime}}\\big)$ small. The final cost $\\sum_{c=1}^{M}|D_{c}|$ characterizes the overall sample complexity to explore every active action once over all contexts. ", "page_idx": 5}, {"type": "text", "text": "It is clear that the minimax value of this sequential game is given by ", "page_idx": 5}, {"type": "equation", "text": "$$\nU_{1}^{\\star}(G,M)=\\operatorname*{max}_{A_{1}\\subseteq V}\\operatorname*{min}_{\\stackrel{D_{1}\\subseteq A_{1}}{A_{1}\\subseteq N_{\\mathrm{out}}(D_{1})}}\\cdot\\cdot\\cdot\\operatorname*{max}_{\\stackrel{A_{M}\\subseteq V}{\\cup_{c=1}^{M-1}D_{c}\\nrightarrow A_{M}}}\\operatorname*{min}_{A_{M}\\subseteq N_{\\mathrm{out}}(D_{M})}\\sum_{c=1}^{M}|D_{c}|.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The following lemma characterizes the quantity $U_{1}^{\\star}(G,M)$ up to an $O(\\log|V|)$ factor. ", "page_idx": 5}, {"type": "text", "text": "Lemma 3.1 (Minimax value of sequential game I). There exists an absolute constant $C>0$ that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\beta_{M}(G)\\leq U_{1}^{\\star}(G,M)\\leq C\\beta_{M}(G)\\log|V|.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Moreover, the learner can achieve a slightly larger upper bound $O(\\beta_{M}(G)\\log^{2}|V|)$ using a polynomial-time algorithm. ", "page_idx": 5}, {"type": "text", "text": "The second sequential game is motivated by bandit learning with an arbitrary context sequence. ", "page_idx": 5}, {"type": "text", "text": "Definition 2 (Sequential game II). Given a directed graph $G=(V,E)$ and a positive integer $M$ , the sequential game starts with an empty set $D_{0}=\\mathcal{D}$ , and at time $t=1,2,\\cdot\\cdot\\cdot$ : ", "page_idx": 5}, {"type": "text", "text": "1. the adversary chooses an integer $c_{t}\\in[M]$ (and a set $A_{c_{t}}\\subseteq V$ if $c_{t}$ does not appear before). The adversary must ensure that $A_{c_{t}}\\backslash N_{\\mathrm{out}}(D_{t-1})$ is non-empty; ", "page_idx": 5}, {"type": "text", "text": "The game terminates at time $T$ whenever the adversary has no further move (i.e. $\\cup_{c}A_{c}\\subseteq N_{\\mathrm{out}}(D_{T}))$ , and the learner\u2019s goal is to minimize the duration $T$ of the game. ", "page_idx": 5}, {"type": "text", "text": "The new sequential game reflects the case where the context sequence might not be self-avoiding, so instead of taking a set of actions at once, the learner now needs to take actions non-consecutively. Clearly the sequential game $\\mathrm{II}$ is more difficult for the learner as it subsumes the sequential game I when the context sequence is self-avoiding: the set $D_{c}$ in Definition 1 is simply the collection of $v_{t}$ \u2019s in Definition 2 whenever $c_{t}=c$ . Consequently, the minimax values satisfy $\\bar{U}_{2}^{\\star}(G,M)\\geq U_{1}^{\\star}(G,M)$ The following lemma proves an upper bound on $U_{2}^{\\star}(G,M)$ . ", "page_idx": 5}, {"type": "text", "text": "Lemma 3.2 (Minimax value of sequential game II). There exists a polynomial-time algorithm for the learner which achieves ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{U_{2}^{\\star}(G,M)\\leq\\beta_{\\sf d o m}(G,M)\\leq\\operatorname*{min}\\{\\mathfrak{m}(G),C\\overline{{\\beta}}_{M}(G)\\log^{2}|V|\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $C>0$ is an absolute constant, ${\\overline{{\\beta}}}_{M}(G)$ is given in (3), and ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\beta_{\\mathsf{d o m}}(G,M)=\\operatorname*{max}\\bigg\\{\\displaystyle\\sum_{c=1}^{M}|B_{c}|:\\bigcup_{c}B_{c}\\;i s\\;a c y c l i c,\\;B_{c}\\subseteq V_{c}\\;d o m i n a t e s\\;s o m e\\;V_{c}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\quad w i t h\\;d i s j o i n t\\;V_{1},\\cdots,V_{M}\\subseteq V,\\;a n d\\,|B_{c}|\\leq\\delta(V_{c})(1+\\log|V|).\\bigg\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "3Both sets $(A_{c},D_{c})$ are allowed to be empty. ", "page_idx": 5}, {"type": "text", "text": "3.2 Learning under self-avoiding contexts ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Given a learner\u2019s algorithm for the first sequential game, we are ready to provide an algorithm for bandit learning under any self-avoiding context sequence. The algorithm relies on the well-known idea of arm elimination (Even-Dar et al., 2006): for each context $c\\in[M]$ , we maintain an active set $A_{c}$ consisting of all probably good actions so far under this context based on usual confidence bounds of the rewards. To embed the sequential games into the algorithm, we further make use of the layering technique in (Lykouris et al., 2020; Dann et al., 2020): for $\\ell\\in\\mathbb{N}$ , we construct the set $A_{c,\\ell}$ as the active set on layer $\\ell$ such that all actions in $A_{c,\\ell-1}$ have been taken for at least $\\ell-1$ times. In other words, the active set $A_{c,\\ell}$ is formed based on $\\ell-1$ reward observations of all currently active actions. As higher layer indicates higher estimation accuracy, the learner now aims to minimize the duration of each layer $\\ell$ , which is precisely the place we will play an independent sequential game. ", "page_idx": 6}, {"type": "text", "text": "Algorithm 1: Arm elimination algorithm for self-avoiding contexts ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Input: time horizon $T$ , action set $[K]$ , context set $[M]$ , feedback graph $G$ , a subroutine $\\boldsymbol{\\mathcal{A}}$ for the   \nsequential game I, failure probability $\\delta\\in(0,1)$ .   \nInitialize: active sets $A_{c,1}\\leftarrow[K]$ for all contexts $c\\in[M]$ on layer 1.   \nfor $c=1$ to $M$ do for $\\ell=1,2,\\cdots$ do compute $D_{c,\\ell}\\subseteq A_{c,\\ell}\\backslash N_{\\mathrm{out}}(\\cup_{c^{\\prime}<c}D_{c^{\\prime},\\ell})$ according to the subroutine $\\boldsymbol{\\mathcal{A}}$ , based on past plays $\\big(A_{c^{\\prime},\\ell}\\backslash N_{\\mathrm{out}}(\\cup_{i<c^{\\prime}}D_{i,\\ell})\\big)_{c^{\\prime}\\leq c}$ and $(D_{c^{\\prime},\\ell})_{c^{\\prime}<c}$ ; choose each action in $D_{c,\\ell}$ once (break the loop if $c_{t}\\neq c$ or $t>T$ during this process), and update $t$ accordingly; compute the empirical rewards $\\bar{r}_{c,a}$ for all actions based on all historic reward observations; choose the following active set on the next layer: $A_{c,\\ell+1}\\leftarrow\\left\\{a\\in A_{c,\\ell}:\\bar{r}_{c,a}\\geq\\operatorname*{max}_{a^{\\prime}\\in A_{c,\\ell}}\\bar{r}_{c,a^{\\prime}}-2\\sqrt{\\frac{\\log(2M K T/\\delta)}{\\ell}}\\right\\};$ (5) move to the next layer $\\ell\\gets\\ell+1$ . end ", "page_idx": 6}, {"type": "text", "text": "end ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The description of the algorithm is summarized in Algorithm 1, and we assume without loss of generality that the self-avoiding contexts comes in the order of $1,\\cdot\\cdot\\cdot,M$ (the duration of some contexts might be zero). During each context, Algorithm 1 sequentially constructs a shrinking sequence of active sets $A_{c,1}\\supseteq A_{c,2}\\supseteq\\cdots,$ , and on each layer $\\ell$ , the algorithm plays the sequential game I based on the current status (past plays $(A_{c^{\\prime},\\ell})_{c^{\\prime}\\leq c}$ , or equivalently $\\bar{(A_{c^{\\prime},\\ell}^{-}\\backslash N_{\\mathrm{out}}(\\cup_{i<c^{\\prime}}D_{i,\\ell}))}_{c^{\\prime}\\leq c}$ , of the adversary, and past plays $(D_{c^{\\prime},\\ell})_{c^{\\prime}<c}$ of the learner).4 After the rewards of all actions of $A_{c,\\ell}$ have been observed once, the algorithm constructs the active set $A_{c,\\ell+1}$ for the next layer based on the confidence bound (5) and sample size $\\ell$ . ", "page_idx": 6}, {"type": "text", "text": "The following theorem summarizes the performance of the algorithm. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3.3 (Regret upper bound of Algorithm 1). Let the subroutine $\\boldsymbol{\\mathcal{A}}$ for the sequential game I be the polynomial-time algorithm given by Lemma 3.1. Then with probability at least $1-\\delta$ , the regret of Algorithm $^{\\,I}$ is upper bounded by ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathsf{R}_{T}(\\mathsf{A l g\\ 1};G,M,\\mathcal{C}_{\\mathsf{S A}})=O\\left(\\sqrt{T\\beta_{M}(G)\\log^{2}(K)\\log(M K T/\\delta)}\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "On a high level, by classical confid\u221aence bound arguments, each action chosen on layer $\\ell$ suffers from an instantaneous regret $\\widetilde{O}(1/\\sqrt{\\ell})$ . Moreover, Lemma 3.1 shows that the number of actions chosen on a given layer is at most $\\widetilde{\\cal O}(\\beta_{M}(G))$ . A combination of these two observations leads to the $\\widetilde{\\cal O}(\\sqrt{\\beta_{M}(G)T})$ upper bound in Theorem 3.3, and a full proof is provided in Appendix C. ", "page_idx": 6}, {"type": "text", "text": "3.3 Learning under general contexts ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The learning algorithm under a general context sequence is described in Algorithm 2. Similar to Algorithm 1, for each context $c$ we break the learning process into different layers, construct active sets $A_{c,\\ell}$ for each layer, and move to the next layer whenever all actions in $A_{c,\\ell}$ have been observed once on layer $\\ell$ . The only difference lies in the choice of actions on layer $\\ell$ , where the plays from the sequential game $\\mathrm{II}$ are now used. The following theorem summarizes the performance of Algorithm 2, whose proof is very similar to Theorem 3.3 and deferred to Appendix C. ", "page_idx": 7}, {"type": "text", "text": "Algorithm 2: Arm elimination under general contexts ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Input: time horizon $T$ , action set $[K]$ , context set $[M]$ , feedback graph $G$ , a subroutine $\\boldsymbol{\\mathcal{A}}$ for the   \nsequential game II, failure probability $\\delta\\in(0,1)$ .   \nInitialize: active sets $A_{c,\\ell}\\gets[K]$ for all contexts $c\\in[M]$ and layers $\\ell\\geq1$ ; set of actions   \n$D_{\\ell}\\leftarrow\\emptyset$ chosen on layer $\\ell$ ; the current layer index $\\ell(c)\\gets1$ for all $c\\in[M]$ .   \nfor $t=1$ to $T$ do receive the context $c_{t}$ , and compute the current layer index $\\ell_{t}=\\ell(c_{t})$ ; according to subroutine $\\boldsymbol{\\mathcal{A}}$ , choose an action $a_{t}\\in A_{c_{t},\\ell_{t}}$ based on the active sets $(A_{c,\\ell_{t}})_{c\\in[M]}$ and previously taken actions $D_{\\ell_{t}}$ on the current layer; update the set of actions on layer $\\ell_{t}$ via $D_{\\ell_{t}}\\leftarrow D_{\\ell_{t}}\\cup\\{a_{t}\\}$ ; for $c\\in[M]$ do compute the new layer index $\\ell_{\\mathsf{n e w}}(c)=\\operatorname*{min}\\{\\ell:A_{c,\\ell}\\not\\subseteq N_{\\mathrm{out}}(D_{\\ell})\\};$ if $\\bar{\\ell_{\\mathsf{n e w}}}(c)>\\ell(c)$ then compute the empirical rewards $\\bar{r}_{c,a}$ for all actions based on all historic observations; choose the following active set on the new layer: $A_{c,\\ell_{\\mathsf{n e w}}(c)}\\gets\\left\\{a\\in A_{c,\\ell(c)}:\\bar{r}_{c,a}\\geq\\operatorname*{max}_{a^{\\prime}\\in A_{c,\\ell(c)}}\\bar{r}_{c,a^{\\prime}}-2\\sqrt{\\frac{\\log(2M K T/\\delta)}{\\ell_{\\mathsf{n e w}}(c)-1}}\\right\\};$ update the layer index $\\ell(c)\\gets\\ell_{\\mathsf{n e w}}(c)$ . end end ", "page_idx": 7}, {"type": "text", "text": "end ", "page_idx": 7}, {"type": "text", "text": "Theorem 3.4 (Regret upper bound of Algorithm 2). Let the subroutine $\\boldsymbol{\\mathcal{A}}$ for the sequential game II be the polynomial-time algorithm given by Lemma 3.2. Then with probability at least $1-\\delta$ , the regret of Algorithm 2 is upper bounded by ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathsf{R}_{T}(\\mathsf{A l g\\ 2};G,M)=O\\left(\\sqrt{T\\beta_{\\mathsf{d o m}}(G,M)\\log(M K T/\\delta)}\\right).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "By the second inequality in Lemma 3.2, Theorem 3.4 implies Theorem 1.4. Corollary 1.5 then follows from the following result. ", "page_idx": 7}, {"type": "text", "text": "Lemma 3.5. For undirected or transitively closed graph $G$ , it holds that $\\beta_{\\mathsf{d o m}}(G,M)~=$ $O(\\beta_{M}(G)\\log|V|)$ . ", "page_idx": 7}, {"type": "text", "text": "4 Discussions ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "4.1 Weakly observable feedback graphs ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Naturally, we may ask what results we would get under a weaker assumption $/$ a more general feedback structure. If the feedback graph $G$ is instead weakly observable5, then under complete cross-learning, an explore-then-commit (ETC) policy can achieve regret $\\widetilde{\\cal O}(\\delta(G)^{1/3}T^{2/3})$ by first exploring the minimum dominating $\\mathrm{set}^{6}$ uniformly for time $\\delta(G)^{1/3}T^{2/3}$ , and then committing to the empirically best action that has suboptimality bounded by $\\widetilde{\\cal O}(\\delta(G)^{1/3}T^{-1/3})$ with high probability. This matches the existing lower bound in (Alon et al., 2015 ) and is hence near-optimal. ", "page_idx": 7}, {"type": "text", "text": "4.2 Incomplete cross-learning ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "It is possible to further relax the assumption of complete cross-learning. Suppose the feedback across contexts is characterized by another directed graph $G_{[M]}$ (and denote $G_{[K]}$ across actions respectively), and consider a product feedback graph $G_{[K]}\\times G_{[M]}^{\\dot{}}$ over the context-action pairs such that $(a_{1},c_{1})\\rightarrow(a_{2},c_{2})$ if $a_{1}\\rightarrow a_{2}$ in $G_{[K]}$ and $c_{1}\\to c_{2}$ in $G_{[M]}$ . Then we can get the following generalized results. ", "page_idx": 8}, {"type": "text", "text": "4.2.1 Weakly observable feedback graphs on actions ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "When the feedback graph $G_{[K]}$ is weakly observable, following the argument in Section 4.1, we can achieve regret $\\widetilde{\\cal O}\\Big(\\big(\\delta(G_{[K]})\\mathsf{\\dot{m}}(G_{[M]})\\big)^{1/3}T^{2/3}\\Big)$ by running an ETC subroutine for each context as follows: for every context $c\\in[M]$ , we keep an \u201cexploration\u201d counter $n_{c}$ . At each time $t$ with context $c_{t}$ , if $n_{c t}\\lesssim\\delta(G_{[K]})^{1/3}\\mathsf{m}(\\bar{G}_{[M]})_{.}^{-2/3}T^{2/3}$ , we are in the \u201cexploration\u201d stage and continue to uniformly explore the minimum dominating set of $G_{[K]}$ . Then we increase the counter for all observed contexts, i.e. $n_{c}\\gets n_{c}+1$ for all $c\\in N_{\\mathrm{out}}(c_{t})$ in $G_{[M]}$ . Otherwise, we \u201ccommit\u201d to the empirically best action that has suboptimality bounded by $\\widetilde{\\cal O}\\Big(\\big(\\delta(G_{[K]}){\\sf m}(G_{[M]})\\big)^{1/3}T^{-1/3}\\Big)$ with high probability. ", "page_idx": 8}, {"type": "text", "text": "The key observation is that the number of times we are in the \u201cexploration\u201d stage is $\\widetilde{\\cal O}\\Big(\\big(\\delta(\\dot{G}_{[K]}){\\sf m}(G_{[M]})\\big)^{1/3}T^{2/3}\\Big)$ . This can be seen from a layering argument, similar to the one in Section 3.2, that the number of actually played contexts on each layer is at most $\\mathsf{m}(G_{[M]})$ . Together with the bounded rewards and the bounded suboptimality in the \u201ccommit\u201d stage, this proves the regret upper bound. ", "page_idx": 8}, {"type": "text", "text": "Combining the context sequence construction in Section 2 and the lower bound argument in (Alon et al., 2015), one can also prove a matching lower bound $\\Omega{\\left(\\left(\\delta(G_{[K]})\\mathsf{m}(G_{[M]})\\right)^{1/3}T^{2/3}\\right)}$ . ", "page_idx": 8}, {"type": "text", "text": "4.2.2 Strongly observable feedback graphs on actions ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "When $G_{[K]}$ is strongly observable, it is straightforward to generalize our upper (for self-avoiding contexts) and lower bounds in Theorem 1.1 and 1.3 with $\\beta_{M}(G_{[K]})$ replaced by ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\beta_{M}(G_{[K]}\\times G_{[M]})=\\operatorname*{max}\\left\\{\\sum_{c=1}^{M}|I_{c}|:I_{c}\\right.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "and $\\beta_{\\mathsf{d o m}}(G_{[K]})$ in Theorem 3.4 by ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\beta_{\\mathsf{d o m}}(G_{[K]}\\times G_{[M]})=\\operatorname*{max}\\bigg\\{\\sum_{c=1}^{M}|B_{c}|:\\bigcup_{c}B_{c}\\;i s\\;a c y c l i c\\;i n\\;G_{[K]}\\times G_{[M]},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where the new graph quantities are defined on the product graph $G_{[K]}\\,\\times\\,G_{[M]}$ . For general contexts, this gives a tight upper bound $\\tilde{O}\\bigl(\\sqrt{T\\beta_{M}(G_{[K]}\\times G_{[M]})}\\bigr)$ when $G_{[K]}$ and $G_{[M]}$ are either both undirected or both transitively closed7. Most generally, we have a loose upper bound $\\widetilde{O}\\big(\\sqrt{T\\operatorname*{min}\\{\\mathsf{m}(G_{[K]}\\times G_{[M]}),\\alpha(G_{[K]})M\\}}\\big)$ . ", "page_idx": 8}, {"type": "text", "text": "4.3 Gap between upper and lower bounds ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Although we provide tight upper and lower bounds for specific classes of context sequences (selfavoiding in Theorem 1.3) or feedback graphs (undirected or transitively closed in Corollary 1.5), in general the quantities $\\beta_{M}(G)$ in Theorem 1.1 and $\\operatorname*{min}\\{\\overline{{\\beta}}_{M}(G),\\mathfrak{m}(G)\\}$ in Theorem 1.4 exhibit a gap. The following lemma gives an upper bound on this gap. ", "page_idx": 8}, {"type": "text", "text": "Lemma 4.1. For any graph $G$ , it holds that ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\beta_{M}(G)\\leq\\operatorname*{min}\\{\\overline{{\\beta}}_{M}(G),\\mathfrak{m}(G)\\}\\leq\\operatorname*{max}\\left\\{\\frac{\\rho(G)}{M},1\\right\\}\\beta_{M}(G),\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "where $\\rho(G)$ denotes the length of the longest path in $G$ . ", "page_idx": 9}, {"type": "text", "text": "Lemma 4.1 shows that if $G$ does not contain long paths or $M$ is large, the gap between ${\\overline{{\\beta}}}_{M}(G)$ and $\\beta_{M}(G)$ is not significant. We also comment on the challenge of closing this gap. First, we do not know a tight characterization of the minimax value of the sequential game $\\mathrm{II}$ (cf. Definition 2), and the upper bound $\\beta_{\\mathsf{d o m}}(G,M)$ in Lemma 3.2 could be loose, as shown in the following example. ", "page_idx": 9}, {"type": "text", "text": "Example 1. Consider an acyclic graph $G\\,=\\,(V,E)$ with $K M$ vertices $\\{(i,j)\\}_{i\\in[K],j\\in[M]}$ and edges $\\left(i,j\\right)\\,\\rightarrow\\,\\left(i^{\\prime},j^{\\prime}\\right)$ if either $i<i^{\\prime}$ and $j\\neq j^{\\prime}$ , or $i\\,=\\,i^{\\prime}$ and $j<j^{\\prime}$ , and all self-loops. By choosing $\\dot{B_{c}}=\\dot{V_{c}}=\\dot{\\{(i,c):i\\in[K]\\}}$ in the definition of $\\beta_{\\mathsf{d o m}}(G,M)$ in Lemma 3.2, it is clear that $\\beta_{\\mathsf{d o m}}(G,M)=K M$ . However, we show that the minimax value is $U_{2}^{\\star}(G,M)=K+M-1$ . The lower bound follows from $U_{2}^{\\star}(G,M)\\geq U_{1}^{\\star}(G,M)\\geq\\beta_{M}(G)\\geq K^{\\star}+M-1$ , as $I_{1}=\\{(i,M):$ $i\\;\\in\\;[K]\\}$ and $I_{c}\\,=\\,\\left\\{(1,M^{\\circ}{+}\\,1\\,{-}\\,\\stackrel{.}{c})\\right\\}$ for $2\\,\\leq\\,c\\,\\leq\\,M$ satisfy the constraints in the definition of $\\beta_{M}(G)$ in (2). For the upper bound, we consider the following strategy for the learner in the sequential game $I I$ : $v_{t}=\\left(i_{t},j_{t}\\right)$ is the smallest element (under the lexicographic order over pairs) in $A_{c_{t}}\\backslash N_{\\mathrm{out}}(D_{t-1})$ . To show why $U_{2}^{\\star}(G,M)\\leq K+M-1,$ , let $D_{c}$ be the final set of vertices chosen by the learner under context c. By the lexicographic order and the structure of $G$ , each $D_{c}$ can only consist of vertices in one column. Moreover, for different $c\\neq c^{\\prime}$ , the row indices of $D_{c}$ must be entirely no smaller or entirely no larger than the row indices of $D_{c^{\\prime}}$ . These constraints ensure that $\\begin{array}{r}{\\sum_{c=1}^{M}|D_{c}|\\le K+M-1.}\\end{array}$ ", "page_idx": 9}, {"type": "text", "text": "This example shows the importance of non-greedy approaches when choosing $v_{t}$ . In the special case where $A_{c}=\\{(i,c):i\\in[\\bar{K}]\\}$ is the $c$ -th column, within $A_{c}$ this is an independent set, so any greedy approach that does not look outside $A_{c}$ will treat the vertices in $A_{c}$ indifferently. In contrast, the above approach makes use of the global structure of the graph $G$ . ", "page_idx": 9}, {"type": "text", "text": "The second challenge lies in the proof of the lower bound. Instead of the sequential game where the adversary and the learner take turns to play actions, the current lower bound argument assumes that the adversary tells all his plays to the learner ahead of time. We expect the sequential structure to be equally important for the lower bounds, and it is interesting to work out an argument for the minimax lower bound to arrive at a sequential quantity like $U_{2}^{\\star}(G,M)$ . ", "page_idx": 9}, {"type": "text", "text": "4.4 Other open problems ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Performance of the UCB algorithm. The UCB algorithm under feedback graphs has been analyzed for both multi-armed (Lykouris et al., 2020) and contextual bandits (Dann et al., 2020). However, both results only show a regret upper bound $\\widetilde{O}(\\sqrt{\\mathsf{m}(G)T})$ , even in the case of multi-armed bandits (i.e. $M=1$ ). It is interesting to understand for algorithms without forced exploration (such as UCB), if the MAS number $\\mathsf{m}(G)$ (rather than $\\alpha(G)$ or $\\bar{\\beta}_{M}(G)]$ ) turns out to be fundamental. ", "page_idx": 9}, {"type": "text", "text": "Regret for small $T$ . Note that our upper bounds hold for all values of $T$ , but our lower bound requires $T\\;\\geq\\;\\beta_{M}(G)^{3}$ . This is not an artifact of the analysis, as the optimal regret becomes fundamentally different for smaller $T$ . The case of multi-armed bandits has been\u221a solved completely in a recent work (Koc\u00e1k and Carpentier, 2023), where the regret is a mixture of $\\sqrt{T}$ and $T^{2/3}$ terms. We anticipate the same behavior for contextual bandits, but the exact form is unknown. ", "page_idx": 9}, {"type": "text", "text": "Stochastic contexts. In this paper we assume that the contexts are generated adversarially, but the case of stochastic contexts also draws some recent attention (Balseiro et al., 2019; Schneider and Zimmert, 2024), and sometimes there is a fundamental gap between the optimal performances under stochastic and adversarial contexts (Han et al., 2024). It is an interesting question whether this is the case for contextual bandits with graph feedback. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work is generously funded by the NSF grant CCF 2106508. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Alekh Agarwal, Miroslav Dud\u00edk, Satyen Kale, John Langford, and Robert Schapire. Contextual bandit learning with predictable rewards. In Artificial Intelligence and Statistics, pages 19\u201326. PMLR, 2012. ", "page_idx": 10}, {"type": "text", "text": "Shipra Agrawal and Nikhil Devanur. Linear contextual bandits with knapsacks. Advances in Neural Information Processing Systems, 29, 2016.   \nShipra Agrawal and Navin Goyal. Thompson sampling for contextual bandits with linear payoffs. In International conference on machine learning, pages 127\u2013135. PMLR, 2013.   \nNoga Alon and Joel H Spencer. The probabilistic method. John Wiley & Sons, 2016.   \nNoga Alon, Nicolo Cesa-Bianchi, Ofer Dekel, and Tomer Koren. Online learning with feedback graphs: Beyond bandits. In Conference on Learning Theory, pages 23\u201335. PMLR, 2015.   \nNoga Alon, Nicolo Cesa-Bianchi, Claudio Gentile, Shie Mannor, Yishay Mansour, and Ohad Shamir. Nonstochastic multi-armed bandits with graph-structured feedback. SIAM Journal on Computing, 46(6):1785\u20131826, 2017.   \nPeter Auer. Using confidence bounds for exploitation-exploration trade-offs. Journal of Machine Learning Research, 3(Nov):397\u2013422, 2002.   \nSantiago Balseiro, Negin Golrezaei, Mohammad Mahdian, Vahab Mirrokni, and Jon Schneider. Contextual bandits with cross-learning. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.   \nG\u00e1bor Bart\u00f3k, Dean P Foster, D\u00e1vid P\u00e1l, Alexander Rakhlin, and Csaba Szepesv\u00e1ri. Partial monitoring\u2014classification, regret bounds, and algorithms. Mathematics of Operations Research, 39(4): 967\u2013997, 2014.   \nDjallel Bouneffouf, Irina Rish, and Charu Aggarwal. Survey on applications of multi-armed and contextual bandits. In 2020 IEEE Congress on Evolutionary Computation (CEC), pages 1\u20138. IEEE, 2020.   \nHoushuang Chen, Shuai Li, Chihao Zhang, et al. Understanding bandits with graph feedback. Advances in Neural Information Processing Systems, 34:24659\u201324669, 2021.   \nWei Chu, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual bandits with linear payoff functions. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, pages 208\u2013214. JMLR Workshop and Conference Proceedings, 2011.   \nVasek Chvatal. A greedy heuristic for the set-covering problem. Mathematics of operations research, 4(3):233\u2013235, 1979.   \nChristoph Dann, Yishay Mansour, Mehryar Mohri, Ayush Sekhari, and Karthik Sridharan. Reinforcement learning with feedback graphs. Advances in Neural Information Processing Systems, 33: 16868\u201316878, 2020.   \nKhaled Eldowa, Emmanuel Esposito, Tom Cesari, and Nicol\u00f2 Cesa-Bianchi. On the minimax regret for online learning with feedback graphs. Advances in Neural Information Processing Systems, 36, 2024.   \nEyal Even-Dar, Shie Mannor, Yishay Mansour, and Sridhar Mahadevan. Action elimination and stopping conditions for the multi-armed bandit and reinforcement learning problems. Journal of machine learning research, 7(6), 2006.   \nUriel Feige, Shaf iGoldwasser, L\u00e1szl\u00f3 Lov\u00e1sz, Shmuel Safra, and Mario Szegedy. Approximating clique is almost np-complete. In [1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science, pages 2\u201312. IEEE Computer Society, 1991.   \nDean Foster, Dylan J Foster, Noah Golowich, and Alexander Rakhlin. On the complexity of multiagent decision making: From learning in games to partial monitoring. In The Thirty Sixth Annual Conference on Learning Theory, pages 2678\u20132792. PMLR, 2023a.   \nDylan J Foster, Sham M Kakade, Jian Qian, and Alexander Rakhlin. The statistical complexity of interactive decision making. arXiv preprint arXiv:2112.13487, 2021.   \nDylan J Foster, Noah Golowich, and Yanjun Han. Tight guarantees for interactive decision making with the decision-estimation coefficient. In The Thirty Sixth Annual Conference on Learning Theory, pages 3969\u20134043. PMLR, 2023b.   \nZijun Gao, Yanjun Han, Zhimei Ren, and Zhengqing Zhou. Batched multi-armed bandits problem. Advances in Neural Information Processing Systems, 32, 2019.   \nFabrizio Grandoni. A note on the complexity of minimum dominating set. Journal of Discrete Algorithms, 4(2):209\u2013214, 2006.   \nYanjun Han, Tsachy Weissman, and Zhengyuan Zhou. Optimal no-regret learning in repeated first-price auctions. Operations Research, 2024.   \nBotao Hao, Tor Lattimore, and Chao Qin. Contextual information-directed sampling. In International Conference on Machine Learning, pages 8446\u20138464. PMLR, 2022.   \nWoonghee Tim Huh and Paat Rusmevichientong. A nonparametric asymptotic analysis of inventory planning with censored demand. Mathematics of Operations Research, 34(1):103\u2013123, 2009.   \nRichard M Karp. Reducibility among combinatorial problems. Springer, 2010.   \nTom\u00e1\u0161 Koc\u00e1k and Alexandra Carpentier. Online learning with feedback graphs: The true shape of regret. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 17260\u201317282. PMLR, 23\u201329 Jul 2023.   \nTor Lattimore. Minimax regret for partial monitoring: Infinite outcomes and rustichini\u2019s regret. In Conference on Learning Theory, pages 1547\u20131575. PMLR, 2022.   \nLihong Li, Yu Lu, and Dengyong Zhou. Provably optimal algorithms for generalized linear contextual bandits. In International Conference on Machine Learning, pages 2071\u20132080. PMLR, 2017.   \nThodoris Lykouris, \u00c9va Tardos, and Drishti Wali. Feedback graph regret bounds for thompson sampling and ucb. In Aryeh Kontorovich and Gergely Neu, editors, Proceedings of the 31st International Conference on Algorithmic Learning Theory, volume 117 of Proceedings of Machine Learning Research, pages 592\u2013614. PMLR, 08 Feb\u201311 Feb 2020.   \nShie Mannor and Ohad Shamir. From bandits to experts: On the value of side-observations. In J. Shawe-Taylor, R. Zemel, P. Bartlett, F. Pereira, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems, volume 24. Curran Associates, Inc., 2011.   \nJon Schneider and Julian Zimmert. Optimal cross-learning for contextual bandits with unknown context distributions. Advances in Neural Information Processing Systems, 36, 2024.   \nMengxiao Zhang, Yuheng Zhang, Olga Vrousgou, Haipeng Luo, and Paul Mineiro. Practical contextual bandits with feedback graphs. Advances in Neural Information Processing Systems, 36, 2024.   \nHaoyu Zhao and Wei Chen. Stochastic one-sided full-information bandit. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 150\u2013166. Springer, 2019.   \nYinglun Zhu and Paul Mineiro. Contextual bandits with smooth regret: Efficient learning in continuous action spaces. In International Conference on Machine Learning, pages 27574\u201327590. PMLR, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Auxilary Lemmas ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Lemma A.1 (Lemma 8 of (Alon et al., 2015)). For any directed graph $G\\,=\\,(V,E)$ , one has $\\delta(G)\\leq50\\alpha(G)\\log|V|$ . ", "page_idx": 12}, {"type": "text", "text": "For a directed graph $G$ , there is a well-known approximate algorithm for finding the smallest dominating set: starting from $D=\\emptyset$ , recursively find the vertex $v$ with the maximum out-degree in the subgraph induced by $V\\backslash N_{\\mathrm{out}}(D)$ , and update $D\\leftarrow D\\cup\\{v\\}$ . The following lemma summarizes the performance of this algorithm. ", "page_idx": 12}, {"type": "text", "text": "Lemma A.2 (Chvatal (1979)). For any graph $G=(V,E)$ , the above procedure outputs a dominating set $D$ with ", "page_idx": 12}, {"type": "equation", "text": "$$\n|D|\\leq(1+\\log|V|)\\delta(G).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Lemma A.3 (A special case of Lemma 3 in (Gao et al., 2019)). Let $Q_{1},\\ldots Q_{n}$ be probability measures on some common measure space $(\\Omega,{\\mathcal{F}})$ , with $n\\geq2$ , and $\\Phi:\\Omega\\rightarrow[n]$ any measurable test function. Then ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\sum_{i=1}^{n}Q_{i}(\\Phi\\neq i)\\ge\\frac{1}{2n}\\sum_{i=2}^{n}\\exp(-\\mathsf{K L}(Q_{i}\\|Q_{1})).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "B Deferred Proof for the Lower Bound ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In this appendix, we give the complete proof of the minimax lower bound $\\mathsf{R}_{T}^{\\star}(G,M,\\mathcal{C}_{\\mathsf{S A}})\\;=$ $\\Omega(\\sqrt{\\beta_{M}(G)T})$ for $T\\geq\\beta_{M}(G)^{3}$ and general $(G,M)$ , implying Theorem 1.1. ", "page_idx": 12}, {"type": "text", "text": "Let $I_{1},\\cdot\\cdot\\cdot,I_{M}$ be the independent sets achieving the maximum in (2), by removing empty sets, combining $(I_{i},I_{i+1})$ whenever $|I_{i}|=1$ , and possibly removing the last set $I_{M}$ if $\\left|I_{M}\\right|=1$ , we arrive at disjoint subsets $J_{1},\\cdot\\cdot\\cdot,J_{m}$ of $[K]$ such that the following conditions hold: ", "page_idx": 12}, {"type": "text", "text": "\u2022 $m\\leq M$ , $K_{c}\\triangleq|J_{c}|\\geq2$ for all $c\\in[m]$ , and $J_{i}\\neq J_{j}$ for $i<j$ ; \u2022 the only possible non-self-loop edges among $J_{c}=\\{a_{c,1},\\cdot\\cdot\\cdot\\,,a_{c,K_{c}}\\}$ can only point to $\\boldsymbol{a}_{c,1}$ ; $\\begin{array}{r}{\\bullet\\,\\sum_{c=1}^{m}K_{c}\\geq\\sum_{c=1}^{M}|I_{c}|-1=\\beta_{M}(G)-1\\geq\\beta_{M}(G)/2}\\end{array}$ whenever $\\beta_{M}(G)\\geq2$ .8 ", "page_idx": 12}, {"type": "text", "text": "Given sets $J_{1},\\cdot\\cdot\\cdot,J_{m}$ , we are ready to specify the hard instance. Let $u=(u_{1},\\cdot\\cdot\\cdot\\,,u_{m})\\in\\Omega:=$ $[K_{1}]\\times\\cdot\\cdot\\times[K_{m}]$ be a parameter vector, the joint reward distribution $P^{u}$ of $(r_{t,c,a})_{c\\in[M],a\\in[K]}$ is a product distribution $\\begin{array}{r}{P^{u}=\\prod_{c\\in[M],a\\in[K]}{\\mathsf{B e r n}}(\\mu_{c,a}^{u})}\\end{array}$ , where the mean parameters for the Bernoulli distributions are $\\mu_{c,a}^{u}=0$ whenever $c>m$ , and ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mu_{c,a}^{u}=\\left\\{\\begin{array}{l l}{\\frac{1}{4}+\\Delta}&{\\mathrm{if~}a=a_{c,1},}\\\\ {\\frac{1}{4}+2\\Delta}&{\\mathrm{if~}a=a_{c,u_{c}}\\mathrm{~and~}u_{c}\\neq1,}\\\\ {\\frac{1}{4}}&{\\mathrm{if~}a\\in J_{c}\\backslash\\{a_{c,u_{c}}\\},}\\\\ {0}&{\\mathrm{if~}a\\not\\in J_{c},}\\end{array}\\right.\\quad\\mathrm{for~}c\\in[m].}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Here $\\Delta\\in(0,1/4)$ is a gap parameter to be chosen later. We summarize some useful properties from the above construction: ", "page_idx": 12}, {"type": "text", "text": "1. under context $c\\in[m]$ , the best action under $P_{u}$ is $\\boldsymbol{a}_{c,u_{c}}$ , and all other actions suffer from an instantaneous regret at least $\\Delta$ ; 2. under context $c\\in[m]$ , actions outside $J_{c}$ suffer from an instantaneous regret at least $1/4$ ; 3. for $u=(u_{1},\\cdot\\cdot\\cdot\\,,u_{m})\\in\\Omega$ and $u^{c}:=(u_{1},\\cdot\\cdot\\cdot\\cdot,u_{c-1},1,u_{c+1},\\cdot\\cdot\\cdot,u_{m})$ , the $\\mathrm{KL}$ divergence between the observed reward distributions $P^{u}(a)$ and $P^{u^{c}}(a)$ when choosing the action $a$ is $\\begin{array}{r l}{\\mathsf{K L}(P^{u^{c}}(a)\\|P^{u}(a))\\overset{(a)}{=}\\bigg\\{\\mathsf{K L}(\\mathsf{B e r n}(1/4)\\|\\mathsf{B e r n}(1/4+2\\Delta))}&{\\mathrm{if~}a\\to a_{c,u_{c}}}\\\\ {0}&{\\mathrm{otherwise~}}\\\\ {\\overset{(b)}{\\le}\\frac{64\\Delta^{2}}{3}\\mathbb{1}(a\\notin J_{\\le c}\\backslash\\{a_{c,u_{c}}\\}).}\\end{array}$ ", "page_idx": 12}, {"type": "text", "text": "Here (a) follows from our construction of $P^{u}$ that the only difference between $P^{u}$ and $P^{u^{c}}$ is the reward of action $\\boldsymbol{a}_{c,u_{c}}$ , which is observed iff $a~\\rightarrow~a_{c,u_{c}}$ ; (b) is due to the property of $\\left(J_{1},\\cdot\\cdot\\cdot,J_{m}\\right)$ that any action in $J_{\\leq c}\\backslash\\{a_{c,u_{c}}\\}$ does not point to $\\boldsymbol{a}_{c,u_{c}}$ , where $J_{\\le c}:=\\cup_{c^{\\prime}\\le c}J_{c^{\\prime}}$ . ", "page_idx": 13}, {"type": "text", "text": "Finally, we partition the time horizon $[T]$ into consecutive blocks $T_{1},\\cdot\\cdot\\cdot,T_{m}$ (whose sizes will be specified later), and choose the context sequence as $c_{t}=c$ for all $t\\in T_{c}$ . For a fixed policy, let $\\mathsf{R}_{T}$ be the worst-case expected regret of this policy. By the second property of the construction, it is clear that for all $u\\in\\Omega$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathsf{R}_{T}\\geq\\frac{1}{4}\\sum_{c=1}^{m}\\sum_{t\\in T_{c}}\\mathbb{E}_{(P^{u})^{\\otimes(t-1)}}\\big[\\mathbb{1}\\big(a_{t}\\not\\in J_{c}\\big)\\big].\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "When $u$ is uniformly distributed over $\\Omega$ , we also have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{R}_{T}\\stackrel{(a)}{\\geq}\\mathbb{E}_{u}\\Bigg[\\displaystyle\\Delta\\sum_{c=1}^{m}\\mathsf{E}_{(P^{*})^{\\top}}\\mathbb{E}_{(P^{*})^{\\top}(P^{*})^{\\top}}[\\mathsf{I}(a_{t}\\ne a_{c,u_{c}})]\\Bigg]}\\\\ &{\\quad\\stackrel{(b)}{\\leq}\\displaystyle\\Delta\\sum_{c=1}^{m}\\sum_{t\\in T_{c}}\\mathbb{E}_{u\\setminus\\xi}\\Big[\\mathbb{E}_{u}\\Big[\\mathbb{E}_{(P^{*})^{\\top}}\\{\\mathsf{I}(a_{t}\\ne a_{c,u_{c}})\\}]\\Big]}\\\\ &{\\quad\\stackrel{(c)}{\\geq}\\displaystyle\\Delta\\sum_{c=1}^{m}\\sum_{t\\in T_{c}}\\mathbb{E}_{u\\setminus\\xi}\\Bigg[\\frac{1}{2K_{c}}\\sum_{u_{c}=2}^{K}\\exp\\Big(\\!-\\!\\mathrm{KL}\\Big(\\big(P^{u^{*}})^{\\top}(1\\!\\!-\\!1)\\big)\\Big|\\big(P^{u})^{\\odot(t-1)}\\Big)\\Big)\\Bigg]}\\\\ &{\\quad\\stackrel{(d)}{\\geq}\\displaystyle\\Delta\\sum_{c=1}^{m}\\sum_{t\\in T_{c}}\\mathbb{E}_{u\\setminus\\xi}\\Bigg[\\mathbb{E}_{c}-1\\exp\\Bigg(\\!-\\!\\frac{1}{K_{c}}\\sum_{u_{c}=2}^{K}\\mathrm{KL}\\Big(\\big(P^{u^{*}})^{\\odot(t-1)}\\big)\\Big|\\big(P^{u})^{\\odot(t-1)}\\Big)\\Bigg)\\Bigg]}\\\\ &{\\quad\\stackrel{(c)}{\\geq}\\displaystyle\\frac{\\Delta}{4}\\sum_{c=1}^{m}\\sum_{t\\in T_{c}}\\mathbb{E}_{u\\setminus\\xi}\\Bigg[\\exp\\Bigg(\\!-\\!\\frac{64\\Delta^{2}}{3(K_{c}-1)}\\sum_{u_{c}=2}^{K}\\mathbb{E}_{\\xi}\\Bigg[P^{u\\setminus\\xi})^{\\top}\\Bigg]\\Bigg]\\Bigg(a_{c}\\ne b_{c}^{2}\\Bigg)\\xi_{2\\in\\mathcal{V}}\\{a_{c,u_{c}}\\}\\Bigg)\\Bigg]\\Bigg),}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where (a) lower bounds the minimax regret by the Bayes regret, with the help of the first property; (b) decomposes the expectation over uniformly distributed $u$ into $u\\backslash\\{u_{c}\\}$ and $u_{c}\\in[K_{c}]$ ; (c) follows from Lemma A.3; (d) uses the convexity of $x\\mapsto e^{-x}$ ; (e) results from the chain rule of KL divergence, the third property of the construction, and that $K_{c}\\geq2$ for all $c\\in[m]$ . ", "page_idx": 13}, {"type": "text", "text": "Next we upper bound the exponent in (7) as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{t=2}^{K_{c}}\\displaystyle\\sum_{s<t}\\mathbb{E}_{(P^{u})^{s}(s^{\\prime}-1)}[1(a_{s}\\notin J_{\\le c}\\backslash\\{a_{c,u_{c}}\\})]}\\\\ {\\displaystyle}&{\\le\\displaystyle\\sum_{u_{c}=2}^{K_{c}}\\displaystyle\\sum_{c<s\\in\\mathbb{C}_{r^{\\prime}}}\\sum_{\\mathbb{E}_{\\tau}}\\mathbb{E}_{(P^{u})^{s}(s^{\\prime}-1)}[1(a_{s}\\notin J_{c})]+\\displaystyle\\sum_{u_{c}=2}^{K_{c}}\\sum_{s<t}\\mathbb{E}_{(P^{u})^{s}(s^{\\prime}-1)}[1(a_{s}\\notin J_{\\le c})+1(a_{c}=a_{c,u_{c}})]}\\\\ {\\displaystyle}&{\\le\\displaystyle\\sum_{u_{c}=2}^{K_{c}}\\sum_{c^{\\prime}\\le c\\le s\\in T_{c^{\\prime}}}\\mathbb{E}_{(P^{u})^{s}(s^{\\prime}-1)}[1(a_{s}\\notin J_{c^{\\prime}})]+\\displaystyle\\sum_{u_{c}=2}^{K_{c}}\\sum_{s\\in T_{c}}\\mathbb{E}_{(P^{u})^{s}(s^{\\prime}-1)}[1(a_{s}=a_{c,u_{c}})]}\\\\ {\\displaystyle}&{\\overset{(b)}{\\le}4(K_{c}-1)\\mathsf{R}_{T}+\\displaystyle\\sum_{u_{c}=2}^{K_{c}}\\sum_{s\\in T_{c}}\\mathbb{E}_{(P^{u})^{s}(s^{\\prime}-1)}[1(a_{s}=a_{c,u_{c}})].}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Plugging it back into (7), we get ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\mathfrak{L}_{T}\\geq\\frac{\\Delta}{4}\\sum_{c=1}^{m}\\sum_{t\\in T_{c}}\\mathbb{E}_{u\\setminus\\{u_{c}\\}}\\left[\\exp\\left(-\\frac{64\\Delta^{2}}{3(K_{c}-1)}\\left(4(K_{c}-1)\\mathsf{R}_{T}+\\sum_{u_{c}=2}^{K_{c}}\\sum_{s\\in T_{c}}\\mathbb{E}_{(P^{u^{c}})^{\\otimes(s-1)}}\\big[\\mathbb{I}\\left(a_{s}=a_{c,u_{c}}\\right)\\mathbb{I}\\left(\\mathbb{R}_{u}\\right)\\mathbb{E}_{u_{c}\\times\\mathbb{T}_{c}}\\right)\\right)\\right]}}\\\\ {{\\displaystyle\\quad\\overset{(f)}{\\geq}\\frac{\\Delta}{4}\\sum_{c=1}^{M}\\mathbb{E}_{u\\setminus\\{u_{c}\\}}\\left[\\exp\\left(-\\frac{64\\Delta^{2}}{3}\\left(4\\mathsf{R}_{T}+\\frac{\\vert T_{c}\\vert}{K_{c}-1}\\right)\\right)\\right],}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where (f) crucially uses that $u^{c}=(u_{1},\\cdot\\cdot\\cdot,u_{c-1},1,u_{c+1},\\cdot\\cdot\\cdot,u_{m})$ does not depend on $u_{c}$ , so that the sum may be moved inside the expectation to get $\\begin{array}{r}{\\sum_{u_{c}=2}^{K_{c}}\\mathbb{1}(a_{s}=a_{c,u_{c}})\\le1}\\end{array}$ . Now choosing ", "page_idx": 14}, {"type": "equation", "text": "$$\n|T_{c}|=\\frac{K_{c}}{\\sum_{c^{\\prime}=1}^{m}K_{c^{\\prime}}}\\cdot T\\le\\frac{2K_{c}T}{\\beta_{M}(G)},\\qquad\\Delta=\\sqrt{\\frac{\\beta_{M}(G)}{16T}}\\in\\left(0,\\frac{1}{4}\\right),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "we arrive at the final lower bound ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\mathsf{R}}_{T}\\geq\\frac{\\sqrt{\\beta_{M}(G)T}}{16}\\exp\\!\\bigg(\\!-\\!\\frac{4\\beta_{M}(G)}{3T}\\!\\bigg(4{\\mathsf{R}}_{T}+\\frac{4T}{\\beta_{M}(G)}\\bigg)\\bigg)}\\\\ &{\\quad\\ge\\frac{\\sqrt{\\beta_{M}(G)T}}{16e^{6}}\\exp\\!\\bigg(\\!-\\!\\frac{16\\beta_{M}(G)}{3T}{\\mathsf{R}}_{T}\\bigg)\\ge\\frac{\\sqrt{\\beta_{M}(G)T}}{16e^{6}}\\exp\\!\\Bigg(\\!-\\!\\frac{16{\\mathsf{R}}_{T}}{3\\sqrt{\\beta_{M}(G)T}}\\Bigg),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the last inequality is due to the assumption $T\\geq\\beta_{M}(G)^{3}$ . Now we readily conclude from (8) the desired lower bound $\\mathsf{R}_{T}=\\Omega(\\sqrt{\\beta_{M}(G)T})$ . ", "page_idx": 14}, {"type": "text", "text": "C Deferred Proofs for the Upper Bounds ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Throughout the proofs, we will use $\\alpha(A)\\triangleq\\alpha(G|_{A})$ (resp. $\\delta(A),\\mathfrak{m}(A))$ to denote the independence number (resp. dominating number, MAS number) of the subgraph induced by $A\\subseteq V$ when the graph $G$ is clear from the context. ", "page_idx": 14}, {"type": "text", "text": "C.1 Proof of Lemma 3.1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The lower bound $U_{1}^{\\star}(G,M)\\ge\\beta_{M}(G)$ is easy: let $I_{1},\\cdot\\cdot\\cdot,I_{M}$ be $M$ independent sets with $I_{i}\\ne I_{j}$ for all $i<j$ . Then the choice $A_{c}=I_{c}$ is always feasible for the adversary, for $I_{c}$ is disjoint from $N_{\\mathrm{out}}(\\cup_{c^{\\prime}<c}I_{c^{\\prime}})$ . For the learner, the only subset $D_{c}\\subseteq I_{c}$ which dominates $I_{c}$ is $D_{c}=I_{c}$ , hence $\\begin{array}{r}{U_{1}^{\\star}(G,M)\\ge\\sum_{c=1}^{M}|I_{c}|}\\end{array}$ . Taking the maximum then gives $U_{1}^{\\star}(G,M)\\ge\\beta_{M}(G)$ by (2). ", "page_idx": 14}, {"type": "text", "text": "To prove the upper bound $U_{1}^{\\star}(G,M)\\leq C\\beta_{M}(G)\\log|V|$ , the learner chooses $D_{c}$ as follows. Given $A_{c}$ , the learner finds the smallest dominating set $J_{c}\\subseteq A_{c}$ and the largest independent set $I_{c}\\subseteq A_{c}$ , and sets $D_{c}=I_{c}\\cup J_{c}$ . Clearly this choice of $D_{c}$ is feasible for the learner, and since $I_{i}\\neq A_{j}$ for $i<j$ , we have $I_{i}\\neq I_{j}$ as well. Consequently, ", "page_idx": 14}, {"type": "equation", "text": "$$\nU^{\\star}(G,M)\\leq\\sum_{c=1}^{M}|D_{c}|\\leq\\sum_{c=1}^{M}(|J_{c}|+|I_{c}|)\\overset{(a)}{=}\\sum_{c=1}^{M}O(|I_{c}|\\log|V|)\\overset{(b)}{=}O(\\beta_{M}(G)\\log|V|),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where (a) uses $|J_{c}|=\\delta(A_{c})=O(\\alpha(A_{c})\\log|V|)=O(|I_{c}|\\log|V|)$ in Lemma A.1, and (b) follows from the definition of $\\beta_{M}(G)$ in (2). ", "page_idx": 14}, {"type": "text", "text": "Since it is NP-hard to find either the smallest dominating set or the largest independent set (Karp, 2010; Grandoni, 2006), the above choice of $D_{c}$ is not computationally efficient. To arrive at a polynomial-time algorithm, we may use a greedy algorithm to find an $O(\\log|V|)$ -approximate smallest dominating set ${\\widetilde{J}}_{c}$ such that $|\\widetilde{J}_{c}|=O(\\delta(A_{c})\\log|V|)$ (cf. Lemma A.2). For $I_{c}$ , although finding the largest independent set is APX-hard (Feige et al., 1991), the constructive proof of (Alon et al., 2015, Lemma 8) gives a polynomial-time randomized algorithm which finds $\\widetilde{I_{c}}\\subseteq A_{c}$ such that $|\\widetilde{I}_{c}|=\\Omega(\\delta(A_{c})/\\log|V|)$ and the average degree among $\\widetilde{I_{c}}$ is at most $O(1)$ . The learner now chooses $D_{c}=\\widetilde{I}_{c}\\cup\\widetilde{J}_{c}$ . By the average degree constraint and Tur\u00e1n\u2019s theorem (Alon and Spencer, 2016, Theorem 3.2.1), each $\\widetilde{I}_{c}$ contains an independent subset $I_{c}$ with $|I_{c}|=\\Omega(|\\widetilde{I}_{c}|)$ . Since ", "page_idx": 14}, {"type": "equation", "text": "$$\n|\\widetilde{J}_{c}|=O(\\delta(A_{c})\\log|V|)=O(|\\widetilde{I}_{c}|\\log^{2}|V|)=O(|I_{c}|\\log^{2}|V|),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "we conclude that $\\begin{array}{r}{\\sum_{c=1}^{M}|D_{c}|=O(\\sum_{c=1}^{M}|I_{c}|\\log^{2}|V|)=O(\\beta_{M}(G)\\log^{2}|V|).}\\end{array}$ ", "page_idx": 14}, {"type": "text", "text": "C.2 Proof of Lemma 3.2 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The second inequality is straightforward: $\\beta_{\\mathsf{d o m}}(G,M)\\,\\leq\\,{\\mathsf{m}}(G)$ since $\\cup_{c}B_{c}$ is acyclic, and the other inequality follows from $|B_{c}|\\,=\\,O(\\delta(V_{c})\\log|V|)\\,=\\,O(\\alpha(V_{c})\\log^{2}|V|)$ in Lemma A.1. To prove the first inequality, we consider a simple greedy algorithm for the learner, where $v_{t}\\in A_{c_{t}}$ is the vertex with the largest out-degree in the induced subgraph by $A_{c_{t}}\\backslash N_{\\mathrm{out}}(D_{t-1})$ . Intuitively, $A_{c_{t}}\\backslash N_{\\mathrm{out}}(D_{t-1})$ is the set of nodes in $A_{c_{t}}$ that remain unexplored by the learner by time $t$ . Under this greedy algorithm, for $c\\in[M]$ , define ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "equation", "text": "$$\nV_{c}=\\bigcup_{t:c_{t}=c}\\left(N_{\\mathrm{out}}(v_{t})\\bigcap(A_{c_{t}}\\backslash N_{\\mathrm{out}}(D_{t-1}))\\right),\\qquad B_{c}=\\{v_{t}:c_{t}=c\\}\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We claim that $V_{c}$ are pairwise disjoint and $|B_{c}|\\leq\\delta(V_{c})(1+\\log|V|)$ , and thereby complete the proof of $\\begin{array}{r}{\\sum_{c=1}^{M}|B_{c}|\\le\\beta_{\\mathsf{d o m}}(G,M)}\\end{array}$ . The first claim simply follows from the pairwise disjointness of the sets $\\bar{N}_{\\mathrm{out}}(v_{t})\\bigcap(A_{c_{t}}\\backslash N_{\\mathrm{out}}(D_{t-1}))$ for different $t$ . For the second claim, let $t_{1}<\\cdot\\cdot-t_{n}$ be all the time steps where $c_{t}=c$ , and ", "page_idx": 15}, {"type": "equation", "text": "$$\nV_{c,i}\\triangleq\\bigcup_{j=i}^{n}\\left(N_{\\mathrm{out}}(v_{t_{j}})\\bigcap(A_{c}\\backslash N_{\\mathrm{out}}(D_{t_{j}-1}))\\right),\\quad i\\in[n].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "It is clear that $V_{c,i+1}=V_{c,i}\\backslash N_{\\mathrm{out}}(v_{t_{i}})$ . Since $v_{t_{i}}$ has the largest out-degree in the induced subgraph by $A_{c}\\backslash N_{\\mathrm{out}}(D_{t_{i}-1})\\supseteq V_{c,i}$ , and $N_{\\mathrm{out}}(v_{t_{i}})\\cap(A_{c}\\backslash N_{\\mathrm{out}}(D_{t_{i}-1}))=N_{\\mathrm{out}}(v_{t_{i}})\\cap V_{c,i}$ , this is also the vertex with the largest out-degree in $V_{c,i}$ . Therefore, the sets $\\{V_{c,i}\\}_{i=1}^{n+1}$ evolve from $V_{c,1}=V_{c}$ to $V_{c,n+1}=\\emptyset$ as follows: one recursively picks the vertex with the largest out-degree in $V_{c,i}$ , and removes its out-neighbors to get $V_{c,i+1}$ . This is a well-known approximate algorithm for computing $\\delta(V_{c})$ , described above Lemma A.2, with ", "page_idx": 15}, {"type": "equation", "text": "$$\n|B_{c}|=n\\leq\\delta(V_{c})(1+\\log|V_{c}|)\\leq\\delta(V_{c})(1+\\log|V|),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "as desired. ", "page_idx": 15}, {"type": "text", "text": "C.3 Proof of Lemma 3.5 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "If $G$ is undirected, then $\\beta_{\\mathsf{d o m}}(G)\\leq\\mathsf{m}(G)=\\alpha(G)=\\beta_{M}(G)$ easily holds. It remains to consider the case where $G$ is transitively closed. ", "page_idx": 15}, {"type": "text", "text": "Note that in a transitively closed graph, every vertex set has an independent dominating subset (by tracing to the ancestors). Therefore, for the maximizing sets $B_{1},\\cdot\\cdot\\cdot,B_{M}$ in the definition of $\\beta_{{\\mathsf{d o m}}}$ , we may run the above procedure to find independent dominating subsets $I_{1},\\cdot\\cdot\\cdot,I_{M}$ of $V_{1},\\ldots,V_{M}$ respectively, with $I_{c}\\subseteq B_{c}$ and ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{c=1}^{M}|I_{c}|\\geq\\sum_{c=1}^{M}\\delta(V_{c})\\geq\\frac{1}{C^{\\prime}\\log|V|}\\sum_{c=1}^{M}|B_{c}|.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Now consider the induced subgraph $G^{\\prime}$ by $\\cup_{c}I_{c}$ . Clearly $G^{\\prime}$ is acyclic, and the length of longest path in $G^{\\prime}$ is at most $M$ (otherwise, two points on a path belong to the same $I_{c}$ , and transitivity will violate the independence). Invoking Lemma 4.1 now gives ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{c=1}^{M}|I_{c}|={\\mathfrak{m}}(G^{\\prime})\\leq{\\frac{\\rho(G^{\\prime})}{M}}\\beta_{M}(G^{\\prime})\\leq\\beta_{M}(G^{\\prime})\\leq\\beta_{M}(G),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and combining the above two inequalities completes the proof. ", "page_idx": 15}, {"type": "text", "text": "C.4 Proof of Theorem 3.3 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The proof is decomposed into three claims: with probability at least $1-\\delta$ , ", "page_idx": 15}, {"type": "text", "text": "1. for every $c\\in[M]$ and $\\ell\\in\\mathbb{N}$ , when the confidence bound (5) is formed, either every action in $A_{c,\\ell}$ has been observed for at least $\\ell$ times or $|A_{c,\\ell}|=1$ ;   \n2. for every $c\\,\\in\\,[M]$ and $\\ell\\,\\in\\,\\mathbb N$ , the best action $a^{\\star}(c)$ under context $c$ belongs to $A_{c,\\ell}$ , and every action in $A_{c,\\ell}$ has suboptimality gap at most $\\operatorname*{min}\\{1,4\\Delta_{\\ell-1}\\}$ , with $\\Delta_{\\ell}$ \u225c $\\sqrt{\\log(2M K T/\\delta)/\\ell}$ ;   \n3. for every $\\ell\\in\\mathbb{N}$ , the total number $N_{\\ell}$ of actions taken on layer $\\ell$ in those subsets $A_{c,\\ell}$ with $|A_{c,\\ell}|>1$ is $O(\\beta_{M}(G)\\log^{2}K)$ . ", "page_idx": 15}, {"type": "text", "text": "The first claim simply follows from (1) when $G$ is strongly observable, any subset of size $>1$ is strongly observable, and (2) $A_{c,\\ell}\\subseteq N_{\\mathrm{out}}\\big(\\cup_{c^{\\prime}\\leq c}D_{c^{\\prime},\\ell}\\big)$ required by the sequential game I, so that on each layer $\\ell^{\\prime}\\in[\\ell]$ there is at least one action which observes $a\\in A_{c,\\ell}\\subseteq A_{c,\\ell^{\\prime}}$ . For the second claim, the first claim, the usual Hoeffding concentration, and a union bound imply that $|\\bar{r}_{c,a}-\\mu_{c,a}|\\le\\Delta_{\\ell}$ in (5) for all $c\\in[M]$ and $a\\in A_{c,\\ell}$ , when $|{A_{c,\\ell}}|>1$ and with probability at least $1-\\delta$ . Conditioned on this event: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The best arm $a^{\\star}(c)$ is not eliminated by (5), because $\\begin{array}{r l r}{\\bar{r}_{c,a^{\\star}(c)}}&{\\geq}&{\\mu_{c,a^{\\star}(c)}\\;-\\;\\Delta\\ell\\;\\;\\geq}\\end{array}$ $\\begin{array}{r}{\\operatorname*{max}_{a^{\\prime}\\in A_{c,\\ell}}\\mu_{c,a^{\\prime}}-\\Delta_{\\ell}\\geq\\operatorname*{max}_{a^{\\prime}\\in A_{c,\\ell}}\\bar{r}_{c,a^{\\prime}}-2\\Delta_{\\ell}}\\end{array}$ ;   \n\u2022 The instantaneous regret of choosing any action $a\\in A_{c,\\ell+1}$ is at most $\\operatorname*{min}\\{1,4\\Delta_{\\ell}\\}$ , for $\\mu_{c,a}\\geq\\bar{r}_{c,a}-\\Delta_{\\ell}\\geq\\bar{r}_{c,a^{\\star}(c)}-3\\Delta_{\\ell}\\geq\\mu_{c,a^{\\star}(c)}-4\\Delta_{\\ell}$ , and $|\\mu_{c,a}-\\mu_{c,a^{\\star}(c)}|\\leq1$ trivially holds. ", "page_idx": 16}, {"type": "text", "text": "Consequently the second claim holds for $|{A_{c,\\ell}}|>1$ . For the case $|A_{c,\\ell}|=1$ , note that starting from $A_{c,1}=[K]$ , the above argument implies that the best arm is never eliminated until $|A_{c,\\ell^{\\prime}}|=1$ at some layer $\\overline{{\\ell^{\\prime}\\leq\\ell}}$ conditioned on the high probability event, which implies the single action in $A_{c,\\ell}$ is the best action and incurs 0 regret. The last claim is simply the reduction to the sequential game I, where Lemma 3.1 shows that $\\begin{array}{r}{\\bar{N_{\\ell}}=\\sum_{c=1}^{M}|D_{c,\\ell}|=O(\\beta_{M}(\\bar{G})\\log^{2}K)}\\end{array}$ . ", "page_idx": 16}, {"type": "text", "text": "Combining the above claims and that we incur 0 regret whenever $|A_{c,\\ell}|=1$ , with probability at least $1-\\delta$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathsf{R}_{T}(\\mathsf{A l g\\ 1};G,M,\\mathcal{C}_{\\mathsf{S A}})\\leq\\sum_{\\ell=1}^{\\infty}N_{\\ell}\\operatorname*{min}\\{1,4\\Delta_{\\ell}\\},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $N_{\\ell}\\,\\leq\\,N\\,:=\\,O(\\beta_{M}(G)\\log^{2}K)$ , and $\\begin{array}{r}{\\sum_{\\ell=1}^{\\infty}N_{\\ell}\\,=\\,T}\\end{array}$ . It is straightforward to see that the choice $N_{1}=\\cdots=N_{m}=N$ and $N_{m+1}=T\\overset{\\cdot}{-}\\bar{N}m$ for a suitable $m\\in\\mathbb{N}$ maximizes the above sum, and the maximum value is the target regret upper bound in Theorem 3.3. ", "page_idx": 16}, {"type": "text", "text": "C.5 Proof of Theorem 3.4 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The proof follows verbatim the same lines in the proof of Theorem 3.3, except that the total number $N_{\\ell}$ of actions taken on layer $\\ell$ is now at most $\\beta_{\\mathsf{d o m}}(G,M)$ in the third claim, by Lemma 3.2. ", "page_idx": 16}, {"type": "text", "text": "C.6 Proof of Lemma 4.1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Let $V_{1}\\subseteq V$ be a maximum acyclic subset, then $\\rho(G|_{V_{1}})\\le\\rho(G)$ . Consider the following recursive process: at time $t=1,2,\\cdot\\cdot\\cdot$ , let $J_{t}$ be the set of vertices in $V_{t}$ with in-degree zero (which always exist as $V_{t}$ is acyclic), and $V_{t+1}=V_{t}\\backslash J_{t}$ . This recursion can only last for at most $\\rho(G)$ steps, for otherwise there is a path of length larger than $\\rho(G)$ . Then each $J_{t}$ is an independent set, for every vertex of $J_{t}$ has in-degree zero in $V_{t}\\supseteq J_{t}$ . For the same reason we also have $J_{i}\\neq J_{j}$ for $i>j$ . This means that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathsf{m}(G)=|V_{1}|=\\sum_{t}|J_{t}|\\leq\\operatorname*{max}\\left\\{\\frac{\\rho(G)}{M},1\\right\\}\\beta_{M}(G),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the last inequality follows from picking $M$ largest sets among $\\{J_{t}\\}$ . ", "page_idx": 16}, {"type": "text", "text": "C.7 Proof of the statement in Section 4.2.2 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we show that when $G_{[K]}$ and $G_{[M]}$ are either both undirected or both transitively closed, the product graph quantities $\\dot{\\beta_{\\mathsf{d o m}}}:=\\dot{\\beta_{\\mathsf{d o m}}}(G_{[K]}\\times G_{[M]})$ and $\\beta_{M}:=\\beta_{M}\\bigl(G_{[K]}\\times G_{[M]}\\bigr)$ satisfy ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\beta_{\\mathsf{d o m}}=O(\\beta_{M}\\log K).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Combining with the $\\Omega(\\sqrt{\\beta_{M}T})$ lower bound, this shows the tightness of the upper bound $\\widetilde{O}\\big(\\sqrt{\\beta_{\\mathsf{d o m}}T}\\big)$ . The idea here is similar to Section C.3: ", "page_idx": 16}, {"type": "text", "text": "When $G_{[K]}$ and $G_{[M]}$ are both undirected, the union set $\\cup_{c}B_{c}$ in the definition of $\\beta_{{\\mathsf{d o m}}}$ is an independent set thanks to the acyclic requirement. Thus $\\beta_{{\\sf d o m}}\\leq\\beta_{M}$ . ", "page_idx": 16}, {"type": "text", "text": "When $G_{[K]}$ and $G_{[M]}$ are both transitively closed, for the maximizing sets $B_{1},\\ldots,B_{M}$ in the definition of $\\beta_{{\\mathsf{d o m}}}$ , we can again find independent dominating subsets $I_{c}\\subseteq B_{c}$ (by transitive closure of $G_{[K]})$ with ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{c=1}^{M}|I_{c}|\\geq\\frac{1}{C^{\\prime}\\log K}\\sum_{c=1}^{M}|B_{c}|=\\frac{1}{C^{\\prime}\\log K}\\beta_{\\mathsf{d o m}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Now it suffices to find independent subsets $J_{c}\\,\\subseteq\\,[K]\\,\\times\\,\\{c\\}$ that satisfy $J_{c}\\ne J_{c^{\\prime}}$ when $c<c^{\\prime}$ and $\\begin{array}{r}{\\sum_{c}|J_{c}|=\\sum_{c}|I_{c}|}\\end{array}$ . Toward this end, we first suppose there are $c$ and $c^{\\prime}$ such that $u_{c}\\rightarrow u_{c^{\\prime}}$ and $v_{c}\\leftarrow v_{c^{\\prime}}$ f or $\\bar{u_{c}},v_{c}\\in I_{c}$ and $u_{c^{\\prime}},v_{c^{\\prime}}\\in I_{c^{\\prime}}$ . This implies $c\\leftrightarrow c^{\\prime}$ in $G_{[M]}$ by the product graph structure. Then ", "page_idx": 17}, {"type": "text", "text": "\u2022 $I_{c}|_{[K]}$ and ${\\cal I}_{c^{\\prime}}|_{[K]}$ are disjoint since $\\bigcup_{c}I_{c}$ is acyclic;   \n\u2022 by transitive closure of $G_{[K]}$ and that $I_{c}$ , $I_{c^{\\prime}}$ are independent, $I_{c}|_{[K]}\\cup I_{c^{\\prime}}|_{[K]}$ has path length at most 1;   \n\u2022 from above, there exist disjoint and independent sets $S_{1}$ and $S_{2}$ such that $S_{1}\\cup S_{2}=$ $I_{c}|_{[K]}\\cup I_{c^{\\prime}}|_{[K]}$ and $S_{1}\\neq S_{2}$ . ", "page_idx": 17}, {"type": "text", "text": "where we denote the set projection ", "page_idx": 17}, {"type": "equation", "text": "$$\nS|_{[K]}=\\{a\\in[K]:(a,c)\\in S{\\mathrm{~for~some~}}c\\in[M]\\}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Without loss of generality, assume $c<c^{\\prime}$ . In this case, we can \u201crearrange\u201d them by letting ${\\cal J}_{c}={\\cal\\Omega}$ $S_{1}\\times\\{c\\}$ and $J_{c^{\\prime}}=S_{2}\\times\\{c^{\\prime}\\}$ , so $J_{c}\\neq J_{c^{\\prime}}$ . Now suppose there is a loop on the set level, i.e. there are $c_{1},\\dots,c_{m}\\,\\in\\,[M]$ with $I_{c_{1}}\\,\\rightarrow\\,\\cdot\\,\\cdot\\,\\rightarrow I_{c_{m}}\\,\\rightarrow\\,I_{c_{1}}$ . Similarly, we must have that $\\{c_{1},\\ldots,c_{m}\\}$ form a clique in $G_{[M]}$ , $I_{c_{1}}|_{[K]},\\ldots,I_{c_{m}}|_{[K]}$ disjoint in $G_{[K]}$ , and the path length in $\\textstyle{\\bigcup_{j=1}^{m}I_{c_{j}}}$ is at most $m$ . Then we can again \u201crearrange\u201d them and get independent sets $J_{c_{j}}\\ \\neq\\ J_{c_{k}}$ for $c_{j}~<~c_{k}$ and $j,k\\in[m]$ , and $\\begin{array}{r}{\\sum_{j=1}^{m}|J_{c_{j}}|=\\sum_{j=1}^{m}|I_{c_{j}}|}\\end{array}$ . In other cases, we simply let $J_{c}=I_{c}$ and arrive at $J_{1},\\ldots,J_{M}$ that are acyclic on the set level, i.e. up to reordering of the indices, we have $J_{c}\\neq J_{c^{\\prime}}$ when $c<c^{\\prime}$ . Together with Eq (9), we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\beta_{\\mathsf{d o m}}\\le C^{\\prime}\\log K\\sum_{c=1}^{M}|I_{c}|=C^{\\prime}\\log K\\sum_{c=1}^{M}|J_{c}|\\le C^{\\prime}\\beta_{M}\\log K.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: The main claims in the abstract and introduction include a lower bound and two upper bounds (and algorithms achieving them) for the considered setup, which are the contributions of this work. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 18}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: The gap between the lower bound and the upper bound under the most general assumptions is discussed with an example in Section 4.3. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 18}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The complete proof for the lower bound is in Appendix B and those for the upper bounds are in Appendix C. Auxilary lemmas are collected and referenced in Appendix A. The problem setup is in Section 1.2. Assumptions are stated in the theorem statements. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 19}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: This work does not include experiments. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 19}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: This work does not include experiments. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 20}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: This work does not include experiments. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 20}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: This work does not include experiments. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: This work does not include experiments. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 21}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The authors have reviewed the NeurIPS code of ethics. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 21}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: This work aims to understand the theoretical limits of a family of contextual bandit problems and has no direct societal impact. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 21}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 22}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: This work is theoretical and has no such risks. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 22}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: This work does not use existing assets. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: This work does not release new assets. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 23}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: This work does not involve or include such experiments. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 23}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: This work does not involve or include such experiments. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 23}]