[{"type": "text", "text": "Poisson-Gamma Dynamical Systems with Non-Stationary Transition Dynamics ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Bayesian methodologies for handling count-valued time series have gained promi  \n2 nence due to their ability to infer interpretable latent structures and to estimate   \n3 uncertainties, and thus are especially suitable for dealing with noisy and incomplete   \n4 count data. Among these Bayesian models, Poisson-Gamma Dynamical Systems   \n5 (PGDSs) are proven to be effective in capturing the evolving dynamics underlying   \n6 observed count sequences. However, the state-of-the-art PGDS still falls short in   \n7 capturing the time-varying transition dynamics that are commonly observed in   \n8 real-world count time series. To mitigate this limitation, a non-stationary PGDS   \n9 is proposed to allow the underlying transition matrices to evolve over time, and   \n10 the evolving transition matrices are modeled by the specifically-designed Dirich  \n11 let Markov chains. Leveraging Dirichlet-Multinomial-Beta data augmentation   \n12 techniques, a fully-conjugate and efficient Gibbs sampler is developed to perform   \n13 posterior simulation. Experiments show that, in comparison with related models,   \n14 the proposed non-stationary PGDS achieves improved predictive performance   \n15 due to its capacity to learn non-stationary dependency structure captured by the   \n16 time-evolving transition matrices. ", "page_idx": 0}, {"type": "text", "text": "17 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "18 In recent years, there has been an increasing interest in modeling count time series. For instance,   \n19 some previous works [1, 2, 3] are concerned with how to learn the evolving topics behind text   \n20 corpus (frequencies of words) over time. Some works [4, 5, 6, 7] try to predict global immigrant   \n21 trends underlying international population movements. Count time series are often overdispersed,   \n22 sparse, high-dimensional, and thus can not be well modeled by widely used dynamic models such   \n23 as linear dynamical systems [8, 9]. Recently, many works [10, 11, 12, 13, 14, 15, 16] prefer to   \n24 choose distributions of the gamma-Poisson family to build their hierarchical Bayesian models. In   \n25 particular, these models enjoy strong explainability and can estimate uncertainty especially when the   \n26 observations are noisy and incomplete. Among these works, Poisson-Gamma Dynamical Systems   \n27 (PGDSs) [13] received a lot of attention because PGDS can learn how the latent dimensions excite   \n28 each other to capture complicated dynamics in observed count series. For instance, a very inspiring   \n29 research paper may motivate other researchers to publish papers on related topics [17]. The outbreak   \n30 of COVID-19 in one state, may lead to the rapid rising of COVID-19 cases in the nearby states and   \n31 vice versa [18]. In particular, PGDS can be efficiently learned with a tractable Gibbs sampling scheme   \n32 via Poisson-Logarithmic data augmentation and marginalization technique [11]. Due to its strong   \n33 flexibility, PGDS achieves better performance in predicting missing entities and future observations,   \n34 compared with related models [9, 15].   \n35 Despite these advantages, PGDS still can not capture the time-varying transition dynamics underlying   \n36 observed count sequences, which are commonly observed in real-world scenarios [19]. For instance,   \n37 during the initial stage of the COVID-19 pandemic, the worldwide counts of infectious patients were   \n38 significantly affected by various local policies, government interventions, and emergent events [20,   \n39 21, 22]. The cross transition dynamics among the different monitoring areas were also evolving as   \n40 the corresponding policies and interventions changed over time. Hence, PGDS unavoidably makes a   \n41 certain amount of approximation error in capturing the aforementioned non-stationary count time   \n42 series, using a time-invariant transition kernel.   \n43 To mitigate this limitation, Non-Stationary Poisson-Gamma Dynamical Systems (NS-PGDSs), a novel   \n44 kind of Poisson-gamma dynamical systems with non-stationary transition dynamics are developed.   \n45 More specifically, NS-PGDS captures the evolving transition dynamics by the specifically-designed   \n46 Dirichlet Markov chains. Via the Dirichlet-Multinomial-Beta data augmentation strategy, the Non  \n47 Stationary Poisson-Gamma Dynamical Systems can be inferred with a conjugate-yet-efficient Gibbs   \n48 sampler. Our contributions are summarized as follows:   \n49 \u2022 We propose a Non-Stationary Poisson-Gamma Dynamical System (NS-PGDS), a novel   \n50 Poisson-gamma dynamical system with time-evolving transition matrices that can well   \n51 capture non-stationary transition dynamics underlying observed count series.   \n52 \u2022 Three Dirichlet Markov chains are dedicated to improving the flexibility and expressiveness   \n53 of NS-PGDSs, for capturing the complex transition dynamics behind sequential count data.   \n54 \u2022 Fully-conjugate-yet-efficient Gibbs samplers are developed via Dirichlet-Multinomial-Beta   \n55 augmentation techniques to perform posterior simulation for the proposed Dirichlet Markov   \n56 chains.   \n57 \u2022 Extensive experiments are conducted on four real-world datasets, to evaluate the performance   \n58 of the proposed NS-PGDS in predicting missing and future unseen observations. We also   \n59 provide exploratory analysis to demonstrate the explainable latent structure inferred by the   \n60 proposed NS-PGDS. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "61 2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "62 Let $\\pmb{y}^{(t)}=\\left[\\pmb{y}_{1}^{(t)},\\cdot\\cdot\\cdot\\mathrm{~,~}\\pmb{y}_{V}^{(t)}\\right]^{\\mathrm{T}}\\in\\mathbb{N}^{V}$ be a vector of nonnegative count valued observations at time $t$ .   \n63 To capture the latent dynamics underlying count sequences, some previous works [23, 24] model the   \n64 observations as ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\pmb{y}^{(t)}=p\\left(\\pmb{z}^{(t)}\\right),\\;\\;\\pmb{z}^{(t)}=f^{-1}\\left(\\pmb{x}^{(t)}\\right),\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "65 where $p\\left(\\cdot\\right)$ is the observation likelihood function, and $f\\left(\\cdot\\right)$ is an invertible link function that maps   \n66 the parameters of observation component to continuous-valued latent variables $\\pmb{x}^{(t)}\\,\\in\\,\\mathbb{R}^{K}$ . The   \n67 latent factor $\\pmb{x}^{(t)}$ evolves over time according to a linear dynamical system (LDS) given by $\\pmb{x}^{(t)}\\sim$   \n68 $\\mathcal{N}(A\\pmb{x}^{(t-1)},\\pmb{\\Lambda}^{-1})$ , where $\\pmb{A}$ is the state transition matrix of size $K\\times K$ , and $\\mathbf{A}=\\mathrm{diag}\\left(\\lambda_{1},\\cdots,\\lambda_{K}\\right)$   \n69 is the inverse covariance matrix with $\\lambda_{k}^{-1}$ determining the variance of $k$ -th latent dimension. Han   \n70 et al. [23] adopted the Extended Rank likelihood function to model count observations using LDS   \n71 with time complexity $\\mathcal{O}((K+V)^{3})$ , which prevents it from practical applications for analyzing   \n72 large-scale count data.   \n73 Recently, Acharya et al. [15] and Schein et al. [13, 16] developed Poisson-gamma family models for   \n74 sequential count observations. Gamma Process Dynamic Poisson Factor Analysis (GP-DPFA) [15]   \n75 models count data as $\\begin{array}{r}{y_{v}^{(t)}\\sim\\operatorname{Pois}(\\sum_{k=1}^{K}\\lambda_{k}\\phi_{v k}\\theta_{k}^{(t)})}\\end{array}$ , where $\\theta_{k}^{(t)}$ represents the strength of $k$ -th latent   \n76 factor at time , and $\\phi_{v k}$ captures the involvement degree of $k$ -th factor to -th observed dimension.   \n77 To ensure the model identifiability, we can impose a restriction as $\\begin{array}{r}{\\sum_{v}\\phi_{v k}=1}\\end{array}$ , and thus place a   \n78 Dirichlet prior over $\\phi_{k}=\\left[\\phi_{1k},\\cdot\\cdot\\cdot\\,,\\phi_{V k}\\right]^{T}$ as $\\phi_{k}\\sim\\operatorname{Dir}\\left(\\epsilon_{0},\\cdot\\cdot\\cdot\\mathbf{\\epsilon},\\epsilon_{0}\\right)$ .   \n79 To capture the underlying dynamics, the latent factor $\\theta_{k}^{(t)}$ evolves over time according to a gamma   \n80 Markov chain as \u03b8(kt) $\\theta_{k}^{(t)}\\sim\\mathrm{Gam}(\\theta_{k}^{(t-1)},c_{t})$ , where $c_{t}$ is the rate parameter of the gamma distribution to   \n81 control the variance of the gamma Markov chains. Although GP-DPFA can well fti one-dimensional   \n82 count sequences, it fails to learn how the latent dimensions interact with each other.   \n83 To address this concern, Schein et al. [13] developed Poisson-gamma dynamical systems to   \n84 capture the underlying transition dynamics. In particular, $\\theta_{k}^{(t)}$ evolves over time as $\\theta_{k}^{(t)}~\\sim$ ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "image", "img_path": "4kCr61XYQJ/tmp/88a98c4473e1de08f7c132a49816582512c30a70d22b8c376448be3c233e1418.jpg", "img_caption": [], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 1: The graphical representation of the NS-PGDS. The time interval is divided into equallyspaced sub-intervals. Each sub-interval contains $M$ time steps. The transition dynamics is stationary within a sub-interval. In particular, the transition matrices evolve over sub-intervals via Dirichlet Markov processes while latent factors evolve over time steps via Eq.(1). ", "page_idx": 2}, {"type": "text", "text": "85 Gam(\u03c40 kK2=1 \u03c0kk2\u03b8(kt2\u2212 , where $\\pi_{k k_{2}}$ represents how $k_{2}$ -th latent factor excites the $k$ -th latent   \n86 factor at next time step, and $\\begin{array}{r}{\\sum_{k=1}^{K}\\pi_{k k_{2}}=1}\\end{array}$ . ", "page_idx": 2}, {"type": "image", "img_path": "4kCr61XYQJ/tmp/776e8e7b620184748620d4a40e70e2b052f1e7e84e3ac71057d360b3e9e93b87.jpg", "img_caption": ["87 3 Non-Stationary Poisson-Gamma Dynamical Systems "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Real-world count time sequences are often nonstationary because the external interventional environments are always changing over time. The stationary PGDS with a time-invariant transition kernel fails to capture such time-varying transition dynamics. For instance, the transition dynamics behind COVID-19 infectious processes are time-varying, and highly affected by various interventional policies. Hence, to mitigate this limitation, we model the count sequences as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{y_{v}^{(t)}\\sim\\operatorname{Pois}\\left(\\delta^{(t)}\\sum_{k=1}^{K}\\phi_{v k}\\theta_{k}^{(t)}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "in which, the latent factors are specified by ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\theta_{k}^{(t)}\\sim\\mathrm{Gam}\\left(\\tau_{0}\\sum_{k_{2}=1}^{K}\\pi_{k k_{2}}^{(t-1)}\\theta_{k_{2}}^{(t-1)},\\tau_{0}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Figure 2: An example illustrates the Poisson-gamma 99 dynamical systems with non-stationary transition kernels. The three gamma dynamic processes independently evolve over time during the $(i-1)$ -th interval. During $i$ -th interval, $\\theta_{1}^{(t)}$ and $\\theta_{2}^{(t)}$ gradually 100 starts to interact with each other while $\\theta_{3}^{(t)}$ r emains 101 independent to the other two dimensions. During 102 $(i+1)$ -th interval all the three latent components start to interact with each other. 103 ", "page_idx": 2}, {"type": "text", "text": "where the multiplicative term $\\delta^{(t)}\\,\\,\\,\\,\\,\\,\\sim$ $\\mathrm{Gam}\\left(\\epsilon_{0},\\epsilon_{0}\\right)$ and the transition matrices are time-varying as $\\Pi^{(t)}~\\equiv~\\left[\\pi_{k k_{2}}^{(t)}\\right]_{k,k_{2}=1}^{K}$ As shown in Figure 2, to model the time-varying ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "104 transition dynamics, we assume the whole time   \n105 interval can be divided into $I$ equally-spaced sub-intervals. The transition kernel behind complicated   \n106 dynamic counts is assumed to be static within each sub-interval, while evolving over sub-intervals,   \n107 to capture non-stationary behaviours. In another word, the proposed model allows the latent factors   \n108 to evolve over time steps while the transition matrices change over sub-intervals but assumed to be   \n109 stationary within each sub-interval, as shown in Figure 1. In particular, we let each sub-interval   \n110 contains $M$ time steps, and the $i$ -th interval contains time steps $\\{t\\mid t=(i-1)\\,M+1,\\cdots\\,,i M\\}$ .   \n111 We define $i\\left(t\\right)$ as the function that maps time step $t$ to its corresponding sub-interval.   \n112 Dirichlet-Dirichlet Markov processes. To capture how the underlying transition kernel smoothly   \n113 evolves over sub-intervals, we first propose the Dirichlet-Dirichlet (Dir-Dir) Markov chain as ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\pi_{k}^{(i)}\\mid\\pi_{k}^{(i-1)}\\sim\\mathrm{Dir}\\left(\\eta K\\pi_{1k}^{(i-1)},\\cdots,\\eta K\\pi_{K k}^{(i-1)}\\right),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "114 where $\\pi_{k}^{(i)}$ represents the $k$ -th column of $\\Pi^{(i)}$ , and the prior of the scaling parameter $\\eta$ is given by   \n115 $\\eta\\sim\\mathrm{Gam}\\left(e_{0},f_{0}\\right)$ .   \n116 The initial states are defined as $\\theta_{k}^{(1)}~\\sim~\\mathrm{Gam}\\left(\\tau_{0}\\nu_{k},\\tau_{0}\\right)$ . The prior for the transition ker  \n117 nel of the first sub-interval is given by $\\pi_{k}^{(1)}\\ \\sim\\ \\mathrm{Dir}\\left(\\nu_{1}\\nu_{k},\\cdot\\cdot\\cdot\\ ,\\xi\\nu_{k},\\cdot\\cdot\\cdot,\\nu_{K}\\nu_{k}\\right)$ , where $\\nu_{k}\\textrm{\\,\\textsim{}}$   \n118 $\\mathrm{Gam}(\\frac{\\gamma_{0}}{K},\\beta)$ and $\\xi,\\beta\\,\\sim\\,\\mathrm{Gam}\\left(\\epsilon_{0},\\epsilon_{0}\\right)$ . Note that the expectation and variance of the transition   \n119 kernel at $i$ -th sub-interval can be calculated as ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathsf E\\left[\\pi_{k}^{\\left(i\\right)}\\mid\\pi_{k}^{\\left(i-1\\right)}\\right]=\\pi_{k}^{\\left(i-1\\right)},\\quad\\mathsf{V a r}\\left[\\pi_{k_{1}k}^{\\left(i\\right)}\\mid\\pi_{k}^{\\left(i-1\\right)}\\right]=\\frac{\\pi_{k_{1}k}^{\\left(i-1\\right)}\\left(1-\\pi_{k_{1}k}^{\\left(i-1\\right)}\\right)}{\\eta K+1},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "120 respectively. The transition dynamics of $i$ -th sub-interval inherits the information of the previous   \n121 sub-interval, and also adapts to the data observed in the current sub-interval. The scaling parameter $\\eta$   \n122 controls the variance of the transition matrices.   \n123 The prior specification defined in Eq.(2) by rescaling the transition matrix at the previous   \n124 sub-interval allows the transition dynamics to change smoothly, and thus might be insuffi  \n125 cient to capture the rapid changes observed in complicated dynamics. To further improve   \n126 the flexibility of the transition structure, two modified Dirichlet Markov chains are studied to   \n127 capture the correlation structure between the dimensions of the transition matrices over time.   \n128 Dirichlet-Gamma-Dirichlet Markov processes. We first   \n129 introduce the Dirichlet-Gamma-Dirichlet (Dir-Gam-Dir)   \n130 Markov chain to model the evolving transition matrices   \n131 as ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "image", "img_path": "4kCr61XYQJ/tmp/796abc8e75634b99b7e408e35e133e74939ae6e32f21e274a62be089bdadc66c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pmb{\\pi}_{k}^{(i)}\\sim\\mathrm{Dir}\\left(\\alpha_{1k}^{(i)},\\cdots,\\alpha_{K k}^{(i)}\\right),}\\\\ &{\\alpha_{k_{1}k}^{(i)}\\sim\\mathrm{Gam}\\left(\\gamma_{k}^{(i-1)}\\sum_{k_{2}=1}^{K}\\psi_{k k_{1}k_{2}}^{(i-1)}\\pi_{k_{2}k}^{(i-1)},c_{k}^{(i)}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "132 where we use \u03c8(kik\u221211k)2 to capture the mutation between two   \n133 consecutive sub-intervals, and its prior is given by ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\left(\\psi_{k1k_{2}}^{\\left(i-1\\right)},\\cdot\\cdot\\cdot\\mathbf{\\Phi},\\psi_{k K k_{2}}^{\\left(i-1\\right)}\\right)\\sim\\mathrm{Dir}\\left(\\epsilon_{0},\\cdot\\cdot\\cdot\\mathbf{\\Phi},\\epsilon_{0}\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "134 and \u03b3k , ck $\\gamma_{k}^{(i)},c_{k}^{(i)}\\sim\\mathrm{Gam}\\left(\\epsilon_{0},\\epsilon_{0}\\right)$ . Compared with the con  \n135 struction defined by Eq.(2), the expectation of Dirichlet  \n136 Gamma-Dirichlet Markov chain is ", "page_idx": 3}, {"type": "text", "text": "Figure 3: Diagrams of the proposed Dirichlet Markov constructions. (a) is the Dir-Dir construction. (b) is the Dir-GamDir construction which takes mutation into account. (c) illustrates the PR-GamDir construction which adopts Poisson randomized gamma distribution and can be equivalently represented as Eq.(5). ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathsf E\\left[\\pi_{k}^{(i)}\\mid\\pi_{k}^{(i-1)}\\right]=\\Psi_{k}^{(i-1)}\\pi_{k}^{(i-1)}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "137 This construction takes interactions among components of columns into account. Hence it will   \n138 dramatically improve the flexibility of our model and thus better fit more complicated dynamics,   \n139 compared with Dir-Dir Markov chains that only yield smoothing transition dynamics.   \n140 Poisson-randomized-gamma-Dirichlet Markov processes. By leveraging the Poisson-randomized   \n141 gamma distribution [25], we introduce another type of time-varying transition kernels, which also   \n142 model the interactions among components like Dir-Gam-Dir construction but may induce different   \n143 properties such as sparsity. The Poisson-randomized-gamma-Dirichlet (PR-Gam-Dir) Markov chain   \n144 can be formulated as ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pi_{k}^{(i)}\\sim\\mathrm{Dir}\\left(\\alpha_{1k}^{(i)},\\cdots,\\alpha_{K k}^{(i)}\\right),\\;\\alpha_{k_{1}k}^{(i)}\\sim\\mathrm{RG1}\\left(\\epsilon^{\\alpha},\\gamma_{k}^{(i-1)}\\sum_{k2=1}^{K}\\psi_{k k_{1}k_{2}}^{(i-1)}\\pi_{k_{2}k}^{(i-1)},c_{k}^{(i)}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "145 where $\\mathrm{RG}1\\left(\\cdot\\right)$ denotes the randomized gamma distribution of the first type. Similarly, for $\\psi_{k k_{1}k_{2}}^{(i-1)}$   \n146 , and $\\boldsymbol{c}_{k}^{(i)}$ , the priors are given by ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\left(\\psi_{k1k_{2}}^{\\left(i-1\\right)},\\cdots,\\psi_{k K k_{2}}^{\\left(i-1\\right)}\\right)\\sim\\operatorname{Dir}\\left(\\epsilon_{0},\\cdots,\\epsilon_{0}\\right),\\ \\gamma_{k}^{\\left(i\\right)},c_{k}^{\\left(i\\right)}\\sim\\operatorname{Gam}\\left(\\epsilon_{0},\\epsilon_{0}\\right),\\,\\mathrm{respectively}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "147 The diagrams of three Dirichlet Markov constructions are shown in Figure 3. ", "page_idx": 3}, {"type": "text", "text": "148 4 Markov Chain Monte Carlo Inference ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "149 In this section, we present the Gibbs sampler for the proposed NS-PGDS. We only illustrate the key   \n150 points of the derivation and the details can be found in the appendix.   \n151 Lemma 1 If $y\\,\\sim\\,\\mathrm{NB}\\left(a,g\\left(\\zeta\\right)\\right)$ and $l\\,\\sim\\,\\mathrm{CRT}\\left(y,a\\right)$ , where NB (\u00b7) refers to negative-binomial   \n152 distribution, CRT (\u00b7) represents Chinese restaurant table distribution $[26]$ , and $g\\left(z\\right)=1\\!-\\!\\exp\\left(-z\\right)$ .   \n153 Then the joint distribution of $y$ and $l$ can be equivalently distributed as $y\\sim\\mathrm{SumLog}\\left(l,g\\left(\\zeta\\right)\\right)$ and   \n154 $l\\sim\\mathrm{Pois}\\left(a\\zeta\\right)[I I],$ i.e. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{NB}\\left(y;a,g\\left(\\zeta\\right)\\right)\\mathrm{CRT}\\left(l;y,a\\right)=\\mathrm{SumLog}\\left(y;l,g\\left(\\zeta\\right)\\right)\\mathrm{Pois}\\left(l;a\\zeta\\right),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "155 where SumLog (l, g (\u03b6)) = li and $x_{i}\\sim\\operatorname{Log}\\left(g\\left(\\zeta\\right)\\right)$ are independently and identically loga  \n156 rithmic distributed random variables $I27J$ . ", "page_idx": 4}, {"type": "text", "text": "Lemma 2 Suppose $\\mathbf{n}=(n_{1},\\cdots\\,,n_{K})$ and ", "page_idx": 4}, {"type": "equation", "text": "$$\n{\\bf n}\\mid n\\sim\\mathrm{DirMult}\\left(n,r_{1},\\cdot\\cdot\\cdot{\\bf\\nabla},r_{K}\\right),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "157 where DirMult $(\\cdot)$ refers to Dirichlet-multimonial distribution. We sample the augmented variable   \n158 $q\\ \\mid\\ n\\ \\sim\\ \\mathrm{Beta}\\left(n,r.\\right)$ , where $\\textstyle r.\\ =\\ \\sum_{k=1}^{K}r_{k}$ . According to [28], conditioning on $q$ , we have   \n159 $n_{k}\\sim\\mathrm{NB}\\left(r_{k},q\\right)$ . ", "page_idx": 4}, {"type": "text", "text": "160 Sampling $y_{v k}^{(t)}$ : Use the relationship between Poisson and multinomial distributions, we sample ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\left(\\left(y_{v k}^{(t)}\\right)_{k=1}^{K}\\mid-\\right)\\sim\\mathrm{Mult}\\left(y_{v}^{(t)},\\left(\\frac{\\phi_{v k}\\theta_{k}^{(t)}}{\\sum_{k=1}^{K}\\phi_{v k}\\theta_{k}^{(t)}}\\right)_{k=1}^{K}\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "161 Sampling $\\phi_{k}$ : Via Dirichlet-multinomial conjugacy, the posterior of $\\phi_{k}$ is ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(\\phi_{k}\\mid-)\\sim\\operatorname{Dir}\\left(\\epsilon_{0}+\\sum_{t=1}^{T}y_{1k}^{(t)},\\cdot\\cdot\\cdot\\mathrm{~,~}\\epsilon_{0}+\\sum_{t=1}^{T}y_{V k}^{(t)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "162 Sampling $\\theta_{k}^{(t)}$ : To sample from the posterior of $\\theta_{k}^{(t)}$ , we first sample the auxiliary variables. Setting   \n163 l\u00b7(kT +1)= 0 and \u03b6(T +1) = 0, we sample the augmented variables backwards from t = T, \u00b7 \u00b7 \u00b7 , 2, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left(l_{k}^{(t)}\\mid-\\right)\\sim\\mathrm{{CRT}}\\left(y_{\\cdot k}^{(t)}+l_{\\cdot k}^{(t+1)},\\tau_{0}\\sum_{k_{2}=1}^{K}\\pi_{k k_{2}}^{i(t-1)}\\theta_{k_{2}}^{(t-1)}\\right),\\qquad\\qquad\\qquad}\\\\ {\\left(l_{k1}^{(t)},\\cdots,l_{k K}^{(t)}\\mid-\\right)\\sim\\mathrm{{Mult}}\\left(l_{k\\cdot}^{(t)},\\left(\\frac{\\pi_{k1}^{i(t-1)}\\theta_{1}^{(t-1)}}{\\sum_{k_{2}=1}^{K}\\pi_{k k_{2}}^{i(t-1)}\\theta_{k_{2}}^{(t-1)}},\\cdots,\\frac{\\pi_{k K}^{i(t-1)}\\theta_{K}^{(t-1)}}{\\sum_{k_{2}=1}^{K}\\pi_{k k_{2}}^{i(t-1)}\\theta_{k_{2}}^{(t-1)}}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "164 Let us define $\\begin{array}{r}{l_{\\cdot k}^{(t)}\\,=\\,\\sum_{k_{1}=1}^{K}l_{k_{1}k}^{(t)}}\\end{array}$ and $\\begin{array}{r}{\\zeta^{(t)}\\,=\\,\\ln(1+\\frac{\\delta^{(t)}}{\\tau_{0.}}+\\zeta^{(t+1)})\\,}\\end{array}$ . After sampling the auxiliary   \n165 variables, then for $t=1,\\cdot\\cdot\\cdot,T$ , by Poisson-gamma conjugacy, we obtain ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left(\\theta_{k}^{(t)}\\mid-\\right)\\sim\\operatorname{Gam}\\left(y_{\\cdot k}^{(t)}+l_{\\cdot k}^{(t+1)}+\\tau_{0}\\sum_{k_{2}=1}^{K}\\pi_{k k_{2}}^{i(t-1)}\\theta_{k_{2}}^{(t-1)},\\tau_{0}+\\delta^{(t)}+\\zeta^{(t+1)}\\tau_{0}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "166 Sampling $\\Pi^{(i)}$ : We only illustrate Gibbs sampling algorithm for PR-Gam-Dir construction, sampling   \n167 algorithms for other constructions can be found in the appendix. We define $M$ as the length of each   \n168 sub-interval, and $I$ as the number of intervals. For $i\\;=\\;I,\\cdots,2$ , because $(l_{1k}^{(i)},\\cdot\\cdot\\cdot\\;,l_{K k}^{(i)})$ and   \n169 $(g_{.1k}^{(i+1)},\\cdot\\cdot\\cdot\\ ,g_{.K k}^{(i+1)})$ are multinomially distributed, where $\\begin{array}{r}{l_{k_{1}k}^{(i)}\\;=\\;\\sum_{(i-1)M+1}^{i M}l_{k_{1}k}^{(t)}}\\end{array}$ refers to the   \n170 summation of l(kt1)k over i-th sub-interval and same notation for other variables. By the definition   \n171 of Dirichlet-multinomial distribution and Lemma 2, defining  1 g(kIk+1)= 0, we sample the auxiliary   \n117723 vTahreian blwees  fausr $(q_{k}^{(i)}\\mid-)\\sim\\mathrm{Beta}(l_{\\cdot k}^{(i)}+g_{\\cdot k}^{(i+1)},\\alpha_{\\cdot k}^{(i)})$ $(l_{k_{1}k}^{(i)}+g_{\\cdot k_{1}k}^{(i+1)})\\sim\\mathrm{NB}(\\alpha_{k_{1}k}^{(i)},q_{k}^{(i)})$   \n$(h_{k_{1}k}^{(i)}\\mid-)\\sim\\mathrm{CRT}(l_{k_{1}k}^{(i)}+g_{\\cdot k_{1}k}^{(i+1)},\\alpha_{k_{1}k}^{(i)})$ $h_{k_{1}k}^{(i)}\\sim$   \n174 $\\mathrm{Pois}(-\\alpha_{k_{1}k}^{(i)}\\mathrm{ln}(1-q_{k_{\\cdot}}^{(i)}))$ . For Dirichlet-Randomized-Gamma-Dirichlet Markov construction defined   \n175 by Eq.(4), we can equivalently represent it as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\alpha_{k_{1}k}^{(i)}\\sim\\mathrm{Gam}\\left(g_{k_{1}k}^{(i)}+\\epsilon^{\\alpha},c_{k}^{(i)}\\right),\\,\\,g_{k_{1}k}^{(i)}=\\mathrm{Pois}\\left(\\gamma^{(i-1)}\\sum_{k2=1}^{K}\\psi_{k k_{1}k_{2}}^{(i-1)}\\pi_{k_{2}k}^{(i-1)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "176 We define $\\begin{array}{r}{\\lambda_{k_{1}k}^{(i-1)}\\;\\triangleq\\;\\gamma_{k}^{(i-1)}\\sum_{k2=1}^{K}\\psi_{k k_{1}k_{2}}^{(i-1)}\\pi_{k_{2}k}^{(i-1)}}\\end{array}$ =1 \u03c8(kik\u22121k)\u03c0(ki\u2212k1)for notation conciseness. By Poisson-gamma   \n177 conjugacy, we have $(\\alpha_{k_{1}k}^{(i)}\\mid-)\\sim\\mathrm{Gam}(g_{k_{1}k}^{(i)}+\\epsilon^{\\alpha}+h_{k_{1}k}^{(i)},c_{k}^{(i)}-\\ln(1-q_{k}^{(i)}))$ h(ki)k, c(ki) \u2212ln(1 \u2212q(ki ))). If \u03f5\u03b1 > 0, we can   \n178 sample the posterior of $g_{k_{1}k}^{(i)}$ via $(g_{k_{1}k}^{(i)}\\mid-)\\sim\\mathrm{Bessel}(\\epsilon^{\\alpha}-1,2\\sqrt{\\alpha_{k_{1}k}^{(i)}c_{k}^{(i)}\\lambda_{k_{1}k}^{(i-1)}}\\,)$ , where Bessel (\u00b7)   \n179 denotes Bessel distribution. If $\\epsilon^{\\alpha}=0$ , we sample $g_{k_{1}k}^{(i)}$ via ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\left(g_{k_{1}k}^{(i)}\\mid-\\right)\\sim\\left\\{\\begin{array}{l l}{\\mathrm{Pois}\\left(\\frac{c_{k}^{(i)}\\lambda_{k_{1}k}^{(i-1)}}{c_{k}^{(i)}-\\ln\\left(1-q_{k}^{(i)}\\right)}\\right)}&{\\mathrm{if}\\;h_{k_{1}k}^{(i)}=0}\\\\ {\\mathrm{SCH}\\left(h_{k_{1}k}^{(i)},\\frac{c_{k}^{(i)}\\lambda_{k_{1}k}^{(i-1)}}{c_{k}^{(i)}-\\ln\\left(1-q_{k}^{(i)}\\right)}\\right)}&{\\mathrm{otherwise,}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "180 where $\\mathrm{SCH}\\left(\\cdot\\right)$ denotes the shifted confluent hypergeometric distribution [16]. Defining $g_{k_{1}k}^{(i)}\\,=$   \n181 $\\begin{array}{r}{g_{\\boldsymbol{k}_{1}\\cdot\\boldsymbol{k}}^{(i)}=\\sum_{k2=1}^{K}g_{\\boldsymbol{k}_{1}\\boldsymbol{k}_{2}\\boldsymbol{k}}^{(i)}}\\end{array}$ , we first augment ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\left(g_{k_{1}1k}^{(i)},\\cdot\\cdot\\cdot\\cdot,g_{k_{1}K k}^{(i)}\\right)\\sim\\mathrm{Mult}\\left(g_{k_{1}k}^{(i)},\\left(\\psi_{k k_{1}k_{2}}^{(i-1)}\\pi_{k_{2}k}^{(i-1)}\\right)_{k_{2}=1}^{K}\\right),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "$g_{k_{1}k_{2}k}^{(i)}\\sim\\mathrm{Pois}(\\gamma^{(i-1)}\\psi_{k k_{1}k_{2}}^{(i-1)}\\pi_{k_{2}k}^{(i-1)})$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\left(\\psi_{k1k_{2}}^{(i-1)},\\cdot\\cdot\\cdot,\\psi_{k K k_{2}}^{(i-1)}\\right)\\mid-\\right)\\sim\\operatorname{Dir}\\left(\\epsilon_{0}+g_{1k_{2}k}^{(i)},\\cdot\\cdot\\cdot,\\epsilon_{0}+g_{K k_{2}k}^{(i)}\\right),\\mathrm{~and}}\\\\ &{\\left(\\pmb{\\pi}_{k}^{(i-1)}\\mid-\\right)\\sim\\operatorname{Dir}\\left(\\alpha_{1k}^{(i-1)}+l_{1k}^{(i-1)}+g_{\\cdot1k}^{(i)},\\cdot\\cdot\\cdot,\\alpha_{K k}^{(i-1)}+l_{K k}^{(i-1)}+g_{\\cdot K k}^{(i)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "183 Specifically, we have $\\alpha_{k_{1}k}^{(1)}=\\nu_{k_{1}}\\nu_{k}$ = \u03bdk1\u03bdk, if k1 \u0338= k, and \u03b1(k11)k $\\alpha_{k_{1}k}^{(1)}=\\xi\\nu_{k}$ , if $k_{1}=k$ . ", "page_idx": 5}, {"type": "text", "text": "184 5 Related Work ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "185 Modeling count time sequences has been receiving increasing attentions in statistical and machine   \n186 learning communities. Han et al. [23] adopted linear dynamical systems to capture the underlying   \n187 dynamics of the data and leveraged Extended Rank likelihood function to model count observations.   \n188 Some Poisson-gamma models assume that the count vector at each time step is modeled by Poisson   \n189 factor analysis (PFA) [11] and leverage special stochastic processes to model the temporal dependen  \n190 cies of latent factors. For example, gamma process dynamic Poisson factor analysis (GP-DPFA) [15]   \n191 adopts gamma Markov chains which assumes the latent factor of the next time step is drawn from   \n192 a gamma distribution with the shape parameter be the latent factor of the current time step. Schein   \n193 et al. [13] proposed Poisson-gamma dynamical systems (PGDSs), which take the interactions among   \n194 latent dimensions into account and use a transition matrix to capture the interactions. Deep dynamic   \n195 Poisson factor analysis (DDPFA) [29] adopts recurrent neural networks (RNNs) to capture the com  \n196 plex long-term dependencies of latent factors. Yang and Koeppl [30] applied Poisson-gamma count   \n197 model to analyze relational data arising from longitudinal networks, which can capture the evolution   \n198 of individual node-group memberships over time. Many modifications of PGDS have been proposed   \n199 in recent years. Guo et al. [31] proposed deep Poisson-gamma dynamical systems which aim to   \n200 capture the long-range temporal dependencies. Schein et al. [16] employed Poisson-randomized   \n201 gamma distribution to build a new transition process of latent factors. Chen et al. [32] proposed   \n202 Switching Poisson-gamma dynamical systems (SPGDS), allowing PGDS to select from several tran  \n203 sition matrices, and thus can better adapt to nonlinear dynamics. In contrast to SPGDS, the number   \n204 of transition matrices of the proposed NS-PGDS is not limited and thus can be adopted to analyze   \n205 various complicated non-stationary count sequences. Filstroff et al. [33] extensively analyzed many   \n206 gamma Markov chains for non-negative matrix factorization and introduced new gamma Markov   \n207 chains with well-defined stationary distribution (BGAR). ", "page_idx": 5}, {"type": "text", "text": "208 6 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "209 We conducted experiments for both predictive and exploratory analysis to demonstrate the ability of   \n210 the proposed model in capturing non-stationary count time sequences. The baseline models included   \n211 in the experiments are: 1) Gamma process dynamic Poisson factor analysis (GP-DPFA) [15].   \n212 GP-DPFA models the evolution of latent components as $\\theta_{k}^{(t)}\\sim\\operatorname{Gam}(\\theta_{k}^{(t-1)},c_{t})$ , in which each   \n213 component evolves independently of the other components. 2) Gamma Markov chains on the   \n214 rate parameter of gamma distribution (GMC-RATE) [33]. GMC-RATE adopts gamma Markov   \n215 chains defined via the rate parameter of the gamma distribution to model the evolution of $\\theta_{k}^{(t)}$   \n216 as \u03b8 $\\theta_{k}^{(t)}\\sim\\mathrm{Gam}(\\alpha,\\beta/\\theta_{k}^{(t-1)})$ . 3) Gamma Markov chains on the rate parameter with hierarchical   \n217 auxiliary variable (GMC-HIER) [33]. GMC-HIER models the evolution of latent components with   \n218 an auxiliary variables as $z_{k}^{(t)}\\sim\\mathrm{Gam}(\\alpha_{z},\\beta_{z}\\theta_{k}^{(t-1)})$ ) an d \u03b8(kt)\u223cGam(a\u03b8, \u03b2\u03b8z(kt )). 4) Autogressive   \n219 beta-gamma procecss (BGAR) [34, 33]. BGAR is also a gamma Markov model. In contrast to   \n220 the above models, there is a well-defined stationary distribution for BGAR. 5) Poisson-gamma   \n222221 tdhyen eavmoilcuatli osny sotef $\\theta_{k}^{(t)}$ aGs $\\begin{array}{r}{\\theta_{k}^{(t)}\\sim\\mathrm{Gam}(\\tau_{0}\\sum_{k_{2}=1}^{K}\\pi_{k k_{2}}\\theta_{k_{2}}^{(t-1)},\\tau_{0})}\\end{array}$ .dimensions into account, and models   \n223 The real-world datasets used in the experiments are: 1) Integrated Crisis Early Warning System   \n224 (ICEWS): ICEWS is an international relations event dataset, comprising interaction events between   \n225 countries extracted from news corpora. For ICEWS dataset, we have $T\\,=\\,365$ time steps and   \n226 $V=6197$ dimensions, and we set $M=30$ . 2) NIPS: NIPS dataset contains the papers published in   \n227 the NeurIPS conference from 1987 to 2015. We have $T=28$ time steps and $V=2000$ dimensions   \n228 for NIPS dataset and we set $M=5$ . 3) U.S. Earthquake Intensity (USEI): USEI contains a   \n229 collection of damage and felt reports for U.S. (and a few other countries) earthquakes. We use the   \n230 monthly reports from 1957-1986 and have $T=348,V=64$ and set $M=34$ . 4) COVID-19: This   \n231 dataset contains daily death cases data for states in the United States, spanning from March 2020 to   \n232 June 2020. For this dataset, we have $V=51$ dimensions and $T=90$ time steps and set $M=20$ . ", "page_idx": 5}, {"type": "table", "img_path": "4kCr61XYQJ/tmp/09f72e8d73f21898b27dfe405bb6188560966cff2b399039f3ec982af73d2bf0.jpg", "table_caption": [], "table_footnote": ["Table 1: Results of predictive analysis. \"S\" means data smoothing and \"F\" means data forecasting. "], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "233 6.1 Predictive Analysis ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "234 To compare the predictive performance of the proposed model with the baselines, we considered two   \n235 standard tasks: data smoothing and forecasting. For data smoothing task, our objective is to predict   \n236 $\\pmb{y}^{(t)}$ given the remaining data observation $\\mathbf{\\boldsymbol{Y}}\\backslash\\mathbf{\\boldsymbol{y}}^{(t)}$ . To this end, we randomly masked 10 percents of   \n237 the observed data over non-adjacent time steps, and predicted the masked values. For forecasting task,   \n238 we held out data of the last $S$ time steps, and predicted $\\pmb{y}^{(T+1)},\\cdot\\cdot\\cdot\\mathbf{\\Omega},\\pmb{y}^{(T+S)}$ given $\\pmb{y}^{(1)},\\cdot\\cdot\\cdot\\mathbf{\\nabla},\\pmb{{y}}^{(\\overline{{T}})}$ . In   \n239 this experiment we set $S=2$ . We ran the baseline models including GP-DPFA, PGDS, GMC-RATE,   \n240 GMC-HIER, BGAR, using their default settings as provided in [15, 13, 33]. For the NS-PGDS, we set   \n241 $K=100$ for ICEWS, $K=10$ for other datasets, and set $\\tau_{0}=1,\\gamma_{0}=50$ , $\\epsilon_{0}=0.1$ . We performed   \n242 4000 Gibbs sampling iterations. In the experiments, we found that the Gibbs sampler started to   \n243 converge after 1000 iterations, and thus we set the burn-in time be 2000 iterations. We retained   \n244 every hundredth sample, and averaged the predictions over the samples. Mean relative error (MRE)   \n245 and mean absolute error (MAE) are adopted to evaluate the model\u2019s predictive capability, which   \n246 are defined as $\\begin{array}{r}{\\mathrm{MRE}=\\frac{1}{T V}\\sum_{t}\\sum_{v}\\frac{|y_{v}^{(t)}-\\hat{y}_{v}^{(t)}|}{1+y_{v}^{(t)}}}\\end{array}$ and $\\begin{array}{r}{\\mathrm{MAE}=\\frac{1}{T V}\\sum_{t}\\sum_{v}\\mid y_{v}^{(t)}-\\hat{y}_{v}^{(t)}}\\end{array}$ | respectively, ", "page_idx": 6}, {"type": "text", "text": "247 where $y_{v}^{(t)}$ indicates the true count and $\\hat{y}_{v}^{(t)}$ is the prediction. ", "page_idx": 6}, {"type": "text", "text": "248 As the experiment results shown in Table 1, the NS-PGDS exhibits improved performance in both   \n249 data smoothing and forecasting tasks. We attribute this enhanced capability to the time-varying   \n250 transition kernels, which effectively adapt to the non-stationary environment, and thus achieve   \n251 improved predictive performance. For some datasets (e.g. ICEWS) and tasks, the effectiveness of the   \n252 Dir-Gam-Dir and $\\mathrm{Pr}$ -Gam-Dir constructions does not be exhibited in the numerical results. However,   \n253 these two constructions indeed induce more informative patterns compared with Dir-Dir construction,   \n254 as shown in the exploratory analysis.   \n256 We used ICEWS and NIPS datasets for exploratory analysis, and chose the NS-PGDS with Dirichlet  \n257 Dirichlet Markov chains for illustration. Figure 4(a) and Figure 4(b) demonstrate the top 2 latent   \n258 factors inferred by NS-PGDS from ICEWS dataset. From Figure 4(a) we can see that the main labels   \n259 are \u201cIraq (IRQ)\u2013United States (USA)\", \u201cIraq (IRQ)\u2013United Kingdom (UK)\", \u201cRussia (RUS)\u2013United   \n260 States (USA)\", and so on. This latent factor probably corresponds to the topic about Iraq war. Besides,   \n261 in Figure 4(a), there is a peak around March, 2003, and we know that the Iraq war broke out exactly on   \n262 20 March, 2003. In addition, the most dominant labels shown in Figure 4(b) are \u201cJapan (JPN)\u2013United   \n263 States (USA)\", \u201cChina (CHN)\u2013United States (USA)\", \u201cNorth Korea (PRK)\u2013United States (USA)\",   \n264 \u201cSouth Korea (KOR)\u2013United States (USA)\", and so on. We can infer that this latent factor corresponds   \n265 to \u201cSix-Party Talks\" and other accidents about it.   \n266 Figure 4(c) demonstrates the evolving trends of the top 5 latent factors inferred by the NS-PGDS   \n267 from NIPS dataset, and the legend indicates the representative words of the corresponding latent   \n268 factors. Clearly, the green and blue lines correspond to the latent factors of neural network re  \n269 search which started to decline from the 1990s. From the 1990s we see that the latent factors   \n270 about statistical and probabilistic methods began to dominate the NeurIPS conference. In addi  \n271 tion, the NS-PGDS also captured the revival of neural networks (blue line) from the 2010s. The   \n272 above observations from the latent structure inferred by the NS-PGDS match our prior knowledge.   \n273 ", "page_idx": 6}, {"type": "image", "img_path": "4kCr61XYQJ/tmp/0741184f4659fffe185498d651a522c753c608d3ee9223cefb81ffe0cb574f1f.jpg", "img_caption": ["Figure 4: The latent factors inferred by the NS-PGDS. (a) and (b) illustrate the top 2 latent factors inferred from ICEWS dataset, (a) corresponds to Iraq war and (b) corresponds to the Six-Party Talks. (c) illustrates the evolving trends of the top 5 latent factors inferred from NIPS dataset. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Next, we explored the time-varying transition matrices inferred by the NS-PGDS. We chose NIPS dataset for illustratiuon, and set $K=10$ and the interval length $M$ to be 5. The time-varying transition matrices are shown from Figure 5(b) to Figure 5(f). At the beginning, matrices shown in Figure 5(b) and Figure 5(c) are close to identity matrices. Then the transition matrices tend to become block diagonal matrices with 2 blocks, as shown in Figure 5(d)-5(f). The representative words for latent factors in the first block are \u201cstate-linear-classification\", \u201cnetwork-neural-networks\", \u201ckernelimage-space\", \u201cnetwork-neuralnetworks\", \u201cneural-networks-state\". ", "page_idx": 7}, {"type": "image", "img_path": "4kCr61XYQJ/tmp/31aba141cf77b4bcc60993bebce0b4248bb6903d63438053360429d9cc7e276d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "288 Figure 5: Transition matrices inferred from NIPS dataset. (a)   \n289 illustrates the transition matrix inferred by the PGDS. (b)-(f)   \n290 illustrate the time-varying transition matrices inferred by the   \n291 NS-PGDS.   \n293 The representative words for latent factors in the second block are \u201cimage-sparse-matrix\", \u201ckernel  \n294 supervised-random\", \u201cmatrix-sample-random\", \u201cinference-prior-latent\", \u201cstate-policy-gamma\". The   \n295 first block primarily captured the correlations among the research topics about neural networks.   \n296 The second block reflects that, from the 1990s, statistical learning and Bayesian methods began to   \n297 dominate, and these topics are highly correlated. Figure 5(a) illustrates the transition matrix inferred   \n298 by the PGDS, which is averaged over all time steps. Compared with the NS-PGDS, the PGDS can   \n299 not capture the informative time-varying transition dynamics. We also analyzed the features of the   \n300 proposed Dirichlet Markov chains. The left column of Figure 6 demonstrates transition matrices   \n301 of the first four sub-intervals of ICEWS dataset inferred by the NS-PGDS (Dir-Dir). Because of   \n302 the Dir-Dir construction, the consecutive transition matrices smoothly change over time and thus   \n303 the NS-PGDS may lack sufficient flexibility to capture rapid dynamics. The middle column of   \n304 Figure 6 illustrates the transition matrices inferred by the NS-PGDS (Dir-Gam-Dir), which takes   \n305 mutations among latent components into account and captured more complicated patterns. Transition   \n306 matrices inferred by the PR-Gam-Dir construction are shown in the right column of Figure 6, these   \n307 matrices not only exhibited sufficient flexibility but also captured sparser patterns compared with the   \nDir-Gam-Dir construction. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "image", "img_path": "4kCr61XYQJ/tmp/21467d06aea54ced1ca09f262577ad7cd04fb466f9c867998351b5de6106e9c5.jpg", "img_caption": ["Figure 6: From top to bottom are the first four transition matrices inferred by different Dirichlet Markov chains from ICEWS dataset. Top row: Matrices inferred by the Dir-Dir construction. Middle row: Matrices inferred by the Dir-Gam-Dir construction. Bottom row: Matrices inferred by the PR-Gam-Dir construction. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "309 7 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "310 The Poisson-gamma dynamical systems with time-varying transition matrices, have been proposed to   \n311 capture complicated dynamics observed in non-stationary count sequences. In particular, Dirichlet   \n312 Markov chains are constructed to allow the underlying transition matrices to evolve over time.   \n313 Although the Dirichlet Markov processes lack conjugacy, we have developed tractable-but-efficient   \n314 Gibbs sampling algorithms to perform posterior simulation. The experiment results demonstrate the   \n315 improved performance of the proposed NS-PGDS in data smoothing and forecasting tasks, compared   \n316 with the PGDS with a stationary transition kernel. Moreover, the experimental results on several   \n317 real-world data sets show the explainable structures inferred by the proposed NS-PGDS. For the   \n318 future work, we plan to design a method that can find the point of change and thus the length of each   \n319 sub-interval can be determined automatically instead of a constant. We also consider to generalize   \n320 Dirichlet belief networks by incorporating the proposed Dirichlet Markov chain constructions, which   \n321 allow the hierarchical topics to mutate across layers, and thus can generate more rich text information.   \n322 And we also consider to capture non-stationary interaction dynamics among individuals over online   \n323 social networks in the future research. ", "page_idx": 8}, {"type": "text", "text": "324 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "325 [1] David M Blei and John D Lafferty. Dynamic topic models. In Proceedings of the 23rd   \n326 international conference on Machine learning, pages 113\u2013120, 2006.   \n327 [2] Xuerui Wang and Andrew McCallum. Topics over time: a non-markov continuous-time model   \n328 of topical trends. In Proceedings of the 12th ACM SIGKDD international conference on   \n329 Knowledge discovery and data mining, pages 424\u2013433, 2006.   \n330 [3] Patrick J\u00e4hnichen, Florian Wenzel, Marius Kloft, and Stephan Mandt. Scalable generalized   \n331 dynamic topic models. In International Conference on Artificial Intelligence and Statistics,   \n332 pages 1427\u20131435, 2018.   \n333 [4] Daniel Sheldon and Thomas G Dietterich. Collective graphical models. In Proceedings of the   \n334 24th International Conference on Neural Information Processing Systems, pages 1161\u20131169,   \n335 2011.   \n336 [5] James Raymer, Arkadiusz Wi\u00b4sniowski, Jonathan J Forster, Peter WF Smith, and Jakub Bijak.   \n337 Integrated modeling of european migration. Journal of the American Statistical Association,   \n338 108(503):801\u2013819, 2013.   \n339 [6] Tom Wilson. Methods for estimating sub-state international migration: The case of australia.   \n340 Spatial Demography, 5(3):171\u2013192, 2017.   \n341 [7] Philippe Wanner. How well can we estimate immigration trends using google data? Quality &   \n342 Quantity, 55(4):1181\u20131202, 2021.   \n343 [8] R. E. Kalman. A New Approach to Linear Filtering and Prediction Problems. Journal of Basic   \n344 Engineering, 82(1):35\u201345, 1960.   \n345 [9] Zoubin Ghahramani and Sam T Roweis. Learning nonlinear dynamical systems using an   \n346 em algorithm. In Proceedings of the 11th International Conference on Neural Information   \n347 Processing Systems, pages 431\u2013437, 1998.   \n348 [10] M Zhou and L Carin. Augment-and-conquer negative binomial processes. Advances in Neural   \n349 Information Processing Systems, 4:2546\u20132554, 2012.   \n350 [11] Mingyuan Zhou and Lawrence Carin. Negative binomial process count and mixture modeling.   \n351 IEEE Transactions on Pattern Analysis & Machine Intelligence, 37(02):307\u2013320, 2015.   \n352 [12] Aaron Schein, John Paisley, David M Blei, and Hanna Wallach. Bayesian poisson tensor   \n353 factorization for inferring multilateral relations from sparse dyadic event counts. In Proceedings   \n354 of the 21th ACM SIGKDD International conference on knowledge discovery and data mining,   \n355 pages 1045\u20131054, 2015.   \n356 [13] Aaron Schein, Mingyuan Zhou, and Hanna Wallach. Poisson-gamma dynamical systems. In   \n357 Proceedings of the 30th International Conference on Neural Information Processing Systems,   \n358 pages 5012\u20135020, 2016.   \n359 [14] Aaron Schein, Mingyuan Zhou, David Blei, and Hanna Wallach. Bayesian poisson tucker   \n360 decomposition for learning the structure of international relations. In International Conference   \n361 on Machine Learning, pages 2810\u20132819, 2016.   \n362 [15] Ayan Acharya, Joydeep Ghosh, and Mingyuan Zhou. Nonparametric bayesian factor analysis   \n363 for dynamic count matrices. In Artificial Intelligence and Statistics, pages 1\u20139, 2015.   \n364 [16] Aaron Schein, Scott W Linderman, Mingyuan Zhou, David M Blei, and Hanna Wallach.   \n365 Poisson-randomized gamma dynamical systems. In Proceedings of the 33rd International   \n366 Conference on Neural Information Processing Systems, pages 782\u2013793, 2019.   \n367 [17] Jonathan Chang and David Blei. Relational topic models for document networks. In Proceedings   \n368 of the Twelth International Conference on Artificial Intelligence and Statistics, pages 81\u201388,   \n369 2009.   \n370 [18] H Juliette T Unwin, Swapnil Mishra, Valerie C Bradley, Axel Gandy, Thomas A Mellan, Helen   \n371 Coupland, Jonathan Ish-Horowicz, Michaela AC Vollmer, Charles Whittaker, Sarah L Filippi,   \n372 et al. State-level tracking of covid-19 in the united states. Nature Communications, 11(1):1\u20139,   \n373 2020.   \n374 [19] Rainer Winkelmann. Econometric Analysis of Count Data. Springer Publishing Company,   \n375 Incorporated, 5th edition, 2008.   \n376 [20] Guy Grossman, Soojong Kim, Jonah M Rexer, and Harsha Thirumurthy. Political partisanship   \n377 influences behavioral responses to governors\u2019 recommendations for covid-19 prevention in the   \n378 united states. Proceedings of the National Academy of Sciences, 117(39):24144\u201324153, 2020.   \n379 [21] IHME COVID-19 Forecasting Team. Modeling covid-19 scenarios for the united states. Nature   \n380 medicine, 27(1):94\u2013105, 2021.   \n381 [22] Luzhao Feng, Ting Zhang, Qing Wang, Yiran Xie, Zhibin Peng, Jiandong Zheng, Ying Qin,   \n382 Muli Zhang, Shengjie Lai, Dayan Wang, et al. Impact of covid-19 outbreaks and interventions   \n383 on influenza in china and the united states. Nature communications, 12(1):3249, 2021.   \n384 [23] Shaobo Han, Lin Du, Esther Salazar, and Lawrence Carin. Dynamic rank factor model for text   \n385 streams. In Proceedings of the 27th International Conference on Neural Information Processing   \n386 Systems-Volume 2, pages 2663\u20132671, 2014.   \n387 [24] Rahi Kalantari and Mingyuan Zhou. Graph gamma process generalized linear dynamical   \n388 systems. arXiv preprint arXiv:2007.12852, 2020.   \n389 [25] Lin Yuan and John D Kalbfleisch. On the bessel distribution and related problems. Annals of   \n390 the Institute of Statistical Mathematics, 52:438\u2013447, 2000.   \n391 [26] Yee Whye Teh, Michael I Jordan, Matthew J Beal, and David M Blei. Hierarchical dirichlet   \n392 processes. Journal of the American Statistical Association, 101(476):1566\u20131581, 2006.   \n393 [27] Norman L Johnson, Adrienne W Kemp, and Samuel Kotz. Univariate discrete distributions,   \n394 volume 444. John Wiley & Sons, 2005.   \n395 [28] Mingyuan Zhou. Nonparametric bayesian negative binomial factor analysis. Bayesian Analysis,   \n396 13(4):1065\u20131093, 2018.   \n397 [29] Chengyue Gong and Win-bin Huang. Deep dynamic poisson factorization model. In Proceedings   \n398 of the 31st International Conference on Neural Information Processing Systems, pages 1665\u2013   \n399 1673, 2017.   \n400 [30] Sikun Yang and Heinz Koeppl. Dependent relational gamma process models for longitudinal   \n401 networks. In International Conference on Machine Learning, pages 5551\u20135560, 2018.   \n402 [31] Dandan Guo, Bo Chen, Hao Zhang, and Mingyuan Zhou. Deep poisson gamma dynamical   \n403 systems. In Proceedings of the 32nd International Conference on Neural Information Processing   \n404 Systems, pages 8451\u20138461, 2018.   \n405 [32] Wenchao Chen, Bo Chen, Yicheng Liu, Qianru Zhao, and Mingyuan Zhou. Switching poisson   \n406 gamma dynamical systems. In Proceedings of the Twenty-Ninth International Conference on   \n407 International Joint Conferences on Artificial Intelligence, pages 2029\u20132036, 2021.   \n408 [33] Louis Filstroff, Olivier Gouvert, C\u00e9dric F\u00e9votte, and Olivier Capp\u00e9. A comparative study of   \n409 gamma markov chains for temporal non-negative matrix factorization. IEEE Transactions on   \n410 Signal Processing, 69:1614\u20131626, 2021.   \n411 [34] Peter AW Lewis, Edward McKenzie, and David Kennedy Hugus. Gamma processes. Stochastic   \n412 Models, 5(1):1\u201330, 1989.   \n413 [35] John Frank Charles Kingman. Poisson processes, volume 3. Clarendon Press, 1992.   \n415 Notation. When expressing the full conditionals for Gibbs sampling, we use the shorthand \u201c\u2013\u201d to   \n416 denote all other variables. We use \u201c\u00b7\u201d as an index summation shorthand, e.g., $\\textstyle x_{\\cdot j}=\\sum_{i}x_{i j}$ .   \n417 In this section, we present a fully-conjugate and efficient Gibbs sampler for the proposed NS-PGDS.   \n418 The sampling algorithms depend on several key technical results, which we will repeatedly exploit,   \n419 thus we list them below.   \n420 Negative-binomial Distribution. Let $y\\,\\sim\\,{\\mathrm{Pois}}\\left(c\\lambda\\right)$ , and $\\lambda\\,\\sim\\,\\mathrm{Gam}(a,b)$ . If we marginalize   \n421 over $\\lambda$ , then $\\begin{array}{r}{y\\sim\\mathrm{NB}\\left(a,\\frac{c}{b+c}\\right)}\\end{array}$ is a negative-binomial distributed random variable. We can further   \n422 parameterize it as $y\\sim\\mathrm{NB}\\left(a,g\\left(\\zeta\\right)\\right)$ , where $g\\left(z\\right)=1-\\exp\\left(-z\\right)$ and $\\zeta=\\ln\\left(1+\\frac{c}{b}\\right)$ . ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Lemma 1. If $y\\sim\\mathrm{NB}\\left(a,g\\left(\\zeta\\right)\\right)$ and $l\\sim\\mathrm{CRT}\\left(y,a\\right)$ , where $\\mathrm{CRT}\\left(\\cdot\\right)$ represents Chinese restaurant table distribution [26], then the joint distribution of $y$ and $l$ can be equivalently distributed as $y\\sim\\mathrm{SumLog}\\left(l,g\\left(\\zeta\\right)\\right)$ and $l\\sim\\mathrm{Pois}\\left(a\\zeta\\right)$ [11], i.e. ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\mathrm{NB}\\left(y;a,g\\left(\\zeta\\right)\\right)\\mathrm{CRT}\\left(l;y,a\\right)=\\mathrm{SumLog}\\left(y;l,g\\left(\\zeta\\right)\\right)\\mathrm{Pois}\\left(l;a\\zeta\\right),\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "423 where SumLog (l, g (\u03b6)) = li and $x_{i}\\sim\\operatorname{Log}\\left(g\\left(\\zeta\\right)\\right)$ are independently and identically loga  \n424 rithmic distributed random v ariables [27].   \n425 Lemma 2. Suppose $\\mathbf{n}=(n_{1},\\cdots,n_{K})$ and $\\mathbf{n}\\mid n\\sim\\operatorname{DirMult}\\left(n,r_{1},\\cdots,r_{K}\\right)$ , where DirMult (\u00b7)   \n426 refers to Dirichlet-multimonial distribution. We sample the augmented variable q $\\mid n\\sim\\operatorname{Beta}\\left(n,r.\\right)$ ,   \n427 where $\\begin{array}{r}{r_{\\cdot}=\\sum_{k=1}^{K}r_{k}}\\end{array}$ . According to [28], conditioning on $q$ , we have $n_{k}\\sim\\mathrm{NB}\\left(r_{k},q\\right)$ .   \n428 Lemma 3. If $\\begin{array}{r}{y.=\\sum_{s=1}^{S}y_{s}}\\end{array}$ , and $y_{s}\\overset{\\mathrm{i.i.d}}{\\sim}\\operatorname{Pois}(\\lambda_{s}),s=1,\\cdots,S$ . Then $\\begin{array}{r}{y.\\sim\\operatorname{Pois}(\\sum_{s=1}^{S}\\lambda_{s})}\\end{array}$ and   \n429 (y1, \u00b7 \u00b7 \u00b7 , yS) \u223cMult(y\u00b7, ( sS\u03bb=11 \u03bbs , \u00b7 \u00b7 \u00b7 , sS\u03bb=S1 \u03bbs )), where Mult (\u00b7) represents multinomial distribu  \n430 tion [35].   \n431 Sampling $y_{v k}^{(t)}$ : Use the relationship between Poisson and multinomial distributions as described by   \n432 Lemma 3, given observed counts and latent parameters, we sample ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "equation", "text": "$$\n\\left(\\left(y_{v k}^{(t)}\\right)_{k=1}^{K}\\mid-\\right)\\sim\\mathrm{Mult}\\left(y_{v}^{(t)},\\left(\\frac{\\phi_{v k}\\theta_{k}^{(t)}}{\\sum_{k=1}^{K}\\phi_{v k}\\theta_{k}^{(t)}}\\right)_{k=1}^{K}\\right).\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "433 Then the distribution of y(vtk) isyvk $y_{v k}^{(t)}\\sim\\operatorname{Pois}(\\delta^{(t)}\\phi_{v k}\\theta_{k}^{(t)})$ . ", "page_idx": 11}, {"type": "text", "text": "434 Sampling $\\phi_{k}$ : Via Dirichlet-multinomial conjugacy, the posterior of $\\phi_{k}$ is ", "page_idx": 11}, {"type": "equation", "text": "$$\n(\\phi_{k}\\mid-)\\sim\\operatorname{Dir}\\left(\\epsilon_{0}+\\sum_{t=1}^{T}y_{1k}^{(t)},\\cdot\\cdot\\cdot\\,,\\epsilon_{0}+\\sum_{t=1}^{T}y_{V k}^{(t)}\\right).\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "435 Marginalizing over $\\theta_{k}^{(t)}$ : Note that $\\begin{array}{r}{y_{v}^{(t)}=y_{v}^{(t)}=\\sum_{k=1}^{K}y_{v k}^{(t)}}\\end{array}$ and $y_{v k}^{(t)}\\sim\\operatorname{Pois}(\\delta^{(t)}\\phi_{v k}\\theta_{k}^{(t)})$ . Then   \n436 we define $\\begin{array}{r}{y_{\\cdot k}^{(t)}=\\sum_{v=1}^{V}y_{v k}^{(t)}}\\end{array}$ . Because $\\begin{array}{r}{\\sum_{v=1}^{V}\\phi_{v k}=1}\\end{array}$ , we obvtkain $y_{.k}^{(t)}\\sim\\operatorname{Pois}(\\delta^{(t)}\\theta_{k}^{(t)})$ . ", "page_idx": 11}, {"type": "text", "text": "437 We start by marginalizing over $\\theta_{k}^{(T)}$ , using the definition of negative-binomial distribution, we obtain ", "page_idx": 11}, {"type": "equation", "text": "$$\ny_{\\cdot k}^{(T)}\\sim\\mathrm{NB}\\left(\\tau_{0}\\sum_{k_{2}=1}^{K}\\pi_{k k_{2}}^{i(T-1)}\\theta_{k_{2}}^{(T-1)},g\\left(\\zeta^{(T)}\\right)\\right),\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "438 where $\\begin{array}{r}{\\zeta^{(T)}=\\ln(1+\\frac{\\delta^{(T)}}{\\tau_{0}})}\\end{array}$ . Next, we further marginalize over $\\theta_{k}^{(T-1)}$ . To this end, we first sample   \n439 auxiliary variables ", "page_idx": 11}, {"type": "equation", "text": "$$\nl_{k}^{(T)}\\sim\\mathrm{CRT}\\left(y_{\\cdot k}^{(T)},\\tau_{0}\\sum_{k_{2}=1}^{K}\\pi_{k k_{2}}^{i(T-1)}\\theta_{k_{2}}^{(T-1)}\\right).\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "440 By Lemma 1, the joint distribution of $y_{\\cdot k}^{(T)}$ and l(kT )can be expressed as ", "page_idx": 11}, {"type": "equation", "text": "$$\ny_{\\cdot k}^{(T)}\\sim\\operatorname{SumLog}\\left(l_{k}^{(T)},g\\left(\\zeta^{(T)}\\right)\\right)\\mathrm{~and~}l_{k}^{(T)}\\sim\\operatorname{Pois}\\left(\\zeta^{(T)}\\tau_{0}\\sum_{k_{2}=1}^{K}\\pi_{k k_{2}}^{i(T-1)}\\theta_{k_{2}}^{(T-1)}\\right).\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "441 Via Lemma 3, we re-express the auxiliary variables as ", "page_idx": 12}, {"type": "equation", "text": "$$\nl_{k}^{(T)}=l_{k}^{(T)}=\\sum_{k_{2}=1}^{K}l_{k k_{2}}^{(T)},\\mathrm{~and~obtain}\\,l_{k k_{2}}^{(T)}\\sim\\mathrm{Pois}\\left(\\zeta^{(T)}\\tau_{0}\\pi_{k k_{2}}^{i(T-1)}\\theta_{k_{2}}^{(T-1)}\\right).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "442 Then we define $\\begin{array}{r}{l_{\\cdot k}^{(T)}=\\sum_{k_{1}=1}^{K}l_{k_{1}k}^{(T)}}\\end{array}$ . Leveraging Lemma 3 and $\\begin{array}{r}{\\sum_{k_{1}=1}^{K}\\pi_{k_{1}k}^{i(T-1)}=1}\\end{array}$ \u03c0i(T \u22121)= 1, we obtain ", "page_idx": 12}, {"type": "equation", "text": "$$\nl_{\\cdot,k}^{\\left(T\\right)}\\sim\\mathrm{Pois}\\left(\\zeta^{\\left(T\\right)}\\tau_{0}\\theta_{k}^{\\left(T-1\\right)}\\right)\\mathrm{~and~}\\left(l_{\\cdot1k}^{\\left(T\\right)},\\cdots,l_{K k}^{\\left(T\\right)}\\right)\\sim\\mathrm{Mult}\\left(l_{\\cdot k}^{\\left(T\\right)},\\left(\\pi_{1k}^{i\\left(T-1\\right)},\\cdots,\\pi_{K k}^{i\\left(T-1\\right)}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "443 Next, note that y \u00b7(kT \u22121)\u223cPois(\u03b4(T \u22121)\u03b8(kT \u22121)), if we introduce m(kT $m_{k}^{(T-1)}=y_{.k}^{(T-1)}+l_{.k}^{(T)}$ y\u00b7(kT \u22121)+ l\u00b7(kT  ), then we   \n444 have ", "page_idx": 12}, {"type": "equation", "text": "$$\nm_{k}^{(T-1)}\\sim\\mathrm{Pois}\\left(\\theta_{k}^{(T-1)}\\left(\\delta^{(T-1)}+\\zeta^{(T)}\\tau_{0}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "445 Because the prior of $\\theta_{k}^{(T-1)}$ is gamma distributed, by the definition of negative-binomial distribution,   \n44 6 we can again marginalize over $\\theta_{k}^{(T-1)}$ to obtain ", "page_idx": 12}, {"type": "equation", "text": "$$\nm_{k}^{(T-1)}\\sim\\mathrm{NB}\\left(\\tau_{0}\\sum_{k_{2}=1}^{K}\\pi_{k k_{2}}^{i(T-2)}\\theta_{k_{2}}^{(T-2)},g\\left(\\zeta^{(T-1)}\\right)\\right),\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "447 where $\\begin{array}{r}{\\zeta^{(T-1)}=\\ln(1+\\frac{\\delta^{(T-1)}}{\\tau_{0}}+\\zeta^{(T)})}\\end{array}$ . Then we introduce auxiliary variables ", "page_idx": 12}, {"type": "equation", "text": "$$\nl_{k}^{(T-1)}\\sim\\mathrm{CRT}\\left(m_{k}^{(T-1)},\\tau_{0}\\sum_{k_{2}=1}^{K}\\pi_{k k_{2}}^{i(T-2)}\\theta_{k_{2}}^{(T-2)}\\right).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "448 And similar to the case for $t=T$ , we can obtain ", "page_idx": 12}, {"type": "equation", "text": "$$\nl_{\\cdot,k}^{(T-1)}\\sim\\mathrm{Pois}\\left(\\zeta^{(T-1)}\\tau_{0}\\theta_{k}^{(T-2)}\\right)\\ \\mathrm{and}\\ m_{k}^{(T-2)}\\sim\\mathrm{NB}\\left(\\tau_{0}\\sum_{k_{2}=1}^{K}\\pi_{k k_{2}}^{i(T-3)}\\theta_{k_{2}}^{(T-3)},g\\left(\\zeta^{(T-2)}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "449 Thus we have marginalized over $\\theta_{k}^{(T-2)}$ . Note that we can repeat this marginalization process   \n450 recursively until t = 1 with \u03b6(t) = ln(1 + \u03b4\u03c40 $\\begin{array}{r}{\\zeta^{(t)}=\\ln(1+\\frac{\\delta^{(t)}}{\\tau_{0}}+\\zeta^{(t+1)})}\\end{array}$ (t) + \u03b6(t+1)) and m(kT $m_{k}^{(T)}=y_{\\cdot k}^{(T)}$ to maginalize over all the   \n451 $\\theta_{k}^{(t)}$ .   \n452 Sampling $\\theta_{k}^{(t)}$ : Via the above marginalization process, to sample from the posterior of $\\theta_{k}^{(t)}$ , we   \n453 first sample the auxiliary variables. Setting l\u00b7(kT +1)= 0 and \u03b6(T +1) = 0, we sample the augmented   \n454 variables backwards from $t=T,\\cdots\\,,2$ , ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left(l_{k}^{(t)}\\mid-\\right)\\sim\\mathrm{{CRT}}\\left(y_{\\cdot k}^{(t)}+l_{\\cdot k}^{(t+1)},\\tau_{0}\\underbrace{\\sum_{k_{2}=1}^{K}\\pi_{k k_{2}}^{i(t-1)}\\theta_{k_{2}}^{(t-1)}}_{k_{2}=1}\\right),\\ \\ \\ \\ \\ \\ }\\\\ {\\left(l_{k1}^{(t)},\\cdots,l_{k K}^{(t)}\\mid-\\right)\\sim\\mathrm{{Mult}}\\left(l_{k}^{(t)},\\left(\\frac{\\pi_{k1}^{i(t-1)}\\theta_{1}^{(t-1)}}{\\sum_{k_{2}=1}^{K}\\pi_{k k_{2}}^{i(t-1)}\\theta_{k_{2}}^{(t-1)}},\\cdots,\\frac{\\pi_{k K}^{i(t-1)}\\theta_{K}^{(t-1)}}{\\sum_{k_{2}=1}^{K}\\pi_{k k_{2}}^{i(t-1)}\\theta_{k_{2}}^{(t-1)}}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "455 And via Lemma 3, we obtain ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\left(l_{1k}^{(t)},\\cdot\\cdot\\cdot\\,,l_{K k}^{(t)}\\right)\\sim\\mathrm{Mult}\\left(l_{\\cdot k}^{(t)},\\pi_{1k}^{i(t-1)},\\cdot\\cdot\\cdot\\,,\\pi_{K k}^{i(t-1)}\\right)\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "456 We compute $\\zeta^{(t)}$ recursively via ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\zeta^{(t)}=\\ln\\left(1+\\frac{\\delta^{(t)}}{\\tau_{0}}+\\zeta^{(t+1)}\\right).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "457 After sampling the auxiliary variables, then for $t=1,\\cdot\\cdot\\cdot,T$ , by Poisson-gamma conjugacy, we   \n458 obtain ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\theta_{k}^{(1)}\\mid-\\right)\\sim\\mathrm{Gam}\\left(y_{\\cdot k}^{(1)}+l_{\\cdot k}^{(2)}+\\tau_{0}\\nu_{k},\\tau_{0}+\\delta^{(1)}+\\zeta^{(2)}\\tau_{0}\\right),}\\\\ &{\\left(\\theta_{k}^{(t)}\\mid-\\right)\\sim\\mathrm{Gam}\\left(y_{\\cdot k}^{(t)}+l_{\\cdot k}^{(t+1)}+\\tau_{0}\\displaystyle\\sum_{k_{2}=1}^{K}\\pi_{k k_{2}}^{i(t-1)}\\theta_{k_{2}}^{(t-1)},\\tau_{0}+\\delta^{(t)}+\\zeta^{(t+1)}\\tau_{0}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "459 460 Sampling $\\Pi^{(i)}$ : We define $M$ as the length of each sub-interval, and $I$ as the number of intervals. 446612 cFoonr $i=I$ ,,  bwye  oEbqt.a(i1n0), $(l_{1k}^{(I)},\\cdot\\cdot\\cdot\\;,l_{K k}^{(I)})$ is multinomial distributed. Thus by multinomial-Dirichlet ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\left(\\pi_{k}^{\\left(I\\right)}\\mid-\\right)\\sim\\mathrm{Dir}\\left(\\alpha_{1k}^{\\left(I\\right)}+l_{1k}^{\\left(I\\right)},\\cdot\\cdot\\cdot\\,,\\alpha_{K k}^{\\left(I\\right)}+l_{K k}^{\\left(I\\right)}\\right),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "463 where $l_{k_{1}k}^{(I)}$ indicates the summation of $l_{k_{1}k}^{(t)}$ over $I$ -th sub-interval, i.e. $\\begin{array}{r}{l_{k_{1}k}^{(I)}=\\sum_{t=(I-1)M+1}^{T}l_{k_{1}k}^{(t)}}\\end{array}$ ", "page_idx": 13}, {"type": "text", "text": "464 Inference for Dirichlet-Dirichlet Markov chains. For Dirichlet-Dirichlet Markov chains, $\\alpha_{k_{1}k}^{(i)}=$   \n465 $\\eta K\\pi_{k_{1}k}^{(i-1)}$ . By Eq.(10), $(l_{1k}^{(i)},\\cdot\\cdot\\cdot\\,,l_{K k}^{(i)})$ is multinomial distributed. If we marginalize $(\\pi_{1k}^{(i)},\\cdot\\cdot\\cdot\\ ,\\bar{\\pi}_{K k}^{(i)})$ ,   \n466 $(l_{1k}^{(i)},\\cdot\\cdot\\cdot\\,,l_{K k}^{(i)})$ will be Dirichlet-multinomial distributed. Thus by Lemma 2, for $i=I$ , we first sample   \n467 the auxiliary variables as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\left(q_{k}^{(I)}\\mid-\\right)\\sim\\mathrm{Beta}\\left(l_{\\cdot k}^{(I)},\\eta K\\right)\\mathrm{~and~}\\left(h_{k_{1}k}^{(I)}\\mid-\\right)\\sim\\mathrm{CRT}\\left(l_{k_{1}k}^{(I)},\\eta K\\pi_{k_{1}k}^{(I-1)}\\right).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "446698 y,  wEeq .s(a1m8)p,l $(h_{1k}^{(i)},\\cdot\\cdot\\cdot\\,,h_{K k}^{(i)})$ a riisa ballesso  aDsirichlet-multinomial distributed. Thus for $i=$ $I-1,\\cdots\\,,2$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\left(q_{k}^{(i)}\\mid-\\right)\\sim\\mathrm{Beta}\\left(l_{\\cdot k}^{(i)}+h_{\\cdot k}^{(i+1)},\\eta K\\right)\\mathrm{~and~}\\left(h_{k1k}^{(i)}\\mid-\\right)\\sim\\mathrm{CRT}\\left(l_{k1k}^{(i)}+h_{k1k}^{(i+1)},\\eta K\\pi_{k1k}^{(i-1)}\\right),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "470 where $\\begin{array}{r}{l_{k_{1}k}^{(i)}=\\sum_{(i-1)M+1}^{i M}l_{k_{1}k}^{(t)}}\\end{array}$ k refers to the summation of l(kt)k over $i$ -th interval. Via Lemma 2,   \n471 conditioning on qk , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\left(l_{k_{1}k}^{(i)}+h_{k_{1}k}^{(i+1)}\\right)\\sim\\mathrm{NB}\\left(\\eta K\\pi_{k_{1}k}^{(i-1)},q_{k}^{(i)}\\right).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "472 Then via Lemma 1, we obtain ", "page_idx": 13}, {"type": "equation", "text": "$$\nh_{k_{1}k}^{(i)}\\sim\\mathrm{Pois}\\left(-\\eta K\\pi_{k_{1}k}^{(i-1)}\\mathrm{ln}\\left(1-q_{k}^{(i)}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "473 Note that by Eq.(17), $h_{k_{1}k}^{(i)}$ is Poisson distributed and by Lemma 3, we obtain ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left(h_{1k}^{\\left(i\\right)},\\cdot\\cdot\\cdot\\cdot,h_{K k}^{\\left(i\\right)}\\right)\\sim\\mathrm{Mult}\\left(h_{\\cdot k}^{\\left(i\\right)},\\left(\\pi_{1k}^{\\left(i-1\\right)},\\cdot\\cdot\\cdot\\cdot,\\pi_{K k}^{\\left(i-1\\right)}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "474 In addition, note that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left(l_{1k}^{(i-1)},\\cdot\\cdot\\cdot,\\,\\,\\ l_{K k}^{(i-1)}\\right)\\sim\\mathrm{Mult}\\left(l_{\\cdot k}^{(i-1)},\\left(\\pi_{1k}^{(i-1)},\\cdot\\cdot\\cdot\\,,\\pi_{K k}^{(i-1)}\\right)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "475 via Dirichlet-multinomial conjugacy, for $i=I-1,\\cdots\\,,2$ , we obtain ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\left(\\pi_{k}^{\\left(i\\right)}\\mid-\\right)\\sim\\mathrm{Dir}\\left(\\eta K\\pi_{1k}^{\\left(i-1\\right)}+l_{1k}^{\\left(i\\right)}+h_{1k}^{\\left(i+1\\right)},\\cdots,\\eta K\\pi_{K k}^{\\left(i-1\\right)}+l_{K k}^{\\left(i\\right)}+h_{K k}^{\\left(i+1\\right)}\\right).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "476 Specifically, for $i=1$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\left(\\pi_{k}^{(1)}\\mid-\\right)\\sim\\operatorname{Dir}\\left(\\nu_{1}\\nu_{k}+l_{1k}^{(1)}+h_{1k}^{(2)},\\cdots,\\xi\\nu_{k}+l_{k k}^{(1)}+h_{k k}^{(2)},\\cdots,\\nu_{K}\\nu_{k}+l_{K k}^{(1)}+h_{K k}^{(2)}\\right).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "477 For sampling $\\eta$ , note that $(h_{k_{1}k}^{(i)}\\mid-)\\sim\\mathrm{Pois}(-\\eta K\\pi_{k_{1}k}^{(i-1)}\\mathrm{ln}\\left(1-q_{k}^{(i)}\\right))$ , $i=I,\\cdots,2$ . Given the   \n478 prior $\\eta\\sim\\mathrm{Gam}\\left(e_{0},f_{0}\\right)$ , via Poisson-gamma conjugacy, we obtain ", "page_idx": 13}, {"type": "equation", "text": "$$\n(\\eta\\mid-)\\sim\\operatorname{Gam}\\left(e_{0}+\\sum_{i=2}^{I}\\sum_{k_{1}=1}^{K}\\sum_{k_{2}=1}^{K}h_{k_{1}k_{2}}^{(i)},f_{0}-K\\sum_{i=2}^{I}\\sum_{k=1}^{K}\\ln\\left(1-q_{k}^{(i)}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "479 Inference for Dirichlet-Gamma-Dirichlet Markov chains. For Dirichlet-Gamma-Dirichlet Markov   \n480 chains ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\alpha_{k_{1}k}^{(i)}\\sim\\mathrm{Gam}\\left(\\gamma_{k}^{(i-1)}\\sum_{k2=1}^{K}\\psi_{k k_{1}k_{2}}^{(i-1)}\\pi_{k_{2}k}^{(i-1)},c_{k}^{(i)}\\right).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "481 By Eq.(10), $(l_{1k}^{(i)},\\cdot\\cdot\\cdot\\,,l_{K k}^{(i)})$ is multinomial distributed. If we marginalize $(\\pi_{1k}^{(i)},\\cdot\\cdot\\cdot\\,,\\pi_{K k}^{(i)})$ ,   \n482 $(l_{1k}^{(i)},\\cdot\\cdot\\cdot\\,,l_{K k}^{(i)})$ will be Dirichlet-multinomial distributed. Thus by Lemma 2, for $i\\,=\\,I$ , we first   \n483 sample the auxiliary variables as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left(q_{k}^{(I)}\\mid-\\right)\\sim\\mathrm{Beta}\\left(l_{\\cdot k}^{(I)},\\alpha_{\\cdot k}^{(I)}\\right)\\mathrm{~and~}\\left(h_{k_{1}k}^{(I)}\\mid-\\right)\\sim\\mathrm{CRT}\\left(l_{k_{1}k}^{(I)},\\alpha_{k_{1}k}^{(I)}\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "484 Similarly, by Eq.(27), $(g_{\\cdot1k}^{(i)},\\cdot\\cdot\\cdot\\,,g_{\\cdot K k}^{(i)})$ g\u00b7(iK)k) is also Dirichlet-multinomial distributed. Thus for i =   \n485 $I-1,\\cdots\\,,2$ , we sample the auxiliary variables as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left(q_{k}^{(i)}\\mid-\\right)\\sim\\mathrm{Beta}\\left(l_{\\cdot k}^{(i)}+g_{\\cdot k}^{(i+1)},\\alpha_{\\cdot k}^{(i)}\\right)\\mathrm{~and~}\\left(h_{k_{1}k}^{(i)}\\mid-\\right)\\sim\\mathrm{CRT}\\left(l_{k_{1}k}^{(i)}+g_{\\cdot k_{1}k}^{(i+1)},\\alpha_{k_{1}k}^{(i)}\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "486 Via Lemma 2, conditioning on $q_{k}^{\\left(i\\right)}$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left(l_{k_{1}k}^{\\left(i\\right)}+g_{\\cdot k_{1}k}^{\\left(i+1\\right)}\\right)\\sim\\mathrm{NB}\\left(\\alpha_{k_{1}k}^{\\left(i\\right)},q_{k}^{\\left(i\\right)}\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "487 Then via Lemma 1, we obtain ", "page_idx": 14}, {"type": "equation", "text": "$$\nh_{k_{1}k}^{(i)}\\sim\\operatorname{Pois}\\left(-\\alpha_{k_{1}k}^{(i)}\\mathrm{ln}\\left(1-q_{k}^{(i)}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "488 Thus via Poisson-gamma conjugacy, we obtain ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left(\\alpha_{k_{1}k}^{(i)}\\mid-\\right)\\sim\\mathrm{Gam}\\left(\\gamma_{k}^{(i-1)}\\sum_{k2=1}^{K}\\psi_{k k_{1}k_{2}}^{(i-1)}\\pi_{k_{2}k}^{(i-1)}+h_{k_{1}k}^{(i)},c_{k}^{(i)}-\\ln\\left(1-q_{k}^{(i)}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "489 Marginalizing over $\\alpha_{k_{1}k}^{(i)}$ , and via the definition of negative-binomial distribution, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\nh_{k1k}^{(i)}\\sim\\mathrm{NB}\\left(\\gamma_{k}^{(i-1)}\\sum_{k2=1}^{K}\\psi_{k k1k_{2}}^{(i-1)}\\pi_{k2k}^{(i-1)},\\frac{-\\ln\\left(1-q_{k}^{(i)}\\right)}{c_{k}^{(i)}-\\ln\\left(1-q_{k}^{(i)}\\right)}\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "490 Then using Lemma 1, we sample ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left(g_{k_{1}k}^{(i)}\\mid-\\right)\\sim\\mathrm{CRT}\\left(h_{k_{1}k}^{(i)},\\gamma_{k}^{(i-1)}\\sum_{k2=1}^{K}\\psi_{k k_{1}k_{2}}^{(i-1)}\\pi_{k_{2}k}^{(i-1)}\\right),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "491 and obtain ", "page_idx": 14}, {"type": "equation", "text": "$$\ng_{k1k}^{(i)}\\sim\\mathrm{Pois}\\left(\\gamma_{k}^{(i-1)}\\sum_{k2=1}^{K}\\psi_{k k1k_{2}}^{(i-1)}\\pi_{k2k}^{(i-1)}\\mathrm{ln}\\left(1-\\ln\\left(1-q_{k}^{(i)}\\right)/c_{k}^{(i)}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "492 If we define gkk = gk\u00b7k $\\begin{array}{r}{g_{k_{1}k}^{(i)}=g_{k_{1}\\cdot k}^{(i)}=\\sum_{k2=1}^{K}g_{k_{1}k_{2}k}^{(i)}}\\end{array}$ , and augment ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left(g_{k_{1}1k}^{(i)},\\cdot\\cdot\\cdot\\cdot,g_{k_{1}K k}^{(i)}\\right)\\sim\\mathrm{Mult}\\left(g_{k_{1}k}^{(i)},\\left(\\psi_{k k_{1}k_{2}}^{(i-1)}\\pi_{k_{2}k}^{(i-1)}\\right)_{k_{2}=1}^{K}\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "493 By Lemma 3, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\ng_{k_{1}k_{2}k}^{(i)}\\sim\\mathrm{Pois}\\left(\\gamma^{(i-1)}\\psi_{k k_{1}k_{2}}^{(i-1)}\\pi_{k_{2}k}^{(i-1)}\\mathrm{ln}\\left(1-\\ln\\left(1-q_{k}^{(i)}\\right)/c_{k}^{(i)}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "494 Using Lemma 3 and kK1 \u03c8(kik\u221211k)2 = 1, we have, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left(g_{\\cdot1k}^{\\left(i\\right)},\\cdot\\cdot\\cdot\\cdot,g_{\\cdot K k}^{\\left(i\\right)}\\right)\\sim\\mathrm{Mult}\\left(g_{\\cdot k}^{\\left(i\\right)},\\left(\\pi_{k_{1}k}^{\\left(i-1\\right)}\\right)_{k_{1}=1}^{K}\\right),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "495 ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left(g_{1k_{2}k}^{(i)},\\cdot\\cdot\\cdot\\,,g_{K k_{2}k}^{(i)}\\right)\\sim\\mathrm{Mult}\\left(g_{\\cdot k_{2}k}^{(i)},\\left(\\psi_{k k_{1}k_{2}}^{(i-1)}\\right)_{k1=1}^{K}\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "496 Thus by Dirichlet-multinomial conjugacy, for $i=I,\\cdots,2$ , we can obtain ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\left(\\psi_{k1k_{2}}^{(i-1)},\\cdots,\\psi_{k K k_{2}}^{(i-1)}\\right)\\,|\\,-\\right)\\sim\\operatorname{Dir}\\left(\\epsilon_{0}+g_{1k_{2}k}^{(i)},\\cdots,\\epsilon_{0}+g_{K k_{2}k}^{(i)}\\right),}\\\\ &{\\qquad\\qquad\\qquad\\left(\\pi_{k}^{(i-1)}\\,|\\,-\\right)\\sim\\operatorname{Dir}\\left(\\alpha_{1k}^{(i-1)}+l_{1k}^{(i-1)}+g_{\\cdot1k}^{(i)},\\cdots,\\alpha_{K k}^{(i-1)}+l_{K k}^{(i-1)}+g_{\\cdot K k}^{(i)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "497 For sampling $\\gamma_{k}^{(i-1)}$ 1), note that by Eq.(26) and kK1 \u03c8(kik\u221211k)2 = 1, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\ng_{\\cdot k}^{(i)}=\\sum_{k_{1}=1}^{K}g_{k_{1}k}^{(i)}\\mathrm{~and~}g_{\\cdot k}^{(i)}\\sim\\mathrm{Pois}\\left(\\gamma_{k}^{(i-1)}\\mathrm{ln}\\left(1-\\ln\\left(1-q_{k}^{(i)}\\right)/c_{k}^{(i)}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "498 Thus via Poisson-gamma conjugacy, we obtain ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left(\\gamma_{k}^{(i-1)}\\mid-\\right)\\sim\\operatorname{Gam}\\left(\\epsilon_{0}+g_{.k}^{(i)},\\epsilon_{0}+\\ln\\left(1-\\ln\\left(1-q_{k}^{(i)}\\right)\\right)\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "499 By gamma-gamma conjugacy, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left(c_{k}^{(i)}\\mid-\\right)\\sim\\mathrm{Gam}\\left(\\epsilon_{0}+\\gamma_{k}^{(i-1)},\\epsilon_{0}+\\sum_{k_{1}=1}^{K}\\alpha_{k_{1}k}^{(i)}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "500 Inference for Dirichlet-Randomized-Gamma-Dirichlet Markov chains. For Dirichlet501 Randomized-Gamma-Dirichlet Markov chains, ", "text_level": 1, "page_idx": 15}, {"type": "equation", "text": "$$\n\\alpha_{k_{1}k}^{(i)}\\sim\\mathrm{RG1}\\left(\\epsilon^{\\alpha},\\gamma^{(i-1)}\\sum_{k2=1}^{K}\\psi_{k k_{1}k_{2}}^{(i-1)}\\pi_{k_{2}k}^{(i-1)},c_{k}^{(i)}\\right),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "502 which can be equivalently represented as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\alpha_{k_{1}k}^{(i)}\\sim\\mathrm{Gam}\\left(g_{k_{1}k}^{(i)}+\\epsilon^{\\alpha},c_{k}^{(i)}\\right),\\ \\mathrm{and}\\ g_{k_{1}k}^{(i)}=\\mathrm{Pois}\\left(\\gamma^{(i-1)}\\sum_{k2=1}^{K}\\psi_{k k_{1}k_{2}}^{(i-1)}\\pi_{k_{2}k}^{(i-1)}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "503 By Eq.(10), $(l_{1k}^{(i)},\\cdot\\cdot\\cdot\\,,l_{K k}^{(i)})$ is multinomial distributed. If we marginalize $(\\pi_{1k}^{(i)},\\cdot\\cdot\\cdot\\,,\\pi_{K k}^{(i)})$   \n504 $(l_{1k}^{(i)},\\cdot\\cdot\\cdot\\,,l_{K k}^{(i)})$ will be Dirichlet-multinomial distributed. Thus by Lemma 2, for $i\\,=\\,I$ , we first   \n505 sample the auxiliary variables as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left(q_{k}^{(I)}\\mid-\\right)\\sim\\mathrm{Beta}\\left(l_{\\cdot k}^{(I)},\\alpha_{\\cdot k}^{(I)}\\right)\\mathrm{~and~}\\left(h_{k_{1}k}^{(I)}\\mid-\\right)\\sim\\mathrm{CRT}\\left(l_{k_{1}k}^{(I)},\\alpha_{k_{1}k}^{(I)}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "506 Similarly, by Eq.(39), $(g_{\\cdot1k}^{(i)},\\cdot\\cdot\\cdot\\,,g_{\\cdot K k}^{(i)})$ g\u00b7(iK)k) is also Dirichlet-multinomial distributed. Thus for i =   \n507 $I-1,\\cdots\\,,2$ , we sample the auxiliary variables as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left(q_{k}^{(i)}\\mid-\\right)\\sim\\mathrm{Beta}\\left(l_{\\cdot k}^{(i)}+g_{\\cdot k}^{(i+1)},\\alpha_{\\cdot k}^{(i)}\\right)\\mathrm{~and~}\\left(h_{k_{1}k}^{(i)}\\mid-\\right)\\sim\\mathrm{CRT}\\left(l_{k_{1}k}^{(i)}++g_{\\cdot k_{1}k}^{(i+1)},\\alpha_{k_{1}k}^{(i)}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "508 Via Lemma 2, conditioning on $q_{k}^{\\left(i\\right)}$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left(l_{k_{1}k}^{\\left(i\\right)}+g_{\\cdot k_{1}k}^{\\left(i+1\\right)}\\right)\\sim\\mathrm{NB}\\left(\\alpha_{k_{1}k}^{\\left(i\\right)},q_{k}^{\\left(i\\right)}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "509 Then via Lemma 1, we obtain ", "page_idx": 15}, {"type": "equation", "text": "$$\nh_{k_{1}k}^{(i)}\\sim\\operatorname{Pois}\\left(-\\alpha_{k_{1}k}^{(i)}\\mathrm{ln}\\left(1-q_{k}^{(i)}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "510 Via Poisson-gamma conjugacy, we first sample ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left(\\alpha_{k_{1}k}^{(i)}\\mid-\\right)\\sim\\mathrm{Gam}\\left(g_{k_{1}k}^{(i)}+\\epsilon^{\\alpha}+h_{k_{1}k}^{(i)},c_{k}^{(i)}-\\ln\\left(1-q_{k}^{(i)}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "511 If \u03f5\u03b1 > 0, we can sample the posterior of g(ki1)k via ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left(g_{k_{1}k}^{(i)}\\mid-\\right)\\sim\\mathrm{Bessel}\\left(\\epsilon^{\\alpha}-1,2\\sqrt{\\alpha_{k_{1}k}^{(i)}c_{k}^{(i)}\\gamma_{k}^{(i-1)}\\sum_{k2=1}^{K}\\psi_{k k_{1}k_{2}}^{(i-1)}\\pi_{k_{2}k}^{(i-1)}}\\right),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "512 where Bessel $(\\cdot)$ denotes Bessel distribution. If $\\epsilon^{\\alpha}=0$ , we sample $g_{k_{1}k}^{(i)}$ via ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left(g_{k_{1}k}^{(i)}\\mid-\\right)\\sim\\left\\{\\begin{array}{r}{\\mathrm{Pois}\\left(\\frac{c_{k}^{(i)}\\gamma_{k}^{(i-1)}\\sum_{k2=1}^{K}\\psi_{k k_{1}k_{2}}^{(i-1)}\\pi_{k2}^{(i-1)}}{c_{k}^{(i)}-\\ln\\left(1-q_{k}^{(i)}\\right)}\\right)\\qquad\\mathrm{if}\\ h_{k_{1}k}^{(i)}=0}\\\\ {\\mathrm{SCH}\\left(h_{k_{1}k}^{(i)},\\frac{c_{k}^{(i)}\\gamma_{k}^{(i-1)}\\sum_{k2=1}^{K}\\psi_{k k_{1}k_{2}}^{(i-1)}\\pi_{k_{2}k}^{(i-1)}}{c_{k}^{(i)}-\\ln\\left(1-q_{k}^{(i)}\\right)}\\right)\\quad\\mathrm{otherwise},}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "513 where $\\mathrm{SCH}\\left(\\cdot\\right)$ denotes the shifted confluent hypergeometric distribution [16]. ", "page_idx": 16}, {"type": "text", "text": "514 Defining gk1k = gk1\u00b7k $\\begin{array}{r}{g_{k_{1}k}^{(i)}=g_{k_{1}\\cdot k}^{(i)}=\\sum_{k2=1}^{K}g_{k_{1}k_{2}k}^{(i)}}\\end{array}$ , we first augment ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left(g_{k_{1}1k}^{(i)},\\cdot\\cdot\\cdot\\cdot,g_{k_{1}K k}^{(i)}\\right)\\sim\\mathrm{Mult}\\left(g_{k_{1}k}^{(i)},\\left(\\psi_{k k_{1}k_{2}}^{(i-1)}\\pi_{k_{2}k}^{(i-1)}\\right)_{k_{2}=1}^{K}\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "515 By Lemma 3, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\ng_{k_{1}k_{2}k}^{(i)}\\sim\\mathrm{Pois}\\left(\\gamma^{(i-1)}\\psi_{k k_{1}k_{2}}^{(i-1)}\\pi_{k_{2}k}^{(i-1)}\\right),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "516 and because $\\begin{array}{r}{\\sum_{k_{1}}^{K}\\psi_{k k_{1}k_{2}}^{(i-1)}=1}\\end{array}$ )= 1, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~~~~\\left(g_{1k}^{(i)},\\cdot\\cdot\\cdot\\cdot,g_{\\cdot K k}^{(i)}\\right)\\sim\\mathrm{Mult}\\left(g_{\\cdot k}^{(i)},\\left(\\pi_{k_{1}k}^{(i-1)}\\right)_{k_{1}=1}^{K}\\right),\\,\\mathrm{and}}\\\\ &{\\left(g_{1k_{2}k}^{(i)},\\cdot\\cdot\\cdot\\cdot,g_{K k_{2}k}^{(i)}\\right)\\sim\\mathrm{Mult}\\left(g_{\\cdot k_{2}k}^{(i)},\\left(\\psi_{k k_{1}k_{2}}^{(i-1)}\\right)_{k1=1}^{K}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "517 Thus by Dirichlet-multinomial conjugacy, for $i=I,\\cdots,2$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left(\\left(\\psi_{k1k_{2}}^{(i-1)},\\cdot\\cdot\\cdot,\\psi_{k K k_{2}}^{(i-1)}\\right)\\mid-\\right)\\sim\\mathrm{Dir}\\left(\\epsilon_{0}+g_{1k_{2}k}^{(i)},\\cdot\\cdot\\cdot\\cdot,\\epsilon_{0}+g_{K k_{2}k}^{(i)}\\right),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "518 ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left(\\pmb{\\pi}_{k}^{(i-1)}\\mid-\\right)\\sim\\mathrm{Dir}\\left(\\alpha_{1k}^{(i-1)}+l_{1k}^{(i-1)}+g_{\\cdot1k}^{(i)},\\cdots,\\alpha_{K k}^{(i-1)}+l_{K k}^{(i-1)}+g_{\\cdot K k}^{(i)}\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "519 Via Poisson-gamma conjugacy, we obtain ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left(\\gamma_{k}^{(i-1)}\\mid-\\right)\\sim\\mathrm{Gam}\\left(\\epsilon_{0}+g_{.k}^{(i)},\\epsilon_{0}+1\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "520 By gamma-gamma conjugacy, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left(c_{k}^{(i)}\\mid-\\right)\\sim\\mathrm{Gam}\\left(\\epsilon_{0}+\\gamma_{k}^{(i-1)},\\epsilon_{0}+\\sum_{k_{1}=1}^{K}\\alpha_{k_{1}k}^{(i)}\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "521 Specifically, for $i=1$ , we have $\\alpha_{k_{1}k}^{(1)}=\\nu_{k_{1}}\\nu_{k}$ , if $k_{1}\\neq k$ . And $\\alpha_{k_{1}k}^{(1)}=\\xi\\nu_{k}$ , if $k_{1}=k$ . ", "page_idx": 16}, {"type": "text", "text": "522 Sampling $\\nu_{k}$ and $\\xi$ : As we sample $\\Pi^{(i)}$ , by the definition of Dirichlet-multinomial distribution, we   \n523 obtain ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\big(l_{1k}^{(1)}+g_{\\cdot1k}^{(2)},\\cdot\\cdot\\cdot,l_{K k}^{(1)}+g_{\\cdot K k}^{(2)}\\big)\\sim\\mathrm{DirMult}\\left(\\nu_{1}\\nu_{K},\\cdot\\cdot\\cdot,\\xi\\nu_{k},\\cdot\\cdot\\cdot,\\nu_{K}\\nu_{k}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "524 where l(k11)k $\\begin{array}{r}{l_{k_{1}k}^{(1)}=\\sum_{t=1}^{M}l_{k_{1}k}^{(t)}}\\end{array}$ tM=1 l(kt1)k. In particular, with a little abuse of notation here, for Dir-Dir construction,   \n525 we take g\u00b7(k21)k $g_{\\!\\cdot\\!k_{1}k}^{(2)}=h_{k_{1}k}^{(2)}$ . We first sample ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left(h_{k_{1}k}^{(1)}\\mid-\\right)\\sim\\left\\{\\begin{array}{c c}{\\mathrm{CRT}\\left(l_{k_{1}k}^{(1)}+g_{\\cdot k_{1}k}^{(2)},\\nu_{k_{1}}\\nu_{k}\\right)}&{k_{1}\\neq k}\\\\ {\\mathrm{CRT}\\left(l_{k_{1}k}^{(1)}+g_{\\cdot k_{1}k}^{(2)},\\xi\\nu_{k}\\right)}&{k_{1}=k.}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "526 Then we sample ", "page_idx": 16}, {"type": "equation", "text": "$$\nq_{k}^{(1)}\\sim\\mathrm{Beta}\\left(l_{\\cdot k}^{(1)}+g_{\\cdot k}^{(2)},\\nu_{k}\\left(\\sum_{k_{1}\\neq k}\\nu_{k1}+\\xi\\right)\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "527 We further introduce ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle n_{k}=\\!h_{k k}^{(1)}+\\sum_{k_{1}\\neq k}\\!h_{k_{1}k}^{(1)}+\\sum_{k_{2}\\neq k}\\!h_{k k_{2}}^{(1)}+l_{k}^{(1)}},\\;\\mathrm{and}}}\\\\ {{\\displaystyle\\rho_{k}=\\!\\tau_{0}\\zeta^{(1)}-\\ln\\left(1-q_{k}^{(1)}\\right)\\left(\\xi+\\sum_{k_{1}\\neq k}\\nu_{k_{1}}\\right)-\\sum_{k_{2}\\neq k}\\ln\\left(1-q_{k_{2}}^{(1)}\\right)\\nu_{k_{2}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "528 Via Poisson-gamma conjugacy, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle(\\xi\\mid-)\\sim\\mathrm{Gam}\\left(\\frac{\\gamma_{0}}{K}+\\sum_{k}h_{k k}^{(1)},\\beta-\\sum_{k}\\nu_{k}\\mathrm{ln}\\left(1-q_{k}^{(1)}\\right)\\right),}\\\\ &{\\displaystyle(\\nu_{k}\\mid-)\\sim\\mathrm{Gam}\\left(\\frac{\\gamma_{0}}{K}+n_{k},\\beta+\\rho_{k}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "529 ", "page_idx": 17}, {"type": "text", "text": "530 Sampling ${\\delta^{(t)}}$ and $\\beta$ : Via Poisson-gamma conjugacy ", "text_level": 1, "page_idx": 17}, {"type": "equation", "text": "$$\n\\left(\\delta^{(t)}\\mid-\\right)\\sim\\mathrm{Gam}\\left(\\epsilon_{0}+\\sum_{v=1}^{V}y_{v}^{(t)},\\epsilon_{0}+\\sum_{k=1}^{K}\\theta_{k}^{(t)}\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "531 And by gamma-gamma conjugacy, we obtain ", "page_idx": 17}, {"type": "equation", "text": "$$\n(\\beta\\mid-)\\sim\\mathrm{Gam}\\left(\\epsilon_{0}+\\gamma_{0},\\epsilon_{0}+\\sum_{k=1}^{K}\\nu_{k}\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "532 The full procedure of our Gibbs sampling algorithms are summarized in Algorithm 1, Algorithm 2   \n533 and Algorithm 3. ", "page_idx": 17}, {"type": "text", "text": "Algorithm 1 Gibbs sampling algorithm for NS-PGDS (Dir-Dir Markov construction) Input: observed count sequence $\\{\\pmb{y}^{(t)}\\}_{t=1}^{T}$ , iterations $\\mathcal{I}$ . Initialize the model\u2019s rank $K$ , hyperparameters $\\gamma_{0},\\epsilon_{0},e_{0},f_{0}$ . for iter $=1$ to $\\mathcal{I}$ do Sample $\\{y_{v k}^{(t)}\\}_{v,k}$ via Eq.(6). Sample $\\{\\phi_{k}\\}_{k}$ via Eq.(7). Sample $\\{\\delta^{(t)}\\}_{t}$ via Eq.(48). Update $\\zeta^{(t)}$ as $\\begin{array}{r}{\\zeta^{(T+1)}=0,~~\\zeta^{(t)}=\\ln\\left(1+\\frac{\\delta^{(t)}}{\\tau_{0}}+\\zeta^{(t+1)}\\right),\\,t=T,\\cdot\\cdot\\cdot,1.}\\end{array}$ Set $l_{\\cdot k}^{(T+1)}=0$ . for $t=T$ to 2 do Sample $\\{l_{k\\cdot}^{(t)}\\}_{k}$ and $\\{l_{k k_{2}}^{(t)}\\}_{k,k_{2}}$ via Eq.(8) and Eq.(9) respectively. end for for $t=1$ to $T$ do Sample $\\{\\boldsymbol{\\theta}_{k}^{(t)}\\}_{k}$ via Eq.(12) and Eq.(13). end for for $i=1$ to $I,\\mathbf{do}$ Sample $\\{\\boldsymbol{q}_{k}^{(i)}\\}_{k}$ and $\\{h_{k_{1}k}^{(i)}\\}_{k_{1},k}$ via Eq.(16), Eq.(44) and Eq.(45). Sample $\\{\\pmb{\\pi}_{k}^{(i)}\\}_{k}$ via Eq.(14) and Eq.(19). Sample $\\eta$ via Eq.(21). end for Sample $\\xi$ , $\\{\\nu_{k}\\}_{k}$ , $\\beta$ via Eq.(46), Eq.(47) and Eq.(49) respectively. end for   \nOutput posterior means: $\\underline{{{\\theta}}}_{k}^{\\left(1:T\\right)}\\right\\}_{k},\\:\\left\\{\\phi_{k}\\right\\}_{k},\\:\\left\\{\\pi_{k}^{\\left(i\\right)}\\right\\}_{k},\\:\\delta^{\\left(1:T\\right)},\\:\\xi,\\:\\left\\{\\nu_{k}\\right\\}_{k},\\:\\beta.$   \nInput: observed count sequence $\\{\\pmb{y}^{(t)}\\}_{t=1}^{T}$ , iterations $\\mathcal{I}$ .   \nInitialize the model\u2019s rank $K$ , hyperparameters $\\gamma_{0},\\epsilon_{0},e_{0},f_{0}$ .   \nforSSaammpp $=1$ $\\{y_{v k}^{(t)}\\}_{v,k}$ $\\mathcal{I}$ i oav iEa qE.(q7.)(.6). $\\{\\phi_{k}\\}_{k}$ Sample $\\{\\delta^{(t)}\\}_{t}$ via Eq.(48). Update $\\zeta^{(t)}$ as $\\begin{array}{r}{\\zeta^{(T+1)}=0,~~\\zeta^{(t)}=\\ln\\left(1+\\frac{\\delta^{(t)}}{\\tau_{0}}+\\zeta^{(t+1)}\\right),\\,t=T,\\cdot\\cdot\\cdot,1.}\\end{array}$ fSoert $l_{\\cdot k}^{(T+1)}=0$ .do $t=T$ Sample $\\{l_{k\\cdot}^{(t)}\\}_{k}$ and $\\{l_{k k_{2}}^{(t)}\\}_{k,k_{2}}$ via Eq.(8) and Eq.(9) respectively. end for for $t=1$ to $T$ do Sample $\\{\\boldsymbol{\\theta}_{k}^{(t)}\\}_{k}$ via Eq.(12) and Eq.(13). end for for $i=1$ to $I\\,{\\bf d o}$ Sample $\\{\\alpha_{k_{1}k}^{(i)}\\}_{k_{1},k}$ and $\\{c_{k}^{(i)}\\}_{k}$ via Eq.(24) and Eq.(32). Sample $\\{q_{k}^{(i)}\\}_{k}$ and $\\{h_{k_{1}k}^{(i)}\\}_{k_{1},k}$ via Eq.(22), Eq.(23), Eq.(44) and Eq.(45). Sample $\\left\\{g_{k_{1}k}\\right\\}_{k_{1},k}$ and $\\left\\{g_{k_{1}k_{2}k}\\right\\}_{k_{1},k_{2},k}$ via Eq.(25) and Eq.(26) respectively. Sample $\\{\\psi_{k k_{1}k_{2}}\\}_{k,k_{1},k_{2}}$ via Eq.(28). Sample $\\{\\gamma_{k}^{(i)}\\}_{k}$ via Eq.(31). Sample $\\{\\pmb{\\pi}_{k}^{(i)}\\}_{k}$ via Eq.(14) and Eq.(29). end for Sample $\\xi$ , $\\{\\nu_{k}\\}_{k}$ , $\\beta$ via Eq.(46), Eq.(47) and Eq.(49) respectively.   \nend for   \nOutput posterior means: $\\{\\theta_{k}^{(1:T)}\\}_{k},\\{\\phi_{k}\\}_{k},\\{\\pi_{k}^{(i)}\\}_{k},\\delta^{(1:T)},\\xi,\\{\\nu_{k}\\}_{k},\\beta.$   \nInput: observed count sequence $\\{\\pmb{y}^{(t)}\\}_{t=1}^{T}$ , iterations $\\mathcal{I}$ .   \nInitialize the model\u2019s rank $K$ , hyperparameters $\\gamma_{0},\\epsilon_{0},e_{0},f_{0}$ .   \nforS iatmerp $=1$ $\\{y_{v k}^{(t)}\\}_{v,k}$ $\\mathcal{I}$ ovia Eq.(6). Sample $\\{\\phi_{k}\\}_{k}$ via Eq.(7). Sample $\\{\\delta^{(t)}\\}_{t}$ via Eq.(48). Update $\\zeta^{(t)}$ as $\\begin{array}{r}{\\zeta^{(T+1)}=0,~~\\zeta^{(t)}=\\ln\\left(1+\\frac{\\delta^{(t)}}{\\tau_{0}}+\\zeta^{(t+1)}\\right),\\,t=T,\\cdot\\cdot\\cdot,1.}\\end{array}$   \nfSoert $l_{\\cdot k}^{(T+1)}=0$ .do $t=T$ Sample $\\{l_{k\\cdot}^{(t)}\\}_{k}$ and $\\{l_{k k_{2}}^{(t)}\\}_{k,k_{2}}$ via Eq.(8) and Eq.(9) respectively. end for for $t=1$ to $T$ do Sample $\\{\\boldsymbol{\\theta}_{k}^{(t)}\\}_{k}$ via Eq.(12) and Eq.(13). end for for $i=1$ to $I\\,{\\bf d o}$ Sample $\\{\\alpha_{k_{1}k}^{(i)}\\}_{k_{1},k}$ and $\\{c_{k}^{(i)}\\}_{k}$ via Eq.(33) and Eq.(43). Sample $\\{q_{k}^{(i)}\\}_{k}$ and $\\{h_{k_{1}k}^{(i)}\\}_{k_{1},k}$ via Eq.(33), Eq.(34), Eq.(44) and Eq.(45). Sample $\\left\\{g_{k_{1}k}\\right\\}_{k_{1},k}$ via Eq.(36) and Eq.(37). Sample $\\left\\{g_{k_{1}k_{2}k}\\right\\}_{k_{1},k_{2},k}$ via Eq.(38). Sample $\\{\\gamma_{k}^{(i)}\\}_{k}$ via Eq.(42). Sample $\\{\\tilde{\\psi_{k k_{1}k_{2}}}\\}_{k,k_{1},k_{2}}$ via Eq.(40). Sample $\\{\\pmb{\\pi}_{k}^{(i)}\\}_{k}$ via Eq.(14), and Eq.(41). end for Sample $\\xi$ , $\\{\\nu_{k}\\}_{k}$ , $\\beta$ via Eq.(46), Eq.(47) and Eq.(49) respectively.   \nend for   \nOutput posterior means: $\\{\\theta_{k}^{(1:T)}\\}_{k},\\{\\phi_{k}\\}_{k},\\{\\pi_{k}^{(i)}\\}_{k},\\delta^{(1:T)},\\xi,\\{\\nu_{k}\\}_{k},\\beta.$ ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "534 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 19}, {"type": "text", "text": "Justification: The main contributions of this paper are the constructions of Poisson-Gamma dynamical systems with non-stationary transition dynamics and the corresponding Gibbs sampler. The constructions can be found in sec.2 and sec.3 and the derivation of Gibbs sampler can be found in sec.4 and the appendix. The experiments have demonstrated the effectiveness and features of the proposed model. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "535   \n536   \n537   \n538   \n539   \n540   \n541   \n542   \n543   \n544   \n545   \n546   \n547   \n548   \n549   \n550   \n551   \n552   \n553   \n554   \n555 ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper. ", "page_idx": 19}, {"type": "text", "text": "\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. \u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "57 Justification: As we discussed in the conclusion part, the length of each sub-interval is a   \n58 constant and is treated as a hyper-parameter of the model. In the future work, we plan to   \n59 design a method that can find the point of change and thus the length of each sub-interval   \n60 can be determined automatically.   \n61 Guidelines:   \n62 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n63 the paper has limitations, but those are not discussed in the paper.   \n64 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n65 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n66 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n67 model well-specification, asymptotic approximations only holding locally). The authors   \n68 should reflect on how these assumptions might be violated in practice and what the   \n69 implications would be.   \n70 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n71 only tested on a few datasets or with a few runs. In general, empirical results often   \n72 depend on implicit assumptions, which should be articulated.   \n73 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n74 For example, a facial recognition algorithm may perform poorly when image resolution   \n75 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n76 used reliably to provide closed captions for online lectures because it fails to handle   \n77 technical jargon.   \n78 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n79 and how they scale with dataset size.   \n80 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n81 address problems of privacy and fairness.   \n82 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n83 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n84 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n85 judgment and recognize that individual actions in favor of transparency play an impor  \n86 tant role in developing norms that preserve the integrity of the community. Reviewers   \n87 will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 20}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The derivation of the Gibbs sampler is the main theoretical part of this paper which can be found in sec.4 and the appendix. ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results. \u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced. \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems. \u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. \u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. 4. Experimental Result Reproducibility ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We carefully described the proposed model in sec.2 and sec.3 and the experi", "page_idx": 21}, {"type": "text", "text": "ment details can be found in sec.6.1.   \nGuidelines: \u2022 The answer NA means that the paper does not include experiments. \u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. \u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. \u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. \u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 21}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 21}, {"type": "text", "text": "Justification: The authors will release the data and code as soon as possible if this paper could be accepted. ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "671 6. Experimental Setting/Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "672 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n673 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n674 results? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "676 Justification: The experiment details can be found in sec.6.1.   \n677 Guidelines:   \n78 \u2022 The answer NA means that the paper does not include experiments.   \n79 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n80 that is necessary to appreciate the results and make sense of them.   \n81 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n82 material. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "683 7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "684 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n685 information about the statistical significance of the experiments? ", "page_idx": 22}, {"type": "text", "text": "Justification: Table 1 reports the predictive performance of the proposed model and the corresponding standard deviation. The results are computed by running the Gibbs sampling several times from different initialization. ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 22}, {"type": "text", "text": "11 8. Experiments Compute Resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "716 Justification: The experiments are conducted on a server with an Intel(R) Xeon(R) CPU   \n717 E5-2699Cv4 $@$ 2.20GHz and 64G RAM.   \n718 Guidelines:   \n719 \u2022 The answer NA means that the paper does not include experiments.   \n720 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n721 or cloud provider, including relevant memory and storage.   \n722 \u2022 The paper should provide the amount of compute required for each of the individual   \n723 experimental runs as well as estimate the total compute.   \n724 \u2022 The paper should disclose whether the full research project required more compute   \n725 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n726 didn\u2019t make it into the paper).   \n727 9. Code Of Ethics   \n728 Question: Does the research conducted in the paper conform, in every respect, with the   \n729 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n730 Answer: [Yes]   \n731 Justification: We have checked the NeurIPS Code of Ethics and make sure this work is with   \n732 the NeurIPS Code of Ethics.   \n733 Guidelines:   \n734 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n735 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n736 deviation from the Code of Ethics.   \n737 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n738 eration due to laws or regulations in their jurisdiction).   \n739 10. Broader Impacts   \n740 Question: Does the paper discuss both potential positive societal impacts and negative   \n741 societal impacts of the work performed?   \n742 Answer: [Yes]   \n743 Justification: For positive societal impacts, we have discussed in conclusion section for the ", "page_idx": 23}, {"type": "text", "text": "potential application for textual analysis and social networks. And the authors think this work does not have potential negative societal impacts. ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 23}, {"type": "text", "text": "769 11. Safeguards   \n770 Question: Does the paper describe safeguards that have been put in place for responsible   \n771 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n772 image generators, or scraped datasets)?   \n773 Answer: [NA]   \n774 Justification: This work poses no such risks.   \n775 Guidelines:   \n776 \u2022 The answer NA means that the paper poses no such risks.   \n777 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n778 necessary safeguards to allow for controlled use of the model, for example by requiring   \n779 that users adhere to usage guidelines or restrictions to access the model or implementing   \n780 safety filters.   \n781 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n782 should describe how they avoided releasing unsafe images.   \n783 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n784 not require this, but we encourage authors to take this into account and make a best   \n785 faith effort.   \n786 12. Licenses for existing assets   \n787 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n788 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n789 properly respected?   \n790 Answer: [NA]   \n791 Justification: This paper does not use existing assets.   \n792 Guidelines:   \n793 \u2022 The answer NA means that the paper does not use existing assets.   \n794 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n795 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n796 URL.   \n797 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n798 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n799 service of that source should be provided.   \n800 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n801 package should be provided. For popular datasets, paperswithcode.com/datasets   \n802 has curated licenses for some datasets. Their licensing guide can help determine the   \n803 license of a dataset.   \n804 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n805 the derived asset (if it has changed) should be provided.   \n806 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n807 the asset\u2019s creators.   \n808 13. New Assets   \n809 Question: Are new assets introduced in the paper well documented and is the documentation   \n810 provided alongside the assets?   \n811 Answer: [NA]   \n812 Justification: This paper does not release new assets.   \n813 Guidelines:   \n814 \u2022 The answer NA means that the paper does not release new assets.   \n815 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n816 submissions via structured templates. This includes details about training, license,   \n817 limitations, etc.   \n818 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n819 asset is used.   \n820 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n21 create an anonymized URL or include an anonymized zip file.   \n822 14. Crowdsourcing and Research with Human Subjects   \n823 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n824 include the full text of instructions given to participants and screenshots, if applicable, as   \n825 well as details about compensation (if any)?   \n826 Answer: [NA]   \n827 Justification: This paper does not involve crowdsourcing nor research with human subjects.   \n828 Guidelines:   \n829 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n830 human subjects.   \n831 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n832 tion of the paper involves human subjects, then as much detail as possible should be   \n833 included in the main paper.   \n834 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n835 or other labor should be paid at least the minimum wage in the country of the data   \n836 collector.   \n837 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n838 Subjects   \n839 Question: Does the paper describe potential risks incurred by study participants, whether   \n840 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n841 approvals (or an equivalent approval/review based on the requirements of your country or   \n842 institution) were obtained?   \n843 Answer: [NA]   \n844 Justification: This paper does not involve crowdsourcing nor research with human subjects.   \n845 Guidelines:   \n846 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n847 human subjects.   \n848 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n849 may be required for any human subjects research. If you obtained IRB approval, you   \n850 should clearly state this in the paper.   \n851 \u2022 We recognize that the procedures for this may vary significantly between institutions   \n852 and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the   \n853 guidelines for their institution.   \n854 \u2022 For initial submissions, do not include any information that would break anonymity (if   \n855 applicable), such as the institution conducting the review. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}]