{"references": [{"fullname_first_author": "J. Chee", "paper_title": "QuIP: 2-Bit Quantization of Large Language Models With Guarantees", "publication_date": "2023", "reason": "This paper proposes QuIP, a 2-bit quantization method for LLMs that is directly compared to and built upon in the current work."}, {"fullname_first_author": "T. Dettmers", "paper_title": "QLoRA: Efficient Finetuning of Quantized LLMs", "publication_date": "2023", "reason": "This paper introduces QLoRA, a method for fine-tuning quantized LLMs that is relevant to the current work's exploration of low-rank adaptation."}, {"fullname_first_author": "E. Frantar", "paper_title": "OPTQ: Accurate Quantization for Generative Pre-trained Transformers", "publication_date": "2023", "reason": "This paper introduces OPTQ, an accurate quantization method for LLMs, which is used as a baseline and compared against in this work."}, {"fullname_first_author": "Y. Li", "paper_title": "LoftQ: LORA-Fine-Tuning-Aware Quantization for Large Language Models", "publication_date": "2023", "reason": "This paper introduces LoftQ, another post-training quantization method, that is directly compared to the current work."}, {"fullname_first_author": "A. Tseng", "paper_title": "QuIP#: Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks", "publication_date": "2024", "reason": "This paper introduces QuIP#, an improved version of QuIP, which is used as a direct comparison and baseline for the proposed method."}]}