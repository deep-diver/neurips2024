[{"type": "text", "text": "Compressing Large Language Models using Low Rank and Low Precision Decomposition ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Rajarshi Saha Naomi Sagan Varun Srivastava Stanford University Stanford University Stanford University ", "page_idx": 0}, {"type": "text", "text": "Andrea J. Goldsmith Princeton University ", "page_idx": 0}, {"type": "text", "text": "Mert Pilanci Stanford University ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The prohibitive sizes of Large Language Models (LLMs) today make it difficult to deploy them on memory-constrained edge devices. This work introduces CALDERA \u2013 a new post-training LLM compression algorithm that harnesses the inherent low-rank structure of a weight matrix W by approximating it via a lowrank, low-precision decomposition as $\\mathbf{W}\\approx\\mathbf{Q}+\\mathbf{LR}$ . Here, $\\mathbf{L}$ and $\\mathbf{R}$ are low rank factors, and the entries of $\\mathbf{Q}$ , $\\mathbf{L}$ and $\\mathbf{R}$ are quantized. The model is compressed by substituting each layer with its $\\mathbf{Q}+\\mathbf{LR}$ decomposition, and the zero-shot performance of the compressed model is evaluated. Additionally, $\\mathbf{L}$ and $\\mathbf{R}$ are readily amenable to low-rank adaptation, consequently enhancing the zero-shot performance. CALDERA obtains this decomposition by formulating it as an optimization problem $\\mathrm{min}_{\\mathbf{Q},\\mathbf{L},\\mathbf{R}}\\|(\\mathbf{Q}+\\mathbf{LR}-\\mathbf{W})\\mathbf{\\dot{X}}^{\\top}\\|_{\\mathrm{F}}^{2}$ , where $\\mathbf{X}$ is the calibration data, and $\\mathbf{Q},\\mathbf{L},\\mathbf{R}$ are constrained to be representable using low-precision formats. Theoretical upper bounds on the approximation error of CALDERA are established using a rank-constrained regression framework, and the tradeoff between compression ratio and model performance is studied by analyzing the impact of target rank and quantization bit budget. Results illustrate that compressing LlaMa-2 7B/13B/70B and LlaMa-3 8B models using CALDERA outperforms existing post-training LLM compression techniques in the regime of less than 2.5 bits per parameter. The implementation is available at: https://github.com/pilancilab/caldera. ", "page_idx": 0}, {"type": "text", "text": "Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large Language Models (LLMs) stand out due to their remarkable ability to generate human-like text, thereby supporting a diverse range of applications ranging from writing assistance to code generation. These models leverage vast datasets and significant computational resources to achieve their impressive functionality. The architecture of LLMs typically includes multiple layers, each with weight matrices essential for encoding various aspects of the training data \u2013 from simple syntactic patterns to complex semantic relationships. However, the substantial size of these trained models leads to high computational costs and considerable energy consumption during inference, which can be challenging for deployment in resource-constrained environments. As LLMs continue to expand in scale, compression techniques to reduce the memory and computational requirements of the models are becoming crucial to ensure their broad accessibility. ", "page_idx": 0}, {"type": "text", "text": "Due to the correlated nature of language syntax and semantics learned during training, often, the weight matrices of LLMs exhibit redundancy, which manifests as a low-rank structure. This redundancy suggests the potential for compression without substantial loss in performance. This work introduces CALDERA: Calibration Aware Low-Precision DEcomposition with Low-Rank Adaptation, which compresses LLMs by leveraging the approximate low rank structure inherent in these weight matrices. Given a matrix $\\mathbf{W}\\in\\mathbb{R}^{n\\times d}$ , CALDERA approximates it as $\\mathbf{W}\\approx\\mathbf{Q}+\\mathbf{LR}$ , where $\\mathbf{Q}\\in\\mathbb{R}^{n\\times d}$ , $\\mathbf{L}\\in\\mathbb{R}^{n\\times k}$ and $\\mathbf{R}\\in\\mathbb{R}^{k\\times d}$ . Here, the left and right low rank factors, respectively $\\mathbf{L}$ and $\\mathbf{R}$ , are tall and wide matrices, and $k$ is the target rank. Furthermore, the entries of $\\mathbf{Q}$ , $\\mathbf{L}$ and $\\mathbf{R}$ are represented using low-precision formats with $\\mathrm{B_{Q}}$ , $\\mathrm{B_{L}}$ and $\\mathrm{B_{R}}$ bits per entry, respectively. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "image", "img_path": "lkx3OpcqSZ/tmp/1eda46351a403f62c640d89f6c20764e062099e93a0627d2d40142b05d9b4219.jpg", "img_caption": ["Figure 1: Decaying spectrum of weight matrices (aka, \"approximate low-rank\") "], "img_footnote": [], "page_idx": 1}, {"type": "image", "img_path": "lkx3OpcqSZ/tmp/246c84951828852ea65fcc6a477f61174e7520d75b4e2aa4acb9ecbe1e3b82e8.jpg", "img_caption": ["Figure 2: CALDERA decomposes a full-precision weight matrix into a low-rank component (LR), which captures the contribution of the top singular values using $\\mathrm{B_{L}}$ , $\\mathrm{B_{R}}$ bits, and $\\mathbf{Q}$ for the trailing singular values with $\\mathrm{B_{Q}}$ bits, enabling flexible precision settings for each component. Typically, $\\mathrm{B_{Q}<B_{L},\\bar{B}_{R}}$ . "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Since the singular value proflie (aka spectrum) of the weight matrices of an LLM follow a decaying profile as shown in Fig. 1, the low-rank factors $\\mathbf{L}$ and $\\mathbf{R}$ capture the effect of the large singular components of W with high fidelity. Moreover, the backbone $\\mathbf{Q}$ , which is quantized aggressively \u2013 for instance, using $\\mathrm{B_{Q}}=2$ bits, coarsely captures the essence of the moderately decaying and low singular components of W. CALDERA substitutes each weight matrix W in an LLM, with its approximate low-precision and low-rank decomposition $\\mathbf{Q}+\\mathbf{LR}$ , resulting in a post-training quantization strategy that delivers state-of-the-art zero-shot performance. In addition, since usually $\\bar{k}\\ll\\operatorname*{min}\\{n,d\\}$ , implying that the total number of parameters in LR is much smaller compared to the number of entries in W (i.e., $k(n+d)\\ll n d)$ , CALDERA can readily fine-tune (or \"adapt\") the low rank factors $\\mathbf{L}$ and $\\mathbf{R}$ in order to boost the zero-shot results. ", "page_idx": 1}, {"type": "text", "text": "1.1 Significance and Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Recent efforts have explored various avenues for compression, including but not limited to weight pruning, quantization, and the use of parameter-efficient training methods \u2013 each approach offering distinct advantages and tradeoffs. This section briefly reviews the current methodologies, highlighting the contributions and limitations of some studies closely related to this work. ", "page_idx": 1}, {"type": "text", "text": "LLM Compression and Outlier Mitigation: Recent studies like SmoothQuant [42], OPTQ [8], QuIP [3], AQLM [7], and QuIP# [36] consider the challenging regime of sub-4 bit post-training LLM quantization. These works collectively emphasize the need to manage the impact of outliers, i.e., weights with unusually high magnitudes. Accommodating outliers necessitates choosing the dynamic range (or scale) of a quantizer to be high, consequently increasing the quantization error. QuIP equalizes (and reduces) the weight matrices by using a randomized matrix transform, and subsequently, QuIP# employs E8 lattice to make the weights more amenable to vector quantization. Both QuIP and QuIP# use a refined variant of the column-wise quantization method proposed in OPTQ, wherein error feedback from previously quantized columns of a matrix is used to compensate for the error incurred while quantizing subsequent columns. CALDERA utilizes this diverse arsenal of strategies and builds on top of QuIP#, while capitalizing on the approximate low-rank structure of LLM weight matrices. While it is possible to obtain even more aggressively compressed LLMs [24], this approach requires training from scratch, which is computationally demanding. ", "page_idx": 1}, {"type": "text", "text": "Parameter Efficient Fine-Tuning (PEFT): In a related yet distinct vein of work, PEFT methods have gained significant momentum, aiming to adapt LLMs to specific tasks without extensive computational overhead. Recent studies such as QLoRA [5], LoftQ [22], and LQ-LoRA [13] have explored the intersection of PEFT and quantization, demonstrating that fine-tuning through low-rank updates, as originally proposed in LoRA [16], can mitigate the performance losses due to quantization. Given that CALDERA yields a decomposition $\\mathbf{Q}+\\mathbf{LR}$ , the low-rank components are particularly suitable for fine-tuning with any existing PEFT methods, thereby enhancing the zero-shot capabilities. ", "page_idx": 1}, {"type": "text", "text": "Low Rank Approximation: The rank- $k$ approximation of a matrix $\\mathbf{A}\\in\\mathbb{R}^{n\\times d}$ can be represented by the factorization $\\mathbf{A}\\approx\\mathbf{LR}$ , with $\\mathbf{L}\\,\\in\\,\\mathbf{\\dot{R}}^{n\\times k}$ and $\\mathbf{R}\\,\\in\\,\\mathbb{R}^{k\\times d}$ , where $k\\,\\leq\\,\\operatorname*{min}\\{n,d\\}$ . Known as the Burer-Monteiro factorization, this method substantially decreases the number of parameters, thus reducing computational demands. Recent studies such as LoRD [18], ASVD [44], FWSVD [15], LASER [33], LQER [45], and ZeroQuant-V2 [43] have explored the efficacy of low-rank structures in LLM weights, treating low-rank factorization and quantization independently. In contrast, LPLR [31] approaches this by uniquely formulating a joint optimization problem for generic matrices, while simultaneously leveraging the equalization property of randomized transforms, as in [3]. CALDERA formally leverages this inherent low-rank structure for LLM compression alongside existing frameworks such as QuIP# [36] and LoftQ [22], providing additional flexibility for compression. Furthermore, rigorous theoretical guarantees are derived using a rank-constrained regression framework for obtaining a low precision and low-rank decomposition, thereby also analytically demonstrating its superiority over rank-agnostic strategies. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2 Problem Formulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In a neural network layer, a weight matrix W transforms an input activation $\\mathbf{x}$ into an output activation given by $\\mathbf{W}\\mathbf{x}$ . This transformation can be succinctly described using the matrix\u2019s singular value decomposition (SVD). For any matrix $\\mathbf{A}\\in\\mathbb{R}^{n\\times d}$ , the SVD is $\\begin{array}{r}{{\\bf A}=\\sum_{i}\\sigma_{i}{\\bf u}_{i}{\\bf v}_{i}}\\end{array}$ , where $\\sigma_{i},\\mathbf{u}_{i},\\mathbf{v}_{i}$ are the $i^{\\mathrm{th}}$ singular value and the corresponding left and right singular vectors, respectively. The impact of each singular component $\\mathbf{u}_{i}\\mathbf{v}_{i}$ on the matrix\u2019s transformation is determined by the magnitude of $\\sigma_{i}$ . Given that weight matrices exhibit a decaying singular value profile (Fig. 1), indicating an approximate low-rank structure, lesser contributing singular components can be pruned with minimal impact on the functionality of the matrix, ensuring minimal distortion in the output activations. ", "page_idx": 2}, {"type": "text", "text": "CALDERA approximates the weight matrix of a neural network, W, as a low-precision, low-rank decomposition, $\\mathbf{W}\\approx\\mathbf{Q}+\\mathbf{L}\\mathbf{R}$ , with all components $\\mathbf{Q},\\mathbf{L},\\mathbf{R}$ in low-precision format. Unlike previous works such as [13, 22, 44, 45], which represent the low-rank factors $\\mathbf{L}$ and $\\mathbf{R}$ in highprecision (16 or 32-bit floating point), this work extends their representation to low-precision. This further reduces the memory footprint while preserving performance. Alternatively, for the same memory footprint, it allows the target rank $k$ to be higher, thereby capturing the low rank structure with higher fidelity by including more of the higher singular value components. The following paragraph formalizes this as a constrained optimization problem. ", "page_idx": 2}, {"type": "text", "text": "For a given quantizer, let $\\mathbb{Q}$ denote the set of discrete quantization points in $\\mathbb{R}$ . For B-bit quantization, the cardinality of $\\mathbb{Q}$ satisfies $\\mathrm{log}_{2}|\\mathbb{Q}|\\leq\\mathrm{B}$ . Consider a matrix $\\mathbf{W}\\in\\mathbb{R}^{n\\times d}$ . The goal of this work is to obtain an decomposition $\\mathbf{W}\\approx\\mathbf{Q}+\\mathbf{LR}$ by approximately solving the minimization problem ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{Q},\\mathbf{L},\\mathbf{R}}\\big\\|(\\mathbf{Q}+\\mathbf{L}\\mathbf{R}-\\mathbf{W})\\mathbf{X}^{\\top}\\big\\|_{\\mathrm{F}}^{2}\\quad\\mathrm{subject}\\,\\,\\mathbf{0}\\quad\\mathbf{Q}\\in\\mathbb{Q}_{\\mathrm{Q}}^{n\\times d},\\,\\,\\mathbf{L}\\in\\mathbb{Q}_{\\mathrm{L}}^{n\\times k},\\,\\,\\mathrm{and}\\,\\,\\,\\mathbf{R}\\in\\mathbb{Q}_{\\mathrm{R}}^{k\\times d}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Here, $\\mathbb{Q}_{\\mathrm{Q}},\\mathbb{Q}_{\\mathrm{L}}$ and $\\mathbb{Q}_{\\mathrm{R}}$ denote the lattice codebooks used to quantize $\\mathbf{Q},\\mathbf{L}$ and $\\mathbf{R}$ , using $\\mathrm{B_{Q},B_{L}}$ and $\\mathrm{B_{R}}$ bits, respectively. Furthermore, $\\mathbf{X}\\in\\mathbb{R}^{m\\times d}$ is a calibration matrix that aims to preserve the Frobenius norm error of the compressed layer output activations. If W is the first layer\u2019s weight matrix, $\\mathbf{X}$ includes input embeddings from a calibration dataset, such as a subset of RedPajama [34], with the $i^{\\mathrm{th}}$ row representing the $i^{\\mathrm{th}}$ datapoint. For intermediate layers, $\\mathbf{X}$ contains the input activations, which are the output activations of the preceding layer. ", "page_idx": 2}, {"type": "text", "text": "Using the Frobenius norm of the output of a layer as a proxy objective for quantizing the weight matrices of an LLM is a popular strategy, and was used in prior work of Nagel et al. [27] . This proxy objective function is particularly useful for post-training quantization of LLMs because their large size makes it difficult to apply sophisticated compression methods. ", "page_idx": 2}, {"type": "text", "text": "3 Proposed Algorithm: Calibration-Aware Low-Precision Decomposition with Low Rank Adaptation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "This section introduces CALDERA to approximately solve (1) and get a $\\mathbf{Q}+\\mathbf{LR}$ decomposition of a weight matrix W using the calibration matrix $\\mathbf{X}$ . The pseudocode is provided in Alg. 1. It consists of a nested loop for alternately optimizing the variables $\\mathbf{Q},\\mathbf{L}$ and R. Suppose $\\mathrm{Q_{Q}}$ , $\\mathrm{Q_{L}}$ and $\\mathrm{Q}_{\\mathrm{R}}$ , respectively, denote quantizers used for quantizing Q, L and R. For instance, they can refer to uniformly dithered scalar quantizers, as described in App. G.2. Initially, the low-rank factors are set to 0, and W is quantized using the LDLQ quantizer proposed in [3, $\\S3.1]$ . LDLQ is an adaptive quantization method that iteratively quantizes [8] each column of $\\mathbf{W}$ using $\\mathrm{Q_{Q}}$ to get $\\mathbf{Q}$ as ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{Q}^{(k)}=\\operatorname{Q}_{\\mathbf{Q}}(\\mathbf{W}^{(k)}+(\\mathbf{W}^{(1:k-1)}-\\mathbf{Q}^{(1:k-1)})\\mathbf{a}_{k}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathbf{Q}^{(k)},\\mathbf{W}^{(k)}$ denote the $k^{\\mathrm{th}}$ column, $\\mathbf{W}^{(1:k-1)}$ denotes the first $k$ columns, $\\mathrm{Q_{Q}}$ has a bit-budget $\\mathrm{B_{Q}}$ , and $\\mathbf{a}_{k}\\in\\mathbb{R}^{k-1}$ is a learnable sequence of vectors. Update Eq. (2) incorporates linear feedback from already quantized columns, it can be seen that $\\mathbf{Q}$ satisfies $\\mathbf{Q}\\bar{=}\\operatorname{Q}_{\\mathrm{Q}}\\left(\\mathbf{W}\\bar{+}\\left(\\mathbf{W}-\\mathbf{Q}\\right)\\mathbf{M}\\right)$ , where the feedback matrix $\\mathbf{M}$ is a strictly upper triangular matrix with columns ${\\bf a}_{k}$ . Defining $\\mathbf{H}\\triangleq{\\frac{1}{m}}\\mathbf{X}^{\\top}\\mathbf{X}$ to be the (scaled) Hessian of the least squares objective in (1), [3] show that the optimal feedback matrix is the $\\mathbf{M}$ obtained from the LDL decomposition of $m\\mathbf{H}$ , given by $m\\mathbf{H}=(\\mathbf{M}\\mathbf{\\dot{+}}\\mathbf{I})\\mathbf{D}(\\mathbf{M}\\mathbf{+}\\mathbf{I})^{\\top}$ . ", "page_idx": 3}, {"type": "text", "text": "Subsequently, $\\mathbf{Q}$ is fixed and the Low-Precision Low-Rank (LPLR) factorization of the residual, $\\left(\\mathbf{W}-\\mathbf{\\bar{Q}}\\right)$ , is computed. This is done by the LPLRFACTORIZE submodule (Alg. 2), which is a refined version of the LPLR algorithm proposed in [31]. For a given matrix A, Alg. 2 minimizes ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{L},\\mathbf{R}}\\big\\|\\big(\\mathbf{LR}-\\mathbf{A}\\big)\\mathbf{X}^{\\top}\\big\\|_{\\mathrm{F}}^{2}\\quad\\mathrm{subject}\\ \\mathbf{0}\\quad\\mathbf{L}\\in\\mathbb{Q}_{\\mathrm{L}}^{n\\times k},\\ \\mathrm{and}\\ \\mathbf{R}\\in\\mathbb{Q}_{\\mathrm{R}}^{k\\times d},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathrm{Q_{L}}$ and $\\mathrm{Q}_{\\mathrm{R}}$ use $\\mathrm{B_{L}}$ and $\\mathrm{B_{R}}$ bits, respectively. In contrast to [31], the objective in (3) is calibration-data aware. Therefore, the update equations are derived using a rank-constrained regression framework, as described in App. B. Moreover, lines 7 to 14 in LPLRFACTORIZE iteratively refine the estimates of $\\mathbf{L}$ and $\\mathbf{R}$ , and can only yield a smaller Frobenius norm error. The left and right low-rank factor update equations are described as follows. ", "page_idx": 3}, {"type": "text", "text": "Initialization: In the absence of quantization constraints, a globally optimal solution to the optimization problem (3) can be found as described later in lemma 4.2. Consequently, the low-rank factors are initialized using rank-constrained regression in lines 2 \u2013 4. Since subsequent quantization disrupts optimality of the solution, the factors are iteratively updated to minimize this distortion. ", "page_idx": 3}, {"type": "text", "text": "Updating L: To update the left factor $\\mathbf{L}$ , lines 5 and 9 of Alg. 2 solves $\\mathrm{min}_{\\mathbf{Z}}\\|({\\mathbf{Z}}{\\mathbf{R}}-{\\mathbf{A}}){\\mathbf{X}}^{\\top}\\|_{\\mathrm{F}}^{2}$ . For a fixed $\\mathbf{R}$ , this is a least squares minimization, whose solution is available is closed form as $\\mathbf{\\hat{L}}=\\left(\\mathbf{AX^{\\top}}\\right)\\left(\\mathbf{RX^{\\top}}\\right)^{\\dagger}=\\mathbf{AHR^{\\top}}(\\mathbf{RHR^{\\top}})^{-1}$ , as derived in App. C.1. ", "page_idx": 3}, {"type": "text", "text": "Updating R: Line 8 of Alg. 2, updates the right factor $\\mathbf{R}$ by keeping $\\mathbf{L}$ fixed and solving $\\mathrm{min}_{\\mathbf{Z}}\\|(\\bar{\\mathbf{L}}\\bar{\\mathbf{Z}}-\\mathbf{A})\\mathbf{X}^{\\top}\\|_{\\mathrm{F}}^{2}$ . As this is an under-determined linear system, there exist multiple solutions for $\\mathbf{Z}$ , all attaining the same objective function value. It is shown in App. C.1 that $\\dot{\\mathbf{R}}=\\mathbf{L}^{\\dagger}\\mathbf{A}\\mathbf{H}\\mathbf{H}^{\\dagger}$ is a solution. The corresponding error is also obtained, which is used in the derivation of Thm. 4.1. ", "page_idx": 3}, {"type": "text", "text": "Computational Complexity: A high-level calculation is provided here, and detailed discussions can be found in App. D. It is worthwhile to note that the closed form expressions of $\\grave{\\mathbf{L}}$ and $\\grave{\\mathbf{R}}$ , which are iteratively quantized, are functions of the Hessian $\\begin{array}{r}{\\mathbf{H}=\\frac{1}{m}\\mathbf{X}^{\\top}\\mathbf{X}}\\end{array}$ . Therefore, $\\mathbf{H}$ can be computed offline initially, per LLM, by doing a single forward pass, and subsequently used for all model quantization experiments. For each layer, this pre-processing includes computing $\\mathbf{H}$ and its LDL decomposition, along with computing $\\dot{\\mathbf{H}}\\mathbf{H}^{\\dagger}$ , requiring a total of $\\mathrm{O}(m d^{2}+2d^{3})$ multiplications. Each outer iteration involves an LDLQ quantization. Quantizing the $k^{\\mathrm{th}}$ column has complexity $\\mathrm{O}(n k)$ , since feedback from $k$ already quantized columns need to be incorporated. Hence, quantizing a matrix in $\\mathbb{R}^{n\\times d}$ entails $\\mathrm{O}(n^{2}+3n)$ complexity. Moreover, LPLRFACTORIZE requires $\\mathrm{O}(m^{2}(\\stackrel{-}{n}+d))$ to initialize, and subsequently, each inner iteration entails $\\mathrm{O}(n d k)$ . Assuming $n,d\\ge m\\gg k$ , and keeping only the dominant terms, the total complexity of CALDERA, not including the complexity of the pre-processing discussed earlier, is O $\\left(\\mathrm{T_{out}^{\\bar{\\;}}}\\left(\\bar{n^{2}}+m^{2}(n+d)+n d k\\mathrm{\\;T_{in}}\\right)\\right)$ . ", "page_idx": 3}, {"type": "text", "text": "Fine tuning via Low-Rank Adaptation: Once the weight matrices of each layer are replaced by its $\\mathbf{Q}+\\mathbf{LR}$ approximation, the zero-shot performance of (post-training) quantized model can be evaluated. $\\S5$ shows that CALDERA quantized models outperform existing strategies. Additionally, if desired, the low-rank factors $\\mathbf{L}$ and $\\mathbf{R}$ can be further fine-tuned using low-rank adaptation [13, 16, 22] on a small task-specific dataset. While the initialization of the fine-tuning step has quantized $\\mathbf{Q},\\mathbf{L}$ and $\\mathbf{R}$ , the fine-tuned factors are represented using 16-bits (BF16 format). Although this leads to a slight increase in the memory footprint, the performance gains from fine-tuning are substantial. ", "page_idx": 3}, {"type": "text", "text": "4 Approximation Error Analysis ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The approximation error upper bounds are derived via a rank-constrained regression framework. Thm. 4.1 below (formally stated and proved in App. C.4) is an informal version of the main theoretical result of this paper, and provides an upper bound on the Frobenius norm error when CALDERA approximates a weight matrix W is as $\\mathbf{W}\\approx\\mathbf{Q}+\\mathbf{LR}$ by solving the optimization problem (1) using Alg. 1. For convenience of analysis, it is assumed that the dynamic range of $\\mathrm{Q_{Q}}$ , denoted as $\\mathrm{R}$ , is chosen to be high enough, ensuring it remains unsaturated. Consequently, for a scalar input the quantization error from QQ has zero mean and bounded variance, given by \u220642 =(2BQR2\u22121)2 . ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Theorem 4.1. Approximation error of CALDERA (Informal) Given $\\mathbf{W}\\in\\mathbb{R}^{n\\times d}$ and $\\mathbf{X}\\in\\mathbb{R}^{m\\times d}$ with $m\\leq d$ , let $\\mathbf{D}$ be obtained from the LDL decomposition $\\mathbf{X}^{\\top}\\mathbf{X}=m\\mathbf{H}=(\\mathbf{M}+\\mathbf{I})\\mathbf{D}(\\mathbf{M}+\\mathbf{I})^{\\top}$ , and $\\lambda_{\\mathrm{max}}$ , $\\lambda_{\\mathrm{min}}$ denote the max and min eigenvalues of H. Additionally, let $\\mathbf{Q}\\triangleq\\mathrm{LDLQ}(\\mathbf{W},\\mathrm{Q}_{\\mathrm{Q}})$ , where $\\mathrm{Q_{Q}}$ has dynamic range $\\mathrm{R}$ and bit-budget $\\mathrm{B_{Q}}$ , the quantization error be $\\eta\\triangleq\\mathrm{Q}_{\\mathrm{Q}}(\\mathbf{Q}+(\\mathbf{W}-\\mathbf{\\Lambda}$ $\\mathbf{Q})\\mathbf{M})-(\\mathbf{Q}+(\\mathbf{W}-\\mathbf{Q})\\mathbf{M})$ , and $\\sigma_{1}\\ge...\\ge\\sigma_{k}$ . . . be the singular values of $\\mathbf{X}(\\mathbf{W}-\\mathbf{Q})^{\\top}$ . If the taanrdg aanrke $k$ est aatiss $\\begin{array}{r}{0.25\\lambda_{\\mathrm{min}}^{1/2}(m\\sigma_{1})^{-1}\\lambda_{\\mathrm{max}}^{-3/2}\\sum_{i>k}\\sigma_{i}^{2}\\leq k\\leq m}\\end{array}$ ,  arentdu rtnhee dd byyn aAmlgi.c  1r asnagtiessf yof $\\mathrm{Q_{L}}$ QR   s  RL =\u03c3k\u221a2m\u03c31\u03bbmin $\\mathrm{R}_{\\mathrm{R}}=\\sigma_{1}$ $\\mathbf{Q},\\mathbf{L}$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{1}{n m}\\operatorname{\\mathbb{E}}\\left\\|(\\mathbf{Q}+\\mathbf{LR}-\\mathbf{W})\\mathbf{X}^{\\top}\\right\\|_{\\mathrm{F}}^{2}\\leq\\frac{1}{n}\\sum_{i>k}\\operatorname{\\mathbb{E}}\\lambda_{i}(\\eta\\mathbf{D}\\eta^{\\top})+\\epsilon\\lesssim\\frac{4d\\lambda_{\\operatorname*{max}}\\mathrm{R}^{2}}{\\pi\\left(2^{\\mathrm{B}_{\\mathrm{Q}}}-1\\right)^{2}}\\left(1-\\frac{k}{2n}\\right)^{2}+\\epsilon,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "while utilizing an average budget of 12 log2 mk\u03f5\u03c3\u03c31k\u03bb\u03bbmmianx bits per parameter for the low-rank factors L and $\\mathbf{R}$ , when $n\\approx d$ . Here, the expectation is over the stochasticity of the quantizers. ", "page_idx": 4}, {"type": "text", "text": "An informal version of the main result is provided here, and the formal version including specific constant values, along with the derivation, can be found in App. C.4. The requirement $m\\leq d$ is not restrictive, because when $\\mathbf{H}$ is positive definite, (1) can be rewritten as $\\|(\\bar{\\mathbf{Q}^{+}}\\mathbf{L}\\mathbf{R}-\\mathbf{W})\\mathbf{X}^{\\top}\\|_{\\mathrm{F}}^{2}=$ $\\lVert(\\mathbf{Q}+\\mathbf{LR}-\\mathbf{W})\\mathbf{H}^{1/2}\\rVert_{\\mathrm{F}}^{2}$ , ensuring $m=d$ . This is detailed further in App. C.5. The approximation error upper bound given by Thm. 4.1 can be directly compared with the result of Chee et al. [3, Thm. 1], which states that for vanilla LDLQ without LPLRFACTORIZE, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left\\|(\\mathbf{Q}-\\mathbf{W})\\mathbf{X}^{\\top}\\right\\|_{\\mathrm{F}}^{2}\\leq\\mathbb{E}\\left[\\mathrm{Tr}\\left(\\eta\\mathbf{D}\\eta^{\\top}\\right)\\right]=\\sum_{i=1}^{n}\\mathbb{E}\\lambda_{i}(\\eta\\mathbf{D}\\eta^{\\top}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Evidently, Alg. 1 yields a smaller error provided, $\\begin{array}{r}{\\sum_{i>k}\\mathbb{E}\\lambda_{i}(\\eta\\mathbf{D}\\eta^{\\top})<\\sum_{i=1}^{k}\\mathbb{E}\\lambda_{i}(\\eta\\mathbf{D}\\eta^{\\top})-\\epsilon,}\\end{array}$ where $\\epsilon$ can be chosen to be arbitrarily small. Furthe rmore, since the expres sion in Thm. 4.1 consists of two terms, namely, the rank-constrained regression error, which depends on the target rank $k$ , and the additive quantization error of $\\epsilon$ , which is dictated by the bit-budgets used for $\\mathbf{L}$ and $\\mathbf{R}$ , this upper ", "page_idx": 4}, {"type": "text", "text": "Algorithm 1: CALDERA: Calibration Aware Low-Precision DEcomposition with Low-Rank Adaptation ", "page_idx": 4}, {"type": "text", "text": "Input: Matrix: $\\mathbf{W}\\in\\mathbb{R}^{n\\times d}$ , Target rank: $k$ , Calibration matrix: $\\mathbf{X}\\in\\mathbb{R}^{m\\times d}$ , Outer and inner iterations: $\\mathrm{T_{out}}$ , $\\mathrm{T_{in}}$ , Quantizers: $\\mathrm{Q_{Q}}$ , $\\mathrm{Q_{L}}$ , $\\mathrm{Q}_{\\mathrm{R}}$ , Flag: EnableLoRA, Fine-tune rank: $r$ ", "page_idx": 4}, {"type": "text", "text": "Output: LPLR decomposition: $\\mathbf{Q}\\in\\mathbb{Q}_{\\mathrm{Q}}^{n\\times d}$ , $\\mathbf{L}\\in\\mathbb{Q}_{\\mathrm{L}}^{n\\times k}$ , $\\mathbf{R}\\in\\mathbb{Q}_{\\mathrm{R}}^{k\\times d}$ s.t. $\\mathbf{W}\\mathbf{X}^{\\top}\\approx(\\mathbf{Q}+\\mathbf{L}\\mathbf{R})\\mathbf{X}^{\\top}$ ", "page_idx": 4}, {"type": "text", "text": "1 Initialize: $t\\gets0$ , $\\mathbf{L}_{0}\\gets\\mathbf{0}$ , $\\mathbf R_{0}\\gets\\mathbf0$ , MinError $\\leftarrow\\infty$   \n2 while $t<\\mathrm{T_{out}}$ do   \n3 Update $\\mathbf{Q}$ $\\mathfrak{2:}\\,\\mathbf{Q}_{t+1}\\gets\\mathrm{LDLQ}(\\mathbf{W}-\\mathbf{L}_{t}\\mathbf{R}_{t},\\mathrm{Q}_{\\mathrm{Q}})$   \n4 Update low-rank factors:   \n5 i $\\begin{array}{r l}&{\\mathbf{L}_{t+1},\\mathbf{R}_{t+1}\\leftarrow\\mathrm{LPLRFACTORIZE}(\\mathbf{W}-\\mathbf{Q}_{t+1},k,\\mathbf{X},\\mathbf{Q}_{\\mathrm{L}},\\mathbf{Q}_{\\mathrm{R}},\\mathrm{T}_{\\mathrm{in}})}\\\\ &{\\mathbf{f}\\parallel\\!(\\mathbf{Q}_{t+1}+\\mathbf{L}_{t+1}\\mathbf{R}_{t+1}-\\mathbf{W})\\mathbf{X}^{\\top}\\!\\parallel_{\\mathrm{F}}^{2}<\\mathrm{MinError~\\mathbf{then}}}\\\\ &{\\left\\vert\\begin{array}{r l}{\\mathbf{Q}_{\\mathrm{best}}\\leftarrow\\mathbf{Q}_{t+1},\\mathbf{L}_{\\mathrm{best}}\\leftarrow\\mathbf{L}_{t+1},\\mathbf{R}_{\\mathrm{best}}\\leftarrow\\mathbf{R}_{t+1},}\\\\ {\\mathrm{,~MinError~}\\!\\leftarrow\\!\\Vert\\!(\\mathbf{Q}_{t+1}+\\mathbf{L}_{t+1}\\mathbf{R}_{t+1}-\\mathbf{W})\\mathbf{X}^{\\top}\\Vert_{\\mathrm{F}}^{2}}\\end{array}\\right.}\\end{array}$   \n6   \n7 end   \n8 t t + 1   \n9 end ", "page_idx": 4}, {"type": "text", "text": "12 end ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "13 return Qbest, Lbest, Rbest ", "page_idx": 4}, {"type": "text", "text": "Input: Matrix: $\\mathbf{A}\\in\\mathbb{R}^{n\\times d}$ , Target rank: $k$ , Calibration matrix: $\\mathbf{X}\\in\\mathbb{R}^{m\\times d}$ , Iterations: $\\mathrm{T_{in}}$ ,   \nQuantizers: $\\mathrm{Q_{L}}$ , $\\mathrm{Q}_{\\mathrm{R}}$   \nOutput: Low precision Low Rank factors: $\\mathbf{L}\\in\\mathbb{Q}^{n\\times k}$ , $\\mathbf{R}\\in\\mathbb{Q}^{k\\times d}$ s.t. $\\mathbf{A}\\mathbf{X}^{\\top}\\approx\\mathbf{L}\\mathbf{R}\\mathbf{X}^{\\top}$   \n1 Initialize: Iteration counter: $i\\gets0$   \n2 Compute SVD of $\\mathbf{X}$ as $\\mathbf{U}\\widetilde{\\boldsymbol{\\Sigma}}\\mathbf{V}^{\\top}$ .   \n3 Compute SVD of U\u22a4XA\u22a4as \\`U \\`\u03a3 V\\`\u22a4   \n4 Get right low-rank factor: $\\mathbf{R}_{0}\\gets\\mathrm{Q}_{\\mathrm{R}}(\\mathbf{I}_{k}^{\\top}\\dot{\\Sigma}\\dot{\\mathbf{V}}^{\\top})$   \n5 Get left low-rank factor: $\\mathbf{L}_{0}\\triangleq\\mathrm{Q}_{\\mathrm{L}}(\\dot{\\mathbf{L}}_{0})$ , where $\\begin{array}{r}{\\dot{\\mathbf{L}}_{0}=\\arg\\operatorname*{min}_{\\mathbf{Z}\\in\\mathbb{R}^{k\\times d}}\\left\\|(\\mathbf{Z}\\mathbf{R}_{0}-\\mathbf{A})\\mathbf{X}^{\\top}\\right\\|_{\\mathrm{F}}^{2}}\\end{array}$   \n6 $\\mathbf{L}_{\\mathrm{best}}\\leftarrow\\mathbf{L}_{0}$ , $\\mathbf{R}_{\\mathrm{best}}\\leftarrow\\mathbf{R}_{0}$ , MinError $\\leftarrow\\left\\|(\\mathbf{L}_{0}\\mathbf{R}_{0}-\\mathbf{A})\\mathbf{X}^{\\top}\\right\\|_{\\mathrm{F}}^{2}$ .   \n7 while $i<\\mathrm{T}_{\\mathrm{in}}$ do   \n8 Update right: $\\mathbf{R}_{i+1}\\leftarrow\\mathrm{Q}_{\\mathrm{R}}(\\dot{\\mathbf{R}}_{i+1})$ , where $\\begin{array}{r}{\\dot{\\mathbf{R}}_{i+1}=\\operatorname*{arg\\min}_{\\mathbf{Z}\\in\\mathbb{R}^{k\\times d}}\\left\\|\\left(\\mathbf{L}_{i}\\mathbf{Z}-\\mathbf{A}\\right)\\mathbf{X}^{\\top}\\right\\|_{\\mathrm{F}}^{2}}\\end{array}$   \n9 Update left: $\\mathbf{L}_{i+1}\\leftarrow\\mathrm{Q}_{\\mathrm{L}}(\\dot{\\mathbf{L}}_{i+1})$ , where $\\begin{array}{r}{\\dot{\\mathbf{L}}_{i+1}=\\operatorname*{arg\\,min}_{\\mathbf{Z}\\in\\mathbb{R}^{n\\times k}}\\left\\|\\left(\\mathbf{Z}\\mathbf{R}_{i}-\\mathbf{A}\\right)\\mathbf{X}^{\\top}\\right\\|_{\\mathrm{F}}^{2}}\\end{array}$   \n10 if $\\left\\|(\\mathbf{L}_{i+1}\\mathbf{R}_{i+1}-\\mathbf{A})\\mathbf{X}^{\\top}\\right\\|_{\\mathrm{F}}^{2}<$ MinError then   \n11 $\\begin{array}{r l}{\\mathrm{|}}&{{}\\mathbf{L}_{\\mathrm{best}}\\leftarrow\\mathbf{L}_{i+1}}\\end{array}$ , $\\mathbf{R}_{\\mathrm{best}}\\leftarrow\\mathbf{R}_{i+1}$ , MinError \u2190 (Li+1Ri+1 \u2212A)X\u22a4 2   \n12 end   \n13 i \u2190i + 1   \n14 end   \n15 return Lbest, Rbest ", "page_idx": 5}, {"type": "text", "text": "bound can be made arbitrarily small by ensuring that the two terms are approximately equal, i.e., $\\mathbb{E}\\|(\\mathbf{Q}+\\mathbf{L}\\mathbf{R}-\\mathbf{W})\\mathbf{X}^{\\top}\\|_{\\mathrm{F}}^{2}$ is upper bounded by $2\\epsilon$ . This is apparent in the following regimes: ", "page_idx": 5}, {"type": "text", "text": "(i) $k\\ll n$ : In this regime, $k$ is treated as a constant as $n$ grows. Then, if the bit-budget $\\mathrm{B_{Q}}$ satisfies $\\begin{array}{r}{\\mathrm{B}_{\\mathrm{Q}}\\geq\\log_{2}\\left(2\\mathrm{R}(\\pi\\epsilon)^{-1/2}\\sqrt{n m d\\lambda_{\\mathrm{max}}}+1\\right),\\quad\\mathrm{then}\\quad\\mathbb{E}\\left\\|(\\mathbf{Q}+\\mathbf{L}\\mathbf{R}-\\mathbf{W})\\mathbf{X}^{\\top}\\right\\|_{\\mathrm{F}}^{2}\\leq2\\epsilon.}\\end{array}$ (ii) $k=\\mathrm{O}(n)$ : For a fixed $\\mathrm{B_{Q}}$ , if $k$ is allowed to grow with dimension $n$ , then choosing $k$ to satisfy $k\\geq2n-(2^{\\mathrm{B}_{\\mathrm{Q}}}-1)\\mathrm{R}^{-1}(\\pi\\epsilon)^{1/2}(m d\\lambda_{\\operatorname*{max}}^{-1/2})\\sqrt{n}\\quad\\mathrm{ensures}\\quad\\mathbb{E}\\left\\|(\\mathbf{Q}+\\mathbf{LR}-\\mathbf{W})\\mathbf{X}^{\\top}\\right\\|_{\\mathrm{F}}^{2}\\leq2\\epsilon.$ ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "This implies that the upper bound can be made arbitrarily small by either (i) increasing the bit-budget of the backbone, i.e., $\\mathrm{B_{Q}}$ , for a fixed rank $k$ , or (ii) increasing the rank $k$ for a fixed $\\mathrm{B_{Q}}$ , for example, $\\mathrm{B_{Q}}=2$ . Alternatively stated, this provides a tunable knob for controlling the error by trading off the allocated bit-budget between the backbone $\\mathbf{Q}$ and the low-rank factors $\\mathbf{L},\\mathbf{R}$ . ", "page_idx": 5}, {"type": "text", "text": "4.1 Analysis Outline ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, a brief proof sketch is presented, highlighting the major challenges in the proof and how they are addressed. For analysis, $\\mathbf{Q}$ is assumed to be updated prior to $\\mathbf{L},\\mathbf{R}$ in Alg. 1. However, in practice, the update order is inconsequential, and can be swapped, depending on whichever yields a smaller error. The complete derivation of the approximation error is provided in App. C. A key ingredient of the proof is the solution of the rank-constrained regression problem, which is defined as, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathrm{rank}(\\mathbf{Z})\\leq k}\\left\\|\\mathbf{X}\\mathbf{Z}-\\mathbf{Y}\\right\\|_{\\mathrm{F}}^{2}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Although this problem is non-convex, it can be solved to global optimality via two SVDs [41]. The following lemma characterizes the solution to the optimization problem in (5). ", "page_idx": 5}, {"type": "text", "text": "Lemma 4.2. Given $\\mathbf{Y}\\in\\mathbb{R}^{m\\times n}$ , and full rank $\\mathbf{X}\\in\\mathbb{R}^{m\\times d}$ , where $m\\leq d$ . Let $\\mathbf{X}=\\mathbf{U}\\widetilde{\\Sigma}\\mathbf{V}^{\\top}$ and $\\mathbf{U}^{\\top}\\mathbf{Y}=\\dot{\\mathbf{U}}\\dot{\\Sigma}\\dot{\\mathbf{V}}^{\\top}$ denote full SVDs of $\\mathbf{X}$ and $\\mathbf{U}^{\\top}\\mathbf{Y}$ . Then, for $k\\leq m$ , the solution of (5) is given by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{Z}_{*}\\triangleq\\underset{\\mathrm{rank}(\\mathbf{Z})\\leq k}{\\arg\\operatorname*{min}}\\ \\Vert\\mathbf{X}\\mathbf{Z}-\\mathbf{Y}\\Vert_{\\mathrm{F}}^{2}=\\left(\\mathbf{V}\\mathbf{I}_{m}\\boldsymbol{\\Sigma}^{-1}\\dot{\\mathbf{U}}\\mathbf{I}_{k}\\right)\\left(\\mathbf{I}_{k}^{\\top}\\dot{\\mathbf{\\Sigma}}\\mathbf{\\dot{V}}^{\\top}\\right),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\pmb{\\Sigma}:=\\widetilde{\\pmb{\\Sigma}}\\mathbf{I}_{m}\\in\\mathbb{R}^{m\\times m}$ is a diagonal matrix consisting of the non-zero singular values of X. Moreover, denoting the non-zero singular values of $\\mathbf{Y}$ as $\\bar{\\{\\sigma_{i}({\\bf Y})\\}}_{i=1}^{m}$ , the optimal value of (7) is ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathrm{rank}\\left(\\mathbf{Z}\\right)\\leq k}\\left\\|\\mathbf{X}\\mathbf{Z}-\\mathbf{Y}\\right\\|_{\\mathrm{F}}^{2}=\\left\\|\\mathbf{X}\\mathbf{Z}_{*}-\\mathbf{Y}\\right\\|_{\\mathrm{F}}^{2}=\\sum_{i=k+1}^{m}\\sigma_{i}^{2}\\left(\\mathbf{Y}\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The complete lemma (with the case $m\\,>\\,d,$ ), and the derivation, are provided in App. B. Using lemma 4.2, the approximation error of LPLRFACTORIZE is analyzed in App. C.3. Specifically, lemma C.3 shows that for any input matrix A, Alg. 2 with suitably chosen $\\mathrm{B_{L}}$ and $\\mathrm{B_{R}}$ , ensures that $\\mathbb{E}\\left\\|(\\mathbf{LR}-\\mathbf{A})\\mathbf{X}^{\\top}\\right\\|_{\\mathrm{F}}^{2}$ , as in (3), can be upper bounded by twice the sum of squared trailing singular values, (ref. (6)). While proving lemma C.3, it is assumed that if $\\mathrm{Q_{L}}$ or $\\mathrm{Q}_{\\mathrm{R}}$ gets saturated, a trivial output of $\\mathbf{L}=\\mathbf{0},\\mathbf{R}=\\mathbf{0}$ is returned. Therefore, lemmas C.1 and C.2 specify choosing the dynamic ranges $\\mathrm{R}_{\\mathrm{R}}$ and $\\mathrm{R_{L}}$ to be sufficiently high so that saturation happens with a very low probability. The proof of Thm. 4.1 is completed by using the LDL decomposition of $m\\mathbf{H}$ as proposed in [3], along with an application of Marchenko-Pastur approximation to bound the expected eigenvalues of the quantization error, i.e., $\\mathbb{E}\\lambda_{i}\\left(\\eta\\eta^{\\top}\\right)$ , yielding the final inequality. ", "page_idx": 6}, {"type": "text", "text": "5 Numerical Simulations ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The efficacy of CALDERA is assessed by using it to compress four popular open source LLMs from Meta AI, namely, LLaMa-2 7B, LLaMa-2 13B, LLaMa-2 70B [35] and LLaMa-3 8B [26]. The framework is built in PyTorch on top of the QuIP# [36] and LoftQ [22], and is available at https://github.com/pilancilab/caldera. ", "page_idx": 6}, {"type": "text", "text": "Baselines. The full-rank matrix $\\mathbf{Q}$ , also referred to as the backbone, is quantized to 2-bits using the LDLQ procedure from QuIP [3, 36], employing an E8 lattice quantizer [39]. For CALDERA, which allows even the low-rank factors, $\\mathbf{L}$ and $\\mathbf{R}$ , to be represented in low-precision, the quantization is also performed with an E8 lattice. Prior to running Alg. 1, a randomized Hadamard transform (RHT) is applied to the left and the right of the input weight matrix, as the incoherence pre-processing step, to equalize the magnitude of the entries making them more robust to quantization. In other words, CALDERA decomposition is performed on $\\widetilde{\\mathbf{W}}\\triangleq\\mathbf{H}_{\\mathrm{L}}^{\\top}\\mathbf{W}\\mathbf{H}_{\\mathrm{R}}$ , where ${\\bf H}_{\\mathrm{L}}$ and ${\\bf{H}}_{\\mathrm{{R}}}$ are Hadamard matrices, right-multiplied by a diagonal matrix with i.i.id. $\\{\\pm1\\}$ entries. In addition, the Hessian matrix obtained from the calibration data is substituted by $\\widetilde{\\mathbf{H}}\\triangleq\\mathbf{H}_{\\mathrm{R}}^{\\top}\\mathbf{H}\\mathbf{H}_{\\mathrm{R}}$ . As described in [3], this improves the quantization error incurred by LDLQ. Furthe r details are provided in App. E.2. ", "page_idx": 6}, {"type": "text", "text": "Metrics. The performance of CALDERA is evaluated using perplexity on the test splits of the Wikitext2 [25] and C4 [6] datasets, as well as task-specific goodness-of-fit metrics such as zeroshot accuracy for sequence classification. Specifically, zero-shot accuracy was measured on the Winogrande [19], RTE [1, 40], PiQA [2], ARC-Easy, and ARC-Challenge [4] tasks. App. E.3 provides more details regarding these benchmarks. Perplexity was measured using a sequence length equal to the model\u2019s maximum context length, i.e., 4096 for LLaMa-2, and 8192 for LLaMa-3. Zero-shot experiments were performed using EleutherAI\u2019s Language Model Evaluation Harness [9]. ", "page_idx": 6}, {"type": "text", "text": "5.1 Zero-shot Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Tables 1 and 2 report the perplexities and accuracies for CALDERA with varying target rank $(k)$ of $\\mathbf{L}$ and R. A smaller value is better for perplexity, which is defined as the $\\exp(\\cdot)$ of the training objective, while zero-shot accuracies are reported as percentages. Per-parameter bit budgets range from 2.1 (e.g., rank-64 factors in 4-bit precision) to 2.4 bits (e.g., rank-64 factors in half precision or rank-256 factors in 4-bit precision). For comparison, the $\\mathbf{Q}+\\mathbf{LR}$ decomposition of weight matrices found in the QuIP# codebase was performed on each model. For the sake of direct comparison, fine-tuning of the diagonal matrices in RHT was omitted. As QuIP# does not support quantized factors, $\\mathbf{L}$ and $\\mathbf{R}$ are rank-64 in order to ensure that the per-parameter bit-budget remains in the $2-2.4$ range. As another baseline comparison, each model is quantized using QuIP# without any low-rank factors. Results for the unquantized models are also provided. ", "page_idx": 6}, {"type": "text", "text": "For all models, the rank-256 CALDERA decomposition with 4-bit factors had the lowest perplexity and generally had the highest accuracies. As CALDERA supports quantizing low-rank factors with minimal performance loss, more singular components can be captured compared to using half-precision factors while employing the same number of bits. Consequently, the low-rank factors can regain the performance that was compromised when the backbone $\\mathbf{Q}$ was quantized to 2 bits. Since zero-shot experiments have some inherent randomness and low-rank regularization effects [33], the zero-shot accuracies reported here are not as directly indicative of quantization performance as the perplexity results. In addition, $\\S5.3$ , demonstrates that degradation in zero-shot accuracy can be recovered via LoRA fine-tuning. It is worthwhile to note these results substantiate the claims of [17], which report that low-bit quantization of LLaMa-3 8B, significantly deteriorates model performance across various post-training quantization techniques, more so than with the LLaMa-2 series. ", "page_idx": 6}, {"type": "table", "img_path": "lkx3OpcqSZ/tmp/f1fca3acc849bacad370277e5e6d468888a5de12f0d03b51c4168f1105b476ef.jpg", "table_caption": ["Table 1: Zero-shot perplexities (denoted by $\\downarrow$ ) and accuracies $(\\uparrow)$ for LLaMa-2. $\\mathrm{BQ}=2$ bits throughout. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "lkx3OpcqSZ/tmp/3859b070a94256e45a8819f0e3e73855c79c842359b0f1f1585145b98b9533ec.jpg", "table_caption": ["Table 2: Zero-shot perplexities (denoted by \u2193) and accuracies (\u2191) for LLaMa-3 8B. $\\mathrm{B_{Q}}=2$ bits throughout. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "5.2 Fine-tuning of Randomized Hadamard Transform (RHT) Parameters ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "As CALDERA presents a general optimization framework for matrix decompositions of the form $\\mathbf{Q}+\\mathbf{LR}$ , it can easily be extended with additional heuristics to improve performance. This section serves as a proof of concept, by examining one such heuristic: Fine-tuning of randomized Hadamard transform parameters. This technique, proposed in QuIP# [36], involves fine-tuning the diagonal Rademacher matrices with $\\pm1$ entries in the RHT to minimize the cross-entropy loss between the output of the original and quantized models on the calibration dataset. Subsequently, RHT fine-tuning is performed on the models quantized using CALDERA in $\\S5.1$ .1 Details on specific fine-tuning hyperparameters can be found in App. E.4. ", "page_idx": 7}, {"type": "text", "text": "Perplexity and zero-shot results in Tables 3 and 4 match the trends in $\\S5.1$ , i.e., CALDERA with rank-256 factors typically performs best, with the exception of RTE. In addition, perplexities are substantially lower than without the fine-tuning of randomized Hadamard transform parameters. ", "page_idx": 7}, {"type": "table", "img_path": "lkx3OpcqSZ/tmp/9b7c016f29d64833299d2fb2c1114d36119507206434283b1d1b0ca7efed1151.jpg", "table_caption": ["Table 3: Zero-shot perplexities and accuracies for LLaMa-2 7B, with end-to-end fine-tuning of randomized Hadamard transform parameters. $\\mathrm{B_{Q}}=2$ bits throughout. \\*See Footnote 1. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "lkx3OpcqSZ/tmp/8b3da7e589a38d3cfb3c3bb02e6de43260b0c3abb50677f1e3f1c9a979f69648.jpg", "table_caption": ["Table 4: Zero-shot perplexities and accuracies for LLaMa-3 8B, with end-to-end fine-tuning of randomized Hadamard transform parameters. $\\mathrm{B_{Q}}=2$ bits throughout. \\*See Footnote 1. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.3 Low Rank Adaptation (LoRA) Fine-tuning Results ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In addition to RHT FT as described above, once the $\\mathbf{Q}+\\mathbf{LR}$ decomposition with target rank $k$ is obtained, and $k$ takes values 64, 128 and 256, fine-tuning the top $r$ $(\\leq k)$ singular components on a specific downstream datasets can recover the performance lost due to quantization. We consider three such tasks \u2013 (i) language modeling on Wikitext (Wiki2), (ii) recognizing textual entailment (RTE), and (iii) commonsense reasoning (WinoGrande). Throughout all experiments in Table 5, $r=64$ is chosen and those singular components are fine-tuned to 16-bit precision, i.e., BF16 format. The tasks (ii) and (iii) are sequence classification tasks, and the pre-trained LLaMa model is augmented with a linear classification head, which is fine-tuned along with the low-rank factors [29]. In other words, the approximation is written as $\\mathbf{W}\\approx\\mathbf{Q}+\\mathbf{L}_{1}\\mathbf{R}_{1}+\\mathbf{L}_{2}\\mathbf{R}_{2}$ , where $\\mathbf{L}_{1}\\,\\in\\,\\mathbb{R}^{n\\times r}$ , $\\mathbf{L}_{2}\\,\\in\\,\\mathbb{R}^{n\\times(k-r)}$ , $\\mathbf{R}_{1}\\in\\mathbb{R}^{r\\times d}$ , $\\mathbf{R}_{2}\\in\\mathbb{R}^{(k-r)\\times d}$ , $\\mathbf{L}=\\left[\\mathbf{L}_{1}\\ |\\ \\mathbf{L}_{2}\\right]$ , $\\mathbf{R}^{\\top}=[\\mathbf{R}_{1}^{\\top}\\mid\\mathbf{R}_{2}^{\\top}]$ . The value of $r$ is set to 64 and $\\mathbf{L}_{2},\\mathbf{R}_{2}$ are fined-tuned to $\\mathbf{L}_{\\mathrm{bf16}}$ , $\\mathbf{R}_{\\mathrm{bfl6}}$ using low-rank adaptation similar to [13, 16, 22]. Doing this significantly on a small task-specific dataset like WikiText2, RTE, or Winogrande, can noticeably boost the zero-shot accuracy, as can be seen from Table 5.2 ", "page_idx": 8}, {"type": "text", "text": "Experimental details can be found in App. E.4. For each dataset, ten checkpoints are saved during the course of fine-tuning, and the best test performance is reported in Table 5. For datasets where test labels are not available, evaluation performance is reported instead. ", "page_idx": 8}, {"type": "text", "text": "For comparison, results from the LoftQ [22] and LQ-LoRA [13] papers are also reported, where available. As these papers were published before the release of LLaMa-3, only LLaMa-2 results are available.3 In each case, CALDERA achieves better performance at a lower bit budget. ", "page_idx": 8}, {"type": "text", "text": "5.4 Autoregressive Generation Throughput ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The low-rank (LR) component in CALDERA can recover some of the accuracy lost due to the aggressive 2-bit quantization of $\\mathbf{Q}$ . However, CALDERA also needs to dequantize and multiply the low-rank factors, which results in a slight (albeit, acceptable) throughput degradation compared to QuIP# (shown in Table 6). Nevertheless, CALDERA\u2019s throughput is significantly higher than that of the unquantized model. This is because compressing weight matrices results in a smaller volume of data transfer from and to the GPU\u2019s SRAM, speeding up forward passes. It is worthwhile to note that ", "page_idx": 8}, {"type": "table", "img_path": "lkx3OpcqSZ/tmp/807d93c1c56926cbd406fa4f93935b0aab052c88f1b759103b79163b88b045b6.jpg", "table_caption": ["Table 5: CALDERA fine-tuning results for LLaMa-2 7B and LLaMa-3 8B. $\\mathrm{B_{L}}$ , $\\mathrm{B_{R}}$ are the bit-budgets of L and R for the low-rank initialization. Rank-64 fine-tuned factors are represented in BF16 precision. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Llama-2 70B runs into out-of-memory (OOM) as an A10G GPU only has $24\\:\\mathrm{GiB}$ of VRAM, which is not enough for 70B parameters in FP16 format (which approximately requires $140\\,\\mathrm{GiB})$ ). ", "page_idx": 9}, {"type": "text", "text": "Notably, the throughput is higher when the LR factors are in 16-bit compared to when they are in 4-bit. This is because CALDERA used QuIP#\u2019s lattice dequantizers for the low-rank factors as well, adding to the compute overhead. Moreover, QuIP# also used fused kernels, and CALDERA\u2019s throughput can be improved by leveraging such optimizations. Since this work is primarily motivated with the goal of closing the gap with respect to uncompressed models in the 2 to ", "page_idx": 9}, {"type": "table", "img_path": "lkx3OpcqSZ/tmp/531a22c34313d4b7a97fac1a589c5b16ae066117510c35ec4fd2a3c85f1351fd.jpg", "table_caption": ["Table 6: Throughputs for meta-llama/Llama-2-{7,70}b-hf on an NVIDIA A10G GPU for a batch size and sequence length of 1 $\\mathrm{B_{Q}}=2$ for all rows) "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "2.5 bits per parameter regime, throughput improvement using custom kernels, paged attention, etc., is left for future work. We discuss the broader impacts of our work along with limitations in App. H. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, the problem of obtaining a low-precision and low-rank decomposition of an LLM weight matrix was considered. A $\\mathbf{Q}+\\mathbf{LR}$ decomposition efficiently captures the high singular components of the weight matrix with sufficient fidelity, while coarsely compressing the less significant moderateto-low singular components. An optimization-theoretically motivated algorithm was proposed to obtain this decomposition, which iteratively optimized the quantized backbone $\\mathbf{Q}$ and the low-rank factors $\\mathbf{L},\\mathbf{R}$ . Additionally, it was shown that $\\mathbf{L}$ and $\\mathbf{R}$ can be efficiently fine-tuned using low-rank adaptation to boost the zero-shot performance of the quantized model. By utilizing a rank-constrained regression framework, an upper bound was established on the approximation error of the algorithm, and it was shown that this upper bound can be significantly smaller than prior bounds in the literature. Finally, the proposed method was empirically evaluated by compressing the LlaMA family of LLMs in the challenging sub-2.5 bits per parameter regime. The proposed approach can also be used to complement existing compression strategies; thereby making it efficient to distribute compressed LLMs and deploy them on regular consumer hardware, making them more accessible to researchers. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was supported in part by the National Science Foundation (NSF) under Grant DMS2134248; in part by the NSF CAREER Award under Grant CCF-2236829; in part by the U.S. Army Research Office Early Career Award under Grant W911NF-21-1-0242; and in part by the Office of Naval Research under Grant N00014-24-1-2164. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] L. Bentivogli, I. Dagan, H. T. Dang, D. Giampiccolo, and B. Magnini. The Fifth PASCAL Recognizing Textual Entailment Challenge, 2009.   \n[2] Y. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi. PIQA: Reasoning about Physical Commonsense in Natural Language. In Thirty-Fourth AAAI Conference on Artificial Intelligence, 2020.   \n[3] J. Chee, Y. Cai, V. Kuleshov, and C. D. Sa. QuIP: 2-Bit Quantization of Large Language Models With Guarantees. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[4] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord. Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge. arXiv:1803.05457v1, 2018.   \n[5] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer. QLoRA: Efficient Finetuning of Quantized LLMs. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id $\\equiv$ OUIFPHEgJU.   \n[6] J. Dodge, M. Sap, A. Marasovi\u00b4c, W. Agnew, G. Ilharco, D. Groeneveld, M. Mitchell, and M. Gardner. Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus, 2021.   \n[7] V. Egiazarian, A. Panferov, D. Kuznedelev, E. Frantar, A. Babenko, and D. Alistarh. Extreme Compression of Large Language Models via Additive Quantization, 2024. URL https: //arxiv.org/abs/2401.06118.   \n[8] E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh. OPTQ: Accurate Quantization for Generative Pre-trained Transformers. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=tcbBPnfwxS.   \n[9] L. Gao, J. Tow, B. Abbasi, S. Biderman, S. Black, A. DiPof,i C. Foster, L. Golding, J. Hsu, A. Le Noac\u2019h, H. Li, K. McDonell, N. Muennighoff, C. Ociepa, J. Phang, L. Reynolds, H. Schoelkopf, A. Skowron, L. Sutawika, E. Tang, A. Thite, B. Wang, K. Wang, and A. Zou. A framework for few-shot language model evaluation, 12 2023. URL https://zenodo.org/ records/10256836.   \n[10] G. H. Golub and C. F. van Loan. Matrix Computations. JHU Press, fourth edition, 2013. ISBN 1421407949 9781421407944. URL http://www.cs.cornell.edu/cv/GVL4/ golubandvanloan.htm.   \n[11] R. Gray and T. Stockham. Dithered quantizers. IEEE Transactions on Information Theory, 39 (3):805\u2013812, 1993. doi: 10.1109/18.256489.   \n[12] S. Gugger, L. Debut, T. Wolf, P. Schmid, Z. Mueller, S. Mangrulkar, M. Sun, and B. Bossan. Accelerate: Training and inference at scale made simple, efficient and adaptable. https: //github.com/huggingface/accelerate, 2022.   \n[13] H. Guo, P. Greengard, E. P. Xing, and Y. Kim. LQ-LoRA: Low-rank Plus Quantized Matrix Decomposition for Efficient Language Model Finetuning. arxiv:2311.12023, 2023. URL https://arxiv.org/abs/2311.12023.   \n[14] N. Halko, P. G. Martinsson, and J. A. Tropp. Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions. SIAM Review, 53(2):217\u2013288, 2011. doi: 10.1137/090771806. URL https://doi.org/10.1137/090771806.   \n[15] Y.-C. Hsu, T. Hua, S. Chang, Q. Lou, Y. Shen, and H. Jin. Language model compression with weighted low-rank factorization. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id $\\equiv$ uPv9Y3gmAI5.   \n[16] E. J. Hu, yelong shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. LoRA: Low-Rank Adaptation of Large Language Models. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=nZeVKeeFYf9.   \n[17] W. Huang, X. Ma, H. Qin, X. Zheng, C. Lv, H. Chen, J. Luo, X. Qi, X. Liu, and M. Magno. How Good Are Low-bit Quantized LLaMA3 Models? An Empirical Study, 2024.   \n[18] A. Kaushal, T. Vaidhya, and I. Rish. LORD: Low Rank Decomposition Of Monolingual Code LLMs For One-Shot Compression, 2023.   \n[19] S. Keisuke, L. B. Ronan, B. Chandra, and C. Yejin. WinoGrande: An Adversarial Winograd Schema Challenge at Scale, 2019.   \n[20] A. Krishnamoorthy and D. Menon. Matrix inversion using cholesky decomposition. In 2013 Signal Processing: Algorithms, Architectures, Arrangements, and Applications (SPA), pages 70\u201372, 2013.   \n[21] H. J. Levesque, E. Davis, and L. Morgenstern. The winograd schema challenge. In Proceedings of the Thirteenth International Conference on Principles of Knowledge Representation and Reasoning, KR\u201912, page 552\u2013561. AAAI Press, 2012. ISBN 9781577355601.   \n[22] Y. Li, Y. Yu, C. Liang, P. He, N. Karampatziakis, W. Chen, and T. Zhao. LoftQ: LoRAFine-Tuning-Aware Quantization for Large Language Models. arxiv:2310.08659, 2023. URL https://arxiv.org/abs/2310.08659.   \n[23] S.-Y. Liu, C.-Y. Wang, H. Yin, P. Molchanov, Y.-C. F. Wang, K.-T. Cheng, and M.-H. Chen. Dora: Weight-decomposed low-rank adaptation, 2024.   \n[24] S. Ma, H. Wang, L. Ma, L. Wang, W. Wang, S. Huang, L. Dong, R. Wang, J. Xue, and F. Wei. The era of 1-bit llms: All large language models are in 1.58 bits, 2024.   \n[25] S. Merity, C. Xiong, J. Bradbury, and R. Socher. Pointer Sentinel Mixture Models, 2016.   \n[26] Meta AI. Introducing Meta Llama 3: The most capable openly available LLM to date. https: //ai.meta.com/blog/meta-llama-3/, 2024. Accessed: 2024-05-07.   \n[27] M. Nagel, R. A. Amjad, M. Van Baalen, C. Louizos, and T. Blankevoort. Up or Down? Adaptive Rounding for Post-Training Quantization. In Proceedings of the 37th International Conference on Machine Learning, volume 119, pages 7197\u20137206, 2020.   \n[28] S. Rajbhandari, J. Rasley, O. Ruwase, and Y. He. Zero: Memory optimizations toward training trillion parameter models, 2020.   \n[29] S. Raschka. Building a gpt-style llm classifier from scratch, 2024. URL https://magazine. sebastianraschka.com/p/building-a-gpt-style-llm-classifier. Blog post.   \n[30] J. Rasley, S. Rajbhandari, O. Ruwase, and Y. He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD \u201920, page 3505\u20133506, New York, NY, USA, 2020. Association for Computing Machinery. ISBN 9781450379984. doi: 10.1145/3394486.3406703. URL https://doi.org/10.1145/ 3394486.3406703.   \n[31] R. Saha, V. Srivastava, and M. Pilanci. Matrix Compression via Randomized Low Rank and Low Precision Factorization. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=rxsCTtkqA9.   \n[32] L. Schuchman. Dither Signals and Their Effect on Quantization Noise. IEEE Transactions on Communication Technology, 12(4):162\u2013165, 1964. doi: 10.1109/TCOM.1964.1088973.   \n[33] P. Sharma, J. T. Ash, and D. Misra. The Truth is in There: Improving Reasoning in Language Models with Layer-Selective Rank Reduction, 2023.   \n[34] Together Computer. Redpajama: an open dataset for training large language models, October 2023. URL https://github.com/togethercomputer/RedPajama-Data.   \n[35] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini, R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M.-A. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra, I. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and T. Scialom. Llama 2: Open Foundation and Fine-Tuned Chat Models, 2023.   \n[36] A. Tseng, J. Chee, Q. Sun, V. Kuleshov, and C. D. Sa. QuIP#: Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks, 2024.   \n[37] M. Udell and A. Townsend. Why are big data matrices approximately low rank? SIAM Journal on Mathematics of Data Science, 1(1):144\u2013160, 2019. doi: 10.1137/18M1183480. URL https://doi.org/10.1137/18M1183480.   \n[38] R. Vershynin. High-Dimensional Probability: An Introduction with Applications in Data Science. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2018. doi: 10.1017/9781108231596.   \n[39] M. Viazovska. The sphere packing problem in dimension 8. Annals of Mathematics, 185(3), May 2017. ISSN 0003-486X. doi: 10.4007/annals.2017.185.3.7. URL http://dx.doi.org/ 10.4007/annals.2017.185.3.7.   \n[40] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman. GLUE: A MultiTask Benchmark and Analysis Platform for Natural Language Understanding, 2019. In the Proceedings of ICLR.   \n[41] S. Xiang, Y. Zhu, X. Shen, and J. Ye. Optimal exact least squares rank minimization. In Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201912, page 480\u2013488, New York, NY, USA, 2012. Association for Computing Machinery. ISBN 9781450314626. doi: 10.1145/2339530.2339609. URL https: //doi.org/10.1145/2339530.2339609.   \n[42] G. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, and S. Han. SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models. In Proceedings of the 40th International Conference on Machine Learning, 2023.   \n[43] Z. Yao, X. Wu, C. Li, S. Youn, and Y. He. ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation, 2023.   \n[44] Z. Yuan, Y. Shang, Y. Song, Q. Wu, Y. Yan, and G. Sun. ASVD: Activation-aware Singular Value Decomposition for Compressing Large Language Models, 2023.   \n[45] C. Zhang, J. Cheng, G. A. Constantinides, and Y. Zhao. LQER: Low-Rank Quantization Error Reconstruction for LLMs, 2024. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Introduction 1   \n1.1 Significance and Related Works 2 ", "page_idx": 13}, {"type": "text", "text": "2 Problem Formulation 3 ", "page_idx": 13}, {"type": "text", "text": "3 Proposed Algorithm: Calibration-Aware Low-Precision Decomposition with Low Rank Adaptation 3 ", "page_idx": 13}, {"type": "text", "text": "Approximation Error Analysis 4.1 Analysis Outline 6 ", "page_idx": 13}, {"type": "text", "text": "5 Numerical Simulations 7   \n5.1 Zero-shot Results 7   \n5.2 Fine-tuning of Randomized Hadamard Transform (RHT) Parameters 8   \n5.3 Low Rank Adaptation (LoRA) Fine-tuning Results 9   \n5.4 Autoregressive Generation Throughput . 9 ", "page_idx": 13}, {"type": "text", "text": "6 Conclusions 10 ", "page_idx": 13}, {"type": "text", "text": "A Notations 15 ", "page_idx": 13}, {"type": "text", "text": "B Rank-constrained Regression 15 ", "page_idx": 13}, {"type": "text", "text": "C Derivations for Calibration-Aware Low-Precision and Low-Rank Decomposition:   \nCALDERA 17   \nC.1 Update Equations for Low-Rank Factors in LPLRFACTORIZE submodule . 17   \nC.2 Dynamic Ranges of Quantizers . . 18   \nC.3 Proof of Lemma C.3: Approximation Error Upper Bound for LPLRFACTORIZE . . 19   \nC.4 Proof of Thm. 4.1: Approximation Error Upper Bound for CALDERA . 22   \nC.5 A simplification for positive definite Hessians 24 ", "page_idx": 13}, {"type": "text", "text": "D Computational Complexity of CALDERA 25 ", "page_idx": 13}, {"type": "text", "text": "E Additional Experimental Details 25 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "E.1 System Details 2265 E.2 Parameters of CALDERA Quantization   \nE.3 Details about Goodness-of-Fit Metrics 222667 E.4 Fine-tuning Parameter Details   \nE.5 Computation of Average Bit Budget Per-Parameter   \nE.6 Additional Notes on Numerical Simulations 27 ", "page_idx": 13}, {"type": "text", "text": "F Additional Numerical Simulations 27   \nF.1 Ablation Study of CALDERA Parameters with respect to Frobenius Norm Error . 27   \nF.2 Experiments on Mistral-7B 29   \nG Auxiliary Results 29   \nG.1 Useful Results from Linear Algebra and Probability 29   \nG.2 Uniformly dithered scalar quantizer 30 ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "H Limitations and Further Discussions 31 ", "page_idx": 13}, {"type": "text", "text": "A Notations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "This section begins by outlining key notations used in both linear algebra and probability theory. Boldface uppercase and lowercase letters, such as A and a, represent matrices and vectors respectively. I denotes the identity matrix, and its dimension is assumed to be imminent from the context. The first $k$ columns of the identity matrix $\\mathbf{I}$ as ${\\bf{I}}_{k}$ , and let $\\mathbf{\\overline{{I}}}_{k}$ be the submatrix formed by the last $k$ columns. The singular values of $\\mathbf{A}$ are denoted by $\\sigma_{\\mathrm{max}}(\\mathbf{A})\\,=\\,\\sigma_{1}\\,\\geq\\,\\sigma_{2}\\,\\geq\\,\\ldots\\,\\geq\\,\\sigma_{r}\\,=\\,\\sigma_{\\mathrm{min}}(\\mathbf{A})$ , where $r=\\mathrm{rank}(\\mathbf{A})$ . Similarly, the eigenvalues are denoted as $\\lambda_{1}(\\mathbf{A}),\\ldots,\\lambda_{r}(\\mathbf{A})$ . The max-norm of $\\mathbf{A}$ is defined as $\\left\\|A\\right\\|_{\\operatorname*{max}}=\\operatorname*{max}_{i,j}|A_{i j}|$ , the spectral norm of $\\mathbf{A}$ is defined as $\\begin{array}{r}{\\|\\mathbf{A}\\|_{2}=\\operatorname*{sup}_{\\|\\mathbf{x}\\|=1}\\|\\mathbf{A}\\mathbf{x}\\|=}\\end{array}$ $\\sigma_{\\mathrm{max}}(\\mathbf{A})$ , and the Frobenius norm is $\\begin{array}{r}{\\|\\mathbf{A}\\|_{\\mathrm{F}}\\,=\\,\\left(\\sum_{i,j}A_{i j}^{2}\\right)^{1/2}\\,=\\,\\mathrm{Tr}\\left(\\mathbf{A}^{\\top}\\mathbf{A}\\right)\\,=\\,\\left(\\sum_{k\\in[r]}\\sigma_{k}^{2}\\right)^{1/2}}\\end{array}$ For any matrix $\\mathbf{X}$ , its Moore-Penrose pseudo-inverse is denoted by $\\mathbf{X}^{\\dagger}$ . The notations $\\approx$ , $\\lesssim$ and $\\gtrsim$ are used to denote approximate equality and inequalities that hold asymptotically in the limit when dimensions grow to infinity. In other words, for any two functions $A(n)$ and $B(n)$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\nA(n)\\lesssim B(n)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$\\operatorname*{lim}_{n\\to\\infty}A(n)\\leq\\operatorname*{lim}_{n\\to\\infty}B(n)$ ", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Notations $\\approx\\mathrm{and}\\gtrsim$ are defined analogously. Wherever relevant, dimension-dependent terms are highlighted blue. ", "page_idx": 14}, {"type": "table", "img_path": "lkx3OpcqSZ/tmp/b662f29a515d10639a67aca9d52618b9cc05d5e86013cc95d3bb66fe8ec86b85.jpg", "table_caption": ["Table 7: Notations used in this paper "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "B Rank-constrained Regression ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Recall that the submatrix formed by the first $m$ columns of the identity matrix I is denoted as ${\\mathbf{I}}_{m}$ , and let $\\overline{{\\mathbf{I}}}_{m}$ be the submatrix formed by the last $m$ columns. The dimension of I is inferred depending on context. Consider the following: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathrm{rank}(\\mathbf{Z})\\leq k}\\left\\|\\mathbf{X}\\mathbf{Z}-\\mathbf{Y}\\right\\|_{\\mathrm{F}}^{2}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Although this problem is non-convex, it can be solved to global optimality via two SVDs. The following lemma characterizes the solution the rank-constrained regression in (5). ", "page_idx": 15}, {"type": "text", "text": "Lemma B.1. (Global optimality of rank-constrained regression) Suppose $\\mathbf{Y}\\in\\mathbb{R}^{m\\times n}$ is given, and suppose $\\textbf{X}\\in\\mathbb{R}^{m\\times d}$ is full rank, i.e., $\\operatorname{rank}(\\mathbf{X})\\ =\\ \\operatorname*{min}\\{m,d\\}$ , with SVD, $\\mathbf{X}\\,=\\,\\mathbf{U}\\widetilde{\\Sigma}\\mathbf{V}^{\\top}$ . Furthermore, let $\\partial\\dot{\\mathbf{U}}\\dot{\\mathbf{U}}^{\\top}$ denote the full SVD of $\\mathbf{U}^{\\top}\\mathbf{Y}$ if $m\\leq d,$ , or the full SVD of $\\left(\\mathbf{U}\\mathbf{I}_{d}\\right)^{\\top}\\mathbf{Y}~i f$ $m>d$ . Then, for $k\\leq m$ , the solution of (5) is given by ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{Z}_{*}:=\\underset{\\operatorname{rank}(\\mathbf{Z})\\leq k}{\\arg\\operatorname*{min}}\\;\\|\\mathbf{X}\\mathbf{Z}-\\mathbf{Y}\\|_{\\mathrm{F}}^{2}=\\left\\{\\left(\\mathbf{V}\\mathbf{I}_{m}\\Sigma^{-1}\\dot{\\mathbf{U}}\\mathbf{I}_{k}\\right)\\left(\\mathbf{I}_{k}^{\\top}\\dot{\\Sigma}\\mathbf{V}^{\\top}\\right)\\right.\\quad i f\\quad m\\leq d,}\\\\ {\\left(\\mathbf{V}\\Sigma^{-1}\\dot{\\mathbf{U}}\\mathbf{I}_{k}\\right)\\left(\\mathbf{I}_{k}^{\\top}\\dot{\\Sigma}\\mathbf{V}^{\\top}\\right)\\quad}&{o t h e r w i s e.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Here, $\\Sigma$ is a diagonal matrix of the non-zero singular values of $\\mathbf{X}_{\\mathbf{\\theta}}$ , defined as $\\pmb{\\Sigma}:=\\widetilde{\\pmb{\\Sigma}}\\mathbf{I}_{m}\\in\\mathbb{R}^{m\\times m}$ w ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{h e n\\;m\\leq d,\\;a n d\\;\\Sigma:=\\mathbf{I}_{d}^{\\top}\\widetilde{\\Sigma}\\in\\mathbb{R}^{d\\times d}\\;w h e n\\;d<m.\\;A d d i t i o n a l l y,\\;t h e\\;o p t i m a l\\;v a l u e\\;i s}\\\\ &{\\operatorname*{min}_{\\mathrm{rank}\\left(\\mathbf{Z}\\right)\\leq k}\\left\\|\\mathbf{X}\\mathbf{Z}-\\mathbf{Y}\\right\\|_{\\mathrm{F}}^{2}}\\\\ &{\\qquad=\\left\\|\\mathbf{X}\\mathbf{Z}_{*}-\\mathbf{Y}\\right\\|_{\\mathrm{F}}^{2}=\\left\\{\\sum_{i=k+1}^{m}\\sigma_{i}^{2}\\left(\\left(\\mathbf{U}\\mathbf{I}_{d}\\right)^{\\top}\\mathbf{Y}\\right),\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. Case $\\mathbf{m}\\leq\\mathbf{d}$ : Since the full SVD of $\\mathbf{X}\\,\\in\\,\\mathbb{R}^{m\\times d}$ is $\\mathbf{X}=\\mathbf{U}\\widetilde{\\Sigma}\\mathbf{V}^{\\top}$ , where $\\mathbf{U}\\,\\in\\,\\mathbb{R}^{m\\times m}$ , $\\widetilde{\\Sigma}\\,\\in\\,\\mathbb{R}^{m\\times d}$ , and $\\textbf{V}\\in\\,\\mathbb{R}^{d\\times d}$ , the last $(d\\mathrm{~-~}m)$ columns of $\\widetilde{\\Sigma}$ will be zero, i.e., $\\widetilde{\\Sigma}\\mathbf{\\overline{{I}}}_{m-d}\\;=\\;\\mathbf{0}$ Let $\\mathbf{Z}^{\\prime}:=\\mathbf{V}^{\\top}\\mathbf{Z}\\,\\in\\,\\mathbb{R}^{d\\times n}$ be the transformed optimization variable. Since $\\mathbf{V}$ is a unitary matrix, $\\operatorname{rank}(\\mathbf{Z})\\leq\\,k$ if and only if $\\operatorname{rank}(\\mathbf{Z}^{\\prime})\\,\\leq\\,k$ . Splitting $\\mathbf{Z}^{\\prime}\\,\\in\\,\\mathbb{R}^{d\\times n}$ into $\\mathbf{Z}^{\\prime\\prime}:=\\mathbf{I}_{m}^{\\top}\\mathbf{Z}^{\\prime}\\in\\mathbb{R}^{\\sin\\times n}$ and $\\overline{{\\mathbf{Z}}}^{\\prime\\prime}:=\\bar{\\mathbf{I}}_{d-m}^{\\top}\\mathbf{Z}^{\\prime}\\in\\mathbb{R}^{(d-m)\\times n}$ , it can be seen that $\\widetilde\\Sigma{\\bf Z}^{\\prime}\\,=\\,\\widetilde\\Sigma{\\bf I}_{d}{\\bf Z}^{\\prime\\prime}+\\widetilde\\Sigma\\bar{\\bf I}_{m-d}\\overline{{{\\bf Z}}}^{\\prime\\prime}\\,=\\,\\Sigma{\\bf Z}^{\\prime\\prime}$ , where $\\pmb{\\Sigma}:=\\widetilde{\\pmb{\\Sigma}}\\mathbf{I}_{m}\\in\\mathbb{R}^{m\\times m}$ is a diagonal matrix comprised of the non-zero singular values of $\\mathbf{X}$ . Then, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\operatorname{rank}(\\mathbf{Z})\\leq k}\\left\\|\\mathbf{X}\\mathbf{Z}-\\mathbf{Y}\\right\\|_{\\mathrm{F}}^{2}\\equiv\\operatorname*{min}_{\\operatorname{rank}(\\mathbf{Z}^{\\prime})\\leq k}\\left\\|\\widetilde{\\mathbf{Z}}\\mathbf{Z}^{\\prime}-\\mathbf{U}^{\\top}\\mathbf{Y}\\right\\|_{\\mathrm{F}}^{2}\\equiv\\operatorname*{min}_{\\operatorname{rank}(\\mathbf{Z}^{\\prime\\prime})\\leq k}\\left\\|\\mathbf{Z}\\mathbf{Z}^{\\prime\\prime}-\\mathbf{U}^{\\top}\\mathbf{Y}\\right\\|_{\\mathrm{F}}^{2}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Note that the objective function value is independent of $\\overline{{\\mathbf{Z}}}^{\\prime\\prime}$ , as $\\overline{{\\mathbf{Z}}}^{\\prime\\prime}$ lies in the null space of $\\mathbf{X}^{\\top}$ , and $\\operatorname{rank}(\\mathbf{Z}^{\\prime\\prime})\\leq k$ follows from the fact that rank of a submatrix cannot exceed the full matrix. Since $\\mathbf{X}$ is full rank, $\\Sigma$ is invertible, and the minimization in (10) is equivalent to ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\operatorname{rank}\\left(\\widetilde{\\mathbf{Z}}\\right)\\leq k}\\left\\|\\widetilde{\\mathbf{Z}}-\\mathbf{U}^{\\top}\\mathbf{Y}\\right\\|_{\\mathrm{F}}^{2}=\\sum_{i=k+1}^{m}\\sigma_{i}^{2}\\left(\\mathbf{Y}\\right),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\widetilde{\\mathbf Z}:=\\Sigma\\mathbf Z^{\\prime\\prime}\\in\\mathbb{R}^{m\\times n}$ . The equality in (11) follows as a consequence of Eckart-Young-Mirsky theorem (lemma G.1), which states that $\\|\\widetilde{\\mathbf{Z}}-\\mathbf{U}^{\\top}\\mathbf{Y}\\|_{\\mathrm{F}}^{2}$ is minimized by taking the best rank- $k$ approximation of $\\mathbf{U}^{\\top}\\mathbf{Y}$ , and the fact that the singular values of $\\mathbf{U}^{\\top}\\mathbf{Y}$ and $\\mathbf{Y}$ are the same as $\\mathbf{U}$ is unitary. Moreover, as the SVD of $\\mathbf{U}^{\\top}\\mathbf{Y}$ is $\\dot{\\mathbf{U}}\\dot{\\mathbf{\\Sigma}}\\dot{\\mathbf{V}^{\\top}}$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\widetilde{\\mathbf Z}_{\\ast}:=\\operatorname*{\\arg\\operatorname*{min}}_{\\operatorname{rank}(\\widetilde{\\mathbf Z})\\leq k}\\left\\|\\widetilde{\\mathbf Z}-\\mathbf U^{\\top}\\mathbf Y\\right\\|_{\\mathrm F}^{2}=\\left(\\dot{\\mathbf U}\\mathbf I_{k}\\right)\\left(\\mathbf I_{k}^{\\top}\\dot{\\mathbf Z}\\dot{\\mathbf V}^{\\top}\\right)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{{nd}},\\quad\\mathbf{Z}_{*}^{\\prime\\prime}:=\\mathbf{\\Sigma}^{-1}\\widetilde{\\mathbf{Z}}_{*}=\\left(\\mathbf{\\Sigma}^{-1}\\dot{\\mathbf{U}}\\mathbf{I}_{k}\\right)\\left(\\mathbf{I}_{k}^{\\top}\\dot{\\mathbf{\\Sigma}}\\dot{\\mathbf{U}}^{\\top}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "In other words, if $\\mathbf{Z}_{\\ast}^{\\prime}$ denotes the solution of the middle optimization problem in (10), $\\mathbf{I}_{m}^{\\top}\\mathbf{Z}_{*}=\\mathbf{Z}_{*}^{\\prime\\prime}$ . Furthermore, note that setting $\\overline{{\\mathbf{I}}}_{d-m}^{\\top}\\widetilde{\\mathbf{Z}}_{*}=\\mathbf{0}$ ensures $\\mathrm{rank}(\\mathbf{Z}_{*}^{\\prime})=\\mathrm{rank}(\\mathbf{Z}_{*}^{\\prime\\prime})\\leq k$ , while keeping the objective value in (10) unchanged. Hence, an optimal solution is given by ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{Z}_{*}^{\\prime}=\\left[\\left(\\mathbf{Z}^{-1}\\hat{\\mathbf{U}}\\mathbf{I}_{k}\\right)\\left(\\mathbf{I}_{k}^{\\top}\\hat{\\mathbf{Z}}\\hat{\\mathbf{V}}^{\\top}\\right)\\right]\\quad\\mathrm{and},\\quad\\mathbf{Z}_{*}=\\mathbf{VZ}_{*}^{\\prime}=\\left(\\mathbf{V}\\mathbf{I}_{m}\\boldsymbol{\\Sigma}^{-1}\\hat{\\mathbf{U}}\\mathbf{I}_{k}\\right)\\left(\\mathbf{I}_{k}^{\\top}\\hat{\\mathbf{Z}}\\hat{\\mathbf{V}}^{\\top}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Case $\\mathbf{m}>\\mathbf{d}$ : Recalling that the full SVD of $\\mathbf{X}\\,\\in\\,\\mathbb{R}^{m\\times d}$ is $\\mathbf{X}=\\mathbf{U}\\widetilde{\\Sigma}\\mathbf{V}^{\\top}$ , where $\\mathbf{U}\\,\\in\\,\\mathbb{R}^{m\\times m}$ , $\\widetilde{\\Sigma}\\in\\mathbb{R}^{m\\times d}$ , and $\\mathbf{V}\\in\\mathbb{R}^{d\\times d}$ , in this case, the last $m-d$ rows of $\\widetilde{\\Sigma}$ will be zero, i.e., $\\overline{{\\mathbf{I}}}_{m-d}^{\\top}\\widetilde{\\Sigma}=\\mathbf{0}$ . Denote $\\mathbf{Z}^{\\prime}:=\\mathbf{V}^{\\top}\\mathbf{Z}$ . This time, as $\\Sigma:=\\mathbf{I}_{d}^{\\top}\\widetilde\\Sigma$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathrm{rank}(\\mathbf{Z})\\leq k}\\left\\|\\mathbf{X}\\mathbf{Z}-\\mathbf{Y}\\right\\|_{\\mathrm{F}}^{2}\\equiv\\operatorname*{min}_{\\mathrm{rank}(\\mathbf{Z}^{\\prime})\\leq k}\\left\\|\\widetilde{\\mathbf{Z}}\\mathbf{Z}^{\\prime}-\\mathbf{U}^{\\top}\\mathbf{Y}\\right\\|_{\\mathrm{F}}^{2}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\equiv\\underset{\\mathbf{rank}(\\mathbf{Z}^{\\prime})\\leq k}{\\operatorname*{min}}\\left\\|\\mathbf{Z}\\mathbf{Z}^{\\prime}-\\mathbf{I}_{d}^{\\top}\\mathbf{U}^{\\top}\\mathbf{Y}\\right\\|_{\\mathrm{F}}^{2}+\\left\\|\\bar{\\mathbf{I}}_{m-d}^{\\top}\\mathbf{U}^{\\top}\\mathbf{Y}\\right\\|_{\\mathrm{F}}^{2}}\\\\ &{\\equiv\\underset{\\mathbf{rank}(\\mathbf{Z}^{\\prime\\prime})\\leq k}{\\operatorname*{min}}\\left\\|\\mathbf{Z}^{\\prime\\prime}-\\mathbf{I}_{d}^{\\top}\\mathbf{U}^{\\top}\\mathbf{Y}\\right\\|_{\\mathrm{F}}^{2}+\\left\\|\\bar{\\mathbf{I}}_{m-d}^{\\top}\\mathbf{U}^{\\top}\\mathbf{Y}\\right\\|_{\\mathrm{F}}^{2}}\\\\ &{\\overset{\\mathrm{(i)}}{=}\\underset{i=k+1}{\\overset{m}{\\sum}}\\sigma_{i}^{2}\\left((\\mathbf{U}\\mathbf{I}_{d})^{\\top}\\mathbf{Y}\\right)+\\left\\|(\\mathbf{U}\\bar{\\mathbf{I}}_{m-d})^{\\top}\\mathbf{Y}\\right\\|_{\\mathrm{F}}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\mathbf{Z}^{\\prime\\prime}:=\\pmb{\\Sigma}\\mathbf{Z}^{\\prime}=\\pmb{\\Sigma}\\mathbf{V}^{\\top}\\mathbf{Z}$ . Note that the term $\\left\\|(\\mathbf{U}\\overline{{\\mathbf{I}}}_{m-d})^{\\top}\\mathbf{Y}\\right\\|_{\\mathrm{F}}^{2}$ is the irreducible error, and (i) is, once again, a consequence of the fact that $\\left\\|\\mathbf{Z}^{\\prime}-\\mathbf{I}_{d}^{\\top}\\mathbf{U}^{\\top}\\mathbf{Y}\\right\\|_{\\mathrm{F}}^{2}$ is minimized by taking the best rank- $k$ approximation of $\\mathbf{I}_{d}^{\\top}\\mathbf{U}^{\\top}\\mathbf{Y}$ . Moreover, since the SVD of $\\mathbf{I}_{d}^{\\top}\\mathbf{U}^{\\top}\\mathbf{Y}$ is $\\partial\\dot{\\mathbf{U}}\\dot{\\mathbf{\\Xi}}\\dot{\\mathbf{V}}^{\\top}$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\mathbf Z}_{*}^{\\prime\\prime}:=\\underset{\\mathrm{rank}({\\mathbf Z}^{\\prime\\prime})\\leq k}{\\arg\\operatorname*{min}}\\left\\|{\\mathbf Z}^{\\prime\\prime}-{\\mathbf I}_{d}^{\\top}{\\mathbf U}^{\\top}{\\mathbf Y}\\right\\|_{\\mathrm{F}}^{2}=\\left(\\dot{{\\mathbf U}}{\\mathbf I}_{k}\\right)\\left({\\mathbf I}_{k}^{\\top}\\dot{{\\mathbf\\Sigma}}{\\mathbf V}^{\\top}\\right),}\\\\ &{{\\mathbf Z}_{*}={\\mathbf V}{\\boldsymbol\\Sigma}^{-1}{\\mathbf Z}_{*}^{\\prime\\prime}=\\left({\\mathbf V}{\\boldsymbol\\Sigma}^{-1}\\dot{{\\mathbf U}}{\\mathbf I}_{k}\\right)\\left({\\mathbf I}_{k}^{\\top}\\dot{{\\mathbf\\Sigma}}{\\mathbf V}^{\\top}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Remark: It is not necessary for $\\mathbf{X}$ to be full rank. An equivalent result can be derived with $\\Sigma^{-1}$ replaced by $\\pmb{\\Sigma}^{\\dag}$ . ", "page_idx": 16}, {"type": "text", "text": "Computational complexity of rank-constrained regression: Arriving at the globally optimal solution in lemma B.1 requires computing two SVDs, namely $\\mathbf{X}\\in\\mathbb{R}^{m\\times d}$ , which entails a complexity of $\\mathrm{O}(d m^{2})$ , and $\\mathbf{U}^{\\top}\\mathbf{Y}\\,\\,\\dot{\\in}\\,\\mathbb{R}^{m\\times n}$ , with a complexity of $\\dot{\\mathrm{O}}(n m^{2})$ . Hence, the total computational complexity is $\\mathrm{O}(m^{2}(n+d))$ ). ", "page_idx": 16}, {"type": "text", "text": "C Derivations for Calibration-Aware Low-Precision and Low-Rank Decomposition: CALDERA ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "C.1 Update Equations for Low-Rank Factors in LPLRFACTORIZE submodule ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "As discussed in $\\S3$ , the left and right low-rank factors in the LPLRFACTORIZE sub-module (Alg. 2) are found by solving least squares minimization problem. The closed form expressions can be obtained by solving the normal equations directly. In what follows, the same expressions are also derived explicitly via the singular value decomposition, as it gives an expression for the error (for example, refer to (17)) \u2013 which consists of an additive and irreducible error term. ", "page_idx": 16}, {"type": "text", "text": "Updating L with fixed $\\mathbf{R}$ : For a fixed $\\mathbf{R}\\in\\mathbb{R}^{k\\times d}$ , the left low rank factor is computed in lines 5 and 9 of Alg. 2 by solving ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{Z}\\in\\mathbb{R}^{k\\times n}}\\left\\|\\mathbf{XR}^{\\top}\\mathbf{Z}-\\mathbf{Y}\\right\\|_{\\mathrm{F}}^{2},\\mathrm{~where~}\\mathbf{Y}:=\\mathbf{XA}^{\\top}\\in\\mathbb{R}^{m\\times n}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Let $\\mathbf{X}\\mathbf{R}^{\\top}=\\mathbf{U}_{\\mathrm{XR}}\\widetilde{\\pmb{\\Sigma}}_{\\mathrm{XR}}\\mathbf{V}_{\\mathrm{XR}}^{\\top}$ be the full SVD of $\\mathbf{X}\\mathbf{R}^{\\top}$ , where ${\\bf U}_{\\mathrm{XR}}\\in\\mathbb{R}^{m\\times m}$ , $\\widetilde{\\Sigma}_{\\mathrm{XR}}\\in\\mathbb{R}^{m\\times k}$ , and $\\mathbf{V}_{\\mathrm{XR}}\\in\\mathbb{R}^{k\\times k}$ . Rec all from App. B that I denotes the submatrix formed by the first $m$ columns of the identity matrix ${\\mathbf{I}}_{m}$ , and $\\overline{{\\mathbf{I}}}_{m}$ is the submatrix formed by the last $m$ columns. Denoting $\\boldsymbol{\\Sigma}_{\\mathrm{XR}}:=\\mathbf{I}_{k}^{\\top}\\widetilde{\\boldsymbol{\\Sigma}}_{\\mathrm{XR}}$ and $\\mathbf{Z}^{\\prime}:=\\mathbf{V}_{\\mathrm{XR}}^{\\top}\\mathbf{Z}$ , since $\\overline{{\\mathbf{I}}}_{m-k}^{\\top}\\widetilde{\\Sigma}_{\\mathrm{XR}}=\\mathbf{0}$ , it can be seen that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{{\\bf Z}\\in\\mathbb{R}^{k\\times n}}{\\operatorname*{min}}\\left\\|{\\bf U}_{\\mathrm{XR}}\\tilde{\\bf Z}_{\\mathrm{XR}}{\\bf V}_{\\mathrm{XR}}^{\\top}{\\bf Z}-{\\bf Y}\\right\\|_{\\mathrm{F}}^{2}=\\underset{{\\bf X}\\in\\mathbb{R}^{k\\times n}}{\\operatorname*{min}}\\left\\|\\tilde{\\bf\\Sigma}_{\\mathrm{XR}}{\\bf V}_{\\mathrm{XR}}^{\\top}{\\bf Z}-{\\bf U}_{\\mathrm{XR}}^{\\top}{\\bf Y}\\right\\|_{\\mathrm{F}}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\left\\|({\\bf U}_{\\mathrm{XR}}\\bar{\\bf I}_{m-k})^{\\top}{\\bf Y}\\right\\|_{\\mathrm{F}}^{2}+\\underset{{\\bf Z}^{\\prime}\\in\\mathbb{R}^{k\\times n}}{\\operatorname*{min}}\\left\\|{\\bf\\Sigma}_{\\mathrm{XR}}{\\bf Z}^{\\prime}-({\\bf U}_{\\mathrm{XR}}{\\bf I}_{k})^{\\top}{\\bf Y}\\right\\|_{\\mathrm{F}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The last term of (17) is minimized by setting $\\mathbf{Z}^{\\prime}\\leftarrow\\mathbf{Z}_{*}^{\\prime}:=\\Sigma_{\\mathrm{XR}}^{-1}(\\mathbf{U}_{\\mathrm{XR}}\\mathbf{I}_{k})^{\\top}\\mathbf{Y}$ . Since $\\begin{array}{r}{\\mathbf{Z}_{*}^{\\prime}=\\mathbf{V}_{\\mathrm{XR}}^{\\top}\\mathbf{Z}_{*}=}\\end{array}$ $\\mathbf{V}_{\\mathrm{XR}}^{\\top}\\dot{\\mathbf{L}}^{\\top}$ , this yields the left low rank factor to be ", "page_idx": 16}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "$\\dot{\\mathbf{L}}^{\\top}=\\mathbf{V}_{\\mathrm{XR}}\\Sigma_{\\mathrm{XR}}^{-1}(\\mathbf{U}_{\\mathrm{XR}}\\mathbf{I}_{k})^{\\top}\\mathbf{X}\\mathbf{A}^{\\top}\\implies\\dot{\\mathbf{L}}=(\\mathbf{A}\\mathbf{X}^{\\top})(\\mathbf{R}\\mathbf{X}^{\\top})^{\\dagger}\\overset{\\mathrm{(i)}}{=}\\mathbf{A}\\mathbf{H}\\mathbf{R}^{\\top}(\\mathbf{R}\\mathbf{H}\\mathbf{R}^{\\top})^{-1},$ (18) where (i) follows from the explicit expression for the pseudoinverse of the wide matrix $\\mathbf{R}\\mathbf{X}^{\\top}\\in\\mathbb{R}^{k\\times d}$ ", "page_idx": 16}, {"type": "text", "text": "Updating $\\mathbf{R}$ with fixed L: For a fixed $\\mathbf{L}\\in\\mathbb{R}^{n\\times k}$ , the right low-rank factor is computed in lines 4 and 8 of Alg. 2 by solving ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{Z}\\in\\mathbb{R}^{d\\times k}}\\|\\mathbf{X}\\mathbf{Z}\\mathbf{L}^{\\top}-\\mathbf{Y}\\|_{\\mathrm{F}}^{2},\\mathrm{~where~}\\mathbf{Y}:=\\mathbf{X}\\mathbf{A}^{\\top}\\in\\mathbb{R}^{m\\times n}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Let $\\mathbf{X}=\\mathbf{U}_{\\mathrm{X}}\\widetilde{\\pmb{\\Sigma}}_{\\mathrm{X}}\\mathbf{V}_{\\mathrm{X}}^{\\top}$ be the full SVD of $\\mathbf{X}\\,\\in\\,\\mathbb{R}^{m\\times d}$ , where $\\mathbf{U}_{\\mathrm{X}}\\,\\in\\,\\mathbb{R}^{m\\times m}$ , $\\ensuremath{\\widetilde{\\Sigma}}_{\\ensuremath{\\mathrm{{X}}}}\\,\\in\\,\\ensuremath{\\mathbb{R}}^{m\\times d}$ , and $\\mathbf{V}_{\\mathrm{X}}\\in\\mathbb{R}^{d\\times d}$ .  Then, denoting $\\mathbf{Z}^{\\prime}:=\\mathbf{Z}\\mathbf{L}^{\\top}$ , the minimization (19) is equivalent to ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{Z}^{\\prime}\\in\\mathbb{R}^{d\\times n}}\\|\\widetilde{\\mathbf{\\Sigma}}_{\\mathbf{X}}\\mathbf{V}_{\\mathbf{X}}^{\\top}\\mathbf{Z}^{\\prime}-\\mathbf{U}_{\\mathbf{X}}^{\\top}\\mathbf{Y}\\|_{\\mathrm{F}}^{2}\\equiv\\operatorname*{min}_{\\mathbf{Z}^{\\prime\\prime}\\in\\mathbb{R}^{d\\times n}}\\|\\widetilde{\\mathbf{\\Sigma}}_{\\mathbf{X}}\\mathbf{Z}^{\\prime\\prime}-\\mathbf{U}_{\\mathbf{X}}^{\\top}\\mathbf{Y}\\|_{\\mathrm{F}}^{2},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Note that (20) is an undetermined linear system with multiple solutions. In particular, since $\\widetilde{\\Sigma}_{\\mathrm{X}}\\in$ $\\mathbb{R}^{m\\times d}$ , the last $(d-m)$ columns of $\\widetilde{\\Sigma}_{\\mathrm{X}}$ consist of zeros, i.e., $\\widetilde{\\Sigma}_{\\mathrm{X}}\\overline{{\\mathbf{I}}}_{d-m}=\\mathbf{0}$ . Therefore, a solu tion of this system is given by ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{{\\mathbf Z}_{*}^{\\prime\\prime}=\\dot{{\\boldsymbol\\Sigma}}^{\\dagger}\\dot{{\\mathbf U}}_{\\mathrm{X}}^{\\top}{\\mathbf Y}\\quad\\mathrm{or},\\quad{\\mathbf V}_{\\mathrm{X}}^{\\top}{\\mathbf Z}_{*}^{\\prime}=\\widetilde{{\\boldsymbol\\Sigma}}_{\\mathrm{X}}^{\\dagger}{\\mathbf U}_{\\mathrm{X}}^{\\top}{\\mathbf Y}\\quad\\mathrm{or},\\quad{\\mathbf Z}_{*}^{\\prime}={\\mathbf V}_{\\mathrm{X}}\\widetilde{{\\boldsymbol\\Sigma}}_{\\mathrm{X}}^{\\dagger}{\\mathbf U}_{\\mathrm{X}}^{\\top}{\\mathbf Y}}\\\\ &{\\mathrm{or},\\quad{\\mathbf Z}_{*}={\\mathbf V}_{\\mathrm{X}}\\widetilde{{\\boldsymbol\\Sigma}}_{\\mathrm{X}}^{\\dagger}{\\mathbf U}_{\\mathrm{X}}^{\\top}{\\mathbf Y}({\\mathbf L}^{\\top})^{\\dagger}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This implies $\\begin{array}{r}{\\dot{\\mathbf{R}}=\\mathbf{L}^{\\dagger}\\mathbf{A}\\mathbf{X}^{\\top}\\mathbf{U}_{\\mathrm{X}}(\\widetilde{\\pmb{\\Sigma}}_{\\mathrm{X}}^{\\dagger})^{\\top}\\mathbf{V}_{\\mathrm{X}}.}\\end{array}$ ", "page_idx": 17}, {"type": "text", "text": "The proof is completed by noting that $\\mathbf{X}^{\\top}\\mathbf{U}_{\\mathrm{X}}(\\widetilde{\\pmb{\\Sigma}}_{\\mathrm{X}}^{\\dag})^{\\top}\\mathbf{V}_{\\mathrm{X}}=\\mathbf{H}\\mathbf{H}^{\\dag}$ , because, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{H}\\mathbf{H}^{\\dagger}=\\mathbf{X}^{\\top}\\mathbf{X}\\left(\\mathbf{X}^{\\top}\\mathbf{X}\\right)^{\\dagger}=\\mathbf{X}^{\\top}\\mathbf{U}_{\\mathrm{X}}\\widetilde{\\Sigma}_{\\mathrm{X}}\\mathbf{V}_{\\mathrm{X}}^{\\top}\\left(\\mathbf{V}_{\\mathrm{X}}\\widetilde{\\Sigma}_{\\mathrm{X}}^{\\top}\\mathbf{U}_{\\mathrm{X}}^{\\top}\\mathbf{\\widetilde{Z}}_{\\mathrm{X}}\\mathbf{V}_{\\mathrm{X}}^{\\top}\\right)^{\\dagger}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad=\\mathbf{X}^{\\top}\\mathbf{U}_{\\mathrm{X}}\\widetilde{\\Sigma}_{\\mathrm{X}}\\mathbf{V}_{\\mathrm{X}}^{\\top}\\left(\\mathbf{V}_{\\mathrm{X}}\\widetilde{\\Sigma}_{\\mathrm{X}}^{\\top}\\widetilde{\\Sigma}_{\\mathrm{X}}\\mathbf{V}_{\\mathrm{X}}^{\\top}\\right)^{\\dagger}}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\mathbf{X}^{\\top}\\mathbf{U}_{\\mathrm{X}}\\widetilde{\\Sigma}_{\\mathrm{X}}\\mathbf{V}_{\\mathrm{X}}^{\\top}\\mathbf{V}_{\\mathrm{X}}\\left(\\widetilde{\\Sigma}_{\\mathrm{X}}^{\\top}\\widetilde{\\Sigma}_{\\mathrm{X}}\\right)^{\\dagger}\\mathbf{V}_{\\mathrm{X}}^{\\top}}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\mathbf{X}^{\\top}\\mathbf{U}_{\\mathrm{X}}\\widetilde{\\Sigma}_{\\mathrm{X}}\\mathbf{V}_{\\mathrm{X}}^{\\top}\\mathbf{V}_{\\mathrm{X}}\\left(\\widetilde{\\Sigma}_{\\mathrm{X}}^{\\top}\\widetilde{\\Sigma}_{\\mathrm{X}}\\right)^{\\dagger}\\mathbf{V}_{\\mathrm{X}}^{\\top}}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\mathbf{X}^{\\top}\\mathbf{U}_{\\mathrm{X}}\\widetilde{\\Sigma}_{\\mathrm{X}}\\left(\\widetilde{\\Sigma}_{\\mathrm{X}}^{\\top}\\widetilde{\\Sigma}_{\\mathrm{X}}\\right)^{\\dagger}\\mathbf{V}_{\\mathrm{X}}^{\\top}=\\mathbf{U}_{\\mathrm{X}}\\left(\\widetilde{\\Sigma}_{\\mathrm{X}}^{\\dagger}\\right)^{\\top}\\mathbf{V}_{\\mathrm{X}}^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "C.2 Dynamic Ranges of Quantizers ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Lemmas C.1 and C.2, stated in this section provide sufficient conditions to ensure that the quantizers $\\mathrm{Q_{L}}$ and $\\mathrm{Q}_{\\mathrm{R}}$ , used to quantize the left and right low-rank factors in the LPLRFACTORIZE submodule, remain unsaturated. ", "page_idx": 17}, {"type": "text", "text": "Lemma C.1. (Dynamic range of right quantizer) Given matrices $\\mathbf{A}\\in\\mathbb{R}^{n\\times d}$ and $\\mathbf{X}\\in\\mathbb{R}^{m\\times d},$ , let $\\sigma_{\\mathrm{max}}$ denote the maximum singular value of $\\mathbf{X}\\mathbf{A}^{\\top}$ . Then, the quantizer $\\mathrm{Q}_{\\mathrm{R}}$ remains unsaturated $i f$ the dynamic range is chosen to be $\\mathrm{R}_{\\mathrm{R}}=\\sigma_{\\mathrm{max}}$ . ", "page_idx": 17}, {"type": "text", "text": "Proof. Note that the input to the right quantizer $\\mathrm{Q}_{\\mathrm{R}}$ , i.e., $\\mathbf{I}_{k}^{\\top}\\dot{\\Sigma}\\dot{\\mathbf{V}}$ satisfies $\\|\\mathbf{I}_{k}^{\\top}\\dot{\\Sigma}\\dot{\\mathbf{V}}^{\\top}\\|_{\\operatorname*{max}}\\ \\leq$ $\\|\\mathbf{I}_{k}^{\\top}\\dot{\\boldsymbol{\\Sigma}}\\dot{\\boldsymbol{\\mathbf{V}}}^{\\top}\\|_{2}=\\|\\mathbf{I}_{k}^{\\top}\\dot{\\boldsymbol{\\Sigma}}\\|_{2}=\\|\\dot{\\boldsymbol{\\Sigma}}\\|_{2}=\\|\\boldsymbol{\\mathbf{U}}^{\\top}\\boldsymbol{\\mathbf{X}}\\boldsymbol{\\mathbf{A}}^{\\top}\\|_{2}=\\|\\boldsymbol{\\mathbf{X}}\\boldsymbol{\\mathbf{A}}^{\\top}\\|_{2}.$ . This completes the proof. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "Since the input to $\\mathrm{Q_{L}}$ is dependent on the output of $\\mathrm{Q}_{\\mathrm{R}}$ , the following lemma C.2 provides an upper bound on the input to $\\mathrm{Q_{L}}$ , provided that $\\mathrm{Q}_{\\mathrm{R}}$ was unsaturated. ", "page_idx": 17}, {"type": "text", "text": "Lemma C.2. (Dynamic range of left quantizer) Given matrices $\\mathbf{A}\\in\\mathbb{R}^{n\\times d}$ and $\\mathbf{X}\\in\\mathbb{R}^{m\\times n}$ , let $\\lambda_{\\operatorname*{min}}$ denote the smallest eigenvalue of $\\begin{array}{r}{\\mathbf{\\dot{H}}=\\frac{1}{m}\\mathbf{X}^{\\top}\\mathbf{X}}\\end{array}$ , and let $\\sigma_{\\mathrm{max}}$ and $\\sigma_{k}$ denote the largest and the $k^{\\mathrm{th}}$ singular values of $\\mathbf{XA^{\\top}}$ , respectively. For some small $\\epsilon>0$ and an absolute constant $C$ , suppose the number of bits for the right quantizer $\\mathrm{Q}_{\\mathrm{R}}$ satisfies ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{B_{R}}\\geq\\log_{2}\\left(\\frac{4C\\sigma_{\\operatorname*{max}}}{\\sigma_{k}\\log2}\\left(\\sqrt{d}+\\sqrt{k}+\\sqrt{\\log\\left(\\frac{8\\|\\mathbf{X}\\mathbf{A}^{\\top}\\|_{\\mathrm{F}}^{2}}{\\epsilon}\\right)}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then, if the dynamic range of $\\mathrm{Q_{L}}$ is chosen to be ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{R}_{\\mathrm{L}}=\\frac{2\\sigma_{\\mathrm{max}}}{\\sigma_{k}\\sqrt{m\\lambda_{\\mathrm{min}}}},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "then $\\mathrm{Q_{L}}$ remains unsaturated with probability exceeding $1-0.25\\;\\epsilon\\left\\|\\mathbf{X}\\mathbf{A}^{\\top}\\right\\|_{\\mathrm{F}}^{-2}$ . ", "page_idx": 17}, {"type": "text", "text": "Proof. In Line 5 of Alg. 2, the input to $\\mathrm{Q_{L}}$ is $\\dot{\\mathbf{L}}_{0}=\\left(\\mathbf{W}\\mathbf{X}^{\\top}\\right)\\left(\\mathbf{R}_{0}\\mathbf{X}^{\\top}\\right)^{\\dagger}$ . In the rest of the proof, the subscript 0 in $\\dot{\\mathbf{L}}_{0}$ and $\\dot{\\mathbf{R}}_{0}$ is dropped for brevity. Recall the notation for the full SVD of $\\mathbf{X}\\mathbf{R}^{\\top}$ , i.e., $\\mathbf{X}\\mathbf{R}^{\\top}=\\mathbf{U}_{\\mathrm{XR}}\\widetilde{\\pmb{\\Sigma}}_{\\mathrm{XR}}\\mathbf{V}_{\\mathrm{XR}}$ , where ${\\bf U}_{\\mathrm{XR}}\\in\\mathbb{R}^{m\\times m}$ , $\\widetilde{\\Sigma}_{\\mathrm{XR}}\\in\\mathbb{R}^{m\\times k}$ , and ${\\bf V}_{\\mathrm{XR}}\\in\\mathbb{R}^{k\\times k}$ . From Eq. (18), $\\grave{\\mathbf{L}}$ can be expre ssed as $\\begin{array}{r}{\\dot{\\mathbf{L}}=\\mathbf{V}_{\\mathrm{XR}}\\mathbf{\\Sigma}_{\\mathrm{XR}}^{\\dagger}(\\mathbf{U}_{\\mathrm{XR}}\\mathbf{I}_{k})^{\\top}\\mathbf{X}\\mathbf{A}^{\\top}}\\end{array}$ . Consequently, $\\|\\dot{\\mathbf{L}}\\|_{\\operatorname*{max}}$ is upper bounded as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\dot{\\mathbf{L}}\\|_{\\operatorname*{max}}\\leq\\|\\dot{\\mathbf{L}}\\|_{2}\\stackrel{(\\mathrm{i})}{=}\\left\\|\\boldsymbol{\\Sigma}_{\\mathrm{XR}}^{\\dagger}\\left(\\mathbf{U}_{\\mathrm{XR}}\\mathbf{I}_{k}\\right)^{\\top}\\mathbf{X}\\mathbf{A}^{\\top}\\right\\|_{2}\\stackrel{\\mathrm{(ii)}}{\\leq}\\left\\|\\boldsymbol{\\Sigma}_{\\mathrm{XR}}^{\\dagger}\\right\\|_{2}\\left\\|\\left(\\mathbf{U}_{\\mathrm{XR}}\\mathbf{I}_{k}\\right)^{\\top}\\mathbf{X}\\mathbf{A}^{\\top}\\right\\|_{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\stackrel{\\mathrm{(iii)}}{\\leq}\\left\\|\\boldsymbol{\\Sigma}_{\\mathrm{XR}}^{\\dagger}\\right\\|_{2}\\left\\|\\mathbf{X}\\mathbf{A}^{\\top}\\right\\|_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Here, (i) holds because $\\mathbf{V}_{\\mathrm{XR}}$ is a unitary matrix, (ii) follows from submultiplicativity of spectral norm, and (iii) follows from the fact that $\\left(\\mathbf{U}_{\\mathrm{XR}}\\mathbf{I}_{k}\\right)\\left(\\mathbf{U}_{\\mathrm{XR}}\\mathbf{I}_{k}\\right)^{\\top}\\preccurlyeq\\mathbf{I}$ and lemma G.4. Furthermore, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\boldsymbol{\\Sigma}_{\\mathrm{XR}}^{\\dag}\\right\\|_{2}=\\sigma_{\\mathrm{min}}^{-1}(\\boldsymbol{\\Sigma}_{\\mathrm{XR}})\\overset{\\mathrm{(i)}}{\\leq}\\left((m\\lambda_{\\mathrm{min}})^{1/2}\\,\\sigma_{\\mathrm{min}}(\\mathbf{R})\\right)^{-1}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\overset{\\mathrm{(ii)}}{\\leq}(m\\lambda_{\\mathrm{min}})^{-1/2}\\left(\\sigma_{\\mathrm{min}}\\left(\\mathbf{I}_{k}^{\\top}\\dot{\\boldsymbol{\\Sigma}}\\dot{\\mathbf{V}}^{\\top}\\right)-\\|\\mathbf{E}_{\\mathrm{R}}\\|_{2}\\right)^{-1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\sigma_{\\mathrm{min}}(\\Sigma_{\\mathrm{XR}})$ is the smallest non-zero singular value of $\\mathbf{X}\\mathbf{R}^{\\top}$ , (i) follows from lemma G.4 and (ii) follows from lemma G.5. Here, $\\mathbf{E}_{\\mathrm{R}}\\in\\mathbb{R}^{k\\times d}$ is the quantization error from quantizing right low-rank factor, and consist of unbiased random variables with bounded variance, as described in lemma G.6. Since $\\dot{\\Sigma}$ contains the singular values of $\\mathbf{U}^{\\top}\\mathbf{X}\\mathbf{A}^{\\top}$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sigma_{\\operatorname*{min}}\\left(\\mathbf{I}_{k}^{\\mathsf{T}}\\dot{\\mathbf{Z}}\\dot{\\mathbf{V}}^{\\mathsf{T}}\\right)\\overset{\\mathrm{(i)}}{=}\\sigma_{\\operatorname*{min}}(\\mathbf{I}_{k}^{\\mathsf{T}}\\dot{\\mathbf{Z}})=\\sigma_{k}(\\dot{\\mathbf{Z}})=\\sigma_{k}\\left(\\mathbf{U}^{\\mathsf{T}}\\mathbf{X}\\mathbf{A}^{\\mathsf{T}}\\right)\\overset{\\mathrm{(ii)}}{=}\\sigma_{k}\\left(\\mathbf{X}\\mathbf{A}^{\\mathsf{T}}\\right)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where (i) and (ii) follow because $\\grave{\\mathbf{V}}$ and $\\mathbf{U}$ are unitary matrices. Substituting (27) in (26) and (26) in (25), ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\dot{\\mathbf{L}}\\|_{\\operatorname*{max}}\\leq\\left\\|\\mathbf{\\Sigma}\\mathbf{\\dot{x}}_{\\mathrm{R}}^{\\dagger}\\right\\|_{2}\\left\\|\\mathbf{X}\\mathbf{A}^{\\top}\\right\\|_{2}\\leq(m\\lambda_{\\operatorname*{min}})^{-1/2}\\left(\\sigma_{k}\\left(\\mathbf{X}\\mathbf{A}^{\\top}\\right)-\\left\\|\\mathbf{E}_{\\mathrm{R}}\\right\\|_{2}\\right)^{-1}\\left\\|\\mathbf{X}\\mathbf{A}^{\\top}\\right\\|_{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Lemma C.1 suggests choosing the dynamic range of $\\mathrm{Q}_{\\mathrm{R}}$ to be $\\lVert\\mathbf{X}\\mathbf{A}^{\\top}\\rVert_{2}$ to ensure that $\\mathrm{Q}_{\\mathrm{R}}$ remains unsaturated. As a result, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|\\mathbf{E}_{\\mathrm{R}}\\|_{\\operatorname*{max}}\\leq\\Delta_{\\mathrm{R}}:=\\frac{2\\|\\mathbf{X}\\mathbf{A}^{\\top}\\|_{2}}{2^{\\mathrm{B_{R}}}-1}\\implies\\|\\mathbf{E}_{\\mathrm{R}}\\|_{\\psi_{2}}\\leq\\frac{2\\|\\mathbf{X}\\mathbf{A}^{\\top}\\|_{2}}{\\left(2^{\\mathrm{B_{R}}}-1\\right)\\log2},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\lVert\\cdot\\rVert_{\\psi_{2}}$ denotes the subgaussian norm (refer to lemma G.2). Subsequently, lemma G.2 yields that for some absolute constant $C$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|\\mathbf{E}_{\\mathrm{R}}\\|_{2}\\leq\\frac{2C\\|\\mathbf{X}\\mathbf{A}^{\\top}\\|_{2}}{\\left(2^{\\mathrm{B_{R}}}-1\\right)\\log2}\\left(\\sqrt{d}+\\sqrt{k}+t\\right)\\quad\\mathrm{with~probability~exceding~}1-2e^{-t^{2}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Setting $t={\\sqrt{\\log\\left({\\frac{8\\|\\mathbf{X}\\mathbf{A}^{\\top}\\|_{\\mathrm{F}}^{2}}{\\epsilon}}\\right)}}$ yields ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left\\|\\mathbf{E}_{\\mathrm{R}}\\right\\|_{2}\\leq\\frac{2C\\|\\mathbf{X}\\mathbf{A}^{\\top}\\|_{2}}{\\left(2^{\\mathrm{B_{R}}}-1\\right)\\log2}\\left(\\sqrt{d}+\\sqrt{k}+\\sqrt{\\log\\left(\\frac{8\\|\\mathbf{X}\\mathbf{A}^{\\top}\\|_{\\mathrm{F}}^{2}}{\\epsilon}\\right)}\\right),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "with probability exceeding $1-{\\frac{\\epsilon}{4\\|\\mathbf{X}\\mathbf{A}^{\\top}\\|_{\\mathrm{F}}^{2}}}$ . Note that if the bit budget $\\mathrm{B_{R}}$ is chosen to satisfy ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathrm{B_{R}}\\geq\\log_{2}\\left(\\frac{4C\\sigma_{1}}{\\sigma_{k}\\log2}\\left(\\sqrt{d}+\\sqrt{k}+\\sqrt{\\log\\left(\\frac{8\\|\\mathbf{X}\\mathbf{A}^{\\top}\\|_{\\mathrm{F}}^{2}}{\\epsilon}\\right)}\\right)\\right),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "then $\\lVert{\\bf E}_{\\mathrm{R}}\\rVert_{2}\\leq\\sigma_{k}/2$ . Consequently, from (28), ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\Vert\\dot{\\mathbf{L}}\\Vert_{\\mathrm{max}}\\leq\\frac{2\\sigma_{1}}{\\sigma_{k}\\sqrt{m\\lambda_{\\mathrm{min}}}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This completes the proof. ", "page_idx": 18}, {"type": "text", "text": "C.3 Proof of Lemma C.3: Approximation Error Upper Bound for LPLRFACTORIZE ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Lemma C.3. (Approximation error of LPLRFACTORIZE) Given $\\mathbf{A}\\in\\mathbb{R}^{n\\times d}$ and $\\mathbf{X}\\in\\mathbb{R}^{m\\times d}$ with $m\\leq d,$ , let $\\lambda_{\\mathrm{max}}$ and $\\lambda_{\\mathrm{min}}$ denote the maximum and minimum eigenvalues of $\\begin{array}{r}{\\mathbf{H}=\\frac{1}{m}\\mathbf{X}^{\\top}\\mathbf{X}}\\end{array}$ , and let ", "page_idx": 18}, {"type": "text", "text": "$\\sigma_{1}\\ge...\\ge\\sigma_{k}$ denote the singular values of $\\mathbf{XA^{\\top}}$ . Suppose the target rank $k$ satisfies ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{\\lambda_{\\operatorname*{min}}^{1/2}}{4m\\sigma_{1}\\lambda_{\\operatorname*{max}}^{3/2}}\\sum_{i>k}\\sigma_{i}^{2}\\leq k\\leq m,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "the dynamic ranges of quantizers $\\mathrm{Q_{L}}$ and $\\mathrm{Q}_{\\mathrm{R}}$ are respectively set as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{R_{L}}=\\frac{2\\sigma_{1}}{\\sigma_{k}\\sqrt{m\\lambda_{\\mathrm{min}}}}\\quad a n d\\quad\\mathrm{R_{R}}=\\sigma_{1},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and for some absolute constant $C$ and arbitrarily small $\\epsilon$ that satisfies $0<\\epsilon\\leq4m k\\lambda_{\\operatorname*{max}}^{2}\\;\\lambda_{\\operatorname*{min}}^{-1}\\;\\sigma_{1},$ , suppose the bit-budgets of $\\mathrm{Q_{L}}$ and $\\mathrm{Q}_{\\mathrm{R}}$ satisfy ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{B_{L}}\\geq\\log_{2}\\left(4\\frac{\\sigma_{1}^{2}}{\\sigma_{k}}\\sqrt{\\frac{n k}{\\epsilon}\\frac{\\lambda_{\\mathrm{max}}}{\\lambda_{\\mathrm{min}}}}+1\\right),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\ensuremath{\\mathrm{B}}_{\\mathrm{R}}\\geq\\operatorname*{max}\\{\\ensuremath{\\mathrm{B}}_{1},\\ensuremath{\\mathrm{B}}_{2}\\},\\ \\ \\ w h e r e\\,\\ensuremath{\\mathrm{B}}_{1}:=\\log_{2}\\left(2\\sigma_{1}\\sqrt{\\frac{k d}{\\epsilon}\\frac{\\lambda_{\\mathrm{max}}}{\\lambda_{\\mathrm{min}}}}+1\\right),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{B}_{2}:=\\log_{2}\\left(\\frac{4C\\sigma_{1}}{\\sigma_{k}\\log2}\\left(\\sqrt{d}+\\sqrt{k}+\\sqrt{\\log\\left(\\frac{8\\sum_{i}\\sigma_{i}^{2}}{\\epsilon}\\right)}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then, the factors L, R returned by Alg. 1 satisfy $\\begin{array}{r}{\\mathbb{E}\\left\\|(\\mathbf{LR}-\\mathbf{A})\\mathbf{X}^{\\top}\\right\\|_{\\mathrm{F}}^{2}\\leq\\sum_{i>k}\\sigma_{i}^{2}+\\epsilon,}\\end{array}$ , where the expectation is over the stochasticity of quantizers $\\mathrm{Q_{L}}$ and $\\mathrm{Q}_{\\mathrm{R}}$ . ", "page_idx": 19}, {"type": "text", "text": "Proof. Firstly, note that upper bounding the error $\\mathbb{E}\\|(\\mathbf{A}-\\mathbf{L}_{0}\\mathbf{R}_{0})\\mathbf{X}^{\\top}\\|_{\\mathrm{F}}^{2}$ suffices, since the lines 7 to 13 in Alg. 1 refine the estimates of $\\mathbf{L}$ and $\\mathbf{R}$ and can only yield a smaller Frobenius norm error. Consider the quantized low rank factorization $\\mathbf{A}\\approx\\mathrm{Q_{L}}\\left(\\dot{\\mathbf{L}}_{0}\\right)\\mathrm{{Q_{R}}}\\left(\\mathbf{I}_{k}^{\\top}\\dot{\\mathbf{\\Sigma}}\\dot{\\mathbf{V}}^{\\top}\\right)$ . Let ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbf{E}_{\\mathrm{L}}:=\\mathrm{Q}_{\\mathrm{L}}\\left(\\dot{\\mathbf{L}}_{0}\\right)-\\dot{\\mathbf{L}}_{0},\\quad\\mathrm{and}\\quad\\mathbf{E}_{\\mathrm{R}}:=\\mathrm{Q}_{\\mathrm{R}}\\left(\\mathbf{I}_{k}^{\\top}\\dot{\\mathbf{Z}}\\dot{\\mathbf{V}}^{\\top}\\right)-\\mathbf{I}_{k}^{\\top}\\dot{\\mathbf{Z}}\\dot{\\mathbf{V}}^{\\top}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "denote the quantization error matrices. Furthermore, let ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\xi_{*}\\triangleq\\operatorname*{min}_{\\operatorname{rank}(\\mathbf{Z})\\leq k}\\lVert(\\mathbf{A}-\\mathbf{Z})\\mathbf{X}^{\\top}\\rVert_{\\mathrm{F}}^{2}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "denote the optimal value of the unquantized rank-constrained regression problem. Since the dynamic ranges of quantizers $\\mathrm{Q}_{\\mathrm{R}}$ and $\\mathrm{Q_{L}}$ are chosen according to lemmas C.1 and C.2 respectively, the entries of ${\\bf E}_{\\mathrm{L}}$ and $\\mathbf{E}_{\\mathrm{R}}$ are unbiased random variables with bounded variance as in lemma G.6. Let $\\Delta_{\\mathrm{R}}:=2\\mathrm{R}_{\\mathrm{R}}/(2^{\\mathrm{B_{R}}}-1)$ , and $\\Delta_{\\mathrm{L}}:=2\\mathrm{R}_{\\mathrm{L}}/(2^{\\mathrm{B_{L}}}-1)$ denote the quantization resolutions. Conditioned on the event that the left quantizer $\\mathrm{Q_{L}}$ is unsaturated, the error matrix satisfies $\\mathbb{E}\\mathbf{E}_{\\mathrm{L}}=\\mathbf{0}$ . This yields, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left\\|\\left(\\mathrm{Q}_{\\mathrm{L}}\\left(\\dot{\\mathbf{L}}_{0}\\right)\\mathrm{Q}_{\\mathrm{R}}\\left(\\mathbf{I}_{k}^{\\top}\\dot{\\mathbf{Z}}\\dot{\\mathbf{V}}^{\\top}\\right)-\\mathbf{A}\\right)\\mathbf{X}^{\\top}\\right\\|_{\\mathrm{F}}^{2}}\\\\ &{=\\mathbb{E}\\left\\|\\left(\\left(\\dot{\\mathbf{L}}_{0}+\\mathbf{E}_{\\mathrm{L}}\\right)\\mathrm{Q}_{\\mathrm{R}}\\left(\\mathbf{I}_{k}^{\\top}\\dot{\\mathbf{Z}}\\dot{\\mathbf{V}}^{\\top}\\right)-\\mathbf{A}\\right)\\mathbf{X}^{\\top}\\right\\|_{\\mathrm{F}}^{2}}\\\\ &{\\overset{\\mathrm{(i)}}{=}\\underbrace{\\mathbb{E}\\left\\|\\left(\\dot{\\mathbf{L}}_{0}\\mathrm{Q}_{\\mathrm{R}}\\left(\\mathbf{I}_{k}^{\\top}\\dot{\\mathbf{Z}}\\dot{\\mathbf{V}}^{\\top}\\right)-\\mathbf{A}\\right)\\mathbf{X}^{\\top}\\right\\|_{\\mathrm{F}}^{2}}_{\\mathrm{T}_{1}}+\\underbrace{\\mathbb{E}\\left\\|\\mathbf{E}_{\\mathrm{L}}\\mathrm{Q}_{\\mathrm{R}}\\left(\\mathbf{I}_{k}^{\\top}\\dot{\\mathbf{Z}}\\dot{\\mathbf{V}}^{\\top}\\right)\\mathbf{X}^{\\top}\\right\\|_{\\mathrm{F}}^{2}}_{\\mathrm{T}_{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where (i) follows from the fact that the error matrix is unbiased. ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\mathbf{L}}_{0}=\\arg\\operatorname*{min}_{{\\mathbf{Z}}\\in\\mathbb{R}^{k\\times d}}\\left\\|\\left({\\mathbf{Z}}{\\mathbf{R}}_{0}-{\\mathbf{A}}\\right){\\mathbf{X}}^{\\top}\\right\\|_{{\\mathbf{F}}}^{2},\\,\\mathrm{term~T}_{1}\\,\\mathrm{canbe~upper~bounded~as}}\\\\ &{\\qquad\\mathbb{E}\\left\\|\\left(\\hat{\\mathbf{L}}_{0}{\\mathbf{Q}}_{\\mathbb{R}}\\left({\\mathbf{I}}_{k}^{\\top}\\hat{\\mathbf{Z}}\\hat{\\mathbf{V}}^{\\top}\\right)-{\\mathbf{A}}\\right){\\mathbf{X}}^{\\top}\\right\\|_{{\\mathbf{F}}}^{2}}\\\\ &{\\qquad\\overset{(i)}{\\leq}\\mathbb{E}\\left\\|\\left({\\mathbf{V}}{\\mathbf{Z}}^{-1}\\hat{\\mathbf{U}}{\\mathbf{L}}_{k}{\\mathbf{Q}}_{\\mathbb{R}}\\left({\\mathbf{I}}_{k}^{\\top}\\hat{\\mathbf{Z}}\\hat{\\mathbf{V}}^{\\top}\\right)-{\\mathbf{A}}\\right){\\mathbf{X}}^{\\top}\\right\\|_{{\\mathbf{F}}}^{2}}\\\\ &{\\qquad=\\mathbb{E}\\left\\|\\left({\\mathbf{V}}{\\mathbf{Z}}^{-1}\\hat{\\mathbf{U}}{\\mathbf{I}}_{k}\\left({\\mathbf{I}}_{k}^{\\top}\\hat{\\mathbf{Z}}\\hat{\\mathbf{V}}^{\\top}+{\\mathbf{E}}_{\\mathbb{R}}\\right)-{\\mathbf{A}}\\right){\\mathbf{X}}^{\\top}\\right\\|_{{\\mathbf{F}}}^{2}}\\\\ &{\\qquad=\\left\\|\\left({\\mathbf{V}}{\\mathbf{Z}}^{-1}\\hat{\\mathbf{U}}{\\mathbf{I}}_{k}{\\mathbf{I}}_{k}^{\\top}\\hat{\\mathbf{Z}}\\hat{\\mathbf{V}}^{\\top}-{\\mathbf{A}}\\right){\\mathbf{X}}^{\\top}\\right\\|_{{\\mathbf{F}}}^{2}+\\mathbb{E}\\left\\|{\\mathbf{V}}{\\mathbf{Z}}^{-1}\\hat{\\mathbf{U}}{\\mathbf{I}}_{k}{\\mathbf{E}}_{\\mathbb{R}}{\\mathbf{X}}^{\\top}\\right\\|_{{\\mathbf{F}}}^{2},}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\varepsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where inequality (i) is obtained by replacing the minimizer $\\dot{\\mathbf{L}}_{0}$ with a different, but appropriately chosen matrix. Here, $\\Sigma$ is the diagonal matrix containing the non-zero singular values of $\\mathbf{X}$ . ", "page_idx": 20}, {"type": "text", "text": "Since $\\kappa^{2}:=\\lambda_{\\mathrm{max}}/\\lambda_{\\mathrm{min}}$ , the second term in (37) is ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\left\\|{\\mathbf{V}}{\\mathbf{Z}}^{-1}\\hat{{\\mathbf{U}}}{\\mathbf{I}}_{{\\mathbf{k}}}{\\mathbf{E}}_{\\mathbf{R}}{\\mathbf{X}}^{\\top}\\right\\|_{\\mathrm{F}}^{2}\\stackrel{(\\mathrm{i})}{=}\\mathbb{E}\\left\\|{\\mathbf{Z}}^{-1}\\hat{{\\boldsymbol{\\Pi}}}{\\mathbf{I}}_{{\\mathbf{k}}}{\\mathbf{E}}_{\\mathbf{R}}{\\mathbf{X}}^{\\top}\\right\\|_{\\mathrm{F}}^{2}}\\\\ &{=\\mathbb{E}\\left[\\mathrm{Tr}\\left({\\mathbf{Z}}^{-1}\\hat{{\\mathbf{U}}}{\\mathbf{I}}_{{\\mathbf{k}}}{\\mathbf{E}}_{\\mathbf{R}}{\\mathbf{X}}^{\\top}{\\mathbf{X}}{\\mathbf{E}}_{\\mathbf{R}}^{\\top}\\hat{{\\mathbf{U}}}^{\\top}{\\mathbf{Z}}^{-1}\\right)\\right]}\\\\ &{\\stackrel{(\\mathrm{ii})}{\\leq}m\\lambda_{\\operatorname*{max}}\\mathbb{E}\\left[\\mathrm{Tr}\\left({\\mathbf{Z}}^{-1}\\hat{{\\mathbf{U}}}{\\mathbf{I}}_{{\\mathbf{k}}}{\\mathbf{E}}_{\\mathbf{R}}{\\mathbf{E}}_{\\mathbf{R}}^{\\top}{\\mathbf{I}}_{{\\mathbf{k}}}^{\\top}{\\mathbf{U}}^{\\top}{\\mathbf{Z}}^{-1}\\right)\\right]}\\\\ &{\\stackrel{(\\mathrm{ii})}{=}m\\lambda_{\\operatorname*{max}}\\mathbb{E}\\left[\\mathrm{Tr}\\left({\\mathbf{Z}}^{-2}\\hat{{\\mathbf{U}}}{\\mathbf{I}}_{{\\mathbf{k}}}{\\mathbf{E}}_{\\mathbf{R}}{\\mathbf{E}}_{\\mathbf{R}}^{\\top}{\\mathbf{I}}_{{\\mathbf{k}}}^{\\top}{\\mathbf{U}}^{\\top}\\right)\\right]}\\\\ &{\\stackrel{(\\mathrm{iv})}{\\leq}\\kappa^{2}\\mathbb{E}\\left[\\mathrm{Tr}\\left({\\mathbf{I}}_{{\\mathbf{k}}}{\\mathbf{E}}_{\\mathbf{R}}{\\mathbf{E}}_{\\mathbf{R}}^{\\top}{\\mathbf{I}}_{{\\mathbf{k}}}^{\\top}\\right)\\right]}\\\\ &{\\stackrel{(\\mathrm{v})}{=}\\kappa^{2}\\mathbb{E}\\left[\\mathrm{Tr}\\left({\\mathbf{E}}_{\\mathbf{R}}{\\mathbf{E}}_{\\mathbf{\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Here, (i) follows since $\\mathbf{V}$ is a unitary matrix, (ii) follows from lemma G.4, (iii) follows from the cyclic property of trace, (iv) follows as $\\dot{\\mathbf{U}}$ is unitary, and (v) follows since $\\mathbf{I}_{k}^{\\top}\\mathbf{I}_{k}=\\mathbf{I}$ as $k\\leq m$ . Moreover, since $\\mathbb{E}\\mathbf{E}_{\\mathrm{R}}=\\mathbf{0}$ , term $\\mathrm{T_{2}}$ can be upper bounded as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left\\|\\mathbf{E}_{\\mathrm{L}}\\mathrm{Q}_{\\mathrm{R}}\\left(\\mathbf{I}_{k}^{\\top}\\hat{\\mathbf{Z}}\\hat{\\mathbf{V}}^{\\top}\\right)\\mathbf{X}^{\\top}\\right\\|_{\\mathrm{F}}^{2}=\\mathbb{E}\\left\\|\\mathbf{E}_{\\mathrm{L}}\\left(\\mathbf{I}_{k}^{\\top}\\hat{\\mathbf{Z}}\\hat{\\mathbf{V}}^{\\top}+\\mathbf{E}_{\\mathrm{R}}\\right)\\mathbf{X}^{\\top}\\right\\|_{\\mathrm{F}}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\mathbb{E}\\left\\|\\mathbf{E}_{\\mathrm{L}}\\mathbf{I}_{k}^{\\top}\\hat{\\mathbf{Z}}\\hat{\\mathbf{V}}^{\\top}\\mathbf{X}^{\\top}\\right\\|_{\\mathrm{F}}^{2}+\\mathbb{E}\\left\\|\\mathbf{E}_{\\mathrm{L}}\\mathbf{E}_{\\mathrm{R}}\\mathbf{X}^{\\top}\\right\\|_{\\mathrm{F}}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The first term in (39) is ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left\\|\\mathbf{E}_{\\mathrm{L}}\\mathbf{I}_{k}^{\\top}\\hat{\\mathbf{Z}}\\hat{\\mathbf{V}}^{\\top}\\mathbf{X}^{\\top}\\right\\|_{\\mathrm{F}}^{2}=\\mathrm{Tr}\\left(\\mathbf{E}_{\\mathrm{L}}\\mathbf{I}_{k}^{\\top}\\dot{\\Sigma}\\hat{\\mathbf{V}}^{\\top}\\mathbf{X}^{\\top}\\mathbf{X}\\hat{\\mathbf{V}}\\dot{\\Sigma}^{\\top}\\mathbf{I}_{k}\\mathbf{E}_{\\mathrm{L}}^{\\top}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\stackrel{(\\mathrm{i})}{\\leq}m\\lambda_{\\mathrm{max}}~\\mathrm{Tr}\\left(\\mathbf{E}_{\\mathrm{L}}\\mathbf{I}_{k}^{\\top}\\dot{\\Sigma}\\dot{\\Sigma}^{\\top}\\mathbf{I}_{k}\\mathbf{E}_{\\mathrm{L}}^{\\top}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\mathrm{(ii)}}\\\\ &{\\leq m\\lambda_{\\mathrm{max}}~\\sigma_{1}^{2}~\\mathrm{Tr}\\left(\\mathbf{E}_{\\mathrm{L}}\\mathbf{E}_{\\mathrm{L}}^{\\top}\\right)\\leq n k~\\frac{\\Delta_{\\mathrm{L}}^{2}}{4}~m\\lambda_{\\mathrm{max}}~\\sigma_{1}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where (i) and (ii) follow from lemma G.4 and the fact that $\\grave{\\mathbf{V}}$ is a unitary matrix. Similarly, the second term of (39) is ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\left\\|\\mathbf{E}_{\\mathbf{L}}\\mathbf{E}_{\\mathbf{R}}\\mathbf{X}^{\\top}\\right\\|_{\\mathrm{F}}^{2}=\\mathbb{E}\\left[\\mathrm{Tr}\\left(\\mathbf{E}_{\\mathbf{L}}\\mathbf{E}_{\\mathbf{R}}\\mathbf{X}^{\\top}\\mathbf{X}\\mathbf{E}_{\\mathbf{R}}^{\\top}\\mathbf{E}_{\\mathbf{L}}^{\\top}\\right)\\right]}\\\\ {\\quad\\quad\\quad\\quad\\leq m\\lambda_{\\operatorname*{max}}\\;\\mathbb{E}\\left[\\mathrm{Tr}\\left(\\mathbf{E}_{\\mathbf{R}}\\mathbf{E}_{\\mathbf{R}}^{\\top}\\mathbf{E}_{\\mathbf{L}}^{\\top}\\mathbf{E}_{\\mathbf{L}}\\right)\\right]}\\\\ {\\quad\\quad\\quad=m\\lambda_{\\operatorname*{max}}\\;\\mathbb{E}\\left[\\displaystyle\\sum_{i=1}^{k}\\left(\\mathbf{E}_{\\mathbf{R}}\\mathbf{E}_{\\mathbf{R}}^{\\top}\\right)_{i i}\\left(\\mathbb{E}\\left[\\mathbf{E}_{\\mathbf{L}}\\mathbf{E}_{\\mathbf{L}}^{\\top}\\right]\\right)_{i i}\\right]}\\\\ {\\quad\\quad\\quad\\overset{(i)}{\\leq}m\\lambda_{\\operatorname*{max}}\\;\\frac{n\\Delta_{\\mathrm{L}}^{2}}{4}\\;\\mathbb{E}\\left[\\displaystyle\\sum_{i=1}^{k}\\left(\\mathbf{E}_{\\mathbf{R}}\\mathbf{E}_{\\mathbf{R}}^{\\top}\\right)_{i i}\\right]}\\\\ {\\quad\\quad=m\\lambda_{\\operatorname*{max}}\\;\\frac{n\\Delta_{\\mathrm{L}}^{2}}{4}\\left\\mathbb{E}\\left\\|\\mathbf{E}_{\\mathbf{R}}\\right\\|_{\\mathrm{F}}^{2}\\leq k\\,\\frac{n\\Delta_{\\mathrm{L}}^{2}}{4}\\;\\frac{d\\Delta_{\\mathrm{R}}^{2}}{4}\\;m\\lambda_{\\operatorname*{max}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Here, (i) follows because $\\mathbb{E}\\left[\\mathbf{E}_{\\mathrm{L}}^{\\top}\\mathbf{E}_{\\mathrm{L}}\\right]$ is a diagonal matrix since ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left(\\mathbb{E}\\left[\\mathbf{E}_{\\mathrm{L}}^{\\top}\\mathbf{E}_{\\mathrm{L}}\\right]\\right)_{i j}=\\sum_{l=1}^{n}\\mathbb{E}\\left[E_{l i}E_{l j}\\right]=\\left\\{\\sum_{l=1}^{n}\\mathbb{E}\\left[E_{l i}^{2}\\right]\\leq\\frac{n\\Delta_{\\mathrm{L}}^{2}}{4}\\quad\\mathrm{for}\\;i=j,\\right.\\quad\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "As a consequence of (37) to (41), ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left\\|\\left(\\mathrm{Q}_{\\mathrm{L}}\\left(\\mathbf{\\dot{L}}_{0}\\right)\\mathrm{Q}_{\\mathrm{R}}\\left(\\mathbf{I}_{k}^{\\top}\\dot{\\mathbf{D}}\\dot{\\mathbf{V}}^{\\top}\\right)-\\mathbf{A}\\right)\\mathbf{X}^{\\top}\\right\\|_{\\mathrm{F}}^{2}}\\\\ &{\\qquad\\qquad\\leq\\xi_{*}+k d\\,\\frac{\\Delta_{\\mathrm{R}}^{2}}{4}\\kappa^{2}+n k\\,\\frac{\\Delta_{\\mathrm{L}}^{2}}{4}\\,m\\lambda_{\\mathrm{max}}\\,\\sigma_{1}^{2}+k\\,\\frac{n\\Delta_{\\mathrm{L}}^{2}}{4}\\,\\frac{d\\Delta_{\\mathrm{R}}^{2}}{4}\\,m\\lambda_{\\mathrm{max}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Since $\\mathrm{R}_{\\mathrm{R}}=\\sigma_{1}$ , the second term of (43) does not exceed $0.25\\epsilon$ if ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{B_{R}}\\geq\\log_{2}\\left(2\\sigma_{1}\\kappa\\sqrt{\\frac{k d}{\\epsilon}}+1\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "This needs to be considered in conjunction with the lower bound on $\\mathrm{B_{R}}$ in lemma C.1, which yields the maximum of two quantities in the theorem statement. Since $\\begin{array}{r}{\\mathrm{R}_{\\mathrm{L}}=\\frac{2\\sigma_{1}}{\\sigma_{k}\\sqrt{m\\lambda_{\\mathrm{min}}}}}\\end{array}$ \u221a2\u03c31 , the third term of (43) does not exceed $0.25\\epsilon$ if ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{B_{L}}\\geq\\log_{2}\\left(4\\frac{\\sigma_{1}^{2}}{\\sigma_{k}}\\sqrt{\\frac{n k}{\\epsilon}\\frac{\\lambda_{\\mathrm{max}}}{\\lambda_{\\mathrm{min}}}}+1\\right),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "provided $\\mathrm{Q_{L}}$ stays unsaturated. Furthermore, if $\\epsilon\\leq4k m\\lambda_{\\operatorname*{max}}^{2}\\;\\lambda_{\\operatorname*{min}}^{-1}\\;\\sigma_{1}$ , choosing $\\mathrm{B_{R}}$ and $\\mathrm{B_{L}}$ as above ensures that the third term of (43) is also upper bounded by 0.25\u03f5. ", "page_idx": 21}, {"type": "text", "text": "The approximation error upper bound in (43) holds conditioned on the event that $\\mathrm{Q_{L}}$ was unsaturated, which according to lemma C.2, is ensured with probability exceeding $1-0.25\\epsilon\\left\\|\\mathbf{X}\\mathbf{A}^{\\top}\\right\\|_{\\mathrm{F}}^{-2}$ . For analysis purposes, it is assumed that when $\\mathrm{Q_{L}}$ gets saturated, Alg. 1 returns $\\mathbf{L}=\\mathbf{0}$ and $\\mathbf{R}=$ 0, as a result of which, the approximation error is upper bounded by $\\|\\mathbf{XA}^{\\top}\\|_{\\mathrm{F}}^{2}$ .4 Then, since $\\mathrm{Pr}\\left(\\mathrm{Q_{L}}\\right)$ is unsat.) $\\geq1-0.25\\ \\epsilon\\ \\|\\mathbf{X}\\mathbf{A}^{\\top}\\|_{\\mathrm{F}}^{2}$ , using Cauchy-Schwarz inequality for expectations, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left\\|(\\mathbf{LR}-\\mathbf{A})\\mathbf{X}^{\\top}\\right\\|_{\\mathrm{F}}^{2}=\\mathbb{E}\\left[\\left\\|(\\mathbf{LR}-\\mathbf{A})\\mathbf{X}^{\\top}\\right\\|_{\\mathrm{F}}^{2}~\\right]\\mathrm{Q}_{\\mathrm{L}}\\mathrm{~is~unsat.}\\right]+\\mathbb{E}\\left[\\left\\|(\\mathbf{0}-\\mathbf{A})\\mathbf{X}^{\\top}\\right\\|_{\\mathrm{F}}^{2}~\\right]\\mathrm{Q}_{\\mathrm{L}}\\mathrm{~is~sat.}}\\\\ {\\leq\\xi_{*}+\\displaystyle\\frac{3\\epsilon}{4}+\\operatorname*{Pr}\\left(\\mathrm{Q}_{\\mathrm{L}}\\mathrm{~is~sat.}\\right)\\displaystyle\\sum_{i}\\sigma_{i}^{2}~~\\leq~~\\xi_{*}+\\epsilon.~~~~~~~~~~~~~~~~~~~~~~~~~~~(40\\mathrm{Q}_{\\mathrm{L}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "This completes the proof. ", "page_idx": 21}, {"type": "text", "text": "C.4 Proof of Thm. 4.1: Approximation Error Upper Bound for CALDERA ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Theorem C.4. Approximation error of CALDERA (Formal) Given $\\mathbf{W}\\in\\mathbb{R}^{n\\times d}$ and $\\mathbf{X}\\in\\mathbb{R}^{m\\times d}$ with $m\\leq d$ , let $\\mathbf{D}$ be obtained from the LDL decomposition $\\mathbf{X}^{\\top}\\mathbf{X}=m\\mathbf{H}=(\\mathbf{M}+\\mathbf{I})\\mathbf{D}(\\mathbf{M}+\\mathbf{I})^{\\top}$ , and $\\lambda_{\\mathrm{max}}$ , $\\lambda_{\\mathrm{min}}$ denote the max and min eigenvalues of $\\mathbf{H}$ . Additionally, let $\\mathbf{Q}\\triangleq\\mathrm{LDLQ}(\\mathbf{W},\\mathrm{Q}_{\\mathrm{Q}}),$ , where $\\mathrm{Q_{Q}}$ has dynamic range R and bit-budget $\\mathrm{B_{Q}}$ , the quantization error be $\\eta\\triangleq\\mathrm{Q}_{\\mathrm{Q}}(\\mathbf{Q}+(\\mathbf{W}-\\mathbf{\\Lambda}$ $\\mathbf{Q})\\mathbf{M})-(\\mathbf{Q}+(\\mathbf{W}-\\mathbf{Q})\\mathbf{M})$ , and $\\sigma_{1}\\ge...\\ge\\sigma_{k}$ . . . be the singular values of $\\mathbf{X}(\\mathbf{W}-\\mathbf{Q})^{\\top}$ . If the target rank $k$ satisfies ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{\\lambda_{\\operatorname*{min}}^{1/2}}{4m\\sigma_{1}\\lambda_{\\operatorname*{max}}^{3/2}}\\sum_{i>k}\\sigma_{i}^{2}\\leq k\\leq m,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "the dynamic ranges of $\\mathrm{Q_{L}}$ and $\\mathrm{Q}_{\\mathrm{R}}$ are set as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{R_{L}}=\\frac{2\\sigma_{1}}{\\sigma_{k}\\sqrt{m\\lambda_{\\mathrm{min}}}}\\quad a n d,\\quad\\mathrm{R_{R}}=\\sigma_{1},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and for some absolute constant $C$ and arbitrarily small $\\epsilon$ that satisfies $0<\\epsilon\\leq4m k\\lambda_{\\operatorname*{max}}^{2}\\;\\lambda_{\\operatorname*{min}}^{-1}\\;\\sigma_{1},$ suppose the bit-budgets of $\\mathrm{Q_{L}}$ and $\\mathrm{Q}_{\\mathrm{R}}$ are set so that they satisfy ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{B_{L}}\\geq\\log_{2}\\left(4\\frac{\\sigma_{1}^{2}}{\\sigma_{k}}\\sqrt{\\frac{n k}{\\epsilon}\\frac{\\lambda_{\\operatorname*{max}}}{\\lambda_{\\operatorname*{min}}}}+1\\right),}\\\\ {a n d}&{\\mathrm{B_{R}}\\geq\\operatorname*{max}\\{\\mathrm{B_{1}},\\mathrm{B_{2}}\\},\\quad w h e r e\\mathrm{~B_1:=\\log_2\\left(2\\sigma_{1}\\sqrt{\\frac{k d}{\\epsilon}\\frac{\\lambda_{\\operatorname*{max}}}{\\lambda_{\\operatorname*{min}}}}+1\\right),}\\\\ {a n d}&{\\mathrm{B_2}:=\\log_{2}\\left(\\frac{4C\\sigma_{1}}{\\sigma_{k}\\log2}\\left(\\sqrt{d}+\\sqrt{k}+\\sqrt{\\log\\left(\\frac{8\\sum_{i}\\sigma_{i}^{2}}{\\epsilon}\\right)}\\right)\\right),}\\\\ {\\ldots}&{\\quad\\ldots\\quad\\frac{\\ldots\\quad n}{(1-\\sqrt{\\lambda_{\\operatorname*{max}}\\ldots})}\\underset{\\ldots}{\\longrightarrow}\\quad\\ldots\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "then $\\mathbf{Q}$ , L and $\\mathbf{R}$ returned by CALDERA (Alg. 1) satisfy ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left\\|(\\mathbf{Q}+\\mathbf{LR}-\\mathbf{W})\\mathbf{X}^{\\top}\\right\\|_{\\mathrm{F}}^{2}\\leq m\\sum_{i>k}\\mathbb{E}\\lambda_{i}(\\eta\\mathbf{D}\\eta^{\\top})+\\epsilon\\lesssim\\frac{4m d\\lambda_{\\operatorname*{max}}\\mathrm{R}^{2}}{\\pi(2^{\\mathrm{B}_{\\mathrm{Q}}}-1)^{2}}\\left(1-\\frac{k}{n}\\right)\\left(n-\\frac{k}{2}\\right)+\\epsilon,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. As in $\\S C.3$ , an upper bound is obtained on $\\mathbb{E}\\left\\|(\\mathbf{Q}_{0}+\\mathbf{L}_{0}\\mathbf{R}_{0}-\\mathbf{W})\\mathbf{X}^{\\top}\\right\\|_{\\mathrm{F}}^{2}$ , since after the first iteration, the alternating steps 3 and 4 in Alg. 2 simply refine the estimates of $\\mathbf{\\ddot{Q}}$ , $\\mathbf{L}$ and $\\mathbf{R}$ , and can only yield a smaller error. For convenience of notation, in what follows, the subscript 0 is omitted. ", "page_idx": 22}, {"type": "text", "text": "Since $\\mathbf{L}$ , $\\mathbf{R}$ is obtained after applying LPLRFACTORIZE submodule to $({\\bf W}\\!-\\!{\\bf Q})$ , lemma C.3 suggests that if $\\mathrm{R_{L}}$ , $\\mathrm{R}_{\\mathrm{R}}$ , $\\mathrm{B_{L}}$ , and $\\mathrm{B_{R}}$ are chosen appropriately, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}\\|(\\mathbf{Q}+\\mathbf{LR}-\\mathbf{W})\\mathbf{X}^{\\top}\\|_{\\mathrm{F}}^{2}=\\mathbb{E}\\|(\\mathbf{LR}-(\\mathbf{W}-\\mathbf{Q}))\\mathbf{X}^{\\top}\\|_{\\mathrm{F}}^{2}\\leq\\sum_{i>k}\\mathbb{E}\\sigma_{i}^{2}\\left((\\mathbf{Q}-\\mathbf{W})\\mathbf{X}^{\\top}\\right)+\\epsilon,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the expectation in the upper bound on the right hand side is over the randomness in $\\mathbf{Q}$ due to the stochasticity in quantizer $\\mathrm{Q}_{\\mathrm{Q}}$ . Since $\\mathbf{Q}=\\mathrm{LDLQ}(\\mathbf{W},\\mathrm{Q}_{\\mathrm{Q}})$ is the LDLQ quantizer of Chee et al. [3] (or, BLOCKLDLQ quantizer of Tseng et al. [36], which is a successor of LDLQ proposed by Chee et al. [3]), it is shown that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbf{Q}=\\mathrm{Q}_{\\mathrm{Q}}\\bigl(\\mathbf{Q}+\\bigl(\\mathbf{W}-\\mathbf{Q}\\bigr)\\mathbf{M}\\bigr),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\mathbf{M}$ is a strictly upper triangular matrix. This is a consequence of the fact that LDLQ quantizes one column at a time, while simultaneously incorporating a linear feedback from the already quantized columns. Moreover, if $\\eta=\\mathrm{Q}_{\\mathrm{Q}}(\\mathbf{W}+(\\mathbf{W}-\\mathbf{Q})\\mathbf{M})-\\widetilde{(\\mathbf{W}+(\\mathbf{W}-\\mathbf{Q})\\mathbf{M})}$ denotes the quantization error, then it can be seen that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbf{Q}-\\mathbf{W}=\\boldsymbol{\\eta}(\\mathbf{M}+\\mathbf{I})^{-1}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Moreover, since $m\\mathbf{H}=\\mathbf{X}^{\\top}\\mathbf{X}=(\\mathbf{M}+\\mathbf{I})\\mathbf{D}(\\mathbf{M}+\\mathbf{I})^{\\top}.$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\sigma_{i}^{2}\\left((\\mathbf{Q}-\\mathbf{W})\\mathbf{X}^{\\top}\\right)=\\mathbb{E}\\lambda_{i}\\left((\\mathbf{Q}-\\mathbf{W})\\mathbf{H}(\\mathbf{Q}-\\mathbf{W})^{\\top}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\overset{(\\mathrm{i})}{=}\\mathbb{E}\\lambda_{i}\\left(\\pmb{\\eta}(\\mathbf{M}+\\mathbf{I})^{-1}(\\mathbf{M}+\\mathbf{I})\\mathbf{D}(\\mathbf{M}+\\mathbf{I})^{\\top}(\\mathbf{M}+\\mathbf{I})^{-\\top}\\pmb{\\eta}^{\\top}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\mathbb{E}\\lambda_{i}(\\pmb{\\eta}\\mathbf{D}\\pmb{\\eta}^{\\top}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where (i) follows from (48). From (47), ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}\\|(\\mathbf{Q}+\\mathbf{LR}-\\mathbf{W})\\mathbf{X}^{\\top}\\|_{\\mathrm{F}}^{2}\\leq\\sum_{i>k}\\mathbb{E}\\lambda_{i}(\\eta\\mathbf{D}\\eta^{\\top})+\\epsilon.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Since $\\begin{array}{r}{\\eta\\mathbf{D}\\eta^{\\top}=\\sum_{j=1}^{d}D_{j j}\\pmb{\\eta}_{j}\\pmb{\\eta}_{j}^{\\top}}\\end{array}$ , where $D_{j j}$ denotes the $j^{\\mathrm{th}}$ diagonal entry of $\\mathbf{D}$ and $\\eta_{j}$ is the $j^{\\mathrm{th}}$ column of $\\pmb{\\eta}\\in\\mathbb{R}^{n\\times d}$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\lambda_{i}\\left(\\eta\\mathbf{D}\\eta^{\\top}\\right)=\\lambda_{i}\\left(\\sum_{j=1}^{d}D_{j j}\\eta_{j}\\eta_{j}^{\\top}\\right)\\leq d\\;\\left(\\operatorname*{max}_{j}D_{j j}\\right)\\;\\lambda_{i}\\left(\\frac{1}{d}\\sum_{j=1}^{d}\\eta_{j}\\eta_{j}^{\\top}\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Note that $\\eta$ consists of quantization errors, which, for uniformly dithered scalar quantization, are zero mean random variables with bounded variance upper bounded by \u220642 = (2BQR2\u22121)2 . As the exact error distribution for non-subtractively dithered quantization (which is commonly used in practice), is not fully understood, a simplification is made for easier analysis, assuming the quantizer is subtractively dithered. For subtractively dithered quantizers, if Schuchman\u2019s conditions are met [11, 32], this results in a uniform error distribution in the interval $\\left[-\\frac{\\Delta}{2},\\frac{\\Delta}{2}\\right]$ . Therefore, assuming quantization errors are independent and identically distributed, for large dimensions $d$ , the eigenvalues of $\\textstyle{\\frac{1}{d}}\\sum_{j=1}^{d}\\eta_{j}\\eta_{j}^{\\top}$ are distributed according to the Marchenko-Pastur distribution, given by $\\begin{array}{r}{\\mathrm{f}_{\\mathrm{mp}}(x):=\\frac{2d}{\\pi n\\Delta^{2}x}\\sqrt{\\left(\\lambda_{+}-x\\right)\\left(x-\\lambda_{-}\\right)}.}\\end{array}$ , where $\\begin{array}{r}{\\lambda_{\\pm}:=\\frac{\\Delta^{2}}{4}\\left(1\\pm\\sqrt{\\frac{n}{d}}\\right)^{2}}\\end{array}$ . Furthermore, denoting $\\begin{array}{r}{q:=\\mathrm{F}_{\\mathrm{mp}}^{-1}\\left(1-\\frac{k}{n}\\right)}\\end{array}$ , where $\\mathrm{F}_{m p}^{-1}$ is the inverse cumulative distribution function (CDF) of $\\mathrm{f}_{\\mathrm{mp}}(x)$ , it can be seen that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i>k}\\mathbb{E}\\lambda_{i}\\left(\\frac{1}{d}\\sum_{j}\\eta_{j}\\eta_{j}^{\\top}\\right)\\approx(n-k)\\int_{\\lambda_{-}}^{q}\\frac{x}{2\\pi(n/d)(\\Delta^{2}/4)}\\frac{\\sqrt{\\left(\\lambda_{+}-x\\right)\\left(x-\\lambda_{-}\\right)}}{x}d x}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\displaystyle\\frac{\\sqrt{n d}}{\\pi}\\left(1-\\frac{k}{n}\\right)\\left(\\mathrm{F}_{\\mathrm{mp}}^{-1}\\left(1-\\frac{k}{n}\\right)-\\lambda_{-}\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\stackrel{\\mathrm{(ii)}}{\\leq}\\displaystyle\\frac{\\Delta^{2}}{\\pi}\\left(1-\\frac{k}{n}\\right)\\left(n-\\frac{k}{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Here, $\\approx$ denotes that the equaility holds asymptotically, (i) follows from upper bounding the integral using the fact that $\\left(\\lambda_{+}-x\\right)\\left(x-\\lambda_{-}\\right)$ is maximized at = \u03bb++2\u03bb\u2212. Furthermore, (ii) follows after substituting the values of $\\lambda_{\\pm}$ and using the following linear upper bound on the inverse CDF of Marchenko-Pastur distribution, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathrm{F}_{\\mathrm{mp}}^{-1}(x)\\leq\\lambda_{+}-\\left({\\frac{\\lambda_{+}-\\lambda_{-}}{2}}\\right)(1-x).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "It can be verified by inspection that $\\mathrm{F}_{\\mathrm{mp}}(x)$ is lower bounded by $\\textstyle\\left({\\frac{\\lambda_{+}-\\lambda_{-}}{2}}\\right)x-\\left({\\frac{\\lambda_{+}^{2}-\\lambda_{-}^{2}}{4}}\\right)$ , and inequality (53) is a consequence of this fact. Furthermore, from lemma G.3, $\\mathrm{max}_{j}D_{j j}\\,\\le\\,\\lambda_{\\mathrm{max}}$ . Substituting (52) in (51), and substituting $\\begin{array}{r}{\\Delta^{2}=\\frac{4\\mathrm{R}^{2}}{(2^{\\mathrm{B}}\\mathrm{Q}-1)^{2}}}\\end{array}$ (2B4QR\u22121)2 , completes the proof. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "Informal version of Thm. C.4: Consider $n\\,\\approx\\,d$ . From Thm. C.4, a (simplified) asymptotic dependence on the bit-budgets $\\mathrm{B_{L}}$ and $\\mathrm{B_{R}}$ can be obtained. In what follows, constant factors inside the $\\mathrm{log_{2}}$ (\u00b7) have been ignored. Comparing the expressions for $\\mathrm{B_{1}}$ and $\\mathrm{B_{2}}$ , it can be seen that the desired bit-budgets are ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathrm{B_{L}}\\geq\\log_{2}\\left(4\\frac{\\sigma_{1}^{2}}{\\sigma_{k}}\\sqrt{\\frac{n k}{\\epsilon}\\frac{\\lambda_{\\operatorname*{max}}}{\\lambda_{\\operatorname*{min}}}}\\right)\\quad\\mathrm{and,}\\quad\\mathrm{B_{R}}\\geq\\log_{2}\\left(2\\sigma_{2}\\sqrt{\\frac{k d}{\\epsilon}\\frac{\\lambda_{\\operatorname*{max}}}{\\lambda_{\\operatorname*{min}}}}\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "For $n\\approx d$ , this yields an average bit-budget of ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{1}{2}\\left(\\mathrm{B_{L}}+\\mathrm{B_{R}}\\right)=\\frac{1}{2}\\log_{2}\\left(\\frac{8k}{\\epsilon}\\frac{\\sigma_{1}^{3}}{\\sigma_{k}}\\frac{\\lambda_{\\operatorname*{max}}}{\\lambda_{\\operatorname*{min}}}\\sqrt{n d}\\right)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "C.5 A simplification for positive definite Hessians ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "The expressions in (8) are a little involved, but they can be simplified if $\\mathbf{H}$ is positive definite, i.e., all eigenvalues are strictly greater than 0. Let $\\mathbf{H}=\\mathbf{\\dot{U}}\\mathbf{\\dot{A}}\\mathbf{U}^{\\top}$ be the eigenvalue decomposition of $\\mathbf{H}$ , and let $\\mathbf{H}^{\\frac{1}{2}}=\\mathbf{U}\\Lambda^{\\frac{1}{2}}\\mathbf{U}^{\\top}$ be its symmetric square root. Furthermore assume that $\\mathbf{H}$ is positive definite, i.e., all diagonal entries of $\\Lambda$ are strictly positive. In practice, this can be ensured by regularizing $\\mathbf{H}$ through the addition of an identity matrix, scaled by a small amount. ", "page_idx": 23}, {"type": "text", "text": "Then, consider the following equivalent optimization problems: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{\\mathrm{rank}({\\bf Z})\\leq k}{\\operatorname*{min}}\\left\\|({\\bf A}-{\\bf Z}){\\bf X}^{\\top}\\right\\|_{\\mathrm{F}}^{2}\\equiv\\underset{\\mathrm{rank}({\\bf Z})\\leq k}{\\operatorname*{min}}\\mathrm{Tr}\\left(({\\bf A}-{\\bf Z}){\\bf H}({\\bf A}-{\\bf Z})^{\\top}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\equiv\\underset{\\mathrm{rank}({\\bf Z})\\leq k}{\\operatorname*{min}}\\left\\|({\\bf A}-{\\bf Z}){\\bf H}^{\\frac{1}{2}}\\right\\|_{\\mathrm{F}}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\equiv\\underset{\\mathrm{rank}({\\bf Z})\\leq k}{\\operatorname*{min}}\\left\\|({\\bf A}-{\\bf Z}){\\bf U}{\\bf A}^{\\frac{1}{2}}\\right\\|_{\\mathrm{F}}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\equiv\\underset{\\mathrm{rank}({\\bf Z})\\leq k}{\\operatorname*{min}}\\left\\|{\\bf Y}-{\\bf Z}{\\bf U}{\\bf A}^{\\frac{1}{2}}\\right\\|_{\\mathrm{F}}^{2}\\qquad\\mathrm{(where~}\\mathrm{\\bf~Y}\\triangleq{\\bf A}{\\bf U}\\Lambda^{\\frac{1}{2}})}\\\\ &{\\qquad\\qquad\\qquad\\equiv\\underset{\\mathrm{rank}({\\bf Z})\\leq k}{\\operatorname*{min}}\\left\\|{\\bf Y}-{\\bf Z}^{\\prime}\\right\\|_{\\mathrm{F}}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Here, $\\mathbf{Z}^{\\prime}\\triangleq\\mathbf{Z}\\mathbf{U}\\mathbf{A}^{\\frac{1}{2}}$ , and the constraint $\\operatorname{rank}(\\mathbf{Z}^{\\prime})\\leq k$ follow from the fact that multiplying $\\mathbf{Z}$ by an invertible matrix, $\\mathbf{U}\\mathbf{A}^{\\frac{1}{2}}$ , keeps its rank unchanged. The final optimization problem can be solved optimally by considering the SVD of $\\mathbf{Y}$ as $\\mathbf{Y}=\\mathbf{\\bar{U}}\\mathbf{\\Sigma}\\mathbf{y}\\mathbf{\\bar{\\Sigma}}$ , and the optimal solution is ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbf{Z}_{\\ast}^{\\prime}=\\left(\\mathbf{U}\\mathbf{I}_{k}\\right)\\left(\\mathbf{I}_{k}^{\\top}\\boldsymbol{\\Sigma}\\mathbf{V}^{\\top}\\right),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where ${\\bf{I}}_{k}$ denotes the first $k$ columns of the identity matrix. So the final solution is given by, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbf{Z}_{*}=\\left(\\mathbf{U}\\mathbf{I}_{k}\\right)\\left(\\mathbf{I}_{k}^{\\top}\\boldsymbol{\\Sigma}\\mathbf{V}^{\\top}\\boldsymbol{\\Lambda}^{-\\frac{1}{2}}\\mathbf{U}^{\\top}\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "This yields a closed form expression for the optimal solution of the rank-constrained regression problem for the case of positive definite Hessians. ", "page_idx": 23}, {"type": "text", "text": "D Computational Complexity of CALDERA ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Complexity of pre-preprocessing: For a given calibration dataset/activation $\\textbf{X}\\in\\mathbb{R}^{m\\times d}$ , the Hessian is computed as $\\begin{array}{r}{\\mathbf{\\dot{H}}=\\frac{1}{m}\\mathbf{X}^{\\top}\\mathbf{X}}\\end{array}$ , which requires $\\mathrm{O}(m d^{2})$ compute. Subsequently, the LDL decomposition of $\\mathbf{H}$ is computed, which entails $\\mathrm{O}(d^{3})$ complexity [20]. Moreover, $\\mathbf{H}\\mathbf{H}^{\\dagger}$ , which is used in the update equation of \\`R is also computed beforehand, with a complexity of $\\mathrm{O}(d^{3})$ . therefore, the total complexity of pre-processing is $\\mathrm{O}(\\dot{m}d^{2}+2d^{3})$ . ", "page_idx": 24}, {"type": "text", "text": "Complexity of LDLQ: Each iteration of CALDERA invokes a single call to LDLQ. An LDLQ call involves updating each column of an $n\\times d$ iteratively. From (2), updating the $k^{\\mathrm{th}}$ column requires multiplying a matrix (of already quantized columns) in $\\mathbb{R}^{n\\times(k-1)}$ with a vector in $\\mathbb{R}^{k-1}$ , implying a complexity of $\\mathrm{O}(n(k-1))$ ). Subsequently, if $\\mathrm{Q}_{\\mathrm{Q}}$ is uniform scalar quantization, quantizing the (feedback incorporated) $k^{\\mathrm{th}}$ column entails $\\mathrm{O}(n)$ compute, implying a total of $\\mathrm{O}(n k)$ compute for the $k^{\\mathrm{th}}$ column. Hence, quantizing all columns from $k=1,\\dotsc,n$ would require O $(n\\sum_{k=1}^{n}k)=$ $\\begin{array}{r}{\\mathrm{O}\\left(\\frac{n^{2}}{2}+\\frac{3n}{2}\\right)=\\mathrm{O}(n^{2})}\\end{array}$ compute. ", "page_idx": 24}, {"type": "text", "text": "Complexity of LPLRFACTORIZE: Every iteration of CALDERA invokes a call to LPLRFACTORIZE, which itself consists of $\\mathrm{T_{in}}$ inner iterations. Each inner iteration of LPLRFACTORIZE, consists of initializing the left and right low rank factors using rank-constrained regression. As seen in $\\S B$ , it has a computational complexity of O $\\left(m^{2}(n+{\\bar{d}})\\right)$ . Subsequently, for any matrix A, the left low rank factor as $\\overset{\\cdot}{\\mathbf{L}}=\\mathbf{AHR}^{\\top}(\\mathbf{RHR}^{\\top})^{-1}$ can be found using successive matrix multiplications. While a gradient descent based algorithm can possibly speed up this computation, in implementation, closed form expressions are computed directly as they are computationally affordable for the hidden dimensions of the LLMs considered. Computing A $(\\mathbf{H}\\mathbf{R}^{\\top})$ requires O $(d k(n+d))$ , computing $(\\mathbf{RHR}^{\\top})^{-1}$ entails O $\\left(d k(k+d)+k^{3}\\right)$ , and multiplying them together requires O $\\left(n k^{2}\\right)$ . Keeping the dominant term, the aggregated computational complexity for computing the left low-rank factor in each inner iteration is $\\mathrm{O}\\left(n d k\\right)$ . ", "page_idx": 24}, {"type": "text", "text": "Additionally, the left low-rank factor can be computed as $\\dot{\\mathbf{R}}=\\mathbf{L}^{\\dagger}\\mathbf{A}(\\mathbf{H}\\mathbf{H}^{\\dagger})$ , where computing $\\mathbf{L}^{\\dagger}\\mathbf{A}$ is $\\mathrm{O}\\left(n d k\\right)$ . Since $\\mathbf{H}\\mathbf{H}^{\\dagger}$ is already computed beforehand, multiplying $\\mathbf{L}^{\\dagger}\\mathbf{A}$ and $\\mathbf{H}\\mathbf{H}^{\\dagger}$ entails O $\\left(k d^{2}\\right)$ . Once again, keeping the dominant term, the complexity is $\\mathrm{O}\\left(n d k\\right)$ . Hence, the total complexity of each inner iteration of LPLRFACTORIZE is $\\mathrm{O}\\left(n d k\\right)$ . ", "page_idx": 24}, {"type": "text", "text": "Remark: Compared to the two-stage LPLR algorithm proposed in [31], the left and right low-rank factors are iteratively refined in Alg. 2 since this additional compute can be afforded for quantizing LLM weight matrices. Moreover, SVD computations can be sped up with randomized SVD [14]. ", "page_idx": 24}, {"type": "text", "text": "E Additional Experimental Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "E.1 System Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "The code for this paper is available at https://github.com/pilancilab/caldera. The framework is built in PyTorch on top of the QuIP# [36] and LoftQ [22] repositories, and utilizes HuggingFace implementations of all datasets and LLMs. Experiments were performed on either NVIDIA RTX A6000, NVIDIA A10G, or NVIDIA H100 GPUs. Hessian computation for all but LLaMa-2 70B was performed on four NVIDIA A10 GPUs (provisioned as Amazon AWS EC2 instances), parallelized by distributing different layers to different GPUs. Hessian computation for LLaMa-2 70B was performed on a single H100 GPU. Model quantization was performed on four NVIDIA RTX A6000 GPUs, parallelized in the same manner as Hessian computation. All zero-shot and fine-tuning experiments were also run on A6000 GPUs, except for LLaMa-2 70B zero-shot experiments, which were run on an H100 GPU. Low-rank adaptation experiments were parallelized across four GPUs via HuggingFace Accelerate [12], with DeepSpeed ZeRO Level 2 [28, 30]. The use of Llama family of LLMs for research is guided by a community license that can be founded at https://llama.meta.com/license/ and https://llama.meta.com/llama3/license/. ", "page_idx": 24}, {"type": "text", "text": "E.2 Parameters of CALDERA Quantization ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "For all CALDERA decompositions, the number of alternating iterations between updating $\\mathbf{Q}$ and $\\mathbf{L}$ , $\\mathbf{R}$ (i.e., $\\mathrm{T_{out}}$ in Alg. 1) is 15. For decompositions with quantized low-rank factors, the number of LPLR iterations (i.e., $\\mathrm{T_{in}}$ in Alg. 2) is 10. ", "page_idx": 25}, {"type": "text", "text": "For both CALDERA and QuIP# decompositions, the calibration dataset consists of 256 random samples from the RedPajama dataset. The number of tokens in each calibration data point is equal to the context length of the corresponding model, i.e., 4096 for LLaMa-2 7B and 8192 for LLaMa-3 8B. Note that this calibration dataset is significantly smaller than the 6144 datapoints used in [36]. ", "page_idx": 25}, {"type": "text", "text": "An additional heuristic applied during the CALDERA decomposition is an update to the Hessian matrix based on the subspace spanned by LR: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\widetilde{\\mathbf{H}}\\triangleq\\mathbf{H}-\\mathbf{M}\\mathbf{V}\\mathbf{V}^{\\top}\\mathbf{M}^{\\top},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Where $\\mathbf{H}=\\mathbf{M}\\mathbf{M}^{\\top}$ is the LDL decomposition of the Hessian and $\\mathbf{LRM}=\\mathbf{U}\\boldsymbol{\\Sigma}\\mathbf{V}^{\\top}$ is the singular value decomposition of the product LRM. The updatedH  is used in all LPLR updates, and the original Hessian is used for updating the low-rank factors via Alg. 2. This heuristic was found in the QuIP# codebase, and empirically speeds up convergence of CALDERA, as is discussed in App. F.1. Additionally, the SVD steps while quantizing 70B models was replaced by randomized SVD [14] for computational speedup. ", "page_idx": 25}, {"type": "text", "text": "E.3 Details about Goodness-of-Fit Metrics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In addition to perplexity on the WikiText2 and C4 datasets, CALDERA was evaluated using zero-shot accuracy on the following sequence classification tasks: ", "page_idx": 25}, {"type": "text", "text": "1. Winogrande [19]: a collection of $44\\mathrm{k}$ problems problems inspired by the Winograd Schema Challenge [21], specifically designed to be robust to learned biases in model training datasets. Specifically, Winogrande is a two-choice fill-in-the-blank task that requires significant commonsense reasoning.   \n2. RTE [1]: Recognizing Text Entailment is a task in the General Language Understanding Evaluation (GLUE) benchmark [40]. Given two statements, the model must classify whether or not the second follows from the first.   \n3. PiQA [2]: Physical Interaction: Question Answering is a question-answering task that requires understanding about physical relationships between objects.   \n4. ArcE [4]: In general, the AI2 Reasoning Challenge is a set of multiple-choice questions that require a grade-school level of knowledge. ArcE is the subset that is labeled Easy.   \n5. ArcC [4]: The ARC-Challenge problems are in the same format as ARC-Easy, but the dataset only includes problems that were answered incorrectly by selected algorithms. ", "page_idx": 25}, {"type": "text", "text": "The unquantized (16-bit) numbers mentioned in Tabs. 1 and 2 are either copied from [36] for the tasks available reported therein. For some ones that could not be easily found, inference was performed directly on the 16-bit model downloaded from HuggingFace. ", "page_idx": 25}, {"type": "text", "text": "E.4 Fine-tuning Parameter Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Fine-tuning of the diagonal Rademacher matrices in the RHT was performed over a calibration dataset sampled from the training split of RedPajama. Due to computational constraints, each sample contains 512 tokens, rather than the full context length of the model. The calibration dataset is 256 samples in total, with 192 data points in the training split and 64 in the evaluation split. RHT fine-tuning was performed for 5 epochs with a learning rate of $10^{-3}$ . The epoch with the lowest evaluation loss was used for further zero-shot and fine-tuning experiments. ", "page_idx": 25}, {"type": "text", "text": "Parameters for low-rank adaptation experiments can be found in Table 8. Overall, the number of epochs and therefore number of training steps, was determined empirically based on when the evaluation accuracy plateaued. ", "page_idx": 25}, {"type": "text", "text": "For the comparison of CALDERA to LQ-LoRA in Winogrande accuracy, the LQ-LoRA result reported in [13] did not involve fine-tuning directly on the Winogrande dataset, but rather on a subset of C4 and Wikitext2. ", "page_idx": 25}, {"type": "table", "img_path": "lkx3OpcqSZ/tmp/eb0bb3b623b70a6dea493887448efdcc0898b7c63cda191b832d7dd0368a89ec.jpg", "table_caption": ["Table 8: Hyperparameter settings for low-rank adaptation\\*. Batch size refers to the per-device batch size. All fine-tuning experiments are parallelized across four GPUs. "], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "E.5 Computation of Average Bit Budget Per-Parameter ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Assume that the dimensions of the seven weight matrices in each transformer layer have dimensions $n_{i}\\times d_{i}$ , where $i\\in\\{1,\\ldots,7\\}$ . Also, for the models we consider, each transformer layer has matrices of the same dimensions. In general, if the full-rank matrix $\\mathbf{Q}$ has a bit budget of $B_{0}$ , the factors $\\mathbf{L}$ and $\\mathbf{R}$ both have a bit budget of $B_{\\mathrm{LR}}$ , and the rank of the factors is $k$ , the average number of bits per parameter is ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\frac{\\sum_{i=1}^{7}\\left(B_{\\mathrm{Q}}n_{i}d_{i}+k B_{\\mathrm{LR}}(n_{i}+d_{i})\\right)}{\\sum_{i=1}^{7}n_{i}d_{i}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "For LLaMa-2 7B, all self-attention matrices are $4096\\times4096$ , the gate and up projections are $11008\\times4096$ , and the down projection is $4096\\times11008$ . So, $\\left(n_{i},d_{i}\\right)\\,=\\,\\left(4096,4096\\right)$ for $i\\ \\in$ $.\\,.\\,.\\,,4\\},(n_{5},d_{5})=(n_{6},d_{6})\\,\\bar{=}\\,(11008,4096)$ , and $(n_{7},d_{7})=(4096,1100\\dot{8})$ . ", "page_idx": 26}, {"type": "text", "text": "For LLaMa-3 8B, $(n_{1},d_{1})\\ =\\ (n_{4},d_{4})\\ =\\ (4096,4096)$ , $(n_{2},d_{2})\\ =\\ (n_{3},d_{3})\\ =\\ (4096,1024),$ $(n_{5},d_{5})=(n_{6},d_{6})=(14336,4096)$ , and $(n_{7},d_{7})=(4096,14336)$ . ", "page_idx": 26}, {"type": "text", "text": "In $\\S5.3$ , $r\\,=\\,64$ of the $k$ factors are fine-tuned in 16-bit precision. This results in the following number of bits per parameter: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\frac{\\sum_{i=1}^{7}\\left(B_{\\mathrm{Q}}n_{i}d_{i}+(k-r)B_{\\mathrm{LR}}(n_{i}+d_{i})+16r(n_{i}+d_{i})\\right)}{\\sum_{i=1}^{7}n_{i}d_{i}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "E.6 Additional Notes on Numerical Simulations ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "For LLaMa-2 70B, the zero-shot perplexities and accuracies reported in the QuIP# paper [36] could not be replicated. As such, the QuIP# performance reported in this paper are based on recreations of those experiments. ", "page_idx": 26}, {"type": "text", "text": "F Additional Numerical Simulations ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "F.1 Ablation Study of CALDERA Parameters with respect to Frobenius Norm Error ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "The per-iteration relative Frobenius norm error of CALDERA, for a few selected weight matrices of LLaMa-2 7B, is plotted in Figure 3. The data-aware relative Frobenius norm error is defined as ", "page_idx": 26}, {"type": "equation", "text": "$$\n{\\frac{\\left\\|\\left(\\mathbf{Q}+\\mathbf{LR}-\\mathbf{W}\\right)\\mathbf{X}^{\\top}\\right\\|_{\\mathrm{F}}}{\\left\\|\\mathbf{W}\\mathbf{X}^{\\top}\\right\\|_{\\mathrm{F}}}},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where W is the unquantized weight matrix, $\\mathbf{X}$ is the calibration data, and $\\mathbf{Q}+\\mathbf{LR}$ is the CALDERA decomposition. The following ablations of CALDERA decomposition parameters are considered: ", "page_idx": 26}, {"type": "text", "text": "1. 16-Bit Factors: $\\mathbf{Q}$ is quantized to 2 bits via LDLQ. The low-rank factors are kept in half precision, so no LPLR iterations are required after rank-constrained regression is performed. The randomized Hadamard transform is performed on the W and $\\mathbf{H}$ matrices are prior to quantization.   \n2. 4-Bit Factors: The low-rank factors are quantized to 4 bits of precision using an E8 lattice quantizer, and 10 iterations of LPLRFACTORIZE (Alg. 2) are run for each update of the factors (i.e., $T_{\\mathrm{in}}=10$ in Alg. 1). Otherwise, the decompostion parameters are the same as in 16-Bit Factors.   \n3. 4-Bit Factors (No RHT): The parameters of CALDERA are the same as in 4-Bit Factors, except the randomized Hadamard transform step is omitted. ", "page_idx": 26}, {"type": "text", "text": "4. 4-Bit Factors (No LDLQ): The parameters of CALDERA are the same as in 4-Bit Factors, except $\\mathbf{Q}$ is quantized to 2-bit precision using direct E8 lattice quantization instead of the LDLQ algorithm. ", "page_idx": 27}, {"type": "text", "text": "5. 16-Bit Factors (Hessian Update): The parameters of CALDERA are the same as in 16-Bit Factors, except the Hessian update step described in App. E.2 is performed prior to LDLQ. ", "page_idx": 27}, {"type": "text", "text": "For each ablation, the rank of $\\mathbf{L}$ and $\\mathbf{R}$ varies between $k\\in\\{64,128,256\\}$ . For comparison, QuIP# with the same-size low-rank factors is performed, using the same calibration data and the low-rank decomposition from the QuIP# codebase. ", "page_idx": 27}, {"type": "image", "img_path": "lkx3OpcqSZ/tmp/592bca66c3bdf9fdf7552ffdf5aee2a55f8606dc657c01d3ec28bb3459706ad8.jpg", "img_caption": ["Figure 3: Relative data-aware Frobenius norm error per iteration of CALDERA for selected matrices of LLaMa-2 7B layer 25. For all experiments, the bit precision of $\\mathbf{Q}$ is 2, and the calibration dataset is the same as used in $\\S5$ . The first iteration of CALDERA with the Hessian update is omitted, as it has a large error, inhibiting plot readability. "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Overall, CALDERA with 16-bit factors consistently achieves a lower error than QuIP#. Additionally, the Hessian update heuristic improves convergence and often, but not always, reduces the final error achieved after 50 iterations of Alg. 1. CALDERA with 4-bit factors has higher Frobenius-norm error than with 16-bit factors, but the degradation is minor. On the other hand, replacing LDLQ with a lattice quantizer significantly degrades the Frobenius norm error, and omitting the randomized Hadamard transform worsens the error for some of the weight matrices considered. ", "page_idx": 27}, {"type": "image", "img_path": "lkx3OpcqSZ/tmp/7a6cd628bb647e7ae5ebb3c4ec2af6c061018dcbe6cb1baf6dc9c46ae3531cab.jpg", "img_caption": ["Figure 4: Relative data-aware Frobenius norm error per iteration of LPLRFACTORIZE, for the decomposition $\\mathbf{W}\\approx\\mathbf{LR}$ , for two matrices in LLaMa-2 7B layer 25. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "To demonstrate the convergence of LPLRFACTORIZE (Alg. 2), the per-iteration relative data-aware error of LPLRFACTORIZE for the factorization $\\mathbf{W}\\approx\\mathbf{LR}$ is plotted in Figure 4. For all curves plotted, the randomized Hadamard transform is performed on W and $\\mathbf{H}$ before the factorization is computed, and both factors are quantized to 4 bits of precision via an E8 lattice quantizer. ", "page_idx": 28}, {"type": "text", "text": "In both cases, the alternating minimization iterations reduce the error. For the Up projection matrix, this reduction is nominal, whereas, for the Key projection matrix, the alternating iterations result in a significant improvement. ", "page_idx": 28}, {"type": "text", "text": "F.2 Experiments on Mistral-7B ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Perplexity and language modeling benchmark accuracy results for quantizing Mistral 7B via CALDERA and QuIP# are in Table 9. Results are consistent with those in Section 5.1. ", "page_idx": 28}, {"type": "text", "text": "Table 9: Evaluations of Wikitext2 and C4 perplexities, as well as percent accuracies on some common language modeling benchmarks, on CALDERA-compressed Mistral 7B. All quantizations use calibration datasets released on Huggingface by the authors of QuIP#. $\\mathrm{B_{Q}}=2$ bits throughout, and $\\mathrm{B_{L}}=\\mathrm{B_{R}}=4$ bits where low-rank factors are present. For fairness of comparison, QuIP# numbers reported do not include RHT finetuning. ", "page_idx": 28}, {"type": "table", "img_path": "lkx3OpcqSZ/tmp/c6bec6881dcc8f28b42f93a79d2114542aa7b0bc43e4c65d883cdaba41aeff9d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "G Auxiliary Results ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "G.1 Useful Results from Linear Algebra and Probability ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "This section states some useful linear algebra results as lemmas. Some of them are stated without proof, which can be easily proved or found in a linear algebra textbook such as [10]. ", "page_idx": 28}, {"type": "text", "text": "Lemma G.1. (Eckart-Young-Mirsky theorem) For any matrix A with SVD given by $\\mathbf{A}=\\mathbf{U}\\boldsymbol{\\Sigma}\\mathbf{V}^{\\top}$ , the solution of $\\mathbf{A}_{k}:=\\mathrm{arg~min}_{\\mathrm{rank}(\\widehat{\\mathbf{A}})\\leq k}\\|\\mathbf{A}-\\widehat{\\mathbf{A}}\\|_{\\mathrm{F}}^{2}$ is given by $\\mathbf{A}_{k}=\\Big(\\mathbf{U}\\boldsymbol{\\Sigma}^{1/2}\\mathbf{I}_{k}\\Big)\\,\\Big(\\mathbf{I}_{k}^{\\top}\\boldsymbol{\\Sigma}^{1/2}\\mathbf{V}^{\\top}\\Big),$ and $\\begin{array}{r}{\\|\\mathbf{A}-\\mathbf{A}_{k}\\|_{\\mathrm{F}}^{2}=\\sum_{i>k}\\sigma_{i}^{2}(\\mathbf{A})}\\end{array}$ . ", "page_idx": 28}, {"type": "text", "text": "Lemma G.2. (Vershynin $I38,$ , Thm 4.4.5]) Let $\\mathbf{X}$ be a $d\\times m$ random matrix whose entries $X_{i j}$ are independent, zero-mean, subgaussian random variables. Then, for any $t~>~0$ , $\\|\\mathbf{X}\\|_{2}\\leq$ $C K\\left({\\sqrt{d}}+{\\sqrt{m}}+t\\right)$ with probability exceeding $1-2e^{-t^{2}}$ , where $K\\,=\\,\\operatorname*{max}_{i,j}\\,\\|A_{i j}\\|_{\\psi_{2}}$ , and $\\lVert\\cdot\\rVert_{\\psi_{2}}$ denotes the subgaussian norm, and $C$ is an absolute constant. ", "page_idx": 28}, {"type": "text", "text": "Remark: Lemma G.2 states a high probability upper bound on the spectral norm of a random matrix in terms of the subgaussian norm of the entries of the matrix. The subgaussian norm of a subgaussian random variable $X$ is defined as $\\|X\\|_{\\psi_{2}}\\triangleq\\operatorname*{inf}\\{t\\ge0\\ |\\ \\mathbb{E}[e^{X^{2}/t^{2}}]\\le2\\}$ . It can be shown that any bounded random variable $X$ is subgaussian, and satisfies $\\|X\\|_{\\psi_{2}}\\leq\\frac{\\|X\\|_{\\infty}}{\\log2}$ ", "page_idx": 29}, {"type": "text", "text": "Lemma G.3. Suppose the LDL decomposition of a matrix $\\mathbf{H}\\,\\in\\,\\mathbb{R}^{d\\times d}$ is given by $\\mathbf{H}\\,=\\,(\\mathbf{M}\\,+$ $\\mathbf{I})\\mathbf{D}(\\mathbf{M}+\\mathbf{I})^{\\top}$ , where $\\mathbf{M}$ is strictly upper triangular and $\\mathbf{D}$ is diagonal. Then, $\\mathrm{max}_{j}D_{j j}\\leq\\lambda_{\\mathrm{max}}$ , where $\\lambda_{\\mathrm{max}}$ denotes the largest eigenvalue of $\\mathbf{H}$ . ", "page_idx": 29}, {"type": "text", "text": "Proof. Let ${\\bf{e}}_{j}$ denote the $j^{\\mathrm{th}}$ canonical basis vector. Then, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbf{e}_{j}^{\\top}\\mathbf{H}\\mathbf{e}_{j}=\\mathbf{e}_{j}^{\\top}(\\mathbf{M}+\\mathbf{I})\\mathbf{D}(\\mathbf{M}+\\mathbf{I})^{\\top}\\mathbf{e}_{j}\\overset{\\mathrm{(i)}}{=}D_{j j}+\\sum_{i>j}M_{j i}^{2}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Moreover, from properties of eigenvalues, ${\\bf e}_{j}^{\\top}{\\bf H}{\\bf e}_{j}\\,\\le\\,\\lambda_{\\mathrm{max}}$ . Therefore, for any $j$ , $D_{j j}\\,\\le\\,D_{j j}\\,+$ $\\begin{array}{r}{\\sum_{i>j}M_{j i}^{2}\\leq\\lambda_{\\operatorname*{max}}}\\end{array}$ . Since it holds true for all $j$ , this completes the proof. \u53e3 ", "page_idx": 29}, {"type": "text", "text": "Lemma G.4. (Loewner ordering for matrix products) For any matrix A and $\\mathbf{B}$ , ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\sigma_{\\mathrm{min}}^{2}(\\mathbf{A})\\;\\mathbf{B}^{\\top}\\mathbf{B}\\prec\\mathbf{B}^{\\top}\\mathbf{A}^{\\top}\\mathbf{A}\\mathbf{B}\\prec\\sigma_{\\mathrm{max}}^{2}(\\mathbf{A})\\;\\mathbf{B}^{\\top}\\mathbf{B}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Lemma G.5. (Minimum singular value) For matrices A and $\\mathbf{B}$ , $\\sigma_{\\mathrm{min}}(\\mathbf{A+B})\\geq\\sigma_{\\mathrm{min}}(\\mathbf{A})-\\|\\mathbf{B}\\|_{2}$ . ", "page_idx": 29}, {"type": "text", "text": "G.2 Uniformly dithered scalar quantizer ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Let us consider quantizing a scalar $x$ with $|x|\\leq\\mathrm{R}$ . Given B bits, the scalar quantizer with dynamic range $\\mathrm{R}$ is described by first specifying the $M=2^{\\mathrm{B}}$ quantization points ", "page_idx": 29}, {"type": "equation", "text": "$$\n{\\cal q}_{1}=-\\mathrm{R},q_{2}=-\\mathrm{R}+\\Delta,q_{3}=-\\mathrm{R}+2\\Delta,\\ldots,q_{M}=-\\mathrm{R}+(M-1)\\Delta,\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $\\begin{array}{r}{\\Delta=\\frac{2\\mathrm{R}}{M-1}}\\end{array}$ is the resolution. The quantizer operation is defined as: ", "page_idx": 29}, {"type": "equation", "text": "$$\n{\\mathrm{Q}}_{\\mathrm{R,B}}(x)={\\left\\{\\begin{array}{l l}{q_{k+1}\\;\\;{\\mathrm{with~probability}}\\;\\;r,}\\\\ {q_{k}\\;\\;\\;\\;{\\mathrm{with~probability}}\\;\\;1-r,}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $k\\,=\\,\\arg\\,\\operatorname*{max}_{j}\\{q_{j}\\,\\leq\\,x\\}$ , i.e., $x\\,\\in\\,[q_{k},q_{k+1})$ , and $\\begin{array}{r}{r\\,=\\,\\frac{x-q_{k}}{\\Delta}}\\end{array}$ . As shown in the following lemma G.6, such a quantizer satisfies ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{E}\\,\\stackrel{\\cdot}{\\left[\\mathrm{Q}_{\\mathrm{R,B}}(x)\\right]}=x\\quad\\mathrm{and}\\quad\\mathbb{E}\\,\\big(\\mathrm{Q}_{\\mathrm{R,B}}(x)-x\\big)^{2}\\leq\\frac{\\Delta^{2}}{4}=\\frac{\\mathrm{R}^{2}}{\\big(2^{\\mathrm{B}}-1\\big)^{2}},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "i.e., it is unbiased and the error variance depends on $\\mathrm{R}$ and B. Here, the $\\mathbb{E}(\\cdot)$ is over the randomness from dithering in (59). If the input $x$ to the quantizer falls outside this range, i.e., $x>\\mathrm{R}$ or $x<-\\mathrm{R}$ , the quantizer is said to be saturated. To quantize any matrix $\\mathbf{X}$ $,\\mathrm{Q}_{\\mathrm{R,B}}(\\mathbf{X})$ is obtained by quantizing each entry independently, i.e., $\\left[\\operatorname{Q}_{\\mathrm{R,B}}(\\mathbf{X})\\right]_{i j}\\triangleq\\operatorname{Q}_{\\mathrm{R,B}}(X_{i j})$ . ", "page_idx": 29}, {"type": "text", "text": "Lemma G.6. For scalar $x$ with $|x|\\leq\\mathrm{R},$ denote the quantization error of uniformly dithered B\u2013bit scalar quantizer as $\\epsilon=\\mathrm{Q_{R,B}}(x)-x$ . Then, $\\mathbb{E}[\\epsilon]=0$ and $\\begin{array}{r}{V a r\\left(\\epsilon\\right)\\leq\\frac{\\mathrm{R}^{2}}{\\left(2^{\\mathrm{B}}-1\\right)^{2}}}\\end{array}$ , where the expectation $\\mathbb{E}$ is over the randomness due to dithering in the quantizer operation. ", "page_idx": 29}, {"type": "text", "text": "Proof. Suppose $x\\in[q_{k},q_{k+1})$ and $q_{k+1}=q_{k}+\\Delta$ , where $\\begin{array}{r}{\\Delta=\\frac{2\\mathrm{R}}{2^{\\mathrm{B}}-1}}\\end{array}$ 2B2R\u22121. Then, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{E}\\;\\mathrm{Q}_{\\mathrm{R,B}}(x)=q_{k+1}\\;\\frac{x-q_{k}}{\\Delta}+q_{k}\\;\\left(1-\\frac{x-q_{k}}{\\Delta}\\right)=\\frac{\\left(q_{k}+\\Delta\\right)\\left(x-q_{k}\\right)+q_{k}\\left(\\Delta-x+q_{k}\\right)}{\\Delta}=x.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Furthermore, the variance can be upper bounded as ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Var}\\left(\\mathrm{Q}_{\\mathrm{R},\\mathrm{B}}(x)-x\\right)^{2}=(q_{k+1}-x)^{2}\\frac{\\left(x-q_{k}\\right)}{\\Delta}+(q_{k}-x)^{2}\\left(1-\\frac{x-q_{k}}{\\Delta}\\right)}\\\\ &{\\phantom{\\mathrm{Var}}\\leq(q_{k+1}-x)\\left(x-q_{k}\\right)}\\\\ &{\\phantom{\\mathrm{Var}}\\leq\\underset{x\\in[q_{k},q_{k+1})}{\\operatorname*{sup}}\\left(q_{k+1}-x\\right)(x-q_{k})}\\\\ &{\\phantom{\\mathrm{Var}}=\\left(q_{k+1}-\\frac{q_{k}+q_{k+1}}{2}\\right)\\left(\\frac{q_{k}+q_{k+1}}{2}-q_{k}\\right)=\\frac{\\Delta^{2}}{4}=\\frac{\\mathrm{R}^{2}}{(2^{\\mathrm{B}}-1)^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "When the input $x$ to the quantizer exceeds R, the quantizer is said to be saturated, causing the quantization error to deviate from zero mean and bounded variance. Thus, it is crucial to ensure that the quantizer operates within the unsaturated regime with high probability. ", "page_idx": 30}, {"type": "text", "text": "H Limitations and Further Discussions ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Our proposed algorithm CALDERA exploits low-rank structure in weight matrices of LLMs, and is seen to boost the performance of existing methods in the literature of LLM compression. It does so by providing an additional degree of freedom to control the compression ratio \u2013 namely, the target rank $(k)$ . Theoretical guarantees show improved performance over existing benchmarks, when the matrix being compressed is inherently low-rank to begin with \u2013 something that is observed in LLMs. This also implies that CALDERA can be used for any other application scenarios that require matrix compression, as discussed in [31]. Its effectiveness largely depends on the presence of an inherent approximate low-rank structure, which is seen in many real-world matrices [37]. ", "page_idx": 30}, {"type": "text", "text": "Despite attaining a lower loss for LLM compression, since CALDERA is designed to tackle an optimization problem through an iterative process, it requires slightly more computational resources. For instance, compressing Llama-2 7B or Mistral-7B models (with rank-256) took approximately 34 GPU-hours (on NVIDIA A10G GPUs provisioned from an AWS G5 instance), and compressing Llama-2 13B took 59 GPU-hours (on a locally hosted NVIDIA A6000 GPU). Moreover, LLaMa-2 70B can be quantized via CALDERA with rank-256 factors in approximately 90 GPU hours (on an H100 from Lambda labs), which is on par with QuIP# (with RHT finetuning), which reports 100 GPU hours (on A100). It should be noted that these wallclock times can be reasonably afforded, and are not prohibitively large. Furthermore, since the compression of an LLM is a one-time computational expense, this cost becomes highly amortized considering the frequent use of LLMs for inference following the deployment of the compressed model. ", "page_idx": 30}, {"type": "text", "text": "Although CALDERA often achieves perplexities and accuracies similar to unquantized models, a gap remains, as shown in Tables 1 and 2. This indicates there is potential for enhancing quantization strategies. For example, the target rank $(k)$ can be treated as a hyper-parameter, and adjusted across different layers. Sharma et al. [33] demonstrate that such a layer-specific rank reduction can improve generalization capabilities. Additionally, improved fine-tuning strategies such as those proposed by Liu et al. [23], which reduce the gap between LoRA and full fine-tuning, can be incorporated with the low-rank adaptation step of CALDERA. Detailed investigations are left for future work. ", "page_idx": 30}, {"type": "text", "text": "Broader Impacts: Compressing models enables their deployment in resource-constrained settings, facilitating educational and technological advancements with limited infrastructure. Deploying on edge devices for inference also enhances privacy by reducing the need for data to be sent to centralized servers for inference, thereby enhancing user privacy and data security. Additionally, lower computational requirements of inference using compressed models is a step towards adoption of environment-friendly AI strategies. ", "page_idx": 30}, {"type": "text", "text": "On the other hand, as the compression is often not always lossless, any technique may result in reduced accuracy or loss of nuanced understanding, potentially impacting the reliability of the models in critical applications. Conseuqently, due diligence should be exercised when deploying these models. From a broader perspective, LLMs are quite powerful, and their easier deployment can possibly lead to misuse, such as the spread of misinformation or automated generation of harmful content. Consequently, robust regulatory frameworks are necessary, as LLMs continue becoming more accessible to the general audience. ", "page_idx": 30}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The abstract proposes an algorithm for compression large language models (LLMs) using low-precision and low-rank approximation. The algorithm, named CALDERA, presented in this work achieves that goal. The performance of CALDERA is studied theoretically by deriving explicit approximation error guarantees, as well as numerically by compressing Llama and Mistral families of models for a variety of tasks, and shows improved performance over existing benchmarks that don\u2019t take into account low-rankness of the weight matrices. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 31}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Limitations are discussed explicitly in Appendix Sec. H. Assumptions made with regard to theory are stated in the theorem statement. These assumptions hold in practice as our algorithm is seen to perform well in numerical simulations. Extensive experimentation were performed on as many setups as allowed within our computational budget. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best ", "page_idx": 31}, {"type": "text", "text": "judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 32}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: All the assumptions are stated in the main Theorem statement of $\\S4$ . A brief outline of the analysis (i.e., proof sketch) is provided in $\\S4.1$ . Complete derivations of the proof are provided in App. C. The derivation is obtained by providing lemmas that build up to the proof, and the lemmas are appropriately cross-referenced. Auxiliary lemmas and well-known results that are used the derivation are stated as auxiliary lemmas in App. G. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and lemmas that the proof relies upon should be properly referenced. ", "page_idx": 32}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Experimental details are provided in $\\S5$ and App. F. Open source datasets are used, and the codebase associated with the paper is also open-sourced. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: The codebase developed for this paper is open-sourced at https://github.com/pilancilab/caldera. The README flie provides necessary instructions to reproduce experiments from the paper. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 33}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: Experimental details are provided in Sec. 5 and App. E. Code is opensourced at https://github.com/pilancilab/caldera, along with a README file that provides instructions on how to reproduce the results from the paper. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 33}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [No] ", "page_idx": 34}, {"type": "text", "text": "Justification: As LLMs are computationally expensive to process and run inference on, error bars are not reported. However, since code is open-sourced, results can be repeated with different configurations. ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 34}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: Details are provided in Sec. 5 and App. E. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 34}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: The research conducted in the paper conforms, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. \u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. ", "page_idx": 34}, {"type": "text", "text": "\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 35}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: Broader societal impacts are discussed in App. H ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 35}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [No] ", "page_idx": 35}, {"type": "text", "text": "Justification: No new datasets are used in this work. All datasets are already publicly open-sourced. Models compressed using CALDERA will be open-sourced. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 35}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: All the creators and original owners of assets used in this paper have been properly credited, and the associated license and terms of use mentioned and respected. Details can be found in $\\S5$ and App. E. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 36}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: Compressed models will be released publicly. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 36}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: This paper does not involve crowd-sourcing or research with human subjects. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 36}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: This paper does not involve crowd-sourcing or research with human subjects. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 37}]