[{"figure_path": "aYWtfsf3uP/tables/tables_2_1.jpg", "caption": "Table 1: Comparison between OPROVI-TV and prior results on RMDP with S \u00d7 A-rectangular TV robust sets under various settings (generative model/offline dataset/interactive data collection). For the infinite horizon \u03b3-discounted RMDPs, we denote H\u2081 := (1 \u2212 \u03b3)\u207b\u00b9 as the effective horizon length. In the offline setting, Crob and Cfull represent the robust partial coverage coefficient and full coverage coefficient, respectively. In the general case, our lower bound reads intractable, meaning that there exist hard instances where it is impossible to learn the nearly optimal robust policy via a finite number of interactive samples.", "description": "This table compares the sample complexity of the proposed OPROVI-TV algorithm with existing algorithms for solving robust Markov Decision Processes (RMDPs).  It shows the sample complexity under different data oracle settings (generative model, offline dataset, interactive data collection) and model assumptions, highlighting the improvements achieved by OPROVI-TV in the interactive data collection setting under the vanishing minimal value assumption.", "section": "1 Introduction"}]