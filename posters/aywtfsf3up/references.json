{"references": [{"fullname_first_author": "Agarwal, A.", "paper_title": "Reinforcement learning: Theory and algorithms", "publication_date": "2019-00-00", "reason": "This paper provides a comprehensive overview of reinforcement learning theory and algorithms, which serves as a foundational resource for the current research."}, {"fullname_first_author": "Azar, M. G.", "paper_title": "Minimax regret bounds for reinforcement learning", "publication_date": "2017-00-00", "reason": "This paper establishes minimax optimal regret bounds for reinforcement learning, providing a benchmark for sample efficiency that is relevant to the current work."}, {"fullname_first_author": "Iyengar, G. N.", "paper_title": "Robust dynamic programming", "publication_date": "2005-00-00", "reason": "This paper introduces the concept of robust dynamic programming, which is foundational to distributionally robust reinforcement learning and directly related to the theoretical analysis in the current work."}, {"fullname_first_author": "Sutton, R. S.", "paper_title": "Reinforcement learning: An introduction", "publication_date": "2018-00-00", "reason": "This book provides a comprehensive introduction to reinforcement learning, establishing fundamental concepts and algorithms that are relevant to the background of the current research."}, {"fullname_first_author": "Yang, W.", "paper_title": "Toward theoretical understandings of robust Markov decision processes: Sample complexity and asymptotics", "publication_date": "2022-00-00", "reason": "This paper provides theoretical understandings of robust Markov decision processes with total variation distance, which is directly relevant to the main theoretical contributions of the current work."}]}