[{"type": "text", "text": "On the Mode-Seeking Properties of Langevin Dynamics ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 The Langevin Dynamics framework, which aims to generate samples from the   \n2 score function of a probability distribution, is widely used for analyzing and   \n3 interpreting score-based generative modeling. While the convergence behavior of   \n4 Langevin Dynamics under unimodal distributions has been extensively studied in   \n5 the literature, in practice the data distribution could consist of multiple distinct   \n6 modes. In this work, we investigate Langevin Dynamics in producing samples   \n7 from multimodal distributions and theoretically study its mode-seeking properties.   \n8 We prove that under a variety of sub-Gaussian mixtures, Langevin Dynamics is   \n9 unlikely to find all mixture components within a sub-exponential number of steps in   \n10 the data dimension. To reduce the mode-seeking tendencies of Langevin Dynamics,   \n11 we propose Chained Langevin Dynamics, which divides the data vector into patches   \n12 of constant size and generates every patch sequentially conditioned on the previous   \n13 patches. We perform a theoretical analysis of Chained Langevin Dynamics by   \n14 reducing it to sampling from a constant-dimensional distribution. We present   \n15 the results of several numerical experiments on synthetic and real image datasets,   \n16 supporting our theoretical results on the iteration complexities of sample generation   \n17 from mixture distributions using the chained and vanilla Langevin Dynamics. ", "page_idx": 0}, {"type": "text", "text": "18 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "19 A central task in unsupervised learning involves learning the underlying probability distribution of   \n20 training data and efficiently generating new samples from the distribution. Score-based generative   \n21 modeling (SGM) (Song et al., 2020c) has achieved state-of-the-art performance in various learning   \n22 tasks including image generation (Song and Ermon, 2019, 2020; Ho et al., 2020; Song et al., 2020a;   \n23 Ramesh et al., 2022; Rombach et al., 2022), audio synthesis (Chen et al., 2020; Kong et al., 2020),   \n24 and video generation (Ho et al., 2022; Blattmann et al., 2023). In addition to the successful empirical   \n25 results, the convergence analysis of SGM has attracted significant attention in the recent literature   \n26 (Lee et al., 2022, 2023; Chen et al., 2023; Li et al., 2023, 2024).   \n27 Stochastic gradient Langevin dynamics (SGLD) (Welling and Teh, 2011), as a fundamental method  \n28 ology to implement and interpret SGM, can produce samples from the (Stein) score function of a   \n29 probability density, i.e., the gradient of the log probability density function with respect to data. It   \n30 has been widely recognized that a pitfall of SGLD is its slow mixing rate (Wooddard et al., 2009;   \n3 Raginsky et al., 2017; Lee et al., 2018). Specifically, Song and Ermon (2019) shows that under a   \n32 multi-modal data distribution, the samples from Langevin dynamics may have an incorrect relative   \n33 density across the modes. Based on this finding, Song and Ermon (2019) proposes anneal Langevin   \n34 dynamics, which injects different levels of Gaussian noise into the data distribution and samples with   \n35 SGLD on the perturbed distribution. While outputting the correct relative density across modes can   \n36 be challenging for SGLD, a natural question is whether SGLD would be able to find all the modes of   \n37 a multi-modal distribution.   \n38 In this work, we study this question by analyzing the mode-seeking properties of SGLD. The notion   \n39 of mode-seekingness (Bishop, 2006; Ke et al., 2021; Li and Farnia, 2023) refers to the property that a   \n40 generative model captures only a subset of the modes of a multi-modal distribution. We note that   \n41 a similar problem, known as metastability, has been studied in the context of Langevin diffusion,   \n42 a continuous-time version of SGLD described by stochastic differential equation (SDE) (Bovier   \n43 et al., 2002, 2004; Gayrard et al., 2005). Specifically, Bovier et al. (2002) gave a sharp bound on   \n44 the mean hitting time of Langevin diffusion and proved that it may require exponential (in the space   \n45 dimensionality $d$ ) time for transition between modes. Regarding discrete SGLD, Lee et al. (2018)   \n46 constructed a probability distribution whose density is close to a mixture of two well-separated   \n47 isotropic Gaussians, and proved that SGLD could not find one of the two modes within an exponential   \n48 number of steps. However, further exploration of mode-seeking tendencies of SGLD and its variants   \n49 such as annealed Langevin dynamics for general distributions is still lacking in the literature.   \n50 In this work, we theoretically formulate and demonstrate the potential mode-seeking tendency of   \n51 SGLD. We begin by analyzing the convergence under a variety of Gaussian mixture probability   \n52 distributions, under which SGLD could fail to visit all the mixture components within sub-exponential   \n53 steps (in the data dimension). Subsequently, we generalize this result to mixture distributions with   \n54 sub-Gaussian modes. This generalization extends our earlier result on Gaussian mixtures to a   \n55 significantly larger family of mixture models, as the sub-Gaussian family includes any distribution   \n56 over an $\\ell_{2}$ -norm-bounded support set. Furthermore, we extend our theoretical results to anneal   \n57 Langevin dynamics with bounded noise scales.   \n58 To reduce SGLD\u2019s large iteration complexity shown under a high-dimensional input vector, we   \n59 propose Chained Langevin Dynamics (Chained- $L D_{\\epsilon}$ ). Since SGLD could suffer from the curse of   \n60 dimensionality, we decompose the sample $\\textbf{x}\\in\\mathbb{R}^{d}$ into $d/Q$ patches $\\mathbf{x}^{(1)},\\cdots,\\mathbf{x}^{(d/Q)}$ , each of   \n61 constant size $Q$ , and sequentially generate every patch $\\mathbf{x}^{(q)}$ for all $q\\in[d/Q]$ statistically conditioned   \n62 on previous patches, i.e., $P(\\mathbf{x}^{(q)}\\mid\\mathbf{x}^{(0)},\\cdot\\cdot\\cdot\\mathbf{x}^{(q-1)})$ . The combination of all patches generated from   \n63 the conditional distribution faithfully follows the probability density $P(\\mathbf{x})$ , while learning each patch   \n64 requires less cost due to the reduced dimension. We also provide a theoretical analysis of Chained-LD   \n65 by reducing the convergence of a $d$ -dimensional sample to the convergence of each patch.   \n66 Finally, we present the results of several numerical experiments to validate our theoretical findings.   \n67 For synthetic experiments, we consider moderately high-dimensional Gaussian mixture models,   \n68 where the vanilla and annealed Langevin dynamics could not find all the components within a million   \n69 steps, while Chained-LD could capture all the components with correct frequencies in $\\mathcal{O}(10^{4})$ steps.   \n70 For experiments on real image datasets, we consider a mixture of two modes by using the original   \n7 images from MNIST/Fashion-MNIST training dataset (black background and white digits/objects)   \n72 as the first mode and constructing the second mode by i.i.d. flipping the images (white background   \n73 and black digits/objects) with probability 0.5. Following from Song and Ermon (2019), we trained   \n74 a Noise Conditional Score Network (NCSN) to estimate the score function. Our numerical results   \n75 indicate that vanilla Langevin dynamics can fail to capture the two modes, as also observed by Song   \n76 and Ermon (2019). On the other hand, Chained-LD was capable of finding both modes regardless of   \n77 initialization. We summarize the contributions of this work as follows:   \n78 \u2022 Theoretically studying the mode-seeking properties of vanilla and annealed Langevin dynamics,   \n79 \u2022 Proposing Chained Langevin Dynamics (Chained-LD), which decomposes the sample into patches   \n80 and sequentially generates each patch conditioned on previous patches,   \n81 \u2022 Providing a theoretical analysis of the convergence behavior of Chained-LD,   \n82 \u2022 Numerically comparing the mode-seeking properties of vanilla, annealed, and chained Langevin   \n83 dynamics.   \n84 Notations: We use $[n]$ to denote the set $\\{1,2,\\cdots\\,,n\\}$ . Also, in the paper, $\\lVert\\cdot\\rVert$ refers to the $\\ell_{2}$ norm.   \n85 We use $\\mathbf{0}_{n}$ and ${\\mathbf{1}}_{n}$ to denote a 0-vector and 1-vector of length $n$ . We use $\\textstyle I_{n}$ to denote the identity   \n86 matrix of size $n\\times n$ . In the text, TV stands for the total variation distance. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "87 2 Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "88 Langevin Dynamics: The convergence guarantees for Langevin diffusion, a continuous version of   \n89 Langevin dynamics, are classical results extensively studied in the literature (Bhattacharya, 1978;   \n90 Roberts and Tweedie, 1996; Bakry and \u00c9mery, 1983; Bakry et al., 2008). Langevin dynamics, also   \n91 known as Langevin Monte Carlo, is a discretization of Langevin diffusion typically modeled as a   \n92 Markov Chain Monte Carlo (Welling and Teh, 2011). For unimodal distributions, e.g., the probability   \n93 density function that is log-concave or satisfies log-Sobolev inequality, the convergence of Langevin   \n94 dynamics is provably fast (Dalalyan, 2017; Durmus and Moulines, 2017; Vempala and Wibisono,   \n95 2019). However, for multimodal distributions, the non-asymptotic convergence analysis is much more   \n96 challenging (Cheng et al., 2018). Raginsky et al. (2017) gave an upper bound on the convergence time   \n97 of Langevin dynamics for arbitrary non-log-concave distributions with certain regularity assumptions,   \n98 which, however, could be exponentially large without imposing more restrictive assumptions. Lee   \n99 et al. (2018) studied the special case of a mixture of Gaussians of equal variance and provided   \n100 heuristic analysis of sampling from general non-log-concave distributions.   \n101 Mode-Seekingness of Langevin Dynamics: The investigation of the mode-seekingness of gener  \n102 ative models starts with different generative adversarial network (GAN) (Goodfellow et al., 2014)   \n103 model formulations and divergence measures, from both the practical (Goodfellow, 2016; Poole   \n104 et al., 2016) and theoretical (Shannon et al., 2020; Li and Farnia, 2023) perspectives. In the context   \n105 of Langevin dynamics, mode-seekingness is closely related to a lower bound on the transition time   \n106 between two modes, e.g., two local maximums. Bovier et al. (2002, 2004); Gayrard et al. (2005)   \n107 studied the mean hitting time of the continuous Langevin diffusion. Lee et al. (2018) proved the   \n108 existence of a mixture of two Gaussian distributions whose covariance matrices differ by a constant   \n109 factor, Langevin dynamics cannot find both modes in polynomial time.   \n110 Score-based Generative Modeling: Since Song et al. (2020b) proposed sliced score matching   \n111 which can train deep models to learn the score functions of implicit probability distributions on high  \n112 dimensional data, score-based generative modeling (SGM) has been going through a spurt of growth.   \n113 Annealed Langevin dynamics (Song and Ermon, 2019) estimates the noise score of the probability   \n114 density perturbed by Gaussian noise and utilizes stochastic gradient Langevin dynamics to generate   \n115 samples from a sequence of decreasing noise scales. Song and Ermon (2020) conducted a heuristic   \n116 analysis of the effect of noise levels on the performance of annealed Langevin dynamics. Denoising   \n117 diffusion probabilistic model (DDPM) (Ho et al., 2020) incorporates a step-by-step introduction of   \n118 random noise into data, followed by learning to reverse this diffusion process in order to generate   \n119 desired data samples from the noise. Song et al. (2020c) unified anneal Langevin dynamics and   \n120 DDPM via a stochastic differential equation. A recent line of work focuses on the non-asymptotic   \n121 convergence guarantees for SGM with an imperfect score estimation under various assumptions on   \n122 the data distribution (Block et al., 2020; De Bortoli et al., 2021; Lee et al., 2022; Chen et al., 2023;   \n123 Benton et al., 2023; Li et al., 2023, 2024). ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "124 3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "125 3.1 Langevin Dynamics ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "126 Generative modeling aims to produce samples such that their distribution is close to the underlying true   \n127 distribution $P$ . For a continuously differentiable probability density $P(\\mathbf{x})$ on $\\mathbb{R}^{d}$ , its score function is   \n128 defined as the gradient of the log probability density function (PDF) $\\nabla_{\\mathbf{x}}\\log P(\\mathbf{x})$ . Langevin diffusion   \n129 is a stochastic process defined by the stochastic differential equation (SDE) ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{x}_{t}=-\\nabla_{\\mathbf{x}}\\log P(\\mathbf{x}_{t})\\,\\mathrm{d}t+\\sqrt{2}\\,\\mathrm{d}\\mathbf{w}_{t},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "130 where ${\\bf w}_{t}$ is the Wiener process on $\\mathbb{R}^{d}$ . To generate samples from Langevin diffusion, Welling and   \n131 Teh (2011) proposed stochastic gradient Langevin dynamics (SGLD), a discretization of the SDE for   \n132 $T$ iterations. Each iteration of SGLD is defined as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{x}_{t}=\\mathbf{x}_{t-1}+\\frac{\\delta_{t}}{2}\\nabla_{\\mathbf{x}}\\log P(\\mathbf{x}_{t-1})+\\sqrt{\\delta_{t}}\\epsilon_{t},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "133 where $\\delta_{t}$ is the step size and $\\pmb{\\epsilon}_{t}\\sim\\mathcal{N}(\\mathbf{0}_{d},\\pmb{I}_{d})$ is Gaussian noise. It has been widely recognized   \n134 that Langevin diffusion could take exponential time to mix without additional assumptions on the   \n135 probability density (Bovier et al., 2002, 2004; Gayrard et al., 2005; Raginsky et al., 2017; Lee et al.,   \n136 2018). To combat the slow mixing, Song and Ermon (2019) proposed annealed Langevin dynamics   \n137 by perturbing the probability density with Gaussian noise of variance $\\sigma^{2}$ , i.e., ", "page_idx": 2}, {"type": "equation", "text": "$$\nP_{\\sigma}(\\mathbf{x}):=\\int P(\\mathbf{z})\\mathcal{N}(\\mathbf{x}\\mid\\mathbf{z},\\sigma^{2}I_{d})\\,\\mathrm{d}\\mathbf{z},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "138 and running SGLD on the perturbed data distribution $P_{\\sigma_{t}}(\\mathbf{x})$ with gradually decreasing noise levels   \n139 $\\left\\{\\sigma_{t}\\right\\}_{t\\in[T]}$ , i.e., ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{x}_{t}=\\mathbf{x}_{t-1}+\\frac{\\delta_{t}}{2}\\nabla_{\\mathbf{x}}\\log P_{\\sigma_{t}}(\\mathbf{x}_{t-1})+\\sqrt{\\delta_{t}}\\epsilon_{t},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "140 where $\\delta_{t}$ is the step size and $\\displaystyle\\pmb{\\epsilon}_{t}\\sim\\mathcal{N}(\\mathbf{0}_{d},\\pmb{I}_{d})$ is Gaussian noise. When the noise level $\\sigma$ is vanishingly   \n141 small, the perturbed distribution is close to the true distribution, i.e., $P_{\\sigma}(\\mathbf{x})\\approx P(\\mathbf{x})$ . Since we do   \n142 not have direct access to the (perturbed) score function, Song and Ermon (2019) proposed the Noise   \n143 Conditional Score Network (NCSN) $\\mathbf{s}_{\\theta}(\\mathbf{x},\\sigma)$ to jointly estimate the scores of all perturbed data   \n144 distributions, i.e., ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\forall\\sigma\\in\\{\\sigma_{t}\\}_{t\\in[T]}\\,,\\,\\,\\mathbf{s}_{\\pmb{\\theta}}(\\mathbf{x},\\sigma)\\approx\\nabla_{\\mathbf{x}}\\log P_{\\sigma}(\\mathbf{x}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "145 To train the NCSN, Song and Ermon (2019) adopted denoising score matching, which minimizes the   \n146 following loss ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}\\left(\\pmb{\\theta};\\left\\{\\sigma_{t}\\right\\}_{t\\in[T]}\\right):=\\frac{1}{2T}\\sum_{t\\in[T]}\\sigma_{t}^{2}\\mathbb{E}_{\\mathbf{x}\\sim P}\\mathbb{E}_{\\tilde{\\mathbf{x}}\\sim\\mathcal{N}(\\mathbf{x},\\sigma_{t}^{2}I_{d})}\\left[\\left|\\left|\\mathbf{s}_{\\theta}(\\tilde{\\mathbf{x}},\\sigma_{t})-\\frac{\\tilde{\\mathbf{x}}-\\mathbf{x}}{\\sigma_{t}^{2}}\\right|\\right|^{2}\\right].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "147 Assuming the NCSN has enough capacity, $\\mathbf{s}_{\\theta^{\\ast}}\\left(\\mathbf{x},\\sigma\\right)$ minimizes the loss $\\mathcal{L}\\left(\\theta;\\{\\sigma_{t}\\}_{t\\in[T]}\\right)$ if and only   \n148 if $\\mathbf{s}_{\\theta^{*}}(\\mathbf{x},\\sigma_{t})=\\nabla_{\\mathbf{x}}\\log P_{\\sigma_{t}}(\\mathbf{x})$ almost surely for all $t\\in[T]$ . ", "page_idx": 3}, {"type": "text", "text": "149 3.2 Multi-Modal Distributions ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "150 Our work focuses on multi-modal distributions. We use $\\textstyle P=\\sum_{i\\in[k]}w_{i}P^{(i)}$ to represent a mixture   \n151 of $k$ modes, where each mode $P^{(i)}$ is a probability density with frequency $w_{i}$ such that $w_{i}\\,>\\,0$   \n152 for all $i\\in[k]$ and $\\textstyle\\sum_{i\\in[k]}w_{i}=1$ . In our theoretical analysis, we consider Gaussian mixtures and   \n153 sub-Gaussian mixtures, i.e., every component $P^{(i)}$ is a Gaussian or sub-Gaussian distribution. A   \n154 probability distribution $p(\\mathbf{z})$ of dimension $d$ is defined as a sub-Gaussian distribution with parameter   \n155 $\\dot{\\nu}^{2}$ if, given the mean vector $\\boldsymbol{\\mu}:=\\mathbb{E}_{\\mathbf{z}\\sim p}[\\mathbf{z}]$ , the moment generating function (MGF) of $p$ satisfies the   \n156 following inequality for every vector $\\alpha\\in\\mathbb{R}^{d}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbf{z}\\sim p}\\left[\\exp\\left(\\alpha^{T}(\\mathbf{z}-\\pmb{\\mu})\\right]\\leq\\exp\\!\\left(\\frac{\\nu^{2}\\left\\|\\pmb{\\alpha}\\right\\|_{2}^{2}}{2}\\right).\\right.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "157 We remark that sub-Gaussian distributions include a wide variety of distributions such as Gaussian   \n158 distributions and any distribution within a bounded $\\ell_{2}$ -norm distance from the mean $\\pmb{\\mu}$ . From   \n159 equation 2 we note that the perturbed distribution is the convolution of the original distribution   \n160 and a Gaussian random variable, i.e., for random variables $\\textbf{z}\\sim p$ and $\\mathbf{t}\\sim\\mathcal{N}(\\bar{\\mathbf{0}_{d}},I_{d})$ , their sum   \n161 ${\\bf z}\\!+\\!{\\bf t}\\sim p_{\\sigma}$ follows the perturbed distribution with noise level $\\sigma$ . Therefore, a perturbed (sub)Gaussian   \n162 distribution remains (sub)Gaussian. We formalize this property in Proposition 1 and defer the proof   \n163 to Appendix A for completeness.   \n164 Proposition 1. Suppose the perturbed distribution of a $d$ -dimensional probability distribution $p$ with   \n165 noise level $\\sigma$ is $p_{\\sigma}$ , then the mean of the perturbed distribution is the same as the original distribution,   \n166 i.e., $\\mathbb{E}_{\\mathbf{z}\\sim p_{\\sigma}}[\\mathbf{z}]=\\mathbb{E}_{\\mathbf{z}\\sim p}[\\mathbf{z}]$ . If $p=\\mathcal{N}(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})$ is a Gaussian distribution, $p_{\\sigma}=\\mathcal{N}(\\pmb{\\mu},\\bar{\\Sigma}+\\sigma^{2}I_{d})$ is also   \n167 a Gaussian distribution. If $p$ is a sub-Gaussian distribution with parameter $\\nu^{2}$ , $p_{\\sigma}$ is a sub-Gaussian   \n168 distribution with parameter $(\\nu^{2}+\\sigma^{2})$ . ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "169 4 Theoretical Analysis of the Mode-Seeking Properties of Langevin Dynamics ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "170 In this section, we theoretically investigate the mode-seeking properties of vanilla and annealed   \n171 Langevin dynamics. We begin with analyzing Langevin dynamics in Gaussian mixtures. ", "page_idx": 3}, {"type": "text", "text": "172 4.1 Langevin Dynamics in Gaussian Mixtures ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "173 Assumption 1. Consider a data distribution $\\begin{array}{r}{P:=\\sum_{i=0}^{k}w_{i}P^{(i)}}\\end{array}$ as a mixture of Gaussian distribu  \n174 tions, where $1\\leq k=o(d)$ and $w_{i}>0$ is a positive constant such that $\\textstyle\\sum_{i=0}^{k}w_{i}=1$ . Suppose that   \n175 $P^{(i)}=\\mathcal{N}(\\pmb{\\mu}_{i},\\nu_{i}^{2}\\pmb{I}_{d})$ is a Gaussian distribution over $\\mathbb{R}^{d}$ for all $i\\in\\{0\\}\\cup[k]$ such that for all $i\\in[k]$ ,   \n176 $\\nu_{i}<\\nu_{0}$ and $\\begin{array}{r}{\\|\\dot{\\mu_{i}}-\\mu_{0}\\|^{2}\\leq\\frac{\\nu_{0}^{2}-\\nu_{i}^{2}}{2}\\left(\\log\\left(\\frac{\\nu_{i}^{2}}{\\nu_{0}^{2}}\\right)-\\frac{\\nu_{i}^{2}}{2\\nu_{0}^{2}}+\\frac{\\nu_{0}^{2}}{2\\nu_{i}^{2}}\\right)d}\\end{array}$ . Denote $\\nu_{\\operatorname*{max}}:=\\operatorname*{max}_{i\\in[k]}\\nu_{i}$ .   \n177 Regarding the first requirement $\\nu_{i}<\\nu_{0}$ , we first note that the probability density $p(\\mathbf{z})$ of a Gaussian   \n178 distribution $\\mathcal{N}(\\pmb{\\mu},\\nu^{2}\\pmb{I}_{d})$ decays exponentially in terms of $\\scriptstyle{\\frac{\\|\\mathbf{z}-\\mu\\|^{2}}{\\nu^{2}}}$ . When a state $\\mathbf{z}$ is sufficiently far   \n179 from all modes (i.e., $\\|\\mathbf{z}\\|\\gg\\|\\pmb{\\mu}_{i}\\|)$ , the Gaussian distribution with the largest variance (i.e., $P^{(0)}$ in   \n180 Assumption 1) dominates all other modes because $\\begin{array}{r}{\\frac{\\|\\mathbf{z}-\\pmb{\\mu_{0}}\\|^{2}}{\\nu_{0}^{2}}\\approx\\frac{\\|\\mathbf{z}\\|^{2}}{\\nu_{0}^{2}}\\gg\\frac{\\|\\mathbf{z}\\|^{2}}{\\nu_{i}^{2}}\\approx\\frac{\\|\\mathbf{z}-\\pmb{\\mu_{i}}\\|^{2}}{\\nu_{i}^{2}}}\\end{array}$ \u2225z\u2212\u00b5i\u22252. We call   \n181 such mode $P^{(0)}$ the universal mode. Therefore, if ${\\bf z}$ is initialized far from all modes, it can only   \n182 converge to the universal mode because the gradient information of other modes is masked. Once   \n183 ${\\bf z}$ enters the universal mode $P^{(0)}$ , if the step size $\\delta_{t}$ of Langevin dynamics is small (i.e., $\\delta_{t}\\leq\\nu_{0}^{2}$ ),   \n184 it would take exponential steps to escape the local mode $P^{(0)}$ ; while if the step size is large (i.e.,   \n185 $\\delta_{t}>\\nu_{0}^{2},$ , the state $\\mathbf{z}$ would again be far from all modes and thus the universal mode $P^{(0)}$ dominates   \n186 all other modes. Hence, $\\mathbf{z}$ can only visit the universal mode unless the stochastic noise $\\epsilon_{t}$ miraculously   \n187 leads it to the region of another mode. In addition, it can be verified that log \u03bd\u03bdi22 \u22122\u03bd\u03bdi22 + is a   \n188 positive constant for $\\nu_{i}<\\nu_{0}$ , thus the second requirement of Assumption 1 essentially represents   \n189 $\\left\\|\\pmb{\\mu}_{i}-\\pmb{\\mu}_{0}\\right\\|^{2}\\leq\\mathcal{O}(d)$ . We formalize the intuition in Theorem 1 and defer the proof to Appendix A.1.   \n190 Theorem 1. Consider a data distribution $P$ satisfying Assumption 1. We follow Langevin dynamics   \n191 for $T=\\exp(\\mathcal{O}(d))$ steps. Suppose the sample is initialized in $P^{(0)}$ , then with probability at least   \n192 $1-T\\cdot\\exp(-\\Omega(d))$ , we have $\\begin{array}{r}{\\|\\mathbf x_{t}-\\pmb{\\mu}_{i}\\|^{2}>\\frac{\\nu_{0}^{2}+\\nu_{\\mathrm{max}}^{2}}{2}d}\\end{array}$ for all $t\\in\\left\\{0\\right\\}\\cup\\left[T\\right]$ and $i\\in[k]$ .   \n193 We note that \u2225xt \u2212\u00b5i\u22252 > \u03bd02+2\u03bd2ma is a strong notion of mode-seekingness, since the probability   \n194 density of mode $P^{(i)}=\\mathcal{N}(\\pmb{\\mu}_{i},\\nu_{i}^{2}\\pmb{I}_{d})$ concentrates around the $\\ell_{2}$ -norm ball $\\left\\{{\\bf z}:\\|{\\bf z}-{\\pmb{\\mu}}_{i}\\|^{2}\\leq\\nu_{i}^{2}d\\right\\}$   \n195 This notion can also easily be translated into a lower bound in terms of other distance measures such   \n196 as total variation distance and Wasserstein 2-distance. Moreover, in Theorem 2 we extend the result   \n197 to annealed Langevin dynamics with bounded noise level, and the proof is deferred to Appendix A.2.   \n198 Theorem 2. Consider a data distribution $P$ satisfying Assumption 1. We follow annealed Langevin   \n199 dynamics for $T=\\exp(\\mathcal{O}(d))$ steps with noise levels $c_{\\sigma}\\geq\\sigma_{0}\\geq\\cdot\\cdot\\geq\\sigma_{T}\\geq0,$ for constant $c_{\\sigma}>0$ .   \n200 In addition, assume for all $i\\;\\in\\;[k]$ , $\\begin{array}{r}{\\left\\|{\\pmb{\\mu}}_{i}-{\\pmb{\\mu}}_{0}\\right\\|^{2}\\;\\leq\\;\\frac{\\nu_{0}^{2}-\\nu_{i}^{2}}{2}\\left(\\log\\left(\\frac{\\nu_{i}^{2}+c_{\\sigma}^{2}}{\\nu_{0}^{2}+c_{\\sigma}^{2}}\\right)-\\frac{\\nu_{i}^{2}+c_{\\sigma}^{2}}{2\\nu_{0}^{2}+c_{\\sigma}^{2}}+\\frac{\\nu_{0}^{2}+c_{\\sigma}^{2}}{2\\nu_{i}^{2}+c_{\\sigma}^{2}}\\right)d\\pmb{\\nu}_{i}\\;}\\end{array}$   \n201 Suppose that the sample is initialized in $P_{\\sigma_{0}}^{(0)}$ , then with probability at least $1-T\\cdot\\exp(-\\Omega(d))$ , we   \n202 have $\\begin{array}{r}{\\|\\mathbf{x}_{t}-\\pmb{\\mu}_{i}\\|^{2}>\\frac{\\nu_{0}^{2}+\\nu_{\\operatorname*{max}}^{2}+2\\sigma_{t}^{2}}{2}d}\\end{array}$ for all $t\\in\\{0\\}\\cup[T]$ and $i\\in[k]$ . ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "203 4.2 Langevin Dynamics in Sub-Gaussian Mixtures ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "204 We further generalize our results to sub-Gaussian mixtures. We impose the following assumptions on   \n205 the mixture. It is worth noting that these assumptions automatically hold for Gaussian mixtures.   \n206 Assumption 2. Consider a data distribution $\\begin{array}{r}{P:=\\sum_{i=0}^{k}w_{i}P^{(i)}}\\end{array}$ as a mixture of sub-Gaussian   \n207 distributions, where $1\\,\\leq\\,k\\,=\\,o(d)$ and $w_{i}~>~0$ is a positive constant such that $\\textstyle\\sum_{i=0}^{k}w_{i}\\,=\\,1$ .   \n208 Suppose that $P^{(0)}=\\mathcal{N}(\\pmb{\\mu}_{0},\\nu_{0}^{2}I_{d})$ is Gaussian and for all $i\\in[k],\\,P^{(i)}$ satisfies   \n209 i. $P^{(i)}$ is a sub-Gaussian distribution of mean $\\pmb{\\mu}_{i}$ with parameter $\\nu_{i}^{2}$ ,   \n210 ii. $P^{(i)}$ is differentiable and $\\nabla P^{(i)}(\\pmb{\\mu}_{i})=\\mathbf{0}_{d},$ ,   \n211 iii. the score function of $P^{(i)}$ is $L_{i}$ -Lipschitz such that $\\begin{array}{r}{L_{i}\\leq\\frac{c_{L}}{\\nu_{i}^{2}}}\\end{array}$ for some constant $c_{L}>0$ ,   \n212 $\\begin{array}{r l}&{i\\nu.\\ \\nu_{0}^{2}>\\operatorname*{max}\\left\\{1,\\frac{4(c_{L}+c_{\\nu}c_{L})}{c_{\\nu}(1-c_{\\nu})}\\right\\}\\frac{\\nu_{\\operatorname*{max}}}{1-c_{\\nu}}\\,f o r\\,c o n s t a n t\\,c_{\\nu}\\in\\left(0,1\\right),\\,w h e r e\\,\\nu_{\\operatorname*{max}}:=\\mathrm{ruct}}\\\\ &{\\nu.\\ \\left\\|\\mu_{i}-\\mu_{0}\\right\\|^{2}\\leq\\frac{(1-c_{\\nu})\\nu_{0}^{2}-\\nu_{i}^{2}}{2(1-c_{\\nu})}\\left(\\log\\frac{c_{\\nu}\\nu_{i}^{2}}{(c_{L}^{2}+c_{\\nu}c_{L})\\nu_{0}^{2}}-\\frac{\\nu_{i}^{2}}{2(1-c_{\\nu})\\nu_{0}^{2}}+\\frac{(1-c_{\\nu})\\nu_{0}^{2}}{2\\nu_{i}^{2}}\\right)d.}\\end{array}$ 4(cc\u03bd2L(1+\u2212c\u03bdc\u03bdc)L) 1\u03bd2\u2212macx\u03bd for constant c\u03bd \u2208(0, 1), where \u03bdmax := maxi\u2208[k] \u03bdi,   \n213   \n214 We validate the feasibility of Assumption 2.v. in Lemma 9 in the Appendix. With Assumption 2, we   \n215 show the mode-seeking tendency of Langevin dynamics under sub-Gaussian distributions in Theorem   \n216 3 and defer the proof to Appendix A.3.   \nRequire: Patch size $Q$ , dimension $d$ , conditional score function estimator $\\mathbf{s}_{\\theta}$ , number of iterations   \n$T$ , noise levels $\\left\\{\\sigma_{t}\\right\\}_{t\\in[T Q/d]}$ , step size $\\left\\{\\delta_{t}\\right\\}_{t\\in[T Q/d]}$ .   \n1: Initialize x0, and divide x0 into d/Q patches x(01 ), \u00b7 \u00b7 \u00b7 x (0d/Q)of equal size Q   \n2: for $q\\gets1$ to $d/Q$ do   \n3: for $t\\gets1$ to $T Q/d$ do   \n4: $\\begin{array}{r}{\\mathbf{x}_{t}^{(q)}\\leftarrow\\mathbf{x}_{t-1}^{(q)}+\\frac{\\delta_{t}}{2}\\mathbf{s}_{\\theta}\\left(\\mathbf{x}_{t}^{(q)}\\mid\\sigma_{t},\\mathbf{x}_{t}^{(1)},\\cdot\\cdot\\cdot\\mathbf{\\delta},\\mathbf{x}_{t}^{(q-1)}\\right)+\\sqrt{\\delta_{t}}\\epsilon_{t},}\\end{array}$ where $\\epsilon_{t}\\sim\\mathcal{N}(\\mathbf{0}_{Q},I_{Q})$   \n5: end for   \n6: $\\mathbf{x}_{0}^{(q)}\\gets\\mathbf{x}_{T Q/d}^{(q)}$   \n7: end for   \n8: return xT Q/d   \n217 Theorem 3. Consider a data distribution $P$ satisfying Assumption 2. We follow Langevin dynamics   \n218 for $T=\\exp(\\mathcal{O}(d))$ steps. Suppose the sample is initialized in $P^{(0)}$ , then with probability at least   \n219 $1-T\\cdot\\exp(-{\\mathcal{O}}(d))$ , we have $\\begin{array}{r}{\\left\\|{\\bf x}_{t}-\\pmb{\\mu}_{i}\\right\\|^{2}>\\left(\\frac{\\nu_{0}^{2}}{2}+\\frac{\\nu_{\\operatorname*{max}}^{2}}{2(1-c_{\\nu})}\\right)d^{2}}\\end{array}$ for all $t\\in\\{0\\}\\cup[T]$ and $i\\in[k]$ .   \n220 Finally, we slightly modify Assumption 2 and extend our results to annealed Langevin dynamics   \n221 under sub-Gaussian mixtures in Theorem 4. The details of Assumption 3 and the proof of Theorem 4   \n222 are deferred to Appendix A.4. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "223 Theorem 4. Consider a data distribution $P$ satisfying Assumption 3. We follow annealed Langevin 224 dynamics for $T\\;=\\;\\exp({\\mathcal{O}}(d))$ steps with noise levels $c_{\\sigma}\\;\\geq\\;\\sigma_{0}\\;\\geq\\;\\cdot\\cdot\\;\\geq\\;\\sigma_{T}\\;\\geq\\;0$ . Suppose 225 the sample is initialized in $P_{\\sigma_{0}}^{(0)}$ , then with probability at least $1\\,-\\,T\\,\\cdot\\,\\exp(-\\mathcal{O}(d))$ , we have $\\begin{array}{r}{\\left\\|\\mathbf{x}_{t}-\\pmb{\\mu}_{i}\\right\\|^{2}>\\left(\\frac{\\nu_{0}^{2}+\\sigma_{t}^{2}}{2}+\\frac{\\nu_{\\operatorname*{max}}^{2}+\\sigma_{t}^{2}}{2(1-c_{\\nu})}\\right)d}\\end{array}$ for all $t\\in\\{0\\}\\cup[T]$ and $i\\in[k]$ . ", "page_idx": 5}, {"type": "text", "text": "227 5 Chained Langevin Dynamics ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "228 To reduce the mode-seeking tendencies of vanilla and annealed Langevin dynamics, we propose   \n229 Chained Langevin Dynamics (Chained-LD) in Algorithm 1. While vanilla and annealed Langevin   \n230 dynamics apply gradient updates to all coordinates of the sample in every step, we decompose the   \n231 sample into patches of constant size and generate each patch sequentially to alleviate the exponen  \n232 tial dependency on the dimensionality. More precisely, we divide a sample $\\mathbf{x}$ into $d/Q$ patches   \n233 $\\mathbf{x}^{(1)},\\cdot\\cdot\\cdot\\mathbf{x}^{(d/Q)}$ of some constant size $Q$ , and apply annealed Langevin dynamics to sample each   \n234 patch $\\mathbf{x}^{(q)}$ (for $q\\in[d/Q])$ from the conditional distribution $P(\\mathbf{x}^{(q)}\\mid\\mathbf{x}^{(1)},\\cdot\\cdot\\cdot\\mathbf{x}^{(q-1)})$ .   \n235 An ideal conditional score function estimator $\\mathbf{s}_{\\theta}$ could jointly estimate the scores of all perturbed   \n236 conditional patch distribution, i.e., $\\forall\\sigma\\in\\{\\sigma_{t}\\}_{t\\in[T Q/d]}\\,,q\\in[d/Q]$ , ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{s}_{\\theta}\\left(\\mathbf{x}^{\\left(q\\right)}\\mid\\sigma,\\mathbf{x}^{\\left(1\\right)},\\cdot\\cdot\\cdot\\mathbf{\\Lambda},\\mathbf{x}^{\\left(q-1\\right)}\\right)\\approx\\nabla_{\\mathbf{x}^{\\left(q\\right)}}\\log P_{\\sigma}(\\mathbf{x}^{\\left(q\\right)}\\mid\\mathbf{x}^{\\left(1\\right)},\\cdot\\cdot\\cdot\\mathbf{x}^{\\left(q-1\\right)}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "237 Following from Song and Ermon (2019), we use the denoising score matching to train the estimator.   \n238 For a given $\\sigma$ , the denoising score matching objective is ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\ell(\\theta;\\sigma):=\\frac{1}{2}\\mathbb{E}_{\\mathbf{x}\\sim P}\\mathbb{E}_{\\widetilde{\\mathbf{x}}\\sim N(\\mathbf{x},\\sigma^{2}I_{d})}\\sum_{q\\in[d/Q]}\\left[\\left\\|\\mathbf{s}_{\\theta}\\left(\\mathbf{x}^{(q)}\\mid\\sigma,\\mathbf{x}^{(1)},\\cdots,\\mathbf{x}^{(q-1)}\\right)-\\frac{\\widetilde{\\mathbf{x}}^{(q)}-\\mathbf{x}^{(q)}}{\\sigma^{2}}\\right\\|^{2}\\right].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "239 Then, combining the objectives gives the following loss ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}\\left(\\pmb{\\theta};\\{\\sigma_{t}\\}_{t\\in[T Q/d]}\\right):=\\frac{d}{T Q}\\sum_{t\\in[T Q/d]}\\sigma_{t}^{2}\\ell(\\pmb{\\theta};\\sigma_{t}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "240 As shown in Vincent (2011), an estimator $\\mathbf{s}_{\\theta}$ with enough capacity minimizes the loss $\\mathcal{L}$ if and only if   \n241 $\\mathbf{s}_{\\theta}$ outputs the scores of all perturbed conditional patch distribution almost surely. Ideally, if a sampler   \n242 perfectly generates every patch, combining all patches gives a sample from the original distribution   \n243 since $\\begin{array}{r}{\\dot{P(\\mathbf{x})}=\\prod_{q\\in[d/Q]}\\dot{P}\\big(\\mathbf{x}^{(q)}\\mid\\mathbf{x}^{(1)},\\cdots\\mathbf{x}^{(q-1)}\\big)}\\end{array}$ . In Theorem 5 we give a linear reduction from   \n244 producing samples of dimension $d$ using Chained-LD to learning the distribution of a $Q$ -dimensional   \n245 variable for constant $Q$ . The proof of Theorem 5 is deferred to Appendix A.5.   \n246 Theorem 5. Consider a sampler algorithm taking the first $q-1$ patches $\\mathbf{x}^{(1)},\\cdots,\\mathbf{x}^{(q-1)}$ as input   \n247 and outputing a sample of the next patch $\\mathbf{x}^{(q)}$ with probability $\\hat{P}\\left(\\mathbf{x}^{(q)}\\mid\\mathbf{x}^{(1)},\\cdots,\\mathbf{x}^{(q-1)}\\right)$ for all   \n248 $q\\in[d/Q]$ . Suppose that for every $q\\in[d/Q]$ and any given previous patches $\\mathbf{x}^{(1)},\\cdot\\cdot\\cdot,\\mathbf{x}^{(q-1)}$ , the   \n249 sampler algorithm can achieve ", "page_idx": 5}, {"type": "image", "img_path": "RNeb41ybNL/tmp/d4accd58fe4acf0be394442508620c666510510049c66b769e1bf8de62a86d70.jpg", "img_caption": ["Figure 1: Samples from a mixture of three Gaussian modes generated by vanilla, annealed, and chained Langevin dynamics. Three axes are $\\ell_{2}$ distance from samples to the mean of the three modes. The samples are initialized in mode 0. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\nT V\\left(\\hat{P}\\left(\\mathbf{x}^{(q)}\\mid\\mathbf{x}^{(1)},\\cdot\\cdot\\cdot\\mathbf{\\mu},\\mathbf{x}^{(q-1)}\\right),P\\left(\\mathbf{x}^{(q)}\\mid\\mathbf{x}^{(1)},\\cdot\\cdot\\cdot\\mathbf{\\mu},\\mathbf{x}^{(q-1)}\\right)\\right)\\leq\\varepsilon\\cdot\\frac{Q}{d}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "250 in $\\tau(\\varepsilon,d)$ iterations for some $\\varepsilon>0$ . Then, equipped with the sampler algorithm, the Chained- $L D$   \n251 algorithm in $\\textstyle{\\frac{d}{Q}}\\cdot\\tau(\\varepsilon,d)$ iterations can achieve ", "page_idx": 6}, {"type": "equation", "text": "$$\nT V\\left({\\hat{P}}(\\mathbf{x}),P(\\mathbf{x})\\right)\\leq\\varepsilon.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "252 6 Numerical Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "253 In this section, we empirically evaluated the mode-seeking tendencies of vanilla, annealed, and   \n254 chained Langevin dynamics. We performed numerical experiments on synthetic Gaussian mixture   \n255 models and real image datasets including MNIST (LeCun, 1998) and Fashion-MNIST (Xiao et al.,   \n256 2017). Details on the experiment setup are deferred to Appendix B.   \n257 Synthetic Gaussian mixture model: We define the data distribution $P$ as a mixture of three Gaussian   \n258 components in dimension $d=100$ , where mode 0 defined as $P^{(0)}=\\mathcal{N}(\\mathbf{0}_{d},3I_{d})$ is the universal   \n259 mode with the largest variance, and mode 1 and mode 2 are respectively defined as $\\dot{P}^{(1)}=\\mathcal{N}(\\mathbf{1}_{d},\\pmb{I}_{d})$   \n260 and $P^{(2)}=\\mathcal{N}(-\\mathbf{1}_{d},\\pmb{I}_{d})$ . The frequencies of the three modes are 0.2, 0.4 and 0.4, i.e., ", "page_idx": 6}, {"type": "image", "img_path": "RNeb41ybNL/tmp/d6ff72cd2389c766b60c949cf405d81978fc6163af4e20e4d7054e64b99df4ff.jpg", "img_caption": ["Figure 2: Samples from a mixture distribution of the original and flipped images from the MNIST dataset generated by vanilla, annealed, and chained Langevin dynamics. The samples are initialized as original images from MNIST. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "equation", "text": "$$\nP=0.2P^{(0)}+0.4P^{(1)}+0.4P^{(2)}=0.2\\mathcal{N}(\\mathbf{0}_{d},3{I}_{d})+0.4\\mathcal{N}(\\mathbf{1}_{d},{I}_{d})+0.4\\mathcal{N}(-\\mathbf{1}_{d},{I}_{d}).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "261 As shown in Figure 1, vanilla and annealed Langevin dynamics cannot find mode 1 or 2 within $10^{6}$   \n262 iterations if the sample is initialized in mode 0, while chained Langevin dynamics can find the other   \n263 two modes in 1000 steps and correctly recover their frequencies as gradually increasing the number   \n264 of iterations. In Appendix B.1 we present additional experiments on samples initialized in mode 1 or   \n265 2, which also verify the mode-seeking tendencies of vanilla and annealed Langevin dynamics.   \n266 Image datasets: We construct the distribution as a mixture of two modes by using the original images   \n267 from MNIST/Fashion-MNIST training dataset (black background and white digits/objects) as the   \n268 first mode and constructing the second mode by i.i.d. randomly filpping an image (white background   \n269 and black digits/objects) with probability 0.5. Regarding the neural network architecture of the score   \n270 function estimator, for vanilla and annealed Langevin dynamics we use U-Net (Ronneberger et al.,   \n271 2015) following from Song and Ermon (2019). For chained Langevin dynamics, we proposed to use   \n272 Recurrent Neural Network (RNN) architectures. We note that for a sequence of inputs, the output of   \n273 RNN from the previous step is fed as input to the current step. Therefore, in the scenario of chained   \n274 Langevin dynamics, the hidden state of RNN contains information about the previous patches and   \n275 allows the network to estimate the conditional score function $\\nabla_{\\mathbf{x}^{(q)}}\\log P(\\mathbf{x}^{(q)}\\mid\\mathbf{x}^{(1)},\\cdot\\cdot\\cdot\\mathbf{x}^{(q-1)})$ .   \n276 More implementation details are deferred to Appendix B.2.   \n277 The numerical results on image datasets are shown in Figures 2 and 3. Vanilla Langevin dynamics   \n278 fails to generate reasonable samples, as also observed in Song and Ermon (2019). When the sample   \n279 is initialized as original images from the datasets, annealed Langevin dynamics tends to generate   \n280 samples from the same mode, while chained Langevin dynamics can generate samples from both   \n281 modes. Additional experiments are deferred to Appendix B.2. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "image", "img_path": "RNeb41ybNL/tmp/1bb7632dbb4797b4b55ed6cf8c0f4fe997b14dc6fe4ef49a8f2c72e7245fbf1a.jpg", "img_caption": ["Figure 3: Samples from a mixture distribution of the original and flipped images from the FashionMNIST dataset generated by vanilla, annealed, and chained Langevin dynamics. The samples are initialized as original images from Fashion-MNIST. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "282 7 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "283 In this work, we theoretically and numerically studied the mode-seeking properties of vanilla and   \n284 annealed Langevin dynamics sampling methods under a multi-modal distribution. We characterized   \n285 Gaussian and sub-Gaussian mixture models under which Langevin dynamics are unlikely to find all   \n286 the components within a sub-exponential number of iterations. To reduce the mode-seeking tendency   \n287 of vanilla Langevin dynamics, we proposed Chained Langevin Dynamics (Chained-LD) and analyzed   \n288 its convergence behavior. Studying the connections between Chained-LD and denoising diffusion   \n289 models will be an interesting topic for future exploration. ", "page_idx": 8}, {"type": "text", "text": "290 Limitations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "291 Our RNN-based implementation of Chained-LD is currently limited to image data generation tasks.   \n292 An interesting future direction is to extend the application of Chained-LD to other domains such as   \n293 audio and text data. Another future direction could be to study the convergence of Chained-LD under   \n294 an imperfect score estimation which we did not address in our analysis. ", "page_idx": 8}, {"type": "text", "text": "295 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "296 Bakry, D., Barthe, F., Cattiaux, P., and Guillin, A. (2008). A simple proof of the poincar\u00e9 inequality   \n297 for a large class of probability measures. Electronic Communications in Probability [electronic   \n298 only], 13:60\u201366.   \n299 Bakry, D. and \u00c9mery, M. (1983). Diffusions hypercontractives. Seminaire de Probabilites XIX, page   \n300 177.   \n301 Benton, J., De Bortoli, V., Doucet, A., and Deligiannidis, G. (2023). Linear convergence bounds for   \n302 diffusion models via stochastic localization. arXiv preprint arXiv:2308.03686.   \n303 Bhattacharya, R. (1978). Criteria for recurrence and existence of invariant measures for multidimen  \n304 sional diffusions. The Annals of Probability, pages 541\u2013553.   \n305 Bishop, C. M. (2006). Pattern recognition and machine learning. Springer google schola, 2:645\u2013678.   \n306 Blattmann, A., Rombach, R., Ling, H., Dockhorn, T., Kim, S. W., Fidler, S., and Kreis, K. (2023).   \n307 Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings   \n308 of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22563\u201322575.   \n309 Block, A., Mroueh, Y., and Rakhlin, A. (2020). Generative modeling with denoising auto-encoders   \n310 and langevin sampling. arXiv preprint arXiv:2002.00107.   \n311 Bovier, A., Eckhoff, M., Gayrard, V., and Klein, M. (2002). Metastability and low lying spectra in   \n312 reversible markov chains. Communications in mathematical physics, 228:219\u2013255.   \n313 Bovier, A., Eckhoff, M., Gayrard, V., and Klein, M. (2004). Metastability in reversible diffusion   \n314 processes i: Sharp asymptotics for capacities and exit times. Journal of the European Mathematical   \n315 Society, 6(4):399\u2013424.   \n316 Chen, N., Zhang, Y., Zen, H., Weiss, R. J., Norouzi, M., and Chan, W. (2020). Wavegrad: Estimating   \n317 gradients for waveform generation. In International Conference on Learning Representations.   \n318 Chen, S., Chewi, S., Li, J., Li, Y., Salim, A., and Zhang, A. R. (2023). Sampling is as easy as learning   \n319 the score: theory for diffusion models with minimal data assumptions. In International Conference   \n320 on Learning Representations.   \n321 Cheng, X., Chatterji, N. S., Abbasi-Yadkori, Y., Bartlett, P. L., and Jordan, M. I. (2018). Sharp con  \n322 vergence rates for langevin dynamics in the nonconvex setting. arXiv preprint arXiv:1805.01648.   \n323 Dalalyan, A. S. (2017). Theoretical guarantees for approximate sampling from smooth and log  \n324 concave densities. Journal of the Royal Statistical Society Series B: Statistical Methodology,   \n325 79(3):651\u2013676.   \n326 De Bortoli, V., Thornton, J., Heng, J., and Doucet, A. (2021). Diffusion schr\u00f6dinger bridge with   \n327 applications to score-based generative modeling. Advances in Neural Information Processing   \n328 Systems, 34:17695\u201317709.   \n329 Durmus, A. and Moulines, \u00c9. (2017). Nonasymptotic convergence analysis for the unadjusted   \n330 langevin algorithm. The Annals of Applied Probability, 27(3):1551\u20131587.   \n331 Gayrard, V., Bovier, A., and Klein, M. (2005). Metastability in reversible diffusion processes   \n332 ii: Precise asymptotics for small eigenvalues. Journal of the European Mathematical Society,   \n333 7(1):69\u201399.   \n334 Goodfellow, I. (2016). Nips 2016 tutorial: Generative adversarial networks. arXiv preprint   \n335 arXiv:1701.00160.   \n336 Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A.,   \n337 and Bengio, Y. (2014). Generative adversarial nets. Advances in neural information processing   \n338 systems, 27.   \n339 Ho, J., Chan, W., Saharia, C., Whang, J., Gao, R., Gritsenko, A., Kingma, D. P., Poole, B., Norouzi,   \n340 M., Fleet, D. J., et al. (2022). Imagen video: High definition video generation with diffusion   \n341 models. arXiv preprint arXiv:2210.02303.   \n342 Ho, J., Jain, A., and Abbeel, P. (2020). Denoising diffusion probabilistic models. Advances in neural   \n343 information processing systems, 33:6840\u20136851.   \n344 Ke, L., Choudhury, S., Barnes, M., Sun, W., Lee, G., and Srinivasa, S. (2021). Imitation learning   \n345 as f-divergence minimization. In Algorithmic Foundations of Robotics XIV: Proceedings of the   \n346 Fourteenth Workshop on the Algorithmic Foundations of Robotics 14, pages 313\u2013329. Springer.   \n347 Kong, Z., Ping, W., Huang, J., Zhao, K., and Catanzaro, B. (2020). Diffwave: A versatile diffusion   \n348 model for audio synthesis. In International Conference on Learning Representations.   \n349 Laurent, B. and Massart, P. (2000). Adaptive estimation of a quadratic functional by model selection.   \n350 Annals of statistics, pages 1302\u20131338.   \n351 LeCun, Y. (1998). The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/.   \n352 Lee, H., Lu, J., and Tan, Y. (2022). Convergence for score-based generative modeling with polynomial   \n353 complexity. Advances in Neural Information Processing Systems, 35:22870\u201322882.   \n354 Lee, H., Lu, J., and Tan, Y. (2023). Convergence of score-based generative modeling for general   \n355 data distributions. In International Conference on Algorithmic Learning Theory, pages 946\u2013985.   \n356 PMLR.   \n357 Lee, H., Risteski, A., and Ge, R. (2018). Beyond log-concavity: Provable guarantees for sampling   \n358 multi-modal distributions using simulated tempering langevin monte carlo. Advances in neural   \n359 information processing systems, 31.   \n360 Li, C. T. and Farnia, F. (2023). Mode-seeking divergences: theory and applications to gans. In   \n361 International Conference on Artificial Intelligence and Statistics, pages 8321\u20138350. PMLR.   \n362 Li, G., Huang, Y., Efimov, T., Wei, Y., Chi, Y., and Chen, Y. (2024). Accelerating convergence of   \n363 score-based diffusion models, provably. arXiv preprint arXiv:2403.03852.   \n364 Li, G., Wei, Y., Chen, Y., and Chi, Y. (2023). Towards non-asymptotic convergence for diffusion-based   \n365 generative models. In The Twelfth International Conference on Learning Representations.   \n366 Lin, G., Milan, A., Shen, C., and Reid, I. (2017). Refinenet: Multi-path refinement networks for   \n367 high-resolution semantic segmentation. In Proceedings of the IEEE conference on computer vision   \n368 and pattern recognition, pages 1925\u20131934.   \n369 Poole, B., Alemi, A. A., Sohl-Dickstein, J., and Angelova, A. (2016). Improved generator objectives   \n370 for gans. arXiv preprint arXiv:1612.02780.   \n371 Raginsky, M., Rakhlin, A., and Telgarsky, M. (2017). Non-convex learning via stochastic gradient   \n372 langevin dynamics: a nonasymptotic analysis. In Conference on Learning Theory, pages 1674\u2013   \n373 1703. PMLR.   \n374 Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M. (2022). Hierarchical text-conditional   \n375 image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3.   \n376 Roberts, G. O. and Tweedie, R. L. (1996). Exponential convergence of langevin distributions and   \n377 their discrete approximations. Bernoulli, pages 341\u2013363.   \n378 Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. (2022). High-resolution image   \n379 synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer   \n380 vision and pattern recognition, pages 10684\u201310695.   \n381 Ronneberger, O., Fischer, P., and Brox, T. (2015). U-net: Convolutional networks for biomedical   \n382 image segmentation. In Medical image computing and computer-assisted intervention\u2013MICCAI   \n383 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III   \n384 18, pages 234\u2013241. Springer.   \n385 Sak, H., Senior, A., and Beaufays, F. (2014). Long short-term memory based recurrent neural network   \n386 architectures for large vocabulary speech recognition. arXiv preprint arXiv:1402.1128.   \n387 Shannon, M., Poole, B., Mariooryad, S., Bagby, T., Battenberg, E., Kao, D., Stanton, D., and   \n388 Skerry-Ryan, R. (2020). Non-saturating gan training as divergence minimization. arXiv preprint   \n389 arXiv:2010.08029.   \n390 Song, J., Meng, C., and Ermon, S. (2020a). Denoising diffusion implicit models. arXiv preprint   \n391 arXiv:2010.02502.   \n392 Song, Y. and Ermon, S. (2019). Generative modeling by estimating gradients of the data distribution.   \n393 Advances in neural information processing systems, 32.   \n394 Song, Y. and Ermon, S. (2020). Improved techniques for training score-based generative models.   \n395 Advances in neural information processing systems, 33:12438\u201312448.   \n396 Song, Y., Garg, S., Shi, J., and Ermon, S. (2020b). Sliced score matching: A scalable approach to   \n397 density and score estimation. In Uncertainty in Artificial Intelligence, pages 574\u2013584. PMLR.   \n398 Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. (2020c). Score  \n399 based generative modeling through stochastic differential equations. In International Conference   \n400 on Learning Representations.   \n401 Vempala, S. and Wibisono, A. (2019). Rapid convergence of the unadjusted langevin algorithm:   \n402 Isoperimetry suffices. Advances in neural information processing systems, 32.   \n403 Vincent, P. (2011). A connection between score matching and denoising autoencoders. Neural   \n404 computation, 23(7):1661\u20131674.   \n405 Welling, M. and Teh, Y. W. (2011). Bayesian learning via stochastic gradient langevin dynamics. In   \n406 Proceedings of the 28th international conference on machine learning (ICML-11), pages 681\u2013688.   \n407 Citeseer.   \n408 Wooddard, D. B., Schmidler, S. C., and Huber, M. (2009). Conditions for rapid mixing of parallel and   \n409 simulated tempering on multimodal distributions. The Annals of Applied Probability, 19(2):617\u2013   \n410 640.   \n411 Xiao, H., Rasul, K., and Vollgraf, R. (2017). Fashion-mnist: a novel image dataset for benchmarking   \n412 machine learning algorithms. arXiv preprint arXiv:1708.07747. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "413 A Theoretical Analysis on the Mode-Seeking Tendency of Langevin Dynamics ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "414 We begin by introducing some well-established lemmas used in our proof. We first provide the proof   \n415 of Proposition 1 for completeness: ", "page_idx": 12}, {"type": "text", "text": "416 Proof of Proposition $^{\\,l}$ . By the definition in equation 2, we have ", "page_idx": 12}, {"type": "equation", "text": "$$\np_{\\sigma}({\\bf z})=\\int p({\\bf t})\\mathcal{N}({\\bf z}\\mid{\\bf t},\\sigma^{2}I_{d})\\,\\mathrm{d}{\\bf t}=\\int p({\\bf t})\\mathcal{N}({\\bf z}-{\\bf t}\\mid{\\bf0}_{d},\\sigma^{2}I_{d})\\,\\mathrm{d}{\\bf t}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "417 For random variables $\\mathbf{t}\\sim p$ and $\\mathbf{y}\\sim\\mathcal{N}(\\mathbf{0}_{d},\\mathbf{I}_{d})$ , their sum $\\mathbf{z}=\\mathbf{t}+\\mathbf{y}\\sim p_{\\sigma}$ follows the perturbed   \n418 distribution with noise level $\\sigma$ . Therefore, ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbf{z}\\sim p_{\\sigma}}[\\mathbf{z}]=\\mathbb{E}_{(\\mathbf{t}+\\mathbf{y})\\sim p_{\\sigma}}[\\mathbf{t}+\\mathbf{y}]=\\mathbb{E}_{\\mathbf{t}\\sim p}[\\mathbf{t}]+\\mathbb{E}_{\\mathbf{y}\\sim\\mathcal{N}(\\mathbf{0}_{d},I_{d})}[\\mathbf{y}]=\\mathbb{E}_{\\mathbf{t}\\sim p}[\\mathbf{t}].\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "419 If $\\mathbf{t}\\sim p={\\mathcal{N}}({\\boldsymbol{\\mu}},{\\boldsymbol{\\Sigma}})$ follows a Gaussian distribution, we have $\\mathbf{z}=\\mathbf{t}+\\mathbf{y}\\sim p_{\\sigma}=\\mathcal{N}(\\pmb{\\mu},\\pmb{\\Sigma}+\\sigma^{2}\\pmb{I}_{d})$ .   \n420 If $p$ is a sub-Gaussian distribution with parameter $\\nu^{2}$ , we have $\\mathbf{z}=\\mathbf{t}+\\mathbf{y}\\sim p_{\\sigma}$ is a sub-Gaussian   \n421 distribution with parameter $(\\nu^{2}+\\sigma^{2})$ . Hence we obtain Proposition 1. \u53e3 ", "page_idx": 12}, {"type": "text", "text": "422 We use the following lemma on the tail bound for multivariate Gaussian random variables. ", "page_idx": 12}, {"type": "text", "text": "423 Lemma 1 (Lemma 1, Laurent and Massart (2000)). Suppose that a random variable $\\mathbf{z}\\sim\\mathcal{N}(\\mathbf{0}_{d},\\pmb{I}_{d})$ .   \n424 Then for any $\\lambda>0$ , ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left(\\|\\mathbf{z}\\|^{2}\\geq d+2\\sqrt{d\\lambda}+2\\lambda\\right)\\leq\\exp(-\\lambda),}\\\\ {\\mathbb{P}\\left(\\|\\mathbf{z}\\|^{2}\\leq d-2\\sqrt{d\\lambda}\\right)\\leq\\exp(-\\lambda).}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "425 We also use a tail bound for one-dimensional Gaussian random variables and provide the proof here   \n426 for completeness. ", "page_idx": 12}, {"type": "text", "text": "427 Lemma 2. Suppose a random variable $Z\\sim\\mathcal{N}(0,1)$ . Then for any $t>0$ , ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbb{P}(Z\\geq t)=\\mathbb{P}(Z\\leq-t)\\leq{\\frac{\\exp(-t^{2}/2)}{\\sqrt{2\\pi}t}}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "428 Proof of Lemma 2. Since $\\textstyle{\\frac{z}{t}}\\geq1$ for all $z\\in[t,\\infty)$ , we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbb{P}(Z\\geq t)=\\frac{1}{\\sqrt{2\\pi}}\\int_{t}^{\\infty}\\exp\\left(-\\frac{z^{2}}{2}\\right)\\,\\mathrm{d}z\\leq\\frac{1}{\\sqrt{2\\pi}}\\int_{t}^{\\infty}\\frac{z}{t}\\exp\\left(-\\frac{z^{2}}{2}\\right)\\,\\mathrm{d}z=\\frac{\\exp(-t^{2}/2)}{\\sqrt{2\\pi}t}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "429 Since the Gaussian distribution is symmetric, we have $\\mathbb{P}(Z\\geq t)=\\mathbb{P}(Z\\leq-t)$ . Hence we obtain the   \n430 desired bound. \u53e3 ", "page_idx": 12}, {"type": "text", "text": "431 A.1 Proof of Theorem 1: Langevin Dynamics under Gaussian Mixtures ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "432 Without loss of generality, we assume that $\\pmb{\\mu}_{0}=\\mathbf{0}_{d}$ for simplicity. Let $r$ and $n$ respectively denote   \n433 the rank and nullity of the vector space $\\{\\pmb{\\mu}_{i}\\}_{i\\in[k]}$ , then we have $r+n=d$ and $0\\leq r\\leq k=o(d)$ .   \n434 Denote $\\mathbf{R}\\in\\mathbb{R}^{d\\times r}$ an orthonormal basis of the vector space $\\{\\pmb{\\mu}_{i}\\}_{i\\in[k]}$ , and denote $\\mathbf{N}\\in\\mathbb{R}^{d\\times n}$ an   \n435 orthonormal basis of the null space of $\\{\\pmb{\\mu}_{i}\\}_{i\\in[k]}$ . Now consider decomposing the sample $\\mathbf{x}_{t}$ by ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbf{r}_{t}:=\\mathbf{R}^{T}\\mathbf{x}_{t},\\,\\mathrm{and}\\,\\,\\mathbf{n}_{t}:=\\mathbf{N}^{T}\\mathbf{x}_{t},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "436 where $\\mathbf{r}_{t}\\in\\mathbb{R}^{r}$ , $\\mathbf{n}_{t}\\in\\mathbb{R}^{n}$ . Then we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbf{x}_{t}=\\mathbf{R}\\mathbf{r}_{t}+\\mathbf{N}\\mathbf{n}_{t}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "437 Similarly, we decompose the noise $\\epsilon_{t}$ into ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\epsilon_{t}^{(\\mathbf{r})}:=\\mathbf{R}^{T}\\epsilon_{t},\\,\\mathrm{and}\\;\\epsilon_{t}^{(\\mathbf{n})}:=\\mathbf{N}^{T}\\epsilon_{t},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "438 where $\\epsilon_{t}^{(\\mathbf{r})}\\in\\mathbb{R}^{r}$ , $\\epsilon_{t}^{(\\mathbf{n})}\\in\\mathbb{R}^{n}$ . Then we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\epsilon_{t}=\\mathbf{R}\\epsilon_{t}^{(\\mathbf{r})}+\\mathbf{N}\\epsilon_{t}^{(\\mathbf{n})}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "439 Since a linear combination of a Gaussian random variable still follows Gaussian distribution, by   \n440 $\\pmb{\\epsilon}_{t}\\sim\\mathcal{N}(\\mathbf{0}_{d},\\pmb{I}_{d})$ , ${\\bf R}^{T}{\\bf R}={\\cal I}_{r}$ , and ${\\bf N}^{T}{\\bf N}=I_{n}$ we obtain ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\epsilon_{t}^{(\\mathbf{r})}\\sim\\mathcal{N}(\\mathbf{0}_{r},I_{r}),\\,\\mathrm{and}\\;\\epsilon_{t}^{(\\mathbf{n})}\\sim\\mathcal{N}(\\mathbf{0}_{n},I_{n}).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "441 By the definition of Langevin dynamics in equation 1, the two components of $\\mathbf{x}_{t}$ follow from the   \n442 update rule: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle{\\bf n}_{t}={\\bf n}_{t-1}+\\frac{\\delta_{t}}{2}{\\bf N}^{T}\\nabla_{\\bf x}\\log P({\\bf x}_{t-1})+\\sqrt{\\delta_{t}}\\epsilon_{t}^{({\\bf n})},}\\\\ {\\displaystyle{\\bf r}_{t}={\\bf r}_{t-1}+\\frac{\\delta_{t}}{2}{\\bf R}^{T}\\nabla_{\\bf x}\\log P({\\bf x}_{t-1})+\\sqrt{\\delta_{t}}\\epsilon_{t}^{({\\bf r})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "443 It is worth noting that since $\\mathbf{N}^{T}\\pmb{\\mu}_{i}=\\mathbf{0}_{n}$ . To show $\\begin{array}{r}{\\|\\mathbf x_{t}-\\pmb{\\mu}_{i}\\|^{2}>\\frac{\\nu_{0}^{2}+\\nu_{\\mathrm{max}}^{2}}{2}d}\\end{array}$ , it suffices to prove ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\left\\|\\mathbf{n}_{t}\\right\\|^{2}>\\frac{\\nu_{0}^{2}+\\nu_{\\mathrm{max}}^{2}}{2}d.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "444 We start by proving that the initialization of the state $\\mathbf{x}_{\\mathrm{0}}$ has a large norm on the null space with high   \n445 probability in the following proposition.   \n446 Proposition 2. Suppose that a sample $\\mathbf{x}_{\\mathrm{0}}$ is initialized in the distribution $P^{(0)}$ , i.e., ${\\bf x}_{0}\\sim P^{(0)}$ , then   \n447 for any constant $\\nu_{\\mathrm{max}}<\\nu_{0}$ , with probability at least $1-\\exp(-\\Omega(d))$ , we have $\\begin{array}{r}{\\left\\|\\mathbf{n}_{0}\\right\\|^{2}\\geq\\frac{3\\nu_{0}^{2}+\\nu_{\\mathrm{max}}^{2}}{4}d.}\\end{array}$ .   \n448 Proof of Proposition 2. Since $\\mathbf{x}_{0}\\sim P^{(0)}=\\mathcal{N}(\\mathbf{0}_{d},\\nu_{0}^{2}I_{d})$ and ${\\bf N}^{T}{\\bf N}=I_{n}$ , we know ${\\bf n}_{0}={\\bf N}^{T}{\\bf x}_{0}\\sim$   \n449 $\\mathcal{N}(\\dot{\\mathbf{0}_{n}},\\dot{\\nu}_{0}^{2}I_{n})$ . Therefore, by Lemma 1 we can bound ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(\\|\\mathbf{n}_{0}\\|^{2}\\leq\\frac{3\\nu_{0}^{2}+\\nu_{\\operatorname*{max}}^{2}}{4}d\\right)=\\mathbb{P}\\left(\\frac{\\|\\mathbf{n}_{0}\\|^{2}}{\\nu_{0}^{2}}\\leq d-2\\sqrt{d\\cdot\\left(\\frac{\\nu_{0}^{2}-\\nu_{\\operatorname*{max}}^{2}}{8\\nu_{0}^{2}}\\right)^{2}d}\\right)}\\\\ &{\\phantom{\\mathbb{P}\\left(\\|\\mathbf{n}_{0}\\|^{2}\\leq\\frac{3\\nu_{0}^{2}+\\nu_{\\operatorname*{max}}^{2}}{4}d\\right)}\\leq\\mathbb{P}\\left(\\frac{\\|\\mathbf{n}_{0}\\|^{2}}{\\nu_{0}^{2}}\\leq n-2\\sqrt{n\\left(\\frac{\\nu_{0}^{2}-\\nu_{\\operatorname*{max}}^{2}}{8\\nu_{0}^{2}}\\right)^{2}\\frac{d}{2}}\\right)}\\\\ &{\\phantom{\\mathbb{P}\\left(\\|\\mathbf{n}_{0}\\|^{2}\\leq\\frac{3\\nu_{0}^{2}+\\nu_{\\operatorname*{max}}^{2}}{4}d\\right)}\\leq\\exp\\left(-\\left(\\frac{\\nu_{0}^{2}-\\nu_{\\operatorname*{max}}^{2}}{8\\nu_{0}^{2}}\\right)^{2}\\frac{d}{2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "450 where the second last step follows from the assumption $d-n=r=o(d)$ . Hence we complete the   \n451 proof of Proposition 2. \u53e3   \n452 Then, with the assumption that the initialization satisfies $\\begin{array}{r}{\\left\\lVert\\mathbf{n}_{0}\\right\\rVert^{2}\\geq\\frac{3\\nu_{0}^{2}+\\nu_{\\mathrm{max}}^{2}}{4}d}\\end{array}$ , the following proposi  \n453 tion shows that remains large with high probability. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "454 Proposition 3. Consider a data distribution $P$ satisfies the constraints specified in Theorem 1. 455 We follow the Langevin dynamics for $T\\;=\\;\\exp({\\mathcal{O}}(d))$ steps. Suppose that the initial sample 445567 $\\begin{array}{r}{\\left\\|\\mathbf{n}_{0}\\right\\|^{2}\\;\\geq\\;\\frac{3\\nu_{0}^{2}+\\nu_{\\mathrm{max}}^{2}}{4}d}\\end{array}$ .h probability at least $1\\mathrm{~-~}T\\cdot\\exp(-\\Omega(d))$ , we have that $\\begin{array}{r}{\\left\\|\\mathbf{n}_{t}\\right\\|^{2}>\\frac{\\nu_{0}^{2}+\\nu_{\\mathrm{max}}^{2}}{2}d}\\end{array}$ $t\\in\\left\\{0\\right\\}\\cup\\left[T\\right]$ ", "page_idx": 13}, {"type": "text", "text": "458 Proof of Proposition 3. To establish a lower bound on $\\|\\mathbf{n}_{t}\\|$ , we consider different cases of the step   \n459 size $\\delta_{t}$ . Intuitively, when $\\delta_{t}$ is large enough, ${\\bf n}_{t}$ will be too noisy due to the introduction of random   \n460 noise $\\sqrt{\\delta_{t}}\\epsilon_{t}^{(\\mathbf{n})}$ in equation 5. While for small $\\delta_{t}$ , the update of ${\\bf n}_{t}$ is bounded and thus we can   \n461 iteratively analyze $\\mathbf{n}_{t}$ . We first handle the case of large $\\delta_{t}$ in the following lemma.   \n462 Lemma 3. If $\\delta_{t}>\\nu_{0}^{2}$ , with probability at least $1-\\exp(-\\Omega(d))$ , for $\\mathbf{n}_{t}$ satisfying equation 5, we   \n463 have $\\begin{array}{r}{\\left\\|\\mathbf{n}_{t}\\right\\|^{2}\\geq\\frac{3\\nu_{0}^{2}+\\nu_{\\mathrm{max}}^{2}}{4}d}\\end{array}$ regardless of the previous state $\\mathbf{x}_{t-1}$ .   \n464 Proof of Lemma 3. Denote $\\begin{array}{r}{\\mathbf{v}:=\\mathbf{n}_{t-1}+\\frac{\\delta_{t}}{2}\\mathbf{N}^{T}\\nabla_{\\mathbf{x}}\\log P(\\mathbf{x}_{t-1})}\\end{array}$ for simplicity. Note that $\\mathbf{v}$ is fixed   \n465 for any given $\\mathbf{x}_{t-1}$ . We decompose $\\epsilon_{t}^{(\\mathbf{n})}$ into a vector aligning with $\\mathbf{v}$ and another vector orthogonal   \n466 to v. Consider an orthonormal matrix $\\mathbf{M}\\in\\mathbb{R}^{n\\times(n-1)}$ such that ${\\bf M}^{T}{\\bf v}={\\bf0}_{n-1}$ and $\\mathbf{M}^{T}\\mathbf{M}=I_{n-1}$ .   \n467 By denoting $\\mathbf{u}:=\\epsilon_{t}^{(\\mathbf{n})}-\\mathbf{M}\\mathbf{M}^{T}\\epsilon_{t}^{(\\mathbf{n})}$ we have $\\mathbf{M}^{T}\\ensuremath{\\mathbf{u}}=\\ensuremath{\\mathbf{0}}_{n-1}$ , thus we obtain ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\Vert\\mathbf{n}_{t}\\right\\Vert^{2}=\\left\\Vert\\mathbf{v}+\\sqrt{\\delta_{t}}\\epsilon_{t}^{(\\mathbf{n})}\\right\\Vert^{2}}\\\\ &{\\qquad=\\left\\Vert\\mathbf{v}+\\sqrt{\\delta_{t}}\\mathbf{u}+\\sqrt{\\delta_{t}}\\mathbf{M}\\mathbf{M}^{T}\\epsilon_{t}^{(\\mathbf{n})}\\right\\Vert^{2}}\\\\ &{\\qquad=\\left\\Vert\\mathbf{v}+\\sqrt{\\delta_{t}}\\mathbf{u}\\right\\Vert^{2}+\\left\\Vert\\sqrt{\\delta_{t}}\\mathbf{M}\\mathbf{M}^{T}\\epsilon_{t}^{(\\mathbf{n})}\\right\\Vert^{2}}\\\\ &{\\qquad\\geq\\left\\Vert\\sqrt{\\delta_{t}}\\mathbf{M}\\mathbf{M}^{T}\\epsilon_{t}^{(\\mathbf{n})}\\right\\Vert^{2}}\\\\ &{\\qquad\\geq\\nu_{0}^{2}\\left\\Vert\\mathbf{M}^{T}\\epsilon_{t}^{(\\mathbf{n})}\\right\\Vert^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "468 Since $\\epsilon_{t}^{(\\mathbf{n})}\\sim\\mathcal{N}(\\mathbf{0}_{n},I_{n})$ and $\\mathbf{M}^{T}\\mathbf{M}=I_{n-1}$ , we obtain $\\mathbf{M}^{T}\\epsilon_{t}^{(\\mathbf{n})}\\sim\\mathcal{N}(\\mathbf{0}_{n-1},I_{n-1})$ . Therefore, by   \n469 Lemma 1 we can bound ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{P}\\left(\\left\\|\\mathbf{n}_{t}\\right\\|^{2}\\leq\\frac{3\\nu_{0}^{2}+\\nu_{\\operatorname*{max}}^{2}}{4}d\\right)\\leq\\mathbb{P}\\left(\\left\\|\\mathbf{M}^{T}\\boldsymbol{\\epsilon}_{t}^{(\\mathbf{n})}\\right\\|^{2}\\leq\\frac{3\\nu_{0}^{2}+\\nu_{\\operatorname*{max}}^{2}}{4\\nu_{0}^{2}}d\\right)}&{}\\\\ {=\\mathbb{P}\\left(\\left\\|\\mathbf{M}^{T}\\boldsymbol{\\epsilon}_{t}^{(\\mathbf{n})}\\right\\|^{2}\\leq d-2\\sqrt{d\\cdot\\left(\\frac{\\nu_{0}^{2}-\\nu_{\\operatorname*{max}}^{2}}{8\\nu_{0}^{2}}\\right)^{2}d}\\right)}\\\\ &{\\leq\\mathbb{P}\\left(\\left\\|\\mathbf{M}^{T}\\boldsymbol{\\epsilon}_{t}^{(\\mathbf{n})}\\right\\|^{2}\\leq(n-1)-2\\sqrt{(n-1)\\left(\\frac{\\nu_{0}^{2}-\\nu_{\\operatorname*{max}}^{2}}{8\\nu_{0}^{2}}\\right)^{2}\\frac{d}{2}}\\right)}\\\\ &{\\leq\\exp\\left(-\\left(\\frac{\\nu_{0}^{2}-\\nu_{\\operatorname*{max}}^{2}}{8\\nu_{0}^{2}}\\right)^{2}\\frac{d}{2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "470 where the second last step follows from the assumption $d-n=r=o(d)$ . Hence we complete the   \n471 proof of Lemma 3. \u53e3   \n472 We then consider the case when $\\delta_{t}\\leq\\nu_{0}^{2}$ . Let ${\\bf r}:={\\bf R}^{T}{\\bf x}$ and $\\mathbf{n}:=\\mathbf{N}^{T}\\mathbf{x}$ , then $\\mathbf{x}=\\mathbf{R}\\mathbf{r}+\\mathbf{N}\\mathbf{n}$ . We   \n473 first show that when $\\begin{array}{r}{\\left\\|\\mathbf{n}\\right\\|^{2}\\geq\\frac{\\nu_{0}^{2}+\\nu_{\\operatorname*{max}}^{2}}{2}d,\\bar{P}^{(i)}(\\mathbf{x})}\\end{array}$ is exponentially smaller than $P^{(0)}(\\mathbf{x})$ for all $i\\in[k]$   \n474 in the following lemma.   \n475 Lemma 4. Given that $\\begin{array}{r}{\\left\\|\\mathbf{n}\\right\\|^{2}\\geq\\frac{\\nu_{0}^{2}+\\nu_{\\operatorname*{max}}^{2}}{2}d}\\end{array}$ and $\\begin{array}{r}{\\left\\|\\mu_{i}\\right\\|^{2}\\leq\\frac{\\nu_{0}^{2}-\\nu_{i}^{2}}{2}\\left(\\log\\left(\\frac{\\nu_{i}^{2}}{\\nu_{0}^{2}}\\right)-\\frac{\\nu_{i}^{2}}{2\\nu_{0}^{2}}+\\frac{\\nu_{0}^{2}}{2\\nu_{i}^{2}}\\right)d}\\end{array}$ for all   \n476 $i\\in[k]$ , we have $\\begin{array}{r}{\\frac{P^{(i)}(\\mathbf{x})}{P^{(0)}(\\mathbf{x})}\\leq\\exp(-\\Omega(d))}\\end{array}$ for all $i\\in[k]$ . ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "477 Proof of Lemma 4. For all $i\\in[k]$ , define $\\begin{array}{r}{\\rho_{i}(\\mathbf{x}):=\\frac{P^{(i)}(\\mathbf{x})}{P^{(0)}(\\mathbf{x})}}\\end{array}$ P (i)(x), then ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\rho_{i}(\\mathbf{x})=\\frac{P^{(i)}(\\mathbf{x})}{P^{(0)}(\\mathbf{x})}=\\frac{\\left(2\\pi\\nu_{i}^{2}\\right)^{-d/2}\\exp{\\left(-\\frac{1}{2\\nu_{i}^{2}}\\left|\\mathbf{x}-\\mu_{i}\\right|\\right)}^{2}}{\\left(2\\pi\\nu_{0}^{2}\\right)^{-d/2}\\exp{\\left(-\\frac{1}{2\\nu_{i}^{2}}\\left|\\mathbf{x}\\right|^{2}\\right)}}}\\\\ &{\\qquad=\\left(\\frac{\\nu_{0}^{2}}{\\nu_{i}^{2}}\\right)^{d/2}\\exp{\\left(\\frac{1}{2\\nu_{0}^{2}}\\left|\\mathbf{x}\\right|^{2}-\\frac{1}{2\\nu_{i}^{2}}\\left|\\mathbf{\\left|x-\\mu_{i}\\right|}\\right|^{2}\\right)}}\\\\ &{\\qquad=\\left(\\frac{\\nu_{0}^{2}}{\\nu_{i}^{2}}\\right)^{d/2}\\exp{\\left(\\left(\\frac{1}{2\\nu_{0}^{2}}-\\frac{1}{2\\nu_{i}^{2}}\\right)\\left|\\mathbf{Nn}\\right|^{2}+\\left(\\frac{\\left|\\mathbf{Rr}\\right|^{2}}{2\\nu_{0}^{2}}-\\frac{\\left|\\mathbf{Rr}-\\mu_{i}\\right|^{2}}{2\\nu_{i}^{2}}\\right)\\right)}}\\\\ &{\\qquad=\\left(\\frac{\\nu_{0}^{2}}{\\nu_{i}^{2}}\\right)^{d/2}\\exp{\\left(\\left(\\frac{1}{2\\nu_{0}^{2}}-\\frac{1}{2\\nu_{i}^{2}}\\right)\\left|\\mathbf{n}\\right|^{2}+\\left(\\frac{\\left|\\mathbf{r}\\right|^{2}}{2\\nu_{0}^{2}}-\\frac{\\left|\\mathbf{r}-\\mathbf{R}^{T}\\right|\\mu_{i}^{2}}{2\\nu_{i}^{2}}\\right)\\right)}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "478 where the last step follows from the definition that $\\mathbf{R}\\in\\mathbb{R}^{d\\times r}$ an orthonormal basis of the vector space   \n479 $\\{\\pmb{\\mu}_{i}\\}_{i\\in[k]}$ and $\\mathbf{N}^{T}\\mathbf{N}=I_{n}$ . Since $\\nu_{0}^{2}>\\nu_{i}^{2}$ , the quadratic term $\\frac{\\left\\|\\mathbf{r}\\right\\|^{2}}{2\\nu_{0}^{2}}-\\frac{\\left\\|\\mathbf{r}\\!-\\mathbf{R}^{T}\\pmb{\\mu}_{i}\\right\\|^{2}}{2\\nu_{i}^{2}}$ is maximized at ", "page_idx": 14}, {"type": "text", "text": "480 $\\begin{array}{r}{\\mathbf{r}=\\frac{\\nu_{0}^{2}\\mathbf{R}^{T}\\mu_{i}}{\\nu_{0}^{2}-\\nu_{i}^{2}}}\\end{array}$ \u03bd02R \u00b52i. Therefore, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{\\left\\Vert\\mathbf{r}\\right\\Vert^{2}}{2\\nu_{0}^{2}}-\\frac{\\left\\Vert\\mathbf{r}-\\mathbf{R}^{T}\\boldsymbol{\\mu}_{i}\\right\\Vert^{2}}{2\\nu_{i}^{2}}\\leq\\frac{\\nu_{0}^{4}\\left\\Vert\\mathbf{R}^{T}\\boldsymbol{\\mu}_{i}\\right\\Vert^{2}}{2\\nu_{0}^{2}(\\nu_{0}^{2}-\\nu_{i}^{2})^{2}}-\\frac{1}{2\\nu_{i}^{2}}\\left(\\frac{\\nu_{0}^{2}}{\\nu_{0}^{2}-\\nu_{i}^{2}}-1\\right)^{2}\\left\\Vert\\mathbf{R}^{T}\\boldsymbol{\\mu}_{i}\\right\\Vert^{2}=\\frac{\\left\\Vert\\boldsymbol{\\mu}_{i}\\right\\Vert^{2}}{2(\\nu_{0}^{2}-\\nu_{i}^{2})}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "481 Hence, for $\\begin{array}{r}{\\left\\|\\mathbf{n}\\right\\|^{2}\\geq\\frac{\\nu_{0}^{2}+\\nu_{\\operatorname*{max}}^{2}}{2}d}\\end{array}$ and $\\begin{array}{r}{\\left\\|\\mu_{i}\\right\\|^{2}\\leq\\frac{\\nu_{0}^{2}-\\nu_{i}^{2}}{2}\\left(\\log\\left(\\frac{\\nu_{i}^{2}}{\\nu_{0}^{2}}\\right)-\\frac{\\nu_{i}^{2}}{2\\nu_{0}^{2}}+\\frac{\\nu_{0}^{2}}{2\\nu_{i}^{2}}\\right)d\\,}\\end{array}$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\rho_{i}(\\mathbf{x})=\\left(\\frac{\\nu_{0}^{2}}{\\nu_{i}^{2}}\\right)^{d/2}\\exp\\left(\\left(\\frac{1}{2\\nu_{0}^{2}}-\\frac{1}{2\\nu_{i}^{2}}\\right)|\\mathbf{n}|^{2}+\\left(\\frac{\\|\\mathbf{r}\\|^{2}}{2\\nu_{0}^{2}}-\\frac{\\left\\|\\mathbf{r}-\\mathbf{R}^{T}\\boldsymbol{\\mu}_{i}\\right\\|^{2}}{2\\nu_{i}^{2}}\\right)\\right)}\\\\ &{\\qquad\\leq\\left(\\frac{\\nu_{0}^{2}}{\\nu_{i}^{2}}\\right)^{d/2}\\exp\\left(\\left(\\frac{1}{2\\nu_{0}^{2}}-\\frac{1}{2\\nu_{i}^{2}}\\right)\\frac{\\nu_{0}^{2}+\\nu_{i}^{2}}{2}d+\\frac{\\|\\boldsymbol{\\mu}_{i}\\|^{2}}{2(\\nu_{0}^{2}-\\nu_{i}^{2})}\\right)}\\\\ &{\\qquad=\\exp\\left(-\\left(\\log\\left(\\frac{\\nu_{i}^{2}}{\\nu_{0}^{2}}\\right)-\\frac{\\nu_{i}^{2}}{2\\nu_{0}^{2}}+\\frac{\\nu_{0}^{2}}{2\\nu_{i}^{2}}\\right)\\frac{d}{2}+\\frac{\\|\\boldsymbol{\\mu}_{i}\\|^{2}}{2(\\nu_{0}^{2}-\\nu_{i}^{2})}\\right)}\\\\ &{\\qquad\\leq\\exp\\left(-\\left(\\log\\left(\\frac{\\nu_{i}^{2}}{\\nu_{0}^{2}}\\right)-\\frac{\\nu_{i}^{2}}{2\\nu_{0}^{2}}+\\frac{\\nu_{0}^{2}}{2\\nu_{i}^{2}}\\right)\\frac{d}{4}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "482 Notice that for function $\\begin{array}{r}{f(z)=\\log z-\\frac{z}{2}+\\frac{1}{2z}}\\end{array}$ , we have $f(1)=0$ and $\\begin{array}{r}{\\frac{\\mathrm{d}}{\\mathrm{d}z}f(z)=\\frac{1}{z}-\\frac{1}{2}-\\frac{1}{2z^{2}}=}\\end{array}$   \n483 $-{\\frac{1}{2}}\\left({\\frac{1}{z}}-1\\right)^{2}<0$ when $z\\in(0,1)$ . Thus, $\\begin{array}{r}{\\log\\left(\\frac{\\nu_{i}^{2}}{\\nu_{0}^{2}}\\right)-\\frac{\\nu_{i}^{2}}{2\\nu_{0}^{2}}+\\frac{\\nu_{0}^{2}}{2\\nu_{i}^{2}}}\\end{array}$ is a positive constant for $\\nu_{i}<\\nu_{0}$ ,   \n484 i.e., $\\rho_{i}({\\bf x})=\\exp(-\\Omega(d))$ . Therefore we finish the proof of Lemma 4. \u53e3   \n485 Lemma 4 implies that when $\\lVert\\mathbf{n}\\rVert$ is large, the Gaussian mode $P^{(0)}$ dominates other modes $P^{(i)}$ . To   \n486 bound $\\|{\\bf n}_{t}\\|$ , we first consider a simpler case that $\\|{\\bf n}_{t-1}\\|$ is large. Intuitively, the following lemma   \n487 proves that when the previous state $\\mathbf{n}_{t-1}$ is far from a mode, a single step of Langevin dynamics with   \n488 bounded step size is not enough to find the mode.   \n489 Lemma 5. Suppose $\\delta_{t}\\leq\\nu_{0}^{2}$ and $\\left\\|\\mathbf{n}_{t-1}\\right\\|^{2}>36\\nu_{0}^{2}d,$ , then for $\\mathbf{n}_{t}$ following from equation 5, we have   \n490 $\\left\\|\\mathbf{n}_{t}\\right\\|^{2}\\geq\\nu_{0}^{2}d$ with probability at least $1-\\exp(-\\Omega(d))$ . ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "491 Proof of Lemma 5. From the recursion of $\\mathbf{n}_{t}$ in equation 5 we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\bf n}_{t}={\\bf n}_{t-1}+\\frac{\\delta_{t}}{2}{\\bf N}^{T}\\nabla_{\\bf x}\\log P({\\bf x}_{t-1})+\\sqrt{\\delta_{t}}\\epsilon_{t}^{({\\bf n})}}\\ ~}\\\\ {{\\displaystyle~~={\\bf n}_{t-1}-\\frac{\\delta_{t}}{2}\\sum_{i=0}^{k}\\frac{P^{(i)}({\\bf x}_{t-1})}{P({\\bf x}_{t-1})}\\cdot\\frac{{\\bf N}^{T}({\\bf x}_{t-1}-\\mu_{i})}{\\nu_{i}^{2}}+\\sqrt{\\delta_{t}}\\epsilon_{t}^{({\\bf n})}}\\ ~}\\\\ {{\\displaystyle~~=\\left(1-\\frac{\\delta_{t}}{2}\\sum_{i=0}^{k}\\frac{P^{(i)}({\\bf x}_{t-1})}{P({\\bf x}_{t-1})}\\cdot\\frac{1}{\\nu_{i}^{2}}\\right){\\bf n}_{t-1}+\\sqrt{\\delta_{t}}\\epsilon_{t}^{({\\bf n})}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "492 By Lemma 4, we have $\\begin{array}{r}{\\frac{P^{(i)}(\\mathbf{x}_{j-1})}{P^{(0)}(\\mathbf{x}_{j-1})}\\leq\\exp(-\\Omega(d))}\\end{array}$ for all $i\\in[k]$ , therefore ", "page_idx": 15}, {"type": "equation", "text": "$$\n1-\\frac{\\delta_{t}}{2}\\sum_{i=0}^{k}\\frac{P^{(i)}(\\mathbf{x}_{t-1})}{P(\\mathbf{x}_{t-1})}\\cdot\\frac{1}{\\nu_{i}^{2}}\\geq1-\\frac{\\delta_{t}}{2}\\cdot\\frac{1}{\\nu_{0}^{2}}-\\frac{\\delta_{t}}{2}\\sum_{i\\in[k]}\\frac{w_{i}P^{(i)}(\\mathbf{x}_{t-1})}{w_{0}P^{(0)}(\\mathbf{x}_{t-1})}\\cdot\\frac{1}{\\nu_{i}^{2}}\\geq1-\\frac{1}{2}-\\exp(-\\Omega(d))>\\frac{1}{3}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "493 On the other hand, from \u03f5t(n)\u223cN(0n, In) we know $\\begin{array}{r}{\\frac{\\langle\\mathbf{n}_{t-1},\\epsilon_{t}^{(\\mathbf{n})}\\rangle}{\\|\\mathbf{n}_{t-1}\\|}\\sim\\mathcal{N}(0,1)}\\end{array}$ for any fixed $\\ensuremath{\\mathbf{n}}_{t-1}\\neq\\ensuremath{\\mathbf{0}}_{n}$ ,   \n494 hence by Lemma 2 we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\frac{\\langle\\mathbf{n}_{t-1},\\boldsymbol{\\epsilon}_{t}^{(\\mathbf{n})}\\rangle}{\\|\\mathbf{n}_{t-1}\\|}\\geq\\frac{\\sqrt{d}}{4}\\right)=\\mathbb{P}\\left(\\frac{\\langle\\mathbf{n}_{t-1},\\boldsymbol{\\epsilon}_{t}^{(\\mathbf{n})}\\rangle}{\\|\\mathbf{n}_{t-1}\\|}\\leq-\\frac{\\sqrt{d}}{4}\\right)\\leq\\frac{4}{\\sqrt{2\\pi d}}\\exp\\left(-\\frac{d}{32}\\right)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "495 Combining equation 6, equation 7 and equation 8 gives that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\left\\Vert\\mathbf{n}_{t}\\right\\Vert^{2}\\geq\\left(\\frac{1}{3}\\right)^{2}\\left\\Vert\\mathbf{n}_{t-1}\\right\\Vert^{2}-2\\nu_{0}|\\langle\\mathbf{n}_{t-1},\\epsilon_{t}^{(\\mathbf{n})}\\rangle|}\\\\ {\\qquad\\geq\\frac{1}{9}\\left\\Vert\\mathbf{n}_{t-1}\\right\\Vert^{2}-\\frac{\\nu_{0}\\sqrt{d}}{2}\\left\\Vert\\mathbf{n}_{t-1}\\right\\Vert}\\\\ {\\qquad\\geq\\frac{1}{9}\\cdot36\\nu_{0}^{2}d-\\frac{\\nu_{0}\\sqrt{d}}{2}\\cdot6\\nu_{0}\\sqrt{d}}\\\\ {\\qquad=\\nu_{0}^{2}d}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "496 with probability at least $\\begin{array}{r}{1-\\frac{8}{\\sqrt{2\\pi d}}\\exp\\left(-\\frac{d}{32}\\right)=1-\\exp(-\\Omega(d))}\\end{array}$ . This proves Lemma 5. ", "page_idx": 16}, {"type": "text", "text": "497 We then proceed to bound $\\|\\mathbf{n}_{t}\\|$ iteratively for $\\left\\|\\mathbf{n}_{t-1}\\right\\|^{2}\\leq36\\nu_{0}^{2}d$ . Recall that equation 5 gives ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{n}_{t}=\\mathbf{n}_{t-1}+\\frac{\\delta_{t}}{2}\\mathbf{N}^{T}\\nabla_{\\mathbf{x}}\\log P(\\mathbf{x}_{t-1})+\\sqrt{\\delta_{t}}\\pmb{\\epsilon}_{t}^{(\\mathbf{n})}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "498 We notice that the difficulty of solving ${\\bf n}_{t}$ exhibits in the dependence of $\\log P(\\mathbf{x}_{t-1})$ on $\\mathbf{r}_{t-1}$ . Since   \n499 $\\begin{array}{r}{P=\\sum_{i=0}^{k}w_{i}P^{(i)}=\\sum_{i=0}^{k}w_{i}\\mathcal{N}(\\mu_{i},\\nu_{i}^{2}I_{d})}\\end{array}$ , we can rewrite the score function as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\nabla_{\\mathbf{x}}\\log P(\\mathbf{x})={\\frac{\\nabla_{\\mathbf{x}}P(\\mathbf{x})}{P(\\mathbf{x})}}=-\\sum_{i=0}^{k}{\\frac{P^{(i)}(\\mathbf{x})}{P(\\mathbf{x})}}\\cdot{\\frac{\\mathbf{x}-\\mu_{i}}{\\nu_{i}^{2}}}=-{\\frac{\\mathbf{x}}{\\nu_{0}^{2}}}+\\sum_{i\\in[k]}{\\frac{P^{(i)}(\\mathbf{x})}{P(\\mathbf{x})}}\\left({\\frac{\\mathbf{x}}{\\nu_{0}^{2}}}-{\\frac{\\mathbf{x}-\\mu_{i}}{\\nu_{i}^{2}}}\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "500 Now, instead of directly working with $\\mathbf{n}_{t}$ , we consider a surrogate recursion $\\hat{\\mathbf{n}}_{t}$ such that $\\hat{\\mathbf{n}}_{0}=\\mathbf{n}_{0}$   \n501 and for all $t\\geq1$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\hat{\\bf n}_{t}=\\hat{\\bf n}_{t-1}-\\frac{\\delta_{t}}{2\\nu_{0}^{2}}\\hat{\\bf n}_{t-1}+\\sqrt{\\delta_{t}}\\epsilon_{t}^{({\\bf n})}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "502 The advantage of the surrogate recursion is that $\\hat{{\\bf n}}_{t}$ is independent of $\\mathbf{r}$ , thus we can obtain the   \n503 closed-form solution to $\\hat{{\\mathbf{n}}}_{t}$ . Before we proceed to bound $\\hat{\\mathbf{n}}_{t}$ , we first show that $\\hat{{\\bf n}}_{t}$ is sufficiently close   \n504 to the original recursion ${\\bf n}_{t}$ in the following lemma.   \n505 Lemma 6. For any $t\\geq1$ , given that $\\delta_{j}\\leq\\nu_{0}^{2}$ and $\\begin{array}{r}{\\frac{\\nu_{0}^{2}+\\nu_{\\mathrm{max}}^{2}}{2}d\\leq\\|\\mathbf{n}_{j-1}\\|^{2}\\leq36\\nu_{0}^{2}d}\\end{array}$ for all $j\\in[t]$ and   \n506 $\\begin{array}{r}{\\left\\|\\mu_{i}\\right\\|^{2}\\leq\\frac{\\nu_{0}^{2}-\\nu_{i}^{2}}{2}\\left(\\log\\left(\\frac{\\nu_{i}^{2}}{\\nu_{0}^{2}}\\right)-\\frac{\\nu_{i}^{2}}{2\\nu_{0}^{2}}+\\frac{\\bar{\\nu}_{0}^{2}}{2\\nu_{i}^{2}}\\right)c}\\end{array}$ d for all $i\\in[k]$ , we have $\\begin{array}{r}{\\|\\hat{\\mathbf{n}}_{t}-\\mathbf{n}_{t}\\|\\leq\\frac{t}{\\exp\\left(\\Omega\\left(d\\right)\\right)}\\sqrt{d}.}\\end{array}$ .   \n507 Proof of Lemma 6. Upon comparing equation 5 and equation 10, by equation 9 we have that for all   \n508 $j\\in[t]$ , ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\left\\|\\hat{\\mathbf{n}}_{j}-\\mathbf{n}_{j}\\right\\|=\\left\\|\\hat{\\mathbf{n}}_{j-1}-\\frac{\\delta_{j}}{2\\nu_{0}^{2}}\\hat{\\mathbf{n}}_{j-1}-\\mathbf{n}_{j-1}-\\frac{\\delta_{j}}{2}\\mathbf{N}^{T}\\nabla_{\\mathbf{x}}\\log P(\\mathbf{x}_{j-1})\\right\\|}\\\\ {\\displaystyle=\\left\\|\\left(1-\\frac{\\delta_{j}}{2\\nu_{0}^{2}}\\right)\\left(\\hat{\\mathbf{n}}_{j-1}-\\mathbf{n}_{j-1}\\right)+\\frac{\\delta_{j}}{2}\\sum_{i\\in[k]}\\frac{P^{(i)}(\\mathbf{x}_{j-1})}{P(\\mathbf{x}_{j-1})}\\left(\\frac{1}{\\nu_{i}^{2}}-\\frac{1}{\\nu_{0}^{2}}\\right)\\mathbf{n}_{j-1}\\right\\|}\\\\ {\\displaystyle\\leq\\left(1-\\frac{\\delta_{j}}{2\\nu_{0}^{2}}\\right)\\|\\hat{\\mathbf{n}}_{j-1}-\\mathbf{n}_{j-1}\\|+\\sum_{i\\in[k]}\\frac{\\delta_{j}}{2}\\frac{P^{(i)}(\\mathbf{x}_{j-1})}{P(\\mathbf{x}_{j-1})}\\left(\\frac{1}{\\nu_{i}^{2}}-\\frac{1}{\\nu_{0}^{2}}\\right)\\|\\mathbf{n}_{j-1}\\|}\\\\ {\\displaystyle\\leq\\|\\hat{\\mathbf{n}}_{j-1}-\\mathbf{n}_{j-1}\\|+\\sum_{i\\in[k]}\\frac{\\delta_{j}}{2}\\frac{P^{(i)}(\\mathbf{x}_{j-1})}{P^{(0)}(\\mathbf{x}_{j-1})}\\left(\\frac{1}{\\nu_{i}^{2}}-\\frac{1}{\\nu_{0}^{2}}\\right)6\\nu_{0}\\sqrt{d}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "509 By Lemma 4, we hav e PP  ((i0))((xxjj\u2212\u221211)) \u2264exp(\u2212\u2126(d)) for all i \u2208[k], hence we obtain a recursive bound ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\|\\hat{\\mathbf{n}}_{j}-\\mathbf{n}_{j}\\|\\leq\\|\\hat{\\mathbf{n}}_{j-1}-\\mathbf{n}_{j-1}\\|+\\frac{1}{\\exp(\\Omega(d))}\\sqrt{d}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "510 Finally, by $\\hat{\\mathbf{n}}_{0}=\\mathbf{n}_{0}$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|\\hat{\\mathbf{n}}_{t}-\\mathbf{n}_{t}\\|=\\sum_{j\\in[t]}\\left(\\|\\hat{\\mathbf{n}}_{j}-\\mathbf{n}_{j}\\|-\\|\\hat{\\mathbf{n}}_{j-1}-\\mathbf{n}_{j-1}\\|\\right)\\leq\\frac{t}{\\exp(\\Omega(d))}\\sqrt{d}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "511 Hence we obtain Lemma 6. ", "page_idx": 17}, {"type": "text", "text": "512 We then proceed to analyze $\\hat{\\mathbf{n}}_{t}$ , The following lemma gives us the closed-form solution of $\\hat{\\mathbf{n}}_{t}$ . We   \n513 slightly abuse the notations here, e.g., $\\begin{array}{r}{\\prod_{i=c_{1}}^{c_{2}}\\left[1-\\frac{\\delta_{i}}{2\\nu_{0}^{2}}\\right)=1}\\end{array}$ and $\\sum_{j=c_{1}}^{c_{2}}\\delta_{j}=0$ for $c_{1}>c_{2}$ . ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{{5}14}&{\\mathbf{Lemma}\\ 7.\\ F o r\\,a l l\\ t\\ge0,\\,\\hat{\\mathbf{n}}_{t}\\sim{\\mathcal{N}}\\left(\\prod_{i=1}^{t}\\left(1-\\frac{\\delta_{i}}{2\\nu_{0}^{2}}\\right)\\mathbf{n}_{0},\\,\\sum_{j=1}^{t}\\prod_{i=j+1}^{t}\\left(1-\\frac{\\delta_{i}}{2\\nu_{0}^{2}}\\right)^{2}\\delta_{j}I_{n}\\right)\\!,}\\\\ {{5}15}&{t h e\\,m e a n\\,a n d\\,c o v a r i a n c e\\,s a t i s f y\\prod_{i=1}^{t}\\left(1-\\frac{\\delta_{i}}{2\\nu_{0}^{2}}\\right)^{2}+\\frac{1}{\\nu_{0}^{2}}\\sum_{j=1}^{t}\\prod_{i=j+1}^{t}\\left(1-\\frac{\\delta_{i}}{2\\nu_{0}^{2}}\\right)^{2}\\delta_{j}\\ge1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "516 Proof of Lemma 7. We prove the two properties by induction. When $t=0$ , they are trivial. Suppose   \n517 they hold for $t-1$ , then for the distribution of $\\hat{\\mathbf{n}}_{t}$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\hat{\\mathbf{n}}_{t}=\\hat{\\mathbf{n}}_{t-1}-\\frac{\\delta_{t}}{2\\nu_{0}^{2}}\\hat{\\mathbf{n}}_{t-1}+\\sqrt{\\delta_{t}}\\epsilon_{t}^{(\\mathbf{n})}}\\\\ {\\displaystyle\\sim N\\left(\\left(1-\\frac{\\delta_{t}}{2\\nu_{0}^{2}}\\right)\\prod_{i=1}^{t-1}\\left(1-\\frac{\\delta_{i}}{2\\nu_{0}^{2}}\\right)\\mathbf{n}_{0},\\,\\left(1-\\frac{\\delta_{t}}{2\\nu_{0}^{2}}\\right)^{2}\\sum_{j=1}^{t-1}\\prod_{i=j+1}^{t-1}\\left(1-\\frac{\\delta_{i}}{2\\nu_{0}^{2}}\\right)^{2}\\delta_{j}I_{n}+\\delta_{t}I_{n}\\right)}\\\\ {\\displaystyle=N\\left(\\prod_{i=1}^{t}\\left(1-\\frac{\\delta_{i}}{2\\nu_{0}^{2}}\\right)\\mathbf{n}_{0},\\,\\sum_{j=1}^{t}\\prod_{i=j+1}^{t}\\left(1-\\frac{\\delta_{i}}{2\\nu_{0}^{2}}\\right)^{2}\\delta_{j}I_{n}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "518 For the second property, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\prod_{i=1}^{t}\\left(1-\\frac{\\delta_{i}}{2\\nu_{0}^{2}}\\right)^{2}+\\frac{1}{\\nu_{0}^{2}}\\sum_{j=1}^{t}\\prod_{i=j+1}^{t}\\left(1-\\frac{\\delta_{i}}{2\\nu_{0}^{2}}\\right)^{2}\\delta_{j}}\\\\ &{=\\left(1-\\frac{\\delta_{t}}{2\\nu_{0}^{2}}\\right)^{2}\\left(\\prod_{i=1}^{t-1}\\left(1-\\frac{\\delta_{i}}{2\\nu_{0}^{2}}\\right)^{2}+\\frac{1}{\\nu_{0}^{2}}\\sum_{j=1}^{t-1}\\prod_{i=j+1}^{t-1}\\left(1-\\frac{\\delta_{i}}{2\\nu_{0}^{2}}\\right)^{2}\\delta_{j}\\right)+\\frac{1}{\\nu_{0}^{2}}\\delta_{t}}\\\\ &{\\geq\\left(1-\\frac{\\delta_{t}}{2\\nu_{0}^{2}}\\right)^{2}+\\frac{1}{\\nu_{0}^{2}}\\delta_{t}=1+\\frac{\\delta_{t}^{2}}{4\\nu_{0}^{4}}\\geq1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "519 Hence we finish the proof of Lemma 7. ", "page_idx": 17}, {"type": "text", "text": "520 Armed with Lemma 7, we are now ready to establish the lower bound on $\\|\\hat{\\mathbf{n}}_{t}\\|$ . For simplicity,   \n521 denote $\\begin{array}{r}{\\alpha:=\\prod_{i=1}^{t}\\left(1-\\frac{\\delta_{i}}{2\\nu_{0}^{2}}\\right)^{2}}\\end{array}$ and $\\begin{array}{r}{\\beta:=\\frac{1}{\\nu_{0}^{2}}\\sum_{j=1}^{t}\\prod_{i=j+1}^{t}\\left(1-\\frac{\\delta_{i}}{2\\nu_{0}^{2}}\\right)^{2}\\delta_{j}}\\end{array}$ . By Lemma 7 we know   \n522 $\\hat{\\mathbf{n}}_{t}\\sim\\mathcal{N}(\\alpha\\mathbf{n}_{0},\\beta\\nu_{0}^{2}I_{n})$ , so we can write $\\hat{\\mathbf{n}}_{t}=\\alpha\\mathbf{n}_{0}+\\sqrt{\\beta}\\nu_{0}\\epsilon$ , where $\\pmb{\\epsilon}\\sim\\mathcal{N}(\\mathbf{0}_{n},\\pmb{I}_{n})$ .   \n523 Lemma 8. Given that $\\begin{array}{r}{\\left\\lVert\\hat{\\mathbf{n}}_{0}\\right\\rVert^{2}\\geq\\frac{3\\nu_{0}^{2}+\\nu_{\\mathrm{max}}^{2}}{4}d,}\\end{array}$ , we have $\\begin{array}{r}{\\left\\|\\hat{\\mathbf{n}}_{t}\\right\\|^{2}\\geq\\frac{5\\nu_{0}^{2}+3\\nu_{\\mathrm{max}}^{2}}{8}d}\\end{array}$ with probability at least   \n524 $1-\\exp\\left(-\\Omega(d)\\right)$ . ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "525 Proof of Lemma 8. By $\\hat{\\mathbf{n}}_{t}=\\alpha\\mathbf{n}_{0}+\\sqrt{\\beta}\\nu_{0}\\epsilon$ we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left\\|\\hat{\\mathbf{n}}_{t}\\right\\|^{2}=\\alpha^{2}\\left\\|\\mathbf{n}_{0}\\right\\|^{2}+\\beta\\nu_{0}^{2}\\left\\|\\epsilon\\right\\|^{2}+2\\alpha\\sqrt{\\beta}\\nu_{0}\\langle\\mathbf{n}_{0},\\epsilon\\rangle\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "526 By Lemma 1 we can bound ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{P}\\left(\\|\\boldsymbol{\\epsilon}\\|^{2}\\leq\\displaystyle\\frac{3\\nu_{0}^{2}+\\nu_{\\operatorname*{max}}^{2}}{4\\nu_{0}^{2}}d\\right)=\\mathbb{P}\\left(\\|\\boldsymbol{\\epsilon}\\|^{2}\\leq d-2\\sqrt{d\\cdot\\left(\\frac{\\nu_{0}^{2}-\\nu_{\\operatorname*{max}}^{2}}{8\\nu_{0}^{2}}\\right)^{2}d}\\right)}&{}\\\\ {\\leq\\mathbb{P}\\left(\\|\\boldsymbol{\\epsilon}\\|^{2}\\leq(n-1)-2\\sqrt{(n-1)\\left(\\frac{\\nu_{0}^{2}-\\nu_{\\operatorname*{max}}^{2}}{8\\nu_{0}^{2}}\\right)^{2}\\frac{d}{2}}\\right)}&{}\\\\ {\\leq\\exp\\left(-\\left(\\frac{\\nu_{0}^{2}-\\nu_{\\operatorname*{max}}^{2}}{8\\nu_{0}^{2}}\\right)^{2}\\frac{d}{2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "527 where the second last step follows from the assumption $d-n=r=o(d)$ . Since $\\pmb{\\epsilon}\\sim\\mathcal{N}(\\mathbf{0}_{n},\\pmb{I}_{n})$ ,   \n528 we know $\\begin{array}{r}{\\frac{\\left<\\mathbf{n}_{0},\\epsilon\\right>}{\\left|\\left|\\mathbf{n}_{0}\\right|\\right|}\\sim\\mathcal{N}(0,1)}\\end{array}$ . Therefore by Lemma 2, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\frac{\\langle\\mathbf{n}_{0},\\boldsymbol{\\epsilon}\\rangle}{\\|\\mathbf{n}_{0}\\|}\\leq-\\frac{\\nu_{0}^{2}-\\nu_{\\operatorname*{max}}^{2}}{4\\nu_{0}\\sqrt{3\\nu_{0}^{2}+\\nu_{\\operatorname*{max}}^{2}}}\\sqrt{d}\\right)\\leq\\frac{4\\nu_{0}\\sqrt{3\\nu_{0}^{2}+\\nu_{\\operatorname*{max}}^{2}}}{\\sqrt{2\\pi}(\\nu_{0}^{2}-\\nu_{\\operatorname*{max}}^{2})\\sqrt{d}}\\exp\\left(-\\frac{(\\nu_{0}^{2}-\\nu_{\\operatorname*{max}}^{2})^{2}d}{32\\nu_{0}^{2}(3\\nu_{0}^{2}+\\nu_{\\operatorname*{max}}^{2})}\\right)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "529 Conditioned on $\\begin{array}{r}{\\left\\|\\hat{\\mathbf{n}}_{0}\\right\\|^{2}\\ge\\frac{3\\nu_{0}^{2}+\\nu_{\\mathrm{max}}^{2}}{4}d,\\left\\|\\epsilon\\right\\|^{2}>\\frac{3\\nu_{0}^{2}+\\nu_{\\mathrm{max}}^{2}}{4\\nu_{0}^{2}}d}\\end{array}$ and $\\begin{array}{r}{\\frac{1}{\\|\\mathbf{n}_{0}\\|}\\langle\\mathbf{n}_{0},\\epsilon\\rangle\\,>\\,-\\frac{\\nu_{0}^{2}-\\nu_{\\mathrm{max}}^{2}}{4\\nu_{0}\\sqrt{3\\nu_{0}^{2}+\\nu_{\\mathrm{max}}^{2}}}\\sqrt{d},}\\end{array}$   \n530 since Lemma 7 gives $\\alpha^{2}+\\beta\\geq1$ we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\dot{\\mathbf{h}}_{t}\\right|^{2}=\\alpha^{2}\\left\\Vert\\mathbf{n}_{0}\\right\\Vert^{2}+\\beta\\nu_{0}^{2}\\left\\Vert\\epsilon\\right\\Vert^{2}+2\\alpha\\sqrt{\\beta}\\nu_{0}\\langle\\mathbf{n}_{0},\\epsilon\\rangle}\\\\ &{\\qquad\\geq\\alpha^{2}\\left\\Vert\\mathbf{n}_{0}\\right\\Vert^{2}+\\beta\\nu_{0}^{2}\\left\\Vert\\epsilon\\right\\Vert^{2}-2\\alpha\\sqrt{\\beta}\\nu_{0}\\left\\Vert\\mathbf{n}_{0}\\right\\Vert\\frac{\\nu_{0}^{2}-\\nu_{\\mathrm{max}}^{2}}{4\\nu_{0}\\sqrt{3\\nu_{0}^{2}+\\nu_{\\mathrm{max}}^{2}}}\\sqrt{d}}\\\\ &{\\qquad\\geq\\alpha^{2}\\left\\Vert\\mathbf{n}_{0}\\right\\Vert^{2}+\\beta\\nu_{0}^{2}\\left\\Vert\\epsilon\\right\\Vert^{2}-2\\alpha\\sqrt{\\beta}\\nu_{0}\\left\\Vert\\mathbf{n}_{0}\\right\\Vert\\left\\Vert\\epsilon\\right\\Vert\\cdot\\frac{\\nu_{0}^{2}-\\nu_{\\mathrm{max}}^{2}}{6\\nu_{0}^{2}+2\\nu_{\\mathrm{max}}^{2}}}\\\\ &{\\qquad\\geq\\left(1-\\frac{\\nu_{0}^{2}-\\nu_{\\mathrm{max}}^{2}}{6\\nu_{0}^{2}+2\\nu_{\\mathrm{max}}^{2}}\\right)\\left(\\alpha^{2}\\left\\Vert\\mathbf{n}_{0}\\right\\Vert^{2}+\\beta\\nu_{0}^{2}\\left\\Vert\\epsilon\\right\\Vert^{2}\\right)}\\\\ &{\\qquad\\geq\\frac{5\\nu_{0}^{2}+3\\nu_{\\mathrm{max}}^{2}}{6\\nu_{0}^{2}+2\\nu_{\\mathrm{max}}^{2}}\\left(\\alpha^{2}+\\beta\\right)\\cdot\\frac{3\\nu_{0}^{2}+\\nu_{\\mathrm{max}}^{2}}{4}d}\\\\ &{\\qquad\\geq\\frac{5\\nu_{0}^{2}+3\\nu_{\\mathrm{max}}^{2}}{8}d.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "531 Hence by union bound, we complete the proof of Lemma 8. ", "page_idx": 18}, {"type": "text", "text": "532 Upon having all the above lemmas, we are now ready to establish Proposition 3 by induction. Suppose   \n533 the theorem holds for all $T$ values of $1,\\cdot\\cdot\\cdot\\,,T-1$ . We consider the following 3 cases:   \n534   \n535   \n536   \n537   \n538   \n539   \n540   \n541   \n542 ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "\u2022 If there exists some $t\\in[T]$ such that $\\delta_{t}>\\nu_{0}^{2}$ , by Lemma 3 we know that with probability at least $1-\\exp(-\\Omega(d))$ , we have $\\begin{array}{r}{\\left\\|\\mathbf{n}_{t}\\right\\|^{2}\\geq\\frac{3\\nu_{0}^{2}+\\nu_{\\operatorname*{max}}^{2}}{4}d}\\end{array}$ 3\u03bd02+4\u03bd2maxd, thus the problem reduces to the two sub-arrays $\\mathbf{n}_{0},\\cdots,\\mathbf{n}_{t-1}$ and $\\mathbf{n}_{t},\\cdot\\cdot\\cdot,\\mathbf{n}_{T}$ , which can be solved by induction. \u2022 Suppose $\\delta_{t}\\leq\\nu_{0}^{2}$ for all $t\\in[T]$ . If there exists some $t\\in[T]$ such that $\\left\\|\\mathbf{n}_{t-1}\\right\\|^{2}>36\\nu_{0}^{2}d,$ by Lemma 5 we know that with probability at least $1-\\exp(-\\Omega(d))$ , we have $\\left\\|\\mathbf{n}_{t}\\right\\|^{2}\\geq$ 3\u03bd02+4\u03bd2maxd, thus the problem similarly reduces to the two sub-arrays n0, \u00b7 \u00b7 \u00b7 , nt\u22121 and $\\mathbf{n}_{t},\\cdot\\cdot\\cdot,\\mathbf{n}_{T}$ , which can be solved by induction. \u2022 Suppose $\\delta_{t}\\,\\leq\\,\\nu_{0}^{2}$ and $\\left\\|\\mathbf{n}_{t-1}\\right\\|^{2}\\,\\leq\\,36\\nu_{0}^{2}d$ for all $t\\,\\in\\,[T]$ . Conditioned on $\\left\\|\\mathbf{n}_{t-1}\\right\\|^{2}>$ $\\textstyle{\\frac{\\nu_{0}^{2}+\\nu_{\\mathrm{max}}^{2}}{2}}d$ for all $t\\in[T]$ , by Lemma 6 we have that for $T=\\exp(\\mathcal{O}(d))$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|\\hat{{\\bf n}}_{T}-{\\bf n}_{T}\\|<\\left(\\sqrt{\\frac{5\\nu_{0}^{2}+3\\nu_{\\mathrm{max}}^{2}}{8}}-\\sqrt{\\frac{\\nu_{0}^{2}+\\nu_{\\mathrm{max}}^{2}}{2}}\\right)\\sqrt{d}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "543 ", "page_idx": 18}, {"type": "text", "text": "By Lemma 8 we have that with probability at least $1-\\exp(-\\Omega(d))$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left\\|\\hat{\\mathbf{n}}_{T}\\right\\|^{2}\\geq\\frac{5\\nu_{0}^{2}+3\\nu_{\\mathrm{max}}^{2}}{8}d.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|\\mathbf{n}_{T}\\|\\geq\\|\\hat{\\mathbf{n}}_{T}\\|-\\|\\hat{\\mathbf{n}}_{T}-\\mathbf{n}_{T}\\|>\\sqrt{\\frac{\\nu_{0}^{2}+\\nu_{\\operatorname*{max}}^{2}}{2}d}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "545 ", "page_idx": 19}, {"type": "text", "text": "Hence by induction we obtain $\\begin{array}{r}{\\left\\|\\mathbf{n}_{t}\\right\\|^{2}>\\frac{\\nu_{0}^{2}+\\nu_{\\mathrm{max}}^{2}}{2}d}\\end{array}$ for all $t\\in[T]$ with probability at least ", "page_idx": 19}, {"type": "equation", "text": "$$\n(1-(T-1)\\exp(-\\Omega(d)))\\cdot(1-\\exp(-\\Omega(d)))\\geq1-T\\exp(-\\Omega(d)).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "546 Therefore we complete the proof of Proposition 3. ", "page_idx": 19}, {"type": "text", "text": "547 Finally, combining Propositions 2 and 3 finishes the proof of Theorem 1. ", "page_idx": 19}, {"type": "text", "text": "548 A.2 Proof of Theorem 2: Annealed Langevin Dynamics under Gaussian Mixtures ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "549 To establish Theorem 2, we first note from Proposition 1 that perturbing a Gaussian distribution   \n550 $\\mathcal{N}(\\pmb{\\mu},\\nu^{2}\\pmb{I}_{d})$ with noise level $\\sigma$ results in a Gaussian distribution $\\!\\!N(\\pmb{\\mu},(\\nu^{2}+\\sigma^{2})I_{d})$ . Therefore, for   \n551 a Gaussian mixture $\\begin{array}{r}{P=\\sum_{i=0}^{k}w_{i}P^{(i)}=\\sum_{i=0}^{k}w_{i}\\mathcal{N}(\\mu_{i},\\nu_{i}^{2}I_{d}).}\\end{array}$ , the perturbed distribution of noise   \n552 level $\\sigma$ is ", "page_idx": 19}, {"type": "equation", "text": "$$\nP_{\\sigma}=\\sum_{i=0}^{k}w_{i}\\mathcal{N}(\\pmb{\\mu}_{i},(\\nu_{i}^{2}+\\sigma^{2})\\pmb{I}_{d}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "553 Similar to the proof of Theorem 1, we decompose ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{x}_{t}=\\mathbf{R}\\mathbf{r}_{t}+\\mathbf{N}\\mathbf{n}_{t},\\,\\mathrm{and}\\,\\epsilon_{t}=\\mathbf{R}\\epsilon_{t}^{(\\mathbf{r})}+\\mathbf{N}\\epsilon_{t}^{(\\mathbf{n})},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "554 where $\\mathbf{R}\\in\\mathbb{R}^{d\\times r}$ an orthonormal basis of the vector space $\\{\\pmb{\\mu}_{i}\\}_{i\\in[k]}$ and $\\mathbf{N}\\in\\mathbb{R}^{d\\times n}$ an orthonormal   \n555 basis of the null space of $\\{\\pmb{\\mu}_{i}\\}_{i\\in[k]}$ . Now, we prove Theorem 2 by applying the techniques developed   \n556 in Appendix A.1 via substituting $\\nu^{2}$ with $\\nu^{2}+\\sigma_{t}^{2}$ at time step $t$ .   \n557 First, by Proposition 2, suppose that the sample is initialized in the distribution $P_{\\sigma_{0}}^{(0)}$ , then with   \n558 probability at least $1-\\exp(-\\Omega(d))$ , we have ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left\\|\\mathbf{n}_{0}\\right\\|^{2}\\geq\\frac{3(\\nu_{0}^{2}+\\sigma_{0}^{2})+(\\nu_{\\operatorname*{max}}^{2}+\\sigma_{0}^{2})}{4}d=\\frac{3\\nu_{0}^{2}+\\nu_{\\operatorname*{max}}^{2}+4\\sigma_{0}^{2}}{4}d.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "559 Then, with the assumption that the initialization satisfies $\\begin{array}{r}{\\left\\|\\mathbf{n}_{0}\\right\\|^{2}\\geq\\frac{3\\nu_{0}^{2}+\\nu_{\\mathrm{max}}^{2}+4\\sigma_{0}^{2}}{4}d}\\end{array}$ , the following   \n560 proposition similar to Proposition 3 shows that $\\|\\mathbf{n}_{t}\\|$ remains large with high probability.   \n561 Proposition 4. Consider a data distribution $P$ satisfies the constraints specified in Theorem 2.   \n562 We follow annealed Langevin dynamics for $T\\,=\\,\\exp(O(d))$ steps with noise level $c_{\\sigma}~\\geq~\\sigma_{0}~\\geq~$   \n556643 $\\sigma_{1}\\,\\geq\\,\\sigma_{2}\\,\\geq\\,\\cdots\\,\\geq\\,\\sigma_{T}\\,\\geq\\,0$ efno rw istoh mper ocboanbsitliatny t $c_{\\sigma}~>~0$ ,i nwitei ahla svae mthpalte   \n$\\begin{array}{r}{\\|\\mathbf{n}_{0}\\|^{2}\\geq\\frac{3\\nu_{0}^{2}+\\nu_{\\mathrm{max}}^{2}+4\\sigma_{0}^{2}}{4}d,}\\end{array}$ $1-T\\cdot\\exp(-\\Omega(d))$ $\\|\\mathbf{n}_{t}\\|^{2}>$   \n565 \u03bd02+\u03bd2max+2\u03c3t2d for all t \u2208{0} \u222a[T].   \n566 Proof of Proposition 4. We prove Proposition 4 by induction. Suppose the theorem holds for all $T$   \n567 values of $1,\\cdot\\cdot\\cdot,T-1$ . We consider the following 3 cases: ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "\u2022 If there exists some $t\\in[T]$ such that $\\delta_{t}>\\nu_{0}^{2}+\\sigma_{t}^{2}$ , by Lemma 3 we know that with probatbhiluits yt hate  lperaostb $1-\\exp(-\\Omega(d))$ , e wtew oh asvueb $\\begin{array}{r}{\\|\\mathbf{n}_{t}\\|^{2}\\geq\\frac{\\bar{3}(\\nu_{0}^{2}+\\sigma_{t}^{2})+(\\nu_{\\operatorname*{max}}^{2}+\\sigma_{t}^{2})}{4}d=\\frac{3\\nu_{0}^{2}+\\nu_{\\operatorname*{max}}^{2}+4\\sigma_{t}^{2}}{4}d}\\end{array}$ $\\mathbf{n}_{0},\\cdots,\\mathbf{n}_{t-1}$ $\\mathbf{n}_{t},\\cdot\\cdot\\cdot,\\mathbf{n}_{T}$ be solved by induction. ", "page_idx": 19}, {"type": "text", "text": "568   \n569   \n570   \n571   \n572   \n573   \n574   \n575   \n576 ", "page_idx": 19}, {"type": "text", "text": "\u2022 Suppose $\\delta_{t}\\,\\leq\\nu_{0}^{2}+\\sigma_{t}^{2}$ for all $t\\in[T]$ . If there exists some $t\\in[T]$ such that $\\left\\|\\mathbf{n}_{t-1}\\right\\|^{2}>$ $36\\bar{(\\nu_{0}^{2}+\\sigma_{t-1}^{2})}\\bar{d}\\,\\geq\\,3\\bar{6}(\\nu_{0}^{2}+\\sigma_{t}^{2})d$ , by Lemma 5 we know that with probability at least $1\\mathrm{~-~}\\exp(-\\Omega(d))$ , we have $\\begin{array}{r}{\\|\\mathbf{n}_{t}\\|^{2}\\;\\geq\\;(\\nu_{0}^{2}+\\sigma_{t}^{2})d\\;>\\;\\frac{3\\nu_{0}^{2}+\\nu_{\\operatorname*{max}}^{2}+4\\sigma_{t}^{2}}{4}d}\\end{array}$ , thus the problem similarly reduces to the two sub-arrays $\\mathbf{n}_{0},\\cdots,\\mathbf{n}_{t-1}$ and $\\mathbf{n}_{t},\\cdot\\cdot\\cdot,\\mathbf{n}_{T}$ , which can be solved by induction. ", "page_idx": 19}, {"type": "text", "text": "\u2022 Suppose $\\delta_{t}\\leq\\nu_{0}^{2}+\\sigma_{t}^{2}$ and $\\left\\|\\mathbf{n}_{t-1}\\right\\|^{2}\\leq36(\\nu_{0}^{2}+\\sigma_{t-1}^{2})d$ for all $t\\in[T]$ . Consider a surrogate sequence $\\hat{{\\mathbf{n}}}_{t}$ such that $\\hat{\\mathbf{n}}_{0}=\\mathbf{n}_{0}$ and for all $t\\geq1$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{n}}_{t}=\\hat{\\mathbf{n}}_{t-1}-\\frac{\\delta_{t}}{2\\nu_{0}^{2}+2\\sigma_{t}^{2}}\\hat{\\mathbf{n}}_{t-1}+\\sqrt{\\delta_{t}}\\epsilon_{t}^{(\\mathbf{n})}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "579 ", "page_idx": 20}, {"type": "text", "text": "580   \n581 ", "page_idx": 20}, {"type": "text", "text": "Since $\\nu_{0}>\\nu_{i}$ and $c_{\\sigma}\\geq\\sigma_{t}$ for all $t\\in\\left\\{0\\right\\}\\cup\\left[T\\right]$ , we have $\\begin{array}{r}{\\frac{\\nu_{i}^{2}+c_{\\sigma}^{2}}{\\nu_{0}^{2}+c_{\\sigma}^{2}}\\geq\\frac{\\nu_{i}^{2}+\\sigma_{t}^{2}}{\\nu_{0}^{2}+\\sigma_{t}^{2}}}\\end{array}$ . Notice that for function $\\begin{array}{r}{f(z)=\\log z-\\frac{z}{2}+\\frac{1}{2z}}\\end{array}$ , we have $\\begin{array}{r}{\\frac{\\mathrm{d}}{\\mathrm{d}z}f(z)=\\frac{1}{z}-\\frac{1}{2}-\\frac{1}{2z^{2}}=-\\frac{1}{2}\\left(\\frac{1}{z}-1\\right)^{2}\\leq0}\\end{array}$ Thus, by the assumption ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left\\|\\mu_{i}-\\mu_{0}\\right\\|^{2}\\leq\\frac{\\nu_{0}^{2}-\\nu_{i}^{2}}{2}\\left(\\log\\left(\\frac{\\nu_{i}^{2}+c_{\\sigma}^{2}}{\\nu_{0}^{2}+c_{\\sigma}^{2}}\\right)-\\frac{\\nu_{i}^{2}+c_{\\sigma}^{2}}{2\\nu_{0}^{2}+c_{\\sigma}^{2}}+\\frac{\\nu_{0}^{2}+c_{\\sigma}^{2}}{2\\nu_{i}^{2}+c_{\\sigma}^{2}}\\right)d,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "582 ", "page_idx": 20}, {"type": "text", "text": "we have that for all $t\\in[T]$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left\\|\\mu_{i}-\\mu_{0}\\right\\|^{2}\\leq\\frac{\\nu_{0}^{2}-\\nu_{i}^{2}}{2}\\left(\\log\\left(\\frac{\\nu_{i}^{2}+\\sigma_{t}^{2}}{\\nu_{0}^{2}+\\sigma_{t}^{2}}\\right)-\\frac{\\nu_{i}^{2}+\\sigma_{t}^{2}}{2\\nu_{0}^{2}+\\sigma_{t}^{2}}+\\frac{\\nu_{0}^{2}+\\sigma_{t}^{2}}{2\\nu_{i}^{2}+\\sigma_{t}^{2}}\\right)d.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "583   \n584 ", "page_idx": 20}, {"type": "text", "text": "Conditioned on $\\begin{array}{r}{\\left\\|\\mathbf{n}_{t-1}\\right\\|^{2}>\\frac{\\nu_{0}^{2}+\\nu_{\\operatorname*{max}}^{2}+2\\sigma_{t-1}^{2}}{2}d}\\end{array}$ for all $t\\in[T]$ , by Lemma 6 we have that for T = exp(O(d)), ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\|\\hat{\\mathbf{n}}_{T}-\\mathbf{n}_{T}\\|<\\left(\\sqrt{\\frac{5\\nu_{0}^{2}+3\\nu_{\\operatorname*{max}}^{2}+8\\sigma_{T}^{2}}{8}}-\\sqrt{\\frac{\\nu_{0}^{2}+\\nu_{\\operatorname*{max}}^{2}+2\\sigma_{T}^{2}}{2}}\\right)\\sqrt{d}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "585 ", "page_idx": 20}, {"type": "text", "text": "By Lemma 8 we have that with probability at least $1-\\exp(-\\Omega(d))$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left\\|\\hat{\\mathbf{n}}_{T}\\right\\|^{2}\\geq\\frac{5\\nu_{0}^{2}+3\\nu_{\\operatorname*{max}}^{2}+8\\sigma_{T}^{2}}{8}d.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "586 ", "page_idx": 20}, {"type": "text", "text": "Combining the two inequalities implies the desired bound ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\|\\mathbf{n}_{T}\\|\\geq\\|\\hat{\\mathbf{n}}_{T}\\|-\\|\\hat{\\mathbf{n}}_{T}-\\mathbf{n}_{T}\\|>\\sqrt{\\frac{\\nu_{0}^{2}+\\nu_{\\operatorname*{max}}^{2}+2\\sigma_{T}^{2}}{2}}d.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "587   \n588 ", "page_idx": 20}, {"type": "text", "text": "Hence by induction we obtain \u2225nt\u22252 > \u03bd0+\u03bdm 2ax+2\u03c3t2d for all t \u2208{0} \u222a[T] with probability at least ", "page_idx": 20}, {"type": "equation", "text": "$$\n(1-(T-1)\\exp(-\\Omega(d)))\\cdot(1-\\exp(-\\Omega(d)))\\geq1-T\\exp(-\\Omega(d)).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "589 Therefore we complete the proof of Proposition 4. ", "page_idx": 20}, {"type": "text", "text": "590 Finally, combining equation 11 and Proposition 4 finishes the proof of Theorem 2. ", "page_idx": 20}, {"type": "text", "text": "591 A.3 Proof of Theorem 3: Langevin Dynamics under Sub-Gaussian Mixtures ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "592 The proof framework is similar to the proof of Theorem 1. To begin with, we validate Assumption   \n593 2.v. in the following lemma:   \n594 Lemma 9. For constants $\\nu_{0},\\nu_{i},c_{\\nu},c_{L}$ satisfying Assumptions 2.iii. and 2.iv., we have $\\begin{array}{r}{\\frac{(1-c_{\\nu})\\nu_{0}^{2}-\\nu_{i}^{2}}{2\\left(1-c_{\\nu}\\right)}>}\\end{array}$   \n595 0 and $\\begin{array}{r}{\\log\\frac{c_{\\nu}\\nu_{i}^{2}}{(c_{L}^{2}+c_{\\nu}c_{L})\\nu_{0}^{2}}-\\frac{\\nu_{i}^{2}}{2(1-c_{\\nu})\\nu_{0}^{2}}+\\frac{(1-c_{\\nu})\\nu_{0}^{2}}{2\\nu_{i}^{2}}>0}\\end{array}$ are both positive constants.   \n596 Proof of Lemma 9. From Assumption 2.iv. that $\\begin{array}{r}{\\nu_{0}^{2}>\\frac{\\nu_{\\mathrm{max}}^{2}}{1-c_{\\nu}}\\geq\\frac{\\nu_{i}^{2}}{1-c_{\\nu}}}\\end{array}$ , we easily obtain $\\begin{array}{r}{\\frac{(1-c_{\\nu})\\nu_{0}^{2}-\\nu_{i}^{2}}{2\\left(1-c_{\\nu}\\right)}>}\\end{array}$   \n597 0 is a positive constant. For the second property, let $\\begin{array}{r}{f(z):=\\log\\frac{c_{\\nu}\\nu_{i}^{2}}{(c_{L}^{2}+c_{\\nu}c_{L})z}-\\frac{\\nu_{i}^{2}}{2(1-c_{\\nu})z}+\\frac{(1-c_{\\nu})z}{2\\nu_{i}^{2}}}\\end{array}$   \n598 For any $\\begin{array}{r}{z>\\frac{\\nu_{i}^{2}}{1-c_{\\nu}}}\\end{array}$ , the derivative of $f(z)$ satisfies ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}}{\\mathrm{d}z}f(z)=-\\frac{1}{z}+\\frac{\\nu_{i}^{2}}{2(1-c_{\\nu})z^{2}}+\\frac{1-c_{\\nu}}{2\\nu_{i}^{2}}=\\frac{\\nu_{i}^{2}}{2(1-c_{\\nu})}\\left(\\frac{1-c_{\\nu}}{\\nu_{i}^{2}}-\\frac{1}{z}\\right)^{2}>0.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "599 Therefore, when $\\begin{array}{r}{\\frac{4\\left(c_{L}^{2}+c_{\\nu}c_{L}\\right)}{c_{\\nu}\\left(1-c_{\\nu}\\right)}\\leq1}\\end{array}$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\nf(\\nu_{0}^{2})>f\\left(\\frac{\\nu_{i}^{2}}{1-c_{\\nu}}\\right)=\\log\\frac{c_{\\nu}(1-c_{\\nu})}{c_{L}^{2}+c_{\\nu}c_{L}}\\geq\\log4>0.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "600 When $\\begin{array}{r}{\\frac{4\\left(c_{L}^{2}+c_{\\nu}c_{L}\\right)}{c_{\\nu}\\left(1-c_{\\nu}\\right)}>1}\\end{array}$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(\\nu_{0}^{2})>f\\left(\\frac{4(c_{L}^{2}+c_{\\nu}c_{L})}{c_{\\nu}(1-c_{\\nu})}\\frac{\\nu_{i}^{2}}{1-c_{\\nu}}\\right)=2\\log\\frac{c_{\\nu}(1-c_{\\nu})}{2(c_{L}^{2}+c_{\\nu}c_{L})}-\\frac{c_{\\nu}(1-c_{\\nu})}{8(c_{L}^{2}+c_{\\nu}c_{L})}+\\frac{2(c_{L}^{2}+c_{\\nu}c_{L})}{c_{\\nu}(1-c_{\\nu})}}\\\\ &{\\qquad\\ge2-2\\log2-\\frac{2(c_{L}^{2}+c_{\\nu}c_{L})}{c_{\\nu}(1-c_{\\nu})}-\\frac{c_{\\nu}(1-c_{\\nu})}{8(c_{L}^{2}+c_{\\nu}c_{L})}+\\frac{2(c_{L}^{2}+c_{\\nu}c_{L})}{c_{\\nu}(1-c_{\\nu})}>2-2\\log2-\\frac{1}{2}>0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "601 Thus we obtain Lemma 9. ", "page_idx": 21}, {"type": "text", "text": "602 Without loss of generality, we assume $\\pmb{\\mu}_{0}=\\mathbf{0}_{d}$ . Similar to the proof of Theorem 1, we decompose ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{x}_{t}=\\mathbf{R}\\mathbf{r}_{t}+\\mathbf{N}\\mathbf{n}_{t},\\,\\mathrm{and}\\,\\epsilon_{t}=\\mathbf{R}\\epsilon_{t}^{(\\mathbf{r})}+\\mathbf{N}\\epsilon_{t}^{(\\mathbf{n})},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "603 where $\\mathbf{R}\\in\\mathbb{R}^{d\\times r}$ an orthonormal basis of the vector space $\\{\\pmb{\\mu}_{i}\\}_{i\\in[k]}$ and $\\mathbf{N}\\in\\mathbb{R}^{d\\times n}$ an orthonormal   \n604 basis of the null space of $\\{\\pmb{\\mu}_{i}\\}_{i\\in[k]}$ . To show $\\begin{array}{r}{\\|\\mathbf{x}_{t}-\\pmb{\\mu}_{i}\\|^{2}>\\left(\\frac{\\nu_{0}^{2}}{2}+\\frac{\\nu_{\\operatorname*{max}}^{2}}{2(1-c_{\\nu})}\\right)d}\\end{array}$ , it suffices to prove   \n605 $\\begin{array}{r}{\\left\\|\\mathbf{n}_{t}\\right\\|^{2}\\;>\\;\\left(\\frac{\\nu_{0}^{2}}{2}+\\frac{\\nu_{\\operatorname*{max}}^{2}}{2\\left(1-c_{\\nu}\\right)}\\right)d}\\end{array}$ . By Proposition 2, if $\\mathbf{x}_{\\mathrm{0}}$ is initialized in the distribution $P^{(0)}$ , i.e.,   \n606 ${\\bf x}_{0}\\sim P^{(0)}$ , since $\\begin{array}{r}{\\nu_{0}^{2}>\\frac{1}{1-c_{\\nu}}\\nu_{\\mathrm{max}}^{2}}\\end{array}$ , with probability at least $1-\\exp(-\\Omega(d))$ we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left\\|\\mathbf{n}_{0}\\right\\|^{2}\\geq\\left(\\frac{3\\nu_{0}^{2}}{4}+\\frac{\\nu_{\\operatorname*{max}}^{2}}{4(1-c_{\\nu})}\\right)d.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "607 Then, conditioned on $\\begin{array}{r}{\\left\\|\\mathbf{n}_{0}\\right\\|^{2}\\;\\geq\\;\\left(\\frac{3\\nu_{0}^{2}}{4}+\\frac{\\nu_{\\operatorname*{max}}^{2}}{4(1-c_{\\nu})}\\right)d}\\end{array}$ , the following proposition shows that $\\left\\|\\mathbf{n}_{t}\\right\\|$   \n608 remains large with high probability.   \n609 Proposition 5. Consider a distribution $P$ satisfying Assumption 2. We follow the Langevin dynamics   \n610 for $T=\\exp(O(d))$ steps. Suppose that the initial sample satisfies $\\begin{array}{r}{\\left\\|\\mathbf{n}_{0}\\right\\|^{2}\\geq\\left(\\frac{3\\nu_{0}^{2}}{4}+\\frac{\\nu_{\\operatorname*{max}}^{2}}{4(1-c_{\\nu})}\\right)d,}\\end{array}$   \n611 then with probability at least $1-T\\cdot\\exp(-\\Omega(d))$ , we have that $\\begin{array}{r}{\\left\\|\\mathbf{n}_{t}\\right\\|^{2}>\\left(\\frac{\\nu_{0}^{2}}{2}+\\frac{\\nu_{\\operatorname*{max}}^{2}}{2\\left(1-c_{\\nu}\\right)}\\right)d}\\end{array}$ for all   \n612 $t\\in\\{0\\}\\cup[T]$ .   \n613 Proof of Proposition 5. Firstly, by Lemma 3, if $\\delta_{t}>\\nu_{0}^{2}$ , since $\\begin{array}{r}{\\nu_{0}^{2}>\\frac{\\nu_{\\mathrm{max}}^{2}}{1-c_{\\nu}}}\\end{array}$ , we similarly have that   \n614 \u2225nt\u22252 \u2265 3\u03bd402 +4(\u03bd12m\u2212acx\u03bd) with probability at least $1-\\exp(-\\Omega(d))$ regardless of the previous   \n615 state $\\mathbf x_{t-1}$ . We then consider the case when $\\delta_{t}\\leq\\nu_{0}^{2}$ . Intuitively, we aim to prove that the score   \n616 function is close to $-\\frac{\\mathbf{x}}{\\nu_{0}^{2}}$ when $\\begin{array}{r}{\\left\\|\\mathbf{n}\\right\\|^{2}\\geq\\left(\\frac{\\nu_{0}^{2}}{2}+\\frac{\\nu_{\\operatorname*{max}}^{2}}{2(1-c_{\\nu})}\\right)d}\\end{array}$ . Towards this goal, we first show that   \n617 $P^{(0)}(\\mathbf{x})$ is exponentially larger than $P^{(i)}(\\mathbf{x})$ for all $i\\in[k]$ in the following lemma:   \n618 Lemma 10. Suppose $P$ satisfies Assumption 2. Then for any $\\begin{array}{r}{\\left\\|\\mathbf{n}\\right\\|^{2}\\geq\\left(\\frac{\\nu_{0}^{2}}{2}+\\frac{\\nu_{\\operatorname*{max}}^{2}}{2\\left(1-c_{\\nu}\\right)}\\right)d,}\\end{array}$ , we have   \n619 $\\begin{array}{r}{\\frac{P^{(i)}(\\mathbf{x})}{P^{(0)}(\\mathbf{x})}\\leq\\exp(-\\Omega(d))\\,a n d\\,\\frac{\\left\\|\\nabla_{\\mathbf{x}}P^{(i)}(\\mathbf{x})\\right\\|}{P(\\mathbf{x})}\\leq\\exp(-\\Omega(d))\\,f o r\\,a l l\\,i\\in[k].}\\end{array}$   \n620 Proof of Lemma $I O$ . We first give an upper bound on the sub-Gaussian probability density. For any   \n621 vector $\\mathbf{v}\\in\\mathbb{R}^{d}$ , by considering some vector $\\mathbf{m}\\in\\mathbb{R}^{d}$ , from Markov\u2019s inequality and the definition in   \n622 equation 4 we can bound ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}_{\\mathbf{z}\\sim P^{(i)}}\\left(\\mathbf{m}^{T}(\\mathbf{z}-\\mu_{i})\\geq\\mathbf{m}^{T}(\\mathbf{v}-\\mu_{i})\\right)\\leq\\frac{\\mathbb{E}_{\\mathbf{z}\\sim P^{(i)}}\\left[\\exp\\left(\\mathbf{m}^{T}(\\mathbf{z}-\\mu_{i})\\right)\\right]}{\\exp\\left(\\mathbf{m}^{T}\\left(\\mathbf{v}-\\mu_{i}\\right)\\right)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\exp\\left(\\frac{\\nu_{i}^{2}\\left\\Vert\\mathbf{m}\\right\\Vert^{2}}{2}-\\mathbf{m}^{T}(\\mathbf{v}-\\mu_{i})\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "623 Upon optimizing the last term at $\\begin{array}{r}{\\mathbf{m}=\\frac{\\mathbf{v}-\\pmb{\\mu}_{i}}{\\nu_{i}^{2}}}\\end{array}$ , we obtain ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{P}_{\\mathbf{z}\\sim P^{(i)}}\\left((\\mathbf{v}-\\pmb{\\mu}_{i})^{T}(\\mathbf{v}-\\mathbf{z})\\leq0\\right)\\leq\\exp\\left(-\\frac{\\|\\mathbf{v}-\\pmb{\\mu}_{i}\\|^{2}}{2\\nu_{i}^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "624 Denote $\\mathbb{B}:=\\left\\{\\mathbf{z}:(\\mathbf{v}-\\pmb{\\mu}_{i})^{T}(\\mathbf{v}-\\mathbf{z})\\leq0\\right\\}$ . To bound $\\mathbb{P}_{\\mathbf{z}\\sim P^{(i)}}(\\mathbf{z}\\in\\mathbb{B})$ , we first note that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\log P^{(i)}(\\mathbf{v})-\\log P^{(i)}(\\mathbf{z})}\\\\ &{=\\int_{0}^{1}\\langle\\mathbf{v}-\\mathbf{z},\\nabla\\log P^{(i)}(\\mathbf{v}+\\lambda(\\mathbf{z}-\\mathbf{v}))\\rangle\\,\\mathrm{d}\\lambda}\\\\ &{=\\langle\\mathbf{v}-\\mathbf{z},\\nabla\\log P^{(i)}(\\mathbf{v})\\rangle+\\int_{0}^{1}\\langle\\mathbf{v}-\\mathbf{z},\\nabla\\log P^{(i)}(\\mathbf{v}+\\lambda(\\mathbf{z}-\\mathbf{v}))-\\nabla\\log P^{(i)}(\\mathbf{v})\\rangle\\,\\mathrm{d}\\lambda}\\\\ &{\\leq\\|\\mathbf{v}-\\mathbf{z}\\|\\left\\|\\nabla\\log P^{(i)}(\\mathbf{v})\\right\\|+\\int_{0}^{1}\\|\\mathbf{v}-\\mathbf{z}\\|\\left\\|\\nabla\\log P^{(i)}(\\mathbf{v}+\\lambda(\\mathbf{z}-\\mathbf{v}))-\\nabla\\log P^{(i)}(\\mathbf{v})\\right\\|\\,\\mathrm{d}\\lambda}\\\\ &{\\leq\\|\\mathbf{v}-\\mathbf{z}\\|\\cdot L_{i}\\,\\|\\mathbf{v}-\\boldsymbol{\\mu}_{i}\\|+\\int_{0}^{1}\\|\\mathbf{v}-\\mathbf{z}\\|\\cdot L_{i}\\,\\|\\lambda(\\mathbf{z}-\\mathbf{v})\\|\\,\\mathrm{d}\\lambda}\\\\ &{\\leq\\frac{L_{i}c_{v}}{2c_{L}}\\,\\|\\mathbf{v}-\\boldsymbol{\\mu}_{i}\\|^{2}+\\left(\\frac{C L+c_{v}}{2c_{v}}\\right)L_{i}\\,\\|\\mathbf{v}-\\mathbf{z}\\|^{2}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "625 where equation 14 follows from Assumption 2.ii. that $\\nabla\\log P^{(i)}(\\pmb{\\mu}_{i})=\\pmb{0}_{d}$ and Assumption 2.iii.   \n626 that the score function $\\nabla\\log{P^{(i)}}$ is $L_{i}$ -Lipschitz. Therefore we obtain ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{P}_{\\mathbf{z}\\sim P^{(i)}}(\\mathbf{z}\\in\\mathbb{B})=\\int_{\\mathbf{z}\\in\\mathbb{B}}P^{(i)}(\\mathbf{z})\\,\\mathrm{d}\\mathbf{z}}\\\\ {\\displaystyle\\ge\\int_{\\mathbf{z}\\in\\mathbb{B}}P^{(i)}(\\mathbf{v})\\exp\\left(-\\frac{L_{i}c_{\\nu}}{2c_{L}}\\left\\|\\mathbf{v}-\\pmb{\\mu}_{i}\\right\\|^{2}-\\frac{c_{L}+c_{\\nu}}{2c_{\\nu}}L_{i}\\left\\|\\mathbf{v}-\\mathbf{z}\\right\\|^{2}\\right)\\,\\mathrm{d}\\mathbf{z}}\\\\ {\\displaystyle=P^{(i)}(\\mathbf{v})\\exp\\left(-\\frac{L_{i}c_{\\nu}}{2c_{L}}\\left\\|\\mathbf{v}-\\pmb{\\mu}_{i}\\right\\|^{2}\\right)\\int_{\\mathbf{z}\\in\\mathbb{B}}\\exp\\left(-\\frac{c_{L}+c_{\\nu}}{2c_{\\nu}}L_{i}\\left\\|\\mathbf{v}-\\mathbf{z}\\right\\|^{2}\\right)\\,\\mathrm{d}\\mathbf{z}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "627 By observing that $g:\\mathbb{B}\\rightarrow\\{\\mathbf{z}:(\\mathbf{v}-\\pmb{\\mu}_{i})^{T}(\\mathbf{v}-\\mathbf{z})\\geq0\\}$ with $g(\\mathbf{z})=2\\mathbf{v}-\\mathbf{z}$ is a bijection such that   \n628 $\\|\\mathbf{v-z}\\|=\\|\\mathbf{v-\\boldsymbol{g}}(\\mathbf{z})\\|$ for any $\\mathbf{z}\\in\\mathbb{B}$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\int_{\\mathbf{z}\\in\\mathbb{B}}\\exp\\left(-\\frac{c_{L}+c_{\\nu}}{2c_{\\nu}}L_{i}\\left\\Vert\\mathbf{v}-\\mathbf{z}\\right\\Vert^{2}\\right)\\,\\mathrm{d}\\mathbf{z}=\\frac{1}{2}\\int_{\\mathbf{z}\\in\\mathbb{R}^{d}}\\exp\\left(-\\frac{c_{L}+c_{\\nu}}{2c_{\\nu}}L_{i}\\left\\Vert\\mathbf{v}-\\mathbf{z}\\right\\Vert^{2}\\right)\\,\\mathrm{d}\\mathbf{z}}\\\\ {\\displaystyle=\\frac{1}{2}\\left(\\frac{2\\pi c_{\\nu}}{\\left(c_{L}+c_{\\nu}\\right)L_{i}}\\right)^{\\frac{d}{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "629 Hence, by combining equation 13, equation 15, and equation 16, we obtain ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\exp\\left(-\\displaystyle\\frac{\\|\\mathbf{v}-\\pmb{\\mu}_{i}\\|^{2}}{2\\nu_{i}^{2}}\\right)\\geq\\mathbb{P}_{\\mathbf{z}\\sim P^{(i)}}\\left(\\left(\\mathbf{v}-\\pmb{\\mu}_{i}\\right)^{T}(\\mathbf{v}-\\mathbf{z})\\leq0\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\geq P^{(i)}(\\mathbf{v})\\exp\\left(-\\displaystyle\\frac{L_{i}c_{\\nu}}{2c_{L}}\\,\\|\\mathbf{v}-\\pmb{\\mu}_{i}\\|^{2}\\right)\\cdot\\frac{1}{2}\\left(\\frac{2\\pi c_{\\nu}}{(c_{L}+c_{\\nu})L_{i}}\\right)^{\\frac{d}{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "630 By Assumption 2.iii. that $\\begin{array}{r}{L_{i}\\leq\\frac{c_{L}}{\\nu_{i}^{2}}}\\end{array}$ we obtain the following bound on the probability density: ", "page_idx": 22}, {"type": "equation", "text": "$$\nP^{(i)}(\\mathbf{v})\\leq2\\left(\\frac{2\\pi c_{\\nu}\\nu_{i}^{2}}{(c_{L}+c_{\\nu})c_{L}}\\right)^{-\\frac{d}{2}}\\exp\\left(-\\frac{1-c_{\\nu}}{2\\nu_{i}^{2}}\\left\\Vert\\mathbf{v}-\\pmb{\\mu}_{i}\\right\\Vert^{2}\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "631 Then we can bound the ratio of $P^{(i)}$ and $P^{(0)}$ . For all $i\\in[k]$ , define $\\begin{array}{r}{\\rho_{i}(\\mathbf{x}):=\\frac{P^{(i)}(\\mathbf{x})}{P^{(0)}(\\mathbf{x})}}\\end{array}$ P ((i0))(x), then we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{n}(\\mathbf{x})=\\frac{P^{(i)}(\\mathbf{x})}{P^{(0)}(\\mathbf{x})}\\leq\\frac{2(2\\pi c_{\\nu}\\nu_{i}^{2}/(c_{L}^{2}+c_{\\nu}c_{L}))^{-d/2}\\exp\\big(-(1-c_{\\nu})\\,\\|\\mathbf{x}-\\mu_{i}\\|^{2}/2\\nu_{i}^{2}\\big)}{(2\\pi\\nu_{0}^{2})^{-d/2}\\exp\\big(-\\|\\mathbf{x}\\|^{2}/2\\nu_{0}^{2}\\big)}}\\\\ &{=2\\left(\\frac{(c_{L}^{2}+c_{\\nu}c_{L})\\nu_{0}^{2}}{c_{\\nu}\\nu_{i}^{2}}\\right)^{\\frac{\\mathfrak{q}}{2}}\\exp\\left(\\frac{\\|\\mathbf{x}\\|^{2}}{2\\nu_{0}^{2}}-\\frac{(1-c_{\\nu})\\,\\|\\mathbf{x}-\\mu_{i}\\|^{2}}{2\\nu_{i}^{2}}\\right)}\\\\ &{=2\\left(\\frac{(c_{L}^{2}+c_{\\nu}c_{L})\\nu_{0}^{2}}{c_{\\nu}\\nu_{i}^{2}}\\right)^{\\frac{\\mathfrak{q}}{2}}\\exp\\left(\\left(\\frac{1}{2\\nu_{0}^{2}}-\\frac{1-c_{\\nu}}{2\\nu_{i}^{2}}\\right)\\|\\mathbf{N}\\mathbf{n}\\|^{2}+\\left(\\frac{\\|\\mathbf{R}\\mathbf{r}\\|^{2}}{2\\nu_{0}^{2}}-\\frac{(1-c_{\\nu})\\,\\|\\mathbf{R}\\mathbf{r}-\\mu_{i}\\|^{2}}{2\\nu_{i}^{2}}\\right)\\right)}\\\\ &{=2\\left(\\frac{(c_{L}^{2}+c_{\\nu}c_{L})\\nu_{0}^{2}}{c_{\\nu}\\nu_{i}^{2}}\\right)^{\\frac{\\mathfrak{q}}{2}}\\exp\\left(\\left(\\frac{1}{2\\nu_{0}^{2}}-\\frac{1-c_{\\nu}}{2\\nu_{i}^{2}}\\right)\\|\\mathbf{n}\\|^{2}+\\left(\\frac{\\|\\mathbf{r}\\|^{2}}{2\\nu_{0}^{2}}-\\frac{(1-c_{\\nu})\\,\\|\\mathbf{r}-\\mathbf{R}^{T}\\mu_{i}\\|^ \n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "632 where the last step follows from the definition that $\\mathbf{R}\\in\\mathbb{R}^{d\\times r}$ an orthogonal basis of the vector space   \n633 $\\{\\pmb{\\mu}_{i}\\}_{i\\in[k]}$ and $\\mathbf{N}^{T}\\mathbf{N}=I_{n}$ . Since $\\nu_{i}^{2}<(1-c_{\\nu})\\nu_{0}^{2}$ , the quadratic term $\\begin{array}{r}{\\frac{\\|\\mathbf{r}\\|^{2}}{2\\nu_{0}^{2}}-\\frac{\\left(1-c_{\\nu}\\right)\\left\\|\\mathbf{r}-\\mathbf{R}^{T}\\pmb{\\mu}_{i}\\bar{\\left\\|}^{2}\\right.}{2\\nu_{i}^{2}}}\\end{array}$ is   \n634 maximized at (1\u2212c\u03bd)\u03bd022RT \u00b52i. Therefore, we obtain ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{\\left\\|\\mathbf{r}\\right\\|^{2}}{2\\nu_{0}^{2}}-\\frac{\\left(1-c_{\\nu}\\right)\\left\\|\\mathbf{r}-\\mathbf{R}^{T}\\pmb{\\mu}_{i}\\right\\|^{2}}{2\\nu_{i}^{2}}\\leq\\frac{(1-c_{\\nu})\\left\\|\\pmb{\\mu}_{i}\\right\\|^{2}}{2((1-c_{\\nu})\\nu_{0}^{2}-\\nu_{i}^{2})}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "635 $\\begin{array}{r}{\\left\\|\\mu_{i}-\\mu_{0}\\right\\|^{2}\\leq\\frac{(1-c_{\\nu})\\nu_{0}^{2}-\\nu_{i}^{2}}{2(1-c_{\\nu})}\\left(\\log\\frac{c_{\\nu}\\nu_{i}^{2}}{(c_{L}^{2}+c_{\\nu}c_{L})\\nu_{0}^{2}}-\\frac{\\nu_{i}^{2}}{2(1-c_{\\nu})\\nu_{0}^{2}}+\\frac{(1-c_{\\nu})\\nu_{0}^{2}}{2\\nu_{i}^{2}}\\right)d\\nu_{0}^{2}.}\\end{array}$ and $\\left\\|\\mathbf{n}\\right\\|^{2}\\geq$   \n636 $\\begin{array}{r}{\\left(\\frac{\\nu_{0}^{2}}{2}+\\frac{\\nu_{\\mathrm{max}}^{2}}{2(1-c_{\\nu})}\\right)d.}\\end{array}$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\gamma_{i}(\\mathbf{x})\\leq2\\left(\\frac{\\left(c_{l}^{2}+c_{\\nu}c_{L}\\right)\\nu_{0}^{2}}{c_{\\nu}\\nu_{i}^{2}}\\right)^{\\frac{d}{2}}\\exp\\left(\\left(\\frac{1}{2\\nu_{0}^{2}}-\\frac{1-c_{\\nu}}{2\\nu_{i}^{2}}\\right)\\|\\mathbf{n}\\|^{2}+\\frac{\\left(1-c_{\\nu}\\right)\\|\\mu_{i}\\|^{2}}{2\\left((1-c_{\\nu})\\nu_{0}^{2}-\\nu_{i}^{2}\\right)}\\right)}\\\\ &{\\leq2\\left(\\frac{\\left(c_{L}^{2}+c_{\\nu}c_{L}\\right)\\nu_{0}^{2}}{c_{\\nu}\\nu_{i}^{2}}\\right)^{\\frac{d}{2}}\\exp\\left(\\left(\\frac{1}{2\\nu_{0}^{2}}-\\frac{1-c_{\\nu}}{2\\nu_{i}^{2}}\\right)\\left(\\frac{\\nu_{0}^{2}}{2}+\\frac{\\nu_{i}^{2}}{2(1-c_{\\nu})}\\right)d+\\frac{\\left(1-c_{\\nu}\\right)\\|\\mu_{i}\\|^{2}}{2\\left((1-c_{\\nu})\\nu_{0}^{2}-\\nu_{i}^{2}\\right)}\\right)}\\\\ &{=2\\exp\\left(-\\left(\\log\\frac{c_{\\nu}\\nu_{i}^{2}}{\\left(c_{L}^{2}+c_{\\nu}c_{L}\\right)\\nu_{0}^{2}}-\\frac{\\nu_{i}^{2}}{2\\left(1-c_{\\nu}\\right)\\nu_{0}^{2}}+\\frac{\\left(1-c_{\\nu}\\right)\\nu_{0}^{2}}{2\\nu_{i}^{2}}\\right)\\frac{d}{2}+\\frac{\\left(1-c_{\\nu}\\right)\\|\\mu_{i}\\|^{2}}{2\\left((1-c_{\\nu})\\nu_{0}^{2}-\\nu_{i}^{2}\\right)}\\right)}\\\\ &{\\leq2\\exp\\left(-\\left(\\log\\frac{c_{\\nu}\\nu_{i}^{2}}{\\left(c_{L}^{2}+c_{\\nu}c_{L}\\right)\\nu_{0}^{2}}-\\frac{\\nu_{i}^{2}}{2\\left(1-c_{\\nu}\\right)\\nu_{0}^{2}}+\\frac{\\left(1-c_{\\\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "637 From Lemma 9, we obtain $\\rho_{i}(\\mathbf{x})\\leq\\exp(-\\Omega(d))$ . ", "page_idx": 23}, {"type": "text", "text": "638 To show $\\begin{array}{r l}{\\left\\|\\nabla_{\\mathbf{x}}P^{(i)}(\\mathbf{x})\\right\\|}&{{}\\leq\\exp(-\\Omega(d))}\\end{array}$ , from Assumptions 2.ii. and 2.iii. we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\frac{\\nabla_{\\mathbf x}P^{(i)}(\\mathbf x)}{P^{(i)}(\\mathbf x)}\\right\\|=\\left\\|\\frac{\\nabla_{\\mathbf x}P^{(i)}(\\mathbf x)}{P^{(i)}(\\mathbf x)}-\\frac{\\nabla_{\\mathbf x}P^{(i)}(\\mu_{i})}{P^{(i)}(\\mu_{i})}\\right\\|=\\left\\|\\nabla_{\\mathbf x}\\log P^{(i)}(\\mathbf x)-\\nabla_{\\mathbf x}\\log P^{(i)}(\\mu_{i})\\right\\|}\\\\ &{\\qquad\\qquad\\qquad\\leq L_{i}\\left\\|\\mathbf x-\\boldsymbol\\mu_{i}\\right\\|\\leq\\frac{c_{L}}{\\nu_{i}^{2}}\\left\\|\\mathbf x-\\boldsymbol\\mu_{i}\\right\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "663490 sTmhearlle,f obrye $\\begin{array}{r}{\\frac{\\left\\|\\nabla_{\\mathbf{x}}P^{(i)}(\\mathbf{x})\\right\\|}{P(\\mathbf{x})}\\,\\leq\\,\\frac{c_{L}}{\\nu_{i}^{2}}\\rho_{i}(\\mathbf{x})\\left\\|\\mathbf{x}-\\pmb{\\mu}_{i}\\right\\|}\\end{array}$ $\\|\\mathbf{x}-\\pmb{\\mu}_{i}\\|\\;=\\;\\exp(o(d))$ $\\rho_{i}(\\mathbf{x})\\leq\\exp(-\\Omega(d))$ $\\begin{array}{r l}{\\left\\|\\nabla_{\\mathbf{x}}P^{(i)}(\\mathbf{x})\\right\\|}&{{}\\leq\\exp(-\\Omega(d))}\\end{array}$ $\\left\\|\\mathbf{x}-\\pmb{\\mu}_{i}\\right\\|=$ 641 $\\exp(\\Omega(d))$ is exceedingly large, from equation 17 we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{\\left\\|\\nabla_{\\mathbf x}P^{(i)}(\\mathbf x)\\right\\|}{P(\\mathbf x)}\\leq\\frac{2c_{L}}{\\nu_{i}^{2}}\\left(\\frac{\\left(c_{L}^{2}+c_{\\nu}c_{L}\\right)\\nu_{0}^{2}}{c_{\\nu}\\nu_{i}^{2}}\\right)^{\\frac{d}{2}}\\exp\\left(\\frac{\\left\\|\\mathbf x\\right\\|^{2}}{2\\nu_{0}^{2}}-\\frac{\\left(1-c_{\\nu}\\right)\\left\\|\\mathbf x-\\boldsymbol\\mu_{i}\\right\\|^{2}}{2\\nu_{i}^{2}}\\right)\\left\\|\\mathbf x-\\boldsymbol\\mu_{i}\\right\\|.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "642 Since $\\begin{array}{r}{\\nu_{0}^{2}>\\frac{\\nu_{i}^{2}}{1-c_{\\nu}}}\\end{array}$ , when $\\|\\mathbf{x}-\\pmb{\\mu}_{i}\\|=\\exp(\\Omega(d))\\gg\\|\\pmb{\\mu}_{i}\\|$ we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\exp\\left(\\frac{\\left\\|\\mathbf{x}\\right\\|^{2}}{2\\nu_{0}^{2}}-\\frac{\\left(1-c_{\\nu}\\right)\\left\\|\\mathbf{x}-\\pmb{\\mu}_{i}\\right\\|^{2}}{2\\nu_{i}^{2}}\\right)=\\exp(-\\Omega(\\left\\|\\mathbf{x}-\\pmb{\\mu}_{i}\\right\\|^{2})).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "643 Therefore $\\begin{array}{r l}{\\left\\|\\nabla_{\\mathbf{x}}P^{(i)}(\\mathbf{x})\\right\\|}&{{}\\leq\\exp(-\\Omega(d))}\\end{array}$ . Thus we complete the proof of Lemma 10. ", "page_idx": 24}, {"type": "text", "text": "644 Similar to Lemma 5, the following lemma proves that when the previous state $\\mathbf{n}_{t-1}$ is far from a   \n645 mode, a single step of Langevin dynamics with bounded step size is not enough to find the mode.   \n646 Lemma 11. Suppose $\\delta_{t}\\leq\\nu_{0}^{2}$ and $\\left\\|\\mathbf{n}_{t-1}\\right\\|^{2}>36\\nu_{0}^{2}d,$ then we have $\\left\\|\\mathbf{n}_{t}\\right\\|^{2}\\geq\\nu_{0}^{2}d$ with probability at   \n647 least $1-\\exp(-\\Omega(d))$ .   \n648 Proof of Lemma $_{l l}$ . For simplicity, denote $\\begin{array}{r}{\\begin{array}{r l}{\\mathbf{v}}&{:=\\mathbf{\\n}_{t-1}+\\frac{\\delta_{t}}{2}\\mathbf{N}^{T}\\nabla_{\\mathbf{x}}\\log P(\\mathbf{x}_{t-1})}\\end{array}}\\end{array}$ . Since ${\\cal P}\\;=\\;$   \n649 $\\textstyle\\sum_{i=0}^{k}w_{i}P^{(i)}$ and $P^{(0)}=\\mathcal{N}(\\pmb{\\mu}_{0},\\nu_{0}^{2}I_{d})$ , the score function can be written as ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\mathbf x}\\log P(\\mathbf x)=\\cfrac{\\nabla_{\\mathbf x}P(\\mathbf x)}{P(\\mathbf x)}=\\cfrac{\\nabla_{\\mathbf x}w_{0}P^{(0)}(\\mathbf x)}{P(\\mathbf x)}+\\sum_{i\\in[k]}\\cfrac{\\nabla_{\\mathbf x}w_{i}P^{(i)}(\\mathbf x)}{P(\\mathbf x)}}\\\\ &{\\qquad\\qquad=-\\cfrac{w_{0}P^{(0)}(\\mathbf x)}{P(\\mathbf x)}\\cdot\\cfrac{\\mathbf x}{\\nu_{0}^{2}}+\\sum_{i\\in[k]}\\cfrac{w_{i}\\nabla_{\\mathbf x}P^{(i)}(\\mathbf x)}{P(\\mathbf x)}}\\\\ &{\\qquad\\qquad=-\\cfrac{\\mathbf x}{\\nu_{0}^{2}}+\\sum_{i\\in[k]}\\cfrac{w_{i}P^{(i)}(\\mathbf x)}{P(\\mathbf x)}\\cdot\\cfrac{\\mathbf x}{\\nu_{0}^{2}}+\\sum_{i\\in[k]}\\cfrac{w_{i}\\nabla_{\\mathbf x}P^{(i)}(\\mathbf x)}{P(\\mathbf x)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "650 For $\\left\\|\\mathbf{n}_{t-1}\\right\\|^{2}>36\\nu_{0}^{2}d$ by Lemma 10 we have $\\begin{array}{r l}{\\lefteqn{\\frac{\\left\\|\\nabla_{\\mathbf{x}}P^{(i)}(\\mathbf{x}_{t-1})\\right\\|}{P(\\mathbf{x}_{t-1})}\\leq\\exp(-\\Omega(d))}}\\end{array}$ . Since $\\delta_{t}\\leq\\nu_{0}^{2}$ , we   \n651 can bound the norm of $\\mathbf{v}$ by ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathbf{v}\\|=\\bigg\\|\\mathbf{n}_{t-1}+\\frac{\\delta_{t}}{2}\\mathbf{N}^{T}\\nabla_{\\mathbf{x}}\\log P(\\mathbf{x}_{t-1})\\bigg\\|}\\\\ &{\\qquad=\\bigg\\|\\mathbf{n}_{t-1}-\\frac{\\delta_{t}}{2\\nu_{0}^{2}}\\mathbf{n}_{t-1}+\\sum_{i\\in[k]}\\frac{w_{i}\\delta_{t}}{2\\nu_{0}^{2}}\\frac{P^{(i)}(\\mathbf{x}_{t-1})}{P(\\mathbf{x}_{t-1})}\\mathbf{n}_{t-1}+\\sum_{i\\in[k]}\\frac{w_{i}\\delta_{t}}{2}\\frac{\\mathbf{N}^{T}\\nabla_{\\mathbf{x}}P^{(i)}(\\mathbf{x}_{t-1})}{P(\\mathbf{x}_{t-1})}\\bigg\\|}\\\\ &{\\qquad\\geq\\bigg\\|\\left(1-\\frac{\\delta_{t}}{2\\nu_{0}^{2}}+\\sum_{i\\in[k]}\\frac{w_{i}\\delta_{t}}{2\\nu_{0}^{2}}\\frac{P^{(i)}(\\mathbf{x}_{t-1})}{P(\\mathbf{x}_{t-1})}\\right)\\mathbf{n}_{t-1}\\right\\|-\\sum_{i\\in[k]}\\frac{w_{i}\\delta_{t}}{2}\\frac{\\|\\nabla_{\\mathbf{x}}P^{(i)}(\\mathbf{x}_{t-1})\\|}{P(\\mathbf{x}_{t-1})}}\\\\ &{\\qquad\\geq\\frac{1}{2}\\|\\mathbf{n}_{t-1}\\|-\\sum_{i\\in[k]}\\frac{w_{i}\\delta_{t}}{2}\\exp(-\\Omega(d))}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "652 On the other hand, from \u03f5t(n)\u223cN(0n, In) we know $\\begin{array}{r}{\\frac{\\langle\\mathbf v,\\epsilon_{t}^{(\\mathbf n)}\\rangle}{\\|\\mathbf v\\|}\\sim\\mathcal{N}(0,1)}\\end{array}$ for any fixed $\\mathbf{v}\\neq\\mathbf{0}_{n}$ , hence   \n653 by Lemma 2 we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\frac{\\langle\\mathbf{v},\\pmb{\\epsilon}_{t}^{(\\mathbf{n})}\\rangle}{\\|\\mathbf{v}\\|}\\geq\\frac{\\sqrt{d}}{4}\\right)=\\mathbb{P}\\left(\\frac{\\langle\\mathbf{v},\\pmb{\\epsilon}_{t}^{(\\mathbf{n})}\\rangle}{\\|\\mathbf{v}\\|}\\leq-\\frac{\\sqrt{d}}{4}\\right)\\leq\\frac{4}{\\sqrt{2\\pi d}}\\exp\\left(-\\frac{d}{32}\\right)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "654 Combining the above inequalities gives ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\|\\mathbf{n}_{t}\\|^{2}=\\left\\|\\mathbf{v}+\\sqrt{\\delta_{t}}\\epsilon_{t}^{\\mathbf{(n)}}\\right\\|^{2}\\geq\\|\\mathbf{v}\\|^{2}-2\\nu_{0}|\\langle\\mathbf{v},\\epsilon_{t}^{\\mathbf{(n)}}\\rangle|\\geq\\|\\mathbf{v}\\|^{2}-\\frac{\\nu_{0}\\sqrt{d}}{2}\\,\\|\\mathbf{v}\\|>\\nu_{0}^{2}d\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "655 with probability at least $\\begin{array}{r}{1-\\frac{8}{\\sqrt{2\\pi d}}\\exp\\left(-\\frac{d}{32}\\right)=1-\\exp(-\\Omega(d))}\\end{array}$ . This proves Lemma 11. ", "page_idx": 24}, {"type": "text", "text": "656 When $\\left\\|\\mathbf{n}_{t-1}\\right\\|^{2}\\,\\leq\\,36\\nu_{0}^{2}d,$ , similar to Theorem 1, we consider a surrogate recursion $\\hat{{\\bf n}}_{t}$ such that   \n657 $\\hat{\\mathbf{n}}_{0}=\\mathbf{n}_{0}$ and for all $t\\geq1$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\hat{\\bf n}_{t}=\\hat{\\bf n}_{t-1}-\\frac{\\delta_{t}}{2\\nu_{0}^{2}}\\hat{\\bf n}_{t-1}+\\sqrt{\\delta_{t}}\\epsilon_{t}^{({\\bf n})}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "658 The following Lemma shows that $\\hat{{\\bf n}}_{t}$ is sufficiently close to the original recursion $\\mathbf{n}_{t}$ . ", "page_idx": 25}, {"type": "text", "text": "659 Lemma 12. For any $t\\geq1$ , given that for all $j\\in[t],\\,\\delta_{j}\\leq\\nu_{0}^{2}$ and $\\begin{array}{r}{\\left(\\frac{\\nu_{0}^{2}}{2}+\\frac{\\nu_{\\operatorname*{max}}^{2}}{2(1-c_{\\nu})}\\right)d\\leq\\left\\|\\mathbf{n}_{j-1}\\right\\|^{2}\\leq}\\end{array}$   \n660 $36\\nu_{0}^{2}d,$ if $\\pmb{\\mu}_{i}$ satisfies Assumption 2.v. for all $i\\in[k]$ , we have $\\begin{array}{r}{\\|\\hat{\\mathbf{n}}_{t}-\\mathbf{n}_{t}\\|\\leq\\frac{t}{\\exp\\left(\\Omega\\left(d\\right)\\right)}\\sqrt{d}.}\\end{array}$ ", "page_idx": 25}, {"type": "text", "text": "661 Proof of Lemma $^{12}$ . By equation 18 we have that for all $j\\in[t]$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\hat{\\mathbf{n}}_{j}-\\mathbf{n}_{j}\\|=\\left\\|\\hat{\\mathbf{n}}_{j-1}-{\\mathbf{n}}_{j-1}-\\frac{\\delta_{j}}{2\\nu_{0}^{2}}\\hat{\\mathbf{n}}_{j-1}-\\frac{\\delta_{j}}{2}\\mathbf{N}^{T}\\nabla_{\\mathbf{x}}\\log P(\\mathbf{x}_{j-1})\\right\\|}\\\\ &{\\qquad\\qquad=\\left\\|\\hat{\\mathbf{n}}_{j-1}-{\\mathbf{n}}_{j-1}-\\displaystyle\\sum_{i\\in[k]}\\frac{w_{i}P^{(i)}(\\mathbf{x}_{j-1})}{\\nu_{0}^{2}P(\\mathbf{x}_{j-1})}\\mathbf{n}_{j-1}-\\displaystyle\\sum_{i\\in[k]}\\frac{w_{i}\\mathbf{N}^{T}\\nabla_{\\mathbf{x}}P^{(i)}(\\mathbf{x}_{j-1})}{P(\\mathbf{x}_{j-1})}\\right\\|}\\\\ &{~~\\leq\\|\\hat{\\mathbf{n}}_{j-1}-\\mathbf{n}_{j-1}\\|+\\displaystyle\\sum_{i\\in[k]}\\frac{w_{i}P^{(i)}(\\mathbf{x}_{j-1})}{\\nu_{0}^{2}P(\\mathbf{x}_{j-1})}\\left\\|\\mathbf{n}_{j-1}\\right\\|+\\displaystyle\\sum_{i\\in[k]}\\frac{w_{i}\\left\\|\\nabla_{\\mathbf{x}}P^{(i)}(\\mathbf{x}_{j-1})\\right\\|}{P(\\mathbf{x}_{j-1})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "662 By Lemma 10, we have )) \u2264 exp(\u2212\u2126(d)) and \u2225\u2207xPP  ((ix)(xj)\u22121)\u2225 $\\begin{array}{r l}{\\frac{\\left\\|\\nabla_{\\mathbf{x}}P^{(i)}(\\mathbf{x}_{j-1})\\right\\|}{P(\\mathbf{x}_{j-1})}\\,\\le\\,\\exp(-\\Omega(d))}&{{}}\\end{array}$ for all P (0)(xj\u22121\u221a   \n663 $i\\in[k]$ , hence from $\\|\\mathbf{n}_{j-1}\\|\\leq6\\nu_{0}\\sqrt{d}$ we obtain a recursive bound ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\|\\hat{\\mathbf{n}}_{j}-\\mathbf{n}_{j}\\|\\leq\\|\\hat{\\mathbf{n}}_{j-1}-\\mathbf{n}_{j-1}\\|+\\frac{1}{\\exp(\\Omega(d))}\\sqrt{d}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "664 Finally, by $\\hat{\\mathbf{n}}_{0}=\\mathbf{n}_{0}$ , we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\|\\hat{\\mathbf{n}}_{t}-\\mathbf{n}_{t}\\|=\\sum_{j\\in[t]}\\left(\\|\\hat{\\mathbf{n}}_{j}-\\mathbf{n}_{j}\\|-\\|\\hat{\\mathbf{n}}_{j-1}-\\mathbf{n}_{j-1}\\|\\right)\\leq\\frac{t}{\\exp(\\Omega(d))}\\sqrt{d}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "665 Hence we obtain Lemma 12. ", "page_idx": 25}, {"type": "text", "text": "666 Armed with the above lemmas, we are now ready to establish Proposition 5 by induction. Please   \n667 note that we also apply some lemmas from the proof of Theorem 1 by substituting $\\nu_{\\mathrm{max}}^{2}$ with $\\frac{\\nu_{\\operatorname*{max}}^{2}}{1\\!-\\!c_{\\nu}}$   \n668 Suppose the theorem holds for all $T$ values of $1,\\cdot\\cdot\\cdot,T-1$ . We consider the following 3 cases: ", "page_idx": 25}, {"type": "text", "text": "\u2022 If there exists some $t\\in[T]$ such that $\\delta_{t}>\\nu_{0}^{2}$ , by Lemma 3 we know that with probability at least $1-\\exp(-\\Omega(d))$ , we have $\\begin{array}{r}{\\left\\|\\mathbf{n}_{t}\\right\\|^{2}\\geq\\left(\\frac{3\\nu_{0}^{2}}{4}+\\frac{\\nu_{\\operatorname*{max}}^{2}}{4\\left(1-c_{\\nu}\\right)}\\right)d}\\end{array}$ , thus the problem reduces to the two sub-arrays $\\mathbf{n}_{0},\\cdots,\\mathbf{n}_{t-1}$ and $\\mathbf{n}_{t},\\cdot\\cdot\\cdot,\\mathbf{n}_{T}$ , which can be solved by induction. ", "page_idx": 25}, {"type": "text", "text": "669   \n670   \n671   \n672   \n673   \n674   \n675   \n676   \n677 ", "page_idx": 25}, {"type": "text", "text": "\u2022 Suppose $\\delta_{t}\\leq\\nu_{0}^{2}$ for all $t\\in[T]$ . If there exists some $t\\in[T]$ such that $\\left\\|\\mathbf{n}_{t-1}\\right\\|^{2}>36\\nu_{0}^{2}d.$ , by Lemma 11 we know that with probability at least $1-\\exp(-\\Omega(d))$ , we have $\\left\\|\\mathbf{n}_{t}\\right\\|^{2}\\geq\\nu_{0}^{2}d>$ $\\begin{array}{r}{\\left(\\frac{3\\nu_{0}^{2}}{4}+\\frac{\\nu_{\\mathrm{max}}^{2}}{4(1-c_{\\nu})}\\right)d.}\\end{array}$ , thus the problem similarly reduces to the two sub-arrays $\\mathbf{n}_{0},\\cdots,\\mathbf{n}_{t-1}$ and $\\mathbf{n}_{t},\\cdot\\cdot\\cdot,\\mathbf{n}_{T}$ , which can be solved by induction. ", "page_idx": 25}, {"type": "text", "text": "\u2022 Suppose $\\delta_{t}\\,\\leq\\,\\nu_{0}^{2}$ and $\\left\\|\\mathbf{n}_{t-1}\\right\\|^{2}\\,\\leq\\,36\\nu_{0}^{2}d$ for all $t\\,\\in\\,[T]$ . Conditioned on $\\left\\|\\mathbf{n}_{t-1}\\right\\|^{2}>$ $\\begin{array}{r}{\\left(\\frac{\\nu_{0}^{2}}{2}+\\frac{\\nu_{\\mathrm{max}}^{2}}{2\\left(1-c_{\\nu}\\right)}\\right)d}\\end{array}$ for all $t\\in[T]$ , by Lemma 12 we have that for $T=\\exp(\\mathcal{O}(d))$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\|\\hat{{\\bf n}}_{T}-{\\bf n}_{T}\\|<\\left(\\sqrt{\\frac{5\\nu_{0}^{2}}{8}+\\frac{3\\nu_{\\mathrm{max}}^{2}}{8(1-c_{\\nu})}}-\\sqrt{\\frac{\\nu_{0}^{2}}{2}+\\frac{\\nu_{\\mathrm{max}}^{2}}{2(1-c_{\\nu})}}\\right)\\sqrt{d}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "678 ", "page_idx": 25}, {"type": "text", "text": "By Lemma 8 we have that with probability at least $1-\\exp(-\\Omega(d))$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left\\|\\hat{\\mathbf{n}}_{T}\\right\\|^{2}\\geq\\left(\\frac{5\\nu_{0}^{2}}{8}+\\frac{3\\nu_{\\operatorname*{max}}^{2}}{8(1-c_{\\nu})}\\right)d.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "equation", "text": "$$\n\\|\\mathbf{n}_{T}\\|\\geq\\|\\hat{\\mathbf{n}}_{T}\\|-\\|\\hat{\\mathbf{n}}_{T}-\\mathbf{n}_{T}\\|>\\sqrt{\\left(\\frac{\\nu_{0}^{2}}{2}+\\frac{\\nu_{\\operatorname*{max}}^{2}}{2(1-c_{\\nu})}\\right)}\\,d.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "680 ", "page_idx": 26}, {"type": "text", "text": "681 ", "page_idx": 26}, {"type": "text", "text": "Hence by induction we obtain $\\begin{array}{r}{\\left\\|\\mathbf{n}_{t}\\right\\|^{2}>\\left(\\frac{\\nu_{0}^{2}}{2}+\\frac{\\nu_{\\operatorname*{max}}^{2}}{2\\left(1-c_{\\nu}\\right)}\\right)d}\\end{array}$ for all $t\\in[T]$ with probability at least ", "page_idx": 26}, {"type": "equation", "text": "$$\n(1-(T-1)\\exp(-\\Omega(d)))\\cdot(1-\\exp(-\\Omega(d)))\\geq1-T\\exp(-\\Omega(d)).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "682 Therefore we complete the proof of Proposition 5. ", "page_idx": 26}, {"type": "text", "text": "683 Finally, combining equation 12 and Proposition 5 finishes the proof of Theorem 3. ", "page_idx": 26}, {"type": "text", "text": "684 A.4 Proof of Theorem 4: Annealed Langevin Dynamics under Sub-Gaussian Mixtures ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "685 Assumption 3. Consider a data distribution $\\begin{array}{r}{P:=\\sum_{i=0}^{k}w_{i}P^{(i)}}\\end{array}$ as a mixture of sub-Gaussian   \n686 distributions, where $1\\,\\leq\\,k\\,=\\,o(d)$ and $w_{i}~>~0$ is a positive constant such that $\\textstyle\\sum_{i=0}^{k}w_{i}\\,=\\,1$ .   \n687 Suppose that $P^{(0)}=\\mathcal{N}(\\pmb{\\mu}_{0},\\nu_{0}^{2}I_{d})$ is Gaussian and for all $i\\in[k],\\,P^{(i)}$ satisfies ", "page_idx": 26}, {"type": "text", "text": "i. $P^{(i)}$ is a sub-Gaussian distribution of mean $\\pmb{\\mu}_{i}$ with parameter $\\nu_{i}^{2}$ , ", "page_idx": 26}, {"type": "text", "text": "iii. for all t \u2208{0} \u222a[T], the score function of P \u03c3(it) is Li,t-Lipschitz such that Li,t \u2264 \u03bdi2 c+L\u03c3t2 for some constant $c_{L}>0$ , ", "page_idx": 26}, {"type": "text", "text": "iv. $\\begin{array}{r}{\\nu_{0}^{2}>\\operatorname*{max}\\left\\{1,\\frac{4(c_{L}^{2}+c_{\\nu}c_{L})}{c_{\\nu}(1-c_{\\nu})}\\right\\}\\frac{\\nu_{\\operatorname*{max}}^{2}+c_{\\sigma}^{2}}{1-c_{\\nu}}-c_{\\sigma}^{2}}\\end{array}$ , 4(ccL(1+\u2212c\u03bdcc)L) \u03bdm1a\u2212xc+c\u03c3\u2212c2\u03c3 for constant c\u03bd \u2208(0, 1), where \u03bdmax := maxi\u2208[k] \u03bdi, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nu.\\ \\left\\Vert\\mu_{i}-\\mu_{0}\\right\\Vert^{2}\\leq\\frac{(1-c_{\\nu})\\nu_{0}^{2}-\\nu_{i}^{2}-c_{\\nu}c_{\\sigma}^{2}}{2(1-c_{\\nu})}\\left(\\log\\frac{c_{\\nu}\\left(\\nu_{i}^{2}+c_{\\sigma}^{2}\\right)}{(c_{L}^{2}+c_{\\nu}c_{L})(\\nu_{0}^{2}+c_{\\sigma}^{2})}-\\frac{(\\nu_{i}^{2}+c_{\\sigma}^{2})}{2(1-c_{\\nu})(\\nu_{0}^{2}+c_{\\sigma}^{2})}+\\frac{(1-c_{\\nu})(\\nu_{0}^{2}+c_{\\sigma}^{2})}{2(\\nu_{i}^{2}+c_{\\sigma}^{2})}\\right)d_{\\nu}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "695 The feasibility of Assumption 3.v. can be validated by substituting $\\nu^{2}$ in Lemma 9 with $\\nu^{2}+c_{\\sigma}^{2}$ .   \n696 To establish Theorem 4, we first note from Proposition 1 that for a sub-Gaussian mixture ${\\cal P}\\,=$   \n697 $\\textstyle\\sum_{i=0}^{k}w_{i}P^{(i)}$ , the perturbed distribution of noise level $\\sigma$ is $\\begin{array}{r}{P_{\\sigma}\\,=\\,\\sum_{i=0}^{k}w_{i}P_{\\sigma}^{(i)}}\\end{array}$ , where ${\\cal P}^{(0)}\\,=$   \n698 ${\\mathcal{N}}(\\mu_{0},(\\nu_{i}^{2}{+}\\sigma^{2})I_{d})$ and $P^{(i)}$ is a sub-Gaussian distribution with mean $\\pmb{\\mu}_{i}$ and sub-Gaussian parameter   \n699 $(\\nu_{i}^{2}+\\sigma^{2})$ . Similar to the proof of Theorem 1, we decompose ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{x}_{t}=\\mathbf{R}\\mathbf{r}_{t}+\\mathbf{N}\\mathbf{n}_{t},\\,\\mathrm{and}\\,\\epsilon_{t}=\\mathbf{R}\\epsilon_{t}^{(\\mathbf{r})}+\\mathbf{N}\\epsilon_{t}^{(\\mathbf{n})},}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "700 where $\\mathbf{R}\\in\\mathbb{R}^{d\\times r}$ an orthonormal basis of the vector space $\\{\\pmb{\\mu}_{i}\\}_{i\\in[k]}$ and $\\mathbf{N}\\in\\mathbb{R}^{d\\times n}$ an orthonormal   \n701 basis of the null space of $\\{\\pmb{\\mu}_{i}\\}_{i\\in[k]}$ . Now, we prove Theorem 4 by applying the techniques developed   \n702 in Appendix A.1 and A.3 via substituting $\\nu^{2}$ and $\\frac{\\nu^{2}}{1\\!-\\!c_{\\nu}}$ with $\\frac{\\nu^{2}\\!+\\!\\sigma_{t}^{2}}{1\\!-\\!c_{\\nu}}$ at time step $t$ . Note that for all   \n703 $t\\in\\{0\\}\\cup[T]$ , Assumption 3.iv. implies $\\begin{array}{r}{\\nu_{0}^{2}+\\sigma_{t}^{2}>\\operatorname*{max}\\left\\{1,\\frac{4(c_{L}^{2}+c_{\\nu}c_{L})}{c_{\\nu}(1-c_{\\nu})}\\right\\}\\frac{\\nu_{\\mathrm{max}}^{2}+\\sigma_{t}^{2}}{1-c_{\\nu}}}\\end{array}$ t because c\u03c3 \u2265\u03c3t.   \n704 First, by Proposition 2, suppose that the sample is initialized in the distribution $P_{\\sigma_{0}}^{(0)}$ , then with   \n705 probability at least $1-\\exp(-\\Omega(d))$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left\\|\\mathbf{n}_{0}\\right\\|^{2}\\geq\\left(\\frac{3(\\nu_{0}^{2}+\\sigma_{0}^{2})}{4}+\\frac{\\nu_{\\operatorname*{max}}^{2}+\\sigma_{0}^{2}}{4(1-c_{\\nu})}\\right)d.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "706 Then, with the assumption that the initialization satisfies \u2225n0\u22252 \u2265 3(\u03bd024+\u03c302)+ $\\begin{array}{r}{\\left\\|\\mathbf{n}_{0}\\right\\|^{2}\\,\\geq\\,\\left(\\frac{3(\\nu_{0}^{2}+\\sigma_{0}^{2})}{4}+\\frac{\\nu_{\\operatorname*{max}}^{2}+\\sigma_{0}^{2}}{4(1-c_{\\nu})}\\right)d,}\\end{array}$ , the   \n707 following proposition similar to Proposition 5 shows that $\\|\\mathbf{n}_{t}\\|$ remains large with high probability.   \n708 Proposition 6. Consider a distribution $P$ satisfying Assumption 3. We follow annealed Langevin   \n709 dynamics for $T\\,=\\,\\exp(O(d))$ steps with noise level $c_{\\sigma}\\,\\geq\\,\\sigma_{0}\\,\\geq\\,\\sigma_{1}\\,\\geq\\,\\cdots\\,\\geq\\,\\sigma_{T}\\,\\geq\\,0$ for some   \n710 constant $c_{\\sigma}>0$ . Suppose that the initial sample satisfies $\\begin{array}{r}{\\left\\|\\mathbf{n}_{0}\\right\\|^{2}\\geq\\left(\\frac{3(\\nu_{0}^{2}+\\sigma_{0}^{2})}{4}+\\frac{\\nu_{\\operatorname*{max}}^{2}+\\sigma_{0}^{2}}{4(1-c_{\\nu})}\\right)d,}\\end{array}$ , then   \n711 with probability at least 1 \u2212T \u00b7 exp(\u2212\u2126(d)), we have that \u2225nt\u22252 > \u03bd02+2\u03c3t2 $\\begin{array}{r}{\\|\\mathbf{n}_{t}\\|^{2}>\\left(\\frac{\\nu_{0}^{2}+\\sigma_{t}^{2}}{2}+\\frac{\\nu_{\\operatorname*{max}}^{2}+\\sigma_{t}^{2}}{2(1-c_{\\nu})}\\right)d}\\end{array}$ for all   \n712 $t\\in\\{0\\}\\cup[T]$ .   \n713 Proof of Proposition $6$ . We prove Proposition 6 by induction. Suppose the theorem holds for all $T$   \n714 values of $1,\\cdot\\cdot\\cdot,T-1$ . We consider the following 3 cases: ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "\u2022 If there exists some $t\\,\\in\\,[T]$ such that $\\delta_{t}\\,>\\,\\nu_{0}^{2}\\,+\\,\\sigma_{t}^{2}$ , by Lemma 3 we know that with probability at least $1-\\exp(-\\Omega(d))$ , we have $\\begin{array}{r}{\\left\\|\\mathbf{n}_{t}\\right\\|^{2}\\geq\\left(\\frac{3(\\nu_{0}^{2}+\\sigma_{t}^{2})}{4}+\\frac{\\nu_{\\operatorname*{max}}^{2}+\\sigma_{t}^{2}}{4(1-c_{\\nu})}\\right)d}\\end{array}$ , thus the problem reduces to the two sub-arrays $\\mathbf{n}_{0},\\cdots,\\mathbf{n}_{t-1}$ and $\\mathbf{n}_{t},\\cdot\\cdot\\cdot,\\mathbf{n}_{T}$ , which can be solved by induction. ", "page_idx": 27}, {"type": "text", "text": "715   \n716   \n717   \n718   \n719   \n720   \n721   \n722   \n723   \n724   \n725   \n726 ", "page_idx": 27}, {"type": "text", "text": "\u2022 Suppose $\\delta_{t}\\,\\leq\\nu_{0}^{2}+\\sigma_{t}^{2}$ for all $t\\in[T]$ . If there exists some $t\\in[T]$ such that $\\left\\|\\mathbf{n}_{t-1}\\right\\|^{2}>$ $36\\bar{(\\nu_{0}^{2}+\\sigma_{t-1}^{2})}\\bar{d}\\geq3\\bar{6}(\\nu_{0}^{2}+\\sigma_{t}^{2})d$ , by Lemma 11 we know that with probability at least $1\\mathrm{~-~}\\exp(-\\Omega(d))$ , we have $\\begin{array}{r}{\\|\\mathbf{n}_{t}\\|^{2}\\;\\geq\\;(\\nu_{0}^{2}+\\sigma_{t}^{2})d\\;>\\;\\left(\\frac{3(\\nu_{0}^{2}+\\sigma_{t}^{2})}{4}+\\frac{\\nu_{\\operatorname*{max}}^{2}+\\sigma_{t}^{2}}{4(1-c_{\\nu})}\\right)d}\\end{array}$ , thus the problem similarly reduces to the two sub-arrays $\\mathbf{n}_{0},\\cdots,\\mathbf{n}_{t-1}$ and $\\mathbf{n}_{t},\\cdot\\cdot\\cdot,\\mathbf{n}_{T}$ , which can be solved by induction. ", "page_idx": 27}, {"type": "text", "text": "\u2022 Suppose $\\delta_{t}\\leq\\nu_{0}^{2}+\\sigma_{t}^{2}$ and $\\left\\|\\mathbf{n}_{t-1}\\right\\|^{2}\\leq36(\\nu_{0}^{2}+\\sigma_{t-1}^{2})d$ for all $t\\in[T]$ . Consider a surrogate sequence $\\hat{{\\mathbf{n}}}_{t}$ such that $\\hat{\\mathbf{n}}_{0}=\\mathbf{n}_{0}$ and for all $t\\geq1$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{n}}_{t}=\\hat{\\mathbf{n}}_{t-1}-\\frac{\\delta_{t}}{2\\nu_{0}^{2}+2\\sigma_{t}^{2}}\\hat{\\mathbf{n}}_{t-1}+\\sqrt{\\delta_{t}}\\epsilon_{t}^{(\\mathbf{n})}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "727 ", "page_idx": 27}, {"type": "text", "text": "Since $\\nu_{0}>\\nu_{i}$ and $c_{\\sigma}\\geq\\sigma_{t}$ for all $t\\in\\left\\{0\\right\\}\\cup\\left[T\\right]$ , we have $\\begin{array}{r}{\\frac{\\nu_{i}^{2}+c_{\\sigma}^{2}}{\\nu_{0}^{2}+c_{\\sigma}^{2}}>\\frac{\\nu_{i}^{2}+\\sigma_{t}^{2}}{\\nu_{0}^{2}+\\sigma_{t}^{2}}}\\end{array}$ . Notice that for function $\\begin{array}{r}{f(z)=\\log z-\\frac{z}{2}+\\frac{1}{2z}}\\end{array}$ , we have $\\begin{array}{r}{\\frac{\\mathrm{d}}{\\mathrm{d}z}f(z)=\\frac{1}{z}-\\frac{1}{2}-\\frac{1}{2z^{2}}=-\\frac{1}{2}\\left(\\frac{1}{z}-1\\right)^{2}\\leq0}\\end{array}$ Thus, by Assumption 3.v. we have that for all $t\\in[T]$ , ", "page_idx": 27}, {"type": "text", "text": "728 ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\|\\mu_{i}-\\mu_{0}\\|^{2}\\leq\\frac{(1-c_{\\nu})\\nu_{0}^{2}-\\nu_{i}^{2}-c_{\\nu}c_{\\sigma}^{2}}{2(1-c_{\\nu})}\\left(\\log\\frac{c_{\\nu}(\\nu_{i}^{2}+c_{\\sigma}^{2})}{(c_{L}^{2}+c_{\\nu}c_{L})(\\nu_{0}^{2}+c_{\\sigma}^{2})}\\right.}}\\\\ &{}&{\\left.-\\ \\frac{(\\nu_{i}^{2}+c_{\\sigma}^{2})}{2(1-c_{\\nu})(\\nu_{0}^{2}+c_{\\sigma}^{2})}+\\frac{(1-c_{\\nu})(\\nu_{0}^{2}+c_{\\sigma}^{2})}{2(\\nu_{i}^{2}+c_{\\sigma}^{2})}\\right)d}\\\\ &{}&{\\leq\\frac{(1-c_{\\nu})\\nu_{0}^{2}-\\nu_{i}^{2}-c_{\\nu}\\sigma_{t}^{2}}{2(1-c_{\\nu})}\\left(\\log\\frac{c_{\\nu}(\\nu_{i}^{2}+\\sigma_{t}^{2})}{(c_{L}^{2}+c_{\\nu}c_{L})(\\nu_{0}^{2}+\\sigma_{t}^{2})}\\right.}\\\\ &{}&{\\qquad\\qquad\\qquad\\qquad\\qquad\\left.-\\ \\frac{(\\nu_{i}^{2}+\\sigma_{t}^{2})}{2(1-c_{\\nu})(\\nu_{0}^{2}+\\sigma_{t}^{2})}+\\frac{(1-c_{\\nu})(\\nu_{0}^{2}+\\sigma_{t}^{2})}{2(\\nu_{i}^{2}+\\sigma_{t}^{2})}\\right)d}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "729 ", "page_idx": 27}, {"type": "text", "text": "730 ", "page_idx": 27}, {"type": "text", "text": "Conditioned on $\\begin{array}{r}{\\|\\mathbf{n}_{t-1}\\|^{2}>\\left(\\frac{\\nu_{0}^{2}+\\sigma_{t-1}^{2}}{2}+\\frac{\\nu_{\\operatorname*{max}}^{2}+\\sigma_{t-1}^{2}}{2(1-c_{\\nu})}\\right)d}\\end{array}$ \u03bdm2a(x1\u2212+c\u03c3t)\u22121 d for all t \u2208[T], by Lemma 12 we have that for $T=\\exp(\\mathcal{O}(d))$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\|\\hat{\\mathbf{n}}_{T}-\\mathbf{n}_{T}\\|<\\left(\\sqrt{\\frac{5(\\nu_{0}^{2}+\\sigma_{T}^{2})}{8}+\\frac{3(\\nu_{\\operatorname*{max}}^{2}+\\sigma_{T}^{2})}{8(1-c_{\\nu})}}-\\sqrt{\\frac{\\nu_{0}^{2}+\\sigma_{T}^{2}}{2}+\\frac{\\nu_{\\operatorname*{max}}^{2}+\\sigma_{T}^{2}}{2(1-c_{\\nu})}}\\right)\\sqrt{d}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "731 ", "page_idx": 27}, {"type": "text", "text": "By Lemma 8 we have that with probability at least $1-\\exp(-\\Omega(d))$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\|\\hat{\\mathbf{n}}_{T}\\|^{2}\\geq\\left(\\frac{5(\\nu_{0}^{2}+\\sigma_{T}^{2})}{8}+\\frac{3(\\nu_{\\operatorname*{max}}^{2}+\\sigma_{T}^{2})}{8(1-c_{\\nu})}\\right)d.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "732 ", "page_idx": 27}, {"type": "text", "text": "Combining the two inequalities implies the desired bound ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\|\\mathbf{n}_{T}\\|\\geq\\|\\hat{\\mathbf{n}}_{T}\\|-\\|\\hat{\\mathbf{n}}_{T}-\\mathbf{n}_{T}\\|>\\sqrt{\\left(\\frac{\\nu_{0}^{2}+\\sigma_{T}^{2}}{2}+\\frac{\\nu_{\\operatorname*{max}}^{2}+\\sigma_{T}^{2}}{2(1-c_{\\nu})}\\right)}\\,d.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "733 ", "page_idx": 27}, {"type": "text", "text": "734 ", "page_idx": 27}, {"type": "text", "text": "Hence by induction we obtain $\\begin{array}{r}{\\left\\|\\mathbf{n}_{t}\\right\\|^{2}>\\left(\\frac{\\nu_{0}^{2}+\\sigma_{T}^{2}}{2}+\\frac{\\nu_{\\operatorname*{max}}^{2}+\\sigma_{T}^{2}}{2\\left(1-c_{\\nu}\\right)}\\right)d}\\end{array}$ for all $t\\in[T]$ with probability at least ", "page_idx": 27}, {"type": "equation", "text": "$$\n(1-(T-1)\\exp(-\\Omega(d)))\\cdot(1-\\exp(-\\Omega(d)))\\geq1-T\\exp(-\\Omega(d)).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "735 Therefore we complete the proof of Proposition 6. ", "page_idx": 27}, {"type": "text", "text": "736 Finally, combining equation 20 and Proposition 6 finishes the proof of Theorem 4. ", "page_idx": 27}, {"type": "text", "text": "737 A.5 Proof of Theorem 5: Convergence Analysis of Chained Langevin Dynamics ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "738 For simplicity, denote $\\mathbf{x}^{[q]}=\\left\\{\\mathbf{x}^{(1)},\\cdots,\\mathbf{x}^{(q)}\\right\\}$ . By the definition of total variation distance, for all   \n739 $q\\in[d/Q]$ we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname{rv}\\left(\\hat{P}\\left(x^{[i]}\\right),P\\left(x^{[i]}\\right)\\right)}\\\\ &{=\\frac{1}{2}\\int\\left|\\hat{P}\\left(x^{[i]}\\right)-P\\left(x^{[i]}\\right)\\right|\\,\\mathrm{d}x^{[i]}}\\\\ &{=\\frac{1}{2}\\int\\left|\\hat{P}\\left(x^{[i]}\\right)\\left[x^{[i-1]}\\right)\\hat{P}\\left(x^{[i-1]}\\right)-P\\left(x^{[i]}\\mid x^{[i-1]}\\right)P\\left(x^{[i-1]}\\right)\\right|\\,\\mathrm{d}x^{[i]}}\\\\ &{\\leq\\frac{1}{2}\\int\\left|\\hat{P}\\left(x^{[i]}\\left[x^{[i-1]}\\right]\\hat{P}\\left(x^{[i-1]}\\right)-\\hat{P}\\left(x^{[i]}\\mid x^{[i-1]}\\right)P\\left(x^{[i-1]}\\right)\\right|\\,\\mathrm{d}x^{[i]}}\\\\ &{\\qquad+\\frac{1}{2}\\int\\left|\\hat{P}\\left(x^{[i]}\\left[x^{[i-1]}\\right]\\right)P\\left(x^{[i-1]}\\right)-P\\left(x^{[i]}\\mid x^{[i-1]}\\right)P\\left(x^{[i-1]}\\right)\\right|\\,\\mathrm{d}x^{[i]}}\\\\ &{=\\frac{1}{2}\\int\\hat{P}\\left(x^{[i]}\\left[x^{[i-1]}\\right]\\,\\mathrm{d}x^{[i]}\\right)\\int\\hat{P}\\left(x^{[i-1]}\\right)-P\\left(x^{[i-1]}\\right)\\left[\\mathrm{d}x^{[i-1]}\\right]\\,\\mathrm{d}x^{[i]}}\\\\ &{\\qquad+\\frac{1}{2}\\int\\left|\\hat{P}\\left(x^{[i]}\\left[x^{[i-1]}\\right]\\right)-P\\left(x^{[i]}\\mid x^{[i-1]}\\right)\\right|\\,\\mathrm{d}x^{[i]}\\int P\\left(x^{[i-1]}\\right)\\,\\mathrm{d}x^{[i-1]}}\\\\ &{=\\operatorname{rv}\\left(\\hat{P}\\left(x^{[i-1]}\\right),P\\left(x^{[i-1]}\\right)\\right)+\\operatorname{rv}\\left(\\hat{P}\\left(x^{[i]}\\mid x^{[i-1]}\\right)\\right)\\int\\hat{P}\\left(x^{[i-1]}\\right)\\,\\mathrm{d}x^{[i-1]}}\\\\ &{\\leq\\operatorname{rv}\\left(\\hat{P}\\left(x^{[i-1]}\\right),P\\left(x^\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "740 Upon summing up the above inequality for all $q\\in[d/Q]$ , we obtain ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{TV}\\left(\\hat{P}(\\mathbf{x}),P(\\mathbf{x})\\right)=\\displaystyle\\sum_{q=1}^{d/Q}\\left(\\mathrm{TV}\\left(\\hat{P}\\left(\\mathbf{x}^{[q]}\\right),P\\left(\\mathbf{x}^{[q]}\\right)\\right)-\\mathrm{TV}\\left(\\hat{P}\\left(\\mathbf{x}^{[q-1]}\\right),P\\left(\\mathbf{x}^{[q-1]}\\right)\\right)\\right)}\\\\ &{\\quad\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\leq\\displaystyle\\sum_{q=1}^{d/Q}\\varepsilon\\cdot\\frac{Q}{d}=\\varepsilon}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "741 Thus we finish the proof of Theorem 5. ", "page_idx": 28}, {"type": "text", "text": "742 B Additional Experiments ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "743 Algorithm Setup: Our choices of algorithm hyperparameters are based on Song and Ermon (2019).   \n744 We consider $L=10$ different standard deviations such that $\\{\\lambda_{i}\\}_{i\\in[L]}$ is a geometric sequence with   \n745 $\\lambda_{1}=1$ and $\\lambda_{10}=0.01$ . For annealed Langevin dynamics with $T$ iterations, we choose the noise   \n746 levels $\\{\\sigma_{t}\\}_{t\\in[T]}$ by repeating every element of $\\{\\lambda_{i}\\}_{i\\in[L]}$ for $T/L$ times and we set the step size as   \n747 $\\delta_{t}=2\\times10^{-5}\\cdot\\sigma_{t}^{2}/\\sigma_{T}^{2}$ for every $t\\in[T]$ . For vanilla Langevin dynamics with $T$ iterations, we use the   \n748 same step size as annealed Langevin dynamics. For chained Langevin dynamics with $T$ iterations, the   \n749 patch size $Q$ is chosen depending on different tasks. For every patch of chained Langevin dynamics,   \n750 we choose the noise levels $\\left\\{\\sigma_{t}\\right\\}_{t\\in[T Q/d]}$ by repeating every element of $\\{\\lambda_{i}\\}_{i\\in[L]}$ for $T Q/\\bar{d L}$ times   \n751 and we set the step size as $\\delta_{t}=2\\stackrel{.}{\\times}10^{-5}\\cdot\\sigma_{t}^{2}/\\sigma_{T Q/d}^{2}$ for every $t\\in[T Q/d]$ . ", "page_idx": 28}, {"type": "text", "text": "752 B.1 Synthetic Gaussian Mixture Model ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "753 We choose the data distribution $P$ as a mixture of three Gaussian components in dimension $d=100$ : ", "page_idx": 28}, {"type": "equation", "text": "$$\nP=0.2P^{(0)}+0.4P^{(1)}+0.4P^{(2)}=0.2\\mathcal{N}(\\mathbf{0}_{d},3{I}_{d})+0.4\\mathcal{N}(\\mathbf{1}_{d},{I}_{d})+0.4\\mathcal{N}(-\\mathbf{1}_{d},{I}_{d}).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "754 Since the distribution is given, we assume that the sampling algorithms have access to the ground-truth   \n755 score function. We set the batch size as 1000 and patch size $Q=10$ for chained Langevin dynamics.   \n756 We use $T\\in\\{10^{3},10^{4},10^{5},10^{6}\\}$ iterations for vanilla, annealed, and chained Langevin dynamics.   \n757 The initial samples are i.i.d. chosen from $P^{(0)}$ , $P^{(1)}$ , or $P^{(2)}$ , and the results are presented in Figures   \n758 1, 4, and 5 respectively. The two subfigures above the dashed line illustrate the samples from the   \n759 initial distribution and target distribution, and the subfigures below the dashed line are the samples   \n760 generated by different algorithms. A sample $\\mathbf{x}$ is clustered in mode 1 if it satisfies $\\|\\mathbf{x}-\\mu_{1}\\|^{2}\\overset{}{\\leq}5d$   \n761 and $\\left\\|\\mathbf{x}-{\\pmb{\\mu}}_{1}\\right\\|^{2}\\leq\\left\\|\\mathbf{x}-{\\pmb{\\mu}}_{2}\\right\\|^{2}$ ; in mode 2 if $\\left\\|\\mathbf{x}-\\mu_{2}\\right\\|^{2}\\leq5d$ and $\\left\\|\\mathbf{x}-\\pmb{\\mu}_{1}\\right\\|^{2}>\\left\\|\\mathbf{\\vec{x}}-\\pmb{\\mu}_{2}\\right\\|^{2}$ ; and in   \n762 mode 0 otherwise. The experiments were run on an Intel Xeon CPU with 2.90GHz. ", "page_idx": 28}, {"type": "image", "img_path": "RNeb41ybNL/tmp/ed26823452c2026507b605d521d2a99a26ec722655d4c909ef3fc295921b2edd.jpg", "img_caption": ["Figure 4: Samples from a mixture of three Gaussian modes generated by vanilla, annealed, and chained Langevin dynamics. Three axes are $\\ell_{2}$ distance from samples to the mean of the three modes. The samples are initialized in mode 1. "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "763 B.2 Image Datasets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "764 Our implementation and hyperparameter selection are based on Song and Ermon (2019). During   \n765 training, we i.i.d. randomly filp an image with probability 0.5 to construct the two modes (i.e., original   \n766 and filpped images). All models are optimized by Adam with learning rate 0.001 and batch size 128   \n767 for a total of 200000 training steps, and we use the model at the last iteration to generate the samples.   \n768 We perform experiments on MNIST (LeCun, 1998) (CC BY-SA 3.0 License) and Fashion-MNIST   \n769 (Xiao et al., 2017) (MIT License) datasets and we set the patch size as $Q=14$ .   \n770 For the score networks of vanilla and annealed Langevin dynamics, following from Song and Ermon   \n771 (2019), we use the 4-cascaded RefineNet (Lin et al., 2017), a modern variant of U-Net (Ronneberger   \n772 et al., 2015) with residual design. For the score networks of chained Langevin dynamics, we use the   \n773 official PyTorch implementation of an LSTM network (Sak et al., 2014) followed by a linear layer.   \n774 For MNIST and Fashion-MNIST datasets, we set the input size of the LSTM as $Q=14$ , the number   \n775 of features in the hidden state as 1024, and the number of recurrent layers as 2. The inputs of LSTM   \n776 include inputting tensor, hidden state, and cell state, and the outputs of LSTM include the next hidden   \n777 state and cell state, which can be fed to the next input. To estimate the noisy score function, we first   \n778 input the noise level $\\sigma$ (repeated for $Q$ times to match the input size of LSTM) and all-0 hidden and   \n779 cell states to obtain an initialization of the hidden and cell states. Then, we divide a sample into $d/Q$   \n780 patches and input the sequence of patches to the LSTM. For every output hidden state corresponding   \n781 to one patch, we apply a linear layer of size $1024\\times Q$ to estimate the noisy score function of the   \n782 patch.   \n783 To generate samples, we use $T\\in\\{3000,10000,30000,100000\\}$ iterations for vanilla, annealed, and   \n784 chained Langevin dynamics. The initial samples are chosen as either original or flipped images   \n785 from the dataset, and the results for MNIST and Fashion-MNIST datasets are presented in Figures 2,   \n786 6, 3, and 7 respectively. The two subfigures above the dashed line illustrate the samples from the   \n787 initial distribution and target distribution, and the subfigures below the dashed line are the samples   \n788 generated by different algorithms. High-quality figures generated by annealed and chained Langevin   \n789 dynamics for $T=100000$ iterations are presented in Figures 8 and 9.   \n790 All experiments were run with one RTX3090 GPU. It is worth noting that the training and inference   \n791 time of chained Langevin dynamics using LSTM is considerably faster than vanilla/annealed Langevin   \n792 dynamics using RefineNet. For a course of 200000 training steps on MNIST/Fashion-MNIST, due   \n793 to the different network architectures, LSTM takes around 2.3 hours while RefineNet takes around   \n794 9.2 hours. Concerning image generation, chained Langevin dynamics is significantly faster than   \n795 vanilla/annealed Langevin dynamics since every iteration of chained Langevin dynamics only updates   \n796 a patch of constant size, while every iteration of vanilla/annealed Langevin dynamics requires   \n797 computing all coordinates of the sample. One iteration of chained Langevin dynamics using LSTM   \n798 takes around $1.97~\\mathrm{ms}$ , while one iteration of vanilla/annealed Langevin dynamics using RefineNet   \n799 takes around $43.7\\;\\mathrm{ms}$ . ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 29}, {"type": "image", "img_path": "RNeb41ybNL/tmp/8991a17569767e242217c2bec49d4b8cf38cb057037ab4277d1da05c1a623ace.jpg", "img_caption": ["Figure 5: Samples from a mixture of three Gaussian modes generated by vanilla, annealed, and chained Langevin dynamics. Three axes are $\\ell_{2}$ distance from samples to the mean of the three modes. The samples are initialized in mode 2. "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 30}, {"type": "image", "img_path": "RNeb41ybNL/tmp/e5ecfa992ba064850f072cafcbf0aff7b19752e6597b0c40a26d685d641bcc0e.jpg", "img_caption": [], "img_footnote": ["Figure 6: Samples from a mixture distribution of the original and flipped images from the MNIST dataset generated by vanilla, annealed, and chained Langevin dynamics. The samples are initialized as flipped images from MNIST. "], "page_idx": 31}, {"type": "text", "text": "800 C Boarder Impacts ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "801 This paper presents work whose goal is to advance the field of machine learning. No potential societal   \n802 consequence of this work needs to be highlighted here. ", "page_idx": 31}, {"type": "image", "img_path": "RNeb41ybNL/tmp/f5bda44f7f39fe358f87bfba8dbae7671fac46df3b185f5ca397c1a2bb394908.jpg", "img_caption": [], "img_footnote": ["Figure 7: Samples from a mixture distribution of the original and flipped images from the FashionMNIST dataset generated by vanilla, annealed, and chained Langevin dynamics. The samples are initialized as flipped images from Fashion-MNIST. "], "page_idx": 32}, {"type": "image", "img_path": "RNeb41ybNL/tmp/ef6e3334c4d420e6c554cc8c2423a493bb5b8f15bc970315b59b6a74f25dd3c3.jpg", "img_caption": [], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "Figure 8: Samples from a mixture distribution of the original and flipped images from the MNIST dataset generated by annealed and chained Langevin dynamics for $T\\,=\\,100000$ iterations. The samples are initialized as the original or flipped images from MNIST. ", "page_idx": 33}, {"type": "text", "text": "Annealed Langevin Dynamics ", "page_idx": 34}, {"type": "image", "img_path": "RNeb41ybNL/tmp/a5913d28e26f6dcbf23366de602d1210c461682ab709ceaf2980ba45bf3f28b2.jpg", "img_caption": [], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "Chained Langevin Dynamics ", "page_idx": 34}, {"type": "text", "text": "Figure 9: Samples from a mixture distribution of the original and flipped images from the FashionMNIST dataset generated by annealed and chained Langevin dynamics for $T=100000$ iterations. The samples are initialized as the original or flipped images from Fashion-MNIST. ", "page_idx": 34}, {"type": "text", "text": "803 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "804 1. Claims   \n805 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n806 paper\u2019s contributions and scope?   \n807 Answer: [Yes]   \n808 Justification: We list the paper\u2019s contributions about the mode-seeking tendencies of vanilla,   \n809 annealed, and chained Langevin Dynamics in the abstract and at the end of Section 1. The   \n810 scope of this work is also discussed in Section 1.   \n811 Guidelines:   \n812 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n813 made in the paper.   \n814 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n815 contributions made in the paper and important assumptions and limitations. A No or   \n816 NA answer to this question will not be perceived well by the reviewers.   \n817 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n818 much the results can be expected to generalize to other settings.   \n819 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n820 are not attained by the paper.   \n821 2. Limitations   \n822 Question: Does the paper discuss the limitations of the work performed by the authors?   \n823 Answer: [Yes]   \n824 Justification: The limitations of this work are discussed in Section 7.   \n825 Guidelines:   \n826 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n827 the paper has limitations, but those are not discussed in the paper.   \n828 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n829 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n830 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n831 model well-specification, asymptotic approximations only holding locally). The authors   \n832 should reflect on how these assumptions might be violated in practice and what the   \n833 implications would be.   \n834 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n835 only tested on a few datasets or with a few runs. In general, empirical results often   \n836 depend on implicit assumptions, which should be articulated.   \n837 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n838 For example, a facial recognition algorithm may perform poorly when image resolution   \n839 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n840 used reliably to provide closed captions for online lectures because it fails to handle   \n841 technical jargon.   \n842 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n843 and how they scale with dataset size.   \n844 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n845 address problems of privacy and fairness.   \n846 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n847 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n848 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n849 judgment and recognize that individual actions in favor of transparency play an impor  \n850 tant role in developing norms that preserve the integrity of the community. Reviewers   \n851 will be specifically instructed to not penalize honesty concerning limitations.   \n852 3. Theory Assumptions and Proofs   \n853 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n854 a complete (and correct) proof?   \n56 Justification: The assumptions and proof of every theorem are clearly stated. For Theorem 1,   \n57 the assumptions are listed in Assumption 1 and the proof is in Appendix A.1; for Theorem 2,   \n58 the assumptions are listed in Assumption 1 and the proof is in Appendix A.2; for Theorem 3,   \n59 the assumptions are listed in Assumption 2 and the proof is in Appendix A.3; for Theorem   \n860 4, the assumptions are listed in Assumption 3 and the proof is in Appendix A.4. ", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 36}, {"type": "text", "text": "872 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Justification: We provide all the information about our numerical experiments in Section 6 and Appendix B that enable readers to reproduce our results. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in ", "page_idx": 36}, {"type": "text", "text": "909 some way (e.g., to registered users), but it should be possible for other researchers   \n910 to have some path to reproducing or verifying the results.   \n911 5. Open access to data and code   \n912 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n913 tions to faithfully reproduce the main experimental results, as described in supplemental   \n914 material?   \n915 Answer: [Yes]   \n916 Justification: We submitted our code in supplementary materials for the readers to reproduce   \n917 our numerical results.   \n918 Guidelines:   \n919 \u2022 The answer NA means that paper does not include experiments requiring code.   \n920 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n921 public/guides/CodeSubmissionPolicy) for more details.   \n922 \u2022 While we encourage the release of code and data, we understand that this might not be   \n923 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n924 including code, unless this is central to the contribution (e.g., for a new open-source   \n925 benchmark).   \n926 \u2022 The instructions should contain the exact command and environment needed to run to   \n927 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n928 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n929 \u2022 The authors should provide instructions on data access and preparation, including how   \n930 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n931 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n932 proposed method and baselines. If only a subset of experiments are reproducible, they   \n933 should state which ones are omitted from the script and why.   \n934 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n935 versions (if applicable).   \n936 \u2022 Providing as much information as possible in supplemental material (appended to the   \n937 paper) is recommended, but including URLs to data and code is permitted.   \n938 6. Experimental Setting/Details   \n939 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n940 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n941 results?   \n942 Answer: [Yes]   \n943 Justification: All details of the numerical experiments are listed in Appendix B.   \n944 Guidelines:   \n945 \u2022 The answer NA means that the paper does not include experiments.   \n946 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n947 that is necessary to appreciate the results and make sense of them.   \n948 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n949 material.   \n950 7. Experiment Statistical Significance   \n951 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n952 information about the statistical significance of the experiments?   \n953 Answer: [Yes]   \n954 Justification: For the results of synthetic data, we report 1000 samples for every experiment.   \n955 For the results of image datasets, we report 49 or 100 samples for every experiment.   \n956 Guidelines:   \n957 \u2022 The answer NA means that the paper does not include experiments.   \n958 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n959 dence intervals, or statistical significance tests, at least for the experiments that support   \n960 the main claims of the paper.   \n961 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n962 example, train/test split, initialization, random drawing of some parameter, or overall   \n963 run with given experimental conditions).   \n964 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n965 call to a library function, bootstrap, etc.)   \n966 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n967 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n968 of the mean.   \n969 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n970 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n971 of Normality of errors is not verified.   \n972 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n973 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n974 error rates).   \n975 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n976 they were calculated and reference the corresponding figures or tables in the text.   \n977 8. Experiments Compute Resources   \n978 Question: For each experiment, does the paper provide sufficient information on the com  \n979 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n980 the experiments?   \n981 Answer: [Yes]   \n982 Justification: Information on the computational resources of the numerical experiments is   \n983 provided in Appendix B.   \n984 Guidelines:   \n985 \u2022 The answer NA means that the paper does not include experiments.   \n986 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n987 or cloud provider, including relevant memory and storage.   \n988 \u2022 The paper should provide the amount of compute required for each of the individual   \n989 experimental runs as well as estimate the total compute.   \n990 \u2022 The paper should disclose whether the full research project required more compute   \n991 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n992 didn\u2019t make it into the paper).   \n993 9. Code Of Ethics   \n994 Question: Does the research conducted in the paper conform, in every respect, with the   \n995 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n996 Answer: [Yes]   \n997 Justification: We read the guidelines and wrote the paper according to the NeurIPS Code of   \n998 Ethics.   \n999 Guidelines:   \n1000 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n1001 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n1002 deviation from the Code of Ethics.   \n1003 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n1004 eration due to laws or regulations in their jurisdiction).   \n1005 10. Broader Impacts   \n1006 Question: Does the paper discuss both potential positive societal impacts and negative   \n1007 societal impacts of the work performed?   \n1008 Answer: [Yes]   \n1009 Justification: As discussed in Appendix C, this work has potential societal consequences,   \n1010 none of which needs to be specially highlighted.   \n1011 Guidelines: ", "page_idx": 37}, {"type": "text", "text": "", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 39}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA]   \nJustification: The models used in this work do not have a high risk of misuse. Guidelines:   \n\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 39}, {"type": "text", "text": "", "page_idx": 39}, {"type": "text", "text": "1051 12. Licenses for existing assets ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "052 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n053 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n054 properly respected?   \n056 Justification: All assets are properly cited in our work.   \n057 Guidelines: ", "page_idx": 39}, {"type": "text", "text": "", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. ", "page_idx": 39}, {"type": "text", "text": "1065 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n1066 package should be provided. For popular datasets, paperswithcode.com/datasets   \n1067 has curated licenses for some datasets. Their licensing guide can help determine the   \n1068 license of a dataset.   \n1069 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n1070 the derived asset (if it has changed) should be provided.   \n1071 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n1072 the asset\u2019s creators.   \n1073 13. New Assets   \n1074 Question: Are new assets introduced in the paper well documented and is the documentation   \n1075 provided alongside the assets?   \n1076 Answer: [NA]   \n1077 Justification: This paper does not release new assets.   \n1078 Guidelines:   \n1079 \u2022 The answer NA means that the paper does not release new assets.   \n1080 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n1081 submissions via structured templates. This includes details about training, license,   \n1082 limitations, etc.   \n1083 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n1084 asset is used.   \n1085 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n1086 create an anonymized URL or include an anonymized zip file.   \n1087 14. Crowdsourcing and Research with Human Subjects   \n1088 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n1089 include the full text of instructions given to participants and screenshots, if applicable, as   \n1090 well as details about compensation (if any)?   \n1091 Answer: [NA]   \n1092 Justification: Crowdsourcing or research with human subjects is not involved in this work.   \n1093 Guidelines:   \n1094 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n1095 human subjects.   \n1096 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n1097 tion of the paper involves human subjects, then as much detail as possible should be   \n1098 included in the main paper.   \n1099 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n1100 or other labor should be paid at least the minimum wage in the country of the data   \n1101 collector.   \n1102 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n1103 Subjects   \n1104 Question: Does the paper describe potential risks incurred by study participants, whether   \n1105 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n1106 approvals (or an equivalent approval/review based on the requirements of your country or   \n1107 institution) were obtained?   \n1108 Answer: [NA]   \n1109 Justification: Crowdsourcing or research with human subjects is not involved in our paper.   \n1110 Guidelines:   \n1111 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n1112 human subjects.   \n1113 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n1114 may be required for any human subjects research. If you obtained IRB approval, you   \n1115 should clearly state this in the paper. ", "page_idx": 40}, {"type": "text", "text": "\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 41}]