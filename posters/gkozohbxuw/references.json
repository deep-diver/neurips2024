{"references": [{"fullname_first_author": "Edward J Hu", "paper_title": "LoRA: Low-rank adaptation of large language models", "publication_date": "2022-00-00", "reason": "This paper introduces the LoRA (Low-Rank Adaptation) technique, a core method used and improved upon in the current paper, providing parameter-efficient fine-tuning for LLMs."}, {"fullname_first_author": "Brendan McMahan", "paper_title": "Communication-efficient learning of deep networks from decentralized data", "publication_date": "2017-00-00", "reason": "This foundational paper on federated learning (FL) provides the core algorithms and concepts that the current paper builds upon and improves for LLM fine-tuning."}, {"fullname_first_author": "Armen Aghajanyan", "paper_title": "Intrinsic dimensionality explains the effectiveness of language model fine-tuning", "publication_date": "2021-08-00", "reason": "This paper introduces the concept of intrinsic dimensionality in LLMs, which is theoretically analyzed in the current work to explain the generalization ability of the proposed method."}, {"fullname_first_author": "Jonathan Baxter", "paper_title": "A model of inductive bias learning", "publication_date": "2000-00-00", "reason": "This paper provides a theoretical framework for analyzing inductive bias in learning models, which is used in the theoretical analysis of the generalization ability of the proposed FL method."}, {"fullname_first_author": "Sara Babakniya", "paper_title": "SLORA: Federated parameter efficient fine-tuning of language models", "publication_date": "2023-00-00", "reason": "This paper is a direct competitor method to the proposed method, providing a baseline for comparison and highlighting the improvements achieved by the proposed method."}]}