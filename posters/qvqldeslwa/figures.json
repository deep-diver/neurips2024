[{"figure_path": "QvqLdeSLWA/figures/figures_1_1.jpg", "caption": "Figure 1: Current diffusion features widely suffer from content shift, i.e., content differences between inputs and features. Due to the inherent connection, content shift can be suppressed with off-the-shelf generation techniques.", "description": "The figure illustrates the concept of \"content shift\" in diffusion features.  It shows that the features extracted from a diffusion model (convolutional and attention features) don't perfectly match the input image. There are subtle but noticeable differences in content, which the authors call \"content shift\". The figure suggests that this content shift can be mitigated using off-the-shelf image generation techniques.", "section": "1 Introduction"}, {"figure_path": "QvqLdeSLWA/figures/figures_3_1.jpg", "caption": "Figure 2: The overall process of feature extraction. The original image is first processed into UNet inputs via VAE and noise addition. Afterward, we collect the output activations of each resolution in the upsampling stage as convolutional features. At the same time, the cross-attention layers of UNet produce similarity maps, which are averaged over all upsampling layers as attention features.", "description": "This figure illustrates the feature extraction process.  It starts with an input image, which is encoded using a Variational Autoencoder (VAE) and then noise is added. This noisy image is fed into a pre-trained diffusion UNet.  The UNet's activations from the upsampling stages are collected as convolutional features.  Additionally, the average of the similarity maps from the cross-attention layers is taken as the attention features. These features are then used for downstream discriminative tasks.", "section": "3 Preliminaries: Diffusion Feature"}, {"figure_path": "QvqLdeSLWA/figures/figures_3_2.jpg", "caption": "Figure 3: The averaged results over three repeats with quality prompts. Horse-21 (high quality) and CIFAR10 (low quality) benefit from prompts closer to the image quality, suggesting the negative effect of content shift at small timesteps.", "description": "This figure shows the results of an experiment testing the impact of prompt quality on the performance of a model.  Two datasets, Horse-21 (high-quality images) and CIFAR-10 (low-quality images) were used.  Different prompts were used which reflected different levels of image quality (low, neutral, high). The results show that using prompts that accurately describe the image quality leads to better performance, suggesting that content shift (differences between the features and the input image) negatively impacts the model's performance, especially at small timesteps.", "section": "4 Exploration of Content Shift"}, {"figure_path": "QvqLdeSLWA/figures/figures_4_1.jpg", "caption": "Figure 4: Content shift is caused by the reconstruction process within diffusion model activations. This visualization consists of activations obtained during a single network forward pass.", "description": "This figure visualizes the process of content shift in diffusion models. It shows how the early activations of a diffusion model add noise to the input image (a). These noisy inputs are progressively denoised by the model, and in the middle stages (d), a reconstructed \"clean\" activation is obtained. However, this reconstructed image (d) differs from the original input image (a). This difference, what authors call \"content shift\", is caused by information lost during the noisy process and subsequent reconstruction. The final output noise (e) demonstrates the model's attempt to refine this reconstructed activation toward the original image, further illustrating that the content shift occurs during the reconstruction phase.", "section": "4.2 Cause of Content Shift"}, {"figure_path": "QvqLdeSLWA/figures/figures_5_1.jpg", "caption": "Figure 5: Overview of the GATE guideline and our implementation. GATE evaluates if a technique can suppress content shift based on the result of Img2Img generation. If a technique makes the result more similar to the input, it is considered to be potentially helpful. We further implement GATE by choosing three off-the-shelf generation techniques and amalgamating features obtained with different combinations of the techniques.", "description": "This figure illustrates the proposed GATE (GenerAtion Techniques Enhanced) guideline and its implementation.  The GATE guideline uses Img2Img generation to evaluate whether a generation technique reduces content shift in diffusion features.  A technique is considered helpful if it generates an image closer to the original input.  The implementation uses three generation techniques (Fine-Grained Prompts, ControlNet, and LoRA) and combines features from these techniques using feature amalgamation.", "section": "5 Suppression of Content Shift"}, {"figure_path": "QvqLdeSLWA/figures/figures_6_1.jpg", "caption": "Figure 6: Img2Img generation results according to the GATE guideline, validating the potential of the three selected techniques for suppressing content shift.", "description": "This figure shows the results of image-to-image (Img2Img) generation using three different techniques: Fine-Grained Prompts, ControlNet, and LoRA. Each technique aims to suppress content shift in diffusion features by steering the reconstruction process towards the original clean image. The input image is shown alongside the reference image (generated using a high-repainting Img2Img approach to amplify content divergence), as well as the results obtained with each generation technique.  The figure visually demonstrates the effectiveness of these techniques in mitigating content shift and improving the quality of diffusion features, supporting the claim that off-the-shelf generation techniques can be effectively used to improve diffusion features.", "section": "5 Suppression of Content Shift"}, {"figure_path": "QvqLdeSLWA/figures/figures_6_2.jpg", "caption": "Figure 7: The first row shows features extracted at different timesteps. The second row is from different combinations of generation techniques and shows stronger diversity.", "description": "This figure visualizes the impact of different timesteps and generation techniques on diffusion features. The top row displays features extracted using the basic model at various timesteps, showing the evolution of features during the reconstruction process.  The bottom row showcases features obtained by combining different generation techniques, illustrating how these methods enhance the diversity of the generated features. This diversity is crucial because it enhances robustness and generalizability when these features are used in downstream tasks.", "section": "5 Suppression of Content Shift"}, {"figure_path": "QvqLdeSLWA/figures/figures_9_1.jpg", "caption": "Figure 8: Effect of GATE without feature amalgamation. Images with various scenes are shown for generalizability. In the second image, the attention feature is asked to focus on the horse and ignore the rider. The mIoU performance is on a single Horse-21 split, with red/blue for the best/worst.", "description": "This figure shows the effect of the proposed GATE method on feature quality without using feature amalgamation. It displays feature visualizations (attention and convolutional features) for three different images using different methods: baseline, fine-grained prompts, ControlNet, LORA, and all combined. The results show that using the proposed GATE method enhances feature quality and improves the ability to focus on specific features, even in complex scenes, compared to using the baseline method. The mIoU performance is also shown for each method on a single Horse-21 split.", "section": "6.4 Ablation Study: Effect without Feature Amalgamation"}, {"figure_path": "QvqLdeSLWA/figures/figures_16_1.jpg", "caption": "Figure 1: Current diffusion features widely suffer from content shift, i.e., content differences between inputs and features. Due to the inherent connection, content shift can be suppressed with off-the-shelf generation techniques.", "description": "This figure illustrates the concept of \"content shift\" in diffusion features.  The input image (a horse) is processed by a diffusion model, and the resulting features (activations within the model) are shown.  The key point is that the features don't perfectly match the input image; there are subtle differences, which the authors refer to as content shift.  These differences impede the accuracy of using these features for downstream tasks. The figure shows that this can be mitigated through off-the-shelf generation techniques.", "section": "1 Introduction"}, {"figure_path": "QvqLdeSLWA/figures/figures_19_1.jpg", "caption": "Figure 8: Effect of GATE without feature amalgamation. Images with various scenes are shown for generalizability. In the second image, the attention feature is asked to focus on the horse and ignore the rider. The mIoU performance is on a single Horse-21 split, with red/blue for the best/worst.", "description": "This figure shows the impact of using the proposed GATE method without feature amalgamation.  It displays three example images (a kitchen, a street scene, and a portrait), and their corresponding convolutional and attention features. Each image's features are shown under different conditions (Baseline, Fine-Grained Prompts, ControlNet, LoRA, and All Combined). This visualization allows for a qualitative comparison of the effects of the individual and combined methods on content shift and feature clarity. The mIoU performance metric for a single Horse-21 split is provided below each image, highlighting the relative success of each approach.", "section": "6.4 Ablation Study: Effect without Feature Amalgamation"}, {"figure_path": "QvqLdeSLWA/figures/figures_20_1.jpg", "caption": "Figure 5: Overview of the GATE guideline and our implementation. GATE evaluates if a technique can suppress content shift based on the result of Img2Img generation. If a technique makes the result more similar to the input, it is considered to be potentially helpful. We further implement GATE by choosing three off-the-shelf generation techniques and amalgamating features obtained with different combinations of the techniques.", "description": "This figure illustrates the proposed GATE (GenerAtion Techniques Enhanced) guideline and its implementation.  The GATE guideline evaluates the effectiveness of off-the-shelf generation techniques in suppressing content shift in diffusion features.  It uses Img2Img generation to assess whether a technique brings the generated image closer to the original input image. If it does, then the technique is considered to effectively suppress content shift. The implementation part shows how three chosen techniques (Fine-Grained Prompts, ControlNet, and LoRA) are combined and their features are amalgamated for better results. ", "section": "5 Suppression of Content Shift"}, {"figure_path": "QvqLdeSLWA/figures/figures_22_1.jpg", "caption": "Figure 1: Current diffusion features widely suffer from content shift, i.e., content differences between inputs and features. Due to the inherent connection, content shift can be suppressed with off-the-shelf generation techniques.", "description": "This figure illustrates the concept of content shift in diffusion features.  It shows that there are differences between the input image and the features extracted from a diffusion model. These differences, termed 'content shift',  negatively impact the quality of diffusion features. The figure also proposes that off-the-shelf image generation techniques can be used to mitigate this content shift by leveraging the inherent connection between generation and feature extraction in diffusion models. The figure visually shows different stages in the process: input image, convolutional features directly from the diffusion model, and features after content shift suppression using generation techniques.", "section": "1 Introduction"}, {"figure_path": "QvqLdeSLWA/figures/figures_22_2.jpg", "caption": "Figure 1: Current diffusion features widely suffer from content shift, i.e., content differences between inputs and features. Due to the inherent connection, content shift can be suppressed with off-the-shelf generation techniques.", "description": "This figure illustrates the problem of content shift in diffusion features.  The left side shows a standard diffusion model feature extraction process, highlighting the difference between the input image and the resulting features. This difference is labeled \"content shift\". The right side demonstrates how off-the-shelf generation techniques can be used to mitigate content shift and produce better features that are more consistent with the input image. This shows the core concept of the paper, which is to address the limitations of traditional diffusion feature extraction by leveraging readily available image generation techniques.", "section": "Exploration of Content Shift"}, {"figure_path": "QvqLdeSLWA/figures/figures_22_3.jpg", "caption": "Figure 1: Current diffusion features widely suffer from content shift, i.e., content differences between inputs and features. Due to the inherent connection, content shift can be suppressed with off-the-shelf generation techniques.", "description": "This figure illustrates the concept of \"content shift\" in diffusion features. It shows that the features extracted from a diffusion model often differ from the input image in terms of content, such as the exact shape of an object. This difference hinders the performance of diffusion features in discriminative tasks. The figure also suggests that off-the-shelf generation techniques can be used to suppress content shift and improve the quality of diffusion features.", "section": "1 Introduction"}, {"figure_path": "QvqLdeSLWA/figures/figures_22_4.jpg", "caption": "Figure 1: Current diffusion features widely suffer from content shift, i.e., content differences between inputs and features. Due to the inherent connection, content shift can be suppressed with off-the-shelf generation techniques.", "description": "This figure illustrates the concept of content shift in diffusion features.  It shows how the features extracted from a diffusion model (the 'Diffusion Model' box) can differ from the input image. These differences are highlighted as 'Content Shift'. The figure proposes that off-the-shelf generation techniques can be used to mitigate this content shift, leading to better features. It visually compares convolutional features, attention features, and how the application of generation techniques affects the features, resulting in suppressed content shift and better features.", "section": "Exploration of Content Shift"}, {"figure_path": "QvqLdeSLWA/figures/figures_22_5.jpg", "caption": "Figure 1: Current diffusion features widely suffer from content shift, i.e., content differences between inputs and features. Due to the inherent connection, content shift can be suppressed with off-the-shelf generation techniques.", "description": "This figure illustrates the concept of content shift in diffusion features.  It shows how the features extracted from a diffusion model (the internal activations) can differ from the input image. These differences, termed \"content shift,\" represent discrepancies in details such as object shapes and exact forms. The figure highlights that these differences can be mitigated by applying off-the-shelf image generation techniques to the process of feature extraction.  The figure visually compares input images to their corresponding convolutional and attention features (produced by the diffusion model), clearly showing the differences in content between the input and the features. It then suggests that the content shift can be reduced by using off-the-shelf image generation techniques during the feature extraction process.", "section": "1 Introduction"}, {"figure_path": "QvqLdeSLWA/figures/figures_22_6.jpg", "caption": "Figure 1: Current diffusion features widely suffer from content shift, i.e., content differences between inputs and features. Due to the inherent connection, content shift can be suppressed with off-the-shelf generation techniques.", "description": "The figure illustrates the concept of content shift in diffusion features, which is a discrepancy between input images and the features extracted from a diffusion model.  It shows how off-the-shelf generation techniques can potentially mitigate this issue by reducing the difference between the input and the generated features.  The figure uses visual examples to highlight the problem and proposed solution.", "section": "1 Introduction"}]