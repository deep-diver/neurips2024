[{"type": "text", "text": "Prioritize Alignment in Dataset Distillation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "2 Dataset Distillation aims to compress a large dataset into a significantly more   \n3 compact, synthetic one without compromising the performance of the trained mod  \n4 els. To achieve this, existing methods use the agent model to extract information   \n5 from the target dataset and embed it into the distilled dataset. Consequently, the   \n6 quality of extracted and embedded information determines the quality of the dis  \n7 tilled dataset. In this work, we find that existing methods introduce misaligned   \n8 information in both information extraction and embedding stages. To alleviate   \n9 this, we propose Prioritize Alignment in Dataset Distillation (PAD), which aligns   \n10 information from the following two perspectives. 1) We prune the target dataset   \n11 according to the compressing ratio to filter the information that can be extracted   \n12 by the agent model. 2) We use only deep layers of the agent model to perform the   \n13 distillation to avoid excessively introducing low-level information. This simple   \n14 strategy effectively filters out misaligned information and brings non-trivial im  \n15 provement for mainstream matching-based distillation algorithms. Furthermore,   \n16 built on trajectory matching, PAD achieves remarkable improvements on vari  \n17 ous benchmarks, achieving state-of-the-art performance. The code and distilled   \n18 datasets will be made public. ", "page_idx": 0}, {"type": "text", "text": "19 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "20 Dataset Distillation (DD) [43] aims to compress a large dataset into a small synthetic dataset that   \n21 preserves important features for models to achieve comparable performances. Ever since being   \n22 introduced, DD has gained a lot of attention because of its wide applications in practical fields such   \n23 as privacy preservation [5, 44], continual learning [28, 35], and neural architecture search [12, 32].   \n24 Recently, matching-based methods [46, 42, 6] have achieved promising performance in distilling   \n25 high-quality synthetic datasets. Generally, the process of these methods can be summarized into two   \n26 steps: (1) Information Extraction: an agent model is used to extract important information from the   \n27 target dataset by recording various metrics such as gradients [49], distributions [48], and training   \n28 trajectories [1], (2) Information Embedding: the synthetic samples are optimized to incorporate the   \n29 extracted information, which is achieved by minimizing the differences between the same metric   \n30 calculated on the synthetic data and the one recorded in the previous step.   \n31 In this work, we first reveal both steps will introduce misaligned information, which is redundant   \n32 and potentially detrimental to the quality of the synthetic data. Then, by analyzing the cause of this   \n33 misalignment, we propose alleviating this problem through the following two perspectives.   \n34 Typically, in the Information Extraction step, most distillation methods allow the agent model to   \n35 see all samples in the target dataset. This means information extracted by the agent model comes   \n36 from samples with various difficulties (see Figure 1(a)). However, according to previous study ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "image", "img_path": "UGwdz3kjht/tmp/3c782dd431543b54cdac5c7d5cb5f3ee21820239f3acebd9b9871ff5affa01cf.jpg", "img_caption": ["(b) Parameters used for distillation "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: (a) Compared with using all samples without differentiation in IPCs (left), PAD meticulously selects a subset of samples for different IPCs to align the expected difficulty of information required (right). (b) Different layers distill different patterns (left). PAD masks out (grey box) shallow-layer parameters during metric matching in accordance with IPCs (right). ", "page_idx": 1}, {"type": "text", "text": "37 [10], information related to easy samples is only needed when the compression ratio is high. This   \n38 misalignment leads to the sub-optimal of the distillation performance.   \n39 To alleviate the above issue, we first use data selection methods to measure the difficulty of each   \n40 sample in the target dataset. Then, during the distillation, a data scheduler is employed to ensure only   \n41 data whose difficulty is aligned with the compression ratio is available for the agent model.   \nIn the Information Embedding step, most distillation methods except DM [48] choose to use all   \n43 parameters of the agent model to perform the distillation. Intuitively, this will ensure the information   \n44 extracted by the agent model is fully utilized. However, we find shallow layer parameters of the   \n45 model can only provide low-quality, basic signals, which are redundant for dataset distillation in   \n46 most cases. Conversely, performing the distillation with only parameters from deep layers will yield   \n47 high-quality synthetic samples. We attribute this contradiction to the fact that deeper layers in DNNs   \n48 tend to learn higher-level representations of input data [27, 37].   \n49 Based on our findings, to avoid embedding misaligned information in the Information Embedding step,   \n50 we propose to use only parameters from deeper layers of the agent model to perform distillation, as   \n51 illustrated in Figure 1(b). This simple change brings significant performance improvement, showing   \n52 its effectiveness in aligning information.   \n53 Through experiments, we validate that our two-step alignment strategy is effective for distillation   \n54 methods based on matching gradients [49], distributions [48], and trajectories [1]. Moreover, by   \n55 applying our alignment strategy on trajectory matching [1, 10], we propose our novel method named   \n56 Prioritize Alignment in Dataset Distillation (PAD). After conducting comprehensive evaluation   \n57 experiments, we show PAD achieves state-of-the-art (SOTA) performance. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "58 2 Misaligned Information in Dataset Distillation ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "59 Generally, we can summarize the distillation process of matching-based methods into the following   \n60 two steps: (1) Information Extraction: use an agent model to extract essential information from the   \n61 target dataset, realized by recording metrics such as gradients [49], distributions [48], and training   \n62 trajectories [1], (2) Information Embedding: the synthetic samples are optimized to incorporate the   \n63 extracted information, realized by minimizing the differences between the same metric calculated on   \n64 the synthetic data and the one recorded in the first step. ", "page_idx": 1}, {"type": "image", "img_path": "UGwdz3kjht/tmp/32eaddfebfbad4ca2e36b89289b08a5380d4150509163eeec4b199a47d3f27f0.jpg", "img_caption": ["Figure 2: Distillation performance on CIFAR-10 where data points are removed with different ratios. Removing unnecessary data points helps to improve the performance of methods based on matching gradients, distributions, and trajectories, both in low and high IPC cases. "], "img_footnote": [], "page_idx": 2}, {"type": "image", "img_path": "UGwdz3kjht/tmp/b32925818f26c8b5997f1f52245c01ecca8be82ec0d39d4661a503ef09d679bb.jpg", "img_caption": [], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 3: Distillation performances on CIFAR-10 where $\\mathfrak{n}\\%$ (ratio) shallow layer parameters are not utilized during distillation. Discarding shallow-layer parameters is beneficial for methods based on matching gradients, distributions, and trajectories, both in low and high IPC cases. ", "page_idx": 2}, {"type": "text", "text": "65 In this section, through analyses and experimental verification, we show the above two steps both   \n66 will introduce misaligned information to the synthetic data. ", "page_idx": 2}, {"type": "text", "text": "67 2.1 Misaligned Information Extracted by Agent Models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "68 In the information extraction step, an agent model is employed to extract information from the target   \n69 dataset. Generally, most existing methods [1, 6, 49, 46] allow the agent model to see the full dataset.   \n70 This implies that the information extracted by the agent model originates from samples with diverse   \n71 levels of difficulty. However, the expected difficulty of distilled information varies with changes in   \n72 IPC: smaller IPCs prefer easier information while larger IPCs should distill harder one [10].   \n73 To verify if this misalignment will influence the quality of synthetic data, we perform the distillation   \n74 where hard/easy samples of target dataset are removed with various ratios. As the results reported in   \n75 Figure 2, pruning unaligned data points is beneficial for all matching-based methods. This proves the   \n76 misalignment indeed will influence the distillation performance and can be alleviated by flitering out   \n77 misaligned data from the target dataset. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "78 2.2 Misaligned Information Embedded by Metric Matching ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "79 Most existing methods use all parameters of the agent model to compute the metric used for matching.   \n80 Intuitively, this helps to improve the distillation performance, since in this way all information   \n81 extracted by the agent model will be embedded into the synthetic dataset. However, since shallow   \n82 layers in DNNs tend to learn basic distributions of data [27, 37], using parameters from these layers   \n83 can only provide low-level signals that turned out to be redundant in most cases.   \n84 As can be observed in Figure 3, it is evident that across all matching-based methods, the removal   \n85 of shallow layer parameters consistently enhances performance, regardless of the IPC setting. This   \n86 proves employing over-shallow layer parameters to perform the distillation will introduce misaligned   \n87 information to the synthetic data, compromising the quality of distilled data. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "88 3 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "89 To alleviate the information misalignment issue, based on trajectory matching (TM) [1, 10], we   \n90 propose Prioritizing Alignment in Dataset Distillation (PAD). PAD can also be applied to methods   \n91 based on matching gradients [49] and distributions [48], which are introduced in Appendix A.1. ", "page_idx": 3}, {"type": "text", "text": "92 3.1 Preliminary of Trajectory Matching ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "93 Following the two-step procedure, to extract information, TM-based methods [1, 10] first train agent   \n94 models on the real dataset $\\mathcal{D}_{R}$ and record the changes of the parameters. Specifically, let $\\{\\theta_{t}^{*}\\}_{0}^{\\breve{N}}$ be   \n95 an expert trajectory, which is a parameter sequence recorded during the training of agent model. At   \n96 each iteration of trajectory matching, $\\theta_{t}^{*}$ and $\\theta_{t+M}^{*}$ are randomly selected from expert trajectories as   \n97 the start and target parameters.   \n98 To embed the information into the synthetic data, TM methods minimize the distance between the   \n99 expert trajectory and the student trajectory. Let $\\widehat{\\theta}_{t}$ denote the parameters of the student agent model   \n100 trained on synthetic dataset $\\mathcal{D}_{S}$ at timestep $t$ . The student trajectory progresses by doing gradient   \n101 descent on the cross-entropy loss $l$ for $N$ steps: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{\\theta}_{t+i+1}=\\hat{\\theta}_{t+i}-\\alpha\\nabla l(\\hat{\\theta}_{t+i},\\mathcal{D}_{S}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "102 Finally, the synthetic data is optimized by minimizing the distance metric, which is formulated as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\frac{\\lvert|\\hat{\\theta}_{t+N}-\\theta_{t+M}^{*}\\rvert|}{\\lvert|\\theta_{t+M}^{*}-\\theta_{t}^{*}\\rvert|},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "103 3.2 Filtering Information Extraction ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "104 In section 2.1, we show using data selection to filter out unmatched samples could alleviate the   \n105 misalignment caused in Information Extraction step. According to previous work [10], TM-based   \n106 methods prefer easy information and choose to match only early trajectories when IPC is small.   \n107 Conversely, hard information is preferred by high IPCs and they match only late trajectories. Hence,   \n108 we should use easy samples to train early trajectories, while late trajectories should be trained with   \n109 hard samples. To realize this efficiently, we first use the data selection method to measure the difficulty   \n110 of samples contained in the target dataset. Then, during training expert trajectories, a scheduler is   \n111 implemented to gradually incorporate hard samples into the training set while excluding easier ones   \n112 from it.   \n113 Difficulty Scoring Function Identifying the difficulty of data for DNNs to learn has been well   \n114 studied in data selection area [29, 17, 16, 40]. For simplicity consideration, we use Error L2-Norm   \n115 (EL2N) score [33] as the metric to evaluate the difficulty of training examples (other metrics can also   \n116 be chosen, see Section 4.3.2). Specifically, let $x$ and $y$ denote a data point and its label, respectively.   \n117 Then, the EL2N score can be calculated by: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\chi_{t}(x,y)=\\mathbb{E}||p(w_{t},x)-y||_{2},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "118 where $p(w_{t},x)\\;=\\;\\sigma(f(w_{t},x))$ is the output of a model $f$ at training step $t$ transformed into a   \n119 probability distribution. In consistent with [40], samples with higher EL2N scores are considered as   \n120 harder samples in this paper.   \n121 Scheduler The scheduler can be divided into the following stages. Firstly, the hardest samples are   \n122 removed from the training set, ensuring that it exclusively comprises data meeting a predetermined   \n123 initial ratio (IR). Then, during training expert trajectories, samples are gradually added to the training   \n124 set in order of increasing difficulty. After incorporating all the data into the training set, the scheduler   \n125 will begin to remove easy samples from the target dataset. Unlike the gradual progression involved in   \n126 adding data, the action of reducing data is completed in a single operation, since now the model has   \n127 been trained on simple samples for a sufficient time. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "table", "img_path": "UGwdz3kjht/tmp/b9f98bdde263e12d41969216ad4e8531d45781bf6c9d5d856e1068e3ae7663aa.jpg", "table_caption": [], "table_footnote": ["Table 1: Comparison with previous dataset distillation methods (bottom: matching-based, top: others) on CIFAR-10, CIFAR-100 and Tiny ImageNet. ConvNet is used for the distillation and evaluation. Our method consistently outperforms prior matching-based methods. "], "page_idx": 4}, {"type": "text", "text": "128 3.3 Filtering Information Embedding ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "129 To filter out misaligned information introduced by matching shallow-layer parameters, we propose   \n130 to add a parameter selection module that masks out part of shallow layers for metric computation.   \n131 Specifically, parameters of an agent network can be represented as a flattened array of length $L$ that   \n132 stores weights of agent models ordered from shallow to deep layers (parameters within the same   \n133 layer are sorted in default order). The parameter selection sets a threshold ratio $\\alpha$ such that the first   \n134 $k=L\\cdot\\alpha$ parameters are not used for distillation. Then the parameters used for matching can now be   \n135 formulated as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\theta}_{t+N}=\\{\\underbrace{\\hat{\\theta}_{0},\\hat{\\theta}_{1},\\cdot\\cdot\\cdot\\cdot,\\hat{\\theta}_{k-1}}_{\\mathrm{discard}},\\underbrace{\\hat{\\theta}_{k},\\hat{\\theta}_{k+1},\\cdot\\cdot\\cdot,\\hat{\\theta}_{L}}_{\\mathrm{used~for~matching}}\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "136 In practice, the ratio $\\alpha$ should vary with the change of IPC. For smaller IPCs, it is necessary to   \n137 incorporate basic information thus $\\alpha$ should be lower. Conversely, basic information is redundant in   \n138 larger IPC cases, so $\\alpha$ should be higher accordingly. ", "page_idx": 4}, {"type": "text", "text": "139 4 Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "140 4.1 Settings ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "141 We compare PAD with several prominent dataset distillation methods, which can be divided into two   \n142 categories: matching-based approaches including DC [49], DM [48], DSA [47], CAFE [42], MTT [1],   \n143 FTD [6], DATM [10], TESLA [4], and kernel-based approaches including KIP [31], FRePo [50],   \n144 RCIG [26]. The assessment is conducted on widely recognized datasets: CIFAR-10, CIFAR-100[18],   \n145 and Tiny ImageNet [20]. We implemented our method based on DATM [10]. In both the distillation   \n146 and evaluation phases, we apply the standard set of differentiable augmentations commonly used in   \n147 previous studies [1, 6, 10]. By default, networks are constructed with instance normalization unless   \n148 explicitly labeled with \"-BN,\" indicating batch normalization (e.g., ConvNet-BN). For CIFAR-10   \n149 and CIFAR-100, distillation is typically performed using a 3-layer ConvNet, while Tiny ImageNet   \n150 requires a 4-layer ConvNet. Cross-architecture experiments also utilize LeNet [21], AlexNet [19],   \n151 VGG11 [39], and ResNet18 [11]. More details can be found in the appendix. ", "page_idx": 4}, {"type": "text", "text": "152 4.2 Main Results ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "153 CIFAR and Tiny ImageNet We conduct comprehensive experiments to compare the performance   \n154 of our method with previous works. As the results presented in Table 1, PAD outperforms previous   \n155 matching-based methods on three datasets except for the case when $\\mathrm{IPC}{=}1$ . When compared with   \n156 kernel-based methods which use a larger network to perform the distillation, our technique exhibits   \n157 superior performance in most cases, particularly when the compression ratio exceeds $1\\%$ . As can be   \n158 observed, PAD performs relatively better when IPC is high, suggesting our filtering out misaligned   \n159 information strategy becomes increasingly effective as IPC increases. ", "page_idx": 4}, {"type": "table", "img_path": "UGwdz3kjht/tmp/c697b7917460133a1c5e3d05d3bee454c5dac14cf27fd7991ae16a57d63c3b6c.jpg", "table_caption": [], "table_footnote": ["Table 2: Cross-architecture evaluation of distilled data on unseen networks. Results worse than random selection are indicated with red color. $\\uparrow$ denotes the performance improvement brought by our method compared with random selection. Tiny denotes Tiny ImageNet. "], "page_idx": 5}, {"type": "table", "img_path": "UGwdz3kjht/tmp/067c04e366bec152dc4e62941ae175e85d576abde8c14d29953bdd7d63f1dc4d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "Table 3: (a) Cross-Architecture evaluation on CIFAR-100 IPC50. (b) Ablation studies on the modules of our method on CIFAR-10 IPC10. (c) Results of different sets of data selection hyper-parameters on CIFAR-10 IPC10. ", "page_idx": 5}, {"type": "text", "text": "160 Cross Architecture Generalization We evaluate the generalizability of our distilled data in both   \n161 low and high IPC cases. As results reported in Table 3(a), when IPC is small, our distilled data   \n162 outperforms the previous SOTA method DATM on ResNet and AlexNet while maintaining comparable   \n163 accuracy on VGG. This suggests that our distilled data on high compressing ratios generalizes well   \n164 across various unseen networks. Moreover, as reflected in Table 2, our distilled datasets on large IPCs   \n165 also have the best performance on most evaluated architectures, showing good generalizability in the   \n166 low compressing ratio case. ", "page_idx": 5}, {"type": "text", "text": "167 4.3 Ablation Study ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "168 To validate the effectiveness of each component of our method, we conducted ablation experiments   \n169 on modules (section 4.3.1) and their hyper-parameter settings (section 4.3.2 and section 4.3.2). ", "page_idx": 5}, {"type": "text", "text": "170 4.3.1 Modules ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "171 Our method incorporates two separate modules to fliter information extraction (FIEX) and information   \n172 embedding (FIEM), respectively. To verify their isolated effectiveness, we conduct an ablation study   \n173 by applying two modules individually. As depicted in Table 3(b), both FIEX and FIEM bring   \n174 improvements, implying their efficacy. By applying these two modules, we are able to effectively   \n175 remove unaligned information, improving the distillation performance. ", "page_idx": 5}, {"type": "text", "text": "176 4.3.2 Hyper-parameters of Filtering Information Extraction ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "177 Initial Ratio and Data Addition Epoch To filter the information learned by agent models, we   \n178 initialize the training set with only easy samples, and the size is determined by a certain ratio of   \n179 the total size. Then, we gradually add hard samples into the training set. In practice, we use two   \n180 hyper-parameters to control the addition process: the initial ratio (IR) of training data for training   \n181 set initialization and the end epoch of hard sample addition (AEE). These two parameters together   \n182 control the amount of data agent models can see at each epoch and the speed of adding hard samples. ", "page_idx": 5}, {"type": "table", "img_path": "UGwdz3kjht/tmp/9280e7e94318d394e8e249b360a1d462f9c40c0a29a5f7ffed293bc314672e0c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "UGwdz3kjht/tmp/1f5f3a442efd3fd6594d6aea4e7de63dbb7a6bf95e9a05920161ea79d2d605e9.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "UGwdz3kjht/tmp/8a9e8e921612c658a3494d3b3d7b1536472195ea844228bcc8fc9a243ffaa695.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "(a) Using EL2N to measure the difficulty of samples has the best performance. ", "page_idx": 6}, {"type": "text", "text": "(b) As IPC increases, removing more shallow-layer parameters becomes more effective. ", "page_idx": 6}, {"type": "text", "text": "(c) Using layer depth to select parameters outperforms using matching loss. ", "page_idx": 6}, {"type": "text", "text": "Table 4: (a) Ablation of different difficulty scoring functions on CIFAR-10. (b) Results of masking out different ratios of shallow-layer parameters across various IPCs on CIFAR-10. (c) Ablation on the strategy used for parameter selection on CIFAR-10 ", "page_idx": 6}, {"type": "image", "img_path": "UGwdz3kjht/tmp/d5d55559bcccf900d43d66440e4d7274c3dea4a5c873f0b3ecc8dee2ddca28b9.jpg", "img_caption": ["Figure 4: Synthetic images of CIFAR-10 IPC50 obtained by PAD with different ratios of parameter selection. Smoother image features indicate that by removing some shallow-layer parameters during matching, PAD successfully filters out coarse-grained low-level information. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "183 In Table 3(c), we show the distillation results where different hyper-parameters are utilized. In   \n184 general, a larger initial ratio and faster speed of addition bring better performances. Although the   \n185 distillation benefited more from learning simpler information when IPC is small [10], our findings   \n186 indicate that excessively removing difficult samples (e.g., more than a quarter) early in the training   \n187 phase can adversely affect the distilled data. This negative impact is likely due to the excessive   \n188 removal leading to distorted feature distributions within each category. On the other hand, reasonably   \n189 improving the speed of adding hard samples allows the agent model to achieve a more balanced   \n190 learning of information of varying difficulty across different stages.   \n191 Other Difficulty Scoring Functions Identifying the difficulty of data points is the key to flitering   \n192 out misaligned information in the extraction step. Here, we compare the effect of using other   \n193 difficulty-scoring functions to evaluate the difficulty of data. (1) prediction loss of a pre-trained   \n194 ResNet. (2) uncertainty score [3]. (3) EL2N [33]. As can be observed in Table 4(a), EL2N performs   \n195 the best across various IPCs; thus, we use it to measure how hard each data point is as default in our   \n196 method. Note that this can also be replaced with a more advanced data selection algorithm. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "197 4.3.3 Ratios of Parameter Selection ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "198 It is important to find a good balance between the percentage of shallow-layer parameters removed   \n199 from matching and the loss of information. In Table 4(b), we show results obtained on different   \n200 IPCs by discarding various ratios of shallow-layer parameters. The impact of removing varying   \n201 proportions of shallow parameters on the distilled data and its relationship with changes in IPC   \n202 is consistent with prior conclusions. For small IPCs, distilled data requires more low-level basic   \n203 information. Thus, removing too many shallow-layer parameters causes a negative effect on the   \n204 classification performance. By contrast, high-level semantic information is more important when   \n205 it comes to large IPCs. With increasing ratios of shallow-layer parameters being discarded, we can   \n206 ensure that low-level information is effectively filtered out from the distilled data. ", "page_idx": 6}, {"type": "text", "text": "207 5 Discussion ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "208 5.1 Distilled Images with Filtering Information Embedding ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "209 To see the concrete patterns brought by removing shallow-layer parameters to perform the trajectory   \n210 matching, we present distilled images obtained by discarding various ratios of shallow-layer parame  \n211 ters in Figure 4. As can be observed in Figure 4(a), without removing any shallow-layer parameters   \n212 to filter misaligned information, synthetic images are interspersed with substantial noises. These   \n213 noises often take the form of coarse and generic information, such as the overall color distribution   \n214 and edges in the image, which provides minimal utility for precise classification.   \n215 By contrast, images distilled by our enhanced methodology (see Figure 4(b) and Figure 4(c)), which   \n216 includes meticulous masking out shallow-layer parameters during trajectory matching according to the   \n217 compressing ratio, contain more fine-grained and smoother features. These images also encapsulate   \n218 a broader range of semantic information, which is crucial for helping the model make accurate   \n219 classifications. Moreover, we observe a clear trend: as the amount of the removed shallow-layer   \n220 parameters increases, the distilled images exhibit clearer and smoother features. ", "page_idx": 6}, {"type": "image", "img_path": "UGwdz3kjht/tmp/ebff74085d0747586f51094e155c8444fa0da0e0b72429b37bdb2d99937e84ae.jpg", "img_caption": ["Figure 5: Losses of different layers of ConvNet after matching trajectories for 0, 1000, and 5000 iterations. We notice a similar phenomenon on both small (IPC1 and IPC10) and large IPCs (IPC500): losses of shallow-layer parameters fluctuate along the matching process, while losses of deep-layer parameters show a clear trend of decreasing. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "UGwdz3kjht/tmp/23c59e3b6e8a509c2d205460bdaeca9922356b3cc3ccbb490cb51f7c4f9384ea.jpg", "img_caption": ["Figure 6: Synthetic images visualization with parameter selection. Matching parameters in shallow layers produces an abundance of low-level texture features, whereas patterns generated by matching deep-layer parameters embody richer high-level semantic information. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "221 5.2 Rationale for Parameter Selection ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "222 In this section, we analyze from the perspective of trajectory matching why shallow-layer parameters   \n223 should be masked out. In Figure 5, we present the changes in trajectory matching loss across different   \n224 layers as the distillation progresses. Compared to the deep-layer parameters of the agent model,   \n225 a substantial number of shallow-layer parameters exhibit low loss values that fluctuate during the   \n226 matching process (see Figure 5). By contrast, the loss values of the deep layers are much higher but   \n227 consistently decrease as distillation continues. This suggests that matching shallow layers primarily   \n228 conveys low-level information that is readily captured by the synthetic data and quickly saturated.   \n229 Consequently, the excessive addition of such low-level information produces noise, reducing the   \n230 quality of distilled datasets.   \n231 For a concrete visualization, we provide distilled images resulting from using only shallow-layer   \n232 parameters or only deep-layer parameters to match trajectories in Figure 6. The coarse image features   \n233 depicted in Figure 6(a) further substantiate our analysis.   \n235 In the previous section, we observed a positive correlation between the depth of the model layers   \n236 and the magnitude of their trajectory-matching losses. Notably, the loss in the first layer of the   \n237 ConvNet was higher compared to other shallow layers. Consequently, we further compared different   \n238 parameter alignment strategies, specifically by sorting the parameters based on their matching losses   \n239 and excluding a certain proportion of parameters with lower losses. Higher loss values indicate   \n240 greater discrepancies in parameter weights; thus, continuing to match these parameters can inject   \n241 more information into the synthetic data. As shown in Table 4(c), sorting by loss results in an   \n242 improvement compared with no parameter alignment, but flitering based on parameter depth proves   \n243 to be more effective. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "244 6 Related Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "245 Introduced by [43], dataset distillation aims to synthesize a compact set of data that allows models to   \n246 achieve similar test performances compared with the original dataset. Since then, a number of studies   \n247 have explored various approaches. These methods can be divided into three types: kernel-based,   \n248 matching-based, and using generative models [45].   \n249 Kernel-based methods are able to achieve closed-form solutions for the inner optimization [31] via   \n250 kernel ridge regression with NTK [22]. FRePo [50] distills a compact dataset through neural feature   \n251 regression and reduces the training cost.   \n252 Matching-based methods first use agent models to extract information from the target dataset   \n253 by recording a specific metric [7, 23, 38, 24]. Representative works that design different metrics   \n254 include DC [49] that matches gradients, DM [48] that matches distributions, and MTT [1] that   \n255 matches training trajectories. Then, the distilled dataset is optimized by minimizing the matched   \n256 distance between the metric computed on synthetic data and the record one from the previous step.   \n257 Following this workflow, many works have been proposed to improve the efficacy of the distilled   \n258 dataset. For example, CAFE [42] preserves the real feature distribution and the discriminative power   \n259 of the synthetic data and achieves prominent generalization ability across various architectures.   \n260 DREAM [25] employs K-Means to select representative samples for distillation and improves the   \n261 distillation efficiency. DATM [10] proposes to match early trajectories for small IPCs and late   \n262 trajectories for large IPCs, achieving SOTA performances on several benchmarks. Moreover, new   \n263 metrics such as spatial attention maps [36, 15] have also been introduced and achieved promising   \n264 performance in distilling large-scale datasets.   \n265 Generative models such as GANs [8, 13, 14, 41] and diffusion models [34, 30, 9] can also be used to   \n266 distill high quality datasets. DiM [41] uses deep generative models to store information of the target   \n267 dataset. GLaD [2] transfers synthetic data optimization from the pixel space to the latent space by   \n268 employing deep generative priors. It enhances the generalizability of previous distillation methods. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "269 7 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "270 In this work, we find a limitation of existing Dataset Distillation methods in that they will introduce   \n271 misaligned information to the distilled datasets. To alleviate this, we propose PAD, which incorporates   \n272 two modules to fliter out misaligned information. For information extraction, PAD prunes the target   \n273 dataset based on sample difficulty for different IPCs so that only information with aligned difficulty   \n274 is extracted by the agent model. For information embedding, PAD discards part of shallow-layer   \n275 parameters to avoid injecting low-level basic information into the synthetic data. PAD achieves   \n276 SOTA performance on various benchmarks. Moreover, we show PAD can also be applied to methods   \n277 based on matching gradients and distribution, bringing remarkable improvements across various IPC   \n278 settings.   \n279 Limitations Our alignment strategy could also be applied to methods based on matching gradients   \n280 and distributions (see Appendix A.1). However, due to the limitation of computing resources,   \n281 for methods based on matching distributions and gradients, we have only validated our method\u2019s   \n282 effectiveness on DM [48] and DC [49] (see Table 5 and Table 6). ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "283 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "284 [1] George Cazenavette, Tongzhou Wang, Antonio Torralba, Alexei A. Efros, and Jun-Yan Zhu.   \n285 Dataset distillation by matching training trajectories. 2022 IEEE/CVF Conference on Computer   \n286 Vision and Pattern Recognition (CVPR), pages 10708\u201310717, 2022.   \n287 [2] George Cazenavette, Tongzhou Wang, Antonio Torralba, Alexei A. Efros, and Jun-Yan Zhu.   \n288 Generalizing dataset distillation via deep generative prior. 2023 IEEE/CVF Conference on   \n289 Computer Vision and Pattern Recognition (CVPR), pages 3739\u20133748, 2023.   \n290 [3] Cody Coleman, Christopher Yeh, Stephen Mussmann, Baharan Mirzasoleiman, Peter Bailis,   \n291 Percy Liang, Jure Leskovec, and Matei Zaharia. Selection via proxy: Efficient data selection for   \n292 deep learning. arXiv preprint arXiv:1906.11829, 2019.   \n293 [4] Justin Cui, Ruochen Wang, Si Si, and Cho-Jui Hsieh. Scaling up dataset distillation to imagenet  \n294 1k with constant memory. In International Conference on Machine Learning, 2022.   \n295 [5] Tian Dong, Bo Zhao, and Lingjuan Lyu. Privacy for free: How does dataset condensation help   \n296 privacy? ArXiv, abs/2206.00240, 2022.   \n297 [6] Jiawei Du, Yiding Jiang, Vincent Y. F. Tan, Joey Tianyi Zhou, and Haizhou Li. Minimizing the   \n298 accumulated trajectory error to improve dataset distillation. 2023 IEEE/CVF Conference on   \n299 Computer Vision and Pattern Recognition (CVPR), pages 3749\u20133758, 2022.   \n300 [7] Jiawei Du, Qin Shi, and Joey Tianyi Zhou. Sequential subset matching for dataset distillation.   \n301 ArXiv, abs/2311.01570, 2023.   \n302 [8] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil   \n303 Ozair, Aaron C. Courville, and Yoshua Bengio. Generative adversarial networks. Communica  \n304 tions of the ACM, 63:139 \u2013 144, 2014.   \n305 [9] Jianyang Gu, Saeed Vahidian, Vyacheslav Kungurtsev, Haonan Wang, Wei Jiang, Yang You,   \n306 and Yiran Chen. Efficient dataset distillation via minimax diffusion. ArXiv, abs/2311.15529,   \n307 2023.   \n308 [10] Ziyao Guo, Kai Wang, George Cazenavette, Hui Li, Kaipeng Zhang, and Yang You. Towards   \n309 lossless dataset distillation via difficulty-aligned trajectory matching. ArXiv, abs/2310.05773,   \n310 2023.   \n311 [11] Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image   \n312 recognition. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),   \n313 pages 770\u2013778, 2015.   \n314 [12] Haifeng Jin, Qingquan Song, and Xia Hu. Auto-keras: An efficient neural architecture search   \n315 system. Proceedings of the 25th ACM SIGKDD International Conference on Knowledge   \n316 Discovery & Data Mining, 2018.   \n317 [13] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative   \n318 adversarial networks. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition   \n319 (CVPR), pages 4396\u20134405, 2018.   \n320 [14] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila.   \n321 Analyzing and improving the image quality of stylegan. 2020 IEEE/CVF Conference on   \n322 Computer Vision and Pattern Recognition (CVPR), pages 8107\u20138116, 2019.   \n323 [15] Samir Khaki, Ahmad Sajedi, Kai Wang, Lucy Z. Liu, Yuri A. Lawryshyn, and Konstantinos N.   \n324 Plataniotis. Atom: Attention mixer for efficient dataset distillation, 2024.   \n325 [16] Krishnateja Killamsetty, Durga Sivasubramanian, Ganesh Ramakrishnan, Abir De, and   \n326 Rishabh K. Iyer. Grad-match: Gradient matching based data subset selection for efficient   \n327 deep model training. In International Conference on Machine Learning, 2021.   \n328 [17] Krishnateja Killamsetty, Durga Sivasubramanian, Ganesh Ramakrishnan, Rishabh Iyer Univer  \n329 sity of Texas at Dallas, Indian Institute of Technology Bombay Institution One, and IN Two.   \n330 Glister: Generalization based data subset selection for efficient and robust learning. In AAAI   \n331 Conference on Artificial Intelligence, 2020.   \n332 [18] Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009.   \n333 [19] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep   \n334 convolutional neural networks. Communications of the ACM, 60:84 \u2013 90, 2012.   \n335 [20] Ya Le and Xuan S. Yang. Tiny imagenet visual recognition challenge. 2015.   \n336 [21] Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning   \n337 applied to document recognition. Proc. IEEE, 86:2278\u20132324, 1998.   \n338 [22] Jaehoon Lee, Lechao Xiao, Samuel S. Schoenholz, Yasaman Bahri, Roman Novak,   \n339 Jascha Narain Sohl-Dickstein, and Jeffrey Pennington. Wide neural networks of any depth   \n340 evolve as linear models under gradient descent. Journal of Statistical Mechanics: Theory and   \n341 Experiment, 2020, 2019.   \n342 [23] Saehyung Lee, Sanghyuk Chun, Sangwon Jung, Sangdoo Yun, and Sung-Hoon Yoon. Dataset   \n343 condensation with contrastive signals. In International Conference on Machine Learning, 2022.   \n344 [24] Haoyang Liu, Tiancheng Xing, Luwei Li, Vibhu Dalal, Jingrui He, and Haohan Wang. Dataset   \n345 distillation via the wasserstein metric. ArXiv, abs/2311.18531, 2023.   \n346 [25] Yanqing Liu, Jianyang Gu, Kai Wang, Zheng Hua Zhu, Wei Jiang, and Yang You. Dream: Effi  \n347 cient dataset distillation by representative matching. 2023 IEEE/CVF International Conference   \n348 on Computer Vision (ICCV), pages 17268\u201317278, 2023.   \n349 [26] Noel Loo, Ramin M. Hasani, Mathias Lechner, and Daniela Rus. Dataset distillation with   \n350 convexified implicit gradients. ArXiv, abs/2302.06755, 2023.   \n351 [27] Aravindh Mahendran and Andrea Vedaldi. Visualizing deep convolutional neural networks   \n352 using natural pre-images. International Journal of Computer Vision, 120:233 \u2013 255, 2016.   \n353 [28] Wojciech Masarczyk and Ivona Tautkute. Reducing catastrophic forgetting with learning on   \n354 synthetic data. In CVPR Workshop, 2020.   \n355 [29] Baharan Mirzasoleiman, Jeff A. Bilmes, and Jure Leskovec. Coresets for data-efficient training   \n356 of machine learning models. In International Conference on Machine Learning, 2019.   \n357 [30] Brian B. Moser, Federico Raue, Sebasti\u00e1n M. Palacio, Stanislav Frolov, and Andreas Dengel.   \n358 Latent dataset distillation with diffusion models. ArXiv, abs/2403.03881, 2024.   \n359 [31] Timothy Nguyen, Zhourung Chen, and Jaehoon Lee. Dataset meta-learning from kernel   \n360 ridge-regression. ArXiv, abs/2011.00050, 2020.   \n361 [32] Ramakanth Pasunuru and Mohit Bansal. Continual and multi-task architecture search. ArXiv,   \n362 abs/1906.05226, 2019.   \n363 [33] Mansheej Paul, Surya Ganguli, and Gintare Karolina Dziugaite. Deep learning on a data diet:   \n364 Finding important examples early in training. In Neural Information Processing Systems, 2021.   \n365 [34] Robin Rombach, A. Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High  \n366 resolution image synthesis with latent diffusion models. 2022 IEEE/CVF Conference on   \n367 Computer Vision and Pattern Recognition (CVPR), pages 10674\u201310685, 2021.   \n368 [35] Andrea Rosasco, Antonio Carta, Andrea Cossu, Vincenzo Lomonaco, and Davide Bacciu.   \n369 Distilled replay: Overcoming forgetting through synthetic samples. In International Workshop   \n370 on Continual Semi-Supervised Learning, 2021.   \n371 [36] Ahmad Sajedi, Samir Khaki, Ehsan Amjadian, Lucy Z. Liu, Yuri A. Lawryshyn, and Kon  \n372 stantinos N. Plataniotis. Datadam: Efficient dataset distillation with attention matching. In   \n373 Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages   \n374 17097\u201317107, October 2023.   \n375 [37] Ramprasaath R. Selvaraju, Abhishek Das, Ramakrishna Vedantam, Michael Cogswell, Devi   \n376 Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based   \n377 localization. International Journal of Computer Vision, 128:336 \u2013 359, 2016.   \n378 [38] Seung-Jae Shin, Heesun Bae, DongHyeok Shin, Weonyoung Joo, and Il-Chul Moon. Loss  \n379 curvature matching for dataset selection and condensation. In International Conference on   \n380 Artificial Intelligence and Statistics, 2023.   \n381 [39] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale   \n382 image recognition. CoRR, abs/1409.1556, 2014.   \n383 [40] Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari Morcos. Beyond   \n384 neural scaling laws: beating power law scaling via data pruning. Advances in Neural Information   \n385 Processing Systems, 35:19523\u201319536, 2022.   \n386 [41] Kai Wang, Jianyang Gu, Daquan Zhou, Zheng Hua Zhu, Wei Jiang, and Yang You. Dim:   \n387 Distilling dataset into generative model. ArXiv, abs/2303.04707, 2023.   \n388 [42] Kai Wang, Bo Zhao, Xiangyu Peng, Zheng Hua Zhu, Shuo Yang, Shuo Wang, Guan Huang,   \n389 Hakan Bilen, Xinchao Wang, and Yang You. Cafe learning to condense dataset by aligning   \n390 features. 2022.   \n391 [43] Tongzhou Wang, Jun-Yan Zhu, Antonio Torralba, and Alexei A. Efros. Dataset distillation,   \n392 2020.   \n393 [44] Qiying Yu, Yang Liu, Yimu Wang, Ke Xu, and Jingjing Liu. Multimodal federated learning via   \n394 contrastive representation ensemble. ArXiv, abs/2302.08888, 2023.   \n395 [45] Ruonan Yu, Songhua Liu, and Xinchao Wang. Dataset distillation: A comprehensive review.   \n396 IEEE Transactions on Pattern Analysis and Machine Intelligence, 46:150\u2013170, 2023.   \n397 [46] Bo Zhao and Hakan Bilen. Dataset condensation with differentiable siamese augmentation. In   \n398 International Conference on Machine Learning, 2021.   \n399 [47] Bo Zhao and Hakan Bilen. Dataset condensation with differentiable siamese augmentation. In   \n400 International Conference on Machine Learning, 2021.   \n401 [48] Bo Zhao and Hakan Bilen. Dataset condensation with distribution matching. 2023 IEEE/CVF   \n402 Winter Conference on Applications of Computer Vision (WACV), pages 6503\u20136512, 2021.   \n403 [49] Bo Zhao, Konda Reddy Mopuri, and Hakan Bilen. Dataset condensation with gradient matching.   \n404 ArXiv, abs/2006.05929, 2020.   \n405 [50] Yongchao Zhou, Ehsan Nezhadarya, and Jimmy Ba. Dataset distillation using neural feature   \n406 regression. ArXiv, abs/2206.00719, 2022. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "table", "img_path": "UGwdz3kjht/tmp/05621d54270be0c4101bc30b6bca47878aa07b101f832993cb3c4eed159a7fc6.jpg", "table_caption": [], "table_footnote": ["(a) Removing various ratios of hard/easy samples improves DC on small/large IPCs. "], "page_idx": 12}, {"type": "table", "img_path": "UGwdz3kjht/tmp/79a61fc58263829ddc86a528f47912d538f436391a2c1aa26614c02ac4bc8ff2.jpg", "table_caption": [], "table_footnote": [], "page_idx": 12}, {"type": "text", "text": "(b) Removing various ratios of hard/easy samples improves DM on small/large IPCs. ", "page_idx": 12}, {"type": "text", "text": "Table 5: Results of filtering information extraction by removing hard/easy samples in DC(a) and DM(b) on CIFAR-10. ", "page_idx": 12}, {"type": "table", "img_path": "UGwdz3kjht/tmp/bb7e23957f23b1c03434feb95eecaae9c303aee697db35befdf354975f35e96e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 12}, {"type": "text", "text": "(a) Matching gradients from deep-layer parameters leads to improvements. ", "page_idx": 12}, {"type": "table", "img_path": "UGwdz3kjht/tmp/72b67f508744ed33e08aab3474e1ef77dcd01671146d4dff0cf4cdc796199f94.jpg", "table_caption": [], "table_footnote": [], "page_idx": 12}, {"type": "text", "text": "(b) Matching distributions from deep-layer parameters leads to improvements. ", "page_idx": 12}, {"type": "text", "text": "Table 6: Results of filtering information embedding by masking out shallow-layer parameters for metric computation in DC(a) and DM(b) on CIFAR-10. ", "page_idx": 12}, {"type": "text", "text": "407 A Appendix ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "408 A.1 Filtering Misaligned Information in DC and DM ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "409 Although PAD is implemented based on trajectory matching methods, we also test our proposed   \n410 data alignment and parameter alignment on gradient matching and distribution matching. The   \n411 performances of enhanced DC and DM with each of the two modules are reported in Table 5 and   \n412 Tabl 6, respectively. We provide details of how we integrate these two modules into gradient matching   \n413 and distribution matching in the following sections.   \n414 Gradient Matching We use the official implementation1 of DC [49]. In the Information Extraction   \n415 step, DC uses an agent model to calculate the gradients after being trained on the target dataset. We   \n416 employ filter misaligned information in this step as follows: When IPC is small, a certain ratio of   \n417 hard samples is removed from the target dataset so that the recorded gradients only contain simple   \n418 information. Conversely, when IPC becomes large, we remove easy samples instead.   \n419 In the Information Embedding step, DC optimizes the synthetic data by back-propagating on the   \n420 gradient matching loss. The loss is computed by summing the differences in gradients between   \n421 each pair of model parameters. Thus, we apply parameter selection by discarding a certain ratio of   \n422 parameters in the shallow layers.   \n423 Distribution Matching We use the official implementation of DM [48], which can be accessed   \n424 via the same link as DC. In the Information Extraction step, DM uses an agent model to generate   \n425 embeddings of input images from the target dataset. Similarly, filtering information extraction is   \n426 applied by removing hard samples for small IPCs and easy samples for large IPCs.   \n427 In the Information Embedding step, since DM only uses the output of the last layer to match   \n428 distributions, we modify the implementation of the network such that outputs of each layer in the   \n429 model are returned by the forward function. Then, we perform parameter selection following the   \n430 same practice as before. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "431 A.2 Experiment Settings ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "432 We use DATM [10] as the backbone TM algorithm and our proposed PAD is built upon. Thus, our   \n433 configurations for distillation, evaluation, and network are consistent with DATM.   \n434 Distillation. We conduct the distillation process for 10,000 iterations to ensure full convergence of   \n435 the optimization. By default, ZCA whitening is applied in all the experiments.   \n436 Evaluation. We train a randomly initialized network on the distilled dataset and evaluate its per  \n437 formance on the entire validation set of the original dataset. Following DATM [10], the evaluation   \n438 networks are trained for 1000 epochs to ensure full optimization convergence. For fairness, the   \n439 experimental results of previous distillation methods in both low and high IPC settings are sourced   \n440 from [10].   \n441 Network. We employ a range of networks to assess the generalizability of our distilled datasets.   \n442 For scaling ResNet, LeNet, and AlexNet to Tiny-ImageNet, we modify the stride of their initial   \n443 convolutional layer from 1 to 2. In the case of VGG, we adjust the stride of its final max pooling   \n444 layer from 1 to 2. The MLP used in our evaluations features a single hidden layer with 128 units.   \n445 Hyper-parameters. Hyper-parameters of our experiments on CIFAR-10, CIFAR-100, and Tiny  \n446 ImageNet are reported in Table 7. Hyper-parameters can be divided into three parts including data   \n447 alignment (DA), parameter alignment (PA) and trajectory matching (TM). Soft labels are applied in   \n448 all experiments , we set its momentum to 0.9.   \n449 Compute resources. Our experiments are run on 4 NVIDIA A100 GPUs, each with $80\\ \\mathrm{GB}$ of   \n450 memory. The amount of GPU memory needed is mainly determined by the batch size of synthetic   \n451 data and the number of steps that the agment model is trained on synthetic data. To reduce the GPU   \n452 usage when IPC is large, one can apply TESLA [4] or simply reducing the synthetic steps $N$ or the   \n453 synthetic batch size. However, the decrement of hyper-parameters shown in Table 7 could result in   \n454 performance degradation. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "table", "img_path": "UGwdz3kjht/tmp/419242b89102bf2884359209d2ae48d1dbd3b7221dedb8677d17d3f6f7555bec.jpg", "table_caption": ["Table 7: Hyper-parameters for different benchmarks. "], "table_footnote": [], "page_idx": 13}, {"type": "image", "img_path": "UGwdz3kjht/tmp/dd39b3cfcd5d9afa933a09947473b4d63e2e63ab4bb5b23109d95531a737ad1a.jpg", "img_caption": ["Figure 7: Distilled images of CIFAR-10 IPC10 "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "UGwdz3kjht/tmp/7886b83bbe0c4fe2a6160c0f865bd099681e26641e157523040cab7c021571de.jpg", "img_caption": ["Figure 8: Distilled images of CIFAR-10 IPC10 "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "UGwdz3kjht/tmp/931d7c7ca64b59188d06251482bcd22795a814dc37cd6208adb9877b20e00a6e.jpg", "img_caption": ["Figure 9: Distilled images of CIFAR-10 IPC10 "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "455 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "456 1. Claims   \n457 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n458 paper\u2019s contributions and scope?   \n459 Answer: [Yes]   \n460 Justification: Our main claim does accurately reflect the paper\u2019s contributions and scope.   \n461 Guidelines:   \n462 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n463 made in the paper.   \n464 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n465 contributions made in the paper and important assumptions and limitations. A No or   \n466 NA answer to this question will not be perceived well by the reviewers.   \n467 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n468 much the results can be expected to generalize to other settings.   \n469 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n470 are not attained by the paper.   \n471 2. Limitations   \n472 Question: Does the paper discuss the limitations of the work performed by the authors?   \n473 Answer: [Yes]   \n474 Justification: We discuss limitations at the end of the paper.   \n475 Guidelines:   \n476 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n477 the paper has limitations, but those are not discussed in the paper.   \n478 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n479 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n480 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n481 model well-specification, asymptotic approximations only holding locally). The authors   \n482 should reflect on how these assumptions might be violated in practice and what the   \n483 implications would be.   \n484 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n485 only tested on a few datasets or with a few runs. In general, empirical results often   \n486 depend on implicit assumptions, which should be articulated.   \n487 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n488 For example, a facial recognition algorithm may perform poorly when image resolution   \n489 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n490 used reliably to provide closed captions for online lectures because it fails to handle   \n491 technical jargon.   \n492 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n493 and how they scale with dataset size.   \n494 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n495 address problems of privacy and fairness.   \n496 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n497 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n498 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n499 judgment and recognize that individual actions in favor of transparency play an impor  \n500 tant role in developing norms that preserve the integrity of the community. Reviewers   \n501 will be specifically instructed to not penalize honesty concerning limitations.   \n502 3. Theory Assumptions and Proofs   \n503 Question: For each theoretical result, does the paper provide the full set of assumptions and ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "504 a complete (and correct) proof? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 17}, {"type": "text", "text": "Justification: We didn\u2019t present any theoretical results in this paper. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 18}, {"type": "text", "text": "518 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: All hyper-parameters and computing resources needed for experiments are listed in the Appendix. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 18}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 18}, {"type": "text", "text": "561   \n562   \n563   \n564   \n565   \n566   \n567   \n568   \n569   \n570   \n571   \n572   \n573   \n574   \n575   \n576   \n577   \n578   \n579   \n580   \n581   \n582   \n583   \n584   \n585   \n586   \n587   \n588   \n589   \n590   \n591   \n592   \n593   \n594   \n595   \n596   \n597   \n598   \n599   \n600   \n601   \n602   \n603   \n604   \n605   \n606   \n607   \n608   \n609   \n610   \n611   \n612 ", "page_idx": 19}, {"type": "text", "text": ": [yes] Justification: Our code will be made public. Guidelines: \u2022 The answer NA means that paper does not include experiments requiring code. \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. \u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). \u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. \u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. \u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. \u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). \u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.   \n6. Experimental Setting/Details Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: All details are listed in the Appendix. Guidelines: \u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material.   \n7. Experiment Statistical Significance Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Our experiment results are reflected by classification accuracy. Guidelines: \u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. \u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). \u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) \u2022 The assumptions made should be given (e.g., Normally distributed errors). \u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "621 8. Experiments Compute Resources ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "622 Question: For each experiment, does the paper provide sufficient information on the com  \n623 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n24 the experiments?   \n626 Justification: All details are listed in the Appendix.   \n627 Guidelines: ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 20}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 20}, {"type": "text", "text": "639 Answer: [Yes]   \n640 Justification: We follow the code of ethics.   \n641 Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 20}, {"type": "text", "text": "647 10. Broader Impacts ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "648 Question: Does the paper discuss both potential positive societal impacts and negative   \n649 societal impacts of the work performed?   \n650 Answer: [NA]   \n651 Justification: Our work doesn\u2019t have societal impacts.   \n652 Guidelines: ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to ", "page_idx": 20}, {"type": "text", "text": "664 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n665 that a generic algorithm for optimizing neural networks could enable people to train   \n666 models that generate Deepfakes faster.   \n667 \u2022 The authors should consider possible harms that could arise when the technology is   \n668 being used as intended and functioning correctly, harms that could arise when the   \n669 technology is being used as intended but gives incorrect results, and harms following   \n670 from (intentional or unintentional) misuse of the technology.   \n671 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n672 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n673 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n674 feedback over time, improving the efficiency and accessibility of ML).   \n675 11. Safeguards   \n676 Question: Does the paper describe safeguards that have been put in place for responsible   \n677 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n678 image generators, or scraped datasets)?   \n679 Answer: [NA]   \n680 Justification: This is not applicable to our work.   \n681 Guidelines:   \n682 \u2022 The answer NA means that the paper poses no such risks.   \n683 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n684 necessary safeguards to allow for controlled use of the model, for example by requiring   \n685 that users adhere to usage guidelines or restrictions to access the model or implementing   \n686 safety filters.   \n687 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n688 should describe how they avoided releasing unsafe images.   \n689 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n690 not require this, but we encourage authors to take this into account and make a best   \n691 faith effort.   \n692 12. Licenses for existing assets   \n693 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n694 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n695 properly respected?   \n696 Answer: [Yes]   \n697 Justification: We cite all code, data, and previous works in a proper manner.   \n698 Guidelines:   \n699 \u2022 The answer NA means that the paper does not use existing assets.   \n700 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n701 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n702 URL.   \n703 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n704 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n705 service of that source should be provided.   \n706 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n707 package should be provided. For popular datasets, paperswithcode.com/datasets   \n708 has curated licenses for some datasets. Their licensing guide can help determine the   \n709 license of a dataset.   \n710 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n711 the derived asset (if it has changed) should be provided.   \n712 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n713 the asset\u2019s creators.   \n714 13. New Assets   \n715 Question: Are new assets introduced in the paper well documented and is the documentation   \n716 provided alongside the assets?   \n17 Answer: [NA]   \n18 Justification: This is not applicable to our work.   \n19 Guidelines:   \n720 \u2022 The answer NA means that the paper does not release new assets.   \n721 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n722 submissions via structured templates. This includes details about training, license,   \n23 limitations, etc.   \n724 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n725 asset is used.   \n26 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n727 create an anonymized URL or include an anonymized zip file.   \n28 14. Crowdsourcing and Research with Human Subjects   \n29 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n730 include the full text of instructions given to participants and screenshots, if applicable, as   \n731 well as details about compensation (if any)?   \n32 Answer: [NA]   \n733 Justification: This is not applicable to our work.   \n34 Guidelines:   \n35 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n736 human subjects.   \n737 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n738 tion of the paper involves human subjects, then as much detail as possible should be   \n39 included in the main paper.   \n40 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n41 or other labor should be paid at least the minimum wage in the country of the data   \n42 collector.   \n43 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n44 Subjects   \n45 Question: Does the paper describe potential risks incurred by study participants, whether   \n46 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n47 approvals (or an equivalent approval/review based on the requirements of your country or   \n48 institution) were obtained?   \n49 Answer: [NA]   \n50 Justification: This is not applicable to our work.   \n751 Guidelines:   \n52 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n53 human subjects.   \n54 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n55 may be required for any human subjects research. If you obtained IRB approval, you   \n56 should clearly state this in the paper.   \n57 \u2022 We recognize that the procedures for this may vary significantly between institutions   \n758 and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the   \n59 guidelines for their institution.   \n760 \u2022 For initial submissions, do not include any information that would break anonymity (if   \n761 applicable), such as the institution conducting the review. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}]