[{"type": "text", "text": "Cost-aware Bayesian Optimization via the Pandora\u2019s Box Gittins Index ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Qian Xie1 Raul Astudillo2 Peter I. Frazier1 Ziv Scully1 Alexander Terenin1 1Cornell University 2Caltech ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Bayesian optimization is a technique for efficiently optimizing unknown functions in a black-box manner. To handle practical settings where gathering data requires use of finite resources, it is desirable to explicitly incorporate function evaluation costs into Bayesian optimization policies. To understand how to do so, we develop a previously-unexplored connection between cost-aware Bayesian optimization and the Pandora\u2019s Box problem, a decision problem from economics. The Pandora\u2019s Box problem admits a Bayesian-optimal solution based on an expression called the Gittins index, which can be reinterpreted as an acquisition function. We study the use of this acquisition function for cost-aware Bayesian optimization, and demonstrate empirically that it performs well, particularly in medium-high dimensions. We further show that this performance carries over to classical Bayesian optimization without explicit evaluation costs. Our work constitutes a first step towards integrating techniques from Gittins index theory into Bayesian optimization. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Bayesian optimization is a framework for optimizing functions whose evaluation is time-consuming or expensive. It is widely used for hyperparameter tuning of machine learning algorithms [33], robot control [27], material design [42], and other areas. Bayesian optimization works by forming a probabilistic model for the objective function, and then chooses where to sample via an acquisition function that balances the explore-exploit trade-offs arising from uncertainty in this model. ", "page_idx": 0}, {"type": "text", "text": "We study cost-aware Bayesian optimization, where one must pay a cost to acquire another sample and this cost may vary with where the function is evaluated. Costs are an important factor in practical scenarios. For instance, in hyperparameter tuning using GPUs rented from a cloud provider, training a neural network for twice as many epochs may carry twice the financial cost. ", "page_idx": 0}, {"type": "text", "text": "Despite its practical relevance, cost-aware Bayesian optimization is less-studied than standard Bayesian optimization, where budgets are framed in terms of the number of function evaluations and costs are not explicitly considered. Many existing theoretically-principled cost-aware approaches [41, 23, 25, 3, 6] rely on multi-step lookahead computations that are computationally expensive and can be numerically brittle, limiting their applicability. Other approaches lack a theoretical foundation and risk having poor performance on certain problems. For example, one of the most popular cost-aware acquisition functions used in practice, expected improvement per unit cost [33], has recently been theoretically shown by Astudillo et al. [3] to perform arbitrarily-worse than the optimal policy. Thus, in the cost-aware setting, there is a need for theoretically-principled and computationally-straightforward acquisition functions with good empirical performance. ", "page_idx": 0}, {"type": "text", "text": "In this work, we develop such an approach. To do so, we introduce a novel link between cost-aware Bayesian optimization and a discrete-space decision problem from economics called the Pandora\u2019s ", "page_idx": 0}, {"type": "text", "text": "Box problem [39, 12, 32]. The Pandora\u2019s Box problem admits an explicit Bayesian-optimal solution. We show how this solution can be used to develop a novel acquisition function class for two costaware Bayesian optimization settings: (i) expected budget-constrained Bayesian optimization, where there is a constraint on the expected cost of the samples taken, and (ii) cost-per-sample Bayesian optimization where the total costs incurred are subtracted from the final objective function value. The resulting acquisition functions are closely connected to expected improvement variants, but incorporate costs in a different, non-multiplicative way. ", "page_idx": 1}, {"type": "text", "text": "We evaluate the proposed acquisition function, termed the Pandora\u2019s Box Gittins index (PBGI), on a comprehensive set of experiments to understand its strengths and weaknesses. On both sufficientlyeasy low-dimensional problems and too-difficult high-dimensional ones, performance is comparable to baselines. On most medium-hard problems of moderate dimension, however, the proposed acquisition function generally outperforms baselines, in the worst case approximately matching their performance. Surprisingly, we find this performance carries over to the classical setting with uniform costs. We also discuss limitations, including behavior on problems where baselines are stronger. ", "page_idx": 1}, {"type": "text", "text": "The Pandora\u2019s Box Gittins index is a version of the Gittins index [19], a general framework for deriving optimal policies for a variety of bandit-like decision problems [38, 13, 21] which is widelyused in queueing theory and related areas [20, 1, 31]. Our work thus opens a novel angle of attack for designing acquisition functions specialized to specific practical settings of interest. ", "page_idx": 1}, {"type": "text", "text": "Contributions. In this work, we (i) connect the Pandora\u2019s Box problem with a variant of cost-aware Bayesian optimization over a discrete search space. Using this connection, we (ii) explore the use of Gittins indices, which are Bayesian-optimal for the Pandora\u2019s Box problem, as an acquisition function for general cost-aware Bayesian optimization where data is incorporated via the posterior distribution. We (iii) demonstrate the resulting acquisition function has strong empirical performance on a variety of problems of moderate-to-high dimension, including the varying-cost problems it was designed for, as well as classical cost-unaware problems. ", "page_idx": 1}, {"type": "text", "text": "2 Cost-aware Bayesian optimization ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In black-box optimization, we are interested in finding the global optimum of an unknown (potentially stochastic) function $f\\,:\\,X\\,\\rightarrow\\,\\mathbb{R}$ defined on some compact domain, using pointwise function evaluations of $f$ at locations $x_{1},..,x_{T}\\in X$ that we select sequentially. We are interested in policies that achieve low simple regret\u2014see Garnett [17], Sec. 10.1\u2014namely ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathbb{E}\\operatorname*{sup}_{x\\in X}f(x)-\\mathbb{E}\\operatorname*{max}_{1\\leq t\\leq T}f(x_{t})\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where the expectation is taken over all sources of randomness in the function $f$ and the policy, including the stopping time $T$ and sequential selections $x_{1},..,x_{T}$ . In our setup, obtaining a new function evaluation at a point $x$ carries a non-zero cost $c(x)\\in\\mathbb{R}_{+}$ , assumed automatically-differentiable unless discussed otherwise. We consider settings that integrate costs into the problem in different ways: ", "page_idx": 1}, {"type": "text", "text": "(a) In the expected budget-constrained setting, there is a budget $B\\in\\mathbb{R}_{+}$ , and the algorithm is not allowed to exceed this budget in expectation. ", "page_idx": 1}, {"type": "text", "text": "(b) In the cost-per-sample setting, at each time the algorithm must choose whether to pay a cost and obtain a new function evaluation, or to stop and return some previously-observed point. In this setting, we add the total sum of costs at termination time to the regret. ", "page_idx": 1}, {"type": "text", "text": "Note that the cost function $c:X\\to\\mathbb{R}_{+}$ can be constant, which we term uniform costs. In this case, (a) reduces to standard black-box optimization with a finite time horizon, and (b) reduces to a variant of stopping-aware Bayesian optimization. These are not the only possible settings: one can also consider other variants including the almost-sure budget-constrained setting where the algorithm is not allowed to exceed the budget in a strict manner. Since we are interested primarily in the role of costs rather than stopping times in this work, we mostly work with budget constraints throughout this paper, especially almost-sure budget constraints for our empirical results. We will use the cost-per-sample setting as a conceptual framework with which to study budget-constrained settings. ", "page_idx": 1}, {"type": "text", "text": "2.1 Probabilistic models and acquisition functions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Bayesian optimization algorithms for solving various black-box optimization problems work by (i) building a probabilistic model of $f$ \u2014that is, a probability distribution which quantifies what is known about $f$ given the data points $(\\bar{x}_{t},y_{t})_{t=1}^{T}$ seen so far\u2014where $y_{t}=f(x_{t})$ are previous function evaluations\u2014then (ii) using the model and its uncertainty to decide where to evaluate the unknown function next. For an introduction, see Frazier [15] and Garnett [17]. Following standard practice, we work with Gaussian process models [30]. Let $f\\mid x_{1:t},y_{1:t}$ be the respective posterior distribution. ", "page_idx": 2}, {"type": "text", "text": "To decide where to evaluate $f$ next, one uses the model to define a (potentially random) acquisition function $\\alpha_{t}:X\\to\\mathbb{R}$ , which quantifies how promising a particular location is given what is known so far. We then evaluate $f$ at ", "page_idx": 2}, {"type": "equation", "text": "$$\nx_{t+1}=\\operatorname*{arg\\,max}_{x\\in X}\\alpha_{t}(x),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "obtaining an additional data point that is used to reduce uncertainty and further improve the model. ", "page_idx": 2}, {"type": "text", "text": "2.2 Expected improvement per unit cost ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The most popular cost-aware acquisition function is expected improvement per unit cost (EIPC) [33], defined via ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\alpha_{t}^{\\mathrm{EIPC}}(x)=\\frac{\\mathrm{EI}_{f|x_{1:t},y_{1:t}}(x;\\operatorname*{max}_{1\\leq\\tau\\leq t}y_{\\tau})}{c(x)}\\qquad\\mathrm{EI}_{\\psi}(x;y)=\\mathbb{E}\\operatorname*{max}(0,\\psi(x)-y)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where we have written $\\alpha_{t}^{\\mathrm{EIPC}}(\\cdot)$ in terms of the general expected improvement function $\\mathrm{EI}_{\\psi}$ , defined with respect to some random function $\\psi:X\\rightarrow\\mathbb{R}$ , and a comparator point $y$ . With this notation, EIPC can be interpreted as the ratio of the expected improvement, with respect to the current posterior and using the best point seen so far as the comparator, to the cost. ", "page_idx": 2}, {"type": "text", "text": "In the uniform-cost case, where the costs $c(x)\\,=\\,C\\,\\in\\,\\mathbb{R}_{+}$ are constant for all $x$ , this acquisition function reduces to the classical expected improvement $(E I)$ acquisition function $\\alpha_{t}^{\\mathrm{{EI}}}({\\dot{x}})\\;=\\;$ $\\operatorname{EI}_{f|x_{1:t},y_{1:t}}(x;\\operatorname*{max}_{1\\leq\\tau\\leq t}y_{\\tau})$ . In turn, expected improvement can be derived by considering the setup where the unknown function $f$ is randomly sampled from the model\u2019s prior. If we imagine that the optimization process continues for only one more time step before stopping, maximizing expected improvement is the optimal strategy in this one-step-lookahead scenario. ", "page_idx": 2}, {"type": "text", "text": "Since EIPC reduces to EI in the uniform-cost case where $c(x)=C$ , it follows that it chooses the same points whether $C=0.0001$ or $C=1\\,000\\,000$ . This is somewhat peculiar: one might expect that a cost-aware acquisition function should be more risk-averse if costs are high, and vice versa if they are low. Thus, EIPC is perhaps best suited to settings where heterogeneity of costs is the main factor at play: without heterogeneity, one can simply apply standard acquisition function variants [10]. However, even with heterogeneous costs, Astudillo et al. [3] show there exist reasonable problems where EIPC performs arbitrarily worse than the optimal policy in an approximation-ratio sense, due to over-sampling low-value low-cost points. Analogous behavior can occur in multi-fidelity settings, which Wu et al. [40] argue can be especially undesirable in the presence of model-misspecification. ", "page_idx": 2}, {"type": "text", "text": "In spite of this rather negative outlook, EIPC has been shown to work well on many practical problems, is computationally efficient and reliable, and is the standard cost-aware choice in BoTorch [4]. We therefore ask: can one develop a technically-principled and computationally-straightforward alternative with at-least-comparable empirical performance? ", "page_idx": 2}, {"type": "text", "text": "3 The Pandora\u2019s Box Gittins index for Bayesian optimization ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "To develop a cost-aware acquisition function, we study a simplified decision problem that captures key difficulties of the main problem but is tractable enough to yield analytic insights. An analogous strategy is used classically to derive expected improvement, by exactly solving a simplified one-step decision problem. We study a different simplified decision problem, which can also be solved exactly, but where the simplification is spatial rather than temporal in nature. Specifically, we connect Bayesian optimization with the Pandora\u2019s Box problem from economics. To do so, we describe Pandora\u2019s Box in Section 3.1 and its solution in Section 3.2, showing along the way how these ideas can be reinterpreted from the view of Bayesian optimization. We illustrate this in Figure 1. Then, in Section 3.3, we use Pandora\u2019s Box to derive a novel class of cost-aware acquisition functions. ", "page_idx": 2}, {"type": "image", "img_path": "Ouc1F0Sfb7/tmp/20e61663d988a298e19ffc0ea0e4cc89229b00ecaecf4dacef6692ae0bd81111.jpg", "img_caption": ["Figure 1: An illustration of this work\u2019s key idea. We view cost-aware Bayesian optimization as an extension of the Pandora\u2019s Box problem, and derive the cost-aware acquisition function $\\alpha_{t}^{\\mathrm{PBGI}}$ by incorporating the posterior into the Bayesian-optimal Pandora\u2019s Box acquisition function $\\alpha^{\\star}$ . "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "3.1 The Pandora\u2019s Box problem ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The Pandora\u2019s Box problem [39, 19] is a sequential decision-making problem. It begins with a finite set of boxes, which we collect into a set and label $X=\\{1,..,\\bar{N}\\}$ . Each box has a hidden reward, denoted by $f(x)$ , and an inspection cost, denoted by $c(x)$ . The rewards are given by mutually independent random variables whose distributions are known and vary between different boxes. ", "page_idx": 3}, {"type": "text", "text": "The decision-making process starts with a set of closed boxes, and proceeds in discrete time steps. At time $t$ , one can choose to do one of two things: ", "page_idx": 3}, {"type": "text", "text": "1. Open a box $x_{t}\\,\\in\\,X$ . This incurs cost $c(\\boldsymbol{x}_{t})$ , but reveals the exact value $f(x_{t})$ of the reward inside the box, which is drawn using the box\u2019s respective reward distribution. 2. Stop opening new boxes, and take the reward from the best opened box. This ends the decisionmaking process, and yields a terminal reward equal to the maximum value among the boxes opened so far, with the convention that at least one box must be opened. ", "page_idx": 3}, {"type": "text", "text": "The policy\u2019s goal is to maximize the expected net utility, which is the reward of the best open box minus the expected total costs of all boxes opened so far, and is written ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{E}\\operatorname*{max}_{1\\leq t\\leq T}f(x_{t})-\\mathbb{E}\\sum_{t=1}^{T}c(x_{t})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $T$ is a random variable that denotes the number of opened boxes, indicating that the policy terminates at time $T+1$ . ", "page_idx": 3}, {"type": "text", "text": "If we subtract the objective (4) from $\\mathbb{E}\\operatorname{sup}_{x\\in X}f(x)$ , which is constant with respect to the policy, we obtain the sum of the simple regret objective (1) defined in Section 2 and expected total costs. The Pandora\u2019s Box problem is therefore equivalent to a special case of cost-aware black-box optimization, specifically the cost-per-sample variant of Section 2, where (a) the domain $X$ is a finite set, and (b) the objective function $f$ is random, with independent $f(x)$ and $f(\\boldsymbol{x}^{\\prime})$ for $x\\neq x^{\\prime}$ . We will return to this point in the sequel, but first study the Pandora\u2019s Box problem in more detail. ", "page_idx": 3}, {"type": "text", "text": "3.2 Optimally solving Pandora\u2019s Box ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The Pandora\u2019s Box problem gives rise to an explore-exploit tradeoff: a policy must balance the opportunity gained from learning the value of the reward contained inside the box with the cost of opening it. Since the reward distributions are known, this tradeoff is captured within a Markov decision process (MDP). By general MDP theory, there exists an optimal policy describing which box, if any, one should open for a given configuration\u2014we call such a policy Bayesian-optimal. ", "page_idx": 3}, {"type": "text", "text": "This MDP can be solved explicitly, with a remarkably simple solution, first derived in an economics setting by Weitzman [39]. We start by associating with each box $x\\in X$ a number $\\alpha^{\\star}(x)$ known as the Gittins index [19]. Define ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\alpha^{\\star}(x)=g\\qquad\\qquad\\qquad{\\mathrm{~where~}}g{\\mathrm{~solves}}\\qquad\\qquad\\qquad{\\mathrm{~EI}}_{f}(x;g)=c(x)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\operatorname{EI}_{f}(x;y)$ , previously defined in (3) of Section 2, is the expected improvement of $x$ relative to $y$ \u2014the same expression which appeared in the expected improvement acquisition function variants $\\alpha_{t}^{\\mathrm{EI}}$ and $\\alpha_{t}^{\\mathrm{EIPC}}$ . Note that, unlike in those cases, $\\alpha^{\\star}$ is not time-dependent due to the lack of correlations or conditioning. Since $\\operatorname{EI}_{f}(x;g)$ is strictly decreasing in $g$ , the root-finding problem (5) admits a unique solution for every value of $c(x)$ . ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "To understand what $\\alpha^{\\star}(x)$ represents, consider a single closed box $x$ , and suppose there is a second, open box with reward $f^{*}$ . Is opening box $x$ better than taking the reward $f^{*}$ from the open box? This amounts to whether the expected improvement from opening $x$ balances out the opening cost $c(x)$ : one can show that opening $x$ is better if and only if $\\operatorname{EI}_{f}(x;f^{*})>c(x)$ . The value $\\alpha^{\\star}(x)$ tells us how large does the alternative reward $f^{*}$ need to be, for stopping and taking it to be at least as good as opening box $x_{\\mathrm{~\\,~}}$ \u2014a kind of fair value which makes different boxes directly comparable to one another. ", "page_idx": 4}, {"type": "text", "text": "If we decide which box to open via the aforementioned fair values, we obtain the Gittins index policy, which proceeds as follows. At each time $t$ , let $f_{t}^{*}=\\operatorname*{max}_{1\\leq\\tau\\leq t}f(x_{\\tau})$ be the maximum reward among all open boxes, and let $\\boldsymbol{x}_{t}^{*}$ be the box of maximum Gittins index value $\\alpha^{\\star}(x)$ among unopened boxes, with ties broken according to a given ordering. With this notation, using a tie-breaking rule that stops as early as possible\u2014but noting that other tie-breaking rules, including stopping as late as possible, or stopping with some probability, are also valid\u2014we get: ", "page_idx": 4}, {"type": "text", "text": "\u2022 If $f_{t}^{*}<\\alpha^{\\star}(x_{t}^{*})$ , the policy opens box $x^{*}$ .   \n\u2022 If $f_{t}^{*}\\geq\\alpha^{\\star}(x_{t}^{*})$ , the policy stops and receives terminal reward $f_{t}^{*}$ . ", "page_idx": 4}, {"type": "text", "text": "It turns out that opening boxes according to the order determined by their fair values, in the sense above, is not only a good idea, but is outright Bayesian-optimal. We state this formally as follows. ", "page_idx": 4}, {"type": "text", "text": "Theorem 1 (Weitzman [39]). Let $X$ be a finite set, let $f:X\\to\\mathbb{R}$ be a finite-mean random function for which $f(x)$ is independent of $f(\\boldsymbol{x}^{\\prime})$ for $x\\neq x^{\\prime}$ , and let $c:X\\to\\mathbb{R}_{+}$ , without loss of generality, be deterministic. Then, for the cost-per-sample problem, the policy defined by maximizing the Gittins index acquisition function $\\alpha^{\\star}$ with its associated stopping rule is Bayesian-optimal. ", "page_idx": 4}, {"type": "text", "text": "In the language of Bayesian optimization, this means that not only is there an explicit Bayesianoptimal policy for the Pandora\u2019s Box setting, but this policy also takes the form of maximizing an acquisition function. This gives an explicit solution for the cost-per-sample setting, thereby showing Pandora\u2019s Box fits our original goal of finding a simplified decision problem that sheds insights on cost-aware Bayesian optimization. For an alternative proof, see Kleinberg et al. [24], Theorem 1. Using Lagrangian relaxation, we show that the obtained solution carries over to the expected budget-constrained setting. ", "page_idx": 4}, {"type": "text", "text": "Theorem 2. Consider the expected budget-constrained problem, with the assumptions of Theorem 1. Assume the problem is feasible and the constraint is active, namely $\\begin{array}{r}{\\operatorname*{min}_{x\\in X}c(x)<B<\\sum_{x\\in X}c(x)}\\end{array}$ . Then there exists a $\\lambda>0$ and a tie-breaking rule such that the policy defined by ma ximizing the Gittins index acquisition function $\\alpha^{\\star}(\\cdot)$ , defined using costs $\\lambda c(x)$ , is Bayesian-optimal. ", "page_idx": 4}, {"type": "text", "text": "A proof is given in Appendix B. This result extends a special case of Aminian et al. [2], Theorem 1. Compared to that work, we consider only Pandora\u2019s Box, but allow general reward distributions\u2014 including those with infinite support, such as Gaussian rewards. The optimal $\\lambda$ depends on the budget constraint $B$ implicitly via a convex optimization problem given in Appendix B. In budget-constrained problems, we therefore view $\\lambda$ as a hyperparameter, which controls the degree to which the algorithm is risk-averse vs. risk-seeking\u2014precisely what we argued was missing from EIPC in Section 2. ", "page_idx": 4}, {"type": "text", "text": "One can intuitively understand $\\lambda$ using a needle-in-haystack metaphor. Suppose one wants to find the best needle, but can only search a fraction of the haystack in expectation, represented by the budget $B$ . The key phrase is in expectation: if the search is not promising, one can stop early and avoid wasting budget, otherwise one can search more of the haystack. This prompts the question: in what situations should one continue searching? The answer depends on the interplay between the budget $B$ , costs $c$ , and best value $f_{t}^{*}$ seen so far, and is encoded by $\\lambda$ in Theorem 2. Larger $\\lambda$ -values correspond to smaller budgets $B$ , incentivizing one to search less of the haystack. Crucially, the optimal acquisition function $\\alpha^{\\star}$ behaves differently for different $\\lambda$ -values, and therefore for different budgets: roughly, smaller budgets cause $\\alpha^{*}$ to explore less. We will return to this in Section 3.3.3 and Figure 3. ", "page_idx": 4}, {"type": "text", "text": "3.3 An acquisition function class for cost-aware Bayesian optimization ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To adapt $\\alpha^{\\star}$ to the Bayesian optimization setting, we need to handle two differences: (i) $X$ does not need to be discrete, and (ii) a general probabilistic model is used for $f$ . Since Theorem 1 ostensibly ", "page_idx": 4}, {"type": "image", "img_path": "Ouc1F0Sfb7/tmp/3e891f7ee233e86dc01845736389bebef1bcd395fa24054b00e20d007ac508f7.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 2: A Bayesian optimization problem with varying costs on which EIPC has poor performance, inspired by Astudillo et al. [3], Section A. The domain is $X=[-500,500]$ , which we visualize on the subinterval $[-5,5]$ . Left: illustration of the non-uniform prior variance, which is given by a Mat\u00e9rn-5/2 kernel scaled by a narrow bump function. Center: the cost function, which is a narrow bump-shaped function. Right: regret curves for EIPC and PBGI. Legend refers only to regret curves. ", "page_idx": 5}, {"type": "text", "text": "requires $f(x)$ to be independent of $f(\\boldsymbol x^{\\prime})$ for all $x\\neq x^{\\prime}$ , the key question is how to incorporate data and spatial correlations into $\\alpha^{\\star}$ . We propose to do so in the simplest and most obvious way: namely, at each time $t$ , we plug the posterior distribution $f\\mid x_{1:t},y_{1:t}$ in place of $f$ . This yields three variants, depending on the precise cost-aware setting one is interested in: ", "page_idx": 5}, {"type": "text", "text": "1. Budget-constrained: define Pandora\u2019s Box Gittins index (PBGI) acquisition function ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\alpha_{t}^{\\mathrm{PBGI}}(x)=g\\qquad\\qquad\\mathrm{where~}g\\mathrm{~solves}\\qquad\\qquad\\mathrm{EI}_{f|x_{1:t},y_{1:t}}(x;g)=\\lambda c(x)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "and $\\lambda$ is a hyperparameter that should be tuned to match the evaluation budget $B$ . ", "page_idx": 5}, {"type": "text", "text": "2. Cost-per-sample: we can directly apply $\\alpha_{t}^{\\mathrm{PBGI}}$ in this setting as well, but now $\\lambda$ is instead interpreted as unit-conversion factor which ensures costs and rewards have the same units, and the Pandora\u2019s Box stopping rule is used for deciding when to terminate the optimization procedure and return the best observed value. ", "page_idx": 5}, {"type": "text", "text": "3. Adaptive decay: here, we do not have a pre-defined budget or cost-based stopping rule. Define the Pandora\u2019s Box Gittins index with adaptive decay $(P B G I{-}D)$ acquisition function $\\alpha^{\\mathrm{PBGI-D}}(x)$ analogously to $\\alpha^{\\mathrm{PBGI}}(x)$ , but with a time-dependent $\\lambda_{t}$ schedule, set according to the Pandora\u2019s Box stopping rule. Specifically, $\\lambda_{0}$ is initialized to a given value, then set ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\lambda_{\\tau+1}=\\left[\\begin{array}{l l}{\\lambda_{\\tau}}&{\\mathrm{if~}f_{\\tau}^{*}\\geq\\alpha_{\\tau}^{\\mathrm{PBGI}}(x_{\\tau}^{*};\\lambda_{\\tau})}\\\\ {\\lambda_{\\tau}}&{\\mathrm{otherwise}}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\beta>1$ is a decay factor that is used to decrease $\\lambda_{\\tau}$ at any time $\\tau$ when the Pandora\u2019s Box stopping rule triggers. The advantage of this variant is that it can be more robust to different ranges of the policy hyperparameters: we discuss this further in Appendix D.2. ", "page_idx": 5}, {"type": "text", "text": "To understand this acquisition function class, one can think of it via the following approximation: for the general cost-aware Bayesian optimization problem, we (a) correctly incorporate observed data into the prior to obtain the posterior, but then (b) pick new samples according to the rule that would have been Bayesian-optimal if the posterior had no correlations. Said differently, $\\alpha^{\\mathrm{PBGI}}$ arises from exactly solving a simplified dynamic program, where the simplification is of a spatial nature, rather than the usual temporal lookahead. One can therefore expect this acquisition function to work best in situations where correlations are not the decisive factor for determining performance. ", "page_idx": 5}, {"type": "text", "text": "In what problems does this happen? In stationary kernels, correlations encode local dependence. Therefore, one can expect $\\alpha^{\\mathrm{PBGI}}$ to be approximately-optimal in settings where the key decisions involve choosing between different far-away data points. One can intuitively expect this to occur more often in high-dimensional problems, where the volume of the search space is large and most points are far away from each other. We will examine this point empirically in the sequel. ", "page_idx": 5}, {"type": "text", "text": "3.3.1 Computation. To compute $\\alpha^{\\mathrm{PBGI}}$ efficiently, note that $y\\mapsto\\operatorname{EI}_{\\psi}(x;y)$ is monotone. As a result, the value $g$ in (6) can be computed efficiently via a bisection search. In Appendix B.2, we show that (i) its gradient can be computed straightforwardly via an explicit analytical expression without any additional optimization, and (ii) the resulting computational costs are much closer to those of expected improvement than those of expensive multi-step-lookahead-based approaches. ", "page_idx": 5}, {"type": "image", "img_path": "Ouc1F0Sfb7/tmp/4e102db5ed66d6659c39705d47e7b8ee4ea0ab4b5b3c4a67523218283ec8204a.jpg", "img_caption": ["Figure 3: Left: contour plots showing how EI (left) and PBGI (center-left, center-right) depend on the posterior mean and standard deviation at a given point (lighter colors indicate higher values). We see that PBGI values high standard deviation more than EI. Right: PBGI performance across values of $\\lambda$ , under the setup of the Bayesian regret experiment of Section 4 with $d=8$ . We plot the median of $n=256$ samples, along with quartiles to show variability. We see that large $\\lambda$ -values decrease regret sooner, but eventually lose out to smaller $\\lambda$ -values. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "3.3.2 Extension to stochastic and non-automatically-differentiable costs. One can show that Theorem 1 holds in the more general case where the cost function is random, as long as one substitutes $c(x)$ with its mean. The same holds for Theorem 2, as shown in Appendix B.5. In this setting, therefore, stochasticity of $c(x)$ affects performance, but does not change the optimal strategy. Building on these observations, we propose extensions of $\\alpha_{t}^{\\mathrm{PBGI}}$ to the more general setting where costs are modeled using a log-normal process in Appendix B.4. ", "page_idx": 6}, {"type": "text", "text": "3.3.3 Qualitative behavior and comparisons. Compared to cost-unaware acquisition functions such as EI, PBGI can be more risk-averse if costs are large or more risk-seeking if costs are small. In varying-cost budget-constrained settings, this tradeoff is mediated by $\\lambda$ , and the obtained decisions can differ significantly from those of widely-used baselines such as EIPC. In particular, PBGI can make qualitatively different decisions on problems where there is a high-variance point with a large cost, among a set of many low-variance low-cost points. In Figure 2, we adapt the construction of Astudillo et al. [3], Section A into a one-dimensional Bayesian optimization problem with a non-stationary prior, and observe that EIPC indeed has substantially worse performance than PBGI. ", "page_idx": 6}, {"type": "text", "text": "The PBGI acquisition functions depends on $f\\mid x_{1:t},y_{1:t}$ through its mean and standard deviation at each point. We plot this in Figure 3. This shows for large $\\lambda$ that PBGI can resemble EI, whereas for small $\\lambda$ it is nearly linear. Specifically, in the $\\lambda\\to0$ limit, we have ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\alpha_{t}^{\\mathrm{PBGI}}(\\cdot)\\approx\\mu_{t}(\\cdot)+\\sigma_{t}(\\cdot)\\sqrt{2\\log\\frac{\\sigma_{t}(\\cdot)}{\\lambda c(\\cdot)}}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\mu_{t}$ and $\\sigma_{t}$ are the mean and standard deviation of $f\\mid x_{1:t},y_{1:t}$ . This expression is similar to the upper confidence bound (UCB) acquisition function whose dependence is exactly linear, with heterogeneous learning rate parameter set to \u03b7t = $\\begin{array}{r}{\\eta_{t}\\,=\\,\\sqrt{2\\log\\frac{\\sigma_{t}(x)}{\\lambda c(x)}}}\\end{array}$ . A derivation is given in Appendix B.3. For small $\\lambda$ , one can thus view PBGI as giving a way to automatically tune UCB\u2019s confidence parameter in a careful way depending on $c(x)$ . ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We now empirically evaluate the Gittins-index-based acquisition function on cost-aware problems. We also evaluate on the same problems with a spatially-constant cost function, a setting we term uniform costs\u2014this facilitates comparisons with classical, cost-unaware baselines. In both cases, mirroring practical settings, we work with a deterministic, algorithm-independent evaluation budget. ", "page_idx": 6}, {"type": "text", "text": "We implement all methods in BoTorch [4] using Mat\u00e9rn Gaussian processes with smoothness $\\nu=5/2$ . For Bayesian regret experiments where objectives are sampled from the prior, we fix the length scale to be $\\kappa=\\bar{10}^{-1}$ for both the prior and the posterior. For synthetic and empirical experiments, we apply maximum marginal likelihood optimization to dynamically adjust the length scale every iteration. To ensure that our results are not sensitive to these and other hyperparameter choices, all experiments were repeated with alternatives given in Appendix D.4. Each experiment was repeated for 16 seeds to assess variability, unless stated otherwise. Experimental details are in Appendix C. ", "page_idx": 6}, {"type": "image", "img_path": "Ouc1F0Sfb7/tmp/60f5192b612924ed1e61d265f833e57bc189a6af143c8ed204954418231c5ad6.jpg", "img_caption": ["Figure 4: Bayesian regret curves, shown using medians, as well as quartiles to indicate experiment variability. We see in the cost-aware setting that both PBGI variants exhibit comparable performance to baselines for $d=8$ , and decisively outperform baselines in $d=16$ and $d=32$ . This behavior is roughly-mirrored in the uniform-cost setting, with two notable distinctions: (a) UCB also exhibits strong performance for $d=16$ matching PBGI and PBGI-D, and (b) all methods perform comparably to random search for $d=32$ under uniform costs. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "We evaluate both PBGI variants from Section 3.3, namely $\\alpha^{\\mathrm{PBGI}}$ with $\\lambda=10^{-4}$ , and $\\alpha^{\\mathrm{PBGI-D}}$ with $\\lambda_{0}=10^{-1}$ and $\\beta=2$ . To assure ourselves that performance differences are not primarily due to tuning, we deliberately use the same $\\lambda$ -values for PBGI and the same $(\\lambda_{0},\\beta)$ -values for PBGI-D on all problems. Comparisons with other $(\\lambda_{0},\\beta)$ -values can be found in Appendix D.2. ", "page_idx": 7}, {"type": "text", "text": "For varying-cost problems\u2014that is, those with spatially non-constant cost\u2014we compare with expected improvement per unit cost (EIPC), multi-fidelity max-value entropy search (MFMES) [35] and budgeted multi-step expected improvement (BMSEI), which was proposed by Astudillo et al. [3] and has state-of-the-art cost-aware performance. For uniform-cost problems\u2014that is, those with constant costs\u2014we compare with expected improvement $(E I)$ , Thompson sampling (TS), upper confidence bound (UCB) with a time-dependent schedule [34], max-value entropy search (MES) [37], knowledge gradient $(K G)$ [16], and multi-step expected improvement (MSEI) [23]. We choose these because (i) they are standard, and (ii) acquisition function optimization succeeds for them on our problems, reducing confounding. ", "page_idx": 7}, {"type": "text", "text": "4.1 Bayesian regret ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "For our first experiment, we examine how well the proposed acquisition functions perform on random functions sampled from the prior. To quantify the effect of problem difficulty, we vary the dimension of the domain $X=[0,1]^{\\hat{d}}$ , and consider $d\\in\\{8,16,32\\}$ . Results, in terms of empirical regret curves and their associated quartiles, are shown in Figure 4. Additional results for $d=4$ , demonstrating comparable performance between both PBGI variants and all baselines, as well as sensitivity comparisons involving the Gaussian process prior with varying kernel hyperparameters such as different smoothness values and length scales, are included in Appendix C. ", "page_idx": 7}, {"type": "text", "text": "In the low-dimensional case of $d=8$ , most uniform-cost and cost-aware approaches achieve similar performance. Once we increase dimension to $d=16$ , we see bigger differences: here, both PBGI variants achieve a modest improvement compared to expected improvement per unit cost. Strikingly, both PBGI variants are also competitive in the uniform-cost setting\u2014in spite of being designed for cost-aware problems. This can be explained via the curse of dimensionality: as dimension increases, the problem begins to look more like the uncorrelated Pandora\u2019s Box problem where using Gittins index is Bayesian-optimal. Eventually, however, the problem becomes too difficult for meaningful progress to be made within our computational budget, as seen for the uniform-cost problem with $d=32$ , where no method outperforms random search. ", "page_idx": 7}, {"type": "image", "img_path": "Ouc1F0Sfb7/tmp/2d23eaeba178151798207a7c3ee98c4d0d1b3fe0765ee87cecb7bf371e21f01f.jpg", "img_caption": ["Figure 5: Synthetic benchmark regret curves, shown using medians, as well as quartiles to assess variability. All objective functions are defined with dimension $d=16$ . We see in the cost-aware setting that PBGI and PBGI-D perform strongest on the heavily-multimodal Ackley function, matching the non-myopic BMSEI baseline. On the Levy and unimodal Rosenbrock function, PBGI-D instead matches\u2014and for some cost budgets outperforms\u2014the EIPC baseline, significantly outperforming BMSEI. Uniform-cost results are similar: PBGI performs well on Ackley and Levy, but is outperformed by most baselines and PBGI-D on Rosenbrock. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "4.2 Synthetic benchmarks ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Next, we consider standard synthetic global optimization benchmark functions. To represent a variety of geometric properties, we examine the Ackley, Levy, and Rosenbrock functions. A visualization of the two-dimensional versions of these functions is given in Appendix A. ", "page_idx": 8}, {"type": "text", "text": "Figure 5 presents results for $d=16$ . Additional results for $d=4,8$ showing that PBGI and all baselines perform similar, are in Appendix C. We see that the behavior of different acquisition functions varies according to the the function. On the Ackley function, PBGI and PBGI-D outperform most baselines, except for the non-myopic BMSEI policy in the cost-aware setting. In contrast, on the Levy function, PBGI-D only outperforms the EIPC baseline on small-enough cost horizons, and PBGI is worse than PBGI-D on cost-aware problems: the same also holds for the BMSEI baseline, indicating that using multi-step lookahead actually reduces performance here\u2014we will return to this momentarily in the context of the Rosenbrock function. We conclude that PBGI can in principle offer stronger performance than PBGI-D, as long as $\\lambda$ is not-too-suboptimal, while PBGI-D tends to be less-performant but is more robust to this hyperparameter choice. ", "page_idx": 8}, {"type": "text", "text": "We also examine performance on the unimodal, banana-shaped Rosenbrock-function. In this case, PBGI-D performs the strongest, matching expected improvement variants, while outperforming PBGI and multi-step lookahead baselines. This can intuitively be explained by the one-step optimality of expected improvement, which better-exploits the unimodal objective, while PBGI and multi-stepbased acquisition functions are more conservative and therefore require different $\\lambda$ -tuning to perform best. We conclude that PBGI-D may be a better choice in settings where there is a potential mismatch between the objective and the prior in terms of unimodality, as this can be counteracted in part by its decay behavior, particularly when compared to PBGI. ", "page_idx": 8}, {"type": "image", "img_path": "Ouc1F0Sfb7/tmp/01fc1099d41e4e66ab948beace5ec6265df0d8878a71d1435e310fad01dbf24b.jpg", "img_caption": ["Figure 6: Empirical benchmark regret curves, shown using medians, as well as quartiles to show variability. We see in both the cost-aware and uniform-cost settings that PBGI exhibits stronger performance on the Pest Control and Lunar Lander problems, while PBGI-D together with the EI and EIPC baselines performs strongly on the Robot Pushing problem. Note that in the cost-aware variant of Robot Pushing, the non-myopic BMSEI baseline performs poorly, potentially mirroring the behavior previously seen on the unimodal Rosenbrock function in Figure 5. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "4.3 Empirical objectives ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Finally, we benchmark PBGI policies on three empirical global optimization problems motivated by applied challenges: Pest Control where $d\\,=\\,25$ [29], Lunar Lander where $d\\,=\\,12$ [14], and Robot Pushing where $d\\,=\\,14$ [37]. Detailed descriptions of these problems and associated cost functions are in Appendix C. Note that, for Lunar Lander and Robot Pushing, the cost functions used are not automatically-differentiable. To avoid this challenge and illustrate how our acquisition function can be used when the cost function is unknown, we apply unknown-cost PBGI and baseline variants, where the costs are modeled using a second independent log-Gaussian process: details on this unknown-cost PBGI variant, including its analytic form, are given in Appendix B.4. ", "page_idx": 9}, {"type": "text", "text": "From Figure 6, we see that the PBGI outperforms baselines on Pest Control and Lunar Lander, in both the cost-aware and uniform-cost settings. On the other hand, PBGI performs poorly on Robot Pushing, where instead expected improvement and PBGI-D perform best, and the non-myopic BMSEI baseline performs poorly. This mirrors behavior previously seen on the unimodal Rosenbrock function, from which we suspect unimodality-like behavior may be at play here as well. Note also that the performance gap between PBGI and UCB is substantially bigger here than on the Bayesian regret or synthetic problems: this may be in part because we tune UCB using the schedule of Srinivas et al. [34], which is explicitly designed for the Bayesian regret setting, and may be less-ideal for other objectives. In comparison, PBGI\u2019s tuning works reasonably well on all three problem classes simultaneously. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we introduced a new acquisition function class for cost-aware Bayesian optimization, the Pandora\u2019s Box Gittins index, based on an unexplored connection between Bayesian optimization and the Pandora\u2019s Box problem from economics. We observed promising performance from two variants of this acquisition function class on both cost-aware problems which are the focus of this work, and, additionally, on classical uniform-cost problems. Performance gains tended to be largest on higher-dimensional and multi-modal problems. Our work constitutes a first step toward integrating ideas from Gittins index theory, including insights from generalizations of Pandora\u2019s Box, and related areas such as queueing theory, into Bayesian optimization. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We thank James T. Wilson for his suggestions, which helped us significantly improve the paper\u2019s presentation. AT is grateful to Anastasios Angelopoulos for hosting him at UC Berkeley on November 17th, 2022: though the visit and planned talk that day had to be cancelled last-minute due to labor strikes, the cancellation ultimately resulted in an unexpected chain of events that, months later, led to AT and ZS exchanging ideas about Gaussian processes and Gittins index theory, which ultimately led to this work. PF was partially supported by AFSOR FA9550-20-1-0351. ZS was supported by the NSF under grant numbers CMMI-2307008, DMS-2023528, and DMS-2022448. AT was supported by Cornell University, jointly via the Center for Data Science for Enterprise and Society, the College of Engineering, and the Ann S. Bowers College of Computing and Information Science. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] S. Aalto, U. Ayesta, and R. Righter. On the Gittins Index in the $M/G/1$ Queue. Queueing Systems, 2009. Cited on page 2.   \n[2] M. R. Aminian, V. Manshadi, and R. Niazadeh. Markovian Search with Socially Aware Constraints. Management Science, 2024. Cited on pages 5, 22.   \n[3] R. Astudillo, D. Jiang, M. Balandat, E. Bakshy, and P. I. Frazier. Multi-step Budgeted Bayesian Optimization with Unknown Evaluation Costs. Advances in Neural Information Processing Systems, 2021. Cited on pages 1, 3, 6\u20138, 14, 16, 25, 27, 28.   \n[4] M. Balandat, B. Karrer, D. Jiang, S. Daulton, B. Letham, A. G. Wilson, and E. Bakshy. BoTorch: A Framework for Efficient Monte-Carlo Bayesian Optimization. Advances in Neural Information Processing Systems, 2020. Cited on pages 3, 7, 25.   \n[5] T. Ba\u00b8sar and P. Bernhard. $H^{\\infty}$ -optimal Control and Related Minimax Design Problems: A Dynamic Game Approach. Springer, 2008. Cited on page 24.   \n[6] S. Belakaria, J. R. Doppa, N. Fusi, and R. Sheth. Bayesian Optimization over Iterative Learners with Structured Responses: A Budget-aware Planning Approach. Artificial Intelligence and Statistics, 2023. Cited on page 1.   \n[7] D. P. Bertsekas. Nonlinear Programming. Athena Scientific, 1999. Cited on page 24.   \n[8] J. F. Bonnans and A. Shapiro. Perturbation Analysis of Optimization Problems. Springer, 2013. Cited on page 24.   \n[9] S. P. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004. Cited on page 23.   \n[10] S. R. Chowdhury and A. Gopalan. On Kernelized Multi-armed Bandits. International Conference on Machine Learning, 2017. Cited on page 3.   \n[11] J. M. Danskin. The Theory of Max-min and its Application to Weapons Allocation Problems. Springer, 1967. Cited on page 24.   \n[12] L. Doval. Whether or Not to Open Pandora\u2019s Box. Journal of Economic Theory, 2018. Cited on page 2.   \n[13] I. Dumitriu, P. Tetali, and P. Winkler. On Playing Golf with Two Balls. SIAM Journal on Discrete Mathematics, 2003. Cited on page 2.   \n[14] D. Eriksson, M. Pearce, J. R. Gardner, R. D. Turner, and M. Poloczek. Scalable Global Optimization via Local Bayesian Optimization. Advances in Neural Information Processing Systems, 2019. Cited on pages 10, 26.   \n[15] P. I. Frazier. Bayesian Optimization, Recent Advances in Optimization and Modeling of Contemporary Problems. 2018. Cited on page 3.   \n[16] P. I. Frazier, W. Powell, and S. Dayanik. The Knowledge-gradient Policy for Correlated Normal Beliefs. INFORMS Journal on Computing, 2009. Cited on page 8.   \n[17] R. Garnett. Bayesian Optimization. 2023. Cited on pages 2, 3.   \n[18] J. C. Gittins. Bandit Processes and Dynamic Allocation Indices. Journal of the Royal Statistical Society, Series B: Statistical Methodology, 1979. Cited on page 14.   \n[19] J. C. Gittins, K. D. Glazebrook, and R. R. Weber. Multi-armed Bandit Allocation Indices. Wiley, 2011. Cited on pages 2, 4.   \n[20] K. D. Glazebrook and J. Ni\u00f1o-Mora. Parallel Scheduling of Multiclass $M/M/m$ Queues: Approximate and Heavy-Traffic Optimization of Achievable Performance. Operations Research, 2001. Cited on page 2.   \n[21] A. Gupta, H. Jiang, Z. Scully, and S. Singla. The Markovian Price of Information. Integer Programming and Combinatorial Optimization, 2019. Cited on page 2.   \n[22] J. M. Hern\u00e1ndez-Lobato, M. W. Hoffman, and Z. Ghahramani. Predictive Entropy Search for Efficient Global Optimization of Black-box Functions. Advances in Neural Information Processing Systems, 2014. Cited on page 25.   \n[23] S. Jiang, D. Jiang, M. Balandat, B. Karrer, J. R. Gardner, and R. Garnett. Efficient Nonmyopic Bayesian Optimization via One-shot Multi-step Trees. Advances in Neural Information Processing Systems, 2020. Cited on pages 1, 8, 25.   \n[24] R. Kleinberg, B. Waggoner, and E. G. Weyl. Descending Price Optimally Coordinates Search. Economics and Computation, 2016. Cited on pages 5, 19, 20.   \n[25] E. H. Lee, D. Eriksson, V. Perrone, and M. Seeger. A Nonmyopic Approach to Cost-constrained Bayesian Optimization. Uncertainty in Artificial Intelligence, 2021. Cited on page 1.   \n[26] Y. L. Li, T. G. J. Rudner, and A. G. Wilson. A Study of Bayesian Neural Network Surrogates for Bayesian Optimization. International Conference on Learning Representations, 2024. Cited on page 26.   \n[27] R. Martinez-Cantin. Bayesian Optimization with Adaptive Kernels for Robot Control. International Conference on Robotics and Automation, 2017. Cited on page 1.   \n[28] P. Milgrom and I. Segal. Envelope Theorems for Arbitrary Choice Sets. Econometrica, 2002. Cited on pages 22, 23.   \n[29] C. Oh, J. Tomczak, E. Gavves, and M. Welling. Combinatorial Bayesian Optimization using the Graph Cartesian Product. Advances in Neural Information Processing Systems, 2019. Cited on pages 10, 26.   \n[30] C. E. Rasmussen and C. K. Williams. Gaussian Processes for Machine Learning. MIT Press, 2006. Cited on pages 3, 28.   \n[31] Z. Scully, I. Grosof, and M. Harchol-Balter. The Gittins Policy is Nearly Optimal in the $M/G/k$ under Extremely General Conditions. Measurement and Analysis of Computing Systems, 2020. Cited on page 2.   \n[32] S. Singla. The Price of Information in Combinatorial Optimization. Symposium on Discrete Algorithms, 2018. Cited on page 2.   \n[33] J. Snoek, H. Larochelle, and R. Adams. Practical Bayesian Optimization of Machine Learning Algorithms. Advances in Neural Information Processing Systems, 2012. Cited on pages 1, 3.   \n[34] N. Srinivas, A. Krause, S. M. Kakade, and M. Seeger. Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design. International Conference on Machine Learning, 2010. Cited on pages 8, 10, 25.   \n[35] S. Takeno, H. Fukuoka, Y. Tsukada, T. Koyama, M. Shiga, I. Takeuchi, and M. Karasuyama. Multi-fidelity Bayesian Optimization with Max-value Entropy Search and its Parallelization. International Conference on Machine Learning, 2020. Cited on page 8.   \n[36] A. Terenin. Gaussian Processes and Statistical Decision-making in Non-Euclidean Spaces. PhD Thesis, Imperial College London, 2022. Cited on page 13.   \n[37] Z. Wang and S. Jegelka. Max-value Entropy Search for Efficient Bayesian Optimization. International Conference on Machine Learning, 2017. Cited on pages 8, 10, 27.   \n[38] R. R. Weber. On the Gittins Index for Multiarmed Bandits. Annals of Applied Probability, 1992. Cited on page 2.   \n[39] M. L. Weitzman. Optimal Search for the Best Alternative. Econometrica, 1979. Cited on pages 2, 4, 5, 14, 16, 19.   \n[40] J. Wu, S. Toscano-Palmerin, P. I. Frazier, and A. G. Wilson. Practical Multi-fidelity Bayesian Optimization for Hyperparameter Tuning. Uncertainty in Artificial Intelligence, 2020. Cited on page 3.   \n[41] X. Yue and R. A. Kontar. Why Non-myopic Bayesian Optimization is Promising and How Far Should We Look-ahead? A Study via Rollout. Artificial Intelligence and Statistics, 2020. Cited on page 1.   \n[42] Y. Zhang, D. W. Apley, and W. Chen. Bayesian Optimization for Materials Design with Mixed Quantitative and Qualitative Variables. Scientific Reports, 2020. Cited on page 1. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "image", "img_path": "Ouc1F0Sfb7/tmp/b1f305c3e0b87684f5cd83be09cd1a341372b2ed31ea216b7468239e0d08ceb4.jpg", "img_caption": ["Figure 7: An illustration of the two-dimensional Ackley, Levy, and Rosenbrock functions.1 From the visual, one can see that these functions differ in terms of multimodality and ridge-regions within the optimization landscape. "], "img_footnote": [], "page_idx": 12}, {"type": "text", "text": "A Illustrations ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Here we provide a set of additional illustrations to aid understanding of our results. ", "page_idx": 12}, {"type": "text", "text": "A.1 Visualization of synthetic benchmark functions ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "To better understand how the behavior of Bayesian optimization algorithms on the three different synthetic benchmark functions might be affected by their geometric shape, Figure 7 provides a visual illustration of their two-dimensional variants.1 This allows us to visually see the multimodality of the Ackley function, multimodality and ridge-like regions in the Levy function, and unimodality of the Rosenbrock function. While we use higher-dimensional versions of these in our experiments, this illustration provides some intuition for what the resulting the optimization landscape might look like, helping contextualize results. ", "page_idx": 12}, {"type": "text", "text": "A.2 Comparison of behaviors between EI and PBGI ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In Figure 8, we see that EI\u2019s maximum occurs at a point near $x\\approx0.365$ , which is located closer to the observed data and is fairly close to the maximum of the posterior mean. Similarly, large- $\\cdot\\lambda$ PBGI\u2019s maximum occurs at a similar point near $x\\approx0.358$ , indicating that the algorithm makes similar decisions in spite of the fact that the acquisition function\u2019s shape is different. In contrast, small- $\\cdot\\lambda$ PBGI\u2019s maximum occurs at a point near $x\\approx0.86$ : it favors a riskier approach, preferring a location where the mean is smaller, but the variance is larger. ", "page_idx": 12}, {"type": "image", "img_path": "Ouc1F0Sfb7/tmp/bb4953932fe8d1e76b95cadf45091515b7709147906fd7143c3e1ba05a68d289.jpg", "img_caption": ["Figure 8: Comparison between three acquisition functions, namely Expected Improvement (EI), Pandora\u2019s Box Gittins Index (PBGI) with a large $\\lambda$ -value of $\\lambda=10^{0}$ , and Pandora\u2019s Box Gittins Index with a small $\\lambda$ -value of $\\lambda=10^{-5}$ . All three are computed using the same posterior distribution with four data points. The maximum is shown as a large dot. "], "img_footnote": [], "page_idx": 12}, {"type": "image", "img_path": "Ouc1F0Sfb7/tmp/a2cd614b47573e204bb88af942eabbdb3e921253f83c22911d7fb03b3a0433c4.jpg", "img_caption": ["Expected Improvement (cost-unaware) Pandora\u2019s Box Gittins Index "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "Figure 9: A Bayesian optimization problem with varying costs on which expected improvement, which ignores the cost function, has poor performance. Like the EIPC example of Figure 2, the construction also mirrors Astudillo et al. [3], Section A. The domain is $X=[-500,500]$ , which we visualize on the subinterval $[-5,5]$ . Left: illustration of the non-uniform prior variance, which is given by a Mat\u00e9rn-5/2 kernel scaled by a narrow bump function. Center: the cost function, which is a narrow bump-shaped function. Right: regret curves. ", "page_idx": 13}, {"type": "text", "text": "This example shows potential differences in behavior between EI and PBGI, and illustrates how these differences are mediated by the parameter $\\lambda$ . ", "page_idx": 13}, {"type": "text", "text": "A.3 An example where expected improvement underperforms ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In Section 3.3.3, we showed a cost-aware Bayesian optimization problem on which expected improvement per unit cost (EIPC) has poor performance. Here, we show that this problem can be modified so that ordinary expected improvement (EI), which ignores the cost function, also has poor performance\u2014a somewhat obvious, but nonetheless important sanity check that we make to ensure that costs play a sufficiently-important role in problems of this class to merit their consideration. This is shown in Figure 9. It is not hard to construct a less-visualization-friendly variant of these problems on which both expected improvement per unit cost and ordinary expected improvement perform poorly, by considering cost functions which are appropriately-weighted sums of bump functions. ", "page_idx": 13}, {"type": "text", "text": "B Theory and calculations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Below, we provide additional ideas to help understand the Pandora\u2019s Box problem and acquisition function that results from its considerations. ", "page_idx": 13}, {"type": "text", "text": "B.1 Additional intuition on Pandora\u2019s Box ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In what follows, we sketch a viewpoint from which one can see the key idea behind why Theorem 1 holds. Rather than considering the full Pandora\u2019s Box problem with a general set of open and closed boxes, consider first the case where there is exactly one closed and one open box. To slightly simplify notation, let $f$ denote the random reward inside the closed box, let $c$ denote the cost of opening the closed box, assumed deterministic, and let $g$ denote the visible reward of the open box. Our possible actions are as follows: ", "page_idx": 13}, {"type": "text", "text": "1. Open the closed box. In this case, we pay a cost of $c$ , but subsequently get to choose between taking the realized value $f$ , or instead taking $g$ from the box that was originally open. In expectation, the total value obtained by taking this action is $\\mathbb{E}(\\operatorname*{max}(f,g))-\\bar{c}$ . 2. Take the reward from the open box. The total value obtained is $g$ . ", "page_idx": 13}, {"type": "text", "text": "We can therefore analytically solve for the optimal policy of this respective Markov decision process: we open the closed box if $\\mathbb{E}(\\operatorname*{max}(f,g))\\ -c\\ \\geq\\ g$ , and take the reward from the open box if $\\mathbb{E}(\\operatorname*{max}(f,g))-c\\leq g$ , with both actions optimal in the case of equality. As consequence, if $g$ is such that both actions are optimal, the same value is obtained no matter whether one chooses to open the box or not. Rewriting the preceding expressions slightly, this occurs when ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{E}\\operatorname*{max}(f-g,0)=c\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where in the case of multiple boxes the left-hand-side becomes the expected improvement function. The insight of Weitzman [39]\u2014and indeed of Gittins [18] in a much more general setting\u2014is that ", "page_idx": 13}, {"type": "text", "text": "one can modify the Pandora\u2019s Box Markov decision process by replacing closed boxes with open boxes whose value $g$ satisfies (9) without changing the optimal policy. As a consequence, the optimal policy is precisely the Gittins index policy of Theorem 1. ", "page_idx": 14}, {"type": "text", "text": "As a final point, note that the assumption that $c$ is deterministic is made without loss of generality: if $c$ is instead stochastic but has finite expectation, the same reasoning applies, but with the value $c$ in (9) replaced with its expected value. We will make use of this in Appendix B.4. ", "page_idx": 14}, {"type": "text", "text": "B.2 Gradient of the PBGI acquisition function ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In most Bayesian optimization setups, gradient-based methods including multi-start stochastic gradient descent, BFGS, and L-BFGS-B, are used to effectively optimize analytical acquisition functions, such as EI and UCB. These methods can also be used to optimize the PBGI acquisition function. To facilitate this, we provide the gradient formula for the PBGI acquisition function. In what follows, mirroring the preceding and following sections, if costs are stochastic then $c(x)$ should be replaced with its respective mean. We also omit time subscripts to ease notation. ", "page_idx": 14}, {"type": "text", "text": "Proposition 3 (Gradient of PBGI). Let $\\mu(x)$ and $\\sigma(x)$ be the mean and standard deviation of the posterior Gaussian process $(f\\mid x_{1:t},y_{1:t})(x)$ . With this notation, the gradient of the acquisition function $\\alpha^{\\mathrm{PBGI}}(x)$ is given by ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\nabla\\alpha^{\\mathrm{PBGI}}(x)=\\nabla\\mu(x)+\\frac{\\phi\\Big(\\frac{\\mu(x)-\\alpha^{\\mathrm{PBGI}}(x)}{\\sigma(x)}\\Big)\\nabla\\sigma(x)-\\lambda\\nabla c(x)}{\\Phi\\Big(\\frac{\\mu(x)-\\alpha^{\\mathrm{PBGI}}(x)}{\\sigma(x)}\\Big)},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\phi$ and $\\Phi$ denote the density and cumulative distribution function of a standard normal distribution, respectively. ", "page_idx": 14}, {"type": "text", "text": "Proof. Recall that when $\\psi(x)\\sim\\mathrm{N}(\\mu(x),\\sigma(x))$ is Gaussian, the expected improvement with respect to the comparator $y$ is given as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname{EI}_{\\psi}(x;y)=(\\mu(x)-y)\\Phi\\!\\left({\\frac{\\mu(x)-y}{\\sigma(x)}}\\right)+\\sigma(x)\\phi\\!\\left({\\frac{\\mu(x)-y}{\\sigma(x)}}\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Next, note by definition of $\\alpha^{\\mathrm{PBGI}}$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname{EI}_{f\\mid x_{1:t},y_{1:t}}(x;\\alpha^{\\operatorname{PBGI}}(x))=\\lambda c(x).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Differentiating this with respect to $x$ on both sides gives ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\nabla\\operatorname{EI}_{f\\mid x_{1:t},y_{1:t}}(x;\\alpha^{\\mathrm{PBGI}}(x))=\\lambda\\nabla c(x).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Applying the product and chain rule to the left-hand-side gives ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla\\mathrm{EI}_{f\\mid x_{1:t},y_{1:t}}(x;\\alpha^{\\mathrm{PBGI}}(x))=(\\nabla\\mu(x)-\\nabla\\alpha^{\\mathrm{PBGI}}(x))\\Phi\\bigg(\\frac{\\mu(x)-\\alpha^{\\mathrm{PBGI}}(x)}{\\sigma(x)}\\bigg)}\\\\ &{\\qquad+\\left(\\mu(x)-\\alpha^{\\mathrm{PBGI}}(x)\\right)\\!\\phi\\bigg(\\!\\frac{\\mu(x)-\\alpha^{\\mathrm{PBGI}}(x)}{\\sigma(x)}\\!\\bigg)\\nabla\\bigg(\\!\\frac{\\mu(x)-\\alpha^{\\mathrm{PBGI}}(x)}{\\sigma(x)}\\!\\bigg)}\\\\ &{\\qquad+\\nabla\\sigma(x)\\phi\\bigg(\\!\\frac{\\mu(x)-\\alpha^{\\mathrm{PBGI}}(x)}{\\sigma(x)}\\!\\bigg)}\\\\ &{\\qquad+\\sigma(x)\\phi^{\\prime}\\bigg(\\!\\frac{\\mu(x)-\\alpha^{\\mathrm{PBGI}}(x)}{\\sigma(x)}\\!\\bigg)\\nabla\\bigg(\\!\\frac{\\mu(x)-\\alpha^{\\mathrm{PBGI}}(x)}{\\sigma(x)}\\!\\bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Recall the identity for the derivative of the Gaussian density, namely ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\phi^{\\prime}(x)=-x\\phi(x).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Applying this identity to (17) gives ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sigma(x)\\phi^{\\prime}\\bigg(\\frac{\\mu(x)-\\alpha^{\\mathrm{PBGI}}(x)}{\\sigma(x)}\\bigg)\\nabla\\bigg(\\frac{\\mu(x)-\\alpha^{\\mathrm{PBGI}}(x)}{\\sigma(x)}\\bigg)}\\\\ &{\\qquad=-(\\mu(x)-\\alpha^{\\mathrm{PBGI}}(x))\\phi\\bigg(\\frac{\\mu(x)-\\alpha^{\\mathrm{PBGI}}(x)}{\\sigma(x)}\\bigg)\\nabla\\bigg(\\frac{\\mu(x)-\\alpha^{\\mathrm{PBGI}}(x)}{\\sigma(x)}\\bigg)}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which is equal to the negation of (15), hence (15) and (17) cancel: we get ", "page_idx": 15}, {"type": "equation", "text": "$$\n(\\nabla\\mu(x)-\\nabla\\alpha^{\\mathrm{PBGI}}(x))\\Phi\\bigg(\\frac{\\mu(x)-\\alpha^{\\mathrm{PBGI}}(x)}{\\sigma(x)}\\bigg)+\\nabla\\sigma(x)\\phi\\bigg(\\frac{\\mu(x)-\\alpha^{\\mathrm{PBGI}}(x)}{\\sigma(x)}\\bigg)=\\lambda\\nabla c(x).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Rearranging this gives the expression in the claim. ", "page_idx": 15}, {"type": "text", "text": "B.3 Small-cost limit of PBGI acquisition function ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We now derive the small- $\\cdot\\lambda$ limiting expression for PBGI which was presented in Section 3.3.3. According to (11) and (12), we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\alpha_{t}^{\\mathrm{PBGI}}(\\cdot)=\\mu_{t}(\\cdot)-\\sigma_{t}(\\cdot)u(\\cdot)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\mu_{t}$ and $\\sigma_{t}$ are the mean and the standard deviation of $f\\mid x_{1:t},y_{1:t}$ , and $u(\\cdot)$ is the solution of ", "page_idx": 15}, {"type": "equation", "text": "$$\nu(\\cdot)\\Phi(u(\\cdot))+\\phi(u(\\cdot))=\\frac{\\lambda c(\\cdot)}{\\sigma_{t}(\\cdot)}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "When $\\lambda\\to0$ , the solution satisfies $u(\\cdot)\\rightarrow-\\infty$ . This implies that both $\\Phi(u(\\cdot))\\to0$ and $\\phi(u(\\cdot))\\rightarrow$ 0. Since $\\phi(u(\\cdot))$ dominates $\\Phi(u(\\cdot))$ , we can approximate the solution $u(\\cdot)$ by $\\begin{array}{r}{\\phi(u(\\cdot))\\approx\\frac{\\lambda c(\\cdot)}{\\sigma_{t}(\\cdot)}}\\end{array}$ \u03bbc(\u00b7). By substituting the probability density function of a standard normal distribution for $\\phi$ , we obtain ", "page_idx": 15}, {"type": "equation", "text": "$$\nu(\\cdot)\\approx-\\sqrt{2\\log\\biggl(\\frac{\\sigma_{t}(\\cdot)}{\\lambda c(\\cdot)}\\biggr)}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Thus, we can conclude that in the limit $\\lambda\\to0$ , the Gittins index becomes ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\alpha_{t}^{\\mathrm{PBGI}}(x;\\lambda)\\approx\\mu_{t}(x)+\\sigma_{t}(x)\\sqrt{2\\log\\left(\\frac{\\sigma_{t}(x)}{\\lambda c(x)}\\right)}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "As consequence, we expect PBGI-D, whose $\\lambda$ -parameter eventually decays to zero, to potentially inherit consistency and other properties known for UCB. Specifically, since PBGI-D decreases the value of $\\lambda$ to zero over time by dividing it with a constant every time the Gittins stopping rule triggers, its behavior should eventually be well-described by the above limit. As a result of this limit, unexplored regions where $\\sigma({\\boldsymbol{x}})\\gg0$ should eventually have acquisition values that are larger than regions where $\\sigma(x)\\approx0$ . We believe this should hold regardless of whether the costs are known\u2014our main focus\u2014or unknown, especially if the cost function $c(x)$ is uniformly bounded above and below. This because (25)\u2019s dominant term in the $\\lambda\\to0$ limit is $\\sigma(x){\\sqrt{2\\log{\\frac{1}{\\lambda}}}}$ , which does not depend on the cost $c(x)$ . ", "page_idx": 15}, {"type": "text", "text": "B.4 Closed-form expression for the PBGI unknown-cost variant ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In the Lunar Lander and Robot Pushing empirical examples of Section 4, the cost function does not admit an analytic, automatically-differentiable form, and can only be evaluated in a black-box manner. To handle this, we model the logarithm of the costs as a Gaussian process, and condition this process on the costs observed at locations evaluated so far. This mirrors how unknown costs are handled in other acquisition functions, such as the budgeted multi-step expected improvement acquisition function of Astudillo et al. [3]. ", "page_idx": 15}, {"type": "text", "text": "From the viewpoint of the Pandora\u2019s Box problem, stochastic costs make little difference: following the discussion in Appendix B.1, the optimality results of Weitzman [39] continue to hold even if costs are stochastic, so long as the costs in the formula for $\\alpha^{\\star}$ are replaced with expected costs. Mirroring this, if we plug in the mean of a log-normal random variable into the definition of $\\alpha^{\\mathrm{PBGI}}$ , we obtain the following acquisition function. ", "page_idx": 15}, {"type": "text", "text": "Definition 4. Let $c(x)$ be log-normal for all x. For a dataset $(x_{1},y_{1}),..,(x_{t},y_{t})$ , let $\\mu_{\\ln c}$ and $\\sigma_{\\ln c}$ be the posterior mean and posterior standard deviation of the log-costs. Define the unknown-cost Pandora\u2019s Box Gittins index acquisition function by ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\alpha_{t}^{\\mathrm{PBGI.U}}(x)=g\\,\\quad\\,w h e r e\\,\\,g\\,s o l v e s\\quad\\mathrm{EI}_{f\\mid x_{1:t},y_{1:t}}(x;g)=\\lambda\\exp\\biggl(\\mu_{\\ln c}(x)+\\frac{\\sigma_{\\ln c}(x)^{2}}{2}\\biggr).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The interpretation of $\\lambda$ , namely as a hyperparameter that determines the expected budget the algorithm will use before reaching its respective stopping time, is the same as in the known-cost setting. One can define cost-per-sample and decay variants in the same manner as well. ", "page_idx": 15}, {"type": "text", "text": "We now prove Theorem 2. In what follows, recall that the assumptions of Theorem 1 are (i) $X$ is discrete, (ii) $\\mathbb{E}\\left|f(x)\\right|<\\infty$ for all $x$ , (iii) $f(x)$ and $f(\\boldsymbol{x}^{\\prime})$ are independent for $x\\neq x^{\\prime}$ , and (iv) the budget satisfies $B>\\operatorname*{min}_{x\\in X}c(x)$ . Further, Theorem 1 was stated for deterministic costs: more generally, we allow for stochastic costs satisfying $0<\\mathbb{E}\\,c(x)<\\infty$ , and in such cases instead define $\\alpha^{\\star}$ using the expected costs. ", "page_idx": 16}, {"type": "text", "text": "We begin by defining the Markov decision process (MDP) under study and stating the properties of it that we will use. Define: ", "page_idx": 16}, {"type": "text", "text": "1. States: let $S=S^{\\circ}\\cup\\partial S$ be the union of two disjoint sets, namely the set $S^{\\circ}$ of non-terminal states and set $\\partial\\cal S$ of terminal states. These are described below: (a) Non-terminal states: let $S^{\\circ}$ consist of all finite sequences of length $|X|$ taking values in $\\mathbb{R}\\cup\\{\\boxtimes\\}$ , where numbers represent the reward inside an open box, and $\\boxtimes$ represents a closed box, along with terminal states described below. (b) Terminal states: let $\\partial S=\\mathbb{R}$ represent the reward of the box chosen by the learner.   \n2. Actions: let $A=X$ , where actions represent either opening a closed box, or taking a reward from an open box.   \n3. Costs: define the non-terminal cost function $c:S\\times A\\rightarrow\\mathbb{R}$ by $c(s,a)=c(a)$ .   \n4. Rewards: define the terminal reward function $\\partial r:\\partial S\\rightarrow\\mathbb{R}$ by $\\partial r(s)=s$ .   \n5. Transition kernel: define a Markov transition kernel such that for a non-terminal state $s$ and an action $a$ : (a) If box $i$ is closed, that is, $s_{i}=\\boxtimes$ , and $a$ corresponds to opening this closed box, then the MDP transitions to a new state $s^{\\prime}$ , where $s_{i}^{\\prime}$ represents a random draw of the value in box $i$ and all other components $s_{j}^{\\prime}$ for $j\\neq i$ remains the same as $s_{j}$ ; (b) If box $i$ is open, that is, $s_{i}\\in\\mathbb{R}$ , and $a$ corresponds to taking the reward $s_{i}$ from this open box, then the MDP deterministically transitions to a terminal state $s^{\\prime}=s_{i}$ . ", "page_idx": 16}, {"type": "text", "text": "This defines a class of time-homogeneous undiscounted Markov decision processes, parameterized by the reward distribution $f$ , of bounded expected value, and cost function $c$ . We consider Markov policies, which are probability kernels mapping states to probability measures over actions. For this class of MDPs, all such policies are guaranteed to terminate in finite time, because at most one can open all the boxes before being forced to select one and thereby enter a terminal state. As consequence, by standard MDP theory: ", "page_idx": 16}, {"type": "text", "text": "1. The value function $V^{(\\pi,c)}:S\\rightarrow\\mathbb{R}$ is well-defined, where the superscripts denote the policy   \nand the cost with respect to which the MDP is defined.   \n2. The optimal value function $V^{(*,c)}:S\\rightarrow\\mathbb{R}$ is also well-defined.   \n3. There exists an optimal policy $\\pi^{(*,c)}$ which achieves the optimal value $V^{(*,c)}$ .   \n4. The map $\\mathbb{R}_{+}^{|X|}\\to\\mathbb{R}$ defined by $c\\mapsto V^{(\\pi,c)}$ is affine for all $\\pi$ .   \n5. The map $\\mathbb{R}_{+}^{|X|}\\to\\mathbb{R}$ defined by $c\\mapsto V^{(*,c)}$ is convex, since it is a supremum of affine functions. ", "page_idx": 16}, {"type": "text", "text": "We immediately note that this formulation extends to cover two variations of interest: ", "page_idx": 16}, {"type": "text", "text": "1. Stochastic costs. One can handle this by replacing $c$ with a probability kernel. In this case, letting $m$ be the mean costs, we have $\\bar{V^{(\\pi,c)}}^{*}\\!=V^{(\\bar{\\pi},m)}$ . Using this, we henceforth work with deterministic costs without loss of generality. 2. Simple regret as a terminal reward. One can also consider an MDP with a stochastic terminal reward which subtracts the best-in-hindsight term $\\operatorname{sup}_{x\\in X}f(x)$ from the terminal rewards $\\partial r$ given above\u2014this gives an objective equal to simple regret up to a minus sign. Since this only changes the rewards up to a random constant which is independent of the chosen actions, by linearity of expectation, the value function of this modified MDP is equal to those of our MDP up to a constant. We therefore omit this term without loss of generality. ", "page_idx": 16}, {"type": "text", "text": "The expected budget-constrained setting does not directly incorporate costs into the MDP itself: more precisely, it takes $c(s,a)=0$ within the MDP formulation, which means the value function of this modified MDP is $V^{(\\pi,0)}$ . Instead, costs are incorporated as a constraint set on the policy class one considers. With this notation, the values of the optimization problems for Bayesian-optimal policy in the expected budget-constrained and cost-per-sample problems are ", "page_idx": 17}, {"type": "equation", "text": "$$\nV_{\\mathrm{ebc}}^{(\\ast,c)}=\\operatorname*{sup}_{\\pi\\in\\Pi_{B}^{(c)}}V^{(\\pi,0)}\\qquad\\qquad\\qquad\\qquad V^{(\\ast,c)}=\\operatorname*{sup}_{\\pi\\in\\Pi}V^{(\\pi,c)}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\Pi$ is the set of all Markov policies, and the feasible set for the expected budget-constrained problem is ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\Pi_{B}^{(c)}=\\left\\{\\pi\\in\\Pi:\\mathbb{E}\\sum_{t=1}^{T^{(\\pi)}}c(x_{t})\\leq B\\right\\}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and $T^{(\\pi)}$ is policy $\\pi$ \u2019s stopping time, that is ", "page_idx": 17}, {"type": "equation", "text": "$$\nT^{(\\pi)}=\\operatorname*{inf}\\{t\\geq1:s_{t}\\in\\partial S\\}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which is bounded above by $|X|<\\infty$ . This defines the optimization problems under study. Define the set of maximizers ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\Pi^{(*,c)}=\\Big\\{\\pi\\in\\Pi:V^{(\\pi,c)}=V^{(*,c)}\\Big\\}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which is non-empty, since, as said above, by MDP theory the supremum defining $V^{(*,c)}$ is achieved. Define also the set of feasible policies which satisfy the constraints in a tight manner, namely ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\Pi_{B,\\mathrm{eq}}^{(c)}=\\left\\{\\pi\\in\\Pi_{B}^{(c)}:\\mathbb{E}\\sum_{t=1}^{T^{(\\pi)}}c(x_{t})=B\\right\\}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We are now ready to prove the claim in question. For this, we employ a Lagrangian duality argument: loosely speaking, this will reveal the cost-per-sample $\\lambda$ to be the Lagrange multipliers associated with the expected budget constraint. We prove the main claim via a series of lemmas, starting from handling the Lagrange-multiplier-part of the argument. ", "page_idx": 17}, {"type": "text", "text": "Lemma 5. Define the function ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{A}:[0,\\infty)\\to\\mathbb{R}\\qquad\\qquad\\qquad\\mathcal{A}(\\lambda)=V^{(*,\\lambda c)}+\\lambda B.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Suppose that the infimum of $\\boldsymbol{\\mathcal{A}}$ is achieved, and denote it by $\\lambda_{B}^{*}\\in[0,\\infty)$ . Suppose further that there exists an optimal policy for the cost-per-sample MDP for which the expected budget constraint is tight, namely \u03c0  \u2208\u03a0(\u2217,\u03bb\u2217Bc) \u2229\u03a0(Bc,)eq. Then we have ", "page_idx": 17}, {"type": "equation", "text": "$$\nV_{\\mathrm{ebc}}^{(\\widehat{\\pi},c)}=V_{\\mathrm{ebc}}^{(\\ast,c)}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. We start by expressing the expected budget-constrained optimization problem in an unconstrained form via Lagrange multipliers, obtaining ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{V_{\\mathrm{ebc}}^{(*,c)}=}&{\\underset{\\pi\\in\\Pi_{B}^{(c)}}{\\operatorname*{sup}}V^{(\\pi,0)}=\\underset{\\pi\\in\\Pi}{\\operatorname*{sup}}\\,\\underset{\\lambda\\geq0}{\\operatorname*{inf}}\\left(V^{(\\pi,0)}-\\lambda\\left(\\mathbb{E}\\displaystyle\\sum_{t=1}^{T^{(\\pi)}}c(x_{t})-B\\right)\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\underset{\\lambda\\geq0}{\\operatorname*{inf}}\\,\\underset{\\pi\\in\\Pi}{\\operatorname*{sup}}\\left(V^{(\\pi,0)}-\\lambda\\left(\\mathbb{E}\\displaystyle\\sum_{t=1}^{T^{(\\pi)}}c(x_{t})-B\\right)\\right)}\\\\ &{\\qquad\\qquad\\qquad=\\underset{\\lambda\\geq0}{\\operatorname*{inf}}\\left(V^{(*,\\lambda c)}+\\lambda B\\right)=\\underset{\\lambda\\geq0}{\\operatorname*{inf}}\\,\\mathcal{A}(\\lambda)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the second line follows from the Lemma 10 since the respective Lagrangian equals $V^{(\\pi,\\lambda c)}$ up to a constant, and the third line follows by definition of $V^{(*,\\lambda c)}$ is by definition the terminal reward minus the cumulative costs. ", "page_idx": 17}, {"type": "text", "text": "Now, suppose the infimum of A is achieved, and let \u03bb\u2217B \u2208[0, \u221e) be any minimizer. Then V e(b\u2217c,\u03bb\u2217Bc)= $V^{(*,\\lambda_{B}^{*}c)}+\\lambda_{B}^{*}B$ . Using this, and the fact that the policy $\\widehat{\\pi}$ by assumption achieves the supremum over $\\operatorname*{sup}_{\\pi\\in\\Pi}V^{(\\pi,\\lambda_{B}^{*}c)}$ and satisfies $\\widehat{\\pi}\\in\\Pi_{B}^{(c)}$ , we get ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\pi\\in\\Pi_{B}^{(c)}}V^{(\\pi,0)}=\\operatorname*{sup}_{\\pi\\in\\Pi}V^{(\\pi,\\lambda_{B}^{*}c)}+\\lambda_{B}^{*}B=\\operatorname*{sup}_{\\pi\\in\\Pi_{B}^{(c)}}V^{(\\pi,\\lambda_{B}^{*}c)}+\\lambda_{B}^{*}B.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Thus, the optimization objectives, defining the expected budget-constrained problem and the cost-persample problem with costs $\\lambda_{B}^{*}c$ , are equal up to a constant. Therefore, their minimizer sets coincide, and the claim follows. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "Lemma 5 reveals that as long as the optimization problems arising from Lagrange multipliers are achieved, and the resulting policy $\\widehat{\\pi}$ is feasible, the expected budget-constrained problem will admit the same optimum as its associated cost-per-sample problem. By MDP theory, we know the supremum over $\\Pi$ is achieved: we now show the infimum involving $\\lambda$ is achieved as well, which essentially amounts to ruling out arbitrarily-large $\\lambda$ values. ", "page_idx": 18}, {"type": "text", "text": "Lemma 6. The map $\\boldsymbol{\\mathcal{A}}$ is convex. Moreover, the infimum $\\operatorname*{inf}_{\\lambda\\geq0}A(\\lambda)$ is achieved. ", "page_idx": 18}, {"type": "text", "text": "Proof. First, note that convexity follows straightforwardly from convexity of $c\\mapsto V^{(*,c)}$ and the fact that the sum of convex functions is convex. Next, since the feasible set is $\\lambda\\in[0,\\infty)$ and the objective is convex, either the infimum is achieved, or the objective is non-increasing. We prove the latter property cannot hold. For this, first consider the policy $\\pi^{\\prime}$ that deterministically opens some box $x^{\\prime}\\in X$ whose cost is $c(\\boldsymbol{x}^{\\prime})<B$ \u2014note that our assumptions guarantee the existence of at least one such box\u2014and selects the value in it. Therefore if we define $\\mathcal{A}^{\\prime}(\\lambda)=V^{(\\pi^{\\prime},\\lambda c)}+\\lambda B$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{A}^{\\prime}(\\lambda)=\\;V^{(\\pi^{\\prime},\\lambda c)}+\\lambda B\\le V^{(\\ast,\\lambda c)}+\\lambda B=\\mathcal{A}(\\lambda).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "At the same time, we have $V^{(\\pi^{\\prime},\\lambda c)}=\\mathbb{E}\\,f(x^{\\prime})-\\lambda c(x^{\\prime})$ therefore ", "page_idx": 18}, {"type": "equation", "text": "$$\nA^{\\prime}(\\lambda)=\\mathbb{E}\\,f(x^{\\prime})-\\lambda\\underbrace{(c(x^{\\prime})-B)}_{\\mathrm{negative}}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which means the map $\\mathcal{A^{\\prime}}$ is affine and strictly increasing with respect to $\\lambda$ . Thus, the map $\\boldsymbol{\\mathcal{A}}$ is lower-bounded by a strictly increasing affine function, and therefore cannot be non-increasing. We conclude that the infimum is achieved. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "The next part is to show that the optimal cost-per-sample policy is feasible for the expected budgetconstrained problem. Then to do so, we need to verify a certain convergence criterion, given below. In the following, note that $\\lambda^{\\prime}$ \u2014which can be negative\u2014is never used in the definition of any optimal policy, only to compute the value of a given policy, which still makes sense even with negative costs. ", "page_idx": 18}, {"type": "text", "text": "Lemma 7. For any monotone sequence $\\lambda_{n}\\to\\lambda$ where $\\lambda_{n}>0$ and $\\lambda>0$ , and any $\\lambda^{\\prime}\\in\\mathbb{R}$ , there exist policies $\\pi_{n}^{*}\\in\\mathrm{{II}}^{(*,\\lambda_{n}c)}$ and $\\bar{\\pi^{*}}\\in\\Pi^{(*,\\lambda c)}$ such that ", "page_idx": 18}, {"type": "equation", "text": "$$\nV^{(\\pi_{n}^{*},\\lambda^{\\prime}c)}\\rightarrow V^{(\\pi^{*},\\lambda^{\\prime}c)}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. First, note that ", "page_idx": 18}, {"type": "equation", "text": "$$\nV^{(\\pi_{n}^{*},\\lambda^{\\prime}c)}=\\underbrace{V^{(\\pi_{n}^{*},\\lambda c)}}_{\\underset{(\\mathrm{a})}{\\bigcup}}+(\\lambda^{\\prime}-\\lambda_{n})\\,\\mathbb{E}\\,\\underbrace{\\sum_{t=1}^{T^{(\\pi_{n}^{*})}}c(x_{t})}_{\\mathrm{(b)}}\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Using this, by passing limits through the respective sums and products, it suffices to prove convergence of (a) and (b) separately, with the same sequence choice $\\pi_{n}^{*}$ in both cases. By Weitzman [39]\u2014see Kleinberg et al. [24], Theorem 1, for an alternative proof\u2014any policies $\\pi_{n}^{*},\\pi^{*}$ which maximize the respective Gittins indices $\\alpha_{n}^{*},\\alpha^{*}$ are optimal: we will therefore choose $\\pi_{n}^{*},\\pi^{*}$ from this set of policies. This choice is not unique due to the possibility of ties, both in terms of which boxes to open, and when to stop: we will show that tie-breaking choices do not affect convergence of (a), and will make a suitable choice of tie-breaking rules, depending on the sequence $\\lambda_{n}$ in the claim\u2019s assumptions, to prove convergence of (b). ", "page_idx": 18}, {"type": "text", "text": "Part I: convergence of (a). Define $\\kappa_{n}(x)\\;=\\;\\operatorname*{min}(f(x),\\alpha_{n}^{*}(x))$ , and define $\\kappa$ analogously. By Kleinberg et al. [24], Theorem 1, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\nV^{(\\pi^{*},\\lambda_{n}c)}=\\mathbb{E}\\operatorname*{max}_{x\\in X}\\kappa_{n}(x)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and analogously for $\\lambda,\\,\\alpha^{*}$ , and $\\kappa$ . We now argue that $\\alpha_{n}^{*}\\to\\alpha^{*}$ monotonically pointwise. Since $\\lambda_{n}\\to\\lambda$ converges monotonically, consider ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname{EI}_{f}(x,g)=\\lambda_{n}c(x).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Recall that $\\operatorname{EI}_{f}(x,g)$ is continuous and strictly decreasing in $g$ : hence, its inverse in $g$ exists and is also strictly decreasing and continuous. We conclude that $\\alpha_{n}^{*}\\to\\alpha^{*}$ monotonically pointwise. From this, convergence of the respective expectations $\\mathbb{E}\\operatorname*{max}_{x\\in X}\\kappa_{n}(x)$ , and therefore convergence of (a), follows by the Monotone Convergence Theorem. ", "page_idx": 19}, {"type": "text", "text": "Part II: convergence of $(b)$ . We will need an identity involving the order in which boxes are opened: for this, we first prove that once a tie-breaking rule is chosen, for large enough $n$ , $\\pi_{n}^{*}$ and $\\pi^{*}$ open boxes in the same order. Since the number of boxes is finite, and $\\alpha_{n}^{*}\\to\\alpha^{*}$ monotonically pointwise from (a): it follows for $n$ large enough that, if there are no ties in $\\alpha^{*}(x)$ , then $\\pi_{n}^{*}$ opens boxes in the same order as $\\pi^{*}$ . If there are ties in $\\alpha^{*}$ , we choose a tie-breaking rule so that $\\pi_{n}^{*}$ and $\\pi^{*}$ open boxes in the same order. It follows that, once this choice is made, for $n$ large enough, all policies open boxes in the same order $x_{1},..,x_{|X|}$ , with different $\\pi_{n}$ and $\\pi^{*}$ possibly stopping at different times. ", "page_idx": 19}, {"type": "text", "text": "The identity we seek will differ depending on how tie-breaking rules regarding when to stop are handled: recall that ties can occur when opening the box $x_{t}\\in X$ at time $t$ , we have $\\alpha^{*}(x_{t})=f_{t}^{*}$ , where $f_{t}^{*}$ is the best observed value up to time $t$ . We adopt the following tie-breaking rule for stopping, depending on whether or not $\\lambda_{n}$ is an increasing or decreasing sequence: ", "page_idx": 19}, {"type": "text", "text": "\u2022 If $\\lambda_{n}$ is increasing: let $\\pi^{*}=\\pi_{+}^{*}$ be the policy that opens the best closed box in the event of a tie. ", "page_idx": 19}, {"type": "text", "text": "\u2022 If $\\lambda_{n}$ is decreasing: let $\\pi^{*}=\\pi_{-}^{*}$ be the policy that stops in the event of a tie. ", "page_idx": 19}, {"type": "text", "text": "Note also that if $\\lambda_{n}$ is both increasing and decreasing, it is constant, and the claim we want to prove holds: therefore, we suppose it is not, which makes the choice between $\\pi_{+}^{*}$ and $\\pi_{-}^{*}$ uniquely determined. We make the same tie-breaking choices for $\\pi_{n}^{*}$ , letting $\\pi_{n,+}^{*}$ and $\\pi_{n,-}^{*}$ be the respective policies. Let $x_{1},..,x_{|X|}$ denote boxes in the order they are opened. Then the expected total costs are ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\mathbb{E}\\displaystyle\\sum_{t=1}^{(\\pi_{+}^{*})}c(x_{t})\\overset{(1)}{\\cong}\\displaystyle\\sum_{i=1}^{|X|}\\mathbb{P}(T^{(\\pi_{+}^{*})}\\geq i)c(x_{i})\\overset{(\\mathrm{ii})}{=}\\displaystyle\\sum_{i=1}^{|X|}\\mathbb{P}(f_{j}^{*}\\leq\\alpha^{*}(x_{j}),1\\leq j\\leq i)c(x_{i})}\\\\ {\\displaystyle\\overset{(\\mathrm{ii})}{=}\\displaystyle\\sum_{i=1}^{|X|}\\mathbb{P}(f(x_{j})\\leq\\alpha^{*}(x_{i}),1\\leq j\\leq i)c(x_{i})\\overset{(\\mathrm{iv})}{=}\\displaystyle\\sum_{i=1}^{|X|}\\prod_{j=1}^{i-1}\\mathbb{P}(f(x_{j})\\leq\\alpha^{*}(x_{i}))c(x_{i})}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where (i) follows by writing the probability as an expectation of an appropriate indicator, (ii) follows by noting that under the policy $\\pi_{+}^{*}$ , the event $T^{(\\pi_{+}^{*})}\\bar{\\geq}i$ occurs if and only if at each time $j=1,..,i$ the best observed value $f_{j}^{*}$ is not higher than the Gittins index of $x_{j}$ , (iii) follows by expanding the maximum which defines $f_{j}^{*}$ and using monotonicity of $\\alpha^{*}(x_{j})$ in $j$ to simplify the resulting events, and (iv) follows by independence of $f(x_{j})$ and $f(x_{j^{\\prime}})$ for $j\\neq j^{\\prime}$ . Using this, it suffices to show ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{\\mathbb{P}(f(x_{j})\\leq\\alpha_{n}^{*}(x_{i}))\\to\\mathbb{P}(f(x_{j})\\leq\\alpha^{*}(x_{i}))\\qquad}&&{\\mathrm{if~}\\pi^{*}=\\pi_{+}^{*},\\mathrm{~and~}}\\\\ &{\\mathbb{P}(f(x_{j})<\\alpha_{n}^{*}(x_{i}))\\to\\mathbb{P}(f(x_{j})<\\alpha^{*}(x_{i}))\\qquad}&&{\\mathrm{if~}\\pi^{*}=\\pi_{-}^{*}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The former equals the cumulative distribution function of $f(x_{j})$ , which is right-continuous, evaluated at $\\alpha_{n}^{*}(x_{i})$ . The latter is similar but is instead left-continuous. For $\\pi_{n,+}^{*}$ , since $\\lambda_{n}$ is increasing, we have that $\\alpha_{n}(x)$ is decreasing, and the claim follows by right-continuity. For $\\pi_{n,-}^{*}$ since $\\lambda_{n}$ is decreasing, we have that $\\alpha_{n}(x)$ is increasing, and the claim follows by left-continuity. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "We are now ready to prove the key property needed to apply Lemma 5. ", "page_idx": 19}, {"type": "text", "text": "$\\lambda_{B}^{*}>0$ $\\widehat{\\pi}\\in\\Pi^{(*,\\lambda_{B}^{*}c)}\\cap\\Pi_{B,\\mathrm{eq}}^{(c)}$ ", "page_idx": 19}, {"type": "text", "text": "Proof. By the Envelope Theorem\u2014specifically, Lemma 13\u2014we have for $\\lambda\\in[0,\\infty)$ that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\partial_{v}A(\\lambda)=\\partial_{v}\\bigg(V^{(*,\\lambda c)}+\\lambda B\\bigg)}\\\\ &{\\qquad\\qquad=\\partial_{v}\\left(\\underset{\\pi\\in\\Pi^{(*,\\lambda c)}}{\\operatorname*{sup}}\\left(V^{(\\pi,0)}-\\lambda\\left(\\underset{t=1}{\\mathbb{E}}\\underset{c=1}{\\overset{T^{(\\pi)}}{\\sum}}c(x_{t})-B\\right)\\right)\\right)\\Bigg)}\\\\ &{\\qquad=\\underset{\\pi^{\\prime}\\in\\Pi^{(*,\\lambda c)}}{\\operatorname*{max}}\\partial_{v}\\left(V^{(\\pi,0)}-\\lambda\\left(\\underset{t=1}{\\mathbb{E}}\\underset{c=1}{\\overset{T^{(\\pi)}}{\\sum}}c(x_{t})-B\\right)\\right)\\Bigg|_{\\pi=\\pi^{\\prime}}}\\\\ &{\\qquad=v B-\\underset{\\pi\\in\\Pi^{(*,\\lambda c)}}{\\operatorname*{min}}v\\,\\underset{t=1}{\\overset{T^{(\\pi)}}{\\sum}}c(x_{t})}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where all G\u00e2teaux derivatives\u2014see Definition 11\u2014are taken with respect to $\\lambda$ and exist by convexity, and the pointwise convergence condition of Lemma 13 follows by first noting that ", "page_idx": 20}, {"type": "equation", "text": "$$\nV^{(\\pi,0)}-\\lambda\\left(\\mathbb{E}\\sum_{t=1}^{T^{(\\pi)}}c(x_{t})-B\\right)=V^{(\\pi,\\lambda c)}+\\lambda B\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and applying Lemma 7. By the first-order optimality conditions, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\partial_{v}\\mathcal{A}(\\lambda_{B}^{*})\\geq0.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Combining this with the above, we conclude ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\pi\\in\\Pi^{(*,\\lambda c)}}\\upsilon\\operatorname{\\mathbb{E}}\\sum_{t=1}^{T^{(\\pi)}}c(x_{t})\\leq v B.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Since $\\lambda_{B}^{*}>0$ , this expression holds for $v$ equal to $\\pm1$ : plugging this in, we get ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\pi\\in\\Pi^{(*,\\lambda_{B}^{*}c)}}\\mathbb E\\sum_{t=1}^{T^{(\\pi)}}c(x_{t})\\leq B\\qquad\\qquad\\qquad B\\leq\\operatorname*{max}_{\\pi\\in\\Pi^{(*,\\lambda_{B}^{*}c)}}\\mathbb E\\sum_{t=1}^{T^{(\\pi)}}c(x_{t}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Now, let $\\widehat{\\pi}_{-}$ be a policy from the minimizer set, and let $\\widehat{\\pi}_{+}$ be a policy from the maximizer set. Define a third p o licy $\\widehat{\\pi}_{\\alpha}$ which randomizes between the two,   choosing $\\widehat{\\pi}_{-}$ with probability $\\alpha$ and $\\widehat{\\pi}_{+}$ with probability $1-\\alpha$ . Since $\\widehat{\\pi}_{+}$ and $\\widehat{\\pi}_{-}$ are optimal, they achieve  th e same value, so by conv e xity $\\widehat{\\pi}_{\\alpha}$ also achieves the same va l ue: the r efore, $\\widehat{\\pi}_{\\alpha}\\in\\Pi^{(*,\\lambda_{B}c)}$ . On the other hand, the expected total co s ts of $\\widehat{\\pi}_{\\alpha}$ are a convex combination of the e x pected costs of $\\widehat{\\pi}_{-}$ and $\\widehat{\\pi}_{+}$ : since the former lower-bounds $B$ and the latter upper-bounds $B$ , there exists an $\\alpha\\in[0,1]$ for which the costs of $\\pi_{\\alpha}$ are exactly $B$ . Taking $\\widehat{\\pi}=\\widehat{\\pi}_{\\alpha}$ for this value of $\\alpha$ , we obtain $\\widehat{\\pi}\\in\\Pi_{B,\\mathrm{eq}}^{(c)}$ and the claim follows. \u53e3 ", "page_idx": 20}, {"type": "text", "text": "To complete the proof, we show the minimizer set of $\\boldsymbol{\\mathcal{A}}$ does not contain zero. ", "page_idx": 20}, {"type": "text", "text": "Lemma 9. Let $\\lambda_{B}^{*}\\in[0,\\infty)$ be any minimizer of $\\mathcal{A}$ , then $\\lambda_{B}^{*}>0$ . ", "page_idx": 20}, {"type": "text", "text": "Proof. From the derivative calculation used in proof of Lemma 8, plugging in $v\\,=\\,1$ into the respective G\u00e2teaux derivative, we obtain ", "page_idx": 20}, {"type": "equation", "text": "$$\n{\\frac{\\operatorname{d}}{\\operatorname{d}\\!\\lambda}}\\,A(\\lambda)=B-\\operatorname*{min}_{\\pi\\in\\Pi^{(*,\\lambda c)}}\\mathbb{E}\\sum_{t=1}^{T^{(\\pi)}}c(x_{t}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "If $\\lambda=0$ , then the costs are zero: thus, any policy that opens every box, then selects the highest reward among opened boxes, is optimal. Therefore, we have $T^{(\\pi)}=|X|$ , and every optimal policy\u2019s cost is equal to the total cost of all the boxes. This gives ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}}{\\mathrm{d}\\lambda}\\,\\mathcal{A}(0)=B-\\sum_{x\\in X}c(x)<0\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the inequality follows by the assumption that the expected budget constraint is active. The claim follows. \u53e3 ", "page_idx": 20}, {"type": "text", "text": "With these results at hand, we are now ready to prove the main claim. ", "page_idx": 21}, {"type": "text", "text": "Theorem 2. Consider the expected budget-constrained problem, with the assumptions of Theorem 1. Assume the problem is feasible and the constraint is active, namely $\\begin{array}{r}{\\operatorname*{min}_{x\\in X}c(x)\\stackrel{}{<}B<\\overbrace{\\sum_{x\\in X}c(x)}}\\end{array}$ . Then there exists a $\\lambda>0$ and a tie-breaking rule such that the policy defined by ma ximi\u2208zing the Gittins index acquisition function $\\alpha^{\\star}(\\cdot)$ , defined using costs $\\lambda c(x)$ , is Bayesian-optimal. ", "page_idx": 21}, {"type": "text", "text": "Proof. Combine Lemma 5 with Lemmas 6, 8 and 9. ", "page_idx": 21}, {"type": "text", "text": "We conclude by comparing this claim and proof with the results of Aminian et al. [2]: ", "page_idx": 21}, {"type": "text", "text": "1. The objective of their expected budget-constrained problem is more general: while we focus on minimizing simple regret, Aminian et al. [2] study how to maximize utility, which is defined in a broader context\u2014one example being the difference between the best observed value and cumulative costs, which we study here. Our expected budget constraint is similarly a special case, which, in their terminology, takes all weights on indicators denoting inspection to be identical, and takes all selection weights to be zero. ", "page_idx": 21}, {"type": "text", "text": "2. We do not assume that $f(x)$ has finite support for all $x$ . This results in a significant technical difference: we must apply a sharp envelope theorem with an explicit supremum to conclude that the expected budget constraint is satisfied. More-straightforward results such as those presented by Milgrom and Segal [28], or their subdifferential-formulated analogues as found in the proof of Aminian et al. [2], Proposition 1 (iii), would not suffice: due to an explicit counterexample involving upside-down-absolute-value functions, without suitable structure, one cannot conclude that the supremum is achieved and the resulting inequality is tight. As an alternative to our approach, one could instead appeal to finiteness of the class of deterministic policies\u2014which Aminian et al. [2] assume, leading to piecewise-linear value functions. We avoid these assumptions, leading to a more technical but more general argument. ", "page_idx": 21}, {"type": "text", "text": "We conclude by noting that one can also consider variants of the problems we have studied under an almost-sure budget constraint, for instance ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\Pi_{B,\\mathrm{as}}^{(c)}=\\left\\{\\pi\\in\\Pi:\\mathbb{P}\\left(\\sum_{t=1}^{T^{(\\tau)}}c(x_{t})\\leq B\\right)=1\\right\\}\\qquad\\qquad V_{\\mathrm{asbc}}^{(*,c)}=\\operatorname*{sup}_{\\pi\\in\\Pi_{B,\\mathrm{as}}^{(c)}}V^{(\\pi,0)}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For this problem, this optimal value is upper-bounded by the optimal value of the expected budgetconstrained optimization\u2014specifically, $\\bar{V_{\\mathrm{asbc}}^{(*,c)}}\\leq V_{\\mathrm{ebc}}^{(*,c)}$ V e(b\u2217c,c ), since \u03a0(Bc,)as \u2286\u03a0(Bc ). ", "page_idx": 21}, {"type": "text", "text": "B.6 Auxillary results from optimization theory ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Below we state and prove three results from optimization theory: a Lagrange Multiplier Theorem for optimization problems with inequality constraints, and two Envelope Theorems. These results are not new, but are difficult to find at the level of generality we need them at: most references begin by assuming the domain of optimization is $\\mathbb{R}^{d}$ , or, if not that, that it is a Banach space, whereas we need results that hold when the domain of optimization is an arbitrary set, without a linear structure or a topology. In light of this, we have found it easier to simply prove the claims we need. ", "page_idx": 21}, {"type": "text", "text": "Lemma 10. Let $X$ be an arbitrary set, let $f\\,:\\,X\\,\\rightarrow\\,\\mathbb{R}$ and let $g\\;:\\;X\\;\\rightarrow\\;\\mathbb{R}.$ . Then defining ${\\mathcal{L}}(x,\\lambda)=f(x)-\\lambda(g(x)-y)$ for $\\lambda\\geq0$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{x\\in X}f(x)=\\operatorname*{sup}_{x\\in X}\\operatorname*{inf}_{\\lambda\\geq0}{\\mathcal{L}}(x,\\lambda).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Moreover, suppose there exist $x^{*},\\lambda^{*}$ which satisfy $\\begin{array}{r}{\\mathcal{L}(x^{*},\\lambda^{*})=\\operatorname*{inf}_{\\lambda\\geq0}\\operatorname*{sup}_{x\\in X}\\mathcal{L}(x,\\lambda)}\\end{array}$ and $g(x^{*})=$ y. Then we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{x\\in X}\\operatorname*{inf}_{\\lambda\\geq0}{\\mathcal{L}}(x,\\lambda)=\\operatorname*{inf}_{\\lambda\\geq0}\\operatorname*{sup}_{x\\in X}{\\mathcal{L}}(x,\\lambda).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. First, we show that for all $x$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\lambda\\geq0}{\\mathcal{L}}(x,\\lambda)={\\binom{f(x)}{-\\infty}}\\quad g(x)\\leq y\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "To show this, suppose first that $g(x)\\leq y$ . Then $\\lambda(g(x)-y)\\leq0.$ , hence its negation is non-negative, and minimized at $\\lambda=0$ . Now, suppose the converse. Then $\\lambda(g(x)-y)\\bar{>}\\;0$ , so its negation is negative, and the objective can be made arbitrarily close to $-\\infty$ by scaling $\\lambda$ . Using this, write ", "page_idx": 22}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{{\\underset{x\\in X}{\\operatorname*{sup}}}\\ {\\underset{\\lambda\\geq0}{\\operatorname*{inf}}}\\,{\\mathcal{L}}(x,\\lambda)={\\operatorname*{max}}\\left({\\underset{x\\in X}{\\operatorname*{sup}}}\\ \\operatorname*{inf}_{\\lambda\\geq0}{\\mathcal{L}}(x,\\lambda),\\ {\\underset{x\\in X}{\\operatorname*{sup}}}\\ \\operatorname*{inf}_{\\lambda\\geq0}{\\mathcal{L}}(x,\\lambda)\\right)}\\\\ &{\\qquad\\qquad={\\operatorname*{max}}\\left({\\underset{x\\in X}{\\operatorname*{sup}}}\\ f(x),-\\infty\\right)={\\underset{x\\in X}{\\operatorname*{sup}}}\\ f(x).}\\end{array}}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We now argue that, under the claim\u2019s additional assumption, one can swap the order of the supremum and infimum. Let $x^{*},\\lambda^{*}$ be a pair for which the constraints are tight. Then ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\lambda\\geq0}{\\operatorname*{inf}}\\ \\underset{x\\in X}{\\operatorname*{sup}}\\ \\mathcal{L}(x,\\lambda)\\stackrel{\\mathrm{(i)}}{=}\\underset{x\\in X}{\\operatorname*{sup}}\\ \\mathcal{L}(x,\\lambda^{*})\\stackrel{\\mathrm{(ii)}}{=}\\underset{x\\in X}{\\operatorname*{sup}}\\ \\mathcal{L}(x,\\lambda^{*})}\\\\ &{\\quad\\quad g(x)\\leq y}\\\\ &{\\quad\\quad\\stackrel{\\mathrm{(iii)}}{=}\\underset{x\\in X}{\\operatorname*{sup}}\\ \\underset{\\lambda\\geq0}{\\operatorname*{inf}}\\ \\mathcal{L}(x,\\lambda)\\stackrel{\\mathrm{(iv)}}{\\leq}\\underset{x\\in X}{\\operatorname*{sup}}\\ \\underset{\\lambda\\geq0}{\\operatorname*{inf}}\\ \\mathcal{L}(x,\\lambda)}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where (i) follows by definition of $\\lambda^{*}$ , (ii) follows by the fact that $x^{*}$ achieves the supremum and satisfies $g(x^{*})=y$ , (iii) follows by the fact that, when restricted to the set $\\{x\\in X:g(x)=y\\}$ , $\\mathcal{L}(x,\\lambda)$ is constant in $\\lambda$ for all $x$ -values, hence the infimum is taken over constant functions, (iv) follows by making the feasible set larger. Combining this with the sup-inf inequality [9] ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{x\\in X}\\operatorname*{inf}_{\\lambda\\geq0}{\\mathcal{L}}(x,\\lambda)\\leq\\operatorname*{inf}_{\\lambda\\geq0}\\operatorname*{sup}_{x\\in X}{\\mathcal{L}}(x,\\lambda)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "gives the claim. ", "page_idx": 22}, {"type": "text", "text": "It is easy to see that this argument holds even if one lets $y$ take values in an infinite-dimensional vector space, as long as $\\lambda$ takes values within a convex cone which is suitably paired with the aforementioned vector space, but we will not need this level of generality. The arguments we employ will be cleanest if we work with directional derivatives in the sense of G\u00e2teaux, as opposed to left-derivatives and right-derivatives of real-valued functions, which in our setting are equivalent but require more management of minus signs. For this, we adopt the following notation. ", "page_idx": 22}, {"type": "text", "text": "Definition 11. Let $\\mathcal{X}$ be a topological vector space, let $X\\subseteq\\mathcal{X}$ , and let $f:X\\to\\mathbb{R}$ be a function. The G\u00e2teaux derivative $\\partial_{v}f(x)\\in\\mathbb{R}$ of a function $f$ at a point $x\\in X$ in the direction $v\\in\\mathcal{X}$ , $i f$ it exists, is defined as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\partial_{v}f(x)=\\operatorname*{lim}_{\\varepsilon\\to0^{+}}{\\frac{f(x+\\varepsilon v)-f(x)}{\\varepsilon}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Note that we require no properties of our G\u00e2teaux derivatives: in particular, $\\partial_{v}f(x)$ can be non-linear in $v$ , though one can easily see that it will always satisfy $\\partial_{\\alpha v}f(x)=\\alpha\\partial_{v}f(\\dot{x})$ for $\\alpha\\geq0$ . If $f$ is a convex function, one can show by monotonicity of finite differences that $\\partial_{v}f(x)$ always exists\u2014in the non-extended-valued sense defined above\u2014on the relative interior of the effective domain of $f$ . ", "page_idx": 22}, {"type": "text", "text": "Our arguments need an appropriate Envelope Theorem. The first statement we need is essentially equivalent to Milgrom and Segal [28], Theorem 1, which is formulated in the language of leftderivatives and right-derivatives: Milgrom and Segal [28] state in a footnote that their claim also holds in a general normed vector space, provided one works with directional differentiation. In fact, the claim is even more general than that, and does not require a norm. Since we find this variant to be particularly clean, elegant, and instructive, we prove the claim in full generality below. ", "page_idx": 22}, {"type": "text", "text": "Lemma 12. Let $\\Theta$ be a subset of a topological vector space, and let $X$ be an arbitrary set. Let $f:X\\times\\Theta\\rightarrow\\mathbb{R}$ be bounded above in its first argument, and define $\\mathcal{V}$ to be ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathcal{V}(\\theta)=\\operatorname*{sup}_{x\\in X}f(x,\\theta)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Suppose that, for every $\\theta\\in\\Theta$ , the supremum is achieved, and let $X^{*}(\\theta)$ be the maximizer set. For any v, suppose that the G\u00e2teaux derivatives $\\partial_{v}\\mathcal{V}(\\theta)$ and $\\partial_{v}f(x,\\theta)$ exist and are finite-valued, with the convention that the G\u00e2teaux derivative of $f$ is taken in its second argument. Then ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{x^{*}\\in X^{*}(\\theta)}\\partial_{v}f(x^{*},\\theta)\\leq\\partial_{v}\\mathcal{V}(\\theta).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. Fix $\\theta\\in\\Theta$ , and let $x^{*}(\\theta)\\in X^{*}(\\theta)$ be an arbitrary maximizer. Note that, for all $\\theta^{\\prime}\\in\\Theta$ and all $x^{*}(\\theta^{\\prime})\\in X^{*}(\\theta^{\\prime})$ , by optimality we have ", "page_idx": 23}, {"type": "equation", "text": "$$\nf(x^{*}(\\theta),\\theta^{\\prime})\\leq f(x^{*}(\\theta^{\\prime}),\\theta^{\\prime})=\\mathcal{V}(\\theta^{\\prime})\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "with equality for $\\theta^{\\prime}\\,=\\,\\theta$ . Letting $\\varepsilon\\:>\\:0$ be sufficiently small, taking $\\theta^{\\prime}\\,=\\,\\theta\\,+\\,\\varepsilon v$ , subtracting $f(x^{*}(\\theta),\\theta)=\\mathcal{V}(\\theta)$ from both sides, and dividing by $\\varepsilon$ , we get ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{f(x^{*}(\\theta),\\theta+\\varepsilon v)-f(x^{*}(\\theta),\\theta)}{\\varepsilon}\\leq\\frac{\\mathcal{V}(\\theta+\\varepsilon v)-\\mathcal{V}(\\theta)}{\\varepsilon}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "thus taking limits as $\\varepsilon\\rightarrow0$ gives ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\partial_{v}f(x^{*}(\\theta),\\theta)\\leq\\partial_{v}\\mathcal{V}(\\theta).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "This holds for all choices $x^{*}(\\theta)\\in X^{*}(\\theta)$ , and the claim follows. ", "page_idx": 23}, {"type": "text", "text": "For our arguments to go through, we need a sharper version of this result, with the inequality replaced with an equality. Results like this go back at least to Danskin [11], and are proven by Bonnans and Shapiro [8], Proposition 4.12, Ba\u00b8sar and Bernhard [5], Theorem 10.1, and Bertsekas [7]: however, all of their claims require topological assumptions on the domain of optimization. Moreover, it is easy to see\u2014for instance by considering an envelope made up of upside-down absolute value functions, where $\\mathcal{V}(\\theta)$ is constant but $f(x,\\theta)$ is not\u2014that the inequality cannot be tight without similar assumptions. Below, we show that being affine in $\\theta$ and a certain convergence criterion suffice, even if $X$ is an arbitrary set. The argument is similar to that of the aforementioned references. ", "page_idx": 23}, {"type": "text", "text": "Lemma 13. With the assumptions and notations in Lemma 12, suppose further that $f(x,\\theta)$ is affine in $\\theta$ for all $x$ , and that for any monotone $\\theta_{n}\\to\\theta$ there exist $x^{*}(\\theta_{n})\\in X^{*}(\\theta_{n})$ and $x^{*}(\\theta)\\in X^{*}(\\theta)$ such that $f(x^{*}(\\theta_{n}),v)\\ '\\to f(\\bar{x}^{*}(\\theta),v)$ . Then we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{x^{*}\\in X^{*}(\\theta)}\\partial_{v}f(x^{*},\\theta)=\\operatorname*{max}_{x^{*}\\in X^{*}(\\theta)}\\partial_{v}f(x^{*},\\theta)=\\partial_{v}f(x^{*}(\\theta),\\theta)=\\partial_{v}\\mathcal{V}(\\theta).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. For any $\\varepsilon$ , note that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\gamma(\\theta+\\varepsilon v)-\\gamma(\\theta)}{\\varepsilon}=\\frac{f(x^{*}(\\theta+\\varepsilon v),\\theta+\\varepsilon v)-f(x^{*}(\\theta),\\theta)}{\\varepsilon}}\\\\ &{\\qquad\\qquad\\qquad=\\frac{f(x^{*}(\\theta+\\varepsilon v),\\theta+\\varepsilon v)-f(x^{*}(\\theta+\\varepsilon v),\\theta)}{\\varepsilon}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\frac{\\mathrm{negaive}}{\\varepsilon}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\frac{f(x^{*}(\\theta+\\varepsilon v),\\theta)-f(x^{*}(\\theta),\\theta)}{\\varepsilon}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\frac{f(x^{*}(\\theta+\\varepsilon v),\\theta+\\varepsilon v)-f(x^{*}(\\theta+\\varepsilon v),\\theta)}{\\varepsilon}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=f(x^{*}(\\theta+\\varepsilon v),v)}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the respective term is negative because $f(x^{\\ast}(\\theta+\\varepsilon v),\\theta)\\,\\leq\\,f(x^{\\ast}(\\theta),\\theta)$ , which holds by optimality of $x^{\\ast}(\\theta)$ . Taking limits gives ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\partial_{v}\\mathcal{V}(\\theta)\\leq f(x^{*}(\\theta),v)=\\partial_{v}f(x^{*}(\\theta),\\theta)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the final equality follows from the expression for the G\u00e2teaux derivative of a linear function. Combining this expression with the Lemma 12 shows that the respective supremum is achieved, and gives the claim. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "C Experimental setup ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We implement all experiments in BOTORCH [4]. Following standard practice, we initialize each optimization algorithm with $2(d+1)$ values drawn using a quasirandom Sobol sequence, where $d$ is the dimension of the domain. All computations were run on CPU, with individual experiments ran in parallel on various nodes of the Cornell G2 cluster, each allocated up to 4GB of memory. Exceptions include KG, MSEI, and BMSEI for higher dimensions, which required substantially more memory, up to 32GB. Most individual runs took several minutes at most, with exception of the more-expensive KG, MSEI and BMSEI baselines: more information with a direct runtime comparison is given in Appendix D.1. ", "page_idx": 24}, {"type": "text", "text": "Gaussian process models. In the Bayesian regret experiments shown in Figure 4, we use Mat\u00e9rn kernels with identical fixed hyperparameters\u2014namely smoothness $5/2$ and length scale $10^{-1}$ \u2014for both the Gaussian process prior used to sample the objective function, and the Gaussian process used for Bayesian optimization. To maintain consistency, we do not standardize data in this variant. For the synthetic and empirical experiments shown in Figure 5 and Figure 6, we use Mat\u00e9rn kernels with smoothness $5/2$ and length scales learned from data via maximum marginal likelihood optimization, and standardize input variables to be in $[0,1]^{d}$ along with output variables to be zero-mean and unit-variance, following BoTorch defaults. In the unknown-cost experiments, we model the objective and the logarithm of the cost function using independent Gaussian processes. Additional Bayesian regret experiment results, showing the effect of varying the kernel smoothness and length scale hyperparameters, are provided in Appendix D.4. ", "page_idx": 24}, {"type": "text", "text": "Acquisition function optimization. This is done as follows. We begin by computing acquisition values at $200d$ points spread across the domain $X$ , where $d$ is the dimension of $X$ . For all acquisition functions except MSEI and BMSEI, which use a modification described below, the initial $200d$ points are generated using a Sobol sequence design. From these, $10d$ points are selected according to the initialization heuristic used by BoTorch, detailed in Balandat et al. [4], Appendix F.1. We then use multi-start L-BFGS-B to optimize the acquisition function from each selected point. The point with highest acquisition value among the $10d$ optimized points is chosen as the next evaluation point. ", "page_idx": 24}, {"type": "text", "text": "We now detail the modified strategy used for MSEI and BMSEI: here, the initial $200d$ points are selected using the warm-start initialization strategy described in Jiang et al. [23], Appendix D and Astudillo et al. [3], Appendix F. This strategy uses the optimal solution from the previous iteration, targeting the branch that originates from the tree\u2019s root and whose fantasy sample most closely matches the actual observed value of the previously suggested candidate on the true function. This modification favors MSEI and BMSEI, slightly disadvantaging PBGI and other baselines. ", "page_idx": 24}, {"type": "text", "text": "Acquisition function hyperparameters. For PBGI, we choose the constant hyperparameter to be $\\bar{\\lambda}=10^{-4}$ . For PBGI-D, we choose the initial value to be $\\lambda_{0}\\,=\\,0.1$ and the constant decay factor $\\beta=2$ . For both variants, we compute the Gittins indices using 100 iterations of bisection search without any early stopping or other performance and reliability optimizations. For UCB, we follow the schedule in Srinivas et al. [34], Theorem 1 given by $\\eta_{t}\\doteq2\\log(d t^{2}\\pi^{2}/6\\delta)$ , where $d$ is the dimension. We also adopt the choice of $\\delta=10^{-1}$ and a scale-down factor of 5, as used in that work\u2019s experiments. For MSEI and BMSEI, we use 4 lookahead steps, each with a batch size of 1 and a single fantasy point. ", "page_idx": 24}, {"type": "text", "text": "Omitted baselines. We omit MSEI from the Bayesian regret plots for $d=16$ with $\\kappa=10^{-1}$ and for $d=32$ with all length scale choices because we were unable to get it to work reliably in these settings: the implementation of Jiang et al. [23] results in frequent crashes due to running out of memory and related issues when used on higher-difficulty problems. We also omit KG from $d=32$ , BMSEI from the cost-aware Pest Control and Robot Pushing experiments, and MES from the Bayesian regret experiment with $d=32$ for the same reasons. ", "page_idx": 24}, {"type": "text", "text": "In addition to the baselines mentioned in Section 4, we also implemented the predictive entropy search $(P E S)$ acquisition function of Hern\u00e1ndez-Lobato et al. [22], but could not get its computations to run reliably in an automatic-differentiation-based environment without resulting in NaN gradients. Hern\u00e1ndez-Lobato et al. [22] document this behavior, and suggest using finite-differencing in situations where it occurs: however, from initial examination, we found this to decrease performance on higher-dimensional problems. We therefore opted to restrict ourselves to automatically-differentiable baselines and omit PES, to ensure that performance differences seen can reliably be attributed to the acquisition functions used, and not to gradient computation. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "Objective functions: Bayesian regret. For Bayesian regret, this is straightforward: the objective is simply a draw from a Fourier feature approximation of the respective Gaussian process prior, drawn in such a way that different baselines with the same random number seed share the same objective, but objectives for different seeds are different draws from the same prior. We use a total of 1024 Fourier features. ", "page_idx": 25}, {"type": "text", "text": "Objective functions: synthetic benchmarks. The synthetic benchmark functions we use are as follows. We use variants with dimension $d=4,8,16$ in our experiments. ", "page_idx": 25}, {"type": "text", "text": "Ackley: this is ", "page_idx": 25}, {"type": "equation", "text": "$$\nf_{\\mathrm{A}}(x_{1},..,x_{d})=20-20\\exp\\left(-0.2\\sqrt{\\frac{1}{d}\\sum_{i=1}^{d}x_{i}^{2}}\\right)\\,-\\exp\\left(\\frac{1}{d}\\sum_{i=1}^{d}\\cos(2\\pi x_{i})\\right)\\,+\\,e.c.,\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "with search domain $X=[-1,1]^{d}$ . ", "page_idx": 25}, {"type": "text", "text": "Levy: this is $f_{\\mathrm{L}}=100\\widehat{f_{\\mathrm{L}}}$ where ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\widehat{f}_{\\mathrm{L}}(x_{1},..,x_{d})=s_{1}+\\sum_{i=1}^{d-1}(w_{i}-1)^{2}\\bigl(1+10\\sin(\\pi w_{i}+1)^{2}\\bigr)+(w_{d}-1)^{2}\\bigl(1+\\sin(2\\pi w_{d})^{2}\\bigr)\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "with $\\begin{array}{r}{w_{i}=1+\\frac{x_{i}-1}{4}}\\end{array}$ and $s_{1}=\\sin(\\pi w_{1})^{2}$ , and search domain $X=[-10,10]^{d}$ . ", "page_idx": 25}, {"type": "text", "text": "Rosenbrock: this is $f_{\\mathrm{R}}=10^{5}\\widehat{f}_{\\mathrm{R}}$ where ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\widehat{f}_{\\mathrm{R}}(x_{1},..,x_{d})=\\sum_{i=1}^{d-1}(100(x_{i+1}-x_{i}^{2})^{2}+(x_{i}-1)^{2})\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "with search domain $X=[-5,10]^{d}$ . ", "page_idx": 25}, {"type": "text", "text": "Cost function. In the cost-aware Bayesian regret and synthetic benchmark experiments, we use the cost function ", "page_idx": 25}, {"type": "equation", "text": "$$\nc(x)=20\\|S(x)\\|_{1}+1\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $S$ is an affine map used to standardize the input domain: specifically, $S(x)=A x+b$ where $A$ is a diagonal matrix and $b$ is a vector, both chosen so that the image of $X$ under $S$ is $[0,1]^{d}$ . ", "page_idx": 25}, {"type": "text", "text": "Objective functions: empirical. We now detail the empirical objective functions. ", "page_idx": 25}, {"type": "text", "text": "Pest Control $d=25.$ ). The pest control problem, as described in Oh et al. [29] aims to minimize the spread of pests as well as the costs of prevention treatment. We adopt the experiment setup from Li et al. [26], framing this as a categorical optimization problem with 25 variables, each representing a stage of intervention with 5 values reflecting different treatments. The objective function combines the spread of pests and the costs of prevention. ", "page_idx": 25}, {"type": "text", "text": "In our cost-aware experiment, we use the cost of prevention as the cost function. This can be computed in an automatically-differentiable manner, thus this problem is a known-cost problem. ", "page_idx": 25}, {"type": "text", "text": "Lunar Lander $^{d}=12.$ ). Following the setup in Eriksson et al. [14], we consider a reinforcement learning problem optimizing a controller for the lunar lander as implemented in OpenAI Gym, which includes 12 continuous input dimensions for engine throttle adjustments. The state space captures the lander\u2019s position, angle, time derivatives, and leg contact status. The controller\u2019s actions allow for directional booster firings or inaction. The objective is to maximize the average final reward over 50 randomly generated environments. ", "page_idx": 25}, {"type": "text", "text": "For cost-aware experimentation, we choose the cost to be the average number of simulation time steps, assuming batch processing in groups of 16. This assumption is based on the implementation found in the code associated with Li et al. [26]. Note that this objective involves the number of actual simulation steps used, and is therefore not automatically-differentiable: and we thus consider this problem to be an unknown-cost problem. ", "page_idx": 25}, {"type": "image", "img_path": "Ouc1F0Sfb7/tmp/62f7f7e3e4a59e78c6718f101c489fa34bea644acf2fada61fd159ad6dec0e0a.jpg", "img_caption": ["Figure 10: Runtime comparison of PBGI against baselines for computing the acquisition function on the Ackley benchmark across different dimensions $(d=4,8,16)$ ). We see that runtime of PBGI is slightly slower than EI and TS, but significantly faster than KG and MSEI. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "Robot Pushing $'d=14_{,}$ ). In this work, we adapt one of the three versions of the robot pushing problem designed by Wang and Jegelka [37], where two robots work to push two objects to their specified targets. The problem\u2019s complexity is captured through 14 control parameters, including each robot\u2019s initial placement and motion settings. The objective minimizes the sum of the distances from the final position of each object to its respective target. ", "page_idx": 26}, {"type": "text", "text": "In our cost-aware experiments, we test both known-cost and unknown-cost variants. The known-cost variant is the maximum of the two robots\u2019 operational duration and the unknown-cost variants variant is the sum of their traversal distances representing the total energy use, similar to the cost function used for energy-aware robot pushing benchmark of Astudillo et al. [3], but with one modification: we use the distance traversed by the robot arms instead of the distance the objects being moved. To understand the effect of this difference, we include the results for both the unknown-cost and known-cost versions in Appendix D.3. ", "page_idx": 26}, {"type": "text", "text": "D Additional experimental results ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Here, we provide additional experimental results to better understand performance differences and other aspects of policy behavior, including the effect of various problem hyperparameters. ", "page_idx": 26}, {"type": "text", "text": "D.1 Runtime comparison ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Here, we provide a runtime comparison between PBGI and various baselines, including inexpensive baselines such as EI and TS, and expensive ones such as MSEI. We do so in the Ackley synthetic benchmark setting, using the same hyperparameter settings as the main experiments. We measure the time to compute and optimize the acquisition function. ", "page_idx": 26}, {"type": "text", "text": "Results can be seen in Figure 10. We see that PBGI is slightly slower than EI and TS, but significantly faster than either KG or MSEI, though the runtime of the latter decreases substantially as it accumulates more data. Overall, we conclude that PBGI\u2019s runtime is closer to that of classical acquisition functions than sophisticated lookahead-based variants. ", "page_idx": 26}, {"type": "text", "text": "D.2 Hyperparameter choice for PBGI-D ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Next, we examine the behavior of different hyperparameter choices for $\\lambda_{0}$ and $\\beta$ in PBGI-D. Recall that this variant sets $\\begin{array}{r}{\\lambda_{t}=\\frac{\\lambda_{t-1}}{\\beta}}\\end{array}$ at times when the Gittins stopping rule triggers, and $\\lambda_{t}=\\lambda_{t-1}$ at all other times. To understand this, we examined regret curves in the Bayesian regret setting of Section 4 with $d=8$ under different choices. Results can be seen in Figure 11. We see that behavior of different $\\lambda_{0}$ -values is qualitatively similar to that of different $\\lambda$ -values in PBGI, but with substantially smaller differences between variants, which are detectable primarily due to the relatively-large number of ", "page_idx": 26}, {"type": "image", "img_path": "Ouc1F0Sfb7/tmp/eac7a0c685efe2e1758449ace7effa61847c5ad46baca8c5c16f1db8015e8a47.jpg", "img_caption": ["Cumulative Cost "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Figure 11: Behavior of PBGI-D under different choices of the initial value $\\lambda_{0}$ , and the coefficient of decay $\\beta$ , on the Bayesian regret experiment with $d=8$ and other parameters set the same way as in Section 4. We show medians under $n=256$ samples, along with quartiles to assess variability. We see that smaller $\\lambda_{0}$ -values act similarly to the behavior seen in Figure 3 for PBGI, trading off risk-seeking vs. risk-averse behavior, but with a substantially smaller gap between the two variants. Larger $\\beta$ -values lead to more-abrupt decay curves for $\\lambda$ , with an effect qualitatively similar to $\\lambda_{0}$ . In both cases, though our relatively-large sample size allows us to see small differences between methods, their overall impact on performance is close-to-nonexistent on the scale of variability. ", "page_idx": 27}, {"type": "text", "text": "seeds used in each experiment. Smaller values of $\\lambda_{0}$ tend to lead to very slightly to higher regret on small time scales, but very slightly lower regret on larger time scales. Behavior for $\\beta$ is similar: larger values lead to a more gradual decay of $\\lambda$ , but with minimal overall impact on performance, especially in comparison to between-seed variability. We conclude that, in terms of regret, PBGI-D is less sensitive to hyperparameter choice compared to PBGI. ", "page_idx": 27}, {"type": "text", "text": "D.3 Effect of unknown costs ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "The Robot Pushing empirical benchmark involves two cost functions: a known-cost variant representing total operational duration, and an known-cost variant representing total distance traversed, a proxy for energy use similar to the variant considered by Astudillo et al. [3]. One can therefore ask: how different is the resulting algorithm behavior in these two settings? Figure 12 shows this: it reveals that for the known-cost variant, EIPC and PBGI-D perform similarly, whereas for the unknown-cost variant, PBGI-D achieves the best performance on all except the shortest time horizons, where EIPC is instead competitive. Other baselines, most notably BMSEI, substantially underperform EIPC and PBGI-D, behaving similarly in both settings. ", "page_idx": 27}, {"type": "text", "text": "D.4 Kernel and problem hyperparameters ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Choice of kernel. To check whether our results are sensitive to the kernel used for the Gaussian process model, we replicated the Bayesian regret experiments with Mat\u00e9rn kernels with smoothness parameters $\\nu=3/2,5/2$ , as well as the squared exponential kernel, which is the limit of Mat\u00e9rn kernels as $\\nu\\to\\infty$ [30]. ", "page_idx": 27}, {"type": "text", "text": "Similar to the original results of Figure 4, we can clearly see from Figure 13 and Figure 14 that behavior splits into three regimes: ", "page_idx": 27}, {"type": "text", "text": "1. Easy: $d$ sufficiently-small, most policies achieve similar performance.   \n2. Medium-hard: $d$ moderate-to-large, both PBGI variants perform better than baselines.   \n3. Very hard: $d$ sufficiently large, no policy outperforms random search. ", "page_idx": 27}, {"type": "text", "text": "We also see that $d\\,=\\,32$ lands in the very-hard regime for the uniform-cost case but not for the cost-aware case: intuitively, this occurs because costs can reduce the effective volume of the search space, since high-cost regions without promising points can be excluded from search. ", "page_idx": 27}, {"type": "text", "text": "This behavior is consistent among different kernels, but where the exact threshold at which regimes switch differs. In particular, for the squared exponential kernel, the separation between the medium and the hard regime appears earlier than for the other variants: all policies there have similar performance to random search when $d=16$ . ", "page_idx": 27}, {"type": "image", "img_path": "Ouc1F0Sfb7/tmp/19e3233b7fd07cb97f012560a19167316a89d31dc551408247fd7cc5a6d45a40.jpg", "img_caption": ["Figure 12: Experimental results for the Robot Pushing empirical benchmark, with the known-cost variant (left) and unknown-cost variant (right). We see that performance overall is similar, with EIPC and PBGI-D performing strongest. Their relative performance is similar in the known-cost variant, whereas in the unknown-cost variant PBGI-D outperforms EIPC on sufficiently-large horizons, and vice-versa on sufficiently small horizons. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "Choice of length scale. To check whether our results are sensitive to the Gaussian process model\u2019s length scale, we compare $\\kappa=10^{-1}$ with $\\kappa=5\\cdot10^{-1}$ and $\\kappa=10^{0}$ . From Figure 15 and Figure 16, which show uniform-cost and cost-aware results, respectively, we can see that PBGI variants also have much better performances as the dimension increases in both scenarios. Since $\\kappa=10^{0}$ and $\\kappa=5\\times10^{-1}$ result in easier problems than $\\kappa=10^{-1}$ , in the uniform-cost case $d=32$ lands into the medium-hard regime rather than the very-hard regime. ", "page_idx": 28}, {"type": "text", "text": "Synthetic benchmark dimension. To better understand the effect of problem dimension in settings outside of Bayesian regret, we repeat the synthetic benchmark experiments with $d=4$ , $d=8$ and $d=16$ . Results in Figure 18. Since $d=4$ and $d=8$ are easier to solve, here PBGI variants perform comparably to baselines. ", "page_idx": 28}, {"type": "image", "img_path": "Ouc1F0Sfb7/tmp/b71e126fe5902c4ded8369add61b84104a6b254784ca3a37d13f5d442d8c67b3.jpg", "img_caption": ["Figure 13: Comparison of Bayesian regret across Gaussian process priors with different kernels over different dimensions, in the uniform-cost setting. All length scales are $\\kappa=10^{-1}$ . We see that overall behavior is similar, but the precise thresholds at which each example switches between the easy, medium-hard, and very hard regimes differ. "], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "Ouc1F0Sfb7/tmp/667a8a933127a3493f8e391d525df2b417127a840ce97fe26a3552fbc80c8bb4.jpg", "img_caption": ["Figure 14: Comparison of Bayesian regret across Gaussian process priors with different kernels over different dimensions, in the cost-aware setting. All length scales are $\\kappa=10^{-1}$ . We see that overall behavior is similar, but the precise thresholds at which each example switches between the easy, medium-hard, and very hard regimes differ. "], "img_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "Ouc1F0Sfb7/tmp/c898cca5b739afe619861c1c69950dae1635cba8cda2ee6da25e6f01a708a681.jpg", "img_caption": ["Figure 15: Comparison of Bayesian regret across different length scales and dimensions, with a Mat\u00e9rn-5/2 kernel, in the uniform-cost setting. We see similar overall behavior, but each example switches between the easy, medium-hard, and very hard regimes at different precise thresholds. "], "img_footnote": [], "page_idx": 31}, {"type": "image", "img_path": "Ouc1F0Sfb7/tmp/8f052ca307ced60d82c10c52ea9d2880810e1959364c864c0c6acb23883b778f.jpg", "img_caption": ["Figure 16: Comparison of Bayesian regret across different length scales and dimensions, with a Mat\u00e9rn-5/2 kernel, in the cost-aware setting. We see similar overall behavior, but each example switches between the easy, medium-hard, and very hard regimes at different precise thresholds. "], "img_footnote": [], "page_idx": 32}, {"type": "image", "img_path": "Ouc1F0Sfb7/tmp/24b5be72e946d4c7bcf014bd1063b5f814f0aadafd5762a388d0632f33668392.jpg", "img_caption": ["Figure 17: Comparison of regret for synthetic benchmark functions under different dimensions, in the uniform-cost setting. We see that all methods perform similarly for $d=4$ , with differences between the most-competitive methods emerging as dimension increases to $d=8$ and $d=16$ . "], "img_footnote": [], "page_idx": 33}, {"type": "image", "img_path": "Ouc1F0Sfb7/tmp/d67444558698204cf3d4d24b7f0f836202b78fb30d9cd060a02e45f392fd6d94.jpg", "img_caption": ["Figure 18: Comparison of regret for synthetic benchmark functions under different dimensions, in the cost-aware setting. We see that all methods perform similarly for $d=4$ , with differences between the most-competitive methods emerging as dimension increases to $d=8$ and $d=16$ . "], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We have reviewed all claims made and checked that they accurately reflect the contributions and scope. ", "page_idx": 35}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We describe explicit limitations throughout the work, and in particular in the experimental section. ", "page_idx": 35}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 35}, {"type": "text", "text": "Justification: All claims are precise, and are either referenced, or proven in the appendix. ", "page_idx": 35}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: Yes, we include an experimental appendix documenting this. ", "page_idx": 35}, {"type": "text", "text": "5. Open access to data and code ", "page_idx": 35}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 35}, {"type": "text", "text": "Justification: Code is provided in supplementary material. ", "page_idx": 35}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: The experiment details are fully described in the appendix. ", "page_idx": 35}, {"type": "text", "text": "7. Experiment Statistical Significance ", "page_idx": 35}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: Yes, all experiments include quartiles and sample sizes to assess variability. 8. Experiments Compute Resources ", "page_idx": 35}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: This paper only studies relatively small-scale problems compared to other machine learning areas, and each individual experiment can run on an ordinary laptop. ", "page_idx": 35}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics HTTPS://NEURIPS.CC/PUBLIC/ETHICSGUIDELINES? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 36}, {"type": "text", "text": "Justification: We have checked and believe that we conform to the guidelines. ", "page_idx": 36}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: This is a theory-and-methods-focused paper with no explicit societal application or impact. ", "page_idx": 36}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: We do not release a dataset or model. ", "page_idx": 36}, {"type": "text", "text": "12. Licenses for existing assets ", "page_idx": 36}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 36}, {"type": "text", "text": "Justification: We do not use any assets except publicly-available Python packages, which are documented as dependencies for our code, and their companion papers are cited. ", "page_idx": 36}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 36}, {"type": "text", "text": "Justification: We do not introduce any new assets. ", "page_idx": 36}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "page_idx": 36}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: There are no human subjects. ", "page_idx": 36}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 36}, {"type": "text", "text": "Justification: There are no human subjects. ", "page_idx": 36}]