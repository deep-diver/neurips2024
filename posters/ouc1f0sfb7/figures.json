[{"figure_path": "Ouc1F0Sfb7/figures/figures_3_1.jpg", "caption": "Figure 1: An illustration of this work's key idea. We view cost-aware Bayesian optimization as an extension of the Pandora's Box problem, and derive the cost-aware acquisition function aPBGI by incorporating the posterior into the Bayesian-optimal Pandora's Box acquisition function a*.", "description": "This figure illustrates the key idea of the paper by showing the connection between the Pandora's Box problem and cost-aware Bayesian optimization. The Pandora's Box problem is a simplified decision problem that helps to understand cost-aware Bayesian optimization. The figure shows how the Bayesian-optimal solution to the Pandora's Box problem can be used to develop a new acquisition function for cost-aware Bayesian optimization. This new acquisition function incorporates the posterior distribution of the objective function, which makes it more effective than existing acquisition functions.", "section": "3 The Pandora's Box Gittins index for Bayesian optimization"}, {"figure_path": "Ouc1F0Sfb7/figures/figures_5_1.jpg", "caption": "Figure 2: A Bayesian optimization problem with varying costs on which EIPC has poor performance, inspired by Astudillo et al. [3], Section A. The domain is X = [-500, 500], which we visualize on the subinterval [-5, 5]. Left: illustration of the non-uniform prior variance, which is given by a Mat\u00e9rn-5/2 kernel scaled by a narrow bump function. Center: the cost function, which is a narrow bump-shaped function. Right: regret curves for EIPC and PBGI. Legend refers only to regret curves.", "description": "This figure shows a Bayesian optimization problem with non-uniform prior variance and a narrow bump-shaped cost function.  The plot on the right compares the regret (the difference between the best possible objective value and the achieved objective value) of the Expected Improvement per unit cost (EIPC) acquisition function and the Pandora's Box Gittins Index (PBGI) acquisition function. The results illustrate that PBGI outperforms EIPC in this scenario, highlighting its improved performance in the presence of varying costs.", "section": "2.2 Expected improvement per unit cost"}, {"figure_path": "Ouc1F0Sfb7/figures/figures_6_1.jpg", "caption": "Figure 3: Left: contour plots showing how EI (left) and PBGI (center-left, center-right) depend on the posterior mean and standard deviation at a given point (lighter colors indicate higher values). We see that PBGI values high standard deviation more than EI. Right: PBGI performance across values of \u03bb, under the setup of the Bayesian regret experiment of Section 4 with d = 8. We plot the median of n = 256 samples, along with quartiles to show variability. We see that large \u03bb-values decrease regret sooner, but eventually lose out to smaller \u03bb-values.", "description": "This figure compares the Expected Improvement (EI) acquisition function with the Pandora's Box Gittins Index (PBGI) acquisition function. The left side shows contour plots illustrating how EI and PBGI respond to different combinations of posterior mean and standard deviation. The right side shows the performance of PBGI across different values of the hyperparameter \u03bb.  The results are from a Bayesian regret experiment with 8 dimensions.", "section": "3.3 An acquisition function class for cost-aware Bayesian optimization"}, {"figure_path": "Ouc1F0Sfb7/figures/figures_7_1.jpg", "caption": "Figure 4: Bayesian regret curves, shown using medians, as well as quartiles to indicate experiment variability. We see in the cost-aware setting that both PBGI variants exhibit comparable performance to baselines for d = 8, and decisively outperform baselines in d = 16 and d = 32. This behavior is roughly-mirrored in the uniform-cost setting, with two notable distinctions: (a) UCB also exhibits strong performance for d = 16 matching PBGI and PBGI-D, and (b) all methods perform comparably to random search for d = 32 under uniform costs.", "description": "This figure shows the Bayesian regret (a measure of the optimization algorithm's performance) for different dimensions (d = 8, 16, 32) of the problem space under both uniform and varying costs.  The plots show the median regret and quartiles across multiple runs, illustrating variability.  The results indicate that the Pandora's Box Gittins Index (PBGI) and its adaptive decay variant (PBGI-D) perform comparably to or better than other state-of-the-art Bayesian optimization methods, especially in higher dimensions (d = 16, 32) and under varying costs.", "section": "4 Experiments"}, {"figure_path": "Ouc1F0Sfb7/figures/figures_8_1.jpg", "caption": "Figure 5: Synthetic benchmark regret curves, shown using medians, as well as quartiles to assess variability. All objective functions are defined with dimension d = 16. We see in the cost-aware setting that PBGI and PBGI-D perform strongest on the heavily-multimodal Ackley function, matching the non-myopic BMSEI baseline. On the Levy and unimodal Rosenbrock function, PBGI-D instead matches and for some cost budgets outperforms\u2014the EIPC baseline, significantly outperforming BMSEI. Uniform-cost results are similar: PBGI performs well on Ackley and Levy, but is outperformed by most baselines and PBGI-D on Rosenbrock.", "description": "The figure compares the performance of different Bayesian optimization acquisition functions on three synthetic benchmark functions (Ackley, Levy, and Rosenbrock) with dimension d=16.  The results are shown for both cost-aware and uniform-cost settings.  The plots show that PBGI and PBGI-D generally perform well, especially on the Ackley function, often outperforming other baselines like EIPC and BMSEI. However, performance varied across different functions and settings.", "section": "4.2 Synthetic benchmarks"}, {"figure_path": "Ouc1F0Sfb7/figures/figures_9_1.jpg", "caption": "Figure 6: Empirical benchmark regret curves, shown using medians, as well as quartiles to show variability. We see in both the cost-aware and uniform-cost settings that PBGI exhibits stronger performance on the Pest Control and Lunar Lander problems, while PBGI-D together with the EI and EIPC baselines performs strongly on the Robot Pushing problem. Note that in the cost-aware variant of Robot Pushing, the non-myopic BMSEI baseline performs poorly, potentially mirroring the behavior previously seen on the unimodal Rosenbrock function in Figure 5.", "description": "This figure shows the empirical results on three real-world problems: Pest Control, Lunar Lander, and Robot Pushing.  The plots display the best observed value against the cumulative cost for both cost-aware and uniform cost settings.  The figure demonstrates that the Pandora's Box Gittins Index (PBGI) and its variant (PBGI-D) generally outperform or match the performance of other baselines, though the performance varies across problems and settings.  The results for the Robot Pushing problem highlight a potential limitation of the non-myopic BMSEI baseline in the cost-aware setting.", "section": "4.3 Empirical objectives"}, {"figure_path": "Ouc1F0Sfb7/figures/figures_12_1.jpg", "caption": "Figure 7: An illustration of the two-dimensional Ackley, Levy, and Rosenbrock functions.\u00b9 From the visual, one can see that these functions differ in terms of multimodality and ridge-regions within the optimization landscape.", "description": "This figure shows 3D plots of the Ackley, Levy, and Rosenbrock functions in two dimensions.  The plots visually demonstrate the differences in the functions' characteristics, including the multimodality of the Ackley function (many peaks and valleys), the multimodality and ridge-like structures of the Levy function, and the unimodal (single peak) nature of the Rosenbrock function.  These differences are relevant because the optimization strategies used in the paper's experiments will perform differently depending on the function's landscape.", "section": "A Illustrations"}, {"figure_path": "Ouc1F0Sfb7/figures/figures_12_2.jpg", "caption": "Figure 8: Comparison between three acquisition functions, namely Expected Improvement (EI), Pandora's Box Gittins Index (PBGI) with a large \u03bb-value of \u03bb = 10<sup>0</sup>, and Pandora's Box Gittins Index with a small \u03bb-value of \u03bb = 10<sup>\u22125</sup>. All three are computed using the same posterior distribution with four data points. The maximum is shown as a large dot.", "description": "This figure compares three acquisition functions: Expected Improvement (EI), Pandora's Box Gittins Index (PBGI) with a large \u03bb (\u03bb = 10<sup>0</sup>), and PBGI with a small \u03bb (\u03bb = 10<sup>\u22125</sup>).  All three functions are calculated using the same posterior distribution and four data points.  The plot shows how the acquisition functions behave differently based on the value of \u03bb. The top row shows the posterior distributions, and the bottom row shows the corresponding acquisition function.  The large \u03bb value leads to an acquisition function that is similar to EI, while the small \u03bb value produces a more explorative acquisition function.", "section": "Illustrations"}, {"figure_path": "Ouc1F0Sfb7/figures/figures_13_1.jpg", "caption": "Figure 2: A Bayesian optimization problem with varying costs on which EIPC has poor performance, inspired by Astudillo et al. [3], Section A. The domain is X = [-500, 500], which we visualize on the subinterval [-5, 5]. Left: illustration of the non-uniform prior variance, which is given by a Mat\u00e9rn-5/2 kernel scaled by a narrow bump function. Center: the cost function, which is a narrow bump-shaped function. Right: regret curves for EIPC and PBGI. Legend refers only to regret curves.", "description": "This figure shows a Bayesian optimization problem with varying costs. The left panel shows a non-uniform prior distribution. The center panel shows a cost function that is a narrow bump around 0. The right panel shows the regret curves for two algorithms: EIPC and PBGI.  EIPC has poor performance compared to PBGI in this scenario because it doesn't handle the costs effectively, and oversamples low-value, low-cost points, as discussed in the paper.", "section": "2.2 Expected improvement per unit cost"}, {"figure_path": "Ouc1F0Sfb7/figures/figures_26_1.jpg", "caption": "Figure 10: Runtime comparison of PBGI against baselines for computing the acquisition function on the Ackley benchmark across different dimensions (d = 4, 8, 16). We see that runtime of PBGI is slightly slower than EI and TS, but significantly faster than KG and MSEI.", "description": "This figure compares the runtime of the Pandora's Box Gittins Index (PBGI) acquisition function against several baselines (EI, MSEI, TS, KG) for different dimensions (d=4, 8, 16) of the Ackley benchmark function.  The x-axis represents the cumulative cost, and the y-axis shows the runtime. The plot shows that while PBGI is slightly slower than EI and TS, especially in higher dimensions, it is significantly faster than the more computationally expensive methods KG and MSEI.", "section": "Experiments"}, {"figure_path": "Ouc1F0Sfb7/figures/figures_27_1.jpg", "caption": "Figure 11: Behavior of PBGI-D under different choices of the initial value \u03bb\u2080, and the coefficient of decay \u03b2, on the Bayesian regret experiment with d = 8 and other parameters set the same way as in Section 4. We show medians under n = 256 samples, along with quartiles to assess variability. We see that smaller \u03bb\u2080-values act similarly to the behavior seen in Figure 3 for PBGI, trading off risk-seeking vs. risk-averse behavior, but with a substantially smaller gap between the two variants. Larger \u03b2-values lead to more-abrupt decay curves for \u03bb, with an effect qualitatively similar to \u03bb\u2080. In both cases, though our relatively-large sample size allows us to see small differences between methods, their overall impact on performance is close-to-nonexistent on the scale of variability.", "description": "The figure shows the effect of hyperparameters \u03bb\u2080 and \u03b2 on the performance of PBGI-D in a Bayesian regret experiment with d = 8.  The left plot shows median regret curves, with quartiles illustrating variability. The right plot displays the decay of \u03bb over time for different values of \u03bb\u2080 and \u03b2. The results demonstrate that performance is relatively insensitive to the choice of \u03bb\u2080 and \u03b2, with small differences that are less than the variability between experiment runs with different random seeds.", "section": "Experiments"}, {"figure_path": "Ouc1F0Sfb7/figures/figures_28_1.jpg", "caption": "Figure 6: Empirical benchmark regret curves, shown using medians, as well as quartiles to show variability. We see in both the cost-aware and uniform-cost settings that PBGI exhibits stronger performance on the Pest Control and Lunar Lander problems, while PBGI-D together with the EI and EIPC baselines performs strongly on the Robot Pushing problem. Note that in the cost-aware variant of Robot Pushing, the non-myopic BMSEI baseline performs poorly, potentially mirroring the behavior previously seen on the unimodal Rosenbrock function in Figure 5.", "description": "This figure compares the performance of PBGI and other acquisition functions (EI, EIPC, MSEI, BMSEI, MES, TS, MFMES, UCB, KG, PBGI-D, RS) on three empirical global optimization problems: Pest Control, Lunar Lander, and Robot Pushing.  The results are shown as regret curves (median and quartiles) for both cost-aware and uniform-cost settings. PBGI shows stronger performance on Pest Control and Lunar Lander, while PBGI-D performs well on Robot Pushing along with EI and EIPC.  BMSEI performs poorly on the cost-aware variant of Robot Pushing, echoing the results from the Rosenbrock function in Figure 5.", "section": "4.3 Empirical objectives"}, {"figure_path": "Ouc1F0Sfb7/figures/figures_29_1.jpg", "caption": "Figure 13: Comparison of Bayesian regret across Gaussian process priors with different kernels over different dimensions, in the uniform-cost setting. All length scales are \u03ba = 10\u22121. We see that overall behavior is similar, but the precise thresholds at which each example switches between the easy, medium-hard, and very hard regimes differ.", "description": "This figure compares Bayesian regret across different Gaussian process kernels (Mat\u00e9rn-3/2, Mat\u00e9rn-5/2, Squared Exponential) and various dimensions (d = 4, 8, 16, 32) in the context of uniform costs.  The results show that across different kernels, the overall behavior is quite similar, although the exact transition points between 'easy', 'medium-hard', and 'very hard' problem regimes vary.  This suggests the methodology's robustness to kernel selection.", "section": "4 Experiments"}, {"figure_path": "Ouc1F0Sfb7/figures/figures_30_1.jpg", "caption": "Figure 14: Comparison of Bayesian regret across Gaussian process priors with different kernels over different dimensions, in the cost-aware setting. All length scales are \u03ba = 10\u22121. We see that overall behavior is similar, but the precise thresholds at which each example switches between the easy, medium-hard, and very hard regimes differ.", "description": "This figure compares the performance of different Bayesian optimization algorithms on problems with varying dimensionality and kernel types in a cost-aware setting.  The results show that the overall behavior of the algorithms is consistent across different kernel types, but the transition points between easy, medium-hard, and very-hard problem difficulty vary depending on the specific kernel and problem dimension.", "section": "Experiments"}, {"figure_path": "Ouc1F0Sfb7/figures/figures_31_1.jpg", "caption": "Figure 13: Comparison of Bayesian regret across Gaussian process priors with different kernels over different dimensions, in the uniform-cost setting. All length scales are \u03ba = 10\u22121. We see that overall behavior is similar, but the precise thresholds at which each example switches between the easy, medium-hard, and very hard regimes differ.", "description": "This figure shows the results of Bayesian regret experiments with different kernels (Mat\u00e9rn 3/2, Mat\u00e9rn 5/2, Squared Exponential) and various dimensions (d = 4, 8, 16, 32) in a setting without explicit costs.  The plots illustrate how the regret changes over cumulative cost for multiple algorithms. The results show a similar trend across all kernels, highlighting three distinct performance regimes based on problem difficulty: easy (low dimensions), medium-hard (moderate dimensions), and very-hard (high dimensions).", "section": "4 Experiments"}, {"figure_path": "Ouc1F0Sfb7/figures/figures_32_1.jpg", "caption": "Figure 13: Comparison of Bayesian regret across Gaussian process priors with different kernels over different dimensions, in the uniform-cost setting. All length scales are \u03ba = 10\u22121. We see that overall behavior is similar, but the precise thresholds at which each example switches between the easy, medium-hard, and very hard regimes differ.", "description": "This figure compares the Bayesian regret of different algorithms across various Gaussian process kernels (Mat\u00e9rn 3/2, Mat\u00e9rn 5/2, Squared Exponential) and dimensions (d=4, 8, 16, 32) under uniform costs.  The results show that the relative performance of the algorithms varies across different kernels and dimensions, although overall patterns remain consistent. The three distinct performance regimes (easy, medium-hard, very hard) are observed, with the transition points differing based on the kernel and dimension.", "section": "4.1 Bayesian regret"}, {"figure_path": "Ouc1F0Sfb7/figures/figures_33_1.jpg", "caption": "Figure 17: Comparison of regret for synthetic benchmark functions under different dimensions, in the uniform-cost setting. We see that all methods perform similarly for d = 4, with differences between the most-competitive methods emerging as dimension increases to d = 8 and d = 16.", "description": "The figure displays the comparison of regret across different synthetic benchmark functions (Ackley, Levy, Rosenbrock) under varying dimensions (d=4, 8, 16).  It shows the performance of several Bayesian optimization algorithms (EI, MES, MSEI, TS, PBGI, UCB, PBGI-D, KG) against a random search (RS) baseline. The x-axis represents the number of function evaluations, and the y-axis shows the log regret.  The results indicate that for lower dimensions (d=4), most algorithms show similar performance. However, as the dimensionality increases, differences in performance between the algorithms become more pronounced.", "section": "4.2 Synthetic benchmarks"}, {"figure_path": "Ouc1F0Sfb7/figures/figures_34_1.jpg", "caption": "Figure 17: Comparison of regret for synthetic benchmark functions under different dimensions, in the uniform-cost setting. We see that all methods perform similarly for d = 4, with differences between the most-competitive methods emerging as dimension increases to d = 8 and d = 16.", "description": "This figure compares the performance of several Bayesian optimization algorithms on three different synthetic benchmark functions (Ackley, Levy, and Rosenbrock) across different dimensions (d=4, 8, 16).  The results are presented as regret curves, showing the performance of each algorithm in terms of cumulative cost (x-axis) and log regret (y-axis). The dashed black line represents random search (RS) as a baseline. The plot demonstrates that for lower dimensions (d=4), all algorithms exhibit relatively similar performance, but as the dimension increases (d=8, 16), there are noticeable differences in their efficiency and ability to minimize regret, with some algorithms performing considerably better than others.", "section": "4.2 Synthetic benchmarks"}]