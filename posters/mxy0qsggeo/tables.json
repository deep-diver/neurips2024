[{"figure_path": "MXY0qsGgeO/tables/tables_5_1.jpg", "caption": "Table 1: SD-Turbo evaluated on the attribute binding categories of T2I-CompBench and the LAION aesthetic score predictor [83] for different reward models.", "description": "This table presents the performance of the SD-Turbo model on the attribute binding categories of the T2I-CompBench benchmark, using different reward models.  It shows the impact of each reward model (CLIPScore, HPSv2, ImageReward, PickScore) individually and in combination on the Color, Shape, Texture attributes, as well as the overall aesthetic score.  The table helps demonstrate the effectiveness of combining reward models to improve image quality and prompt faithfulness.", "section": "4.1 Effect of Reward Models"}, {"figure_path": "MXY0qsGgeO/tables/tables_6_1.jpg", "caption": "Table 2: Quantitative Results on T2I-CompBench. ReNO combined with (1) PixArt-a DMD [12, 13, 102], (2) SD-Turbo [81], (3) SDXL-Turbo [81], (4) HyperSD [75] demonstrates superior compositional generation ability in both attribute binding, object relationships, and complex compositions. The best value is bolded, and the second-best value is underlined. Multi-step results taken from [13, 22].", "description": "This table presents a quantitative comparison of different text-to-image models on the T2I-CompBench benchmark. It shows the performance of four one-step models (PixArt-a DMD, SD-Turbo, SDXL-Turbo, and HyperSDXL) with and without the ReNO optimization method.  The results are broken down by different aspects of image quality, including color, shape, texture, spatial and non-spatial relationships, and overall composition complexity.  Multi-step models (from other research) are also included as a benchmark for comparison.  The best performing model in each category is shown in bold.", "section": "4 Experiments"}, {"figure_path": "MXY0qsGgeO/tables/tables_6_2.jpg", "caption": "Table 3: Quantitative Results on GenEval. ReNO combined with (1) PixArt-a DMD [12, 13, 102], (2) SD-Turbo [81], (3) SDXL-Turbo [81], (4) HyperSDXL [75] improves results across all categories. The best value is bolded, and the second-best value is underlined. Multi-step results taken from [22].", "description": "This table presents a quantitative comparison of different text-to-image models on the GenEval benchmark.  It shows the performance of four one-step models (PixArt-a DMD, SD-Turbo, SDXL-Turbo, and HyperSDXL) both with and without the ReNO optimization.  The results are broken down by several categories (Mean, Single, Two, Counting, Colors, Position, Color Attribution) reflecting different aspects of image generation quality and faithfulness to the prompt.  Multi-step models are also included for comparison, highlighting ReNO's efficiency in improving one-step models.", "section": "4 Experiments"}, {"figure_path": "MXY0qsGgeO/tables/tables_8_1.jpg", "caption": "Table 4: We measure the average LPIPS and DINO similarity scores over images generated for 50 different seeds for 100 prompts from Parti-Promtps.", "description": "This table presents the results of an experiment evaluating the diversity of images generated by different models. The experiment used 100 prompts from the Parti-Prompts dataset and generated images using 50 different random seeds for each prompt. The diversity was measured using two metrics: LPIPS (Learned Perceptual Image Patch Similarity) and DINO (self-supervised vision transformer). Lower scores indicate higher diversity. The table compares the diversity of images generated by SD-Turbo, SD-Turbo + ReNO, SD2.1 (50-step), SDXL-Turbo, SDXL-Turbo + ReNO, and SDXL (50-step).", "section": "4.5 Effect of ReNO on the Diversity of Generated Images"}, {"figure_path": "MXY0qsGgeO/tables/tables_9_1.jpg", "caption": "Table 5: Performance comparison of ReNO and DOODL over the first 50 prompts of each of the Attribute Binding categories in T2I-CompBench. We report scores from default T2I-Compbench evaluation using BLIP-VQA as well as the optimized CLIPScore before and after optimization.", "description": "This table compares the performance of ReNO and DOODL on the attribute binding task of the T2I-CompBench benchmark. It shows the scores before and after optimization using both BLIP-VQA and CLIPScore, highlighting the improvements achieved by ReNO.  The table also includes computational cost information (time per iteration and total time, VRAM usage) for each method.", "section": "4.6 Comparison to Multi-step Noise Optimization Methods"}, {"figure_path": "MXY0qsGgeO/tables/tables_9_2.jpg", "caption": "Table 6: Computational cost comparison of ReNO optimizing four reward models on an A100 GPU.", "description": "This table presents a comparison of the computational cost for applying ReNO to four different one-step text-to-image models.  The metrics shown are seconds per iteration (and total time), VRAM usage in gigabytes, the number of parameters in the model, and the image resolution.  It highlights the trade-off between model size and inference time for ReNO.", "section": "4.4 Computational Cost of ReNO"}, {"figure_path": "MXY0qsGgeO/tables/tables_19_1.jpg", "caption": "Table 7: Comparison of ReNO and Direct Preference Optimization (DPO) with a SDXL-based model. SDXL Base result taken from [13].", "description": "This table compares the performance of ReNO against direct preference optimization (DPO) using an SDXL-based model.  The comparison focuses on attribute binding (color, shape, texture) and aesthetic scores.  It highlights that while DPO improves the scores compared to the baseline SDXL, ReNO achieves superior performance across all metrics, demonstrating its potential for generalizing to unseen prompt distributions better than traditional fine-tuning methods.", "section": "B Further Quantitative Results"}, {"figure_path": "MXY0qsGgeO/tables/tables_19_2.jpg", "caption": "Table 2: Quantitative Results on T2I-CompBench. ReNO combined with (1) PixArt-a DMD [12, 13, 102], (2) SD-Turbo [81], (3) SDXL-Turbo [81], (4) HyperSD [75] demonstrates superior compositional generation ability in both attribute binding, object relationships, and complex compositions. The best value is bolded, and the second-best value is underlined. Multi-step results taken from [13, 22].", "description": "This table presents a quantitative comparison of different Text-to-Image (T2I) models on the T2I-CompBench benchmark.  It shows the performance of four one-step models (PixArt-a DMD, SD-Turbo, SDXL-Turbo, and HyperSD) both with and without the ReNO optimization method. The results are broken down by several sub-categories assessing different aspects of image generation quality, such as color, shape, texture, spatial and non-spatial relationships, and overall complexity of the generated images. Multi-step models' results are also included for comparison.", "section": "4 Experiments"}, {"figure_path": "MXY0qsGgeO/tables/tables_20_1.jpg", "caption": "Table 9: Computational cost comparison of ReNO compared to DOODL.", "description": "This table compares the computational cost of ReNO against DOODL, a multi-step noise optimization method.  It shows that ReNO, even with multiple reward models, is significantly faster (120x) and requires less VRAM than DOODL while achieving comparable improvements in the T2I-CompBench benchmark. The table highlights ReNO's efficiency advantage in terms of computation time and memory usage.", "section": "B Further Quantitative Results"}, {"figure_path": "MXY0qsGgeO/tables/tables_20_2.jpg", "caption": "Table 1: SD-Turbo evaluated on the attribute binding categories of T2I-CompBench and the LAION aesthetic score predictor [83] for different reward models.", "description": "This table presents a quantitative analysis of the performance of the SD-Turbo model on the T2I-CompBench benchmark, specifically focusing on the attribute binding aspect (color, shape, texture) and aesthetic scores. The analysis is conducted using different reward models, individually and in combinations.  It showcases the impact of different reward models on the model's ability to accurately capture attributes and produce aesthetically pleasing images. The results are numerical scores, reflecting the model's performance under different conditions.", "section": "4.1 Effect of Reward Models"}, {"figure_path": "MXY0qsGgeO/tables/tables_21_1.jpg", "caption": "Table 2: Quantitative Results on T2I-CompBench. ReNO combined with (1) PixArt-a DMD [12, 13, 102], (2) SD-Turbo [81], (3) SDXL-Turbo [81], (4) HyperSD [75] demonstrates superior compositional generation ability in both attribute binding, object relationships, and complex compositions. The best value is bolded, and the second-best value is underlined. Multi-step results taken from [13, 22].", "description": "This table presents a quantitative comparison of different text-to-image models on the T2I-CompBench benchmark.  It shows the performance of four one-step models (PixArt-a DMD, SD-Turbo, SDXL-Turbo, HyperSD) both with and without the ReNO optimization technique. The results are broken down by sub-categories of compositional generation (attribute binding: color, shape, texture, spatial; object relationship: non-spatial, spatial; complex).  Multi-step model results are also included for comparison.", "section": "4 Experiments"}, {"figure_path": "MXY0qsGgeO/tables/tables_25_1.jpg", "caption": "Table 12: Comparison of 512 \u00d7 512 FLUX-schnell with and without ReNO on the attribute binding categories of T2I-CompBench.", "description": "This table shows the quantitative results comparing the performance of the FLUX-schnell model with and without ReNO on the \"Attribute Binding\" sub-task of the T2I-CompBench benchmark.  The results are broken down into three categories: Color, Shape, and Texture.  Each category shows the improvement gained by applying the ReNO optimization technique to the FLUX-schnell model. The table demonstrates that ReNO improves the performance across all categories.", "section": "E.8 FLUX-schnell results"}]