[{"type": "text", "text": "The Feature Speed Formula: a flexible approach to scale hyper-parameters of deep neural networks ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "L\u00e9na\u00efc Chizat Institute of Mathematics, EPFL Lausanne, Switzerland lenaic.chizat@epfl.ch ", "page_idx": 0}, {"type": "text", "text": "Praneeth Netrapalli Google DeepMind pnetrapalli@google.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Deep learning succeeds by doing hierarchical feature learning, yet tuning hyperparameters (HP) such as initialization scales, learning rates etc., only give indirect control over this behavior. In this paper, we introduce a key notion to predict and control feature learning: the angle $\\theta_{\\ell}$ between the feature updates and the backward pass (at layer index $\\ell$ ). We show that the magnitude of feature updates after one GD step, at any training time, can be expressed via a simple and general feature speed formula in terms of this angle $\\theta_{\\ell}$ , the loss decay, and the magnitude of the backward pass. This angle $\\theta_{\\ell}$ is controlled by the conditioning of the layer-tolayer Jacobians and at random initialization, it is determined by the spectrum of a certain kernel, which coincides with the Neural Tangent Kernel when $\\ell=$ depth. Given $\\theta_{\\ell}$ , the feature speed formula provides us with rules to adjust HPs (scales and learning rates) so as to satisfy certain dynamical properties, such as feature learning and loss decay. We investigate the implications of our approach for ReLU MLPs and ResNets in the large width-then-depth limit. Relying on prior work, we show that in ReLU M\u221aLPs with iid initialization, the angle degenerates with depth as $\\cos(\\theta_{\\ell})\\,=\\,\\Theta(1/\\sqrt{\\ell})$ . In contrast, ResNets with branch scale $O(1/{\\sqrt{\\operatorname*{depth}}})$ maintain a non-degenerate angle $\\cos(\\theta_{\\ell})=\\Theta(1)$ . We use these insights to recover key properties of known HP scalings (such as $\\mu\\mathrm{P}_{.}$ ), and also introduce a new HP scaling for large depth ReLU MLPs with favorable theoretical properties. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The ability of deep Neural Networks (NNs) to learn hierarchical representations of their inputs has been argued to be behind their strong performance in data-intensive machine learning tasks [LeCun et al., 2015]. Yet, the process via which gradient-based training leads to feature learning remains mysterious and defies our intuition; some architectures can even reach zero loss without feature learning at all [Jacot et al., 2018]. This limited understanding makes it difficult to design NNs architectures and hyper-parameters (HP) scalings, and begs the development of tools to quantify feature learning. ", "page_idx": 0}, {"type": "text", "text": "In this paper, we demonstrate that the backward-feature angle (BFA) $\\theta_{\\ell}$ between the feature updates and the backward pass (at layer index $\\boldsymbol{\\ell}$ ) is a central object in this quest. Indeed, we show that the magnitude of feature updates after one GD step, at any training time, can be expressed via a simple feature speed formula in terms of this angle, the loss decay and the magnitude of the backward pass. Given the knowledge of $\\theta_{\\ell}$ , this leads to a general approach to quantify key dynamical properties of the training dynamics of NNs \u2013 such as the speed of feature learning and loss decay \u2013 and to characterize the HP scalings satisfying these properties. ", "page_idx": 0}, {"type": "text", "text": "\u2022 We prove the feature speed formula ( $\\operatorname{Thm}2.1\\$ ) which quantifies the feature updates in terms of the BFA $\\theta_{\\ell}$ , the loss decay and the magnitude of the backward pass at layer $\\ell$ . This formula, valid in any architecture and with an elementary proof, helps exploring the space of HP scalings, and understanding when feature learning arises.   \n\u2022 In Section 3, we develop tools to quantify the BFA. In particular, we show that, in the batch-size 1 case, $\\theta_{\\ell}$ can be estimated in terms of the spectrum of the backward to feature kernel (BFK) $K_{\\ell}$ and is related to the conditioning of layer-to-layer Jacobians (Thm. 3.2). We study the case of MLPs and ResNets at random initialization, and obtain that for a depth $L$ , $\\cos(\\theta_{L})=\\Theta(L^{-1/2})$ for ReLU MLPs (Prop. 5.1, exploiting a result in Jelassi et al. [2023]) and that $\\cos(\\theta_{L})=\\Theta(1)$ for linear ResNets with branch scale $O(L^{-1/2})$ (Prop. 5.2).   \n\u2022 In Section 4, we consider several properties of NN training dynamics that can be conveniently studied with our tools, including feature learning and loss decay. Enforcing these properties leads to explicit contraints on the magnitude of the forward, backward pass and learning rates in general architectures (Prop. 4.1).   \n\u2022 In Section 5, we show how various HP scalings for large width-then-depth MLPs and ResNets can be characterized by enforcing these properties. In particular we recover depth $\\mu\\mathrm{P}$ [Bordelon et al., 2023, Yang et al., 2023b] for ResNets (Table 2) and, for ReLU MLP, we introduce a scaling with output scale wdiedptthh (Table 1) that does not suffer from vanishing loss decay, in contrast to the one studied in [Jelassi et al., 2023].   \n\u2022 Finally, in Section 6 we develop a more \u201caxiomatic\u201d approach: starting from a minimal list of desiderata, which include a notion of gradient stability, we show that we recover, in a certain extent, the convenient properties considered in Section 4. This section focuses on homogeneous architectures for which we show, along the way, a backward speed formula (Prop. 6.1) and an invariance under block-wise rescaling (Prop. 6.2). ", "page_idx": 1}, {"type": "text", "text": "Related work The theory of NNs has recently benefited from important insights from asymptotic analyses in the large width and/or depth limits. Our work is in the continuity of those. ", "page_idx": 1}, {"type": "text", "text": "Analyses of wide and deep NNs at random initialization led to identifying critical initialization scalings that enable signal propagation [Poole et al., 2016, Hanin and Rolnick, 2018, Hanin, 2018]. They also identified dynamical isometry [Pennington et al., 2018], namely the concentration of the singular spectrum of the layer-to-layer Jacobians around 1, as an important indicator of training performance. Our analysis gives a concrete justification of the link between dynamical isometry and successful training, as we show that it is related to the alignment between the backward pass and feature updates. These questions have also been studied in ResNets, see e.g. [Hayou et al., 2021, Marion et al., 2022, Li et al., 2021] for signal propagation and [Tarnowski et al., 2019, Ling and Qiu, 2019] for dynamical isometry. ", "page_idx": 1}, {"type": "text", "text": "In 2018, two viewpoints for the dynamics of wide NNs were simultaneously introduced: a feature learning limit for two-layer MLPs [Mei et al., 2018, Chizat and Bach, 2018, Rotskoff and VandenEijnden, 2018] and a limit without feature learning for general NNs [Jacot et al., 2018, Du et al., 2018, Allen-Zhu et al., 2019]. These works highlighted the crucial role of HP scalings \u2013 learning rates and initialization \u2013 in the behavior of large NNs [Chizat et al., 2019]. ", "page_idx": 1}, {"type": "text", "text": "In order to classify HPs scalings, [Yang and Hu, 2021] formulated the maximal update $\\mu$ -criterion (it is part of the properties we study in Section 4). This criterion led to a full classification of HP scalings in the infinite hidden width limit (at fixed depth), and singled-out the so-called $\\mu$ -parameterization $(\\mu{\\bf P})$ as ideal for this criterion. We note that, provided alignment holds, our analysis allows in particular to characterize $\\mu\\mathrm{P}$ in an elementary way. See also [Yang et al., 2023a] for another simple derivation of $\\mu\\mathrm{P}$ using matrix spectral norms (but that does not give tight control on the magnitude of feature learning and does not a priori apply to the large depth asymptotics). Several works have since shown the practical value of these analyses in predicting the behavior of NNs [Vyas et al., 2023] and improving HP tuning [Yang et al., 2021]. ", "page_idx": 1}, {"type": "text", "text": "When restricted to the output layer of a NN, our notion of alignment/BFA coincides with that studied in Baratin et al. [2021], Atanasov et al. [2021], Lou et al. [2022], Wang et al. [2022] and the BFK we consider coincides with the NTK [Jacot et al., 2018]. We extend these concepts to study and quantify feature learning at any layer (not just at the output layer). Here we focus on the batch-size 1 setting, but the large batch-size setting of these works is a natural next direction for our analysis. ", "page_idx": 1}, {"type": "text", "text": "Finally, several recent works have studied feature learning in infinite width and depth NNs, starting with [Jelassi et al., 2023] for MLPs, and [Bordelon et al., 2023, Yang et al., 2023b] for ResNets. The two latter identified the $1/{\\sqrt{\\mathrm{depth}}}$ branch scaling as providing desirable properties, in particular that of $H P$ transfer [Yang et al., 2021]. These works take the infinite width limit as a first step in their analysis, before studying the resulting objects, resulting in a technical analysis. In our approach, we first take the step-size to 0 (as in [Jelassi et al., 2023]) and study in detail the structure of the back-propagation equations, before taking the large width-then-depth limit, as a last step. ", "page_idx": 2}, {"type": "text", "text": "Notations For integers $a,b\\in\\mathbb{Z}$ , we write $[a:b]=\\{a,\\ldots,b\\}$ . For any vector $x\\in\\mathbb{R}^{m}$ we denote by $\\|x\\|_{\\mathrm{rms}}:=m^{-1/2}\\|x\\|_{2}$ its root mean-square (RMS) norm. We use this as a proxy for the typical entry size of a vector, which is justified as long as that vector is dense. ", "page_idx": 2}, {"type": "text", "text": "2 The Feature Speed Formula ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Consider a depth- $L$ NN architecture defined by the recursion, for $\\ell\\in[1:L]$ , ", "page_idx": 2}, {"type": "equation", "text": "$$\nf_{0}\\in\\mathbb{R}^{m_{0}},\\qquad\\qquad f_{\\ell}=T_{\\ell}(f_{\\ell-1},w_{\\ell})\\in\\mathbb{R}^{m_{\\ell}},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\log s(f_{L})\\in\\mathbb{R}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $w_{\\ell}\\in\\mathbb{R}^{p_{\\ell}}$ are trainable parameters and we assume that the maps $T_{\\ell}:\\mathbb{R}^{m_{\\ell-1}}\\times\\mathbb{R}^{p_{\\ell}}\\rightarrow\\mathbb{R}^{m_{\\ell}}$ admit elementary (log-exp) selections1[Bolte and Pauwels, 2020]. By flattening the tensors, one can encode most practical NN architectures in Eq. (1). For instance, $m_{0}$ is typically the product of batch-size (or context length) with input dimension. We denote by $\\begin{array}{r}{b_{\\ell}=\\big(\\frac{\\partial\\bar{\\mathcal{L}}}{\\partial f_{\\ell}}\\big)^{\\top}\\in\\dot{\\mathbb{R}}^{m_{\\ell}}}\\end{array}$ the vectors of the backward pass. A gradient descent (GD) step with layerwise learning-rate (LR) $\\eta_{\\ell}\\cdot\\delta t>0$ for $\\ell\\in[1:L]$ consists in adding to each $w_{\\ell}$ the update ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\delta w_{\\ell}=-\\eta_{\\ell}\\cdot\\delta t\\cdot\\nabla_{\\ell}\\mathcal{L}=-\\eta_{\\ell}\\cdot\\delta t\\cdot\\Bigl(\\frac{\\partial\\mathcal{L}}{\\partial w_{\\ell}}\\Bigr)^{\\top}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "We are interested on the evolution of the NN over a single GD step with infinitesimally small step-size $\\delta t\\ll1$ . For any quantity $x$ associated to the NN, we denote $\\dot{x}$ its instantaneous velocity $\\begin{array}{r}{\\dot{x}:=\\operatorname*{lim}_{\\delta t\\downarrow0}\\frac{\\delta x}{\\delta t}}\\end{array}$ when it exists. In particular, we have $\\dot{w}_{\\ell}=-\\eta_{\\ell}\\nabla_{\\ell}\\mathcal{L}$ . ", "page_idx": 2}, {"type": "text", "text": "The following identity is the seed of our approach. It expresses at any training time the speed of features in terms of other interpretable quantities, including the backward to feature angle (BFA) $\\theta_{v}$ . ", "page_idx": 2}, {"type": "text", "text": "Theorem 2.1 (Feature speed formula). Let $v\\;\\in\\;[1:L]$ . If $\\begin{array}{r}{\\sum_{\\ell\\leq v}\\eta_{\\ell}\\|\\nabla_{\\ell}\\mathcal{L}\\|_{2}^{2}\\,=\\,0}\\end{array}$ then $\\Dot{f}_{v}\\,=\\,0$ . Otherwise, the (non-oriented) angle $\\theta_{v}$ between $\\dot{f}_{v}$ and $-b_{v}$ is well defined in $[0,\\pi/2[$ and it holds ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\|\\dot{f}_{v}\\|_{2}=\\frac{\\sum_{\\ell\\leq v}\\eta_{\\ell}\\|\\nabla_{\\ell}\\mathcal{L}\\|_{2}^{2}}{\\cos(\\theta_{v})\\cdot\\|b_{v}\\|_{2}}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Proof. By the chain rule, we have $\\begin{array}{r}{\\dot{f}_{v}=\\sum_{\\ell\\leq v}\\frac{\\partial f_{v}}{\\partial w_{\\ell}}\\dot{w}_{\\ell}=-\\sum_{\\ell\\leq v}\\eta\\ell\\frac{\\partial f_{v}}{\\partial w_{\\ell}}\\bigg(\\frac{\\partial\\mathcal{L}}{\\partial w_{\\ell}}\\bigg)^{\\top}}\\end{array}$ . It follows ", "page_idx": 2}, {"type": "equation", "text": "$$\n-b_{v}^{\\top}\\dot{f}_{v}=\\sum_{\\ell\\leq v}\\eta_{\\ell}\\frac{\\partial\\mathcal{L}}{\\partial f_{v}}\\frac{\\partial f_{v}}{\\partial w_{\\ell}}\\Big(\\frac{\\partial\\mathcal{L}}{\\partial w_{\\ell}}\\Big)^{\\top}=\\sum_{\\ell\\leq v}\\eta_{\\ell}\\Big(\\frac{\\partial\\mathcal{L}}{\\partial w_{\\ell}}\\Big)\\Big(\\frac{\\partial\\mathcal{L}}{\\partial w_{\\ell}}\\Big)^{\\top}=\\sum_{\\ell\\leq v}\\eta_{\\ell}\\|\\nabla_{\\ell}\\mathcal{L}\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Clearly, if $\\begin{array}{r}{\\sum_{\\ell\\leq v}\\eta_{\\ell}\\|\\nabla_{\\ell}\\mathcal{L}\\|_{2}^{2}=0}\\end{array}$ then $\\dot{w}_{\\ell}=0$ for $\\ell\\leq v$ and thus $\\dot{f}_{v}=0$ . Otherwise $\\theta_{v}$ is well defined and it holds $\\begin{array}{r}{\\|\\bar{b}_{v}\\|_{2}\\|\\dot{f}_{v}\\|_{2}\\cos(\\theta_{v})=-b_{v}^{\\top}\\dot{f}_{v}=\\sum_{\\ell\\leq v}\\eta_{\\ell}\\|\\nabla_{\\ell}\\mathcal{L}\\|_{2}^{2}}\\end{array}$ and the claim follows. (In terms of the BFK defined below, Eq. (3) is equivalent to $\\begin{array}{r}{b_{v}^{\\top}K_{v}b_{v}=\\sum_{\\ell\\leq v}\\eta_{\\ell}\\|\\nabla_{\\ell}\\mathcal{L}\\|_{2}^{2}}\\end{array}$ , for $v\\in[1:L]$ .) ", "page_idx": 2}, {"type": "text", "text": "To better appreciate the content of Thm. 2.1, let us re-express it in terms of root mean-square (RMS) norms. Let $\\begin{array}{r}{\\dot{\\mathcal{L}}_{\\leq v}:=\\sum_{\\ell\\leq v}\\eta_{\\ell}\\|\\nabla_{\\ell}\\|_{2}^{2}}\\end{array}$ be the contribution to the loss decrease of all the parameters before $f_{v}$ in the forward pass, and note that $\\dot{\\mathcal{L}}_{\\leq L}=\\dot{\\mathcal{L}}$ . Then, the identity (2) rewrites as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\frac{\\lVert\\dot{f}_{v}\\rVert_{\\mathrm{rms}}}{\\dot{\\mathcal{L}}_{\\le v}}=\\frac{1}{\\cos(\\theta_{v})\\cdot m_{v}\\cdot\\lVert b_{v}\\rVert_{\\mathrm{rms}}}=:S_{v}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Here $S_{v}$ can be interpreted as the sensitivity (and, in the terminology of [Chizat et al., 2019], $1/S_{v}$ as the laziness) of the feature $v$ : it is the proportionality factor between loss decay and feature speed. This formula is valid at any training time and involves three key quantities: the scale of the backward pass $\\|b_{v}\\|_{\\mathrm{rms}}$ , the size of the feature $m_{v}$ , and the BFA $\\theta_{v}$ . Let us now build tools to quantify the BFA. ", "page_idx": 3}, {"type": "text", "text": "3 Quantifying the backward-feature angles (BFA) ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Information about the BFA $\\theta_{v}$ can be gained from the Backward to Feature Kernel (BFK). ", "page_idx": 3}, {"type": "text", "text": "Definition 3.1 (Backward to Feature Kernel). For $v\\in[1:L]$ , the BFK is the psd matrix defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\nK_{v}:=\\sum_{\\ell\\leq v}\\eta_{\\ell}\\Big(\\frac{\\partial f_{v}}{\\partial w_{\\ell}}\\Big)\\Big(\\frac{\\partial f_{v}}{\\partial w_{\\ell}}\\Big)^{\\top}\\in\\mathbb{R}^{m_{v}\\times m_{v}}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "By construction, it holds $\\dot{f}_{v}=-K_{v}b_{v}$ . In other words, the BFK takes a backward pass vector as input and returns the (negative of the) feature velocity. For $v=L$ , $K_{v}$ coincides with the Neural Tangent Kernel [Jacot et al., 2018]. We now show how the sprectrum of $K_{v}$ relates to BFA. ", "page_idx": 3}, {"type": "text", "text": "Theorem 3.2. Let $\\lambda_{1}\\geq\\cdot\\cdot>\\lambda_{m_{v}}\\geq0$ be the sorted eigenvalues of $K_{v}$ and let $\\begin{array}{r}{M_{p}:=\\frac{1}{m_{v}}\\sum_{i=1}^{m_{v}}\\lambda_{i}^{p}}\\end{array}$ be its spectral moments. It holds $\\begin{array}{r}{\\frac{\\lambda_{m v}}{\\lambda_{1}}\\leq\\cos(\\theta_{v})\\leq1}\\end{array}$ . Moreover, if $b_{v}$ is Gaussian and independent from $K_{v}$ , then as $m_{v}\\rightarrow\\infty$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\cos(\\theta_{v})\\stackrel{p r.}{\\rightarrow}\\frac{M_{1}}{\\sqrt{M_{2}}}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "as soon as $\\sqrt{M_{2}}/M_{1}$ and $\\sqrt{M_{4}}/M_{2}$ are uniformly bounded (i.e. are upper bounded by some $C>0$ with probability going to $1$ as $m_{v}\\rightarrow\\infty$ ). ", "page_idx": 3}, {"type": "text", "text": "Proof of Thm. 3.2. By the chain rule, it holds ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\dot{f}_{v}=-\\sum_{\\ell\\leq v}\\eta_{\\ell}\\frac{\\partial f_{v}}{\\partial w_{\\ell}}\\Big(\\frac{\\partial\\mathcal{L}}{\\partial w_{\\ell}}\\Big)^{\\top}=-\\sum_{\\ell\\leq v}\\eta_{\\ell}\\frac{\\partial f_{v}}{\\partial w_{\\ell}}\\Big(\\frac{\\partial f_{v}}{\\partial w_{\\ell}}\\Big)^{\\top}\\Big(\\frac{\\partial\\mathcal{L}}{\\partial f_{v}}\\Big)^{\\top},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "hence $\\dot{f}_{v}=-K_{v}b_{v}$ . Denoting $K_{v}^{1/2}$ the unique psd square-root of $K_{v}$ , it follows ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\cos(\\theta_{v})=\\frac{-b_{v}^{\\top}\\dot{f}_{v}}{\\Vert\\dot{f}_{v}\\Vert_{2}\\Vert b_{v}\\Vert_{2}}=\\frac{\\Vert K_{v}^{1/2}b_{v}\\Vert_{2}^{2}}{\\Vert K_{v}b_{v}\\Vert_{2}\\Vert b_{v}\\Vert_{2}}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The first claim follows from Eq. (6) and the worst-case bounds $\\|K_{v}b_{v}\\|_{2}\\;\\;\\leq\\;\\;\\lambda_{1}\\|b_{v}\\|_{2}$ and $\\|K_{v}^{1/2}b_{v}\\|_{2}\\;\\geq\\;\\sqrt{\\lambda_{m_{v}}}\\|b_{v}\\|_{2}$ . The second claim is related to the trace estimation method via random matrix-vector products [Martinsson and Tropp, 2020, Chap. 4]. We assume without loss of generality that $\\mathbf{E}[\\|b_{v}\\|_{2}^{2}]\\,=\\,1$ and by Lem. 3.3, we can write $Z=\\|K_{v}^{1/2}b_{v}\\|_{2}^{2}=a(1+b)$ where $a=\\mathbf{E}[Z|K_{v}]=M_{1}$ and ${\\bf E}[b^{2}]\\rightarrow0$ as $m_{v}\\rightarrow\\infty$ . An analogous decomposition holds for $\\|\\bar{K_{v}}(b_{v})\\|_{2}^{2}$ with $a=M_{2}$ and the second claim follows. \u53e3 ", "page_idx": 3}, {"type": "text", "text": "Lemma 3.3. Let $K\\ \\in\\ \\mathbb{R}^{m\\times m}$ be a (potentially random) psd matrix and $\\begin{array}{r}{a\\,\\sim\\,\\Im(0,\\frac{1}{m}I_{m})}\\end{array}$ be independent. Then $\\mathbf{E}[\\|K a\\|_{2}^{2}\\mid K]=M_{2}(K)$ and $\\begin{array}{r}{\\mathrm{Var}[\\|K a\\|_{2}^{2}\\mid K]=\\frac{2}{m}M_{4}(K)}\\end{array}$ where $M_{p}(K):=$ $\\textstyle{\\frac{1}{m}}\\sum_{i=1}^{m}\\lambda_{i}^{p}$ and $\\lambda_{1},\\dots,\\lambda_{m}\\geq0$ are the eigenvalues of $K$ . ", "page_idx": 3}, {"type": "text", "text": "The second claim expresses the BFA in terms of the spread of the spectrum of the BFK, in an asymptotically exact way. Its assumptions hold at random initialization in the large width limit of typical NNs, provided $f_{v}$ is directly followed by a weight-matrix multiplication in the forward pass, so that $b_{v}$ is the output of a random matrix/vector multiplication. Asymptotic independence can be guaranteed in quite general contexts, see Yang [2020]. For MLP or ResNets with batch-size one, we show in Section 5 that $\\cos(\\theta_{v})$ is tightly related to the conditioning of layer-to-layer Jacobians, studied in the dynamical isometry literature [Pennington et al., 2017]. ", "page_idx": 3}, {"type": "text", "text": "4 Ensuring feature learning in scaled NNs ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "4.1 Properties for scaled NNs ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Consider a sequence of NNs and parameters as in (1) with some diverging architectural parameters such as depth or width. We refer to such a sequence as a scaled NN. In search of the optimal scaling of NNs, it is crucial to understand how HP scalings influence the properties of the training dynamics. In this section, we discuss the following properties: ", "page_idx": 4}, {"type": "text", "text": "(SP) Signal propagation. It holds $\\|f_{v}\\|_{\\mathrm{rms}}=\\Theta(1)$ for $v\\in[1:L-1]$ . ", "page_idx": 4}, {"type": "text", "text": "(FL) Feature learning. It holds $\\|\\dot{f}_{L-1}\\|_{\\mathrm{rms}}=\\Theta(1)$ . ", "page_idx": 4}, {"type": "text", "text": "(LD) Loss decay. It holds $-\\dot{\\mathcal{L}}=\\Theta(1)$ . ", "page_idx": 4}, {"type": "text", "text": "(BC) Balanced contributions. It holds $\\eta_{\\ell}\\|\\nabla_{\\ell}\\mathcal{L}\\|_{2}^{2}=\\Theta\\big(\\eta_{\\ell^{\\prime}}\\|\\nabla_{\\ell^{\\prime}}\\mathcal{L}\\|_{2}^{2}\\big)$ for any $\\ell,\\ell^{\\prime}\\in[1:L]$ . ", "page_idx": 4}, {"type": "text", "text": "We discuss these specific properties because they are amenable to our tools and enforcing them requires $(L-1)+\\mathbf{\\bar{l}}+1+(L-1)=2L$ degrees of freedom, which exactly matches the number of free HPs if one counts one scale HP (such as the variance of the weights) and one LR per block. One may wonder if property (BC) is truly desirable: this is the topic of Section 6, where we adopt a more axiomatic approach and recover, for homogeneous architectures, (a more general version of) property (BC) as a consequence of enforcing gradient stability. Also, while enforcing these properties is reasonable when increasing depth and width, they should be rethought for other asymptotics. ", "page_idx": 4}, {"type": "text", "text": "Property (SP) specifies $L-1$ scale HPs , but leaves the scale of $f_{L}$ free. The reason for not including $f_{L}$ in (SP) is that $\\|f_{L}\\|_{\\mathrm{rms}}=o(1)$ does not lead to vanishing gradient in general, so this behavior should not be excluded a priori. How should one then fix the scale of the output? The next proposition shows that for property (FL) to hold, the quantity that should be suitably normalized is the norm of the backward pass. ", "page_idx": 4}, {"type": "text", "text": "Proposition 4.1. A scaled NN (1) satisfies $(F L)$ , $(L D)$ , and $(B C)$ if and only if ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\|b_{L-1}\\|_{\\mathrm{rms}}=\\Theta\\Big(\\frac{1}{m_{L-1}\\cdot\\cos(\\theta_{L-1})}\\Big)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\forall\\ell\\in[1\\!:\\!L],\\ \\eta_{\\ell}=\\Theta\\Big(\\frac{1}{L\\|\\nabla_{\\ell}\\mathcal{L}\\|_{2}^{2}}\\Big).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "sPuromo tf.o  bPer obpaelratnyc (eLd, Dt)h irse lqeuairdes st $\\begin{array}{r}{\\sum_{\\ell=1}^{L}\\eta_{\\ell}\\|\\nabla_{\\ell}\\mathcal{L}\\|_{2}^{2}=-\\dot{\\mathcal{L}}=\\Theta(1)}\\end{array}$ p aerntdy rreeqquuiirreess the terms in the $\\boldsymbol{(\\mathrm{{FL})}}$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\|\\dot{f}_{L-1}\\|_{\\mathrm{rms}}=\\frac{\\sum_{\\ell=1}^{L-1}\\eta_{\\ell}\\|\\nabla_{\\ell}\\mathcal{L}\\|_{2}^{2}}{\\cos(\\theta_{L-1})\\cdot m_{L-1}\\cdot\\|b_{L-1}\\|_{\\mathrm{rms}}}=\\Theta(1)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "which leads to (7). Conversely, it is clear that Eq. (8) and (7) imply (FL), (LD) and (BC). ", "page_idx": 4}, {"type": "text", "text": "4.2 Towards automatic HP scaling ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The criterion of Prop. (4.1), complemented with the property (SP), suggest a method to automatically adjust the scales and learning rates in any architecture. In general, properties (SP), (FL), (BC) and (LD) can be enforced as follows: ", "page_idx": 4}, {"type": "text", "text": "\u2022 (SP): Forward layer normalization. Enforcing property (SP) can be done along with the computation of the forward pass, this is the usual layer normalization. \u2022 (FL): Backward layer normalization. Provided $\\theta_{L-1}$ is known or measured, Eq. (7) can be enforced via a backward analog to layer normalization: one inserts a scaling factor in the forward pass between $f_{L-1}$ and $f_{L}$ , adjusted so that Eq. (7) holds. ", "page_idx": 4}, {"type": "text", "text": "\u2022 (BC) & (LD): Scale invariant learning rates. Directly tune the LRs via Eq. (8). ", "page_idx": 4}, {"type": "text", "text": "We refer to the resulting scaling as FSC as it normalizes the Forward pass, the Sensitivities and the Contributions. Let us make some observations regarding the scale invariant LRs: ", "page_idx": 4}, {"type": "text", "text": "\u2022 Link with Polyak step-size. In convex optimization, to minimize a convex and Lipschitz continuous function $f^{^{\\prime}:\\mathbb{R}^{d}\\,\\stackrel{}{\\to}\\,\\mathbb{R}}$ such that $\\textstyle\\operatorname*{min}_{x\\in\\mathbb{R}^{d}}f(x)=0$ , the Polyak-step-size [Polyak, 1987, Hazan and Kakade, 2019] for the GD algorithm $x_{t+1}=x_{t}-\\eta_{t}\\nabla f(x_{t})$ is given by $\\begin{array}{r}{\\eta_{t}=\\frac{f\\left(x_{t}\\right)}{\\|\\nabla f\\left(x_{t}\\right)\\|_{2}^{2}}}\\end{array}$ . With this step-size, GD achieves the optimal convergence rate for first order methods over the class of convex and Lipschitz functions. Eq. (8) require a layerwise version of this step-size schedule. ", "page_idx": 5}, {"type": "text", "text": "\u2022 Interplay with adaptive methods (Adagrad [Duchi et al., 2011], ADAM [Kingma and Ba, 2015]). Adaptive gradient method typically divide the gradient by a quantity which grows linearly rather than quadratically with the norm of the gradient, such as $\\begin{array}{r}{\\dot{\\delta W_{\\ell}}=-\\tilde{\\eta_{\\ell}}\\cdot\\frac{\\nabla_{\\ell}\\mathcal{L}}{\\|\\nabla_{\\ell}\\mathcal{L}\\|_{2}}}\\end{array}$ . For such an algorithm, properties (BC) and (LD), suggests the LR $\\begin{array}{r}{\\tilde{\\eta}_{\\ell}=\\Theta\\big(\\frac{1}{L\\cdot\\|\\nabla_{\\ell}\\mathcal{L}\\|_{2}}\\big)}\\end{array}$ , in place of Eq. (8). ", "page_idx": 5}, {"type": "text", "text": "\u2022 Scale invariance and $-2$ homogeneity. These LRs arise naturally when one wants to make the gradient descent invariant to how scale is enforced (via initialization scale or via scaling factors). We show in App. C that any choice of LR that leads to this invariance must be a positively homogeneous function of the layer-wise gradient of degree $-2$ , as in Eq. (8). We also show in Prop. 6.2 that these LRs make homogeneous architectures invariant to the choice of layer-wise scalings $\\sigma_{1},\\ldots,\\sigma_{L}$ , given a fixed global scale $\\textstyle\\prod_{\\ell=1}^{L}\\sigma_{\\ell}$ . ", "page_idx": 5}, {"type": "text", "text": "5 Scaling width and depth of MLPs and ResNets ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "5.1 BFA for single input MLPs and ResNets at initialization ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Multilayer Perceptron Consider a ReLU MLP architecture with a single input $x=g_{0}\\in\\mathbb{R}^{d}$ and a forward pass given, for $\\ell\\in[1:L-1]$ , by ", "page_idx": 5}, {"type": "equation", "text": "$$\nf_{\\ell}=W_{\\ell}g_{\\ell-1},\\qquad\\quad g_{\\ell}=\\phi(f_{\\ell}),\\qquad\\quad f_{L}=W_{L}g_{L-1},\\qquad\\quad\\mathcal{L}=\\mathrm{loss}(f_{L})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\phi(u)=\\operatorname*{max}\\{0,u\\}$ is the ReLU nonlinearity and acts entrywise on vectors. The architecture HPs are the input width $m_{0}=d$ , the widths of the hidden layers $m_{1}=\\cdot\\cdot\\cdot=m_{L-1}=m$ (assumed equal), the output width $m_{L}=k$ . The trainable parameters are $\\forall\\ell\\in[1:L]$ , $W_{\\ell}\\in\\mathbb{R}^{m_{\\ell}\\times m_{\\ell-1}}$ . Such NNs are of the form (1) and are thus covered by Thm. 2.1. Let us study their properties at random initialization under the following assumptions: ", "page_idx": 5}, {"type": "text", "text": "(H1) the weights $W_{\\ell}$ are independent $\\mathcal{N}(0,\\sigma_{\\ell}^{2})$ random variables for $\\ell\\in[1:L]$ .   \n(H2) either $k=\\Theta(1)$ or the loss is linear. ", "page_idx": 5}, {"type": "text", "text": "In this setting, the following statements gather consequences of results from the literature on random NNs and of Thm. 3.2 to derive the scale of the forward and backward passes and the BFA. We require (H2) as a technical assumption to avoid dealing with cases where $b_{L}$ strongly depends on the forward pass, where different scalings may arise2. ", "page_idx": 5}, {"type": "text", "text": "In what follows we write $A=\\Theta(B)$ when there exists $c,C>0$ independent of $d,m,k,L,\\|x\\|_{2}$ and $\\|b_{L}\\|_{2}$ such that the probability that $A/B\\in[c,C]$ goes to 1 in the specified asymptotic. The new result in the following proposition is the BFA estimate, which relies crucially on a delicate computation due to [Jelassi et al., 2023]. ", "page_idx": 5}, {"type": "text", "text": "Proposition 5.1 (Large width and depth MLP). Assume (H1-2) and for $\\ell\\in\\left[1:L-1\\right]$ , let $\\sigma_{\\ell}=$ $\\sqrt{2/m_{\\ell-1}}$ . As $m\\rightarrow\\infty,$ , it holds ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|f_{v}\\|_{\\mathrm{rms}}=\\Theta(\\|x\\|_{\\mathrm{rms}}),\\qquad\\qquad\\qquad\\|b_{v}\\|_{2}=\\Theta(\\sqrt{m}\\,\\sigma_{L}\\,\\|b_{L}\\|_{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Moreover, $i f(B C)$ holds then $\\cos(\\theta_{v})=\\Theta(v^{-1/2})$ . ", "page_idx": 5}, {"type": "text", "text": "ResNets Consider now a ResNet with a branch scale parameter $\\beta\\in[0,1]$ , as in Li et al. [2021]: with a single input $x=f_{0}\\in\\mathbb{R}^{d}$ , the forward pass is given, for $\\ell\\in[2:L-1]$ , by ", "page_idx": 5}, {"type": "equation", "text": "$$\nf_{1}=W_{1}x,\\ \\ \\ \\ f_{\\ell}=\\sqrt{1-\\beta^{2}}f_{\\ell-1}+\\beta W_{\\ell}\\phi(f_{\\ell-1}),\\ \\ \\ \\ f_{L}=W_{L}f_{L-1},\\ \\ \\ \\ \\mathcal{L}=\\mathrm{loss}(f_{L})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "2Say, if $\\begin{array}{r}{\\mathrm{loss}(f)=\\frac{1}{2}\\|f\\|_{2}^{2}}\\end{array}$ , we have $\\|b_{L-1}\\|_{\\mathrm{rms}}=\\|W_{L}^{\\top}W_{L}f_{L-1}\\|_{\\mathrm{rms}}=\\Theta(\\sigma_{L}\\operatorname*{max}\\{1,\\sqrt{k/m}\\}\\|b_{L}\\|_{2})$ (by Lem. 3.3 and properties of the Marcenko-Pastur law), while under (H2) we have $\\|b_{L-1}\\|_{\\mathrm{rms}}=\\Theta(\\sigma_{L}\\|b_{L}\\|_{2})$ . ", "page_idx": 5}, {"type": "image", "img_path": "wsHMb4J2o9/tmp/04a86bb86d685fab8e9f33f80b236f391914bc8f3a6ab8551dd8050e0c527d09.jpg", "img_caption": ["(a) BFA vs. layer $\\left[L=200\\right]$ ) "], "img_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "wsHMb4J2o9/tmp/e356751797c57ebf6337a23834be4194717a64e096ded0df7de8c2e968f105a8.jpg", "img_caption": ["(b) Output BFA vs. depth "], "img_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "wsHMb4J2o9/tmp/f986770c9980a33d782a468e51fef93d01cfa4fb7562759e67d822889a908c89.jpg", "img_caption": ["(c) Output BFA vs. scale "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 1: Backward-Feature Angle (BFA) $\\theta_{v}$ observed at initialization in MLPs $\\begin{array}{r}{\\beta=1}\\end{array}$ ) and ResNets (width $m=200)$ ), for a few random realizations. (a) for all architectures, BFA $\\theta_{v}$ varies in the first few layers and then stabilizes. (b) BFA at output layer $\\theta_{L-1}$ is asymptotically independent of depth, with a non-trivial angle only when $\\beta\\,\\propto\\,^{1}\\!/\\!\\sqrt{L}$ (same color scheme as (a)). (c) for a branch scale $\\beta=c/\\sqrt{L}$ , the factor $c$ directly determines asymptotic output BFA $\\theta_{L-1}$ (averaged over 5 draws). ", "page_idx": 6}, {"type": "text", "text": "where $\\phi(u)=u$ in our theoretical results. The architecture HPs are the input width $m_{0}=d$ , the widths of the hidden layers $m_{1}=\\cdot\\cdot\\cdot=m_{L-1}=m$ , the output width $m_{L}\\,=\\,k$ . The trainable parameters are $\\forall\\ell\\in[1:\\bar{L}]$ , $W_{\\ell}\\in\\mathbb{R}^{m_{\\ell}\\times m_{\\ell-1}}$ . When $\\beta=1$ , we recover a MLP. ", "page_idx": 6}, {"type": "text", "text": "Here we limit ourselves to the case of linear activation where we can directly apply a result from [Marion and Chizat, 2024] to estimate the BFA. We believe that the same result and proof technique extend to the ReLU activation and other variants of ResNets; these extensions are left to future work. ", "page_idx": 6}, {"type": "text", "text": "Proposition 5.2 (Large width and depth linear ResNet). Assume (H1-2), let $\\phi(x)=x$ , $\\beta=O(1/\\sqrt{L})$ and for $\\ell\\in[1:L-1]$ , let $\\sigma_{\\ell}=\\Theta(\\bar{1}/\\sqrt{m_{\\ell-1}})$ . As $m\\rightarrow\\infty$ it holds: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|f_{\\ell}\\|_{\\mathrm{rms}}=\\Theta(\\|x\\|_{\\mathrm{rms}}),\\qquad\\qquad\\qquad\\|b_{\\ell}\\|_{2}=\\Theta\\Big(\\sqrt{m}\\sigma_{L}\\|b_{L}\\|_{2}\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Moreover, $i f(B C)$ holds then $\\cos(\\theta_{v})=\\Theta(1)$ . ", "page_idx": 6}, {"type": "text", "text": "Numerical experiments We consider3 one GD step in the model (12) with ReLU nonlinearity, without training $W_{1}$ (input dimension $d=10$ , output dimension $k=1$ , master LR $\\delta t=0.001)$ ). Fig. 1, represent BFA, computed via $\\theta_{v}\\approx\\operatorname{arccos}(\\dot{-}b_{v}^{\\top}\\delta f_{v})$ where $\\delta f_{v}$ is the change of feature $f_{v}$ after one GD step. The results are consistent with Prop. 5.1 and 5.2. Interestingly, the l\u221aast plot suggests that there exists a function $\\varphi:\\mathbb{R}_{+}\\to\\lvert0,\\pi/2\\rvert$ such that for a branch scale $\\dot{\\boldsymbol{\\beta}}=c/\\sqrt{L}$ , the BFA converges to $\\varphi(c)$ (it can be conjectured numerically that $\\cos(\\varphi(c))\\approx c^{-1/2}$ for $c\\gg1$ ). ", "page_idx": 6}, {"type": "text", "text": "5.2 Characterizing HP scalings for MLPs ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Let us now discuss specific choices of HP scalings for single-input MLP architectures as in Eq. (10) (or Eq. (12) with $\\beta=1$ ) and at initialization. We consider $6\\:\\mathrm{HPs}$ : the scale of initialization $\\sigma_{1}$ and LR $\\eta_{1}$ of the input layer, the scale $\\sigma_{\\mathrm{hid}}:=\\sigma_{2}=\\cdots=\\sigma_{L-1}$ and LRs $\\eta_{\\mathrm{hid}}:=\\eta_{2}=\\cdot\\cdot\\cdot=\\eta_{L-1}$ of the hidden layers, and the scale $\\sigma_{L}$ and LR $\\eta_{L}$ of the output layer. The HP scalings mentioned in the next theorem are the following (see Table 1): ", "page_idx": 6}, {"type": "text", "text": "\u2022 NTK: the standard scaling with LRs adjusted to satisfy (LD) and (BC) [Jacot et al., 2018]; \u2022 $\\mathbf{MF+}\\mu\\mathbf{P}$ : the scaling proposed in [Jelassi et al., 2023] constructed by imposing the so-called \u201cmean-field\u201d output scale $\\sigma_{L}\\propto1/m$ and then enforcing (FL) by adjusting the learning rates; \u2022 FSC: the HP scaling singled-out by Prop. 5.3, obtained by adjusting the Forward scales, Sensitivities, and Contributions. ", "page_idx": 6}, {"type": "text", "text": "The properties of HP scalings depend on $\\Vert x\\Vert_{2}$ and $\\|b_{L}\\|_{2}$ . We consider two typical settings: ", "page_idx": 6}, {"type": "text", "text": "\u2022 (Dense) Where $\\|x\\|_{2}={\\sqrt{d}}$ and $\\begin{array}{r}{\\|b_{L}\\|_{2}=\\frac{1}{\\sqrt{k}}}\\end{array}$ . This is representative of a dense whitened input and a RMS loss $\\mathrm{loss}(f_{L})~=~\\|f_{L}\\,-\\,y\\|_{2}^{2}/k$ for some dense signal $y\\,\\in\\,\\mathbb{R}^{k}$ with $\\|\\bar{y}\\|_{\\mathrm{rms}}=\\Theta(1)$ as, e.g., in image generation applications. ", "page_idx": 6}, {"type": "text", "text": "Table 1: HP scalings for MLPs under the dense setting (for the sparse setting, replace $k$ and $d$ by 1). For $L$ fixed, both $\\mathbf{M}\\mathbf{F}{+}\\mu\\mathbf{P}$ and FSC coincide with $\\mu\\mathrm{P}.$ . Values in red are exact, the others are up to a multiplicative factor in $\\Theta(1)$ . ", "page_idx": 7}, {"type": "table", "img_path": "wsHMb4J2o9/tmp/63db73f5fc2643844973f02d990b9c9be5f5d1ddb1be751475e6e8fc9d85f293.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 2: FSC scalings identified in Prop. 5.4 for ResNets. All HPs are specified up to a multiplicative factor in $\\Theta(1)$ . When $\\beta=\\Theta(L^{-1/2})$ and $k=d=\\Theta(1)$ , these scalings coincide with the so-called \u201cdepth $\\mu\\mathrm{P}^{\\circ}$ introduced in [Bordelon et al., 2023] and also studied in Yang et al. [2023b]. ", "page_idx": 7}, {"type": "table", "img_path": "wsHMb4J2o9/tmp/a997cc8ff609fed3b18b1a868adfb94bbabed487add03a18686233ac74b1b047.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "\u2022 (Sparse) Where $\\|x\\|_{2}=1$ and $\\|b_{L}\\|_{2}=1$ . This is representative of a one-hot encoding input and the multiclass logistic loss (aka cross-entropy where $\\|b_{L}\\|_{2}=\\Theta(\\log(k)))$ . This setting is typical of natural language processing tasks. ", "page_idx": 7}, {"type": "text", "text": "The scalings are reported in Table 1. We have also introduced scalings in terms of output width $k$ for NTK and $\\mathbf{M}\\mathbf{F}+\\mu\\mathbf{P}$ to ensure a non-degenerate behavior as $k\\gg1$ , although these are generally not written in the literature. ", "page_idx": 7}, {"type": "text", "text": "Proposition 5.3 (MLP scalings). Under the assumptions of Prop. 5.1, the following hold at random initialization: ", "page_idx": 7}, {"type": "text", "text": "(i) The scaling $M F\\!+\\!\\mu P$ satisfies $(S P)$ , $(B C)$ , $(F L)$ but not $(L D)$ ;   \n(ii) The scaling NTK satisfies (SP), (BC), (LD) but not $(F L)$ ;   \n(iii) Properties (SP), (BC), (LD), (FL) hold if and only if the scaling is FSC. ", "page_idx": 7}, {"type": "text", "text": "This theorem identifies the new HP scaling FSC for deep ReLU MLP where the scale of the output layer depends on the depth. We compare empirically the sensitivities (Eq. (4)) of the various scalings in Fig. 2, and the results are consistent with theory. Finally, let us mention that even though FSC fixes some degeneracies of deep MLPs, other problems arise when considering multiple inputs, such as degeneracy of the conjugate kernel and NTK [Hayou et al.], which make ReLU MLPs a fundamentally flawed model at large depth. Hence, the analysis of large depth scalings for MLPs is mostly of theoretical interest. ", "page_idx": 7}, {"type": "text", "text": "5.3 Characterizing HP scalings for ResNets ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We now discuss HP scalings for single-input ResNets (Eq. (12)) with $\\beta=O(1/\\sqrt{L})$ . ", "page_idx": 7}, {"type": "text", "text": "Proposition 5.4 (ResNets scalings). Take $\\beta=O(1/\\sqrt{L})$ , consider the same 6 degrees of freedom as in the previous section and assume that the conclusions of Prop. 5.2 holds. Then properties $(S P)$ , (BC), (LD) and (FL) hold at initialization if and only if the scalings are those given in Table 2. ", "page_idx": 7}, {"type": "image", "img_path": "wsHMb4J2o9/tmp/03cf2c9827cc09d0fba43c567a17d05aae85831f2b6863e6394bc7ac8f958d32.jpg", "img_caption": ["(a) ReLU MLP $\\lvert\\beta=1$ ) "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "wsHMb4J2o9/tmp/3a95617cd124684b180b106a82d762b2b7061fe6e9f9152c2f3e2282ddf6b5b4.jpg", "img_caption": ["(b) ReLU ResNet "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 2: Sensitivities $S_{L-1}$ (see Eq. (4)) of the last layer of activations $(g_{L-1}$ in the MLP and $f_{L-1}$ in the ResNet) computed via the formula $\\lVert\\delta f_{L-1}\\rVert_{\\mathrm{rms}}/|\\delta\\mathcal{L}|$ where $\\delta$ denotes the change after one GD step (master learning rate $\\delta t=0.01$ , $d=10$ , $n=1$ input sample on the unit sph\u221aere and $k=1$ ). (a) ReLU MLP of width $m=400$ . From our t\u221aheory we have for NTK, $S=\\Theta(1\\bar{/}{\\sqrt{m}})$ (close to 0 and constant with depth); for $\\mathbf{MF}\\mathbf{+}\\mu\\mathbf{P}\\mathbf{\\Lambda}S=\\Theta(\\sqrt{L})$ and for the FSC $S=\\Theta(1)$ (b) ReLU ResNet of width $m=400$ : for both choices of branch scale, the sensitivities is stable around a nonzero value. ", "page_idx": 8}, {"type": "text", "text": "6 Minimal desiderata and stability under homogeneity ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "It is not clear a priori why the property (BC) studied in Section 4should be enforced. In this section, we consider homogeneous architectures, such as ReLU MLPs, and show that a slightly more general version of (BC) is a related to a notion of gradient stability. ", "page_idx": 8}, {"type": "text", "text": "6.1 Stability and backward speed formula ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "For a general architecture of the form Eq. (1), consider the following stability property (S), which is necessary if one wants to have comparable behavior between the first GD step and the next. It is related to the usual notion of smoothness in optimization: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\frac{\\mathrm{d}}{\\mathrm{d}t}\\nabla_{\\ell}\\mathcal{L}\\|_{2}/\\|\\nabla_{\\ell}\\mathcal{L}\\|_{2}=O(1)\\;\\mathrm{for}\\;\\ell\\in[1:L].}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "We will study this property in a ReLU MLP with a single input as in Eq. (10) (the extension to linear ResNets is simple as only the BFAs change). In this case we have $\\bar{\\nabla}_{\\ell}\\mathcal{L}=b_{\\ell}g_{\\ell-1}^{\\top}$ and thus $\\|\\nabla_{\\ell}\\mathcal{L}\\|_{F}=\\|b_{\\ell}\\|_{2}\\|g_{\\ell-1}\\|_{2}$ . It follows ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\frac{\\|\\frac{\\mathrm{d}}{\\mathrm{d}t}\\nabla_{\\ell}\\mathcal{L}\\|_{F}}{\\|\\nabla_{\\ell}\\mathcal{L}\\|_{F}}\\leq\\frac{\\|\\dot{b}_{\\ell}\\|_{2}\\|g_{\\ell-1}\\|_{2}+\\|b_{\\ell}\\|_{2}\\|\\dot{g}_{\\ell-1}\\|_{2}}{\\|b_{\\ell}\\|_{2}\\|g_{\\ell-1}\\|_{2}}=\\frac{\\|\\dot{b}_{\\ell}\\|_{2}}{\\|b_{\\ell}\\|_{2}}+\\frac{\\|\\dot{g}_{\\ell-1}\\|_{2}}{\\|g_{\\ell-1}\\|_{2}}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "We can thus ensure the gradient stability by ensuring, for all $\\ell\\in[1:L]$ , (FS) Forward stability. It holds \u2225g\u02d9\u2113\u22121\u22252 $\\begin{array}{r}{{\\frac{\\|{\\dot{g}}\\ell-1\\|_{2}}{\\|g_{\\ell-1}\\|_{2}}}=O(1)}\\end{array}$ for $\\ell\\in[1:L]$ , and (BS) Backward stability. It holds $\\begin{array}{r}{\\frac{\\|\\dot{b}_{\\ell}\\|_{2}}{\\|b_{\\ell}\\|_{2}}=O(1)}\\end{array}$ for $\\ell\\in[1:L]$ . ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "We focus on these simpler desiderata (FS) and (BS) instead of (S) for the rest of the discussion. To estimate $\\dot{b}_{v}$ , we rely on a \u201cbackward\u201d version of the feature speed formula that holds in 1- homogeneous NNs. ", "page_idx": 8}, {"type": "text", "text": "Proposition 6.1 (Backward speed formula). Consider a general architecture of the form (1), take $v\\in[1:L]$ and assume that the map $f_{v}\\rightarrow f_{L}$ is positively 1-homogeneous4. $I f\\!-\\!f_{L}^{\\top}\\nabla^{2}\\mathrm{loss}[f_{L}]\\dot{f}_{L}\\;+$ $\\begin{array}{r}{\\sum_{\\ell>v}\\eta_{\\ell}\\|\\nabla_{\\ell}\\mathcal{L}\\|_{2}^{2}=0}\\end{array}$ then $\\dot{b}_{v}=0$ . Otherwise, the (non-oriented) angle $\\tilde{\\theta}_{v}$ between $f_{v}$ and $b_{v}$ is well defined in $[0,\\pi/2[$ and it holds ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\|\\dot{b}_{v}\\|_{2}=\\frac{-f_{L}^{\\top}\\nabla^{2}\\mathrm{loss}[f_{L}]\\dot{f}_{L}+\\sum_{\\ell>v}\\eta_{\\ell}\\|\\nabla_{\\ell}\\mathcal{L}\\|_{2}^{2}}{\\|f_{v}\\|_{2}\\cos(\\tilde{\\theta}_{v})}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "4For Euler\u2019s identity to hold, we also assume that its selection [Bolte and Pauwels, 2020] is 0-homogeneous. ", "page_idx": 8}, {"type": "text", "text": "In the context of ReLU MLPs with a linear loss, we have by differentiating the back-propagation recursion and noticing that all terms involving $\\phi^{\\prime\\prime}$ are zero almost surely5 that: ", "page_idx": 9}, {"type": "equation", "text": "$$\n{\\dot{b}}_{v}=\\sum_{\\ell>v}\\eta_{\\ell}{\\Big(}{\\frac{\\partial g_{\\ell-1}}{\\partial f_{v}}}{\\Big)}^{\\top}g_{\\ell-1}b_{\\ell}^{\\top}b_{\\ell}=\\sum_{\\ell>v}\\eta_{\\ell}\\|b_{\\ell}\\|_{2}^{2}{\\Big(}{\\frac{\\partial g_{\\ell-1}}{\\partial f_{v}}}{\\Big)}^{\\top}{\\Big(}{\\frac{\\partial g_{\\ell-1}}{\\partial f_{v}}}{\\Big)}f_{v}={\\tilde{K}}_{v}f_{v}\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "where the last expression defines $\\tilde{K}_{v}$ . Reasoning as in Thm. (3.2), since $f_{v}$ is Gaussian at initialization and noticing that $\\tilde{K}_{v}$ has a structure similar to that of $K_{L-v}$ , we have that $\\cos(\\tilde{\\theta}_{v})=\\Theta(\\sqrt{L-v})$ , see the details in Lem. A.1. We can thus estimate $\\dot{b}_{v}$ just as well as $\\dot{f}_{v}$ . ", "page_idx": 9}, {"type": "text", "text": "6.2 Scale invariance for homogeneous architectures ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Homogeneous architectures such as ReLU MLP satisfy scale invariance properties that are important to take into account in our discussion. The following result presents a general invariance under blockwise rescaling, provided one uses scale-invariant LRs. This is related to known invariance results under global rescaling for scale invariant losses [Van Laarhoven, 2017, Li et al., 2022, Wan et al., 2020]. ", "page_idx": 9}, {"type": "text", "text": "Proposition 6.2 (Invariance under block-wise rescaling). Consider a function $f_{L}(w_{1},\\dots,w_{L})$ (the NN, in our context) which is separately positively 1-homogeneous in each of its blocks of parameters $w_{\\ell}\\in\\mathbb{R}^{p_{\\ell}}$ . Let $\\theta_{0}=(w_{1}(0),\\dots,w_{L}(0))$ and let $\\tilde{{\\boldsymbol{\\theta}}}_{0}=\\sigma\\odot{\\boldsymbol{\\theta}}_{0}:=\\left(\\sigma_{1}w_{1}(0),\\dots,\\sigma_{L}w_{L}(0)\\right)$ for some scale factors $\\sigma\\in\\mathbb{R}_{+}^{L}$ . Let $\\theta(t)$ and $\\tilde{\\theta}(t)$ be the iterates of $G D$ on $\\mathcal{L}:\\theta\\mapsto\\mathrm{loss}(f_{L}(\\theta))$ with $L R$ satisfying $\\eta_{\\ell}(t)\\|\\nabla_{\\ell}\\mathcal{L}(\\theta(t))\\|_{2}^{2}=\\tilde{\\eta}_{\\ell}(t)\\|\\nabla_{\\ell}\\mathcal{L}(\\tilde{\\theta}(t))\\|_{2}^{2}$ and starting from $\\theta_{0}$ and $\\widetilde{\\theta}_{0}$ respectively. If $\\textstyle\\prod_{\\ell=1}^{L}\\sigma_{\\ell}=1$ then $\\tilde{\\theta}(t)=\\sigma\\odot\\theta(t)$ for all $t\\geq1$ . ", "page_idx": 9}, {"type": "text", "text": "6.3 Characterization of admissible scalings for ReLU MLPs ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In view of Prop. 6.2, for homogeneous architectures, one can ignore (SP) since any GD dynamics is equivalent to a dynamics where (SP) holds at initialization. However, if the scale of the forward pass is not normalized, (FL) needs now to be adapted to a scale-free version, as follows: ", "page_idx": 9}, {"type": "text", "text": "(RFL) Relative feature learning. It holds $\\|\\dot{f}_{L-1}\\|_{2}/\\|f_{L-1}\\|_{2}=\\Theta(1).$ ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We are finally in position to gather all these insights and characterize all admissible scalings for ReLU MLPs, i.e. scalings that satisfy the minimal desiderata (RFL), (LD), (FS) and (BS) at initialization. ", "page_idx": 9}, {"type": "text", "text": "Theorem 6.3 (Minimal desiderata for MLPs). Consider a ReLU MLP with 6 degrees of freedom: three initialization scales $\\sigma_{1},\\sigma_{\\mathrm{hid}},\\sigma_{L}$ and three LRs $\\eta_{1},\\eta_{\\mathrm{hid}},\\eta_{L}$ . Assume $\\|b_{L}\\|_{2}=\\|g_{0}\\|_{\\mathrm{rms}}=1$ and a linear loss for simplicity. Then the minimal desiderata $(R F L),\\,(L D),\\,(F S)$ and $(B S)$ hold at initialization in the limit $m\\rightarrow\\infty$ then $L\\rightarrow\\infty$ if and only if ", "page_idx": 9}, {"type": "equation", "text": "$$\n(\\sqrt{d}\\sigma_{1})\\cdot(\\sqrt{m/2}\\sigma_{\\mathrm{hid}})^{L-2}\\cdot\\sigma_{L}=\\Theta(\\sqrt{L}/m),\\quad C_{1}+C_{\\mathrm{hid}}=\\Theta(1)\\quad\\,a n d\\quad C_{L}=O(1),\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "where C1 = \u03b71\u2225\u22071L\u222522, Chid = \u2113L=\u221221 \u03b7 $\\begin{array}{r}{C_{\\mathrm{hid}}\\,=\\,\\sum_{\\ell=2}^{L-1}\\eta_{\\mathrm{hid}}\\|\\nabla_{\\ell}\\mathcal{L}\\|_{2}^{2}\\,}\\end{array}$ and $C_{L}\\,=\\,\\eta_{L}\\|\\nabla_{L}\\mathcal{L}\\|_{2}^{2}$ . In particular, the scaling FSC (Table $^{\\,l}$ ) satisfies these desiderata. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Starting from the feature speed formula, our approach allows to conveniently recover and characterize in an elementary fashion certain properties of existing HP scalings and to discover new ones, with essentially all the technical difficulty contained in the estimation of the BFA. The limitations of our approach are related to the blind spots of Thm. (2.1): it can only quantify feature speed for (S)GD (and does not apply to variants in its current form) and at \u201ccut nodes\u201d in the NN architecture, where all the signal goes through (in particular, it does not apply inside the blocks of a ResNet). ", "page_idx": 9}, {"type": "text", "text": "In future works, besides removing these limitations, it would be interesting to have a better understanding of the BFA, both from a quantitative and a qualitative viewpoint. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was supported in part by the International Centre for Theoretical Sciences (ICTS) for participating in the meeting - Data Science: Probabilistic and Optimization Methods (code:ICTS/dspom2023/7) ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-parameterization. In International conference on machine learning, pages 242\u2013252. PMLR, 2019.   \nAlexander Atanasov, Blake Bordelon, and Cengiz Pehlevan. Neural networks as kernel learners: The silent alignment effect. In International Conference on Learning Representations, 2021.   \nAristide Baratin, Thomas George, C\u00e9sar Laurent, R Devon Hjelm, Guillaume Lajoie, Pascal Vincent, and Simon Lacoste-Julien. Implicit regularization via neural feature alignment. In International Conference on Artificial Intelligence and Statistics, pages 2269\u20132277. PMLR, 2021.   \nJ\u00e9r\u00f4me Bolte and Edouard Pauwels. A mathematical model for automatic differentiation in machine learning. Advances in Neural Information Processing Systems, 33:10809\u201310819, 2020.   \nBlake Bordelon, Lorenzo Noci, Mufan Bill Li, Boris Hanin, and Cengiz Pehlevan. Depthwise hyperparameter transfer in residual networks: Dynamics and scaling limit. arXiv preprint arXiv:2309.16620, 2023.   \nL\u00e9na\u00efc Chizat and Francis Bach. On the global convergence of gradient descent for over-parameterized models using optimal transport. Advances in Neural Information Processing Systems, 31, 2018.   \nL\u00e9naic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming. Advances in Neural Information Processing Systems, 32, 2019.   \nSimon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes overparameterized neural networks. In International Conference on Learning Representations, 2018.   \nJohn Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of machine learning research, 12(7), 2011.   \nBoris Hanin. Which neural net architectures give rise to exploding and vanishing gradients? Advances in neural information processing systems, 31, 2018.   \nBoris Hanin and Mihai Nica. Products of many large random matrices and gradients in deep neural networks. Communications in Mathematical Physics, 376(1):287\u2013322, 2020.   \nBoris Hanin and David Rolnick. How to start training: The effect of initialization and architecture. Advances in Neural Information Processing Systems, 31, 2018.   \nSoufiane Hayou, Arnaud Doucet, and Judith Rousseau. On the impact of the activation function on deep neural networks training. In International conference on machine learning, pages 2672\u20132680.   \nSoufiane Hayou, Eugenio Clerico, Bobby He, George Deligiannidis, Arnaud Doucet, and Judith Rousseau. Stable ResNet. In International Conference on Artificial Intelligence and Statistics, pages 1324\u20131332. PMLR, 2021.   \nElad Hazan and Sham Kakade. Revisiting the Polyak step size. arXiv preprint arXiv:1905.00313, 2019.   \nArthur Jacot, Franck Gabriel, and Cl\u00e9ment Hongler. Neural Tangent Kernel: Convergence and generalization in neural networks. Advances in Neural Information Processing Systems, 31, 2018.   \nSamy Jelassi, Boris Hanin, Ziwei Ji, Sashank J Reddi, Srinadh Bhojanapalli, and Sanjiv Kumar. Depth dependence of $\\mu$ -P learning rates in ReLU MLPs. arXiv preprint arXiv:2305.07810, 2023.   \nDiederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations (ICLR), San Diega, CA, USA, 2015.   \nYann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436\u2013444, 2015.   \nMufan Li, Mihai Nica, and Dan Roy. The future is log-Gaussian: ResNets and their infinite-depth-and-width limit at initialization. Advances in Neural Information Processing Systems, 34:7852\u20137864, 2021.   \nZhiyuan Li, Srinadh Bhojanapalli, Manzil Zaheer, Sashank Reddi, and Sanjiv Kumar. Robust training of neural networks using scale invariant architectures. In International Conference on Machine Learning, pages 12656\u201312684. PMLR, 2022.   \nZenan Ling and Robert C. Qiu. Spectrum concentration in deep residual learning: a free probability approach. IEEE Access, 7:105212\u2013105223, 2019.   \nYizhang Lou, Chris E Mingard, and Soufiane Hayou. Feature learning and signal propagation in deep neural networks. In International Conference on Machine Learning, pages 14248\u201314282. PMLR, 2022.   \nPierre Marion and L\u00e9na\u00efc Chizat. Deep linear networks for regression are implicitly regularized towards flat minima. arXiv preprint arXiv:2405.13456, 2024.   \nPierre Marion, Adeline Fermanian, G\u00e9rard Biau, and Jean-Philippe Vert. Scaling ResNets in the large-depth regime. arXiv preprint arXiv:2206.06929, 2022.   \nPer-Gunnar Martinsson and Joel A Tropp. Randomized numerical linear algebra: Foundations and algorithms. Acta Numerica, 29:403\u2013572, 2020.   \nSong Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of two-layer neural networks. Proceedings of the National Academy of Sciences, 115(33):E7665\u2013E7671, 2018.   \nJeffrey Pennington, Samuel Schoenholz, and Surya Ganguli. Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice. Advances in Neural Information Processing Systems, 30, 2017.   \nJeffrey Pennington, Samuel Schoenholz, and Surya Ganguli. The emergence of spectral universality in deep networks. In International Conference on Artificial Intelligence and Statistics, pages 1924\u20131932. PMLR, 2018.   \nBoris T. Polyak. Introduction to optimization. 1987.   \nBen Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Exponential expressivity in deep neural networks through transient chaos. Advances in Neural Information Processing Systems, 29, 2016.   \nGrant M. Rotskoff and Eric Vanden-Eijnden. Neural networks as interacting particle systems: Asymptotic convexity of the loss landscape and universal scaling of the approximation error. stat, 1050:22, 2018.   \nWojciech Tarnowski, Piotr Warcho\u0142, Stanis\u0142aw Jastrz\u00b8ebski, Jacek Tabor, and Maciej Nowak. Dynamical isometry is achieved in residual networks in a universal way for any activation function. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 2221\u20132230. PMLR, 2019.   \nTwan Van Laarhoven. L2 regularization versus batch and weight normalization. arXiv preprint arXiv:1706.05350, 2017.   \nNikhil Vyas, Alexander Atanasov, Blake Bordelon, Depen Morwani, Sabarish Sainathan, and Cengiz Pehlevan. Feature-learning networks are consistent across widths at realistic scales. arXiv preprint arXiv:2305.18411, 2023.   \nRuosi Wan, Zhanxing Zhu, Xiangyu Zhang, and Jian Sun. Spherical motion dynamics: Learning dynamics of neural network with normalization, weight decay, and SGD. arXiv preprint arXiv:2006.08419, 2020.   \nZhichao Wang, Andrew Engel, Anand Sarwate, Ioana Dumitriu, and Tony Chiang. Spectral evolution and invariance in linear-width neural networks. arXiv preprint arXiv:2211.06506, 2022.   \nGreg Yang. Tensor programs III: Neural matrix laws. arXiv preprint arXiv:2009.10685, 2020.   \nGreg Yang and Edward J Hu. Tensor programs IV: Feature learning in infinite-width neural networks. In International Conference on Machine Learning, pages 11727\u201311737. PMLR, 2021.   \nGreg Yang, Edward Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao. Tuning large neural networks via zero-shot hyperparameter transfer. Advances in Neural Information Processing Systems, 34:17084\u201317097, 2021.   \nGreg Yang, James B Simon, and Jeremy Bernstein. A spectral condition for feature learning. arXiv preprint arXiv:2310.17813, 2023a.   \nGreg Yang, Dingli Yu, Chen Zhu, and Soufiane Hayou. Feature learning in infinite-depth neural networks. In NeurIPS 2023 Workshop on Mathematics of Modern Machine Learning, 2023b.   \nHuishuai Zhang, Da Yu, Mingyang Yi, Wei Chen, and Tie-Yan Liu. Stabilize deep ResNet with a sharp scaling factor $\\tau$ . Machine Learning, 111(9):3359\u20133392, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Proofs omitted from the main text ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Proof of Lem. 3.3. Writing $K=V D V^{\\top}$ with $D=\\mathrm{diag}(\\lambda_{1},\\ldots,\\lambda_{m})$ and $V\\in\\mathbb{R}^{m\\times m}$ orthonormal, we have $\\|K a\\|_{2}^{2}=a^{\\top}V D^{2}V^{\\top}a$ . Conditioned on $K$ , the vector $u=V^{\\top}a$ is isotropic Gaussian so $\\begin{array}{r}{\\mathbf{E}u_{i}^{2}={\\frac{1}{m}}}\\end{array}$ for $i\\in[1:m]$ . Hence, on the one hand ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbf{E}[\\|K a\\|_{2}^{2}\\mid K]=\\mathbf{E}\\Big[\\sum_{i=1}^{m}\\lambda_{i}^{2}u_{i}^{2}\\mid(\\lambda_{i})_{i=1}^{m}\\Big]=\\sum_{i=1}^{m}\\lambda_{i}^{2}\\mathbf{E}[u_{i}^{2}]=\\frac{1}{m}\\sum_{i=1}^{m}\\lambda_{i}^{2}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "On the other hand, using the fact that the variance of a chi-square random variable is 2, ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\mathrm{Var}[\\|K a\\|_{2}^{2}\\mid K]=\\mathbf{E}\\Big[\\Big(\\displaystyle\\sum_{i=1}^{m}\\lambda_{i}^{2}(u_{i}^{2}-1/m)\\Big)^{2}\\mid(\\lambda_{i})_{i=1}^{m}\\Big]}}\\\\ {{=\\displaystyle\\sum_{i=1}^{m}\\lambda_{i}^{4}\\mathbf{E}[(u_{i}^{2}-1/m)^{2}]+\\displaystyle\\sum_{i\\neq j}^{m}\\lambda_{i}^{2}\\lambda_{j}^{2}\\mathbf{E}[(u_{i}^{2}-1/m)(u_{j}^{2}-1/m)]=\\frac{2}{m^{2}}\\displaystyle\\sum_{i=1}^{m}\\lambda_{i}^{4}.\\;\\Omega}}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Proof of Prop.5.1. When $\\beta=1$ , Eq. (11) is classical from the signal propagation literature [Poole et al., 2016, Hanin and Rolnick, 2018, Hanin, 2018] (the fluctuations around the limit have also been studied in Hanin and Nica [2020]). Note that these results are proved with $k=\\Theta(1)$ , but Hanin [2018] allows to conclude as well when $k$ diverges at least if the initial gradient $\\begin{array}{r}{b_{L}=\\big(\\frac{\\partial\\mathcal{L}}{\\partial f_{L}}\\big)^{\\top}\\in\\mathbb{R}^{k}}\\end{array}$ is independent of the randomness of the weights, which is what (H2) guarantees. We note that analogous results have been derived for a variety of activation functions, and we focus on ReLU only for conciseness. ", "page_idx": 12}, {"type": "text", "text": "Let us now discuss the BFAs, assuming for simplicity that $\\eta_{1}=0$ as the contribution of $w_{1}$ to the BFK is asymptotically negligible assuming (BC). We consider the BFA at $g_{v}$ and denote $z_{v}:=(\\partial\\mathcal{L}/\\partial g_{v})^{\\top}$ . The main result of [Jelassi et al., 2023] can be restated as follows: with $k=d=\\Theta(1)$ , the choice $\\begin{array}{r}{\\sigma_{L}=\\frac{1}{m}}\\end{array}$ and learning-rates $\\eta_{\\ell}=\\Theta(L^{-3/2})$ , it holds $\\|{\\dot{g}}_{L-1}\\|_{\\mathrm{rms}}=\\Theta(1)$ . In view of (11), it holds in their setting for $\\ell\\in[2:L-1]$ ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\|\\nabla_{\\ell}\\mathcal{L}\\|_{2}^{2}=\\|b_{\\ell}g_{\\ell-1}^{\\top}\\|_{2}^{2}=\\|g_{\\ell-1}\\|_{2}^{2}\\|b_{\\ell}\\|_{2}^{2}=\\Theta(m_{\\ell-1}\\cdot m_{L-1}\\cdot\\sigma_{L}^{2})=\\Theta(m_{\\ell-1}/m_{L-1}).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Using $\\eta_{\\ell}~=~\\Theta(L^{-3/2})$ , it follows $\\begin{array}{r l r}{\\sum_{\\ell=2}^{L-1}\\eta_{\\ell}\\|\\nabla_{\\ell}\\mathcal{L}\\|_{2}^{2}}&{{}\\!\\!=\\!\\!}&{\\Theta(L^{-1/2})}\\end{array}$ . By Thm. 2.1 and using $m_{L-1}\\|z_{L-1}\\|_{\\mathrm{rms}}=\\Theta(1)$ , we get ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\|\\dot{g}_{L-1}\\|_{\\mathrm{rms}}=\\frac{\\sum_{\\ell\\leq L}\\eta_{\\ell}\\|\\nabla_{\\ell}\\mathcal{L}\\|_{2}^{2}}{\\cos(\\theta_{L-1})\\cdot m_{L-1}\\cdot\\|z_{L-1}\\|_{\\mathrm{rms}}}=\\Theta(1)\\mathrm{~\\\\\\}\\Rightarrow\\mathrm{~\\\\\\}\\cos(\\theta_{L-1})=\\Theta(L^{-1/2}).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "This shows the result for the BFA at $g_{L-1}$ and the result holds as well for the BFA at $f_{L-1}$ up to hidden constants. \u53e3 ", "page_idx": 12}, {"type": "text", "text": "Interestingly, in view of Thm. (3.2), we can interpret the result of [Jelassi et al., 2023] as a computation on the spectral moments of a certain random matrix, as stated in the following lemma. ", "page_idx": 12}, {"type": "text", "text": "Lemma A.1 (Spectrum of BFK and FBK in ReLU MLPs). For a ReLU MLP at random initialization satisfying $(S P)$ and $(B C)$ , consider the BFK (at $g_{v}$ instead of $f_{v}$ ): ", "page_idx": 12}, {"type": "equation", "text": "$$\nK_{v}=\\sum_{\\ell\\leq v}\\eta_{\\ell}\\Big(\\frac{\\partial g_{v}}{\\partial w_{\\ell}}\\Big)\\Big(\\frac{\\partial g_{v}}{\\partial w_{\\ell}}\\Big)^{\\top}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "and $\\theta_{v}$ the (non-oriented) angle between $\\dot{g}_{v}$ and $z_{\\ell}\\;:=\\;(\\partial\\mathcal{L}/\\partial g_{v})^{\\top}$ . Then, in the notations of Thm. (3.2), it holds as hidden width diverges $\\cos(\\theta_{v})=\\left(M_{1}(K_{v})/\\sqrt{M_{2}(K_{v})}\\right)=\\Theta(v^{-1/2})$ . ", "page_idx": 12}, {"type": "text", "text": "Consider also, for a linear loss, the kernel $\\tilde{K}_{v}$ such that $\\dot{b}_{v}\\,=\\,\\tilde{K}_{v}f_{v}$ (see Eq. (14)) and $\\tilde{\\theta}_{v}$ the (non-oriented) angle between $f_{v}$ and $-\\dot{b}_{v}$ . Then it holds, as hidden width diverges, $\\cos(\\tilde{\\theta}_{v})\\,=$ $\\Theta\\big(M_{1}(\\tilde{K}_{v})/\\sqrt{M_{2}(\\tilde{K}_{v})}\\big)=\\Theta((L-v)^{-1/2}).$ . ", "page_idx": 12}, {"type": "text", "text": "Proof. We have already seen in the proof of Prop.5.1 that $\\cos(\\theta_{v})=\\Theta(v^{-1/2})$ . It thus remains to see that the assumptions of Thm. (3.2) are satisfied: the independence of $z_{\\ell}$ follows from [Hanin and Nica, 2020, Prop. 2] and the Gaussianity of $z_{\\ell}$ is direct since $z_{\\ell}=W_{\\ell+1}^{\\top}b_{\\ell+1}$ where $W_{\\ell+1}$ is Gaussian and independent from $b_{\\ell+1}$ . Also, we have the more explicit expression ", "page_idx": 13}, {"type": "equation", "text": "$$\nK_{v}=\\sum_{\\ell=1}^{v}\\eta_{\\ell}\\|g_{\\ell-1}\\|_{2}^{2}\\Big(\\frac{\\partial g_{v}}{\\partial f_{\\ell}}\\Big)\\Big(\\frac{\\partial g_{v}}{\\partial f_{\\ell}}\\Big)^{\\top}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where \u2202\u2202gfv $\\begin{array}{r}{\\frac{\\partial g_{v}}{\\partial f_{\\ell}}=D_{v}W_{v}\\ldots D_{\\ell+1}W_{\\ell+1}D_{\\ell}}\\end{array}$ and $D_{i}=\\mathrm{diag}(\\phi(f_{i}))$ (by [Hanin and Nica, 2020, Prop. 2], these matrices can be taken as matrices with Bernoulli random variables on the diagonals, independent from everything else). Since under (SP) and (BC) we have that $\\eta_{\\ell}\\|g_{\\ell-1}\\|_{2}^{2}$ is constant for $\\ell\\in[1:L{-}1]$ , it follows ", "page_idx": 13}, {"type": "equation", "text": "$$\nK_{v}\\propto\\sum_{\\ell=1}^{v}(D_{v}W_{v}\\ldots D_{\\ell+1}W_{\\ell+1}D_{\\ell})(D_{v}W_{v}\\ldots D_{\\ell+1}W_{\\ell+1}D_{\\ell})^{\\top}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "For the second claim, we have (see Section 6) $\\begin{array}{r}{\\tilde{K}_{v}=\\sum_{\\ell=v+1}^{L}\\eta_{\\ell}\\|b_{\\ell}\\|_{2}^{2}\\Big(\\frac{\\partial g_{\\ell-1}}{\\partial f_{v}}\\Big)^{\\top}\\Big(\\frac{\\partial g_{\\ell-1}}{\\partial f_{v}}\\Big)}\\end{array}$ . Under (SP) and (BC), we have $\\eta_{\\ell}\\vert\\vert b_{\\ell}\\vert\\vert_{2}^{2}$ is constant for $\\ell\\in[2\\,:L]$ , hence it follows ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\tilde{K}_{v}\\propto\\sum_{\\ell=v+1}^{L}(D_{\\ell-1}W_{\\ell-1}\\ldots W_{v+1}D_{v})^{\\top}(D_{\\ell-1}W_{\\ell-1}\\ldots W_{v+1}D_{v}).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "By comparing the expressions for $K_{v}$ and $\\tilde{K}_{v}$ , we see that $\\tilde{K}_{v}$ has the same distribution of nonzero eigenvalues as $K_{L-v}$ (potentially up to a global multiplicative factor) and the conclusion follows. ", "page_idx": 13}, {"type": "text", "text": "Proof of Prop. 5.2. The estimate for $\\|f_{\\ell}\\|_{\\mathrm{rms}}$ is classical, see e.g. Li et al. [2021]. For the backward pass estimate, we rely on [Marion and Chizat, 2024, Lem. 3] (see also Zhang et al. [2022] for related results with the ReLU activation function), which implies that $\\sigma_{\\mathrm{min}}(\\ell\\to v)=\\Theta(1)$ and $\\sigma_{\\operatorname*{max}}(\\ell\\to v)=\\Theta(1)$ , where $\\sigma_{\\operatorname*{min}}(\\ell\\to v)$ and $\\sigma_{\\operatorname*{max}}(\\ell\\to v)$ are the smallest, respectively largest singular value of \u2202\u2202ffv\u2113 . The estimate on $\\begin{array}{r}{b_{\\ell}=\\left(\\frac{\\partial f_{L}}{\\partial f_{v}}\\right)^{\\top}b_{L}}\\end{array}$ directly follows. ", "page_idx": 13}, {"type": "text", "text": "For the BFA, we will apply the first bound of Thm. 3.2, namely $\\cos(\\theta_{v})\\geq\\lambda_{\\operatorname*{min}}(K_{v})/\\lambda_{\\operatorname*{max}}(K_{v})$ where $\\lambda_{\\mathrm{min}}(K_{v})$ and $\\lambda_{\\operatorname*{max}}(K_{v})$ are the smallest, respectively largest, eigenvalues of $K_{v}$ . In the forward pass (12), let us write $g_{\\ell}=\\phi(f_{\\ell})$ and $h_{\\ell}=W_{\\ell}g_{\\ell-1}$ . By direct computations, it holds (here $w_{\\ell}$ is the vectorization of $W_{\\ell}$ ): ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\ddot{\\mathrm{v}}_{v}=\\sum_{\\ell=1}^{v}\\eta_{\\ell}\\Big(\\frac{\\partial f_{v}}{\\partial w_{\\ell}}\\Big)\\Big(\\frac{\\partial f_{v}}{\\partial w_{\\ell}}\\Big)^{\\top}=\\sum_{\\ell=2}^{v}\\eta_{\\ell}\\lVert g_{\\ell-1}\\rVert_{2}^{2}\\Big(\\frac{\\partial f_{v}}{\\partial h_{\\ell}}\\Big)\\Big(\\frac{\\partial f_{v}}{\\partial h_{\\ell}}\\Big)^{\\top}\\quad=\\sum_{\\ell=2}^{v}\\eta_{\\ell}\\beta^{2}\\lVert g_{\\ell-1}\\rVert_{2}^{2}\\Big(\\frac{\\partial f_{v}}{\\partial f_{\\ell}}\\Big)\\Big(\\frac{\\partial f_{v}}{\\partial f_{\\ell}}\\Big)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Using the inequalities ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\lambda_{\\operatorname*{min}}(K)\\ge\\beta^{2}\\sum_{\\ell=2}^{v}\\eta_{\\ell}\\|g_{\\ell-1}\\|_{2}^{2}\\sigma_{\\operatorname*{min}}\\Big(\\frac{\\partial f_{v}}{\\partial f_{\\ell}}\\Big)^{2},\\;\\;\\;\\;\\;\\lambda_{\\operatorname*{max}}(K)\\le\\beta^{2}\\sum_{\\ell=2}^{v}\\eta_{\\ell}\\|g_{\\ell-1}\\|_{2}^{2}\\sigma_{\\operatorname*{max}}\\Big(\\frac{\\partial f_{v}}{\\partial f_{\\ell}}\\Big)^{2}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "we deduce $\\cos(\\theta_{v})\\geq\\lambda_{\\operatorname*{min}}(K_{v})/\\lambda_{\\operatorname*{max}}(K_{v})=\\Theta(1).$ . ", "page_idx": 13}, {"type": "text", "text": "Proof of Prop. 5.3. In this proof, we say that a claim is found \u201cby direct computation\u201d when it can be directly deduced from the conclusion of Prop. 5.1. In particular, for the computation of scale invariant LRs, we use the fact that $\\|\\nabla_{\\ell}\\mathcal{L}\\|_{2}=\\|b_{\\ell}g_{\\ell-1}^{\\top}\\|_{F}=\\|b_{\\ell}\\|_{2}\\cdot\\|g_{\\ell-1}\\|_{2}$ . Also, by Prop. 5.1, under (SP) and (BC) it holds $\\cos(\\theta_{L-1})=\\Theta(L^{-1/2})$ . ", "page_idx": 13}, {"type": "text", "text": "(i) One has that $\\mathbf{M}\\mathbf{F}{+}\\mu\\mathbf{P}$ satisfies (SP), (BC) by direct computation, and (FL) by Prop. 4.1. We have seen in the proof of Prop. 5.1 that $-\\dot{\\mathcal{L}}=\\Theta(L^{-1/2})$ , so (LD) does not hold. ", "page_idx": 13}, {"type": "text", "text": "(ii) For NTK, (SP),\u221a (LD) and (BC) can be checked by direct computation. For (FL), we have $\\|b_{L-1}\\|_{\\mathrm{rms}}=\\Theta(1/\\sqrt{m})$ so : ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\|\\dot{f}_{L-1}\\|_{\\mathrm{rms}}=\\Theta\\Big(\\frac{1}{\\cos(\\theta_{v})\\cdot m\\cdot\\|b_{L-1}\\|_{\\mathrm{rms}}}\\Big)=\\Theta\\Big(\\frac{1}{\\cos(\\theta_{L-1})\\cdot\\sqrt{m}}\\Big).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "But in the considered asymptotics $\\sqrt{m}\\cdot\\cos(\\theta_{L-1})=\\Theta(\\sqrt{m/L})\\to\\infty$ so $\\|\\dot{f}_{L-1}\\|_{\\mathrm{rms}}=o(1)$ . (iii) Properties (SP) sp\u221aecifies $\\sigma_{1}$ and $\\sigma_{2}=\\cdot=\\sigma_{L-1}$ \u221a, and Prop. 4.1 gives, with (FL), $\\|b_{L-1}\\|_{\\mathrm{rms}}=$ $\\begin{array}{r}{\\Theta(\\frac{1}{\\cos(\\theta_{L-1})m})=\\Theta(\\sqrt{L}/m)}\\end{array}$ which imposes $\\sigma_{L}=\\sqrt{k L}/m$ . Then the LR are given by (8). \u53e3 ", "page_idx": 14}, {"type": "text", "text": "Proof of Prop. 5.4. Properties (SP) specifies $\\sigma_{1}$ and $\\sigma_{\\mathrm{hid}}\\,=\\,\\sigma_{2}\\,=\\,\\cdot\\,=\\,\\sigma_{L-1}$ , and Prop. 4.1 gives, with (FL), $\\begin{array}{r}{\\|b_{L-1}\\|_{\\mathrm{rms}}=\\Theta(\\frac{1}{\\cos(\\theta_{L-1})m})=\\Theta(1/m)}\\end{array}$ which requires $\\sigma_{L}=\\sqrt{k}/m$ . Then the LR are characterized by (8). \u53e3 ", "page_idx": 14}, {"type": "text", "text": "Proof of Prop. 6.1. By the chain rule and Euler\u2019s identity for positively 1-homogeneous functions, it holds ", "page_idx": 14}, {"type": "equation", "text": "$$\nb_{v}^{\\top}f_{v}={\\frac{\\partial{\\mathcal{L}}}{\\partial f_{v}}}f_{v}={\\frac{\\partial{\\mathcal{L}}}{\\partial f_{L}}}{\\frac{\\partial f_{L}}{\\partial f_{v}}}f_{v}={\\frac{\\partial{\\mathcal{L}}}{\\partial f_{L}}}f_{L}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Now, by differentiating in time both sides we get ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\dot{b}_{v}^{\\top}f_{v}+b_{v}^{\\top}\\dot{f}_{v}=f_{L}^{\\top}\\nabla^{2}\\mathrm{loss}[f_{L}]\\dot{f}_{L}+\\frac{\\partial\\mathcal{L}}{\\partial f_{L}}\\dot{f}_{L}=f_{L}^{\\top}\\nabla^{2}\\mathrm{loss}[f_{L}]\\dot{f}_{L}+\\dot{\\mathcal{L}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We have $\\begin{array}{r}{\\dot{\\mathcal{L}}\\ =\\ -\\sum_{\\ell=1}^{L}\\eta_{\\ell}\\|\\nabla_{\\ell}\\mathcal{L}\\|_{2}^{2}}\\end{array}$ and moreover, from the proof of Thm. 2.1, $-b_{v}^{\\top}\\dot{f}_{v}\\;\\;=\\;$ $\\begin{array}{r}{\\sum_{\\ell\\leq v}\\eta_{\\ell}\\|\\nabla_{\\ell}\\mathcal{L}\\|_{2}^{2}}\\end{array}$ . So we deduce ", "page_idx": 14}, {"type": "equation", "text": "$$\n-\\dot{b}_{v}^{\\top}f_{v}=-f_{L}^{\\top}\\nabla^{2}\\mathrm{loss}[f_{L}]\\dot{f}_{L}+\\sum_{\\ell>v}\\eta_{\\ell}\\|\\nabla_{\\ell}\\mathcal{L}\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We conclude by writing $-\\dot{b}_{v}^{\\top}f_{v}=\\lVert\\dot{b}_{v}\\rVert_{2}\\lVert f_{v}\\rVert_{2}\\cos(\\tilde{\\theta}_{v})$ and rearranging. ", "page_idx": 14}, {"type": "text", "text": "Proof of Prop. 6.2. By assumption at time $t=0$ , it holds $\\tilde{\\theta}(0)=\\sigma\\odot\\theta(0)$ so let us prove the result by recursion. Assume that the claim is true at iteration $t$ . Since $\\prod\\sigma_{\\ell}=1$ , it holds $f_{L}(\\theta(t))=f_{L}(\\tilde{\\theta}(t))$ . Moreover, since $\\frac{\\partial f_{L}}{\\partial w_{\\ell}}$ is 0-homogeneous in $w_{\\ell}$ and separatel y 1-homogeneous in $(w_{i})_{i\\neq\\ell}$ . It follows ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\ell}\\mathcal{L}(\\tilde{\\theta}(t))=\\Big(\\displaystyle\\frac{\\partial f_{L}}{\\partial w_{\\ell}}[\\tilde{\\theta}(t)]\\Big)^{\\top}\\nabla\\mathrm{loss}(f_{L}(\\tilde{\\theta}(t)))}\\\\ &{\\quad\\quad\\quad\\quad=(\\displaystyle\\prod_{i\\neq\\ell}\\sigma_{i})\\Big(\\displaystyle\\frac{\\partial f_{L}}{\\partial w_{\\ell}}[\\theta(t)]\\Big)^{\\top}\\nabla\\mathrm{loss}(f_{L}(\\theta(t)))=\\displaystyle\\frac{1}{\\sigma_{\\ell}}\\nabla_{\\ell}\\mathcal{L}(\\theta(t)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "In particular, we deduce that the LRs are related by \u03b7\u03b7\u02dc((tt)) = \u2225\u2225\u2207\u2207\u2113LL((\u03b8\u03b8\u02dc((tt))))\u2225\u2225222 . It follows, for any $\\ell\\in[1:L]$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\tilde{w}_{\\ell}(t+1)=\\tilde{w}_{\\ell}(t)-\\tilde{\\eta}(t)\\nabla_{\\ell}\\mathcal{L}(\\tilde{\\theta}(t))=\\sigma_{\\ell}w_{\\ell}(t)-\\sigma_{\\ell}^{2}\\eta(t)\\frac{1}{\\sigma_{\\ell}}\\nabla_{\\ell}\\mathcal{L}(\\theta(t))=\\sigma_{\\ell}w_{\\ell}(t+1).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "This proves $\\tilde{\\theta}(t+1)=\\sigma\\odot\\theta(t+1)$ and the claim follows by recursion. ", "page_idx": 14}, {"type": "text", "text": "B Initializing with zero output weights ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Let us mention an interesting degree of freedom for FSC in Table 1: up to adjusting the initial LR, it is possible to initialize the output layer with 0 while still satisfying FSC at the next step. If one initializes the output layer $W_{L}$ with 0 then all gradients are 0 at time 0 except that for $W_{L}$ which leads to the update (non-infinitesimal in this paragraph): ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\delta W_{L}(0)=-\\eta_{L}(0)\\cdot b_{L}(0)g_{L-1}^{\\top}(0).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The second forward pass is the same as the first one, with the only difference that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f_{L}(1)=-\\eta_{L}(0)\\lVert g_{L-1}(0)\\rVert_{2}^{2}b_{L}(0).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Assuming $b_{L}(0)=b_{L}(1)$ (linear loss) for simplicity, this leads to a second backward pass: ", "page_idx": 14}, {"type": "equation", "text": "$$\nz_{L-1}(1):=\\Big(\\frac{\\partial\\mathcal{L}}{\\partial g_{L-1}}(1)\\Big)^{\\top}=(-\\eta_{L}(0)b_{L}(0)g_{L-1}(0)^{\\top})^{\\top}b_{L}(0)=-\\eta_{L}(0)\\cdot\\|b_{L}(0)\\|_{2}^{2}g_{L-1}(0).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "For the second GD step to satisfy (FL), we just need to ensure ", "page_idx": 15}, {"type": "equation", "text": "$$\nm\\|z_{L-1}(1)\\|_{\\mathrm{rms}}=\\Theta(\\sqrt{L})\\qquad\\qquad\\Leftrightarrow\\qquad\\quad\\eta_{L}(0)=\\Theta\\Bigl(\\frac{\\sqrt{L}}{m\\|b_{L}(0)\\|_{2}^{2}}\\Bigr).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This is the LR to be used at time 0, for the second step to satisfy (SP), (FL), (LD) and (BC). ", "page_idx": 15}, {"type": "text", "text": "Proof of Thm. 6.3. Prop. 6.2 shows that in fact only the\u221a product $\\sigma_{1}\\cdot\\sigma_{h}^{L-2}\\cdot\\sigma_{L}$ is a relevant degree of freedom of scale. We can thus fix (arbitrarily) $\\sigma_{1}=1/\\sqrt{d}$ and $\\sigma_{\\mathrm{hid}}=2/\\sqrt{m}$ so that (SP) is satisfied; we then have $\\|b_{v}\\|_{2}\\|f_{v}\\|_{2}=\\Theta(m\\sigma_{L})$ for $v\\in[1:L-1]$ . Desideratum (RFL) requires ", "page_idx": 15}, {"type": "equation", "text": "$$\n{\\frac{\\|{\\dot{f}}_{L-1}\\|_{2}}{\\|f_{L-1}\\|_{2}}}={\\frac{C_{1}+C_{h}}{\\cos(\\theta_{L-1})\\|b_{L-1}\\|_{2}\\|f_{L-1}\\|_{2}}}=\\Theta(1)\\qquad\\Leftrightarrow\\qquad\\sigma_{L}\\asymp(C_{1}+C_{h}){\\frac{\\sqrt{L}}{m}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "using that $\\cos(\\theta_{L-1})=\\Theta(1/\\sqrt{L})$ . Moreover, (LD) requires $C_{1}+C_{h}+C_{L}=\\Theta(1)$ . At this stage, the output scale $\\sigma_{L}$ is not yet entirely determined since $C_{1}+C_{h}=o(1)$ is not excluded. This is where (BS) comes into play. It requires in particular ", "page_idx": 15}, {"type": "equation", "text": "$$\n{\\frac{\\|\\dot{b}_{1}\\|_{2}}{\\|b_{1}\\|_{2}}}={\\frac{C_{\\mathrm{hid}}+C_{L}}{\\cos(\\tilde{\\theta}_{1})\\|b_{1}\\|_{2}\\|f_{1}\\|_{2}}}=O(1)\\qquad\\Leftrightarrow\\qquad\\bigl(C_{\\mathrm{hid}}+C_{L}\\bigr){\\frac{\\sqrt{L}}{m}}=O(\\sigma_{L})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "using that $\\cos(\\tilde{\\theta}_{1})\\,=\\,\\Theta(1/\\sqrt{L})$ by Lem. A.1. Combining both conditions for $\\sigma_{L}$ imply, on the one hand, that $C_{h}+C_{L}=O(C_{1}+C_{\\mathrm{hid}})$ hence $C_{1}+C_{\\mathrm{hid}}=\\Theta(1)$ and $\\sigma_{L}=\\Theta(\\frac{\\sqrt{L}}{m})$ , which are equivalent to the constraints written in the theorem. Conversely, it is not difficult to see that these constraints lead to satisfying (RFL), (LD), (FS) and (BS). \u53e3 ", "page_idx": 15}, {"type": "text", "text": "C Characterization of reparameterization invariant LR ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Consider a function $\\begin{array}{r}{f:\\prod_{\\ell=1}^{L}\\mathbb{R}^{p_{\\ell}}\\rightarrow\\mathbb{R}}\\end{array}$ admitting a (selection) derivative and, for a fixed scale vector $\\alpha\\in(\\mathbb{R}_{+}^{*})^{L}$ consider the function $g(y)=f(\\alpha\\cdot y)$ where $\\alpha\\cdot x$ denotes $(\\alpha_{1}x_{1},\\dots,\\alpha_{L}x_{L})$ . Consider one step of GD on the two functions, given for $\\ell\\in[1:L]$ , by ", "page_idx": 15}, {"type": "equation", "text": "$$\nx_{\\ell}^{\\prime}=x_{\\ell}-\\eta_{\\ell}\\nabla_{\\ell}f(x),\\qquad\\qquad\\qquad y_{\\ell}^{\\prime}=y_{\\ell}-\\eta_{\\ell}\\nabla_{\\ell}g(y)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "with identical starting points, that is $x_{\\ell}=\\alpha_{\\ell}\\cdot y_{\\ell}$ for $\\ell\\in[1:L]$ . ", "page_idx": 15}, {"type": "text", "text": "Proposition C.1. Consider adaptive learning rates, which are of the form $\\eta_{\\ell}=\\eta_{\\ell}(\\nabla f(x))$ . Then $x^{\\prime}=\\alpha\\cdot y^{\\prime}$ for all $\\alpha\\in(\\mathbb{R}_{+}^{*})^{L}$ if and only if \u03b7\u2113is $(-2)$ -homogeneous in $\\nabla_{\\boldsymbol{\\ell}}f(\\boldsymbol{x})$ and 0-homogeneous in $\\nabla_{\\boldsymbol{\\ell^{\\prime}}}f(\\boldsymbol{x})$ for $\\ell\\neq\\ell^{\\prime}$ . ", "page_idx": 15}, {"type": "text", "text": "One such LR is precisely that suggested by Prop. 4.1: $\\eta_{\\ell}\\propto\\|\\nabla_{\\ell}f(x)\\|_{2}^{-2}$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. For $\\ell\\in[1:L]$ , it holds ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\alpha_{\\ell}y_{\\ell}^{\\prime}=\\alpha_{\\ell}y_{\\ell}-\\alpha_{\\ell}\\eta_{\\ell}(\\nabla g(y))\\nabla_{\\ell}g(y)=x_{\\ell}-\\alpha_{\\ell}^{2}\\eta_{\\ell}(\\alpha\\cdot\\nabla f(x))\\nabla_{\\ell}f(x).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then ${\\boldsymbol{\\alpha}}\\cdot{\\boldsymbol{y}}^{\\prime}={\\boldsymbol{x}}^{\\prime}$ for all $\\alpha\\in(\\mathbb{R}_{+}^{*})^{L}$ is equivalent to ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\alpha_{\\ell}^{2}\\eta_{\\ell}(\\alpha\\cdot\\nabla f(x))=\\eta_{\\ell}(\\nabla f(x)),\\quad\\forall\\alpha\\in(\\mathbb{R}_{+}^{*})^{L}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which is the claimed homogeneity property. ", "page_idx": 15}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Justification: ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 16}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: Assumptions, that limit the scope of the theoretical results, are clearly stated. The limited practical relevance of some findings is mentioned. The conclusion discusses some limitations. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 16}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: Some of the proofs are deferred to the appendix, but all are provided. Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 17}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Justification: Experiments (with synthetic data) corroborate closely the theoretical results. Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 17}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: A link with the code to reproduce the experiments is provided. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 18}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: Experimental details are given either in the captions of the plots or in the main text. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 18}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: our plots include several random runs (which is more precise than error bars). Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: the experiments are small scale and run in less than 10 minutes on a personal laptop. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 19}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 19}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: There is no societal impact of the work performed. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 19}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 20}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 20}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 20}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. ", "page_idx": 20}, {"type": "text", "text": "\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 21}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 21}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] Justification: ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 21}]