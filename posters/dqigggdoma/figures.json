[{"figure_path": "DqiggGDOmA/figures/figures_3_1.jpg", "caption": "Figure 1: Schematic overview of EASI. ES acts as a generator in adversarial competition with a neural network discriminator distinguishing between simulation and reality state transitions. The discriminator serves as the fitness function, guiding the evolution of the physical parameter distributions.", "description": "This figure shows a schematic overview of the Evolutionary Adversarial Simulator Identification (EASI) method.  The figure depicts the interaction between four main components: the Evolutionary Strategy (ES), which evolves the simulator's physical parameters; the policy (a neural network), which interacts with the simulator; the simulator itself, which generates state transitions; and a discriminator neural network, which aims to distinguish real-world state transitions from simulated ones. The discriminator's output acts as the fitness function for the ES, guiding the parameter evolution to make the simulator's state transitions more closely match those in the real world.", "section": "4 Evolutionary Adversarial Simulator Identification"}, {"figure_path": "DqiggGDOmA/figures/figures_4_1.jpg", "caption": "Figure 1: Schematic overview of EASI. ES acts as a generator in adversarial competition with a neural network discriminator distinguishing between simulation and reality state transitions. The discriminator serves as the fitness function, guiding the evolution of the physical parameter distributions.", "description": "This figure provides a schematic overview of the Evolutionary Adversarial Simulator Identification (EASI) method.  It shows the main components of the system: an Evolutionary Strategy (ES) which acts as a generator of simulator parameters, a simulator, a discriminator neural network that distinguishes between real and simulated state transitions, and a feedback loop guided by the discriminator's output. The ES evolves the simulator's parameters based on the discriminator's assessment of how well the simulated state transitions match the real-world transitions. The discriminator's output acts as a fitness function, guiding the evolutionary process. The ultimate goal is to find a set of simulator parameters that produce state transitions highly similar to those observed in the real world.", "section": "4 Evolutionary Adversarial Simulator Identification"}, {"figure_path": "DqiggGDOmA/figures/figures_5_1.jpg", "caption": "Figure 2: Experiment tasks in simulation: Cartpole, Go2, Ant, and Ballbalance, presented in order from left to right.", "description": "This figure shows four different simulated robotic control tasks used in the paper's experiments.  From left to right, these are: a simple inverted pendulum on a cart (Cartpole), a quadrupedal robot (Go2), a simplified ant-like robot (Ant), and a ball balancing task on a three-legged platform (Ballbalance). These tasks represent different complexities and degrees of freedom, allowing for a thorough evaluation of the proposed sim-to-real transfer method.", "section": "5 Experiment"}, {"figure_path": "DqiggGDOmA/figures/figures_5_2.jpg", "caption": "Figure 3: Left: Cartpole task in reality, aiming to keep the pendulum on the cart balanced for as long as possible. Right: Go2 task in reality, aiming to keep running forward and track a specified speed.", "description": "This figure shows two real-world robotic tasks used to evaluate the EASI algorithm's sim-to-real transfer capabilities. The left panel displays the Cartpole task, where the goal is to balance a pendulum on a cart by applying appropriate control actions. The right panel shows the Go2 task, which involves controlling a quadrupedal robot to maintain forward locomotion while adhering to a specified speed.", "section": "5 Experiment"}, {"figure_path": "DqiggGDOmA/figures/figures_6_1.jpg", "caption": "Figure 8: The convergence process for different parameters being searched in the Go2 environment. WD means target parameter within initial parameter distribution, while OOD means out of initial parameter distribution.", "description": "This figure shows the parameter convergence process of EASI for the Go2 robot in a sim-to-sim setting. It displays the evolution of ten parameters over 40 generations, comparing two scenarios: WD (target parameters within the initial distribution) and OOD (target parameters outside the initial distribution). The plots illustrate how EASI efficiently adjusts parameter distributions to align the simulator with a target environment, regardless of whether the initial parameters are within or outside the target range. The dashed red line indicates the true value of the target parameter.", "section": "Supplementary"}, {"figure_path": "DqiggGDOmA/figures/figures_7_1.jpg", "caption": "Figure 5: Performance on the pseudo-real environment over the process of training.", "description": "This figure visualizes the performance of policies trained using EASI, UDR (Uniform Domain Randomization), and an Oracle (ideal performance) in a pseudo-real environment. The x-axis represents the number of policy training steps, and the y-axis represents the return (cumulative reward) in the pseudo-real environment.  The shaded area around each line represents the standard deviation across multiple runs. The figure demonstrates that EASI achieves performance comparable to the Oracle, significantly outperforming UDR in all four tested environments (Cartpole, Go2, Ant, and Ballbalance).", "section": "5.1 Sim-to-Sim Policy Transfer"}, {"figure_path": "DqiggGDOmA/figures/figures_7_2.jpg", "caption": "Figure 6: Policy performance in target environments with different parameters.", "description": "This figure visualizes the performance of policies trained using EASI and UDR in various target environments.  Each bar represents the average return achieved by the policy in a specific target environment, with error bars indicating variability. The x-axis shows the scaling factor applied to specific parameters (Cart DoF P, Thigh DoF P, Body Mass, Ball Mass). The y-axis shows the average return (cumulative reward). The figure demonstrates that EASI-trained policies generally outperform UDR policies across all target environments and parameters, highlighting EASI's ability to improve sim-to-real transfer by optimizing the simulator parameters.", "section": "5.1 Sim-to-Sim Policy Transfer"}, {"figure_path": "DqiggGDOmA/figures/figures_8_1.jpg", "caption": "Figure 7: (a) Using the same policy, there is a significant difference in the motion spectrum of the Go2's joints between the simulation and the real environment due to the reality gap. After optimizing with EASI, the motion spectrum of the Go2's joints in the simulation becomes closer to that in the real environment. (b) Comparison of speed tracking in real environments between policies trained with EASI parameter distribution and trained with UDR parameter distribution.", "description": "This figure compares the frequency spectrum of the Go2 robot's joint movements and its speed tracking performance in simulation and reality.  The left subplot (a) shows that before using EASI, there is a significant difference between the simulation and reality due to the reality gap. After applying EASI, the simulation's frequency spectrum is much closer to the real-world data, indicating that EASI successfully reduces the reality gap. The right subplot (b) shows the speed tracking performance in the real world, demonstrating that the policy trained with EASI parameters outperforms the one trained with UDR parameters, achieving more accurate and stable control.", "section": "5.2 Sim-to-Real Policy Transfer"}, {"figure_path": "DqiggGDOmA/figures/figures_8_2.jpg", "caption": "Figure 7: (a) Using the same policy, there is a significant difference in the motion spectrum of the Go2's joints between the simulation and the real environment due to the reality gap. After optimizing with EASI, the motion spectrum of the Go2's joints in the simulation becomes closer to that in the real environment. (b) Comparison of speed tracking in real environments between policies trained with EASI parameter distribution and trained with UDR parameter distribution.", "description": "This figure compares the performance of policies trained using EASI and UDR in a real-world scenario.  Subfigure (a) shows the frequency spectrums of joint movements in simulation (with EASI and UDR) and reality, demonstrating how EASI significantly reduces the difference between simulation and reality. Subfigure (b) shows the velocity tracking performance of both methods, indicating that EASI achieves better performance.", "section": "5.2 Sim-to-Real Policy Transfer"}, {"figure_path": "DqiggGDOmA/figures/figures_13_1.jpg", "caption": "Figure 8: The convergence process for different parameters being searched in the Go2 environment. WD means target parameter within initial parameter distribution, while OOD means out of initial parameter distribution.", "description": "This figure visualizes the parameter search process of EASI in the Go2 environment. It presents ten out of 25 parameters being optimized, showing the convergence of parameters towards their target values.  Two scenarios are compared: 'WD' (target parameter within the initial parameter distribution) and 'OOD' (target parameter outside the initial distribution). The plots illustrate how EASI adjusts parameter distributions across generations (x-axis), regardless of the initial distribution.", "section": "Supplementary"}]