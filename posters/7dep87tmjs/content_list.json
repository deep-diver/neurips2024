[{"type": "text", "text": "Learning with Fitzpatrick Losses ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Seta Rakotomandimby Ecole des Ponts seta.rakotomandimby@enpc.fr ", "page_idx": 0}, {"type": "text", "text": "Jean-Philippe Chancelier Ecole des Ponts jean-philippe.chancelier@enpc.fr ", "page_idx": 0}, {"type": "text", "text": "Michel De Lara Ecole des Ponts michel.delara@enpc.fr ", "page_idx": 0}, {"type": "text", "text": "Mathieu Blondel Google DeepMind mblondel@google.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Fenchel-Young losses are a family of convex loss functions, encompassing the squared, logistic and sparsemax losses, among others. Each Fenchel-Young loss is implicitly associated with a link function, for mapping model outputs to predictions. For instance, the logistic loss is associated with the soft argmax link function. Can we build new loss functions associated with the same link function as FenchelYoung losses? In this paper, we introduce Fitzpatrick losses, a new family of convex loss functions based on the Fitzpatrick function. A well-known theoretical tool in maximal monotone operator theory, the Fitzpatrick function naturally leads to a refined Fenchel-Young inequality, making Fitzpatrick losses tighter than FenchelYoung losses, while maintaining the same link function for prediction. As an example, we introduce the Fitzpatrick logistic loss and the Fitzpatrick sparsemax loss, counterparts of the logistic and the sparsemax losses. This yields two new tighter losses associated with the soft argmax and the sparse argmax, two of the most ubiquitous output layers used in machine learning. We study in details the properties of Fitzpatrick losses and in particular, we show that they can be seen as Fenchel-Young losses using a modified, target-dependent generating function. We demonstrate the effectiveness of Fitzpatrick losses for label proportion estimation. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Loss functions are a cornerstone of statistics and machine learning: they measure the difference, or \u201closs,\u201d between a ground-truth target and a model prediction. As such, they have attracted a wealth of research. Proper losses (a.k.a. proper scoring rules) [17, 16] measure the discrepancy between a target distribution and a probability forecast. They are essentially primal-primal Bregman divergences, with both the target and the prediction belonging to the same primal space. They are typically explicitly composed with a link function [26, 30], in order to map the model output to a prediction. A disadvantage of this explicit composition is that it often makes the resulting composite loss function nonconvex. A related family of loss functions are Fenchel-Young losses [6, 7], which encompass many commonly-used loss functions in machine learning including the squared, logistic, sparsemax and perceptron losses. Fenchel-Young losses can be seen as primal-dual Bregman divergences [1], with the target belonging to the primal space and the model output belonging to the dual space. In contrast to proper losses, each Fenchel-Young loss is implicitly associated with a given link function, mapping the dual-space model output to a primal-space prediction (for instance, the soft argmax is the link function associated with the logistic loss). This crucial difference makes Fenchel-Young losses always convex. Can we build new convex losses associated with the same link function as Fenchel-Young losses? ", "page_idx": 0}, {"type": "image", "img_path": "7Dep87TMJs/tmp/bef2fb65f3cb5afa436ba8fecdf66daf5c81d574733c9f80d00c556393d0c209.jpg", "img_caption": ["Figure 1: We introduce Fitzpatrick losses, a new family of loss functions generated by a convex regularization function $\\Omega$ , that lower-bound Fenchel-Young losses generated by the same $\\Omega$ , while maintaining the same link function $\\widehat{y}_{\\Omega}=\\nabla\\Omega^{*}$ . In particular, we use our framework to instantiate the counterparts of the logistic and sparsemax losses, two instances of Fenchel-Young losses, associated with the soft argmax and the sparse argmax. In the figures above, we plot $L(y,\\theta)$ , where $y=e_{1}$ , $\\theta=(s,0)$ and $\\bar{L}\\in\\{L_{F[\\partial\\Omega]},\\bar{L_{\\Omega\\oplus\\Omega^{*}}}\\}$ , confirming the lower-bound property. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "In this paper, we introduce Fitzpatrick losses, a new family of primal-dual convex loss functions. Our proposal builds upon the Fitzpatrick function, a well-known theoretical object in maximal monotone operator theory [15, 11, 2]. So far, the Fitzpatrick function had been used as a theoretical tool to represent maximal monotone operators [28] and to construct Bregman-like primal-primal divergences [10], but it had not been used to construct primal-dual loss functions for machine learning, as we do. Crucially, the Fitzpatrick function naturally leads to a refined Fenchel-Young inequality, making Fitzpatrick losses tighter than Fenchel-Young losses. Yet, their predictions are produced using the same link function, suggesting that we can use Fitzpatrick losses as a tighter replacement for the corresponding Fenchel-Young losses (Figure 1). We make the following contributions. ", "page_idx": 1}, {"type": "text", "text": "\u2022 After reviewing some background, we introduce Fitzpatrick losses. They can be thought as a tighter version of Fenchel-Young losses, that use the same link function. \u2022 We instantiate two new loss functions in this family: the Fitzpatrick logistic loss and the Fitzpatrick sparsemax loss. They are the counterparts of the logistic and sparsemax losses, two instances of Fenchel-Young losses. We therefore obtain two new tighter losses for the soft argmax and the sparse argmax, two of the most popular output layers in machine learning. \u2022 We study in detail the properties of Fitzpatrick losses. We show that Fitzpatrick losses are equivalent to Fenchel-Young losses with a modified, target-dependent generating function. \u2022 We demonstrate the effectiveness of Fitzpatrick losses for probabilistic classification on 11 datasets. ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Convex analysis ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "numbers by We define $[k]:=\\{1,\\ldots,k\\}$ $\\mathbb{R}_{++}:=(0,+\\infty)$ , the non-negative real numbers by . We denote the probability simplex by $\\mathbb{R}_{+}:=[0,+\\infty)$ $\\bar{\\triangle}^{k}:=\\lbrace p\\in\\mathbb{R}_{+}^{k}:\\,\\sum_{i=1}^{k}p_{i}=$ and the positive real $1\\}$ the extended reals by $\\overline{{\\mathbb{R}}}:=\\mathbb{R}\\cup\\{-\\infty,+\\infty\\}$ . We denote the indicator function of a set $\\mathcal{C}$ by $\\iota_{\\mathcal{C}}(y)=0$ if $y\\in{\\mathcal{C}},+\\infty$ otherwise. We denote the effective domain of a function $\\Omega\\colon\\mathbb{R}^{k}\\rightarrow\\overline{{\\mathbb{R}}}$ by $\\operatorname{dom}\\Omega:=\\{y\\in\\mathbb{R}^{k}:\\Omega(y)<+\\infty\\}$ . A function $\\Omega\\colon\\mathbb{R}^{k}\\rightarrow\\overline{{\\mathbb{R}}}$ is said to be proper if it never takes the value $-\\infty$ and that $\\mathrm{dom}\\,\\Omega\\neq\\emptyset$ . We denote the Euclidean projection onto a nonempty closed convex set $\\mathcal{C}$ by $P c(\\theta)=\\mathrm{argmin}_{y\\in C}\\;\\|y-\\theta\\|_{2}^{2}$ . ", "page_idx": 1}, {"type": "text", "text": "For a function $\\Omega:\\mathbb{R}^{k}\\rightarrow\\overline{{\\mathbb{R}}}$ , its subdifferential $\\partial\\Omega$ (possibly empty) is defined by ", "page_idx": 1}, {"type": "equation", "text": "$$\n(y^{\\prime},\\theta^{\\prime})\\in\\partial\\Omega\\iff\\theta^{\\prime}\\in\\partial\\Omega(y^{\\prime})\\iff\\Omega(y)\\geq\\Omega(y^{\\prime})+\\langle y-y^{\\prime},\\theta^{\\prime}\\rangle,\\ \\forall y.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "When $\\Omega$ is convex and differentiable, the subdifferential is a singleton and we have $\\partial\\Omega(y^{\\prime})\\;=\\;$ $\\{\\nabla\\Omega(y^{\\prime})\\}$ . The normal cone to a set $\\mathcal{C}$ at $y^{\\prime}$ is defined by ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\theta^{\\prime}\\in N_{\\mathcal{C}}(y^{\\prime})\\iff\\left\\langle y-y^{\\prime},\\theta^{\\prime}\\right\\rangle\\leq0\\quad\\forall y\\in\\mathcal{C}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "if $y^{\\prime}\\in\\mathcal{C}$ and $N_{\\mathcal{C}}(y^{\\prime})=\\emptyset$ if $y^{\\prime}\\notin{\\mathcal{C}}$ . The Fenchel conjugate $\\Omega^{*}:\\mathbb{R}^{k}\\rightarrow\\overline{{\\mathbb{R}}}$ of a function $\\Omega:\\mathbb{R}^{k}\\rightarrow\\overline{{\\mathbb{R}}}$ is defined by ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\Omega^{\\ast}(\\theta):=\\operatorname*{sup}_{y^{\\prime}\\in\\mathbb{R}^{k}}\\langle y^{\\prime},\\theta\\rangle-\\Omega(y^{\\prime}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "From standard results in convex analysis [27, Proposition 11.3], when $\\Omega:\\mathbb{R}^{k}\\rightarrow\\overline{{\\mathbb{R}}}$ is a proper convex l.s.c. (lower semicontinuous) function, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\partial\\Omega^{\\ast}(\\theta)=\\underset{y^{\\prime}\\in\\mathbb{R}^{k}}{\\mathrm{argmax}}\\langle y^{\\prime},\\theta\\rangle-\\Omega(y^{\\prime}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "When the argmax is unique, it is equal to $\\nabla\\Omega^{*}(\\theta)$ . We define the generalized Bregman divergence [18] $D_{\\Omega}:\\mathbb{R}^{k}\\times\\mathbb{R}^{k}\\rightarrow\\overline{{\\mathbb{R}}}_{+}$ generated by a proper convex l.s.c. function $\\Omega:\\mathbb{R}^{k}\\rightarrow\\overline{{\\mathbb{R}}}$ , ", "page_idx": 2}, {"type": "equation", "text": "$$\nD_{\\Omega}(y,y^{\\prime}):=\\Omega(y)-\\Omega(y^{\\prime})-\\operatorname*{sup}_{\\theta^{\\prime}\\in\\partial\\Omega(y^{\\prime})}\\langle y-y^{\\prime},\\theta^{\\prime}\\rangle,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "with the convention $+\\infty+(-\\infty)=+\\infty$ (we comment on this convention in Appendix C). When $\\Omega$ is differentiable, it recovers the classical Bregman divergence ", "page_idx": 2}, {"type": "equation", "text": "$$\nD_{\\Omega}(y,y^{\\prime}):=\\Omega(y)-\\Omega(y^{\\prime})-\\langle y-y^{\\prime},\\nabla\\Omega(y^{\\prime})\\rangle.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Both $y$ and $y^{\\prime}$ belong to the primal space. ", "page_idx": 2}, {"type": "text", "text": "2.2 Fenchel-Young losses ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Definition and properties ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The Fenchel-Young loss [7] $L_{\\Omega\\oplus\\Omega^{*}}:\\mathbb{R}^{k}\\times\\mathbb{R}^{k}\\to\\overline{{\\mathbb{R}}}$ generated by a proper convex l.s.c. function $\\Omega$ is ", "page_idx": 2}, {"type": "equation", "text": "$$\nL_{\\Omega\\oplus\\Omega^{*}}(y,\\theta):=\\Omega\\oplus\\Omega^{*}(y,\\theta)-\\langle y,\\theta\\rangle:=\\Omega(y)+\\Omega^{*}(\\theta)-\\langle y,\\theta\\rangle.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "As its name indicates, it is grounded in the Fenchel-Young inequality ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\left\\langle y,\\theta\\right\\rangle\\leq\\Omega(y)+\\Omega^{*}(\\theta)\\quad\\forall y,\\theta\\in\\mathbb{R}^{k}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The Fenchel-Young loss enjoys many desirable properties, notably it is non-negative and it is convex in $y$ and $\\theta$ separately. The Fenchel-Young loss can be seen as a primal-dual Bregman divergence [1, 7], where $y$ belongs to the primal space and $\\theta$ belongs to the dual space. ", "page_idx": 2}, {"type": "text", "text": "Link functions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "To map an element $\\theta$ of a dual space to an element $y$ of a primal space, we define the link function (potentially set-valued) associated with the loss $L$ by ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\theta\\mapsto\\{y\\colon L(y,\\theta)=0\\}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Given a proper convex function $\\Omega$ , the associated Fenchel-Young loss ${\\cal L}_{\\Omega\\oplus\\Omega^{*}}$ produces the canonical link function $\\partial\\Omega^{*}$ , since ", "page_idx": 2}, {"type": "equation", "text": "$$\nL_{\\Omega\\oplus\\Omega^{*}}(y,\\theta)=0\\iff y\\in\\partial\\Omega^{*}(\\theta).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "In particular when $\\Omega$ is strictly convex, the Fenchel-Young loss is positive, meaning that it satisfies the identity of indiscernibles ", "page_idx": 2}, {"type": "equation", "text": "$$\nL_{\\Omega\\oplus\\Omega^{*}}(y,\\theta)=0\\iff y=\\nabla\\Omega^{*}(\\theta).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "In the remainder of this paper, we will use the notation $\\widehat{y}_{\\Omega}(\\theta)$ for the canonical link function $\\partial\\Omega^{*}(\\theta)$ . When the function $\\Omega^{*}$ is differentiable, $\\widehat{y}_{\\Omega}(\\theta)$ will  d enote $\\nabla\\Omega^{*}(\\theta)$ . Since $\\Omega^{*}$ is convex, $\\widehat{y}_{\\Omega}$ is monotone (see Section 2.3). As shown in [7], the monotonicity implies that $\\theta$ and $\\widehat{y}_{\\Omega}(\\theta)$ are sorted the same way, i.e., $\\theta_{i}>\\theta_{j}\\Longrightarrow\\widehat{y}_{\\Omega}(\\theta)_{i}\\geq\\widehat{y}_{\\Omega}(\\theta)_{j}$ . Link functions also play an imp o rtant role in the loss gradient, as we have ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\partial_{\\theta}L_{\\Omega\\oplus\\Omega^{*}}(y,\\theta)=\\widehat{y}_{\\Omega}(\\theta)-y.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Examples of Fenchel-Young loss instances and their associated link function ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We give a few examples of Fenchel-Young losses. With the squared 2-norm, $\\Omega(y^{\\prime})=\\textstyle{\\frac{1}{2}}\\|y^{\\prime}\\|_{2}^{2}$ , we obtain the squared loss ", "page_idx": 3}, {"type": "equation", "text": "$$\nL_{\\Omega\\oplus\\Omega^{*}}(y,\\theta)=L_{\\mathrm{squared}}(y,\\theta):=\\frac{1}{2}\\|y-\\theta\\|_{2}^{2}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "and the identity link ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\widehat{y}_{\\Omega}(\\theta)=\\theta.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "With the indicator of a convex set $\\mathcal{C}$ $,\\Omega(y^{\\prime})=\\iota_{\\mathcal{C}}(y^{\\prime})$ , we obtain the perceptron loss ", "page_idx": 3}, {"type": "equation", "text": "$$\nL_{\\Omega\\oplus\\Omega^{*}}(y,\\theta)=L_{\\mathrm{perceptron}}(y,\\theta):=\\operatorname*{max}_{y^{\\prime}\\in\\mathcal{C}}\\,\\,\\langle y^{\\prime},\\theta\\rangle-\\langle y,\\theta\\rangle.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "and the argmax link ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\widehat{y}_{\\Omega}(\\theta)=\\mathop{\\underset{y\\in\\mathcal{C}}{\\operatorname{argmax}}}_{y\\in\\mathcal{C}}\\;\\langle y,\\theta\\rangle.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "With the squared 2-norm restricted to some convex set $\\mathcal{C}$ $\\begin{array}{r}{\\mathrm{\\boldmath~\\hat{\\rho}~},\\Omega(y^{\\prime})=\\frac{1}{2}\\|y^{\\prime}\\|_{2}^{2}+\\iota c(y^{\\prime})}\\end{array}$ , we obtain the sparseMAP loss [24] ", "page_idx": 3}, {"type": "equation", "text": "$$\nL_{\\Omega\\oplus\\Omega^{*}}(y,\\theta)=L_{\\mathrm{sparseMAP}}(y,\\theta):=\\frac{1}{2}\\|y-\\theta\\|_{2}^{2}-\\frac{1}{2}\\|P_{C}(y)-\\theta\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The link is the Euclidean projection onto $\\mathcal{C}$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\n{\\widehat{y}}_{\\Omega}(\\theta)=P_{\\mathcal{C}}(\\theta).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "When the set is $\\mathcal{C}=\\bigtriangleup^{k}$ , we obtain the sparsemax loss [21] and the sparsemax link $\\widehat{y}_{\\Omega}(\\theta)=P_{\\triangle^{k}}(\\theta)$ , which is known to produce sparse probability distributions. With the Shannon negentropy restricted to the probability simplex, $\\bar{\\Omega^{\\prime}}(y^{\\prime}):=\\langle y^{\\prime},\\log\\bar{y^{\\prime}}\\rangle+\\iota_{\\triangle^{k}}(y^{\\prime})$ , we obtain the logistic loss ", "page_idx": 3}, {"type": "equation", "text": "$$\nL_{\\Omega\\oplus\\Omega^{*}}(y,\\theta)=L_{\\mathrm{logistic}}(y,\\theta):=\\log\\sum_{i=1}^{k}\\exp(\\theta_{i})+\\langle y,\\log y\\rangle-\\langle y,\\theta\\rangle,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "and the soft argmax link (also known as softmax) ", "page_idx": 3}, {"type": "equation", "text": "$$\n{\\widehat{y}}_{\\Omega}(\\theta)=\\operatorname{softargmax}(\\theta):=\\exp(\\theta)/\\sum_{i=1}^{k}\\exp(\\theta_{i}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "2.3 Maximal monotone operators and the Fitzpatrick function ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "An operator $A$ , that is, a subset $A\\subset\\mathbb{R}^{k}\\times\\mathbb{R}^{k}$ , is called monotone if for all $(y,\\theta)\\in A$ and all $(y^{\\prime},\\theta^{\\prime})\\in A$ , we have ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\left\\langle y^{\\prime}-y,\\theta^{\\prime}-\\theta\\right\\rangle\\geq0.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We overload the notation to denote $A(y):=\\{\\theta\\colon(y,\\theta)\\in A\\}$ . A monotone operator $A$ is said to be maximal if there does not exist $(y,\\theta)\\notin A$ such that $A\\cup\\{(\\bar{y},\\theta)\\}$ is still monotone. It is well-known that the subdifferential $\\partial\\Omega$ of a proper convex l.s.c. function $\\Omega$ is maximal monotone. For more details on monotone operators, see [3, 28]. ", "page_idx": 3}, {"type": "text", "text": "A well-known object in monotone operator theory, the Fitzpatrick function associated with a monotone operator $A$ [15, 11, 2], denoted $F[A]:\\mathbb{R}^{\\dot{k}}\\times\\mathbb{R}^{k}\\rightarrow\\bar{\\mathbb{R}}$ , is defined by ", "page_idx": 3}, {"type": "equation", "text": "$$\nF[A](y,\\theta):=\\operatorname*{sup}_{(y^{\\prime},\\theta^{\\prime})\\in A}\\langle y-y^{\\prime},\\theta^{\\prime}\\rangle+\\langle y^{\\prime},\\theta\\rangle.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In particular, with $A=\\partial\\Omega$ , we have ", "page_idx": 3}, {"type": "equation", "text": "$$\nF[\\partial\\Omega](y,\\theta)=\\operatorname*{sup}_{(y^{\\prime},\\theta^{\\prime})\\in\\partial\\Omega}\\langle y-y^{\\prime},\\theta^{\\prime}\\rangle+\\langle y^{\\prime},\\theta\\rangle=\\operatorname*{sup}_{y^{\\prime}\\in\\mathrm{dom}}\\left[\\langle y^{\\prime},\\theta\\rangle+\\operatorname*{sup}_{\\theta^{\\prime}\\in\\partial\\Omega(y^{\\prime})}\\langle y-y^{\\prime},\\theta^{\\prime}\\rangle\\right].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The Fitzpatrick function was studied in depth in [2]. In particular, it is jointly convex and satisfies ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\langle y,\\theta\\rangle\\leq F[\\partial\\Omega](y,\\theta)\\leq\\Omega\\oplus\\Omega^{*}(y,\\theta)=\\Omega(y)+\\Omega^{*}(\\theta)\\quad\\forall y,\\theta\\in\\mathbb{R}^{k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "From Danskin\u2019s theorem, when $\\operatorname{dom}\\Omega$ is compact, we also have ", "page_idx": 4}, {"type": "equation", "text": "$$\ny_{F[\\partial\\Omega]}^{\\star}(y,\\theta):=\\partial_{\\theta}F[\\partial\\Omega](y,\\theta)=\\operatorname*{argmax}_{y^{\\prime}\\in\\mathrm{dom}\\,\\Omega}\\ \\left[\\langle y^{\\prime},\\theta\\rangle+\\operatorname*{sup}_{\\theta^{\\prime}\\in\\partial\\Omega(y^{\\prime})}\\langle y-y^{\\prime},\\theta^{\\prime}\\rangle\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The Fitzpatrick function $F[\\partial\\Omega](y,\\theta)$ and $\\Omega\\oplus\\Omega^{\\ast}(y,\\theta)=\\Omega(y)+\\Omega^{\\ast}(\\theta)$ play a similar role but the latter is separable in $y$ and $\\theta$ , while the former is not. In particular this makes the subdifferential $\\partial_{\\theta}F[\\partial\\Omega](\\bar{y},\\theta)$ depend on both $y$ and $\\theta$ , while $\\partial_{\\theta}(\\Omega\\oplus\\Omega^{*})\\bar{(}y,\\theta)=\\partial\\Omega^{*}(\\theta)$ depends only on $\\theta$ . ", "page_idx": 4}, {"type": "text", "text": "The Fitzpatrick function was used in [10] to theoretically study primal-primal Bregman-like divergences. As discussed in more detail in Section 3.4, using these divergences for machine learning would require us to compose them with an explicit link function, which would typically break convexity. In the next section, we introduce new primal-dual losses based on the Fitzpatrick function. ", "page_idx": 4}, {"type": "text", "text": "3 Fitzpatrick losses ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "3.1 Definition and properties ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Inspired by the inequality in (3), which we can view as a refined Fenchel-Young inequality, we introduce Fitzpatrick losses, a new family of loss functions generated by a convex l.s.c. function $\\Omega$ . ", "page_idx": 4}, {"type": "text", "text": "Definition 1 Fitzpatrick loss   \nLet $\\Omega:\\mathbb{R}^{k}\\rightarrow\\overline{{\\mathbb{R}}}$ be a proper convex l.s.c. function. When $y\\in\\dim\\Omega$ and $\\boldsymbol\\theta\\in\\mathbb{R}^{k}$ , we define   \nthe Fitzpatrick loss $L_{F[\\partial\\Omega]}^{\\'}:\\mathbb{R}^{k}\\times\\mathbb{R}^{k}\\rightarrow\\bar{\\mathbb{R}}$ generated by $\\Omega$ as $\\begin{array}{r l}&{L_{F[\\partial\\Omega]}(y,\\theta):=F[\\partial\\Omega](y,\\theta)-\\langle y,\\theta\\rangle}\\\\ &{=\\underset{(y^{\\prime},\\theta^{\\prime})\\in\\partial\\Omega}{\\operatorname*{sup}}\\langle y-y^{\\prime},\\theta^{\\prime}\\rangle+\\langle y^{\\prime},\\theta\\rangle-\\langle y,\\theta\\rangle}\\\\ &{=\\underset{(y^{\\prime},\\theta^{\\prime})\\in\\partial\\Omega}{\\operatorname*{sup}}\\langle y^{\\prime}-y,\\theta-\\theta^{\\prime}\\rangle.}\\end{array}$ ", "page_idx": 4}, {"type": "text", "text": "When y \u0338\u2208dom \u2126, $\\begin{array}{r}{L_{F[\\partial\\Omega]}(y,\\theta):=+\\infty.}\\end{array}$ ", "page_idx": 4}, {"type": "text", "text": "Fitzpatrick losses enjoy similar properties as Fenchel-Young losses, while being tighter than FenchelYoung losses. ", "page_idx": 4}, {"type": "text", "text": "Proposition 1 Properties of Fitzpatrick losses 1. Non-negativity: for all $(y,\\theta)\\in\\mathbb{R}^{k}\\times\\mathbb{R}^{k}$ , $L_{F[\\partial\\Omega]}(y,\\theta)\\geq0.$ . 2. Same link function: $L_{\\Omega\\oplus\\Omega^{*}}(y,\\theta)=L_{F[\\partial\\Omega]}(y,\\theta)=0\\iff y=\\widehat{y}_{\\Omega}(\\theta).$ 3. Convexity: ${\\cal L}_{F[\\partial\\Omega]}(y,\\theta)$ is convex in y and $\\theta$ separately. 4. (Sub-)Gradient: $\\partial_{\\theta}L_{F[\\partial\\Omega]}(y,\\theta)=y_{F[\\partial\\Omega]}^{\\star}(y,\\theta)-y$ where $y_{F[\\partial\\Omega]}^{\\star}(y,\\theta)$ is given by (4). 5. Tighter inequality: for all $y,\\theta)\\in\\mathbb{R}^{k},\\,0\\leq L_{F\\left[\\partial\\Omega\\right]}(y,\\theta)\\leq L_{\\Omega\\oplus\\Omega^{*}}(y,\\theta).$ ", "page_idx": 4}, {"type": "text", "text": "A proof is given in Appendix B.2. Because the Fitzpatrick loss and the Fenchel-Young loss generated by the same $\\Omega$ have the same link function $\\widehat{y}_{\\Omega}$ , they share the same minimizers w.r.t. $\\theta$ for $y$ fixed.   \nHowever, the Fitzpatrick loss is always a l o wer bound of the corresponding Fenchel-Young loss.   \nMoreover, they have different gradients w.r.t. $\\theta\\colon\\partial_{\\theta}L_{\\Omega\\oplus\\Omega^{*}}(y,\\theta)={\\widehat{y}}_{\\Omega}(\\theta)-y$ vs. $\\partial_{\\theta}L_{F[\\partial\\Omega]}(y,\\theta)=$   \n$y_{F[\\partial\\Omega]}^{\\star}(y,\\theta)-y$ . It is worth noticing that $y_{F[\\partial\\Omega]}^{\\star}(y,\\theta)$ depends on both $y$ and $\\theta$ , contrary to $\\widehat{y}_{\\Omega}(\\theta)$ . ", "page_idx": 4}, {"type": "text", "text": "When $\\Omega$ is a twice differentiable function on its domain (which is for instance the case of the squared 2-norm or the negentropy), we next show that Fitzpatrick losses enjoy a particularly simple expression and become a squared Mahalanobis-like distance. ", "page_idx": 4}, {"type": "text", "text": "Proposition 2 Expressions of $F[\\partial\\Omega](y,\\theta)$ and $L_{F[\\partial\\Omega]}(y,\\theta)$ when $\\Omega$ is twice differentiable   \nLet $\\Omega:\\mathbb{R}^{k}\\rightarrow\\overline{{\\mathbb{R}}}$ be a convex function such that dom $\\Omega$ is an open set. Let us assume that $\\Omega$ is   \ntwice differentiable. Then, for any $y\\in\\dim\\Omega$ , we have that $\\begin{array}{r l}&{F[\\partial\\Omega](y,\\theta)=\\langle y,\\nabla\\Omega(y^{\\star})\\rangle+\\langle y^{\\star},\\theta\\rangle-\\langle y^{\\star},\\nabla\\Omega(y^{\\star})\\rangle}\\\\ &{L_{F[\\partial\\Omega]}(y,\\theta)=\\langle y^{\\star}-y,\\theta-\\nabla\\Omega(y^{\\star})\\rangle}\\\\ &{\\qquad\\qquad\\qquad=\\langle y^{\\star}-y,\\nabla^{2}\\Omega(y^{\\star})(y^{\\star}-y)\\rangle}\\end{array}$   \nwhere $y^{\\star}=y_{F[\\partial\\Omega]}^{\\star}(y,\\theta)$ is a solution w.r.t. $y^{\\prime}\\in\\operatorname{int}\\dim\\Omega$ of $\\nabla^{2}\\Omega(y^{\\prime})(y^{\\prime}-y)=\\theta-\\nabla\\Omega(y^{\\prime}).$ ", "page_idx": 5}, {"type": "text", "text": "A proof is given in B.3. When $\\Omega$ is constrained (i.e., when it contains an indicator function), we show in Section 3.5 that the above expression becomes a lower bound. ", "page_idx": 5}, {"type": "text", "text": "3.2 Examples ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We now present the Fitzpatrick loss counterparts of various Fenchel-Young losses. ", "page_idx": 5}, {"type": "text", "text": "Squared loss. ", "text_level": 1, "page_idx": 5}, {"type": "table", "img_path": "7Dep87TMJs/tmp/fca15e2712d8f76ed1eb89261ebd9245e382dc1eaa9e6a98be2f384b27d70c58.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "A proof is given in Appendix B.4. Therefore, the Fenchel-Young and Fitzpatrick losses generated by $\\Omega$ coincide, up to a factor $\\textstyle{\\frac{1}{2}}$ . ", "page_idx": 5}, {"type": "text", "text": "Perceptron loss. ", "text_level": 1, "page_idx": 5}, {"type": "table", "img_path": "7Dep87TMJs/tmp/386fe4b44f483b1f0cdcdf0b4267498bb4aa678c45c781d0cd323a72c1d472cf.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "A proof is given in Appendix B.5. Therefore, the Fenchel-Young and Fitzpatrick losses generated by $\\Omega$ exactly coincide in this case. ", "page_idx": 5}, {"type": "text", "text": "Fitzpatrick sparseMAP and Fitzpatrick sparsemax losses. As our first example where FenchelYoung and Fitzpatrick losses substantially differ, we introduce the Fitzpatrick sparseMAP loss, which is the Fitzpatrick counterpart of the sparseMAP loss [24]. ", "page_idx": 5}, {"type": "text", "text": "Proposition 5 Fitzpatrick sparseMAP loss   \nWhen $\\begin{array}{r}{\\Omega(y^{\\prime})\\,=\\,\\frac{1}{2}\\|y^{\\prime}\\|_{2}^{2}+\\iota c(y^{\\prime})}\\end{array}$ , where $\\mathcal{C}$ is a closed convex set, we have for all $y\\in\\mathcal{C}$ and   \n$\\theta\\in\\mathbb{R}^{k}$ $L_{F[\\partial\\Omega]}(y,\\theta)=2\\Omega^{*}\\left((y+\\theta)/2\\right)-\\left<y,\\theta\\right>=\\left<y^{\\star}-y,\\theta-y^{\\star}\\right>$   \nwhere we used $y^{\\star}$ as a shorthand for $y_{F[\\partial\\Omega]}^{\\star}(y,\\theta)=\\nabla\\Omega^{\\ast}((y+\\theta)/2)=P_{\\mathcal{C}}((y+\\theta)/2).$ ", "page_idx": 5}, {"type": "text", "text": "A proof is given in Appendix B.6. As a special case, when $\\mathcal{C}=\\bigtriangleup^{k}$ , we call the obtained loss the Fitzpatrick sparsemax loss, as it is the counterpart of the sparsemax loss [21]. Like the sparseMAP and sparsemax losses, these new losses rely on the Euclidean projection as a core building block. The Euclidean projection onto the probability simplex $\\triangle^{k}$ can be computed exactly in $O(k)$ expected time and $O(k\\log k)$ worst-case time [9, 22, 14, 12]. ", "page_idx": 5}, {"type": "text", "text": "Fitzpatrick logistic loss. We now derive the Fitzpatrick couterpart of the logistic loss. Before stating the next proposition, we recall the definition of the Lambert $W$ function [13]. For $z\\geq0$ , $W(z)$ is the inverse of the function $f(w)=w\\exp(w)$ , that is, $W(z)=f^{-1}(z)=\\bar{w}$ . ", "page_idx": 6}, {"type": "image", "img_path": "7Dep87TMJs/tmp/1803445ac0e6a6e0f98c23ec408b8a0cdc1f926e73240028be7bbe683edeeaa5.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "A proof and the value of $\\lambda^{\\star}=\\lambda^{\\star}(y,\\theta)\\in\\mathbb{R}$ are given in Appendix B.7. To obtain $\\lambda^{\\star}(y,\\theta)$ , we need to solve a one-dimensional root equation, which can be done using for instance a bisection. ", "page_idx": 6}, {"type": "text", "text": "3.3 Relation with Fenchel-Young losses ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "On first sight, Fitzpatrick losses and Fenchel-Young losses appear quite different. In the next proposition, we show that the Fitzpatrick loss generated by $\\Omega$ is in fact equal to the Fenchel-Young loss generated by the modified, target-dependent function ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\Omega_{y}(y^{\\prime}):=\\Omega(y^{\\prime})+D_{\\Omega}(y,y^{\\prime}),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $D_{\\Omega}$ is the generalized Bregman divergence defined in (1). In particular, Lemma 1 in the appendix shows that if $\\Omega=\\Psi+\\iota_{\\mathcal{C}}$ , then $\\Omega_{y}(y^{\\prime})=\\Psi_{y}(y^{\\prime})+\\iota_{\\mathcal{C}}(y^{\\prime})=\\bar{\\Psi_{}}(y^{\\prime})+D_{\\Psi}(y,y^{\\prime})+\\iota_{\\mathcal{C}}(y^{\\prime}).$ . ", "page_idx": 6}, {"type": "table", "img_path": "7Dep87TMJs/tmp/e2af55db0c579d8e363c02dd3d40a87cb58256a3960448281432deb392797689.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "This characterization of the Fitzpatrick function $F[\\partial\\Omega]$ is also new to our knowledge. A proof is given in Appendix B.8. Proposition 7 is very useful, as it means that Fitzpatrick losses inherit from all the known properties of Fenchel-Young losses, analyzed in prior works [7, 5]. In particular, Fenchel-Young losses are smooth (i.e., with Lipschitz gradients) when $\\Omega$ is strongly convex. We therefore immediately obtain that Fitzpatrick losses are smooth if $\\Omega$ is strongly convex and $D_{\\Omega}$ is convex in its second argument, which is the case when $\\Omega(y^{\\prime})\\,=\\,\\textstyle{\\frac{1}{2}}\\|y^{\\prime}\\|_{2}^{2}$ and $\\bar{\\Omega}(y^{\\prime})\\,=\\,\\langle y^{\\prime},\\log y^{\\prime}\\rangle$ . Therefore, the Fitzpatrick sparsemax and logistic losses are smooth. However in the general case, this does not hold, as $D_{\\Omega}$ is usually not convex in its second argument. Proposition 7 also provides a mean to compute Fitzpatrick losses and their gradient. Finally, it suggests a very natural geometric interpretation of Fitzpatrick losses, as presented in Figure 2. ", "page_idx": 6}, {"type": "text", "text": "3.4 Relation with generalized Bregman divergences ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "As we stated before, the generalized Bregman divergence $D_{\\Omega}(y,y^{\\prime})$ in (1) is a primal-primal divergence, as both $y$ and $y^{\\prime}$ belong to the same primal space. In contrast, Fenchel-Young losses $L_{\\Omega\\oplus\\Omega^{*}}(y,\\theta)$ are primal-dual, since $y$ belongs to the primal space and $\\theta$ belongs to the dual space. ", "page_idx": 6}, {"type": "image", "img_path": "7Dep87TMJs/tmp/b5ed74c695d3504707e0fe52a7ca08e51b5f0d168e82e102dccced53e5a62ab3.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 2: Geometric interpretation, with $\\Omega(y^{\\prime})=\\textstyle{\\frac{1}{2}}\\|y^{\\prime}\\|_{2}^{2}$ . The Fenchel-Young loss $L_{\\Omega\\oplus\\Omega^{*}}(y,\\theta)$ is the gap (depicted with a double-headed arrow) between $\\Omega(y)$ and $\\left\\langle y,\\theta\\right\\rangle-\\Omega^{*}(\\theta)$ , the value at $y$ of the tangent with slope $\\theta$ and intercept $-\\Omega^{*}(\\theta)$ . As per Proposition 7, the Fitzpatrick loss $L_{F[\\partial\\Omega]}(y,\\theta)$ is equal to $L_{\\Omega_{y}\\oplus\\Omega_{y}^{*}}(y,\\theta)$ and is therefore equal to the gap between $\\Omega_{y}(y)=\\Omega(y)$ and $\\langle y,\\dot{\\theta}\\rangle-\\Omega_{y}^{*}(\\theta)$ , the value at $y$ of the tangent with slope $\\theta$ and intercept $-\\Omega_{y}^{*}(\\theta)$ . Since $\\Omega_{y}(y^{\\prime})=\\Omega(y^{\\prime})+D_{\\Omega}(y,y^{\\prime})$ , we have that $\\Omega_{y}(y^{\\prime})\\,\\geq\\,\\Omega(y^{\\prime})$ , with equality when $y\\,=\\,y^{\\prime}$ . We therefore have $\\Omega_{y}^{\\ast}(\\theta)\\,\\leq\\,\\Omega^{\\ast}(\\theta)$ , implying that the Fitzpatrick loss is a lower bound of the Fenchel-Young loss. ", "page_idx": 7}, {"type": "text", "text": "Both can however be related, since ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\theta^{\\prime}\\in\\partial\\Omega(y^{\\prime})}{\\operatorname*{inf}}\\,L_{\\Omega\\oplus\\Omega^{*}}(y,\\theta^{\\prime})=\\underset{\\theta^{\\prime}\\in\\partial\\Omega(y^{\\prime})}{\\operatorname*{inf}}\\,\\Omega(y)+\\Omega^{*}(\\theta^{\\prime})-\\langle y,\\theta^{\\prime}\\rangle}\\\\ &{\\quad=\\Omega(y)+\\underset{\\theta^{\\prime}\\in\\partial\\Omega(y^{\\prime})}{\\operatorname*{inf}}\\,\\Omega^{*}(\\theta^{\\prime})-\\langle y,\\theta^{\\prime}\\rangle}\\\\ &{\\quad=\\Omega(y)-\\underset{\\theta^{\\prime}\\in\\partial\\Omega(y^{\\prime})}{\\operatorname*{sup}}-\\Omega^{*}(\\theta^{\\prime})+\\langle y,\\theta^{\\prime}\\rangle}\\\\ &{\\quad=\\Omega(y)-\\Omega(y^{\\prime})-\\underset{\\theta^{\\prime}\\in\\partial\\Omega(y^{\\prime})}{\\operatorname*{sup}}\\,\\langle y-y^{\\prime},\\theta^{\\prime}\\rangle}\\\\ &{\\quad=D_{\\Omega}(y,y^{\\prime})}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where, in the penultimate line, we have used that $\\Omega^{*}(\\theta^{\\prime})=\\langle y^{\\prime},\\theta^{\\prime}\\rangle-\\Omega(y^{\\prime})$ , as $\\theta^{\\prime}\\in\\partial\\Omega(y^{\\prime})$ . This identity suggests that we can create Bregman-like primal-primal divergences by replacing $\\Omega\\oplus\\Omega^{*}$ with $F[\\partial\\Omega]$ , ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal D_{F[\\partial\\Omega]}(y,y^{\\prime}):=\\operatorname*{inf}_{\\theta^{\\prime}\\in\\partial\\Omega(y^{\\prime})}L_{F[\\partial\\Omega]}(y,\\theta^{\\prime})=\\operatorname*{inf}_{\\theta^{\\prime}\\in\\partial\\Omega(y^{\\prime})}F[\\partial\\Omega](y,\\theta^{\\prime})-\\langle y,\\theta^{\\prime}\\rangle.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "This recovers one of the two Bregman-like divergences proposed in [10], the other one replacing the inf above by a sup. As stated in [10], $F[\\partial\\Omega]$ and $\\Omega\\oplus\\Omega^{*}$ are representations of $\\partial\\Omega$ . ", "page_idx": 7}, {"type": "text", "text": "In order to use a primal-primal divergence as a loss, we need to explicitly compose it with a link function, such as $\\hat{\\mathcal{Y}}_{\\Omega}(\\theta)\\doteq\\nabla\\Omega^{*}(\\theta)$ . Unfortunately, $D_{\\Omega}(y,\\widehat{y}_{\\Omega}(\\theta))$ or $\\dot{\\mathcal{D}}_{F[\\partial\\Omega]}(y,\\widehat{\\y_{\\Omega}}(\\theta))$ are typically nonconvex functions of $\\theta$ , while Fenchel-Young and Fitzpatrick losses are always convex. In addition, differentiating through $\\widehat{\\boldsymbol{y}}_{\\Omega}(\\boldsymbol{\\theta})$ typically requires implicit differentiation [19, 4], while Fenchel-Young and Fitzpatrick losses enjoy easy-to-compute gradients, thanks to Danskin\u2019s theorem. ", "page_idx": 7}, {"type": "text", "text": "3.5 Lower bound on Fitzpatrick losses ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "If $\\Omega=\\Psi+\\iota_{\\mathcal{C}}$ , where $\\Psi:\\mathbb{R}^{k}\\rightarrow\\overline{{\\mathbb{R}}}$ is a proper convex Legendre-type function and $c\\subseteq\\operatorname{dom}\\Psi$ , then it was shown in [7, Proposition 3] that Fenchel-Young losses satisfy the lower bound ", "page_idx": 7}, {"type": "equation", "text": "$$\nD_{\\Psi}(y,\\widehat{y})\\leq L_{\\Omega\\oplus\\Omega^{*}}(y,\\theta),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "with equality if $\\mathcal{C}=\\operatorname{dom}\\Psi$ , where we used $\\widehat{y}$ as a shorthand for $\\widehat{y}_{\\Omega}(\\theta)$ . We now show that a similar result holds for Fitzpatrick losses. Similarly   as before, we define $\\Psi_{y}(\\dot{y}^{\\prime}):=\\Psi(y^{\\prime})+D_{\\Psi}(y,y^{\\prime})$ . ", "page_idx": 7}, {"type": "table", "img_path": "7Dep87TMJs/tmp/a9547e5102517027fdb747a85313d0eb042d0c325ed6cf1cf322831d3459adcf.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "A proof is given in Appendix B.9. If $\\Psi_{y}$ is $\\mu$ -strongly convex, we obtain $\\begin{array}{r}{\\frac{\\mu}{2}\\|y-y^{\\star}\\|_{2}^{2}\\leq D_{\\Psi_{y}}(y,y^{\\star})}\\end{array}$ . ", "page_idx": 8}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Experimental setup. We follow exactly the same experimental setup as in [6, 7]. We consider a dataset of $n$ pairs $(x_{i},y_{i})$ of feature vector $x_{i}\\in\\mathbb{R}^{d}$ and label proportions $y_{i}\\in\\Delta^{k}$ , where $d$ is the number of features and $k$ is the number of classes. At inference time, given an unknown input vector $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ , our goal is to estimate a vector of label proportions $\\widehat{y}\\in\\bar{\\Delta^{k}}$ . A model is specified by a matrix $W\\in\\mathbb{R}^{k\\times d}$ and a convex l.s.c. function $\\Omega:\\mathbb{R}^{k}\\rightarrow\\overline{{\\mathbb{R}}}$ . Pr e dictions are then produced by the generalized linear model $x\\mapsto\\widehat{y}_{\\Omega}(W x)$ . At training time, we estimate the matrix $\\dot{W}\\in\\mathbb{R}^{k\\times d}$ by minimizing the convex objective ", "page_idx": 8}, {"type": "equation", "text": "$$\nR_{L,\\lambda}(W):=\\sum_{i=1}^{n}L(y_{i},W x_{i})+\\frac{\\lambda}{2}\\left\\|W\\right\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $L\\in\\left\\{L_{\\Omega\\oplus\\Omega^{*}},L_{F\\lbrack\\partial\\Omega\\rbrack}\\right\\}$ . We focus on the (Fitzpatrick) sparsemax and the (Fitzpatrick) logistic losses. We optimize (5) using the L-BFGS algorithm [20]. The gradient of the Fenchel-Young loss is given in (2), while the gradient of the Fitzpatrick loss is given in Proposition 1, item 4. Experiments were conducted on a Intel Xeon E5-2667 clocked at 3.30GHz with 192 GB of RAM running on Linux. Our implementation relies on the SciPy [29] and scikit-learn [25] libraries. ", "page_idx": 8}, {"type": "text", "text": "We ran experiments on 11 standard multi-label benchmark datasets1 (see Table 2 in Appendix A for statistics on the datasets). For all datasets, we removed samples with no label, normalized samples to have zero mean unit variance, and normalized labels to lie in the probability simplex. We chose the hyperparameter $\\lambda\\in\\{10^{-4},10^{-3},\\ldots,10^{4}\\}$ against the validation set. We report test set mean squared error (also known as Brier score in a probabilistic forecasting context) in Table 1. ", "page_idx": 8}, {"type": "text", "text": "Results. We found that the logistic loss and the Fitzpatrick logistic loss are comparable on most datasets, with the logistic loss significantly winning on 2 datasets and the Fitzpatrick logistic loss significantly winning on 2 datasets, out of 11. Since the Fitzpatrick logistic loss is slightly more computationally demanding, requiring to solve a root equation while the logistic loss does not, we believe that the logistic loss remains the best choice when we wish to use the softargmax as link function $\\widehat{y}_{\\Omega}$ . ", "page_idx": 8}, {"type": "text", "text": "Similarly, we found that the sparsemax loss and the Fitzpatrick sparsemax loss are comparable on most datasets, with the sparsemax loss significantly winning on only 1 dataset out of 11 and the Fitzpatrick loss significantly winning on 2 datasets out of 11. Since the two losses both use the Euclidean projection onto the simplex $P_{\\triangle^{k}}$ as their link function $\\widehat{y}_{\\Omega}$ , we conclude that the Fitzpatrick sparsemax loss is a serious contender to the sparsemax loss, especially when predicting sparse label proportions is important. ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We proposed to leverage the Fitzpatrick function, a theoretical tool from monotone operator theory, to build a new family of primal-dual convex loss functions for machine learning. We reinterpreted Fitzpatrick losses as lower bounds of Fenchel-Young losses that maintains the same link function. Our paper therefore challenges the idea that there can only be one loss function, convex in each argument, associated with a certain link function. For instance, we created the Fitzpatrick logistic and sparsemax losses, that are associated with the soft argmax and sparse argmax links, traditionally associated with the logistic and sparsemax losses, respectively. We believe that even more loss functions with the same link can be created, which calls for a systematic study of their properties and respective beneftis. In future work, we intend to study calibration guarantees for Fitzpatrick losses, to test new link functions from the maximal monotone operator theory and to implement more efficient training for the Fitzpatrick logistic loss. ", "page_idx": 8}, {"type": "table", "img_path": "7Dep87TMJs/tmp/6e686c103cd500fab7eb8e4fc6d89751a83d4c07d1ea6de5e5651af559e8eaa2.jpg", "table_caption": [], "table_footnote": ["Table 1: Test performance comparison between the sparsemax loss, the logistic loss and their Fitzpatrick counterparts on the task of label proportion estimation, with regularization parameter $\\lambda$ tuned against the validation set. For each dataset, label proportion errors are measured using the mean squared error (MSE). We use bold if the error is at least 0.005 lower than its counterpart. "], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] S.-i. Amari. Information geometry and its applications, volume 194. Springer, 2016.   \n[2] H. Bauschke, D. McLaren, and H. Sendov. Fitzpatrick functions: Inequalities, examples, and remarks on a problem by S. Fitzpatrick. Journal of Convex Analysis, 13, 07 2005.   \n[3] H. H. Bauschke and P. L. Combettes. Convex analysis and monotone operator theory in Hilbert spaces. CMS Books in Mathematics/Ouvrages de Math\u00e9matiques de la SMC. Springer-Verlag, New York, second edition, 2017.   \n[4] M. Blondel, Q. Berthet, M. Cuturi, R. Frostig, S. Hoyer, F. Llinares-Lopez, F. Pedregosa, and J.-P. Vert. Efficient and modular implicit differentiation. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 5230\u20135242. Curran Associates, Inc., 2022.   \n[5] M. Blondel, F. Llinares-L\u00f3pez, R. Dadashi, L. Hussenot, and M. Geist. Learning energy networks with generalized Fenchel-Young losses. Advances in Neural Information Processing Systems, 35:12516\u201312528, 2022.   \n[6] M. Blondel, A. Martins, and V. Niculae. Learning classifiers with Fenchel-Young losses: Generalized entropies, margins, and algorithms. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 606\u2013615. PMLR, 2019.   \n[7] M. Blondel, A. F. Martins, and V. Niculae. Learning with Fenchel-Young losses. Journal of Machine Learning Research, 21(35):1\u201369, 2020.   \n[8] J. M. Borwein and A. S. Lewis. Convex analysis and nonlinear optimization, volume 3 of CMS Books in Mathematics/Ouvrages de Math\u00e9matiques de la SMC. Springer, New York, second edition, 2006. Theory and examples.   \n[9] P. Brucker. An $O(n)$ algorithm for quadratic knapsack problems. Operations Research Letters, 3(3):163\u2013166, 1984.   \n[10] R. S. Burachik and J. E. Mart\u00ednez-Legaz. On Bregman-type distances for convex functions and maximally monotone operators. Set-Valued and Variational Analysis, 26:369\u2013384, 2018.   \n[11] R. S. Burachik and B. F. Svaiter. Maximal monotone operators, convex functions and a special family of enlargements. Set-Valued Analysis, 10:297\u2013316, 2002.   \n[12] L. Condat. Fast projection onto the simplex and the $\\ell_{1}$ ball. Mathematical Programming, 158(1-2):575\u2013585, 2016.   \n[13] R. M. Corless, G. H. Gonnet, D. E. G. Hare, D. J. Jeffrey, and D. E. Knuth. On the Lambert W function. Advances in Computational Mathematics, 5(1):329\u2013359, Dec 1996.   \n[14] J. C. Duchi, S. Shalev-Shwartz, Y. Singer, and T. Chandra. Efficient projections onto the $\\ell_{1}$ -ball for learning in high dimensions. In Proc. of ICML, 2008.   \n[15] S. Fitzpatrick. Representing monotone operators by convex functions. In Workshop/Miniconference on Functional Analysis and Optimization, volume 20, pages 59\u201366. Australian National University, Mathematical Sciences Institute, 1988.   \n[16] T. Gneiting and A. E. Raftery. Strictly proper scoring rules, prediction, and estimation. Journal of the American statistical Association, 102(477):359\u2013378, 2007.   \n[17] P. D. Gr\u00fcnwald and A. P. Dawid. Game theory, maximum entropy, minimum discrepancy and robust Bayesian decision theory. 2004.   \n[18] K. C. Kiwiel. Proximal minimization methods with generalized Bregman functions. SIAM journal on control and optimization, 35(4):1142\u20131168, 1997.   \n[19] S. G. Krantz and H. R. Parks. The implicit function theorem: history, theory, and applications. Springer Science & Business Media, 2002.   \n[20] D. C. Liu and J. Nocedal. On the limited memory BFGS method for large scale optimization. Mathematical programming, 45(1):503\u2013528, 1989.   \n[21] A. Martins and R. Astudillo. From softmax to sparsemax: A sparse model of attention and multi-label classification. In International conference on machine learning, pages 1614\u20131623. PMLR, 2016.   \n[22] C. Michelot. A finite algorithm for finding the projection of a point onto the canonical simplex of $\\mathbb{R}^{n}$ . Journal of Optimization Theory and Applications, 50(1):195\u2013200, 1986.   \n[23] J. J. Moreau. Inf-convolution, sous-additivit\u00e9, convexit\u00e9 des fonctions num\u00e9riques. J. Math. Pures Appl. (9), 49:109\u2013154, 1970.   \n[24] V. Niculae, A. Martins, M. Blondel, and C. Cardie. Sparsemap: Differentiable sparse structured inference. In International Conference on Machine Learning, pages 3799\u20133808. PMLR, 2018.   \n[25] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, et al. Scikit-learn: Machine learning in python. the Journal of machine Learning research, 12:2825\u20132830, 2011.   \n[26] M. D. Reid and R. C. Williamson. Composite binary losses. The Journal of Machine Learning Research, 11:2387\u20132422, 2010.   \n[27] R. T. Rockafellar and R. J.-B. Wets. Variational Analysis. sv, Berlin, 1998.   \n[28] E. K. Ryu and W. Yin. Large-scale convex optimization: algorithms & analyses via monotone operators. Cambridge University Press, 2022.   \n[29] P. Virtanen, R. Gommers, T. E. Oliphant, M. Haberland, T. Reddy, D. Cournapeau, E. Burovski, P. Peterson, W. Weckesser, J. Bright, et al. Scipy 1.0: fundamental algorithms for scientific computing in python. Nature methods, 17(3):261\u2013272, 2020.   \n[30] R. C. Williamson, E. Vernet, and M. D. Reid. Composite multiclass losses. Journal of Machine Learning Research, 17(222):1\u201352, 2016. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "A Datasets statistics ", "text_level": 1, "page_idx": 11}, {"type": "table", "img_path": "7Dep87TMJs/tmp/8c2a66ea3805c68a81116d3e342ddd0902fb433b6fba737786ce2b061f24297b.jpg", "table_caption": [], "table_footnote": ["Table 2: Datasets statistics "], "page_idx": 11}, {"type": "text", "text": "B Proofs ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "B.1 Lemmas ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "Lemma 1 Generalized Bregman divergence for constrained $\\Omega$   \nLet $\\Omega\\,=\\,\\Psi\\,+\\,\\iota c$ , where $\\Psi\\,:\\,\\mathbb{R}^{k}\\,\\rightarrow\\,\\overline{{\\mathbb{R}}}$ is proper convex l.s.c. and $\\mathcal{C}\\,\\subseteq\\,\\dim\\Psi$ such that   \n$\\mathrm{ri}\\mathcal{C}\\cap\\mathrm{ri}\\,\\mathrm{dom}\\,\\Psi\\neq\\emptyset,$ , where riC is the relative interior of $\\mathcal{C}$ . Then, for all $y,y^{\\prime}\\in\\mathbb{R}^{k}$ $D_{\\Omega}(y,y^{\\prime})=D_{\\Psi}(y,y^{\\prime})+D_{\\iota c}(y,y^{\\prime}).$ ", "page_idx": 11}, {"type": "text", "text": "Proof. ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "As $\\mathcal{C}$ , $\\mathrm{dom}\\,\\Psi\\subset\\mathbb{R}^{k}$ and $\\mathrm{ri}\\mathcal{C}\\cap\\mathrm{ri}\\,\\mathrm{dom}\\,\\Psi\\neq\\emptyset$ , we can apply [3, Proposition 6.19] and [3, Theorem 16.47] to write $\\partial\\Omega(y^{\\prime})=\\partial\\Psi(y^{\\prime})+\\partial\\iota_{\\mathcal{C}}(y^{\\prime})$ . ", "page_idx": 11}, {"type": "text", "text": "Thus, we have ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D_{\\Omega}(y,y^{\\prime}):=\\Omega(y)-\\Omega(y^{\\prime})-\\underset{\\theta^{\\prime}\\in\\partial\\Omega(y^{\\prime})}{\\operatorname*{sup}}\\langle y-y^{\\prime},\\theta^{\\prime}\\rangle}\\\\ &{\\quad\\quad\\quad=\\Psi(y)+\\iota_{C}(y)-\\Psi(y^{\\prime})-\\iota_{C}(y^{\\prime})-\\underset{\\theta^{\\prime}\\in\\partial\\Psi(y^{\\prime})+\\partial\\iota_{C}(y^{\\prime})}{\\operatorname*{sup}}\\langle y-y^{\\prime},\\theta^{\\prime}\\rangle}\\\\ &{\\quad\\quad\\quad=\\Psi(y)-\\Psi(y^{\\prime})-\\underset{\\theta^{\\prime}\\in\\partial\\Psi(y^{\\prime})}{\\operatorname*{sup}}\\langle y-y^{\\prime},\\theta^{\\prime}\\rangle+\\iota_{C}(y)-\\iota_{C}(y^{\\prime})-\\underset{\\theta^{\\prime}\\partial\\iota_{C}(y^{\\prime})}{\\operatorname*{sup}}\\langle y-y^{\\prime},\\theta^{\\prime}\\rangle}\\\\ &{\\quad\\quad\\quad=D_{\\Psi}(y,y^{\\prime})+D_{\\iota_{C}}(y,y^{\\prime}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "table", "img_path": "7Dep87TMJs/tmp/3776e3e6bc31e5d68273e264b89f5a7da5f623862b00e18db72c681eec102abb.jpg", "table_caption": [], "table_footnote": [], "page_idx": 11}, {"type": "text", "text": "Proof. Using (1) and the fact that $\\partial\\iota_{\\mathcal{C}}=N_{\\mathcal{C}}$ [3, Example 16.13], we obtain that ", "page_idx": 11}, {"type": "equation", "text": "$$\nD_{\\iota c}(y,y^{\\prime}):=\\iota c(y)-\\iota c(y^{\\prime})-\\operatorname*{sup}_{\\theta^{\\prime}\\in N_{\\mathcal{C}}(y^{\\prime})}\\langle y-y^{\\prime},\\theta^{\\prime}\\rangle.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "When $y^{\\prime}\\in\\mathcal{C}$ and $y\\in{\\mathcal{C}}$ , ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\theta^{\\prime}\\in N_{C}(y^{\\prime})}\\langle y-y^{\\prime},\\theta^{\\prime}\\rangle=\\operatorname*{sup}_{\\theta^{\\prime}\\in\\mathbb{R}^{k}\\atop\\{z-y^{\\prime},\\theta^{\\prime}\\}\\leq0}\\langle y-y^{\\prime},\\theta^{\\prime}\\rangle=0.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "When $y^{\\prime}\\in\\mathcal{C}$ and $y\\notin\\mathcal{C},D_{\\iota c}(y,y^{\\prime})=+\\infty$ , as $+\\infty+\\left(-\\infty\\right)=+\\infty$ in the definition of the Bregman divergence. Therefore, when $y^{\\prime}\\in\\dot{C}\\;D_{\\iota c}\\left(y,y^{\\prime}\\right)=\\iota c\\left(\\dot{y}\\right)$ . ", "page_idx": 11}, {"type": "text", "text": "When $y^{\\prime}\\notin\\mathcal{C},N_{\\mathcal{C}}(y^{\\prime})=\\varnothing$ . Again, in the definition of the Bregman divergence, $+\\infty+(-\\infty)=+\\infty$ and we use the convention $\\mathrm{sup}_{\\emptyset}=-\\infty$ . ", "page_idx": 11}, {"type": "text", "text": "Lemma 3 Bregman divergence of $\\Psi_{y}$ ", "page_idx": 12}, {"type": "text", "text": "Let $\\Psi:\\mathbb{R}^{k}\\rightarrow\\overline{{\\mathbb{R}}}$ be proper, convex and twice differentiable on the interior of its domain. For all $y\\in\\mathbb{R}^{k}$ , let $\\Psi_{y}:=\\Psi+D_{\\Psi}(y,\\cdot)$ . Then, for all $y,y^{\\prime},y^{\\prime\\prime}\\in\\operatorname{int}\\operatorname{dom}\\Psi,$ , ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D_{\\Psi_{y}}(y^{\\prime\\prime},y^{\\prime})=D_{\\Psi}(y,y^{\\prime\\prime})-D_{\\Psi}(y,y^{\\prime})+D_{\\Psi}(y^{\\prime\\prime},y^{\\prime})+\\langle y^{\\prime\\prime}-y^{\\prime},\\nabla^{2}\\Psi(y^{\\prime})(y-y^{\\prime})\\rangle}\\\\ &{\\qquad\\qquad=\\langle y^{\\prime\\prime}-y,\\nabla\\Psi(y^{\\prime\\prime})\\rangle-\\langle y^{\\prime\\prime}-y,\\nabla\\Psi(y^{\\prime})\\rangle+\\langle y^{\\prime\\prime}-y^{\\prime},\\nabla^{2}\\Psi(y^{\\prime})(y-y^{\\prime})\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "and in particular for all $y,y^{\\prime}\\in\\operatorname{int}\\operatorname{dom}\\Psi$ ", "page_idx": 12}, {"type": "equation", "text": "$$\nD_{\\Psi_{y}}(y,y^{\\prime})=\\langle y-y^{\\prime},\\nabla^{2}\\Psi(y^{\\prime})(y-y^{\\prime})\\rangle.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Proof. For all $y,y^{\\prime}\\in\\operatorname{int}\\operatorname{dom}\\Psi$ , as $\\Psi$ is convex and differentiable on int $\\mathrm{dom}\\,\\Psi$ , we have that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Psi_{y}(y^{\\prime})=\\Psi(y^{\\prime})+D_{\\Psi}(y,y^{\\prime})}\\\\ &{\\qquad\\quad=\\Psi(y^{\\prime})+\\Psi(y)-\\Psi(y^{\\prime})-\\langle y-y^{\\prime},\\nabla\\Psi(y^{\\prime})\\rangle}\\\\ &{\\qquad\\quad=\\Psi(y)-\\langle y-y^{\\prime},\\nabla\\Psi(y^{\\prime})\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "and therefore, as $\\Psi$ is twice differentiable on int dom $\\Psi$ , we get that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\nabla\\Psi_{y}(y^{\\prime})=\\nabla^{2}\\Psi(y^{\\prime})(y^{\\prime}-y)+\\nabla\\Psi(y^{\\prime}).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Therefore, for all $y,y^{\\prime},y^{\\prime\\prime}\\in\\operatorname{dom}\\Psi,$ , ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D_{\\Psi_{y}}(y^{\\prime\\prime},y^{\\prime})=\\Psi_{y}(y^{\\prime\\prime})-\\Psi_{y}(y^{\\prime})-\\langle y^{\\prime\\prime}-y^{\\prime},\\nabla\\Psi_{y}(y^{\\prime})\\rangle}\\\\ &{\\qquad\\qquad=\\Psi(y^{\\prime\\prime})+D_{\\Psi}(y,y^{\\prime\\prime})-\\Psi(y^{\\prime})-D_{\\Psi}(y,y^{\\prime})-\\langle y^{\\prime\\prime}-y^{\\prime},\\nabla^{2}\\Psi(y^{\\prime})(y^{\\prime}-y)\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad-\\langle y^{\\prime\\prime}-y^{\\prime},\\nabla\\Psi(y^{\\prime})\\rangle}\\\\ &{\\qquad\\qquad=D_{\\Psi}(y,y^{\\prime\\prime})-D_{\\Psi}(y,y^{\\prime})+D_{\\Psi}(y^{\\prime\\prime},y^{\\prime})+\\langle y^{\\prime\\prime}-y^{\\prime},\\nabla^{2}\\Psi(y^{\\prime})(y-y^{\\prime})\\rangle}\\\\ &{\\qquad\\qquad=\\langle y^{\\prime\\prime}-y,\\nabla\\Psi(y^{\\prime\\prime})\\rangle-\\langle y^{\\prime\\prime}-y,\\nabla\\Psi(y^{\\prime})\\rangle+\\langle y^{\\prime\\prime}-y^{\\prime},\\nabla^{2}\\Psi(y^{\\prime})(y-y^{\\prime})\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "and in particular using the just last established equality with the triplet $(y,y,y^{\\prime})$ , we have for all $y,y^{\\prime}\\in\\mathbf{\\bar{dom}}\\,\\Psi$ , ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D_{\\Psi_{y}}(y,y^{\\prime})=\\langle y-y,\\nabla\\Psi(y)\\rangle-\\langle y-y,\\nabla\\Psi(y^{\\prime})\\rangle+\\langle y-y^{\\prime},\\nabla^{2}\\Psi(y^{\\prime})(y-y^{\\prime})\\rangle}\\\\ &{\\qquad\\qquad=\\langle y-y^{\\prime},\\nabla^{2}\\Psi(y^{\\prime})(y-y^{\\prime})\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Lemma 4 Generalized Bregman divergence of negentropy ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Let $\\alpha\\,\\in\\,\\mathbb{R}$ . Let $\\begin{array}{r}{\\Psi(y^{\\prime})\\,:=\\,\\sum_{i=1}^{k}y_{i}^{\\prime}\\log y_{i}^{\\prime}-\\alpha\\sum_{i=1}^{k}y_{i}^{\\prime}}\\end{array}$ be defined for $y^{\\prime}\\,\\in\\,\\mathbb{R}_{+}^{k}$ . Then, for y, y\u2032 \u2208Rk+, ", "page_idx": 12}, {"type": "equation", "text": "$$\nD_{\\Psi}(y,y^{\\prime})=\\sum_{i=1}^{k}y_{i}\\log{\\frac{y_{i}}{y_{i}^{\\prime}}}-\\sum_{i=1}^{k}(y_{i}-y_{i}^{\\prime})+\\iota_{\\mathbb{R}_{++}^{k}}(y^{\\prime}).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Proof. First, if $y^{\\prime}\\in\\mathbb{R}_{++}^{k}$ , $\\Psi$ is differentiable at $y^{\\prime}$ and $\\nabla\\Psi(y^{\\prime})_{i}=\\log y_{i}+1-\\alpha$ . Thus, $\\partial\\Psi(y^{\\prime})=$ $\\{\\nabla\\Psi(y^{\\prime})\\}$ and ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D_{\\Psi}(y,y^{\\prime})=\\Psi(y)-\\Psi(y^{\\prime})-\\displaystyle\\operatorname*{sup}_{\\theta^{\\prime}\\in\\partial\\Psi(y^{\\prime})}\\langle y-y^{\\prime},\\theta^{\\prime}\\rangle,}\\\\ &{\\quad\\quad\\quad\\quad=\\Psi(y)-\\Psi(y^{\\prime})-\\langle y-y^{\\prime},\\nabla\\Psi(y^{\\prime})\\rangle,}\\\\ &{\\quad\\quad\\quad\\quad=\\displaystyle\\sum_{i=1}^{k}y_{i}\\log\\displaystyle\\frac{y_{i}}{y_{i}^{\\prime}}-\\displaystyle\\sum_{i=1}^{k}(y_{i}-y_{i}^{\\prime}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Second, if we prove that $\\partial\\Psi(y^{\\prime})\\,=\\,\\emptyset$ when there exists a component $i$ such that $y_{i}^{\\prime}=0$ , we can conclude the proof, as $\\mathrm{sup}_{\\emptyset}=-\\infty$ by convention. Indeed, Let us assume that $y_{i}^{\\prime}=0$ . Suppose that $\\theta^{\\prime}\\in\\partial\\Psi(y^{\\prime})$ . Then, by definition of subgradients, ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\langle y^{\\prime\\prime}-y^{\\prime},\\theta^{\\prime}\\rangle+\\Psi(y^{\\prime})\\leq\\Psi(y^{\\prime\\prime}),\\;\\forall y^{\\prime\\prime}\\in\\mathbb{R}_{++}^{k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "We consider $\\varepsilon>0$ and choose $y^{\\prime\\prime}=y^{\\prime}+\\varepsilon e_{i}$ , where $e_{i}$ is the $i$ -th canonical base vector. Thus, we obtain ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\varepsilon\\theta_{i}\\leq\\Psi(y^{\\prime}+\\varepsilon e_{i})-\\Psi(y^{\\prime}),}\\\\ {\\displaystyle\\qquad=\\sum_{j=1}^{k}y_{j}^{\\prime}\\log y_{j}^{\\prime}+\\varepsilon\\log\\varepsilon-\\alpha\\sum_{j=1}^{k}y_{j}^{\\prime}-\\alpha\\varepsilon-\\big(\\displaystyle\\sum_{j=1}^{k}y_{j}^{\\prime}\\log y_{j}^{\\prime}-\\alpha\\displaystyle\\sum_{j=1}^{k}y_{j}^{\\prime}\\big),}\\\\ {\\displaystyle\\qquad=\\varepsilon\\log\\varepsilon-\\alpha\\varepsilon,}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "as $y_{i}=0$ and $0\\log0=0$ by convention. By noticing that $\\begin{array}{r}{\\operatorname*{lim}_{\\varepsilon\\to0^{+}}\\big(\\varepsilon\\log\\varepsilon-\\alpha\\varepsilon\\big)/\\varepsilon=-\\infty}\\end{array}$ , we get a contradiction, which concludes the proof. ", "page_idx": 13}, {"type": "text", "text": "Lemma 5 Value and gradient of $\\Psi_{y}^{*}$ ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Let $\\Psi:\\mathbb{R}^{k}\\rightarrow\\overline{{\\mathbb{R}}}$ be a proper strictly convex twice differentiable function. Let us assume that $D_{\\Psi}(y,y^{\\prime})$ is convex w.r.t. $y^{\\prime}$ , for all $y\\in\\dim\\Psi$ . We remind that $\\dot{\\Psi}_{y}(y^{\\prime})=\\Psi(y^{\\prime})+D_{\\Psi}(y,y^{\\prime})$ . Then, for all $\\theta\\in\\mathbb{R}^{k}$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\Psi_{y}^{*}(\\theta)=\\langle\\tilde{y},\\theta\\rangle-\\Psi(y)+\\langle y-\\tilde{y},\\nabla\\Psi(\\tilde{y})\\rangle}\\\\ &{\\nabla\\Psi_{y}^{*}(\\theta)=\\tilde{y}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\tilde{y}$ is the unique element of the set ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{argmax}_{y^{\\prime}\\in\\mathrm{dom}}\\langle y^{\\prime},\\theta\\rangle+\\langle y-y^{\\prime},\\nabla\\Psi(y^{\\prime})\\rangle=\\big\\{y^{\\prime}\\;\\big|\\;\\nabla^{2}\\Psi(y^{\\prime})(y^{\\prime}-y)=\\theta-\\nabla\\Psi(y^{\\prime})\\big\\}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof. As $\\Psi\\leq\\Psi_{y}$ , we have $\\operatorname{dom}\\Psi_{y}\\subset\\operatorname{dom}\\Psi$ . Thus, we get ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Psi_{y}^{*}(\\theta)=\\underset{y^{\\prime}\\in\\mathrm{dom}}{\\operatorname*{sup}}\\langle y^{\\prime},\\theta\\rangle-\\Psi_{y}(y^{\\prime})}\\\\ &{\\qquad=\\underset{y^{\\prime}\\in\\mathrm{dom}}{\\operatorname*{sup}}\\langle y^{\\prime},\\theta\\rangle-\\big(\\Psi(y^{\\prime})+\\Psi(y)-\\Psi(y^{\\prime})-\\langle y-y^{\\prime},\\nabla\\Psi(y^{\\prime})\\rangle\\big)}\\\\ &{\\qquad=\\underset{y^{\\prime}\\in\\mathrm{dom}}{\\operatorname*{sup}}\\langle y^{\\prime},\\theta\\rangle-\\Psi(y)+\\langle y-y^{\\prime},\\nabla\\Psi(y^{\\prime})\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "As $\\Psi$ is strictly convex and $D_{\\Psi}(y,y^{\\prime})$ is convex w.r.t. $y^{\\prime}$ , we have that $\\Psi_{y}$ is strictly convex. Thus, according to [8, Theorem 4.2.5], $\\Psi_{y}^{*}$ is differentiable. Using Danskin\u2019s theorem, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\{\\nabla\\Psi_{y}^{*}(\\theta)\\right\\}=\\underset{y^{\\prime}\\in\\mathrm{dom}\\,\\Psi}{\\mathrm{argmax}}\\left\\langle y^{\\prime},\\theta\\right\\rangle-\\Psi(y)+\\left\\langle y-y^{\\prime},\\nabla\\Psi(y^{\\prime})\\right\\rangle}\\\\ &{\\qquad\\qquad=\\underset{y^{\\prime}\\in\\mathrm{dom}\\,\\Psi}{\\mathrm{argmax}}\\left\\langle y^{\\prime},\\theta\\right\\rangle+\\left\\langle y-y^{\\prime},\\nabla\\Psi(y^{\\prime})\\right\\rangle\\!.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Setting the gradient of the inner function to zero concludes the proof. ", "page_idx": 13}, {"type": "text", "text": "Lemma 6 Gradient of $\\Psi_{y}^{*},$ squared norm case ", "page_idx": 13}, {"type": "text", "text": "Let $\\begin{array}{r}{\\Psi(y^{\\prime}):=\\frac{1}{2}\\|y^{\\prime}\\|_{2}^{2}}\\end{array}$ and $\\Psi_{y}$ defined as in Lemma 5. Then, the gradient of $\\Psi_{y}^{*}$ is given by ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\nabla\\Psi_{y}^{*}(\\theta)=\\frac{y+\\theta}{2}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof. Using Lemma 5 with $\\nabla\\Psi(y^{\\prime})=y^{\\prime}$ and $\\nabla^{2}\\Psi(y^{\\prime})=I$ , we obtain that $\\nabla\\Psi_{y}^{*}(\\theta)$ is the solution w.r.t. $y^{\\prime}$ of the equation $y^{\\prime}-y=\\theta-y^{\\prime}$ . Rearranging the terms concludes the proof. ", "page_idx": 13}, {"type": "text", "text": "Before stating the next lemma, we recall the definition of the Lambert function [13] $W:\\mathbb{R}_{+}\\to\\mathbb{R}_{+}$ . The function $W$ is the inverse of the function $f\\,:\\,\\mathbb{R}_{+}\\,\\rightarrow\\,\\mathbb{R}_{+}$ where $f(w)\\,=\\,w\\exp(w)$ for all $w\\in\\mathbb{R}_{+}$ . ", "page_idx": 14}, {"type": "table", "img_path": "7Dep87TMJs/tmp/9029b6520b790f23e7ddf2d9442d3ce1d09ce43df413e80fa3cf8f552858f811.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "Proof. Using Lemma 5, we know that $\\tilde{y}$ is the solution of $\\nabla^{2}\\Psi(\\tilde{y})(\\tilde{y}-y)=\\theta-\\nabla\\Psi(\\tilde{y})$ . Using $\\nabla\\Psi(\\tilde{y})=\\log\\tilde{y}+1-\\alpha$ and $\\nabla^{2}\\Psi(\\tilde{y})=1/\\tilde{y}$ (where logarithm and division are performed elementwise), we obtain for all $i\\in[k]$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n(\\widetilde y_{i}-y_{i})/\\widetilde y_{i}=\\theta_{i}-\\log\\widetilde y_{i}-1+\\alpha\\iff1-y_{i}/\\widetilde y_{i}=\\theta_{i}-\\log\\widetilde y_{i}-1+\\alpha.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "When $y_{i}=0$ , we immediatly have $\\tilde{y}_{i}=\\exp({\\theta_{i}-2+\\alpha})$ . When $y_{i}>0$ , after rearranging, we obtain ", "page_idx": 14}, {"type": "equation", "text": "$$\n{\\frac{y_{i}}{\\tilde{y}_{i}}}\\exp\\left({\\frac{y_{i}}{\\tilde{y}_{i}}}\\right)=y_{i}\\exp(-(\\theta_{i}-2+\\alpha))\\iff{\\frac{y_{i}}{\\tilde{y}_{i}}}=W(y_{i}\\exp(-(\\theta_{i}-2+\\alpha))),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "hence the result. ", "page_idx": 14}, {"type": "text", "text": "Lemma 8 Gradient of $\\Omega_{y}^{*}$   \nLet $\\Psi:\\mathbb{R}^{k}\\rightarrow\\overline{{\\mathbb{R}}}$ be a proper strictly convex l.s.c. function and assume that, for all $y\\in\\mathbb{R}^{k}$ , the   \nfunction $D_{\\Psi}(y,\\cdot)$ is convex. Let $\\Omega_{y}=\\Psi_{y}+\\iota c$ with $\\Psi_{y}$ defined as in Lemma 5 and assume that   \n$\\mathcal{C}$ is a closed convex set included in dom $\\Psi$ . Then $\\nabla\\Omega_{y}^{*}(\\theta)$ is the unique element of the set $\\operatorname*{argmax}_{y^{\\prime}\\in\\mathcal{C}}\\;\\langle y^{\\prime},\\theta\\rangle+\\langle y-y^{\\prime},\\nabla\\Psi(y^{\\prime})\\rangle.$ ", "page_idx": 14}, {"type": "text", "text": "Proof. The result again follows from Danskin\u2019s theorem. ", "page_idx": 14}, {"type": "text", "text": "Lemma 9 Dual of simplex-constrained conjugate ", "page_idx": 14}, {"type": "text", "text": "Let $\\Psi:\\mathbb{R}^{k}\\rightarrow\\overline{{\\mathbb{R}}}$ be a proper strictly convex function. Then, ", "page_idx": 14}, {"type": "equation", "text": "$$\n(\\Psi+\\iota_{\\triangle^{k}})^{*}(\\theta)=\\operatorname*{min}_{\\tau\\in\\mathbb{R}}\\tau+(\\Psi+\\iota_{\\mathbb{R}_{+}^{k}})^{*}(\\theta-\\tau\\mathbf{1}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\nabla(\\Psi+\\iota_{\\triangle^{k}})^{*}(\\theta)=\\nabla(\\Psi+\\iota_{\\mathbb{R}_{+}^{k}})^{*}(\\theta-\\tau^{\\star}{\\bf1}),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\tau^{\\star}$ denotes an optimal dual variable (not necessarily unique). ", "page_idx": 14}, {"type": "text", "text": "Proof. ", "text_level": 1, "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\Psi+\\iota_{\\triangle^{k}}\\right)^{*}(\\theta)=\\underset{y^{\\prime}\\in\\triangle^{k}}{\\operatorname*{max}}\\left<y^{\\prime},\\theta\\right>-\\Psi(y^{\\prime})}\\\\ &{\\qquad\\qquad\\qquad=\\underset{y^{\\prime}\\in\\mathbb{R}_{+}^{k}}{\\operatorname*{max}}\\,\\underset{\\tau\\in\\mathbb{R}}{\\operatorname*{min}}\\langle y^{\\prime},\\theta\\rangle-\\Psi(y^{\\prime})-\\tau(\\left<y^{\\prime},\\mathbf{1}\\right>-1)}\\\\ &{\\qquad\\qquad\\quad=\\underset{\\tau\\in\\mathbb{R}}{\\operatorname*{min}}\\,\\tau+\\underset{y^{\\prime}\\in\\mathbb{R}_{+}^{k}}{\\operatorname*{max}}\\,\\langle y^{\\prime},\\theta-\\tau\\mathbf{1}\\rangle-\\Psi(y^{\\prime})}\\\\ &{\\qquad\\qquad\\quad=\\underset{\\tau\\in\\mathbb{R}}{\\operatorname*{min}}\\,\\tau+(\\Psi+\\iota_{\\mathbb{R}_{+}^{k}})^{*}(\\theta-\\tau\\mathbf{1}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where we used that, as the constraints of belonging to the simplex are affine, they are qualified and we can invert the max and the min. ", "page_idx": 14}, {"type": "text", "text": "As $\\Psi$ is strictly convex, $(\\Psi+\\iota_{\\triangle^{k}})^{*}$ and $(\\Psi+\\iota_{\\mathbb{R}_{+}^{k}})^{*}$ are differentiable [8, Theorem 4.2.5]. We conclude using Danskin\u2019s theorem. ", "page_idx": 14}, {"type": "text", "text": "Lemma 10 Gradient of $\\Omega_{y}^{*}$ , negentropy, constrained to the simplex   \nLet $\\Psi(y^{\\prime})\\,=\\,\\langle y^{\\prime},\\log y^{\\prime}\\rangle$ , if $\\,^{\\cdot}y^{\\prime}\\in\\mathbb{R}_{+}^{k}$ , $+\\infty$ otherwise. Then, for all $y\\in\\mathbb{R}_{+}^{k}$ , the gradient of   \n$\\Omega_{y}=\\Psi_{y}+\\iota_{\\triangle^{k}}$ (as defined in Lemma 8) is given by $\\nabla\\Omega_{y}^{*}(\\theta)_{i}=\\left\\{\\begin{array}{l l}{\\mathrm{e}^{-\\lambda^{\\star}}\\mathrm{e}^{\\theta_{i}}\\quad}&{i f\\,y_{i}=0,}\\\\ {\\frac{y_{i}}{W\\left(y_{i}\\mathrm{e}^{\\lambda^{\\star}-\\theta_{i}}\\right)}\\quad}&{i f\\,y_{i}>0.}\\end{array}\\right.$   \nwhere $\\lambda^{\\star}$ is the unique solution of the equation $\\mathrm{e}^{-\\lambda^{\\star}}\\sum_{i:y_{i}=0}\\mathrm{e}^{\\theta_{i}}+\\sum_{i:y_{i}>0}\\frac{y_{i}}{W\\big(y_{i}\\mathrm{e}^{-(\\theta_{i}-\\lambda^{\\star})}\\big)}=1.$ ", "page_idx": 15}, {"type": "text", "text": "Proof. From Lemma 8 and Lemma 9, since $\\operatorname{dom}\\Psi_{y}=\\mathbb{R}_{+}^{k}$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\boldsymbol{y}^{\\star}=\\nabla\\Omega_{\\boldsymbol{y}}^{\\ast}(\\boldsymbol{\\theta})=\\nabla\\Psi_{\\boldsymbol{y}}^{\\ast}(\\boldsymbol{\\theta}-\\tau^{\\star}\\mathbf{1})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\tau^{\\star}$ is a solution of ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\tau\\in\\mathbb{R}}\\tau+\\Psi_{y}^{*}(\\theta-\\tau\\mathbf{1}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Setting the gradient of the inner function to zero, we get ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\langle\\nabla\\Psi_{y}^{*}(\\theta-\\tau^{\\star}{\\bf1}),{\\bf1}\\rangle=1.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Using Lemma 7, we obtain that $\\tau^{\\star}$ satisfies ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathrm{e}^{-\\tau^{\\star}-2}\\sum_{i:y_{i}=0}\\mathrm{e}^{\\theta_{i}}+\\sum_{i:y_{i}>0}\\frac{y_{i}}{W\\big(y_{i}\\mathrm{e}^{-(\\theta_{i}-\\tau^{\\star}-2)}\\big)}=1.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By monotonicity in $\\tau^{\\star}$ , we conclude that $\\tau^{\\star}$ is unique. Using the change of variable $\\tau^{\\star}=\\lambda^{\\star}+2$ concludes the proof. ", "page_idx": 15}, {"type": "text", "text": "B.2 Proof of Proposition 1 (Properties of Fitzpatrick losses) ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Apart from differentiability, the proofs follow from the study of Fitzpatrick functions found in [15, 2, 28]. We include the proofs for completeness. ", "page_idx": 15}, {"type": "text", "text": "Link function and non-negativity. We recall that ", "text_level": 1, "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L_{F[\\partial\\Omega]}(y,\\theta)=\\underset{(y^{\\prime},\\theta^{\\prime})\\in\\partial\\Omega}{\\operatorname*{sup}}\\langle y^{\\prime}-y,\\theta-\\theta^{\\prime}\\rangle}\\\\ &{\\qquad\\qquad\\qquad=-\\underset{(y^{\\prime},\\theta^{\\prime})\\in\\partial\\Omega}{\\operatorname*{inf}}\\langle y^{\\prime}-y,\\theta^{\\prime}-\\theta\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "From the monotonicity of $\\partial\\Omega$ , we have that if $(y,\\theta)\\in\\partial\\Omega$ and $(y^{\\prime},\\theta^{\\prime})\\in\\partial\\Omega$ , then $\\langle y^{\\prime}\\!-\\!y,\\theta^{\\prime}\\!-\\!\\theta\\rangle\\geq0$ . Therefore, for all $(y,\\theta)\\in\\partial\\Omega$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{(y^{\\prime},\\theta^{\\prime})\\in\\partial\\Omega}\\langle y^{\\prime}-y,\\theta^{\\prime}-\\theta\\rangle=0,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "with the infimum being attained at $(y^{\\prime},\\theta^{\\prime})=(y,\\theta)$ . This proves the link function. ", "page_idx": 15}, {"type": "text", "text": "From the maximality of $\\partial\\Omega$ , if $(y,\\theta)\\notin\\partial\\Omega$ , there exists $(y^{\\prime},\\theta^{\\prime})\\in\\partial\\Omega$ such that $\\left\\langle y^{\\prime}-y,\\theta^{\\prime}-\\theta\\right\\rangle<0$ . Therefore, for all $(y,\\theta)\\notin\\partial\\Omega$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{(y^{\\prime},\\theta^{\\prime})\\in\\partial\\Omega}\\langle y^{\\prime}-y,\\theta^{\\prime}-\\theta\\rangle<0.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This proves the non-negativity. ", "page_idx": 15}, {"type": "text", "text": "Convexity. We recall that ", "page_idx": 15}, {"type": "equation", "text": "$$\nL_{F[\\partial\\Omega]}(y,\\theta)=F[\\partial\\Omega](y,\\theta)-\\langle y,\\theta\\rangle\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where ", "page_idx": 15}, {"type": "equation", "text": "$$\nF\\big[\\partial\\Omega\\big](y,\\theta)=\\operatorname*{sup}_{(y^{\\prime},\\theta^{\\prime})\\in\\partial\\Omega}\\langle y-y^{\\prime},\\theta^{\\prime}\\rangle+\\langle y^{\\prime},\\theta\\rangle=\\operatorname*{sup}_{(y^{\\prime},\\theta^{\\prime})\\in\\partial\\Omega}\\langle y^{\\prime},\\theta\\rangle+\\langle y,\\theta^{\\prime}\\rangle-\\langle y^{\\prime},\\theta^{\\prime}\\rangle.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The function $(y,\\theta)\\mapsto\\langle y^{\\prime},\\theta\\rangle+\\langle y,\\theta^{\\prime}\\rangle-\\langle y^{\\prime},\\theta^{\\prime}\\rangle$ is jointly convex in $(y,\\theta)$ for all $(y^{\\prime},\\theta^{\\prime})$ . Since the supremum preserves convexity, $F[\\partial\\Omega](y,\\theta)$ is jointly convex in $(y,\\theta)$ . The function $\\langle y,\\theta\\rangle$ is separately convex $/$ concave in $y$ and $\\theta$ but not jointly convex $/$ concave in $(y,\\theta)$ . Therefore, $L_{F[\\partial\\Omega]}(y,\\dot{\\theta})$ is separately convex in $y$ and $\\theta$ . ", "page_idx": 15}, {"type": "text", "text": "Differentiability. Since $\\Omega(y^{\\prime})$ is strictly convex and $y^{\\prime}\\mapsto D_{\\Omega}(y,y^{\\prime})$ is convex, $\\Omega_{y}(y^{\\prime})=\\Omega(y^{\\prime})\\,+$ $D_{\\Omega}(y,y^{\\prime})$ is strictly convex in $y^{\\prime}$ . From the duality between strict convexity and differentiability, $\\Omega_{y}^{*}(\\theta)$ is differentiable in $\\theta$ . ", "page_idx": 16}, {"type": "text", "text": "Tighter inequality. Using ", "text_level": 1, "page_idx": 16}, {"type": "equation", "text": "$$\n\\partial\\Omega=\\{(y^{\\prime},\\theta^{\\prime})\\colon\\Omega(y)\\geq\\Omega(y^{\\prime})+\\langle y-y^{\\prime},\\theta^{\\prime}\\rangle\\;\\forall y\\}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\Omega^{\\ast}(\\theta)=\\operatorname*{sup}_{y^{\\prime}\\in\\mathbb{R}^{k}}\\langle y^{\\prime},\\theta\\rangle-\\Omega(y^{\\prime}),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "we get for any $(y^{\\prime},\\theta^{\\prime})\\in\\partial\\Omega$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle y-y^{\\prime},\\theta^{\\prime}\\rangle+\\langle y^{\\prime},\\theta\\rangle\\leq\\Omega(y)-\\Omega(y^{\\prime})+\\langle y^{\\prime},\\theta\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\Omega(y)+\\Omega^{*}(\\theta).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore ", "page_idx": 16}, {"type": "equation", "text": "$$\nF[\\partial\\Omega](y,\\theta)=\\operatorname*{sup}_{(y^{\\prime},\\theta^{\\prime})\\in\\partial\\Omega}\\langle y-y^{\\prime},\\theta^{\\prime}\\rangle+\\langle y^{\\prime},\\theta\\rangle\\leq\\Omega(y)+\\Omega^{*}(\\theta).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "B.3 Proof of Proposition 2 (Expression of Fitzpatrick loss when $\\Omega$ is twice differentiable) ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We recall that ", "page_idx": 16}, {"type": "equation", "text": "$$\nF[\\partial\\Omega](y,\\theta)=\\operatorname*{sup}_{(y^{\\prime},\\theta^{\\prime})\\in\\partial\\Omega}\\langle y,\\theta^{\\prime}\\rangle+\\langle y^{\\prime},\\theta\\rangle-\\langle y^{\\prime},\\theta^{\\prime}\\rangle=\\operatorname*{sup}_{y^{\\prime}\\in\\mathrm{dom}}\\langle y^{\\prime},\\theta\\rangle+\\operatorname*{sup}_{\\theta^{\\prime}\\in\\partial\\Omega(y^{\\prime})}\\langle y,\\theta^{\\prime}\\rangle-\\langle y^{\\prime},\\theta^{\\prime}\\rangle.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Since $\\Omega$ is differentiable, we have $\\partial\\Omega(y^{\\prime})=\\{\\nabla\\Omega(y^{\\prime})\\}$ and therefore $\\theta^{\\prime}=\\nabla\\Omega(y^{\\prime})$ , which gives ", "page_idx": 16}, {"type": "equation", "text": "$$\nF[\\partial\\Omega](y,\\theta)=\\operatorname*{sup}_{y^{\\prime}\\in\\mathbb{R}^{k}}\\langle y,\\nabla\\Omega(y^{\\prime})\\rangle+\\langle y^{\\prime},\\theta\\rangle-\\langle y^{\\prime},\\nabla\\Omega(y^{\\prime})\\rangle.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Setting the gradient of the inner function w.r.t. $y^{\\prime}$ to zero, we get ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\nabla^{2}\\Omega(y^{\\prime})y+\\theta-\\nabla\\Omega(y^{\\prime})-\\nabla^{2}\\Omega(y^{\\prime})y^{\\prime}=0.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Using the $y^{\\prime}=y^{\\star}$ and $\\theta^{\\prime}=\\nabla\\Omega(y^{\\prime})$ in Definition 1, we then obtain ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{L_{F[\\partial\\Omega]}(y,\\theta)=\\langle y^{\\prime}-y,\\theta-\\theta^{\\prime}\\rangle}&{}\\\\ {=\\langle y^{\\prime}-y,\\theta-\\nabla\\Omega(y^{\\prime})\\rangle}&{}\\\\ {=\\langle y^{\\prime}-y,\\nabla^{2}\\Omega(y^{\\prime})(y^{\\prime}-y)\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "B.4 Proof of Proposition 3 (squared loss) ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Using Proposition 2 with $\\nabla\\Omega(y^{\\prime})=y^{\\prime}$ and $\\nabla^{2}\\Omega(y^{\\prime})=I$ , we obtain ", "page_idx": 16}, {"type": "equation", "text": "$$\ny+\\theta-2y^{\\prime}=0\\iff y^{\\prime}={\\frac{y+\\theta}{2}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We therefore obtain ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L_{F[O\\Omega]}(y,\\theta)=\\left<\\frac{y+\\theta}{2}-y,\\theta-\\frac{y+\\theta}{2}\\right>}\\\\ &{\\qquad\\qquad\\qquad=\\left<\\frac{\\theta-y}{2},\\frac{\\theta-y}{2}\\right>}\\\\ &{\\qquad\\qquad\\qquad=\\frac{1}{4}\\lVert y-\\theta\\rVert_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "B.5 Proof of Proposition 4 (perceptron loss) ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "A proof of the Fitzpatrick function for this case was given in [2, Example 3.1]. We include a proof for completeness. Since $\\boldsymbol\\Omega=\\iota_{\\mathcal{C}}$ , we have dom $\\Omega={\\mathcal{C}}$ . Therefore, for all $y\\in{\\mathcal{C}}$ and $\\theta\\in\\mathbb{R}^{k}$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F[\\partial\\Omega](y,\\theta)=\\underset{y^{\\prime}\\in\\mathrm{dom}}{\\operatorname*{sup}}\\langle y^{\\prime},\\theta\\rangle+\\underset{\\theta^{\\prime}\\in\\partial\\Omega(y^{\\prime})}{\\operatorname*{sup}}\\langle y-y^{\\prime},\\theta^{\\prime}\\rangle}\\\\ &{=\\underset{y^{\\prime}\\in\\mathcal{C}}{\\operatorname*{sup}}\\langle y^{\\prime},\\theta\\rangle-\\left(\\iota_{\\mathcal{C}}(y)-\\iota_{\\mathcal{C}}(y^{\\prime})-\\underset{\\theta^{\\prime}\\in\\partial\\iota_{\\mathcal{C}}(y^{\\prime})}{\\operatorname*{sup}}\\langle y-y^{\\prime},\\theta^{\\prime}\\rangle\\right)}\\\\ &{=\\underset{y^{\\prime}\\in\\mathcal{C}}{\\operatorname*{sup}}\\langle y^{\\prime},\\theta\\rangle-D_{\\iota_{\\mathcal{C}}}(y,y^{\\prime})}\\\\ &{=\\underset{y^{\\prime}\\in\\mathcal{C}}{\\operatorname*{sup}}\\langle y^{\\prime},\\theta\\rangle,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where in the third line we used that $\\iota c(y)=\\iota c(y^{\\prime})=0$ and where in the last line we used Lemma 2. Therefore, for all $y\\in\\mathbb{R}^{k}$ and $\\boldsymbol\\theta\\in\\mathbb{R}^{k}$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\nF[\\partial\\Omega](y,\\theta)=\\operatorname*{sup}_{y^{\\prime}\\in\\mathcal{C}}\\langle y^{\\prime},\\theta\\rangle+\\iota_{\\mathcal{C}}(y)=\\iota_{\\mathcal{C}}(y)+\\iota_{\\mathcal{C}}^{*}(\\theta).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "B.6 Proof of Proposition 5 (Fitzpatrick sparseMAP loss) ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "A proof of the Fitzpatrick function for this case was given in [2, Example 3.13]. We provide an alternative proof. ", "page_idx": 17}, {"type": "text", "text": "From Proposition 7, we know that ", "page_idx": 17}, {"type": "equation", "text": "$$\nF[\\partial\\Omega](y,\\theta)=\\Omega_{y}(y)+\\Omega_{y}^{\\ast}(\\theta)=\\Omega(y)+\\Omega_{y}^{\\ast}(\\theta),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Omega_{y}(y^{\\prime})=\\displaystyle\\frac{1}{2}\\|y^{\\prime}\\|_{2}^{2}+\\displaystyle\\frac{1}{2}\\|y-y^{\\prime}\\|_{2}^{2}+\\iota c(y^{\\prime})}\\\\ &{\\qquad\\quad=\\|y^{\\prime}\\|_{2}^{2}+\\displaystyle\\frac{1}{2}\\|y\\|_{2}^{2}-\\langle y,y^{\\prime}\\rangle+\\iota c(y^{\\prime})}\\\\ &{\\qquad=2\\Omega(y^{\\prime})+\\Omega(y)-\\langle y,y^{\\prime}\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Using conjugate calculus, we obtain ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\Omega_{y}^{*}(\\theta)=2\\Omega^{*}\\left(\\frac{y+\\theta}{2}\\right)-\\Omega(y).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, ", "page_idx": 17}, {"type": "equation", "text": "$$\nF[\\partial\\Omega](y,\\theta)=2\\Omega^{*}\\left(\\frac{y+\\theta}{2}\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "From Proposition 7, the supremum w.r.t. $y^{\\prime}$ is achieved at $y^{\\star}=\\nabla\\Omega^{\\ast}((y+\\theta)/2)=P c((y+\\theta)/2)$ . We therefore obtain ", "page_idx": 17}, {"type": "equation", "text": "$$\nL_{F[\\partial\\Omega]}(y,\\theta)=\\langle y^{\\star}-y,\\theta-y^{\\star}\\rangle.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "B.7 Proof of Proposition 6 (Fitzpatrick logistic loss) ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Differentiability w.r.t. $\\theta$ and formula of gradient. According to Proposition 7, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\nL_{F[\\partial\\Omega]}(y,\\theta)=\\Omega_{y}(y)+\\Omega_{y}^{\\ast}(\\theta)-\\langle y,\\theta\\rangle.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Thus the differentiability w.r.t. $\\theta$ of $L_{F[\\partial\\Omega]}(y,\\theta)$ follows from the differentiability of $\\Omega_{y}^{*}(\\theta)$ . Lemma 10 yields the differentiability of $\\Omega_{y}^{*}(\\theta)$ and a formula for its gradient $y_{F[\\partial\\Omega]}^{\\star}(y,\\theta):=\\,\\nabla\\Omega_{y}^{\\ast}(\\theta)$ . ", "page_idx": 17}, {"type": "equation", "text": "$$\ny_{F[\\partial\\Omega]}^{\\star}(y,\\theta)_{i}=\\left\\{\\begin{array}{c c}{\\mathrm{e}^{-\\lambda^{\\star}}\\mathrm{e}^{\\theta_{i}},\\,\\,\\mathrm{if}\\,\\,y_{i}=0,}\\\\ {\\frac{y_{i}}{W(y_{i}\\mathrm{e}^{\\lambda^{\\star}-\\theta_{i}})},\\,\\,\\mathrm{if}\\,\\,y_{i}>0.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "It follows that $\\nabla_{\\theta}L_{F[\\partial\\Omega]}(y,\\theta)=y_{F[\\partial\\Omega]}^{\\star}(y,\\theta)-y$ . ", "page_idx": 17}, {"type": "text", "text": "Formula of the Fitzpatrick logistic loss. We use $y^{\\star}$ as a shorthand for $y_{F[\\partial\\Omega]}^{\\star}(y,\\theta)$ . As we know that $\\Omega_{y}^{\\ast}(\\theta)=\\langle y^{\\star},\\theta\\rangle-\\Omega_{y}(y^{\\star})$ , we use again Proposition 7 to get ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L_{F[\\partial\\Omega]}(y,\\theta)=\\Omega_{y}(y)+\\langle y^{\\star},\\theta\\rangle-\\Omega_{y}(y^{\\star})-\\langle y,\\theta\\rangle}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\Omega(y)-(\\Omega(y^{\\star})+D_{\\Omega}(y,y^{\\star}))+\\langle y^{\\star}-y,\\theta\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "as $\\Omega_{y}(y^{\\prime})=\\Omega(y){+}D_{\\Omega}(y,y^{\\prime})$ and in particular $\\Omega_{y}(y)=\\Omega(y)$ . Furthermore, as $y^{\\star}\\in\\triangle^{k}\\cap\\mathbb{R}_{++}^{k},$ , $\\Omega$ is differentiable at $y^{\\star}$ and $D_{\\Omega}(y,y^{\\star})=\\Omega(y)\\!-\\!\\Omega(y^{\\star})\\!-\\!\\langle y\\!-\\!y^{\\star},\\nabla\\Omega(y^{\\star})\\rangle$ , where $\\nabla\\Omega(y^{\\star})=\\log{y^{\\star}}\\!+\\!1$ . Thus ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L_{F[\\partial\\Omega]}(y,\\theta)=\\langle y-y^{\\star},\\nabla\\Omega(y^{\\star})\\rangle+\\langle y^{\\star}-y,\\theta\\rangle}\\\\ &{\\qquad\\qquad\\qquad=\\langle y^{\\star}-y,\\theta-\\log y^{\\star}-1\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Bisection formula for $\\lambda^{\\star}$ and bounds. We also get from Lemma 10 a bisection formula for $\\lambda^{\\star}$ , which is a shorthand for $\\lambda_{F[\\partial\\Omega]}^{\\star}(y,\\theta)$ . ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathrm{e}^{-\\lambda^{\\star}}\\sum_{i:y_{i}=0}\\mathrm{e}^{\\theta_{i}}+\\sum_{i:y_{i}>0}\\frac{y_{i}}{W\\big(y_{i}\\mathrm{e}^{-(\\theta_{i}-\\lambda^{\\star})}\\big)}=1.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We focus here on a lower bound and an upper bound for $\\lambda^{\\star}\\in\\mathbb{R}$ . Let us prove that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\log\\sum_{i=1}^{k}\\mathrm{e}^{\\theta_{i}}\\leq\\lambda^{\\star}\\leq\\log2+\\operatorname*{max}\\Big\\{\\log\\sum_{i:y_{i}=0}^{k}\\mathrm{e}^{\\theta_{i}},\\log\\ell_{0}(y)+\\operatorname*{max}_{i:y_{i}>0}\\theta_{i}+2\\ell_{0}(y)y_{i}\\Big\\},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\ell_{0}(y)=\\mathrm{Card}(j:y_{j}\\neq0)$ . ", "page_idx": 18}, {"type": "text", "text": "For the lower bound, we use the concavity of the Lambert function W, which impliesW (yie1\u03bb\u22c6\u2212\u03b8i) \u2265 yie\u03bb1\u22c6\u2212\u03b8i . Thus, ", "page_idx": 18}, {"type": "equation", "text": "$$\n1\\geq\\mathrm{e}^{-\\lambda^{\\star}}\\sum_{i:y_{i}=0}\\mathrm{e}^{\\theta_{i}}+\\sum_{i:y_{i}>0}\\frac{y_{i}}{y_{i}\\mathrm{e}^{\\lambda^{\\star}-\\theta_{i}}},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which in turn implies ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathrm{e}^{\\lambda^{\\star}}\\geq\\sum_{i:y_{i}=0}\\mathrm{e}^{\\theta_{i}}+\\sum_{i:y_{i}>0}\\mathrm{e}^{\\theta_{i}}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and yields the lower bound. ", "page_idx": 18}, {"type": "text", "text": "For the upper bound, the function g(\u03bb) = e\u2212\u03bb  i:yi=1 e\u03b8i +  i:yi>0W (yiyei\u03bb\u2212\u03b8i) i s continuous and decreasing (as it is a positive combination of decreasing functions) and $g(-\\infty)=+\\infty$ . Thus if we find a $\\lambda$ such that $g(\\bar{\\lambda})<1$ , we know that $\\lambda^{\\star}\\leq\\lambda$ . ", "page_idx": 18}, {"type": "text", "text": "We deal with each term of $g(\\lambda)$ separately. If $\\lambda\\in\\mathbb{R}$ satisfies ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\mathrm{e}^{-\\lambda}\\sum_{i:y_{i}=0}\\mathrm{e}^{\\theta_{i}}\\leq\\frac{1}{2}}}\\\\ {{\\displaystyle\\operatorname*{max}_{i:y_{i}>0}\\frac{y_{i}}{W(y_{i}\\mathrm{e}^{\\lambda-\\theta_{i}})}\\leq\\frac{1}{2\\ell_{0}(y)},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "then ", "page_idx": 18}, {"type": "equation", "text": "$$\ng(\\lambda)=\\underbrace{\\mathrm{e}^{-\\lambda}\\sum_{i:y_{i}=0}\\mathrm{e}^{\\theta_{i}}}_{\\leq1/2}+\\sum_{i:y_{i}>0}\\underbrace{\\frac{y_{i}}{W\\left(y_{i}\\mathrm{e}^{\\lambda-\\theta_{i}}\\right)}}_{\\leq1/\\left(2\\ell_{0}(y)\\right)}\\leq1.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Thus, all $\\lambda$ satisfying the following inequalties are upper bounds of $\\lambda^{\\star}$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{2\\sum_{i:y_{i}=0}\\mathrm{e}^{\\theta_{i}}\\le\\mathrm{e}^{\\lambda}}}\\\\ &{2\\ell_{0}(y)y_{i}\\le W(y_{i}\\mathrm{e}^{\\lambda-\\theta_{i}}),\\forall i:y_{i}>0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "As $W$ is monotone and $W^{-1}(t)=t\\mathrm{e}^{t}$ , we get ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\log2+\\log\\sum_{i:y_{i}=0}\\mathrm{e}^{\\theta_{i}}\\leq\\lambda}\\\\ {\\displaystyle\\qquad2\\ell_{0}(y)\\mathrm{e}^{2\\ell_{0}(y)y_{i}}\\leq\\mathrm{e}^{\\lambda^{\\star}-\\theta_{i}},\\forall i:y_{i}>0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Thus taking $\\begin{array}{r}{\\lambda=\\operatorname*{max}\\Big\\{\\log2+\\log\\sum_{i:y_{i}=0}^{k}\\mathrm{e}^{\\theta_{i}},\\operatorname*{max}_{i:y_{i}>0}\\log2+\\log\\ell_{0}(y)+\\theta_{i}+2\\ell_{0}(y)y_{i}\\Big\\}}\\end{array}$ yields an upper bound of $\\lambda^{\\star}$ . ", "page_idx": 19}, {"type": "text", "text": "B.8 Proof of Proposition 7 (characterization of $F[\\partial\\Omega]$ using $D_{\\Omega}$ ) ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Let $(y,\\theta)\\in\\mathrm{dom}\\,\\Omega\\times\\mathbb{R}^{k}$ . We have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F[\\partial\\Omega](y,\\theta)=\\underset{(y^{\\prime},\\theta^{\\prime})\\in\\Omega(2)}{\\operatorname*{sup}}\\left(y^{\\prime},\\theta\\right)-\\left\\langle y^{\\prime},\\theta^{\\prime}\\right\\rangle+\\left\\langle y^{\\prime},\\theta\\right\\rangle}\\\\ &{=\\underset{y^{\\prime}\\in\\mathrm{dom}}{\\operatorname*{sup}}\\left\\{(y^{\\prime},\\theta)+\\underset{(e\\in\\partial\\Omega(\\nu)}{\\operatorname*{sup}}\\langle y-y^{\\prime},\\theta^{\\prime}\\rangle\\right\\}}\\\\ &{=\\underset{y^{\\prime}\\in\\mathrm{dom}}{\\operatorname*{sup}}\\left\\{(y^{\\prime},\\theta)-\\Omega(y^{\\prime})+\\Omega(y^{\\prime})+\\underset{(e\\in\\partial\\Omega(\\nu)^{\\prime})}{\\operatorname*{sup}}\\langle y-y^{\\prime},\\theta^{\\prime}\\rangle\\right\\}}\\\\ &{=\\Omega(y)+\\underset{y^{\\prime}\\in\\mathrm{dom}}{\\operatorname*{sup}}\\left\\{(y^{\\prime},\\theta)-\\left(\\Omega(y^{\\prime})+\\Omega(y)-\\Omega(y^{\\prime})-\\underset{(e\\in\\partial\\Omega(\\nu)^{\\prime})}{\\operatorname*{sup}}\\langle y-y^{\\prime},\\theta^{\\prime}\\rangle\\right)\\right\\}}\\\\ &{=\\Omega(y)+\\underset{y^{\\prime}\\in\\mathrm{dom}}{\\operatorname*{sup}}\\left(y^{\\prime},\\theta\\right)-\\left(\\Omega(y^{\\prime})+D_{\\Omega}(y,y^{\\prime})\\right)}\\\\ &{=\\Omega(y)+(\\Omega+D_{\\Omega}(y,\\cdot))^{*}\\left(\\theta\\right)}\\\\ &{=\\Omega_{\\mathrm{p}}(y)+\\underset{(e\\in\\partial\\Omega(\\nu)}{\\operatorname*{sup}}\\langle\\theta\\rangle,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The supremum above is achieved at $y^{\\prime}\\in\\partial\\Omega_{y}^{*}(\\theta)=y_{F[\\partial\\Omega]}^{\\star}(y,\\theta)$ . ", "page_idx": 19}, {"type": "text", "text": "When $\\Omega=\\Psi+\\iota_{\\mathcal{C}}$ , where $c\\subseteq\\operatorname{dom}\\Psi$ , using Lemma 1 and 2, we have for all $y\\in{\\mathcal{C}}$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\Omega_{y}(y^{\\prime})=\\Psi(y^{\\prime})+D_{\\Psi}(y,y^{\\prime})+\\iota_{\\mathcal{C}}(y^{\\prime}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "B.9 Proof of Proposition 8 (lower bound) ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "It was shown in [7, Proposition 3] that if $f=g+\\iota_{C}$ , where $g$ is Legendre type with $c\\subseteq\\operatorname{dom}\\Psi$ , then for all $y\\in{\\mathcal{C}}$ and $\\boldsymbol{\\theta}\\in\\mathbb{R}^{k}$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n0\\leq D_{g}(y,\\nabla f^{*}(\\theta))\\leq L_{f\\oplus f^{*}}(y,\\theta),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "with equality if ${\\mathcal{C}}=\\dim g$ . Using $g=\\Psi_{y}$ , $f=\\Omega_{y}=\\Psi_{y}+\\iota_{\\mathcal{C}},y^{\\star}=\\nabla\\Omega_{y}^{\\ast}(\\theta)=y_{F[\\partial\\Omega]}^{\\star}(y,\\theta)$ , and Lemma 3, we therefore obtain ", "page_idx": 19}, {"type": "equation", "text": "$$\nD_{\\Psi_{y}}(y,y^{\\star})=\\langle y-y^{\\star},\\nabla^{2}\\Psi(y^{\\star})(y-y^{\\star})\\rangle\\leq L_{\\Omega_{y}\\oplus\\Omega_{y}^{\\star}}(y,\\theta)=L_{F[\\partial\\Omega]}(y,\\theta).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "If $\\Psi_{y}$ is $\\mu$ -strongly convex and $D_{\\Psi}$ is convex in its second argument, then $\\Psi_{y}$ is $\\mu$ -strongly convex as well. Therefore, we also have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{\\mu}{2}\\|y-y^{\\star}\\|_{2}^{2}\\leq D_{\\Psi_{y}}(y,y^{\\star}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "C Comment on the inf-addition convention $+\\infty+(-\\infty)=+\\infty$ ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The question of adding conflicting infinites has been thoroughly studied by Moreau in [23]. In this paper, Moreau introduced two additions for $\\mathbb{R}\\cup\\{-\\infty,+\\infty\\}$ . One of Moreau\u2019s two conventions for summing the infinites can be found under the name of inf-addition in [27, Chapter 1]. The other convention is named sup-addition. ", "page_idx": 19}, {"type": "text", "text": "Using the inf-addition on $\\mathbb{R}\\cup\\{-\\infty,+\\infty\\}$ allows, for instance, the equivalence of $\\scriptstyle\\operatorname*{min}_{x\\in C}f(x)$ and $\\mathrm{min}_{x\\in\\mathbb{R}^{n}}\\;f(x)+\\iota_{C}(x)$ . The same goes for the sup-addition but for maximization problems. We used the inf-addition convention for the definition of the generalized Bregman divergence in (1). As the generalized Bregman divergence is to be thought of as a generalization of a distance, it is generally a quantity that will be minimized. Therefore, the inf-addition is a good fti for the definition of the generalized Bregman divergence.   \nIn the proof of Proposition 7 in B.8, the choice of inf-addition is corroborated by the fact that we want to calculate, for some fixed $y\\,\\in\\,\\mathbb{R}^{k}$ , $\\bigl(\\Omega(\\cdot)+D_{\\Omega}(y,\\cdot)\\bigr)(\\theta)\\,=\\,\\operatorname*{sup}_{y^{\\prime}}\\bigl\\langle y^{\\prime},\\bar{\\theta}\\bigr\\rangle\\,-\\,\\bigl(\\Omega(y^{\\prime})\\,+$ $D_{\\Omega}(y,y^{\\prime})),\\forall y^{\\prime}\\in\\mathbb{R}^{k}$ .   \nThe minus sign transforms the inf-addition into the sup-addition when distributed. If we had chosen the sup-addition in the definition of the Bregman divergence $D_{\\Omega}$ , we would here calculate a supremum of an inf-addition, thus entering unknown territory. ", "page_idx": 20}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: As stated in the abstract and in the introduction, we have defined Fitzpatrick losses in Definition 1 and studied their properties in Proposition 1. We instantiated the Fitzpatrick sparseMAP and the Fitzpatrick logistic loss in Proposition 5 and Proposition 6; we have studied the properties of Fitzpatrick losses in Proposition 1 and Proposition 7; we have gathered the results of the label proportion estimation in Table 1. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: We discuss the computational limitations of the Fitzpatrick logistic loss. As stated in Proposition 6 and in Section 4, the computation of the loss value and gradient involves solving a root equation. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: All results in the paper are proved in the appendix. We strived to make the proofs self-contained. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 22}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: In Section 4, we describe the training setup, the linear model we use for predictions and give a link to access the datasets we use. We also indicate that we use the L-BFGS algorithm for training. Furthermore, Proposition 5 and Proposition 6 yield formulae for the gradients of the new losses we introduce. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 22}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 23}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We provided a link for the used datasets in the experiments. The code will be opened upon release. For now, we provide instructions to reproduce the main experimental results. These instructions involved basic supervised learning tools such as L-BFGS algorithm, linear prediction model and standard cross-validation for the hyperparameter. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 23}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: The selection of hyperparameter are discussed in section 4. We use predetermined train-test splits that come with the datasets. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 23}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The split between training sets and test sets are fixed in advance so there is no variability when conducting the label proportion estimation tests. Furthermore, as we minimize convex losses, the convergence of the minimization algorithm is independent of the starting point. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 24}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The characteristics of the computer we used to conduct the tests. On the largest dataset, our experiment only takes a couple of hours to run. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The research conducted does not involve human subjects. We did not create new datasets and used standard open access datasets for multiclassification. After reviewing the \u201csocietal impact and potential harmful consequences\u201d from the Code of Ethics, we conclude that the research conduct in the paper does not pose a risk of harmful consequences. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. \u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. ", "page_idx": 24}, {"type": "text", "text": "\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 25}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: The research conducted in this paper is theoretical and has no societal impact. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 25}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: The datasets used in the tests are publicly available. No new prediction model has been released in this paper. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 25}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: In Section 4, the libraries used for the implementation of the numerical experiments are credited and cited. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 26}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 26}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 26}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}]