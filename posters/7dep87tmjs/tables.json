[{"figure_path": "7Dep87TMJs/tables/tables_5_1.jpg", "caption": "Table 1: Test performance comparison between the sparsemax loss, the logistic loss and their Fitzpatrick counterparts on the task of label proportion estimation, with regularization parameter \u03bb tuned against the validation set. For each dataset, label proportion errors are measured using the mean squared error (MSE). We use bold if the error is at least 0.005 lower than its counterpart.", "description": "This table compares the performance of four different loss functions (sparsemax, Fitzpatrick sparsemax, logistic, and Fitzpatrick logistic) on eleven different datasets.  The performance metric is mean squared error (MSE), which measures the difference between predicted and actual label proportions.  Bold values indicate cases where the MSE is at least 0.005 lower than the corresponding non-Fitzpatrick loss.", "section": "4 Experiments"}, {"figure_path": "7Dep87TMJs/tables/tables_5_2.jpg", "caption": "Table 1: Test performance comparison between the sparsemax loss, the logistic loss and their Fitzpatrick counterparts on the task of label proportion estimation, with regularization parameter \u03bb tuned against the validation set. For each dataset, label proportion errors are measured using the mean squared error (MSE). We use bold if the error is at least 0.005 lower than its counterpart.", "description": "This table compares the performance of four different loss functions (sparsemax, Fitzpatrick sparsemax, logistic, and Fitzpatrick logistic) on eleven multi-label datasets.  The performance metric is mean squared error (MSE), which measures the difference between predicted and true label proportions.  Bold values indicate a significant improvement (at least 0.005 MSE) of a Fitzpatrick loss over its corresponding standard loss function.", "section": "Experiments"}, {"figure_path": "7Dep87TMJs/tables/tables_6_1.jpg", "caption": "Table 1: Test performance comparison between the sparsemax loss, the logistic loss and their Fitzpatrick counterparts on the task of label proportion estimation, with regularization parameter \u03bb tuned against the validation set. For each dataset, label proportion errors are measured using the mean squared error (MSE). We use bold if the error is at least 0.005 lower than its counterpart.", "description": "This table compares the performance of four different loss functions (sparsemax, Fitzpatrick sparsemax, logistic, and Fitzpatrick logistic) on eleven different datasets for the task of label proportion estimation.  The performance metric is mean squared error (MSE), and bold values indicate an MSE improvement of at least 0.005 compared to the corresponding non-Fitzpatrick loss.", "section": "4 Experiments"}, {"figure_path": "7Dep87TMJs/tables/tables_8_1.jpg", "caption": "Table 1: Test performance comparison between the sparsemax loss, the logistic loss and their Fitzpatrick counterparts on the task of label proportion estimation, with regularization parameter \u03bb tuned against the validation set. For each dataset, label proportion errors are measured using the mean squared error (MSE). We use bold if the error is at least 0.005 lower than its counterpart.", "description": "This table compares the performance of four different loss functions: sparsemax loss, Fitzpatrick sparsemax loss, logistic loss, and Fitzpatrick logistic loss.  The comparison is done on eleven different datasets, with the performance metric being the mean squared error (MSE) of label proportion estimation.  The results show that the Fitzpatrick losses sometimes offer slightly better results than their Fenchel-Young counterparts, but the differences are small and not consistently favorable to the Fitzpatrick losses.", "section": "4 Experiments"}, {"figure_path": "7Dep87TMJs/tables/tables_9_1.jpg", "caption": "Table 1: Test performance comparison between the sparsemax loss, the logistic loss and their Fitzpatrick counterparts on the task of label proportion estimation, with regularization parameter \u03bb tuned against the validation set. For each dataset, label proportion errors are measured using the mean squared error (MSE). We use bold if the error is at least 0.005 lower than its counterpart.", "description": "This table presents a comparison of the performance of four different loss functions on eleven multi-label classification datasets.  The loss functions compared are the standard sparsemax and logistic losses, along with their corresponding Fitzpatrick loss counterparts.  Performance is measured by the mean squared error (MSE) of label proportion estimates.  The bold values indicate cases where the Fitzpatrick loss shows an improvement of at least 0.005 MSE compared to its standard counterpart.", "section": "4 Experiments"}, {"figure_path": "7Dep87TMJs/tables/tables_11_1.jpg", "caption": "Table 2: Datasets statistics", "description": "This table presents the statistics of eleven benchmark datasets used in the experiments section of the paper. For each dataset, it provides information on the type of data (e.g., audio, music, text, images, video, microarray), the number of training, development, and test samples, the number of features, the number of classes, and the average number of labels per sample.", "section": "4 Experiments"}, {"figure_path": "7Dep87TMJs/tables/tables_11_2.jpg", "caption": "Table 1: Test performance comparison between the sparsemax loss, the logistic loss and their Fitzpatrick counterparts on the task of label proportion estimation, with regularization parameter \u03bb tuned against the validation set. For each dataset, label proportion errors are measured using the mean squared error (MSE). We use bold if the error is at least 0.005 lower than its counterpart.", "description": "This table compares the performance of four different loss functions (Sparsemax, Fitzpatrick sparsemax, Logistic, and Fitzpatrick logistic) on eleven multi-label datasets in terms of mean squared error (MSE).  The regularization parameter (\u03bb) was tuned via cross-validation.  Bold values indicate that the MSE is at least 0.005 lower than the corresponding loss function without the Fitzpatrick refinement.", "section": "4 Experiments"}, {"figure_path": "7Dep87TMJs/tables/tables_14_1.jpg", "caption": "Table 1: Test performance comparison between the sparsemax loss, the logistic loss and their Fitzpatrick counterparts on the task of label proportion estimation, with regularization parameter \u03bb tuned against the validation set. For each dataset, label proportion errors are measured using the mean squared error (MSE). We use bold if the error is at least 0.005 lower than its counterpart.", "description": "This table compares the performance of four different loss functions for label proportion estimation across eleven datasets.  The loss functions are the standard sparsemax and logistic losses, and their counterparts using the newly proposed Fitzpatrick loss.  Mean squared error (MSE) is used as the performance metric, and bold values indicate where a Fitzpatrick loss outperforms its standard equivalent by at least a margin of 0.005.", "section": "Experiments"}]