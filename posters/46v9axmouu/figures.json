[{"figure_path": "46V9axmOuU/figures/figures_1_1.jpg", "caption": "Figure 1: (a) Comparison of text-to-image generation among different APO methods on various unseen fine-tuned diffusion models. (b) Comparison between model-specific and model-generalized adapters under the model-generalized setting.", "description": "This figure shows a comparison of different Automatic Prompt Optimization (APO) methods' performance on unseen text-to-image diffusion models.  (a) illustrates the generation quality of several APO methods across different models, highlighting their limitations in generalizing to unseen models. (b) contrasts model-specific and model-generalized approaches to prompt adaptation, showcasing the advantages of a generalized approach for improved performance on unseen models. The examples demonstrate that model-generalized adapters achieve higher image quality compared to model-specific adapters when handling unseen models.", "section": "Introduction"}, {"figure_path": "46V9axmOuU/figures/figures_3_1.jpg", "caption": "Figure 2: Illustration of our proposed AP-Adapter.", "description": "This figure illustrates the two-stage process of the AP-Adapter. The first stage uses in-context learning with a large language model (LLM) to rewrite natural language prompts into keyword prompts. These prompts are then fed into the text encoder of a Stable Diffusion (SD) model. The second stage leverages an adapter module to enhance the keyword prompts by decoding domain-specific content from images. Domain prototypes maintain domain-specific details. The adapter integrates information from text and prototypes, aligning the output with manually crafted prompts in the feature space.  The final adapted embedding is used for image generation.", "section": "3 Method"}, {"figure_path": "46V9axmOuU/figures/figures_7_1.jpg", "caption": "Figure 3: Results of diverse APO methods on MGAPO task. Each column displays images from the same APO method, and each row features images generated by the same SD model. APO methods with matching color and SD model share the same domain, indicating that the training data for the APO model was generated by that specific SD model.", "description": "This figure compares the image generation results of different automatic prompt optimization (APO) methods on various unseen Stable Diffusion (SD) models. Each column represents a different APO method (including a novel method called AP-Adapter), and each row shows the results obtained from a specific SD model. The color-coding helps to visualize which SD models were used to train each APO method.  The goal is to demonstrate how well each APO method generalizes to unseen SD models, maintaining image quality and semantic consistency.", "section": "4.4 Comparison to Other Methods"}, {"figure_path": "46V9axmOuU/figures/figures_8_1.jpg", "caption": "Figure 3: Results of diverse APO methods on MGAPO task. Each column displays images from the same APO method, and each row features images generated by the same SD model. APO methods with matching color and SD model share the same domain, indicating that the training data for the APO model was generated by that specific SD model.", "description": "This figure compares the results of different automatic prompt optimization (APO) methods on a model-generalized automatic prompt optimization (MGAPO) task. Each column represents a different APO method, and each row shows images generated using the same stable diffusion (SD) model.  The matching color between columns and rows indicates that the APO model was trained on images from that specific SD model. This visualization effectively demonstrates the generalization capabilities (or lack thereof) of various APO methods when applied to unseen SD models.", "section": "4.4 Comparison to Other Methods"}, {"figure_path": "46V9axmOuU/figures/figures_9_1.jpg", "caption": "Figure 1: (a) Comparison of text-to-image generation among different APO methods on various unseen fine-tuned diffusion models. (b) Comparison between model-specific and model-generalized adapters under the model-generalized setting.", "description": "This figure showcases a comparison of different automatic prompt optimization (APO) methods' performance on unseen text-to-image diffusion models.  Part (a) illustrates the image generation quality for various models using multiple existing APO methods.  Part (b) specifically contrasts the results of a model-specific adapter (trained on a single, fixed model) versus a model-generalized adapter (trained on multiple models), demonstrating the improved generalization capacity of the latter.", "section": "Introduction"}, {"figure_path": "46V9axmOuU/figures/figures_14_1.jpg", "caption": "Figure 2: Illustration of our proposed AP-Adapter.", "description": "This figure illustrates the two-stage process of the AP-Adapter.  The first stage uses in-context learning with a large language model (LLM) to rewrite natural language prompts into keyword prompts, which are then fed into the text encoder of a Stable Diffusion model.  The second stage improves automatic keyword prompts by decoding domain-specific information from images (using CLIP) and aligning it with manually crafted prompts. Domain-specific information is captured through domain prototypes, which serve as anchors for adjusting the prompt representation. The adapter module then integrates information from text and prototypes to produce a final adapted embedding for generating high-quality images.", "section": "3 Method"}, {"figure_path": "46V9axmOuU/figures/figures_15_1.jpg", "caption": "Figure 1: (a) Comparison of text-to-image generation among different APO methods on various unseen fine-tuned diffusion models. (b) Comparison between model-specific and model-generalized adapters under the model-generalized setting.", "description": "This figure shows a comparison of different automatic prompt optimization (APO) methods' performance on unseen diffusion models.  (a) demonstrates how various APO methods perform in generating images from various unseen diffusion models. The results highlight the limitations of existing model-specific methods. (b) compares the performance of model-specific and model-generalized adapters. It emphasizes the challenge of model generalization in APO and highlights the potential of the proposed AP-Adapter.", "section": "1 Introduction"}, {"figure_path": "46V9axmOuU/figures/figures_15_2.jpg", "caption": "Figure 3: Results of diverse APO methods on MGAPO task. Each column displays images from the same APO method, and each row features images generated by the same SD model. APO methods with matching color and SD model share the same domain, indicating that the training data for the APO model was generated by that specific SD model.", "description": "This figure compares the performance of different automatic prompt optimization (APO) methods on the model-generalized automatic prompt optimization (MGAPO) task. Each column represents a different APO method, and each row shows results from a different stable diffusion (SD) model.  The colors of the cells match the SD models used to train the APO models. The image generation quality from each APO model is compared across unseen SD models to demonstrate the ability of the models to generalize.", "section": "4.4 Comparison to Other Methods"}, {"figure_path": "46V9axmOuU/figures/figures_15_3.jpg", "caption": "Figure 5: Visualization of text-conditioned domain distinctiveness. (a) Natural language description of the image. (b) Keyword prompts output by the first-stage large language model. (c) Features output by the second-stage AP-adapter. (d) Manually designed prompts.", "description": "This figure visualizes the domain distinctiveness of different text representations used for image generation.  It shows t-SNE plots of (a) natural language descriptions, (b) keyword prompts generated by a large language model, (c) adapted prompt embeddings produced by the AP-Adapter, and (d) manually designed prompts.  The plots illustrate how the AP-Adapter improves the domain distinctiveness of the prompts compared to the initial natural language descriptions and keyword prompts, making them more suitable for generalization to unseen models.", "section": "4.7 Visualization of Conditioned Features"}, {"figure_path": "46V9axmOuU/figures/figures_16_1.jpg", "caption": "Figure 1: (a) Comparison of text-to-image generation among different APO methods on various unseen fine-tuned diffusion models. (b) Comparison between model-specific and model-generalized adapters under the model-generalized setting.", "description": "The figure demonstrates the performance of different automatic prompt optimization (APO) methods on unseen text-to-image diffusion models.  Panel (a) compares several methods (including the authors' proposed AP-Adapter) on various models, showcasing their ability to generate high-quality images. Panel (b) focuses specifically on the generalization capabilities of model-specific and model-generalized adapters, highlighting the strengths and weaknesses of each approach when dealing with unseen models.", "section": "1 Introduction"}, {"figure_path": "46V9axmOuU/figures/figures_17_1.jpg", "caption": "Figure 3: Results of diverse APO methods on MGAPO task. Each column displays images from the same APO method, and each row features images generated by the same SD model. APO methods with matching color and SD model share the same domain, indicating that the training data for the APO model was generated by that specific SD model.", "description": "This figure shows the results of different automatic prompt optimization (APO) methods on the model-generalized automatic prompt optimization (MGAPO) task. Each column represents a different APO method, while each row shows images generated using a specific stable diffusion (SD) model.  The color-coding links the APO method and SD model used to generate the images.  This visualization demonstrates how well each APO method generalizes to unseen models, highlighting the performance differences between model-specific and model-generalized approaches.", "section": "4.4 Comparison to Other Methods"}, {"figure_path": "46V9axmOuU/figures/figures_17_2.jpg", "caption": "Figure 3: Results of diverse APO methods on MGAPO task. Each column displays images from the same APO method, and each row features images generated by the same SD model. APO methods with matching color and SD model share the same domain, indicating that the training data for the APO model was generated by that specific SD model.", "description": "This figure compares the results of different automatic prompt optimization (APO) methods on the model-generalized automatic prompt optimization (MGAPO) task. Each column represents a different APO method, and each row shows images generated by a specific stable diffusion (SD) model.  The consistent coloring within rows highlights images generated by the same SD model using various APO methods. This visual comparison emphasizes how each APO method performs differently across various unseen SD models, showcasing their generalization capabilities in the MGAPO setting.", "section": "4.4 Comparison to Other Methods"}, {"figure_path": "46V9axmOuU/figures/figures_18_1.jpg", "caption": "Figure 12: Linear combinations of domain prototypes from ith models and domain prototypes from jth models.", "description": "This figure visualizes the impact of linear combinations of domain prototypes on image generation. It shows how blending prototypes from different models (ith and jth models) affects the generated images.  The experiment demonstrates the ability of the AP-Adapter to control the style of generated images by adjusting the weights of different domain prototypes.  The image rows represent varying linear combinations of the selected ith and jth prototypes, showing style transitions between different model characteristics.", "section": "C.4 More Cases for Domain Prototypes Analysis"}, {"figure_path": "46V9axmOuU/figures/figures_19_1.jpg", "caption": "Figure 13: Injection of ith domain prototype information by adjusting the value of \u03b7i.", "description": "This figure visualizes how injecting different proportions of domain prototype information affects image generation.  By modifying the weighting factor (\u03b7i) applied to each domain prototype, the generated images shift in style and characteristics, reflecting the influence of different domain prototypes on the overall image generation process.", "section": "Visualization of Conditioned Features"}, {"figure_path": "46V9axmOuU/figures/figures_20_1.jpg", "caption": "Figure 14: Demonstration of Ablation Study of Loss Functions.", "description": "This figure shows the results of an ablation study on the loss functions used in the AP-Adapter model.  Each column represents a different scenario where one of the loss functions (Lc, La, Lr, Ld) was removed.  The final column shows the results when all loss functions are used. The images illustrate the impact of each loss function on the quality and consistency of the generated images, showcasing the importance of each component in achieving optimal results.", "section": "4.6 Ablation Study"}, {"figure_path": "46V9axmOuU/figures/figures_20_2.jpg", "caption": "Figure 3: Results of diverse APO methods on MGAPO task. Each column displays images from the same APO method, and each row features images generated by the same SD model. APO methods with matching color and SD model share the same domain, indicating that the training data for the APO model was generated by that specific SD model.", "description": "This figure compares the results of different automatic prompt optimization (APO) methods on the model-generalized automatic prompt optimization (MGAPO) task. Each column represents a different APO method, and each row shows images generated using a specific Stable Diffusion (SD) model. The color-coding helps to visualize which APO method was trained on which SD model; methods with the same color and row were trained on the same SD model. This allows for a direct comparison of how well different APO methods generalize to unseen SD models.", "section": "4.4 Comparison to Other Methods"}, {"figure_path": "46V9axmOuU/figures/figures_20_3.jpg", "caption": "Figure 14: Demonstration of Ablation Study of Loss Functions.", "description": "This figure shows the results of an ablation study on the loss functions used in the AP-Adapter model.  Each row represents a different prompt and shows the images generated when different loss functions are removed.  The purpose is to demonstrate the individual contribution of each loss function to the overall image quality and to highlight the importance of using all loss functions for optimal performance.", "section": "More Ablation Study"}, {"figure_path": "46V9axmOuU/figures/figures_20_4.jpg", "caption": "Figure 1: (a) Comparison of text-to-image generation among different APO methods on various unseen fine-tuned diffusion models. (b) Comparison between model-specific and model-generalized adapters under the model-generalized setting.", "description": "Figure 1(a) shows the results of several Automatic Prompt Optimization (APO) methods applied to various unseen diffusion models, highlighting the differences in image quality and semantic consistency.  Figure 1(b) directly compares a model-specific approach to the proposed model-generalized approach for APO, demonstrating the improved generalization ability of the latter on unseen models.", "section": "1 Introduction"}]