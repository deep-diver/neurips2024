[{"type": "text", "text": "EFFI-LEARNER: Enhancing Efficiency of Generated Code via Self-Optimization ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Dong Huang\u2217 The University of Hong Kong dhuang@cs.hku.hk ", "page_idx": 0}, {"type": "text", "text": "Jianbo Dai\u2217 University of Edinburgh j6dj6d@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Han Weng Beijing University of Posts and Telecommunications han.weng@bupt.edu.cn ", "page_idx": 0}, {"type": "text", "text": "", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Puzhen Wu University College Dublin puzhen.wu@ucdconnect.ie ", "page_idx": 0}, {"type": "text", "text": "Yuhao Qing The University of Hong Kong yhqing@cs.hku.hk ", "page_idx": 0}, {"type": "text", "text": "Heming Cui The University of Hong Kong Shanghai AI Laboratory heming@cs.hku.hk ", "page_idx": 0}, {"type": "text", "text": "Zhijiang Guo\u2020 University of Cambridge zg283@cam.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Jie M. Zhang King\u2019s College London jie.zhang@kcl.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large language models (LLMs) have shown remarkable progress in code generation, but their generated code often suffers from inefficiency, resulting in longer execution times and higher memory consumption. To address this issue, we propose EFFI-LEARNER, a self-optimization framework that utilizes execution overhead profiles to improve the efficiency of LLM-generated code. EFFI-LEARNER first generates code using an LLM, then executes it locally to capture execution time and memory usage profiles. These profiles are fed back to the LLM, which then revises the code to reduce overhead. To evaluate the effectiveness of EFFILEARNER, we conduct extensive experiments on the EffiBench, HumanEval, and MBPP with 16 open-source and 6 closed-source models. Our evaluation results demonstrate that through iterative self-optimization, EFFI-LEARNER significantly enhances the efficiency of LLM-generated code. For example, the execution time (ET) of StarCoder2-15B for the EffiBench decreases from 0.93 (s) to 0.12 (s) which reduces $87.1\\%$ execution time requirement compared with the initial code. The total memory usage (TMU) of StarCoder2-15B also decreases from 22.02 $(\\mathbf{M}\\mathbf{b}^{*}\\mathbf{s})$ to 2.03 $(\\mathbf{M}\\mathbf{b}^{*}\\mathbf{s})$ , which decreases $90.8\\%$ total memory consumption during the execution process. The source code of EFFI-LEARNER was released in https://github.com/huangd1999/EffiLearner. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large language models (LLMs) have recently achieved significant advancements across various tasks [49, 4, 5, 42]. LLMs such as GPT-4 [50] and Copilot [44] have demonstrated considerable efficacy in code-related applications, including code completion [11, 6], debuggingg [25, 12], and translation [54, 1]. These innovative tools have been seamlessly integrated into popular development environments, significantly augmenting developer productivity by providing intelligent code ", "page_idx": 0}, {"type": "text", "text": "Problem: Given a non-empty array of integers nums, every element appears twice except for one. Find that single one. You must implement a solution with a linear runtime complexity and use only constant extra space. ", "page_idx": 1}, {"type": "text", "text": "Example: Input: nums $=[4,1,2,1,2]$ ", "page_idx": 1}, {"type": "text", "text": "Output: 4 ", "page_idx": 1}, {"type": "text", "text": "assert singleNumber([4, 1, 2, 1, 2]) == 4 ", "page_idx": 1}, {"type": "table", "img_path": "KhwOuB0fs9/tmp/24862b02231948d0298a2047c821fab1dd57c0d322acb3f1c683d3a6a2f872f0.jpg", "table_caption": [], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: A case for the task with code and EFFI-LEARNER refined version. The lower left panel shows the initial completion generated by an LLM, its proflie shows its inefficiency. The lower right panel shows the final efficient answer output by applying EFFI-LEARNER. ", "page_idx": 1}, {"type": "text", "text": "recommendations based on natural language instructions. However, the primary focus of existing efforts has predominantly been on the correctness of the generated code [18], ensuring it meets the functional requirements and adheres to syntactical norms. ", "page_idx": 1}, {"type": "text", "text": "Despite advancements in ensuring code correctness, there remains a significant gap in the literature regarding the efficiency of code produced by LLMs. Efficiency is crucial as it translates to faster execution and lower memory and processing power consumption, which is especially important in resource-constrained environments such as mobile devices or embedded systems [18, 53, 13, 48, 17, 56, 9, 41, 52]. Recent studies [56, 48] reveal that LLM-generated code often exhibits lower efficiency in terms of execution time and memory usage when compared to canonical solutions. For instance, on a benchmark that evaluates efficiency, EffiBench [27], even the most powerful LLM (e.g., GPT-4-Turbo) generates code with suboptimal efficiency. The average and worst-case execution times are 1.69 and 45.49 times longer than those of the canonical solutions, respectively. This inefficiency underscores the need for developing new methods focused on evaluating and improving the efficiency of code generated by LLMs, ensuring that they produce correct and highly efficient code. ", "page_idx": 1}, {"type": "text", "text": "To bridge this gap, we draw inspiration from the methodology used by coders on coding platforms. When addressing a programming problem, coders typically write an initial program that is executable on the test cases. Next, they execute the code and obtain a proflie of the execution time and memory usage overhead, as shown in Figure 1. Based on this overhead proflie, the coder optimizes the code to enhance its efficiency. During this process, the coder extracts key information (e.g., execution times and memory usage of each line) from the overhead profile, which helps identify lines or operators that require significant overhead (e.g., loops that execute multiple times or unnecessary variables being saved). This information assists the coder in optimizing their code. ", "page_idx": 1}, {"type": "text", "text": "With this motivation, we propose EFFI-LEARNER to improve the efficiency of LLM-generated code. As shown in Figure 2, EFFI-LEARNER first requires the LLM to generate code based on the task description. Then, EFFI-LEARNER executes the generated code locally and captures the execution time and memory usage profile. These overhead profiles are fed back into the LLM, which then revises the code to reduce the overhead. Through multi-iteration self-optimization, the efficiency of the LLM-generated code is improved. While it\u2019s true that the iterative process requires extra time, it\u2019s crucial to recognize the long-term advantages that come with this investment. By optimizing the code, we can enhance the overall efficiency once it\u2019s deployed. ", "page_idx": 1}, {"type": "text", "text": "To evaluate the effectiveness of EFFI-LEARNER, we conduct extensive experiments on the EffiBench and two commonly used code generation benchmarks (i.e. HumanEval [11] and MBPP [6]) with 16 open-source and 6 closed-source models. We compare the efficiency of the code generated by the LLM before and after applying EFFI-LEARNER. The experimental results demonstrate that EFFI-LEARNER significantly improves the efficiency of the LLM-generated code. For example, the execution time (ET) of StarCoder2-15B decreases from 0.93 (s) to 0.12 (s) which reduces $87.1\\%$ execution time requirement compared with the initial code. The max memory usage (MU) of DeepSeek-6.7B-Ins also decreased from 259.73 (Mb) to 36.97 (Mb), which reduces $85.8\\%$ max memory consumption for the code execution requirement. The total memory usage (TMU) of StarCoder2-15B also decreases from 22.02 $(\\mathbf{M}\\mathbf{b}^{*}\\mathbf{s})$ to $2.03\\;(\\mathrm{Mb^{*}s})$ , which decreases $90.8\\%$ total memory consumption during the execution process. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 LLMs for Code Generation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The increasing popularity of LLMs for code generation has coincided with the growing availability of open-source code repositories and the need to boost developer productivity. Initial efforts focused on training models specifically for coding tasks, such as CodeT5 [61], AlphaCode [33], CodeGen [47], InCoder [19], StarCoder [32], SantaCoder [3] and DeepSeek Coder [15]. Contrastingly, models such as Codex [11] and CodeLLaMA [55] represent a subsequent stride, being finetuned from foundation models [8, 59]. These code LLMs have been applied to various tasks, including code generation [11, 14], program repair [25, 28], automated testing [31, 16], code translation [54, 1], type prediction [45, 64], and code summarization [26, 2]. Among these, code generation, where models generate code snippets based on natural language descriptions or docstrings, has become a critical domain for evaluating LLMs. While LLMs have achieved impressive results in code generation tasks like HumanEval [11] and MBPP [6], their efficiency has received less attention. Recent studies [56, 27, 48] have shown that LLM-generated code exhibits lower efficiency in execution time and memory usage compared to canonical solutions. These findings highlight the need for further research and development to improve the efficiency of LLM-generated code. In this work, we propose the first method that significantly improves the efficiency of code generated by a wide range of LLMs. ", "page_idx": 2}, {"type": "text", "text": "2.2 Learning From Feedback ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "A prevalent strategy for improving the behavior of LLMs is learning from feedback, mirroring human learning where individuals refine their actions through trial, error, and correction [7, 43]. Early efforts involve using human feedback to evaluate and refine models [30, 51, 21]. To minimize human intervention, another strategy focuses on automated feedback. These methods iteratively learn from automatically generated feedback signals, understanding the consequences of their actions and adapting their behaviors. The source of this automated feedback can be diverse, ranging from the LLM itself [40, 57], external tools [22, 37] or verifiers [36], external knowledge sources [20, 68] and even generation logits [67]. In code generation, the program executor is frequently used as a source of feedback for refining the model\u2019s initial code. For example, Self-Edit [69] and Self-Evolve [29] execute the initial program on example test cases and provide the execution results as feedback, prompting the LLM to refine the code. Self-Debug [12] explores using program explanation, unit tests, and program interpreters as feedback types. ALGO [70] employs a more fine-grained approach by generating a reference oracle program that solves the problem with an exhaustive search. Feedback is then collected by comparing the generated outputs with the oracle. While existing work primarily focuses on using feedback to edit the initial code to ensure correctness, our method explores using overhead profiles to improve the efficiency of the code. ", "page_idx": 2}, {"type": "text", "text": "3 EFFI-LEARNER ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Inspired by the optimization strategies employed by human coders on coding platforms, we propose a framework EFFI-LEARNER to enhance the efficiency of LLM-generated code. Human coders typically analyze execution time and memory usage proflies to identify bottlenecks and optimize their code. EFFI-LEARNER leverages this principle by integrating a self-optimization loop into the code generation process. As illustrated in Figure 2, EFFI-LEARNER consists of three main components: Code Generation, Overhead Profiling, and Code Refinement, each playing a crucial role in the self-optimization process. ", "page_idx": 2}, {"type": "image", "img_path": "KhwOuB0fs9/tmp/4ee953013f6e4e8d2711720f29a2673759aed9dbf04efd4c34e062ad98507a35.jpg", "img_caption": ["Figure 2: Pipeline of EFFI-LEARNER. LLMs first generate code for the given problem. This code is then executed locally to gather overhead profiles. These profiles are subsequently utilized by the LLMs to optimize the code in successive iterations, thereby enhancing the overall efficiency of the generated code. A comprehensive illustration is provided in the Appendix Figure 4-Figure 11. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "3.1 Code Generation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Given a task description or code generation requirement, the LLM generates an initial version of the code. The LLM takes the task description as input and produces code that aims to solve the task. ", "page_idx": 3}, {"type": "text", "text": "3.2 Overhead Profiling ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The generated code is executed locally to capture its execution time and memory usage overhead profiles. During this step, the code is run on a set of open test cases, and the execution time and memory usage for each line of code are recorded. This information forms the overhead proflies that provide insights into the efficiency of the generated code. ", "page_idx": 3}, {"type": "text", "text": "Execution Time Profiling In this step, we measure the execution time of each line of code to identify potential bottlenecks and inefficiencies. To perform execution time profiling, we utilize the line_profiler library in Python. During the profliing process, we run the generated code on a set of open test cases provided by the dataset. The line_profiler library tracks the execution time of each line of code for all the test cases combined. This helps us assess the code\u2019s performance under different conditions and identify any performance bottlenecks. The execution time profiling results are reported based on the total consumption for all open test cases. The profiling output includes information such as the line number, the number of times each line is executed, and the total time spent on each line. These profiles serve as input for the subsequent code refinement step. ", "page_idx": 3}, {"type": "text", "text": "Memory Usage Profliing Memory usage profliing is another essential aspect of the EFFI-LEARNER framework. It helps us understand how the generated code utilizes memory resources and identifies any memory-related inefficiencies or leaks. To profile memory usage, we employ the memory_profiler library in Python. During the memory usage profiling process, we run the generated code on the set of open test cases. The memory_profiler library monitors the memory usage of each line of code throughout the execution of all the test cases combined. It captures the memory usage at different points, such as before and after function calls, loop iterations, and memory allocation statements. The memory usage profliing results are reported based on the total consumption for all open test cases. The profiling output includes information such as the line number and the corresponding memory usage. These proflies provide valuable insights into the memory efficiency of the generated code and help identify areas for optimization. ", "page_idx": 3}, {"type": "text", "text": "3.3 Code Refinement ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "This component leverages the execution time and memory usage profiles to optimize the generated code. In this step, the LLM analyzes the overhead proflies to refine the code for better efficiency. To enable self-optimization, we feed the overhead proflies back into the LLM along with the generated code. The LLM analyzes patterns in the overhead proflies, such as high execution time or excessive memory usage, and correlates them with specific code segments. It then applies optimization techniques, such as loop unrolling, memorization, data structure optimization, algorithm substitution, and code simplification, to improve the efficiency of the identified code segments. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "During the self-optimization process, the LLM considers factors such as the impact of each optimization on the overall efficiency, the trade-offs between execution time and memory usage, and the preservation of code correctness. It aims to strike a balance between performance improvement and maintaining the functional integrity of the code. The LLM iteratively refines the code based on the overhead profiles, applying optimizations until a satisfactory level of efficiency is achieved or a predefined number of iterations is reached. The optimized code is then validated against the open test cases to ensure its functional correctness. By leveraging the execution time and memory usage proflies, the self-optimization step enables the LLM to improve the efficiency of the generated code. ", "page_idx": 4}, {"type": "text", "text": "3.4 Prompt Construction ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We carefully design prompts to guide LLMs in optimizing code efficiency while ensuring the optimized code passes predefined test cases. The prompt template (Figure 3) used in EFFI-LEARNER\u2019s self-optimization stage includes a task description, test case, initial code, overhead analysis, and optimization rules. The task description provides context and requirements, the test case ensures correctness, and the initial code is the starting point for optimization. The overhead analysis highlights performance metrics and areas for improvement, while the optimization rules focus the LLM on enhancing efficiency, encapsulating the optimized code, and excluding the test case from the code block. This comprehensive prompt equips the LLM with the necessary information to effectively optimize code, maintain consistency across models and tasks, and facilitate comparison of their code optimization capabilities, advancing the field of LLM-driven code optimization. Details of the template can be found in Appendex A.3. ", "page_idx": 4}, {"type": "text", "text": "4 Evaluation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "4.1 Dataset and Metrics ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We evaluate EFFI-LEARNER on EffiBench [27]. Following their settings, we use Execution Time (ET), Normalized Execution Time (NET), Max Memory Usage (MU), Normalized Max Memory Usage (NMU), Total Memory Usage (TMU), and Normalized Total Memory Usage (NTMU) as metrics. We provide a detailed definition of these metrics in A.5. Following the setup of EffiBench, evaluation metrics were only calculated on the tasks that generated code for both the initial version and EFFI-LEARNER optimized code that can pass all private test cases provided by the dataset3. ", "page_idx": 4}, {"type": "text", "text": "Following Huang et al. [27], we utilize the open test cases to calculate the efficiency metrics during the self-optimization process, while private test cases provided by EffiBench were used for the final result evaluation. For HumanEval and MBPP datasets, we set the test cases provided by HumanEval and MBPP as open test cases, while test cases provided by EvalPlus [35] (i.e., HumanEval-Plus, MBPP-Plus) as private test cases that were used to calculate the final results. ", "page_idx": 4}, {"type": "text", "text": "4.2 Implementation Details ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "All of the experiments are conducted in an edge server with an Intel Xeon Platinum 8336C CPU with 128 cores, and $8*$ NVIDIA A100-SXM GPUs Total memory capacity of 2.0TiB. ", "page_idx": 4}, {"type": "text", "text": "Models We evaluate EFFI-LEARNER\u2019s effectiveness on both open-source and closed-source LLMs4. For open-source models, we evaluate EFFI-LEARNER with OpenCodeInterpreter (1.3B, 6.7B, and 33B) [72], DeepSeek-Instruct (1.3B, 6.7B, and 33B) [24], CodeLlama (7B, 13B, 34B, and 70B) [55], XwinCoder (7B and 34B) [58], StarCoder (3B, 7B, and 15B) [32], and WizardCoder-13B [38], where the detailed versions of LLMs are demonstrated in supplementary flie. For closed-source models, we use GPT-3.5-Turbo, GPT-4-Turbo, GPT-4 [50], Claude-3-haiku, and Claude-3-sonnet5. These LLMs have achieved competitive pass $@1$ scores in various code generation tasks [11, 6, 35]. ", "page_idx": 4}, {"type": "table", "img_path": "KhwOuB0fs9/tmp/4bd488ced00f714b319bc3dc0885856d052eff7c863f91cd0442f2ea1039e31a.jpg", "table_caption": ["Table 1: Code efficiency of LLMs with EFFI-LEARNER on EffiBench. The percentage in the brackets indicates the extent of the reduction for each respective item. Top performing LLMs are highlighted. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Setup We first collect the generated code from each LLM and evaluate its correctness using open test cases. Only the code that passes all test cases is considered for efficiency evaluation. This approach ensures consistency in the evaluated tasks across different self-optimization iterations, as EFFI-LEARNER focuses on improving the efficiency of initially correct code without altering its pass $@1$ score. By evaluating a diverse set of open-source and closed-source LLMs, we aim to provide a comprehensive assessment of the efficiency of LLM-generated code and the effectiveness of EFFI-LEARNER in improving code efficiency across different models and architectures. ", "page_idx": 5}, {"type": "text", "text": "4.3 Main Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Open-source LLMs As shown in Table 1, we observe that the efficiency metrics for all models have been increased in most experiments once we apply EFFI-LEARNER to optimize the efficiency of LLM-generated code. For example, in OpenCodeInterpreter-1.3B, the execution time for its generated code decreases from 1.60 (s) to 1.29 (s), a reduction of $19.4\\%$ in execution time. Additionally, the ", "page_idx": 5}, {"type": "table", "img_path": "KhwOuB0fs9/tmp/4eddcb3c3c4b23b6a440ef59dd4ee5d95926d936dc447a0d85b2810763c6c79c.jpg", "table_caption": ["Table 2: Effect of the number of self-optimization steps in EFFI-LEARNER. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "TMU of OpenCodeInterpreter-1.3B decreases from 89.16 $(\\mathbf{M}\\mathbf{b}^{*}\\mathbf{s})$ to 70.63 $(\\mathbf{M}\\mathbf{b}^{*}\\mathbf{s})$ . Furthermore, in certain edge cases, EFFI-LEARNER significantly enhances efficiency. For example, the ET of StarCoder2-15B decreases from 0.93 (s) to 0.12 (s) and the NET also decreases from 7.48 to 1.03, reducing execution time requirements by $87.1\\%$ compared to the initial code. The MU and NMU of DeepSeek-6.7B-Ins also decrease from 259.73 (Mb) and 7.25 to 36.97 (Mb) and 1.06, reducing the maximum memory consumption by $85.8\\%$ for the code execution requirement. Moreover, we can also observe that the TMU and NTMU of StarCoder2-15B also decrease from 22.02 $(\\mathbf{M}\\mathbf{b}^{*}\\mathbf{s})$ and 10.88 to 2.03 $(\\mathbf{M}\\mathbf{b}^{*}\\mathbf{s})$ and 1.06, which decreases $90.8\\%$ memory consumption during the execution process. These results demonstrate the effectiveness of EFFI-LEARNER in optimizing the code generated by open-source LLMs. ", "page_idx": 6}, {"type": "text", "text": "Closed-source LLMs Similar to open-source LLMs, we observe that the efficiency metrics for most closed-source LLMs have been improved after applying EFFI-LEARNER to optimize the efficiency of the generated code. For instance, the execution time for code generated by GPT-3.5-Turbo-0301 decreases from 0.36 (s) to 0.28 (s), reducing the execution time by $22.2\\%$ . The MU and NMU of GPT-3.5-Turbo-0301 also decrease from 91.25 (Mb) and 2.45 to 36.08 (Mb) and 0.99, respectively, which reduces the max memory consumption for code execution by $60.5\\%$ . Furthermore, the TMU and NTMU of GPT-3.5-Turbo-0301 decrease from 157.50 $(\\mathbf{M}\\mathbf{b}^{*}\\mathbf{s})$ and 19.75 to 12.43 $(\\mathbf{M}\\mathbf{b}^{*}\\mathbf{s})$ and 1.64, respectively, decreasing memory consumption during the execution process by $92.1\\%$ . ", "page_idx": 6}, {"type": "text", "text": "The improvements in efficiency metrics across both open-source and closed-source LLMs highlight the generalizability and adaptability of EFFI-LEARNER. By iteratively refining the generated code based on efficiency proflier feedback, EFFI-LEARNER enables LLMs to produce more efficient code without compromising the correctness of the generated solutions. The consistent improvements across various models and architectures demonstrate the potential of EFFI-LEARNER as a model-agnostic approach for optimizing the efficiency of LLM-generated code in real-world applications. ", "page_idx": 6}, {"type": "text", "text": "4.4 Impact of Self-Optimization Steps ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To investigate the impact of the number of self-optimization steps on the efficiency of the EFFILEARNER-optimized code, we conduct an ablation study by varying the number of steps from 0 to 5. Table 2 for CodeLlama-70B and GPT-3.5-Turbo-0301 at different self-optimization steps. ", "page_idx": 6}, {"type": "text", "text": "CodeLlama-70B We can observe that after the first self-optimization step, the MU decreases from 109.61 (Mb) to 26.47 (Mb), reducing $75.9\\%$ maximum memory requirement compared with the initial code generated by CodeLlama-70B. Similarly, the TMU decreases from 54.15 $(\\mathbf{M}\\mathbf{b}^{*}\\mathbf{s})$ to 6.69 $(\\mathbf{M}\\mathbf{b}^{*}\\mathbf{s})$ , reducing $87.6\\%$ of memory consumption during code execution. As the number of steps increases, the efficiency metrics gradually improve. By the fifth step, the ET reaches 0.47 (s), reducing the $1.9\\%$ execution time requirement compared with the first-step generated code, and the TMU settles at 14.53, reducing $0.2\\%$ total memory usage from the first step. ", "page_idx": 6}, {"type": "text", "text": "GPT-3.5-Turbo-0301 Similar to CodeLlama-70B, the MU decreases from 91.25 (Mb) to 36.09 (Mb) after the first self-optimization step, reducing $60.4\\%$ maximum memory requirement compared with the initial code. The TMU also shows a substantial reduction from 157.50 $(\\mathbf{M}\\mathbf{b}^{*}\\mathbf{s})$ to 13.70 $(\\mathbf{M}\\mathbf{b}^{*}\\mathbf{s})$ , ", "page_idx": 6}, {"type": "text", "text": "Table 3: Contribution of different components in EFFI-LEARNER. We evaluate how different feedback profliers affect the efficiency of LLM-generated code. Unsupervised self-refine only requires LLMs to optimize the efficiency of the code. Result-Aware Self-Refine feedback the ET, MU, and TMU to the LLMs and require it to improve the efficiency. Memory Profiler and Execution Time Profiler feedback the memory proflier and execution time proflier to the LLMs and then LLMs can based on the profile optimize the efficiency of the code. ", "page_idx": 7}, {"type": "table", "img_path": "KhwOuB0fs9/tmp/3811c045ddfc2fb8f8eab7b44fd85465529fb3b1e1019c5055abaea2b2ad7db6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "reducing $91.3\\%$ of memory consumption during code execution. As the number of steps increases, the efficiency metrics continue to improve steadily. By the fifth step, the ET reaches 0.28 (s), reducing the $15.2\\%$ execution time requirement compared with the first-step generated code, and the TMU settles at 12.43 $\\mathrm{(Mb^{*}s)}$ , reducing $9.3\\%$ total memory usage from the first step. ", "page_idx": 7}, {"type": "text", "text": "Table 2 demonstrates the significant impact of the number of self-optimization steps on the efficiency of the EFFI-LEARNER-optimized code. For both CodeLlama-70B and GPT-3.5-Turbo-0301, the first self-optimization step yields the most substantial improvements in code efficiency. As the number of steps increases, the efficiency metrics continue to improve, albeit with diminishing returns. By the fifth step, the efficiency metrics reach their lowest values, demonstrating the effectiveness of EFFI-LEARNER\u2019s iterative self-optimization approach in enhancing the efficiency of LLM-generated code. The evaluation results highlight that the majority of efficiency improvements occur in the first few steps, with subsequent steps contributing to further refinements of the optimized code. ", "page_idx": 7}, {"type": "text", "text": "4.5 Feedback of Overhead Profile ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To show the effectiveness of the overhead profile in guiding LLMs to refine their generated code, we compare the performance of EFFI-LEARNER with two alternative approaches: Unsupervised Self-Refine and Result-Aware Self-Refine [40, 57]. Unsupervised Self-Refine uses a prompt that directly requires the LLM to refine the code without providing additional information. Result-Aware Self-Refine feeds the ET, MU, and TMU, then requires the LLM to refine the code based on these metrics. Table 3 presents the code efficiency metrics for CodeLlama-70B and GPT-3.5-Turbo-0301 using different code refinement approaches. ", "page_idx": 7}, {"type": "text", "text": "CodeLlama-70B Unsupervised Self-Refine and Result-Aware Self-Refine result in significant increases in ET, memory usage (MU), and TMU compared to the initial version. Unsupervised Self-Refine increases ET by $51.9\\%$ , MU by $154.9\\%$ , and TMU by $518.8\\%$ , while Result-Aware SelfRefine increases ET by $51.9\\%$ , MU by $157.8\\%$ , and TMU by $523.2\\%$ . In contrast, EFFI-LEARNER incorporates the overhead profile feedback and achieves a $9.6\\%$ reduction in ET, a $75.9\\%$ reduction in MU, and a $92.9\\%$ reduction in TMU compared to the initial version. ", "page_idx": 7}, {"type": "text", "text": "GPT-3.5-Turbo-0301 Unsupervised Self-Refine and Result-Aware Self-Refine show some improvements in ET and MU compared to the initial version. Unsupervised Self-Refine reduces ET by $11.1\\%$ and MU by $14.1\\%$ , while Result-Aware Self-Refine reduces ET by $16.7\\%$ and MU by $35.7\\%$ . However, both approaches lead to substantial increases in TMU, with Unsupervised Self-Refine increasing TMU by $98.7\\%$ and Result-Aware Self-Refine increasing TMU by $24.1\\%$ . On the other hand, EFFI-LEARNER achieves a $22.2\\%$ reduction in ET, a $60.5\\%$ reduction in MU, and a $92.1\\%$ reduction in TMU compared to the initial version. ", "page_idx": 7}, {"type": "text", "text": "These results highlight the importance of the overhead proflie feedback in guiding LLMs to generate more efficient code. Without the overhead profile, the code refinement process using alternative ", "page_idx": 7}, {"type": "text", "text": "Table 4: Results of EFFI-LEARNER on HumanEval dataset, where we evaluate CodeLlama family generated code\u2019s efficiency. Full results are listed in Appendix Table 8. ", "page_idx": 8}, {"type": "table", "img_path": "KhwOuB0fs9/tmp/379ad6dc66434f6a49c1c9fb1e2f0ef9ec3039d94de57b5716edfd2337bc3d10.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "prompts fails to improve code efficiency and even leads to significant performance degradation. The overhead profile provides valuable insights into the resource consumption of the generated code, enabling LLMs to make targeted optimizations and achieve substantial efficiency improvements. ", "page_idx": 8}, {"type": "text", "text": "4.6 Discussion ", "text_level": 1, "page_idx": 8}, {"type": "table", "img_path": "KhwOuB0fs9/tmp/43ac779932f6d086a289fb3e38171bf30417fbbbe51899e64e28770ad4b5af32.jpg", "table_caption": ["Table 5: Code efficiency of widely-studied LLMs reported by EFFI-LEARNER. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Generalizability across benchmarks In Table 1, we evaluated EFFI-LEARNER\u2019s effectiveness on the EffiBench dataset. To illustrate EFFI-LEARNER\u2019s generalizability in other datasets, we conduct experiments on the HumanEval and MBPP datasets in Appendix Table 8 and Table 9. We also provide EFFI-LEARNER\u2019s effectiveness on the HumanEval dataset in CodeLlama models in Table 4. We can observe that the coding efficiency of CodeLlama and other LLMs (See Table 8) also increases when we utilize EFFI-LEARNER to optimize LLM-generated code. For example, the ET of CodeLlama-70B decreases from 0.21 (s) to 0.18 (s), which reduces $14.3\\%$ execution time. As shown in Table 8 and Table 9, results demonstrate that EFFI-LEARNER can consistently improve the efficiency of LLM-generated code for other datasets. ", "page_idx": 8}, {"type": "table", "img_path": "KhwOuB0fs9/tmp/697f6040f29cdbf78b24de1b010d6bf55abe42a182a58b418e1ae30953fbef0a.jpg", "table_caption": ["Table 6: Pass $@1$ of LLMs generated initial code and EFFI-LEARNER optimized code. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Generalizability across LLMs In Table 1, we evaluate EFFI-LEARNER\u2019s effectiveness on six types of open-source LLMs. To illustrate EFFI-LEARNER\u2019s generalizability in other LLMs, we also conduct experiments on other LLMs in Table 5. Our evaluation results demonstrate that EFFI-LEARNER can improve the efficiency of LLM-generated code for different LLMs. For example, the execution time of Mistral-7B-codealpaca-lora decreases from 2.36 (s) to 1.45 (s), which reduces $38.6\\%$ execution time compared with the initial code. The total memory usage of Phind-CodeLlama-34B-v2 also decreases from 337.30 $(\\mathrm{Mb^{*}s})$ ) to 65.64 $(\\mathrm{Mb^{*}s})$ ), which reduces $80.5\\%$ total memory requirement. ", "page_idx": 9}, {"type": "text", "text": "Impact on correctness We provide the pass $@1$ of LLM-generated initial code and EFFI-LEARNER optimized code for EffiBench in Table 6. We observe that the pass $@1$ of EFFI-LEARNER optimized code may be lower than LLM-generated initial code. The key reason is that during the self-optimization process, EFFI-LEARNER only uses public test cases to guide code efficiency optimization for correct initial code. However, since public test cases may not cover all edge cases in the private test cases (test cases used to evaluate pass $@1$ of LLMs), this can cause the pass $@1$ of EFFI-LEARNER generated code to be lower than the initial code. Nevertheless, we observe that the pass $@1$ of EFFI-LEARNER only decreases by about $0\\%$ to $0.5\\%$ , which means that only a few of the codes will be incorrect. As shown in Table 1, we can observe that the code efficiency is largely increased. We believe that this minor decrease in pass $@1$ is worthwhile considering the significant efficiency gains. ", "page_idx": 9}, {"type": "text", "text": "Case study To illustrate how EFFI-LEARNER improves the efficiency of LLM-generated code, we provide a case illustration in Appendix Figure 4-Figure 11. As shown in Figure 6, we can observe that the execution time of the initial code is 23.59 (s) while in the self-optimized code, the execution time decreases from 23.59 (s) to 3.36 (s). The key reason is that in the initial code, the algorithm uses a standard unidirectional Breadth-First Search (BFS), which explores all possible states level by level starting from the initial state. This method results in a large number of states to explore, leading to significant computational overhead. In contrast, the self-optimized code employs a bidirectional BFS, which simultaneously searches from both the initial state and the target state. This reduces the search space by meeting in the middle, significantly decreasing the number of states that need to be explored and thereby improving the execution time. ", "page_idx": 9}, {"type": "text", "text": "Error Analysis We also provide a case illustration to explain why some code efficiency does not improve significantly when EFFI-LEARNER is applied to LLM-generated code. As shown in Appendix Figure 12-Figure 18, we observe that the initial code only requires 0.0012 (s) to execute, while in the optimized code, the execution time is still 0.0011 (s). The key reason for this minimal improvement is that both implementations already operate with the same theoretical time complexity of $O(\\log(\\operatorname*{min}(m,n)))$ . Given the problem\u2019s constraints and small input sizes, the actual runtime differences are overshadowed by the inherent efficiency of the binary search algorithm. Additionally, the overhead of function calls and Python runtime operations can further minimize the observed performance gains. Therefore, while the optimized code may offer clearer partition management and slight improvements, the overall efficiency remains largely unchanged due to the already optimized nature of the initial approach. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper focuses on the critical issue of efficiency in code generated by LLMs. While LLMs have shown impressive capabilities in code generation, their output often suffers from suboptimal efficiency, leading to slower execution and higher resource consumption. To tackle this challenge, we propose EFFI-LEARNER, a novel self-optimization framework that leverages execution overhead profiles to guide LLMs in improving code efficiency. Extensive experiments and analysis demonstrate that EFFI-LEARNER significantly enhances the efficiency of LLM-generated code, achieving substantial reductions in execution time and memory usage. For future work, we would like to investigate the application of EFFI-LEARNER to other programming tasks and languages, as well as explore the potential benefits of incorporating domain-specific knowledge into the optimization process. ", "page_idx": 9}, {"type": "text", "text": "6 ACKNOWLEDGMENT ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The work is supported in part by National Key R&D Program of China (2022ZD0160201), HK RGC RIF (R7030-22), HK ITF (GHP/169/20SZ), a Huawei Flagship Research Grant in 2023, HK RGC GRF (Ref: 17208223 & 17204424), and the HKU-CAS Joint Laboratory for Intelligent System Software. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Ahmad, W. U., Tushar, M. G. R., Chakraborty, S., and Chang, K. AVATAR: A parallel corpus for java-python program translation. In Rogers, A., Boyd-Graber, J. L., and Okazaki, N. (eds.), Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9- 14, 2023, pp. 2268\u20132281. Association for Computational Linguistics, 2023. doi: 10.18653/V1/ 2023.FINDINGS-ACL.143. URL https://doi.org/10.18653/v1/2023.findings-acl. 143. ", "page_idx": 10}, {"type": "text", "text": "[2] Ahmed, T. and Devanbu, P. T. Few-shot training llms for project-specific code-summarization. In 37th IEEE/ACM International Conference on Automated Software Engineering, ASE 2022, Rochester, MI, USA, October 10-14, 2022, pp. 177:1\u2013177:5. ACM, 2022. doi: 10.1145/3551349. 3559555. URL https://doi.org/10.1145/3551349.3559555. ", "page_idx": 10}, {"type": "text", "text": "[3] Allal, L. B., Li, R., Kocetkov, D., Mou, C., Akiki, C., Ferrandis, C. M., Muennighoff, N., Mishra, M., Gu, A., Dey, M., Umapathi, L. K., Anderson, C. J., Zi, Y., Lamy-Poirier, J., Schoelkopf, H., Troshin, S., Abulkhanov, D., Romero, M., Lappert, M., Toni, F. D., del R\u00edo, B. G., Liu, Q., Bose, S., Bhattacharyya, U., Zhuo, T. Y., Yu, I., Villegas, P., Zocca, M., Mangrulkar, S., Lansky, D., Nguyen, H., Contractor, D., Villa, L., Li, J., Bahdanau, D., Jernite, Y., Hughes, S., Fried, D., Guha, A., de Vries, H., and von Werra, L. Santacoder: don\u2019t reach for the stars! CoRR, abs/2301.03988, 2023. doi: 10.48550/ARXIV.2301.03988. URL https://doi.org/10.48550/arXiv.2301.03988. ", "page_idx": 10}, {"type": "text", "text": "[4] Anil, R., Borgeaud, S., Wu, Y., Alayrac, J., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., Millican, K., Silver, D., Petrov, S., Johnson, M., Antonoglou, I., Schrittwieser, J., Glaese, A., Chen, J., Pitler, E., Lillicrap, T. P., Lazaridou, A., Firat, O., Molloy, J., Isard, M., Barham, P. R., Hennigan, T., Lee, B., Viola, F., Reynolds, M., Xu, Y., Doherty, R., Collins, E., Meyer, C., Rutherford, E., Moreira, E., Ayoub, K., Goel, M., Tucker, G., Piqueras, E., Krikun, M., Barr, I., Savinov, N., Danihelka, I., Roelofs, B., White, A., Andreassen, A., von Glehn, T., Yagati, L., Kazemi, M., Gonzalez, L., Khalman, M., Sygnowski, J., and et al. Gemini: A family of highly capable multimodal models. CoRR, abs/2312.11805, 2023. doi: 10.48550/ARXIV.2312.11805. URL https://doi.org/10.48550/arXiv.2312.11805. ", "page_idx": 10}, {"type": "text", "text": "[5] Anthropic. Introducing the next generation of claude, 2024. URL https://www.anthropic. com/news/claude-3-family. ", "page_idx": 10}, {"type": "text", "text": "[6] Austin, J., Odena, A., Nye, M. I., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C. J., Terry, M., Le, Q. V., and Sutton, C. Program synthesis with large language models. CoRR, abs/2108.07732, 2021. URL https://arxiv.org/abs/2108.07732. ", "page_idx": 10}, {"type": "text", "text": "[7] Boyd, E. M. and Fales, A. W. Reflective learning: Key to learning from experience. Journal of humanistic psychology, 23(2):99\u2013117, 1983. ", "page_idx": 10}, {"type": "text", "text": "[8] Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html. ", "page_idx": 10}, {"type": "text", "text": "[9] Capra, E., Francalanci, C., and Slaughter, S. A. Is software \u201cgreen\u201d? application development environments and energy efficiency in open source applications. Information and Software Technology, 54(1):60\u201371, 2012.   \n[10] Chaudhary, S. Code alpaca: An instruction-following llama model for code generation. https: //github.com/sahil280114/codealpaca, 2023.   \n[11] Chen, M., Tworek, J., Jun, H., Yuan, Q., de Oliveira Pinto, H. P., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavarian, M., Winter, C., Tillet, P., Such, F. P., Cummings, D., Plappert, M., Chantzis, F., Barnes, E., HerbertVoss, A., Guss, W. H., Nichol, A., Paino, A., Tezak, N., Tang, J., Babuschkin, I., Balaji, S., Jain, S., Saunders, W., Hesse, C., Carr, A. N., Leike, J., Achiam, J., Misra, V., Morikawa, E., Radford, A., Knight, M., Brundage, M., Murati, M., Mayer, K., Welinder, P., McGrew, B., Amodei, D., McCandlish, S., Sutskever, I., and Zaremba, W. Evaluating large language models trained on code. CoRR, abs/2107.03374, 2021. URL https://arxiv.org/abs/2107.03374.   \n[12] Chen, X., Lin, M., Sch\u00e4rli, N., and Zhou, D. Teaching large language models to self-debug. CoRR, abs/2304.05128, 2023. doi: 10.48550/ARXIV.2304.05128. URL https://doi.org/ 10.48550/arXiv.2304.05128.   \n[13] Coignion, T., Quinton, C., and Rouvoy, R. \u00e5\u00e5. In Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering, pp. 79\u201389, 2024.   \n[14] Dai, J., Lu, J., Feng, Y., Ruan, R., Cheng, M., Tan, H., and Guo, Z. MHPP: exploring the capabilities and limitations of language models beyond basic code generation. CoRR, abs/2405.11430, 2024. doi: 10.48550/ARXIV.2405.11430. URL https://doi.org/10. 48550/arXiv.2405.11430.   \n[15] DeepSeekAI. Deepseek coder: Let the code write itself, 2023. URL https://deepseekcoder. github.io/.   \n[16] Deng, Y., Xia, C. S., Yang, C., Zhang, S. D., Yang, S., and Zhang, L. Large language models are edge-case fuzzers: Testing deep learning libraries via fuzzgpt. CoRR, abs/2304.02014, 2023. doi: 10.48550/ARXIV.2304.02014. URL https://doi.org/10.48550/arXiv.2304.02014.   \n[17] Du, M., Luu, A. T., Ji, B., and $\\mathrm{Ng}$ , S.-K. Mercury: An efficiency benchmark for llm code synthesis. arXiv preprint arXiv:2402.07844, 2024.   \n[18] Fan, A., Gokkaya, B., Harman, M., Lyubarskiy, M., Sengupta, S., Yoo, S., and Zhang, J. M. Large language models for software engineering: Survey and open problems. In 2023 IEEE/ACM International Conference on Software Engineering: Future of Software Engineering (ICSE-FoSE), pp. 31\u201353. IEEE, 2023.   \n[19] Fried, D., Aghajanyan, A., Lin, J., Wang, S., Wallace, E., Shi, F., Zhong, R., Yih, S., Zettlemoyer, L., and Lewis, M. Incoder: A generative model for code infliling and synthesis. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/pdf?id=hQwb-lbM6EL.   \n[20] Gao, L., Dai, Z., Pasupat, P., Chen, A., Chaganty, A. T., Fan, Y., Zhao, V. Y., Lao, N., Lee, H., Juan, D., and Guu, K. RARR: researching and revising what language models say, using language models. In Rogers, A., Boyd-Graber, J. L., and Okazaki, N. (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pp. 16477\u201316508. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.ACL-LONG.910. URL https://doi.org/10.18653/v1/2023.acl-long.910.   \n[21] Glaese, A., McAleese, N., Trebacz, M., Aslanides, J., Firoiu, V., Ewalds, T., Rauh, M., Weidinger, L., Chadwick, M. J., Thacker, P., Campbell-Gillingham, L., Uesato, J., Huang, P., Comanescu, R., Yang, F., See, A., Dathathri, S., Greig, R., Chen, C., Fritz, D., Elias, J. S., Green, R., Mokr\u00e1, S., Fernando, N., Wu, B., Foley, R., Young, S., Gabriel, I., Isaac, W., Mellor, J., Hassabis, D., Kavukcuoglu, K., Hendricks, L. A., and Irving, G. Improving alignment of dialogue agents via targeted human judgements. CoRR, abs/2209.14375, 2022. doi: 10.48550/ARXIV.2209.14375. URL https://doi.org/10.48550/arXiv.2209.14375.   \n[22] Gou, Z., Shao, Z., Gong, Y., Shen, Y., Yang, Y., Duan, N., and Chen, W. CRITIC: large language models can self-correct with tool-interactive critiquing. CoRR, abs/2305.11738, 2023. doi: 10.48550/ARXIV.2305.11738. URL https://doi.org/10.48550/arXiv.2305.11738.   \n[23] Gunasekar, S., Zhang, Y., Aneja, J., Mendes, C. C. T., Giorno, A. D., Gopi, S., Javaheripi, M., Kauffmann, P., de Rosa, G., Saarikivi, O., Salim, A., Shah, S., Behl, H. S., Wang, X., Bubeck, S., Eldan, R., Kalai, A. T., Lee, Y. T., and Li, Y. Textbooks are all you need. CoRR, abs/2306.11644, 2023. doi: 10.48550/ARXIV.2306.11644. URL https://doi.org/10.48550/arXiv.2306. 11644.   \n[24] Guo, D., Zhu, Q., Yang, D., Xie, Z., Dong, K., Zhang, W., Chen, G., Bi, X., Wu, Y., Li, Y., et al. Deepseek-coder: When the large language model meets programming\u2013the rise of code intelligence. arXiv preprint arXiv:2401.14196, 2024.   \n[25] Haque, M. M. A., Ahmad, W. U., Lourentzou, I., and Brown, C. Fixeval: Execution-based evaluation of program fixes for competitive programming problems. CoRR, abs/2206.07796, 2022. doi: 10.48550/ARXIV.2206.07796. URL https://doi.org/10.48550/arXiv.2206. 07796.   \n[26] Hasan, M., Muttaqueen, T., Ishtiaq, A. A., Mehrab, K. S., Haque, M. M. A., Hasan, T., Ahmad, W. U., Iqbal, A., and Shahriyar, R. Codesc: A large code-description parallel dataset. In Zong, C., Xia, F., Li, W., and Navigli, R. (eds.), Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021, volume ACL/IJCNLP 2021 of Findings of ACL, pp. 210\u2013218. Association for Computational Linguistics, 2021. doi: 10.18653/V1/2021. FINDINGS-ACL.18. URL https://doi.org/10.18653/v1/2021.findings-acl.18.   \n[27] Huang, D., Zhang, J. M., Qing, Y., and Cui, H. Effibench: Benchmarking the efficiency of automatically generated code. arXiv preprint arXiv:2402.02037, 2024.   \n[28] Jiang, N., Liu, K., Lutellier, T., and Tan, L. Impact of code language models on automated program repair. In 45th IEEE/ACM International Conference on Software Engineering, ICSE 2023, Melbourne, Australia, May 14-20, 2023, pp. 1430\u20131442. IEEE, 2023. doi: 10.1109/ ICSE48619.2023.00125. URL https://doi.org/10.1109/ICSE48619.2023.00125.   \n[29] Jiang, S., Wang, Y., and Wang, Y. Selfevolve: A code evolution framework via large language models. CoRR, abs/2306.02907, 2023. doi: 10.48550/ARXIV.2306.02907. URL https: //doi.org/10.48550/arXiv.2306.02907.   \n[30] Kreutzer, J., Khadivi, S., Matusov, E., and Riezler, S. Can neural machine translation be improved with user feedback? In Bangalore, S., Chu-Carroll, J., and Li, Y. (eds.), Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 3 (Industry Papers), pp. 92\u2013105. Association for Computational Linguistics, 2018. doi: 10.18653/V1/N18-3012. URL https://doi.org/10.18653/v1/ n18-3012.   \n[31] Lemieux, C., Inala, J. P., Lahiri, S. K., and Sen, S. Codamosa: Escaping coverage plateaus in test generation with pre-trained large language models. In 45th IEEE/ACM International Conference on Software Engineering, ICSE 2023, Melbourne, Australia, May 14-20, 2023, pp. 919\u2013931. IEEE, 2023. doi: 10.1109/ICSE48619.2023.00085. URL https://doi.org/10. 1109/ICSE48619.2023.00085.   \n[32] Li, R., Allal, L. B., Zi, Y., Muennighoff, N., Kocetkov, D., Mou, C., Marone, M., Akiki, C., Li, J., Chim, J., Liu, Q., Zheltonozhskii, E., Zhuo, T. Y., Wang, T., Dehaene, O., Davaadorj, M., Lamy-Poirier, J., Monteiro, J., Shliazhko, O., Gontier, N., Meade, N., Zebaze, A., Yee, M., Umapathi, L. K., Zhu, J., Lipkin, B., Oblokulov, M., Wang, Z., V, R. M., Stillerman, J., Patel, S. S., Abulkhanov, D., Zocca, M., Dey, M., Zhang, Z., Moustafa-Fahmy, N., Bhattacharyya, U., Yu, W., Singh, S., Luccioni, S., Villegas, P., Kunakov, M., Zhdanov, F., Romero, M., Lee, T., Timor, N., Ding, J., Schlesinger, C., Schoelkopf, H., Ebert, J., Dao, T., Mishra, M., Gu, A., Robinson, J., Anderson, C. J., Dolan-Gavitt, B., Contractor, D., Reddy, S., Fried, D., Bahdanau, D., Jernite, Y., Ferrandis, C. M., Hughes, S., Wolf, T., Guha, A., von Werra, L., and de Vries, H. Starcoder: may the source be with you! CoRR, abs/2305.06161, 2023. doi: 10.48550/ARXIV.2305.06161. URL https://doi.org/10.48550/arXiv.2305.06161.   \n[33] Li, Y., Choi, D. H., Chung, J., Kushman, N., Schrittwieser, J., Leblond, R., Eccles, T., Keeling, J., Gimeno, F., Lago, A. D., Hubert, T., Choy, P., de Masson d\u2019Autume, C., Babuschkin, I., Chen, X., Huang, P., Welbl, J., Gowal, S., Cherepanov, A., Molloy, J., Mankowitz, D. J., Robson, E. S., Kohli, P., de Freitas, N., Kavukcuoglu, K., and Vinyals, O. Competition-level code generation with alphacode. CoRR, abs/2203.07814, 2022. doi: 10.48550/ARXIV.2203.07814. URL https://doi.org/10.48550/arXiv.2203.07814.   \n[34] Li, Y., Bubeck, S., Eldan, R., Giorno, A. D., Gunasekar, S., and Lee, Y. T. Textbooks are all you need II: phi-1.5 technical report. CoRR, abs/2309.05463, 2023. doi: 10.48550/ARXIV.2309. 05463. URL https://doi.org/10.48550/arXiv.2309.05463.   \n[35] Liu, J., Xia, C. S., Wang, Y., and Zhang, L. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. Advances in Neural Information Processing Systems, 36, 2024.   \n[36] Lu, J., Dou, Z., Wang, H., Cao, Z., Dai, J., Wan, Y., Huang, Y., and Guo, Z. Autocv: Empowering reasoning with automated process labeling via confidence variation. CoRR, abs/2405.16802, 2024. doi: 10.48550/ARXIV.2405.16802. URL https://doi.org/10.48550/arXiv.2405. 16802.   \n[37] Lu, J., Liu, Z., Wan, Y., Huang, Y., Wang, H., Yang, Z., Tang, J., and Guo, Z. Process-driven autoformalization in lean 4. CoRR, abs/2406.01940, 2024. doi: 10.48550/ARXIV.2406.01940. URL https://doi.org/10.48550/arXiv.2406.01940.   \n[38] Luo, Z., Xu, C., Zhao, P., Sun, Q., Geng, X., Hu, W., Tao, C., Ma, J., Lin, Q., and Jiang, D. Wizardcoder: Empowering code large language models with evol-instruct. arXiv preprint arXiv:2306.08568, 2023.   \n[39] Luo, Z., Xu, C., Zhao, P., Sun, Q., Geng, X., Hu, W., Tao, C., Ma, J., Lin, Q., and Jiang, D. Wizardcoder: Empowering code large language models with evol-instruct. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=UnUwSIgK5W.   \n[40] Madaan, A., Tandon, N., Gupta, P., Hallinan, S., Gao, L., Wiegreffe, S., Alon, U., Dziri, N., Prabhumoye, S., Yang, Y., Gupta, S., Majumder, B. P., Hermann, K., Welleck, S., Yazdanbakhsh, A., and Clark, P. Self-refine: Iterative refinement with self-feedback. In Oh, A., Naumann, T., Globerson, A., Saenko, K., Hardt, M., and Levine, S. (eds.), Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/ 91edff07232fb1b55a505a9e9f6c0ff3-Abstract-Conference.html.   \n[41] Mancebo, J., Garcia, F., and Calero, C. A process for analysing the energy efficiency of software. Information and Software Technology, 134:106560, 2021.   \n[42] Meta. Introducing meta llama 3: The most capable openly available llm to date, 2024. URL https://ai.meta.com/blog/meta-llama-3/.   \n[43] Metcalfe, J. Learning from errors. Annual review of psychology, 68:465\u2013489, 2017.   \n[44] Microsoft. The world\u2019s most widely adopted ai developer tool., 2024. URL https://github. com/features/copilot.   \n[45] Mir, A. M., Latoskinas, E., Proksch, S., and Gousios, G. Type4py: Practical deep similarity learning-based type inference for python. In 44th IEEE/ACM 44th International Conference on Software Engineering, ICSE 2022, Pittsburgh, PA, USA, May 25-27, 2022, pp. 2241\u20132252. ACM, 2022. doi: 10.1145/3510003.3510124. URL https://doi.org/10.1145/3510003. 3510124.   \n[46] Muennighoff, N., Liu, Q., Zebaze, A. R., Zheng, Q., Hui, B., Zhuo, T. Y., Singh, S., Tang, X., von Werra, L., and Longpre, S. Octopack: Instruction tuning code large language models. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum? id=mw1PWNSWZP.   \n[47] Nijkamp, E., Pang, B., Hayashi, H., Tu, L., Wang, H., Zhou, Y., Savarese, S., and Xiong, C. Codegen: An open large language model for code with multi-turn program synthesis. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/pdf?id= iaYcJKpY2B_.   \n[48] Niu, C., Zhang, T., Li, C., Luo, B., and Ng, V. On evaluating the efficiency of source code generated by llms. arXiv preprint arXiv:2404.06041, 2024.   \n[49] OpenAI. GPT-3.5 Turbo, 2023. URL https://platform.openai.com/docs/models/ gpt-3-5.   \n[50] OpenAI. GPT-4 Technical Report. CoRR, abs/2303.08774, 2023. doi: 10.48550/arXiv.2303. 08774. URL https://doi.org/10.48550/arXiv.2303.08774.   \n[51] Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P. F., Leike, J., and Lowe, R. Training language models to follow instructions with human feedback. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/ 2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html.   \n[52] Pereira, R., Couto, M., Ribeiro, F., Rua, R., Cunha, J., Fernandes, J. P., and Saraiva, J. Ranking programming languages by energy efficiency. Science of Computer Programming, 205:102609, 2021.   \n[53] Qiu, R., Zeng, W. W., Tong, H., Ezick, J., and Lott, C. How efficient is llm-generated code? a rigorous & high-standard benchmark. arXiv preprint arXiv:2406.06647, 2024.   \n[54] Rozi\u00e8re, B., Lachaux, M., Chanussot, L., and Lample, G. Unsupervised translation of programming languages. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6- 12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ ed23fbf18c2cd35f8c7f8de44f85c08d-Abstract.html.   \n[55] Rozi\u00e8re, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X. E., Adi, Y., Liu, J., Remez, T., Rapin, J., Kozhevnikov, A., Evtimov, I., Bitton, J., Bhatt, M., Canton-Ferrer, C., Grattafiori, A., Xiong, W., D\u00e9fossez, A., Copet, J., Azhar, F., Touvron, H., Martin, L., Usunier, N., Scialom, T., and Synnaeve, G. Code llama: Open foundation models for code. CoRR, abs/2308.12950, 2023. doi: 10.48550/ARXIV.2308.12950. URL https://doi.org/10.48550/arXiv.2308. 12950.   \n[56] Shi, J., Yang, Z., and Lo, D. Efficient and green large language models for software engineering: Vision and the road ahead. arXiv preprint arXiv:2404.04566, 2024.   \n[57] Shinn, N., Cassano, F., Gopinath, A., Narasimhan, K., and Yao, S. Reflexion: language agents with verbal reinforcement learning. In Oh, A., Naumann, T., Globerson, A., Saenko, K., Hardt, M., and Levine, S. (eds.), Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/ 2023/hash/1b44b878bb782e6954cd888628510e90-Abstract-Conference.html.   \n[58] Team, X.-L. Xwin-lm, 9 2023. URL https://github.com/Xwin-LM/Xwin-LM.   \n[59] Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher, L., Canton-Ferrer, C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev, A., Koura, P. S., Lachaux, M., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi, ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "K., Schelten, A., Silva, R., Smith, E. M., Subramanian, R., Tan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S., and Scialom, T. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288, 2023. doi: 10.48550/ARXIV.2307.09288. URL https://doi.org/10.48550/arXiv.2307.09288. ", "page_idx": 15}, {"type": "text", "text": "[60] Viswanathan, V., Zhao, C., Bertsch, A., Wu, T., and Neubig, G. Prompt2model: Generating deployable models from natural language instructions. arXiv preprint arXiv:2308.12261, 2023.   \n[61] Wang, Y., Wang, W., Joty, S. R., and Hoi, S. C. H. Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation. In Moens, M., Huang, X., Specia, L., and Yih, S. W. (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pp. 8696\u20138708. Association for Computational Linguistics, 2021. doi: 10.18653/V1/2021.EMNLP-MAIN.685. URL https://doi.org/10.18653/v1/ 2021.emnlp-main.685.   \n[62] Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A., Khashabi, D., and Hajishirzi, H. Self-instruct: Aligning language models with self-generated instructions. In Rogers, A., BoydGraber, J. L., and Okazaki, N. (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pp. 13484\u201313508. Association for Computational Linguistics, 2023. doi: 10.18653/V1/ 2023.ACL-LONG.754. URL https://doi.org/10.18653/v1/2023.acl-long.754.   \n[63] Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., and Le, Q. V. Finetuned language models are zero-shot learners. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id=gEZrGCozdqR.   \n[64] Wei, J., Durrett, G., and Dillig, I. Typet5: Seq2seq type inference using static analysis. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/pdf?id $=$ 4TyNEhI2GdN.   \n[65] Wei, Y., Wang, Z., Liu, J., Ding, Y., and Zhang, L. Magicoder: Empowering code generation with oss-instruct. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/ forum?id $\\equiv$ XUeoOBid3x.   \n[66] Xu, C., Sun, Q., Zheng, K., Geng, X., Zhao, P., Feng, J., Tao, C., Lin, Q., and Jiang, D. Wizardlm: Empowering large pre-trained language models to follow complex instructions. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id $\\equiv$ CfXh93NDgH.   \n[67] Yao, Y., Wu, H., Guo, Z., Zhou, B., Gao, J., Luo, S., Hou, H., Fu, X., and Song, L. Learning from correctness without prompting makes LLM efficient reasoner. CoRR, abs/2403.19094, 2024. doi: 10.48550/ARXIV.2403.19094. URL https://doi.org/10.48550/arXiv.2403.19094.   \n[68] Yu, W., Zhang, Z., Liang, Z., Jiang, M., and Sabharwal, A. Improving language models via plugand-play retrieval feedback. CoRR, abs/2305.14002, 2023. doi: 10.48550/ARXIV.2305.14002. URL https://doi.org/10.48550/arXiv.2305.14002.   \n[69] Zhang, K., Li, Z., Li, J., Li, G., and Jin, Z. Self-edit: Fault-aware code editor for code generation. In Rogers, A., Boyd-Graber, J. L., and Okazaki, N. (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pp. 769\u2013787. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.ACL-LONG.45. URL https://doi.org/10.18653/v1/ 2023.acl-long.45.   \n[70] Zhang, K., Wang, D., Xia, J., Wang, W. Y., and Li, L. ALGO: synthesizing algorithmic programs with generated oracle verifiers. In Oh, A., Naumann, T., Globerson, A., Saenko, K., Hardt, M., and Levine, S. (eds.), Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/ 2023/hash/abe1eb21ceb046209c96a0f5e7544ccc-Abstract-Conference.html.   \n[71] Zhao, C., Jia, X., Viswanathan, V., Wu, T., and Neubig, G. Self-guide: Better task-specific instruction following via self-synthetic finetuning. arXiv preprint arXiv:2407.12874, 2024.   \n[72] Zheng, T., Zhang, G., Shen, T., Liu, X., Lin, B. Y., Fu, J., Chen, W., and Yue, X. Opencodeinterpreter: Integrating code generation with execution and refinement. arXiv preprint arXiv:2402.14658, 2024. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "A.1 Limitations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "EFFI-LEARNER presents a compelling solution for enhancing the efficiency of code generated by LLMs. However, it\u2019s crucial to acknowledge several potential limitations. Firstly, the multi-iteration self-optimization process at the heart of EFFI-LEARNER can be time-consuming, particularly when applied to intricate programming tasks. Yet, it\u2019s important to note that this investment of time can yield significant long-term benefits, as the optimized codes, once deployed, can considerably improve efficiency. Moreover, the process of feeding overhead profiles to LLMs to prompt code optimization may consume more tokens. This is due to the fact that the length of the overhead proflies necessitates additional tokens. Lastly, the effectiveness of EffiLearner has been primarily evaluated on Python. Therefore, its performance in different programming languages or environments may vary, underscoring the need for further testing and validation of this approach in a diverse range of contexts. ", "page_idx": 17}, {"type": "text", "text": "A.2 Broader Impacts ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Positive Societal Impacts The proposed method EFFI-LEARNER has the potential to bring about several positive societal impacts. Primarily, it can significantly enhance productivity by enabling developers to complete tasks more quickly and efficiently, due to the faster execution times and lower memory and processing power consumption of the optimized code. This is particularly beneficial in resource-constrained environments, such as mobile devices or embedded systems, where conserving resources is crucial. Moreover, EFFI-LEARNER\u2019s focus on improving code efficiency can lead to more cost-effective solutions, as lower resource consumption translates to reduced operational costs. ", "page_idx": 17}, {"type": "text", "text": "Negative Societal Impacts The increasing effectiveness of LLM-based tools like EFFI-LEARNER could also have negative societal impacts. For instance, it could lead to an over-reliance on these systems, potentially resulting in a decline in human coding skills and a lack of understanding of the underlying code. This could be problematic in situations where human intervention is necessary. Additionally, there is a risk of job displacement in the coding and programming industry, as the demand for human coders may decrease, particularly for routine coding tasks. Lastly, while EFFI-LEARNER can improve code efficiency, it does not necessarily address potential security vulnerabilities or privacy issues in the code, which could have significant societal impacts if not properly managed. ", "page_idx": 17}, {"type": "text", "text": "A.3 Prompt Template ", "text_level": 1, "page_idx": 17}, {"type": "image", "img_path": "KhwOuB0fs9/tmp/b1fe69dbfa3abc47e4538facb70c8f449964879b1d88d8ee30208766f65ce31c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "KhwOuB0fs9/tmp/6d0268696a62706bf46cfc77c2d23bffeec3dbe2fd4b736dafbf3330c8381577.jpg", "table_caption": ["Table 7: Evaluation results of EFFI-LEARNER and baselines. Since the finetuned link for GPT-3.5- turbo from PIE is not available, we use the fine-tuned CodeLlama 7B for experiments. Due to the fine-tuned PIE CodeLlama 7B does not have the same correct tasks as the original CodeLlama, we then do not provide the initial version for the experiments. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "A.4 Comparison with Baselines ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "To demonstrate EFFI-LEARNER\u2019s effectiveness with existing baselines and some prompt engineering methods that can refine the prompt from the correctness to the efficiency of LLM-generated code. ", "page_idx": 18}, {"type": "table", "img_path": "KhwOuB0fs9/tmp/f11f8d78dec94d980d260a407863e6ab5a989717050e1a3b10f8d542420c1874.jpg", "table_caption": ["Table 8: Evaluation results of EFFI-LEARNER\u2019s effectiveness in the HumanEval dataset. "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "KhwOuB0fs9/tmp/47b984b27866f5be0e4f7b835e29f6145c46b67994e6c8381a7dbac88f9dfcca.jpg", "table_caption": ["Table 9: Evaluation results of EFFI-LEARNER\u2019s effectiveness in the MBPP dataset "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "A.5 Detailed Evaluation Metric for Efficiency ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In our work, we adopt the efficiency metrics proposed by EffiBench [27] to evaluate the effectiveness of EFFI-LEARNER in improving the efficiency of LLM-generated code. ", "page_idx": 19}, {"type": "text", "text": "Execution Time (ET) Execution time (ET) measures the average time taken for code execution. Mathematically, ET is defined as: ", "page_idx": 19}, {"type": "equation", "text": "$$\nE T={\\frac{1}{N}}\\sum^{N}T_{\\mathrm{code}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $E T$ is the execution time metric, $T_{\\mathrm{code}}$ is the execution time of the code (with all the test cases), and $N$ is the number of codes generated by code generation models used for evaluation. ", "page_idx": 19}, {"type": "text", "text": "Normalized Execution Time (NET) Normalized Execution Time (NET) measures the execution time required by generated code relative to that of a canonical solution. We define NET as: ", "page_idx": 19}, {"type": "equation", "text": "$$\nN E T={\\frac{1}{N}}\\sum^{N}{\\frac{T_{\\mathrm{code}}}{T_{\\mathrm{canonical}}}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $T_{\\mathrm{code}}$ is the execution time of the generated code, and $T_{\\mathrm{canonical}}$ is the execution time of the canonical solution. A NET value greater than 1 indicates that the generated code is slower than the canonical solution, while a value less than 1 suggests the generated code is faster. ", "page_idx": 19}, {"type": "text", "text": "Max Memory Usage (MU) Max Memory Usage (MU) measures the average max memory consumption during code execution. Mathematically, MU is defined as: ", "page_idx": 19}, {"type": "equation", "text": "$$\nM U=\\frac{1}{N}\\sum^{N}M_{\\mathrm{code}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $M U$ is the memory usage metric, $M_{\\mathrm{code}}$ is the max memory consumption of the generated code among all the test cases, and $N$ is the number of code instances generated by code generation models used for evaluation. This metric is critical for assessing the resource efficiency of generated code, particularly in environments with limited maximum memory capacity. ", "page_idx": 19}, {"type": "text", "text": "Normalized Max Memory Usage (NMU) Normalized Max Memory Usage (NMU) quantifies how the max memory efficiency of the generated code compares to the canonical solution. We define NMU as: ", "page_idx": 20}, {"type": "equation", "text": "$$\nN M U=\\frac{1}{N}\\sum^{N}\\frac{M_{\\mathrm{code}}}{M_{\\mathrm{canonical}}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $N M U$ is the normalized max memory usage metric, $M_{\\mathrm{code}}$ is the max memory usage of the generated code, and $M_{\\mathrm{canonical}}$ is the max memory usage of the canonical solution. An NMU value less than 1 indicates that the generated code is more memory-efficient than the canonical solution, whereas a value greater than 1 suggests it is less efficient in terms of memory usage. This metric provides a relative measure of the memory optimization in the generated code in comparison to a standard baseline. ", "page_idx": 20}, {"type": "text", "text": "Total Memory Usage (TMU) Total Memory Usage (TMU) assesses the efficiency of memory usage throughout the execution of code, taking into account both the magnitude and duration of memory utilization. To calculate TMU, first, monitor and record the memory usage at discrete time intervals during the execution, resulting in a memory usage profile $M(t)$ , where $t$ represents time. Then, compute the area under the curve of $M(t)$ over the total execution time, $T_{\\mathrm{total}}$ , using numerical integration methods such as the trapezoidal rule: ", "page_idx": 20}, {"type": "equation", "text": "$$\nT M U=\\frac{1}{N}\\sum^{N}\\int_{0}^{T_{\\mathrm{total}}}M(t)\\,d t\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "A lower TMU value indicates higher memory efficiency, reflecting an optimized balance between the amount of memory used and the duration of its usage. ", "page_idx": 20}, {"type": "text", "text": "Normalized Total Memory Usage (NTMU) The Normalized Total Memory Usage (NTMU) offers a comparison of the dynamic memory efficiency between the generated code and the canonical solution. To determine NTMU, calculate the TMU for both the generated code and the canonical solution. Normalize the TMU of the generated code by dividing it by the TMU of the canonical solution: ", "page_idx": 20}, {"type": "equation", "text": "$$\nN T M U=\\frac{1}{N}\\sum\\frac{N}{T M U_{\\mathrm{canonical}}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $T M U_{\\mathrm{code}}$ is the TMU of the generated code and $T M U_{\\mathrm{canonical}}$ is the TMU of the canonical solution. An NTMU value less than 1 signifies that the generated code manages dynamic memory more efficiently compared to the canonical solution, while a value greater than 1 indicates a less efficient management of dynamic memory. This metric provides insight into the relative use of dynamic memory of generated code compared to an established benchmark. ", "page_idx": 20}, {"type": "text", "text": "A.6 Additional Related Work ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Instruction Tuning for Code Instruction tuning has proven effective in enhancing the usability and overall performance of LLMs across various language tasks [51, 63, 60, 71]. This approach has been extended to the domain of code generation. The core challenge is the acquisition of high-quality instructional data, which is often labor-intensive. To address this, recent research has focused on developing methods to generate synthetic instruction data. Studies have shown that textbook-quality synthetic data alone can improve a model\u2019s coding and reasoning capabilities [23, 34]. One early effort was Self-Instruct [62], which utilized LLMs to generate synthetic instruction-response pairs using carefully crafted prompts. The same LLM was then instruction-tuned on this synthetic data. Code Alpaca [10] applied the Self-Instruct approach with GPT models, tailoring it specifically for code generation, editing, and optimization tasks. Building upon this, WizardCoder [39] adapted the Evol-Instruct technique [66] to the coding domain by designing heuristic prompts to create more complex and diverse synthetic data. OSS-Instruct [65] took a different approach by leveraging LLMs to automatically generate new coding problems inspired by random code snippets from open-source repositories. In contrast, Octopack [46] focused on collecting and filtering high-quality Git commit messages that resemble natural language instructions. ", "page_idx": 20}, {"type": "text", "text": "Problem: You have a lock in front of you with 4 circular wheels. Each wheel has 10 slots: $\\because0^{\\circ}$ , $^{,}1^{,},^{,}2^{,},^{,}3^{,},^{,}4^{,},^{,}5^{,},^{,}6^{,},^{,}7^{,},^{,}8^{,},^{,}9^{,}$ The wheels can rotate freely and wrap around: for example we can turn $\\mathbf{\\nabla}^{\\prime}\\mathbf{9}^{\\prime}$ to be $\\because0^{\\circ}$ , or $\\because0^{\\circ}$ to be $\\cdot\\,_{9},$ . Each move consists of turning one wheel one slot. The lock initially starts at $'0000`$ , a string representing the state of the 4 wheels. You are given a list of deadends dead ends, meaning if the lock displays any of these codes, the wheels of the lock will stop turning and you will be unable to open it. Given a target representing the value of the wheels that will unlock the lock, return the minimum total number of turns required to open the lock, or -1 if it is impossible. ", "page_idx": 21}, {"type": "text", "text": "Example 1:   \nInput: deadends $=[\"0201\",\"0101\",\"0102\",\"1212\",\"2002\"],\\mathrm{target}=\"0202\"$   \nOutput: 6 Output: 6   \nExplanation: A sequence of valid moves would be $\"0000\"->\"1000\"->\"1100\"->$ $\"1200\"->\"1201\"->\"1202\"->\"0202\"$ . Note that a sequence like \" $\"0000\"->\"0001\"$ $->\\mathrm{{}^{\\\"}0002\"}->\\mathrm{{}^{\\\"}0102\"}->\\mathrm{{}^{\\\"}0202\"}$ would be invalid, because the wheels of the lock become stuck after the display becomes the dead end $\"0102\"$ . Example 2:   \nInput: deadends $=[\"8888\"]$ , target $\\mathbf{\\Sigma}=\\mathbf{\\Sigma}^{\"}0009^{\\prime}$ \"   \nOutput: 1   \nExplanation: We can turn the last wheel in reverse to move from $\"0000\"->\"0009\"$ .   \nExample 3:   \nInput: deadends $=[\"8887\"$ ,\"8889\",\"8878\",\"8898\",\"8788\",\"8988\",\"7888\",\"9888\"], target $=$   \n\"8888\" ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "Output: -1 Explanation: We cannot reach the target without getting stuck. ", "page_idx": 21}, {"type": "text", "text": "Constraints: \u2022 $1\\leq$ deadends.length $\\leq500$ \u2022 deadends[i].length $==4$ \u2022 target.length $==4$ \u2022 target will not be in the list deadends. \u2022 target and deadends[i] consist of digits only. ", "page_idx": 21}, {"type": "text", "text": "solution $=$ Solution()   \nassert solution.openLock([\"0201\",\"0101\", $\"0102\"$ ,\"1212\",\"2002\"], \"0202\") == 6   \nassert solution.openLock([\"8888\"], $\"0009\")\\;\\;{=}\\;\\;1$   \nassert solution.openLock([\"8887\",\"8889\",\"8878\",\"8898\",\"8788\",\"8988\",\"7888\",\"9888\"], \"8888\") == -1 ", "page_idx": 21}, {"type": "text", "text": "Figure 4: A case illustration of GPT-4-turbo-preview (OpenLock). Part A: Task Description. ", "page_idx": 21}, {"type": "text", "text": "Completion ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "from typing import List from collections import deque ", "page_idx": 22}, {"type": "text", "text": "class Solution: def openLock(self, deadends: List[str], target: str) $->$ int: def neighbors(node): for i in range(4): $\\tt x=$ int(node[i]) for d in (-1, 1): $\\textbf{y}=\\begin{array}{r}{(\\textbf{x}+\\textbf{d})\\ \\ \\%\\ \\ 10}\\end{array}$ $\\dot{\\mathrm{yield~node}}\\left[:\\dot{\\mathrm{a}}\\right]\\;\\;+\\;\\;\\mathrm{str}\\left(\\mathrm{y}\\right)\\;\\;+\\;\\;\\mathrm{node}\\left[\\dot{\\mathrm{\\bf~1}}+\\dot{\\mathrm{\\bf~1}}:\\right]$ dead $=$ set(deadends) queue $=$ deque $:[(\\,^{\\prime}\\,0000\\,^{\\prime}$ , 0)]) seen $=\\{\\cdot0000\\cdot\\}$ while queue: node, depth $=$ queue.popleft() if node $==$ target: return depth if node in dead: continue for nei in neighbors(node): if nei not in seen: seen.add(nei) queue.append((nei, depth $^{+}\\ 1)$ ) return -1 ", "page_idx": 22}, {"type": "text", "text": "Figure 5: A case illustration of GPT-4-turbo-preview (OpenLock). Part B: GPT-4-turbo-preview generated code. ", "page_idx": 22}, {"type": "text", "text": "Execution Time Profile ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Total time: 23.5891 s File: 752_tmp.py Function: openLock at line 24 ", "page_idx": 23}, {"type": "table", "img_path": "KhwOuB0fs9/tmp/e5d7940567d763973dcc92866585d5f90edb5fde8c0303d841b3f1e8d83f57cb.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "KhwOuB0fs9/tmp/df2e4256142f407356a910b0a429aaf170e91fb20cb8224e29ecc05d4ce220ff.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "Figure 6: A case illustration of GPT-4-turbo-preview (OpenLock). Part C: Execution time profile. ", "page_idx": 23}, {"type": "text", "text": "Memory Usage Profile ", "text_level": 1, "page_idx": 24}, {"type": "table", "img_path": "KhwOuB0fs9/tmp/cb4e6b50fb0b8712d137d56fc8a534ae9e19d68b48243534e81f8acf8224816c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "Figure 7: A case illustration of GPT-4-turbo-preview (OpenLock). Part D: Memory usage profile. ", "page_idx": 24}, {"type": "text", "text": "from typing import List from collections import deque ", "page_idx": 25}, {"type": "text", "text": "class Solution: def openLock(self, deadends: List[str], target: str) $->$ int: \"\"\"Calculate the minimum number of moves to open the lock from '0000' to the target avoiding $\\hookrightarrow$ deadends.\"\"\" if $\"\\,0000\"$ in deadends: return -1 if target $\\begin{array}{r l}{==}&{{}^{\\prime}0000^{\\prime}}\\end{array}$ : return 0 def getNextStates(state: str) $->$ List[str]: \"\"\"Generate the next states for a given lock state.\"\"\" res $=$ [] for i in range(4): current_digit $=$ int(state[i]) for diff in [-1, 1]: $\\#$ decrease or increase the digit new_digit $=$ (current_digit $^+$ diff) % 10 res.append(state[:i] $^+$ str(new_digit) $^+$ state[i $^{+1}$ :]) return res def bfs(queue: Deque[str], distances: dict, other_distances: dict) $->$ int: \"\"\"Perform a step of BFS, expanding the current queue.\"\"\" for _ in range(len(queue)): current $=$ queue.popleft() step_count $=$ distances[current] for nextState in getNextStates(current): if nextState in deadends_set or nextState in distances: continue if nextState in other_distances: return step_count $^{+}\\ 1\\ +$ other_distances[nextState] distances[nextState] $=$ step_count $^{+1}$ queue.append(nextState) return -1 deadends_set $=$ set(deadends) # Early exit if the initial state is a deadend if $\"\\,0000\"$ in deadends_set: return -1 # Initialize BFS structures for bidirectional search queue1, queue2 $=$ deque(['0000']), deque([target]) distance1, distance2 $=\\{^{11}0000^{\\prime\\prime}$ : 0}, {target: 0} # Perform BFS from both ends while queue1 and queue2: result $=$ bfs(queue1, distance1, distance2) if result $\\smash{\\xi=\\xi-1}$ : return result result $=$ bfs(queue2, distance2, distance1) if result $\\smash{\\xi=\\xi-1}$ : return result return -1 ", "page_idx": 25}, {"type": "text", "text": "Figure 8: A case illustration of GPT-4-turbo-preview (OpenLock). Part E: Self-Optimization Code. ", "page_idx": 25}, {"type": "text", "text": "Execution Time Profile ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Total time: 3.35559 s File: 752_tmp.py Function: openLock at line 24 ", "page_idx": 26}, {"type": "image", "img_path": "KhwOuB0fs9/tmp/ab671dfa00a3dd88a2652abedf15df5a787a9638e8a0a20b12685b1d26e22535.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Figure 9: A case illustration of GPT-4-turbo-preview (OpenLock). Part F1: Execution time profile for Self-Optimization Code. ", "page_idx": 26}, {"type": "text", "text": "Execution Time Profile ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Total time: 0.000697948 s File: 752_tmp.py Function: getNextStates at line 32 ", "page_idx": 27}, {"type": "image", "img_path": "KhwOuB0fs9/tmp/56262a295c8ab73e07febe0f5cdbab63460ea2ff33de335cd180f0c88f7dfc66.jpg", "img_caption": [], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Figure 10: A case illustration of GPT-4-turbo-preview (OpenLock). Part F2: Execution time profile for Self-Optimization Code. ", "page_idx": 27}, {"type": "table", "img_path": "KhwOuB0fs9/tmp/448e321293368ecd9e7c7d9e6b4c5e6320abf84665002abd7c07c34a82c1830e.jpg", "table_caption": ["Figure 11: A case illustration of GPT-4-turbo-preview (OpenLock). Part G: Memory usage profile. "], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "Problem: Given two sorted arrays nums1 and nums2 of size m and n respectively, return the median of the two sorted arrays.   \nThe overall run time complexity should be ${\\mathcal{O}}(\\log(m+n))$ . ", "page_idx": 29}, {"type": "text", "text": "Example 1: ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Input: nums $\\mathrm{~\\small~\\displaystyle~l~}=\\mathrm{~\\small~\\displaystyle~\\left[1\\:,3\\right]~}$ , $\\mathrm{nums}2\\ =\\ [2]$   \nOutput: 2.00000   \nExplanation: merged array $\\ =\\ [1,2,3]$ and median is 2. ", "page_idx": 29}, {"type": "text", "text": "Example 2: ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Input: nums $1\\ =\\ [1\\,,2]$ , nums2 = [3,4] Output: 2.50000 Explanation: merged array $\\begin{array}{r l}{=}&{{}[1,2,3,4]}\\end{array}$ and median is $\\left(2\\right.\\dot{\\left.\\right.}+\\left.\\right.3\\right)$ $/\\ 2\\ =\\ 2.5$ . ", "page_idx": 29}, {"type": "text", "text": "Constraints: ", "text_level": 1, "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{~\\verb~\"~nums1.1ength~=~m~}}\\\\ &{\\mathrm{~\\verb~\"~nums2.1ength~=~n~}}\\\\ &{\\mathrm{~\\verb~\"~0\\leqm\\leq1000}}\\\\ &{\\mathrm{~\\verb~\"~0\\leqn\\leq1000}}\\\\ &{\\mathrm{~\\verb~\"~1\\leqm+n\\leq2000}}\\\\ &{\\mathrm{~\\verb~\"~-10^{6}\\leq n u m s1~\\cite[]{a k i2000000000}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "solution $=$ Solution() assert solution.findMedianSortedArrays([1, 3], [2]) $==~2,0$ assert solution.findMedianSortedArrays([1, 2], [3, $4{\\bf]}\\ )\\ \\mathrm{~=~}\\,2\\,.\\,5$ ", "page_idx": 29}, {"type": "text", "text": "Figure 12: A case illustration of GPT-4-turbo-preview (FindMedianSortedArrays). Part A: Task Description. ", "page_idx": 29}, {"type": "text", "text": "class Solution: def findMedianSortedArrays(self, nums1, nums2) $->$ float: A, ${\\texttt{B}}=$ nums1, nums2 total $=$ len(A) $^+$ len(B) half $=$ total // 2 if $\\mathtt{l e n}(\\mathtt{B})\\ <\\ \\mathtt{l e n}(\\mathtt{A})$ : A, $\\texttt{B}=\\texttt{B}$ , A l, $\\textbf{r}=\\textbf{0}$ , len(A) - 1 while True: $\\textbf{i}=\\textbf{(}\\mathbf{1}+\\textbf{r})\\ \\ /\\ /\\ \\textbf{2}\\ \\#\\ \\ A$ $B\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!=\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! $ Aleft $=\\textsf{A}[\\textsf{i}]$ if $\\dot{\\textbf{1}}>=\\textbf{0}$ else float(\"-infinity\") Aright $=\\mathrm{~\\tt~A~}[\\mathrm{i~\\alpha~+~\\alpha~1~}]$ if $\\left(\\textbf{i}+\\textbf{1}\\right)~<$ len(A) else float(\"infinity\") Bleft $=$ B[j] if $j~>=~0$ else float(\"-infinity\") Bright $\\mathbf{\\mu}=\\textsf{B}[\\dot{\\mathbf{j}}+\\mathbf{\\mu}1]$ if $(\\mathtt{j}\\ +\\ 1)\\ <\\ 1\\mathrm{en}(\\mathtt{B})$ else float(\"infinity\") if Aleft $<=$ Bright and Bleft $<=$ Aright: if total $\\%2$ : return min(Aright, Bright) return (max(Aleft, Bleft) $^+$ min(Aright, Bright)) / 2 elif Aleft $>$ Bright: $\\textbf{r}=\\dot{\\textbf{1}}-\\textbf{1}$ else: l = i + 1 ", "page_idx": 29}, {"type": "text", "text": "Figure 13: A case illustration of GPT-4-turbo-preview (FindMedianSortedArrays). Part B: GPT-4- turbo-preview generated code. ", "page_idx": 29}, {"type": "text", "text": "Task Description ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Timer unit: 1e-06 s ", "page_idx": 30}, {"type": "text", "text": "Total time: 0.00119913 s   \nFile: 4_tmp.py   \nFunction: findMedianSortedArrays at line 45 ", "page_idx": 30}, {"type": "table", "img_path": "KhwOuB0fs9/tmp/7ebd2646ef14b4385412838a2ec3572c8928bc9581d68bdcec6164d463cd078e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "Figure 14: A case illustration of GPT-4-turbo-preview (FindMedianSortedArrays). Part C: Execution time profile. ", "page_idx": 30}, {"type": "text", "text": "Task Description ", "text_level": 1, "page_idx": 31}, {"type": "table", "img_path": "KhwOuB0fs9/tmp/07b342481fa0760e5cb679a3a3d8f835c0cf3924a471ef50a9d1fda79c38bf3e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 31}, {"type": "text", "text": "Figure 15: A case illustration of GPT-4-turbo-preview (FindMedianSortedArrays). Part D: Memory usage profile. ", "page_idx": 31}, {"type": "text", "text": "Task Description ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "class Solution: def findMedianSortedArrays(self, nums1, nums2) $->$ float: # Ensure nums1 is the smaller array to minimize binary search domain if len(nums1) $>$ len(nums2): nums1, nums2 $=$ nums2, nums1 x, $\\mathtt{y}~=~\\mathtt{l e n}(\\mathtt{n u m s}1)$ ), len(nums2) low, high $=~0$ , ${\\tt x}$ while low $<=$ high: partitionX $=$ (low $^+$ high) // 2 partitionY $\\mathbf{\\Sigma}=\\textbf{(x}+\\textbf{y}+\\textbf{i)}\\,//\\,\\textbf{2}\\,.$ - partitionX # If partitionX is 0 it means nothing is there on left side. Use -inf for maxLeftX maxLeftX $=$ float('-inf') if partitionX $\\r==\\ 0$ else nums1[partitionX - 1] # If partitionX is length of input then there is nothing on right side. Use +inf for $\\hookrightarrow$ minRightX minRightX $=$ float('inf') if partitionX $==x$ else nums1[partitionX] maxLeftY $=$ float('-inf') if partitionY $\\r==\\ 0$ else nums2[partitionY - 1] minRightY $=$ float('inf') if partitionY $\\mathbf{\\mu=\\Phi\\mathfrak{M}}$ else nums2[partitionY] if maxLeftX $<=$ minRightY and maxLeftY $<=$ minRightX: # The partition is correct if $(\\textbf{x}+\\textbf{y})\\;\\;\\%\\;\\;2\\;=\\;\\;0$ : return (max(maxLeftX, maxLeftY) $^+$ min(minRightX, minRightY)) / 2.0 else: return max(maxLeftX, maxLeftY) elif maxLeftX $>$ minRightY: # We are too far on right side for partitionX. Go on left $\\hookrightarrow$ side. high $=$ partitionX - 1 else: $\\#$ We are too far on left side for partitionX. Go on right side. low $=$ partitionX $^{+1}$ ", "page_idx": 32}, {"type": "text", "text": "Figure 16: A case illustration of GPT-4-turbo-preview (FindMedianSortedArrays). Part E: SelfOptimization Code. ", "page_idx": 32}, {"type": "text", "text": "Task Description ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Timer unit: 1e-06 s ", "page_idx": 33}, {"type": "text", "text": "Total time: 0.00114754 s   \nFile: 4_tmp.py   \nFunction: findMedianSortedArrays at line 45 ", "page_idx": 33}, {"type": "image", "img_path": "KhwOuB0fs9/tmp/9e937ae2cfa1eb6fb447844936a37fe19841766c098103fa0012f60ffce8adff.jpg", "img_caption": [], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "Figure 17: A case illustration of GPT-4-turbo-preview (FindMedianSortedArrays). Part F: Execution time profile. ", "page_idx": 33}, {"type": "text", "text": "", "text_level": 1, "page_idx": 34}, {"type": "image", "img_path": "KhwOuB0fs9/tmp/f28aae566f39e9f08fc25d7a40c7dbda8de6e4ad3efc4fa31e4547296dbe51c0.jpg", "img_caption": [], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "Figure 18: A case illustration of GPT-4-turbo-preview (FindMedianSortedArrays). Part G: Memory usage profile. ", "page_idx": 34}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: In this paper, we provide EFFI-LEARNER to improve the efficiency of LLMgenerated code. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 35}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: We discuss the limitation of EFFI-LEARNER in Appendix ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 35}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: The paper does not include theoretical results. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 36}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We provide all source code of EFFI-LEARNER in Supplementary file Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 36}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: The source code has been uploaded into the supplementary file. The dataset used in the paper also has instructions to access. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 37}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: We provide the experimental setting in the main paper and appendix. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 37}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Justification: Our proposed method is an inference-only approach for LLM and we adopt the greedy-decoding strategy for all of our experiments, making the experiment results of each session consistent. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 37}, {"type": "text", "text": "", "page_idx": 38}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: All information was provided in the Appendix. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 38}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Justification: The research follow the NeurIPS Code of Ethics. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 38}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: We discuss societal impacts in the Appendix. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 38}, {"type": "text", "text": "", "page_idx": 39}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: the paper poses no such risks. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 39}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: We have cite all evaluated models and datasets. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 39}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 40}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: the paper does not release new assets. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 40}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: the paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 40}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Justification: the paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 40}]