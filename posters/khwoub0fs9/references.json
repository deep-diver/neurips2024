{"references": [{"fullname_first_author": "Chen, M.", "paper_title": "Evaluating large language models trained on code", "publication_date": "2021-07-03", "reason": "This paper is foundational for evaluating LLMs on code-related tasks, providing the HumanEval benchmark used extensively in the current research."}, {"fullname_first_author": "Austin, J.", "paper_title": "Program synthesis with large language models", "publication_date": "2021-08-07", "reason": "This paper introduces the MBPP benchmark, another key evaluation dataset for code generation models, which is also utilized in the current work."}, {"fullname_first_author": "Brown, T. B.", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-06", "reason": "This paper establishes the foundational capabilities of large language models as few-shot learners, influencing the development of LLMs for code generation."}, {"fullname_first_author": "Li, Y.", "paper_title": "Competition-level code generation with AlphaCode", "publication_date": "2022-03-07", "reason": "This paper introduces a strong baseline model (AlphaCode) in code generation, against which the efficiency of EFFI-LEARNER is compared."}, {"fullname_first_author": "Huang, D.", "paper_title": "EffiBench: Benchmarking the efficiency of automatically generated code", "publication_date": "2024-02-02", "reason": "This paper introduces the EffiBench dataset, the primary benchmark used for evaluating and comparing the efficiency improvements achieved by EFFI-LEARNER."}]}