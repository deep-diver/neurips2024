[{"figure_path": "SLXHVsbU8r/figures/figures_1_1.jpg", "caption": "Figure 1: (a) End-to-end autonomous driving paradigms. 1) The vanilla architecture that directly predicts control command. 2) The modularized design that combines various preceding tasks. 3) Our proposed framework with unsupervised pretext task. (b) Comparison of training cost, inference speed and average L2 error between our method and [21, 22] on 8 NVIDIA Tesla A100 GPUs.", "description": "This figure compares three different approaches to end-to-end autonomous driving: vanilla, modularized, and the authors' unsupervised approach.  Subfigure (a) illustrates the architectural differences, showing how the proposed method simplifies the design by using an unsupervised pretext task instead of multiple supervised subtasks. Subfigure (b) presents a quantitative comparison of these three methods in terms of training cost (GPU days), inference speed (FPS), and average L2 error.  The results highlight the superior efficiency and performance of the authors' unsupervised approach.", "section": "1 Introduction"}, {"figure_path": "SLXHVsbU8r/figures/figures_2_1.jpg", "caption": "Figure 2: The architecture of our UAD. The inference pipeline is marked by black arrows with blue background, which plans ego trajectory based on the input multi-view images. The training pipeline consists of Angular Perception Pretext (orange arrows with khaki background) and Direction-Aware Planning (orange arrows with purple background). \u201cF\u201d in BEV feature indicates the driving direction.", "description": "This figure shows the overall architecture of the Unsupervised Autonomous Driving (UAD) model proposed in the paper.  It's broken down into two main parts: the inference pipeline and the training pipeline. The inference pipeline takes multi-view images as input and outputs a planned ego trajectory.  The training pipeline involves two sub-tasks: the Angular Perception Pretext, which learns spatial and temporal relationships in the scene without manual annotation, and Direction-Aware Planning, which improves the robustness of the planning process using a self-supervised strategy.  The figure uses color-coded arrows to highlight the flow of information in both the inference and training stages.", "section": "3 Method"}, {"figure_path": "SLXHVsbU8r/figures/figures_3_1.jpg", "caption": "Figure 3: (a) Label generation for angular perception pretext. (b) Illustration of dreaming decoder.", "description": "This figure shows the architecture of the proposed Angular Perception Pretext. (a) illustrates the process of generating labels for the angular perception pretext. The input is multi-view images and 2D bounding boxes detected by an object detector.  These are projected into the BEV space, creating a BEV object mask. This mask is then partitioned into sectors to produce the angular objectness label. (b) shows the dreaming decoder which comprises multiple GRU layers. It uses cross attention to interact with angular queries and angular BEV features. This produces the angular query for the next timestep. ", "section": "3 Method"}, {"figure_path": "SLXHVsbU8r/figures/figures_4_1.jpg", "caption": "Figure 4: Illustration of direction-aware learning strategy.", "description": "This figure illustrates the direction-aware learning strategy used in the paper.  The BEV feature and mask are rotated with different angles. The rotated features are then used for both the pretext and planning tasks. A consistency loss is applied to the predicted trajectories of each augmented view to improve the robustness of the planning module for directional changes and input noises.", "section": "3 Method"}, {"figure_path": "SLXHVsbU8r/figures/figures_8_1.jpg", "caption": "Figure 2: The architecture of our UAD. The inference pipeline is marked by black arrows with blue background, which plans ego trajectory based on the input multi-view images. The training pipeline consists of Angular Perception Pretext (orange arrows with khaki background) and Direction-Aware Planning (orange arrows with purple background). \u201cF\u201d in BEV feature indicates the driving direction.", "description": "This figure shows the overall architecture of the Unsupervised Autonomous Driving (UAD) framework. It illustrates both the inference and training pipelines. The inference pipeline uses multi-view images as input to plan the ego trajectory.  The training pipeline consists of two parts: the Angular Perception Pretext, which models the driving scene without manual annotations, and the Direction-Aware Planning, which uses a self-supervised strategy to improve robustness.  The \"F\" in the BEV feature indicates the driving direction.", "section": "3 Method"}, {"figure_path": "SLXHVsbU8r/figures/figures_14_1.jpg", "caption": "Figure 2: The architecture of our UAD. The inference pipeline is marked by black arrows with blue background, which plans ego trajectory based on the input multi-view images. The training pipeline consists of Angular Perception Pretext (orange arrows with khaki background) and Direction-Aware Planning (orange arrows with purple background). \u201cF\u201d in BEV feature indicates the driving direction.", "description": "This figure shows the overall architecture of the proposed Unsupervised Autonomous Driving (UAD) framework.  It highlights two key components: the Angular Perception Pretext, which learns spatial and temporal information about the driving scene in an unsupervised manner, and the Direction-Aware Planning, which uses a self-supervised approach to improve the robustness of trajectory prediction. The diagram details the flow of information and the different modules involved in each component, including the use of angular queries, cross-attention mechanisms, and a dreaming decoder. The caption also clarifies the distinction between the inference and training pipelines.", "section": "3 Method"}, {"figure_path": "SLXHVsbU8r/figures/figures_15_1.jpg", "caption": "Figure 5: (a) Qualitative results in nuScenes. (b) Qualitative results in CARLA.", "description": "This figure shows qualitative results of the proposed method UAD in both the nuScenes and CARLA datasets.  (a) shows results in nuScenes, likely depicting predicted trajectories overlaid on real-world imagery of driving scenes. (b) likely shows similar results but within the simulated CARLA environment. These visualizations aim to show the quality of the autonomous driving behavior generated by the model, demonstrating accurate trajectory predictions and safe navigation.", "section": "4. More Visualizations"}, {"figure_path": "SLXHVsbU8r/figures/figures_15_2.jpg", "caption": "Figure 5: (a) Qualitative results in nuScenes. (b) Qualitative results in CARLA.", "description": "This figure shows qualitative results of the proposed method. (a) shows the results in nuScenes dataset. It presents the predicted angular-wise objectness and the planning result. (b) shows the qualitative results in CARLA simulator. It presents a comparison of the planning results between Transfuser, ST-P3, and the proposed method.", "section": "4. More Visualizations"}, {"figure_path": "SLXHVsbU8r/figures/figures_16_1.jpg", "caption": "Figure 10: Visualization of angular perception and planning in Carla.", "description": "This figure visualizes the angular perception and planning results in the CARLA simulator.  The top row shows an example where the model successfully identifies objects in different sectors (CAM_FRONT, CAM_FRONT_LEFT, CAM_FRONT_RIGHT, CAM_BACK_LEFT, CAM_BACK, CAM_BACK_RIGHT). The angular objectness predictions are shown, along with the resulting planning trajectory, compared to the planning trajectories of Transfuser and ST-P3. The middle and bottom rows show additional successful and less successful examples of the proposed method.", "section": "A.10 More Visualizations"}]