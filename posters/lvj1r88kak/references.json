{"references": [{"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-01", "reason": "This paper introduced the Transformer architecture, a foundational model for many subsequent advancements in natural language processing and computer vision, including the Mamba model discussed in the target paper."}, {"fullname_first_author": "Ze Liu", "paper_title": "Swin Transformer: Hierarchical vision transformer using shifted windows", "publication_date": "2021-10-27", "reason": "The Swin Transformer architecture is used as a backbone in the empirical study of the target paper, making it a crucial reference for understanding the experimental setup and results."}, {"fullname_first_author": "Albert Gu", "paper_title": "Mamba: Linear-time sequence modeling with selective state spaces", "publication_date": "2023-12-01", "reason": "This is the primary subject of the target paper, providing the core model and design principles which are analyzed and further developed."}, {"fullname_first_author": "Angelos Katharopoulos", "paper_title": "Transformers are rnns: Fast autoregressive transformers with linear attention", "publication_date": "2020-06-01", "reason": "This paper introduced linear attention, a crucial concept that is contrasted and compared with the Mamba model in the target paper."}, {"fullname_first_author": "Dongchen Han", "paper_title": "Bridging the divide: Reconsidering softmax and linear attention", "publication_date": "2024-01-01", "reason": "This is another paper by the same authors, offering related work and providing additional context for the comparison between linear attention and Mamba."}]}