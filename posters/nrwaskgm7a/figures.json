[{"figure_path": "NrwASKGm7A/figures/figures_1_1.jpg", "caption": "Figure 1: Our iterative self-training framework progressively scales up the hallucination annotation dataset size (left) and simultaneously increases the annotator's accuracy (right) in three stages.", "description": "This figure illustrates the iterative self-training framework used in the paper.  It shows how the dataset size and annotator accuracy improve over three stages.  Stage 1 starts with a small dataset and a weak annotator.  Stage 2 increases the dataset size by adding more responses and trains a stronger annotator. Stage 3 further expands the dataset by adding more topics and results in an ultra annotator that surpasses GPT-4 in accuracy. The figure visually represents the framework's iterative process of scaling up both the data and the model's performance.", "section": "1 Introduction"}, {"figure_path": "NrwASKGm7A/figures/figures_2_1.jpg", "caption": "Figure 2: The schema of EM-based interactive self-training framework. In the E-step, given unlabeled new data from the Data Growth Flywheel, the annotator predicts N candidate outputs y. Then the representative annotation y* is chosen via self-consistency. As a result, we construct a larger dataset by collecting the new annotations. In the M-step, we train an annotator on the larger dataset aligned to our training format. This annotation process consists of three phases: Factual Existence Judgment, Reference Information Extraction, and Hallucination Type Judgment. As a result, we gain a stronger annotator with higher accuracy.", "description": "This figure illustrates the Expectation-Maximization (EM) based iterative self-training framework used in the paper.  The E-step focuses on automatically annotating a larger dataset using the current best annotator and a self-consistency strategy to select the most reliable annotation for each data point. This produces a larger, improved dataset. The M-step uses this larger dataset to train a more accurate annotator. This process iterates, progressively scaling the dataset and improving annotation accuracy. The annotation process itself is broken down into three phases: Factual Existence Judgment, Reference Information Extraction, and Hallucination Type Judgment.", "section": "3 Method"}, {"figure_path": "NrwASKGm7A/figures/figures_4_1.jpg", "caption": "Figure 1: Our iterative self-training framework progressively scales up the hallucination annotation dataset size (left) and simultaneously increases the annotator's accuracy (right) in three stages.", "description": "The figure illustrates the iterative self-training framework used in the paper. It shows how the dataset size increases through three stages, leading to improved annotator accuracy.  The left side depicts the growth of the dataset size, starting small and increasing to a massive dataset. The right side shows the corresponding increase in annotator accuracy, starting with a weak annotator and progressing to a strong annotator.  This framework is based on the Expectation Maximization algorithm, where each iteration involves an expectation step (annotating a scaled dataset) and a maximization step (training a new, more accurate annotator).", "section": "1 Introduction"}]