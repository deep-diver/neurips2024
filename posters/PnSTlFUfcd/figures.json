[{"figure_path": "PnSTlFUfcd/figures/figures_0_1.jpg", "caption": "Figure 1: Diagrammatic representation of runtime verification and shielding.", "description": "This figure shows a diagram that illustrates the runtime verification and shielding process in reinforcement learning. The agent interacts with the environment by taking actions based on the current state. A model checker monitors the agent's actions to ensure they comply with safety properties. If the model checker detects an unsafe action, it sends a warning to a shield, which intervenes to prevent the unsafe action from being executed. The shield ensures that the agent's actions remain safe while allowing the agent to maximize rewards.", "section": "Introduction"}, {"figure_path": "PnSTlFUfcd/figures/figures_8_1.jpg", "caption": "Figure 2: Episode reward and cost for tabular RL \u2018colour\u2019 gridworld environment.", "description": "This figure shows the results of three different reinforcement learning algorithms on a tabular RL colour gridworld environment. The algorithms are Q-learning, Q-learning with cost function and counterfactual experiences, and Q-learning with shielding.  The figure contains three subfigures, one for each of the three safety properties (green, blue, purple) used for the experiments. Each subfigure shows both the reward and the cost over the number of steps. The shaded regions represent standard deviations over 5 random initializations.", "section": "7 Empirical Evaluation"}, {"figure_path": "PnSTlFUfcd/figures/figures_8_2.jpg", "caption": "Figure 7: Episode reward and violation rate for deep RL Atari Seaquest.", "description": "This figure shows the performance of three different algorithms on the Atari Seaquest game: DreamerV3, DreamerV3 with an augmented Lagrangian penalty, and DreamerV3 with the proposed shielding approach.  The left side shows the cumulative reward obtained, while the right side shows the violation rate for two safety properties. For both properties, the shielding approach shows a good balance between reward and constraint satisfaction.  In contrast, the baseline DreamerV3 maximizes reward without considering safety and DreamerV3 (LAG) shows difficulty finding an optimal tradeoff between reward and safety.", "section": "7 Empirical Evaluation"}, {"figure_path": "PnSTlFUfcd/figures/figures_22_1.jpg", "caption": "Figure 4: Colour gridworld environment. Top left hand corner (agent) is the start position. The agent must navigate to the goal position in the bottom right hand corner of grid-world. The coloured states labelled blue, green and purple correspondingly.", "description": "This figure shows a 9x9 grid world environment used in the paper's experiments. The agent (a Wall-E robot) starts in the top-left corner and must navigate to the goal (a plant) in the bottom-right corner.  There are three other colored states: green, blue, and purple. The agent receives a reward only when it reaches the goal.  The safety properties in the experiments restrict the agent's movement relative to these colored states.", "section": "D.1 Colour Gridworld"}, {"figure_path": "PnSTlFUfcd/figures/figures_24_1.jpg", "caption": "Figure 2: Episode reward and cost for tabular RL \u2018colour\u2019 gridworld environment.", "description": "This figure shows the performance of three different reinforcement learning algorithms on a tabular RL task in a color gridworld environment. The algorithms are compared based on their reward and cost over a fixed number of training steps.  The three algorithms are: Q-Learning (baseline), Q-Learning (COST-CF) which uses a cost function and counterfactual experiences, and Q-Learning (Shield), the proposed shielding method in this paper. Three different safety properties are tested, each with a different level of stochasticity (p) to demonstrate scalability.", "section": "7 Empirical Evaluation"}, {"figure_path": "PnSTlFUfcd/figures/figures_24_2.jpg", "caption": "Figure 6: Atari Seaquest environment [10, 50]. The goal is to rescue divers (small blue people), while shooting enemy sharks and submarines.", "description": "The figure shows a screenshot of the Atari Seaquest game.  The objective of the game is for the player to rescue divers while shooting enemy sharks and submarines. The player must manage oxygen levels and avoid getting hit by enemies. This image is used to illustrate the environment for the deep reinforcement learning experiments described in the paper.  The game's partially observable nature, meaning the agent doesn't have full knowledge of the environment state, makes it a challenging environment for safe reinforcement learning.", "section": "7 Empirical Evaluation"}, {"figure_path": "PnSTlFUfcd/figures/figures_25_1.jpg", "caption": "Figure 7: Episode reward and violation rate for deep RL Atari Seaquest.", "description": "This figure shows the performance of three different approaches on the Atari Seaquest game: DreamerV3 (baseline), DreamerV3 with an augmented Lagrangian penalty, and DreamerV3 with the proposed shielding method.  The plots show the reward and violation rate over the total number of steps.  DreamerV3 with shielding effectively balances reward and safety, while the baseline only maximizes reward and the Lagrangian approach struggles to balance the two.", "section": "Empirical Evaluation"}, {"figure_path": "PnSTlFUfcd/figures/figures_30_1.jpg", "caption": "Figure 2: Episode reward and cost for tabular RL \u2018colour\u2019 gridworld environment.", "description": "This figure shows the results of the tabular RL experiments conducted on the \u2018colour\u2019 gridworld environment.  The figure presents the episode reward and cost for four different algorithms: Q-Learning (without counterfactual experiences), Q-learning (with counterfactual experiences), Q-Learning (Shield), and Q-Learning (Shield-NO-CF).  Each algorithm is evaluated for three different safety properties (Properties 1, 2, and 3), with each property having a different level of stochasticity (p-value). The plots illustrate the trade-off between reward and safety (cost) achieved by the various approaches.  The shaded regions around the lines represent confidence intervals across multiple runs of the experiments. The x-axis represents the total number of steps.", "section": "Empirical Evaluation"}, {"figure_path": "PnSTlFUfcd/figures/figures_30_2.jpg", "caption": "Figure 5: Episode reward and cost for tabular RL \u2018colour\u2019 gridworld environment.", "description": "The figure shows the results of three different algorithms (Q-Learning, Q-Learning with penalties, and Shielding) on the tabular RL colour gridworld environment for three different safety properties.  Each plot shows the reward and cost over time (total number of steps). The safety properties have varying complexity.  It illustrates how the Shielding algorithm balances reward and safety.", "section": "7 Empirical Evaluation"}, {"figure_path": "PnSTlFUfcd/figures/figures_31_1.jpg", "caption": "Figure 5: Episode reward and cost for tabular RL \u2018colour\u2019 gridworld environment.", "description": "This figure shows the results of the tabular RL experiments on the colour gridworld environment. The x-axis represents the total number of steps taken during training. The y-axis on the left shows the episode reward obtained by the agent, while the y-axis on the right shows the cumulative cost incurred. For each safety property (1-3), there are multiple lines, one for each of four different values of the cost coefficient C used in the Q-learning algorithm with cost-based penalties (COST-CF).  The shaded regions represent the standard deviation across multiple runs. The figure demonstrates the trade-off between reward and safety; higher cost coefficients (C) lead to lower costs (fewer safety violations) but also lower rewards. The result is shown for each safety property from the paper, with the corresponding stochasticity parameter.", "section": "Empirical Evaluation"}, {"figure_path": "PnSTlFUfcd/figures/figures_31_2.jpg", "caption": "Figure 5: Episode reward and cost for tabular RL \u2018colour\u2019 gridworld environment.", "description": "This figure shows the results of three different algorithms on three different safety properties in a tabular reinforcement learning environment. The algorithms are: Q-learning (without safety), Q-learning with cost function and counterfactual data, and the proposed shielding algorithm. The safety properties have varying complexities regarding the minimal bad prefix size.  The x-axis represents the total number of steps, while the y-axis represents the reward and cost for each algorithm.  The shaded area represents the standard deviation over 5 runs.", "section": "7 Empirical Evaluation"}, {"figure_path": "PnSTlFUfcd/figures/figures_32_1.jpg", "caption": "Figure 12: Episode reward and cost for Q-learning, Q-learning (COST-CF) and Q-learning (Shield) all from the main paper. With smaller levels of stochasticity p", "description": "The figure shows the performance of three different RL algorithms on a gridworld environment with varying levels of stochasticity (p). The algorithms are standard Q-learning, Q-learning with penalties (COST-CF), and the proposed shielding method (Shield). The figure plots both reward and cost over training steps for three different properties (1, 2, 3) with different stochasticity levels(p = 0.1, 0.1, 0.05).  The shielding method generally shows a better balance between reward and safety. ", "section": "F.5 Level of stochasticity p"}]