[{"type": "text", "text": "Shielding Regular Safety Properties in Reinforcement Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 To deploy reinforcement learning (RL) systems in real-world scenarios we need   \n2 to consider requirements such as safety and constraint compliance, rather than   \n3 blindly maximizing for reward. In this paper we study RL with regular safety   \n4 properties. We present a constrained problem based on the satisfaction of regular   \n5 safety properties with high probability and we compare our setup to the some   \n6 common constrained Markov decision processes (CMDP) settings. We also present   \n7 a meta-algorithm with provable safety-guarantees, that can be used to shield the   \n8 agent from violating the regular safety property during training and deployment.   \n9 We demonstrate the effectiveness and scalability of our framework by evaluating   \n10 our meta-algorithm in both the tabular and deep RL setting. ", "page_idx": 0}, {"type": "text", "text": "11 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "12 The field of safe reinforcement learning (RL) [6, 28] has gained in  \n13 creasing interest, as practitioners begin to understand the challenges   \n14 of applying RL in the real world [26]. There exist several dis  \n15 tinct paradigms in the literature, including constrained optimization   \n16 [2, 20, 49, 58, 62, 74], logical constraint satisfaction [17, 24, 36\u2013   \n17 38, 66], safety-critical control [15, 19, 53], all of which are unified   \n18 by prioritizing safety- and risk-awareness during the decision making   \n19 process.   \n20 Constrained Markov decision processes (CMDP) [4] have emerged   \n21 as a popular framework for modelling safe RL, or RL with con  \n22 straints. Typically, the goal is to obtain a policy that maximizes   \n23 reward while simultaneously ensuring that the expected cumulative cost remains below a pre-defined   \n24 threshold. A key limitation of this setting is that constraint violations are enforced in expectation   \n25 rather than with high probability, the constraint thresholds also have limited semantic meaning, can   \n26 be very challenging to tune and in some cases inappropriate for highly safety-critical scenarios   \n27 [66]. Furthermore, the cost function in the CMDP is typically Markovian and thus fails to capture a   \n28 significantly expressive class of safety properties and constraints.   \n29 Regular safety properties [9] are interesting because for all but the simplest properties the correspond  \n30 ing cost function is non-Markovian. Our problem setup consists of the standard RL objective with   \n31 regular safety properties as constraints, we note that there has been a significant body of work that   \n32 combines temporal logic constraints with RL [17, 24, 36\u201338, 66], although many of these do not   \n33 explicitly separate reward and safety in the same way that we do.   \n34 Our approach relies on shielding [3], which is a safe exploration strategy that ensures the satisfaction   \n35 of temporal logic constraints by deploying the learned policy in conjunction with a reactive system   \n36 that overrides any unsafe actions. Most shielding approaches typically make highly restrictive   \n37 assumptions, such as full knowledge of the environment dynamics [3], or access to a simulator [29],   \n38 although there has been recent work to deal with these restrictions [30, 39, 73]. In this paper, we   \n39 opt for the most permissive setting, where the dynamics of the environment are unknown, runtime   \n40 verification of the agent is realized by finite horizon model checking with a learned approximation of   \n41 the environment dynamics. However, in principle our framework is flexible enough to accommodate   \n42 more standard model checking procedures as long as certain assumptions are met.   \n43 Our approach can be summarised as an online shielding approach (see Fig. 1), that dynamically   \n44 identifies unsafe actions during training and deployment, and deploys a safe \u2018backup policy\u2019 when   \n45 necessary. We summarise the main contributions of our paper as follows:   \n46 (1) We state a constrained RL problem based on the satisfaction of regular safety properties with high   \n47 probability, and we identify the conditions whereby our setup generalizes several CMDP settings,   \n48 including expected and probabilistic cumulative cost constraints.   \n49 (2) We present several model checking algorithms that can verify the finite-horizon satisfaction   \n50 probability of regular safety properties, this includes statistical model checking procedures that can   \n51 be used if either the transition probabilities are unavailable or if the state space is too large.   \n52 (3) We develop a set of sample complexity results for the statistical model checking procedures   \n53 introduced in point (2), which are then used to develop a shielding meta-algorithm with provable   \n54 safety guarantees, even in the most permissive setting (i.e., no access to the transition probabilities).   \n55 (4) We empirically demonstrate the effectiveness of our framework on a variety of regular safety   \n56 properties in both a tabular and deep RL settings. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "image", "img_path": "PnSTlFUfcd/tmp/98b5b4a89ab0f93657ed488f27fbca51116bbb513f132bb1e52f8f50c5865c50.jpg", "img_caption": ["Figure 1: Diagrammatic representation of runtime verification and shielding. "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "57 2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "58 Safety Paradigms in Reinforcement Learning. There exist many safety paradigms in RL, the most   \n59 popular being constrained MDPs. For CMDPs several constrained optimization algorithms have   \n60 been developed, most are gradient-based methods built upon Lagrange relaxations of the constrained   \n61 problem [20, 49, 58, 62] or projection-based local policy search [2, 74]. Model-based approaches to   \n62 CMDP [7, 11, 41, 64] have also gathered recent interest as they enjoy better sample complexity than   \n63 their model-free counterparts, which can be imperative for safe learning [44].   \n64 Linear Temporal Logic (LTL) constraints [17, 24, 36\u201338, 66] for RL have been developed as an   \n65 alternative to CMDPs to specify stricter and more expressive constraints. The LTL formula is typically   \n66 treated as the entire task specification, although some works have aimed to separate LTL satisfaction   \n67 and reward into two distinct objectives [66]. The typical procedure in this setting is to identify end   \n68 components of the MDP that satisfy the LTL constraint and construct a corresponding reward function   \n69 such that the optimal policy satisfies the LTL constraint with maximal probability. Formal PAC-style   \n70 guarantees have been developed for this setting [27, 36, 66, 71] although they typically rely on   \n71 non-trivial assumptions. We note that LTL constraints can capture regular safety properties, although   \n72 we explicitly separate reward and safety, making the work in this paper distinct from previous work.   \n73 More rigorous safety-guarantees can be obtained by using safety fliters [3], control barrier functions   \n74 (CBF) [5], and model predictive safety certification (MPSC) [67, 68]. To achieve zero-violation   \n75 training these methods typically assume that the dynamics of the system are known and thus they   \n76 are typically restricted to low-dimensional systems. While these methods come from safety-critical   \n77 control, they are closely related to safe reinforcement learning [15].   \n78 Learning Over Regular Structures. RL and regular properties have been studied in conjunction   \n79 before, perhaps most famously as \u2018Reward Machines\u2019 [42, 43] \u2013 a type of finite state automaton that   \n80 specifies a different reward function at each automaton state. Reward machines do not explicitly   \n81 deal with safety, rather non-Markovian reward functions that depend on histories distinguished by   \n82 regular languages. Several methods have been developed to exploit the structure of these automata   \n83 and dramatically speed up learning [42, 43, 55, 61], e.g., counter factual experiences.   \n84 Regular decision processes (RDP) [13] are a specific class non-Markovian DPs [8] that have also   \n85 been studied in several works [13, 22, 51, 59, 65]. Most of these works are theoretical and slightly   \n86 out-of-scope for this paper, as the RDP setting does not explicitly handle safety and encompasses   \n87 both non-Markovian rewards and transition probabilities.   \n88 Shielding. From formal methods, shielding for safe RL [3] forces hard constraints on policies, using   \n89 a reactive system that \u2018shields\u2019 the agent from taking unsafe actions. Synthesising a correct-by  \n90 construction reactive \u2018shield\u2019 typically requires access to the environment dynamics and can be   \n91 computationally demanding when the state or action space is large. Several recent works have aimed   \n92 to scale the concept of shielding to more general settings, relaxing the prerequisite assumptions for   \n93 shielding, by either only assuming access to a \u2018black box\u2019 model for planning [29], or learning a world   \n94 model from scratch [30, 39, 73]. Other notable works that can be viewed as shielding include, MASE   \n95 [69] \u2013 a safe exploration algorithm with access to an \u2018emergency reset button\u2019, and Recovery-RL   \n96 [63] \u2013 which has access to a \u2018recovery policy\u2019 that is activated when the probability of reaching an   \n97 unsafe state is too high. A simple form of shielding with LTL specifications has also been considered   \n98 [37, 54], but experimentally these methods have only been tested in quite simple settings. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "99 3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "100 For a finite set $\\boldsymbol{S}$ , let $P o w(S)$ denote the power set of $\\boldsymbol{S}$ . Also, let $D i s t(S)$ denote the set of   \n101 distributions over $\\boldsymbol{S}$ , where a distribution $\\mu:S\\to[0,1]$ is a function such that $\\begin{array}{r}{\\sum_{s\\in S}\\mu(s)=1}\\end{array}$ . Let   \n102 $S^{*}$ and $S^{\\omega}$ denote the set of finite and infinite sequences over $\\boldsymbol{S}$ respectively. T he set of all finite and   \n103 infinite sequences is denoted $S^{\\infty}=S^{*}\\cup S^{\\omega}$ . We denote as $|\\rho|$ the length of a sequence $\\rho\\in S^{\\infty}$ ,   \n104 where $|\\rho|=\\infty$ if $\\rho\\in S^{\\omega}$ . We also denote as $\\rho[i]$ the $i+1$ -th element of a sequence, when $i<|\\rho|$ ,   \n105 and we denote as $\\rho\\downarrow=\\rho[|\\rho|-1]$ the last element of a sequence, when $\\rho\\in S^{*}$ . A sequence $\\rho_{1}$ is a   \n106 prefix of $\\rho_{2}$ , denoted $\\rho_{1}\\preceq\\rho_{2}$ , if $|\\rho_{1}|\\le|\\rho_{2}|$ and $\\rho_{1}[i]=\\bar{\\rho_{2}[i]}$ for all $0\\leq i\\leq|\\rho_{1}|$ . A sequence $\\rho_{1}$ is   \n107 a proper prefix of $\\rho_{2}$ , denoted $\\rho_{1}\\prec\\rho_{2}$ , if $\\rho_{1}\\preceq\\rho_{2}$ and $\\rho_{1}\\neq\\rho_{2}$ .   \n108 Labelled MDPs and Markov Chains. An MDP is a tuple $\\mathcal{M}=(\\mathcal{S},\\mathcal{A},\\mathcal{P},\\mathcal{P}_{0},\\mathcal{R},A P,L)$ , where   \n109 $\\boldsymbol{S}$ and $\\boldsymbol{\\mathcal{A}}$ are finite sets of states and actions resp.; $\\mathcal{P}\\;:\\;\\mathcal{S}\\;\\times\\;\\mathcal{A}\\;\\rightarrow\\;D i s t(\\mathcal{S})$ is the transition   \n110 function; $\\mathcal{P}_{0}\\in D i s t(S)$ is the initial state distribution; $\\mathcal{R}:S\\times\\mathcal{A}\\rightarrow[0,1]$ is the reward function;   \n111 $A P$ is a set of atomic propositions, where $\\Sigma\\;=\\;P o w(A P)$ is the alphabet over $A P$ ; and $L:$   \n112 $s\\rightarrow\\Sigma$ is a labelling function, where $L(s)$ denotes the set of atoms that hold in a given state   \n113 $s\\in S$ . A memory-less (stochastic) policy is a function $\\pi:S\\to D i s t(A)$ and its value function,   \n114 denoted $V_{\\pi}\\,:\\,\\!S\\,\\rightarrow\\,\\mathbb{R}$ is defined as the expected reward from a given state under policy $\\pi$ , i.e.,   \n115 $\\begin{array}{r}{V_{\\pi}(s)=\\mathbb{E}_{\\pi}[\\sum_{t=0}^{T}\\mathcal{R}(s_{t},a_{t})|s_{0}=s]}\\end{array}$ , where $T$ is a fixed episode length. Furthermore, denote as   \n116 $\\mathcal{M}_{\\pi}\\,=\\,(S,\\mathcal{P}_{\\pi},\\mathcal{P}_{0},A P,L)$ the Markov chain induced by a fixed policy $\\pi$ , where the transition   \n117 function is such that $\\begin{array}{r}{\\mathcal{P}_{\\pi}(s^{\\prime}|s)=\\sum_{a\\in A}\\mathcal{P}(s^{\\prime}|s,a)\\pi(a|s)}\\end{array}$ . A path $\\rho\\in S^{\\infty}$ through $\\mathcal{M}_{\\pi}$ is a finite (or   \n118 infinite) sequence of states. Using standard results from measure theory it can be shown that the set   \n119 of all paths $\\left\\{\\rho\\in S^{\\omega}\\mid\\rho_{p r e f}\\preceq\\rho\\right\\}$ with a common prefix $\\rho_{p r e f}$ is measurable [9].   \n120 Probabilistic CTL. (PCTL) [9] is a branching-time temporal logic for specifying properties of   \n121 stochastic systems. A well-formed PCTL property can be constructed with the following grammar, ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Phi:=\\!\\!{\\mathrm{true~}}|{\\mathrm{~}}a{\\mathrm{~}}|\\neg\\Phi{\\mathrm{~}}|{\\mathrm{~}}\\Phi\\wedge\\Phi{\\mathrm{~}}|{\\mathrm{~}}{\\mathbb{P}}_{\\bowtie p}[\\varphi]}\\\\ &{\\varphi:=\\!X\\Phi{\\mathrm{~}}|{\\mathrm{~}}\\Phi U\\Phi{\\mathrm{~}}|{\\mathrm{~}}\\Phi U^{\\leq n}\\Phi}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "122 where $a\\in A P,\\bowtie\\in\\{<,>,\\leq,\\geq\\}$ is a binary comparison operator, and $p\\in[0,1]$ is a probability.   \n123 Negation $\\neg$ and conjunction $\\wedge$ are the familiar logical operators from propositional logic, and next $X$ ,   \n124 until $U$ and bounded until $U^{\\leq n}$ are the temporal operators from CTL [9]. We make the distinction   \n125 here between state formula $\\Phi$ and path formula $\\varphi$ . The satisfaction relation for state formula $\\Phi$ is   \n126 defined in the standard way for Boolean connectives. For probabilistic quantification we say that   \n127 $s\\Vdash\\mathbb{P}_{\\bowtie p}[\\varphi]$ iff $\\operatorname*{Pr}(s\\,\\,|\\!=\\,\\varphi):=\\operatorname*{Pr}(\\rho\\,\\in\\,S^{\\omega}\\:\\mid\\,\\rho[0]\\,=\\,s,\\rho\\,\\,|\\!=\\,\\varphi)\\,\\approx\\,p$ . Let $\\operatorname*{Pr}^{\\mathcal{M}}(s\\mid=\\varphi)$ be the   \n128 probability w.r.t. the Markov chain $\\mathcal{M}$ . For path formula $\\varphi$ the satisfaction relation is as follows, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\rho\\big\\vert\\!\\!}&{\\big=X\\Phi}&{\\mathrm{iff}\\quad\\rho[1]\\big\\vert=\\Phi}\\\\ {\\rho\\big\\vert\\!\\!}&{\\Phi_{1}U\\Phi_{2}}&{\\mathrm{iff}\\quad\\exists\\,j\\ge0\\,s.t.\\,(\\rho[j]\\big\\vert=\\Phi_{2}\\wedge\\forall0\\leq i<j,\\rho[i]\\big\\vert=\\Phi_{1})}\\\\ {\\rho\\big\\vert\\!\\!}&{\\Phi_{1}U^{\\leq n}\\Phi_{2}}&{\\mathrm{iff}\\quad\\exists\\,0\\leq j\\leq n\\,s.t.\\,(\\rho[j]\\big\\vert=\\Phi_{2}\\wedge\\forall0\\leq i<j,\\rho[i]\\big\\vert=\\Phi_{1})}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "129 From the standard operators of propositional logic we may derive disjunction $\\vee$ , implication $\\rightarrow$ and   \n130 coimplication $\\leftrightarrow$ . We also note that the common temporal operators \u2018eventually\u2019 $\\diamondsuit$ and \u2019always\u2019 $\\boxed{\\begin{array}{r l}\\end{array}}$ ,   \n131 and their bounded counterparts $\\langle>^{\\leq n}$ and $\\boxed{\\varDelta}^{<n}$ can be derived in a familiar way, i.e., $\\diamondsuit\\cdot:=\\operatorname{true}U\\Phi$ ,   \n132 $\\boxed{\\Phi}:=\\lnot\\diamond\\lnot\\Phi,$ , resp. ${\\diamondsuit\\bar{\\Psi}}$ :: $:=$ true $U{\\leq}^{n}\\Phi$ , $\\boxed{\\Omega^{\\le n}\\Phi:=\\neg\\diamond^{\\le n}\\neg\\Phi}$ .   \n133 Regular Safety Property. A linear time property $P_{s a f e}\\subseteq\\Sigma^{\\omega}$ over the alphabet $\\Sigma$ is a safety property   \n134 if for all words $w\\in\\Sigma^{\\omega}\\setminus P_{s a f e}$ , there exists a finite prefix $w_{p r e f}$ of $w$ such that $P_{s a f e}\\cap\\{\\bar{w^{\\prime}}\\in\\mathbf{\\bar{\\Sigma}}^{\\omega}\\}$   \n135 $w_{p r e f}\\,\\preceq\\,w^{\\prime}\\}\\,=\\,\\emptyset$ . Any such sequence $w_{p r e f}$ is called a bad prefix for $P_{s a f e}$ , a bad prefix $w_{p r e f}$   \n136 is called minimal iff there does not exist $\\bar{w}^{\\prime\\prime}\\prec w_{p r e f}$ such that $w^{\\prime\\prime}$ is a bad prefix for $P_{s a f e}$ . Let   \n137 $B a d P r e f(P_{s a f e})$ and MinBadPref $\\because(P_{s a f e})$ denote the set of of bad and minimal bad prefixes resp.   \n138 A safety property $P_{s a f e}\\in\\Sigma^{\\omega}$ is regular if the set BadPref $\\left(P_{s a f e}\\right)$ constitutes a regular language. That   \n139 is, there exists some deterministic finite automata (DFA) that accepts the bad prefixes for $P_{s a f e}$ [9],   \n140 that is, a path $\\rho\\in S^{\\omega}$ is \u2018unsafe\u2019 if the trace trace $\\cdot(\\rho)=L(\\rho[0]),\\bar{L}(\\rho[1]),\\cdot\\cdot\\cdot\\bar{\\in}\\,\\Sigma^{\\omega}$ is accepted by   \n141 the corresponding DFA.   \n142 Definition 3.1 (DFA). A deterministic finite automata is a tuple $\\mathcal{D}=(\\mathcal{Q},\\Sigma,\\Delta,\\mathcal{Q}_{0},\\mathcal{F}),$ , where $\\mathcal{Q}$   \n143 is a finite set of states, $\\Sigma$ is a finite alphabet, $\\Delta:{\\mathcal{Q}}\\times\\Sigma\\to{\\mathcal{Q}}$ is the transition function, $\\mathcal{Q}_{0}$ is the   \n144 initial state, and ${\\mathcal{F}}\\subseteq{\\mathcal{Q}}$ is the set of accepting states. The extended transition function $\\Delta^{*}$ is the   \n145 total function $\\Delta^{*}:\\mathcal{Q}\\times\\Sigma^{*}\\to\\mathcal{Q}$ defined recursively as $\\Delta^{*}(q,w)=\\Delta(\\Delta^{*}(q,\\stackrel{.}{w}\\backslash w\\downarrow),w\\downarrow)$ . The   \n146 language accepted by $D F\\!A\\;\\mathcal{D}$ is denoted $\\mathcal{L}(\\mathcal{D})=\\{w\\in\\Sigma^{*}\\mid\\Delta^{*}(\\mathcal{Q}_{0},w)\\in\\mathcal{F}\\}$ .   \n147 Furthermore, we denote as $P_{s a f e}^{H}\\subseteq\\Sigma^{\\omega}$ the corresponding finite-horizon safety property for $H\\in\\mathbb{Z}_{+}$ ,   \n148 where for all words $w\\,\\in\\,\\Sigma^{\\omega}\\setminus P_{s a f e}^{H}$ there exists $w_{p r e f}\\,\\preceq\\,w$ such that $|w_{p r e f}|\\leq H$ and $w_{p r e f}\\in$   \n149 $B a d P r e f(P_{s a f e})$ . We model check regular safety properties by synchronizing the DFA and Markov   \n150 chain in a standard way \u2013 by computing the product Markov chain.   \n151 Definition 3.2 (Product Markov Chain). Let $\\mathcal{M}=(S,\\mathcal{P},\\mathcal{P}_{0},A P,L)$ be a Markov chain and $\\mathcal{D}=$   \n152 $(\\mathcal{Q},\\Sigma,\\Delta,\\mathcal{Q}_{0},\\mathcal{F})$ be a DFA. The product Markov chain is $\\mathcal{M}\\otimes\\mathcal{D}=(\\mathcal{S}\\times\\mathcal{Q},\\mathcal{P}^{\\prime},\\mathcal{P}_{0}^{\\prime},\\{a c c e p t\\},L^{\\prime}),$ ,   \n153 where $L^{\\prime}(\\langle s,q\\rangle)\\ =\\ \\{a c c e p t\\}$ if $q\\ \\in\\ {\\mathcal{F}}$ and $L^{\\prime}(\\langle s,q\\rangle)\\;=\\;\\otimes\\;o/w$ , $\\mathcal{P}_{0}^{\\prime}(\\langle s,q\\rangle)\\;=\\;\\mathcal{P}_{0}(s)$ if $q=$   \n154 $\\Delta(\\mathcal{Q}_{0},L(s))$ and $0\\;o/w,$ and ${\\mathcal P}^{\\prime}(\\langle s^{\\prime},q^{\\prime}\\rangle|\\langle s,q\\rangle)={\\mathcal P}(s^{\\prime}|s)\\;i f q^{\\prime}=\\Delta(q,L(s^{\\prime}))$ and $0\\;o/w$ .   \n155 To compute the satisfaction probability of $P_{s a f e}$ for a given state $s\\in S$ we consider the set of paths   \n156 $\\rho\\in S^{\\omega}$ from $s$ and the corresponding trace in the DFA. We provide the following definition.   \n157 Definition 3.3 (Satisfaction probability for $P_{s a f e}$ ). Let $\\mathcal{M}=(S,\\mathcal{P},\\mathcal{P}_{0},A P,L)$ be a Markov chain   \n158 and let $\\mathcal{D}=(\\mathcal{Q},\\Sigma,\\Delta,\\mathcal{Q}_{0},\\mathcal{F})$ be the DFA such that $\\mathcal{L}(\\mathcal{D})=B a d P r e f(P_{s a f e})$ . For a path $\\rho\\in S^{\\omega}$   \n159 in the Markov chain, let $t r a c e(\\rho)\\,=\\,L(\\rho[0]),L(\\rho[1]),\\ldots\\,\\in\\,\\Sigma^{\\omega}$ be the corresponding word over   \n160 $\\Sigma=P o w(A P)$ . From a given state $s\\in S$ the satisfaction probability for $P_{s a f e}$ is defined as follows, ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{Pr}^{\\mathcal{M}}(s\\models P_{s a f e}):=\\mathrm{Pr}^{\\mathcal{M}}(\\rho\\in S^{\\omega}\\ |\\ \\rho[0]=s,t r a c e(\\rho)\\not\\in\\mathcal{L}(\\mathcal{D}))\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "161 Perhaps more importantly, we note that this satisfaction probability can be written as the following   \n162 reachability probability in the product Markov chain, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{Pr}^{\\mathcal{M}}(s\\models P_{s a f e})=\\mathrm{Pr}^{\\mathcal{M}\\otimes\\mathcal{D}}(\\langle s,q_{s}\\rangle\\models\\wp_{a c c e p t})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "163 where $q_{s}=\\Delta(\\mathcal{Q}_{0},L(s))$ and $\\diamondsuit$ accept is a PCTL path formula that reads, \u2018eventually accept\u2019 [9]. ", "page_idx": 3}, {"type": "text", "text": "164 For the corresponding finite-horizon safety property $P_{s a f e}^{H}$ we state the following result. ", "page_idx": 3}, {"type": "text", "text": "165 Proposition 3.4 (Satisfaction probability for $P_{s a f e}^{H}$ ). Let $\\mathcal{M}$ and $\\mathcal{D}$ be the MDP and DFA in Defn. 3.3.   \n166 For a path $\\rho\\,\\in\\,{\\cal S}^{\\omega}$ in the Markov chain, let $t r a c e_{H}(\\rho)\\;=\\;L(\\rho[0]),L(\\rho[1])\\ldots,L(\\rho[H])$ be the   \n167 corresponding finite word over $\\Sigma=P o w(A P)$ . For a given state $s\\in S$ the finite horizon satisfaction   \n168 probability for $P_{s a f e}$ is defined as follows, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{Pr}^{\\mathcal{M}}(s\\models P_{s a f e}^{H}):=\\mathrm{Pr}^{\\mathcal{M}}(\\rho\\in S^{\\omega}\\ |\\ \\rho[0]=s,t r a c e_{H}(\\rho)\\not\\in\\mathcal{L}(\\mathcal{D}))\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "169 where $H\\in\\mathbb{Z}_{+}$ is some fixed model checking horizon. Similar to before, we show that the finite   \n170 horizon satisfaction probability can be written as the following bounded reachability probability, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{Pr}^{\\mathcal{M}}(s\\vert=P_{s a f e}^{H})=\\mathrm{Pr}^{\\mathcal{M}\\otimes\\mathcal{D}}(\\left\\langle s,q_{s}\\right\\rangle\\,\\big\\vert\\not\\varphi\\leqslant\\mathcal{S}^{\\leq H}a c c e p t)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "171 where $q_{s}=\\Delta(\\mathcal{Q}_{0},L(s))$ is as before and $\\diamondsuit^{\\leq H}$ accept is the corresponding step-bounded PCTL path   \n172 formula that reads, \u2018eventually accept in $H$ timesteps\u2019.   \n173 The unbounded reachability probability can be computed by solving a system of linear equations, the   \n174 bounded reachability probability can be computed with $\\mathcal{O}(H)$ matrix multiplications, in both cases   \n175 the time complexity of the procedure is a polynomial in the size of the product Markov chain [9].   \n177 In this paper, we are interested in the quantitative model checking of regular safety properties for   \n178 a fixed finite horizon $H$ and in the context of episodic RL, i.e., where the length of the episode $T$   \n179 is fixed. In particular, at every timestep we constrain the (step-bounded) reachability probability   \n180 $\\operatorname*{Pr}(\\langle s,q\\rangle\\not\\in\\dot{\\gamma}^{\\leq H}a c c e p t)$ in the product Markov chain $\\mathcal{M}_{\\pi}\\otimes\\mathcal{D}$ . We assume that $H$ is chosen so as   \n181 to avoid any irrecoverable states [35, 64], i.e., those that lead to a violation of the safety property no   \n182 matter the sequence of actions taken, the precise details of this notion are presented in Section 6. We   \n183 specify the following constrained problem,   \n184 Problem 4.1 (Step-wise bounded regular safety property constraint). Let $P_{s a f e}$ be a regular safety   \n185 property, $\\mathcal{D}$ be the DFA such that $\\mathcal{L}(\\bar{\\mathcal{D}})=B a d\\bar{P}r e f(\\bar{P}_{s a f e})$ and $\\mathcal{M}$ be the $M D P$ ; ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\pi}V_{\\pi}\\quad s u b j e c t\\,t o\\quad\\operatorname*{Pr}\\left(\\left\\langle s_{t},q_{t}\\right\\rangle\\left\\vert=\\diamond^{\\leq H}a c c e p t\\right)\\leq p_{1}\\quad\\forall t\\in[0,T]\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "186 where all probability is taken under the product Markov Chain $\\mathcal{M}_{\\pi}\\otimes\\mathcal{D}$ , $p_{1}\\in[0,1]$ is a probability   \n187 threshold, $H$ is the model checking horizon and $T$ is the fixed episode length.   \n188 The hyperparameter $p_{1}$ is be directly used to trade-off safety and exploration in a semantically   \n189 meaningful way; $p_{1}$ prescribes the probability of satisfying the finite-horizon safety property $P_{s a f e}^{H}$ at   \n190 each timestep. In particular, if $p_{1}$ is sufficiently small then we can guarantee (with high-probability)   \n191 that the regular safety property $P_{s a f e}$ is satisfied for the entire episode length $T$ .   \n192 Proposition 4.2. Let $P_{s a f e}^{T}$ denote the (episodic) regular safety property for a fixed episode length   \n193 $T$ . Then satisfying $\\operatorname*{Pr}\\left(\\left\\langle s_{t},q_{t}\\right\\rangle\\,\\middle|=\\bigdiamond^{\\leq H}a c c e p t\\right)\\leq p_{1}$ for all $t\\,\\in\\,[0,T]$ guarantees that $\\operatorname*{Pr}(s_{0}\\Vdash$   \n194 $P_{s a f e}^{T})\\ge1-p_{1}\\cdot\\lceil T/H\\rceil$ , where $s_{0}\\sim\\mathcal{P}_{0}$ is the initial state.   \n195 Comparison to CMDP. In the remainder of this section, we compare our problem setup to various   \n196 CMDP settings [4], with the aim of unifying different perspectives from safe RL. The purpose of this   \n197 is to show that our proposed method for solving Problem 4.1 can also be used to satisfy other more   \n198 common CMDP constraints. First, we define the following cost function that prescribes a scalar cost   \n199 $C>0$ when the regular safety property $P_{s a f e}$ is violated and 0 otherwise.   \n200 Definition 4.3 (Cost function). Let $P_{s a f e}$ be a regular safety property and let $\\mathcal{D}$ be the DFA such   \n201 that $\\mathcal{L}(\\mathcal{D})=B a d P r e f(P_{s a f e})$ , modified such that for all $q\\in{\\mathcal{F}}$ , $q\\rightarrow\\mathcal{Q}_{0}$ . The cost function is then   \n202 defined as, ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{C}(\\langle s,q\\rangle)=\\left\\{C\\begin{array}{l l}{\\!\\!i f a c c e p t\\in L^{\\prime}(\\langle s,q\\rangle)}\\\\ {\\!\\!0}&{\\!\\!o t h e r w i s e}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "203 where $C>0$ is some generic scalar cost and $L^{\\prime}$ is the labelling function defined in Def. 3.2. ", "page_idx": 4}, {"type": "text", "text": "204 Resetting the DFA. Rather than reset the environment, the DFA is reset once it reaches an accepting   \n205 state, so as to measure the rate of constraint satisfaction over a fixed episode length $T$ . This can easily   \n206 be realized by replacing any outgoing transitions from the accepting states with transitions back to   \n207 the initial state, i.e., for all $q\\in{\\mathcal{F}}$ , $q\\rightarrow\\mathcal{Q}_{0}$ .   \n208 Non-Markovian costs. The cost function is Markov on the product states $\\langle s,q\\rangle\\in{\\cal S}\\times{\\cal Q}$ . However,   \n209 in most cases the cost function is non-Markovian in the original state space $\\boldsymbol{S}$ , since the automaton   \n210 state $q\\in\\mathcal{Q}$ could depend on some arbitrary history of states. Thus our problem setup generalizes the   \n211 standard CMDP framework with non-Markovian safety constraints.   \n212 Invariant properties. Invariant properties $P_{i n v}(\\Phi)$ , also written $\\boxed{\\bigtriangledown}\\Phi$ (\u2018always $\\Phi'$ ), where $\\Phi$ is a   \n213 propositional state formula, are the simplest type of safety properties where the cost function is still   \n214 Markov in the original state space. In this case we are operating in the standard CMDP framework,   \n215 we also note that checking invariant properties with a fixed model checking horizon has been studied   \n216 in previous works, as bounded safety [29, 30] and safety for a finite horizon [45].   \n217 The most common type of CMDP constraints are expected cumulative (cost) constraints, which   \n218 constrain the expected cost below a given threshold. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Problem 4.4 (Expected cumulative constraint [4, 58]). ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{\\pi}{\\operatorname*{max}}\\,V_{\\pi}}&{{}s u b j e c t\\,t o\\quad\\mathbb{E}_{\\langle s_{t},q_{t}\\rangle\\sim\\mathcal{M}_{\\pi}\\otimes\\mathcal{D}}\\left[\\sum_{t=0}^{T}\\mathcal{C}\\big(\\langle s_{t},q_{t}\\rangle\\big)\\right]\\le d_{1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "219 where $d_{1}\\in\\mathbb{R}_{+}$ is the cost threshold and $T$ is the fixed episode length. ", "page_idx": 4}, {"type": "text", "text": "220 Probabilistic cumulative (cost) constraints, are a stricter class of constraints that constrain the   \n221 cumulative cost with high probability, rather than in expectation. ", "page_idx": 5}, {"type": "text", "text": "Problem 4.5 (Probabilistic cumulative constraint [18, 56]). ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{\\pi}{\\operatorname*{max}}\\,V_{\\pi}}&{{}s u b j e c t\\,t o\\quad\\mathbb{P}_{\\langle s_{t},q_{t}\\rangle\\sim\\mathcal{M}_{\\pi}\\otimes\\mathcal{D}}\\left[\\sum_{t=0}^{T}\\mathcal{C}\\big(\\langle s_{t},q_{t}\\rangle\\big)\\leq d_{2}\\right]\\geq1-\\delta_{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "222 where $d_{2}\\in\\mathbb{R}_{+}$ is the cost threshold, $\\delta_{2}$ is a tolerance parameter, and $T$ is the fixed episode length. ", "page_idx": 5}, {"type": "text", "text": "223 We also consider instantaneous constraints, which bound the cost \u2018almost surely\u2019 at each timestep   \n224 $t\\in[0,T]$ . These are an even stricter type of constraint for highly safety-critical applications. ", "page_idx": 5}, {"type": "text", "text": "Problem 4.6 (Instantaneous constraint [23, 60, 69]). ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\tau}V_{\\pi}\\quad s u b j e c t\\:t o\\quad\\mathbb{P}_{\\langle s_{t},q_{t}\\rangle\\sim\\mathcal{M}_{\\pi}\\otimes\\mathcal{D}}\\big[\\mathcal{C}\\big(\\langle s_{t},q_{t}\\rangle\\big)\\leq d_{3}\\big]=1\\quad\\forall t\\in[0,T]\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "225 where $d_{3}\\in\\mathbb{R}_{+}$ is the cost threshold and $T$ is the fixed episode length. ", "page_idx": 5}, {"type": "text", "text": "226 In particular, these problems define a constrained set of feasible policies \u03a0. We make the distinction   \n227 here between a feasible policy and a solution to the problem, the former being any policy satisfying   \n228 the constraints of the problem and the later being the optimal policy within the feasible set $\\Pi$ .   \n229 Theorem 4.7. A feasible policy for Problem 4.1 is also a feasible policy for Problems 4.4, 4.5 and   \n230 4.6 under specific parameter settings for $p_{1}$ , $d_{1}$ , $d_{2}$ and $\\delta_{2}$ , and $d_{3}$ .   \n231 In Appendix G we provide a full set of statements that outline the relationships between the con  \n232 strained problems presented in this section. The significance of these results is that they demonstrate   \n233 by solving Problem 4.1 with our proposed method we can obtain feasible policies for Problems 4.4,   \n234 4.5 and 4.6, although for most of these problems there is no direct relationship between our problem   \n235 setup, in particular we can say little about whether the optimal policy for one problem is necessarily   \n236 optimal for another. Nevertheless, we find it interesting to explore the relationships between our setup   \n237 and other perhaps more common constrained RL problems. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "238 5 Model checking ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "239 In this section we outline several procedures for checking the finite-horizon satisfaction probability   \n240 of regular safety properties and we summarise the settings in which they can be used. ", "page_idx": 5}, {"type": "text", "text": "241 Assumption 5.1. We are given access to the \u2018true\u2019 transition probabilities $\\mathcal{P}$ . ", "page_idx": 5}, {"type": "text", "text": "242 Assumption 5.2. We are given access to a \u2018black box\u2019 model that perfectly simulates the \u2018true\u2019   \n243 transition probabilities $\\mathcal{P}$ .   \n244 Assumption 5.3. We are given access to an approximate dynamic model $\\widehat{\\mathcal{P}}\\approx\\mathcal{P}$ , where the total   \n245 variation $(T V)$ distance $\\bar{D_{T V}}(\\mathcal{P}_{\\pi}(\\cdot\\mid s),\\widehat{\\mathcal{P}}_{\\pi}(\\cdot\\mid s))\\leq\\epsilon/H,$ , for all $s\\in S$ .1   \n246 Exact model checking. Under Assumption 5.1 we can precisely compute the (finite horizon)   \n247 satisfaction probability of $P_{s a f e}$ , in the Markov chain $\\mathcal{M}_{\\pi}$ induced by the fixed policy $\\pi$ in time   \n248 $\\mathcal{O}(\\mathrm{poly}(\\mathrm{size}(\\mathcal{M}_{\\pi}\\otimes\\mathcal{D}))\\cdot H)$ [9], where $\\mathcal{D}$ is the DFA such that $\\mathcal{L}(\\mathcal{D}\\dot{)}=B a d P\\dot{r}e f(\\dot{P}_{s a f e})$ and $H$   \n249 is the model checking horizon. $H$ should not be too large and so the complexity of exact model   \n250 checking ultimately depends on the size of the product $\\mathcal{M}_{\\pi}\\otimes\\mathcal{D}$ , and so if the size of either the MDP   \n251 or DFA is too large then exact model checking may be infeasible.   \n252 Monte-Carlo model checking. To address the limitations of exact model checking, we can drop   \n253 Assumption 5.1. Rather, under Assumption 5.2, we can sample sufficiently many paths from a   \n254 \u2018black box\u2019 model of the environment dynamics and estimate the reachability probability $\\operatorname*{Pr}(\\langle s,q\\rangle\\vDash$   \n255 $\\diamondsuit^{\\leq H}a c c e p t)$ in the product Markov chain $\\mathcal{M}_{\\pi}\\otimes\\mathcal{D}$ , by computing the proportion of accepting paths.   \n256 Using statistical bounds, such as Hoeffding\u2019s inequality [40] or Bernstein-type bounds [52], we can   \n257 bound the error of this estimate, with high probability.   \n258 Proposition 5.4. Let $\\epsilon>0$ , $\\delta>0$ , $s\\in S$ and $H\\ge1$ be given. Under Assumption 5.2, we can   \n259 obtain an $\\epsilon$ -approximate estimate for the probability $\\operatorname*{Pr}(\\langle s,\\stackrel{\\cdot}{q}\\rangle\\,|=\\diamond^{\\leq H}a c c e p t)$ with probability at   \n260 least $1-\\delta_{i}$ , by sampling $\\begin{array}{r}{m\\geq\\frac{1}{2\\epsilon^{2}}\\log\\left(\\frac{2}{\\delta}\\right)}\\end{array}$ paths from the \u2018black box\u2019 model.   \n261 We note that the time complexity of these statistical methods does not depend in the size of the   \n262 product MDP or DFA, since the product states $\\langle s,q\\rangle\\in{\\mathcal{S}}\\times{\\mathcal{Q}}$ can be computed on-the- $\\boldsymbol{\\cdot}\\boldsymbol{\\cdot}$ , rather the   \n263 time complexity depends on the horizon $H$ , the desired level of accuracy $\\epsilon$ , failure probability $\\delta$ .   \n264 Model checking with approximate models. In most realistic cases neither the \u2018true\u2019 transition   \n265 probabilities nor a perfect \u2018black box\u2019 model is available to us before-hand. Under Assumption   \n266 5.3 we can model check with an \u2018approximate\u2019 model of the MDP dynamics, which can either be   \n267 constructed ahead of time (offilne) or learned from experience, with maximum likelihood (or similar).   \n268 We can then either exact model check in with the \u2018approximate\u2019 probabilities, or if the MDP is too   \n269 large, we can leverage statistical model checking by sampling paths from the \u2019approximate\u2019 model.   \n270 Proposition 5.5. Let $\\epsilon>0$ , $\\delta>0$ , $s\\in S$ and $H\\ge1$ be given. Under Assumption 5.3 we can make   \n271 the following two statements:   \n272 (1) We can obtain an $\\epsilon$ -approximate estimate for $\\operatorname*{Pr}(\\langle s,q\\rangle\\,\\vline=\\langle\\rangle^{\\leq H}\\substack{a c c e p t})$ with probability 1 by   \n273 exact model checking with the transition probabilities of ${\\widehat{\\mathcal{P}}}_{\\pi}$ in time $\\mathcal{O}(p o l y(s i z e(\\mathcal{M}_{\\pi}\\otimes\\mathcal{D}))\\cdot H)$ .   \n274 (2) We can obtain an \u03f5-approximate estimate for $\\operatorname*{Pr}(\\langle s,q\\rangle\\vline=\\diamond\\Sigma^{H}a c c e p t)$ with probability at least   \n275 $1-\\delta,$ , by sampling $m\\geq\\textstyle{\\frac{2}{\\epsilon^{2}}}\\log\\left({\\frac{2}{\\delta}}\\right)$ paths from the \u2018approximate\u2019 dynamics model ${\\widehat{\\mathcal{P}}}_{\\pi}$ . ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "276 6 Shielding the policy ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "277 At a high-level, the shielding meta-algorithm   \n278 works by switching between the \u2018task policy\u2019   \n279 trained with RL to maximize rewards and a   \n280 \u2018backup policy\u2019, which typically constitutes a   \n281 low-reward, possibly rule-based policy that is   \n282 guaranteed to be safe. In some cases this   \n283 \u2018backup policy\u2019 may be available to us before   \n284 training, although in most realistic cases it will   \n285 need to be learned. In our case we switch   \n286 from the \u2018task policy\u2019 to the \u2018backup policy   \n287 when the reachability probability $\\operatorname{Pr}(\\langle s,q\\rangle\\,\\,|{=}$   \n288 $\\diamondsuit^{\\leq H}a c c e p t)$ exceeds the probability threshold   \n289 $p_{1}$ . To check this we can use any of the model   \n290 checking procedures presented earlier. The   \n291 \u2018backup policy\u2019 is used when the reachability   \n292 probability exceeds $p_{1}$ . Intuitively if the \u2018backup   \n293 policy\u2019 is guaranteed to be safe, then our system   \n294 should satisfy the constraints of Problem 4.1,   \n295 independent of the \u2018task policy\u2019.   \n296 Backup policy. In general we assume no knowl  \n297 edge of the safety dynamics before training and   \n298 so the \u2018backup policy\u2019 needs to be learned. In   \n299 particular, we can use the cost function defined   \n300 in Defn. 4.3 and train the \u2018backup policy\u2019 with   \n301 RL to minimize the expected discounted cost ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Input: model checking parameters $(\\epsilon,\\,\\delta,\\,p,\\,H)$ ,   \nlabelling function $L$ , DFA $\\mathcal{D}=(\\mathcal{Q},\\Sigma,\\Delta,\\mathcal{Q}_{0},\\mathcal{F})$ .   \nOptional: probabilities $\\mathcal{P}$ , \u2018backup policy\u2019 $\\pi_{s a f e}$ .   \nInitialize: \u2018task policy\u2019 $\\pi_{t a s k}$ , \u2018backup policy\u2019 $\\pi_{s a f e}$   \nand (approximate) probabilities $\\widehat{\\mathcal{P}}$ .   \nfor each episode do Observe $s_{0}$ , $L\\!\\left(s_{0}\\right)$ and $q_{0}\\gets\\Delta(\\mathcal{Q}_{0},L(s_{0}))$ for $t=0,\\dots,T$ do $\\triangleright$ Fixed episode length Sample action $a\\sim\\pi_{t a s k}(\\cdot\\mid s_{t})$ if $\\bar{\\operatorname*{Pr}}(\\langle s,q\\rangle\\models\\diamond\\scriptscriptstyle\\diamond\\dot{x}^{H}a c c e\\dot{p t})\\check{\\leq}p_{1}$ then // Use the proposed action $a_{t}\\gets a$ else // Override the action $a_{t}\\sim\\pi_{s a f e}(\\cdot\\mid s_{t},q_{t})$ Play $a_{t}$ and observe $s_{t+1}$ , $L(s_{t+1}),r_{t}$ $q_{t+1}\\leftarrow\\Delta(q_{t},L(s_{t+1}))$ , $c_{t}\\gets1[q_{t+1}\\in\\mathcal{F}]$ Update $\\pi_{t a s k}$ with $\\left(s_{t},a_{t},s_{t+1},r_{t}\\right)$ Update $\\pi_{s a f e}$ with $(s_{t},q_{t},a_{t},s_{t+1},q_{t+1},c_{t})$ Update $\\widehat{\\mathcal{P}}$ with $(s_{t},a_{t},s_{t+1})$ ", "page_idx": 6}, {"type": "text", "text": "302 $(\\mathbb{E}_{\\pi}[\\sum_{t=0}^{T}\\gamma^{t}\\mathcal{C}(s_{t},q_{t})])$ . Importantly, we note that the cost function is defined on the product state   \n303 space $s\\times\\mathcal{Q}$ and so the \u2018backup policy\u2019 must also operate on this state space, possibly leading   \n304 to slower convergence. However, we can eliminate this issue entirely by training the \u2018backup pol  \n305 icy\u2019 with counterfactual experiences [42, 43] \u2013 a method originally used for reward machines that   \n306 generates additional synthetic data for the policy, by simulating experience from each automaton   \n307 state.   \n308 Meta Algorithm. We now present the structure of the shielding meta-algorithm (see Algorithm   \n309 1). The precise realization of this algorithm can vary depending on problem setting, tabular, deep   \n310 RL, etc., however the main structure of the algorithm remains the same. In particular, during   \n311 interaction with the environment we shield the agent by checking that the reachability probability   \n312 $\\operatorname*{Pr}(\\langle s,q\\rangle\\models\\diamond\\diamond{s t}_{a c c e p t})$ does not exceed threshold $p_{1}$ . Then, with the new accumulated experience   \n313 we update the \u2018task policy\u2019 denoted $\\pi_{t a s k}$ and the \u2018backup policy\u2019 denoted $\\pi_{s a f e}$ with RL, and if need be   \n314 we update our (approximate) dynamics model accordingly. In principle, the underlying RL algorithm   \n315 used to train either \u2018task policy\u2019 or \u2018backup policy\u2019 can differ, and the dynamics model can be a   \n316 simple maximum likelihood estimate or something more complex, e.g., Gaussian Process model   \n317 [25, 70], ensemble of parametric neural networks [21, 44] or a world model [32, 33].   \n318 Global Safety Guarantees. In the tabular setting we can guarantee the safety of the system described   \n319 in Algorithm 1 under various assumptions, even when doing Monte-Carlo model checking on an   \n320 \u2018approximate\u2019 model of the environment dynamics. First, we provide the following definitions.   \n321 Definition 6.1 (Non-critical state). A product state $\\langle s,q\\rangle\\in{\\mathcal{S}}\\times{\\mathcal{Q}}$ is said to be non-critical for $a$   \n322 given model checking horizon $H$ if for all policies \u03c0 we have $\\operatorname*{Pr}(\\langle s,q\\rangle\\models\\diamond\\llangle t_{a c c e p t})=0$ .   \n323 Definition 6.2 (Irrecoverable). A critical state $\\langle s,q\\rangle\\,\\in\\,{\\mathcal{S}}\\,\\times\\,{\\mathcal{Q}}$ is said to be irrecoverable with   \n324 probability $p_{1}$ if for all policies $\\pi$ we have $\\operatorname*{Pr}(\\langle s,q\\rangle\\,\\in\\,\\langle\\!\\supset a c c e p t\\rangle\\,\\geq\\,p_{1}$ . In other words, for any   \n325 sequence of actions $a_{0},a_{1},\\ldots$ the minimum probability $\\mathrm{Pr}^{m i n}(\\langle s,q\\rangle\\,\\vline=\\,\\diamondsuit a c c e p t)$ of reaching an   \n326 accepting state is $p_{1}$ , where $\\begin{array}{r}{\\operatorname*{Pr}^{m i n}(\\langle s,q\\rangle\\,|=\\diamond\\operatorname{acce}p t)=\\operatorname*{inf}_{\\pi}\\operatorname*{Pr}^{M_{\\pi}\\otimes\\mathcal{D}}(\\langle s,q\\rangle\\,|=\\diamond\\operatorname{acce}p t)}\\end{array}$ ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "327 The safety-guarantees for Algorithm 1 rely on the following assumptions. ", "page_idx": 7}, {"type": "text", "text": "328 Assumption 6.3. We assume $H$ is sufficiently large so that it is not possible to transition from any   \n329 non-critical state to an irrecoverable state. Furthermore we assume that there exists some $H^{*}<H$   \n330 such that $i f\\mathrm{Pr}^{m i n}(\\langle s,q\\rangle\\vDash\\left\\{a c c e p t\\right)=p_{1}$ then $\\mathrm{Pr}^{m i n}(\\langle s,q\\rangle\\,|=\\diamond\\diamond^{\\leq H^{*}}a c c e p t)=p_{1}$ .   \n331 Assumption 6.4. The initial state $\\langle s_{0},L(s_{0})\\rangle$ is non-critical and for any state $\\langle s,q\\rangle\\in{\\mathcal{S}}\\times{\\mathcal{Q}}$ that is   \n332 not irrecoverable, the \u2018backup policy\u2019 $\\pi_{s a f e}$ is satisfies $\\mathrm{Pr}^{\\mathcal{M}_{\\pi_{C}}\\otimes\\mathcal{D}}(\\langle s,q\\rangle\\,\\left|=\\diamond\\uit^{\\le H}a c c e p t\\right)\\le p_{1}$   \n333 Theorem 6.5. Under Assumption 6.3 and 6.4, and provided that every state action pair $(s,a)\\in S\\!\\times\\!A$   \n334 has been visited at least $\\begin{array}{r}{\\mathcal{O}\\left(\\frac{H^{2}|S|^{2}}{\\epsilon^{2}}\\log\\left(\\frac{|A||S|^{2}}{\\delta}\\right)\\right)}\\end{array}$ times. Then with probability $1-\\delta$ the system   \n335 satisfies the constraints of Problem 4.1, independent of the \u2018task policy\u2019.   \n336 The theory is quite conservative here due to the strong dependence on $|S|$ , in practice we can replace   \n337 the outer $\\bar{|}S|^{2}$ by the maximum number of successor states from any given state. With regards to our   \n338 assumptions, both are not overly restrictive. Assumption 6.3 essentially states that any irrecoverable   \n339 states, will reach the accepting state with some probability $>g$ within a fixed horizon $H^{*}$ . Similar   \n340 statements have been considered in prior work [35, 64]. Assumption 6.4 states that the \u2018backup   \n341 policy\u2019 satisfies $\\operatorname*{Pr}(\\langle s,q\\rangle\\models\\diamond\\widecheck{\\Sigma}^{H}a c c\\widecheck{e}p t)\\leq p_{1}$ if possible, we would expect this to be the case when   \n342 training the \u2018backup policy\u2019 with RL to minimize cost. The analysis for Theorem 6.5 then follows   \n343 by showing that the system can be recovered to a non-critical state after entering a critical but not   \n344 irrecoverable state. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "345 7 Empirical Evaluation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "346 We implemented two separate realizations of Algorithm 1, the first adapted to tabular environments   \n347 which implements both exact or statistical model checking over the learned transition probabilities, the   \n348 second is adapted to (visual) deep RL, making use of world models [32, 33], specifically DreamerV3   \n349 [34], to learn a latent dynamics model for model checking and policy optimization.   \n350 Tabular RL. We conduct experiments on a simple \u2018colour\u2019 grid  \n351 world environment, with regular safety properties of increasing   \n352 difficulty. In short, the goal is to navigate from a starting state   \n353 to a goal position as frequently as possible, while respecting a   \n354 given regular safety property during training. The environment is   \n355 stochastic \u2013 with some probability $p$ the agent\u2019s action is ignored   \n356 and another action is chosen uniformly instead. For smaller $p$ val  \n357 ues the environment becomes more deterministic and the safety property typically becomes easier to   \n358 satisfy with higher probability, we refer the reader to Appendix D.1 for more details. Table 1 outlines   \n359 the three safety properties used for our environments. We use PCTL-like notation to describe the   \n360 safety properties, although strictly speaking (2) and (3) are actually $\\mathrm{PCTL}^{*}$ path formula. Regardless   \n361 of this slight technical detail, properties (1)-(3) are valid regular safety properties, as we can come up   \n362 with a DFA that accepts the bad prefixes for them.   \n363 We compare our approach to Q-learning (without any penalties), and Q-learning on the product   \n364 state space, with penalties provided by the cost function (Defn. 4.3) and trained with counterfactual   \n365 experiences [43]. In all cases, by separating reward and safety into two distinct policies, we are able   \n366 to effectively trade-off the two objectives. Q-learning simply finds the best policy ignoring the costs,   \n367 and Q-learning with penalties is able to find a safe policy, but struggles to meaningfully balance both   \n368 objectives (see Fig. 2). Hyperparameter settings for all experiments are detailed in Appendix E. In   \n369 addition, we provide an extensive series of ablation studies in Appendix F for these experiments. For   \n370 example, we show that we don\u2019t loose much by using Monte Carlo model checking as opposed to   \n371 exact model checking with the \u2018true\u2019 probabilities. We also show that tuning the cost coefficient $C$   \n372 offers no meaningful way to trade-off reward and the probability of constraint satisfaction.   \n373 Deep RL. We deploy our version of Algo  \n374 rithm 1 built on DreamerV3 [34] on Atari   \n375 Seaquest, provided as part of the Arcade Learn  \n376 ing Environment (ALE)[10, 50]. We experi  \n377 ment with two different regular safety proper  \n378 ties: (1) $(\\square\\neg s u r f a c e\\to\\square(\\bar{s}u r f a c e\\to\\dot{d i}\\bar{\\nu}e r)$ ) $\\wedge$   \n379 $(\\boxed{\\Omega\\lnot o u t\\lnot o f\\lnot o x y g e n})\\land(\\boxed{\\Omega\\lnot h i t})$ and (2) \u25a1diver \u2227   \n380 $\\neg s u r f a c e\\to\\diamondsuit^{\\leq30}$ surface. We compare our ap  \n381 proach to the base DreamerV3 algorithm and   \n382 a version of DreamerV3 that implements the   \n383 augmented Lagrangian penalty framework, sim  \n384 ilarly to [7, 41], for additional details see Ap  \n385 pendix B.1.   \n386 Again our approach is able to effectively trade  \n387 off both objectives, while (base) DreamerV3 ig  \n388 nores the cost, the Lagrangian approach appears   \n389 to learn a safe policy that is not always efficient   \n390 in terms of reward (see Fig. 3). We refer the   \n391 reader to Appendix D.2 for more details of the   \n392 environment and an extended discussion.   \n393 Separating Reward and Safety. The separa  \n394 tion of reward and safety objectives into two dis  \n395 tinct policies has been demonstrated as an effec  \n396 tive strategy towards safety-aware decision mak  \n397 ing [3, 30, 46, 63], in many cases the safety ob  \n398 jective is simpler and can be more quickly learnt   \n399 [46]. In our experiments it is clear that when   \n400 the system enters a critical state, the \u2018backup   \n401 policy\u2019 is able to efficiently guide the system   \n402 back to a non-critical state where the task policy   \n403 can continue collecting reward. However, there   \n404 is evidence that the complete separation of poli  \n405 cies is not always appropriate [31] and penalties   \n406 or a slight coupling of the policies is required   \n407 to stop the \u2018task\u2019 and \u2018backup policy\u2019 fighting   \n408 for control of the system. Furthermore, by separating reward and safety, we typically loose any   \n409 asymptotic convergence guarantees, similar to the situation faced for hierarchical RL [61], although   \n410 there has been recent work to develop convergence guarantees for shielding [75]. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "table", "img_path": "PnSTlFUfcd/tmp/929e0a0333ab86e5a22d30e111619f62f899b72bf14227a4054d848a570ba2e3.jpg", "table_caption": ["Table 1: Safety properties "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "image", "img_path": "PnSTlFUfcd/tmp/96e2448c99817567edca73febe705dfe030a0a10e65fad068a6c252e4ceff5d7.jpg", "img_caption": ["Figure 2: Episode reward and cost for tabular RL \u2018colour\u2019 gridworld environment. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "image", "img_path": "PnSTlFUfcd/tmp/f8b5f5e1cb45eb77ed9970ee563ffeefc27114310995b218eea485f130270b1d.jpg", "img_caption": ["Figure 3: Episode reward and violation rate for deep RL Atari Seaquest. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "411 8 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "412 In this paper we propose a shielding meta-algorithm for the runtime verification of regular safety   \n413 properties, given as a probabilistic constraint on the system. We provide a thorough theoretical   \n414 examination of the problem and develop probabilistic safety guarantees for the meta-algorithm,   \n415 which hold under reasonable assumptions. Empirically, we demonstrate that shielding is able to   \n416 effectively balance both reward and safety, in both the tabular and deep RL setting. A more thorough   \n417 theoretical and empirical examinations of the conditions for when shielding is appropriate would be   \n418 an interesting direction for future work. ", "page_idx": 8}, {"type": "text", "text": "419 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "420 [1] Pieter Abbeel and Andrew Y Ng. 2005. Exploration and apprenticeship learning in reinforcement   \n421 learning. In Proceedings of the 22nd international conference on Machine learning. 1\u20138.   \n422 [2] Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. 2017. Constrained policy opti  \n423 mization. In International conference on machine learning. PMLR, 22\u201331.   \n424 [3] Mohammed Alshiekh, Roderick Bloem, R\u00fcdiger Ehlers, Bettina K\u00f6nighofer, Scott Niekum,   \n425 and Ufuk Topcu. 2018. Safe reinforcement learning via shielding. In Proceedings of the AAAI   \n426 Conference on Artificial Intelligence, Vol. 32.   \n427 [4] Eitan Altman. 1999. Constrained Markov decision processes: stochastic modeling. Routledge.   \n428 [5] Aaron D Ames, Samuel Coogan, Magnus Egerstedt, Gennaro Notomista, Koushil Sreenath,   \n429 and Paulo Tabuada. 2019. Control barrier functions: Theory and applications. In 2019 18th   \n430 European control conference (ECC). IEEE, 3420\u20133431.   \n431 [6] Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Man\u00e9.   \n432 2016. Concrete problems in AI safety. arXiv preprint arXiv:1606.06565 (2016).   \n433 [7] Yarden As, Ilnura Usmanova, Sebastian Curi, and Andreas Krause. 2022. Constrained policy   \n434 optimization via bayesian world models. arXiv preprint arXiv:2201.09802 (2022).   \n435 [8] Fahiem Bacchus, Craig Boutilier, and Adam Grove. 1996. Rewarding behaviors. In Proceedings   \n436 of the National Conference on Artificial Intelligence. 1160\u20131167.   \n437 [9] Christel Baier and Joost-Pieter Katoen. 2008. Principles of model checking. MIT press.   \n438 [10] M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. 2013. The Arcade Learning Environ  \n439 ment: An Evaluation Platform for General Agents. Journal of Artificial Intelligence Research   \n440 47 (jun 2013), 253\u2013279.   \n441 [11] Felix Berkenkamp, Matteo Turchetta, Angela Schoellig, and Andreas Krause. 2017. Safe   \n442 model-based reinforcement learning with stability guarantees. Advances in neural information   \n443 processing systems 30 (2017).   \n444 [12] James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal   \n445 Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and   \n446 Qiao Zhang. 2018. JAX: composable transformations of Python+NumPy programs. http:   \n447 //github.com/google/jax   \n448 [13] Ronen I Brafman, Giuseppe De Giacomo, et al. 2019. Regular Decision Processes: A Model   \n449 for Non-Markovian Domains.. In IJCAI. 5516\u20135522.   \n450 [14] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang,   \n451 and Wojciech Zaremba. 2016. Openai gym. arXiv preprint arXiv:1606.01540 (2016).   \n452 [15] Lukas Brunke, Melissa Greeff, Adam W Hall, Zhaocong Yuan, Siqi Zhou, Jacopo Panerati,   \n453 and Angela P Schoellig. 2022. Safe learning in robotics: From learning-based control to safe   \n454 reinforcement learning. Annual Review of Control, Robotics, and Autonomous Systems 5 (2022),   \n455 411\u2013444.   \n456 [16] Emma Brunskill, Bethany R Leffler, Lihong Li, Michael L Littman, and Nicholas Roy. 2009.   \n457 Provably efficient learning with typed parametric models. (2009).   \n458 [17] Mingyu Cai, Shaoping Xiao, Zhijun Li, and Zhen Kan. 2021. Optimal probabilistic motion   \n459 planning with potential infeasible LTL constraints. IEEE transactions on automatic control 68,   \n460 1 (2021), 301\u2013316.   \n461 [18] Weiqin Chen, Dharmashankar Subramanian, and Santiago Paternain. 2024. Probabilistic   \n462 constraint for safety-critical reinforcement learning. IEEE Trans. Automat. Control (2024).   \n463 [19] Richard Cheng, G\u00e1bor Orosz, Richard M Murray, and Joel W Burdick. 2019. End-to-end safe   \n464 reinforcement learning through barrier functions for safety-critical continuous control tasks. In   \n465 Proceedings of the AAAI conference on artificial intelligence, Vol. 33. 3387\u20133395.   \n466 [20] Yinlam Chow, Mohammad Ghavamzadeh, Lucas Janson, and Marco Pavone. 2018. Risk  \n467 constrained reinforcement learning with percentile risk criteria. Journal of Machine Learning   \n468 Research 18, 167 (2018), 1\u201351.   \n469 [21] Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. 2018. Deep rein  \n470 forcement learning in a handful of trials using probabilistic dynamics models. Advances in   \n471 neural information processing systems 31 (2018).   \n472 [22] Roberto Cipollone, Anders Jonsson, Alessandro Ronca, and Mohammad Sadegh Talebi. 2024.   \n473 Provably Efficient Offilne Reinforcement Learning in Regular Decision Processes. Advances in   \n474 Neural Information Processing Systems 36 (2024).   \n475 [23] Gal Dalal, Krishnamurthy Dvijotham, Matej Vecerik, Todd Hester, Cosmin Paduraru, and Yuval   \n476 Tassa. 2018. Safe exploration in continuous action spaces. arXiv preprint arXiv:1801.08757   \n477 (2018).   \n478 [24] Giuseppe De Giacomo, Luca Iocchi, Marco Favorito, and Fabio Patrizi. 2020. Restraining   \n479 bolts for reinforcement learning agents. In Proceedings of the AAAI Conference on Artificial   \n480 Intelligence, Vol. 34. 13659\u201313662.   \n481 [25] Marc Deisenroth and Carl E Rasmussen. 2011. PILCO: A model-based and data-efficient   \n482 approach to policy search. In Proceedings of the 28th International Conference on machine   \n483 learning (ICML-11). 465\u2013472.   \n484 [26] Gabriel Dulac-Arnold, Daniel Mankowitz, and Todd Hester. 2019. Challenges of real-world   \n485 reinforcement learning. arXiv preprint arXiv:1904.12901 (2019).   \n486 [27] Jie Fu and Ufuk Topcu. 2014. Probably approximately correct MDP learning and control with   \n487 temporal logic constraints. arXiv preprint arXiv:1404.7073 (2014).   \n488 [28] Javier Garc\u0131a and Fernando Fern\u00e1ndez. 2015. A comprehensive survey on safe reinforcement   \n489 learning. Journal of Machine Learning Research 16, 1 (2015), 1437\u20131480.   \n490 [29] M Giacobbe, Mohammadhosein Hasanbeig, Daniel Kroening, and Hjalmar Wijk. 2021. Shield  \n491 ing atari games with bounded prescience. In Proceedings of the International Joint Conference   \n492 on Autonomous Agents and Multiagent Systems, AAMAS.   \n493 [30] Alexander W Goodall and Francesco Belardinelli. 2023. Approximate Model-Based Shielding   \n494 for Safe Reinforcement Learning. arXiv preprint arXiv:2308.00707 (2023).   \n495 [31] Alexander W Goodall and Francesco Belardinelli. 2024. Leveraging Approximate Model-based   \n496 Shielding for Probabilistic Safety Guarantees in Continuous Environments. arXiv preprint   \n497 arXiv:2402.00816 (2024).   \n498 [32] David Ha and J\u00fcrgen Schmidhuber. 2018. Recurrent World Models Facilitate Policy Evo  \n499 lution. In Advances in Neural Information Processing Systems, S. Bengio, H. Wallach,   \n500 H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (Eds.), Vol. 31. Curran As  \n501 sociates, Inc. https://proceedings.neurips.cc/paper_files/paper/2018/file/   \n502 2de5d16682c3c35007e4e92982f1a2ba-Paper.pdf   \n503 [33] Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and   \n504 James Davidson. 2019. Learning latent dynamics for planning from pixels. In International   \n505 conference on machine learning. PMLR, 2555\u20132565.   \n506 [34] Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. 2023. Mastering diverse   \n507 domains through world models. arXiv preprint arXiv:2301.04104 (2023).   \n508 [35] Alexander Hans, Daniel Schneegass, Anton Sch\u00e4fer, and Steffen Udluft. 2008. Safe Exploration   \n509 for Reinforcement Learning. 143\u2013148.   \n510 [36] Mohammadhosein Hasanbeig, Alessandro Abate, and Daniel Kroening. 2018. Logically  \n511 constrained reinforcement learning. arXiv preprint arXiv:1801.08099 (2018).   \n512 [37] Mohammadhosein Hasanbeig, Alessandro Abate, and Daniel Kroening. 2020. Cautious rein  \n513 forcement learning with logical constraints. arXiv preprint arXiv:2002.12156 (2020).   \n514 [38] Mohammadhosein Hasanbeig, Daniel Kroening, and Alessandro Abate. 2020. Deep reinforce  \n515 ment learning with temporal logics. In Formal Modeling and Analysis of Timed Systems: 18th   \n516 International Conference, FORMATS 2020, Vienna, Austria, September 1\u20133, 2020, Proceedings   \n517 18. Springer, 1\u201322.   \n518 [39] Chloe He, Borja G Le\u00f3n, and Francesco Belardinelli. [n.d.]. Do androids dream of electric   \n519 fences? Safety-aware reinforcement learning with latent shielding. CEUR Workshop Proceed  \n520 ings. https://ceur-ws.org/Vol-3087/paper_50.pdf   \n521 [40] Wassily Hoeffding. 1963. Probability Inequalities for Sums of Bounded Random Variables. J.   \n522 Amer. Statist. Assoc. 58, 301 (1963), 13\u201330.   \n523 [41] Weidong Huang, Jiaming Ji, Borong Zhang, Chunhe Xia, and Yaodong Yang. 2023. Safe Dream  \n524 erV3: Safe Reinforcement Learning with World Models. arXiv preprint arXiv:2307.07176   \n525 (2023).   \n526 [42] Rodrigo Toro Icarte, Toryn Klassen, Richard Valenzano, and Sheila McIlraith. 2018. Using   \n527 reward machines for high-level task specification and decomposition in reinforcement learning.   \n528 In International Conference on Machine Learning. PMLR, 2107\u20132116.   \n529 [43] Rodrigo Toro Icarte, Toryn Q Klassen, Richard Valenzano, and Sheila A McIlraith. 2022.   \n530 Reward machines: Exploiting reward function structure in reinforcement learning. Journal of   \n531 Artificial Intelligence Research 73 (2022), 173\u2013208.   \n532 [44] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. 2019. When to trust your model:   \n533 Model-based policy optimization. Advances in neural information processing systems 32   \n534 (2019).   \n535 [45] Nils Jansen, Bettina K\u00f6nighofer, Sebastian Junges, Alex Serban, and Roderick Bloem. 2020.   \n536 Safe reinforcement learning using probabilistic shields. In 31st International Conference on   \n537 Concurrency Theory (CONCUR 2020). Schloss-Dagstuhl-Leibniz Zentrum f\u00fcr Informatik.   \n538 [46] Nils Jansen, Bettina K\u00f6nighofer, Sebastian Junges, Alexandru C Serban, and Roderick Bloem.   \n539 2018. Safe reinforcement learning via probabilistic shields. arXiv preprint arXiv:1807.06096   \n540 (2018).   \n541 [47] Sham Kakade, Michael J Kearns, and John Langford. 2003. Exploration in metric state spaces. In   \n542 Proceedings of the 20th International Conference on Machine Learning (ICML-03). 306\u2013312.   \n543 [48] Michael Kearns and Satinder Singh. 2002. Near-optimal reinforcement learning in polynomial   \n544 time. Machine learning 49 (2002), 209\u2013232.   \n545 [49] Qingkai Liang, Fanyu Que, and Eytan Modiano. 2018. Accelerated primal-dual policy opti  \n546 mization for safe reinforcement learning. arXiv preprint arXiv:1802.06480 (2018).   \n547 [50] Marlos C. Machado, Marc G. Bellemare, Erik Talvitie, Joel Veness, Matthew J. Hausknecht, and   \n548 Michael Bowling. 2018. Revisiting the Arcade Learning Environment: Evaluation Protocols   \n549 and Open Problems for General Agents. Journal of Artificial Intelligence Research 61 (2018),   \n550 523\u2013562.   \n551 [51] Sultan Javed Majeed, Marcus Hutter, et al. 2018. On Q-learning Convergence for Non-Markov   \n552 Decision Processes.. In IJCAI, Vol. 18. 2546\u20132552.   \n553 [52] Andreas Maurer and Massimiliano Pontil. 2009. Empirical bernstein bounds and sample   \n554 variance penalization. arXiv preprint arXiv:0907.3740 (2009).   \n555 [53] Stephen McIlvanna, Nhat Nguyen Minh, Yuzhu Sun, Mien Van, and Wasif Naeem. 2022.   \n556 Reinforcement learning-enhanced control barrier functions for robot manipulators. arXiv   \n557 preprint arXiv:2211.11391 (2022).   \n558 [54] Rohan Mitta, Hosein Hasanbeig, Jun Wang, Daniel Kroening, Yiannis Kantaros, and Alessandro   \n559 Abate. 2024. Safeguarded Progress in Reinforcement Learning: Safe Bayesian Exploration   \n560 for Control Policy Synthesis. In Proceedings of the AAAI Conference on Artificial Intelligence,   \n561 Vol. 38. 21412\u201321419.   \n562 [55] Andrew Y Ng, Daishi Harada, and Stuart Russell. 1999. Policy invariance under reward   \n563 transformations: Theory and application to reward shaping. In Icml, Vol. 99. 278\u2013287.   \n564 [56] Santiago Paternain, Miguel Calvo-Fullana, Luiz FO Chamon, and Alejandro Ribeiro. 2022. Safe   \n565 policies for reinforcement learning via primal-dual methods. IEEE Trans. Automat. Control 68,   \n566 3 (2022), 1321\u20131336.   \n567 [57] Aravind Rajeswaran, Igor Mordatch, and Vikash Kumar. 2020. A game theoretic framework for   \n568 model based reinforcement learning. In International conference on machine learning. PMLR,   \n569 7953\u20137963.   \n570 [58] Alex Ray, Joshua Achiam, and Dario Amodei. 2019. Benchmarking safe exploration in deep   \n571 reinforcement learning. arXiv preprint arXiv:1910.01708 7, 1 (2019), 2.   \n572 [59] Alessandro Ronca and Giuseppe De Giacomo. 2021. Efficient PAC reinforcement learning in   \n573 regular decision processes. arXiv preprint arXiv:2105.06784 (2021).   \n574 [60] Yanan Sui, Alkis Gotovos, Joel Burdick, and Andreas Krause. 2015. Safe exploration for   \n575 optimization with Gaussian processes. In International conference on machine learning. PMLR,   \n576 997\u20131005.   \n577 [61] Richard S Sutton, Doina Precup, and Satinder Singh. 1999. Between MDPs and semi-MDPs: A   \n578 framework for temporal abstraction in reinforcement learning. Artificial intelligence 112, 1-2   \n579 (1999), 181\u2013211.   \n580 [62] Chen Tessler, Daniel J Mankowitz, and Shie Mannor. 2018. Reward constrained policy opti  \n581 mization. arXiv preprint arXiv:1805.11074 (2018).   \n582 [63] Brijen Thananjeyan, Ashwin Balakrishna, Suraj Nair, Michael Luo, Krishnan Srinivasan, Minho   \n583 Hwang, Joseph E Gonzalez, Julian Ibarz, Chelsea Finn, and Ken Goldberg. 2021. Recovery   \n584 rl: Safe reinforcement learning with learned recovery zones. IEEE Robotics and Automation   \n585 Letters 6, 3 (2021), 4915\u20134922.   \n586 [64] Garrett Thomas, Yuping Luo, and Tengyu Ma. 2021. Safe reinforcement learning by imagining   \n587 the near future. Advances in Neural Information Processing Systems 34 (2021), 13859\u201313869.   \n588 [65] Rodrigo Toro Icarte, Ethan Waldie, Toryn Klassen, Rick Valenzano, Margarita Castro, and   \n589 Sheila McIlraith. 2019. Learning reward machines for partially observable reinforcement   \n590 learning. Advances in neural information processing systems 32 (2019).   \n591 [66] Cameron Voloshin, Hoang Le, Swarat Chaudhuri, and Yisong Yue. 2022. Policy optimization   \n592 with linear temporal logic constraints. Advances in Neural Information Processing Systems 35   \n593 (2022), 17690\u201317702.   \n594 [67] Kim P Wabersich and Melanie N Zeilinger. 2018. Linear model predictive safety certification   \n595 for learning-based control. In 2018 IEEE Conference on Decision and Control (CDC). IEEE,   \n596 7130\u20137135.   \n597 [68] Kim Peter Wabersich and Melanie N Zeilinger. 2021. A predictive safety fliter for learning-based   \n598 control of constrained nonlinear dynamical systems. Automatica 129 (2021), 109597.   \n599 [69] Akifumi Wachi, Yanan Sui, Yisong Yue, and Masahiro Ono. 2018. Safe exploration and opti  \n600 mization of constrained mdps using gaussian processes. In Proceedings of the AAAI Conference   \n601 on Artificial Intelligence, Vol. 32.   \n602 [70] Christopher KI Williams and Carl Edward Rasmussen. 2006. Gaussian processes for machine   \n603 learning. Vol. 2. MIT press Cambridge, MA.   \n604 [71] Eric M Wolff, Ufuk Topcu, and Richard M Murray. 2012. Robust control of uncertain Markov   \n605 decision processes with temporal logic specifications. In 2012 IEEE 51st IEEE Conference on   \n606 decision and control (CDC). IEEE, 3372\u20133379.   \n607 [72] Jorge Nocedal Stephen J Wright. 2006. Numerical optimization.   \n608 [73] Wenli Xiao, Yiwei Lyu, and John Dolan. 2023. Model-based Dynamic Shielding for Safe   \n609 and Efficient Multi-agent Reinforcement Learning. In Proceedings of the 2023 International   \n610 Conference on Autonomous Agents and Multiagent Systems. 1587\u20131596.   \n611 [74] Tsung-Yen Yang, Justinian Rosca, Karthik Narasimhan, and Peter J Ramadge. 2020. Projection  \n612 based constrained policy optimization. arXiv preprint arXiv:2010.03152 (2020).   \n613 [75] Wen-Chi Yang, Giuseppe Marra, Gavin Rens, and Luc De Raedt. 2023. Safe reinforcement   \n614 learning via probabilistic logic shields. arXiv preprint arXiv:2303.03226 (2023). ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Algorithm 2 Exact Model Checking [9]   \nInput: model checking parameters $(p,H)$ , current state $\\langle s,q\\rangle$ , current action $a$ , product MC   \n$\\mathcal{M}_{\\pi}\\otimes\\mathcal{D}=(\\mathcal{S}\\times\\mathcal{Q},\\mathcal{P}^{\\prime},\\mathcal{P}_{0}^{\\prime},\\{a c c e p t\\},L^{\\prime})$   \nOutput: true if $\\operatorname*{Pr}(\\langle s,q\\rangle\\models\\dot{\\diamond}^{\\leq H}a c c e p t)\\leq p_{1}$   \nInitialize zero vector $\\mathbf{x}^{(0)}\\leftarrow\\mathbf{0}$ with size $\\vert S\\vert\\times\\vert\\mathcal{Q}\\vert$   \nInitialize probability matrix $\\mathbf{A}\\gets(\\mathcal{P}^{\\prime}(s,\\dot{t}))_{s,t\\notin a c c e p t}$ (ignoring accepting states)   \nInitialize probability vector $\\mathbf{b}\\gets(\\mathcal{P}^{\\prime}(s,a c c e p t))_{s\\notin a c c e p t}$ (going to accepting states)   \n// Iterate over the model checking horizon   \nfor $i=1,\\dots,H$ do Compute $\\mathbf{\\dot{x}}^{(i)}=\\mathbf{A}\\mathbf{x}^{(i-1)}+\\mathbf{b}$   \n// Get the corresponding probability   \nLet X \u2190x\u27e8s,q\u27e9   \nIf $X<p$ return true else return false ", "page_idx": 14}, {"type": "text", "text": "Algorithm 3 Monte-Carlo Model Checking ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Input: model checking parameters $(\\epsilon,\\,\\delta,\\,p,\\,H)$ , current state $\\langle s,q\\rangle$ , current action $a$ , policy $\\pi$ ,   \nlabelling function $L$ , DFA $\\mathcal{D}=(\\mathcal{Q},\\Sigma,\\Delta,\\mathcal{Q}_{0},\\mathcal{F})$ and (approximate) transition probabilities $\\mathcal{P}$   \nOutput: true if $\\operatorname*{Pr}(\\langle s,q\\rangle\\models\\diamond\\llcorner t_{a c c e p t})\\leq p_{1}$   \nChoose $m\\geq2/(\\epsilon^{2})\\log(2/\\delta)$   \nfor $i=1,\\hdots,m$ do Set $s_{0}\\leftarrow s$ , $q_{0}\\leftarrow q$ and $a_{0}\\leftarrow a$ // Sample a path through the model for $j=1,\\dots,H$ do Sample next state $s_{j}\\sim\\mathcal{P}(\\cdot\\mid s_{j-1},a_{j-1}),$ , Compute $q_{j}\\leftarrow\\Delta(\\bar{q}_{j-1},L(s_{j}))$ , Sample action $a_{j}\\sim\\pi(\\cdot\\mid s_{j})$ // Check if the path is accepting Let $X_{i}\\leftarrow1\\left[q_{H}\\in\\mathcal{F}\\right]$   \n// Construct probability estimate   \nLet $\\begin{array}{r}{\\widetilde{X}\\leftarrow\\frac{1}{m}\\sum_{i=1}^{m}X_{i}}\\end{array}$   \nIf $\\widetilde{X}<p-\\epsilon$ return true else return false ", "page_idx": 14}, {"type": "text", "text": "Algorithm 5 DreamerV3 [34] with Shielding (Regular Safety Property) ", "page_idx": 15}, {"type": "text", "text": "Initialize: replay buffer $D$ with $S$ random episodes, world model parameters $\\theta$ , \u2018task policy\u2019 \u03c0task and \u2018backup policy\u2019 $\\pi_{s a f e}$ randomly. ", "page_idx": 15}, {"type": "text", "text": "for each episode do Observe $O_{0}$ , $L\\!\\left(s_{0}\\right)$ and $q_{0}\\gets\\Delta(\\mathcal{Q}_{0},L(s_{0}))$ for $\\mathfrak{t}=1$ , ..., T do Sample action $a\\sim\\pi_{t a s k}$ from the task policy // Estimate the reachability probability using the world model $p_{\\theta}$ if $\\operatorname*{Pr}(\\langle s,q\\rangle\\models\\diamond\\llcorner t_{a c c e p t})\\mathring{\\leq}p_{1}$ then Use proposed action $a_{t}\\gets a$ else // Override action $a_{t}\\sim\\pi_{s a f e}$ Play action $a_{t}$ and observe $o_{t+1}$ , $L(s_{t+1})$ and $r_{t}$ Compute $q_{t+1}\\leftarrow\\Delta(q_{t},L(s_{t+1}))$ , Compute cost $c_{t}\\gets1[q_{t+1}\\in\\mathcal{F}]$ Append $(o_{t},a_{t},r_{t},c_{t},o_{t+1})$ to the replay buffer $D$ if update then // World model learning Sample a batch $B$ of transition sequences $\\left\\{\\left(o_{t^{\\prime}},a_{t^{\\prime}},r_{t^{\\prime}},c_{t^{\\prime}},o_{t^{\\prime}+1}\\right)\\right\\}\\sim\\mathcal{D}$ . Update the world model parameters $\\theta$ with maximum likelihood. // Task policy optimization \u2018Imagine\u2019 sequences $\\left\\{\\hat{o}_{t^{\\prime}:t^{\\prime}+H},\\hat{r}_{t^{\\prime}:t^{\\prime}+H},\\hat{c}_{t^{\\prime}:t^{\\prime}+H}\\right\\}$ with the \u2018task policy\u2019 $\\pi_{t a s k}$ Update the \u2018task policy\u2019 $\\pi_{t a s k}$ with RL (to maximize reward). Update the corresponding value critics with maximum likelihood // Backup policy optimization \u2018Imagine\u2019 sequences $\\left\\{\\hat{o}_{t^{\\prime}:t^{\\prime}+H},\\hat{r}_{t^{\\prime}:t^{\\prime}+H},\\hat{c}_{t^{\\prime}:t^{\\prime}+H}\\right\\}$ with the \u2018backup policy\u2019 $\\pi_{s a f e}$ Update the \u2018backup policy\u2019 $\\pi_{s a f e}$ with RL (to minimize cost) Update the corresponding value critics with maximum likelihood ", "page_idx": 15}, {"type": "text", "text": "616 B Technical Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "617 B.1 Augmented Lagrangian ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "618 We first define the following objective functions, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{J_{\\mathcal{R}}(\\pi)}&{=}&{\\mathbb{E}_{\\pi}\\left[\\displaystyle\\sum_{t=0}^{T}\\mathcal{R}(s_{t},a_{t})\\right]}\\\\ {J_{\\mathcal{C}}(\\pi)}&{=}&{\\mathbb{E}_{\\pi}\\left[\\displaystyle\\sum_{t=0}^{T}\\mathcal{C}(s_{t},a_{t})\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "619 The augmented Lagrangian [72] is an adaptive penalty-based technique for the following constrained   \n620 optimization problem, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\pi}J_{\\mathcal{R}}(\\pi)\\quad\\mathrm{subject\\;to}\\quad J_{\\mathcal{C}}(\\pi)\\leq d\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "621 where $d$ is some cost threshold. The corresponding Lagrangian is given by, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\pi}\\operatorname*{min}_{\\lambda\\geq0}\\left[J_{\\mathcal{R}}(\\pi)-\\lambda\\left(J_{\\mathcal{C}}(\\pi)-d\\right)\\right]=\\operatorname*{max}_{\\pi}\\left\\{\\!\\!\\!J_{\\mathcal{R}}(\\pi)\\!\\!\\!\\right.\\quad\\mathrm{if}\\ J_{\\mathcal{C}}(\\pi)<d\\!\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "622 The LHS is an equivalent form for the constrained optimization problem (RHS), since if $\\pi$ is feasible,   \n623 i.e. $J_{\\mathcal{C}}(\\pi)<d$ then the maximum value for $\\lambda$ is $\\lambda=0$ . If $\\pi$ is not feasible then $\\lambda$ can be arbitrarily   \n624 large to solve this equation. Unfortunately this form of the objective function is non-smooth when   \n625 moving from feasible to infeasible policies, thus we introduce a proximal relaxation of the augmented ", "page_idx": 15}, {"type": "text", "text": "626 Lagrangian [72], ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\pi}\\operatorname*{min}_{\\lambda\\geq0}\\left[J_{\\mathcal{R}}(\\pi)-\\lambda\\left(J_{\\mathcal{C}}(\\pi)-d\\right)+\\frac{1}{\\mu_{k}}(\\lambda-\\lambda_{k})^{2}\\right]\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "627 where $\\mu_{k}$ is a non-decreasing penalty multiplier dependent on the gradient step $k$ . The new term   \n628 that has been introduced here encourages the $\\lambda$ to stay close to the previous value $\\lambda_{k}$ , resulting in a   \n629 smooth and differentiable function. The derivative w.r.t $\\lambda$ gives us the following gradient update step, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\lambda_{k+1}={\\binom{\\lambda_{k}+\\mu_{k}(J_{\\mathcal{C}}(\\pi)-d)}{0}}\\quad\\mathrm{if~}\\lambda_{k}+\\mu_{k}(J_{\\mathcal{C}}(\\pi)-d)\\geq0\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "630 At each gradient step, the penalty multiplier $\\mu_{k}$ is updated in a non-decreasing way by using some   \n631 small fixed (power) parameter $\\sigma$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mu_{k+1}=\\operatorname*{max}\\{(\\mu_{k})^{1+\\sigma},1\\}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "632 The policy $\\pi$ is then updated by taking gradient steps of the following unconstrained objective, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\tilde{J}(\\pi,\\lambda_{k},\\mu_{k})=J_{\\mathcal{R}}(\\pi)-\\Psi_{\\mathcal{C}}(\\pi,\\lambda_{k},\\mu_{k})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "633 where, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\Psi_{\\cal C}(\\pi,\\lambda_{k},\\mu_{k})=\\left\\{\\!\\!\\!\\begin{array}{l l}{{{\\lambda_{k}(J_{\\mathcal{C}}(\\pi)-d)}+{\\frac{\\mu_{k}}{2}}(J_{\\mathcal{C}}(\\pi)-d)^{2}}}&{{\\mathrm{if}\\;\\lambda_{k}+\\mu_{k}(J_{\\mathcal{C}}(\\pi)-d)\\geq0}}\\\\ {{-{\\frac{(\\lambda_{k})^{2}}{2\\mu_{k}}}}}&{{\\mathrm{otherwise}}}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "634 C Technical Proofs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "635 C.1 Proof of Proposition 3.4 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "636 Proposition 3.4 (restated) (Satisfaction probability for $P_{s a f e}^{H}$ ). Let $\\mathcal{M}$ and $\\mathcal{D}$ be the MDP and   \n637 DFA from before (Defn. 3.3). For a path $\\rho\\ \\in\\ S^{\\omega}$ in the Markov chain, let $t r a c e_{H}(\\rho)\\;\\,=\\;\\;$   \n638 $L(\\rho[0]),L(\\rho[1])\\ldots,L(\\rho[H])$ be the corresponding finite word over $\\Sigma=P o w(A P)$ . For a given   \n639 state $s\\in S$ the finite horizon satisfaction probability for $P_{s a f e}$ is defined as follows, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{Pr}^{\\mathcal{M}}(s\\models P_{s a f e}^{H}):=\\mathrm{Pr}^{\\mathcal{M}}(\\rho\\in S^{\\omega}\\ |\\ \\rho[0]=s,t r a c e_{H}(\\rho)\\not\\in\\mathcal{L}(\\mathcal{D}))\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "640 where $H\\in\\mathbb{Z}_{+}$ is some fixed model checking horizon. Similar to before, we show that the finite   \n641 horizon satisfaction probability can be written as the following bounded reachability probability, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{Pr}^{\\mathcal{M}}(s\\vert=P_{s a f e}^{H})=\\mathrm{Pr}^{\\mathcal{M}\\otimes\\mathcal{D}}(\\left\\langle s,q_{s}\\right\\rangle\\,\\big\\vert\\not\\varphi\\leqslant\\mathcal{S}^{\\leq H}a c c e p t)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "642 where $q_{s}=\\Delta(\\mathcal{Q}_{0},L(s))$ is as before and $\\diamondsuit^{\\leq H}$ accept is the corresponding step-bounded PCTL path   \n643 formula that reads, \u2018eventually accept in $H$ timesteps\u2019.   \n644 Proof. Let $P_{s a f e}$ be a regular safety property and let $\\mathcal{D}=(\\mathcal{Q},\\Sigma,\\Delta,\\mathcal{Q}_{0},\\mathcal{F})$ be the DFA such that   \n645 $\\mathcal{L}(\\mathcal{D})\\,=\\,B a d\\bar{P}r e f(P_{s a f e})$ . We provide a formal definition for $P_{s a f e}$ and the corresponding finite   \n646 horizon property $P_{s a f e}^{H}$ , respectively: ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P_{s a f e}=\\{w\\in\\Sigma^{\\omega}\\mid\\forall w_{p r e f}\\in\\Sigma^{\\omega}s.t.\\;w_{p r e f}\\preceq w,w_{p r e f}\\notin\\mathcal{L}(\\mathcal{D})\\}}\\\\ &{P_{s a f e}^{H}=\\{w\\in\\Sigma^{\\omega}\\mid\\forall w_{p r e f}\\in\\Sigma^{\\omega}s.t.\\;w_{p r e f}\\preceq w\\land\\lvert w_{p r e f}\\rvert\\leq H+1,w_{p r e f}\\notin\\mathcal{L}(\\mathcal{D})\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "647 Let $\\mathcal{M}=(\\mathcal{S},\\mathcal{P},\\mathcal{P}_{0},A P,L)$ be a Markov chain and consider the product Markov chain $\\mathcal{M}\\otimes\\mathcal{D}$   \n648 from Defn. 3.2. For any path $\\rho=s_{0},s_{1},s_{2},\\ldots$ , there exists a unique run $q_{0},q_{1},q_{2},\\ldots$ for the trace   \n649 $t r a c e(\\rho)=L(s_{0}),L(\\dot{s_{1}}),L(\\dot{s_{2}})\\ldots,$ and denote, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\rho^{+}=\\langle s_{0},q_{0}\\rangle,\\langle s_{1},q_{1}\\rangle,\\langle s_{2},q_{2}\\rangle\\dots\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "650 where start state is $\\langle s_{0},\\Delta(\\mathcal{Q}_{0},L(s_{0}))\\rangle$ . Before we deal with probabilities let\u2019s just consider a   \n651 fixed path $\\rho\\,\\in\\,S^{\\omega}$ , the finite trace $i r a c e_{H}(\\rho)\\;=\\;L(\\rho[0]),L(\\rho[1])\\ldots,L(\\rho[H])$ , the unique run   \n652 $q_{0},q_{1},q_{2},\\ldots,q_{H}$ and the path $\\rho^{+}\\in\\Sigma^{\\omega}\\times\\mathcal{Q}^{\\omega}$ in the product Markov chain. We prove the following   \n653 statement, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\rho\\models P_{s a f e}^{H}\\quad{\\mathrm{if~and~only~if}}\\quad\\rho^{+}\\models\\odot a c c e p t^{\\leq H}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "654 We start with the $(\\rightarrow)$ direction, in particular, $\\rho\\not\\in P_{s a f e}^{H}$ if and only if $t r a c e_{H}(\\rho)\\in\\mathcal{L}(\\mathcal{D})$ . Recall   \n655 that by definition $\\mathcal{L}(\\mathcal{D})=\\{w\\,\\in\\,\\Sigma^{*}\\,\\mid\\,\\Delta^{*}(\\mathcal{Q}_{0},w)\\in\\mathring{\\mathcal{F}}\\}$ , and so $t r a c e_{H}(\\rho)\\,\\in\\,\\mathcal{L}(\\mathcal{D})$ implies that   \n656 $q_{H}=\\Delta^{*}(\\mathcal{Q}_{0},t r a c e_{H}(\\rho))\\in\\mathcal{\\bar{F}}$ , which by construction implies that $\\rho^{+}\\v{U}\\v{U}=\\diamondsuit a c c e p t{^{\\leq}}\\v{H}$ .   \n657 The opposite direction $(\\leftarrow)$ is a little more involved, in particular, $\\rho^{+}\\,\\,\\Bigl\\vert=\\,\\diamond\\!\\,a c c e p t^{\\leq{\\cal H}}$ implies that   \n658 for the unique run $q_{0},q_{1},q_{2},\\ldots,q_{H}$ there exists $t\\,\\leq\\,H$ such that $q_{t}\\,\\in\\,{\\mathcal{F}}$ . We notice that since   \n659 $\\mathcal{L}(\\mathcal{D})=B\\bar{a}d P r e f(P_{s a f e})$ then once the DFA reaches an accepting state it will remain in an accepting   \n660 state for the rest of the run. Therefore, $q_{t}\\in\\mathcal{F}$ for $t\\leq H$ implies that $q_{H}\\in\\mathcal{F}$ . Then by definition   \n661 the trace trace $\\operatorname{\\Delta}_{H}(\\rho)$ that determined the unique run $q_{0},q_{1},q_{2},\\ldots,q_{H}$ must be in the language $\\mathcal{L}(\\mathcal{D})$   \n662 which again by definition implies that $\\rho\\not\\in\\dot{P}_{s a f e}^{H}$ .   \n663 We now deal with the probabilities. First we note that the DFA $\\mathcal{D}$ does not affect the probabilities of   \n664 the product Markov chain \u2013 it can be shown that for every measurable set $P$ of paths in $\\mathcal{M}$ , ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}^{\\mathcal{M}}(P)=\\operatorname*{Pr}^{\\mathcal{M}\\otimes\\mathcal{A}}(\\rho^{+}\\mid\\rho\\in P)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "665 see [9]. It now remains to construct this set $P$ in the proper way. In particular, if $P$ is the set of paths   \n666 starting in some state $s\\in S$ and that refute $P_{s a f e}$ in the next $H$ timesteps, i.e., ", "page_idx": 17}, {"type": "equation", "text": "$$\nP=\\{\\rho\\in S^{\\omega}\\mid\\rho[0]=s,\\{w^{\\prime}\\in\\Sigma^{*}\\mid w_{p r e f}\\preceq t r a c e(\\rho)\\wedge|w_{p r e f}|\\leq H+1\\}\\cap\\mathcal{L}(\\mathcal{D})\\neq\\emptyset\\}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "667 and $P^{+}$ is defined as the set of paths starting from the corresponding state $\\langle s,q_{s}\\rangle$ (where $q_{s}\\,=$   \n668 $\\Delta(\\mathcal{Q}_{0},L(s)))$ in $\\mathcal{M}\\otimes\\mathcal{D}$ that eventually reach an accepting state of $\\mathcal{D}$ in the next $H$ steps, i.e. ", "page_idx": 17}, {"type": "equation", "text": "$$\nP^{+}=\\{\\rho^{+}\\in(S\\times Q)^{\\omega}\\mid\\rho^{+}[0]=\\langle s,q_{s}\\rangle\\wedge\\rho^{+}\\left|=\\diamond^{\\leq H}a c c e p t\\right\\}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "669 Then by construction we have, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{Pr}^{\\mathcal{M}}(P)=\\mathrm{Pr}^{\\mathcal{M}\\otimes\\mathcal{D}}(\\rho^{+}\\mid\\rho[0]=s,\\rho\\in P)=\\mathrm{Pr}^{\\mathcal{M}\\otimes\\mathcal{D}}(P^{+})\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "670 Finally the probability $\\mathrm{Pr}^{\\mathcal{M}}(P)$ and $\\mathrm{Pr}^{\\mathcal{M}}(s\\vDash P_{s a f e}^{H})$ are related as follows, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{Pr}^{\\mathcal{M}}(s\\models P_{s a f e}^{H})=1-\\operatorname*{Pr}^{\\mathcal{M}}(P)}\\\\ &{\\qquad\\qquad\\qquad=1-\\operatorname*{Pr}^{\\mathcal{M}\\otimes\\mathcal{D}}(P^{+})}\\\\ &{\\qquad\\qquad\\quad=1-\\operatorname*{Pr}^{\\mathcal{M}\\otimes\\mathcal{D}}(\\langle s,q_{s}\\rangle\\models\\wp^{\\leq H}\\mathit{a c c e p t})}\\\\ &{\\qquad\\qquad\\quad=\\operatorname*{Pr}^{\\mathcal{M}\\otimes\\mathcal{D}}(\\langle s,q_{s}\\rangle\\models\\wp^{\\leq H}\\mathit{a c c e p t})}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "671 ", "page_idx": 17}, {"type": "text", "text": "672 C.2 Proof of Proposition 4.2 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "673 Proposition 4.2 (restated). Let $P_{s a f e}^{T}$ denote the (episodic) regular safety property for a fixed episode   \n674 length $T$ . Then satisfying $\\operatorname*{Pr}\\left(\\langle s_{t},q_{t}\\rangle\\:\\middle|=\\langle\\rangle^{\\leq H}a c c e p t\\right)\\:\\leq\\:p_{1}$ for all $t\\ \\in\\ [0,T]$ guarantees that   \n675 $\\operatorname*{Pr}(s_{0}\\models P_{s a f e}^{T})\\ge1-p_{1}\\cdot\\lceil T/H\\rceil$ , where $s_{0}\\sim\\mathcal{P}_{0}$ is the initial state.   \n676 Proof. Consider splitting up the episode in to $\\lceil T/H\\rceil$ chunks with length at most $H$ . Let   \n677 $X_{0},X_{1},\\dots X_{\\lceil T/H\\rceil-1}$ be the indicator random variables defined as follows, ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "equation", "text": "$$\nX_{i}=\\left\\{{1\\quad\\mathrm{if~}\\langle s_{i\\cdot H},q_{i\\cdot H}\\rangle\\left|=\\diamond^{\\leq H}{a c c e p t}\\right.}\\right.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "678 Since $\\operatorname*{Pr}(\\langle s_{t},q_{t}\\rangle\\models\\diamond\\llcorner t_{a c c e p t})\\leq p_{1}$ for all $t\\in[0,T]$ then the probability $\\operatorname*{Pr}(X_{i}=1)\\leq p_{1}$ . By   \n679 construction we have, ", "page_idx": 17}, {"type": "equation", "text": "$$\n{\\mathrm{if}}\\quad\\bigcap_{i=0}^{\\left[T/H\\right]-1}X_{i}=0\\quad{\\mathrm{then}}\\quad s_{0}\\neq P_{s a f e}^{T}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "680 Intuitively we satisfy $P_{s a f e}$ for the entire episode length if we never enter an accepting state in each of   \n681 the $\\lceil T/\\dot{H}\\rceil$ chunks. The final result is then obtained by taking a union bound as follows, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{Pr}(s_{0}\\models P_{s q\\ell}^{T})\\geq\\operatorname*{Pr}\\left(\\begin{array}{c}{\\lceil T/H\\rceil-1}\\\\ {\\bigcap}\\\\ {i=0}\\end{array}\\right)}\\\\ &{\\qquad\\qquad\\qquad=1-\\operatorname*{Pr}\\left(\\begin{array}{c}{\\lceil T/H\\rceil-1}\\\\ {\\bigcup}\\\\ {i=0}\\end{array}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\geq1-\\displaystyle\\sum_{i=0}^{\\lceil T/H\\rceil-1}\\operatorname*{Pr}(X_{i}=1)}\\\\ &{\\qquad\\qquad\\qquad\\geq1-p_{1}\\cdot\\lceil T/H\\rceil}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "682 ", "page_idx": 18}, {"type": "text", "text": "683 C.3 Proof of Proposition 5.4 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "684 Proposition 5.4 (restated). Let $\\epsilon>0,$ , $\\delta>0$ , $s\\in S$ be given. Under Assumption 5.2, we can obtain   \n685 an \u03f5-approximate estimate for $\\operatorname*{Pr}(\\langle s,q\\rangle\\models\\diamond\\diamond{s t}_{a c c e p t})$ with probability at least $1-\\delta$ , by sampling   \n686 $\\begin{array}{r}{m\\geq\\frac{1}{2\\epsilon^{2}}\\log\\left(\\frac{2}{\\delta}\\right)}\\end{array}$ paths from the \u2018black box\u2019 model.   \n687 Proof. In words, we estimate $\\operatorname*{Pr}(\\langle s,q\\rangle\\,\\,|=\\,\\diamond^{\\leq H}a c c e p t)$ by sampling $m$ paths from a \u2018black box\u2019   \n688 model of the environment dynamics. We label each path as satisfying or not and return the proportion   \n689 of satisfying traces as an estimate for $\\operatorname*{Pr}(\\langle s,q\\rangle\\,|{=}\\,\\diamond^{\\leq H}a c c e p t)$ . We proceed as follows, let $\\rho_{1},\\ldots\\rho_{m}$   \n690 be a sequence of paths sampled from the \u2018black box\u2019 model and let $t r a c e(\\rho_{1}),...\\,t r a c e(\\rho_{m})$ be the   \n691 corresponding traces. Furthermore, let $X_{1},\\ldots,X_{m}$ be indicator r.v.s such that, ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "equation", "text": "$$\nX_{i}={\\left\\{\\!\\!\\begin{array}{l l}{1}&{{\\mathrm{if}}\\;t r a c e(\\rho_{1})\\mapsto\\diamond\\!\\!\\stackrel{\\leq}{\\sim}\\!\\!\\!\\begin{array}{l}{a c c e p t,}\\\\ {0}&{{\\mathrm{otherwise}}}\\end{array}\\!\\!\\right.}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "692 Recall that tr $\\imath c e(\\rho_{1})\\models\\diamond\\uit^{\\leq H_{a}}$ ccept can be checked in time $O(\\mathrm{poly}(H))$ . Now let, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\overline{{{X}}}=\\frac{1}{m}\\sum_{i=1}^{m}X_{i}\\mathrm{~where~}\\mathbb{E}[\\overline{{{X}}}]=\\operatorname*{Pr}(\\langle s,q\\rangle\\models\\mathcal{O}^{\\leq H}a c c e p t)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "693 then by Hoeffding\u2019s inequality [40], ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[|\\overline{{X}}-\\mathbb{E}[\\overline{{X}}]|\\geq\\epsilon\\right]\\leq2\\exp\\left(-2m\\epsilon^{2}\\right)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "694 Bounding the RHS from above by $\\delta$ and rearranging gives the desired result. ", "page_idx": 18}, {"type": "text", "text": "695 C.4 Proof of Proposition 5.5 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "696 We start by introducing the following lemma. ", "page_idx": 18}, {"type": "text", "text": "697 Lemma C.1 (Error amplification for trace distributions). Let $\\widehat{\\mathcal{P}}\\approx\\mathcal{P}$ be such that, ", "page_idx": 18}, {"type": "equation", "text": "$$\nD_{T V}\\left(\\mathcal{P}(\\cdot\\mid s),\\widehat{\\mathcal{P}}(\\cdot\\mid s)\\right)\\leq\\alpha\\,\\forall s\\in S\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "698 Let the start state $s_{0}\\in\\mathcal{S}$ be given, and let $\\mathcal{P}_{t}(\\cdot)$ and $\\widehat{\\mathcal{P}}_{t}(\\cdot)$ denote the path distribution (at time $t$ ) for   \n699 the two transition probabilities $\\mathcal{P}$ and $\\widehat{\\mathcal{P}}$ respectively. Then the total variation distance between the   \n700 two path distributions (at time $t$ ) are bounded as follows, ", "page_idx": 18}, {"type": "equation", "text": "$$\nD_{T V}\\left(\\mathcal{P}_{t}(\\cdot),\\widehat{\\mathcal{P}}_{t}(\\cdot)\\right)\\leq\\alpha t\\;\\forall t\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "701 Proof. We will prove this fact by doing an induction on $t$ . We recall that $\\mathcal{P}_{t}(\\cdot)$ and $\\widehat{\\mathcal{P}}_{t}(\\cdot)$ denote the   \n702 path distribution (at time $t$ ) for the two transition probabilities $\\mathcal{P}$ and $\\widehat{\\mathcal{P}}$ respectively. Formally we   \n703 define them as follows, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{P}_{t}(\\rho)=\\operatorname*{Pr}(s_{0},\\dotsc...,s_{t}\\preceq\\rho\\mid s_{0}=s,\\mathcal{P})}\\\\ &{\\widehat{\\mathcal{P}}_{t}(\\rho)=\\operatorname*{Pr}(s_{0},\\dotsc...,s_{t}\\preceq\\rho\\mid s_{0}=s,\\widehat{\\mathcal{P}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "704 These probabilities read as follows, \u2018the probability of the sequence $s_{0},\\ldots,s_{t}\\,\\preceq\\,\\rho$ at time $t^{\\star}$ , or   \n705 similarly \u2018the probability that the sequence $s_{0},\\ldots,s_{t}$ is a prefix of $\\rho$ at time $t'$ Since the start state   \n706 $s_{0}\\in\\mathcal S$ is given we note that, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{P}_{0}(\\cdot)=\\widehat{\\mathcal{P}}_{0}(\\cdot)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "707 Before we continue with the induction on $t$ we make the following observation, for any path $\\rho\\in S^{\\omega}$   \n708 we have by the triangle inequality, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\mathcal{P}_{t}(\\rho)-\\widehat{\\mathcal{P}}_{t}(\\rho)\\right|=\\left|\\mathcal{P}(s_{t}\\mid s_{t-1})\\mathcal{P}_{t-1}(\\rho)-\\widehat{\\mathcal{P}}(s_{t}\\mid s_{t-1})\\widehat{\\mathcal{P}}_{t-1}(\\rho)\\right|}\\\\ &{\\qquad\\qquad\\qquad\\leq\\mathcal{P}_{t-1}(\\rho)\\left|\\mathcal{P}(s_{t}\\mid s_{t-1})-\\widehat{\\mathcal{P}}(s_{t}\\mid s_{t-1})\\right|+\\widehat{\\mathcal{P}}(s_{t}\\mid s_{t-1})\\left|\\mathcal{P}_{t-1}(\\rho)-\\widehat{\\mathcal{P}}_{t-1}(\\rho)\\right|}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "709 Now we continue with the induction on $t$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{2D_{T V}(\\mathcal{P}_{t}(\\cdot),\\widehat{\\mathcal{P}}_{t}(\\cdot))=\\displaystyle\\sum_{\\rho\\in\\mathcal{S}^{\\star}}\\Big|\\mathcal{P}_{t}(\\rho)-\\widehat{\\mathcal{P}}_{t}(\\rho)\\Big|}&{}\\\\ {\\leq\\displaystyle\\sum_{\\rho\\in\\mathcal{S}^{\\star}}\\mathcal{P}_{t-1}(\\rho)\\,\\Big|\\mathcal{P}(s_{t}\\mid s_{t-1})-\\widehat{\\mathcal{P}}(s_{t}\\mid s_{t-1})\\Big|}&{}\\\\ {+\\displaystyle\\sum_{\\rho\\in\\mathcal{S}^{\\star}}\\widehat{\\mathcal{P}}(s_{t}\\mid s_{t-1})\\,\\Big|\\mathcal{P}_{t-1}(\\rho)-\\widehat{\\mathcal{P}}_{t-1}(\\rho)\\Big|}&{}\\\\ {\\leq\\displaystyle\\sum_{\\rho\\in\\mathcal{S}^{\\star}}\\mathcal{P}_{t-1}(\\rho)\\cdot(2\\alpha)+\\displaystyle\\sum_{\\rho\\in\\mathcal{S}^{\\star}}\\Big|\\mathcal{P}_{t-1}(\\rho)-\\widehat{\\mathcal{P}}_{t-1}(\\rho)\\Big|}&{}\\\\ {=2\\alpha+2D_{T V}(\\mathcal{P}_{t-1}(\\cdot),\\widehat{\\mathcal{P}}_{t-1}(\\cdot))}&{}\\\\ {\\leq2\\alpha t}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "710 The final result is obtained by an induction on $t$ where the base case comes from $\\begin{array}{r}{\\mathcal{P}_{0}(\\cdot)=\\widehat{\\mathcal{P}}_{0}(\\cdot)}\\end{array}$ . ", "page_idx": 19}, {"type": "text", "text": "711 Proposition 5.5 (restated). Let $\\epsilon>0$ , $\\delta>0$ , $s\\in S$ and horizon $H\\ge1$ be given. Under Assumption   \n712 5.3 we can make the following two statements:   \n713 (1) We can obtain an \u03f5-approximate estimate for $\\operatorname*{Pr}(\\langle s,q\\rangle\\,\\vline=\\,\\diamond\\!\\le\\!H_{a c c e p t})$ with probability 1 by   \n714 exact model checking with the transition probabilities of ${\\widehat{\\mathcal{P}}}_{\\pi}$ in time $\\mathcal{O}(p o l y(s i z e(\\mathcal{M}_{\\pi}\\otimes\\mathcal{D}))\\cdot H)$ .   \n715 (2) We can obtain an \u03f5-approximate estimate for $\\operatorname*{Pr}(\\langle s,q\\rangle\\vline=\\diamond\\Sigma^{H}a c c e p t)$ with probability at least   \n716 $1-\\delta$ , by sampling $m\\geq\\textstyle{\\frac{2}{\\epsilon^{2}}}\\log\\left({\\frac{2}{\\delta}}\\right)$ paths from the \u2018approximate\u2019 dynamics model ${\\widehat{\\mathcal{P}}}_{\\pi}$ .   \n717 Proof. We start by proving statement (1) and then statement (2) will follow quickly. First let   \n718 $\\operatorname*{Pr}(\\langle s,q\\rangle\\models\\diamond\\diamond{s t}_{a c c e p t})$ and $\\widehat{\\operatorname*{Pr}}(\\langle s,q\\rangle\\models\\diamond\\llangle H_{a c c e p t})$ denote the acceptance probabilities for the   \n719 two transition probabilities $\\mathcal{P}$ and $\\widehat{\\mathcal{P}}$ respectively. We also let $g(\\cdot)$ and $\\widehat g(\\cdot)$ denote the average trace   \n720 distribution (over the next $H$ timesteps) for the two transition probabilities $\\mathcal{P}$ andP respectively,   \n721 where, ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle g(\\rho)=\\frac{1}{H}\\sum_{t=1}^{H}\\mathcal{P}_{t}(\\rho)}}\\\\ {{\\displaystyle\\widehat{g}(\\rho)=\\frac{1}{H}\\sum_{t=1}^{H}\\widehat{\\mathcal{P}}_{t}(\\rho)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "722 Before we continue with the proof of (1) we make the following observations, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\bullet\\operatorname*{max}_{\\langle s,q\\rangle}\\left|\\mathrm{Pr}(\\langle s,q\\rangle\\left|=\\diamond\\right.^{\\leq H}{\\mathit{a c c e p t}})-\\widehat{\\mathrm{Pr}}(\\langle s,q\\rangle\\left|=\\diamond\\right.^{\\leq H}{\\mathit{a c c e p t}})\\right|\\leq1\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "724   \n725 ", "page_idx": 20}, {"type": "text", "text": "\u2022 Let $f(x):x\\in\\mathcal{X}\\to[0,1]$ be a real-valued function. Let $\\mathcal{P}_{1}(\\cdot)$ and $\\mathcal{P}_{2}(\\cdot)$ be probability distributions over the space $\\mathcal{X}$ , then. ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left|\\mathbb{E}_{x\\sim\\mathcal{P}_{1}(\\cdot)}[f(x)]-\\mathbb{E}_{x\\sim\\mathcal{P}_{2}(\\cdot)}[f(x)]\\right|\\le D_{T V}(\\mathcal{P}_{1}(\\cdot),\\mathcal{P}_{2}(\\cdot))\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "726 We continue by showing the following, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{Pr}\\{(s,q)\\in\\ensuremath{\\mathcal{S}}^{B}u\\mathrm{~}_{\\alpha\\in\\ensuremath{\\mathbb{R}}^{n}}\\}-\\operatorname*{Pr}_{1}(s,q)=\\phi^{\\leq n}u\\mathrm{~}_{\\alpha\\in\\ensuremath{\\mathbb{R}}^{n}}}\\\\ &{\\qquad=\\left\\lbrace\\ensuremath{\\mathbb{R}}_{\\ell}\\setminus\\left[\\{s,q\\}\\right]+\\odot^{\\leq n}u\\mathrm{corpt}\\right\\rbrace\\right\\rbrace}\\\\ &{\\qquad\\qquad\\leq\\operatorname*{Pr}\\left(g^{\\star},\\tilde{g}(\\cdot)\\right)}\\\\ &{\\qquad=\\frac{1}{2}\\sum_{q\\in\\ensuremath{\\mathbb{R}}}\\ensuremath{\\mathbb{I}_{q}}(p)-\\tilde{g}(\\rho)}\\\\ &{\\qquad=\\frac{1}{2}\\sum_{q\\in\\ensuremath{\\mathbb{R}}}\\left[\\ensuremath{\\mathbb{I}_{q}}^{H}\\left(p\\right)-\\tilde{g}(\\rho)\\right]}\\\\ &{\\qquad=\\frac{1}{2}\\frac{1}{H}\\sum_{q\\in\\ensuremath{\\mathbb{R}}}\\left[\\frac{|\\mathbf{I}_{q}^{H}}{\\sum_{q}\\ensuremath{\\mathbb{I}_{q}}(p)-\\tilde{g}_{t}(\\rho)}\\right]}\\\\ &{\\qquad\\leq\\frac{1}{2H}\\sum_{i=1}^{H}\\left[\\underset{r\\leq0}{\\overset{n}{\\sum}}p_{r}(\\rho)-\\tilde{p}_{t}(\\rho)\\right]}\\\\ &{\\qquad\\leq\\frac{1}{2H}\\sum_{i=1}^{H}\\left[e^{\\phi_{r}}/H\\right]}\\\\ &{\\qquad=\\phi\\frac{1}{H}\\sum_{i=1}^{H}H(e/H)}\\\\ &{\\qquad=e^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "727 The first inequality (Eq. 47) comes from our earlier observations. The second inequality (Eq. 50) is   \n728 straightforward and the final inequality (Eq. 51) is obtained by applying Lemma C.1 and Assumption   \n729 5.3. We note that this result is similar to the simulation lemma [48], which has been proved many   \n730 times for several different settings [1, 16, 47, 57].   \n731 This concludes the proof of statement (1), since we have shown that $\\widehat{\\operatorname*{Pr}}(\\langle s,q\\rangle\\models\\diamond\\ll H_{a c c e p t})$ is an   \n732 $\\epsilon/2$ -approximate estimate of $\\operatorname*{Pr}(\\langle s,q\\rangle\\models\\odot^{\\leq H}{a c c e p t})$ , under the A ssumption 5.3. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "733 The proof of statement (2) follows quickly. We have established that, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left|\\operatorname*{Pr}(\\langle s,q\\rangle\\models\\odot^{\\leq H}{\\boldsymbol{a c c e p t}})-\\widehat{\\operatorname*{Pr}}(\\langle s,q\\rangle\\models\\odot^{\\leq H}{\\boldsymbol{a c c e p t}})\\right|\\leq\\epsilon/2\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "734 It remains to obtain an $\\epsilon/2$ -approximate estimate of $\\widehat{\\mathrm{Pr}}(\\langle s,q\\rangle\\;\\mapsto\\;\\diamond\\scriptscriptstyle\\zeta^{\\leq H}a c c e p t)$ . By using the   \n735 same reasoning as in the proof of Proposition 5.4. We can obtain an $\\epsilon/2$ -approximate estimate   \n736 of $\\widehat{\\operatorname*{Pr}}(\\langle s,q\\rangle\\models\\diamond\\llcorner t_{a c c e p t})$ by sampling $m$ paths, $\\rho_{1},\\ldots\\rho_{m}$ , from the approximate dynamics model   \n737 $\\widehat{\\mathcal{P}}$ . Then provided, ", "page_idx": 20}, {"type": "equation", "text": "$$\nm\\geq\\frac{2}{\\epsilon^{2}}\\log\\left(\\frac{2}{\\delta}\\right)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "738 with probability $1-\\delta$ we can obtain $\\epsilon/2$ -approximate estimate of $\\widehat{\\operatorname*{Pr}}(\\langle s,q\\rangle\\models\\diamond\\llcorner t_{a c c e p t})$ and by   \n739 extension an $\\epsilon$ -approximate estimate of $\\operatorname*{Pr}(\\langle s,q\\rangle\\,|{=}\\,\\diamond^{\\leq H}a c c e p t)$ .  This concludes the proof. \u53e3 ", "page_idx": 20}, {"type": "text", "text": "740 C.5 Proof of Theorem 6.5 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "741 Theorem 6.5 (restated). Under Assumption 6.3 and 6.4, and provided that every state action pair   \n742 $(s,a)\\in S\\times A$ has been visited at least $\\begin{array}{r}{\\mathcal{O}\\left(\\frac{H^{2}|S|^{2}}{\\epsilon^{2}}\\log\\left(\\frac{|A||S|^{2}}{\\delta}\\right)\\right)}\\end{array}$ times. Then with probability $1-\\delta$   \n743 the system satisfies the constraints of Problem 4.1, independent of the \u2018task policy\u2019.   \n744 Proof. We split the proof up in to three parts, (1), (2) and (3). In part (1) we show that the given   \n745 sample complexity bound gives us an approximate model of the environment dynamics with high   \n746 probability. In part (2) we use our assumptions to reason about the probabilistic recoverability   \n747 of the system when it enters a critical state. In part (3) we put everything together and deal with   \n748 approximation error $\\epsilon$ the remaining failure probability that are both unavoidable for the statistical   \n749 model checking procedures used to shield the system. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "750 (1) We show that the following holds with probability $1-\\delta/2$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\nD_{T V}\\left(\\mathcal{P}_{\\pi}(\\cdot\\mid s),\\widehat{\\mathcal{P}}_{\\pi}(\\cdot\\mid s)\\right)\\leq\\epsilon/H\\,\\forall s\\in\\mathcal{S}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "751 when every state action pair $(s,a)\\in S\\times A$ has been visited at least, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{O}\\left(\\frac{H^{2}|S|^{2}}{\\epsilon^{2}}\\log\\left(\\frac{|\\mathcal{A}||S|^{2}}{\\delta}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "752 times. First we let $\\#(s,a)$ denote the total number of times that $(s,a)$ has been observed, similarly   \n753 we let $\\#(s^{\\prime},s,a)$ denote the total number of times that $(s^{\\prime},s,a)$ has been observed. The maximum   \n754 likelihood estimate for the unknown probability $\\mathcal{P}(\\boldsymbol{s}^{\\prime}\\mid,\\boldsymbol{s},\\boldsymbol{a})$ is $\\widehat{\\mathcal{P}}(s^{\\prime}\\mid s,a)=\\#(s^{\\prime},s,a)/\\#(s,a)$   \n755 Let us fix some $(s,a)\\in S\\times A$ , and $s^{\\prime}\\in\\mathcal{S}$ , we let $p_{s^{\\prime}}=\\mathcal{P}(s^{\\prime}\\mid s,a)$ denote the true probability of   \n756 transitioning to $s^{\\prime}$ from $(s,a)$ and we let $\\hat{p}_{s^{\\prime}}=\\#(s^{\\prime},s,a)/\\#(s,\\stackrel{.}{a})$ denote our estimate. We note that   \n757 $\\mathbb{E}[\\hat{p}_{s^{\\prime}}]=p_{s^{\\prime}}$ , i.e. $\\hat{p}_{s^{\\prime}}$ is an unbiased estimator for $p_{s^{\\prime}}$ . Let $m=\\#(s,a)$ also be the number of times   \n758 that $(s,a)$ has been observed, then by Hoeffding\u2019s inequality [40] we have, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[|p_{s^{\\prime}}-\\hat{p}_{s^{\\prime}}|\\ge\\frac{\\epsilon}{H|S|}\\right]\\le2\\exp\\left(-2m\\frac{\\epsilon^{2}}{H^{2}|S|^{2}}\\right)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "759 Bounding the LHS from above by $1-\\delta/2(|A||S|^{2})$ and rearranging gives the following lower bound   \n760 for $m$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\nm\\geq\\frac{H^{2}|S|^{2}}{2\\epsilon^{2}}\\log{\\left(\\frac{4|A||S|^{2}}{\\delta}\\right)}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "761 Taking a union bound over all $(s^{\\prime},s,a)\\in\\mathcal{S}\\times\\mathcal{S}\\times\\mathcal{A}$ , then for all state action pairs $(s,a)\\in S\\times A$   \n762 we have the following with probability at least $1-\\delta$ . ", "page_idx": 21}, {"type": "equation", "text": "$$\n2D_{T V}\\left(\\mathcal{P}(\\cdot\\mid s,a),\\widehat{\\mathcal{P}}(\\cdot\\mid s,a)\\right)=\\sum_{s^{\\prime}\\in S}\\left|p_{s^{\\prime}}-\\widehat{p}_{s^{\\prime}}\\right|\\leq\\sum_{s^{\\prime}\\in S}\\frac{\\epsilon}{H|S|}\\leq\\epsilon/H\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "763 Now fix some $s\\in S$ and we observe the following, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{2D_{T V}\\left(\\mathcal{P}_{\\pi}(\\cdot\\ |\\ s),\\widehat{\\mathcal{P}}_{\\pi}(\\cdot\\ |\\ s)\\right)=\\displaystyle\\sum_{s^{\\prime}\\in\\mathcal{S}}\\left|\\mathcal{P}_{\\pi}(s^{\\prime}\\ |\\ s)-\\widehat{\\mathcal{P}}_{\\pi}(s^{\\prime}\\ |\\ s)\\right|}\\\\ &{\\quad=\\displaystyle\\sum_{s^{\\prime}\\in\\mathcal{S}}\\displaystyle\\sum_{a\\in A}|\\mathcal{P}(s^{\\prime}\\ |\\ s,a)\\pi(a\\ |\\ s)-\\widehat{\\mathcal{P}}(s^{\\prime}\\ |\\ s,a)\\pi(a\\ |\\ s)|}\\\\ &{\\quad=\\displaystyle\\sum_{a\\in A}\\pi(a\\ |\\ s)\\displaystyle\\sum_{s^{\\prime}\\in\\mathcal{S}}|\\mathcal{P}(s^{\\prime}\\ |\\ s,a)-\\widehat{\\mathcal{P}}(s^{\\prime}\\ |\\ s,a)|}\\\\ &{\\quad=\\displaystyle\\sum_{a\\in A}\\pi(a\\ |\\ s)2D_{T V}\\left(\\mathcal{P}(\\cdot\\ |\\ s,a),\\widehat{\\mathcal{P}}(\\cdot\\ |\\ s,a)\\right)}\\\\ &{\\quad\\overset{\\leq}\\leq\\epsilon/H}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "764 Thus with probability at least $1-\\delta/2$ we have for all $s\\in S$ that, ", "page_idx": 21}, {"type": "equation", "text": "$$\nD_{T V}\\left({\\mathcal{P}}_{\\pi}(\\cdot\\mid s),{\\widehat{\\mathcal{P}}}_{\\pi}(\\cdot\\mid s)\\right)\\leq\\epsilon/H\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "765 (2) Using Assumption 6.3 and 6.4 we can argue about the safety of the system. Suppose firstly,   \n766 that we can check the condition $\\operatorname*{Pr}(\\langle s,q\\rangle\\models\\diamond\\widecheck{\\leq}^{H}a c c e p t)\\leq p_{1}$ , precisely and without any failure   \n767 probability (we will deal with statistical model checking in part (3)). From any non-critical state we   \n768 can transition arbitrarily to a critical state, although under Assumption 6.3 this critical state is not   \n769 irrecoverable with probability $\\geq p_{1}$ . We now consider the following two cases: ", "page_idx": 21}, {"type": "text", "text": "770 (i) $\\operatorname*{Pr}(\\langle s,q\\rangle\\models\\diamond\\llcorner t_{a c c e p t})\\leq p_{1}$ under the \u2018task\u2019 policy. ", "page_idx": 21}, {"type": "text", "text": "771 (ii) $\\operatorname*{Pr}(\\langle s,q\\rangle\\models\\diamond\\llcorner t_{a c c e p t})>p_{1}$ under the \u2018task\u2019 policy. ", "page_idx": 22}, {"type": "text", "text": "772 For case (i) we can safely use the \u2018task\u2019 policy and return to a non-critical state within $H$ timesteps   \n773 with probability at least $1-p_{1}$ . For case (ii) we deploy the \u2018safe\u2019 policy and under Assumption 6.4   \n774 we can return to a non-critical state within $H$ timesteps with probability at least $1-p_{1}$ . We have now   \n775 established an invariant, since from every non-critical state we can return to a non-critical state with   \n776 probability $1-p_{1}$ and thus satisfy $\\operatorname*{Pr}(\\langle\\dot{s},q\\rangle\\models\\diamond\\llcorner t_{a c c e p t})\\leq p_{1}$ at every timestep $t\\in[0,T]$ .   \n777 (3) We now make a similar argument but for the statistical model checking procedure where we   \n778 can only obtain an $\\epsilon$ -approximate estimate for the probability $\\operatorname*{Pr}(\\langle s,q\\rangle\\models\\check{\\odot}^{\\dot{\\leq}H}a c c e p t)$ with high   \n779 probability. Let us denote our $\\epsilon_{}$ -approximate estimate $\\widehat{\\operatorname*{Pr}}(\\langle s,q\\rangle\\models\\rangle^{\\leq H}a c c e p t)$ , rather than check   \n780 the condition $\\operatorname*{Pr}(\\langle s,q\\rangle\\models\\diamond\\scriptscriptstyle\\diamond\\boldsymbol{H}_{a c c e p t})\\leq p_{1}$ , we can check condition $\\widehat{\\operatorname*{Pr}}(\\langle s,q\\rangle\\models\\diamond\\llcorner t_{a c c e p t})\\leq$   \n781 $p_{1}-\\epsilon$ , and if $\\widehat{\\mathrm{Pr}}(\\langle s,q\\rangle\\,\\vline=\\langle\\rangle^{\\leq H}a c c e p t)$ is indeed an $\\epsilon_{\\mathrm{:}}$ -approximate estimate then this guarantees   \n782 $\\operatorname*{Pr}(\\langle s,q\\rangle\\models\\diamond\\widecheck{\\gamma}^{\\leq H}\\widecheck{a c c e p t})\\leq p_{1}$ . Consider the following two cases:   \n783 (i) Our estimate $\\widehat{\\mathrm{Pr}}(\\langle s,q\\rangle\\models\\langle\\rangle^{\\leq H}a c c e p t)\\leq p_{1}-\\epsilon$   \n784 (ii) Our estimate $\\widehat{\\mathrm{Pr}}(\\langle s,q\\rangle\\,\\left|=\\langle\\rangle^{\\leq H}a c c e p t\\right)>p_{1}-\\epsilon$   \n785 For case (i) we can safely use the \u2018task\u2019 policy and return to a non-critical state within $H$ timesteps   \n786 with probability at least $1-p_{1}$ . For case (ii) we deploy the \u2018safe\u2019 policy and under Assumption 6.4   \n787 we can return to a non-critical state within $H$ timesteps with probability at least $1-p_{1}$ . Again we   \n788 have established an invariant, since from every non-critical state we can return to a non-critical state   \n789 with probability $1-p_{1}$ and thus satisfy $\\operatorname*{Pr}(\\langle\\dot{s},q\\rangle\\models\\diamond\\llcorner t_{a c c e p t})\\leq p_{1}$ at every timestep $t\\in[0,T]$ .   \n790 We still need to deal with the failure probability of the statistical model checking procedure at   \n791 each timestep, by choosing failure probability $\\bar{1}-\\delta/2T$ we can guarantee (by a union bound) an   \n792 $\\epsilon$ -approximate estimate for each timestep with probability $1-\\delta/2$ . Finally, taking a union bound   \n793 over part (1) and (2) gives the desired total failure probability $1-\\delta$ . ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "794 ", "page_idx": 22}, {"type": "text", "text": "795 D Environment Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "796 D.1 Colour Gridworld ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "797 The colour gridworld environment is a simple $9\\,\\times\\,9$ grid, with   \n798 state space $|{\\cal S}|=81$ and action space $|{\\mathcal{A}}|=5$ , where each action   \n799 corresponds to the following movements: Left,Right, $U p$ , Down, Stay.   \n800 The objective is to navigate from the start state in one corner of the   \n801 grid, to the goal state in the other corner, after reaching the goal state   \n802 the agent is then sent back to the start state. The agent must navigate   \n803 to the goal state as many times as possible in a fixed episode length   \n804 of $T=1000$ . The reward function is a sparse reward that gives the   \n805 agent $+1$ reward for reaching the goal and 0 otherwise. When the   \n806 environment is fully deterministic the maximum achievable reward   \n807 is 58.   \n808 In addition to the goal state, there are three other distinct states,   \n809 green, blue and purple, each labelled with their corresponding   \n810 colours, see Fig. 4. The set of atomic propositions is thus $A P=$   \n811 $\\{g r e e n,b l u e,p u r p l e,g o a l\\}$ , the safety properties are specified over   \n812 the set $A P$ , in particular we conduct experiments with 3 different   \n813 safety properties of increasing complexity: ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "image", "img_path": "PnSTlFUfcd/tmp/7976986742147723aa5b61efd5b2432c6d9a97396da96444e6287b9d62f754e8.jpg", "img_caption": [], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Figure 4: Colour gridworld environment. Top left hand corner (agent) is the start position. The agent must navigate to the goal position in the bottom right hand corner of gridworld. The coloured states labelled blue, green and purple correspondingly. ", "page_idx": 22}, {"type": "text", "text": "814 $\\begin{array}{l}{\\cdot\\ (1)\\boxed{\\cdot\\,}g r e e n}\\\\ {\\cdot\\ (2)\\boxed{\\cdot\\,}g o a l\\rightarrow\\diamond^{\\leq10}b l u e}\\\\ {\\cdot\\ (3)\\boxed{\\cdot\\,}g o a l\\rightarrow\\diamond^{\\leq10}\\boxed{\\cdot\\,}^{\\leq5}p u r p l e}\\end{array}$   \n815   \n816   \n817 Property (1) is a simple invariant property $P_{i n v}(\\neg g r e e n)$ that states the green state must always be   \n818 avoided. Property (2) and (3) are more complex safety properties that interfere with the goal state. In   \n819 particular, property (2) states that once the goal state is reached then the blue state must be reached   \n820 within 10 steps, this actually has no direct consequences on the maximum reward achievable but may   \n821 interfere with convergence as the goal state seemingly leads to a high penalty if the blue state is not   \n822 reached.   \n823 Property (3) states that once the goal state is reached then the purple state must be reached within 10   \n824 steps and then purple must hold for the next 5 timesteps. In safety property both interferes with the   \n825 goal and has direct consequences on the maximum achievable reward as staying in purple for 5 steps   \n826 does not lead to progress towards the goal state. In terms of the size of the DFA $|\\mathcal{Q}|$ , property (1) is   \n827 an invariant so the cost function is Markov and the size of the DFA is 2, for property (2) and (3) the   \n828 size of the DFA is 12 and 62 respectively.   \n829 Each of the safety properties are tested with the cor  \n830 responding $p$ value for the environment, detailed in   \n831 Table 1, which is repeated here for reference. The $p$   \n832 value corresponds to the level of stochasticity in the   \n833 environment. In particular, if $p=0.25$ then there is   \n834 a $25\\%$ chance of the agents action being overridden   \n835 with another random action chosen uniformly. Given   \n836 the environment is stochastic then it is difficult to satisfy the safety properties with probability 1.   \n837 Through preliminary statistical analysis we computed the maximum satisfaction probabilities for   \n838 each property, to help inform an appropriate $p$ value to test with. With $p=0.25$ , property (1) can be   \n839 satisfies with very high probability close to 1, while still achieving maximum reward. With $p=0.25$   \n840 property (2) can be satisfied with probability $\\approx0.93$ while still achieving maximum reward. With   \n841 $p=0.1$ property (3) can be satisfied with probability $\\approx0.75$ while still achieving good reward.   \n842 Hyperparameter settings. We discuss some of the hyperparameter settings for our shielding   \n843 approach that are not detailed in Table 5.   \n844 Property (1): we use a model checking horizon of $H=3$ , and probability threshold $p_{1}=1.0$ , with   \n845 the number of samples $m=4096$ , we can obtain a roughly $\\epsilon=0.05$ approximate estimate of the   \n846 finite horizon satisfaction probability with failure probability $\\delta=0.01$ .   \n847 Property (2): we use a model checking horizon of $H=10$ , and probability threshold $p_{1}=0.9$ , with   \n848 the number of samples $m=8192$ , we can obtain a roughly $\\epsilon=0.05$ approximate estimate of the   \n849 finite horizon satisfaction probability with a smaller failure probability $\\delta=0.001$ .   \n850 Property (3): again we use a model checking horizon of $H=10$ , and probability threshold $p_{1}=0.6$ ,   \n851 with the number of samples $m=1024$ , we can obtain roughly a $\\epsilon=0.1$ approximate estimate of the   \n852 finite horizon satisfaction probability with failure probability $\\delta=0.01$ .   \n853 Extended discussion of results. First we provide slightly larger figures that than provided in the   \n854 main paper, see Figure 5.   \n855 In general we observe that our shielding method is able to effectively trade-off reward and safety, in   \n856 all cases converging to a system that obtains superior or comparable performance with the baseline.   \n857 For property (1) we might expect our method to be able to recover the optimal policy that avoids the   \n858 green state, it is clear in this case that the shielding procedure has harmed convergence and perhaps   \n859 further investigation and hyperparameter tuning will encourage improvements. For property (2) and   \n860 (3) the results are what we expect \u2013 we can recover the best policy that satisfies the step-wise bounded   \n861 safety property with the desired probability $p_{1}$ .   \n862 The intuitive reason for why simply penalising Q-learning doesn\u2019t work, is that tuning the cost   \n863 coefficient $C$ is challenging for stochastic environments, where safety cannot be enforced \u2018almost   \n864 surely\u2019 (with probability 1), and the precise value of $C$ offers little to no semantic meaning. For   \n865 different levels of stochasticity $p$ values it is hard to know what desired level of safety we can achieve   \n866 while still converging to a high reward policy, making tuning $C$ even harder without knowing more   \n867 about the structure of the environment. In Appendix $\\boldsymbol{\\mathrm F}$ we study more closely the effect of $C$ and   \n868 $p$ . Furthermore, we note te sensitivity of our method to the chosen model checking horizon $H$ . In   \n869 particular, if $H$ is too large we might expect the system to be overly conservative, we also address   \n870 this in more detail in Appendix F. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "table", "img_path": "PnSTlFUfcd/tmp/b611361d865aa2a73498569b445a675cda70d87f54f495cab7c26e804529af0b.jpg", "table_caption": ["Table 2: Safety properties and $p$ value "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "image", "img_path": "PnSTlFUfcd/tmp/406222f3a6a01beda1a03daff4af2fde134db0c00f6c670ee94368e5d6b25d0e.jpg", "img_caption": ["Figure 5: Episode reward and cost for tabular RL \u2018colour\u2019 gridworld environment. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "871 D.2 Atari Seaquest ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "872 Our DreamerV3 [34] based shielding procedure is tested   \n873 on Atari Seaquest, provided as part of the Arcade Learn  \n874 ing Environment (ALE)[10, 50]. Seaquest is a partially   \n875 observable environment meaning we do not have direct   \n876 access to the underlying state space $\\boldsymbol{S}$ , we are however   \n877 provided with observations $o\\in O$ as pixel images which   \n878 correspond to $64\\times64\\times3$ tensors. Fortunately Dream  \n879 erV3 is specifically designed to operate in visual settings   \n880 and is able to effectively learn a predictive world model   \n881 that closely approximate the environment dynamics. The   \n882 action space of Seaquest is finite, specifically $|{\\mathcal{A}}|=18$ ,   \n883 where each action corresponds to a joystick movement and fire button interaction. Rewards are   \n884 obtained by \u2018shooting\u2019 an enemy shark or submarine, or by rescuing divers and returning them to the   \n885 surface. In addition, the agent must manage its oxygen resources and avoid being hit by sharks and   \n886 the enemy submarines which fire back, see Fig. 6. The environment is also made stochastic by using   \n887 \u2018sticky actions\u2019 [50], where the agents previous action is repeated with probability $p=0.25$ . ", "page_idx": 24}, {"type": "image", "img_path": "PnSTlFUfcd/tmp/700098f40fab7744fb68103baeca63da865362abccc93ce2ea4cfabab38780e2.jpg", "img_caption": ["Figure 6: Atari Seaquest environment [10, 50]. The goal is to rescue divers (small blue people), while shooting enemy sharks and submarines. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "888 In terms of safety properties we experiment with the following two properties, ", "page_idx": 24}, {"type": "text", "text": "889 ", "page_idx": 24}, {"type": "text", "text": "890 ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\bullet\\ (1)\\ (\\Sigma)-s u r f a c e\\to\\Sigma(s u r f a c e\\to d i\\nu e r))\\wedge(\\Sigma)\\neg o u t\\neg o f\\neg o x y g e n)\\wedge(\\Sigma)\\neg h i t)}}\\\\ {{\\bullet\\ (2)\\,\\Sigma d i\\nu e r\\wedge\\neg s u r f a c e\\to\\diamond^{\\leq30}s u r f a c e}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "891 Property (1) states that after diving (i.e. not surface), the agent must only surface with a diver on   \n892 board, and never run out-of-oxygen and never get hit by an enemy. The size of the DFA for this   \n893 property is $|\\mathcal \u1e0a D \u1e0c |=4$ . Property (2) states that once a diver is on board the agent must surface within 30   \n894 timesteps (i.e. rescue the diver).   \n895 Hyperparameter settings. For our shielding approach almost all the hyperparameters are specified   \n896 in Appendix E. The only hyperparameter that varies is the model checking horizon $H$ . For property   \n897 (1) we use $H=30$ , empirically this seems adequate enough to avoid running out-of-oxygen and   \n898 begin surfacing in enough time. For property (2) we use $H=50$ , this is to avoid picking up a diver   \n899 at the bottom of the ocean where it may not be possible to return to the surface in 30 timesteps.   \n900 Extended discussion of results. First we provide slightly larger figures that than provided in the   \n901 main paper, see Figure 7   \n902 For both safety properties DreamerV3 with shielding obtains comparative performance in terms of   \n903 reward with the unmodified DreamerV3 baseline. Of course this baseline entirely ignores the safety   \n904 properties and simply maximizes reward. We remark on the differences between the safety properties   \n905 themselves, property (1) in particular specifies the natural safety properties of the environment, since   \n906 violating property (1) results in a death, the agent only start with 4 lives (and can gain one more ever   \n907 10000 points) and so satisfying property (1) is beneficial for long term reward, short the behaviour   \n908 satisfying property (1) is correlated with higher reward and we might expect the globally optimal   \n909 policy in the environment to never violated property (1). Property (2) specifies that once a diver is   \n910 recovered the submarine must return to the surface in 30 timesteps, we would not expect that the   \n911 globally optimal policy satisfies this property (2) rather we would expect to converge to a locally   \n912 optimal policy satisfying property (2) while still obtaining good reward.   \n913 With respect to the baseline DreamerV3 (LAG) which has access to the cost function, we see that in   \n914 both cases it fails to reliable learn a safe policy that simultaneously maximizes reward. For property   \n915 (2) DreamerV3 (LAG) appear to do slightly better in terms of safety, however when qualitatively   \n916 inspecting the runs for property (2) we see the DreamerV3 (LAG) agent intentionally get hit by   \n917 enemy submarines/sharks to re-spawn on the surface without actually having to navigate there. This   \n918 may be a more effective way to satisfy the safety property with high probability but it clearly leads to   \n919 worse long term reward. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "image", "img_path": "PnSTlFUfcd/tmp/3eb1c80251b5ae4e280989cf8d97f182027ae8b52deb11184d07ed476caf0176.jpg", "img_caption": ["Figure 7: Episode reward and violation rate for deep RL Atari Seaquest. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "920 E Hyperparameters & Implementation Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "921 E.1 Access to Code ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "922 To maintain a high standard of anonymity we provide code for the experiments run on \u2018colour\u2019   \n923 gridworld as supplementary material, rather than through GitHub. The colour gridworld environment   \n924 is implemented with the Gym [14] interface. Tabular Q-learning is implemented with numpy in Python,   \n925 the model checking procedures (both exact and Monte Carlo) are implemented with JAX [12] which   \n926 supports vectorized computation on GPU and CPU. The code for the Atari Seaquest experiments   \n927 are not currently available, although our code base was heavily derived from the code base for   \n928 Approximate Model-based Shielding (AMBS) [30], see https://github.com/sacktock/AMBS   \n929 (MIT License).   \n930 Training details. For collecting both sets of experiments we has access to 2 Nvidia Tesla A30   \n931 (24GB RAM) GPU and a 24-core/48 thread Intel Xeon CPU each with 32GB RAM. For the \u2018colour\u2019   \n932 gridworld experiments each run can take several minutes up to a day depending on which property is   \n933 being tested, for example one run for property (3) can take roughly 1.5 days as the product state space   \n934 is fairly large. For the Atari Seaquest experiments each run can take 8 hours to 1 day depending on   \n935 the precise configuration of DreamerV3, in general we see a slow down of $\\times2$ when using shielding   \n936 compared to the unmodified DreamerV3 baseline. Memory requirements may differ depending on   \n937 the DreamerV3 configuration used, for the xlarge DreamerV3 configuration 32GB of GPU memory   \n938 should suffice.   \n939 Statistical significance. Error bars are provided for each of our experiments. In particular, we report   \n940 5 random initializations (seeds) for each experiment, the error bars are non-parametric (bootstrap) $95\\%$   \n941 confidence intervals, provided by seaborn.lineplot with default parameters: errorbar $a\\mathbf{\\Psi}^{\\prime}=(\\mathbf{\\Psi}^{\\prime}\\cos\\mathbf{i}\\mathbf{\\Psi}^{\\prime}$ ,   \n942 95), $\\mathtt{n\\_b o o t=1000}$ . The error bars capture the randomness in the initialization of the DreamerV3   \n943 world model and policy parameters, the randomness of the environment and any randomness in the   \n944 batch sampling.   \n948 In this section we provide several ablation studies for the \u2018colour\u2019 gridworld environment. We test the   \n949 most significant hyperparameters and algorithmic components of our method including the baseline   \n950 (Q-learning with penalties). In particular we demonstrate the counter factual experiences is crucial   \n951 for learning the safety properties of the environment when the size of the corresponding DFA is non   \n952 trivial. We also experiment with using exact model checking \u2013 demonstrating that we don\u2019t loose   \n953 much by using statistical model checking procedures. Furthermore, we experiment with the cost   \n954 coefficient $C$ , the model checking horizon $H$ and the level of stochasticity $p$ . ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "table", "img_path": "PnSTlFUfcd/tmp/4bee09695971335bbc483e8e11e7eba766027da035d209ce2668a59ef45dcddb.jpg", "table_caption": [""], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "PnSTlFUfcd/tmp/65dccea9bc943727105bfbe8acf32e3cdae33057bc44cd77ba3327170cb1fb86.jpg", "table_caption": ["Table 4: Q-learning with counter factual experiences [43] "], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "PnSTlFUfcd/tmp/44a6854ad9a72888a712302e7c38d34a30d3990f2080daf9c7cca73969c34f6f.jpg", "table_caption": ["Table 5: Q-learning with shielding (Algorithm 1) "], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "PnSTlFUfcd/tmp/9db27bef5f2ba39c724e594fde0ac293cc49737c7031cd57236b3fc756380c05.jpg", "table_caption": [""], "table_footnote": [], "page_idx": 28}, {"type": "table", "img_path": "PnSTlFUfcd/tmp/94c5cab17552700b9b5b2c7c3f1098dbaef3f99f23df362039376417911195e7.jpg", "table_caption": ["Table 7: Augmented Lagrangian [7, 41, 72] "], "table_footnote": [], "page_idx": 29}, {"type": "table", "img_path": "PnSTlFUfcd/tmp/057e466ff3fdd50f7270c55abb14482c2d18280b5e222b52526bcb00a3fed9f9.jpg", "table_caption": ["Table 8: DreamerV3 with Shielding (Algorithm 5) "], "table_footnote": [], "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "955 F.1 Counter factual experiences ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "956 We run our method and the baseline (Q-learning with penalties) without counterfactual experiences   \n957 to train the \u2018backup policy\u2019 or penalized task policy (baseline).   \n958 For property (2) and (3) we see a significant   \n959 drop in safety performance, since learning to   \n960 respect the safety property over the much larger   \n961 product state space will require much more ex  \n962 perience and without exploiting the structure of   \n963 the DFA (using counter factual experiences) to   \n964 generate synthetic data the task behaviour will   \n965 be much more quickly learnt. For property (1),   \n966 the invariant property, we observe identical per  \n967 formance as the DFA is trivial (only 2 states),   \n968 and so counter factual experiences is essentially   \n969 redundant in this case. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "970 F.2 Exact model checking ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "971 We run our method (Shielding) with two differ  \n972 ent configurations: exact model checking with   \n973 the \u2018approximate\u2019 transition probabilities (learn  \n974 ing from experience) and exact model check  \n975 ing with the \u2018true\u2019 transition probabilities. We   \n976 compare these two methods to the configuration   \n977 used in the main paper: Monte Carlo (statisti  \n978 cal) model checking with the learned transition   \n979 probabilities.   \n980 In all cases we see that Shield (MC-Approx)   \n981 obtains almost identical performance to Shield   \n982 (Exact-True), which demonstrates that we don\u2019t   \n983 loose much by statistical model checking with   \n984 the learned probabilities, when for example we   \n985 don\u2019t have access to the transition probabilities   \n986 ahead of time, or the MDP is too large to ex  \n987 act model check. We see some variance with   \n988 Shield (Exact-Approx), which can be explained   \n989 by sub-optimal convergence in terms of reward,   \n990 although note that the safety performance is con  \n991 sistent with the other configurations. Perhaps ex  \n992 act model checking with an inaccurate model of   \n993 the transition probabilities restricts exploration   \n994 to areas of the state space that are actually safe. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 30}, {"type": "image", "img_path": "PnSTlFUfcd/tmp/48e7853fbb1c19d5d5f8ad0736993aba53806181b37dfc54cd576d8208c1e1e5.jpg", "img_caption": [], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "Figure 8: Episode reward and cost for Q-learning (Shield) and Q-learning (COST-CF) with and without counterfactual experiences (CF). ", "page_idx": 30}, {"type": "image", "img_path": "PnSTlFUfcd/tmp/3a52505252cedf89f951d227fc605243868302a6876ff5fe6de4822afebf4e2f.jpg", "img_caption": ["Figure 9: Episode reward and cost for Shield (Exact-True) \u2013 exact model checking with the \u2018true\u2019 probabilities, Shield (Exact-Approx) - exact model checking with the learning transition probabilities, and Shield (MC-Approx) \u2013 from the main paper. "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "996 We experiment with different values for the cost coefficient $C$ used for our baseline (Q-learning with   \n997 penalties). In particular, we use $C\\in\\{0.1,1.0,10.0,100.0\\}$ , we expect that a larger cost coefficient   \n998 will penalize unsafe behaviour more harshly and result in \u2018safer\u2019 behaviour (i.e., fewer safety-property   \n999 violations).   \n1000 Unsurprisingly, across the board, by increasing   \n1001 the cost coefficient $C$ we obtain a policy that has   \n1002 fewer safety-property violations. The improved   \n1003 \u2018safety performance\u2019 is of course at the expense   \n1004 of reward or task performance, this is a trade-off   \n1005 we would expect. In particular for $C=100.0$   \n1006 we see that the learned policy essentially avoids   \n1007 the goal state (achieving zero reward) all but   \n1008 guaranteeing safety (no safety-violations). The   \n1009 purpose of this ablation study is to demonstrate   \n1010 that while we can achieve any desired level of   \n1011 safety by tuning the cost coefficient $C$ , the actual   \n1012 value of $C$ offers little to no semantic meaning   \n1013 for the probability of violating the safety prop  \n1014 erty. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "1015 F.4 Model checking horizon $H$ ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "1016 As was alluded to in the main paper, our method   \n1017 can be very sensitive to the model checking hori  \n1018 zon (hyperparameter) $H$ . In particular, if $H$ is   \n1019 too large then we might expect the system to ", "page_idx": 31}, {"type": "image", "img_path": "PnSTlFUfcd/tmp/f0d4afc7b2f49741e1743a7ce89327f967fab3d43c191e6dab00f025d28cbed1.jpg", "img_caption": [], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "Figure 10: Episode reward and cost for Q-learning (COST-CF) \u2013 baseline from the main paper, with different cost coefficients $C$ . ", "page_idx": 31}, {"type": "text", "text": "1020 exhibit overly conservative behaviour. As a rule of thumb we suggest that $H$ should be set to roughly   \n021 the shortest path in the DFA from the initial state to an accepting state \u2013 this can easily be computed   \n022 by using Dijkstra\u2019s (shortest-path) algorithm. In this ablation we experiment with much larger $H$   \n023 than recommended. This significantly impacts the performance of our proposed approach. However,   \n1024 we do propose a solution, Q-learning (Shield-Rec) which in short, checks that the action proposed by   \n1025 the \u2018task policy\u2019 is recoverable with the \u2018backup policy\u2019, or in other words by playing with the action   \n1026 $a\\sim\\pi_{t a s k}$ proposed by the \u2018task policy\u2019 We can still satisfy $\\operatorname*{Pr}(\\langle s,q\\rangle\\models\\diamond\\widecheck{\\neg}^{\\dim}\\widecheck{\\phantom{\\sum}}\\!\\!\\!a c\\widecheck{\\boldsymbol{c}}\\!\\!e p\\widecheck{t})\\leq p_{1}$ by using   \n1027 the \u2018backup policy\u2019 after playing $a$ .   \n1028 In general we observe that when $H$ is too large   \n1029 our original method (Shield) is overly conser  \n1030 vative, sacrificing reward or task performance   \n1031 for safety guarantees. Our proposed solution   \n1032 (Shield-Rec) is alleviates this issue partly, pro  \n1033 viding reasonable safety performance and com  \n1034 parable task performance. We note that this   \n1035 solution is clearly not perfect as is it appears   \n1036 to be slightly more permissive allowing more   \n1037 safety-violations than necessary. More investi  \n1038 gation into this framework would be interesting   \n1039 future work, and perhaps more hyperparameter   \n1040 tuning, specifically by tuning $p_{1}$ , could improve   \n1041 this method. The goal would be to obtain an al  \n1042 gorithm that is not overly sensitive to $H$ , and as   \n1043 long as $H$ is sufficiently big to guarantee safety   \n1044 we don\u2019t see much performance degradation by   \n1045 further increasing $H$ .   \n1047 Finally we investigate the effect of the level of stochasticity of the environment. Specifically, the value   \n1048 $p$ corresponding the the probability that the agent\u2019s action is ignored and another action is chosen   \n1049 (uniformly at random) from the action space and played instead. For example, of $p=0.25$ and the   \n1050 agent chooses the action Right, there is a $75\\%$ chance that the agent goes right and a $25\\%$ chance   \n1051 the agent goes a different direction. If $p=0.0$ (deterministic environment) then achieving complete   \n1052 safety (zero-violations) becomes easier as the agent has complete control of the environment through   \n1053 their actions.   \n1054 We experiment with the following $p$ values: $p=$   \n1055 0.1 for property (1), $p\\,=\\,0.1$ for property (2)   \n1056 and $p=0.05$ for property (3). For these smaller   \n1057 $p$ values we would expect it to be easier for   \n1058 our methods including the baseline to achieve   \n1059 a higher-rate of safety and possibly complete   \n1060 safety in some cases.   \n1061 We see a similar situation as in the main paper,   \n1062 Q-learning (without penalties) simply finds the   \n1063 best policy ignoring costs. However, Q-learning   \n1064 (with penalties) is able to obtain the same perfor  \n1065 mance now as our method Q-learning (Shield),   \n1066 both in terms of reward and cost. With a smaller   \n1067 $p$ value the safety-property can be satisfied with   \n1068 higher probability while still visiting the goal   \n1069 state frequently and obtaining high reward. In   \n1070 particular, these $p$ values are chosen such that   \n1071 each of the safety properties can be satisfies with   \n1072 probability at least 0.9 from the goal state, thus   \n1073 penalizing safety-violations with $C=10.0$ ap  \n1074 pears to be enough to guarantee safety above   \n1075 0.9 at each timestep while still achieving high ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 31}, {"type": "image", "img_path": "PnSTlFUfcd/tmp/99c175ea7d289de15662b9a94d60ea652bfab9badc8c8849e1a225b80a6d6a85.jpg", "img_caption": ["Property 1 $p=0.25$ \uff0c $H=10$", "Figure 11: Episode reward and cost for Q-learning (Shield) - from the main paper, Q-learning (Shield) with bigger $H$ and Q-learning (Shield-Rec) with bigger $H$ . "], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 32}, {"type": "image", "img_path": "PnSTlFUfcd/tmp/b86eb98331af803b874b6b402a2c445058211f87f9a8b4bbb0dc550982f714c8.jpg", "img_caption": [], "img_footnote": [], "page_idx": 32}, {"type": "text", "text": "Figure 12: Episode reward and cost for Q-learning, Q-learning (COST-CF) and Q-learning (Shield) \u2013 all from the main paper. With smaller levels of stochasticity $p$ ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "1076 reward. For different values of $C$ we might expect the baseline to have a different performance   \n1077 profile. ", "page_idx": 32}, {"type": "text", "text": "1078 G Comparison to CMDP ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "1079 In this additional section we analyze the relationships between our problem setup and other common   \n1080 CMDP settings, for both the finite horizon and corresponding (discounted) infinite horizon problems. ", "page_idx": 32}, {"type": "text", "text": "1081 G.1 Finite Horizon ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "1082 For reference we restate Problem 4.1 here. ", "page_idx": 32}, {"type": "text", "text": "1083 Problem 4.1 (restated) (Step-wise bounded regular safety property constraint). Let $P_{s a f e}$ be a regular   \n1084 safety property, $\\mathcal{D}$ be the DFA such that $\\mathcal{L}(\\mathcal{D})=B a d P r e f(P_{s a f e})$ and $\\mathcal{M}$ be the $M D P$ ; ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\pi}V_{\\pi}\\quad s u b j e c t\\,t o\\quad\\operatorname*{Pr}\\left(\\left\\langle s_{t},q_{t}\\right\\rangle\\left\\vert=\\diamond^{\\leq H}a c c e p t\\right)\\leq p_{1}\\quad\\forall t\\in[0,T]\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "1085 where all probability is taken under the product Markov Chain $\\mathcal{M}_{\\pi}\\otimes\\mathcal{D}$ , $p_{1}\\in[0,1]$ is a probability   \n1086 threshold, $H$ is the model checking horizon and $T$ is the fixed episode length. ", "page_idx": 32}, {"type": "text", "text": "1087 G.1.1 Expected Cumulative Constraint ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "1088 First we restate Problem 4.4. ", "page_idx": 32}, {"type": "text", "text": "Problem 4.4 (restated) (Expected cumulative constraint [4, 58]). ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{\\pi}{\\operatorname*{max}}\\,V_{\\pi}}&{{}s u b j e c t\\,t o\\quad\\mathbb{E}_{\\langle s_{t},q_{t}\\rangle\\sim\\mathcal{M}_{\\pi}\\otimes\\mathcal{D}}\\left[\\sum_{t=0}^{T}\\mathcal{C}\\big(\\langle s_{t},q_{t}\\rangle\\big)\\right]\\le d_{1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "1089 where $d_{1}\\in\\mathbb{R}_{+}$ is the cost threshold and $T$ is the fixed episode length. ", "page_idx": 32}, {"type": "text", "text": "1090 Proposition G.1. A feasible policy $\\pi$ for Problem 4.1 with parameters $p_{1}\\in[0,1]$ is also a feasible   \n1091 policy for Problem 4.4 with parameter $d_{1}\\in\\mathbb{R}_{+}$ , provided that $d_{1}\\geq(T+1)\\cdot p_{1}$ . ", "page_idx": 33}, {"type": "text", "text": "1092 Proof. For $t\\in[0,T]$ we define, the following random variables, $X_{0},\\ldots,X_{T}$ , where ", "page_idx": 33}, {"type": "equation", "text": "$$\nX_{t}=\\mathcal{C}(\\langle s_{t},q_{t}\\rangle)=1\\left[a c c e p t\\in L^{\\prime}(\\langle s_{t},q_{t}\\rangle)\\right]\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "1093 where,", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[X_{t}\\right]=\\mathbb{E}\\left[1\\left[a c c e p t\\in L^{\\prime}(\\langle s_{t},q_{t}\\rangle)\\right]\\right]}\\\\ &{\\qquad=\\operatorname*{Pr}\\left(a c c e p t\\in L^{\\prime}(\\langle s_{t},q_{t}\\rangle)\\right)}\\\\ &{\\qquad\\quad\\leq p_{1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "1094 The argument is straightforward if at every timestep $t\\in[0,T]$ we have $\\operatorname*{Pr}(\\langle s_{t},q_{t}\\rangle\\models\\langle\\rangle^{\\leq H}a c c e p t)\\leq$   \n1095 $p_{1}$ then with probability $\\le~p_{1}$ we have accept $\\in\\ \\bar{L}(\\langle s_{t},q_{t}\\rangle)$ . Then, under mild assumptions   \n1096 (i.e. $\\mathcal{C}(\\langle s_{t},q_{t}\\rangle)<\\infty)$ we consider the following decomposition of the expected cumulative cost, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathbb{E}_{\\boldsymbol\\pi}\\left[\\sum_{t=0}^{T}\\mathcal{C}(\\langle s_{t},q_{t}\\rangle)\\right]=\\mathbb{E}_{\\boldsymbol\\pi}\\left[\\sum_{t=0}^{T}X_{t}\\right]}}\\\\ &{=\\mathbb{E}_{s_{0}\\sim\\mathcal{P}_{0}(\\cdot)}\\left[X_{0}\\right]+\\mathbb{E}_{s_{1}\\sim\\mathcal{P}_{1}(\\cdot)}\\left[X_{1}\\right]+\\ldots+\\mathbb{E}_{s_{T}\\sim\\mathcal{P}_{T}(\\cdot)}\\left[X_{T}\\right]}\\\\ &{=\\mathbb{E}_{\\boldsymbol\\pi}\\left[X_{0}\\right]+\\mathbb{E}_{\\boldsymbol\\pi}\\left[X_{1}\\right]+\\ldots+\\mathbb{E}_{\\boldsymbol\\pi}\\left[X_{T}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "1097 We replace the subscript $\\mathbf{\\Psi}^{\\bullet}\\langle s_{t},q_{t}\\rangle\\sim\\mathcal{M}_{\\pi}\\otimes\\mathcal{D}^{\\ast}$ here for brevity. Clearly by linearity of expectations   \n1098 this statement holds. Although it is worth noting that each expectation is taken under a different   \n1099 marginal state distribution (i.e. $\\mathcal{P}_{t}(\\cdot))$ , which depends on $\\pi$ (apart from the initial state distribution   \n1100 $\\mathcal{P}_{0}(\\bar{\\cdot}))$ . From now on we will write this is implicitly (i.e. Eq. 72), rather than writing the marginal   \n1101 state distribution (at time $t$ ) for each expectation. Using our earlier observations we can now bound   \n1102 the expected cumulative cost from above as follows, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathbb{E}_{\\boldsymbol\\pi}\\left[\\sum_{t=0}^{T}\\mathcal{C}(\\langle s_{t},q_{t}\\rangle)\\right]=\\mathbb{E}_{\\boldsymbol\\pi}\\left[X_{0}\\right]+\\mathbb{E}_{\\boldsymbol\\pi}\\left[X_{1}\\right]+\\ldots+\\mathbb{E}_{\\boldsymbol\\pi}\\left[X_{T-1}\\right]+\\mathbb{E}_{\\boldsymbol\\pi}\\left[X_{T}\\right]}}\\\\ &{\\leq(T+1)\\cdot p_{1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "1103 ", "page_idx": 33}, {"type": "text", "text": "1104 Proposition G.2. The converse is not strictly true, since there may be a feasible policy $\\pi$ for Problem   \n1105 4.4 with threshold $d_{1}\\leq(T+1)\\cdot p_{1}$ which does not satisfy the constraints of Problem 4.1. ", "page_idx": 33}, {"type": "text", "text": "1106 Proof. We want to prove the following statement, a policy $\\pi$ satisfying, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\pi}\\left[\\sum_{t=0}^{T}\\mathcal{C}(\\langle s_{t},q_{t}\\rangle)\\right]\\leq(T+1)\\cdot p_{1}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "1107 does not imply that, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(\\left\\langle s_{t},q_{t}\\right\\rangle\\bigg|=\\bigdiamond^{\\leq H}a c c e p t\\right)\\leq p_{1}\\quad\\forall t\\in[0,T]\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "1108 To prove this we will show that there may be some policy $\\pi$ that satisfies Eq. 75, but does not satisfy   \n1109 Eq. 76 at some timestep $t$ . For simplicity we consider the first timestep (i.e. $t=0$ ). First we assume   \n1110 $\\pi$ is such that Eq. 75 holds, assuming $H\\le T$ then clearly we have, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\boldsymbol\\pi}\\left[\\sum_{t=0}^{H}\\mathcal{C}(\\langle s_{t},q_{t}\\rangle)\\right]\\leq\\mathbb{E}_{\\boldsymbol\\pi}\\left[\\sum_{t=0}^{T}\\mathcal{C}(\\langle s_{t},q_{t}\\rangle)\\right]\\leq(T+1)\\cdot p_{1}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "1111 Let $\\operatorname{Pr}(\\langle s_{0},q_{0}\\rangle\\ \\vert=\\ \\langle\\rangle^{\\leq H}a c c e p t)$ denote the proportion of accepting paths from the initial state   \n1112 $s_{0}\\;\\sim\\;\\mathcal{P}_{0}(\\cdot)$ and automaton state $q_{0}\\ =\\ \\Delta(\\bar{\\mathcal{Q}}_{0},\\bar{L}(s_{0}))$ . Suppose $\\pi$ is such that $\\operatorname*{Pr}(\\langle s_{0},q_{0}\\rangle\\ \\in$   \n1113 $\\diamondsuit^{\\leq H}a c c e p t)>p_{1}$ . We note that for each path $\\rho\\in S^{\\omega}$ and corresponding $t r a c e(\\rho)\\in\\dot{\\Sigma^{\\omega}}$ such that   \n1114 $t r a c e(\\rho)\\vDash\\bigl\\Rightarrow\\Sigma^{\\leq H}a c c e_{\\cdot}$ pt the sum $\\textstyle\\sum_{t=0}^{H}{\\mathcal{C}}(\\langle s_{t},q_{t}\\rangle)\\geq1$ , and now we have, ", "page_idx": 33}, {"type": "equation", "text": "$$\n(T+1)\\cdot p_{1}\\geq\\mathbb{E}_{\\boldsymbol\\pi}\\left[\\sum_{t=0}^{T}\\mathcal{C}(\\langle s_{t},q_{t}\\rangle)\\right]\\geq\\mathbb{E}_{\\boldsymbol\\pi}\\left[\\sum_{t=0}^{H}\\mathcal{C}(\\langle s_{t},q_{t}\\rangle)\\right]>p_{1}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "1115 Now clearly for all $p_{1}\\in[0,1]$ and $T\\in\\mathbb{Z}_{+}$ the following holds, ", "page_idx": 34}, {"type": "equation", "text": "$$\np_{1}<(T+1)\\cdot p_{1}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "1116 This implies that there may exist some $\\pi$ satisfying Eq. 75 and such that $\\operatorname*{Pr}(\\langle s_{0},q_{0}\\rangle\\ \\ \\in$   \n1117 $\\diamondsuit{\\leq}^{H}a c c e p t)>p_{1}$ , i.e. does not satisfy Eq. 76 at timestep $t=0$ .   \n1118 Proposition G.3. A feasible policy $\\pi$ for Problem 4.4 with threshold $d_{1}\\leq p_{1}$ , satisfies $\\operatorname*{Pr}(\\langle s_{t},q_{t}\\rangle\\gets$   \n1119 $\\diamondsuit{\\leq}\\hat{H}_{a c c e p t})\\leq p_{1}$ for all $t\\in[0,T]$ . This bound is tight. ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "1120 Proof. Firstly, a feasible policy $\\pi$ for Problem 4.4 with threshold $d_{1}\\leq p_{1}$ clearly satisfies, ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\pi}\\left[\\sum_{t=0}^{T}\\mathcal{C}(\\langle s_{t},q_{t}\\rangle)\\right]\\leq p_{1}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "1121 Assuming $H\\le T$ , then this implies that for all $t^{\\prime}\\in[0,T-H]$ we have, ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\boldsymbol\\pi}\\left[\\sum_{t=t^{\\prime}}^{t^{\\prime}+H}\\mathcal{C}(\\langle s_{t},q_{t}\\rangle)\\right]\\leq\\mathbb{E}_{\\boldsymbol\\pi}\\left[\\sum_{t=0}^{T}\\mathcal{C}(\\langle s_{t},q_{t}\\rangle)\\right]\\leq p_{1}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "1122 Let $\\operatorname*{Pr}(\\langle s_{t^{\\prime}},q_{t^{\\prime}}\\rangle\\,\\vline=\\langle\\rangle^{\\leq H}a c c e p t)$ denote the proportion of accepting paths at timestep $t^{\\prime}$ , where $s_{t^{\\prime}}\\sim$   \n1123 $\\mathcal{P}_{t^{\\prime}}(\\cdot)$ . Here $\\mathcal{P}_{t^{\\prime}}(\\cdot)$ denotes the marginal state distribution at time $t^{\\prime}$ . Recall that for each path $\\rho\\in S^{\\omega}$   \n1124 and corresponding $t r a c e(\\rho)\\in\\Sigma^{\\omega}$ such that $t r a c e(\\rho)\\in\\diamond^{\\leq H}a c c e p t$ the sum $\\textstyle\\sum_{t=t^{\\prime}}^{t^{\\prime}+H}{\\mathcal{C}}(\\langle s_{t},q_{t}\\rangle)\\geq1$   \n1125 Without loss of generality fix some $t^{\\prime}\\in[0,T-H]$ and suppose that $\\operatorname*{Pr}(\\langle s_{t^{\\prime}},q_{t^{\\prime}}\\rangle\\,\\vert=\\diamond^{\\leq H}a c c e p t)>$   \n1126 $p_{1}$ . This implies that, ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\pi}\\left[\\sum_{t=0}^{T}\\mathcal{C}(\\langle s_{t},q_{t}\\rangle)\\right]\\ge\\mathbb{E}_{\\pi}\\left[\\sum_{t=t^{\\prime}}^{t^{\\prime}+H}\\mathcal{C}(\\langle s_{t},q_{t}\\rangle)\\right]>p_{1}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "1127 Which is a contradiction. Therefore, it must be the case that when Eq. 80 is satisfied then so is   \n1128 $\\operatorname*{Pr}(\\langle s_{t},q_{t}\\rangle\\models\\diamond\\llcorner s t_{a c c e p t}])\\leq p_{1}$ for all $t\\in[0,T-H]$ . For the remaining $t^{\\prime}\\in[T-H,T]$ a similar   \n1129 argument can be made, the only detail is to ensure the sum in Eq. 81 is up to $T$ rather than $t^{\\prime}+H$ .   \n1130 To prove that this bound is tight we can again show the possible existence of a counter example. In   \n1131 particular, we want to prove the following statement, a policy $\\pi$ satisfying, ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\pi}\\left[\\sum_{t=0}^{T}\\mathcal{C}(\\langle s_{t},q_{t}\\rangle)\\right]\\leq p_{1}+c\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "1132 for some constant $c>0$ , does not imply that, ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(\\left\\langle s_{t},q_{t}\\right\\rangle\\bigg|=\\bigdiamond^{\\leq H}a c c e p t\\right)\\leq p_{1}\\quad\\forall t\\in[0,T]\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "1133 We will show that there may exist some policy $\\pi$ that satisfies Eq. 83 but does not satisfy Eq. 84 at   \n1134 some timestep $t$ . Firstly, we assume $\\pi$ is such that Eq. 83 holds, this implies that for all $t^{\\prime}\\in[0,T\\!-\\!H]$   \n1135 we have, ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\boldsymbol\\pi}\\left[\\sum_{t=t^{\\prime}}^{t^{\\prime}+H}\\mathcal{C}(\\langle s_{t},q_{t}\\rangle)\\right]\\leq\\mathbb{E}_{\\boldsymbol\\pi}\\left[\\sum_{t=0}^{T}\\mathcal{C}(\\langle s_{t},q_{t}\\rangle)\\right]\\leq p_{1}+c\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "1136 Fix some $t^{\\prime}\\in[0,T-H]$ and once again let $\\operatorname*{Pr}(\\langle s_{t^{\\prime}},q_{t^{\\prime}}\\rangle\\,\\vline=\\diamondsuit^{\\leq H}a c c e p t)$ denote the proportion of   \n1137 accepting paths at timestep $t^{\\prime}$ . Suppose $\\pi$ is such that $\\mathrm{Pr}(\\langle\\dot{s}_{t^{\\prime}},q_{t^{\\prime}}\\rangle\\,\\equiv\\diamond\\Sigma^{H}\\dot{a}c c e p t)>p_{1}$ . Again recall   \n1138 that for each path $\\rho\\in S^{\\omega}$ and corresponding trace $t r a c e(\\rho)\\in\\Sigma^{\\omega}$ such that $t r a c e(\\rho)\\stackrel{\\cdot}{\\sim}\\stackrel{\\cdot}{\\sim}\\stackrel{\\cdot}{\\sim}\\stackrel{\\cdot}{=}\\stackrel{\\cdot}{=}\\stackrel{\\cdot}{=}\\stackrel{\\cdot}{=}\\stackrel{\\cdot}{=}\\longrightarrow$ t   \n1139 the sum $\\textstyle\\sum_{t=t^{\\prime}}^{t^{\\prime}+H}{\\mathcal{C}}(\\langle s_{t},q_{t}\\rangle)\\geq1$ , and so, ", "page_idx": 34}, {"type": "equation", "text": "$$\np_{1}+c\\geq\\mathbb{E}_{\\boldsymbol\\pi}\\left[\\sum_{t=0}^{T}\\mathcal{C}(\\langle s_{t},q_{t}\\rangle)\\right]\\geq\\mathbb{E}_{\\boldsymbol\\pi}\\left[\\sum_{t=t^{\\prime}}^{t^{\\prime}+H}\\mathcal{C}(\\langle s_{t},q_{t}\\rangle)\\right]>p_{1}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "1140 Now clearly for all $p_{1}\\in[0,1]$ and $c>0$ , the following holds, ", "page_idx": 34}, {"type": "equation", "text": "$$\np_{1}<p_{1}+c\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "1141 This implies that there may exist some $\\pi$ satisfying Eq. 83 and such that $\\operatorname*{Pr}(\\langle s_{t^{\\prime}},q_{t^{\\prime}}\\rangle\\;\\;\\in$   \n1142 $\\diamondsuit{\\leq}^{H}a c c e p t)>p_{1}$ , i.e. does not satisfy Eq. 84 at timestep $t=t^{\\prime}$ . ", "page_idx": 34}, {"type": "text", "text": "1143 G.1.2 Probabilistic Cumulative Constraint ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "1144 First we restate Problem 4.5. Problem 4.5 (restated) (Probabilistic cumulative constraint [18, 56]). ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{\\pi}{\\operatorname*{max}}\\,V_{\\pi}}&{{}s u b j e c t\\,t o\\quad\\mathbb{P}_{\\langle s_{t},q_{t}\\rangle\\sim\\mathcal{M}_{\\pi}\\otimes\\mathcal{D}}\\left[\\sum_{t=0}^{T}\\mathcal{C}\\big(\\langle s_{t},q_{t}\\rangle\\big)\\leq d_{2}\\right]\\geq1-\\delta_{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "1145 where $d_{2}\\in\\mathbb{R}_{+}$ is the cost threshold, $\\delta_{2}$ is a tolerance parameter and $T$ is the fixed episode length.   \n1146 Proposition G.4. A feasible policy $\\pi$ for Problem 4.1 with parameters $p_{1}~\\in~[0,1]$ is also $a$   \n1147 feasible policy for Problem 4.5 with parameters $d_{2}~\\in~\\mathbb{R}_{+}$ and $\\delta_{2}~\\in~(0,1]$ , provided that,   \n1148 $d_{2}\\geq\\sqrt{(T+1)/2\\cdot\\log(1/\\delta_{2})}+(T+1)\\cdot p_{1}$ . ", "page_idx": 35}, {"type": "text", "text": "1149 Proof. For $t\\in[0,T]$ we define the following random variables, $X_{0},\\ldots,X_{T}$ , where, ", "page_idx": 35}, {"type": "equation", "text": "$$\nX_{t}=\\mathcal{C}(\\langle s_{t},q_{t}\\rangle)=1\\left[a c c e p t\\in L^{\\prime}(\\langle s_{t},q_{t}\\rangle)\\right]\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "1150 and we make the same following observation, ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[X_{t}\\right]=\\mathbb{E}\\left[1\\left[a c c e p t\\in L^{\\prime}(\\langle s_{t},q_{t}\\rangle)\\right]\\right]}\\\\ &{\\qquad=\\operatorname*{Pr}\\left(a c c e p t\\in L^{\\prime}(\\langle s_{t},q_{t}\\rangle)\\right)}\\\\ &{\\qquad\\quad\\leq p_{1}\\cdot\\delta}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "1151 See the proof of Prop. G.1 for details, the argument is identical. Once again, under mild assumptions   \n1152 (i.e. $\\bar{c}(\\bar{\\langle s_{t},q_{t}\\rangle})<\\bar{\\infty})$ ) we consider the following decomposition of the expected cumulative cost, ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathbb{E}_{\\boldsymbol\\pi}\\left[\\sum_{t=0}^{T}\\mathcal{C}(\\langle s_{t},q_{t}\\rangle)\\right]=\\mathbb{E}_{\\boldsymbol\\pi}\\left[X_{0}\\right]+\\mathbb{E}_{\\boldsymbol\\pi}\\left[X_{1}\\right]+\\ldots+\\mathbb{E}_{\\boldsymbol\\pi}\\left[X_{T}\\right]}}\\\\ &{\\leq\\left(T+1\\right)\\cdot p_{1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "1153 Again we replace the subscript $\\mathbf{\\Psi}^{\\bullet}\\langle s_{t},q_{t}\\rangle\\sim\\mathcal{M}_{\\pi}\\otimes\\mathcal{D}^{\\ast}$ here for brevity, see the proof of Prop. G.1 for the   \n1154 full details. Before we proceed we must first deal with the dependence between the random variables   \n1155 $X_{0},\\ldots,X_{T}$ . Strictly speaking it is not the case that $\\operatorname*{Pr}(X_{t}=1\\mid X_{t-1},\\ldots,X_{0})=\\operatorname*{Pr}(X_{t}=1)$ .   \n1156 However, we have already established that $\\operatorname*{Pr}(X_{t}=1)\\leq p_{1}$ , as such we can simulate $X_{0},\\ldots,X_{T}$   \n1157 as a sequence of independent coin flips $Y_{0},\\ldots,Y_{T}$ with probability $p_{1}$ , it is then the case that   \n1158 $\\begin{array}{r}{\\mathbb{P}[\\sum_{t=0}^{T}X_{t}>d_{2}]\\le\\mathbb{P}[\\sum_{t=0}^{T}Y_{t}>d_{2}]}\\end{array}$ . We can now continue by bounding the probability we care   \n1159 about, ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{1-\\mathbb{P}\\displaystyle\\left[\\sum_{t=0}^{T}C(\\langle u_{t},q\\rangle)\\leq d_{q}\\right]=\\mathbb{P}\\displaystyle\\left[\\sum_{t=0}^{T}C(\\langle u_{t},q\\rangle)>d_{q}\\right]}&{}\\\\ {=\\mathbb{P}\\displaystyle\\left[\\sum_{t=0}^{T}X_{t}>d_{q}\\right]}&{}\\\\ {\\leq\\mathbb{P}\\displaystyle\\left[\\sum_{t=0}^{T}V_{t}>d_{q}\\right]}&{}\\\\ {=\\mathbb{P}\\displaystyle\\left[\\sum_{t=0}^{T}Y_{t}>(T+1)\\cdot p_{t}+d_{q}-(T+1)\\cdot p_{t}\\right]}&{}\\\\ {=\\mathbb{P}\\displaystyle\\left[\\sum_{t=0}^{T}V_{t}>\\mathbb{E}\\left[\\sum_{t=0}^{T}V_{t}\\right]+d_{q}-(T+1)\\cdot p_{t}\\right]}&{}\\\\ {\\leq\\exp\\left(-\\frac{2\\cdot(d_{q}-(T+1)\\cdot p_{t})^{2}}{\\sum_{t=0}^{T}\\left(\\operatorname*{max}\\{Y_{t}\\}\\right)^{2}}\\right)}&{}\\\\ {=\\exp\\left(-\\frac{2\\cdot(d_{q}-(T+1)\\cdot p_{t})^{2}}{\\sum_{t=0}^{T}\\left(\\operatorname*{max}\\{Y_{t}\\}\\right)}\\right)}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "1160 The first inequality (Eq. 96) comes from our earlier construction and the second (Eq. 99) is obtained   \n1161 from Hoeffding\u2019s inequality [40] for bounded random variables. Finally, bounding the final expression   \n1162 from above by $\\delta_{2}$ and rearranging gives the desired result. \u53e3   \n1163 Proposition G.5. A feasible policy $\\pi$ for Problem 4.5 with parameters $\\delta_{2}\\leq p_{1}$ and $d_{2}<1$ , satisfies   \n1164 $\\mathrm{Pr}(\\bar{\\langle}s_{t},q_{t}\\rangle\\in\\diamond\\llangle^{\\leq H}a c c e p t)\\\\\\\\stackrel{\\cdot}{\\leq}p_{1}$ for all $t\\in[0,T]$ . This bound is tight. ", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "1165 Proof. A feasible policy $\\pi$ for Problem 4.5 with parameters $\\delta_{2}\\leq p_{1}$ and $d_{2}<1$ clearly implies that, ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\sum_{t=0}^{T}\\mathcal{C}(\\langle s_{t},q_{t}\\rangle)<1\\right]\\geq1-p_{1}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "1166 Assuming $H\\le T$ , then this implies that for all $t^{\\prime}\\in[0,T-H]$ we have, ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\sum_{t=t^{\\prime}}^{t^{\\prime}+H}\\mathcal{C}(\\langle s_{t},q_{t}\\rangle)<1\\right]\\geq\\mathbb{P}\\left[\\sum_{t=0}^{T}\\mathcal{C}(\\langle s_{t},q_{t}\\rangle)<1\\right]\\geq1-p_{1}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "1167 Let $\\operatorname*{Pr}(\\langle s_{t^{\\prime}},q_{t^{\\prime}}\\rangle\\,\\vline=\\langle\\rangle^{\\leq H}a c c e p t)$ denote the proportion of accepting paths at timestep $t^{\\prime}$ , where $s_{t^{\\prime}}\\sim$   \n1168 $\\mathcal{P}_{t^{\\prime}}(\\cdot)$ . Again $\\mathcal{P}_{t^{\\prime}}(\\cdot)$ denotes the marginal state distribution at time $t^{\\prime}$ . Recall that for each path $\\rho\\in S^{\\omega}$   \n1169 and corresponding $t r a c e(\\rho)\\in\\Sigma^{\\omega}$ such that trac $\\colon(\\rho)\\~\\vert=\\langle\\rangle^{\\leq H}$ accept the sum tt=+t\u2032H C(\u27e8st, qt\u27e9) \u22651.   \n1170 Without loss of generality fix some $t^{\\prime}\\in[0,T-H]$ and suppose that $\\operatorname*{Pr}(\\langle s_{t^{\\prime}},q_{t^{\\prime}}\\rangle\\,\\vert=\\diamond^{\\leq H}a c c e p t)>$   \n1171 $p_{1}$ . This implies that, ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\sum_{t=0}^{T}\\mathcal{C}(\\langle s_{t},q_{t}\\rangle)\\geq1\\right]\\geq\\mathbb{P}\\left[\\sum_{t=t^{\\prime}}^{t^{\\prime}+H}\\mathcal{C}(\\langle s_{t},q_{t}\\rangle)\\geq1\\right]>p_{1}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "1172 Which is a contradiction. Therefore, it must be the case that when Eq. 101 is satisfied then so is   \n1173 $\\operatorname*{Pr}(\\langle s_{t},q_{t}\\rangle\\models\\diamond\\llcorner s t_{a c c e p t}])\\leq p_{1}$ for all $t\\in[0,T-H]$ . For the remaining $t^{\\prime}\\in[T-H,T]$ a similar   \n1174 argument can be made, the only detail is to ensure the sum in Eq. 102 is up to $T$ rather than $t^{\\prime}+H$ . To   \n1175 prove that this bound is tight we can show the possible existence of a counter example. In particular,   \n1176 we want to prove the following statement, a policy $\\pi$ satisfying, ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\sum_{t=0}^{T}\\mathcal{C}(\\langle s_{t},q_{t}\\rangle)<1\\right]\\geq1-(p_{1}+c)\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "1177 for some constant $c>0$ does not imply that, ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(\\left\\langle s_{t},q_{t}\\right\\rangle\\bigg|=\\bigdiamond^{\\leq H}a c c e p t\\right)\\leq p_{1}\\quad\\forall t\\in[0,T]\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "1178 We will show that there may exist some policy $\\pi$ that satisfies Eq. 104 but does not satisfy Eq. 105   \n1179 at some timestep $t$ . Firstly, we assume $\\pi$ is such that Eq. 104 holds, this implies that for all   \n1180 $t^{\\prime}\\in[0,T-H]$ we have, ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\sum_{t=t^{\\prime}}^{t^{\\prime}+H}\\mathcal{C}(\\langle s_{t},q_{t}\\rangle)<1\\right]\\geq\\mathbb{P}\\left[\\sum_{t=0}^{T}\\mathcal{C}(\\langle s_{t},q_{t}\\rangle)<1\\right]\\geq1-(p_{1}+c)\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "1181 Fix some $t^{\\prime}\\in[0,T-H]$ and let $\\operatorname*{Pr}(\\left\\langle s_{t^{\\prime}},q_{t^{\\prime}}\\right\\rangle\\,\\left|=\\diamond^{\\leq H}a c c e p t)$ denote the proportion of accepting   \n1182 paths at timestep $t^{\\prime}$ . Suppose that $\\pi$ is such that $\\operatorname*{Pr}(\\langle s_{t^{\\prime}},q_{t^{\\prime}}\\rangle\\left\\vert=\\langle\\rangle^{\\leq H}a c c e p t\\right)>^{\\prime}\\!\\!p_{1}$ . Again recall that   \n1183 for each path $\\rho\\in S^{\\omega}$ and corresponding $\\t r a c e(\\rho)\\in\\Sigma^{\\omega}$ such that trac $\\bar{\\cdot}(\\rho)\\bar{\\left|-\\right.}\\bar{\\diamond}^{\\leq H}\\bar{a}^{c c t}$ pt the sum   \n1184 $\\textstyle\\sum_{t=t^{\\prime}}^{t^{\\prime}+H}{\\mathcal{C}}(\\langle s_{t},q_{t}\\rangle)\\geq1$ , and so, ", "page_idx": 36}, {"type": "equation", "text": "$$\np_{1}+c\\geq\\mathbb{P}\\left[\\sum_{t=0}^{T}\\mathcal{C}(\\langle s_{t},q_{t}\\rangle)\\geq1\\right]\\geq\\mathbb{P}\\left[\\sum_{t=t^{\\prime}}^{t^{\\prime}+H}\\mathcal{C}(\\langle s_{t},q_{t}\\rangle)\\geq1\\right]>p_{1}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "1185 Now clearly for all $p_{1}\\in[0,1]$ and $c>0$ , the following holds, ", "page_idx": 36}, {"type": "equation", "text": "$$\np_{1}<p_{1}+c\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "1186 This implies that there may exist some $\\pi$ satisfying Eq. 104 and such that $\\operatorname*{Pr}(\\langle s_{t^{\\prime}},q_{t^{\\prime}}\\rangle\\;\\;\\in$   \n1187 $\\diamondsuit{\\leq}^{H}a c c e p t)>p_{1}$ , i.e. does not satisfy Eq. 105 at timestep $t=t^{\\prime}$ . ", "page_idx": 36}, {"type": "text", "text": "1188 G.1.3 Instantaneous constraint ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "1189 First we restate Problem 4.6. ", "page_idx": 37}, {"type": "text", "text": "Problem 4.6 (restated) (Instantaneous constraint [23, 60, 69]). ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\pi}V_{\\pi}\\quad s u b j e c t\\:t o\\quad\\mathbb{P}_{\\langle s_{t},q_{t}\\rangle\\sim\\mathcal{M}_{\\pi}\\otimes\\mathcal{D}}\\big[\\mathcal{C}\\big(\\langle s_{t},q_{t}\\rangle\\big)\\leq d_{3}\\big]=1\\quad\\forall t\\in[0,T]\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "1190 Proposition G.6. A feasible policy $\\pi$ for Problem 4.6 with threshold $d_{3}<1$ (otherwise the problem   \n1191 is trivial) is a feasible policy for Problem 4.1 if and only $i f p_{1}=0$ .   \n1192 Proof. We start by proving the $4.6\\Rightarrow4.1$ direction. A feasible policy $\\pi$ for Problem 4.6 with $d_{3}<1$   \n1193 satisfies, ", "page_idx": 37}, {"type": "text", "text": "", "page_idx": 37}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(\\mathcal{C}(\\langle s_{t},q_{t}\\rangle)<1\\right)=1\\quad\\forall t\\in[0,T]\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "1194 which implies that, ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(\\mathcal{C}(\\langle s_{t},q_{t}\\rangle)=0\\right)=1\\quad\\forall t\\in[0,T]\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "1195 and by Defn. 4.3, ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(a c c e p t\\not\\in L^{\\prime}(\\langle s_{t},q_{t}\\rangle)\\right)=1\\quad\\forall t\\in[0,T]\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "1196 Then if for all $t\\in[0,T]$ , accept $\\not\\in L^{\\prime}(\\langle s_{t},q_{t}\\rangle)$ then we have $\\mathrm{Pr}(\\langle s_{0},q_{0}\\rangle\\not\\in\\langle\\gamma a c c e p t)=1$ , where   \n1197 $q_{0}\\,=\\,\\Delta(Q_{0},L(s_{0})\\bar{\\big)}$ and by extension we have $\\operatorname*{Pr}(\\langle s_{t},q_{t}\\rangle\\not\\in\\mathcal{O}\\dot{a}c c e p t{\\leq}^{H})=1$ for all $t\\in[0,T]$   \n1198 This completes the proof of this direction.   \n1199 Now we prove the $4.1\\Rightarrow4.6$ direction. A policy $\\pi$ satisfying $\\operatorname*{Pr}(\\langle s_{t},q_{t}\\rangle\\:|=\\langle\\rangle a c c e p t^{\\leq H}))=0$ for all   \n1200 $t\\in[0,T]$ implies that $\\operatorname*{Pr}(\\langle s_{t},q_{t}\\rangle\\not\\models\\diamond\\dot{\\boldsymbol{\\alpha}}\\dot{c}e p t^{\\dot{\\leq}H})=1$ for all $t\\in[0,T]$ which implies the following, ", "page_idx": 37}, {"type": "text", "text": "", "page_idx": 37}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(a c c e p t\\not\\in L^{\\prime}(\\langle s_{t},q_{t}\\rangle)\\right)=1\\quad\\forall t\\in[0,T]\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "1201 and by Defn. 4.3, ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[\\mathcal{C}(\\langle s_{t},q_{t}\\rangle)=0\\right]=1\\quad\\forall t\\in[0,T]\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "1202 which implies that, ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[\\mathcal{C}(\\langle s_{t},q_{t}\\rangle)<1\\right]=1\\quad\\forall t\\in[0,T]\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "1203 which concludes the proof. ", "page_idx": 37}, {"type": "text", "text": "1204 G.2 Infinite Horizon ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "1205 While in this paper we only consider finite horizon problems with a fixed episode length $T$ , we note   \n1206 that we can also make a set of similar statements for the infinite horizon (discounted) setting. In this   \n1207 section we provide the corresponding statements and proofs for the infinite horizon setting. Firstly,   \n1208 we consider the following infinite horizon problem.   \n1209 Problem G.7 (Step-wise bounded regular safety property constraint). Let $P_{s a f e}$ be a regular safety   \n1210 property, $\\mathcal{D}$ be the DFA such that $\\mathcal{L}(\\mathcal{D})=B a d P r e f(P_{s a f e})$ and $\\mathcal{M}$ be the $M D P$ ; ", "page_idx": 37}, {"type": "text", "text": "", "page_idx": 37}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\tau}V_{\\pi}\\quad s u b j e c t\\,t o\\quad\\operatorname*{Pr}\\left(\\left\\langle s_{t},q_{t}\\right\\rangle\\bigg|=\\diamondsuit^{\\leq H}a c c e p t\\right)\\leq p_{1}\\quad\\forall t=0,1,2,\\ldots.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "1211 where all probability is taken under the product Markov chain $\\mathcal{M}_{\\pi}\\otimes\\mathcal{D}_{\\pi}$ , $p_{1}\\in[0,1]$ is a probability   \n1212 threshold $H$ is the model checking horizon . ", "page_idx": 37}, {"type": "text", "text": "1213 G.2.1 Expected Cumulative Constraint ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Problem G.8 (Expected cumulative constraint). ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{\\pi}{\\operatorname*{max}}\\,V_{\\pi}}&{{}s u b j e c t\\,t o\\quad\\mathbb{E}_{\\langle s_{t},q_{t}\\rangle\\sim\\mathcal{M}_{\\pi}\\otimes\\mathcal{D}}\\Big[\\sum_{t=0}^{\\infty}\\gamma^{t}\\mathcal{C}\\big(\\langle s_{t},q_{t}\\rangle\\big)\\Big]\\le d_{1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "1214 where $d_{1}\\in\\mathbb{R}_{+}$ is the cost threshold and $\\gamma\\in[0,1)$ is the discount factor. ", "page_idx": 37}, {"type": "text", "text": "1215 Proposition G.9. A feasible policy $\\pi$ for Problem $G.7$ with parameters $p_{1}\\in[0,1]$ , is also a feasible   \n1216 policy for Problem G.8 with parameter $d_{1}\\in\\mathbb{R}_{+}$ , provided that $d_{1}\\geq T\\cdot p_{1}$ , where $T=1/(1-\\gamma)$ is   \n1217 the effective horizon. ", "page_idx": 37}, {"type": "text", "text": "1218 Proof. For $t=0,1,2,\\ldots$ we define, the following random variables, $X_{0},X_{1},X_{2},\\ldots,$ , where, ", "page_idx": 38}, {"type": "equation", "text": "$$\nX_{t}=\\mathcal{C}(\\langle s_{t},q_{t}\\rangle)=1\\left[a c c e p t\\in L^{\\prime}(\\langle s_{t},q_{t}\\rangle)\\right]\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "1219 where, ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[X_{t}\\right]=\\mathbb{E}\\left[1\\left[a c c e p t\\in L^{\\prime}(\\langle s_{t},q_{t}\\rangle)\\right]\\right]}\\\\ &{\\qquad=\\operatorname*{Pr}\\left(a c c e p t\\in L^{\\prime}(\\langle s_{t},q_{t}\\rangle)\\right)}\\\\ &{\\qquad\\quad\\leq p_{1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "1220 The argument for this is straightforward. If at every timestep $t=0,1,2,\\ldots$ we have $\\operatorname*{Pr}(\\langle s_{t},q_{t}\\rangle\\gets$   \n1221 $\\langle\\rangle^{\\leq H}a\\overline{{c}}\\overline{{c}}e p t)\\leq p_{1}$ then with probability $\\leq p_{1}$ we have accept $\\in\\mathcal{L}(\\langle s_{t},q_{t}\\rangle)$ . Let $T=1/(1-\\gamma)$   \n1222 be the effective horizon, then under mild assumptions (i.e. $\\mathcal{C}(\\langle s_{t},q_{t}\\rangle)<\\infty)$ we can consider the   \n1223 following decomposition of the expected cumulative cost, ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\boldsymbol\\pi}\\left[\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t}\\mathcal{C}(\\langle s_{t},q_{t}\\rangle)\\right]=\\mathbb{E}_{\\boldsymbol\\pi}\\left[\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t}X_{t}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\mathbb{E}_{s_{0}\\sim\\mathcal{P}_{0}(\\cdot)}\\left[X_{0}\\right]+\\gamma\\cdot\\mathbb{E}_{s_{1}\\sim\\mathcal{P}_{1}(\\cdot)}\\left[X_{1}\\right]+\\ldots}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\gamma^{T}\\cdot\\mathbb{E}_{s_{T}\\sim\\mathcal{P}_{T}(\\cdot)}\\left[X_{T}\\right]+\\ldots}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\mathbb{E}_{\\boldsymbol\\pi}\\left[X_{0}\\right]+\\gamma\\cdot\\mathbb{E}_{\\boldsymbol\\pi}\\left[X_{1}\\right]+\\ldots+\\gamma^{T}\\cdot\\mathbb{E}_{\\boldsymbol\\pi}\\left[X_{T}\\right]+\\ldots.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "1224 We replace the subscript $\\left\\langle s_{t},q_{t}\\right\\rangle\\sim\\mathcal{M}_{\\pi}\\otimes\\mathcal{D}^{\\ast}$ here for brevity. Clearly by linearty of expectations   \n1225 this statement holds. Although it is worth noting that each expectation is taken under a different   \n1226 marginal state distribution (i.e. $\\mathcal{P}_{t}(\\cdot)\\$ ), which depends on $\\pi$ (apart from the initial state distribution   \n1227 $\\mathcal{P}_{0}(\\cdot))$ . From now on we will write this is implicitly (i.e. Eq. 121), rather than writing the marginal   \n1228 state distribution (at time $t$ ) for each expectation. Using our earlier observations we can now bound   \n1229 the expected cumulative cost from above as follows, ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\boldsymbol\\pi}\\left[\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t}\\mathcal{C}(\\left\\langle s_{t},q_{t}\\right\\rangle)\\right]=\\mathbb{E}_{\\boldsymbol\\pi}\\left[X_{0}\\right]+\\gamma\\cdot\\mathbb{E}_{\\boldsymbol\\pi}\\left[X_{1}\\right]+\\ldots+\\gamma^{T}\\cdot\\mathbb{E}_{\\boldsymbol\\pi}\\left[X_{T}\\right]+\\ldots}\\\\ &{\\qquad\\qquad\\qquad\\leq p_{1}+\\gamma\\cdot p_{1}+\\ldots\\qquad+\\gamma^{T-1}\\cdot p_{1}+\\gamma^{T}\\cdot p_{1}+\\ldots}\\\\ &{\\qquad\\qquad\\qquad=p_{1}\\cdot\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t}=p_{1}\\cdot\\left(1/(1-\\gamma)\\right)=T\\cdot p_{1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "1230 ", "page_idx": 38}, {"type": "text", "text": "1231 Proposition G.10. The converse is not strictly true, since there may be a feasible policy $\\pi$ for   \n1232 Problem G.8 with threshold $d_{1}\\leq T\\cdot p_{1}$ which does not satisfy the constraints of Problem G.7 ", "page_idx": 38}, {"type": "text", "text": "1233 We want to prove the following statement, a policy $\\pi$ satisfying, ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\pi}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}\\mathcal{C}(\\langle s_{t},q_{t}\\rangle)\\right]\\leq T\\cdot p_{1}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "1234 does not imply that, ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(\\left\\langle s_{t},q_{t}\\right\\rangle\\bigg|=\\bigodot^{\\leq H}a c c e p t\\right)\\leq p_{1}\\quad\\forall t=0,1,2,\\ldots.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "1235 Proof. To prove this we will show that there may be some policy $\\pi$ that satisfies Eq. 125, but does   \n1236 not satisfy Eq. 126 at some timestep $t$ . For simplicity we consider the first timestep (i.e. $t=0$ ). First   \n1237 we assume $\\pi$ is such that Eq. 125 holds, then clearly we have, ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\pi}\\left[\\sum_{t=0}^{H}\\gamma^{t}\\mathcal{C}(\\langle s_{t},q_{t}\\rangle)\\right]\\le\\mathbb{E}_{\\pi}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}\\mathcal{C}(\\langle s_{t},q_{t}\\rangle)\\right]\\le T\\cdot p_{1}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "1238 Let $\\operatorname*{Pr}(\\langle s_{0},q_{0}\\rangle\\models\\diamond\\diamond\\varPsi_{a c c e p t})$ denote the proportion of accepting paths from the initial state $s_{0}\\sim$   \n1239 $\\mathcal{P}_{0}(\\cdot)$ . Suppose $\\pi$ is such that $\\operatorname{\\dot{Pr}}(\\langle s_{0},q_{0}\\rangle\\models\\langle\\dot{\\odot}^{\\dot{\\le}H}a c c e p t)>\\dot{p_{1}}$ . We note that for each path $\\rho\\in S^{\\omega}$   \n1240 and corresponding $t r a c e(\\rho)\\in\\Sigma^{\\omega}$ such that trace $(\\rho)\\ensuremath{\\left\\vert=\\diamond^{\\leq H}\\right\\vert}$ accept the sum $\\textstyle\\sum_{t=0}^{H}\\gamma^{t}{\\mathcal{C}}(\\langle s_{t},q_{t}\\rangle)\\geq$   \n1241 $\\gamma^{H}$ , and so, ", "page_idx": 38}, {"type": "text", "text": "", "page_idx": 39}, {"type": "equation", "text": "$$\nT\\cdot p_{1}\\geq\\mathbb{E}_{\\pi}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}\\mathcal{C}(\\langle s_{t},q_{t}\\rangle)\\right]\\geq\\mathbb{E}_{\\pi}\\left[\\sum_{t=0}^{H}\\gamma^{t}\\mathcal{C}(\\langle s_{t},q_{t}\\rangle)\\right]>p_{1}\\cdot\\gamma^{H}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "1242 Now clearly for all $p_{1}\\in[0,1],\\gamma\\in[0,1)$ , $H\\in\\mathbb{Z}_{+}$ and $T=1/(1-\\gamma)$ the following holds, ", "page_idx": 39}, {"type": "equation", "text": "$$\np_{1}\\cdot\\gamma^{H}<T\\cdot p_{1}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "1243 This implies that there may exist some $\\pi$ satisfying Eq. 125 and such that $\\operatorname*{Pr}(\\langle s_{0},q_{0}\\rangle\\ \\in$   \n1244 $\\diamondsuit{\\leq}^{H}a c c e p t)>p_{1}$ , i.e. does not satisfy Eq. 126 at timestep $t=0$ . \u5382   \n1245 Proposition G.11. A feasible policy $\\pi$ for Problem 4.4 with threshold $d_{1}\\,\\leq\\,p_{1}\\cdot\\gamma^{T+H}$ satisfies   \n1246 $\\mathrm{Pr}(\\bar{\\langle}s_{t},q_{t}\\rangle\\gets\\diamond\\Sigma^{H}a c{c e p t})\\leq\\dot{p_{1}}$ up to the effective horizon $T=1/(1-\\gamma)$ . This bound is tight.   \n1247 Proof. Let $\\underline{{T}}=1/(1\\!-\\!\\gamma)$ be the effective horizon. A feasible policy $\\pi$ for Problem 4.4 with threshold   \n1248 $d_{1}\\leq p_{1}\\cdot\\gamma^{T+H}$ clearly satisfies, ", "page_idx": 39}, {"type": "text", "text": "", "page_idx": 39}, {"type": "text", "text": "", "page_idx": 39}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\pi}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}\\mathcal{C}(\\langle s_{t},q_{t}\\rangle)\\right]\\leq p_{1}\\cdot\\gamma^{T+H}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "1249 which implies that for all $t^{\\prime}\\in[0,T]$ we have, ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{p_{1}\\cdot\\gamma^{T+H}\\geq\\mathbb{E}_{\\boldsymbol\\pi}\\left[\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t}\\mathcal{C}(\\langle s_{t},q_{t}\\rangle)\\right]\\geq\\mathbb{E}_{\\boldsymbol\\pi}\\left[\\displaystyle\\sum_{t=t^{\\prime}}^{t^{\\prime}+H}\\gamma^{t}\\mathcal{C}(\\langle s_{t},q_{t}\\rangle)\\right]}&{}\\\\ {=\\mathbb{E}_{\\boldsymbol\\pi}\\left[\\displaystyle\\gamma^{t^{\\prime}}\\displaystyle\\sum_{t=t^{\\prime}}^{t^{\\prime}+H}\\gamma^{t-t^{\\prime}}\\mathcal{C}(\\langle s_{t},q_{t}\\rangle)\\right]}&{}\\\\ {=\\gamma^{t^{\\prime}}\\cdot\\mathbb{E}_{\\boldsymbol\\pi}\\left[\\displaystyle\\sum_{t=t^{\\prime}}^{t^{\\prime}+H}\\gamma^{t-t^{\\prime}}\\mathcal{C}(\\langle s_{t},q_{t}\\rangle)\\right]}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "1250 Let $\\mathrm{Pr}(\\langle s_{t^{\\prime}},q_{t^{\\prime}}\\rangle\\;\\in\\;\\diamond^{\\leq H}a c c e p t)$ denote the proportion of accepting paths at timestep $t^{\\prime}$ , where   \n1251 $s_{t^{\\prime}}\\;\\sim\\;\\mathcal{P}_{t^{\\prime}}(\\cdot)$ . Here $\\mathcal{P}_{t^{\\prime}}(\\cdot)$ denotes the marginal state distribution at time $t^{\\prime}$ . Recall that for   \n1252 each path $\\rho\\,\\in{\\mathcal{S}}^{\\omega}$ and corresponding $t r a c e(\\bar{\\rho})\\,\\in\\,\\Sigma^{\\omega}$ such that $t r a c e(\\rho)\\;\\in\\;\\diamond^{\\leq H}a c c e p t$ the sum   \n1253 $\\textstyle\\sum_{t=t^{\\prime}}^{t^{\\prime}+H}\\gamma^{t-t^{\\prime}}{\\mathcal{C}}(\\langle s_{t},q_{t}\\rangle)\\geq\\gamma^{H}$ . Without loss of generality fix some $t^{\\prime}\\,\\in\\,[0,T]$ and suppose that   \n1254 $\\operatorname*{Pr}(\\langle s_{t^{\\prime}},q_{t^{\\prime}}\\rangle\\,\\vert=\\diamond^{\\leq H}a c c e p t)>p_{1}$ . This implies that, ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\boldsymbol\\pi}\\left[\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t}\\mathcal{C}(\\langle s_{t},q_{t}\\rangle)\\right]\\ge\\gamma^{t^{\\prime}}\\cdot\\mathbb{E}_{\\boldsymbol\\pi}\\left[\\displaystyle\\sum_{t=t^{\\prime}}^{t^{\\prime}+H}\\gamma^{t-t^{\\prime}}\\mathcal{C}(\\langle s_{t},q_{t}\\rangle)\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\displaystyle>p_{1}\\cdot\\gamma^{H}\\cdot\\gamma^{t^{\\prime}}\\ge p_{1}\\cdot\\gamma^{T+H}}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "1255 Which is a contradiction. Therefore, it must be the case that when Eq. 130 is satisfied then so is   \n1256 $\\operatorname*{Pr}(\\langle s_{t},q_{t}\\rangle\\models\\diamond\\diamond\\boldsymbol{\\backslash}^{\\leq H}a c c e p t])\\leq p_{1}$ for all $t\\in[0,T]$ . To prove that this bound is tight we can again   \n1257 show the possible existence of a counter example. In particular, we want to prove the following   \n1258 statement, a policy $\\pi$ satisfying, ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\pi}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}\\mathcal{C}(\\langle s_{t},q_{t}\\rangle)\\right]\\le p_{1}\\cdot\\gamma^{T+H}+c\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "1259 for some constant $c>0$ , does not imply that, ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(\\left\\langle s_{t},q_{t}\\right\\rangle\\bigg|=\\bigdiamond^{\\leq H}a c c e p t\\right)\\leq p_{1}\\quad\\forall t\\in[0,T]\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "1260 We will show that there may exist some policy $\\pi$ that satisfies Eq. 136 but does not satisfy Eq. 137 at   \n1261 some timestep $t$ . For simplicity we consider timestep $t=T$ , although we note that with a little extra   \n1262 work we could come up with a proof for any $t\\in[0,T]$ . Firstly, we assume $\\pi$ is such that Eq. 136   \n1263 holds, then we have, ", "page_idx": 39}, {"type": "text", "text": "", "page_idx": 40}, {"type": "equation", "text": "$$\np_{1}\\cdot\\gamma^{T+H}+c\\geq\\mathbb{E}_{\\boldsymbol\\pi}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}\\mathcal{C}(\\langle s_{t},q_{t}\\rangle)\\right]\\geq\\mathbb{E}_{\\boldsymbol\\pi}\\left[\\sum_{t=T}^{T+H}\\gamma^{t}\\mathcal{C}(\\langle s_{t},q_{t}\\rangle)\\right]\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "1264 Let $\\operatorname*{Pr}(\\langle s_{T},q_{T}\\rangle\\models\\diamond\\diamond\\varTheta^{\\leq H}a c c e p t)$ denote the proportion of accepting paths at timestep $T$ . Suppose $\\pi$   \n1265 is such that $\\operatorname*{Pr}(\\langle s_{T},q_{T}\\rangle\\models\\diamond\\llcorner t_{a c c e p t})>p_{1}$ . We note that for each path $\\rho\\in S^{\\omega}$ and corresponding   \n1266 $t r a c e(\\rho)\\in\\Sigma^{\\omega}$ such that $t r a c e(\\rho)\\in\\odot^{\\leq H}c$ ccept the sum $\\begin{array}{r}{\\sum_{t=T}^{T+H}\\gamma^{t}\\mathcal{C}(\\langle s_{t},q_{t}\\rangle)\\geq\\gamma^{T+H}}\\end{array}$ , and so, ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p_{1}\\cdot\\gamma^{T+H}+c\\geq\\mathbb{E}_{\\boldsymbol\\pi}\\left[\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t}\\mathcal{C}(\\langle s_{t},q_{t}\\rangle)\\right]}\\\\ &{\\qquad\\qquad\\qquad\\geq\\mathbb{E}_{\\boldsymbol\\pi}\\left[\\displaystyle\\sum_{t=T}^{T+H}\\gamma^{t}\\mathcal{C}(\\langle s_{t},q_{t}\\rangle)\\right]}\\\\ &{\\qquad\\qquad\\qquad\\geq p_{1}\\cdot\\gamma^{T+H}}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "1267 Now clearly for all $p_{1}\\in[0,1],\\gamma\\in[0,1)$ , $c>0$ , $H\\in\\mathbb{Z}_{+}$ and $T=1/(1-\\gamma)$ , the following holds, ", "page_idx": 40}, {"type": "equation", "text": "$$\np_{1}\\cdot\\gamma^{T+H}<p_{1}\\cdot\\gamma^{T+H}+c\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "1268 This implies that there may exist some $\\pi$ satisfying Eq. 136 and such that $\\operatorname*{Pr}(\\langle s_{T},q_{T}\\rangle\\subseteq$   \n1269 $\\diamondsuit{\\leq}^{H}a c c e p t)>p_{1}$ , i.e. does not satisfy Eq. 137 at timestep $t=T$ . ", "page_idx": 40}, {"type": "text", "text": "1270 G.3 Probabilistic Cumulative Constraint ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Problem G.12 (Probabilistic cumulative constraint). ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{\\pi}{\\operatorname*{max}}\\,V_{\\pi}}&{{}s u b j e c t\\,t o\\quad\\mathbb{P}_{\\langle s_{t},q_{t}\\rangle\\sim M_{\\pi}\\otimes\\mathcal{D}}\\Big[\\sum_{t=0}^{\\infty}\\gamma^{t}\\mathcal{C}\\big(\\langle s_{t},q_{t}\\rangle\\big)\\leq d_{2}\\Big]\\geq1-\\delta_{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "1271 where $d_{2}\\in\\mathbb{R}_{+}$ is the cost threshold, $\\delta_{2}$ is a tolerance parameter and $\\gamma\\in[0,1)$ is the discount factor.   \n1272   \n1273 Proposition G.13. A feasible policy $\\pi$ for Problem $G.7$ with parameters $p_{1}\\,\\in\\,[0,1]$ , is also $a$   \n1274 feasible policy for Problem $G.l2$ with parameters $d_{2}\\in\\mathbb{R}_{+}$ and $\\delta_{2}\\,\\in\\,(0,1]$ , provided that, $d_{2}\\geq$   \n1275 $\\sqrt{(\\lceil\\log(T)\\rceil\\cdot T)/2\\cdot\\log(1/\\delta_{2})}+\\lceil\\log(T)\\rceil\\cdot T\\cdot p_{1}+1$ , where $T=1/(1-\\gamma)$ is the effective horizon.   \n1276 ", "page_idx": 40}, {"type": "text", "text": "", "page_idx": 40}, {"type": "text", "text": "1277 Proof. Again $t=0,1,2,\\ldots$ we define the following random variables, $X_{0},X_{1},X_{2},\\dots$ , where, ", "page_idx": 40}, {"type": "equation", "text": "$$\nX_{t}=\\mathcal{C}(\\langle s_{t},q_{t}\\rangle)=1\\left[a c c e p t\\in L^{\\prime}(\\langle s_{t},q_{t}\\rangle)\\right]\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "1278 and we make the following observation, ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[X_{t}\\right]=\\mathbb{E}\\left[1\\left[a c c e p t\\in L^{\\prime}(\\langle s_{t},q_{t}\\rangle)\\right]\\right]}\\\\ &{\\qquad=\\operatorname*{Pr}\\left(a c c e p t\\in L^{\\prime}(\\langle s_{t},q_{t}\\rangle)\\right)}\\\\ &{\\qquad\\quad\\leq p_{1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "1279 See the proof of Prop. G.9, the argument is identical. Under mild assumptions (i.e. $\\mathcal{C}(\\langle s_{t},q_{t}\\rangle)<\\infty)$   \n1280 we consider the following decomposition of the (undiscounted) expected cumulative cost up to   \n1281 timestep $\\lceil\\log(T)\\rceil\\cdot T-1$ , ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathbb{E}_{\\boldsymbol\\pi}\\left[\\sum_{t=0}^{\\lceil\\log(T)\\rceil\\cdot T-1}\\mathcal{C}(\\langle s_{t},q_{t}\\rangle)\\right]=\\mathbb{E}_{\\boldsymbol\\pi}\\left[X_{0}\\right]+\\mathbb{E}_{\\boldsymbol\\pi}\\left[X_{1}\\right]+\\ldots+\\mathbb{E}_{\\boldsymbol\\pi}\\left[X_{\\lceil\\log(T)\\rceil\\cdot T-1}\\right]}}\\\\ &{\\le\\lceil\\log(T)\\rceil\\cdot T\\cdot p_{1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "1282 Again we replace the subscript $\\mathbf{\\Psi}^{*}\\langle s_{t},q_{t}\\rangle\\sim\\mathcal{M}_{\\pi}\\otimes\\mathcal{D}^{*}$ here for brevity, see the proof of Prop. G.9 for   \n1283 more details. Before we proceed we must first deal with the dependence between the random variables   \n1284 $X_{0},...\\,X_{\\lceil\\log(T)\\rceil\\cdot T-1}$ . Strictly speaking it is not the case that $\\operatorname*{Pr}(X_{t}\\;=\\;1\\;\\;|\\;\\;X_{t-1},\\ldots,X_{0})\\;=$ ", "page_idx": 40}, {"type": "text", "text": "1285 $\\operatorname*{Pr}(X_{t}=1)$ . However, we have already established that $\\operatorname*{Pr}(X_{t}=1)\\leq p_{1}$ , as such we can simulate 11228867 $X_{0},\\ldots,X_{\\lceil\\log(T)\\rceil\\cdot T-1}$ t haas ta $Y_{0},\\ldots,Y_{\\lceil\\log(T)\\rceil\\cdot T-1}$ . itNh oprwo bwaeb icliatny $p_{1}$ $\\begin{array}{r}{\\mathbb{P}[\\sum_{t=0}^{\\lceil\\log(T)\\rceil\\cdot T-1}X_{t}>d_{2}]\\le\\mathbb{P}[\\sum_{t=0}^{\\lceil\\log(T)\\rceil\\cdot T-1}Y_{t}>d_{2}]}\\end{array}$ 1288 bound the probability that we care about, ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\varepsilon\\left[\\sum_{k=0}^{\\infty}\\sqrt{\\varepsilon(x_{\\infty}(x_{\\infty}))}\\leq a\\right]-\\varepsilon\\left[\\sum_{k=0}^{\\infty}\\sqrt{\\varepsilon(x_{\\infty}(x_{\\infty}))-\\varepsilon}\\right]}&{\\quad{\\mathrm{(abo)}}}\\\\ &{=\\left[\\sum_{k=0}^{\\infty}\\sqrt{\\varepsilon(x_{\\infty}x_{\\infty})}\\right]}\\\\ &{=\\left[\\sum_{k=1}^{\\infty}\\sum_{k=0}^{\\infty}\\sqrt{\\varepsilon(x_{\\infty}x_{\\infty})}\\right]}\\\\ &{\\quad=\\left[\\sum_{k=0}^{\\infty}\\sqrt{\\varepsilon(x_{\\infty}x_{\\infty})}\\right]\\times\\left(\\sum_{k=1}^{\\infty}\\sqrt{\\varepsilon(x_{\\infty}x_{\\infty})}\\right)}\\\\ &{\\leq\\varepsilon\\left[\\sum_{k=0}^{\\infty}\\sqrt{\\varepsilon(x_{\\infty}x_{\\infty})}\\right]\\times+\\varepsilon\\left[\\sum_{k={\\infty}}^{\\infty}\\sqrt{\\varepsilon(x_{\\infty}x_{\\infty})}\\right]}\\\\ &{\\leq\\varepsilon\\left[\\sum_{k={\\infty}}^{\\infty-1}\\sqrt{\\varepsilon(x_{\\infty}(x_{\\infty}))}\\right]}\\\\ &{\\leq\\varepsilon\\left[\\sum_{k={\\infty}}^{\\infty-1}\\sqrt{\\varepsilon(x_{\\infty}x_{\\infty})}\\right]\\times\\varepsilon\\left[\\sum_{k={\\infty}}^{\\infty-1}\\sqrt{\\varepsilon(x_{\\infty}(x_{\\infty}))}\\right]\\cdot\\varepsilon\\left[\\sum_{k={\\infty}}^{\\infty-1}\\right]}\\\\ &{\\quad=\\left[\\sum_{k={\\infty}}^{\\infty-1}\\sqrt{\\varepsilon(x_{\\infty}x_{\\infty})}\\right]\\times\\left[\\sum_{k={\\infty}}^{\\infty-1}\\sqrt{\\varepsilon(x_{\\infty}(x_{\\infty}))}\\right]\\cdot\\varepsilon\\left[\\sum_{k={\\infty}}^{\\infty-1}\\sqrt{\\varepsilon(x_{\\infty}x_{\\infty})}\\right]}\\\\ &{\\quad=\\varepsilon\\left[\\left[\\sum_{k=0}^{\\infty-1}\\sqrt{\\varepsilon(x_{\\infty}x_{\\infty})}\\right]\\times\\left[\\sum_{k={\\infty}}^{\\infty-1}\\sqrt{\\varepsilon(x_{\\infty}x_{\\infty})}\\right]\\cdot\\varepsilon\\left[\\sum_{k={\\infty}}^{\\infty-1}\\sqrt{\\varepsilon(x_{\\infty}x_{\\infty})}\\right]}\\\\ &{\\leq\\varepsilon\\left[-2\\varepsilon\\left(d_{{\\infty}}\\sqrt{\\varepsilon(x_{ \n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "1289 Here the first inequality (Eq. 152) comes from the following two facts, certainly   \n1290 $\\begin{array}{r}{\\sum_{t=0}^{\\lceil\\log(T)\\rceil\\cdot T-1}\\gamma^{t}X_{t}\\ \\stackrel{}{\\leq}\\ \\sum_{t=0}^{\\cdot\\lceil\\log(T)\\rceil\\cdot T-1}X_{t}}\\end{array}$ and we have that $\\begin{array}{r}{\\sum_{t=\\lceil\\log(T)\\rceil\\cdot T}^{\\infty}\\gamma^{t}X_{t}\\ \\leq\\ 1}\\end{array}$ . The sec  \n1291 ond fact is a little harder to see, first we note that $\\dim_{\\gamma\\to1}\\gamma^{T}=1/e$ , where $T=1/(1-\\gamma)$ is the   \n1292 effective horizon. Then we can rewrite, ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{t=\\lceil\\log(T)\\rceil\\cdot T}^{\\infty}\\gamma^{t}X_{t}=\\left(\\gamma^{\\lceil\\log(T)\\rceil\\cdot T}\\right)\\cdot\\left(\\displaystyle\\sum_{t=\\lceil\\log(T)\\rceil\\cdot T}^{\\infty}\\gamma^{t-\\lceil\\log(T)\\rceil\\cdot T}X_{t}\\right)}\\\\ &{\\quad=\\left((\\gamma^{T})^{\\lceil\\log(T)\\rceil}\\right)\\cdot\\left(\\displaystyle\\sum_{t=\\lceil\\log(T)\\rceil\\cdot T}^{\\infty}\\gamma^{t-\\lceil\\log(T)\\rceil\\cdot T}X_{t}\\right)}\\\\ &{\\quad\\leq\\left(\\displaystyle\\frac{1}{e}^{\\lceil\\log(T)\\rceil}\\right)\\cdot\\left(\\displaystyle\\frac{1}{1-\\gamma}\\right)\\leq\\left(\\displaystyle\\frac{1}{e}^{\\rceil\\log(T)}\\right)\\cdot T=\\displaystyle\\frac{1}{T}\\cdot T=1}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "1293 The second inequality (Eq. 153) comes from our earlier construction. The final inequality (Eq. 156)   \n1294 is obtained from Hoeffding\u2019s inequality [40] for bounded random variables. Finally, by bounding the   \n1295 final expression (Eq. 157) from above by $\\delta_{2}$ and rearranging gives the desired result. \u53e3   \n1296 Proposition G.14. A feasible policy $\\pi$ for Problem G.12 with parameters $\\delta_{2}\\leq p_{1}$ and $d_{2}<\\gamma^{T+H}$ ,   \n1297 satisfies $\\operatorname*{Pr}(\\langle s_{t},q_{t}\\rangle\\models\\diamond\\llcorner t_{a c c e p t})\\leq\\\"{p}_{1}$ up to the effective horizon $T=1/(1-\\gamma)$ . This bound is   \n1298 tight.   \n1299 Proof. A feasible policy $\\pi$ for Problem G.12 with parameters $\\delta_{2}\\,\\leq\\,p_{1}$ and $d_{2}\\,<\\,\\gamma^{T+H}$ clearly   \n1300 implies that, ", "page_idx": 41}, {"type": "text", "text": "", "page_idx": 41}, {"type": "text", "text": "", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}\\mathcal{C}(\\langle s_{t},q_{t}\\rangle)<\\gamma^{T+H}\\right]\\geq1-p_{1}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "1301 and certainly for all $t^{\\prime}\\in[0,T]$ we have that, ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{1-p_{1}\\leq\\mathbb{P}\\left[\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t}(\\varepsilon(s_{t},\\phi_{t}))<\\gamma^{T+H}\\right]}\\\\ &{\\qquad\\leq\\mathbb{P}\\left[\\displaystyle\\sum_{t=r}^{t+H}\\gamma^{t}(\\varepsilon(s_{t},\\phi_{t}))<\\gamma^{T+H}\\right]}\\\\ &{\\qquad=\\mathbb{P}\\left[\\displaystyle\\sum_{t^{\\prime}=t^{\\prime}}^{t^{\\prime}+H}\\gamma^{t}\\varepsilon^{t}\\varepsilon^{t}\\mathcal{C}((s_{t},\\phi_{t}))<\\gamma^{T+H}\\right]}\\\\ &{\\qquad=\\mathbb{P}\\left[\\displaystyle\\sum_{t=r}^{t^{\\prime}+H}\\gamma^{t-t^{\\prime}}\\varepsilon(\\varepsilon(s_{t},\\phi_{t}))<(\\gamma^{T+H}/\\gamma^{t})\\right]}\\\\ &{\\qquad\\leq\\mathbb{P}\\left[\\displaystyle\\sum_{t=r}^{t+H}\\gamma^{t-t^{\\prime}}\\varepsilon(\\varepsilon(s_{t},\\phi_{t}))<\\gamma^{H}\\right]}\\end{array},\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "1302 Let $\\mathrm{Pr}(\\left\\langle s_{t^{\\prime}},q_{t^{\\prime}}\\right\\rangle\\;\\left\\vert=\\;\\diamond^{\\leq H}a c c e p t)$ denote the proportion of accepting paths at timestep $t^{\\prime}$ , where   \n1303 $s_{t^{\\prime}}\\;\\sim\\;\\mathcal{P}_{t^{\\prime}}(\\cdot)$ . Here $\\mathcal{P}_{t^{\\prime}}(\\cdot)$ denotes the marginal state distribution at time $t^{\\prime}$ . Recall that for   \n1304 each path $\\rho\\,\\in{\\mathcal{S}}^{\\omega}$ and corresponding t $r a c e(\\bar{\\rho})\\,\\in\\,\\Sigma^{\\omega}$ such that $t r a c e(\\rho)\\;\\in\\;\\diamond^{\\leq H}a c c e p t$ the sum   \n1305 $\\textstyle\\sum_{t=t^{\\prime}}^{t^{\\prime}+H}\\gamma^{t-t^{\\prime}}{\\mathcal{C}}(\\langle s_{t},q_{t}\\rangle)\\,\\geq\\,\\gamma^{H}$ . Without loss of generality fix some $t^{\\prime}\\,\\in\\,[0,T]$ and suppose that   \n1306 $\\operatorname*{Pr}(\\langle s_{t^{\\prime}},q_{t^{\\prime}}\\rangle\\,\\vert=\\diamond^{\\leq H}a c c e p t)>p_{1}$ . This implies that, ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}\\mathcal{C}(\\langle s_{t},q_{t}\\rangle)\\geq\\gamma^{T+H}\\right]\\geq\\mathbb{P}\\left[\\sum_{t=t^{\\prime}}^{t^{\\prime}+H}\\gamma^{t-t^{\\prime}}\\mathcal{C}(\\langle s_{t},q_{t}\\rangle)\\geq\\gamma^{H}\\right]>p_{1}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "1307 Which is a contradiction. Therefore, it must be the case that when Eq. 161 is satisfied then so is   \n1308 $\\operatorname*{Pr}(\\langle s_{t},q_{t}\\rangle\\models\\diamond\\diamond\\boldsymbol{\\backslash}^{\\leq H}a c c e p t])\\leq p_{1}$ for all $t\\in[0,T]$ . To prove that this bound is tight we can show   \n1309 the possible existence of a counter example. In particular, we want to prove the following statement,   \n1310 a policy $\\pi$ satisfying, ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}{\\mathcal{C}}(\\langle s_{t},q_{t}\\rangle)<\\gamma^{T+H}\\right]\\geq1-(p_{1}+c)\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "1311 for some constant $c>0$ does not imply that, ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(\\left\\langle s_{t},q_{t}\\right\\rangle\\bigg|=\\bigdiamond^{\\leq H}a c c e p t\\right)\\leq p_{1}\\quad\\forall t\\in[0,T]\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "1312 We will show that there may exist some policy $\\pi$ that satisfies Eq. 168 but does not satisfy Eq. 169 at   \n1313 some timestep $t$ . Firstly, we assume $\\pi$ is such that Eq. 168 holds, this implies that for all $t^{\\prime}\\in[0,T]$   \n1314 we have, ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{1-(p_{1}+c)\\leq\\mathbb{P}\\left[\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t}\\mathcal{C}(\\langle s_{t},q_{t}\\rangle)<\\gamma^{T+H}\\right]}\\\\ &{}&{\\leq\\mathbb{P}\\left[\\displaystyle\\sum_{t=t^{\\prime}}^{t^{\\prime}+H}\\gamma^{t}\\mathcal{C}(\\langle s_{t},q_{t}\\rangle)<\\gamma^{T+H}\\right]}\\\\ &{}&{\\leq\\mathbb{P}\\left[\\displaystyle\\sum_{t=t^{\\prime}}^{t^{\\prime}+H}\\gamma^{t-t^{\\prime}}\\mathcal{C}(\\langle s_{t},q_{t}\\rangle)<\\gamma^{H}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "1315 Fix some $t^{\\prime}\\in[0,T]$ and let $\\operatorname*{Pr}(\\left\\langle s_{t^{\\prime}},q_{t^{\\prime}}\\right\\rangle\\)\\left\\vert=\\diamond^{\\leq H}a c c e p t\\right\\rangle$ denote the proportion of accepting paths   \n1316 at timestep $t^{\\prime}$ . Suppose that $\\pi$ is such that $\\mathrm{Pr}(\\langle s_{t^{\\prime}},\\mathring{q_{t^{\\prime}}}\\rangle^{\\prime}\\,|=\\,\\diamond^{\\leq H}a c c e p t)>\\,p_{1}$ . Again recall that   \n1317 for each path $\\rho\\in S^{\\omega}$ and corresponding trace $:\\!(\\rho)\\in\\Sigma^{\\omega}$ such that t $r a c e(\\rho)\\in\\odot^{\\leq H}$ accept the sum   \n1318 tt\u2032=+t\u2032H \u03b3t\u2212t\u2032C(\u27e8st, qt\u27e9) \u2265\u03b3H, and so, ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p_{1}+c\\geq\\mathbb{P}\\left[\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t}\\mathcal{C}(\\langle s_{t},q_{t}\\rangle)\\geq\\gamma^{T+H}\\right]}\\\\ &{\\qquad\\quad\\geq\\mathbb{P}\\left[\\displaystyle\\sum_{t=t^{\\prime}}^{t^{\\prime}+b}\\gamma^{t-t^{\\prime}}\\mathcal{C}(\\langle s_{t},q_{t}\\rangle)\\geq\\gamma^{H}\\right]>p_{1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "1319 Now clearly for all $p_{1}\\in[0,1]$ , and $c>0$ , the following holds, ", "page_idx": 43}, {"type": "equation", "text": "$$\np_{1}<p_{1}+c\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "1320 This implies that there may exist some $\\pi$ satisfying Eq. 168 such that $\\operatorname*{Pr}(\\langle s_{t^{\\prime}},q_{t^{\\prime}}\\rangle\\,\\vert=\\diamond^{\\leq H}a c c e p t)>$   \n1321 $p_{1}$ , i.e. does not satisfy Eq. 169 at timestep $t=t^{\\prime}$ . \u53e3 ", "page_idx": 43}, {"type": "text", "text": "1322 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Justification: The proof of safety guarantees (Theorem 6.5) is provided in Appendix C.5. Furthermore, we provide experimental results demonstrating the scalability of our approach in Section 7. ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 44}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 44}, {"type": "text", "text": "Justification: The limitations of our framework are discussed under in Section 7 in the paragraph Separating Reward and Safety, furthermore the limitations of our proposed method are also discussed and explored in Appendix F. ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 44}, {"type": "text", "text": "1373 3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: For the key proofs in the main paper we explicitly provide the assumptions used, the proofs of each theoretical result from the main paper can also be found in Appendix C. Guidelines: \u2022 The answer NA means that the paper does not include theoretical results. \u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced. \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems. \u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. \u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. 4. Experimental Result Reproducibility Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 45}, {"type": "text", "text": "", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: other than access to code, we provide pseudo-code for the algorithms used in our experiments (see Appendix A) ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "1374   \n1375   \n1376   \n1377   \n1378   \n1379   \n1380   \n1381   \n1382   \n1383   \n1384   \n1385   \n1386   \n1387   \n1388   \n1389   \n1390   \n1391   \n1392   \n1393   \n1394   \n1395   \n1396   \n1397   \n1398   \n1399   \n1400   \n1401   \n1402   \n1403   \n1404   \n1405   \n1406   \n1407   \n1408   \n1409   \n1410   \n1411   \n1412   \n1413   \n1414   \n1415   \n1416   \n1417   \n1418   \n1419   \n1420   \n1421   \n1422   \n1423   \n1424   \n1425   \n1426   \n1427 ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in ", "page_idx": 45}, {"type": "text", "text": "1428 some way (e.g., to registered users), but it should be possible for other researchers   \n1429 to have some path to reproducing or verifying the results.   \n1430 5. Open access to data and code   \n1431 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n1432 tions to faithfully reproduce the main experimental results, as described in supplemental   \n1433 material?   \n1434 Answer: [Yes]   \n1435 Justification: We provide access to the code for our first set of experiments in the supple  \n1436 mentary material, with a corresponding script to reproduce the results in the main paper. For   \n1437 the second set of experiments we provide directions to the code base that we adapted and   \n1438 throughout the paper and appendices we provide sufficient details to reproduce these results   \n1439 without too much difficulty.   \n1440 Guidelines:   \n1441 \u2022 The answer NA means that paper does not include experiments requiring code.   \n1442 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n1443 public/guides/CodeSubmissionPolicy) for more details.   \n1444 \u2022 While we encourage the release of code and data, we understand that this might not be   \n1445 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n1446 including code, unless this is central to the contribution (e.g., for a new open-source   \n1447 benchmark).   \n1448 \u2022 The instructions should contain the exact command and environment needed to run to   \n1449 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n1450 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n1451 \u2022 The authors should provide instructions on data access and preparation, including how   \n1452 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n1453 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n1454 proposed method and baselines. If only a subset of experiments are reproducible, they   \n1455 should state which ones are omitted from the script and why.   \n1456 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n1457 versions (if applicable).   \n1458 \u2022 Providing as much information as possible in supplemental material (appended to the   \n1459 paper) is recommended, but including URLs to data and code is permitted.   \n1460 6. Experimental Setting/Details   \n1461 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n1462 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n1463 results?   \n1464 Answer: [Yes]   \n1465 Justification: We provide a thorough description of the environmental settings in Appendix   \n1466 D.1 and D.2, furthermore, hyperparameters and details with regards to access to the code   \n1467 are provided in Appendix E.   \n1468 Guidelines:   \n1469 \u2022 The answer NA means that the paper does not include experiments.   \n1470 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n1471 that is necessary to appreciate the results and make sense of them.   \n1472 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n1473 material.   \n1474 7. Experiment Statistical Significance   \n1475 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n1476 information about the statistical significance of the experiments?   \n1477 Answer: [Yes] ,   \n1478 Justification: We provide error bars for all of our experiments, over 5 random initializations   \n1479 (seeds), provided by seaborn.lineplot, see Appendix E for details.   \n1480 Guidelines:   \n1481 \u2022 The answer NA means that the paper does not include experiments.   \n1482 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n1483 dence intervals, or statistical significance tests, at least for the experiments that support   \n1484 the main claims of the paper.   \n1485 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n1486 example, train/test split, initialization, random drawing of some parameter, or overall   \n1487 run with given experimental conditions).   \n1488 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n1489 call to a library function, bootstrap, etc.)   \n1490 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n1491 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n1492 of the mean.   \n1493 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n1494 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n1495 of Normality of errors is not verified.   \n1496 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n1497 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n1498 error rates).   \n1499 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n1500 they were calculated and reference the corresponding figures or tables in the text.   \n1501 8. Experiments Compute Resources   \n1502 Question: For each experiment, does the paper provide sufficient information on the com  \n1503 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n1504 the experiments?   \n1505 Answer: [Yes]   \n1506 Justification: We provide details the compute resources used for our experiments in Appendix   \n1507 E and the expected time and memory requirements for an individual run.   \n1508 Guidelines:   \n1509 \u2022 The answer NA means that the paper does not include experiments.   \n1510 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n1511 or cloud provider, including relevant memory and storage.   \n1512 \u2022 The paper should provide the amount of compute required for each of the individual   \n1513 experimental runs as well as estimate the total compute.   \n1514 \u2022 The paper should disclose whether the full research project required more compute   \n1515 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n1516 didn\u2019t make it into the paper).   \n1517 9. Code Of Ethics   \n1518 Question: Does the research conducted in the paper conform, in every respect, with the   \n1519 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n1520 Answer: [Yes]   \n1521 Justification: We have reviewed the NeurIPS Code of Ethics and we do not think the research   \n1522 in this paper has any particular ethical concerns, to the best of our ability we have tried to   \n1523 maintain anonymity.   \n1524 Guidelines:   \n1525 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n1526 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n1527 deviation from the Code of Ethics.   \n1528 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n1529 eration due to laws or regulations in their jurisdiction). ", "page_idx": 46}, {"type": "text", "text": "", "page_idx": 47}, {"type": "text", "text": "1530 10. Broader Impacts ", "page_idx": 47}, {"type": "text", "text": "1531 Question: Does the paper discuss both potential positive societal impacts and negative   \n1532 societal impacts of the work performed?   \n1533 Answer: [NA] .   \n1534 Justification: As our paper is mostly foundational we do not foresee any immediate positive   \n1535 or negative societal impact of this research.   \n1536 Guidelines:   \n1537 \u2022 The answer NA means that there is no societal impact of the work performed.   \n1538 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n1539 impact or why the paper does not address societal impact.   \n1540 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n1541 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n1542 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n1543 groups), privacy considerations, and security considerations.   \n1544 \u2022 The conference expects that many papers will be foundational research and not tied   \n1545 to particular applications, let alone deployments. However, if there is a direct path to   \n1546 any negative applications, the authors should point it out. For example, it is legitimate   \n1547 to point out that an improvement in the quality of generative models could be used to   \n1548 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n1549 that a generic algorithm for optimizing neural networks could enable people to train   \n1550 models that generate Deepfakes faster.   \n1551 \u2022 The authors should consider possible harms that could arise when the technology is   \n1552 being used as intended and functioning correctly, harms that could arise when the   \n1553 technology is being used as intended but gives incorrect results, and harms following   \n1554 from (intentional or unintentional) misuse of the technology.   \n1555 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n1556 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n1557 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n1558 feedback over time, improving the efficiency and accessibility of ML).   \n1559 11. Safeguards   \n1560 Question: Does the paper describe safeguards that have been put in place for responsible   \n1561 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n1562 image generators, or scraped datasets)?   \n1563 Answer: [NA] .   \n1564 Justification: We do not believe that the new assets provided in the paper pose any such   \n1565 risks.   \n1566 Guidelines:   \n1567 \u2022 The answer NA means that the paper poses no such risks.   \n1568 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n1569 necessary safeguards to allow for controlled use of the model, for example by requiring   \n1570 that users adhere to usage guidelines or restrictions to access the model or implementing   \n1571 safety filters.   \n1572 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n1573 should describe how they avoided releasing unsafe images.   \n1574 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n1575 not require this, but we encourage authors to take this into account and make a best   \n1576 faith effort.   \n1577 12. Licenses for existing assets   \n1578 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n1579 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n1580 properly respected? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "1582 Justification: We are the original creators/owners of the code used for the first set of   \n1583 experiments, for the second set of experiments we explicitly cite the paper and provide the   \n1584 URL for the code that we have adapted in this paper, which is available under the MIT   \n1585 License as stated in Appendix E.   \n1586 Guidelines:   \n1587 \u2022 The answer NA means that the paper does not use existing assets.   \n1588 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n1589 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n1590 URL.   \n1591 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n1592 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n1593 service of that source should be provided.   \n1594 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n1595 package should be provided. For popular datasets, paperswithcode.com/datasets   \n1596 has curated licenses for some datasets. Their licensing guide can help determine the   \n1597 license of a dataset.   \n1598 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n1599 the derived asset (if it has changed) should be provided.   \n1600 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n1601 the asset\u2019s creators.   \n1602 13. New Assets   \n1603 Question: Are new assets introduced in the paper well documented and is the documentation   \n1604 provided alongside the assets?   \n1605 Answer: [Yes]   \n1606 Justification: We provide   \n1607 Guidelines:   \n1608 \u2022 The answer NA means that the paper does not release new assets.   \n1609 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n1610 submissions via structured templates. This includes details about training, license,   \n1611 limitations, etc.   \n1612 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n1613 asset is used.   \n1614 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n1615 create an anonymized URL or include an anonymized zip file.   \n1616 14. Crowdsourcing and Research with Human Subjects   \n1617 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n1618 include the full text of instructions given to participants and screenshots, if applicable, as   \n1619 well as details about compensation (if any)?   \n1620 Answer: [NA] .   \n1621 Justification: The paper does not involve crowdsourcing nor research with human subjects.   \n1622 Guidelines:   \n1623 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n1624 human subjects.   \n1625 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n1626 tion of the paper involves human subjects, then as much detail as possible should be   \n1627 included in the main paper.   \n1628 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n1629 or other labor should be paid at least the minimum wage in the country of the data   \n1630 collector.   \n1631 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human ", "page_idx": 49}, {"type": "text", "text": "632 Subjects ", "page_idx": 49}, {"type": "text", "text": "1633 Question: Does the paper describe potential risks incurred by study participants, whether   \n1634 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n1635 approvals (or an equivalent approval/review based on the requirements of your country or   \n1636 institution) were obtained?   \n1637 Answer: [NA] .   \n1638 Justification: The paper does not involve crowdsourcing nor research with human subjects.   \n1639 Guidelines:   \n1640 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n1641 human subjects.   \n1642 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n1643 may be required for any human subjects research. If you obtained IRB approval, you   \n1644 should clearly state this in the paper.   \n1645 \u2022 We recognize that the procedures for this may vary significantly between institutions   \n1646 and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the   \n1647 guidelines for their institution.   \n1648 \u2022 For initial submissions, do not include any information that would break anonymity (if   \n1649 applicable), such as the institution conducting the review. ", "page_idx": 50}]