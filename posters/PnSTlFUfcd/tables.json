[{"figure_path": "PnSTlFUfcd/tables/tables_7_1.jpg", "caption": "Table 1: Safety properties", "description": "This table lists three regular safety properties used in the tabular RL experiments.  Property (1) is a simple invariant property stating that the green state must always be avoided.  Property (2) is more complex, requiring that if the goal state is reached, the blue state must be reached within 10 timesteps.  Property (3) is the most complex, requiring that if the goal state is reached, the purple state must be reached within 10 timesteps and remain for at least 5 consecutive timesteps. The properties are evaluated with different levels of stochasticity (p-value) in the environment.", "section": "7 Empirical Evaluation"}, {"figure_path": "PnSTlFUfcd/tables/tables_23_1.jpg", "caption": "Table 1: Safety properties", "description": "This table shows the three regular safety properties used in the tabular RL experiments.  Property (1) is a simple invariant property that states that the agent must always avoid the green state. Property (2) states that once the goal state is reached, the blue state must be reached within 10 steps.  Property (3) states that once the goal state is reached, the purple state must be reached within 10 steps and the purple state must hold for the next 5 timesteps.  The table also lists the probability (p) of a random action being chosen instead of the agent's selected action for each property.", "section": "7 Empirical Evaluation"}, {"figure_path": "PnSTlFUfcd/tables/tables_27_1.jpg", "caption": "Table 3: Q-learning", "description": "This table presents the hyperparameters used for the Q-learning algorithm in the paper's experiments.  It includes the learning rate (\u03b1), discount factor (\u03b3), exploration type (Boltzmann), and temperature (\u03c4).  The Boltzmann exploration type is a common method for balancing exploration and exploitation during reinforcement learning.", "section": "E Hyperparameters & Implementation Details"}, {"figure_path": "PnSTlFUfcd/tables/tables_27_2.jpg", "caption": "Table 4: Q-learning with counter factual experiences [43]", "description": "This table presents the hyperparameters used for Q-learning with counterfactual experiences in the colour gridworld experiment.  It lists the learning rate, discount factor, exploration type (Boltzmann), temperature parameter, and cost coefficient.", "section": "E.2 Colour Gridworld"}, {"figure_path": "PnSTlFUfcd/tables/tables_27_3.jpg", "caption": "Table 5: Q-learning with shielding (Algorithm 1)", "description": "This table presents the hyperparameters used for the Q-learning algorithm with shielding.  It specifies the type of model checking used (Monte Carlo), whether an approximate model was used (True), the shielding policy employed (Task), and the variables that change across experiments, including the number of samples, approximation error, failure probability, model checking horizon, and satisfaction probability.  The prior is listed as uninformative.  Finally, the table provides references to other tables detailing the hyperparameters for the 'task' and 'backup' policies.", "section": "Empirical Evaluation"}, {"figure_path": "PnSTlFUfcd/tables/tables_28_1.jpg", "caption": "Table 6: DreamerV3 [34]", "description": "This table lists the hyperparameters used for the DreamerV3 model in the Atari Seaquest experiments.  It is broken down into three sections: General, World Model, and Actor Critic.  Each section specifies various parameters such as batch size, replay capacity, number of layers/units, activation functions, loss scales, learning rates, and others. These settings are crucial for understanding the experimental setup and results in the deep RL section.", "section": "7 Empirical Evaluation"}, {"figure_path": "PnSTlFUfcd/tables/tables_29_1.jpg", "caption": "Table 7: Augmented Lagrangian [7, 41, 72]", "description": "This table presents the hyperparameters used for the augmented Lagrangian method in the deep RL experiments.  It shows the values used for the penalty multiplier, initial Lagrange multiplier, penalty power, cost coefficient, and cost threshold.  The penalty critic parameters are referenced to Table 6, indicating they use the same Actor Critic settings defined there.", "section": "E Hyperparameters & Implementation Details"}, {"figure_path": "PnSTlFUfcd/tables/tables_29_2.jpg", "caption": "Table 8: DreamerV3 with Shielding (Algorithm 5)", "description": "This table lists the hyperparameters used in the DreamerV3 with shielding algorithm.  It specifies values for approximation error, number of samples, failure probability, look-ahead/shielding horizon, satisfaction probability, and cost coefficient. It also indicates that the 'task policy' and 'backup policy' hyperparameters are detailed in Table 6. ", "section": "Empirical Evaluation"}]