[{"type": "text", "text": "Provably Faster Algorithms for Bilevel Optimization via Without-Replacement Sampling ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Junyi Li and Heng Huang Department of Computer Science, Institute of Health Computing University of Maryland College Park College Park, MD, 20742 junyili.ai@gmail.com, henghuanghh@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Bilevel Optimization has experienced significant advancements recently with the introduction of new efficient algorithms. Mirroring the success in single-level optimization, stochastic gradient-based algorithms are widely used in bilevel optimization. However, a common limitation in these algorithms is the presumption of independent sampling, which can lead to increased computational costs due to the complicated hyper-gradient formulation of bilevel problems. To address this challenge, we study the example-selection strategy for bilevel optimization in this work. More specifically, we introduce a without-replacement sampling based algorithm which achieves a faster convergence rate compared to its counterparts that rely on independent sampling. Beyond the standard bilevel optimization formulation, we extend our discussion to conditional bilevel optimization and also two special cases: minimax and compositional optimization. Finally, we validate our algorithms over both synthetic and real-world applications. Numerical results clearly showcase the superiority of our algorithms. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Bilevel optimization [51, 47] has received a lot of interest recently due to its wide-ranging applicability in machine learning tasks, including hyper-parameter optimization [39], meta-learning [57] and personalized federated learning [46]. A bilevel optimization problem is a type of nested two-level problems as follows: ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x\\in\\mathbb R^{p}}h(x):=f(x,y_{x})\\mathrm{~s.t.~}y_{x}=\\arg\\operatorname*{min}_{y\\in\\mathbb R^{d}}g(x,y),\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "which includes an outer problem $f(x,y)$ and a inner problem $g(x,y)$ . The outer problem $f(x,y)$ relies on the solution $y_{x}$ of the inner problem $g(x,y)$ . Eq. (1) can be solved through gradient descent: $x_{t+1}=x_{t}-\\eta\\nabla h(x_{t})$ , with $\\eta$ be the stepsize and $\\dot{\\nabla}h(x_{t})$ be gradient at the state $x_{t}$ . Specially, the gradient $\\nabla h(x)$ [17] is a function of the inner problem minimizer $y_{x}$ , first order derivatives $\\nabla_{x}f(x,y)$ , $\\nabla_{y}f(x,y)$ , $\\nabla_{y}g(x,y)$ and second order derivatives $\\nabla_{x y}g(x,y)$ , $\\nabla_{y^{2}}g(x,y)$ . In particular, $y_{x}$ can be solved though gradient descent: $\\boldsymbol{y}_{t+1}=\\boldsymbol{y}_{t}-\\gamma\\nabla_{\\boldsymbol{y}}g(\\boldsymbol{x},\\dot{\\boldsymbol{y}}_{t})$ with $\\gamma$ be the stepsize and $\\nabla_{y}g(x_{t},y_{t})$ be gradient at the iterate $\\left({{x}_{t}},{{y}_{t}}\\right)$ . In the standard setup, the outer and inner problems are defined over two datasets ${\\cal D}_{u}=\\{\\xi_{i},i\\in[m]\\}$ and ${\\mathcal{D}}_{l}=\\{\\zeta_{j},i\\in[n]\\}$ , respectively: ", "page_idx": 0}, {"type": "equation", "text": "$$\nf(x,y)={\\frac{1}{m}}\\sum_{i=1}^{m}f(x,y;\\xi_{i}),g(x,y)={\\frac{1}{n}}\\sum_{j=1}^{n}g(x,y;\\zeta_{j}),\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "Naturally, we sample from the datasets to estimate the first and second-order derivatives in the hyper-gradient formulation. And to guarantee convergence, previous approaches require the examples ", "page_idx": 0}, {"type": "text", "text": "Table 1: Comparisons of the Bilevel Opt. & Conditional Bilevel Opt. algorithms for finding an $\\epsilon$ -stationary point. We include comparison with stochastic gradient descent type of methods and a more comprehensive comparison including other acceleration methods can be found in Table 2 of Appendix. An $\\epsilon_{}$ -stationary point is defined as $\\|\\nabla h(x)\\|\\leq\\epsilon$ . $G c(f,\\epsilon)$ and $G c(g,\\epsilon)$ denote the number of gradient evaluations $w.r t$ . $f(x,y)$ and $g(x,y)$ ; $J V(g,\\epsilon)$ denotes the number of Jacobianvector products; $H V(g,\\epsilon)$ is the number of Hessian-vector products. $m$ and $n$ are the number of data examples for the outer and inner problems, in particular, $n$ is the maximum number of inner problems for conditional bilevel optimization. Our methods have a dependence over example numbers $(m a x(m,n))^{q}$ , $q$ is a value decided by without-replacement sampling strategy and can have value in $[0,1]$ (A herding-based permutation [33] can let $q=0$ ). ", "page_idx": 1}, {"type": "table", "img_path": "BNnZwbZGpm/tmp/7902e16c2de24fede3c288ad33a8fcac79fc99023a90e52e86e2610ba0b25aee.jpg", "table_caption": [], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "be mutually independent [25] even at the same state $(x,y)$ . For example, two different examples $\\zeta_{1}$ and $\\zeta_{2}$ are sampled to estimate $\\nabla_{y}g(x,y)$ and $\\nabla_{y^{2}}g(x,y)$ , respectively. This leads to extra computational cost in practice. Indeed, two backward passes are required to evaluate $\\nabla_{x}f(x,y)$ and $\\nabla_{y}\\bar{f}(x,y)$ , while five backward passes are needed to evaluate $\\nabla_{y}g(x,y)$ , $\\nabla_{y^{2}}g(x,y)$ and $\\nabla_{x y}g(x,y)$ for a given state of $(x,y)$ . In contrast, one backward pass are needed to evaluate the former and two backward passes for the latter if not sampling independently for each property. This leads to notable computational cost difference for the current large-scale machine learning models [4]. Therefore, it is beneficial to explore other example-selection strategies beyond independent sampling. More specifically, we aim to answer the following question in this work: Can we solve the bilevel optimization problem without using independent sampling? ", "page_idx": 1}, {"type": "text", "text": "Although example-selection is under-explored for bilevel optimization, it has been well studied in the single level optimization literature [2, 42, 44]. For single level problems: $\\operatorname*{min}_{\\;z\\in\\mathbb{R}^{p}}\\;f(x)\\;=$ x\u2208Rp $\\textstyle{\\frac{1}{n}}\\sum_{i=1}^{n}f(x;\\xi_{i})$ , we iteratively perform the stochastic gradient step $\\boldsymbol{x}_{t+1}=\\boldsymbol{x}_{t}-\\eta\\nabla f(\\boldsymbol{x}_{t},\\xi_{t})$ , where the sample gradient is used to estimate the full gradient and the example $\\xi_{t}$ is sampled according to some order. More specifically, the sampling schemes are roughly divided into two categories: withreplacement sampling and without-replacement sampling. Traditional stochastic gradient descent typically employs with-replacement sampling to ensure an unbiased estimation of the full gradient in each iteration. In contrast, the without-replacement sampling has biased estimation at each iteration. Despite this, when averaging the gradients of consecutive examples, without-replacement sampling can approximate the full gradient more efficiently [34]. In fact, any permutation-based withoutreplacement sampling has that the gradient estimation error contracts at rate of $O(K^{-2})$ [34] where $K$ is the number of steps, while stochastic gradient descent only has a rate of $O(K^{-1})$ . Inspired by the properties of without-replacement sampling demonstrated in the single-level optimization, we design the algorithm named WiOR-BO (and its variants) which performs without-replacement sampling for bilevel optimization. Naturally, WiOR-BO does not require the independent sampling as in previous methods. In fact, our algorithm enjoys favorable convergence rate. As shown in Table 1, WiOR-BO converges faster compared with their counterparts (e.g. StocBiO [25]) using independent sampling. Actually, WiOR-BO gets comparable convergence rate with methods using more complicated variance reduction techniques (e.g. MRBO [52]). In Table 2, we perform a more comprehensive comparison with other variance reduction techniques and in Section C of Appendix, we further compare WiOR-BO with these methods. ", "page_idx": 1}, {"type": "text", "text": "The contributions of our work can be summarized as: ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "1. We propose WiOR-BO which performs without-sampling to solve the bilevel optimization problem and converges to an \u03f5 stationary point with a rate of $\\mathrm{O}(\\epsilon^{-3})$ . This rate improves over the ${\\mathrm{O}}(\\epsilon^{-4})$ rate of its counterparts (e.g. stocBiO [25]) using independent sampling. 2. We propose WiOR-CBO for the conditional bilevel optimization problems and show that our algorithm converges to an $\\epsilon$ stationary point with a rate of $\\bar{\\mathrm{O}(\\epsilon^{-4})}$ . This rate improves over the $\\mathrm{O}(\\epsilon^{-6})$ rate of its counterparts (e.g. DL-SGD [23]) using independent-sampling. ", "page_idx": 1}, {"type": "text", "text": "3. We customize our algorithms to the special cases of minimax and compositional optimization problems and demonstrate similar favorable convergence rates. ", "page_idx": 2}, {"type": "text", "text": "Notations. $\\nabla\\left(\\nabla_{x}\\right)$ denotes full gradient (partial gradient, over variable $x$ ), higher order derivatives follow similar rules. $[n]$ represents sequence of integers from 1 to $n$ . $O(\\cdot)$ is the big $\\mathrm{o}$ notation, and we hide logarithmic terms. ", "page_idx": 2}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Bilevel Optimization has its roots tracing back to the 1960s, beginning with the regularization method proposed by [51], and then followed by many research works [14, 47, 43, 55]. In the field of machine learning, similar implicit differentiation techniques were used to solve Hyper-parameter Optimization [28, 6, 12]. Exact solutions for Bilevel Optimization solve the inner problem for each outer variable but are inefficient. More efficient algorithms solve the inner problem with a fixed number of steps, and use the \u2018back-propagation through time\u2019 technique to compute the hypergradient [13, 35, 16, 40, 45]. Recently, single loop algorithms [17, 22, 25, 26, 52, 9, 30, 24] are introduced to perform the outer and inner updates alternatively. Besides, other aspects of the bilevel optimization are also investigated: [23] studied a type of conditional bilevel optimization where the inner problem depends on the example of the outer problem; [53, 56] studied the Hessian-free bilevel optimization and proposed an algorithm with optimal convergence rate; [32] proposed an efficient single loop algorithm for the general non-convex bilevel optimization; [54, 31] studies the bilevel optimization in the federated learning setting. ", "page_idx": 2}, {"type": "text", "text": "Sample-selection in stochastic optimization has been extensively studied for the case of single level optimization problems [2, 3, 19, 38]. Beyond the with-replacement sampling adopted by SGD, various without-replacement strategies were also studied in the literature. The random-reshuffling strategy shuffles the data samples at the start of each training epoch, and its property was first studied in [42] via the non-commutative arithmetic-geometric mean conjecture and followed by [20, 19]. [44, 36] studied the shuffilng-once strategy, where the data samples are only shuffled at the start of the training once; [8] investigated data echoing where one data sample is repeatedly used. Quasi-Monte Carlo sampling is also applied in stochastic gradient descent [5, 34]. Recently, [34, 33] proposed to use average gradient error to study the convergence of various example order strategies and then proposed a herding-based sampling strategy [50] to proactively minimize the average gradient error. ", "page_idx": 2}, {"type": "text", "text": "3 Without-Replacement Sampling in Bilevel Optimization ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this work, we focus on the following finite-sum bilevel optimization problem: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x\\in\\mathbb{R}^{p}}h(x):=f(x,y_{x})=\\frac{1}{m}\\sum_{i=1}^{m}f(x,y_{x};\\xi_{i}),\\mathrm{~s.t.~}y_{x}=\\arg\\operatorname*{min}_{y\\in\\mathbb{R}^{d}}g(x,y)=\\frac{1}{n}\\sum_{j=1}^{n}g(x,y;\\zeta_{j})\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where both the outer and the inner problems are defined over a finite dataset, and we denote them as ${\\mathcal{D}}_{u}=\\{\\xi_{i},i\\in[m]\\}$ and $\\mathcal{D}_{l}=\\bar{\\{\\zeta_{j},j\\in[n]\\}}$ , respectively. Under mild assumptions, the hypergradient of $h(x)$ is: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla h(x)=\\nabla_{x}f(x,y_{x})-\\nabla_{x y}g(x,y_{x})u_{x},\\ u_{x}=\\nabla_{y^{2}}g(x,y_{x})^{-1}\\nabla_{y}f(x,y_{x})}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Our objective is to minimize $h(x)$ by exploiting Eq. (3) with examples from $\\mathcal{D}_{u}$ and $\\mathcal{D}_{l}$ at each step. More specifically, for a given example order $\\pi$ denoting the following example-selection sequence as $\\{\\xi_{t}^{\\pi}\\in\\mathcal{D}_{u}\\}$ and $\\{\\zeta_{t}^{\\pi}\\in\\mathcal{D}_{l}\\}$ for $t\\in[T]$ , where $T$ denotes the length of the sequence and can be arbitrarily large. We then perform the following update steps iteratively for $t\\in[T]$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{y_{t+1}=y_{t}-\\gamma_{t}\\nabla_{y}g(x_{t},y_{t};\\zeta_{t}^{\\pi})}\\\\ &{u_{t+1}=u_{t}-\\rho_{t}\\big(\\nabla_{y^{2}}g(x_{t},y_{t};\\zeta_{t}^{\\pi})u_{t}-\\nabla_{y}f(x_{t},y_{t};\\xi_{t}^{\\pi})\\big)}\\\\ &{x_{t+1}=x_{t}-\\eta_{t}\\big(\\nabla_{x}f(x_{t},y_{t};\\xi_{t}^{\\pi})-\\nabla_{x y}g\\big(x_{t},y_{t};\\zeta_{t}^{\\pi}\\big)u_{t}\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where Eq. (4) adopts the single-loop [30] update. More specifically, Eq. (4a) performs gradient descent to estimate the minimizer $y_{x}$ ; Eq. (4b) performs gradient descent to estimate $u_{x}$ defined in Eq. (3); and Eq. (4c) perform the gradient descent step given the estimation of $y_{x}$ and $u_{x}$ . ", "page_idx": 2}, {"type": "text", "text": "1: Input: Initial states $\\v x_{I}^{0}$ , $y_{I}^{0}$ and $u_{I}^{0}$ ; learning rates $\\{\\gamma_{i}^{r},\\rho_{i}^{r},\\eta_{i}^{r}\\},i\\in[I],r\\in[R],I:=\\operatorname{lcm}(m,n)$   \ndenotes the least common multiple of $m$ and $n$ .   \n2: for epochs $r=1$ to $R$ do   \n3: Randomly sample $I/m$ permutations of outer dataset and concatenate them to have $\\{\\xi_{i}^{\\pi},i\\in$   \n$[I]\\}$ , sample $I/n$ permutations of inner dataset and concatenate them to have $\\{\\zeta_{i}^{\\pi},i\\in[I]\\}$ ;   \n4: Set $y_{0}^{r}=y_{I}^{r-1}$ , $u_{0}^{r}=\\mathcal{P}_{\\iota}(u_{I}^{r-1})$ and $x_{0}^{r}=x_{I}^{r-1}$   \n5: for $i=0$ to $I-1$ do   \n6: $\\begin{array}{r l}&{y_{i+1}^{r}=y_{i}^{r}-\\gamma_{i}^{r}\\nabla_{y}g(x_{i}^{r},y_{i}^{r};\\zeta_{i}^{\\pi});u_{i+1}^{r}=u_{i}^{r}-\\rho_{i}^{r}\\big(\\nabla_{y^{2}}g(x_{i}^{r},y_{i}^{r};\\zeta_{i}^{\\pi})u_{i}^{r}-\\nabla_{y}f(x_{i}^{r},y_{i}^{r};\\xi_{i}^{\\pi})\\big);}\\\\ &{x_{i+1}^{r}=x_{i}^{r}-\\eta_{i}^{r}\\big(\\nabla_{x}f(x_{i}^{r},y_{i}^{r};\\xi_{i}^{\\pi})-\\nabla_{x y}g(x_{i}^{r},y_{i}^{r};\\zeta_{i}^{\\pi})u_{i}^{r}\\big);}\\end{array}$   \n7:   \n8: end for   \n9: end for ", "page_idx": 3}, {"type": "text", "text": "Note that at each step $t$ , we use a pair of examples $\\left\\{\\xi_{t}^{\\pi},\\zeta_{t}^{\\pi}\\right\\}$ to estimate the involved properties and require only three backward passes, i.e. $\\nabla f(x_{t},y_{t};\\xi_{t}^{\\pi})$ , $\\nabla g(x_{t},y_{t};\\zeta_{t}^{\\pi})$ and $\\nabla(\\nabla_{y}g(x_{t},y_{t};\\zeta_{t}^{\\pi}))$ . In contrast, if we independently sample examples for each property, seven backward passes are needed. For large-scale machine learning models where back-propagation is expensive, our method can lead to significant computation saving in practice. ", "page_idx": 3}, {"type": "text", "text": "Equation (4) is independent of the specific order of samples. It can accommodate both withreplacement and without-replacement sampling strategies. In particular, we consider two most widely-used without-replacement sampling methods: random-reshuffling and shuffle-once. For random-reshuffling, we set $T=R\\times l c m(m,n)$ , where $l c m(m,n)$ denotes the least common multiple of $m$ and $n$ and $R$ is a constant. More specifically, we generate $R\\times l c m(m,n)/m$ random permutations of examples in $\\mathcal{D}_{u}$ then concatenate them to get $\\left\\{\\xi_{t}^{\\pi}\\,\\in\\,\\mathcal{D}_{u}\\right\\}$ , and we follow similar procedure to generate $\\{\\zeta_{t}^{\\pi}\\in\\mathcal{D}_{u}\\}$ . In algorithm 1, we show our WiOR-BO algorithm for solving Eq. (2) with random-reshuffilng sampling. Note that in Line 4 of Algorithm 1, $\\mathcal{P}_{\\iota}(\\cdot)$ is the projection operation to an $\\iota$ -size ball and we perform projection to make $u_{t}$ be bounded during updates. Shuffleonce is another widely used without-replacement sampling strategy, where a single permutation is generated and reused for both $\\mathcal{D}_{u}$ and $\\mathcal{D}_{l}$ , and we can modify Line 3 of Algorithm 1 to incorporate shuffle-once. Finally, compared to independent sampling-based algorithm such as stocBiO [25], we require extra space to generate and store the orders of examples, but this cost is negligible compared to memory and computational cost of training large scale models. ", "page_idx": 3}, {"type": "text", "text": "3.1 Conditional Bilevel Optimization ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In Eq. (2), we assume the outer dataset $\\mathcal{D}_{u}$ and the inner dataset $\\mathcal{D}_{l}$ are independent with each other. However, it is also common in practice that the inner problem not only relies on the outer variable $x$ , but also the current outer example. This type of problems is known as conditional (contextual) bilevel optimization problems [23] in the literature and we consider the finite setting as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\in\\mathbb{R}^{p}}h(x):=f(x,y_{x})=\\frac{1}{m}\\sum_{i=1}^{m}f(x,y_{x}^{(\\xi_{i})},\\xi_{i}),\\mathrm{s.t.~}y_{x}^{(\\xi_{i})}=\\operatornamewithlimits{a r g\\,m i n}_{y\\in\\mathbb{R}^{d}}g^{(\\xi_{i})}(x,y)=\\frac{1}{n\\xi_{i}}\\sum_{j=1}^{n\\xi_{i}}g(x,y;\\zeta_{\\xi_{i},j}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Note that in Eq. (5), the outer problem is defined on a finite dataset $\\mathcal{D}_{u}=\\{\\xi_{i},i\\in[m]\\}$ , and for each example $\\xi_{i}\\in\\mathcal{D}_{u}$ , we have a inner problem $g^{(\\xi_{i})}(x,y)$ defined on a dataset $\\mathcal{D}_{l,\\xi_{i}}=\\{\\zeta_{\\xi_{i},j},j\\in[n_{\\xi_{i}}]\\}$ . The hyper-gradient of Eq. (5) is: ", "page_idx": 3}, {"type": "equation", "text": "$$\n7h(x)=\\frac{1}{m}\\sum_{i=1}^{m}\\big(\\nabla_{x}f(x,y_{x}^{\\xi_{i}};\\xi_{i})-\\nabla_{x y}g^{(\\xi_{i})}(x,y_{x}^{\\xi_{i}})u_{x}^{\\xi_{i}}\\big),u_{x}^{\\xi_{i}}=\\nabla_{y^{2}}g^{(\\xi_{i})}(x,y_{x})^{-1}\\nabla_{y}f(x,y_{x};\\xi_{i}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The key difference of Eq. (6) with Eq. (3) is that $y_{x}^{\\xi_{i}}$ and $u_{x}^{\\xi_{i}}$ are defined for each example $\\xi_{i}\\in\\mathcal{D}_{u}$ . To minimize $h(x)$ in Eq. (5), we assume a sample order $\\pi$ denoting the following example selection sequence as $\\{\\xi_{t}^{\\bar{\\pi}}\\in\\mathcal{D}_{u},t\\in[T]\\}$ , where $T$ denotes the example sequence length and can be arbitrarily ", "page_idx": 3}, {"type": "text", "text": "1: Input: Initial state $x^{0}$ , learning rates $\\{\\eta_{i}^{r}\\},i\\in[m],r\\in[R]$ .   \n2: for epochs $r=1$ to $R$ do   \n3: Randomly sample a permutation of outer dataset to have $\\{\\xi_{i}^{\\pi},i\\in[m]\\}$ and set $x_{0}^{r}=x_{m}^{r-1}$ or   \n$x_{0}$ if $r=1$ .   \n4: for $i=0$ to $m-1$ do   \n5: Initial states $y^{0}$ and $u^{0}$ , learning rates $\\{\\gamma_{j}^{s},\\rho_{j}^{s}\\},j\\in[n_{i}],s\\in[S]$ .   \n6: for $s=1$ to $S$ do   \n7: Sample a permutation of inner dataset to have $\\{\\zeta_{\\xi_{i},j}^{\\pi},j\\:\\in\\:[n_{i}]\\}$ , set $y_{0}^{s}\\,=\\,y_{n_{i}}^{s-1}$ and   \n$u_{0}^{s}=\\mathscr{P}_{\\iota}(u_{n_{i}}^{s-1})$ or $y_{0}^{s}=y_{0}$ and $u_{0}^{s}=u_{0}$ if $s=1$ .   \n8: for $j=0$ to $n_{i}-1$ do   \n9: $\\begin{array}{r}{y_{j+1}^{s}=y_{j}^{s}-\\gamma_{j}^{s}\\nabla_{y}g(x_{i}^{r},y_{j}^{s};\\zeta_{\\xi_{i},j}^{\\pi})}\\end{array}$ ;   \n10: $u_{j+1}^{s^{\\ast}\\ i}=u_{j}^{s}-\\rho_{j}^{s}\\big(\\bar{\\nabla}_{y^{2}}g(x_{i}^{r},y_{j}^{s};\\mathring{\\zeta}_{\\xi_{i},j}^{\\pi})u_{j}^{s}-\\nabla_{y}f(x_{i}^{r},y_{j}^{s};\\xi_{i}^{\\pi})\\big);$   \n11: end for   \n12: end for   \n13: $x_{i\\neq1}^{r}=x_{i}^{r}-\\eta_{i}^{r}(\\nabla_{x}f(x_{i}^{r},y_{n_{i}}^{S};\\xi_{i}^{\\pi})-\\nabla_{x y}g^{\\xi_{i}^{\\pi}}(x_{i}^{r},y_{n_{i}}^{S})u_{n_{i}}^{S});$   \n14: end for   \n15: end for ", "page_idx": 4}, {"type": "text", "text": "large. We then perform the following update steps iteratively: ", "page_idx": 4}, {"type": "equation", "text": "$$\nx_{t+1}=x_{t}-\\eta_{t}\\big(\\nabla_{x}f(x_{t},\\hat{y}_{t};\\xi_{t}^{\\pi})-\\nabla_{x y}g^{\\xi_{t}^{\\pi}}(x_{t},\\hat{y}_{t})\\hat{u}_{t}\\big)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\hat{y}_{t}$ and $\\hat{u}_{t}$ are estimations of $y_{x_{t}}^{\\xi_{t}^{\\pi}}\\ u_{x_{t}}^{\\xi_{t}^{\\pi}}$ xtt . We get them by performing the following rules $T_{l}$ steps over the sample order $\\{\\zeta_{\\xi_{t}^{\\pi},t_{l}}^{\\pi}\\in\\mathcal{D}_{l,\\xi_{t}^{\\pi}},t_{l}\\in[T_{l}]\\}\\}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\prime_{t_{l}+1}=y_{t_{l}}-\\gamma_{t_{l}}\\nabla_{y}g(x_{t},y_{t_{l}};\\zeta_{\\xi_{t}^{\\pi},t_{l}}^{\\pi}),u_{t_{l}+1}=u_{t_{l}}-\\rho_{t_{l}}(\\nabla_{y^{2}}g(x_{t},y_{t_{l}},\\zeta_{\\xi_{t}^{\\pi},t_{l}}^{\\pi})u_{t_{l}}-\\nabla_{y}f(x_{t},y_{t_{l}};\\xi_{t}^{\\pi}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where both the outer variable state $x_{t}$ and sample $\\xi_{t}^{\\pi}$ are fixed. We then set $\\hat{y}_{t}=y_{T_{l}}$ and $\\hat{u}_{t}=u_{T_{l}}$ in Eq. (7). Note that Eq. (7) and Eq. (8) perform a double-loop update rule [23]. Since the inner problem relies on the example of the outer problem, we compute $\\bar{y}_{x}^{\\xi_{i}}$ and $u_{x}^{\\xi_{i}}$ for each new state $x$ and sample $\\xi_{i}$ instead of reusing them as in the single loop update of Eq. (4). ", "page_idx": 4}, {"type": "text", "text": "Note that an important feature of our algorithm is that we select samples based on an example order $\\pi$ at each step instead of sampling independently. Similar to the unconditional bilevel optimization, various example orders can be incorporated. For random-reshuffling, we set $T=R m$ with $R$ be a constant. Then we generate $R$ random permutations of examples in $\\mathcal{D}_{u}$ and concatenate them to get the example order $\\{\\xi_{t}^{\\pi}\\,\\in\\,\\mathcal{D}_{u},t\\,\\in\\,[\\bar{T}]\\}$ . Similarly, we concatenate $S$ random permutations of the inner dataset $\\mathcal{D}_{l,\\xi_{t}}$ to get $\\left\\{\\zeta_{\\xi_{t}^{\\pi},t_{l}}^{\\pi}\\in\\mathcal{D}_{l,\\xi_{t}^{\\pi}},t_{l}\\in[T_{l}]\\right\\}\\right\\}$ with a sequence length $T_{l}=S\\times n_{\\xi_{i}}$ . We summarize this in Algorithm 2 and denote it as WiOR-CBO. We slightly abuse the notation to replace $n_{\\xi_{i}}$ with $n_{i}$ in the inner loop update for better clarity. ", "page_idx": 4}, {"type": "text", "text": "3.2 MiniMax and Compositional Optimization Problems ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Our discussion of without-replacement sampling for bilevel optimization can be customized for two important nested optimization problems: compositional optimization and minimax optimization problems, as they can be viewed as a special type of bilevel optimization problem. Suppose we let the outer problem only rely on $y$ , set the inner problem as $\\begin{array}{r}{g(\\dot{\\boldsymbol{x}},\\boldsymbol{y})=\\frac{1}{2}\\|\\dot{\\boldsymbol{y}}-\\boldsymbol{r}(\\boldsymbol{x})\\|^{2}}\\end{array}$ . Consider the finite case of $f(y)$ and $r(x)$ , we have the following compositional optimization problem: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x\\in\\mathbb{R}^{p}}h(x):=\\frac{1}{m}\\sum_{i=1}^{m}f(r(x);\\xi_{i})=\\frac{1}{m}\\sum_{i=1}^{m}f\\big(\\frac{1}{n}\\sum_{j=1}^{n}r(x;\\zeta_{j});\\xi_{i}\\big)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Then we can derive the hyper-gradient as: $\\nabla h(x)=\\nabla_{x}r(x)u_{x}$ , where $\\boldsymbol{u}_{x}=\\nabla_{y}f(\\boldsymbol{r}(x))$ , meanwhile, we have $\\nabla_{y}g(x,y)\\,=\\,\\dot{y}\\,-\\,\\dot{r}(x)$ . Then we can instantiate Algorithm 1 to get an algorithm for compositional optimization problem using without-replacement sampling (Algorithm 3 of Appendix). Similar to the SCGD algorithm [49], we track the exponential moving average of the inner state $y$ , besides we also track the exponential moving average of inner gradient (the update rule of $u$ in Algorithm 3). ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Next, similar to the unconditional case, the conditional compositional optimization problem can be specialized from the conditional bilevel optimization problem Eq. (5) to have: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x\\in\\mathbb{R}^{p}}h(x):=\\frac{1}{m}\\sum_{i=1}^{m}f(\\frac{1}{n_{i}}\\sum_{j=1}^{n_{i}}r(x;\\zeta_{\\xi_{i},j});\\xi_{i})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "and we can instantiate Algorithm 2 in the special case of Eq. (10) to get Algorithm 5 (in the Appendix). The minimax optimization can also be viewed as a special type of bilevel optimization. More specifically, if we have $g(x,y)=-f(x,y)$ in Eq. (2), then we get the following minimax problem: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x\\in\\mathbb{R}^{p}}h(x):=\\frac{1}{m}\\sum_{i=1}^{m}\\operatorname*{max}_{y\\in\\mathbb{R}^{d}}f(x,y;\\xi_{i})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In the special case of Eq. (11), the hyper-gradient $\\nabla h(x)$ in Eq. (3) degenerates to $\\nabla h(x)\\;=\\;$ $\\nabla_{x}f(x,\\bar{y}_{x})$ due to the fact that $\\nabla_{y}f(x,y_{x})\\;=\\;-\\nabla_{y}g(x,y_{x})\\;=\\;0$ by the optimality condition. Meanwhile, we have $\\nabla_{y}g(x,y)\\;=\\;-\\nabla_{y}f(x,y)$ . As a result, we can simplify Algorithm 1 to get an alternative update algorithm for minimax optimization using without-replacement sampling (Algorithm 4 in the Appendix). Note that due to the linearity of minimax problem $w.r t$ examples, the conditional case for minimax optimization degenerates to the unconditional case. [11, 7] also considers the without-replacement sampling for the minimax problem, however, they focus on the strongly-convex-strongly-concave and two-sided Polyak-\u0141ojasiewicz settings, in contrast, our WiOR-MiniMax can be applied to nonconvex-strongly-concave setting. ", "page_idx": 5}, {"type": "text", "text": "4 Theoretical Analysis ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we provide theoretical analysis of our algorithms. We first state some assumptions: ", "page_idx": 5}, {"type": "text", "text": "Assumption 4.1. Function $f(x,y;\\xi),\\xi\\;\\in\\;{\\mathcal{D}}_{u}$ , is possibly non-convex, $L$ -smooth and has $C_{f}$ - bounded gradient w.r.t. the variable $y$ . ", "page_idx": 5}, {"type": "text", "text": "Assumption 4.2. Function $g(x,y;\\zeta),\\zeta\\,\\in\\,\\mathcal{D}_{l}$ , is $\\mu$ -strongly convex w.r.t $y$ for any given $x$ , and $L$ -smooth. Furthermore, $\\nabla_{x y}g(x,y;\\zeta)$ and $\\nabla_{y^{2}}g(x,y;\\zeta)$ are Lipschitz continuous with constants $L_{x y}$ and $L_{y^{2}}$ respectively. ", "page_idx": 5}, {"type": "text", "text": "Assumption 4.3. For all examples $\\xi_{i}\\in D_{u}$ and states $(x,y)\\in\\mathbb{R}^{p+d}$ , there exists an upper bound A such that the sample gradient errors satisfy $\\begin{array}{r}{\\|\\nabla f(x,y;\\xi_{i})-\\nabla f(x,y)\\|\\le A.}\\end{array}$ . ", "page_idx": 5}, {"type": "text", "text": "Assumption 4.4. For all examples $\\zeta_{j}\\in D_{l}$ and states $(x,y)\\in\\mathbb{R}^{p+d}$ , there exists an upper bound A such that the sample gradient errors satisfy $\\|\\nabla g(x,y;\\zeta_{i})-\\nabla g(x,y)\\|\\leq A$ and $\\Vert\\nabla^{2}g(x,y;\\zeta_{i})-$ $\\nabla^{2}g(x,y)\\|\\leq A$ . ", "page_idx": 5}, {"type": "text", "text": "As stated in The assumption 4.1 and 4.2, we consider the non-convex-strongly-convex bilevel optimization problems, this class of problems is widely studied in bilevel optimization literature [17, 25].Assumption 4.3 and 4.4 guarantee that the example gradients are good estimation of full gradient and is widely used in the analysis of single-level finite-sum optimization problems [36]. Note that as the hyper-gradient (Eq. 3) involves second order derivatives, Assumption 4.4 also requires that the second order example gradients have bounded bias. ", "page_idx": 5}, {"type": "text", "text": "We next make the following assumptions about the example selection sequence. We assume $\\{\\xi_{t}^{\\pi}\\in$ $\\mathcal{D}_{u},t\\in[T]\\}$ and $\\{\\zeta_{t}^{\\pi}\\in\\mathcal{D}_{l},t\\in[T]\\}$ satisfy: ", "page_idx": 5}, {"type": "text", "text": "Assumption 4.5. Given sequences of samples: $\\{\\xi_{t}^{\\pi}\\,\\in\\,\\mathcal{D}_{u},t\\,\\in\\,[T]\\}$ and $\\{\\zeta_{t}^{\\pi}\\,\\in\\,\\mathcal{D}_{l},t\\,\\in\\,[T]\\}$ , we assume there exists some constants $\\alpha,C$ such that for any step $t$ and any interval $k>0$ , we have: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\Big\\|\\frac{1}{k}\\sum_{\\tau=t}^{t+k-1}\\nabla f\\big(x_{t},y_{t};\\xi_{\\tau}^{\\pi}\\big)-\\nabla f\\big(x_{t},y_{t}\\big)\\Big\\|^{2}\\leq k^{-\\alpha}C^{2}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "and similar bounds hold for $\\nabla g$ and $\\nabla^{2}g$ (See Appendix B.1 for the full version of this assumption.) ", "page_idx": 5}, {"type": "text", "text": "Assumption 4.5 measures how well the sample gradients approximate the full gradient over an interval on average. Similar assumptions are used to study the sample order in single level optimization problems [34]. If samples are selected independently at each step, we get $\\alpha=1$ and $C=A$ based on Assumption 4.3 and 4.4. For any permutation-based order (e.g. random-reshuffilng and shuffle-once), we can show the assumption holds with $\\alpha=2$ and $C=\\operatorname*{max}(m,n)\\times A$ . Note that the $C$ depends on the number of samples $m\\:(n)$ , this dependence can be reduced to $(\\operatorname*{max}(m,n))^{0.5}$ for sufficiently small learning rates [34, 37]. More recently, [33] proposed a herding-based algorithm which explicitly optimizes the gradient error in Assumption 4.5 and shows that we can reach $\\alpha=2$ and $C=O(A)$ . ", "page_idx": 6}, {"type": "text", "text": "Bilevel Optimization. We are ready to show the convergence of Algorithm 1. Firstly, we denote the following potential function $\\mathcal{G}_{t}:=\\bar{\\mathcal{G}}_{t}=h(x_{t})+\\phi_{y}\\|y_{t}-y_{x_{t}}\\|^{2}+\\bar{\\phi_{u}}\\|u_{t}-u_{x_{t}}\\|^{2}$ , where $\\phi_{y}$ and $\\phi_{u}$ are constants that relates to the smoothness parameters of $h(x)$ . Then we have: ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.6. Suppose Assumptions 4.1-4.4 are satisfied, and Assumption 4.5 is satisfied with some $\\alpha$ and $C$ for the example order we use in Algorithm $^{\\,l}$ . We choose learning rates $\\eta_{t}=\\eta$ , $\\gamma_{t}=c_{1}\\eta_{:}$ , $\\rho_{t}=c_{2}\\eta$ and denote $T=R\\times I$ be the total number of steps. Then for any pair of values $(E,\\,k)$ which has $T=E\\times k$ and $\\begin{array}{r}{\\eta\\le\\frac{1}{k\\tilde{L}_{0}}}\\end{array}$ , we have: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\frac1E\\sum_{e=0}^{E-1}\\|\\nabla h(x_{k e})\\|^{2}\\le\\frac{2\\Delta}{k E\\eta}+2\\tilde{L}^{2}k^{-\\alpha}C^{2}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $c_{1}$ $,\\;c_{2},\\;\\tilde{L}_{0},$ $\\tilde{L}$ are constants related to the smoothness parameters of $h(x)$ , $\\Delta$ is the initial sub-optimality, $R$ and $I$ are defined in Algorithm $^{\\,l}$ . ", "page_idx": 6}, {"type": "text", "text": "To reach an $\\epsilon$ stationary point, we choose $\\begin{array}{r}{\\eta=\\frac{1}{k\\tilde{L}_{0}}}\\end{array}$ , and for $(E,k)$ , we choose: $\\begin{array}{r}{E=\\frac{4\\tilde{L}_{0}\\Delta}{\\epsilon^{2}}}\\end{array}$ and $k=$ $\\left(\\frac{4\\tilde{L}^{2}C^{2}}{\\epsilon^{2}}\\right)^{1/\\alpha}$ , which means we need to choose the number of epochs $\\begin{array}{r}{R\\,=\\,\\frac{T}{I}\\,=\\,\\frac{4\\tilde{L}_{0}(4\\tilde{L}^{2}C^{2})^{1/\\alpha}\\Delta}{I\\epsilon^{2+2/\\alpha}}}\\end{array}$ Specially, if we choose examples independently at each step, we have $C=A$ and $\\alpha=1$ , then we need $\\bar{T}=O(\\epsilon^{-4})$ steps to reach an $\\epsilon_{}$ -stationary point. This recovers the rate of stocBiO algorithm [25] and SOBA [9]. Next, if we use some permutation-based without-replacement sampling strategy witch has $C=(\\operatorname*{max}(m,n))^{q}\\times A\\,(q\\in[\\bar{0},1])$ and $\\alpha=2$ , then we need $T=O((\\operatorname*{max}(\\bar{m},n))^{q}\\bar{\\epsilon}^{-3})$ . In particular, by adopting a herding-based permutation as in [33], we get $q=0$ , and our WiOR-BO strictly outperforms the independent sampling-based stocBiO and SOBA algorithms. ", "page_idx": 6}, {"type": "text", "text": "Conditional Bilevel Optimization. Next, we study the convergence rate of Algorithm 2. Besides Assumptions 4.3 and 4.4, we need the bias of sample hyper-gradient be bounded too: ", "page_idx": 6}, {"type": "text", "text": "Assumption 4.7. For all examples $\\xi_{i}\\in D_{u}$ and states $x\\in\\mathbb{R}^{p}$ , there exists an upper bound A such that the sample gradient errors satisfy $\\begin{array}{r}{\\|\\nabla h(x;\\xi_{i})-\\nabla h(x)\\|\\leq A}\\end{array}$ . ", "page_idx": 6}, {"type": "text", "text": "We also augment Assumption 4.5 to include the case of $\\nabla h(x)$ (Seem Appendix B.1 for more details.). Then we are ready to show the convergence rate, we use the potential function ${\\mathcal G}_{t}=h(x_{t})$ and have: ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.8. Suppose Assumptions 4.1-4.4 and 4.7 are satisfied, and Assumption 4.5 is satisfied with some $\\alpha$ and $C$ for the example order we use in Algorithm 2. We choose learning rates $\\begin{array}{r}{\\eta_{t}=\\dot{\\eta}=\\frac{1}{8k L}}\\end{array}$ , $\\begin{array}{r}{\\gamma_{t}=\\gamma=\\frac{1}{256k\\,L\\kappa}}\\end{array}$ , $\\begin{array}{r}{\\rho_{t}=\\rho=\\frac{1}{512k L\\kappa}}\\end{array}$ and denote $T=R\\times m$ be the total number of outer steps and $T_{l}=S\\times\\operatorname*{max}(n_{i})$ be the maximum inner steps. Then for any pair of values $(E,\\,k)$ which has $T=E\\times k$ and $T_{l}=E_{l}\\times k$ , we have: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\frac{1}{E}\\sum_{e=0}^{E-1}\\|\\nabla h(x_{k e})\\|^{2}\\leq\\frac{8\\Delta_{h}}{k E\\eta}+C_{u}(1-\\frac{k\\mu\\rho}{2})^{E_{l}}\\Delta_{u}+C_{y}(1-\\frac{k\\mu\\gamma}{2})^{E_{l}}\\Delta_{y}+(C^{'})^{2}C^{2}k^{-\\alpha}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $C_{u},\\,C_{y},\\,C^{'},$ $\\tilde{L}$ are constants related to the smoothness parameters of $h(x),\\,\\Delta_{h},\\,\\Delta_{u},\\,\\Delta_{y}$ are the initial sub-optimality, R and S are defined in Algorithm 2. ", "page_idx": 6}, {"type": "text", "text": "To reach an \u03f5 stationary point, we choose: E = 128\u03f52L\u00af\u2206, and $\\begin{array}{r}{\\mathrm{~,~}k\\mathrm{~=~}\\left(\\frac{2(C^{'})^{2}C^{2}}{\\epsilon^{2}}\\right)^{1/\\alpha}}\\end{array}$ and $E_{l}\\ =$ $O(l o g(\\epsilon^{-1}))$ . Then the sample complexity of Algorithm 2 is $E E_{l}k^{2}=O(C^{4/\\alpha}\\epsilon^{-(2+4/\\alpha)})$ , where we omit the logarithmic term $E_{l}$ . Specially, if we choose examples independently at each step, then we have sample complexity $O(\\epsilon^{-6})$ steps for an $\\epsilon$ -stationary point. This recovers the rate of DL-SGD algorithm [23]. Next, if we use some permutation-based without-replacement sampling strategy, then we have sample complexity $O((\\operatorname*{max}(m,n))^{2q}\\epsilon^{-4})$ , which can match the state-of-art algorithm RT-MLMC [23] if we choose an example order with $q=0$ [33]. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "MiniMax and Compositional Optimization. As special cases of Bilevel Optimization, we can also show that Algorithm 3, Algorithm 4 and Algorithm 5 converge under similar rate. Please see Corollary B.21 and Corollary B.20 for more details. ", "page_idx": 7}, {"type": "text", "text": "5 Applications and Numerical Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we verify the effectiveness of the proposed algorithm through a synthetic invariant risk minimization task and two real world bilevel tasks: Hyper-Data Cleaning and Hyper-representation Learning. The code is written in Pytorch. Our experiments were conducted on servers equipped with 8 NVIDIA A5000 GPUs. Experiments are averaged over five independent runs, and the standard deviation is indicated by the shaded area in the plots. ", "page_idx": 7}, {"type": "text", "text": "5.1 Invariant Risk-Minimization ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we consider a synthetic invariant risk minimization task [21]. More specifically, we perform logistic regression over a set of examples $\\{(\\dot{c_{i}},b_{i}),\\dot{\\iota}\\in[M]\\}$ with $c$ be the input and $b$ be the target. However, we does not have access to $c$ but a set of noisy observations $\\{c_{j,i},j\\in n_{i}\\}$ for each $c_{i}$ . To learn the coefficient $x$ between $c$ and $b$ , we optimize the following objective: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{min}_{\\boldsymbol{x}\\in\\mathbb{R}^{p}}\\displaystyle\\frac{1}{m}\\sum_{i=1}^{m}\\log(1+\\exp(-b_{i}y_{\\boldsymbol{x}}^{(i)})),}\\\\ &{\\qquad\\mathrm{~s.t.~}y_{\\boldsymbol{x}}^{(i)}=\\arg\\operatorname*{min}_{\\boldsymbol{y}\\in\\mathbb{R}^{d}}\\displaystyle\\frac{1}{n_{i}}\\sum_{j=1}^{n_{i}}\\frac{1}{2}\\|\\boldsymbol{y}-\\boldsymbol{c}_{j,i}\\boldsymbol{x}\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "image", "img_path": "BNnZwbZGpm/tmp/890f33ab0fc49e1eac3097b9b005edc552adf93abf58399089e0e81b7fdb5418.jpg", "img_caption": ["Figure 1: Comparison of different sampling strategies for the Invariant RiskMinimization task. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "This is a conditional bilevel optimization problem and we can solve it using our WiOR-CBO. We generate synthetic data to test the effects of different sample order (sample generation process is described in Appendix A). More specifically, we compare independent-sampling (WiR-CBO), shuffleonce (WiOR-CBO (SO)) and random-reshuffling (WiOR-CBO (RR)). As shown in Figure 1, the two without-replacement sampling methods outperforms the independent sampling one. ", "page_idx": 7}, {"type": "text", "text": "5.2 Hyper-Data Cleaning ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In the Hyper-Data Cleaning task, we are given noisy training dataset whose labels are corrupted by noise, meanwhile, we have a validation set whose labels are not corrupted. Our target is then using the clean validation samples to identify corrupted samples in the training set. More specifically, we learn optimal weights for training samples such that a model learned over the weighted training set performs well on the validation set. We can formulate this task as a bilevel optimization problem (Eq. (2)). A mathematical formulation of the task is included in Appendix A. ", "page_idx": 7}, {"type": "text", "text": "Dataset and Baselines. We construct datasets based on MNIST [29]. For the training set, we randomly sample 40000 images from the original training dataset and then randomly perturb a fraction of labels of samples. For the validation set, we randomly select 5000 clean images from the original training dataset. In our experiments, we test our WiOR-BO algorithm (Algorithm 1), with the shuffle-once (WiOR-BO-SO) and random-reshuffilng (WiOR-BO-RR) sampling; additionally, we also consider the following non-adaptive bilevel algorithms as baselines: reverse [16], stocBiO [25], BSA [17], AID-CG [18], MRBO [52], VRBO [52] and WiR-BO (the variant of our WiOR-BO using independent sampling, which is similar to the single loop algorithms such as SOBA [9], AmIGO [1] and FLSA [30]). We perform grid search for hyper-parameters and report the best results. ", "page_idx": 7}, {"type": "text", "text": "We summarize the results in Figure 2. In the figure, we compare the validation loss and F1 score w.r.t. both Hyper-iterations and running time. As shown in the figure, the two variants of our algorithm WiOR-BO-SO and WiOR-BO-RR get similar performance and they both outperform other baselines. In particular, note that WiR-BO differs with WiOR-BO-SO (RR) only over the example order, but the ", "page_idx": 7}, {"type": "image", "img_path": "BNnZwbZGpm/tmp/23336ce23ee1f90804854b8942d9877ae7f60b646241de007a225c4ee24f5269.jpg", "img_caption": ["Figure 2: Comparison of different algorithms for the Hyper-data Cleaning task. The top two plots show validation error/F1 score vs Number of Hyper-Iterations and the bottom two plots show validation error/F1 score vs Running Time. The fraction of the noisy samples is 0.6. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "BNnZwbZGpm/tmp/60b37f22eaf897de969138abcba1f6eef72477c22b9984be132be663c9710961.jpg", "img_caption": ["Figure 3: Comparison of different algorithms for the Hyper-Representation Task over the Omniglot Dataset. From Left to Right: 5-way-1-shot, 5-way-5-shot, 20-way-1-shot, 20-way-5-shot. later converges much faster in terms of running time, this is partially due to less umber of backward passes needed by without-replacement sampling per iteration. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.3 Hyper-Representation Learning ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we consider Hyper-representation learning task. In this task, we learn a hyperrepresentation of the data such that a linear classifier can be learned quickly with a small number of samples. We consider the Omniglot [27] and MiniImageNet [41] data sets. This task can be viewed as a conditional bilevel optimization problem (Eq. (2)) and a mathematical formulation of the task is included in Appendix A. ", "page_idx": 8}, {"type": "text", "text": "Dataset and Baselines. The details of the datasets are included in Appendix A and we consider N-way-K-shot classification task following [48]. In our experiments, we test our WiOR-CBO (Algorithm 2), using the shuffle-once (WiOR-BO-SO) and random-reshuffling (WiOR-BO-RR) sampling. Besides, we compare with the following baselines: DL-SGD [23] and RT-MLMC [23]. Note that our WiOR-CBO can also use independent sampling, and it has similar performance as DL-SGD. We perform grid search of hyper-parameters for each method and report the best results. ", "page_idx": 8}, {"type": "text", "text": "We summarize the experimental results for the Omniglot dataset in Figure 3 and we defer the results for MiniImageNet to the Appendix A. As shown in the figure, our WiOR-CBO SO/RR outperforms DL-SGD and is comparable to the state-of-the-art algorithm RT-MLMC. ", "page_idx": 8}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this work, we investigated example-selection of bilevel optimization. Beyond the classical independent sampling, we assumed an example-order based on without-replacement sampling, such as random-reshuffling and shuffle-once. We proposed the WiOR-BO algorithm for bilevel optimization and show the algorithm converges to an $\\epsilon$ -stationary point with rate $\\bar{O}(\\epsilon^{-3})$ . After that, we also discussed the conditional bilevel optimization problems and introduced an algorithm with convergence rate of $O(\\epsilon^{-4})$ . As special cases of bilevel optimization, we studied the minimax and compositional optimization problems. Finally, we validated the efficacy of our algorithms through one synthetic and two real-world tasks; the numerical results show the superiority of our algorithms. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was partially supported by NSF IIS 2347592, 2347604, 2348159, 2348169, DBI 2405416, CCF 2348306, CNS 2347617. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] M. Arbel and J. Mairal. Amortized implicit differentiation for stochastic bilevel optimization. arXiv preprint arXiv:2111.14580, 2021.   \n[2] L. Bottou. Stochastic gradient descent tricks. In Neural Networks: Tricks of the Trade: Second Edition, pages 421\u2013436. Springer, 2012.   \n[3] L. Bottou, F. E. Curtis, and J. Nocedal. Optimization methods for large-scale machine learning. SIAM review, 60(2):223\u2013311, 2018.   \n[4] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot learners, 2020.   \n[5] A. Buchholz, F. Wenzel, and S. Mandt. Quasi-monte carlo variational inference. In International Conference on Machine Learning, pages 668\u2013677. PMLR, 2018.   \n[6] D. Chen and M. T. Hagan. Optimal use of regularization and cross-validation in neural network modeling. In IJCNN\u201999. International Joint Conference on Neural Networks. Proceedings (Cat. No. 99CH36339), volume 2, pages 1275\u20131280. IEEE, 1999.   \n[7] H. Cho and C. Yun. Sgda with shuffling: faster convergence for nonconvex-p $\\{\\backslash L\\}$ minimax optimization. arXiv preprint arXiv:2210.05995, 2022.   \n[8] D. Choi, A. Passos, C. J. Shallue, and G. E. Dahl. Faster neural network training with data echoing. arXiv preprint arXiv:1907.05550, 2019. [9] M. Dagr\u00e9ou, P. Ablin, S. Vaiter, and T. Moreau. A framework for bilevel optimization that enables stochastic and global variance reduction algorithms. arXiv preprint arXiv:2201.13409, 2022.   \n[10] M. Dagr\u00e9ou, T. Moreau, S. Vaiter, and P. Ablin. A lower bound and a near-optimal algorithm for bilevel empirical risk minimization. In S. Dasgupta, S. Mandt, and Y. Li, editors, Proceedings of The 27th International Conference on Artificial Intelligence and Statistics, volume 238 of Proceedings of Machine Learning Research, pages 82\u201390. PMLR, 02\u201304 May 2024.   \n[11] A. Das, B. Sch\u00f6lkopf, and M. Muehlebach. Sampling without replacement leads to faster rates in finite-sum minimax optimization, 2022.   \n[12] C. B. Do, C.-S. Foo, and A. Y. Ng. Efficient multiple hyperparameter learning for log-linear models. In NIPS, volume 2007, pages 377\u2013384. Citeseer, 2007.   \n[13] J. Domke. Generic methods for optimization-based modeling. In Artificial Intelligence and Statistics, pages 318\u2013326. PMLR, 2012.   \n[14] M. C. Ferris and O. L. Mangasarian. Finite perturbation of convex programs. Applied Mathematics and Optimization, 23(1):263\u2013273, 1991.   \n[15] C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 1126\u20131135. JMLR. org, 2017.   \n[16] L. Franceschi, M. Donini, P. Frasconi, and M. Pontil. Forward and reverse gradient-based hyperparameter optimization. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 1165\u20131173. JMLR. org, 2017.   \n[17] S. Ghadimi and M. Wang. Approximation methods for bilevel programming. arXiv preprint arXiv:1802.02246, 2018.   \n[18] R. Grazzi, L. Franceschi, M. Pontil, and S. Salzo. On the iteration complexity of hypergradient computation. In International Conference on Machine Learning, pages 3748\u20133758. PMLR, 2020.   \n[19] M. G\u00fcrb\u00fczbalaban, A. Ozdaglar, and P. A. Parrilo. Why random reshuffling beats stochastic gradient descent. Mathematical Programming, 186:49\u201384, 2021.   \n[20] J. Haochen and S. Sra. Random shuffling beats SGD after finite epochs. In K. Chaudhuri and R. Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 2624\u20132633. PMLR, 09\u201315 Jun 2019.   \n[21] L. He and S. P. Kasiviswanathan. Debiasing conditional stochastic optimization. arXiv preprint arXiv:2304.10613, 2023.   \n[22] M. Hong, H.-T. Wai, Z. Wang, and Z. Yang. A two-timescale framework for bilevel optimization: Complexity analysis and application to actor-critic. arXiv preprint arXiv:2007.05170, 2020.   \n[23] Y. Hu, J. Wang, Y. Xie, A. Krause, and D. Kuhn. Contextual stochastic bilevel optimization, 2023.   \n[24] F. Huang, J. Li, S. Gao, and H. Huang. Enhanced bilevel optimization via bregman distance. Advances in Neural Information Processing Systems, 35:28928\u201328939, 2022.   \n[25] K. Ji, J. Yang, and Y. Liang. Provably faster algorithms for bilevel optimization and applications to meta-learning. arXiv preprint arXiv:2010.07962, 2020.   \n[26] P. Khanduri, S. Zeng, M. Hong, H.-T. Wai, Z. Wang, and Z. Yang. A near-optimal algorithm for stochastic bilevel optimization via double-momentum. arXiv preprint arXiv:2102.07367, 2021.   \n[27] B. Lake, R. Salakhutdinov, J. Gross, and J. Tenenbaum. One shot learning of simple visual concepts. In Proceedings of the annual meeting of the cognitive science society, volume 33, 2011.   \n[28] J. Larsen, L. K. Hansen, C. Svarer, and M. Ohlsson. Design and regularization of neural networks: the optimal use of a validation set. In Neural Networks for Signal Processing VI. Proceedings of the 1996 IEEE Signal Processing Society Workshop, pages 62\u201371. IEEE, 1996.   \n[29] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.   \n[30] J. Li, B. Gu, and H. Huang. A fully single loop algorithm for bilevel optimization without hessian inverse. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 7426\u20137434, 2022.   \n[31] J. Li, F. Huang, and H. Huang. Communication-efficient federated bilevel optimization with global and local lower level problems. Advances in Neural Information Processing Systems, 36, 2024.   \n[32] R. Liu, Z. Liu, W. Yao, S. Zeng, and J. Zhang. Moreau envelope for nonconvex bi-level optimization: A single-loop and hessian-free solution strategy, 2024.   \n[33] Y. Lu, W. Guo, and C. M. De Sa. Grab: Finding provably better data permutations than random reshuffling. Advances in Neural Information Processing Systems, 35:8969\u20138981, 2022.   \n[34] Y. Lu, S. Y. Meng, and C. De Sa. A general analysis of example-selection for stochastic gradient descent. In International Conference on Learning Representations (ICLR), volume 10, 2022.   \n[35] D. Maclaurin, D. Duvenaud, and R. Adams. Gradient-based hyperparameter optimization through reversible learning. In International Conference on Machine Learning, pages 2113\u2013 2122, 2015.   \n[36] K. Mishchenko, A. Khaled, and P. Richt\u00e1rik. Random reshuffling: Simple analysis with vast improvements. Advances in Neural Information Processing Systems, 33:17309\u201317320, 2020.   \n[37] A. Mohtashami, S. Stich, and M. Jaggi. Characterizing & finding good data orderings for fast convergence of sequential gradient methods, 2022.   \n[38] L. M. Nguyen, Q. Tran-Dinh, D. T. Phan, P. H. Nguyen, and M. Van Dijk. A unified convergence analysis for shuffling-type gradient methods. The Journal of Machine Learning Research, 22(1):9397\u20139440, 2021.   \n[39] T. Okuno, A. Takeda, and A. Kawana. Hyperparameter learning via bilevel nonsmooth optimization. arXiv preprint arXiv:1806.01520, 2018.   \n[40] F. Pedregosa. Hyperparameter optimization with approximate gradient. arXiv preprint arXiv:1602.02355, 2016.   \n[41] S. Ravi and H. Larochelle. Optimization as a model for few-shot learning. In International conference on learning representations, 2017.   \n[42] B. Recht and C. Re. Toward a noncommutative arithmetic-geometric mean inequality: Conjectures, case-studies, and consequences. In S. Mannor, N. Srebro, and R. C. Williamson, editors, Proceedings of the 25th Annual Conference on Learning Theory, volume 23 of Proceedings of Machine Learning Research, pages 11.1\u201311.24, Edinburgh, Scotland, 25\u201327 Jun 2012. PMLR.   \n[43] S. Sabach and S. Shtern. A first order method for solving convex bilevel optimization problems. SIAM Journal on Optimization, 27(2):640\u2013660, 2017.   \n[44] I. Safran and O. Shamir. How good is sgd with random shuffling? In Conference on Learning Theory, pages 3250\u20133284. PMLR, 2020.   \n[45] A. Shaban, C.-A. Cheng, N. Hatch, and B. Boots. Truncated back-propagation for bilevel optimization. arXiv preprint arXiv:1810.10667, 2018.   \n[46] A. Shamsian, A. Navon, E. Fetaya, and G. Chechik. Personalized federated learning using hypernetworks. In M. Meila and T. Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 9489\u20139502. PMLR, 18\u201324 Jul 2021.   \n[47] M. Solodov. An explicit descent method for bilevel convex optimization. Journal of Convex Analysis, 14(2):227, 2007.   \n[48] O. Vinyals, C. Blundell, T. Lillicrap, D. Wierstra, et al. Matching networks for one shot learning. In Advances in neural information processing systems, pages 3630\u20133638, 2016.   \n[49] M. Wang, E. X. Fang, and H. Liu. Stochastic compositional gradient descent: algorithms for minimizing compositions of expected-value functions. Mathematical Programming, 161(1\u20132):419\u2013449, May 2016.   \n[50] M. Welling. Herding dynamical weights to learn. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 1121\u20131128, 2009.   \n[51] R. A. Willoughby. Solutions of ill-posed problems (an tikhonov and vy arsenin). SIAM Review, 21(2):266, 1979.   \n[52] J. Yang, K. Ji, and Y. Liang. Provably faster algorithms for bilevel optimization. arXiv preprint arXiv:2106.04692, 2021.   \n[53] Y. Yang, P. Xiao, and K. Ji. Achieving $O(\\epsilon^{-1.5})$ complexity in hessian/jacobian-free stochastic bilevel optimization, 2023.   \n[54] Y. Yang, P. Xiao, and K. Ji. Simfbo: Towards simple, flexible and communication-efficient federated bilevel learning. Advances in Neural Information Processing Systems, 36, 2024.   \n[55] M. Ye, B. Liu, S. Wright, P. Stone, and Q. Liu. Bome! bilevel optimization made easy: A simple first-order approach, 2022.   \n[56] P. Yu, J. Li, and H. Huang. Dropout enhanced bilevel training. In The Twelfth International Conference on Learning Representations, 2023.   \n[57] L. Zintgraf, K. Shiarli, V. Kurin, K. Hofmann, and S. Whiteson. Fast context adaptation via meta-learning. In International Conference on Machine Learning, pages 7693\u20137702. PMLR, 2019. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A More Details for Numerical Experiments ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we introduce more details of the experiments. ", "page_idx": 13}, {"type": "text", "text": "A.1 Invariant Risk-Minimization ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The data generation process is as follows: we first randomly sample a ground truth $x^{*}$ , then we randomly generate $m=1000$ input variables $c$ , and calculate the ground truth label $b=c x$ , then for each $c_{i}$ , we generate a set of $n=100$ noisy observations by adding Guassian noise of scale 0.1. We also add a $L_{2}$ regularization term of strength 0.1 for the outer problem. During trainging, we use both the inner and outer learing rates of 0.001. ", "page_idx": 13}, {"type": "text", "text": "A.2 Hyper-Data Cleaning ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The formulation of the problem is as follows: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l l r}{\\lefteqn{\\operatorname*{min}_{x\\in\\mathbb{R}^{p}}h(x):=f(x,y_{x})=\\frac{1}{N^{(v a l)}}\\sum_{n=1}^{N^{(v a l)}}\\Theta(y_{x};\\xi_{n}^{v a l})}}\\\\ &{}&{\\mathrm{s.t.~}y_{x}=\\arg\\operatorname*{min}_{y\\in\\mathbb{R}^{d}}g(x,y)=\\sum_{n=1}^{N^{(t r)}}x_{n}\\Theta(y;\\xi_{n}^{t r})}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "In the a(bvoalv)e formulation, we have a pair of (noisy) training set $\\{\\xi_{n}^{t r}\\}_{n=1}^{N^{(t r)}}$ and validation set {\u03bevna l}nN =1 , and $x_{n},n\\in[N^{(t r)}]$ are weights for training samples, $y$ is the parameter of a model, and we denote the model by $\\Theta$ . Note that $y_{x}$ is the model learned over the weighted training set. We fit a model with 3 fully connected layers for the MNIST dataset. We also use $L_{2}$ regularization with coefficient $10^{-3}$ to satisfy the strong convexity condition. In the Experiments, we choose inner learning rate $(\\gamma,\\rho)$ as 0.1 and outer learning rate $\\eta$ 1000. ", "page_idx": 13}, {"type": "image", "img_path": "BNnZwbZGpm/tmp/76fe4022390ef00288bf4d2f3158bfb2ec8e2bac3972e9a6f03c09ce52e5ad78.jpg", "img_caption": [], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "Figure 4: Comparison of different algorithms for the Hyper-Representation Task over the MiniImageNet Dataset. From Left to Right: 5-way-1-shot, 5-way-5-shot, 20-way-1-shot, 20-way-5-shot. ", "page_idx": 13}, {"type": "text", "text": "A.3 Hyper-Representation Learning ", "text_level": 1, "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x\\in\\mathbb{R}^{p}}h(x):=f(x,y_{x})=\\frac{1}{N}\\sum_{n=1}^{N}\\big(\\frac{1}{N_{n}^{v a l}}\\sum_{i=1}^{N_{n}^{v a l}}\\Theta(x,y_{x}^{(\\mathcal{T}_{n})};\\xi_{i}^{v a l})\\big)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathrm{s.t.}\\;y_{x}^{(\\mathcal{T}_{n})}=\\underset{y\\in\\mathbb{R}^{d}}{\\arg\\operatorname*{min}}\\;g^{(\\mathcal{T}_{n})}(x,y)=\\frac{1}{N_{n}^{t r}}\\sum_{i=1}^{N_{n}^{t r}}\\Theta(x,y;\\xi_{i}^{t r})\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "$\\{\\xi_{i}^{t r}\\}_{i=1}^{N_{n}^{t r}}$ oavned  fvoarlmidualtaitoino ns,e tw $\\{\\xi_{i}^{v a l}\\}_{i=1}^{N_{n}^{v a l}}$ $N$ t.a $\\Theta$ sd eafnidn eesa tchhe  tamsokd ${\\mathcal{T}}_{n}$ $x$ s  isd ethfien epda rbaym ae tpera ior f otfh ter abianciknbg osneet model and $y$ is the parameter of the linear classifier. In summary, the lower level problem is to learn the optimal linear classifier $y$ given the backbone $x$ , and the upper level problem is to learn the optimal backbone parameter $x$ . ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "The Omniglot dataset includes 1623 characters from 50 different alphabets and each character consists of 20 samples. We follow the experimental protocols of [48] to divide the alphabets to train/validation/test with 33/5/12, respectively. We perform $N$ -way- $K$ -shot classification, more specifically, for each task, we randomly sample $N$ characters and for each character, we sample $K$ samples for training and 15 samples for validation. We augment the characters by performing rotation operations (multipliers of 90 degrees). We use a 4-layer convolutional neural network where each convolutional layer has 64 filters of $3\\!\\times\\!3$ [15]. For the MiniImageNet, it has 64 training classes and 16 validation classes. Similar to Omniglot, we also perform the $N$ -way- $\\mathcal{K}$ -shot classification. We use a 4-layer convolutional neural network where each convolutional layer has 64 fliters of $3\\!\\times\\!3$ [15] for experiments. For the experiments, we use inner learning rates 0.4 and outer learning rates 0.1 for Omniglot related experiments and inner learning rates 0.01 and outer learning rates 0.05 for MiniImageNet-related experiments. We perform 4 inner gradient descent steps and set $K_{m}a x=6$ for the RT-MLMC method. The experimental results for the MiniImageNet dataset is shown in Figure 4. ", "page_idx": 14}, {"type": "text", "text": "B Proof for Theorems ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we provide proof for the convergence results in the main text. First, we restate all assumptions needed in our proof below: ", "page_idx": 14}, {"type": "text", "text": "Assumption B.1 (Assumption 4.1). The function $f(x,y)$ is possibly non-convex and uniformly $L$ -smooth, i.e. for any $x_{1}$ , $x_{2}\\in\\mathscr{X}$ , $y_{1},y_{2}\\in\\mathbb{R}^{d}$ , any $\\xi\\in\\mathcal{D}_{u}$ . Denote $z_{1}=(x_{1},y_{1})$ , $z_{2}=(x_{2},y_{2})$ , then we have: ", "page_idx": 14}, {"type": "equation", "text": "$$\nf(z_{1};\\xi)\\leq f(z_{2};\\xi)+\\langle\\nabla f(z_{2},\\xi),z_{1}-z_{2}\\rangle+\\frac{L}{2}||z_{1}-z_{2}||^{2}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "or equivalently: $||\\nabla f(z_{1};\\xi)-\\nabla f(z_{2};\\xi)||\\leq L||z_{1}-z_{2}||$ . We also assume for any $x\\in\\mathscr{X}$ and any $y\\in\\mathbb{R}^{d}$ , and we denote $z=(x,y)$ , then we have $||\\nabla_{y}f(z)||\\leq C_{f}$ . ", "page_idx": 14}, {"type": "text", "text": "Assumption B.2 (Assumption 4.2). Function $g(x,y)$ is uniformly $\\mu$ -strongly convex w.r.t $y$ for any given $x$ and uniformly $L$ -smooth, i.e. for any $y_{1},y_{2}\\in\\mathbb{R}^{d}$ and $\\zeta\\in\\mathcal{D}_{l}$ , we have: ", "page_idx": 14}, {"type": "equation", "text": "$$\ng(x,y_{1};\\zeta)\\geq g(x,y_{2};\\zeta)+\\langle\\nabla_{y}g(x,y_{2};\\zeta),y_{2}-y_{1}\\rangle+\\frac{\\mu}{2}||y_{2}-y_{1}||^{2}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and we have: ", "page_idx": 14}, {"type": "equation", "text": "$$\ng(z_{1};\\zeta)\\leq g(z_{2};\\zeta)+\\langle\\nabla g(z_{2};\\zeta),z_{1}-z_{2}\\rangle+\\frac{L}{2}||z_{1}-z_{2}||^{2}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "equivalently: $||\\nabla g(z_{1};\\zeta)-\\nabla g(z_{2};\\zeta)||\\,\\leq\\,L||z_{1}-z_{2}||$ . Furthermore, we have $\\nabla_{x y}g(x,y)$ and $\\nabla_{y^{2}}g(x,y)$ are Lipschitz continuous with constant $L_{x y}$ and $L_{y^{2}}$ respectively, i.e. we have: $||\\check{\\nabla}_{x y}g(z_{1};\\zeta)-\\nabla_{x y}g(z_{2};\\zeta)||\\leq L_{x y}||z_{1}-z_{2}||$ and $||\\nabla_{y^{2}}\\bar{g}(z_{1};\\zeta)-\\bar{\\nabla}_{y^{2}}g(z_{2};\\zeta)||\\leq L_{y^{2}}||z_{1}-z_{2}||$ . ", "page_idx": 14}, {"type": "text", "text": "Assumption B.3 (Assumption 4.3). For all examples $\\xi_{i}\\in D_{u}$ and states $(x,y)\\in\\mathbb{R}^{p+d}$ , there exists an outer bound A such that the sample gradient errors satisfy $\\begin{array}{r}{\\|\\nabla f(x,y;\\xi_{i})-\\nabla f(x,y)\\|\\le A.}\\end{array}$ . ", "page_idx": 14}, {"type": "text", "text": "Assumption B.4 (Assumption 4.4). For all examples $\\zeta_{j}\\in D_{l}$ and states $(x,y)\\in\\mathbb{R}^{p+d}$ , there exists an outer bound A such that the sample gradient errors satisfy $\\|\\nabla g(x,y;\\zeta_{i})-\\nabla g(x,y)\\|\\leq A$ and $\\|\\nabla^{2}g(x,y;\\zeta_{i})-\\nabla^{2}g(x,y)\\|\\leq A$ . ", "page_idx": 14}, {"type": "text", "text": "Assumption B.5 (Assumption 4.7). For all examples $\\xi_{i}\\in D_{u}$ and states $x\\in\\mathbb{R}^{p}$ , there exists an upper bound A such that the sample gradient errors satisfy $\\begin{array}{r}{\\|\\nabla h(x;\\xi_{i})-\\nabla h(x)\\|\\leq A}\\end{array}$ . ", "page_idx": 14}, {"type": "text", "text": "Assumption B.6. Given two sequences $\\left\\{\\xi_{t}^{\\pi}\\;\\;\\;\\;\\;\\in\\;\\;\\;\\;\\;\\mathcal{D}_{u},t\\;\\;\\;\\;\\;\\in\\;\\;\\;\\;[T]\\right\\}$ and $\\left\\{\\zeta_{t}^{\\pi}\\right.\\quad\\in$ $\\begin{array}{r l r l}{\\mathcal{D}_{l},t}&{{}}&{\\in}&{{}\\quad[T]\\}}\\end{array}$ , andweanayssu ,existwse shoamvee: co $\\alpha,\\ \\ C$ $t$ $\\begin{array}{r l r}{k}&{{}}&{>}&{0.}\\end{array}$ $\\begin{array}{r l}{\\|\\frac{1}{k}\\sum_{\\tau=t}^{t+k-1}\\nabla f(x_{t},y_{t};\\xi_{\\tau}^{\\pi})}&{{}-}\\end{array}$ $\\begin{array}{r l r}{\\nabla f(x_{t},y_{t})\\big\\|^{2}}&{{}\\leq}&{k^{-\\alpha}C^{2}}\\end{array}$ , $\\begin{array}{r l r}{\\left\\|\\frac{1}{k}\\sum_{\\tau=t}^{t+k-1}\\nabla g(x_{t},y_{t};\\zeta_{\\tau}^{\\pi})\\right.}&{{}-}&{\\nabla g(x_{t},y_{t})\\left\\|^{2}\\right.\\quad\\leq\\quad k^{-\\alpha}C^{2},}\\end{array}$ $\\begin{array}{r l}&{\\left|\\frac{1}{k}\\sum_{\\tau=t}^{t+k-1}\\nabla^{2}g(x_{t},y_{t};\\zeta_{\\tau}^{\\pi})-\\nabla^{2}g(x_{t},y_{t})\\right|\\right|^{2}\\,\\leq\\,k^{-\\alpha}C^{2},\\left\\|\\frac{1}{k}\\sum_{\\tau=t}^{t+k-1}\\nabla h(x_{t};\\xi_{\\tau}^{\\pi})-\\nabla h(x_{t})\\right\\|^{2}\\,\\leq\\,k^{-\\alpha}C^{2},}\\end{array}$ $k^{-\\alpha}C^{2}(\\mathrm{Only}$ for the conditional bilevel optimization case). ", "page_idx": 14}, {"type": "text", "text": "Algorithm 3 Without-Replacement Compositional Optimization (WiOR-Comp) ", "page_idx": 15}, {"type": "text", "text": "1: Input: Initial states $x_{I}^{0},y_{I}^{0}$ and $u_{I}^{0}$ ; learning rates $\\{\\gamma_{i}^{r},\\rho_{i}^{r},\\eta_{i}^{r}\\},i\\in[I],r\\in[R],I=\\operatorname{lcm}(m,n)$ .   \n2: for epochs $r=1$ to $R$ do   \n3: Generate sample order sequence $\\{\\xi_{i}^{\\pi}\\}$ and $\\{\\zeta_{i}^{\\pi}\\}$ for $i\\in[I]$ as Line 3 of Algorithm 1;   \n4: Set $y_{0}^{r}=y_{I}^{r-1}$ , $u_{0}^{r}=\\mathcal{P}_{\\iota}(u_{I}^{r-1})$ and $x_{0}^{r}=x_{I}^{r-1}$   \n5: for $i=0$ to $I-1$ do   \n6: $y_{i+1}^{r}=(1-\\gamma_{i}^{r})y_{i}^{r}+\\gamma_{i}^{r}r(x_{i}^{r},\\zeta_{i}^{\\pi});u_{i+1}^{r}=(1-\\rho_{i}^{r})u_{i}^{r}+\\rho_{i}^{r}\\nabla_{y}f(y_{i}^{r};\\xi_{i}^{\\pi});$   \n7: $x_{i+1}^{r^{\\star}}=x_{i}^{r}-\\eta_{i}^{r}\\nabla r(x_{i}^{r};\\zeta_{i}^{\\pi})u_{i}^{r}$ ;   \n8: end for   \n9: end for ", "page_idx": 15}, {"type": "text", "text": "Algorithm 4 Without-Replacement MiniMax Opt. (WiOR-MiniMax) ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "1: Input: Initial states $x_{m}^{0}$ , $y_{m}^{0}$ ; learning rates $\\{\\gamma_{i}^{r},\\eta_{i}^{r}\\},i\\in[m],r\\in[R]$ .   \n2: for epochs $r=1$ to $R$ do   \n3: sample a permutation of outer dataset to have $\\{\\xi_{i}^{\\pi},i\\in[m]\\}$ ;   \n4: Set $\\bar{y}_{0}^{r}=\\bar{y}_{m}^{r-1}$ and $x_{0}^{r}=x_{m}^{r-1}$   \n5: for $i=0$ to $m-1$ do   \n6: $y_{i+1}^{r}=y_{i}^{r}+\\gamma_{i}^{r}\\nabla_{y}f(x_{i}^{r},y_{i}^{r};\\xi_{i}^{\\pi});$   \n7: $x_{i+1}^{r}=x_{i}^{r}-\\eta_{i}^{r}\\nabla_{x}f(x_{i}^{r},y_{i}^{r};\\xi_{i}^{\\pi})$ ;   \n8: end for   \n9: end for ", "page_idx": 15}, {"type": "text", "text": "B.1 Proof for Bilevel Optimization Problems ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Suppose we define: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\Phi(x,y)=\\nabla_{x}f(x,y)-\\nabla_{x y}g(x,y)\\times[\\nabla_{y^{2}}g(x,y)]^{-1}\\nabla_{y}f(x,y)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then we have $\\nabla h(x)=\\Phi(x,y_{x})$ . Furthermore, we have the following proposition: ", "page_idx": 15}, {"type": "text", "text": "Proposition B.7. Suppose Assumptions 4.1 and 4.2 hold, the following statements hold: ", "page_idx": 15}, {"type": "text", "text": "a) $y_{x}$ is Lipschitz continuous in $x$ with constant $\\rho=\\kappa$ , where $\\begin{array}{r}{\\kappa=\\frac{L}{\\mu}}\\end{array}$ is the condition number of $g(x,y)$ .   \nb) $\\|\\Phi(x_{1};y_{1})-\\Phi(x_{2};y_{2})\\|^{2}\\leq\\hat{L}^{2}(\\|x_{1}-x_{2}\\|^{2}+\\|y_{1}-y_{2}\\|^{2}),\\,w h e r e\\,\\hat{L}=O(\\kappa^{2}).$   \nc) $h(x)$ is smooth in $x$ with constant $\\bar{L}$ i.e., for any given $x_{1},x_{2}\\in X$ , we have $\\|\\nabla h(x_{2})-$ $\\nabla h(x_{1})\\|\\le\\bar{L}\\|x_{2}-x_{1}\\|$ where $\\bar{L}=O(\\kappa^{3})$ . ", "page_idx": 15}, {"type": "text", "text": "This is a standard results in bilevel optimization and we omit the proof here. ", "page_idx": 15}, {"type": "text", "text": "The main technique we use to analysis the convergence of Algorithm 1 is aggregated analysis, in other words, we perform analysis over a block of length $k$ instead of the more common per step analysis in stochastic bilevel optimization. Note that this technique is essential in analyzing without-replacement sampling for single-level optimization problems, and we extend them to the bilevel setting. More specifically, suppose the total number of training steps are $T\\,:=\\,R\\times I$ , where $R$ and $I$ are denoted in Algorithm 1, then we consider a block of training steps $[t,t+k]$ for $t\\,\\in\\,[T-k]$ . Furthermore, we denote: $\\begin{array}{r}{\\bar{\\eta}_{t}\\,=\\,\\sum_{\\tau=t}^{t+k-1}\\eta_{\\tau}}\\end{array}$ t\u03c4+=kt\u22121\u03b7\u03c4, and \u03bd\u00aft = $\\begin{array}{r}{\\bar{\\nu}_{t}\\,=\\,\\frac{1}{\\bar{\\eta}_{t}}\\sum_{\\tau=t}^{t+k-1}\\eta_{\\tau}\\dot{\\nu}_{\\tau};\\;\\bar{\\gamma}_{t}\\,=\\,}\\end{array}$ $\\sum_{\\tau=t}^{t+k-1}\\gamma_{\\tau}$ and $\\begin{array}{r}{\\bar{w}_{t}\\;=\\;\\frac{1}{\\bar{\\gamma}_{t}}\\sum_{\\tau=t}^{t+k-1}\\gamma_{\\tau}w_{\\tau}}\\end{array}$ ; $\\textstyle\\bar{\\rho}_{t}\\;=\\;\\sum_{\\tau=t}^{t+k-1}\\rho_{\\tau}$ , and $\\begin{array}{r}{\\bar{q}_{t}\\ =\\ \\frac{1}{\\bar{\\rho}_{t}}\\sum_{\\tau=t}^{t+k-1}\\rho_{\\tau}q_{\\tau}}\\end{array}$ $\\nu_{t}=\\nabla_{x}f(x_{t},y_{t};\\xi_{t})-\\nabla_{x y}g(x_{t},y_{t};;\\zeta_{t})u_{t}$ , $w_{t}=\\nabla_{y}g(x_{t},y_{t};\\zeta_{t})$ and $q_{t}=\\nabla_{y^{2}}g(x_{t},y_{t};\\zeta_{t})u_{t}\\mathrm{~-~}$ $\\nabla_{y}f(x_{t},y_{t};\\xi_{t})$ . In particular, we denote $r_{x}(u)$ as: ", "page_idx": 15}, {"type": "equation", "text": "$$\nr_{x}(u)=\\frac{1}{2}u^{T}\\nabla_{y^{2}}g(x,y_{x})u-\\nabla_{y}f(x,y_{x})^{T}u,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "by assumption it is $L$ smooth and $\\mu$ strongly convex. It is straightforward to see that $u_{x}=$ arg min $r_{x}(u)$ , where $u_{x}$ is defined in Eq. (3). Finally, in the proof, we consider the general $u{\\in}\\mathbb{R}^{d}$ case of different orders for all involved example derivatives. In other words, we have $\\{\\xi_{t,1}\\},\\{\\xi_{t,2}\\}$ , ", "page_idx": 15}, {"type": "text", "text": "1: Input: Initial state $x^{0}$ , learning rates $\\{\\eta_{i}^{r}\\},i\\in[m],r\\in[R]$ .   \n2: for epochs $r=1$ to $R$ do   \n3: Randomly sample a permutation of outer dataset to have $\\{\\xi_{i}^{\\pi},i\\in[m]\\}$ and set $x_{0}^{r}=x_{m}^{r-1}$ or   \n$x_{0}$ if $r=1$ .   \n4: for $i=0$ to $m-1$ do   \n5: Initial states $y^{0}$ and $u^{0}$ , learning rates $\\{\\gamma_{j}^{s},\\rho_{j}^{s}\\},j\\in[n_{i}],s\\in[S]$ .   \n6: for $s=1$ to $S$ do   \n7: Sample a permutation of inner dataset to have $\\{\\zeta_{\\xi_{i},j}^{\\pi},i\\;\\in\\;[n_{i}]\\}$ , set $y_{0}^{s}\\,=\\,y_{n_{i}}^{s-1}$ and   \n$u_{0}^{s}=\\mathscr{P}_{\\epsilon}(u_{n_{i}}^{s-1})$ or $y_{0}^{s}=y_{0}$ and $u_{0}^{s}=u_{0}$ if $s=1$ .   \n8: for $j=0$ to $n_{i}-1$ do   \n9: $\\begin{array}{r}{\\dot{y_{j+1}^{s}}=(1-\\gamma_{j}^{s})y_{j}^{s}+\\gamma_{j}^{s}r(x_{i}^{r};\\zeta_{\\xi_{i},j}^{\\pi})}\\end{array}$ ;   \n10: $u_{j\\pm1}^{s}=(1-\\rho_{j}^{s})u_{j}^{s}+\\rho_{j}^{s}\\nabla f(y_{j}^{s};\\xi_{i}^{\\pi})$ ;   \n11: end for   \n12: end for   \n13: $\\boldsymbol{x}_{i\\pm1}^{r}=\\boldsymbol{x}_{i}^{r}-\\eta_{i}^{r}\\nabla{r}^{\\xi_{i}^{\\pi}}(\\boldsymbol{x}_{i}^{r})\\boldsymbol{u}_{n_{i}}^{S};$   \n14: end for ", "page_idx": 16}, {"type": "text", "text": "", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "15: end for ", "page_idx": 16}, {"type": "text", "text": "$\\{\\zeta_{t,1}\\},\\{\\zeta_{t,2}\\}$ and $\\{\\zeta_{t,3}\\}$ for $\\nabla_{y}f(x,y),\\nabla_{x}f(x,y),\\nabla_{y}g(x,y),\\nabla_{y^{2}}g(x,y),\\nabla_{x y}g(x,y)$ respectively. The two orders used by Algorithm 1 is a special case of the proof and is practically appealing as they can reuse computation as we discussed in the introduction. ", "page_idx": 16}, {"type": "text", "text": "Lemma B.8. For $\\tau>t\\geq0,$ , the bias of $\\begin{array}{r l}&{\\|\\sum_{\\bar{\\tau}=t}^{\\tau-1}\\eta_{\\bar{\\tau}}\\big(\\nu_{\\bar{\\tau}}-\\nabla h(x_{t})\\big)\\|^{2},\\,\\big\\|\\sum_{\\bar{\\tau}=t}^{\\tau-1}\\rho_{\\bar{\\tau}}\\big(\\nabla r_{x_{t}}(u_{t})-q_{\\bar{\\tau}}\\big)\\big\\|^{2}}\\end{array}$ and $\\begin{array}{r l}&{\\|\\sum_{\\bar{\\tau}=t}^{\\tau-1}\\gamma_{\\bar{\\tau}}\\big(\\nabla_{y}g(x_{\\bar{\\tau}},y_{\\bar{\\tau}},\\zeta_{\\bar{\\tau},1})-\\nabla_{y}g(x_{t},y_{t})\\big)\\|^{2}}\\end{array}$ are bounded by Eq. (15), Eq. (16) and Eq. (17). ", "page_idx": 16}, {"type": "text", "text": "Proof. First for $\\begin{array}{r}{\\|\\sum_{\\bar{\\tau}=t}^{\\tau-1}\\eta_{\\bar{\\tau}}\\bigl(\\nu_{\\bar{\\tau}}-\\nabla h(x_{t})\\bigr)\\|^{2}}\\end{array}$ , we have: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Big\\|\\displaystyle\\sum_{t=1}^{m}\\psi_{t}(\\nabla h(x_{t})-\\nu_{t})\\Big\\|^{2}}\\\\ &{=\\displaystyle\\prod_{t=1}^{m}\\psi_{t}(\\nabla_{x}f(x_{t},y_{t}))-\\nabla_{x y}g(x_{t},y_{t})u_{x}-\\big(\\nabla_{x}f(x_{t},y_{t};\\xi_{t},\\xi_{t})-\\nabla_{x y}g(x_{t},y_{t};\\xi_{t},x_{t})\\big)\\Big\\|^{2}}\\\\ &{\\le2\\displaystyle\\prod_{t=1}^{m}\\psi_{t}\\big(\\nabla_{x}f(x_{t},y_{t})-\\nabla_{x}f(x_{t},y_{t};\\xi_{t},\\xi_{t})\\big)\\Big\\|^{2}+2\\displaystyle\\prod_{t=1}^{m-1}\\psi_{t}\\big(\\nabla_{x y}g(x_{t},y_{t})u_{x}-\\nabla_{x y}g(x_{t},y_{t})\\big)}\\\\ &{\\le2\\displaystyle\\prod_{t=1}^{m-1}\\psi_{t}\\big(\\nabla_{x}f(x_{t},y_{t})-\\nabla_{x}f(x_{t},y_{t};\\xi_{t},\\xi_{t})\\big)\\Big\\|^{2}}\\\\ &{\\qquad+2\\displaystyle\\prod_{t=1}^{m}\\psi_{t}\\big(\\nabla_{x y}g(x_{t},y_{t};\\xi_{t},\\xi_{t})\\big)\\Big\\|^{2}+\\displaystyle\\frac{4C_{\\mu}^{2}}{\\mu^{2}}\\displaystyle\\prod_{t=1}^{r-1}\\psi_{t}\\big(\\nabla_{x y}g(x_{t},y_{t})-\\nabla_{x y}g(x_{t},y_{t};\\xi_{t},x_{t})\\big)}\\\\ &{\\qquad+2\\displaystyle\\prod_{t=1}^{m-1}\\psi_{t}\\big(\\nabla_{x y}g(x_{t},y_{t};\\xi_{t},\\xi_{t})\\big(u_{x},\\xi_{t}\\big)\\big)\\Big\\|^{2}+4L_{\\mu}^{2}\\displaystyle\\prod_{t=1}^{m-1}\\psi_{t}\\big(\\nabla_{x y}g(x_{t},y_{t})\\big)}\\\\ &{\\le2\\displaystyle\\prod_{t=1}^{m-1}\\psi_{t}\\big(\\nabla_{x}f(x_{t},y_{t})-\\nabla_{x}f(x_{t},y_{t\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For the first and the third terms above, we have: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle||\\sum_{\\tau=t}^{\\tau-1}\\eta_{\\tau}\\big(\\nabla_{x}f(x_{t},y_{x_{t}})-\\nabla_{x}f(x_{\\bar{\\tau}},y_{\\bar{\\tau}};\\xi_{\\bar{\\tau},2})\\big)||^{2}}\\\\ {\\displaystyle\\leq3L^{2}(\\bar{\\eta}_{t})^{2}\\|y_{x_{t}}-y_{t}\\|^{2}+3L^{2}\\bar{\\eta}_{t}\\sum_{\\bar{\\tau}=t}^{\\tau-1}\\eta_{\\bar{\\tau}}\\|z_{t}-z_{\\bar{\\tau}}\\|^{2}+3\\|\\sum_{\\bar{\\tau}=t}^{\\tau-1}\\eta_{\\bar{\\tau}}\\big(\\nabla_{x}f(x_{t},y_{t})-\\nabla_{x}f(x_{t},y_{t};\\xi_{\\bar{\\tau},2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{\\bar{\\tau}=t}^{\\tau-1}\\eta_{\\bar{\\tau}}\\left(\\nabla_{x y}g(x_{t},y_{x_{t}})-\\nabla_{x y}g(x_{\\bar{\\tau}},y_{\\bar{\\tau}};\\zeta_{\\bar{\\tau},3})\\right)\\|^{2}}\\\\ &{\\le3L_{x y}^{2}(\\bar{\\eta}_{t}^{\\tau})^{2}\\|y_{x_{t}}-y_{t}\\|^{2}+3L_{x y}^{2}\\bar{\\eta}_{t}^{\\tau}\\displaystyle\\sum_{\\bar{\\tau}=t}^{\\tau-1}\\eta_{\\bar{\\tau}}\\|z_{t}-z_{\\bar{\\tau}}\\|^{2}+3\\|\\displaystyle\\sum_{\\bar{\\tau}=t}^{\\tau-1}\\eta_{\\bar{\\tau}}\\big(\\nabla_{x y}g(x_{t},y_{t})-\\nabla_{x y}g(x_{t},y_{t};\\zeta_{\\bar{\\tau},3})\\big)\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Combine Eq (13) and (14) with Eq. (12) and denote $\\begin{array}{r}{\\tilde{L}_{1}^{2}=\\left(L^{2}+\\frac{2L_{x y}^{2}C_{f}^{2}}{\\mu^{2}}\\right)}\\end{array}$ to have: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{\\bar{\\tau}=t}^{\\tau-1}\\eta_{\\bar{\\tau}}\\big(\\nu_{\\bar{\\tau}}-\\nabla h(x_{t})\\big)\\|^{2}}\\\\ &{\\le6\\tilde{L}_{1}^{2}(\\bar{\\eta}_{t}^{\\tau})^{2}\\|y_{x_{t}}-y_{t}\\|^{2}+6\\tilde{L}_{1}^{2}\\bar{\\eta}_{t}^{\\tau}\\displaystyle\\sum_{\\bar{\\tau}=t}^{\\tau-1}\\eta_{\\bar{\\tau}}\\big\\|z_{t}-z_{\\bar{\\tau}}\\big\\|^{2}+8L^{2}\\big(\\bar{\\eta}_{t}^{\\tau}\\big)^{2}\\|u_{x_{t}}-u_{t}\\|^{2}+8L^{2}\\bar{\\eta}_{t}^{\\tau}\\displaystyle\\sum_{\\bar{\\tau}=t}^{\\tau-1}\\eta_{\\bar{\\tau}}\\big\\|u_{t}-\\bar{\\eta}_{t}\\big\\|^{2}}\\\\ &{\\qquad+6\\|\\displaystyle\\sum_{\\bar{\\tau}=t}^{\\tau-1}\\eta_{\\bar{\\tau}}\\big(\\nabla_{x}f(x_{t},y_{t})-\\nabla_{x}f(x_{t},y_{t};\\xi_{\\tau,2})\\big)\\|^{2}+\\displaystyle\\frac{12C_{f}^{2}}{\\mu^{2}}\\|\\displaystyle\\sum_{\\bar{\\tau}=t}^{\\tau-1}\\eta_{\\bar{\\tau}}\\big(\\nabla_{x y}g(x_{t},y_{t})-\\nabla_{x y}g(x_{t},y_{t})\\big)\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Next, for the term $\\begin{array}{r}{\\|\\sum_{\\bar{\\tau}=t}^{\\tau-1}\\rho_{\\bar{\\tau}}(\\nabla r_{x_{t}}(u_{t})-q_{\\bar{\\tau}})\\|^{2}}\\end{array}$ , we have: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Big|\\displaystyle\\sum_{t=0}^{T}\\rho_{t}(\\nabla_{x}(u_{t})-\\phi_{t})\\Big|^{2}}\\\\ &{\\le\\operatorname*{lim}_{t\\to0}\\left\\{\\displaystyle\\sum_{s\\in\\mathcal{T}_{s}}\\rho_{s}(\\nabla_{x}\\phi_{s},\\nabla_{x}\\phi_{s})\\,u_{t}-\\nabla_{x}f(x_{t},y_{t},\\xi_{s})-(\\nabla_{x}\\phi(x_{t},y_{t}),u_{t}-\\nabla_{x}f(x_{t},y_{t}))\\right\\}^{2}}\\\\ &{\\le2\\displaystyle\\sum_{t=0}^{T}\\rho_{t}(\\nabla_{x}f(x_{t},y_{t})-\\nabla_{x}f(x_{t},y_{t};\\xi_{s}))\\Big|^{2}}\\\\ &{\\qquad+2\\displaystyle\\sum_{t=0}^{T}\\rho_{t}(\\nabla_{x}\\phi(x_{t},y_{t})-\\nabla_{x}f(x_{t},y_{t};\\xi_{s}))\\Big|^{2}}\\\\ &{\\qquad+2\\displaystyle\\sum_{t=0}^{T}\\rho_{t}(\\nabla_{x}\\phi(x_{t},y_{t})-\\nabla_{x}f(x_{t},y_{t};\\xi_{s}))\\Big|^{2}}\\\\ &{\\le2\\displaystyle\\sum_{t=0}^{T}\\rho_{t}(\\nabla_{y}f(x_{t},y_{t})-\\nabla_{y}f(x_{t},y_{t};\\xi_{s})))^{2}}\\\\ &{\\qquad+4\\displaystyle\\sum_{t=0}^{T}\\rho_{t}\\nabla_{x}g(x_{t},y_{t};\\xi_{s})(\\xi_{s},\\pi_{t})(\\pi_{t}-\\theta_{s})^{2}+\\displaystyle\\frac{C_{\\mu}^{2}}{\\nu}\\displaystyle\\sum_{s\\in\\mathcal{T}_{s}}^{T}\\rho_{t}(\\nabla_{y}\\phi(x_{t},y_{t})-\\nabla_{y}g(x_{t},y_{t};\\xi_{s}))}\\\\ &{\\le2\\displaystyle\\sum_{t=0}^{T}\\rho_{t}(\\nabla_{y}f(x_{t},y_{t})-\\nabla_{y}f(x_{t},y_{t};\\xi_{s})))^{2}+4\\displaystyle\\mathcal{L}\\mu\\displaystyle\\sum_{s\\in\\mathcal{T}_{s}}^{T}\\rho_{t}(\\nabla_{x}\\phi(x_{t},y_{t\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The first and the third terms can be bounded similarly as in Eq. (13) and Eq. (14), and denote $\\begin{array}{r}{\\tilde{L}_{2}^{2}=\\left(L^{2}+\\frac{2L_{y^{2}}^{2}C_{f}^{2}}{\\mu^{2}}\\right)}\\end{array}$ , then we have: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\big\\|\\sum_{\\tau=t}^{\\tau-1}\\rho_{\\tau}\\big(\\nabla r_{x_{t}}(u_{t})-q_{\\tau}\\big)\\big\\|^{2}}\\\\ {\\displaystyle\\leq6\\tilde{L}_{2}^{2}\\big(\\bar{\\rho}_{t}^{\\tau}\\big)^{2}\\|y_{x_{t}}-y_{t}\\|^{2}+6\\tilde{L}_{2}^{2}\\bar{\\rho}_{t}^{\\tau}\\sum_{\\bar{\\tau}=t}^{\\tau-1}\\rho_{\\bar{\\tau}}\\|z_{t}-z_{\\bar{\\tau}}\\|^{2}+4L^{2}\\bar{\\rho}_{t}^{\\tau}\\sum_{\\bar{\\tau}=t}^{\\tau-1}\\rho_{\\bar{\\tau}}\\|u_{t}-u_{\\bar{\\tau}}\\|^{2}}\\\\ {\\displaystyle\\qquad+\\,6\\|\\sum_{\\tau=t}^{\\tau-1}\\rho_{\\tau}\\big(\\nabla_{y}f(x_{t},y_{t})-\\nabla_{y}f(x_{t},y_{t};\\xi_{\\tau,1})\\big)\\|^{2}+\\frac{12C_{f}^{2}}{\\mu^{2}}\\|\\sum_{\\bar{\\tau}=t}^{\\tau-1}\\rho_{\\bar{\\tau}}\\big(\\nabla_{y^{2}}g(x_{t},y_{t})-\\nabla_{y^{2}}g(x_{t},y_{t})\\big)\\|^{2}}\\\\ {\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\dots}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Finally, for $\\begin{array}{r l}&{\\|\\sum_{\\bar{\\tau}=t}^{\\tau-1}\\gamma_{\\bar{\\tau}}\\big(\\nabla_{y}g(x_{\\bar{\\tau}},y_{\\bar{\\tau}},\\zeta_{\\bar{\\tau},1})-\\nabla_{y}g(x_{t},y_{t})\\big)\\|^{2}}\\end{array}$ , we have: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\displaystyle\\sum_{\\tau=t}^{\\tau-1}\\gamma_{\\tau}\\big(\\nabla_{y}g(x_{\\bar{\\tau}},y_{\\bar{\\tau}},\\zeta_{\\bar{\\tau},1})-\\nabla_{y}g(x_{t},y_{t})\\big)\\|^{2}}\\\\ &{\\le2\\|\\displaystyle\\sum_{\\tau=t}^{\\tau-1}\\gamma_{\\bar{\\tau}^{2}}\\big(\\nabla_{y}g(x_{\\bar{\\tau}},y_{\\bar{\\tau}},\\zeta_{\\bar{\\tau},1})-\\nabla_{y}g(x_{t},y_{t},\\zeta_{\\bar{\\tau},1})\\big)\\big)\\|^{2}}\\\\ &{\\qquad+2\\|\\displaystyle\\sum_{\\tau=t}^{\\tau-1}\\gamma_{\\bar{\\tau}^{2}}\\big(\\nabla_{y}g(x_{t},y_{t},\\zeta_{\\bar{\\tau},1})-\\nabla_{y}g(x_{t},y_{t})\\big)\\big\\|^{2}}\\\\ &{\\le2L^{2}\\gamma_{\\tau}^{\\tau}\\displaystyle\\sum_{\\tau=t}^{\\tau-1}\\gamma_{\\bar{\\tau}^{2}}\\|z_{\\bar{\\tau}}-z_{t}\\|^{2}+2\\|\\displaystyle\\sum_{\\tau=t}^{\\tau-1}\\gamma_{\\bar{\\tau}^{2}}\\big(\\nabla_{y}g(x_{t},y_{t})-\\nabla_{y}g(x_{t},y_{t},\\zeta_{\\tau},1)\\big)\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This completes the proof. ", "page_idx": 18}, {"type": "text", "text": "Lemma B.9. For all $t\\geq0$ and any constant $m>0$ , suppose $\\begin{array}{r}{\\bar{\\eta}_{t}<\\frac{1}{2\\bar{L}}}\\end{array}$ , the iterates generated satisfy: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\iota(x_{t+k})\\leq h(x_{t})-\\frac{\\bar{\\eta}_{t}}{2}\\|\\nabla h(x_{t})\\|^{2}-\\frac{\\bar{\\eta}_{t}}{4}\\|\\bar{\\nu}_{t}\\|^{2}+3\\tilde{L}_{1}^{2}\\bar{\\eta}_{t}\\|y_{x_{t}}-y_{t}\\|^{2}+3\\tilde{L}_{1}^{2}\\displaystyle\\sum_{\\tau=t}^{t+k-1}\\eta_{\\tau}\\|z_{t}-z_{\\tau}\\|^{2}}}\\\\ {{\\displaystyle\\qquad\\qquad+\\,4L^{2}\\bar{\\eta}_{t}\\|u_{x_{t}}-u_{t}\\|^{2}+4L^{2}\\displaystyle\\sum_{\\tau=t}^{t+k-1}\\eta_{\\tau}\\|u_{t}-u_{\\tau}\\|^{2}+\\displaystyle\\frac{3}{\\bar{\\eta}_{t}}\\|\\displaystyle\\sum_{\\tau=t}^{t+k-1}\\eta_{\\tau}\\big(\\nabla_{x}f(x_{t},y_{t})-\\nabla_{x}f(x_{t},y_{t})\\big)}}\\\\ {{\\displaystyle\\qquad+\\,\\frac{6C_{f}^{2}}{\\bar{\\eta}_{t}\\mu^{2}}\\|\\displaystyle\\sum_{\\tau=t}^{t+k-1}\\eta_{\\tau}\\big(\\nabla_{x y}g(x_{t},y_{t})-\\nabla_{x y}g(x_{t},y_{t};\\zeta_{\\tau,3})\\big)\\|^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. By the smoothness of $f$ , we have: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle h(x_{t+k})\\leq h(x_{t})+\\langle\\nabla h(x_{t}),x_{t+k}-x_{t}\\rangle+\\frac{\\bar{L}}{2}\\|x_{t+k}-x_{t}\\|^{2}}\\\\ {\\displaystyle=h(x_{t})-\\bar{\\eta}_{t}\\langle\\nabla h(x_{t}),\\bar{\\nu}_{t}\\rangle+\\frac{\\bar{\\eta}_{t}^{2}\\bar{L}}{2}\\|\\bar{\\nu}_{t}\\|^{2}}\\\\ {\\displaystyle\\overset{(a)}{=}h(x_{t})-\\frac{\\bar{\\eta}_{t}}{2}\\|\\nabla h(x_{t})\\|^{2}+\\frac{\\bar{\\eta}_{t}}{2}\\|\\nabla h(x_{t})-\\bar{\\nu}_{t}\\|^{2}-\\left(\\frac{\\bar{\\eta}_{t}}{2}-\\frac{\\bar{\\eta}_{t}^{2}\\bar{L}}{2}\\right)\\|\\bar{\\nu}_{t}\\|^{2}}\\\\ {\\displaystyle\\overset{(b)}{\\leq}h(x_{t})-\\frac{\\bar{\\eta}_{t}}{2}\\|\\nabla h(x_{t})\\|^{2}+\\frac{\\bar{\\eta}_{t}}{2}\\|\\nabla h(x_{t})-\\bar{\\nu}_{t}\\|^{2}-\\frac{\\bar{\\eta}_{t}}{4}\\|\\bar{\\nu}_{t}\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where equality $(a)$ uses $\\begin{array}{r}{\\langle a,b\\rangle\\,=\\,\\frac{1}{2}[\\|a\\|^{2}+\\|b\\|^{2}-\\|a-b\\|^{2}]}\\end{array}$ ; (b) follows the assumption that $\\bar{\\eta}_{t}<1/2\\bar{L}$ . Next, for the third term, by using Eq. (15), we have: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\bar{\\eta}_{t}}{2}\\|\\nabla h(x_{t})-\\bar{\\nu}_{t}\\|^{2}\\leq3\\tilde{L}_{1}^{2}\\bar{\\eta}_{t}\\|y_{x_{t}}-y_{t}\\|^{2}+3\\tilde{L}_{1}^{2}\\displaystyle\\sum_{\\tau=t}^{t+k-1}\\eta_{\\tau}\\|z_{t}-z_{\\tau}\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+4L^{2}\\bar{\\eta}_{t}\\|u_{x_{t}}-u_{t}\\|^{2}+4L^{2}\\displaystyle\\sum_{\\tau=t}^{t+k-1}\\eta_{\\tau}\\|u_{t}-u_{\\tau}\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\displaystyle\\frac{3}{\\bar{\\eta}_{t}}\\|\\displaystyle\\sum_{\\tau=t}^{t+k-1}\\eta_{\\tau}\\big(\\nabla_{x}f(x_{t},y_{t})-\\nabla_{x}f(x_{t},y_{t};\\xi_{\\tau,2})\\big)\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\displaystyle\\frac{6C_{f}^{2}}{\\bar{\\eta}_{t}\\mu^{2}}\\|\\displaystyle\\sum_{\\tau=t}^{t+k-1}\\eta_{\\tau}\\big(\\nabla_{x y}g(x_{t},y_{t})-\\nabla_{x y}g(x_{t},y_{t};\\zeta_{\\tau,3})\\big)\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\begin{array}{r}{\\tilde{L}_{1}^{2}=\\left(L^{2}+\\frac{2L_{x y}^{2}C_{f}^{2}}{\\mu^{2}}\\right)}\\end{array}$ . This completes the proof. ", "page_idx": 19}, {"type": "text", "text": "Lemma B.10. When $\\begin{array}{r}{\\bar{\\gamma}_{t}<\\frac{1}{4L}}\\end{array}$ , the error of the inner variable $y_{t}$ are bounded through Eq. (19). ", "page_idx": 19}, {"type": "text", "text": "Proof. then by proposition B.19 and choose $\\begin{array}{r}{\\bar{\\gamma}_{t}<\\frac{1}{4L}}\\end{array}$ , we have: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\|y_{t+k}-y_{x_{t}}\\|^{2}\\leq(1-\\frac{\\mu\\bar{\\gamma}_{t}}{2})\\|y_{t}-y_{x_{t}}\\|^{2}-\\frac{\\bar{\\gamma}_{t}^{2}}{2}\\|\\nabla_{y}g(x_{t},y_{t})\\|^{2}+\\frac{4\\bar{\\gamma}_{t}}{\\mu}\\|\\nabla_{y}g(x_{t},y_{t})-\\bar{w}_{t}\\|^{2}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Furthermore, by the generalized triangle inequality, we have: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{y_{t+k}-y_{x_{t+k}}\\|^{2}\\leq(1+\\displaystyle\\frac{\\mu\\bar{\\gamma}_{t}}{4})\\|y_{t+k}-y_{x_{t}}\\|^{2}+(1+\\displaystyle\\frac{4}{\\mu\\bar{\\gamma}_{t}})\\|y_{x_{t+k}}-y_{x_{t}}\\|^{2}}\\\\ &{\\qquad\\qquad\\leq(1+\\displaystyle\\frac{\\mu\\bar{\\gamma}_{t}}{4})\\|y_{t+k}-y_{x_{t}}\\|^{2}+\\displaystyle\\frac{5\\kappa^{2}}{\\mu\\bar{\\gamma}_{t}}\\|x_{t}-x_{t+k}\\|^{2}}\\\\ &{\\qquad\\qquad\\leq(1-\\displaystyle\\frac{\\mu\\bar{\\gamma}_{t}}{4})\\|y_{t}-y_{x_{t}}\\|^{2}-\\displaystyle\\frac{\\bar{\\gamma}_{t}^{2}}{2}\\|\\nabla_{y}g(x_{t},y_{t})\\|^{2}+\\displaystyle\\frac{5\\kappa^{2}}{\\mu\\bar{\\gamma}_{t}}\\|x_{t}-x_{t+k}\\|^{2}}\\\\ &{\\qquad\\qquad+\\displaystyle\\frac{10L^{2}}{\\mu}\\sum_{\\tau=t}^{t+k-1}\\gamma_{\\tau}\\|z_{t}-z_{\\tau}\\|^{2}+\\displaystyle\\frac{10}{\\mu\\bar{\\gamma}_{t}}\\|\\sum_{\\tau=t}^{t+k-1}\\gamma_{\\tau}\\big(\\nabla_{y}g(x_{t},y_{t})-\\nabla_{y}g(x_{t},y_{t};\\zeta_{\\tau,1})\\big)^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the second inequality is due to $\\begin{array}{r}{\\bar{\\gamma}_{t}<\\frac{1}{4L}<\\frac{1}{\\mu}}\\end{array}$ and uses Eq. (17). This completes the proof. ", "page_idx": 19}, {"type": "text", "text": "Lemma B.11. When $\\begin{array}{r}{\\bar{\\rho}_{t}<\\frac{1}{4L}}\\end{array}$ , the error of variable $u_{t}$ are bounded through Eq. (20). ", "page_idx": 19}, {"type": "text", "text": "Proof. suppose we choose $\\begin{array}{r}{\\bar{\\rho}_{t}<\\frac{1}{4L}}\\end{array}$ , then we have: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\|u_{t+k}-u_{x_{t}}\\|^{2}\\leq(1-\\frac{\\mu\\bar{\\rho}_{t}}{2})\\|u_{t}-u_{x_{t}}\\|^{2}-\\frac{(\\bar{\\rho}_{t})^{2}}{2}\\|\\nabla r_{x_{t}}(u_{t})\\|^{2}+\\frac{4\\bar{\\rho}_{t}}{\\mu}\\|\\nabla r_{x_{t}}(u_{t})-\\bar{q}_{t}\\|^{2}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Furthermore, by the generalized triangle inequality, we have: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{u_{t+k}-u_{x+\\pm1}\\Big|^{2}\\le(1+\\frac{\\mu\\hat{\\mu}_{t}}{4})\\|u_{t+k}-u_{x}\\|^{2}+(1+\\frac{4}{\\mu\\hat{\\mu}_{t}})\\|u_{x+k}-u_{x}\\|^{2}}\\\\ &{\\qquad\\qquad\\leq(1+\\frac{\\mu\\hat{\\mu}_{t}}{4})\\|u_{t+k}-u_{x}\\|^{2}+\\frac{5\\hat{L}_{2}^{2}}{\\mu\\hat{\\mu}_{t}}\\|x_{t}-x_{t+k}\\|^{2}}\\\\ &{\\qquad\\leq(1-\\frac{\\mu\\hat{\\mu}_{t}}{4})\\|u_{t}-u_{x}\\|^{2}-\\frac{\\hat{\\mu}_{t}}{2}\\|\\nabla_{x}\\tau_{(u_{t})}\\|^{2}+\\frac{5\\hat{L}_{2}^{2}}{\\mu\\hat{\\mu}_{t}}\\|x_{t}-x_{t+k}\\|^{2}}\\\\ &{\\qquad\\qquad+\\underbrace{30\\hat{L}_{2}^{2}\\hat{\\mu}_{t}}_{\\textit{t}}\\|y_{x_{t}}-y_{t}\\|^{2}+\\frac{30\\hat{L}_{2}^{2}}{\\mu}\\sum_{\\tau=t}^{t-k-1}\\rho_{t}\\big\\|z_{t}-z_{\\tau}\\big\\|^{2}+\\frac{20\\hat{L}_{2}^{2}}{\\mu}\\sum_{\\tau=t}^{t-k-1}\\rho_{\\tau}\\big\\|u_{t}-u}\\\\ &{\\qquad+\\underbrace{30}_{\\mu\\hat{\\mu}_{t}}\\frac{t+k-1}{\\sum_{\\tau=t}}\\rho_{\\tau}\\big(\\nabla_{y}f(x_{t},y_{t})-\\nabla_{y}f(x_{t},y_{t};\\xi_{\\tau_{t}})\\big)\\big\\|^{2}}\\\\ &{\\qquad+\\underbrace{60C_{\\eta}^{2}}_{\\textit{t}}\\big\\|\\frac{t+k-1}{\\sum_{\\tau=t}}\\rho_{\\tau}\\big(\\nabla_{y}g(x_{t},y_{t})-\\nabla_{y}g(x_{t},y_{t};\\xi_{\\tau_{t}})\\big)\\big\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad(20)}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where in the last inequality, we use the condition that $\\begin{array}{r}{\\bar{\\rho}_{t}\\,<\\,\\frac{1}{4L}}\\end{array}$ and Eq. (16). This completes the proof. \u53e3 ", "page_idx": 20}, {"type": "text", "text": "Lemma B.12. Suppose $\\begin{array}{r}{\\bar{\\eta}_{t}\\,\\le\\,\\operatorname*{min}\\left(\\frac{1}{96\\tilde{L}_{1}^{2}},\\frac{1}{32L^{2}c_{1}^{2}},\\frac{1}{128L^{2}},\\frac{1}{64L^{2}c_{2}^{2}},\\frac{1}{96\\tilde{L}_{2}^{2}c_{2}^{2}}\\right)}\\end{array}$ , then the drift of $z_{\\tau}=$ $(x_{\\tau},y_{\\tau})$ and $u_{\\tau}$ can be bounded as in $E q$ . (25) and Eq. (24). ", "page_idx": 20}, {"type": "text", "text": "Proof. We first have: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle x_{t}-x_{\\tau}\\|^{2}=\\|\\sum_{\\bar{\\tau}=t}^{\\tau-1}\\eta_{\\bar{\\tau}}\\nu_{\\tau}\\|^{2}\\leq2\\|\\sum_{\\bar{\\tau}=t}^{\\tau-1}\\eta_{\\bar{\\tau}}\\big(\\nu_{\\tau}-\\nabla h(x_{t})\\big)\\|^{2}+2(\\bar{\\eta}_{t}^{\\tau})^{2}\\|\\nabla h(x_{t})\\|^{2}}\\\\ {\\displaystyle|y_{t}-y_{\\tau}\\|^{2}=\\|\\sum_{\\bar{\\tau}=t}^{\\tau-1}\\gamma_{\\bar{\\tau}}\\omega_{\\tau}\\|^{2}\\leq2\\|\\sum_{\\bar{\\tau}=t}^{\\tau-1}\\gamma_{\\bar{\\tau}}\\big(\\nabla_{y}g(x_{\\bar{\\tau}},y_{\\bar{\\tau}},\\zeta_{\\bar{\\tau},1})-\\nabla_{y}g(x_{t},y_{t})\\big)\\|^{2}+2(\\bar{\\gamma}_{t}^{\\tau})^{2}\\|\\nabla_{y}g(x_{t},y_{t})\\|^{2}}\\\\ {\\displaystyle u_{t}-u_{\\tau}\\|^{2}=\\|\\sum_{\\bar{\\tau}=t}^{\\tau-1}\\rho_{\\tau}q_{\\bar{\\tau}}\\|^{2}\\leq2\\|\\sum_{\\bar{\\tau}=t}^{\\tau-1}\\rho_{\\bar{\\tau}}\\big(q_{\\bar{\\tau}}-\\nabla r_{x_{t}}(u_{t})\\big)\\|^{2}+2(\\bar{\\rho}_{t}^{\\tau})^{2}\\|\\nabla r_{x_{t}}(u_{t})\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For the term $\\|z_{t}-z_{\\tau}\\|^{2}$ , we have $\\|z_{t}-z_{\\tau}\\|^{2}=\\|x_{t}-x_{\\tau}\\|^{2}+\\|y_{t}-y_{\\tau}\\|^{2}$ . Combine Eq. (21) with Eq. (15) and (17), sum $\\tau$ over $[t,t+k-1]$ and use Proposition B.18 to have: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\sum_{\\tau=t}^{t+k-1}\\eta_{\\tau}\\|z_{t}-z_{\\tau}\\|^{2}\\leq\\sum_{\\tau=t}^{t+k-1}\\eta_{\\tau}\\Big(\\displaystyle\\sum_{\\tau=t}^{\\tau-1}\\big(12\\tilde{L}_{1}^{2}\\bar{\\eta}_{t}^{\\tau}\\eta_{\\tau}+4L^{2}\\bar{\\gamma}_{t}^{\\tau}\\gamma_{\\tau}\\big)\\|z_{t}-z_{\\tau}\\|^{2}\\Big)}}\\\\ {{\\displaystyle\\qquad\\qquad\\qquad+2\\bar{\\alpha}_{t}\\bar{\\eta}_{t}^{2}\\|\\nabla h(x_{t})\\|^{2}+2\\bar{\\alpha}_{t}\\bar{\\gamma}_{t}^{2}\\|\\nabla_{y}g(x_{t},y_{t})\\|^{2}}}\\\\ {{\\displaystyle\\qquad\\qquad\\qquad+12\\bar{L}_{1}^{2}\\bar{\\alpha}_{t}(\\bar{\\eta}_{t})^{2}\\|y_{x_{t}}-y_{t}\\|^{2}+16L^{2}\\bar{\\alpha}_{t}(\\bar{\\eta}_{t})^{2}\\|u_{x_{t}}-u_{t}\\|^{2}}}\\\\ {{\\displaystyle\\qquad\\qquad\\qquad+16L^{2}\\bar{\\eta}_{t}\\,\\sum_{\\tau=t}^{t+k-1}\\eta_{\\tau}\\Big(\\displaystyle\\sum_{\\tau=t}^{\\tau-1}\\eta_{\\tau}\\|u_{t}-u_{\\tau}\\|^{2}\\Big)}}\\\\ {{\\displaystyle\\qquad\\qquad\\qquad+12\\bar{\\eta}_{t}\\eta_{t}^{2}k^{2-\\alpha}C^{2}+\\displaystyle\\frac{24C_{f}^{2}}{\\mu^{2}}\\bar{\\eta}_{t}\\eta_{t}^{2}k^{2-\\alpha}C^{2}+4\\bar{\\eta}_{t}\\gamma_{t}^{2}k^{2-\\alpha}C^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "By the fact that $\\tau\\le t+k$ , we have: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\sum_{\\tau=t}^{t+k-1}\\eta_{\\tau}\\|z_{t}-z_{\\tau}\\|^{2}\\leq\\sum_{\\tau=t}^{t+k-1}\\left(12\\tilde{L}_{1}^{2}(\\bar{\\eta}_{t})^{2}\\eta_{\\tau}+4L^{2}\\bar{\\eta}_{t}\\bar{\\gamma}_{t}\\gamma_{\\tau}\\right)\\|z_{t}-z_{\\tau}\\|^{2}}}\\\\ {{\\displaystyle\\qquad\\qquad\\qquad\\qquad+2\\bar{\\eta}_{t}^{3}\\|\\nabla h(x_{t})\\|^{2}+2\\bar{\\eta}_{t}\\bar{\\gamma}_{t}^{2}\\|\\nabla_{y}g(x_{t},y_{t})\\|^{2}}}\\\\ {{\\displaystyle\\qquad\\qquad\\qquad+12\\tilde{L}_{1}^{2}(\\bar{\\eta}_{t})^{3}\\|y_{x_{t}}-y_{t}\\|^{2}+16L^{2}(\\bar{\\eta}_{t})^{3}\\|u_{x_{t}}-u_{t}\\|^{2}}}\\\\ {{\\displaystyle\\qquad\\qquad\\qquad\\qquad+16L^{2}(\\bar{\\eta}_{t})^{2}\\sum_{\\tau=t}^{t+k-1}\\eta_{\\tau}\\|u_{t}-u_{\\tau}\\|^{2}}}\\\\ {{\\displaystyle\\qquad\\qquad\\qquad\\qquad+12\\bar{\\eta}_{t}\\eta_{t}^{2}k^{2-\\alpha}C^{2}+\\frac{24C_{f}^{2}}{\\mu^{2}}\\bar{\\eta}_{t}\\eta_{t}^{2}k^{2-\\alpha}C^{2}+4\\bar{\\eta}_{t}\\gamma_{t}^{2}k^{2-\\alpha}C^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For $\\lVert u_{t}-u_{\\tau}\\rVert^{2}$ , we combine Eq. (21) with Eq. (16) and sum $\\tau$ over $[t,t+k-1]$ and use the fact that $\\tau\\le t+k$ to have: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{\\tau=t}^{t+k-1}\\eta_{\\tau}\\|u_{t}-u_{\\tau}\\|^{2}\\leq2\\bar{\\eta}_{t}(\\bar{\\rho}_{t})^{2}\\|\\nabla r_{x_{t}}(u_{t})\\|^{2}+12\\tilde{L}_{2}^{2}\\bar{\\eta}_{t}(\\bar{\\rho}_{t})^{2}\\|y_{x_{t}}-y_{t}\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+12\\tilde{L}_{2}^{2}\\bar{\\eta}_{t}\\bar{\\rho}_{t}\\displaystyle\\sum_{\\tau=t}^{t+k-1}\\rho_{\\tau}\\|z_{t}-z_{\\tau}\\|^{2}+8L^{2}\\bar{\\eta}_{t}\\bar{\\rho}_{t}\\displaystyle\\sum_{\\tau=t}^{t+k-1}\\rho_{\\tau}\\|u_{t}-u_{\\tau}\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+12\\bar{\\eta}_{t}\\rho_{t}^{2}k^{2-\\alpha}C^{2}+\\displaystyle\\frac{24C_{f}^{2}\\bar{\\eta}_{t}}{\\mu^{2}}\\rho_{t}^{2}k^{2-\\alpha}C^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Suppose we have $\\begin{array}{r}{\\gamma_{\\tau}=c_{1}\\eta_{\\tau}=\\frac{20\\kappa\\tilde{L}_{3}}{\\mu}\\eta_{\\tau}}\\end{array}$ , $\\rho_{\\tau}=c_{2}\\eta_{\\tau}=40\\kappa\\hat{L}\\eta_{\\tau}$ and set: ", "page_idx": 21}, {"type": "equation", "text": "$$\n(\\bar{\\eta}_{t})^{2}<\\operatorname*{min}\\left(\\frac{1}{96\\tilde{L}_{1}^{2}},\\frac{1}{32L^{2}c_{1}^{2}},\\frac{1}{128L^{2}},\\frac{1}{64L^{2}c_{2}^{2}},\\frac{1}{96\\tilde{L}_{2}^{2}c_{2}^{2}}\\right)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then we combine Eq. (22) and Eq. (23) to have: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{\\tau=t}^{t+k-1}\\eta_{\\tau}\\|u_{t}-u_{\\tau}\\|^{2}\\leq4c_{2}^{2}(\\bar{\\eta}_{t})^{3}\\|\\nabla r_{x_{t}}(u_{t})\\|^{2}+4\\bar{\\eta}_{t}^{3}\\|\\nabla h(x_{t})\\|^{2}+4c_{1}^{2}\\bar{\\eta}_{t}^{3}\\|\\nabla_{y}g(x_{t},y_{t})\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\left(24+\\frac{48C_{f}^{2}}{\\mu^{2}}+8c_{1}^{2}+24c_{2}^{2}+\\frac{48c_{2}^{2}C_{f}^{2}}{\\mu^{2}}\\right)\\bar{\\eta}_{t}\\eta_{t}^{2}k^{2-\\alpha}C^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{\\tau=t}^{t+k-1}\\eta_{\\tau}\\|z_{t}-z_{\\tau}\\|^{2}\\leq4c_{2}^{2}(\\bar{\\eta}_{t})^{3}\\|\\nabla r_{x_{t}}(u_{t})\\|^{2}+4\\bar{\\eta}_{t}^{3}\\|\\nabla h(x_{t})\\|^{2}+4c_{1}^{2}\\bar{\\eta}_{t}^{3}\\|\\nabla_{y}g(x_{t},y_{t})\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\left(24+\\frac{48C_{f}^{2}}{\\mu^{2}}+8c_{1}^{2}+24c_{2}^{2}+\\frac{48c_{2}^{2}C_{f}^{2}}{\\mu^{2}}\\right)\\bar{\\eta}_{t}\\eta_{t}^{2}k^{2-\\alpha}C^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "This completes the proof. ", "page_idx": 21}, {"type": "text", "text": "Suppose we define the potential function $\\mathcal{G}(t)$ : ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{G}_{t}=h(x_{t})+\\phi_{y}\\|y_{t}-y_{x_{t}}\\|^{2}+\\phi_{u}\\|u_{t}-u_{x_{t}}\\|^{2}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\begin{array}{r}{\\phi_{u}=\\frac{32L^{2}\\bar{\\eta}_{t}}{\\mu\\bar{\\rho}_{t}}}\\end{array}$ 32L2 \u03b7\u00aft, \u03d5  = $\\begin{array}{r}{\\phi_{y}=\\frac{8\\tilde{L}_{3}^{2}\\bar{\\eta}_{t}}{\\mu\\bar{\\gamma}_{t}}}\\end{array}$ and $\\tilde{L}_{3}^{2}=960\\kappa^{2}\\tilde{L}_{2}^{2}+3\\tilde{L}_{1}^{2}.$ . ", "page_idx": 21}, {"type": "text", "text": "Theorem B.13. Suppose Assumptions 4.1-4.4 are satisfied, and Assumption 4.5 is satisfied with some $\\alpha$ and $C$ for the example order we use in Algorithm $^{\\,l}$ . We choose learning rates $\\eta_{t}=\\eta_{i}$ , $\\gamma=c_{1}\\eta_{}$ $\\rho=c_{2}\\eta$ and denote $T=R\\times I$ be the total number of steps. Then for any pair of values $(E,\\,k)$ which has $T=E\\times k$ and $\\begin{array}{r}{\\eta\\le\\frac{1}{k\\tilde{L}_{0}}}\\end{array}$ , we have: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{1}{E}\\sum_{e=0}^{E-1}\\|\\nabla h(x_{k e})\\|^{2}\\leq\\frac{2\\Delta}{k E\\eta}+2\\tilde{L}^{2}k^{-\\alpha}C^{2}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $c_{1}$ , $c_{2}$ , $\\tilde{L}_{0}$ , $\\tilde{L}$ are constants related to the smoothness parameters of $h(x)$ , $\\Delta$ is the initial sub-optimality, $R$ and $I$ are defined in Algorithm $I$ . ", "page_idx": 21}, {"type": "text", "text": "Proof. First we combine Lemma B.9-Lemma B.11, and use the condition that $\\begin{array}{r c l}{\\gamma_{t}~=~\\frac{20\\kappa\\tilde{L}_{3}}{\\mu}\\eta_{t}}\\end{array}$ $\\rho_{t}=40\\kappa\\hat{L}\\eta_{t}$ , and by the assumption about gradient error: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathcal{G}_{t+k}-\\mathcal{G}_{t}\\leq-\\displaystyle\\frac{\\bar{\\eta}_{t}}{2}\\|\\nabla h(x_{t})\\|^{2}-\\frac{4c_{1}\\tilde{L}_{3}^{2}(\\bar{\\eta}_{t})^{2}}{\\mu}\\|\\nabla_{y}g(x_{t},y_{t})\\|^{2}-\\frac{16c_{2}L^{2}(\\bar{\\eta}_{t})^{2}}{\\mu}\\|\\nabla r_{x_{t}}(u_{t})\\|^{2}}&{}\\\\ {-\\displaystyle\\bar{L}_{3}^{2}\\bar{\\eta}_{t}\\|y_{x_{t}}-y_{t}\\|^{2}-4L^{2}\\bar{\\eta}_{t}\\|u_{x_{t}}-u_{t}\\|^{2}}&{}\\\\ {+\\displaystyle\\sum_{\\tau=t}^{t+k-1}\\big(960\\kappa^{2}\\tilde{L}_{2}^{2}\\eta_{\\tau}+3\\tilde{L}_{1}^{2}\\eta_{\\tau}+80\\kappa^{2}\\tilde{L}_{3}^{2}\\eta_{\\tau}\\big)\\|z_{t}-z_{\\tau}\\|^{2}}&{}\\\\ {+\\displaystyle\\sum_{\\tau=t}^{t+k-1}\\big(640\\kappa^{2}L^{2}\\eta_{\\tau}+4L^{2}\\eta_{\\tau}\\big)\\|u_{t}-u_{\\tau}\\|^{2}}&{}\\\\ {+\\left(3+\\frac{6C_{f}^{2}}{\\mu^{2}}+\\frac{80\\tilde{L}_{3}^{2}}{\\mu^{2}}+960\\kappa^{2}+\\frac{1920\\kappa^{2}C_{f}^{2}}{\\mu^{2}}\\right)\\frac{\\eta_{t}^{2}k^{2-\\alpha}C^{2}}{\\bar{\\eta}_{t}}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Suppose we denote $\\tilde{L}_{4}^{2}=\\left(960\\kappa^{2}\\tilde{L}_{2}^{2}+3\\tilde{L}_{1}^{2}+80\\kappa^{2}\\tilde{L}_{3}^{2}+640\\kappa^{2}L^{2}+4L^{2}\\right)$ , and set: ", "page_idx": 22}, {"type": "equation", "text": "$$\n(\\bar{\\eta}_{t})^{2}<\\operatorname*{min}\\big(\\frac{1}{16\\tilde{L}_{4}^{2}},\\frac{\\tilde{L}_{3}^{4}}{c_{1}^{2}\\mu^{2}\\tilde{L}_{4}^{4}},\\frac{16L^{4}}{c_{2}^{2}\\mu^{2}\\tilde{L}_{4}^{4}},\\frac{\\tilde{L}_{3}^{2}}{48\\tilde{L}_{4}^{2}\\tilde{L}_{2}^{2}c_{2}^{2}},\\frac{\\tilde{L}_{3}^{2}}{48\\tilde{L}_{4}^{2}\\tilde{L}_{1}^{2}},\\frac{1}{8\\tilde{L}_{4}^{2}}\\big)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and we bound the terms $\\lVert u_{x_{t}}-u_{t}\\rVert^{2}$ and $\\|z_{t}-z_{\\tau}\\|^{2}$ through Lemma B.12 to have: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{G}_{t+k}-\\mathcal{G}_{t}\\leq-\\displaystyle\\frac{\\bar{\\eta}_{t}}{2}\\|\\nabla h(x_{t})\\|^{2}+\\big(24+\\displaystyle\\frac{48C_{f}^{2}}{\\mu^{2}}+8c_{1}^{2}+24c_{2}^{2}+\\displaystyle\\frac{48c_{2}^{2}C_{f}^{2}}{\\mu^{2}}\\big)\\tilde{L}_{4}^{2}\\bar{\\eta}_{t}\\eta_{t}^{2}k^{2-\\alpha}C^{2}}\\\\ &{\\qquad\\qquad\\qquad+\\big(3+\\displaystyle\\frac{6C_{f}^{2}}{\\mu^{2}}+\\frac{80\\tilde{L}_{3}^{2}}{\\mu^{2}}+960\\kappa^{2}+\\displaystyle\\frac{1920\\kappa^{2}C_{f}^{2}}{\\mu^{2}}\\big)\\displaystyle\\frac{\\eta_{t}^{2}k^{2-\\alpha}C^{2}}{\\bar{\\eta}_{t}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Next suppose we set $\\tilde{L}_{4}\\bar{\\eta}_{t}<1$ , then we have: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle{\\mathcal G}_{t+k}-{\\mathcal G}_{t}\\leq-\\frac{\\bar{\\eta}_{t}}{2}\\|\\nabla h(x_{t})\\|^{2}+\\left(24+\\frac{48C_{f}^{2}}{\\mu^{2}}+8c_{1}^{2}+24c_{2}^{2}+\\frac{48c_{2}^{2}C_{f}^{2}}{\\mu^{2}}+\\right.}}\\\\ {{\\displaystyle\\left.3+\\frac{6C_{f}^{2}}{\\mu^{2}}+\\frac{80\\tilde{L}_{3}^{2}}{\\mu^{2}}+960\\kappa^{2}+\\frac{1920\\kappa^{2}C_{f}^{2}}{\\mu^{2}}\\right)\\frac{\\eta_{t}^{2}k^{2-\\alpha}C^{2}}{\\bar{\\eta}_{t}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "For ease of notation we denote L\u02dc2 = 24+48\u00b5C2f2 +8c21 +24c22 +48c\u00b5222Cf2 +3+6C2f \u00b5 + 80\u00b5L2\u02dc3 +960\u03ba2 + 1920\u00b5\u03ba22Cf2 , then we have: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathcal{G}_{t+k}-\\mathcal{G}_{t}\\leq-\\frac{\\bar{\\eta}_{t}}{2}\\|\\nabla h(x_{t})\\|^{2}+\\frac{\\tilde{L}^{2}\\eta_{t}^{2}k^{2-\\alpha}C^{2}}{\\bar{\\eta}_{t}}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Sum the above inequality over $E$ phases to have: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{e=0}^{E-1}\\frac{\\bar{\\eta}_{k e}}{2}\\|\\nabla h(x_{k e})\\|^{2}\\leq\\mathcal{G}_{0}-\\mathcal{G}_{k E}+\\displaystyle\\sum_{e=0}^{E-1}\\frac{\\tilde{L}^{2}\\eta_{k e}^{2}k^{2-\\alpha}C^{2}}{\\bar{\\eta}_{k e}}}}\\\\ &{\\leq\\Delta_{h}+\\phi_{y}\\Delta_{y}+\\phi_{u}\\Delta_{u}+\\displaystyle\\sum_{e=0}^{E-1}\\frac{\\tilde{L}^{2}\\eta_{k e}^{2}k^{2-\\alpha}C^{2}}{\\bar{\\eta}_{k e}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "we define $\\Delta_{h}=h(x_{0})-h^{*}$ as the initial sub-optimality of the function, $\\Delta_{y}=\\|y_{0}-y_{x_{0}}\\|^{2}$ as the initial sub-optimality of the inner variable estimation, $\\Delta_{u}=\\|u_{0}-u_{x_{0}}\\|^{2}$ as the initial sub-optimality of the hyper-gradient estimation. Then suppose we choose $\\eta_{t}=\\eta$ be some constant, then we have: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{k\\eta}{2}\\sum_{e=0}^{E-1}\\|\\nabla h(x_{k e})\\|^{2}\\leq\\Delta_{h}+\\phi_{y}\\Delta_{y}+\\phi_{u}\\Delta_{u}+\\tilde{L}^{2}E k^{1-\\alpha}C^{2}\\eta\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Divide by $k E\\eta/2$ on both sides to have: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{1}{E}\\sum_{e=0}^{E-1}\\|\\nabla h(x_{k e})\\|^{2}\\leq\\frac{2}{k E\\eta}\\big(\\Delta_{h}+\\phi_{y}\\Delta_{y}+\\phi_{u}\\Delta_{u}\\big)+2\\tilde{L}^{2}k^{-\\alpha}C^{2}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Suppose we choose the largest $\\begin{array}{r}{\\eta=\\frac{1}{k\\tilde{L}_{0}}}\\end{array}$ such that the conditions of Lemma B.9-Lemma B.12 are satisfied, then we have: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{1}{E}\\sum_{e=0}^{E-1}\\|\\nabla h(x_{k e})\\|^{2}\\leq\\frac{2\\tilde{L}_{0}}{E}\\big(\\Delta+\\phi_{y}\\Delta_{y}+\\phi_{u}\\Delta_{u}\\big)+2\\tilde{L}^{2}k^{-\\alpha}C^{2}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Then to reach an $\\epsilon_{}$ -stationary point, we need to have: ", "page_idx": 23}, {"type": "equation", "text": "$$\nE\\ge\\frac{4\\tilde{L}_{0}\\Delta}{\\epsilon^{2}},\\;\\mathrm{and}\\;,k\\ge\\left(\\frac{4\\tilde{L}^{2}C^{2}}{\\epsilon^{2}}\\right)^{1/\\alpha}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "in other words $\\begin{array}{r}{T=E k\\ge\\frac{4\\tilde{L}_{0}(4\\tilde{L}^{2}C^{2})^{1/\\alpha}\\Delta}{\\epsilon^{2+2/\\alpha}}}\\end{array}$ 4L\u02dc0(4L\u02dc22+2C/2\u03b1)1/\u03b1\u2206. This completes the proof. ", "page_idx": 23}, {"type": "text", "text": "B.2 Proof for conditional bilevel optimization problems ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this section we study the convergence rate of Algorithm 2. ", "page_idx": 23}, {"type": "text", "text": "Lemma B.14. For all $t\\geq0$ and any constant $k>0$ , suppose $\\begin{array}{r}{\\bar{\\eta}_{t}<\\frac{1}{4L}}\\end{array}$ , the iterates generated satisfy: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\iota(x_{t+k})\\leq h(x_{t})-\\displaystyle\\frac{k\\eta}{8}\\|\\nabla h(x_{t})\\|^{2}-\\displaystyle\\frac{k\\eta}{4}\\|\\bar{\\nu}_{t}\\|^{2}+3\\big(1+\\displaystyle\\frac{4\\kappa^{2}\\big(192C_{f}^{2}+96*24\\tilde{L}_{2}^{2}\\big)}{\\mu^{2}}+\\displaystyle\\frac{48\\tilde{L}_{1}^{2}}{\\mu^{2}}\\big)\\eta k^{1-\\alpha}C}\\\\ &{\\qquad\\qquad\\qquad+\\ 12\\eta k L^{2}(1-\\displaystyle\\frac{k\\mu\\rho}{2})^{E_{l}}\\Delta_{u}+3\\eta k(2\\tilde{L}_{1}^{2}+4*96\\kappa^{2}\\tilde{L}_{2}^{2})(1-\\displaystyle\\frac{k\\mu\\gamma}{2})^{E_{l}}\\Delta_{y}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. By the smoothness of $f$ , follow similar derivation as in Eq. (18), we have: ", "page_idx": 23}, {"type": "equation", "text": "$$\nh(x_{t+k})\\leq h(x_{t})-\\frac{\\bar{\\eta}_{t}}{2}\\|\\nabla h(x_{t})\\|^{2}+\\frac{\\bar{\\eta}_{t}}{2}\\|\\nabla h(x_{t})-\\bar{\\nu}_{t}\\|^{2}-\\frac{\\bar{\\eta}_{t}}{4}\\|\\bar{\\nu}_{t}\\|^{2}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "In particular, for the term $\\begin{array}{r}{\\|\\sum_{\\bar{\\tau}=t}^{\\tau-1}\\eta_{\\bar{\\tau}}\\bigl(\\nu_{\\bar{\\tau}}-\\nabla h(x_{t})\\bigr)\\|^{2}}\\end{array}$ , we have: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\lefteqn{\\big|\\sum_{\\tau=t}^{-1}\\eta_{\\tau}\\big(\\nabla h(x_{t})-\\nu_{\\tau}\\big)\\big|^{2}}}\\\\ &{\\leq3\\big\\|\\displaystyle\\sum_{\\tau=t}^{\\tau-1}\\eta_{\\tau}\\big(\\nabla h(x_{t})-\\nabla h(x_{t};\\xi_{\\tau})\\big)\\big\\|^{2}+3\\big\\|\\displaystyle\\sum_{\\tau=t}^{\\tau-1}\\eta_{\\tau}\\big(\\nabla h(x_{t};\\xi_{\\tau})-\\nabla h(x_{\\tau};\\xi_{\\tau})\\big)\\big\\|^{2}+3\\big\\|\\displaystyle\\sum_{\\tau=t}^{\\tau-1}\\eta_{\\tau}\\big(\\nabla h(x_{t};\\xi_{\\tau})-\\nabla h(x_{\\tau};\\xi_{\\tau})\\big)\\big\\|^{2}}\\\\ &{\\leq3\\big\\|\\displaystyle\\sum_{\\tau=t}^{\\tau-1}\\eta_{\\tau}\\big(\\nabla h(x_{t})-\\nabla h(x_{t};\\xi_{\\tau})\\big)\\big\\|^{2}+3\\bar{L}^{2}\\eta_{\\tau}^{-1}\\frac{\\eta}{\\tau=t}\\eta_{\\tau}\\big\\|x_{t}-x_{\\tau}\\big\\|^{2}+3\\bar{\\eta}_{t}^{\\tau}\\displaystyle\\sum_{\\tau=t}^{\\tau-1}\\eta_{\\tau}\\big\\|\\nabla h(x_{\\tau};\\xi_{\\tau})-\\nu_{\\tau}\\big\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "For the third term, we follow similar derivation as in Eq. (12)-Eq. (15), we have: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\left|\\left(\\nabla h(x_{\\tau};\\xi_{\\tau})-\\nu_{\\tau}\\right)\\right|^{2}}&\\\\ &{=\\left\\|\\left(\\nabla_{x}f(x_{\\tau},y_{x_{\\tau}}^{\\xi_{\\tau}};\\xi_{\\tau})-\\nabla_{x y}g^{\\xi_{\\tau}}(x_{\\tau},y_{x_{\\tau}}^{\\xi_{\\tau}})u_{x_{\\tau}}^{\\xi_{\\tau}}-\\left(\\nabla_{x}f(x_{\\tau},y_{\\tau,T_{l}};\\xi_{\\tau})-\\nabla_{x y}g^{\\xi_{\\tau}}(x_{\\tau},y_{\\tau,T_{l}})u_{\\tau,T_{l}}\\right)\\right)\\right\\|^{2}}\\\\ &{\\leq2\\tilde{L}_{1}^{2}\\|y_{x_{\\tau}}^{\\xi_{\\tau}}-y_{\\tau,T_{l}}\\|^{2}+4L^{2}\\|u_{x_{\\tau}}^{\\xi_{\\tau}}-u_{\\tau,T_{l}}\\|^{2}}&{(28)}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\begin{array}{r}{\\tilde{L}_{1}^{2}=\\left(L^{2}+\\frac{2L_{x y}^{2}C_{f}^{2}}{\\mu^{2}}\\right)}\\end{array}$ . Combine Eq. (27) and Eq. (28), we have: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle h({x}_{t+k})\\leq h({x}_{t})-\\frac{\\bar{\\eta}_{t}}{2}\\|\\nabla h({x}_{t})\\|^{2}-\\frac{\\bar{\\eta}_{t}}{4}\\|\\bar{\\nu}_{t}\\|^{2}+\\frac{3}{2\\bar{\\eta}_{t}}\\big\\|\\sum_{\\tau=t}^{t+k-1}\\eta_{\\tau}\\big(\\nabla h({x}_{t})-\\nabla h({x}_{t};{\\xi}_{\\tau})\\big)\\big\\|^{2}}\\\\ {\\displaystyle+\\,\\frac{3\\bar{L}^{2}}{2}\\sum_{\\tau=t}^{t+k-1}\\eta_{\\tau}\\big\\|{x}_{t}-{x}_{\\tau}\\big\\|^{2}+\\sum_{\\tau=t}^{t+k-1}\\eta_{\\tau}\\big(3\\tilde{L}_{1}^{2}\\|y_{x_{\\tau}}^{\\xi_{\\tau}}-y_{\\tau,T_{l}}\\|^{2}+6L^{2}\\|u_{x_{\\tau}}^{\\xi_{\\tau}}-u_{\\tau,T_{l}}\\|^{2}\\big)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "For the term $\\begin{array}{r}{\\sum_{\\tau=t}^{t+k-1}\\eta_{\\tau}\\bigl\\|x_{t}-x_{\\tau}\\bigr\\|^{2}}\\end{array}$ , by Eq. (27) and Eq. (28), we have: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|x_{t}-x_{\\tau}\\right\\|^{2}\\leq2\\displaystyle\\prod_{\\tau=0}^{\\tau-1}\\eta_{\\tau}\\left(\\nu_{\\tau}-\\nabla h(x_{t})\\right)\\|^{2}+2(\\bar{\\eta}_{t}^{\\tau})^{2}\\|\\nabla h(x_{t})\\|^{2}}\\\\ &{\\qquad\\qquad\\leq6\\|\\displaystyle\\sum_{\\tau=t}^{\\tau-1}\\eta_{\\tau}\\big(\\nabla h(x_{t})-\\nabla h(x_{t};\\xi_{\\tau})\\big)\\big\\|^{2}+6\\bar{L}^{2}\\eta_{\\tau}^{\\tau-1}\\eta_{\\tau}\\big\\|x_{t}-x_{\\tau}\\big\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad+6\\eta_{t}^{\\tau}\\displaystyle\\sum_{\\tau=t}^{\\tau-1}\\eta_{\\tau}\\big(2\\bar{L}_{1}^{2}\\|y_{x_{\\tau}}^{\\varepsilon}-y_{\\tau},\\mathbb{L}\\|^{2}+4L^{2}\\|u_{x_{\\tau}}^{\\varepsilon}-u_{\\tau},\\mathbb{L}\\|^{2}\\big)+2(\\bar{\\eta}_{t}^{\\tau})^{2}\\|\\nabla h(x_{t})\\|^{2}}\\\\ &{\\qquad\\qquad\\leq6\\eta_{t}^{2}k^{2-\\alpha}C^{2}+6\\bar{L}^{2}\\eta_{t}\\displaystyle\\sum_{\\tau=t}^{t+k-1}\\eta_{\\tau}\\big\\|x_{t}-x_{\\tau}\\big\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad+6\\eta_{t}+\\displaystyle\\sum_{\\tau=t}^{t+k-1}\\eta_{\\tau}\\big(2\\bar{L}_{1}^{2}\\|y_{x_{\\tau}}^{\\varepsilon}-y_{\\tau},\\mathbb{L}\\|^{2}+4L^{2}\\|u_{x_{\\tau}}^{\\varepsilon}-u_{\\tau},\\mathbb{L}\\|^{2}\\big)+2(\\bar{\\eta}_{t})^{2}\\|\\nabla h(x_{t})\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Multiply $\\eta_{\\tau}$ on both sides and sum over $[t,t+k-1]$ , we have: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{1-6\\bar{L}^{2}(\\bar{\\eta}_{t})^{2}\\displaystyle\\sum_{\\tau=t}^{t+k-1}\\eta_{\\tau}\\big\\|x_{t}-x_{\\tau}\\big\\|^{2}\\leq6(\\bar{\\eta}_{t})^{2}\\displaystyle\\sum_{\\bar{\\tau}=t}^{t+k-1}\\eta_{\\tau}\\big(2\\tilde{L}_{1}^{2}\\|y_{x_{\\tau}}^{\\xi_{\\tau}}-y_{\\bar{\\tau},T_{t}}\\|^{2}+4L^{2}\\|u_{x_{\\tau}}^{\\xi_{\\tau}}-u_{\\bar{\\tau},T_{t}}\\|^{2}\\big)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\displaystyle2(\\bar{\\eta}_{t})^{3}\\|\\nabla h(x_{t})\\|^{2}+6\\bar{\\eta}_{t}\\eta_{t}^{2}k^{2-\\alpha}C^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By the condition that $\\begin{array}{r}{\\bar{\\eta}_{t}<\\frac{1}{4L}}\\end{array}$ , we have: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{\\tau=t}^{t+k-1}\\eta_{\\tau}\\big\\|x_{t}-x_{\\tau}\\big\\|^{2}\\leq12(\\bar{\\eta}_{t})^{2}\\displaystyle\\sum_{\\bar{\\tau}=t}^{t+k-1}\\eta_{\\bar{\\tau}}\\big(2\\tilde{L}_{1}^{2}\\|y_{x_{\\tau}}^{\\xi_{\\tau}}-y_{\\bar{\\tau},T_{l}}\\|^{2}+4L^{2}\\|u_{x_{\\tau}}^{\\xi_{\\tau}}-u_{\\bar{\\tau},T_{l}}\\|^{2}\\big)}&{}\\\\ {+\\,4(\\bar{\\eta}_{t})^{3}\\|\\nabla h(x_{t})\\|^{2}+12\\bar{\\eta}_{t}\\eta_{t}^{2}k^{2-\\alpha}C^{2}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Combine Eq. (30) with Eq. (29), then we have: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\iota(x_{t+k})\\leq h(x_{t})-(\\frac{\\bar{\\eta}_{t}}{2}-6\\bar{L}^{2}(\\bar{\\eta}_{t})^{3})\\|\\nabla h(x_{t})\\|^{2}-\\frac{\\bar{\\eta}_{t}}{4}\\|\\bar{\\nu}_{t}\\|^{2}+\\frac{3}{2\\bar{\\eta}_{t}}\\|\\sum_{\\tau=t}^{t+k-1}\\eta_{\\tau}\\big(\\nabla h(x_{t})-\\nabla h(x_{t};\\xi_{\\tau})\\big)}}\\\\ &{}&{+\\,18\\bar{L}^{2}\\bar{\\eta}_{t}\\eta_{t}^{2}k^{2-\\alpha}C^{2}+(1+12(\\bar{\\eta}_{t})^{2}\\bar{L}^{2})\\underbrace{\\iota+k-1}_{\\tau=t}\\eta_{\\tau}\\big(3\\bar{L}_{1}^{2}\\|y_{x_{\\tau}}^{\\xi_{\\tau}}-y_{\\tau,T_{1}}\\|^{2}+6L^{2}\\|u_{x_{\\tau}}^{\\xi_{\\tau}}-u_{\\tau}\\|^{2}\\big)}\\\\ &{}&{\\leq h(x_{t})-\\frac{\\bar{\\eta}_{t}}{8}\\|\\nabla h(x_{t})\\|^{2}-\\frac{\\bar{\\eta}_{t}}{4}\\|\\bar{\\nu}_{t}\\|^{2}+\\frac{3}{\\bar{\\eta}_{t}}\\eta_{t}k^{2-\\alpha}C^{2}}\\\\ &{}&{+\\,\\sum_{\\tau=t}^{t+k-1}\\eta_{\\tau}\\big(6\\tilde{L}_{1}^{2}\\|y_{x_{\\tau}}^{\\xi_{\\tau}}-y_{\\tau,T_{1}}\\|^{2}+12L^{2}\\|u_{x_{\\tau}}^{\\xi_{\\tau}}-u_{\\tau,T_{1}}\\|^{2}\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the second inequality uses the conditions that $\\begin{array}{r}{\\bar{\\eta}_{t}<\\frac{1}{4L}}\\end{array}$ . By Lemma B.16,we have: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\iota(x_{t+k})\\leq h(x_{t})-\\displaystyle\\frac{k\\eta}{8}\\|\\nabla h(x_{t})\\|^{2}-\\displaystyle\\frac{k\\eta}{4}\\|\\bar{\\nu}_{t}\\|^{2}+3\\big(1+\\displaystyle\\frac{4\\kappa^{2}\\big(192C_{f}^{2}+96*24\\tilde{L}_{2}^{2}\\big)}{\\mu^{2}}+\\displaystyle\\frac{48\\tilde{L}_{1}^{2}}{\\mu^{2}}\\big)\\eta k^{1-\\alpha}C}\\\\ &{}&{+\\ 12\\eta k L^{2}\\big(1-\\displaystyle\\frac{k\\mu\\rho}{2}\\big)^{E_{l}}\\Delta_{u}+3\\eta k\\big(2\\tilde{L}_{1}^{2}+4*96\\kappa^{2}\\tilde{L}_{2}^{2}\\big)\\big(1-\\displaystyle\\frac{k\\mu\\gamma}{2}\\big)^{E_{l}}\\Delta_{y}\\qquad\\qquad\\qquad(32)}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\Delta_{u}$ and $\\Delta_{y}$ denotes the upper bounds of the initial estimation errors of $y_{x}$ and $u_{x}$ . This completes the proof. $\\sqsupset$ ", "page_idx": 24}, {"type": "text", "text": "Lemma B.15. Suppose we have $\\begin{array}{r l r}{\\bar{\\gamma}_{l}}&{{}<}&{\\frac{1}{2L}}\\end{array}$ and $\\bar{\\rho}_{l}\\ <\\ \\frac{1}{2L}$ , then $\\begin{array}{r}{\\sum_{l^{\\prime}=l}^{l+k-1}\\gamma_{l^{\\prime}}\\|y_{l^{\\prime}}\\;-\\;y_{l}\\|^{2}}\\end{array}$ and $\\begin{array}{r l}{\\sum_{l^{\\prime}=l}^{l+k-1}\\rho_{l^{\\prime}}\\Vert u_{l}-u_{l^{\\prime}}\\Vert^{2}}\\end{array}$ can be bounded as in Eq. (33) and Eq. (34) ", "page_idx": 24}, {"type": "text", "text": "Proof. We first bound $\\begin{array}{r}{\\sum_{l^{\\prime}=l}^{l+k-1}\\gamma_{l^{\\prime}}\\Vert y_{l^{\\prime}}-y_{l}\\Vert^{2}}\\end{array}$ , in fact, we have: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{y_{l}-y_{l^{\\prime}}\\|^{2}=\\|\\displaystyle\\sum_{\\bar{l}^{\\prime}=l}^{l^{\\prime}-1}{\\gamma_{\\bar{l}^{\\prime}}w_{\\bar{l}^{\\prime}}\\|^{2}\\leq2\\|\\displaystyle\\sum_{\\bar{l}^{\\prime}=l}^{l^{\\prime}-1}{\\gamma_{\\bar{l}^{\\prime}}\\left(\\nabla_{y}g^{\\zeta_{l^{\\prime}}}(x,y_{\\bar{l}^{\\prime}})-\\nabla_{y}g(x,y_{l})\\right)\\|^{2}}+2(\\bar{\\gamma}_{\\bar{l}}^{l^{\\prime}})^{2}\\|\\nabla_{y}g(x,y_{l})\\|^{2}}}}\\\\ {{\\leq4L^{2}\\bar{\\gamma}_{l}^{l^{\\prime}}\\displaystyle\\sum_{\\bar{l}^{\\prime}=l}^{l^{\\prime}-1}{\\gamma_{\\bar{l}^{\\prime}}\\|y_{\\bar{l}^{\\prime}}-y_{l}\\|^{2}}+2(\\bar{\\gamma}_{\\bar{l}}^{l^{\\prime}})^{2}\\|\\nabla_{y}g(x,y_{l})\\|^{2}+4\\|\\displaystyle\\sum_{\\bar{l}^{\\prime}=l}^{l^{\\prime}-1}{\\gamma_{\\bar{l}^{\\prime}}\\left(\\nabla_{y}g^{\\zeta_{l^{\\prime}}}(x,y_{l})-\\nabla_{y}g(x,y_{l})\\right)}}}\\\\ {{\\leq4L^{2}\\bar{\\gamma}_{l}\\displaystyle\\sum_{\\bar{l}^{\\prime}=l}^{l+k-1}{\\gamma_{\\bar{l}}\\|y_{\\bar{l}^{\\prime}}-y_{l}\\|^{2}}+(\\bar{\\gamma}_{\\bar{l}})^{2}\\|\\nabla_{y}g(x,y_{l})\\|^{2}+4\\gamma_{\\bar{l}}^{2}k^{2-\\alpha}C^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Multiple $\\gamma_{l^{'}}$ on both sides, and then sum $l^{'}$ in $[l,l+k-1]$ , we have: ", "page_idx": 25}, {"type": "equation", "text": "$$\n(1-4L^{2}\\bar{\\gamma}_{l}^{2})\\sum_{l^{\\prime}=l}^{l+k-1}\\gamma_{l^{\\prime}}\\Vert y_{l^{\\prime}}-y_{l}\\Vert^{2}\\leq2(\\bar{\\gamma}_{l})^{3}\\Vert\\nabla_{y}g(x,y_{l})\\Vert^{2}+4\\bar{\\gamma}_{l}\\gamma_{l}^{2}k^{2-\\alpha}C^{2}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Since we have $\\begin{array}{r}{\\bar{\\gamma}_{l}<\\frac{1}{2L}}\\end{array}$ , we have: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\sum_{l^{\\prime}=l}^{l+k-1}\\gamma_{l^{\\prime}}\\Vert y_{l^{\\prime}}-y_{l}\\Vert^{2}\\leq4(\\bar{\\gamma}_{l})^{3}\\Vert\\nabla_{y}g(x,y_{l})\\Vert^{2}+8\\bar{\\gamma}_{l}\\gamma_{l}^{2}k^{2-\\alpha}C^{2}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Next for $\\begin{array}{r l}{\\sum_{l^{\\prime}=l}^{l+k-1}\\rho_{l^{\\prime}}\\Vert u_{l}-u_{l^{\\prime}}\\Vert^{2}}\\end{array}$ , we have: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{u_{t}-u_{t^{\\prime}}\\|^{2}=\\displaystyle\\|\\sum_{\\stackrel{t}{t}=t}^{t-1}\\rho_{t}\\dot{\\tau}_{t^{\\prime}}\\|^{2}\\leq2\\|\\sum_{\\stackrel{t}{t}=t}^{t^{\\prime}-1}\\rho_{t}\\left(q_{t^{\\prime}}-\\nabla r_{x}(u_{t})\\right)\\|^{2}+2(\\overline{{\\rho_{t}^{\\prime}}})^{2}\\|\\nabla r_{x}(u_{t})\\|^{2}}}\\\\ {{\\leq12\\tilde{L}_{2}^{2}\\left(\\overline{{\\rho_{t}^{\\prime}}}\\right)^{2}\\|y_{x}-y_{T1}\\|^{2}+8L^{2}\\widehat{\\rho_{t}^{\\prime}}\\displaystyle\\sum_{\\stackrel{t}{t}=t}^{t^{\\prime}-1}\\rho_{t}\\left\\|u_{t}-u_{t^{\\prime}}\\right\\|^{2}+2(\\overline{{\\rho_{t}^{\\prime}}})^{2}\\|\\nabla r_{x}(u_{t})\\|^{2}}}\\\\ {{+\\displaystyle\\frac{24C_{f}^{2}}{\\mu^{2}}\\displaystyle\\|\\sum_{\\stackrel{t}{t}=t}^{t^{\\prime}-1}\\rho_{t^{\\prime}}(\\nabla_{y^{2}}g(x,y_{T1})-\\nabla_{y^{2}}g^{\\zeta_{t^{\\prime}}}(x,y_{T1}))\\|^{2}}}\\\\ {{\\leq12\\tilde{L}_{2}^{2}(\\overline{{\\rho_{t}}})^{2}\\|y_{x}-y_{T1}\\|^{2}+8L^{2}\\widehat{\\rho_{t}}\\displaystyle\\sum_{\\stackrel{t}{t}=t}^{t+k-1}\\rho_{t}\\gamma\\|u_{t}-u_{t^{\\prime}}\\|^{2}+2(\\bar{\\rho_{t}})^{2}\\|\\nabla r_{x}(u_{t})\\|^{2}+\\displaystyle\\frac{24C_{f}^{2}}{\\mu^{2}}\\rho_{t}^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Multiple $\\rho_{l^{\\prime}}$ on both sides, and then sum $l^{'}$ in $[l,l+k-1]$ , we have: ", "page_idx": 25}, {"type": "equation", "text": "$$\n1-8L^{2}\\bar{\\rho}_{l}^{2})\\sum_{l^{\\prime}=l}^{l+k-1}\\rho_{l^{\\prime}}\\|u_{l}-u_{l^{\\prime}}\\|^{2}\\leq12\\tilde{L}_{2}^{2}(\\bar{\\rho}_{l})^{3}\\|y_{x}-y_{T_{l}}\\|^{2}+2(\\bar{\\rho}_{l})^{3}\\|\\nabla r_{x}(u_{l})\\|^{2}+\\frac{24C_{f}^{2}}{\\mu^{2}}\\bar{\\rho}_{l}\\rho_{l}^{2}k^{2-\\alpha}C_{f}^{2},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Since we have $\\begin{array}{r}{\\bar{\\rho}_{l}<\\frac{1}{2L}}\\end{array}$ , we have: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\sum_{l^{\\prime}=l}^{l+k-1}\\rho_{l^{\\prime}}\\|u_{l}-u_{l^{\\prime}}\\|^{2}\\leq24\\tilde{L}_{2}^{2}(\\bar{\\rho}_{l})^{3}\\|y_{x}-y_{T_{l}}\\|^{2}+4(\\bar{\\rho}_{l})^{3}\\|\\nabla r_{x}(u_{l})\\|^{2}+\\frac{48C_{f}^{2}}{\\mu^{2}}\\bar{\\rho}_{l}\\rho_{l}^{2}k^{2-\\alpha}C^{2}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Lemma B.16. When \u03b3\u00aft <1281L\u03ba and \u03c1\u00aft <2561L\u03ba , the error of the inner variable $y_{l}$ and the variable $u_{l}$ are bounded through Eq. (35) and Eq. (36). ", "page_idx": 25}, {"type": "text", "text": "Proof. Since the outer iteration $t$ is fixed for each inner loop, we omit $t$ and $\\xi_{t}$ in the notation for clarity. Furthermore, we assume performing the updates to $y$ and $u$ separately in Algorithm 2. Although the alternative updates in Algorithm 2 is appealing in practice as we can reuse $\\nabla_{y}g$ for ", "page_idx": 25}, {"type": "text", "text": "$\\nabla_{y^{2}}g$ , update $y$ first and $u$ next can avoid treating complicated higher order terms. Follow similar derivation of Lemma B.10, if $\\begin{array}{r}{\\bar{\\gamma}_{t}<\\frac{1}{4L}}\\end{array}$ , we have: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle|y_{l+k}-y_{x}||^{2}\\leq(1-\\frac{\\mu\\bar{\\gamma}_{l}}{2})\\|y_{l}-y_{x}\\|^{2}-\\frac{\\bar{\\gamma}_{l}^{2}}{2}\\|\\nabla_{y}g(x,y_{l})\\|^{2}}\\\\ {\\displaystyle\\qquad\\qquad+\\,8L\\kappa\\sum_{l^{\\prime}=l}^{l+k-1}\\gamma_{l^{\\prime}}\\|y_{l^{\\prime}}-y_{l}\\|^{2}+\\frac{8}{\\mu\\bar{\\gamma}_{l}}\\|\\sum_{l^{\\prime}=l}^{l+k-1}\\gamma_{l^{\\prime}}\\left(\\nabla_{y}g(x,y_{l})-\\nabla_{y}g(x,y_{l};\\zeta_{l^{\\prime}})\\right)\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Combine with Eq. (33) and use the condition that $\\begin{array}{r}{\\bar{\\gamma}_{l}<\\frac{1}{128L\\kappa}<\\frac{1}{4L}}\\end{array}$ , we have: ", "page_idx": 26}, {"type": "equation", "text": "$$\ny_{l+k}-y_{x}\\|^{2}\\leq(1-\\frac{\\mu\\bar{\\gamma}_{l}}{2})\\|y_{l}-y_{x}\\|^{2}-\\frac{\\bar{\\gamma}_{l}^{2}}{4}\\|\\nabla_{y}g(x,y_{l})\\|^{2}+64L\\kappa\\bar{\\gamma}_{l}\\gamma_{l}^{2}k^{2-\\alpha}C^{2}+\\frac{8}{\\mu\\bar{\\gamma}_{l}}\\gamma_{l}^{2}k^{2-\\alpha}C^{2}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Suppose we perform $E_{l}$ rounds, then by telescoping, we have: ", "page_idx": 26}, {"type": "equation", "text": "$$\ny_{T_{l}}-y_{x}\\|^{2}\\leq(1-\\frac{k\\mu\\gamma}{2})^{E_{l}}\\|y_{0}-y_{x}\\|^{2}+\\frac{24k^{-\\alpha}C^{2}}{\\mu^{2}}+\\sum_{e=0}^{E_{l}-1}(1-\\frac{k\\mu\\gamma}{2})^{E_{l}-e}\\bigg(-\\frac{k^{2}\\gamma^{2}}{4}\\|\\nabla_{y}g(x,y_{e k})\\|^{2}\\bigg).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Next, follow similar derivations as in Lemma B.11 and choose $\\begin{array}{r}{\\bar{\\rho}_{t}<\\frac{1}{4L}}\\end{array}$ , then we have: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\bf\\sigma}_{-k}-u_{x}\\|^{2}\\leq(1-\\frac{\\mu\\bar{\\rho}_{l}}{2})\\|u_{l}-u_{x}\\|^{2}-\\frac{(\\bar{\\rho}_{l})^{2}}{2}\\|\\nabla r_{x}(u_{l})\\|^{2}+\\frac{24\\tilde{L}_{2}^{2}(\\bar{\\rho}_{t})}{\\mu}\\|y_{x}-y_{T_{l}}\\|^{2}}}\\\\ {{\\displaystyle\\qquad\\qquad+16L\\kappa\\sum_{l^{\\prime}=l}^{l+k-1}\\rho_{l^{\\prime}}\\|u_{l}-u_{l^{\\prime}}\\|^{2}+\\frac{48C_{f}^{2}}{\\mu^{3}\\bar{\\rho}_{l}}\\|\\sum_{l^{\\prime}=l}^{l+k-1}\\rho_{l^{\\prime}}\\left(\\nabla_{y^{2}}g(x,y_{T_{l}})-\\nabla_{y^{2}}g(x,y_{T_{l}};\\zeta_{l^{\\prime}})\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Combine with Eq. (34) and use the condition that $\\begin{array}{r}{\\bar{\\rho}_{l}<\\frac{1}{256L\\kappa}<\\frac{1}{4L}}\\end{array}$ , we have: ", "page_idx": 26}, {"type": "equation", "text": "$$\nu_{l+k}-u_{x}\\|^{2}\\leq(1-\\frac{\\mu\\bar{\\rho}_{l}}{2})\\|u_{l}-u_{x}\\|^{2}-\\frac{(\\bar{\\rho}_{l})^{2}}{4}\\|\\nabla r_{x}(u_{l})\\|^{2}+\\frac{48\\tilde{L}_{2}^{2}(\\bar{\\rho}_{l})}{\\mu}\\|y_{x}-y_{T_{l}}\\|^{2}+\\frac{96C_{f}^{2}}{\\mu^{3}\\bar{\\rho}_{l}}\\rho_{l}^{2}k^{2-\\bar{\\epsilon}}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "we perform $E_{l}$ rounds, then by telescoping, we have: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\|u_{T_{l}}-u_{x}\\|^{2}\\leq(1-\\frac{k\\mu\\rho}{2})^{E_{l}}\\|u_{0}-u_{x}\\|^{2}+\\frac{192C_{f}^{2}C^{2}k^{-\\alpha}}{\\mu^{4}}+\\frac{96\\tilde{L}_{2}^{2}}{\\mu^{2}}\\|y_{x}-y_{T_{l}}\\|^{2}}\\\\ {+\\displaystyle\\sum_{e=0}^{E_{l}-1}(1-\\frac{k\\mu\\rho}{2})^{E_{l}-e}\\bigg(-\\frac{k^{2}\\rho^{2}}{4}\\|\\nabla_{y}r_{x}(u_{e k})\\|^{2}\\bigg)}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "This completes the proof. ", "page_idx": 26}, {"type": "text", "text": "Theorem B.17. Suppose Assumptions 4.1-4.4 and 4.7 are satisfied, and Assumption 4.5 is satisfied with some $\\alpha$ and $C$ for the example order we use in Algorithm 2. We choose learning rates $\\eta_{t}=\\eta=$ $\\frac{1}{8k L}$ , $\\begin{array}{r}{\\gamma_{t}=\\gamma=\\frac{1}{256k L\\kappa}}\\end{array}$ , $\\begin{array}{r}{\\rho_{t}=\\rho\\stackrel{.}{=}\\frac{1}{512k L\\kappa}}\\end{array}$ and denote $T=R\\times m$ be the total number of outer steps and $T_{l}=S\\times\\operatorname*{max}(n_{i})$ be the maximum inner steps. Then for any pair of values $(E,\\,k)$ which has $T=E\\times k$ and $T_{l}=E_{l}\\times k$ , we have: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\frac{1}{E}\\sum_{e=0}^{E-1}\\|\\nabla h(x_{k e})\\|^{2}\\leq\\frac{8\\Delta_{h}}{k E\\eta}+C_{u}(1-\\frac{k\\mu\\rho}{2})^{E_{l}}\\Delta_{u}+C_{y}(1-\\frac{k\\mu\\gamma}{2})^{E_{l}}\\Delta_{y}+(C^{'})^{2}C^{2}k^{-\\alpha}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $C_{u},$ , $C_{y},\\,C^{'}$ , $\\tilde{L}$ are constants related to the smoothness parameters of $h(x),\\,\\Delta_{h},\\,\\Delta_{u},\\,\\Delta_{y}$ are the initial sub-optimality, $R$ and $S$ are defined in Algorithm 2. ", "page_idx": 26}, {"type": "text", "text": "Proof. First, by Lemma B.14, we have: ", "text_level": 1, "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\iota(x_{t+k})\\leq h(x_{t})-\\displaystyle\\frac{k\\eta}{8}\\|\\nabla h(x_{t})\\|^{2}-\\displaystyle\\frac{k\\eta}{4}\\|\\bar{\\nu}_{t}\\|^{2}+3\\big(1+\\displaystyle\\frac{4\\kappa^{2}\\big(192C_{f}^{2}+96*24\\tilde{L}_{2}^{2}\\big)}{\\mu^{2}}+\\displaystyle\\frac{48\\tilde{L}_{1}^{2}}{\\mu^{2}}\\big)\\eta k^{1-\\alpha}C}\\\\ &{\\qquad\\qquad\\qquad+\\ 12\\eta k L^{2}(1-\\displaystyle\\frac{k\\mu\\rho}{2})^{E_{l}}\\Delta_{u}+3\\eta k(2\\tilde{L}_{1}^{2}+4*96\\kappa^{2}\\tilde{L}_{2}^{2})(1-\\displaystyle\\frac{k\\mu\\gamma}{2})^{E_{l}}\\Delta_{y}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Sum the above inequality over $E$ phases to have: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\varepsilon\\eta}{8}\\displaystyle\\sum_{e=0}^{E-1}\\|\\nabla h(x_{k e})\\|^{2}\\leq\\Delta_{h}+12\\eta k E L^{2}(1-\\frac{k\\mu\\rho}{2})^{E_{l}}\\Delta_{u}+3\\eta k E(2\\tilde{L}_{1}^{2}+4*96\\kappa^{2}\\tilde{L}_{2}^{2})(1-\\frac{k\\mu\\gamma}{2})^{E_{l}}\\Delta_{u}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\,3E\\big(1+\\frac{4\\kappa^{2}\\big(192C_{f}^{2}+96*24\\tilde{L}_{2}^{2}\\big)}{\\mu^{2}}+\\frac{48\\tilde{L}_{1}^{2}}{\\mu^{2}}\\big)\\eta k^{1-\\alpha}C^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Divide by $k E\\eta/8$ on both sides to have: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{E}\\sum_{e=0}^{E-1}||\\nabla h(x_{k e})||^{2}\\leq\\frac{8\\Delta_{h}}{k E\\eta}+96L^{2}(1-\\frac{k\\mu\\rho}{2})^{E_{l}}\\Delta_{u}+24(2\\tilde{L}_{1}^{2}+4*96\\kappa^{2}\\tilde{L}_{2}^{2})(1-\\frac{k\\mu\\gamma}{2})^{E_{l}}\\Delta_{y}}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad+\\,24\\big(1+\\frac{4\\kappa^{2}(192C_{f}^{2}+96*24\\tilde{L}_{2}^{2})}{\\mu^{2}}+\\frac{48\\tilde{L}_{1}^{2}}{\\mu^{2}}\\big)k^{-\\alpha}C^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "By the condition \u03b7 < 4k1 L\u00af, \u03b3 $\\begin{array}{r}{\\gamma<\\frac{1}{128k L\\kappa}}\\end{array}$ and $\\begin{array}{r}{\\rho<\\frac{1}{256k L\\kappa}}\\end{array}$ , we choose $\\begin{array}{r}{\\eta=\\frac{1}{8k\\bar{L}}}\\end{array}$ $\\begin{array}{r}{\\gamma=\\frac{1}{256k L\\kappa}}\\end{array}$ and 5121kL\u03ba, then we have: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\frac{1}{E}\\sum_{e=0}^{E-1}\\|\\nabla h(x_{k e})\\|^{2}\\leq\\frac{64\\bar{L}\\Delta_{h}}{E}+96L^{2}(1-\\frac{k\\mu\\rho}{2})^{E_{l}}\\Delta_{u}+24(2\\tilde{L}_{1}^{2}+4*96\\kappa^{2}\\tilde{L}_{2}^{2})(1-\\frac{k\\mu\\gamma}{2})^{E_{l}}\\Delta_{y}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Then to reach an $\\epsilon_{}$ -stationary point, we need to have: ", "page_idx": 27}, {"type": "equation", "text": "$$\nE\\geq\\frac{128\\bar{L}\\Delta}{\\epsilon^{2}},\\mathrm{~and~},k^{\\alpha}\\geq\\frac{48C^{2}}{\\epsilon^{2}}\\bigg(1+\\frac{4\\kappa^{2}(192C_{f}^{2}+96*24\\tilde{L}_{2}^{2})}{\\mu^{2}}+\\frac{48\\tilde{L}_{1}^{2}}{\\mu^{2}}\\bigg)\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and $E_{l}=O(l o g(\\epsilon^{-1}))$ . This completes the proof. ", "page_idx": 27}, {"type": "text", "text": "Proposition B.18. Given Assumptions 4.3-4.4, Assumption 4.7 and Assumption 4.5 hold, then we have: $\\begin{array}{r}{\\|\\sum_{\\bar{\\tau}=t}^{\\tau-1}\\eta_{\\bar{\\tau}}\\big(\\nabla_{y}g(x_{t},y_{x_{t}})-\\nabla_{y}g(x_{\\bar{\\tau}},y_{\\bar{\\tau}};\\zeta_{\\bar{\\tau},1})\\big)\\|^{2}\\,\\le\\eta^{2}k^{2-\\alpha}C^{2}}\\end{array}$ for any $\\tau\\leq t+k-1$ . The same upper bound is achieved for other properties: $\\nabla h(x)$ , $\\nabla_{x}f(x,y),\\nabla_{y}g(x,y)$ , $\\nabla_{y^{2}}g(x,y)$ , $\\nabla_{x y}g(x,y)$ ", "page_idx": 27}, {"type": "text", "text": "Proof. This proposition can be adapted from Lemma 2 of [34]. ", "page_idx": 27}, {"type": "text", "text": "Proposition B.19. Suppose we have function $g(y)$ , which is $L$ -smooth and $\\mu$ -strongly-convex, then suppose $\\begin{array}{r}{\\gamma<\\frac{1}{2L}}\\end{array}$ , the progress made by one step of gradient descent is: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\|y_{t+1}-y^{*}\\|^{2}\\leq\\big(1-\\frac{\\mu\\gamma}{2}\\big)\\|y_{t}-y^{*}\\|^{2}-\\frac{\\gamma^{2}}{2}\\|\\nabla_{y}g(y_{t})\\|^{2}+\\frac{4\\gamma}{\\mu}\\|\\nabla_{y}g(y_{t})-w_{t}\\|^{2}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $y^{\\ast}$ is the minimum of $g(y)$ and we have update rule $y_{t+1}=y_{t}-\\gamma\\omega_{t}$ . ", "page_idx": 27}, {"type": "text", "text": "Proof. First, by the strong convexity of of function $g(y)$ , we have: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{g(y^{*})\\geq g(y_{t})+\\langle\\nabla_{y}g(y_{t}),y^{*}-y_{t}\\rangle+\\displaystyle\\frac{\\mu}{2}\\|y^{*}-y_{t}\\|^{2}}}\\\\ {{\\quad\\quad=g(y_{t})+\\langle\\nabla_{y}g(y_{t}),y^{*}-y_{t+1}\\rangle+\\langle\\nabla_{y}g(y_{t}),y_{t+1}-y_{t}\\rangle+\\displaystyle\\frac{\\mu}{2}\\|y^{*}-y_{t}\\|^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Then by $L$ -smoothness, we have: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\frac{L}{2}\\|y_{t+1}-y_{t}\\|^{2}\\geq g(y_{t+1})-g(y_{t})-\\langle\\nabla_{y}g(y_{t}),y_{t+1}-y_{t}\\rangle\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Combining the 37 with 38, we have: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{g(y^{*})\\geq g(y_{t+1})+\\langle\\nabla_{y}g(y_{t}),y^{*}-y_{t}\\rangle+\\gamma\\langle\\nabla_{y}g(y_{t}),w_{t}\\rangle+\\frac{\\mu}{2}\\|y^{*}-y_{t}\\|^{2}-\\frac{L}{2}\\|y_{t+1}-y_{t}\\|^{2}}\\\\ &{\\qquad\\geq g(y_{t+1})+\\frac{\\gamma}{2}\\|w_{t}\\|^{2}+\\frac{\\gamma}{2}\\|\\nabla_{y}g(y_{t})\\|^{2}-\\frac{\\gamma}{2}\\|w_{t}-\\nabla_{y}g(y_{t})\\|^{2}+\\langle w_{t},y^{*}-y_{t}\\rangle}\\\\ &{\\qquad\\qquad+\\langle\\nabla_{y}g(y_{t})-w_{t},y^{*}-y_{t}\\rangle+\\frac{\\mu}{2}\\|y^{*}-y_{t}\\|^{2}-\\frac{L\\gamma^{2}}{2}\\|\\omega_{t}\\|^{2}}\\\\ &{\\qquad\\geq g(y_{t+1})+\\big(\\frac{\\gamma}{2}-\\frac{L\\gamma^{2}}{2}\\big)\\|w_{t}\\|^{2}+\\frac{\\gamma}{2}\\|\\nabla_{y}g(y_{t})\\|^{2}-\\frac{\\gamma}{2}\\|w_{t}-\\nabla_{y}g(y_{t})\\|^{2}+\\langle w_{t},y^{*}-y_{t}\\rangle}\\\\ &{\\qquad\\qquad+\\langle\\nabla_{y}g(y_{t})-w_{t},y^{*}-y_{t}\\rangle+\\frac{\\mu}{2}\\|y^{*}-y_{t}\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "By definition of $y^{\\ast}$ , we have $g(y^{\\ast})\\geq g(y_{t+1})$ . Thus, we obtain ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{0\\geq\\big(\\frac{\\gamma}{2}-\\frac{L\\gamma^{2}}{2}\\big)\\|w_{t}\\|^{2}+\\frac{\\gamma}{2}\\|\\nabla_{y}g(y_{t})\\|^{2}-\\frac{\\gamma}{2}\\|w_{t}-\\nabla_{y}g(y_{t})\\|^{2}+\\langle w_{t},y^{*}-y_{t}\\rangle}\\\\ {+\\langle\\nabla_{y}g(y_{t})-w_{t},y^{*}-y_{t}\\rangle+\\frac{\\mu}{2}\\|y^{*}-y_{t}\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Considering the outer bound of the second term $\\langle\\nabla_{y}g(y_{t})-w_{t},y^{*}-y_{t}\\rangle$ , we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n-\\left\\langle\\nabla_{y}g(y_{t})-w_{t},y^{*}-y_{t}\\right\\rangle\\leq\\frac{1}{\\mu}\\|\\nabla_{y}g(y_{t})-w_{t}\\|^{2}+\\frac{\\mu}{4}\\|y^{*}-y_{t}\\|^{2}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Combining with Eq. 39: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{0\\geq\\big(\\frac{\\gamma}{2}-\\displaystyle\\frac{L\\gamma^{2}}{2}\\big)\\|w_{t}\\|^{2}+\\frac{\\gamma}{2}\\|\\nabla_{y}g(y_{t})\\|^{2}-\\big(\\frac{\\gamma}{2}+\\displaystyle\\frac{1}{\\mu}\\big)\\|w_{t}-\\nabla_{y}g(y_{t})\\|^{2}+\\langle w_{t},y^{*}-y_{t}\\rangle}\\\\ {\\quad\\qquad+\\displaystyle\\frac{\\mu}{4}\\|y^{*}-y_{t}\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "By $y_{t+1}=y_{t}-\\gamma\\omega_{t}$ , we have: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{y_{t+1}-y^{*}\\|^{2}=\\|y_{t}-\\gamma\\omega_{t}-y^{*}\\|^{2}=\\|y_{t}-y^{*}\\|^{2}-2\\gamma\\langle\\omega_{t},y_{t}-y^{*}\\rangle+\\gamma^{2}\\|\\omega_{t}\\|^{2}}\\\\ {\\leq\\big(1-\\displaystyle\\frac{\\mu\\gamma}{2}\\big)\\|y_{t}-y^{*}\\|^{2}-\\gamma^{2}\\|\\nabla_{y}g(y_{t})\\|^{2}+L\\gamma^{3}\\|\\omega_{t}\\|^{2}+\\big(\\gamma^{2}+\\displaystyle\\frac{2\\gamma}{\\mu}\\big)\\|\\nabla_{y}g(y_{t})-w_{t}\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Then since we choose $\\begin{array}{r}{\\gamma<\\frac{1}{4L}<\\frac{1}{\\mu}}\\end{array}$ , we obtain: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\|y_{t+1}-y^{*}\\|^{2}\\leq\\big(1-\\frac{\\mu\\gamma}{2}\\big)\\|y_{t}-y^{*}\\|^{2}-\\frac{\\gamma^{2}}{2}\\|\\nabla_{y}g(y_{t})\\|^{2}+\\frac{4\\gamma}{\\mu}\\|\\nabla_{y}g(y_{t})-w_{t}\\|^{2}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "This completes the proof. ", "page_idx": 28}, {"type": "text", "text": "Corollary B.20. For Algorithm 3 and Algorithm 4, suppose Assumptions (the bounded gradient assumption is not required for the minimax problem) and conditions are satisfied as in Theorem 4.6, to reach an $\\epsilon$ -stationary point, we need $\\bar{O(\\epsilon^{-4})}$ examples if they are sampled independently, while $O((\\operatorname*{max}(m,n))^{p}\\epsilon^{-3})$ if some random-permutation-based without-replacement sampling is used. ", "page_idx": 28}, {"type": "text", "text": "Corollary B.21. For Algorithm 5, suppose Assumptions and conditions are satisfied as in Theorem 4.8, to reach an $\\epsilon$ -stationary point, we need $O(\\epsilon^{-\\hat{6}})$ examples if they sampled independently, while $O((\\operatorname*{max}(m,n))^{2p}\\epsilon^{-4})$ if examples are sampled following some random-permutation-based withoutreplacement sampling. ", "page_idx": 28}, {"type": "text", "text": "It is straightforward to derive the above corollaries from Theorem 4.6 and Theorem 4.8, we omit the proof here. ", "page_idx": 28}, {"type": "text", "text": "C Comparison of WiOR-BO with Other Acceleration Methods of Bilevel Optimization ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In this section, we compare our algorithms with other variance reduction based algorithms for bilevel optimization. For the unconditional case, WiOR-BO obtain the same $O(\\epsilon^{-3})$ rate as STORM-based ", "page_idx": 28}, {"type": "text", "text": "Table 2: Comparisons of the Bilevel Opt. & Conditional Bilevel Opt. algorithms for finding an $\\epsilon$ -stationary point. An $\\epsilon$ -stationary point is defined as $\\|\\nabla h(x)\\|\\,\\leq\\,\\epsilon$ . $G c(f,\\epsilon)$ and $G c(g,\\epsilon)$ denote the number of gradient evaluations w.r.t. $f(x,y)$ and $g(x,y);J V(g,\\epsilon)$ denotes the number of Jacobian-vector products; $H V(g,\\epsilon)$ is the number of Hessian-vector products. $m$ and $n$ are the number of data examples for the outer and inner problems, in particular, $n$ is the maximum number of inner problems for conditional bilevel optimization. Our methods have a dependence over example numbers $(m a x(m,n))^{q}$ , $q$ is a value decided by without-replacement sampling strategy and can have value in $[0,1]$ (A herding-based permutation [33] can let $q=0$ ). ", "page_idx": 29}, {"type": "table", "img_path": "BNnZwbZGpm/tmp/9728fda1dfdd1946d6829d5497c3dfe20ff6fa581819f058e44055e9b06b48b0.jpg", "table_caption": [], "table_footnote": [], "page_idx": 29}, {"type": "text", "text": "algorithm MRBO [52], while is slower than SVRG-type algorithms which have $O(n^{q}\\epsilon^{-2})$ such as VRBO [52], SABA [9] and SRBA [10]. This relationship is similar to that for the single level optimization problems. However, as a SGD-type algorithm, WiOR-BO is much simpler to implement in practice compared to STORM-based and ARVG-based algorithms. More specifically, WIORBO has fewer hyper-parameters (learning rates for inner and outer problems) to tune compared to MRBO, which has six independent hyper-parameters, and the optimal theoretical convergence rate is achieved only if the complicated conditions among hyper-parameters are satisfied in Theorem 1 of [52] are satisfied, and this requires significant effort in practice. Next, SRBO/SABA evaluates the full hyper-gradient at the start of each outer loop, where we need to evaluate the first and second order derivatives over all samples in one step, which is very expensive for modern ML models (such as the transformer model with billions of parameters). In contrast, our WiOR-BO never evaluates the full gradient. Note that the inner loop length $I=l c m(m,n)$ is analogous to the concept of \"epoch\" in single level optimization: where we go over the data samples following a given order, but at each step we evaluate gradient over a mini-batch of samples. The practical advantage of our WiOR-BO is further verified by the superior performance over MRBO and VRBO in the Hyper-Data Cleaning Task. ", "page_idx": 29}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We provide detailed discussion of our proposed algorithms in the section \u2018Without Replacement Sampling in Bilevel Optimization\u2019, and followed by theoretical and numerical studies in the subsequent sections. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 30}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Our theoretical analysis is based on a set of assumptions and we discuss their limitations at the start of the section \u2018Theoretical Analysis\u2019. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 30}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Assumptions and Proofs are included in the \u2018Theoretical Analysis\u2019 section in the main text and appendix. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 31}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: we clearly discuss the experimental settings in detail in section \u2018applications and numerical experiments\u2019 ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 31}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: the datasets used in experiments are publicly available and we include the code implementation in the supplementary material. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 32}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: Most of the experimental details can be found in Section 5, while the rest is included in the Appendix A. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 32}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The numerical experiments in the paper are averaged over 5 independent runs and we report the mean and standard deviation (shadowed area). ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 32}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 33}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We state the compute resource to run our experiments at the start of Section 5. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 33}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We follow all of NeurIPS Code of Ethics ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 33}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: This paper presents work related to the theoretical aspect of Machine Learning. The topic of the paper, bilevel optimization, has many applications in machine learning. So there are some potential societal consequences of our work when our algorithms are applied to various machine learning tasks, but none which we feel is directly related to our algorithm. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 34}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 34}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: All the datasets used in our experiments are publicly available Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 35}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 35}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 35}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 36}]