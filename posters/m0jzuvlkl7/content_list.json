[{"type": "text", "text": "AR-Pro: Counterfactual Explanations for Anomaly Repair with Formal Properties ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xiayan Ji\u2217 Anton Xue\u2217 Eric Wong Oleg Sokolsky Insup Lee ", "page_idx": 0}, {"type": "text", "text": "Department of Computer and Information Science University of Pennsylvania Philadelphia, PA 19104 {xjiae,antonxue,exwong,sokolsky,lee}@seas.upenn.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anomaly detection is widely used for identifying critical errors and suspicious behaviors, but current methods lack interpretability. We leverage common properties of existing methods and recent advances in generative models to introduce counterfactual explanations for anomaly detection. Given an input, we generate its counterfactual as a diffusion-based repair that shows what a non-anomalous version should have looked like. A key advantage of this approach is that it enables a domain-independent formal specification of explainability desiderata, offering a unified framework for generating and evaluating explanations. We demonstrate the effectiveness of our anomaly explainability framework, AR-Pro, on vision (MVTec, VisA) and time-series (SWaT, WADI, HAI) anomaly datasets. The code used for the experiments is accessible at: https://github.com/xjiae/arpro. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anomaly detectors measure how much their inputs deviate from an established norm, where too much deviation implies an instance that warrants closer inspection [15, 52]. For example, unusual network traffic may indicate potential malicious attacks that necessitate a review of security logs [16, 30], irregular sensor readings in civil engineering may suggest structural weaknesses [40, 49, 53], and atypical financial transactions point to potential fraud [3, 47]. As a benign occurrence, unexpected anomalies in scientific data can also lead to new insights and discoveries [25, 45]. ", "page_idx": 0}, {"type": "text", "text": "Although state-of-the-art anomaly detectors are good at catching anomalies, they often rely on black-box models. This opacity undermines reliability: inexperienced users might over-rely on the model without understanding its rationale, while experts may not trust model predictions that are not backed by well-founded explanations. This limitation of common machine learning techniques has led to growing interest in model explainability [12], particularly in domains such as medicine [50] and law [14]. We refer to [36, 44] and the references therein for recent surveys on explainability. ", "page_idx": 0}, {"type": "text", "text": "While many anomaly detection methods can localize which parts of the input are anomalous, this may not be a satisfactory explanation, especially when the data is complex. In medicine [20], heart sound recordings [17] and EEG brain data [5] can differ greatly between individuals; in industrial manufacturing, it can be hard to understand subtle defects of PCB for inexperienced workers [73]. Thus, even when the location of the anomaly is known, it may be hard to articulate why it is anomalous. In such cases, it can be helpful to ask the counterfactual question: \u201cWhat should a non-anomalous version look like?\u201d For example, a doctor might ask what changes in a patient\u2019s chest X-ray might improve diagnostic outcomes [58], while a quality assurance engineer may ask what changes would fix the defect [6]. ", "page_idx": 0}, {"type": "image", "img_path": "m0jZUvlKl7/tmp/61eaee1f3664e95ec9a2018b73cf44a2be19d9f6831bd9e42098f37566f8bb79.jpg", "img_caption": ["Figure 1: Overview of the AR-Pro framework. We first identify an input\u2019s anomalous region and then use property-guided diffusion to repair it. This repair is the counterfactual anomaly explanation, where the following properties are defined with respect to a linearly decomposable anomaly detector (AD). (Overall Improvement) The repair has a lower anomaly score. (Similarity) The repair should resemble the original. (Localized Improvement) The score over the repaired region should improve. (Non-degradation) The score over the non-anomalous region should not significantly worsen. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "To answer such counterfactual questions, we begin with two observations. First, anomalies are commonly localized to small regions of the input [13, 38, 73]. Second, recent generative models, e.g., diffusion [24] models, can be trained to produce high-quality, non-anomalous examples. These observations motivate us to repair anomalies as a counterfactual explanation. However, simply generating a repair is not sufficient; we must also ensure its quality. For example, it may be undesirable for a repair to significantly improve the anomaly score but barely resemble the original input. Therefore, it is important to measure the quality of repairs using formal properties. ", "page_idx": 1}, {"type": "text", "text": "However, formalizing broadly applicable properties is challenging, as different domains (e.g., vision, time series) and tasks (e.g., manufacturing, security) have unique considerations. To overcome this, we further observe that many anomaly detectors in practice [4] satisfy a condition that we call linear decomposability: the overall anomaly score is an aggregation of feature-wise anomaly scores. Importantly, this is a strong but common condition with which we may formalize the desiderata of counterfactual explanations. Conveniently, many of these desiderata are, in fact, domainindependent. We leverage these conditions to formalize a general, domain-independent framework for counterfactual anomaly explanation: we use a generative model to produce a repair and then evaluate this repair with respect to the detector model. ", "page_idx": 1}, {"type": "text", "text": "We present an overview of our anomaly explainability framework, AR-Pro, in Figure 1. While anomaly repair [21, 31, 70] and counterfactual explanations [56] have been explored in the literature, we are the first to study this in a unified context. We summarize our contributions as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We observe that many anomaly detectors satisfy linear decomposability and use this condition to define general, domain-independent properties for counterfactual explanations. This approach lets us measure explanation quality with respect to the anomaly detector, as shown in Figure 1. \u2022 We use these properties to guide diffusion models towards a high-quality repair of the anomalous input. Such a repair serves as the counterfactual explanation of the anomaly. \u2022 Our framework, AR-Pro, can produce semantically meaningful repairs that outperform off-theshelf diffusion models with respect to our explainability criteria. Our vision anomaly benchmarks include MVTec [13] and VisA [73]. Our time-series anomaly benchmarks include SWaT [38], HAI [54], and WADI [2]. ", "page_idx": 1}, {"type": "text", "text": "2 Overview and Formal Properties ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In Section 2.1, we first observe that many anomaly detector paradigms are linearly decomposable. Intuitively, this condition means that the overall anomaly score is an aggregation of feature-wise anomaly scores. Next, we use linear decomposability in Section 2.2 to formalize general, domain-independent properties of counterfactual explanations. We will use the terms counterfactual, explanation, and repair interchangeably throughout this paper. ", "page_idx": 1}, {"type": "image", "img_path": "m0jZUvlKl7/tmp/2a45a3540ce4ac196ea325b5a93c6f23d2284bf6635aac1a637ee0e93fc10846.jpg", "img_caption": ["Figure 2: Reconstruction-based anomaly detection exemplifies linear decomposition. The anomalous input $x\\in\\mathbb{R}^{n}$ is first reconstructed into $\\hat{x}\\in\\mathbb{R}^{n}$ , and the feature-wise anomaly scores are given by $\\dot{\\alpha_{i}}(x)=|\\hat{x}_{i}-x_{i}|^{2}\\in\\mathbb{R}^{n}$ for $i=1,\\dots,n$ . Then, the standard $\\ell^{2}$ reconstruction-based anomaly score is a linear combination of the feature scores: $s(x)=\\alpha_{1}(x)+\\cdot\\cdot+\\alpha_{n}(x)$ . "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "2.1 Common Anomaly Detectors are Linearly Decomposable ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Many anomaly detection techniques use a scoring function $s:\\mathbb{R}^{n}\\to\\mathbb{R}$ to measure the anomalousness an input $x\\in\\mathbb{R}^{n}$ , where $s(x)$ is called the anomaly score of $x$ . This is commonly done in a twostage process: the feature-wise anomaly scores $\\alpha_{1}(x),\\ldots,\\alpha_{n}(x)\\in\\mathbb{R}$ are first computed and then aggregated [15]. We observe that this aggregation often satisfies the following form: ", "page_idx": 2}, {"type": "text", "text": "Definition 2.1 (Linear Decomposition). The feature-wise anomaly scores $\\alpha:\\mathbb{R}^{n}\\,\\rightarrow\\,\\mathbb{R}^{n}$ and regularizer $\\beta:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ linearly decomposes the anomaly score $s:\\mathbb{R}^{n}\\to\\mathbb{R}$ if for all $x\\in\\mathbb{R}^{n}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\ns(x)=\\alpha_{1}(x)+\\cdot\\cdot\\cdot+\\alpha_{n}(x)+\\beta(x).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "While linear decomposability appears to be a strong assumption, it is common in practice. We show a number of examples below, where we also flexibly refer to $\\alpha:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}^{n}$ as the feature scores. ", "page_idx": 2}, {"type": "text", "text": "Example 2.2. In reconstruction-based anomaly detection [8, 29], the input $x$ is passed through an encoder-decoder architecture to generate a reconstruction $\\hat{x}$ . The motivation is that it should be harder to reconstruct out-of-distribution (anomalous) inputs. Empirically, it is observed that if $x$ is anomalous, then $\\hat{x}$ will have a large reconstruction error when feature $i$ is in the anomalous region, e.g., a pixel in the defect area of a manufacturing artifact image. A typical example of such an anomaly score is: ", "page_idx": 2}, {"type": "equation", "text": "$$\ns(x)=|{\\hat{x}}_{1}-x_{1}|^{2}+\\cdot\\cdot\\cdot+|{\\hat{x}}_{n}-x_{n}|^{2},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "which is also known as the $\\ell^{2}$ reconstruction error. This lets us define the feature-wise anomaly scores by $\\alpha_{i}(x)=|\\hat{x}_{i}-x_{i}|^{2}$ , and we illustrate this example in Figure 2. ", "page_idx": 2}, {"type": "text", "text": "Example 2.3. In maximum likelihood-based anomaly detection [15], one measures the likelihood of a test input $x$ with respect to a set of non-anomalous training examples. Intuitively, an out-ofdistribution (anomalous) $x$ should be unlikely with respect to the non-anomalous training examples and will thus have a lower likelihood. In variants such as normalizing flow-based anomaly detection for images [66], it is common to define a joint probability distribution over all the features: ", "page_idx": 2}, {"type": "equation", "text": "$$\ns(x)=-\\big[\\log p_{1}(x)+\\cdot\\cdot\\cdot+\\log p_{n}(x)+\\log\\vert\\mathrm{det}\\,J(x)\\vert\\big],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $p_{1}(x),\\ldots,p_{n}(x)$ are the probabilities of each feature that lets us define $\\alpha_{i}(x)=-\\log p_{i}(x)$ , while the change-of-variable Jacobian $\\log\\vert\\operatorname*{det}J(x)\\vert$ may be viewed as a regularization term. ", "page_idx": 2}, {"type": "text", "text": "Example 2.4. In language modeling [61], it is common to measure the anomaly of a token sequence based on the likelihood of each token [7]. Although similar to the vision case, the standard formulation for language models is different: given a token sequence $x_{1},\\ldots,x_{n}\\in\\{1,\\ldots,\\mathsf{v o c a b\\_s i z e}\\}$ , its measure of unlikeliness (anomalousness) may be defined as: ", "page_idx": 2}, {"type": "equation", "text": "$$\ns(x)=-{\\frac{1}{n}}\\left[\\log p(x_{1})+\\log p(x_{2}|x_{1})+\\log p(x_{3}|x_{1},x_{2})+\\cdot\\cdot+\\log p(x_{n}|x_{1},\\ldots,x_{n-1})\\right],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $p$ is a probabilistic generative model, and so $\\alpha_{i}(x)=-(1/n)\\log p(x_{i}|x_{1},\\dots,x_{i-1})$ . This is also known as a preplexity measure, which has been used to detect jailbreaks against LLMs [64]. ", "page_idx": 2}, {"type": "text", "text": "Beyond the above examples, anomaly detectors for time-series data [29, 60, 63] and text [35] also commonly use this convention. We further remark that the feature-wise anomaly scores $\\alpha$ are related to feature attribution scores in explainability literature [33, 51, 55]. ", "page_idx": 2}, {"type": "text", "text": "2.2 Formal Properties for Counterfactual Explanations ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We now present the formal properties of counterfactual explanations. We will assume a given anomaly score function $s$ that is linearly decomposed by the feature-wise score function $\\alpha$ and regularizer $\\beta$ Given some input $x$ , it is common to convert $\\alpha(x)\\in\\mathbb{R}^{n}$ into a binary-valued vector $\\omega(x)\\in\\{0,1\\}^{n}$ to classify which input feature is anomalous, and this is commonly done by a threshold: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\omega(x)=(\\alpha_{1}(x)\\geq\\tau_{1},\\ldots,\\alpha_{n}(x)\\geq\\tau_{n}),\\quad\\mathrm{for~some~feature-wise~threshold~}\\tau\\in\\mathbb{R}^{n}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "A binarization of the feature-wise scores suggests the need for region-specific anomaly scores, which we implement with the following indexing scheme on $s(x)$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\ns_{z}(x)=\\beta(x)+\\sum_{i:z_{i}=1}\\alpha_{i}(x),\\quad{\\mathrm{for~all~}}z\\in\\{0,1\\}^{n}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Then, the anomalous and non-anomalous regions have scores $s_{\\omega(x)}(x)$ and $s_{\\overline{{\\omega}}(x)}(x)$ , respectively, where $\\overline{{\\omega}}(x)=1\\!-\\!\\omega(x)$ denotes non-anomalous region. We next enumerate some common desiderata of counterfactual explanations, where we will refer to the anomalous input as $x_{\\mathsf{b a d}}$ and the repaired version as $x_{\\mathrm{{fix}}}$ . ", "page_idx": 3}, {"type": "text", "text": "Property 1 (Overall Improvement): The anomaly score should improve. Because a \u201crepaired\u201d version should fix the anomaly by definition, one would reasonably expect that: ", "page_idx": 3}, {"type": "equation", "text": "$$\ns(x_{\\mathsf{f i x}})<s(x_{\\mathsf{b a d}}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Property 2 (Similarity): The repair should resemble the original. When $s$ is generated by a complex machine-learning model, it may be the case that it has a value $x_{\\mathsf{f i x}}$ where $s(\\bar{x_{\\mathsf{f i x}}})\\ll s(x_{\\mathsf{b a d}}^{\\mathsf{\\bar{\\alpha}}})$ , but $x_{\\mathrm{{fix}}}$ and $x_{\\mathsf{b a d}}$ bear little resemblance. In the case of vision models, $x_{\\mathrm{{fix}}}$ may even resemble static noise. Such extreme dissimilarities between $x_{\\mathsf{f i x}}$ and $x_{\\mathsf{b a d}}$ are not desirable because a user cannot be expected to feasibly interpret this information. Thus, we desire a similarity condition as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\overline{{\\omega}}(x_{\\mathsf{b a d}})\\odot x_{\\mathsf{b a d}}\\approx\\overline{{\\omega}}(x_{\\mathsf{b a d}})\\odot x_{\\mathsf{f i x}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\odot$ denotes element-wise vector multiplication. This similarity condition states that the nonanomalous regions of the original and the repair, as given by $\\omega(x_{\\mathsf{b a d}})$ , should remain similar. ", "page_idx": 3}, {"type": "text", "text": "Property 3 (Localized Improvement): The anomalous region should improve. However, P1 and P2 are not sufficient. For example, one might have $s(x_{\\mathsf{f i x}})<s(x_{\\mathsf{b a d}})$ , but have a higher score on the anomalous region $\\omega(x_{\\mathsf{b a d}})$ . This is not desirable because it means that $x_{\\mathsf{f i x}}$ has not actually fixed the anomalous region of $x_{\\mathrm{bad}}$ . To ensure progress, we would like: ", "page_idx": 3}, {"type": "equation", "text": "$$\ns_{\\omega(x_{\\mathsf{b a d}})}(x_{\\mathsf{f i x}})<s_{\\omega(x_{\\mathsf{b a d}})}(x_{\\mathsf{b a d}}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Property 4 (Non-degradation): The non-anomalous region should not significantly worsen. Even when the above properties are satisfied, it is possible that the proposed repair could inadvertently increase the anomaly score on the non-anomalous region $\\overline{{\\omega}}(x_{\\mathsf{b a d}})$ . This would mean repairing the anomalous region at the cost of corrupting the non-anomalous parts. We thus state the property against this as follows, where $\\delta_{4}>0$ is a given tolerance threshold: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{S_{\\overline{{\\omega}}\\left(x_{\\mathsf{b a d}}\\right)}\\left(x_{\\mathsf{f i x}}\\right)\\leq s_{\\overline{{\\omega}}\\left(x_{\\mathsf{b a d}}\\right)}(x_{\\mathsf{b a d}})+\\delta_{4}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The benefti of our above formulation is that it encapsulates general, domain-independent desiderata of anomaly repairs. Importantly, this is achieved under the mild assumption of a linearly decomposable anomaly detector. We comment that some overlap among our proposed formal properties may arise in certain scenarios, and alternative sets could be more tailored for specific applications. Our goal, however, is to provide a foundational set of properties that ensures broad applicability, allowing further customization to suit individual application needs. ", "page_idx": 3}, {"type": "text", "text": "3 Property-guided Generation of Counterfactual Explanations ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We now outline the process of conducting a property-guided repair for an anomalous input. In Section 3.1, we first introduce the generalized setup, where we define the four previously outlined properties as objective functions and frame the problem using risk-constrained optimization. Although this formulation clarifies the objectives, it is generally intractable. Therefore, in Section 3.2, we propose a diffusion-based algorithm to approximate the solution and achieve high-quality repairs. ", "page_idx": 3}, {"type": "image", "img_path": "m0jZUvlKl7/tmp/a5505a8f2a62212f17bc8e24b10412812ee3e686073fe82a85d301d2a06fba7f.jpg", "img_caption": ["Figure 3: We run property-guided diffusion with masked in-filling. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "3.1 A Generalized Formulation with Properties as Loss Functions ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We first present a generalized setup for generating repairs. We conceptualize this in terms of a repair model $\\mathcal{R}_{s,\\omega}$ parametrized by an anomaly score function $s:\\mathbb{R}^{n}\\,\\rightarrow\\,\\mathbb{R}$ with known linear decompositions $\\alpha,\\beta$ and a feature-wise binarization $\\omega$ . To generate a repair, we sample the model: ", "page_idx": 4}, {"type": "equation", "text": "$$\nx_{\\mathsf{f i x}}\\sim\\mathcal{R}_{s,\\omega}(x_{\\mathsf{b a d}}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where a probabilistic formulation is relevant in the context of variational auto-encoders [27] or diffusion models [24]. However, it is important that $x_{\\mathrm{{fix}}}$ obeys the formal properties as outlined in Section 2.2. To do this, we cast these properties as loss functions listed below: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L_{1}=s(x_{\\mathsf{f i x}})}\\\\ &{L_{2}=\\left\\|\\overline{{\\omega}}(x_{\\mathsf{b a d}})\\odot(x_{\\mathsf{f i x}}-x_{\\mathsf{b a d}})\\right\\|_{2}}\\\\ &{L_{3}=\\operatorname*{max}\\bigl\\{0,\\,s_{\\omega(x_{\\mathsf{b a d}})}(x_{\\mathsf{f i x}})-s_{\\omega(x_{\\mathsf{b a d}})}(x_{\\mathsf{b a d}})\\bigr\\}}\\\\ &{L_{4}=\\operatorname*{max}\\bigl\\{0,\\,s_{\\overline{{\\omega}}(x_{\\mathsf{b a d}})}(x_{\\mathsf{f i x}})-s_{\\overline{{\\omega}}(x_{\\mathsf{b a d}})}(x_{\\mathsf{b a d}})-\\delta_{4}\\bigr\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Our rationale is as follows. First, because the primary objective of anomaly repair is to reduce the anomaly score, we set $L_{1}$ as simply the score of $x_{\\mathsf{f i x}}$ . Second, we would like to ensure that $x_{\\mathsf{f i x}}$ and $x_{\\mathsf{b a d}}$ are similar in the non-anomalous region, and so formulate $L_{2}$ as the $\\ell^{2}$ distance between $x_{\\mathrm{{fix}}}$ and $x_{\\mathsf{b a d}}$ over $\\overline{{\\omega}}(x_{\\mathsf{b a d}})$ . Third, we formulate $L_{3}$ to apply a penalty when the anomalous region degrades in performance. Fourth, we allow for a degradation of score in the non-anomalous region $\\overline{{\\omega}}(x_{\\mathsf{b a d}})$ , up to some tolerance threshold $\\delta_{4}$ . We cast these as a risk-constrained optimization problem as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\underset{\\theta}{\\mathrm{minimize}}}&{\\underset{x_{\\mathrm{fix}}\\sim\\mathcal{R}_{s,\\omega}(x_{\\mathrm{bad}};\\theta)}{\\mathbb{E}}{\\mathbb{E}}~}&{s(x_{\\mathrm{fix}})}\\\\ {\\mathrm{subject~to}}&{\\underset{x_{\\mathrm{fix}}\\sim\\mathcal{R}_{s,\\omega}(x_{\\mathrm{bad}};\\theta)}{\\mathbb{P}}\\big[L_{2}\\leq\\delta_{2},~L_{3}\\leq0,~L_{4}\\leq0\\big]\\geq1-\\delta}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\theta$ is a parameter of the repair model, $\\delta_{2}\\,>\\,0$ is a given threshold for $L_{2}$ and $\\delta\\,>\\,0$ is a given failure probability for the violation of at least one of (P2), (P3), or (P4). We acknowledge that multiple formulations for property-based losses are valid; however, the chosen approach is optimally suited to our context. ", "page_idx": 4}, {"type": "text", "text": "3.2 Formal Property-guided Diffusion ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We adopt techniques from guided diffusion [19] to generate repairs $x_{\\mathsf{f i x}}$ . We first give a brief overview of a standard diffusion process and then adapt it to perform property-guided generation. We refer to [62] for a comprehensive guide on diffusion but attempt to make the exposition self-contained. ", "page_idx": 4}, {"type": "text", "text": "Background. A basic variant of diffusion models takes the form: ", "page_idx": 4}, {"type": "equation", "text": "$$\np(x_{t-1}|x_{t})=\\mathcal{N}(\\mu_{\\theta}(x_{t},t),b_{t}^{2}I),\\quad\\mathrm{for}\\,t=T,\\dots,1,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mu_{\\theta}$ is the denoising model with parameters $\\theta$ , and $b_{1}<\\cdot\\cdot\\cdot<b_{T}$ is the variance schedule. When $\\mu_{\\theta}$ is trained on non-anomalous data (see [62] for training details), one can generate non-anomalous samples of the training distribution by running the following iterative process: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{x_{T}\\sim{\\mathcal N}(0,I),\\quad x_{t-1}=\\mu_{\\theta}(x_{t},t)+b_{t}z_{t},\\quad z_{t}\\sim{\\mathcal N}(0,I),\\quad\\mathrm{for}\\;t=T,\\ldots,1,}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $x_{T}$ is the initial noise and $x_{0}$ is the output sample. The idea is to repeatedly remove noise from a Gaussian $x_{T}$ using $\\mu_{\\theta}$ until the final $x_{0}$ resembles a high-resolution image without defects, for instance. The iterations (3) is also known as backward process. ", "page_idx": 4}, {"type": "text", "text": "Property-guided Diffusion. We next show how to adapt the denoising iterations (3) to produce repairs. We use two main ideas: first, we use guidance [19] to slightly nudge the iterates $x_{t-1}$ of (3) ", "page_idx": 4}, {"type": "text", "text": "at every step using a property-based loss, to encourage that the final iterate $x_{0}$ , which we take to be $x_{\\mathrm{{fix}}}$ , is more amenable to our properties. Second, we used masked-infilling [39] to ensure that the non-anomalous region ${\\overline{{\\omega}}}\\!\\left(x_{\\mathsf{b a d}}\\right)$ is generally preserved by the iterates. We implement this modified iteration as follows: beginning from the initial noise $x_{\\mathrm{fix},T}\\sim\\mathcal{N}(0,I)$ , let: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{x}_{\\mathrm{fix},t-1}=\\underbrace{\\mu_{\\theta}\\bigl(x_{\\mathrm{fix},t},t\\bigr)+b_{t}z_{t}}_{\\mathrm{Denoising~step}}-\\underbrace{\\eta_{t}\\nabla L(x_{\\mathrm{fix},t})}_{\\mathrm{Guidance~term}},\\quad z_{t}\\sim\\mathcal{N}(0,I),}\\\\ &{\\quad x_{\\mathrm{bad},t}=\\sqrt{a_{t}}x_{\\mathrm{bad}}+\\sqrt{1-a_{t}}\\epsilon_{t},\\quad\\epsilon_{t}\\sim\\mathcal{N}(0,I),}\\\\ &{\\quad x_{\\mathrm{fix},t-1}=\\overline{{\\omega}}(x_{\\mathrm{bad}})\\odot x_{\\mathrm{bad},t}+\\omega\\bigl(x_{\\mathrm{bad}}\\bigr)\\odot\\hat{x}_{\\mathrm{fix},t-1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "for $t=T,\\dots,1$ , where $\\begin{array}{r}{a_{t}=\\prod_{i=1}^{t}(1-b_{i})}\\end{array}$ and $\\eta_{1}\\,<\\,\\cdot\\,\\cdot\\,<\\,\\eta_{T}$ is the guidance schedule. The property-based loss is given by: ", "page_idx": 5}, {"type": "equation", "text": "$$\nL(x_{\\mathrm{fix},t})=\\lambda_{1}L_{1}+\\lambda_{2}L_{2}+\\lambda_{3}L_{3}+\\lambda_{4}L_{4},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "with $L_{1},L_{2},L_{3},L_{4}$ as in Section 3.1 and weights $\\lambda_{1},\\lambda_{2},\\lambda_{3},\\lambda_{4}>0$ . In the above, (4) first generates $\\hat{x}_{\\mathrm{fix},t-1}$ from $x_{\\mathsf{f i x},t}$ by combining a standard denoising step with a guidance term. Then, (6) combines $\\hat{x}_{\\mathrm{fix},t-1}$ with $\\hat{x}_{\\mathsf{b a d},t}$ , from (5), in a masked-infilling operation [39] to yield $x_{\\mathsf{f i x},t-1}$ . This masked in-fliling ensures similarity between $x_{\\mathsf{b a d}}$ and $x_{\\mathrm{{fix}}}$ (i.e., $x_{\\mathsf{f i x},0}$ ) over the non-anomalous region ${\\overline{{\\omega}}}\\!\\left(x_{\\mathsf{b a d}}\\right)$ . ", "page_idx": 5}, {"type": "text", "text": "We emphasize that the diffusion iterations given by (4), (5), (6), and do not guarantee the satisfaction of our formal properties. Rather, these iterations tend toward an output that better respects these properties \u2014 as we later shown in our experiments. In particular, our property-based losses define a way to evaluate the quality to which each property is satisfied or violated. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Our experiments evaluate the performance of AR-Pro across vision and time-series datasets. In particular, we aim to address the following research questions: ", "page_idx": 5}, {"type": "text", "text": "\u2022 (RQ1) Empirical Validation: How well does AR-Pro repair anomalies for different domains? In particular, we investigate how well the four properties are satisfied when the diffusion process is guided or unguided, as in the baseline.   \n\u2022 (RQ2) Ablation Study: How do the different hyper-parameters affect the repair quality? We focus on the weights $\\lambda_{1},\\lambda_{2},\\lambda_{3},\\lambda_{4}$ for the four property-based losses. ", "page_idx": 5}, {"type": "text", "text": "Vision Anomaly Models and Datasets. For anomaly detectors, we used the anomalib [4] implementation of Fastflow [66] (with ResNet-50-2 backbone [68]) and Efficient-AD [11]. For datasets, we used the VisA [73] and MVTec-AD [13] datasets. VisA and MVTec-AD involve anomaly detection in the context of industrial manufacturing, where VisA consists of 12 image classes, and MVTec-AD consists of 15 image classes. Both FastFlow and Efficient-AD were trained with AdamW and a learning rate of $10^{-4}$ until convergence. ", "page_idx": 5}, {"type": "text", "text": "Time-series Anomaly Datasets and Models. For anomaly detectors, we used the GPT-2 [48] and Llama2 [59] architectures for time-series anomaly detection. In particular, we use only the first 6 layers of GPT-2 and the first 4 layers of Llama-2 (with an embedding dimension of 1024) to accelerate training. For datasets, we used the SWaT (51 features) [38], HAI (86 features) [54], and WADI (127 features) [2] datasets, split into sliding windows of size 100. Both our versions of GPT-2 and Llama-2 were trained with AdamW and a learning rate of $10^{-5}$ until convergence. ", "page_idx": 5}, {"type": "text", "text": "Diffusion-based Repair Models. We used the HuggingFace implementation of DDPM [24] for vision data and Diffusion-TS [67] for time-series data. Both models were trained on the non-anomalous instances of their respective datasets using AdamW and a learning rate of $10^{-4}$ until convergence. ", "page_idx": 5}, {"type": "text", "text": "Evaluation Metrics. We use the four property-based loss functions defined in Section 3.1 as our evaluation metrics. In particular, we measure the improvement of property-guided diffusion over un-guided diffusion. We adapt these metrics below, where we write $\\omega$ to mean $\\omega(x_{\\mathsf{b a d}})$ for brevity: ", "page_idx": 5}, {"type": "text", "text": "\u2022 Property 1 (Overall Improvement): $M_{s}\\equiv s(x_{\\sf f i x})$ . \u2022 Property 2 (Similarity): $M_{d}\\equiv\\left|\\left|\\overline{{\\omega}}\\odot\\left(x_{\\mathsf{f i x}}-x_{\\mathsf{b a d}}\\right)\\right|\\right|_{2}$ \u2022 Property 3 (Localized Improvement): $M_{\\omega}\\equiv s_{\\omega}(x_{\\mathsf{f i x}})-s_{\\omega}(x_{\\mathsf{b a d}})$ \u2022 Property 4 (Non-degradation): $M_{\\overline{{\\omega}}}\\equiv s_{\\overline{{\\omega}}}(x_{\\mathsf{f i x}})-s_{\\overline{{\\omega}}}(x_{\\mathsf{b a d}})$ ", "page_idx": 5}, {"type": "table", "img_path": "m0jZUvlKl7/tmp/cc629a3cca2b93ca1c8a8a77cd68e259836b72593a0b4aa29d0c0fb9351d5565.jpg", "table_caption": [], "table_footnote": ["Table 1: AR-Pro outperforms a non-guided diffusion baseline across our four metrics. We show the results for all VisA and MVTec-AD categories, where $\\Delta$ is the median percentage improvement. "], "page_idx": 6}, {"type": "text", "text": "Each experiment employs a representative anomaly detector and dataset with predefined train-test splits. The performance of the anomaly detectors is detailed in Appendix B. For $\\omega$ , each feature-wise threshold $\\tau_{i}$ is taken to be the $90\\%$ quantile of the training set\u2019s feature-wise anomaly scores. ", "page_idx": 6}, {"type": "text", "text": "4.1 (RQ1) Empirical Validation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We now evaluate the performance of AR-Pro for generating anomaly repairs with respect to the four evaluation metrics. We report the mean and standard deviation values. Because there may be a large range of values across classes, we report the median improvement $(\\Delta)$ of the guided generation over the baseline. We present results for the FastFlow anomaly detector on the VisA and MVTec dataset in Table 1, and refer to Appendix C for additional results with Efficient-AD. ", "page_idx": 6}, {"type": "text", "text": "Quantitative Results. For all the categories, the guided results show significant improvement over the baseline on the formal criteria, with an average improvement of $84.27\\%$ . There is a wide range of $M_{s}$ values for baseline models in certain classes, possibly due to deviations in the color scheme of the generated images from their training distribution. Hence, we report the median improvement across the classes in the last row to mitigate the impact of outliers. ", "page_idx": 6}, {"type": "text", "text": "We show results for time-series data in Table 2. AR-Pro achieves lower error on $M_{d},M_{\\omega}$ , and $M_{\\overline{{\\omega}}}$ , with an overall improvement of $60.03\\%$ over the baseline. Specifically, Llama-2 achieves an average improvement of $67.17\\%$ in formal metrics, while GPT-2 increases by $53.90\\%$ on average. Compared to image data, the performance on $M_{d}$ is not as competitive and exhibits considerable variability, likely due to the broader range of adjustments required for repairs to ensure smooth signals. ", "page_idx": 6}, {"type": "text", "text": "In addition, we evaluate whether the guided repairs generate non-anomalous samples by comparing them against a conformity threshold with $95\\%$ confidence derived from the training set [9]. Treating the non-anomalous class as the negative, we report the True Negative Rate (TNR) in Table 3. The TNR of most VisA and MVTec-AD categories reaches $100\\%$ , surpassing the majority of baseline values. As a result, the guided repair achieves a median TNR of $100\\%$ for both VisA and MVTec, representing an average improvement of $2.50\\%$ over the baseline. Across the three time-series datasets (SWaT, HAI, and WADI), the guided repair obtains an average TNR of $95.33\\%$ , which is $92.66\\%$ higher than the baseline average of $2.67\\%$ . Overall, $99.26\\%$ of guided repairs across both domains are classified as non-anomalous with $95\\%$ confidence, statistically confirming the effectiveness of AR-Pro. ", "page_idx": 6}, {"type": "text", "text": "Qualitative Results. We present qualitative examples here to illustrate that our method can generate semantically meaningful repairs, as shown in Figure 4 for VisA and Figure 5 for MVTec-AD. For example in Figure 4, we observe that for categories PCB 1, PCB 2, PCB 3, and PCB 4, the baseline fails to rigorously repair the anomaly: the orientation of the board is reversed for PCB 1; unintended white marks appear on the lower left of PCB 2; the dark lines in the potentiometer change for PCB 3; and the \u201cFC-75\u201d label is missing for PCB 4. However, by integrating formal property guidance, our approach accurately reconstructs all these details while effectively removing the anomalies. For the MVTec-AD examples, AR-Pro produces repairs that more closely resemble the original inputs compared to those generated by the baseline. This demonstrates that our method\u2019s generated repairs adhere more rigorously to the formal properties. ", "page_idx": 6}, {"type": "table", "img_path": "m0jZUvlKl7/tmp/d2e1bfa72c0f8bf0525b4e2583199466266e6bd34443ef2f4371e3937b7b9df0.jpg", "table_caption": [], "table_footnote": ["Table 2: Comparison of baseline and guided performance across four metrics for SWaT, WADI, and HAI dataset categories with Llama2 and GPT2 model. $\\Delta$ is the median improvement percentage of the guided result from baseline. "], "page_idx": 7}, {"type": "table", "img_path": "m0jZUvlKl7/tmp/045ac3449b73822fc72524277b3a11263a1c4d1840864c1ff5281f96a72a1cfa.jpg", "table_caption": [], "table_footnote": ["Table 3: True Negative Rate (TNR) for categories in the vision (VisA, MVTec) and time-series (SWaT, HAI, and WADI) anomaly detection datasets. "], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "In Figure 6, we present examples from our time-series repair, where the anomalous time segment is shaded in red. Our results demonstrate that AR-Pro generates a signal that resembles the original signal better, as shown in the first two plots of Figure 6. In addition, we recover the sensor time series to non-anomalous values when the baseline fails to repair the anomaly, as shown in the last two plots of Figure 6. ", "page_idx": 7}, {"type": "text", "text": "More repair examples are available in Appendix D. However, although the quality of the generated repairs has improved, we notice that this enhancement comes with a trade-off of increased inference time. Further details can be found in Appendix E. ", "page_idx": 7}, {"type": "text", "text": "4.2 (RQ2) Ablation Study ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We randomly sampled 100 instances to compute the mean of each metric in order to evaluate the effect of hyper-parameters $\\lambda_{1},\\lambda_{2},\\lambda_{3},\\lambda_{4}$ associated with each property-based loss. Each line in the plots represents results obtained while keeping the other hyper-parameters at 1.0. The ablation results for the time series are presented in Figure 7, with additional plots in Appendix F. ", "page_idx": 7}, {"type": "image", "img_path": "m0jZUvlKl7/tmp/88e18f74d1dad60d97a0c123ae5539f424d87d5f2aca2ea1a4aadae23baa7e34.jpg", "img_caption": ["Figure 4: The original input and ground truth anomaly mask are displayed in the first two columns. The baseline method fails to preserve close similarity to the input PCB boards, as highlighted in the third column. Guided vision repair examples in the fourth column address these deficiencies. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "m0jZUvlKl7/tmp/2030a891dc70bf28c92384ef6a671d402d1e888293a0e8d5af823059936e0bf6.jpg", "img_caption": ["Figure 5: MVTec repairs with AR-Pro; better resemble the original compared to the baseline. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Our observations indicate that variations in the scale of property hyperparameters do not significantly impact the formal metrics, as the range of change remains relatively small. In addition, no consistent trends were observed when varying the $\\lambda_{1},\\lambda_{2},\\lambda_{3},\\lambda_{4}$ hyper-parameters. This suggests that our framework demonstrates robust performance, and extensive tuning may be unnecessary. ", "page_idx": 8}, {"type": "text", "text": "5 Related Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Numerous techniques have been developed for anomaly detection across various domains [3, 49]. Traditional approaches to anomaly detection include clustering [43, 57] and statistical methods [34] such as ARIMA [41] and Gaussian models [52]. However, these methods often struggle with high-dimensional data. More recently, deep learning-based anomaly detection [20, 28], including autoencoders [27] and GANs [18], can detect high-dimensional anomalies via reconstruction error. For time-series data, LSTMs [32] and transformer-based models [60, 63, 69, 72] have shown exceptional performance. Additionally, diffusion models are emerging as promising tools for visual anomaly detection [42, 71]. While these methods vary in strengths and are continually improving, explaining anomalies remains a challenge [10]. Most current methods rely on feature importance scores or visualizations [44], such as gradients or reconstructions [46], which often fail to provide actionable insights. The lack of formal frameworks [65] and consistent evaluation metrics [1] complicates this issue. For example, the absence of formal metrics leads to inconsistencies in evaluation [26], underscoring the need for more rigorous approaches and reliable criteria [37]. Most similar to our work is [56], which also performs time series-specific generation of counterfactual explanations in the form of anomaly repairs but considers a different set of properties and does not use diffusion. Our work also focuses on generative modeling to produce counterfactual explanations in the form of anomaly repairs. For vision, the current leading paradigms are diffusion models [24] and generative adversarial networks [23]. Diffusion models are also applicable to time-series data [67], and we refer to [22] for a survey on other techniques. ", "page_idx": 8}, {"type": "image", "img_path": "m0jZUvlKl7/tmp/739e314b53d7d4917b19b1c61c9691fe4683d6f7364138d4de0656d89b31ce22.jpg", "img_caption": ["Figure 6: Original input is the blue line, property guided fix is the green line, and the baseline is the red line. The first image shows that the baseline generates a spurious signal when there is no anomaly. The second image shows that the baseline repairs the anomaly, but not as effectively as with guidance. The last two images show instances where the baseline fails to repair the anomaly. "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "m0jZUvlKl7/tmp/648d2fea14559a7efb6a913cb443864f2f19fce7a996b7fc339700ad61704efb.jpg", "img_caption": ["Figure 7: Varying the hyper-parameters does significantly change $M_{s}$ , $M_{d},M_{\\omega}$ , and $M_{\\overline{{\\omega}}}$ . "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "6 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The main theoretical contribution of our work is the identification of common counterfactual explanation desiderata for linearly decomposable anomaly detectors. While we have identified four formal properties, we acknowledge that other valid ones may also exist. Moreover, we recommend that practitioners evaluate and choose the properties necessary for the particular problem, and this is made possible by the form of our diffusion guidance function in (7). The quality of anomaly repairs depends on the performance of the anomaly detector and the generative model. While there may have been limitations in our efforts, we found it challenging to use variational auto-encoders [27] for generating high-quality repairs. Furthermore, our implementation is focused on diffusion models, but the ideas presented can also be extended to other generative techniques. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We present AR-Pro, a framework for generating and evaluating counterfactual explanations in anomaly detection. We use the fact that common anomaly detectors are linearly decomposable, which lets us define formal, general, domain-independent properties for explainability. Using these properties, we show how to generate high-quality counterfactuals using a property-guided diffusion setup. We demonstrate the effectiveness of AR-Pro on vision and time-series datasets and showcase our improvement over off-the-shelf diffusion models. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] C. Agarwal, S. Krishna, E. Saxena, M. Pawelczyk, N. Johnson, I. Puri, M. Zitnik, and H. Lakkaraju. Openxai: Towards a transparent evaluation of model explanations. Advances in Neural Information Processing Systems, 35:15784\u201315799, 2022. [2] C. M. Ahmed, V. R. Palleti, and A. P. Mathur. Wadi: a water distribution testbed for research in the design of secure cyber physical systems. In Proceedings of the 3rd international workshop on cyber-physical systems for smart water networks, pages 25\u201328, 2017. [3] M. Ahmed, A. N. Mahmood, and M. R. Islam. A survey of anomaly detection techniques in financial domain. Future Generation Computer Systems, 55:278\u2013288, 2016.   \n[4] S. Akcay, D. Ameln, A. Vaidya, B. Lakshmanan, N. Ahuja, and U. Genc. Anomalib: A deep learning library for anomaly detection. In 2022 IEEE International Conference on Image Processing (ICIP), pages 1706\u20131710. IEEE, 2022.   \n[5] N. Alahmadi, S. A. Evdokimov, Y. Kropotov, A. M. M\u00fcller, and L. J\u00e4ncke. Different resting state eeg features in children from switzerland and saudi arabia. Frontiers in human neuroscience, 10:559, 2016.   \n[6] A. L. Alfeo and M. G. Cimino. Counterfactual-based feature importance for explainable regression of manufacturing production quality measure. In ICPRAM, pages 48\u201356, 2024.   \n[7] G. Alon and M. Kamfonas. Detecting language model attacks with perplexity. arXiv preprint arXiv:2308.14132, 2023.   \n[8] J. An and S. Cho. Variational autoencoder based anomaly detection using reconstruction probability. Special lecture on IE, 2(1):1\u201318, 2015. [9] A. N. Angelopoulos and S. Bates. A gentle introduction to conformal prediction and distributionfree uncertainty quantification. arXiv preprint arXiv:2107.07511, 2021.   \n[10] L. Antwarg, R. M. Miller, B. Shapira, and L. Rokach. Explaining anomalies detected by autoencoders using shapley additive explanations. Expert systems with applications, 186:115736, 2021.   \n[11] K. Batzner, L. Heckler, and R. K\u00f6nig. Efficientad: Accurate visual anomaly detection at millisecond-level latencies. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 128\u2013138, 2024.   \n[12] V. Belle and I. Papantonis. Principles and practice of explainable machine learning. Frontiers in big Data, 4:688969, 2021.   \n[13] P. Bergmann, M. Fauser, D. Sattlegger, and C. Steger. Mvtec ad\u2013a comprehensive real-world dataset for unsupervised anomaly detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9592\u20139600, 2019.   \n[14] A. Bibal, M. Lognoul, A. De Streel, and B. Fr\u00e9nay. Legal requirements on explainability in machine learning. Artificial Intelligence and Law, 29:149\u2013169, 2021.   \n[15] V. Chandola, A. Banerjee, and V. Kumar. Anomaly detection: A survey. ACM computing surveys (CSUR), 41(3):1\u201358, 2009.   \n[16] Z. Chen, G. Xu, V. Mahalingam, L. Ge, J. Nguyen, W. Yu, and C. Lu. A cloud computing based network monitoring and threat detection system for critical infrastructures. Big Data Research, 3:10\u201323, 2016.   \n[17] G. D. Clifford, C. Liu, B. Moody, D. Springer, I. Silva, Q. Li, and R. G. Mark. Classification of normal/abnormal heart sound recordings: The physionet/computing in cardiology challenge 2016. In 2016 Computing in cardiology conference (CinC), pages 609\u2013612. IEEE, 2016.   \n[18] A. Creswell, T. White, V. Dumoulin, K. Arulkumaran, B. Sengupta, and A. A. Bharath. Generative adversarial networks: An overview. IEEE signal processing magazine, 35(1):53\u201365, 2018.   \n[19] P. Dhariwal and A. Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:8780\u20138794, 2021.   \n[20] T. Fernando, H. Gammulle, S. Denman, S. Sridharan, and C. Fookes. Deep learning for medical anomaly detection\u2013a survey. ACM Computing Surveys (CSUR), 54(7):1\u201337, 2021.   \n[21] A. Filos, P. Tigkas, R. McAllister, N. Rhinehart, S. Levine, and Y. Gal. Can autonomous vehicles identify, recover from, and adapt to distribution shifts? In International Conference on Machine Learning, pages 3145\u20133153. PMLR, 2020.   \n[22] F. Gatta, F. Giampaolo, E. Prezioso, G. Mei, S. Cuomo, and F. Piccialli. Neural networks generative models for time series. Journal of King Saud University-Computer and Information Sciences, 34(10):7920\u20137939, 2022.   \n[23] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139\u2013144, 2020.   \n[24] J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851, 2020.   \n[25] A. H\u00f8st-Madsen, E. Sabeti, and C. Walton. Data discovery and anomaly detection using atypicality: Theory. IEEE Transactions on Information Theory, 65(9):5302\u20135322, 2019.   \n[26] J. Jiang, F. Leofante, A. Rago, and F. Toni. Formalising the robustness of counterfactual explanations for neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence, 2023.   \n[27] D. P. Kingma and M. Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.   \n[28] D. Kwon, H. Kim, J. Kim, S. C. Suh, I. Kim, and K. J. Kim. A survey of deep learning-based network anomaly detection. Cluster Computing, 22:949\u2013961, 2019.   \n[29] C.-Y. Lai, F.-K. Sun, Z. Gao, J. H. Lang, and D. S. Boning. Nominality score conditioned time series anomaly detection by point/sequential reconstruction. arXiv preprint arXiv:2310.15416, 2023.   \n[30] X. A. Larriva-Novo, M. Vega-Barbas, V. A. Villagr\u00e1, and M. S. Rodrigo. Evaluation of cybersecurity data set characteristics for their applicability to neural networks algorithms detecting cybersecurity anomalies. IEEE Access, 8:9005\u20139014, 2020.   \n[31] V. Lin, K. J. Jang, S. Dutta, M. Caprio, O. Sokolsky, and I. Lee. Dc4l: Distribution shift recovery via data-driven control for deep learning models. In 6th Annual Learning for Dynamics & Control Conference, pages 1526\u20131538. PMLR, 2024.   \n[32] B. Lindemann, B. Maschler, N. Sahlab, and M. Weyrich. A survey on anomaly detection for technical systems using lstm networks. Computers in Industry, 131:103498, 2021.   \n[33] S. M. Lundberg and S.-I. Lee. A unified approach to interpreting model predictions. Advances in neural information processing systems, 30, 2017.   \n[34] C. Manikopoulos and S. Papavassiliou. Network intrusion and fault detection: a statistical anomaly approach. IEEE Communications Magazine, 40(10):76\u201382, 2002.   \n[35] A. Manolache, F. Brad, and E. Burceanu. Date: Detecting anomalies in text via self-supervision of transformers. arXiv preprint arXiv:2104.05591, 2021.   \n[36] R. Marcinkevi\u02c7cs and J. E. Vogt. Interpretable and explainable machine learning: a methodscentric overview with concrete examples. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 13(3):e1493, 2023.   \n[37] J. Marques-Silva and A. Ignatiev. Delivering trustworthy ai through formal xai. In Proceedings of the AAAI Conference on Artificial Intelligence, 2022.   \n[38] A. P. Mathur and N. O. Tippenhauer. Swat: A water treatment testbed for research and training on ics security. In 2016 international workshop on cyber-physical systems for smart water networks (CySWater), pages 31\u201336. IEEE, 2016.   \n[39] C. Meng, Y. He, Y. Song, J. Song, J. Wu, J.-Y. Zhu, and S. Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073, 2021.   \n[40] A. Moallemi, A. Burrello, D. Brunelli, and L. Benini. Exploring scalable, distributed real-time anomaly detection for bridge health monitoring. IEEE Internet of Things Journal, 9(18):17660\u2013 17674, 2022.   \n[41] H. Z. Moayedi and M. Masnadi-Shirazi. Arima model for network traffic prediction and anomaly detection. In 2008 international symposium on information technology, volume 4, pages 1\u20136. IEEE, 2008.   \n[42] A. Mousakhan, T. Brox, and J. Tayyub. Anomaly detection with conditioned denoising diffusion models. arXiv preprint arXiv:2305.15956, 2023.   \n[43] G. M\u00fcnz, S. Li, and G. Carle. Traffic anomaly detection using k-means clustering. In Gi/itg workshop mmbnet, volume 7, 2007.   \n[44] M. Nauta, J. Trienes, S. Pathak, E. Nguyen, M. Peters, Y. Schmitt, J. Schl\u00f6tterer, M. van Keulen, and C. Seifert. From anecdotal evidence to quantitative evaluation methods: A systematic review on evaluating explainable ai. ACM Computing Surveys, 55(13s):1\u201342, 2023.   \n[45] G. Pallotta, M. Vespe, and K. Bryan. Vessel pattern knowledge discovery from ais data: A framework for anomaly detection and route prediction. Entropy, 15(6):2218\u20132245, 2013.   \n[46] G. Pang and C. Aggarwal. Toward explainable deep anomaly detection. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, pages 4056\u20134057, 2021.   \n[47] T. Pourhabibi, K.-L. Ong, B. H. Kam, and Y. L. Boo. Fraud detection: A systematic literature review of graph-based anomaly detection approaches. Decision Support Systems, 133:113303, 2020.   \n[48] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.   \n[49] D. Ramotsoela, A. Abu-Mahfouz, and G. Hancke. A survey of anomaly detection in industrial wireless sensor networks with critical water system infrastructure as a case study. Sensors, 18(8):2491, 2018.   \n[50] M. Reyes, R. Meier, S. Pereira, C. A. Silva, F.-M. Dahlweid, H. v. Tengg-Kobligk, R. M. Summers, and R. Wiest. On the interpretability of artificial intelligence in radiology: challenges and opportunities. Radiology: artificial intelligence, 2(3):e190043, 2020.   \n[51] M. T. Ribeiro, S. Singh, and C. Guestrin. \" why should i trust you?\" explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pages 1135\u20131144, 2016.   \n[52] P. J. Rousseeuw and M. Hubert. Anomaly detection by robust statistics. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 8(2):e1236, 2018.   \n[53] K. K. Santhosh, D. P. Dogra, and P. P. Roy. Anomaly detection in road traffic using visual surveillance: A survey. ACM Computing Surveys (CSUR), 53(6):1\u201326, 2020.   \n[54] H.-K. Shin, W. Lee, J.-H. Yun, and H. Kim. {HAI} 1.0:{HIL-based} augmented $\\{{\\mathrm{ICS}}\\}$ security dataset. In 13Th USENIX workshop on cyber security experimentation and test (CSET 20), 2020.   \n[55] K. Simonyan. Deep inside convolutional networks: Visualising image classification models and saliency maps. arXiv preprint arXiv:1312.6034, 2013.   \n[56] D. Sulem, M. Donini, M. B. Zafar, F.-X. Aubet, J. Gasthaus, T. Januschowski, S. Das, K. Kenthapadi, and C. Archambeau. Diverse counterfactual explanations for anomaly detection in time series. arXiv preprint arXiv:2203.11103, 2022.   \n[57] I. Syarif, A. Prugel-Bennett, and G. Wills. Unsupervised clustering approach for network anomaly detection. In Networked Digital Technologies: 4th International Conference, NDT 2012, Dubai, UAE, April 24-26, 2012. Proceedings, Part I 4, pages 135\u2013145. Springer, 2012.   \n[58] J. J. Thiagarajan, K. Thopalli, D. Rajan, and P. Turaga. Training calibration-based counterfactual explainers for deep learning models in medical image analysis. Scientific reports, 12(1):597, 2022.   \n[59] H. Touvron, L. Martin, K. Stone, P.-E. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, A. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \n[60] S. Tuli, G. Casale, and N. R. Jennings. Tranad: Deep transformer networks for anomaly detection in multivariate time series data. arXiv preprint arXiv:2201.07284, 2022.   \n[61] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \n[62] L. Weng. What are diffusion models? lilianweng.github.io, Jul 2021.   \n[63] J. Xu, H. Wu, J. Wang, and M. Long. Anomaly transformer: Time series anomaly detection with association discrepancy. arXiv preprint arXiv:2110.02642, 2021.   \n[64] Z. Xu, Y. Liu, G. Deng, Y. Li, and S. Picek. A comprehensive study of jailbreak attack versus defense for large language models. In Findings of the Association for Computational Linguistics ACL 2024, pages 7432\u20137449, 2024.   \n[65] J. Yu, A. Ignatiev, P. J. Stuckey, N. Narodytska, and J. Marques-Silva. Eliminating the impossible, whatever remains must be true: On extracting and applying background knowledge in the context of formal explanations. In Proceedings of the AAAI Conference on Artificial Intelligence, 2023.   \n[66] J. Yu, Y. Zheng, X. Wang, W. Li, Y. Wu, R. Zhao, and L. Wu. Fastflow: Unsupervised anomaly detection and localization via 2d normalizing flows. arXiv preprint arXiv:2111.07677, 2021.   \n[67] X. Yuan and Y. Qiao. Diffusion-ts: Interpretable diffusion for general time series generation. arXiv preprint arXiv:2403.01742, 2024.   \n[68] S. Zagoruyko and N. Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016.   \n[69] G. Zerveas, S. Jayaraman, D. Patel, A. Bhamidipaty, and C. Eickhoff. A transformer-based framework for multivariate time series representation learning. In Proceedings of the 27th ACM SIGKDD conference on knowledge discovery & data mining, pages 2114\u20132124, 2021.   \n[70] A. Zhang, S. Song, J. Wang, and P. S. Yu. Time series data cleaning: From anomaly detection to anomaly repairing. Proceedings of the VLDB Endowment, 10(10):1046\u20131057, 2017.   \n[71] H. Zhang, Z. Wang, Z. Wu, and Y.-G. Jiang. Diffusionad: Denoising diffusion for anomaly detection. arXiv preprint arXiv:2303.08730, 2023.   \n[72] T. Zhou, P. Niu, L. Sun, R. Jin, et al. One fits all: Power general time series analysis by pretrained lm. Advances in neural information processing systems, 36, 2024.   \n[73] Y. Zou, J. Jeong, L. Pemula, D. Zhang, and O. Dabeer. Spot-the-difference self-supervised pre-training for anomaly detection and segmentation. In European Conference on Computer Vision, pages 392\u2013408. Springer, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Computational Resources ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "All experiments were done on a server with three NVIDIA GeForce RTX 4090 GPUs. ", "page_idx": 14}, {"type": "text", "text": "B Anomaly Detector Performance ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we report the performance of anomaly detectors. The results for FastFlow [66] are presented in Table 4, and the results for GPT-2 [48] are shown in Table 5. We did not perform extensive parameter tuning, as the performance of anomaly detectors is not the primary focus of our work. In addition, it is recommended that users adhere to usage guidelines of using GPT-2 or implementing safety filters. ", "page_idx": 14}, {"type": "text", "text": "C Additional Experiments ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We include additional model EfficientAD [11] and dataset MVTec [13]. The results are shown in Table 6. EfficientAD achieved an $88.24\\%$ average improvement across the four metrics on MVTec-AD and $74.85\\%$ improvement on VisA. ", "page_idx": 14}, {"type": "text", "text": "D More Qualitative Examples ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Additional VisA examples can be found in Figure 8, while additional MVTec-AD examples can be found in Figure 9 and Figure 10. Additional time-series examples can be found in Figure 11. ", "page_idx": 14}, {"type": "text", "text": "E Inference Time ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We randomly sampled 50 instances from the testing set to compare inference times, as shown in Table 7. We observed that although property guidance generates higher-quality repairs, it results in slightly longer inference times for time series data and significantly longer times for image data, possibly due to the higher dimensionality of the inputs. In total, it took about 40 hours to finish RQ1 for VisA and 25 hours to finish SWaT. For RQ2, it took 5 hours for SWaT. Improving computation time will be a focus of our future work. ", "page_idx": 14}, {"type": "text", "text": "F More Ablation Plots ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In addition to $\\lambda_{1}$ to $\\lambda_{4}$ , we also perform ablation on the overall guidance scale, which we denote using $\\lambda_{\\phi}$ . The The ablation study for $\\lambda_{\\phi}$ on time series can be found in Figure 12. The ablation for image data, using cashew class as an example can be found in Figure 13 and Figure 14. ", "page_idx": 14}, {"type": "table", "img_path": "m0jZUvlKl7/tmp/94b462e56148a3e37eca337b5ec9e0f098fdf2808f505a021693ce18fa7fb0ff.jpg", "table_caption": [], "table_footnote": ["Table 4: AUROC Scores of Fastflow for Various Categories in VisA "], "page_idx": 14}, {"type": "table", "img_path": "m0jZUvlKl7/tmp/f1de63cdaaa94e0b87c3bcaf834178bd3a6eadd3d61e63e118f237ae2ae9f4e6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "m0jZUvlKl7/tmp/d7f334f3e6f3dbff3c8b8b5b52d6af22ad1372d3ebb5130c3d969f5f1f9492ce.jpg", "table_caption": [], "table_footnote": ["Table 6: Comparison of baseline and guided performance across four metrics for MVTec dataset categories with EfficientAD and FastFlow. $\\Delta$ is the median improvement percentage of the guided result from baseline. "], "page_idx": 15}, {"type": "table", "img_path": "m0jZUvlKl7/tmp/584133bf8546ccf153447590d11228de17940506f3ac4f163d82d9b24f9519ea.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "m0jZUvlKl7/tmp/5cf5c56499661cab729e85a569954744119caf9b1fd1895d87b78db1893e41d5.jpg", "img_caption": ["Figure 8: More VisA examples. With AR-Pro, we have anomaly repairs resemble inputs better, compared with baseline. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "m0jZUvlKl7/tmp/e4aa0dcbb650315c9ce3d2ee685605fa841ae44f472ea3e200208a49317b80d7.jpg", "img_caption": ["Figure 9: More MVTec examples (Part 1). With AR-Pro, anomaly repairs resemble inputs better compared with the baseline. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "m0jZUvlKl7/tmp/0e06c7f50c67e09ca70b54354a2fbd972933c991263c84105b69db00eedc927b.jpg", "img_caption": ["Figure 10: More MVTec examples (Part 2). With AR-Pro, anomaly repairs resemble inputs better compared with the baseline. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "m0jZUvlKl7/tmp/3d5e8236c00bf5ce65b47196d6a6888ebf879464d6627fa1221a08ad2a0226e9.jpg", "img_caption": ["(c) Baseline fails to repair anomaly. ", "(d) Baseline fails to repair anomaly. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Figure 11: The first image shows an instance where the baseline repairs the anomaly, but not as effectively as with property guidance. The last three images show instances where the baseline fails to repair the anomaly. ", "page_idx": 18}, {"type": "image", "img_path": "m0jZUvlKl7/tmp/2db3dc35f1b154f0d163c7ad8e601c374169dd79a9911a9fcf7b4d4f03cd1e5c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 12: Effects of hyperparameter $\\lambda_{\\phi}$ on $M_{s}$ , $M_{d},M_{\\omega}$ and $M_{\\overline{{\\omega}}}$ on SWaT; the effect of $\\lambda_{\\phi}$ varies across metrics, but the range remain relatively small. ", "page_idx": 19}, {"type": "image", "img_path": "m0jZUvlKl7/tmp/564a8143cacc2dad9a37da10003cf881810b18fd9051af5745f18396c18a81ab.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 13: Effects of hyperparameter on $M_{s}$ , $M_{d}$ , $M_{\\omega}$ and $M_{\\overline{{\\omega}}}$ on VisA cashew class; the effect of $\\lambda_{\\phi}$ varies across metrics, but the range remain relatively small. ", "page_idx": 19}, {"type": "image", "img_path": "m0jZUvlKl7/tmp/eb952da4b1ff9e7b8acbc575a3167b69b097935a55fbd1daa27debbd22fd33c3.jpg", "img_caption": ["Figure 14: Effects of hyperparameter $\\lambda_{\\phi}$ on ${\\cal M}_{s},{\\cal M}_{d},{\\cal M}_{\\omega}$ and $M_{\\overline{{\\omega}}}$ on VisA cashew class; the effect of $\\lambda_{\\phi}$ varies across metrics, but the range remain relatively small. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope, as they clearly outline the development of a domainindependent framework for anomaly repair based on formal properties and the demonstration of its effectiveness through empirical results on VisA and SWaT datasets. This alignment ensures that the reader\u2019s expectations are met and that the paper delivers on its proposed objectives. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 20}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: We discuss the limitation in Section 6. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 20}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: Although we did not have proofs, we listed the assumptions in Section 2 and Section 3. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 21}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We attach the code for reproducing the result to the zip file. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 21}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Please find the appendix for the implementation details. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 22}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We include the experiment details in Section 4. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 22}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: We reported the mean and standard deviation for our main results in Table 1.   \nWe also report TNR with $95\\%$ statistical significance in Table 3. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We indicate GPU details in Appendix A and the amount of compute required for each of the individual experimental runs as well as estimate the total compute in Appendix E. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 23}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 23}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: There is no noticeable negative societal impact of the work performed. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 23}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 24}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: We call for users adhere to usage guidelines of GPT2 in Appendix B or implementing safety filters. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 24}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We cited the original paper that produced the code packages and datasets. We also state which version of the model is used. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We communicate the details of the code as part of our submissions in the zip file. This includes details about training, license, limitations, etc. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 25}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 25}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}]