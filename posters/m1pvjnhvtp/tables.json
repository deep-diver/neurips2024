[{"figure_path": "m1PVjNHvtP/tables/tables_2_1.jpg", "caption": "Table 1: Comparison with existing optimizer layers for imposing constraints on the outputs of neural networks", "description": "This table compares GLinSAT with other state-of-the-art methods for imposing constraints on neural network outputs.  It contrasts the constraint type handled, GPU parallel computing capabilities, whether matrix factorization is needed, if exact gradients are available, and whether explicit or implicit backpropagation is used.", "section": "1 Introduction"}, {"figure_path": "m1PVjNHvtP/tables/tables_7_1.jpg", "caption": "Table 2: Average allocated GPU memory and solution time of different satisfiability layers during batch processing of projection and backpropagation when \u03b8 is set to 0.1 in TSP training phase", "description": "This table compares the GPU memory usage and computation time of various satisfiability layers (CvxpyLayers, OptNet, LinSAT, and GLinSAT) during the training phase of solving Traveling Salesperson Problems (TSP).  It shows the performance difference for both projection and backpropagation, separating dense and sparse matrix implementations of LinSAT and GLinSAT.  The results are broken down by the type of backpropagation used (explicit or implicit) and indicate the impact of different approaches on computational efficiency and memory requirements.", "section": "3 Experimental Results"}, {"figure_path": "m1PVjNHvtP/tables/tables_7_2.jpg", "caption": "Table 2: Average allocated GPU memory and solution time of different satisfiability layers during batch processing of projection and backpropagation when  is set to 0.1 in TSP training phase", "description": "This table shows a comparison of the GPU memory and time used for projection and backpropagation by different satisfiability layers (CvxpyLayers, OptNet, LinSAT, and GLinSAT) when training on the Traveling Salesman Problem (TSP) with two types of constraints: TSP with start and end city constraints and TSP with priority constraints.  The results are shown separately for the projection and backpropagation phases of each layer, providing a comprehensive view of their computational efficiency.  The table also shows how the different implementations of GLinSAT (dense/sparse, explicit/implicit) perform. ", "section": "3 Experimental Results"}, {"figure_path": "m1PVjNHvtP/tables/tables_8_1.jpg", "caption": "Table 4: Mean F1 scores across different satisfiability layers in partial graph matching problem", "description": "This table presents the mean F1 scores achieved by different satisfiability layers (CvxpyLayers, OptNet, LinSAT, and GLinSAT) on a partial graph matching problem.  The results are broken down by the inverse temperature parameter (\u03b8) used in the entropy-regularized linear programming formulation.  The table allows for a comparison of the performance of different methods for imposing linear constraints in a neural network context.", "section": "3.2 Partial graph matching with outliers"}, {"figure_path": "m1PVjNHvtP/tables/tables_8_2.jpg", "caption": "Table 2: Average allocated GPU memory and solution time of different satisfiability layers during batch processing of projection and backpropagation when \u03b8 is set to 0.1 in TSP training phase", "description": "This table compares the GPU memory usage and the time spent on projection and backpropagation for different satisfiability layers (CvxpyLayers, OptNet, LinSAT, GLinSAT) when solving the Traveling Salesman Problem (TSP).  It shows the performance for two variations of the TSP: TSP with starting and ending cities constraints, and TSP with priority constraints.  The results are broken down by whether a dense or sparse matrix representation was used, and whether explicit or implicit backpropagation was used in GLinSAT. The table highlights the efficiency gains of GLinSAT, especially the implicit version, in terms of both memory usage and computation time.", "section": "3 Experimental Results"}, {"figure_path": "m1PVjNHvtP/tables/tables_9_1.jpg", "caption": "Table 6: Feasibility ratio and average gap obtained from using different 1/\u03b8 in validation", "description": "This table shows the feasibility ratio and average gap obtained from using different values of the inverse temperature parameter (1/\u03b8) in the validation stage of the power system unit commitment experiment.  It compares results obtained by using the proposed GLinSAT-Sparse-Implicit method with a sigmoid activation function, showing how the feasibility of solutions increases as 1/\u03b8 approaches 0.  The table also includes results using Gurobi-LP, which solves the linear programming problem with the integer unit commitment variables fixed, to compare performance with GLinSAT.", "section": "3.4 Power system unit commitment"}, {"figure_path": "m1PVjNHvtP/tables/tables_18_1.jpg", "caption": "Table 2: Average allocated GPU memory and solution time of different satisfiability layers during batch processing of projection and backpropagation when \u03b8 is set to 0.1 in TSP training phase", "description": "This table compares the GPU memory usage and computation time of various satisfiability layers (CvxpyLayers, OptNet, LinSAT, and GLinSAT) during the training phase of solving Traveling Salesperson Problems (TSPs) with two different types of constraints: TSP with start and end city constraints, and TSP with priority constraints.  The results show GLinSAT's efficiency compared to other methods in terms of memory and processing speed for both types of constraints, especially when using implicit backpropagation.", "section": "3 Experimental Results"}, {"figure_path": "m1PVjNHvtP/tables/tables_19_1.jpg", "caption": "Table 3: Mean tour length and feasibility ratio obtained from using different \u03b8 and post-processing methods in TSP validation stage", "description": "This table presents the results of experiments on the Traveling Salesman Problem (TSP) with two types of constraints (TSP-StartEnd and TSP-Priority).  The table compares the performance of different satisfiability layers (LinSAT and GLinSAT variants) in terms of mean tour length and feasibility ratio, after applying two different post-processing techniques: rounding and beam search.  The results are shown for different values of the regularization parameter (\u03b8).", "section": "3.1 Constrained traveling salesman problem"}, {"figure_path": "m1PVjNHvtP/tables/tables_20_1.jpg", "caption": "Table 2: Average allocated GPU memory and solution time of different satisfiability layers during batch processing of projection and backpropagation when \\(\\theta\\) is set to 0.1 in TSP training phase", "description": "This table shows the GPU memory usage and the time spent on projection and backpropagation for different satisfiability layers when solving the Traveling Salesman Problem (TSP).  It compares the performance of GLinSAT against other methods such as CvxpyLayers, OptNet, and LinSAT, considering both dense and sparse matrix implementations and explicit/implicit backpropagation. The results are broken down by the type of TSP constraint (TSP-StartEnd and TSP-Priority).", "section": "3 Experimental Results"}, {"figure_path": "m1PVjNHvtP/tables/tables_21_1.jpg", "caption": "Table 2: Average allocated GPU memory and solution time of different satisfiability layers during batch processing of projection and backpropagation when \u03b8 is set to 0.1 in TSP training phase", "description": "This table compares the GPU memory usage and computation time for projection and backpropagation steps of various satisfiability layers (CvxpyLayers, OptNet, LinSAT, and GLinSAT) on two versions of the Traveling Salesperson Problem (TSP).  It shows the performance differences across dense and sparse matrix implementations, and explicit vs. implicit backpropagation methods.  The results highlight the efficiency gains of GLinSAT, especially in the implicit backpropagation approach.", "section": "3 Experimental Results"}, {"figure_path": "m1PVjNHvtP/tables/tables_24_1.jpg", "caption": "Table 2: Average allocated GPU memory and solution time of different satisfiability layers during batch processing of projection and backpropagation when \u03b8 is set to 0.1 in TSP training phase", "description": "This table compares the GPU memory usage and computation time of various satisfiability layers (CvxpyLayers, OptNet, LinSAT, and GLinSAT) during the training phase of a Traveling Salesperson Problem (TSP).  It shows the memory used for projection and backpropagation separately, as well as the time taken for each.  The results are broken down by the specific satisfiability layer used and whether a dense or sparse matrix representation was used, and whether explicit or implicit backpropagation was utilized. The purpose is to demonstrate the efficiency of GLinSAT in comparison to other methods.", "section": "3 Experimental Results"}]