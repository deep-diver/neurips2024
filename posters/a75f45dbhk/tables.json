[{"figure_path": "a75F45dBHK/tables/tables_6_1.jpg", "caption": "Table 4.1: The performance (test accuracy) of in-context learning on the associative recall task with different sequence lengths and a vocabulary size of 20. The results for the baseline models are drawn from Poli et al. [2023], Fu et al. [2023]. The symbol x indicates that the Transformer model failed to complete the task within a week or the model does not fit in memory.", "description": "This table presents the test accuracy of different models on the associative recall task. The task evaluates in-context learning ability, where the model is given a set of key-value pairs and must generate a value from a given key.  Different sequence lengths (128, 512, 2K, 8K, 32K, 128K) and a vocabulary size of 20 are used to assess performance. The 'x' indicates that either the Transformer model could not finish the task in a week or it ran out of memory.", "section": "4.1 Synthetic In-context Learning"}, {"figure_path": "a75F45dBHK/tables/tables_7_1.jpg", "caption": "Table 4.3: Average GLUE Score of BERT-base and BERT-large [Devlin et al., 2018] in comparison to Orchid-BERT-base and Orchid-BERT-base, and M2-BERT-base and M2-BERT-large Dao et al. [2022]. Baseline results are drawn from [Fu et al., 2023].", "description": "This table presents a comparison of the GLUE scores achieved by different BERT models: the original BERT-base and BERT-large, the M2-BERT models (using block-diagonal matrices for efficiency), and the Orchid-BERT models (using the Orchid layer for sequence mixing).  It shows that Orchid-BERT models achieve comparable or better GLUE scores while using significantly fewer parameters, highlighting the efficiency and effectiveness of the Orchid layer.", "section": "4 Experiments"}, {"figure_path": "a75F45dBHK/tables/tables_8_1.jpg", "caption": "Table 4.4: Performance comparison of Orchid with ViT-based models on ImageNet-1k. Baseline results are drawn from [Fu et al., 2023].", "description": "This table presents the results of image classification experiments on the ImageNet-1k dataset.  It compares the performance of the Orchid model against several other ViT-based models, including ViT-b, ViT-b+Monarch, Hyena-ViT-b, and M2-ViT-b.  The table shows the Top-1 and Top-5 accuracy for each model, indicating the percentage of correctly classified images within the top 1 and top 5 predictions, respectively. The model sizes (in millions of parameters) are also provided.  The baseline results are referenced from the work of Fu et al. (2023).", "section": "4.3 Image Classification"}, {"figure_path": "a75F45dBHK/tables/tables_8_2.jpg", "caption": "Table 4.5: Performance comparison of Orchid with ViT-based models on CIFAR-10 dataset. Orchid's performance is also evaluated over different patch sizes, 4 \u00d7 4, 2 \u00d7 2 and 1 \u00d7 1 pixels. Orchid-s and Orchid-m refers to the ViT architecture composed of 6 layers with hidden sizes of 128 and 220, respectively. Cross-Correlation (Type II) conditioning network (equation 3) is identified with -cc and the rest are using type I (equation 2). Baseline results are drawn from [Fu et al., 2023] and [Knigge et al., 2023].", "description": "This table presents a comparison of the performance of the Orchid model against other ViT-based models on the CIFAR-10 image classification dataset.  It shows top-1 accuracy results for different models with varying architectures and sizes. The impact of different patch sizes (4x4, 2x2, 1x1 pixels) on Orchid's performance is also shown.  Furthermore, it highlights the difference in performance between using the two proposed conditioning networks (Type I and Type II).  Baseline results from comparative studies are included.", "section": "4.3 Image Classification"}, {"figure_path": "a75F45dBHK/tables/tables_15_1.jpg", "caption": "Table 4.3: Average GLUE Score of BERT-base and BERT-large [Devlin et al., 2018] in comparison to Orchid-BERT-base and Orchid-BERT-base, and M2-BERT-base and M2-BERT-large Dao et al. [2022]. Baseline results are drawn from [Fu et al., 2023].", "description": "This table presents a comparison of the average GLUE scores achieved by different BERT models.  It compares the standard BERT-base and BERT-large models with the Orchid-BERT models (using the Orchid architecture) and the M2-BERT models (another efficient transformer variant). The table shows the GLUE score, model size (in number of parameters), and the percentage change in GLUE score relative to the standard BERT models. This allows for a direct comparison of performance and efficiency between the different model architectures.", "section": "4.2 Language Modeling"}, {"figure_path": "a75F45dBHK/tables/tables_19_1.jpg", "caption": "Table 4.3: Average GLUE Score of BERT-base and BERT-large [Devlin et al., 2018] in comparison to Orchid-BERT-base and Orchid-BERT-base, and M2-BERT-base and M2-BERT-large Dao et al. [2022]. Baseline results are drawn from [Fu et al., 2023].", "description": "This table compares the GLUE scores achieved by different BERT models (base and large variants) along with their corresponding parameter counts.  It contrasts the performance of standard BERT models against M2-BERT (using Monarch Mixer) and Orchid-BERT (using the proposed Orchid architecture).  The difference in GLUE scores (\u0394 GLUE Score) between the baseline BERT models and the modified architectures (M2-BERT and Orchid-BERT) is also presented.  It showcases Orchid-BERT's ability to achieve comparable or better GLUE scores with fewer parameters.", "section": "4.2 Language Modeling"}, {"figure_path": "a75F45dBHK/tables/tables_19_2.jpg", "caption": "Table 4.2: The test accuracy of the associative recall task with varying vocabulary sizes and a sequence length of 128.", "description": "This table shows the performance of different models on the associative recall task, varying the vocabulary size (number of possible token values) while keeping the sequence length fixed at 128. The results highlight how model performance changes as the complexity of the task increases with the vocabulary size.  It allows comparison of different models and their ability to handle increased complexity.", "section": "4 Experiments"}, {"figure_path": "a75F45dBHK/tables/tables_20_1.jpg", "caption": "Table 4.4: Performance comparison of Orchid with ViT-based models on ImageNet-1k. Baseline results are drawn from [Fu et al., 2023].", "description": "This table compares the performance of Orchid against other Vision Transformer (ViT) based models on the ImageNet-1k dataset.  It shows the top-1 and top-5 accuracy for several models, including a baseline ViT, ViT with Monarch, Hyena-ViT, M2-ViT, and Orchid.  The table highlights Orchid's improved performance compared to the baselines.", "section": "4.3 Image Classification"}, {"figure_path": "a75F45dBHK/tables/tables_20_2.jpg", "caption": "Table C.4: Accuracy on the Speech Commands dataset with 10 classes. S4-M2 refers to the S4 model [24] where dense matrices in the MLP layers are replaced with block-diagonal matrices. Baseline results are sourced from [44], and X indicates that the Transformer model could not fit in GPU memory.", "description": "This table presents the accuracy results of different models on the Speech Commands dataset, a speech classification task.  The models compared include a standard Transformer, Performer, CKConv, WaveGan-D, S4, S4 with block-diagonal matrices (S4-M2), and the proposed Orchid model. The table highlights Orchid's competitive performance, achieving accuracy comparable to state-of-the-art models while potentially offering better scalability.  The 'X' indicates that the Transformer model couldn't fit within the available GPU memory.", "section": "C.5 Speech Classification"}]