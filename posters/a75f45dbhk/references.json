{"references": [{"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017", "reason": "This paper introduced the Transformer architecture, a foundational model for many modern sequence modeling approaches, including the Orchid model described in this paper."}, {"fullname_first_author": "Alexey Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2020", "reason": "This work extended the Transformer architecture to image processing, demonstrating its versatility and providing a baseline for comparison in image classification experiments within the Orchid paper."}, {"fullname_first_author": "Yi Tay", "paper_title": "Efficient transformers: A survey", "publication_date": "2022", "reason": "This survey paper provides a comprehensive overview of efficient Transformer models, contextualizing the Orchid model's contributions within the broader landscape of efficient sequence modeling research."}, {"fullname_first_author": "Iz Beltagy", "paper_title": "Longformer: The long-document transformer", "publication_date": "2020", "reason": "This paper introduced the Longformer architecture, addressing the quadratic complexity of attention mechanisms in long sequences, offering a direct comparison point for Orchid's efficiency in handling long sequences."}, {"fullname_first_author": "Nikita Kitaev", "paper_title": "Reformer: The efficient transformer", "publication_date": "2020", "reason": "This paper presented the Reformer model, another approach to address the limitations of traditional Transformers for long sequences, which provided another point of comparison for the Orchid architecture's efficiency."}]}