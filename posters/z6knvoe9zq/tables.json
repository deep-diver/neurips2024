[{"figure_path": "z6KNvOe9zQ/tables/tables_7_1.jpg", "caption": "Table 1: Frozen transfer evaluations of vision models pre-trained on the MMC4 dataset. Vision models are pre-trained from scratch for all methods. \u201cIN-1k\u201d denotes image classification on ImageNet [33]. \u201cret.\u201d and \u201ccap.\u201d denote image-text retrieval and image captioning, respectively. *The method names refer to implementing those methods with our experiment setting but not their trained checkpoints. For pre-training tasks, Con. image-text contrastive; Cap. image captioning; Mat. image-text matching; Mask. mask data modeling; Gen. auto-regressive text generation; Reg. image (feature) regression. The names in parentheses refer to the pre-training task but not their trained checkpoints. \u2020Note that CoCa and BLIP2 need to pass each sample through the language model 2 and 3 times, respectively, to perform multi-task learning.", "description": "This table presents the results of a frozen transfer evaluation on the MMC4 dataset for various vision models pre-trained from scratch using different methods.  It compares the performance across multiple tasks, including ImageNet-1k classification, COCO retrieval and captioning, and Flickr30k retrieval. The table highlights the performance differences among various pre-training methods, showing the effectiveness of Latent Compression Learning (LCL) on this dataset.", "section": "4.2 Comparison with Vision Pre-training Methods"}, {"figure_path": "z6KNvOe9zQ/tables/tables_7_2.jpg", "caption": "Table 1: Frozen transfer evaluations of vision models pre-trained on the MMC4 dataset. Vision models are pre-trained from scratch for all methods. \u201cIN-1k\u201d denotes image classification on ImageNet [33]. \u201cret.\u201d and \u201ccap.\u201d denote image-text retrieval and image captioning, respectively. * The method names refer to implementing those methods with our experiment setting but not their trained checkpoints. For pre-trainig tasks, Con. image-text contrastive; Cap. image captioning; Mat. image-text matching; Mask. mask data modeling; Gen. auto-regressive text generatiton; Reg. image (feature) regression. The names in parentheses refer to the pre-training task but not their trained checkpoints. \u2020 Note that CoCa and BLIP2 need to pass each sample through the language model 2 and 3 times, respectively, to perform multi-task learning.", "description": "This table presents the results of a frozen transfer evaluation on the MMC4 dataset.  Multiple vision models were trained from scratch using different pre-training methods. The evaluation metrics include ImageNet-1k classification accuracy, COCO retrieval performance (Top-1 and  IR@1), and captioning performance (B@4 and CIDEr) on COCO and NoCaps datasets. The table compares the performance of LCL to other methods like CLIP, CoCa, BLIP2, BEIT3, Flamingo, and Emu, highlighting LCL's effectiveness in leveraging interleaved data for pre-training.", "section": "4.2 Comparison with Vision Pre-training Methods"}, {"figure_path": "z6KNvOe9zQ/tables/tables_8_1.jpg", "caption": "Table 1: Frozen transfer evaluations of vision models pre-trained on the MMC4 dataset. Vision models are pre-trained from scratch for all methods. \u201cIN-1k\u201d denotes image classification on ImageNet [33]. \u201cret.\u201d and \u201ccap.\u201d denote image-text retrieval and image captioning, respectively. * The method names refer to implementing those methods with our experiment setting but not their trained checkpoints. For pre-training tasks, Con. image-text contrastive; Cap. image captioning; Mat. image-text matching; Mask. mask data modeling; Gen. auto-regressive text generation; Reg. image (feature) regression. The names in parentheses refer to the pre-training task but not their trained checkpoints. \u2020 Note that CoCa and BLIP2 need to pass each sample through the language model 2 and 3 times, respectively, to perform multi-task learning.", "description": "This table presents the results of a frozen transfer evaluation on the MMC4 dataset for various vision models pre-trained from scratch using different methods.  It compares the performance of these models on image classification (ImageNet-1k), image-text retrieval (COCO ret., Flickr30k ret.), and image captioning (COCO cap., NoCaps cap.).  The table highlights the impact of different pre-training tasks (e.g., contrastive learning, captioning, matching, masked modeling, auto-regressive generation, regression) and the use of paired versus interleaved data on model performance.  It emphasizes that the reported results stem from replicating these methods under the authors' experiment settings, not using pre-trained checkpoints of those methods.", "section": "4.2 Comparison with Vision Pre-training Methods"}, {"figure_path": "z6KNvOe9zQ/tables/tables_8_2.jpg", "caption": "Table 4: Transfer evaluation results of pre-trained ViT/L-14 on multi-modal benchmarks. The transfer adopts the downstream model and training pipeline of LLaVA-1.5 [46].", "description": "This table presents the results of transfer learning experiments using a pre-trained ViT-L-14 model on various multi-modal benchmark datasets.  The experiments compare the performance of different pre-training methods, including OpenAI CLIP, OpenCLIP, and the proposed LCL method. The results are shown for both \"frozen transfer\" (only the downstream task model is trained) and \"full transfer\" (both the pre-trained vision model and the downstream task model are trained) settings.  The benchmarks include VQAv2, GQA, VisWiz, SQA, POPE, MME, MMB, and SEEDI, evaluating the model's performance across various multi-modal tasks.", "section": "4.2 Comparison with Vision Pre-training Methods"}, {"figure_path": "z6KNvOe9zQ/tables/tables_9_1.jpg", "caption": "Table 5: Frozen transfer evaluations of LCL pre-training on different datasets. We ensured that all entries had seen the same number of images during pre-training to ensure fairness.", "description": "This table presents the results of the frozen transfer evaluation of the Latent Compression Learning (LCL) method on various datasets.  The \"frozen transfer\" setting means only the parameters of the added classifier were trained, not the pre-trained vision model's parameters. The table compares the performance of LCL pre-trained on LAION, MMC4, Obelics and a combined LAION+MMC4 dataset. The performance metrics include ImageNet-1k classification accuracy (acc-1), COCO retrieval Recall@1 and Intersection over Union (IOU)@1 (COCO ret.), Flickr30k retrieval Recall@1 and IOU@1 (Flickr30k ret.), COCO captioning CIDEr score (COCO cap.), and NoCaps captioning CIDEr score (NoCaps cap).  This allows for a comparison of LCL's performance across different datasets and to evaluate the benefit of using combined training data.", "section": "4.4 Ablation Study"}, {"figure_path": "z6KNvOe9zQ/tables/tables_9_2.jpg", "caption": "Table 6: Ablations of the training loss and loss balancing weight in LCL. Models are evaluated under frozen transfer setting.", "description": "This table presents the ablation study results for the Latent Compression Learning (LCL) method. It explores the impact of different training loss components (contrastive loss only, generative loss only, and the combined LCL loss) and varying loss balancing weights (\u03bb) on the model's performance.  The performance is evaluated using the frozen transfer setting on the COCO retrieval and captioning tasks, measured by the TR@1, IR@1, B@4, and CIDEr metrics. The table allows for analysis of how different components of the training objective and the balance between them affect the final model performance.", "section": "4.4 Ablation Study"}, {"figure_path": "z6KNvOe9zQ/tables/tables_9_3.jpg", "caption": "Table 6: Ablations of the training loss and loss balancing weight in LCL. Models are evaluated under frozen transfer setting.", "description": "This table shows the ablation study results on the training loss and the loss balancing weight in Latent Compression Learning (LCL).  The study varies the training loss by using only contrastive loss, only generation loss, and the combination of both (LCL).  It also varies the balancing weight (lambda) between the contrastive loss and generation loss to find the optimal balance. The results are evaluated under the frozen transfer setting.  The table demonstrates the effectiveness of the combined loss function and the optimal weight for the best performance.", "section": "4.4 Ablation Study"}, {"figure_path": "z6KNvOe9zQ/tables/tables_15_1.jpg", "caption": "Table 1: Frozen transfer evaluations of vision models pre-trained on the MMC4 dataset. Vision models are pre-trained from scratch for all methods. \u201cIN-1k\u201d denotes image classification on ImageNet [33]. \u201cret.\u201d and \u201ccap.\u201d denote image-text retrieval and image captioning, respectively. * The method names refer to implementing those methods with our experiment setting but not their trained checkpoints. For pre-training tasks, Con. image-text contrastive; Cap. image captioning; Mat. image-text matching; Mask. mask data modeling; Gen. auto-regressive text generation; Reg. image (feature) regression. The names in parentheses refer to the pre-training task but not their trained checkpoints. \u2020 Note that CoCa and BLIP2 need to pass each sample through the language model 2 and 3 times, respectively, to perform multi-task learning.", "description": "This table presents the results of a frozen transfer evaluation on the MMC4 dataset for various vision models pre-trained from scratch.  It compares different pre-training methods (contrastive learning, captioning, matching, masked modeling, text generation, and regression) across multiple tasks: ImageNet-1k classification, COCO retrieval, Flickr30k retrieval, COCO captioning, and NoCaps captioning.  The table highlights the performance differences between models trained on paired versus interleaved data and using different pre-training objectives.", "section": "4.2 Comparison with Vision Pre-training Methods"}, {"figure_path": "z6KNvOe9zQ/tables/tables_17_1.jpg", "caption": "Table 8: Hyper-parameters in transfer evaluation.", "description": "This table lists the hyperparameters used for different downstream tasks during the transfer learning phase.  It specifies the optimizer, learning rate, weight decay, optimizer momentum, learning rate schedule, warmup steps, total training steps, and batch size for image classification, image-text retrieval, image captioning, and multi-modal dialogue tasks, including separate hyperparameters for the two stages of multi-modal dialogue.", "section": "A.2 Evaluation"}, {"figure_path": "z6KNvOe9zQ/tables/tables_17_2.jpg", "caption": "Table 1: Frozen transfer evaluations of vision models pre-trained on the MMC4 dataset. Vision models are pre-trained from scratch for all methods. \u201cIN-1k\u201d denotes image classification on ImageNet [33]. \u201cret.\u201d and \u201ccap.\u201d denote image-text retrieval and image captioning, respectively. * The method names refer to implementing those methods with our experiment setting but not their trained checkpoints. For pre-training tasks, Con. image-text contrastive; Cap. image captioning; Mat. image-text matching; Mask. mask data modeling; Gen. auto-regressive text generation; Reg. image (feature) regression. The names in parentheses refer to the pre-training task but not their trained checkpoints. \u2020 Note that CoCa and BLIP2 need to pass each sample through the language model 2 and 3 times, respectively, to perform multi-task learning.", "description": "This table presents the results of a frozen transfer evaluation on the MMC4 dataset for various vision models pre-trained from scratch using different methods.  The table compares performance across multiple tasks (image classification, image-text retrieval, image captioning) and different pre-training approaches (contrastive learning, masked autoencoding, autoregressive text generation).  It highlights the performance differences when using paired versus interleaved image-text data and notes certain implementation details for specific methods.", "section": "4.2 Comparison with Vision Pre-training Methods"}, {"figure_path": "z6KNvOe9zQ/tables/tables_19_1.jpg", "caption": "Table 1: Frozen transfer evaluations of vision models pre-trained on the MMC4 dataset. Vision models are pre-trained from scratch for all methods. \u201cIN-1k\u201d denotes image classification on ImageNet [33]. \u201cret.\u201d and \u201ccap.\u201d denote image-text retrieval and image captioning, respectively. *The method names refer to implementing those methods with our experiment setting but not their trained checkpoints. For pre-training tasks, Con. image-text contrastive; Cap. image captioning; Mat. image-text matching; Mask. mask data modeling; Gen. auto-regressive text generation; Reg. image (feature) regression. The names in parentheses refer to the pre-training task but not their trained checkpoints. \u2020Note that CoCa and BLIP2 need to pass each sample through the language model 2 and 3 times, respectively, to perform multi-task learning.", "description": "This table presents the results of a frozen transfer evaluation on the MMC4 dataset for various vision models pre-trained from scratch using different methods.  It compares the performance of these models on image classification (ImageNet-1k), image-text retrieval (COCO ret., Flickr30k ret.), and image captioning (COCO cap., NoCaps cap.). The table highlights the performance differences based on different pre-training tasks (e.g., contrastive learning, autoregressive text generation) and whether the full models or just the vision encoders were fine-tuned during the transfer task.  It also notes specific details about CoCa and BLIP2's multiple passes through the language model.", "section": "4.2 Comparison with Vision Pre-training Methods"}, {"figure_path": "z6KNvOe9zQ/tables/tables_20_1.jpg", "caption": "Table 1: Frozen transfer evaluations of vision models pre-trained on the MMC4 dataset. Vision models are pre-trained from scratch for all methods. \u201cIN-1k\u201d denotes image classification on ImageNet [33]. \u201cret.\u201d and \u201ccap.\u201d denote image-text retrieval and image captioning, respectively. * The method names refer to implementing those methods with our experiment setting but not their trained checkpoints. For pre-trainig tasks, Con. image-text contrastive; Cap. image captioning; Mat. image-text matching; Mask. mask data modeling; Gen. auto-regressive text generatiton; Reg. image (feature) regression. The names in parentheses refer to the pre-training task but not their trained checkpoints. \u2020 Note that CoCa and BLIP2 need to pass each sample through the language model 2 and 3 times, respectively, to perform multi-task learning.", "description": "This table presents the results of a frozen transfer evaluation of various vision models pretrained on the MMC4 dataset.  The evaluation focuses on several downstream tasks: ImageNet-1k classification, COCO retrieval, Flickr30k retrieval, COCO captioning, and NoCaps captioning.  The table compares the performance of different pre-training methods, highlighting the performance of the proposed LCL method against existing state-of-the-art approaches. Note that the table specifies whether the methods were pretrained from scratch or used pretrained checkpoints, and details the pre-training tasks involved (e.g., contrastive learning, captioning).", "section": "4.2 Comparison with Vision Pre-training Methods"}]