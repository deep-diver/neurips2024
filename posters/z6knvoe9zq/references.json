{"references": [{"fullname_first_author": "A. Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-12", "reason": "This paper introduces CLIP, a highly influential model that leverages large-scale image-text data for vision model pre-training, directly inspiring the current work."}, {"fullname_first_author": "J.-B. Alayrac", "paper_title": "Flamingo: a visual language model for few-shot learning", "publication_date": "2022-12-31", "reason": "Flamingo is a significant advancement in visual language modeling that handles interleaved image-text data, a key aspect addressed in this paper."}, {"fullname_first_author": "K. He", "paper_title": "Masked autoencoders are scalable vision learners", "publication_date": "2022-06-10", "reason": "This work introduces Masked Autoencoders (MAE), a highly effective self-supervised learning method for vision models, which the current paper builds upon."}, {"fullname_first_author": "G. Del\u00e9tang", "paper_title": "Language modeling is compression", "publication_date": "2023-09-06", "reason": "This paper establishes a theoretical connection between language modeling and compression learning, which is a core concept used in this paper."}, {"fullname_first_author": "H. Bao", "paper_title": "BEiT: BERT pre-training of image transformers", "publication_date": "2021-06-22", "reason": "BEiT introduces a novel pre-training method for image transformers using masked image modeling, a technique relevant to the current paper's approach."}]}