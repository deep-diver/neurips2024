[{"figure_path": "z6KNvOe9zQ/figures/figures_1_1.jpg", "caption": "Figure 1: Comparison of different training frameworks. (a) Contrastive learning framework from CLIP [55] pre-trains vision encoders from scratch with image-text pairs, but it does not support interleaved data. (b) Our proposed LCL pre-training framework can effectively pre-train vision encoders from scratch with interleaved image-text data. In these two frameworks, the text encoder or the language model that provides supervision can be optionally discarded during the transfer stage. (c) Multi-modal incremental training process uses interleaved image-text data to align the pre-trained vision encoder and the language model, but it cannot pre-train vision encoders from scratch.", "description": "This figure compares three different vision model pre-training frameworks: CLIP, the proposed Latent Compression Learning (LCL), and multi-modal incremental training.  It highlights the key differences in how each method handles image-text data (paired vs. interleaved) and whether it can train vision encoders from scratch or requires pre-trained models.  CLIP uses paired data and trains from scratch, LCL also trains from scratch but uses interleaved data, while multi-modal incremental training only aligns pre-trained models and does not train from scratch. The figure illustrates that only LCL can effectively leverage interleaved data to train vision encoders from scratch.", "section": "1 Introduction"}, {"figure_path": "z6KNvOe9zQ/figures/figures_3_1.jpg", "caption": "Figure 2: Overview of our proposed Latent Compression Learning for vision model pre-training. Image latent representation is extracted via a vision encoder and subsequently input into a language model alongside textual embedding. Two complementary losses are utilized to learn robust visual representation from scratch on interleaved image-text data: a contrastive loss ensures consistency between the visual latent representation and its preceding context, while an auto-regressive loss enhances the predictability of visual representation for subsequent text.", "description": "This figure illustrates the Latent Compression Learning (LCL) framework.  Interleaved image and text data are processed. The image undergoes a vision encoder to produce latent visual representations which are then fed, along with the text embeddings, into a causal language model.  Training involves two losses: a contrastive loss comparing visual representations to preceding contexts, and an autoregressive loss predicting subsequent text based on the visual representations. This dual-loss approach aims to learn robust visual representations from interleaved image-text data, effectively compressing the high-level semantic information into model parameters.", "section": "3 Method"}, {"figure_path": "z6KNvOe9zQ/figures/figures_16_1.jpg", "caption": "Figure 3: Illustration of \u201cfrozen transfer\u201d evaluation. The vision encoder is frozen during transfer tuning. (a) Image classification: an attention probe and a linear classifier are built upon the vision encoder. (b) Image-text retrieval: an attention probe is used to extract global visual feature, which is trained to align with the text feature from the text encoder. (c) Text generation: an MLP is utilized to align the visual feature with the text embedding space, and the multi-modal embedding is fed into the language model for auto-regressive text generation.", "description": "This figure illustrates the three downstream tasks used for evaluating the pre-trained vision models.  The vision encoder is frozen (weights are not updated) during the transfer learning phase.  \n(a) Image Classification: An attention pooling layer and a linear classifier are added on top of the vision encoder.  Only the weights of the added layers are trained.\n(b) Image-text Retrieval: A global image representation is extracted using an attention pooling layer and a linear layer; a text encoder processes the text. A similarity comparison is then performed between the image and text representations.\n(c) Text Generation: An MLP is used to align the vision encoder's output with the text embedding space, enabling multi-modal inputs to the language model for text generation.", "section": "A.2 Evaluation"}]