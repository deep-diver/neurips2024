[{"figure_path": "Kw6MRGFx0R/tables/tables_5_1.jpg", "caption": "Table 1: Impact of the main components measured for a Phi-2 (2.7B) and LLaMA-2 (7B) model in terms of perplexity on WikiText2. N/A - indicates failure (> 105 perplexity).", "description": "This table shows the impact of the proposed method's components on the perplexity scores of Phi-2 (2.7B) and LLaMA-2 (7B) models.  It compares the perplexity when neither data-free holistic distillation nor layer-wise input-agnostic weight binarization is used, then with just layer-wise input-agnostic weight binarization, and finally with both techniques.  The results demonstrate the importance of both proposed methods for achieving good model performance, especially for smaller models.", "section": "4 Ablation studies and analysis"}, {"figure_path": "Kw6MRGFx0R/tables/tables_6_1.jpg", "caption": "Table 2: Effect of different Knowledge Distillation strategies measured using a Phi-2 (2.7B) model in terms of perplexity on WikiText2.", "description": "This table presents the perplexity scores on the WikiText2 benchmark for a Phi-2 (2.7B) model using different knowledge distillation (KD) strategies.  The strategies compared are: CE (Cross-Entropy loss using hard labels), KD (standard KD with soft labels), KD-MSE (MSE loss between student and teacher logits), and KD-MSE with swap (KD-MSE with teacher blocks gradually swapped with student blocks). The results show the impact of these strategies on the model's performance, highlighting the effectiveness of the KD-MSE with swap approach.", "section": "4.3 Effect of distillation"}, {"figure_path": "Kw6MRGFx0R/tables/tables_7_1.jpg", "caption": "Table 3: Effect of progressive weight quantization strategies measured using a LLama-2 (7B) model in terms of perplexity on WikiText2.", "description": "This table demonstrates the impact of using different quantization methods as a starting point for the proposed binary quantization approach. It compares the perplexity scores achieved on the WikiText2 benchmark when starting from full precision (FP16), GPTQ quantized weights, and OmniQuant quantized weights. The results highlight that using higher-quality quantized weights as input to the proposed method leads to better final performance.", "section": "4.5 Effect of progressive weights quantization"}, {"figure_path": "Kw6MRGFx0R/tables/tables_9_1.jpg", "caption": "Table 4: Weights only quantization results of LLaMA and LLaMA-2 models in terms of perplexity on WikiText2.", "description": "This table presents the results of different weight-only quantization methods on LLaMA and LLaMA-2 models, measured by their perplexity scores on the WikiText2 benchmark.  The methods compared include RTN (Round to Nearest), GPTQ, AWQ, OmniQuant, and QuIP#, each with different quantization bit-widths (W4A16, W4A16g128, W3A16, W3A16g128) and grouping strategies. The table shows the perplexity scores achieved by each method across various model sizes, providing a quantitative comparison of their performance.", "section": "Results"}]