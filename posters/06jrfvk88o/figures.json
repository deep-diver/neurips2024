[{"figure_path": "06JRFVK88O/figures/figures_5_1.jpg", "caption": "Figure 1: An overview of our IMAX-PPO algorithm. Each local observation of of an ally agent i includes information about itself, as well as enemy and ally agents in its neighborhood (which changes over time). The output of the IL component is the predicted next states of neighboring enemy agents (predictions for the non-neighbor enemies will be masked out).", "description": "This figure illustrates the IMAX-PPO algorithm's architecture. Each allied agent receives local observations, including its own state and the states of nearby allies and enemies.  The Imitation Learning (IL) component predicts the next states of neighboring enemies using this local information, augmenting the input to the MAPPO policy. Non-neighboring enemy information is masked out.", "section": "5 IMAX-PPO: Imitation-enhanced Multi-Agent EXtended PPO Algorithm"}, {"figure_path": "06JRFVK88O/figures/figures_8_1.jpg", "caption": "Figure 2: Win-rate curves on SMACv2 environment.", "description": "This figure presents the win-rate curves for seven different multi-agent reinforcement learning algorithms across fifteen sub-tasks in the SMACv2 environment.  The x-axis represents the number of training steps, and the y-axis represents the win rate, averaged over 32 rounds of evaluation. Each sub-task corresponds to a specific scenario with varying numbers of allies and enemies. The figure highlights the relative performance of the algorithms in terms of convergence speed and final win rate.", "section": "6 Experiments"}, {"figure_path": "06JRFVK88O/figures/figures_19_1.jpg", "caption": "Figure 1: An overview of our IMAX-PPO algorithm. Each local observation of of an ally agent i includes information about itself, as well as enemy and ally agents in its neighborhood (which changes over time). The output of the IL component is the predicted next states of neighboring enemy agents (predictions for the non-neighbor enemies will be masked out).", "description": "This figure shows a schematic of the IMAX-PPO algorithm.  Each allied agent receives local observations (its own state and those of nearby allies and enemies).  An imitation learning (IL) component predicts the next states of nearby enemy agents. This prediction, along with the agent's local observations, is fed into the main multi-agent actor-critic algorithm (MAPPO) to improve the policy learning of allied agents.  The algorithm is decentralized, meaning agents act independently, but the critic utilizes global information.", "section": "5 IMAX-PPO: Imitation-enhanced Multi-Agent EXtended PPO Algorithm"}, {"figure_path": "06JRFVK88O/figures/figures_20_1.jpg", "caption": "Figure 4: Win-rate curves on GRF environment.", "description": "This figure shows the win rates of different MARL algorithms (MAPPO, IPPO, QMIX, QPLEX, SupMAPPO, IMAX-PPO (GAIL), and IMAX-PPO (InQ)) across three different GRF scenarios (3_vs_1, counter_easy, and counter_hard) plotted against training steps.  The horizontal dashed lines represent the average win rates obtained by each algorithm after training.  The results demonstrate that IMAX-PPO (InQ) consistently achieves the highest win rate across all scenarios, showcasing its superior performance compared to other state-of-the-art algorithms.", "section": "6 Experiments"}, {"figure_path": "06JRFVK88O/figures/figures_20_2.jpg", "caption": "Figure 5: Win-rate curves on Gold Miner environment.", "description": "The figure shows the win rates of different multi-agent reinforcement learning algorithms in the Gold Miner environment over the course of training.  The x-axis represents the number of training steps, and the y-axis represents the win rate.  The algorithms compared include MAPPO, IPPO, QMIX, QPLEX, SupMAPPO, IMAX-PPO (GAIL), and IMAX-PPO (InQ). The Gold Miner environment has three difficulty levels: easy, medium, and hard, each represented by a separate plot.  IMAX-PPO (InQ) consistently outperforms other methods across all difficulty levels.", "section": "6 Experiments"}, {"figure_path": "06JRFVK88O/figures/figures_21_1.jpg", "caption": "Figure 6: Learning curves with different methods on SMACv2.", "description": "This figure compares the learning curves of different multi-agent reinforcement learning algorithms on SMACv2, a challenging StarCraft II benchmark. The x-axis represents the number of training steps, while the y-axis shows the win rate.  The algorithms compared include QMIX, QMIX-IMAX (the authors' proposed algorithm integrating imitation learning), and MAPPO-IMAX (another variant of the authors' algorithm using MAPPO as the baseline).  The figure is divided into subplots, each showing results for a different combination of faction (Protoss, Terran, Zerg) and team size (5 vs. 5, 10 vs. 10). The shaded areas represent the standard deviation across multiple runs, showing variability in performance.", "section": "D Ablation Study"}]