[{"figure_path": "QWsLks8LCO/figures/figures_1_1.jpg", "caption": "Figure 1: Complex decision problems that require a good understanding of the environment's dynamics and the objective are still challenging for current vision-language models, e.g., the answer elicited by GPT-4 is sketchy and misleading. Instead, Learning before Interaction (LBI) enables grounded reasoning by simulating the task in the given question. LBI utilizes the simulator to train a MARL policy and generate the answer by running the converged policy on the simulator.", "description": "This figure illustrates the limitations of current vision-language models like GPT-4 in solving complex multi-agent decision-making problems.  GPT-4's response to a sample question is shown to be vague and inaccurate.  The figure then introduces the Learning Before Interaction (LBI) framework as a solution. LBI uses a simulator to train a multi-agent reinforcement learning (MARL) policy which is then used to generate a more accurate and grounded answer by simulating the interaction in the given environment.", "section": "1 Introduction"}, {"figure_path": "QWsLks8LCO/figures/figures_3_1.jpg", "caption": "Figure 2: Datasets construction and VQ-VAE training.", "description": "This figure illustrates the process of creating the VisionSMAC dataset and training the Vector Quantized Variational Autoencoder (VQ-VAE).  State-based trajectories from the SMAC benchmark are first parsed to generate images and task descriptions using a parser. These images and descriptions, along with the original state-based trajectories, then serve as the input for VQ-VAE training, which learns to encode images into discrete tokens.", "section": "3.1 VisionSMAC"}, {"figure_path": "QWsLks8LCO/figures/figures_4_1.jpg", "caption": "Figure 3: The overview of Learning before Interaction.", "description": "This figure shows a detailed overview of the Learning Before Interaction (LBI) framework. It begins with reward-free data collection, where the dynamics model is trained using language and image tokenizers to predict the next state and action.  A language-guided reward labeling step then uses a reward model to infer rewards based on expert trajectories. These reward-free and reward-labeled trajectories are used to train the policy model via behavior-regularized reinforcement learning. The process involves an inner-loop policy used to generate data for reward model training and an interaction with an external environment to update the policy.", "section": "3 Methodology"}, {"figure_path": "QWsLks8LCO/figures/figures_9_1.jpg", "caption": "Figure 4: Visualization of the prediction from dynamics and reward model, where \"np-op\" and \"s\" denote no-operation and stopping, respectively.", "description": "This figure visualizes the predictions of the dynamics and reward models. The top three rows show image sequences generated by the dynamics model for three different maps (MMM2, 3s_vs_5z, and 5m_vs_6m). The bottom row focuses on a specific scenario from the 5m_vs_6m map, highlighting the learned reward function for Agent 1 at a critical juncture where the agent has low health.  The bar charts represent learned vs. game rewards for Agent 1's actions (no-operation, stopping, moving in cardinal directions, and selecting an enemy to attack). The figure showcases the model's ability to generate long-horizon, consistent trajectories and provide explainable reward functions for various situations.", "section": "5.4 Visualization"}, {"figure_path": "QWsLks8LCO/figures/figures_18_1.jpg", "caption": "Figure 2: Datasets construction and VQ-VAE training.", "description": "This figure illustrates the process of creating the VisionSMAC dataset and training the VQ-VAE.  The VisionSMAC dataset is created by converting state-based trajectories from the StarCraft Multi-Agent Challenge (SMAC) benchmark into images and language descriptions using a parser.  The VQ-VAE is then trained on these images to generate discrete representations of each frame. These discrete representations are later used in the interactive simulator.  The image shows the pipeline including the state-based trajectory data, a parser converting these into images and task descriptions, and the VQ-VAE training.", "section": "3.1 VisionSMAC"}, {"figure_path": "QWsLks8LCO/figures/figures_18_2.jpg", "caption": "Figure 3: The overview of Learning before Interaction.", "description": "This figure illustrates the Learning Before Interaction (LBI) framework.  It shows the process of reward-free data collection using an off-policy MARL algorithm to collect trajectories, followed by language-guided reward labeling using a reward model. The policy model is then updated using a behavior-regularized RL approach, improving the policy in the simulator environment.  The final output is an image sequence generated by the interaction of the dynamics model and the converged policy.", "section": "3 Methodology"}, {"figure_path": "QWsLks8LCO/figures/figures_21_1.jpg", "caption": "Figure 3: The overview of Learning before Interaction.", "description": "This figure shows the overall architecture of the Learning Before Interaction (LBI) framework.  It details the three main stages: reward-free data collection, language-guided reward labeling, and policy model update. The reward-free data collection stage uses a randomly initialized off-policy MARL algorithm to collect trajectories from the dynamics model. These trajectories are then used in the language-guided reward labeling stage, where the reward model assigns rewards to state-action pairs based on the trajectories and task descriptions.  Finally, the policy model is updated using a behavior-regularized RL algorithm. This figure provides a high-level overview of the entire LBI process.", "section": "3 Methodology"}, {"figure_path": "QWsLks8LCO/figures/figures_22_1.jpg", "caption": "Figure 3: The overview of Learning before Interaction.", "description": "This figure shows the overview of the proposed Learning before Interaction (LBI) framework.  It illustrates the three main stages: reward-free data collection, language-guided reward labeling, and policy updates.  The reward-free data collection uses a randomly initialized off-policy MARL algorithm to generate reward-free trajectories using the dynamics model. The language-guided reward labeling uses the reward model to assign rewards to these trajectories using the task description. Finally, the policy model is updated using behavior-regularized RL, combining the reward-free trajectories and the language-guided rewards. The LBI framework aims to leverage a generative world model (consisting of a dynamics and a reward model) for improved multi-agent decision making.", "section": "3 Methodology"}]