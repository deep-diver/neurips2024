[{"figure_path": "PgTHgLUFi3/tables/tables_7_1.jpg", "caption": "Table 1: Accuracy achieved by different strategies under limited computational budget. Our Unbiased and Collective strategies outperform the vanilla Top-n and PriSM baselines on all datasets except for Shakespeare, although the gap is not significant for that dataset. In addition, the modifications proposed for local training allow PriSM to significantly improve its performance for ResNet architecture, and sometimes even surpass other methods in the current setting.", "description": "This table presents the accuracy results of different model sharding strategies on various datasets (CIFAR-10, TinyImagenet, CIFAR-100, Shakespeare).  It compares the performance of two novel strategies (Unbiased and Collective) against existing methods (Top-n and PriSM), considering different keep ratios (the proportion of model parameters used).  The results show the impact of both the sampling strategy and modifications to local training on model accuracy, highlighting the superior performance of the proposed methods in many scenarios.", "section": "4.2 Results"}, {"figure_path": "PgTHgLUFi3/tables/tables_7_2.jpg", "caption": "Table 1: Accuracy achieved by different strategies under limited computational budget. Our Unbiased and Collective strategies outperform the vanilla Top-n and PriSM baselines on all datasets except for Shakespeare, although the gap is not significant for that dataset. In addition, the modifications proposed for local training allow PriSM to significantly improve its performance for ResNet architecture, and sometimes even surpass other methods in the current setting.", "description": "This table compares the performance of different model sharding strategies on various datasets under limited computational resources.  The strategies include Top-n, PriSM (with and without modifications), Unbiased, and Collective.  Accuracy is reported for different keep ratios, showing the effectiveness of the novel Unbiased and Collective methods in improving model accuracy compared to existing techniques.", "section": "4.2 Results"}, {"figure_path": "PgTHgLUFi3/tables/tables_14_1.jpg", "caption": "Table 1: Accuracy achieved by different strategies under limited computational budget. Our Unbiased and Collective strategies outperform the vanilla Top-n and PriSM baselines on all datasets except for Shakespeare, although the gap is not significant for that dataset. In addition, the modifications proposed for local training allow PriSM to significantly improve its performance for ResNet architecture, and sometimes even surpass other methods in the current setting.", "description": "This table compares the performance of different model sharding strategies (Top-n, PriSM, Unbiased, and Collective) on various datasets (CIFAR-10, TinyImagenet, CIFAR-100, Shakespeare) under different keep ratios (0.1, 0.2, and a heterogeneous setup with 60% clients having keep ratio 0.2 and 40% with 0.4).  The results showcase that the proposed Unbiased and Collective strategies generally outperform the baselines, particularly for ResNet models.  Modifications to the PriSM strategy, inspired by the Unbiased method,  also yield improvements.", "section": "4.2 Results"}, {"figure_path": "PgTHgLUFi3/tables/tables_14_2.jpg", "caption": "Table 1: Accuracy achieved by different strategies under limited computational budget. Our Unbiased and Collective strategies outperform the vanilla Top-n and PriSM baselines on all datasets except for Shakespeare, although the gap is not significant for that dataset. In addition, the modifications proposed for local training allow PriSM to significantly improve its performance for ResNet architecture, and sometimes even surpass other methods in the current setting.", "description": "This table presents the accuracy results of different model sharding strategies on various datasets (CIFAR-10, TinyImagenet, CIFAR-100, Shakespeare) with limited computational resources.  It compares the performance of the proposed 'Unbiased' and 'Collective' strategies against baseline methods (Top-n and PriSM), showing the impact of different sampling strategies and training modifications on the accuracy of the models.", "section": "4.2 Results"}]