[{"figure_path": "PgTHgLUFi3/figures/figures_8_1.jpg", "caption": "Figure 1: Communication efficiency. The original PriSM method is too explorative (high ANME), while our \u2018unbiased\u2019 modification (+Wallenius) makes it the most exploitative strategy and allows to achieve the best performance in some experiments with limited computational budget.", "description": "This figure compares the communication efficiency and diversity of different sampling strategies used for spectral model sharding.  The x-axis represents the number of communication rounds, and the y-axis on the left shows the test accuracy of a ResNet model on CIFAR-100.  The y-axis on the right shows the average normalized marginal entropy (ANME), a measure of how diverse the sampling strategy is.  High ANME indicates exploration, while low ANME indicates exploitation.  The figure shows that the original PriSM method is highly explorative, while the proposed \u2018unbiased\u2019 modification makes it more exploitative, leading to better performance in limited computation settings. The proposed Unbiased and Collective strategies show a balance between exploration and exploitation.", "section": "4.2 Results"}, {"figure_path": "PgTHgLUFi3/figures/figures_8_2.jpg", "caption": "Figure 1: Communication efficiency. The original PriSM method is too explorative (high ANME), while our \u2018unbiased\u2019 modification (+Wallenius) makes it the most exploitative strategy and allows to achieve the best performance in some experiments with limited computational budget.", "description": "This figure shows the average normalized marginal entropy (ANME) for different sampling strategies across communication rounds.  ANME measures the diversity of samples selected by each strategy; high ANME indicates high diversity (exploration) and low ANME suggests low diversity (exploitation).  PriSM initially shows high ANME (exploratory), while the modification using Wallenius' method shifts it towards lower ANME (exploitative). Other strategies such as Unbiased and Collective occupy intermediate positions. The figure suggests that a balance between exploration and exploitation is needed to optimize performance, especially with limited computational resources.", "section": "4.2 Results"}, {"figure_path": "PgTHgLUFi3/figures/figures_12_1.jpg", "caption": "Figure 2: Convergence analysis. When being trained longer, the proposed strategies demonstrate the decrease of the cross-entropy loss of the global model on the training set. Unbiased strategy reaches the train accuracy of 97.0%, and Collective strategy achieves 98.4%. This serves as empirical evidence of convergence for our method.", "description": "This figure shows the convergence behavior of the proposed Unbiased and Collective sampling strategies during the training process.  The x-axis represents the number of communication rounds, and the y-axis represents the train cross-entropy loss.  The plot demonstrates that both strategies effectively reduce the training loss over an extended training period, reaching high training accuracies (97.0% and 98.4%, respectively). This visual evidence supports the claim that both strategies lead to model convergence.", "section": "4.2 Results"}, {"figure_path": "PgTHgLUFi3/figures/figures_12_2.jpg", "caption": "Figure 3: Impact of data heterogeneity on communication efficiency. When data distribution between clients is closer to i.i.d, the most exploitative Top-n strategy demonstrates the best training speed in the beginning, however it overfits soon. For more heterogeneous data, strategies with exploration achieve much better performance.", "description": "This figure shows the accuracy on the test set for different strategies (Top-n, PriSM + Wallenius + ClipLR, Unbiased, and Collective) across different communication rounds for two scenarios: high data heterogeneity (\u03b1 = 1) and low data heterogeneity (\u03b1 = 10).  The results highlight how the choice of strategy impacts model performance differently depending on the level of data heterogeneity between clients.  With high heterogeneity, the more explorative strategies (Unbiased and Collective) eventually outperform Top-n, while with low heterogeneity, Top-n shows initial advantage but overfits.", "section": "4.2 Results"}, {"figure_path": "PgTHgLUFi3/figures/figures_13_1.jpg", "caption": "Figure 4: Comparison with FedAvg weight updates. For ResNet model trained on CIFAR-10, updates provided by the Top-n strategy significantly deviate from those of FedAvg method. This correlates with the worse performance in this experiment.", "description": "This figure compares the optimization path of FedAvg with different spectral model sharding strategies.  The cosine similarity (using an exponentially weighted moving average with a weight of 0.95) is calculated between the server model updates for each strategy and the FedAvg updates. The results show a significant divergence in the Top-n strategy, indicating its deviation from the FedAvg approach and potentially contributing to its lower performance.", "section": "4.2.2 Discussion"}]