[{"figure_path": "SyMhGilvCv/figures/figures_1_1.jpg", "caption": "Figure 1: A schematic illustrating how typical PEFT methods like LoRA achieve personalization of a foundation model for multiple tasks, such as Yes/No text classification or code completion, during inference.", "description": "This figure illustrates the typical parameter-efficient fine-tuning (PEFT) methods, such as LoRA.  The user sends an input query to a server, which then uses task-specific adapters to modify a frozen foundation model.  The modified model generates a response. This highlights the server-side computational overhead of storing and managing multiple task-specific adapters.  The figure serves as a visual comparison for the proposed LoPA method, which avoids this server-side burden.", "section": "1 Introduction"}, {"figure_path": "SyMhGilvCv/figures/figures_3_1.jpg", "caption": "Figure 2: An illustration of LoPA. No task-specific adapters need to be stored on the server. | represents the concatenation of the soft prompt Z and the input prompt Xe i.e. X = concat(Z|Xe)", "description": "The figure illustrates the architecture of Low-Rank Prompt Adaptation (LoPA). It shows how the soft prompt (Z) is constructed from two components: a task-specific component (Zs) and an instance-specific component (Z1). The instance-specific component is generated using a low-rank decomposition (u x v) to enhance parameter efficiency. The task-specific and instance-specific components are combined using a gating function (g). The resulting soft prompt is then concatenated with the input prompt (Xe) before being fed into the foundation model (FM). No task-specific adapters are needed on the server, making LoPA parameter-efficient and server-side adapter-free.", "section": "3 Proposed Methodology"}, {"figure_path": "SyMhGilvCv/figures/figures_7_1.jpg", "caption": "Figure 3: Performance comparison of baselines as a function of m on (a)-(c) GLUE benchmark and (d) CruxEval-O (with DeepseekCoder-1.3B as FM). Tunable parameters shown relative to the method with the most. Higher performance and fewer parameters indicate better results.", "description": "This figure compares the performance of different prompt tuning methods (S-IDPG, PT, LoPA, and LoRA) across various soft prompt lengths (m).  Subfigures (a)-(c) show results on the GLUE benchmark (RTE, MRPC, and SST-2 tasks), while subfigure (d) shows results on the CruxEval-O task, using the DeepseekCoder-1.3B foundation model. The x-axis represents the length of the soft prompt, the left y-axis shows the performance (accuracy or F1 score), and the right y-axis represents the number of tunable parameters relative to the method with the most parameters.  The figure demonstrates that LoPA generally achieves comparable performance to the best-performing methods (LoRA and full fine-tuning) but with significantly fewer parameters, particularly as the soft prompt length increases.", "section": "4 Experiments, Results, and Discussion"}, {"figure_path": "SyMhGilvCv/figures/figures_8_1.jpg", "caption": "Figure 3: Performance comparison of baselines as a function of m on (a)-(c) GLUE benchmark and (d) CruxEval-O (with DeepseekCoder-1.3B as FM). Tunable parameters shown relative to the method with the most. Higher performance and fewer parameters indicate better results.", "description": "This figure compares the performance of different prompt-tuning methods (LoPA, S-IDPG, PT, and Lora) on various tasks (RTE, MRPC, SST-2, and CruxEval-O) against the number of tunable parameters used.  It shows how performance changes as the length (m) of the soft prompt increases.  Higher performance with fewer parameters is better. The results indicate that LoPA generally outperforms other methods, especially with longer prompts on some tasks, without requiring significantly more parameters.", "section": "4.2 Baseline Comparison"}, {"figure_path": "SyMhGilvCv/figures/figures_12_1.jpg", "caption": "Figure 6: Convergence plots for the PEFT approaches Prompt-tuning (PT), IDPG and the proposed LoPA on the NLU task QQP.", "description": "The figure shows the training loss and validation accuracy curves for three parameter-efficient fine-tuning (PEFT) approaches on the QQP task from the GLUE benchmark.  Prompt Tuning (PT), IDPG, and the proposed Low-Rank Prompt Adaptation (LoPA) methods are compared.  The plot illustrates that LoPA converges faster than PT and IDPG and achieves higher validation accuracy.  This suggests that LoPA is a more efficient and effective approach to adapting large language models.", "section": "7.2 Convergence Analysis of Soft-Prompting Approaches"}, {"figure_path": "SyMhGilvCv/figures/figures_13_1.jpg", "caption": "Figure 7: Convergence plots for the PEFT approaches Prompt-tuning (PT), IDPG and the proposed LoPA on the NLU task MNLI.", "description": "This figure shows the training loss and validation accuracy for three different parameter-efficient fine-tuning (PEFT) methods on the MNLI (Multi-Genre Natural Language Inference) task.  The methods compared are Prompt Tuning (PT), Instance-Dependent Prompt Generation (IDPG), and the authors' proposed method, Low-Rank Prompt Adaptation (LoPA).  The plots illustrate the convergence behavior of each method during training, showing how quickly the loss decreases and the accuracy increases.  The figure provides evidence of LoPA's superior convergence compared to the other methods, showcasing faster improvement in both loss and accuracy.", "section": "4.3 Ablation Study"}, {"figure_path": "SyMhGilvCv/figures/figures_13_2.jpg", "caption": "Figure 3: Performance comparison of baselines as a function of m on (a)-(c) GLUE benchmark and (d) CruxEval-O (with DeepseekCoder-1.3B as FM). Tunable parameters shown relative to the method with the most. Higher performance and fewer parameters indicate better results.", "description": "This figure compares the performance of different prompt tuning methods (Prompt Tuning, S-IDPG, LoRA, and the proposed LoPA) on several tasks (RTE, MRPC, SST-2, and CruxEval-O) as the length of the soft prompt (m) varies. It illustrates that LoPA consistently outperforms other methods and achieves comparable performance to LoRA, while using significantly fewer parameters.", "section": "4.2 Baseline Comparison"}, {"figure_path": "SyMhGilvCv/figures/figures_13_3.jpg", "caption": "Figure 6: Convergence plots for the PEFT approaches Prompt-tuning (PT), IDPG and the proposed LoPA on the NLU task QQP.", "description": "This figure shows the training loss and validation accuracy curves for three parameter-efficient fine-tuning (PEFT) methods: Prompt Tuning (PT), Instance-Dependent Prompt Generation (IDPG), and the proposed Low-Rank Prompt Adaptation (LoPA) on the Question-Question Pair (QQP) task from the GLUE benchmark.  The plots illustrate the convergence behavior of each method during training, indicating how quickly they learn and how well they generalize to unseen data.  Comparing the curves provides insights into the relative efficiency and effectiveness of these PEFT approaches.", "section": "7.2 Convergence Analysis of Soft-Prompting Approaches"}, {"figure_path": "SyMhGilvCv/figures/figures_13_4.jpg", "caption": "Figure 6: Convergence plots for the PEFT approaches Prompt-tuning (PT), IDPG and the proposed LoPA on the NLU task QQP.", "description": "This figure shows the training loss and validation accuracy for three parameter-efficient fine-tuning (PEFT) methods on the QQP (Question-Question Pair) task from the GLUE benchmark. The three methods compared are Prompt Tuning (PT), Instance-Dependent Prompt Generation (IDPG), and the proposed Low-Rank Prompt Adaptation (LoPA).  The plots illustrate the convergence behavior of each method during training. LoPA shows faster convergence and higher validation accuracy compared to PT and IDPG.", "section": "7.2 Convergence Analysis of Soft-Prompting Approaches"}]