[{"figure_path": "SyMhGilvCv/tables/tables_4_1.jpg", "caption": "Table 1: Performance on GLUE tasks. We report the average of accuracy and F1 for both MRPC and QQP. For all the other tasks, we report accuracy. Approaches below the dotted line do not require any modification to the model on the server side. Bold denotes the best-performing tuning method for the given model. Underline marks the best result among all prompt tuning methods.", "description": "This table presents the performance comparison of different prompt tuning methods and other parameter-efficient fine-tuning methods on the GLUE benchmark.  It shows the accuracy (and F1 score for MRPC and QQP) achieved by each method on six different natural language understanding tasks.  The table highlights the parameter efficiency of each method and indicates whether the method requires server-side modifications.  It demonstrates that the proposed LoPA method achieves competitive performance with significantly fewer parameters than other methods that do require server-side modifications.", "section": "4 Experiments, Results, and Discussion"}, {"figure_path": "SyMhGilvCv/tables/tables_5_1.jpg", "caption": "Table 1: Performance on GLUE tasks. We report the average of accuracy and F1 for both MRPC and QQP. For all the other tasks, we report accuracy. Approaches below the dotted line do not require any modification to the model on the server side. Bold denotes the best-performing tuning method for the given model. Underline marks the best result among all prompt tuning methods.", "description": "This table presents the performance of various parameter-efficient fine-tuning (PEFT) methods and full fine-tuning (FFT) on the GLUE benchmark.  It compares different methods' accuracy on six natural language understanding tasks and highlights the parameter efficiency of methods that do not modify the model on the server.  The table shows that the proposed method (LoPA) achieves comparable performance to state-of-the-art methods while being more parameter-efficient and not requiring server-side modifications.", "section": "4 Experiments, Results, and Discussion"}, {"figure_path": "SyMhGilvCv/tables/tables_6_1.jpg", "caption": "Table 3: Performance of LoPA on GLUE tasks with respect to function encoding Z. concat(.) represents the concatenation of Zs and Z\u2081. max(.) represents the element-wise max operation. We report the average of accuracy and F1 for both MRPC and QQP. For all the other tasks, we report accuracy. Bold denotes the best-performing encoding function for LoPA.", "description": "This table presents the performance of the proposed Low-rank Prompt Adaptation (LoPA) method on the GLUE benchmark across various function encoding strategies for combining task-specific and instance-specific components of the soft prompt (Z).  The results show the impact of different fusion methods on the overall accuracy and F1-score across six different tasks.", "section": "4.2 Baseline Comparison"}, {"figure_path": "SyMhGilvCv/tables/tables_9_1.jpg", "caption": "Table 2: Performance comparison on CruxEval and MBPP tasks. We report average pass@1 scores. Approaches below the dotted line are prompt-tuning methods, which do not require any modification to the model on the server side. Bold denotes the best-performing tuning method for the given model. Underline marks the best result among all prompt-tuning methods. OOM indicates that the corresponding tuning approach exceeded the available GPU memory and ran out of memory.", "description": "This table presents the performance comparison results of LoPA against other fine-tuning methods on two code understanding tasks (CruxEval-I and CruxEval-O) and one code generation task (MBPP).  It shows the average pass@1 score for each method across various foundation models of different sizes. The table highlights the parameter efficiency of LoPA compared to other methods while demonstrating comparable or superior performance in most cases.", "section": "4 Experiments, Results, and Discussion"}, {"figure_path": "SyMhGilvCv/tables/tables_12_1.jpg", "caption": "Table 1: Performance on GLUE tasks. We report the average of accuracy and F1 for both MRPC and QQP. For all the other tasks, we report accuracy. Approaches below the dotted line do not require any modification to the model on the server side. Bold denotes the best-performing tuning method for the given model. Underline marks the best result among all prompt tuning methods.", "description": "This table presents the performance of various prompt tuning methods and other parameter-efficient fine-tuning techniques on the GLUE benchmark.  It compares the accuracy (and F1 score for MRPC and QQP) achieved by different methods, highlighting the parameter efficiency and server-side modification requirements of each. The best-performing method for each model and the overall best prompt tuning method are also identified.", "section": "4 Experiments, Results, and Discussion"}]