{"references": [{"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper is foundational to the field of large language models and their few-shot learning capabilities, directly influencing the work done in this paper."}, {"fullname_first_author": "Edward J Hu", "paper_title": "LoRA: Low-rank adaptation of large language models", "publication_date": "2021-06-01", "reason": "This paper introduces the LoRA method, a key parameter-efficient fine-tuning technique which is used as a comparison baseline in this paper."}, {"fullname_first_author": "Xiang Lisa Li", "paper_title": "Prefix-tuning: Optimizing continuous prompts for generation", "publication_date": "2021-01-01", "reason": "This paper introduces the prefix-tuning method, a key prompt-tuning technique which is used as a comparison baseline in this paper."}, {"fullname_first_author": "Brian Lester", "paper_title": "The power of scale for parameter-efficient prompt tuning", "publication_date": "2021-04-01", "reason": "This paper explores prompt tuning, a central method compared against in this paper, offering insights into its advantages and limitations."}, {"fullname_first_author": "Xiao Liu", "paper_title": "P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks", "publication_date": "2021-10-01", "reason": "This paper introduces P-tuning v2, another important prompt-tuning technique that serves as a comparison baseline in this paper."}]}