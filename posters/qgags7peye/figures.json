[{"figure_path": "QgaGs7peYe/figures/figures_4_1.jpg", "caption": "Figure 1: Example levels of Sokoban, where the goal is to push all four boxes into the four red-bordered target spaces. A box can only be pushed, not pulled, making the level irrecoverable if the boxes get stuck. We paint a random empty space blue (which still acts as an empty tile) and predict whether the agent will stand on the blue location within 5 steps.", "description": "This figure shows three example game states from the Sokoban environment used in the paper's experiments.  Sokoban is a game where the player must push boxes into designated goal locations. The figure highlights the complexity of the task, showing different layouts and the need for planning. A blue tile is added to one of the levels to demonstrate the \"event prediction\" task, where the model is asked to predict if the agent will reach this specific location within a certain number of steps.", "section": "5 Experiments"}, {"figure_path": "QgaGs7peYe/figures/figures_5_1.jpg", "caption": "Figure 2: Final accuracy of action prediction and F1 score of event prediction with inner state approach on the testing dataset. The error bar represents two standard errors across 9 seeds.", "description": "This figure displays the performance of action and event prediction using the inner state approach.  It shows the accuracy of predicting all five future actions correctly (no partial credit) and the F1 score for event prediction (predicting whether a specific event occurs within a certain time window). Separate lines depict results for four different RL agent types (MuZero, Thinker, DRC, and IMPALA). The 'inner state' lines represent results when the model has access to the agent's internal state (e.g., plans, hidden layer activations), while 'baseline' lines show results when only the current state and action are used.  Error bars represent the standard deviation across multiple runs.  The x-axis shows the size of the training dataset used.", "section": "5.1 Inner State Approach"}, {"figure_path": "QgaGs7peYe/figures/figures_6_1.jpg", "caption": "Figure 2: Final accuracy of action prediction and F1 score of event prediction with inner state approach on the testing dataset. The error bar represents two standard errors across 9 seeds.", "description": "This figure shows the results of using an inner state approach for predicting future actions and events.  It compares the performance of the predictor when given access to the agent's inner state (e.g., plans, neuron activations) versus when only given the current state and action.  The performance is measured using accuracy for action prediction and F1 score for event prediction. The error bars represent the standard deviation across nine independent runs.", "section": "5.1 Inner State Approach"}, {"figure_path": "QgaGs7peYe/figures/figures_7_1.jpg", "caption": "Figure 4: Change in the final accuracy of action prediction and F1 score of event prediction for the world model ablation settings. The error bar represents two standard errors across 3 seeds.", "description": "This figure displays the effect of world model inaccuracies on the performance of action and event prediction.  It shows the change in accuracy and F1 score compared to a default, accurate model setting. The three model ablation settings shown are a smaller model, a stochastic model, and a partially observable Markov decision process (POMDP).  The results reveal that the inner state approach of explicit planning agents shows more robustness to inaccuracies compared to the simulation-based approaches for action prediction. For event prediction, the effects are more varied.", "section": "5.3 World Model Ablation"}, {"figure_path": "QgaGs7peYe/figures/figures_12_1.jpg", "caption": "Figure 5: The predicted states output by the trained world model, where the starting state is shown in the leftmost column and the input action is five consecutive UP actions.", "description": "This figure visualizes the outputs of the trained world model under various settings.  The leftmost column shows the initial state, and each subsequent column represents a time step in the simulation (with five consecutive \"UP\" actions as input).  The rows represent different model variations: Default (standard model), Small Model (a smaller, less complex model), Stochastic (a model with added randomness), and POMDP (a partially observable model).  Comparing across the rows provides a visual demonstration of the impacts of these variations on model accuracy in predicting the agent's subsequent states.", "section": "5 Experiments"}, {"figure_path": "QgaGs7peYe/figures/figures_13_1.jpg", "caption": "Figure 6: Running average solving rate over the last 200 episodes in Sokoban in both the default setting and the model ablation settings. For the default case, the shaded area represents two standard errors across 3 seeds.", "description": "This figure shows the learning curves of four different reinforcement learning agents (MuZero, Thinker, DRC, and IMPALA) trained on the Sokoban environment.  The x-axis represents the number of frames (or training steps) and the y-axis represents the average solving rate over the last 200 episodes.  The plot includes four subplots corresponding to the different experimental settings: default, small model, stochastic, and POMDP. The shaded regions in the default setting subplot show the standard error for each agent across three different random seeds. These curves illustrate how well the agents learn to solve Sokoban tasks under various conditions, indicating differences in learning speed and performance between various RL algorithms.", "section": "5 Experiments"}, {"figure_path": "QgaGs7peYe/figures/figures_14_1.jpg", "caption": "Figure 4: Change in the final accuracy of action prediction and F1 score of event prediction for the world model ablation settings. The error bar represents two standard errors across 3 seeds.", "description": "This figure shows the performance difference between the default setting and three model ablation settings: small model, stochastic action, and partially observable Markov Decision Process (POMDP).  The x-axis represents the size of the training data used for the predictors. The y-axis shows the changes in both accuracy of action prediction and F1 score of event prediction.  The three ablation settings demonstrate the impact of reduced model capacity (small model), unexpected stochasticity (stochastic action), and partial observability (POMDP) on the performance of the different prediction methods (inner state approach for MuZero, Thinker, and simulation-based approach for DRC, IMPALA). The results highlight the robustness of the inner state approach for explicit planning agents (MuZero, Thinker) under these challenging conditions.", "section": "5.3 World Model Ablation"}, {"figure_path": "QgaGs7peYe/figures/figures_15_1.jpg", "caption": "Figure 8: Ablation experiments on the chosen inner state. The error bars represent two standard errors across 9 seeds.", "description": "This figure presents the results of ablation experiments conducted on the inner state approach for action and event prediction.  Different choices of inner states were tested for MuZero, DRC, and IMPALA agents, evaluating the impact on predictive accuracy. For MuZero, using either the top rollout or the top 3 rollouts was compared. For DRC, using the hidden state at the last tick or all ticks was investigated.  IMPALA was tested with the output of either the last residual block or all three residual blocks. The results show that the choice of inner state has a relatively minor impact on the overall predictive accuracy, suggesting robustness in the chosen approach.", "section": "D Ablation on Inner State Approach"}]