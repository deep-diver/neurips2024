[{"figure_path": "aDQlAz09dS/tables/tables_2_1.jpg", "caption": "Table 1: Comparison to related works.", "description": "This table compares TREACLE with three other related works in terms of their usage of query embedding, response consistency, prompt and LLM selection, long-term budget, and robustness to new models.  A checkmark indicates that the method includes that specific feature or capability.  The table highlights that TREACLE uniquely combines several key features not found in any single prior work.", "section": "2 Related Work"}, {"figure_path": "aDQlAz09dS/tables/tables_16_1.jpg", "caption": "Table 2: Performance with time-varying API query latency.", "description": "This table compares the accuracy of TREACLE and the Calibrated Cascade method under two scenarios: one with time-varying API latency and one with constant latency.  It shows that TREACLE is more robust to fluctuating latency, maintaining higher accuracy than the Calibrated Cascade approach which assumes constant latency.  The \"Update time\" column indicates how long it takes to adjust for the changed latency for each method.", "section": "5.2 Results"}, {"figure_path": "aDQlAz09dS/tables/tables_17_1.jpg", "caption": "Table 3: Characterization of LLM performance in terms of accuracy, latency, and price, for a single query with temperature equal to 0. The Llama models do not have a direct monetary price because they are open-source and we run them locally.", "description": "This table presents the performance of different LLMs and prompting strategies on three datasets (GSM8K, CSQA, LLC).  For each model and prompt, it shows the accuracy achieved on training and test sets, average latency per query, and average monetary price per query. Note that Llama models don't have a monetary price since they're open source and run locally. This table provides a quantitative basis for comparing and selecting models based on their accuracy, cost, and latency trade-offs.", "section": "A.2 Data collection and training"}, {"figure_path": "aDQlAz09dS/tables/tables_17_2.jpg", "caption": "Table 3: Characterization of LLM performance in terms of accuracy, latency, and price, for a single query with temperature equal to 0. The Llama models do not have a direct monetary price because they are open-source and we run them locally.", "description": "This table presents the performance of different LLMs across three datasets (GSM8K, CSQA, LLC) using three different prompting strategies (plain text, domain expert, CoT). For each LLM and prompt combination, it shows the training and testing accuracy, average query latency, and average monetary price. Note that the Llama models do not have a monetary price as they are open-source and run locally.", "section": "A.2 Data collection and training"}, {"figure_path": "aDQlAz09dS/tables/tables_17_3.jpg", "caption": "Table 3: Characterization of LLM performance in terms of accuracy, latency, and price, for a single query with temperature equal to 0. The Llama models do not have a direct monetary price because they are open-source and we run them locally.", "description": "This table presents the performance of different LLMs (Llama and GPT variants) on three different datasets (GSM8K, CSQA, and LLC). For each LLM and dataset, the table shows the accuracy, average latency, and average monetary price for a single query with temperature set to 0.  Note that Llama models don't have a direct monetary price because they are open source and run locally. The table is useful for understanding the trade-offs between accuracy, latency, and cost across different LLMs and for different reasoning tasks.", "section": "A.2 Data collection and training"}, {"figure_path": "aDQlAz09dS/tables/tables_18_1.jpg", "caption": "Table 3: Characterization of LLM performance in terms of accuracy, latency, and price, for a single query with temperature equal to 0. The Llama models do not have a direct monetary price because they are open-source and we run them locally.", "description": "This table presents the performance of different LLMs across three datasets (GSM8K, CSQA, LLC) using different prompting strategies.  For each LLM and prompt type, it shows the accuracy achieved on the training and test sets, the average latency per query, and the average monetary price per query.  Note that the Llama models do not have a direct monetary price as they are open-source and run locally.", "section": "C.2 Model and Prompt Characterization"}, {"figure_path": "aDQlAz09dS/tables/tables_18_2.jpg", "caption": "Table 3: Characterization of LLM performance in terms of accuracy, latency, and price, for a single query with temperature equal to 0. The Llama models do not have a direct monetary price because they are open-source and we run them locally.", "description": "This table presents the performance of different LLMs (Llama-2-7b, Llama-2-13b, GPT-3.5-turbo, GPT-4, and MetaMath) across three datasets (GSM8K, CSQA, LLC) using various prompting strategies (plain text, domain expert, and CoT few-shot).  For each LLM and prompt combination, the table lists the accuracy, average latency (in seconds per query), and average monetary price (in dollars per query).  Note that Llama models are open-source and thus do not incur a direct monetary cost, unlike the GPT models which utilize commercial APIs.", "section": "C.2 Model and Prompt Characterization"}, {"figure_path": "aDQlAz09dS/tables/tables_18_3.jpg", "caption": "Table 3: Characterization of LLM performance in terms of accuracy, latency, and price, for a single query with temperature equal to 0. The Llama models do not have a direct monetary price because they are open-source and we run them locally.", "description": "This table presents the performance of different LLMs (Llama and GPT variants) across three datasets (GSM8K, CSQA, LLC) using three different prompting strategies (plain text, domain expert, chain-of-thought).  For each LLM and prompt combination, the average accuracy, latency, and monetary price (where applicable) are reported for both the training and test sets.  The table highlights the varied capabilities and costs associated with different LLMs and prompting approaches.", "section": "C.2 Model and Prompt Characterization"}, {"figure_path": "aDQlAz09dS/tables/tables_20_1.jpg", "caption": "Table 3: Characterization of LLM performance in terms of accuracy, latency, and price, for a single query with temperature equal to 0. The Llama models do not have a direct monetary price because they are open-source and we run them locally.", "description": "This table presents the performance of different LLMs and prompting strategies on three datasets (GSM8K, CSQA, LLC).  For each model and prompt combination, it lists the training and testing accuracy, average latency per query, and average monetary price per query.  Note that the Llama models do not have a monetary price because they are open-source and run locally. The table helps to understand the trade-offs between accuracy, latency, and cost for various LLMs and prompting strategies, providing essential data for the TREACLE model's decision-making process.", "section": "A.2 Data collection and training"}, {"figure_path": "aDQlAz09dS/tables/tables_20_2.jpg", "caption": "Table 3: Characterization of LLM performance in terms of accuracy, latency, and price, for a single query with temperature equal to 0. The Llama models do not have a direct monetary price because they are open-source and we run them locally.", "description": "This table presents the performance of different LLMs on three datasets (GSM8K, CSQA, LLC) using three different prompting strategies (plain text, standard few-shot, CoT few-shot).  For each LLM and prompt combination, the table shows the training and testing accuracy, average latency, and average monetary price per query. Note that the Llama models do not have a direct monetary price because they are open-source and run locally.", "section": "Appendix A.3 Baselines"}, {"figure_path": "aDQlAz09dS/tables/tables_20_3.jpg", "caption": "Table 3: Characterization of LLM performance in terms of accuracy, latency, and price, for a single query with temperature equal to 0. The Llama models do not have a direct monetary price because they are open-source and we run them locally.", "description": "This table presents the performance of different LLMs (Llama and GPT variants) on three different datasets (GSM8K, CSQA, LLC). For each LLM, three different prompts are used (plain text, domain expert, and CoT). The table shows the accuracy, average latency, and average monetary price for each LLM-prompt combination. The Llama models are open-source and run locally, so they do not have a direct monetary price.", "section": "C.2 Model and Prompt Characterization"}, {"figure_path": "aDQlAz09dS/tables/tables_20_4.jpg", "caption": "Table 3: Characterization of LLM performance in terms of accuracy, latency, and price, for a single query with temperature equal to 0. The Llama models do not have a direct monetary price because they are open-source and we run them locally.", "description": "This table presents the performance of different LLMs (Llama and GPT variants) on three reasoning datasets (GSM8K, CSQA, and LLC).  For each LLM and dataset, it shows the accuracy, latency, and price (where applicable) of using a single query with a temperature of 0.  The table highlights the trade-offs between accuracy, latency, and cost for different models and datasets. Note that the Llama models are open source and run locally, so they do not have a direct monetary price.", "section": "C.2 Model and Prompt Characterization"}, {"figure_path": "aDQlAz09dS/tables/tables_21_1.jpg", "caption": "Table 3: Characterization of LLM performance in terms of accuracy, latency, and price, for a single query with temperature equal to 0. The Llama models do not have a direct monetary price because they are open-source and we run them locally.", "description": "This table presents a quantitative comparison of various LLMs across three different reasoning datasets (GSM8K, CSQA, and LLC). For each LLM and dataset combination, the table lists the average accuracy, latency, and monetary price (where applicable) observed when answering a single query with temperature set to 0.  The results offer insights into the tradeoffs between accuracy, speed, and cost of different LLMs.  Note that Llama models do not have a direct monetary price because they are open-source and were run locally.", "section": "C.2 Model and Prompt Characterization"}, {"figure_path": "aDQlAz09dS/tables/tables_22_1.jpg", "caption": "Table 3: Characterization of LLM performance in terms of accuracy, latency, and price, for a single query with temperature equal to 0. The Llama models do not have a direct monetary price because they are open-source and we run them locally.", "description": "This table presents the performance characterization of different LLMs (Llama and GPT variants) across three datasets (GSM8K, CSQA, LLC) using three different prompting strategies (plain text, domain expert, CoT).  For each LLM and prompt combination, the table provides the training and testing accuracy, average latency per query, and average monetary price per query.  Note that the Llama models do not have a direct monetary price since they are open-source and run locally.", "section": "A.2 Data collection and training"}, {"figure_path": "aDQlAz09dS/tables/tables_23_1.jpg", "caption": "Table 3: Characterization of LLM performance in terms of accuracy, latency, and price, for a single query with temperature equal to 0. The Llama models do not have a direct monetary price because they are open-source and we run them locally.", "description": "This table presents the performance of different LLMs (Llama and GPT variants) on three different datasets (GSM8K, CSQA, LLC). For each LLM and dataset, it shows the accuracy, average latency, and average monetary price (for GPT models only) for a single query with a temperature of 0.  The Llama models are open-source and run locally, so their monetary price is not applicable.", "section": "A.3 Baselines"}, {"figure_path": "aDQlAz09dS/tables/tables_24_1.jpg", "caption": "Table 3: Characterization of LLM performance in terms of accuracy, latency, and price, for a single query with temperature equal to 0. The Llama models do not have a direct monetary price because they are open-source and we run them locally.", "description": "This table presents the performance of different LLMs (Llama and GPT variants) on three datasets (GSM8K, CSQA, LLC) using three different prompting strategies (plain text, domain expert, and chain-of-thought).  For each LLM and prompt combination on each dataset, the table lists the accuracy, average latency, and monetary price (where applicable) of a single query with a temperature setting of 0.  The Llama models are open source and run locally, so they don't have a direct monetary cost.  This data helps to quantify the accuracy-cost-latency tradeoffs of various LLMs and prompts that are relevant to the paper's proposed TREACLE framework.", "section": "A.2 Data collection and training"}, {"figure_path": "aDQlAz09dS/tables/tables_24_2.jpg", "caption": "Table 3: Characterization of LLM performance in terms of accuracy, latency, and price, for a single query with temperature equal to 0. The Llama models do not have a direct monetary price because they are open-source and we run them locally.", "description": "This table presents a comprehensive characterization of various Large Language Models (LLMs) across three different datasets (GSM8K, CSQA, LLC).  For each LLM and dataset, it details the accuracy achieved on training and testing sets, the average latency experienced per query, and the average monetary price (where applicable). The table highlights the trade-offs between accuracy, latency, and cost, crucial factors for selecting appropriate LLMs for various applications. The temperature parameter used for all queries was set to 0, enabling a fair comparison of the models' baseline performance.", "section": "C.2 Model and Prompt Characterization"}]