{"references": [{"fullname_first_author": "Karl Cobbe", "paper_title": "Training verifiers to solve math word problems", "publication_date": "2021-10-27", "reason": "This paper introduces the GSM8K dataset, a benchmark used extensively for evaluating the model's performance on mathematical reasoning tasks."}, {"fullname_first_author": "Jason Wei", "paper_title": "Chain-of-thought prompting elicits reasoning in large language models", "publication_date": "2022-XX-XX", "reason": "This paper introduces the chain-of-thought prompting technique, which is a key prompting strategy used and analyzed in the current research."}, {"fullname_first_author": "Aman Madaan", "paper_title": "Automix: Automatically mixing language models", "publication_date": "2023-XX-XX", "reason": "This paper introduces AutoMix, a related method that uses multiple LLMs, providing a relevant comparison to the proposed TREACLE framework."}, {"fullname_first_author": "Lingjiao Chen", "paper_title": "FrugalGPT: How to use large language models while reducing cost and improving performance", "publication_date": "2023-05-05", "reason": "This paper introduces FrugalGPT, a cost-constrained LLM selection method, which is directly compared with the proposed method in this research."}, {"fullname_first_author": "Volodymyr Mnih", "paper_title": "Human-level control through deep reinforcement learning", "publication_date": "2015-XX-XX", "reason": "This paper introduces Deep Q-Network (DQN), the reinforcement learning algorithm used to train the TREACLE policy, providing foundational support for the core methodology."}]}