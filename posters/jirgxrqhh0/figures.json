[{"figure_path": "JiRGxrqHh0/figures/figures_6_1.jpg", "caption": "Figure 1: Enforcement of Agent Truthfulness. Average net improvement in loss over local training is plotted for iid (dotted line) and two non-iid agent distributions (D-0.6: dashed, D-0.3: solid). For both CIFAR10 (left) and MNIST (right), agents maximize their net improvement in loss when they are truthful (0% added) about their true cost. This matches our theory in Theorem 5.", "description": "This figure shows the average improvement in loss (compared to local training) for different data distributions and when agents report their costs truthfully or with some percentage added or subtracted from the true cost.  The results for CIFAR-10 and MNIST datasets are shown side-by-side.  It demonstrates that the maximum improvement in loss occurs when agents report their costs truthfully, aligning with the theoretical findings of Theorem 5 in the paper.", "section": "Experimental Results"}, {"figure_path": "JiRGxrqHh0/figures/figures_7_1.jpg", "caption": "Figure 2: Reduction in Agent Loss. The average agent loss for baselines on CIFAR10 (top row) and MNIST (bottom row) under iid (left), and two non-iid data distributions (center: D-0.6, right: D-0.3). Traditional FL is an upper bound on agent loss (if agents did not free ride). FACT improves agent loss over local training by up to a factor of 3x for CIFAR10 and 4x for MNIST.", "description": "This figure compares the average agent loss for three different training methods: local training, FACT training, and traditional federated learning (FL).  It shows the results for both CIFAR-10 and MNIST datasets, under iid (independent and identically distributed) and two non-iid (non-independent and identically distributed) data settings.  The results demonstrate that FACT significantly reduces agent loss compared to local training, and achieves better performance than traditional FL, which is known to suffer from the free-rider problem.", "section": "Experimental Results"}, {"figure_path": "JiRGxrqHh0/figures/figures_8_1.jpg", "caption": "Figure 3: Elimination of Free Riding via Penalty. The penalty term  Pfr(mi) plus data collection costs cimi is plotted for CIFAR10 (left) and MNIST (right) for varying data contributions mi. These combined costs are minimized at the local optimum m*, as predicted by Theorem 3.", "description": "This figure shows the combined free-riding penalty and data costs for CIFAR-10 and MNIST datasets. The x-axis represents the amount of data contributed by an agent (mi), and the y-axis represents the total cost (penalty + data cost).  The plot demonstrates that the total cost is minimized when the agent contributes the locally optimal amount of data (m*), which is predicted by Theorem 3 in the paper. This visually confirms that the proposed penalty mechanism effectively discourages free-riding behavior by making it more costly for agents to contribute less than the optimal amount of data.", "section": "4 Eliminating Free Riding via Penalization"}, {"figure_path": "JiRGxrqHh0/figures/figures_9_1.jpg", "caption": "Figure 4: FACT Eliminates Free Riding in Realistic Settings. When training an image classifier for diagnosing skin cancer, agents participating in FACT achieve much lower loss (66% less) than if they did not participate (left). Agents maximize their improvement in loss over local training when they are truthful; reporting inflated or deflated costs diminishes improvement in loss (middle). Agents minimize penalties when using their locally optimal amount of data (m* = 801) for training (right).", "description": "This figure demonstrates FACT's effectiveness in a real-world scenario of skin cancer diagnosis.  The left panel shows that agents using FACT achieve significantly lower loss (66%) compared to local training. The middle panel illustrates the truthfulness mechanism; agents maximize their loss improvement when reporting their true costs.  Inflating or deflating the cost reduces the benefit, enforcing truthful behavior.  The right panel visualizes the penalty function, demonstrating that the minimum penalty and optimal data usage align, which eliminates free-riding.", "section": "6.1 Real-World Case Study: Inter-Hospital Skin Cancer Diagnosis"}, {"figure_path": "JiRGxrqHh0/figures/figures_14_1.jpg", "caption": "Figure 5: Test Loss for CIFAR10 (top) and MNIST (bottom) in Heterogeneous Settings. FL outperforms local training on iid (left) and mild (middle) & strong (right) non-iid Dirichlet settings.", "description": "This figure compares the test loss for both federated learning (FL) and local training across three different data distribution settings: IID, mildly non-IID, and strongly non-IID.  The non-IID settings use Dirichlet distributions with \u03b1 = 0.3 and 0.6 to model heterogeneity in data. The results clearly demonstrate the robustness of federated training using FedAvg, consistently outperforming local training, particularly in the non-IID scenarios where data is more unevenly distributed across clients.", "section": "Experimental Results"}, {"figure_path": "JiRGxrqHh0/figures/figures_15_1.jpg", "caption": "Figure 5: Test Loss for CIFAR10 (top) and MNIST (bottom) in Heterogeneous Settings. FL outperforms local training on iid (left) and mild (middle) & strong (right) non-iid Dirichlet settings.", "description": "This figure compares the test loss of federated learning (FL) and local training on CIFAR-10 and MNIST datasets under different data distribution scenarios. It shows that federated learning consistently outperforms local training across various settings, including iid (independent and identically distributed) and non-iid (non-independent and identically distributed) data with varying degrees of heterogeneity.  The results highlight the benefits of federated learning, especially in non-iid scenarios where data is not evenly distributed across participating agents.", "section": "6 Experimental Results"}, {"figure_path": "JiRGxrqHh0/figures/figures_15_2.jpg", "caption": "Figure 7: HAM10000 Test Loss (Left) and Accuracy (Right) for Federated and Local Training.", "description": "This figure shows the test loss and accuracy for both federated and local training on the HAM10000 dataset. The left plot displays the test loss over epochs, while the right plot shows the test accuracy over epochs.  The results show that federated learning outperforms local training for this specific dataset. The error bars around the accuracy represent the variation across multiple training runs.", "section": "Experimental Results"}]