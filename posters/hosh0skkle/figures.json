[{"figure_path": "HOSh0SKklE/figures/figures_4_1.jpg", "caption": "Figure 1: Relative expansion (Definition 3) on the sets (A, B). Expansion requires that certain subsets U C B have neighborhoods N(U) such that P(N(U)|A) \u2265 cP(U|B). These probabilities are represented graphically on the right-hand-side as the fractions |N(U) \u2229 A|/|A| and |U|/|B|.", "description": "This figure illustrates the concept of relative expansion, a key condition in the paper's theoretical analysis.  The left side shows two sets, A (green) and B (blue), with set U (light blue circle) being a subset of B. The neighborhood of U, N(U), is represented by the dashed green oval. Relative expansion means that the ratio of the probability of N(U) in set A to the probability of U in set B must be greater than or equal to a constant c. The right side provides a simplified graphical representation of this condition as fractions, emphasizing the relationship between the size of the neighborhood of U within A and the size of U within B.", "section": "3 Setup and Shortcomings of Existing Bounds"}, {"figure_path": "HOSh0SKklE/figures/figures_16_1.jpg", "caption": "Figure 2: Examples of good (left) and bad (right) robust expansion. In both cases, there is a core subset V \u2282 N(U) that accounts for most of the edge weight incident on U (at least a 1 \u2212 \u03b7 fraction, for some small \u03b7). The robust expansion is good when every such subset has large probability.", "description": "This figure illustrates the concept of robust expansion, a key idea in the paper.  Robust expansion is a generalization of standard expansion that accounts for the possibility of a few 'bad' edges in the graph connecting two sets. The left panel depicts good robust expansion, where even after removing a small fraction of the edges (the bad edges) there is still a significant connection between sets A and B. The right panel depicts bad robust expansion where even a small fraction of edge removals completely breaks the connection between the sets. The authors define robust expansion formally in Definition 7 and use it in Section 4 to derive bounds that are more robust to adversarial examples and noise.", "section": "4 Error Bounds for Weakly-Supervised Classifiers"}, {"figure_path": "HOSh0SKklE/figures/figures_36_1.jpg", "caption": "Figure 3: Example of our neighborhood oracle n, constructed using a targeted paraphrase procedure. For a covered point x \u2208 S (in this case, x \u2208 Sbad, since it is a true negative point mislabeled by our example weak rules \u1ef9), we first generate an uncovered point x' \u2208 Ti using a constrained paraphrase model and rejection sampling to ensure the ground-truth label remains negative (we use a model trained on the gold labels as a stand-in for the ground truth y). Next, we use GPT-4 to rewrite x' using the opposite word from {horrible, incredible} than the one that originally appeared. This generates another point x'' \u2208 S\u2081. Since we enforce that x and x'' are covered by different words, we know that if x \u2208 Sgood (resp. Shad), x'' \u2208 Shad (resp. Sgood).", "description": "This figure illustrates the process of generating neighborhood points using a paraphrase model and GPT-4.  Starting with a point mislabeled by the weak supervision model, it shows how to generate a point in the uncovered region and then use that to create a point with the correct label via paraphrase.  This process is used to check the expansion condition empirically.", "section": "Experiments"}]