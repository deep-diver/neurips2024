[{"type": "text", "text": "WeiPer: OOD Detection using Weight Perturbations of Class Projections ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Maximilian Granz ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Manuel Heurich ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Institute for Computer Science Free University of Berlin Arnimallee 7 14195 Berlin maximilian.granz@fu-berlin.de ", "page_idx": 0}, {"type": "text", "text": "Institute for Computer Science Free University of Berlin Arnimallee 7 14195 Berlin manuel.heurich@fu-berlin.de ", "page_idx": 0}, {"type": "text", "text": "Tim Landgraf   \nInstitute for Computer Science   \nFree University of Berlin   \nArnimallee 7 14195 Berlin   \ntim.landgraf@fu-berlin.de ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recent advances in out-of-distribution (OOD) detection on image data show that pre-trained neural network classifiers can separate in-distribution (ID) from OOD data well, leveraging the class-discriminative ability of the model itself. Methods have been proposed that either use logit information directly or that process the model\u2019s penultimate layer activations. With \"WeiPer\", we introduce perturbations of the class projections in the final fully connected layer which creates a richer representation of the input. We show that this simple trick can improve the OOD detection performance of a variety of methods and additionally propose a distancebased method that leverages the properties of the augmented WeiPer space. We achieve state-of-the-art OOD detection results across multiple benchmarks of the OpenOOD framework, especially pronounced in difficult settings in which OOD samples are positioned close to the training set distribution. We support our findings with theoretical motivations and empirical observations, and run extensive ablations to provide insights into why WeiPer works. Our code is available at: https://github.com/mgranz/weiper. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Out-of-Distribution (OOD) detection has emerged as a pivotal area of machine learning research. It addresses the challenge of recognizing input data that deviates significantly from the distribution seen during training. This capability is critical because machine learning models, particularly deep neural networks, are known to make overconfident and incorrect predictions on such unseen data Hendrycks & Gimpel (2016). The need for OOD detection is driven by practical considerations. In real-world applications, a model frequently encounters data that is not represented in its training set. For instance, in autonomous driving, a system trained in one geographic location might face drastically different road conditions in another. Without robust OOD detection, these models risk making unsafe decisions Amodei et al. (2016). ", "page_idx": 0}, {"type": "text", "text": "Over the last few years, the field has made significant steps towards setting up benchmarks and open baseline implementations. Thanks to the efforts of the OpenOOD team Zhang et al. (2023b); Yang et al. (2022), we can evaluate new methods across CIFAR10, CIFAR100 and ImageNet, and compare them against a variety of methods, on the same network checkpoints. To this date, however, there is no single method outperforming the competition on all datasets Tajwar et al. (2021), which indicates a variety of ways in which OOD data differs from the training set. Here, we introduce WeiPer, a method that can be applied to any pretrained model, any training loss used, with no limitation on the data modality to separate ID and OOD datapoints. WeiPer creates a representation of the data by projecting the latent representation of the penultimate layer onto a cone of vectors around the class-projections of the final layer\u2019s weight matrix. This allows extracting additional structural information on the training distribution compared to using the class projections alone and specifically exploits the fact that the OOD data often extends into the cluster of positive samples of the respective class in a conical shape (see Figure 1). In addition to WeiPer, our KL-divergence-based method WeiPer+KLD represents a novel OOD detection score that is based on the following observation: ", "page_idx": 0}, {"type": "image", "img_path": "8HeUvbImKT/tmp/4452a5a8fd3f2984e2bc4007c4779f3b684aec5a83dd7e72a67f55f00dada0c0.jpg", "img_caption": ["Figure 1: Why random perturbations? Left: We visualize densities of CIFAR10 (ID, blue) and CIFAR100 (OOD, red) as contour plots along the two logit dimensions spanned by $\\pmb{w}_{0}$ and $\\pmb{w}_{1}$ , zoomed in on the positive cluster of class zero. The blue axis denotes the vector associated with that class, and one of its perturbations is depicted by the turquoise line. Right: When projecting the data onto both vectors, we obtain the densities shown in the top and bottom panel, respectively. The vertical blue lines mark the 5-percentile (highest $5\\%$ ) of the true ID data (CIFAR10, blue). At this decision boundary, the classifier would produce false positives in the marked dashed red tail area. A single perturbation of the class-associated vector yields already a reduction of the false positive rate (FPR) from $1.34\\%$ to $0.79\\%$ . Visually, we confirm that OOD data mostly resides close to 0, extending into the positive cluster in a particular conical shape, which is exploited by the cone of WeiPer vectors. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "When ignoring the individual dimensions and examining the activation distribution across all dimensions, we observe that ID samples exhibit a similar \"fingerprint\" distribution. The more feature dimensions there are, the better our estimate of this source distribution becomes. We demonstrate that measuring the discrepancy between the per-sample distribution and the training set\u2019s mean distribution in the augmented WeiPer space leads to improved OOD detection accuracy. We evaluate WeiPer on OpenOOD using our proposed KL-divergence-based scoring function (KLD), MSP Hendrycks & Gimpel (2016), and ReAct Sun et al. (2021). Additionally, we conduct an ablation study to understand the influence of each component of WeiPer and analyze WeiPer\u2019s performance. Our results confirm that the weight perturbations allow WeiPer to outperform the competition on two out of eight benchmarks, demonstrating consistently better performance on near OOD tasks. WeiPer represents a versatile, off-the-shelf method for state-of-the-art post-hoc OOD detection. However, the performance of WeiPer comes at a cost: The larger the WeiPer space, the more memory is required. ", "page_idx": 1}, {"type": "text", "text": "\u2022 We discover that OOD detection can be improved by considering linear projections of the penultimate layer that correlate with the final output, i.e., the class representations. We construct these projections by perturbing the weights of the final layer. \u2022 We uncover a fingerprint-like nature of the ID samples in both the penultimate space and our newly found perturbed space, proposing a novel post-hoc detection method that leverages this structure. The activation distributions of the penultimate space and our WeiPer space over the dimensions of each sample are similar for each ID input, yielding distributions in both spaces that we compare to the mean ID distribution using KL divergence. \u2022 We evaluate our findings by testing the proposed methods and two other MSP-based methods on the perturbed class projections using the OpenOOD benchmark, achieving state-of-the-art performance on near OOD tasks. ", "page_idx": 2}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "OOD detection. Generally, we can distinguish two types of OOD detection methods - one that requires retraining of the model, including novel loss variants, data augmentations, or even outlier exposure settings. Here, we focus on post-hoc methods that can be added with little effort to any existing pipeline. They can be applied to any pretrained model, irrespective of its architecture, loss objective or data modality. Post-hoc methods can be distinguished further in: ", "page_idx": 2}, {"type": "text", "text": "1) Confidence-based methods Guo et al. (2017); Hendrycks et al. (2022a,b); Liu et al. (2023a, 2020); Wang et al. (2022) process samples in the model\u2019s logit space, i.e. using the network directly to detect ID/OOD data points. A prominent example is the Maximum Softmax Probability (MSP) (Hendrycks & Gimpel, 2016) which simply uses the maximum logit as the main OOD decision metric. Some methods Ahn et al. (2023); Djurisic et al. (2022); Sun et al. (2021) additionally introduce transformations such as cutoffs of the features in the penultimate layer or masks on the weight matrix Sun & Li (2021) to allocate where ID data resides and combine these with confidence metrics. Several recent methods have employed f-divergences to improve OOD detection, focusing on enhancing the boundary definition between ID and OOD samples Darrin et al. (2022); Picot et al. (2022). ", "page_idx": 2}, {"type": "text", "text": "2) Distance-based methods Bendale & Boult (2015); Lee et al. (2018); Liu et al. (2023b); Ren et al. (2021); Sastry & Oore (2020); Sun et al. (2022); Zhang et al. (2023a) define distance measures between the training distribution and an input sample in latent space, i.e. primarily the penultimate layer of the network. Deep Nearest Neighbors Sun et al. (2022) uses the distance to the $k$ -th closest neighbor in latent space, while MDS Lee et al. (2018) models the data as Gaussian and uses the Mahalonibis-Distance. Models of the data distribution can improve the OOD detection performance, e.g. using histograms to approximate the training density and then define a distance measure on them. A recent work Liu et al. (2023b) proposed creating a histogram-based distribution on the product of the penultimate activations and the gradient of a separate KL-loss and then defined a metric on these modified discrete densities. ", "page_idx": 2}, {"type": "text", "text": "Both approaches of 1) and 2) are not exclusive. NNGuide Park et al. (2023) combines both confidence and distance measures into a joint score, improving performance in case one of the scores fails. ", "page_idx": 2}, {"type": "text", "text": "Random weight perturbations and projections. Weight perturbations, i.e. adding noise values to the weights of a network, have been used for a variety of applications: in sensitivity analyses Cheney et al. (2017); Xiang et al. (2019), for studying robustness against adversarial attacks Rakin et al. (2018); Wu et al. (2020), and as training regularization Khan et al. (2018); Wen et al. (2018). Random projections from the latent space of the neural network have been described in the context of generative modeling Bonneel et al. (2014); Jerome H. Friedman & Schroeder (1984); Kolouri et al. (2016); Liutkus et al. (2019); Nguyen et al. (2021); Paty & Cuturi (2019), e.g. to improve the Wasserstein distance calculation or for robustness. A previous work described random projections from the penultimate layer to detect out-of-distribution samples with a normalizing flow Kuan & Mueller (2022). ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "3.1 Preliminaries ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We consider a pretrained neural network classifier $f:\\mathcal{X}\\rightarrow\\mathbb{R}^{C}$ that maps samples $x$ from an input space $\\mathcal{X}\\,\\in\\,\\mathbb{R}^{\\dot{D}}$ to a logit vector $f(\\pmb{x})\\,\\in\\,\\mathbb{R}^{C}$ , by applying a linear projection $W_{\\mathrm{fc}}$ to the feature representation in the penultimate layer ", "page_idx": 3}, {"type": "equation", "text": "$$\n(z_{1},...,z_{K})^{T}=z=h(\\pmb{x})=(h_{1}(\\pmb{x}),...,h_{K}(\\pmb{x}))^{T}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "such that $f(\\pmb{x})\\;=\\;\\pmb{W}_{\\mathrm{fc}}^{T}\\pmb{z}$ , with $D,\\;K$ and $C$ representing the dimensionality of the input, the penultimate layer and the output layer, respectively. We define the rows of the final weight layer to be $w_{1},...,w_{C}$ . ", "page_idx": 3}, {"type": "text", "text": "In the following it is useful to introduce $Z$ as random vector from which we draw our latent samples. We denote the densities of the latent activations of the training data with $p_{Z_{\\mathrm{train}}}$ , of the test data with $p_{Z_{\\mathrm{test}}}$ and those of OOD samples with $p_{Z_{\\mathrm{ood}}}$ admitted by the random vectors $Z_{\\mathrm{train}},Z_{\\mathrm{test}}$ and $Z_{\\mathrm{ood}}$ , respectively. To ease notation, we will treat $Z$ and all its subsets, both as sets, e.g. $Z_{\\mathrm{train}}$ is the set of all training activations in the penultimate layer. ", "page_idx": 3}, {"type": "text", "text": "An OOD detector is a binary classifier $O$ that decides if samples are drawn from an ID or OOD distribution by usually only considering samples drawn from $p_{Z_{\\mathrm{test}}}$ or $p_{Z_{\\mathrm{{ood}}}}$ . Commonly, this is achieved by thresholding a scalar score function $S$ . ", "page_idx": 3}, {"type": "equation", "text": "$$\nO(\\pmb{x})=\\left\\{\\!\\!\\begin{array}{l l}{\\mathrm{ID}}&{\\mathrm{if}\\;S(\\pmb{x})>\\lambda}\\\\ {\\mathrm{OOD}}&{\\mathrm{otherwise}}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "For MSP, the score function is simply the maximum softmax probability ", "page_idx": 3}, {"type": "equation", "text": "$$\nS(\\pmb{x})=\\operatorname{MSP}(\\pmb{x})=\\operatorname*{max}_{i=1,\\ldots,C}\\frac{e^{f(\\pmb{x})_{i}}}{\\sum_{j=1}^{C}e^{f(\\pmb{x})_{j}}}=:\\operatorname{MSP}(f(\\pmb{x})).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Note, that for clarity, we define MSP also as a function of the logits. Other methods propose metrics on the penultimate layer, e.g. by incorporating distance measures between a given latent activation $_{z}$ of a new sample and the distribution of activations $p_{Z_{\\mathrm{train}}}$ of the training set. ", "page_idx": 3}, {"type": "text", "text": "3.2 WeiPer: Weight perturbations ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "A neural network classifier maps the data distribution to the distribution of the logits $W_{\\mathrm{fc}}Z$ . The training objective of the network ensures an optimal separation of classes and lets the model learn to exploit features in $Z$ specific to the training distribution. OOD samples, hence, often yield lower logit scores. Confidence methods leverage this property, but could potentially be improved by capturing more of the underlying distribution of the penultimate layer. A confidence score measures properties of the logit distribution $W_{\\mathrm{fc}}Z$ . Is there additional information in the penultimate layer of the network, and if so, how can we utilize it? ", "page_idx": 3}, {"type": "text", "text": "Applying the weight matrix $W_{\\mathrm{fc}}$ to the penultimate space can be understood as $C$ projections of $Z$ onto the row vectors $\\pmb{w}$ . According the Cramer-Wold theorem (Cram\u00e9r & Wold (1936)), we can reconstruct the source density $p z$ from all one-dimensional linear projections, and Cuesta-Albertos et al. (2007) has shown that a K-dimensional subset of projections suffices (for more details see Appendix A.1.1). The question remains which projections extract the most relevant information? ", "page_idx": 3}, {"type": "text", "text": "Drawing vectors $\\pmb{w}\\in W=\\mathcal{N}(0,I)$ from a standard normal and projecting onto them often results in similar densities for ID and OOD data, i.e. ${\\pmb w}^{T}Z_{\\mathrm{train}}\\approx{\\pmb w}^{T}Z_{\\mathrm{ood}}$ , deteriorating detection performance (see Table 1, RP). This aligns with Papyan et al. (2020), suggesting limited information in the penultimate layer compared to the logits. We hypothesize that the latent distribution shows relevant structure only along certain dimensions. We applied PCA to the latent activations $Z$ and inspected the resulting projections. This analysis supports the notion that the informative dimensions lie in the directions of the class projections $w_{1},...,w_{C}$ (see Appendix A.1.3). Hence, we construct projections that correlate with these vectors but at the same time deviate enough to obtain new information. ", "page_idx": 3}, {"type": "image", "img_path": "8HeUvbImKT/tmp/ed201917399660c2e01d7eec43a74e58e190824422f5a5a71969b443f06074a4.jpg", "img_caption": ["Figure 2: WeiPer perturbs the weight vectors of $W_{\\mathrm{fc}}$ by an angle controlled by $\\delta$ . For each weight, we construct $r$ perturbations resulting in $r$ weight matrices $\\tilde{\\boldsymbol{W}_{1}},...,\\tilde{\\boldsymbol{W}_{r}}$ . KLD: For $W e i P e r{+}\\mathrm{KLD}$ , we treat $z_{1},...,z_{k}\\sim p_{z}$ and $w_{1,1}^{T}z,...,w_{r,C}^{T}z\\stackrel{<}{\\sim}p_{\\tilde{W}z}$ as samples of the same distribution induced by $z$ , respectively. We approximate the densities with histograms and smooth the result with uniform kernel $T_{k_{s}}$ . Afterwards, we compare the densities $T_{k_{s}}(q_{z})$ with the mean distribution over the training samples $\\mathbb{E}_{z\\in Z_{\\mathrm{train}}}(q_{z})$ for $q_{z}=p_{z}$ and $q_{z}=p_{\\tilde{W}z}$ , respectively. MSP: For a score function $S$ on the logit space $\\mathbb{R}^{C}$ , we define the perturbed score $S_{\\mathrm{{WeiPer}}}$ as the mean over all the perturbed logit spaces $\\tilde{W}z$ . We choose $S=\\ensuremath{\\mathrm{MSP}}$ and call the resulting detector $\\mathrm{MSP}_{\\mathrm{WeiPer}}$ . "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Definition of WeiPer We define perturbations $\\eta$ , drawn from a standard normal and add them to $W_{\\mathrm{fc}}$ . To ensure that all perturbed vectors have the same angular deviation from the original weight vector, we normalize the perturbations to be the same length as their corresponding row vector and multiply them by a factor $\\delta$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\tilde{\\mathbf{w}}_{i,j}=\\mathbf{w}_{j}+\\delta\\frac{\\pmb{\\eta}_{i}\\lVert\\mathbf{w}_{j}\\rVert}{\\lVert\\pmb{\\eta}_{i}\\rVert}=:\\mathbf{w}_{j}+\\tilde{\\eta}_{i},\\quad\\pmb{\\eta}_{i}\\sim\\mathcal{N}(\\mathbf{0},I_{K})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "for $i=1,...,r$ , where $\\delta$ represents the length ratio between ${\\pmb w}_{j}$ and the perturbation $\\eta_{i}$ . For large $K=\\dim(Z)$ , ${\\pmb w}_{j}$ and $\\tilde{\\pmb{\\eta}}_{i}$ are almost orthogonal and thus $\\delta$ actually adjusts the angle $\\alpha\\approx\\arctan(\\delta)$ of the perturbed vector bundle. We set $\\delta$ to be constant across all $j=1,...,K$ and treat both $\\delta$ and $r$ as hyperparameters. This proceedure is related to the Distributional Sliced Wasserstein distance Nguyen et al. (2021) as they sample projections from a distribution such that the mean angle between the projections is greater than arccos $(C)$ for a constant $C$ .The whole set of vectors we define is ", "page_idx": 4}, {"type": "equation", "text": "$$\nW=\\{\\tilde{\\pmb{w}}_{1,1},...,\\tilde{\\pmb{w}}_{1,C},...,\\tilde{\\pmb{w}}_{r,C}\\}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We can think of the resulting weight matrix $\\Tilde{W}$ as $r$ repetitions of the weight matrix $W_{\\mathrm{fc}}$ on which we add perturbation matrices $\\tilde{\\pmb{H}}_{i}$ . The $j$ -th row $\\tilde{\\pmb{H}}_{i,j}$ corresponds to a perturbation vector $\\tilde{\\pmb{\\eta}}_{j}$ , normalized to match the respective row ${\\pmb w}_{j}$ . ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\tilde{W}:=\\left[\\begin{array}{c}{\\tilde{W}_{1}}\\\\ {\\vdots}\\\\ {\\tilde{W}_{r}}\\end{array}\\right]=\\left[\\begin{array}{c}{W_{\\mathrm{fc}}+\\tilde{H}_{1}}\\\\ {\\vdots}\\\\ {W_{\\mathrm{fc}}+\\tilde{H}_{r}}\\end{array}\\right],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Since $\\tilde{\\pmb{W}}_{i}\\pmb{Z}=\\pmb{W}_{\\mathrm{fc}}\\pmb{Z}+\\pmb{H}_{i}\\pmb{Z},$ , we call $\\tilde{W}Z$ the perturbed logit space. Our weight perturbations method, we call WeiPer, essentially increases the output dimension of a model. Hence, it can be combined with many scoring functions. We demonstrate this with the two following postprocessors. ", "page_idx": 4}, {"type": "text", "text": "3.3 Baseline MSP scoring function ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "If the perturbations do not deviate too much from the class projections ${\\pmb w}_{j}$ , i.e. the row vectors of the final layer, the class cluster will still be separated from the other classes in the new projections and we can apply MSP on the perturbed logit space. In fact, we find that class clusters on the perturbed projections can be better distinguished from the OOD cluster than on the original class projection defined by $W_{\\mathrm{fc}}$ (see improvements of $W e i P e r{+}\\mathbf{M}\\mathbf{S}\\mathbf{P}(\\mathbf{\\emx})$ over MSP in Table 2). Figure 1 illustrates a visual example. We calculate the MSP on the perturbed logit space as ", "page_idx": 4}, {"type": "image", "img_path": "8HeUvbImKT/tmp/70c1927da552d6b602f06207f70cd70d952993d1e99c0f99f43f0ea402ae94d6.jpg", "img_caption": ["Figure 3: Histogram of all 512 activations in the penultimate layer (left pair) and the activations in WeiPer space (right pair) of a ResNet18 trained on CIFAR10. We perturb the weight matrix 100 times to produce a $10\\cdot100=1000.$ -dimensional perturbed logit space. For each pair, the left panel shows the mean distribution over all samples $\\mathrm{(ID=CIFAR10}$ , $\\mathrm{OOD}=\\mathrm{CIFAR100})$ . The right panels show the distribution $p_{z}$ and $p_{\\tilde{W}z}$ , respectively, for two randomly chosen samples with smoothing applied $\\langle s_{1}=s_{2}=2$ ) "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\nW e i P e r+\\mathrm{MSP}(\\pmb{x}):=\\mathrm{MSP}_{\\mathrm{WeiPer}}(\\pmb{x}):=\\frac{1}{r}\\sum_{i=1}^{r}\\mathrm{MSP}(\\tilde{W}_{i}z)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "the mean over all the maximum softmax predictions of the perturbed logits. We analyze why $\\mathrm{MSP}_{\\mathrm{WeiPer}}$ could be capable of capturing more of the penultimate layer distribution than MSP in Appendix A.1.2. ", "page_idx": 5}, {"type": "text", "text": "3.4 Our KL divergence score function ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Following our line of argument motivated by Theorem A.1, it seems natural to choose a density-based score function. When pooling all activations of the penultimate layer, an ID sample\u2019s activation distribution exhibits remarkable differences to that of an OOD sample. We observe the following properties: ", "page_idx": 5}, {"type": "text", "text": "\u2022 The majority of samples exhibit a bimodal distribution of their penultimate activations. An activation either belongs to the mode close to zero, or to the second mode (and rarely takes values in between).   \n\u2022 ID samples share a similar activation distribution. The mean activation distribution can serve as a prototype \u2013 see the upper left panel in Figure 3.   \n\u2022 The activation distribution is specific to the ID samples, i.e. the activation distribution of OOD samples differs from its distribution of ID samples and thus from the ID prototype. ", "page_idx": 5}, {"type": "text", "text": "Concluding on all three points, we make the assumption that all features $z=h(x)$ of an $\\mathrm{ID}$ input $x$ can be thought of as samples ", "page_idx": 5}, {"type": "equation", "text": "$$\nz_{1},...,z_{K}\\sim p_{z},\\mathrm{where}\\ (z_{1},...,z_{K})^{T}=z,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "of the same underlying activation distribution $p_{z}$ . Furthermore, the density of $p_{z}$ matches the mean distribution over all ID samples ", "page_idx": 5}, {"type": "equation", "text": "$$\np_{z}\\approx\\mathbb{E}_{z^{\\prime}\\in Z_{\\mathrm{train}}}[p_{z^{\\prime}}].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We assume, the same is true for the logits. They naturally separate into a non-class cluster and a class cluster with the ratio $1:C-1$ . Here, we could apply the same procedure, but especially for datasets with a small number of classes we would only get $C$ samples. This is where the cone of WeiPer vectors creates an advantage: They sit at a fixed angle to a class projection and thus preserve the class structure similarly across each projection onto one vector of the cone (e.g., like Figure 1 right - bottom panel). Analogous to Equation (9), we treat each projection ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\pmb{w}_{1,1}^{T}\\pmb{z},...,\\pmb{w}_{r,C}^{T}\\pmb{z}\\sim p_{\\tilde{W}z}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "as a sample of the same underlying distribution and observe that ", "page_idx": 5}, {"type": "equation", "text": "$$\np_{\\tilde{W}z}\\approx\\mathbb{E}_{z^{\\prime}\\in Z_{\\mathrm{train}}}[p_{\\tilde{W}z^{\\prime}}].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We demonstrate both behaviors in Figure 3. ", "page_idx": 6}, {"type": "text", "text": "In practice, we discretize $p_{z}$ and $p_{\\tilde{W}z}$ as histogram-based densities by splitting the value range into $n_{\\mathrm{bins}}$ bins (see Equation (21) in the Appendix). Compared to the mean distribution, $p_{z}$ and $p_{\\tilde{W}z}$ still have a sparse signal. We smoothen the densities with a function ", "page_idx": 6}, {"type": "equation", "text": "$$\nT_{k_{s}}(p(t)):={\\mathrm{normalize}}((p*k_{s})(t)+\\varepsilon)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "by convolving $p$ with a uniform kernel $k_{s}$ of size $s$ and prevent densities from being zero by adding $\\varepsilon>0$ which we set to the fixed values $\\varepsilon:=0.01$ for the penultimate layer and $\\varepsilon:=0.025$ for the WeiPer space. Note, that tuning both epsilons might increase performance as we observed in early stages of our experiments, but will add two additional hyperparameters. We normalize the density to sum up to one again, here defined by normalize. Afterwards, we compare each of the densities with the KL divergence, respectively: ", "page_idx": 6}, {"type": "equation", "text": "$$\nD_{\\mathrm{KL}}(\\pmb{x}\\mid q_{z},k_{s},\\varepsilon):=\\mathrm{KL}\\big(T_{k_{s}}(q_{z})\\parallel\\mathbb{E}_{\\pmb{z}\\in Z_{\\mathrm{ruin}}}[q_{z}]\\big)+\\mathrm{KL}\\big(\\mathbb{E}_{\\pmb{z}\\in Z_{\\mathrm{ruin}}}[q_{z}]\\parallel T_{k_{s}}(q_{z})\\big),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $q_{z}$ is either $p_{z}$ or $p_{\\tilde{W}z}$ . We discuss why our method does not suffer from the curse-ofdimensionality in contrast to other methods as investigated by Ghosal et al. (2023) in Appendix A.1.5 ", "page_idx": 6}, {"type": "text", "text": "$W e i P e r{+}\\mathrm{KLD}$ combines the KL divergence on the penultimate space, the KL divergence and MSP on the perturbed logit space into one final score: ", "page_idx": 6}, {"type": "equation", "text": "$$\nW e i P e r+\\mathrm{KLD}(x):=D_{\\mathrm{KL}}(x\\mid p_{z},s_{1})+\\lambda_{1}D_{\\mathrm{KL}}(x\\mid p_{\\tilde{W}z},s_{2})-\\lambda_{2}\\,\\mathrm{MSP}_{\\mathrm{Weiper}}(x)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The full list of hyperparameters is $r$ and $\\delta$ for the WeiPer application and $n_{\\mathrm{bins}},\\lambda_{1},\\lambda_{2},s_{1},s_{2}$ for the KL divergence score function. Figure 2 provides a visual explanation and a quick overview of WeiPer and both its postprocessors. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Setup. We evaluate WeiPer using the OpenOOD Zhang et al. (2023b) framework that includes three vision benchmarks: CIFAR10 Krizhevsky (2009), CIFAR100 Krizhevsky (2009), and ImageNet Deng et al. (2009). Each of them contains a respective ID dataset $\\mathcal{D}_{\\mathbf{in}}$ and several OOD datasets, subdivided into near datasets $\\mathcal{D}_{\\mathbf{near}}$ and far datasets $\\mathcal{D}_{\\mathbf{far}}$ (see Table 1). The terms near and far indicate their similarity to $\\mathcal{D}_{\\mathbf{in}}$ and, therefore, the difficulty of separating their samples. ", "page_idx": 6}, {"type": "text", "text": "OpenOOD also provides three model checkpoints trained on each CIFAR dataset whereas for ImageNet the methods are evaluated on a single official torchvision Marcel & Rodriguez (2010) checkpoint of ResNet50 He et al. (2016) and ViT-B/16 Dosovitskiy et al. (2020) respectively. We report our scores together with the results of Zhang et al. (2023b) in Table 2. ", "page_idx": 6}, {"type": "text", "text": "Due to resource constraints, we only evaluate our methods on the models trained with the standard preprocessor, that includes random cropping, horizontal flipping and normalizing, on the cross entropy objective. Additionally to the KL divergence score function and MSP, we evaluate WeiPer on ReAct. But instead of combining ReAct with the energy-based score function Liu et al. (2020) as in OpenOOD, we apply MSPWeiPer and call it WeiPer+ReAct. The hyperparameters of our methods were tuned by finding the best combination over a predefined and discrete range of values on the OpenOOD validation sets to assure a fair comparison to the competition (see Table 8). For ImageNet, results are based on a ", "page_idx": 6}, {"type": "text", "text": "subset of the training data, comprising 300,000 randomly selected, balanced samples (300 per class). For an analysis across different training set sizes, refer to Table 6. ", "page_idx": 6}, {"type": "table", "img_path": "8HeUvbImKT/tmp/20e3b75a42eb78b760e132a0aa2182a0cb6cd26e9d2bf210f6f92863c2a2f49f.jpg", "table_caption": ["Table 1: The individual benchmark datasets. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Metrics. We evaluate the methods with the Area Under the Receiver Operating characteristic Curve, AUROC, Bradley (1997) metric as a thresholdindependent score and the FPR95 as a quality metric. The FPR95 score reports the False Positive Rate at the True Positive Rate threshold $95\\%$ . ", "page_idx": 6}, {"type": "text", "text": "Table 2: OOD Detection results of top performing methods on the CIFAR10, CIFAR100 and ImageNet-1K benchmarks (For a comparison with every other evaluated method of OpenOOD and standard deviation over the CIFAR models, see Appendices A.5 and A.6). The top performing results for each benchmark are displayed in bold and we underline the second best result. Due to WeiPer\u2019s random nature, we report the median AUROC score over 10 different seeds. For an easy comparison, we portray the following ablations for CIFAR10 which are seperated by a line: The KLD results are the $W e i P e r{+}\\mathrm{KLD}$ results without MSP and RP is WeiPer+KLD with weight independent random projections drawn from a standard Gaussian. While WeiPer+KLD performs strongly especially on near datasets using ResNet backbones, its performance deteriorates with ViTs (see Section 4 for discussion). ", "page_idx": 7}, {"type": "table", "img_path": "8HeUvbImKT/tmp/75017a95be695f62ea5f9c665f79f8b910a2df10a821d764a2e923f1ea2e70aa.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Results. Table 2 reports the performance of WeiPer in comparison to the state-of-the-art OOD detectors on each benchmark. We compare our approach based on the $\\mathcal{D}_{\\mathbf{near}}$ and $\\mathcal{D}_{\\mathbf{far}}$ detection performances and report the mean over all datasets in each category. Table 3 portrays the mean relative performance on $\\mathcal{D}_{\\mathbf{near}}$ and $\\mathcal{D}_{\\mathbf{far}}$ of every postprocessor. The score is calculated as follows: ", "page_idx": 7}, {"type": "equation", "text": "$$\nS_{\\mathrm{rel}}(P):=\\frac{1}{3}\\big(A_{\\mathrm{CIFAR10}}(P)+A_{\\mathrm{CIFAR100}}(P)+\\frac{1}{2}(A_{\\mathrm{ImageNet(ResNet50)}}(P)+A_{\\mathrm{ImageNet(ViT)}}(P))\\big)\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where ", "page_idx": 7}, {"type": "equation", "text": "$$\nA_{\\mathcal{D}}(P):=\\frac{\\mathrm{AUROC}_{\\mathcal{D}_{\\mathrm{near/far}}}(P)}{\\operatorname*{max}_{P\\in\\mathcal{P}}\\mathrm{AUROC}_{\\mathcal{D}_{\\mathrm{near/far}}}(P)}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "It is designed such that each result on each dataset $\\mathcal{D}$ is equally weighted and scoring 1.0 means that the postprocessor $P$ is top performing across all datasets. ", "page_idx": 7}, {"type": "text", "text": "$W e i P e r{+}\\mathrm{KLD}$ achieves three out of eight top AUROC scores and the best performance on all near benchmarks, establishing a new state of the art performance by a significant margin (see Table 3). ", "page_idx": 7}, {"type": "text", "text": "Especially for the most challenging benchmark, separating $\\mathcal{D}_{\\mathbf{near}}$ on ImageNet with a ResNet50, we outperform our strongest competitor, ASH Djurisic et al. (2022), by $1.88\\%$ AUROC (we even achieve an AUROC score of 80.29 when using a 1M training samples instead of $300\\mathrm{k}$ , see Table 6). Additionally, $W e i P e r{+}\\mathrm{KLD}$ performs well on many far benchmarks, being the best method for ResNet50 on ImageNet, reaching into the top three positions on CIFAR10 far and into the top three on the CIFAR100 far benchmark. With its relative performance in Table 3, WeiPer $^+$ KLD reaches 3rd place overall in the far benchmark. ", "page_idx": 7}, {"type": "text", "text": "Only on Vit-B/16 trained on ImageNet, WeiPer+KLD shows a significant performance dent, especially on the far benchmark. ViT-B/16 uses a comparably narrow penultimate layer having fewer features than classes and therefore compresses the class clusters. Some dimensions may thus compress two classes while others represent a feature specific to only one class. This introduces more noise into $p_{z}$ which could impair the detection performance. Future experiments will reveal whether WeiPer benefits from higher dimensionalities of the latent space. ", "page_idx": 7}, {"type": "table", "img_path": "8HeUvbImKT/tmp/56d9617e8f221162fdc7288e06b679eae7a8b2db4f69c51ca1d0aaa631ee21b7.jpg", "table_caption": ["Table 3: Mean relative scores of all the postprocessors (post-hoc methods), see Equation (15). "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "WeiPer on existing methods. Additionally, WeiPer enhances the MSP performance by $1.4.1\\%$ AUROC across all benchmarks and WeiPer+ReAct consistently outperforms ReAct with an energybased score, although in their evaluation, this variant was better than $\\mathrm{ReAct+MSP}$ (see Table 3). ", "page_idx": 8}, {"type": "text", "text": "Ablation study. We determine the effect of each hyperparameter in Figure 4 by freezing single hyperparameters and optimizing only the one in question. As expected, increasing the number of random perturbations $r$ leads to a better median performance, while the standard deviation decreases for larger $r$ . Note, that it is possible to have better performance for lower $r$ by rerolling the weights a few times and choosing the best performing ones. All methods show a significant performance boost compared to using no perturbations $\\delta=0$ and seem to be best at $\\delta=2$ for WeiPer+KLD, which corresponds to an angle of $\\alpha\\approx63^{\\circ}$ and $\\delta=4$ $\\langle\\alpha\\approx76^{\\circ}]$ ) for MSP and ReAct. ", "page_idx": 8}, {"type": "text", "text": "On CIFAR10, WeiPer+KLD only improves marginally by applying $\\mathrm{MSP}_{\\mathrm{WeiPer}}$ , which is not the case for the other benchmarks (see Table 7), where $\\lambda_{2}>0$ . Furthermore, we study the performance of random projections that are independent from the weights $W_{\\mathrm{fc}}$ . We show that using only random projections (RP, see Table 2) without adding $\\mathrm{MSP}_{\\mathrm{WeiPer}}$ , we are hardly able to detect any OOD samples. This supports the claim that utilizing the class directions is necessary. The supplementary material presents all the other KLD-specific hyperparameters and we also investigate their influences to the performance in Figure 6. We outline the selected parameters for each benchmark in Table 7. ", "page_idx": 8}, {"type": "image", "img_path": "8HeUvbImKT/tmp/dff9b079a37a1d43db552f3ac16e71bc713898fbd4c6c10cb572e1ca489edb52.jpg", "img_caption": ["Figure 4: We investigate the effect of WeiPer hyperparameters $r$ and $\\delta$ on the performance of the three postprocessors. The left pair shows results on CIFAR10, the right pair corresponds to ImageNet (using ResNet18 for both). Models were tested using their respective near OOD datasets. The panels corresponding to $\\delta$ depict AUROC performance minus the initial AUROC performance at $\\delta=0$ . The graphs show the mean over 25 runs and the shaded area around them represents the value range (min to max) over those runs. All other parameters of the methods were fixed to the optimal setting. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "5 Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "$W e i P e r{+}\\mathrm{KLD}$ has more hyperparameters than other competitors: 6 in total. As discussed in the previous section, $r$ can be seen as a memory / performance trade-off (see Figure 4). In the supplementary material (see Figure 6) we investigate the other parameters and find that they all have only one local maximum in the range we were searching and should thus be easy to optimize. We tried to choose the same smoothing size $s_{1}=s_{2}$ for both densities, but the ablations show that both are optimal at different sizes. While $\\mathrm{MSP}_{\\mathrm{WeiPer}}$ is not really used for CIFAR10 $\\lambda_{2}\\approx0)$ it is beneficial for CIFAR100 and ImageNet. As WeiPer blows up the dimension we also conduct a memory and time comparison to other methods in Table 4 and Table 5. We demonstrate that with a combination of a confidence and a distance based metric it is possible to achieve competitive near results across the board where all other methods seem to deteriorate in at least one benchmark. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We show that multiple random perturbations of the class projections in the final layer of the network can provide additional information that we can exploit for detecting out-of-distribution samples. WeiPer creates a representation of the data by projecting the latent activation of a sample onto vector bundles around the class-specific weight vectors of the final layer. We then employ a new approach to construct a score allowing the subsequent separation of ID and OOD data. It relies on the fingerprintlike nature of features of the penultimate and the WeiPer-representations by assuming they were sampled by the same underlying distribution. In a thorough evaluation, we first show that WeiPer enhances MSP and ReAct+MSP performance significantly and show that WeiPer+KLD achieves top scores in most benchmarks, representing the new state-of-the-art solution in post-hoc OOD methods on near benchmarks. ", "page_idx": 9}, {"type": "text", "text": "7 Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We appreciate the reviewers\u2019 comments, which greatly helped enhance this manuscript and inspired us to conduct additional key experiments. Maximilian Granz was supported by the Elsa-NeumannScholarship from the state of Berlin, which provided essential funding for the initial stages of this research. We also thank Leon Sixt and Manolis Panagiotou for their feedback throughout the project. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Ahn, Y. H., Park, G.-M., and Kim, S. T. Line: Out-of-distribution detection by leveraging important neurons. 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 19852\u201319862, 2023. URL https://api.semanticscholar.org/CorpusID:257757417. ", "page_idx": 9}, {"type": "text", "text": "Amodei, D., Olah, C., Steinhardt, J., Christiano, P. F., Schulman, J., and Man\u00e9, D. Concrete problems in ai safety. ArXiv, abs/1606.06565, 2016. URL https://api.semanticscholar. org/CorpusID:10242377. ", "page_idx": 9}, {"type": "text", "text": "Bendale, A. and Boult, T. E. Towards open set deep networks. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1563\u20131572, 2015. URL https://api. semanticscholar.org/CorpusID:14240373. ", "page_idx": 9}, {"type": "text", "text": "Bonneel, N., Rabin, J., Peyr\u00e9, G., and Pfister, H. Sliced and radon wasserstein barycenters of measures. Journal of Mathematical Imaging and Vision, 51:22 \u2013 45, 2014. URL https://api. semanticscholar.org/CorpusID:1907942. ", "page_idx": 9}, {"type": "text", "text": "Bradley, A. P. The use of the area under the roc curve in the evaluation of machine learning algorithms. Pattern Recognit., 30:1145\u20131159, 1997. URL https://api.semanticscholar. org/CorpusID:13806304. ", "page_idx": 9}, {"type": "text", "text": "Cheney, N., Schrimpf, M., and Kreiman, G. On the robustness of convolutional neural networks to internal architecture and weight perturbations. ArXiv, abs/1703.08245, 2017. URL https: //api.semanticscholar.org/CorpusID:13217484. ", "page_idx": 9}, {"type": "text", "text": "Cram\u00e9r, H. and Wold, H. Some theorems on distribution functions. Journal of the London Mathematical Society, s1-11(4):290\u2013294, 1936. doi: https://doi.org/10.1112/jlms/s1-11.4.290. URL https: //londmathsoc.onlinelibrary.wiley.com/doi/abs/10.1112/jlms/s1-11.4.290.   \nCuesta-Albertos, J., Fraiman, R., and Ransford, T. A sharp form of the cram\u00e9r\u2013wold theorem. Journal of Theoretical Probability, 20:201\u2013209, 06 2007. doi: 10.1007/s10959-007-0060-7.   \nDarrin, M., Piantanida, P., and Colombo, P. Rainproof: An umbrella to shield text generators from out-of-distribution data. arXiv preprint arXiv:2212.09171, 2022.   \nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp. 248\u2013255, 2009. doi: 10.1109/CVPR.2009.5206848.   \nDjurisic, A., Bozanic, N., Ashok, A., and Liu, R. Extremely simple activation shaping for out-ofdistribution detection. ArXiv, abs/2209.09858, 2022. URL https://api.semanticscholar. org/CorpusID:252383259.   \nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. arXiv e-prints, art. arXiv:2010.11929, October 2020. doi: 10.48550/arXiv.2010.11929.   \nGhosal, S. S., Sun, Y., and Li, Y. How to overcome curse-of-dimensionality for out-of-distribution detection?, 2023.   \nGuo, C., Pleiss, G., Sun, Y., and Weinberger, K. Q. On calibration of modern neural networks. In Precup, D. and Teh, Y. W. (eds.), Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 1321\u20131330. PMLR, 06\u201311 Aug 2017. URL https://proceedings.mlr.press/v70/guo17a.html.   \nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770\u2013778, 2016. doi: 10.1109/CVPR.2016.90.   \nHendrycks, D. and Gimpel, K. A baseline for detecting misclassified and out-of-distribution examples in neural networks. ArXiv, abs/1610.02136, 2016. URL https://api.semanticscholar.org/ CorpusID:13046179.   \nHendrycks, D., Basart, S., Mazeika, M., Mostajabi, M., Steinhardt, J., and Song, D. X. Scaling out-of-distribution detection for real-world settings. In International Conference on Machine Learning, 2022a. URL https://api.semanticscholar.org/CorpusID:227407829.   \nHendrycks, D., Basart, S., Mazeika, M., Zou, A., Kwon, J., Mostajabi, M., Steinhardt, J., and Song, D. Scaling out-of-distribution detection for real-world settings. In Chaudhuri, K., Jegelka, S., Song, L., Szepesvari, C., Niu, G., and Sabato, S. (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 8759\u20138773. PMLR, 17\u201323 Jul 2022b. URL https://proceedings.mlr.press/v162/ hendrycks22a.html.   \nJerome H. Friedman, W. S. and Schroeder, A. Projection pursuit density estimation. Journal of the American Statistical Association, 79(387):599\u2013608, 1984. doi: 10.1080/01621459.1984.10478086. URL https://www.tandfonline.com/doi/abs/10.1080/01621459.1984.10478086.   \nKhan, M. E., Nielsen, D., Tangkaratt, V., Lin, W., Gal, Y., and Srivastava, A. Fast and scalable bayesian deep learning by weight-perturbation in adam. In International Conference on Machine Learning, 2018. URL https://api.semanticscholar.org/CorpusID:49187225.   \nKolouri, S., Zou, Y., and Rohde, G. K. Sliced wasserstein kernels for probability distributions. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 5258\u20135267, 2016. doi: 10.1109/CVPR.2016.568.   \nKrizhevsky, A. Learning multiple layers of features from tiny images. pp. 32\u201333, 2009. URL https://www.cs.toronto.edu/\\~kriz/learning-features-2009-TR.pdf.   \nKuan, J.-L. and Mueller, J. W. Back to the basics: Revisiting out-of-distribution detection baselines. ArXiv, abs/2207.03061, 2022. URL https://api.semanticscholar.org/CorpusID: 250334470.   \nLee, K., Lee, K., Lee, H., and Shin, J. A simple unified framework for detecting out-ofdistribution samples and adversarial attacks. ArXiv, abs/1807.03888, 2018. URL https: //api.semanticscholar.org/CorpusID:49667948.   \nLiu, W., Wang, X., Owens, J. D., and Li, Y. Energy-based out-of-distribution detection. ArXiv, abs/2010.03759, 2020. URL https://api.semanticscholar.org/CorpusID:222208700.   \nLiu, X., Lochman, Y., and Chrsitopher, Z. Gen: Pushing the limits of softmax-based out-ofdistribution detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023a.   \nLiu, Y., Tian, C. X., Li, H., Ma, L., and Wang, S. Neuron activation coverage: Rethinking out-ofdistribution detection and generalization. ArXiv, abs/2306.02879, 2023b. URL https://api. semanticscholar.org/CorpusID:259075869.   \nLiutkus, A., Simsekli, U., Majewski, S., Durmus, A., and St\u00f6ter, F.-R. Sliced-Wasserstein flows: Nonparametric generative modeling via optimal transport and diffusions. In Chaudhuri, K. and Salakhutdinov, R. (eds.), Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 4104\u20134113. PMLR, 09\u201315 Jun 2019. URL https://proceedings.mlr.press/v97/liutkus19a.html.   \nMarcel, S. and Rodriguez, Y. Torchvision the machine-vision package of torch. In Proceedings of the 18th ACM International Conference on Multimedia, MM \u201910, pp. 1485\u20131488, New York, NY, USA, 2010. Association for Computing Machinery. ISBN 9781605589336. doi: 10.1145/1873951. 1874254. URL https://doi.org/10.1145/1873951.1874254.   \nNguyen, K., Ho, N., Pham, T., and Bui, H. Distributional sliced-wasserstein and applications to generative modeling. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id $\\equiv$ QYjO70ACDK.   \nPapyan, V., Han, X., and Donoho, D. L. Prevalence of neural collapse during the terminal phase of deep learning training. Proceedings of the National Academy of Sciences of the United States of America, 117:24652 \u2013 24663, 2020. URL https://api.semanticscholar.org/CorpusID: 221172897.   \nPark, J., Jung, Y. G., and Teoh, A. B. J. Nearest neighbor guidance for out-of-distribution detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1686\u20131695, 2023.   \nPaty, F.-P. and Cuturi, M. Subspace robust Wasserstein distances. In Chaudhuri, K. and Salakhutdinov, R. (eds.), Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 5072\u20135081. PMLR, 09\u201315 Jun 2019. URL https://proceedings.mlr.press/v97/paty19a.html.   \nPicot, M., Noiry, N., Piantanida, P., and Colombo, P. Adversarial attack detection under realistic constraints. 2022.   \nRakin, A. S., He, Z., and Fan, D. Parametric noise injection: Trainable randomness to improve deep neural network robustness against adversarial attack. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 588\u2013597, 2018. URL https: //api.semanticscholar.org/CorpusID:53716271.   \nRen, J. J., Fort, S., Liu, J. Z., Roy, A. G., Padhy, S., and Lakshminarayanan, B. A simple fix to mahalanobis distance for improving near-ood detection. ArXiv, abs/2106.09022, 2021. URL https://api.semanticscholar.org/CorpusID:235458597.   \nSastry, C. S. and Oore, S. Detecting out-of-distribution examples with Gram matrices. In III, H. D. and Singh, A. (eds.), Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 8491\u20138501. PMLR, 13\u201318 Jul 2020. URL https://proceedings.mlr.press/v119/sastry20a.html.   \nSun, Y. and Li, Y. Dice: Leveraging sparsification for out-of-distribution detection. 2021. URL https://api.semanticscholar.org/CorpusID:250626952.   \nSun, Y., Guo, C., and Li, Y. React: Out-of-distribution detection with rectified activations. In Neural Information Processing Systems, 2021. URL https://api.semanticscholar.org/CorpusID: 244709089.   \nSun, Y., Ming, Y., Zhu, X., and Li, Y. Out-of-distribution detection with deep nearest neighbors. In International Conference on Machine Learning, 2022. URL https://api.semanticscholar. org/CorpusID:248157551.   \nTajwar, F., Kumar, A., Xie, S. M., and Liang, P. No true state-of-the-art? ood detection methods are inconsistent across datasets. ArXiv, abs/2109.05554, 2021. URL https://api.semanticscholar. org/CorpusID:237264010.   \nWang, H., Li, Z., Feng, L., and Zhang, W. Vim: Out-of-distribution with virtual-logit matching. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4911\u20134920, 2022. URL https://api.semanticscholar.org/CorpusID:247595202.   \nWen, Y., Vicol, P., Ba, J., Tran, D., and Grosse, R. B. Flipout: Efficient pseudo-independent weight perturbations on mini-batches. ArXiv, abs/1803.04386, 2018. URL https://api. semanticscholar.org/CorpusID:3861760.   \nWu, D., Wang, Y., and Xia, S. Revisiting loss landscape for adversarial robustness. ArXiv, abs/2004.05884, 2020. URL https://api.semanticscholar.org/CorpusID:215744953.   \nXiang, L., Zeng, X., Niu, Y., and Liu, Y. Study of sensitivity to weight perturbation for convolution neural network. IEEE Access, PP:1\u20131, 07 2019. doi: 10.1109/ACCESS.2019.2926768.   \nYang, J., Wang, P., Zou, D., Zhou, Z., Ding, K., Peng, W., Wang, H., Chen, G., Li, B., Sun, Y., Du, X., Zhou, K., Zhang, W., Hendrycks, D., Li, Y., and Liu, Z. OpenOOD: Benchmarking generalized out-of-distribution detection. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022. URL https://openreview.net/forum?id=gT6j4_ tskUt.   \nZhang, J., Fu, Q., Chen, X., Du, L., Li, Z., Wang, G., xiaoguang Liu, Han, S., and Zhang, D. Out-of-distribution detection based on in-distribution data patterns memorization with modern hopfield energy. 2023a. URL https://openreview.net/forum?id $\\cdot$ KkazG4lgKL.   \nZhang, J., Yang, J., Wang, P., Wang, H., Lin, Y., Zhang, H., Sun, Y., Du, X., Zhou, K., Zhang, W., Li, Y., Liu, Z., Chen, Y., and Hai, L. Openood v1.5: Enhanced benchmark for out-of-distribution detection. arXiv preprint arXiv:2306.09301, 2023b. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 WeiPer: Theoretical details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1.1 Weight perturbations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Theorem A.1 (Cuesta-Albertos et al. (2007)). Let $X$ and $Y$ be two $\\mathbb{R}^{K}$ -valued random vectors. Suppose the absolute moments $m_{k}:=\\mathbb{E}(\\|X\\|^{k})$ are finite and $\\begin{array}{r}{\\sum_{k=1}^{\\infty}(m_{k})^{-1/k}=\\infty.}\\end{array}$ . If the set $W_{X Y}=\\{{\\pmb w}\\in\\mathbb{R}^{K}:{\\pmb w}^{T}X\\triangleq{\\pmb w}^{T}Y\\}$ has positive Lebesgue measure, then $X\\overset{d}{=}Y$ . ", "page_idx": 13}, {"type": "text", "text": "We provide a simple proof for the case that $W_{X Y}\\,=\\,\\mathbb{R}^{K}$ . For the complete proof, we refer to Cuesta-Albertos et al. (2007). ", "page_idx": 13}, {"type": "text", "text": "Proof. The characteristic function of a random vector $X$ is defined as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\phi_{X}(\\pmb{w}):=\\int e^{i\\pmb{w}^{T}\\pmb{x}}d X\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "By the uniqueness theorem, every random vector $X$ has a unique characteristic funtion $\\phi_{X}$ . If the assumption in the theorem holds, then for all realizations $\\pmb{x}=X(\\omega)$ and $\\pmb{y}=Y(\\omega)$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n{\\pmb w}^{T}{\\pmb x}={\\pmb w}^{T}{\\pmb y}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "and thus $\\phi_{X}=\\phi_{Y}$ . Therefore we have $X=Y$ by the uniqueness. ", "page_idx": 13}, {"type": "text", "text": "Note, that is enough cover all the directions in $\\mathbb{R}^{K}$ instead of covering the whole space with the set of projections $W$ . Since if $t w\\notin W$ for $t\\in\\mathbb R$ , but $w\\in W\\cap W_{X Y}$ then $t w\\in W_{X Y}$ . For $\\delta>0$ our set of perturbed class projections indeed covers the all directions if $r\\rightarrow\\infty$ . ", "page_idx": 13}, {"type": "text", "text": "To generally apply the theorem, $Z$ must be defined on a bounded set with finite measure (Hausdorff moment problem), which is true for virtually all practical problems. More importantly, $W$ needs to be a $K$ -dimensional subset of $\\mathbb{R}^{K}$ . Note that the theorem also applies for a set of weight matrices ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\left\\{W\\in\\mathbb{R}^{K\\times K}:W X\\overset{d}{=}W Y\\right\\}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "when their row vectors form a $K$ dimensional set as their marginal distributions ${\\pmb w}_{i}^{T}X\\triangleq{\\pmb w}_{i}^{T}Y$ would be equal for $i=1,...,K$ . We are using this theorem solely as motivation since it is not possible to draw direct implications. However, with a score function $S$ we are measuring properties of the logit distribution of the training data $\\dot{W}_{\\mathrm{fc}}Z_{\\mathrm{train}}\\,=\\,(w_{1}^{T}Z_{\\mathrm{train}},...,w_{C}^{T}Z_{\\mathrm{train}})$ and check if they match the properties of some unknown logit distribution $W_{\\mathrm{{fc}}}Z$ that might be test data or OOD data. In the ideal case the logits match in distribution ", "page_idx": 13}, {"type": "equation", "text": "$$\nW_{\\mathrm{fc}}Z_{\\mathrm{train}}\\overset{d}{=}W_{\\mathrm{fc}}Z{\\mathrm{~if~and~only~if~}}S{\\mathrm{~is~maximized}},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "e.g. if $D$ is a distance and $S=-D$ , $S(\\pmb{W}_{\\mathrm{fc}}Z_{\\mathrm{test}})\\,=\\,0$ and $S(W_{\\mathrm{fc}}Z_{\\mathrm{ood}})\\mathrm{~<~}0$ . Now the theorem says that if we chose a $K$ -dimensional set $W$ of projections and we had a score function $S_{\\mathrm{{WeiPer}}}$ that fulflils the property of Equation (20) on the infinite dimensional space spanned by the projections of $W$ , not just the distributions on the projection would be equal when $S_{\\mathrm{WeiPer}}$ is minimized but also the penultimate distributions $Z_{\\mathrm{train}}\\,{\\overset{d}{=}}\\,Z_{\\mathrm{test}}^{-}$ . ", "page_idx": 13}, {"type": "text", "text": "A.1.2 MSP on the perturbed logit space ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Continuing from the previous section, $S_{\\mathrm{WeiPer}}~=~\\mathrm{MSP}_{\\mathrm{WeiPer}}$ is a score function on the infinite dimensional space spanned by $W$ for $r\\rightarrow\\infty$ , so ideally $\\mathrm{MSP}_{\\mathrm{WeiPer}}(Z_{\\mathrm{test}})=0$ then not only the logit distributions match $W_{\\mathrm{fc}}Z_{\\mathrm{train}}\\overset{d}{=}W_{\\mathrm{fc}}Z_{\\mathrm{test}}$ , but also their penultimate distributions $Z_{\\mathrm{train}}\\triangleq Z_{\\mathrm{test}}$ which would make $\\mathrm{MSP}_{\\mathrm{WeiPer}}$ a stronger metric than MSP. ", "page_idx": 13}, {"type": "text", "text": "A.1.3 PCA on the penultimate space ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We draw the following conclusions from Figure 5: The OOD and ID distributions differ much stronger along the first $C$ principal components, and they are more similar for the other components. This indicates, most of the signal may lie in the $C$ -dimensional subspace. ", "page_idx": 13}, {"type": "image", "img_path": "8HeUvbImKT/tmp/2fc1d7d2fb946344419c4e97aea603bc3d1675205036bbc763fb7ea036eb7de1.jpg", "img_caption": ["Figure 5: We applied PCA to $Z_{\\mathrm{Train}}$ of CIFAR10 and projected $Z_{\\mathrm{train}}$ (blue), $Z_{\\mathrm{test}}$ (purple) and $Z_{\\mathrm{ood}}$ (red, CIFAR100) to the first 20 principal components. We observe density spikes in the first 10 dimensions, likely corresponding to the class clusters. The dimensions 10-19 exhibit less structure as their densities appear to be Gaussian. Along these directions the ID and OOD data are more similar compared to the first ten principal components. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "We argue that instead of taking random projections, we can utilize the class projections. Since we have a trained classifier at hand, it is likely that the informative dimensions are: $\\mathrm{span}(\\pmb{w}_{1},...,\\pmb{w}_{C})$ , the span of the row vectors of $W_{\\mathrm{fc}}$ . Hence, a better choice than Gaussian vectors for the set of projections $W$ are vectors $\\mathbf{\\nabla}w$ that correlate with these basis vectors in $W_{\\mathrm{fc}}$ but at the same time deviate enough such that we obtain new information from projections onto them. ", "page_idx": 14}, {"type": "text", "text": "A.1.4 KL divergence: Density definition ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We gave a brief description of our construction of the densities in the main paper. The formal definition is: ", "page_idx": 14}, {"type": "equation", "text": "$$\np_{z}(t):=\\frac{1}{K l_{b}}\\sum_{i=1}^{K}[z_{i}\\in b(t)].\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Here $l_{b}$ is the bin length, $b(t)$ is the bin range in which $t$ falls, and $[.]$ is the Iverson bracket which evaluates to one if true or zero if the statement is false. Note that we are dividing by $l_{b}$ such that the density integrates to one. This is the usual definition for descretizing a density into equal sized bins. ", "page_idx": 14}, {"type": "text", "text": "A.1.5 Curse of dimensionality ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In contrast to other distance-based methods, our KL divergence score does not suffer from the curse-of-dimensionality, which deteriorates the performance of methods like KNN Sun et al. (2022) as investigated by Ghosal et al. (2023). We disregard the dimension and only consider the activations in the penultimate space or in the perturbed logit space. In our case, more dimensions means more samples to approximate the activation distribution $p_{z}$ . We believe that our method thrives when applied to networks with higher dimensional penultimate space, but this still needs to be evaluated in future experiments. However, in the perturbed logit space, we can control the size of the space with $r$ (see Figure 4). Our ablation results show that increasing $r$ and thus blowing up the dimensionality only increases performance. ", "page_idx": 14}, {"type": "text", "text": "Table 4: Time taken in seconds to setup the method or for inference. More precisely, we measure the time of postprocessor.setup() and postprocessor.inference() in OpenOODs evaluator.py . We take the mean over 20 iterations and denote the standard deviation after the $\\pm$ sign. We mark the maximal time in bold and underline the second longest time taken. We compare WeiPer+KLD $r=100$ as in the paper) to its closest competitors and show that it is competitive with other methods in terms of computation time. While WeiPer $^+$ KLD is on the higher end of the spectrum in terms of computation time, it remains comparable to other methods. It\u2019s worth noting that computation time can be adjusted by trading off performance with lower $r$ values. ", "page_idx": 15}, {"type": "table", "img_path": "8HeUvbImKT/tmp/c5f9d32fc6fe383d59abd12c8708e9b74fd2fa15f280b7f5f4215c9690afea28.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Table 5: Memory consumption in MiB to setup the method or for inference. More precisely, we compare the memory of postprocessor.setup() and postprocessor.inference() in OpenOODs evaluator.py before and after its execution with psutil . We take the mean over 20 iterations of the data between the $20\\%$ and $80\\%$ quantile to diminish the effect of outliers (e.g., caused by interfering processes) and denote the standard deviation after the $\\pm$ sign. We mark the maximal memory in bold and underline the second highest demand. WeiPer $^+$ KLDs $(r\\,=\\,100)$ memory consumption for its setup is among the lower demanding methods while it has a comparably high demand for inference. Note that for optimal results we choose $r=100$ , but smaller values of $r$ also provide competitive results (see Figure 4). Thus memory can be traded against performance where resources are constraint. ", "page_idx": 15}, {"type": "table", "img_path": "8HeUvbImKT/tmp/2bd4a702ab749365ef792e5ad11ca46ce609bada15a7f68cb7257d7350502f28.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Table 6: AUROC results on ImageNet with ResNet50 on the near and far benchmark with different training set sizes. Each split is a random sample of the data set with each class appearing exactly as often as each other class. We chose the optimal set of hyperparameters on ImageNet, but reduced the number of repeats $r$ to 50 instead of 100. ", "page_idx": 15}, {"type": "table", "img_path": "8HeUvbImKT/tmp/45d475714854f124920a69731c9cd70a0f0cd54837f45edc712d00a4f071c75d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "8HeUvbImKT/tmp/7357c5880590f22df222c7d8f77492e62284c5979baf6407107375d0dc0f4b68.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Figure 6: KLD specific hyperparamters: We fixed the optimal hyperparameters and varied the one parameter in question by conducting 10 runs over the same fixed parameter setting on CIFAR10 and ImageNet as ID against their near OOD datasets. We report the mean and the minimum to maximum range (transparent). We set $r=5$ instead of $r=100$ for ImageNet to save resources. Thus the noise on the results is stronger for the ImageNet ablations. All of the parameters except the kernel sizes only have a single local maximum which indicates that they should be easy to optimize. The most important parameters seem to be the kernel sizes $s_{1}$ and $s_{2}$ that we use for smoothing followed by $n_{\\mathrm{bins}}$ . Note that $s_{1}$ and $s_{2}$ have a different optimum, which means it is not possible to simply choose $s_{1}=s_{2}$ and reduce the count of hyperparameters. $\\lambda_{1}=0$ is the score function without the KLD specific WeiPer application. $\\lambda_{2}$ is the application of MSPWeiPer which is not beneficial for CIFAR10, but shows to be effective on ImageNet. ", "page_idx": 16}, {"type": "table", "img_path": "8HeUvbImKT/tmp/7d925173e54824b6c014537580fe14eaa89d9fa457dee013290238ca9db2296a.jpg", "table_caption": ["Table 7: The hyperparameter sets for each experiment. The number of repeats $r$ was predefined since we found increasing it always boosts the performance at the cost of time and memory consumption. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Table 8: Set of values for the hyperparameter search. ", "page_idx": 17}, {"type": "table", "img_path": "8HeUvbImKT/tmp/6fa8fbc32be5699a06040190af6c601d1ec288ecf94ec3ef17e19ba6e719285e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "A.4 Penultimate layer distribution ", "text_level": 1, "page_idx": 17}, {"type": "image", "img_path": "8HeUvbImKT/tmp/265cfbbccf46c299221685b2fb548185d38e12b900940266af6ee1fe3ce74c2e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Figure 7: Density plots for CIFAR10 and CIFAR100 of a ResNet18 trained for 300 epochs. We present the densities as in Figure 3, but this time we show it for more datasets and for two different samples $z_{1},z_{2}$ in the penultimate and the perturbed logit space, respectively. The OOD set for the ID set CIFAR10 is CIFAR100 and vice versa for CIFAR100. The range of the $y$ -axis is adjusted such that the differences between ID and OOD become visible. We therefore report the mean over the maximum density ${\\mathrm{max}}_{p}$ of the penultimate dimensions to show, up to which value the maximum would go. We apply smoothing over $k_{s}$ neighbors in each plot and construct the histograms with $n_{\\mathrm{bins}}$ bins. We report the parameters in Table 9. The class clusters and the activation clusters are clearly visible for CIFAR10 and merge into the bigger cluster for CIFAR100, probably because of the lower class to non-class ratio. It is harder to see for CIFAR100, but for both datasets, it seems harder for the OOD data to sample in the class cluster or the activated feature cluster. ", "page_idx": 17}, {"type": "image", "img_path": "8HeUvbImKT/tmp/b18ae82dfe34a999f35605ea5cb272c88e951ef6bb4373a9fb8d2ebfe889449b.jpg", "img_caption": ["Figure 8: Density plots for ImageNet (ResNet50 and ViT-B/16). We chose SSB-hard as OOD set and apply the same plotting procedure as defined for CIFAR10/CIFAR100. For the respective plotting parameters, see Table 9. For ViT-B/16, the class clusters are distinguishable from the non-class clusters but not for ResNet50. Still, the difference between ID and OOD is captured in the higher activations which could explain why activation shaping Djurisic et al. (2022); Sun et al. (2021) works well for ImageNet. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Table 9: Plotting parameters: $s$ is the kernel size for the uniform kernel that was used for smoothing. and $\\begin{array}{r}{\\operatorname*{max}_{p}\\,=\\,\\operatorname*{max}_{t}\\mathbb{E}_{z\\in Z_{\\mathrm{train}}}[p_{z}](t)}\\end{array}$ denotes the maximum of the mean density of the penultimate densities $p_{t}$ . The perturbed densities $p_{\\tilde{W}z}$ are scaled similarly. ", "page_idx": 18}, {"type": "table", "img_path": "8HeUvbImKT/tmp/139b2328bfcfc1d3de78fb4f8006fc35185bc1e51a7629391217ffa372a9fa2c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "A.5 Full CIFAR results ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Table 10: Full CIFAR10 postprocessor results on the three ResNet18 checkpoints provided by OpenOOD trained with Cross Entropy and standard preprocessing. The $\\pm$ indicates the standard deviation of all methods over three different model checkpoints. ", "page_idx": 19}, {"type": "table", "img_path": "8HeUvbImKT/tmp/e0c0eae01d1c5263b7bc86f461554f34da1ceaa7fd66925fb6e7a76051822e2e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "8HeUvbImKT/tmp/524f58789c5cc3a3a964694f79b4a7731725ac0b901b6f07b20af0e5a9810346.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "8HeUvbImKT/tmp/bb15325bea62dc89600a0fc4baf026fd5730fa2e6ce543d4c625b038a303a301.jpg", "table_caption": ["Table 11: Full CIFAR100 postprocessor results on the three ResNet18 checkpoints provided by OpenOOD trained with Cross Entropy and standard preprocessing. "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "8HeUvbImKT/tmp/4344ea2804fc61434d80d76d8b87123a2810b72086c6747809d1c9c5d05c2616.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "A.6 Full ImageNet results ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Table 12: Full ImageNet postprocessor results on ResNet50 trained with Cross Entropy and standard preprocessing. We achieve three out of five best AUROC performances outperforming the competition. ", "page_idx": 21}, {"type": "table", "img_path": "8HeUvbImKT/tmp/51950bfb16351ffcb969f889f4f23197f27844032de1ee34e7761dd0640016f9.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "8HeUvbImKT/tmp/06718811cc950fce1df2e12a38704fa9bcaf7ff65ff7bb78c0b0a71e6395bdac.jpg", "table_caption": ["Table 13: Full ImageNet postprocessor results on ViT/16-B trained with Cross Entropy and standard preprocessing. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "A.7 Compute resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "All experiments are conducted on a local machine with the following key specifications: AMD EPYC 7543 (32-Core Processor) with 256GB RAM and 1x NVIDIA RTX A5000 (24GB VRAM). To streamline the experimental process, we pre-compute the penultimate output of each backbone model and dataset combination. This is possible as we do not alter the training objective for our evaluation, achieving a reduction of disk usage to $<\\!2.8\\mathrm{GB}$ when stored as FP16 tensors compared to sum of the original dataset sizes of CIFAR10, CIFAR100 and ImageNet-1K. ", "page_idx": 22}, {"type": "text", "text": "For all benchmarks, the 24GB VRAM suffices for both, the model inference and postprocessor optimization. Depending on the chosen batch size, this offers a runtime / VRAM trade-off and is therefore well achievable on smaller GPUs. ", "page_idx": 22}, {"type": "text", "text": "Excluding the inference step for the penultimate output, we report an inference time for the postprocessor optimization of 18 seconds for CIFAR10 and ${<}10$ minutes for ImageNet-1K per iteration. An iteration refers to processing the full dataset starting from the penultimate layer output. For the full duration without pre-processing, one would add the inference time of the respective ResNet[18/50] or ViT/16-B model. ", "page_idx": 22}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The abstract introduces the key contribution WeiPer, the KL-Divergence-based scoring function and the setting in which it outperforms its competition. Furthermore, our introduction briefly presents our methods mechanism and lists our contributions and mentions its limitations. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 23}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: In the Limitations section, we address two points of critique that we feel are most relevant: the number of hyperparameters and the memory consumption (also analyzed in Table 5). In our methods section, we clearly state our assumptions which are be justified by observations (e.g. in the figures) and by the empirical results. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 23}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Our paper only includes one theoretical result (Theorem A.1) which is proofed in Cuesta-Albertos et al. (2007). However, we provide a proof for our case. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 24}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: With the use of the standardized OpenOOD benchmark, the results of our paper are well reproducible. We provide details on the hyperparameter choices in Table 7 and Table 8 and the exact code to reproduce the results with the supplementary material. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 24}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 25}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We provide our implementation in the supplementary material. It is integrated in the OpenOOD framework, which makes it easy to setup. All required packages will be installed when setting up OpenOOD. The benchmark datasets are publicly available. When published, we will release a public GitHub repository with our implementation. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 25}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We describe the training details of the used baseline models and their checkpoints in the Experiments section. With our method functioning in a post-hoc fashion, these training checkpoints remain unchanged. The optimization details for our postprocessor are also located in section 4. For the specific hyperparameter ranges and found configurations see Table 7 and Table 8. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 25}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Due to WeiPers random nature our results exhibit noise dependent on the hyperparameter $r$ and we display the minimum to maximum range in Figure 4 and Figure 6. We chose minimum to maximum results instead of standard deviation as the maximal performing perturbations could be strategically sampled. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 26}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We report the memory usage / performance trade-off in Figure 4. In Appendix A.7, we report on specifications of our machine, GPU usage and execution times. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 26}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Our research does not involve human subjects or participants. The datasets conform with the NeurIPS Code of Ethics, specifically, the listed concerns. Our work does not introduce new data. ", "page_idx": 26}, {"type": "text", "text": "Our work aims to contribute to safety in AI. It proposed a method that is applicable to various applications that utilize Machine Learning, but is neither obstructing nor contributing to areas with a larger societal impact. ", "page_idx": 26}, {"type": "text", "text": "Finally, our research conforms with the NeurIPS Code of Conduct. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 27}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: We classify our contribution as foundational research, as it is a tool to improve Machine Learning models in various use cases. Hence, a specific assessment of the societal impact is difficult. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 27}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 27}, {"type": "text", "text": "Justification: Both, data and models, are not prone to a high risk of misuse, see questions 9.   \nand 10. Hence, we propose no safeguards in our work. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 27}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Our work builds on data, the evaluation framework OpenOOD and previous OOD detectors that are introduced and cited in the Experiments section and Related Work section, respectively. Our contribution is licensed by CC BY 4.0. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 28}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The implementation of our contribution is well annotated and we provide optimization details as well as a thorough description of our method, as it is the key contribution of our work. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 28}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: Our work does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. ", "page_idx": 28}, {"type": "text", "text": "\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 29}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: Our work does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 29}]