[{"figure_path": "NGpMCH5q7Y/figures/figures_1_1.jpg", "caption": "Figure 1: Human daily hierarchical control.", "description": "This figure illustrates the hierarchical control in daily human activities.  The prefrontal cortex (top level) generates high-level commands (e.g., \"get up and walk\").  These commands are then executed by a lower-level neural network, Central Pattern Generators (CPGs), in the spinal cord which controls the intricate muscle coordination without conscious attention. This reflects how humans can abstract and provide high-level knowledge while lower-level details are handled automatically.", "section": "1 Introduction"}, {"figure_path": "NGpMCH5q7Y/figures/figures_3_1.jpg", "caption": "Figure 2: The overall framework of the human knowledge guided hierarchical MARL. The general architecture is proposed in the middle which is separated into three levels. Agents can develop their own policies with traditional MARL algorithm shown at bottom left. The graph-based group controller is depicted at top right to enhance agents' coordination. The knowledge controller is comprised with fuzzy logic rules to represent human knowledge which is demonstrated at bottom right. The hyper-networks of knowledge integration is illustrated at top left to allow agents freely decide the use of proposed human knowledge.", "description": "This figure presents the overall framework of the proposed Hierarchical Human Knowledge-guided Multi-Agent Reinforcement Learning (hhk-MARL) method. It shows how three main components, namely the knowledge controller, group controller, and knowledge integration, work together to improve the learning efficiency of MARL agents. The hierarchical structure of the framework is also depicted, which consists of a top level knowledge controller that provides high-level knowledge, a bottom level local agent networks that learn individual policies, and a middle level knowledge integration that combines these two levels effectively.", "section": "3 Human knowledge guided hierarchical framework"}, {"figure_path": "NGpMCH5q7Y/figures/figures_6_1.jpg", "caption": "Figure 4: Experimental results for our approaches and their corresponding baselines in five scenarios. The shaded region denotes standard deviation of average evaluation over 3 trials.", "description": "This figure presents the experimental results comparing the performance of the proposed Hhk-MARL framework with three different MARL algorithms (IQL, QMIX, Qatten) against their baselines across five different scenarios in the StarCraft Multi-Agent Challenge (SMAC) environment. Each scenario involves a varying number of agents (5 vs 6, 10 vs 11, 18 vs 20, 27 vs 30, and 35 vs 40). The x-axis represents the number of training steps (in millions), and the y-axis represents the median test win rate (%). The shaded area around each line indicates the standard deviation across three independent trials.  The results show that the Hhk-MARL framework, even with suboptimal human prior knowledge, consistently improves the performance and training speed compared to the baseline MARL algorithms.", "section": "4.2 Result and evaluation"}, {"figure_path": "NGpMCH5q7Y/figures/figures_6_2.jpg", "caption": "Figure 3: Membership function: 'e_d is small'. X-axis denotes the observation value for variable e_d and Y-axis denotes the truth value.", "description": "This figure shows a membership function graph for a fuzzy logic rule. The x-axis represents the observed distance to the nearest enemy (e_d), and the y-axis represents the degree to which the condition \"e_d is small\" is satisfied. The graph illustrates how the fuzzy set 'small' maps the distance values to a degree of membership between 0 and 1. This function determines the weight assigned to the rule 'If the enemy is close, attack' in the knowledge controller.", "section": "4.1 Experimental setting"}, {"figure_path": "NGpMCH5q7Y/figures/figures_7_1.jpg", "caption": "Figure 5: Performance comparison between baselines and our methods under the number of agents increase. The error bar is based on standard deviation", "description": "This figure compares the performance of three baseline multi-agent reinforcement learning (MARL) algorithms (IQL, QMIX, Qatten) with their corresponding versions enhanced by the proposed hierarchical human knowledge-guided MARL framework (hhk-MARL).  The median test win rates are shown across five scenarios with varying numbers of agents (5, 10, 18, 27, 35). Error bars represent standard deviation, indicating variability in performance. The figure demonstrates that the hhk-MARL framework consistently improves performance across all algorithms and varying agent counts, highlighting its scalability and effectiveness.", "section": "4.2 Result and evaluation"}, {"figure_path": "NGpMCH5q7Y/figures/figures_8_1.jpg", "caption": "Figure 7: Ablation studies under '10m vs 11m' scenario. (a) ablation study on the function of each module in our method; (b) ablation study on the influence of various suboptimal human knowledge.", "description": "This figure presents the results of ablation studies conducted on the proposed 'human knowledge guided hierarchical MARL' framework.  Panel (a) shows the impact of removing individual components of the framework (group controller, human knowledge, and trainable weights) on the learning performance.  Panel (b) illustrates how different amounts and qualities of human knowledge affect the learning process, comparing results using 1, 3, 5, and 8 fuzzy logic rules, as well as random knowledge.  The figure helps understand the contribution of each component of the framework and how robustness to noisy human input is achieved.", "section": "4.3 Ablation study"}, {"figure_path": "NGpMCH5q7Y/figures/figures_8_2.jpg", "caption": "Figure 9: The cooperation graph from hhkIQL during one battle episode based on the change of each agent's \u03bb under '10m vs 11m' scenario.", "description": "This figure shows the dynamic cooperation graph generated by the group controller in the hhkIQL algorithm during a single episode of the '10m vs 11m' scenario in the StarCraft Multi-Agent Challenge (SMAC). Each node represents an agent, and the edges represent the cooperation tendency between agents.  The weights on the edges (\u03bbi,j values) indicate the strength of the cooperative tendency. The figure visualizes how the cooperation relationships between agents evolve over time during the battle. The graph changes dynamically as agents interact and their observations change.", "section": "A.7 Dynamic graph"}, {"figure_path": "NGpMCH5q7Y/figures/figures_15_1.jpg", "caption": "Figure 8: Membership functions used in SMAC.", "description": "This figure shows the membership functions used for the fuzzy logic rules in the StarCraft Multi-Agent Challenge (SMAC) environment.  Each subplot displays the membership function for a specific fuzzy set used as a precondition in the fuzzy rules. These functions map observation values to degrees of membership, indicating the extent to which a particular observation satisfies a specific linguistic term, such as \"small\", \"large\", etc.  These functions are essential for determining the truth value of fuzzy logic rule preconditions and calculating the action taken by an agent.", "section": "A.6 Suboptimal human knowledge applied in experiment"}, {"figure_path": "NGpMCH5q7Y/figures/figures_16_1.jpg", "caption": "Figure 9: The cooperation graph from hhkIQL during one battle episode based on the change of each agent's \u03bbi under '10m vs 11m' scenario.", "description": "This figure shows the dynamic cooperation graph generated by the group controller module of the hhkIQL algorithm during a single episode of the 10m vs 11m scenario in the StarCraft Multi-Agent Challenge (SMAC). Each node represents an agent, and the edges represent the cooperation tendency between agents, calculated based on local observations.  The graph illustrates how agent collaborations evolve over time within a single episode. The thickness or presence of an edge suggests the strength of the cooperative relationship between the two agents.", "section": "A.7 Dynamic graph"}]