[{"type": "text", "text": "SPARKLE: A Unified Single-Loop Primal-Dual Framework for Decentralized Bilevel Optimization ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Shuchen Zhu\\* Boao Kong\\* Songtao Lu Peking University Peking University IBM Research shuchenzhu@stu.pku.edu.cn kongboao@stu.pku.edu.cn songtao@ibm.com ", "page_idx": 0}, {"type": "text", "text": "Xinmeng Huang University of Pennsylvania xinmengh@sas.upenn.edu ", "page_idx": 0}, {"type": "text", "text": "Kun Yuant Peking University kunyuan@pku.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "This paper studies decentralized bilevel optimization, in which multiple agents collaborate to solve problems involving nested optimization structures with neighborhood communications. Most existing literature primarily utilizes gradient tracking to mitigate the infuence of data heterogeneity, without exploring other well-known heterogeneity-correction techniques such as EXTRA or Exact Diffusion. Additionally, these studies often employ identical decentralized strategies for both upper- and lower-level problems, neglecting to leverage distinct mechanisms across different levels. To address these limitations, this paper proposes SPARKLE, a unified Single-loop Primal-dual AlgoRithm frameworK for decentraLized bilEvel optimization. SPARKLE offers the flexibility to incorporate various heterogeneitycorrection strategies into the algorithm. Moreover, SPARKLE allows for different strategies to solve upper- and lower-level problems. We present a unified convergence analysis for SPARKLE, applicable to all its variants, with state-of-the-art convergence rates compared to existing decentralized bilevel algorithms. Our results further reveal that EXTRA and Exact Diffusion are more suitable for decentralized bilevel optimization, and using mixed strategies in bilevel algorithms brings more benefits than relying solely on gradient tracking. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Numerous modern machine learning tasks, such as reinforcement learning [25], meta-learning [4], adversarial learning [36], hyper-parameter optimization [19], and imitation learning [3], entail nested optimization formulations that extend beyond the traditional single-level paradigm. For instance, hyper-parameter optimization aims to identify the optimal hyper-parameters for a specific learning task in the upper level by minimizing the validation loss, achieved through training models in the lower-level process. This nested optimization structure has spurred significant attention towards Stochastic Bilevel Optimization (SBO). Since the size of data samples involved in bilevel problems has become increasingly large, this paper investigates decentralized algorithms over a network of $n$ agents (nodes) that collaborate to solve the following distributed bilevel optimization problem: ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x\\in\\mathbb{R}^{p}}\\;\\;\\;\\Phi(x)=f(x,y^{\\star}(x)):=\\frac{1}{n}\\sum_{i=1}^{n}f_{i}(x,y^{\\star}(x)),\n$$", "text_format": "latex", "page_idx": 0}, {"type": "equation", "text": "$$\n{\\mathrm{s.t.}}\\quad y^{\\star}(x)=\\operatorname*{argmin}_{y\\in\\mathbb{R}^{q}}\\Big\\{g(x,y):=\\frac{1}{n}\\sum_{i=1}^{n}g_{i}(x,y)\\Big\\}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "In this formulation, each agent $i$ holds a private upper-level objective function $f_{i}:\\mathbb{R}^{p}\\times\\mathbb{R}^{q}\\rightarrow\\mathbb{R}$ and a strongly convex lower-level objective function $g_{i}:\\mathbb{R}^{p}\\times\\mathbb{R}^{q}\\rightarrow\\mathbb{R}$ defined as: ", "page_idx": 1}, {"type": "equation", "text": "$$\nf_{i}(x,y)=\\mathbb{E}_{\\phi\\sim\\mathcal{D}_{f_{i}}}[F_{i}(x,y;\\phi)],\\qquad g_{i}(x,y)=\\mathbb{E}_{\\xi\\sim\\mathcal{D}_{g_{i}}}[G_{i}(x,y;\\xi)],\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\mathcal{D}_{f_{i}}$ and $\\mathcal{D}_{g_{i}}$ represent the local data distributions at agent $i$ . This paper does not make any assumptions about these data distributions, implying there might be data heterogeneity across agents. ", "page_idx": 1}, {"type": "text", "text": "Linear speedup and transient iteration complexity. A decentralized stochastic algorithm achieves linear speedup if its iteration complexity decreases linearly with the network size $n$ .Additionally, the transient iteration complexity refers to the number of transient iterations a decentralized algorithm must undergo to achieve the asymptotic linear speedup stage. The fewer the transient iterations, the faster the algorithm can achieve linear speedup. This paper aims to develop decentralized stochastic bilevel algorithms that can achieve linear speedup with as few transient iterations as possible. ", "page_idx": 1}, {"type": "text", "text": "Limitations in previous works. A significant challenge in decentralized bilevel optimization lies in accurately estimating the hyper-gradient $\\nabla\\Phi(x)$ through neighborhood communications. Several studies have emerged to effectively address this challenge, such as those by [9, 33, 52, 16, 21, 40, 57, 29]. However, existing works suffer from several critical limitations: ", "page_idx": 1}, {"type": "text", "text": "\u00b7 Stringent assumptions and inadequate convergence analysis. Many existing studies rely on stringent assumptions to ensure convergence. For instance, references [9, 10, 33, 21, 52] assume bounded gradients, while reference [29] assumes bounded data heterogeneity (also known as bounded gradient dissimilarity). These restrictive assumptions do not arise in centralized bilevel optimization, implying their potential unnecessity. Moreover, some of these works suffer from inadequate convergence analysis, unable to clarify the transient iteration complexity [9, 33, 10] or provide a sharp estimation of the influence of network topologies [52, 21]. ", "page_idx": 1}, {"type": "text", "text": ". Limited exploration of various heterogeneity-correction techniques. Several concurrent studies [16, 57, 40] have utilized Gradient Tracking (GT) [50, 13, 38] to remove the assumption of bounded data heterogeneity. However, it remains uncertain whether GT is the most suitable mechanism for decentralized bilevel optimization. Many other techniques are also useful for addressing data heterogeneity in single-level decentralized optimization, such as EXTRA [45] and Exact-Diffusion (ED) [56, 30, 54] (which is also known as $\\mathrm{D^{2}}$ [46]). Even within GT, there are variants including Adapt-Then-Combine GT (ATC-GT) [50], non-ATC-GT [38], and semiATC-GT [13]. It remains unexplored whether these techniques for mitigating data heterogeneity converge and even outperform GT when employed in decentralized bilevel algorithms. ", "page_idx": 1}, {"type": "text", "text": ". Unknown effects of employing different upper- and lower-level update strategies. In bilevel optimization, the challenges in solving the upper- and lower-level problems differ substantially. For instance, the upper-level problem (1a) is non-convex, whereas the lower-level problem (1b) is strongly convex. Moreover, estimating the gradient at the lower level is considerably simpler compared to estimating the hyper-gradient at the upper level. Understanding the roles of updates at each level is crucial to develop more efficient algorithms. However, most existing algorithms employ the same decentralized methods to solve both the upper- and lower-level problems. For example, references [9, 52, 29] utilize decentralized gradient descent (DGD) for updates at both levels while [57, 40, 16] leverage GT, overlooking the potential advantages of mixed strategies. ", "page_idx": 1}, {"type": "text", "text": "To address these limitations, several critical questions naturally arise: Should each heterogeneitycorrection mechanism listed in [50, 13, 38, 56, 30, 54, 46] be explored one-by-one? Should we consider combining any two of these techniques to update the upper and lower-level problems, respectively? It is evident that examining each individual heterogeneity-correction technique, and even exploring their combinations, would involve an unbearable amount of effort. ", "page_idx": 1}, {"type": "text", "text": "Main results and contributions. This paper addresses all the aforementioned limitations without exhaustively exploring all heterogeneity-correction techniques. Our main results are as follows. ", "page_idx": 1}, {"type": "text", "text": "\u00b7 A unified decentralized bilevel framework. To avoid examining each single heterogeneitycorrection technique, we propose SPARKLE, a unified Single-loop Primal-dual AlgoRithm frameworK for decentraLized bilEvel optimization._ By specifying certain hyper-parameters, SPARKLE can be tailored to SPARKLE-EXTRA, SPARKLE-ED, and SPARKLE-GT, which employ EXTRA [45], ED [56, 30], or multiple GT variants [50, 13, 38], respectively, to facilitate the upper and lower-level problems. Additionally, SPARKLE is the first algorithm enabling distinct updating strategies across different levels; for example, one can utilize GT in the upperlevel but ED in the lower-level, resulting in a brand new SPARKLE-GT-ED algorithm. ", "page_idx": 1}, {"type": "table", "img_path": "g5DyqerUpX/tmp/c8a35482014185b531adeebb8a902850cd13363570493bba3e6ac87bdf91ebe0.jpg", "table_caption": ["Table 1: Comparison between different decentralized stochastic bilevel algorithms. $K$ denotes the number of (upper-level) iterations; $1-\\rho$ denotes the spectral gap of the mixing matrix (see Assumption 2); $b^{2}$ bounds the gradientdsimilarity, $\\varepsilon$ isthetargetsaonarituhha $\\begin{array}{r}{\\sum_{k=0}^{K-1}\\mathbb{E}[\\|\\nabla\\Phi(\\bar{x}^{k})\\|^{2}]/K<\\varepsilon;p}\\end{array}$ and $q$ arethe dimensions of the upper- and lower-level variables, reflecting per-round communication costs. Assumptions of bounded gradient, Lipschitz continuity, and bounded gradient dissimilarity are abbreviated as BG, LC, and BGD, respectively. We also list the best-known results of single-level GT, EXTRA, and ED at the bottom. "], "table_footnote": ["The convergence rate when $K\\rightarrow\\infty$ (smaller is better). The number of gradient/Jacobian/Hessian evaluations per agent to achieve $\\varepsilon$ accuracy when $\\epsilon\\to0$ (smaller is beter). The communication costs per agent to achieve $\\varepsilon$ stationarity when $\\epsilon\\to0$ (smaller is better). The transient iteration complexity to achieve linear speedup (smaller is better). \u201cN.A.\u201d means that the algorithm cannot achieve linear speedup or the transient time cannot be accessed from existing convergence analysis. Additional assumptions beyond Assumption 1. \\* LoPA solves the personalized problem, where the lower-level objectives are local to agents. \u5eff $a>0$ measures the relative sparsity of the mixing weights ${\\mathbf{W}}_{x}$ $\\mathbf{W}_{y}$ $\\mathbf{W}_{z}$ , which can be very small in certain cases. Here $1-\\rho$ in Tran. Iter. denotes the smallest spectral gap of $\\mathbf{W}_{x},\\mathbf{W}_{y},\\mathbf{W}_{z}$ . See more discussions in Appendix C.2.3. "], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "\u00b7 A unified and sharp analysis of various heterogeneity-correction schemes. We provide a unified convergence analysis for SPARKLE, which immediately applies to all SPARKLE variants with distinct heterogeneity-correction techniques. The analysis does not require restrictive assumptions such as gradient boundedness used in [9, 10, 33, 21, 52] or data-heterogeneity bounded used in [29]. Moreover, our analysis demonstrates the provable superiority of SPARKLE compared to existing algorithms, as evidenced by the convergence rates listed in Table 1. Most importantly, our analysis shows that both SPARKLE-EXTRA and SPARKLE-ED outperform SPARKLE-GT (see Table 1), implying that GT is not the best scheme for decentralized bilevel optimization. ", "page_idx": 2}, {"type": "text", "text": "\u00b7 Mixing strategies outperform employing GT alone. We demonstrate how optimization at different levels affects convergence rates. Our theoretical analysis suggests that the updating strategy at the lower level is crucial in determining the overall performance in decentralized bilevel algorithms. Building upon this insight, we establish that incorporating the ED or EXTRA strategy in the lower-level update phase leads to better transient iteration complexity than relying solely on the GT mechanism in both levels as proposed in [10, 16, 40], see Table 2 for more details. ", "page_idx": 2}, {"type": "text", "text": "\u00b7 Comparable performance with single-level algorithms. We elucidate the comparison between bilevel and single-level stochastic decentralized optimization. On one hand, we demonstrate that the convergence performance of all our proposed algorithms is not inferior to their single-level counterparts (see the bottom part in Table 1). On the other hand, by considering specific lower-level loss functions, our bilevel results directly yield the non-asymptotic convergence of corresponding ", "page_idx": 2}, {"type": "text", "text": "Table 2: The transient iteration complexity of SPARKLE with mixed updating strategies at various levels. The smaller the transient iteration complexity is, the faster the algorithm will achieve its linear speedup stage. The first row and column respectively indicate the updating strategy for the upper- and lower-level problems. Please refer to Appendix B.3 for more implementation details and Appendix C.2.4 for proofs . ", "page_idx": 3}, {"type": "table", "img_path": "g5DyqerUpX/tmp/d9f9bf7205c1999f7397a3024a2cb8b9b151a15405598352ca1d397b0241096e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "single-level algorithms. This is the first result demonstrating bilevel optimization essentially subsumes the convergence of the single-level optimization. ", "page_idx": 3}, {"type": "text", "text": "Our main results are listed in Table 1. All SPARKLE variants achieve the state-of-the-art asymptotic rate, asymptotic gradient complexity, asymptotic communication cost, and transient iteration complexity under more relaxed assumptions compared to existing methods. ", "page_idx": 3}, {"type": "text", "text": "Related works. A significant challenge in decentralized bilevel optimization is accurately estimating the hyper-gradient $\\nabla\\Phi(x)$ , necessitating solving global lower-level problems and estimating Hessian inversion. To this end, various decentralized techniques have been applied in bilevel optimization, including Neumann series in [52], JHIP oracle in [9], HIGP oracle in [10], and augmented Lagrangianbased communication in [33]. Additionally, reference [29] proposes a single-loop algorithm utilizing decentralized SOBA. To enhance algorithmic robustness against data heterogeneity, recent studies have employed Gradient Tracking (GT) in both lower- and upper-level optimization. However, existing works built upon GT suffer from several limitations. Results of [16, 9] concentrate solely on deterministic cases, while reference [40] addresses personalized problems in the lower-level, which do not require achieving global consensus in the lower-level problem. Moreover, [9, 10] introduce computationally expensive inner loops for GT steps. None of these works can establish smaller transient iteration complexity than D-SOBA for decentralized SBO, even though the latter algorithm employs no heterogeneity-correction technique. ", "page_idx": 3}, {"type": "text", "text": "The unified framework for single-level decentralized optimization has been extensively studied in the literature. References [1, 49, 26] propose frameworks for decentralized composite optimization in deterministic settings, while [2] investigates a framework under stochastic settings. However, none of these works can be directly applied to decentralized bilevel algorithms. Several studies [21, 57] utilize variance reduction techniques to accelerate the convergence of stochastic decentralized bilevel algorithms. Our proposed SPARKLE framework is orthogonal to variance reduction; it can also incorporate variance-reduced gradient estimation to achieve improved convergence rates. More relevant works on decentralized optimization and bilevel optimization are discussed in Appendix A. ", "page_idx": 3}, {"type": "text", "text": "Notations. We use lowercase letters to represent vectors and uppercase letters to represent matrices. We introduce $\\mathrm{col}\\{x_{1},...,x_{n}\\}:=[x_{1}^{\\top},...,\\stackrel{\\cdot}{x}_{n}^{\\top}]^{\\top}\\in\\mathbb{R}^{p n}$ for brevity. Variables with overbar denote the average over all agents. For example, $\\textstyle{\\bar{x}}^{k}=\\sum_{i=1}^{n}x_{i}^{k}/n$ . We denote $\\textstyle{\\overline{{A}}}=A-{\\frac{1}{n}}\\mathbf{1}_{n}\\mathbf{1}_{n}^{\\top}$ for matrix $A\\in\\mathbb{R}^{n\\times n}$ ,where $\\mathbf{1}_{n}\\,\\in\\,\\mathbb{R}^{n}$ denotes the $n$ -dimensional vector with all entries being one. For a function $f(x,y):\\mathbb{R}^{p}\\times\\mathbb{R}^{q}\\to\\mathbb{R}$ , we use $\\nabla_{1}f(x,y)\\in\\mathbb{R}^{p}$ \uff0c $\\nabla_{2}f(x,y)\\in\\mathbb{R}^{q}$ to represent its partial gradients with respect to $x$ and $y$ , respectively. Similarly, $\\nabla_{12}f(x,y)\\in\\mathbb{R}^{p\\times q}$ $\\nabla_{22}f(x,y)\\in\\mathbb{R}^{q\\times q}$ represent the corresponding Jacobian and Hessian matrix. We use the notation $\\lesssim$ to denote inequalities that hold up to constants related to the initialization of algorithms and smoothness constants. ", "page_idx": 3}, {"type": "text", "text": "2 SPARKLE: A unified framework for decentralized bilevel optimization ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "This section develops SPARKLE, a unified framework for decentralized bilevel optimization, and discusses its numerous variants by specifying certain hyper-parameters. ", "page_idx": 3}, {"type": "text", "text": "2.1  Three pillar subproblems in decentralized bilevel optimization. ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "When solving the upper-level problem (1a), it is critical to obtain the hyper-gradient $\\nabla\\Phi({\\boldsymbol{x}})$ ,which can be expressed as [22] ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\nabla\\Phi(x)=\\nabla_{1}f(x,y^{\\star}(x))-\\nabla_{12}^{2}g(x,y^{\\star}(x))\\left[\\nabla_{22}^{2}g(x,y^{\\star}(x))\\right]^{-1}\\nabla_{2}f(x,y^{\\star}(x)).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Evaluating this hyper-gradient is computationally expensive due to the inversion of the Hessian matrix. This evaluation becomes even more challenging over a decentralized network of collaborative agents. First, the inverse of the Hessian matrix cannot be obtained by simply averaging the local Hessian inversesdueto $\\begin{array}{r}{[\\frac{1}{n}\\sum_{i=1}^{n}\\nabla_{22}^{2}g_{i}(x,y^{\\star}(x))]^{-1}\\neq\\frac{1}{n}\\sum_{i=1}^{n}[\\nabla_{22}^{2}g_{i}(x;\\dot{y^{\\star}}(x))]^{-1}}\\end{array}$ Ssecond,theglbal averaging operation cannot be realized through decentralized communication. To overcome these challenges, one can introduce an auxiliary variable $z^{\\star}(x)\\!:=[\\nabla_{22}^{2}g(x,y^{\\star}(x))]^{-1}\\nabla_{2}f(x,y^{\\star}(x))$ [12], which is the solution to a quadratic problem ", "page_idx": 4}, {"type": "equation", "text": "$$\nz^{\\star}(x)=\\underset{z\\in\\mathbb{R}^{q}}{\\mathrm{argmin}}\\left\\lbrace\\frac{1}{2}z^{\\top}\\nabla_{22}^{2}g\\left(x,y^{\\star}(x)\\right)z-z^{\\top}\\nabla_{2}f\\left(x,y^{\\star}(x)\\right)\\right\\rbrace.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Once $z^{\\star}(x)$ is derived by solving (4), we can substitute it into (3) to achieve $\\nabla\\Phi(x)$ ", "page_idx": 4}, {"type": "text", "text": "Following this idea, solving the distributed bilevel optimization problem (1) essentially involves solving three subproblems, where $\\begin{array}{r}{h_{i}(x,y^{\\star}(x),z):=\\frac{\\mathtt{f}}{2}z^{\\top}\\nabla_{22}^{2}g_{i}(x,y^{\\star}(x))z-z^{\\top}\\nabla_{2}f_{i}(\\bar{x_{,}}y^{\\star}(x))}\\end{array}$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{x^{\\star}=\\displaystyle\\operatorname*{argmin}_{x\\in\\mathbb R^{p}}\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}f_{i}(x,y^{\\star}(x)),}}\\\\ {{y^{\\star}(x)=\\displaystyle\\operatorname*{argmin}_{y\\in\\mathbb R^{q}}\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}g_{i}(x,y),}}\\\\ {{z^{\\star}(x)=\\displaystyle\\operatorname*{argmin}_{z\\in\\mathbb R^{q}}\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}h_{i}(x,y^{\\star}(x),z).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Given thevariable $x$ ,onecan achieve $y^{\\star}(x)$ by solving the lower-level problem in (5b). With $y^{\\star}(x)$ determined, $z^{\\star}(x)$ can be obtained by solving the auxiliary-level problem in (5c). Subsequently, with $z^{\\star}(x)$ available, one can directly compute the hyper-gradient and solve the upper-level problem in (5a) using gradient descent. This constitutes the primary methodology to solve problem (1). ", "page_idx": 4}, {"type": "text", "text": "A bilevel algorithm essentially solves three subproblems listed in (5), each formulated as a single-level decentralized optimization problem. Nevertheless, primary approaches may suffer from nested loops in algorithmic development. A few recent studies [12, 11, 57, 29] propose to solve each problem in (5a)-(5c) approximately with one single iteration, leading to practical single-loop bilevel algorithms. For example, applying a D-SGD step [43] to each of (5a)-(5c) yields the D-SOBA method [29], while further leveraging the GT technique leads to decentralized bilevel methods in [9, 16, 57, 21]. ", "page_idx": 4}, {"type": "text", "text": "However, it is less explored whether numerous other heterogeneity-correction techniques [50, 13, 38, 56, 30, 54, 46] beyond GT can be incorporated into algorithmic design to achieve even better performance in bilevel optimization. To avoid exploring each case individually, we next introduce a general framework that unifies all these techniques for solving single-level problems. ", "page_idx": 4}, {"type": "text", "text": "2.2  A unified framework for decentralized single-level optimization. ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this subsection, we consider solving the single-level problem minrERp  > $\\begin{array}{r}{\\operatorname*{min}_{x\\in\\mathbb{R}^{p}}\\frac{1}{n}\\sum_{i=1}^{n}f_{i}(x)}\\end{array}$ over a network of $n$ nodes. For each $k$ -th $(k\\geq0)$ iteration, we let $\\boldsymbol{x}_{i}^{k}$ denote the local $x$ -variable maintained by the $i$ -th agent. Furthermore, we associate the topology with a weight matrix $W=[w_{i j}]_{i,j=1}^{n}\\in$ $\\mathbb{R}^{n\\times n}$ in which $w_{i j}\\in(0,1)$ if node $j$ is connected to node $i$ otherwise $w_{i j}=0$ We use bold symbols to denote stacked vectors or matrices across agents. For example, $\\mathbf{x}^{k}=\\operatorname{col}\\{x_{1}^{k},...,x_{n}^{k}\\}\\in\\mathbb{R}^{p n}$ and $\\mathbf{W}=W\\otimes I_{p}$ , where $\\otimes$ denotes the Kronecker product operator. ", "page_idx": 4}, {"type": "text", "text": "A unified framework with moving average. Building on the formulation in [1, 2], we develop a unified primal-dual framework with moving average for decentralized optimization: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{r}^{k+1}=(1-\\theta)\\mathbf{r}^{k}+\\theta\\mathbf{g}^{k},\\quad\\mathbf{x}^{k+1}=\\mathbf{Cx}^{k}-\\alpha\\mathbf{Ar}^{k+1}-\\mathbf{Bd}^{k},\\quad\\mathbf{d}^{k+1}=\\mathbf{d}^{k}+\\mathbf{Bx}^{k+1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here $\\mathbf{x}^{k}$ denotes the primal variable, $\\mathbf{d}^{k}$ denotes the dual variable introduced to mitigate the influence of data-heterogeneity, $\\mathbf{g}^{k}$ stacks all (stochastic) gradients evaluated at $\\boldsymbol{x}_{i}^{k}$ for $1\\leq\\bar{i}\\leq n,\\mathbf{r}^{k}$ denotes the momentum introduced to boost training with coefficient $\\theta\\in[0,1]$ , and $\\alpha>0$ is the learning rate. Matrices ${\\bf A},{\\bf B},{\\bf C}\\in\\mathbb{R}^{p n\\times p n}$ are adapted from the mixing matrix $\\mathbf{W}$ , which determine how agents communicate with each other. See Appendix B.1 for more detailed motivations. ", "page_idx": 4}, {"type": "text", "text": "Framework (6) unifies various decentralized techniques in the literature. For instance, by letting $\\theta=1$ and specifying $\\mathbf{A},\\mathbf{B},\\mathbf{C}$ delicately, framework (6) reduces to ED, EXTRA, and numerous GT ", "page_idx": 4}, {"type": "text", "text": "Algorithm 1 SPARKLE: A unified framework for decentralized stochastic bilevel optimization ", "page_idx": 5}, {"type": "text", "text": "Require: Initialize $\\mathbf{x}^{0}=\\mathbf{y}^{0}=\\mathbf{z}^{0}=\\mathbf{r}^{0}=\\mathbf{0}$ $\\mathbf{d}_{x}^{0}=\\mathbf{d}_{y}^{0}=\\mathbf{d}_{z}^{0}=\\mathbf{0}$ learning rate $\\alpha_{k},\\beta_{k},\\gamma_{k},\\theta_{k}$ for $k=0,1,\\cdots,K-1$ do. $\\begin{array}{r l r l}&{\\mathbf{y}^{k+1}=\\mathbf{\\tilde{C}}_{y}\\mathbf{y}^{k}-\\beta_{k}\\hat{\\mathbf{A}}_{y}\\mathbf{\\tilde{y}}^{k}-\\mathbf{B}_{y}\\mathbf{d}_{y}^{k},}&&{\\mathbf{d}_{y}^{k+1}=\\mathbf{d}_{y}^{k}+\\mathbf{B}_{y}\\mathbf{y}^{k+1};}&&{\\mathrm{\\,\\,\\tilde{p}=\\mathrm{lower-level\\,\\,update}}}\\\\ &{\\mathbf{z}^{k+1}=\\mathbf{C}_{z}\\mathbf{z}^{k}-\\gamma_{k}\\mathbf{A}_{z}\\mathbf{p}^{k}-\\mathbf{B}_{z}\\mathbf{d}_{z}^{k},}&&{\\mathbf{d}_{z}^{k+1}=\\mathbf{d}_{z}^{k}+\\mathbf{B}_{z}\\mathbf{z}^{k+1};}&&{\\mathrm{\\,\\,\\tilde{p}=\\mathrm{uaxiliary-level\\,\\,update}}}\\\\ &{\\mathbf{r}^{k+1}=(1-\\theta_{k})\\mathbf{r}^{k}+\\theta_{k}\\mathbf{u}^{k};}&&{\\mathrm{\\,\\,\\tilde{p}=\\mathrm{momentum\\,\\,update}}}\\\\ &{\\mathbf{x}^{k+1}=\\mathbf{C}_{x}\\mathbf{x}^{k}-\\alpha_{k}\\mathbf{A}_{x}\\mathbf{r}^{k+1}-\\mathbf{B}_{x}\\mathbf{d}_{x}^{k},}&{\\mathbf{d}_{x}^{k+1}=\\mathbf{d}_{x}^{k}+\\mathbf{B}_{x}\\mathbf{x}^{k+1};}&&{\\mathrm{\\,\\,\\tilde{p}=\\mathrm{upper-level\\,\\,update}}}\\end{array}$ end for ", "page_idx": 5}, {"type": "text", "text": "variants, see Table 3 and Appendix B.1 for more details. Framework (6) is closely related to the unified decentralized method developed in [1, 2]. The primary difference lies in the incorporation of themomentum variable $\\mathbf{r}^{k}$ , which can help improve the transient iteration complexity of the framework (6) and relax the smoothness condition for bilevel algorithms [11]. A detailed comparison between framework (6) and that proposed in [1, 2] is provided in Appendix B.2. ", "page_idx": 5}, {"type": "text", "text": "2.3  A unified framework for decentralized bilevel optimization. ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "By utilizing the unified framework (6) to approximately solve each subproblem in (5) with only one iteration, we achieve SPARKLE, a unified single-loop framework for decentralized bilevel optimization. In particular, we independently sample data $\\xi_{i}^{\\dot{k}}\\sim\\mathcal{D}_{f_{i}}$ $\\zeta_{i}^{k}\\sim\\mathcal{D}_{g_{i}}$ withineachnode at iteration $k$ , and evaluate stochastic gradients/Jacobians/Hessians as follows ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{l_{i}^{k}=\\nabla_{1}F_{i}(x_{i}^{k},y_{i}^{k};\\xi_{i}^{k}),\\qquad b_{i}^{k}=\\nabla_{2}F_{i}(x_{i}^{k},y_{i}^{k};\\xi_{i}^{k}),\\qquad v_{i}^{k}=\\nabla_{2}G_{i}(x_{i}^{k},y_{i}^{k};\\zeta_{i}^{k}),}\\\\ &{J_{i}^{k}=\\nabla_{12}^{2}G_{i}(x_{i}^{k},y_{i}^{k};\\zeta_{i}^{k}),\\quad H_{i}^{k}=\\nabla_{22}^{2}G_{i}(x_{i}^{k},y_{i}^{k};\\zeta_{i}^{k}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Next we stack the descent directions for variables of each level as follows ", "page_idx": 5}, {"type": "text", "text": "lower-levelstochsticgradient: $\\begin{array}{r l}&{\\mathbf v^{k}=\\mathrm{col}\\{v_{1}^{k},...,v_{n}^{k}\\},}\\\\ &{\\mathbf p^{k}=\\mathrm{col}\\{H_{1}^{k}z_{1}^{k}-b_{1}^{k},...,H_{n}^{k}z_{n}^{k}-b_{n}^{k}\\},}\\\\ &{\\mathbf u^{k}=\\mathrm{col}\\{l_{1}^{k}-J_{1}^{k}z_{1}^{k+1},...,l_{n}^{k}-J_{n}^{k}z_{n}^{k+1}\\}.}\\end{array}$   \nauxilliary-level stochstic gradient:   \nupper-level stochstic gradient: ", "page_idx": 5}, {"type": "text", "text": "The SPARKLE algorithm is detailed in Algorithm 1. In this algorithm, we utilize different dual variables ${\\bf d}_{s}$ and communication matrices $\\mathbf{A}_{s},\\mathbf{B}_{s},\\mathbf{C}_{s}$ for each variable $s\\in\\{x,y,z\\}$ to optimize their respective objective functions. We use momentum $\\mathbf{r}^{k}$ only for updating the upper-level variable, which is sufficient to enhance convergence of bilevel algorithms and relax the smoothness condition. ", "page_idx": 5}, {"type": "text", "text": "Versatility in decentralized strategies. SPARKLE is highly versatile, supporting various decentralized strategies by allowing the specification of different communication matrices $\\mathbf{A}_{s}$ \uff0c $\\mathbf{B}_{s}$ , and $\\mathbf{C}_{s}$ . For example, by setting ${\\bf A}_{s}={\\bf W}$ \uff0c $\\mathbf B_{s}=(\\mathbf I-\\mathbf W)^{1/2}$ , and ${\\bf C}_{s}={\\bf W}^{2}$ for any $s\\in\\{x,y,z\\}$ SPARKLE will utilize EXTRA to update variables $x$ $y$ ,and $z$ , resulting in the SPARKLE-EXTRA variant. Other variants can be achieved by setting $\\mathbf{A}_{s}$ \uff0c $\\mathbf{B}_{s}$ , and $\\mathbf{C}_{s}$ according to Table 3. These variants can be implemented more efficiently than listed in Algorithm 1, see Appendix B.3. ", "page_idx": 5}, {"type": "text", "text": "Flexibility across optimization levels. SPARKLE supports different optimization and communication mechanisms for each level of (5), which can be directly achieved by choosing different $\\mathbf{A}_{s}$ $\\mathbf{B}_{s}$ , and $\\mathbf{C}_{s}$ matrices for each level $s\\in\\{x,y,z\\}$ . For example, SPARKLE can utilize GT to update the upper-level variable $x$ while employing ED to update the auxiliary- and lower-level variables $y$ and $z$ . Throughout this paper, we denote SPAR KLE using the decentralized mechanism $\\mathbf{L}$ for the lower-level and auxiliary variables, and $\\mathbf{U}$ for the upper-level in Algorithm 1, by SPARKLE-L-U, or simply SPARKLE- $\\mathbf{L}$ if $\\mathbf{L}=\\mathbf{U}$ In addition, SPARKLE even supports utilizing different mixing matrices $\\mathbf{W}_{x},\\mathbf{W}_{y},\\mathbf{W}_{z}$ across levels. ", "page_idx": 5}, {"type": "text", "text": "3  Convergence analysis ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we establish the convergence properties of the SPARKLE framework and examine the influence of different decentralized techniques utilized across optimization levels. ", "page_idx": 5}, {"type": "table", "img_path": "g5DyqerUpX/tmp/05f30a56792aca129a954a8a144f1c7f0e79988b436034d451700a4c9e09e2d8.jpg", "table_caption": ["Table 3: SPARKLE facilitates different decentralized techniques by specifying ${\\bf A}_{s},{\\bf B}_{s},{\\bf C}_{s}$ for $s\\!\\in\\!\\{x,y,z\\}$ We denote the stacked local variables and the associate gradients estimates by $\\mathbf{s}{\\in}\\{\\mathbf{x},\\mathbf{y},\\mathbf{z}\\}$ and $\\mathbf{g}(\\mathbf{s})$ ,respectively. The update rule refers to the specific algorithmic recursion for each level. See derivations in Appendix B.2. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "3.1 Assumptions ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Before presenting the theoretical guarantees, we first introduce the following assumptions used throughout this paper. ", "page_idx": 6}, {"type": "text", "text": "Assumption 1. There exist constants $\\mu_{g},L_{f,0},L_{f,1},L_{g,1},L_{g,2}$ such that for any $1\\leq i\\leq n$ ", "page_idx": 6}, {"type": "text", "text": "1. $\\nabla f_{i},\\nabla g_{i},\\nabla^{2}g_{i}$ are $L_{f,1},L_{g,1},L_{g,2}$ Lipschitz. continuous, respectively;   \n2. $\\|\\nabla_{2}f_{i}\\left(x,y^{\\star}(x)\\right)\\|\\leq L_{f,0}$ for any $x\\in\\mathbb{R}^{p}$ 3.\uff0c   \n3. $g_{i}(x,y)$ is $\\mu_{g}$ -strongly convex with respect to y for any fixed $x\\in\\mathbb{R}^{p}$ ", "page_idx": 6}, {"type": "text", "text": "Moreover, we define $L:=\\operatorname*{max}\\{L_{f,0},L_{f,1},L_{g,1},L_{g,2}\\}\\;a n d\\,\\kappa:=L/\\mu_{g}.$ ", "page_idx": 6}, {"type": "text", "text": "Assumption 2. For each $s\\in\\{x,y,z\\}$ , the corresponding mixing matrix $W_{s}\\in\\mathbb{R}^{n\\times n}$ is non-negative, symmetric and doubly stochastic, i.e., ", "page_idx": 6}, {"type": "equation", "text": "$$\nW_{s}=W_{s}^{\\top},\\quad W_{s}\\mathbf{1}_{n}=\\mathbf{1}_{n},\\quad(W_{s})_{i j}\\geq0,\\quad\\forall\\,1\\leq i,j\\leq n,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "and the corresponding communication graph is strongly-connected, i.e., its eigenvalues satisfy $1=\\lambda_{1}(W_{s})>\\bar{\\lambda}_{2}(W_{s})\\geq...\\geq\\lambda_{n}(W_{s})$ and $\\rho(W_{s}):=\\operatorname*{max}\\left\\{|\\lambda_{2}(W_{s})|\\,,|\\lambda_{n}(W_{s})|\\right\\}<1$ ", "page_idx": 6}, {"type": "text", "text": "Thevalue $1-\\rho(W_{s})$ is referred to as the spectral gap in the literature [34, 53, 31] of $W_{s}$ ,which measures the connectivity of the communication graph. It would approach O for sparse networks. For example, it holds that $1\\dot{-}\\rho(W_{s})=\\Theta(1/n^{2})$ for the matrix $W_{s}$ induced by a ring graph. ", "page_idx": 6}, {"type": "text", "text": "Assumption 3. For any $s\\in\\{x,y,z\\}$ we assume the communication matrices $A_{s},B_{s},C_{s}$ usedin SPARKLE are polynomial functions of $W_{s}$ .Furthermore,we assume $A_{s},C_{s}$ are doubly stochastic, and $\\mathrm{Null}(B_{s})=\\mathrm{Span}\\{{\\bf1}_{n}\\}$ .In addition,we assume all eigenvalues of the augmented matrix ", "page_idx": 6}, {"type": "equation", "text": "$$\nL_{s}:=\\left[\\begin{array}{c c}{\\overline{{C_{s}}}-B_{s}^{2}}&{B_{s}}\\\\ {-B_{s}}&{\\overline{{I_{n}}}}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "are strictly less than one in magnitude, where $\\begin{array}{r}{\\overline{{C_{s}}}\\triangleq C_{s}-\\frac{1}{n}\\mathbf{1}_{n}\\mathbf{1}_{n}^{\\top}}\\end{array}$ and $\\begin{array}{r}{\\overline{{I_{n}}}\\triangleq I_{n}-\\frac{1}{n}\\mathbf{1}_{n}\\mathbf{1}_{n}^{\\top}}\\end{array}$ ", "page_idx": 6}, {"type": "text", "text": "We remark that Assumption 3 is mild and is satisfied by all choices listed in Table 3. See more discussions in Appendix C.2.2. ", "page_idx": 6}, {"type": "text", "text": "Assumption 4. We assume $\\nabla F_{i}(x,y;\\xi),\\nabla G_{i}(x,y;\\xi)$ and $\\nabla^{2}G_{i}(x,y;\\xi)$ to be unbiased estimates $\\nabla f_{i}(x,y),\\nabla g_{i}(x,y)$ and $\\nabla^{2}g_{i}(x,y)$ with bounded variances $\\sigma_{f,1}^{2},\\sigma_{g,1}^{2},\\sigma_{g,2}^{2}$ respecively. ", "page_idx": 6}, {"type": "text", "text": "3.2  Convergence theorem ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Under the above assumptions, we establish the convergence properties as follows. Proof details can be found in Appendix C. ", "page_idx": 6}, {"type": "text", "text": "Theorem 1. Under Assumptions $I-4$ there exist proper constant step-sizes $\\alpha$ $\\beta$ \uff0c $\\gamma$ andmomentum coefficient $\\theta$ , such that the SPARKLE framework listed in Algorithm $^{\\,l}$ will converge as follow: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{K+1}\\sum_{k=0}^{K}\\mathbb{E}[\\|\\nabla\\Phi(\\bar{x}^{k})\\|^{2}]\\lesssim\\frac{\\kappa^{5}\\sigma}{\\sqrt{n K}}+\\kappa^{\\frac{16}{3}}\\left(\\delta_{y,1}+\\delta_{z,1}\\right)\\frac{\\sigma^{\\frac{2}{3}}}{K^{\\frac{2}{3}}}+\\kappa^{\\frac{7}{2}}\\delta_{x,1}\\frac{\\sigma^{\\frac{1}{2}}}{K^{\\frac{3}{4}}}}\\\\ &{\\displaystyle\\qquad+\\left(\\kappa^{\\frac{26}{5}}\\delta_{y,2}+\\kappa^{6}\\delta_{z,2}\\right)\\frac{\\sigma^{\\frac{2}{5}}}{K^{\\frac{4}{5}}}+\\left(\\kappa^{\\frac{16}{3}}\\delta_{y,3}+\\kappa^{\\frac{14}{3}}\\delta_{z,3}+\\kappa^{\\frac{8}{3}}\\delta_{x,3}\\right)\\frac{1}{K}+\\left(\\kappa C_{\\alpha}+\\kappa^{4}C_{\\theta}\\right)\\frac{1}{K},}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\sigma\\triangleq\\operatorname*{max}\\{\\sigma_{f,1},\\sigma_{g,1},\\sigma_{g,2}\\},$ $\\{\\delta_{s,i}\\}_{i=1}^{3}$ are constants depending onl on ${\\bf W}_{s},{\\bf A}_{s},{\\bf B}_{s},{\\bf C}_{s}$ for $s\\in\\{x,y,z\\}$ and $C_{\\alpha},C_{\\theta}$ areconstantsindependentof $K$ SeeLemma $^{17}$ for their detailed values. ", "page_idx": 7}, {"type": "text", "text": "In the deterministic scenario with $\\sigma=0$ ,SPARKLE converges at the rate ${\\mathcal{O}}(1/K)$ ,seetheformal theorem and derivation in Appendix C.3. This recovers the rate in [15] under even milder assumptions. Unlike reference [15], which only considers GT in the deterministic setting, SPARKLE is a unified bilevel framework for the more general stochastic setting. ", "page_idx": 7}, {"type": "text", "text": "Linear speedup. According to Theorem 1, SPARKLE achieves an asymptotic linear speedup as $K$ approaches infinity, which applies to all SPARKLE variants regardless of the decentralized strategies employed and whether they are utilized at different optimization levels. Furthermore, the asymptotically dominant term $\\kappa^{5}\\sigma/(\\sqrt{n K})$ matches exactly with the single-node bilevel algorithm SOBA [12] when $n=1$ , implying the tightness of Theorem 1 in terms of the asymptotic rate. ", "page_idx": 7}, {"type": "text", "text": "Remark 1. We establish an upper bound for the consensus error $\\begin{array}{r}{\\frac{1}{K}\\displaystyle\\sum_{k=0}^{K}\\mathbb{E}\\left[\\frac{\\|\\mathbf{x}^{k}-\\bar{\\mathbf{x}}^{k}\\|^{2}}{n}+\\frac{\\|\\mathbf{y}^{k}-\\bar{\\mathbf{y}}^{k}\\|^{2}}{n}\\right]\\!.}\\end{array}$ Pleaserefer toLemma $_{l9}$ in Appendix C.2.1 for more details. ", "page_idx": 7}, {"type": "text", "text": "3.3  Transient iteration complexity ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "With the non-asymptotic rate established in Theorem 1, we can derive the transient iteration complexity of SPARKLE as follows. The proof is in Lemma 18. ", "page_idx": 7}, {"type": "text", "text": "Corollary 1. Under the same assumptions as in Theorem $^{\\,l}$ ,the transient iteration complexityof SPARKLE\u2014with the infuence of $\\kappa$ and $\\sigma^{2}$ omitted for brevity\u2014-is on the order of ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\left\\{n^{2}\\delta_{x},n^{3}\\delta_{y},n^{3}\\delta_{z},n\\hat{\\delta}_{x},n\\hat{\\delta}_{y},n\\hat{\\delta}_{z}\\right\\},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\delta_{s},\\hat{\\delta}_{s}$ only depend ${\\bf W}_{s},{\\bf A}_{s},{\\bf B}_{s},{\\bf C}_{s}$ for $s\\in\\{x,y,z\\}$ . Their values are in Lemma 18. ", "page_idx": 7}, {"type": "text", "text": "We obtain the transient iteration complexity of each variant of SPARKLE by applying Corollary 1. Corollary 2. For SPARKLE-ED and SPARKLE-EXTRA, if we choose ${\\bf W}_{y}={\\bf W}_{z}$ ,itholdsthat ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{\\delta_{x}=\\mathcal{O}\\left((1-\\rho(\\mathbf{W}_{x}))^{-2}\\right),}&&{\\delta_{y}=\\delta_{z}=\\mathcal{O}\\left((1-\\rho(\\mathbf{W}_{y}))^{-2}\\right),}\\\\ &{\\widehat{\\delta}_{x}=\\mathcal{O}\\left((1-\\rho(\\mathbf{W}_{x}))^{-\\frac{3}{2}}\\right),}&&{\\widehat{\\delta}_{y}=\\widehat{\\delta}_{z}=\\mathcal{O}\\left((1-\\rho(\\mathbf{W}_{y}))^{-2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Furthermore, if we choose $\\mathbf{W}_{x}\\,=\\,\\mathbf{W}_{y}\\,=\\,\\mathbf{W}_{z}$ anddenote $\\rho\\,\\triangleq\\,\\rho(\\mathbf{W}_{x})$ the transient iteration complexity derived in (8) can be simplifed as $n^{3}/(1-\\rho)^{2}$ ", "page_idx": 7}, {"type": "text", "text": "Corollary 3. For SPARKLE-GT and its variants with semi/non-ATC-GT, if we let ${\\bf W}_{y}={\\bf W}_{z}$ ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\delta_{x}=\\mathcal{O}\\left((1-\\rho(\\mathbf{W}_{x}))^{-2}\\right),\\qquad\\quad\\delta_{y}=\\delta_{z}=\\mathcal{O}\\left((1-\\rho(\\mathbf{W}_{y}))^{-2}\\right),}\\\\ &{\\hat{\\delta}_{x}=\\mathcal{O}\\left((1-\\rho(\\mathbf{W}_{x}))^{-2}\\right),\\qquad\\quad\\hat{\\delta}_{y}=\\hat{\\delta}_{z}=\\mathcal{O}\\left((1-\\rho(\\mathbf{W}_{y}))^{-\\frac{8}{3}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Furthermore, if we let $\\mathbf{W}_{x}=\\mathbf{W}_{y}=\\mathbf{W}_{z}$ and denote $\\rho\\triangleq\\rho(\\mathbf{W}_{x})$ the transient iteration complexity derived in (8) can be simplified as m $\\operatorname{ax}\\{n^{3}/(1-\\rho)^{2},n/(1-\\rho)^{8/3}\\}$ ", "page_idx": 7}, {"type": "text", "text": "Remark 2 (SOTA transient iterations). Comparing with algorithms listed in Table 1, all SPARKLE variants achieve smaller transient iteration complexity, implying that they can achieve linear speedup much faster than the other algorithms, especially over sparse network topologies with $1-\\rho\\to0$ ", "page_idx": 7}, {"type": "text", "text": "Remark 3 (GT is not the best technique for decentralized $S B O$ ). While GT is widely adopted in the literature [16, 21, 57] to facilitate decentralized SBO, a comparison of Corollary 2 and 3 reveals that both SPARKLE-EXTRA and SPARKLE-ED outperform SPARKLE-GT in terms of transient iteration complexity. This implies that EXTRA and ED are better than GT for decentralized SBO. ", "page_idx": 7}, {"type": "text", "text": "3.4  Different strategies across optimization levels ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Corollary 1 clarifies how different update strategies for $x,\\,y$ and $z$ impact the transient iterations through constants $\\{\\delta_{s},\\hat{\\delta}_{s}\\}$ for $s\\;\\in\\;\\{x,y,z\\}$ . Since $\\delta_{y}~=~\\delta_{z}$ and $\\hat{\\delta}_{y}^{\\ \\^{-}}=\\ \\hat{\\delta}_{z}$ when ${\\bf W}_{y}\\;=\\;{\\bf W}_{z}$ (Lemma 18), we naturally employ the same strategy to update $y$ and $z$ . The following corollary studies the utilization of both ED and GT in SPARKLE. See the transient iterations complexity of other mixed strategies in Appendix C.2.4 and Table 2. ", "page_idx": 8}, {"type": "text", "text": "Corollary 4. For SPARKLE-ED-GT which uses $E D$ to update $y$ and $z$ and $G T$ to update $x$ f $\\mathbf{W}_{x}=\\mathbf{W}_{y}=\\mathbf{W}_{z}$ and we denote $\\rho=\\rho(\\mathbf{W}_{x})$ ,it then holds that ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\delta_{x}=\\delta_{y}=\\delta_{z}=\\mathcal{O}\\left((1-\\rho)^{-2}\\right),\\qquad\\hat{\\delta}_{x}=\\hat{\\delta}_{y}=\\hat{\\delta}_{z}=\\mathcal{O}\\left((1-\\rho)^{-2}\\right),\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "which implies that the transient iteration complexity in (8) can be simplified as $n^{3}/(1-\\rho)^{2}$ ", "page_idx": 8}, {"type": "text", "text": "Remark 4 (Mixed strategies outperform employing GT only). Comparing Corollary 3 and 4, we find that using ED to update $y$ and $z$ will lead to smaller $\\hat{\\delta}_{y}$ and $\\hat{\\delta}_{z}$ , which improves the transient iteration complexity compared to employing GT only in all optimization levels (see Corollary 3). ", "page_idx": 8}, {"type": "text", "text": "3.5  Different topologies across optimization levels ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In SPARKLE, we can utilize different topologies across levels. Theorem 1 and Corollary 1 have clarified the influence of using different topologies across levels through the constants $\\{\\delta_{s},\\hat{\\delta}_{s}\\}$ for $s\\in\\{x,y,z\\}$ . For instance, when substituting $\\{\\delta_{s},\\hat{\\delta}_{s}\\}$ established in (9) into (8), SPARKLE-ED has the following transient iteration complexity: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\{n^{2}(1-\\rho(\\mathbf{W}_{x}))^{-2},n^{3}(1-\\rho(\\mathbf{W}_{y}))^{-2}\\}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\mathbf{W}_{x}$ is the mixing matrix for updating $x$ , while $\\mathbf{W}_{y}$ is for updating $y$ and $z$ . As long as $(1-\\rho(\\mathbf{W}_{x}))^{-1}\\lesssim\\sqrt{n}(1\\!-\\!\\rho(\\mathbf{W}_{y}))^{-1}$ holds, SPARKLE-ED retains the transient iteration complexity of $n^{3}(1-\\rho(\\mathbf{W}_{y}))^{-2}$ , which allows for the utilization of a sparser network topology when updating $x$ , thereby reducing communication overheads. Consequently, the ratio $a$ of the communication volume per round for the variables $x$ and $y$ can be significantly less than one. See Appendix C.2.3 for discussion on how to use different topologies across levels in other SPARKLE variants. ", "page_idx": 8}, {"type": "text", "text": "3.6  Recovering single-level decentralized optimization ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Previous works typically study single-level and bilevel optimization separately.  By taking $G_{i}(x,y,\\xi)\\,\\equiv\\,|y|^{2}\\bar{/2}$ and $F_{i}(x,y,\\phi)\\,=\\,F_{i}(x,\\phi)$ into (2), the decentralized SBO problem (1) reduces to stochastic single-level optimization. By setting $\\mathbf{z}^{k}\\equiv\\,0$ $\\mathbf{y}^{k}\\,\\equiv\\,0$ $u_{i}^{k}\\,=\\,\\nabla_{1}f_{i}(x_{i}^{k},\\xi_{i}^{k})$ SPARKLE reduces to the single-level framework (6), whose convergence can be naturally guaranteed by Theorem 1. Please refer to Appendix C.4 for the detailed proof and results. This is the first result demonstrating that bilevel optimization essentially subsumes the convergence of single-level optimization. ", "page_idx": 8}, {"type": "text", "text": "4   Numerical experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we present experiments to validate our theoretical findings. We first explore how update strategies and network structures infuence the convergence of SPARKLE. Then we compare SPARKLE to the existing decentralized SBO algorithms. Additional experiments about a decentralized SBO problem with synthetic data are in Appendix D.1. ", "page_idx": 8}, {"type": "text", "text": "Hyper-cleaning on FashionMNIST dataset. We consider a data hyper-cleaning problem [44] on a corrupted FashionMNIST dataset [48]. Problem formulations and experimental setups can be found in Appendix D.2. Firstly, we equip SPARKLE with different decentralized strategies in different optimization levels and then compare them with D-SOBA [29], MA-DSBO-GT [10], and MDBO [21] using the corruption rate $p=0.1,0.2,0.3$ , respectively. As is shown in Figure 1, all the SPARKLE-based algorithms generally achieve higher test accuracy than D-SOBA, while ED and EXTRA especially outperform GT. Meanwhile, using mixed strategies (i.e., SPARKLE-ED-GT and SPARKLE-EXTRA-GT) achieves similar test accuracy with SPARKLE-ED and SPARKLEEXTRA and outperform SPARKLE-GT, respectively. These observations match with the theoretical results in Corollary 2-4 and Remark 3, 4. ", "page_idx": 8}, {"type": "image", "img_path": "g5DyqerUpX/tmp/951d461fc33ff788f1c5a06105542d9c48920d0a76b8fbcda984fc6eaed5fd9c.jpg", "img_caption": ["Figure 1: The test accuracy on hyper-cleaning with various SPARKLE-based algorithms using different corruptionrates $p$ .(Left: $p=0.1$ ,Middle: $p=0.2$ ,Right: $p=0.3.$ "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "g5DyqerUpX/tmp/39f8da026267aad27543a0b16418fbb13b270e0fddca2c1f4c89a63d3d646e2f.jpg", "img_caption": ["Figure 2: Test accuracy of SPARKLE-EXTRA on Figure 3: The upper-level loss against samples generhyper-cleaning. (Left: fixed graph for $_x$ and varying  ated by one agent of different algorithms in the policy graph for $y,z$ ; Right: fixed for $y,z$ and varying for $x$ \uff09 evaluation. (Left: $n=10$ , Right: $n=20.$ "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Next, we test SPARKLE-EXTRA with two communication strategies including fixed topology for updating $x$ and varying topology for $y,z$ , and fixed topology for updating $y,z$ and varying topology for $x$ . As illustrated in Figure 2, maintaining a fixed topology for $x$ while reducing the connectivity of the topology for $y$ and $z$ will deteriorate the algorithmic performance. Conversely, preserving the topology for $y$ and $z$ while decreasing the connectivity for $x$ has little impact on the performance. This suggests that the influence of the network topology for $y$ and $z$ on the algorithm dominates over the topology for $x$ , which is consistent with our discussion in Section 3.5. We also numerically examine the influence of moving average on convergence, see discussions in Appendix D.2. ", "page_idx": 9}, {"type": "text", "text": "Distributed policy evaluation in reinforcement learning. We consider a multi-agent MDP problem in reinforcement learning on a distributed setting with $\\bar{n^{\\star}}\\in\\{10,20\\}$ agents respectively, which can be formulated as a decentralized SBO problems [52]. Here, we compare SPARKLE with existing decentralized SBO approaches including MDBO [21] and the stochastic extension of SLDBO [16] over a Ring graph. Figure 3 illustrates that SPARKLE converges faster and achieves a lower sample complexity than the other baselines, especially when $n=20$ , which shows the empirical benefits of SPARKLE in decentralized SBO algorithms with a large number of agents and sparse communication modes. More experimental details are in Appendix D.3. ", "page_idx": 9}, {"type": "text", "text": "Decentralized meta-learning. We investigate decentralized meta-learning on miniImageNet [47] with multiple tasks [18], formulating it as a decentralized bilevel optimization problem. This approach minimizes the validation loss with respect to shared parameters as the upper-level loss, while the training loss is managed by task-specific parameters at the lower level. Additional details about the experiment can be found in Appendix D.4. Our method, SPARKLE, is benchmarked against D-SOBA [29] and MAML [18], demonstrating a significant improvement in training accuracy. ", "page_idx": 9}, {"type": "text", "text": "5  Conclusions and limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper proposes SPARKLE, a unified single-loop primal-dual framework for decentralized stochastic bilevel optimization. Being highly versatile, SPARKLE can support different decentralized mechanisms and topologies across optimization levels. Moreover, all SPARKLE variants have been demonstrated to achieve state-of-the-art convergence rate compared to existing algorithms. However, SPARKLE currently supports only strongly-convex problems in the lower-level optimization. Its compatibility with generally-convex lower-level problems remains unknown. Additionally, the condition number of the lower-level problem significantly impacts the performance, as is the case with existing bilevel algorithms. We aim to address these limitations in future work. ", "page_idx": 9}, {"type": "text", "text": "6Acknowledgment ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The work of Shuchen Zhu, Boao Kong, and Kun Yuan is supported by Natural Science Foundation of China under Grants 92370121, 12301392, and W2441021. This work is also supported by Open Project of Key Laboratory of Mathematics and Information Networks, Ministry of Education, China. No.KF202302. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1]  S. A. Alghunaim, E. K. Ryu, K. Yuan, and A. H. Sayed. Decentralized proximal gradient algorithms with linear convergence rates. IEEE Transactions on Automatic Control, 66(6):2787-2794, 2020.   \n[2]  S. A. Alghunaim and K. Yuan. A unified and refined convergence analysis for non-convex decentralized learning. IEEE Transactions on Signal Processing, 2022. [3] S. Arora, S. Du, S. Kakade, Y. Luo, and N. Saunshi. Provable representation learning for imitation learning via bi-level optimization. In International Conference on Machine Learning, pages 367-376. PMLR, 2020.   \n[4]  L. Bertinetto, J. Henriques, P. Torr, and A. Vedaldi. Meta-learning with differentiable closed-form solvers. In International Conference on Learning Representations (ICLR), 2019. International Conference on Learning Representations, 2019.   \n[5]  T.-H. Chang, M. Hong, and X. Wang. Multi-agent distributed optimization via inexact consensus admm. IEEE Transactions on Signal Processing, 63(2):482-497, 2014.   \n[6]  J. Chen and A. H. Sayed. Diffusion adaptation strategies for distributed optimization and learning over networks. IEEE Transactions on Signal Processing, 60(8):4289-4305, 2012. [7]  T. Chen, Y. Sun, Q. Xiao, and W. Yin. A single-timescale method for stochastic bilevel optimization. In International Conference on Artificial Intelligence and Statistics, pages 2466-2488. PMLR, 2022.   \n[8]  T. Chen, Y. Sun, and W. Yin. Closing the gap: Tighter analysis of alternating stochastic gradient methods for bilevel problems. Advances in Neural Information Processing Systems, 34:25294-25307, 2021.   \n[9]  X. Chen, M. Huang, and S. Ma. Decentralized bilevel optimization. arXiv preprint arXiv:2206.05670, 2022.   \n[10]  X. Chen, M. Huang, S. Ma, and K. Balasubramanian. Decentralized stochastic bilevel optimization with improved per-iteration complexity. In International Conference on Machine Learning, pages 4641-4671. PMLR, 2023.   \n[11]  X. Chen, T. Xiao, and K. Balasubramanian. Optimal algorithms for stochastic bilevel optimization under relaxed smoothness conditions. arXiv preprint arXiv:2306.12067, 2023.   \n[12]  M. Dagreou, P. Ablin, S. Vaiter, and T. Moreau. A framework for bilevel optimization that enables stochastic and global variance reduction algorithms. Advances in Neural Information Processing Systems, 35:26698-26710, 2022.   \n[13]  P. Di Lorenzo and G. Scutari. Next: In-network nonconvex optimization. IEEE Transactions on Signal and Information Processing over Networks, 2(2):120-136, 2016.   \n[14]  J. Domke. Generic methods for optimization-based modeling. In Artificial Intelligence and Statistics, pages 318-326. PMLR, 2012.   \n[15] J. Dong, Z. Cao, T. Zhang, J. Ye, S. Wang, F. Feng, L. Zhao, et al. Eflops: Algorithm and system co-design for a high performance distributed training platform. In 2020 IEEE International Symposium on High Performance Computer Architecture (HPCA), pages 610-622, 2020.   \n[16]  Y. Dong, S. Ma, J. Yang, and C. Yin. A single-loop algorithm for decentralized bilevel optimization. arXiv preprint arXiv:2311.08945, 2023.   \n[17]  J. C. Duchi, A. Agarwal, and M. JI Wainwright. Dual averaging for distributed optimization: Convergence analysis and network scaling. IEEE Transactions on Automatic control, 57(3):592-606, 2011.   \n[18]  C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep networks. International Conference on Machine Learning, pages 1126-1135, 2017.   \n[19]  L. Franceschi, P. Frasconi, S. Salzo, R. Grazzi, and M. Pontil. Bilevel programming for hyperparameter optimization and meta-learning. In International Conference on Machine Learning, pages 1568-1577. PMLR, 2018.   \n[20]  B. Gao, Y. Yang, and Y. xiang Yuan. Lancbio: dynamic lanczos-aided bilevel optimization via krylov subspace. arXiv preprint arXiv:2404.03331, 2024.   \n[21]  H. Gao, B. Gu, and M. T. Thai. On the convergence of distributed stochastic bilevel optimization algorithms over a network. In International Conference on Artificial Intelligence and Statistics, pages 9238-9281. PMLR, 2023.   \n[22]  S. Ghadimi and M. Wang.  Approximation methods for bilevel programming. arXiv preprint arXiv:1802.02246, 2018.   \n[23]  R. Grazzi, L. Franceschi, M. Pontil, and S. Salzo. On the iteration complexity of hypergradient computation. In International Conference on Machine Learning, pages 3748-3758. PMLR, 2020.   \n[24]  Z. Guo, Q. Hu, L. Zhang, and T. Yang. Randomized stochastic variance-reduced methods for multi-task stochastic bilevel optimization. arXiv preprint arXiv:2105.02266, 2021.   \n[25] M. Hong, H.-T. Wai, Z. Wang, and Z. Yang. A two-timescale stochastic algorithm framework for bilevel optimization: Complexity analysis and application to actor-critic. SIAM Journal on Optimization, 33(1):147-180,2023.   \n[26]  D. Jakovetic. A unification and generalization of exact distributed first-order methods. EEE Transactions on Signal and Information Processing over Networks, 5(1):31-46, 2018.   \n[27] K. Ji, J. Yang, and Y. Liang. Bilevel optimization: Convergence analysis and enhanced design. In International Conference on Machine Learning, pages 4882-4892. PMLR, 2021.   \n[28]  A. Koloskova, T. Lin, and S. U. Stich. An improved analysis of gradient tracking for decentralized machine learning. Advances in Neural Information Processing Systems, 34:11422-11435, 2021.   \n[29]  B. Kong, S. Zhu, S. Lu, X. Huang, and K. Yuan. Decentralized bilevel optimization over graphs: Loopless algorithmic update and transient iteration complexity. arXiv preprint arXiv:2402.03167, 2024.   \n[30]  Z. Li, W. Shi, and M. Yan. A decentralized proximal-gradient method with network independent step-sizes and separated convergence rates. IEEE Transactions on Signal Processing, July 2019. early acces. Also available on arXiv:1704.07807.   \n[31]  X. Lian, C. Zhang, H. Zhang, C.-J. Hsieh, W. Zhang, and J. Liu. Can decentralized algorithms outperform centralized algorithms? A case study for decentralized parallel stochastic gradient descent. In Advances in Neural Information Processing Systems, pages 5330-5340, 2017.   \n[32]  T. Lin, S. P. Karimireddy, S.U. Stich, and M. Jaggi. Quasi-global momentum: Accelerating decentralized deep learning on heterogeneous data. In International Conference on Machine Learning, 2021.   \n[33] S.Lu,S.Zng,.CuMquillante, LHorh,BKngury,J Liu, and M.ng. A stchastic linariz augmented lagrangian method for decentralized bilevel optimization. Advances in Neural Information Processing Systems, 35:30638-30650, 2022.   \n[34]  Y. Lu and C. De Sa. Optimal complexity in decentralized training. In International Conference on Machine Learning, pages 7111-7123. PMLR, 2021.   \n[35]  D. Maclaurin, D. Duvenaud, and R. Adams. Gradient-based hyperparameter optimization through reversible learning. In International Conference on Machine Learning, pages 2113-2i22. PMLR, 2015.   \n[36]  A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations, 2018.   \n[37]  A. Nedi, A. Olshevsky, and M G. Rabbat. Ntwork topology and communication-computation tradeoffs in decentralized optimization. Proceedings of the IEEE, 106(5):953-976, 2018.   \n[38]  ANedic, A. Olshevsky, and W. Shi. Achieving geometric convergence for distributed optimization over time-varying graphs. SIAM Journal on Optimization, 27(4):2597-2633, 2017.   \n[39]  A.Nedic and A. Ozdaglar. Distributed subgradient methods for multi-agent optimization. IEEE Transactions on Automatic Control, 54(1):48-61, 2009.   \n[40]  Y. Niu, J. Xu, Y. Sun, Y. Huang, and L. Chai. Distributed stochastic bilevel optimization: Improved complexity and heterogeneity analysis. arXiv preprint arXiv:2312.14690, 2023.   \n[41]  G. Qu and N. Li. Harnessing smoothness to accelerate distributed optimization. IEEE Transactions on Control of Network Systems, 5(3):1245-1260, 2018.   \n[42] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 115:211-252, 2015.   \n[43] A. H. Sayed. Adaptive networks. Proceedings of the IEEE, 102(4):460-497, 2014.   \n[44]  A. Shaban, C.-A. Cheng, N. Hatch, and B. Boots. Truncated back-propagation for bilevel optimization. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 1723-1732. PMLR, 2019.   \n[45] W. Shi, Q. Ling, G. Wu, and W. Yin. EXTRA: An exact first-order algorithm for decentralized consensus optimization. SIAM Journal on Optimization, 25(2):944-966, 2015.   \n[46]  H. Tang, X. Lian, M. Yan, C. Zhang, and J. Liu. $\\mathrm{D^{2}}$ : Decentralized training over decentralized data. In International Conference on Machine Learning, pages 4848-4856, 2018.   \n[47]  O. Vinyals, C. Blundell, T. Lillicrap, D. Wierstra,et al. Matching networks for one shot learning. Advances in Neural Information Processing Systems, 29, 2016.   \n[48]  H. Xiao, K. Rasul, and R. Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.   \n[49] J. Xu, Y. Tian, Y. Sun, and G. Scutari. Distributed algorithms for composite optimization: Unified framework and convergence analysis. IEEE Transactions on Signal Processing, 69:3555-3570, 2021.   \n[50] J. Xu, S. Zhu, Y. C. Soh, and L. Xie. Augmented distributed gradient methods for multi-agent optimization under uncoordinated constant stepsizes. In IEEE Conference on Decision and Control (CDC), pages 2055-2060, Osaka, Japan, 2015.   \n[51]  J. Yang, K. Ji, and Y. Liang. Provably faster algorithms for bilevel optimization. Advances in Neural Information Processing Systems, 34:13670-13682, 2021.   \n[52]  S. Yang, X. Zhang, and M. Wang. Decentralized gossip-based stochastic bilevel optimization over communication networks. Advances in Neural Information Processing Systems, 35:238-252, 2022.   \n[53]  K. Yuan, S. A. Alghunaim, and X. Huang. Removing data heterogeneity influence enhances network topology dependence of decentralized SGD. Journal of Machine Learning Research, 24(280):1-53, 2023.   \n[54]  K. Yuan, S. A. Alghunaim, B. Ying, and A. H. Sayed. On the infuence of bias-correction on distributed stochastic optimization. IEEE Transactions on Signal Processing, 2020.   \n[55]  K. Yuan, Q. Ling, and W. Yin. On the convergence of decentralized gradient descent. SIAM Journal on Optimization, 26(3):1835-1854, 2016.   \n[56]  K. Yuan, B. Ying, X. Zhao, and A. H. Sayed. Exact dffusion for distributed optimization and learning - Part I: Algorithm development. IEEE Transactions on Signal Processing, 67(3):708 - 723, 2018.   \n[57]  Y. Zhang, M. T. Thai, J. Wu, and H. Gao. On the communication complexity of decentralized bilevel optimization. arXiv preprint arXiv:2311.11342, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix for \u201cSPARKLE: A Unified Single-Loop Primal-Dual Framework for Decentralized Bilevel Optimization' ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A More related works 15 ", "page_idx": 13}, {"type": "text", "text": "B More details of SPARKLE 15   \nB.1 Primal-dual deviation 15   \nB.2 Specific instances 16   \nB.3  Implementation details 17   \nC  Convergence analysis 18   \nC.1 Proof of Theorem 1 18   \nC.1.1 Notations 18   \nC.1.2 Basic transformations 19   \nC.1.3 Proof sketch 22   \nC.1.4 Technical lemmas 23   \nC.1.5 Descent lemmas for the upper-level 23   \nC.1.6 Descent lemmas for the lower- and auxiliary-level 30   \nC.1.7 Consensus error analysis 35   \nC.1.8 Proof of the main theorem 47   \nC.2 Analysis of consensus error and transient iteration complexity 54   \nC.2.1 Consensus Error 56   \nC.2.2 Essential matrix norms for analysis 59   \nC.2.3 Theoretical gap between upper-level and lower-level 59   \nC.2.4 The transient iteration complexities of some specific examples in SPARKLE. 60   \nC.3 Convergence analysis in deterministic scenarios 61   \nC.4Degenerating to single-level algorithms 62   \n0Experimental details 64   \nD.1 Synthetic bilevel optimization 64   \nD.2Hyper-cleaning on FashionMNIST dataset 65   \nD.3  Distributed policy evaluation in reinforcement learning 68   \nD.4  Decentralized meta-learning 69 ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A More related works ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Bilevel optimization. Bilevel optimization presents substantial difficulties compared to single-level optimization due to its nested structure. Estimating hyper-gradient $\\nabla\\Phi(x)$ of the upper level involves solving lower-level problems and estimating the Hessian inverse, which requires additional calculations. Many algorithms and techniques have been proposed to solve the challenge. Approximate Implicit Differentiation (AID)-based algorithms [14, 22, 23, 27] leverage the implicit gradient form of $\\nabla\\Phi(x)$ , which entails solving a linear system to obtain the Hessian-inverse-vector product. Similarly, [8, 25] utilize the Neumann series to handle the Hessian inverse. Iterative Differentiation (ITD)-based algorithms [19, 35, 14, 23, 27] use iterative methods solving the lower-level problem and then estimate the hyper-gradient through automatic differentiation. However, these approaches introduce inner steps, leading to extra computational overhead and memory spaces. [12] proposes a singlelevel algorithm called SOBA, which approximating the Hessian-inverse-vector product by solving a quadratic programming problem. A recent work [20] utilizes the Krylov subspace technique and the Lanczos process to approximate it in deterministic scenarios. For stochastic bilevel optimization, various methods have been employed to improve the convergence rate, such as momentum [7, 11] and variance reduction [51, 27, 24]. ", "page_idx": 14}, {"type": "text", "text": "Decentralized optimization.  Decentralized optimization is developed to deal with large-scale optimization problems, where datasets are distributed among multiple agents. Without a central server, each agent only gets access to its own local data and communications are limited to its neighbors in a network. Compared with centralized algorithms, decentralized ones preserve data privacy, and are more robust to contingencies in the communication network. However, due to the absence of a central server, decentralized optimization requires communication among agents, posing greater challenges for convergence, especially in the presence of severe data heterogeneity. To tackle this issue, various algorithms have emerged, such as decentralized gradient descent [39, 55], diffusion strategies [6], dual averaging [17], EXTRA [45], Exact Diffusion (a.k.a. $\\mathrm{D^{2}}$ )[56, 30, 46], gradient tracking [50, 13, 38], and decentralized ADMM [5]. In stochastic scenarios, a common method for decentralized optimization is the decentralized stochastic gradient descent (DsGD), which has gained a lot of attentions recently. It has been proved to achieve linear speedup asymptotically and shares the same asymptotic rate with centralized stochastic gradient descent [31]. ", "page_idx": 14}, {"type": "text", "text": "B More details of SPARKLE ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Primal-dual deviation ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Here we provide a detailed motivation of the update framework (6) for decentralized single-level algorithms. First, we rewrite the single-level distributed optimization problem in the following equivalent form: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x_{i}\\in\\mathbb{R}^{d}}f(x_{1},...,x_{n})=\\frac{1}{n}\\sum_{i=1}^{n}f_{i}\\left(x_{i}\\right),\\quad\\mathrm{s.t.~}x_{1}=...=x_{n},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where each $f_{i}$ is smooth and possibly non-convex. To simplify the notation, we assume that $d=1$ without loss of generality. Now we introduce three symmetric matrices $A,B,D$ such that $A$ is a doubly stochastic communication matrix with $\\rho(A)\\,<\\,1$ , and $B,D$ satisfy $\\mathrm{Null}B\\,=\\,\\mathrm{Null}D\\,=$ $\\operatorname{Span}\\!\\left\\{1_{n}\\right\\}$ . In general, $B\\left(D\\right)$ determines the topology of a connected graph $\\mathcal{G}_{B}$ $\\mathit{\\Pi}^{\\mathcal{G}_{D}})$ over agents. The constraint $B x=0$ $\\mathit{D x}=0$ ) is equivalent to: ", "page_idx": 14}, {"type": "equation", "text": "$$\nx_{i}=x_{j}{\\mathrm{~if~}}x_{i},x_{j}{\\mathrm{~are~adjacent~in~}}\\mathcal{G}_{B}\\left(\\mathcal{G}_{D}\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "To simplify the derivation, we additionally assume that $A,B,D$ are pairwise commutative. Then for $\\boldsymbol{x}=(x_{1},...,x_{n})$ ,we have: ", "page_idx": 14}, {"type": "equation", "text": "$$\nx_{1}=\\ldots=x_{n}\\Leftrightarrow B x=0\\Leftrightarrow D x=0\\Leftrightarrow A x=x.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Therefore, (10) can be equivalently reformulated as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x\\in\\mathbb{R}^{n}}f(A x),\\quad{\\mathrm{s.t.}}\\,B x=0.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We construct the augmented Lagrangian function of the problem (11) as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\rho}(x,d)=f(A x)+\\langle d,B x\\rangle+\\frac{\\rho}{2}\\|D x\\|^{2},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $x$ denotes the primal variable, $d$ denotes the dual variable or Lagrangian multiplier associated with the consensus constraint, $\\|D x\\|^{2}$ serves as the penalty term measuring the deviation from $D x\\,=\\,0$ , or equivalently $B x\\,=\\,0$ $\\rho>0$ is the penalty coefficient. Though the introduction of matrices $A,D$ is essentially a matter of equivalent substitution, it enhances the universality of the algorithm framework we get. ", "page_idx": 15}, {"type": "text", "text": "Following classical primal-dual methods, we alternately perform gradient descent on $x$ and gradient ascenton $d$ in the $k$ -th iteration: ", "page_idx": 15}, {"type": "equation", "text": "$$\nx^{k+1}=x^{k}-\\alpha(A\\nabla f(A x^{k})+B d^{k}+\\rho D^{2}x^{k}),\\quad d^{k+1}=d^{k}+\\beta B x^{k+1},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\alpha,\\beta$ denote the step-sizes. By making the change of variables ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\hat{x}^{k}=A x^{k},\\,\\hat{d}^{k}=\\sqrt{\\frac{\\alpha}{\\beta}}A d^{k},\\,\\widehat{B}=\\sqrt{\\alpha\\beta}B,\\,\\widehat C=I-\\alpha\\rho D^{2},\\,\\widehat A=A^{2},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "we obtain ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\hat{x}^{k+1}=\\widehat{C}\\hat{x}^{k}-\\alpha\\widehat{A}\\nabla f(\\hat{x}^{k})-\\widehat{B}\\hat{d}^{k},\\quad\\hat{d}^{k+1}=\\hat{d}^{k}+\\widehat{B}\\hat{x}^{k+1}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "One should note that the definition implies that $\\widehat{A},\\widehat{C}$ are doubly stochastic communication matrices under appropriate selections of $\\alpha,\\rho$ . Finally, thanks to the introduction of moving-average iteration of (12), we can obtain the framework (6) which serves as the foundation for our algorithm design. See more details in Section 2.3. ", "page_idx": 15}, {"type": "text", "text": "B.2  Specific instances ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Relation to some existing single-level algorithm frameworks According to (12), our framework at single-level is ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{x}^{k+1}=\\mathbf{C}\\mathbf{x}^{k}-\\alpha\\mathbf{A}\\mathbf{g}^{k}-\\mathbf{B}\\mathbf{d}^{k},\\mathbf{d}^{k+1}=\\mathbf{d}^{k}+\\mathbf{B}\\mathbf{x}^{k+1},k=0,1,\\ldots\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\alpha$ is the step-size, $\\mathbf{g}^{k}$ denotes the estimated gradient at the $k$ -th iteration, $\\mathbf{d}$ serves as the dual variable. ", "page_idx": 15}, {"type": "text", "text": "Replacing $\\mathbf{C}$ with CA, we get UDA[1] , and equivalently, SUDA [2]: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{x}^{k+1}=\\mathbf{CAx}^{k}-\\alpha\\mathbf{Ag}^{k}-\\mathbf{Bd}^{k},\\mathbf{d}^{k+1}=\\mathbf{d}^{k}+\\mathbf{Bx}^{k+1},k=0,1,\\ldots\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore, following SUDA, we can also recover some common state-of-the-art heterogeneity methods as follows by selecting specific $\\mathbf{A},\\mathbf{B},\\mathbf{C}$ . First, from (13) we get ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\mathbf x}^{k+2}-{\\mathbf x}^{k+1}={\\mathbf C}({\\mathbf x}^{k+1}-{\\mathbf x}^{k})-\\alpha{\\mathbf A}\\left({\\mathbf g}^{k+1}-{\\mathbf g}^{k}\\right)-{\\mathbf B}\\left({\\mathbf d}^{k+1}-{\\mathbf d}^{k}\\right)}\\\\ &{\\qquad\\qquad\\qquad={\\mathbf C}\\left({\\mathbf x}^{k+1}-{\\mathbf x}^{k}\\right)-\\alpha{\\mathbf A}\\left({\\mathbf g}^{k+1}-{\\mathbf g}^{k}\\right)-{\\mathbf B}^{2}{\\mathbf x}^{k+1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Thus, for $k\\geq0$ we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{x}^{k+2}=\\left(\\mathbf{I}-\\mathbf{B}^{2}+\\mathbf{C}\\right)\\mathbf{x}^{k+1}-\\mathbf{C}\\mathbf{x}^{k}-\\alpha\\mathbf{A}\\left(\\mathbf{g}^{k+1}-\\mathbf{g}^{k}\\right),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "with $\\mathbf{x}^{1}=\\mathbf{C}\\mathbf{x}^{0}-\\alpha\\mathbf{A}\\mathbf{g}^{0}$ ", "page_idx": 15}, {"type": "text", "text": "Some specific instances  We next show that how to choose $\\mathbf{A},\\mathbf{B},\\mathbf{C}$ to get some common heterogeneity methods. ", "page_idx": 15}, {"type": "text", "text": "\u00b7 ED: Taking $\\mathbf{A}=\\mathbf{W},\\mathbf{B}=(\\mathbf{I}-\\mathbf{W})^{1/2}$ and $\\mathbf{C}=\\mathbf{W}$ , we get ED: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{x}^{k+2}=\\mathbf{W}\\left(2\\mathbf{x}^{k+1}-\\mathbf{x}^{k}-\\alpha\\left(\\mathbf{g}^{k+1}-\\mathbf{g}^{k}\\right)\\right),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "with $\\mathbf{x}^{1}=\\mathbf{W}\\left(\\mathbf{x}^{0}-\\alpha\\mathbf{g}^{0}\\right)$ ", "page_idx": 15}, {"type": "text", "text": "\u00b7 EXTRA: Taking $\\mathbf{A}=\\mathbf{I},\\mathbf{B}=(\\mathbf{I}-\\mathbf{W})^{1/2}$ with $\\mathbf{C}=\\mathbf{W}$ , we get EXTRA: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{x}^{k+2}=\\mathbf{W}\\left(2\\mathbf{x}^{k+1}-\\mathbf{x}^{k}\\right)-\\alpha\\left(\\mathbf{g}^{k+1}-\\mathbf{g}^{k}\\right),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and $\\mathbf{x}^{1}=\\mathbf{W}\\mathbf{x}^{0}-\\alpha\\mathbf{g}^{0}$ ", "page_idx": 16}, {"type": "text", "text": "\u00b7 Adapt-then-combine gradient tracking (ATC-GT): The iteration of ATC-GT is ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{x}^{k+1}=\\mathbf{W}\\left(\\mathbf{x}^{k}-\\alpha\\mathbf{h}^{k}\\right),\\mathbf{h}^{k+1}=\\mathbf{W}\\left(\\mathbf{h}^{k}+\\mathbf{g}^{k+1}-\\mathbf{g}^{k}\\right)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "with $\\mathbf{h}^{0}=\\mathbf{W}\\mathbf{g}^{0},\\mathbf{x}^{0}=\\mathbf{W}\\mathbf{x}^{0}\\left(x_{1}^{0}=...=x_{n}^{0}\\right)$ . It follows that for $k\\geq0$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{x}^{k+2}-\\mathbf{W}\\mathbf{x}^{k+1}=\\mathbf{W}\\mathbf{x}^{k+1}-\\mathbf{W}^{2}\\mathbf{x}^{k}-\\alpha\\mathbf{W}\\left(\\mathbf{h}^{k+1}-\\mathbf{W}\\mathbf{h}^{k}\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then we obtain ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{x}^{k+2}=2\\mathbf{W}\\mathbf{x}^{k+1}-\\mathbf{W}^{2}\\mathbf{x}^{k}-\\alpha\\mathbf{W}^{2}\\left(\\mathbf{g}^{k+1}-\\mathbf{g}^{k}\\right),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "with $\\mathbf{x}^{1}=\\mathbf{W}^{2}\\mathbf{x}^{0}-\\alpha\\mathbf{W}^{2}\\mathbf{g}^{0}$ . Thus, we can take $\\mathbf{A}=\\mathbf{W^{2}},\\mathbf{B}=(\\mathbf{I}-\\mathbf{W})^{2},\\mathbf{C}=\\mathbf{W^{2}}$ to implement ATC-GT. ", "page_idx": 16}, {"type": "text", "text": "\u00b7 Semi-ATC-GT: The iteration of Semi-ATC-GT is ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{x}^{k+1}=\\mathbf{W}\\left(\\mathbf{x}^{k}-\\alpha\\mathbf{h}^{k}\\right),\\mathbf{h}^{k+1}=\\mathbf{W}\\mathbf{h}^{k}+\\mathbf{g}^{k+1}-\\mathbf{g}^{k}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "with $\\mathbf{h}^{0}=\\mathbf{W}\\mathbf{g}^{0},\\mathbf{x}^{0}=\\mathbf{W}\\mathbf{x}^{0}$ $(x_{1}^{0}=\\ldots=x_{n}^{0})$ 0. Like ATC-GT, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{x}^{k+2}=2\\mathbf{W}\\mathbf{x}^{k+1}-\\mathbf{W}^{2}\\mathbf{x}^{k}-\\alpha\\mathbf{W}\\left(\\mathbf{g}^{k+1}-\\mathbf{g}^{k}\\right),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "with $\\mathbf{x}^{1}=\\mathbf{W}^{2}\\mathbf{x}^{0}-\\alpha\\mathbf{W}\\mathbf{g}^{0}$ . Thus, we can take $\\mathbf{A}=\\mathbf{W},\\mathbf{B}=(\\mathbf{I}-\\mathbf{W})^{2},\\mathbf{C}=\\mathbf{W}^{2}$ to implement semi-ATC-GT. ", "page_idx": 16}, {"type": "text", "text": "\u00b7 Non-ATC-GT: The iteration of Non-ATC-GT is ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{x}^{k+1}=\\mathbf{W}\\mathbf{x}^{k}-\\alpha\\mathbf{h}^{k},\\mathbf{h}^{k+1}=\\mathbf{W}\\mathbf{h}^{k}+\\mathbf{g}^{k+1}-\\mathbf{g}^{k}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "with $\\mathbf{h}^{0}=\\mathbf{W}\\mathbf{g}^{0},\\mathbf{x}^{0}=\\mathbf{W}\\mathbf{x}^{0}\\left(x_{1}^{0}=...=x_{n}^{0}\\right)$ . We have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{x}^{k+2}=2\\mathbf{W}\\mathbf{x}^{k+1}-\\mathbf{W}^{2}\\mathbf{x}^{k}-\\alpha\\left(\\mathbf{g}^{k+1}-\\mathbf{g}^{k}\\right),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "with $\\mathbf{x}^{1}=\\mathbf{W}^{2}\\mathbf{x}^{0}-\\alpha\\mathbf{g}^{0}$ . Thus, we can take $\\mathbf{A}=\\mathbf{I},\\mathbf{B}=(\\mathbf{I}-\\mathbf{W})^{2},\\mathbf{C}=\\mathbf{W}^{2}$ to implement Non-ATC-GT. ", "page_idx": 16}, {"type": "text", "text": "B.3  Implementation details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Given the update method $\\mathbf{L}$ , we update the lower-level variable $y$ at the $k$ -th $(k\\geq0)$ iteration as follows. For brevity, we define $\\begin{array}{r}{y_{i}^{-\\bar{1}}=y_{i}^{0},v_{i}^{-1}=0,o_{i}^{0}=\\sum_{j=1}^{n}(\\dot{W}_{y})_{i j}v_{j}^{0}}\\end{array}$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\left(y_{i}^{k+1}=\\sum_{j=1}^{n}(W_{y})_{i j}\\left(2y_{j}^{k}-y_{j}^{k-1}-\\beta_{k}\\left(v_{i}^{k}-v_{i}^{k-1}\\right)\\right)\\right.}&{\\mathrm{if~}\\mathbf{L}=E D}\\\\ &{\\left.\\left|y_{i}^{k+1}=\\sum_{j=1}^{n}(W_{y})_{i j}\\left(2y_{j}^{k}-y_{j}^{k-1}\\right)-\\beta_{k}\\left(v_{i}^{k}-v_{i}^{k-1}\\right)\\right.}&{\\mathrm{if~}\\mathbf{L}=E X T R A}\\\\ &{\\left.\\left|y_{i}^{k+1}=\\sum_{j=1}^{n}(W_{y})_{i j}\\left(y_{j}^{k}-\\beta_{k}o_{j}^{k}\\right),o_{i}^{k+1}=\\sum_{j=1}^{n}(W_{y})_{i j}\\left(o_{j}^{k}+v_{i}^{k+1}-v_{i}^{k}\\right)\\right.}&{\\mathrm{if~}\\mathbf{L}=G T}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Similarly, we update the auxiliary variable $z$ at the $k$ -th $(k\\geq0)$ iteration as follows. For brevity, we define $\\begin{array}{r}{z_{i}^{-1}=z_{i}^{0},p_{i}^{-1}=0,h_{i}^{0}=\\bar{\\sum}_{j=1}^{n}(W_{z})_{i j}p_{j}^{0}}\\end{array}$ .Note that we use the same method $\\mathbf{L}$ to update $z$ as we do for the lower-level variable $y$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\left\\{\\begin{array}{l l}{z_{i}^{k+1}=\\sum_{j=1}^{n}(W_{z})_{i j}\\left(2z_{j}^{k}-z_{j}^{k-1}-\\gamma_{k}\\left(p_{i}^{k}-p_{i}^{k-1}\\right)\\right)}&{\\mathrm{if~}\\mathbf{L}=E D}\\\\ {z_{i}^{k+1}=\\sum_{j=1}^{n}(W_{z})_{i j}\\left(2z_{j}^{k}-z_{j}^{k-1}\\right)-\\gamma_{k}\\left(p_{i}^{k}-p_{i}^{k-1}\\right)}&{\\mathrm{if~}\\mathbf{L}=E X T R}\\end{array}\\right.}\\\\ &{\\left.z_{i}^{k+1}=\\sum_{j=1}^{n}(W_{z})_{i j}\\left(z_{j}^{k}-\\gamma_{k}h_{j}^{k}\\right),h_{i}^{k+1}=\\sum_{j=1}^{n}(W_{z})_{i j}\\left(h_{j}^{k}+p_{i}^{k+1}-p_{i}^{k}\\right)}&{\\mathrm{if~}\\mathbf{L}=G T}\\\\ &{.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Given the update method $\\mathbf{U}$ , we update the upper-level variable $x$ at the $k$ -th $(k\\geq0)$ iteration as follows. For brevity, we defne $\\begin{array}{r}{x_{i}^{-\\bar{1}}=x_{i}^{0},t_{i}^{0}=\\bar{\\sum}_{j=1}^{n}(W_{y})_{i j}r_{j}^{1}}\\end{array}$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\{x_{i}^{k+1}=\\sum_{j=1}^{n}(W_{x})_{i j}\\left(2x_{j}^{k}-x_{j}^{k-1}-\\alpha_{k}\\left(r_{i}^{k+1}-r_{i}^{k}\\right)\\right)\\right.}\\\\ &{\\left.x_{i}^{k+1}=\\sum_{j=1}^{n}(W_{x})_{i j}\\left(2x_{j}^{k}-x_{j}^{k-1}\\right)-\\alpha_{k}\\left(r_{i}^{k+1}-r_{i}^{k}\\right)\\right.}\\\\ &{\\left.x_{i}^{k+1}=\\sum_{j=1}^{n}(W_{x})_{i j}\\left(x_{j}^{k}-\\alpha_{k}t_{j}^{k}\\right),t_{i}^{k+1}=\\sum_{j=1}^{n}(W_{x})_{i j}\\left(t_{j}^{k}+r_{i}^{k+2}-r_{i}^{k+1}\\right)\\right.}\\\\ &{\\left.\\!\\cdot\\!\\cdot\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then the practical implementation of SPARKLE with mixed strategies is ", "page_idx": 17}, {"type": "table", "img_path": "g5DyqerUpX/tmp/9e890e80e0fd740d3ed898381b74b7fb551f191947108673d33efa3f05373259.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "C Convergence analysis ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "C.1 Proof of Theorem 1 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "C.1.1 Notations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We use lowercase letters to represent vectors and uppercase letters to represent matrices. Stacked vectors $[x_{1}^{\\top},...,x_{n}^{\\top}]^{\\top}$ is denoted by $\\operatorname{col}\\{x_{1},...,x_{n}\\}$ for brevity. We denote a block diagonal matrix with diagonal block $M_{i}(1\\leq i\\leq l)$ by blkdiag $\\{\\dot{M}_{1},...,M_{l}\\}$ , and a diagonal matrix with diagonal elements $d_{i}(1\\leq i\\leq k)$ by $\\mathrm{diag}\\{d_{1},...,d_{k}\\}$ . The Kronecker product operator is denoted by $\\otimes$ . For a variable $v$ , we use $v_{i}^{k}$ to represent its components at $k$ -th iteration and $i$ -th agent. ", "page_idx": 17}, {"type": "text", "text": "Moreover, we use an overbar above an iterator to denote the average over all agents. For example. $\\textstyle\\bar{x}^{k}\\,=\\,\\sum_{i=1}^{n}x_{i}^{k}/n$ Upright bold smbols aresed to enote stacked vectors ormatrices acros agents. For example, $\\mathbf{x}^{k}:=\\operatorname{col}\\{x_{1}^{k},...,x_{n}^{k}\\},{\\bar{\\mathbf{x}}}^{k}:=\\operatorname{col}\\{{\\bar{x}}^{k},...,{\\bar{x}}^{k}\\}\\,(n{\\mathrm{~times}}),\\mathbf{W}_{x}:=W_{x}\\,(x_{1}^{k},...,x_{n}^{k})$ ${\\mathbf{W}}_{x}:=W_{x}\\otimes I_{d i m(x)}$ Denote the 2-norm of a matrix by $\\Vert\\cdot\\Vert$ ", "page_idx": 17}, {"type": "text", "text": "Next, we define following $\\sigma$ -fields which will be used in our convergence analysis: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{F}_{k}=\\sigma\\left(\\mathbf{y}^{0},\\boldsymbol{\\cdot}\\cdot\\boldsymbol{\\cdot},\\mathbf{y}^{k+1},\\mathbf{z}^{0},\\ldots,\\mathbf{z}^{k+1},\\mathbf{x}^{0},\\ldots,\\mathbf{x}^{k},\\mathbf{r}^{0},\\ldots,\\mathbf{r}^{k}\\right),}\\\\ &{\\mathcal{U}_{k}=\\sigma\\left(\\mathbf{y}^{0},\\ldots,\\mathbf{y}^{k+1},\\mathbf{z}^{0},\\ldots,\\mathbf{z}^{k},\\mathbf{x}^{0},\\ldots,\\mathbf{x}^{k},\\mathbf{r}^{0},\\ldots,\\mathbf{r}^{k}\\right),}\\\\ &{\\mathcal{G}_{k}=\\sigma\\left(\\mathbf{y}^{0},\\ldots,\\mathbf{y}^{k},\\mathbf{z}^{0},\\ldots,\\mathbf{z}^{k},\\mathbf{x}^{0},\\ldots,\\mathbf{x}^{k},\\mathbf{r}^{0},\\ldots,\\mathbf{r}^{k}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and denote $\\mathbb{E}[\\cdot|\\mathcal{F}_{k}]$ by $\\mathbb{E}_{k},\\mathbb{E}[\\cdot|\\mathcal{U}_{k}]$ by $\\widetilde{\\mathbb{E}}_{k},\\mathbb{E}[\\cdot|\\mathcal{G}_{k}]$ by ${\\widehat{\\mathbb{E}}}_{k}$ for brevity. ", "page_idx": 17}, {"type": "text", "text": "Define ", "page_idx": 17}, {"type": "equation", "text": "$$\nz^{\\star}(x)=\\left(\\sum_{i=1}^{n}\\nabla_{22}^{2}g_{i}\\left(x,y^{\\star}(x)\\right)\\right)^{-1}\\left(\\sum_{i=1}^{n}\\nabla_{2}f_{i}\\left(x,y^{\\star}(x)\\right)\\right),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then, for $k=0,1,\\cdot\\cdot\\cdot$ , define: ", "page_idx": 17}, {"type": "equation", "text": "$$\nz_{\\star}^{k+1}=\\left(\\sum_{i=1}^{n}\\nabla_{22}^{2}g_{i}\\left(\\bar{x}^{k},y^{\\star}(\\bar{x}^{k})\\right)\\right)^{-1}\\left(\\sum_{i=1}^{n}\\nabla_{2}f_{i}\\left(\\bar{x}^{k},y^{\\star}(\\bar{x}^{k})\\right)\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "For convenience, we define $\\begin{array}{r}{\\mathbf{x}^{-1}=\\mathbf{x}^{0},\\mathbf{y}^{-1}=\\mathbf{y}^{0},y^{\\star}(\\bar{x}^{-1})=y^{\\star}(\\bar{x}^{0}),z_{\\star}^{0}=z_{\\star}^{1}.}\\end{array}$ ", "page_idx": 17}, {"type": "text", "text": "C.1.2 Basic transformations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We begin with conducting SUDA-like [2] transformations, which is fundamental of the following proofs. ", "page_idx": 18}, {"type": "text", "text": "Firstly, we define $\\mathbf{t}^{k}$ to track the averaged stochastic gradients among agents as follows ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{t}_{y}^{k}=\\mathbf{B}_{y}(\\mathbf{d}_{y}^{k}-\\mathbf{B}_{y}\\mathbf{y}^{k})+\\beta\\mathbf{A}_{y}\\nabla_{2}\\mathbf{g}(\\bar{\\mathbf{x}}^{k},\\bar{\\mathbf{y}}^{k}),}\\\\ &{\\mathbf{t}_{z}^{k}=\\mathbf{B}_{z}(\\mathbf{d}_{z}^{k}-\\mathbf{B}_{z}\\mathbf{z}^{k})+\\gamma\\mathbf{A}_{z}\\mathbf{p}^{k}(\\bar{\\mathbf{x}}^{k},\\bar{\\mathbf{y}}^{k+1}),}\\\\ &{\\mathbf{t}_{x}^{k}=\\mathbf{B}_{x}(\\mathbf{d}_{x}^{k}-\\mathbf{B}_{x}\\mathbf{x}^{k})+\\alpha\\mathbf{A}_{x}\\widetilde{\\nabla}\\Phi(\\bar{\\mathbf{x}}^{k}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{p}^{k}(\\bar{\\mathbf{x}}^{k},\\bar{\\mathbf{y}}^{k+1})=\\cot\\left\\{\\nabla_{22}^{2}g_{i}(\\bar{x}^{k},\\bar{y}^{k+1})z_{\\star}^{k}-\\nabla_{2}f_{i}(\\bar{x}^{k},\\bar{y}^{k+1})\\right\\}_{i=1}^{n},}\\\\ &{\\qquad\\widetilde{\\nabla}\\Phi(\\bar{\\mathbf{x}}^{k})=\\cot\\left\\{\\nabla_{1}f_{i}(\\bar{x}^{k},y^{\\star}(\\bar{x}^{k}))-\\nabla_{12}g_{i}(\\bar{x}^{k},y^{\\star}(\\bar{x}^{k}))z_{\\star}^{k+1}\\right\\}_{i=1}^{n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then the iteration of $\\mathbf{y},\\mathbf{z},\\mathbf{x}$ in Algorithm 1 can be written as: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{iteration~of~}\\mathbf{y}:\\quad}&{\\left\\{\\mathbf{y}^{k+1}=(\\mathbf{C}_{y}-\\mathbf{B}_{y}^{2})\\mathbf{y}^{k}-\\mathbf{t}_{y}^{k}-\\beta\\mathbf{A}_{y}\\left[\\mathbf{v}^{k}-\\nabla_{2}\\mathbf{g}(\\bar{\\mathbf{x}}^{k},\\bar{\\mathbf{y}}^{k})\\right],\\right.}\\\\ {\\mathrm{iteration~of~}\\mathbf{x}:\\quad}&{\\left\\{\\mathbf{t}_{y}^{k+1}=\\mathbf{t}_{y}^{k}+\\mathbf{B}_{y}^{2}\\mathbf{y}^{k}+\\beta\\mathbf{A}_{y}\\left[\\nabla_{2}\\mathbf{g}(\\bar{\\mathbf{x}}^{k+1},\\bar{\\mathbf{y}}^{k+1})-\\nabla_{2}\\mathbf{g}(\\bar{\\mathbf{x}}^{k},\\bar{\\mathbf{y}}^{k})\\right],\\right.}\\\\ {\\mathrm{iteration~of~}\\mathbf{z}:\\quad}&{\\left\\{\\mathbf{z}^{k+1}=(\\mathbf{C}_{z}-\\mathbf{B}_{z}^{2})\\mathbf{z}^{k}-\\mathbf{t}_{z}^{k}-\\gamma\\mathbf{A}_{z}\\left[\\mathbf{p}^{k}-\\mathbf{p}^{k}(\\bar{\\mathbf{x}}^{k},\\bar{\\mathbf{y}}^{k+1})\\right],}\\\\ {\\mathrm{it}_{z}^{k+1}=\\mathbf{t}_{z}^{k}+\\mathbf{B}_{z}^{2}\\mathbf{z}^{k}+\\gamma\\mathbf{A}_{z}\\left[\\mathbf{p}^{k+1}(\\bar{\\mathbf{x}}^{k+1},\\bar{\\mathbf{y}}^{k+2})-\\mathbf{p}^{k}(\\bar{\\mathbf{x}}^{k},\\bar{\\mathbf{y}}^{k+1})\\right],}\\\\ {\\mathrm{iteration~of~}\\mathbf{x}:\\quad}&{\\left\\{\\mathbf{x}^{k+1}=(\\mathbf{C}_{x}-\\mathbf{B}_{x}^{2})\\mathbf{x}^{k}-\\mathbf{t}_{x}^{k}-\\alpha\\mathbf{A}_{x}\\left[\\mathbf{r}^{k+1}-\\widetilde{\\nabla}\\Phi(\\bar{\\mathbf{x}}^{k})\\right],\\right.}\\\\ {\\mathrm{iteration~of~}\\mathbf{x}:\\quad}&{\\left\\{\\mathbf{t}_{x}^{k+1}=\\mathbf{t}_{x}^{k}+\\mathbf{B}_{x}^{2}\\mathbf \n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Next, we present the transformation of the matrices $A,B,C$ . For a communication matrix $W_{s}$ for the variable $\\bar{s}\\in\\{x,y,z\\}$ satisfying Assumption 2, there exists an orthogonal matrix $U$ such that: ", "page_idx": 18}, {"type": "equation", "text": "$$\nW=U_{s}\\hat{\\Lambda}_{s}U_{s}^{\\top}=\\left[\\frac{1}{\\sqrt{n}}\\mathbf{1}\\quad\\hat{U}_{s}\\right]\\left[\\!\\!\\begin{array}{c c}{1}&{0}\\\\ {0}&{\\Lambda_{s}\\!\\!\\Bigg]\\left[\\!\\!\\begin{array}{c}{\\frac{1}{\\sqrt{n}}\\mathbf{1}^{\\top}}\\\\ {\\hat{U}_{s}^{\\top}}\\end{array}\\!\\!\\right],\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\Lambda_{s}=\\mathrm{diag}\\{\\lambda_{s i}\\}_{i=2}^{n},\\hat{U}_{s}^{\\top}\\in\\mathbb{R}^{n\\times(n-1)}$ satisfies $\\begin{array}{r}{\\hat{U}_{s}\\hat{U}_{s}^{\\top}=I_{n}-\\frac{1}{n}\\mathbf{1}_{n}\\mathbf{1}_{n}^{\\top}}\\end{array}$ and $\\mathbf{1}_{n}^{\\top}\\hat{U}_{s}=0$ . Then it follows that: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbf{W}_{s}=\\mathbf{U}_{s}\\hat{\\mathbf{A}}_{s}\\mathbf{U}_{s}^{\\top}=\\left[\\frac{1}{\\sqrt{n}}\\mathbf{1}\\otimes I_{d i m(s)}\\quad\\hat{\\mathbf{U}}_{s}\\right]\\left[I_{d i m(s)}\\quad0\\right]\\left[\\frac{1}{\\sqrt{n}}\\mathbf{1}^{\\top}\\otimes I_{d i m(s)}\\right],\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $d i m(s)$ denotes the dimension of the corresponding variable, ${\\bf{\\cal N}}_{s}\\ =\\ \\Lambda_{s}\\,\\otimes\\,I_{d i m(s)}\\ \\in$ $\\mathbb{R}^{d(n-1)\\times[d i m(s)\\cdot(n-1)]}$ \uff0c $\\mathbf{U}_{s}~\\in~\\mathbb{R}^{[d i m(s)\\cdot n]\\times[d i m(s)\\cdot n]}$ is an orthogonal matrix, and $\\hat{\\mathbf{U}}_{s}\\;=\\;\\hat{U}_{s}\\;\\otimes$ $I_{d i m(s)}\\in\\mathbb{R}^{[d i m(s)\\cdot n]\\times[d i m(s)\\cdot(n-1)]}$ satisies: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{U}}_{s}^{\\top}\\hat{\\mathbf{U}}_{s}=I_{d i m(s)\\cdot(n-1)},\\quad\\hat{\\mathbf{U}}_{s}\\hat{\\mathbf{U}}_{s}^{\\top}=\\left[I_{n}-\\frac{1}{n}\\mathbf{1}^{\\top}\\right]\\otimes I_{d i m(s)},\\quad(\\mathbf{1}^{\\top}\\otimes I_{d i m(s)})\\hat{\\mathbf{U}}_{s}=\\mathbf{0}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Now we add subscript $s$ for $\\mathbf{W}_{s}$ . Then, as ${\\bf A}_{s},{\\bf B}_{s}^{2},{\\bf C}_{s}$ can be expressed as a polynomial of $\\mathbf{W}_{s}$ for $s\\in\\{x,y,z\\}$ according to Assumption 2, we have the orthogonal decomposition: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbf{A}_{s}=\\mathbf{U}_{s}\\hat{\\mathbf{A}}_{s a}\\mathbf{U}_{s}^{\\top}=\\left[\\frac{1}{\\sqrt{n}}\\mathbf{1}\\otimes I_{\\mathrm{dim}(s)}}&{\\hat{\\mathbf{U}}_{s}\\right]\\left[\\cfrac{I_{\\mathrm{dim}(s)}}{\\mathbf{0}}\\quad\\mathbf{0}\\right]\\left[\\cfrac{1}{\\sqrt{n}}\\mathbf{1}^{\\top}\\otimes I_{\\mathrm{dim}(s)}\\right],}\\\\ {\\mathbf{B}_{s}^{2}=\\mathbf{U}_{s}\\hat{\\mathbf{A}}_{s b}^{2}\\mathbf{U}_{s}^{\\top}=\\left[\\cfrac{1}{\\sqrt{n}}\\mathbf{1}\\otimes I_{\\mathrm{dim}(s)}\\quad\\hat{\\mathbf{U}}_{s}\\right]\\left[\\cfrac{\\mathbf{0}}{\\mathbf{0}}\\quad\\mathbf{0}_{s b}^{2}\\right]\\left[\\cfrac{1}{\\sqrt{n}}\\mathbf{1}^{\\top}\\otimes I_{\\mathrm{dim}(s)}\\right],}\\\\ {\\mathbf{C}_{s}=\\mathbf{U}_{s}\\hat{\\mathbf{A}}_{s c}\\mathbf{U}_{s}^{\\top}=\\left[\\cfrac{1}{\\sqrt{n}}\\mathbf{1}\\otimes I_{\\mathrm{dim}(s)}\\quad\\hat{\\mathbf{U}}_{s}\\right]\\left[\\cfrac{I_{\\mathrm{dim}(s)}}{\\mathbf{0}}\\quad\\mathbf{0}_{s c}\\right]\\left[\\cfrac{1}{\\sqrt{n}}\\mathbf{1}^{\\top}\\otimes I_{\\mathrm{dim}(s)}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\Lambda_{s a}=\\underbrace{\\mathrm{diag}\\{\\lambda_{s a,i}\\}_{i=2}^{n}}_{\\Lambda_{s a}}\\otimes I_{\\mathrm{dim}(s)},\\quad\\Lambda_{s b}=\\underbrace{\\mathrm{diag}\\{\\lambda_{s b,i}\\}_{i=2}^{n}}_{\\Lambda_{s b}}\\otimes I_{\\mathrm{dim}(s)},\\quad\\Lambda_{s c}=\\underbrace{\\mathrm{diag}\\{\\lambda_{s c,i}\\}_{i=2}^{n}}_{\\Lambda_{s c}}\\otimes I_{\\mathrm{dim}(s)}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Moreover,each $\\Lambda_{s b}$ is positive definite because of the null space condition in Assumption 2. Then, multilying both sides of (18),(19 and (20)by $\\mathbf{U}_{y}^{\\top},\\mathbf{U}_{z}^{\\top},\\mathbf{U}_{x}^{\\top}$ respetively,wege: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{iter.~of~}\\mathbf{y}:\\left\\{\\begin{array}{l l}{\\mathbf{U}_{y}^{\\top}\\mathbf{y}^{k+1}=(\\hat{\\mathbf{A}}_{y c}-\\hat{\\mathbf{A}}_{y b}^{2})\\mathbf{U}_{y}^{\\top}\\mathbf{y}^{k}-\\mathbf{U}_{y}^{\\top}\\mathbf{t}_{y}^{k}-\\beta\\hat{\\mathbf{A}}_{y a}\\mathbf{U}_{y}^{\\top}\\left[\\mathbf{v}^{k}-\\nabla_{2}\\mathbf{g}(\\bar{\\mathbf{x}}^{k},\\bar{\\mathbf{y}}^{k})\\right],}\\\\ {\\mathbf{U}_{y}^{\\top}\\mathbf{t}_{y}^{k+1}=\\mathbf{U}_{y}^{\\top}\\mathbf{t}_{y}^{k}+\\hat{\\mathbf{A}}_{y b}^{2}\\mathbf{U}_{y}^{\\top}\\mathbf{y}^{k}+\\beta\\hat{\\mathbf{A}}_{y a}\\mathbf{U}_{y}^{\\top}\\left[\\nabla_{2}\\mathbf{g}(\\bar{\\mathbf{x}}^{k+1},\\bar{\\mathbf{y}}^{k+1})-\\nabla_{2}\\mathbf{g}(\\bar{\\mathbf{x}}^{k},\\bar{\\mathbf{y}}^{k})\\right],}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{~r.~of~}\\mathbf{z}:\\left\\{\\mathbf{U}_{z}^{\\top}\\mathbf{z}^{k+1}=(\\hat{\\mathbf{A}}_{z c}-\\hat{\\mathbf{A}}_{z b}^{2})\\mathbf{U}_{z}^{\\top}\\mathbf{z}^{k}-\\mathbf{U}_{z}^{\\top}\\mathbf{t}_{z}^{k}-\\gamma\\hat{\\mathbf{A}}_{z a}\\mathbf{U}_{z}^{\\top}\\left[\\mathbf{p}^{k}-\\mathbf{p}^{k}(\\bar{\\mathbf{x}}^{k},\\bar{\\mathbf{y}}^{k+1})\\right],\\right.}\\\\ {\\left.\\nabla_{z}^{\\top}\\mathbf{t}_{z}^{k+1}=\\mathbf{U}_{z}^{\\top}\\mathbf{t}_{z}^{k}+\\hat{\\mathbf{A}}_{z b}^{2}\\mathbf{U}_{z}^{\\top}\\mathbf{z}^{k}+\\gamma\\hat{\\mathbf{A}}_{z a}\\mathbf{U}_{z}^{\\top}\\left[\\mathbf{p}^{k+1}(\\bar{\\mathbf{x}}^{k+1},\\bar{\\mathbf{y}}^{k+2})-\\mathbf{p}^{k}(\\bar{\\mathbf{x}}^{k},\\bar{\\mathbf{y}}^{k+1})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{iter.~of~}\\mathbf{x}:\\left\\{\\begin{array}{l l}{\\mathbf{U}_{x}^{\\top}\\mathbf{x}^{k+1}=(\\hat{\\mathbf{A}}_{x c}-\\hat{\\mathbf{A}}_{x b}^{2})\\mathbf{U}_{x}^{\\top}\\mathbf{x}^{k}-\\mathbf{U}_{x}^{\\top}\\mathbf{t}_{x}^{k}-\\alpha\\hat{\\mathbf{A}}_{x a}\\mathbf{U}^{\\top}\\left[\\mathbf{r}^{k+1}-\\widetilde{\\nabla}\\Phi(\\bar{\\mathbf{x}}^{k})\\right],}\\\\ {\\mathbf{U}_{x}^{\\top}\\mathbf{t}_{x}^{k+1}=\\mathbf{U}_{x}^{\\top}\\mathbf{t}_{x}^{k}+\\hat{\\mathbf{A}}_{x b}^{2}\\mathbf{U}_{x}^{\\top}\\mathbf{x}^{k}+\\alpha\\hat{\\mathbf{A}}_{x a}\\mathbf{U}_{x}^{\\top}\\left[\\widetilde{\\nabla}\\Phi(\\bar{\\mathbf{x}}^{k+1})-\\widetilde{\\nabla}\\Phi(\\bar{\\mathbf{x}}^{k})\\right].}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then, due to Eq. (17), we have: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\mathbf{1}^{\\top}\\otimes I_{d})\\mathbf{t}_{y}^{k}=(\\mathbf{1}^{\\top}\\otimes I_{d})\\left(\\mathbf{B}_{y}(\\mathbf{d}_{y}^{k}-\\mathbf{B}_{y}\\mathbf{y}^{k})+\\beta\\mathbf{A}_{y}\\nabla_{2}\\mathbf{g}(\\bar{\\mathbf{x}}^{k},\\bar{\\mathbf{y}}^{k})\\right)}\\\\ &{\\qquad\\qquad\\qquad=n\\beta\\nabla_{2}g(\\bar{x}^{k},\\bar{y}^{k}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\mathbf{1}^{\\top}\\otimes I_{d})\\mathbf{t}_{z}^{k}=(\\mathbf{1}^{\\top}\\otimes I_{d})\\left(\\mathbf{B}_{z}(\\mathbf{d}_{z}^{k}-\\mathbf{B}_{z}\\mathbf{z}^{k})+\\gamma\\mathbf{A}_{z}\\mathbf{p}^{k}(\\bar{\\mathbf{x}}^{k},\\bar{\\mathbf{y}}^{k+1})\\right)}\\\\ &{\\qquad\\qquad\\qquad=\\gamma\\displaystyle\\sum_{i=1}^{n}\\left[\\nabla_{22}^{2}g_{i}(\\bar{x}^{k},\\bar{y}^{k+1})z_{\\star}^{k}-\\nabla_{2}f_{i}(\\bar{x}^{k},\\bar{y}^{k+1})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\mathbf{1}^{\\top}\\otimes I_{d})\\mathbf{t}_{x}^{k}=(\\mathbf{1}^{\\top}\\otimes I_{d})\\left(\\mathbf{B}_{x}(\\mathbf{d}_{x}^{k}-\\mathbf{B}_{x}\\mathbf{x}^{k})+\\alpha\\mathbf{A}_{x}\\widetilde{\\nabla}\\Phi(\\bar{\\mathbf{x}}^{k})\\right)}\\\\ &{\\qquad\\qquad\\qquad=\\alpha\\displaystyle\\sum_{i=1}^{n}\\left[\\nabla_{1}f_{i}(\\bar{x}^{k},y^{\\star}(\\bar{x}^{k}))-\\nabla_{12}g_{i}(\\bar{x}^{k},y^{\\star}(\\bar{x}^{k}))z_{\\star}^{k+1}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Substituting (25), (26), (27) into (22), (23), (24), respectively. Then use (21) and the structure of $\\hat{\\mathbf{U}}_{y},\\hat{\\mathbf{U}}_{z},\\hat{\\mathbf{U}_{x}}$ wehave ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{iter.~of~}\\mathbf{y}:\\left\\{\\begin{array}{l l}{\\bar{y}^{k+1}=\\bar{y}^{k}-\\beta\\bar{v}^{k},}\\\\ {\\hat{\\mathbf{U}}_{y}^{\\top}\\mathbf{y}^{k+1}=(\\mathbf{A}_{y c}-\\mathbf{A}_{y b}^{2})\\hat{\\mathbf{U}}_{y}^{\\top}\\mathbf{y}^{k}-\\hat{\\mathbf{U}}_{y}^{\\top}\\mathbf{t}_{y}^{k}-\\beta\\mathbf{A}_{y a}\\hat{\\mathbf{U}}_{y}^{\\top}\\left[\\mathbf{v}^{k}-\\nabla_{2}\\mathbf{g}(\\bar{\\mathbf{x}}^{k},\\bar{\\mathbf{y}}^{k})\\right],}\\\\ {\\hat{\\mathbf{U}}_{y}^{\\top}\\mathbf{t}_{y}^{k+1}=\\hat{\\mathbf{U}}_{y}^{\\top}\\mathbf{t}_{y}^{k}+\\Lambda_{y b}^{2}\\hat{\\mathbf{U}}_{y}^{\\top}\\mathbf{y}^{k}+\\beta\\mathbf{A}_{y a}\\hat{\\mathbf{U}}_{y}^{\\top}\\left[\\nabla_{2}\\mathbf{g}(\\bar{\\mathbf{x}}^{k+1},\\bar{\\mathbf{y}}^{k+1})-\\nabla_{2}\\mathbf{g}(\\bar{\\mathbf{x}}^{k},\\bar{\\mathbf{y}}^{k})\\right],}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{sf}\\ \\mathbf{z}:\\left\\{\\begin{array}{l l}{\\bar{z}^{k+1}=\\bar{z}^{k}-\\gamma\\bar{p}^{k},}\\\\ {\\hat{\\mathbf{U}}_{z}^{\\top}\\mathbf{z}^{k+1}=(\\mathbf{A}_{z c}-\\mathbf{A}_{z b}^{2})\\hat{\\mathbf{U}}_{z}^{\\top}\\mathbf{z}^{k}-\\hat{\\mathbf{U}}_{z}^{\\top}\\mathbf{t}_{z}^{k}-\\gamma\\mathbf{A}_{z a}\\hat{\\mathbf{U}}_{z}^{\\top}\\left[\\mathbf{p}^{k}-\\mathbf{p}^{k}(\\bar{\\mathbf{x}}^{k},\\bar{\\mathbf{y}}^{k+1})\\right],}\\\\ {\\hat{\\mathbf{U}}_{z}^{\\top}\\mathbf{t}_{z}^{k+1}=\\hat{\\mathbf{U}}_{z}^{\\top}\\mathbf{t}_{z}^{k}+\\mathbf{A}_{z b}^{2}\\hat{\\mathbf{U}}_{z}^{\\top}\\mathbf{z}^{k}+\\gamma\\mathbf{A}_{z a}\\hat{\\mathbf{U}}_{z}^{\\top}\\left[\\mathbf{p}^{k+1}(\\bar{\\mathbf{x}}^{k+1},\\bar{\\mathbf{y}}^{k+2})-\\mathbf{p}^{k}(\\bar{\\mathbf{x}}^{k},\\bar{\\mathbf{y}}^{k+1})\\right],}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\{\\begin{array}{l l}{\\bar{x}^{k+1}=\\bar{x}^{k}-\\alpha\\bar{r}^{k+1},}\\\\ {\\hat{\\mathbf{U}}_{x}^{\\top}\\mathbf{x}^{k+1}=(\\hat{\\mathbf{A}}_{x c}-\\hat{\\mathbf{A}}_{x b}^{2})\\hat{\\mathbf{U}}_{x}^{\\top}\\mathbf{x}^{k}-\\hat{\\mathbf{U}}_{x}^{\\top}\\mathbf{t}_{x}^{k}-\\alpha\\hat{\\mathbf{A}}_{x a}\\hat{\\mathbf{U}}_{x}^{\\top}\\left[\\mathbf{r}^{k+1}-\\widetilde{\\nabla}\\Phi(\\bar{\\mathbf{x}}^{k})\\right],}\\\\ {\\hat{\\mathbf{U}}_{x}^{\\top}\\mathbf{t}_{x}^{k+1}=\\hat{\\mathbf{U}}_{x}^{\\top}\\mathbf{t}_{x}^{k}+\\hat{\\mathbf{A}}_{x b}^{2}\\hat{\\mathbf{U}}_{x}^{\\top}\\mathbf{x}^{k}+\\alpha\\hat{\\mathbf{A}}_{x a}\\hat{\\mathbf{U}}_{x}^{\\top}\\left[\\widetilde{\\nabla}\\Phi(\\bar{\\mathbf{x}}^{k+1})-\\widetilde{\\nabla}\\Phi(\\bar{\\mathbf{x}}^{k})\\right].}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The above three equations are equivalent to: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left[\\begin{array}{c}{\\hat{\\mathbf{U}}_{y}^{\\top}\\mathbf{y}^{k+1}}\\\\ {\\Lambda_{y b}^{-1}\\hat{\\mathbf{U}}_{y}^{\\top}\\mathbf{t}_{y}^{k+1}}\\end{array}\\right]=\\left[\\begin{array}{c c}{\\mathbf{A}_{y c}-\\Lambda_{y b}^{2}}&{-\\Lambda_{y b}}\\\\ {\\Lambda_{y b}}&{\\mathbf{I}}\\end{array}\\right]\\left[\\begin{array}{c}{\\hat{\\mathbf{U}}_{y}^{\\top}\\mathbf{y}^{k}}\\\\ {\\mathbf{A}_{y b}^{-1}\\hat{\\mathbf{U}}_{y}^{\\top}\\mathbf{t}_{k}^{y}}\\end{array}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad-\\beta\\left[\\begin{array}{c}{\\Lambda_{y a}\\hat{\\mathbf{U}}_{y}^{\\top}\\left[\\mathbf{v}^{k}-\\nabla_{2}\\mathbf{g}(\\bar{\\mathbf{x}}^{k},\\bar{\\mathbf{y}}^{k})\\right]}\\\\ {\\Lambda_{y b}^{-1}\\Lambda_{y a}\\hat{\\mathbf{U}}_{y}^{\\top}\\left[\\nabla_{2}\\mathbf{g}(\\bar{\\mathbf{x}}^{k+1},\\bar{\\mathbf{y}}^{k+1})-\\nabla_{2}\\mathbf{g}(\\bar{\\mathbf{x}}^{k},\\bar{\\mathbf{y}}^{k})\\right]}\\end{array}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left[\\begin{array}{c}{\\hat{\\mathbf{U}}_{z}^{\\top}\\mathbf{z}^{k+1}}\\\\ {\\Lambda_{z b}^{-1}\\hat{\\mathbf{U}}_{z}^{\\top}\\mathbf{t}_{z}^{k+1}}\\end{array}\\right]=\\!\\left[\\begin{array}{c c}{\\mathbf{A}_{z c}-\\mathbf{A}_{z b}^{2}}&{-\\mathbf{A}_{z b}}\\\\ {\\mathbf{A}_{z b}}&{\\mathbf{I}}\\end{array}\\right]\\left[\\begin{array}{c}{\\hat{\\mathbf{U}}_{z}^{\\top}\\mathbf{z}^{k}}\\\\ {\\mathbf{A}_{z b}^{-1}\\hat{\\mathbf{U}}_{z}^{\\top}\\mathbf{t}_{k}^{z}}\\end{array}\\right]}\\\\ &{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! \n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left[\\begin{array}{c}{\\hat{\\mathbf{U}}_{x}^{\\top}\\mathbf{x}^{k+1}}\\\\ {\\Lambda_{x b}^{-1}\\hat{\\mathbf{U}}_{x}^{\\top}\\mathbf{t}_{x}^{k+1}}\\end{array}\\right]=\\left[\\begin{array}{c c}{\\Lambda_{x c}-\\Lambda_{x b}^{2}}&{-\\Lambda_{x b}}\\\\ {\\Lambda_{x b}}&{\\mathbf{I}}\\end{array}\\right]\\left[\\begin{array}{c}{\\hat{\\mathbf{U}}_{x}^{\\top}\\mathbf{x}^{k}}\\\\ {\\Lambda_{x b}^{-1}\\hat{\\mathbf{U}}_{x}^{\\top}\\mathbf{t}_{k}^{x}}\\end{array}\\right]}\\\\ &{\\quad\\quad\\quad\\quad\\quad-\\alpha\\left[\\begin{array}{c}{\\Lambda_{x a}\\hat{\\mathbf{U}}_{x}^{\\top}\\left[\\mathbf{r}^{k+1}-\\widetilde{\\nabla}\\Phi(\\bar{\\mathbf{x}}^{k})\\right]}\\\\ {\\Lambda_{x b}^{-1}\\Lambda_{x a}\\hat{\\mathbf{U}}_{x}^{\\top}\\left[\\widetilde{\\nabla}\\Phi(\\bar{\\mathbf{x}}^{k+1})-\\widetilde{\\nabla}\\Phi(\\bar{\\mathbf{x}}^{k})\\right]}\\end{array}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For $\\mathbf{s}\\in\\{\\mathbf{x},\\mathbf{y},\\mathbf{z}\\}$ , define: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{e}_{s}^{k}=\\left[\\begin{array}{c}{\\hat{\\mathbf{U}}_{s}^{\\top}\\mathbf{s}^{k}}\\\\ {\\Lambda_{s b}^{-1}\\hat{\\mathbf{U}}_{s}^{\\top}\\mathbf{t}_{s}^{k}}\\end{array}\\right],\\quad\\mathbf{M}_{s}=\\left[\\begin{array}{c c}{\\mathbf{\\Lambda}\\mathbf{\\Lambda}\\mathbf{\\Lambda}_{\\mathbf{A}s}-\\mathbf{\\Lambda}\\mathbf{A}_{s b}^{2}}&{-\\mathbf{A}_{s b}}\\\\ {\\mathbf{\\Lambda}\\mathbf{\\Lambda}_{\\mathbf{A}s}}&{\\mathbf{I}}\\end{array}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then (28), (29), (30) are respectively equivalent to: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\mathbf e}_{y}^{k+1}={\\mathbf M}_{y}{\\mathbf e}_{y}^{k}-\\beta\\left[\\begin{array}{c}{{\\mathbf A}_{y a}\\hat{{\\mathbf U}}_{y}^{\\top}\\left[{\\mathbf v}^{k}-\\nabla_{2}{\\mathbf g}(\\bar{{\\mathbf x}}^{k},\\bar{{\\mathbf y}}^{k})\\right]}\\\\ {{\\mathbf\\Lambda}_{y b}^{-1}{\\mathbf A}_{y a}\\hat{{\\mathbf U}}_{y}^{\\top}\\left[\\nabla_{2}{\\mathbf g}(\\bar{{\\mathbf x}}^{k+1},\\bar{{\\mathbf y}}^{k+1})-\\nabla_{2}{\\mathbf g}(\\bar{{\\mathbf x}}^{k},\\bar{{\\mathbf y}}^{k})\\right]}\\end{array}\\right],}\\\\ &{{\\mathbf e}_{z}^{k+1}={\\mathbf M}_{z}{\\mathbf e}_{z}^{k}-\\gamma\\left[\\begin{array}{c}{{\\mathbf A}_{z a}\\hat{{\\mathbf U}}_{z}^{\\top}\\left[{\\mathbf p}^{k}-{\\mathbf p}^{k}(\\bar{{\\mathbf x}}^{k},\\bar{{\\mathbf y}}^{k+1})\\right]}\\\\ {{\\mathbf\\Lambda}_{z b}^{-1}{\\mathbf A}_{z a}\\hat{{\\mathbf U}}_{z}^{\\top}\\left[{\\mathbf p}^{k+1}(\\bar{{\\mathbf x}}^{k+1},\\bar{{\\mathbf y}}^{k+2})-{\\mathbf p}^{k}(\\bar{{\\mathbf x}}^{k},\\bar{{\\mathbf y}}^{k+1})\\right]}\\end{array}\\right],}\\\\ &{{\\mathbf e}_{x}^{k+1}={\\mathbf M}_{x}{\\mathbf e}_{k}^{x}-\\alpha\\left[\\begin{array}{c}{{\\mathbf A}_{x a}\\hat{{\\mathbf U}}_{x}^{\\top}\\left[{\\mathbf r}^{k+1}-\\tilde{\\nabla}\\Phi(\\bar{{\\mathbf x}}^{k})\\right]}\\\\ {{\\mathbf\\Lambda}_{x b}^{-1}{\\mathbf A}_{x a}\\hat{{\\mathbf U}}_{x}^{\\top}\\left[\\tilde{\\nabla}\\Phi(\\bar{{\\mathbf x}}^{k+1})-\\tilde{\\nabla}\\Phi(\\bar{{\\mathbf x}}^{k})\\right]}\\end{array}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Assumption 2, 3 imply that all eigenvalues of ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left[\\begin{array}{l l l}{\\mathrm{diag}\\{0,\\Lambda_{s c}-\\Lambda_{s b}^{2}\\}}&{-\\mathrm{diag}\\{0,\\Lambda_{s b}\\}}\\\\ {\\mathrm{diag}\\{0,\\Lambda_{s b}\\}}&{\\mathrm{diag}\\{0,1,...,1\\}}\\end{array}\\right]}\\\\ &{=\\left[\\begin{array}{l l}{U_{s}^{\\top}}&\\\\ &{U_{s}^{\\top}}\\end{array}\\right]\\left[\\begin{array}{l l}{C_{s}-\\frac{1}{n}1_{n}1_{n}^{\\top}-B_{s}^{2}}&{-B_{s}}\\\\ &{B_{s}}&{I_{n}-\\frac{1}{n}1_{n}1_{n}^{\\top}}\\end{array}\\right]\\left[\\begin{array}{l l}{U_{s}}&\\\\ &{U_{s}}\\end{array}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "are strictly less than one in magnitude. Thus by symmetrically exchanging columns and rows of the matrix, we know that equivalently, all eigenvalues of ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left[\\begin{array}{c c}{\\Lambda_{s c}-\\Lambda_{s b}^{2}}&{-\\Lambda_{s b}}\\\\ {\\Lambda_{s b}}&{I_{n-1}}\\end{array}\\right]\\mathrm{~and~}{\\bf M}_{s}=\\left[\\begin{array}{c c}{\\Lambda_{s c}-\\Lambda_{s b}^{2}}&{-\\Lambda_{s b}}\\\\ {\\Lambda_{s b}}&{I_{n-1}\\otimes I_{d i m(s)}}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "are strictly less than one in magnitude,. ", "page_idx": 20}, {"type": "text", "text": "Then according to Lemma 3, for $s\\in\\{x,y,z\\}$ \uff0c $\\mathbf{M}_{s}$ has the similarity transformation: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbf{M}_{s}=\\mathbf{O}_{s}\\mathbf{\\Gamma}_{s}\\mathbf{O}_{s}^{-1},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\mathbf{O}_{s}$ is invertible and $\\|\\mathbf{T}_{s}\\|<1$ . Moreover, we define $\\hat{\\mathbf{e}}_{s}^{k}=\\mathbf{O}_{s}^{-1}\\mathbf{e}_{s}^{k}$ . It yields ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\mathbf{e}}_{y}^{k+1}=\\mathbf{r}_{y}\\hat{\\mathbf{e}}_{y}^{k}-\\beta\\mathbf{O}_{y}^{-1}\\left[\\begin{array}{c}{\\Delta_{y a}\\hat{\\mathbf{U}}_{y}^{\\top}\\left[\\mathbf{v}^{k}-\\nabla_{2}\\mathbf{g}(\\bar{\\mathbf{x}}^{k},\\bar{\\mathbf{y}}^{k})\\right]}\\\\ {\\Lambda_{y b}^{-1}\\Lambda_{y a}\\hat{\\mathbf{U}}_{y}^{\\top}\\left[\\nabla_{2}\\mathbf{g}(\\bar{\\mathbf{x}}^{k+1},\\bar{\\mathbf{y}}^{k+1})-\\nabla_{2}\\mathbf{g}(\\bar{\\mathbf{x}}^{k},\\bar{\\mathbf{y}}^{k})\\right]}\\end{array}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\mathbf{e}}_{z}^{k+1}=\\mathbf{P}_{y}\\hat{\\mathbf{e}}_{z}^{k}-\\gamma\\mathbf{O}_{z}^{-1}\\left[\\begin{array}{c}{\\mathbf{A}_{z a}\\hat{\\mathbf{U}}_{z}^{\\top}\\left[\\mathbf{p}^{k}-\\mathbf{p}^{k}(\\bar{\\mathbf{x}}^{k},\\bar{\\mathbf{y}}^{k+1})\\right]}\\\\ {\\mathbf{A}_{z b}^{-1}\\mathbf{A}_{z a}\\hat{\\mathbf{U}}_{z}^{\\top}\\left[\\mathbf{p}^{k+1}(\\bar{\\mathbf{x}}^{k+1},\\bar{\\mathbf{y}}^{k+2})-\\mathbf{p}^{k}(\\bar{\\mathbf{x}}^{k},\\bar{\\mathbf{y}}^{k+1})\\right]}\\end{array}\\right],}\\\\ &{\\hat{\\mathbf{e}}_{x}^{k+1}=\\mathbf{I}_{x}\\hat{\\mathbf{e}}_{x}^{k}-\\alpha\\mathbf{O}_{x}^{-1}\\left[\\begin{array}{c}{\\mathbf{A}_{x a}\\hat{\\mathbf{U}}_{x}^{\\top}\\left[\\mathbf{r}^{k+1}-\\widetilde{\\nabla}\\Phi(\\bar{\\mathbf{x}}^{k})\\right]}\\\\ {\\mathbf{A}_{x b}^{-1}\\mathbf{A}_{x a}\\hat{\\mathbf{U}}_{x}^{\\top}\\left[\\widetilde{\\nabla}\\Phi(\\bar{\\mathbf{x}}^{k+1})-\\widetilde{\\nabla}\\Phi(\\bar{\\mathbf{x}}^{k})\\right]}\\end{array}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then, for $\\mathbf{s}\\in\\{\\mathbf{x},\\mathbf{y},\\mathbf{z}\\}$ , the consensus errors between different agents have the upper bound of: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\big\\|\\mathbf{s}^{k}-\\bar{\\mathbf{s}}^{k}\\big\\|^{2}=\\|\\hat{\\mathbf{U}}_{s}^{\\top}\\mathbf{s}^{k}\\|^{2}\\leq\\|\\mathbf{e}_{s}^{k}\\|^{2}\\leq\\|\\mathbf{O}_{s}\\|^{2}\\|\\hat{\\mathbf{e}}_{s}^{k}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Thus, we can define: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\Delta_{k}=\\kappa^{2}\\|\\mathbf{O}_{x}\\|^{2}\\|\\hat{\\mathbf{e}}_{x}^{k}\\|^{2}+\\kappa^{2}\\|\\mathbf{O}_{y}\\|^{2}\\|\\hat{\\mathbf{e}}_{y}^{k+1}\\|^{2}+\\|\\mathbf{O}_{z}\\|^{2}\\|\\hat{\\mathbf{e}}_{z}^{k+1}\\|^{2}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "to measure the consensus error during the iteration. ", "page_idx": 21}, {"type": "text", "text": "We also define ", "page_idx": 21}, {"type": "equation", "text": "$$\nI_{k}=\\|\\bar{z}^{k+1}-z_{\\star}^{k+1}\\|^{2}+\\kappa^{2}\\|\\bar{y}^{k+1}-y^{\\star}(\\bar{x}^{k})\\|^{2},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "to measure the estimation accuracy of the lower- and auxiliary-level problems. ", "page_idx": 21}, {"type": "text", "text": "C.1.3  Proof sketch ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Before proceeding with the formal proof, we first present the structure of the proof in Appendix C. ", "page_idx": 21}, {"type": "image", "img_path": "g5DyqerUpX/tmp/2c2d5ed393da5acca124e490cac70d0c2ad3e81b34c764a91df006ed2adbd17f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "C.1.4  Technical lemmas ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Lemma 1. Suppose Assumptions $^{\\,l}$ hold, we know $\\nabla\\Phi(x),\\widetilde{\\nabla}\\Phi(x).$ $z^{\\star}(x)$ and $y^{\\star}(x)$ defined above are $L_{\\nabla\\Phi}\\,,\\,\\widetilde{L},\\,\\bar{L}_{z^{\\star}},$ $L_{y^{\\star}}$ - Lipschitz continuous respectively with the constants satisfying: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L_{\\nabla\\Phi}\\leq L_{f,1}+\\frac{2L_{f,1}L_{g,1}}{\\mu_{g}}+L_{g,2}L_{f,0}+\\frac{2L_{g,1}L_{f,0}L_{g,2}+L_{g,1}^{2}L_{f,1}}{\\mu_{g}^{2}}+\\frac{L_{g,2}L_{g,1}^{2}L_{f,0}}{\\mu_{g}^{3}},}\\\\ &{\\quad\\tilde{L}\\leq L_{f,1}+\\frac{2L_{f,1}L_{g,1}+L_{g,2}L_{f,0}}{\\mu_{g}}+\\frac{2L_{g,1}L_{f,0}L_{g,2}+L_{g,1}^{2}L_{f,1}}{\\mu_{g}^{2}}+\\frac{L_{g,2}L_{g,1}L_{f,0}}{\\mu_{g}^{3}},}\\\\ &{L_{y^{*}}\\leq\\frac{L_{g,1}}{\\mu_{g}},}\\\\ &{L_{z^{*}}\\leq\\sqrt{1+L_{y^{*}}^{2}}\\left(\\frac{L_{f,1}}{\\mu_{g}}+\\frac{L_{f,0}L_{g,2}}{\\mu_{g}^{2}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "And we also have: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|z^{\\star}(x)\\|\\leq\\frac{L_{f,0}}{\\mu_{g}},\\quad\\forall x\\in\\mathbb{R}^{p}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. See Lemma 2.2 in [22] and Lemma B.2 in [11]. ", "page_idx": 22}, {"type": "text", "text": "Lemma 2. Suppose that $g(x)$ .s $\\mu$ -strongly convex and $L$ -smooth. Then for any $x$ and $\\begin{array}{r}{0<\\alpha<\\frac{2}{\\mu+L}}\\end{array}$ we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|{x-\\alpha\\nabla g(x)-x^{\\star}}\\right\\|\\leq\\left({1-\\alpha\\mu}\\right)\\left\\|x-x^{\\star}\\right\\|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $x^{\\star}=\\operatorname{argmin}g(x)$ ", "page_idx": 22}, {"type": "text", "text": "Proof. See Lemma 10 in [41]. ", "page_idx": 22}, {"type": "text", "text": "Lemma 3. Given diagonal matrices $A,B,C,D\\in\\mathbb{R}^{(n-1)\\times(n-1)}$ ,and ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbf{M}=\\left[\\begin{array}{l l}{A\\otimes I_{d}}&{B\\otimes I_{d}}\\\\ {C\\otimes I_{d}}&{D\\otimes I_{d}}\\end{array}\\right].\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Supposethattheeigenvaluesof $\\mathbf{M}$ are strictly less than one in magnitude. Then there exist an invertiblematrix O anda matrix $\\mathbf{T}$ With $\\|\\mathbf{T}\\|<1$ such that M has the similarity transformation: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbf{M}=\\mathbf{O}\\mathbf{T}\\mathbf{O}^{-1}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. See Lemma 1 in [2]. ", "page_idx": 22}, {"type": "text", "text": "Remark 5. Asserting the existence of T with $\\|\\mathbf{T}\\|<1$ Lemma $^3$ onlyguarantees theconvergence of SPARKLE. However, to obtain a precise non-asymptotic convergence rate, one must construct appropriate O and T. See more details in Appendix C.2.2. ", "page_idx": 22}, {"type": "text", "text": "C.1.5 Descent lemmas for the upper-level ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this subsection, we estimate the upper bound of the errors induced by the moving average in hyper-gradient estimation, as well as the upper bound of $\\|\\nabla\\Phi(x)\\|^{2}$ based on $I_{k},\\Delta_{k}$ ", "page_idx": 22}, {"type": "text", "text": "Lemma 4. Suppose Assumptions 1- 4 hold. We have: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\left\\|\\mathbb{E}_{k}\\bar{u}^{k}-\\nabla\\Phi(\\bar{x}^{k})\\right\\|^{2}\\leq\\!\\frac{20}{n}L^{2}(\\Delta_{k}+n I_{k}),}\\\\ {\\left\\|\\mathbb{E}_{k}\\mathbf{u}^{k}-\\widetilde{\\nabla}\\Phi(\\bar{\\mathbf{x}}^{k})\\right\\|^{2}\\leq\\!20L^{2}(\\Delta_{k}+n I_{k}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. Cauchy Schwartz inequality implies that: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\displaystyle\\mathbb{E}_{\\lambda}u^{k}-\\nabla\\Phi\\left(x^{k}\\right)\\right\\|^{2}}\\\\ &{\\le\\displaystyle\\sum_{i=1}^{n}\\left\\|\\nabla_{\\lambda}f_{i}\\left(x_{i}^{k},y_{i}^{k+1}\\right)-\\nabla_{\\lambda}f_{i}\\left(x^{k},y^{k+1}\\right)\\right\\|^{2}+\\displaystyle5\\sum_{i=1}^{n}\\left\\|\\nabla_{\\lambda}f_{i}\\left(x^{k},y^{k+1}\\right)-\\nabla_{\\lambda}f_{i}\\left(x^{k},y^{*}(x^{k})\\right)\\right\\|}\\\\ &{\\quad+\\displaystyle5\\sum_{i=1}^{n}\\left\\|\\nabla_{\\lambda}^{2}g_{i}\\left(x_{i}^{k},y_{i}^{k+1}\\right)\\left(z_{i}^{k+1}-z_{i}^{k+1}\\right)\\right\\|^{2}}\\\\ &{\\quad+\\displaystyle5\\sum_{i=1}^{n}\\left\\|\\big(\\nabla_{\\lambda}^{2}g_{i}\\left(x_{i}^{k},y_{i}^{k+1}\\right)-\\nabla_{\\lambda}^{2}g_{i}\\left(x^{k},y^{k+1}\\right)\\right)z_{i}^{k+1}\\right\\|^{2}}\\\\ &{\\quad+\\displaystyle5\\sum_{i=1}^{n}\\left\\|\\big(\\nabla_{\\lambda}^{2}g_{i}\\left(x^{k},y_{i}^{k+1}\\right)-\\nabla_{\\lambda}^{2}g_{i}\\left(x^{k},y^{*}(x^{k})\\right)\\right)z_{i}^{k+1}\\right\\|^{2}}\\\\ &{\\le\\displaystyle\\sum_{i=1}^{n}\\left\\|\\big(\\nabla_{\\lambda}^{2}g_{i}\\left(x^{k},y^{k+1}\\right)-\\nabla_{\\lambda}^{2}g_{i}\\left(x^{k},y^{*}(x^{k})\\right)\\right)z_{i}^{k+1}\\right\\|^{2}}\\\\ &{\\le\\operatorname*{lim}\\left(L_{j,1}^{2}+\\nu^{2}\\hat{L}_{j,0}^{2}\\left(\\left\\|\\kappa^{k}-\\kappa^{k}\\right\\|^{2}+\\left\\|y^{k+1}-y^{k+1}\\right\\|^{2}\\right)+\\left\\|\\mathbf{y}^{k+1}-y^{k}(\\kappa^{k})\\right\\|^{2}\\right)}\\\\ &{\\quad+\\operatorname*{lim}_{\\lambda}^{2}\\left(\\left\\|z^{k+1} \n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "For the term $\\left\\|\\mathbb{E}_{k}\\bar{u}^{k}-\\nabla\\Phi\\left(\\bar{x}^{k}\\right)\\right\\|^{2}$ , we have: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left\\Vert\\mathbb{E}_{k}\\bar{u}^{k}-\\nabla\\Phi(\\bar{x}^{k})\\right\\Vert^{2}\\leq\\frac{1}{n}\\left\\Vert\\mathbb{E}_{k}\\mathbf{u}^{k}-\\widetilde{\\nabla}\\Phi\\left(\\bar{\\mathbf{x}}^{k}\\right)\\right\\Vert^{2}\\leq20L^{2}\\left(\\frac{\\Delta_{k}}{n}+I_{k}\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Lemma 5. Suppose that Assumptions 1- 4 hold. We have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle n^{2}\\sum_{k=0}^{K}\\mathbb{E}\\left[\\|\\bar{u}^{k}-\\mathbb{E}_{k}[\\bar{u}^{k}]\\|^{2}\\right]=\\displaystyle\\sum_{k=0}^{K}\\mathbb{E}\\left[\\|{\\mathbf{u}}^{k}-\\mathbb{E}_{k}[{\\mathbf{u}}^{k}]\\|^{2}\\right]}\\\\ &{\\displaystyle\\leq9\\sigma_{g,2}^{2}\\sum_{k=0}^{K}\\left(\\mathbb{E}\\|{\\mathbf{z}}^{k+1}-\\bar{\\mathbf{z}}^{k+1}\\|^{2}+\\mathbb{E}\\|\\bar{\\mathbf{z}}^{k+1}-{\\mathbf{z}}_{\\star}^{k+1}\\|^{2}\\right)+3(K+1)n\\left(\\sigma_{f,1}^{2}+3\\sigma_{g,2}^{2}\\frac{L_{f,0}^{2}}{\\mu_{g}^{2}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. For $k\\geq0$ , Cauchy Schwartz inequality implies that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\frac{1}{3}\\mathbb{E}_{k}\\left[\\|\\mathbf{u}^{k}-\\mathbb{E}_{k}[\\mathbf{u}^{k}]\\|^{2}\\right]}\\\\ &{\\le\\mathbb{E}_{k}\\left[\\displaystyle\\sum_{i=1}^{n}\\|\\nabla_{1}f_{i}(x_{i}^{k},y_{i}^{k+1},\\xi_{i}^{k})-\\nabla_{1}f_{i}(x_{i}^{k},y_{i}^{k+1})\\|^{2}\\right]}\\\\ &{\\quad+\\mathbb{E}_{k}\\left[\\displaystyle\\sum_{i=1}^{n}\\|\\left(\\nabla_{12}g_{i}(x_{i}^{k},y_{i}^{k+1},\\zeta_{i}^{k})-\\nabla_{12}g_{i}(x_{i}^{k},y_{i}^{k+1})\\right)z_{i}^{k+1}\\|^{2}\\right]}\\\\ &{\\le n\\sigma_{f,1}^{2}+\\sigma_{g,2}^{2}\\|z^{k+1}\\|^{2}}\\\\ &{\\le n\\sigma_{f,1}^{2}+3\\sigma_{g,2}^{2}\\left(\\|\\mathbf{z}^{k+1}-\\bar{\\mathbf{z}}^{k+1}\\|^{2}+\\|\\bar{\\mathbf{z}}^{k+1}-\\mathbf{z}_{\\star}^{k+1}\\|^{2}+\\|\\mathbf{z}_{\\star}^{k+1}\\|^{2}\\right)}\\\\ &{\\le n\\sigma_{f,1}^{2}+3\\sigma_{g,2}^{2}\\left(\\|\\mathbf{z}^{k+1}-\\bar{\\mathbf{z}}^{k+1}\\|^{2}+\\|\\bar{\\mathbf{z}}^{k+1}-\\mathbf{z}_{\\star}^{k+1}\\|^{2}+n\\frac{L_{f,0}^{2}}{\\rho_{g}^{2}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Then taking expectation and summation on both sides, we get ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k=0}^{K}\\mathbb{E}\\left[\\|\\mathbf{u}^{k}-\\mathbb{E}_{k}[\\mathbf{u}^{k}]\\|^{2}\\right]}\\\\ &{\\le9\\sigma_{g,2}^{2}\\displaystyle\\sum_{k=0}^{K}\\left(\\mathbb{E}\\|\\mathbf{z}^{k+1}-\\bar{\\mathbf{z}}^{k+1}\\|^{2}+\\mathbb{E}\\|\\bar{\\mathbf{z}}^{k+1}-\\mathbf{z}_{\\star}^{k+1}\\|^{2}\\right)+3(K+1)n\\left(\\sigma_{f,1}^{2}+3\\sigma_{g,2}^{2}\\displaystyle\\frac{L_{f,0}^{2}}{\\mu_{g}^{2}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Since samples among agents are independent, it follows that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{k=0}^{K}\\mathbb{E}_{k}\\left[\\|\\bar{\\boldsymbol{u}}^{k}-\\mathbb{E}_{k}[\\bar{\\boldsymbol{u}}^{k}]\\|^{2}\\right]=\\frac{1}{n^{2}}\\sum_{k=0}^{K}\\mathbb{E}_{k}\\left[\\|\\mathbf{u}^{k}-\\mathbb{E}_{k}[\\mathbf{u}^{k}]\\|^{2}\\right].\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Taking expectations, we get the conclusion. ", "page_idx": 24}, {"type": "text", "text": "Lemma 6. Suppose that Assumptions 1- 4, and Lemmas 4, $^{5}$ hold.If ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\alpha\\leq\\frac{1}{2L_{\\nabla\\Phi}},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{4}\\sum_{k=0}^{K}\\mathbb{E}\\left\\|\\bar{r}^{k+1}\\right\\|^{2}}\\\\ &{\\displaystyle\\leq\\!\\frac{\\Phi(\\bar{x}_{0})-\\operatorname*{inf}\\Phi}{\\alpha}+10\\left(L^{2}+\\frac{\\theta\\sigma_{g,2}^{2}}{n}\\right)\\sum_{k=0}^{K}\\mathbb{E}\\left(\\frac{\\Delta_{k}}{n}+I_{k}\\right)+\\frac{3\\theta}{n}(K+1)\\left(\\sigma_{f,1}^{2}+2\\sigma_{g,2}^{2}\\frac{L_{f,0}^{2}}{\\mu_{g}^{2}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. The $L_{\\nabla\\Phi}$ -smoothness of $\\Phi$ indicates that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{k}[\\Phi\\left(\\bar{x}^{k+1}\\right)]-\\Phi(\\bar{x}^{k})}\\\\ &{\\leq\\left\\langle\\nabla\\Phi(\\bar{x}^{k}),\\left(-\\alpha\\mathbb{E}_{k}[\\bar{r}^{k+1}]\\right)\\right\\rangle+\\frac{L_{\\nabla\\Phi}\\alpha^{2}}{2}\\mathbb{E}_{k}\\|\\bar{r}^{k+1}\\|^{2}}\\\\ &{=\\left\\langle\\nabla\\Phi(\\bar{x}^{k})-\\mathbb{E}_{k}[\\bar{u}^{k}],-\\alpha\\mathbb{E}_{k}[\\bar{r}^{k+1}]\\right\\rangle+\\frac{L_{\\nabla\\Phi}}{2}\\alpha^{2}\\mathbb{E}_{k}\\|\\bar{r}^{k+1}\\|^{2}-\\alpha\\left\\langle\\mathbb{E}_{k}[\\bar{u}^{k}],\\mathbb{E}_{k}[\\bar{r}^{k+1}]\\right\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Then, due to $\\mathbb{E}_{k}[\\bar{u}^{k}]=\\theta^{-1}(\\mathbb{E}_{k}[\\bar{r}^{k+1}]-(1-\\theta)\\bar{r}^{k})$ , we have: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{k}[\\Phi(\\hat{x}^{k+1})]-\\Phi(\\bar{x}^{k})}\\\\ &{\\le\\frac{\\alpha}{2}\\|\\nabla\\Phi(\\hat{x}^{k})-\\mathbb{E}_{k}[\\bar{u}^{k}]\\|^{2}+\\frac{\\alpha}{2}\\|\\mathbb{E}_{k}[\\bar{r}^{k+1}]\\|^{2}}\\\\ &{\\quad+\\frac{L\\tau_{\\Phi}}{2}\\alpha^{2}\\mathbb{E}_{k}\\|r^{k+1}\\|^{2}-\\alpha\\cdot\\big\\langle\\mathbb{E}_{k}[\\bar{u}^{k}],\\mathbb{E}_{k}[\\bar{r}^{k+1}]\\big\\rangle}\\\\ &{=\\frac{\\alpha}{2}\\|\\nabla\\Phi(\\hat{x}^{k})-\\mathbb{E}_{k}[\\bar{u}^{k}]\\|^{2}+(-\\frac{\\alpha}{2}+\\frac{L\\tau_{\\Phi}}{2}\\alpha^{2})\\mathbb{E}_{k}\\|r^{k+1}\\|^{2}-\\frac{\\alpha(1-\\theta)}{2\\theta}\\|\\mathbb{E}_{k}[\\bar{r}^{k+1}]-\\bar{r}^{k}\\|^{2}}\\\\ &{\\quad+\\frac{\\alpha(1-\\theta)}{2\\theta}\\left(\\|r^{k}\\|^{2}-\\mathbb{E}_{k}\\|\\bar{r}^{k+1}\\|^{2}\\right)+\\frac{\\alpha}{2\\theta}\\mathbb{E}_{k}\\|\\bar{r}^{k+1}-\\mathbb{E}_{k}[\\bar{r}^{k+1}]\\|^{2}}\\\\ &{\\le\\frac{\\alpha}{2}\\|\\nabla\\Phi(\\bar{x}^{k})-\\mathbb{E}_{k}[\\bar{u}^{k}]\\|^{2}+(-\\frac{\\alpha}{2}+\\frac{L\\tau_{\\Phi}}{2}\\alpha^{2})\\mathbb{E}_{k}\\|r^{k+1}\\|^{2}+\\frac{\\alpha\\theta}{2}\\mathbb{E}_{k}\\|\\bar{u}^{k}-\\mathbb{E}_{k}[\\bar{u}^{k}]\\|^{2}}\\\\ &{\\quad+\\frac{\\alpha(1-\\theta)}{2\\theta}\\left(\\|r^{k}\\|^{2}-\\mathbb{E}_{k}\\|\\bar{r}^{k+1}\\|^{2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the frst equality uses $2\\left\\langle\\bar{r}^{k},\\mathbb{E}_{k}[\\bar{r}^{k+1}]\\right\\rangle=\\|\\bar{r}^{k}\\|^{2}+\\|\\mathbb{E}_{k}[\\bar{r}^{k+1}]\\|^{2}-\\|\\bar{r}^{k}-\\mathbb{E}_{k}[\\bar{r}^{k+1}]\\|^{2}$ and $\\mathbb{E}_{k}\\|\\bar{r}^{k+1}\\|^{2}=\\|\\mathbb{E}_{k}[\\bar{r}^{k+1}]\\|^{2}+\\mathbb{E}_{k}\\|\\bar{r}^{k+1}-\\mathbb{E}_{k}[\\bar{r}^{k+1}]\\|^{2}$ ", "page_idx": 24}, {"type": "text", "text": "Taking expetatn and summation, anduig $\\begin{array}{r}{\\alpha\\leq\\frac{1}{2L\\nabla\\Psi}}\\end{array}$ ,we get ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\operatorname*{inf}\\Phi-\\Phi(\\bar{x}_{0})}\\\\ {\\displaystyle\\leq\\!\\frac{\\alpha}{2}\\displaystyle\\sum_{k=0}^{K}\\!\\mathbb{E}\\|\\nabla\\Phi(\\bar{x}^{k})-\\mathbb{E}_{k}[\\bar{u}^{k}]\\|^{2}-\\displaystyle\\frac{\\alpha}{4}\\displaystyle\\sum_{k=0}^{K}\\mathbb{E}\\|\\bar{r}^{k+1}\\|^{2}+\\displaystyle\\frac{\\alpha\\theta}{2}\\displaystyle\\sum_{k=0}^{K}\\mathbb{E}\\left[\\mathbb{E}_{k}\\|\\bar{u}^{k}-\\mathbb{E}_{k}[\\bar{u}^{k}]\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Since samples of different agents are independent, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}_{k}\\|\\bar{\\boldsymbol{u}}^{k}-\\mathbb{E}_{k}[\\bar{\\boldsymbol{u}}^{k}]\\|^{2}=\\frac{1}{n^{2}}\\mathbb{E}_{k}\\|\\mathbf{u}^{k}-\\mathbb{E}_{k}[\\mathbf{u}^{k}]\\|^{2}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Combining it with the conclusion of Lemma 4 and 5, we get from (42) that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\cfrac{\\alpha}{4}\\displaystyle\\sum_{k=0}^{K}\\mathbb{E}\\left\\|\\bar{r}^{k+1}\\right\\|^{2}}\\\\ &{\\leq\\Phi(\\bar{x}_{0})-\\operatorname*{inf}\\Phi+\\displaystyle\\frac{\\alpha}{2}\\sum_{k=0}^{K}\\mathbb{E}\\|\\nabla\\Phi(\\bar{x}^{k})-\\mathbb{E}_{k}[\\bar{u}^{k}]\\|^{2}+\\displaystyle\\frac{\\alpha\\theta}{2}\\displaystyle\\sum_{k=0}^{K}\\mathbb{E}\\left[\\mathbb{E}_{k}\\|\\bar{u}^{k}-\\mathbb{E}_{k}[\\bar{u}^{k}]\\|^{2}\\right]}\\\\ &{\\leq\\Phi(\\bar{x}_{0})-\\operatorname*{inf}\\Phi+10\\alpha\\left(L^{2}+\\displaystyle\\frac{\\theta\\sigma_{g,2}^{2}}{n}\\right)\\displaystyle\\sum_{k=0}^{K}\\mathbb{E}\\left(\\frac{\\Delta_{k}}{n}+I_{k}\\right)+\\displaystyle\\frac{3\\alpha\\theta}{n}(K+1)\\left(\\sigma_{f,1}^{2}+2\\sigma_{g,2}^{2}\\displaystyle\\frac{L_{f,0}^{2}}{\\mu_{g}^{2}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Lemma 7. Suppose that Assumptions 1- 4 hold, then we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\sum_{k=0}^{K}\\mathbb{E}\\left[\\left\\|\\mathbb{E}_{k}[\\mathbf{r}^{k+1}]-\\widetilde{\\nabla}\\Phi(\\bar{\\mathbf{x}}^{k})\\right\\|^{2}\\right]}\\\\ &{\\displaystyle\\leq\\frac{1-\\theta}{\\theta}\\left\\|\\widetilde{\\nabla}\\Phi(\\bar{\\mathbf{x}}^{0})\\right\\|^{2}+2\\displaystyle\\sum_{k=0}^{K}\\mathbb{E}\\left[\\left\\|\\mathbb{E}_{k}[\\mathbf{u}^{k}]-\\widetilde{\\nabla}\\Phi(\\bar{\\mathbf{x}}^{k})\\right\\|^{2}\\right]}\\\\ &{\\displaystyle\\quad+\\,\\frac{2\\tilde{L}^{2}(1-\\theta)^{2}}{\\theta^{2}}\\sum_{k=0}^{K-1}\\mathbb{E}\\left[\\left\\|\\bar{\\mathbf{x}}^{k+1}-\\bar{\\mathbf{x}}^{k}\\right\\|^{2}\\right]+(1-\\theta)\\theta\\displaystyle\\sum_{k=0}^{K}\\mathbb{E}\\left[\\left\\|\\mathbf{u}^{k}-\\mathbb{E}_{k}[\\mathbf{u}^{k}]\\right\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. We define $\\mathbf{u}^{-1}=\\mathbf{0}$ for brevity. From the definition of $\\mathbb{E}_{k}$ , we have : ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{k-1}\\left[\\left\\|\\mathbf{r}^{k}-\\widetilde{\\nabla}\\Phi(\\bar{\\mathbf{x}}^{k-1})\\right\\|^{2}\\right]}\\\\ &{=\\!\\!\\mathbb{E}_{k-1}\\left[\\left\\|\\mathbb{E}_{k-1}[\\mathbf{r}^{k}]-\\widetilde{\\nabla}\\Phi(\\bar{\\mathbf{x}}^{k-1})\\right\\|^{2}\\right]+\\mathbb{E}_{k-1}\\left[\\left\\|\\mathbf{r}^{k}-\\mathbb{E}_{k-1}[\\mathbf{r}^{k}]\\right\\|^{2}\\right]}\\\\ &{=\\!\\!\\mathbb{E}_{k-1}\\left[\\left\\|\\mathbb{E}_{k-1}[\\mathbf{r}^{k}]-\\widetilde{\\nabla}\\Phi(\\bar{\\mathbf{x}}^{k-1})\\right\\|^{2}\\right]+\\theta^{2}\\mathbb{E}_{k-1}\\left[\\left\\|\\mathbf{u}^{k-1}-\\mathbb{E}_{k-1}[\\mathbf{u}^{k-1}]\\right\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Jensen's inequality implies that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{k}\\left[\\left\\|\\mathbb{E}_{k}[\\mathbf{r}^{k+1}]-\\widetilde{\\nabla}\\Phi(\\widetilde{\\mathbf{x}}^{k})\\right\\|^{2}\\right]}\\\\ &{\\le(1-\\theta)\\mathbb{E}_{k}\\left[\\left\\|\\mathbf{r}^{k}-\\widetilde{\\nabla}\\Phi(\\widetilde{\\mathbf{x}}^{k-1})\\right\\|^{2}\\right]}\\\\ &{\\quad+\\,\\theta\\mathbb{E}_{k}\\left[\\left\\|\\left(\\mathbb{E}_{k}[\\mathbf{u}^{k}]-\\widetilde{\\nabla}\\Phi(\\widetilde{\\mathbf{x}}^{k})\\right)+\\theta^{-1}(1-\\theta)\\left(\\widetilde{\\nabla}\\Phi(\\widetilde{\\mathbf{x}}^{k-1})-\\widetilde{\\nabla}\\Phi(\\widetilde{\\mathbf{x}}^{k})\\right)\\right\\|^{2}\\right]}\\\\ &{\\le(1-\\theta)\\mathbb{E}_{k}\\left[\\left\\|\\mathbf{r}^{k}-\\widetilde{\\nabla}\\Phi(\\widetilde{\\mathbf{x}}^{k-1})\\right\\|^{2}\\right]+2\\theta\\mathbb{E}_{k}\\left[\\left\\|\\mathbb{E}_{k}[\\mathbf{u}^{k}]-\\widetilde{\\nabla}\\Phi(\\widetilde{\\mathbf{x}}^{k})\\right\\|^{2}\\right]}\\\\ &{\\quad+\\,\\frac{2(1-\\theta)^{2}}{\\theta}\\mathbb{E}_{k}\\left[\\left\\|\\widetilde{\\nabla}\\Phi(\\widetilde{\\mathbf{x}}^{k-1})-\\widetilde{\\nabla}\\Phi(\\widetilde{\\mathbf{x}}^{k})\\right\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Substituting (43) into (44) , and taking expectation and summation on both sides, we get: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\theta\\displaystyle\\sum_{k=0}^{K}\\mathbb{E}\\left[\\left\\|\\mathbb{E}_{k-1}[\\mathbf{r}^{k}]-\\widetilde{\\nabla}\\Phi(\\bar{\\mathbf{x}}^{k-1})\\right\\|^{2}\\right]}\\\\ &{\\leq\\!\\mathbb{E}\\left[\\left\\|\\mathbf{r}^{0}-\\widetilde{\\nabla}\\Phi(\\bar{\\mathbf{x}}^{-1})\\right\\|^{2}\\right]-\\mathbb{E}\\left[\\left\\|\\mathbb{E}_{K}[\\mathbf{r}^{K+1}]-\\widetilde{\\nabla}\\Phi(\\bar{\\mathbf{x}}^{k})\\right\\|^{2}\\right]+2\\theta\\displaystyle\\sum_{k=0}^{K}\\mathbb{E}\\left[\\left\\|\\mathbb{E}_{k}[\\mathbf{u}^{k}]-\\widetilde{\\nabla}\\Phi(\\bar{\\mathbf{x}}^{k})\\right\\|^{2}\\right]}\\\\ &{\\quad+\\displaystyle\\frac{2(1-\\theta)^{2}}{\\theta}\\displaystyle\\sum_{k=0}^{K}\\mathbb{E}\\left[\\left\\|\\widetilde{\\nabla}\\Phi(\\bar{\\mathbf{x}}^{k-1})-\\widetilde{\\nabla}\\Phi(\\bar{\\mathbf{x}}^{k})\\right\\|^{2}\\right]+(1-\\theta)\\theta^{2}\\displaystyle\\sum_{k=0}^{K}\\mathbb{E}\\left[\\left\\|\\mathbf{u}^{k}-\\mathbb{E}_{k}[\\mathbf{u}^{k}]\\right\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Finally, notethat $\\mathbf{x}^{-1}=\\mathbf{x}^{0}$ \uff0c $\\mathbf{r}^{0}=\\mathbf{0}$ and $\\mathbb{E}_{-1}=\\mathbb{E}_{0}$ Subtracting $\\theta\\mathbb{E}\\left[\\left\\|\\mathbb{E}_{-1}[\\mathbf{r}^{0}]-\\widetilde{\\nabla}\\Phi(\\bar{\\mathbf{x}}^{-1})\\right\\|^{2}\\right]=$ $\\theta\\left\\|\\widetilde{\\nabla}\\Phi(\\bar{\\bf x}^{0})\\right\\|^{2}$ from both sides of this equation, we get: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\sum_{k=0}^{K}\\mathbb{E}\\left[\\left\\|\\mathbb{E}_{k}[\\mathbf{r}^{k+1}]-\\widetilde{\\nabla}\\Phi(\\bar{\\mathbf{x}}^{k})\\right\\|^{2}\\right]}\\\\ &{\\displaystyle\\leq\\frac{1-\\theta}{\\theta}\\left\\|\\widetilde{\\nabla}\\Phi(\\bar{\\mathbf{x}}^{0})\\right\\|^{2}+2\\displaystyle\\sum_{k=0}^{K}\\mathbb{E}\\left[\\left\\|\\mathbb{E}_{k}[\\mathbf{u}^{k}]-\\widetilde{\\nabla}\\Phi(\\bar{\\mathbf{x}}^{k})\\right\\|^{2}\\right]}\\\\ &{\\displaystyle\\quad+\\,\\frac{2\\tilde{L}^{2}(1-\\theta)^{2}}{\\theta^{2}}\\sum_{k=0}^{K-1}\\mathbb{E}\\left[\\left\\|\\bar{\\mathbf{x}}^{k+1}-\\bar{\\mathbf{x}}^{k}\\right\\|^{2}\\right]+(1-\\theta)\\theta\\displaystyle\\sum_{k=0}^{K}\\mathbb{E}\\left[\\left\\|\\mathbf{u}^{k}-\\mathbb{E}_{k}[\\mathbf{u}^{k}]\\right\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Lemma 8 (Descent lemma). Suppose that Assumptions 1- 4 and Lemmas 4, 5 hold. If ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\frac{\\alpha^{2}}{\\theta^{2}}(1-\\theta)\\leq\\frac{1}{32L_{\\nabla\\Phi}^{2}},\\,\\alpha\\leq\\frac{1}{10L_{\\nabla\\Phi}},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "then we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sum_{k=0}^{K}\\mathbb{E}\\|\\nabla\\Phi(\\bar{x}^{k})\\|^{2}\\lesssim\\frac{\\Phi(\\bar{x}_{0})-\\operatorname*{inf}\\Phi}{\\alpha}+\\left(L^{2}+\\left(\\theta(1-\\theta)+L_{\\nabla\\Phi}\\alpha\\theta^{2}\\right)\\sigma_{g,2}^{2}\\right)\\sum_{k=0}^{K}\\mathbb{E}\\left[\\frac{\\Delta_{k}}{n}+I_{k}\\right]}\\\\ {+\\left(K+1\\right)\\left(\\theta(1-\\theta)+L_{\\nabla\\Phi}\\alpha\\theta^{2}\\right)(\\sigma_{f,1}^{2}+\\kappa^{2}\\sigma_{g,2}^{2})+\\frac{(1-\\theta)^{2}}{\\theta}\\|\\nabla\\Phi(\\bar{x}^{0})\\|^{2}.}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. The $L_{\\nabla\\Phi}$ -smoothness of $\\Phi$ indicates that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{k}[\\Phi(\\bar{x}^{k+1})]-\\Phi(\\bar{x}^{k})}\\\\ &{\\leq\\left\\langle\\nabla\\Phi(\\bar{x}^{k}),-\\alpha\\mathbb{E}_{k}[\\bar{r}^{k+1}]\\right\\rangle+\\frac{L_{\\nabla\\Phi}\\alpha^{2}}{2}\\mathbb{E}_{k}\\left\\|\\bar{r}^{k+1}\\right\\|^{2}}\\\\ &{=-\\alpha\\left\\langle\\nabla\\Phi(\\bar{x}^{k}),\\mathbb{E}_{k}[\\bar{r}^{k+1}]-\\nabla\\Phi(\\bar{x}^{k})\\right\\rangle-\\alpha\\|\\nabla\\Phi(\\bar{x}^{k})\\|^{2}+\\frac{L_{\\nabla\\Phi}}{2}\\alpha^{2}\\mathbb{E}_{k}\\left\\|\\bar{r}^{k+1}\\right\\|^{2}}\\\\ &{\\leq-\\,\\frac{\\alpha}{2}\\|\\nabla\\Phi(\\bar{x}^{k})\\|^{2}+\\frac{\\alpha}{2}\\|\\mathbb{E}_{k}[\\bar{r}^{k+1}]-\\nabla\\Phi(\\bar{x}^{k})\\|^{2}+\\frac{L_{\\nabla\\Phi}}{2}\\alpha^{2}\\mathbb{E}_{k}\\left\\|\\bar{r}^{k+1}\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Taking expectation and summation on both sides, we get: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k=0}^{K}\\alpha\\mathbb{E}\\|\\nabla\\Phi(\\bar{x}^{k})\\|^{2}}\\\\ &{\\displaystyle\\leq2(\\Phi(\\bar{x}^{0})-\\operatorname*{inf}\\Phi)+\\displaystyle\\sum_{k=0}^{K}\\alpha\\mathbb{E}\\|\\mathbb{E}_{k}[\\bar{r}^{k+1}]-\\nabla\\Phi(\\bar{x}^{k})\\|^{2}+\\displaystyle\\sum_{k=0}^{K}L_{\\nabla\\Phi}\\alpha^{2}\\mathbb{E}\\left\\|\\bar{r}^{k+1}\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Define auxiliary series $m^{k}$ as: ", "page_idx": 26}, {"type": "equation", "text": "$$\nm^{0}=\\bar{r}^{0}=0,m^{k+1}=(1-\\theta)m^{k}+\\theta\\nabla\\Phi(\\bar{x}^{k}).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Note that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{k}\\left\\|\\bar{r}^{k+1}\\right\\|^{2}=\\left\\|\\mathbb{E}_{k}\\bar{r}^{k+1}\\right\\|^{2}+\\mathbb{E}_{k}\\left\\|\\bar{r}^{k+1}-\\mathbb{E}_{k}\\bar{r}^{k+1}\\right\\|^{2}}\\\\ &{\\qquad\\qquad\\leq2\\|\\mathbb{E}_{k}[\\bar{r}^{k+1}]-\\nabla\\Phi(\\bar{x}^{k})\\|^{2}+2\\|\\nabla\\Phi(\\bar{x}^{k})\\|^{2}+\\theta^{2}\\mathbb{E}_{k}\\|\\bar{u}^{k}-\\mathbb{E}_{k}\\bar{u}^{k}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Then using the Jenson's Inequality, we get: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\mathbb{E}_{k}\\bar{r}^{k+1}-m^{k+1}\\|^{2}=\\|(1-\\theta)(\\bar{r}^{k}-m^{k})+\\theta(\\mathbb{E}_{k}\\bar{u}^{k}-\\nabla\\Phi(\\bar{x}^{k}))\\|^{2}}\\\\ {\\leq(1-\\theta)\\|\\bar{r}^{k}-m^{k}\\|^{2}+\\theta\\|\\mathbb{E}_{k}\\bar{u}^{k}-\\nabla\\Phi(\\bar{x}^{k})\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Itfollowsthatfor $k\\geq0$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad{\\mathbb{E}}\\|{\\mathbb{E}}_{k}\\bar{r}^{k+1}-m^{k+1}\\|^{2}}\\\\ &{\\leq\\!(1-\\theta){\\mathbb{E}}\\|{\\mathbb{E}}_{k-1}[\\bar{r}^{k}]-m^{k}\\|^{2}+(1-\\theta)\\theta^{2}{\\mathbb{E}}\\|\\bar{u}^{k-1}-{\\mathbb{E}}_{k-1}\\bar{u}^{k-1}\\|^{2}+\\theta{\\mathbb{E}}\\|{\\mathbb{E}}_{k}\\bar{u}^{k}-\\nabla\\Phi(\\bar{x}^{k})\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where for brevity we define $\\bar{u}^{-1}=0$ ", "page_idx": 27}, {"type": "text", "text": "Taking the summation on both sides from $k=0$ to $K$ ,weget ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k=0}^{K}\\theta\\mathbb{E}\\|\\mathbb{E}_{k}\\bar{r}^{k+1}-m^{k+1}\\|^{2}\\leq\\displaystyle\\sum_{k=0}^{K-1}\\theta\\mathbb{E}\\|\\mathbb{E}_{k}\\bar{r}^{k+1}-m^{k+1}\\|^{2}+\\mathbb{E}\\|\\mathbb{E}_{K}\\bar{r}^{K+1}-m^{K+1}\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\displaystyle\\sum_{k=0}^{K}\\theta\\mathbb{E}\\|\\mathbb{E}_{k}\\bar{u}^{k}-\\nabla\\Phi(\\bar{x}^{k})\\|^{2}+\\displaystyle\\sum_{k=0}^{K-1}(1-\\theta)\\theta^{2}\\mathbb{E}\\|\\bar{u}^{k}-\\mathbb{E}_{k}\\bar{u}^{k}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "On the other hand, due to the definition of $m^{k}$ and Jenson's Inequality, we have: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|m^{k+1}-\\nabla\\Phi(\\bar{x}^{k})\\|^{2}=\\|(1-\\theta)(m^{k}-\\nabla\\Phi(\\bar{x}^{k}))\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=(1-\\theta)^{2}\\|m^{k}-\\nabla\\Phi(\\bar{x}^{k-1})+\\nabla\\Phi(\\bar{x}^{k-1})-\\nabla\\Phi(\\bar{x}^{k})\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq(1-\\theta)\\|m^{k}-\\nabla\\Phi(\\bar{x}^{k-1})\\|^{2}+\\displaystyle\\frac{(1-\\theta)^{2}}{\\theta}L_{\\nabla\\Phi}^{2}\\alpha^{2}\\|\\bar{r}^{k}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Taking the summation, we get ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k=0}^{K}\\theta\\|m^{k+1}-\\nabla\\Phi(\\bar{x}^{k})\\|^{2}\\leq\\|m^{0}-\\nabla\\Phi(\\bar{x}^{-1})\\|^{2}+\\displaystyle\\sum_{k=0}^{K}\\frac{(1-\\theta)^{2}}{\\theta}L_{\\nabla\\Phi}^{2}\\alpha^{2}\\|\\bar{r}^{k}\\|^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad=(1-\\theta)^{2}\\|\\nabla\\Phi(\\bar{x}^{0})\\|^{2}+\\displaystyle\\sum_{k=0}^{K}\\frac{(1-\\theta)^{2}}{\\theta}L_{\\nabla\\Phi}^{2}\\alpha^{2}\\|\\bar{r}^{k}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Combining (48) and (49), we obtain: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k=0}^{K}\\beta g|\\mathbb{E}_{k}\\nu^{k+1}-\\nabla\\Phi(\\hat{z}^{k})|^{2}}\\\\ &{\\le2\\displaystyle\\sum_{k=0}^{K}\\beta g|\\mathbb{E}_{k}\\nu^{k+1}-m^{k+1}|^{2}+2\\displaystyle\\sum_{k=0}^{K}\\theta|m^{k+1}-\\nabla\\Phi(\\hat{x}^{k})|^{2}}\\\\ &{\\le2\\displaystyle\\sum_{k=0}^{K}\\beta g|\\mathbb{E}_{k}\\ u^{k}-\\nabla\\Phi(\\hat{z}^{k})|^{2}+2\\displaystyle\\sum_{k=1}^{K-1}(1-\\theta)^{2}\\mathbb{E}_{\\mathbb{R}}\\ u^{k}-\\mathbb{E}_{\\mathbb{R}}\\ u^{k}|^{2}}\\\\ &{\\quad+2\\displaystyle\\sum_{k=0}^{K}\\displaystyle\\sum_{\\theta}(1-\\theta)^{2}L_{\\mathsf{e}}^{2}\\alpha^{2}\\mathbb{E}_{\\mathbb{I}}\\nu^{k}|^{2}+2(1-\\theta)^{2}\\|\\nabla\\Phi(\\hat{z}^{\\theta})\\|^{2}}\\\\ &{\\le2\\displaystyle\\sum_{k=0}^{K}\\beta g|\\mathbb{E}_{k}\\ u^{k}-\\nabla\\Phi(\\hat{z}^{k})|^{2}+2\\displaystyle\\sum_{k=0}^{K-1}\\left(1+2\\frac{1-\\theta}{\\theta}L_{\\mathsf{e}}^{2}\\alpha^{2}\\right)(1-\\theta)\\theta^{2}\\mathbb{E}|\\hat{u}^{k}-\\mathbb{E}_{\\mathbb{R}}\\hat{u}^{k}|^{2}}\\\\ &{\\quad+2(1-\\theta)^{2}\\|\\nabla\\Phi(\\hat{z}^{k})\\|^{2}+2\\displaystyle\\sum_{k=0}^{K-1}\\left(1-\\theta\\right)^{2}L_{\\mathsf{e}}^{2}\\alpha^{2}\\left(2\\mathbb{E}\\|\\mathbb{E}_{k}\\|^{p+1}\\right)-\\nabla\\Phi(\\hat{x}^{k})\\|^{2}+2\\mathbb{E}|\\nabla\\Phi(\\hat{x}^{k})\\|^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where the last inequality uses (47). ", "page_idx": 27}, {"type": "text", "text": "(45) indicates that $4\\frac{1-\\theta}{\\theta}L_{\\nabla\\Phi}^{2}\\alpha^{2}\\leq\\frac{\\theta}{8}$ . Subtracting ", "page_idx": 27}, {"type": "equation", "text": "$$\n2\\sum_{k=0}^{K-1}\\frac{(1-\\theta)^{2}}{\\theta}L_{\\nabla\\Phi}^{2}\\alpha^{2}\\cdot2\\mathbb{E}\\|\\mathbb{E}_{k}[\\bar{r}^{k+1}]-\\nabla\\Phi(\\bar{x}^{k})\\|^{2}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "from both sides of (50), we have: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k=0}^{K}\\theta\\mathbb{E}\\|\\mathbb{E}_{k}\\bar{r}^{k+1}-\\nabla\\Phi(\\bar{x}^{k})\\|^{2}}\\\\ &{\\displaystyle\\leq4\\displaystyle\\sum_{k=0}^{K}\\theta\\mathbb{E}\\|\\mathbb{E}_{k}\\bar{u}^{k}-\\nabla\\Phi(\\bar{x}^{k})\\|^{2}+8\\displaystyle\\sum_{k=0}^{K-1}(1-\\theta)\\theta^{2}\\mathbb{E}\\|\\bar{u}^{k}-\\mathbb{E}_{k}\\bar{u}^{k}\\|^{2}+4(1-\\theta)^{2}\\|\\nabla\\Phi(\\bar{x}^{0})\\|^{2}}\\\\ &{\\displaystyle\\quad+\\,\\frac{\\theta}{4}\\displaystyle\\sum_{k=0}^{K-1}\\mathbb{E}\\|\\nabla\\Phi(\\bar{x}^{k})\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Substituting (47), (51) into (46), we get: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\sum_{k=0}^{K}\\alpha\\mathbb{E}\\big\\|\\nabla\\Phi(\\hat{x}^{k})\\big\\|^{2}}\\\\ &{\\le2(\\Phi(\\hat{x}^{(0)}-\\operatorname*{inf}\\Phi)+\\displaystyle\\sum_{k=0}^{K}(\\alpha+2L_{\\nabla\\Phi}\\alpha^{2})\\mathbb{E}\\|\\mathbb{E}_{k}[\\hat{r}^{k+1}]-\\nabla\\Phi(\\hat{x}^{k})\\|^{2}+\\displaystyle\\sum_{k=0}^{K}2L_{\\nabla\\Phi}\\alpha^{2}\\mathbb{E}\\|\\nabla\\Phi(\\hat{x}^{k})\\|^{2}}\\\\ &{\\quad+\\displaystyle\\sum_{k=0}^{K}2L_{\\nabla\\Phi}\\alpha^{2}\\theta^{2}\\mathbb{E}\\mathbb{E}_{k}\\|\\hat{u}^{k}-\\mathbb{E}_{k}\\hat{u}^{k}\\|^{2}}\\\\ &{\\le2(\\Phi(\\hat{x}_{0})-\\operatorname*{inf}\\Phi)+5\\alpha\\displaystyle\\sum_{k=0}^{K}\\mathbb{E}\\big\\|\\mathbb{E}_{k}\\hat{u}^{k}-\\nabla\\Phi(\\hat{x}^{k})\\big\\|^{2}+5\\frac{\\alpha}{\\theta}(1-\\theta)^{2}\\|\\nabla\\Phi(\\hat{x}^{0})\\|^{2}}\\\\ &{\\quad+\\displaystyle\\sum_{k=0}^{K}\\Big(10\\frac{\\alpha}{\\theta}(1-\\theta)+2L_{\\nabla\\Phi}\\alpha^{2}\\Big)\\theta^{2}\\mathbb{E}\\|\\hat{u}^{k}-\\mathbb{E}_{k}\\hat{u}^{k}\\|^{2}+\\frac{\\alpha}{2}\\displaystyle\\sum_{k=0}^{K-1}\\mathbb{E}\\|\\nabla\\Phi(\\hat{x}^{k})\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where the last inequality uses $\\begin{array}{r}{\\alpha\\leq\\frac{1}{10L_{\\nabla\\Phi}}}\\end{array}$ ", "page_idx": 28}, {"type": "text", "text": "Subtracting $\\begin{array}{r}{\\frac{\\alpha}{2}\\sum_{k=0}^{K-1}\\mathbb{E}\\|\\nabla\\Phi(\\bar{x}^{k})\\|^{2}}\\end{array}$ from both sides of (52), and substituting (38), (39) into it, we get: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{2}\\displaystyle\\sum_{k=0}^{K}\\alpha\\mathbb{E}|\\nabla\\Phi(x^{k})|^{2}}\\\\ &{\\le2(\\Phi(x^{0})-\\operatorname*{inf}\\phi)+100L\\lambda^{2}\\displaystyle\\sum_{k=0}^{K}\\mathbb{E}\\left[\\frac{\\Delta\\mathbf{k}}{n}+I_{k}\\right]+5\\frac{\\alpha}{\\beta}(1-\\theta^{2})^{2}|\\nabla\\Phi(x^{0})|^{2}}\\\\ &{\\quad+\\left(\\frac{10\\theta}{n}(1-\\theta)+2L\\mathrm{e}\\uprho^{2}\\right)\\theta^{-}\\cdot9\\sigma_{x^{2},x}^{2}\\displaystyle\\sum_{k=0}^{K}\\left(\\mathbb{E}\\left[\\delta^{k+1}-x^{k+1}\\right]^{2}+\\mathbb{E}\\|z^{k+1}-x_{k}^{k+1}\\|^{2}\\right)}\\\\ &{\\quad+\\frac{(10\\theta^{2}(1-\\theta)+2L\\mathrm{e}\\uprho^{2})\\theta^{-}}{n^{2}}\\theta^{-}\\cdot3(K+1)n\\left(\\sigma_{x,1}^{2}+3\\sigma_{x^{2},2}^{2}\\frac{L_{f}^{2}}{\\mu_{g}^{2}}\\right)}\\\\ &{\\le2(\\Phi(x)-\\operatorname*{inf}\\phi)+\\left(100L\\lambda^{2}+9\\left(10\\uprho(1-\\theta)+2L\\mathrm{e}\\uprho^{2}\\right)\\frac{\\partial_{x^{2},\\theta}^{2}}{\\partial_{x}}\\right)\\displaystyle\\sum_{k=0}^{K}\\mathbb{E}\\left[\\frac{\\Delta\\mathbf{k}}{n}+I_{k}\\right]}\\\\ &{\\quad+3(K+1)\\left(10\\upalpha(1-\\theta)+2L\\mathrm{e}\\uprho\\mathrm{e}^{2}\\theta\\right)\\frac{\\partial}{\\partial_{x}}\\left(\\sigma_{f,1}^{2}+3\\sigma_{x^{2},2}^{2}\\frac{L_{f}^{2}}{\\mu_{g}^{2}}\\right)}\\\\ &{\\quad+5\\frac{\\alpha}{\\beta}(1-\\theta^{2})^{2}|\\nabla\\Phi(x^{0})|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Finally, multiplying $\\frac{2}{\\alpha}$ on both sides, we get: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\displaystyle\\sum_{k=0}^{K}\\mathbb{E}\\|\\nabla\\Phi(\\bar{x}^{k})\\|^{2}\\lesssim\\frac{\\Phi(\\bar{x}_{0})-\\operatorname*{inf}\\Phi}{\\alpha}+\\left(L^{2}+\\left(\\theta(1-\\theta)+L_{\\nabla\\Phi}\\alpha\\theta^{2}\\right)\\frac{\\sigma_{g,2}^{2}}{n}\\right)\\sum_{h=0}^{K}\\mathbb{E}\\left[\\frac{\\Delta_{k}}{n}+I_{k}\\right]}\\\\ &{}&{\\displaystyle+\\,\\frac{K+1}{n}\\left(\\theta(1-\\theta)+L_{\\nabla\\Phi}\\alpha\\theta^{2}\\right)(\\sigma_{f,1}^{2}+\\kappa^{2}\\sigma_{g,2}^{2})+\\frac{(1-\\theta)^{2}}{\\theta}\\|\\nabla\\Phi(\\bar{x}^{0})\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "C.1.6  Descent lemmas for the lower- and auxiliary-level ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "The following lemmas present the error analysis of the estimation of $y^{\\star}(\\bar{x}^{k})$ and $z_{\\star}^{k}$ , i.e., the term $I_{k}$ Lemma 9 (Estimation error of $y^{\\star}(x)$ 0.Suppose Assumptions $I.$ -4hold, and: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\beta\\leq\\frac{\\mu_{g}}{32L_{g,1}^{2}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Then we have the estimation error of $y^{\\star}$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\|\\bar{y}^{0}-y^{\\star}(\\bar{x}^{0})\\|^{2}+\\sum_{k=0}^{K}\\mathbb{E}[\\|\\bar{y}^{k+1}-y^{\\star}(\\bar{x}^{k})\\|^{2}]}\\\\ &{\\le\\displaystyle\\frac{4}{\\beta\\mu_{g}}\\|\\bar{y}^{0}-y^{\\star}(\\bar{x}^{0})\\|^{2}+\\sum_{k=1}^{K}\\frac{6\\alpha^{2}L_{y^{\\star}}^{2}}{\\beta^{2}\\mu_{g}^{2}}\\mathbb{E}\\|\\bar{r}^{k}\\|^{2}+\\displaystyle\\sum_{k=1}^{K}\\frac{6}{\\mu_{g}^{2}}L_{g,1}^{2}\\mathbb{E}\\left[\\frac{\\|\\mathbf{O}_{x}\\|^{2}\\|\\hat{\\mathbf{e}}_{x}^{k}\\|^{2}+\\|\\mathbf{O}_{y}\\|^{2}\\|\\hat{\\mathbf{e}}_{y}^{k}\\|^{2}}{n}\\right]}\\\\ &{\\displaystyle\\ +\\,\\frac{4K\\beta\\sigma_{g,1}^{2}}{n\\mu_{g}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "and ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k=0}^{K}\\mathbb{E}\\left[\\|\\bar{y}^{k+1}-\\bar{y}^{k}\\|^{2}\\right]}\\\\ &{\\displaystyle\\leq\\frac{\\beta^{2}L_{g,1}^{2}}{n}\\left(4+\\frac{48L_{g,1}^{2}}{\\mu_{g}^{2}}\\right)\\sum_{k=1}^{K}\\mathbb{E}\\left(\\|\\mathbf{O}_{x}\\|^{2}\\|\\hat{\\mathbf{e}}_{x}^{k}\\|^{2}+\\|\\mathbf{O}_{y}\\|^{2}\\|\\hat{\\mathbf{e}}_{y}^{k}\\|^{2}\\right)+\\frac{48\\alpha^{2}L_{g,1}^{2}}{\\mu_{g}^{2}}L_{y}^{2}\\displaystyle\\sum_{k=1}^{K}\\mathbb{E}\\|\\bar{r}^{k}\\|^{2}}\\\\ &{\\displaystyle\\ \\ +\\,\\frac{3(K+1)\\beta^{2}}{n}\\sigma_{g,1}^{2}+\\frac{32\\beta L_{g,1}^{2}}{\\mu_{g}}\\|\\bar{y}^{0}-y^{\\star}(\\bar{x}^{0})\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Proof. For each $k\\geq0$ , due to the independence of samples, we have: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\widehat{\\mathbb{E}}_{k}[\\|\\bar{y}^{k+1}-y^{*}(\\bar{x}^{k})\\|^{2}]=\\widehat{\\mathbb{E}}_{k}[\\|\\bar{y}^{k}-\\beta\\bar{v}^{k}-y^{\\star}(\\bar{x}^{k})\\|^{2}]}\\\\ &{=\\!\\!\\widehat{\\mathbb{E}}_{k}\\left[\\left\\|\\bar{y}^{k}-\\beta\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\nabla_{2}g_{i}(x_{i}^{k},y_{i}^{k})-y^{\\star}(\\bar{x}^{k})+\\beta\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\big(\\nabla_{2}g_{i}(x_{i}^{k},y_{i}^{k})-v_{i}^{k}\\big)\\right\\|^{2}\\right]}\\\\ &{\\leq\\!\\left\\|\\bar{y}^{k}-\\beta\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\nabla_{2}g_{i}(\\bar{x}^{k},\\bar{y}^{k})-y^{\\star}(\\bar{x}^{k})+\\beta\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\!\\big(\\nabla_{2}g_{i}(\\bar{x}^{k},\\bar{y}^{k})-\\nabla_{2}g_{i}(x_{i}^{k},y_{i}^{k})\\big)\\right\\|^{2}+\\beta^{2}\\displaystyle\\frac{\\sigma_{g,1}^{2}}{n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Then, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\beta}{8}\\mathbb{E}\\{\\|\\hat{\\theta}^{+}(\\theta^{+})-y^{*}(\\theta^{+})\\|^{2}\\}}\\\\ &{\\leq\\Big(1+\\frac{\\beta\\mu_{0}}{2}\\Big)\\Bigg[\\frac{1}{\\theta^{+}}-\\beta\\frac{1}{n}\\frac{\\sqrt{5}}{n}\\nabla_{2}g(\\hat{x}^{+},y^{+})-y^{*}(\\hat{x}^{+})\\Big]^{2}}\\\\ &{\\quad+\\beta^{2}\\left(1+\\frac{\\beta\\mu_{0}}{2}\\right)\\Bigg]\\frac{1}{n}\\frac{\\sqrt{5}}{n}\\nabla_{2}\\mathbb{E}\\{\\|\\theta_{+}^{+}\\|y^{+}-\\nabla_{2}g(\\hat{x}^{+},y^{+})\\|\\Bigg\\}\\Bigg|^{2}+\\beta^{2}\\frac{\\alpha_{0}^{2}\\beta_{1}^{2}}{n}}\\\\ &{\\leq\\Big(1+\\frac{\\beta\\mu_{0}}{2}\\Big)\\left(1-\\beta\\mu_{0}\\right)\\mathbb{I}\\Bigg\\{\\|\\hat{y}^{+}-y^{+}\\|^{2}\\right\\}}\\\\ &{\\quad+\\beta^{2}\\bigg(1+\\frac{\\beta\\mu_{0}}{2}\\bigg)\\mathbb{I}\\Bigg\\{\\frac{1}{n}\\frac{\\sqrt{5}}{n}-\\frac{x^{2}\\beta_{1}^{2}}{n}+\\frac{\\|y^{+}-\\hat{y}^{+}\\|^{2}}{n}\\Bigg\\}+\\beta^{2}\\frac{\\alpha_{0}^{2}\\beta_{1}^{2}}{n}}\\\\ &{\\leq(1-\\beta\\mu_{0})\\bigg[\\Big(1+\\frac{\\beta\\mu_{0}}{2}\\Big)\\|y^{+}-y^{+}\\|^{2}\\Big(1+\\frac{\\beta\\mu_{0}}{2}\\Big)\\|^{2}+\\Big(1+\\frac{\\beta\\mu_{0}}{2}\\Big)\\|y^{+}\\|^{2}-y^{+}\\|^{2}\\bigg]\\Bigg\\}}\\\\ &{\\quad+\\beta^{2}\\bigg(1+\\frac{\\gamma\\beta_{0}}{2}\\bigg)L_{z}^{2}\\left(\\frac{\\|\\hat{x}^{+}-\\hat{x}^{+}\\|^{2}}{n}+\\frac{\\|\\hat{y}^{+}-\\hat{y}^{+}\\|^{2}}{n}\\right)+\\beta^{2}\\frac{\\beta\\|\\\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where the first and the third inequality is due to the Jenson's inequality, the second inequality holds according to Lemma 2 and the fact that \u03b2 \u2264 32L'g $\\begin{array}{r}{\\beta\\;\\leq\\;\\frac{\\mu_{g}}{32L_{g,1}^{2}}\\;\\leq\\;\\frac{1^{\\bullet}}{3(\\mu_{g}+L_{g,1})}}\\end{array}$ 3(\u03bcg+Lg,1 and the last inequality uses $\\beta\\mu_{g}\\le\\frac{1}{3}$ . Taking the summation and expectation on the both sides, we get: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k=0}^{K}\\frac{\\beta\\mu_{g}}{2}\\mathbb{E}[\\|\\bar{y}^{k}-y^{\\star}(\\bar{x}^{k-1})\\|^{2}]+\\mathbb{E}[\\|\\bar{y}^{k+1}-y^{\\star}(\\bar{x}^{k})\\|^{2}]}\\\\ &{\\displaystyle\\leq\\mathbb{E}\\|\\bar{y}^{0}-y^{\\star}(\\bar{x}^{0})\\|^{2}+\\sum_{k=0}^{K}\\mathbb{E}\\left[\\frac{3\\alpha^{2}}{\\beta\\mu_{g}}L_{y^{\\star}}^{2}\\|\\bar{r}^{k}\\|^{2}+\\frac{3\\beta}{\\mu_{g}}L_{g,1}^{2}\\left(\\frac{\\|{\\bf x}^{k}-\\bar{{\\bf x}}^{k}\\|^{2}}{n}+\\frac{\\|{\\bf y}^{k}-\\bar{{\\bf y}}^{k}\\|^{2}}{n}\\right)+\\beta^{2}\\frac{\\sigma_{g,1}^{2}}{n}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Using (36) and the fact that $\\mathbf{x}^{0},\\mathbf{y}^{0}$ is consensual, it follows that: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\sum_{\\varepsilon=0}^{\\tilde{\\varepsilon}+1}\\frac{\\beta\\mu_{g}}{2}\\mathbb{E}[\\|\\bar{y}^{k}-y^{\\star}(\\bar{x}^{k-1})\\|^{2}]\\leq2\\|\\bar{y}^{0}-y^{\\star}(\\bar{x}^{0})\\|^{2}+\\sum_{k=1}^{K}\\frac{3\\alpha^{2}}{\\beta\\mu_{g}}L_{y^{*}}^{2}\\mathbb{E}\\|\\bar{r}^{k}\\|^{2}}\\\\ {+\\sum_{k=1}^{K}\\frac{3\\beta}{\\mu_{g}}L_{g,1}^{2}\\mathbb{E}\\left[\\frac{\\|\\mathbf{O}_{x}\\|^{2}\\|\\hat{\\mathbf{e}}_{x}^{k}\\|^{2}+\\|\\mathbf{O}_{y}\\|^{2}\\|\\hat{\\mathbf{e}}_{y}^{k}\\|^{2}}{n}\\right]+\\frac{2K\\beta^{2}\\sigma_{g,1}^{2}}{n}.}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "On the other hand, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\widehat{\\mathbb{E}}_{k}\\left[\\|\\tilde{y}^{k+1}-\\bar{y}^{k}\\|^{2}\\right]}\\\\ &{\\le\\!\\beta^{2}\\Bigg\\|\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\nabla_{2}g_{i}(x_{i}^{k},y_{i}^{k})\\Bigg\\|^{2}+\\frac{\\beta^{2}}{n}\\sigma_{g,1}^{2}}\\\\ &{\\le\\!2\\beta^{2}\\left(\\left\\|\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\left(\\nabla_{2}g_{i}(x_{i}^{k},y_{i}^{k})-\\nabla_{2}g_{i}(\\bar{x}^{k},\\bar{y}^{k})\\right)\\right\\|^{2}+\\left\\|\\nabla_{2}g(\\bar{x}^{k},\\bar{y}^{k})-\\nabla_{2}g(\\bar{x}^{k},y^{\\star}(\\bar{x}^{k}))\\right\\|^{2}\\right)}\\\\ &{\\quad+\\frac{\\beta^{2}}{n}\\sigma_{g,1}^{2}}\\\\ &{\\le\\!\\frac{2\\beta^{2}L_{g,1}^{2}}{n}\\left(\\|\\mathbf{x}^{k}-\\bar{\\mathbf{x}}^{k}\\|^{2}+\\|\\mathbf{y}^{k}-\\bar{\\mathbf{y}}^{k}\\|^{2}+2\\|\\bar{\\mathbf{y}}^{k+1}-\\mathbf{y}^{\\star}(\\bar{\\mathbf{x}}^{k})\\|^{2}+2\\|\\bar{\\mathbf{y}}^{k+1}-\\bar{\\mathbf{y}}^{k}\\|^{2}\\right)+\\frac{\\beta^{2}}{n}\\sigma_{g,1}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where the second inequality uses $\\nabla_{2}g(\\bar{x}^{k},y^{\\star}(\\bar{x}^{k}))=0$ ", "page_idx": 31}, {"type": "text", "text": "Note that $\\begin{array}{r l r}{\\beta^{2}\\!}&{{}\\le}&{\\!\\frac{\\mu_{g}^{2}}{32L_{g,1}^{4}}\\;\\le\\;\\frac{1}{8L_{g,1}^{2}}}\\end{array}$ .Subtracting $2\\beta^{2}L_{g,1}^{2}\\|\\bar{y}^{k+1}\\,-\\,\\bar{y}^{k}\\|$ on both sides, and taking expectation and summation, we get: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\hat{t}=\\hat{t}\\displaystyle\\frac{\\hat{\\sigma}_{j}^{2}}{n}\\sum_{s=1}^{n-1}\\Big(\\alpha\\ln\\frac{1}{n}\\Big(b^{2}+\\eta^{2}\\Big)+\\frac{\\eta^{2}\\hat{\\sigma}_{j}^{2}}{n}\\Big)+\\frac{8\\hat{\\sigma}_{j}^{2}\\hat{\\sigma}_{j}^{2}}{n}\\chi_{j}\\sin^{4}\\eta^{3}+\\gamma^{3}\\left(\\frac{\\alpha^{2}\\eta^{2}}{n}\\right)^{2}}\\\\ &{\\quad\\le\\displaystyle\\sum_{s=1}^{n-1}\\Bigg(\\frac{\\sin\\frac{\\eta}{\\alpha}}{n}\\Big)\\Big(\\sin\\frac{\\eta}{\\alpha}-\\sin^{2}{\\frac{\\eta}{\\alpha}}\\Big)+\\frac{8\\pi^{2}\\hat{\\sigma}_{j}^{2}}{n}\\frac{\\hat{\\sigma}_{j}^{2}}{n}\\sin\\frac{\\eta}{\\alpha}\\Big(b^{4}+\\eta^{2}\\Big)}\\\\ &{\\quad+\\frac{4\\pi^{2}\\hat{\\sigma}_{j}^{2}}{n}\\frac{\\hat{\\sigma}_{j}^{2}}{n}}\\\\ &{\\quad\\le\\displaystyle\\frac{4\\hat{t}\\displaystyle\\langle\\mathbf{X}_{j}\\rangle^{2}}{n}\\frac{\\hat{\\sigma}_{j}^{2}}{n},}\\\\ &{\\quad\\le\\displaystyle\\frac{4\\hat{t}\\displaystyle\\langle\\mathbf{I}_{j}\\rangle\\hat{\\sigma}_{j}^{2}}{n}\\sum_{s=1}^{n-1}\\Big(\\eta\\Big(\\frac{\\eta}{\\alpha}\\Big)^{2}\\Big\\vert\\Big\\vert\\mathbf{X}_{j}\\Big\\vert^{2}+\\Big\\vert\\mathbf{I}_{j}\\Big\\vert^{2}\\Big\\vert+\\frac{2\\pi^{2}}{n}\\Big)\\frac{\\hat{\\sigma}_{j}^{2}}{n},}\\\\ &{\\quad\\quad+8\\beta^{2}L_{2}^{2}\\sin\\frac{\\eta}{n}\\Bigg(b\\bigg(\\frac{\\eta}{\\alpha}\\Big)^{2}\\Big\\vert\\Big\\vert\\mathbf{X}_{j}\\Big\\vert^{2}+\\Big\\vert\\mathbf{I}_{j}\\Big\\vert^{2}\\Big\\vert+\\frac{8\\pi^{2}}{n}\\Big)\\frac{\\hat{\\sigma}_{j}^{2}}{n}\\frac{\\hat{\\sigma}_{j}^{2}}{n}\\Bigg)}\\\\ &{\\quad\\quad+8\\beta^{2}L_{2}^{2}\\sin \n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where the second inequalty holdsince $\\mathbf{x}^{0},\\mathbf{y}^{0}$ are qalt last inequality holds since $\\begin{array}{r}{\\bar{\\beta}\\leq\\frac{\\mu_{g}}{32L_{g,1}^{2}}}\\end{array}$ \u53e3 ", "page_idx": 31}, {"type": "text", "text": "Lemma 10 (Estimation error of $z^{\\star}(x)$ ). Suppose that Assumptions 1- 4hold, and ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\gamma<\\operatorname*{min}\\left\\{\\frac{1}{\\mu_{g}},\\frac{n L_{g,1}^{2}}{\\mu_{g}^{2}\\sigma_{g,2}^{2}},\\frac{n\\mu_{g}}{36\\sigma_{g,2}^{2}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We have: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\sum_{k=0}^{K+1}\\mathbb{E}\\|\\bar{z}^{k}-z_{\\star}^{k}\\|^{2}}\\\\ &{\\leq\\displaystyle\\sum_{k=0}^{K}\\frac{9\\alpha^{2}L_{z^{*}}^{2}}{\\gamma^{2}\\mu_{g}^{2}}\\mathbb{E}\\|\\bar{r}^{k}\\|^{2}}\\\\ &{\\quad+\\displaystyle\\top2\\kappa^{2}\\sum_{k=1}^{K}\\mathbb{E}\\left[\\frac{\\kappa^{2}\\|\\mathbf{O}_{x}\\|^{2}\\|\\hat{\\mathbf{e}}_{x}^{k}\\|^{2}+\\|\\mathbf{O}_{x}\\|^{2}\\|\\hat{\\mathbf{e}}_{z}^{k}\\|^{2}}{n}\\right]+72\\kappa^{2}\\displaystyle\\sum_{k=0}^{K}\\mathbb{E}\\left[\\frac{\\kappa^{2}\\|\\mathbf{O}_{y}\\|^{2}\\|\\hat{\\mathbf{e}}_{y}^{k+1}\\|^{2}}{n}\\right]}\\\\ &{\\quad+\\displaystyle\\sum_{k=0}^{K}72\\kappa^{4}\\mathbb{E}\\left[\\|\\bar{y}^{k+1}-y^{\\star}(\\bar{x}^{k})\\|^{2}\\right]+\\frac{3\\|z_{\\star}^{1}\\|^{2}}{\\mu_{g}\\gamma}+\\frac{6(K+1)\\gamma}{\\mu_{g}n}\\left(3\\sigma_{g,2}^{2}\\frac{L_{f,0}^{2}}{\\mu_{g}^{2}}+\\sigma_{f,1}^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Proof. For each $k\\geq0$ note that $z_{\\star}^{k}=\\nabla_{22}^{2}g(\\bar{x}^{k},y^{\\star}(\\bar{x}^{k}))^{-1}\\nabla_{2}f_{2}(\\bar{x}^{k},y^{\\star}(\\bar{x}^{k}))$ , we have: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\widetilde{\\mathbb{E}}_{k}\\big[\\bar{z}^{k+1}\\big]-z_{\\star}^{k+1}=\\bar{z}^{k}-\\frac{\\gamma}{n}\\sum_{i=1}^{n}\\big(\\nabla_{22}^{2}g_{i}(x_{i}^{k},y_{i}^{k+1})z_{i}^{k}-\\nabla_{2}f_{i}(x_{i}^{k},y_{i}^{k+1})\\big)-z_{\\star}^{k+1}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle=\\left[I-\\frac{\\gamma}{n}\\sum_{i=1}^{n}\\nabla_{22}^{2}g_{i}(x_{i}^{k},y_{i}^{k+1})\\right](\\bar{z}^{k}-z_{\\star}^{k+1})+\\frac{\\gamma}{n}\\sum_{i=1}^{n}\\left[\\nabla_{2}f_{i}(x_{i}^{k},y_{i}^{k+1})-\\nabla_{2}f_{i}(\\bar{x}^{k},y^{\\star}(\\bar{x}^{k}))\\right]}}\\\\ {{\\displaystyle~~+\\frac{\\gamma}{n}\\sum_{i=1}^{n}\\nabla_{22}^{2}g_{i}(x_{i}^{k},y_{i}^{k+1})(\\bar{z}^{k}-z_{i}^{k})+\\frac{\\gamma}{n}\\sum_{i=1}^{n}\\left[\\nabla_{22}^{2}g_{i}(\\bar{x}^{k},y^{\\star}(\\bar{x}^{k}))-\\nabla_{22}^{2}g_{i}(x_{i}^{k},y_{i}^{k+1})\\right]z_{\\star}^{k+1}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Then we have: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\left|\\frac{\\bar{g}_{1}}{\\bar{R}_{1}}\\right|^{2+\\bar{n}-\\frac{2}{p_{1}+1}}\\right|^{2}}\\\\ &{\\le(1+\\gamma_{0}^{2})\\left|\\left|\\left[-\\frac{2}{3}\\frac{\\bar{g}_{2}^{*}}{p_{1}+1}\\nabla_{\\overline{{\\theta}}}^{2}\\bar{R}_{1}(\\theta_{1}^{*},\\theta_{1}^{*+1})\\right](\\theta^{*}-\\theta_{2}^{*+1})\\right|^{2}}\\\\ &{\\quad+3\\gamma^{2}\\left(1+\\frac{2}{\\gamma_{0}\\bar{R}_{1}}\\right)\\left|\\frac{1}{\\bar{\\theta}_{1}^{*}}\\right|^{2}\\mathrm{Eq}_{2}[(\\theta_{1}^{*},\\theta_{1}^{*+1})\\cdot\\nabla_{\\theta}](\\theta^{*},\\theta_{2}^{*+1})\\right|^{2}}\\\\ &{\\quad+3\\gamma^{2}\\left(1+\\frac{2}{\\gamma_{0}\\bar{R}_{1}}\\right)\\left|\\left|\\frac{\\bar{g}_{1}}{\\bar{R}_{2}}\\right|\\left|\\nabla_{\\overline{{\\theta}}}^{2}\\left|\\left|\\bar{R}_{2}(\\theta^{*},\\theta_{2}^{*+1})-\\nabla_{\\theta}\\bar{R}_{1}(\\theta^{*},\\theta_{2}^{*+1})\\right|^{2}\\right|}\\\\ &{\\quad+3\\gamma^{2}\\left(1+\\frac{2}{\\gamma_{0}\\bar{R}_{1}}\\right)\\left|\\frac{1}{\\bar{\\theta}_{1}^{*}}\\right|\\sqrt{1+\\frac{2}{\\gamma_{0}\\bar{R}_{1}}}\\left|(\\theta^{*},\\theta^{*})\\left(\\theta^{*},\\theta^{*}\\right)\\right|-\\nabla_{\\theta}^{*}\\left|\\left|\\left(\\frac{\\bar{R}_{1}\\bar{\\theta}_{1}^{*+1}}{\\bar{R}_{1}^{2}}\\right)\\right|^{2+\\frac{2}{\\gamma_{0}\\bar{R}_{1}^{2}}}\\right|^{2}}\\\\ &{\\quad+3\\gamma^{2}\\left(1+\\frac{2}{\\gamma_{0}\\bar{R}_{1}}\\right)\\left|\\frac{1}{\\bar{\\theta}_{1}^{ \n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where the first and third inequality uses Jensen's inequality and Cauchy Schwartz inequality, the second inequality holds due to Assumption 1 and $\\gamma\\mu_{g}<1$ , the last inequality holds since $z^{\\star}(x)$ $L_{z^{\\star}}$ Lipschitz continuous. ", "page_idx": 32}, {"type": "text", "text": "Moreover, the independence of samples implies that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{\\mathbb{E}}_{k}\\left\\|\\bar{z}^{k+1}-\\widetilde{\\mathbb{E}}_{k}[\\bar{z}^{k+1}]\\right\\|^{2}=\\gamma^{2}\\widetilde{\\mathbb{E}}_{k}\\left\\|\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}(H_{i}^{k}-\\widetilde{\\mathbb{E}}_{k}[H_{i}^{k}])z_{i}^{k}+\\displaystyle\\frac{1}{n}\\sum_{i}(b_{i}^{k}-\\widetilde{\\mathbb{E}}_{k}[b_{i}^{k}])\\right\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\displaystyle\\frac{2\\gamma^{2}}{n}\\left(\\sigma_{g,2}^{2}\\displaystyle\\frac{\\|{\\bf z}^{k}\\|^{2}}{n}+\\sigma_{f,1}^{2}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\leq\\displaystyle\\frac{2\\gamma^{2}}{n}\\left(3\\sigma_{g,2}^{2}\\left(\\displaystyle\\frac{\\|{\\bf z}^{k}-\\bar{\\bf z}_{k}\\|^{2}}{n}+\\|\\bar{z}^{k}-z_{\\star}^{k}\\|^{2}+\\displaystyle\\frac{L_{f,0}^{2}}{\\mu_{g}^{2}}\\right)+\\sigma_{f,1}^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "As $\\gamma$ satisfies ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\frac{6\\sigma_{g,2}^{2}\\gamma^{2}}{n}\\leq\\frac{6\\gamma L_{g,1}^{2}}{\\mu_{g}^{2}},\\quad\\frac{6\\sigma_{g,2}^{2}\\gamma^{2}}{n}\\leq\\frac{\\gamma\\mu_{g}}{6},\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "we get: ", "text_level": 1, "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\widetilde{\\mathbb{E}}_{k}[\\|\\bar{z}^{k+1}-z_{\\star}^{k+1}\\|^{2}]=\\widetilde{\\mathbb{E}}_{k}\\|\\widetilde{\\mathbb{E}}_{k}[\\bar{z}^{k+1}]-z_{\\star}^{k+1}\\|^{2}+\\widetilde{\\mathbb{E}}_{k}\\|\\bar{z}^{k+1}-\\widetilde{\\mathbb{E}}_{k}[\\bar{z}^{k+1}]\\|^{2}}\\\\ &{\\leq\\left(1-\\frac{\\gamma\\mu_{g}}{3}\\right)\\|\\bar{z}^{k}-z_{\\star}^{k}\\|^{2}+\\frac{3\\alpha^{2}L_{z^{*}}^{2}}{\\gamma\\mu_{g}}\\|\\bar{r}^{k}\\|^{2}+\\frac{12\\gamma}{\\mu_{g}}L_{g,1}^{2}\\frac{\\|{\\mathbf{z}}^{k}-\\bar{\\mathbf{z}}^{k}\\|^{2}}{n}+\\frac{2\\gamma^{2}}{n}\\left(3\\sigma_{g,2}^{2}\\frac{L_{f,0}^{2}}{\\mu_{g}^{2}}+\\sigma_{f,1}^{2}\\right)}\\\\ &{\\quad+\\frac{12\\gamma}{\\mu_{g}}\\left(\\frac{L_{g,2}^{2}L_{f,0}^{2}}{\\mu_{g}^{2}}+L_{f,1}^{2}\\right)\\left(\\frac{\\|{\\mathbf{x}}^{k}-\\bar{\\mathbf{x}}^{k}\\|^{2}}{n}+\\frac{\\|{\\mathbf{y}}^{k+1}-\\bar{\\mathbf{y}}^{k+1}\\|^{2}}{n}+\\|\\bar{y}^{k+1}-y^{\\star}(\\bar{x}^{k})\\|^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Taking expectation and summation on both sides, we get ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k=0}^{K}\\frac{\\gamma\\mu_{g}}{3}\\mathbb{E}\\|\\bar{z}^{k}-z_{\\star}^{k}\\|^{2}+\\mathbb{E}\\|\\bar{z}^{K+1}-z_{\\star}^{K+1}\\|^{2}}\\\\ &{\\le\\!\\mathbb{E}\\|\\bar{z}^{0}-z_{\\star}^{0}\\|^{2}+\\displaystyle\\sum_{k=0}^{K}\\left[\\frac{3\\alpha^{2}L_{z^{*}}^{2}}{\\gamma\\mu_{g}}\\mathbb{E}\\|\\bar{r}^{k}\\|^{2}+\\frac{12\\gamma}{\\mu_{g}}L_{g,1}^{2}\\frac{\\mathbb{E}\\|\\mathbf{z}^{k}-\\bar{\\mathbf{z}}^{k}\\|^{2}}{n}+\\frac{2\\gamma^{2}}{n}\\left(3\\sigma_{g,2}^{2}\\frac{L_{f,0}^{2}}{\\mu_{g}^{2}}+\\sigma_{f,1}^{2}\\right)\\right]}\\\\ &{\\displaystyle\\ +\\sum_{k=0}^{K}\\frac{12\\gamma}{\\mu_{g}}\\left(\\frac{L_{g,2}^{2}L_{f,0}^{2}}{\\mu_{g}^{2}}+L_{f,1}^{2}\\right)\\mathbb{E}\\left[\\frac{\\|\\mathbf{x}^{k}-\\bar{\\mathbf{x}}^{k}\\|^{2}}{n}+\\frac{\\|\\mathbf{y}^{k+1}-\\bar{\\mathbf{y}}^{k+1}\\|^{2}}{n}+\\|\\bar{y}^{k+1}-y^{\\star}(\\bar{x}^{k})\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "It follows that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\sum_{k=0}^{K+1}\\mathbb{E}\\|\\bar{z}^{k}-z_{\\star}^{k}\\|^{2}}\\\\ &{\\leq\\displaystyle\\sum_{k=0}^{K}\\frac{9\\alpha^{2}L_{z}^{2}}{\\gamma^{2}\\mu_{g}^{2}}\\mathbb{E}\\|\\bar{r}^{k}\\|^{2}}\\\\ &{\\quad+\\displaystyle\\mathcal{T}2\\kappa^{2}\\sum_{k=1}^{K}\\mathbb{E}\\left[\\frac{\\kappa^{2}\\|\\mathbf{O}_{x}\\|^{2}\\|\\hat{\\mathbf{e}}_{x}^{k}\\|^{2}+\\|\\mathbf{O}_{x}\\|^{2}\\|\\hat{\\mathbf{e}}_{z}^{k}\\|^{2}}{n}\\right]+72\\kappa^{2}\\displaystyle\\sum_{k=0}^{K}\\mathbb{E}\\left[\\frac{\\kappa^{2}\\|\\mathbf{O}_{y}\\|^{2}\\|\\hat{\\mathbf{e}}_{y}^{k+1}\\|^{2}}{n}\\right]}\\\\ &{\\quad+\\displaystyle\\sum_{k=0}^{K}72\\kappa^{4}\\mathbb{E}\\left[\\|\\bar{y}^{k+1}-y^{\\star}(\\bar{x}^{k})\\|^{2}\\right]+\\frac{3\\|z_{k}^{1}\\|^{2}}{\\mu_{g}\\gamma}+\\frac{6(K+1)\\gamma}{\\mu_{g}n}\\left(3\\sigma_{g,2}^{2}\\frac{L_{f,0}^{2}}{\\mu_{g}^{2}}+\\sigma_{f,1}^{2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "since $z_{\\star}^{0}=z_{\\star}^{1}$ and $\\mathbf{z}^{0}$ is consensual. ", "page_idx": 33}, {"type": "text", "text": "Then, we combine the results in Lemmas 9, 10 and give an upper bound of $\\mathbb{E}[I_{k}]$ ", "page_idx": 33}, {"type": "text", "text": "Lemma 11. Suppose that Lemmas $^{\\,g}$ and 10 hold. Then we have: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k=-1}^{K}\\mathbb{E}[I_{k}]\\leq\\left(\\frac{9\\alpha^{2}L_{z}^{2}}{\\gamma^{2}\\mu_{g}^{2}}+\\frac{438\\kappa^{4}\\alpha^{2}}{\\beta^{2}\\mu_{g}^{2}}L_{y}^{2}.\\right)\\displaystyle\\sum_{k=0}^{K}\\mathbb{E}\\|\\bar{r}^{k}\\|^{2}+510\\kappa^{4}\\displaystyle\\sum_{k=0}^{K}\\mathbb{E}\\left[\\frac{\\Delta_{k}}{n}\\right]+\\frac{3\\|z_{k}^{1}\\|^{2}}{\\mu_{g}\\gamma}}\\\\ &{\\qquad\\qquad\\qquad+\\left.\\frac{6(K+1)\\gamma}{\\mu_{g}n}\\left(3\\sigma_{g,2}^{2}\\frac{L_{f,0}^{2}}{\\mu_{g}^{2}}+\\sigma_{f,1}^{2}\\right)+73\\kappa^{4}\\left(\\frac{4}{\\beta\\mu_{g}}\\|\\bar{y}^{0}-y^{\\star}(\\bar{x}^{0})\\|^{2}+\\frac{4K\\sigma_{g,1}^{2}}{n\\mu_{g}}\\beta\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Remark 6. Here $I_{-1}=\\|\\bar{z}^{0}-z_{\\star}^{0}\\|^{2}+\\kappa^{2}\\|\\bar{y}^{0}-y^{\\star}(\\bar{x}^{-1})\\|^{2}$ . The aim of introducing this term is to simplify the subsequent proofs of other lemmas. ", "page_idx": 33}, {"type": "text", "text": "Proof. Lemma 10 implies that: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\sum_{k=0}^{K+1}\\mathbb{E}\\|\\bar{z}^{k}-z_{\\star}^{k}\\|^{2}}\\\\ &{\\leq\\displaystyle\\sum_{k=0}^{K}\\frac{9\\alpha^{2}L_{z^{*}}^{2}}{\\gamma^{2}\\mu_{g}^{2}}\\mathbb{E}\\|\\bar{r}^{k}\\|^{2}}\\\\ &{\\quad+\\displaystyle\\top2\\kappa^{2}\\displaystyle\\sum_{k=1}^{K}\\mathbb{E}\\left[\\frac{\\kappa^{2}\\|\\Phi_{x}\\|^{2}\\|\\hat{\\mathbf{e}}_{x}^{k}\\|^{2}+\\|\\mathbf{O}_{x}\\|^{2}\\|\\hat{\\mathbf{e}}_{z}^{k}\\|^{2}}{n}\\right]+72\\kappa^{2}\\displaystyle\\sum_{k=0}^{K}\\mathbb{E}\\left[\\frac{\\kappa^{2}\\|\\mathbf{O}_{y}\\|^{2}\\|\\hat{\\mathbf{e}}_{y}^{k+1}\\|^{2}}{n}\\right]}\\\\ &{\\quad+\\displaystyle\\sum_{k=0}^{K}72\\kappa^{4}\\mathbb{E}\\left[\\|\\bar{y}^{k+1}-y^{\\star}(\\bar{x}^{k})\\|^{2}\\right]+\\frac{3\\|z_{k}^{1}\\|^{2}}{\\mu_{g}\\gamma}+\\frac{6(K+1)\\gamma}{\\mu_{g}n}\\left(3\\sigma_{g,2}^{2}\\frac{L_{f,0}^{2}}{\\mu_{g}^{2}}+\\sigma_{f,1}^{2}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Then, using Lemma 9, we have: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\underset{k=1}{\\overset{k}{\\sum}}\\:\\mathbb{E}\\big[\\hat{L}_{k}\\big]=\\underset{\\ell=0}{\\overset{k}{\\sum}}\\:[\\hat{\\mathbf{z}}_{k}^{k}-\\hat{\\mathbf{z}}_{k}^{k}][\\hat{\\mathbf{z}}-\\boldsymbol{x}_{k}^{k}]^{2}+\\boldsymbol{x}_{k}^{2}\\,\\overset{k+1}{\\sum}\\sum_{k=1}^{k}\\mathbb{E}\\big[\\hat{\\mathbf{z}}_{k}^{k}-\\boldsymbol{y}^{k}(\\hat{\\mathbf{z}}^{k-1})\\big]^{2}}\\\\ &{\\leq\\underset{k=0}{\\overset{k}{\\sum}}\\:\\frac{9\\uprho_{k}^{2}L_{k}^{2}}{\\sqrt{\\rho_{k}^{2}}}[\\hat{\\mathbf{z}}]^{k}}\\\\ &{\\quad+2\\boldsymbol{x}_{k}^{2}\\,\\overset{k}{\\sum}\\:\\mathbb{E}\\left[\\left[\\Phi_{0}^{2}\\left|\\frac{\\hat{\\mathbf{z}}_{k}^{2}}{\\rho_{k}^{2}}\\right|^{2}\\right]\\hat{\\mathbf{z}}_{k}^{k}\\right]^{2}+2\\boldsymbol{\\mathbf{0}}{\\overset{k}{\\sum}}\\Bigg[\\eta\\left[\\frac{\\eta}{\\sqrt{3}}\\right]\\left[\\Phi_{0}^{2}\\left|\\frac{\\hat{\\mathbf{z}}_{k}^{4}+1}{\\eta\\left[\\sqrt{3}\\right]}\\right|^{2}\\right]}\\\\ &{\\quad+\\frac{6\\left(K+1\\right)}{\\rho_{k}\\eta_{k}}\\left(\\lambda_{0}^{2}\\frac{L_{k}^{2}}{\\rho_{k}^{2}}\\sigma_{k}^{2}\\sigma_{k}^{2}\\sigma_{k}^{2}\\right)+\\mathcal{O}_{2,k}\\Bigg[\\frac{4}{\\beta_{k}\\eta_{k}}\\left|\\Phi_{0}^{2}-\\boldsymbol{y}^{k}(\\hat{\\mathbf{z}}^{k})\\right|^{2}+\\sum_{k=1}^{K}\\hat{\\rho}_{k}^{2}\\frac{L_{k}^{2}}{\\rho_{k}^{2}}\\sigma_{k}^{2}\\sigma_{k}^{2}\\left|\\Phi_{1}^{2}\\right|^{2}\\Bigg]}\\\\ &{\\quad+\\frac{3\\left(\\frac{1}{\\sqrt{3}}\\right)^{2}}{\\rho_{k}\\gamma}+\\mathcal{O}_{\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "C.1.7 Consensus error analysis ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "In this subsection we aim to bound the consensus erors of $y,z,x$ (i.e. the terms $\\|\\hat{\\mathbf e}_{y}^{k}\\|^{2},\\|\\hat{\\mathbf e}_{z}^{k}\\|^{2}$ ,and $\\|\\hat{\\mathbf e}_{x}^{k}\\|^{2})$ ", "page_idx": 34}, {"type": "text", "text": "Lemma 12 (Consensus error of $y$ ). Suppose that Assumptions 1- 4 hold, and ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\beta^{2}\\leq\\frac{(1-\\|\\Gamma_{y}\\|)^{2}}{8L_{g,1}^{2}\\|\\mathbf{O}_{y}^{-1}\\|^{2}\\|\\mathbf{O}_{y}\\|^{2}\\|\\mathbf{A}_{y a}\\|^{2}}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\displaystyle\\sum_{k=0}^{K+1}\\mathbb{E}\\|\\hat{\\mathbf{e}}_{y}^{k}\\|^{2}\\leq3\\displaystyle\\sum_{k=0}^{K}\\frac{\\beta^{2}\\|\\mathbf{O}_{y}^{-1}\\|^{2}}{(1-\\|\\mathbf{T}_{y}\\|)^{2}}\\|\\mathbf{A}_{y b}^{-1}\\|^{2}\\|\\mathbf{A}_{y a}\\|^{2}L_{g,1}^{2}\\mathbb{E}\\left[\\|\\bar{\\mathbf{x}}^{k+1}-\\bar{\\mathbf{x}}^{k}\\|^{2}+\\|\\bar{\\mathbf{y}}^{k+1}-\\bar{\\mathbf{y}}^{k}\\|^{2}\\right]}\\\\ {\\displaystyle+\\left.\\frac{\\|\\mathbf{O}_{x}\\|^{2}}{3\\|\\mathbf{O}_{y}\\|^{2}}\\sum_{k=0}^{K}\\mathbb{E}\\|\\hat{\\mathbf{e}}_{x}^{k}\\|^{2}+\\frac{3(K+1)\\beta^{2}\\|\\mathbf{O}_{y}^{-1}\\|^{2}\\|\\mathbf{A}_{y a}\\|^{2}}{1-\\|\\mathbf{T}_{y}\\|}n\\sigma_{g,1}^{2}+\\frac{2\\mathbb{E}\\|\\hat{\\mathbf{e}}_{y}^{0}\\|^{2}}{1-\\|\\mathbf{T}_{y}\\|}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\mathbf{\\hat{e}}_{y}^{k+1}\\right\\|^{2}}\\\\ &{=\\left\\|\\mathbf{r}_{y}\\hat{\\mathbf{e}}_{y}^{k}-\\beta\\mathbf{O}_{y}^{-1}\\left[\\begin{array}{c}{\\Lambda_{y a}\\hat{\\mathbf{U}}_{y}^{\\top}\\left[\\hat{\\mathbf{E}}_{k}[\\mathbf{e}^{k}]-\\nabla_{2}\\mathbf{g}(\\bar{\\mathbf{x}}^{k},\\mathbf{y}^{k})\\right]}\\\\ {\\Lambda_{y b}^{-1}\\Lambda_{y a}\\hat{\\mathbf{U}}_{y}^{\\top}\\left[\\nabla_{2}\\mathbf{g}(\\bar{\\mathbf{x}}^{k+1},\\mathbf{y}^{k+1})-\\nabla_{2}\\mathbf{g}(\\bar{\\mathbf{x}}^{k},\\bar{\\mathbf{y}}^{k})\\right]}\\end{array}\\right]-\\beta\\mathbf{O}_{y}^{-1}\\left[\\begin{array}{c}{\\Lambda_{y a}\\bar{\\mathbf{U}}_{y}^{\\top}\\left[\\mathbf{v}_{0}^{k}-\\hat{\\mathbf{E}}_{k}[\\mathbf{v}^{k}]\\right]}\\\\ {0}\\end{array}\\right]\\right\\|^{2}}\\\\ &{=\\left\\|\\mathbf{r}_{y}\\hat{\\mathbf{e}}_{y}^{k}-\\beta\\mathbf{O}_{y}^{-1}\\left[\\begin{array}{c}{\\Lambda_{y b}\\bar{\\mathbf{A}}_{y a}\\bar{\\mathbf{U}}_{y}^{\\top}\\left[\\hat{\\nabla}_{2}\\mathbf{g}(\\bar{\\mathbf{x}}^{k},\\nabla^{k})-\\nabla_{2}\\mathbf{g}(\\bar{\\mathbf{x}}^{k},\\bar{\\mathbf{y}}^{k})\\right]}\\\\ {\\Lambda_{y b}\\bar{\\mathbf{A}}_{y a}\\bar{\\mathbf{U}}_{y}^{\\top}\\left[\\nabla_{2}\\mathbf{g}(\\bar{\\mathbf{x}}^{k+1},\\nabla^{k+1})-\\nabla_{2}\\mathbf{g}(\\bar{\\mathbf{x}}^{k},\\bar{\\mathbf{y}}^{k})\\right]}\\end{array}\\right]\\right\\|^{2}}\\\\ &{\\quad+\\beta^{2}\\left\\|\\mathbf{O}_{y}^{-1}\\left[\\begin{array}{c}{\\Lambda_{y a}\\bar{\\mathbf{U}}_{y}^{\\top}\\left[\\mathbf{v}_{k}^{k}-\\hat{\\mathbf{E}}_{k}[\\mathbf{v}^{k}]\\right]}\\\\ {0}\\end{array}\\right]\\right\\|^{2}-2\\left\\\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "due to Eq.(33). Then, for the first term in the right-hand side of (60), we have: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\widehat{\\mathbb{E}}_{k}\\left[\\left\\|\\Gamma_{\\Psi}\\right\\|_{\\Psi}^{k}\\!\\!\\!\\!\\!\\!+\\!\\!\\beta\\!\\!\\!\\!_{\\Psi}\\!\\!\\!\\!_{\\nu}^{-1}\\left[\\begin{array}{c}{\\lambda_{\\Psi}\\!\\!\\!\\!\\!\\!+\\!\\!\\!\\!\\frac{\\Delta_{\\Psi}}{\\Delta_{\\Psi}}\\!\\!\\!\\!\\!(\\overline{{\\Psi}}_{\\Psi}|\\!\\!\\!\\!\\!-\\!\\!\\!\\zeta_{2}\\!\\!\\!\\!\\!+\\!\\!\\!\\frac{\\lambda_{\\Psi}}{\\Delta_{\\Psi}}\\!\\!\\!\\!\\!\\!(\\overline{{\\mathbf{x}}}^{k},\\cdot\\!\\!\\!\\!\\!\\Delta_{\\Psi})\\!\\!\\!\\!\\!\\Big)}\\\\ {\\Lambda_{\\Psi}\\!\\!\\!\\!\\!\\!+\\!\\!\\!\\!\\frac{\\Delta_{\\Psi}}{\\Delta_{\\Psi}}\\!\\!\\!\\!\\!\\!(\\overline{{\\mathbf{x}}}^{k}\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\frac{\\!\\!\\Delta_{\\Psi}}{\\Delta_{\\Psi}}\\!\\!\\!\\!\\!\\!)\\!\\!\\!}\\\\ {\\Lambda_{\\Psi}\\!\\!\\!\\!\\!\\!+\\!\\!\\!\\!\\!\\frac{\\Delta_{\\Psi}}{\\Delta_{\\Psi}}\\!\\!\\!\\!\\!\\!(\\overline{{\\mathbf{x}}}^{k}\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\frac{\\!\\!\\Delta_{\\Psi}}{\\Delta_{\\Psi}}\\!\\!\\!\\!\\!\\!)\\!\\!\\!\\!}\\\\ {\\leq\\!\\!\\!\\!\\!\\!\\|\\mathbf{r}_{y}\\|\\!\\!\\!\\!\\!\\|\\psi_{y}^{k}\\|\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\frac{\\delta_{\\mathbf{r}}^{2}}{1-|\\mathbf{r}_{y}|\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\zeta_{2}}\\!\\!\\!\\!\\!\\!\\!\\|\\mathbf{\\zeta}_{\\phi}^{k}\\!\\!\\!\\!\\!\\!\\!\\prod\\left[\\left[\\begin{array}{c}{\\lambda_{\\Psi}\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\lambda_{\\Psi}\\!\\!\\!\\!\\!\\!(\\overline{{\\mathbf{x}}}^{k}\\cdot\\!\\!\\!\\!\\!\\mathbf{x}^{k}\\!\\!\\!\\!\\!)\\!\\!\\!\\!-\\!\\!\\!\\!\\nabla_{2}g(\\overline{{\\mathbf{x}}}^{k},\\cdot\\!\\!\\!\\!\\!\\mathbf{y}^\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where the first inequality uses the Jenson's inequality,the second inequality hold since $\\|\\hat{\\mathbf{U}}_{y}^{\\top}\\|\\leq1$ For the second term, we have: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{\\mathbb{E}}_{k}\\left[\\left\\|\\mathbf{O}_{y}^{-1}\\left[\\begin{array}{c}{\\mathbf{A}_{y a}\\hat{\\mathbf{U}}_{y}^{\\top}\\left[\\mathbf{v}^{k}-\\widehat{\\mathbb{E}}_{k}[\\mathbf{v}^{k}]\\right]}\\\\ {\\mathbf{0}}\\end{array}\\right]\\right\\|^{2}\\right]\\leq\\|\\mathbf{O}_{y}^{-1}\\|^{2}\\|\\mathbf{A}_{y a}\\|^{2}n\\sigma_{g,1}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "For the third them, we have: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{\\mathbb{E}}_{k}\\left[\\left\\langle\\mathbf{\\Gamma}_{y}\\hat{\\mathbf{e}}_{y}^{k},\\beta\\mathbf{O}_{y}^{-1}\\left[\\begin{array}{c}{\\mathbf{\\Delta}_{\\mathbf{A}_{y a}}\\hat{\\mathbf{U}}_{y}^{\\top}\\left[\\mathbf{v}^{k}-\\widehat{\\mathbb{E}}_{k}[\\mathbf{v}^{k}]\\right]}\\\\ {\\mathbf{0}}\\end{array}\\right]\\right\\rangle\\right]=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Next, for the last term, we have: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\widehat{\\mathbb{E}}_{k}\\left\\langle\\mathbf{O}_{y}^{-1}\\left[\\begin{array}{c c}{\\Delta_{y}\\mathbf{\\bar{u}}_{y}^{\\top}\\left[\\widehat{\\mathbb{E}}_{k}|\\mathbf{v}^{k}|-\\nabla_{2}\\mathbf{g}(\\mathbf{g}^{k},\\widehat{\\mathbf{g}}^{k})\\right]}\\\\ {\\Delta_{y}\\mathbf{\\bar{u}}_{y}^{-1}\\mathbf{\\bar{u}}_{y}\\mathbf{\\bar{u}}_{y}^{\\top}\\left[\\nabla_{2}\\mathbf{g}(\\widehat{\\mathbf{x}}^{k+1},\\widehat{\\mathbf{p}}^{k+1})-\\nabla_{2}\\mathbf{g}(\\mathbf{x}^{k},\\widehat{\\mathbf{g}}^{k})\\right]}\\end{array}\\right],\\mathbf{O}_{y}^{-1}\\left[\\begin{array}{c c}{\\Delta_{y}\\mathbf{\\bar{u}}_{y}^{\\top}\\left[\\mathbf{v}^{k}-\\widehat{\\mathbb{E}}_{k}|\\mathbf{v}^{k}|\\right]}\\\\ {\\mathbf{O}_{y}^{-1}\\left[\\mathbf{O}_{y}^{-1}\\right]\\mathcal{R}_{k}^{-1}}\\end{array}\\right]\\right\\rangle}\\\\ &{\\leq\\frac{1}{2}\\|\\mathbf{O}_{y}^{-1}\\|^{2}\\widehat{\\mathbb{E}}_{k}\\left\\|\\left[\\begin{array}{c c}{\\Delta_{y}\\mathbf{A}_{y}^{\\top}\\left[\\widehat{\\mathbb{E}}_{k}|\\mathbf{v}^{k}-\\nabla_{2}\\mathbf{g}(\\widehat{\\mathbf{x}}^{k},\\widehat{\\mathbf{y}}^{k})\\right]}\\\\ {\\Delta_{y}\\mathbf{\\bar{A}}_{x y}^{-1}\\mathbf{A}_{y}\\bar{\\mathbb{G}}_{y}^{\\top}\\left[\\nabla_{2}\\mathbf{g}(\\widehat{\\mathbf{x}}^{k+1},\\widehat{\\mathbf{y}}^{k+1})-\\nabla_{2}\\mathbf{g}(\\widehat{\\mathbf{x}}^{k},\\widehat{\\mathbf{y}}^{k})\\right]}\\\\ {+\\left.\\frac{1}{2}\\|\\mathbf{O}_{y}^{-1}\\|^{2}\\widehat{\\mathbb{E}}_{k}\\right\\|\\left[\\begin{array}{c c}{\\Delta_{y}\\mathbf{\\bar{u}}_{y}^{\\top}\\left[\\mathbf{v}^{k}-\\widehat{\\mathbb{E}}_{k}|\\mathbf{v}^{k}\\right]}\\\\ {0}\\\\ {\\epsilon_{2}\\vdots}\\\\ {+\\frac{1\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Taking expectations on both sides of (60), and plugging (61), (62), (63), (64) into it, we obtain: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\left[\\|\\hat{\\mathbf{e}}_{y}^{k+1}\\|^{2}\\right]}\\\\ &{\\le2\\frac{\\beta^{2}\\|\\mathbf{O}_{y}^{-1}\\|^{2}}{1-\\|\\mathbf{I}_{y}\\|}\\|\\mathbf{A}_{y b}^{-1}\\|^{2}\\|\\mathbf{A}_{y a}\\|^{2}L_{g,1}^{2}\\mathbb{E}\\left[\\|\\bar{\\mathbf{x}}^{k+1}-\\bar{\\mathbf{x}}^{k}\\|^{2}+\\|\\bar{\\mathbf{y}}^{k+1}-\\bar{\\mathbf{y}}^{k}\\|^{2}\\right]+\\|\\mathbf{r}_{y}\\|\\mathbb{E}\\|\\hat{\\mathbf{e}}_{y}^{k}\\|^{2}}\\\\ &{\\quad+\\,2\\frac{\\beta^{2}\\|\\mathbf{O}_{y}^{-1}\\|^{2}}{1-\\|\\mathbf{I}_{y}\\|}\\|\\|\\mathbf{A}_{y a}\\|^{2}L_{g,1}^{2}\\mathbb{E}\\left[\\|\\mathbf{x}^{k}-\\bar{\\mathbf{x}}^{k}\\|^{2}+\\|\\mathbf{y}^{k}-\\bar{\\mathbf{y}}^{k}\\|^{2}\\right]+2\\beta^{2}\\|\\mathbf{O}_{y}^{-1}\\|^{2}\\|\\mathbf{A}_{y a}\\|^{2}n\\sigma_{g,1}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Taking summation over $k$ and using $\\|\\mathbf{x}^{k}-\\bar{\\mathbf{x}}^{k}\\|^{2}\\leq\\|\\mathbf{O}_{x}\\|^{2}\\|\\hat{\\mathbf{e}}_{x}^{k}\\|^{2},\\|\\mathbf{y}^{k}-\\bar{\\mathbf{y}}^{k}\\|^{2}\\leq\\|\\mathbf{O}_{y}\\|^{2}\\|\\hat{\\mathbf{e}}_{y}^{k}\\|^{2}$ we get: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad(1-\\|\\nabla_{y}\\|)\\displaystyle\\sum_{k=0}^{K}\\mathbb{E}\\big[\\|\\hat{\\mathbf{e}}_{y}^{k}\\|^{2}\\big]}\\\\ &{\\leq\\!\\sum_{k=0}^{K}\\frac{\\beta^{2}\\|\\theta\\|_{\\mathcal{H}}^{2}\\|^{2}}{1-\\|\\Gamma_{y}\\|}\\big\\|\\mathrm{A}_{y_{\\mathcal{H}}}^{-1}\\big\\|^{2}\\big\\|\\mathrm{A}_{y_{\\mathcal{H}}}\\big\\|^{2}\\boldsymbol{L}_{\\mathcal{E}_{y}^{1}}^{2}\\mathbb{E}\\big[\\|\\hat{\\mathbf{x}}^{k+1}-\\hat{\\mathbf{x}}^{k}\\|^{2}+\\|\\hat{\\mathbf{y}}^{k+1}-\\hat{\\mathbf{y}}^{k}\\|^{2}\\big]}\\\\ &{\\quad+\\mathbb{E}\\big[\\|\\hat{\\mathbf{e}}_{y}^{0}\\|^{2}-\\mathbb{E}\\|\\hat{\\mathbf{e}}_{y}^{k+1}\\|^{2}+2\\sum_{k=0}^{K}\\frac{\\beta^{2}\\|\\mathbf{\\hat{y}}^{0}-\\|^{2}\\|^{2}}{1-\\|\\Gamma_{y}\\|}\\|\\|\\mathrm{A}_{y_{\\mathcal{H}}}\\|^{2}\\boldsymbol{L}_{\\mathcal{E}_{y}^{1}}^{2}\\mathbb{E}\\big[\\|\\mathbf{0}_{x}\\|^{2}\\|\\hat{\\mathbf{e}}_{x}^{k}\\|^{2}+\\|\\mathbf{0}_{y}\\|^{2}\\|\\hat{\\mathbf{e}}_{y}^{k}\\|^{2}\\big]}\\\\ &{\\quad+2(K+1)\\beta^{2}\\|\\mathbf{0}_{x}^{-1}\\|^{2}\\|\\mathrm{A}_{y_{\\mathcal{H}}}\\|^{2}\\boldsymbol{n}\\sigma_{y,1}^{2}}\\\\ &{\\leq\\!\\!\\!\\!\\sum_{k=0}^{K}\\!\\frac{\\beta^{2}\\|\\mathbf{0}_{y}^{-1}\\|^{2}}{1-\\|\\Gamma_{y}\\|}\\big\\|\\mathrm{A}_{y_{\\mathcal{H}}}^{-1}\\big\\|^{2}\\big\\|\\mathrm{A}_{y_{\\mathcal{H}}}\\big\\|^{2}\\boldsymbol{L}_{\\mathcal{E}_{y}^{1}}^{2}\\mathbb{E}\\big[\\|\\hat{\\mathbf{x}}^{k+1}-\\hat{\\mathbf{x \n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where the last inequality uses $\\beta^{2}\\leq\\frac{(1-\\|\\Gamma_{y}\\|)^{2}}{8L_{g,1}^{2}\\|\\mathbf{O}_{y}^{-1}\\|^{2}\\|\\mathbf{O}_{y}\\|^{2}\\|\\mathbf{A}_{y a}\\|^{2}}$ ", "page_idx": 36}, {"type": "text", "text": "It follows that ", "text_level": 1, "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k=0}^{K+1}\\mathbb{E}\\left[\\|\\hat{\\mathbf{e}}_{y}^{k}\\|^{2}\\right]\\leq3\\displaystyle\\sum_{k=0}^{K}\\frac{\\beta^{2}\\|\\mathbf{O}_{y}^{-1}\\|^{2}}{(1-\\|\\mathbf{r}_{y}\\|)^{2}}\\|\\mathbf{A}_{y b}^{-1}\\|^{2}\\|\\mathbf{A}_{y a}\\|^{2}L_{g,1}^{2}\\mathbb{E}\\left[\\|\\bar{\\mathbf{x}}^{k+1}-\\bar{\\mathbf{x}}^{k}\\|^{2}+\\|\\bar{\\mathbf{y}}^{k+1}-\\bar{\\mathbf{y}}^{k}\\|^{2}\\right]}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad+\\displaystyle\\frac{\\|\\mathbf{O}_{x}\\|^{2}}{3\\|\\mathbf{O}_{y}\\|^{2}}\\displaystyle\\sum_{k=0}^{K}\\mathbb{E}\\left[\\|\\hat{\\mathbf{e}}_{x}^{k}\\|^{2}\\right]+\\frac{3(K+1)\\beta^{2}\\|\\mathbf{O}_{y}^{-1}\\|^{2}\\|\\mathbf{A}_{y a}\\|^{2}}{1-\\|\\mathbf{r}_{y}\\|}n\\sigma_{g,1}^{2}+\\displaystyle\\frac{2\\mathbb{E}\\|\\hat{\\mathbf{e}}_{y}^{0}\\|^{2}}{1-\\|\\mathbf{r}_{y}\\|}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Lemma 13 (Consensus error of $z$ ). Suppose that Assumptions 1- 4 hold, and $\\gamma$ satisfies ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\frac{6\\gamma^{2}\\|\\mathbf{O}_{z}^{-1}\\|^{2}\\|\\mathbf{O}_{z}\\|^{2}\\|\\mathbf{A}_{z a}\\|^{2}}{1-\\|\\mathbf{I}_{z}\\|}\\cdot(2L^{2}+2(1-\\|\\mathbf{I}_{z}\\|)\\sigma_{g,2}^{2})\\leq\\frac{1-\\|\\mathbf{r}_{z}\\|}{4}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "We have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\underset{k=0}{\\overset{k+1}{\\sum}}\\left[(\\|\\tilde{e}_{s,1}^{k}\\|^{2})^{2}\\right.}\\\\ &{\\left.\\underset{k=0}{\\overset{k+1}{\\sum}}(L_{g,1}^{0}\\!+\\!1\\!-\\!\\left\\Vert\\mathbf{F}_{s,1}\\right\\Vert_{H_{s}^{0}}^{2})\\!\\!\\right]\\!|\\mathbf{D}_{s,1}^{-1}|\\!\\!|^{2}|\\mathbf{\\Lambda}|_{H_{s}^{0}}\\!\\right]^{2}\\underset{k=0}{\\overset{K}{\\sum}}\\mathbb{E}\\left[|\\vert\\hat{e}^{k}-z_{s,1}^{k}\\vert|^{2}\\right]+\\frac{2\\mathbb{E}\\left\\Vert\\vert\\vert\\tilde{e}_{s,1}^{0}\\vert\\right\\vert_{H}^{2}}{1-\\left\\Vert\\mathbf{F}_{s,1}\\right\\Vert_{H_{s}^{0}}^{2}}}\\\\ &{\\quad\\left.+\\frac{8\\gamma^{2}\\left\\Vert\\vert\\mathbf{D}_{s,1}^{-1}\\vert\\right\\Vert_{H_{s}^{0}}^{2}\\right\\Vert\\left\\vert\\mathbf{\\Lambda}_{H_{s}^{0}}\\right\\Vert^{2}}{(1-\\left\\Vert\\mathbf{F}_{s,1}\\right\\Vert_{H}^{2})^{2}}\\underset{k=0}{\\overset{K}{\\sum}}\\mathbb{E}\\left[\\left(L_{f,1}^{2}+L_{g,2}^{2}\\frac{L_{f,0}^{2}}{L_{g,2}^{0}}+L_{g,1}^{2}L_{s,*}^{2}\\right)\\left\\Vert\\mathbb{S}^{k+1}-\\mathbb{K}_{1}^{k}\\right\\Vert_{H_{s}^{0}}^{2}\\right]}\\\\ &{\\quad+\\frac{8\\gamma^{2}\\left\\Vert\\mathbf{D}_{s,1}^{-1}\\left\\Vert\\vert\\mathbf{D}_{s,1}^{-1}\\right\\Vert_{H_{s}^{0}}^{2}\\right\\Vert\\left\\vert\\mathbf{\\Lambda}_{H_{s}^{0}}\\right\\Vert^{2}}{(1-\\left\\Vert\\mathbf{F}_{s,1}\\right\\Vert_{H_{s}^{0}}^{2})^{2}}\\underset{k=0}{\\overset{K}{\\sum}}\\mathbb{E}\\left[\\left(L_{f,1}^{2}+L_{g,2}^{2}\\frac{L_{f,0}^{2}}{L_{g,2}^{\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Proof. Firstly, Eq. (34) implies that: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\mathbf{e}}_{z}^{k+1}=\\!\\mathbf{P}_{z}\\hat{\\mathbf{e}}_{z}^{k}-\\gamma\\mathbf{O}_{z}^{-1}\\left[\\begin{array}{c}{\\!\\!\\!\\Delta_{z a}\\hat{\\mathbf{U}}_{z}^{\\top}\\left[\\widetilde{\\mathbb{E}}_{k}[\\mathbf{p}^{k}]-\\mathbf{p}^{k}(\\bar{\\mathbf{x}}^{k},\\bar{\\mathbf{y}}^{k+1})\\right]\\!\\!\\right]}\\\\ {\\!\\!\\!\\Delta_{z b}^{-1}\\Lambda_{z a}\\hat{\\mathbf{U}}_{z}^{\\top}\\left[\\mathbf{p}^{k+1}(\\bar{\\mathbf{x}}^{k+1},\\bar{\\mathbf{y}}^{k+2})-\\mathbf{p}^{k}(\\bar{\\mathbf{x}}^{k},\\bar{\\mathbf{y}}^{k+1})\\right]\\!\\!\\right]}\\\\ &{\\qquad+\\gamma\\mathbf{O}_{z}^{-1}\\left[\\begin{array}{c}{\\!\\!\\!\\Delta_{z a}\\hat{\\mathbf{U}}_{z}^{\\top}\\left[\\widetilde{\\mathbb{E}}_{k}[\\mathbf{p}^{k}]-\\mathbf{p}^{k}\\right]\\!\\!\\right].}\\\\ {\\!\\!\\!\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\mathrm{~0~}}\\end{array}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Then using Cauchy Schwartz inequality, we get ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\hat{\\mathbf{e}}_{z}^{k+1}\\|^{2}}\\\\ &{\\leq\\left\\|\\mathbf{r}_{z}\\hat{\\mathbf{e}}_{z}^{k}-\\gamma\\mathbf{O}_{z}^{-1}\\left[\\begin{array}{c}{\\mathbf{A}_{z a}\\hat{\\mathbf{U}}_{z}^{\\top}\\left[\\widetilde{\\mathbb{E}}_{k}[\\mathbf{p}^{k}]-\\mathbf{p}^{k}(\\bar{\\mathbf{x}}^{k},\\bar{\\mathbf{y}}^{k+1})\\right]}\\\\ {\\mathbf{A}_{z b}^{-1}\\mathbf{A}_{z a}\\hat{\\mathbf{U}}_{z}^{\\top}\\left[\\mathbf{p}^{k+1}(\\bar{\\mathbf{x}}^{k+1},\\bar{\\mathbf{y}}^{k+2})-\\mathbf{p}^{k}(\\bar{\\mathbf{x}}^{k},\\bar{\\mathbf{y}}^{k+1})\\right]}\\end{array}\\right]\\right\\|^{2}}\\\\ &{\\quad+\\gamma^{2}\\left\\|\\mathbf{O}_{z}^{-1}\\left[\\begin{array}{c}{\\mathbf{A}_{z a}\\hat{\\mathbf{U}}_{z}^{\\top}\\left[\\widetilde{\\mathbb{E}}_{k}[\\mathbf{p}^{k}]-\\mathbf{p}^{k}\\right]}\\\\ {0}\\end{array}\\right]\\right\\|^{2}-2\\left\\langle\\mathbf{r}_{z}\\hat{\\mathbf{e}}_{z}^{k},\\gamma\\mathbf{O}_{z}^{-1}\\left[\\begin{array}{c}{\\mathbf{A}_{z a}\\hat{\\mathbf{U}}_{z}^{\\top}\\left[\\widetilde{\\mathbb{E}}_{k}[\\mathbf{p}^{k}]-\\mathbf{p}^{k}\\right]}\\end{array}\\right]\\right\\rangle}\\\\ &{\\quad+\\gamma^{2}\\left\\|\\mathbf{O}_{z}^{-1}\\left[\\begin{array}{c}{\\mathbf{A}_{z a}\\hat{\\mathbf{U}}_{z}^{\\top}\\left[\\widetilde{\\mathbb{E}}_{k}[\\mathbf{p}^{k}]-\\mathbf{p}^{k}(\\bar{\\mathbf{x}}^{k},\\bar{\\mathbf{y}}^{k+1})\\right]}\\\\ {\\mathbf{A}_{z b}^{-1}\\mathbf{A}_{z a}\\hat{\\mathbf{U}}_{z}^{\\top}\\left[\\mathbf{p}^{k+1}(\\bar{\\mathbf{x}}^{k+1},\\bar{\\mathbf{y}}^{k+2})-\\mathbf{p}^{k}(\\bar{\\mathbf{x}}^{k},\\bar{\\mathbf{y}}^{k+1})\\right]}\\end{array}\\right]\\right\\| \n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "To obtain the upper bound of the right-hand side of the above equation, we first estimate some individual terms in it as follows. Note that: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\widetilde{\\mathbb{E}}_{k}||\\widetilde{\\mathbb{E}}_{k}|[\\mathbf{p}^{k}]-\\mathbf{p}^{k}(\\widetilde{\\mathbf{x}}^{k},\\mathbf{\\tilde{y}}^{k+1})|^{2}}\\\\ &{=\\displaystyle\\sum_{i=1}^{n}\\widetilde{\\mathbb{E}}_{k}\\left||\\nabla_{22}^{2}g_{i}(x_{i}^{k},y_{i}^{k+1})z_{i}^{k}-\\nabla_{2}f_{i}(x_{i}^{k},y_{i}^{k+1})-\\left(\\nabla_{22}^{2}g_{i}(\\overline{{x}}^{k},\\bar{y}^{k+1})z_{k}^{\\star}-\\nabla_{2}f_{i}(\\bar{x}^{k},\\bar{y}^{k+1})\\right)|\\right|^{2}}\\\\ &{\\le3\\displaystyle\\sum_{i=1}^{n}\\widetilde{\\mathbb{E}}_{k}\\left||\\nabla_{22}^{2}g_{i}(x_{i}^{k},y_{i}^{k+1})(z_{i}^{k}-z_{k}^{k})|\\right|^{2}+3\\displaystyle\\sum_{i=1}^{n}\\widetilde{\\mathbb{E}}_{k}\\left||(\\nabla_{22}^{2}g_{i}(x_{i}^{k},y_{i}^{k+1})-\\nabla_{22}^{2}g_{i}(\\bar{x}^{k},\\bar{y}^{k+1}))z_{k}^{k}|\\right|}\\\\ &{\\quad+3\\displaystyle\\sum_{i=1}^{n}\\widetilde{\\mathbb{E}}_{k}\\left||\\nabla_{2}f_{i}(\\bar{x}^{k},\\bar{y}^{k+1})-\\nabla_{2}f_{i}(x_{i}^{k},y_{i}^{k+1})|\\right|^{2}}\\\\ &{\\le6L_{g,1}^{2}(\\|\\mathbf{z}^{k}-\\bar{\\mathbf{z}}^{k}\\|^{2}+\\|\\bar{\\mathbf{z}}^{k}-\\mathbf{z}_{k}^{k}\\|^{2})+3\\left(L_{g,2}^{2}\\frac{L_{f,0}^{2}}{\\mu_{g}^{2}}+L_{f,1}^{2}\\right)\\left(\\|\\mathbf{x}^{k}-\\bar{\\mathbf{x}}^{k}\\|^{2}+\\|\\mathbf{y}^{k+1}-\\bar{\\mathbf{y}}^{k+1}\\|^{2}\\right\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "and ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{\\tilde{R}}_{k}\\|{\\bf y}^{k+1}({\\tilde{\\mathbf{x}}}^{k+1},{\\tilde{\\mathbf{y}}}^{k+2})-{\\mathbf{P}}^{k}({\\tilde{\\mathbf{x}}}^{k},{\\tilde{\\mathbf{y}}}^{k+1})\\|^{2}}\\\\ &{=\\displaystyle\\sum_{i=1}^{n}\\mathbb{\\tilde{R}}_{k}\\|\\nabla_{22}^{2}g_{i}({\\tilde{x}}^{k+1},{\\tilde{y}}^{k+2})z_{\\star}^{k+1}-\\nabla_{2}f_{i}({\\tilde{x}}^{k+1},{\\tilde{y}}^{k+2})-\\nabla_{22}^{2}g_{i}({\\tilde{x}}^{k},{\\tilde{y}}^{k+1})z_{\\star}^{k}+\\nabla_{2}f_{i}({\\tilde{x}}^{k},{\\tilde{y}}^{k+1})}\\\\ &{\\le3\\displaystyle\\sum_{i=1}^{n}\\mathbb{\\tilde{R}}_{k}\\|(\\nabla_{22}^{2}g_{i}({\\tilde{x}}^{k+1},{\\tilde{y}}^{k+2})-\\nabla_{22}^{2}g_{i}({\\tilde{x}}^{k},{\\tilde{y}}^{k+1}))z_{\\star}^{k+1}\\|^{2}}\\\\ &{\\quad+3\\displaystyle\\sum_{i=1}^{n}\\mathbb{\\tilde{R}}_{k}\\|\\nabla_{22}^{2}g_{i}({\\tilde{x}}^{k},{\\tilde{y}}^{k+1})(z_{\\star}^{k+1}-z_{\\star}^{k})\\|^{2}+3\\displaystyle\\sum_{i=1}^{n}\\mathbb{\\tilde{R}}_{k}\\|\\nabla_{2}f_{i}({\\tilde{x}}^{k+1},{\\tilde{y}}^{k+2})-\\nabla_{2}f_{i}({\\tilde{x}}^{k},{\\tilde{y}}^{k+1})\\|}\\\\ &{\\le3\\mathbb{\\tilde{R}}_{k}\\left[\\left(L_{f,1}^{2}+L_{g,2}^{2}\\frac{L_{f,0}^{2}}{\\mu_{g}^{2}}\\right)(\\|{\\tilde{\\mathbf{x}}}^{k+1}-{\\tilde{\\mathbf{x}}}^{k}\\|^{2}+\\|{\\tilde{\\mathbf{y}}}^{k+2}-{\\tilde{\\mathbf{y \n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Then we present the bound of the right-hand side of (67). For the first term, we have the following evaluations: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\~\\widetilde{\\mathbb{E}}_{k}\\left[\\left\\|\\mathbf{Y}_{z}\\hat{\\mathbf{e}}_{z}^{k}-\\gamma\\mathbf{O}_{z}^{-1}\\left[\\begin{array}{c}{\\Lambda_{z a}\\hat{\\mathbf{U}}_{z}^{\\top}\\left[\\widetilde{\\mathbb{E}}_{k}[\\mathbf{p}^{k}]-\\mathbf{p}^{k}(\\overline{{\\mathbf{x}}}^{k},\\overline{{\\mathbf{y}}}^{k+1})\\right]}\\\\ {\\Lambda_{z b}^{-1}\\Lambda_{z a}\\hat{\\mathbf{U}}_{z}^{\\top}\\left[\\mathbf{p}^{k+1}(\\overline{{\\mathbf{x}}}^{k+1},\\overline{{\\mathbf{y}}}^{k+2})-\\mathbf{p}^{k}(\\overline{{\\mathbf{x}}}^{k},\\overline{{\\mathbf{y}}}^{k+1})\\right]}\\end{array}\\right]\\right\\|\\right]}\\\\ &{\\leq\\!\\|\\mathbf{T}_{z}\\|\\|\\!\\left\\|\\hat{\\mathbf{e}}_{z}^{k}\\right\\|^{2}+\\frac{\\gamma^{2}\\|\\mathbf{{\\mathbb{O}}}_{z}^{-1}\\|^{2}}{1-\\|\\mathbf{\\mathbb{I}}_{z}\\|}\\widetilde{\\mathbb{E}}_{k}\\left[\\left\\|\\left[\\begin{array}{c}{\\Lambda_{z a}\\hat{\\mathbf{U}}_{\\overline{{z}}}^{\\top}\\left[\\widetilde{\\mathbb{E}}_{k}[\\mathbf{p}^{k}]-\\mathbf{p}^{k}(\\overline{{\\mathbf{x}}}^{k},\\overline{{\\mathbf{y}}}^{k+1})\\right]}\\\\ {\\Lambda_{z b}+\\Lambda_{z a}\\hat{\\mathbf{U}}_{z}^{\\top}\\left[\\mathbf{p}^{k+1}(\\overline{{\\mathbf{x}}}^{k+1},\\overline{{\\mathbf{y}}}^{k+2})-\\mathbf{p}^{k}(\\overline{{\\mathbf{x}}}^{k},\\overline{{\\mathbf{y}}}^{k+1})\\right]}\\end{array}\\right]\\right\\|^{2}\\right]}\\\\ &{\\leq\\!\\|\\mathbf{T}_{z}\\|\\|\\!\\left\\|\\hat{\\mathbf{e}}_{z}^{k}\\right\\|^{2}+\\frac{\\gamma^{2}\\|\\mathbf{{\\mathbb{O}}}_{z}^{-1}\\|^{2}\\|\\mathbf{{\\mathbb{O}}}_{x z}\\|^{2}}{1-\\|\\mathbf{\\mathbb{I}}_{z}\\|}\\widetilde{\\mathbb{E}}_{k}\\left[\\|\\widetilde{\\mathbb{E}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where the first inequality use Jensen's inequality and the second inequality use $\\|\\hat{\\mathbf{U}}_{z}^{\\top}\\|\\leq1$ ", "page_idx": 38}, {"type": "text", "text": "For the second term, since $\\mathbf{z}^{k},\\mathbf{y}^{k+1}\\in\\mathcal{U}_{k}$ , we have: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\lvert\\widetilde{\\mathbb{E}}_{k}\\left\\|\\mathbf{O}_{z}^{-1}\\left[\\begin{array}{l}{\\Lambda_{z a}\\hat{\\mathbf{U}}_{z}^{\\top}\\left[\\widetilde{\\mathbb{E}}_{k}[\\mathbf{p}^{k}]-\\mathbf{p}^{k}\\right]}\\\\ {0}\\end{array}\\right]\\right\\|^{2}}\\\\ &{\\leq\\lVert\\mathbf{O}_{z}^{-1}\\rVert^{2}\\lVert\\Lambda_{z a}\\rVert^{2}\\widetilde{\\mathbb{E}}_{k}\\left[\\rVert\\widetilde{\\mathbb{E}}_{k}[\\mathbf{p}^{k}]-\\mathbf{p}^{k}\\right\\rVert^{2}\\right]}\\\\ &{\\leq2\\lVert\\mathbf{O}_{z}^{-1}\\rVert^{2}\\lVert\\Lambda_{z a}\\rVert^{2}(\\lVert\\mathbf{z}^{k}\\rVert^{2}\\sigma_{g,2}^{2}+n\\sigma_{f,1}^{2})}\\\\ &{\\leq6\\lVert\\mathbf{O}_{z}^{-1}\\rVert^{2}\\lVert\\Lambda_{z a}\\rVert^{2}\\left(\\left(\\lVert\\mathbf{z}^{k}-\\overline{{\\mathbf{z}}}^{k}\\rVert^{2}+\\lVert\\bar{\\mathbf{z}}^{k}-\\mathbf{z}_{\\star}^{k}\\rVert^{2}+n\\frac{L_{f,1}^{2}}{\\mu_{g}^{2}}\\right)\\sigma_{g,2}^{2}+n\\sigma_{f,1}^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "For the third term, we have: ", "page_idx": 39}, {"type": "equation", "text": "$$\n2\\widetilde{\\mathbb{E}}_{k}\\left\\langle\\mathbf{\\Gamma}_{\\mathbf{\\hat{z}}}\\hat{\\mathbf{e}}_{z}^{k},\\gamma\\mathbf{O}_{z}^{-1}\\left[\\begin{array}{c}{\\mathbf{\\Lambda}_{\\mathbf{\\hat{z}}a}\\hat{\\mathbf{U}}_{z}^{\\top}\\left[\\widetilde{\\mathbb{E}}_{k}[\\mathbf{p}^{k}]-\\mathbf{p}^{k}\\right]}\\\\ {0}\\end{array}\\right]\\right\\rangle=0,\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "since $\\hat{\\mathbf e}_{z}^{k}\\in\\mathcal{U}_{k}$ ", "page_idx": 39}, {"type": "text", "text": "Next, for the last two terms, we have: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{\\mathbb{E}}_{k}\\left[\\left\\|\\mathbf{O}_{z}^{-1}\\left[\\begin{array}{c}{\\Lambda_{z a}\\hat{\\mathbf{U}}_{z}^{\\top}\\left[\\tilde{\\mathbb{E}}_{k}[\\mathbf{p}^{k}]-\\mathbf{p}^{k}(\\bar{\\mathbf{x}}^{k},\\bar{\\mathbf{y}}^{k+1})\\right]}\\\\ {\\Lambda_{z b}^{-1}\\Lambda_{z a}\\hat{\\mathbf{U}}_{z}^{\\top}\\left[\\mathbf{p}^{k+1}(\\bar{\\mathbf{x}}^{k+1},\\bar{\\mathbf{y}}^{k+2})-\\mathbf{p}^{k}(\\bar{\\mathbf{x}}^{k},\\bar{\\mathbf{y}}^{k+1})\\right]}\\end{array}\\right]\\right\\|^{2}\\right]}\\\\ &{\\leq\\!\\|\\mathbf{O}_{z}^{-1}\\|^{2}\\|\\Lambda_{z a}\\|^{2}\\widetilde{\\mathbb{E}}_{k}\\left[\\|\\widetilde{\\mathbb{E}}_{k}[\\mathbf{p}^{k}]-\\mathbf{p}^{k}(\\bar{\\mathbf{x}}^{k},\\bar{\\mathbf{y}}^{k+1})\\|^{2}\\right]}\\\\ &{\\quad+\\|\\mathbf{O}_{z}^{-1}\\|^{2}\\|\\Lambda_{z a}\\|^{2}\\|\\Lambda_{z b}^{-1}\\|^{2}\\widetilde{\\mathbb{E}}_{k}\\left[\\|\\mathbf{p}^{k+1}(\\bar{\\mathbf{x}}^{k+1},\\bar{\\mathbf{y}}^{k+2})-\\mathbf{p}^{k}(\\bar{\\mathbf{x}}^{k},\\bar{\\mathbf{y}}^{k+1})\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "equation", "text": "$$\n\\widetilde{\\ensuremath{\\mathbb{E}}}_{k}\\left[\\left\\|\\mathbf{O}_{z}^{-1}\\left[\\begin{array}{c}{\\mathbf{A}_{z a}\\hat{\\mathbf{U}}_{z}^{\\top}\\left[\\widetilde{\\ensuremath{\\mathbb{E}}}_{k}[\\ensuremath{\\mathbf{p}}^{k}]-\\ensuremath{\\mathbf{p}}^{k}\\right]}\\\\ {0}\\end{array}\\right]\\right\\|^{2}\\right]\\leq\\|\\mathbf{O}_{z}^{-1}\\|^{2}\\|\\mathbf{A}_{z a}\\|^{2}\\widetilde{\\ensuremath{\\mathbb{E}}}_{k}\\left[\\|\\widetilde{\\ensuremath{\\mathbb{E}}}_{k}[\\ensuremath{\\mathbf{p}}^{k}]-\\ensuremath{\\mathbf{p}}^{k}\\|^{2}\\right].\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Taking the expectation $\\widetilde{\\mathbb{E}}_{k}$ on both sides of (67) and plugging (70), (71), (72), (73) and (74) into it, we obtain: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\lambda}\\left\\{\\|\\hat{e}^{k+\\frac{1}{2}}\\|^{2}\\right\\}}\\\\ &{\\leq\\mathbb{E}_{\\lambda}\\left\\{\\|\\hat{e}^{k+\\frac{3}{2}}\\|^{2}+\\frac{2^{\\lambda}}{\\|\\lambda-\\|}\\sum_{i=1}^{2\\delta}\\mathbb{E}_{\\lambda}\\Bigg\\|\\frac{\\hat{e}_{\\lambda}}{\\delta}\\left[\\|\\hat{\\mathbf{D}}_{i}\\|^{2}\\hat{e}^{k}-\\hat{\\mathbf{p}}^{k}(\\mathbf{x}^{\\lambda},\\mathbf{y}^{\\lambda+1})\\|^{2}\\right]}\\\\ &{\\quad+\\frac{2^{\\lambda}\\|\\hat{e}^{2}\\|^{2}\\|\\hat{\\mathbf{D}}_{i}\\|^{2}\\|\\hat{\\mathbf{D}}_{i}\\|^{2}\\|}{1-\\sum_{i}^{\\delta}\\|\\hat{\\mathbf{D}}_{i}\\|}\\frac{1}{\\delta}\\Bigg\\{\\|\\hat{\\mathbf{p}}^{k+1}(\\mathbf{x}^{\\lambda},\\mathbf{y}^{\\lambda+2})-\\hat{\\mathbf{p}}^{k}(\\mathbf{x}^{\\lambda},\\mathbf{y}^{\\lambda+1})\\|^{2}\\Bigg\\}}\\\\ &{\\quad+\\frac{2^{\\lambda}/2}{2}\\|\\hat{\\mathbf{D}}_{i}\\|^{2}\\|\\hat{\\mathbf{D}}_{i+}\\|^{2}\\mathbb{E}_{\\lambda}\\Bigg\\}\\\\ &{\\leq\\mathbb{E}\\|\\hat{\\mathbf{D}}_{i}\\|^{2}\\|\\hat{e}_{\\lambda}\\|^{2}+\\|\\mathbb{D}_{i}\\|^{2}\\mathbb{E}_{\\mathbb{I}_{\\lambda}}^{2}\\|\\hat{\\mathbf{D}}_{i}\\|^{2}\\Bigg\\}\\\\ &{\\leq\\mathbb{E}\\|\\hat{\\mathbf{D}}_{i}\\|\\hat{e}_{\\lambda}^{k}\\|^{2}+\\|\\mathbb{D}_{i}\\|^{2}\\|\\mathbb{D}_{i}\\|^{2}\\Bigg\\{\\frac{1}{\\delta}\\|\\hat{\\mathbf{A}}_{\\lambda}\\|^{2}\\left(\\frac{\\hat{\\mathbf{p}}^{\\lambda}}{\\delta}\\alpha_{2}^{2}+\\sigma_{\\hat{\\lambda}}^{2}\\right)}\\\\ &{\\quad+\\frac{\\|\\hat{\\mathbf{D}}_{2}\\|^{2}\\|\\hat{\\mathbf{D}}_{i}\\|^{2}\\|\\hat{\\mathbf{A}}_{\\lambda\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where the second inequality uses (36), (68), (69), and (71). ", "page_idx": 40}, {"type": "text", "text": "Thanks to ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\frac{6\\gamma^{2}\\|\\mathbf{O}_{z}^{-1}\\|^{2}\\|\\mathbf{O}_{z}\\|^{2}\\|\\mathbf{A}_{z a}\\|^{2}}{1-\\|\\mathbf{I}_{z}\\|}\\cdot(2L^{2}+2(1-\\|\\mathbf{I}_{z}\\|)\\sigma_{g,2}^{2})\\leq\\frac{1-\\|\\mathbf{r}_{z}\\|}{4},\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "we have: ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad{\\widetilde{\\mathbb{E}}}_{k}\\left[\\|\\hat{\\mathbf{e}}_{k}^{k+1}\\|^{2}\\right]}\\\\ &{\\leq\\frac{1+3\\|\\Gamma_{\\Sigma}\\|}{4}\\left\\|{\\hat{\\mathbf{e}}}_{\\widetilde{z}}^{k}\\|^{2}+\\frac{12\\gamma^{2}(L_{g,1}^{2}+(1-\\|\\Gamma_{\\Sigma}\\|)\\sigma_{g,2}^{2})\\|\\mathbf{{Q}}_{\\Sigma}^{-1}\\|^{2}\\|\\mathbf{A}_{z a}\\|^{2}}{1-\\|\\Gamma_{\\Sigma}\\|}\\widetilde{{\\mathbf{Q}}}_{k}\\left[\\|{\\pmb{\\hat{z}}}^{k}-{\\pmb{z}}_{\\star}^{k}\\|^{2}\\right]}\\\\ &{\\quad+\\,12n^{\\gamma}^{2}\\|{\\mathbf{Q}}_{\\Sigma}^{-1}\\|^{2}\\|\\mathbf{A}_{z a}\\|^{2}\\left(\\frac{L_{f,0}^{2}}{\\mu_{g,2}^{2}}\\sigma_{g,2}^{2}+\\sigma_{f,1}^{2}\\right)}\\\\ &{\\quad+\\,\\frac{1-\\|\\Gamma_{\\Sigma}\\|}{4\\|\\mathbf{Q}_{\\Sigma}\\|^{2}\\kappa^{2}(\\|\\mathbf{o}_{x}\\|^{2}\\|{\\pmb{\\hat{e}}}_{x}^{k}\\|^{2}+\\|\\mathbf{O}_{y}\\|^{2}\\|{\\pmb{\\hat{e}}}_{y}^{k+1}\\|^{2})}}\\\\ &{\\quad+\\,\\frac{6\\gamma^{2}\\|\\mathbf{Q}_{\\Sigma}^{-1}\\|^{2}\\|\\mathbf{A}_{x,\\widetilde{z}}^{-1}\\|^{2}\\|\\mathbf{A}_{z a}\\|^{2}}{1-\\|\\mathbf{P}_{\\Sigma}\\|}\\left(L_{f,1}^{2}+L_{g,2}^{2}\\frac{L_{f,0}^{2}}{\\mu_{g,2}^{2}}\\right)\\widetilde{\\mathbb{E}}_{k}\\left[\\|{\\pmb{\\hat{x}}}^{k+1}-{\\pmb{\\hat{x}}}^{k}\\|^{2}+\\|{\\pmb{\\hat{y}}}^{k+2}-{\\pmb{\\hat{y}}}^{k+1}\\|^{2}\\right]}\\\\ &{\\quad+\\,\\frac{6\\gamma^{2}\\|\\mathbf{Q}_{\\Sigma}^{-1}\\|^{2}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Taking summation and expectation on both sides, we get: ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{3}{4}(1-\\|\\mathbf{r}_{\\bot}\\|)\\frac{\\displaystyle\\sum_{k=0}^{K}\\mathbb{E}\\left[\\|e_{z}^{k}\\|^{2}\\right]}{\\displaystyle\\sum_{k=0}^{K}\\mathbb{E}\\left[\\|e_{z}^{k+1}\\|^{2}\\right]}}\\\\ &{\\quad+\\frac{12\\gamma^{2}(1\\delta_{1}^{-2})}{2}\\frac{\\displaystyle(1\\!-\\!\\frac{\\gamma}{\\delta_{1}})\\|e_{z}^{\\frac{2}{3}}\\|[\\mathbf{{x}}_{+}^{-1}]\\|^{2}\\|\\mathbf{A}_{\\delta_{1}}\\|^{2}}{\\displaystyle1-\\int\\|\\mathbf{r}_{\\bot}\\|}\\sum_{k=0}^{K}\\mathbb{E}\\left[\\|\\mathbf{z}^{k}-\\mathbf{z}_{k}^{k}\\|^{2}\\right]}\\\\ &{\\quad+\\frac{1-\\|\\mathbf{r}_{\\bot}\\|}{4\\|\\mathbf{O}_{k}\\|^{2}}\\frac{\\displaystyle\\sum_{k=0}^{K}\\mathbb{E}\\left[\\|\\mathbf{z}_{k}\\|^{2}\\|\\mathbf{z}_{k}^{k}\\|^{2}\\|+\\|\\mathbf{O}_{k}\\|^{2}\\|\\mathbf{z}_{k}^{k+1}\\|^{2}\\right]}{\\displaystyle\\sum_{k=0}^{K}\\mathbb{E}\\left[\\|\\mathbf{z}_{k}\\|^{2}\\|\\mathbf{z}_{k}^{k}\\|^{2}\\|\\right]}}\\\\ &{\\quad+12\\alpha\\gamma^{2}\\|\\mathbf{O}_{k}^{-1}\\|\\mathbf{\\widehat{z}}_{k+1}\\|^{2}\\left(\\frac{L_{f}^{2}\\delta_{2}^{2}}{\\displaystyle\\sum_{k=0}^{K}\\mathbb{E}\\left[\\|\\mathbf{z}_{k}^{k}\\|^{2}\\|\\mathbf{z}_{k}^{2}\\|^{2}+\\|\\mathbf{z}_{k}\\|^{2}\\|\\right]}\\right)}\\\\ &{\\quad+\\frac{6\\gamma^{2}\\|\\mathbf{O}_{k}^{-1}\\|^{2}\\|\\mathbf{A}_{\\delta_{1}}\\|^{2}}{\\displaystyle1-\\int\\|\\mathbf{r}_{\\bot}\\|}\\sum_{k=0}^{K}\\mathbb{E}\\left[\\left(L_{f,k+1}^{2}+L_{g,k}^{2}\\frac{L_{f,k}^{2}}{\\mu_{k}^{2}}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Thus, ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\sum_{k=0}^{K+1}\\mathbb{E}\\left[\\|\\hat{\\mathbf{e}}_{z}^{k}\\|^{2}\\right]}\\\\ &{\\leq\\!\\!\\frac{16\\gamma^{2}\\left(L_{g,1}^{2}+(1-\\|\\mathbf{r}_{z}\\|)\\sigma_{g,2}^{2}\\right)\\|\\mathbf{O}_{z}^{-1}\\|^{2}\\|\\mathbf{A}_{z u}\\|^{2}}{(1-\\|\\mathbf{r}_{z}\\|)^{2}}\\sum_{k=0}^{K}\\mathbb{E}\\left[\\|\\bar{\\mathbf{z}}^{k}-\\mathbf{z}_{\\star}^{k}\\|^{2}\\right]+\\frac{2\\mathbb{E}\\|\\|\\hat{\\mathbf{e}}_{z}^{0}\\|^{2}\\right]}{1-\\|\\mathbf{r}_{z}\\|}}\\\\ &{\\quad+\\frac{8\\gamma^{2}\\|\\mathbf{0}_{z}^{-1}\\|^{2}\\|\\mathbf{A}_{z b}^{-1}\\|^{2}\\|\\mathbf{A}_{z u}\\|^{2}}{(1-\\|\\mathbf{r}_{z}\\|)^{2}}\\sum_{k=0}^{K}\\mathbb{E}\\left[\\left(L_{f,1}^{2}+L_{g,2}^{2}\\frac{L_{f,0}^{2}}{\\mu_{g}^{2}}+L_{g,1}^{2}L_{z\\star}^{2}\\right)\\|\\bar{\\mathbf{x}}^{k+1}-\\bar{\\mathbf{x}}^{k}\\|^{2}\\right]}\\\\ &{\\quad+\\frac{8\\gamma^{2}\\|\\mathbf{0}_{z}^{-1}\\|^{2}\\|\\mathbf{A}_{z b}^{-1}\\|^{2}\\|\\mathbf{A}_{z u}\\|^{2}}{(1-\\|\\mathbf{r}_{z}\\|)^{2}}\\sum_{k=0}^{K}\\mathbb{E}\\left[\\left(L_{f,1}^{2}+L_{g,2}^{2}\\frac{L_{f,0}^{2}}{\\mu_{g}^{2}}\\right)\\|\\bar{\\mathbf{y}}^{k+2}-\\bar{\\mathbf{y}}^{k+1}\\|^{2}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle+\\,16(K+1)n\\gamma^{2}\\frac{\\|{\\bf{O}}_{z}^{-1}\\|^{2}\\|\\mathbf{\\hat{A}}_{z a}\\|^{2}}{1-\\|{\\bf{I}}_{z}\\|}\\left(\\frac{L_{f,0}^{2}}{\\mu_{g}^{2}}\\sigma_{g,2}^{2}+\\sigma_{f,1}^{2}\\right)}}\\\\ {{\\displaystyle+\\,\\frac{\\kappa^{2}}{3\\|{\\bf{O}}_{z}\\|^{2}}\\sum_{k=0}^{K}\\mathbb{E}(\\|{\\bf{O}}_{x}\\|^{2}\\|\\hat{\\bf{e}}_{x}^{k}\\|^{2}+\\|{\\bf{O}}_{y}\\|^{2}\\|\\hat{\\bf{e}}_{y}^{k+1}\\|^{2})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Lemma 14 (Consensus error of $x$ ). Suppose that Assumptions 1- 4 and Lemmas 4, 5, and 7 hold. We have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k=0}^{K+1}\\mathbb{E}\\|\\hat{\\mathbf{e}}_{x}^{k}\\|^{2}}\\\\ &{\\leq\\displaystyle\\frac{\\mathbb{E}\\|\\hat{\\mathbf{e}}_{x}^{0}\\|^{2}}{1-\\|\\mathbf{r}_{x}\\|}+\\frac{2\\alpha^{2}\\|\\mathbf{0}_{x}^{-1}\\|^{2}\\|\\mathbf{A}_{x z}\\|^{2}}{(1-\\|\\mathbf{r}_{x}\\|)^{2}}\\left[\\frac{1-\\theta}{\\theta}\\left\\|\\tilde{\\nabla}\\Phi(\\mathbf{x}^{0})\\right\\|^{2}\\right]}\\\\ &{\\displaystyle~+\\frac{6n\\alpha^{2}\\theta(K+1)\\|\\mathbf{0}_{x}^{-1}\\|^{2}\\|\\mathbf{A}_{x z}\\|^{2}}{1-\\|\\mathbf{r}_{x}\\|}\\left(\\theta+\\frac{1-\\theta}{1-\\|\\mathbf{r}_{x}\\|}\\right)\\left(\\sigma_{f,1}^{2}+3\\sigma_{g,2}^{2}\\frac{L_{f,0}^{2}}{\\mu_{g}^{2}}\\right)}\\\\ &{\\displaystyle~+\\frac{\\alpha^{2}\\|\\mathbf{0}_{x}^{-1}\\|^{2}\\|\\mathbf{A}_{x z}\\|^{2}}{1-\\|\\mathbf{r}_{x}\\|}\\left(\\frac{80L^{2}}{1-\\|\\mathbf{r}_{x}\\|}+18\\theta\\left(\\theta+\\frac{1-\\theta}{1-\\|\\mathbf{r}_{x}\\|}\\right)\\sigma_{g,2}^{2}\\right)\\sum_{k=0}^{K}\\mathbb{E}\\left[\\Delta_{k}+n I_{k}\\right]}\\\\ &{\\displaystyle~+\\frac{2\\tilde{L}^{2}\\alpha^{2}\\|\\mathbf{0}_{x}^{-1}\\|^{2}\\|\\mathbf{A}_{x z}\\|^{2}}{(1-\\|\\mathbf{r}_{x}\\|)^{2}}\\left(\\|\\mathbf{A}_{x\\theta}^{-1}\\|^{2}+\\frac{2(1-\\theta)^{2}}{\\theta^{2}}\\right)\\sum_{k=0}^{K-1}\\mathbb{E}\\left[\\left\\|\\tilde{\\mathbf{x}}^{k+1}-\\tilde{\\mathbf{x}}^{k}\\right\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Proof. Firstly, the term $\\|\\hat{\\mathbf e}_{x}^{k+1}\\|^{2}$ can be deformed as ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\mathbf{c}_{h}^{k+1}\\right\\|^{2}}\\\\ &{=\\left\\|\\mathbf{r}_{x}\\dot{\\mathbf{e}}_{x}^{k}-\\alpha\\mathbf{O}_{x}^{-1}\\left[\\begin{array}{c}{\\Lambda_{x}\\boldsymbol{\\Omega}\\dot{\\mathbf{U}}_{x}^{\\top}\\left[\\hat{\\mathbf{r}}^{k+1}-\\tilde{\\nabla}\\Phi(\\hat{\\mathbf{x}}^{k})\\right]}\\\\ {\\Lambda_{x}^{-1}\\mathbf{A}_{x}\\boldsymbol{\\Omega}\\dot{\\mathbf{U}}_{x}^{\\top}\\left[\\tilde{\\nabla}\\Phi(\\hat{\\mathbf{x}}^{k+1})-\\tilde{\\nabla}\\Phi(\\hat{\\mathbf{x}}^{k})\\right]}\\end{array}\\right]\\right\\|^{2}}\\\\ &{=\\left\\|\\mathbf{r}_{x}\\dot{\\mathbf{e}}_{x}^{k}-\\alpha\\mathbf{O}_{x}^{-1}\\left[\\begin{array}{c}{\\Lambda_{x}\\boldsymbol{\\Omega}\\frac{\\hat{\\mathbf{U}}_{x}^{\\top}}{2}\\left[\\hat{\\mathbf{E}}_{k}[\\hat{\\mathbf{r}}^{k+1}]-\\tilde{\\nabla}\\Phi(\\hat{\\mathbf{x}}^{k})\\right]}\\\\ {\\Lambda_{x}^{-1}\\mathbf{A}_{x}\\boldsymbol{\\Omega}\\frac{\\hat{\\mathbf{U}}_{y}^{\\top}}{2}\\left[\\hat{\\nabla}\\Phi(\\hat{\\mathbf{x}}^{k+1})-\\tilde{\\nabla}\\Phi(\\hat{\\mathbf{x}}^{k})\\right]}\\end{array}\\right]\\right\\|^{2}}\\\\ &{\\quad+\\alpha^{2}\\mathbb{E}_{k}\\left[\\left\\|\\mathbf{O}_{x}^{-1}\\left[\\mathbf{A}_{x}\\boldsymbol{\\Omega}\\hat{\\mathbf{U}}_{x}^{\\top}\\left[\\mathbb{E}_{k}[\\hat{\\mathbf{e}}^{k+1}]-\\hat{\\mathbf{r}}^{k+1}\\right]\\right]\\right\\|^{2}\\right]}\\\\ &{\\quad-\\left.2\\left\\langle\\mathbf{r}_{x}\\dot{\\mathbf{e}}_{x}^{k},\\alpha\\mathbf{O}_{x}^{-1}\\left[\\begin{array}{c}{\\Lambda_{x}\\boldsymbol{\\Omega}\\hat{\\mathbf{U}}_{x}^{\\top}\\left[\\mathbb{E}_{k}[\\hat{\\mathbf{r}}^{k+1}]-\\mathbf{r}^{k+1}\\right]}\\end{array}\\right]\\right\\rangle}\\\\ &{\\quad+2\\alpha^{2}\\left\\langle\\mathbf{O}_{x}^{-1}\\left[\\begin{array}{c\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "due to Eq. (35). ", "page_idx": 41}, {"type": "text", "text": "Then, for the first term of the right-hand side of (76), we use Jensen's Inequality and get: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{k}\\left[\\left\\|\\Gamma\\mathbf{\\hat{e}}_{x}^{k}-\\alpha\\mathbf{O}_{x}^{-1}\\left[\\begin{array}{c}{\\Lambda_{x u}\\hat{\\mathbf{U}}_{x}^{\\top}\\left[\\mathbb{E}_{k}[\\mathbf{r}^{k+1}]-\\widetilde\\nabla\\Phi(\\bar{\\mathbf{x}}^{k})\\right]}\\\\ {\\Lambda_{x b}^{-1}\\mathbf{A}_{x a}\\hat{\\mathbf{U}}_{x}^{\\top}\\left[\\widetilde\\nabla\\Phi(\\bar{\\mathbf{x}}^{k+1})-\\widetilde\\nabla\\Phi(\\bar{\\mathbf{x}}^{k})\\right]}\\end{array}\\right]\\right\\|^{2}\\right]}\\\\ &{\\leq\\!\\|\\mathbf{r}_{x}\\|\\|\\hat{\\mathbf{e}}_{x}^{k}\\|^{2}+\\frac{\\alpha^{2}\\|\\mathbf{O}_{x}^{-1}\\|_{2}^{2}}{1-\\|\\mathbf{I}_{x}\\|}\\mathbb{E}_{k}\\left[\\left\\|\\left[\\begin{array}{c}{\\Lambda_{x u}\\hat{\\mathbf{U}}_{x}^{\\top}\\left[\\mathbb{E}_{k}[\\mathbf{r}^{k+1}]-\\widetilde\\nabla\\Phi(\\bar{\\mathbf{x}}^{k})\\right]}\\\\ {\\Lambda_{x b}^{-1}\\mathbf{A}_{x a}\\hat{\\mathbf{U}}_{x}^{\\top}\\left[\\widetilde\\nabla\\Phi(\\bar{\\mathbf{x}}^{k+1})-\\widetilde\\nabla\\Phi(\\bar{\\mathbf{x}}^{k})\\right]}\\end{array}\\right]\\right\\|^{2}\\right]}\\\\ &{\\leq\\!\\|\\mathbf{r}_{x}\\|\\|\\hat{\\mathbf{e}}_{x}^{k}\\|^{2}+\\frac{\\alpha^{2}\\|\\mathbf{O}_{x}^{-1}\\|_{2}^{2}\\|\\mathbf{A}_{x a}\\|^{2}}{1-\\|\\mathbf{r}_{x}\\|}\\mathbb{E}_{k}\\left[\\left\\|\\mathbb{E}_{k}[\\mathbf{r}^{k+1}]-\\widetilde\\nabla\\Phi(\\bar{\\mathbf{x}}^{k})\\right\\|^{2}\\right],}\\\\ &{\\quad+\\frac{\\alpha^{2}\\|\\mathbf{O}_{x}^{-1}\\|_{2}^{2}\\|\\mathbf{A}_{x a}\\|^{2}\\|\\mathbf{A}_{x}^{-1}\\|^{2}\\mathbb{E}_{k}\\left[\\left\\|\\widetilde\\nabla\\Phi(\\bar{\\mathbf{x}}^{k+1})-\\widetilde\\nabla\\Phi(\\bar{\\mathbf{x}}^{k})\\right\\|^{ \n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "For the second term in the right-hand side of (76), we have: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathbb{E}_{k}\\left\\|\\mathbf{O}_{x}^{-1}\\left[\\begin{array}{c}{\\mathbf{A}_{x a}\\hat{\\mathbf{U}}_{x}^{\\top}\\left[\\mathbb{E}_{k}[\\mathbf{r}^{k+1}]-\\mathbf{r}^{k+1}\\right]}\\\\ {0}\\end{array}\\right]\\right\\|^{2}\\leq\\!\\|\\mathbf{O}_{x}^{-1}\\|^{2}\\|\\mathbf{A}_{x a}\\|^{2}\\mathbb{E}_{k}\\|\\mathbb{E}_{k}[\\mathbf{r}^{k+1}]-\\mathbf{r}^{k+1}\\|^{2}}\\\\ &{}&{=\\!\\theta^{2}\\|\\mathbf{O}_{x}^{-1}\\|^{2}\\|\\mathbf{A}_{x a}\\|^{2}\\mathbb{E}_{k}\\|\\mathbb{E}_{k}[\\mathbf{u}^{k}]-\\mathbf{u}^{k}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Like (72), we have: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{k}\\left[\\left\\langle\\mathbf{\\Gamma}_{x}\\hat{\\mathbf{e}}_{x}^{k},\\alpha\\mathbf{O}_{x}^{-1}\\left[\\begin{array}{c}{\\mathbf{A}_{x a}\\hat{\\mathbf{U}}_{x}^{\\top}\\left[\\mathbb{E}_{k}[\\mathbf{r}^{k+1}]-\\mathbf{r}^{k+1}\\right]}\\\\ {0}\\end{array}\\right]\\right\\rangle\\right]=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Next, for the last term, we have: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2\\alpha^{2}\\mathbb{E}_{k}\\left\\langle\\mathbf{O}_{x}^{-1}\\left[\\begin{array}{c}{\\mathbf{A}_{x a}\\hat{\\mathbf{U}}_{x}^{\\top}\\left[\\mathbb{E}_{k}[\\mathbf{r}^{k+1}]-\\widetilde{\\nabla}\\Phi(\\bar{\\mathbf{x}}^{k})\\right]}\\\\ {\\mathbf{A}_{x b}^{-1}\\mathbf{A}_{x a}\\hat{\\mathbf{U}}_{x}^{\\top}\\left[\\widetilde{\\nabla}\\Phi(\\bar{\\mathbf{x}}^{k+1})-\\widetilde{\\nabla}\\Phi(\\bar{\\mathbf{x}}^{k})\\right]}\\end{array}\\right],\\mathbf{O}_{x}^{-1}\\left[\\begin{array}{c}{\\mathbf{A}_{x a}\\hat{\\mathbf{U}}_{x}^{\\top}\\left[\\mathbb{E}_{k}[\\mathbf{r}^{k+1}]-\\mathbf{r}^{k+1}\\right]}\\\\ {0}\\end{array}\\right]\\right\\rangle}\\\\ &{\\le\\!\\alpha^{2}\\|\\mathbf{O}_{x}^{-1}\\|^{2}\\|\\mathbf{A}_{x a}\\|^{2}\\mathbb{E}_{k}\\left[\\|\\mathbb{E}_{k}[\\mathbf{r}^{k+1}]-\\widetilde{\\nabla}\\Phi(\\bar{\\mathbf{x}}^{k})\\|^{2}+\\|\\mathbb{E}_{k}[\\mathbf{r}^{k+1}]-\\mathbf{r}^{k+1}\\|^{2}\\right]}\\\\ &{\\quad+\\alpha^{2}\\|\\mathbf{O}_{x}^{-1}\\|^{2}\\|\\mathbf{A}_{x a}\\|^{2}\\|\\mathbf{A}_{x b}^{-1}\\|^{2}\\mathbb{E}_{k}\\left[\\|\\widetilde{\\nabla}\\Phi(\\bar{\\mathbf{x}}^{k+1})-\\widetilde{\\nabla}\\Phi(\\bar{\\mathbf{x}}^{k})\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Taking the expectation on both sides of (76), and plugging (77), (78), (79), (80) into it, we obtain: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{k}\\|\\hat{\\mathbf{e}}_{x}^{k+1}\\|^{2}}\\\\ &{\\leq\\!\\|{\\mathbf{r}}_{x}\\|\\|\\hat{\\mathbf{e}}_{x}^{k}\\|^{2}+2\\alpha^{2}\\theta^{2}\\|{\\mathbf{0}}_{x}^{-1}\\|^{2}\\|{\\mathbf{A}}_{x a}\\|^{2}\\mathbb{E}_{k}\\|\\mathbb{E}_{k}[{\\mathbf{u}}^{k}]-{\\mathbf{u}}^{k}\\|^{2}}\\\\ &{\\quad+\\,\\frac{2\\alpha^{2}\\|{\\mathbf{0}}_{x}^{-1}\\|^{2}\\|{\\mathbf{A}}_{x a}\\|^{2}}{1-\\|{\\mathbf{r}}_{x}\\|}\\mathbb{E}_{k}\\left[\\|\\mathbb{E}_{k}[{\\mathbf{r}}^{k+1}]-\\tilde{\\nabla}\\Phi(\\bar{\\mathbf{x}}^{k})\\|^{2}+\\|{\\mathbf{\\Lambda}}_{x b}^{-1}\\|^{2}\\|\\tilde{\\nabla}\\Phi(\\bar{\\mathbf{x}}^{k+1})-\\tilde{\\nabla}\\Phi(\\bar{\\mathbf{x}}^{k})\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Taking expectation and summation on both sides, we obtain: ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad(1-\\frac{\\gamma}{2})\\sum_{k=0}^{\\infty}t_{k}^{3}\\mu_{k}^{2}\\mu_{k}^{2}}\\\\ &{\\leq\\lambda(k)\\mu_{k}^{2}-\\sum_{k=0}^{\\infty}t_{k}^{3}\\mu_{k}^{2}\\mu_{k}^{2}\\Bigg[\\frac{\\gamma}{1-\\gamma}\\Bigg(k\\frac{\\gamma}{2}\\Bigg(k\\frac{\\gamma}{2}\\Bigg)^{\\frac{3}{2}}t\\Delta_{k}t\\Bigg)^{\\frac{3}{2}}\\sum_{k=0}^{\\infty}t_{k}^{3}\\mu_{k}^{4}\\mu_{k}^{4}-\\lambda\\mu_{k}^{4}t_{k}^{2}}\\\\ &{\\quad+\\frac{\\gamma\\beta}{2}t_{k}^{2}\\frac{\\gamma}{2}(2\\lambda_{k}\\gamma\\mu_{k}^{2})\\Bigg(\\frac{\\gamma}{1-\\gamma}\\Bigg(k\\frac{\\gamma}{2}\\Bigg(k\\frac{\\gamma}{2}\\Bigg)^{\\frac{3}{2}}+2\\frac{\\gamma}{2}\\Bigg(k\\frac{\\gamma}{2}\\Bigg)^{\\frac{3}{2}}\\Bigg[\\mu_{k}\\Bigg(\\mu_{k}^{4}-\\frac{\\gamma}{2}\\Bigg(k\\frac{\\gamma}{2}\\Bigg)^{\\frac{3}{2}}\\Bigg)\\Bigg]\\Bigg)}\\\\ &{\\quad+\\frac{2\\gamma\\beta}{2}t_{k}^{2}\\frac{\\gamma}{2}t_{k}^{2}\\Bigg[\\frac{\\gamma}{1-\\gamma}\\Bigg(k\\frac{\\gamma}{2}\\Bigg)^{\\frac{3}{2}}t(\\mu_{k}-\\mu_{k}^{2})\\frac{t_{k}^{2}}{2}\\Bigg\\]\\Bigg[\\left(k\\frac{\\gamma}{\\mu_{k}}+k\\frac{\\gamma}{2}\\right)t\\Bigg]}\\\\ &{\\quad+\\Bigg[2\\gamma\\beta^{2}(\\mu_{k}^{2}+\\frac{\\gamma}{2})\\Bigg(\\gamma\\frac{t_{k}^{2}}{2}\\Bigg)\\Bigg]\\Bigg[2\\lambda_{k}\\gamma\\mu_{k}^{2}}\\\\ &{\\leq\\lambda\\Bigg]\\mu_{k}^{2}-\\sum_{k=0}^{\\infty}t_{k}^{3}\\Bigg[2\\gamma\\beta^{2}(\\mu_{k}^{4}-\\frac{\\gamma}{2})\\Bigg(\\frac{\\gamma}{1-\\gamma}\\Bigg(k\\frac{\\gamma}{2}\\Bigg)^{\\frac{3}{2}}t(\\mu_{\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where the first inequality uses L- Lipschitz continuity of V\u03a6 , Lemma 7, and the second inequality uses Lemma 4 , Lemma 5 and ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\mathbf{z}^{k+1}-\\bar{\\mathbf{z}}^{k+1}\\|^{2}\\leq\\Delta_{k},\\quad\\|\\bar{\\mathbf{z}}^{k+1}-\\mathbf{z}_{\\star}^{k+1}\\|^{2}\\leq n I_{k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Hence we get ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\sum_{k=0}^{K+1}\\mathbb{E}\\|\\hat{\\mathbf{e}}_{x}^{k}\\|^{2}}\\\\ &{\\leq\\displaystyle\\frac{\\mathbb{E}\\|\\hat{\\mathbf{e}}_{x}^{0}\\|^{2}}{1-\\|\\mathbf{r}_{x}\\|}+\\frac{2\\alpha^{2}\\|\\mathbf{0}_{x}^{-1}\\|^{2}\\|\\mathbf{A}_{x z}\\|^{2}}{(1-\\|\\mathbf{r}_{x}\\|)^{2}}\\left[\\frac{1-\\theta}{\\theta}\\left\\|\\tilde{\\nabla}\\Phi(\\mathbf{x}^{0})\\right\\|^{2}\\right]}\\\\ &{\\quad+\\frac{6n\\alpha^{2}\\theta(K+1)\\|\\mathbf{0}_{x}^{-1}\\|^{2}\\|\\mathbf{A}_{x z}\\|^{2}}{1-\\|\\mathbf{r}_{x}\\|}\\left(\\theta+\\frac{1-\\theta}{1-\\|\\mathbf{r}_{x}\\|}\\right)\\left(\\sigma_{f,1}^{2}+3\\sigma_{g,2}^{2}\\frac{L_{f,0}^{2}}{\\mu_{g}^{2}}\\right)}\\\\ &{\\quad+\\frac{\\alpha^{2}\\|\\mathbf{0}_{x}^{-1}\\|^{2}\\|\\mathbf{A}_{x z}\\|^{2}}{1-\\|\\mathbf{r}_{x}\\|}\\left(\\frac{80L^{2}}{1-\\|\\mathbf{r}_{x}\\|}+18\\theta\\left(\\theta+\\frac{1-\\theta}{1-\\|\\mathbf{r}_{x}\\|}\\right)\\sigma_{g,2}^{2}\\right)\\displaystyle\\sum_{k=0}^{K}\\mathbb{E}\\left[\\Delta_{k}+n I_{k}\\right]}\\\\ &{\\quad+\\frac{2\\tilde{L}^{2}\\alpha^{2}\\|\\mathbf{0}_{x}^{-1}\\|^{2}\\|\\mathbf{A}_{x z}\\|^{2}}{(1-\\|\\mathbf{r}_{x}\\|)^{2}}\\left(\\|\\mathbf{A}_{x\\theta}^{-1}\\|^{2}+\\frac{2(1-\\theta)^{2}}{\\theta^{2}}\\right)\\displaystyle\\sum_{k=0}^{K-1}\\mathbb{E}\\left[\\left\\|\\tilde{\\mathbf{x}}^{k+1}-\\tilde{\\mathbf{x}}^{k}\\right\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "The following lemma gather the consensus analysis of $x,y,z$ together: ", "page_idx": 43}, {"type": "text", "text": "Lemma 15. Take ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\footnotesize\\displaystyle\\frac{3\\kappa^{2}\\beta^{2}\\|\\mathbf{0}_{y}\\|^{2}\\|\\mathbf{0}_{y}^{-1}\\|^{2}}{(1-\\|\\mathbf{T}_{y}\\|)^{2}}\\|\\boldsymbol{\\Lambda}_{y b}^{-1}\\|^{2}\\|\\boldsymbol{\\Lambda}_{y a}\\|^{2}L_{g,1}^{2}+16\\gamma^{2}L^{2}\\left(2\\kappa^{2}+L_{z^{\\star}}^{2}\\right)\\frac{\\|\\mathbf{0}_{z}\\|^{2}\\|\\|\\mathbf{0}_{z}^{-1}\\|^{2}\\|\\boldsymbol{\\Lambda}_{z a}\\|^{2}\\|\\boldsymbol{\\Lambda}_{z b}^{-1}\\|^{2}}{(1-\\|\\mathbf{T}_{z}\\|)^{2}}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad+4\\kappa^{2}\\tilde{L}^{2}\\left(1+\\frac{(1-\\theta)^{2}}{\\theta^{2}\\|\\Lambda_{x b}^{-1}\\|^{2}}\\right)\\alpha^{2}\\frac{\\|\\mathbf{O}_{x}\\|^{2}\\|\\mathbf{O}_{x}^{-1}\\|^{2}\\|\\Lambda_{x a}\\|^{2}\\|\\Lambda_{x b}^{-1}\\|^{2}}{(1-\\|\\Gamma_{x}\\|)^{2}},}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad(1-\\|\\mathbf{C}_{z}\\|)^{2}\\|\\mathbf{O}_{z}^{-1}\\|^{2}\\|\\Lambda_{z a}\\|^{2}\\|\\Lambda_{z b}^{-1}\\|^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad(1-\\|\\mathbf{C}_{y}\\|)^{2}\\|\\Lambda_{z b}^{-1}\\|^{2}\\|\\Lambda_{y b}^{-1}\\|^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad(1-\\|\\mathbf{C}_{z}\\|)^{2}\\|\\mathbf{C}_{z}^{-1}\\|^{2}\\|\\Lambda_{z a}\\|^{2}}\\|^{2}\\|\\Lambda_{z{z}}\\|^{2}\\|\\Lambda_{z}^{-1}\\|^{2}\\|^{2}\\|\\Lambda^{2}\\|\\Lambda_{z}\\|^{2}.}\\end{array}\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Suppose that Assumptions 1- 4 and Lemmas 12, 13, 14 hold, and $\\alpha,\\beta$ satisfy ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\alpha^{2}\\leq\\frac{(1-\\|\\Gamma_{x}\\|)^{2}}{24\\kappa^{2}\\|\\mathbf{O}_{x}^{-1}\\|^{2}\\|\\mathbf{O}_{x}\\|^{2}\\|\\Lambda_{x a}\\|^{2}\\,\\Big[80L^{2}+18\\theta(1-\\|\\Gamma_{x}\\|)\\,\\Big(\\theta+\\frac{1-\\theta}{1-\\|\\Gamma_{x}\\|}\\Big)\\,\\sigma_{g,2}^{2}\\Big]},}\\\\ &{\\eta_{2}\\beta^{2}\\leq\\frac{1}{1248L_{g,1}^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "We have: ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{4}\\underset{\\mathrm{i}=0}{\\overset{\\because}{\\sum}}[\\mathfrak{A}_{\\Delta}]}\\\\ &{\\leq(\\mathfrak{m}+\\delta\\alpha^{2}L_{\\gamma+1}^{2})\\simeq\\frac{K}{2}\\mathbf{1}[\\mathfrak{p}^{+1}+1]^{2}+\\frac{32L_{\\gamma}^{2}\\eta_{0}\\Delta}{\\mu_{0}}\\Vert\\nabla^{2}-\\gamma^{*}(\\theta^{*})\\Vert^{2}+3(K+1)\\eta_{0}\\partial_{\\sigma_{\\Delta}^{2}}^{2}}\\\\ &{\\quad+\\frac{\\sigma^{2}[\\mathfrak{p}^{-1}]^{2}\\left(\\Omega_{0}^{1}-1\\right)^{2}\\alpha}{1-[\\Gamma_{1}]}\\Vert\\mathfrak{p}\\rVert_{L^{2}}^{2}\\left(\\frac{9K\\left(2\\mathrm{i}^{2}-1\\right)}{1-[\\Gamma_{1}]}+18\\mathfrak{o}\\left(\\theta+\\frac{1-\\theta}{1-1}\\right)\\right)\\frac{\\sigma^{2}}{1-\\mu_{0}}\\sum_{i=1}^{\\infty}\\Vert\\mathfrak{p}\\rVert_{L_{i}|}}\\\\ &{\\quad+\\frac{\\left(\\Omega_{0}^{1}\\right)^{2}\\left(\\Omega_{0}^{1}-1\\right)^{2}[\\mathfrak{p}]_{i}\\left(\\frac{1-\\theta}{1-1}\\right)^{2}}{1-[\\Gamma_{1}]}\\frac{1}{|\\mathrm{van}|^{2}}(\\frac{1-\\theta}{1-1}+(1-\\Gamma_{1})\\eta_{0})\\frac{\\sigma^{2}}{1-\\mu_{0}}\\sum_{i=1}^{\\infty}\\Vert\\mathfrak{p}\\rVert_{L_{i}|}}\\\\ &{\\quad+\\frac{32\\sigma^{2}[\\left(K+1\\right)(0-1)^{2}(\\Omega_{0}^{1}-1)^{2}]\\left(\\mathfrak{p}\\right)}{1-[\\Gamma_{1}]}\\frac{2\\sigma^{2}[\\left(\\Omega_{0}^{1}-1\\right)^{2}(\\Omega_{0}^{1}-2\\mathrm{i}^{2}-1)]-1}{(\\Gamma_{1})^{2}}\\frac{2\\sigma^{2}[\\mathfrak{p}_{i}]_{i}\\left(\\frac{1-\\theta}{1-1}\\right)}{|\\mathrm{van}|^{2}}+\\frac{2010\\left(\\frac \n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Proof. Adding (59), (66) and (75) together, we get: ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\kappa^{2}\\|\\mathbf{g}_{t}\\|_{L^{\\frac{1}{\\alpha}}=\\frac{\\alpha}{2}}^{\\frac{\\alpha}{2}}\\mathbb{E}\\|(\\mathbf{g}_{t}^{*}\\|_{L^{\\frac{3}{\\alpha}}}^{2}+\\kappa^{2}\\|\\mathbf{g}_{t}\\|_{L^{\\frac{3}{\\alpha}}}^{2\\frac{\\alpha}{2}}\\mathbb{E}\\|\\mathbf{g}_{t}\\|_{L^{\\frac{3}{\\alpha}}}^{2}]+\\|\\mathbf{g}_{t}\\|_{L^{\\frac{3}{\\alpha}}}^{\\frac{\\alpha+1}{2}}\\mathbb{E}\\|\\mathbf{g}_{t}\\|_{L^{\\frac{3}{\\alpha}}}^{2}}\\\\ &{\\leq\\lambda^{2}\\kappa^{2}\\frac{\\sqrt{\\beta}}{16}\\frac{\\beta^{2}\\|\\mathbf{g}_{t}\\|_{L^{\\frac{3}{\\alpha}}}^{2}\\|^{2}}{(1-\\mathbb{E}\\|\\mathbf{g}_{t}\\|_{L^{\\frac{3}{\\alpha}}}^{2})^{\\frac{3}{2}}}\\|\\mathbf{g}_{t}\\|_{L^{\\frac{3}{\\alpha}}}^{2}\\mathbb{E}\\|\\mathbf{g}_{t}\\|_{L^{\\frac{3}{\\alpha}}}^{4\\alpha+1}-\\kappa^{\\frac{1}{\\alpha}}\\|^{2}+\\|\\mathbf{g}^{*}\\|_{L^{\\frac{3}{\\alpha}}}^{2}}\\\\ &{\\quad+\\frac{\\kappa^{2}\\|\\mathbf{g}_{t}\\|_{L^{\\frac{3}{\\alpha}}}^{2}}{1-\\mathbb{E}\\|\\mathbf{g}_{t}\\|_{L^{\\frac{3}{\\alpha}}}^{2}}\\mathbb{E}\\mathbb{E}\\left[\\|\\mathbf{g}_{t}\\|_{L^{\\frac3{\\alpha}}}^{2}+\\frac{3\\kappa^{2}(K+1)\\beta^{2}}{1-\\mathbb{E}\\|\\mathbf{g}_{t}\\|_{L^{\\frac{3}{\\alpha}}}^{2}}\\mathbb{E}\\|\\mathbf{g}_{t}\\|_{L^{\\frac{3}{\\alpha}}}^{2}\\mathrm{Le}_{\\frac{\\alpha}{2}}\\!+\\frac{2\\kappa^{2}\\|\\mathbf{g}_{t}\\|_{L^{\\frac{3}{\\alpha}}}^{2}\\mathbb{E}\\|\\mathbf{g}_{t}\\|_{L^{\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "$\\begin{array}{r l}&{\\quad+R(K+1)=\\frac{1}{\\beta}\\int_{0}^{\\infty}\\lvert\\mathrm{d}\\phi_{i-1}^{\\prime}\\rvert\\frac{\\partial}{\\partial\\phi_{i-1}}\\rvert\\frac{\\partial^{2}}{\\partial\\phi_{i-1}^{\\prime}}\\left(\\frac{\\partial}{\\partial\\phi_{i-2}}\\right)^{-1}\\frac{\\partial^{2}}{\\partial\\phi_{i-1}^{\\prime}}\\left(\\frac{\\partial}{\\partial\\phi_{i-3}}\\right)^{2}}\\\\ &{\\quad+\\frac{c^{2}}{\\partial\\phi_{i-1}^{\\prime}}\\frac{\\partial^{2}}{\\partial\\phi_{i-2}^{\\prime}}\\frac{\\partial^{2}}{\\partial\\phi_{i-1}^{\\prime}}\\frac{\\partial^{2}}{\\partial\\phi_{i-1}^{\\prime}}\\frac{\\partial^{2}}{\\partial\\phi_{i-1}^{\\prime}}\\frac{\\partial^{2}}{\\partial\\phi_{i-2}^{\\prime}}\\left(\\frac{1}{\\partial\\phi_{i-3}}\\right)^{2}\\Bigg[-\\frac{\\partial}{\\partial\\phi_{i-1}}\\Bigg]\\frac{\\partial^{2}}{\\partial\\phi_{i-1}^{\\prime}}}\\\\ &{\\quad+\\frac{c^{2}}{\\partial\\phi_{i-1}^{\\prime}}\\frac{\\partial^{2}}{\\partial\\phi_{i-1}^{\\prime}}\\frac{\\partial^{2}}{\\partial\\phi_{i-2}^{\\prime}}\\frac{\\partial^{2}}{\\partial\\phi_{i-1}^{\\prime}}\\Bigg[\\frac{\\partial}{\\partial\\phi_{i-3}}\\Bigg]\\frac{\\partial^{2}}{\\partial\\phi_{i-1}^{\\prime}}\\Bigg[\\frac{\\partial}{\\partial\\phi_{i-3}}\\Bigg]\\frac{\\partial^{2}}{\\partial\\phi_{i-1}^{\\prime}}}\\\\ &{\\quad+\\frac{c^{2}\\partial^{2}}{\\partial\\phi_{i-1}^{\\prime}}\\frac{\\partial^{2}}{\\partial\\phi_{i-1}^{\\prime}}\\frac{\\partial^{2}}{\\partial\\phi_{i-1}^{\\prime}}\\frac{\\partial^{2}}{\\partial\\phi_{i-1}^{\\prime}}\\Bigg[\\frac{\\partial}{\\partial\\phi_{i-1}^{\\prime}}\\frac{\\partial^{2}}{\\partial\\phi_{i-2}^{\\prime}}+\\frac{1}{\\partial\\phi_{i-1}^{\\prime}}\\frac{\\partial^{2}}{\\partial\\phi_{i-2} $ $\\begin{array}{r l}&{\\le\\frac{3}{4}\\sum_{k=1}^{k+1}\\mathbb{E}\\left(x^{2}\\|\\mathbf{0}\\|_{2}\\|^{2}\\|x_{k}^{2}\\|^{2}+x^{2}\\|\\mathbf{0}\\|_{0}\\|_{2}^{2}\\|x_{k}^{2}\\|^{2}+|\\mathbf{0}\\|_{2}^{2}\\|\\mathbb{I}\\|_{L^{1}}^{2}|x_{1}^{2}\\|^{2}\\right)}\\\\ &{\\quad+(\\eta+4\\,s^{2}\\,L_{g}^{2},n)\\mathbb{E}\\sum_{k=1}^{k+1}\\mathbb{E}\\left(\\eta+\\frac{32L_{g}^{2}\\lambda_{g}^{2}}{\\eta_{k}n}\\|y_{k}^{2}\\exp\\left(\\eta+\\gamma\\right)\\|^{2}+3(K+1)\\lambda_{g}^{2}\\sigma_{\\sigma_{k}}^{2}\\right)}\\\\ &{\\quad+\\frac{\\,k^{2}}{12}\\big(\\|\\mathbf{0}\\|_{2}\\|^{2}\\|y_{1}\\|^{2}\\|x_{0}\\|_{2}^{2}\\big)\\mathbb{E}\\bigg(\\frac{8M_{H}^{2}}{1-\\|\\mathbf{0}\\|_{2}^{2}}\\bigg)+8\\eta^{2}\\bigg(\\frac{1-R}{1-\\|\\mathbf{0}\\|_{2}}\\bigg)\\sigma_{\\sigma_{k}}^{2}\\bigg)\\sum_{k=1}^{K}\\mathbb{E}\\eta_{k+1}}\\\\ &{\\quad+\\frac{10.17}{1-\\|\\mathbf{0}\\|_{2}^{2}\\|\\mathbf{0}\\|_{2}^{2}-1}\\mathbb{I}\\|_{L^{1}}^{2}\\|y_{1}^{2}\\|x_{0}^{2}\\|_{2}^{2}-\\frac{1}{1-\\|\\mathbf{0}\\|_{2}^{2}}\\mathbb{I}\\|_{L^{1}}^{2}\\|y_{1}^{2}\\|x_{0}\\|_{2}^{2}}\\\\ &{\\quad+\\frac{3^{2}L_{g}^{2}\\rho(K+1)(1-\\|\\mathbf{0}\\|_{2}^{2}\\|\\mathbf{0}\\|_{2}^{2}\\|y_{1}^{2}\\|^{2}\\|x_{0}\\|_{2}^{2}\\|y_{1}^{2}\\|^{2})\\mathbb{E}\\frac $ ", "page_idx": 45}, {"type": "text", "text": "where the second inequality uses (54) and ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\eta_{2}L_{g,1}^{2}\\beta^{2}\\cdot52+\\frac{\\kappa^{2}\\alpha^{2}\\|\\mathbf{O}_{x}^{-1}\\|^{2}\\|\\mathbf{O}_{x}\\|^{2}\\|\\mathbf{A}_{x a}\\|^{2}}{1-\\|\\mathbf{I}_{x}\\|}\\left(\\frac{80L^{2}}{1-\\|\\mathbf{I}_{x}\\|}+18\\theta\\left(\\theta+\\frac{1-\\theta}{1-\\|\\mathbf{I}_{x}\\|}\\right)\\sigma_{g,2}^{2}\\right)\\leq\\frac{1}{12}.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Hence: ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{4}\\displaystyle\\sum_{k=0}^{K}\\mathbb{E}[\\Delta_{k}]}\\\\ &{\\leq(\\eta_{1}+48\\kappa^{2}L_{y}\\eta_{2})a^{2}\\sum_{k=0}^{K}\\mathbb{E}[\\|\\mathbf{r}^{k+1}\\|^{2}+\\frac{32L_{y,1}^{2}\\eta_{3}\\beta}{\\mu_{2}}\\|\\nabla^{0}-\\mathbf{y}^{*}(\\bar{\\boldsymbol{x}}^{0})\\|^{2}+3(K+1)\\eta_{2}\\beta^{2}\\sigma_{\\theta,1}^{2}}\\\\ &{\\quad+\\frac{\\kappa^{2}\\|\\mathbf{0}_{2}\\|^{2}\\|\\mathbf{0}_{2}\\|^{2}\\|\\mathbf{1}_{\\infty}\\|^{2}\\alpha^{2}}{1-\\|\\mathbf{r}\\|_{\\infty}^{2}}\\Big(\\frac{80L^{2}}{1-\\|\\mathbf{r}\\|_{\\infty}}\\Big)+18\\theta\\left(\\theta+\\frac{1-\\theta}{1-\\|\\mathbf{r}_{x}\\|}\\right)\\sigma_{\\theta,2}^{2}\\Big)\\sum_{k=0}^{K}\\mathbb{E}[n I_{k}]}\\\\ &{\\quad+\\frac{\\|\\mathbf{0}_{4}\\|^{2}\\|\\mathbf{0}_{2}\\|^{2}\\|\\mathbf{1}_{\\infty}\\|^{2}}{1-\\|\\mathbf{r}\\|_{\\infty}^{2}}\\cdot\\frac{\\|\\mathbf{0}_{7}\\|^{2}\\left(L_{y,1}^{2}+\\left(1-\\|\\mathbf{r}_{x}\\|\\right)\\sigma_{\\theta,2}^{2}\\right)}{1-\\|\\mathbf{r}\\|_{\\infty}}\\sum_{k=-1}^{K}\\mathbb{E}[n I_{k}]}\\\\ &{\\quad+\\frac{3\\kappa^{2}\\beta^{2}(K+1)\\|\\mathbf{0}_{9}\\|^{2}\\|\\mathbf{0}_{9}\\|^{2}\\|\\mathbf{0}_{9}\\|^{2}\\|\\mathbf{1}_{\\infty}\\|^{2}}{1-\\|\\mathbf{r}\\|_{\\infty}^{2}}\\alpha_{9,1}^{2}+\\frac{2\\kappa^{2}\\|\\mathbf{0}_{2}\\|^{2}\\|\\mathbf{0}_{9}\\|^{2}\\|\\mathbf{0}_{9}\\|^{2}}{1-\\|\\mathbf{r}\\|_{\\infty}^{2}}+\\frac{2 \n$$", "text_format": "latex", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{+\\,16(K+1)n\\gamma^{2}\\frac{\\|\\mathbf{O}_{z}\\|^{2}\\|\\mathbf{O}_{z}^{-1}\\|^{2}\\|\\mathbf{A}_{z a}\\|^{2}}{1-\\|\\mathbf{P}_{z}\\|}\\left(\\frac{L_{f,0}^{2}}{\\mu_{g}^{2}}\\sigma_{g,2}^{2}+\\sigma_{f,1}^{2}\\right)}\\\\ &{+\\,24(K+1)n\\kappa^{2}\\alpha^{2}\\theta\\left(\\theta+\\frac{1-\\theta}{1-\\|\\mathbf{T}_{x}\\|}\\right)\\frac{\\|\\mathbf{O}_{x}\\|^{2}\\|\\mathbf{O}_{x}^{-1}\\|^{2}\\|\\mathbf{A}_{x a}\\|^{2}}{1-\\|\\mathbf{T}_{x}\\|}\\left(\\frac{L_{f,0}^{2}}{\\mu_{g}^{2}}\\sigma_{g,2}^{2}+\\sigma_{f,1}^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "C.1.8 Proof of the main theorem ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Before giving the final result of the convergence analysis, we present the following Lemma that combines the results in the analysis of $I_{k}$ and $\\Delta_{k}$ ", "page_idx": 46}, {"type": "text", "text": "Lemma 16. Suppose that Assumptions 1- 4 and Lemmas 6, 11, 15 hold. If $\\alpha,\\beta,\\gamma,\\theta$ satisfy ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\alpha^{2}\\leq\\frac{(1-\\|\\Gamma_{x}\\|)^{2}}{16\\|\\mathbf{0}_{x}\\|^{2}\\|\\mathbf{0}_{x}^{-1}\\|^{2}\\|\\mathbf{1}_{x a}\\|^{2}\\left[80L^{2}+18\\theta\\left(\\theta+\\frac{1-\\theta}{1-\\|\\Gamma_{x}\\|}\\right)\\left(1-\\|\\Gamma_{x}\\|\\right)\\sigma_{g,2}^{2}\\right]\\cdot2040\\kappa^{6}},}\\\\ &{\\beta^{2}\\eta_{2}\\leq\\frac{1}{1024L_{g,1}^{2}},}\\\\ &{\\gamma^{2}\\leq\\frac{(1-\\|\\Gamma_{z}\\|)^{2}}{256\\|\\mathbf{0}_{z}\\|^{2}\\|\\mathbf{0}_{z}^{-1}\\|^{2}\\|\\mathbf{1}_{z a}\\|^{2}\\left[L_{g,1}^{2}+\\left(1-\\|\\Gamma_{z}\\|\\right)\\sigma_{g,2}^{2}\\right]\\cdot2040\\kappa^{4}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "and ", "page_idx": 46}, {"type": "equation", "text": "$$\n40\\left[4(\\eta_{1}+48\\kappa^{2}L_{y^{\\star}}^{2}\\eta_{2})\\alpha^{2}+\\frac{1}{1020\\kappa^{4}}\\left(\\frac{9\\alpha^{2}L_{z^{\\star}}^{2}}{\\gamma^{2}\\mu_{g}^{2}}+\\frac{438\\kappa^{4}\\alpha^{2}}{\\beta^{2}\\mu_{g}^{2}}L_{y^{\\star}}^{2}\\right)\\right]\\left(L^{2}+\\frac{\\theta\\sigma_{g,2}^{2}}{n}\\right)\\le\\frac{1}{4080\\kappa^{4}},\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "then we have: ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\frac{\\dot{\\Phi}}{\\omega}\\mathbb{E}\\big[\\Delta_{k}+n k_{1}\\big]\\dot{\\varsigma}^{2}k^{2}\\left(\\eta+\\hbar^{2}E_{\\omega,\\epsilon}^{2}\\right)\\alpha^{2}\\left(\\frac{n(\\Phi(\\tilde{\\rho}_{2})-\\mathrm{i}\\omega\\cdot\\mathbf{I}\\mathbf{H}^{\\epsilon})}{\\alpha}+\\theta(K+1)(\\sigma_{2,1}^{2}+\\kappa^{2}\\sigma_{3,2}^{2})\\right)}\\\\ {+}&{\\left(\\frac{-2\\epsilon^{2}}{2\\hbar\\omega^{2}}+\\frac{n(\\Phi^{2}\\tilde{E}_{2})^{2}}{\\hbar\\omega^{2}}+\\frac{n(\\Phi^{2}\\tilde{\\rho}_{2})^{2}}{2\\hbar\\omega^{2}}\\right)\\left(\\frac{n(\\Phi(\\tilde{\\rho}_{2})-\\mathrm{i}\\omega k_{2}\\mathbf{I}^{\\epsilon})}{-2\\hbar\\omega^{2}}+\\theta(K+1)(\\sigma_{2,1}^{2}+\\kappa^{2}\\sigma_{3,2}^{2})\\right)}\\\\ {+}&{\\frac{\\dot{\\kappa}\\theta^{2}\\beta(K+1)(\\mathbf{I}\\mathbf{S}_{\\perp})^{2}\\vert\\mathbf{I}\\vert\\Phi(\\mathbf{I}\\cdot\\mathbf{I}\\mathbf{S}_{\\perp}^{\\epsilon})^{2}}{1-\\hbar\\left(\\mathbf{I}\\mathbf{S}_{\\perp}\\right)^{2}}\\frac{n(\\mathbf{I}\\mathbf{S}_{\\perp})^{2}}{\\hbar\\omega^{2}}w_{\\perp}^{2}+\\kappa^{2}\\eta\\beta^{2}(K+1)\\sigma_{2,1}^{2}}\\\\ &{+\\frac{\\dot{\\kappa}\\theta^{2}\\beta(\\mathbf{I}\\mathbf{S}_{\\perp})^{2}\\vert\\mathbf{I}\\vert\\Phi(\\mathbf{I}\\cdot\\mathbf{I}\\mathbf{S}_{\\perp}^{\\epsilon})^{2}}{\\hbar\\omega^{2}}+\\frac{\\epsilon^{2}}{2\\hbar\\omega^{2}}\\vert\\mathbf{I}\\vert\\frac{\\dot{\\mathbf{D}}(\\mathbf{I}\\cdot\\mathbf{I}\\mathbf{S}_{\\perp}^{\\epsilon})}{\\hbar\\omega}+\\frac{\\kappa^{2}}{2}\\vert\\mathbf{I}\\vert\\frac{\\dot{\\mathbf{D}}(\\mathbf{I}\\cdot\\mathbf{I}\\mathbf{S}_{\\perp}^{\\epsilon})}{\\hbar\\omega}^{2}}\\\\ {+}&{\\frac{\\dot{\\\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Proof. Combining (57) and (83), we obtain ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{n(n-1)}\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}_{\\beta}[\\sigma\\sigma_{i},\\log\\sigma_{i}^{2}]}\\\\ &{~~+\\alpha_{1}\\alpha_{2}\\sigma_{n(n-1)}\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}_{\\beta}[\\sigma\\sigma_{i},\\log\\sigma_{i}^{2}]}\\\\ &{~~+\\alpha_{2}\\sigma_{n(n-1)}\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}_{\\beta}[\\sigma\\sigma_{i},\\log\\sigma_{i}^{2}]}\\\\ &{~~+\\alpha_{3}\\sigma_{n}^{2}[(n-1)\\sigma_{n}^{2},\\log\\sigma_{n}^{2}]+\\frac{1}{n(n-1)}\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}_{\\beta}[\\sigma\\sigma_{i},\\log\\sigma_{i}^{2}]}\\\\ &{~~+\\alpha_{4}\\sigma_{n}^{2}[(n-1)\\sigma_{n}^{2},\\log\\sigma_{n}^{2}]}\\\\ &{~~+\\frac{4}{n(n-1)}\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}_{\\beta}[\\sigma\\sigma_{i},\\log\\sigma_{i}^{2}]\\left(\\frac{\\log\\sigma_{n}^{2}}{n(n-1)}\\right)\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}_{\\beta}[\\sigma\\sigma_{i},\\log\\sigma_{n}^{2}]}\\\\ &{~~+\\frac{4}{n(n-1)}\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}_{\\beta}[\\sigma\\sigma_{i},\\log\\sigma_{n}^{2}]\\left(\\frac{\\log\\sigma_{n}^{2}}{n(n-1)}\\right)\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}_{\\beta}[\\sigma\\sigma_{i},\\log\\sigma_{n}^{2}]}\\\\ &{~~+\\frac{4}{n(n-1)}\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}_{\\beta}[\\sigma\\sigma_{i},\\log\\sigma_{n}^{2}][\\sigma\\sigma_{i},\\log\\sigma_{n}^{2}]\\left(\\frac{\\log\\sigma_{n}^{2}}{n(n-1)}\\right)^{2}}\\\\ &{~~+\\alpha_{3}\\sigma_{n}^{2}[(n-1)\\sigma_{n}^{2},\\log\\sigma_{n}^{2}]\\left(\\frac{\\log\\sigma_{n}^{2}}{n(n-1 \n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Subtracting the term ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{4\\frac{\\kappa^{2}\\|\\mathbf{O}_{x}^{-1}\\|^{2}\\|\\mathbf{O}_{x}\\|^{2}\\|\\mathbf{A}_{x a}\\|^{2}\\alpha^{2}}{1-\\|\\mathbf{Gamma}_{x}\\|}\\left(\\frac{80L^{2}}{1-\\|\\mathbf{Gamma}_{x}\\|}+18\\theta\\left(\\theta+\\frac{1-\\theta}{1-\\|\\mathbf{\\Gamma}_{x}\\|}\\right)\\sigma_{g,2}^{2}\\right)\\underset{k=0}{\\overset{K}{\\sum}}\\mathbb{E}[n I_{k}]}\\\\ &{+\\frac{4\\|\\mathbf{O}_{z}\\|^{2}\\|\\mathbf{O}_{z}^{-1}\\|^{2}\\|\\mathbf{A}_{z a}\\|^{2}}{1-\\|\\mathbf{\\Gamma}_{z}\\|}\\frac{16\\gamma^{2}(L_{g,1}^{2}+(1-\\|\\mathbf{\\Gamma}_{z}\\|)\\sigma_{g,2}^{2})}{1-\\|\\mathbf{\\Gamma}_{z}\\|}\\sum_{k=-1}^{K}\\mathbb{E}[n I_{k}]+\\frac{1}{2}\\underset{k=0}{\\overset{K}{\\sum}}\\mathbb{E}[\\Delta_{k}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "from both sides of (86) and using the restriction of $\\alpha,\\gamma$ in (84), we can get: ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\frac{1}{20408t^{4}}\\left(\\displaystyle\\sum_{k=0}^{K}\\mathbb{E}[\\Delta_{k}]+\\displaystyle\\sum_{k=1}^{K}\\mathbb{E}[n I_{k}]\\right)}\\\\ &{\\le4(\\eta+48\\kappa^{2}L_{g^{*}}^{2}\\eta)\\alpha_{g}^{2}\\displaystyle\\sum_{k=0}^{K}\\mathbb{E}[\\|\\mathbf{r}^{k+1}\\|^{2}+\\frac{128L_{g^{*}}^{2}\\eta/2}{\\mu_{g}}]\\|\\nabla^{0}-\\mathbf{y}^{*}(\\bar{x}^{0})\\|^{2}+12(K+1)\\eta_{2}\\beta^{2}\\sigma_{g,1}^{2}}\\\\ &{\\quad+\\frac{12\\kappa^{2}\\beta^{2}(K+1)\\|\\mathbf{D}_{g}\\|^{2}\\|(\\mathbb{I}_{\\mathcal{S}}^{0})\\|^{2}\\|\\left(\\Delta_{g}\\right)\\|^{2}}{1-\\|\\mathbf{D}_{g}\\|}\\alpha_{g,1}^{2}+\\frac{8\\kappa^{2}\\|\\mathbf{D}_{g}\\|^{2}\\mathbb{E}\\|\\nabla_{g}\\|}{1-\\|\\mathbf{D}_{g}\\|}+\\frac{8\\|\\mathbf{D}_{g}\\|^{2}\\mathbb{E}\\|\\nabla_{g}\\|^{2}}{1-\\|\\mathbf{D}_{g}\\|}}\\\\ &{\\quad+\\frac{4\\kappa^{2}\\|\\mathbf{D}_{g}\\|^{2}\\mathbb{E}\\|\\nabla_{g}\\|^{2}}{1-\\|\\mathbf{C}_{g}\\|}+\\frac{8\\kappa^{2}\\alpha^{2}\\|\\mathbf{D}_{g}\\|^{2}\\|\\mathbf{D}_{g}\\|^{2}\\|\\mathbf{A}_{\\infty}\\|^{2}}{(1-\\|\\mathbf{C}_{g}\\|)^{2}}\\left[\\frac{1-\\theta}{\\theta}\\left\\|\\tilde{\\nabla}\\Phi(\\mathbf{x}^{0})\\right\\|^{2}\\right]}\\\\ &{\\quad+64(K+1)n\\gamma^{2}\\frac{\\|\\mathbf{D}_{g}\\|^{2}\\|\\mathbf{Q}\\|^{2}\\|\\mathbf{S}_{\\infty}\\|^{2}\\|^{2}\\|\\mathbf{A}_{\\infty}\\|^{2}}{1-\\|\\mathbf{C}_{g}\\|}\\left(\\frac{L_{f,0}^{2}}{\\mu_{g}^{2}}\\sigma_{g,2 \n$$", "text_format": "latex", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad+\\frac{1}{10{R_{0}}^{3}}\\left(\\frac{8\\pi^{2}L^{2}}{9r_{0}^{4}}+\\frac{48\\pi^{2}\\alpha_{2}^{2}L^{2}}{9r_{0}^{4}}\\right)\\underset{=0}{\\overset{...}{\\longrightarrow}}\\frac{\\sum_{\\mathbf{{C}}}[\\mathbf{{F}}_{1}^{\\bot}]^{2}}{[\\mathbf{{F}}_{2}(\\mathbf{F}_{3}^{\\bot})]}\\frac{\\xi_{\\mathbf{C}}[\\mathbf{{F}}_{1}^{\\bot}]^{2}}{[\\mathbf{{F}}_{2}(\\mathbf{F}_{4}^{\\bot})]^{2}}+\\frac{[\\mathbf{{K}}_{1}^{\\bot}]^{4}}{[\\mathbf{{F}}_{2}(\\mathbf{F}_{3}^{\\bot})]^{2}}\\left(\\frac{8\\pi^{2}L^{2}L^{2}}{9r_{0}^{4}}+\\sigma_{3}^{2}\\right)}\\\\ &{\\quad+\\frac{1}{10{R_{0}}^{3}}\\frac{\\xi_{\\mathbf{B}}[\\mathbf{{F}}_{1}^{\\bot}]^{2}}{[\\mathbf{{F}}_{2}(\\mathbf{F}_{3}^{\\bot})]^{2}}+\\frac{[\\mathbf{{K}}_{1}^{\\bot}]^{4}}{[\\mathbf{{D}}_{1}(\\mathbf{F}_{4}^{\\bot})]^{2}}\\frac{\\xi_{\\mathbf{B}}[\\mathbf{{F}}_{2}^{\\bot}]^{2}}{[\\mathbf{{F}}_{3}(\\mathbf{F}_{4}^{\\bot})]^{2}}+\\frac{[\\mathbf{{K}}_{2}^{\\bot}]^{4}}{[\\mathbf{{F}}_{3}(\\mathbf{F}_{4}^{\\bot})]^{2}}L^{2}}\\\\ &{\\quad+\\left[(\\mathbf{{I}}_{1}+\\alpha_{2}^{2}L^{2})\\underset{=0}{,0}{\\overset{....}{\\longrightarrow}}\\frac{1}{10{R_{0}}^{3}}\\right]\\frac{\\left(\\mathbf{{S}}_{2}^{\\bot}\\right)^{2}}{[\\mathbf{{F}}_{2}(\\mathbf{F}_{3}^{\\bot})]^{2}}+\\frac{\\mathbf{{B}} \n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "where the second inequality holds since ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\frac{128L_{g,1}^{2}\\eta_{2}\\beta}{\\mu_{g}}\\leq\\frac{1}{8\\beta\\mu_{g}}.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Then taking (41) into the concern, we know: ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{388^{3}}\\left(\\frac{\\sqrt{6}}{4}\\frac{\\mathcal{E}_{2,1}}{8}\\|\\sum_{i=1}^{6}\\frac{\\mathcal{E}_{1}}{\\varepsilon_{i}-1}\\|\\tilde{\\phi}_{i}\\|_{L}\\right)}\\\\ &{\\le\\operatorname*{lim}\\bigg\\{40\\bigg;48^{2}\\bigg\\}+\\frac{\\Gamma^{2}}{168^{3}}\\bigg(8\\frac{\\Gamma^{2}\\Gamma^{2}}{9999}+\\frac{4888^{4}\\pi^{2}\\varepsilon_{r}^{2}}{99996}\\bigg)\\bigg(\\left(V^{3}+\\frac{4888^{2}}{999}\\right)\\sum_{i=1}^{5}\\big\\pi_{13}+n\\lambda_{1}\\bigg)}\\\\ &{\\quad+4\\bigg[4(\\eta+488^{2}\\varepsilon_{r}^{2})\\sigma^{2}+\\frac{\\Gamma^{2}}{1688^{3}}\\bigg(\\frac{\\sqrt{6}\\pi^{2}\\varepsilon_{r}^{2}}{1680}+\\frac{\\sqrt{6}\\pi^{2}\\varepsilon_{r}^{2}}{1680}\\bigg)\\bigg]\\left(9\\varepsilon_{1}^{2}-8\\eta^{2}\\right)}\\\\ &{\\quad+108(\\kappa+1961-4\\xi^{2}-9\\varepsilon_{r}^{2})\\sigma^{2}+\\frac{1}{12888}\\bigg(3\\frac{\\Gamma^{2}\\Gamma^{2}}{9999}+\\frac{4888^{2}\\pi^{2}\\varepsilon_{r}^{2}}{1680}\\bigg)\\bigg(\\sigma_{1}^{2}+2\\eta^{2}\\frac{\\Gamma^{2}\\Gamma^{2}}{168^{3}}\\bigg)}\\\\ &{\\quad+12(\\kappa+110\\alpha^{2}\\rho^{2}+12996(\\kappa+10))\\bigg(1-18\\eta^{3}\\left(\\frac{\\sqrt{6}\\pi^{2}\\varepsilon_{r}^{2}}{1680}+\\frac{\\sqrt{6}\\pi^{2}\\varepsilon_{r}^{2}}{1680}\\right)\\bigg)\\bigg(\\sigma_{1}^{2}+2\\eta^{2}\\frac{\\Gamma^{2}\\Gamma^{2}}{1680}\\bigg)}\\\\ &{\\quad+12(\\kappa+110\\alpha^{2}\\rho^{2}+1296(\\kappa+10))\\bigg\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Since ", "page_idx": 49}, {"type": "equation", "text": "$$\n40\\left[4(\\eta_{1}+48\\kappa^{2}L_{y^{\\star}}^{2}\\eta_{2})\\alpha^{2}+\\frac{1}{1020\\kappa^{4}}\\left(\\frac{9\\alpha^{2}L_{z^{\\star}}^{2}}{\\gamma^{2}\\mu_{g}^{2}}+\\frac{438\\kappa^{4}\\alpha^{2}}{\\beta^{2}\\mu_{g}^{2}}L_{y^{\\star}}^{2}\\right)\\right]\\left(L^{2}+\\frac{\\theta\\sigma_{g,2}^{2}}{n}\\right)\\le\\frac{1}{4080\\kappa^{4}},\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "it follows that ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{=}{\\overset{\\sum}{\\sum}}\\mathbb{E}[\\Delta_{k}+n\\,k]}\\\\ &{\\leq\\bigg[\\kappa^{4}(n+\\kappa^{2}L_{g}^{-2},v)\\alpha^{2}+\\bigg(\\frac{\\alpha^{2}L_{g}^{2}}{\\gamma^{2}n}+\\frac{\\kappa^{4}(n^{2}-2)}{\\beta\\gamma^{2}n^{4}}\\bigg)\\bigg]\\,\\frac{n(\\phi(x_{0})-\\mathrm{i}d\\phi)}{\\alpha}}\\\\ &{\\quad+\\theta(K+1)\\bigg[\\kappa^{4}(n+\\kappa^{2}L_{g}^{-2},v)\\alpha^{2}+\\bigg(\\frac{\\alpha^{2}L_{g}^{2}}{\\gamma^{2}n^{4}}+\\frac{\\kappa^{4}(n^{2}-2)}{\\beta\\gamma\\beta_{0}^{2}}\\bigg)\\bigg]\\,(\\sigma_{2}^{2}+\\kappa^{2}\\sigma_{2}^{2})}\\\\ &{\\quad+\\frac{\\omega^{2}\\beta\\left(\\kappa+1\\right)\\left\\vert1-1\\right\\vert\\nabla_{\\perp}\\mathrm{i}\\right\\vert\\nabla_{\\perp}\\mathrm{i}\\right\\vert\\mathrm{D}\\frac{1}{\\beta}}{n}\\bigg[\\mathrm{\\Phi}_{2}\\bigg]\\,\\bigg[\\mathrm{tr}^{2}\\,\\mathrm{s}^{2}+\\kappa^{4}\\eta_{0}\\beta^{2}\\Lambda^{2}}\\\\ &{\\quad+\\frac{\\kappa^{4}\\left\\vert0\\right\\vert\\beta\\right\\vert\\frac{1}{2}\\left\\vert\\beta\\right\\vert\\frac{1}{2}\\left\\vert0\\right\\vert-\\kappa^{2}\\left\\vert0\\right\\vert\\beta\\right\\vert\\mathrm{L}}{1-\\left\\vert0\\right\\vert\\beta\\right\\vert\\frac{1}{2}\\left\\vert0\\right\\vert-\\mathrm{i}\\frac{\\beta}{2}}+\\frac{\\kappa^{6}\\alpha^{2}\\left\\vert0\\right\\vert\\beta\\right\\vert\\frac{1}{2}\\left\\vert0\\right\\vert-1\\right\\vert^{2}\\left\\vert1\\right\\vert\\Delta_{\\perp}\\right\\vert^{2}}{\\left(1-\\left\\vert1\\right\\vert\\Gamma_{\\perp}\\right)^{2}}\\bigg]}\\\\ &{\\quad+\\frac{\\kappa^{4}\\left\\vert0\\right\\vert\\beta\\right\\vert\\frac{1}{2}\\left\\vert0\\right\\vert-\\mathrm{i}\\frac{\\beta}{2}\\left\\vert0\\right\\vert\\beta\\right\\vert\\frac{1}{2}\\left\\vert0\\right\\vert}{1-\\left\\vert0\\right\\vert\\mathrm{L}}+\\frac{\\kappa^{6}\\alpha^{2}\\left\\vert0\\right\\vert\\beta\\right\\vert\\frac{1\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Then, we finish the proof of this lemma. ", "page_idx": 49}, {"type": "text", "text": "Finally, we can give the proof of Lemma 17, which is a detailed version of Theorem 1: ", "page_idx": 49}, {"type": "text", "text": "Lemma 17 (Detailed version of Theorem 1). Suppose that Assumptions 1- 4 hold. Then there exist constantstep-sizes $\\alpha$ $\\beta,\\,\\gamma,\\,\\theta$ suchthat ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\frac{1}{K+1}\\displaystyle\\sum_{k=0}^{K}\\|\\nabla\\Phi(\\hat{\\alpha}^{k})\\|^{2}}\\\\ &{\\lesssim\\frac{\\kappa^{5}\\sigma}{\\sqrt{n K}}+\\kappa^{5}\\left[\\left(\\frac{\\|\\mathbf{O}_{\\eta}\\|^{2}\\|\\mathbf{O}_{\\kappa}^{-1}\\|^{2}\\|\\mathbf{A}_{\\infty}\\|^{2}}{1-\\|\\mathbf{C}_{\\eta}\\|}\\right)^{\\frac{3}{2}}+\\left(\\frac{\\|\\mathbf{O}_{\\eta}\\|^{2}\\|\\mathbf{O}_{\\kappa}^{-1}\\|^{2}\\|\\mathbf{A}_{\\infty}\\|^{2}}{1-\\|\\mathbf{C}_{\\eta}\\|}\\right)^{\\frac{3}{2}}\\right]\\frac{\\sigma^{\\frac{5}{8}}}{K^{\\frac{5}{8}}}}\\\\ &{\\quad+\\kappa^{4}\\left(\\frac{\\|\\mathbf{O}_{\\eta}\\|\\|\\mathbf{O}_{\\kappa}^{-1}\\|^{2}\\|\\mathbf{A}_{\\infty}\\|}{1-\\|\\mathbf{C}_{\\eta}\\|}\\right)^{\\frac{5}{2}}\\frac{\\sigma^{\\frac{1}{8}}}{K^{\\frac{7}{8}}}}\\\\ &{\\quad+\\left[\\kappa^{\\frac{9}{8}}\\left(\\frac{\\|\\mathbf{O}_{\\eta}\\|^{2}\\|\\mathbf{O}_{\\kappa}^{-1}\\|^{2}\\|\\mathbf{A}_{\\infty}\\|^{2}\\|\\mathbf{A}_{\\infty}^{\\frac{3}{2}}\\|^{2}}{\\eta(1-\\|\\mathbf{C}_{\\eta})\\|^{2}}\\right)^{\\frac{5}{6}}+\\kappa^{6}\\left(\\frac{\\|\\mathbf{O}_{\\eta}\\|^{2}\\|\\mathbf{O}_{\\kappa}^{-1}\\|^{2}\\|\\mathbf{A}_{\\infty}\\|^{2}\\|\\mathbf{A}_{\\infty}^{-1}\\|^{2}}{\\eta(1-\\|\\mathbf{C}_{\\eta})\\|^{2}}\\right)^{\\frac{1}{6}}\\right]\\frac{\\sigma^{\\frac{5}{8}}}{K^{\\frac{7}{8}}}}\\\\ &{\\quad+\\left[\\kappa^{\\frac{19}{8}}\\left(\\frac{\\|\\mathbf{O}_{\\eta}\\|^{2}\\|\\mathbf{O}_{\\kappa}^{-1}\\|^{2}\\|\\mathbf{A}_{\\infty}\\|^{2}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "where $\\sigma=\\operatorname*{max}\\{\\sigma_{f,1},\\sigma_{g,1},\\sigma_{g,2}\\},\\,C_{\\alpha},C_{\\theta}$ are defined as: ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r}{C_{\\alpha}=L_{\\nabla\\Phi}+\\kappa^{3}\\frac{\\|\\mathbf{O}_{x}\\|\\|\\mathbf{O}_{x}^{-1}\\|\\|\\mathbf{A}_{x\\alpha}\\|L}{1-\\|\\mathbf{I}_{x}\\|}+\\kappa^{3}L\\left(\\frac{\\|\\mathbf{O}_{x}\\|\\|\\mathbf{O}_{x}^{-1}\\|\\|\\mathbf{A}_{x\\alpha}\\|\\|\\mathbf{A}_{x\\beta}^{-1}\\|}{1-\\|\\mathbf{I}_{x}\\|}\\right)^{\\frac{1}{2}}+\\kappa^{4}\\left(\\frac{L_{g,1}^{2}}{\\mu_{g}}+\\frac{\\sigma_{g,1}^{2}}{n\\mu_{g}}\\right)}\\\\ {+\\,\\kappa^{4}\\frac{\\|\\mathbf{O}_{y}\\|\\|\\mathbf{O}_{y}^{-1}\\|\\|\\mathbf{A}_{y\\alpha}\\|L_{g,1}}{1-\\|\\mathbf{I}_{y}\\|}+\\kappa^{\\frac{9}{2}}L_{g,1}\\left(\\frac{\\|\\mathbf{O}_{y}\\|\\|\\mathbf{O}_{y}^{-1}\\|\\|\\mathbf{A}_{y\\alpha}\\|\\|\\mathbf{A}_{y\\beta}^{-1}\\|}{1-\\|\\mathbf{I}_{y}\\|}\\right)^{\\frac{1}{2}}+\\kappa^{4}\\left(\\mu_{g}+\\frac{\\mu_{g}^{2}\\sigma_{g,1}^{2}}{n L_{g,1}^{2}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\quad+\\kappa^{6}\\frac{\\|\\mathbf{O}_{z}\\|\\|\\mathbf{O}_{z}^{-1}\\|\\|\\mathbf{A}_{z a}\\|\\sqrt{L^{2}+(1-\\|\\mathbf{r}_{z}\\|)\\sigma_{g,2}^{2}}}{1-\\|\\mathbf{r}_{z}\\|}+\\kappa^{\\frac{11}{2}}L\\left(\\frac{\\|\\mathbf{O}_{z}\\|\\|\\mathbf{O}_{z}^{-1}\\|\\|\\mathbf{A}_{z a}\\|\\|\\mathbf{A}_{z b}^{-1}\\|}{1-\\|\\mathbf{r}_{z}\\|}\\right)^{\\frac{1}{2}},}\\\\ &{}&{\\quad\\quad{\\cal C}_{\\theta}=\\frac{\\sigma_{g,2}^{2}}{n L_{g,1}^{2}}+\\frac{\\sigma_{g,2}^{2}}{L^{2}}+1}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Proof.Take ${\\cal L}_{1}={\\cal L}^{2}+\\big(\\theta(1-\\theta)+{\\cal L}_{\\nabla\\Phi}\\alpha\\theta^{2}\\big)\\,{\\frac{\\sigma_{g,2}^{2}}{n}}$ and use the conclusion of Lemmas 8 and 16, weget: ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\frac{1}{N K}\\sum_{i=1}^{N}\\nabla\\bar{\\Phi}(\\hat{\\boldsymbol x}^{i})\\Big\\Vert^{2}}\\\\ &{\\le\\frac{C(\\hat{\\boldsymbol x})(0)-1}{\\alpha(K+1)}+\\frac{1}{N}\\left(\\theta\\left(1-\\theta\\right)+L\\alpha\\epsilon\\omega^{(i)}\\right)(\\rho_{+}^{2}+\\kappa^{2}\\omega_{\\partial}^{2})+\\frac{\\left(1-\\theta\\right)^{2}}{\\theta K+1}\\|\\nabla\\Phi(\\hat{\\boldsymbol x}^{i})\\|^{2}}\\\\ &{\\quad+L_{1}\\left[\\kappa\\left(\\eta+\\kappa^{2}\\hat{L}_{2}^{2}\\eta\\right)\\omega^{2}+\\left(\\frac{\\alpha^{2}L_{2}^{2}\\omega^{2}}{\\gamma_{0}^{2}K}+\\frac{\\kappa^{2}\\omega^{2}}{\\beta_{0}^{2}K}\\right)L_{2}^{2}\\right]\\left(\\frac{6\\hat{\\boldsymbol x}(0)-1-\\theta\\hat{\\boldsymbol x}^{i}}{1+\\kappa^{2}}\\right)+\\frac{\\beta}{\\mu}\\left(\\sigma_{+}^{2}+\\kappa^{2}\\omega_{\\partial}^{2}\\right)}\\\\ &{\\quad+L_{2}\\kappa^{2}\\|\\boldsymbol{\\Phi}(\\hat{\\boldsymbol x})\\|^{2}\\left(0,\\frac{17}{N}\\right)\\|\\boldsymbol{\\Phi}(\\hat{\\boldsymbol x}^{i})\\|^{2}\\rho_{-}^{2}+L_{1}\\kappa^{2}\\eta\\mu^{2}\\frac{\\nu^{2}\\hat{\\boldsymbol x}^{i}}{2}}\\\\ &{\\quad+\\frac{L_{2}\\hat{L}_{1}\\kappa^{2}\\|\\boldsymbol{\\Phi}(\\hat{\\boldsymbol x})\\|^{2}\\left(0,\\frac{17}{N}\\right)\\|\\boldsymbol{\\Phi}(\\hat{\\boldsymbol x}^{i})\\|^{2}}{\\theta\\left(1-\\frac{7}{N}\\right)\\left(\\eta+\\frac{8}{N}\\right)^{2}}+\\frac{L_{2}\\kappa^{2}\\|\\boldsymbol{\\Phi}(\\hat{\\boldsymbol x}^{i})\\|^{2}}{\\theta\\left(1-\\frac{7}{N}\\right)\\left(\\eta\\right)}}\\\\ &{\\quad+\\frac{L_{1}\\kappa^{2}\\|\\boldsymbol{\\Phi}(\\hat{\\boldsymbol x})\\|^{2}\\left(0,\\frac{17}{N}\\right)\\|\\boldsymbol{\\Phi}(\\hat{\\boldsymbol x}^{i})\\|^{2}}{\\theta \n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Define: ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\zeta_{0}^{y}=\\frac{1}{n}\\sum_{i=1}^{n}\\|\\nabla_{2}g_{i}(\\bar{x}_{0},\\bar{y}_{0})-\\nabla_{2}g(\\bar{x}_{0},\\bar{y}_{0})\\|^{2},}\\\\ {\\displaystyle\\zeta_{0}^{z}=\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}\\left[\\|\\nabla_{2}^{2}g_{i}(\\bar{x}_{0},\\bar{y}_{1})-\\nabla_{2}^{2}g(\\bar{x}_{0},\\bar{y}_{1})\\|^{2}\\|\\boldsymbol{z}_{\\star}^{1}\\|^{2}+\\|\\nabla_{2}g_{i}(\\bar{x}_{0},\\bar{y}_{1})-\\nabla_{2}g(\\bar{x}_{0},\\bar{y}_{1})\\|^{2}\\right],}\\\\ {\\displaystyle\\zeta_{0}^{x}=\\frac{1}{n}\\sum_{i=1}^{n}\\|\\nabla_{1}f_{i}(\\bar{x}_{0},y^{*}(\\bar{x}_{0}))-\\nabla_{1}f(\\bar{x}_{0},y^{*}(\\bar{x}_{0}))\\|^{2}}\\\\ {\\displaystyle\\qquad+\\frac{1}{n}\\sum_{i=1}^{n}\\|\\nabla_{1}^{2}g_{i}(\\bar{x}_{0},y^{*}(\\bar{x}_{0}))-\\nabla_{12}^{2}g(\\bar{x}_{0},y^{*}(\\bar{x}_{0}))\\|^{2}\\|z_{\\star}^{1}\\|^{2},}\\\\ {\\displaystyle\\hat{\\zeta}_{0}=\\frac{1}{n}\\left\\|\\tilde{\\nabla}\\Phi(\\bar{\\mathbf{x}}^{0})\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Then we take: ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\alpha_{1}=\\kappa^{-4}\\sqrt{\\frac{n}{K\\sigma^{2}}},}}\\\\ {{\\displaystyle\\alpha_{x,2}=\\left(\\frac{(1-\\|{\\bf T}_{x}\\|)^{2}}{\\kappa^{10}K\\|{\\bf O}_{x}\\|^{2}\\|{\\bf O}_{x}^{-1}\\|^{2}\\|\\Lambda_{x a}\\|^{2}\\sigma^{2}}\\right)^{\\frac{1}{4}}}}\\\\ {{\\displaystyle\\alpha_{y,2}=\\left(\\frac{1-\\|{\\bf T}_{y}\\|}{\\kappa^{13}K\\|{\\bf O}_{y}\\|^{2}\\|{\\bf O}_{y}^{-1}\\|^{2}\\|\\Lambda_{y a}\\|^{2}\\sigma_{g,1}^{2}}\\right)^{\\frac{1}{3}},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\alpha_{\\nu,3}=\\left(\\frac{\\displaystyle\\eta_{1}(1-\\frac{1}{\\nu}|\\tau_{1}|)^{2}}{\\displaystyle\\kappa_{\\nu}^{2}\\kappa_{\\nu}^{3}\\big[\\Phi_{0}|\\frac{1}{\\nu}|\\tau_{0}^{2}-\\frac{1}{\\nu}|\\tau_{1}\\big]\\lambda_{0}\\frac{1}{\\nu}|\\tau_{0}^{2}}\\right)^{1},}\\\\ &{\\alpha_{\\nu,2}=\\left(\\frac{\\displaystyle\\eta_{1}(1-\\frac{1}{\\nu}|\\tau_{1}|)}{\\displaystyle\\kappa_{\\nu}^{3}\\big[\\Phi_{0}|\\frac{1}{\\nu}|\\tau_{0}^{2}-\\frac{1}{\\nu}|\\tau_{1}|\\big]\\lambda_{0}\\frac{1}{\\nu}|\\tau_{0}^{2}}\\right)^{\\frac{1}{\\nu}},}\\\\ &{\\alpha_{\\nu,3}=\\left(\\frac{\\displaystyle\\eta_{1}(1-\\frac{1}{\\nu}|\\tau_{1}|)}{\\displaystyle\\kappa_{\\nu}^{2}\\kappa_{\\nu}^{4}\\big[\\Phi_{0}|\\frac{1}{\\nu}|\\big[\\Phi_{0}|\\frac{1}{\\nu}|\\big]^{2}(\\lambda_{0}\\frac{1}{\\nu}|\\tau_{1}^{2}-\\frac{1}{\\nu}|\\tau_{0}^{2})\\big]^{\\frac{1}{\\nu}}}\\right)^{\\frac{1}{\\nu}},}\\\\ &{\\alpha_{\\nu,4,2}=\\left(\\frac{\\displaystyle\\eta_{1}(1-\\frac{1}{\\nu}|\\tau_{1}|)}{\\displaystyle\\kappa_{\\nu}^{2}\\kappa_{\\nu}^{3}\\big[\\Phi_{0}|\\frac{1}{\\nu}|\\tau_{0}^{2}-\\frac{1}{\\nu}|\\tau_{1}\\big]\\lambda_{0}\\frac{1}{\\nu}|\\tau_{0}^{2}}\\right)^{\\frac{1}{\\nu}},}\\\\ &{\\alpha_{\\nu,4,2}=\\left(\\frac{\\displaystyle\\eta_{1}(1-\\frac{1}{\\nu}|\\tau_{1}|)}{\\displaystyle\\kappa_{\\nu}^{2}\\kappa_{\\nu}^{4}\\big[\\Phi_{0}|\\frac{1}{\\nu}|\\big[\\Phi_{0}|\\frac{1}{\\nu}|\\big]^{2}(\\lambda_{0}\\frac{1}{\\nu}|\\tau_{0} \n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "and ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\mathfrak{z}=\\left(C_{\\theta}+\\displaystyle\\frac{1}{\\theta_{1}}+\\displaystyle\\frac{1}{\\theta_{2}}\\right)^{-1},}&\\\\ &{\\mathfrak{z}=\\Theta\\left(C_{\\alpha}+\\displaystyle\\frac{\\sqrt{1-\\theta}}{\\theta}\\kappa^{3}+\\displaystyle\\frac{1}{\\alpha_{1}}+\\displaystyle\\frac{1}{\\alpha_{y,2}}+\\displaystyle\\frac{1}{\\alpha_{y,3}}+\\displaystyle\\frac{1}{\\alpha_{z,3}}+\\displaystyle\\frac{1}{\\alpha_{y,2}}+\\displaystyle\\frac{1}{\\alpha_{z b,2}}+\\displaystyle\\frac{1}{\\alpha_{x,2}}+\\displaystyle\\frac{1}{\\alpha_{x,3}}+\\displaystyle\\frac{1}{\\alpha_{z,2}}+\\displaystyle\\frac{1}{\\alpha_{x,2}}\\right)^{-1}}&\\\\ &{\\mathfrak{z}\\=\\Theta\\left(\\kappa^{4}\\alpha\\right),}&\\\\ &{\\gamma=\\Theta\\left(\\kappa^{4}\\alpha\\right),}&{\\scriptscriptstyle(80)}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "It yields $L_{1}=\\Theta(L^{2})$ , and (45), (53), (56), (40), (58), (65), (82), (84), and (85) hold. It implies that the restrictions on the step-sizes $\\alpha,\\beta,\\gamma,\\theta$ in all previous lemma conditions hold. Thus all previous lemmas hold. We obtain: ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{N}\\sum_{k=1}^{N}\\sum_{l=1}^{N}\\Gamma(\\nabla_{k}\\rho^{\\top}\\rho^{\\top})^{2}}\\\\ &{\\leq\\frac{c^{2}(k-1)-16\\lambda}{N}\\frac{1}{N}+\\frac{c^{2}(k)^{2}(l+1)^{2}}{N}+\\frac{c^{2}(k)}{N}+\\frac{c^{2}(k)(1-\\lambda)^{2}}{1-|\\nabla_{k}|^{2}}\\frac{1}{N}+\\frac{c^{2}(k)(1-\\lambda)^{2}}{\\lambda}+}\\\\ &{+\\frac{1}{N}\\left[\\frac{c^{2}(10,12)\\lambda+|\\nabla_{k}|^{2}\\rho^{\\top}|\\nabla_{k}|^{2}}{\\lambda}+\\frac{c^{2}(10,12)\\lambda+|\\nabla_{k}|^{2}\\rho^{\\top}|\\nabla_{k}|^{2}}{|\\nabla_{k}|^{2}}+\\frac{c^{2}(10,12)\\lambda-|\\nabla_{k}|^{2}\\rho^{\\top}|\\nabla_{k}|^{2}}{|\\nabla_{k}|^{2}}\\right]}\\\\ &{\\quad-\\frac{1}{N}\\left[c^{2}(10,12)\\lambda^{2}-1\\right]}\\\\ &{\\leq c^{2}\\eta^{2}+\\frac{1}{N}\\left[1-\\frac{c^{2}(10,12)\\lambda+|\\nabla_{k}|^{2}\\rho^{\\top}|\\nabla_{k}|^{2}}{\\lambda}+\\frac{c^{2}(10,12)\\lambda-|\\nabla_{k}|^{2}\\rho^{\\top}|\\nabla_{k}|^{2}}{|\\nabla_{k}|^{2}}+\\frac{c^{2}(10,12)\\lambda-|\\nabla_{k}|^{2}\\rho^{\\top}|\\nabla_{k}|^{2}}{|\\nabla_{k}|^{2}}\\right]}\\\\ &{\\quad+c^{2}\\eta^{2}+\\frac{1}{N}\\left[c^{2}(10,12)\\lambda^{2}-1\\right]-\\frac{c^{2}\\eta^{2}}{N}\\left[c^{2}(10,12)\\lambda^{3}+\\lambda^{2}\\rho^{\\top}_{0}(1-\\lambda)^{2}\\right]}\\\\ &{\\quad+c^{2}\\eta^{2}+\\frac{1}{N}\\left[c^{2}(10,1 \n$$$$\n\\begin{array}{r l}&{\\quad_{1}\\Biggl\\{\\Gamma\\}^{-1}}\\\\ &{\\quad_{2}\\Biggl\\{\\Gamma\\}^{1/2}}\\\\ &{\\quad+\\kappa^{2}\\omega^{2}\\left(6+\\frac12\\kappa^{2}\\right)\\Biggr\\}\\Biggl\\{\\frac{4}{\\pi}\\int_{\\frac{\\Gamma}{2}}\\Biggl[\\Gamma_{2}\\Biggr(\\frac12\\kappa^{2}\\Gamma_{1}\\times\\Gamma_{2}^{-1}\\chi_{-}\\chi_{0}^{2}\\}}\\\\ &{\\qquad\\quad+\\kappa^{2}\\omega^{2}\\left(6+\\frac12\\kappa^{2}\\right)\\pi^{2}\\left(\\frac12\\kappa^{2}\\Gamma_{1}^{-1}\\chi_{0}^{2}\\right)}\\\\ &{\\qquad\\quad+\\kappa\\left(\\frac1{2}\\frac{\\Gamma_{2}}\\right)\\log\\left(1-\\frac12\\kappa^{2}\\Gamma_{2}^{-1}\\chi_{0}^{2}\\right)}\\\\ &{\\qquad\\quad+\\kappa^{2}\\left(\\frac1{2}\\left(\\frac1{2}\\kappa^{2}\\Gamma_{1}^{-1}\\chi_{0}^{2}\\right)+\\kappa\\left(\\frac1{2}\\kappa^{-1}\\Gamma_{1}^{0}\\right)\\left(\\frac1{2}\\kappa^{-1}\\frac{\\Gamma_{2}^{-1}\\chi_{0}^{2}}{\\Gamma_{1}\\Gamma_{2}^{-1}}\\right)}\\\\ &{\\qquad\\quad+\\kappa\\left(\\frac1{2}\\kappa^{2}\\left(\\frac12\\kappa^{2}\\Gamma_{2}^{-1}\\chi_{0}^{2}\\right)+\\frac{4}{\\pi}\\kappa\\left(\\frac1{2}\\kappa^{-1}\\frac{\\Gamma_{2}^{-1}\\chi_{0}^{2}}{\\Gamma_{1}\\Gamma_{2}^{-1}}\\right)+\\frac{4}{\\Gamma_{2}}\\Biggl[\\Gamma_{2}\\Biggr(\\frac12\\kappa^{-1}\\Gamma_{1}\\frac{2}{\\Gamma_{2}}\\Biggr)\\log\\left(\\frac12\\kappa^{2}\\Gamma_{2}^{-1}\\right.}\\\\ &{\\qquad\\quad\\left.+\\left(\\frac{8}{\\pi}\\kappa^{2}\\left(\\frac1{2}\\kappa^{-1}\\Gamma_{2}^{-1}\\right)\\log\\left(\\frac1{2}\\kappa^{2}\\Gamma_{2}^{-1}\\right)+\\kappa^{2}\\left(\\frac1{2}\\kappa^{-1}\\frac{\\Gamma_{2}^{-1}\\chi_{0}^{2}}{\\Gamma_{1\n$$", "text_format": "latex", "page_idx": 52}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad_{4}+_{2}+_{10}|\\mathcal{G}_{z,1}|(|\\bar{\\phi}_{1}|,\\bar{\\phi}_{2}|)|(\\Delta_{1}\\bar{\\phi}_{2}|)|(\\Delta_{2}\\bar{\\phi}_{3}|)|(\\Delta_{3}\\bar{\\phi}_{4}|)}\\\\ &{=\\left(\\left.\\omega^{2}\\Delta_{3}\\right|^{3}\\bar{\\phi}_{1}^{(1)}\\left(\\Delta_{3}\\bar{\\phi}_{2}^{(1)}-\\Delta_{3}\\bar{\\phi}_{3}^{(1)}\\right)\\bar{\\phi}_{2}^{(1)}+\\omega^{2}\\bar{\\phi}_{4}\\right)\\left(\\left.1\\right|^{2}\\left(\\Delta_{3}\\bar{\\phi}_{2}^{(1)}-\\Delta_{3}\\bar{\\phi}_{3}^{(1)}\\right)\\bar{\\phi}_{3}^{(1)}\\right)}\\\\ &{\\quad+\\left[\\omega^{2}\\Delta_{3}\\right]\\left(\\frac{1}{2}\\bar{\\phi}_{3}^{(1)}-\\Delta_{3}\\bar{\\phi}_{4}^{(1)}\\left(\\frac{1}{2}\\bar{\\phi}_{4}^{(1)}-\\Delta_{3}\\bar{\\phi}_{4}^{(1)}\\left(\\frac{1}{2}\\bar{\\phi}_{4}^{(1)}-\\Delta_{3}\\bar{\\phi}_{5}^{(1)}\\right)+\\frac{\\omega^{2}}{2}\\bar{\\phi}_{5}^{(1)}\\right)|(\\Delta_{3}\\bar{\\phi}_{2}^{(1)}+\\Delta_{3}\\bar{\\phi}_{6}^{(1)})|(\\Delta_{3}\\bar{\\phi}_{3}^{(1)}+\\Delta_{3}\\bar{\\phi}_{2}^{(1)})}\\\\ &{\\quad_{4}+_{5}|\\bar{\\phi}_{3}|(|\\bar{\\phi}_{3}|,\\bar{\\phi}_{4}^{(1)}|)|(\\Delta_{3}\\bar{\\phi}_{4}^{(1)}-\\Delta_{3}\\bar{\\phi}_{3}^{(1)}+\\Delta_{3}\\bar{\\phi}_{2}^{(1)}}\\\\ &{\\quad+\\omega^{2}\\Delta_{3}\\bar{\\phi}_{3}^{(1)}\\left(\\frac{1}{2}\\bar{\\phi}_{4}^{(1)\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "where the last inequality uses (89). ", "page_idx": 53}, {"type": "text", "text": "Finally, substituting (88) and (89) into the last inequality, we can get: ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{K+1}\\displaystyle\\sum_{k=0}^{K}\\|\\nabla\\Phi(z^{k})\\|^{2}}\\\\ &{\\lesssim\\frac{\\kappa^{5}\\sigma}{\\sqrt{n}K}+\\kappa^{5}\\left(\\left(\\frac{\\|\\mathbf{O}_{g}\\|^{2}\\|\\mathbf{O}_{g}^{-1}\\|^{2}\\|\\mathbf{A}_{g}\\|^{2}}{1-\\|\\mathbf{O}_{g}\\|}\\right)^{4}\\right)^{\\frac{3}{4}}+\\left(\\frac{\\|\\mathbf{O}_{g}\\|^{2}\\|\\mathbf{O}_{g}^{-1}\\|^{2}\\|\\mathbf{A}_{\\infty}\\|^{2}}{1-\\|\\mathbf{O}_{g}\\|}\\right)^{\\frac{3}{4}}\\right)\\frac{\\sigma^{5}}{K^{5}}}\\\\ &{\\quad+\\kappa^{4}\\left(\\frac{\\|\\mathbf{O}_{g}\\|\\|\\mathbf{O}_{g}^{-1}\\|\\|\\mathbf{A}_{g}\\|}{1-\\|\\mathbf{O}_{g}\\|}\\right)^{\\frac{3}{4}}\\frac{\\sigma^{5}}{K^{5}}}\\\\ &{\\quad+\\left[\\kappa^{5}\\left(\\frac{\\|\\mathbf{O}_{g}\\|^{2}\\|\\mathbf{O}_{g}^{-1}\\|^{2}\\|\\mathbf{A}_{g}\\|^{2}\\|\\mathbf{A}_{g}\\|^{2}}{\\|\\mathbf{O}_{g}\\|-\\|\\mathbf{O}_{g}\\|}\\right)^{\\frac{1}{2}}+\\kappa^{6}\\left(\\frac{\\|\\mathbf{O}_{g}\\|^{2}\\|\\mathbf{O}_{g}^{-1}\\|^{2}\\|\\mathbf{A}_{g}\\|^{2}\\|\\mathbf{A}_{g}\\|^{2}}{\\|\\mathbf{O}_{g}\\|-\\|\\mathbf{O}_{g}\\|}\\right)^{\\frac{3}{4}}\\right]\\frac{\\sigma^{5}}{K^{4}}}\\\\ &{\\quad+\\left[\\kappa^{5}\\left(\\frac{\\|\\mathbf{O}_{g}\\|^{2}\\|\\mathbf{O}_{g}^{-1}\\|^{2}\\|\\mathbf{A}_{g}\\|^{2}\\|\\mathbf{A}_{g}\\|^{2}\\|\\mathbf{A}_{g}\\|^{2}}{1-\\|\\mathbf{O}_{g}\\|}\\right)^{\\frac{3}{4}}+\\kappa^{5}\\left(\\frac{\\|\\mathbf{O}_{g}\\|^{2}\\|\\mathbf{O}_{g}^\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "where $\\sigma=\\operatorname*{max}\\{\\sigma_{f,1},\\sigma_{g,1},\\sigma_{g,2}\\}$ ", "page_idx": 53}, {"type": "text", "text": "Remark 7. From the proof of Lemma 17, the impact of the moving average technique on variance reductionbecomes evident.hetem ${\\frac{\\theta}{n}}\\sigma^{2}$ absorb $\\alpha^{2}\\eta_{1}\\bar{\\sigma}^{2}$ which includes thehighorderterm $\\alpha^{4}\\sigma^{2}$ Additionally, compared to $y,z,$ thequadratic term related to $\\sigma^{2}$ of $x$ has an extra term $\\theta$ multiplied in the numerator $(\\bar{\\alpha^{2}}\\bar{\\theta}\\sigma^{2})$ .These details reduce the impacts of noise to terms related to $x$ confirming the conclusion that terms related to $y,z$ dominate the rate in precious sections. Notably, taking $\\theta<1$ is indispensable our proof. If we take $\\theta=1$ , there would be a constant term ${\\scriptstyle{\\frac{1}{n}}}\\sigma^{2}$ in the convergence rate (see the first inequality of (87)), since the coeffcient $\\alpha^{2}/\\beta^{2}+\\alpha^{2}/\\gamma^{2}=\\mathcal{O}(1)$ .This would not guarantee the convergence of SPARKLE. ", "page_idx": 53}, {"type": "text", "text": "C.2  Analysis of consensus error and transient iteration complexity ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "From Lemma 17, we can immediately obtain the transient time complexity of Algorithm 1. Here we omit the impacts of the condition number $\\kappa$ ", "page_idx": 53}, {"type": "text", "text": "Lemma 18. The transient time complexity of Algorithm $^{\\,l}$ has an upper bound of: ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{max}\\left\\{n^{3}\\left(\\frac{\\|\\mathbf{O}_{y}\\|^{2}\\|\\mathbf{O}_{y}^{-1}\\|^{2}}{1-\\|\\mathbf{I}_{y}\\|}\\right)^{2}\\|\\mathbf{A}_{y a}\\|^{2},n^{3}\\left(\\frac{\\|\\mathbf{O}_{z}\\|^{2}\\|\\mathbf{O}_{z}^{-1}\\|^{2}}{1-\\|\\mathbf{I}_{z}\\|}\\right)^{2}\\|\\mathbf{A}_{z a}\\|^{2},}\\\\ &{\\qquad\\quad n^{2}\\left(\\frac{\\|\\mathbf{O}_{x}\\|\\|\\mathbf{O}_{x}^{-1}\\|}{1-\\|\\mathbf{I}_{x}\\|}\\right)^{2}\\|\\mathbf{A}_{x a}\\|^{2},n\\left(\\frac{\\|\\mathbf{O}_{y}\\|\\|\\mathbf{O}_{y}^{-1}\\|\\|\\mathbf{A}_{y b}^{-1}\\|}{1-\\|\\mathbf{I}_{y}\\|}\\right)^{\\frac{4}{3}}\\|\\mathbf{A}_{y a}\\|,}\\\\ &{\\qquad\\quad n\\left(\\frac{\\|\\mathbf{O}_{z}\\|\\|\\mathbf{O}_{z}^{-1}\\|\\|\\mathbf{A}_{z b}^{-1}\\|}{1-\\|\\mathbf{I}_{z}\\|}\\right)^{\\frac{4}{3}}\\|\\mathbf{A}_{z a}\\|,n\\left(\\frac{\\|\\mathbf{O}_{x}\\|^{2}\\|\\mathbf{O}_{x}^{-1}\\|^{2}\\|\\mathbf{A}_{x a}\\|^{2}\\|\\mathbf{A}_{x b}^{-1}\\|^{2}}{1-\\|\\mathbf{I}_{x}\\|}\\right)^{\\frac{2}{3}},}\\\\ &{\\qquad\\quad n\\frac{\\|\\mathbf{O}_{x}\\|\\|\\mathbf{O}_{x}^{-1}\\|\\|\\mathbf{A}_{x a}\\|\\|\\mathbf{A}_{x b}^{-1}\\|}{1-\\|\\mathbf{I}_{x}\\|},n\\Bigg\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Proof. According to lemma 17, SPARKLE achieves linear speedup if: ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\frac{1}{\\sqrt{n K}}\\gtrsim\\left[\\left(\\frac{\\|\\mathbf{O}_{y}\\|^{2}\\|\\mathbf{O}_{y}^{-1}\\|^{2}\\|\\mathbf{A}_{y a}\\|^{2}}{1-\\|\\mathbf{I}_{y}\\|}\\right)^{\\frac{1}{3}}+\\left(\\frac{\\|\\mathbf{O}_{z}\\|^{2}\\|\\mathbf{O}_{z}^{-1}\\|^{2}\\|\\mathbf{A}_{z a}\\|^{2}}{1-\\|\\mathbf{I}_{z}\\|}\\right)^{\\frac{1}{3}}\\right]\\frac{1}{K^{\\frac{2}{3}}}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{+\\left(\\frac{\\|\\mathbf{O}_{x}\\|\\|\\mathbf{O}_{x}^{-1}\\|\\|\\mathbf{A}_{x\\theta}\\|}{1-\\|\\mathbf{P}_{y}\\|}\\right)^{\\frac{1}{2}}\\frac{1}{K^{\\frac{3}{4}}}}\\\\ &{+\\left[\\left(\\frac{\\|\\mathbf{O}_{y}\\|^{2}\\|\\mathbf{O}_{y}^{-1}\\|^{2}\\|\\mathbf{A}_{y\\theta}\\|^{2}\\|\\mathbf{A}_{y\\theta}^{-1}\\|^{2}}{n(1-\\|\\mathbf{F}_{y}\\|)^{2}}\\right)^{\\frac{1}{8}}+\\left(\\frac{\\|\\mathbf{O}_{z}\\|^{2}\\|\\mathbf{O}_{z}^{-1}\\|^{2}\\|\\mathbf{A}_{z\\theta}\\|^{2}\\|\\mathbf{A}_{z\\theta}^{-1}\\|^{2}}{n(1-\\|\\mathbf{F}_{z}\\|)^{2}}\\right)^{\\frac{1}{8}}\\right]\\frac{1}{K^{\\frac{3}{4}}}}\\\\ &{+\\left[\\left(\\frac{\\|\\mathbf{O}_{y}\\|^{2}\\|\\mathbf{O}_{y}^{-1}\\|^{2}\\|\\mathbf{A}_{y\\theta}\\|^{2}\\|\\mathbf{A}_{y\\theta}^{-1}\\|^{2}\\zeta_{0}^{y}\\right)^{\\frac{1}{8}}}{1-\\|\\mathbf{F}_{y}\\|}+\\left(\\frac{\\|\\mathbf{O}_{z}\\|^{2}\\|\\mathbf{O}_{z}^{-1}\\|^{2}\\|\\mathbf{A}_{z\\theta}\\|^{2}\\|\\mathbf{A}_{z\\theta}^{-1}\\|^{2}\\zeta_{0}^{\\zeta_{0}}\\right)^{\\frac{1}{3}}}+\\left(\\frac{\\|\\mathbf{O}_{z}\\|^{2}\\|\\mathbf{O}_{z}^{-1}\\|^{2}\\|\\mathbf{A}_{z\\theta}\\|^{2}\\|\\mathbf{A}_{z\\theta}^{-1}\\|^{2}\\zeta_{0}^{\\zeta_{0}}}{1-\\|\\mathbf{F}_{z}\\|}\\right)^{\\frac{1}{2}}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "equation", "text": "$$\n+\\left(\\frac{\\|\\mathbf{O}_{x}\\|^{2}\\|\\mathbf{O}_{x}^{-1}\\|^{2}\\|\\mathbf{A}_{x a}\\|^{2}\\|\\mathbf{A}_{x b}^{-1}\\|^{2}\\zeta_{0}^{x}}{1-\\|\\mathbf{r}_{x}\\|}\\right)^{\\frac{1}{3}}\\right]\\frac{1}{K}+\\left(C_{\\alpha}+C_{\\theta}\\right)\\frac{1}{K}.\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "It holds when $K$ satisfies: ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\left(\\frac{|0|\\bar{g}_{1}|^{2}(\\Omega_{0}^{\\prime};1|\\bar{g}_{1},\\bar{\\Omega}_{0}^{\\prime})|^{3}}{1-|\\bar{g}_{1}|^{2}|\\bar{g}_{1}|}\\right)^{1}\\frac{1}{K}\\le\\frac{1}{\\sqrt{n K}}}\\\\ &{}&{\\left(\\frac{|0|\\bar{g}_{1}|^{2}(\\Omega_{0}^{\\prime};1|\\bar{g}_{1},\\bar{\\Omega}_{0}^{\\prime})|^{3}}{1-|\\bar{g}_{1}|^{2}|\\bar{g}_{1}|}\\right)^{1}\\frac{1}{K}\\le\\frac{1}{\\sqrt{n K}}}\\\\ &{}&{\\left(\\frac{|0|\\bar{g}_{1}|^{2}(\\Omega_{0}^{\\prime};1|\\bar{g}_{1},\\bar{\\Omega}_{0}^{\\prime})|^{3}}{1-|\\bar{g}_{1}|^{2}|\\bar{g}_{1}|}\\right)^{2}\\frac{1}{K}\\le\\frac{1}{\\sqrt{n K}}}\\\\ &{}&{\\left(\\frac{|0|\\bar{g}_{1}|^{2}(\\Omega_{0}^{\\prime};1|\\bar{g}_{1},\\bar{\\Omega}_{0}^{\\prime})|^{3}}{1-|\\bar{g}_{1}|^{2}|\\bar{g}_{1}|}\\right)^{1}\\frac{1}{K}\\le\\frac{1}{\\sqrt{n K}}}\\\\ &{}&{\\left(\\frac{|0|\\bar{g}_{1}|^{2}(\\Omega_{0}^{\\prime};1|\\bar{g}_{1},\\bar{\\Omega}_{0}^{\\prime})|^{3}}{1-|\\bar{g}_{1}|^{2}|\\bar{g}_{1}|}\\right)^{2}\\frac{1}{K}\\le\\frac{1}{\\sqrt{n K}}}\\\\ &{}&{\\left(\\frac{|0|\\bar{g}_{1}|^{2}(\\Omega_{0}^{\\prime};1|\\bar{g}_{1},\\bar{\\Omega}_{0}^{\\prime})|^{3}}{1-|\\bar{g}_{1}|^{2}|\\bar{g}_{1}|}\\right)^{2}\\frac{1}{K}\\le\\frac{1}{\\sqrt{n K}}}\\\\ &{}&{\\left(\\frac{|0|\\bar\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "Then we get: ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{K\\gtrsim\\operatorname*{max}\\left\\{n^{3}\\left(\\frac{\\|\\mathbf{O}_{y}\\|^{2}\\|\\mathbf{O}_{y}^{-1}\\|^{2}}{1-\\|\\mathbf{I}_{y}\\|}\\right)^{2}\\|\\mathbf{A}_{y s}\\|^{2},n^{3}\\left(\\frac{\\|\\mathbf{O}_{z}\\|^{2}\\|\\mathbf{O}_{z}^{-1}\\|^{2}}{1-\\|\\mathbf{I}_{z}\\|}\\right)^{2}\\|\\mathbf{A}_{z s}\\|^{2},}\\\\ &{\\qquad\\qquad\\quad n^{2}\\left(\\frac{\\|\\mathbf{O}_{x}\\|\\|\\mathbf{O}_{x}^{-1}\\|}{1-\\|\\mathbf{I}_{x}\\|}\\right)^{2}\\|\\mathbf{A}_{x s}\\|^{2},n\\left(\\frac{\\|\\mathbf{O}_{y}\\|\\|\\mathbf{O}_{y}^{-1}\\|\\|\\mathbf{A}_{y s}^{-1}\\|}{1-\\|\\mathbf{I}_{y}\\|}\\right)^{\\frac{4}{3}}\\|\\mathbf{A}_{y s}\\|,}\\\\ &{\\qquad\\qquad\\quad n\\left(\\frac{\\|\\mathbf{O}_{z}\\|\\|\\mathbf{O}_{z}^{-1}\\|\\|\\mathbf{A}_{z s}^{-1}\\|}{1-\\|\\mathbf{I}_{z}\\|}\\right)^{\\frac{4}{3}}\\|\\mathbf{A}_{z s}\\|,n\\left(\\frac{\\|\\mathbf{O}_{x}\\|^{2}\\|\\mathbf{O}_{x}^{-1}\\|^{2}\\|\\mathbf{A}_{x s}\\|^{2}\\|\\mathbf{A}_{x s}^{-1}\\|^{2}}{1-\\|\\mathbf{I}_{x}\\|}\\right)^{\\frac{2}{3}},}\\\\ &{\\qquad\\qquad\\quad n\\frac{\\|\\mathbf{O}_{x}\\|\\|\\mathbf{O}_{x}^{-1}\\|\\|\\mathbf{A}_{x s}\\|\\|\\mathbf{A}_{x s}^{-1}\\|}{1-\\|\\mathbf{I}_{x}\\|},n\\Bigg\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "C.2.1 Consensus Error ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Lemma 19. Suppose that Assumptions 1- 4 hold. Then there exist constant step-sizes $\\alpha,\\beta,\\gamma,\\theta$ such thatLemma $^{17}$ holdsand ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\frac{1}{K}\\displaystyle\\sum_{k=0}^{K}\\mathbb{E}\\left[\\frac{\\|\\mathbf{x}^{k}-\\bar{\\mathbf{x}}^{k}\\|^{2}}{n}+\\frac{\\|\\mathbf{y}^{k}-\\bar{\\mathbf{y}}^{k}\\|^{2}}{n}\\right]}\\\\ &{\\displaystyle\\lesssim_{K}\\!\\frac{n}{K}\\left(\\frac{\\|\\mathbf{O}_{z}\\|^{2}\\|\\mathbf{O}_{z}^{-1}\\|^{2}\\|\\mathbf{A}_{z a}\\|^{2}}{1-\\|\\mathbf{r}_{z}\\|}+\\frac{\\|\\mathbf{O}_{y}\\|^{2}\\|\\mathbf{O}_{y}^{-1}\\|^{2}\\|\\mathbf{A}_{y a}\\|^{2}}{1-\\|\\mathbf{r}_{y}\\|}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "where $\\lesssim_{K}$ denotes the the asymptotic rate when $K\\rightarrow\\infty$ ", "page_idx": 55}, {"type": "text", "text": "Proof.Suppose $\\alpha,\\,\\beta,\\,\\gamma$ and $\\theta$ satisfy the constraints given in (88) and (89), which ensures that Theorem 1 (Lemma 17) holds. ", "page_idx": 55}, {"type": "text", "text": "For clarity, we define the constants: ", "page_idx": 55}, {"type": "equation", "text": "$$\nc_{1}=\\frac{9\\alpha^{2}L_{z^{\\star}}^{2}}{\\gamma^{2}\\mu_{g}^{2}}+\\frac{438\\kappa^{4}\\alpha^{2}}{\\beta^{2}\\mu_{g}^{2}}L_{y^{\\star}}^{2},\\quad c_{2}=10\\left(L^{2}+\\frac{\\theta\\sigma_{g,2}^{2}}{n}\\right).\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Then there exist $\\alpha,\\beta,\\gamma$ and $\\theta$ that satisfy the constraints in (88) and (89), and also: ", "page_idx": 55}, {"type": "equation", "text": "$$\nc_{1}\\leq0.01L^{-2},\\quad c_{2}\\leq11L^{2}.\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "We take such values for step-sizes in the following proof. ", "page_idx": 55}, {"type": "text", "text": "We proceed by substituting (41) into (57), yielding: ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{k=-1}^{K}\\mathbb{E}[I_{k}]\\leq4c_{1}\\left(\\frac{\\Phi(\\bar{x}_{0})-\\operatorname*{inf}\\Phi}{\\alpha}+c_{2}\\sum_{k=0}^{K-1}\\mathbb{E}\\left(\\frac{\\Delta_{k}}{n}+I_{k}\\right)+\\frac{3\\theta}{n}K\\left(\\sigma_{f,1}^{2}+2\\sigma_{g,2}^{2}\\frac{L_{f,0}^{2}}{\\mu_{g}^{2}}\\right)\\right)}\\\\ {\\displaystyle\\qquad+\\left.510\\kappa^{4}\\sum_{k=0}^{K}\\mathbb{E}\\left[\\frac{\\Delta_{k}}{n}\\right]+\\frac{3\\|z_{*}^{1}\\|^{2}}{\\mu_{g}\\gamma}}\\\\ {\\displaystyle\\qquad+\\,\\frac{6(K+1)\\gamma}{\\mu_{g}n}\\left(3\\sigma_{g,2}^{2}\\frac{L_{f,0}^{2}}{\\mu_{g}^{2}}+\\sigma_{f,1}^{2}\\right)+73\\kappa^{4}\\left(\\frac{4}{\\beta\\mu_{g}}\\|\\bar{y}^{0}-y^{*}(\\bar{x}^{0})\\|^{2}+\\frac{4K\\sigma_{g,1}^{2}}{n\\mu_{g}}\\beta\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Subtracting $\\begin{array}{r}{4c_{1}c_{2}\\sum_{k=0}^{K-1}\\mathbb{E}[I_{k}]}\\end{array}$ from both sides, we get: ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\displaystyle\\sum_{k=-1}^{K}\\mathbb{E}[I_{k}]\\lesssim\\!\\frac{\\Phi(\\bar{x}_{0})-\\operatorname*{inf}\\Phi}{\\alpha}+\\frac{\\theta}{n}K\\left(\\sigma_{f,1}^{2}+\\sigma_{g,2}^{2}\\frac{L_{f,0}^{2}}{\\mu_{g}^{2}}\\right)+\\kappa^{4}\\displaystyle\\sum_{k=0}^{K}\\mathbb{E}\\left[\\frac{\\Delta_{k}}{n}\\right]+\\frac{\\|z_{*}^{1}\\|^{2}}{\\mu_{g}\\gamma}}\\\\ &{}&{\\displaystyle+\\,\\frac{K\\gamma}{\\mu_{g}n}\\left(\\sigma_{g,2}^{2}\\frac{L_{f,0}^{2}}{\\mu_{g}^{2}}+\\sigma_{f,1}^{2}\\right)+\\kappa^{4}\\left(\\frac{1}{\\beta\\mu_{g}}\\|{\\bar{y}}^{0}-y^{*}({\\bar{x}}^{0})\\|^{2}+\\frac{K\\sigma_{g,1}^{2}}{n\\mu_{g}}\\beta\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Substituting (57) into (41), we obtain: ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\frac{1}{4}\\sum_{k=0}^{K}\\mathbb{E}\\left\\|r^{k+1}\\right\\|^{2}}\\\\ &{\\leq\\displaystyle\\frac{\\Phi(\\bar{x}_{0})-\\operatorname*{inf}\\Phi}{\\alpha}+c_{2}\\sum_{k=0}^{K}\\mathbb{E}\\left[\\frac{\\Delta_{k}}{n}\\right]+c_{2}c_{1}\\sum_{k=0}^{K}\\mathbb{E}\\|r^{k}\\|^{2}}\\\\ &{\\quad+\\displaystyle c_{2}\\left[510_{k}^{4}\\sum_{k=0}^{K}\\mathbb{E}\\left[\\frac{\\Delta_{k}}{n}\\right]+\\frac{3\\|z_{k}^{*}\\|^{2}}{\\mu_{g}\\gamma}+\\frac{6(K+1)\\gamma}{\\mu_{g}n}\\left(3\\sigma_{g,2}^{2}\\frac{L_{f,0}^{2}}{\\mu_{g}^{2}}+\\sigma_{f,1}^{2}\\right)\\right.}\\\\ &{\\quad\\left.\\quad+73\\kappa^{4}\\left(\\frac{4}{\\beta\\mu_{g}}\\|\\bar{y}^{0}-y^{*}(\\bar{x}^{0})\\|^{2}+\\frac{4K\\sigma_{g,1}^{2}}{n\\mu_{g}}\\beta\\right)\\right]}\\\\ &{\\quad+\\frac{3\\theta}{n}(K+1)\\left(\\sigma_{f,1}^{2}+2\\sigma_{g,2}^{2}\\frac{L_{f,0}^{2}}{\\mu_{g}^{2}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Subtracting $\\scriptstyle c_{2}c_{1}\\sum_{k=0}^{K}\\mathbb{E}\\|{\\bar{r}}^{k}\\|^{2}$ from both sides, we get ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\sum_{k=0}^{K}\\mathbb{E}\\left\\|\\bar{r}^{k+1}\\right\\|^{2}}\\\\ &{\\lesssim\\!\\frac{\\Phi(\\bar{x}_{0})-\\operatorname*{inf}\\Phi}{\\alpha}+\\kappa^{4}\\displaystyle\\sum_{k=0}^{K}\\mathbb{E}\\left[\\frac{\\Delta_{k}}{n}\\right]+\\frac{\\theta}{n}K\\left(\\sigma_{f,1}^{2}+\\sigma_{g,2}^{2}\\frac{L_{f,0}^{2}}{\\mu_{g}^{2}}\\right)}\\\\ &{\\quad+\\displaystyle\\frac{\\|z_{\\star}^{1}\\|^{2}}{\\mu_{g}\\gamma}+\\frac{K\\gamma}{\\mu_{g}n}\\left(\\sigma_{g,2}^{2}\\frac{L_{f,0}^{2}}{\\mu_{g}^{2}}+\\sigma_{f,1}^{2}\\right)+\\kappa^{4}\\left(\\frac{1}{\\beta\\mu_{g}}\\|\\bar{y}^{0}-y^{\\star}(\\bar{x}^{0})\\|^{2}+\\frac{K\\sigma_{g,1}^{2}}{n\\mu_{g}}\\beta\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Taking ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\eta_{3}=\\left(\\frac{\\kappa^{2}\\|\\mathbf{O}_{x}^{-1}\\|^{2}\\|\\mathbf{O}_{x}\\|^{2}\\|\\mathbf{A}_{x a}\\|^{2}\\alpha^{2}}{(1-\\|\\mathbf{I}_{x}\\|)^{2}}+\\frac{\\|\\mathbf{O}_{z}\\|^{2}\\|\\mathbf{O}_{z}^{-1}\\|^{2}\\|\\mathbf{A}_{z a}\\|^{2}}{1-\\|\\mathbf{I}_{z}\\|}\\cdot\\frac{\\gamma^{2}(L_{g,1}^{2}+(1-\\|\\mathbf{I}_{z}\\|)\\sigma_{g,2}^{2})}{1-\\|\\mathbf{I}_{z}\\|}\\right),\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "and combining previous results with (83), we obtain ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\le(\\lambda_{1}+\\kappa^{2}L_{2}^{2})\\eta_{1}\\alpha_{2}^{2}\\frac{b_{2}^{2}}{\\omega_{1}^{2}}\\chi_{1}\\mu_{1}^{-1}+\\eta_{2}\\alpha_{3}\\mu_{1}^{3}\\nu^{2}-\\gamma(\\nu^{2})(\\mu_{1}^{2}+\\kappa_{1}\\omega_{2}\\nu_{3}^{2}+\\eta_{3}\\omega_{2}^{2}+\\eta_{3}^{2}\\omega_{1}^{2})}\\\\ &{\\le\\frac{\\bar{a}^{2}\\bar{b}^{2}\\bar{b}\\bar{b}\\bar{b}\\bar{b}\\omega_{1}^{2}\\left(\\eta_{1}^{2}\\omega_{1}^{2}-\\eta_{2}^{2}\\right)\\left(\\kappa_{1}^{2}-\\omega_{1}^{2}\\omega_{1}^{2}+\\eta_{1}\\sqrt{\\kappa_{1}\\omega_{2}^{2}}\\right)}{\\gamma_{1}\\omega_{1}^{2}}+\\frac{\\bar{a}^{2}\\bar{b}^{2}\\bar{b}^{2}}{11}\\frac{b_{2}^{2}}{\\omega_{1}^{2}}\\mu_{1}^{-1}}\\\\ &{\\quad+\\frac{\\bar{a}^{2}\\bar{b}^{2}\\bar{b}\\bar{b}\\bar{b}\\omega_{1}^{2}}{\\gamma_{1}\\omega_{1}^{2}}+\\frac{\\bar{a}^{2}\\bar{b}^{2}\\bar{b}\\bar{b}\\omega_{1}^{2}}{\\gamma_{1}\\omega_{1}^{2}}\\mu_{1}^{-1}\\left(\\kappa_{1}^{2}\\omega_{1}\\nu_{3}^{2}\\right)\\left(\\frac{1}{\\omega_{1}}\\nu_{1}^{2}\\right)^{2}}\\\\ &{\\quad+\\kappa\\frac{1}{8}(1-\\gamma^{2}\\frac{\\|\\bar{\\omega}\\|_{\\mathrm{p}}^{2}}{\\omega_{1}^{2}})\\frac{\\left(1-\\gamma\\right)^{2}\\bar{b}\\omega_{1}^{2}}{\\gamma_{1}\\omega_{1}^{2}}\\left(\\eta_{1}^{2}\\omega_{1}^{2}+\\eta_{2}^{2}\\right)}\\\\ &{\\quad+K\\kappa^{2}\\frac{\\bar{b}^{2}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "(88) and (89) imply that ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\eta_{1}\\lesssim\\kappa^{2}+\\kappa^{2}\\frac{\\|\\mathbf{O}_{x}\\|^{2}\\|\\mathbf{O}_{x}^{-1}\\|^{2}\\|\\mathbf{A}_{x a}\\|^{2}}{(1-\\|\\mathbf{I}_{x}\\|)^{2}},\\quad\\eta_{2}\\lesssim\\kappa^{2},}\\\\ &{(\\eta_{1}+\\kappa^{2}L_{y^{\\star}}^{2}\\eta_{2})\\alpha^{2}\\lesssim\\kappa^{-4},\\quad\\eta_{3}\\lesssim\\kappa^{-4}}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "where $\\eta_{1},\\eta_{2}$ are defined in Lemma 15. ", "page_idx": 57}, {"type": "text", "text": "Then taking $\\alpha,\\beta,\\gamma,\\theta$ such that (88),(89), (91) hold and $\\kappa^{4}[(\\eta_{1}+\\kappa^{2}L_{y^{\\star}}^{2}\\eta_{2})\\alpha^{2}+\\eta_{3}]$ is a sufficiently small constant, we can derive the following result: ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Phi_{i}^{\\varepsilon}}&{=\\varepsilon^{-1}\\Phi_{i}\\;,}\\\\ &{+\\left(\\left|\\begin{array}{l}{0+\\varepsilon^{2}\\varepsilon^{2}\\eta_{0}^{\\varepsilon}}\\\\ {+\\varepsilon^{2}\\eta_{0}^{\\varepsilon}}\\end{array}\\right|\\partial_{x}^{2}+\\varepsilon^{2}\\eta_{i}^{\\varepsilon}\\right)\\Bigg[\\frac{1}{\\sin^{2}\\eta_{0}^{\\varepsilon}}\\left(\\eta_{0}^{2}+\\varepsilon^{2}\\eta_{1}^{\\varepsilon}\\right)+\\frac{1}{\\sqrt{2}\\eta_{0}^{\\varepsilon}}\\left(\\eta_{0}^{2}+\\varepsilon^{2}\\eta_{1}^{\\varepsilon}\\right)\\Bigg]}\\\\ &{+\\left(\\left|\\begin{array}{l}{1+\\varepsilon^{2}\\eta_{0}^{2}\\eta_{0}^{\\varepsilon}}\\\\ {+\\varepsilon^{2}\\eta_{1}^{\\varepsilon}\\right)\\sin^{2}\\eta_{0}^{\\varepsilon}+\\left(\\frac{1}{\\sqrt{2}\\eta_{1}^{\\varepsilon}}\\frac{\\eta_{0}^{2}}{\\sqrt{6}}\\right)\\alpha_{1}^{2}+\\frac{2(\\eta_{0}^{2}-\\eta_{1}^{\\varepsilon})\\alpha_{1}^{2}}{\\sqrt{6}\\eta_{0}^{2}\\left(\\eta_{0}^{2}+\\varepsilon^{2}\\eta_{1}^{\\varepsilon}\\right)}+\\frac{2(\\eta_{0}^{2}-\\eta_{1}^{\\varepsilon})\\alpha_{1}^{2}}{\\sqrt{6}\\eta_{0}^{2}\\left(\\eta_{0}^{2}+\\varepsilon^{2}\\eta_{1}^{\\varepsilon}\\right)}}\\\\ &{+\\frac{\\varepsilon^{2}\\eta_{0}^{2}\\left(\\eta_{0}^{2}+\\varepsilon^{2}\\eta_{1}^{\\varepsilon}\\right)\\left\\langle\\eta_{0}^{2}\\eta_{0}^{2}\\eta_{0}^{\\varepsilon}\\right.}\\\\ &{+\\frac{4(\\eta_{0}^{2}+\\varepsilon^{2}\\eta_{1}^{\\varepsilon})\\left\\langle\\eta_{0}^{2}\\eta_{0}^{2}\\eta_{0}^{\\varepsilon}\\right.\\left(\\eta_{0}^{2}+\\varepsilon^{2}\\eta_{1}^{\\varepsilon}\\right)\\left(\\eta_{0}^{2}+\\varepsilon^{2}\\eta_{1}^{\\varepsilon}\\right)\\left(\\eta_{0}^{2}+\\varepsilon^ \n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "From (88) and (89), we can determine the asymptotic orders for $\\alpha,\\beta,\\gamma$ and $\\theta$ when $K\\rightarrow\\infty$ ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\alpha=\\mathcal{O}\\left(\\kappa^{-4}\\sqrt{\\frac{n}{K\\sigma^{2}}}\\right),\\quad\\beta=\\mathcal{O}\\left(\\sqrt{\\frac{n}{K\\sigma^{2}}}\\right),\\quad\\gamma=\\mathcal{O}\\left(\\sqrt{\\frac{n}{K\\sigma^{2}}}\\right),\\quad\\theta=\\mathcal{O}\\left(\\kappa\\sqrt{\\frac{n}{K\\sigma^{2}}}\\right).\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "Then we get ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\frac{1}{K}\\sum_{k=0}^{K}\\mathbb{E}\\left[\\frac{\\Delta_{k}}{n}\\right]\\lesssim_{K}\\frac{\\kappa^{2}n}{K}\\left(\\frac{\\|\\mathbf{O}_{z}\\|^{2}\\|\\mathbf{O}_{z}^{-1}\\|^{2}\\|\\mathbf{A}_{z a}\\|^{2}}{1-\\|\\mathbf{r}_{z}\\|}+\\frac{\\|\\mathbf{O}_{y}\\|^{2}\\|\\mathbf{O}_{y}^{-1}\\|^{2}\\|\\mathbf{A}_{y a}\\|^{2}}{1-\\|\\mathbf{r}_{y}\\|}\\right),\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "where $\\lesssim_{K}$ denotes the the asymptotic rate when $K\\rightarrow\\infty$ ", "page_idx": 57}, {"type": "text", "text": "Then using (36) and the definition of $\\Delta_{k}$ ,weget ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\frac{1}{K}\\displaystyle\\sum_{k=0}^{K}\\mathbb{E}\\left[\\frac{\\|\\mathbf{x}^{k}-\\bar{\\mathbf{x}}^{k}\\|^{2}}{n}+\\frac{\\|\\mathbf{y}^{k}-\\bar{\\mathbf{y}}^{k}\\|^{2}}{n}\\right]}\\\\ &{\\displaystyle\\lesssim_{K}\\!\\frac{n}{K}\\left(\\frac{\\|\\mathbf{O}_{z}\\|^{2}\\|\\mathbf{O}_{z}^{-1}\\|^{2}\\|\\mathbf{A}_{z a}\\|^{2}}{1-\\|\\mathbf{r}_{z}\\|}+\\frac{\\|\\mathbf{O}_{y}\\|^{2}\\|\\mathbf{O}_{y}^{-1}\\|^{2}\\|\\mathbf{A}_{y a}\\|^{2}}{1-\\|\\mathbf{r}_{y}\\|}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "In particular, the corresponding result of SPARKLE variants that using EXTRA, ED or GT is ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\frac{1}{K}\\sum_{k=0}^{K}\\mathbb{E}\\left[\\frac{\\|\\mathbf{x}^{k}-\\bar{\\mathbf{x}}^{k}\\|^{2}}{n}+\\frac{\\|\\mathbf{y}^{k}-\\bar{\\mathbf{y}}^{k}\\|^{2}}{n}\\right]\\lesssim_{K}\\frac{n}{K}\\left(\\frac{1}{1-\\rho_{y}}+\\frac{1}{1-\\rho_{z}}\\right),\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "where $\\rho_{y},\\rho_{z}$ are spectrum gaps of relevant mixing matrices. ", "page_idx": 58}, {"type": "text", "text": "C.2.2 Essential matrix norms for analysis ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Common heterogeneity-correction algorithms, including ED, EXTRA and GT, satisfy Assumption 3, according to transformations (31), (32) and discussions in [2, Appendix B.2]. Then Lemma 3 ensures that $\\|\\mathbf{T}\\|<1$ . From Lemma 18, the transient time complexity depends on the coefficients $\\|\\mathbf{O}\\|^{2},\\|\\mathbf{O}^{-1}\\|^{2}$ $\\|\\mathbf{A}_{a}\\|^{2},\\|\\mathbf{A}_{b}^{-1}\\|^{2}$ , and $\\|\\mathbf{T}\\|^{2}$ . The solution of these matrices is constructive. Table 4 presents the upper bounds of these coefficients with different communication modes. Please refer to [2, Appendix B.2] for more details about the construction of these matrices and the computation of relevant norms. It is required that $W$ is positive definite for ED, EXTRA, and we denote the smallest nonzero eigenvalue of $W$ by $\\rho$ $\\rho$ can view as a constant. Otherwise we replace W with $t\\mathbf{I}+(1-t)\\mathbf{W}$ for some constant $t\\in(0,1)$ (e.g. $t=1/2$ ", "page_idx": 58}, {"type": "text", "text": "Substitutingvaluesof $\\lVert\\mathbf{O}_{s}\\rVert,\\lVert\\mathbf{O}_{s}^{-1}\\rVert,\\lVert\\mathbf{A}_{s a}\\rVert,\\lVert\\mathbf{A}_{s b}^{-1}\\rVert,\\lVert\\mathbf{D}_{s}\\rVert$ into (90) , we obtain the explicit transient iteration complexity for some specific examples of Algorithm 1, which are listed in Table 2. Note that all GT variants exhibit the same transient iteration complexity. ", "page_idx": 58}, {"type": "table", "img_path": "g5DyqerUpX/tmp/fa9b8f71f61d4813da2e113c18687d1ec9d2cbc3033653f507829d6217396e00.jpg", "table_caption": ["Table 4: Upper bounds of coefficients for different heterogeneity-correction modes in Lemma 18, wherenotation $\\scriptscriptstyle\\mathcal{O}$ is omitted for $\\lVert\\bf O\\rVert$ and $\\lVert\\mathbf O^{-1}\\rVert$ "], "table_footnote": [], "page_idx": 58}, {"type": "text", "text": "C.2.3  Theoretical gap between upper-level and lower-level ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Note that $\\|\\mathbf{A}_{s a}\\|\\leq1$ . We rewrite the upper bound of the transient iteration complexity in Lemma 18 as ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\mathrm{max}\\{n^{3}\\delta_{y},n^{3}\\delta_{z},n^{2}\\delta_{x},n\\hat{\\delta}_{y},n\\hat{\\delta}_{z},n\\hat{\\delta}_{x}\\}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "where ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\delta_{y}=\\left(\\frac{\\|\\mathbf{O}_{y}\\|^{2}\\|\\mathbf{O}_{y}^{-1}\\|^{2}}{1-\\|\\mathbf{I}_{y}\\|}\\right)^{2}\\|\\mathbf{A}_{y a}\\|^{2},\\delta_{z}=\\left(\\frac{\\|\\mathbf{O}_{z}\\|^{2}\\|\\mathbf{O}_{z}^{-1}\\|^{2}}{1-\\|\\mathbf{I}_{z}\\|}\\right)^{2}\\|\\mathbf{A}_{z a}\\|^{2},\\delta_{x}=\\left(\\frac{\\|\\mathbf{O}_{x}\\|\\|\\mathbf{O}_{x}^{-1}\\|}{1-\\|\\mathbf{I}_{z}\\|}\\|\\mathbf{A}_{z a}\\|\\right)^{2},}\\\\ &{\\hat{\\delta}_{y}=\\left(\\frac{\\|\\mathbf{O}_{y}\\|\\|\\mathbf{O}_{y}^{-1}\\|\\|\\mathbf{A}_{y b}^{-1}\\|}{1-\\|\\mathbf{I}_{y}\\|}\\right)^{\\frac{4}{3}},\\hat{\\delta}_{z}=\\left(\\frac{\\|\\mathbf{O}_{z}\\|\\|\\mathbf{O}_{z}^{-1}\\|\\|\\mathbf{A}_{z b}^{-1}\\|}{1-\\|\\mathbf{I}_{z}\\|}\\right)^{\\frac{4}{3}},}\\\\ &{\\hat{\\delta}_{x}=\\left(\\frac{\\|\\mathbf{O}_{x}\\|^{2}\\|\\mathbf{O}_{x}^{-1}\\|^{2}\\|\\mathbf{A}_{x b}^{-1}\\|^{2}}{1-\\|\\mathbf{I}_{x}\\|}\\right)^{\\frac{2}{3}}+\\frac{\\|\\mathbf{O}_{x}\\|\\|\\mathbf{O}_{x}^{-1}\\|\\|\\mathbf{A}_{x b}^{-1}\\|}{1-\\|\\mathbf{I}_{x}\\|}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "Suppose that we use the same communication matrices and heterogeneity-correction methods for updating $x,y,z$ i.e. ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathbf{O}_{x}\\|=\\|\\mathbf{O}_{y}\\|=\\|\\mathbf{O}_{z}\\|,\\|\\mathbf{O}_{x}^{-1}\\|=\\|\\mathbf{O}_{y}^{-1}\\|=\\|\\mathbf{O}_{z}^{-1}\\|,\\|\\mathbf{r}_{x}\\|=\\|\\mathbf{r}_{y}\\|=\\|\\mathbf{r}_{z}\\|,}\\\\ &{\\|\\mathbf{A}_{x a}\\|=\\|\\mathbf{A}_{y a}\\|=\\|\\mathbf{A}_{z a}\\|,\\|\\mathbf{A}_{x b}^{-1}\\|=\\|\\mathbf{A}_{y b}^{-1}\\|=\\|\\mathbf{A}_{z b}^{-1}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "equation", "text": "$$\n\\delta_{x}\\lesssim\\delta_{y}=\\delta_{z},\\,\\hat{\\delta}_{x}\\lesssim\\hat{\\delta}_{y}=\\hat{\\delta}_{z},\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Now we fix the update strategies for $y,z$ (94) implies that we can appropriately increase $\\delta_{x},\\hat{\\delta}_{x}$ while keeping the transient iteration complexity (92) unchanged (at most scaled by a constant factor). For example, we can use a moderately sparser communication network for updating $x$ than $y,z$ We illustrate this point with three examples: SPARKLE-ED, SPARKLE-EXTRA and SPARKLE-GT (variants), where $y,z$ share the same communication matrix $\\mathbf{W}_{y}$ ", "page_idx": 59}, {"type": "text", "text": "\u00b7 SPARKLE-ED, SPARKLE-EXTRA: From Table 4, we have ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\delta_{x}=\\mathcal{O}\\left((1-\\rho(\\mathbf{W}_{x}))^{-2}\\right),\\delta_{y}=\\delta_{z}=\\mathcal{O}\\left((1-\\rho(\\mathbf{W}_{y}))^{-2}\\right),}\\\\ &{\\hat{\\delta}_{x}=\\mathcal{O}\\left((1-\\rho(\\mathbf{W}_{x}))^{-\\frac{3}{2}}\\right),\\hat{\\delta}_{y}=\\hat{\\delta}_{z}=\\mathcal{O}\\left((1-\\rho(\\mathbf{W}_{y}))^{-2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Substituting these values into (92), we get the transient iteration complexity is bounded by ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\left\\lbrace n^{2}(1-\\rho(\\mathbf{W}_{x}))^{-2},n^{3}(1-\\rho(\\mathbf{W}_{y}))^{-2}\\right\\rbrace\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "SPARKLE-ED will keep the transient iteration complexity $n^{3}(1-\\rho(\\mathbf{W}_{y}))^{-2}$ (the dominated term) if ", "page_idx": 59}, {"type": "equation", "text": "$$\n(1-\\rho(\\mathbf{W}_{x}))^{-1}\\lesssim\\sqrt{n}(1-\\rho(\\mathbf{W}_{y}))^{-1}.\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "\u00b7 SPARKLE-GT variants: Results in Table 4 imply that ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\delta_{x}=\\mathcal{O}\\left((1-\\rho(\\mathbf{W}_{x}))^{-2}\\right),\\delta_{y}=\\delta_{z}=\\mathcal{O}\\left((1-\\rho(\\mathbf{W}_{y}))^{-2}\\right),}\\\\ &{\\hat{\\delta}_{x}=\\mathcal{O}\\left((1-\\rho(\\mathbf{W}_{x}))^{-2}\\right),\\hat{\\delta}_{y}=\\hat{\\delta}_{z}=\\mathcal{O}\\left((1-\\rho(\\mathbf{W}_{y}))^{-\\frac{8}{3}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Following the same argument as before, we have the following upper bound of the transient iteration complexity of SPARKLE-GT ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\left\\{n^{2}(1-\\rho(\\mathbf{W}_{x}))^{-2},n^{3}(1-\\rho(\\mathbf{W}_{y}))^{-2},n(1-\\rho(\\mathbf{W}_{y}))^{-\\frac{8}{3}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "we get the constraints of the spectral gap $1-\\rho(\\mathbf{W}_{x})$ that maintains the transient iteration complexity max $\\left\\{{n^{3}(1-\\rho(\\mathbf W_{y}))^{-2}},{n(1-\\rho(\\mathbf W_{y}))^{-{\\frac{8}{3}}}}\\right\\}$ ", "page_idx": 59}, {"type": "equation", "text": "$$\n(1-\\rho(\\mathbf{W}_{x}))^{-1}\\lesssim\\operatorname*{max}\\left\\{\\sqrt{n}(1-\\rho(\\mathbf{W}_{y}))^{-1},n^{-1/2}(1-\\rho(\\mathbf{W}_{y}))^{-\\frac{4}{3}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Denote the communication times per agent of $\\mathbf{W}_{x},\\mathbf{W}_{y}$ by $c_{x},c_{y}$ respectively. For example, we have $c_{x}=2$ $c_{y}=n-1$ when taking Ring Graph for $x$ (i.e. $[\\mathbf{W}_{x}]_{i j}\\neq0$ iff $|i-\\dot{j}|\\in\\{0,1,\\bar{n}-1\\}$ ), and Complete Graph for $y$ (i.e. $\\begin{array}{r}{\\mathbf{W}_{y}=\\frac{1}{n}\\mathbf{1}_{n}\\mathbf{1}_{n}^{\\top},}\\end{array}$ ", "page_idx": 59}, {"type": "text", "text": "Then for each agent, the communication cost per round is $\\mathcal{O}(c_{x}p+c_{y}q)$ . If we take $a=c_{x}/c_{y}$ to measure the relative sparsity of the two communication matrices, and consider $c_{y}=\\mathcal{O}(1)$ , then for each agent, the communication cost per round is $\\mathcal{O}(a p+q)$ . (95) and (96) theoretically provide the range of the sparsity (connectivity) degree of $\\mathbf{W}_{x}$ relative to $\\mathbf{W}_{y}$ . From (95) and (96), we can set $a\\ll1$ , while maintaining the transient iteration complexity for SPARKLE-GT, SPARKLE-ED, SPARKLE-EXTRA. ", "page_idx": 59}, {"type": "text", "text": "C.2.4  The transient iteration complexities of some specific examples in SPARKLE. ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "Now we compute the transient iteration complexities of each SPARKLE-L-U algorithm, where $\\mathbf{L},\\mathbf{U}\\in\\{\\mathrm{GT}$ (variants), ED, EXTRA}. For brevity, here we assume that $\\mathbf{W}_{x}=\\mathbf{W}_{y}=\\mathbf{W}_{z}$ usethe same heterogeneity-correction method to $y,z$ , and denote the spectral gap $1-\\rho(\\bar{\\mathbf{W}}_{x})$ by $1-\\rho$ ", "page_idx": 59}, {"type": "text", "text": "Substituting the results in Table 4 into (92) and (93), we get ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\delta_{x}=\\mathcal{O}\\left(\\frac{1}{(1-\\rho)^{2}}\\right),\\delta_{y}=\\delta_{z}=\\mathcal{O}\\left(\\frac{1}{(1-\\rho)^{2}}\\right)\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "for any $\\mathbf{L},\\mathbf{U}\\in\\{\\mathrm{GT}$ (variants), ED, EXTRA}, ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\hat{\\delta}_{x}=\\mathcal{O}\\left(\\frac{1}{(1-\\rho)^{2}}\\right),\\mathcal{O}\\left(\\frac{1}{(1-\\rho)^{3/2}}\\right),\\mathcal{O}\\left(\\frac{1}{(1-\\rho)^{3/2}}\\right)\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "for $\\mathbf{U}=\\{\\mathbf{G}\\mathbf{T}$ (variants), ED, EXTRA} respectively, and ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\hat{\\delta}_{y}=\\hat{\\delta}_{z}=\\mathcal{O}\\left(\\frac{1}{(1-\\rho)^{8/3}}\\right),\\mathcal{O}\\left(\\frac{1}{(1-\\rho)^{2}}\\right),\\mathcal{O}\\left(\\frac{1}{(1-\\rho)^{2}}\\right)\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "for $\\mathbf{L}=\\{\\mathrm{GT\\,(variants),ED,EXTRA}\\}$ respectively. ", "page_idx": 60}, {"type": "text", "text": "Combining the above results, we can directly obtain Table 2, the transient iteration complexities of SPARKLE with mixed heterogeneity-correction techniques in different levels. ", "page_idx": 60}, {"type": "text", "text": "C.3  Convergence analysis in deterministic scenarios ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "The following lemma gives the convergence rate of Algorithm 1 without a moving average when there is no sample noise: ", "page_idx": 60}, {"type": "text", "text": "Lemma 20. Suppose that Assumptions 1- 4 hold. If $\\sigma^{2}=0$ thenthere exist $\\alpha,\\beta,\\gamma$ and $\\theta=1$ such that ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\frac{1}{K+1}\\sum_{k=0}^{K}\\mathbb{E}\\|\\Phi(\\bar{x}^{k})\\|^{2}}\\\\ &{\\le\\left(\\frac{\\kappa^{16}\\|\\mathbf{O}_{y}\\|^{2}\\|\\mathbf{O}_{y}^{-1}\\|^{2}\\|\\mathbf{A}_{y a}\\|^{2}\\|\\mathbf{A}_{y b}^{-1}\\|^{2}\\zeta_{0}^{y}\\right)^{\\frac{1}{3}}\\displaystyle\\frac{1}{K}+\\left(\\frac{\\kappa^{14}\\|\\mathbf{O}_{z}\\|^{2}\\|\\mathbf{O}_{z}^{-1}\\|^{2}\\|\\mathbf{A}_{z a}\\|^{2}\\|\\mathbf{A}_{z b}^{-1}\\|^{2}\\zeta_{0}^{z}\\right)^{\\frac{1}{3}}\\displaystyle\\frac{1}{K}}{1-\\|\\mathbf{C}_{y}\\|}}\\\\ &{\\quad+\\left(\\frac{\\kappa^{8}\\|\\mathbf{O}_{x}\\|^{2}\\|\\mathbf{O}_{x}^{-1}\\|^{2}\\|\\mathbf{A}_{x a}\\|^{2}\\|\\mathbf{A}_{x b}^{-1}\\|^{2}\\zeta_{0}^{x}\\right)^{\\frac{1}{3}}\\displaystyle\\frac{1}{K}+\\widetilde{C}_{\\alpha}\\frac{1}{K}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "where $\\widetilde{C}_{\\alpha}$ is a series of overheads which is defined below. ", "page_idx": 60}, {"type": "text", "text": "Proof. Note that $\\sigma^{2}=0$ implies that $L_{1}=\\Theta(L^{2})$ when $\\alpha=\\mathcal{O}(L_{\\nabla\\Phi}^{-1})$ . Thus (87) implies that: ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\frac{1}{K+1}\\sum_{k=0}^{K}\\mathbb{E}\\|\\Phi(\\bar{x}^{k})\\|^{2}}\\\\ &{\\lesssim\\!\\frac{\\Phi(\\bar{x}_{0})-\\operatorname{inf}\\Phi}{\\alpha(K+1)}}\\\\ &{\\quad+\\displaystyle L^{2}\\left[\\kappa^{4}(\\eta_{1}+\\kappa^{2}L_{y}^{2}\\cdot\\eta_{2})\\alpha^{2}+\\left(\\frac{\\alpha^{2}L_{z^{*}}^{2}}{\\gamma^{2}\\mu_{g}^{2}}+\\frac{\\kappa^{4}\\alpha^{2}}{\\beta^{2}\\mu_{g}^{2}}L_{y^{*}}^{2}\\right)\\right]\\left(\\frac{\\Phi(\\bar{x}_{0})-\\operatorname{inf}\\Phi}{\\alpha(K+1)}\\right)}\\\\ &{\\quad+\\displaystyle L^{2}\\frac{\\kappa^{6}\\|\\mathbf{O}_{y}\\|^{2}\\mathbb{E}\\|\\bar{\\mathbf{e}}_{y}^{0}\\|^{2}}{n(K+1)(1-\\|\\mathbf{I}_{y}\\|)}+L^{2}\\frac{\\kappa^{4}\\|\\mathbf{O}_{z}\\|^{2}\\mathbb{E}\\|\\bar{\\mathbf{e}}_{z}^{0}\\|^{2}}{n(K+1)(1-\\|\\mathbf{I}_{z}\\|)}+L^{2}\\frac{\\kappa^{6}\\|\\mathbf{O}_{z}\\|^{2}\\mathbb{E}\\|\\bar{\\mathbf{e}}_{x}^{0}\\|^{2}}{n(K+1)(1-\\|\\mathbf{I}_{x}\\|)}}\\\\ &{\\quad+\\displaystyle L^{2}\\frac{\\|\\bar{x}_{*}^{1}\\|^{2}}{\\mu_{g}\\gamma(K+1)}+\\frac{L^{2}\\kappa^{4}}{K+1}\\frac{1}{\\beta\\mu_{g}}\\|\\bar{y}_{0}-y^{*}(\\bar{x}^{0})\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "Then we aim to choose the stepsize $\\alpha,\\beta,\\gamma$ . Define: ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{C}_{\\alpha}=L_{\\nabla^{\\Phi}}+\\kappa^{\\frac{1}{\\alpha}}\\frac{\\|\\mathbf{O}_{\\alpha}\\|\\|\\mathbf{O}_{\\alpha}^{-1}\\|\\|\\mathbf{A}_{\\alpha}\\|L}{1-\\|\\mathbf{O}_{\\alpha}\\|}+\\kappa^{\\frac{1}{\\alpha}}L\\left(\\frac{\\|\\mathbf{O}_{\\alpha}\\|\\|\\mathbf{O}_{\\alpha}^{-1}\\|\\|\\mathbf{A}_{\\alpha}\\|\\|\\mathbf{A}_{\\alpha}^{-\\frac{1}{\\alpha}}\\|}{1-\\|\\mathbf{O}_{\\alpha}\\|}\\right)^{\\frac{1}{\\alpha}}}\\\\ &{\\qquad+\\kappa^{\\frac{1}{2\\alpha}}\\frac{L_{\\nabla^{2},1}^{2}}{\\mu_{0}}+\\kappa^{\\frac{4}{\\alpha}}\\frac{\\|\\mathbf{O}_{\\alpha}\\|\\|\\mathbf{O}_{\\alpha}^{-1}\\|\\|\\mathbf{A}_{\\alpha}\\|\\|_{\\mathcal{H}_{1}}}{1-\\|\\mathbf{O}_{\\alpha}\\|}+\\kappa^{4}L_{\\mathcal{O}_{\\alpha}}\\left(\\frac{\\kappa\\|\\mathbf{O}_{\\alpha}\\|\\|\\mathbf{O}_{\\alpha}^{-1}\\|\\|\\mathbf{A}_{\\alpha}\\|\\|\\mathbf{A}_{\\alpha}^{-\\frac{1}{\\alpha}}\\|}{1-\\|\\mathbf{O}_{\\alpha}\\|}\\right)^{\\frac{1}{\\alpha}}}\\\\ &{\\qquad\\qquad+\\kappa^{6}L\\frac{\\|\\mathbf{O}_{\\alpha}\\|\\|\\mathbf{O}_{\\alpha}^{-1}\\|\\|\\mathbf{A}_{\\alpha}\\|}{1-\\|\\mathbf{O}_{\\alpha}\\|}+\\kappa^{\\frac{1}{\\alpha}}L\\left(\\frac{\\|\\mathbf{O}_{\\alpha}\\|\\|\\mathbf{O}_{\\alpha}^{-1}\\|\\|\\mathbf{A}_{\\alpha}\\|\\|\\mathbf{A}_{\\alpha}^{-\\frac{1}{\\alpha}}\\|}{1-\\|\\mathbf{O}_{\\alpha}\\|}\\right)^{\\frac{1}{\\alpha}},}\\\\ &{\\widetilde{C}_{\\tilde{\\alpha}\\beta,2}=\\left(\\frac{1-\\|\\mathbf{O}_{\\alpha}\\|}{\\kappa^{3}\\|\\mathbf{O}_{\\alpha}\\|^{2}\\|\\mathbf{O}_{\\alpha}^{-1}\\|\\|\\mathbf{O}_{\\alpha}\\|^{2}\\|\\mathbf{A}_{\\alpha}^{-\\frac{1}{\\alpha}}\\|\\|\\mathcal{C}_{ \n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "Then there exist ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\alpha=\\Theta\\left(\\widetilde{C}_{\\alpha}+\\widetilde{\\alpha}_{x b,2}^{-1}+\\widetilde{\\alpha}_{y b,2}^{-1}+\\widetilde{\\alpha}_{z b,2}^{-1}\\right)^{-1},\\beta=\\Theta\\left(\\kappa^{4}\\alpha\\right),\\gamma=\\Theta\\left(\\kappa^{4}\\alpha\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "such that (45), (53), (56), (40), (58), (65), (82), and (84) hold. Then all previous lemmas hold. Then from (97) we have: ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\frac{1}{K+1}\\displaystyle\\sum_{k=0}^{K}\\mathbb{E}\\|\\Phi(\\bar{x}^{k})\\|^{2}}\\\\ &{\\lesssim_{\\bar{\\alpha}\\bar{K}}+\\frac{\\alpha^{2}\\mathsf{A}^{1/2}\\|\\mathbf{O}_{y}\\|^{2}\\|\\mathbf{O}_{y}^{-1}\\|^{2}\\|\\mathbf{A}_{y\\parallel}\\|^{2}\\|\\mathbf{A}_{y\\parallel}^{-1}\\|^{2}\\zeta_{0}^{y}}{K(1-\\|\\mathbf{I}_{y}\\|)}}\\\\ &{\\quad+\\frac{\\alpha^{2}\\mathsf{A}^{1/2}\\|\\mathbf{O}_{z}\\|^{2}\\|\\mathbf{O}_{x}^{-1}\\|^{2}\\|\\mathbf{A}_{z\\parallel}\\|^{2}\\|\\mathbf{A}_{x z}\\|^{2}\\|\\zeta_{0}^{-1}}{K(1-\\|\\mathbf{I}_{z}\\|)}+\\frac{\\alpha^{2}\\kappa^{6}\\|\\mathbf{O}_{x}\\|^{2}\\|\\mathbf{O}_{x}^{-1}\\|^{2}\\|\\mathbf{A}_{x z}\\|^{2}\\|\\mathbf{A}_{x\\parallel}^{-1}\\|^{2}\\zeta_{0}^{x}}{K(1-\\|\\mathbf{I}_{x z}\\|)}}\\\\ &{\\lesssim\\left(\\frac{\\kappa^{16}\\|\\mathbf{O}_{y}\\|^{2}\\|\\mathbf{O}_{y}^{-1}\\|^{2}\\|\\mathbf{A}_{y\\parallel}\\|^{2}\\|\\mathbf{A}_{y\\parallel}^{-1}\\|^{2}\\zeta_{0}^{y}\\right)^{\\frac{1}{3}}\\displaystyle\\frac{1}{K}+\\left(\\frac{\\kappa^{14}\\|\\mathbf{O}_{z}\\|^{2}\\|\\mathbf{O}_{z}^{-1}\\|^{2}\\|\\mathbf{A}_{z\\parallel}\\|^{2}\\|\\mathbf{A}_{z\\parallel}^{-1}\\|^{2}\\zeta_{0}^{z}\\right)^{\\frac{1}{3}}\\displaystyle\\frac{1}{K}}\\\\ &{\\quad+\\left(\\frac{\\kappa^{16}\\|\\mathbf{O}_{z}\\|^{2}\\|\\mathbf{O}_{x}^{-1}\\|^{2}\\|\\mathbf{A}_{x z}\\|^{2}\\|\\mathbf{A}_{y\\parallel}^{-1}\\|^{2}\\zeta_{0}^{x}\\right)^{\\frac{1}{3}}\\displaystyle\\frac{1}{K}+\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "C.4  Degenerating to single-level algorithms ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "We consider the bilevel problem with the following upper- and lower-level loss function on the $i$ -th agent: ", "page_idx": 61}, {"type": "equation", "text": "$$\nF_{i}(x,y,\\phi)=F_{i}(x,\\phi),\\quad G_{i}(x,y,\\xi)\\equiv\\frac{\\|y\\|^{2}}{2}.\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "Actually, this optimization problem with respect to $x$ is single-level, since we have $\\mathbf{z}^{k}\\equiv0$ $\\mathbf{y}^{k}\\equiv0$ $u_{i}^{k}\\,=\\,\\dot{\\nabla}_{1}f_{i}(x_{i}^{k},\\xi_{i}^{k})$ by induction. By taking $\\theta=1$ we get the following single-level algorithm framework for decentralized stochastic single-level algorithm. As we discuss in previous sections, it can recover various heterogeneity-correction algorithms, including GT, EXTRA and ED, by selecting specific ${\\bf A}_{x},{\\bf B}_{x},{\\bf C}_{x}$ ", "page_idx": 61}, {"type": "text", "text": "Algorithm 3 SPARKLE: degenerating to single-level decentralized stochastic algorithms ", "page_idx": 62}, {"type": "text", "text": "Require: Initialize $\\mathbf{x}^{0}=\\mathbf{0}$ $\\mathbf{d}_{x}^{0}=\\mathbf{0}$ , learning rate $\\alpha_{k}$ for $k=0,1,\\cdots,K-1$ do $\\mathbf{x}^{k+1}=\\mathbf{C}_{x}\\mathbf{x}^{k}-\\alpha_{k}\\mathbf{A}_{x}\\mathbf{u}^{k}-\\mathbf{B}_{x}\\mathbf{d}_{x}^{k},\\;\\;\\mathbf{d}_{x}^{k+1}=\\mathbf{d}_{x}^{k}+\\mathbf{B}_{x}\\mathbf{x}^{k+1};$ end for ", "page_idx": 62}, {"type": "text", "text": "In this case, we have $z_{k}^{\\star}\\equiv0$ $y_{k}^{\\star}\\equiv0$ . Notice that $L_{y^{\\star}}=0$ \uff0c $L_{z^{\\star}}=0$ It gives ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\eta_{2}=\\mathcal{O}\\left(\\beta^{2}\\frac{\\|\\mathbf{O}_{y}\\|^{2}\\|\\mathbf{O}_{y}^{-1}\\|^{2}\\|\\mathbf{A}_{y a}\\|^{2}\\|\\mathbf{A}_{y b}^{-1}\\|^{2}}{(1-\\|\\mathbf{r}_{y}\\|)^{2}}+\\gamma^{2}\\frac{\\|\\mathbf{O}_{z}\\|^{2}\\|\\mathbf{O}_{z}^{-1}\\|^{2}\\|\\mathbf{A}_{z a}\\|^{2}\\|\\mathbf{A}_{z b}^{-1}\\|^{2}}{(1-\\|\\mathbf{r}_{z}\\|)^{2}}\\right),}\\\\ &{\\eta_{1}=\\mathcal{O}\\left(\\eta_{2}+\\alpha^{2}\\left(1+\\frac{(1-\\theta)^{2}}{\\theta^{2}\\|\\mathbf{A}_{x b}^{-1}\\|^{2}}\\right)\\frac{\\|\\mathbf{O}_{x}\\|^{2}\\|\\mathbf{O}_{x}^{-1}\\|^{2}\\|\\mathbf{A}_{x a}\\|^{2}\\|\\mathbf{A}_{x b}^{-1}\\|^{2}}{(1-\\|\\mathbf{r}_{x}\\|)^{2}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "If we take ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\alpha\\lesssim\\operatorname*{min}\\left\\{1,\\frac{1-\\|\\Gamma_{x}\\|}{\\|\\mathbf{O}_{x}\\|\\|\\mathbf{O}_{x}^{-1}\\|\\|\\mathbf{A}_{x a}\\|},\\left(\\frac{1-\\|\\Gamma_{x}\\|}{\\|\\mathbf{O}_{x}\\|\\|\\mathbf{O}_{x}^{-1}\\|\\|\\mathbf{A}_{x a}\\|\\|\\mathbf{A}_{x b}^{-1}\\|}\\right)^{\\frac{1}{2}}\\right\\}\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "and $\\theta\\,=\\,1,\\,\\beta\\,\\to\\,0,\\,\\gamma\\,\\to\\,0$ ,then (45), (53), (56), (40), (58), (65), (82), and (84) hold. Thus all previous lemmas hold. Then (87) transforms into ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\frac{1}{K+1}\\displaystyle\\sum_{k=0}^{K}\\mathbb{E}\\|\\Phi(\\bar{x}^{k})\\|^{2}}\\\\ &{\\lesssim\\displaystyle\\frac{f(\\bar{x}_{0})-\\operatorname*{inf}f}{\\alpha(K+1)}+\\frac{1}{n}\\left(\\theta(1-\\theta)+\\alpha\\theta^{2}\\right)\\sigma_{f,1}^{2}+\\frac{(1-\\theta)^{2}}{\\theta(K+1)}\\|\\nabla f\\left(\\bar{x}^{0}\\right)\\|^{2}}\\\\ &{\\quad+\\eta_{1}\\alpha^{2}\\left(\\frac{f(\\bar{x}_{0})-\\operatorname*{inf}f}{\\alpha(K+1)}+\\frac{\\theta}{n}\\sigma_{f,1}^{2}\\right)+\\displaystyle\\frac{\\|\\mathbf{0}_{x}\\|^{2}\\mathbb{E}\\|\\bar{\\mathbf{e}}^{\\mathbb{E}}\\|^{2}}{n(K+1)(1-\\|\\mathbf{r}_{x}\\|)}}\\\\ &{\\quad+\\displaystyle\\frac{\\alpha^{2}\\|\\mathbf{0}_{x}\\|^{2}\\|\\mathbf{0}_{x}^{-1}\\|^{2}\\|\\mathbf{A}_{x}\\|^{2}}{n(1-\\|\\mathbf{r}_{x}\\|)^{2}(K+1)}\\left[\\frac{1-\\theta}{\\theta}\\sum_{i=1}^{n}\\|\\nabla f_{i}(\\bar{x}^{0})\\|^{2}\\right]}\\\\ &{\\quad+\\displaystyle\\frac{1}{n}\\left[\\alpha^{2}\\theta\\left(\\theta+\\frac{1-\\theta}{1-\\|\\mathbf{r}_{x}\\|}\\right)\\frac{\\|\\mathbf{0}_{x}\\|^{2}\\|\\mathbf{0}_{x}^{-1}\\|^{2}\\|\\mathbf{A}_{x}\\|^{2}n_{u}\\right]}{1-\\|\\mathbf{r}_{x}\\|}\\sigma_{f,1}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "It follows that ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\frac{1}{K+1}\\displaystyle\\sum_{k=0}^{K}\\mathbb{E}\\|\\Phi(\\bar{x}^{k})\\|^{2}}\\\\ &{\\lesssim\\!\\frac{f(\\bar{x}_{0})-\\operatorname*{inf}f}{\\alpha(K+1)}+\\frac{\\alpha\\sigma_{f,1}^{2}}{n}+\\left(\\alpha^{4}\\displaystyle\\frac{\\|\\mathbf{O}_{x}\\|^{2}\\|\\mathbf{O}_{x}^{-1}\\|^{2}\\|\\mathbf{A}_{x}\\|^{2}\\|\\mathbf{A}_{x}^{-1}\\|^{2}}{(1-\\|\\mathbf{O}_{x}\\|)^{2}}\\right)\\left(\\frac{f(\\bar{x}_{0})-\\operatorname*{inf}f}{\\alpha(K+1)}+\\frac{1}{n}\\sigma_{f,1}^{2}\\right)}\\\\ &{\\quad+\\frac{\\|\\mathbf{O}_{x}\\|^{2}\\mathbb{E}\\|\\mathbf{S}_{x}^{\\perp}\\|^{2}}{n(K+1)(1-\\|\\mathbf{O}_{x}\\|)}+\\alpha^{2}\\displaystyle\\frac{\\|\\mathbf{O}_{x}\\|^{2}\\|\\mathbf{O}_{x}\\|^{2}\\|\\mathbf{A}_{x}\\|^{2}}{1-\\|\\mathbf{C}_{x}\\|}\\sigma_{f,1}^{2}}\\\\ &{\\lesssim\\!\\frac{f(\\bar{x}_{0})-\\operatorname*{inf}f}{\\alpha(K+1)}+\\frac{\\alpha\\sigma_{f,1}^{2}}{n}+\\alpha^{2}\\displaystyle\\frac{\\|\\mathbf{O}_{x}\\|^{2}\\|\\mathbf{O}_{x}^{-1}\\|^{2}\\|\\mathbf{A}_{x}\\|^{2}}{1-\\|\\mathbf{I}_{x}\\|}\\sigma_{f,1}^{2}}\\\\ &{\\quad+\\frac{\\alpha^{2}\\|\\mathbf{O}_{x}\\|^{2}\\|\\mathbf{O}_{x}^{-1}\\|^{2}\\|\\mathbf{A}_{x}\\|^{2}\\|\\mathbf{A}_{x}^{-1}\\|^{2}\\zeta_{0}^{\\varepsilon}}{(K+1)(1-\\|\\mathbf{I}_{x}\\|)}+\\frac{\\alpha^{4}\\|\\mathbf{O}\\|_{x}^{2}\\|\\mathbf{O}_{x}^{-1}\\|^{2}\\|\\mathbf{A}_{x}\\|^{2}\\|\\mathbf{A}_{x}^{-1}\\|^{2}\\sigma_{f,1}^{2}}{n(1-\\|\\mathbf{I \n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "Like (88), we take ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{C_{0}=1+\\displaystyle\\frac{\\|\\mathbf{O}_{x}\\|\\|\\mathbf{O}_{x}^{-1}\\|\\|\\mathbf{A}_{x0}\\|}{1-\\|\\mathbf{I}_{x}\\|}+\\left(\\frac{\\|\\mathbf{O}_{x}\\|\\|\\mathbf{O}_{x}^{-1}\\|\\|\\mathbf{A}_{x0}\\|\\|\\mathbf{A}_{x0}^{-1}\\|}{1-\\|\\mathbf{I}_{x}\\|}\\right)^{\\frac{1}{2}},}\\\\ &{\\alpha_{1}=\\displaystyle\\sqrt{\\frac{n}{K\\sigma_{f,1}^{2}}},\\quad\\alpha_{2}=\\left(\\frac{1-\\|\\mathbf{I}_{x}\\|}{K\\|\\mathbf{O}_{x}\\|^{2}\\|\\mathbf{O}_{x}^{-1}\\|^{2}\\|\\mathbf{A}_{x0}\\|^{2}\\sigma_{f,1}^{2}}\\right)^{\\frac{1}{3}},}\\\\ &{\\alpha_{3}=\\left(\\frac{1-\\|\\mathbf{I}_{x}\\|}{\\|\\mathbf{O}_{x}\\|^{2}\\|\\mathbf{O}_{x}^{-1}\\|^{2}\\|\\mathbf{A}_{x0}\\|^{2}\\|\\mathbf{A}_{x0}^{-1}\\|^{2}\\zeta_{0}^{\\alpha}}\\right)^{\\frac{1}{3}},}\\\\ &{\\alpha_{4}=\\left(\\frac{n(1-\\|\\mathbf{I}_{x}\\|)^{2}}{K\\|\\mathbf{O}\\|\\mathbf{O}_{x}^{2}\\|\\mathbf{O}_{x}^{-1}\\|^{2}\\|\\mathbf{A}_{x0}\\|^{2}\\|\\mathbf{A}_{x0}^{-1}\\|^{2}\\sigma_{f,1}^{2}}\\right)^{\\frac{1}{3}},}\\\\ &{\\alpha=\\Theta\\left(C_{0}+\\frac{1}{\\alpha_{1}}+\\frac{1}{\\alpha_{2}}+\\frac{1}{\\alpha_{3}}+\\frac{1}{\\alpha_{4}}\\right)^{-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "Substituting these values into (98), we get ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{K+1}\\sum_{k=0}^{K}\\mathbb{E}\\|\\Phi(\\bar{x}^{k})\\|^{2}\\lesssim\\frac{\\sigma_{f,1}}{\\sqrt{n K}}+\\left(\\frac{\\|\\mathbf{O}_{x}\\|^{2}\\|\\mathbf{O}_{x}^{-1}\\|^{2}\\|\\Lambda_{x a}\\|^{2}\\sigma_{f,1}^{2}}{1-\\|\\mathbf{I}_{x}\\|}\\right)^{\\frac{1}{3}}K^{-2/3}+\\frac{C_{0}}{K}}\\\\ &{+\\left(\\frac{\\|\\mathbf{O}_{x}\\|^{2}\\|\\mathbf{O}_{x}^{-1}\\|^{2}\\|\\Lambda_{x a}\\|^{2}\\|\\Lambda_{x b}^{-1}\\|^{2}\\zeta_{0}^{x}}{(1-\\|\\mathbf{I}_{x}\\|)}\\right)^{\\frac{1}{3}}\\frac{1}{K}+\\left(\\frac{\\|\\mathbf{O}\\|_{x}^{2}\\|\\mathbf{O}_{x}^{-1}\\|^{2}\\|\\Lambda_{x a}\\|^{2}\\|\\Lambda_{x b}^{-1}\\|^{2}\\sigma_{f,1}^{2}}{n(1-\\|\\mathbf{I}_{x}\\|)^{2}}\\right)^{\\frac{1}{5}}K^{-4/5}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "Like Lemma 18, we get the transient iterating complexity for Algorithm 3 is ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\left\\{n^{3}\\left(\\frac{\\|\\mathbf{O}_{x}\\|^{2}\\|\\mathbf{O}_{x}^{-1}\\|^{2}}{1-\\|\\mathbf{I}_{x}\\|}\\right)^{2}\\|\\mathbf{A}_{x a}\\|^{2},n\\left(\\frac{\\|\\mathbf{O}_{x}\\|\\|\\mathbf{O}_{x}^{-1}\\|\\|\\mathbf{A}_{x b}^{-1}\\|}{1-\\|\\mathbf{I}_{x}\\|}\\right)^{\\frac{4}{3}}\\|\\mathbf{A}_{x a}\\|,n\\right\\}.\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "Substituting the value of relevant norms in Table 4, we get the transient iteration complexity for GT, EXTRA, ED are ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\mathcal{O}\\left(\\operatorname*{max}\\left\\{\\frac{n^{3}}{(1-\\rho)^{2}},\\frac{n}{(1-\\rho)^{8/3}}\\right\\}\\right),\\,\\mathcal{O}\\left(\\frac{n^{3}}{(1-\\rho)^{2}}\\right),\\,\\mathcal{O}\\left(\\frac{n^{3}}{(1-\\rho)^{2}}\\right)\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "respectively,where $\\rho:=\\rho(\\mathbf{W}_{x})$ . These upper bounds are the same as the state-of-the-art results shown in Table 1. It indicates that our analysis accurately captures the impacts of updates at each level on the convergence results. ", "page_idx": 63}, {"type": "text", "text": "D  Experimental details ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "In this section, we provide the details of our numerical experiments discussed in Section 4. We also provide addition experimental results which are not mentioned in the main text due to the space limitation. For all GT variants, we focus on one typical representative, ATC-GT, in our experiments, which we denote as GT for brevity. All experiments described in this section were run on an NVIDIA A100server. ", "page_idx": 63}, {"type": "text", "text": "D.1 Synthetic bilevel optimization ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "Here, we consider problem (1) whose upper- and lower level loss functions on the $i$ -thagents $1\\leq i\\leq N)$ are denoted as: ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f_{i}(x,y)=\\mathbb{E}_{A_{i},b_{i}}\\left[\\left\\|A_{i}y-b_{i}\\right\\|^{2}\\right],}\\\\ &{g_{i}(x,y)=\\mathbb{E}_{A_{i},b_{i}}\\left[\\left\\|A_{i}y-x\\right\\|^{2}+C_{r}\\left\\|y\\right\\|^{2}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 63}, {"type": "image", "img_path": "g5DyqerUpX/tmp/806f9becad77bd24a8ba0723b855dc652ecd661e58371e14639e72d69249e6d8.jpg", "img_caption": ["Figure 4: The estimation error of D-SOBA, SPARKLE-GT, SPARKLE-ED, and SPARKLEEXTRA under different networks and data heterogeneity. "], "img_footnote": [], "page_idx": 64}, {"type": "text", "text": "where $\\boldsymbol{x}\\,\\in\\,\\mathbb{R}^{D},\\boldsymbol{y}\\,\\in\\,\\mathbb{R}^{K}$ and $C_{r}$ denotes a fixed regularization parameter. For each agent $i$ we firstly generate the local solution $y_{i}^{*},x_{i}^{*}$ as $y_{i}^{*}=y^{*}+\\zeta_{i}$ and $x_{i}^{*}=A^{*}b^{*}+\\xi_{i}$ ,where $\\boldsymbol{x}^{*}\\sim\\mathcal{N}(\\boldsymbol{0},\\boldsymbol{I}_{K})$ is a randomly generated vector, each element of $A^{*}$ is independently sampled from $\\mathcal{N}(0,9)$ . The observation $(A_{i},b_{i})$ on agent $i$ is generated in a streaming manner by $A_{i}=A^{*}+\\phi_{i}$ \uff0c $b_{i}=x_{i}^{*}+\\psi_{i}$ \uff0c in which each element of $\\phi_{i}\\in\\mathbb{R}^{K\\times D}$ and $\\psi_{i}\\in\\mathbb{R}^{D}$ are independently generated by $\\mathcal{N}(0,\\sigma_{g}^{2})$ The terms $\\xi_{i}\\sim\\mathcal{N}(0,\\sigma_{h}^{2}I_{K})$ and $\\zeta_{i}\\sim\\mathcal{N}(0,\\sigma_{h}^{2}I_{D})$ control the heterogeneity of data distributions across different agents. ", "page_idx": 64}, {"type": "text", "text": "We set $D\\,=\\,20,K\\,=\\,10,\\sigma_{g}\\,=\\,0.001,C_{r}\\,=\\,0.001$ .Then we set $\\sigma_{h}\\,=\\,0.5$ to represent severe heterogeneity across agents and $\\sigma_{h}=0.1$ for mild heterogeneity. We run D-SOBA, SPARKLE-GT, SPARKLE-ED, and SPARKLE-EXTRA over Ring, 2D-Torus [37], and fully connected networks with $N=64$ agents. The moving-average term $\\theta=0.1$ and the step-size at the $t$ -th iteration are $\\alpha_{t}=\\beta_{t}=\\gamma_{t}=1/(500+0.01t)$ . The batch size is 10. ", "page_idx": 64}, {"type": "text", "text": "Fig ilustratesthe averaged estimation eror $\\begin{array}{r}{\\sum_{i=1}^{N}\\left\\|x_{i}^{(t)}-x^{*}\\right\\|^{2}}\\end{array}$ of the mentioned algorithms with different communication topology and data heterogeneity. It is observed that SPARKLE with ED, EXTRA, GT achieve better convergence performances with decentralized communication networks. Meanwhile, SPARKLE-ED and SPARKLE-EXTRA are more robust to data heterogeneity and the sparsity of network topology than SPARKLE-GT. All the results are consistent with our theoretical results. ", "page_idx": 64}, {"type": "text", "text": "D.2 Hyper-cleaning on FashionMNIST dataset ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "Here, we consider a data hyper-clean problem [44] on FashionMNIST dataset [48]. The FashionMNIST dataset consists of 60000 images for training and 10000 images for testing and we randomly split 50000 training images into a training set and the other 10000 images into a validation set. ", "page_idx": 64}, {"type": "text", "text": "The data hyper-cleaning problem aims to train a classifier from a corrupted dataset, in which the label of each training data is replaced by a random class number with a probability $p$ (i.e. the corruption rate). It can be considered as a stochastic bilevel problem (1) whose upper- and lower-level loss functions on the $i$ -thagents $1\\leq i\\leq n$ are formulated as: ", "page_idx": 64}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle f_{i}({\\boldsymbol x},{\\boldsymbol y})=\\frac{1}{\\left|\\mathcal{D}_{v a l}^{(i)}\\right|}\\sum_{(\\xi_{e},\\zeta_{e})\\in{\\cal D}_{v a l}^{(i)}}{\\cal L}(\\phi(\\xi_{e};{\\boldsymbol y}),\\zeta_{e}),}}\\\\ {{\\displaystyle g_{i}({\\boldsymbol x},{\\boldsymbol y})=\\frac{1}{\\left|\\mathcal{D}_{t r}^{(i)}\\right|}\\sum_{(\\xi_{e},\\zeta_{e})\\in\\mathcal{D}_{t r}^{(i)}}\\sigma({\\boldsymbol x}_{e}){\\cal L}(\\phi(\\xi_{e};{\\boldsymbol y}),\\zeta_{e})+C\\left\\|{\\boldsymbol y}\\right\\|^{2},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 64}, {"type": "image", "img_path": "g5DyqerUpX/tmp/968408be6e3de199c02929e64e5ded072b2f74a4b2357a8e03c131ca30ac1dc4.jpg", "img_caption": ["Figure 5: Hypergradient evaluation times for required test accuracy in hyper-cleaning problem. (Left: $p=0.2$ ;Right: $p=0.3$ "], "img_footnote": [], "page_idx": 65}, {"type": "text", "text": "where $\\phi$ denotes a training model while $y$ denotes its parameters, $L$ denotes the cross-entropy loss functionand ()=(1 +e)-1isthe sigmoid function.D) and $\\mathcal{D}_{v a l}^{(i)}$ denotes the training and validation set of the $i$ -th agent, respectively. $C>0$ is a fixed regularization parameter. ", "page_idx": 65}, {"type": "text", "text": "Data generation and experiment settings. In this experiment, we let $\\phi$ be a two-layer MLP network with a 300-dim hidden layer and ReLU activation while $y$ denotes its parameters. For $1\\leq i\\leq10$ we sample a probability distribution $\\mathcal{P}_{i}$ randomly by Dirichlet distribution with parameters $\\alpha=0.1$ The training and validation images with label $i$ are sent to different agents according the probability distribution Pi. Then D(i) and D(i) Dat are generated suffciently heterogeneous [32]. We set C = 0.001. The batch size is set to 50. ", "page_idx": 65}, {"type": "text", "text": "Convergence performances with different corruption rates. We set the moving-average term $\\theta_{k}=0.2$ and run D-SOBA [29], MA-DSBO-GT [10], MDBO [21] SPARKLE-GT, SPARKLE-ED, SPARKLE-EXTRA, SPARKLE-ED-GT, and SPARKLE-EXTRA-GT on an Adjusted Ring graph with $n=10$ agents and $p=0.1,0.2,0.3$ separately. The step-sizes for all the algorithms are set to $\\alpha_{k}=\\beta_{k}=\\gamma_{k}=0.03$ and the term $\\eta$ in MDBO is set to 0.5. The weight matrix of Adjust Ring $W=[w_{i j}]_{n\\times n}$ satisfies: ", "page_idx": 65}, {"type": "equation", "text": "$$\nw_{i j}=\\left\\{\\frac{a,\\qquad\\mathrm{if}\\;j=i,}{2},\\right.\\quad\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "Moreover, we run SPARKLE with ED in the lower level and auxiliary variable and gradient tracking in the upper level (i.e. SPARKLE-ED-GT) as well as SPARKLE with EXTRA in the lower level and auxiliary variable and gradient tracking in the upper level (i.e. SPARKLE-EXTRA-GT) and compare their test accuracy with the other four algorithms ", "page_idx": 65}, {"type": "text", "text": "Figure 1 shows that SPARKLE-ED and SPARKLE-EXTRA outperforms in different cases than SPARKLE-GT. Meanwhile, SPARKLE-EXTRA, SPARKLE-EXTRA-GT achieve similar test accuracy, as do those for SPARKLE-ED and SPARKLE-ED-GT, which matches our theoretical results in transient iteration analysis. Figure 5 presents the times of gradient evaluation for different test accuracies of these algorithms at $p=0.2,0.3$ , demonstrating similar results. ", "page_idx": 65}, {"type": "text", "text": "Influence of network topology. We set the corruption rate $p=0.3$ ,the step sizes $\\alpha_{k}=\\beta_{k}=\\gamma_{k}=$ 0.02, and the moving-average term $\\theta_{k}\\,=\\,0.2$ . Then we run SPARKLE-EXTRA and SPARKLEEXTRA-GT on a network containing $n=10$ nodes with different topologies in the following two cases: ", "page_idx": 65}, {"type": "text", "text": ". Fixed upper, varied lower: $x$ communicates through a five-peer graph; $y,z$ communicate through different adjusted rings with $\\rho=0.647,0.828,0.924,0.990.$ ", "page_idx": 65}, {"type": "text", "text": "\u00b7 Fixed lower, varied upper: $y,z$ communicate through a five-peer graph; $x$ communicates through different adjusted rings with $\\rho=0.647,0.828,0.924,0.990.$ ", "page_idx": 65}, {"type": "image", "img_path": "g5DyqerUpX/tmp/f86e818f540efd09b1a395e6a5b3a462866f8862c3293f1244c3a0f62d10e523.jpg", "img_caption": [], "img_footnote": [], "page_idx": 66}, {"type": "image", "img_path": "", "img_caption": ["Figure 6: The average test accuracy of SPARKLE-EXTRA and SPARKLE-EXTRA-GT on hypercleaning with different communicating strategy of $x,y,z$ "], "img_footnote": [], "page_idx": 66}, {"type": "table", "img_path": "g5DyqerUpX/tmp/e7ba4a80d1f0c4f4904deb87342d8e9a79ce77b5086692786b5d6e7bef7a18ce.jpg", "table_caption": ["Table 5: Mean and standard deviation of the average test accuracy of last 40 iterations during 10 trials with different moving-average terms "], "table_footnote": [], "page_idx": 66}, {"type": "text", "text": "The weight matrix of five-peer graph $W=[w_{i j}]_{n\\times n}$ satisfies: ", "page_idx": 66}, {"type": "equation", "text": "$$\nw_{i j}=\\left\\{0.2,\\quad\\mathrm{if~}(j-i)\\%n=0,\\pm1,\\pm2,\\right.\n$$", "text_format": "latex", "page_idx": 66}, {"type": "text", "text": "Figure 6 shows the average test accuracy of both SPARKLE-EXTRA and SPARKLE-EXTRA-GT over 10 trials. It indicates that the test accuracy decays with increasing spectral gap of topologies related to $y,z$ while the topology of $x$ is fixed during the whole iterations. However, such convergence gap becomes milder when the topologies of $y,z$ are fixed and that of $x$ varies. This phenomenon supports our theoretical findings, which suggest that the transient iteration complexity is more sensitive to the network topologies of $y,z$ than to that of $x$ ", "page_idx": 66}, {"type": "text", "text": "Influence of moving-average iteration on convergence. Moreover, for $\\theta_{t}=0.05,0.2,0.3$ we run SPARKLE-GT, SPARKLE-ED, SPARKLE-EXTRA, SPARKLE-ED-GT, and SPARKLEEXTRA-GT on an Adjusted Ring graph with $n=10$ agents, $\\alpha_{k}=\\beta_{k}=\\gamma_{k}=0.03$ and $p=0.3$ for 3000 iterations. We obtain the average test accuracy of the last 40 iterations over 10 trials, and present the mean and standard deviation during the different trials in Table 5. We can observe that most algorithms achieve the highest test accuracy when $\\theta=0.2$ , which may prove that a suitable $\\theta$ can benefit the test accuracy in hyper-cleaning problems. ", "page_idx": 66}, {"type": "image", "img_path": "g5DyqerUpX/tmp/7276395c4281bc551ebde4e4ad2d8379b3c18bb053b0fa6eec79ab2e4f5c1cfd.jpg", "img_caption": ["Figure 7: The test loss against samples generated by one agent of different algorithms in the policy evaluation. (Left: $n=20$ ,Right: $n=10.$ \u4e00 "], "img_footnote": [], "page_idx": 67}, {"type": "table", "img_path": "g5DyqerUpX/tmp/c25698e868c96016f7f4e03cde6dd032376b0f6e82332e424bbf893c424b6a96.jpg", "table_caption": ["Table 6: The average training loss of the last 500 iterations for 10 independent trials in the distributed policy evaluation. "], "table_footnote": [], "page_idx": 67}, {"type": "text", "text": "D.3  Distributed policy evaluation in reinforcement learning ", "text_level": 1, "page_idx": 67}, {"type": "text", "text": "Following the result of [52], we consider a multi-agent MDP problem in reinforcement learning on a distributed setting with $n$ agents. Denote $\\boldsymbol{S}$ as the state space. Suppose that the value function in each state $s\\in S$ is a linear function $V(s)=\\phi_{s}^{\\top}x$ , where $\\phi_{s}\\in\\mathbb{R}^{m}$ is a feature and $x\\in\\mathbb{R}^{m}$ is a parameter. To obtain the optimal solution $x^{*}$ , we consider the following Bellman minimization problem: ", "page_idx": 67}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x\\in\\mathbb{R}^{m}}\\quad F(x)=\\frac{1}{n}\\sum_{i=1}^{n}\\left[\\frac{1}{2|S|}\\sum_{s\\in S}\\left(\\phi_{s}^{\\top}x-\\mathbb{E}_{s^{\\prime}}\\left[r^{i}(s,s^{\\prime})+\\gamma\\phi_{s^{\\prime}}^{\\top}x\\middle|s\\right]\\right)^{2}\\right]\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "where $\\boldsymbol{r}^{i}(s,s^{\\prime})$ denotes the reward incurred from transition $s$ to $s^{\\prime}$ on the $i$ -th agent, $\\gamma\\in(0,1)$ denotes the discount factor. The expectation is taken over all random transitions from state $s$ to $s^{\\prime}$ .It canbe viewed as a bilevel optimization problem with the following upper- and lower-level loss: ", "page_idx": 67}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle f_{i}(x,y)=\\frac{1}{2|S|}\\sum_{s\\in S}(\\phi_{s}^{\\top}x-y_{s})^{2},}\\\\ {\\displaystyle g_{i}(x,y)=\\sum_{s\\in S}\\left(y_{s}-\\mathbb{E}_{s^{\\prime}}\\left[r^{i}(s,s^{\\prime})+\\gamma\\phi_{s^{\\prime}}^{\\top}x\\big|s\\right]\\right)^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "where $\\boldsymbol{y}=(y_{1},\\cdot\\cdot\\cdot\\,,y_{|\\mathcal{S}|})^{\\top}\\in\\mathbb{R}^{|\\mathcal{S}|}$ . In our experiment, we set the number of states $\\vert{\\cal S}\\vert=200$ and $m\\,=\\,10$ . For each $s\\ \\in\\ S$ , we generate its feature $\\phi_{s}\\,\\sim\\,U[0,1]^{m}$ . The non-negative transition probabilities are generated randomly and standardized to satisfy $\\begin{array}{r}{\\sum_{s^{\\prime}\\in S}p_{s,s^{\\prime}}=1}\\end{array}$ . The mean reward $\\bar{r}^{i}(s,s^{\\prime})$ are independently generated from the uniform distribution $U[0,1]$ . In each iteration, the stochastic reward $r^{i}(s,s^{\\prime})\\stackrel{\\cdot}{\\sim}{\\mathcal N}(\\bar{r}^{i}(s,s^{\\prime}),0.02^{2})$ ", "page_idx": 67}, {"type": "text", "text": "For $n=10,20$ , we run SPARKLE-ED and SPARKLE-EXTRA as well as existing decentralized SBO algorithms MDBO [21] and SLDBO [16] (here we use the stochastic gradient instead of deterministic gradient) over a Ring graph. For MDBO, the number of Hessian-inverse estimation iterations is set to 5. The step sizes are 0.03 for all methods. Figure 3 illustrates the upper-level loss against samples generated by one agent for 10 independent trials. Table 6 shows the average training loss of the last 500 iterations for 10 independent trials of the four decentralized SBO algorithms as well as single-level ED [56] (For bilevel algorithms, training loss means the upper-level loss here). ", "page_idx": 67}, {"type": "image", "img_path": "g5DyqerUpX/tmp/6444a798cc7a6581686f6b434bfffd31463fe2faa21412d0b8e87190e33bbe1a.jpg", "img_caption": ["Figure 8: The accuracy on training and testing set of different algorithms for the meta-learning problem. "], "img_footnote": [], "page_idx": 68}, {"type": "text", "text": "Both Figure 3 and Table 6 demonstrate that SPARKLE-ED and SPARKLE-EXTRA converge faster than other methods. ", "page_idx": 68}, {"type": "text", "text": "Finally, we create a fixed \"test set\" with 10000 sample generated from $\\boldsymbol{S}$ \uff1aFigure 7 shows the loss on the test set of SPARKLE-ED, SPARKLE-EXTRA, SLDBO, MDBO and single-level ED algorithm, demonstrating the superior performance of SPARKLE compared to other decentralized SBO algorithms. ", "page_idx": 68}, {"type": "text", "text": "D.4  Decentralized meta-learning ", "text_level": 1, "page_idx": 68}, {"type": "text", "text": "We consider a meta-learning problem as described in [18]. There are $R$ tasks $\\{\\mathcal{T}_{s},s=1,\\cdots\\,,R\\}$ Each task $\\mathcal{T}_{s}$ has its own loss function $L(x,y_{s},\\xi)$ , where $\\xi_{s}$ represents a stochastic sample drawn from the data distribution $\\mathcal{D}_{s}$ \uff0c $y_{s}$ denotes the task-specific parameters and $x$ denotes the global parameters shared by all the tasks. In meta-learning problem, we aim to find the parameters $(x^{*},y_{1}^{*},\\cdot\\cdot\\cdot\\,,y_{R}^{*})$ that minimizes the loss function across all $R$ tasks, i.e., ", "page_idx": 68}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x,y_{1},\\cdots,y_{R}}l(x,y_{1},\\cdots,y_{R})=\\frac{1}{R}\\sum_{s=1}^{R}\\mathbb{E}_{\\xi\\sim\\mathcal{D}_{s}}\\left[L(x,y_{s},\\xi)\\right].\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "The problem (102) can be formulated as a decentralized SBO problem with heterogeneous data distributions across $N$ nodes. For $i\\,=\\,1,2,\\cdots\\,,N$ ,let $\\mathcal{D}_{s,i}^{\\mathrm{train}}$ and $\\mathcal{D}_{s,i}^{\\mathrm{val}}$ denote the training and validationdatasetsforthe $s$ -th task $\\tau_{s}$ receivedbynode $i$ respectively.We can then address the meta-learning problem by minimizing (1), with the upper- and lower-level loss functions defined as: ", "page_idx": 68}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle f_{i}(x,y)=\\frac{1}{R}\\sum_{s=1}^{R}\\mathbb{E}_{\\xi\\sim\\mathcal{D}_{s,i}^{\\mathrm{val}}}\\left[L(x,y_{s},\\xi)\\right],}}\\\\ {{\\displaystyle g_{i}(x,y)=\\frac{1}{R}\\sum_{s=1}^{R}\\left[\\mathbb{E}_{\\xi\\sim\\mathcal{D}_{s,i}^{\\mathrm{tain}}}\\left[L(x,y_{s},\\xi)\\right]+\\mathcal{R}(y_{s})\\right],}}\\end{array}\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "where $L$ denotes the cross-entropy loss and $\\mathcal{R}(y_{s})=C_{r}\\left\\|y_{s}\\right\\|^{2}$ is a strongly convex regularization function. ", "page_idx": 68}, {"type": "text", "text": "In this experiment, we compare SPARKLE-ED with D-SOBA [29] and MAML [18] in a decentralized communication setting over a 5-way 5-shot task across a network of $N=8$ nodes connected by Ring graph. The dataset used is minilmageNet [47], derived from ImageNet [42], which comprises 100 classes, each containing 600 images of size $84\\times84$ .We set $R\\,=\\,2000$ and partition these classes into 64 for training, 16 for validation, and 20 for testing. For the training and validation classes, the data is split according to a Dirichlet distribution with parameter $\\alpha=0.1$ [32]. We utilize a four-layer CNN with four convolution blocks, where each block sequentially consists of a $3\\times3$ convolution with 32 filters, batch normalizationm, ReLU activation, and $2\\times2$ max pooling. The batch size is 32 , and $C_{r}=0.001$ . The parameters of the last linear layer are designated as task-specific, while the other parameters are shared globally. For SPARKLE and D-SOBA, the step-sizes are $\\beta=\\gamma=0.1$ and $\\alpha=0.01$ . For MAML, the inner step-size is 0.1 and the outer step-size is 0.001, and the number of inner-loop steps as 3. For all algorithms, the task number is set to 32. And we only repeat the experiment only once due to the time limitation. Figure 8 shows the average accuracy on the training dataset for all nodes, as well as the test accuracy of the three algorithms. We observe that SPARKLE-ED outperforms other algorithms, demonstrating the efficiency of SPARKLE in decentralized meta-learning problems. ", "page_idx": 68}, {"type": "text", "text": "", "page_idx": 69}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 70}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 70}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 70}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 70}, {"type": "text", "text": "Justification: Refer to Abstract and Introduction. ", "page_idx": 70}, {"type": "text", "text": "Guidelines: ", "page_idx": 70}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 70}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 70}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 70}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 70}, {"type": "text", "text": "Justification: Refer to Section Conclusions. ", "page_idx": 70}, {"type": "text", "text": "Guidelines: ", "page_idx": 70}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 70}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 70}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 70}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 70}, {"type": "text", "text": "Justification: Refer to Section Assumptions for our assumptions, and Appendix for detailed proof. ", "page_idx": 71}, {"type": "text", "text": "Guidelines: ", "page_idx": 71}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 71}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 71}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 71}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 71}, {"type": "text", "text": "Justification: Refer to Appendix. ", "page_idx": 71}, {"type": "text", "text": "Guidelines: ", "page_idx": 71}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. () If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 71}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 71}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 71}, {"type": "text", "text": "Answer: [No] ", "page_idx": 72}, {"type": "text", "text": "Justification: We may consider making data and code openly accessible when it is deemed necessary. ", "page_idx": 72}, {"type": "text", "text": "Guidelines: ", "page_idx": 72}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 72}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 72}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 72}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 72}, {"type": "text", "text": "Justification: Refer to Appendix. ", "page_idx": 72}, {"type": "text", "text": "Guidelines: ", "page_idx": 72}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 72}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 72}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 72}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 72}, {"type": "text", "text": "Justification: We show error bars in experiments where we consider them essential. Guidelines: ", "page_idx": 72}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative errorrates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 72}, {"type": "text", "text": "", "page_idx": 73}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 73}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce theexperiments? ", "page_idx": 73}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 73}, {"type": "text", "text": "Justification: Refer to Appendix. ", "page_idx": 73}, {"type": "text", "text": "Guidelines: ", "page_idx": 73}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 73}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 73}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 73}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 73}, {"type": "text", "text": "Justification:Our research conforms, in every respect, with the NeurIPs Code of Ethics. Guidelines: ", "page_idx": 73}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 73}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 73}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 73}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 73}, {"type": "text", "text": "Justification: There is no societal impact of our work. ", "page_idx": 73}, {"type": "text", "text": "Guidelines: ", "page_idx": 73}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 73}, {"type": "text", "text": "", "page_idx": 74}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 74}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 74}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 74}, {"type": "text", "text": "Justification: There is no such risk in the paper. ", "page_idx": 74}, {"type": "text", "text": "Guidelines: ", "page_idx": 74}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safetyfilters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 74}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 74}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 74}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 74}, {"type": "text", "text": "Justification: We comply with the licenses of existing assets used in the paper and provide necessary references. ", "page_idx": 74}, {"type": "text", "text": "Guidelines: ", "page_idx": 74}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 74}, {"type": "text", "text": "\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 75}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 75}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 75}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 75}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 75}, {"type": "text", "text": "Guidelines: ", "page_idx": 75}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 75}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 75}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 75}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 75}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 75}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 75}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 75}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 75}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 75}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 75}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 75}]