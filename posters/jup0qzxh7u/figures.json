[{"figure_path": "Jup0qZxH7U/figures/figures_1_1.jpg", "caption": "Figure 1: Overview of our framework. We first compute the sum of Redundancy Metric between layer i-th and other layers to construct objective function. Then, we solve a linear programming problem to optimize total sparsity ratios S(qi) (qi is pre-layer sparsity) under constraints.", "description": "This figure illustrates the Adaptive Layer Sparsity (ALS) framework.  First, a redundancy metric is calculated between each layer and all other layers of the Large Language Model (LLM). This metric quantifies the redundancy or importance of each layer.  Then, a linear programming problem is formulated and solved. The objective function maximizes the overall sparsity of the model, subject to constraints on the total model size. The solution to this optimization problem determines the optimal sparsity ratio for each layer, which is then applied to selectively prune less important features, resulting in an adaptively sparse model.", "section": "3 Methodology"}, {"figure_path": "Jup0qZxH7U/figures/figures_9_1.jpg", "caption": "Figure 2: (a) Calibration data experiment: PPL decreases slightly with more data. (b) Pruning bounds: Model performance remains relatively stable between 30% and 70% bounds. (c) Model redundancy: Higher RM metric, lower performance.", "description": "This figure presents three subplots visualizing the impact of different factors on the performance of the language model. (a) shows how perplexity decreases slightly as the amount of calibration data increases; (b) illustrates the model's stable performance when pruning bounds are between 30% and 70%; and (c) demonstrates that higher redundancy metrics correlate with lower performance.", "section": "Experimental Results"}, {"figure_path": "Jup0qZxH7U/figures/figures_19_1.jpg", "caption": "Figure 3: 50% sparsity in LLAMA-V1/V2/V3 family. Various sparsity in LLAMA-V2 7B/13B", "description": "This figure visualizes the correlation matrices obtained by solving the problem under different experimental settings. It shows the heatmaps of the Redundancy Metric (RM) for various LLMs (LLaMA-V1 7B, 13B, 30B, 65B; LLaMA-V2 7B, 13B, 70B; LLaMA-V3 8B) at different sparsity levels (50% for the first set of models, and various levels for the second set of models). The color intensity represents the redundancy between layers, with darker colors indicating higher redundancy.", "section": "D Visualization of Correlation Matrices"}, {"figure_path": "Jup0qZxH7U/figures/figures_20_1.jpg", "caption": "Figure 3: 50% sparsity in LLAMA-V1/V2/V3 family. Various sparsity in LLAMA-V2 7B/13B", "description": "This figure visualizes the correlation matrices obtained by solving the redundancy problem using the proposed method for different LLMs at various sparsity levels.  Specifically, it showcases heatmaps representing the Redundancy Metric (RM) for different models, illustrating the correlation between layers.  The color intensity in the heatmaps indicates the level of redundancy, with darker shades representing higher redundancy and lighter shades representing lower redundancy.", "section": "D Visualization of Correlation Matrices"}, {"figure_path": "Jup0qZxH7U/figures/figures_20_2.jpg", "caption": "Figure 3: 50% sparsity in LLAMA-V1/V2/V3 family. Various sparsity in LLAMA-V2 7B/13B", "description": "This figure visualizes the correlation matrices obtained by solving the linear programming problem in the ALS method, showing the redundancy among layers in different LLMs at various sparsity levels (50% for LLaMA-V1/V2/V3 and various levels for LLaMA-V2 7B/13B). The heatmaps represent the Redundancy Metric (RM), with darker colors indicating higher redundancy between layers. The figure aims to illustrate how the proposed ALS method effectively allocates sparsity ratios across different layers based on their redundancy, achieving fine-grained optimization of LLMs.", "section": "D Visualization of Correlation Matrices"}, {"figure_path": "Jup0qZxH7U/figures/figures_21_1.jpg", "caption": "Figure 6: The sparsity ratio allocation in various sparsity in LLAMA-V2 7B/13B family.", "description": "This figure visualizes the sparsity ratio allocation across different layers of LLAMA-V2 7B and 13B models at various sparsity levels (30%, 40%, 50%, 60%, 70%).  Each sub-figure represents a specific model and sparsity level, showing the allocated sparsity ratio for each layer (Q, K, V, O, gate, up, down) using a color gradient.  The color intensity represents the sparsity ratio, with darker shades indicating higher sparsity.", "section": "D Visualization of Correlation Matrices"}, {"figure_path": "Jup0qZxH7U/figures/figures_21_2.jpg", "caption": "Figure 7: The granularity experiment in LLAMA-V2 7B.", "description": "The figure shows the result of an experiment on the impact of granularity on perplexity (PPL) in the LLAMA-V2 7B model.  Initially, PPL remains relatively constant for granularities of 0.1 and 0.5. It then decreases to 9.86 at a granularity of 1 and further to 9.67 at a granularity of 5. Beyond this point, the smoothed curve shows a subsequent rise in PPL, indicating that excessively high granularity may negatively impact model performance.  This illustrates a necessary balance in optimizing granularity to minimize PPL and enhance model accuracy and efficiency.", "section": "E Extra Figures and Explanations"}, {"figure_path": "Jup0qZxH7U/figures/figures_22_1.jpg", "caption": "Figure 8: The comparison of decreasing function", "description": "This figure compares two different decreasing functions used in the paper's Adaptive Layer Sparsity (ALS) method. The x-axis represents the sum of redundancy metrics between a layer and other layers.  The y-axis represents the importance factor (wi) calculated by each function.  The original expression shows a steeper decline, suggesting a more aggressive reduction in importance as redundancy increases. The modified expression exhibits a gentler decrease, which might offer a more balanced adjustment to sparsity ratios across layers. This choice of functions impacts the distribution of sparsity across the layers of the language model and is a crucial element of the ALS algorithm.", "section": "3.3 Linear Optimization"}, {"figure_path": "Jup0qZxH7U/figures/figures_22_2.jpg", "caption": "Figure 8: The comparison of decreasing function", "description": "This figure compares two different decreasing functions used in the Adaptive Layer Sparsity (ALS) method for allocating sparsity ratios across layers.  The original expression,  \u03c9i = exp(\u2212(\u2211j\u2260iRM(xi,xj)\u22121)), is compared against a modified expression. The x-axis represents the sum of redundancy metrics (RM) between a given layer and other layers, while the y-axis represents the resulting importance weight (\u03c9i) for that layer.  The plot shows how the different functions transform the redundancy values into importance weights, highlighting the impact of the function choice on the sparsity allocation.", "section": "3.3 Linear Optimization"}, {"figure_path": "Jup0qZxH7U/figures/figures_23_1.jpg", "caption": "Figure 9: The error bar in 50% sparsity experiment in LLAMA-V2 7B.", "description": "This figure shows the error bars for the accuracy of seven zero-shot tasks using the LLaMA-V2 7B model with 50% sparsity. The error bars represent the standard deviation of multiple runs of the experiments, providing a measure of the variability in the results.  The tasks are winogrande, piqa, openbookqa, hellaswag, boolq, arc-easy, arc-challenge, and rte.  The different colored bars represent the different pruning methods: Magnitude, Magnitude with ALS, SparseGPT, SparseGPT with ALS, Wanda, and Wanda with ALS.", "section": "E.3 Error Bar"}, {"figure_path": "Jup0qZxH7U/figures/figures_24_1.jpg", "caption": "Figure 2: (a) Calibration data experiment: PPL decreases slightly with more data. (b) Pruning bounds: Model performance remains relatively stable between 30% and 70% bounds. (c) Model redundancy: Higher RM metric, lower performance.", "description": "This figure presents the results of three experiments evaluating different aspects of the proposed Adaptive Layer Sparsity (ALS) method. (a) shows that increasing the amount of calibration data leads to a small decrease in perplexity (PPL), demonstrating the robustness of ALS. (b) shows that model performance is stable when the pruning bounds are set between 30% and 70%, which is a practical range for efficient pruning. (c) demonstrates a negative correlation between model redundancy and PPL, suggesting that minimizing redundancy is essential for improving the model's performance.", "section": "Experimental Results"}, {"figure_path": "Jup0qZxH7U/figures/figures_24_2.jpg", "caption": "Figure 2: (a) Calibration data experiment: PPL decreases slightly with more data. (b) Pruning bounds: Model performance remains relatively stable between 30% and 70% bounds. (c) Model redundancy: Higher RM metric, lower performance.", "description": "This figure presents the results of three experiments analyzing the impact of different factors on the performance of the proposed Adaptive Layer Sparsity (ALS) method.  (a) shows the relationship between the amount of calibration data used and the perplexity (PPL) score, indicating a slight improvement with increased data but a limited effect. (b) demonstrates the relative stability of model performance across a range of pruning bounds (30% to 70%), suggesting the robustness of the method. (c) illustrates a negative correlation between model redundancy (measured using the Redundancy Metric or RM) and model performance (PPL), highlighting that minimizing redundancy leads to improved performance.", "section": "Experimental Results"}]