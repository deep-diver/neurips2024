[{"figure_path": "Jup0qZxH7U/tables/tables_6_1.jpg", "caption": "Table 1: WikiText-2 perplexity performance of ALS at varying sparsity rates for sparse LLaMA-V2-7B/13B pruned by the Magnitude, SparseGPT, Wanda metric.", "description": "This table presents the WikiText-2 perplexity scores achieved by applying Adaptive Layer Sparsity (ALS) to various LLaMA-V2 models (7B and 13B parameters) that were initially pruned using three different methods (Magnitude, SparseGPT, and Wanda).  It demonstrates how ALS improves the perplexity (a measure of language model performance) at different sparsity levels (20% to 70%) across these different pruning baselines.", "section": "4.1 Language Modeling"}, {"figure_path": "Jup0qZxH7U/tables/tables_6_2.jpg", "caption": "Table 2: WikiText-2 perplexity performance of ALS at 50% sparsity rates for sparse LLaMA-V1-7B/13B/30B/65B, LLaMA-V2-7B/13B/70B, LLaMA-V3 8B/70B and OPT-6.7B/13B pruned by the Magnitude, SparseGPT, Wanda metric.", "description": "This table presents the WikiText-2 perplexity scores achieved by different sparse language models (LLaMA-V1, LLaMA-V2, LLaMA-V3, and OPT) at a 50% sparsity rate.  The models were pruned using three different methods: Magnitude, SparseGPT, and Wanda.  The table shows the perplexity of each model before and after applying the Adaptive Layer Sparsity (ALS) method, illustrating the impact of ALS on model performance. ", "section": "4.1 Language Modeling"}, {"figure_path": "Jup0qZxH7U/tables/tables_7_1.jpg", "caption": "Table 3: Averaged accuracies (%) for zero-shot tasks at 50% sparsity rate for sparse LLaMA-V1 7B/13B/30B/65B, LLaMA-V2 7B/13B/70B, LLaMA-V3 8B and OPT-6.7B/13B.", "description": "This table presents the average accuracy across seven zero-shot tasks for various LLMs (LLaMA-V1, LLaMA-V2, LLaMA-V3, and OPT) at a 50% sparsity rate.  The results are compared across different pruning methods (Dense, Magnitude, Magnitude w. ALS, SparseGPT, SparseGPT w. ALS, Wanda, and Wanda w. ALS).  The table shows the impact of the ALS method on the accuracy of sparse LLMs.", "section": "4.2 Zero-shot Tasks"}, {"figure_path": "Jup0qZxH7U/tables/tables_7_2.jpg", "caption": "Table 3: Averaged accuracies (%) for zero-shot tasks at 50% sparsity rate for sparse LLaMA-V1 7B/13B/30B/65B, LLaMA-V2 7B/13B/70B, LLaMA-V3 8B and OPT-6.7B/13B.", "description": "This table presents the average accuracy across seven zero-shot tasks for various large language models (LLMs) at 50% sparsity.  It compares the performance of different pruning methods (Magnitude, SparseGPT, Wanda) with and without the Adaptive Layer Sparsity (ALS) approach.  The results show the impact of ALS on the performance of different LLMs under the condition of high sparsity.", "section": "4.2 Zero-shot Tasks"}, {"figure_path": "Jup0qZxH7U/tables/tables_8_1.jpg", "caption": "Table 2: WikiText-2 perplexity performance of ALS at 50% sparsity rates for sparse LLaMA-V1-7B/13B/30B/65B, LLaMA-V2-7B/13B/70B, LLaMA-V3 8B/70B and OPT-6.7B/13B pruned by the Magnitude, SparseGPT, Wanda metric.", "description": "This table presents the WikiText-2 perplexity results for various large language models (LLMs) after applying adaptive layer sparsity (ALS) at a 50% sparsity rate.  It compares the performance of ALS against three baseline pruning methods (Magnitude, SparseGPT, and Wanda) across different LLM sizes and architectures, showcasing the impact of ALS on perplexity scores.", "section": "4 Experimental Results"}, {"figure_path": "Jup0qZxH7U/tables/tables_8_2.jpg", "caption": "Table 7: Results of feature choice from varying component output of each layer on WikiText2 and zero-shot tasks.", "description": "This table presents the results of an ablation study that investigates the impact of different feature choices (input, output, gate) on the performance of the model in terms of perplexity (PPL) on the WikiText-2 dataset and accuracy (ACC) on zero-shot tasks.  It shows how the choice of features affects the model's ability to learn representations and generalize to new tasks.", "section": "4.3 Ablation Study"}, {"figure_path": "Jup0qZxH7U/tables/tables_8_3.jpg", "caption": "Table 8: Impact of normalizing features and per-layer weights for distance function on WikiText2 and zero-shot tasks.", "description": "This table presents the results of an ablation study on the impact of feature and weight normalization on the performance of the proposed method.  It compares the WikiText-2 perplexity (PPL) and average accuracy (ACC) across three different settings: using no normalization (Vanilla), normalizing only features (Feature-Norm), and normalizing both features and weights (Feature-Norm+Weight-Norm).  The results show that applying feature and weight normalization leads to improvements in accuracy, while perplexity remains relatively stable.", "section": "4.3 Ablation Study"}, {"figure_path": "Jup0qZxH7U/tables/tables_9_1.jpg", "caption": "Table 9: WikiText-2 perplexity performance on LLaMA-V2-13B at 50% sparsity rates.", "description": "This table presents the WikiText-2 perplexity scores achieved by the Wanda model with and without ALS (Adaptive Layer Sparsity) and OWL (Outlier Weighed Layerwise Sparsity) at different sparsity levels (50%, 2:4, 4:8).  It demonstrates the comparative performance of these different sparsity allocation methods on a specific LLM model and benchmark dataset, highlighting the effectiveness of ALS.", "section": "4.3 Ablation Study"}, {"figure_path": "Jup0qZxH7U/tables/tables_17_1.jpg", "caption": "Table 3: Averaged accuracies (%) for zero-shot tasks at 50% sparsity rate for sparse LLaMA-V1 7B/13B/30B/65B, LLaMA-V2 7B/13B/70B, LLaMA-V3 8B and OPT-6.7B/13B.", "description": "This table presents the average accuracy across seven zero-shot tasks for various large language models (LLMs) at 50% sparsity.  The models include different versions of LLaMA and OPT, each pruned using different methods (Magnitude, SparseGPT, Wanda) with and without the Adaptive Layer Sparsity (ALS) method. The table allows for comparison of the performance impact of ALS across different LLMs and pruning techniques.", "section": "4.2 Zero-shot Tasks"}, {"figure_path": "Jup0qZxH7U/tables/tables_17_2.jpg", "caption": "Table 3: Averaged accuracies (%) for zero-shot tasks at 50% sparsity rate for sparse LLaMA-V1 7B/13B/30B/65B, LLaMA-V2 7B/13B/70B, LLaMA-V3 8B and OPT-6.7B/13B.", "description": "This table presents the average accuracy across seven zero-shot tasks for various large language models (LLMs) at 50% sparsity.  The results compare the performance of different pruning methods (Magnitude, SparseGPT, Wanda) both with and without the Adaptive Layer Sparsity (ALS) technique. This allows for a direct comparison of the effectiveness of ALS in improving the performance of sparse LLMs.", "section": "4.2 Zero-shot Tasks"}, {"figure_path": "Jup0qZxH7U/tables/tables_17_3.jpg", "caption": "Table 3: Averaged accuracies (%) for zero-shot tasks at 50% sparsity rate for sparse LLaMA-V1 7B/13B/30B/65B, LLaMA-V2 7B/13B/70B, LLaMA-V3 8B and OPT-6.7B/13B.", "description": "This table presents the average accuracy across seven zero-shot tasks for various LLMs (LLaMA-V1, LLaMA-V2, LLaMA-V3, and OPT) at a 50% sparsity level.  It compares the performance of different pruning methods (Dense, Magnitude, Magnitude w. ALS, SparseGPT, SparseGPT w. ALS, Wanda, and Wanda w. ALS).  The results show the impact of the Adaptive Layer Sparsity (ALS) method on improving the accuracy of sparse LLMs. ", "section": "4.2 Zero-shot Tasks"}, {"figure_path": "Jup0qZxH7U/tables/tables_18_1.jpg", "caption": "Table 2: WikiText-2 perplexity performance of ALS at 50% sparsity rates for sparse LLaMA-V1-7B/13B/30B/65B, LLaMA-V2-7B/13B/70B, LLaMA-V3 8B/70B and OPT-6.7B/13B pruned by the Magnitude, SparseGPT, Wanda metric.", "description": "This table presents the WikiText-2 perplexity results for several large language models (LLMs) at a 50% sparsity rate.  Different pruning methods (Magnitude, SparseGPT, Wanda) were used, and the results both with and without the Adaptive Layer Sparsity (ALS) method are shown. This allows for a comparison of the effectiveness of ALS across different LLMs and base pruning techniques.", "section": "4.1 Language Modeling"}, {"figure_path": "Jup0qZxH7U/tables/tables_18_2.jpg", "caption": "Table 1: WikiText-2 perplexity performance of ALS at varying sparsity rates for sparse LLaMA-V2-7B/13B pruned by the Magnitude, SparseGPT, Wanda metric.", "description": "This table presents the WikiText-2 perplexity results for the LLaMA-V2 7B and 13B models at various sparsity levels (20% to 70%).  It compares the performance of ALS against three baseline methods (Magnitude, SparseGPT, and Wanda) both with and without the ALS method applied. The table shows how the perplexity changes across different sparsity levels and methods, demonstrating ALS's effectiveness in maintaining performance even with high sparsity.", "section": "4.1 Language Modeling"}, {"figure_path": "Jup0qZxH7U/tables/tables_19_1.jpg", "caption": "Table 1: WikiText-2 perplexity performance of ALS at varying sparsity rates for sparse LLaMA-V2-7B/13B pruned by the Magnitude, SparseGPT, Wanda metric.", "description": "This table presents the WikiText-2 perplexity scores achieved by applying Adaptive Layer Sparsity (ALS) to the LLaMA-V2 7B and 13B models which were previously pruned using three different methods: Magnitude, SparseGPT, and Wanda. The table shows the perplexity at varying sparsity levels (20%, 30%, 40%, 50%, 60%, 70%).  For each sparsity level and pruning method, the table presents the perplexity before and after applying ALS, demonstrating the effectiveness of ALS in improving the performance of sparse LLMs.", "section": "4.1 Language Modeling"}, {"figure_path": "Jup0qZxH7U/tables/tables_23_1.jpg", "caption": "Table 1: WikiText-2 perplexity performance of ALS at varying sparsity rates for sparse LLaMA-V2-7B/13B pruned by the Magnitude, SparseGPT, Wanda metric.", "description": "This table presents the WikiText-2 perplexity scores achieved by the Adaptive Layer Sparsity (ALS) method at different sparsity levels (20% to 70%) when applied to the LLaMA-V2 7B and 13B models.  For comparison, the perplexity results using three other methods (Magnitude, SparseGPT, and Wanda) are also shown, both with and without ALS. The table highlights the effectiveness of ALS in maintaining low perplexity even at high sparsity levels. ", "section": "4.1 Language Modeling"}]