[{"figure_path": "OrtN9hPP7V/tables/tables_4_1.jpg", "caption": "Table 2: Model configuration performance and size.", "description": "This table shows the FID scores and model parameters (#params) for different configurations of the R3GAN model.  It tracks the changes in FID (Frechet Inception Distance) and model size as the authors progressively remove StyleGAN2's tricks and adopt a more modern architecture. Config A is the StyleGAN2 baseline. Config B removes all StyleGAN2's tricks, demonstrating the importance of the proposed loss function. Config C incorporates the improved loss. Config D modernizes the network backbone and reduces the number of parameters. Config E further modernizes the architecture by increasing the width with depthwise convolution and inverting the bottleneck, achieving state-of-the-art results.", "section": "3 A Roadmap to a New Baseline R3GAN"}, {"figure_path": "OrtN9hPP7V/tables/tables_6_1.jpg", "caption": "Table 3: StackedMNIST 1000-mode coverage.", "description": "This table shows the mode coverage and reverse KL divergence (DKL) for different generative models on the StackedMNIST dataset.  StackedMNIST is a dataset with 1000 uniformly-distributed modes, making it a good benchmark for evaluating a model's ability to avoid mode collapse (generating only a subset of the possible modes) and generate diverse samples.  The table compares several GANs (Generative Adversarial Networks) and likelihood-based models.  The \"# modes \u2191\" column indicates how many of the 1000 modes were successfully covered by the model's generated samples. The \"DKL \u2193\" column shows the reverse KL divergence between the model's generated sample distribution and the true uniform distribution; lower values indicate better mode coverage.", "section": "4.2 Mode Recovery"}, {"figure_path": "OrtN9hPP7V/tables/tables_7_1.jpg", "caption": "Table 4: FFHQ-256. * denotes models that leak ImageNet features.", "description": "This table presents a comparison of different generative models on the FFHQ dataset at 256x256 resolution.  The models are evaluated using FID (Fr\u00e9chet Inception Distance), a metric that measures the similarity between the generated images and real images. The NFE (Number of Function Evaluations) column indicates the computational cost of generating a single sample.  The asterisk (*) next to some model names indicates that those models used ImageNet features, which may introduce bias.  This table highlights the performance of the proposed R3GAN model (Ours-Config E) relative to other state-of-the-art models, emphasizing its efficiency and competitive FID.", "section": "4.3 FID - FFHQ-256 (Optimized)"}, {"figure_path": "OrtN9hPP7V/tables/tables_7_2.jpg", "caption": "Table 5: FFHQ-64. * denotes models that leak ImageNet features.", "description": "This table presents a comparison of different generative models on the FFHQ-64 dataset.  The models are evaluated based on their FID (Fr\u00e9chet Inception Distance) score and the number of forward passes (NFE) required to generate a sample.  The table highlights the performance of the proposed R3GAN model (Ours\u2014Config E) against state-of-the-art GANs and diffusion models.  The asterisk (*) indicates models known to leak ImageNet features, meaning their performance may be artificially inflated due to leveraging pre-trained ImageNet knowledge.", "section": "4.4 FID - FFHQ-64 [33]"}, {"figure_path": "OrtN9hPP7V/tables/tables_7_3.jpg", "caption": "Table 6: CIFAR-10 performance.", "description": "This table presents the Fr\u00e9chet Inception Distance (FID) scores achieved by various GAN models and diffusion models on the CIFAR-10 dataset.  The FID score is a measure of the quality of generated images, with lower scores indicating better image quality. The table also shows the number of function evaluations (NFEs) required for each model, which is a measure of the computational cost of generating images.  The results show that R3GAN (Ours-Config E) achieves a very competitive FID score compared to other leading models, especially considering the significantly lower number of NFEs. This highlights its efficient and high-quality image generation capabilities.", "section": "4.5 FID - CIFAR-10"}, {"figure_path": "OrtN9hPP7V/tables/tables_8_1.jpg", "caption": "Table 7: ImageNet-32.", "description": "This table presents the Fr\u00e9chet Inception Distance (FID) scores achieved by various generative models on the ImageNet-32 dataset.  The models include both GANs (Generative Adversarial Networks) and diffusion models. The Number of Function Evaluations (NFE) required for sample generation is also listed.  The results highlight the performance of the proposed R3GAN model (Ours\u2014Config E) compared to other state-of-the-art models. Notably, it shows the superior performance of R3GAN, even when compared to models that leverage ImageNet feature leakage. This demonstrates the effectiveness of the proposed method in generating high-quality images without relying on external information or tricks.", "section": "4.6 FID - ImageNet-32"}, {"figure_path": "OrtN9hPP7V/tables/tables_8_2.jpg", "caption": "Table 8: ImageNet-64. \u00a7deterministic sampling.", "description": "This table presents the Fr\u00e9chet Inception Distance (FID) scores and number of function evaluations (NFEs) for various generative models on the ImageNet-64 dataset.  The models compared include several state-of-the-art GANs and diffusion models. The FID score is a common metric for evaluating the quality of generated images, with lower scores indicating better image quality.  The NFE (Number of Function Evaluations) represents the computational cost for generating a single sample.  The table highlights the performance of the proposed R3GAN model (Ours-Config E) in comparison to other models. Notably, a subset of the compared models are indicated as having used ImageNet feature leakage, which may influence their FID scores.", "section": "4.7 FID - ImageNet-64"}, {"figure_path": "OrtN9hPP7V/tables/tables_22_1.jpg", "caption": "Table 2: Model configuration performance and size.", "description": "This table presents a comparison of different configurations of the R3GAN model, showing their FID scores and model sizes. The configurations are progressive modifications of StyleGAN2, starting from a fully featured version and gradually removing tricks and modernizing the architecture. Each step in the modifications aims at demonstrating the impact of specific changes on the model's performance. The table illustrates how a minimalist baseline (Config B) can be improved by various techniques, such as a well-behaved loss function (Config C), a modern architecture inspired by ResNet (Config D), and further refinements using ConvNeXt principles (Config E). The results showcase how simplification and modernization can lead to comparable or even better FID scores compared to StyleGAN2 with fewer parameters.", "section": "3 A Roadmap to a New Baseline R3GAN"}]