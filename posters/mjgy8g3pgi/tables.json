[{"figure_path": "mjGy8g3pgi/tables/tables_7_1.jpg", "caption": "Table 5: Comparisons of Yo\u2019LLaVA with LLaVA [2]; GPT-4V results with personalized prompt presented as reference.", "description": "This table compares the performance of Yo\u2019LLaVA against two baselines: Vanilla LLaVA and LLaVA with human-written or LLaVA-generated descriptions, and GPT-4V with text or image prompting.  It shows the recognition accuracy (positive, negative, and weighted average) and question-answering accuracy (visual and text) for each method. The number of tokens used for prompting is also included, highlighting the efficiency of Yo\u2019LLaVA's learnable prompt approach.", "section": "5 Results"}, {"figure_path": "mjGy8g3pgi/tables/tables_7_2.jpg", "caption": "Table 6: Ours vs. MyVLM [34] following the experiment settings in [34]. Yo\u2019LLaVA (Ours) demonstrates advantages over MyVLM without relying on external recognition modules.", "description": "This table compares the performance of YoLLaVA and MyVLM on a visual recognition task.  It shows that YoLLaVA achieves higher accuracy and recall than MyVLM, while not requiring external recognition modules. The comparison uses the same experimental setup as described in MyVLM's paper ([34]).  The table highlights the advantages of YoLLaVA's integrated approach.", "section": "5.3 Comparison with MyVLM"}, {"figure_path": "mjGy8g3pgi/tables/tables_8_1.jpg", "caption": "Table 7: Ablation study on dataset creation. To visualize the question-answering ability, a qualitative example of the personalized model for <bo> is shown (Training photos are provided in Fig. 1).", "description": "This table shows the results of an ablation study on the dataset creation process for the Yo'LLaVA model. The study evaluates the model's ability to answer detailed descriptive questions about a subject.  It shows the impact of different training data components (recognition data, conversation data, and retrieval negative examples) on the model's performance. The qualitative example demonstrates how the fully trained model answers a detailed question about the subject.", "section": "Ablation Studies"}, {"figure_path": "mjGy8g3pgi/tables/tables_12_1.jpg", "caption": "Table 8: Catastrophic forgetting evaluation. Overall, Yo\u2019LLaVA retain nearly identical perforamance with Vanilla LLaVA [10], while offers ability to perform personalized conversation.", "description": "This table presents the results of an ablation study evaluating catastrophic forgetting in Yo\u2019LLaVA, a personalized large multimodal model. The study compares the performance of Yo\u2019LLaVA against the original LLaVA model across three benchmarks: POPE, MMBench, and LLaVA-Wild. The results demonstrate that Yo\u2019LLaVA maintains nearly identical performance to the original LLaVA model, indicating that the model retains its pre-trained knowledge effectively while learning personalized information.  Despite nearly identical performance, Yo\u2019LLaVA offers the capability to perform personalized conversations, demonstrating its efficacy in this new task.", "section": "Catastrophic Forgetting"}, {"figure_path": "mjGy8g3pgi/tables/tables_14_1.jpg", "caption": "Table 5: Comparisons of Yo'LLaVA with LLaVA [2]; GPT-4V results with personalized prompt presented as reference.", "description": "This table compares the performance of Yo'LLaVA against two baselines (LLaVA and GPT-4V) across two tasks: recognition and question answering.  It shows the accuracy results for each model and prompt type (human-written, LLaVA-generated, and GPT-4V-generated) in both positive and negative image recognition. It also presents question answering accuracy for visual and text conversations, highlighting Yo'LLaVA's superiority in both tasks, particularly with fewer tokens.", "section": "5 Results"}, {"figure_path": "mjGy8g3pgi/tables/tables_15_1.jpg", "caption": "Table 5: Comparisons of Yo\u2019LLaVA with LLaVA [2]; GPT-4V results with personalized prompt presented as reference.", "description": "This table compares the performance of Yo\u2019LLaVA against two baselines: Vanilla LLaVA and LLaVA with human-written or automatically generated descriptions.  It evaluates performance on two tasks: recognition accuracy (identifying the personalized subject in images) and question answering (answering questions about the subject in visual and text-only settings).  The table shows that Yo\u2019LLaVA significantly outperforms the baselines, demonstrating its effectiveness in personalizing LLMs.  Additionally, it includes results for GPT-4V using both textual and image prompts, showing that Yo\u2019LLaVA's performance is competitive even compared to a more powerful and resource-intensive model.", "section": "5 Results"}, {"figure_path": "mjGy8g3pgi/tables/tables_19_1.jpg", "caption": "Table 5: Comparisons of Yo\u2019LLaVA with LLaVA [2]; GPT-4V results with personalized prompt presented as reference.", "description": "This table compares the performance of Yo\u2019LLaVA against two baselines: Vanilla LLaVA and LLaVA with human-written descriptions.  It also includes results using GPT-4V with both text and image prompting. The table presents recognition accuracy (positive, negative, and weighted average) and question answering accuracy (visual and text) for different methods.  The number of tokens used in each method is also indicated, highlighting the efficiency of Yo\u2019LLaVA in achieving high accuracy with fewer tokens compared to other methods.", "section": "5 Results"}, {"figure_path": "mjGy8g3pgi/tables/tables_20_1.jpg", "caption": "Table 16: Example of positive recognition question answering", "description": "This table lists 30 example questions and their corresponding answers used for the positive recognition task in the training dataset.  The questions all focus on whether a specific subject is present in an image.  This helps the model to learn to accurately identify the presence of the target subject, rather than just making guesses.", "section": "3.2 Enhancing Recognition with Hard Negative Mining"}, {"figure_path": "mjGy8g3pgi/tables/tables_21_1.jpg", "caption": "Table 17: Example of negative recognition question answering.", "description": "This table provides 30 example questions and their corresponding answers for negative recognition tasks.  These examples demonstrate the model's ability to correctly identify when a subject is NOT present in an image.  The questions are phrased in various ways to test the robustness of the model's understanding and to avoid biases. Each question has a \"No\", indicating that the specified subject is not in the picture.", "section": "F Recognition Questions Template"}, {"figure_path": "mjGy8g3pgi/tables/tables_24_1.jpg", "caption": "Table 21: Examples of GPT-4V's generated personalized text descriptions", "description": "This table presents personalized descriptions of various subjects generated by GPT-4V.  These descriptions are used as input prompts to personalize the LLM (Large Language Model) in the Yo'LLaVA framework.  Each subject is given a detailed textual description intended to capture its visual characteristics for the model's training and recognition purposes.", "section": "4 Experimental Setup"}, {"figure_path": "mjGy8g3pgi/tables/tables_31_1.jpg", "caption": "Table 5: Comparisons of Yo'LLaVA with LLaVA [2]; GPT-4V results with personalized prompt presented as reference.", "description": "This table compares the performance of Yo'LLaVA against two baselines: Vanilla LLaVA and LLaVA with human-written or automatically generated descriptions.  It evaluates performance on two tasks: recognition accuracy (identifying a personalized subject in an image) and question answering (both visual and text-based).  The table shows that Yo'LLaVA significantly outperforms both baselines, especially in recognition, demonstrating the efficacy of its learnable prompt approach in encoding personalized visual knowledge.", "section": "5 Results"}]