[{"type": "text", "text": "Yo\u2019LLaVA: Your Personalized Language and Vision Assistant ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Thao Nguyen Haotian Liu Yuheng Li Mu Cai Utkarsh Ojha Yong Jae Lee University of Wisconsin-Madison ", "page_idx": 0}, {"type": "text", "text": "https://thaoshibe.github.io/YoLLaVA/ ", "page_idx": 0}, {"type": "image", "img_path": "mjGy8g3pgi/tmp/4e749f24787626aa6182da4ad8c0c954d823492ab510b707dcfb54b5390bb50f.jpg", "img_caption": ["Figure 1: Given just a few images of a novel subject (e.g., a dog named <bo>), Yo\u2019LLaVA learns to facilitate textual/visual conversations centered around that subject. "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large Multimodal Models (LMMs) have shown remarkable capabilities across a variety of tasks (e.g., image captioning, visual question answering). While broad, their knowledge remains generic (e.g., recognizing a dog), and they are unable to handle personalized subjects (e.g., recognizing a user\u2019s pet dog). Human reasoning, in contrast, typically operates within the context of specific subjects in our surroundings. For example, one might ask, \u201cWhat should I buy for my dog\u2019s birthday?\"; as opposed to a generic inquiry about \u201cWhat should I buy for a dog\u2019s birthday?\". Similarly, when looking at a friend\u2019s image, the interest lies in seeing their activities (e.g., \u201cmy friend is holding a cat\u201d), rather than merely observing generic human actions (e.g., \u201ca man is holding a cat\"). In this paper, we introduce the novel task of personalizing LMMs, so that they can have conversations about a specific subject. We propose Yo\u2019LLaVA, which learns to embed a personalized subject into a set of latent tokens given a handful of example images of the subject. Our qualitative and quantitative analyses reveal that Yo\u2019LLaVA can learn the concept more efficiently using fewer tokens and more effectively encode the visual attributes compared to strong prompting baselines (e.g., LLaVA). ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Consider the following questions: \u201cWhat is <bo> doing in this photo?\u201d or \u201cI\u2019m thinking about buying a birthday gift for <bo>. What do you recommend?\u201d1 While simple, existing Large Multimodal Models (LMMs) [1\u20134] are not designed to answer such personalized questions. For example, while these models can use their broad knowledge to categorize objects and people in an image (e.g., Fig. 1 (Right), \u201cThere are two individuals who appear in a home setting...\u201d), they cannot recognize those objects as specific subjects known to the user nor provide any personalized details (e.g., \u201cThe man in the image is your friend $\\mathtt{<A>}$ , and he is holding a cat.\u201d), without access to additional context. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Personalized AI assistants would be useful for a wide range of applications including health and wellness, education and learning, entertainment, etc. In particular, the way that individuals interact with and perceive modern AI systems can vary widely, underscoring the need for such systems to adapt to user-specific concepts and contexts [5, 6]. LMMs by default lack personalization primarily due to the nature of their training data (e.g., [7\u20139]), which predominantly consists of common and generic concepts (e.g., person, dog, bicycle), without personalized concepts (e.g., a person named $\\angle A>$ ). Unfortunately, gathering a training dataset that is personalized at scale can be difficult due to privacy concerns and also because the number of images for each subject might be limited (e.g., a user is only willing to share 4-5 images of a person named $\\mathtt{<A>}$ ). ", "page_idx": 1}, {"type": "text", "text": "In this paper, we introduce Yo\u2019LLaVA, a novel personalized LMM built upon the state-of-the-art LLaVA framework [2, 10]. Given just a handful of images of a personalized concept (e.g., a personal stuffed animal), Yo\u2019LLaVA learns to embed the concept into a few special tokens (e.g., <sks>), and can then answer questions about it when prompted. While one could try to describe the personalized visual concept using language (e.g., \u201cMy stuffed animal named <sks> is a yellow-white plush shaped like a dog\u201d), textual descriptions can often be vague and may not capture all visual details (e.g., <sks> has a unique appearance that resembles a Shiba Inu) [11\u201314]. In these cases, learning a visual representation of the personalized concept can be much more precise. ", "page_idx": 1}, {"type": "text", "text": "There are two key challenges for learning a personalized LMM. First, when personalizing an LMM, we want to ensure that its broad pre-trained knowledge is unaffected (i.e., there is no catastrophic forgetting [15, 16]). To this end, we freeze nearly all the LMM\u2019s pre-trained weights, and introduce a set of learnable input tokens [17, 18, 12, 19]: one special token $<\\!\\mathtt{s k s}\\!>$ and $k$ latent tokens $<\\mathtt{t o k e n}_{1}><\\mathtt{t o k e n}_{2}>...<\\mathtt{t o k e n}_{k}>$ . The special token acts as an identifier for the personalized concept, so that the user and model can refer to it in the input and output, respectively, while the latent tokens help to capture <sks>\u2019s relevant visual details. The only pre-trained weights that we train are the output weights for the special token. In this way, the model can acquire new personalized knowledge through the learnable tokens, while retaining all of its prior knowledge in its original weights. This design has the added benefit of being fast to train and lightweight to store. ", "page_idx": 1}, {"type": "text", "text": "The second challenge is enabling the LMM to capture fine-grained visual details. For example, when learning about a personalized subject e.g., $\\mathtt{<A>}$ , we want the model to learn to recognize and distinguish $\\mathtt{<A>}$ from other objects of the same category in a meaningful way; e.g., that $\\mathtt{<A>}$ is an Asian man who wears black glasses, has short black hair, etc., and is visually distinct from other Asian men who have similar features. To this end, we perform hard negative mining [20\u201323] to gather negative examples that are visually similar but not identical to the personalized concept. We then train the model with a large set of questions (e.g., \u201cIs $\\mathtt{<A>}$ is in this photo?\u201d) with both positive and negative image samples (e.g., \u201cYes\u201d and \u201cNo\u201d). In this way, the model learns to embed the fine-grained visual attributes of the personalized concept into the learnable tokens. ", "page_idx": 1}, {"type": "text", "text": "Contributions. In summary, our main contributions are: ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "\u2022 Personalized Large Multimodal Models: We introduce a novel task of personalizing LMMs, enabling them to adapt to and answer to user-specific concepts.   \n\u2022 Efficient framework without forgetting: We introduce Yo\u2019LLaVA \u2013 a personalized LMM that efficiently learns personalized concepts with only a handful of images of each concept, while retaining broad pre-trained knowledge.   \n\u2022 Training dataset: We create a novel dataset specifically designed to explore the task of personalizing LMMs, providing a solid foundation for both training and evaluation.   \n\u2022 Open source: We will publicly release the training and evaluation data for the personalized concept modeling task, as well as our code and models. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Large Multimodal Models. In recent years, we have witnessed the emergence of large language models (LLMs) [1, 24, 25, 3], with significantly improved general question-answering and reasoning capabilities. These advancements have been further extended and we now have systems capable of language understanding as well as visual perception, i.e., Large Multimodal Models (LMMs) [26, 2, 4, 10]. These LMMs represent a groundbreaking frontier, enabling models to process and reason with input images alongside text, with applications spanning various domains such as embodied AI and robotics. However, while these models can showcase their general knowledge in many ways (e.g., recognizing and writing about a famous person or a dog breed in given image), they are not designed to handle personalized queries (e.g., recognizing you or your dog). In this work, we propose a method to extend the existing general purpose knowledge of such a LMM model to some new, personalized knowledge important for a user so that a tailored, personalized experience can be given to that users (e.g., answering questions that relate to your dog). ", "page_idx": 2}, {"type": "text", "text": "Parameter-Efficient Fine-Tuning. Traditionally, fine-tuning has been the standard approach to adapt trained models to new tasks or concepts. However, in the era of LLMs/LMMs, fine-tuning these models can be extremely costly in both compute and memory requirements. To overcome this limitation, Parameter-Efficient Fine-Tuning (PEFT) methods has been introduced to adapt these models for various downstream tasks with only a small number of trainable parameters. There are two main directions: (1) Introducing additional trainable parameters into existing layers of the model (e.g., LoRA [27], LLaMA-Adapter [28]). (2) Soft prompt tuning: learning prompts (e.g., text prompts) that can guide the models to adapt to new tasks or datasets. The latter concept is inspired by the ability of prompt engineering, which leverages task-specific instructions (prompts) to enhance model abilities without modifying model parameters. Soft prompt tuning has shown impressive results in various tasks (e.g., agent tool calling [18]) and the concept has been extended to other domains (e.g., recovering prompts from generated images [12], learning image edits [14]). In this paper, we leverage the idea of soft prompt tuning to learn personalized concepts within the context of LMMs. ", "page_idx": 2}, {"type": "text", "text": "Personalizing Multimodal Models. In the context of image generation, personalization often refers to the task of enabling models to recreate pixel-level visual details of a given subject [29, 13, 30]. Proposed methods often optimize either or both of the following: (1) token(s) for a specific concept (e.g., [13, 30]) or (2) the part/entire of image generation model (e.g., [29]). In contrast, in the NLP community, personalization usually involves making LLMs behave in a specific way (e.g., adopting a humorous or informal tone) [31, 32] or enabling LLMs to provide personalized responses (e.g., recommending movies for specific user [33]). The main approaches include (1) prompting (e.g., modifying system prompts for specific persona \u201cYou are a humorous person\u201d) or (2) information retrieval (e.g., referring to users\u2019 saved metadata during communication). In the context of LMMs, however, personalization has been understudied. Personalizing a LLM requires extracting information not only from text (e.g., \u201c<bo> is a Shiba Inu\u201d), but also from visual inputs (e.g., \u201cThis is a photo of <bo>\u201d). To the best of our knowledge, our paper is a pioneer in the personalization task for LMMs. A concurrent work tackling the same problem is MyVLM [34]; but it relies on external modules to recognize subjects, and is therefore not a completely integrated system. We position our work in the in-between image understanding and personalized conversation: After personalization, the LMM can not only recognize visual aspects of the subject, but also retain reasoning abilities about that subject (e.g., \u201c<bo> is a Shiba Inu, so he may be very alert and loyal\u201d). We also aim to have a lightweight, complete system in which no external modules are involved, relying solely on the LMM itself. ", "page_idx": 2}, {"type": "text", "text": "3 Yo\u2019LLaVA: Personalizing LMMs to Understand User-Specific Concepts ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Given a handful of images of a person or a subject $I^{1},\\ldots,I^{n}$ (e.g., 5 images of your plush toy called <sks>) without any textual labels or captions, our goal is to embed this subject into a pre-trained LMM (in our case, LLaVA [2, 10, 35]), so that both the user and model can communicate using an identifier (e.g., <sks>) for that subject, while also retaining the broad pre-trained knowledge. ", "page_idx": 2}, {"type": "text", "text": "After being personalized, our method (Yo\u2019LLaVA) can: (1) recognize the subject in new images during testing (e.g., Yo\u2019LLaVA can determine whether $\\mathtt{<s k s>}$ is in a photo or not); (2) support visual question answering about the subject (e.g., given a new photo, one can ask about <sks>\u2019s location); and (3) support text-only conversations without any test-time reference images about the subject (e.g., ask questions about intrinsic attributes of <sks> like its color, shape, etc.). ", "page_idx": 2}, {"type": "text", "text": "We start by detailing how we represent the subject as a learnable concept for LLaVA in Sec. 3.1. We then discuss our methods to enable Yo\u2019LLaVA\u2019s to recognize the subject with hard negative example mining in Sec. 3.2, followed by a discussion on enhancing understanding through hard negative examples enhancement in Sec. 3.2. ", "page_idx": 3}, {"type": "image", "img_path": "mjGy8g3pgi/tmp/b0667b561bf0387fb726e5c81376bdf1b4e4fd61d49a6affed65bc1d278651c4.jpg", "img_caption": ["3.1 Personalizing the Subject as a Learnable Prompt ", "Figure 2: Training pipeline. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Prompting is a straightforward and natural way to steer multimodal models. For example, when presented with an image, if one wishes to ask an LMM whether their personal toy (e.g., called <sks>) is in that image, one might begin by providing a personalized description (e.g., \u201c<sks> is a yellow-white plush shaped like a dog\", Table 1, Left). However, manually crafting such prompts can be cumbersome and often impractical, as it can require an excessive number of words (tokens) to accurately capture the subject. Crucially, describing all (subtle) visual details of a subject with words can be extremely chal", "page_idx": 3}, {"type": "text", "text": "lenging, if not, impossible (e.g., describing how your friend looks different from any other person). Inspired by recent research in which shows that learning soft prompts can be a more effective and efficient alternative [18, 14], we propose to represent a personalized description for a subject as a learnable prompt for LMMs (Table 1, Right). This approach is lightweight, requires updating only a few parameters (new tokens and corresponding output weights), while leaving the core parameters (e.g., image encoder, all layers except the output layer of the language model) untouched. ", "page_idx": 3}, {"type": "text", "text": "Specifically, given a set of images $I^{1},\\ldots,I^{n}$ of a subject (e.g., called <sks>), we define a personalized soft-prompt for the subject as: ", "page_idx": 3}, {"type": "text", "text": "Here, <sks> is a newly added vocabulary token that serves as an identifier for the subject, allowing both the user and the model to reference this subject when asking or answering questions. The tokens $\\{<\\mathtt{t o k e n}_{i}>\\}_{i=1}^{k}$ are soft tokens that are learned to embed visual details about the subject. Since <sks> is a new entry to the token vocabulary, we expand the final classifier head matrix of the language model $W$ from $C\\times N$ to $C\\times(N+1)$ , where $C$ is the hidden feature dimension and $N$ is the original vocabulary size. In our Yo\u2019LLaVA framework, the trainable parameters are: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{\\theta}=\\{<\\mathsf{s k s}>\\,,\\,\\,<\\mathsf{t o k e n}_{1}>\\,,\\,\\,\\dots\\,,\\,\\,<\\mathsf{t o k e n}_{k}>,\\,W_{(:,N+1)}\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Herein, we train the $k+1$ newly added input tokens and the final classifier head matrix $W$ associated with the identifier token <sks> only. Except from this, all other components of the pre-trained LLaVA [10] are frozen (i.e., vision encoder, vision projector, and language model). ", "page_idx": 3}, {"type": "text", "text": "To help the model learn the new visual concept, we generate conversational training data triplets $(I^{i},\\mathbf{X}_{\\mathsf{q}}^{\\bar{i}},\\mathbf{X}_{\\mathsf{a}}^{i})$ , where $I^{i}$ is an input image, $\\mathbf{X}_{\\mathsf{q}}^{i}$ is the question, and $\\mathbf{X}_{\\mathsf{a}}^{i}$ is the corresponding answer (Details on dataset creation are in Sec. 3.2 and 3.3). We use the standard masked language modeling loss to compute the probability of the target responses $\\mathbf{X}_{\\mathsf{a}}$ for each conversation of length $L$ by: ", "page_idx": 3}, {"type": "equation", "text": "$$\np(\\mathbf{X_{a}}|I^{i})=\\prod_{j=1}^{L}p_{\\pmb{\\theta}}(\\pmb{x}_{j}|I^{i},\\mathbf{X_{a,<}}_{j}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\pmb{\\theta}$ is the trainable parameters, and $\\mathbf{X}_{\\mathsf{a},<j}$ are the instruction and answer tokens in all turns before the current prediction token $\\pmb{x}_{j}$ , respectively. ", "page_idx": 3}, {"type": "text", "text": "3.2 Enhancing Recognition with Hard Negative Mining ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The most basic and essential ability for a personalized LMM is its capability to recognize a personalized subject (e.g., <sks>). A direct approach to achieve this is by creating visual recognition question and answer templates for training images. These questions can be as simple as asking whether <sks> is in the photo. However, training with only positive examples (or in another words, only images of <sks>) can lead to an undesirable shortcut, where the model learns to always answer \u201cYes\u201d for any question relating to the subject regardless of whether the subject is actually in the photo; rather than learn the necessary visual attributes to recognize the subject. To overcome this, we randomly sample 100 images from LAION [7] to serve as negative examples (images that do not contain <sks>). Training with a mixture of positive and negative examples helps the model understand the visual attributes of the subject (e.g., <sks> is a stuffed animal), but it can also lead to over-generalization. For example, if $\\tt{<s k s>}$ is a yellow dog-shaped plush, the model can overgeneralize and assume that all yellow stuffed animals are <sks>, which is undesirable. The challenge remains in how to improve the model\u2019s ability to distinguish more fine-grained features of the subject that can help differentiate it from visually similar ones (e.g., other similar yellow stuffed animals). ", "page_idx": 3}, {"type": "image", "img_path": "mjGy8g3pgi/tmp/a1e74efbe320dc4c3b97bdf59891f35a5436ac7cfb15053ecc4877faaef148b5.jpg", "img_caption": ["Table 2: Example of training dataset for subject <sks>. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "To overcome this, we employ hard negative mining [20\u201323]. If the subject $\\tt{<s k s>}$ is a stuffed animal, the hard negative examples would be other stuffed animals that are not identical to the subject (Fig. 3, more examples of hard negatives can be found in Appendix I). By exposing the model to a diverse range of visually similar but non-identical objects, we encourage it to learn more discriminative features and prevent over-generalization. We retrieve the negative examples from LAION [36]. Specifically, for each training image ${\\dot{I}}^{i},(i=1,...,n)$ , we retrieve the top $m$ images with highest ", "page_idx": 4}, {"type": "image", "img_path": "mjGy8g3pgi/tmp/d598502f1b3ae80cc56e3f4357fd8a207b1a9d1878b211aec125cb48abcf9b6d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "CLIP image embedding similarity [37]. Finally, the negative Figure 3: Training data creation.   \nexample data are: 100 easy and $n\\times m$ hard negative examples for the subject <sks>. ", "page_idx": 4}, {"type": "text", "text": "To enable the model to recognize subjects in an image, we pair training images with recognition question-answer template. This process involves asking whether a specific subject (e.g., <sks>) is present in the photo. In particular, during training, each positive and negative image is randomly paired with one of the question-answer templates (Details in Appendix F). Answer templates are sampled based on the type of input image (Positive vs. Negative). Essentially, all question-answer pairs are framed as binary classifications, with Yes/No questions determining if the subject (e.g., <sks>) is visible in the photo (See Type 2 and 3 QA in Table 2). ", "page_idx": 4}, {"type": "text", "text": "3.3 Learning to Engage in Natural Conversations about the Subject ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "So far, Yo\u2019LLaVA is capable of recognizing a subject in a new image. However, learning with recognition data alone does not enable the model to communicate with users about anything beyond recognition. For example, while the model may correctly answer whether <sks> is present in an image, it might struggle with other questions (e.g., \u201cDescribe <sks> in detail\u201d, see Tab. 7). ", "page_idx": 4}, {"type": "text", "text": "Thus, we next aim to create more generic conversations for training (e.g., visual question answering). These conversations focus on the subject\u2019s visual characteristics, as opposed to the recognition abilities used in the previous recognition conversations. ", "page_idx": 4}, {"type": "text", "text": "For this, we use a template consisting of 10 manually written, basic questions related to intrinsic attributes, divided into two main categories: human-related questions (e.g., \u201cWhat is the hair color of this person?\u201d) and subject-related questions (e.g., \u201cWhat color is this subject?\u201d). We exclude complex or nuanced questions that may not generalize to all (e.g., \u201cWhat is the tail color of this toy?\u201d). We show a specific example in the Type 1 QA in Table 2 (Please refer to Sec. C for details). For each image $I^{i}$ , we employ LLaVA [10] to generate an answer for each template question, forming a conversation with triplet $(\\dot{I}^{i},\\mathbf{X}_{\\mathsf{q}}^{i},\\mathbf{X}_{\\mathsf{a}}^{i})$ . ", "page_idx": 4}, {"type": "image", "img_path": "mjGy8g3pgi/tmp/4c17b5d228aaa56531d4dc5779b3c5a3049e86fa15a9601c1fd731cf46c9fc69.jpg", "img_caption": [""], "img_footnote": [], "page_idx": 5}, {"type": "table", "img_path": "", "table_caption": [], "table_footnote": ["Table 3: Examples of Yo\u2019LLaVA\u2019s abilities with a learned concept (e.g., a dog named $<\\tt b o>$ ). Yo\u2019LLaVA can recognize and answer questions about that concept. "], "page_idx": 5}, {"type": "text", "text": "A conventional approach would involve training Yo\u2019LLaVA directly with the triplet $(I^{i},\\mathbf{X}_{\\mathsf{q}}^{i},\\mathbf{X}_{\\mathsf{a}}^{i})$ . However, this approach does not effectively facilitate the learning of personalized prompts (i.e., embed new visual knowledge into them), as the model is provided with extra information (the reference image I ) already sufficient to answer the question. For example, if presented with a photo of a stuffed animal <sks> and asked \u201cWhat color is it?\u201d, LLaVA [10] would be able to correctly answer the question without knowing or understanding the visual attributes of $\\tt{<s k s>}$ ; i.e., it can simply use the input image to answer the question. Thus, to encourage the model to distill the visual attributes of the subject into the learnable personalized prompts, we exclude $I^{i}$ during training, which results in training solely with $(\\mathbf{X}_{\\mathsf{q}}^{i},\\mathbf{X}_{\\mathsf{a}}^{i})$ (i.e., we omit image $I^{i}$ in Eq 1 in practice). In this way, Yo\u2019LLaVA correctly learns to embed the relevant visual information of the subject into the soft prompts, and can answer various questions about the visual attributes of the subject, even without any reference image as we shown in our text-only QA experiments (Table 7). ", "page_idx": 5}, {"type": "text", "text": "4 Experimental Setup ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Training. Unless stated otherwise, we use 5 images and $k=16$ tokens to learn the subject. Each conversation is single-turn (one question and one answer). We use AdamW [38] with a 0.001 learning rate and LLaVA-1.5-13B [10] as the base model. Training images include ${\\sim}200$ negative images per subject ( $\\mathord{\\sim}100$ hard negatives from retrieval and $\\mathord{\\sim}100$ easy negatives randomly sampled). We train each subject for up to 15 epochs, saving the best checkpoint based on recognition accuracy on the train set. All experiments are conducted on a single A6000 GPU. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Dataset. We collect a new dataset of 40 subjects: Person (10), Pets (5), Landmarks (5), Objects (15), and Fiction Characters (5). The dataset is divided into train and test splits. The number of images per subject varies from 10-20 images. Please refer to Appendix C for more details about our dataset. ", "page_idx": 6}, {"type": "text", "text": "Baselines. We choose Vanilla LLaVA [2] as our main baseline. We consider two main variants of LLaVA: Naive LLaVA, which is simply LLaVA itself without any personalized information; and LLaVA $^+$ Personalized Description, where LLaVA is assisted with personalized descriptions about the subject. We employ two methods to acquire personalized descriptions: (1) Human-written: We manually write a description for each subject (see Table 4, \u201cHuman\u201d), mimicking a real scenario where a user describes a personalized subject to LMMs. (2) Automated description: We first prompt LLaVA to generate captions for all training images of this subject. We provide two ways to use these captions: (a) We concatenate all captions together resulting in a long, rich description for the subject; (b) We prompt LLaVA to summarize these captions into a brief personalized description (See Table 4 \u201cLLaVA\u201d). These automated descriptions correspond to \u201cLLaVA $^+$ Prompt, Text\u201d with ${\\sim}1.3\\mathrm{k}$ (long) and ${\\sim}16$ (summarized) tokens in Table 5, respectively. ", "page_idx": 6}, {"type": "text", "text": "To expand our evaluation of prompting, we extend our analysis to GPT-4V, a leading proprietary multimodal chatbot. We use the same methodology to generate brief personalized descriptions (Table 4, \u201cGPT-4V\u201d). Additionally, as GPT-4V supports multi-image conversations (a feature not supported by LLaVA), we also experiment with personalized image prompting. Specifically, we present training image(s) of the subject together with an introduction text (e.g., \u201cYou are seeing photo(s) of a subject named <sks>\u201d). These experiments correspond to \u201cGPT- $4\\mathrm{V}+1$ Prompt, Image\u201d with ${\\sim}1\\mathrm{k}$ (given 1 image) and ${\\sim}5\\mathrm{k}$ tokens (given 5 images), respectively (Table 5). Since images convey more information than text, we hypothesize that personalized image prompts represent the upper bound for prompting effectiveness. Notably, due to GPT-4V\u2019s closed-source nature, our approach cannot be directly integrated, making this comparison purely for reference. ", "page_idx": 6}, {"type": "text", "text": "5 Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We showcase Yo\u2019LLaVA\u2019s performance across two primary tasks: (1) Recognition Ability and (2) Question Answering. The first task evaluates Yo\u2019LLaVA\u2019s ability in recognizing personalized subject within a test image, while the second assesses the model\u2019s capacity to have natural conversations (i.e., refer to and respond to queries) about a personalized subject. ", "page_idx": 6}, {"type": "text", "text": "5.1 Recognition Ability ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "First, we evaluate the model\u2019s ability to recognize a personalized subject <sks>. We have 40 subjects, each with 5 to 10 test images containing the corresponding subject. For each subject, all of its test images serve as positive test images, while test images from the remaining 39 categories serve as negative test images. In total, there are 333 positive and 13,320 negative testing samples in our experiment. ", "page_idx": 6}, {"type": "text", "text": "During testing, we show a photo to the model and ask, \u201cCan you see if <sks> is in this photo? Answer with a single word or phrase.\u201d The ground-truth response is \u201cYes\u201d for photos containing $<\\mathtt{s k s}>$ , and \u201cNo\u201d for others. We report the accuracy for positive and negative test images in Table 5. Given the imbalanced test set, we calculate the weighted accuracy: weighted $=0.5*$ accuracy positive $+\\:0.5*$ accuracy negative. ", "page_idx": 6}, {"type": "text", "text": "Table 5 shows results. As expected, the Vanilla LLaVA baseline cannot recognize the personalized subject, an ability not present in it, and we empirically notice that it always answers $\\bf{^{6}N o}$ , it is not <sks>\u201d (thus resulting in 0.5 accuracy). When we prompt it with a short personalized description (whether self-generated or crafted by a human), LLaVA achieves decent accuracy (i.e., 0.819-0.822 with ${\\sim}16$ tokens). On the other hand, overly lengthy descriptions negatively impact its performance (i.e., 0.650 with $1.3\\mathbf{k}$ tokens), likely due to too much side information that may not be helpful (e.g., details about the background). In contrast, Yo\u2019LLaVA displays clear advantages with trainable tokens, achieving the highest accuracy (i.e., 0.924) with the roughly same amount of tokens. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "We also present results using GPT-4V with both text and image prompting. Results indicates that Yo\u2019LLaVA is better than GPT-4V with text prompting (i.e., 0.924 vs. 0.838-0.841). In terms of image prompting, GPT-4V performance improves with more reference images of the subject. Yo\u2019LLaVA with just 16 tokens outperforms GPT-4V with single image prompting ${\\sim}1\\mathrm{k}$ tokens). Also, it is worth noting that Yo\u2019LLaVA, even only with 16 tokens, yields almost comparable results with GPT-4V using $5\\mathrm{k}$ tokens (5 images as reference); see Fig. 4. We anticipate that integrating Yo\u2019LLaVA with GPT-4V could significantly reduce the number of tokens used while maintaining performance; however, we could not try this since GPT-4V is a closed-source framework. ", "page_idx": 7}, {"type": "table", "img_path": "mjGy8g3pgi/tmp/9cefd0d7aba870b8fb15d7104cee145c9dbdd3683a532e36a5002c607bdf7071.jpg", "table_caption": ["Table 5: Comparisons of Yo\u2019LLaVA with LLaVA [2]; GPT- Figure 4: Number of Prompting To4V results with personalized prompt presented as reference. kens vs. Recognition Ability. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "5.2 Question and Answering ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To evaluate the performance of the model on a question-answering task, we develop new benchmarks for both visual and text-only question answering. For the visual component, we present a photo of a <sks> subject and pose questions about it (e.g., \u201cWhere is $\\mathtt{<s k s>?\"}$ ). For the text-only component, we focus on questions concerning the intrinsic visual features of $\\tt{<s k s>}$ (e.g., \u201cIs <sks> a dog or a cat?\u201d). All questions are structured as multiple-choice options (A or B). In total, we create 571 questions; 171/400 visual/text-only questions. Examples are given in Appendix H. We report the accuracy of correctly answering the questions in Tab. 5. ", "page_idx": 7}, {"type": "text", "text": "Yo\u2019LLaVA is the leading method in visual question answering (i.e., 0.929), followed by LLaVA [2] with a human-drafted personalized prompt. Overall, it is evident that if given an image, LMMs can use the presented information to answer questions accurately (e.g., given a photo of a dog, they can correctly identify the color of the dog\u2019s coat without knowing that the dog is named $<\\tt b o>$ ). For text-only question answering, where we do not have a test image and we directly ask questions about the subject, results indicate that text prompt (even by human) may not capture as many details as a trainable prompt, as evidenced by Yo\u2019LLaVA still being the leading method (i.e., accuracy of 0.883) compared with both LLaVA and GPT-4V. When given image(s) as a prompt, GPT-4V can answer all the intrinsic questions very well (i.e., 0.982-0.987). But this is expected, because all the information can be found in the given image. However, it is worth noting that using an image as a personalized prompt requires at least 1k tokens, while Yo\u2019LLaVA uses only 16 tokens! ", "page_idx": 7}, {"type": "text", "text": "5.3 Comparison with MyVLM ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We compare our method with the concurrent work of MyVLM [34] using their dataset, which consists of 29 different objects, and exactly follow their experimental protocol. ", "page_idx": 7}, {"type": "table", "img_path": "mjGy8g3pgi/tmp/e30f359215427c2d7225bb6f36d541abd1f8bd6c0eba12a5951f80258cf7c586.jpg", "table_caption": [], "table_footnote": ["Table 6: Ours vs. MyVLM [34] following the experiment settings in [34]. Yo\u2019LLaVA (Ours) demonstrates advantages over MyVLM without relying on external recognition modules. "], "page_idx": 7}, {"type": "text", "text": "To evaluate a model\u2019s ability to recognize the personalized subject in an image, we utilize the same accuracy metric as MyVLM. If the subject is in the photo, we set the ground truth as \u201cYes\u201d; otherwise, \u201cNo.\u201d We prompt Yo\u2019LLaVA with \u201cCan you see if <sks> is in this photo? Answer with a single word or phrase.\u201d ", "page_idx": 7}, {"type": "image", "img_path": "mjGy8g3pgi/tmp/83e504d03ab5b0e916d01d7c4492bb7e26cc16b4cd50badddf927c9dd49e471c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "mjGy8g3pgi/tmp/1c187a978d71df05854dcc3ee48b50ca07b85d29cebbe36b1e0d8c5fa5445878.jpg", "table_caption": ["Ablation\u00a0Study:\u00a0Effectiveness\u00a0of\u00a0Token\u00a0Length\u00a0and\u00a0Number\u00a0of\u00a0Images ", "Figure 5: Ablation studies on the number of trainable tokens and training images. Overall, the model\u2019s ability to recognize personalized subjects increases as these parameters increase. "], "table_footnote": ["Table 7: Ablation study on dataset creation. To visualize the question-answering ability, a qualitative example of the personalized model for <bo> is shown (Training photos are provided in Fig. 1 ). "], "page_idx": 8}, {"type": "text", "text": "We compare to the reported numbers for MyVLM, which evaluates their concept heads (external face/object recognizers). We also evaluate whether the trained LMM can generate a caption containing the subject\u2019s identifier e.g., <sks>. Following MyVLM, we measure \u201crecall\u201d, which is whether <sks> appears at least once in the generated caption for its image. Table 6 shows the results. Our approach shows clear advantages for both metrics compared to MyVLM, despite being simpler and not relying on an external recognizer (e.g., a face recognizer). ", "page_idx": 8}, {"type": "text", "text": "6 Ablation Studies ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Number of trainable tokens. We set the number of training images per subject at $n=10$ and vary the number of trainable tokens $k$ from 0 to 36. With $k=0$ , training is limited to the identifier token (e.g., <sks>). As shown in Fig. 5 (first row), training just this token yields a low accuracy of $24\\%$ in recognizing the personalized subject. Overall, as the number of latent tokens increases beyond $k=8$ , the model\u2019s ability to recognize personalized objects generally improves for both positive and negative examples. To balance accuracy (higher is better) and token length (lower is more efficient), we select $k=16$ for our final model, which yields $91\\%$ accuracy in this ablation study. ", "page_idx": 8}, {"type": "text", "text": "Number of images. Next, we set the number of trainable tokens to $k=16$ and vary the number of training images from $n=1$ to $n=10$ . Fig. 5 (second row) shows that the model\u2019s recognition ability improves gradually with more photos. We opt for $n=5$ in final version of Yo\u2019LLaVA, as it is the minimum number of training images required to achieve $90\\substack{+\\%}$ accuracy in this ablation study. ", "page_idx": 8}, {"type": "text", "text": "Dataset creation. Finally, we conduct an ablation study on dataset creation. Table 7 presents the weighted accuracy for the recognition task and a qualitative example of a personalized model $<\\tt b o>$ to demonstrate the model\u2019s capability in supporting question answering. Vanilla LLaVA [10] cannot perform either text conversation or recognition (it always answers \u201cNo\u201d to all test images, $50\\%$ ). After training solely on the recognition task (i.e., determining whether $<\\mathtt{s k s}>$ is in a given photo), LLaVA can recognize the subject to some extent (i.e., $70\\%$ ), however, it still cannot perform text conversation tasks. After training with both synthesized conversation and recognition data, both recognition accuracy and conversation ability improve (i.e., $75\\%$ ). Finally, with the introduction of retrieved hard negative examples (Yo\u2019LLaVA), the accuracy is significantly boosted to $91\\%$ . ", "page_idx": 8}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We introduced the novel task of personalizing LLMs, which requires learning visual attributes of a given subject (e.g., a dog named <bo>) from only a handful of images, and then recognizing that subject in new images and performing question answering about that subject when prompted. To tackle this problem, we proposed Yo\u2019LLaVA, where a personalized subject is represented by learnable prompts with an identifier (e.g., <sks>), and a series of $k$ latent tokens (e.g., $<\\mathtt{t o k e n}_{1}>$ ). Experiments showed that Yo\u2019LLaVA can learn the concept more efficiently using fewer tokens and more effectively by capturing more visual attributes compared to strong prompting baselines (e.g., GPT-4 and LLaVA). A promising future direction involves integrating Yo\u2019LLaVA with users\u2019 metadata (e.g., linking personalized concepts about a dog named $<\\tt b o>$ to its medical records or preferences) for enhanced personalization in real-world applications. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements. This work was supported in part by NSF IIS2404180, Adobe Data Science award, Microsoft Accelerate Foundation Models Research Program, and Institute of Information & communications Technology Planning & Evaluation (IITP) grants funded by the Korea government (MSIT) (No. 2022-0-00871, Development of AI Autonomy and Knowledge Enhancement for AI Agent Collaboration) and (No. RS-2022-00187238, Development of Large Korean Language Model Technology for Efficient Pre-training). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] OpenAI. Gpt-4 technical report. In arXiv, 2023.   \n[2] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023.   \n[3] Gemini Team. Gemini: A family of highly capable multimodal models. In arXiv, 2024.   \n[4] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. In arXiv, 2023.   \n[5] Hannah Rose Kirk, Bertie Vidgen, Paul R\u00f6ttger, and Scott A Hale. Personalisation within bounds: A risk taxonomy and policy framework for the alignment of large language models with personalised feedback. In arXiv, 2023.   \n[6] Usman Anwar, Abulhair Saparov, Javier Rando, Daniel Paleka, Miles Turpin, Peter Hase, Ekdeep Singh Lubana, Erik Jenner, Stephen Casper, Oliver Sourbut, Benjamin L. Edelman, Zhaowei Zhang, Mario G\u00fcnther, Anton Korinek, Jose Hernandez-Orallo, Lewis Hammond, Eric Bigelow, Alexander Pan, Lauro Langosco, Tomasz Korbak, Heidi Zhang, Ruiqi Zhong, Se\u00e1n \u00d3 h\u00c9igeartaigh, Gabriel Recchia, Giulio Corsi, Alan Chan, Markus Anderljung, Lilian Edwards, Yoshua Bengio, Danqi Chen, Samuel Albanie, Tegan Maharaj, Jakob Foerster, Florian Tramer, He He, Atoosa Kasirzadeh, Yejin Choi, and David Krueger. Foundational challenges in assuring alignment and safety of large language models. In arXiv, 2024.   \n[7] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion- ${}400\\mathrm{m}$ : Open dataset of clip-filtered 400 million image-text pairs. In arXiv, 2021.   \n[8] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft COCO: Common objects in context. In ECCV, 2014.   \n[9] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In CVPR, 2021.   \n[10] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In arXiv. arXiv:2310.03744, 2023.   \n[11] Daniel Khashabi, Xinxi Lyu, Sewon Min, Lianhui Qin, Kyle Richardson, Sean Welleck, Hannaneh Hajishirzi, Tushar Khot, Ashish Sabharwal, Sameer Singh, and Yejin Choi. Prompt waywardness: The curious case of discretized interpretation of continuous prompts. In ACL, 2022.   \n[12] Yuxin Wen, Neel Jain, John Kirchenbauer, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Hard prompts made easy: Gradient-based discrete optimization for prompt tuning and discovery. In NeurIPS, 2023.   \n[13] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H. Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv, 2022.   \n[14] Thao Nguyen, Yuheng Li, Utkarsh Ojha, and Yong Jae Lee. Visual instruction inversion: Image editing via visual prompting. In NeurIPS, 2023.   \n[15] Cheolhyoung Lee, Kyunghyun Cho, and Wanmo Kang. Mixout: Effective regularization to finetune large-scale pretrained language models. In ICLR, 2020.   \n[16] Yuexiang Zhai, Shengbang Tong, Xiao Li, Mu Cai, Qing Qu, Yong Jae Lee, and Yi Ma. Investigating the catastrophic forgetting in multimodal large language models. In CPAL, 2024.   \n[17] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In ACL, 2021.   \n[18] Shibo Hao, Tianyang Liu, Zhen Wang, and Zhiting Hu. ToolkenGPT: Augmenting frozen language models with massive tools via tool embeddings. In NeurIPS, 2023.   \n[19] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Visual prompt tuning. In ECCV, 2022.   \n[20] Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A unified embedding for face recognition and clustering. In CVPR, 2015.   \n[21] Ben Harwood, Vijay Kumar B G, Gustavo Carneiro, Ian Reid, and Tom Drummond. Smart mining for deep metric learning. In arXiv, 2017.   \n[22] Chao-Yuan Wu, R. Manmatha, Alexander J. Smola, and Philipp Kr\u00e4henb\u00fchl. Sampling matters in deep embedding learning. In arXiv, 2018.   \n[23] Yumin Suh, Bohyung Han, Wonsik Kim, and Kyoung Mu Lee. Stochastic class-based hard example mining for deep metric learning. In CVPR, 2019.   \n[24] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. arXiv preprint arXiv:2303.16199, 2023.   \n[25] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L\u00e9lio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William El Sayed. Mistral 7b. In arXiv, 2023.   \n[26] OpenAI Team. Gpt-4 technical report. 2024.   \n[27] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In ICLR, 2022.   \n[28] Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. arXiv preprint arXiv:2303.16199, 2023.   \n[29] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. arXiv preprint arxiv:2208.12242, 2022.   \n[30] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multiconcept customization of text-to-image diffusion. In CVPR, 2023.   \n[31] Qian Liu, Yihong Chen, Bei Chen, Jian-Guang Lou, Zixuan Chen, Bin Zhou, and Dongmei Zhang. You impress me: Dialogue generation via mutual persona perception. In ACL, 2020.   \n[32] Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela, and Jason Weston. Personalizing dialogue agents: I have a dog, do you have pets too? In Iryna Gurevych and Yusuke Miyao, editors, ACL, 2018.   \n[33] Baolin Peng, Michel Galley, Pengcheng He, Chris Brockett, Lars Liden, Elnaz Nouri, Zhou Yu, Bill Dolan, and Jianfeng Gao. Godel: Large-scale pre-training for goal-directed dialog. In arXiv, 2022.   \n[34] Yuval Alaluf, Elad Richardson, Sergey Tulyakov, Kfir Aberman, and Daniel Cohen-Or. Myvlm: Personalizing vlms for user-specific queries. In arXiv, 2024.   \n[35] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge. 2024.   \n[36] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion5b: An open large-scale dataset for training next generation image-text models. arXiv preprint arXiv:2210.08402, 2022.   \n[37] Romain Beaumont. Clip retrieval: Easily compute clip embeddings and build a clip retrieval system with them, 2022.   \n[38] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \n[39] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with $90\\%*$ chatgpt quality. 2023.   \n[40] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Openclip. 2021. If you use this software, please cite it as below.   \n[41] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. In EMNLP, 2023.   \n[42] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, and Dahua Lin. Mmbench: Is your multi-modal model an all-around player? arXiv:2307.06281, 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Broader Impact ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "The broader impact of Yo\u2019LLaVA, a personalized visual assistant, has potential benefits and risks associated with its deployment and release. Some considerations are unique due to its visual nature, while others share similarities with existing instruction-following LLMs (e.g., Alpaca, Vicuna, etc.). As Yo\u2019LLaVA is built upon LLaVA [2], it inherits some of the issues associated with LLMs and vision encoders (e.g., LLaMA [24], Vicuna [39], and CLIP [40]). In the following, we outline both the risks and mitigation strategies in place for the release of this model. ", "page_idx": 12}, {"type": "text", "text": "Hallucination. Similar to LLMs, Yo\u2019LLaVA might generate outputs that aren\u2019t grounded in facts or input data (Tab. 15). This raises concerns about inferences made, especially in critical applications (e.g., medical). ", "page_idx": 12}, {"type": "text", "text": "Biases. Bias can be transferred from the base models to Yo\u2019LLaVA, both from the vision encoder (CLIP) and the language decoder (LLaMA/Vicuna). This may lead to biased outcomes or unfair representations of diverse content. ", "page_idx": 12}, {"type": "text", "text": "Evaluation complexities. Assessing the performance of Yo\u2019LLaVA is challenging as it involves both language and visual tasks. ", "page_idx": 12}, {"type": "text", "text": "B Catastrophic Forgetting ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Catastrophic forgetting is typically referred to as a scenario in which a neural network (e.g., LMMs) completely or substantially forgets previously acquired knowledge after being trained on a new task. To evaluate the extent of catastrophic forgetting in Yo\u2019LLaVA, we assessed both the Original LLaVA-1.5-13B [10] and Yo\u2019LLaVA using common benchmarks for multimodal models: POPE [41], MMBench [42], and LLaVA-Wild [2]. The results are presented in Table 8. As shown, Yo\u2019LLaVA maintains almost identical performance compared to the Original LLaVA, which is expected, as the all the core weighted of the models are frozen. ", "page_idx": 12}, {"type": "table", "img_path": "mjGy8g3pgi/tmp/10d295a5b662f6d994b49737930793d058168a9c5a6b12155ac1ebc870fdd4e4.jpg", "table_caption": [], "table_footnote": ["Table 8: Catastrophic forgetting evaluation. Overall, Yo\u2019LLaVA retain nearly identical perforamance with Vanilla LLaVA [10], while offers ability to perform personalized conversation. "], "page_idx": 12}, {"type": "text", "text": "C Dataset ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A visualization of our dataset can be found in Table 2. ", "page_idx": 12}, {"type": "text", "text": "For person, we collect micro-influencers (e.g., TikTokers) or personal acquaintances. All pets are personal pets. Landmarks are local landmarks. Objects are obtained from Amazon\u2019s product reviews. Fictional characters are either from movies released in 2023 (e.g., <characterA>) or supporting characters (e.g., <characterC>). All subjects undergo evaluation via LLaVA [10] with the question \u201cDo you know this person/subject?\u201d to ensure LLaVA lacks prior knowledge of the subject. ", "page_idx": 12}, {"type": "text", "text": "D Additional Qualitative Results ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We provide additional qualitative in Tab. 9 (a dog named <butin>), Tab. 10 (a person named $<\\!\\!\\Upsilon\\!>$ ), Tab. 11 (a person named $<\\!\\!\\top\\!>$ ), Tab. 12 (a cat named <mam>), Tab. 13 (a fictional character named <characterC>), and Tab. 14 (a fictional character named <characterE>). ", "page_idx": 13}, {"type": "image", "img_path": "mjGy8g3pgi/tmp/3e0035155153709839d5cf193893a96c1b50dcbf0c8582f1776b91c11c15bcf9.jpg", "img_caption": [], "img_footnote": ["Table 9: Examples of Yo\u2019LLaVA\u2019s abilities with a learned concept <butin>. "], "page_idx": 13}, {"type": "image", "img_path": "mjGy8g3pgi/tmp/a434799265f8a24f32583566e4984d0655101df13d3a8709bd5d376a65340c91.jpg", "img_caption": [], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "mjGy8g3pgi/tmp/a8b35d79dcf638b2bd2ff7a897e44f055f62502284ca46390cd7ea393b9de1bd.jpg", "img_caption": ["\u25b7Visual Conversation "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "mjGy8g3pgi/tmp/c334bccc3d1adeed09157d82ca4a9a6bd2d408c3d943f992675ab542d6dadeed.jpg", "img_caption": [], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "\u2019LLaVA: Yes, I can confirm that $<\\!\\!\\Upsilon\\!>$ is indeed in the photo. He is standing next to another person, holding a bottle of champagne in one hand and a potted plant in the other. They both appear to be in a room with a whiteboard in the background. ", "page_idx": 14}, {"type": "text", "text": "I can confirm that $<\\!\\!\\Upsilon\\!>$ is indeed in the photo. He is standing in the center, wearing a navy blue sweater with a white and red stripe pattern. He is smiling and appears to be enjoying the moment. The table in front of him is fliled with various dishes, including a whole roasted turkey, bowls, and plates of food. There are also vases with flowers on the table, adding a festive touch to the scene. ", "page_idx": 14}, {"type": "text", "text": "\u25b7Visual Conversation ", "text_level": 1, "page_idx": 14}, {"type": "image", "img_path": "mjGy8g3pgi/tmp/cd6cc18b3011529167f4156172d1da2093a83998f6bf659eb164910f8f4400b8.jpg", "img_caption": [], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "mjGy8g3pgi/tmp/bc572b827d69cd0a7e5785931211da73487e749fed0665ad48d1dceb3f73aadb.jpg", "img_caption": ["Table 10: Examples of Yo\u2019LLaVA\u2019s abilities with a learned concept $<\\!\\!\\Upsilon\\!>$ . "], "img_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "mjGy8g3pgi/tmp/7b35f0eeba48bddca935959bf58f4839c3807af686912c7aae5964e62a3b3570.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "mjGy8g3pgi/tmp/9a8b64d56e3f7207051f46b415bbd1d842197d1227a3bf73759a88153b41080d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "mjGy8g3pgi/tmp/a5df319a320c626552feed5c3f7d30b0277c1747fbb962b795a7ce6239937e0f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "mjGy8g3pgi/tmp/9af7a82d75066b75dfe05d25f6a068f6f8b72ed246c7de306df2c79ef031b2ac.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Question: Is $<\\!\\!\\mathrm{T}\\!>$ in this photo? If yes, describe her outfit and location. ", "page_idx": 15}, {"type": "text", "text": "This is an AI generated artwork of $<\\!\\!\\mathrm{T}\\!>$ . Do you think that this is a good illustration of $<\\!\\!\\mathrm{T}\\!>$ . Why or why not? ", "page_idx": 15}, {"type": "text", "text": "Yo\u2019LLaVA: Yes, <T>is in this photo. She is wearing a white top and is standing in the back row, second from the left. ", "page_idx": 15}, {"type": "text", "text": "Yes, I believe the illustration captures $<\\!\\!\\mathrm{T}\\!>$ \u2019s appearance quite well. The character in the image has long dark hair, glasses, and is wearing a cozy sweater, which are all features that match the description of $<\\!\\!\\mathrm{T}\\!>$ . The character is also lying on a bed with a dog and a cat, which adds to the warm and comfortable atmosphere that I imagine $<\\!\\!\\mathrm{T}\\!>$ would enjoy. Overall, I think the illustration does a good job of representing $<\\!\\!\\mathrm{T}\\!>$ . ", "page_idx": 15}, {"type": "text", "text": "\u25b7Visual Conversation ", "text_level": 1, "page_idx": 15}, {"type": "image", "img_path": "mjGy8g3pgi/tmp/e502eac7f0f3157f622d9756c6d728512d8f6dfe8c619b16ca511c6ee14924ed.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "mjGy8g3pgi/tmp/55ba51be6425a44127eb7c87a6325657b32ea0f48ca1b028b8ff7198802bab8c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "mjGy8g3pgi/tmp/791db258f249822f4934a01cd9de4a767d1c62b0409c6a4ff1684e494b2612d1.jpg", "table_caption": [], "table_footnote": ["Table 11: Examples of Yo\u2019LLaVA\u2019s abilities with a learned concept $<\\!\\!\\mathrm{T}\\!>$ . "], "page_idx": 15}, {"type": "image", "img_path": "mjGy8g3pgi/tmp/418b2aaac9db9908b4e348a01864a6f51f006cbb82f1d1b69dee198c11c95414.jpg", "img_caption": ["Table 12: Examples of Yo\u2019LLaVA\u2019s abilities with a learned concept <mam>. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "mjGy8g3pgi/tmp/3ca8873a2746712c6c2acd880f12ada133a75c2edef7d5a7dfeec1b3e4da938a.jpg", "img_caption": ["Yo\u2019LLaVA ", "Table 13: Examples of Yo\u2019LLaVA\u2019s abilities with a learned concept <characterC>. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "mjGy8g3pgi/tmp/f0f41bc3297e08989721b045b7f2fdf98f9565ea625bcd115c6fee777ecc4b7c.jpg", "img_caption": [], "img_footnote": ["Table 14: Examples of Yo\u2019LLaVA\u2019s abilities with a learned concept <characterE>. "], "page_idx": 18}, {"type": "text", "text": "E Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Our approach is not without limitations. First, the accuracy in recognizing very fine-grained details is unsatisfactory, especially when the facial features of a subject are relatively small in the image (see Table 15). Second, due to traits inherited from language models, Yo\u2019LLaVA may generate hallucinations (e.g., provide ungrounded information like a person\u2019s birthday, Table 15). ", "page_idx": 19}, {"type": "image", "img_path": "mjGy8g3pgi/tmp/236b86494145c79a7643bee9878ae713787703265166c7a8be94494327babab2.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "mjGy8g3pgi/tmp/955bdb124384d67fd9a36f20cdfb04b6db65fa9810d77c2ff03fbefafc829574.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "Table 15: Limitation of Yo\u2019LLaVA. ", "page_idx": 19}, {"type": "text", "text": "F Recognition Questions Template ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We present templates for positive and negative question answering in Tables 16 and 17, respectively. Given a training image, we will randomly assign it a template question for answering, forming a training recognition conversation corresponding to either the positive or negative type. There are a total of 30 positive templates and 30 negative templates. ", "page_idx": 20}, {"type": "table", "img_path": "mjGy8g3pgi/tmp/d6d2eb92643cfa10d6f66d5c23efacc218df845654501eeb2185d0d92cd76c06.jpg", "table_caption": ["Table 16: Example of positive recognition question answering. "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "mjGy8g3pgi/tmp/1cc1ea8dd2fe0e73657de2f0b2a91ccce34373fa92fa36f00ad84a9b5a1d3e3f.jpg", "table_caption": ["Table 17: Example of negative recognition question answering. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "G Conversation Data Synthesis Question Template ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The list of question templates used to briefly describe the image content for humans and objects is shown in Tables 18 and 19, respectively. ", "page_idx": 22}, {"type": "text", "text": "We chose to use Riley as a reference to the person because it is a gender-neutral name. For other types of subjects (e.g., objects, pets), we refer to them as this subject. ", "page_idx": 22}, {"type": "text", "text": "After receiving the answer, we replace Riley and this subject with the identifier (e.g., <sks>) to form the training conversation. ", "page_idx": 22}, {"type": "image", "img_path": "mjGy8g3pgi/tmp/908aeccb1fc7cf8392fd1fae40ee42be7397cdfc9fca100daec487f48b3bfe86.jpg", "img_caption": [], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Table 18: List of 10 questions used for conversation synthesis for human. ", "page_idx": 22}, {"type": "image", "img_path": "mjGy8g3pgi/tmp/63a4fe7a5b0e27751ebee485a919e9af8d3332fed1ba7e24bd1dfcf5f2a96daa.jpg", "img_caption": ["Table 19: List of 10 questions used for conversation synthesis for objects and animals. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "H Multiple choices questions answering ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We present a snapshot of multiple-choice question answering for a dog named $<\\tt b o>$ in Table 20. ", "page_idx": 23}, {"type": "image", "img_path": "mjGy8g3pgi/tmp/5ed04f2f469f7140a47650b068e4f91c7d2406d1f2cc4a802b4c4d709dce04fc.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Table 20: Example of multiple choice question answering. ", "page_idx": 23}, {"type": "table", "img_path": "mjGy8g3pgi/tmp/80ba38b8551fde6378d96254bf54a9c431d795dbdbcc20bf392d59eb0cc7bb77.jpg", "table_caption": ["Table 21: Examples of GPT-4V\u2019s generated personalized text descriptions "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "I Visualization of Retrieved Negative Examples ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We visualize the top-100 hard example images retrieved from LAION [7] for a dog named <bo> (Table 22), a stuffed animal <objectF> (Table 23), a cat named <mam> (Table 24), and a person named $\\mathtt{<A>}$ (Table 25). ", "page_idx": 25}, {"type": "image", "img_path": "mjGy8g3pgi/tmp/20f416edadbf664263fb02f120e404d41327f5afe8c5e59f354a2f025713dffe.jpg", "img_caption": ["Table 22: Top 100 hard negative examples for <bo>. "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "mjGy8g3pgi/tmp/a21dd0388410625825151ea3224beae7aa916ba815c2fd21e2105a63d6c3f651.jpg", "img_caption": ["Table 23: Top 100 hard negative examples for <objectF>. "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "mjGy8g3pgi/tmp/1d6877d7f345193e0cb3f22a1ad6240abe7e205d4c3063e6a0dfddf05e554fde.jpg", "img_caption": [], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "", "page_idx": 27}, {"type": "image", "img_path": "mjGy8g3pgi/tmp/dcbc3802bff3a6c7078863b310aae3f39632900cfc21f7eed823a56f241e2fd8.jpg", "img_caption": ["Table 24: Top 100 hard negative examples for <mam>. "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "mjGy8g3pgi/tmp/3f84c7ef01094ce78803f97f457e9fadf4a6deaa9f855a8f38cb941def4217ab.jpg", "img_caption": ["Table 25: Top 100 hard negative examples for $\\mathtt{<A>}$ . "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "mjGy8g3pgi/tmp/793b47fb5e87222a0e1abaf3c5b9fcb924f0e0a27c386aa1e78dc9847b381672.jpg", "img_caption": ["Table 26: Dataset. "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "J GPT-4(V)/ LLaVA Prompts ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "We have listed all the prompts used to invoke GPT-4(V) [1] in Table 27. ", "page_idx": 30}, {"type": "text", "text": "For LLaVA, we employ the same prompts for Image Captioning, Caption Summarization, and Personalized Text Prompting. Since LLaVA does not support conversations involving multiple images, we are unable to conduct experiments on Personalized Image Prompting with LLaVA. ", "page_idx": 30}, {"type": "text", "text": "And example of personalized text description for a selected number of subjects with GPT-4V are given in Tab. 21. ", "page_idx": 30}, {"type": "image", "img_path": "mjGy8g3pgi/tmp/566c2556474e50e17e780fe99f3263735b290903eee1cecc4587011c9daee78d.jpg", "img_caption": ["Table 27: GPT-4V Prompts "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "K Visualizing Learned Tokens ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "It would be interesting to see what information is embedded in latent tokens ${\\tt{<t o k e n}}_{i}{>}$ . To visualize this, we directly ask Yo\u2019LLaVA to Explain ${<}\\mathtt{t o k e n}_{i}{>}^{\\flat}$ . The results for a cat named <mam> and a dog named $<\\tt b o>$ are presented in Tab. 28 (train with $k=16$ tokens). ", "page_idx": 30}, {"type": "text", "text": "Interestingly, each token seems to embed complete sentences about the visual aspects of the training images and the visual characteristics of the subjects (e.g., \u201c<mam> is a cat with a gray and white coat\u201d). We empirically found that while there is duplicate information between these tokens (e.g., \u201c<bo> is a dog\u201d), each token appears to carry distinctive features that other tokens do not have. ", "page_idx": 30}, {"type": "text", "text": "In Tab. 28, we trim the duplicated information and only show the distinctive information that the corresponding tokens carry. This demonstrates that learnable prompts (Yo\u2019LLaVA) not only learn to embed the visual attributes of the subjects into soft-prompts, but also try to distinguish the visual features of the subject. We leave the task of fully explaining and exploring the meaning of these tokens for future research. ", "page_idx": 30}, {"type": "table", "img_path": "mjGy8g3pgi/tmp/fd8659611ee1a2d92129fa6ab149f1662b3898bff3c975ce754a44bf5a890c78.jpg", "table_caption": ["Table 28: We ask Yo\u2019LLaVA to explain each learned token by prompting it with \u201cExplain ${<}\\mathtt{t o k e n}_{i}{>}^{\\flat}$ . "], "table_footnote": [], "page_idx": 31}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We propose a novel task of personalization for Large Multimodal Models and a framework called Yo\u2019LLaVA to achieve this. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 32}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We discuss limitation in Sec. 7 and provide qualitative results for that in Appendix E. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best ", "page_idx": 32}, {"type": "text", "text": "judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 33}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: We do not present any theoretical results. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 33}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: Code and data will be open-sourced. Details about experiments setting can be found in Sec. 4. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.   \n(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: Code and data will be released to public upon acceptance. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 34}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: Details can be found in Sec. 4. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. ", "page_idx": 34}, {"type": "text", "text": "\u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 35}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: Fig. 5 reported variance in accuracy across different categories. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 35}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: Details can be found in Sec. 4. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 35}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We make sure to preserve anonymity (e.g., all figures in paper are blurred faces). ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 36}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: Yes, we discuss Broader Impacts in Sec. A. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 36}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 36}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: Yes, we discuss this in Sec. A. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 36}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 37}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: We cited original paper whenever we use their dataset/ code. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 37}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Justification: Yes, we describe how to collect dataset in Sec. C. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 37}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA]   \nJustification: [NA]   \nGuidelines: \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. \u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 38}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] Justification: [NA] ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 38}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 38}]