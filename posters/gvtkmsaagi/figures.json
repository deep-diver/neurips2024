[{"figure_path": "gVTkMsaaGI/figures/figures_0_1.jpg", "caption": "Figure 1: Sampling densities learned by various posterior inference methods. The prior is a diffusion model sampling a mixture of 25 Gaussians (a) and the posterior is the product of the prior with a constraint that masks all but 9 of the modes (b). Our method (RTB) samples close to the true posterior (c). RL methods with tuned KL regularization yield inaccurate inference (d), while without KL regularization, they mode-collapse (e). A classifier guidance (CG) approximation (f) results in biased outcomes. For details, see \u00a7C.", "description": "This figure compares different posterior inference methods on a two-dimensional Gaussian mixture model.  The goal is to sample from a posterior distribution that is a product of a prior (mixture of 25 Gaussians) and a constraint (masking all but 9 modes). The figure shows that the proposed Relative Trajectory Balance (RTB) method accurately samples from the posterior, while other methods like Reinforcement Learning (RL) with and without KL regularization, and Classifier Guidance (CG) either fail to accurately estimate or mode collapse.", "section": "Background and setting: Diffusion models as hierarchical generative models"}, {"figure_path": "gVTkMsaaGI/figures/figures_6_1.jpg", "caption": "Figure 2: Samples from RTB fine-tuned diffusion posteriors.", "description": "The figure shows samples from posterior models fine-tuned using the proposed Relative Trajectory Balance (RTB) method.  It compares RTB's performance to other methods, including those based on reinforcement learning (RL) with KL regularization, and classifier guidance. The comparison highlights that RTB effectively samples from the true posterior distribution by achieving both high diversity and closeness to true samples of target classes, unlike RL methods with KL regularization (which show inaccurate inference) and those without (which exhibit mode collapse).", "section": "3.1 Class-conditional posterior sampling from unconditional diffusion priors"}, {"figure_path": "gVTkMsaaGI/figures/figures_7_1.jpg", "caption": "Figure 1: Sampling densities learned by various posterior inference methods. The prior is a diffusion model sampling a mixture of 25 Gaussians (a) and the posterior is the product of the prior with a constraint that masks all but 9 of the modes (b). Our method (RTB) samples close to the true posterior (c). RL methods with tuned KL regularization yield inaccurate inference (d), while without KL regularization, they mode-collapse (e). A classifier guidance (CG) approximation (f) results in biased outcomes. For details, see \u00a7C.", "description": "This figure compares different posterior inference methods on a two-dimensional Gaussian mixture model.  The prior (a) shows a mixture of 25 Gaussian distributions. The target posterior (b) is obtained by multiplying the prior with a constraint that keeps only 9 of the modes.  The figure then displays the sampling densities generated by various methods: (c) shows the proposed RTB method, which closely matches the true posterior; (d) shows RL with KL regularization, which is inaccurate; (e) shows RL without KL regularization which leads to mode collapse; (f) illustrates classifier guidance, which results in biased samples. Section C provides additional details.", "section": "Background and setting: Diffusion models as hierarchical generative models"}, {"figure_path": "gVTkMsaaGI/figures/figures_7_2.jpg", "caption": "Figure 1: Sampling densities learned by various posterior inference methods. The prior is a diffusion model sampling a mixture of 25 Gaussians (a) and the posterior is the product of the prior with a constraint that masks all but 9 of the modes (b). Our method (RTB) samples close to the true posterior (c). RL methods with tuned KL regularization yield inaccurate inference (d), while without KL regularization, they mode-collapse (e). A classifier guidance (CG) approximation (f) results in biased outcomes. For details, see \u00a7C.", "description": "This figure compares different posterior inference methods on a 2D Gaussian mixture model.  The prior is a mixture of 25 Gaussians, and the posterior is obtained by masking all but 9 of those Gaussians. The figure shows that the proposed Relative Trajectory Balance (RTB) method accurately samples from the true posterior distribution, while other methods like Reinforcement Learning (RL) with KL regularization and classifier guidance either produce inaccurate or mode-collapsed results.", "section": "Background and setting: Diffusion models as hierarchical generative models"}, {"figure_path": "gVTkMsaaGI/figures/figures_19_1.jpg", "caption": "Figure 1: Sampling densities learned by various posterior inference methods. The prior is a diffusion model sampling a mixture of 25 Gaussians (a) and the posterior is the product of the prior with a constraint that masks all but 9 of the modes (b). Our method (RTB) samples close to the true posterior (c). RL methods with tuned KL regularization yield inaccurate inference (d), while without KL regularization, they mode-collapse (e). A classifier guidance (CG) approximation (f) results in biased outcomes. For details, see \u00a7C.", "description": "This figure compares different posterior inference methods on a two-dimensional Gaussian mixture model.  The prior distribution (a) is a mixture of 25 Gaussian components. A posterior distribution (b) is created by multiplying the prior with a constraint that keeps only 9 of the components. The figure then displays the samples generated by several methods: (c) the proposed Relative Trajectory Balance (RTB) method; (d) Reinforcement Learning (RL) with KL regularization; (e) RL without KL regularization; and (f) Classifier Guidance (CG). The results show that only RTB accurately samples the true posterior while the other methods either produce inaccurate or mode-collapsed results.", "section": "Background and setting: Diffusion models as hierarchical generative models"}, {"figure_path": "gVTkMsaaGI/figures/figures_19_2.jpg", "caption": "Figure 1: Sampling densities learned by various posterior inference methods. The prior is a diffusion model sampling a mixture of 25 Gaussians (a) and the posterior is the product of the prior with a constraint that masks all but 9 of the modes (b). Our method (RTB) samples close to the true posterior (c). RL methods with tuned KL regularization yield inaccurate inference (d), while without KL regularization, they mode-collapse (e). A classifier guidance (CG) approximation (f) results in biased outcomes. For details, see \u00a7C.", "description": "This figure compares different posterior inference methods on a 2D Gaussian mixture model.  The goal is to sample from a posterior distribution that is the product of a prior (a mixture of 25 Gaussians) and a constraint that selects only 9 of the modes.  The figure shows that the proposed method (RTB) effectively samples from the target posterior, unlike other methods such as Reinforcement Learning with and without KL regularization and Classifier Guidance, which either produce inaccurate or mode-collapsed results.", "section": "Background and setting: Diffusion models as hierarchical generative models"}, {"figure_path": "gVTkMsaaGI/figures/figures_19_3.jpg", "caption": "Figure 1: Sampling densities learned by various posterior inference methods. The prior is a diffusion model sampling a mixture of 25 Gaussians (a) and the posterior is the product of the prior with a constraint that masks all but 9 of the modes (b). Our method (RTB) samples close to the true posterior (c). RL methods with tuned KL regularization yield inaccurate inference (d), while without KL regularization, they mode-collapse (e). A classifier guidance (CG) approximation (f) results in biased outcomes. For details, see \u00a7C.", "description": "This figure compares different posterior inference methods on a 2D Gaussian mixture model.  The prior (a) is a mixture of 25 Gaussians. The posterior (b) is obtained by multiplying the prior with a constraint that keeps only 9 of the modes.  The figure shows that Relative Trajectory Balance (RTB) effectively samples from the posterior distribution, while other methods (Reinforcement Learning with and without KL regularization, Classifier Guidance) either produce inaccurate results or suffer from mode collapse.", "section": "Background and setting: Diffusion models as hierarchical generative models"}, {"figure_path": "gVTkMsaaGI/figures/figures_27_1.jpg", "caption": "Figure 1: Sampling densities learned by various posterior inference methods. The prior is a diffusion model sampling a mixture of 25 Gaussians (a) and the posterior is the product of the prior with a constraint that masks all but 9 of the modes (b). Our method (RTB) samples close to the true posterior (c). RL methods with tuned KL regularization yield inaccurate inference (d), while without KL regularization, they mode-collapse (e). A classifier guidance (CG) approximation (f) results in biased outcomes. For details, see \u00a7C.", "description": "This figure compares the sampling densities of different posterior inference methods on a two-dimensional Gaussian mixture model.  The prior distribution is a mixture of 25 Gaussians. The posterior distribution is obtained by multiplying the prior with a constraint that keeps only 9 of the 25 modes. The figure shows that the proposed Relative Trajectory Balance (RTB) method effectively samples from the target posterior distribution.  In contrast, Reinforcement Learning (RL) methods, with or without KL regularization, either mode collapse or produce inaccurate results, while the Classifier Guidance (CG) approximation leads to biased samples.", "section": "2.1 Background and setting: Diffusion models as hierarchical generative models"}, {"figure_path": "gVTkMsaaGI/figures/figures_27_2.jpg", "caption": "Figure 1: Sampling densities learned by various posterior inference methods. The prior is a diffusion model sampling a mixture of 25 Gaussians (a) and the posterior is the product of the prior with a constraint that masks all but 9 of the modes (b). Our method (RTB) samples close to the true posterior (c). RL methods with tuned KL regularization yield inaccurate inference (d), while without KL regularization, they mode-collapse (e). A classifier guidance (CG) approximation (f) results in biased outcomes. For details, see \u00a7C.", "description": "This figure compares different posterior inference methods on a two-dimensional Gaussian mixture model. The prior (a) shows 25 modes, while the true posterior (b) masks all but 9 modes due to a constraint.  The figure demonstrates that the proposed Relative Trajectory Balance (RTB) method accurately samples from the true posterior (c), while other methods either fail (RL, classifier guidance) or mode collapse (RL without regularization).", "section": "Background and setting: Diffusion models as hierarchical generative models"}, {"figure_path": "gVTkMsaaGI/figures/figures_27_3.jpg", "caption": "Figure 1: Sampling densities learned by various posterior inference methods. The prior is a diffusion model sampling a mixture of 25 Gaussians (a) and the posterior is the product of the prior with a constraint that masks all but 9 of the modes (b). Our method (RTB) samples close to the true posterior (c). RL methods with tuned KL regularization yield inaccurate inference (d), while without KL regularization, they mode-collapse (e). A classifier guidance (CG) approximation (f) results in biased outcomes. For details, see \u00a7C.", "description": "This figure compares different posterior inference methods on a two-dimensional Gaussian mixture model.  The goal is to sample from a posterior distribution that's a product of a prior distribution (a mixture of 25 Gaussians) and a constraint function that keeps only 9 of the modes.  The figure shows that the proposed Relative Trajectory Balance (RTB) method accurately samples from this posterior. In contrast, Reinforcement Learning (RL) methods, with or without KL regularization, produce either inaccurate or mode-collapsed results.  Classifier guidance also leads to biased sampling.", "section": "Background and setting: Diffusion models as hierarchical generative models"}, {"figure_path": "gVTkMsaaGI/figures/figures_27_4.jpg", "caption": "Figure 1: Sampling densities learned by various posterior inference methods. The prior is a diffusion model sampling a mixture of 25 Gaussians (a) and the posterior is the product of the prior with a constraint that masks all but 9 of the modes (b). Our method (RTB) samples close to the true posterior (c). RL methods with tuned KL regularization yield inaccurate inference (d), while without KL regularization, they mode-collapse (e). A classifier guidance (CG) approximation (f) results in biased outcomes. For details, see \u00a7C.", "description": "This figure compares different posterior inference methods on a two-dimensional Gaussian mixture model.  The goal is to sample from a posterior distribution (panel b) that is obtained by multiplying the prior distribution (a) by a constraint function that masks most of the modes. The figure demonstrates the sampling densities resulting from several methods: (c) Relative Trajectory Balance (RTB), (d) Reinforcement Learning (RL) with KL-regularization, (e) RL without KL-regularization, (f) classifier guidance. RTB shows the closest approximation to the true posterior density.", "section": "Background and setting: Diffusion models as hierarchical generative models"}, {"figure_path": "gVTkMsaaGI/figures/figures_27_5.jpg", "caption": "Figure 1: Sampling densities learned by various posterior inference methods. The prior is a diffusion model sampling a mixture of 25 Gaussians (a) and the posterior is the product of the prior with a constraint that masks all but 9 of the modes (b). Our method (RTB) samples close to the true posterior (c). RL methods with tuned KL regularization yield inaccurate inference (d), while without KL regularization, they mode-collapse (e). A classifier guidance (CG) approximation (f) results in biased outcomes. For details, see \u00a7C.", "description": "This figure compares the sampling densities produced by different posterior inference methods on a mixture of 25 Gaussians. The goal is to sample from a posterior distribution obtained by multiplying the prior (a mixture of 25 Gaussians) with a constraint that keeps only 9 of the modes.  The figure shows that RTB effectively samples the true posterior, while other methods (RL with/without KL regularization, classifier guidance) either fail to accurately represent the posterior or suffer from mode collapse or biased sampling.", "section": "Background and setting: Diffusion models as hierarchical generative models"}, {"figure_path": "gVTkMsaaGI/figures/figures_27_6.jpg", "caption": "Figure 2: Samples from RTB fine-tuned diffusion posteriors.", "description": "This figure shows samples generated from posterior models fine-tuned using the Relative Trajectory Balance (RTB) method.  It compares RTB's performance to other methods on MNIST and CIFAR-10 datasets for class-conditional image generation.  The results illustrate RTB's ability to generate diverse and high-quality samples while maintaining adherence to the target class, unlike other methods which demonstrate mode collapse or biased sampling.", "section": "3.1 Class-conditional posterior sampling from unconditional diffusion priors"}, {"figure_path": "gVTkMsaaGI/figures/figures_27_7.jpg", "caption": "Figure 1: Sampling densities learned by various posterior inference methods. The prior is a diffusion model sampling a mixture of 25 Gaussians (a) and the posterior is the product of the prior with a constraint that masks all but 9 of the modes (b). Our method (RTB) samples close to the true posterior (c). RL methods with tuned KL regularization yield inaccurate inference (d), while without KL regularization, they mode-collapse (e). A classifier guidance (CG) approximation (f) results in biased outcomes. For details, see \u00a7C.", "description": "This figure compares different posterior inference methods on a two-dimensional Gaussian mixture model.  It shows that the proposed Relative Trajectory Balance (RTB) method accurately samples from the target posterior distribution, whereas other methods like Reinforcement Learning (RL) with or without KL regularization, and Classifier Guidance (CG) fail to do so, either producing inaccurate results or suffering from mode collapse.", "section": "2.1 Background and setting: Diffusion models as hierarchical generative models"}, {"figure_path": "gVTkMsaaGI/figures/figures_27_8.jpg", "caption": "Figure 1: Sampling densities learned by various posterior inference methods. The prior is a diffusion model sampling a mixture of 25 Gaussians (a) and the posterior is the product of the prior with a constraint that masks all but 9 of the modes (b). Our method (RTB) samples close to the true posterior (c). RL methods with tuned KL regularization yield inaccurate inference (d), while without KL regularization, they mode-collapse (e). A classifier guidance (CG) approximation (f) results in biased outcomes. For details, see \u00a7C.", "description": "This figure compares different posterior inference methods on a two-dimensional Gaussian mixture model. The goal is to sample from a posterior distribution that is the product of a prior distribution (a mixture of 25 Gaussians) and a constraint function. The figure shows that the proposed Relative Trajectory Balance (RTB) method accurately samples from the posterior, while other methods like Reinforcement Learning (RL) with and without KL regularization, and Classifier Guidance (CG) fail to do so, either due to inaccurate inference, mode collapse, or biased sampling.", "section": "2.1 Background and setting: Diffusion models as hierarchical generative models"}, {"figure_path": "gVTkMsaaGI/figures/figures_28_1.jpg", "caption": "Figure 1: Sampling densities learned by various posterior inference methods. The prior is a diffusion model sampling a mixture of 25 Gaussians (a) and the posterior is the product of the prior with a constraint that masks all but 9 of the modes (b). Our method (RTB) samples close to the true posterior (c). RL methods with tuned KL regularization yield inaccurate inference (d), while without KL regularization, they mode-collapse (e). A classifier guidance (CG) approximation (f) results in biased outcomes. For details, see \u00a7C.", "description": "This figure compares different posterior inference methods on a two-dimensional Gaussian mixture model.  The goal is to sample from a target posterior distribution that is the product of a prior distribution (a mixture of 25 Gaussians) and a constraint that selects only 9 of the modes.  The figure shows that the proposed Relative Trajectory Balance (RTB) method accurately samples from the target posterior, while several alternative methods (reinforcement learning with and without KL regularization, and classifier guidance) fail to do so, either producing inaccurate samples or suffering from mode collapse (where only a subset of the modes are sampled).", "section": "Background and setting: Diffusion models as hierarchical generative models"}, {"figure_path": "gVTkMsaaGI/figures/figures_28_2.jpg", "caption": "Figure 1: Sampling densities learned by various posterior inference methods. The prior is a diffusion model sampling a mixture of 25 Gaussians (a) and the posterior is the product of the prior with a constraint that masks all but 9 of the modes (b). Our method (RTB) samples close to the true posterior (c). RL methods with tuned KL regularization yield inaccurate inference (d), while without KL regularization, they mode-collapse (e). A classifier guidance (CG) approximation (f) results in biased outcomes. For details, see \u00a7C.", "description": "This figure compares different posterior inference methods on a two-dimensional Gaussian mixture model.  The prior distribution is a mixture of 25 Gaussians, and the posterior is obtained by multiplying this prior with a constraint that keeps only 9 of the modes.  The figure visualizes the sample densities produced by different methods: the true posterior, the proposed Relative Trajectory Balance (RTB) method, Reinforcement Learning (RL) methods with and without KL regularization, and Classifier Guidance (CG). RTB is shown to better approximate the true posterior compared to other methods.", "section": "Background and setting: Diffusion models as hierarchical generative models"}, {"figure_path": "gVTkMsaaGI/figures/figures_28_3.jpg", "caption": "Figure 1: Sampling densities learned by various posterior inference methods. The prior is a diffusion model sampling a mixture of 25 Gaussians (a) and the posterior is the product of the prior with a constraint that masks all but 9 of the modes (b). Our method (RTB) samples close to the true posterior (c). RL methods with tuned KL regularization yield inaccurate inference (d), while without KL regularization, they mode-collapse (e). A classifier guidance (CG) approximation (f) results in biased outcomes. For details, see \u00a7C.", "description": "This figure compares different posterior inference methods on a 2D Gaussian mixture model.  The goal is to sample from a posterior distribution created by multiplying a prior (a mixture of 25 Gaussians) with a constraint that selects only 9 of the modes.  The figure shows that the proposed method (RTB) effectively samples from the target posterior, while other methods (RL with/without KL regularization, Classifier Guidance) either produce inaccurate results or suffer from mode collapse.", "section": "Background and setting: Diffusion models as hierarchical generative models"}, {"figure_path": "gVTkMsaaGI/figures/figures_28_4.jpg", "caption": "Figure 1: Sampling densities learned by various posterior inference methods. The prior is a diffusion model sampling a mixture of 25 Gaussians (a) and the posterior is the product of the prior with a constraint that masks all but 9 of the modes (b). Our method (RTB) samples close to the true posterior (c). RL methods with tuned KL regularization yield inaccurate inference (d), while without KL regularization, they mode-collapse (e). A classifier guidance (CG) approximation (f) results in biased outcomes. For details, see \u00a7C.", "description": "This figure compares different posterior inference methods on a two-dimensional Gaussian mixture model.  It shows that the proposed Relative Trajectory Balance (RTB) method accurately samples from the target posterior distribution, unlike other methods such as Reinforcement Learning (RL) with and without KL regularization, and Classifier Guidance (CG), which suffer from inaccurate inference or mode collapse.", "section": "Background and setting: Diffusion models as hierarchical generative models"}, {"figure_path": "gVTkMsaaGI/figures/figures_28_5.jpg", "caption": "Figure 1: Sampling densities learned by various posterior inference methods. The prior is a diffusion model sampling a mixture of 25 Gaussians (a) and the posterior is the product of the prior with a constraint that masks all but 9 of the modes (b). Our method (RTB) samples close to the true posterior (c). RL methods with tuned KL regularization yield inaccurate inference (d), while without KL regularization, they mode-collapse (e). A classifier guidance (CG) approximation (f) results in biased outcomes. For details, see \u00a7C.", "description": "This figure compares different posterior inference methods on a two-dimensional Gaussian mixture model.  The methods are evaluated on their ability to sample from a posterior distribution defined by a product of a prior distribution (a mixture of 25 Gaussians) and a constraint that keeps only 9 of the modes. The figure shows that the proposed Relative Trajectory Balance (RTB) method accurately samples from the posterior distribution, while other methods such as Reinforcement Learning (RL) with and without KL regularization, and Classifier Guidance (CG), fail to accurately sample the distribution.", "section": "2.1 Background and setting: Diffusion models as hierarchical generative models"}]