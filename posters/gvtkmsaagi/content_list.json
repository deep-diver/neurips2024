[{"type": "text", "text": "Amortizing intractable inference in diffusion models for vision, language, and control ", "text_level": 1, "page_idx": 0}, {"type": "image", "img_path": "gVTkMsaaGI/tmp/904ffc302713f1fab8b3f8b1d4eb950bce0d4d86e5fc20e6e1e10f21c6615244.jpg", "img_caption": [], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Diffusion models have emerged as effective distribution estimators in vision, language, and reinforcement learning, but their use as priors in downstream tasks poses an intractable posterior inference problem. This paper studies amortized sampling of the posterior over data, $\\mathbf{x}\\sim p^{\\mathrm{post}}(\\mathbf{x})\\propto p(\\mathbf{x})r(\\mathbf{x})$ , in a model that consists of a diffusion generative model prior $p(\\mathbf{x})$ and a black-box constraint or likelihood function $r(\\mathbf{x})$ . We state and prove the asymptotic correctness of a data-free learning objective, relative trajectory balance, for training a diffusion model that samples from this posterior, a problem that existing methods solve only approximately or in restricted cases. Relative trajectory balance arises from the generative flow network perspective on diffusion models, which allows the use of deep reinforcement learning techniques to improve mode coverage. We illustrate the broad potential of unbiased inference of arbitrary posteriors under diffusion priors across a collection of experiments: in vision (classifier guidance), language (infliling under a discrete diffusion LLM), and multimodal data (text-to-image generation). Beyond generative modeling, we apply relative trajectory balance to the problem of continuous control with a score-based behavior prior, achieving state-of-the-art results on benchmarks in offline reinforcement learning. Code is available at this link. ", "page_idx": 0}, {"type": "table", "img_path": "gVTkMsaaGI/tmp/21fdac2b3d24c88de11bedd3a4aefe28ef3e9c12175ffcbd602e97c2bd619992.jpg", "table_caption": ["Table 1: Sources of diffusion priors and constraints. "], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Diffusion models [67, 26, 71] are a powerful class of hierarchical generative models, used to model complex distributions over images [50, 11, 62], text [4, 12, 39, 23, 22, 42], and actions in reinforcement learning [29, 82, 31] to name a few. In each of these domains, downstream problems require sampling product distributions, where a pretrained diffusion model serves as a prior $p(\\mathbf x)$ that is multiplied by an auxiliary constraint $r(\\mathbf{x})$ . For example, if $p(\\mathbf x)$ is a prior over images defined by a diffusion model, and $r(\\mathbf{x})\\;=\\;p(c\\;\\mid\\;\\mathbf{x})$ is the likelihood that an image $\\mathbf{X}$ belongs to class $c$ , then class-conditional image generation requires sampling from the Bayesian posterior $p(\\mathbf{x}\\mid c)\\propto p(\\mathbf{x})p(c\\mid\\mathbf{x})$ . In offline reinforcement learning, if $\\mu(a\\mid s)$ is a conditional diffusion model over actions serving as a behavior policy, KL-constrained policy improvement [54, 43] requires sampling from the normalized product of $\\mu(a\\mid s)$ with a Boltzmann distribution defined by a $Q$ -function, $\\pi^{*}(a\\mid s)\\propto\\mu(a\\mid s)\\exp(\\beta Q(s,a))$ . In language modeling, various conditional generation problems [42, 22, 28] amount to posterior sampling under a discrete diffusion model prior. Table 1 summarizes four such problems that the proposed method improves upon prior work. ", "page_idx": 1}, {"type": "text", "text": "The hierarchical nature of the generative process in diffusion models, which generate samples from $p(\\mathbf x)$ by a deep chain of stochastic transformations, makes exact sampling from posteriors $p(\\mathbf{x})r(\\mathbf{x})$ under a black-box function $r(\\mathbf{x})$ intractable. Common solutions to this problem involve inference techniques based on linear approximations [72, 32, 30, 10] or stochastic optimization [21, 47]. Others estimate the \u2018guidance\u2019 term \u2013 the difference in drift functions between the diffusion models sampling the prior and posterior \u2013 by training a classifier on noised data [11], but when such data is not available, one must resort to approximations or Monte Carlo estimates [69, 13, 9], which are challenging to scale to high-dimensional problems. Reinforcement learning methods that have recently been proposed for this problem [7, 15] are biased and prone to mode collapse (Fig. 1). ", "page_idx": 1}, {"type": "text", "text": "Contributions. Inspired by recent techniques in training diffusion models to sample distributions defined by unnormalized densities [88, 61, 77, 64], we propose an asymptotically unbiased training objective, called relative trajectory balance (RTB), for training diffusion models that sample from posterior distributions under a diffusion model prior $(\\S2.2)$ . RTB is derived from the perspective of diffusion models as continuous generative flow networks [37]. This perspective also allows us to freely leverage off-policy training, when data with high density under the posterior is available (\u00a72.3). RTB can be applied to iterative generative processes beyond standard diffusion models: our methods generalize to discrete diffusion models and extend existing methods for autoregressive language models (\u00a72.4). ", "page_idx": 1}, {"type": "text", "text": "Our experiments demonstrate the versatility of our approach in a variety of domains: ", "page_idx": 1}, {"type": "text", "text": "\u2022 In vision, we show that RTB achieves competitive classifier-guided image generation for unconditional diffusion vision priors (\u00a73.1) and can be used to improve caption-conditioned generation under text-to-image foundation model priors (\u00a73.2).   \n\u2022 In language modeling, we report strong results for infliling tasks with discrete diffusion language models (\u00a73.3).   \n\u2022 Finally, we show that RTB achieves state-of-the-art results on continuous control benchmarks that leverage score-based behavior priors (\u00a73.4). ", "page_idx": 1}, {"type": "text", "text": "2 Learning posterior samplers with diffusion priors ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We consider the problem of posterior inference under a prior given by a hierarchical generative model. In this section, we present the mathematical setting (\u00a72.1), our proposed RTB objective $(\\S2.2)$ , and training methods for RTB (\u00a72.3). We will first discuss the case of a diffusion prior over $\\mathbb{R}^{d}$ , and later discuss how the methods generalize to arbitrary hierarchical priors (\u00a72.4). ", "page_idx": 1}, {"type": "text", "text": "Figure 1: Sampling densities learned by various posterior inference methods. The prior is a diffusion model sampling a mixture of 25 Gaussians (a) and the posterior is the product of the prior with a constraint that masks all but 9 of the modes (b). Our method (RTB) samples close to the true posterior (c). RL methods with tuned KL regularization yield inaccurate inference (d), while without KL regularization, they mode-collapse (e). A classifier guidance (CG) approximation (f) results in biased outcomes. For details, see $\\S C$ . ", "page_idx": 2}, {"type": "text", "text": "2.1 Background and setting: Diffusion models as hierarchical generative models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "A denoising diffusion model generates data $\\mathbf{X}_{1}$ by a Markovian generative process: ", "page_idx": 2}, {"type": "equation", "text": "$$\n(n o i s e)\\quad\\mathbf{x}_{0}\\rightarrow\\mathbf{x}_{\\Delta t}\\rightarrow\\mathbf{x}_{2\\Delta t}\\rightarrow\\hdots\\rightarrow\\mathbf{x}_{1}=\\mathbf{x}\\quad(d a t a),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\begin{array}{r}{\\Delta t=\\frac{1}{T}}\\end{array}$ and $T$ is the number of discretization steps.1 The initial distribution $p(\\mathbf{x}_{0})$ is fixed (typically to ${\\dot{N}}(\\mathbf{0},I))$ and the transition from $\\mathbf X_{t-1}$ to $\\mathbf{X}_{t}$ is modeled as a Gaussian perturbation with time-dependent variance: ", "page_idx": 2}, {"type": "equation", "text": "$$\np(\\mathbf{x}_{t+\\Delta t}\\mid\\mathbf{x}_{t})=N(\\mathbf{x}_{t+\\Delta t}\\mid\\mathbf{x}_{t}+u_{t}(\\mathbf{x}_{t})\\Delta t,\\sigma_{t}^{2}\\Delta t I).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The scaling of the mean and variance by $\\Delta t$ is insubstantial for fixed $T$ , but ensures that the diffusion process is well-defined in the limit $T\\to\\infty$ assuming regularity conditions on $u_{t}$ [52, 63]. The process given by $(1,2)$ is then identical to Euler-Maruyama integration of the stochastic differential equation (SDE) $d\\mathbf{x}_{t}=u_{t}\\left(\\mathbf{x}_{t}\\right)d t+\\sigma_{t}\\,d\\mathbf{w}_{t}$ . ", "page_idx": 2}, {"type": "text", "text": "The likelihood of a denoising trajectory $\\mathbf{x}_{0}\\rightarrow\\mathbf{x}_{\\Delta t}\\rightarrow\\dots\\rightarrow\\mathbf{x}_{1}$ factors as ", "page_idx": 2}, {"type": "equation", "text": "$$\np(\\mathbf{x}_{0},\\mathbf{x}_{\\Delta t},\\ldots,\\mathbf{x}_{1})=p(\\mathbf{x}_{0})\\prod_{i=1}^{T}p(\\mathbf{x}_{i\\Delta t}\\mid\\mathbf{x}_{(i-1)\\Delta t})\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "and defines a marginal density over the data space: ", "page_idx": 2}, {"type": "equation", "text": "$$\np(\\mathbf{x}_{1})=\\int\\,p(\\mathbf{x}_{0},\\mathbf{x}_{\\Delta t},\\dots,\\mathbf{x}_{1})\\,d\\mathbf{x}_{0}\\,d\\mathbf{x}_{\\Delta t}\\dots d\\mathbf{x}_{1-\\Delta t}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "A reverse-time process, $\\mathbf{x}_{1}\\longrightarrow\\mathbf{x}_{1-\\Delta t}\\longrightarrow\\cdots\\longrightarrow\\mathbf{x}_{0}$ , with densities $q$ , can be defined analogously, and similarly defines a conditional density over trajectories: ", "page_idx": 2}, {"type": "equation", "text": "$$\nq(\\mathbf{x}_{0},\\mathbf{x}_{\\Delta t},\\ldots,\\mathbf{x}_{1-\\Delta t}\\mid\\mathbf{x}_{1})=\\prod_{i=1}^{T}q(\\mathbf{x}_{(i-1)\\Delta t}\\mid\\mathbf{x}_{i\\Delta t}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "In the training of diffusion models, as discussed below, the process $q$ is typically fixed to a simple distribution (usually a discretized Ornstein-Uhlenbeck process), and the result of training is that $p$ and $q$ are close as distributions over trajectories. ", "page_idx": 2}, {"type": "text", "text": "Diffusion model training as divergence minimization. Diffusion models parametrize the drift $u_{t}\\left(\\mathbf{x}_{t}\\right)$ in (Equation 2) as a neural network $u(\\mathbf{x}_{t},t;\\theta)$ with parameters $\\theta$ and taking $\\mathbf{X}_{t}$ and $t$ as input. We denote the distributions over trajectories induced by (Equation 3, Equation 4) by $p_{\\theta}$ to show their dependence on the parameter. ", "page_idx": 2}, {"type": "text", "text": "In the most common setting, diffusion models are trained to maximize the likelihood of a dataset. In the notation above, this corresponds to assuming $q(\\mathbf{x}_{1})$ is fixed to an empirical measure (with the points of a training dataset $\\mathcal{D}$ assumed to be i.i.d. samples from $q(\\mathbf{x}_{1}))$ . Training minimizes with respect to $\\theta$ the divergence between the processes $q$ and $p_{\\theta}$ : ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{D_{\\mathrm{KL}}(q(\\mathbf{x}_{0},\\mathbf{x}_{\\Delta t},\\ldots,\\mathbf{x}_{1})\\parallel p_{\\theta}(\\mathbf{x}_{0},\\mathbf{x}_{\\Delta t},\\ldots,\\mathbf{x}_{1}))}&{\\quad{\\mathrm{()}}}\\\\ &{=D_{\\mathrm{KL}}(q(\\mathbf{x}_{1})\\parallel p_{\\theta}(\\mathbf{x}_{1}))+\\mathbb{B}_{\\mathbf{x}_{1}\\sim q(\\mathbf{x}_{1})}D_{\\mathrm{KL}}(q(\\mathbf{x}_{0},\\mathbf{x}_{\\Delta t},\\ldots,\\mathbf{x}_{1-\\Delta t}\\mid\\mathbf{x}_{1})\\parallel p_{\\theta}(\\mathbf{x}_{0},\\mathbf{x}_{\\Delta t},\\ldots,\\mathbf{x}_{1-\\Delta t}\\mid\\mathbf{x}_{1}))}\\\\ &{\\geq D_{\\mathrm{KL}}(q(\\mathbf{x}_{1})\\parallel p_{\\theta}(\\mathbf{x}_{1}))=\\mathbb{B}_{\\mathbf{x}_{1}\\sim q(\\mathbf{x}_{1})}[-\\log p_{\\theta}(\\mathbf{x}_{1})]+{\\mathrm{const.}}}&\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the inequality \u2013 an instance of the data processing inequality for the KL divergence \u2013 shows that minimizing the divergence between distributions over trajectories is equivalent to maximizing a lower bound on the data log-likelihood under the model $p_{\\theta}$ . ", "page_idx": 3}, {"type": "text", "text": "As shown in [70], minimization of the KL in (Equation 6) is essentially equivalent to the traditional approach to training diffusion models via denoising score matching [79, 67, 26]. Such training exploits that for typical choices of the noising process $q$ , the optimal $u_{t}\\left(\\mathbf{x}_{t}\\right)$ can be expressed in terms of the Stein score of $q({\\bf x}_{1})$ convolved with a Gaussian, allowing an efficient stochastic regression objective for $u_{t}$ . For full generality of our exposition for arbitrary iterative generative processes, we prefer to think of (Equation 6) as the primal objective and denoising score matching as an efficient means of minimizing it. ", "page_idx": 3}, {"type": "text", "text": "Trajectory balance and distribution-matching training. From (Equation 6) we also see that the bound is tight if the conditionals of $p_{\\theta}$ and $q$ on $\\mathbf{X}_{1}$ coincide, i.e., $q$ is equal to the posterior distribution of $p$ conditioned on $\\mathbf{X}_{1}$ . Indeed, the model $p_{\\theta}$ minimizes (Equation 6) for a distribution with continuous density $q(\\mathbf{x}_{1})$ if and only if, for all denoising trajectories, ", "page_idx": 3}, {"type": "equation", "text": "$$\np_{\\theta}(\\mathbf{x}_{0},\\mathbf{x}_{\\Delta t},\\mathbf{\\phi},\\mathbf{\\phi}.\\dots,\\mathbf{x}_{1})=q(\\mathbf{x}_{1})q(\\mathbf{x}_{0},\\mathbf{x}_{\\Delta t},\\dots,\\mathbf{x}_{1-\\Delta t}\\mid\\mathbf{x}_{1}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "This was named the trajectory balance $(T B)$ constraint by [37] \u2013 by analogy with a constraint for discrete-space iterative sampling [45] \u2013 and is a time-discretized version of a constraint used for enforcing equality of continuous-time path space measures in [51]. ", "page_idx": 3}, {"type": "text", "text": "In [60, 37], the constraint (7) was used for the training of diffusion models in a data-free setting, where instead of i.i.d. samples from $q(\\mathbf{x}_{1})$ one has access to a (possibly unnormalized) density $q(\\mathbf{x}_{1})\\ =\\ e^{-\\mathcal{E}(\\mathbf{x}_{1})}/Z$ from which one wishes to sample. These objectives minimize the squared log-ratio between the two sides of (7), which allows the trajectories $\\mathbf{x}_{0}\\rightarrow\\mathbf{x}_{\\Delta t}\\rightarrow\\dots\\rightarrow\\mathbf{x}_{1}$ used for training to be sampled from any training distribution, such as \u2018exploratory\u2019 modifications of $p_{\\theta}$ or trajectories found by local search (MCMC) in the target space. The flexibility of off-policy exploration that this allows was studied by [64]. Such objectives contrast with on-policy, simulationbased approaches that require differentiating through the sampling process [e.g., 88, 77, 6, 78]. ", "page_idx": 3}, {"type": "text", "text": "2.2 Intractable inference under diffusion priors ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Consider a diffusion model $p_{\\theta}$ , defining a marginal density $p_{\\theta}(\\mathbf{x}_{1})$ , and a positive constraint function $r:\\mathbb{R}^{d}\\to\\mathbb{R}_{>0}$ . We are interested in training a diffusion model $p_{\\phi}^{\\mathrm{post}}$ , with drift function $u_{\\phi}^{\\mathrm{post}}$ , that would sample the product distribution $p^{\\mathrm{post}}(\\mathbf{x}_{1})\\propto p_{\\theta}(\\mathbf{x}_{1})r(\\mathbf{x}_{1})$ . If $r(\\mathbf{x}_{1})=p(\\mathbf{y}\\mid\\mathbf{x}_{1})$ is a conditional distribution over another variable y, then $p^{\\mathrm{post}}$ is the Bayesian posterior $p_{\\theta}(\\mathbf{x}_{1}\\mid\\mathbf{y})$ . ", "page_idx": 3}, {"type": "text", "text": "Because samples from $p^{\\mathrm{post}}(\\mathbf{x}_{1})$ are not assumed to be available, one cannot directly train $p$ using the objective (6). Nor can one directly apply objectives for distribution-matching training, such as those that enforce (7), since the marginal $p_{\\theta}(\\mathbf{x}_{1})$ is not available. However, we make the following observation (proof in $\\S\\mathrm{A}$ ). ", "page_idx": 3}, {"type": "text", "text": "Proposition 1 (Relative TB constraint). If $\\dot{\\boldsymbol{p}}_{\\theta},\\,p_{\\phi}^{\\mathrm{post}}$ , and the scalar $Z_{\\phi}$ jointly satisfy the relative trajectory balance (RTB) constraint ", "page_idx": 3}, {"type": "equation", "text": "$$\nZ_{\\phi}\\cdot p_{\\phi}^{\\mathrm{post}}({\\bf x}_{0},{\\bf x}_{\\Delta t},\\ldots,{\\bf x}_{1})=r({\\bf x}_{1})p_{\\theta}({\\bf x}_{0},{\\bf x}_{\\Delta t},\\ldots,{\\bf x}_{1})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "for every denoising trajectory $\\mathbf{x}_{0}\\rightarrow\\mathbf{x}_{\\Delta t}\\rightarrow\\dots\\rightarrow\\mathbf{x}_{1}$ , then $p_{\\phi}^{\\mathrm{post}}(\\mathbf{x}_{1})\\propto p_{\\theta}(\\mathbf{x}_{1})r(\\mathbf{x}_{1}),$ , i.e., the diffusion model $p_{\\phi}^{\\mathrm{post}}$ samples the posterior distribution. Furthermore, if $p_{\\theta}$ also satisfies the TB constraint (7) with respect to the noising process \ud835\udc5eand some target density $q(\\mathbf{x}_{1})$ , then $p_{\\phi}^{\\mathrm{post}}$ satisfies the TB constraint with respect to the target density $q^{\\mathrm{post}}(\\mathbf{x}_{1})\\propto q(\\mathbf{x}_{1})r(\\mathbf{x}_{1})$ , and $\\begin{array}{r}{Z=\\int q(\\mathbf{x}_{1})r(\\mathbf{x}_{1})\\,d\\mathbf{x}_{1}}\\end{array}$ . ", "page_idx": 3}, {"type": "text", "text": "Note that the two joints appearing in (8) are defined as products over transitions, via (3). ", "page_idx": 3}, {"type": "text", "text": "Relative trajectory balance as a loss. Analogously to the conversion of the TB constraint (7) into a trajectory-dependent training objective in [45, 37], we define the relative trajectory balance loss as the discrepancy between the two sides of (8), seen as a function of the vector $\\phi$ that parametrizes the posterior diffusion model and the scalar $Z_{\\phi}$ (parametrized via $\\log{Z_{\\phi}}$ for numerical stability): ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{RTB}}(\\mathbf{x}_{0}\\rightarrow\\mathbf{x}_{\\Delta t}\\rightarrow\\dots\\rightarrow\\mathbf{x}_{1};\\boldsymbol{\\phi}):=\\left(\\log\\frac{Z_{\\boldsymbol{\\phi}}\\cdot p_{\\phi}^{\\mathrm{post}}(\\mathbf{x}_{0},\\mathbf{x}_{\\Delta t},\\dots,\\mathbf{x}_{1})}{r(\\mathbf{x}_{1})p_{\\theta}(\\mathbf{x}_{0},\\mathbf{x}_{\\Delta t},\\dots,\\mathbf{x}_{1})}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Optimizing this objective to 0 for all trajectories ensures that (8) is satisfied. While the RTB constraint (8) has a similar form to TB (7), RTB involves the ratio of two denoising processes, while TB involves the ratio of a forward and a backward process. However, the name \u2018relative TB\u2019 is justified by interpreting the densities in a TB constraint relative to a measure defined by the prior model; see $\\S2.4$ . ", "page_idx": 4}, {"type": "text", "text": "If we assume $p_{\\theta}(\\mathbf{x}_{0})=p_{\\phi}^{\\mathrm{post}}(\\mathbf{x}_{0})$ are fixed (e.g., to a standard normal), then (9) reduces to ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\left(\\log\\frac{Z_{\\phi}}{r(\\mathbf{x}_{1})}+\\sum_{i=1}^{T}\\log\\frac{p_{\\phi}^{\\mathrm{post}}(\\mathbf{x}_{i\\Delta t}\\mid\\mathbf{x}_{(i-1)\\Delta t})}{p_{\\theta}(\\mathbf{x}_{i\\Delta t}\\mid\\mathbf{x}_{(i-1)\\Delta t})}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Notably, the gradient of this objective with respect to $\\phi$ does not require differentiation (backpropagation) into the sampling process that produced a trajectory $\\mathbf{x}_{0}\\rightarrow\\cdot\\cdot\\cdot\\rightarrow\\mathbf{x}_{1}$ . This offers two advantages over on-policy simulation-based methods: (1) the ability to optimize $\\mathcal{L}_{\\mathrm{RTB}}$ as an off-policy objective, i.e., sampling trajectories for training from a distribution different from $p_{\\phi}^{\\mathrm{post}}$ itself, as discussed further in $\\S2.3$ ; (2) backpropagating only to a subset of the summands in (10), when computing and storing gradients for all steps in the trajectory is prohibitive for large diffusion models (see $\\S\\mathrm{H.l}$ ). ", "page_idx": 4}, {"type": "text", "text": "Comparison with classifier guidance. It is interesting to contrast the RTB training objective with the technique of classifier guidance [11] used for some problems of the same form. If $r(\\mathbf{x}_{1})=p(\\mathbf{y}\\mid\\mathbf{x}_{1})$ is a conditional likelihood, classifier guidance relies upon writing $u_{t}\\big(\\mathbf{x}_{t}\\big)-u_{t}^{\\mathrm{post}}\\big(\\mathbf{x}_{t}\\big)$ explicitly in terms of $\\nabla_{\\mathbf{x}_{t}}\\log p(\\mathbf{y}\\mid\\mathbf{x}_{t})$ , by combining the expression of the optimal drift $u_{t}$ in terms of the score of the target distribution convolved with a Gaussian (cf. $\\S2.1)$ , with the \u2018Bayes\u2019 rule\u2019 for the Stein score: $\\nabla_{\\mathbf{x}_{t}}\\log p(\\mathbf{x}_{t}\\mid\\mathbf{y})=\\nabla_{\\mathbf{x}_{t}}\\log p(\\mathbf{x}_{t})+\\nabla_{\\mathbf{x}_{t}}\\log p(\\mathbf{y}\\mid\\mathbf{x}_{t})$ . ", "page_idx": 4}, {"type": "text", "text": "Classifier guidance gives the exact solution for the posterior drift when a differentiable classifier on noisy data, $\\begin{array}{r}{p(\\mathbf{y}\\mid\\mathbf{x}_{t})=\\int p(\\mathbf{y}\\mid\\mathbf{x}_{1})p(\\mathbf{x}_{1}\\mid\\mathbf{x}_{t})\\,d\\mathbf{x}_{1}}\\end{array}$ , is available. Unfortunately, such a classifier is not, in general, tractable to derive from the classifier on noiseless data, $p(\\mathbf{y}\\mid\\mathbf{x}_{1})$ , and cannot be learned without access to unbiased data samples. RTB is an asymptotically unbiased objective that recovers the difference in drifts (and thus the gradient of the log-convolved likelihood) in a data-free manner. ", "page_idx": 4}, {"type": "text", "text": "2.3 Training, parametrization, and conditioning ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Training and exploration. The choice of which trajectories we use to take gradient steps with the RTB loss can have a large impact on sample efficiency. In on-policy training, we use the current policy $p_{\\phi}^{\\mathrm{post}}$ to generate trajectories $\\tau\\,=\\,(\\mathbf{x}_{0}\\,\\rightarrow\\,.\\,.\\,.\\,\\rightarrow\\,\\mathbf{x}_{1})$ , evaluate the reward $\\log r(\\mathbf{x}_{1})$ and the likelihood of $\\tau$ under $p_{\\theta}$ , and a gradient updates on $\\phi$ to minimize $\\mathcal{L}_{\\mathrm{RTB}}(\\tau;\\phi)$ . ", "page_idx": 4}, {"type": "text", "text": "However, on-policy training may be insufficient to discover the modes of the posterior distribution. In this case, we can perform off-policy exploration to ensure mode coverage. For instance, given samples $\\mathbf{X}_{1}$ that have high density under the target distribution, we can sample noising trajectories $\\mathbf{x}_{1}\\gets\\mathbf{x}_{1-\\Delta t}\\gets...\\gets\\mathbf{x}_{0}$ starting from these samples and use such trajectories for training. Another effective off-policy training technique uses replay buffers. We expect the flexibility of mixing onpolicy training with off-policy exploration to be a strength of RTB over on-policy RL methods, as was shown for distribution-matching training of diffusion models in [64]. ", "page_idx": 4}, {"type": "text", "text": "Conditional constraints and amortization. Above we derived and proved the correctness of the RTB objective for an arbitrary positive constraint $r(\\mathbf{x}_{1})$ . If the constraints depend on other variables y \u2013 for example, \ud835\udc5f(x1; y) = \ud835\udc5d(y | x1) \u2013 then the posterior drift \ud835\udc62p\ud835\udf19ostc an be conditioned on y and the learned scalar $\\log{Z_{\\phi}}$ replaced by a model taking $\\mathbf{y}$ as input. Such conditioning achieves amortized inference and allows generalization to new $\\mathbf{y}$ not seen in training. Similarly, all of the preceding discussion easily generalizes to priors that are conditioned on some context variable. ", "page_idx": 4}, {"type": "text", "text": "Efficient parametrization and Langevin inductive bias. Because the deep features learned by the prior model $u_{\\theta}$ are expected to be useful in expressing the posterior drift \ud835\udc62p\ud835\udf19ost, we can choose to initialize $u_{\\phi}^{\\mathrm{post}}$ as a copy of $u_{\\theta}$ and to fine-tune it, possibly in a parameter-efficient way (as described in each section of $\\S3$ ). This choice is inspired by the method of amortizing inference in large language models by fine-tuning a prior model to sample an intractable posterior [28]. ", "page_idx": 5}, {"type": "text", "text": "Furthermore, if the constraint $r(\\mathbf{x}_{1})$ is differentiable, we can impose an inductive bias on the posterior drift similar to the one introduced for diffusion samplers of unnormalized target densities in [88] and shown to be useful for off-policy methods in [64]. namely, we write ", "page_idx": 5}, {"type": "equation", "text": "$$\nu_{\\phi}^{\\mathrm{post}}(\\mathbf{x}_{t},t)=\\mathrm{NN}_{1}(\\mathbf{x}_{t},t;\\phi)+\\mathrm{NN}_{2}(\\mathbf{x}_{t},t,\\phi)\\nabla_{\\mathbf{x}_{t}}\\log r(\\mathbf{x}_{t}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathrm{NN}_{1}$ and $\\mathrm{NN}_{2}$ are neural networks outputting a vector and a scalar, respectively. This parametrization allows the constraint to provide a signal to guide the sampler at intermediate steps. ", "page_idx": 5}, {"type": "text", "text": "Stabilizing the loss. We propose two simple design choices for stabilizing RTB training. First, the loss in (9) can be replaced by the empirical variance over a minibatch of the quantity inside the square, which removes dependence on $\\log{Z_{\\phi}}$ and is especially useful in conditional settings, consistent with the findings of [64]. This amounts to a relative variant of the VarGrad objective [60] (see (23) in $\\S\\mathrm{G}$ ). Second, we employ loss clipping: to reduce sensitivity to an imperfectly fit prior model, we do not perform updates on trajectories where the loss is close to 0 (see $\\S E,\\S\\mathrm{F})$ . ", "page_idx": 5}, {"type": "text", "text": "2.4 Generative flow networks and extension to other hierarchical processes ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "RTB as TB under the prior measure. The theoretical foundations for continuous generative flow networks [37] establish the correctness of enforcing constraints such as trajectory balance (7) for training sequential samplers, such as diffusion models, to match unnormalized target densities. While we have considered Gaussian transitions and identified transition kernels with their densities with respect to the Lebesgue measure over $\\mathbb{R}^{d}$ , these foundations generalize to more general reference measures. In $\\S B$ , we show how the RTB constraint can be recovered as a special case of the TB constraint for a certain choice of reference measure derived from the prior. ", "page_idx": 5}, {"type": "text", "text": "Extension to arbitrary sequential generation. While our discussion was focused on diffusion models for continuous spaces, the RTB objective can be applied to any Markovian sequential generative process, in particular, one that can be formulated as a generative flow network in the sense of [5, 37]. This includes, in particular, generative models that generate objects by a sequence of discrete steps, including autoregressive models and discrete diffusion models. In the case of discrete diffusion, where the intermediate latent variables $\\mathbf{X}_{t}$ lie not in $\\mathbb{R}^{d}$ but in the space of sequences, one simply replaces the Gaussian transition densities by transition probability masses in the RTB constraint (8) and objective (9). In the case of autoregressive models, where only one sequence of steps can generate any given object, the backward process $q$ becomes trivial, and the RTB constraint for a model \ud835\udc5dp\ud835\udf19ost to sample a sequence $\\mathbf{X}$ from a distribution with density $r(\\mathbf{x})p_{\\theta}(\\mathbf{x})$ is simply $Z_{\\phi}p_{\\phi}^{\\mathrm{post}}({\\bf x})\\,=\\,r({\\bf x})p_{\\theta}({\\bf x})$ for all sequences $\\mathbf{X}$ . We note that a sub-trajectory generalization of this objective was used in [28] to amortize intractable inference in autoregressive language models. ", "page_idx": 5}, {"type": "text", "text": "3 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we present empirical results to validate the efficacy of relative trajectory balance. Our experiments are designed to demonstrate the wide applicability of RTB to sample from posteriors for diffusion priors with arbitrary rewards on vision, language, and continuous control tasks. ", "page_idx": 5}, {"type": "text", "text": "3.1 Class-conditional posterior sampling from unconditional diffusion priors ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We evaluate RTB in a classifier-guided visual task where we wish to learn a diffusion posterior $p_{\\phi}^{\\mathrm{post}}(\\mathbf{x}\\mid c)\\propto p_{\\theta}(\\mathbf{x})p(c\\mid\\mathbf{x})$ given a pretrained diffusion prior $p_{\\theta}(\\mathbf{x})$ and a classifier $r(\\mathbf{x})=p(c\\mid\\mathbf{x})$ . ", "page_idx": 5}, {"type": "text", "text": "Setup. We consider two 10-class image datasets, MNIST and CIFAR-10, using off-the-shelf unconditional diffusion priors from [26] and standard classifiers $p(c\\mid\\mathbf{x})$ for both datasets. We perform parameter-efficient fine-tuning of $p_{\\phi}^{\\mathrm{post}}$ , initialized as a copy of the prior $p_{\\theta}$ , using the RTB objective (see $\\S E.1$ for details). The RTB objective is optimized on trajectories sampled ", "page_idx": 5}, {"type": "text", "text": "Table 2: Classifier-guided posterior sampling with pretrained unconditional diffusion priors. We report the mean\u00b1std of each metric computed across all relevant classes for each experiment set, and highlight $\\pm5\\%$ from highest/lower experimental value. The FID is computed between learned posterior samples and the true samples from the class in question. DP and LGD-MC fail to appropriately model the posterior distribution (high average $\\log r(\\mathbf{x}))$ while DDPO mode-collapses. RTB achieves comparable or superior performance to all other baselines, optimally balancing high reward and diversity as measured by FID. See Table E.1 for conditional variants. ", "page_idx": 6}, {"type": "table", "img_path": "", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "gVTkMsaaGI/tmp/ea1726f2ef6d543d268a1792bf32bcd775992d4919671578b77da7e81e24ab1b.jpg", "img_caption": ["Figure 2: Samples from RTB fine-tuned diffusion posteriors. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "on-policy from the current posterior model. We compare RTB with two RL-based fine-tuning techniques derived from DPOK [15] and DDPO [7] and with two classifier guidance baselines, namely DPS [10], and LGD-MC [69]. We consider three experimental settings: MNIST singledigit posterior (learning to sample images of each digit class $c$ ), CIFAR-10 single-class posterior (analogous to the previous), and MNIST multi-digit posterior. The latter is a multimodal posterior, for which we set $\\begin{array}{r}{r(\\mathbf{x})=\\operatorname*{max}_{i\\in\\{0,2,4,6,8\\}}\\,p(c=i\\mid\\mathbf{x})}\\end{array}$ to generate even digits, and similarly for odd digits. ", "page_idx": 6}, {"type": "text", "text": "Results. Samples from the RTB-fine-tuned posterior models are shown in Fig. 2. In Table 2 we report mean\u00b1std of various metrics across all trained posteriors. We observe that models fine-tuned with RTB generate class samples with both the highest diversity (highest mean pairwise cosine distance in Inceptionv3 feature space) and closeness to true samples of the target classes (FID), while achieving high expected $\\log r(\\mathbf{x})$ . Pure RL fine-tuning (no KL regularization) displays mode collapse characteristics, achieving high rewards in exchange for significantly poorer diversity and FID scores (see also Fig. E.1). Classifier-guidance-based methods, like DP and LGD-MC, exhibit high diversity, but fail to appropriately model the posterior distribution (lowest $\\log r(\\mathbf{x}))$ . Additional results can be found in $\\S E.2$ . ", "page_idx": 6}, {"type": "text", "text": "3.2 Fine-tuning a text-to-image diffusion model ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Diffusion models for text-conditional image generation [e.g. 62] can struggle to consistently generate images $\\mathbf{X}$ that adhere to complex prompts $\\mathbf{z}$ , for example, those that involve composing multiple objects (e.g., \u201cA cat and a dog\u201d) or specify \u201cunnatural\u201d appearances (e.g., \u201cA green-colored rabbit\u201d). Finetuning pretrained text-to-image diffusion models $p_{\\theta}(\\mathbf{x}_{1}\\mid\\mathbf{z})$ as RL policies to maximize some reward $r(\\mathbf{x}_{1},\\mathbf{z})$ based on human preferences has become the standard approach to tackle this issue [7, 15, 76]. Simply maximizing the reward function can result in mode collapse as well as over-optimization of the reward. This is typically handled by constraining the fine-tuned model $\\tilde{p}$ to be close to the prior $p$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{argmax}_{\\tilde{p}}\\mathbb{E}_{\\tilde{p}(\\mathbf{x}_{1}|\\mathbf{z})}\\left[r(\\mathbf{x}_{1},\\mathbf{z})\\right],\\quad D_{\\mathrm{KL}}\\big[\\tilde{p}(\\mathbf{x}_{1}\\mid\\mathbf{z})\\,\\,\\big\\|\\,\\,p(\\mathbf{x}_{1}\\mid\\mathbf{z})\\big]\\leq\\epsilon.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The optimal $\\tilde{p}$ for (12) is $\\tilde{p}(\\mathbf{x}_{1}\\mid\\mathbf{z})\\propto p(\\mathbf{x}_{1}\\mid\\mathbf{z})\\exp(\\beta r(\\mathbf{x}_{1},\\mathbf{z}))$ for some inverse temperature $_\\beta$ . The marginal KL is intractable for diffusion models, so methods like DPOK [15] optimize an upper bound on the marginal $\\mathrm{KL}$ in the form of a per-step KL penalty $-\\gamma\\sum_{i=1}^{T}D_{\\mathrm{KL}}[\\tilde{p}(\\mathbf{x}_{i\\Delta t}\\mid\\mathbf{x}_{(i-1)\\Delta t},\\mathbf{z})||p(\\mathbf{x}_{i\\Delta t}\\mid$ $\\mathbf{x}_{(i-1)\\Delta t},\\mathbf{z})]$ added to the reward. By contrast, RTB can avoid the bias in such an approximation and directly learn to generate unbiased samples from the posterior $\\tilde{p}(\\mathbf{x}_{1}\\mid\\mathbf{z})$ . ", "page_idx": 6}, {"type": "text", "text": "Setup. We demonstrate how RTB can be used to fine-tune pretrained text-to-image diffusion models. We use the latent diffusion model Stable Diffusion v1-5 [62] as a prior over $512\\times512$ images. Following DPOK [15], we use ImageReward [86], which has been trained to match human preferences as well as prompt accuracy to attributes such as the number of objects, color, and compositionality, as the reward $\\log r(\\mathbf{x}_{1},\\mathbf{z})$ . As reference, we present comparisons against DPOK with the default KL regularization $\\gamma=0.01$ and DPOK with $\\gamma=0.0$ , which is equivalent to DDPO [7]. We measure the final average reward and the diversity of the generated image, as measured by the average pairwise cosine distance between CLIP embeddings [57] of a batch of generated images. Further details about the experimental setup and ablations are discussed in $\\mathrm{\\SH}$ . ", "page_idx": 6}, {"type": "image", "img_path": "gVTkMsaaGI/tmp/5e9e86cff988102a9f3d96026150bbc41c548910c81f5a85ae3f0dfd78fb7d87.jpg", "img_caption": ["Figure 4: Images generated from prior (top row), DPOK (middle row) and RTB (bottom row) for 4 different prompts. Images in the same column share the random DDIM seed. More images in $\\S\\mathrm{H.}2$ . "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Results. Fig. 3 plots the diversity versus log reward on a set of prompts from [15, 86]. In terms of average $\\log r(\\mathbf{x}_{1},\\mathbf{z})$ , RTB either matches or outperforms DPOK, while generally achieving lower reward than DDPO. The CLIP diversity score for RTB and DPOK are on average higher than DDPO, which is expected since it does not use KL regularization. For qualitative image assessments, refer to Fig. 4 and $\\S\\mathrm{H.}2$ . Through this experiment, we show that RTB scales well to high dimensional, multimodal data, matching state-of-the-art methods for fine-tuning text-toimage diffusion models. ", "page_idx": 7}, {"type": "image", "img_path": "gVTkMsaaGI/tmp/8d7f8ea82556d988999392415f211fbc968382696c54462fb4c9038a10fe735e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 3: Fine-tuning Stable Diffusion with ImageReward. We report mean $\\log r(\\mathbf{x}_{1},\\mathbf{z})$ and diversity, measured as the mean cosine distance between CLIP embeddings for a batch of 100 generated images.2 ", "page_idx": 7}, {"type": "text", "text": "3.3 Text infilling with discrete diffusion language models ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To evaluate our approach on discrete diffusion models, we consider the problem of text infliling [90], which involves fliling in missing tokens given some context tokens. While discrete diffusion models \u2013 unlike their continuous counterparts \u2013 can be challenging to train [4, 8, 48, 73], score entropy discrete diffusion [SEDD; 42] matches the language modeling performance of autoregressive language models of similar scale. Non-autoregressive generation in diffusion language models can provide useful inductive biases for infilling, such as the ability to attend to context on both sides of a target token. ", "page_idx": 7}, {"type": "text", "text": "Setup. We use the ROCStories corpus [49], a dataset of short stories containing 5 sentences each. We adopt the task setup from [28], where the first 3 sentences of a story $\\mathbf{X}$ and the last sentence $\\mathbf{y}$ are given, and the goal is to generate the fourth sentence z such that the overall story is coherent and consistent. The fourth sentence can involve a turning point in the story and is thus challenging to flil in. We aim to model the posterior $p^{\\mathrm{post}}(\\mathbf{z}\\mid\\mathbf{x},\\mathbf{y})\\propto p(\\mathbf{z}\\mid\\mathbf{x})p_{\\mathrm{reward}}(\\mathbf{y}\\mid\\mathbf{x},\\mathbf{z})$ where $p$ is a SEDD language model prior (a conditional model over $\\mathbf{z}$ given $\\mathbf{x}$ ) and $p_{\\mathrm{reward}}$ is an autoregressive language model fine-tuned with a maximum likelihood objective on a held-out subset of the dataset. As baselines, we consider simply prompting the diffusion language model with $\\mathbf{X}$ (Prompt $\\bf{\\Psi}(\\bf{x})$ ) and x, y (Prompt $(\\mathbf{x},\\mathbf{y}))$ . Additionally, to contextualize the performance, we also consider autoregressive language model baselines from [28], which studied this problem under an autoregressive prior $p(\\mathbf{z}\\mid\\mathbf{x})$ . SFT is trained on 50, 000 examples compared to 1000 for RTB, and serves as an upper bound on the performance in this task. See $\\S\\mathrm{F}$ for further details about the experimental setup. ", "page_idx": 7}, {"type": "text", "text": "Results. Following [28], we use three standard metrics to measure the similarity of the generated infills with the reference infills from the dataset: BERTScore [89] (with DeBERTa [25]), BLEU-4 [53], and GLEU-4 [85]. Table 3 summarizes the results. We observe that the diffusion language model performs significantly better than the autoregressive language model without any fine-tuning. RTB further improves the performance over prompting, and even outperforms the strongest autoregressive baseline of GFlowNet fine-tuning. We provide some examples of generated text in $\\S\\mathrm{F}$ . ", "page_idx": 8}, {"type": "text", "text": "Table 3: Results on the story infilling task with autoregressive and discrete diffusion language models. Metrics are computed with respect to reference infills from the dataset. All metrics are mean\u00b1std over 5 samples for each of the 100 test examples. RTB with discrete diffusion prior performs better than best baseline with autoregressive prior. ", "page_idx": 8}, {"type": "table", "img_path": "gVTkMsaaGI/tmp/391192e0db9ef917b312a8d9c92f3f3195c82bc5e07bb18651a8197adb8f2d1c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "3.4 KL-constrained policy search in offline reinforcement learning ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The goal of RL algorithms is to learn a policy $\\pi(a\\mid s)$ , i.e., a mapping from states $s$ to actions $a$ in an environment, that maximizes the expected cumulative discounted reward [74]. In the offline RL setting [38], the agent has access to a dataset $\\mathcal{D}=\\{(s_{t}^{i},a_{t}^{i},s_{t+1}^{i},r_{t}^{i})\\}_{i=1}^{N}$ of transitions (where each sample $\\left({{s_{t}},{a_{t}},{s_{t+1}},{r_{t}}}\\right)$ indicates that an agent taking action $a_{t}$ at state $s_{t}$ transitioned to the next state $s_{t+1}$ and received reward $r_{t}$ ). This dataset is assumed to be generated by a behavior policy $\\mu(a\\mid s)$ , which may be a diffusion model trained on $\\mathcal{D}$ . Offline RL algorithms must learn a new policy $\\pi$ which achieves high return using only this dataset without interacting with the environment. ", "page_idx": 8}, {"type": "text", "text": "An important problem in offline RL is policy extraction from trained $Q$ -functions [54, 24, 43]. For reliable extrapolation, one wants the policy to predict actions that have high $Q$ -values, but also have high density under the behavior policy $\\mu$ , as naive maximization can result in choosing actions with low probability under $\\mu$ and thus unreliable predictions from the $Q$ -function. This is formulated as a KL-constrained policy search problem: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\underset{\\pi}{\\operatorname{argmax}}\\,\\mathbb{E}_{s\\sim d_{\\mu},a\\sim\\pi(a\\mid s)}[Q(s,a)],\\quad\\mathbb{E}_{s\\sim d_{\\mu}}[D_{\\mathrm{KL}}(\\pi(a\\mid s)\\parallel\\mu(a\\mid s))]\\le\\epsilon,\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $d_{\\mu}$ is the distribution over states induced by following the policy $\\mu$ . The optimal policy $\\pi$ in (13) is the product distribution $\\pi^{*}(a\\mid s)\\propto\\mu(a\\mid s)\\exp(\\beta Q(s,a))$ for some inverse temperature $\\beta$ . If $\\mu(a\\mid s)$ is a conditional diffusion model over continuous actions $a$ conditioned on state $s$ , we use RTB to fine-tune a diffusion behavior policy to sample from $\\pi^{*}$ , using $\\mu$ as the prior and $\\exp(\\beta Q(s,a))$ as the target constraint. We use a $Q$ -function trained using IQL [35]. ", "page_idx": 8}, {"type": "text", "text": "Setup. We test on continuous control tasks in the D4RL suite [17], which consists of offilne datasets collected using a mixture of SAC policies of varying performance. We evaluate on the halfcheetah, hopper and walker2d MuJoCo [75] locomotion tasks, each of which contains three datasets of transitions: \u201cmedium\u201d (collected from an early-stopped policy), \u201cmedium-expert\u201d (collected from both an expert and an early-stopped policy) and \u201cmedium-replay\u201d (transitions stored in the replay buffer prior to early stopping). We compare against standard offilne RL baselines (Behavior Cloning (BC), CQL [36], and IQL [35]) and diffusion-based offline RL methods which are currently state-ofthe-art: Diffuser [D; 29], Decision Diffuser [DD; 2], D-QL [82], IDQL [24], and QGPO [43]. For algorithm implementation details, hyperparameters, and a report of baselines, see $\\S\\mathrm{G}$ . ", "page_idx": 8}, {"type": "text", "text": "Results. Table 4 shows that RTB matches state-of-the-art results across the D4RL tasks. In particular, RTB performs strongly in the medium-replay tasks, which contain the most suboptimal data and consequently the poorest behavior prior. We highlight that our performance is similar to QGPO [43], which learns intermediate energy densities for diffusion posterior sampling. ", "page_idx": 8}, {"type": "text", "text": "4 Other related work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Composing iterative generative processes. Beyond the approximate posterior sampling algorithms and application-specific techniques discussed in $\\S1$ and $\\S3$ , several recent works have explored the use of hierarchical models, such as diffusion models, as modular components in generative processes. Diffusion models can be used to sample product distributions to induce compositional structure in images [40, 14]. Amortized Bayesian inference [34, 59, 58, 19] is another domain of sampling from product distributions where diffusion models are now being used [20]. Beyond product models, [18] studies ways to amortize other kinds of compositions of hierarchical processes, including diffusion models, while [66] proposes methods to sample the product of many iterative processes in application to federated learning. Finally, models without hierarchical structure, such as normalizing flows, have been used to amortize intractable inference in pretrained diffusion models [e.g., 16]. In contrast, our method performs posterior inference by fine-tuning a prior model, developing a direction on flexible extraction of information from large pretrained models [28]. ", "page_idx": 8}, {"type": "table", "img_path": "gVTkMsaaGI/tmp/67b77222a82d493fc05e5f0ca9a07a5e836c87e5ca41445b728730a6996d8cf2.jpg", "table_caption": ["Table 4: Average rewards of trained policies on D4RL locomotion tasks (mean\u00b1std over 5 random seeds). Following past work, numbers within $5\\%$ of maximum in every row are highlighted. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Diffusion samplers. Several prior works seek to amortize MCMC sampling from unnormalized densities by training diffusion models for efficient mode-mixing [6, 88, 77, 61, 78, 3]. Our work is most closely related to continuous GFlowNets [37], which offer an alternative perspective on training diffusion samplers using off-policy flow consistency objectives [37, 87, 64]. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusions and future work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Relative trajectory balance provides a new approach to training diffusion models to generate unbiased posterior samples given a diffusion prior and an arbitrary reward function. Through experiments on a variety of domains \u2013 vision, language, continuous control \u2013 we demonstrated the flexibility and general applicability of RTB. RTB can be optimized with off-policy trajectories, and future work can explore ways to leverage off-policy training, using techniques such as local search [33, 64] to improve sample efficiency and mode coverage. Simulation-based objectives in the style of [88] are also applicable to the amortized sampling problems we consider and should be explored, as should simulation-free extensions, e.g., through objectives that are local in time [44]. The ability to handle arbitrary black-box likelihoods also makes RTB a useful candidate for inverse problems in domains such as 3D object synthesis with likelihood computed via a renderer [e.g., 55, 81], imaging problems in astronomy [e.g., 1], medical imaging [e.g., 72], and molecular structure prediction [e.g., 83]. ", "page_idx": 9}, {"type": "text", "text": "Moreover, RTB could facilitate a breakthrough in modeling molecular dynamics\u2014a notoriously challenging task due to the need to sample rare-event trajectories in chemical simulations\u2014by converting these problems into posterior inference over amplified distributions of rare-event samples. Notably, Seong et al. [65] have already explored a preliminary version of this concept by employing TB with a reward multiplied by the prior likelihood, which is effectively equivalent to RTB. ", "page_idx": 9}, {"type": "text", "text": "Limitations. RTB learns the posterior through simulation-based training, which can be slow and memory-intensive. Additionally, the RTB objective is computed on complete trajectories without any local credit-assignment signal, which can result in high variance in the gradients. ", "page_idx": 9}, {"type": "text", "text": "Broader impact. While our contributions focus on an algorithmic approach for learning posterior samplers with diffusion priors, we acknowledge that like other advances in generative modelling, our approach can potentially be used by nefarious actors to train generative models to produce harmful content and misinformation. At the same time, our approach can be also be used to mitigate biases captured in pretrained models and applied to various scientific problems. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The authors thank Adam Coogan, Yashar Hezaveh, Guillaume Lajoie, and Laurence Perreault Levasseur for helpful suggestions in the course of this project and Mandana Samiei for comments on a draft of the paper. ", "page_idx": 9}, {"type": "text", "text": "The authors acknowledge funding from CIFAR, NSERC, IVADO, UNIQUE, FACS Acuit\u00e9, NRC AI4Discovery, Samsung, and Recursion. ", "page_idx": 10}, {"type": "text", "text": "The research was enabled in part by computational resources provided by the Digital Research Alliance of Canada (https://alliancecan.ca), Mila (https://mila.quebec), and NVIDIA. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Alexandre Adam, Adam Coogan, Nikolay Malkin, Ronan Legin, Laurence Perreault-Levasseur, Yashar Hezaveh, and Yoshua Bengio. Posterior samples of source galaxies in strong gravitational lenses with score-based priors. arXiv preprint arXiv:2211.03812, 2022.   \n[2] Anurag Ajay, Yilun Du, Abhi Gupta, Joshua B. Tenenbaum, Tommi S. Jaakkola, and Pulkit Agrawal. Is conditional generative modeling all you need for decision making? International Conference on Learning Representations (ICLR), 2023.   \n[3] Tara Akhound-Sadegh, Jarrid Rector-Brooks, Avishek Joey Bose, Sarthak Mittal, Pablo Lemos, Cheng-Hao Liu, Marcin Sendera, Siamak Ravanbakhsh, Gauthier Gidel, Yoshua Bengio, Nikolay Malkin, and Alexander Tong. Iterated denoising energy matching for sampling from Boltzmann densities. International Conference on Machine Learning (ICML), 2024.   \n[4] Jacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured denoising diffusion models in discrete state-spaces. Neural Information Processing Systems (NeurIPS), 2021.   \n[5] Yoshua Bengio, Salem Lahlou, Tristan Deleu, Edward J. Hu, Mo Tiwari, and Emmanuel Bengio. GFlowNet foundations. Journal of Machine Learning Research, 24(210):1\u201355, 2023.   \n[6] Julius Berner, Lorenz Richter, and Karen Ullrich. An optimal control perspective on diffusionbased generative modeling. Transactions on Machine Learning Research (TMLR), 2024.   \n[7] Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. International Conference on Learning Representations (ICLR), 2024. [8] Andrew Campbell, Joe Benton, Valentin De Bortoli, Thomas Rainforth, George Deligiannidis, and Arnaud Doucet. A continuous time framework for discrete denoising models. Neural Information Processing Systems (NeurIPS), 2022. [9] Gabriel Cardoso, Yazid Janati el idrissi, Sylvain Le Corff, and Eric Moulines. Monte carlo guided denoising diffusion models for bayesian linear inverse problems. International Conference on Learning Representations (ICLR), 2024.   \n[10] Hyungjin Chung, Jeongsol Kim, Michael Thompson Mccann, Marc Louis Klasky, and Jong Chul Ye. Diffusion posterior sampling for general noisy inverse problems. International Conference on Learning Representations (ICLR), 2023.   \n[11] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat GANs on image synthesis. Neural Information Processing Systems (NeurIPS), 2021.   \n[12] Sander Dieleman, Laurent Sartran, Arman Roshannai, Nikolay Savinov, Yaroslav Ganin, Pierre H Richemond, Arnaud Doucet, Robin Strudel, Chris Dyer, Conor Durkan, et al. Continuous diffusion for categorical data. arXiv preprint arXiv:2211.15089, 2022.   \n[13] Zehao Dou and Yang Song. Diffusion posterior sampling for linear inverse problem solving: A filtering perspective. International Conference on Learning Representations (ICLR), 2024.   \n[14] Yilun Du, Conor Durkan, Robin Strudel, Joshua B. Tenenbaum, Sander Dieleman, Rob Fergus, Jascha Sohl-Dickstein, Arnaud Doucet, and Will Sussman Grathwohl. Reduce, reuse, recycle: Compositional generation with energy-based diffusion models and MCMC. International Conference on Machine Learning (ICML), 2023.   \n[15] Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Reinforcement learning for finetuning text-to-image diffusion models. Neural Information Processing Systems (NeurIPS), 2023.   \n[16] Berthy T. Feng, Jamie Smith, Michael Rubinstein, Huiwen Chang, Katherine L. Bouman, and William T. Freeman. Score-based diffusion models as principled priors for inverse imaging. International Conference on Computer Vision (ICCV), 2023.   \n[17] Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4RL: Datasets for deep data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020.   \n[18] Timur Garipov, Sebastiaan De Peuter, Ge Yang, Vikas Garg, Samuel Kaski, and Tommi Jaakkola. Compositional sculpting of iterative generative processes. Neural Information Processing Systems (NeurIPS), 2023.   \n[19] Marta Garnelo, Jonathan Schwarz, Dan Rosenbaum, Fabio Viola, Danilo J Rezende, SM Eslami, and Yee Whye Teh. Neural processes. arXiv preprint arXiv:1807.01622, 2018.   \n[20] Tomas Geffner, George Papamakarios, and Andriy Mnih. Compositional score modeling for simulation-based inference. International Conference on Machine Learning (ICML), 2023.   \n[21] Alexandros Graikos, Nikolay Malkin, Nebojsa Jojic, and Dimitris Samaras. Diffusion models as plug-and-play priors. Neural Information Processing Systems (NeurIPS), 2022.   \n[22] Ishaan Gulrajani and Tatsunori B Hashimoto. Likelihood-based diffusion language models. Neural Information Processing Systems (NeurIPS), 2023.   \n[23] Xiaochuang Han, Sachin Kumar, and Yulia Tsvetkov. SSD-LM: Semi-autoregressive simplexbased diffusion language model for text generation and modular control. Association for Computational Linguistics (ACL), 2023.   \n[24] Philippe Hansen-Estruch, Ilya Kostrikov, Michael Janner, Jakub Grudzien Kuba, and Sergey Levine. IDQL: Implicit Q-learning as an actor-critic method with diffusion policies. arXiv preprint arXiv:2304.10573, 2023.   \n[25] Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert with disentangled attention. International Conference on Learning Representations (ICLR), 2021.   \n[26] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Neural Information Processing Systems (NeurIPS), 2020.   \n[27] Edward J. Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. International Conference on Learning Representations (ICLR), 2022.   \n[28] Edward J. Hu, Moksh Jain, Eric Elmoznino, Younesse Kaddar, Guillaume Lajoie, Yoshua Bengio, and Nikolay Malkin. Amortizing intractable inference in large language models. International Conference on Learning Representations (ICLR), 2024.   \n[29] Michael Janner, Yilun Du, Joshua Tenenbaum, and Sergey Levine. Planning with diffusion for flexible behavior synthesis. International Conference on Machine Learning (ICML), 2022.   \n[30] Zahra Kadkhodaie and Eero P. Simoncelli. Solving linear inverse problems using the prior implicit in a denoiser. Neural Information Processing Systems (NeurIPS), 2021.   \n[31] Bingyi Kang, Xiao Ma, Chao Du, Tianyu Pang, and Shuicheng Yan. Efficient diffusion policies for offline reinforcement learning. Neural Information Processing Systems (NeurIPS), 2024.   \n[32] Bahjat Kawar, Gregory Vaksman, and Michael Elad. SNIPS: Solving noisy inverse problems stochastically. Neural Information Processing Systems (NeurIPS), 2021.   \n[33] Minsu Kim, Taeyoung Yun, Emmanuel Bengio, Dinghuai Zhang, Yoshua Bengio, Sungsoo Ahn, and Jinkyoo Park. Local search GFlowNets. International Conference on Learning Representations (ICLR), 2024.   \n[34] Diederik P. Kingma and Max Welling. Auto-encoding variational Bayes. International Conference on Learning Representations (ICLR), 2014.   \n[35] Ilya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit Q-learning. International Conference on Learning Representations (ICLR), 2022.   \n[36] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative Q-learning for offline reinforcement learning. Neural Information Processing Systems (NeurIPS), 2020.   \n[37] Salem Lahlou, Tristan Deleu, Pablo Lemos, Dinghuai Zhang, Alexandra Volokhova, Alex Hern\u00e1ndez-Garc\u0131a, L\u00e9na N\u00e9hale Ezzine, Yoshua Bengio, and Nikolay Malkin. A theory of continuous generative flow networks. International Conference on Machine Learning (ICML), 2023.   \n[38] Sergey Levine, Aviral Kumar, G. Tucker, and Justin Fu. Offilne reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.   \n[39] Xiang Li, John Thickstun, Ishaan Gulrajani, Percy S Liang, and Tatsunori B Hashimoto. Diffusion-LM improves controllable text generation. Neural Information Processing Systems (NeurIPS), 2022.   \n[40] Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, and Joshua B Tenenbaum. Compositional visual generation with composable diffusion models. European Conference on Computer Vision (ECCV), 2022.   \n[41] Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-Eval: NLG evaluation using GPT-4 with better human alignment. arXiv preprint arXiv:2303.16634, 2023.   \n[42] Aaron Lou, Chenlin Meng, and Stefano Ermon. Discrete diffusion language modeling by estimating the ratios of the data distribution. arXiv preprint arXiv:2310.16834, 2023.   \n[43] Cheng Lu, Huayu Chen, Jianfei Chen, Hang Su, Chongxuan Li, and Jun Zhu. Contrastive energy prediction for exact energy-guided diffusion sampling in offline reinforcement learning. International Conference on Machine Learning (ICML), 2023.   \n[44] Kanika Madan, Jarrid Rector-Brooks, Maksym Korablyov, Emmanuel Bengio, Moksh Jain, Andrei Nica, Tom Bosc, Yoshua Bengio, and Nikolay Malkin. Learning GFlowNets from partial episodes for improved convergence and stability. International Conference on Machine Learning (ICML), 2022.   \n[45] Nikolay Malkin, Moksh Jain, Emmanuel Bengio, Chen Sun, and Yoshua Bengio. Trajectory balance: Improved credit assignment in GFlowNets. Neural Information Processing Systems (NeurIPS), 2022.   \n[46] Nikolay Malkin, Salem Lahlou, Tristan Deleu, Xu Ji, Edward Hu, Katie Everett, Dinghuai Zhang, and Yoshua Bengio. GFlowNets and variational inference. International Conference on Learning Representations (ICLR), 2023.   \n[47] Morteza Mardani, Jiaming Song, Jan Kautz, and Arash Vahdat. A variational perspective on solving inverse problems with diffusion models. International Conference on Learning Representations (ICLR), 2024.   \n[48] Chenlin Meng, Kristy Choi, Jiaming Song, and Stefano Ermon. Concrete score matching: Generalized score matching for discrete data. Neural Information Processing Systems (NeurIPS), 2022.   \n[49] Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, and James Allen. A corpus and cloze evaluation for deeper understanding of commonsense stories. North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), 2016.   \n[50] Alex Nichol and Prafulla Dhariwal. Improved denoising diffusion probabili1stic models. International Conference on Machine Learning (ICML), 2021.   \n[51] Nikolas N\u00fcsken and Lorenz Richter. Solving high-dimensional Hamilton\u2013Jacobi\u2013Bellman PDEs using neural networks: perspectives from the theory of controlled diffusions and measures on path space. Partial Differential Equations and Applications, 2(4):48, 2021.   \n[52] Bernt \u00d8ksendal. Stochastic Differential Equations: An Introduction with Applications. Springer, 2003.   \n[53] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. Association for Computational Linguistics (ACL), 2002.   \n[54] Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression: Simple and scalable off-policy reinforcement learning. arXiv prepring arXiv:1910.00177, 2019.   \n[55] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. DreamFusion: Text-to-3D using 2D diffusion. International Conference on Learning Representations (ICLR), 2023.   \n[56] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.   \n[57] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. 2021.   \n[58] Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. International Conference on Machine Learning (ICML), 2015.   \n[59] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. International Conference on Machine Learning (ICML), 2014.   \n[60] Lorenz Richter, Ayman Boustati, Nikolas N\u00fcsken, Francisco J. R. Ruiz, and \u00d6mer Deniz Akyildiz. VarGrad: A low-variance gradient estimator for variational inference. Neural Information Processing Systems (NeurIPS), 2020.   \n[61] Lorenz Richter, Julius Berner, and Guan-Horng Liu. Improved sampling via learned diffusions. International Conference on Learning Representations (ICLR), 2023.   \n[62] Robin Rombach, A. Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. Highresolution image synthesis with latent diffusion models. Computer Vision and Pattern Recognition (CVPR), 2022.   \n[63] Simo S\u00e4rkk\u00e4 and Arno Solin. Applied stochastic differential equations. Cambridge University Press, 2019.   \n[64] Marcin Sendera, Minsu Kim, Sarthak Mittal, Pablo Lemos, Luca Scimeca, Jarrid Rector-Brooks, Alexandre Adam, Yoshua Bengio, and Nikolay Malkin. On diffusion models for amortized inference: Benchmarking and improving stochastic control and sampling. arXiv preprint arXiv:2402.05098, 2024.   \n[65] Kiyoung Seong, Seonghyun Park, Seonghwan Kim, Woo Youn Kim, and Sungsoo Ahn. Collective variable free transition path sampling with generative flow network. arXiv preprint arXiv:2405.19961, 2024.   \n[66] Tiago Silva, Amauri H Souza, Luiz Max Carvalho, Samuel Kaski, and Diego Mesquita. Federated contrastive GFlowNets, 2024. URL https://openreview.net/forum?id= VJDFhkwQg6.   \n[67] Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. International Conference on Machine Learning (ICML), 2015.   \n[68] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. International Conference on Learning Representations (ICLR), 2021.   \n[69] Jiaming Song, Qinsheng Zhang, Hongxu Yin, Morteza Mardani, Ming-Yu Liu, Jan Kautz, Yongxin Chen, and Arash Vahdat. Loss-guided diffusion models for plug-and-play controllable generation. International Conference on Maching Learning (ICML), 2023.   \n[70] Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum likelihood training of score-based diffusion models. Neural Information Processing Systems (NeurIPS), 2021.   \n[71] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. International Conference on Learning Representations (ICLR), 2021.   \n[72] Yang Song, Liyue Shen, Lei Xing, and Stefano Ermon. Solving inverse problems in medical imaging with score-based generative models. International Conference on Learning Representations (ICLR), 2022.   \n[73] Haoran Sun, Lijun Yu, Bo Dai, Dale Schuurmans, and Hanjun Dai. Score-based continuous-time discrete diffusion models. International Conference on Learning Representations (ICLR), 2023.   \n[74] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT Press, 2018.   \n[75] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026\u20135033, 2012. doi: 10.1109/IROS.2012.6386109.   \n[76] Masatoshi Uehara, Yulai Zhao, Kevin Black, Ehsan Hajiramezanali, Gabriele Scalia, Nathaniel Lee Diamant, Alex M Tseng, Tommaso Biancalani, and Sergey Levine. Finetuning of continuous-time diffusion models as entropy-regularized control. arXiv preprint arXiv:2402.15194, 2024.   \n[77] Francisco Vargas, Will Grathwohl, and Arnaud Doucet. Denoising diffusion samplers. International Conference on Learning Representations (ICLR), 2023.   \n[78] Francisco Vargas, Shreyas Padhy, Denis Blessing, and Nikolas N\u00fcsken. Transport meets variational inference: Controlled Monte Carlo diffusions. International Conference on Learning Representations (ICLR), 2024.   \n[79] Pascal Vincent. A connection between score matching and denoising autoencoders. Neural computation, 23(7):1661\u20131674, 2011.   \n[80] Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan Lambert, and Shengyi Huang. Trl: Transformer reinforcement learning. https://github. com/huggingface/trl, 2020.   \n[81] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A. Yeh, and Greg Shakhnarovich. Score Jacobian chaining: Lifting pretrained 2D diffusion models for 3D generation. Conference on Computer Vision and Pattern Recognition (CVPR), 2023.   \n[82] Zhendong Wang, Jonathan J Hunt, and Mingyuan Zhou. Diffusion policies as an expressive policy class for offline reinforcement learning. International Conference on Learning Representations (ICLR), 2023.   \n[83] Joseph L Watson, David Juergens, Nathaniel R Bennett, Brian L Trippe, Jason Yim, Helen E Eisenach, Woody Ahern, Andrew J Borst, Robert J Ragotte, Lukas F Milles, et al. De novo design of protein structure and function with rfdiffusion. Nature, 620(7976):1089\u20131100, 2023.   \n[84] Sean Welleck, Ilia Kulikov, Jaedeok Kim, Richard Yuanzhe Pang, and Kyunghyun Cho. Consistency of a recurrent language model with respect to incomplete decoding. Empirical Methods in Natural Language Processing (EMNLP), 2020.   \n[85] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\u2019s neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016.   \n[86] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation. Neural Information Processing Systems (NeurIPS), 2023.   \n[87] Dinghuai Zhang, Ricky Tian Qi Chen, Cheng-Hao Liu, Aaron Courville, and Yoshua Bengio. Diffusion generative flow samplers: Improving learning signals through partial trajectory optimization. International Conference on Learning Representations (ICLR), 2024.   \n[88] Qinsheng Zhang and Yongxin Chen. Path integral sampler: a stochastic control approach for sampling. International Conference on Learning Representations (ICLR), 2022.   \n[89] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. BERTScore: Evaluating text generation with bert. International Conference on Learning Representations, 2020.   \n[90] Wanrong Zhu, Zhiting Hu, and Eric Xing. Text infilling. arXiv preprint arXiv:1901.00158, 2019. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "A Proofs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proposition 1 (Relative TB constraint). I $f_{p_{\\theta},\\ p_{\\phi}^{\\mathrm{post}}}$ , and the scalar $Z_{\\phi}$ jointly satisfy the relative trajectory balance (RTB) constraint ", "page_idx": 16}, {"type": "equation", "text": "$$\nZ_{\\phi}\\cdot p_{\\phi}^{\\mathrm{post}}({\\bf x}_{0},{\\bf x}_{\\Delta t},\\ldots,{\\bf x}_{1})=r({\\bf x}_{1})p_{\\theta}({\\bf x}_{0},{\\bf x}_{\\Delta t},\\ldots,{\\bf x}_{1})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "for every denoising trajectory $\\mathbf{x}_{0}\\rightarrow\\mathbf{x}_{\\Delta t}\\rightarrow\\dots\\rightarrow\\mathbf{x}_{1}$ , then $p_{\\phi}^{\\mathrm{post}}(\\mathbf{x}_{1})\\propto p_{\\theta}(\\mathbf{x}_{1})r(\\mathbf{x}_{1}),$ , i.e., the diffusion model \ud835\udc5dp\ud835\udf19osts amples the posterior distribution. Furthermore, if $p_{\\theta}$ also satisfies the TB constraint (7) with respect to the noising process \ud835\udc5eand some target density $q(\\mathbf{x}_{1})$ , then $p_{\\phi}^{\\mathrm{post}}$ satisfies the TB constraint with respect to the target density $q^{\\mathrm{post}}(\\mathbf{x}_{1})\\propto q(\\mathbf{x}_{1})r(\\mathbf{x}_{1})$ , and $\\begin{array}{r}{Z=\\int q(\\mathbf{x}_{1})r(\\mathbf{x}_{1})\\,d\\mathbf{x}_{1}}\\end{array}$ . ", "page_idx": 16}, {"type": "text", "text": "Proof of Prop. 1. Suppose that $p_{\\theta},p_{\\phi}^{\\mathrm{post}}$ , and $Z$ jointly satisfy (8). Then necessarily $Z\\neq0$ , since the quantities on the right side are positive. We then have, using (4), ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{p}_{\\phi}^{\\mathrm{post}}(\\mathbf{x}_{1})=\\displaystyle\\int{p}_{\\phi}^{\\mathrm{post}}(\\mathbf{x}_{0},\\mathbf{x}_{\\Delta t},\\dots,\\mathbf{x}_{1})\\,d\\mathbf{x}_{0}\\,d\\mathbf{x}_{\\Delta t}\\dots d\\mathbf{x}_{1-\\Delta t}}\\\\ {\\displaystyle=\\cfrac{1}{Z}r(\\mathbf{x}_{1})\\,\\int\\,p_{\\theta}(\\mathbf{x}_{0},\\mathbf{x}_{\\Delta t},\\dots,\\mathbf{x}_{1})\\,d\\mathbf{x}_{0}\\,d\\mathbf{x}_{\\Delta t}\\dots d\\mathbf{x}_{1-\\Delta t}}\\\\ {\\displaystyle=\\cfrac{1}{Z}r(\\mathbf{x}_{1})p_{\\theta}(\\mathbf{x}_{1})}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\propto p_{\\theta}(\\mathbf{x}_{1})r(\\mathbf{x}_{1}),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "as desired. ", "page_idx": 16}, {"type": "text", "text": "Now suppose that $p_{\\theta}$ also satisfies the TB constraint (7) with respect to $q(\\mathbf{x}_{1})$ . Then, for any denoising trajectory, ", "page_idx": 16}, {"type": "equation", "text": "$$\nq(\\mathbf{x}_{0},\\mathbf{x}_{\\Delta t},\\ldots,\\mathbf{x}_{1-\\Delta t}\\mid\\mathbf{x}_{1})={\\frac{p_{\\theta}(\\mathbf{x}_{0},\\mathbf{x}_{\\Delta t},\\ldots,\\mathbf{x}_{1})}{q(\\mathbf{x}_{1})}}={\\frac{p_{\\phi}^{\\mathrm{post}}(\\mathbf{x}_{0},\\mathbf{x}_{\\Delta t},\\ldots,\\mathbf{x}_{1})}{q(\\mathbf{x}_{1})r(\\mathbf{x}_{1})/Z}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "showing that $p_{\\phi}^{\\mathrm{post}}$ satisfies the TB constraint with respect to the noising process $q$ and the (not yet shown to be normalized) density $\\textstyle{\\frac{1}{Z}}q(\\mathbf{x}_{1})r(\\mathbf{x}_{1})$ . We integrate out the variables $\\mathbf{X}_{0},\\mathbf{X}_{\\Delta t},\\ldots,\\mathbf{X}_{1-\\Delta t}$ in (14), giving ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{1=\\displaystyle\\frac{p_{\\phi}^{\\mathrm{post}}(\\mathbf{x}_{1})}{q(\\mathbf{x}_{1})r(\\mathbf{x}_{1})/Z}}\\\\ {q(\\mathbf{x}_{1})r(\\mathbf{x}_{1})=Z p_{\\phi}^{\\mathrm{post}}(\\mathbf{x}_{1}).\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Integrating over $\\mathbf{X}_{1}$ shows $\\begin{array}{r}{\\int q(\\mathbf{x}_{1})r(\\mathbf{x}_{1})\\,d\\mathbf{x}_{1}=Z}\\end{array}$ . ", "page_idx": 16}, {"type": "text", "text": "B Relative TB as TB under the prior measure ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The theoretical foundations for continuous generative flow networks [37] establish the correctness of enforcing constraints such as trajectory balance (7) for training sequential samplers, such as diffusion models, to match unnormalized target densities. While we have considered Gaussian transitions and identified transition kernels with their densities with respect to the Lebesgue measure over $\\mathbb{R}^{d}$ , these foundations generalize to more general reference measures. In application to diffusion samplers, suppose that $\\pi_{\\mathrm{ref}}(\\mathbf{x}_{t})$ is a collection of Lebesgue-absolutely continuous densities over $\\mathbb{R}^{d}$ for $t\\,=\\,0,\\Delta t,\\,\\ldots,1$ and that $\\overrightarrow{\\pi}_{\\mathrm{ref}}(\\mathbf{x}_{t}~\\mid~\\mathbf{x}_{t-\\Delta t})$ , $\\overleftarrow{\\pi}_{\\mathrm{ref}}\\left(\\mathbf{x}_{t-\\Delta t}\\ \\ \\right|\\ \\mathbf{x}_{t}\\right)$ are collections of Lebesgue-absolutely continuous transition kernels. If these densities jointly satisfy the detailed balance condition $\\pi_{\\mathrm{ref}}\\left(\\mathbf{x}_{t}\\right)\\overleftarrow{\\boldsymbol{\\pi}}_{\\mathrm{ref}}\\left(\\mathbf{x}_{t-\\Delta t}\\mid\\mathbf{x}_{t}\\right)=\\pi_{\\mathrm{ref}}\\left(\\mathbf{x}_{t-\\Delta t}\\right)\\overrightarrow{\\boldsymbol{\\pi}}_{\\mathrm{ref}}\\left(\\mathbf{x}_{t}\\mid\\mathbf{x}_{t-\\Delta t}\\right)$ , then they satisfy the conditions to be reference measures. A main result of [37] is that if a pair of forward and backward processes satisfies the trajectory balance constraint (7) jointly with a reward density $r$ , then the forward process $p$ samples from the distribution with density $r$ , with all densities interpreted as relative to the reference measures $\\pi_{\\mathrm{ref}}$ , $\\overleftarrow{\\pi}_{\\mathrm{ref}},\\overrightarrow{\\pi}_{\\mathrm{ref}}$ . ", "page_idx": 16}, {"type": "text", "text": "If $p_{\\theta}$ is a diffusion model that satisfies the TB constraint jointly with some reverse process $q$ and target density $q(\\mathbf{x}_{1})$ , then one can take the reference transition kernels $\\xrightarrow[\\pi_{\\mathrm{ref}},\\,\\overline{{\\pi}}_{\\mathrm{\\,ref}}$ to be $p$ and $q$ , respectively. In this case, the TB constraint for a target density $\\scriptstyle{\\frac{1}{Z}}r\\left(\\mathbf{x}_{1}\\right)$ and forward transition $p_{\\phi}^{\\mathrm{post}}$ is ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{p_{\\phi}^{\\mathrm{post}}(\\mathbf{x}_{0},\\mathbf{x}_{\\Delta t},\\ldots,\\mathbf{x}_{1})}{\\overrightarrow{\\pi}_{\\mathrm{ref}}(\\mathbf{x}_{0},\\mathbf{x}_{\\Delta t},\\ldots,\\mathbf{x}_{1})}=\\frac{\\frac{1}{Z}r(\\mathbf{x}_{1})q(\\mathbf{x}_{0},\\mathbf{x}_{\\Delta t\\rightarrow\\cdots,\\mathbf{x}_{1-\\Delta t}\\mid\\mathbf{x}_{1}})}{\\overleftarrow{\\pi}_{\\mathrm{ref}}(\\mathbf{x}_{0},\\mathbf{x}_{\\Delta t},\\ldots,\\mathbf{x}_{1-\\Delta t}\\mid\\mathbf{x}_{1})},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which is identical to the RTB constraint (8). If (15) holds, then \ud835\udc5dp\ud835\udf19os samples from the distribution with density $\\begin{array}{r}{\\frac{1}{Z}r(\\mathbf{x}_{1})}\\end{array}$ relative to $\\pi_{\\mathrm{ref}}(\\mathbf{x}_{1})$ , which is exactly $\\begin{array}{r}{\\frac{1}{Z}p_{\\theta}(\\mathbf{x}_{1})r(\\mathbf{x}_{1})}\\end{array}$ . We have thus recovered RTB as a case of TB for non-Lebesgue reference measures. ", "page_idx": 17}, {"type": "text", "text": "C Posterior inference on two-dimensional Gaussian mixture model ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Setup We conduct toy experiments in low-dimensional spaces using samples from a Gaussian mixture model with multiple modes to visually demonstrate its validity. The prior distribution $p(\\mathbf{x}_{1})$ is trained on a Gaussian mixture model with 25 evenly weighted modes, while the target posterior $p^{\\mathrm{post}}(\\mathbf{x}_{1})\\,=\\,r(\\mathbf{x}_{1})p(\\mathbf{x}_{1})$ uses a reward $r(\\mathbf{x}_{1})$ to select and re-weight 9 modes from $p(\\mathbf{x}_{1})$ . More specifically, the resulting posterior is: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{p^{\\mathrm{post}}(\\mathbf{x}_{1})=\\displaystyle\\frac{1}{\\sum_{j}\\tilde{\\pi}_{j}}\\sum_{i}\\tilde{\\pi}_{i}N(\\mathbf{x}_{1}\\mid\\mu_{i},\\mathbf{I})}}\\\\ {{\\mathrm{}}}\\\\ {{\\{\\mu_{i}\\}=\\{(-10,-5),(-5,-10),(-5,0),(10,-5),(0,0),(0,5),(5,-5),(5,0),(5,10)\\}}}\\\\ {{\\{\\tilde{\\pi}_{i}\\}=\\{4,10,4,5,10,5,4,15,4\\}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Our objective is to sample from the posterior $p^{\\mathrm{post}}(\\mathbf{x}_{1})$ . We compare our method with several baselines, including policy gradient reinforcement learning (RL) with KL constraint and classifierguided diffusion models. For RL, we implemented the REINFORCE method with a mean baseline and a KL constraint, following recent work training diffusion models to optimize a reward function [7]. Sampling according to the RL policy leads to a distribution $q_{\\theta}(\\mathbf{x}_{1})$ , which is trained with the objective: ", "page_idx": 18}, {"type": "equation", "text": "$$\nJ(\\theta)=\\mathbb{E}_{q_{\\theta}(\\mathbf{x}_{1})}\\left[r(\\mathbf{x}_{1})\\right]+\\alpha D_{\\mathrm{KL}}\\left(q_{\\theta}(\\mathbf{x}_{1})\\|p(\\mathbf{x}_{1})\\right)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "While the exact computation of $K L\\left(q_{\\theta}(\\mathbf{x}_{1})\\|p(\\mathbf{x}_{1})\\right)$ is intractable, we follow the approximation method introduced by Fan et al. [15], which sums the divergence at every diffusion step. This approximation optimizes an upper bound of the marginal KL. ", "page_idx": 18}, {"type": "text", "text": "The other baseline is classifier (energy) guidance, which given a diffusion prior, samples using a posterior score function estimate: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\nabla_{\\mathbf{x}_{t}}\\log p^{\\mathrm{post}}(\\mathbf{x}_{t})\\approx\\nabla_{\\mathbf{x}_{t}}\\log p(\\mathbf{x}_{t})+\\nabla_{\\mathbf{x}_{t}}\\log r(\\mathbf{x}_{t})\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Note that this is a biased approximation of the true intractable score: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\nabla_{\\mathbf{x}_{t}}\\log{p^{\\mathrm{post}}(\\mathbf{x}_{t})}=\\nabla_{\\mathbf{x}_{t}}\\log{p(\\mathbf{x}_{t})}+\\nabla_{\\mathbf{x}_{t}}\\log{\\mathbb{E}_{p(\\mathbf{x}_{1}|\\mathbf{x}_{t})}}\\left[r(\\mathbf{x}_{1})\\right]\\,\\left[\\mathbf{\\Omega}\\right]\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "For our experiments, we follow the source code4 provided in recent diffusion sampler benchmarks [64]. We utilize a batch size of 500, with finetuning at 5,000 training iterations, a learning rate of 0.0001, a diffusion time scale of 5.0, 100 steps, and a log variance range of 4.0. The neural architecture employed is identical to that used in [64]. For pretraining the prior model, we use the same hyperparameters as above, but with 10,000 training iterations using maximum likelihood estimation with true samples. ", "page_idx": 18}, {"type": "text", "text": "Results. As we reported in the main text, in Fig. 1, we present illustrative results. The classifierguided diffusion model shows biased posterior sampling (Fig. 1f), failing to provide accurate inference. RL with a per step KL constraint cannot exactly optimize for the posterior distribution, making the tuning of the KL weight $\\alpha$ crucial to achieving desirable output Fig. C.1. RTB asymptotically achieves the true posterior without introducing a balancing hyperparameter $\\alpha$ . Another advantage of our approach is off-policy exploration for efficient mode coverage. RL methods for fine-tuning diffusion models (e.g., DPOK [15], DDPO [7]) typically use policy gradient style methods that are on-policy. By using a simple off-policy trick introduced by [46, 37] and demonstrated by Sendera et al. [64], we can introduce randomness into the exploration process in diffusion by adding $\\frac{\\epsilon^{2}}{T}$ where $\\epsilon$ is a noise hyperparameter and $T$ is the diffusion timestep, into the variances and annealing it to zero over training iterations. We set $\\epsilon=0.5$ for off-policy exploration. As shown in Fig. C.2, RTB with off-policy exploration gives very close posterior inferences, whereas off-policy exploration in RL with $\\alpha=0.5$ (which is a carefully selected hyperparameter) does not improve performance due to its on-policy nature. ", "page_idx": 18}, {"type": "text", "text": "D Code ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Code for all experiments is available at https://github.com/GFNOrg/diffusion-finetuning and will continue to be maintained and extended. ", "page_idx": 18}, {"type": "image", "img_path": "gVTkMsaaGI/tmp/85a2caa81a31a03de3b4b3ca69a7532c1d94b9fcd62d582bae16badc0cdbe067.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure C.1: Tuning the KL weight $\\alpha$ in reinforcement learning: influences the balance between sticking to the prior distribution and moving towards the modes of the reward density. A higher $\\alpha$ value maintains closer adherence to the prior, while a lower $\\alpha$ allows a gradual shift towards high values of $r(\\mathbf{x})$ . Setting $\\alpha$ below 0.3 tends to cause mode collapse, moving too far from the prior and focusing on maximizing rewards for single modes. $\\alpha=0.5$ gives us samples that closest resembles the posterior. ", "page_idx": 19}, {"type": "image", "img_path": "gVTkMsaaGI/tmp/3cd4fc2bf24381fc014c79abf43767aa0a0560369d14eb689ba8f895123499dd.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure C.2: Off-policy exploration beneftis for RTB training. RTB, with simple off-policy exploration techniques that increase randomness in the diffusion process, significantly improves mode coverage. On the other hand, policy gradient RL methods which are typically used to finetune diffusion models are on-policy, and hence prone to mode collapse. ", "page_idx": 19}, {"type": "text", "text": "E On classifier guidance and RTB posterior sampling ", "text_level": 1, "page_idx": 19}, {"type": "image", "img_path": "gVTkMsaaGI/tmp/d2f9b2aabfa262efa7ce8b3d6e2936ba26c25701972eddd8ed2ceceb977f673e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure E.1: Samples from a posterior model fine-tuned with RL (no KL). We observe early mode collapse, showcasing high-reward samples with minimal diversity. ", "page_idx": 19}, {"type": "text", "text": "E.1 Experimental Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In our experiments, we fine-tune pretrained unconditional diffusion models with our RTB objective, to sample from a posterior distribution in the form $p^{\\mathrm{post}}(x\\mid y)=p(y\\mid x)p(x)$ . In this section, we detail the experimental settings for RTB as well as the compared baselines. ", "page_idx": 19}, {"type": "text", "text": "Experiments setting. For MNIST, we pretrain a noise-predicting diffusion model on $28\\times28$ (upscaled to $32\\times32.$ ) single channel images of digits from the MNIST datasets. We discretize the forward and backward processes into 200 steps and train our model until convergence. For CIFAR-10, we use a pretrained model from [26], trained to generate $32\\!\\times\\!32$ 3-channel images from the CIFAR-10 dataset, while discretizing the noising/denoising processes into 1000 steps. For fine-tuning the prior, we parametrize the posterior with LoRA weights [27], with the number of parameters equal to about $3\\%$ of the prior model\u2019s parameter count. We train our models on a single NVIDA V100 GPU. ", "page_idx": 19}, {"type": "text", "text": "We compute FID as a similarity score estimate of the true posterior distribution from the data. As such, the computation is limited to the total number of per-class-samples present in the data, (between $5\\mathrm{k}$ and $^\\mathrm{6k}$ for CIFAR-10 and MNIST digits, and $30\\mathbf{k}$ for the even/odd task). ", "page_idx": 19}, {"type": "text", "text": "RTB. For RTB fine-tuning, we finetune a diffusion model following the objective in Equation 9. We impose the objective while sampling denoising paths following a DDPM sampling scheme, with only $20\\%$ to $50\\%$ of the original trained steps. We employ loss clipping at 0.1, to account for imperfect constraints in the pretrained prior, and train each of our models for 1500 training iterations, well into convergence trends. ", "page_idx": 20}, {"type": "text", "text": "RL [15]. We implement two RL-based fine-tuning techniques derived from DPOK [15] and DDPO [7], respectively with and without KL regularization. These implementations use a reinforcement learning baseline similar to the one in our experiments described in $\\S3.2$ . By following the same sampling scheme as in our RTB experiments, we enable a direct comparison with RTB. To fine-tune the KL weight, we perform a search over $\\alpha\\in\\{0.01,0.1,1.0\\}$ . ", "page_idx": 20}, {"type": "text", "text": "DP [10]. We implement and adapt the Gaussian version of the posterior sampling scheme in [10], originally devised for noisy inverse problems. This method relaxes some of our experimental constraints, as it requires a differentiable reward $r(\\mathbf{x})$ . We perform a sweep over ten values of the suggest parameter range for the step size $\\zeta\\in[.1,1.]$ on MNIST single-digit sampling, and choose $\\zeta=0.1$ for our experiments. ", "page_idx": 20}, {"type": "text", "text": "LGD-MC [69]. We adapt the implementation of the algorithm in [69] to sample from the classifierbased posteriors in CIFAR-10 and MNIST. Similarly to the DP baseline, we use our pretrained classifier to perform measurements at each sampling step, and use a Monte Carlo estimate of the gradient correction to guide the denoising process. We choose $\\zeta=0.1$ following the DP experiments and default the number of particles to 10 as per the authors\u2019 guidelines. ", "page_idx": 20}, {"type": "text", "text": "E.2 Additional findings. ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Classifier-guidance baselines. We find that the DP and LGD-MC classifier-guidance based baselines struggle to sample from the true posterior distribution in our experimental settings. The baselines achieve the lowers classifier average rewards in all tested settings. Despite choosing $\\zeta=0.1$ as the validate best performing hyperparameter, we also also observe the posterior samples from DP and LGD-MC to be close to the prior. As such, DP and LGD-MC score high in diversity, and low in FID for the Even/Odd experimental scenario, as expected from prior sampling benchmarks, but failing to appropriately model the posterior distribution. ", "page_idx": 20}, {"type": "text", "text": "RL and mode collapse. In the pure Reinforcement Learning objective imposed for the experiments in $\\S3.1$ (no KL), we observe a significantly higher reward than other baseline methods, while showcasing increased FID and lower diversity. In Fig. E.1 we show a random set of 16 samples for posterior models trained on 4 different classes of the CIFAR-10 datasets, as well as the Even objective from the MNIST dataset, after 500 training iterations. In the figure, we observe early mode collapse and reward exploitation, visually evident from the little to no variation amongst samples for each class class, and single-digit collapse in the multi-modal even digits objective (see samples in Fig. 2 for comparison with our RTB-finetuned models). ", "page_idx": 20}, {"type": "text", "text": "Conditional architectures. We repeat the even/odd posterior sampling experiment of $\\S3.1$ in a conditional setting, where the condition is an input to the posterior model. For the posterior architecture, we use a naive modification of the prior with an extra input channel, which is populated a full mask of 0 or 1 for conditioning on the even and odd classes, respectively. The results are shown in Table E.1. We look forward to future work which develops more specialized architectures for handling conditional constraints. ", "page_idx": 20}, {"type": "text", "text": "F Infilling with discrete diffusion ", "text_level": 1, "page_idx": 20}, {"type": "table", "img_path": "gVTkMsaaGI/tmp/4b02c45fdb837820fc46dc998781c91b5bf0bedcf06094d87383058d4a6aa1e1.jpg", "table_caption": ["Table E.1: Conditional experiment for MNIST even/odd posterior. Note that the posterior model in the conditional experiment is different from that in the baselines because it uses a different architecture that includes an additional input channel. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Additional details. We illustrate some examples from the ROC Stories dataset used for training in Table F.1. For the prior we use the sedd-small5 model, which uses an absorbing noising process [4] with a log linear noise schedule, as the diffusion prior $p(\\mathbf{z}\\mid\\mathbf{x})$ . The posterior model is parameterized as a copy of the prior. To condition the diffusion model on the beginning $\\mathbf{X}$ we set the tokens at the appropriate location in the state in the initial time step, $i.e.t=0$ . Our implementation is based on the original SEDD codebase 6. Training this model is computationally expensive (in terms of memory and speed) so we utilize the stochastic TB trick, only propagating the gradients through a subset of the steps of the trajectory. We also use the loss clipping trick as discussed in $\\S2.3$ . Specifically, we clip the loss below a certain threshold to 0, resulting in updates only when the loss is larger. This threshold \u2013 referred to as the loss clipping coefficient \u2013 is a hyperparameter. As this is a conditional problem we also use the relative VarGrad objective. We also use some tempering on the reward likelihood which helps in learning (i.e., $p_{\\mathrm{reward}}(\\mathbf{y}\\mid\\mathbf{x},\\mathbf{z})^{\\beta})$ where $\\beta$ is the inverse temperature parameter. We perform all experiments on an NVIDIA A100-Large GPU. Note that we also tried a baseline of simply fine-tuning the diffusion model on the data but encountered some training instabilities that we could not fix. The hyperparameters used for training RTB in our experiments are detailed in Table F.2. ", "page_idx": 20}, {"type": "table", "img_path": "gVTkMsaaGI/tmp/da76a546684d99fd02852366a69c45f6ee0046b4435e93a1277239ceceea61ad.jpg", "table_caption": ["Table F.1: Examples of training samples for the language infilling task. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "Reward. For training \ud835\udc5dreward we follow the training procedure and implementation from $[28]^{7}$ Specifically, we fine-tune a GPT-2 Large model [56] on the stories dataset with full parameter fine-tuning using the trl library [80]. We trained for 20 epochs with a batch size of 64 and 32 gradient accumulation steps and a learning rate of 0.0005. ", "page_idx": 21}, {"type": "text", "text": "Baselines. For the baselines, we adopt the implementations from [28]. A critical difference in our experiments compared to [28] is that the posterior model is not initialized with a base model that is fine-tuned on the stories dataset. To condition the model on $X$ and $Y$ , as well as for the prompting baseline, we use the following prompt: ", "page_idx": 21}, {"type": "text", "text": "\"Beginning: {X}\\n End: {Y}\\n Middle: ", "page_idx": 21}, {"type": "text", "text": "During training for the autoregressive GFlowNet fine-tuning, a $(\\mathbf{x},\\mathbf{y})$ pair is sampled from the dataset and then sample (batch size) xs for every $(X,Y)$ , and $p_{\\mathrm{reward}}(X Z Y)$ is used as the reward. Both the GFlowNet fine-tuning and supervised fine-tuning baseline use LoRA fine-tuning. We use the default hyperparamteres from [28]. At test time, we sample 100 inflils for each example in the test set from all the models at temperature 0.9, and average over 5 such draws. ", "page_idx": 21}, {"type": "text", "text": "Additional results. Table F.3, Table F.4 and Table F.5 illustrates some examples of the infills generated by the diffusion models. We note that the general quality of the samples is poor, due to a relatively weak prior. At the same time we can observe that the prompting baselines often generate inflils that are unrelated to the current story. We also note that the RTB fine-tuned model can sometimes generate repitions as the reward model tends to assign high likelihood to repititions [84]. We also attempted a LLMEval [41] for evaluating the coherence of the stories but did not obtain statistically significant results. ", "page_idx": 21}, {"type": "table", "img_path": "gVTkMsaaGI/tmp/a2852f84b547ac88efd45daa7ac97337b59755f5c5ee4241fd06a0aa4fa542ac.jpg", "table_caption": ["Table F.2: Hyperparameters for the story infilling task "], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "gVTkMsaaGI/tmp/6c030860e65101341bb9f46b6be8672f6497354b49b6abbb0444ed095ea748b5.jpg", "table_caption": ["Table F.3: Examples of infills generated by the posterior trained with RTB along with reference infills for the stories infilling task. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "G Offline RL ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "G.1 Training details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Our method requires first training a diffusion-based behavior policy $\\pi_{\\theta}$ and a Q-function $Q_{\\psi}$ . Once $\\pi_{\\theta}$ and $Q_{\\psi}$ are trained, The posterior policy $\\pi_{\\gamma}$ is trained using RTB, with its weights initialized to the trained behavior policy weights $\\theta$ . ", "page_idx": 22}, {"type": "text", "text": "The behavior policy $\\pi_{\\theta}$ is parametrized as a state-conditioned noise-predicting denoising diffusion probabilistic model (DDPM) [26] with a linear schedule, and 75 denoising steps. The diffusion model takes as input a state $s$ , a noised action $a_{t}$ and a noise level $t$ and predicts the source noise $\\epsilon$ . The state $s$ and noised action $a_{t}$ are concatenated with Fourier features computed on the noise level $t$ , which are then fed through a 3-layer MLP of hidden dimensionality 256, with layer normalization and a GeLU activation after each hidden layer. The behavior policy is trained using the Adam optimizer with batch size 512 and learning rate 5e-4 for 10000 epochs. The Q-function $Q_{\\psi}$ is trained using IQL. We use the same IQL experimental configurations and training hyperparameters as in [35]. That is, we set $\\tau=0.7$ . The architecture for $Q_{\\psi}$ is a 3-layer MLP with hidden dimensionality 256 and ReLU activations, which is trained using the Adam optimizer with a learning rate 3e-4 and batch size 256 for 750000 gradient steps. The task rewards are normalized as in [35] and the target network is updated with soft updates of $m=0.005$ . The posterior policy $\\pi_{\\gamma}$ is trained using the relative trajectory balance objective. $\\pi_{\\gamma}$ is also parametrized as a state-conditioned noise-predicting DDPM, initialized as a copy of the prior. We additionally use the Langevin dynamics inductive bias (11), and learn an additional MLP for the energy scaling network. The posterior noise prediction network also outputs an additive correction to the output of the prior noise prediction network. That is, the predicted noise of the posterior diffusion model is defined as $\\epsilon(s,a_{t},t):=\\epsilon(s,a_{t},t;\\theta)+\\epsilon(s,a_{t},t;\\gamma)$ , where $\\epsilon(\\cdot;\\theta)$ is the output of the prior noise prediction network and $\\epsilon(\\cdot;\\gamma)$ is the output of the posterior noise prediction network. We train all models on a single NVIDIA A100-Large GPU. The only hyperparameter tuned per task is the temperature $\\alpha$ which we show in Table G.2. ", "page_idx": 22}, {"type": "table", "img_path": "gVTkMsaaGI/tmp/5f603b36c450820f7f5f0db03931b17e9e840b796cc6fd2b34059a5d4d87716a.jpg", "table_caption": ["Table F.4: Examples of inflils generated by Prompt $(\\mathbf{x},\\mathbf{y})$ along with reference inflils for the stories infilling task. "], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "gVTkMsaaGI/tmp/e096f9bed37c50e664164c4a3798b39781d737c2cc9d67eaab37e4ecfeb5c0a5.jpg", "table_caption": ["Table F.5: Examples of infills generated by Prompt $\\bf\\delta(x)$ along with reference infills for the stories infilling task. "], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "gVTkMsaaGI/tmp/4f040802401dd18da072b1ce79ea2cd89370c49818933cc29c40babd4d2cb34c.jpg", "table_caption": ["Table G.1: Mixed vs. online training on DR4L Tasks. We report mean $\\pm$ std over 5 random seeds. "], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "gVTkMsaaGI/tmp/5684aadd0cdd4fb8d40756471f16c33f6d1778dbd7ad1aa39e4ae04b589a691b.jpg", "table_caption": ["Table G.2: Temperature $\\textstyle{\\underline{{\\alpha}={\\frac{1}{\\beta}}}}$ for D4RL tasks "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "Note that in these experiments both the prior and constraint are conditioned on the state s. To prevent having to learn a neural network for $\\log{Z_{\\phi}(\\mathbf{s})}$ , we employ a variant of VarGrad objective [60]. For each state s sampled in the minibatch, we further generate $k=64$ on-policy trajectories $\\tau^{(i)_{}^{k}}{}_{i=1}$ \ud835\udc56=1 with $\\pi_{\\gamma}$ . Each of these trajectories can be used to implicitly estimate $\\log Z(\\mathbf{s})$ : ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\log\\hat{Z}(\\mathbf{s})^{(i)}=\\log\\pi_{\\boldsymbol{\\theta}}(\\tau^{(i)}\\mid\\mathbf{s})+Q_{\\boldsymbol{\\psi}}(\\mathbf{s},\\mathbf{a}_{1}^{(i)})-\\log\\pi_{\\boldsymbol{\\gamma}}(\\tau^{(i)}\\mid\\mathbf{s})\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We then minimize the sample variance across the batch: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{RTB}}^{\\mathrm{VarGrad}}(\\gamma)=\\frac{1}{k}\\sum_{i=1}^{k}\\Bigl(\\log\\hat{Z}(\\mathbf{s})^{(i)}-\\frac{1}{k}\\sum_{j=1}^{k}\\log\\hat{Z}(\\mathbf{s})^{(j)}\\Bigr)^{2}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "RTB allows off-policy training so we are not restricted to train with samples generated on-policy. We thus also leverage the offline dataset, which are samples from the prior and noise them with the DDPM noising process to generate off-policy trajectories with high density under the prior. Since there are actions in the replay buffer from high reward episodes in the tasks, this can help training efficiency compared to purely online training. We ran 5 seeds of training each with mixed training (off-policy and on-policy) and pure on-policy training on the medium-replay tasks, with results shown in Table G.1, where mixed training outperforms pure online training on two of the three tasks. ", "page_idx": 24}, {"type": "text", "text": "G.2 Baseline details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "As is standard in offline RL, we use the reported performance numbers from the previous papers. CQL, IQL are reported from the IQL paper. Diffuser (D), DD, D-QL and QGPO are reported from the QGPO paper. Their implementation improved the performance of D and D-QL compared to their original papers. IDQL results are reported from the IDQL paper. We follow the evaluation protocol of previous work, and report the mean performance over 10 episodes, averaged across 5 random seeds at the end of training (150k training steps). ", "page_idx": 24}, {"type": "text", "text": "H Fine-tuning text-to-image diffusion models ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We build off the DPOK implementation8, which fine-tunes stable-diffusion-v1-5 with ImageReward function. The posterior model to be fine-tuned is initialized as a copy of the prior model. We use LoRA [27] since it is significantly more efficient than fine-tuning the entire model. Sampling of images is done with 50 steps of DDIM [68]. Even with LoRA, it is still difficult to fit gradients of all steps in the diffusion trajectory in memory. To help with this, we use a \u201cstochastic subsampling\u201d trick (\u00a7H.1). ", "page_idx": 25}, {"type": "text", "text": "We train all models on a single NVIDIA A100-Large GPU. For the main experiments, we use the default parameters for DPOK of reward weight $\\beta\\,=\\,10$ and KL weight $=0.01$ . For RTB we fix $\\beta=1.0$ for all prompts. We next perform an ablation over different values of $_\\beta$ . ", "page_idx": 25}, {"type": "text", "text": "We plot in Table H.1 the final average reward and diversity score for models trained with different values of reward weight $_\\beta$ for the prompt \u201cA green colored rabbit.\u201d. As expected, we find that increasing $_\\beta$ increases reward at the cost of diversity for RTB and DPOK. The exception is $\\beta=10$ for RTB which has slightly lower final reward than $\\beta=1$ , which we could attribute to more difficult optimization due to the peaky distribution associated with higher reward weight. ", "page_idx": 25}, {"type": "table", "img_path": "gVTkMsaaGI/tmp/963fc4aa574a9308c5bebf433f1c02e4c5e09002400400023d8476541002902c.jpg", "table_caption": ["Table H.1: Ablation of reward weights $_\\beta$ for \u201cA green colored rabbit.\u201d. "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "H.1 Memory-efficient learning ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We propose two methods to reduce the memory requirement of RTB fine-tuning. ", "page_idx": 25}, {"type": "text", "text": "Stochastic subsampling. The expected gradient of the RTB objective (9) is unaffected by propagating gradient to a randomly sampled subset of the timesteps in a trajectory and rescaling by the inverse proportion of timesteps sampled. Stochastically subsampling timesteps for gradient propagation in this way can significantly decrease memory consumption because computation graphs for the remaining timesteps do not need to be maintained; however, such subsampling increases gradient variance, so it is preferable to keep gradients for as many timesteps as possible to fit in memory. For our text-to-image experiments, we found sampling 8 timesteps out of 50 to keep gradients was sufficient. ", "page_idx": 25}, {"type": "text", "text": "Batched gradient computation. An important property of the RTB objective is that computing its gradient does not require storing the computation graph of all timesteps. The gradient of the RTB objective for a single trajectory is just the sum of per-step log-likelihood gradients scaled by the RTB residual: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\nabla_{\\phi}\\mathcal{L}_{\\mathrm{RTB}}(\\tau;\\phi)=2\\left(\\log\\frac{Z_{\\phi}}{r(\\mathbf{x}_{1})}+\\sum_{i=1}^{T}\\log\\frac{p_{\\phi}^{p o s t}(\\mathbf{x}_{i\\Delta t}\\mid\\mathbf{x}_{(i-1)\\Delta t})}{p_{\\theta}(\\mathbf{x}_{i\\Delta t}\\mid\\mathbf{x}_{(i-1)\\Delta t})}\\right)\\boldsymbol{\\cdot}\\nabla_{\\phi}\\sum_{i=1}^{T}\\log p_{\\phi}^{\\mathrm{post}}(\\mathbf{x}_{i\\Delta t}\\mid\\mathbf{x}_{(i-1)\\Delta t}).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Because the likelihood gradients can be accumulated during the forward pass, this allows for a batched gradient accumulation version of the update. For trajectory length (number of diffusion steps) $T$ and accumulation batch size (number of time steps receiving a gradient signal in each backward pass) \ud835\udc35, the number of batched forward passes required scales as \ud835\udc47\ud835\udc35. ", "page_idx": 25}, {"type": "text", "text": "Only the accumulation batch size $B$ , not the trajectory length $T$ , is constrained by the memory budget. This means we can easily scale training with large number of diffusion steps without increasing the variance of the gradient through stochastic subsampling, with training time growing linearly with number of time steps under a fixed memory budget. Although this method is not used in the main experiments presented here, preliminary experiments confirm these observations. ", "page_idx": 26}, {"type": "text", "text": "We highlight that both methods are not applicable to diffusion samplers based on differentiable simulation (e.g., PIS and DDS), which need to store the entire computation graph of SDE integration. For these methods, the memory requirement scales linearly with the trajectory length. ", "page_idx": 26}, {"type": "text", "text": "H.2 Generated images ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "H.2.1 A green-colored rabbit ", "page_idx": 27}, {"type": "image", "img_path": "gVTkMsaaGI/tmp/51ed4c0cdf9f8562f5bee4cd450f7a9c8ebcbed19da55e519300851565899c7b.jpg", "img_caption": ["Figure H.1: Prior "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "gVTkMsaaGI/tmp/741b9ed495ad2bb4c7e7f1f9667e1fc89bd2a82f44c4a2f08a3b6a569a787fda.jpg", "img_caption": ["Figure H.2: DDPO "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "gVTkMsaaGI/tmp/cc397feb968f76e8d6e99671f378fbdc7c2c83a98b21948bcb1c6d11fa7ec36f.jpg", "img_caption": ["Figure H.3: DPOK "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "gVTkMsaaGI/tmp/f7296073bf09b529bc9484d01b35839a4eaa9ffa81a145895d3db5c8c1b99b7a.jpg", "img_caption": ["Figure H.4: RTB "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "gVTkMsaaGI/tmp/be0e5634be88f30febdab2c53a68579eedf4c6ddf157fb0278a5d84b9b67fa85.jpg", "img_caption": ["Figure H.5: Prior "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "gVTkMsaaGI/tmp/61444f19bf86fb6e77f0152c6570a449f880917a06c013f62a7b5e467b86064c.jpg", "img_caption": ["Figure H.6: DDPO "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "gVTkMsaaGI/tmp/8648ff25733115035da09ffbcba1417bd9997589b9fa29509e05bbac5a8d6e2a.jpg", "img_caption": ["Figure H.7: DPOK "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "gVTkMsaaGI/tmp/7b40f4e159268b52b1e7b38e4e6779269c5549d47d101c6b5a76cfeff1a7bb18.jpg", "img_caption": ["Figure H.8: RTB "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "gVTkMsaaGI/tmp/89c408c82265db21fefaf5677a3adcb38a5732dccb18c4ffc940493dc43fe535.jpg", "img_caption": [], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "", "img_caption": ["Figure H.12: RTB "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "H.2.4 A half - masked rugged laboratory engineer man with cybernetic enhancements as seen from a distance, scif icharacter portrait by greg rutkowski, esuthio, craig mullins. ", "page_idx": 28}, {"type": "image", "img_path": "gVTkMsaaGI/tmp/211d331aab08ff95a64a0caa1fa711e071eaddd4a1b4bdeaefc83b4614167696.jpg", "img_caption": [], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "gVTkMsaaGI/tmp/f21c87707477c7dc35ee90250c3160d52920cb1e03b4018cef5e11febcc55348.jpg", "img_caption": ["Figure H.14: DDPO "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "gVTkMsaaGI/tmp/a50b043aade8b918d875eb53bdcf512ee7b2dd75d85405dc463ea244389842d6.jpg", "img_caption": ["Figure H.15: DPOK "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "gVTkMsaaGI/tmp/18dfc14786497947c223460c86a9851b58aa516dcba20454e709743f3b73301d.jpg", "img_caption": ["Figure H.16: RTB "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "I Compute resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "For classifier guidance experiments $\\S3.1$ we use train on a single NVIDIA V100 GPU. For textconditional image generation $\\S3.2$ , text infilling $\\S\\mathrm{F}$ and offline $\\S\\mathrm{G}$ , we use a single NVIDIA A100 large GPU. The total estimated compute time for all our experiments is 3000 hours. ", "page_idx": 28}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We discuss our theoretical claims about the RTB ojective in $\\S2$ , and report experimental results in $\\S3$ . ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 29}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We discuss the limitations of our proposed method in $\\S5$ . ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: See $\\S\\mathrm{A}$ . ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 29}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We have provided code to reproduce our experiments in $\\S D$ , and described training details in $\\S C$ , $\\S E$ , $\\S\\mathrm{F}.$ , $\\S\\mathrm{G}$ and $\\S\\mathrm{H}$ . ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. ", "page_idx": 29}, {"type": "text", "text": "\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 30}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: Code is provided in $\\S D$ and can be used to reproduce our main results. We will be releasing our code publicly.. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 30}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Training details are outlined in $\\S C$ , $\\S\\mathrm{E}$ , $\\S\\mathrm{F}$ , and $\\S\\mathrm{G}$ . ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 31}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: All experimental results have error bars, except the Stable Diffusion finetuning experiment $\\S3.2$ which was only trained on one seed per prompt due to compute constraints. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 31}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Compute resources used in experiments is outlined in $\\S E$ , $\\S\\mathrm{F}.$ , $\\S\\mathrm{G}$ , $\\S\\mathrm{H}$ and summarised in $\\S\\mathrm{I}$ . ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. ", "page_idx": 31}, {"type": "text", "text": "\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 32}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The paper conforms to the stated ethics guidelines. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 32}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We discuss broader impact of the work in $\\S5$ ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 32}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: We do not release models that have risk of misuse. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 32}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 33}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: Creaters of code we use are properly credited. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 33}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: We do not release new assets. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 33}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: We do not have any crowdsourcing experiments or research with human subjects. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 34}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: We do not have any experiments with human subjects ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 34}]