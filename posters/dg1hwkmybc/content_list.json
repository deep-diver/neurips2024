[{"type": "text", "text": "FINCON: A Synthesized LLM Multi-Agent System with Conceptual Verbal Reinforcement for Enhanced Financial Decision Making ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yangyang $\\mathbf{Y}\\ensuremath{\\mathbf{u}}^{1,\\star}$ , Zhiyuan $\\mathbf{Ya0}^{1,\\star}$ , Haohang ${\\bf L i^{1,\\star}}$ , Zhiyang $\\mathbf{Deng^{1,\\star}}$ , Yuechen $\\mathbf{J}\\mathbf{iang}^{1,\\star}$ , Yupeng $\\mathbf{Cao^{1,\\star}}$ Zhi $\\mathbf{Chen^{1,\\star}}$ , Jordan W. Suchow1, Zhenyu $\\mathbf{Cui}^{1}$ , Rong ${\\bf L i u^{1}}$ , Zhaozhuo $\\mathbf{X}\\mathbf{u}^{1}$ , Denghui Zhang1 Koduvayur Subbalakshmi1, Guojun Xiong2, Yueru $\\mathbf{He^{3}}$ , Jimin Huang 3, Dong $\\mathbf{Li}^{3}$ , Qianqian $\\mathbf{Xie^{3,\\dagger}}$ ", "page_idx": 0}, {"type": "text", "text": "1Stevens Institute of Technology 2Harvard University 3The Fin AI \u22c6These authors contributed equally \u2020 Corresponding author: qianqian.xie@yale.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large language models (LLMs) have shown potential in complex financial tasks, but sequential financial decision-making remains challenging due to the volatile environment and the need for intelligent risk management. While LLM-based agent systems have achieved impressive returns, optimizing multi-source information synthesis and decision-making through timely experience refinement is underexplored. We introduce FINCON, an LLM-based multi-agent framework with CONceptual verbal reinforcement for diverse FINancial tasks. Inspired by real-world investment firm structures, FINCON employs a manager-analyst hierarchy, enabling synchronized cross-functional agent collaboration towards unified goals via natural language interactions. Its dual-level risk-control component enhances decision-making by monitoring daily market risk and updating systematic investment beliefs through self-critique. These conceptualized beliefs provide verbal reinforcement for future decisions, selectively propagated to relevant agents, improving performance while reducing unnecessary peer-to-peer communication costs. FINCON generalizes well across tasks, including single stock trading and portfolio management. 1 ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The intricacies and fluctuations inherent in financial markets pose significant challenges for making high-quality, sequential investment decisions. In tasks such as single stock trading and portfolio management, each intelligent decision is driven by multiple market interactions and the integration of diverse information streams, characterized by varying levels of timeliness and modalities [1, 2]. The primary objective of these tasks is to maximize profti while managing present market risks in an open-ended environment. ", "page_idx": 0}, {"type": "text", "text": "In practice, trading firms often depend on synthesized teamwork, structured hierarchically with functional roles such as data analysts, risk analysts, and portfolio managers communicating across levels [3, 4]. These roles are responsible for the careful integration of diverse resources. However, the cognitive limitations of human team members can hinder their capacity to rapidly process market signals and achieve optimal investment outcomes [5]. ", "page_idx": 0}, {"type": "text", "text": "To enhance investment returns and address the limitations of human decision-making, various studies have explored methods such as deep reinforcement learning (DRL) to develop agent systems that simulate market environments and automate investment strategies [6, 7, 8]. Concurrently, advancements in large language models (LLMs) have shown great potential in performing complex tasks, including reasoning [9, 10], tool-using [11], planning [12], decision-making [13, 14], and even in various financial applications [15, 16, 17, 18, 19], suggesting they may surpass existing agent architectures. Language agents, in particular, are distinguished by their human-like communication and flexible, prompt-based structures, making them well-suited to diverse decision-making settings [20, 21, 22, 23]. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To achieve optimal decision-making performance, two critical factors must be considered: (1) Organizing agents to facilitate effective teamwork and efficient communication, and (2) Enabling agents to continuously learn and refine their actions. Studies have shown that mimicking human organizational structures can successfully coordinate language agents for specific tasks [24, 25, 26]. Additionally, recent advances in textual gradient-based prompt optimization [27, 28] and verbal reinforcement [29, 30] have proven effective in iteratively improving the reasoning and decisionmaking capabilities of language agents. ", "page_idx": 1}, {"type": "text", "text": "Language agent systems designed for financial decision-making, such as FINGPT [31], FINMEM [32], and FINAGENT [33], have shown strong performance. However, they face several limitations. First, their reliance on agents\u2019 risk preferences based on short-term market fluctuations fails to control longterm risk exposure, potentially overlooking fundamental factors driving investment returns. A more effective approach is to quantify investment risks using established measures of risk from quantitative finance [34, 35]. Second, these systems are often limited to single-asset trading tasks, making them less adaptable to multi-asset financial applications like portfolio management. Third, they place significant pressure on a single agent to understand and process information within a constrained context window, which can degrade decision quality. Although approaches like STOCKAGENT [36] use multi-agent systems for stock trading, their reliance on extensive discussions between numerous LLM agents leads to high communication costs and slow decision-making. Moreover, the absence of a clear optimization objective can compromise outcome effectiveness. Additional related work in the literature is discussed in the Appendix A.1. ", "page_idx": 1}, {"type": "text", "text": "To address these issues, we propose FINCON, an LLM-based multi-agent framework for critical financial tasks, such as single-stock trading and portfolio management, as shown in Figure 1. Our main contributions are: 1) Inspired by real-world investment roles, we introduce a novel Synthesized Manager-Analyst hierarchical communication structure with a risk-control component. This structure allocates financial data from different sources to corresponding functional analyst agents, allowing them to focus on specific insights, while the manager consolidates these inputs to make informed trading decisions. The streamlined communication reduces redundant peer-to-peer interaction, lowering costs and improving efficiency. 2) Our framework generalizes beyond stock trading to handle portfolio management, an area not previously addressed by other financial language agent systems. 3) We developed a dual-level risk control component to update risk assessments both within and across episodes. Within episodes, risk is supervised using the Conditional Value at Risk (CVaR), a quantile-based risk measure [37]. Across episodes, we introduced a verbal reinforcement mechanism, where investment beliefs are updated based on reasoning trajectories and profti-and-loss (PnL) trends, distilled into conceptual perspectives. These insights are selectively back-propagated from the manager to relevant analyst agents. Our ablation studies demonstrate the effectiveness of this risk control design in managing market risk and enhancing trading performance. ", "page_idx": 1}, {"type": "image", "img_path": "dG1HwKMYbC/tmp/6d2d2b94103ad233e72eb0c17c9370989ecf3c3fff41b8eca1aea382725458e6.jpg", "img_caption": ["Figure 1: The general framework of FINCON. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Here, we outline the mathematical notations for the two major financial decision-making tasks that will be explicitly discussed in our work. We also formally present the generalized modeling formulation using a Partially Observable Markov Decision Process (POMDP) [38] for financial decision-making tasks. ", "page_idx": 2}, {"type": "text", "text": "2.1 Financial Decision-making Tasks Formulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Single Stock Trading Tasks. FINCON uses analyst agents group $\\{M_{p r}^{i}\\}_{i=1}^{I}$ to process multi-modal market information sources. The processed information is then used by a manager agent $M_{a}$ to make trading decisions (buy, sell, hold), and to provide relevant reasoning texts. Note that the \u201csell\u201d signal means the system makes a \u201cshort-selling\u201d decision, that is, a negative trading position is allowed. Additionally, FINCON evaluates the daily investment risk, followed by prompt-optimization for the manager agent from risk-control component $M_{r}$ . ", "page_idx": 2}, {"type": "text", "text": "Portfolio Trading Tasks. In addition to processing multi-modal market information, the analyst agents also construct a stock pool for portfolio management by considering the statistical correlations between stock returns. The manager agent then makes trading decisions for each stock in the pool. Finally, the manager agent determines the portfolio weights for all stocks using an external optimization solver that applies the mean-variance optimization described below [39]: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\mathbf{w}}\\langle\\mathbf{w},{\\boldsymbol{\\mu}}\\rangle-\\langle\\mathbf{w},{\\boldsymbol{\\Sigma}}\\mathbf{w}\\rangle\\quad\\mathrm{s.t.}\\;w_{n}=\\left\\{\\in[0,1],\\begin{array}{l l}{\\quad\\cdot\\mathbf{\\hat{b}}\\mathbf{u}\\mathbf{y}^{\\prime},}\\\\ {\\quad\\cdot\\mathbf{\\hat{s}}_{\\mathbf{\\hat{s}}}\\mathbf{l}\\mathbf{\\hat{l}}^{\\prime},}\\\\ {\\quad=0,\\quad\\cdot\\mathbf{\\hat{h}}\\mathbf{o}\\mathbf{l}\\mathbf{d}^{\\prime},}\\end{array}\\right.\\forall n\\in\\{1,\\cdots,N\\}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathbf{w}=(w_{1}\\cdots,w_{N})\\in\\mathbb{R}^{N}$ is portfolio weights vector, $\\mu$ and $\\Sigma$ are the shrinkage estimators of $N$ -dimensional sample expected return and $N\\times N$ sample covariance matrix of chosen stocks\u2019 daily return sequences respectively [35]. We note that portfolio weights are rebalanced on daily basis. In our implementation, we begin by calculating the portfolio weights through solving the aforementioned optimization problem. Next, the target positions are determined by linearly scaling these portfolio weights from the previous step. ", "page_idx": 2}, {"type": "text", "text": "2.2 Modeling Quantitative Trading as POMDP ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Formally, we model quantitative trading task as an infinite horizon POMDP [40, 41] with time index $\\mathbb{T}=\\{0,1,2,\\cdots\\}$ and discount factor $\\alpha\\in(0,1]$ . The components of this model are as follows: (1) a state space $\\mathcal X\\times\\mathcal X$ where $\\mathcal{X}$ is the observable component and $\\boldsymbol{\\wp}$ is unobservable component of the financial market; (2) the action space of analyst agents group is $\\begin{array}{r}{\\mathcal{A}=\\prod_{i=1}^{I}\\mathcal{A}^{i}}\\end{array}$ , where $A^{i}$ represents the collection of processed market information in textual format done by agent $i$ (total $I$ analyst agents), and for manager agent, its action space is A, which is modeled as $\\{\\ ^{\\ast}b u y^{\\prime\\prime},\\ ^{\\ast}s e l l^{\\prime\\prime},\\ ^{\\ast}h o l d^{\\prime\\prime}\\}$ for single stock trading task and as $(\\{~^{\\ast}b u y^{\\prime\\prime},~^{\\ast}s e l l^{\\prime\\prime},~^{\\ast}h o l d^{\\prime\\prime}\\}\\times[-1,1])^{\\otimes N}$ for portfolio management task among $N$ -stocks; (3) the reward function $\\mathcal{R}(o,b,a):\\mathcal{X}\\times\\mathcal{Y}\\times\\mathbb{A}\\to\\mathbb{R}$ uses daily profit $\\&$ loss $(\\mathrm{PnL})$ as the output; (4) the observation process $\\{O_{t}\\}_{t\\in\\mathbb{T}}\\subseteq\\mathcal{X}$ is an $I$ -dimensional process, with the $i^{t h}$ entry $\\{O_{t}^{i}\\}_{t\\in\\mathbb{T}}$ representing one type of uni-modal information flow solely processed by the analyst agent $i$ ; (5) the reflection process $\\{B_{t}\\}_{t\\in\\mathbb{T}}\\,\\subseteq\\,\\mathcal{D}$ represents the manager agent\u2019s self-reflection, which is updated from $B_{t}$ to $B_{t+1}$ on daily basis [42]); (7) the processed information flow $\\hat{O}_{t}=(\\hat{O}_{t}^{1},\\cdot\\cdot\\cdot\\,,\\hat{O}_{t}^{I})\\in\\mathcal{A},\\forall\\,t\\in\\mathbb{T}$ , which represents the information processing outputs from analyst agents group. ", "page_idx": 2}, {"type": "text", "text": "Then, our multi-agent system is supposed to learn the policies of all agents: the policies of analyst agents $\\pi_{\\theta^{i}}^{i}\\,:\\,\\mathcal{X}\\,\\rightarrow\\,A^{i},i\\,\\in\\,\\{1,\\cdots\\,,I\\}$ (the ways to process information, i.e. $\\hat{O}_{t}^{i}\\,\\sim\\,\\pi_{\\theta^{i}}^{i}(\\cdot|O_{t}^{i}))$ , and the policy of manager agent $\\pi_{\\theta^{a}}:\\mathcal{A}\\times\\mathcal{V}\\rightarrow\\mathbb{A}$ (the ways to make trading decisions, i.e. $A_{t}\\sim\\pi_{\\theta^{a}}(\\cdot|\\hat{O}_{t},B_{t}))$ such that the system maximizes cumulative trading reward while controlling risk [43]. All policies $\\Pi_{\\pmb{\\theta}}=(\\{\\pi_{\\theta^{i}}^{i}\\}_{i=1}^{I^{\\bullet}},\\pi_{\\theta^{a}})$ are parameterized by textual prompts $\\pmb{\\theta}=(\\{\\theta^{i}\\}_{i=1}^{I},\\theta^{a})$ . By updating prompts via the risk-control component $M_{r}$ , the whole system optimizes policies $\\Pi_{\\theta}$ in a verbal reinforcement manner. By denoting daily profti & loss (PnL) by $R_{t}^{\\Pi_{\\theta}}=\\mathcal{R}(O_{t},B_{t},A_{t})$ , the optimization objective for the whole system can be written as: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\theta}\\mathbb{E}\\Big[\\sum_{t\\in\\mathbb{T}}\\alpha^{t}R_{t}^{\\Pi^{\\theta}}\\Big]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "is a risk-sensitive optimization problem that leverages textual gradient descent, fundamentally differing from DRL algorithms designed for POMDPs. Further details on the textual gradient descent approach are provided in the Appendix A.2. ", "page_idx": 3}, {"type": "text", "text": "3 Architecture of FINCON ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we present the architecture of FINCON using a two-level hierarchy. First, we describe the hierarchical framework for coordinating the agents\u2019 synchronous work and communication. Then, we elaborate on the functionalities of each module that constitutes each agent in FINCON. Finally, we aim to elaborate on how FINCON solves the objective function expressed as Equation (2) through a verbal reinforcement approach. ", "page_idx": 3}, {"type": "text", "text": "3.1 Synthesized Multi-agent Hierarchical Structure Design ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The agent system of FINCON consists of two main components: the Manager-Analyst Agent Group component and the Risk-Control component. ", "page_idx": 3}, {"type": "image", "img_path": "dG1HwKMYbC/tmp/3b4079fc0c5e664b432c90049987780780a02ce8922e0c22a74c1c7470a69e3c.jpg", "img_caption": ["Figure 2: The detailed architecture of FINCON contains two key components: Manager-Analyst agent group and Risk Control. It also presents the between-component interaction of FINCON and decision-making flow. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "3.1.1 Manager-Analyst Agent Group ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Analogous to human investment firm, FINCON establishes a unique hierarchical structure to organize its multi-agent system, synthesizing their efforts to achieve superior decision-making outcomes. ", "page_idx": 3}, {"type": "text", "text": "The primary goal is to enhance information presentation and comprehension while minimizing unnecessary communication costs. The working mechanism of each agent is illustrated in Figure 2. ", "page_idx": 4}, {"type": "text", "text": "Analyst Agents. In FINCON, analyst agents distill concise investment insights from large volumes of multi-source market data, each focused on a specific trading target. To ensure high-quality reasoning by reducing task load and sharpening focus, each agent processes information from a single source in a uni-modal fashion, providing pre-specified outputs based on prompts. This setup mimics an efficient human team, where each analyst specializes in a specific function, filtering out market noise and extracting key insights. These agents assist the manager agent by consolidating denoised investment information from multiple perspectives. We implement seven distinct types of analyst agents using LLMs, each producing unique investment insights, as shown in the upper section of Figure 2. Based on input modalities, three textual data processing agents extract insights and sentiments from daily news and financial reports. An audio agent uses the Whisper API to interpret investment signals from earnings call recordings. Additionally, a data analysis agent and a stock selection agent compute critical financial metrics, such as momentum and CVaR, using tabular time series data. The stock selection agent also oversees portfolio selection by applying the classic risk diversification method in quantitative finance [1]. ", "page_idx": 4}, {"type": "text", "text": "Manager Agent. In FINCON, the manager agent acts as the sole decision-maker, responsible for generating trading actions for sequential financial tasks. For portfolio management, it calculates portfolio weights using convex optimization techniques constrained by directional trading decisions (see optimization problem as presented in Formula (1)). Four key mechanisms support each decision: 1) Consolidating distilled insights from multiple analyst agents. 2) Receiving timely risk alerts and conceptual investment updates from the risk control component. 3) Refining its investment beliefs about the influence of different information sources on trading decisions for specific targets. 4) Conducting self-reflection by reviewing reasoning outcomes from previous trading actions. ", "page_idx": 4}, {"type": "text", "text": "3.1.2 Risk-Control Component ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We have innovatively designed a dual-level risk-control mechanism consisting of within-episode and over-episode risk management. The within-episode mechanism detects market risk within a single training episode, allowing the manager agent to promptly adjust trading actions to mitigate potential losses by accounting for short-term trading performance and market fluctuations. This mechanism also operates during the testing phase. In contrast, the over-episode mechanism functions exclusively during the training stage, providing prompt optimization guidance by comparing the trading performance of the current episode with the previous one. This reflection enables the manager agent to update its investment beliefs based on performance differences. By drawing on prior observations of market risk and profitability patterns, these two mechanisms help avoid repeated investment errors, thereby enhancing future returns. ", "page_idx": 4}, {"type": "text", "text": "Within-Episode Risk Control: The within-episode risk alert is triggered by a sudden drop in the CVaR value. Conditional Value at Risk (CVaR) represents the average of the worst-performing $1\\%$ of daily trading Proftis and Losses (PnLs). A decrease in CVaR typically indicates that recent trading decisions have led to PnLs within this bottom percentile, signaling a potentially high-risk market condition. When this occurs, the manager agent adopts a risk-averse stance for that day\u2019s trading actions, regardless of the prior risk status. ", "page_idx": 4}, {"type": "text", "text": "Over-Episode Risk Control: The over-episode investment belief updates facilitate adjustments in the emphasis placed on analysts\u2019 information distillation and the manager\u2019s action generation. Through the Actor-Critic mechanism, FINCON episodically optimizes its investment strategy for a given trading target, as defined by objective (Equation (2)), by reflecting on a series of winning and losing actions. This episodic reflection is powered by a unique Conceptual Verbal Reinforcement (CVRF). CVRF assesses the performance of consecutive training episodes by analyzing the information perspectives provided by analysts and reflected in the manager\u2019s decision-making. It then conceptualizes and attributes the evaluation outcomes to these specific aspects. By comparing the conceptualized insights from more profitable versus less profitable episodes, the system informs both the manager and analyst agents about necessary belief adjustments, helping prioritize the most relevant market information for increased proftiability, as detailed in Algorithm 1. CVRF leverages text-based gradient descent to offer optimal conceptual investment guidance for the manager agent, refining prompts with the latest investment beliefs. The guidance is organized according to perspectives provided by the respective analyst agents, key financial indicators (such as historical momentum), or other crucial viewpoints. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "table", "img_path": "dG1HwKMYbC/tmp/9b6dcdce5b831835181fbbed2d3f505d92989207a661a837180754198a773d23.jpg", "table_caption": ["Table 1: Analogy between glossaries in model optimizer and prompt optimizer. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "These belief updates are first received by the manager agent and then selectively propagated to relevant agents, minimizing over-communication. Unlike the text-based gradient descent proposed by Tang et al.[28], which uses prompt editing distance as a learning rate, we derive investment belief updates by measuring the overlapping percentage of trading actions between two consecutive training trajectories at each belief update, as presented in Table 1. This approach has proven effective in improving the performance of a synthesized agent system, where each worker has a clearly defined and specialized role. The above describes the workflow of FINCON during the training stage, while the workflow during the testing stage is detailed in the Appendix A.3. ", "page_idx": 5}, {"type": "table", "img_path": "dG1HwKMYbC/tmp/9a57248ad0ed78893d382b35e8d9abf93ef6acb3a18219a89174f8611a95565f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "3.2 Modular Design of FINCON Agents ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Here, we explain the modular design of FINCON agents. Inspired by the recent works of Park et al. [44] and Sumers et al. [45] on developing the cognitive structure of language agents for human-like behavior, agents in FINCON integrate four modules to support their necessary functionalities, along with a shared general configuration, as detailed in Appendix A.4: ", "page_idx": 5}, {"type": "text", "text": "General Configuration and Profiling Module. This module defines task types (e.g., stock trading, portfolio management) and specifies trading targets, including sector and performance details. The profiling module outlines each agent\u2019s roles and responsibilities. The concatenated textual content from these parts is used to query investment-related events from the agents\u2019 memory databases. Perception Module. This module defines how each agent interacts with the market, specifying the information they perceive, receive, and communicate, with interactions tailored to each agent\u2019s role. In detail, it converts raw market data, feedback from other agents, and information retrieved from the memory module into formats compatible with large language models, enabling them to process these inputs effectively. Memory Module. The memory module comprises three key components: ", "page_idx": 5}, {"type": "text", "text": "working memory, procedural memory, and episodic memory. Much like how humans process events in their working memory [46], FINCON agents leverage their working memory to perform a range of tasks, including observation, distillation, and refinement of available memory events, all tailored to the specific roles of the agents. Procedural memory and episodic memory are critical for recording historical actions, outcomes, and reflections during sequential decision-making. Procedural memory is generated after each decision step within an episode, storing data as memory events. For trading inquiries, top events are retrieved from procedural memory and ranked based on recency, relevance, and importance, following a simplified version of the method proposed by Yu et al. [32], with further details provided in Appendix A.13. Each functional analyst agent has distinct procedural memory decay rates, reflecting the timeliness of various financial data sources, which is crucial for aligning multi-type data influencing specific time points and supporting informed decision-making. The manager agent enhances the procedural memory of analyst agents by providing feedback through an access counter. Both analyst and manager agents maintain procedural memory, but they keep different records, as illustrated in Appendix A.4. Episodic memory, exclusive to the manager agent, stores actions, PnL series from previous episodes, and updated conceptual investment beliefs from the risk control component. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Our experiment answers the key research questions (RQs): RQ1: Does FINCON demonstrate robustness across multiple financial decision-making tasks, especially single-asset trading and portfolio management? RQ2: Is the within-episode risk control mechanism in FINCON effective in maintaining superior decision-making performance? RQ3: Is the over-episode risk control mechanism in FINCON effective in timely updating the manager agent\u2019s beliefs to enhance trading performance? ", "page_idx": 6}, {"type": "text", "text": "4.1 Experimental Setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "(i) Multi-Modal Datasets. We construct a market environment representation using real-world financial data, including stock prices, daily news, company fliings (Form 10-Q, Form 10-K, etc.), and ECC audio from January 3, 2022, to June 10, 2023, as detailed in Appendix. A.8. Each data source is assigned to specific analyst agents based on its timeliness. (ii) Evaluation Metrics. We evaluate FINCON and other state-of-the-art (SOTA) agents using metrics such as Cumulative Return $(\\mathbf{CR}\\%)$ , Sharpe Ratio (SR), and Max Drawdown $(\\mathrm{MDD}\\%)$ ). CR and SR are prioritized because they provide comprehensive insights into overall performance and risk-adjusted returns, essential for informed investment decisions. In contrast, MDD focuses on evaluating the potential for significant losses, making it a secondary consideration in this context. Details are provided in Appendix A.10. (iii) Comparative Methods. For single-stock trading, we compare FINCON with DRL agents (A2C, PPO, DQN) and LLM-based agents (GENERATIVE AGENT (GA), FINGPT, FINMEM, FINAGENT) as well as the Buy-and-Hold (B & H) strategy. For portfolio management, we compare FINCON with Markowitz MV, FinRL-A2C, and Equal-Weighted ETF strategy, with further details provided in Appendix A.12. The detailed experiment parameter configurations of the above agent systems are articulated in Appendix. A.14. (iv) Implementation Details. All LLM-based agents use GPT-4-Turbo, with temperature set at 0.3. FINCON is trained from January 3, 2022, to October 4, 2022, and tested from October 5, 2022, to June 10, 2023. DRL agents are trained over the period from January 1, 2018, to October 4, 2022, to ensure that there is sufficient data available for model convergence. Performance is based on the median CR and SR from five repeated epochs. For a more detailed explanation of the experimental setup, please refer to the Appendix A.5. ", "page_idx": 6}, {"type": "text", "text": "4.2 Main Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In response to RQ1, we analyze FINCON\u2019s performance on two types of financial decision-making tasks: single-asset trading and portfolio management. The system\u2019s ability to manage these sequentially complex decisions is thoroughly evaluated in the following sections. ", "page_idx": 6}, {"type": "text", "text": "4.2.1 Single Asset Trading Task ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this task, we evaluate FINCON\u2019s performance against other leading algorithmic trading models by trading eight different stocks. As presented in the tables above, FINCON significantly outperforms both LLM-based and DRL-based approaches in terms of CRs and SRs. Additionally, FINCON achieves one of the lowest MDD values across most trading assets, demonstrating effective risk management while still delivering the highest investment returns. For detailed performance comparisons across all models and metrics, refer to Table 1. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Overall, even with extended training periods, DRL-based models tend to underperform, with the A2C algorithm lagging significantly behind other agents in general. Notably, the training periods for Nio Inc. (NIO) and Coinbase Global Inc. (COIN) require clarification. NIO, which completed its IPO in September 2018, has a slightly shorter training period than other tickers, yet the DRL algorithms for NIO still achieved convergence. In contrast, Coinbase Global Inc. (COIN), which completed its IPO in April 2021, presented a more significant challenge due to the limited available trading data, causing DRL algorithms to struggle with convergence. This limitation underscores a major drawback for DRL agents when trading recently listed IPOs. Consequently, our analysis of COIN focuses on comparisons between FINCON, LLM-based agents, and the buy-and-hold (B & H) strategy. In this context, FINCON demonstrates a clear advantage, achieving a cumulative return of over $57\\%$ and a Sharpe ratio of 0.825. Furthermore, LLM-based agents, which can leverage diverse data types and require minimal training, effectively mitigate the challenges faced by DRL algorithms. ", "page_idx": 7}, {"type": "table", "img_path": "dG1HwKMYbC/tmp/0a28b25a8cc6237351804534c1c16ae3aba19e7060595759593b3da272941021.jpg", "table_caption": [], "table_footnote": ["Table 2: Comparison of key performance metrics during the testing period for the single-asset trading tasks involving six stocks, between FINCON and other algorithmic agents. Note that the highest and second highest CRs and SRs have been tested and found statistically significant using the Wilcoxon signed-rank test. The highest CRs and SRs are highlighted in red, while the second highest are marked in blue. "], "page_idx": 7}, {"type": "text", "text": "In alignment with market trends, FINCON consistently exhibits superior decision-making quality compared to other LLM-based agents, regardless of market conditions\u2014whether bullish (e.g., GOOG, MSFT), bearish (e.g., NIO), or mixed (e.g., TSLA). We attribute this performance to its high-quality distillation of information through a synthesized multi-agent collaboration mechanism, combined with its dual-level risk control design, positioning FINCON as a leader in the space. By contrast, FINGPT primarily relies on sentiment analysis of financial information, failing to fully exploit the potential of LLMs to integrate nuanced textual insights with numerical financial indicators. Similarly, GA and FINMEM use single-agent frameworks without sophisticated information distillation processes or a diverse toolset, placing heavy cognitive demand on the agent to process multi-source information, especially when dealing with large and varied data modalities. Moreover, their static or minimal investment belief systems result in weak filtering of market noise. As illustrated in Figure 7 (a) & (b) of Appendix A.7.2, this limitation leads these models to consistently hold lower positions and hesitate between \u2018buy\u2019 or \u2018sell\u2019 decisions, ultimately resulting in suboptimal performance. ", "page_idx": 7}, {"type": "text", "text": "FINCON overcomes these challenges through its innovative multi-agent synthesis, enabling it to deliver superior outcomes. Although FINAGENT performs well when integrating images and tabular data, it struggles to remain competitive when incorporating audio data, such as ECC recordings, which are critical in real-world trading. Additionally, FINAGENT relies on similarity-based memory retrieval, which can lead to decisions based on outdated information, often resulting in errors. In contrast, FINCON\u2019s memory structure accounts for the varying timeliness of multi-source financial data, significantly enhancing decision quality and overall performance. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "4.2.2 Portfolio Management Task ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this task, we compare FINCON\u2019s performance with the Markowitz Mean-Variance (MV) portfolio [47] and FINRL [48] in managing two small portfolios: Portfolio 1 (TSLA, MSFT, and PFE) and Portfolio 2 (AMZN, GM, and LLY). These assets were selected by the stock selection agent from a pool of 42 stocks, each with sufficient news data (over 800 news articles during the combined training and testing periods), as illustrated in Figure 9 in Appendix A.9. The training and testing periods, the backbone model and the parameter settings are consistent with those used in the single-asset trading task. For the Markowitz MV portfolio, we estimate the covariance matrix and expected returns using the same training data. In the case of FINRL, we use five years of training data prior to the test period. As detailed in Table 3 and Figure 3, our results show that FINCON outperforms both the Markowitz MV portfolio and FINRL as well as the market baseline \u2013 Equal-Weighted ETF, achieving significantly higher CRs and SRs, as well as MDDs. ", "page_idx": 8}, {"type": "text", "text": "However, managing multi-asset portfolios introduces more complexity, leading to a higher likelihood of hallucination compared to single-asset trading. This is due to the increased input length and complexity involved in multi-asset decision-making. While FINCON mitigates this issue by distributing tasks across specialized agents that focus on critical investment insights, it occasionally generates incorrect information, such as non-existent indices of memory events. Handling multi-asset decision-making requires sophisticated logic and substantial market information, which poses a significant challenge for LLMs when processing extended contexts. This complexity has left portfolio management relatively unexplored in previous language agent studies. Nonetheless, FINCON demonstrates considerable potential by constructing agent systems that can tackle complex financial tasks through effective resource optimization, even when managing relatively compact portfolios. ", "page_idx": 8}, {"type": "table", "img_path": "dG1HwKMYbC/tmp/48f49f605b5c63ae8d7d177b59a720641f3f0e0423bd9b919ec18db06ea3a8ff.jpg", "table_caption": ["Table 3: Key performance metrics comparison among all portfolio management strategies of Portfolio 1 & 2. FINCON leads all performance metrics. "], "table_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "dG1HwKMYbC/tmp/cfc9cc745a867c954ffccdc6784668886f5b779e19b08d0558a6660ea72b03c9.jpg", "img_caption": ["Figure 3: Portfolio values of Portfolio 1 & 2 changes over time for all the strategies. The computation of portfolio value refers to Equation 7 in Appendix A.10. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.3 Ablation Studies ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In response to RQ2 and RQ3, we conduct a comprehensive evaluation of our unique risk control component through two ablation studies. Both studies maintain consistency with the training and testing periods used in the main experiments. The first study examines the effectiveness of the withinepisode risk control mechanism, which leverages Conditional Value at Risk (CVaR) to manage risk in real-time, as detailed in Table 4. Comparisons on primary metrics illustrate that the success of utilizing CVaR for within-episode risk control is evident in both bullish and bearish market environments in the single asset trading case. Moreover, in portfolio trading with mixed price trends, our within-episode risk control mechanism performs robustly by monitoring the entire portfolio\u2019s value fluctuations. The second study focuses on the over-episode risk control mechanism, demonstrating its critical role in updating the trading manager agent\u2019s beliefs to provide a more comprehensive understanding of current trading conditions, as articulated in Table 5. The markedly improved CRs and SRs in both decision-making scenarios underscore the effectiveness of using CVRF to update investment beliefs episodically, guiding the agent towards more profitable investment strategies. Additionally, FINCON demonstrates significant learning gains, achieving these results after only four training episodes\u2014substantially fewer than what is typically required by traditional RL algorithmic trading agents. More visualizations and analysis are provided in the Appendix A.7. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "table", "img_path": "dG1HwKMYbC/tmp/8373d97526e2616a4b8c592688140f5d7fa7cffde9ec2e2c365285811842e5e6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "dG1HwKMYbC/tmp/5cabd6d40c025af300201792b6aeb07e13beb61f1fbde06fc62fde62ec2c241b.jpg", "table_caption": ["Table 4: Key metrics FINCON with vs. without implementing CVaR for within-episode risk control. The performance of FINCON with the implementation of CVaR won a leading performance in both single-asset trading and portfolio management tasks. "], "table_footnote": ["Table 5: Key metrics FINCON with vs. without implementing belief updates for over-episode risk control. The performance of FINCON with the implementation of CVRF won a leading performance in both single-asset trading and portfolio management tasks. "], "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we present FINCON, a novel LLM-based multi-agent framework for financial decisionmaking tasks, including single stock trading and portfolio management. Central to FINCON is the Synthesized Manager-Analyst hierarchical communication structure and a dual-level risk control component. This communication method channels financial data from multiple sources to specialized analyst agents, who distill it into key investment insights. The manager agent then synthesizes these insights for decision-making. Our experimental evaluations demonstrate the efficacy of our risk control mechanism in mitigating investment risks and enhancing trading performance. Additionally, the streamlined communication structure reduces overhead. The dual-level risk control component introduces a novel approach to defining agent personas, enabling dynamic updates of risk and market beliefs within agent communication. A valuable future research direction would be to scale FINCON\u2019s framework to manage large-sized portfolios comprising tens of assets, while maintaining the impressive decision-making quality demonstrated with smaller portfolios. Given the LLM\u2019s input length constraint, a critical challenge lies in striking an optimal balance between information conciseness through agent distillation and potential performance deterioration when extending the current context window. Addressing this will be essential for ensuring quality-assured outcomes. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Harry Markowitz. Portfolio selection. The Journal of Finance, 7(1):77\u201391, 1952.   \n[2] Xuan-Hong Dang, Syed Yousaf Shah, and Petros Zerfos. \" the squawk bot\": Joint learning of time series and text data modalities for automated financial information filtering. arXiv preprint arXiv:1912.10858, 2019.   \n[3] John L Maginn, Donald L Tuttle, Dennis W McLeavey, and Jerald E Pinto. Managing investment portfolios: a dynamic process, volume 3. John Wiley & Sons, 2007.   \n[4] Roy Radner. The organization of decentralized information processing. Econometrica: Journal of the Econometric Society, pages 1109\u20131146, 1993.   \n[5] George A Miller. The magical number seven, plus or minus two: Some limits on our capacity for processing information. Psychological review, 63(2):81, 1956.   \n[6] Rundong Wang, Hongxin Wei, Bo An, Zhouyan Feng, and Jun Yao. Commission fee is not enough: A hierarchical reinforced framework for portfolio management. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 626\u2013633, 2021.   \n[7] Weiguang Han, Boyi Zhang, Qianqian Xie, Min Peng, Yanzhao Lai, and Jimin Huang. Select and trade: Towards unified pair trading with hierarchical reinforcement learning. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 4123\u20134134, 2023.   \n[8] Molei Qin, Shuo Sun, Wentao Zhang, Haochong Xia, Xinrun Wang, and Bo An. Earnhft: Efficient hierarchical reinforcement learning for high frequency trading. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 14669\u201314676, 2024.   \n[9] Jie Huang and Kevin Chen-Chuan Chang. Towards reasoning in large language models: A survey. arXiv preprint arXiv:2212.10403, 2022.   \n[10] Mingyu Jin, Qinkai Yu, Haiyan Zhao, Wenyue Hua, Yanda Meng, Yongfeng Zhang, Mengnan Du, et al. The impact of reasoning step length on large language models. arXiv preprint arXiv:2401.04925, 2024.   \n[11] Tianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, and Denny Zhou. Large language models as tool makers. arXiv preprint arXiv:2305.17126, 2023.   \n[12] Jingqing Ruan, Yihong Chen, Bin Zhang, Zhiwei Xu, Tianpeng Bao, Hangyu Mao, Ziyue Li, Xingyu Zeng, Rui Zhao, et al. Tptu: Task planning and tool usage of large language model-based ai agents. In NeurIPS 2023 Foundation Models for Decision Making Workshop, 2023.   \n[13] Chan Hee Song, Jiaman Wu, Clayton Washington, Brian M Sadler, Wei-Lun Chao, and Yu Su. Llm-planner: Few-shot grounded planning for embodied agents with large language models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2998\u20133009, 2023.   \n[14] Bhargavi Paranjape, Scott Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer, and Marco Tulio Ribeiro. Art: Automatic multi-step reasoning and tool-use for large language models. arXiv preprint arXiv:2303.09014, 2023.   \n[15] Qianqian Xie, Weiguang Han, Xiao Zhang, Yanzhao Lai, Min Peng, Alejandro Lopez-Lira, and Jimin Huang. Pixiu: A comprehensive benchmark, instruction dataset and large language model for finance. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 33469\u201333484. Curran Associates, Inc., 2023.   \n[16] Xiao Zhang, Ruoyu Xiang, Chenhan Yuan, Duanyu Feng, Weiguang Han, Alejandro LopezLira, Xiao-Yang Liu, Meikang Qiu, Sophia Ananiadou, Min Peng, Jimin Huang, and Qianqian Xie. D\u00f3lares or dollars? unraveling the bilingual prowess of financial llms between spanish and english. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD \u201924, page 6236\u20136246, New York, NY, USA, 2024. Association for Computing Machinery.   \n[17] Qianqian Xie, Weiguang Han, Zhengyu Chen, Ruoyu Xiang, Xiao Zhang, Yueru He, Mengxi Xiao, Dong Li, Yongfu Dai, Duanyu Feng, et al. The finben: An holistic financial benchmark for large language models. arXiv preprint arXiv:2402.12659, 2024.   \n[18] Gang Hu, Ke Qin, Chenhan Yuan, Min Peng, Alejandro Lopez-Lira, Benyou Wang, Sophia Ananiadou, Wanlong Yu, Jimin Huang, and Qianqian Xie. No language is an island: Unifying chinese and english in financial large language models, instruction data, and benchmarks. arXiv preprint arXiv:2403.06249, 2024.   \n[19] Yuzhe Yang, Yifei Zhang, Yan Hu, Yilin Guo, Ruoli Gan, Yueru He, Mingcong Lei, Xiao Zhang, Haining Wang, Qianqian Xie, et al. Ucfe: A user-centric financial expertise benchmark for large language models. arXiv preprint arXiv:2410.14059, 2024.   \n[20] Joon Sung Park, Joseph C O\u2019Brien, Carrie J Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra of human behavior. arXiv preprint arXiv:2304.03442, 2023.   \n[21] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. Autogen: Enabling next-gen llm applications via multi-agent conversation framework. arXiv preprint arXiv:2308.08155, 2023.   \n[22] Shuofei Qiao, Ningyu Zhang, Runnan Fang, Yujie Luo, Wangchunshu Zhou, Yuchen Eleanor Jiang, Chengfei Lv, and Huajun Chen. Autoact: Automatic agent learning from scratch via self-planning. arXiv preprint arXiv:2401.05268, 2024.   \n[23] Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, et al. Metagpt: Meta programming for multi-agent collaborative framework. arXiv preprint arXiv:2308.00352, 2023.   \n[24] Xudong Guo, Kaixuan Huang, Jiale Liu, Wenhui Fan, Natalia V\u00e9lez, Qingyun Wu, Huazheng Wang, Thomas L Griffiths, and Mengdi Wang. Embodied llm agents learn to cooperate in organized teams. arXiv preprint arXiv:2403.12482, 2024.   \n[25] Junkai Li, Siyu Wang, Meng Zhang, Weitao Li, Yunghwei Lai, Xinhui Kang, Weizhi Ma, and Yang Liu. Agent hospital: A simulacrum of hospital with evolvable medical agents. arXiv preprint arXiv:2405.02957, 2024.   \n[26] Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chi-Min Chan, Heyang Yu, Yaxi Lu, Yi-Hsin Hung, Chen Qian, et al. Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors. In The Twelfth International Conference on Learning Representations, 2023.   \n[27] Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, and Michael Zeng. Automatic prompt optimization with\" gradient descent\" and beam search. arXiv preprint arXiv:2305.03495, 2023.   \n[28] Xinyu Tang, Xiaolei Wang, Wayne Xin Zhao, Siyuan Lu, Yaliang Li, and Ji-Rong Wen. Unleashing the potential of large language models as prompt optimizers: An analogical analysis with gradient-based model optimizers. arXiv preprint arXiv:2402.17564, 2024.   \n[29] Weiran Yao, Shelby Heinecke, Juan Carlos Niebles, Zhiwei Liu, Yihao Feng, Le Xue, Rithesh Murthy, Zeyuan Chen, Jianguo Zhang, Devansh Arpit, et al. Retroformer: Retrospective large language agents with policy gradient optimization. arXiv preprint arXiv:2308.02151, 2023.   \n[30] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36, 2024.   \n[31] Hongyang Yang, Xiao-Yang Liu, and Christina Dan Wang. Fingpt: Open-source financial large language models. arXiv preprint arXiv:2306.06031, 2023.   \n[32] Yangyang Yu, Haohang Li, Zhi Chen, Yuechen Jiang, Yang Li, Denghui Zhang, Rong Liu, Jordan W Suchow, and Khaldoun Khashanah. Finmem: A performance-enhanced llm trading agent with layered memory and character design. arXiv preprint arXiv:2311.13743, 2023.   \n[33] Wentao Zhang, Lingxuan Zhao, Haochong Xia, Shuo Sun, Jiaze Sun, Molei Qin, Xinyi Li, Yuqing Zhao, Yilei Zhao, Xinyu Cai, et al. Finagent: A multimodal foundation agent for financial trading: Tool-augmented, diversified, and generalist. arXiv preprint arXiv:2402.18485, 2024.   \n[34] Freddy Delbaen and Sara Biagini. Coherent risk measures. Springer, 2000.   \n[35] Frank J Fabozzi, Sergio M Focardi, and Petter N Kolm. Quantitative equity investing: Techniques and strategies. John Wiley & Sons, 2010.   \n[36] Chong Zhang, Xinyi Liu, Mingyu Jin, Zhongmou Zhang, Lingyao Li, Zhengting Wang, Wenyue Hua, Dong Shu, Suiyuan Zhu, Xiaobo Jin, et al. When ai meets finance (stockagent): Large language model-based stock trading in simulated real-world environments. arXiv preprint arXiv:2407.18957, 2024.   \n[37] Keith Kuester, Stefan Mittnik, and Marc S Paolella. Value-at-risk prediction: A comparison of alternative strategies. Journal of Financial Econometrics, 4(1):53\u201389, 2006.   \n[38] Matthijs TJ Spaan. Partially observable markov decision processes. In Reinforcement learning: State-of-the-art, pages 387\u2013414. Springer, 2012.   \n[39] Frank J Fabozzi, Harry M Markowitz, and Francis Gupta. Portfolio selection. Handbook of finance, 2, 2008.   \n[40] Yang Liu, Qi Liu, Hongke Zhao, Zhen Pan, and Chuanren Liu. Adaptive quantitative trading: An imitative deep reinforcement learning approach. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 2128\u20132135, 2020.   \n[41] Taylan Kabbani and Ekrem Duman. Deep reinforcement learning approach for trading automation in the stock market. IEEE Access, 10:93564\u201393574, 2022.   \n[42] Thomas L Griffiths, Jian-Qiao Zhu, Erin Grant, and R Thomas McCoy. Bayes in the age of intelligent machines. arXiv preprint arXiv:2311.10206, 2023.   \n[43] Ashwin Rao and Tikhon Jelvis. Foundations of reinforcement learning with applications in finance. Chapman and Hall/CRC, 2022.   \n[44] Theodore Sumers, Shunyu Yao, Karthik Narasimhan, and Thomas L. Griffiths. Cognitive architectures for language agents, 2023.   \n[45] Jintian Zhang, Xin Xu, and Shumin Deng. Exploring collaboration mechanisms for llm agents: A social psychology view. arXiv preprint arXiv:2310.02124, 2023.   \n[46] Anthony D Wagner. Working memory contributions to human learning and remembering. Neuron, 22(1):19\u201322, 1999.   \n[47] Harry M Markowitz and G Peter Todd. Mean-variance analysis in portfolio choice and capital markets, volume 66. John Wiley & Sons, 2000.   \n[48] Xiao-Yang Liu, Hongyang Yang, Qian Chen, Runjia Zhang, Liuqing Yang, Bowen Xiao, and Christina Dan Wang. Finrl: A deep reinforcement learning library for automated stock trading in quantitative finance. arXiv preprint arXiv:2011.09607, 2020.   \n[49] Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with dynamic memory and self-reflection. arXiv preprint arXiv:2303.11366, 2023.   \n[50] Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, and Gao Huang. Expel: Llm agents are experiential learners. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 19632\u201319642, 2024.   \n[51] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R\u00e9mi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation with alphacode. Science, 378(6624):1092\u20131097, 2022.   \n[52] Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, and Weizhu Chen. Codet: Code generation with generated tests. arXiv preprint arXiv:2207.10397, 2022.   \n[53] Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, and Maosong Sun. Communicative agents for software development. arXiv preprint arXiv:2307.07924, 2023.   \n[54] Zelai Xu, Chao Yu, Fei Fang, Yu Wang, and Yi Wu. Language agents with reinforcement learning for strategic play in the werewolf game. arXiv preprint arXiv:2310.18940, 2023.   \n[55] Weiyu Ma, Qirui Mi, Xue Yan, Yuqiao Wu, Runji Lin, Haifeng Zhang, and Jun Wang. Large language models play starcraft ii: Benchmarks and a chain of summarization approach. arXiv preprint arXiv:2312.11865, 2023.   \n[56] J de Curt\u00f2, I de Zarz\u00e0, Gemma Roig, Juan Carlos Cano, Pietro Manzoni, and Carlos T Calafate. Llm-informed multi-armed bandit strategies for non-stationary environments. Electronics, 12(13):2814, 2023.   \n[57] Huaqin Zhao, Zhengliang Liu, Zihao Wu, Yiwei Li, Tianze Yang, Peng Shu, Shaochen Xu, Haixing Dai, Lin Zhao, Gengchen Mai, et al. Revolutionizing finance with llms: An overview of applications and insights. arXiv preprint arXiv:2401.11641, 2024.   \n[58] Zhiwei Liu, Xin Zhang, Kailai Yang, Qianqian Xie, Jimin Huang, and Sophia Ananiadou. Fmdllama: Financial misinformation detection based on large language models. arXiv preprint arXiv:2409.16452, 2024.   \n[59] Yupeng Cao, Zhi Chen, Qingyun Pei, Fabrizio Dimino, Lorenzo Ausiello, Prashant Kumar, KP Subbalakshmi, and Papa Momar Ndiaye. Risklabs: Predicting financial risk using large language model based on multi-sources data. arXiv preprint arXiv:2404.07452, 2024.   \n[60] Qianqian Xie, Dong Li, Mengxi Xiao, Zihao Jiang, Ruoyu Xiang, Xiao Zhang, Zhengyu Chen, Yueru He, Weiguang Han, Yuzhe Yang, et al. Open-finllms: Open multimodal large language models for financial applications. arXiv preprint arXiv:2408.11878, 2024.   \n[61] Lorenzo Canese, Gian Carlo Cardarilli, Luca Di Nunzio, Rocco Fazzolari, Daniele Giardino, Marco Re, and Sergio Span\u00f2. Multi-agent reinforcement learning: A review of challenges and applications. Applied Sciences, 11(11):4948, 2021.   \n[62] Kaiqing Zhang, Zhuoran Yang, and Tamer Ba\u00b8sar. Multi-agent reinforcement learning: A selective overview of theories and algorithms. Handbook of reinforcement learning and control, pages 321\u2013384, 2021.   \n[63] Jakob Foerster, Ioannis Alexandros Assael, Nando De Freitas, and Shimon Whiteson. Learning to communicate with deep multi-agent reinforcement learning. Advances in neural information processing systems, 29, 2016.   \n[64] Toru Lin, Jacob Huh, Christopher Stauffer, Ser Nam Lim, and Phillip Isola. Learning to ground multi-agent communication with autoencoders. Advances in Neural Information Processing Systems, 34:15230\u201315242, 2021.   \n[65] Woojun Kim, Jongeui Park, and Youngchul Sung. Communication in multi-agent reinforcement learning: Intention sharing. In International Conference on Learning Representations, 2020.   \n[66] Changxi Zhu, Mehdi Dastani, and Shihan Wang. A survey of multi-agent reinforcement learning with communication. arXiv preprint arXiv:2203.08975, 2022.   \n[67] Zhiyuan Yao, Zheng Li, Matthew Thomas, and Ionut Florescu. Reinforcement learning in agent-based market simulation: Unveiling realistic stylized facts and behavior. arXiv preprint arXiv:2403.19781, 2024.   \n[68] HaoHang Li and Steve Y Yang. Impact of false information from spoofing strategies: An abm model of market dynamics. In 2022 IEEE Symposium on Computational Intelligence for Financial Engineering and Economics (CIFEr), pages 1\u201310. IEEE, 2022.   \n[69] Kuan Wang, Yadong Lu, Michael Santacroce, Yeyun Gong, Chao Zhang, and Yelong Shen. Adapting llm agents through communication. arXiv preprint arXiv:2310.01444, 2023.   \n[70] Zhao Mandi, Shreeya Jain, and Shuran Song. Roco: Dialectic multi-robot collaboration with large language models. arXiv preprint arXiv:2307.04738, 2023.   \n[71] Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun Du, Joshua B Tenenbaum, Tianmin Shu, and Chuang Gan. Building cooperative embodied agents modularly with large language models. arXiv preprint arXiv:2307.02485, 2023.   \n[72] Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch. Improving factuality and reasoning in language models through multiagent debate. arXiv preprint arXiv:2305.14325, 2023.   \n[73] Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu. Chateval: Towards better llm-based evaluators through multi-agent debate. arXiv preprint arXiv:2308.07201, 2023.   \n[74] Frank Xing. Designing heterogeneous llm agents for financial sentiment analysis. arXiv preprint arXiv:2401.05799, 2024.   \n[75] Irene de Zarz\u00e0 i Cubero, Joaquim de Curt\u00f2 i D\u00edaz, Gemma Roig, and Carlos T Calafate. Optimized financial planning: Integrating individual and cooperative budgeting models with llm recommendations. AI, 5(1):91\u2013114, 2024.   \n[76] Xiangpeng Wan, Haicheng Deng, Kai Zou, and Shiqi Xu. Enhancing the efficiency and accuracy of underlying asset reviews in structured finance: The application of multi-agent framework. arXiv preprint arXiv:2405.04294, 2024.   \n[77] Chong Zhang, Xinyi Liu, Mingyu Jin, Zhongmou Zhang, Lingyao Li, Zhengting Wang, Wenyue Hua, Dong Shu, Suiyuan Zhu, Xiaobo Jin, et al. When ai meets finance (stockagent): Large language model-based stock trading in simulated real-world environments. arXiv preprint arXiv:2407.18957, 2024.   \n[78] Patrick Bolton and Mathias Dewatripont. The firm as a communication network. The Quarterly Journal of Economics, 109(4):809\u2013839, 1994.   \n[79] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022.   \n[80] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824\u201324837, 2022.   \n[81] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems, 36, 2024.   \n[82] Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. Reasoning with language model is planning with world model. arXiv preprint arXiv:2305.14992, 2023.   \n[83] Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, et al. Ghost in the minecraft: Generally capable agents for open-world enviroments via large language models with text-based knowledge and memory. arXiv preprint arXiv:2305.17144, 2023.   \n[84] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. A survey on large language model based autonomous agents. Frontiers of Computer Science, 18(6):1\u201326, 2024.   \n[85] Leslie Pack Kaelbling, Michael L Littman, and Andrew W Moore. Reinforcement learning: A survey. Journal of artificial intelligence research, 4:237\u2013285, 1996.   \n[86] Guojun Xiong, Shufan Wang, Daniel Jiang, and Jian Li. Personalized federated reinforcement learning with shared representations. In Deployable RL: From Research to Practice $@$ Reinforcement Learning Conference 2024, 2024.   \n[87] Guojun Xiong, Ujwal Dinesha, Debajoy Mukherjee, Jian Li, and Srinivas Shakkottai. Dopl: Direct online preference learning for restless bandits with preference feedback. arXiv preprint arXiv:2410.05527, 2024.   \n[88] Zhiyuan Yao, Ionut Florescu, and Chihoon Lee. Control in stochastic environment with delays: A model-based reinforcement learning approach. In Proceedings of the International Conference on Automated Planning and Scheduling, volume 34, pages 663\u2013670, 2024.   \n[89] Bin Zhang, Hangyu Mao, Jingqing Ruan, Ying Wen, Yang Li, Shao Zhang, Zhiwei Xu, Dapeng Li, Ziyue Li, Rui Zhao, et al. Controlling large language model-based agents for large-scale decision-making: An actor-critic approach. arXiv preprint arXiv:2311.13884, 2023.   \n[90] John Hull. Risk Management and Financial Institutions. John Wiley & Sons, 2007.   \n[91] William F. Sharpe. The sharpe ratio. The Journal of Portfolio Management, 21(1):49\u201358, 1994.   \n[92] Andrew Ang and Joseph Chen. Downside risk. Journal of Portfolio Management, 29(4):103\u2013 112, 2003.   \n[93] Xiao-Yang Liu, Guoxuan Wang, and Daochen Zha. Fingpt: Democratizing internet-scale data for financial large language models. arXiv preprint arXiv:2307.10485, 2023.   \n[94] Yu Qin and Yi Yang. What you say and how you say it matters: Predicting stock volatility using verbal and vocal cues. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 390\u2013401, 2019.   \n[95] Linyi Yang, Tin Lok James Ng, Barry Smyth, and Riuhai Dong. Html: Hierarchical transformer-based multi-task learning for volatility prediction. In Proceedings of The Web Conference 2020, pages 441\u2013451, 2020.   \n[96] Yupeng Cao, Zhi Chen, Qingyun Pei, Prashant Kumar, KP Subbalakshmi, and Papa Momar Ndiaye. Ecc analyzer: Extract trading signal from earnings conference calls using large language model for stock performance prediction. arXiv preprint arXiv:2404.18470, 2024. [97] John C Hull. Options, Futures, and Other Derivatives. Pearson Education, 2017.   \n[98] Xiao-Yang Liu, Hongyang Yang, Jiechao Gao, and Christina Dan Wang. FinRL: Deep reinforcement learning framework to automate trading in quantitative finance. ACM International Conference on AI in Finance (ICAIF), 2021.   \n[99] Xiao-Yang Liu, Ziyi Xia, Jingyang Rui, Jiechao Gao, Hongyang Yang, Ming Zhu, Christina Wang, Zhaoran Wang, and Jian Guo. Finrl-meta: Market environments and benchmarks for data-driven financial reinforcement learning. Advances in Neural Information Processing Systems, 35:1835\u20131849, 2022.   \n[100] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International conference on machine learning, pages 1928\u20131937. PMLR, 2016.   \n[101] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.   \n[102] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.   \n[103] Yuliya Plyakha, Raman Uppal, and Grigory Vilkov. Why does an equal-weighted portfolio outperform value-and price-weighted portfolios? Available at SSRN 2724535, 2012. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "[104] Jaap MJ Murre and Joeri Dros. Replication and analysis of ebbinghaus\u2019 forgetting curve. PloS one, 10(7):e0120644, 2015. ", "page_idx": 16}, {"type": "text", "text": "[105] Guardrails ai. https://docs.guardrailsai.com. Open source library for interacting with Large Language Models. ", "page_idx": 16}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "A.1 Related Work ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "LLM Agents for Financial Decision Making. There are considerable efforts towards developing general-purpose LLM agent for sequential decision-making [49, 50], and such type of tasks often involve episodic interactions with environment and verbal reflections for action refinement, such as coding competition [51, 52], software development [53, 23], game-playing [54, 55]. Furthermore, researchers have started to exploit how LLM agents can perform better in harder decision-making tasks from finance [56, 57, 58, 59, 60], in which there are more volatile environments, leading to that the numerous unpredictable elements can obscure an agent\u2019s ability to reflect accurately on the reasons for poor decision outcomes. FinMem [32] enhances single stock trading performance by embedding memory modules with LLM agent for reflection-refinement, and FinAgent [33] improved trading profits via using external quantitative tool to fight against volatile environment. ", "page_idx": 17}, {"type": "text", "text": "Multi-Agent System and Communication Structures. In traditional multi-agent systems [61, 62], the way for agents\u2019 communication is pre-determined, like sharing data or state observations [63, 64, 65, 66, 67, 68]. The emergence of large language model brings flexibility for human-understandable communications [69, 20, 23, 70], so some work tries to elevate decision-making ability of LLMbased multi-agent system by letting agents engage in discussions [71, 21] or debates [72, 73]. The similar peer-communication strategy was as well utilized by the multi-agent system for financial tasks [74, 75, 76]. However, such approach are not optimal for unified-goal financial tasks that prioritize profits [77], because they suffer from potentially ambiguous optimization objectives and are unable to control the unnecessary communication costs [78]. ", "page_idx": 17}, {"type": "text", "text": "Prompt Optimization and Verbal Reinforcement. To enhance the reasoning or decision-making of LLM agents, many prompt optimization techniques have been proposed, like ReAct [79], Chain of Thought (CoT) [80], Tree of Thoughts (ToT) [81], ART [14], intended for that LLM agents can automatically generate intermediate reasoning steps as an iterative program. In addition, to make LLM agents make decisions like humans and generate more understandable reasoning texts, some researchers recommend incorporating cognitive structures [82, 83, 44, 84]. Inspired by these previous work and DRL algorithms [85, 86, 87, 67, 88], verbal reinforcement [29, 30, 89, 24] was developed for LLM agents such that they can update actions based on iterative self-reflection while integrating additional LLM as a prompt optimizer [27, 28]. ", "page_idx": 17}, {"type": "text", "text": "A.2 Textual Gradient-Descent ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In an LLM-based prompt optimizer, a meta-prompt [27, 28] is used to refine the task prompt for better performance. For example, for a mathematical reasoning task, the task prompt might be \"Let\u2019s solve the problem,\" while the meta-prompt could be \"Improve the prompt to help a model better perform mathematical reasoning.\" ", "page_idx": 17}, {"type": "text", "text": "Although prompt optimization lacks explicit gradients to control the update direction, we can simulate \u201ctextual gradient\u201d by using LLMs\u2019 reflection capabilities. By generating feedback from past successes and failures on trading decisions, LLMs can produce \"semantic\" gradient signals that guide the optimization process. ", "page_idx": 17}, {"type": "text", "text": "Adjusting the optimization process\u2019s direction is crucial, similar to tuning the learning rate in traditional parameter optimization. An inappropriate learning rate can cause the process to oscillate or converge too slowly. Similarly, without proper control, the LLM-based optimizer might overshoot or oscillate during prompt optimization. ", "page_idx": 17}, {"type": "text", "text": "To mimic learning rate effects, we measure the overlapping percentage between trading decision sequences from consecutive iterations. We then directly edit the previous task prompt to enhance performance. The meta-prompt instructs the LLM to modify the current prompt based on feedback, ensuring a stable and incremental improvement process. This method allows for effective exploitation of existing prompts, leading to gradual performance enhancement. ", "page_idx": 17}, {"type": "text", "text": "A.3 FINCON Testing Stage Workflow ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "During the testing stage, FINCON will utilize the investment beliefs learned from the training stage, and the over-episode risk control mechanism will no longer operate. However, the within-episode risk control mechanism will still function, allowing the manager agent to adjust trading actions in real-time based on short-term trading performance and market fluctuations. This ensures that even during testing, FINCON can promptly respond to market risks and potentially prevent losses while leveraging the knowledge gained during training. ", "page_idx": 18}, {"type": "text", "text": "Algorithm 2 Testing Stage Algorithm of FINCON ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Initialize trading start date $s$ , stock pool of portfolio and portfolio weights $w_{0}=\\mathbf{0}$ .   \nInherit manager-analysts component $\\{M_{p r}^{i}\\}_{i=1}^{I}\\&M_{a}$ .   \nInherit the reflections $B$ , learned prompts $\\pmb{\\theta}$ , the trained policy $\\Pi_{\\theta}$ .   \nfor $T+1\\leq t\\leq S$ do Run policy $\\Pi_{\\theta}$ (collecting daily PnL $r_{t}$ , portfolio weights $w_{t}$ and daily CVaR value $\\rho_{t}\\$ ). if $\\rho_{t}<\\rho_{t-1}$ or $r_{t}<0$ then Trigger $M_{a}$ self-reflection. end if Get one investment trajectory $\\mathcal{H}$ .   \nend for   \nOutput performance metrics calculation results based on $\\mathcal{H}$ . ", "page_idx": 18}, {"type": "text", "text": "A.4 Figure of Modular Design of Agents in FINCON ", "text_level": 1, "page_idx": 19}, {"type": "table", "img_path": "dG1HwKMYbC/tmp/9ddee713d28bbc67d215d84f2c48a4baeabecff31c1f895c9fe96d6a84c66684.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "Profiling Module ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Manager Agent: ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "1. Role assignment: You are an experienced trading manager in the investment firm ... 2. Role description: Your responsibilities are to consolidate investment insights from analysts and make trading actions on {asset symbols} ... ", "page_idx": 19}, {"type": "text", "text": "Analyst Agents: ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "1. Role assignment: You are the investment analysts for news/ market data/ Form 10-K (Q)/ ECC audio recording ...   \n2. Role duty description: Your responsibilities are to distill investment insights and other indicators like financial sentiment for {asset symbols} ... ", "page_idx": 19}, {"type": "text", "text": "Perception Module ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Manager Agent: ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "1. Perceive: Investment insights from analyst agents; daily risk alert and episode-level investment belief updates from the risk control component. 2. Send: Feedback to analyst agents about their contribution to significant investment ", "page_idx": 19}, {"type": "text", "text": "Analyst Agents: ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "1. Perceive: Market information from certain information sources.   \n2. Send: Relevant market insights to manager agent.   \n3. Receive: Feedback from the manager agent. ", "page_idx": 19}, {"type": "text", "text": "Memory Module ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Manager Agent: ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "1. Working: - Consolidation -Refinement   \n2. Procedural: - Trading action records - Reflection records   \n3. Episodic: - Trajectory history ", "page_idx": 19}, {"type": "text", "text": "Analyst Agents: ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "1. Working: - Observation - Retrieval - Distillation 2. Procedural: - Distilled Investment-related insights - Financial sentiment - Investment report recommended actions ", "page_idx": 19}, {"type": "text", "text": "Action Module ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Manager Agent: ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "1. Conduct: Trading actions.   \n2. Reflect: Trading reasons and analyst agents\u2019 contribution assessment. ", "page_idx": 19}, {"type": "text", "text": "Figure 4: The detailed modular design of the manager and analyst agents. The general configuration and profliing modules generate text-based queries to retrieve investment-related information from the agents\u2019 memory databases. The perceptual and memory modules interact with LLMs via prompts to extract key investment insights. The action module of the manager agent consolidates these insights to facilitate informed trading decisions. ", "page_idx": 19}, {"type": "text", "text": "A.5 Experimental Setup ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Multi-Modal Datasets. We collect a comprehensive multi-modal dataset to simulate a realistic market environment. This dataset includes stock price data, daily financial news, company filing reports (10K and 10Q), and ECC (Earnings Call Conference) audio recordings spanning from January 3, 2022, to June 10, 2023. Each data source is assigned to specific analyst agents based on the timeliness of the information. For example, annual filings (10K) exhibit longer-term persistence, quarterly fliings (10Q) and ECC data have medium-term relevance, and daily financial news provides the most immediate information. ", "page_idx": 20}, {"type": "text", "text": "Evaluation Metrics. We evaluate FINCON and benchmark it against other state-of-the-art LLM-based and DRL-based agent systems using three key financial performance metrics: Cumulative Return $(\\mathbf{CR}\\%)$ [90], Sharpe Ratio (SR) [91], and Max Drawdown $(\\mathrm{MDD}\\%)$ ) [92]. These metrics help quantify each model\u2019s profitability, risk-adjusted returns, and risk management performance, respectively. ", "page_idx": 20}, {"type": "text", "text": "Comparative Methods. In the single-stock trading task, we compare FINCON against seven algorithmic agents and the widely accepted Buy-and-Hold (B & H) baseline. The three DRL-based agents\u2014A2C, PPO, and DQN\u2014are from the FinRL framework [48], while the four state-of-the-art LLM-based agents include GENERATIVE AGENT [20], FINGPT [93], FINMEM [32], and FINAGENT [33]. For portfolio management, we benchmark FINCON against the classical Markowitz MV portfolio selection strategy [1], the RL-based FinRL-A2C agent [48], and the B & H strategy, which holds an equal-weighted position across all assets (equal-weighted ETF). Our focus on classical, RL-based, and B & H methods is due to the current lack of mature LLM-based agents for portfolio management tasks. ", "page_idx": 20}, {"type": "text", "text": "Implementation Details. In our experiments, all LLM-based agent systems, including FINCON, use GPT-4-Turbo as the backbone model, with the temperature parameter set at 0.3 to balance response consistency with creative reasoning. FINCON is trained on financial data from January 3, 2022, to October 4, 2022, and tested on data from October 5, 2022, to June 10, 2023. Since deep reinforcement learning (DRL) agents require extensive data for convergence, their training period is extended to nearly five years (January 1, 2018, to October 4, 2022) to ensure fair comparison. The testing period remains the same across all models. The final performance metrics are based on the test trajectory with the median CR and SR values from five repeated epochs. If the median CR and SR occur in different epochs, performance is assessed based on the trajectory with the median CR value. ", "page_idx": 20}, {"type": "image", "img_path": "dG1HwKMYbC/tmp/ddd8aed35c76b043248dd6d213d02621045d237e6d164bfaaebd8de66b2d8b3b.jpg", "img_caption": ["A.6 Single Stock Trading Result Graphs ", "Figure 5: CRs over time for single-asset trading tasks. FINCON outperformed other comparative strategies, achieving the highest CRs across all six stocks by the end of the testing period, regardless of market conditions. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "A.7 Detailed Ablation Study ", "page_idx": 21}, {"type": "text", "text": "A.7.1 The Effectiveness of Within-Episode Risk Control mechanism via CVaR ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "To answer the RQ2, we conduct the first ablation study. We assess the efficacy of FINCON\u2019s withinepisode risk control mechanisms by monitoring system risk changes through CVaR. To demonstrate the robustness of FINCON, we compare the performance of FINCON with versus without CVaR implementation across two task types: single-asset trading and portfolio management. Furthermore, in single-asset trading tasks, we consider assets in both general bullish and bearish market conditions in the testing phase for comprehensive consideration. ", "page_idx": 22}, {"type": "text", "text": "Our results demonstrate that implementing CVaR in FINCON is highly effective across all financial metrics for both task types, as shown in Table 4 and Fig 6. For single-asset trading tasks, FINCON without within-episode risk control yields negative CRs and significantly higher MDDs, underperforming compared to the Buy-and-Hold strategy (CR of GOOG: $22.42\\%$ , CR of NIO: $-77.{\\bar{2}}10\\%$ ), highlighting the severe consequences of ignoring environmental risks. In portfolio management, the CR increases dramatically from $14.699\\%$ to $113.836\\%$ with within-episode risk control, demonstrating its effectiveness in risk supervision even amid non-uniform market trends. ", "page_idx": 22}, {"type": "image", "img_path": "dG1HwKMYbC/tmp/2b1cba874a8e6d50c958f6b9acc4f1b13b73b2f9214b85227b44df54e89f3203.jpg", "img_caption": ["(c) Multi-Assets "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Figure 6: CRs of FINCON with vs. without implementing CVaR for within-episode risk control show that the CVaR mechanism significantly improves FINCON\u2019s performance. This is evident from two metrics: (a) cumulative returns over time for single stocks in both bullish and bearish market conditions, and (b) portfolio value over time for a multi-asset portfolio. In both cases, FINCON with CVaR demonstrates substantially higher gains. ", "page_idx": 22}, {"type": "text", "text": "Specifically, the success of utilizing CVaR for within-episode risk control is evident in both bullish and bearish market environments, as shown in the single asset trading case. In bullish markets, CVaR sharply captures immediate market shocks and timely informs FINCON to exercise caution, even amidst general optimism. Conversely, in bearish markets, CVaR consistently alerts FINCON to significant price drops, ensuring awareness of market risks. Moreover, in portfolio trading with mixed price trends, our within-episode risk control mechanism performs robustly by monitoring the entire portfolio\u2019s value fluctuations, enabling the trading manager agent to adjust potentially aggressive operations for each asset promptly. ", "page_idx": 22}, {"type": "text", "text": "A.7.2 The Efficacy of Over-Episode Belief Updates Using CVRF ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In the second ablation study, to answer RQ3, we use the same assets to examine the effectiveness of FINCON\u2019s over-episode risk control mechanisms. This is achieved by consistently improving FINCON\u2019s beliefs about market conditions for the targeted assets. To ensure consistent belief output for each training episode, we set the temperature parameter to 0 specifically for belief generation. ", "page_idx": 23}, {"type": "text", "text": "We collect third-time belief updates over four training episodes using our innovative CVRF mechanism. The overlap of trading actions between the last two adjacent episodes increases to over $80\\%$ , and the updated investment beliefs are mostly aligned. To illustrate FINCON\u2019s evolving investment beliefs through iterative training, we use the GOOG investment belief update as an example, as shown in Figure 8. Compared to the initial and final belief updates, each conceptual aspect, such as historical momentum and news insights, is enriched with executable information through our CRVF mechanism, leading to more profitable actions. ", "page_idx": 23}, {"type": "text", "text": "The results in Table 5 and Figure 7 indicate that the over-episode belief update mechanism is more critical than within-episode risk control in enhancing FINCON\u2019s decision-making. Without this functionality, key metrics like CR, SR, and MDD are lower than without the within-episode risk control in single asset trading. Although the CR of $28.432\\%$ outperforms the Equal-Weighted ETF strategy\u2019s $9.3\\bar{4}4\\%$ for portfolio management, the SR of 1.181 is higher than Equal-Weighted ETF strategy\u2019s 0.492, with the belief update feature, performance significantly further improves. It can achieve a CR of $113.836\\%$ and an SR of 3.269. These results demonstrate that using CVRF to update investment beliefs over episodes efficiently steers the agent\u2019s investment beliefs towards more proftiable directions. FINCON achieves superior performance on multiple tasks, with learning gains evident after just four training episodes, requiring far fewer episodes than traditional RL algorithmic trading agents. ", "page_idx": 23}, {"type": "image", "img_path": "dG1HwKMYbC/tmp/6ef0f1715e018fd834c39d853d41d9a6f4dfbaf4b5233d2fc7c34fcb7d263da3.jpg", "img_caption": ["(c) Multi-Assets "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Figure 7: CRs of FINCON with vs. without belief updates for over-episode risk control. (a) The CRs over time for single stocks. The performance of FINCON with belief updates consistently leads in both bullish and bearish market conditions. (b) The portfolio values over time for multi-asset portfolio. FINCON\u2019s performance with belief updates also won a substantially higher gains. ", "page_idx": 23}, {"type": "text", "text": "Updated Investment Belief Contents ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "First Update: ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "{\u2019historical momentum\u2019: \u2019Enhance the use of momentum indicators by establishing systematic rules for entry and exit based on momentum values to improve consistency and predictability in trading actions.\u2019, \u2019news insights\u2019: \u2019Integrate advanced real-time news sentiment analysis to better understand immediate market reactions and adjust trading strategies accordingly...\u2019, ", "page_idx": 24}, {"type": "text", "text": "\u2019Form 10-Q\u2019: \u2019Incorporating annual report insights (10-K) could provide a comprehensive view of the company\u2019s long-term performance and strategic direction, aiding in more informed decision-making.\u2019..., \u2019other aspects\u2019: [\u2019sector trends\u2019, \u2019technological advancements\u2019], ...} ", "page_idx": 24}, {"type": "text", "text": "Last Update: ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "{\u2019historical momentum\u2019: \u2019The more proftiable strategy effectively utilizes negative momentum values to make timely sell decisions, avoiding potential losses during downward trends, and effectively utilizes positive momentum values to make timely buy decisions, facilitating potential gains during upward trends. It is suggested to refine the use of momentum indicators, possibly by adjusting thresholds or incorporating additional short-term momentum metrics to enhance the timing and accuracy of buy/sell decisions.\u2019, ", "page_idx": 24}, {"type": "text", "text": "\u2019news insights\u2019: \u2019It is recommended to further develop a systematic approach to quantify the impact of news, especially the sentiment scores and regulatory changes, to refine trading decisions.\u2019, ", "page_idx": 24}, {"type": "text", "text": "\u2019Form 10-Q\u2019: \u2019It is suggested that even if there is a strong financial performance present 10-Q reports, it is better to prioritize immediate market signals. For future strategies, it is suggested to balance these aspects more evenly, especially in stable or bullish market conditions, to avoid premature exits from potentially profitable positions.\u2019,..., ", "page_idx": 24}, {"type": "text", "text": "\u2019other aspects\u2019: [\u2019sector trends\u2019, \u2019technological advancements\u2019, \u2019macroeconomic factors\u2019, \u2019regulatory risks\u2019], ...} ", "page_idx": 24}, {"type": "text", "text": "Tradng Action Overlapping Percentages ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "1. Second vs. First Training Episode: $46.939\\%$   \n2. Third vs. Second Training Episode: $71.429\\%$   \n3. Fourth vs. Third Training Episode: $81.633\\%$ ", "page_idx": 24}, {"type": "text", "text": "Figure 8: The first time and last time LLM generated investment belief updates by CVRF for GOOG. ", "page_idx": 24}, {"type": "text", "text": "A.8 Raw Data Sources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We assessed the performance of FINCON using multi-modal financial data from January 3, 2022, to June 10, 2022, sourced from reputable databases and APIs including Yahoo Finance (via yfinance), Alpaca News API, and Capital IQ, detailed explained in Table. These data, initially stored in the Raw Financial Data Warehouse as available observations of the financial market environment, are diverged into the corresponding FINCON\u2019s Analysts\u2019 Procedural Memory Databases based on timeliness through working memory\u2019s summarization operation. ", "page_idx": 25}, {"type": "table", "img_path": "dG1HwKMYbC/tmp/394043000790bfa81f6f1ad34c08b4343d5fdd199a2d2e1950c241caae09113e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "A.9 Distribution of Data ", "text_level": 1, "page_idx": 25}, {"type": "image", "img_path": "dG1HwKMYbC/tmp/a870482b716ba443680bd09c43908d2fed703c3b2de921d02c40a36d70cd4790.jpg", "img_caption": ["Table 6: Raw data and memory warehouses of FINCON ", "Figure 9: The distribution of news from REFINITIV REAL-TIME NEWS for the 42 stocks in the experiments "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "dG1HwKMYbC/tmp/75fecf4055c85db81728c0799bef4c38b74acdf83e2647610e655069763052ac.jpg", "img_caption": ["Figure 10: The distribution of 10k10q from Securities and Exchange Commission (SEC) for the 42 stocks in the experiments "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "dG1HwKMYbC/tmp/7910c5fe8af66dd4b67529ed4f73936a032d8690f68fa7aa890bf0c44f9e4565.jpg", "img_caption": ["Figure 11: The distribution of Analyst Report from Zacks Equity Research for the 42 stocks in the experiments "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "A.10 Formulas of Classic Financial Metrics for Risk Estimator and Decision-making Task Performance Evaluation ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "The risk estimator uses the following metrics: ", "page_idx": 26}, {"type": "text", "text": "Profit and Loss (PnL)[97]: PnL quantifies the net outcome of trading activities over a specified period by accounting for the realized gains and losses from financial instruments like stocks and derivatives. ", "page_idx": 26}, {"type": "text", "text": "Value at Risk $(\\mathbf{VaR})$ of PnL[97]: VaR is a statistical tool used to estimate the potential loss in a portfolio, within a defined confidence interval. Mathematically, it is defined as Equation 3: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname{VaR}_{\\alpha}(P n L)=\\operatorname*{inf}\\left\\{l\\in\\mathbb{R}:\\mathbb{P}(P n L\\leq l)\\geq\\alpha\\right\\}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $\\alpha$ is the confidence level. ", "page_idx": 26}, {"type": "text", "text": "Conditional Value at Risk (CVaR) of PnL[97]: CVaR is a statistical tool used to estimate the expected potential loss worse than the VaR value in a portfolio, within a defined confidence interval. Mathematically, it is defined as Equation 4: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathrm{CVaR}_{\\alpha}(P n L)=\\mathbb{E}\\Big\\{P n L|P n L\\leq\\mathrm{VaR}_{\\alpha}(P n L)\\Big\\}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $\\alpha$ is the confidence level. ", "page_idx": 27}, {"type": "text", "text": "The performance evaluation of algorithmic trading agents incorporates the following metrics: ", "page_idx": 27}, {"type": "text", "text": "Cumulative Return of PnL [90]: Cumulative Return is a key trading performance metric because it provides a comprehensive insight into investment performance, especially for strategies that emphasize long-term growth and reinvestment. The effectiveness of different investment strategies is evaluated based on their Cumulative Returns, which reflect the total change in value over time. In this study, we compute Cumulative Returns over the specified period by summing daily logarithmic returns, as outlined in Equation 5. This method is widely accepted in the finance area due to its ability to precisely capture minor price fluctuations and symmetrically address gains and losses. In essence, a higher Cumulative Return typically indicates a more effective strategy. ", "page_idx": 27}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}{\\mathbf{{Cumulative}\\;R e t u r n}=\\displaystyle\\sum_{t=1}^{n}{r_{i}}\\,}&{}\\\\ &{=\\displaystyle\\sum_{t=1}^{n}\\left[\\ln\\left({\\frac{p_{t+1}}{p_{t}}}\\right)\\cdot\\mathrm{{action}}_{t}\\right],}\\end{array}}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $r_{i}$ represents the $\\mathrm{PnL}$ for day $t+1$ , $p_{t}$ is the closing price on day $t_{\\cdot}$ , $p_{t+1}$ is the closing price on day $t+1$ , and action $\\cdot t$ denotes the trading decision made by the model for that day. ", "page_idx": 27}, {"type": "text", "text": "Portfolio Value: Portfolio value represents the total worth of all the investments held in a portfolio at a given point in time. It is a metric used only in the portfolio management task. ", "page_idx": 27}, {"type": "equation", "text": "$$\n{\\bf C u m u l a t i v e~S i m p l e~R e t u r n}_{t}=\\prod_{k=1}^{t}(1+{\\bf D a i l y}\\;{\\bf S i m p l e}\\;{\\bf R e t u r n}_{t})-1\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Portfolio $\\mathbf{Value}_{t}=$ Initial Investment Amount $\\mathbf{\\nabla}\\times\\left(\\mathbf{1}+\\mathbf{C}\\mathbf{\\Psi}\\right)$ umulative Simple Returnt) ", "page_idx": 27}, {"type": "text", "text": "where the initial amount is set as $\\Phi1,000,000$ . ", "page_idx": 27}, {"type": "text", "text": "Sharpe Ratio of PnL [91]: Sharpe Ratio is another core metric for evaluating investment performance and adjusting returns for risk. It is calculated by dividing the portfolio\u2019s average PnL $(R_{p})$ over the risk-free rate $(R_{f})$ by its volatility $(\\sigma_{p})$ , as shown in Equation 8. This metric adjusts returns for risk, with a higher ratio indicating better risk-adjusted performance. Essential in comparing different portfolios or strategies, it contextualizes performance against similar investments. Although a Sharpe Ratio above 1 is typically considered favorable and above 2 as excellent, these benchmarks can vary depending on the context of comparison. ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbf{Sharpe\\;Ratio}=\\frac{R_{p}-R_{f}}{\\sigma_{p}}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Max Drawdown of PnL [92]: Max Drawdown is a metric for assessing risk. It represents the most significant decrease in a portfolio\u2019s value, from its highest $(P_{\\mathrm{peak}})$ to its lowest point $(P_{\\mathrm{trough}})$ until a new peak emerges, detailed in Equation 9. Indicative of investment strategy robustness, a smaller Max Drawdown suggests reduced risk. ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbf{Max}\\,\\mathbf{Drawdown}=\\operatorname*{max}(\\frac{P_{\\mathrm{peak}}-P_{\\mathrm{trough}}}{P_{\\mathrm{peak}}})\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "A.11 Baseline and Comparative Models on Single Stock Trading Task ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Buy-and-Hold strategy (B&H): ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "A passive investment approach, where an investor purchases stocks and holds onto them for an extended period regardless of market fluctuations, is commonly used as a baseline for comparison of stock trading strategies. ", "page_idx": 27}, {"type": "text", "text": "LLM trading agents: ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We evaluate FINCON against four LLM agents in the context of stock trading. ", "page_idx": 28}, {"type": "text", "text": "\u2022 GENERAL-PURPOSE GENERATIVE AGENTS \u2013 GA:The generative AI agent by Park et al. [20], originally intended to simulate realistic human behavior and make everyday decisions, has been adapted here for specific stock trading tasks. This agent\u2019s architecture includes a memory module that employs recency, relevance, and importance metrics to extract pivotal memory events for informed decision-making. However, it does not provide a layered memory module to effectively differentiate the time sensitivities unique to various types of financial data. Additionally, although it features a profliing module to define agent attributes like professional background, the model does not specify the agent\u2019s persona. In our experiments, we modified the original prompt template created by Park et al., which was intended for general daily tasks, to suit financial investment tasks. \u2022 FINGPT: A novel open-source LLM framework specialized for converting incoming textual and numeric information into informed financial decision-making, introduced by Yang et al[31]. It claims superiority over the traditional buy-and-hold strategy. \u2022 FINMEM:FINMEM employs a specialized profliing module and self-adaptive risk settings for enhanced market robustness. Its memory module integrates working memory and layered long-term memory, enabling effective data processing. This allows FINMEM to leverage market insights and improve trading decisions [32]. \u2022 FINAGENT:FINAGENT developed upon FINMEM, which leverages the use of tool-using capabilities of LLMs to incorporate multi-modal financial data [33]. It claims an further improved trading performance on single asset trading (stocks and cryptocurrencies). ", "page_idx": 28}, {"type": "text", "text": "DRL trading agents: ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "As the FINMEM is practiced and examined on the basis of single stock trading and discrete trading actions, we choose three advanced DRL algorithms fitting into the same scenarios according to the previous and shown expressive performance in the work of Liu et al [98, 99]. The DRL training agents only take numeric features as inputs. ", "page_idx": 28}, {"type": "text", "text": "\u2022 Advantage Actor-Critic (A2C): A2C ([100]) is applied to optimize trading actions in the financial environment. It operates by simultaneously updating both the policy (actor) and the value (critic) functions, providing a balance between exploration and exploitation. \u2022 Proximal Policy Optimization (PPO): PPO ([101]) is employed in stock trading due to its stability and efficiency. One salient advantage of PPO is that it maintains a balance between exploration and exploitation by bounding the policy update, preventing drastic policy changes. \u2022 Deep Q-Network (DQN): DQN ([102]) is an adaptation of Q-learning, that can be used to optimize investment strategies. Unlike traditional Q-learning that relies on a tabular approach for storing Q-values, DQN generalizes Q-value estimation across states using deep learning, making it more scalable for complex trading environments. ", "page_idx": 28}, {"type": "text", "text": "A.12 Portfolio Management ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Markowitz Portfolio Selection [1]: introduced by Harry Markowitz in 1952, is a framework for constructing portfolios that optimize expected return for a given level of risk or minimize risk for a given level of expected return. This method uses expected returns, variances, and covariances of asset returns to determine the optimal asset allocation, thereby balancing risk and return through diversification. ", "page_idx": 28}, {"type": "text", "text": "FinRL-A2C [48]: is an RL algorithm proposed to address single stock trading and portfolio optimization problems in Liu et al.. The RL models make trading decisions (i.e., portfolio weights) based on the observation of previous market conditions and the brokerage information of the RL agents. The implementation of this algorithm 2is provided and is used as baselines in our study. ", "page_idx": 28}, {"type": "text", "text": "Equal-Weighted ETF [103]: is a portfolio giving equal allocation to all stocks, similar to a buy-andhold strategy in single-stock trading, can provide a benchmark on market trends. ", "page_idx": 28}, {"type": "text", "text": "A.13 Ranking Metrics for Procedural Memory in FINCON ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Upon receiving an investment inquiry, each agent in FINCON retrieves the top- $.K$ pivotal memory events from its procedural memory, where $K$ is a hyperparameter. These events are selected based on their information retrieval score. For any given memory event $E$ , its information retrieval score $\\gamma^{E}$ is defined by ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\gamma^{E}=S_{\\mathrm{Relevancy}}^{E}+S_{\\mathrm{Importance}}^{E}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "which is adpated from Park et al [20] but with modified relevancy and importance computations, and is scaled to $[0,1]$ before summing up. Upon the arrival of a trade inquiry $P$ in processing memory event $E$ via LLM prompts, the agent computes the relevancy score $S_{\\mathrm{Relevancy}}^{E}$ that measures the cosine similarity between the embedding vectors of the memory event textual content $\\mathbf{m_{E}}$ and the LLM prompt query $\\mathbf{m_{P}}$ , which is defined as follows: ", "page_idx": 29}, {"type": "equation", "text": "$$\nS_{\\mathrm{Relevancy}}^{E}={\\frac{\\mathbf{m}_{\\mathbf{E}}\\cdot\\mathbf{m}_{\\mathbf{P}}}{\\|\\mathbf{m}_{\\mathbf{E}}\\|_{2}\\times\\|\\mathbf{m}_{\\mathbf{P}}\\|_{2}}}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Note that the LLM prompt query inputs trading inquiry and trader characteristics. On the other hand, the importance score $S_{\\mathrm{Importance}}^{\\dot{E}}$ is inversely correlates with the time gap between the inquiry and the event\u2019s memory timestamp $\\delta t=t_{\\mathrm{P}}-t_{E}$ , mirroring Ebbinghaus\u2019s forgetting curve [104]. More precisely, if we denote the initial score value of memory event $v^{E}$ and degrading ratio $\\theta\\in(0,1)$ , then the importance score is computed via ", "page_idx": 29}, {"type": "equation", "text": "$$\nS_{\\mathrm{Importance}}^{E}=v^{E}\\times\\theta^{\\delta t}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Note that the ratio $\\theta$ measures the diminishing importance of an event over time, which is inspired by design of [20]. But in our design, the factors of recency and importance are handled by one equation. Different agents in FINCON admit different choices of $\\{v^{E},\\theta\\}$ for memory event $E$ . ", "page_idx": 29}, {"type": "text", "text": "Additionally, an access counter function facilitates memory event augmentation, so that critical events impacting trading decisions can be augmented by FINCON, while trivial events are gradually faded. This is achieved by using the LLM validation tool Guardrails AI [105] to track critical memory ID. A memory ID deemed critical to investment gains receives $+5$ to its importance score SImportance. This access counter implementation enables FINCON to capture and prioritize crucial events based on type and retrieval frequency. ", "page_idx": 29}, {"type": "text", "text": "A.14 Detailed Configurations in Experiments ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "The training period was chosen to account for the seasonal nature of corporate financial reporting and the duration of data retention in FINCON\u2019s memory module. The selected training duration ensures the inclusion of at least one publication cycle of either Form 10-Q, ECC, or Form 10-K. This strategy ensures that the learned conceptualized investment guidance considers a more comprehensive scope of factors. Additionally, the training duration allowed FINCON sufficient time to establish inferential links between financial news, market indicators, and stock market trends, thereby accumulating substantial experience. Furthermore, we set the number of top memory events retrieved for each agent at 5. We ran FINCON. The reported performance outcomes are based on the setting that achieved the highest cumulative return during the testing phase. ", "page_idx": 29}, {"type": "text", "text": "To maintain consistency in the comparison, the training and testing phases for the other three LLMbased agents were aligned with those of FINMEM. For parameters of other LLM-based agents that are not encompassed by FINMEM\u2019s configuration, they were kept in accordance with their original settings as specified in their respective source codes. ", "page_idx": 29}, {"type": "text", "text": "FINCON\u2019s performance was benchmarked against that of the most effective comparative model, using Cumulative Return and Sharpe Ratio as the primary evaluation metrics. The statistical significance of FINCON\u2019s superior performance was ascertained through the non-parametric Wilcoxon signed-rank test, which is particularly apt for the non-Gaussian distributed data. ", "page_idx": 29}, {"type": "text", "text": "A.15 FINCON performance on extreme market conditions ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "To further illustrate the robustness of FINCON\u2019s performance, we assess its effectiveness in two distinct scenarios: (1) a single-asset trading task using TSLA and (2) a portfolio management task involving a combination of TSLA, MSFT, and PFE. Our evaluation focuses on key financial metrics, including Cumulative Returns (CRs), Sharpe Ratios (SRs), and Maximum Drawdown (MDD). The training period spanned from January 17, 2022, to March 31, 2022, while the testing phase covered April 1, 2022, to October 15, 2022. This specific timeframe was chosen due to the elevated levels of the CBOE Volatility Index (VIX), which averaged above 20, signaling greater market volatility during these months. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "As demonstrated in Table 7 and Figure 12, FINCON is the sole agent system to achieve positive Cumulative Returns (CRs) and Sharpe Ratios (SRs) in single stock trading tasks. Regarding portfolio management tasks, the results of all baselines (four benchmarks) are detailed in Table 8 and Figure 13. In these comparisons, FINCON consistently attained the highest scores in the primary performance metrics. ", "page_idx": 30}, {"type": "table", "img_path": "dG1HwKMYbC/tmp/8165b73be6b5b76d826d8f19af88b674429c6fa7a7327168e16711f2ea77dc6e.jpg", "table_caption": [], "table_footnote": ["Table 7: Key performance comparison for single asset trading under the high volatility condition using TSLA as an example. FINCON leads all performance metrics. "], "page_idx": 30}, {"type": "image", "img_path": "dG1HwKMYbC/tmp/9c6eec6beabac1fee70f3ae4afd411336992d605dc572b9018d654382d459355.jpg", "img_caption": ["Figure 12: CR changes over time across all the strategies under the high volatility condition using TSLA as an example of the single asset trading task. "], "img_footnote": [], "page_idx": 30}, {"type": "table", "img_path": "dG1HwKMYbC/tmp/ef30ab13ae15105f9891247f943cfe43da4687b912e687942561a9d06c780bf3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "Table 8: Key performance comparison among all portfolio management strategies of Portfolio1 under the high volatility condition. FINCON leads all performance metrics. ", "page_idx": 30}, {"type": "image", "img_path": "dG1HwKMYbC/tmp/17614b71ba5a6b05b72dc814914b6ddccf2244c3a34ef33d4530e708fe3defcb.jpg", "img_caption": ["Figure 13: Portfolio1 value changes over time for all the strategies under the high volatility condition. "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 31}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 31}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 31}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 31}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 31}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 31}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 31}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 31}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: See in section 5 ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 32}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: See in Algorithm 1. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 32}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: We provide detailed experiment results. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 32}, {"type": "text", "text": "\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 33}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: Yes ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 33}, {"type": "text", "text": "\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 34}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: See in experiment section. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 34}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [No] ", "page_idx": 34}, {"type": "text", "text": "Justification: See in ablation study. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 34}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of computing workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: See in Section 4 and Appendix. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. ", "page_idx": 34}, {"type": "text", "text": "\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 35}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: We complied with Code Of Ethics. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 35}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: See in section 5 Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 35}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: Described. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 35}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 36}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: Described. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 36}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Justification: Described. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 36}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] Justification: Described. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 37}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: Described. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 37}]