[{"figure_path": "PEEqnXlSCk/figures/figures_1_1.jpg", "caption": "Figure 1: Training validation loss for GPT-6.7B; SDP4Bit is closely aligned with full precision training.", "description": "This figure compares the validation loss during training for a GPT-6.7B language model using three different methods: a baseline (full precision), SDP4Bit (the proposed method), and ZeRO++.  The plot shows that SDP4Bit closely tracks the baseline's performance, indicating that the proposed communication quantization technique does not significantly impact training accuracy.  In contrast, ZeRO++ exhibits a noticeable deviation from the baseline, suggesting a potential accuracy compromise.", "section": "1 Introduction"}, {"figure_path": "PEEqnXlSCk/figures/figures_2_1.jpg", "caption": "Figure 1: Training validation loss for GPT-6.7B; SDP4Bit is closely aligned with full precision training.", "description": "This figure shows the validation loss curves during the training of a 6.7 billion parameter GPT model.  The curves compare the performance of three different training methods: a baseline using full precision,  the ZeRO++ method, and the SDP4Bit method proposed in the paper. The graph demonstrates that SDP4Bit achieves a validation loss curve very close to the full precision baseline, indicating comparable accuracy. The ZeRO++ method shows a higher validation loss. ", "section": "1 Introduction"}, {"figure_path": "PEEqnXlSCk/figures/figures_3_1.jpg", "caption": "Figure 3: Two-level gradient quantization: 8-bit intra-node and 4-bit inter-node quantization.", "description": "This figure illustrates the two-level gradient quantization technique used in SDP4Bit.  It shows how gradients are first quantized to 8 bits within each node (intra-node) using an all-to-all communication. Then, the reduced data is further quantized to 4 bits for communication between nodes (inter-node) via another all-to-all operation. This two-level approach balances accuracy and communication efficiency. The figure uses a visual representation to show the process across multiple nodes and GPUs.", "section": "3 Methodology"}, {"figure_path": "PEEqnXlSCk/figures/figures_3_2.jpg", "caption": "Figure 4: Histogram of (a) weights and (b) weight differences. Each vertical dashed line represents a quantization level corresponding to a 4-bit quantization lattice.", "description": "This figure shows two histograms. The left one displays the distribution of weights, illustrating a wide range of values. The right one shows the distribution of weight differences (between consecutive iterations), demonstrating a narrower and more uniform distribution centered around zero. The vertical dashed lines in both histograms represent the quantization levels for a 4-bit quantization scheme. This visualization supports the paper's claim that quantizing weight differences leads to smaller quantization errors and better performance compared to directly quantizing the weights.", "section": "3 Methodology"}, {"figure_path": "PEEqnXlSCk/figures/figures_7_1.jpg", "caption": "Figure 5: Validation loss comparison for the Baseline, ULq, TLq, and TLq-HS on the GPT-125M model. Uniformly applying 4-bit gradient quantization twice results in a noticeable gap compared to the baseline. In contrast, two-level quantization (8-bit for intra-node and 4-bit for inter-node) mitigates this gap. The Hadamard smoother further reduces the gap, making the loss nearly identical to the baseline.", "description": "This figure compares the validation loss curves of four different methods for training a GPT-125M model: Baseline (full precision), ULq (uniform level quantization), TLq (two-level quantization), and TLq-HS (two-level quantization with Hadamard smoother).  It demonstrates that directly applying 4-bit quantization to gradients twice (ULq) results in significantly higher validation loss compared to the baseline.  The two-level approach (TLq) reduces this gap by quantizing intra-node gradients to 8 bits and inter-node gradients to 4 bits. Finally, the addition of the Hadamard smoother (TLq-HS) further improves the result, achieving validation loss almost identical to the baseline.", "section": "5.2 Accuracy Evaluation"}, {"figure_path": "PEEqnXlSCk/figures/figures_7_2.jpg", "caption": "Figure 6: Comparison of gradient histograms before and after the Hadamard transformation. The transformation reduces the impact of outliers, resulting in a smoother gradient distribution.", "description": "This figure shows two histograms visualizing the distribution of gradients before and after applying the Hadamard transform. The histogram on top represents the gradient distribution before transformation, exhibiting a sharp peak and heavier tails, indicating the presence of outliers.  The bottom histogram shows a much smoother, more Gaussian-like distribution after the Hadamard transform, illustrating how this technique successfully mitigates the effect of outliers by spreading their influence across other gradient components.  This results in a more stable and less noisy gradient signal that is potentially less susceptible to quantization errors.", "section": "3.2 Two-Level Gradient Smooth Quantization"}, {"figure_path": "PEEqnXlSCk/figures/figures_8_1.jpg", "caption": "Figure 7: Scalability of SDP4Bit.", "description": "This figure demonstrates the scalability of SDP4Bit using GPT models of 6.7B and 13B parameters, with tests conducted on up to 128 GPUs.  The left two plots show results on a cluster with slower inter-node bandwidth (Slingshot10), while the right two use a high-bandwidth InfiniBand network.  The bars show the achieved throughput (TFLOPS per GPU) for baseline Megatron-LM and SDP4Bit at varying GPU counts.  SDP4Bit demonstrates consistent speedups across different GPU counts and network conditions.", "section": "5.3 Throughput Evaluation"}, {"figure_path": "PEEqnXlSCk/figures/figures_8_2.jpg", "caption": "Figure 8: Throughput breakdown of SDP4Bit on GPT-2.7B.", "description": "This figure shows a throughput breakdown of the SDP4Bit model on two different hardware platforms: 32 A100 GPUs with Slingshot10 network and 128 H800 GPUs with InfiniBand network.  For each platform, it compares the baseline performance with three variations of the SDP4Bit algorithm: \n- qWD (Quantization on Weight Differences):  shows the impact of only quantizing weight differences on throughput.\n- TLq-HS (Two-Level Gradient Smooth Quantization with Hadamard Smoother): shows the effect of using the two-level gradient quantization and Hadamard smoother.\n- SDP4Bit (the full algorithm): combines both qWD and TLq-HS. The figure displays the E2E throughput in TFLOPS per GPU and the communication time spent on weights and gradients in milliseconds (ms). This allows visualization of how each component contributes to the overall throughput improvement of SDP4Bit across different hardware and network conditions.", "section": "5.3 Throughput Evaluation"}, {"figure_path": "PEEqnXlSCk/figures/figures_17_1.jpg", "caption": "Figure 9: Comparison of validation loss versus wall-clock time for Baseline, ZeRO++, and SDP4Bit on the GPT-6.7B model.", "description": "This figure compares the training progress (validation loss) over time (wall-clock hours) for three different methods: the Baseline (full-precision training), SDP4Bit (the proposed method), and ZeRO++.  It demonstrates that SDP4Bit achieves comparable accuracy to the baseline while being significantly faster than ZeRO++.  The graph shows the validation loss decreasing over time for all three methods, with SDP4Bit closely tracking the baseline's performance.", "section": "5.2 Accuracy Evaluation"}]