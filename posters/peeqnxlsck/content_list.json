[{"type": "text", "text": "SDP4Bit: Toward 4-bit Communication Quantization in Sharded Data Parallelism for LLM Training ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jinda Jia\u2217 Indiana University jindjia@iu.edu ", "page_idx": 0}, {"type": "text", "text": "Cong Xie\u2217 ByteDance Inc. cong.xie@bytedance.com ", "page_idx": 0}, {"type": "text", "text": "Hanlin Lu ByteDance Inc. hanlin.lu@bytedance.com ", "page_idx": 0}, {"type": "text", "text": "Daoce Wang Indiana University daocwang@iu.edu ", "page_idx": 0}, {"type": "text", "text": "Hao Feng Indiana University haofeng@iu.edu ", "page_idx": 0}, {"type": "text", "text": "Chengming Zhang University of Houston czhang59@cougarnet.uh.edu ", "page_idx": 0}, {"type": "text", "text": "Baixi Sun Haibin Lin Zhi Zhang Indiana University ByteDance Inc. ByteDance Inc. sunbaix@iu.edu haibin.lin@bytedance.com zhangzhi.joshua@bytedance.com ", "page_idx": 0}, {"type": "text", "text": "Xin Liu ByteDance Inc. liuxin.ai@bytedance.com ", "page_idx": 0}, {"type": "text", "text": "Dingwen Tao University of Chinese Academy of Sciences taodingwen@ict.ac.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recent years have witnessed a clear trend towards language models with an everincreasing number of parameters, as well as the growing training overhead and memory usage. Distributed training, particularly through Sharded Data Parallelism (ShardedDP) which partitions optimizer states among workers, has emerged as a crucial technique to mitigate training time and memory usage. Yet, a major challenge in the scalability of ShardedDP is the intensive communication of weights and gradients. While compression techniques can alleviate this issue, they often result in worse accuracy. Driven by this limitation, we propose SDP4Bit (Toward 4Bit Communication Quantization in Sharded Data Parallelism for LLM Training), which effectively reduces the communication of weights and gradients to nearly 4 bits via two novel techniques: quantization on weight differences, and twolevel gradient smooth quantization. Furthermore, SDP4Bit presents an algorithmsystem co-design with runtime optimization to minimize the computation overhead of compression. In addition to the theoretical guarantees of convergence, we empirically evaluate the accuracy of SDP4Bit on the pre-training of GPT models with up to 6.7 billion parameters, and the results demonstrate a negligible impact on training loss. Furthermore, speed experiments show that SDP4Bit achieves up to $4.08\\times$ speedup in end-to-end throughput on a scale of 128 GPUs. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large Language Models (LLMs) are increasingly utilized across various applications, leading to a trend toward larger model sizes. This expansion in model size significantly escalates training overheads, making the process more costly and resource-intensive. To mitigate the time-consuming nature of training LLMs, it is common to employ multiple GPUs in a data-parallel configuration. However, naive Data Parallelism (DP) necessitates that each GPU replicates the entire optimizer states, a strategy often impractical due to the limited memory capacity of individual GPUs. This limitation becomes particularly critical with the substantial size of modern LLMs. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Sharded Data Parallelism (ShardedDP) evolves from naive DP to reduce the memory footprint by sharding optimizer states among GPUs. However, the sharding mechanism significantly changes the communication pattern of DP, which brings up new challenges in system optimization. As a result, ShardedDP suffers from heavy communication overheads of both weights and gradients, particularly when internode bandwidth is limited. This can significantly increase the end-to-end (E2E) training time, especially when using a small gradient accumulation step. ", "page_idx": 1}, {"type": "text", "text": "Quantization is a widely used strategy to reduce the communication overhead of naive DP, albeit with some accuracy loss. Unfortunately, few prior studies have specifically addressed the is", "page_idx": 1}, {"type": "image", "img_path": "PEEqnXlSCk/tmp/18a4e152df7a559c72b110c40b77424acb51c076c0e2b5bfa5fc97d7410f3e79.jpg", "img_caption": ["Figure 1: Training validation loss for GPT-6.7B; SDP4Bit is closely aligned with full precision training. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "sue of communication reduction in ShardedDP. Recently, QSDP [18] and $Z\\mathrm{eRO++}$ [32] attempted to quantize the communication of ShardedDP to Int4. However, when pushing the communication ratio to its limits, both QSDP and $Z\\mathrm{eRO++}$ fail to maintain comparable training loss to the baseline. Furthermore, $Z\\mathrm{eRO++}$ lacks theoretical convergence guarantees, and QSDP is limited to one specific quantizer called \u201crandom shift\u201d and strong assumptions. Thus, there is no effective solution to reduce ShardedDP\u2019s communication to nearly 4 bits without compromising the training loss. ", "page_idx": 1}, {"type": "text", "text": "To address these issues, this paper proposes a novel communication reduction strategy, SDP4Bit. SDP4Bit comprises two main techniques: (1) Quantization on Weight Differences: Instead of directly quantizing weights, we apply 4-bit quantization to compress the weight differences between current and previous iterations; (2) Two-Level Gradient Smooth Quantization: We apply 8-bit quantization to intra-node gradients and 4-bit quantization to inter-node gradients, with Hadamard Transform for smoothing the outliers. To the best of our knowledge, SDP4Bit is the first work to successfully reduce both gradients and weights to nearly 4 bits without compromising training accuracy. As shown in Figure 1, the training validation loss for GPT-6.7B using SDP4Bit is closely aligned with full precision training. Our main contributions are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose a low-bit (i.e., nearly 4-bit) communication reduction strategy for ShardedDP that preserves E2E training accuracy.   \n\u2022 We establish a convergence guarantee for the proposed strategy, showing the same convergence rate as the ordinary Stochastic Gradient Descent (SGD), with extended choices of biased compressors and weaker assumptions compared to the previous theoretical results.   \n\u2022 We implement our method within the Megatron-LM framework and enhance it with runtime optimizations such as buffer reuse, operation pruning, and kernel fusion.   \n\u2022 Our results validate that SDP4Bit successfully compresses the communication of weights and gradients to nearly 4 bits, with a negligible impact on final loss. Notably, compared to non-quantized baseline, it achieves $4.08\\times$ speedup for a GPT-18B model trained on 128 H800 GPUs. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Sharded Data Parallelism ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Sharded Data Parallelism (ShardedDP) modifies traditional Data Parallelism (DP) to reduce the memory footprint per GPU. Unlike traditional DP, which duplicates high-precision optimizer states (typically including model weights and momentum variables in Float32) on each GPU, ShardedDP partitions them across all GPUs. Each GPU manages $\\textstyle{\\frac{1}{P}}$ of the optimizer states, hence reducing the corresponding memory footprint by a factor of $\\textstyle{\\frac{1}{P}}$ , where $P$ represents the number of GPUs involved. ", "page_idx": 1}, {"type": "image", "img_path": "PEEqnXlSCk/tmp/99f3b0fd53f276ffaaa3a0cb9f91346201f281483b607c00c12324e174102e31.jpg", "img_caption": [], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "With high-precision model weights sharded across GPUs (referred to as \u201cmain weights\u201d), an allgather operation is required to collect the weights for the forward-backward steps (typically in relatively low precision, such as Float16, referred to as \u201cmodel weights\u201d). For gradient synchronization, a reduce-scatter operation is performed before the optimization steps to ensure that each GPU has the corresponding shard of averaged gradients. In summary, each iteration necessitates an all-gather for weights and a reduce-scatter for gradients. ", "page_idx": 2}, {"type": "text", "text": "Driven by the need to train larger models within the constraints of GPU memory, ShardedDP is incorporated into several popular training frameworks, including Megatron-LM (Distributed Optimizer), DeepSpeed (ZeRO), and PyTorch (FSDP), each with slightly different implementation strategies. Notably, ZeRO-3 and FSDP release the collected weights after each computation to enhance memory efficiency, necessitating additional weight collective communication during the backward pass. Conversely, ZeRO-2 and Megatron-LM retain the collected weights throughout, thus eliminating the need for weight collection during the backward pass. Our weight reduction strategy is particularly well-suited for Megatron-LM, as it maintains a full model\u2019s weights at all times (see Algorithm 2). Additionally, Megatron-LM provides flexible parallelism support, such as tensor parallelism, which partitions models vertically to alleviate memory limitations. This approach enables the training of larger models compared to DeepSpeed. ", "page_idx": 2}, {"type": "text", "text": "2.2 Quantization ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Quantization is a commonly used strategy in data compression. In this paper, we explore symmetric linear (integer) quantization due to its low overhead and latency. It is defined as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\nx_{\\mathrm{int}}=\\mathrm{round}\\left({\\frac{x}{s}}\\cdot(2^{k-1}-1)\\right),\\quad s=\\mathrm{max}(x),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $k$ represents the bit-width of the quantized values, and $s$ is referred to as \"scales\". ", "page_idx": 2}, {"type": "text", "text": "Additionally, group-wise quantization [25] is employed to minimize quantization error by dividing the data into multiple groups and quantizing each group individually. This approach results in a lower compression ratio due to the need to store additional scales. ", "page_idx": 2}, {"type": "text", "text": "2.3 Collective Reduction Communication with Quantization ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "State-of-the-art (SOTA) collective communication libraries (e.g., NCCL, Gloo) employ a ring-based algorithm for its optimal bandwidth [21]. This algorithm executes reduce-scatter operations across $P-1$ rounds, during which each GPU sends local data and aggregates the received data. When quantization is applied, this necessitates $P-1$ rounds of quantization and dequantization, potentially leading to error propagation and increased latency [11]. Some strategies replace reduce-scatter with all-to-all communication, but this increases inter-node communication, typically with lower bandwidth. ", "page_idx": 2}, {"type": "text", "text": "$Z\\mathrm{eRO++}$ [32] modifies this approach by substituting the conventional reduce-scatter (used by QSDP) with two all-to-all operations (shown in Algorithm 1, with different colors to distinguish $Z\\mathrm{eRO++}$ from QSDP). The first operation is confined within each node, and post-reduction, the data size is diminished to $\\textstyle{\\frac{1}{N}}$ , where $N$ is the number of GPUs per node. The subsequent all-to-all operation occurs between GPUs across different nodes that share the same local rank. In $Z\\mathrm{eRO++}$ , each all-to-all operation follows a 4-bit quantization step to minimize communication data size. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "As shown in Figure 5, while this method efficiently integrates quantization into reduce-scatter without augmenting inter-node communication, the repeated 4-bit quantization steps can accumulate quantization errors, potentially leading to suboptimal training outcomes. ", "page_idx": 3}, {"type": "image", "img_path": "PEEqnXlSCk/tmp/8784d5b134fc30624f4cf123efecfdc7dc0e5b21cd1b2e1ae57262fd195ea593.jpg", "img_caption": ["Figure 2: Communication of quantized weight differences. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "3.1 Quantization on Weight Differences (qWD) ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "As discussed in Section 2.1, ShardedDP requires each GPU to send/receive updated weights (main weights) to/from other GPUs in each iteration. However, weights generally exhibit a wide range and directly applying 4-bit quantization leads to significant quantization errors. Even with groupwise quantization, a gap in E2E training loss compared to full precision remains despite using small group sizes. ", "page_idx": 3}, {"type": "text", "text": "qWD: To address this issue, we quantize weight differences instead of the original weights during communication. As illustrated in Figure 2 and Algorithm 2, after the optimizer step, each GPU calculates the ", "page_idx": 3}, {"type": "image", "img_path": "PEEqnXlSCk/tmp/1bafd485172b4f45ec9fa402fbe12c6c37b907cab67004beaea96ff07e0493b7.jpg", "img_caption": ["Figure 4: Histogram of (a) weights and (b) weight differences. Each vertical dashed line represents a quantization level corresponding to a 4-bit quantization lattice. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "differences between the main weights and the model weights. These differences are then quantized and all-gathered across all GPUs. After all-gathering, each GPU dequantizes the received data to obtain the weight differences. These differences are then added to the model weights to obtain the updated weights. ", "page_idx": 3}, {"type": "text", "text": "There are two main benefits to applying quantization to weight differences. ", "page_idx": 3}, {"type": "text", "text": "1) In practice, weight differences are generally easier to quantize. As shown in Figure 4, weight differences are more uniformly distributed in a smaller range compared to the weights themselves, resulting in smaller errors for INT4 quantization. Furthermore, since intuitively the magnitudes of weight differences are smaller than those of weights themselves (informally supposed that $\\|\\delta w_{t}\\|\\;\\bar{=}\\;\\|w_{t}-w_{t-1}\\|\\;<\\;\\|w_{t}\\|)$ and the relative quantization errors are similar between weights and weight differences (informally supposed that $\\begin{array}{r}{\\frac{\\|q(\\delta w_{t})-\\delta w_{t}\\|}{\\|\\delta w_{t}\\|}\\approx\\frac{\\|q(w_{t})-w_{t}\\|}{\\|w_{t}\\|})}\\end{array}$ , the weight differences compression potentially has a smaller error relative to the weights themselves: \u2225q(\u03b4w\u2225tw)\u2212\u2225\u03b4wt\u2225\u2a85\u2225q(w\u2225tw)\u2212\u2225wt\u2225, where q(\u00b7) is the quantization function. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "2) In theory, weight differences compression improves convergence compared to naive weight compression. When extended to biased compressors, we present theoretical guarantees for convergence at the same rate as ordinary SGD, as detailed in Section 4.2. In contrast, we demonstrate that using biased compressors directly on weights can lead to convergence failure, as illustrated in an example in Section 4.1. This proves that biased compressors are not compatible with QSDP or $Z\\mathrm{eRO++}$ . ", "page_idx": 4}, {"type": "text", "text": "3.2 Two-Level Gradient Smooth Quantization ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "3.2.1 Two-level Gradient Quantization (TLq) ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "As discussed in Section 2.3, the 2-step all-to-all communication strategy beneftis gradient communication when quantization is applied. However, it also introduces error accumulation due to consecutive 4-bit quantization steps, necessitating additional rounds of training and communication that diminish the per-iteration communication savings. We observe that applying ULq, which quantizes gradeints to extremely low precision, such as 4-bit, leads to noticeable deviations in training loss compared to full-precision training, as illustrated in Figure 5. ", "page_idx": 4}, {"type": "text", "text": "TLq: Instead of employing a global 4-bit quantization strategy for both inter-node and intra-node all-to-all communications, we propose a two-level precision quantization strategy. This approach balances performance and accuracy by enhancing accuracy without introducing additional overhead. For intra-node all-to-all communication, gradients are quantized to INT8 before sending. After receiving the data, each GPU dequantizes the received data back to the full precision (i.e., FP32) for local reduction. This reduced data is then quantized to INT4 to minimize inter-node communication overhead. The detailed methodology is depicted in Figure 3. Since the two all-to-all operations utilize different network bandwidths, their communications can be effectively overlapped (see Table 4). ", "page_idx": 4}, {"type": "text", "text": "3.2.2 TLq with Hadamard Smoother (TLq-HS) ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "While $T L q$ brings the training loss closer to the baseline, it does not achieve perfect alignment. In gradient quantization, outliers can significantly amplify quantization errors. Although group-wise quantization isolates outliers to minimize their impact on the precision of values in other groups, the values within the same group remain affected. ", "page_idx": 4}, {"type": "text", "text": "TLq-HS: To mitigate the outlier issue, we apply Hadamard transform [9] to the gradients before quantization. The Hadamard transform, a specific type of generalized Fourier transform, exhibits properties such as $H=H^{T}$ and $H\\cdot H^{T}=I$ , distributing outlier information across nearby elements and effectively smoothing them out. For a detailed description of the methodology, see Algorithm 3. ", "page_idx": 4}, {"type": "text", "text": "3.3 Performance Optimizations in Implementation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Optimizing Memory Efficiency for Weight Differences Computation: After the all-gather communication, each GPU receives weight differences from the others, which are then added to the model weights to update them. As discussed in Section 2.1, with ShardedDP enabled in Megatron-LM, each GPU maintains a complete copy of the model weights for forward and backward computation. This contrasts with ZeRO, where model weights are released after computation. By reusing these locally stored model weights in MegatronLM, our implementation eliminates the need for additional buffers to retain model weights for calculating the differences, thus enhancing memory efficiency. ", "page_idx": 4}, {"type": "table", "img_path": "PEEqnXlSCk/tmp/4ff0d1de990b96ac236abf55640a23151945ccd0912a2d7319ef8a3a33c90ff3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "Simplifying Hadamard Transforms: In a naive implementation, the Hadamard transform would be applied at each step before quantization and after dequantization. However, by leveraging the orthogonality of the Hadamard transform, i.e. $H\\cdot H=I$ , we omit the transform after the intra-node all-to-all dequantization (Algorithm 3, Line 6) and before the inter-node quantization (Algorithm 3, Line 8). Furthermore, by utilizing the distributive property, i.e., $\\textstyle\\sum_{i}H{\\dot{g}}_{i}\\,=\\,H\\sum_{i}g_{i}$ , we move the second Hadamard transform from after the inter-node dequan tization (Algor ithm 3, Line 11) to after the final reduction (Algorithm 3, Line 12). These simplifications reduce the unnecessary computational overhead associated with repeated Hadamard transforms. ", "page_idx": 5}, {"type": "text", "text": "Fusing GPU Kernels with Group Size Alignment: To mitigate additional data movement from global memory\u2014which typically exhibits the slowest memory bandwidth\u2014, we fuse the Hadamard transform with the (de)quantization operations into a single CUDA kernel. This fusion allows the operations to run nearly as fast as the quantization operation alone. It is worth noting that, for this fusion to be efficient, there must be an alignment between the two. Specifically, the size of the quantization group must be divisible by the size of the Hadamard matrix, ensuring that memory traffic remains within the kernel block. We choose $H$ to be small (e.g., $32\\!\\times\\!32\\!)$ ) because, at this size, the transform operation on the GPU is typically memory-bound and incurs minimal overhead. While larger $H$ sizes offer better smoothing capabilities, we find that a $32\\!\\times\\!32$ matrix is sufficient to effectively smooth outliers in gradients. ", "page_idx": 5}, {"type": "text", "text": "4 Theoretical Analysis ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Counterexample of Biased Weight Compression ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "One of the advantages of our proposed weight difference compression is the compatibility to both biased and unbiased compressors. Note that using biased compressors directly on weight compression incurs issues in convergence under standard assumptions, as avoided in QSDP [18] or $Z\\mathrm{eRO++}$ [32]. We illustrate such issues in the following toy example. ", "page_idx": 5}, {"type": "text", "text": "Counterexample 4.1. Consider a least square problem with $\\begin{array}{r l r l}{w^{*}}&{{}=}&{}&{{}(0,0)^{\\top}}\\end{array}$ : $\\operatorname*{min}_{w\\in\\mathbb{R}^{2}}$ $\\left[f(w)=\\|w\\|^{2}\\right]$ , and stochastic gradient $g(w)\\;\\;=\\;\\;(4w_{1},0)^{\\top}$ with probability 0.5 and $g(w)\\,=\\,(0,4w_{2})^{\\top}$ with probability 0.5, thus $\\mathbb{E}[g(w)]\\,=\\,\\nabla_{w}f(w)$ . We use the initial value $w_{i n i t}\\ =\\ (1,-1)^{\\top}$ , the learning rate $\\eta~<~0.125$ , and the following nearest ternary quantizer: $s=\\operatorname*{max}(|w|),q(w)=r o u n d(w/s)*s$ , where $|\\cdot|$ is element-wise absolute value, and $\\bar{r o u n d(\\cdot)}$ quantizes each element to the nearest value in $\\{-1,0,1\\}$ . It is easy to check that for SGD under such settings, the weights before quantization will be either $(1-4\\eta,-1)^{\\top}$ or $(1,-1+4\\eta)^{\\top}$ , resulting in $\\ensuremath{\\boldsymbol{q}}(\\ensuremath{\\boldsymbol{w}})\\,=\\,(1,-1)^{\\intercal}$ , which means that SGD with ternary weight quantization gets stuck at the initial value in this case, while SGD without weight quantization and SGD with weight difference quantization both converge to the optimal. ", "page_idx": 5}, {"type": "text", "text": "4.2 Convergence Analysis ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To theoretically analyze the convergence of our distributed training algorithm with communication compression, we focus on the following SGD variant with gradient compression and weight difference compression. We use SGD to solve the following optimization problem: $f^{*}=\\operatorname*{min}_{w}f(w)$ , where $f(w)$ is the objective function, $w\\in\\mathbb{R}^{d}$ is the model parameter. ", "page_idx": 5}, {"type": "text", "text": "Algorithm 4 SGD with SDP4Bit ", "text_level": 1, "page_idx": 5}, {"type": "table", "img_path": "PEEqnXlSCk/tmp/20caa4fd00bf439dfab510ac2cee83d0d629418936fa9c3484d0b3874b72e086.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "Note that we use unbiased compressors for gradient reduction, and arbitrary (potentially biased) compressors for weight collection. We formally define these two classes of compressors as follows. ", "page_idx": 6}, {"type": "text", "text": "Definition 4.1 (Unbiased $\\kappa$ -approximate compressor [1]). An operator $\\mathcal{U}\\,:\\,\\mathbb{R}^{d}\\,\\rightarrow\\,\\mathbb{R}^{d}$ is a $\\kappa$ - approximate compressor for $\\kappa\\geq0$ if $\\mathbb{E}[\\mathcal{U}(v)]=v$ and $\\|\\bar{\\mathcal{\\mathbb{E}}}\\|\\bar{\\mathcal{U}}(v)-\\dot{v}\\|^{2}\\leq\\kappa\\|v\\|^{2},\\forall v\\in\\mathbb{R}^{d}$ . ", "page_idx": 6}, {"type": "text", "text": "Definition 4.2 ( $\\delta^{\\th}$ -approximate compressor [13]). An operator $\\mathcal{C}:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{d}$ is a $\\delta$ -approximate compressor for $\\delta\\in[0,1]$ if $\\mathbb{E}\\|\\mathcal{C}(v)\\stackrel{\\cdot}{-}v\\|^{2}\\leq\\stackrel{\\cdot}{(1-\\delta)}\\|v\\|^{\\frac{\\cdot}{2}},\\forall v\\in\\mathbb{R}^{d}$ . ", "page_idx": 6}, {"type": "text", "text": "Remark 4.1. Note that, in a certain sense, the class of $\\delta$ -approximate compressors contains the class of unbiased compressors. It is easy to check that any $\\kappa$ -approximate unbiased compressor $\\boldsymbol{\\mathcal{U}}$ can be converted to a $\\textstyle{\\frac{1}{1+\\kappa}}$ -approximate biased compressor $\\begin{array}{r}{\\dot{\\mathcal{C}}(\\dot{v})\\,=\\,\\frac{1}{1+\\kappa}\\mathcal{U}(v)}\\end{array}$ . Furthermore, the class of $\\delta\\!\\cdot$ -approximate compressors typically provides more options such as top- $k$ sparsifiers, and top- $k$ low-rank compressors. Thus, we consider arbitrary (biased or unbiased) $\\delta\\!\\cdot$ -approximate compressors for weight compression in our theoretical analysis. ", "page_idx": 6}, {"type": "text", "text": "Remark 4.2. For distributed training with $P$ workers, we define the compressed gradient as $\\tilde{g}_{t}=$ $\\begin{array}{r}{\\mathcal{U}_{g}(g_{t})=\\frac{1}{P}\\sum_{i\\in[P]}\\mathcal{U}_{g}^{\\prime}(g_{t,i})}\\end{array}$ , where $\\begin{array}{r}{g_{t}=\\frac{1}{P}\\sum_{i\\in[P]}g_{t,i}}\\end{array}$ , and $g_{t,i}$ is the stochastic gradient from the ith worker in $t$ iteration. We assume that $\\mathcal{U}_{g}$ is an unbiased $\\kappa$ -approximate compressor of the average gradient $g_{t}$ . ", "page_idx": 6}, {"type": "text", "text": "Assumption 4.1. (Smoothness) We assume that $f(x)$ is $L$ -smooth: $\\|\\nabla f(x)-\\nabla f(y)\\|\\leq L\\|x-$ $\\boldsymbol{y}\\|,\\forall\\boldsymbol{x},\\boldsymbol{y}\\in\\mathbb{R}^{d}$ , which implies $f(y)-f(x)\\leq\\langle\\nabla f(x),y-x\\rangle+\\frac{L}{2}\\|y-x\\|^{2}$ . ", "page_idx": 6}, {"type": "text", "text": "Assumption 4.2. For any stochastic gradient $\\nabla f(w;\\zeta)$ , where $\\zeta$ is an independent random sample, we assume unbiasedness $\\mathbb{E}[\\nabla f(w;\\zeta)|w]\\;=\\;\\nabla f(w)$ , and bounded variance $\\mathbb{E}[\\nabla f(w;\\zeta)~-$ $\\begin{array}{r}{\\dot{\\nabla}f(w)\\|^{2}|w|\\le\\rho\\|\\nabla f(w)\\|^{2}+\\sigma^{2}}\\end{array}$ ([27], Assumption 3). ", "page_idx": 6}, {"type": "text", "text": "We derive the following error bounds on the convergence of SDP4Bit under the above assumptions.   \nAll proofs can be found in Appendix A. ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.1 (Convergence error bound). For arbitrary non-convex function under Assumption 4.1 and Assumption 4.2, taking learning rate \u03b7 \u226410L( \u03b42 +\u03c11\u03ba+\u03c1+\u03ba), Algorithm $^{4}$ converges to a critical point with the following error bound: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\frac{\\sum_{t=0}^{T}\\mathbb{E}[\\|\\nabla f(\\tilde{w}_{t})\\|^{2}]}{T+1}\\le\\frac{80L\\left(\\frac{2}{\\delta}+\\rho\\kappa+\\rho+\\kappa\\right)\\left(f(w_{0})-f^{*}\\right)}{T+1}+4\\sigma\\sqrt{\\frac{(11-\\delta)(\\kappa+1)L(f(w_{0})-f^{*})}{T+1}}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Remark 4.3. Note that compared to QSDP [18], our convergence analysis does not require Polyak\u0141ojasiewicz condition or the specific choice of weight quantization (random shift). In other words, Theorem 4.1 shows that our proposed algorithm has the same $\\begin{array}{r}{{\\mathcal{O}}\\left({\\frac{1}{\\sqrt{T}}}\\right)}\\end{array}$ convergence rate as ordinary SGD for general non-convex functions, but under much weaker assumptions compared to QSDP. ", "page_idx": 6}, {"type": "text", "text": "5 Evaluation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "5.1 Experimental Setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Hardware: The experiments are conducted on two different clusters to evaluate SDP4Bit across varying network environments: 1) 16 nodes, each node equipped with 4 Nvidia A100-SXM4-40GB GPUs. All nodes are interconnected with a 100 Gbps Slingshot10 network, providing slower internode bandwidth. 2) 16 nodes, each node equipped with 8 Nvidia H800-SXM5-80GB GPUs. Each node is connected using 8 InfiniBand links, achieving a total bandwidth of 3.2 Tbps, providing higher inter-node bandwidth. ", "page_idx": 6}, {"type": "text", "text": "Baselines: We use BFloat16/Float32 (weights/gradients) mixed-precision in Megatron-LM [26] as our basic Baseline for both accuracy and E2E throughput analysis. Within each set of experiments, we ensure consistent hyper-parameters to ensure fairness. Detailed parameters are provided in Appendix D. Additionally, we implement another baseline for comparison in Megatron-LM, using the same quantization strategy in $Z\\mathrm{eRO++}$ , employing 4-bit quantization for both weights (groupwise weight quantization, refered to as $q W$ ) and gradients (twice all-to-all with uniform level 4-bit quantization, refer to as $U L q.$ ). ", "page_idx": 6}, {"type": "text", "text": "Dataset and Models: To demonstrate that SDP4Bit does not adversely affect end-to-end training loss, we conduct pre-training on GPT-series [23] models ranging from 125M to 6.7B parameters on the Pile dataset [8], using validation loss as the accuracy measure. Each test runs for 80,000 iterations, processing over 40 billion tokens. For throughput evaluation, we select models ranging from 1.3B to 18B parameters, with end-to-end training throughput as the metric. In these tests, the accumulation step is set to 1. Note that model parallel is required for models larger than 6.7B, and different tensor parallel sizes are used on A100 and H800 clusters for models larger than 13B. Please refer to Appendix B Table 8 for detailed model parallel configuration. ", "page_idx": 6}, {"type": "table", "img_path": "PEEqnXlSCk/tmp/2cc3f5a45e9d26d299c12792b0e14570f7c4b1780a8d48412d9211abe1cbacf6.jpg", "table_caption": ["Table 1: Final validation loss\u2193of pre-training with different quantization strategies. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "5.2 Accuracy Evaluation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "First, we analyze the impact of SDP4Bit on the accuracy of E2E training. As shown in Table 1, the training results for three different model sizes indicate that the final loss of SDP4Bit is comparable to the baseline, with a maximum increase of only $0.24\\%$ . Additionally, Figure 1 details the training curve of GPT-6.7B, demonstrating that the training curve of SDP4Bit perfectly aligns with the baseline. This indicates that the impact of SDP4Bit on accuracy is negligible. In contrast, the 4-bit quantization strategy in $Z\\mathrm{eRO++}$ (which directly applies quantization to weights and uniformly uses 4-bit quantization with all-to-all for gradients) results in significant accuracy degradation. ", "page_idx": 7}, {"type": "text", "text": "Next, we break down and analyze each strategy within SDP4Bit. 1) For weight communication reduction, as shown in Table 1, directly quantizing weights $(q W)$ to 4 bits results in a validation loss that increase of up to $12\\%$ compared to the baseline. In contrast, our weight difference quantization $(q W D)$ method achieves a validation loss nearly identical to the baseline. Notably, we use a consistent quantization group size of 2048 for both tests. 2) For gradient communication reduction, as shown in Figure 5, applying Uniform Level quantization $\\left(U L q\\right)$ , similar to the method used in $Z\\mathrm{eRO++}$ , results in a significant gap in the loss compared to the baseline. In comparison, our Two Level quantization $\\scriptstyle(T L q)$ siginificantly mitigate the loss gap between with baseline. Additionally, Figure 6 illustrates the effectiveness of the Hadamard transformation in smoothing outliers. Table 1 and Figure 5 further demonstrate the contribution of the Hadamard smoother to accuracy. Notably, compared to TLq, TLq-HS further narrows the validation loss gap, making it almost identical to the baseline. ", "page_idx": 7}, {"type": "image", "img_path": "PEEqnXlSCk/tmp/d0364cea681f184b47c7d5a57c954bea0a7cc163f713e370839fcb7aeeec8a1c.jpg", "img_caption": ["Figure 5: Validation loss comparison for the Baseline, ULq, TLq, and TLq-HS on the GPT-125M model. Uniformly applying 4-bit gradient quantization twice results in a noticeable gap compared to the baseline. In contrast, two-level quantization (8-bit for intranode and 4-bit for inter-node) mitigates this gap. The Hadamard smoother further reduces the gap, making the loss nearly identical to the baseline. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "PEEqnXlSCk/tmp/9c9e8c5449fb1c416c9424a2b30b53f607b6b9d6e1c907e58ed5f1554337936f.jpg", "img_caption": ["Figure 6: Comparison of gradient histograms before and after the Hadamard transformation. The transformation reduces the impact of outliers, resulting in a smoother gradient distribution. "], "img_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "PEEqnXlSCk/tmp/f1199883942cc14075d1203aeb011afeb5184cda5f1567b149e832774e0ee8bc.jpg", "table_caption": ["Table 2: E2E throughput\u2191on different model sizes with std. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "PEEqnXlSCk/tmp/83df59b0d6fb0415a05a5d9720176a67783efe814d635a048b6473ce4c278215.jpg", "table_caption": ["Table 3: Final validation loss\u2193of GPT-125M with different group sizes. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.3 Throughput Evaluation ", "text_level": 1, "page_idx": 8}, {"type": "table", "img_path": "PEEqnXlSCk/tmp/dbcc82ff12ff41ec97a04722f845c8aba9bb52ce04ce52a67ec29bc0b90dc4ee.jpg", "table_caption": ["Table 4: Performance of Different Quantization Strategies on GPT-1.3B over 32 A100 with standard deviation. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Next, we evaluate the improved E2E throughput, measured in FLOPS per second, of SDP4Bit on both hardware platforms. For all tests, the results are averaged over 10 iterations after 20 warm-up iterations. As shown in Table 2, SDP4Bit achieves an E2E training speedup of up to $4.08\\times$ . For models with the same model parallel configuration (e.g., 1.3B and 2.7B; 13B and 18B), both the E2E throughput and speedup from SDP4Bit increase as the model size grows due to larger models having higher computational efficiency but also encountering increased communication overhead. ", "page_idx": 8}, {"type": "text", "text": "The throughput of the 1.3B, 2.7B, and 6.7B models across the two platforms indicates that SDP4Bit provides a more significant speedup when network bandwidth is lower. This is because lower bandwidth results in higher communication overhead, which SDP4Bit effectively reduces through efficient quantization techniques. ", "page_idx": 8}, {"type": "image", "img_path": "PEEqnXlSCk/tmp/fd1a863a8b5f0ca46ba18174f8740f4bc9ccd98f222bbf1d2b73b6b044d16912.jpg", "img_caption": ["Figure 7: Scalability of SDP4Bit. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "PEEqnXlSCk/tmp/e60632cc1de1ea4196bf36efbe48d68275221de5edcb41861c126eb124cb845a.jpg", "img_caption": ["SDP4Bit on GPT-2.7B. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "In addition, we demonstrate the scalability of SDP4Bit using GPT models of 6.7B and 13B parameters, with tests conducted on up to 128 GPUs, as shown in Figure 7. Under low bandwidth conditions, SDP4Bit achieves an average speedup of $3.40\\times$ for the $6.7\\mathrm{B}$ model and $2.49\\times$ for the 13B model. In high-bandwidth InfiniBand environments, the speedup averages $3.08\\times$ for the 6.7B model and $3.73\\times$ for the 13B model. The comparatively lower speedup for the 13B model under low bandwidth conditions can be attributed to the introduction of pipeline parallelism, which diminishes the proportion of communication handled by ShardedDP. Overall, SDP4Bit consistently maintains stable speedup performance across various GPU numbers and network environments. ", "page_idx": 8}, {"type": "text", "text": "5.4 Ablation Study ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Components Breakdown. Figure 8 demonstrates the throughput improvement of $\\mathbf{q}\\mathbf{W}\\mathbf{D}$ , TLq-HS, and their combination (SDP4Bit) on two different platforms. $\\mathrm{qWD}$ alone provides a speedup ranging from $1.1\\times$ to $1.2\\times$ , while TLq-HS alone results in an E2E speedup of $1.4\\times$ to $1.8\\times$ . The notable benefit from gradient quantization stems from the high communication overhead associated with Float32 gradients in baseline training, which is higher compared to BFloat16 weights. When they are applied together, SDP4Bit achieves a more substantial speedup, ranging from $1.6\\times$ to $2.4\\times$ . ", "page_idx": 9}, {"type": "text", "text": "TLq-HS vs. ULq. Table 4 compares gradient quantization between TLq-HS and ULq. The results show that although TLq-HS employs 8-bit quantization for intra-node gradient communication, it introduces negligible overhead compared to 4-bit communication. This is due to 1) the high bandwidth of intra-node communication and 2) the fact that most intra-node communication is overlapped with the slower inter-node communication. ", "page_idx": 9}, {"type": "text", "text": "Hadamard Kernel Fusion. Table 4 shows that, compared to the SDP4Bit without fusing Hadamard Transform kernel, our optimized SDP4Bit reduces gradient communication overhead by $29\\%$ . Additionally, we provide a throughput comparison in Table 5 to further illustrate the impact of the Hadamard transformation. The results confirm that our Hadamard kernel fusion effectively reduces the overhead, making the transformation nearly zero-overhead and even matching the performance of quantization without the Hadamard transformation. ", "page_idx": 9}, {"type": "text", "text": "Convergence with Different Group Sizes. Table 3 examines the impact of various quantization granularities on the end-to-end validation loss during the pre-training of the GPT-125M model. For TLq-HS, a gradient quantization group size of 128 presents sufficient, with smaller sizes yielding no significant accuracy improvements. For $\\mathbf{q}\\mathbf{W}\\mathbf{D}$ , a quantization group size of 2048 achieves training accuracy comparable to the baseline. Table 3 also presents the 4-bit weight quantization $(q W)$ while using small group size. It is evident that even with very small group size (e.g., 32), direct 4-bit quantization leads to a significant gap in accuracy compared to the baseline, making 4-bit quantization for weights suboptimal. ", "page_idx": 9}, {"type": "text", "text": "6 Related Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Apart from $Z\\mathrm{eRO++}$ [32] and QSDP [18], which are specifically designed for communication compression in ShardedDP, most previous studies have focused on traditional DP, primarily utilizing gradient compression. This includes both unbiased compression techniques [1, 33, 38, 5], which employ randomized compressors, and biased compression methods with error compensation [12, 31, 30, 29, 24] that require extra storage for residual errors, making them less suitable for resourceintensive training of LLMs. Other strategies like local optimization or federated learning reduce communication frequency rather than volume [16, 28, 35, 34, 2, 20], but increase memory use, complicating their application in LLM training. In addition, techniques like low-precision training [19, 22] and parameter-efficient fine-tuning [10, 3, 14] minimize the volume of trainable variables to reduce communication. In a different vein, weight quantization for inference has also been explored [7, 6, 39, 37, 4], employing more resource-intensive methods compared to those used in training to fine-tune compression parameters. ", "page_idx": 9}, {"type": "text", "text": "The Hadamard transform has been applied to machine learning data, as seen in HQ-MM\u2019s [36] compression of activations and THC\u2019s [15] gradient communication within a parameter server framework. Unlike THC, SDP4Bit enhances collective communication operations and GPU optimization. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we propose SDP4Bit, a communication reduction strategy for Sharded Data Parallelism. SDP4Bit reduces both weight and gradient communication to nearly 4 bits while maintaining model accuracy comparable to the baseline. We implemented SDP4Bit in Megatron-LM and optimized it to reduce quantization overhead. Specifically, our experimental results demonstrate a training speedup of up to $4.08\\times\\mathrm{on}\\:128$ GPUs. This paper focuses on LLM pre-training, but we plan to extend our work to other models and areas such as MoE, computer vision, and fine-tuning in the future. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic. QSGD: Communication-Efficient SGD via Gradient Quantization and Encoding. In NeurIPS, 2017.   \n[2] Debraj Basu, Deepesh Data, Can Karakus, and Suhas Diggavi. Qsparse-local-SGD: Distributed SGD with Quantization, Sparsification and Local Computations. In NeurIPS, 2019.   \n[3] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. QLoRA: Efficient Finetuning of Quantized LLMs. In NeurIPS, 2024.   \n[4] Peiyan Dong, Lei Lu, Chao Wu, Cheng Lyu, Geng Yuan, Hao Tang, and Yanzhi Wang. PackQViT: Faster Sub-8-bit Vision Transformers via Full and Packed Quantization on the Mobile. In NeurIPS, 2024.   \n[5] Fartash Faghri, Iman Tabrizian, Ilia Markov, Dan Alistarh, Daniel M Roy, and Ali RamezaniKebrya. Adaptive Gradient Quantization for Data-Parallel SGD. In NeurIPS, 2020.   \n[6] Elias Frantar and Dan Alistarh. SparseGPT: Massive Language Models Can be Accurately Pruned in One-Shot. In ICML, 2023.   \n[7] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. GPTQ: ACCURATE POSTTRAINING QUANTIZATION FOR GENERATIVE PRE-TRAINED TRANSFORMERS. In ICLR, 2023.   \n[8] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The Pile: An 800GB Dataset of Diverse Text for Language Modeling. arXiv preprint arXiv:2101.00027, 2020.   \n[9] A Hedayat and Walter Dennis Wallis. Hadamard matrices and their applications. The annals of statistics, pages 1184\u20131238, 1978.   \n[10] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. LoRA: Low-Rank Adaptation of Large Language Models. In ICLR, 2021.   \n[11] Jiajun Huang, Sheng Di, Xiaodong Yu, Yujia Zhai, Jinyang Liu, Yafan Huang, Ken Raffenetti, Hui Zhou, Kai Zhao, Zizhong Chen, et al. gZCCL: Compression-Accelerated Collective Communication Framework for GPU Clusters. arXiv preprint arXiv:2308.05199, 2023.   \n[12] Sai Praneeth Karimireddy, Quentin Rebjock, Sebastian Stich, and Martin Jaggi. Error Feedback Fixes SignSGD and other Gradient Compression Schemes. In ICML, 2019.   \n[13] Sai Praneeth Karimireddy, Quentin Rebjock, Sebastian U. Stich, and Martin Jaggi. Error Feedback Fixes SignSGD and other Gradient Compression Schemes. In ICML, 2019.   \n[14] Jeonghoon Kim, Jung Hyun Lee, Sungdong Kim, Joonsuk Park, Kang Min Yoo, Se Jung Kwon, and Dongsoo Lee. Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization. In NeurIPS, 2024.   \n[15] Minghao Li, Ran Ben Basat, Shay Vargaftik, ChonLam Lao, Kevin Xu, Michael Mitzenmacher, and Minlan Yu. THC: Accelerating Distributed Deep Learning Using Tensor Homomorphic Compression. In NSDI, 2024.   \n[16] Tao Lin, Sebastian U Stich, Kumar Kshitij Patel, and Martin Jaggi. Don\u2019t Use Large Minibatches, Use Local SGD. In ICLR, 2020.   \n[17] Ilya Loshchilov and Frank Hutter. Decoupled Weight Decay Regularization. arXiv preprint arXiv:1711.05101, 2017.   \n[18] Ilia Markov, Adrian Vladu, Qi Guo, and Dan Alistarh. Quantized Distributed Training of Large Models with Convergence Guarantees. In ICML, 2023.   \n[19] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed Precision Training. arXiv preprint arXiv:1710.03740, 2017.   \n[20] Giorgi Nadiradze, Amirmojtaba Sabour, Peter Davies, Shigang Li, and Dan Alistarh. Asynchronous Decentralized SGD with Quantized and Local Updates. In NeurIPS, 2021.   \n[21] Pitch Patarasuk and Xin Yuan. Bandwidth Optimal All-reduce Algorithms for Clusters of Workstations. Journal of Parallel and Distributed Computing, 69(2):117\u2013124, 2009.   \n[22] Houwen Peng, Kan Wu, Yixuan Wei, Guoshuai Zhao, Yuxiang Yang, Ze Liu, Yifan Xiong, Ziyue Yang, Bolin Ni, Jingcheng Hu, Ruihang Li, Miaosen Zhang, Chen Li, Jia Ning, Ruizhe Wang, Zheng Zhang, Shuguang Liu, Joe Chau, Han Hu, and Peng Cheng. FP8-LM: Training FP8 Large Language Models. arXiv preprint arXiv:2310.18313, 2023.   \n[23] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language Models are Unsupervised Multitask Learners. 2019.   \n[24] Peter Richt\u00e1rik, Igor Sokolov, and Ilyas Fatkhullin. EF21: A New, Simpler, Theoretically Better, and Practically Faster Error Feedback. In NeurIPS, 2021.   \n[25] Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT. In AAAI, 2020.   \n[26] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism. arXiv preprint arXiv:1909.08053, 2019.   \n[27] Sebastian U. Stich and Sai Praneeth Karimireddy. The Error-Feedback Framework: Better Rates for SGD with Delayed Gradients and Compressed Communication. JMLR, 2020.   \n[28] Sebastian Urban Stich. Local SGD Converges Fast and Communicates Little. In ICLR, 2019.   \n[29] Hanlin Tang, Shaoduo Gan, Ammar Ahmad Awan, Samyam Rajbhandari, Conglong Li, Xiangru Lian, Ji Liu, Ce Zhang, and Yuxiong He. 1-bit Adam: Communication Efficient Large-Scale Training with Adam\u2019s Convergence Speed. In ICML, 2021.   \n[30] Hanlin Tang, Chen Yu, Xiangru Lian, Tong Zhang, and Ji Liu. DOUBLESQUEEZE: Parallel Stochastic Gradient Descent with Double-pass Error-Compensated Compression. In ICML, 2019.   \n[31] Thijs Vogels, Sai Praneeth Karimireddy, and Martin Jaggi. PowerSGD: Practical Low-Rank Gradient Compression for Distributed Optimization. In NeurIPS, 2019.   \n[32] Guanhua Wang, Heyang Qin, Sam Ade Jacobs, Connor Holmes, Samyam Rajbhandari, Olatunji Ruwase, Feng Yan, Lei Yang, and Yuxiong He. ZeRO $^{++}$ : Extremely Efficient Collective Communication for Giant Model Training. In ICML, 2023.   \n[33] Hongyi Wang, Scott Sievert, Shengchao Liu, Zachary Charles, Dimitris Papailiopoulos, and Stephen Wright. ATOMO: Communication-efficient Learning via Atomic Sparsification. In NeurIPS, 2018.   \n[34] Blake Woodworth, Kumar Kshitij Patel, and Nathan Srebro. Minibatch vs Local SGD for Heterogeneous Distributed Learning. In NeurIPS, 2020.   \n[35] Blake Woodworth, Kumar Kshitij Patel, Sebastian Stich, Zhen Dai, Brian Bullins, Brendan Mcmahan, Ohad Shamir, and Nathan Srebro. Is local SGD better than minibatch SGD? In ICML, 2020.   \n[36] Haocheng Xi, Changhao Li, Jianfei Chen, and Jun Zhu. Training Transformers with 4-bit Integers. In NeurIPS, 2023.   \n[37] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models. In ICML, 2023.   \n[38] Jihao Xin, Marco Canini, Peter Richt\u00e1rik, and Samuel Horv\u00e1th. Global-QSGD: Practical Floatless Quantization for Distributed Learning with Theoretical Guarantees. arXiv preprint arXiv:2305.18627, 2023.   \n[39] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong He. ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers. In NeurIPS, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A Proofs ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We use the following lemma (simplified from [27], Lemma 14) without proof. ", "page_idx": 13}, {"type": "text", "text": "Lemma A.1. For every non-negative sequence $\\{r_{t}\\}_{t\\ge0}$ and any parameters $a\\geq0,\\,c\\geq0,\\,T\\geq0,$ , there exists a constant $\\begin{array}{r}{\\eta\\le\\frac{1}{a}}\\end{array}$ , such that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac{1}{T+1}\\sum_{t=0}^{T}\\left(\\frac{r_{t}-r_{t+1}}{\\eta}+c\\eta\\right)=\\frac{1}{T+1}\\frac{r_{0}-r_{T+1}}{\\eta}+c\\eta\\le\\frac{a r_{0}}{T+1}+\\frac{2\\sqrt{c r_{0}}}{\\sqrt{T+1}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Theorem 4.1 (Convergence error bound). For arbitrary non-convex function under Assumption 4.1 and Assumption 4.2, taking learning rate \u03b7 \u2264 $\\begin{array}{r}{\\eta\\le\\frac{1}{10L\\left(\\frac{2}{\\delta}+\\rho\\kappa+\\rho+\\kappa\\right)}}\\end{array}$ , Algorithm 4 converges to a critical point with the following error bound: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac{\\sum_{t=0}^{T}\\mathbb{E}[\\|\\nabla f(\\tilde{w}_{t})\\|^{2}]}{T+1}\\le\\frac{80L\\left(\\frac{2}{\\delta}+\\rho\\kappa+\\rho+\\kappa\\right)\\left(f(w_{0})-f^{*}\\right)}{T+1}+4\\sigma\\sqrt{\\frac{(11-\\delta)(\\kappa+1)L(f(w_{0})-f^{*})}{T+1}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof. By using smoothness (Assumption 4.1), we have ", "page_idx": 13}, {"type": "equation", "text": "$$\nf(w_{t+1})\\leq f(w_{t})-\\eta\\,\\langle\\nabla f(w_{t}),\\mathcal{U}_{g}(g_{t})\\rangle+\\frac{\\eta^{2}L}{2}\\|\\mathcal{U}_{g}(g_{t})\\|^{2}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Taking expectation w.r.t. the random compressor $\\mathcal{U}_{g}$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{g c}[f(w_{t+1})]}\\\\ &{\\le f(w_{t})-\\eta\\left\\langle\\nabla f(w_{t}),g_{t}\\right\\rangle+\\frac{\\eta^{2}L}{2}\\mathbb{E}_{g c}\\Vert{\\mathcal{M}}_{g}(g_{t})\\Vert^{2}}\\\\ &{=f(w_{t})-\\eta\\left\\langle\\nabla f(w_{t}),g_{t}\\right\\rangle+\\frac{\\eta^{2}L}{2}[\\Vert g_{t}\\Vert^{2}+\\mathbb{E}_{g c}\\Vert{\\mathcal{M}}_{g}(g_{t})-g_{t}\\Vert^{2}]}\\\\ &{\\le f(w_{t})-\\eta\\left\\langle\\nabla f(w_{t}),g_{t}\\right\\rangle+\\frac{\\eta^{2}L(\\kappa+1)}{2}\\Vert g_{t}\\Vert^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Conditional on $w_{t}$ , taking expectation on the random sample $\\zeta_{t}$ , we have ", "page_idx": 14}, {"type": "text", "text": "E\u03b6[Egc[f(wt+1)]] ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad_{f}(\\eta,\\phi_{i})-_{f}(\\nabla_{\\phi}\\times\\nabla_{\\phi}(\\phi_{i}),\\phi_{f}(\\eta,\\phi_{i}))+\\frac{\\eta^{2}(\\delta,\\phi_{i})-_{f}(\\eta,\\phi_{i})-_{f}(\\eta,\\phi_{i})}{\\delta(\\alpha,\\phi_{i})}\\nabla_{\\phi}(\\phi_{i})}\\\\ &{=\\int(\\eta,\\phi_{i})-_{f}(\\nabla_{\\phi}\\times\\nabla_{\\phi}(\\phi_{i}),\\phi_{f}(\\eta,\\phi_{i}))+\\frac{\\eta^{2}(\\delta,\\phi_{i})-_{f}(\\eta,\\phi_{i})+_{f}(\\eta,\\phi_{i})}{\\delta(\\alpha,\\phi_{i})}\\nabla_{\\phi}(\\phi_{i})}\\\\ &{\\quad_{f}(\\eta,\\phi_{i})-_{f}(\\nabla_{\\phi}\\times\\nabla_{\\phi}(\\phi_{i}),\\phi_{f}(\\eta,\\phi_{i}))}\\\\ &{\\leq\\int(\\eta,\\phi_{i})-_{f}(\\nabla_{\\phi}\\times\\nabla_{\\phi}(\\phi_{i}),\\phi_{f}(\\eta,\\phi_{i}))+\\frac{\\eta^{2}(\\delta,\\phi_{i})-_{f}(\\eta,\\phi_{i})+_{f}(\\eta,\\phi_{i})}{\\delta(\\alpha,\\phi_{i})}\\nabla_{\\phi}(\\phi_{i})+\\frac{\\eta^{2}(\\delta,\\phi_{i})-_{f}(\\eta,\\phi_{i})}{\\delta(\\alpha,\\phi_{i})}}\\\\ &{\\quad_{f}(\\eta,\\phi_{i})-_{f}(\\nabla_{\\phi}\\times\\nabla_{\\phi}(\\phi_{i}),\\nabla_{\\phi}(\\phi_{i}))+\\frac{\\eta^{2}(\\delta,\\phi_{i})-_{f}(\\eta,\\phi_{i})}{\\delta(\\alpha,\\phi_{i})}\\nabla_{\\phi}(\\phi_{i})}\\\\ &{\\quad+\\frac{\\eta^{2}(\\delta,\\phi_{i})+_{f}(\\eta,\\phi_{i})}{\\delta(\\alpha,\\phi_{i})}}\\\\ &{\\quad_{f}(\\eta,\\phi_{i})-_{f}(\\nabla_{\\phi}\\times\\nabla_{\\phi}(\\phi_{i}),\\nabla_{\\phi}(\\phi_{i}))-\\nabla_{\\phi}(\\nabla_{\\phi}\\times\\nabla_{\\phi}(\\phi_{i\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Again using smoothness, and taking $\\begin{array}{r}{\\eta\\leq\\frac{1}{2L(\\rho+1)(\\kappa+1)}}\\end{array}$ , we have $\\begin{array}{r}{-\\frac{\\eta}{2}\\left[1-\\eta L(\\kappa+1)(\\rho+1)\\right]\\leq-\\frac{\\eta}{4}}\\end{array}$ , and, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\zeta}[\\mathbb{E}_{g c}[f(w_{t+1})]]}\\\\ &{\\leq f(w_{t})-\\frac{\\eta}{2}\\left[1-\\eta L(\\kappa+1)(\\rho+1)\\right]\\|\\nabla f(\\tilde{w}_{t})\\|^{2}+\\frac{\\eta L^{2}}{2}\\|w_{t}-\\tilde{w}_{t}\\|^{2}+\\frac{\\eta^{2}L(\\kappa+1)\\sigma^{2}}{2}}\\\\ &{\\leq f(w_{t})-\\frac{\\eta}{2}\\left[1-\\eta L(\\kappa+1)(\\rho+1)\\right]\\|\\nabla f(\\tilde{w}_{t})\\|^{2}+\\frac{\\eta L^{2}}{2}\\|e_{t}\\|^{2}+\\frac{\\eta^{2}L(\\kappa+1)\\sigma^{2}}{2}}\\\\ &{\\leq f(w_{t})-\\frac{\\eta}{4}\\|\\nabla f(\\tilde{w}_{t})\\|^{2}+\\frac{\\eta L^{2}}{2}\\|e_{t}\\|^{2}+\\frac{\\eta^{2}L(\\kappa+1)\\sigma^{2}}{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where we define the sequence ", "page_idx": 14}, {"type": "equation", "text": "$$\ne_{t}=w_{t}-\\tilde{w}_{t},e_{0}=0.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Now we establish the upper bound of the sequence $\\|e_{t}\\|^{2}$ as follows. ", "page_idx": 14}, {"type": "text", "text": "First, using $w_{t+1}=w_{t}-\\eta\\mathcal{U}_{g}(g_{t})$ and $\\tilde{w}_{t+1}=\\tilde{w}_{t}\\!+\\!\\mathcal{C}_{w}\\big(w_{t+1}\\!-\\!\\tilde{w}_{t}\\big)$ , we have the following equations: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{v_{t+1}-\\tilde{w}_{t+1}=e_{t+1}=w_{t}-\\tilde{w}_{t}-\\eta\\mathcal{U}_{g}(g_{t})-\\mathcal{C}_{w}(w_{t+1}-\\tilde{w}_{t})=e_{t}-\\eta\\mathcal{U}_{g}(g_{t})-\\mathcal{C}_{w}(e_{t}-\\eta\\mathcal{U}_{g}(g_{t}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Taking expectation w.r.t. the random compressor $\\mathcal{C}_{w}$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{w c}[\\|e_{t+1}\\|^{2}]}\\\\ &{=\\mathbb{E}_{w c}[\\|e_{t}-\\eta\\mathcal{U}_{g}(g_{t})-\\mathcal{C}_{w}(e_{t}-\\eta\\mathcal{U}_{g}(g_{t}))\\|^{2}]}\\\\ &{\\leq(1-\\delta)\\|e_{t}-\\eta\\mathcal{U}_{g}(g_{t})\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Taking expectation w.r.t. the random compressor $\\mathcal{U}_{g}$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{g c}[\\mathbb{E}_{w c}[\\|e_{t+1}\\|^{2}]]}\\\\ &{\\leq(1-\\delta)\\mathbb{E}_{g c}[\\|e_{t}-\\eta\\mathcal{U}_{g}(g_{t})\\|^{2}]}\\\\ &{=(1-\\delta)\\mathbb{E}_{g c}[\\|e_{t}-\\eta g_{t}+\\eta g_{t}-\\eta\\mathcal{U}_{g}(g_{t})\\|^{2}]}\\\\ &{=(1-\\delta)\\|e_{t}-\\eta g_{t}\\|^{2}+(1-\\delta)\\eta^{2}\\mathbb{E}_{g c}[\\|g_{t}-\\mathcal{U}_{g}(g_{t})\\|^{2}]}\\\\ &{\\leq(1-\\delta)\\|e_{t}-\\eta g_{t}\\|^{2}+(1-\\delta)\\eta^{2}\\kappa\\|g_{t}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Conditional on $w_{t}$ , taking expectation on the random sample $\\zeta_{t}$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{L}_{\\zeta}[\\mathbb{E}_{g c}[\\mathbb{E}_{w c}[\\|e_{t+1}\\|^{2}]]]}\\\\ &{\\le(1-\\delta)\\mathbb{E}_{\\zeta}[\\|e_{t}-\\eta\\nabla f(\\tilde{w}_{t})+\\eta\\nabla f(\\tilde{w}_{t})-\\eta g_{t}\\|^{2}]+(1-\\delta)\\eta^{2}\\kappa\\mathbb{E}_{\\zeta}[\\|g_{t}-\\nabla f(\\tilde{w}_{t})+\\nabla f(\\tilde{w}_{t})\\|^{2}]}\\\\ &{=(1-\\delta)\\|e_{t}-\\eta\\nabla f(\\tilde{w}_{t})\\|^{2}+(1-\\delta)(\\kappa+1)\\eta^{2}\\mathbb{E}_{\\zeta}[\\|g_{t}-\\nabla f(\\tilde{w}_{t})\\|^{2}]+(1-\\delta)\\eta^{2}\\kappa\\|\\nabla f(\\tilde{w}_{t})\\|^{2}}\\\\ &{\\le(1-\\delta)\\|e_{t}-\\eta\\nabla f(\\tilde{w}_{t})\\|^{2}+(1-\\delta)(\\kappa+1)\\eta^{2}(\\rho\\|\\nabla f(\\tilde{w}_{t})\\|^{2}+\\sigma^{2})+(1-\\delta)\\eta^{2}\\kappa\\|\\nabla f(\\tilde{w}_{t})\\|^{2}}\\\\ &{=(1-\\delta)\\|e_{t}-\\eta\\nabla f(\\tilde{w}_{t})\\|^{2}+(1-\\delta)\\eta^{2}(\\rho\\kappa+\\rho+\\kappa)\\|\\nabla f(\\tilde{w}_{t})\\|^{2}+(1-\\delta)(\\kappa+1)\\eta^{2}\\sigma^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "With $\\forall b>0$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\zeta}[\\mathbb{E}_{g c}[\\mathbb{E}_{w c}[\\|e_{t+1}\\|^{2}]]]}\\\\ &{\\le(1-\\delta)(1+b)\\|e_{t}\\|^{2}+(1-\\delta)(1+b^{-1})\\|\\eta\\nabla f(\\tilde{w}_{t})\\|^{2}+(1-\\delta)\\eta^{2}(\\rho\\kappa+\\rho+\\kappa)\\|\\nabla f(\\tilde{w}_{t})\\|^{2}}\\\\ &{\\quad+\\,(1-\\delta)(\\kappa+1)\\eta^{2}\\sigma^{2}}\\\\ &{=(1-\\delta)(1+b)\\|e_{t}\\|^{2}+(1-\\delta)\\eta^{2}[1+b^{-1}+(\\rho\\kappa+\\rho+\\kappa)]\\|\\nabla f(\\tilde{w}_{t})\\|^{2}+(1-\\delta)(\\kappa+1)\\eta^{2}\\sigma^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then, by taking 2(1\u03b4\u2212\u03b4), we have (1 \u2212\u03b4)(1 + b) = 1 \u2212\u03b42 , 1 + b\u22121 = 2\u03b4\u2212\u03b4 \u2264\u03b42 , and ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\zeta}[\\mathbb{E}_{g c}[\\mathbb{E}_{w c}[\\|e_{t+1}\\|^{2}]]]}\\\\ &{\\le(1-\\frac{\\delta}{2})\\|e_{t}\\|^{2}+(1-\\delta)\\eta^{2}\\left(\\frac{2}{\\delta}+\\rho\\kappa+\\rho+\\kappa\\right)\\|\\nabla f(\\tilde{w}_{t})\\|^{2}+(1-\\delta)(\\kappa+1)\\eta^{2}\\sigma^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We simplify the notation by denoting $\\mathbb{E}[\\|e_{t+1}\\|^{2}]\\,=\\,\\mathbb{E}_{\\zeta}[\\mathbb{E}_{g c}[\\mathbb{E}_{w c}[\\|e_{t+1}\\|^{2}]]]$ , and then unroll the sequence of $e_{t}$ back to $t=0$ . ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{c}[\\|e_{t+1}\\|^{2}]}\\\\ &{\\le\\displaystyle\\sum_{\\tau=0}^{t}(1-\\frac{\\delta}{2})^{t-\\tau}\\left[(1-\\delta)\\eta^{2}\\left(\\frac{2}{\\delta}+\\rho\\kappa+\\rho+\\kappa\\right)\\|\\nabla f(\\tilde{w}_{\\tau})\\|^{2}+(1-\\delta)(\\kappa+1)\\eta^{2}\\sigma^{2}\\right]}\\\\ &{\\le(1-\\delta)\\eta^{2}\\left(\\frac{2}{\\delta}+\\rho\\kappa+\\rho+\\kappa\\right)\\displaystyle\\sum_{\\tau=0}^{t}(1-\\frac{\\delta}{2})^{t-\\tau}\\|\\nabla f(\\tilde{w}_{\\tau})\\|^{2}+(1-\\delta)(\\kappa+1)\\eta^{2}\\sigma^{2}\\displaystyle\\sum_{\\tau=0}^{t}(1-\\frac{\\delta}{2})^{t-\\tau}}\\\\ &{\\le(1-\\delta)\\eta^{2}\\left(\\frac{2}{\\delta}+\\rho\\kappa+\\rho+\\kappa\\right)\\displaystyle\\sum_{\\tau=0}^{t}(1-\\frac{\\delta}{2})^{t-\\tau}\\|\\nabla f(\\tilde{w}_{\\tau})\\|^{2}+\\displaystyle\\frac{2(1-\\delta)(\\kappa+1)\\eta^{2}\\sigma^{2}}{\\delta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Taking $\\begin{array}{r}{\\eta\\le\\frac{1}{10L\\left(\\frac{2}{\\delta}+\\rho\\kappa+\\rho+\\kappa\\right)}}\\end{array}$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\|e_{t+1}\\|^{2}]}\\\\ &{\\le\\displaystyle\\frac{1-\\delta}{100L^{2}\\left(\\frac{2}{\\delta}+\\rho\\kappa+\\rho+\\kappa\\right)}\\sum_{\\tau=0}^{t}(1-\\frac{\\delta}{2})^{t-\\tau}\\|\\nabla f(\\tilde{w}_{\\tau})\\|^{2}+\\displaystyle\\frac{2(1-\\delta)(\\kappa+1)\\eta\\sigma^{2}}{\\delta10L\\left(\\frac{2}{\\delta}+\\rho\\kappa+\\rho+\\kappa\\right)}}\\\\ &{\\le\\displaystyle\\frac{1-\\delta}{100L^{2}\\frac{\\delta}{\\delta}}\\sum_{\\tau=0}^{t}(1-\\frac{\\delta}{2})^{t-\\tau}\\|\\nabla f(\\tilde{w}_{\\tau})\\|^{2}+\\displaystyle\\frac{2(1-\\delta)(\\kappa+1)\\eta\\sigma^{2}}{\\delta10L_{\\tilde{\\delta}}^{2}}}\\\\ &{\\le\\displaystyle\\frac{(1-\\delta)\\delta}{200L^{2}}\\sum_{\\tau=0}^{t}(1-\\frac{\\delta}{2})^{t-\\tau}\\|\\nabla f(\\tilde{w}_{\\tau})\\|^{2}+\\displaystyle\\frac{(1-\\delta)(\\kappa+1)\\eta\\sigma^{2}}{10L}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then, stacking $\\mathbb{E}[\\|e_{t}\\|^{2}]$ and taking total expectation, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=0}^{T}\\mathbb{E}[\\|e_{t+1}\\|^{2}]}\\\\ &{\\le\\frac{(1-\\delta)\\delta}{200L^{2}}\\displaystyle\\sum_{t=0}^{T}\\sum_{\\tau=0}^{t}(1-\\frac{\\delta}{2})^{t-\\tau}\\|\\nabla f(\\tilde{w}_{\\tau})\\|^{2}+\\frac{(T+1)(1-\\delta)(\\kappa+1)\\eta\\sigma^{2}}{10L}}\\\\ &{\\le\\frac{(1-\\delta)\\delta}{200L^{2}}\\displaystyle\\sum_{t=0}^{T}\\left[\\sum_{\\tau=0}^{+\\infty}(1-\\frac{\\delta}{2})^{\\tau}\\right]\\|\\nabla f(\\tilde{w}_{t})\\|^{2}+\\frac{(T+1)(1-\\delta)(\\kappa+1)\\eta\\sigma^{2}}{10L}}\\\\ &{\\le\\displaystyle\\frac{1-\\delta}{100L^{2}}\\displaystyle\\sum_{t=0}^{T}\\|\\nabla f(\\tilde{w}_{t})\\|^{2}+\\frac{(T+1)(1-\\delta)(\\kappa+1)\\eta\\sigma^{2}}{10L}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Putting all the ingredients together and taking total expectation, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\left[\\left(n_{m+1}\\right)\\right]}\\\\ &{=\\underset{0\\leq t\\leq T}{\\leq}\\underset{0\\leq t\\leq T}{\\leq}\\left\\lbrace\\sigma_{\\operatorname*{sup}}(\\log(1))+\\frac{n_{m}^{2}}{2}\\sum_{\\underline{{\\tau}}}^{\\infty}\\mathbb{E}[\\sigma_{\\operatorname*{sup}}(\\log(\\log(1))+\\frac{2(n_{m}+1)\\sigma_{\\operatorname*{L}}^{2}L(4+1)}{2}}\\\\ &{\\leq\\frac{5}{2}\\sum_{\\underline{{\\tau}}}^{\\infty}[\\sigma_{\\operatorname*{sup}}(\\log(1))+\\frac{2}{n_{m}^{2}}\\sum_{\\underline{{\\tau}}}^{\\infty}[\\sigma_{\\operatorname*{sup}}(\\log(1))+\\frac{2(n_{m}+1)\\sigma_{\\operatorname*{L}}^{2}L(4+1)}{2}}\\\\ &{\\leq\\mathbb{E}\\{\\sigma_{\\operatorname*{sup}}(\\log(1))-\\frac{4}{n_{m}^{2}}\\sum_{\\underline{{\\tau}}}^{\\infty}[\\sigma_{\\operatorname*{sup}}(\\log(1))+\\frac{n_{m}^{2}}{2}\\sum_{\\underline{{\\tau}}}^{\\infty}[\\sigma_{\\operatorname*{sup}}(\\log(1)+\\frac{2(n_{m}+1)\\sigma_{\\operatorname*{L}}^{2}L(4+1)}{2}}\\\\ &{\\leq\\mathbb{E}\\{\\sigma_{\\operatorname*{sup}}(\\log(1))-\\frac{4}{n_{m}^{2}}\\sum_{\\underline{{\\tau}}}^{\\infty}[\\sigma_{\\operatorname*{sup}}(\\log(1))+\\frac{n_{m}^{2}}{2}\\sum_{\\underline{{\\tau}}}^{\\infty}[\\sigma_{\\operatorname*{sup}}(\\log(1))+\\frac{2(n_{m}+1)\\sigma_{\\operatorname*{L}}^{2}L(4+1)}{2}}\\\\ &{\\leq\\mathbb{E}\\{\\sigma_{\\operatorname*{sup}}(\\log(1))-\\frac{4}{n_{m}^{2}}\\sum_{\\underline{{\\tau}}}^{\\infty}[\\sigma_{\\operatorname*{sup}}(\\log(1))+\\frac{(n_{m}-1)\\sigma_{\\operatorname*{L}}^{2}L(4+1)}{2}\\sum_{\\underline{{\\tau}}}^{\\infty}[\\sigma_{\\operatorname*{sup}}(\\log(1))^{2}}\\\\ \n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Finally, using Lemma A.1, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\cfrac{1}{T+1}\\displaystyle\\sum_{t=0}^{T}\\mathbb{E}[\\|\\nabla f(\\tilde{w}_{t})\\|^{2}]}\\\\ &{\\leq\\cfrac{8}{T+1}\\cfrac{\\mathbb{E}[f(w_{0})]-f^{*}+f^{*}-\\mathbb{E}[f(w_{T+1})]}{\\eta}+\\cfrac{8(11-\\delta)(\\kappa+1)L\\eta\\sigma^{2}}{20}}\\\\ &{\\leq\\cfrac{80L\\left(\\frac{2}{\\delta}+\\rho\\kappa+\\rho+\\kappa\\right)\\left(f(w_{0})-f^{*}\\right)}{T+1}+4\\sigma\\sqrt{\\cfrac{(11-\\delta)(\\kappa+1)L\\left(f(w_{0})-f^{*}\\right)}{T+1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "B Other Evaluation Results ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "To further demonstrate the effectiveness of SDP4Bit in enhancing training efficiency, we present the relationship between wall clock time and training loss in Figure 9. ", "page_idx": 17}, {"type": "image", "img_path": "PEEqnXlSCk/tmp/4ca531f52d2673e5aaf87556b6b8f07e0407ce5dcde8f1ca8ebd92644ee4580e.jpg", "img_caption": ["Figure 9: Comparison of validation loss versus wall-clock time for Baseline, $Z\\mathrm{eRO++}$ and SDP4Bit on the GPT-6.7B model. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "To further illustrate the impact of the Hadamard transformation on (de)quantization performance, we provide (de)quantization throughput experiment in Table 5, which is tested on an A100 GPU. ", "page_idx": 17}, {"type": "table", "img_path": "PEEqnXlSCk/tmp/72203a8d78e1cfbd7e1ec2694dc5337f7a50bb8d6db27c48b575087a418f75cb.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "C Notations in Training ", "text_level": 1, "page_idx": 17}, {"type": "table", "img_path": "PEEqnXlSCk/tmp/9dfb03e40b538d72b64b3a96e947c2a8c3d6d2c4748566618e8899b5a1e4ced4.jpg", "table_caption": ["Table 5: (De)quantization Throughput with/without Hadamard, including std. dev. "], "table_footnote": ["Table 6: Notations in experiments. "], "page_idx": 17}, {"type": "text", "text": "D Detailed Training Settings ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In the experimental section, we utilize a total of six different sizes of GPT models. Their model configurations are detailed in Table 7. ", "page_idx": 18}, {"type": "text", "text": "For the accuracy experiments, we standardize the batch size to 256, and set sequence length to 2048. We use AdamW [17] optimizer in all the experiments. The detailed training parameters are listed in Table 9. ", "page_idx": 18}, {"type": "text", "text": "In the throughput experiments, to more clearly study the communication bottleneck and ensure consistency across different GPU counts, we set the accumulation step to 1. The batch size is adjusted according to the number of GPUs, and the sequence length (micro batch) is uniformly set to 2048. Due to the different number of GPUs per node in the two architectures, we adjusted the tensor parallel size (TP) and pipeline parallel size (PP) accordingly, referencing [26], to achieve the highest throughput. Specifically, the maximum tensor parallel size is 4 for the 4xA100 environment and 8 for the $8\\mathrm{xH800}$ environment. See detailed parameters in Table 8. ", "page_idx": 18}, {"type": "table", "img_path": "PEEqnXlSCk/tmp/54ce17a3c741b9e334bbeee728f4c6df460fd63a345f6c3fca650831480c545b.jpg", "table_caption": ["Table 7: Model Size Parameters "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "PEEqnXlSCk/tmp/60c8c39a410cb14b39a4145e051a0f62e3e2e16375f3ff457917b59806f81c96.jpg", "table_caption": ["Table 8: Parallel Configuration for Throughput Test "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "PEEqnXlSCk/tmp/0ddad50c1cf256d1cd2cd0b27b1c8a15beeb1c926c56826606cd8b51622c1c1d.jpg", "table_caption": ["Table 9: E2E Convergence Training Parameters "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: The claims in the abstract and introduction accurately shows the contribution in the paper. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: In paper title, Section 1 Introduction, and Section 7 Conclusion, we state that this work focuses on the specific problem of communication compression of ShardDP on the training of LLMs. Furthermore, in Section 2.2 Quantization, we state that this work only considers symmetric linear quantization, and in Section 6 Related Work we explain that there are other types of compression methods that are not used in our algorithms. In Section 5.1 Experimental Setup, we list the models and the dataset we used in the experiments. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 19}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The full set of assumptions could be found in Section 4.2 Convergence Analysis. A complete proof could be found in Appendix A. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 20}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The detailed experiment configurations and hyperparameters could be found in Section 5.1 Experimental Setup, and Appendix B, respectively. Furthermore, the source code could be found in the supplementary materials, with a README file containing the instructions to run our experiments. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 20}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 21}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The dataset could be downloaded from hugginface website. The source code could be found in the supplementary materials, with a README file containing the instructions to run our experiments. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 21}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: All of these detailed hyperparameters and configurations could be found in Section 5.1 Experimental Setup, and Appendix B. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 21}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: We report the experiment results with standard deviations in Section 5 Evaluation. For the figures and plots, we found that the standard deviations and error bars are barely visible since they are too small compared to the average values. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 22}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The detailed information about the computer resources could be found in Section 5.1 Experimental Setup, Hardware. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 22}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We have reviewed the NeurIPS Code of Ethics, and we do not think this work would have any negative social impact or potential harmful consequences. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 22}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The goal of this work is to make model training faster with the model accuracy aligned with the original one. This work does not publish any new dataset or new model or the corresponding new application. This work simply proposes new algorithms that accelerate the training of existing models, thus having no social impact. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 23}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: This paper does not release any new dataset or new model. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 23}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We have cited the original paper that produced the code package or dataset, and included the license in the source code package. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 24}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We have included the license in the source code package, which is aligned with the code packages that our work is based on. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 24}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 24}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 25}]