[{"type": "text", "text": "AdaNCA: Neural Cellular Automata as Adaptors for More Robust Vision Transformer ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yitao Xu Tong Zhang Sabine S\u00fcsstrunk Image and Visual Representation Lab \u00c9cole polytechnique f\u00e9d\u00e9rale de Lausanne,Lausanne, Switzerland {yitao.xu,tong.zhang,sabine.susstrunk}@epfl.ch ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Vision Transformers (ViTs) demonstrate remarkable performance in image classification through visual-token interaction learning, particularly when equipped with local information via region attention or convolutions. Although such architectures improve the feature aggregation from different granularities, they often fail to contribute to the robustness of the networks. Neural Cellular Automata (NCA) enables the modeling of global visual-token representations through local interactions, as its training strategies and architecture design confer strong generalization ability and robustness against noisy input. In this paper, we propose Adaptor Neural Cellular Automata (AdaNCA) for Vision Transformers that uses NCA as plug-and-play adaptors between ViT layers, thus enhancing ViT\u2019s performance and robustness against adversarial samples as well as out-of-distribution inputs. To overcome the large computational overhead of standard NCAs, we propose Dynamic Interaction for more efficient interaction learning. Using our analysis of AdaNCA placement and robustness improvement, we also develop an algorithm for identifying the most effective insertion points for AdaNCA. With less than a $3\\%$ increase in parameters, AdaNCA contributes to more than $10\\%$ of absolute improvement in accuracy under adversarial attacks on the ImageNet1K benchmark. Moreover, we demonstrate with extensive evaluations across eight robustness benchmarks and four ViT architectures that AdaNCA, as a plug-and-play module, consistently improves the robustness of ViTs. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Vision Transformers (ViTs) exhibit impressive performance in image classification, through globally modeling token interactions via self-attention mechanisms [11, 14, 77]. Recent works show that integrating local information into ViTs, e.g., using region attention [26, 38, 39, 54, 65, 70, 73] or convolution [8, 10, 16, 22, 23, 37, 42, 52, 68, 71, 75], further enhances the ViT\u2019s capabilities in image classification. Although advanced local structures contribute to better captures of local information, the robustness of ViTs has not increased. They remain vulnerable to noisy input such as adversarial samples [5, 9, 13, 41, 42] and out-of-distribution (OOD) inputs [17, 18, 28, 30, 67]. ", "page_idx": 0}, {"type": "text", "text": "Recently, Neural Cellular Automata (NCA) was proposed as a lightweight architecture for modeling local cell interactions [44], where cells are represented by 1D vectors. To perform downstream tasks, similarly to the idea of token interactions in ViTs, cells in NCA interact with each other by alternating between a convolution-based Interaction stage and an MLP-based Update stage [46, 49]. The critical difference, however, is that cell interactions in NCA evolve over time by recurrent application of the two stages, whereas ViT computes the token interaction in a single step per layer. During this process, cells dynamically modulate their representations, based on the interactions with their neighbors, and they gradually enlarge their receptive fields. Unlike commonly used convolutional neural networks, NCA maintains resolution during neighborhood expansion. The recurrent update scheme enables the cells to explore various states, thus preventing NCA from overftiting and enhancing its generalization ability [44, 46]. NCA training involves various kinds of stochasticity [45], which enables the models to generalize to input variability and adapt to unpredictable perturbations. It is the modulation of local information and stochasticity during training that make NCA robust against noisy input [49, 48, 55, 62]. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "However, the original NCA has substantial computational overhead when operating in highdimensional space, which is a common scenario in ViTs. This poses a non-trivial challenge when integrating NCA into ViTs. To reduce the dimensionality of interaction results, hence to lower the computational cost, we propose Dynamic Interaction to replace the standard Interaction stage. In this stage, tokens dynamically modify the interaction strategy, based on the observation of their environment. This adaptation to the variability of the environment contributes to the model robustness. Our modified Adaptor NCA (AdaNCA), as a plug-and-play module, improves ViTs performances, as illustrated in Figure 1. Adding AdaNCA to different ViT architectures consistently improves their robustness to both adversarial attacks and OOD input. AdaNCA also improves clean accuracy. ", "page_idx": 1}, {"type": "text", "text": "Motivated by empirical observations of the positive relationship between network redundancy and model robustness [28], we develop a dy", "page_idx": 1}, {"type": "image", "img_path": "BQh1SGvROG/tmp/f3bd7d81abc332b89f946bdbdc1f84710077ae2496bcf97c7c7f29a04ac33771.jpg", "img_caption": ["Figure 1: The accuracy under adversarial attacks (APGD-DLR [9]) versus corruption error on outof-distribution input (ImageNet-C [28]) of various ViT models [16, 23, 38, 42]. AdaNCA improves the robustness of different ViTs against both adversarial attacks and OOD input. $\\star.$ the same model architecture but with more layers. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "namic programming algorithm for computing the most effective insert position for AdaNCA within a ViT, based on our proposed quantification of network redundancy. Our method results in consistent improvements across eight robustness benchmarks and four different baseline ViT architectures. Critically, we demonstrate that the improvements do not originate from the increase in parameters and FLOPS but are attributed to AdaNCA. Our contributions are as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose AdaNCA: It integrates Neural Cellular Automata into ViTs\u2019 middle layers as lightweight Adaptors for the robustness enhancement of ViTs against adversarial attacks and OOD inputs in image classification. With less than $3\\%$ more parameters, AdaNCA-extended ViTs can, under certain adversarial attacks, achieve $10\\%$ higher accuracy.   \n\u2022 We introduce Dynamic Interaction to replace the Interaction stage in standard NCA, thus enhancing model robustness and efficiency in terms of parameters and computation.   \n\u2022 We propose a method for determining the most effective insert positions of AdaNCA, for maximum robustness improvement. ", "page_idx": 1}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Local structure in Vision Transformers ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Since the proposal of Vision Transformer (ViT) [14], a series of works have introduced local structures into ViTs to enhance their performance [8, 10, 16, 22, 23, 37, 42, 52, 62, 68, 70, 71, 73, 75, 76]. Here, we mention one of the earliest local structure modifications and those relevant to our work. Stand-Alone Self-Attention (SASA), as introduced by Ramachandran et al. [54], utilizes sliding window self-attention in ViTs. Following this, Liu et al. [38] develop a non-sliding window attention mechanism that partitions feature maps and computes self-attention, both within and between these partitions; it is termed Shifted Window (Swin) attention. Another method for modeling local information is convolution. D\u2019Ascoli et al. [16] introduce a soft local-inductive bias by using gated positional self-attention, thus fusing self-attention and convolution. Despite these advancements in better modeling of local information, few methods lead to a more robust ViT architecture [42]. This leaves the models to behave subpar when encountering slightly noisy inputs or distribution shifts. ", "page_idx": 1}, {"type": "image", "img_path": "BQh1SGvROG/tmp/9ca7a9f05a71ef04d90c7016e4b8730e8dbaccb0da09f59505cd4b4b6508f155.jpg", "img_caption": [], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 2: Method overview. (a) To improve model performance and robustness, Neural Cellular Automata (NCA) can be inserted into Vision Transformers (ViTs) as Adaptors, hence termed AdaNCA. The details of AdaNCA are presented in Section 3.2. The improvement is maximized when AdaNCA is inserted between two layer sets that each consists of similar layers. (b) The robustness improvement brought by AdaNCA is highly correlated with the corresponding network redundancy quantification of the insert position introduced in Section 3.3. This supports the idea that AdaNCA should be placed between two sets of redundant layers. ", "page_idx": 2}, {"type": "text", "text": "2.2 Robust architecture in Vision Transformers ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Researchers have developed various architectural changes for building more robust ViTs against adversarial attacks, such as FGSM [61] or PGD [41], as well as out-of-distribution (OOD) inputs, such as image corruption [28]. Zhou et al. [78] propose Full Attention Networks to boost the robustness of ViTs against OOD images. Mao et al. [42] first systematically analyze the relationship between different components in a ViT, drawing a positive relationship between convolutional components and the robustness of ViTs against adversarial samples and OOD data. By extending [42] and [78], Guo et al. [23] propose input-dependent average pooling in order to adaptively select different aggregation neighborhoods for different tokens, thus achieving the state-of-the-art robust ViTs in OOD generalization. Different interpretations of the self-attention operation can also lead to more robust architectures [24, 58]. However, the methods that introduce additional architectures [23, 24, 58, 78] are either implemented on ViTs with limited size or focus on non-adversarial robustness. On the contrary, our method introduces NCA as lightweight plug-and-play adaptors into base-level ViTs, thus enhancing their clean accuracy and robustness against both adversarial samples and OOD inputs. ", "page_idx": 2}, {"type": "text", "text": "2.3 Neural Cellular Automata ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Gilpin [20] demonstrates that CA can be represented by convolutional neural networks. By extending [20], Mordvintsev et al. [44] propose NCA in order to mimic the biological cell interactions and model morphogenesis. Following this idea, several works apply NCA in computer vision, including texture synthesis [43, 46, 49, 48, 50], image generation [33, 47, 51, 62], and image segmentation [32, 57]. Randazzo et al. [55] propose applying NCA for modeling collective intelligence on image classification tasks, but the limitation to binary images restricts its practical application. Tesfaldet et al. [62] first establish the connection between ViT and NCA via recurrent local attention. It leads to a more robust model for handling image corruptions in image inpainting tasks. However, their application is limited to image impainting on small datasets such as MNIST [1] and CIFAR10 [36]. The NCA in [62] attempts to emulate a ViT whereas our approach distinguishes itself by not doing so. We first applies NCA in image classification on ImageNet1K with base-level ViT models. Moreover, we propose the new Dynamic Interaction for efficient cell interaction modeling, reducing the computational overhead, and for enhancing the model performance. ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The overview of our method is shown in Figure 2. In this section, we first review the NCA model and ViT architecture. In Section 3.1, we establish the connection between NCA and ViT, in terms of token interaction modeling. We then present the design of our AdaNCA in Section 3.2. We insert AdaNCA into the middle layers of ViT to improve its robustness. We introduce the relationship between the insert position of AdaNCA and the relative improvements of model robustness in Section 3.3. This relationship leads to the algorithm for deciding the most effective placement for AdaNCA. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "3.1 Preliminaries ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Vision Transformers Vision Transformers (ViTs) operate on token maps $\\mathbf{X}\\in\\mathbb{R}^{N\\times C}$ , where the number of tokens is $N$ and each token is represented by a $C$ -dimensional vector. ViTs learn the interaction between these tokens via self-attention [64] and compute the interaction result $\\mathbf{X}_{a t t n}$ , as described in Equation 1. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{X}_{a t t n}=\\sigma\\left({\\frac{\\mathbf{Q}\\mathbf{K}^{\\top}}{\\sqrt{D}}}\\right)\\mathbf{V}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "$\\mathbf{Q},\\mathbf{K},\\mathbf{V}$ stand for query, key, and value, respectively. They are deduced from different linear projections of the input, i.e., $\\mathbf{Q}=\\mathbf{X}\\mathbf{W_{Q}},\\mathbf{K}=\\mathbf{XW_{K}},\\mathbf{V}=\\mathbf{XW_{V}}$ , where $\\mathbf{W_{Q}},\\mathbf{W_{K}},\\mathbf{W_{V}}\\in$ $\\mathbb{R}^{C\\times D}$ . $D$ is the hidden dimensionality in self-attention. $\\sigma$ is Softmax. After self-attention, tokens are fed into a Multilayer Perceptron (MLP) to obtain the updated representations $\\mathbf{X}_{o u t}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{\\bf X}_{o u t}=f_{\\theta}({\\bf X}_{a t t n}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "$f$ is the MLP and $\\theta$ stands for its parameters. The self-attention and MLP form a single ViT block, and a ViT model can be built via stacking ViT blocks. ", "page_idx": 3}, {"type": "text", "text": "Neural Cellular Automata NCA aims at modeling cell interactions. In the 2D domain, cells live on a 2D grid with size $H\\times W$ . Each cell is represented by a vector with dimensionality $C$ . All cells collectively define the cell states $\\mathbf{S}\\in\\mathbb{R}^{H\\times W\\times C}$ . In a single step of NCA, to generate the interaction output $\\mathbf{S}_{\\mathcal{T}}$ , cells first interact with their neighborhoods for an information exchange in the Interaction stage [49]; the interaction is typically instantiated via depth-wise convolutions [44, 46, 48]: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{S}_{\\mathcal{T}}=(\\mathbf{S}\\oplus[\\mathcal{C}_{1},\\mathcal{C}_{2},...,\\mathcal{C}_{\\mathcal{M}}])_{\\oplus}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "$\\mathcal{C}_{i}$ is the $i$ th convolutional kernel, and $\\mathcal{M}$ denotes the total number of kernels. $\\bullet_{\\circledast}$ \u2019 denotes depth-wise convolution. The kernels can either be fixed [49, 48] or learnable [44, 46]. The results of all kernels are concatenated channel-wise in the $\\bigoplus$ operation. $\\mathbf{S}_{\\mathcal{T}}$ is then passed to an MLP in the Update stage [49]: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{S}_{o u t}=f_{\\theta}(\\mathbf{S}_{\\mathcal{T}})\\odot\\mathbf{M}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "$\\mathbf{S}_{o u t}$ is then used to update the cell states in a residual scheme. $f$ is the MLP, and $\\theta$ stands for its parameters. Typically, NCA uses the simplest MLP, with two linear layers and one activation between them. $\\mathbf{M}\\in\\dot{\\mathbb{R}^{H}}\\!\\times\\!W\\,\\check{\\times}\\,C$ , sampled from $B e r n o u l l i(p)$ , is a random binary mask to introduce stochasticity in NCA; it ensures asynchronicity during the cell updates [45]. $\\odot$ is point-wise multiplication. NCA learns an underlying dynamic that governs the cell behaviors [50], as depicted by the stochastic differential equation (SDE) in Equation 5: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\frac{\\partial\\mathbf{S}}{\\partial t}=\\mathcal{F}_{\\Theta}(\\mathbf{S})\\odot\\mathbf{M}=f_{\\theta}\\left[(\\mathbf{S}\\circledast[\\mathcal{C}_{1},\\mathcal{C}_{2},...,\\mathcal{C}_{M}])_{\\oplus}\\right]\\odot\\mathbf{M}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "$\\mathcal{F}$ represents operations in Interaction as well as Update stages. $\\Theta$ is the set containing trainable parameters in the two stages. Discretizing the SDE with $\\Delta t=1$ naturally results in a recurrent residual update scheme: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{S}^{t+1}=\\mathbf{S}^{t}+\\mathcal{F}_{\\Theta}(\\mathbf{S}^{t})\\odot\\mathbf{M}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "After $t\\,=\\,T$ steps, the cell states $S^{T}$ is extracted to accomplish certain downstream tasks. The traditional NCA involves several other specific designs though, in our case, it is impractical to adapt them. We provide a discussion on this topic in Appendix $\\boldsymbol{\\mathrm E}$ . ", "page_idx": 3}, {"type": "text", "text": "3.1.1 Connecting NCA and ViT ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Both NCA and ViT learn interactions between a set of elements, i.e., tokens in ViT and cells in NCA. Hereafter, we refer to a cell in NCA as a token, aligning it with the concept in ViTs. The asynchronicity [45] introduced by the random mask M can be regarded as a cell-wise stochastic depth [31], a more fine-grained version of the sample-wise stochastic depth. In previous NCA works, stochasticity is maintained during testing [46]. Such a scheme is problematic in our case because (1) test-time stochasticity produces obfuscated gradients [3], leading to the circumvention of adversarial ", "page_idx": 3}, {"type": "image", "img_path": "BQh1SGvROG/tmp/49b04d2365127044681542245857be970fb89dc427c96f6fbbb3fd329df60446.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 3: Overview of AdaNCA architecture. Instead of concatenating the interaction results generated by the depth-wise convolutions, our Dynamic Interaction conducts a point-wise weighted sum on them to improve the efficiency and enhance the performance. The weights are obtained based on the token states so that each token can dynamically adjust, according to the inputs, the interaction strategy. The Multi-scale Dynamic Interaction aggregates the results from Dynamic Interaction , where the convolutions have different dilation rates. Then, to finish one step of evolution, the output is fed into the Update stage . ", "page_idx": 4}, {"type": "text", "text": "attacks, and (2) the model can output different results given the same inputs. To this end, we adopt the strategy in dropout-like techniques [31, 59], which compensates activation values during training. Given $\\mathbf{M}\\sim B e r n o u l l i(p)$ , the evolution of NCA in AdaNCA is defined as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{Train}:\\mathbf{S}^{t+1}=\\mathbf{S}^{t}+\\frac{\\mathcal{F}_{\\Theta}(\\mathbf{S}^{t})}{p}\\odot\\mathbf{M};\\quad\\mathbf{Test}:\\mathbf{S}^{t+1}=\\mathbf{S}^{t}+\\mathcal{F}_{\\Theta}(\\mathbf{S}^{t}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We discuss the necessity of such a scheme in Section D.1 in the Appendix. Furthermore, NCA typically outputs the cell states at a random time step $T$ , resulting in random update steps for all cells. Such randomness ensures the stability of NCA across various time steps [44]. Finally, the recurrent steps of NCA during a single training epoch enable the exploration of a wide range of cell states. In the early stage of training, the model is not adequately trained hence serves as a source of noise to itself through the recurrence. With all these components, the trained model can effectively handle the variability and unpredictability of the input thus be robust against noisy input. Our ablation studies in Section 4.3 demonstrate the effectiveness of these strategies in enhancing the model performance. ", "page_idx": 4}, {"type": "text", "text": "3.2 AdaNCA architecture ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The architecture of AdaNCA is shown in Figure 3. It shares a similar update scheme with the standard NCA, as described in Equation 7; but, it is more computationally efficient due to the proposed Dynamic Interaction stage. All convolutional kernels for token interaction are trainable. In the following paragraphs, we first present the design of the Dynamic Interaction stage and then introduce a way for more efficient token interaction by using multi-scale Dynamic Interaction. ", "page_idx": 4}, {"type": "text", "text": "3.2.1 Dynamic Interaction ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The Interaction stage in the original NCA performs a channel-wise concatenation of the $\\mathcal{M}$ output tensors from different depth-wise convolutions. Whereas, our Dynamic Interaction computes a weighted sum of those results. Specifically, a weight computation network $\\mathcal{W}_{\\mathbb{Z}}$ takes the token map $\\mathbf{S}\\in\\mathbf{\\bar{R}}^{H\\times W\\times C}$ as input and outputs per-token scalar weights $\\mathbf{W}_{\\mathbb{Z}m}\\in\\mathbb{R}^{H\\times W\\times1}$ for each of the $\\mathcal{M}$ kernels. We modify Equation 3 to Equation 8: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{S}_{D\\mathcal{T}}=\\sum(\\mathbf{S}*\\mathcal{W}_{\\mathcal{T}})(\\mathbf{S}\\oplus[\\mathcal{C}_{1},\\mathcal{C}_{2},...,\\mathcal{C}_{M}])=\\sum(\\mathbf{S}*\\mathcal{W}_{\\mathcal{Z}})\\mathbf{S}_{m}=\\sum_{m=1}^{M}\\left(\\sum_{c=1}^{C}\\mathbf{W}_{\\mathcal{Z}m}\\odot\\mathbf{S}_{m c}\\right),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbf{S}_{m c}\\in\\mathbb{R}^{H\\times W\\times1}$ , $\\mathbf{S}_{D\\mathbb{Z}},\\mathbf{S}_{m}\\in\\mathbb{R}^{H\\times W\\times C}.\\,^{\\ast}\\ast^{\\ast}$ denotes the convolution. Recall that $\\Lsh$ \u2019 is the depth-wise convolution. We instantiate the weight computation module $\\mathcal{W}_{\\mathbb{Z}}$ by using a two-layer convolutional network. The first layer transforms the dimensionality from $C$ to $\\mathcal{M}$ , and the second layer computes the actual weights, thus producing $\\mathcal{M}$ scalars for each token. Both layers use $3\\times3$ convolutions to factor in information from both the token and its neighbors. To stabilize training, we add a batch normalization between the two convolutions. Our design of the weight computation network coincides with the one in [23]. Although, our focus is on extracting various information from the same neighborhoods rather than on aggregating data from different neighborhoods. ", "page_idx": 5}, {"type": "text", "text": "3.2.2 Multi-scale Dynamic Interaction ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Inspired by [48], which uses multi-scale token Interaction to facilitate long-range token communication, we propose multi-scale Dynamic Interaction. Concretely, all convolutions in Equation 8 now have one more degree of freedom in dilation. Dilation $s$ represents the current operating scale being $s$ , and $s\\in\\{1,2,...,S\\}$ . Hence, the original Dynamic Interaction is a special case where ${\\cal S}=1$ . To increase the feature expressivity, we perform a weighted sum on the outputs of all scales, where the per-token weights ${\\bf W}_{M s}$ are generated by a network $\\mathcal{W}_{M}$ as described in Equation 9. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{S}_{M D Z}=\\sum(\\mathbf{S}*\\mathcal{W}_{M})\\mathbf{S}_{D Z}=\\sum(\\mathbf{S}*\\mathcal{W}_{M})\\mathbf{S}_{D Z}=\\sum_{s=1}^{S}\\left(\\sum_{c=1}^{C}\\mathbf{W}_{M s}\\odot\\mathbf{S}_{D Z s c}\\right),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where ${\\bf W}_{M s}$ , $\\mathbf{S}_{D\\mathcal{T}s c}\\,\\in\\,\\mathbb{R}^{H\\times W\\times1},\\mathbf{S}_{M D\\mathcal{T}},\\mathbf{S}_{D\\mathcal{T}s}\\,\\in\\,\\mathbb{R}^{H\\times W\\times C}$ . The $\\mathcal{W}_{\\mathbb{Z}}$ in Equation 8 is shared across all scales. The weight computation network $\\mathcal{W}_{M}$ mirrors that of $\\mathcal{W}_{\\mathbb{Z}}$ . ", "page_idx": 5}, {"type": "text", "text": "3.3 Insert positions of AdaNCA ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Given a ViT and an AdaNCA, to maximize the robustness improvements, we need to determine where to insert AdaNCA. To this end, we first establish the correlation between the placement of AdaNCA and the robustness enhancement it brings. Motivated by the fact that the network redundancy contributes to the model robustness [28], we hypothesize that the effect of AdaNCA should correlate to the layer redundancy corresponding to the insert position. To quantify the redundancy, we propose the Set Cohesion Index $\\kappa$ . Given a trained model with $L$ layers and two layer indices $i,j\\in\\bar{\\{1,2,...,L\\}}$ where $i<j,\\kappa(i,j)$ is defined in Equation 10. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\kappa(i,j)=\\displaystyle\\frac{1}{(j-i+1)^{2}}\\sum_{m,n\\in[i,j]}S i m(m,n)-\\displaystyle\\frac{\\mathbb{1}_{i>1}}{(i-1)(j-i+1)}\\sum_{m_{1}\\in[1,i-1],n\\in[i,j]}S i m(m_{1},n)}\\\\ {-\\displaystyle\\frac{\\mathbb{1}_{j<L}}{(L-j)(j-i+1)}\\sum_{m\\in[i,j],n_{1}\\in[j+1,L]}S i m(m,n_{1})}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "1 stands for the indicator function. $S i m(m,n)$ is the function for quantifying the output similarity between layer $m$ and $n$ . We choose Centered Kernel Alignment (CKA) [35], a common metric for measuring layer similarities inside or between neural networks [25]. A higher $\\kappa$ stands for a more cohesive layer set defined by layers from $i$ to $j$ . Inserting AdaNCA after layer $i$ would partition the network into two layer sets, and we can compute the sum of $\\kappa$ of the layers before and after $i$ , i.e., $\\mathcal{K}(i)=\\kappa(1,i)+\\dot{\\kappa_{}^{}}(i+1,L)$ . This serves as a quantification of the network redundancy that corresponds to position $i$ . We assume that AdaNCA will not change the layer similarity structure because it is too small compared to a single layer in all ViTs. ", "page_idx": 5}, {"type": "text", "text": "In addition to the quantification of the network redundancy, the robustness improvement, brought by AdaNCA, is quantified using the relative increase in the attack failure rate of the AdaNCA-inserted models and the corresponding baseline. Specifically, if a model can achieve $\\alpha$ clean test accuracy as well as $\\alpha^{\\prime}$ accuracy under adversarial attacks, the attack failure rate is $\\begin{array}{r}{\\beta=\\frac{\\alpha^{\\prime}}{\\alpha}}\\end{array}$ . The robustness improvement $\\gamma$ is then defined as $\\begin{array}{r}{\\gamma~=~\\frac{\\beta_{A d a N C A}-\\beta_{b a s e}}{\\beta_{b a s e}}}\\end{array}$ . In our experiments, we find that $\\gamma$ is significantly correlated with the network redundancy $\\kappa$ (Pearson correlation $r=0.6938,p<0.001)$ . We refer readers to Appendix A for details of the experiments. The results validate our hypothesis and indicate that AdaNCA should be inserted into the position that can maximize network redundancy. We develop a dynamic programming algorithm to find these positions and refer readers to Appendix A.1 for the details. ", "page_idx": 5}, {"type": "text", "text": "Table 1: The performance of AdaNCA-enhanced ViTs and their corresponding baselines. We report the mCE for IM-C (lower is better) and accuracy for other benchmarks. AdaNCA consistently improves the clean accuracy as well as the robustness to adversarial and out-of-distribution inputs of the baseline models. Note that our models also outperform the larger baselines $\\cdot\\star$ sign), indicating that the performance improvement does not merely originate from the increase in the number of parameters or FLOPS. The TAPADL method [23] can lead to more vulnerable models compared to the baselines (green numbers). Bold indicates the best model. \u2020: the test is conducted on models pre-trained on ImageNet22K [42], see Appendix C.9. ", "page_idx": 6}, {"type": "table", "img_path": "BQh1SGvROG/tmp/9b104185a68ad84b9cba3c8d9d7fa71bcaff81a8fc3a26e50cdf56823fef5ebb.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "BQh1SGvROG/tmp/27cf79f93d08fdc666efe04fdcef775c3abdbc4d2291c8d377383eb4668b83f4.jpg", "table_caption": ["Table 2: Comparisons of corruption error on each corruption type of ImageNet-C. AdaNCA consistently improves the performance of the baseline model in all categories (Swin-B), and can achieve better results in most categories compared to the SOTA method (TAPADL-RVT). Bold indicates the best model. Lower is better. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We use four ViT models as the baseline: Swin-base (Swin-B) [38], FAN-Base-Hybrid (FAN-B) [78], RVT-Base-Plus (RVT-B) [42], and ConViT-Base (ConViT-B) [16]. They include a hierarchical model (Swin), a convolution-attention hybrid model (FAN), and two regular models in which all layers share the same structure (RVT and ConViT). RVT is specifically built to be a robust model, whereas ConViT is not. All four models are equipped with different kinds of local structures. We use two SOTA models in terms of robustness against out-of-distribution (OOD) data, TAPADL-RVT and TAPADL-FAN [23] for comparison. Note that the SOTA method involves training with an additional loss (ADL) and that, for completeness, we keep the results from the TAPADL models trained with such a loss. However, as our focus is on the effect of architectural changes, we do not incorporate the ADL loss in training the AdaNCA-enhanced ViTs. We follow the training scheme for each model, respectively, to train the AdaNCA-equipped model from scratch on ImageNet1K. We conduct the analysis presented in Section 3.3 to decide the optimal position to insert multiple AdaNCA modules, in which the ImageNet1K pre-trained weights for analysis are obtained from the PyTorch Image Models library [69]. To balance between the computational cost and robustness improvement, we limit the number of AdaNCA to two or three, depending on the model architecture. The recurrent time step of all AdaNCA is chosen from $\\{[2,2],[2,4],[3,5]\\}$ , and we follow the design principle of ViT; it is to put more computation in the middle or high layers [38]. We fix the random steps during testing to a single integer chosen from the range to achieve precise results and ensure non-stochasticity during adversarial attacks. All the activation functions used in the MLP in AdaNCA are GELU [29], and the input, hidden, and output dimensionalities of the MLP are all the same. We refer readers to Appendix C.6 for the details of the training and insert scheme of AdaNCA. All of our experiments are performed on four Nvidia A100 GPUs. ", "page_idx": 6}, {"type": "text", "text": "4.1 Results on Image Classification ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We test all models on the ImageNet1K validation set for clean accuracy. For the adversarial robustness evaluation, we choose common adversarial attack methods PGD [41], CW [5], APGD-DLR [9], and APGD-CE [9]. Moreover, we also include the natural adversarial examples in ImageNet-A ", "page_idx": 6}, {"type": "image", "img_path": "BQh1SGvROG/tmp/7b28c8acbf4ad31e43334493fffe41ccb3aef82d428f175816f54eb04eab0e48.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 5: Ablation on the (a) scales and (b) numFigure 4: Pair-wise layer similarities. Layer sets ber of kernels used in our multi-scale Dynamic are marked in red boxes. Swin-B-AdanCA has a Interaction. Overly large scales can undermine clearer stage partition, which might be attributed the performance and so do too many or too few to AdaNCA acting as an information transmitter kernels. We choose ${s=2}$ , $\\mathcal{M}=4$ to balance between different layer sets. between the clean accuracy and robustness. ", "page_idx": 7}, {"type": "text", "text": "(IM-A) [13]. For the PGD attack, we align with the settings used in [42]: max magnitude $\\epsilon=1$ , step size $\\alpha=0.5$ , steps $t=5$ . We refer readers to the Appendix C.7 for details of the other attacks. For testing the OOD generalization, we use ImageNet-C (IM-C) [28], ImageNet-R (IM-R) [30], and ImageNet-Sketch (IM-SK) [67]. We report the mean corruption error $(\\mathrm{mCE})$ on ImageNet-C and accuracy on all other kinds of robustness benchmarks in Table 1. Our results highlight that AdaNCA-enhanced ViTs consistently outperform corresponding baselines in various robustness tests as well as in terms of clean accuracy. Importantly, the enlarged baseline models $\\star$ sign) do not bring comparable improvements to AdaNCA, suggesting that the enhancements do not merely stem from the increase in computational budgets. However, the existing method [23] that introduces local structure into ViTs can potentially undermine the adversarial robustness of the baselines. In Table 2, we conduct an in-depth study on the corruption errors of the different categories of common corruptions in ImageNet-C. The results show that AdaNCA enhances robustness without the trade-off seen in methods that ignore texture information [17, 53]. While these methods may improve mCE for non-Blur noise, they often worsen mCE for Blur noise [53]. In contrast, AdaNCA consistently improves robustness across most categories. We refer readers to Appendix C for more results. ", "page_idx": 7}, {"type": "text", "text": "4.2 Layer similarity structure ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Our key assumption in Section 3.3 is that AdaNCA will not change the layer similarity structure due to its small size, and that is why we use pre-trained networks to conduct this analysis. Here, we examine the pair-wise layer similarities in Swin-B [38] and Swin-B-AdaNCA in Figure 4. $\\kappa_{m e a n}$ is the mean of $\\kappa$ from all layer sets. We refer readers to Appendix C.14 for more results. AdaNCA not only preserves the original layer similarity structure but also contributes to a clearer stage partition, validating our assumption in Section 3.3. The results might be attributed to the fact that AdaNCA transmits information between different layer sets and thus layers inside each set do not bother adapting to the layers outside the set. ", "page_idx": 7}, {"type": "text", "text": "4.3 Ablation studies ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We conduct ablation studies on ImageNet100, a 100-class subset of ImageNet1K. Previous studies [7, 15, 72, 74] have shown that ImageNet100 serves as a representative subset of ImageNet1K. Hence, we can obtain representative results for the self-evaluation of the model while efficiently using the computational resources. All the ablation experiments are based on a Swin-tiny [38] model. We insert it after the fourth layer to obtain the best robustness improvement, according to our analysis in Section 3.3 and Appendix A. First, we ablate on two hyperparameters, the number of convolutional kernels used in the Dynamic Interaction stage $(\\mathcal{M})$ and the maximum scale $(S)$ used in the multi-scale Dynamic Interaction stage. The Clean Accuracy and Attack Failure Rate are shown in Figure 5. The attack failure rate is quantified using the same method as in Section 3.3 and Appendix A. Multi-scale interaction can contribute to the performance while overly large scale can lose local information and complicate the process of selecting the interaction neighbors. This issue is also observed in a previous work [62]. Increasing the number of kernels benefits the performance while too many kernels undermine the robustness. According to the results, we choose ${\\cal S}=2$ , $\\mathcal{M}=4$ . We then perform ablations on several design choices: ", "page_idx": 7}, {"type": "table", "img_path": "BQh1SGvROG/tmp/d555fe355bd3f2f174ed169c584b6736de4f8eb5b178bf8d37d77cc7c473a170.jpg", "table_caption": ["Table 3: Ablation studies on design choices. Each of the AdaNCA design choices contributes to improved performance and robustness. Baseline is a Swin-tiny [38] model trained on ImageNet-100. Bold indicates the best model. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "\u2022 Recurrent update (Recur). Ablation on unrolling the recurrence with average time step $T$ in AdaNCA into $T$ independent AdaNCA with time step being 1.   \n\u2022 Stochastic update (StocU). Ablate the stochastic update during training, leading to globally synchronized update of all tokens [45].   \n\u2022 Random step (RandS). Change the recurrence time step from a randomly chosen integer in range $[T_{1},T_{2}]$ to $\\lceil\\bar{(T_{1}+T_{2})}/2\\rceil$ . It cannot be turned on without recurrence.   \n\u2022 Dynamic interaction (DynIn). Ablate the Dynamic Interaction so that the interaction results are simply summed together. The number of kernels remains the same. ", "page_idx": 8}, {"type": "text", "text": "As shown in Table 3, the highest robustness improvement is achieved with all components. Among them, turning off the recurrence leads to the largest drop in the robustness, as recurrence allows the model to explore more cell states than finishing the update in a single step. While it achieves the highest clean accuracy, it uses ${\\sim}4\\mathbf{x}$ more parameters than our method, and the improvement of the clean performance is likely due to more parameters. Without any of the two sources of randomness, stochastic update and random steps, the model cannot adapt to the variability of the inputs and thus exhibits vulnerabilities against adversarial attacks. Finally, turning off our Dynamic Interaction will cause drops in both clean accuracy and robustness, as tokens cannot decide their unique interaction weights and thus cannot generalize to noisy inputs. We provide extended ablation studies in Section D.2 in the Appendix. ", "page_idx": 8}, {"type": "text", "text": "4.4 Noise sensitivity examination ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "A drawback of adversarially robust models is their increased sensitivity to noise in specific frequency bands [60]. For instance, while adversarially trained models are robust against adversarial attacks, they can be sensitive to noise from a larger frequency-band range compared to standard models [60]. Here, we use the method and data from [60] to examine the noise sensitivity of AdaNCA-enhanced ViTs. Specifically, we evaluate the classification performance on a set of images that are mixed with noise of varying magnitudes and frequencies. The noise is sampled from Gaussian distribution with zero mean and the standard deviation indicates its magnitude. Then, it is filtered within various spatial-frequency bands, resulting in different frequencies of noise. Higher classification accuracy on a specific noise type indicates that the model is less sensitive to that noise. Figure 6 presents the results, including human data sourced from [60]. Our findings demonstrate that AdaNCA enhances ViTs by reducing the sensitivity to noise with certain frequency components, equipping ViTs with more human-like noise-resilient abilities. Crucially, AdaNCA improves the model robustness differently than adversarial training since the AdaNCA-enhanced ViTs do not exhibit increased sensitivity to the noise. We quantitatively validate the conclusion and refer readers to Appendix C.16 for the details. ", "page_idx": 8}, {"type": "text", "text": "5 Limitation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "AdaNCA has certain limitations. First, the AdaNCA-equipped ViTs cannot adapt to unseen recurrent steps of AdaNCA, which limits the generalization ability. For example, if the training step range of AdaNCA is [3,5], it cannot produce meaningful results with the test step being 6. AdaNCA introduces a non-negligible computation into the original architecture. Our experiments are conducted on ImageNet1K with the image size of $224\\,\\times\\,224$ . Whether AdaNCA can lead to impressive improvements on larger-scale problems, e.g., ImageNet22K, remains a question. The size of the images can also affect the efficiency of token interaction. ", "page_idx": 8}, {"type": "image", "img_path": "BQh1SGvROG/tmp/673128fbf8fd1d963b0ae2806bed3b9a4ff9cc1cbd13bd4c893c67f6219ba529.jpg", "img_caption": ["Figure 6: Classification accuracy of humans and ViTs on noisy images. The images are perturbed by Gaussian noise with different standard deviations (Magnitude of Noise) and are filtered with different spatial-frequency bands (Frequency of Noise). AdaNCA improves the accuracy on images with certain types of noise (dotted black boxes), indicating that it makes ViTs less sensitive to them. Quantitative results are presented in Appendix C.16. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "6 Broader Impact ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "AdaNCA contributes to more robust ViTs, facilitating their usage in real-world scenarios. We bridge two powerful models, NCA and ViT, on large-scale image classification, potentially encouraging research dedicated to their synergistic combination in more practical settings. Our findings on AdaNCA improving network redundancy can stimulate more works on architectural robustness in deep learning that involves increasing the redundancy to enhance robustness. In the context of this paper, we believe that AdaNCA does not introduce any significant negative implications. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We have proposed AdaNCA, an efficient Neural Cellular Automata (NCA) that, when inserted between their middle layers, improves ViT performances and robustness against adversarial attacks as well as out-of-distribution inputs. We design our model by connecting NCA and ViT, in terms of token interaction modeling, and we propose Dynamic Interaction to improve the computational efficiency of standard NCA. Exploiting the training strategies and design choices in NCA, i.e., stochastic update, random steps, and multi-scale interaction, we further improve the AdaNCA-enhanced ViTs\u2019 clean accuracy and robustness. To decide the placement of AdaNCA, we propose the Set Cohesion Index that quantifies the network redundancy via layer similarity and conclude that AdaNCA should be inserted between two layer sets that consist of redundant layers. Our results demonstrate that AdaNCA consistently improves ViTs performances and robustness. Evidence suggests that the mechanism by which we obtain improvement reduces the sensitivity of ViTs to certain types of noise and makes the noise-resilient ability of ViTs similar to that of humans. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] The MNIST database of handwritten digits. http://yann.lecun.com/exdb/mnist/. ", "page_idx": 10}, {"type": "text", "text": "[2] Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion, and Matthias Hein. Square attack: a query-efficient black-box adversarial attack via random search. In European Conference on Computer Vision, pages 484\u2013501. Springer, 2020.   \n[3] Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. In International Conference on Machine Learning, pages 274\u2013283. PMLR, 2018.   \n[4] Srinadh Bhojanapalli, Ayan Chakrabarti, Daniel Glasner, Daliang Li, Thomas Unterthiner, and Andreas Veit. Understanding robustness of transformers for image classification. In Proceedings of the IEEE/CVF international conference on computer vision, pages 10231\u201310241, 2021.   \n[5] Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017 IEEE Symposium on Security and Privacy $(S P)$ , pages 39\u201357. IEEE, 2017.   \n[6] Aditya Chattopadhay, Anirban Sarkar, Prantik Howlader, and Vineeth N Balasubramanian. Grad-CAM $^{++}$ : Generalized gradient-based visual explanations for deep convolutional networks. In 2018 IEEE winter conference on applications of computer vision (WACV), pages 839\u2013847. IEEE, 2018.   \n[7] Yixin Cheng, Grigorios Chrysos, Markos Georgopoulos, and Volkan Cevher. Multilinear operator networks. In The Twelfth International Conference on Learning Representations, 2023.   \n[8] Xiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang, and Chunhua Shen. Conditional positional encodings for vision transformers. In The Eleventh International Conference on Learning Representations, 2022.   \n[9] Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks. In International Conference on Machine Learning, pages 2206\u20132216. PMLR, 2020.   \n[10] Zihang Dai, Hanxiao Liu, Quoc V Le, and Mingxing Tan. CoAtNet: Marrying convolution and attention for all data sizes. Advances in Neural Information Processing Systems, 34:3965\u20133977, 2021.   \n[11] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Scaling vision transformers to 22 billion parameters. In International Conference on Machine Learning, pages 7480\u20137512. PMLR, 2023.   \n[12] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A largescale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248\u2013255. IEEE, 2009.   \n[13] Josip Djolonga, Jessica Yung, Michael Tschannen, Rob Romijnders, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Matthias Minderer, Alexander D\u2019Amour, Dan Moldovan, et al. On robustness and transferability of convolutional neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16458\u201316468, 2021.   \n[14] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2020.   \n[15] Arthur Douillard, Alexandre Ram\u00e9, Guillaume Couairon, and Matthieu Cord. DyTox: Transformers for continual learning with dynamic token expansion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9285\u20139295, 2022.   \n[16] St\u00e9phane D\u2019Ascoli, Hugo Touvron, Matthew L Leavitt, Ari S Morcos, Giulio Biroli, and Levent Sagun. ConViT: Improving vision transformers with soft convolutional inductive biases. In International Conference on Machine Learning, pages 2286\u20132296. PMLR, 2021.   \n[17] Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A Wichmann, and Wieland Brendel. Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness. In International Conference on Learning Representations, 2018.   \n[18] Robert Geirhos, J\u00f6rn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, and Felix A Wichmann. Shortcut learning in deep neural networks. Nature Machine Intelligence, 2(11):665\u2013673, 2020.   \n[19] Jacob Gildenblat and contributors. Pytorch library for cam methods. https://github.com/ jacobgil/pytorch-grad-cam, 2021.   \n[20] William Gilpin. Cellular automata as convolutional neural networks. Physical Review E, 100 (3):032402, 2019.   \n[21] Andrey Gromov, Kushal Tirumala, Hassan Shapourian, Paolo Glorioso, and Daniel A Roberts. The unreasonable ineffectiveness of the deeper layers. arXiv preprint arXiv:2403.17887, 2024.   \n[22] Jianyuan Guo, Kai Han, Han Wu, Yehui Tang, Xinghao Chen, Yunhe Wang, and Chang Xu. CMT: Convolutional neural networks meet vision transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12175\u201312185, 2022.   \n[23] Yong Guo, David Stutz, and Bernt Schiele. Robustifying token attention for vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 17557\u2013 17568, 2023.   \n[24] Xing Han, Tongzheng Ren, Tan Nguyen, Khai Nguyen, Joydeep Ghosh, and Nhat Ho. Designing robust transformers using robust kernel density estimation. Advances in Neural Information Processing Systems, 36, 2024.   \n[25] Yena Han, Tomaso A Poggio, and Brian Cheung. System identification of neural systems: If we got it right, would we know? In International Conference on Machine Learning, pages 12430\u201312444. PMLR, 2023.   \n[26] Ali Hassani, Steven Walton, Jiachen Li, Shen Li, and Humphrey Shi. Neighborhood attention transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6185\u20136194, 2023.   \n[27] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.   \n[28] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. In International Conference on Learning Representations, 2018.   \n[29] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (GELUs). arXiv preprint arXiv:1606.08415, 2016.   \n[30] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: A critical analysis of out-of-distribution generalization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8340\u20138349, 2021.   \n[31] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with stochastic depth. In Computer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11\u201314, 2016, Proceedings, Part IV 14, pages 646\u2013661. Springer, 2016.   \n[32] John Kalkhof, Camila Gonz\u00e1lez, and Anirban Mukhopadhyay. Med-NCA: Robust and lightweight segmentation with neural cellular automata. In International Conference on Information Processing in Medical Imaging, pages 705\u2013716. Springer, 2023.   \n[33] John Kalkhof, Arlene K\u00fchn, Yannik Frisch, and Anirban Mukhopadhyay. Frequency-time diffusion with neural cellular automata. arXiv preprint arXiv:2401.06291, 2024.   \n[34] Hoki Kim. Torchattacks: A pytorch repository for adversarial attacks. arXiv preprint arXiv:2010.01950, 2020.   \n[35] Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. Similarity of neural network representations revisited. In International Conference on Machine Learning, pages 3519\u20133529. PMLR, 2019.   \n[36] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \n[37] Yawei Li, Kai Zhang, Jiezhang Cao, Radu Timofte, Michele Magno, Luca Benini, and Luc Van Goo. LocalViT: Analyzing locality in vision transformers. In 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 9598\u20139605. IEEE, 2023.   \n[38] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin Transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10012\u201310022, 2021.   \n[39] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al. Swin Transformer v2: Scaling up capacity and resolution. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12009\u201312019, 2022.   \n[40] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11976\u201311986, 2022.   \n[41] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations, 2018.   \n[42] Xiaofeng Mao, Gege Qi, Yuefeng Chen, Xiaodan Li, Ranjie Duan, Shaokai Ye, Yuan He, and Hui Xue. Towards robust vision transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12042\u201312051, 2022.   \n[43] Alexander Mordvintsev and Eyvind Niklasson. $\\mu$ -NCA: Texture generation with ultra-compact neural cellular automata. arXiv preprint arXiv:2111.13545, 2021.   \n[44] Alexander Mordvintsev, Ettore Randazzo, Eyvind Niklasson, and Michael Levin. Growing neural cellular automata. Distill, 2020. doi: 10.23915/distill.00023. https://distill.pub/2020/growingca.   \n[45] Eyvind Niklasson, Alexander Mordvintsev, and Ettore Randazzo. Asynchronicity in neural cellular automata. In Artificial Life Conference Proceedings 33, volume 2021, page 116. MIT Press One Rogers Street, Cambridge, MA 02142-1209, 2021.   \n[46] Eyvind Niklasson, Alexander Mordvintsev, Ettore Randazzo, and Michael Levin. Selforganising textures. Distill, 6(2):e00027\u2013003, 2021.   \n[47] Maximilian Otte, Quentin Delfosse, Johannes Czech, and Kristian Kersting. Generative adversarial neural cellular automata. arXiv preprint arXiv:2108.04328, 2021.   \n[48] Ehsan Pajouheshgar, Yitao Xu, Tong Zhang, and Sabine S\u00fcsstrunk. DyNCA: Real-time dynamic texture synthesis using neural cellular automata. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20742\u201320751, 2023.   \n[49] Ehsan Pajouheshgar, Yitao Xu, Alexander Mordvintsev, Eyvind Niklasson, Tong Zhang, and Sabine S\u00fcsstrunk. Mesh neural cellular automata. ACM Trans. Graph., 2024. doi: 10.1145/ 3658127. URL https://doi.org/10.1145/3658127.   \n[50] Ehsan Pajouheshgar, Yitao Xu, and Sabine S\u00fcsstrunk. NoiseNCA: Noisy seed improves spatio-temporal continuity of neural cellular automata. arXiv preprint arXiv:2404.06279, 2024.   \n[51] Rasmus Berg Palm, Miguel Gonzalez Duque, Shyam Sudhakaran, and Sebastian Risi. Variational neural cellular automata. In International Conference on Learning Representations 2022, 2022.   \n[52] Zhiliang Peng, Wei Huang, Shanzhi Gu, Lingxi Xie, Yaowei Wang, Jianbin Jiao, and Qixiang Ye. Conformer: Local features coupling global representations for visual recognition. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 367\u2013376, 2021.   \n[53] Xinkuan Qiu, Meina Kan, Yongbin Zhou, Yanchao Bi, and Shiguang Shan. Shape-biased cnns are not always superior in out-of-distribution robustness. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 2326\u20132335, 2024.   \n[54] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and Jon Shlens. Stand-alone self-attention in vision models. Advances in Neural Information Processing Systems, 32, 2019.   \n[55] Ettore Randazzo, Alexander Mordvintsev, Eyvind Niklasson, Michael Levin, and Sam Greydanus. Self-classifying mnist digits. Distill, 2020. doi: 10.23915/distill.00027.002. https://distill.pub/2020/selforg/mnist.   \n[56] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 115:211\u2013252, 2015.   \n[57] Mark Sandler, Andrey Zhmoginov, Liangcheng Luo, Alexander Mordvintsev, Ettore Randazzo, et al. Image segmentation via cellular automata. arXiv preprint arXiv:2008.04965, 2020.   \n[58] Baifeng Shi, Yale Song, Neel Joshi, Trevor Darrell, and Xin Wang. Visual attention emerges from recurrent sparse reconstruction. In International Conference on Machine Learning, pages 20041\u201320056. PMLR, 2022.   \n[59] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1):1929\u20131958, 2014.   \n[60] Ajay Subramanian, Elena Sizikova, Najib Majaj, and Denis Pelli. Spatial-frequency channels, shape bias, and adversarial robustness. Advances in Neural Information Processing Systems, 36, 2024.   \n[61] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In 2nd International Conference on Learning Representations, ICLR 2014, 2014.   \n[62] Mattie Tesfaldet, Derek Nowrouzezahrai, and Chris Pal. Attention-based neural cellular automata. Advances in Neural Information Processing Systems, 35:8174\u20138186, 2022.   \n[63] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through attention. In International Conference on Machine Learning, pages 10347\u201310357. PMLR, 2021.   \n[64] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Information Processing Systems, 30, 2017.   \n[65] Ashish Vaswani, Prajit Ramachandran, Aravind Srinivas, Niki Parmar, Blake Hechtman, and Jonathon Shlens. Scaling local self-attention for parameter efficient visual backbones. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12894\u201312904, 2021.   \n[66] vtddggg. Code repo for: RVT: Towards robust vision transformer. https://github.com/ vtddggg/Robust-Vision-Transformer, 2021.   \n[67] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. Learning robust global representations by penalizing local predictive power. Advances in Neural Information Processing Systems, 32, 2019.   \n[68] Yujing Wang, Yaming Yang, Jiangang Bai, Mingliang Zhang, Jing Bai, Jing Yu, Ce Zhang, Gao Huang, and Yunhai Tong. Evolving attention with residual convolutions. In International Conference on Machine Learning, pages 10971\u201310980. PMLR, 2021.   \n[69] Ross Wightman. Pytorch image models. https://github.com/rwightman/ pytorch-image-models, 2019.   \n[70] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. CvT: Introducing convolutions to vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 22\u201331, 2021.   \n[71] Tete Xiao, Mannat Singh, Eric Mintun, Trevor Darrell, Piotr Doll\u00e1r, and Ross Girshick. Early convolutions help transformers see better. Advances in Neural Information Processing Systems, 34:30392\u201330400, 2021.   \n[72] Shipeng Yan, Jiangwei Xie, and Xuming He. DER: Dynamically expandable representation for class incremental learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3014\u20133023, 2021.   \n[73] Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai, Bin Xiao, Lu Yuan, and Jianfeng Gao. Focal attention for long-range interactions in vision transformers. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 30008\u201330022. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper_files/paper/2021/file/ fc1a36821b02abbd2503fd949bfc9131-Paper.pdf.   \n[74] Tan Yu, Xu Li, Yunfeng Cai, Mingming Sun, and Ping Li. S2-mlp: Spatial-shift mlp architecture for vision. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 297\u2013306, 2022.   \n[75] Kun Yuan, Shaopeng Guo, Ziwei Liu, Aojun Zhou, Fengwei Yu, and Wei Wu. Incorporating convolution designs into visual transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 579\u2013588, 2021.   \n[76] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zi-Hang Jiang, Francis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 558\u2013567, 2021.   \n[77] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12104\u201312113, 2022.   \n[78] Daquan Zhou, Zhiding Yu, Enze Xie, Chaowei Xiao, Animashree Anandkumar, Jiashi Feng, and Jose M Alvarez. Understanding the robustness in vision transformers. In International Conference on Machine Learning, pages 27378\u201327394. PMLR, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Appendix: Table of Contents ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A Establish the correlation between AdaNCA placement and robustness improvement 17 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.1 Dynamic programming for AdaNCA placement 18 ", "page_idx": 15}, {"type": "text", "text": "B Notes on statistical significance 18 ", "page_idx": 15}, {"type": "text", "text": "C ImageNet1K experiments 20 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "C.1 AdaNCA settings for each model . . 20   \nC.1.1 Discussion on the choice of AdaNCA steps 20   \nC.2 Discussion on the model choices . . 21   \nC.3 Discussion on the hyperparameters of MLP inside AdaNCA 21   \nC.4 The effect of AdaNCA placement on robustness on ImageNet1K 21   \nC.5 Details of architecture change in Swin-Base\u22c6and ConViT-Base\u22c6 21   \nC.6 Training details . . . 22   \nC.7 Adversarial attacks 22   \nC.7.1 Choice of adversarial attacks 22   \nC.7.2 Black-box attack . 23   \nC.8 The effect of the range of the steps in random step training 23   \nC.9 Reason for using ImageNet22K model for RVT . . 23   \nC.10 Visualization of attention maps . . . 24   \nC.11 Integrating AdaNCA into pre-trained ViT models . 24   \nC.12 Additional results on comparison with current methods . 25   \nC.13 Additional results on same model architecture but with more layers $\\star$ models) . . 25   \nC.14 Additional results on layer similarity structure . . . 26   \nC.15 Additional results on category-wise mCE on ImageNet-C . . 26   \nC.16 Additional results on noise sensitivity examination 26   \nC.17 Datasets information and license . 28   \nC.18 Model and code license . 28 ", "page_idx": 15}, {"type": "text", "text": "D Extended ablation studies 29 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "D.1 The dropout-like strategy . . 29   \nD.2 Dynamic Interaction 29   \nD.3 Multi-scale Dynamic Interaction 31 ", "page_idx": 15}, {"type": "text", "text": "E Differences between traditional NCA and AdaNCA 31 ", "text_level": 1, "page_idx": 15}, {"type": "table", "img_path": "BQh1SGvROG/tmp/856ede490f151e79bcbbb411d41fb3ea27cde4aa83d6ecb181024831a20dfdbe.jpg", "table_caption": [], "table_footnote": ["Table 4: Hyperparameters used in AdaNCA-equipped ViT training for analysis in Section 3.3 on ImageNet100. "], "page_idx": 16}, {"type": "table", "img_path": "BQh1SGvROG/tmp/b70a227ed6698dfefb46ae60b19d5e66f685a1e7a11c2714158de336f3db95b9.jpg", "table_caption": [], "table_footnote": ["Table 5: Hyperparameters in different adversarial attacks used in the analysis in Section 3.3 on ImageNet100 as well as our ablation studies. "], "page_idx": 16}, {"type": "text", "text": "A Establish the correlation between AdaNCA placement and robustness improvement ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Our motivation for investigating the relationship between AdaNCA and robustness stems from an empirical observation, namely that making neural networks more monolithic contributes to their robustness [28]. One of these monolithic ideas is redundancy in networks. It has been observed that several consecutive layers output similar results [4], hence building what we call \u201ca layer set.\" We hypothesize that AdaNCA, as adaptors inside ViTs, should connect the different sets inside a ViT and transmit information between them. In this way, layers inside a set will no longer bother adapting to other layers outside the set, improving the redundancy and thus robustness. To this end, we propose the layer redundancy quantification $\\kappa$ and network redundancy quantification $\\kappa$ , as well as the robustness improvement measurement $\\gamma$ in Section 3.3. ", "page_idx": 16}, {"type": "text", "text": "For the robustness improvement, we insert AdaNCA into all possible positions of 3 ViT models, Swintiny [38], FAN-small-hybrid [78], and RVT-small-plus [42], and train them along with the baseline models on the ImageNet100 dataset [15, 72, 74], a representative 100-class subset of ImageNet1K. The total amount of models is 34, including 3 baselines and 31 AdaNCA models. All AdaNCAmodels have a $1\\%{-}2\\%$ parameter increase and $5\\%$ more FLOPS. The training hyperparameters are given in Table 4. All models are trained for 300 epochs with the Cosine scheduler. Here, we only consider adversarial robustness for simplification, as it is shown that adversarial robustness is correlated with image corruption robustness within one backbone architecture [42]. We use a white-box version of AutoAttack [9] in the AdaNCA placement analysis in Section 3.3, which comprises PGD [41], CW [5], APGD-DLR [9], and APGD-CE [9]. The hyperparameters of the four attacks are in Table 5. ", "page_idx": 16}, {"type": "text", "text": "We show the qualitative results of our experiments on the Set Cohesion Index and robustness improvement in Figure 7 and the quantitative results in Figure 8. Qualitatively, $\\gamma$ generally follows the trend of $\\kappa$ except for the top 3 layers and the last layer. Specifically, $\\gamma$ for the top three layers consistently exhibits a pattern where it decreases in the second layer and increases in the third layer, regardless of the trend in $\\kappa$ , which always increases. We hypothesize that the proximity of the top 3 layers to the input image causes AdaNCA to similarly impact the model\u2019s robustness by adapting the input to the subsequent layers. Furthermore, the position immediately before the final layer likely serves as a transitional stage for output, adapting to different output strategies (e.g., FAN has an additional class attention head while Swin and RVT use average pooling on all tokens to generate features for classification). The distinct behavior of the last layer compared to other layers has also been noted in previous research [21]. Hence, we exclude the models (AdaNCA applied in the top three layers and before the final layer) from our analysis. The resulting number of models is 19. To conduct a cross-model quantitative comparison, we normalize all $\\gamma$ as well as all $\\kappa$ in a single type of model. Each of the 3 models thus has a set of $\\gamma_{n o r m}\\in[0,1]$ as well as $K_{n o r m}\\in[0,1]$ . We plot all sets of $\\gamma_{n o r m}$ and $\\kappa_{n o r m}$ from 3 models in Figure 8. Note that the coordinate (1.0,1.0) has two overlapping points (Swin-tiny and FAN-small-hybrid), hence only 18 points are visible in the figure. \u03b3norm is significantly correlated with $\\kappa_{n o r m}$ $r=0.6938$ , $p=9.8e-4)$ ), as we report in Section 3.3. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "A.1 Dynamic programming for AdaNCA placement ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Given $\\boldsymbol{\\mathcal{U}}$ AdaNCAs, the ViT is expected to be partitioned into $\\mathcal{U}+1$ sets. The dynamic programming involves filling an array $\\mathcal{D}$ , where $\\mathcal{D}[i][u]$ represent the maximum value of $\\kappa$ achievable by partitioning the first $i$ layers into $u$ sets. The $\\mathcal{D}$ can be obtained via Equation 11. ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{D}[i][u]=m a x(\\mathcal{D}[j][u-1]+\\kappa(j,i-1))\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The boundary conditions are: 1) $\\mathcal{D}[i][1]=\\kappa(1,i)$ , which is partitioning the first $i$ layers into 1 set; 2) $\\mathcal{D}[0][u]=\\overline{{0}}$ . By recording the partition position, we can find $\\boldsymbol{\\mathcal{U}}$ partition points for inserting the AdaNCA. The pseudo-code is presented in Algorithm 1. ", "page_idx": 17}, {"type": "table", "img_path": "BQh1SGvROG/tmp/b48385afc9e4f90d1c248baefc7b71088324579ab0dccf3a0c1d9aed408e3052.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Note that here the layer index starts from 0, and so does the input requirement of $\\kappa$ . ", "page_idx": 17}, {"type": "text", "text": "B Notes on statistical significance ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We do not report the error bar for training models with different seeds on ImageNet1K, as it would be very expensive (Table 10). However, we follow the seed used in the released code for each model. ", "page_idx": 17}, {"type": "image", "img_path": "BQh1SGvROG/tmp/385fdb923804f75aab540dbeed302ff8eaea74068a9dcb0d20c5e9d5121a59ec.jpg", "img_caption": ["Pair-wise layer similarity "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Figure 7: Top: Visualization of layer redundancy $(K(i)=\\kappa(1,i\\!+\\!1)\\!+\\!\\kappa(i\\!+\\!2,L))$ and corresponding robustness improvement $\\gamma$ . Bottom: Visualization of pair-wise layer similarity of the 3 ViTs. ", "page_idx": 18}, {"type": "equation", "text": "$$\nr{=}O.6938,\\,p{<}O.O O1\n$$", "text_format": "latex", "page_idx": 18}, {"type": "image", "img_path": "BQh1SGvROG/tmp/7cf804e904e188be77ed9fdaa68a2220a2bcf7569bfd503a8f75cfc15b513222.jpg", "img_caption": ["Figure 8: The relationship between the layer redundancy $\\kappa$ and robustness improvement $\\gamma$ by inserting AdaNCA in the corresponding position. $\\gamma_{n o r m}$ and $\\kappa_{n o r m}$ are obtained by normalizing $\\gamma$ and $\\kappa$ within the results of each model. Note that the coordinate (1.0,1.0) has two overlapping points (Swin-tiny and FAN-small-hybrid). Quantitatively, $\\gamma_{n o r m}$ is significantly correlated with $\\kappa_{n o r m}$ $(r=0.6938,p<0.001)$ . The linear fit is shown in orange. "], "img_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "BQh1SGvROG/tmp/0118a28da324e5100d83c5e1227aff906545d6c94c19efac72b7d55bac63f80f.jpg", "table_caption": ["Table 6: AdaNCA settings for each ViT. "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "BQh1SGvROG/tmp/7d500620ab6d96a80881bce717bd187935faf7bca97a67418ee8bcfffba96267.jpg", "table_caption": ["Table 7: Increasing the number of recurrent steps of AdaNCA during testing can contribute to the model performance. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "We test the adversarial attack using 5 seeds and find the difference between different seeds negligible (standard deviation: PGD 0.08, CW 0.01, APGD-DLR 0.01, APGD-CE 0.02). As all our results on adversarial attacks have differences larger than 0.1, they are statistically significant. ", "page_idx": 19}, {"type": "text", "text": "For ImageNet100 experiments in the ablation study, we train the model 3 times and find the clean accuracy changes within a small range (standard deviation 0.08), and the robustness test results barely change (standard deviation 0.02). We also test the robustness improvement within one model using 5 seeds and the standard deviation is 0.008. ", "page_idx": 19}, {"type": "text", "text": "For the AdaNCA placement analysis, our correlation result is statistically significant $p<0.001)$ . ", "page_idx": 19}, {"type": "text", "text": "C ImageNet1K experiments ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "C.1 AdaNCA settings for each model ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Based on Algorithm 1, we insert AdaNCA into the four chosen baseline models. The specific settings for each AdaNCA are shown in Table 6. Note that insert position 0 means before the network, since FAN-B model uses a complex patch embedding layer, we can treat it as a unique layer that encodes semantic information rather than the simple convolution patch embedding used in other models. In practice, we find inserting AdaNCA after it can contribute to the model performance. Moreover, we add the drop path operation after each AdaNCA, and the drop path rate follows the one of the layer that AdaNCA follows. Hence, the regularization is pretty strong even without the stochastic update. However, as shown in our ablation studies in Section 4.3, the stochastic update indeed contributes to the model performance. Our FLOPS computation for AdaNCA models are based on Test steps. ", "page_idx": 19}, {"type": "text", "text": "C.1.1 Discussion on the choice of AdaNCA steps ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Our choice of AdaNCA steps differs from all previous NCA models, which typically iterate for hundreds of steps. We make this compromise as the computational costs increase linearly as the step grows. We aim to minimize the increase in the number of parameters and FLOPS for scalability and we do not want the source of improvement to merely stem from the increase in the size and computation of the models. We show that more NCA steps will indeed contribute to the model\u2019s performance in Table 7, while it introduces extra FLOPS. The model is the same as in Section 4.3 which is trained using a range of steps being [3, 5]. We want to underscore that it is the architecture and evolution scheme that defines an NCA as introduced in Section 3.1, instead of the number of its recurrent steps. Admittedly, fewer steps will lead to a coarser path to the target state, and can potentially undermine the model\u2019s performance. Notably, with our scheme, AdaNCA has achieved generally strong robustness improvements compared to the baselines. It indicates that our choice of the recurrent steps of AdaNCA is a good balance between computational costs and performance. ", "page_idx": 19}, {"type": "text", "text": "C.2 Discussion on the model choices ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In our experiments, we choose RVT [42], FAN [78], Swin [38], and ConViT [16]. Our choice not only includes different types of ViTs (Regular, Hybrid, Hierarchical) and robust as well as non-robust architectures (RVT, FAN and Swin, ConViT), but also covers other aspects of ViT characteristics. First, RVT and Swin use average pooling on the final token maps to obtain the 1D feature for classification, while FAN and ConViT adopt a separate class token. It has been proved that those two strategies of classification can have a significant influence on the model robustness [42]. Despite this, AdaNCA are effective in both ways for classification. Moreover, all the architectures already contain certain kinds of local structures. RVT contains depth-wise convolution within MLP and a convolutional patch embedding layer. FAN has a ConvNeXt [40] head for patch embedding. Swin has shifted window attention. ConViT has convolution-attention coupled gated positional self-attention layers. Local structures have been proven effective in improving model robustness [42]. Therefore, we partially eliminate the possibility that AdaNCA is effective simply because of introducing local information in ViTs without any local inductive bias, such as the original ViT [14] or DeiT [63]. As shown in our ablation studies in Section 4.3, AdaNCA not only contributes to more parameterefficient models but also improves the model robustness compared to not using the training strategies or design from NCA. Furthermore, in our choices, Swin does not conduct weight exponential moving average (EMA), while the other 3 models perform. It indicates that AdaNCA can be effective with or without model EMA. ", "page_idx": 20}, {"type": "text", "text": "C.3 Discussion on the hyperparameters of MLP inside AdaNCA ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In AdaNCA, we set the hidden dimensionality of the MLP to the same one as the input dimension. It is different from the common design choice of MLP in ViT that uses a 4-time larger hidden layer than the input layer. Moreover, it also deviates from the design of the previous NCA that uses more than 8 times larger hidden layer size than the input one. Instead, we set the hidden dimensionality to be the same as the input one. The reason is that AdaNCA introduces additional parameters and computation in ViTs and we want the extra computational overhead to be as low as possible. The lowest dimensionality that will not cause inevitable information loss is the same one as the input dimension. Therefore, we design the MLP in AdaNCA as a non-compression-non-expand structure. ", "page_idx": 20}, {"type": "text", "text": "C.4 The effect of AdaNCA placement on robustness on ImageNet1K ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Our algorithm for deciding the placement of AdaNCA is based on the correlation between the robustness improvement and the Set Cohesion Index in Section 3.3. All the experiments are conducted on ImageNet100 to efficiently use the computational resources. Here, we compare two schemes for deciding the placement of AdaNCA on ImageNet1K on Swin-B [38] and FAN-B [78] to demonstrate our result validity. The first scheme is No-Prior, which does not involve the knowledge of the layer similarity and performs the most reasonable placement choice. For Swin-B, the choice for placing 3 AdaNCA is to insert them between each stage pair defined by the transition between different embedding dimensionalities, namely after layer 2, layer 4, and layer 22. Coincidentally, this choice aligns with what we obtain from the dynamic programming algorithm, which is to place AdaNCA after layer 2, layer 8, and layer 22. For FAN-B, although it is a hybrid ViT, we only consider inserting AdaNCA into its main network where all layers share the same structure. In this case, the No-Prior chooses to uniformly insert AdaNCA between the 16 layers, that is after layers 5 and 10. Our algorithm decides the placement should instead be after layers 6 and 9. Table 8 shows the results of two Swin-B-AdaNCA models. Inserting AdaNCA with the No-Prior scheme can still contribute to the model performance and robustness, but not as effective as using our proposed method. ", "page_idx": 20}, {"type": "text", "text": "C.5 Details of architecture change in Swin-Base\u22c6and ConViT-Base\u22c6 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "For Swin-Base\u22c6, we add two extra layers in stage 3 with embedding dimensionality being 512, since in the original design most of the computational budgets are dedicated to stage 3, and two layers form a complete Swin operation. For ConViT-Base , we add an extra Gated Positional Self-Attention (GPSA) layer. ", "page_idx": 20}, {"type": "text", "text": "Table 8: Comparison between different schemes of AdaNCA placement. No-Prior: insert AdaNCA without knowledge of layer similarity structure but making the most reasonable choice, that is to insert them between the architectural stage transition where the embedding dimensionality changes for Hierarchical ViT like Swin [38], or insert them uniformly between layers that have the same structure like the main network in FAN [78]. AdaNCA consistently improves the model performance and robustness compared to the baseline models, while the improvement can be maximized by using our AdaNCA placement scheme based on Algorithm 1. ", "page_idx": 21}, {"type": "table", "img_path": "BQh1SGvROG/tmp/160db41858d9d075a672642320a375edb23a23413f959c80bcf0b670d66ba8e9.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "BQh1SGvROG/tmp/99d369397654c4839109113101fea727ae4a691565e7064c4d2562e6a723e34f.jpg", "table_caption": ["Table 9: Hyperparameters used in AdaNCA-equipped ViT training. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "C.6 Training details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We use different training schemes for the four selected baseline models, closely following the parameter settings for each of them (link to reference configuration files: RVT-B,FAN-B,SwinB,ConViT-B,). Most training settings are in line with DeiT [63] since all models used build the training upon the scheme of training a DeiT but with minor changes. Some specific training settings are listed in Table 9. ", "page_idx": 21}, {"type": "text", "text": "All models are trained for 300 epochs. Our ablation studies on the size of the model (Swin-B-abl, ConViT-B-abl) also follow the same training settings. The detailed time consumption for training each model using 4 Nvidia-A100 80G GPUs is in Table 10. The excessive time of RVT is because of the patch-wise augmentation [42]. ", "page_idx": 21}, {"type": "text", "text": "C.7 Adversarial attacks ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We test all AdaNCA-equipped ViTs using four adversarial attacks based on the code [34]. The detailed setting of each attack is given in Table 11. We try setting the Expectation Over Transformation (EOT) to 3 and the results do not change significantly (For Swin-B, APGD-DLR, EOT ${\\mathrm{=}}1$ : 25.124, EOT $^{=3}$ : 25.121). Hence, we fix the EOT to 1. ", "page_idx": 21}, {"type": "text", "text": "C.7.1 Choice of adversarial attacks ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We choose PGD [41] since it is the most popular method for examining model adversarial robustness after architectural changes [58, 42]. However, it is relatively easy to overcome the PGD attack using obfuscated gradients through methods such as random inference, noisy architecture, or nondifferentiable components [3]. Our AdaNCA does not fulflil the requirement for producing obfuscated gradients since it is fully differentiable and we turn off all randomness during test. However, we still want to examine whether recurrence would result in corrupted gradient information due to the drawback of cross entropy loss [9]. Moreover, the step size in PGD can largely affect the result. Hence, we choose Auto-PGD family [9] to automatically decide the step size and incorporate the new Difference of Logits Ratio (DLR) loss, resulting in APGD-CE and APGD-DLR, respectively. We also want to include an optimization-based adversarial attack and thus select the CW [5] attack. ", "page_idx": 21}, {"type": "table", "img_path": "BQh1SGvROG/tmp/90bd3cb094d885d43ec14c56ef876172d5ffd3e781e094ae2aa21a98697f496f.jpg", "table_caption": ["Table 10: Training time of each AdaNCA-equipped ViT. "], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "BQh1SGvROG/tmp/9cf465971d2a8180d18a5c8ab7569de5299a0a0f9eceb7fb17e991b259a13fa4.jpg", "table_caption": ["Table 11: Hyperparameters in different adversarial attacks. "], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "BQh1SGvROG/tmp/1cfb474222d0e761efd63b98dab94e3a9d3a1cbecb12f9a375e033b57f2b4e6d.jpg", "table_caption": ["Table 12: Results of ViTs and AdaNCA models under Square [2] attack on ImageNet100. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "C.7.2 Black-box attack ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We also consider the black-box attack, specifically Square [2]. However, due to the extreme computational cost of performing Square on ImageNet1K, we instead conduct Square attack to three models, Swin-tiny (Swin-T) [38], FAN-small-hybrid (FAN-S) [78], and RVT-small-plus (RVT-S), on ImageNet100. The three models are used in our AdaNCA analysis in Section 3.3. The maximum magnitude $\\epsilon\\,=\\,6/255$ and the number of queries is 1000. Results are shown in Table 12. The AdaNCA placements are in line with the highest robustness improvement placements demonstrated in Figure 7. ", "page_idx": 22}, {"type": "text", "text": "C.8 The effect of the range of the steps in random step training ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Our random stap training strategy contributes to the model robustness as shown in Table 3. In all of our experiments, we adopt a range of 2 in the random step setting. Here, we examine the effect of increasing the range. We use the same setting as described in Section 4.3. Results are given in Table 13. We can observe that more choices of the recurrent steps worsen the model performance. This might stem from too much noise introduced into the training process which leads to underftiting. Therefore, we only adopt a range of 2 in our experiments. ", "page_idx": 22}, {"type": "text", "text": "C.9 Reason for using ImageNet22K model for RVT ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We use the ImageNet22K-pretrained model for RVT-Base-plus in our main results. This is because we do not find an official release of the ImageNet1K-trained weights. Moreover, our trainable parameter count on the model in the released code differs from the one reported in the original paper (ours: 88.5M, in-paper: 91.8M) [42, 66]. Hence, self-training might deviate from the results in the official paper. We tried to contact the author for an ImageNet1K version of the model but did not succeed. ", "page_idx": 22}, {"type": "table", "img_path": "BQh1SGvROG/tmp/afe171217d4b62a770e88b68846fa5196b06679f205266b4c4e4b180eaf6a7c1.jpg", "table_caption": ["Table 13: Results of Swin-Tiny and the AdaNCA-enhanced models with different ranges of random steps on ImageNet100. "], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "BQh1SGvROG/tmp/e6d311994963d6850c531b9d92a648202bdb5364d03329551c517845e6c52894.jpg", "table_caption": ["Table 14: Integrating AdaNCA with a pre-trained ViT on ImageNet1K. All integration schemes perform worse than training Swin-B-AdaNCA from scratch. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "We test the result of the released model (ImageNet22K version) on PGD attack and get an accuracy of 30.47. In the paper, the authors report this number to be 29.9 on the ImageNet1K model. Hence, we assume that the results of adversarial attacks can be used as a representative, though might be slightly optimistic. Importantly, this does not falsify our claim that TAPADL modification undermines the adversarial robustness of RVT, since TAPADL-RVT already falls short in PGD attack when compared to the ImageNet1K version of RVT-Base-plus. However, for the O.O.D test, ImageNet22K model can differ a lot from the ImageNet1K models, as most data would be counted as in-distribution w.r.t ImageNet22K. Therefore, we use the official values reported in the original paper [42] on those benchmarks. Moreover, for the comparison other than adversarial attacks, we use TAPADL-RVT [23] as a proxy of the RVT model. ", "page_idx": 23}, {"type": "text", "text": "C.10 Visualization of attention maps ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We aim to qualitatively show that AdaNCA helps ViTs perform correct classification when facing noise. Here, we use GradCAM $^{++}$ [6] to visualize the attention map of Swin-Base [38] and Swin-BAdaNCA on the clean images as well as images containing adversarial noise. We use APGD-DLR to generate the adversarial images. Results are shown in Figure 9. We can observe that AdaNCAenhanced Swin model focuses more on the objects, while the baseline model attends to areas unrelated to the object on the images when facing adversarial noise. ", "page_idx": 23}, {"type": "text", "text": "C.11 Integrating AdaNCA into pre-trained ViT models ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In our experiments, we train all models from scratch. Implementing AdaNCA as a plug-and-play module for pre-trained ViT models would certainly improve the training speed. To explore this, we experiment by inserting AdaNCA into a pre-trained Swin-base model on ImageNet1K by 1) freezing all ViT layers; 2) training only the boundary layers, where boundary layers are the layers before and after the insert position of AdaNCA; 3) Finetuning all layers. Results are given in Table 14. None of the schemes perform as well as training from scratch. The results indicate that the current NCA is able to adapt to such a scheme. However, it may struggle to effectively transmit information between two pre-trained ViT layers, as these layers have already established strong connections. In contrast, training the model from scratch allows NCA and ViT to synergistically adapt to feature variability, resulting in better overall performance. However, it is worth exploring in the future since the fine-tuning scheme can contribute to the performance. ", "page_idx": 23}, {"type": "image", "img_path": "BQh1SGvROG/tmp/286de9f838aad6fc8dae1b0e90875e4092f8eb2d8f200898e1d7213c79080929.jpg", "img_caption": ["Figure 9: Visualization of attention maps of Swin-Base [38] and Swin-B-AdaNCA using GradCAM $^{++}$ [6] on clean images and images with adversarial noise. AdaNCA helps ViTs focus more on the object when facing noise. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Table 15: Comparison with ViTCA. ViTCA performs worse in clean accuracy and is on-par with AdaNCA in robustness. Note that ViTCA has more parameters than our scheme Dynamic Interaction. The training setting is the same as in Section 4.3 in the main paper. ", "page_idx": 24}, {"type": "text", "text": "# Params (M) FLOPS (G) Clean Acc.(\u2191) Attack Failure Rate (\u2191 ", "page_idx": 24}, {"type": "table", "img_path": "BQh1SGvROG/tmp/e4f57c3cf8faba9ea02ce4e7077825d6d5d5389d9d5a39b7d9c4be8b005da0a5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "C.12 Additional results on comparison with current methods ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We have presented the comparison between our AdaNCA and the current SOTA method, TAPADL models [23], in Section 4.1. Here, we provide an additional comparison with the ViTCA model [62]. In ViTCA, tokens interact with each other through local self-attention. We aim to compare our proposed Dynamic Interaction module with the interaction learning scheme in ViTCA. Results are given in Table 15. Despite having more parameters, the ViTCA-like scheme performs worse in clean accuracy and is on-par with AdaNCA in robustness. This indicates that it is promising to explore the possibility of incorporating ViTCA into our framework. ", "page_idx": 24}, {"type": "text", "text": "C.13 Additional results on same model architecture but with more layers ( $\\dot{\\star}$ models) ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Due to the computational cost of RVT [42] and FAN [78] (Table 10), we do not train them with more layers and thus do not have a $\\star$ version of these two models. Instead, we train the smaller version of models on ImageNet100. We choose RVT-small-plus (RVT-S) and FAN-small-hybrid (FAN-S) as two baselines, which is in line with our other experiments on ImageNet100. We add one extra layer to RVT-small-plus and one extra layer to the FAN network in FAN-small-hybrid, resulting in RVT- $S^{\\star}$ and FAN- $.S^{\\star}$ . We compare it with our AdaNCA-equipped model where AdaNCA is after layer 9 in RVT-S and after layer 5 in the FAN-S, aligning with Table 12. We use the same AutoAttack as described in Section A to measure the robustness improvement and attack failure rate. Results are shown in Table 16. We can observe that the increase in the number of parameters and FLOPS does not account for the improvement in robustness. ", "page_idx": 24}, {"type": "table", "img_path": "BQh1SGvROG/tmp/e8cbc5ca5f5f0e86be790abdec68a407760c56648a3395a3c9ed7ddcb6f38204.jpg", "table_caption": ["Table 16: Results of different ViTs on ImageNet100. $\\star$ means the same model design but with more layers. The improvement AdaNCA brings does not merely stem from the increase in the number of parameters or FLOPS. "], "table_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "BQh1SGvROG/tmp/986fdcf254e59558a73d117f0817015bfde6df99bc6da0eb7506efc4a26dee03.jpg", "table_caption": ["Table 17: Comparisons of corruption error on each corruption type of ImageNet-C. AdaNCA consistently improves the performance of different ViT models. "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "C.14 Additional results on layer similarity structure ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Here, we show the layer similarity structure for FAN-B-Hybrid [78] and ConViT-B [16] with the ImageNet1K weights and the weights after training with AdaNCA. For RVT-Base-plus, we train it with the released code [66] but with a different number of parameters, as discussed in Section C.9. This training is only for obtaining the layer similarity structure, and we find it stable after 30 epochs. The results are shown in Figure 10. In practice, we ignore the stage partition found by the dynamic programming algorithm in Section A.1 that is after the first layer or before the last layer since we want a stage to contain more than 1 layer. The results further validate our assumption that AdaNCA is used as an adaptor between stages and transmits information between them, making them more and more different from each other. Interestingly, not all AdaNCA contributes to drastic clearer stage partitions. For example, the first AdaNCA in FAN-B is not effective in making the stage as clear as the second one. However, the overall Set Cohesion Index increases, and the layer similarity structures in all 4 networks do not change. We believe the deep learning architecture research can benefti from our results, in that developing more architectural changes which contribute to stage partition in the network and examine the effect on network robustness and generalization. More importantly, we use a single non-parametric metric for quantifying the layer similarities. Introducing advanced output similarity quantification methods might lead to new findings and other fascinating results. ", "page_idx": 25}, {"type": "text", "text": "C.15 Additional results on category-wise mCE on ImageNet-C ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We provide additional results on category-wise mCE on ImageNet-C in Table 17. Our test follows the data transformation used in [42]. ", "page_idx": 25}, {"type": "text", "text": "C.16 Additional results on noise sensitivity examination ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We use the data from [60]. In the experiment, human subjects are asked to classify the noisecontaminated images, and the classification accuracy is recorded. Then, the images are fed into the models trained on ImageNet1K, whose outputs are then transformed into a 16-way ImageNet label [56] and compared against the ground truth labels. Their classification accuracy is also recorded and used for comparison with human results. We present the additional results on FAN-B-Hybrid [78] and the SOTA model TAPADL-RVT [23] compared to the AdaNCA-equipped model in Figure 11. ", "page_idx": 25}, {"type": "image", "img_path": "BQh1SGvROG/tmp/040b14c5f8ca0d766c6982a292c6edb0d0387f65de8f5942550c0237ce3d3791.jpg", "img_caption": ["Figure 10: Layer similarity structures of different ViTs. Layer set marked in red boxes. AdaNCA is inserted between each pair of stages. $\\kappa_{m e a n}$ is the mean of $\\kappa$ of all stages marked out. "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "BQh1SGvROG/tmp/65fb228324993ee87dc84f6cef75c9f2651a9409a75a82f2e10fa8db46a03998.jpg", "img_caption": ["Figure 11: Noise sensitivity examination on humans and ViTs. AdaNCA contributes to more humanlike noise-resilient ability and reduces the sensitivity of ViTs on certain types of noise (yellow boxes). "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "Moreover, we adopt a quantitative metric on the test. In the original experiment [60], a Gaussian curve is fit based on the discretized accuracy map. However, such a method ignores the continuous change in the classification pattern. Hence, we propose a simple metric that examines the similarity between the accuracy map of humans and of models. Specifically, given the ground truth accuracy map (human performance) $\\mathcal{A}_{g t}$ , and model accuracy map $A_{m}$ , the similarity $\\Gamma$ is defined as: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\Gamma=100-\\frac{1}{N}\\sum_{\\epsilon,f}|A_{m}(\\epsilon,f)-A_{g t}(\\epsilon,f)|\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "\u03f5, $f$ represent the magnitude and frequency of the noise, respectively. A higher $\\Gamma$ indicates that the model can achieve more similar accuracy maps with humans averaged across all noisy images. $N$ is the total number of types of noise added. Since all models as well as humans perform similarly on low-magnitude noise, we ignore the first two noise levels ( $\\epsilon=0,0.02)$ . Hence, in our experiments, ", "page_idx": 26}, {"type": "text", "text": "Table 18: Quantitative results on the frequency preference examination experiments. We report the similarity of accuracy maps between model results and humans. ", "page_idx": 27}, {"type": "table", "img_path": "BQh1SGvROG/tmp/a2cd1015b924442db2d8c0ebedbd18b28cde512e02e0a241d7de6a0c54d99ad9.jpg", "table_caption": [], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "$N\\,=\\,21$ . We report $\\Gamma$ in Table 18. We also include the results of ResNet50 [27] as well as the L2-adversarially-trained version [60] (ResNet50-Adv, trained with L2-bounded adversarial noise where the maximum magnitude of noise is 0.25), and present the visualizations of the accuracy maps of those two models in Figure. Critically, our results indicate that the adversarially trained model cannot lead to improved accuracy similarities, resulting in a less human-like noise-resilient ability. This is in line with the conclusion of the original work [60], validating our proposed metric. Contrary to adversarial training, we obtain more human-like decision patterns than the compared models, validating our claim in Section 4.4. ", "page_idx": 27}, {"type": "text", "text": "C.17 Datasets information and license ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "\u2022 ImageNet1K [12]. This dataset contains 1.28M training images and 50000 images for validation. We report the top1 accuracy on the 50000 validation images. License: Custom (research, noncommercial). ", "page_idx": 27}, {"type": "text", "text": "\u2022 ImageNet-C [28]. This dataset contains 15 types of 2D image corruption types that are generated by different algorithms. The metric on this dataset is mean corruption error, whose lower value represents a more robust model against those corrupted images. License: CC BY 4.0.   \n\u2022 ImageNet-A [13]. This dataset contains naturally existing adversarial examples that can drastically decrease the accuracy of ImageNet1K-trained CNNs. It is a 200-class subset of the ImageNet1K dataset. License: MIT license.   \n\u2022 ImageNet-R [30]. This dataset contains different artistic renditions of 200 classes from the original ImageNet object classes. The original ImageNet dataset discourages non-real-world images, and thus the artistic renditions render the images to be O.O.D. License: MIT license.   \n\u2022 ImageNet-SK [67]. This dataset contains 50000 images with 1000 classes that match the validation set of the original ImageNet dataset. All the images are black-and-white sketches instead of real-world photographs of the object class. License: MIT license. ", "page_idx": 27}, {"type": "text", "text": "C.18 Model and code license ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "\u2022 Swin-B [38]: MIT License.   \n\u2022 FAN-B [78]: Nvidia Source Code License-NC.   \n\u2022 ConViT-B [16]: Apache 2.0 License.   \n\u2022 RVT [42]: Apache 2.0 License.   \n\u2022 TAPADL [23]: MIT License.   \n\u2022 Code for adversarial attacks [34]: MIT License.   \n\u2022 PyTorch Image Model [69]: Apache 2.0 License.   \n\u2022 Code for GradCAM $^{++}$ [19]: MIT License. ", "page_idx": 27}, {"type": "text", "text": "Table 19: Stochasticity during testing will give a false sense of adversarial robustness. We conduct the same experiments twice, which result in Trials 1 and 2. The training setting is the same as in Section 4.3 in the main paper. ", "page_idx": 28}, {"type": "table", "img_path": "BQh1SGvROG/tmp/3ba081b1c3f0605bb145bffa78048ff55d7aba73abd94e0a800588559677e546.jpg", "table_caption": ["Clean Acc. (\u2191) CW Acc. (\u2191) "], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "Table 20: Removing $\\frac{1}{p}$ in Equation 7 during training and directly testing with synchronized update.   \nOur dropout-like compensation scheme improves AdaNCA\u2019s performance. ", "page_idx": 28}, {"type": "table", "img_path": "BQh1SGvROG/tmp/2a43269ab1c757e7c19c0cbd4b8e793104189d12067f523ff7e71aa122bca1d3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "D Extended ablation studies ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In this section, we discuss the design of our AdaNCA and provide additional ablation studies on the designs of AdaNCA. ", "page_idx": 28}, {"type": "text", "text": "D.1 The dropout-like strategy ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In Section 3.1.1, we introduce our approach of using the dropout-like strategy to perform the stochastic update during training and testing. While previous NCA works claim that the stochasticity can be preserved [44, 46] or can be simply switched off [62] during testing, the proposed dropout-like scheme is necessary in our case. First, We highlight that stochasticity during testing can hinder the evaluation of adversarial robustness by producing obfuscated gradients [3], leading to the circumvention of adversarial attacks. Table 19 illustrates this issue, where we test the classification accuracy under CW attack. Stochasticity also results in inconsistent outputs. This is problematic for practical image classification tasks where reliable output is critical, unlike applications focusing on visual effects [46, 62] or collective behaviors [55]. The change in clean accuracy in Table 19 indicates that given the same image, the stochasticity can lead to different decisions, hindering the deployment of the trained models in real-world scenarios. To further demonstrate the necessity of our scheme, we remove the compensation of the output magnitude during training, i.e., we remove the $\\frac{1}{p}$ in Equation 7 during training and directly test the model with a synchronized update. The results are given in Table 20. The drop in clean accuracy and robustness suggests that downstream ViT layers struggle with varying NCA output magnitudes, indicating the usefulness of our proposed dropout-like method. ", "page_idx": 28}, {"type": "text", "text": "D.2 Dynamic Interaction ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Our motivation for developing the Dynamic Interaction module is the high dimensionality of feature vectors in modern ViT models. We underscore that any operations involving linear transformations of the concatenated interaction results will lead to drastic increases in computational costs, as shown in Table 21. Note that the concatenation scheme nearly doubles the FLOPS for RVT compared to the baseline, leading to difficulties in training. Such an increase renders scalability a challenge. Admittedly, more parameters can contribute to better performance, as shown in Table 22. Note, however, that our Dynamic Interaction achieves $70\\%$ of the improvements of the original NCA concatenation scheme (10.06/14.62) in robustness improvement and $60\\%$ improvement in the clean accuracy $(0.62/1.06)$ , with merely $10\\%$ of the parameters and FLOPS (0.35/2.99 for # Params and $0.2/2.3$ for FLOPS). Therefore, our Dynamic Interaction scheme provides a good trade-off between the performance and computational costs. It is capable of scaling up, allowing us to insert AdaNCA into even larger models, such as current vision-language models where the token dimensionality is even higher. Moreover, we qualitatively showcase in Figure 12, that Dynamic Interaction helps robustify AdaNCA when facing noisy inputs. ", "page_idx": 28}, {"type": "text", "text": "Table 21: The concatenation scheme of the original NCA results in a large increase in the computational costs when the token dimensionality is high, leaving scaling up to large datasets and model sizes a challenge. ", "page_idx": 29}, {"type": "table", "img_path": "BQh1SGvROG/tmp/6d33a7a7b07a0a0b2f8b2ef7d86286367ec437e96b58ba30fcb0c342da547610.jpg", "table_caption": ["# Params (M) FLOPS (G) "], "table_footnote": [], "page_idx": 29}, {"type": "text", "text": "Table 22: Performance of using our Dynamic Interaction and the concatenation scheme of original NCA on ImageNet100. Numbers in parentheses indicate performance improvement compared to the Baseline. Our method achieves a large portion of the performance improvement $(60\\%\\,70\\%)$ brought by the concatenation scheme with much fewer computational costs ( $10\\%)$ . ", "page_idx": 29}, {"type": "table", "img_path": "BQh1SGvROG/tmp/bb678a1d0f941ae06ff83e4c68ffe60d8b5b03728646f58d06726955d45a6be4.jpg", "table_caption": [], "table_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "BQh1SGvROG/tmp/1818b69331ae67fb1cd895626488c4a6acafb878d91e113063c93b77e31f1c1a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "Figure 12: Visualization of the developing token maps of AdaNCA using PCA. Without Dynamic Interaction, AdaNCA cannot recover from the noisy adversarial inputs. Equipped with it, the model can stick to the evolution path similar to the clean inputs. ", "page_idx": 29}, {"type": "text", "text": "Table 23: Overly large neighborhood size during token interaction will undermine the model performance as it complicates the process of figuring out which neighbors to interact with and adds too much noise during the process. While increasing the model capacity can alleviate such an issue, AdaNCA still struggles to handle too-noisy information when the scale is too high. Note that the larger filter scheme achieves on-par results with our method when the scale is 2, indicating that it does not provide additional useful information for token interaction at this scale. ", "page_idx": 30}, {"type": "table", "img_path": "BQh1SGvROG/tmp/110122058d4995586e82eec01dac33cb51ea692b8b303d832877cc7aa42f0598.jpg", "table_caption": [], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "D.3 Multi-scale Dynamic Interaction ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Our motivation for developing multi-scale interaction is to perform more efficient token interaction learning over local scales since ViT already contains global information. Enlarging the neighborhood size will: 1) Complicate the process of selecting the neighbors to interact with; 2) Introduce excessive noise, making it difficult for tokens to accurately acquire neighbor information. 3) Repeatedly acquire the global information provided by ViT. The recurrence further amplifies the noise. In our ablation study in Section 4.3, we notice that increasing the number of scales to 3 will undermine the performance. Such an issue is also observed in previous work [62] where local self-attention is used for interaction learning. While in theory both Dynamic Interaction and self-attention can discount far-away information as the tokens can decide their own interaction neighborhood, overly large neighborhood size can lead to the problems mentioned above. Note that our model gains performance when ${S=2}$ ( $5\\times5$ neighborhood), indicating the usefulness of our multi-scale module. We further explore whether the multi-scale issue can be solved by increasing the model capacity. Specifically, we develop two additional schemes. The first is to replace the dilation in Dynamic Interaction by simply using larger fliters. The other one enlarges the receptive field of the weight computation network $\\mathcal{W}_{M}$ in Equation 9. Instead of using two $3\\times3$ convolution layers, we change the first layer to be a $5\\times5$ convolution. As a result, the receptive field of $\\mathcal{W}_{M}$ becomes $7\\times7$ , matching the one when $S=3$ . We use the same training setting as in Section 4.3. Results are given in Table 23. Although increasing model capacity might alleviate the noise issues, AdaNCA struggles with overly large neighborhoods. ", "page_idx": 30}, {"type": "text", "text": "E Differences between traditional NCA and AdaNCA ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "While AdaNCA is inspired by NCA, it does not fully exploit the designs of traditional NCA. We discuss three key differences between AdaNCA and NCA below: ", "page_idx": 30}, {"type": "text", "text": ". Different initial states. NCA in image generation starts from either constant or randomly initialized cell states, typically referred to as the seed states, while AdaNCA receives the outputs from previous ViT layers. Hence, AdaNCA handles structured inputs at the very beginning. ", "page_idx": 30}, {"type": "text", "text": "2. Different usage of cell states. Traditional NCA does not use all cell states for accomplishing the downstream tasks. After certain steps of evolution, a subset of the cell states is extracted to perform the given task. The unused cell states, termed hidden states, facilitate cell communications as they can be used to store additional information [44]. The dimensionality of the hidden states is typically several times that of the input. When the cell dimensionality is high, as is the case in ViT, adding too many hidden states will bring too much computational costs. Moreover, cells can store effective enough information when they have high dimensionality. Therefore, we do not adopt the hidden states design in AdaNCA.   \n3. No pooling strategy. A critical component of traditional NCA is the pooling strategy. Instead ", "page_idx": 30}, {"type": "text", "text": "of starting from the seed states in every epoch, NCA fetches the starting states from a pool, where the output states from previous epochs are stored. Through this strategy, NCA can explore much longer time steps without suffering from gradients or memory issues. While it is tempting to incorporate the pooling trick into our method, two major concerns hinder its practical implementation: 1) The previous NCA models start from their own outputs in the pooling strategy. In other words, the cell states in the pool are generated by the model itself. However, AdaNCA starts from the outputs of a ViT layer. Such a difference renders the pool trick hard to implement. 2) Taking a step back, even if there is a well-designed pooling strategy for AdaNCA, it will bring too much computational costs during testing. A single step of AdaNCA introduces non-trivial FLOPS during testing, as shown in Table 1. The ultimate goal of the pooling strategy is to ensure the stability of NCA in large time steps, while large time steps will slow down the inference. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Our claim on AdaNCA improving ViT\u2019s robustness is validated in Section 4. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 32}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: The limitation of our work is discussed in Section 5. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 32}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: We do not have theory assumptions and proofs in the paper. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 33}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We disclose all architecture and training details in Section 3.2, 4, C.1 and C.6. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 33}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 33}, {"type": "text", "text": "Answer: [No] ", "page_idx": 34}, {"type": "text", "text": "Justification: We are preparing the code release and will publish the code soon. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 34}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We disclose all training details in Section 4, C.1 and C.6. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 34}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: Please see Section B in the Appendix. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We provide the hardware and time consumption for our experiments in Section C.6 in the Appendix. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 35}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We have read the NeurIPS Code of Ethics and strictly follow it during the research. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 35}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: Please see Section 6. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: The paper does not pose such risks. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 36}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: Please see Section C.17 and C.18 in the Appendix. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 36}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 37}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 37}, {"type": "text", "text": "Answer: [No] ", "page_idx": 37}, {"type": "text", "text": "Justification: We do not include code or model release in the paper, but we are preparing it for a full publication. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 37}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: The paper does not have crowdsourcing and we do not conduct research with human subjects. ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 37}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: The paper does not have crowdsourcing and we do not conduct research with human subjects. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 37}]