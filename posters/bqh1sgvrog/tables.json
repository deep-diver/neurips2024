[{"figure_path": "BQh1SGvROG/tables/tables_6_1.jpg", "caption": "Table 1: The performance of AdaNCA-enhanced ViTs and their corresponding baselines. We report the mCE for IM-C (lower is better) and accuracy for other benchmarks. AdaNCA consistently improves the clean accuracy as well as the robustness to adversarial and out-of-distribution inputs of the baseline models. Note that our models also outperform the larger baselines (* sign), indicating that the performance improvement does not merely originate from the increase in the number of parameters or FLOPS. The TAPADL method [23] can lead to more vulnerable models compared to the baselines (green numbers). Bold indicates the best model. \u2020: the test is conducted on models pre-trained on ImageNet22K [42], see Appendix C.9.", "description": "This table presents a comparison of the performance of several Vision Transformer (ViT) models, both with and without the AdaNCA adaptor.  It shows clean accuracy, as well as robustness against various adversarial attacks and out-of-distribution (OOD) image corruptions (ImageNet-A, ImageNet-C, ImageNet-R, ImageNet-Sketch).  The results demonstrate that AdaNCA consistently improves the robustness and clean accuracy of the base ViT models, and that these improvements are not simply due to an increase in model size or computational complexity.", "section": "4 Results on Image Classification"}, {"figure_path": "BQh1SGvROG/tables/tables_6_2.jpg", "caption": "Table 2: Comparisons of corruption error on each corruption type of ImageNet-C. AdaNCA consistently improves the performance of the baseline model in all categories (Swin-B), and can achieve better results in most categories compared to the SOTA method (TAPADL-RVT). Bold indicates the best model. Lower is better.", "description": "This table presents a comparison of the mean corruption error (mCE) achieved by different models on the ImageNet-C dataset.  The dataset contains various types of image corruptions (noise, blur, weather, and digital).  The table compares the baseline Swin-B model with Swin-B enhanced with AdaNCA and the state-of-the-art (SOTA) TAPADL-RVT model, both with and without AdaNCA. Lower mCE values indicate better robustness to image corruptions.", "section": "4 Results on Image Classification"}, {"figure_path": "BQh1SGvROG/tables/tables_8_1.jpg", "caption": "Table 3: Ablation studies on design choices. Each of the AdaNCA design choices contributes to improved performance and robustness. Baseline is a Swin-tiny [38] model trained on ImageNet-100. Bold indicates the best model.", "description": "This table presents the results of ablation studies conducted on a Swin-tiny model trained on ImageNet-100.  It examines the impact of individual components of the AdaNCA architecture on both accuracy and robustness against adversarial attacks.  The experiment systematically removes features (Recurrent update, Stochastic update, Random step, Dynamic Interaction) one at a time to assess their contribution. The table shows parameters, FLOPs, clean accuracy, and adversarial attack failure rate for each ablation experiment, highlighting the importance of each component in achieving improved performance and robustness. The \"Ours\" row represents the complete AdaNCA model.", "section": "4.3 Ablation studies"}, {"figure_path": "BQh1SGvROG/tables/tables_16_1.jpg", "caption": "Table 4: Hyperparameters used in AdaNCA-equipped ViT training for analysis in Section 3.3 on ImageNet100.", "description": "This table presents the hyperparameters used for training the AdaNCA-enhanced Vision Transformers (ViTs) on the ImageNet100 dataset.  The hyperparameters are crucial for the model's performance and are optimized during the analysis in Section 3.3 of the paper. The table includes parameters such as learning rate, batch size, model EMA decay, stochastic depth, random erase probability, gradient clipping, MixUp, label smoothing, minimum learning rate, and weight decay. Each hyperparameter has a specific value assigned for each of the three different ViT architectures used in the analysis (RVT-S-AdaNCA, FAN-S-AdaNCA, and Swin-T-AdaNCA).", "section": "4 Experiments"}, {"figure_path": "BQh1SGvROG/tables/tables_16_2.jpg", "caption": "Table 5: Hyperparameters in different adversarial attacks used in the analysis in Section 3.3 on ImageNet100 as well as our ablation studies.", "description": "This table lists the hyperparameters used for four different adversarial attacks (PGD, CW, APGD-DLR, APGD-CE) in the experiments.  The hyperparameters are used for both the main analysis in Section 3.3 (ImageNet100) and the ablation studies. Each attack has specific parameters related to its methodology, including maximum magnitude, steps, step size, etc.", "section": "4 Experiments"}, {"figure_path": "BQh1SGvROG/tables/tables_17_1.jpg", "caption": "Table 1: The performance of AdaNCA-enhanced ViTs and their corresponding baselines. We report the mCE for IM-C (lower is better) and accuracy for other benchmarks. AdaNCA consistently improves the clean accuracy as well as the robustness to adversarial and out-of-distribution inputs of the baseline models. Note that our models also outperform the larger baselines (* sign), indicating that the performance improvement does not merely originate from the increase in the number of parameters or FLOPS. The TAPADL method [23] can lead to more vulnerable models compared to the baselines (green numbers). Bold indicates the best model. \u2020: the test is conducted on models pre-trained on ImageNet22K [42], see Appendix C.9.", "description": "This table presents a comparison of the performance of Vision Transformers (ViTs) enhanced with AdaNCA against their baselines across various image classification benchmarks.  It shows clean accuracy, performance under various adversarial attacks (PGD, CW, APGD-DLR, APGD-CE), and robustness to out-of-distribution (OOD) inputs using ImageNet-A, ImageNet-C, ImageNet-R, and ImageNet-SK datasets.  The table highlights AdaNCA's consistent improvement in both clean accuracy and robustness without a significant increase in model parameters or computational cost, surpassing even larger baseline models in several cases.  It also notes that a competing method, TAPADL, may show reduced robustness compared to its baseline.", "section": "4 Results on Image Classification"}, {"figure_path": "BQh1SGvROG/tables/tables_19_1.jpg", "caption": "Table 1: The performance of AdaNCA-enhanced ViTs and their corresponding baselines. We report the mCE for IM-C (lower is better) and accuracy for other benchmarks. AdaNCA consistently improves the clean accuracy as well as the robustness to adversarial and out-of-distribution inputs of the baseline models. Note that our models also outperform the larger baselines (* sign), indicating that the performance improvement does not merely originate from the increase in the number of parameters or FLOPS. The TAPADL method [23] can lead to more vulnerable models compared to the baselines (green numbers). Bold indicates the best model. \u2020: the test is conducted on models pre-trained on ImageNet22K [42], see Appendix C.9.", "description": "This table presents a comparison of the performance of AdaNCA-enhanced Vision Transformers (ViTs) against their baselines across various benchmarks, including clean accuracy, adversarial attacks (PGD, CW, APGD-DLR, APGD-CE), and out-of-distribution (OOD) inputs (ImageNet-A, ImageNet-C, ImageNet-R, ImageNet-SK).  It demonstrates that AdaNCA consistently improves both clean accuracy and robustness without a significant increase in parameters or FLOPS, outperforming even larger versions of the base ViT models in many cases. The table also highlights that a state-of-the-art (SOTA) robustness method (TAPADL) can sometimes lead to models that are less robust than the baselines.", "section": "4 Results on Image Classification"}, {"figure_path": "BQh1SGvROG/tables/tables_19_2.jpg", "caption": "Table 7: Increasing the number of recurrent steps of AdaNCA during testing can contribute to the model performance.", "description": "This table presents an ablation study on the number of recurrent steps used during testing for AdaNCA.  It shows that increasing the number of steps from 4 to 5 slightly improves the clean accuracy and attack failure rate on the ImageNet100 dataset. The results suggest that using more steps could potentially enhance model performance but at the cost of increased computational cost. ", "section": "4.3 Ablation studies"}, {"figure_path": "BQh1SGvROG/tables/tables_21_1.jpg", "caption": "Table 1: The performance of AdaNCA-enhanced ViTs and their corresponding baselines. We report the mCE for IM-C (lower is better) and accuracy for other benchmarks. AdaNCA consistently improves the clean accuracy as well as the robustness to adversarial and out-of-distribution inputs of the baseline models. Note that our models also outperform the larger baselines (* sign), indicating that the performance improvement does not merely originate from the increase in the number of parameters or FLOPS. The TAPADL method [23] can lead to more vulnerable models compared to the baselines (green numbers). Bold indicates the best model. \u2020: the test is conducted on models pre-trained on ImageNet22K [42], see Appendix C.9.", "description": "This table presents a comparison of the performance of vision transformers (ViTs) with and without the AdaNCA adaptor, across various robustness benchmarks.  It shows clean accuracy, performance under several adversarial attacks, and robustness to out-of-distribution inputs (ImageNet-C). The results demonstrate that AdaNCA consistently improves both clean accuracy and robustness without a significant increase in model size or computational cost.  The table also includes a comparison with a state-of-the-art (SOTA) method (TAPADL) that uses an additional loss function for robustness, showing that AdaNCA achieves comparable or better results without requiring that additional loss.", "section": "4 Results on Image Classification"}, {"figure_path": "BQh1SGvROG/tables/tables_21_2.jpg", "caption": "Table 4: Hyperparameters used in AdaNCA-equipped ViT training for analysis in Section 3.3 on ImageNet100.", "description": "This table lists the hyperparameters used for training the Vision Transformers (ViTs) enhanced with AdaNCA (Adaptor Neural Cellular Automata) on the ImageNet100 dataset.  The hyperparameters are categorized and listed for each of the four different ViT models used in the analysis: RVT-S-AdaNCA, FAN-S-AdaNCA, Swin-T-AdaNCA, and ConViT-B-AdaNCA. The hyperparameters cover various aspects of the training process, including the learning rate, batch size, model exponential moving average (EMA) decay, stochastic depth, random erase probability, gradient clipping, MixUp, label smoothing, minimum learning rate, and weight decay.  The table provides detailed settings used to conduct the experiments and analysis presented in Section 3.3 of the research paper, focusing on the impact of AdaNCA placement on model robustness.", "section": "4 Experiments"}, {"figure_path": "BQh1SGvROG/tables/tables_21_3.jpg", "caption": "Table 1: The performance of AdaNCA-enhanced ViTs and their corresponding baselines. We report the mCE for IM-C (lower is better) and accuracy for other benchmarks. AdaNCA consistently improves the clean accuracy as well as the robustness to adversarial and out-of-distribution inputs of the baseline models. Note that our models also outperform the larger baselines (* sign), indicating that the performance improvement does not merely originate from the increase in the number of parameters or FLOPS. The TAPADL method [23] can lead to more vulnerable models compared to the baselines (green numbers). Bold indicates the best model. \u2020: the test is conducted on models pre-trained on ImageNet22K [42], see Appendix C.9.", "description": "This table presents a comparison of the performance of AdaNCA-enhanced Vision Transformers (ViTs) against their baselines across various image classification benchmarks.  It demonstrates AdaNCA's effectiveness in improving both clean accuracy and robustness against adversarial attacks and out-of-distribution data, without significantly increasing the model's size or computational cost. The results highlight AdaNCA's consistent performance improvements across different ViT architectures.", "section": "4 Results on Image Classification"}, {"figure_path": "BQh1SGvROG/tables/tables_22_1.jpg", "caption": "Table 1: The performance of AdaNCA-enhanced ViTs and their corresponding baselines. We report the mCE for IM-C (lower is better) and accuracy for other benchmarks. AdaNCA consistently improves the clean accuracy as well as the robustness to adversarial and out-of-distribution inputs of the baseline models. Note that our models also outperform the larger baselines (* sign), indicating that the performance improvement does not merely originate from the increase in the number of parameters or FLOPS. The TAPADL method [23] can lead to more vulnerable models compared to the baselines (green numbers). Bold indicates the best model. \u2020: the test is conducted on models pre-trained on ImageNet22K [42], see Appendix C.9.", "description": "This table presents a comparison of the performance of AdaNCA-enhanced Vision Transformers (ViTs) against their baselines across various benchmarks.  It shows clean accuracy, performance under different adversarial attacks (PGD, CW, APGD-DLR, APGD-CE), and robustness against out-of-distribution (OOD) inputs (ImageNet-A, ImageNet-C, ImageNet-R, ImageNet-SK).  The table highlights AdaNCA's consistent improvement in both clean accuracy and robustness, demonstrating that the enhancements are not solely due to increased parameters or FLOPS.  It also shows that AdaNCA outperforms even larger baseline models in some cases and that a competing method (TAPADL) can sometimes yield less robust models.", "section": "4 Results on Image Classification"}, {"figure_path": "BQh1SGvROG/tables/tables_22_2.jpg", "caption": "Table 12: Results of ViTs and AdaNCA models under Square [2] attack on ImageNet100.", "description": "This table presents the results of applying the Square attack, a black-box adversarial attack, on three different Vision Transformer (ViT) models: Swin-Tiny, FAN-Small, and RVT-Small.  It compares the performance of the original ViT models against versions enhanced with AdaNCA (Adaptor Neural Cellular Automata) at different insertion points. The metrics shown are Clean Accuracy (the accuracy on clean images), Square Attack Failure Rate, and Square Attack Failure Rate, indicating the robustness of the models against the attack.", "section": "4.1 Results on Image Classification"}, {"figure_path": "BQh1SGvROG/tables/tables_23_1.jpg", "caption": "Table 13: Results of Swin-Tiny and the AdaNCA-enhanced models with different ranges of random steps on ImageNet100.", "description": "This table presents the results of experiments conducted on the ImageNet100 dataset to evaluate the impact of varying the range of recurrent time steps in the AdaNCA model on the model's performance.  The table shows the clean accuracy and the attack failure rates for different ranges of random steps, offering insights into the optimal range for balancing performance and robustness.", "section": "4.3 Ablation studies"}, {"figure_path": "BQh1SGvROG/tables/tables_23_2.jpg", "caption": "Table 14: Integrating AdaNCA with a pre-trained ViT on ImageNet1K. All integration schemes perform worse than training Swin-B-AdaNCA from scratch.", "description": "This table presents the results of integrating AdaNCA into a pre-trained Swin-Base ViT on ImageNet1K.  Three different training strategies were used: freezing all ViT layers, training only the boundary layers (those immediately before and after AdaNCA insertion), and fine-tuning all layers.  The results show that training the model from scratch with AdaNCA consistently yields superior performance compared to integrating AdaNCA into pre-trained models.", "section": "4.1 Results on Image Classification"}, {"figure_path": "BQh1SGvROG/tables/tables_24_1.jpg", "caption": "Table 15: Comparison with ViTCA. ViTCA performs worse in clean accuracy and is on-par with AdaNCA in robustness. Note that ViTCA has more parameters than our scheme Dynamic Interaction. The training setting is the same as in Section 4.3 in the main paper.", "description": "This table compares the performance of AdaNCA with ViTCA, another method that introduces local structures into vision transformers.  The comparison focuses on clean accuracy and robustness against adversarial attacks (using the same training settings as in Section 4.3 of the paper).  Key metrics include the number of parameters, FLOPs (floating-point operations), clean accuracy, and attack failure rate.  The results indicate that while ViTCA has a higher parameter count, AdaNCA achieves comparable or better robustness with fewer parameters.", "section": "C.12 Additional results on comparison with current methods"}, {"figure_path": "BQh1SGvROG/tables/tables_25_1.jpg", "caption": "Table 16: Results of different ViTs on ImageNet100. * means the same model design but with more layers. The improvement AdaNCA brings does not merely stem from the increase in the number of parameters or FLOPS.", "description": "This table compares the performance of several Vision Transformer (ViT) models on the ImageNet100 dataset.  The models include baseline ViTs (Swin-B, FAN-B, RVT-B, ConViT-B) and their corresponding versions with more layers (*).  The table also shows results for versions of these models enhanced with AdaNCA (Adaptor Neural Cellular Automata). The key finding is that AdaNCA improves performance (clean accuracy and robustness under adversarial attacks) without significantly increasing the number of parameters or FLOPS. This suggests that the improvement isn't solely due to increased model size, but rather AdaNCA's architectural contributions.", "section": "4.1 Results on Image Classification"}, {"figure_path": "BQh1SGvROG/tables/tables_25_2.jpg", "caption": "Table 2: Comparisons of corruption error on each corruption type of ImageNet-C. AdaNCA consistently improves the performance of the baseline model in all categories (Swin-B), and can achieve better results in most categories compared to the SOTA method (TAPADL-RVT). Bold indicates the best model. Lower is better.", "description": "This table presents a comparison of the corruption error rates achieved by different models on the ImageNet-C dataset.  It shows the mean corruption error (mCE) for various corruption types (noise, blur, weather, digital), and compares the performance of the baseline Swin-B model with the AdaNCA-enhanced version.  The results also show a comparison against the state-of-the-art (SOTA) method, TAPADL-RVT.  Lower mCE values indicate better performance.", "section": "4 Results on Image Classification"}, {"figure_path": "BQh1SGvROG/tables/tables_27_1.jpg", "caption": "Table 1: The performance of AdaNCA-enhanced ViTs and their corresponding baselines. We report the mCE for IM-C (lower is better) and accuracy for other benchmarks. AdaNCA consistently improves the clean accuracy as well as the robustness to adversarial and out-of-distribution inputs of the baseline models. Note that our models also outperform the larger baselines (* sign), indicating that the performance improvement does not merely originate from the increase in the number of parameters or FLOPS. The TAPADL method [23] can lead to more vulnerable models compared to the baselines (green numbers). Bold indicates the best model. \u2020: the test is conducted on models pre-trained on ImageNet22K [42], see Appendix C.9.", "description": "This table presents a comparison of the performance of AdaNCA-enhanced Vision Transformers (ViTs) against their baselines across various image classification benchmarks.  It shows clean accuracy, performance under various adversarial attacks (PGD, CW, APGD-DLR, APGD-CE), and robustness against out-of-distribution (OOD) inputs (ImageNet-A, ImageNet-C, ImageNet-R, ImageNet-SK).  The results demonstrate AdaNCA's effectiveness in improving both clean accuracy and robustness without significantly increasing model size or computational cost.", "section": "4 Results on Image Classification"}, {"figure_path": "BQh1SGvROG/tables/tables_28_1.jpg", "caption": "Table 1: The performance of AdaNCA-enhanced ViTs and their corresponding baselines. We report the mCE for IM-C (lower is better) and accuracy for other benchmarks. AdaNCA consistently improves the clean accuracy as well as the robustness to adversarial and out-of-distribution inputs of the baseline models. Note that our models also outperform the larger baselines (* sign), indicating that the performance improvement does not merely originate from the increase in the number of parameters or FLOPS. The TAPADL method [23] can lead to more vulnerable models compared to the baselines (green numbers). Bold indicates the best model. \u2020: the test is conducted on models pre-trained on ImageNet22K [42], see Appendix C.9.", "description": "This table presents a comparison of the performance of AdaNCA-enhanced Vision Transformers (ViTs) against their corresponding baselines across various benchmarks.  It shows the clean accuracy, performance under several adversarial attacks (PGD, CW, APGD-DLR, APGD-CE), and robustness to out-of-distribution (OOD) inputs (ImageNet-A, ImageNet-C, ImageNet-R, ImageNet-SK).  The results demonstrate AdaNCA's consistent improvement in both clean accuracy and robustness, even outperforming larger baseline models, indicating that the performance gain is not solely due to increased parameters or FLOPS.  A comparison to the TAPADL method highlights that AdaNCA provides more robust models.", "section": "4 Results on Image Classification"}, {"figure_path": "BQh1SGvROG/tables/tables_28_2.jpg", "caption": "Table 1: The performance of AdaNCA-enhanced ViTs and their corresponding baselines. We report the mCE for IM-C (lower is better) and accuracy for other benchmarks. AdaNCA consistently improves the clean accuracy as well as the robustness to adversarial and out-of-distribution inputs of the baseline models. Note that our models also outperform the larger baselines (* sign), indicating that the performance improvement does not merely originate from the increase in the number of parameters or FLOPS. The TAPADL method [23] can lead to more vulnerable models compared to the baselines (green numbers). Bold indicates the best model. \u2020: the test is conducted on models pre-trained on ImageNet22K [42], see Appendix C.9.", "description": "This table presents a comparison of the performance of various Vision Transformer (ViT) models, both with and without the AdaNCA adaptor.  It shows clean accuracy and robustness against several adversarial attacks and out-of-distribution datasets (ImageNet-A, ImageNet-C, ImageNet-R, ImageNet-SK).  The results highlight AdaNCA's consistent improvement in both clean accuracy and robustness across different ViT architectures, without a significant increase in model parameters or computational cost.  The table also notes that larger versions of baseline models do not achieve similar improvements, suggesting that AdaNCA's contributions are architecture-specific rather than solely due to increased model size.", "section": "4 Results on Image Classification"}, {"figure_path": "BQh1SGvROG/tables/tables_29_1.jpg", "caption": "Table 1: The performance of AdaNCA-enhanced ViTs and their corresponding baselines. We report the mCE for IM-C (lower is better) and accuracy for other benchmarks. AdaNCA consistently improves the clean accuracy as well as the robustness to adversarial and out-of-distribution inputs of the baseline models. Note that our models also outperform the larger baselines (* sign), indicating that the performance improvement does not merely originate from the increase in the number of parameters or FLOPS. The TAPADL method [23] can lead to more vulnerable models compared to the baselines (green numbers). Bold indicates the best model. \u2020: the test is conducted on models pre-trained on ImageNet22K [42], see Appendix C.9.", "description": "This table presents a comparison of the performance of AdaNCA-enhanced Vision Transformers (ViTs) against their baselines across various benchmarks.  It shows clean accuracy, performance under various adversarial attacks (PGD, CW, APGD-DLR, APGD-CE), and robustness against out-of-distribution (OOD) inputs (ImageNet-A, ImageNet-C, ImageNet-R, ImageNet-SK).  The results demonstrate that AdaNCA consistently improves both clean accuracy and robustness without a substantial increase in parameters or computational cost, outperforming even larger versions of the baseline models. The table also highlights that the TAPADL method, while state-of-the-art in some aspects, can lead to less robust models than baselines.", "section": "4 Results on Image Classification"}, {"figure_path": "BQh1SGvROG/tables/tables_29_2.jpg", "caption": "Table 2: Comparisons of corruption error on each corruption type of ImageNet-C. AdaNCA consistently improves the performance of the baseline model in all categories (Swin-B), and can achieve better results in most categories compared to the SOTA method (TAPADL-RVT). Bold indicates the best model. Lower is better.", "description": "This table presents a comparison of the mean corruption error (mCE) for various corruption types from the ImageNet-C dataset.  The comparison is made between a baseline Swin-B model and a version enhanced with AdaNCA.  The table also includes a comparison to the state-of-the-art (SOTA) method, TAPADL-RVT.  Lower mCE values indicate better robustness to image corruptions.", "section": "4 Results on Image Classification"}, {"figure_path": "BQh1SGvROG/tables/tables_30_1.jpg", "caption": "Table 1: The performance of AdaNCA-enhanced ViTs and their corresponding baselines. We report the mCE for IM-C (lower is better) and accuracy for other benchmarks. AdaNCA consistently improves the clean accuracy as well as the robustness to adversarial and out-of-distribution inputs of the baseline models. Note that our models also outperform the larger baselines (* sign), indicating that the performance improvement does not merely originate from the increase in the number of parameters or FLOPS. The TAPADL method [23] can lead to more vulnerable models compared to the baselines (green numbers). Bold indicates the best model. \u2020: the test is conducted on models pre-trained on ImageNet22K [42], see Appendix C.9.", "description": "This table presents a comparison of the performance of various Vision Transformer (ViT) models, both with and without the AdaNCA adaptor, across several image classification benchmarks.  The benchmarks include standard ImageNet accuracy, as well as robustness evaluations against adversarial attacks (PGD, CW, APGD-DLR, APGD-CE), and out-of-distribution (OOD) datasets (ImageNet-A, ImageNet-C, ImageNet-R, ImageNet-SK). The table shows that AdaNCA consistently improves both clean accuracy and robustness across different ViT architectures, without a significant increase in model parameters or computational cost. The results are also compared against a state-of-the-art (SOTA) robust ViT model (TAPADL).", "section": "4 Results on Image Classification"}]