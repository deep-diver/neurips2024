[{"figure_path": "8puv3c9CPg/tables/tables_1_1.jpg", "caption": "Table 1: Performance of ViTs trained from scratch with auxiliary losses. Adding either a disentanglement loss term to encourage disentangled object representations (Disent. Loss) or a pipeline loss to encourage two-stage processing in the attention heads (Pipeline Loss) boosts test accuracy and compositional generalization (Comp. Acc.) for the discrimination task. Both auxiliary losses are required to boost accuracy for the RMTS task.", "description": "This table presents the performance of Vision Transformers (ViTs) trained from scratch on discrimination and relational match-to-sample (RMTS) tasks.  It shows the impact of adding auxiliary loss functions (disentanglement loss and pipeline loss) on training accuracy, test accuracy, and compositional generalization accuracy. The results demonstrate that these losses improve performance, especially for the more complex RMTS task, highlighting the importance of disentangled representations and two-stage processing.", "section": "7 Failure Modes"}, {"figure_path": "8puv3c9CPg/tables/tables_9_1.jpg", "caption": "Table 1: Performance of ViTs trained from scratch with auxiliary losses. Adding either a disentanglement loss term to encourage disentangled object representations (Disent. Loss) or a pipeline loss to encourage two-stage processing in the attention heads (Pipeline Loss) boosts test accuracy and compositional generalization (Comp. Acc.) for the discrimination task. Both auxiliary losses are required to boost accuracy for the RMTS task.", "description": "This table shows the performance of Vision Transformers (ViTs) trained from scratch on discrimination and relational match-to-sample (RMTS) tasks, with and without auxiliary loss functions.  It demonstrates how adding disentanglement loss and/or pipeline loss improves performance, particularly on the more complex RMTS task, highlighting the benefit of disentangled representations and the two-stage processing pipeline in solving relational reasoning problems.", "section": "Failure Modes"}, {"figure_path": "8puv3c9CPg/tables/tables_16_1.jpg", "caption": "Table 2: All behavioral results for ViT-B/16 models trained on all 256 shape-color combinations on the discrimination task.", "description": "This table shows the performance of different Vision Transformer (ViT) models on a discrimination task.  The models were pre-trained on different datasets (CLIP, DINOv2, ImageNet, DINO, MAE) and then fine-tuned on a discrimination task using 256 shape-color combinations.  The table presents the training accuracy, test accuracy (on an IID test set), and realistic accuracy (on a photorealistic held-out test set).  The results highlight the performance differences across various pre-trained ViTs, showing that CLIP and DINOv2 pretrained models generally have higher accuracy than others.", "section": "B ViT B/16: All Model Behavioral Results"}, {"figure_path": "8puv3c9CPg/tables/tables_16_2.jpg", "caption": "Table 1: Performance of ViTs trained from scratch with auxiliary losses. Adding either a disentanglement loss term to encourage disentangled object representations (Disent. Loss) or a pipeline loss to encourage two-stage processing in the attention heads (Pipeline Loss) boosts test accuracy and compositional generalization (Comp. Acc.) for the discrimination task. Both auxiliary losses are required to boost accuracy for the RMTS task.", "description": "This table presents the performance of Vision Transformers (ViTs) trained from scratch on discrimination and relational match-to-sample (RMTS) tasks.  It shows how adding auxiliary loss functions (disentanglement loss and pipeline loss) impacts the model's performance, both in terms of test accuracy and the ability to generalize to unseen combinations of features (compositional generalization). The results highlight the importance of both disentangled representations and a two-stage processing pipeline for success on these tasks.", "section": "7 Failure Modes"}, {"figure_path": "8puv3c9CPg/tables/tables_17_1.jpg", "caption": "Table 2: All behavioral results for ViT-B/16 models trained on all 256 shape-color combinations on the discrimination task.", "description": "This table presents the performance of different ViT-B/16 models on a discrimination task.  The models were trained on all 256 shape-color combinations.  The table shows the training accuracy, test accuracy (on an IID test set), and accuracy on a photorealistic test set. The \"Pretraining Scale\" column indicates the size of the dataset used for pretraining each model.  The results highlight the strong performance of CLIP and DINOv2 pretrained models compared to others. Note the significant drop in performance on the photorealistic test set for all models except CLIP.", "section": "B ViT B/16: All Model Behavioral Results"}, {"figure_path": "8puv3c9CPg/tables/tables_17_2.jpg", "caption": "Table 3: All behavioral results for ViT-B/16 models trained on 32 shape-color combinations on the discrimination task.", "description": "This table presents the performance of different Vision Transformer (ViT) models on a discrimination task, focusing on models trained with only 32 shape-color combinations. The results are categorized by the model's pretraining method, including CLIP, DINOv2, ImageNet, DINO, MAE, and a model trained from scratch.  It details the training accuracy (Train Acc.), the test accuracy on an independent identically distributed (IID) dataset (Test Acc.), and the compositional generalization accuracy (Comp. Acc.), which assesses the model's ability to generalize to unseen combinations of shapes and colors.", "section": "B ViT B/16: All Model Behavioral Results"}, {"figure_path": "8puv3c9CPg/tables/tables_19_1.jpg", "caption": "Table 6: All behavioral results for CLIP-B/32 models.", "description": "This table presents the performance of CLIP-B/32 models on discrimination and RMTS tasks.  The performance is measured across different training conditions: using all 256 shape-color combinations or a subset of 32, and evaluated on in-distribution (IID), compositional generalization, and out-of-distribution (OOD) test sets.  The metrics presented are training accuracy, IID test accuracy, compositional generalization accuracy, and OOD accuracy.", "section": "Generalization Results"}, {"figure_path": "8puv3c9CPg/tables/tables_35_1.jpg", "caption": "Table 7: Performance of ViTs trained from scratch on the discrimination task with auxiliary losses.", "description": "This table presents the performance of Vision Transformers (ViTs) trained from scratch on the discrimination task using auxiliary losses.  It shows how adding disentanglement and pipeline losses impacts training accuracy, test accuracy (on IID data), and compositional generalization accuracy.  The results demonstrate that adding auxiliary losses significantly improves performance, highlighting the importance of both disentangled representations and a two-stage processing pipeline in solving this visual relational reasoning task.", "section": "7 Failure Modes"}, {"figure_path": "8puv3c9CPg/tables/tables_35_2.jpg", "caption": "Table 1: Performance of ViTs trained from scratch with auxiliary losses. Adding either a disentanglement loss term to encourage disentangled object representations (Disent. Loss) or a pipeline loss to encourage two-stage processing in the attention heads (Pipeline Loss) boosts test accuracy and compositional generalization (Comp. Acc.) for the discrimination task. Both auxiliary losses are required to boost accuracy for the RMTS task.", "description": "This table presents the results of experiments using Vision Transformers (ViTs) trained from scratch on same-different tasks.  The impact of adding auxiliary loss functions (disentanglement loss and pipeline loss) on the model's performance is evaluated for both discrimination and relational match-to-sample (RMTS) tasks.  It shows that adding these losses improves accuracy, particularly when both are used together for the RMTS task, demonstrating the benefit of encouraging disentanglement and two-stage processing.", "section": "7 Failure Modes"}]