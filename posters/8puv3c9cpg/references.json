{"references": [{"fullname_first_author": "Vaswani, A.", "paper_title": "Attention is all you need", "publication_date": "2017-12-01", "reason": "This paper introduced the Transformer architecture, a foundational model for many vision transformers (ViTs), the subject of this paper."}, {"fullname_first_author": "Dosovitskiy, A.", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2020-10-26", "reason": "This paper introduced the Vision Transformer (ViT), a direct antecedent of the models studied in this paper."}, {"fullname_first_author": "Radford, A.", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-01", "reason": "This paper introduced the CLIP model, which is the main model used for analysis in the current paper."}, {"fullname_first_author": "Higgins, I.", "paper_title": "Towards a definition of disentangled representations", "publication_date": "2018-12-01", "reason": "This paper introduced the concept of disentangled representations, a key concept used to interpret the results of the current paper."}, {"fullname_first_author": "Geiger, A.", "paper_title": "Finding alignments between interpretable causal variables and distributed neural representations", "publication_date": "2024-01-01", "reason": "This paper introduced the Distributed Alignment Search (DAS) method, a key technique used for mechanistic interpretability in the current paper."}]}