[{"type": "text", "text": "Iterative Methods via Locally Evolving Set Process ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Baojian Zhou1,2 \u2217 Yifan Sun3 Reza Babanezhad Harikandeh4 Xingzhi Guo3 ", "page_idx": 0}, {"type": "text", "text": "Deqing Yang1,2 ", "page_idx": 0}, {"type": "text", "text": "Yanghua Xiao2 ", "page_idx": 0}, {"type": "text", "text": "1 the School of Data Science, Fudan University, 2 Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University 3 Department of Computer Science, Stony Brook University, 4 Samsung SAIT AI Lab. ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Given the damping factor $\\alpha$ and precision tolerance $\\epsilon$ , Andersen et al. [2] introduced Approximate Personalized PageRank (APPR), the de facto local method for approximating the PPR vector, with runtime bounded by $\\Theta(1/(\\alpha\\epsilon))$ independent of the graph size. Recently, Fountoulakis & Yang [12] asked whether faster local algorithms could be developed using $\\tilde{\\mathcal{O}}(1/(\\sqrt{\\alpha}\\bar{\\epsilon}))$ operations. By noticing that APPR is a local variant of Gauss-Seidel, this paper explores the question of whether standard iterative solvers can be effectively localized. We propose to use the locally evolving set process, a novel framework to characterize the algorithm locality, and demonstrate that many standard solvers can be effectively localized. Let $\\overline{{\\mathrm{Vol}}}({\\cal S}_{t})$ and $\\overline{\\gamma}_{t}$ be the running average of volume and the residual ratio of active nodes $\\mathcal{S}_{t}$ during the process. We show $\\overline{{\\mathrm{vol}}}(S_{t})/\\overline{{\\gamma}}_{t}\\leq1/\\epsilon$ and prove APPR admits a new runtime bound $\\tilde{\\mathcal{O}}(\\overline{{\\mathrm{vol}}}(S_{t})/(\\alpha\\overline{{\\gamma}}_{t}))$ mirroring the actual performance. Furthermore, when the geometric mean of residual reduction is $\\Theta({\\sqrt{\\alpha}})$ , then there exists $c\\in(0,2)$ such that the local Chebyshev method has runtime $\\tilde{\\mathcal{O}}(\\overline{{\\mathrm{vol}}}(S_{t})/(\\sqrt{\\alpha}(2-c)))$ without the monotonicity assumption. Numerical results confirm the efficiency of this novel framework and show up to a hundredfold speedup over corresponding standard solvers on real-world graphs. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Personalized PageRank (PPR) vectors are key tools for graph problems such as clustering [2, 3, 30, 36, 54, 57], diffusion [10, 14, 15, 29], random walks [25, 32, 44], neural net training [7, 27, 20, 21], and many others [17, 48]. The Approximate PPR (APPR) [2] and its many variants [6, 9, 13, 37] efficiently approximate PPR vectors by exploring the neighbors of a specific node at each time, only requiring access to a tiny part of the graph \u2013 hence the number of operations needed is independent of graph size. These local solvers are well-suited for large-scale graphs in modern graph data analysis. Specifically, let $\\pmb{A}$ and $_{D}$ be the adjacency and degree matrices of a graph $\\mathcal{G}$ , respectively. Given a source node $s$ and the damping factor $\\alpha\\in(0,1)$ , this paper studies local solvers for the linear system ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\left(I-\\left(1-\\alpha\\right)\\left(I+A D^{-1}\\right)/2\\right)\\pi=\\alpha e_{s},\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "where $e_{s}$ is the standard basis of $s$ and $\\pi$ is the PPR vector [2, 12, 37]. Given the error tolerance $\\epsilon$ , a local solver needs to find $\\hat{\\pi}$ such that $\\|D^{-1}(\\hat{\\pi}-\\pi)\\|_{\\infty}\\leq\\epsilon$ without accessing the entire graph $\\mathcal{G}$ .2 ", "page_idx": 0}, {"type": "text", "text": "Andersen et al. [2] proposed the local APPR algorithm, which pushes large residuals to neighboring nodes until all residuals are small. Its runtime is upper bounded by $\\Theta(1/{\\bar{(\\alpha\\epsilon)}})$ independent of graph size. Based on a variational characterization of Equ. (1), Fountoulakis et al. [13] reformulated the problem as optimizing a quadratic objective plus $\\ell_{1}$ -regularization and later asked [12] whether there exists a local solver with runtime $\\tilde{\\mathcal{O}}(\\mathrm{i}/(\\sqrt{\\alpha}\\bar{\\epsilon}))$ . This corresponds to an accelerated rate since $\\alpha$ is the strongly convex parameter. Recently, Mart\u00ednez-Rubio et al. [37] provided a method based on a nested subspace pursuit strategy, and the corresponding iteration complexity is bounded by $\\tilde{\\mathcal{O}}(|S^{*}|/\\sqrt{\\alpha})$ where $S^{*}$ is the support of the optimal solution. This bound deteriorates to $\\tilde{\\mathcal{O}}(n/\\sqrt{\\alpha})$ when the solution is dense, with $n$ representing the number of nodes in $\\mathcal{G}$ , which could be less favorable than that of standard solvers under similar conditions. Moreover, the nested computational structure provides a constant factor overhead, which could be significant in practice. ", "page_idx": 1}, {"type": "text", "text": "The bound analysis of the above local methods critically depends on the monotonicity properties of the designed algorithms. These requirements may hinder the development of simpler and faster local linear solvers that lack such monotonicity properties. Specifically, the runtime analysis of APPR relies on the non-negativity and decreasing monotonicity of residuals. Conversely, the runtime bounds developed in Fountoulakis et al. [13] and Mart\u00ednez-Rubio et al. [37] depend on the monotonicity of variable updates, ensuring that the sparsity of intermediate variables increases monotonically. ", "page_idx": 1}, {"type": "text", "text": "Our contributions. Based on a refined analysis of APPR, our starting point is to demonstrate that APPR is a local variant of Gauss-Seidel Successive Overrelaxation (GS-SOR) that can be treated as an evolving set process.3 This insight leads us to explore whether standard solvers can be effectively localized to solve Equ. (1). To develop faster local methods with improved local bounds, we propose a novel locally evolving set process framework inspired by its stochastic counterpart [38]. This framework enables the development of faster local methods and circumvents the monotonicity requirement barrier in runtime complexity analysis in the existing literature. For example, our analysis of the local Chebyshev method does not depend on the monotonicity of residual or the active node sets processed. Specifically, ", "page_idx": 1}, {"type": "text", "text": "\u2022 As a core tool, we propose an algorithm framework based on the locally evolving set process. We show that APPR is a local variant of GS-SOR using this process. This framework is powerful enough to facilitate the development of new local solvers. Specifically, standard gradient descent (GD) can be effectively localized for solving this problem and admits $\\Theta(1/(\\alpha\\epsilon))$ runtime bound. \u2022 This local evolving set process provides a novel way to characterize the algorithm locality; hence, new runtime bounds can be derived. Let $\\overline{{\\mathrm{Vol}}}({\\cal S}_{t})$ and $\\overline{\\gamma}_{t}$ be the running average of volume and the residual ratio of active nodes $\\mathcal{S}_{t}$ during the process; we prove the ratio $\\overline{{\\mathrm{vol}}}({\\cal S}_{t})/\\overline{{\\gamma}}_{t}$ serving as a lower bound of $1/\\epsilon$ . We further show both APPR and local GD have $\\tilde{\\mathcal{O}}(\\overline{{\\mathrm{vol}}}(S_{t})/(\\alpha\\overline{{\\gamma}}_{t}))$ runtime bound mirroring the actual performance of these two methods. \u2022 Using our framework, we show there exists $c\\in(0,2)$ such that both the localized Chebyshev and Heavy-Ball methods admit runtime bound $\\tilde{\\mathcal{O}}(\\overline{{\\mathrm{vol}}}(S_{t})/(\\sqrt{\\alpha}(2-c)))$ with the assumption that the geometric mean of active ratio factors is $\\Theta({\\sqrt{\\alpha}})$ . Importantly, our analysis does not require any monotonicity property. The technical novelty is that we effectively characterize residuals of these two methods by using second-order difference equations with parameterized coefficients. \u2022 We demonstrate, over 17 large graphs, that these localized methods can significantly accelerate their standard counterparts by a large margin. Furthermore, our proposed LOCSOR, LOCCH, and LOCHB are significantly faster than APPR and $\\ell_{1}$ -based solvers on two huge-scale graphs. ", "page_idx": 1}, {"type": "text", "text": "Paper structure. We begin by clarifying notations and reviewing APPR in Sec. 2. Sec. 3 introduces the locally evolving set process. Sec. 4 presents localized Chebyshev and Heavy-Ball methods along with our novel techniques. We discuss open questions in Sec. 5. Experiments and conclusions are covered in Sec. 6 and 7, respectively. Detailed related works and all missing proofs are included in the Appendix. Our code is available at https: // github. com/ baojian/ LocalCH . ", "page_idx": 1}, {"type": "text", "text": "2 Notations and Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Notations. We consider an undirected simple graph $\\mathcal{G}(\\boldsymbol{\\upnu},\\boldsymbol{\\mathcal{E}})$ where $\\mathcal{V}=\\{1,2,\\ldots,n\\}$ and $\\mathcal{E}\\subseteq\\mathcal{V}\\times\\mathcal{V}$ with $|{\\mathcal{E}}|\\,=\\,m$ are the node and edge sets, respectively. The set of neighbors of $v$ is denoted as $\\mathcal{N}(v)\\subseteq\\mathcal{V}$ . The adjacency matrix $\\pmb{A}$ of $\\mathcal{G}$ assigns unit weight $a_{u,v}=1$ if $(u,v)\\in\\mathcal{E}$ and 0 otherwise. The $v$ -th entry of the degree matrix $_{D}$ is $d_{v}\\bar{=}|\\mathcal{N}(v)|$ . Given ${\\mathcal{S}}\\subseteq{\\mathcal{V}}$ , we define the volume of $\\boldsymbol{S}$ as $\\begin{array}{r}{\\operatorname{vol}(S)\\triangleq\\sum_{v\\in S}d_{v}}\\end{array}$ . The support of $\\pmb{x}\\in\\mathbb{R}^{n}$ is the set of nonzero indices $\\operatorname{supp}(\\pmb{x})\\triangleq\\{\\boldsymbol{v}:\\boldsymbol{x}_{\\boldsymbol{v}}\\neq$ $0,v\\in\\mathcal{V}\\}$ . The eigendecomposition of $D^{-1/2}A{\\cal D}^{-1/2}=V\\Lambda V^{\\top}$ where each column of $V$ is an eigenvector and $\\mathbf{A}=\\mathbf{diag}(\\lambda_{1},\\lambda_{2},\\ldots,\\lambda_{n})$ with $1=\\lambda_{1}\\geq\\lambda_{2}\\geq\\cdot\\cdot\\geq\\lambda_{n}\\geq-1$ . ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2.1 Revisiting Anderson\u2019s APPR and its local runtime bound ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We use $(p,z)$ for solving Equ. (1) while use $(x,r)$ for solving Equ. (3) or Equ. (4). With the initial setting $p\\gets\\mathbf{0},z\\gets e_{s}$ , APPR obtains a local estimate of $\\pi$ denoted as $\\textbf{\\emph{p}}$ by using a sequence of PUSH operations defined as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\left|\\mathbf{\\deltaAPPR}(\\alpha,\\epsilon,s,\\mathcal{G}):\\right.\\quad\\mathbf{Repeat}\\left(p,z\\right)\\gets\\mathbf{PUSH}(u,\\alpha,p,z)\\ \\mathbf{Until}\\ \\forall v,z_{v}<\\epsilon d_{v};\\ \\mathbf{Return}\\,p.\\\n$$", "text_format": "latex", "page_idx": 2}, {"type": "table", "img_path": "wT2KhEb97a/tmp/b7cea6a02e7bb9f66d15cf041c6d18542cfc0a62b9f0a21e8c5a6ab876bbebfc.jpg", "table_caption": [], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "Lemma 2.1 (Runtime bound of APPR [2]). Given $\\alpha\\in(0,1)$ and the precision $\\epsilon\\leq1/d_{s}$ for node $s\\in\\mathcal{V}$ with $\\mathbf{\\displaystylep\\ensuremath{\\leftarrow}\\,}\\mathbf{0},z\\ensuremath{\\leftarrow}e_{s}$ at the initial, APPR $(\\alpha,\\epsilon,s,\\mathcal{G})$ defined in (2) returns an estimate $\\pmb{p}$ of $\\pi$ . There exists a real implementation of (2) (e.g., Algo. 2) such that the runtime $\\mathcal{T}_{\\mathrm{APPR}}$ satisfies ", "page_idx": 2}, {"type": "text", "text": "Furthermore, the estimate ${\\hat{\\pi}}:=p$ satis ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{T}_{\\mathrm{APPR}}\\leq\\Theta\\left(1/(\\alpha\\epsilon)\\right).}\\\\ &{\\mathrm{~}\\mathrm{f}e s\\;\\|D^{-1}(\\hat{\\pi}-\\pi)\\|_{\\infty}\\leq\\epsilon\\,a n d\\,\\mathrm{vol}(\\mathrm{supp}(\\hat{\\pi}))\\leq2/((1-\\alpha)\\epsilon).}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The main argument for proving Lemma 2.1 is critically based on: 1) $z\\ge0$ and $\\|z\\|_{1}$ decreases during the updates; 2) for each active $u$ , $z_{u}\\ge\\epsilon d_{u}$ , implying that $\\|z\\|_{1}$ is decreased by at least $\\alpha\\epsilon d_{u}$ , consecutively leading to $\\textstyle\\sum_{u}d_{u}\\leq1/(\\alpha\\epsilon)$ . ", "page_idx": 2}, {"type": "text", "text": "2.2 Problem reformulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "To approximate the PPR vector $\\pi$ , the original linear system in Equ. (1) can be reformulated as an equivalent symmetric version defined as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{Q x=b,\\quad\\mathrm{~with~}Q\\triangleq I-\\frac{1-\\alpha}{1+\\alpha}D^{-1/2}A D^{-1/2}\\mathrm{~and~}b\\triangleq\\frac{2\\alpha}{(1+\\alpha)}D^{-1/2}e_{s},}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where again $e_{s}$ is the standard basis of $s$ , and $Q$ is a symmetric positive-definite $M$ -matrix with all eigenvalues in $\\textstyle\\left[{\\frac{2\\alpha}{1+\\alpha}},{\\frac{2}{1+\\alpha}}\\right]$ . To solve Equ. (3) is equivalent to solving a quadratic problem ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\pmb{x}^{*}=\\arg\\operatorname*{min}_{\\pmb{x}\\in\\mathbb{R}^{n}}\\Big\\{f(\\pmb{x})\\triangleq\\frac{1}{2}\\pmb{x}^{\\top}\\pmb{Q}\\pmb{x}-\\pmb{x}^{\\top}b\\Big\\},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $f$ is strongly convex with condition number $1/\\alpha$ . Indeed, Equ. (3) is a symmetrized version of Equ. (1) and has a unique solution $\\pmb{x}^{*}=\\pmb{Q}^{-1}\\pmb{b}$ . The PPR vector $\\pi$ can be recovered from $x^{*}$ by $\\pmb{\\pi}=D^{1/2}\\pmb{x}^{*}$ . It is convenient to denote estimate of $\\pi$ as $\\pmb{\\pi}^{(t)}\\triangleq D^{1/2}\\pmb{x}^{(t)}$ . Given $\\pmb{x}^{(t)}$ , we define the residual $\\pmb{r}^{(t)}\\triangleq\\pmb{b}-\\pmb{Q}\\pmb{x}^{(t)}$ . If $\\pmb{x}^{(t)}$ is returned by a local solver for solving either Equ. (3) or Equ. (4), we then equivalently require $\\|D^{-1/2}(\\pmb{x}^{(t)}-\\pmb{x}^{*})\\|_{\\infty}\\leq\\epsilon$ . Hence, it is enough to have a stop condition $\\|D^{-1/2}\\pmb{r}^{(t)}\\|_{\\infty}\\leq2\\alpha\\epsilon/(1+\\alpha)$ for local solvers of Equ. (3) and Equ. (4).4 ", "page_idx": 2}, {"type": "text", "text": "Fountoulakis et al. [13] demonstrated that APPR is equivalent to a coordinate descent solver for minimizing $f$ in Equ. (4) and introduced an ISTA-style solver by minimizing $f(\\pmb{x})+\\epsilon\\alpha\\|\\pmb{D}^{1/2}\\pmb{x}\\|_{1}$ , which provides a method with runtime bound $\\tilde{\\mathcal{O}}(1/(\\epsilon\\alpha))$ for achieving the same estimation guarantee of APPR. On one hand, one may note that the runtime bound $\\Theta(1\\bar{/}(\\alpha\\epsilon))$ provided in Lemma 2.1 becomes less valuable when $\\epsilon\\leq1/m$ ; on the other hand, all previous local variants [6, 9, 13, 37] of APPR are critically based on some monotonicity property. This limitation could impede the development of faster local methods that might violate the monotonicity assumption. The following two sections present the techniques and tools to address these challenges. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "3 Local Methods via Evolving Set Process ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Our investigation begins with the locally evolving set process, as inspired by the stochastic counterpart [38]. The process reveals that APPR is essentially a local variant of GS-SOR. We then show how to use this process to build faster local solvers based on GS-SOR. We further develop a local parallelizable gradient descent with runtime $\\Theta(1/(\\alpha\\epsilon))$ . ", "page_idx": 3}, {"type": "text", "text": "3.1 Locally evolving set process ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Given $\\alpha,\\epsilon,s$ , and $\\mathcal{G}$ , a local solver for Equ. (3) keeps track of an active set $\\mathcal{S}_{t}\\subset\\mathcal{V}$ at each iteration $t$ . That is, only nodes in $\\mathcal{S}_{t}$ are used to update $\\textbf{\\em x}$ or $\\pmb{r}$ . The next set $\\boldsymbol{S}_{t+1}$ is determined by current $\\mathcal{S}_{t}$ and an associated local solver $\\boldsymbol{\\mathcal{A}}$ . We define this process as the following local evolving set system. ", "page_idx": 3}, {"type": "text", "text": "Definition 3.1 (Locally evolving set process). Given a parameter configuration $\\theta\\triangleq(\\alpha,\\epsilon,s,\\mathcal{G})$ , and a local iterative method $\\boldsymbol{\\mathcal{A}}$ , the locally evolving set process generates a sequence of $(S_{t},\\mathbf{\\boldsymbol{x}}^{(t)},\\mathbf{\\boldsymbol{r}}^{(t)})$ representing as the following dynamic system ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\left(S_{t+1},\\pmb{x}^{(t+1)},\\pmb{r}^{(t+1)}\\right)=\\pmb{\\Phi}_{\\theta}\\left(S_{t},\\pmb{x}^{(t)},\\pmb{r}^{(t)},\\pmb{A}\\right),\\quad\\forall t\\geq0,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $S_{t+1}\\subseteq S_{t}\\cup(\\cup_{u\\in S_{t}}\\mathcal{N}(u))$ and we denote the active set ${\\cal S}_{t}=\\left\\{u_{1},u_{2},\\dots,u_{|{\\cal S}_{t}|}\\right\\}$ . The set $\\mathcal{S}_{t}$ is maintained via a queue data structure. We say this process converges when the last set ${\\cal S}_{T}=\\emptyset$ if there exists such $T$ ; the generated sequence of active nodes are ", "page_idx": 3}, {"type": "equation", "text": "$$\n(S_{0},x^{(0)},r^{(0)})\\;\\to\\;(S_{1},x^{(1)},r^{(1)})\\;\\to\\;(S_{2},x^{(2)},r^{(2)})\\;\\to\\;\\ldots\\;\\to\\;(S_{T}=\\emptyset,x^{(T)},r^{(T)}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The runtime of the local solver, $\\boldsymbol{\\mathcal{A}}$ for this whole local process, is then defined as 5 ", "page_idx": 3}, {"type": "equation", "text": "$$\nT_{\\mathcal{A}}\\triangleq\\sum_{t=0}^{T-1}\\operatorname{vol}(S_{t}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The framework of this set process provides a new way to design local methods. Furthermore, it helps to analyze the convergence and runtime bound of local solvers by characterizing the sequences $\\{\\mathrm{vol}(S_{t})\\}$ , and $\\{\\|\\pmb{r}^{(t)}\\|\\}$ generated by $\\Phi_{\\theta}$ . To analyze a new runtime bound, for $T\\geq1$ , we define the average of the volume of active node sets $\\{\\mathrm{vol}(\\bar{S}_{t})\\}$ and active ratio sequence $\\{\\gamma_{t}\\}$ as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\overline{{\\mathrm{vol}}}(S_{T})\\triangleq\\frac{1}{T}\\sum_{t=0}^{T-1}\\mathrm{vol}(S_{t}),\\quad\\overline{\\gamma}_{T}\\triangleq\\frac{1}{T}\\sum_{t=0}^{T-1}\\left\\{\\gamma_{t}\\triangleq\\frac{\\sum_{i=1}^{|S_{t}|}|\\sqrt{d_{u_{i}}}r_{u_{i}}^{(t+\\Delta_{i})}|}{\\lVert D^{1/2}r^{(t)}\\rVert_{1}}\\right\\},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\Delta_{i}$ is a smaller time magnitude. We define $\\Delta_{i}=(i-1)/|S_{t}|$ for the analysis of APPR and LOCSOR while $\\Delta_{i}=0$ for LOCGD in our later analysis. In the rest, we denote $\\begin{array}{r}{\\mathcal{I}_{T}=\\mathrm{supp}(r^{(T)})}\\end{array}$ . ", "page_idx": 3}, {"type": "text", "text": "These two metrics $\\overline{{\\mathrm{vol}}}({\\cal S}_{T})$ and $\\overline{{\\gamma}}_{T}$ characterize the locality of local methods. To demonstrate this local process, Fig. 1 shows $\\mathrm{vol}({\\cal S}_{t})$ of APPR peaks at the early stage, and the active ratio decreases as the active volume ", "page_idx": 3}, {"type": "image", "img_path": "wT2KhEb97a/tmp/00dafdd6f96a6352cafe9022530d087f2f4c894dcc88da5280a7802c571a254c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 1: Runtime of APPR in the locally evolving set process on the com-dblp graph with $s=0,\\alpha=0.1$ , and $\\epsilon=1/m$ . The red region of the left figure is $\\mathcal{T}_{\\mathrm{APPR}}$ . The right two figures show active ratios and $\\overline{{\\mathrm{vol}}}(S_{T})/\\overline{{\\gamma}}_{T}\\leq1/\\epsilon$ . ", "page_idx": 3}, {"type": "text", "text": "diminishes. The quantity $\\overline{{\\mathrm{vol}}}(S_{T})/\\overline{{\\gamma}}_{T}$ is strictly smaller than $1/\\epsilon$ , indicating that it could serve as a better factor in the runtime analysis. ", "page_idx": 3}, {"type": "text", "text": "5In practice, $\\begin{array}{r}{\\mathcal{T}_{A}:=\\sum_{t=0}^{T-1}(\\mathrm{vol}(S_{t})+|S_{t}|)}\\end{array}$ where we ignore $\\left|{\\cal S}_{t}\\right|$ for simplicity as $\\mathrm{vol}({\\cal S}_{t})$ dominates $\\left|{\\cal S}_{t}\\right|$ . ", "page_idx": 3}, {"type": "text", "text": "3.2 APPR via locally evolving set process ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We first demonstrate how this locally evolving set process can represent APPR. For solving Equ. (1), the set $S_{0}\\,=\\,\\{s\\}$ and the queue-based of APPR (see Algo. 2 in Appendix A) naturally forms a sequence of active sets from $S_{0}=\\{s\\}$ to ${\\cal S}_{T}=\\emptyset$ , hence converging. Active nodes $u$ in queue satisfy $z_{u}\\ge\\epsilon d_{u}$ . To delineate successive iterations $\\mathcal{S}_{t}$ and $\\boldsymbol{S}_{t+1}$ , one can insert $^*$ at the beginning of $\\mathcal{S}_{t}$ . After processing $\\mathcal{S}_{t}$ , it serves as an indicator for the next iteration. The star $^*$ is reinserted into the queue iteratively until the queue is empty. We use a slightly different notation for presenting tuple $\\bar{(S}_{t},p^{(t)},z^{(t)})$ to consistent with Sec. 2.1 and write out such evolving process as follows ", "page_idx": 4}, {"type": "table", "img_path": "wT2KhEb97a/tmp/39c55cfa25067003e466a534c0a15ef76afbdf9b13f765cac22fa7333f355514.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "The following lemma establishes the equivalence between APPR and the local variant of GS-SOR method (see Appendix B.1) and provides a new evolving-based bound. ", "page_idx": 4}, {"type": "text", "text": "Lemma 3.2 (New local evolving-based bound for APPR). Let $\\begin{array}{r}{M=\\alpha^{-1}\\big(I-\\frac{1-\\alpha}{2}\\left(I+A D^{-1}\\right)\\big)}\\end{array}$ and $\\pmb{s}=e_{s}$ . The linear system $M\\pi=s$ is equivalent to Equ. (1). Given $\\pmb{p}^{(0)}=\\mathbf{0}$ , $z^{(0)}=e_{s}$ with $\\omega\\in(0,2)$ , the local variant of GS-SOR (15) for $M\\pi=s$ can be formulated as ", "page_idx": 4}, {"type": "equation", "text": "$$\np^{(t+\\Delta_{i+1})}\\gets p^{(t+\\Delta_{i})}+\\frac{\\omega z_{u_{i}}^{(t+\\Delta_{i})}}{M_{u_{i}u_{i}}}e_{u_{i}},\\quad z^{(t+\\Delta_{i+1})}\\gets z^{(t+\\Delta_{i})}-\\frac{\\omega z_{u_{i}}^{(t+\\Delta_{i})}}{M_{u_{i}u_{i}}}M e_{u_{i}},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $u_{i}$ is an active node in $\\mathcal{S}_{t}$ satisfying $z_{u_{i}}\\ge\\epsilon d_{u_{i}}$ and $\\Delta_{i}=(i-1)/|S_{t}|$ . Furthermore, when $\\begin{array}{r}{\\omega=\\frac{1+\\alpha}{2}}\\end{array}$ , this method reduces to APPR given i n (7), and there exists a real implementation (Aglo. 2) of APPR such that the runtime $\\mathcal{T}_{\\mathrm{APPR}}$ is bounded, that is ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{T}_{\\mathrm{APPR}}\\leq\\frac{\\overline{{\\operatorname{vol}}}(\\mathcal{S}_{T})}{\\alpha\\hat{\\gamma}_{T}}\\ln\\frac{C_{T}}{\\epsilon},\\,w h e r e\\,\\frac{\\overline{{\\operatorname{vol}}}(\\mathcal{S}_{T})}{\\hat{\\gamma}_{T}}\\leq\\frac{1}{\\epsilon},\\,C_{T}=\\frac{2}{(1-\\alpha)|\\mathcal{Z}_{T}|},\\hat{\\gamma}_{T}\\stackrel{\\Delta}{=}\\frac{1}{T}\\sum_{t=0}^{T-1}\\left\\{\\frac{\\sum_{i=1}^{|\\mathcal{S}_{t}|}|{\\boldsymbol{z}}_{i}^{(t+\\Delta_{i})}|}{\\|{\\boldsymbol{z}}^{(t)}\\|_{1}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "3.3 Faster local variant of GS-SOR ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Lemma 3.2 points to the sub-optimality of APPR, as GS-SOR allows for a larger $\\omega$ . For solving Equ. (3), since APPR essentially serves as a local variant of GS-SOR, we can develop a faster local variant based SOR. To extend this method to solve Equ. (3), we propose a local GS-SOR based on an evolving set process, namely LOCSOR, as the following ", "page_idx": 4}, {"type": "table", "img_path": "wT2KhEb97a/tmp/6291182c2aff29c1e8b19296f6ebf9bbc0f9136749231bb88c1871da761ccafe.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "When $\\omega\\in(0,1]$ , the residual $\\pmb{r}$ is still nonnegative and monotonically decreasing, we establish the convergence of LOCSOR stated in the following theorem. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.3 (Runtime bound of LOCSOR $(\\omega=1)$ )). Given the configuration $\\theta=(\\alpha,\\epsilon,s,\\mathcal{G})$ with $\\alpha\\in(0,1)$ and $\\epsilon\\leq1/d_{s}$ and let $r^{(T)}$ and ${\\pmb x}^{(T)}$ be returned by LOCSOR defined in (8) for solving Equ. (3). There exists a real implementation of (8) such that the runtime $\\tau_{\\mathrm{LocSOR}}$ is bounded by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{1+\\alpha}{2}\\cdot\\frac{\\overline{{\\mathrm{vol}}}(S_{T})}{\\alpha\\overline{{\\gamma}}_{T}}\\left(1-\\frac{\\|D^{1/2}r^{(T)}\\|_{1}}{\\|D^{1/2}r^{(0)}\\|_{1}}\\right)\\leq\\mathcal{T}_{\\mathrm{Loc}\\mathrm{SoR}}\\leq\\frac{1+\\alpha}{2}\\cdot\\operatorname*{min}\\left\\{\\frac{1}{\\alpha\\epsilon},\\frac{\\overline{{\\mathrm{vol}}}(S_{T})}{\\alpha\\overline{{\\gamma}}_{T}}\\ln\\frac{C}{\\epsilon}\\right\\}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\overline{{\\mathrm{vol}}}(S_{T})$ and $\\overline{{\\gamma}}_{T}$ are defined in (6) and $\\begin{array}{r}{C=\\frac{1+\\alpha}{(1-\\alpha)|{\\cal Z}_{T}|}}\\end{array}$ with $\\mathcal{I}_{T}=\\mathrm{supp}(r^{(T)})$ . Furthermore, $\\overline{{\\mathrm{vol}}}(S_{T})/\\overline{{\\gamma}}_{T}\\leq1/\\epsilon$ and the local estimate $\\hat{\\pi}:=D^{1/2}x^{(T)}$ satisfies $\\|D^{-1}(\\hat{\\pi}-\\pi)\\|_{\\infty}\\leq\\epsilon.$ . ", "page_idx": 4}, {"type": "text", "text": "Our new evolving bound $\\tilde{\\mathcal{O}}(\\overline{{\\mathrm{vol}}}(S_{T})/(\\alpha\\overline{{\\gamma}}_{T}))$ mirroring the actual performance of APPR and empirically much smaller than $\\Theta(1/(\\alpha\\epsilon))$ as illustrated in Fig. 2. Our lower bounds are quite effective when $\\epsilon$ is relatively large, while our upper bound is better than Anderson\u2019s when $\\epsilon$ is small. When $\\epsilon\\,\\ll\\,\\Theta(1/m)$ , this new bound is superior to both ${\\mathcal O}(1/(\\alpha\\epsilon))$ and $\\tilde{\\mathcal{O}}(1/(\\sqrt{\\alpha}\\epsilon))$ . This superiority is evident when compared to algorithms like ISTA or FISTA [5] to minimize the $\\ell_{1}$ -regularization of $f$ for obtaining an approximate solution of Equ. (3). Additionally, when $\\omega\\in(1,2)$ and recalling that $Q$ is an $M$ -matrix, the standard analysis of SOR shows that the spectral norm of the iteration matrix must be larger than $|\\omega-1|$ . Hence, $0<\\omega<2$ if and only if global SOR converges [55]. When $\\omega^{*}$ is optimal (the point that the spectral radius of the iteration matrix is minimized), we have the following result. ", "page_idx": 5}, {"type": "image", "img_path": "wT2KhEb97a/tmp/781067b1df456a366307ad2d6d017054761652ec9e105d012fb682534e79d045.jpg", "img_caption": ["Figure 2: Comparison of runtime between APPR and LOCSOR (left) and runtime bounds (right) as a function of $\\epsilon$ . We used the same setting as in Fig. 1. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Corollary 3.4. Let $\\omega=\\omega^{\\ast}\\triangleq2/(1+\\sqrt{1-(1-\\alpha)^{2}/(1+\\alpha)^{2}})$ and $S_{t}=\\mathcal{V},\\forall t\\geq0$ during the updates, the global version of LOCSOR has the following convergence bound ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\|r^{(t)}\\|_{2}\\leq\\frac{2}{(1+\\alpha)\\sqrt{d_{s}}}\\left(\\frac{1-\\sqrt{\\alpha}}{1+\\sqrt{\\alpha}}+\\epsilon_{t}\\right)^{t},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\epsilon_{t}$ are small positive numbers with $\\mathrm{lim}_{t\\rightarrow\\infty}\\,\\epsilon_{t}=0$ . ", "page_idx": 5}, {"type": "text", "text": "Asymptotically, when $\\epsilon_{t}\\,=\\,o(\\sqrt{\\alpha})$ , then the runtime of global LOCSOR is $\\tilde{\\mathcal{O}}(m/\\sqrt{\\alpha})$ where $\\tilde{\\mathcal{O}}$ hides $\\log{1/\\epsilon}$ . The main difficulty of analyzing the optimal local LOCSOR is that the nonnegativity and monotonicity of $\\mathbf{\\boldsymbol{r}}^{(t)}$ do not hold. Instead, by using a parameterized second-order difference equation, we develop new techniques based on the Chebyshev method detailed in Sec. 4. ", "page_idx": 5}, {"type": "text", "text": "3.4 Parallelizable local gradient descent ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "One disadvantage of LOCSOR is its limited potential for parallelization. The standard GD ${\\pmb x}^{(t+1)}=$ $\\pmb{x}^{(t)}-\\pmb{\\nabla}f(\\pmb{x}^{(t)})$ (step $\\mathrm{size}=1$ ), in contrast, is easy to parallelize across the coordinates of the update. Instead of updating $\\pmb{r}$ and $\\textbf{\\em x}$ synchronously per-coordinate, we propose the following ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\Phi_{\\theta}\\left(S_{t},x^{(t)},r^{(t)},\\mathcal{A}=\\mathrm{LocGD}\\right):\\;x^{(t+1)}\\gets x^{(t)}+r_{S_{t}}^{(t)},\\;r^{(t+1)}\\gets r^{(t)}-Q r_{S_{t}}^{(t)}\\qquad\\qquad\\qquad\\qquad\\qquad\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Every coordinate in $\\mathcal{S}_{t}$ is updated in parallel at iteration $t$ . Interestingly, LOCGD exhibits nonnegativity and monotonicity properties, and its runtime complexity is similar to that of LOCSOR, as stated in the following theorem (To remind, $\\Delta_{i}=0$ for $\\overline{{\\gamma}}_{T}$ of LOCGD in Equ. (6) ). ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.5 (Runtime bound of LOCGD). Given the configuration $\\theta=(\\alpha,\\epsilon,s,\\mathcal{G})$ with $\\alpha\\in(0,1)$ and $\\epsilon\\leq1/d_{s}$ and let $r^{(T)}$ and ${\\pmb x}^{(T)}$ be returned by LOCGD defined in (9) for solving Equ. (4). There exists a real implementation of (9) such that the runtime $\\mathcal{T}_{\\mathrm{LocGD}}$ is bounded by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{1+\\alpha}{2}\\cdot\\frac{\\overline{{\\mathrm{vol}}}(S_{T})}{\\alpha\\overline{{\\gamma}}_{T}}\\left(1-\\frac{\\|D^{1/2}r^{(T)}\\|_{1}}{\\|D^{1/2}r^{(0)}\\|_{1}}\\right)\\leq\\mathcal{T}_{\\mathrm{LocGD}}\\leq\\frac{1+\\alpha}{2}\\cdot\\operatorname*{min}\\left\\{\\frac{1}{\\alpha\\epsilon},\\frac{\\overline{{\\mathrm{vol}}}(S_{T})}{\\alpha\\overline{{\\gamma}}_{T}}\\ln\\frac{C}{\\epsilon}\\right\\},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $C=(1+\\alpha)/((1-\\alpha)|\\mathcal{Z}_{T}|),\\mathcal{Z}_{T}=\\mathrm{supp}(r^{(T)})$ . Furthermore, $\\overline{{\\mathrm{vol}}}(S_{T})/\\overline{{\\gamma}}_{T}\\leq1/\\epsilon$ and the estimate $\\hat{\\pi}:=D^{1/2}x^{(T)}$ satisfies $\\|D^{-1}(\\hat{\\pmb{\\pi}}-\\pmb{\\pi})\\|_{\\infty}\\leq\\epsilon.$ . ", "page_idx": 5}, {"type": "text", "text": "Note that $\\overline{{\\gamma}}_{T}$ of LOCGD is empirically smaller than that of LOCSOR. Hence, LOCGD is empirically slower than LOCSOR by only a small constant factor (e.g., twice as slow), a finding consistent with observations of their standard counterparts [19]. Nonetheless, LOCGD is much simpler and more amenable to parallelization on platforms such as GPUs compared to APPR. ", "page_idx": 5}, {"type": "text", "text": "4 Accelerated Local Iterative Methods ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "This section presents our key contributions where we propose faster local methods based on the Chebyshev method for solving Equ. (3) and the Heavy-Ball (HB) method for Equ. (4). ", "page_idx": 5}, {"type": "text", "text": "4.1 Local Chebyshev method ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Compared with GS and GD, the standard Chebyshev method offers optimal acceleration in solving Equ. (3). Following existing techniques (e.g., see d\u2019Aspremont et al. [11]), we show there exists an upper runtime bound $\\tilde{\\mathcal{O}}(\\bar{m}/\\sqrt{\\alpha})$ to meeting the stopping condition where $\\tilde{\\mathcal{O}}$ hides $\\log1/\\epsilon$ (we presented it in Theorem C.6). Hence, the Chebyshev method is one of the optimal first-order linear solvers for solving Equ. (3). However, localizing Chebyshev poses greater challenges due to the additional momentum vector involved in updating $\\pmb{x}^{(t)}$ . Our key observation is that if a substantial reduction in the magnitudes of $\\mathbf{\\dot{\\rho}}_{\\mathbf{r}}(t)$ is required within a subset of $\\boldsymbol{S}_{t}$ , then the corresponding momentum coordinates are likely to possess significant acceleration energy. Intuitively, a viable strategy involves localizing both the residual and momentum vectors. For $t\\geq1$ , denote the \u201cmomentum\u201d vector as $\\Delta^{(t)}:=\\bar{\\pmb{x}}^{(t)}-\\pmb{x}^{(t-1)}$ and $\\delta_{t:t+1}=\\delta_{t}\\delta_{t+1}$ , we propose the localized Chebyshev as the following where $t\\geq1$ with the initials ${\\pmb x}^{(0)}={\\bf0},{\\pmb x}^{(1)}={\\pmb r}^{(0)}$ , $\\delta_{0}\\,=\\,0,\\delta_{1}\\,=\\,(1-\\alpha)/(1+\\alpha)$ , and $W\\,=$ $D^{-1/2}A D^{-1/2}$ is normalized adjacency matrix. Our key strategy for analyzing (10) is to rewrite the updates of $\\mathbf{\\boldsymbol{r}}^{(t)}$ as a nonhomogeneous second-order difference equation (see details in Lemma C.8) ", "page_idx": 6}, {"type": "table", "img_path": "wT2KhEb97a/tmp/9c5cf96b2ff718ee0d7543ced850dc24733b98c08a5f808eda921ac8a1c8c17e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\nr^{(t+1)}-2\\delta_{t+1}W r^{(t)}+\\delta_{t:t+1}r^{(t-1)}=\\sum_{j=0}^{t}\\left((1+\\delta_{j:j+1})\\prod_{r=j+1}^{t}\\delta_{r:r+1}Q r_{{S_{j,t}}}^{(j)}\\right),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where we denote $S_{j,t}=S_{j}\\cap\\cdot\\cdot\\cdot\\cap S_{t-1}\\cap\\overline{{S}}_{t}$ given $t\\ge j\\ge0$ where $\\overline{{S}}_{t}=\\mathcal{V}\\backslash S_{t}$ . In the rest, we define $\\tilde{\\alpha}=(1-\\sqrt{\\alpha})/(1+\\sqrt{\\alpha})$ and recall the eigendecomposition of $D^{-1/2}A{\\cal D}^{-1/2}=V\\Lambda V^{\\top}$ . Based on the above Equ. (11), we have the following key lemma. ", "page_idx": 6}, {"type": "text", "text": "Lemma 4.1. Given $t\\geq1,\\pmb{x}^{(0)}=\\mathbf{0},$ , ${\\pmb x}^{(1)}={\\pmb r}^{(0)}$ . The residual $\\mathbf{\\boldsymbol{r}}^{(t)}$ of LOCCH defined in (10) can be expressed as the following ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{V^{\\top}r^{(t)}=\\delta_{1:t}Z_{t}V^{\\top}r^{(0)}+\\delta_{1:t}t u_{0,t}+2\\sum_{k=1}^{t-1}\\delta_{k+1:t}(t-k)u_{k,t},}}\\\\ &{}&{\\qquad\\qquad\\qquad\\qquad\\dots\\quad.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $Z_{t}$ is a diagonal matrix such that $\\|Z_{t}\\|_{2}\\leq1$ and ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\boldsymbol{u}_{k,t}=\\left\\{\\sum_{j=1}^{t-1}\\frac{\\delta_{2:j}}{t}H_{j,t}\\left(I-\\frac{1-\\alpha}{1+\\alpha}\\Lambda\\right)\\boldsymbol{V}^{\\top}\\boldsymbol{r}_{S_{0,j}}^{(0)}\\qquad i f k=0}\\\\ {\\sum_{j=k}^{t-1}\\frac{\\delta_{k+1:j}}{(t-k)}H_{j,t}\\left(\\frac{1+\\alpha}{1-\\alpha}I-\\Lambda\\right)\\boldsymbol{V}^{\\top}\\boldsymbol{r}_{S_{k,j}}^{(k)}\\quad~i f k\\ge1,}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where ${\\cal H}_{k,t}$ is a diagonal matrix such that $\\|\\pmb{H}_{k,t}\\|_{2}\\leq t-k$ . ", "page_idx": 6}, {"type": "text", "text": "This key lemma essentially captures the process of residual reduction $\\mathbf{\\boldsymbol{r}}^{(t)}$ of LOCCH. Specifically, given current iteration $t$ , we define the running residual reduction rate for $r^{(k)}$ with $k=0,1,2,\\ldots,t-1$ of step $t$ as $\\beta_{k,t}$ , that is, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\beta_{k,t}\\triangleq\\frac{\\Vert\\pmb{u}_{k,t}\\Vert_{2}}{\\Vert\\pmb{r}^{(k)}\\Vert_{2}},\\qquad\\beta_{k}\\triangleq\\operatorname*{max}_{t}\\beta_{k,t}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Note that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\beta_{k,t}\\leq\\underbrace{\\frac{2\\|r_{\\overline{{S}}_{k}}^{(k)}\\|_{2}}{(1-\\alpha)\\|r^{(k)}\\|_{2}}}_{\\approx\\mathcal{O}(\\epsilon)}+\\underbrace{\\sum_{j=k+1}^{t-1}\\frac{4\\tilde{\\alpha}^{j-k}(t-j)}{(1-\\alpha)(t-k)}\\frac{\\|r_{S_{k,j}}^{(k)}\\|_{2}}{\\|r^{(k)}\\|_{2}}}_{\\leq\\frac{4\\tilde{\\alpha}}{(1-\\alpha)(1-\\tilde{\\alpha})}},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where whether the last term can be even smaller depends on $\\|\\boldsymbol{r}_{S_{k,j}}^{(k)}\\|_{2}$ for $S_{k,j}=S_{k}\\cap\\cdot\\cdot\\cdot\\cap S_{j-1}\\cap{\\overline{{S}}}_{j}$ However, we notice that the running geometric mean $\\begin{array}{r}{\\overline{{\\beta}}_{t}\\triangleq(\\prod_{j=0}^{t-1}(1+\\beta_{j}))^{1/t}}\\end{array}$ is even smaller in practice. Based on these observations and the assumption on $\\overline{{\\beta}}_{t}$ , we establish the following theorem. ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.2 (Runtime bound of LOCCH). Given the configuration $\\theta=(\\alpha,\\epsilon,s,\\mathcal{G})$ with $\\alpha\\in(0,1)$ and $\\epsilon\\leq1/d_{s}$ and let $r^{(T)}$ and ${\\pmb x}^{(T)}$ be returned by LOCCH defined in (10) for solving Equ. (3). For $t\\geq1$ , the residual magnitude $\\|\\pmb{r}^{(t)}\\|_{2}$ has the following convergence bound ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\|r^{(t)}\\|_{2}\\leq\\delta_{1:t}\\prod_{j=0}^{t-1}(1+\\beta_{j})y_{t},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $y_{t}$ is a sequence of positive numbers solving $y_{t+1}-2y_{t}+y_{t-1}/((1+\\beta_{t-1})(1+\\beta_{t}))=0$ with $y_{0}=y_{1}=\\|\\pmb{r}^{(0)}\\|_{2}$ . Suppose the geometric mean $\\begin{array}{r}{\\overline{{\\beta}}_{t}\\triangleq(\\prod_{j=0}^{t-1}(1+\\beta_{j}))^{1/t}}\\end{array}$ of $\\beta_{t}$ be such that $\\begin{array}{r}{\\overline{{\\beta}}_{t}=1+\\frac{c\\sqrt{\\alpha}}{1-\\sqrt{\\alpha}}}\\end{array}$ where $c\\in[0,2)$ . There exists a real implementation of (10) such that the runtime $\\mathcal{T}_{\\mathrm{LocCH}}$ is bounded by ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal{T}_{\\mathrm{LocCH}}\\leq\\Theta\\left(\\frac{(1+\\sqrt\\alpha)\\overline{{\\mathrm{vol}}}(S_{T})}{\\sqrt\\alpha(2-c)}\\ln\\frac{2y_{T}}{\\epsilon}\\right).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Golub & Overton [18] considered the approximate Chebyshev method by assuming that the inexact residual is sufficiently smaller than $\\epsilon\\|\\pmb{r}^{(t)}\\|_{2}$ , where $\\epsilon$ must be small enough to ensure convergence. However, this assumption is overly stringent for our case. The novelty of our analysis lies in a more elegant treatment of a parameterized second-order difference equation, allowing us to circumvent this assumption. The nested APGD(\u03f5\u02c6), namely ASPR proposed in Mart\u00ednez-Rubio et al. [37] has runtime complexity $\\tilde{\\mathcal{O}}(|S^{*}|\\widetilde{\\mathrm{\\,vol}}\\left(S^{*}\\right)/\\sqrt{\\alpha}+|S^{*}|\\operatorname{\\,vol}\\left(S^{*}\\right))$ where $S^{*}$ is the opti m al support of $\\begin{array}{r}{\\arg\\operatorname*{min}_{\\pmb{x}}\\left\\{f(\\pmb{x})+\\epsilon\\alpha\\|\\pmb{D}^{1/2}\\pmb{x}\\|_{1}\\right\\}}\\end{array}$ and $\\widetilde{\\mathrm{vol}}(S^{*})=\\mathrm{nnz}\\left(Q_{S^{*},S^{*}}\\right)$ . Although it is difficult to compare our bound to this, one limitation of AS P R is that it assumes to call $\\mathrm{APGD}(\\hat{\\epsilon})$ $\\mathcal{O}(|\\mathcal{S}^{*}|)$ times to finish in the worst case. However, our iteration complexity is $\\tilde{\\mathcal{O}}(1/(\\sqrt{\\alpha}(2-c)))$ . Asymptotically, $c=o(\\sqrt{\\alpha})$ $(\\epsilon\\rightarrow0)$ ), our complexity is $\\tilde{\\mathcal{O}}(1/\\sqrt{\\alpha})$ could be better than $\\tilde{\\mathcal{O}}(|S^{*}|/\\sqrt{\\alpha})$ . Fig. 3 presents a preliminary study on ASPR, indicating that it requires more operations than APPR. ", "page_idx": 7}, {"type": "image", "img_path": "wT2KhEb97a/tmp/a3b0d2f29ec6179176ef3fb2dbe63ce0c991d2a3a20d3a25586d23893a764fba.jpg", "img_caption": ["Figure 3: Comparison of runtime between APPR and ASPR. The setting is the same as in Fig. 1. Left $\\epsilon=10^{-4}$ while $\\scriptstyle{\\frac{1}{n}}$ for right. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "We conclude our analysis by presenting a similar result for the local Heavy Ball (HB). Note the HB method is the one when $\\bar{\\delta_{t}}\\delta_{t+1}\\to\\bar{\\tilde{\\alpha}}^{2}$ where $\\tilde{\\alpha}=(1-\\sqrt{\\alpha})/(1+\\sqrt{\\alpha})$ . Hence, it has similar convergence analyses as to LOCCH shown in Theorem D.8. The LOCHB has the following updates ", "page_idx": 7}, {"type": "table", "img_path": "wT2KhEb97a/tmp/dbd2cf78f8f8eea0e49286af597c33061464271548e9d1ce20b81ff9a7ad0988.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "5 Generalization and Open Problems ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Our framework can be applied to various local methods for large-scale linear systems. Extensions of this framework to other linear systems are detailed in Tab. 2 of Appendix E. More broadly, we consider the feasibility of local methods for solving $Q x=b$ , where $^{b}$ is a sparse vector $(|\\operatorname{supp}(b)|\\ll n)$ and $Q$ is a positive definite, graph-induced matrix with bounded eigenvalues. This leads us to question whether all standard iterative methods can be effectively localized, raising two key questions ", "page_idx": 7}, {"type": "text", "text": "1. Given a graph-induced matrix $Q$ and its spectral radius $\\rho(Q)<1$ , a standard solver $\\boldsymbol{\\mathcal{A}}$ , and the corresponding local evolving process $\\Phi_{\\theta}(S_{t},\\mathbf{x}^{(t)},\\mathbf{r}^{(t)},\\mathrm{Loc}\\mathcal{A})$ , does a localized version of $\\boldsymbol{\\mathcal{A}}$ (over $\\mathcal{S}_{t}$ ) converge and have local runtime bounds? ", "page_idx": 7}, {"type": "text", "text": "2. Based on current analysis, Theorem 4.2 relies on the geometric mean of residual reduction on $\\lVert\\pmb{r}^{(k)}\\rVert_{2}$ being small. How feasible is acceleration within locality constraints? Specifically, a stronger bound could be established for solving Equ. (3) via LOCHB and LOCCH, with a graph-independent bound of ", "page_idx": 7}, {"type": "equation", "text": "$$\nT_{\\mathrm{Loc},A}=\\Theta\\left(\\frac{\\overline{{\\mathrm{vol}}}(S_{T})}{\\sqrt{\\alpha}\\overline{{\\gamma}}_{T}}\\ln\\frac{C}{\\epsilon}\\right)\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Additionally, this work primarily focuses on using first-order neighbors at each iteration. An area for future exploration is generalizing to higher-order neighbors to determine if this leads to faster or more efficient methodologies, which remains an open question. ", "page_idx": 8}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We conduct experiments over 17 graphs to solve (3) and explore the local clustering task. We address the following questions: 1) Can iterative solvers be effectively localized? 2) How does the performance of accelerated local methods compare to non-accelerated ones? 3) Can our proposed methods reduce the number of operations required for local clustering? 6 ", "page_idx": 8}, {"type": "text", "text": "Baselines. We consider four baselines: 1) Conjugate Gradient Method (CGM) as a benchmark to compare local and non-local methods; 2) ISTA, the local method proposed by Fountoulakis et al. [13]; 3) FISTA, the momentum-based local algorithm proposed by Hu [22]; and 4) APPR, the classic local method proposed by Andersen et al. [2]. All methods are implemented in Python 3.10 with the numba library [33]. ", "page_idx": 8}, {"type": "text", "text": "Efficiency of localized algorithms. To compare local solvers to their standard counterparts, we set $\\alpha=0.1$ , randomly select 50 nodes from each graph to serve as $e_{s}$ in (3), and run standard GD, SOR, HB, and CH solvers along with their local counterparts: LOCGD, LOCSOR, LOCHB, and LOCCH. We measure the efficiency by the speedup, defined as the ratio between the runtime of the standard and local solver. The range of $\\epsilon$ is $\\begin{array}{r}{\\bar{\\epsilon}\\in[\\frac{\\alpha}{2(1+\\alpha)d_{s}},10^{-4}/n]}\\end{array}$ . The results, presented in Fig. 4, clearly indicate that our design demonstrates significant speedup, especially around $\\epsilon=1/n$ . Remarkably, they still show better performance even when $\\epsilon\\approx\\bar{1}0^{-4}\\bar{/n}$ (Fig. 5). These results suggest that local solvers are preferred over non-local ones when the precision requirement is in this range. ", "page_idx": 8}, {"type": "image", "img_path": "wT2KhEb97a/tmp/4055d396a56909a85cea97351926c149c7f4071fcac1b65fcb1d6a86445f4353.jpg", "img_caption": ["Figure 4: The speedup of local solvers as a function of $\\epsilon$ . The vertical line is $\\epsilon=1/n$ . "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Comparison with local baselines and CGM. We   \nnext compare our three accelerated methods with   \nfour baselines. Fig. 5 presents the $\\ell_{1}$ -estimation er  \nror in terms of the number of operations (quanti  \nfied as $t\\cdot{\\overline{{\\operatorname{vol}}}}(S_{t}))$ ) executed. It is evident that our   \nthree solvers use significantly fewer operations com  \npared to CGM and the other three local methods.   \nAgain, due to maintaining a nondecreasing set of ac- Figure 5: Estimation error as a function of ", "page_idx": 8}, {"type": "image", "img_path": "wT2KhEb97a/tmp/394c8bbcd3a304f8c562cdea4a48fb891df45273a4b4cfa6f641c271a02eee61.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "tive nodes, ISTA and FISTA require more operations operations required. (\u03f5 = 10\u22124/n) ", "page_idx": 8}, {"type": "text", "text": "than the locally evolving set process. Ours are more efficient than APPR, where $\\pmb{r}^{(0)}=e_{s}$ is used. ", "page_idx": 8}, {"type": "image", "img_path": "wT2KhEb97a/tmp/0b6c5db5b107f11004e7cc73e6165484a9cec73c33d03b862b0d46e9535695e6.jpg", "img_caption": ["Figure 6: Performance on large-scale graphs "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Efficiency in terms of $\\alpha$ and huge-graph tests. We demonstrate the performance of local solvers in terms of different $\\alpha$ ranging from 0.005 to 0.25. Interestingly, in Fig. 13, LOCGD show faster convergence when $\\alpha$ is small; this may be because of the advantages of monotonicity properties, which is not present in the accelerated methods. However, in other regions of $\\alpha$ , accelerated methods are faster. We also tested local solvers on two large-scale graphs where papers100M has 111M nodes and 1.6B edges while com-friendster has 65M nodes with 1.8B edges. Results are shown in Fig. 6; compared with current default local methods, it is several times faster, especially on ogbn-papers100M. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Case study on local clustering. Following the experimental setup in Fountoulakis et al. [13], we consider the task of local clustering on 15 graphs. As partially demonstrated in Tab. 1, compared with APPR and FISTA, LOCSOR uses the least operations and is the fastest, demonstrating the advantages of our proposed local solvers. ", "page_idx": 9}, {"type": "table", "img_path": "wT2KhEb97a/tmp/988f7451d2aa7b70f17d00ce554cd9828d8b7d9bb6710c7d5781bfe61828b6fa.jpg", "table_caption": ["Table 1: Operations/runtime comparison on local clustering. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "7 Limitations and Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Our proposed algorithms may have the following limitations: 1) When $\\alpha$ is small, the acceleration effect partially disappears, as observed in Fig. 13. This may be due to the limitations of global counterparts, where the residual may not decrease early; 2) Our new accelerated bound for LocCH depends on an empirically reasonable assumption of residual reduction but lacks theoretical justification. ", "page_idx": 9}, {"type": "text", "text": "We propose using a new locally evolving set process framework to characterize algorithm locality and demonstrate that several standard iterative solvers can be effectively localized, significantly speeding up current local solvers. Our local methods could be efficiently implemented into GPU architecture to accelerate the training of GNNs such as APPNP [27] and PPRGo [7]. We also offer open problems in developing faster local methods. It is worth exploring whether subsampling active nodes stochastically or using different queue strategies (priority rather than FIFO) could help speed up the framework further. It also remains interesting to see how to design local algorithms for conjugate direction-based methods such as CGM. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The authors would like to thank the anonymous reviewers for their helpful comments. The work of Baojian Zhou is sponsored by Shanghai Pujiang Program (No. 22PJ1401300) and the National Natural Science Foundation of China (No. KRH2305047). The work of Deqing Yang is supported by Chinese NSF Major Research Plan No.92270121. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Alon, N., Rubinfeld, R., Vardi, S., and Xie, N. Space-efficient local computation algorithms. In Proceedings of the twenty-third annual ACM-SIAM symposium on Discrete Algorithms (SODA), pp. 1132\u20131139. SIAM, 2012.   \n[2] Andersen, R., Chung, F., and Lang, K. Local graph partitioning using PageRank vectors. In 47th Annual IEEE Symposium on Foundations of Computer Science (FOCS), 2006.   \n[3] Andersen, R., Gharan, S. O., Peres, Y., and Trevisan, L. Almost optimal local graph clustering using evolving sets. Journal of the ACM (JACM), 63(2):1\u201331, 2016.   \n[4] Anikin, A., Gasnikov, A., Gornov, A., Kamzolov, D., Maximov, Y., and Nesterov, Y. Efficient numerical methods to solve sparse linear equations with application to PageRank. Optimization Methods and Software, pp. 1\u201329, 2020.   \n[5] Beck, A. and Teboulle, M. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM J. Imaging Sci., 2:183\u2013202, 2009.   \n[6] Berkhin, P. Bookmark-coloring algorithm for personalized pagerank computing. Internet Mathematics, 3(1):41\u201362, 2006.   \n[7] Bojchevski, A., Klicpera, J., Perozzi, B., Kapoor, A., Blais, M., R\u00f3zemberczki, B., Lukasik, M., and G\u00fcnnemann, S. Scaling graph neural networks with approximate pagerank. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD), 2020.   \n[8] Boyd, S., Diaconis, P., and Xiao, L. Fastest mixing markov chain on a graph. SIAM review, 46 (4):667\u2013689, 2004.   \n[9] Chen, Z., Guo, X., Zhou, B., Yang, D., and Skiena, S. Accelerating personalized PageRank vector computation. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD), pp. 262\u2013273, 2023.   \n[10] Chung, F. The heat kernel as the pagerank of a graph. Proceedings of the National Academy of Sciences (PNAS), 104(50):19735\u201319740, 2007.   \n[11] d\u2019Aspremont, A., Scieur, D., Taylor, A., et al. Acceleration methods. Foundations and Trends\u00ae in Optimization, 5(1-2):1\u2013245, 2021.   \n[12] Fountoulakis, K. and Yang, S. Open problem: Running time complexity of accelerated $\\ell_{1}$ - regularized PageRank. In Conference on Learning Theory (COLT), 2022.   \n[13] Fountoulakis, K., Roosta-Khorasani, F., Shun, J., Cheng, X., and Mahoney, M. W. Variational perspective on local graph clustering. Mathematical Programming, 174(1):553\u2013573, 2019.   \n[14] Fountoulakis, K., Wang, D., and Yang, S. p-norm flow diffusion for local graph clustering. In ICML, 2020.   \n[15] Gasteiger, J., Wei\u00dfenberger, S., and G\u00fcnnemann, S. Diffusion improves graph learning. In Advances in neural information processing systems (NeurIPS), 2019.   \n[16] Gleich, D. and Mahoney, M. Anti-differentiating approximation algorithms: A case study with min-cuts, spectral, and flow. In International Conference on Machine Learning, pp. 1018\u20131025. PMLR, 2014.   \n[17] Gleich, D. F. PageRank beyond the web. siam REVIEW, 57(3):321\u2013363, 2015.   \n[18] Golub, G. H. and Overton, M. L. The convergence of inexa v and richardson iterative methods for solving linear systems. Numerische Mathematik, 53(5):571\u2013593, 1988.   \n[19] Golub, G. H. and Van Loan, C. F. Matrix computations (4th Edition). JHU press, 2013.   \n[20] Guo, X., Zhou, B., and Skiena, S. Subset node representation learning over large dynamic graphs. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, pp. 516\u2013526, 2021.   \n[21] Guo, X., Zhou, B., and Skiena, S. Subset node anomaly tracking over large dynamic graphs. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 475\u2013485, 2022.   \n[22] Hu, C. Local graph clustering using l1-regularized PageRank algorithms. Master\u2019s thesis, University of Waterloo, 2020.   \n[23] Hu, W., Fey, M., Zitnik, M., Dong, Y., Ren, H., Liu, B., Catasta, M., and Leskovec, J. Open graph benchmark: Datasets for machine learning on graphs. Advances in neural information processing systems, 33:22118\u201322133, 2020.   \n[24] Johnson, R. and Zhang, T. Graph-based semi-supervised learning and spectral kernel design. IEEE Transactions on Information Theory, 54(1):275\u2013288, 2008.   \n[25] Kapralov, M., Lattanzi, S., Nouri, N., and Tardos, J. Efficient and local parallel random walks. Advances in Neural Information Processing Systems (NeurIPS), 2021.   \n[26] Klamkin, M. and Newman, D. J. Extensions of the weierstrass product inequalities. Mathematics Magazine, 43(3):137\u2013141, 1970.   \n[27] Klicpera, J., Bojchevski, A., and G\u00fcnnemann, S. Predict then propagate: Graph neural networks meet personalized pagerank. In International Conference on Learning Representations (ICLR), 2019.   \n[28] Kloster, K. and Gleich, D. F. A nearly-sublinear method for approximating a column of the matrix exponential for matrices from large, sparse networks. In Algorithms and Models for the Web Graph: 10th International Workshop, WAW 2013, Cambridge, MA, USA, December 14-15, 2013, Proceedings 10, pp. 68\u201379. Springer, 2013.   \n[29] Kloster, K. and Gleich, D. F. Heat kernel based community detection. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD), 2014.   \n[30] Kloumann, I. M. and Kleinberg, J. M. Community membership identification from small seed sets. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD), 2014.   \n[31] Koutis, I., Miller, G. L., and Peng, R. A nearly-m log n time solver for sdd linear systems. In 2011 IEEE 52nd Annual Symposium on Foundations of Computer Science (FOCS), pp. 590\u2013598. IEEE, 2011.   \n[32] \u0141 a\u02dbcki, J., Mitrovic\u00b4, S., Onak, K., and Sankowski, P. Walking randomly, massively, and efficiently. In Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing (STOC), pp. 364\u2013377, 2020.   \n[33] Lam, S. K., Pitrou, A., and Seibert, S. Numba: A llvm-based python jit compiler. In Proceedings of the Second Workshop on the LLVM Compiler Infrastructure in HPC, pp. 1\u20136, 2015.   \n[34] Leskovec, J., Lang, K. J., Dasgupta, A., and Mahoney, M. W. Community structure in large networks: Natural cluster sizes and the absence of large well-defined clusters. Internet Mathematics, 6(1):29\u2013123, 2009.   \n[35] Leventhal, D. and Lewis, A. S. Randomized methods for linear constraints: convergence rates and conditioning. Mathematics of Operations Research, 35(3):641\u2013654, 2010.   \n[36] Mahoney, M. W., Orecchia, L., and Vishnoi, N. K. A local spectral method for graphs: With applications to improving graph partitions and exploring data graphs locally. The Journal of Machine Learning Research (JMLR), 13(1):2339\u20132365, 2012.   \n[37] Mart\u00ednez-Rubio, D., Wirth, E., and Pokutta, S. Accelerated and sparse algorithms for approximate personalized PageRank and beyond. In Proceedings of Thirty Sixth Conference on Learning Theory (COLT), volume 195 of Proceedings of Machine Learning Research, pp. 2852\u20132876. PMLR, 2023.   \n[38] Morris, B. and Peres, Y. Evolving sets and mixing. In Proceedings of the Thirty-Fifth Annual ACM Symposium on Theory of Computing (STOC), pp. 279\u2013286, New York, NY, USA, 2003. Association for Computing Machinery.   \n[39] Nutini, J., Schmidt, M., Laradji, I., Friedlander, M., and Koepke, H. Coordinate descent converges faster with the gauss-southwell rule than random selection. In ICML, pp. 1632\u20131641. PMLR, 2015.   \n[40] Polyak, B. T. Introduction to optimization. New York, Optimization Software\u201e 1987.   \n[41] Rakhlin, A. and Sridharan, K. Efficient online multiclass prediction on graphs via surrogate losses. In Artificial Intelligence and Statistics, pp. 1403\u20131411. PMLR, 2017.   \n[42] Rubinfeld, R. and Shapira, A. Sublinear time algorithms. SIAM Journal on Discrete Mathematics, 25(4):1562\u20131588, 2011.   \n[43] Saad, Y. Iterative methods for sparse linear systems. SIAM, 2003.   \n[44] Schaub, M. T., Benson, A. R., Horn, P., Lippner, G., and Jadbabaie, A. Random walks on simplicial complexes and the normalized hodge 1-laplacian. SIAM Review, 62(2):353\u2013391, 2020.   \n[45] Spielman, D. A. Algorithms, graph theory, and linear equations in laplacian matrices. In Proceedings of the International Congress of Mathematicians 2010 (ICM 2010) (In 4 Volumes) Vol. I: Plenary Lectures and Ceremonies Vols. II\u2013IV: Invited Lectures, pp. 2698\u20132722. World Scientific, 2010.   \n[46] Spielman, D. A. and Teng, S.-H. Nearly linear time algorithms for preconditioning and solving symmetric, diagonally dominant linear systems. SIAM Journal on Matrix Analysis and Applications, 35(3):835\u2013885, 2014.   \n[47] Stevi\u00b4c, S. Bounded solutions to nonhomogeneous linear second-order difference equations. Symmetry, 9(10):227, 2017.   \n[48] Teng, S.-H. et al. Scalable algorithms for data and network analysis. Foundations and Trends\u00ae in Theoretical Computer Science, 12(1\u20132):1\u2013274, 2016.   \n[49] Tseng, P. and Yun, S. A coordinate gradient descent method for nonsmooth separable minimization. Mathematical Programming, 117:387\u2013423, 2009.   \n[50] Tu, S., Venkataraman, S., Wilson, A. C., Gittens, A., Jordan, M. I., and Recht, B. Breaking locality accelerates block gauss-seidel. In ICML, pp. 3482\u20133491. PMLR, 2017.   \n[51] Vishnoi, N. K. et al. Lx $\\equiv$ b. Foundations and Trends in Theoretical Computer Science, 8(1\u20132): 1\u2013141, 2013.   \n[52] Wang, J.-K., Lin, C.-H., and Abernethy, J. D. A modular analysis of provable acceleration via Polyak\u2019s momentum: Training a wide ReLU network and a deep linear network. In ICML, pp. 10816\u201310827. PMLR, 2021.   \n[53] Wu, F., Souza, A., Zhang, T., Fifty, C., Yu, T., and Weinberger, K. Simplifying graph convolutional networks. In ICML, pp. 6861\u20136871. PMLR, 2019.   \n[54] Yin, H., Benson, A. R., Leskovec, J., and Gleich, D. F. Local higher-order graph clustering. In Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining (KDD), 2017.   \n[56] Zeng, H., Zhang, M., Xia, Y., Srivastava, A., Malevich, A., Kannan, R., Prasanna, V., Jin, L., and Chen, R. Decoupling the depth and scope of graph neural networks. Advances in Neural Information Processing Systems, 34:19665\u201319679, 2021.   \n[57] Zhou, B., Sun, Y., and Harikandeh, R. B. Fast online node labeling for very large graphs. In ICML, pp. 42658\u201342697. PMLR, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendix / supplemental material ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Notations and Proof of Lemma 2.1 16   \nA.1 List of Notations 16   \nA.2 Proof of Lemma 2.1 . 16 ", "page_idx": 14}, {"type": "text", "text": "B Local Iterative Methods via Evolving Set Process 18 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Local Variant of GS-SOR and Proof of Lemma 3.2 18   \nB.2 LOCSOR and Proof of Theorem 3.3 20   \nB.3 Optimal GS-SOR and Proof of Corollary 3.4 . 22   \nB.4 LOCGD and Proof of Theorem 3.5 23 ", "page_idx": 14}, {"type": "text", "text": "C Local Chebyshev Method - LOCCH 25 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "C.1 Nonhomogeneous of Second-order Difference Equation . 25   \nC.2 Properties on Ratio of Chebyshev Polynomials 28   \nC.3 Standard Chebyshev (CH) Method and Proof of Theorem C.6 29   \nC.4 Residual Updates of LOCCH and Proof of Lemma 4.1 30   \nC.5 Convergence of LOCCH and Proof of Theorem 4.2 34   \nC.6 Implementation of LOCCH 37 ", "page_idx": 14}, {"type": "text", "text": "D Local Heavy-Ball Method - LOCHB 37 ", "page_idx": 14}, {"type": "text", "text": "D.1 Standard HB and Proof Theorem D.2 37   \nD.2 Residual Updates of LOCHB and Proof of Theorem D.6 40   \nD.3 Convergence of LOCHB of Proof of Theorem D.8 . . 43   \nD.4 Implementation of LOCHB 45   \nE Instances of Sparse Linear Systems 45   \nE.1 Table of Popular Graph-induced Linear Systems 45 ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "F Experimental Details and Missing Results 46 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "F.1 Datasets and Preprocessing 46   \nF.2 Problems Settings and Baseline Methods 46   \nF.3 Full results of Fig. 15 4 5 6 . . 47   \nF.4 Results on local clustering 52 ", "page_idx": 14}, {"type": "text", "text": "G Related work 53 ", "page_idx": 14}, {"type": "text", "text": "A Notations and Proof of Lemma 2.1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.1 List of Notations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In the rest of the appendix, we use the following notations: ", "page_idx": 15}, {"type": "table", "img_path": "wT2KhEb97a/tmp/d6a427615f8b0fededf10d6ad417379205ee5fc53732ce547d3d3cab68bf49e8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "A.2 Proof of Lemma 2.1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Lemma 2.1 (Runtime bound of APPR [2]). Given $\\alpha\\in(0,1)$ and the precision $\\epsilon\\leq1/d_{s}$ for node $s\\in\\mathcal{V}$ with $p\\gets\\mathbf{0},z\\gets e_{s}$ at the initial, APPR $(\\mathcal{G},\\epsilon,\\alpha,s)$ defined in (2) returns an estimate $\\textbf{\\emph{p}}$ of $\\pi$ . There exists a real implementation of (2) (e.g., Algo. 2) such that the runtime TAPPR satisfies ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{T_{\\mathrm{APPR}}\\leq\\Theta\\left(1/(\\alpha\\epsilon)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Furthermore, the estimate ${\\hat{\\pi}}:=p$ satisfies $||D^{-1}(\\hat{\\pi}-\\pi)||_{\\infty}\\leq\\epsilon\\,a n d\\,\\mathrm{vol}(\\mathrm{supp}(\\hat{\\pi}))\\leq2/((1-\\alpha)\\epsilon).$ ", "page_idx": 15}, {"type": "text", "text": "Proof. To find an upper bound of $\\mathcal{T}_{\\mathrm{APPR}}$ , we add a time index for all active nodes $u_{1},u_{2},\\ldots,u_{t}$ processed in APPR of Algo. 2, a real implementation of (2). The parameter $t$ is the number of active nodes processed, and $u_{i}$ is the node dequeued at time $i$ . So, updates of $\\pmb{p}$ and $_{\\textit{z}}$ are from $(\\mathbf{0},e_{s})=(\\pmb{p}^{(\\bar{0})},z^{(0)})$ to $(\\pmb{p}^{(t)},z^{(t)})$ as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n(p^{(0)},z^{(0)})\\quad\\xrightarrow{u_{1}}\\quad(p^{(1)},z^{(1)})\\quad\\xrightarrow{u_{2}}\\quad(p^{(2)},z^{(2)})\\quad\\cdot\\cdot\\cdot\\quad\\xrightarrow{u_{t}}\\quad(z^{(t)},p^{(t)}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "For each active $u_{i}$ , the updates of $(p,z)$ by definition can be represented as ", "page_idx": 15}, {"type": "equation", "text": "$$\np^{(i)}=p^{(i-1)}+\\alpha z_{u_{i}}^{(i-1)}\\cdot e_{u_{i}},\\quad z^{(i)}=z^{(i-1)}-\\frac{(1+\\alpha)z_{u_{i}}^{(i-1)}}{2}e_{u_{i}}+\\frac{(1-\\alpha)z_{u_{i}}^{(i-1)}}{2}A D^{-1}e_{u_{i}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since $z^{(0)}=e_{s}\\ge\\mathbf{0}$ , then $z^{(i)}\\geq0$ and $\\pmb{p}^{(i)}\\geq0$ for all $i$ by induction. Note that $\\|A D^{-1}e_{u_{i}}\\|_{1}=1$ , we have the following relation from the updates of $_{z}$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\|z^{(i)}\\|_{1}=\\|z^{(i-1)}\\|_{1}-\\alpha z_{u_{i}}^{(i-1)}\\iff z_{u_{i}}^{(i-1)}=\\frac{\\|z^{(i-1)}\\|_{1}-\\|z^{(i)}\\|_{1}}{\\alpha}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Note $\\epsilon d_{u_{i}}\\le z_{u_{i}}^{(i-1)}$ for each active $u_{i}$ . Summing the above equation over $i=1,2,\\ldots,t$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{t}\\epsilon d_{u_{i}}\\leq\\sum_{i=1}^{t}z_{u_{i}}^{(i-1)}=\\sum_{i=1}^{t}\\left(\\frac{\\|z^{(i-1)}\\|_{1}-\\|z^{(i)}\\|_{1}}{\\alpha}\\right)=\\frac{\\|z^{(0)}\\|_{1}-\\|z^{(t)}\\|_{1}}{\\alpha}\\leq\\frac{\\|z^{(0)}\\|_{1}}{\\alpha}=\\frac{1}{\\alpha},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where note $\\|z^{(0)}\\|_{1}\\,=\\,\\|e_{s}\\|_{1}\\,=\\,1$ by the initial condition. Since $\\textstyle\\sum_{i=1}^{t}d_{u_{i}}$ exactly captures the number of operations needed, the runtime of Algo. 2 is then bounde d  as ", "page_idx": 16}, {"type": "equation", "text": "$$\nT_{\\mathrm{APPR}}=\\Theta\\left(\\sum_{i=1}^{t}d_{u_{i}}\\right)\\leq\\Theta\\left(\\frac{1}{\\alpha\\epsilon}\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "To check the quality of estimate $\\textbf{\\emph{p}}$ , using the updates of $\\pmb{p}^{(i)}$ and summing over all $i$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{\\Lambda}^{(t)}=\\alpha\\sum_{i=1}^{t}z_{u_{i}}^{(i-1)}e_{u_{i}}=\\underbrace{\\alpha\\left(\\frac{(1+\\alpha)}{2}I-\\frac{(1-\\alpha)}{2}A D^{-1}\\right)^{-1}}_{\\mathbf{\\Phi}\\mathbf{H}}\\sum_{i=1}^{t}\\left(z^{(i-1)}-z^{(i)}\\right)=\\pi-\\mathbf{H}z^{(t)},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\mathbf{\\delta}\\pi$ is the PPR matrix. The above gives us $\\pmb{\\pi}-\\pmb{p}^{(t)}=\\pmb{\\Pi}\\cdot\\pmb{z}^{(t)}$ . Since $\\mathcal{G}$ is undirected, the $\\mathbf{\\delta}\\pi$ matrix satisfies $\\pi_{v}[u]=(d_{u}/d_{v})\\pi_{u}[v]$ where $\\pi_{v}[u]$ is the $u$ -th element of PPR vector of sourcing node $v$ . Consider each $u$ -th element of $\\boldsymbol{\\Pi}\\cdot\\boldsymbol{z}^{(t)}$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n(\\Pi\\cdot z^{(t)})_{u}=\\sum_{v\\in\\mathcal{V}}z_{v}^{(t)}\\cdot\\pi_{v}[u]=\\sum_{v\\in\\mathcal{V}}z_{v}^{(t)}\\cdot\\frac{d_{u}}{d_{v}}\\pi_{u}[v]\\leq\\epsilon d_{u}\\sum_{v\\in\\mathcal{V}}\\pi_{u}[v]=\\epsilon d_{u},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the last equality is due to $\\begin{array}{r}{\\sum_{v\\in\\mathcal{V}}\\pi_{u}[v]=1}\\end{array}$ . Hence, $(\\pi-p^{(t)})_{u}=(\\Pi\\cdot z^{(t)})_{u}\\leq\\epsilon d_{u}$ , which indicates $\\|D^{-1}(p^{(t)}-\\pmb{\\pi})\\|_{\\infty}\\leq\\epsilon.$ . To see the bound of $\\operatorname{\\boldsymbol{v}}\\mathrm{ol}\\big(\\operatorname{supp}(\\boldsymbol{p}^{(t)})\\big)$ , note for any $u\\in\\mathrm{supp}(p^{(t)})$ , it was an active node and there was at least $\\tilde{z}_{u}(1-\\alpha)/2$ remain in $u$ -th entry of $\\boldsymbol{z}^{(t)}$ where we denote $\\tilde{z}_{u}$ as the residual before the last push operation of node $u$ ; hence ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{u\\in\\mathrm{supp}(p)}d_{u}\\le\\sum_{u\\in\\mathrm{supp}(p)}\\frac{\\tilde{z}_{u}}{\\epsilon}=\\sum_{u\\in\\mathrm{supp}(p)}\\frac{\\tilde{z}_{u}(1-\\alpha)/2}{\\epsilon(1-\\alpha)/2}\\le\\frac{\\sum_{u\\in\\mathrm{supp}(p)}z_{u}^{(t)}}{\\epsilon(1-\\alpha)/2}\\le\\frac{2}{(1-\\alpha)\\epsilon}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "$\\mathbf{Algo.}\\ 2\\ \\mathrm{APPR}(\\alpha,\\epsilon,s,\\mathcal{G})$ via FIFO Queue ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "1: Initialize: $p\\gets\\mathbf{0},z\\gets e_{s},\\mathcal{Q}\\gets\\{*,s\\},t=-1$   \n2: while true do   \n3: $u\\gets\\mathcal{Q}$ .dequeue()   \n4: if $\\mathbf{u}==^{*}$ then   \n5: if $\\mathcal{Q}=\\emptyset$ then   \n6: break   \n7: $t\\gets t+1$ // Starting time of $\\mathcal{S}_{t}$   \n8: Q.enqueue(\\*) // Marker for next $\\boldsymbol{S}_{t+1}$   \n9: continue   \n10: $\\tilde{z}\\gets z_{u}$   \n11: pu pu + \u03b1  z\u02dc   \n12: zu z\u02dc (1 \u03b1)/2   \n13: for v (u) do   \n14: zv \u2190zv + (1\u22122\u03b1)\u00b7dz\u02dc   \n15: if zv \u03f5dv and $v\\not\\in{\\mathcal{Q}}$ then   \n16: Q.enqueue(v)   \n17: if $z_{u}\\ge\\epsilon d_{u}$ and $u\\notin\\mathcal{Q}$ then   \n18: Q.enqueue(u)   \n19: return $\\pmb{p}$ ", "page_idx": 16}, {"type": "text", "text": "Indeed, Lemma 2.1 is a special case of Theorem 1 in [2]. The proof outlined above adheres to the key strategy demonstrated in that theorem, which involves exploring the monotonicity and nonnegativity of $_{\\textit{z}}$ . ", "page_idx": 16}, {"type": "text", "text": "The real implementation of APPR, as shown in Algo. 2, presents a typical queue-based method. It has monotonic properties during the updates of $\\pmb{p}$ and $_{\\textit{z}}$ (Lines 10-16 of Algo. 2). It also holds element-wise that $\\textbf{\\textit{p}}\\geq\\textbf{\\textit{0}}$ and $\\ensuremath{\\varepsilon}\\mathrm{~\\ensuremath~{~\\vert~}~}\\ensuremath{0}$ . The operations of $\\mathcal{Q}$ .enqueue $(u)$ , $\\mathcal{Q}$ .dequeue(), and $v\\not\\in{\\mathcal{Q}}$ are all in $\\mathcal{O}(1)$ . Line 4 to Line 9 is to design the marker for distinguishing between $\\mathcal{S}_{t}$ and $\\boldsymbol{S}_{t+1}$ . If all active nodes are processed and no more active nodes are added into $\\mathcal{Q}$ , then $\\mathcal{Q}$ will be empty, and finally, the algorithm returns an estimate $\\textbf{\\emph{p}}$ of $\\pi$ . ", "page_idx": 16}, {"type": "text", "text": "B Local Iterative Methods via Evolving Set Process ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Justification of an equivalent condition. We make the justification of an equivalent stop condition for solving (3). Note we require a local solver to return an estimate $\\hat{\\pi}$ satisfies ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|D^{-1}\\left(\\hat{\\pmb{\\pi}}-\\pmb{\\pi}\\right)\\|_{\\infty}\\leq\\epsilon\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Since we define $Q x=b$ as $\\begin{array}{r}{\\left(I-\\frac{1-\\alpha}{1+\\alpha}D^{-1/2}A D^{-1/2}\\right)x=\\frac{2\\alpha}{1+\\alpha}D^{-1/2}e_{s}.}\\end{array}$ With $\\pmb{r}^{(t)}=\\pmb{b}-\\pmb{Q}\\pmb{x}^{(t)}$ and the stop condition ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|D^{-1/2}\\pmb{r}^{(t)}\\|_{\\infty}\\leq\\frac{2\\alpha\\epsilon}{1+\\alpha}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "ensures the estimate $\\hat{\\pmb{\\pi}}={\\pmb D}^{1/2}{\\pmb x}^{(t)}$ satisfies (14). To see this, since $\\pmb{\\pi}=\\pmb{D}^{1/2}\\pmb{x}^{*}$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|D^{-1}\\left(\\hat{\\boldsymbol{\\pi}}-\\boldsymbol{\\pi}\\right)\\|_{\\infty}=\\|D^{-1/2}(\\boldsymbol{x}^{(t)}-\\boldsymbol{x}^{*})\\|_{\\infty}}&{}\\\\ {=\\|D^{-1/2}(Q^{-1}b-Q^{-1}r^{(t)}-Q^{-1}b)\\|_{\\infty}}&{}\\\\ {=\\|D^{-1/2}Q^{-1}D^{1/2}D^{-1/2}r^{(t)}\\|_{\\infty}}&{}\\\\ {\\le\\|D^{-1/2}Q^{-1}D^{1/2}\\|_{\\infty}\\cdot\\|D^{-1/2}r^{(t)}\\|_{\\infty}}&{}\\\\ {\\le\\|D^{-1/2}Q^{-1}D^{1/2}\\|_{\\infty}\\cdot\\frac{2\\alpha\\epsilon}{1+\\alpha}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\begin{array}{r}{D^{-1/2}Q^{-1}D^{1/2}=(I-\\frac{1-\\alpha}{1+\\alpha}D^{-1}A)^{-1}=\\sum_{i=0}^{\\infty}(\\frac{1-\\alpha}{1+\\alpha}D^{-1}A)^{i}}\\end{array}$ . This leads to ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\left\\lvert|D^{-1}\\left(\\hat{\\pmb{\\pi}}-\\pmb{\\pi}\\right)\\right\\rvert|_{\\infty}\\leq\\big\\|\\displaystyle\\sum_{i=0}^{\\infty}(\\frac{1-\\alpha}{1+\\alpha}D^{-1}\\pmb{A})^{i}\\big\\|_{\\infty}\\cdot\\frac{2\\alpha\\epsilon}{1+\\alpha}}\\\\ {\\displaystyle\\leq\\frac{1+\\alpha}{2\\alpha}\\cdot\\frac{2\\alpha\\epsilon}{1+\\alpha}}\\\\ {\\displaystyle=\\epsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "B.1 Local Variant of GS-SOR and Proof of Lemma 3.2 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The Gauss-Seidel Successive Over-Relaxation (GS-SOR) solver (see Section 11.2.7 of Golub & Van Loan [19]) for the linear system $M\\pi=s$ via the following forward substitution ", "page_idx": 17}, {"type": "equation", "text": "$$\np_{i}^{(t+1)}=\\omega\\left(s_{i}-\\sum_{j=1}^{i-1}M_{i j}p_{j}^{(t+1)}-\\sum_{j=i+1}^{n}M_{i j}p_{j}^{(t)}\\right)/M_{i i}+(1-\\omega)p_{i}^{(t)},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\pmb{p}$ is updated from $\\pmb{p}^{(t)}$ to $\\pmb{p}^{(t+1)}$ . When the relaxation parameter $\\omega=1$ , GS-SOR reduces to the standard GS method. Equivalently, let $\\Delta_{i}=(i-1)/n$ for $i=1,2,\\dots,n$ , then GS-SOR updates can be sequentially represented as ", "page_idx": 17}, {"type": "equation", "text": "$$\np^{(t+\\Delta_{i+1})}\\gets p^{(t+\\Delta_{i})}+\\frac{\\omega}{M_{i i}}\\left(s_{i}-\\sum_{j=1}^{i-1}M_{i j}p_{j}^{(t+\\Delta_{i})}-\\sum_{j=i}^{n}M_{i j}p_{j}^{(t+\\Delta_{i})}\\right)\\cdot e_{i}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, it is natural to define the following local variant of GS-SOR. ", "page_idx": 17}, {"type": "text", "text": "Definition B.1 (Local variant of GS-SOR). Consider the linear system $M\\pi=s$ . For $t\\geq0$ , we are given an active node set ${\\cal S}_{t}=\\left\\{u_{1},u_{2},\\dots,u_{|{\\cal S}_{t}|}\\right\\}$ and let $\\Delta_{i}=(i-1)/|S_{t}|$ for $i=1,2,\\dots,|S_{t}|$ , providing $\\omega\\in(0,2)$ , it is natural to define the local variant of GS-SOR as follows: ", "page_idx": 17}, {"type": "text", "text": "for $u_{i}$ in $S_{t}:=\\left\\{u_{1},u_{2},\\dots,u_{|S_{t}|}\\right\\}\\epsilon$ o : ", "page_idx": 17}, {"type": "equation", "text": "$$\np^{(t+\\Delta_{i+1})}\\gets p^{(t+\\Delta_{i})}+\\frac{\\omega}{M_{u_{i}u_{i}}}\\left(s_{u_{i}}-\\sum_{j=1}^{i-1}M_{u_{i}u_{j}}p_{u_{j}}^{(t+\\Delta_{i})}-\\sum_{j=i}^{n}M_{u_{i}u_{j}}p_{u_{j}}^{(t+\\Delta_{i})}\\right)\\cdot e_{u_{i}},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\mathcal{S}_{t}\\subseteq\\mathcal{V}$ . When $\\omega=1$ and $S_{t}=\\mathcal{V}=\\{1,2,\\ldots,n\\}$ , it reduces to the standard GS. ", "page_idx": 17}, {"type": "text", "text": "We use the above definition to show APPR is a local variant of GS-SOR as the following. ", "page_idx": 18}, {"type": "text", "text": "Lemma 3.2 (New local evolving-based bound for APPR). Let $\\begin{array}{r}{M=\\alpha^{-1}\\big(I-\\frac{1-\\alpha}{2}\\left(I+A D^{-1}\\right)\\big)}\\end{array}$ and $\\pmb{s}=e_{s}$ . The linear system $M\\pi=s$ is equivalent to Equ. (1). Given $\\pmb{p}^{(0)}=\\mathbf{0}$ , $z^{(0)}=e_{s}$ with $\\omega\\in(0,2)$ , the local variant of GS-SOR (15) for $M\\pi=s$ can be formulated as ", "page_idx": 18}, {"type": "equation", "text": "$$\np^{(t+\\Delta_{i+1})}\\gets p^{(t+\\Delta_{i})}+\\frac{\\omega z_{u_{i}}^{(t+\\Delta_{i})}}{M_{u_{i}u_{i}}}e_{u_{i}},\\quad z^{(t+\\Delta_{i+1})}\\gets z^{(t+\\Delta_{i})}-\\frac{\\omega z_{u_{i}}^{(t+\\Delta_{i})}}{M_{u_{i}u_{i}}}M e_{u_{i}},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $u_{i}$ is an active node in $\\boldsymbol{S}_{t}$ satisfying $z_{u_{i}}\\ge\\epsilon d_{u_{i}}$ and $\\Delta_{i}=(i-1)/|S_{t}|$ . Furthermore, when $\\textstyle\\omega={\\frac{1+\\alpha}{2}}$ , this method reduces to APPR given i n (7), and there exists a real implementation (Aglo. 2) of APPR such that the runtime $\\mathcal{T}_{\\mathrm{A}}$ PPR is bounded by ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{T}_{\\mathrm{APPR}}\\leq\\frac{\\overline{{\\operatorname{vol}}}(\\mathcal{S}_{T})}{\\alpha\\hat{\\gamma}_{T}}\\ln\\frac{C_{T}}{\\epsilon},\\,w h e r e\\,\\frac{\\overline{{\\operatorname{vol}}}(\\mathcal{S}_{T})}{\\hat{\\gamma}_{T}}\\leq\\frac{1}{\\epsilon},\\,C_{T}=\\frac{2}{(1-\\alpha)|\\mathcal{Z}_{T}|},\\hat{\\gamma}_{T}\\stackrel{\\Delta}{=}\\frac{1}{T}\\sum_{t=0}^{T-1}\\left\\{\\frac{\\sum_{i=1}^{|\\mathcal{S}_{t}|}|{\\boldsymbol{z}}_{i}^{(t+\\Delta_{i})}|}{\\|{\\boldsymbol{z}}^{(t)}\\|_{1}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. Note that $M\\pi\\,=\\,s$ is equivalent to Equ. (1). We first rewrite $\\pmb{p}^{(t+\\Delta_{i+1})}$ in terms of the residual $_{z}$ . The residual $_{z}$ at time $t+\\Delta_{i}$ can be written as $z^{(t+\\Delta_{i})}=s-M p^{(t+\\Delta_{i})}$ . Note $s_{u_{i}}-$ $\\begin{array}{r}{\\sum_{j=1}^{i-1}M_{u_{i}u_{j}}p_{u_{j}}^{(t+\\Delta_{i})}-\\sum_{j=i}^{n}M_{u_{i}u_{j}}p_{u_{j}}^{(t+\\Delta_{i})}=(s-M{p}^{(t+\\Delta_{i})})_{u_{i}}=z_{u_{i}}^{(t+\\Delta_{i})}}\\end{array}$ . Then, the updates of lo cal GS-SOR defined in  (15) can be rewritten as p(t+\u2206i+1) = p(t+\u2206i) +Mu\u03c9iui z . Hence, the updates of $z^{(t+\\Delta_{i})}$ can be written as ", "page_idx": 18}, {"type": "equation", "text": "$$\nz^{(t+\\Delta_{i+1})}=s-M p^{(t+\\Delta_{i+1})}=s-M\\left(p^{(t+\\Delta_{i})}+\\frac{\\omega z_{u_{i}}^{(t+\\Delta_{i})}}{M_{u_{i}u_{i}}}\\cdot e_{u_{i}}\\right)=z^{(t+\\Delta_{i})}-\\frac{\\omega z_{u_{i}}^{(t+\\Delta_{i})}}{M_{u_{i}u_{i}}}M e_{u_{i}}z_{i}\\,,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Note the diagonal element $M_{u_{i}u_{i}}=(1+\\alpha)/(2\\alpha)$ . Hence, when $\\begin{array}{r}{\\omega=\\frac{1+\\alpha}{2}}\\end{array}$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p^{(t+\\Delta_{i+1})}=p^{(t+\\Delta_{i})}+\\alpha z_{u_{i}}^{(t+\\Delta_{i})}\\cdot e_{u_{i}}}\\\\ &{z^{(t+\\Delta_{i+1})}=z^{(t+\\Delta_{i})}-\\alpha z_{u_{i}}^{(t+\\Delta_{i})}M e_{u_{i}}=z^{(t+\\Delta_{i})}-z_{u_{i}}^{(t+\\Delta_{i})}(\\frac{1+\\alpha}{2}I-\\frac{1-\\alpha}{2}A D^{-1})e_{u_{i}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The above updates match APPR\u2019s evolving set process formulation in (7). The rest is to show a new runtime bound. Adding $\\ell_{1}$ -norm on both sides of the above equation, then note $\\|z^{(t+\\Delta_{i+1})}\\|_{1}=$ $\\Vert{z}^{(t+\\Delta_{i})}\\Vert_{1}-\\alpha z_{u_{i}}^{(t+\\Delta_{i})}$ z(uti+\u2206i)for i = 1, 2, . . . , |St|. We have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\Vert z^{(t+1)}\\Vert_{1}=\\left(1-\\frac{\\alpha\\sum_{i=1}^{|S_{t}|}z_{u_{i}}^{(t+\\Delta_{i})}}{\\Vert z^{(t)}\\Vert_{1}}\\right)\\Vert z^{(t)}\\Vert_{1}=\\left(1-\\alpha\\beta_{t}\\right)\\Vert z^{(t)}\\Vert_{1}=\\prod_{i=0}^{t}\\left(1-\\alpha\\beta_{t}\\right)\\Vert z^{(0)}\\Vert_{1},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where we define $\\begin{array}{r}{\\beta_{t}:=\\frac{\\sum_{i=1}^{|S_{t}|}|z_{u_{i}}^{(t+\\Delta_{i})}|}{\\lVert z^{(t)}\\rVert_{1}}}\\end{array}$ Let $t=T-1$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\ln\\frac{\\|z^{(T)}\\|_{1}}{\\|z^{(0)}\\|_{1}}=\\sum_{t=0}^{T-1}\\ln\\left(1-\\alpha\\beta_{t}\\right)\\leq-\\sum_{t=0}^{T-1}\\alpha\\beta_{t}\\quad\\Rightarrow\\quad T\\leq\\frac{1}{\\alpha\\hat{\\gamma}_{T}}\\ln\\frac{\\|z^{(0)}\\|_{1}}{\\|z^{(T)}\\|_{1}},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the first inequality is due to $\\ln(1+x)\\leq x$ for $x>-1$ . For each nonzero node $u\\in\\mathcal{T}_{T}=$ $\\{z_{u}^{(T)}:z_{u}^{(T)}\\neq0,u\\in\\nu\\}$ , consider the last time $t^{\\prime}$ that it was altered. Then, either the alteration came from $u$ being an active node, with $z_{u}^{(t^{\\prime})}\\;\\geq\\;d_{u}\\epsilon$ , and after the PUSH operation it became $\\begin{array}{r}{z_{u}^{(t^{\\prime\\prime})}\\geq\\frac{(1-\\alpha)d_{u}}{2}\\epsilon}\\end{array}$ ; or the alteration came from a neighboring node $v_{u}\\in\\mathcal{N}(u)$ pushing its mass onto $u$ , which ensures that $\\begin{array}{r}{z_{u}^{(t^{\\prime\\prime})}\\ge\\frac{(1-\\alpha)}{2d_{v_{u}}}z_{v_{u}}^{(t^{\\prime})}\\ge\\frac{(1-\\alpha)}{2}\\epsilon}\\end{array}$ . These two cases provide a lower bound of $\\textstyle{\\frac{1-\\alpha}{2}}\\epsilon$ Hence, $\\begin{array}{r}{\\|z^{(T)}\\|_{1}\\geq\\frac{\\epsilon\\left(1-\\alpha\\right)|\\mathcal{Z}_{T}|}{2}}\\end{array}$ , which leads to the corresponding constant $C_{T}$ . ", "page_idx": 18}, {"type": "text", "text": "To see the lower bound of $1/\\epsilon$ , note $\\epsilon d_{u_{i}}\\le z_{u_{i}}^{(t+\\Delta_{i})}$ z(uti+\u2206i)for all i = 1, 2, . . . , |St|. Then we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\epsilon\\operatorname{vol}(S_{t})\\leq\\displaystyle\\sum_{i=1}^{|S_{t}|}z_{u_{i}}^{(t+\\Delta_{i})}}\\\\ &{\\qquad\\qquad=\\beta_{t}\\|z^{(t)}\\|_{1}}\\\\ &{\\qquad\\qquad\\leq\\beta_{t},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where we defined \u03b2t :=  |iS=\u2225t1|z |(zt)(ut\u2225i+\u2206i)| and the last inequality is due to the monotonic decreasing of $\\lVert\\boldsymbol{z}^{(t)}\\rVert_{1}$ , i.e., $1\\geq\\|z^{(0)}\\|_{1}\\geq\\cdots\\geq\\|z^{(T)}\\|_{1}$ . Applying the above inequality for all $t=0,1,2\\dots,T-$ 1, it leads to ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad\\epsilon\\operatorname{vol}(S_{t})\\leq\\beta_{t}}\\\\ {\\Rightarrow}&{\\;\\epsilon\\displaystyle\\sum_{t=0}^{T-1}\\operatorname{vol}(S_{t})\\leq\\sum_{t=0}^{T-1}\\beta_{t}}\\\\ &{\\Rightarrow\\quad\\displaystyle\\frac{\\overline{{\\operatorname{vol}}}(S_{T})}{\\hat{\\gamma}_{T}}\\leq\\frac{1}{\\epsilon},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the last derivation is from the fact that $\\begin{array}{r}{\\hat{\\gamma}_{T}=\\frac{1}{T}\\left\\{\\sum_{t=0}^{T-1}\\beta_{t}:=\\frac{\\sum_{i=1}^{|S_{t}|}|z_{u_{i}}^{(t+\\Delta_{i})}|}{\\|z^{(t)}\\|_{1}}\\right\\}\\!.}\\end{array}$ ", "page_idx": 19}, {"type": "text", "text": "Remark B.2. The connection between APPR and the Gauss-Seidel is not new [28, 29, 16, 9]. Our work is the first work that has linked APPR and the Gauss-Seidel with a locally evolving set process. ", "page_idx": 19}, {"type": "text", "text": "B.2 LOCSOR and Proof of Theorem 3.3 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this subsection, recall we defined $\\tilde{\\pmb{r}}^{(t)}={D^{1/2}\\pmb{r}^{(t)}}$ . ", "page_idx": 19}, {"type": "text", "text": "Lemma B.3 (Local iteration complexity of LOCSOR $(\\omega\\le1)$ ). Denote ${\\cal S}_{t}=\\left\\{u_{1},u_{2},\\dots,u_{|{\\cal S}_{t}|}\\right\\}$ as the active node set at the $t$ -th iteration. When $\\omega\\in(0,1]$ , all vectors $\\tilde{\\pmb{r}}^{(t)}\\geq0$ are nonnegative and magnitudes are decreasing $\\|\\tilde{\\pmb{r}}^{(t+1)}\\|_{1}<\\|\\tilde{\\pmb{r}}^{(t)}\\|_{1}$ . Let $T$ be the total number of iterations needed. Then, at iteration $T$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\nT\\in\\frac{(1+\\alpha)}{2\\alpha\\omega\\overline{{\\gamma}}_{T}}\\left[1-\\frac{\\|\\tilde{r}^{(T)}\\|_{1}}{\\|\\tilde{r}^{(0)}\\|_{1}},\\ln\\frac{\\|\\tilde{r}^{(0)}\\|_{1}}{\\|\\tilde{r}^{(T)}\\|_{1}}\\right],\\quad\\overline{{\\gamma}}_{T}\\triangleq\\frac{1}{T}\\sum_{t=0}^{T-1}\\left\\{\\gamma_{t}\\triangleq\\sum_{i=1}^{|S_{t}|}\\frac{\\tilde{r}_{u_{i}}^{(t+\\Delta_{i})}}{\\|\\tilde{r}^{(t)}\\|_{1}}\\right\\},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\begin{array}{r}{\\overline{{\\gamma}}_{t}=t^{-1}\\sum_{\\tau=0}^{t-1}\\gamma_{\\tau}}\\end{array}$ is the mean of active ratio factors defined in Equ. (6). ", "page_idx": 19}, {"type": "text", "text": "Proof. Recall $u_{i}\\in\\mathcal{S}_{t}=\\left\\{u_{1},\\ldots,u_{|\\mathcal{S}_{t}|}\\right\\}$ and $\\begin{array}{r}{\\Delta_{i}=\\frac{i-1}{|S_{t}|}}\\end{array}$ , LOCSOR in Algo. 3 updates ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pmb{x}^{(t+\\Delta_{i+1})}=\\pmb{x}^{(t+\\Delta_{i})}+\\omega\\pmb{r}_{u_{i}}^{(t+\\Delta_{i})}\\cdot\\pmb{e}_{u_{i}}}\\\\ &{\\pmb{r}^{(t+\\Delta_{i+1})}=\\pmb{r}^{(t+\\Delta_{i})}-\\omega\\pmb{r}_{u_{i}}^{(t+\\Delta_{i})}\\cdot\\pmb{e}_{u_{i}}+\\frac{(1-\\alpha)\\omega}{1+\\alpha}\\pmb{r}_{u_{i}}^{(t+\\Delta_{i})}\\cdot\\pmb{D}^{-1/2}\\pmb{A}\\pmb{D}^{-1/2}\\pmb{e}_{u_{i}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Note $\\mathbf{\\nabla}r\\geq\\mathbf{0}$ during updates when $\\omega\\in(0,1]$ and recall $\\tilde{\\pmb{r}}^{(t)}={D^{1/2}\\pmb{r}^{(t)}}$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\tilde{r}^{(t+\\Delta_{i+1})}+\\omega\\tilde{r}_{u_{i}}^{(t+\\Delta_{i})}\\cdot e_{u}\\|_{1}=\\|\\tilde{r}^{(t+\\Delta_{i})}+\\frac{(1-\\alpha)\\omega}{1+\\alpha}\\tilde{r}_{u_{i}}^{(t+\\Delta_{i})}\\cdot A D^{-1}e_{u_{i}}\\|_{1}}\\\\ &{\\quad\\quad\\|\\tilde{r}^{(t+\\Delta_{i+1})}\\|_{1}+\\omega\\tilde{r}_{u_{i}}^{(t+\\Delta_{i})}=\\|\\tilde{r}^{(t+\\Delta_{i})}\\|_{1}+\\frac{(1-\\alpha)\\omega}{1+\\alpha}\\tilde{r}_{u_{i}}^{(t+\\Delta_{i})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Summing over the above equations over $u_{i}$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\|\\tilde{r}^{(t+1)}\\|_{1}=\\|\\tilde{r}^{(t)}\\|_{1}-\\frac{2\\alpha\\omega}{1+\\alpha}\\sum_{i=1}^{|S_{t}|}\\tilde{r}_{u_{i}}^{(t+\\Delta_{i})}=\\left(1-\\frac{2\\alpha\\omega}{1+\\alpha}\\sum_{i=1}^{|S_{t}|}\\frac{\\tilde{r}_{u_{i}}^{(t+\\Delta_{i})}}{\\underset{\\gamma_{t}}{\\underbrace{\\|\\tilde{r}^{(t)}\\|_{1}}}}\\right)\\|\\tilde{r}^{(t)}\\|_{1}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Given $\\{x_{i}\\}_{i=0}^{T-1}$ and $x_{i}~\\in~(0,1)$ , the Weierstrass product inequality provides $\\begin{array}{r}{1-\\sum_{i=0}^{T-1}x_{i}\\ \\leq}\\end{array}$ $\\textstyle\\prod_{i=0}^{T-1}(1-x_{i})$ . By using this inequality, we continue to have a lower bound of $T$ as the following ", "page_idx": 19}, {"type": "text", "text": "$1-\\sum_{t=0}^{T-1}\\frac{2\\alpha\\omega\\gamma_{t}}{1+\\alpha}\\leq\\prod_{t=0}^{T-1}\\left(1-\\frac{2\\alpha\\omega\\gamma_{t}}{1+\\alpha}\\right)=\\frac{\\Vert\\tilde{r}^{(T)}\\Vert_{1}}{\\Vert\\tilde{r}^{(0)}\\Vert_{1}}\\quad\\Rightarrow\\quad\\frac{(1+\\alpha)\\big(1-\\Vert\\tilde{r}^{(T)}\\Vert_{1}/\\Vert\\tilde{r}^{(0)}\\Vert_{1}\\big)}{2\\alpha\\omega\\overline{{\\gamma}}_{T}}\\leq T.$ . ", "page_idx": 19}, {"type": "text", "text": "To get upper bound of $\\gamma_{t}$ , note each active residual $\\tilde{r}_{u_{i}}^{(t+\\Delta_{i})}$ pushes at most ((11\u2212+\u03b1\u03b1))\u03c9 times magnitude to $\\tilde{r}_{u_{i+1}},\\tilde{r}_{u_{i+2}}$ , and $\\tilde{r}_{u_{|S_{t}|}}$ ; hence, $\\textstyle\\sum_{j=i}^{|S_{t}|}\\tilde{r}_{u_{j}}^{(t+\\Delta_{j})}$ will increase by at most r\u02dc(uti+ $\\tilde{r}_{u_{i}}^{(t+\\Delta_{i})}$ $\\begin{array}{r}{\\frac{(1-\\alpha)\\omega}{(1+\\alpha)}\\leq\\tilde{r}_{u_{i}}^{(t+\\Delta_{i})}}\\end{array}$ in total. Hence, overall $u_{i}$ , we h ave ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\big\\|\\tilde{r}_{S_{t}}^{(t)}\\big\\|_{1}=\\sum_{i=1}^{|S_{t}|}\\tilde{r}_{u_{i}}^{(t)}\\le\\sum_{i=1}^{|S_{t}|}\\tilde{r}_{u_{i}}^{(t+\\Delta_{i})}\\le2\\big\\|\\tilde{r}_{S_{t}}^{(t)}\\big\\|_{1}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We reach the following lower and upper bounds of \u03b3t,\u2225\u2225rr\u02dc\u02dc(Stt) \u2225\u222511 $\\begin{array}{r}{\\frac{\\|\\tilde{\\boldsymbol{r}}_{s_{t}}^{(t)}\\|_{1}}{\\|\\tilde{\\boldsymbol{r}}^{(t)}\\|_{1}}\\leq\\gamma_{t}:=\\sum_{i=1}^{|S_{t}|}\\frac{\\tilde{r}_{u_{i}}^{(t+\\Delta_{i})}}{\\|\\tilde{\\boldsymbol{r}}^{(t)}\\|_{1}}\\leq\\frac{2\\|\\tilde{\\boldsymbol{r}}_{S_{t}}^{(t)}\\|_{1}}{\\|\\tilde{\\boldsymbol{r}}^{(t)}\\|_{1}}}\\end{array}$ . To check the upper bound of $T$ , from Equ. (17), $\\begin{array}{r}{\\|\\tilde{r}^{(T)}\\|_{1}=\\prod_{t=0}^{T-1}\\left(1-\\frac{2\\alpha\\omega\\gamma_{t}}{1+\\alpha}\\right)\\|\\tilde{r}^{(0)}\\|_{1}}\\end{array}$ and ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\ln\\frac{\\|\\widetilde{r}^{(T)}\\|_{1}}{\\|\\widetilde{r}^{(0)}\\|_{1}}=\\sum_{t=0}^{T-1}\\ln\\left(1-\\frac{2\\alpha\\omega\\gamma_{t}}{1+\\alpha}\\right)\\leq-\\sum_{t=0}^{T-1}\\frac{2\\alpha\\omega\\gamma_{t}}{1+\\alpha}\\quad\\Rightarrow\\quad T\\leq\\frac{(1+\\alpha)}{2\\alpha\\omega\\overline{{\\gamma}}_{T}}\\ln\\frac{\\|\\widetilde{r}^{(0)}\\|_{1}}{\\|\\widetilde{r}^{(T)}\\|_{1}},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the first inequality is due to $\\ln(1+x)\\leq x$ for $x>-1$ . ", "page_idx": 20}, {"type": "text", "text": "Theorem 3.3 (Runtime bound of LOCSOR $(\\omega=1)$ )). Given the configuration $\\theta=(\\alpha,\\epsilon,s,\\mathcal{G})$ with $\\alpha\\in(0,1)$ and $\\epsilon\\leq1/d_{s}$ and let $r^{(T)}$ and ${\\pmb x}^{(T)}$ be returned by LOCSOR defined in (8) for solving Equ. (3). There exists a real implementation of (8) such that the runtime $\\mathcal{T}_{\\mathrm{L}}$ OCSOR is bounded by ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{1+\\alpha}{2}\\cdot\\frac{\\overline{{\\mathrm{vol}}}(S_{T})}{\\alpha\\overline{{\\gamma}}_{T}}\\left(1-\\frac{\\|D^{1/2}r^{(T)}\\|_{1}}{\\|D^{1/2}r^{(0)}\\|_{1}}\\right)\\leq\\mathcal{T}_{\\mathrm{Loc}\\mathrm{SoR}}\\leq\\frac{1+\\alpha}{2}\\cdot\\operatorname*{min}\\left\\{\\frac{1}{\\alpha\\epsilon},\\frac{\\overline{{\\mathrm{vol}}}(S_{T})}{\\alpha\\overline{{\\gamma}}_{T}}\\ln\\frac{C}{\\epsilon}\\right\\}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\overline{{\\mathrm{vol}}}(S_{T})$ and $\\overline{{\\gamma}}_{T}$ are defined in (6) and $\\begin{array}{r}{C=\\frac{1+\\alpha}{(1-\\alpha)|{\\cal Z}_{T}|}}\\end{array}$ with $\\mathcal{Z}_{T}=\\mathrm{supp}(r^{(T)})$ . Furthermore, $\\overline{{\\mathrm{vol}}}(S_{T})/\\overline{{\\gamma}}_{T}\\leq1/\\epsilon$ and the local estimate $\\hat{\\pi}:=D^{1/2}x^{(T)}$ satisfies $\\|D^{-1}(\\hat{\\pi}-\\pi)\\|_{\\infty}\\leq\\epsilon.$ . ", "page_idx": 20}, {"type": "text", "text": "Proof. After the last iteration $T$ , for each nonzero residual $\\tilde{r}_{u}^{(T)}\\ne0,u\\in\\mathbb{Z}_{T}$ , there must be at least one update that happened at node $u$ : Node $u$ has a neighbor $v_{u}\\in\\mathcal{N}(u)$ , which was active. This neighbor $v_{u}$ pushed some residual $\\frac{(1{-}\\alpha)\\tilde{r}_{v_{u}}^{(t^{\\prime})}}{(1{+}\\alpha)d_{v_{u}}}$ to $u$ where $t^{\\prime}<T$ . Hence, for all $u\\in\\mathcal T_{T}$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\|\\tilde{r}^{(T)}\\|_{1}=\\sum_{u\\in\\cal{T}_{T}}\\tilde{r}_{u}^{(T)}\\ge\\sum_{u\\in\\cal{T}_{T}}\\frac{(1-\\alpha)\\tilde{r}_{v_{u}}^{(t^{\\prime})}}{(1+\\alpha)d_{v_{u}}}\\ge\\sum_{u\\in\\cal{T}_{T}}\\frac{(1-\\alpha)2\\alpha\\epsilon d_{v_{u}}/(1+\\alpha)}{(1+\\alpha)d_{v_{u}}}=\\epsilon|\\mathcal{Z}_{T}|\\frac{2\\alpha(1-\\alpha)}{(1+\\alpha)^{2}},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the second inequality is because $\\tilde{r}_{v_{u}}^{(t^{\\prime})}$ was active before the push operation. Applying the above lower bound of $\\lVert\\tilde{\\pmb{r}^{(T)}}\\rVert_{1}$ to Equ. (16) of Lemma B.3 and note $\\lVert\\tilde{\\pmb{r}}^{(0)}\\rVert_{1}=2\\alpha/(1+\\alpha)$ , we obtain ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{\\|\\tilde{r}^{(0)}\\|_{1}}{\\|\\tilde{r}^{(T)}\\|_{1}}\\leq\\frac{\\|\\tilde{r}^{(0)}\\|_{1}}{\\epsilon|\\mathcal{Z}_{T}|\\cdot\\frac{2\\alpha(1-\\alpha)}{(1+\\alpha)^{2}}}=\\frac{1+\\alpha}{\\epsilon(1-\\alpha)|\\mathcal{Z}_{T}|}:=\\frac{C_{1}}{\\epsilon}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The rest is to prove an upper bound $1/(\\alpha\\epsilon)$ of $\\tau_{\\mathrm{LocSOR}}$ . Recall that for any active node $u$ , we have residual updates from Algo. 3 as the following ", "page_idx": 20}, {"type": "equation", "text": "$$\nD^{1/2}r^{(t+1)}=D^{1/2}r^{(t)}-\\omega r_{u}^{(t)}D^{1/2}e_{u}+\\frac{(1-\\alpha)\\omega r_{u}^{(t)}}{1+\\alpha}A D^{-1}D^{1/2}e_{u}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Move r(ut )D1/2eu to the left and note \u2225AD\u22121D1/2eu\u22251 = \u221adu, we then obtain ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\|D^{1/2}r^{(t+1)}\\|_{1}+\\omega\\sqrt{d_{u}}r_{u}^{(t)}=\\|D^{1/2}r^{(t)}\\|_{1}+\\frac{(1-\\alpha)\\omega}{1+\\alpha}\\sqrt{d_{u}}r_{u}^{(t)}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Hence, for each active $u$ , we have $\\begin{array}{r}{\\frac{2\\alpha\\omega\\sqrt{d_{u}}r_{u}^{(t)}}{1+\\alpha}=\\|D^{1/2}\\pmb{r}^{(t)}\\|_{1}-\\|D^{1/2}\\pmb{r}^{(t+1)}\\|_{1}}\\end{array}$ . Summing them over all active nodes $u$ and noticing $r_{u}^{(t)}\\geq2\\alpha\\epsilon\\sqrt{d_{u}}/(1+\\alpha)$ by the active condition. Note $\\omega=1$ and $\\begin{array}{r}{||D^{1/2}r^{(0)}||_{1}=\\frac{2\\alpha}{1+\\alpha}}\\end{array}$ , we have run time bounded by ", "page_idx": 20}, {"type": "equation", "text": "$$\nT_{\\mathrm{LocSOR}}=\\sum_{u}d_{u}\\leq\\left(\\frac{1+\\alpha}{2\\alpha}\\right)^{2}\\frac{\\sum_{t}(\\Vert D^{1/2}r^{(t)}\\Vert_{1}-\\Vert D^{1/2}r^{(t+1)}\\Vert_{1})}{\\omega\\epsilon}\\leq\\frac{(1+\\alpha)}{2\\alpha\\epsilon}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Combining the above bound and the bound $T$ shown in Lemma B.3, we prove the lower and upper bound of $\\mathcal{T}_{L o c S O R}$ . To check the lower bound of $1/\\epsilon$ ,i.e., $\\overline{{\\mathrm{vol}}}(S_{T})/\\overline{{\\gamma}}_{T}\\leq1/\\epsilon$ , note $\\begin{array}{r}{\\frac{2\\alpha\\epsilon d_{u_{i}}}{1+\\alpha}\\leq\\tilde{r}_{u_{i}}^{(t+\\bar{\\Delta}_{i})}}\\end{array}$ for all $i=1,2,\\dots,|S_{t}|$ . Then we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{2\\alpha\\epsilon}{1+\\alpha}\\operatorname{vol}(S_{t})\\le\\sum_{i=1}^{|S_{t}|}\\tilde{r}_{u_{i}}^{(t+\\Delta_{i})}}}\\\\ &{}&{\\quad=\\gamma_{t}\\|D^{1/2}r^{(t)}\\|_{1}}\\\\ &{}&{\\quad\\le\\gamma_{t}\\|D^{1/2}r^{(0)}\\|_{1}=\\frac{2\\alpha\\gamma_{t}}{1+\\alpha},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the last inequality is due to the monotonic decreasing of $\\|\\boldsymbol{D}^{1/2}\\boldsymbol{r}^{(t)}\\|_{1}$ , i.e., $\\frac{2\\alpha}{1\\!+\\!\\alpha}\\quad\\ge$ $\\|D^{1/2}\\pmb{r}^{(0)}\\|_{1}\\;\\geq\\;\\cdots\\;\\geq\\;\\|\\pmb{r}^{(T)}\\|_{1}$ . Applying the above inequality over all $t\\,=\\,0,1,2\\dots,T\\mathrm{~-~}1$ , it leads to ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad\\epsilon\\operatorname{vol}(S_{t})\\leq\\gamma_{t}}\\\\ {\\Rightarrow\\;}&{\\epsilon\\displaystyle\\sum_{t=0}^{T-1}\\operatorname{vol}(S_{t})\\leq\\sum_{t=0}^{T-1}\\gamma_{t}}\\\\ &{\\Rightarrow\\quad\\displaystyle\\frac{\\overline{{\\operatorname{vol}}}(S_{T})}{\\overline{{\\gamma}}_{T}}\\leq\\frac{1}{\\epsilon}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "$\\mathbf{Algo.3\\LocSOR}(\\alpha,\\epsilon,s,\\mathcal{G},\\omega)$ via FIFO Queue   \n1: Initialize: $\\boldsymbol{r}\\gets c\\boldsymbol{e}_{s}$ , $\\mathbfit{x}\\gets\\mathbf{0}$ , $\\begin{array}{r}{c=\\frac{2\\alpha}{1+\\alpha}}\\end{array}$ , t = \u22121   \n2: $\\mathcal{Q}\\leftarrow\\{*,s\\}$ // As we assume $\\epsilon\\leq1/d_{s}$   \n3: while true do   \n4: $u\\leftarrow\\mathcal{Q}.$ .dequeue()   \n5: if $\\mathbf{u}==^{*}$ then   \n6: if $\\mathcal{Q}=\\emptyset$ then   \n7: break   \n8: $t\\gets t+1$ // Starting time of $\\mathcal{S}_{t}$   \n9: .enqueue(\\*) // Marker for next $\\boldsymbol{S}_{t+1}$   \n10: continue   \n11: $\\tilde{r}\\gets r_{u}$   \n12: if $\\left|\\boldsymbol{r}_{u}\\right|<\\boldsymbol{c}\\cdot\\boldsymbol{\\epsilon}d_{u}$ then   \n13: continue   \n14: xu xu + \u03c9  r\u02dc   \n15: ru ru \u03c9  r\u02dc   \n16: for v (u) do   \n17: $\\begin{array}{r}{r_{v}\\gets r_{v}+\\frac{(1-\\alpha)\\omega}{(1+\\alpha)}\\cdot\\frac{\\tilde{r}}{d_{u}}}\\end{array}$   \n18: if $|r_{v}|\\geq c\\cdot\\epsilon d_{v}$ and $v\\not\\in{\\mathcal{Q}}$ then   \n19: .enqueue $(v)$   \n20: if $\\left|\\boldsymbol{r}_{u}\\right|\\geq c\\cdot\\epsilon d_{u}$ and $u\\notin\\mathcal{Q}$ then   \n21: .enqueue(u)   \n22: return $\\textbf{\\em x}$ ", "page_idx": 21}, {"type": "text", "text": "The real queue-based implementation of LOCSOR is presented in Algo. 3. It has monotonic and nonnegative properties during the updates of $r\\,\\geq\\,{\\bf0}$ and $\\textbf{\\em x}\\geq\\textbf{0}$ when $\\omega\\bar{\\mathbf\\Xi}\\in\\mathsf{\\Gamma}(0,\\bar{1}]$ . Same as APPR, the operations of $\\mathcal{Q}$ .enqueue $(u)$ , $\\mathcal{Q}$ .dequeue(), and $v\\not\\in{\\mathcal{Q}}$ are all in $\\mathcal{O}(1)$ . ", "page_idx": 21}, {"type": "text", "text": "During the updates, one should note that the real vector $\\pmb{r}$ presents $D^{1/2}r^{(t)}$ while the vector $\\textbf{\\em x}$ is $D^{1/2}x^{(t)}$ . In this case, the original active node condition is implicitly shifting from $\\left|\\boldsymbol{r}_{u}\\right|\\geq$ $\\textstyle{\\frac{2\\alpha\\epsilon{\\sqrt{d_{u}}}}{1+\\alpha}}$ to $\\begin{array}{r}{\\sqrt{d_{u}}|r_{u}|\\ge\\frac{2\\alpha\\epsilon d_{u}}{1+\\alpha}}\\end{array}$ . We use this shifted active condition in Lines 11 and 13 and inactive condition in Line 5. When $\\omega\\ \\in\\ (1,2)$ , it is possible $|r_{u}|<c\\cdot\\epsilon d_{u}$ and LOCSOR will ignore this inactive node $u$ during the updates. This step makes sure $\\begin{array}{r}{\\quad S_{t}=\\{u_{i}:\\bar{|r_{u_{i}}^{(t+\\Delta_{i})}|}\\geq\\frac{2\\alpha\\epsilon\\sqrt{d_{u}}}{1+\\alpha}\\}}\\end{array}$ during the updates. ", "page_idx": 21}, {"type": "text", "text": "B.3 Optimal GS-SOR and Proof of Corollary 3.4 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We introduce the following standard result. ", "page_idx": 21}, {"type": "text", "text": "Lemma B.4 (Young [55], Section 12.2, Theorem 2.1 ). Given the GS-SOR method for solving $Q x=b,$ , if the underlying matrix $Q$ is a Stieltjes matrix and set relaxation parameter $\\omega$ as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\omega^{*}=\\frac{2}{1+\\sqrt{1-\\rho(B)^{2}}}=1+\\left(\\frac{\\rho(B)}{1+\\sqrt{1-\\rho(B)^{2}}}\\right)^{2},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\rho(B)$ is the largest eigenvalue (in magnitude) of $B=I-\\mathrm{diag}(Q)^{-1}Q,$ , then ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\omega^{*}-1\\leq\\rho(L_{\\omega^{*}})\\leq\\sqrt{\\omega^{*}-1},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $L_{\\omega}:=(\\mathbf{diag}(Q)-\\omega Q_{L})^{-1}(\\omega Q_{U}-(\\omega-1)\\mathbf{diag}(Q))$ with ${\\cal Q}={\\bf d i a g}(Q)-{\\cal Q}_{U}-{\\cal Q}_{L}$ ", "page_idx": 21}, {"type": "text", "text": "Corollary 3.4. Let $\\omega=\\omega^{\\ast}\\triangleq2/(1+\\sqrt{1-(1-\\alpha)^{2}/(1+\\alpha)^{2}})$ and $S_{t}=\\mathcal{V},\\forall t\\geq0$ , the global version of LOCSOR has the following convergence bound ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|r^{(t)}\\|_{2}\\leq\\frac{2}{(1+\\alpha)\\sqrt{d_{s}}}\\left(\\frac{1-\\sqrt{\\alpha}}{1+\\sqrt{\\alpha}}+\\epsilon_{t}\\right)^{t},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\epsilon_{t}$ are small positive numbers with $\\mathrm{lim}_{t\\rightarrow\\infty}\\,\\epsilon_{t}=0$ . ", "page_idx": 21}, {"type": "text", "text": "Proof. Recall $\\begin{array}{r}{Q=I-\\frac{1-\\alpha}{1+\\alpha}D^{-1/2}A D^{-1/2}}\\end{array}$ and we consider the underlying graph as simple which means $\\pmb{A}$ has 0 diagonal. Hence, $\\begin{array}{r}{\\mathbf{diag}(Q)=I}\\end{array}$ and $_B$ is defined as ", "page_idx": 22}, {"type": "equation", "text": "$$\nB=\\frac{1-\\alpha}{1+\\alpha}D^{-1/2}A D^{-1/2},\\quad\\rho(B)=\\frac{1-\\alpha}{1+\\alpha}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Since $Q$ is a Stieltjes matrix, then Lemma B.4 gives a bound on the spectral radius of $\\mathbf{\\nabla}L_{\\omega}$ as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\rho(L_{\\omega^{*}})\\leq\\left(\\frac{2}{1+\\sqrt{1-\\rho(B)^{2}}}-1\\right)^{1/2}=\\left(\\frac{2(1+\\alpha)}{1+\\alpha+2\\sqrt{\\alpha}}-1\\right)^{1/2}=\\frac{1-\\sqrt{\\alpha}}{1+\\sqrt{\\alpha}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Recall that Gelfand\u2019s formula states [52]: Given spectral radius $\\begin{array}{r}{\\rho(L_{\\omega^{*}})\\,:=\\,\\operatorname*{max}_{i\\in[n]}\\,|\\lambda_{i}(L_{\\omega^{*}})|}\\end{array}$ , where $\\lambda_{i}(\\cdot)$ is the $i^{\\th}$ -th eigenvalue, there exists a sequence $\\{\\epsilon_{t}\\}_{\\ge0}$ such that $\\|L_{\\omega^{*}}^{t}\\|_{2}=\\left(\\rho(L_{\\omega^{*}})+\\epsilon_{t}\\right)^{t}$ and $\\mathrm{lim}_{t\\rightarrow\\infty}\\,\\epsilon_{t}=0$ . The standard SOR method is defined as the following ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{x^{(t+1)}=(\\mathbf{diag}(Q)-\\omega^{*}Q_{L})^{-1}\\left(\\omega^{*}b+(\\omega^{*}Q_{U}-(\\omega^{*}-1)\\mathbf{diag}(Q))\\,x^{(t)}\\right)}\\\\ &{\\qquad=L_{\\omega^{*}}x^{(t)}+\\omega^{*}(\\mathbf{diag}(Q)-\\omega^{*}Q_{L})^{-1}b,}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Note $\\pmb{r}^{(t)}=\\pmb{Q}e^{(t)}=\\pmb{Q}\\pmb{L}_{\\omega^{*}}^{t}\\pmb{Q}^{-1}\\pmb{Q}e^{(0)}=\\pmb{Q}\\pmb{L}_{\\omega^{*}}^{t}\\pmb{Q}^{-1}\\pmb{r}^{(0)}$ . We have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\|\\pmb{r}^{(t)}\\|_{2}=\\|\\pmb{Q}\\pmb{L}_{\\omega^{*}}^{t}\\pmb{Q}^{-1}\\pmb{r}^{(0)}\\|_{2}\\leq\\|\\pmb{Q}\\|_{2}\\|\\pmb{L}_{\\omega^{*}}^{t}\\|_{2}\\|\\pmb{Q}^{-1}\\|_{2}\\|\\pmb{r}^{(0)}\\|_{2}}}\\\\ &{}&{\\leq\\frac{2}{1+\\alpha}\\cdot\\|\\pmb{L}_{\\omega^{*}}^{t}\\|_{2}\\cdot\\frac{1+\\alpha}{2\\alpha}\\cdot\\frac{2\\alpha}{(1+\\alpha)\\sqrt{d_{s}}}=\\frac{2\\|\\pmb{L}_{\\omega^{*}}^{t}\\|_{2}}{(1+\\alpha)\\sqrt{d_{s}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "To meet the stop condition, we require $\\begin{array}{r}{|r_{u}^{(t)}|\\;\\le\\;\\frac{2\\alpha\\epsilon\\sqrt{d_{u}}}{1+\\alpha}}\\end{array}$ . It is enough to make sure $\\lVert\\boldsymbol{r}^{(t)}\\rVert_{2}\\,\\leq$ (1+2\u03b1\u03b1)\u03f5\u221ads . This leads to find t such that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|r^{(t)}\\|_{2}\\leq\\frac{2\\|L_{\\omega^{*}}^{t}\\|_{2}}{(1+\\alpha)\\sqrt{d_{s}}}\\leq\\frac{2\\alpha\\epsilon}{(1+\\alpha)\\sqrt{d_{s}}}\\Leftrightarrow\\left(\\frac{1-\\sqrt{\\alpha}}{1+\\sqrt{\\alpha}}+\\epsilon_{t}\\right)^{t}\\leq\\alpha\\epsilon.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "When $\\epsilon_{t}=o(\\sqrt{\\alpha})$ , then the runtime of global LOCSOR is $\\tilde{\\mathcal{O}}(m/\\sqrt{\\alpha})$ where $\\tilde{\\mathcal{O}}$ hides $\\log{\\frac{1}{\\epsilon}}$ . ", "page_idx": 22}, {"type": "text", "text": "B.4 LOCGD and Proof of Theorem 3.5 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The local gradient descent, namely LOCGD is to use $\\pmb{x}^{(t+1)}=\\pmb{x}^{(t)}+\\pmb{r}_{S_{t}}^{(t)}$ and $\\pmb{r}^{(t+1)}=\\pmb{r}^{(t)}-\\pmb{Q}\\pmb{r}_{S_{t}}^{(t)}$ where |r(uti+\u2206i)| \u22652\u03b1\u03f5\u221adu/(1 + \u03b1)} where \u2206i = 0. Algo. 4 presents our actual implementation of LOCGD via FIFO Queue. ", "page_idx": 22}, {"type": "text", "text": "$\\mathbf{\\overline{{Alg0.\\4\\LocGD(\\alpha,\\epsilon,s,\\mathcal{G})}}}$ via FIFO Queue   \n1: Initialize: $\\boldsymbol{r}\\gets c\\boldsymbol{e}_{s}$ , x \u21900, $\\begin{array}{r}{c=\\frac{2\\alpha}{1+\\alpha}}\\end{array}$   \n2: $\\mathcal{Q}\\leftarrow\\{s\\}$ // Assume $\\begin{array}{r}{\\epsilon\\leq\\frac{1}{d_{s}}}\\end{array}$   \n3: $t=0$   \n4: while $\\mathcal Q\\neq\\emptyset$ do   \n5: $\\mathcal{S}_{t}\\gets[]$   \n6: while $\\tilde{\\mathcal{Q}}\\neq\\varnothing$ do   \n7: $u\\leftarrow\\mathcal{Q}.$ .dequeue()   \n8: t.append $\\textstyle(u,r_{u}))$   \n9: xu \u2190xu + ru   \n10: ru 0   \n11: for $(u,\\tilde{r})\\in S_{t}$ do   \n12: for $v\\in\\mathcal{N}(u)$ do   \n13: $\\begin{array}{r}{r_{v}\\leftarrow r_{v}+\\frac{(1-\\alpha)\\tilde{r}}{(1+\\alpha)d_{u}}}\\end{array}$   \n14: if $|\\boldsymbol{r}_{v}|\\geq c\\cdot\\epsilon d_{v}$ and $v\\not\\in{\\mathcal{Q}}$ then   \n15: Q.enqueue(v)   \n16: t \u2190t + 1   \n17: return x, r ", "page_idx": 22}, {"type": "text", "text": "Algo. 4 presents LOCGD similar to the real queue-based implementation of LOCSOR. It has monotonic and nonnegative properties during the updates of $r\\,\\geq\\,0$ and $\\textbf{\\em x}\\geq{\\bf0}$ . Again, the operations of $\\mathcal{Q}$ .enqueue $(u)$ , $\\mathcal{Q}$ .dequeue(), and $v\\not\\in{\\mathcal{Q}}$ are all in $O(1)$ . During the updates, one should note that $\\pmb{r}$ presents $D^{1/2}r^{(t)}$ while $\\textbf{\\em x}$ is $D^{1/2}x^{(t)}$ . All shifted conditions are the same as of LOCSOR. The key advantage of LOCGD is that it is highly parallelizable, while LOCSOR is truly an online update, so it is hard to parallelize. ", "page_idx": 22}, {"type": "text", "text": "Lemma B.5 (Iterations of LOCGD). With the initial ${\\pmb x}^{(0)}={\\bf0},r^{(0)}={\\pmb b},S_{0}=\\mathrm{supp}({\\pmb r}^{(0)}),$ , denote $\\tilde{\\pmb{r}}^{(t)}=D^{1/2}\\pmb{r}^{(t)}$ . LOCGD defined in (9) has the following properties: 1) $\\mathbf{\\boldsymbol{x}}^{(t)}\\geq0,$ , $\\pmb{r}^{(t)}\\geq0$ and $\\|r^{(t)}\\|\\geq\\|r^{(t+1)}\\|_{1};2)$ The residual and estimation error satisfies ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\|\\tilde{r}^{(t+1)}\\|_{1}=\\left(1-\\frac{2\\alpha\\gamma_{t}}{1+\\alpha}\\right)\\|\\tilde{r}^{(t)}\\|_{1},\\quad\\gamma_{t}=\\sum_{i=1}^{|S_{t}|}\\frac{\\tilde{r}_{u_{i}}^{(t+\\Delta_{i})}}{\\|\\tilde{r}^{(t)}\\|_{1}},\\,w h e r e\\,\\Delta_{i}=0.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. We first show $\\mathbf{\\boldsymbol{x}}^{(t)}\\,\\geq\\,\\mathbf{0},r^{(t)}\\,\\geq\\,\\mathbf{0}$ are all nonnegative vectors during the updates when $b\\geq0$ . This can be seen from the induction. At the initial stage, $\\mathbf{\\boldsymbol{x}}^{(0)}\\geq\\mathbf{0}$ and $\\pmb{r}^{(0)}\\,=\\,\\pmb{b}\\,\\geq\\,0$ . Now assume that for any $t\\,\\geq\\,0$ , $\\mathbf{\\boldsymbol{x}}^{(t)}\\,\\geq\\,0$ and $\\pmb{r}^{(t)}\\,\\geq\\,0$ . Then $\\pmb{x}^{(t+1)}\\,=\\,\\pmb{x}^{(t)}\\,+\\,\\pmb{r}_{S_{k}}^{(t)}\\,\\geq\\,0$ , and $\\begin{array}{r}{\\pmb{r}^{(t+1)}=\\pmb{r}_{\\overline{{S}}_{t}}^{(t)}+\\frac{1-\\alpha}{1+\\alpha}\\pmb{D}^{-1/2}\\pmb{A}\\pmb{D}^{-1/2}\\pmb{r}_{\\overline{{S}}_{t}}^{(t)}\\geq\\mathbf{0}}\\end{array}$ . Therefore, $\\mathbf{\\boldsymbol{x}}^{(t)}\\geq0$ and $\\pmb{r}^{(t)}\\,\\geq\\,0$ for all $t$ . Note $\\begin{array}{r}{\\tilde{r}^{(t+1)}=\\tilde{r}_{\\overline{{S}}_{t}}^{(t)}+\\frac{1-\\alpha}{1+\\alpha}A D^{-1}\\cdot\\tilde{r}_{\\overline{{S}}_{t}}^{(t)}}\\end{array}$ and since $\\lVert A D^{-1}\\tilde{\\mathbf{r}}_{S_{t}}^{(t)}\\rVert_{1}=\\lVert\\tilde{\\mathbf{r}}_{S_{t}}^{(t)}\\rVert_{1}$ , we will have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\|\\widetilde{\\boldsymbol{r}}^{(t+1)}\\|_{1}=\\left(1-\\frac{2\\alpha}{1+\\alpha}\\frac{\\|\\widetilde{\\boldsymbol{r}}_{\\mathcal{S}_{t}}^{(t)}\\|_{1}}{\\|\\widetilde{\\boldsymbol{r}}^{(t)}\\|_{1}}\\right)\\|\\widetilde{\\boldsymbol{r}}^{(t)}\\|_{1},\\mathrm{~where~}\\gamma_{t}:=\\|\\widetilde{\\boldsymbol{r}}_{\\mathcal{S}_{t}}^{(t)}\\|_{1}/\\|\\widetilde{\\boldsymbol{r}}^{(t)}\\|_{1}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Then, we can bound the total residual as the following theorem. ", "page_idx": 23}, {"type": "text", "text": "Theorem 3.5 (Runtime bound of LOCGD). Given the configuration $\\theta=(\\alpha,\\epsilon,s,\\mathcal{G})$ with $\\alpha\\in(0,1)$ and $\\epsilon\\leq1/d_{s}$ and let $r^{(T)}$ and ${\\pmb x}^{(T)}$ be returned by LOCGD defined in (9) for solving Equ. (4). There exists a real implementation of (9) such that the runtime $\\mathcal{T}_{\\mathrm{LocGD}}$ is bounded by ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{1+\\alpha}{2}\\cdot\\frac{\\overline{{\\mathrm{vol}}}(S_{T})}{\\alpha\\overline{{\\gamma}}_{T}}\\left(1-\\frac{\\|\\tilde{r}^{(T)}\\|_{1}}{\\|\\tilde{r}^{(0)}\\|_{1}}\\right)\\leq\\mathcal{T}_{\\mathrm{LocGD}}\\leq\\frac{1+\\alpha}{2}\\cdot\\operatorname*{min}\\left\\{\\frac{1}{\\alpha\\epsilon},\\frac{\\overline{{\\mathrm{vol}}}(S_{T})}{\\alpha\\overline{{\\gamma}}_{T}}\\ln\\frac{C}{\\epsilon}\\right\\},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $C=(1+\\alpha)/((1-\\alpha)|\\mathcal{Z}_{T}|),\\mathcal{Z}_{T}=\\mathrm{supp}(r^{(T)})$ . Furthermore, $\\overline{{\\mathrm{vol}}}(S_{T})/\\overline{{\\gamma}}_{T}\\leq1/\\epsilon$ and the estimate $\\hat{\\pi}:=D^{1/2}x^{(T)}$ satisfies $\\|D^{-1}(\\hat{\\pmb{\\pi}}-\\pmb{\\pi})\\|_{\\infty}\\leq\\epsilon.$ . ", "page_idx": 23}, {"type": "text", "text": "Proof. We first show bound $1/(\\alpha\\epsilon)$ . We first rearrange $\\pmb{r}^{(t+1)}=\\pmb{r}^{(t)}-\\pmb{Q}\\pmb{r}_{S_{t}}^{(t)}$ r(t)into ", "page_idx": 23}, {"type": "equation", "text": "$$\nD^{1/2}r^{(t+1)}+D^{1/2}r_{S_{t}}^{(t)}=D^{1/2}r^{(t)}+\\frac{1-\\alpha}{1+\\alpha}A D^{-1}D^{1/2}r_{S_{t}}^{(t)}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Note $\\mathbf{r}^{(t)}\\geq\\mathbf{0}$ and $\\|{\\cal A}{\\cal D}^{-1}{\\cal D}^{1/2}r_{S_{t}}^{(t)}\\|_{1}=\\|{\\cal D}^{1/2}r_{S_{t}}^{(t)}\\|_{1}$ . Hence, it leads to ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\|D^{1/2}r_{S_{t}}^{(t)}\\|_{1}=\\frac{1+\\alpha}{2\\alpha}\\left(\\|D^{1/2}r^{(t)}\\|_{1}-\\|D^{1/2}r^{(t+1)}\\|_{1}\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "At each local iterative $t$ , by the active node condition $2\\alpha\\epsilon\\sqrt{d_{u}}/(1+\\alpha)\\le r_{u}^{(t)}$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\epsilon\\operatorname{vol}(S_{t})=\\sum_{u\\in S_{t}}\\epsilon d_{u}\\leq\\sum_{u\\in S_{t}}\\frac{(1+\\alpha)\\sqrt{d_{u}}r_{u}^{(t)}}{2\\alpha}=\\frac{1+\\alpha}{2\\alpha}\\left\\|D^{1/2}r_{S_{t}}^{(t)}\\right\\|_{1}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Then the total run time of LOCGD presented in Algo. 4 is ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sum_{t=0}^{T-1}\\operatorname{vol}(S_{t})\\leq\\frac{1}{\\epsilon}\\left(\\frac{1+\\alpha}{2\\alpha}\\right)^{2}\\left(\\|D^{1/2}r^{(0)}\\|_{1}-\\|D^{1/2}r^{(T)}\\|_{1}\\right)\\leq\\frac{1+\\alpha}{2\\alpha\\epsilon}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Th efore, the total run time is at most $\\begin{array}{r}{\\mathcal{T}_{\\mathrm{LocGD}}:=\\sum_{t=0}^{T-1}\\mathrm{vol}(S_{t})\\leq\\frac{1+\\alpha}{2\\alpha\\epsilon}}\\end{array}$ . For estimating the bounds of $T$ , by the Weierstrass product inequality [26] a nd Lemma B.5, we use the similar argument made in Lemma B.3 and continue to have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{(1+\\alpha)}{2\\alpha\\overline{{\\gamma}}_{T}}\\left(1-\\frac{\\|\\tilde{r}^{(T)}\\|_{1}}{\\|\\tilde{r}^{(0)}\\|_{1}}\\right)\\leq T\\leq\\frac{(1+\\alpha)}{2\\alpha\\overline{{\\gamma}}_{T}}\\ln\\frac{\\|\\tilde{r}^{(0)}\\|_{1}}{\\|\\tilde{r}^{(T)}\\|_{1}},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Note that each nonzero r\u02dc(uT )has at least part of the magnitude from the push operation of an active node, say $v_{u}$ at time $t^{\\prime}<T$ . This means each nonzero of $D^{1/2}r^{(T)}$ satisfies ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\tilde{r}_{u}^{(T)}\\geq\\frac{(1-\\alpha)\\tilde{r}_{v_{u}}^{(t^{\\prime})}}{(1+\\alpha)d_{v_{u}}}\\geq\\frac{(1-\\alpha)\\cdot2\\alpha\\epsilon d_{v_{u}}/(1+\\alpha)}{(1+\\alpha)d_{v_{u}}}=\\frac{2\\alpha(1-\\alpha)\\epsilon}{(1+\\alpha)^{2}},\\;\\mathrm{for}\\;u\\in\\mathbb{Z}_{T}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Hence, we have $\\begin{array}{r}{\\|\\tilde{r}^{(T)}\\|_{1}\\geq\\frac{2\\alpha(1-\\alpha)\\epsilon|\\mathcal{Z}_{T}|}{(1+\\alpha)^{2}}}\\end{array}$ and $T$ is further bounded as ", "page_idx": 24}, {"type": "equation", "text": "$$\nT\\leq\\frac{(1+\\alpha)}{2\\alpha\\overline{{\\gamma}}_{T}}\\ln\\frac{\\Vert\\tilde{r}^{(0)}\\Vert_{1}}{\\frac{2\\alpha(1-\\alpha)\\epsilon|\\mathcal{Z}_{T}|}{(1+\\alpha)^{2}}}:=\\frac{(1+\\alpha)}{2\\alpha\\overline{{\\gamma}}_{T}}\\ln\\frac{C_{T}}{\\epsilon},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The lower bound of $1/\\epsilon$ , i.e., $\\overline{{\\mathrm{vol}}}(S_{t})/\\overline{{\\gamma}}_{T}\\leq1/\\epsilon$ , directly follows a similar strategy of previous proof by noticing that $\\|D^{1/2}r^{(t)}\\|_{1}$ is monotonically decreasing. \u53e3 ", "page_idx": 24}, {"type": "text", "text": "Remark B.6. One may consider designing local methods based on Jacobi and Richardson\u2019s iterations. Indeed, these two methods have the same updates as standard GD. Recall the standard GD method to solve (4) is $\\pmb{x}^{(t+1)}=\\pmb{x}^{(t)}+\\pmb{r}^{(t)},\\pmb{r}^{(t+1)}\\overset{\\cdot}{=}\\pmb{r}^{(t)}-\\pmb{Q r}^{(t)}$ . The Richardson\u2019s iteration is ${\\pmb x}^{(t+1)}=$ $(\\boldsymbol{I}-\\omega\\dot{\\boldsymbol{W}})\\boldsymbol{\\mathbf{\\mathit{x}}}^{(t)}+\\omega\\boldsymbol{\\mathbf{\\mathit{b}}}$ , i.e., $\\pmb{x}^{(t+1)}=\\pmb{x}^{(t)}-\\omega(\\pmb{W}\\pmb{x}^{(t)}-\\pmb{b})$ . The optimal $\\omega^{*}=2/(\\lambda_{\\operatorname*{min}}+\\lambda_{\\operatorname*{max}})$ where $\\lambda_{\\operatorname*{min}}=2\\alpha/(1+\\alpha)$ and $\\lambda_{\\operatorname*{max}}\\leq2/(1+\\dot{\\alpha})$ . Hence one can choose $\\omega=1\\leq\\omega^{*}$ [19]. It leads to $\\pmb{x}^{(t+1)}=\\pmb{x}^{(t)}+\\pmb{r}^{(t)}$ . One can get the same result for the Jacobi method. ", "page_idx": 24}, {"type": "text", "text": "C Local Chebyshev Method - LOCCH ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "C.1 Nonhomogeneous of Second-order Difference Equation ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We begin by providing the solutions of the second-order nonhomogeneous equation as the following Lemma C.1 (Stevic\u00b4 [47]). The solution of the second-order nonhomogeneous difference equation ", "page_idx": 24}, {"type": "equation", "text": "$$\nx_{t+1}+p x_{t}+q x_{t-1}=f_{t},\\quad t=1,2,.\\,.\\,.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "is characterized by the following two cases ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathfrak{z}_{t}=\\left\\{\\begin{array}{l l}{\\frac{1}{\\hat{\\lambda}_{2}-\\hat{\\lambda}_{1}}\\left(\\hat{\\lambda}_{1}^{t}\\left(\\hat{\\lambda}_{2}x_{0}-x_{1}-\\sum_{k=1}^{t}\\frac{f_{k}}{\\hat{\\lambda}_{1}^{k}}\\right)+\\hat{\\lambda}_{2}^{t}\\left(x_{1}-\\hat{\\lambda}_{1}x_{0}+\\sum_{k=1}^{t}\\frac{f_{k}}{\\hat{\\lambda}_{2}^{k}}\\right)\\right)}&{p^{2}\\neq4q}\\\\ {(-\\frac{p}{2})^{t}\\left(x_{0}-\\sum_{k=1}^{t}\\frac{k f_{k}}{(-p/2)^{k+1}}\\right)+t(-\\frac{p}{2})^{t-1}\\left(x_{1}-\\left(-\\frac{p}{2}\\right)x_{0}+\\sum_{k=1}^{t}\\frac{f_{k}}{(-p/2)^{k}}\\right)}&{p^{2}=4q}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\hat{\\lambda}_{1},\\hat{\\lambda}_{2}$ are two roots of $\\lambda^{2}+p\\lambda+q=0$ , and the summation follows convention $\\textstyle\\sum_{k=1}^{0}\\cdot=0$ . ", "page_idx": 24}, {"type": "text", "text": "Based on the above lemma, we have the following corollary ", "page_idx": 24}, {"type": "text", "text": "Corollary C.2 (Second-order nonhomogeneous equation). Given $\\vert a\\vert\\le1$ , the second-order nonhomogeneous equation ", "page_idx": 24}, {"type": "equation", "text": "$$\nx_{t+1}-2a x_{t}+x_{t-1}=f_{t}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "has the following solution ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\boldsymbol{\\mathfrak{x}}_{t}=\\left\\{\\begin{array}{l l}{x_{0}+t(x_{1}-x_{0})+\\sum_{k=1}^{t}(t-k)f_{k}}&{i f{a}=1}\\\\ {\\frac{\\sin(\\theta t){x}_{1}-\\sin(\\theta(t-1))x_{0}}{\\sin\\theta}+\\frac{\\sum_{k=1}^{t}\\sin(\\theta(t-k))f_{k}}{\\sin\\theta}}&{i f|a|<1\\,{\\,\\mathrm{where~}}\\theta=\\operatorname{arcconst}}\\\\ {(-1)^{t}(x_{0}-t(x_{0}+x_{1}))+(-1)^{t}\\left(\\sum_{k=1}^{t}(-1)^{-k-1}(t-k)f_{k}\\right)}&{i f a=-1.}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. The first and last two cases can be directly followed from Lemma C.1. Since $\\hat{\\lambda}_{1}$ and $\\hat{\\lambda}_{2}$ are the two complex roots of $\\lambda^{2}\\mathrm{~-~}2a\\lambda+1\\mathrm{~=~}0$ , we write $\\hat{\\lambda}_{1}\\,=\\,r e^{i\\theta}\\,=\\,r(\\cos\\theta+i\\sin\\theta)$ and $\\hat{\\lambda}_{2}=r e^{-i\\theta}=r(\\cos\\theta-i\\sin\\theta)$ . It indicates ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\lambda^{2}-2a\\lambda+1=\\left(\\lambda-r e^{i\\theta}\\right)\\left(\\lambda-r e^{-i\\theta}\\right)=\\lambda^{2}-r(e^{i\\theta}+e^{-i\\theta})\\lambda+r^{2}=0.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Since $1\\,>a^{2}$ , $\\hat{\\lambda}_{1}\\,=\\,a\\,-\\,i\\sqrt{1-a^{2}}$ , and $\\mathbf{Re}(\\hat{\\lambda}_{1})^{2}+\\mathbf{Im}(\\hat{\\lambda}_{1})^{2}=a^{2}+(1-a^{2})=1$ , then $r\\,=\\,1$ . Then $\\theta=\\operatorname{arccos}(\\mathbf{Re}(\\hat{\\lambda}_{1}))=\\operatorname{arccos}(a),$ , and $\\sin(\\theta)=\\mathbf{Im}({\\hat{\\lambda}}_{1})={\\sqrt{1-a^{2}}}$ . Finally, $\\hat{\\lambda}_{1}^{t}=e^{i t\\theta}=$ $\\cos(t\\theta)+i\\sin(t\\theta)$ , and ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\hat{\\lambda}_{1}=\\cos\\theta+i\\sin\\theta,\\quad\\hat{\\lambda}_{2}=\\cos\\theta-i\\sin\\theta}}\\\\ {{{}}}\\\\ {{\\hat{\\lambda}_{1}\\hat{\\lambda}_{2}=e^{i t\\theta}e^{-i t\\theta}=1}}\\\\ {{{}\\hat{\\lambda}_{2}-\\hat{\\lambda}_{1}=-2i\\sin\\theta}}\\\\ {{{}\\hat{\\lambda}_{1}^{t}=\\cos(\\theta t)+i\\sin(\\theta t),\\hat{\\lambda}_{2}^{t}=\\cos(\\theta t)-i\\sin(\\theta t)}}\\\\ {{{}\\hat{\\lambda}_{2}^{t}-\\hat{\\lambda}_{1}^{t}=e^{-i t\\theta}-e^{i t\\theta}=-2i\\sin(\\theta t).}}\\\\ {{{}\\hat{\\lambda}_{1}^{t-1}-\\hat{\\lambda}_{2}^{t-1}=2i\\sin(\\theta(t-1))^{\\hat{\\lambda}_{1}\\hat{\\lambda}_{2}=1}\\hat{\\lambda}_{1}^{t}\\hat{\\lambda}_{2}-\\hat{\\lambda}_{2}^{t}\\hat{\\lambda}_{1}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Based on these, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle x_{t}=\\frac{1}{\\tilde{\\lambda}_{2}-\\tilde{\\lambda}_{1}}\\left(\\dot{\\lambda}_{1}^{t}\\left(\\dot{\\lambda}_{2}x_{0}-x_{1}\\!-\\!\\frac{t}{\\xi_{1}\\sqrt{\\lambda_{1}}}\\right)+\\dot{\\lambda}_{2}^{t}\\left(x_{1}-\\dot{\\lambda}_{1}x_{0}+\\displaystyle\\frac{t}{\\xi_{1}\\sqrt{\\lambda_{1}}}\\right)\\right)}\\\\ {\\displaystyle}&{\\,\\,\\quad=\\frac{\\dot{\\lambda}_{1}^{t}\\left(\\dot{\\lambda}_{2}x_{0}-x_{1}\\right)+\\dot{\\lambda}_{2}^{t}\\left(x_{1}-\\dot{\\lambda}_{1}x_{0}\\right)}{\\tilde{\\lambda}_{2}-\\tilde{\\lambda}_{1}}+\\frac{1}{\\tilde{\\lambda}_{2}-\\tilde{\\lambda}_{1}}\\left(-\\dot{\\lambda}_{1}^{t}\\displaystyle\\sum_{i=1}^{\\ell}\\dot{\\frac{f}{\\lambda_{1}}}+\\dot{\\lambda}_{2}^{t}\\displaystyle\\frac{t}{\\Gamma}\\displaystyle\\frac{f_{i k}}{\\tilde{\\lambda}_{2}}\\right)}\\\\ {\\displaystyle}&{\\,\\,\\quad=\\frac{(2i\\sin(\\theta(t-1))x_{0}-2i\\sin(\\theta(t)x_{1}))}{-2i\\sin(\\theta)}+\\frac{1}{-2i\\sin(\\theta(t-1))}\\left(-\\dot{\\lambda}_{1}^{t}\\displaystyle\\sum_{k=1}^{\\ell}\\dot{\\frac{f}{\\lambda_{k}}}+\\dot{\\lambda}_{2}^{t}\\displaystyle\\sum_{k=1}^{\\ell}\\dot{\\frac{f}{\\lambda_{k}}}\\right)}\\\\ {\\displaystyle}&{\\,\\,\\quad=\\frac{\\sin(\\theta t)x_{1}-\\sin(\\theta(t-1))x_{0}}{\\sin\\theta}+\\frac{1}{-2i\\sin\\theta}\\displaystyle\\sum_{k=1}^{\\ell}\\left(\\dot{\\lambda}_{2}^{t}\\!-\\!\\dot{\\kappa}_{k}^{t-k}\\!\\right)f_{k}}\\\\ {\\displaystyle}&{\\,\\,\\quad=\\frac{\\sin(\\theta t)x_{1}-\\sin(\\theta(t-1))x_{0}}{\\sin\\theta}+\\frac{1}{-2i\\sin\\theta}\\displaystyle\\sum_{k=1}^{\\ell}-2i\\sin(\\theta(t-k))f_{k}}\\\\ {\\displaystyle}&{\\,\\,\\quad=\\frac{\\sin \n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Lemma C.3. For $|\\lambda_{i}|\\,\\leq\\,1,i\\,=\\,1,2,\\dots,n,$ , equations $y_{i}^{(t+1)}\\,-\\,2\\lambda_{i}y_{i}^{(t)}\\,+\\,y_{i}^{(t-1)}\\,=\\,0$ y(t\u22121) = 0 have the following solutions ", "page_idx": 25}, {"type": "equation", "text": "$$\ny_{i}^{(t)}=\\left\\{\\frac{\\sin(\\theta_{i}t)y_{i}^{(1)}-\\sin(\\theta_{i}(t-1))y_{i}^{(0)}}{\\sin(\\theta_{i})}\\right.\\ \\ \\ \\ \\psi\\ \\ \\left.\\left|\\lambda_{i}\\right|<1\\ w h e r e\\ \\theta_{i}=\\operatorname{arccos}(\\lambda_{i})\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Furthermore, when $y_{i}^{(1)}=\\lambda_{i}y_{i}^{(0)}$ for $i=1,2,\\dots,n,$ , then solutions can be simplified as ", "page_idx": 25}, {"type": "equation", "text": "$$\ny_{i}^{(t)}={\\left\\{\\begin{array}{l l l}{y_{i}^{(0)}}&{i f}&{\\lambda_{i}=1}\\\\ {y_{i}^{(0)}\\cos(\\theta_{i}t)}&{i f}&{|\\lambda_{i}|<1\\,w h e r e\\,\\theta_{i}=\\operatorname{arccos}(\\lambda_{i})}\\\\ {z_{i,0}(-1)^{t}}&{i f}&{\\lambda_{i}=-1.}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. The first part is a consequence of Corollary C.2 by letting $f_{t}=0$ . To see the second identity of this lemma, note that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\frac{\\big(\\lambda_{i}\\sin(\\theta_{i}t)-\\sin(\\theta_{i}(t-1))\\big)}{\\sin\\theta_{i}}=\\cos(\\theta_{i}t).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Indeed, by expanding $\\sin(\\theta_{i}(t-1))$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{\\sin(\\theta_{i}(t-1))=\\sin(\\theta_{i}t-\\theta_{i})}\\\\ &{=\\sin(\\theta_{i}t)\\cos(-\\theta_{i})+\\cos(\\theta_{i}t)\\sin(-\\theta_{i})}\\\\ &{=\\sin(\\theta_{i}t)\\cos(\\theta_{i})-\\cos(\\theta_{i}t)\\sin(\\theta_{i})}\\\\ &{=\\lambda_{i}\\sin(\\theta_{i}t)-\\cos(\\theta_{i}t)\\sin(\\theta_{i})}\\end{array}}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Hence, when yi(1)= \u03bbiyi(0), we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\frac{\\sin(\\theta_{i}t)y_{i}^{(1)}-\\sin(\\theta_{i}(t-1))y_{i}^{(0)}}{\\sin(\\theta_{i})}=\\frac{\\left(\\lambda_{i}\\sin(\\theta_{i}t)-\\sin(\\theta_{i}(t-1))\\right)y_{i}^{(0)}}{\\sin\\theta_{i}}=\\cos(\\theta_{i}t)y_{i}^{(0)}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Lemma C.4. Given $t\\geq1,|\\lambda_{i}|\\leq1$ , the n second-order difference equations ", "page_idx": 26}, {"type": "equation", "text": "$$\ny_{i}^{(t+1)}-2\\lambda_{i}y_{i}^{(t)}+y_{i}^{(t-1)}=h_{i,t},\\qquad i=1,2,\\ldots,n.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "have the following solutions ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\iota_{i}^{(t)}=\\left\\{\\begin{array}{l l}{y_{i}^{(0)}+t(y_{i}^{(1)}-y_{i}^{(0)})+\\sum_{k=1}^{t-1}(t-k)h_{i,k}}&{i f\\quad\\lambda_{i}=1}\\\\ {\\frac{\\sin(\\theta_{i}t)y_{i}^{(1)}-\\sin(\\theta_{i}(t-1))y_{i}^{(0)}}{\\sin(\\theta_{i})}+\\sum_{k=1}^{t-1}\\frac{\\sin(\\theta_{i}(t-k))}{\\sin\\theta_{i}}h_{i,k}}&{i f\\quad|\\lambda_{i}|<1\\mathrm{~}w h e r e\\;\\theta_{i}=\\mathrm{arc}}\\\\ {(-1)^{t}(z_{i,0}-t(y_{i}^{(0)}+y_{i}^{(1)}))+\\sum_{k=1}^{t-1}(-1)^{t-k-1}(t-k)h_{i,k}}&{i f\\quad\\lambda_{i}=-1.}\\end{array}\\right.}\\end{array}\n$$$\\theta_{i}=\\operatorname{arccos}(\\lambda_{i})$ ", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Furthermore, with initial conditions $y_{i}^{(1)}=\\lambda_{i}y_{i}^{(0)}$ , $y_{i}^{(t)}$ can be simplified as ", "page_idx": 26}, {"type": "equation", "text": "$$\ny_{i}^{(t)}=\\left\\{\\begin{array}{l l}{y_{i}^{(0)}+\\sum_{k=1}^{t-1}(t-k)h_{i,k}}&{i f\\quad\\lambda_{i}=1}\\\\ {\\cos(\\theta_{i}t)y_{i}^{(0)}+\\sum_{k=1}^{t-1}\\frac{\\sin(\\theta_{i}(t-k))}{\\sin\\theta_{i}}h_{i,k}}&{i f\\quad|\\lambda_{i}|<1\\,w h e r e\\;\\theta_{i}=\\operatorname{arccos}(\\lambda_{i})}\\\\ {(-1)^{t}y_{i}^{(0)}+\\sum_{k=1}^{t-1}(-1)^{t-k-1}(t-k)h_{i,k}}&{i f\\quad\\lambda_{i}=-1.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. The first part is a consequence of Corollary C.2 by letting $f_{t}=h_{i,t}$ . We prove the second part by considering three cases of $\\lambda_{i}$ as ", "page_idx": 26}, {"type": "text", "text": "\u2022 Case 1. When $\\lambda_{i}=1$ , we have $y_{i}^{(t+1)}-2y_{i}^{(t)}+y_{i}^{(t-1)}=h_{i,t}$ yi(t\u22121)= hi,t. For t \u22651, the solution the above is ", "page_idx": 26}, {"type": "equation", "text": "$$\ny_{i}^{(t)}=y_{i}^{(0)}+t(y_{i}^{(1)}-y_{i}^{(0)})+\\sum_{k=1}^{t-1}(t-k)h_{i,k}=y_{i}^{(0)}+\\sum_{k=1}^{t-1}(t-k)h_{i,k},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the second equation is due to $y_{i}^{(1)}=\\lambda_{1}y_{i}^{(0)}=y_{i}^{(0)}$ . ", "page_idx": 26}, {"type": "text", "text": "\u2022 Case 2. When $\\lambda_{n}=-1$ ( $\\mathcal{G}$ is a bipartite graph), we have $y_{i}^{(t+1)}+2y_{i}^{(t)}+y_{i}^{(t-1)}=h_{i,t}$ For $t\\geq1$ , the solution is ", "page_idx": 26}, {"type": "equation", "text": "$$\ny_{i}^{(t)}=(-1)^{t}y_{i}^{(0)}+(-1)^{t}\\sum_{k=1}^{t-1}\\frac{(t-k)h_{i,k}}{(-1)^{k+1}}=(-1)^{t}y_{i}^{(0)}+\\sum_{k=1}^{t-1}(-1)^{t-k-1}(t-k)h_{i,k}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "\u2022 Case 3. When $|\\lambda_{i}|<1$ , and define $\\theta_{i}=\\operatorname{arccos}(\\lambda_{i})$ . We use a similar argument in Lemma C.3 and continue to have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{y_{i}^{(t)}=\\frac{\\sin(\\theta_{i}t)y_{i}^{(1)}-\\sin(\\theta_{i}(t-1))y_{i}^{(0)}}{\\sin\\theta_{i}}+\\frac{\\sum_{k=1}^{t-1}\\sin(\\theta_{i}(t-k))h_{i,k}}{\\sin\\theta_{i}}}\\\\ &{\\quad=\\frac{\\left(\\lambda_{i}\\sin(\\theta_{i}t)-\\sin(\\theta_{i}(t-1))\\right)y_{i}^{(0)}}{\\sin\\theta_{i}}+\\frac{\\sum_{k=1}^{t-1}\\sin(\\theta_{i}(t-k))h_{i,k}}{\\sin\\theta_{i}}}\\\\ &{\\quad=\\cos(\\theta_{i}t)y_{i}^{(0)}+\\displaystyle\\sum_{k=1}^{t-1}\\frac{\\sin(\\theta_{i}(t-k))}{\\sin\\theta_{i}}h_{i,k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "C.2 Properties on Ratio of Chebyshev Polynomials ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "The next lemma presents the properties of Chebyshev polynomials. ", "page_idx": 27}, {"type": "text", "text": "Lemma C.5 (Chebyshev polynomial bound). For $t\\geq1$ , the Chebyshev polynomial of the first kind is defined recursively as ", "page_idx": 27}, {"type": "equation", "text": "$$\nT_{t+1}(x)=2x T_{t}(x)-T_{t-1}(x)\\ \\ \\ \\ w i t h\\ \\ \\ T_{0}(x)=1,\\ \\ \\ T_{1}(x)=x.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "For $t\\geq1$ , define $\\begin{array}{r}{\\delta_{t}=T_{t-1}(\\frac{1+\\alpha}{1-\\alpha})/T_{t}(\\frac{1+\\alpha}{1-\\alpha})}\\end{array}$ , then", "page_idx": 27}, {"type": "text", "text": "1. $\\textstyle T_{t}(x={\\frac{1+\\alpha}{1-\\alpha}})$ and $\\delta_{t}$ defines the following sequence ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\delta_{t+1}=\\left(2\\frac{1+\\alpha}{1-\\alpha}-\\delta_{t}\\right)^{-1},\\;w h e r e\\;\\delta_{1}=\\frac{1-\\alpha}{1+\\alpha}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "2. The closed-form $\\delta_{1:t}$ can be upper bounded as ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\delta_{1:t}=\\frac{1}{T_{t}(\\frac{1+\\alpha}{1-\\alpha})}=\\frac{2}{\\tilde{\\alpha}^{t}+\\tilde{\\alpha}^{-t}}\\leq2\\left(\\frac{1-\\sqrt{\\alpha}}{1+\\sqrt{\\alpha}}\\right)^{t}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "3. Note $\\begin{array}{r}{\\delta_{1}=T_{0}/T_{1}=1/x=\\frac{1-\\alpha}{1+\\alpha}}\\end{array}$ , the sequence $\\{\\delta_{t}\\}$ satisfies $\\delta_{t}<1,\\forall t\\geq1$ and ", "page_idx": 27}, {"type": "equation", "text": "$$\n1=2\\delta_{t+1}x-\\delta_{t}\\delta_{t+1},\\quad t=1,2,\\ldots.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "4. Denote $\\begin{array}{r}{\\delta_{j:t}=\\prod_{i=j}^{t}\\delta_{i}}\\end{array}$ for $t\\ge j\\ge0$ and set the default value $\\delta_{j:j-1}=1$ for $j\\geq0$ , then ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\delta_{1:t}/(\\delta_{1:k})=\\delta_{k+1:t}\\leq2\\tilde{\\alpha}^{t-k},\\,f o r\\,t\\geq k\\geq0.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. For the first item, let $\\textstyle x={\\frac{1+\\alpha}{1-\\alpha}}\\,$ , use the Chebyshev equation, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n1=2\\left(\\frac{1+\\alpha}{1-\\alpha}\\right)\\frac{T_{t}}{T_{t+1}}-\\frac{T_{t-1}}{T_{t+1}}=2\\left(\\frac{1+\\alpha}{1-\\alpha}\\right)\\delta_{t+1}-\\delta_{t}\\delta_{t+1}\\quad\\Rightarrow\\quad\\delta_{t+1}^{-1}=2\\left(\\frac{1+\\alpha}{1-\\alpha}\\right)-\\delta_{t}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "For the second item, for all $t\\geq0$ , if $\\begin{array}{r}{\\xi=\\frac{x+x^{-1}}{2}\\neq0}\\end{array}$ , it is well known that the $T_{t}$ can be rewritten as ", "page_idx": 27}, {"type": "equation", "text": "$$\nT_{t}\\left(\\xi={\\frac{x+x^{-1}}{2}}\\right)={\\frac{x^{t}+x^{-t}}{2}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "For our problem, recall we defined $\\begin{array}{r}{\\tilde{\\alpha}=\\frac{1-\\sqrt{\\alpha}}{1+\\sqrt{\\alpha}}}\\end{array}$ and using $\\textstyle x={\\frac{1+\\alpha}{1-\\alpha}}\\neq0$ , one can verify that ", "page_idx": 27}, {"type": "equation", "text": "$$\nx=\\frac{1+\\alpha}{1-\\alpha}=\\frac{\\tilde{\\alpha}+\\tilde{\\alpha}^{-1}}{2}\\implies T_{t}\\left(x=\\frac{1+\\alpha}{1-\\alpha}\\right)=T_{t}\\left(\\frac{\\tilde{\\alpha}+\\tilde{\\alpha}^{-1}}{2}\\right)=\\frac{\\tilde{\\alpha}^{t}+\\tilde{\\alpha}^{-t}}{2}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\prod_{j=1}^{t}\\delta_{j}=\\frac{T_{0}}{T_{1}}\\cdot\\frac{T_{1}}{T_{2}}\\cdot\\frac{T_{2}}{T_{3}}\\cdot\\cdot\\cdot\\frac{T_{t-1}}{T_{t}}=\\frac{1}{T_{t}}=\\frac{2}{\\tilde{\\alpha}^{t}+\\tilde{\\alpha}^{-t}}=\\frac{2\\tilde{\\alpha}^{t}}{\\tilde{\\alpha}^{2t}+1}\\leq2\\tilde{\\alpha}^{t}=2\\left(\\frac{1-\\sqrt{\\alpha}}{1+\\sqrt{\\alpha}}\\right)^{t}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "For the third item, it is sufficient to show that $\\delta_{t}\\leq\\frac{1}{x}$ for all $t\\geq1$ , for $\\textstyle x={\\frac{1+\\alpha}{1-\\alpha}}$ . This can be done recursively, since $\\delta_{1}=x$ and ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\delta_{t+1}={\\frac{1}{2x-\\delta_{t}}}\\leq{\\frac{1}{2x-{\\frac{1}{x}}}}\\sum_{x}^{x\\geq{\\frac{1}{x}}}{\\frac{1}{x}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "For the last item, note when $t\\geq1$ and $k\\geq1$ , we have the following inequalities ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\prod_{j=1}^{t}\\delta_{j}\\cdot\\prod_{j=1}^{k}\\delta_{j}^{-1}=\\frac{2}{\\tilde{\\alpha}^{t}+\\tilde{\\alpha}^{-t}}\\cdot\\frac{\\tilde{\\alpha}^{k}+\\tilde{\\alpha}^{-k}}{2}=\\frac{\\tilde{\\alpha}^{k}+\\tilde{\\alpha}^{-k}}{\\tilde{\\alpha}^{t}+\\tilde{\\alpha}^{-t}}=\\tilde{\\alpha}^{t-k}\\frac{\\tilde{\\alpha}^{2k}+1}{\\tilde{\\alpha}^{2t}+1}\\leq2\\tilde{\\alpha}^{t-k},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where note $\\frac{\\tilde{\\alpha}^{2k}\\!+\\!1}{\\tilde{\\alpha}^{2t}\\!+\\!1}\\in[1,2]$ for $t\\geq k$ . ", "page_idx": 27}, {"type": "text", "text": "C.3 Standard Chebyshev (CH) Method and Proof of Theorem C.6 ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "This subsection introduces the standard Chebyshev algorithm. Our following theorem is to prove the runtime complexity of the Chebyshev polynomial iteration for solving Equ. (3). ", "page_idx": 28}, {"type": "text", "text": "Theorem C.6 (Standard CH). For $t\\geq1$ , consider the Chebyshev polynomials to solve Equ. (3) as ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{r}^{(t+1)}=\\mathbf{x}^{(t)}+(1+\\delta_{t:t+1})r^{(t)}+\\delta_{t:t+1}\\big(\\mathbf{x}^{(t)}-\\mathbf{x}^{(t-1)}\\big),\\ \\ r^{(t+1)}=2\\delta_{t+1}W r^{(t)}-\\delta_{t:t+1}r^{(t-1)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where ${\\pmb x}^{(0)}={\\bf0},{\\pmb x}^{(1)}={\\pmb x}^{(0)}+{\\pmb r}^{(0)}$ and $\\begin{array}{r}{\\delta_{t+1}=\\left(2\\frac{1+\\alpha}{1-\\alpha}-\\delta_{t}\\right)^{-1}}\\end{array}$ with $\\begin{array}{r}{\\delta_{1}=\\frac{1-\\alpha}{1+\\alpha}}\\end{array}$ . Assume $\\epsilon<1/d_{s}$ then the residual has the following convergence bound ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left\\|\\pmb{r}^{(t)}\\right\\|_{2}\\leq2\\left(\\frac{1-\\sqrt{\\alpha}}{1+\\sqrt{\\alpha}}\\right)^{t}\\|\\pmb{b}\\|_{2}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Let the estimate be $\\hat{\\pmb{\\pi}}={\\pmb D}^{1/2}{\\pmb x}^{(t)}$ , the the runtime of CH for reaching $\\begin{array}{r}{\\|D^{-1/2}\\boldsymbol{r}^{(t)}\\|_{\\infty}\\leq\\frac{2\\alpha\\epsilon}{1+\\alpha}}\\end{array}$ with $\\|D^{-1}(\\hat{\\pi}-\\pi)\\|_{\\infty}\\leq\\epsilon$ guarantee is at most ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathcal{T}_{\\mathrm{CH}}\\leq\\Theta\\left(m\\left\\lceil\\frac{1+\\sqrt{\\alpha}}{2\\sqrt{\\alpha}}\\ln\\frac{2}{\\epsilon}\\right\\rceil\\right)=\\tilde{\\Theta}\\left(\\frac{m}{\\sqrt{\\alpha}}\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. Recall eigendecomposition of $W=V\\Lambda V^{\\top}$ where $V=[\\pmb{v}_{1},\\pmb{v}_{2},\\dots,\\pmb{v}_{n}]$ and each $\\pmb{v}_{i}$ is the eigenvector. For $t\\geq1$ , the residual $\\mathbf{\\boldsymbol{r}}^{(t)}$ can be written as $n$ second-order difference equations as ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{V^{\\top}r^{(t+1)}-2\\delta_{t+1}\\Lambda V^{\\top}r^{(t)}+\\delta_{t:t+1}V^{\\top}r^{(t-1)}=\\mathbf{0},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where each $i$ -th element-wise equation of the above can be written as the following ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{v_{i}^{\\top}r^{(t+1)}-2\\delta_{t+1}\\lambda_{i}v_{i}^{\\top}r^{(t)}+\\delta_{t+1}\\delta_{t}v_{i}^{\\top}r^{(t-1)}=0,\\qquad i=1,2,\\ldots,n.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Define $V^{\\top}{\\boldsymbol r}^{(t)}=\\delta_{1:t}{\\boldsymbol y}^{(t)}$ . Each component $\\pmb{v}_{i}^{\\top}\\pmb{r}^{(t)}$ is $\\pmb{v}_{i}^{\\top}\\pmb{r}^{(t)}=\\delta_{1:t}\\pmb{y}_{i}^{(t)}$ where $\\pmb{v}_{i}^{\\top}\\pmb{r}^{(0)}:=\\pmb{y}_{i}^{(0)}$ by default. The above can be rewritten ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\delta_{1:t+1}y_{i}^{(t+1)}-2\\delta_{t+1}\\delta_{1:t}\\lambda_{i}y_{i}^{(t)}+\\delta_{t+1}\\delta_{t}\\delta_{1:t-1}y_{i}^{(t-1)}=0}\\\\ {\\Longleftrightarrow\\ y_{i}^{(t+1)}-2\\lambda_{i}y_{i}^{(t)}+y_{i}^{(t-1)}=0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\Longleftrightarrow$ follows from $\\delta_{1:t+1}\\neq0$ . Note $\\begin{array}{r}{V^{\\top}{\\boldsymbol{r}}^{(1)}=\\frac{1-\\alpha}{1+\\alpha}V^{\\top}V\\mathbf{A}V^{\\top}{\\boldsymbol{r}}^{(0)}=\\delta_{1}\\mathbf{A}V^{\\top}{\\boldsymbol{r}}^{(0)}}\\end{array}$ , we have ", "page_idx": 28}, {"type": "equation", "text": "$$\nV^{\\top}r^{(0)}=y^{(0)}\\qquad\\Rightarrow\\qquad V^{\\top}r^{(1)}=\\delta_{1}y^{(1)}=\\delta_{1}{\\pmb{\\Lambda}}V^{\\top}r^{(0)}=\\delta_{1}{\\pmb{\\Lambda}}y^{(0)},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where it follows from $\\delta_{1}\\neq0$ . As $\\pmb{y}^{(1)}\\,=\\,\\pmb{\\Lambda}\\pmb{y}^{(0)}$ , follow Equ. (24) of Lemma C.3, $y_{i}^{(t)}$ has the solution ", "page_idx": 28}, {"type": "equation", "text": "$$\ny_{i}^{(t)}=\\left\\{y_{i}^{(0)}\\cos(\\theta_{i}t)\\quad|\\lambda_{i}|<1\\leq\\left\\{|y_{i}^{(0)}|\\quad|\\lambda_{i}|<1\\right.,\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\theta_{i}=\\operatorname{arccos}(\\lambda_{i})$ . We can write down $\\mathbf{\\boldsymbol{r}}^{(t)}$ in terms of $\\pmb{y}^{(t)}$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbf{V}^{\\top}\\mathbf{r}^{(t)}=\\delta_{1:t}\\pmb{y}^{(t)}=\\delta_{1:t}\\pmb{Z}_{t}\\pmb{y}^{(0)},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $Z_{t}$ has two possible forms ", "page_idx": 28}, {"type": "equation", "text": "$$\nZ_{t}={\\left\\{\\begin{array}{l l}{\\mathbf{diag}\\left(1,\\ldots,\\cos(\\theta_{i}t),\\ldots,(-1)^{t}\\right)\\qquad{\\mathrm{for~bipartite~graphs}}}&\\\\ {\\mathbf{diag}\\left(1,\\ldots,\\cos(\\theta_{i}t),\\ldots,\\cos(\\theta_{n}t)\\right)\\qquad{\\mathrm{for~non-bipartite~graphs}}}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Hence, $\\|Z_{t}\\|_{2}\\leq1$ and $\\|Z_{t}\\pmb{y}^{(0)}\\|_{2}\\leq\\|\\pmb{y}^{(0)}\\|_{2}$ . We have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\|r^{(t)}\\|_{2}=\\|V^{\\top}r^{(t)}\\|_{2}\\leq\\delta_{1:t}\\|y^{(0)}\\|_{2}\\leq2\\left(\\frac{1-\\sqrt{\\alpha}}{1+\\sqrt{\\alpha}}\\right)^{t}\\|b\\|_{2},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where the last inequality follows Lemma C.5 and note $z^{(0)}=V^{\\top}r^{(0)}$ . To meet the stop condition of $\\bigl\\{|r_{u}^{(t)}|<2\\alpha\\epsilon\\sqrt{d_{u}}/(1+\\alpha),u\\in\\mathcal{V}\\bigr\\}=\\emptyset,$ , it is sufficient to choose a minimal integer $t$ such that ", "page_idx": 29}, {"type": "equation", "text": "$$\n2\\left(\\frac{1-\\sqrt{\\alpha}}{1+\\sqrt{\\alpha}}\\right)^{t}\\|b\\|_{2}<\\frac{2\\alpha\\epsilon}{(1+\\alpha)\\sqrt{d_{s}}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "To see this, if the above inequality is satisfied, then note for any node $u$ , we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n|r_{u}^{(t)}|\\leq\\|r^{(t)}\\|_{\\infty}\\leq\\|r^{(t)}\\|_{2}\\leq2\\left(\\frac{1-\\sqrt{\\alpha}}{1+\\sqrt{\\alpha}}\\right)^{t}\\|b\\|_{2}<\\frac{2\\alpha\\epsilon}{(1+\\alpha)\\sqrt{d_{s}}}\\leq\\frac{2\\alpha\\epsilon\\sqrt{d_{u}}}{(1+\\alpha)},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where all nodes are inactive. So, it gives $\\begin{array}{r}{t\\ln\\left(\\frac{1-\\sqrt{\\alpha}}{1+\\sqrt{\\alpha}}\\right)\\leq\\ln\\frac{\\epsilon}{2}}\\end{array}$ by noticing $||\\pmb{b}||_{2}=2\\alpha/((1+\\alpha)\\sqrt{d_{s}})$ . It indicates $\\textstyle t\\geq\\left\\lceil\\ln{\\frac{2}{\\epsilon}}\\right\\rceil\\ln\\left({\\frac{1+{\\sqrt{\\alpha}}}{1-{\\sqrt{\\alpha}}}}\\right)\\,$ . As $\\begin{array}{r}{\\frac{1}{\\ln(1+x)}\\leq\\frac{1+x}{x}}\\end{array}$ for $x>0$ , it is sufficient to choose $t$ ", "page_idx": 29}, {"type": "equation", "text": "$$\nt=\\left\\lceil\\frac{1+\\sqrt{\\alpha}}{2\\sqrt{\\alpha}}\\ln\\frac{2}{\\epsilon}\\right\\rceil.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "C.4 Residual Updates of LOCCH and Proof of Lemma 4.1 ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "We propose the following local Chebyshev iteration procedure ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{x}^{(t+1)}=\\pmb{x}^{(t)}+(1+\\delta_{t:t+1})\\pmb{r}_{S_{t}}^{(t)}+\\delta_{t:t+1}\\big(\\pmb{x}^{(t)}-\\pmb{x}^{(t-1)}\\big)_{S_{t}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Our next Lemma is to expanding $\\left(\\pmb{x}^{(t)}-\\pmb{x}^{(t-1)}\\right)_{S_{t}}$ ", "page_idx": 29}, {"type": "text", "text": "Lemma C.7. For $t\\geq1$ with initials $\\mathbf{\\pmb{x}}^{(0)}=\\mathbf{0}$ and $\\pmb{x}^{(1)}=\\pmb{x}^{(0)}+\\pmb{r}_{S_{0}}^{(0)}$ , the local Chebyshev iterative ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{x}^{(t+1)}=\\pmb{x}^{(t)}+(1+\\delta_{t:t+1})\\pmb{r}_{S_{t}}^{(t)}+\\delta_{t:t+1}\\big(\\pmb{x}^{(t)}-\\pmb{x}^{(t-1)}\\big)_{S_{t}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Denote \u2206(t) = x(t) \u2212x(t\u22121), then \u2206(t) = tj\u2212=10 (1 + \u03b4j:j+1) tr\u2212=1j+1 \u03b4r:r+1r(Sjj):t\u22121 , where $\\delta_{0:1}=0$ ${\\bf\\Gamma}_{1}=0,\\,S_{0:t}=S_{0}\\cap S_{1}\\cap\\cdot\\cdot\\cdot\\cap S_{t}$ and $\\delta_{j:j+1}=\\delta_{j}\\delta_{j+1}$ . We have the following ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\big(1+\\delta_{t:t+1}\\big)r\\frac{(t)}{\\bar{S}_{t}}+\\delta_{t:t+1}\\big(x^{(t)}-x^{(t-1)}\\big)\\frac{\\tau}{\\bar{S}_{t}}=\\sum_{j=0}^{t}\\left(\\big(1+\\delta_{j:j+1}\\big)\\prod_{r=j+1}^{t}\\delta_{r:r+1}r_{{S}_{j,t}}^{(j)}\\right),\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $S_{j,t}\\triangleq S_{j:t-1}\\cap\\overline{{S}}_{t}$ . ", "page_idx": 29}, {"type": "text", "text": "Proof. Recall we defined $\\delta_{0}=0$ so that $\\delta_{0:1}=0$ . We prove this lemma by induction. ", "page_idx": 29}, {"type": "text", "text": "For $t=1$ , note $\\delta_{0:1}=0$ and the support of $r^{(0)}$ is $S_{0}=\\mathrm{supp}(r^{(0)})$ , then ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\pmb{x}^{(1)}-\\pmb{x}^{(0)}=(1+\\delta_{0:1})\\pmb{r}^{(0)}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "For $t=2$ , note $S_{1:1}=S_{1}$ and $S_{0:1}=S_{0}\\cap S_{1}$ by our notation, then ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{x}^{(2)}-\\pmb{x}^{(1)}=(1+\\delta_{1:2})\\pmb{r}_{S_{1:1}}^{(1)}+\\delta_{1:2}(\\pmb{x}^{(1)}-\\pmb{x}^{(0)})_{S_{1}}}\\\\ {=(1+\\delta_{1:2})\\pmb{r}_{S_{1:1}}^{(1)}+(1+\\delta_{0:1})\\delta_{1:2}\\pmb{r}_{S_{0:1}}^{(0)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "For $t=3$ , one can build $\\pmb{x}^{(3)}-\\pmb{x}^{(2)}$ based on $\\pmb{x}^{(2)}-\\pmb{x}^{(1)}$ and recall $S_{2:2}=S_{2},S_{1:2}=S_{1}\\cap S_{2}$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pmb{x}^{(3)}-\\pmb{x}^{(2)}=(1+\\delta_{2:3})\\pmb{r}_{S_{2:2}}^{(2)}+\\delta_{2:3}(\\pmb{x}^{(2)}-\\pmb{x}^{(1)})_{S_{2}}}\\\\ &{\\qquad\\qquad=(1+\\delta_{2:3})\\pmb{r}_{S_{2:2}}^{(2)}+\\delta_{2:3}((1+\\delta_{1:2})\\pmb{r}_{S_{1:1}}^{(1)}+(1+\\delta_{0:1})\\delta_{1:2}\\pmb{r}_{S_{0:1}}^{(0)})_{S_{2}}}\\\\ &{\\qquad\\qquad=(1+\\delta_{2:3})\\pmb{r}_{S_{2:2}}^{(2)}+(1+\\delta_{1:2})\\delta_{2:3}\\pmb{r}_{S_{1:2}}^{(1)}+(1+\\delta_{0:1})\\delta_{1:2}\\delta_{2:3}\\pmb{r}_{S_{0:2}}^{(0)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "For $t=4$ , we continue to have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{x^{(4)}-x^{(3)}=(1+\\delta_{3:4})r_{S_{3:3}}^{(3)}+\\delta_{3:4}(x^{(3)}-x^{(2)})_{S_{3:3}}}\\\\ &{\\hphantom{x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x}}\\\\ &{\\hphantom{x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x}=(1+\\delta_{3:4})r_{S_{3:3}}^{(3)}+\\delta_{3:4}((1+\\delta_{2:3})r_{S_{2:2}}^{(2)}+(1+\\delta_{1:2})\\delta_{2:3}r_{S_{1:2}}^{(1)}}\\\\ &{\\hphantom{x x x x x x x x}+(1+\\delta_{0:1})\\delta_{1:2}\\delta_{2:3}r_{S_{0:2}}^{(0)})_{S_{3:3}}}\\\\ &{\\hphantom{x x x x x x x x}+(1+\\delta_{0:1})\\delta_{1:2}\\delta_{2:3}r_{S_{0:2}}^{(0)}\\delta_{3:3}}\\\\ &{\\hphantom{x x x x x x x}=(1+\\delta_{3:4})r_{S_{3:3}}^{(3)}+(1+\\delta_{2:3})\\delta_{3:4}r_{S_{2:3}}^{(2)}+(1+\\delta_{1:2})\\delta_{3:4}\\delta_{2:3}r_{S_{1:3}}^{(1)}}\\\\ &{\\hphantom{x x x x x x x}+(1+\\delta_{0:1})\\delta_{1:2}\\delta_{2:3}\\delta_{3:4}r_{S_{0:3}}^{(0)}}\\\\ &{\\hphantom{x x x x x}=(x^{(4)}-x^{(3)})_{\\overline{{S_{4}}}}=\\big((1+\\delta_{3:4})r_{S_{3:3}}^{(3)}+\\delta_{3:4}(x^{(3)}-x^{(2)})_{S_{3:3}}\\big)_{\\overline{{S}}_{4}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "By induction $t\\geq1$ , ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\pmb{x}^{(t)}-\\pmb{x}^{(t-1)}=\\sum_{j=0}^{t-1}\\Bigg((1+\\delta_{j:j+1})\\prod_{r=j+1}^{t-1}\\delta_{r:r+1}r_{S_{j:t-1}}^{(j)}\\Bigg),\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where the convention notation $\\textstyle\\sum_{i=1}^{0}\\cdot=0$ and $\\textstyle\\prod_{j=i+1}^{i}\\cdot=1$ . To verify the inductive step, consider for $t+1$ , we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pmb{x}^{(t+1)}-\\pmb{x}^{(t)}=(1+\\delta_{t:t+1})\\pmb{r}_{S_{t}}^{(t)}+\\delta_{t:t+1}(\\pmb{x}^{(t)}-\\pmb{x}^{(t-1)})\\delta_{t}}\\\\ &{\\qquad\\qquad=(1+\\delta_{t:t+1})\\pmb{r}_{S_{t}}^{(t)}+\\delta_{t:t+1}(\\pmb{x}^{(t)}-\\pmb{x}^{(t-1)})\\delta_{t}}\\\\ &{\\qquad\\qquad=(1+\\delta_{t:t+1})\\pmb{r}_{S_{t}}^{(t)}+\\delta_{t:t+1}\\left(\\displaystyle\\sum_{j=0}^{t-1}\\left((1+\\delta_{j:j+1})\\displaystyle\\prod_{r=j+1}^{t-1}\\delta_{r:r+1}\\pmb{r}_{S_{j:t-1}}^{(j)}\\right)\\right)_{S_{t}}}\\\\ &{\\qquad\\qquad=\\displaystyle\\sum_{j=0}^{t}\\left((1+\\delta_{j:j+1})\\displaystyle\\prod_{r=j+1}^{t}\\delta_{r:r+1}\\pmb{r}_{S_{j:t}}^{(j)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "To see the second equation, note ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(1+\\delta_{t:t+1})r\\frac{(t)}{S_{t}}+\\delta_{t:t+1}\\big(x^{(t)}-x^{(t-1)}\\big)_{\\overline{{S}}_{t}}}\\\\ &{=(1+\\delta_{t:t+1})r\\frac{(t)}{S_{t}}+\\displaystyle\\sum_{j=0}^{t-1}\\left((1+\\delta_{j:j+1})\\displaystyle\\prod_{r=j+1}^{t}\\delta_{r:r+1}r_{S_{j:t-1}\\cap\\overline{{S}}_{t}}^{(j)}\\right)}\\\\ &{=\\displaystyle\\sum_{j=0}^{t}\\left((1+\\delta_{j:j+1})\\displaystyle\\prod_{r=j+1}^{t}\\delta_{r:r+1}r_{S_{j,t}}^{(j)}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where recall we denote $S_{j,t}\\triangleq S_{j:t-1}\\cap\\overline{{S}}_{t}$ . ", "page_idx": 30}, {"type": "text", "text": "Lemma C.8 (Local Chebyshev updates). Given the updates of $\\pmb{x}^{(t+1)}$ as defined by LOCCH in (10), we have the following local updates ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{\\begin{array}{l l}{\\pmb{x}^{(t+1)}=\\pmb{x}^{(t)}+(1+\\delta_{t:t+1})\\pmb{r}_{\\mathcal{S}_{t}}^{(t)}+\\delta_{t:t+1}\\big(\\pmb{x}^{(t)}-\\pmb{x}^{(t-1)}\\big)_{\\mathcal{S}_{t}}}\\\\ {\\pmb{r}^{(t+1)}-2\\delta_{t+1}\\pmb{W}\\pmb{r}^{(t)}+\\delta_{t:t+1}\\pmb{r}^{(t-1)}=\\sum_{j=0}^{t}\\Big((1+\\delta_{j:j+1})\\prod_{r=j+1}^{t}\\delta_{r:r+1}Q\\pmb{r}_{\\mathcal{S}_{j,t}}^{(j)}\\Big)}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof. We only need to show the second equation of (28). The residual of LOCCH updates is ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{r^{(t+1)}=b-Q x^{(t+1)}}\\\\ &{r^{(t+1)}=b-Q\\big(x^{(t)}+(1+\\delta_{t:t+1})r_{S_{t}}^{(t)}+\\delta_{t:t+1}(x^{(t)}-x^{(t-1)})_{S_{t}}\\big)}\\\\ &{\\qquad=r^{(t)}-(1+\\delta_{t:t+1})Q r_{S_{t}}^{(t)}-\\delta_{t:t+1}Q(x^{(t)}-x^{(t-1)})_{S_{t}}}\\\\ &{\\qquad\\underbrace{r^{(t+1)}+(1+\\delta_{t:t+1})Q r^{(t)}+\\delta_{t:t+1}Q(x^{(t)}-x^{(t-1)})-r^{(t)}}_{u}}\\\\ &{\\qquad=\\underbrace{\\left(1+\\delta_{t:t+1}\\right)Q r_{S_{t}}^{(t)}+\\delta_{t:t+1}Q\\big(x^{(t)}-x^{(t-1)}\\big)_{\\overline{{S_{t}}}}}_{\\mathrm{(i)}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Note $Q(\\mathbf{x}^{(t)}-\\mathbf{x}^{(t-1)})=b-Q\\mathbf{x}^{(t-1)}-(b-Q(\\mathbf{x}^{(t)})=r^{(t-1)}-r^{(t)}$ and then $\\textbf{\\em u}$ becomes ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pmb{u}=\\pmb{r}^{(t+1)}+(1+\\delta_{t:t+1})\\pmb{Q}\\pmb{r}^{(t)}+\\delta_{t:t+1}\\big(\\pmb{r}^{(t-1)}-\\pmb{r}^{(t)}\\big)-\\pmb{r}^{(t)}}\\\\ &{\\quad=\\pmb{r}^{(t+1)}-2\\delta_{t+1}\\pmb{W}\\pmb{r}^{(t)}+\\delta_{t:t+1}\\pmb{r}^{(t-1)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where the last equality is due to $(1+\\delta_{t:t+1})Q r^{(t)}=(1+\\delta_{t:t+1})r^{(t)}-2\\delta_{t:t+1}W r^{(t)}$ by noticing $\\textstyle(1+\\delta_{t}\\delta_{t+1}){\\frac{1-\\alpha^{*}}{1+\\alpha}}=2\\dot{\\delta}_{t+1}$ in Lemma C.5. Hence, we have the second equation. To see the noisy part, note by Lemma C.7 ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad(1+\\delta_{t:t+1})r\\frac{(t)}{S_{t}}+\\delta_{t:t+1}\\big(\\mathbf{x}^{(t)}-\\mathbf{x}^{(t-1)}\\big)_{\\overline{{S}}_{t}}=(1+\\delta_{t:t+1})r\\frac{(t)}{S_{t}}+\\displaystyle\\sum_{j=0}^{t-1}\\Big((1+\\delta_{j:j+1})\\prod_{r=j+1}^{t}\\delta_{r:r+1}r}\\\\ &{\\displaystyle1+\\delta_{t:t+1}\\big)Q r\\frac{(t)}{S_{t}}+\\delta_{t:t+1}Q\\big(\\mathbf{x}^{(t)}-\\mathbf{x}^{(t-1)}\\big)_{\\overline{{S}}_{t}}=\\displaystyle\\sum_{j=0}^{t}\\Big((1+\\delta_{j:j+1})\\prod_{r=j+1}^{t}\\delta_{r:r+1}Q r\\big(j\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Lemma 4.1 (Residual updates of LOCCH). Given $t\\ge1,\\pmb{x}^{(0)}=\\mathbf{0}$ , $\\pmb{x}^{(1)}=\\pmb{x}^{0}+\\pmb{r}_{S_{0}}^{(0)}$ r(S0) . The residual $\\mathbf{\\boldsymbol{r}}^{(t)}$ of LOCCH defined in Equ. (28) satisfies ", "page_idx": 31}, {"type": "equation", "text": "$$\nV^{\\top}r^{(t)}=\\delta_{1:t}Z_{t}V^{\\top}r^{(0)}+\\delta_{1:t}t\\pmb{u}_{0,t}+2\\sum_{k=1}^{t-1}\\delta_{k+1:t}(t-k)\\pmb{u}_{k,t},\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\boldsymbol{u}_{k,t}=\\left\\{\\begin{array}{l l}{\\sum_{j=1}^{t-1}\\delta_{2:j}H_{j,t}\\left(I-\\frac{1-\\alpha}{1+\\alpha}\\Lambda\\right)\\boldsymbol{V}^{\\top}\\boldsymbol{r}_{S_{0,j}}^{(0)}/t}&{i f k=0}\\\\ {\\sum_{j=k}^{t-1}\\left(\\delta_{k+1:j}H_{j,t}\\left(\\frac{1+\\alpha}{1-\\alpha}I-\\Lambda\\right)\\boldsymbol{V}^{\\top}\\boldsymbol{r}_{S_{k,j}}^{(k)}\\right)/(t-k)}&{i f k\\ge1,}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "equation", "text": "$$\n{\\bf Z}_{t}=\\left\\{\\!\\!\\!\\begin{array}{l}{{\\bf d i a g}\\left(1,\\ldots,\\cos(\\theta_{i}t),\\ldots,(-1)^{t}\\right)\\quad{\\it f o r\\,b i p a r t i t e\\,g r a p h s}}\\\\ {{\\bf d i a g}\\left(1,\\ldots,\\cos(\\theta_{i}t),\\ldots,\\cos(\\theta_{n}t)\\right)\\quad{\\it f o r\\,n o n-b i p a r t i t e\\,g r a p h s},}\\end{array}\\ \\right.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "equation", "text": "$$\nH_{k,t}=\\left\\{\\begin{array}{l l}{\\mathbf{diag}\\left(t-k,\\,\\dots,\\frac{\\sin\\left(\\theta_{i}\\left(t-k\\right)\\right)}{\\sin\\theta_{i}},\\,\\dots,(-1)^{t-k-1}(t-k)\\right)\\quad\\,f o r\\,b i p a r t i t e\\,\\,g r a p h s}\\\\ {\\mathbf{diag}\\left(t-k,\\,\\dots,\\frac{\\sin\\left(\\theta_{i}\\left(t-k\\right)\\right)}{\\sin\\theta_{i}},\\,\\dots,\\frac{\\sin\\left(\\theta_{n}\\left(t-k\\right)\\right)}{\\sin\\theta_{n}}\\right)\\quad\\,f o r\\,n o n{-}b i p a r t i t e\\,\\,g r a p h s.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Proof. We first decompose the residual equation in (28) as ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V^{\\top}r^{(t+1)}-2\\delta_{t+1}\\Lambda V^{\\top}r^{(t)}+\\delta_{t:t+1}V^{\\top}r^{(t-1)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\sum_{j=0}^{t}\\left((1+\\delta_{j:j+1})\\prod_{r=j+1}^{t}\\left(\\delta_{r:r+1}\\right)\\left(I-\\frac{1-\\alpha}{1+\\alpha}\\Lambda\\right)V^{\\top}r_{\\mathcal{S}_{j,t}}^{(j)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Define $V^{\\top}{\\boldsymbol r}^{(t)}=\\delta_{1:t}{\\boldsymbol y}^{(t)}$ and $V^{\\top}{r}^{(0)}=\\delta_{1:0}{y}^{(0)}={y}^{(0)}$ by default. Then we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\delta_{1:t+1}y^{(t+1)}-2\\delta_{1:t+1}\\Lambda y^{(t)}+\\delta_{1:t+1}y^{(t-1)}=f^{(t)}\\;\\Rightarrow\\;y^{(t+1)}-2\\Lambda y^{(t)}+y^{(t-1)}=\\frac{f^{(t)}}{\\delta_{1:t+1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Note $\\pmb{V}^{\\top}\\pmb{r}^{(1)}=\\delta_{1}\\pmb{y}^{(1)}=\\pmb{V}^{\\top}\\delta_{1}\\pmb{W}\\pmb{r}^{(0)}=\\delta_{1}\\pmb{\\Lambda}\\pmb{y}^{(0)}$ , which indicates $\\pmb{y}^{(1)}=\\pmb{\\Lambda}\\pmb{y}^{(0)}$ . Then, by the Lemma C.4, each $y_{i}^{(t)}$ has the solution ", "page_idx": 32}, {"type": "equation", "text": "$$\ny_{i}^{(t)}=\\left\\{\\begin{array}{l l}{y_{i}^{(0)}+\\sum_{k=1}^{t-1}(t-k)f_{i}^{(k)}/(\\delta_{1:k+1})}&{\\mathrm{if}\\quad\\lambda_{i}=1}\\\\ {\\cos(\\theta_{i}t)y_{i}^{(0)}+\\sum_{k=1}^{t-1}\\frac{\\sin(\\theta_{i}(t-k))}{\\sin\\theta_{i}}f_{i}^{(k)}/(\\delta_{1:k+1})}&{\\mathrm{if}\\quad|\\lambda_{i}|<1}\\\\ {(-1)^{t}y_{i}^{(0)}+\\sum_{k=1}^{t-1}(-1)^{t-k-1}(t-k)f_{i}^{(k)}/(\\delta_{1:k+1})}&{\\mathrm{if}\\quad\\lambda_{i}=-1.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Use $Z_{t}$ and ${\\cal H}_{k,t}$ , we write the solution of the second-order difference equation as ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{y^{(t)}=Z_{t}y^{(0)}+\\sum_{k=1}^{t-1}\\!\\!H_{k,t}f^{(k)}/(\\delta_{1:k+1})}}\\ ~}\\\\ {{\\displaystyle{\\gamma^{\\top}r^{(t)}=\\delta_{1:t}y^{(t)}=\\delta_{1:t}Z_{t}V^{\\top}r^{(0)}+\\delta_{1:t}\\sum_{k=1}^{t-1}\\!\\!H_{k,t}f^{(k)}/(\\delta_{1:k+1})}}\\ ~}\\\\ {{\\displaystyle{\\gamma^{\\top}r^{(t)}=\\delta_{1:t}Z_{t}V^{\\top}r^{(0)}+\\sum_{k=1}^{t-1}\\delta_{k+2:t}H_{k,t}\\sum_{j=0}^{k}\\left((1+\\delta_{j:j+1})\\prod_{r=j+1}^{k}\\left(\\delta_{r:r+1}\\right)\\left(I-\\frac{1-\\alpha}{1+\\alpha}\\right)V^{\\top}r_{\\delta_{r}}^{(0)}\\right)}}\\ ~}\\\\ {{\\displaystyle{\\operatorname*{max}_{1:t\\leq t}1_{\\leq t}\\quad\\geq\\varepsilon-1_{1:t}\\sum_{k=1}^{t-1}\\delta_{1:t\\leq m}\\left(1+\\delta_{1:t}\\right)_{r=j+1}\\sum_{r=j+1}^{t}\\left(\\delta_{r:r+1}\\right)\\left(I-\\frac{1-\\alpha}{1+\\alpha}\\right)V^{\\top}r_{\\delta_{r}}^{(0)}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Note $\\begin{array}{r}{1+\\delta_{j:j+1}=2\\delta_{j+1}\\frac{1+\\alpha}{1-\\alpha},j\\geq1}\\end{array}$ , then $\\begin{array}{r}{(1+\\delta_{j:j+1})\\prod_{r=j+1}^{k}\\left(\\delta_{r:r+1}\\right)=2\\frac{1+\\alpha}{1-\\alpha}\\delta_{j+1:k}\\delta_{j+1:k+1}.}\\end{array}$ Then, we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle V^{\\top}r^{(t)}=\\delta_{1:t}Z_{t}V^{\\top}r^{(0)}+\\sum_{k=1}^{t-1}\\delta_{k+2:t}H_{k,t}\\left(\\delta_{2:k}\\delta_{1:k+1}\\left(I-\\frac{1-\\alpha}{1+\\alpha}\\Lambda\\right)V^{\\top}r_{S_{0,k}}^{(0)}\\right)}\\ ~}\\\\ {{\\displaystyle~~~~~~~~~~~~~~~~~~~~~~~+\\sum_{k=1}^{t-1}\\delta_{k+2:t}H_{k,t}\\sum_{j=1}^{k}\\left(\\delta_{j+1:k}\\delta_{j+1:k+1}\\left(\\frac{1+\\alpha}{1-\\alpha}I-\\Lambda\\right)V^{\\top}r_{S_{j,k}}^{(j)}\\right)}\\ ~}\\\\ {{\\displaystyle~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~}=\\delta_{1:t}Z_{t}V^{\\top}r^{(0)}+\\delta_{1:t}\\displaystyle\\sum_{k=1}^{t-1}\\delta_{j:k}H_{k,t}\\left(I-\\frac{1-\\alpha}{1+\\alpha}\\Lambda\\right)V^{\\top}r_{S_{0,k}}^{(0)}}\\ ~}\\\\ {{\\displaystyle~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~}+\\sum_{k=1}^{t-1}\\delta_{k+2:t}H_{k,t}\\displaystyle\\sum_{j=1}^{k}\\left(\\delta_{j+1:k}\\delta_{j+1:k+1}\\left(\\frac{1+\\alpha}{1-\\alpha}I-\\Lambda\\right)V^{\\top}r_{S_{j,k}}^{(j)}\\right),}\\ ~}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where the last term can be expanded as ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k=1}^{t-1}\\delta_{k+2t}H_{k,t}\\displaystyle\\sum_{j=1}^{k}\\left(\\delta_{j+1,k}\\delta_{j+1;k+1}\\underbrace{\\left(\\frac{1+\\alpha}{1-\\alpha}I-\\Lambda\\right)V^{\\top}r_{S_{j,k}}^{(j)}}_{w_{S_{j,k}}^{(j)}}\\right)=}\\\\ &{\\displaystyle\\delta_{3t,t}H_{1,t}\\delta_{2;1}\\delta_{22}w_{S_{1,1}^{(1)}}+}\\\\ &{\\displaystyle\\delta_{4t}H_{2,t}\\delta_{2;2}\\delta_{23}w_{S_{1,2}^{(1)}}+\\delta_{i;t}H_{2,t}\\delta_{3;2}\\delta_{3;3}u_{S_{2,2}^{(2)}}^{(2)}+}\\\\ &{\\displaystyle\\delta_{5t,t}H_{3,t}\\delta_{2;3}\\delta_{2;4}w_{S_{1,3}^{(1)}}+\\delta_{5t,H_{3}}\\delta_{3;3}\\delta_{3;4}w_{S_{2,3}^{(2)}}+\\delta_{5t,H_{3}}\\delta_{4;3}\\delta_{4;4}w_{S_{3,3}^{(3)}}^{(3)}+}\\\\ &{\\displaystyle\\delta_{6t,t}H_{4,t}\\delta_{2;4}\\delta_{2;5}w_{S_{1,4}^{(1)}}+\\delta_{6t}H_{4,t}\\delta_{3;4}\\delta_{3;5}w_{S_{2,4}^{(2)}}^{(2)}+\\delta_{6t,H_{4}}\\delta_{4;6}\\delta_{4;5}w_{S_{3,4}^{(3)}}^{(3)}+\\delta_{6t}H_{4,t}\\delta_{5;4}\\delta_{5;5}w_{S_{4,4}^{(4)}}}\\\\ &{\\displaystyle\\delta_{7t,}H_{5,t}\\delta_{2;5}\\delta_{26}w_{S_{1,5}^{(1)}}+\\delta_{7t,}H_{5,t}\\delta_{3;5}\\delta_{3;6}w_{S_{2,5}^{(2)}}^{(2)}+\\delta_{7t,}H_{5,t}\\delta_{4;5}\\delta_{4;6}w_{S_{3,5}^{(3)}}^{(2) \n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "Here, we denote $\\delta_{k+1:k}=1$ . The final iterative update is ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle V^{\\top}r^{(t)}=\\delta_{1:t}Z_{t}V^{\\top}r^{(0)}+\\delta_{1:t}t\\sum_{j=1}^{t-1}\\delta_{2:j}H_{j,t}\\left(I-\\frac{1-\\alpha}{1+\\alpha}\\Lambda\\right)V^{\\top}r_{S_{0,j}}^{(0)}\\bigg/t}}\\\\ {{\\displaystyle}}\\\\ {{\\displaystyle}+\\,2\\sum_{k=1}^{t-1}\\delta_{k+1:t}(t-k)\\sum_{j=k}^{t-1}\\left(\\delta_{k+1:j}H_{j,t}\\left(\\frac{1+\\alpha}{1-\\alpha}I-\\Lambda\\right)V^{\\top}r_{S_{k,j}}^{(k)}\\right)\\bigg/(t-k)\\,.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "C.5 Convergence of LOCCH and Proof of Theorem 4.2 ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Corollary C.9. Let $\\beta_{k}$ be lower bound of residual reduction satisfies $\\|\\boldsymbol{u}_{k,t}\\|_{2}\\leq\\beta_{k}\\|\\boldsymbol{r}^{(k)}\\|_{2}$ , then the upper bound of $\\|\\pmb{r}^{(t)}\\|_{2}$ can be characterized as ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\|r^{(t)}\\|_{2}\\leq\\delta_{1:t}\\prod_{j=0}^{t-1}(1+\\beta_{j})y_{t},\\,w h e r e\\,y_{t+1}-2y_{t}+\\frac{y_{t-1}}{(1+\\beta_{t-1})(1+\\beta_{t})}=0,\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $y_{0}=y_{1}=\\|\\pmb{r}^{0}\\|_{2}$ . ", "page_idx": 33}, {"type": "text", "text": "Proof. Since $\\|\\boldsymbol{u}_{k,t}\\|_{2}\\leq\\beta_{k}\\|\\boldsymbol{r}^{(k)}\\|_{2}$ , the final iterative updates (29) can be bounded as ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V^{\\top}r^{(t)}=\\delta_{1:t}Z_{t}V^{\\top}r^{(0)}+\\delta_{1:t}t u_{0,t}+2\\displaystyle\\sum_{k=1}^{t-1}\\delta_{k+1:t}\\big(t-k\\big)u_{k,t}}\\\\ &{\\|r^{(t)}\\|_{2}\\leq\\delta_{1:t}\\|r^{(0)}\\|_{2}+\\delta_{1:t}t\\beta_{0}\\|r^{(0)}\\|_{2}+2\\displaystyle\\sum_{k=1}^{t-1}\\delta_{k+1:t}\\big(t-k\\big)\\beta_{k}\\|r^{(k)}\\|_{2}}\\\\ &{\\|r^{(t)}\\|_{2}-2\\displaystyle\\sum_{k=1}^{t-1}\\delta_{k+1:t}\\big(t-k\\big)\\beta_{k}\\|r^{(k)}\\|_{2}\\leq\\delta_{1:t}\\big(1+t\\beta_{0}\\big)\\|r^{(0)}\\|_{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $t=0,1,\\ldots,T$ . These $T+1$ (including a trivial one where $\\|\\pmb{r}^{(0)}\\|_{2}\\leq\\|\\pmb{r}^{(0)}\\|_{2})$ inequalities shown in Equ. (31) form a system of linear inequality matrix as ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left(\\begin{array}{c c c c c}{1}&{0}&{0}&{\\cdots}&{0}\\\\ {-z_{21}}&{1}&{0}&{\\cdots}&{0}\\\\ {-z_{31}}&{-z_{32}}&{1}&{\\cdots}&{0}\\\\ {\\vdots}&{\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}\\\\ {-z_{T1}}&{-z_{T2}}&{-z_{T3}}&{\\cdots}&{1}\\end{array}\\right)\\left(\\begin{array}{c}{\\|r^{(1)}\\|_{2}}\\\\ {\\|r^{(2)}\\|_{2}}\\\\ {\\|r^{(3)}\\|_{2}}\\\\ {\\vdots}\\\\ {\\|r^{(T-1)}\\|_{2}}\\end{array}\\right)\\leq\\left(\\begin{array}{c}{\\delta_{1:1}(1+1\\beta_{0})\\|r^{(0)}\\|_{2}}\\\\ {\\delta_{1:2}(1+2\\beta_{0})\\|r^{(0)}\\|_{2}}\\\\ {\\delta_{1:3}(1+3\\beta_{0})\\|r^{(0)}\\|_{2}}\\\\ {\\vdots}\\\\ {\\delta_{1:T}(1+T\\beta_{0}t)\\|r^{(0)}\\|_{2}}\\end{array}\\right):=c,}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $(Z_{L})_{t k}\\;=\\;2\\delta_{k+1:t}(t\\:-\\:k)\\beta_{k}$ for $t\\,=\\,2,3,\\ldots,T$ and $k\\,=\\,1,2,\\dots,t\\,-\\,1$ . Assume that $N\\in\\mathbb{R}^{T\\times T}$ is a strictly lower triangular matrix, then we know the established formula $({\\cal I}+{\\cal N})^{-1}=$ $\\begin{array}{r}{I+\\sum_{k=1}^{T-1}(-1)^{k}N^{k}}\\end{array}$ . Hence, we have the following ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\left(I-Z_{L}\\right)^{-1}=I+\\sum_{k=1}^{T-1}Z_{L}^{k}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Given that $\\left(I-Z_{L}\\right)^{-1}\\geq\\mathbf{0}$ , then we obtain an upper-bound of $\\left(\\begin{array}{c}{||r^{(1)}||_{2}}\\\\ {||r^{(2)}||_{2}}\\\\ {\\vdots}\\\\ {||r^{(T)}||_{2}}\\end{array}\\right)\\leq z\\triangleq(I-Z_{L})^{-1}\\,c.$ ", "page_idx": 34}, {"type": "text", "text": "It leads to the following new second-order difference equation ", "page_idx": 34}, {"type": "equation", "text": "$$\nz_{t}-2\\sum_{k=1}^{t-1}\\delta_{k+1:t}(t-k)\\beta_{k}z_{k}=\\delta_{1:t}(1+t\\beta_{0})z_{0},\\qquad\\mathrm{for}\\;t=1,2,\\ldots,T,\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where the initial value of $z_{0}=\\|\\pmb{r}^{(0)}\\|_{2}$ . Following the argument in Theorem 1 of Golub & Overton [18], we construct a second-order homogeneous equation for $z_{t}$ as ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{z_{t}=\\delta_{1:t}(1+t\\beta_{0})z_{0}+2\\displaystyle\\sum_{k=1}^{t-1}\\delta_{k+1:t}(t-k)\\beta_{k}z_{k}}}\\\\ {{z_{t+1}=\\delta_{1:t+1}(1+(t+1)\\beta_{0})z_{0}+2\\displaystyle\\sum_{k=1}^{t}\\delta_{k+1:t+1}(t+1-k)\\beta_{k}z_{k}}}\\\\ {{\\delta_{t+1}z_{t}=\\delta_{1:t+1}(1+t\\beta_{0})z_{0}+2\\displaystyle\\sum_{k=1}^{t}\\delta_{k+1:t+1}(t-k)\\beta_{k}z_{k}}}\\\\ {{z_{t+1}-\\delta_{t+1}z_{t}=\\delta_{1:t+1}\\beta_{0}z_{0}+2\\displaystyle\\sum_{k=1}^{t}\\delta_{k+1:t+1}\\beta_{k}z_{k},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where Equ. (32) is obtained by the difference between the second equation and the third equation. Similarly, ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle z_{t-1}=\\delta_{1:t-1}(1+(t-1)\\beta_{0})z_{0}+2\\sum_{k=1}^{t-2}\\delta_{k+1:t-1}(t-k-1)\\beta_{k}z_{k}}\\\\ &{\\displaystyle\\delta_{t:t+1}z_{t-1}=\\delta_{1:t+1}(1+(t-1)\\beta_{0})z_{0}+2\\sum_{k=1}^{t-2}\\delta_{k+1:t+1}(t-k-1)\\beta_{k}z_{k}}\\\\ &{\\displaystyle\\delta_{t+1}z_{t}-\\delta_{t:t+1}z_{t-1}=\\delta_{1:t+1}\\beta_{0}z_{0}+2\\sum_{k=1}^{t-1}\\delta_{k+1:t+1}\\beta_{k}z_{k},}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where Equ. (33) is obtained by the difference of the first two. Hence, Equ. (32) \u2212(33) gives us ", "page_idx": 34}, {"type": "equation", "text": "$$\nz_{t+1}-2(1+\\beta_{t})\\delta_{t+1}z_{t}+\\delta_{t:t+1}z_{t-1}=0,\\qquad\\mathrm{for}\\,t=1,2,\\dots,T,\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where two initials are $z_{0}=\\|\\pmb{r}^{(0)}\\|_{0}$ and $z_{1}=\\delta_{1}(1+\\beta_{0})\\lVert r^{(0)}\\rVert_{2}$ . Let $z_{t}=\\delta_{1:t}\\hat{z}_{t}$ , then ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\hat{z}_{t+1}-2(1+\\beta_{t})\\hat{z}_{t}+\\hat{z}_{t-1}=0,\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where two initials are $\\hat{z}_{0}=z_{0}=\\|\\pmb{r}^{(0)}\\|_{0}$ and $\\hat{z}_{1}=(1+\\beta_{0})\\|\\pmb{r}^{(0)}\\|_{2}$ . We finish the proof by setting $\\begin{array}{r}{\\prod_{j=0}^{t-1}(1+\\beta_{j})y_{t}=\\hat{z}_{t}}\\end{array}$ . \u53e3 ", "page_idx": 34}, {"type": "text", "text": "Remark C.10. Key points of the above proof strategy largely follow from Golub & Overton [18]. However, different from the original technique, we generalize the strategy to a parameterized version. Lemma C.11. Given $\\beta_{j}\\geq0$ , the following second-order difference equation ", "page_idx": 34}, {"type": "equation", "text": "$$\nx_{t+1}-2(1+\\beta_{t})x_{t}+x_{t-1}=0.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "has the following solution ", "page_idx": 34}, {"type": "equation", "text": "$$\nx_{t}=\\prod_{j=0}^{t-1}(1+\\beta_{j})y_{t},\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where $\\begin{array}{r}{y_{t+1}-2y_{t}+\\frac{y_{t-1}}{(1+\\beta_{t-1})(1+\\beta_{t})}=0}\\end{array}$ with $y_{0}=x_{0}$ and $y_{1}=x_{1}/(1+\\beta_{0})$ . ", "page_idx": 34}, {"type": "text", "text": "Proof. Assume $\\begin{array}{r}{x_{t}=\\left(-\\frac{1}{2}\\right)^{t}\\prod_{j=0}^{t-1}\\left(-2(1+\\beta_{j})\\right)y_{t}}\\end{array}$ . Then, following the equation, we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\left(-\\frac{1}{2}\\right)^{t+1}\\prod_{j=0}^{t}\\left(-2(1+\\beta_{j})\\right)y_{t+1}}}\\\\ {{\\displaystyle-\\,2(1+\\beta_{t})\\left(-\\frac{1}{2}\\right)^{t}\\prod_{j=0}^{t-1}\\left(-2(1+\\beta_{j})\\right)y_{t}+\\left(-\\frac{1}{2}\\right)^{t-1}\\prod_{j=0}^{t-2}\\left(-2(1+\\beta_{j})\\right)y_{t-1}=0.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Since $\\beta_{j}\\geq0$ , the term $\\begin{array}{r}{\\prod_{j=0}^{t}\\left(-2(1+\\beta_{j})\\right)\\neq0.}\\end{array}$ , we divide it on both sides to have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\left(-\\frac{1}{2}\\right)^{t+1}y_{t+1}+\\left(-\\frac{1}{2}\\right)^{t}y_{t}+\\left(-\\frac{1}{2}\\right)^{t-1}\\frac{1}{4(1+\\beta_{t-1})(1+\\beta_{t})}y_{t-1}=0.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Hence, it is simplified into $\\begin{array}{r}{y_{t+1}-2y_{t}+\\frac{y_{t-1}}{(1+\\beta_{t-1})(1+\\beta_{t})}=0}\\end{array}$ . To make a simplification on $x_{t}$ , we prove the lemma. \u53e3 ", "page_idx": 35}, {"type": "text", "text": "Theorem 4.2 (Runtime bound of LOCCH). Given the configuration $\\theta=(\\alpha,\\epsilon,s,\\mathcal{G})$ with $\\alpha\\in(0,1)$ and $\\epsilon\\leq1/d_{s}$ and let $r^{(T)}$ and ${\\pmb x}^{(T)}$ be returned by LOCCH defined in (10) for solving Equ. (3). For $t\\geq1$ , the residual magnitude $\\|\\pmb{r}^{(t)}\\|_{2}$ has the following convergence bound ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\|r^{(t)}\\|_{2}\\le\\delta_{1:t}\\prod_{j=0}^{t-1}(1+\\beta_{j})y_{t},\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where $y_{t}$ is a sequence of positive numbers solving $y_{t+1}-2y_{t}+y_{t-1}/((1+\\beta_{t-1})(1+\\beta_{t}))=0$ with $y_{0}=y_{1}=\\|\\pmb{r}^{(0)}\\|_{2}$ . Suppose the geometric mean $\\begin{array}{r}{\\overline{{\\beta}}_{t}\\triangleq(\\prod_{j=0}^{t-1}(1+\\beta_{j}))^{1/t}}\\end{array}$ of $\\beta_{t}$ be such that $\\begin{array}{r}{\\overline{{\\beta}}_{t}=1+\\frac{c\\sqrt{\\alpha}}{1-\\sqrt{\\alpha}}}\\end{array}$ where $c\\in[0,2)$ . There exists a real implementation of (9) such that the runtime $\\mathcal{T}_{\\mathrm{LocCH}}$ is bounded by ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathcal{T}_{\\mathrm{LocCH}}\\leq\\Theta\\left(\\frac{(1+\\sqrt\\alpha)\\overline{{\\mathrm{vol}}}(S_{T})}{\\sqrt\\alpha(2-c)}\\ln\\frac{2y_{T}}{\\epsilon}\\right).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Proof. The convergence bound of $\\pmb{r}^{(t)}$ directly follows from Corollary C.9. Since we assume that there exists $c\\in[0,2)$ such that $\\begin{array}{r}{\\prod_{j=0}^{t-1}(1+\\beta_{j})\\leq\\left(1+\\frac{c\\sqrt{\\alpha}}{1-\\sqrt{\\alpha}}\\right)^{t}}\\end{array}$ . Then multiplying both sides by $\\tilde{\\alpha}^{t}$ we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\tilde{\\alpha}^{t}\\prod_{j=0}^{t-1}(1+\\beta_{j})\\leq\\left(1-\\frac{(2-c)\\sqrt{\\alpha}}{1+\\sqrt{\\alpha}}\\right)^{t}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Then we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\|r^{(t)}\\|_{2}\\leq\\delta_{1:t}\\prod_{j=0}^{t-1}(1+\\beta_{j})y_{t}\\stackrel{\\delta_{1:t}\\leq2\\tilde{\\alpha}^{t}}{\\leq}2\\tilde{\\alpha}^{t}\\bar{\\beta}_{t}^{t}y_{t}\\leq\\epsilon}}\\\\ {{\\displaystyle t\\ln\\left(\\frac{1-\\sqrt{\\alpha}}{1+\\sqrt{\\alpha}}\\Big(\\prod_{j=0}^{t-1}(1+\\beta_{j})\\Big)^{1/t}\\right)\\leq\\ln\\left(\\frac{\\epsilon}{2y_{t}}\\right)}}\\\\ {{\\displaystyle t\\geq\\left[\\ln\\left(\\frac{2y_{t}}{\\epsilon}\\right)\\Bigg/\\ln\\left(\\frac{1+\\sqrt{\\alpha}}{(1-\\sqrt{\\alpha})\\overline{{\\beta}}_{t}}\\right)\\right]}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Since \u03b2t =  tj\u2212=10(1 + \u03b2j) 1/t, and by using 1+xx\u2265ln(11+x) and letting $\\begin{array}{r}{x=\\frac{1+\\sqrt{\\alpha}}{\\left(1-\\sqrt{\\alpha}\\right)\\overline{{\\beta}}_{t}}-1>0}\\end{array}$ , then $t$ can be lower bounded further by ", "page_idx": 35}, {"type": "equation", "text": "$$\nt\\geq\\left\\lceil\\frac{1+\\sqrt{\\alpha}}{1+\\sqrt{\\alpha}-(1-\\sqrt{\\alpha})\\overline{{\\beta}}_{t}}\\ln\\left(\\frac{2y_{t}}{\\epsilon}\\right)\\right\\rceil\\geq\\left\\lceil\\ln\\left(\\frac{2y_{t}}{\\epsilon}\\right)\\right\\rceil\\ln\\left(\\frac{1+\\sqrt{\\alpha}}{\\left(1-\\sqrt{\\alpha}\\right)\\overline{{\\beta}}_{t}}\\right)\\right\\rceil.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Since we assumed $\\begin{array}{r}{\\beta_{t}\\,=\\,(1+\\frac{c\\sqrt{\\alpha}}{1-\\sqrt{\\alpha}})}\\end{array}$ , which means $\\begin{array}{r}{1\\,\\le\\,\\bar{\\beta}_{t}\\,=\\,(1+\\frac{c\\sqrt{\\alpha}}{1-\\sqrt{\\alpha}})}\\end{array}$ , so $\\textstyle\\overline{{\\beta}}_{t}\\,\\in\\,\\left[1,\\frac{1+\\sqrt{\\alpha}}{1-\\sqrt{\\alpha}}\\right]$ .   \nThen, we find such an upper bound of $t$ so that LOCCH converges. ", "page_idx": 36}, {"type": "equation", "text": "$$\nt=\\left\\lceil\\frac{1+\\sqrt{\\alpha}}{1+\\sqrt{\\alpha}-(1-\\sqrt{\\alpha})\\overline{{\\beta}}_{t}}\\ln\\left(\\frac{2y_{t}}{\\epsilon}\\right)\\right\\rceil=\\left\\lceil\\frac{1+\\sqrt{\\alpha}}{(2-c)\\sqrt{\\alpha}}\\ln\\left(\\frac{2y_{t}}{\\epsilon}\\right)\\right\\rceil.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "C.6 Implementation of LOCCH ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "We present the implementation of LOCCH as follows: Recall the sequence $\\begin{array}{r}{\\delta_{t+1}~=~\\left(2\\frac{1+\\alpha}{1-\\alpha}\\right.-}\\end{array}$ $\\delta_{t}{\\Big)}^{-1},t\\,=\\,1,2,\\ldots$ with $\\begin{array}{r}{\\delta_{1}\\,=\\,\\frac{1-\\alpha}{1+\\alpha}}\\end{array}$ . Denote $\\tilde{\\pmb{x}}^{(t)}\\,\\triangleq\\,\\pmb{x}^{(t)}\\,-\\pmb{x}^{(t-1)},\\pmb{\\Delta}^{(t)}\\,:=\\,(1+\\delta_{t:t+1})\\pmb{r}^{(t)}\\,+$ $\\delta_{t:t+1}\\tilde{\\pmb{x}}^{(t)}$ , we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pmb{x}^{(t+1)}=\\pmb{x}^{(t)}+(1+\\delta_{t:t+1})\\pmb{r}_{S_{t}}^{(t)}+\\delta_{t:t+1}\\tilde{\\pmb{x}}_{S_{t}}^{(t)}=\\pmb{x}^{(t)}+\\pmb{\\Delta}_{S_{t}}^{(t)}}\\\\ &{\\pmb{r}^{(t+1)}=\\pmb{b}-\\pmb{Q}\\left(\\pmb{x}^{(t)}+(1+\\delta_{t:t+1})\\pmb{r}_{S_{t}}^{(t)}+\\delta_{t:t+1}\\tilde{\\pmb{x}}_{S_{t}}^{(t)}\\right)=\\pmb{r}^{(t)}-\\pmb{Q}\\Delta_{S_{t}}^{(t)}}\\\\ &{\\tilde{\\pmb{x}}^{(t+1)}=\\tilde{\\pmb{x}}^{(t)}+\\Delta_{S_{t}}^{(t)}-\\Delta_{S_{t-1}}^{(t-1)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "\u2022 When $t=0$ , we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\qquad x^{(0)}={\\bf0},\\quad r^{(0)}=b,\\quad\\tilde{x}^{(0)}={\\bf0},\\quad{\\bf\\Delta}\\Delta^{(0)}=r^{(0)}.}}\\\\ {{\\mathrm{\\bf~e}\\,\\,x^{(1)}\\;=\\;r_{S_{0}}^{(0)},\\quad r^{(1)}\\;=\\;\\frac{1-\\alpha}{1+\\alpha}W\\Delta_{S_{0}}^{(0)},\\quad\\tilde{x}^{(1)}\\;=\\;r_{S_{0}}^{(0)},\\quad{\\bf\\Delta}\\Delta^{(1)}\\;=\\;(1+\\alpha)r_{S_{0}}^{(0)}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "\u2022 When $t\\,=\\,1$ , we ha $\\delta_{1:2})\\pmb{r}^{(1)}+\\delta_{1:2}\\tilde{\\pmb{x}}^{(1)}$ . ", "page_idx": 36}, {"type": "text", "text": "\u2022 When $t\\geq1$ , we can recursively calculate the following vectors ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pmb{x}^{(t+1)}=\\pmb{x}^{(t)}+\\pmb{\\Delta}_{S_{t}}^{(t)}}\\\\ &{\\pmb{r}^{(t+1)}=\\pmb{r}^{(t)}-\\pmb{\\Delta}_{S_{t}}^{(t)}+\\frac{1-\\alpha}{1+\\alpha}\\pmb{W}\\pmb{\\Delta}_{S_{t}}^{(t)}}\\\\ &{\\tilde{\\pmb{x}}^{(t+1)}=\\tilde{\\pmb{x}}^{(t)}+\\pmb{\\Delta}_{S_{t}}^{(t)}-\\pmb{\\Delta}_{S_{t-1}}^{(t-1)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Therefore, at per-iteration, we only need to save sub-vectors $\\Delta_{\\mathcal{S}_{t}}$ and $\\Delta_{S_{t-1}}$ and update $\\textbf{\\em x}$ locally. ", "page_idx": 36}, {"type": "text", "text": "D Local Heavy-Ball Method - LOCHB ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "D.1 Standard HB and Proof Theorem D.2 ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Lemma D.1 (The standard HB updates). The updates $\\pmb{x}^{(t)}$ and $\\mathbf{\\boldsymbol{r}}^{(t)}$ of the HB method for solving Equ. (4) can be written as ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pmb{x}^{(t+1)}=\\pmb{x}^{(t)}+(1+\\tilde{\\alpha}^{2})\\pmb{r}^{(t)}+\\tilde{\\alpha}^{2}\\big(\\pmb{x}^{(t)}-\\pmb{x}^{(t-1)}\\big)}\\\\ &{\\pmb{r}^{(t+1)}=2\\tilde{\\alpha}\\pmb{W}\\pmb{r}^{(t)}-\\tilde{\\alpha}^{2}\\pmb{r}^{(t-1)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "The residual updates can be rewritten as a second-order homogeneous equation ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\pmb{y}^{(t+1)}-2\\pmb{\\Lambda}\\pmb{y}^{(t)}+\\pmb{y}^{(t-1)}=\\pmb{0},\\quad\\forall t=1,2,3,\\ldots.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where $\\pmb{y}^{(t)}$ is such that $\\pmb{r}^{(t)}=\\tilde{\\alpha}^{t}\\pmb{V}\\pmb{y}^{(t)},t\\geq0$ with $\\pmb{y}^{(0)}=\\pmb{V}^{\\top}\\pmb{r}^{(0)}=\\pmb{V}^{\\top}\\pmb{b}.$ ", "page_idx": 36}, {"type": "text", "text": "Proof. We follow the standard Polyak\u2019s heavy-ball method [40] as ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\pmb{x}^{(t+1)}=\\pmb{x}^{(t)}-\\eta_{\\alpha}\\nabla f(\\pmb{x}^{(t)})+\\eta_{\\beta}(\\pmb{x}^{(t)}-\\pmb{x}^{(t-1)}),\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where $^{\\!(t)})=Q x^{(t)}-b\\operatorname{and}\\eta_{\\alpha}=4/(\\sqrt{2/(1+\\alpha)}+\\sqrt{2\\alpha/(1+\\alpha)})^{2}=2(1+\\alpha)/(1+\\sqrt{\\alpha})$ )2 = $1+\\tilde{\\alpha}^{2}$ and $\\eta_{\\beta}=(\\sqrt{2/(1+\\alpha)}-\\sqrt{2\\alpha/(1+\\alpha)})^{2}/(\\sqrt{2/(1+\\alpha)}+\\sqrt{2\\alpha/(1+\\alpha)})^{2}=\\tilde{\\alpha}^{2}$ . Hence, it leads to the following updates ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\pmb{x}^{(t+1)}=\\pmb{x}^{(t)}+(1+\\tilde{\\alpha}^{2})\\pmb{r}^{(t)}+\\tilde{\\alpha}^{2}\\big(\\pmb{x}^{(t)}-\\pmb{x}^{(t-1)}\\big).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Inserting ", "page_idx": 37}, {"type": "equation", "text": "$$\nr^{(t)}=Q(x^{*}-x^{(t)})=b-\\left(I-\\frac{1-\\alpha}{1+\\alpha}W\\right)x^{(t)}=b-\\left(I-\\frac{2\\tilde{\\alpha}}{1+\\tilde{\\alpha}^{2}}W\\right)x^{(t)}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "then $\\pmb{x}^{(t+1)}=2\\tilde{\\alpha}\\pmb{W}\\pmb{x}^{(t)}-\\tilde{\\alpha}^{2}\\pmb{x}^{(t-1)}+(1+\\tilde{\\alpha}^{2})\\pmb{b}$ and since ", "page_idx": 37}, {"type": "equation", "text": "$$\nQ W=\\left(I-\\frac{1-\\alpha}{1+\\alpha}W\\right)W=W\\left(I-\\frac{1-\\alpha}{1+\\alpha}W\\right)=W Q\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "So ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{r^{(t+1)}}&{=}&{-Q({\\pmb x}^{(t+1)}-{\\pmb x}^{*})}\\\\ &{=}&{-2\\tilde{\\alpha}Q W{\\pmb x}^{(t)}+\\tilde{\\alpha}^{2}Q{\\pmb x}^{(t-1)}+\\left({\\pmb I}-(1+\\tilde{\\alpha}^{2})Q\\right){\\pmb b}}\\\\ &{=}&{-2\\tilde{\\alpha}Q W{\\pmb x}^{(t)}+\\tilde{\\alpha}^{2}Q{\\pmb x}^{(t-1)}+\\left(-\\tilde{\\alpha}^{2}+2\\tilde{\\alpha}{\\pmb W}\\right){\\pmb b}}\\\\ &{=}&{2\\tilde{\\alpha}W r^{(t)}-\\tilde{\\alpha}^{2}r^{(t-1)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Using $\\pmb{r}^{(t)}=\\tilde{\\alpha}^{t}\\pmb{V}\\pmb{y}^{(t)},t\\geq0$ ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\tilde{\\alpha}^{t+1}V y^{(t+1)}=2\\tilde{\\alpha}W\\tilde{\\alpha}^{t}V y^{(t)}-\\tilde{\\alpha}^{2}\\tilde{\\alpha}^{t-1}V y^{(t-1)}\\quad\\Rightarrow\\quad V y^{(t+1)}=2W V y^{(t)}-V y^{(t-1)}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "As $W=V\\Lambda V^{\\top}$ and $V^{\\top}=V^{-1}$ is orthogonal matrix, we continue to have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r}{V^{\\top}V y^{(t+1)}-2V^{\\top}V\\Lambda V^{\\top}V y^{(t)}+V^{\\top}V y^{(t-1)}=\\mathbf{0}\\quad\\Rightarrow\\quad y^{(t+1)}-2\\Lambda y^{(t)}+y^{(t-1)}=\\mathbf{0}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Theorem D.2 (Convergence analysis of Heavy-Ball (HB)). To solve the minimization problem in Equ. (4), we propose the following standard HB updates as ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\pmb{x}^{(t+1)}=\\pmb{x}^{(t)}+(1+\\tilde{\\alpha}^{2})\\pmb{r}^{(t)}+\\tilde{\\alpha}^{2}\\big(\\pmb{x}^{(t)}-\\pmb{x}^{(t-1)}\\big),\\qquad\\pmb{r}^{(t+1)}=2\\tilde{\\alpha}\\pmb{W}\\pmb{r}^{(t)}-\\tilde{\\alpha}^{2}\\pmb{r}^{(t-1)},\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where the initial condition is ${\\pmb x}^{(0)}\\,=\\,{\\bf0},r^{(0)}\\,=\\,{\\pmb b}$ , $\\pmb{x}^{(1)}\\;=\\;\\pmb{x}^{(0)}\\,+\\,\\Gamma\\pmb{r}^{(0)}$ , $\\pmb{r}^{(1)}\\ =\\ \\pmb{b}\\ -\\ \\pmb{Q}\\pmb{x}^{(1)}$ . Then there exists a constant $\\tau$ such that the total iteration complexity to reach the stop condition $\\{u:|r_{u}|\\leq\\epsilon d_{u},u\\in\\mathcal{V}\\}=\\emptyset$ is ", "page_idx": 37}, {"type": "equation", "text": "$$\nt=\\left\\lceil\\frac{1+\\sqrt{\\alpha}}{2\\sqrt{\\alpha}}\\ln\\frac{C_{t}\\|\\pmb{r}^{(0)}\\|_{2}}{\\epsilon}\\right\\rceil,\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where $C_{t}=1\\;i f\\Gamma=Q^{-1}(I-\\tilde{\\alpha}W)$ (ideal case); $\\begin{array}{r}{C_{t}=\\operatorname*{max}\\left\\{\\frac{1+\\tilde{\\alpha}^{-1}}{\\sqrt{1-\\lambda_{2}^{2}}},1+(1+\\tilde{\\alpha}^{-1})t\\right\\}i f\\Gamma=\\mathbf{0}}\\end{array}$   \n(practical case); and Ct =\u221a12\u2212\u03bb22 if \u0393 = (1\u2212\u03b1\u02dc)2(1+\u03b1)I (G is not bi-partite graph). ", "page_idx": 37}, {"type": "text", "text": "Proof. Recall $W=V\\Lambda V^{\\top}$ and then $\\begin{array}{r}{V^{\\top}{r}^{(t+1)}=2\\tilde{\\alpha}{\\mathbf{A}}V^{\\top}{r}^{(t)}-\\tilde{\\alpha}^{2}V^{\\top}{r}^{(t-1)}}\\end{array}$ . By Lemma D.1, we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\pmb{y}^{(t+1)}-2\\pmb{\\Lambda}\\pmb{y}^{(t)}+\\pmb{y}^{(t-1)}=0,\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where we obtained $n$ second-order difference equations ", "page_idx": 37}, {"type": "equation", "text": "$$\ny_{i}^{(t+1)}-2\\lambda_{i}y_{i}^{(t)}+y_{i}^{(t-1)}=0,\\quad\\forall i=1,2,\\ldots,n.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Follow the Lemma C.3, Equ. (23) has the solution ", "page_idx": 37}, {"type": "equation", "text": "$$\ny_{i}^{(t)}=\\left\\{\\frac{\\sin(\\theta_{i}t)y_{i}^{(1)}-\\sin(\\theta_{i}(t-1))y_{i}^{(0)}}{\\sin(\\theta_{i})}\\quad\\;|\\lambda_{i}|<1\\;\\mathrm{where}\\;\\theta_{i}=\\operatorname{arccos}(\\lambda_{i})\\;,\\right.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where in the case of $|\\lambda_{i}|<1$ . We consider the three cases of $\\mathbf{T}$ ", "page_idx": 37}, {"type": "text", "text": "\u2022 Ideal Case: We can eliminate $t$ in (34), when $\\pmb{y}^{(1)}=\\pmb{\\Lambda}\\pmb{y}^{(0)}$ , we get $y_{i}^{(1)}=\\lambda_{i}y_{i}^{(0)}$ , and then $y_{i}^{(t)}$ can be simplified into ", "page_idx": 38}, {"type": "equation", "text": "$$\ny_{i}^{(t)}=\\left\\{\\begin{array}{l l}{\\frac{(\\lambda_{i}\\sin(\\theta_{i}t)-\\sin(\\theta_{i}(t-1)))y_{i}^{(0)}}{\\sin\\theta_{i}}}&{|\\lambda_{i}|<1}\\\\ {y_{i}^{(0)}\\lambda_{i}^{t}}&{|\\lambda_{i}|=1}\\end{array}\\right.\\left\\}\\lambda_{i}|<1\\right.=\\left\\{y_{i}^{(0)}\\cos(\\theta_{i}t)\\quad|\\lambda_{i}|<1\\leq\\left\\{|y_{i}^{(0)}|\\quad|\\lambda_{i}|<1\\right.}\\\\ {\\left.|y_{i}^{(0)}\\lambda_{i}^{t}\\right.=1}.|\\lambda_{i}|=1\\,\\right\\}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "In this case, $\\mathbf{T}$ needs to be $\\mathbf{\\boldsymbol{\\Gamma}}=\\boldsymbol{Q}^{-1}(\\boldsymbol{I}-\\tilde{\\alpha}\\boldsymbol{W})$ . Therefore, we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\|V^{\\top}r^{(t)}\\|_{2}=\\|r^{(t)}\\|_{2}=\\tilde{\\alpha}^{t}\\|y^{(t)}\\|_{2}\\leq\\tilde{\\alpha}^{t}\\|y^{(0)}\\|_{2}=\\tilde{\\alpha}^{t}\\|V^{\\top}r^{(0)}\\|_{2}=\\tilde{\\alpha}^{t}\\|r^{(0)}\\|_{2}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "\u2022 Practical Case: Just letting $\\pmb{x}^{(1)}=\\pmb{x}^{(0)}=\\pmb{0}$ , we have $\\tilde{\\alpha}\\pmb{y}^{(1)}=\\pmb{y}^{(0)}$ , then ", "page_idx": 38}, {"type": "equation", "text": "$$\ny_{i}^{(t)}=\\left\\{\\begin{array}{l l}{\\frac{\\tilde{\\alpha}^{-1}\\sin(\\theta_{i}t)-\\sin(\\theta_{i}(t-1))}{\\sin(\\theta_{i})}y_{i}^{(0)}}&{|\\lambda_{i}|<1}\\\\ {(1+(\\tilde{\\alpha}^{-1}-\\lambda_{i})t)y_{i}^{(0)}\\lambda_{i}^{t}}&{|\\lambda_{i}|=1}\\end{array}\\right\\}\\leq\\operatorname*{max}\\left\\{\\frac{1+\\tilde{\\alpha}^{-1}}{\\sqrt{1-\\lambda_{2}^{2}}},1+(1+\\tilde{\\alpha}^{-1})t\\right\\}|y_{i}^{(0)}|,\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where $\\theta_{i}=\\operatorname{arccos}(\\lambda_{i})$ . ", "page_idx": 38}, {"type": "text", "text": "\u2022 Non-bipartite graph Case: When the graph is non-bipartite, we can eliminate $t$ , as the following: We choose $\\mathbf{T}=\\tau\\pmb{I}$ , we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\boldsymbol{x}^{(1)}=\\boldsymbol{x}^{(0)}+\\mathbf{r}\\boldsymbol{r}^{(0)}=\\Gamma\\boldsymbol{r}^{(0)},\\qquad\\boldsymbol{r}^{(1)}=b-Q\\boldsymbol{x}^{(1)}=\\boldsymbol{r}^{(0)}-\\tau Q\\boldsymbol{r}^{(0)}}\\\\ {\\boldsymbol{V}^{\\top}\\boldsymbol{r}^{(1)}=(1-\\tau)\\boldsymbol{V}^{\\top}\\boldsymbol{r}^{(0)}+\\frac{\\left(1-\\alpha\\right)\\tau}{1+\\alpha}\\Lambda V^{\\top}\\boldsymbol{r}^{(0)},\\qquad\\boldsymbol{v}_{i}^{\\top}\\boldsymbol{r}^{(1)}=(1-\\tau+\\frac{\\left(1-\\alpha\\right)\\tau}{1+\\alpha}\\lambda_{i})\\boldsymbol{v}_{i}^{\\top}\\boldsymbol{r}^{(0)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "We have the following relations ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\boldsymbol{v}_{i}^{\\top}\\boldsymbol{r}^{(0)}=\\boldsymbol{y}_{i}^{(0)}}\\\\ &{\\boldsymbol{v}_{i}^{\\top}\\boldsymbol{r}^{(1)}=\\tilde{\\alpha}\\boldsymbol{y}_{i}^{(1)}=(1-\\tau+\\frac{(1-\\alpha)\\tau}{1+\\alpha}\\lambda_{i})\\boldsymbol{v}_{i}^{\\top}\\boldsymbol{r}^{(0)}=(1-\\tau+\\frac{(1-\\alpha)\\tau\\lambda_{i}}{1+\\alpha})\\boldsymbol{y}_{i}^{(0)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "To make $t$ disappear when $\\lambda_{i}=1$ , we need $y_{i}^{(0)}=y_{i}^{(1)}$ , or ", "page_idx": 38}, {"type": "equation", "text": "$$\n1-\\tau+\\frac{(1-\\alpha)\\tau}{1+\\alpha}=\\frac{1-\\sqrt{\\alpha}}{1+\\sqrt{\\alpha}}\\iff\\tau=\\frac{1+\\alpha}{\\alpha+\\sqrt{\\alpha}}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "In this case, we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n|y_{i}^{(t)}|\\leq\\frac{2}{\\sqrt{1-\\lambda_{2}^{2}}}|y_{i}^{(0)}|\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "To make sure the algorithm stops when the stop condition is met, it is enough for ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\|r^{(t)}\\|_{2}=\\tilde{\\alpha}^{t}\\|y^{(t)}\\|_{2}\\leq\\tilde{\\alpha}^{t}C_{t}\\|y^{(0)}\\|_{2}=\\tilde{\\alpha}^{t}C_{t}\\|V^{\\top}r^{(0)}\\|_{2}=\\tilde{\\alpha}^{t}C_{t}\\|r^{(0)}\\|_{2}\\leq\\epsilon.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "This means $C_{t}\\|\\pmb{r}^{(0)}\\|_{2}\\tilde{\\alpha}^{t}\\leq\\epsilon$ , which leads to the following ", "page_idx": 38}, {"type": "equation", "text": "$$\nt=\\left\\lceil\\frac{1+\\sqrt{\\alpha}}{2\\sqrt{\\alpha}}\\ln\\frac{C_{t}\\|\\pmb{r}^{(0)}\\|_{2}}{\\epsilon}\\right\\rceil.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Remark D.3. The constant that appears in the bound involves the second largest eigenvalue $\\lambda_{2}$ of $A D^{-1}$ . It is deeply related to the mixing time of random walk [8] where the second largest eigenvalue determines the mixing time of the walk. A smaller absolute value of the second largest eigenvalue indicates that a random walk on the graph will mix (i.e., approach its steady-state distribution) more quickly. Our proof is partially inspired by d\u2019Aspremont et al. [11] where we directly bound $\\mathbf{\\boldsymbol{r}}^{(t)}$ instead of providing bound for $e^{(t)}$ . ", "page_idx": 38}, {"type": "text", "text": "D.2 Residual Updates of LOCHB and Proof of Theorem D.6 ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Lemma D.4. Let the local heavy-ball method be defined as ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{\\pmb x}^{(t+1)}={\\pmb x}^{(t)}+{\\pmb\\Delta}_{{\\pmb\\delta}_{t}}^{(t)},\\quad{\\pmb r}^{(t+1)}={\\pmb r}^{(t)}-Q{\\pmb\\Delta}_{{\\pmb\\delta}_{t}}^{(t)},\\quad{\\pmb\\Delta}^{(t)}=(1+\\tilde{\\alpha}^{2}){\\pmb r}^{(t)}+\\tilde{\\alpha}^{2}\\big({\\pmb x}^{(t)}-{\\pmb x}^{(t-1)}\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where ${\\pmb x}^{(0)}={\\bf0},{\\pmb x}^{(1)}={\\bf\\Gamma}{\\bf r}^{(0)}$ and $\\mathbf{I}=\\mathbf{diag}(\\Gamma_{1},\\Gamma_{2},\\ldots,\\Gamma_{n})$ is initial step size matrix. We have the following expanding sequence ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\tilde{\\alpha}^{2}({\\pmb x}^{(t)}-{\\pmb x}^{(t-1)})_{\\overline{S}_{t}}=(1+\\tilde{\\alpha}^{2})\\displaystyle\\sum_{i=1}^{t-1}\\tilde{\\alpha}^{2(t-i)}r_{{\\pmb{S}}_{i:t-1}\\cap\\overline{S}_{t}}^{(i)}+\\tilde{\\alpha}^{2t}\\Gamma r_{{\\pmb S}_{0:t-1}\\cap\\overline{S}_{t}}^{(0)},}&{\\quad\\forall t\\geq1}\\\\ {\\Delta_{\\overline{S}_{t}}^{(t)}=(1+\\tilde{\\alpha}^{2})\\displaystyle\\sum_{i=1}^{t}\\tilde{\\alpha}^{2(t-i)}r_{{\\pmb S}_{i:t-1}\\cap\\overline{S}_{t}}^{(i)}+\\tilde{\\alpha}^{2t}\\Gamma r_{{\\pmb S}_{0:t-1}\\cap\\overline{S}_{t}}^{(0)},}&{\\quad\\forall t\\geq1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Furthermore, we have the following sequence ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\tilde{\\alpha}^{2}({\\boldsymbol V}^{\\top}-\\frac{1-\\alpha}{1+\\alpha}{\\Delta}{\\boldsymbol V}^{\\top})\\big({\\boldsymbol x}^{(t)}-{\\boldsymbol x}^{(t-1)}\\big)\\_{\\overline{{\\mathcal{S}}}_{t}}=}}\\\\ {{\\displaystyle\\quad(1+\\tilde{\\alpha}^{2})\\sum_{i=1}^{t-1}\\tilde{\\alpha}^{2(t-i)}({\\boldsymbol V}^{\\top}-\\frac{1-\\alpha}{1+\\alpha}{\\Delta}{\\boldsymbol V}^{\\top})r_{\\overline{{\\mathcal{S}}}_{i,t}}^{(i)}+\\mathbf{P}\\tilde{\\alpha}^{2t}({\\boldsymbol V}^{\\top}-\\frac{1-\\alpha}{1+\\alpha}{\\Delta}{\\boldsymbol V}^{\\top})r_{\\overline{{\\mathcal{S}}}_{0,t}}^{(0)}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where we denote $\\overline{{S}}_{i,t}\\triangleq S_{i:t-1}\\cap\\overline{{S}}_{t}$ . ", "page_idx": 39}, {"type": "text", "text": "Proof. We assume all nonzeros in $^{b}$ are active nodes at time $t=0$ and $t=1$ , i.e., ${\\pmb S}_{0}={\\pmb r}^{(0)}=$ $\\operatorname{supp}(b)$ . The local updates can be expressed as ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{x^{(t+1)}=x^{(t)}+(1+\\bar{\\alpha}^{2})^{r}\\delta_{s}^{(t)}+\\bar{\\alpha}^{2}\\big(x^{(t)}-x^{(t-1)}\\big)_{s},}\\\\ &{r^{(t+1)}=b-Q x^{(t+1)}}\\\\ &{\\qquad\\qquad=\\underbrace{r^{(t)}-(1+\\bar{\\alpha}^{2})Q r^{(t)}-\\bar{\\alpha}^{2}Q\\big(x^{(t)}-x^{(t-1)}\\big)}_{\\mathrm{oupingleas}}}\\\\ &{\\qquad\\qquad\\qquad+\\underbrace{(1+\\bar{\\alpha}^{2})Q r_{S_{t}^{(t)}}^{(t)}+\\bar{\\alpha}^{2}Q\\big(x^{(t)}-x^{(t-1)}\\big)_{\\bar{\\alpha}}}_{\\mathrm{mix~wheandmopingababes}}}\\\\ &{\\qquad\\qquad=2\\bar{\\alpha}W r^{(t)}-\\bar{\\alpha}^{2}r^{(t-1)}+\\underbrace{(1+\\bar{\\alpha}^{2})Q r_{S_{t}^{(t)}}^{(t)}}_{\\mathrm{mix~wheandmoping~in}}+\\bar{\\alpha}^{2}Q\\big(x^{(t)}-x^{(t-1)}\\big)_{\\bar{\\alpha}},}\\\\ &{\\qquad\\qquad\\qquad=2\\bar{\\alpha}W r^{(t)}-\\bar{\\alpha}^{2}r^{(t-1)}+(1+\\bar{\\alpha}^{2})\\big(I-\\frac{1-\\alpha}{1+\\alpha}W\\big)r_{S_{t}^{(t)}}^{(t)}+\\bar{\\alpha}^{2}Q\\big(x^{(t)}-x^{(t-1)}\\big)_{\\bar{\\alpha}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "For $t\\geq1$ , we can expand $\\pmb{x}^{(t+1)}-\\pmb{x}^{(t)}$ as the following ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Delta^{(t+1)}=x^{(t)}+(1+\\dot{\\mathcal{A}}^{2})^{p}\\frac{x}{r_{k}}+\\dot{\\mathcal{A}}^{2}\\left(x^{(t)}-x^{(t-1)}\\right)_{\\mathcal{S}_{x}}}&{}\\\\ {\\Delta^{(t)}=x^{(t+1)}-x^{(t)}+(1+\\dot{\\mathcal{A}}^{2})^{p}\\frac{x}{r_{k}}+\\dot{\\mathcal{A}}^{2}\\left((1+\\dot{\\mathcal{A}}^{2})^{p}\\frac{x-t}{r_{k}-1}+\\dot{\\mathcal{A}}^{2}\\left(x^{(t-1)}-x^{(t-2)}\\right)_{\\mathcal{S}_{x}-1}\\right)_{\\mathcal{S}_{x}}}\\\\ {=}&{(1+\\dot{\\mathcal{A}}^{2})^{p}\\frac{x_{0}}{r_{k}}+\\dot{\\mathcal{A}}^{2}(1+\\dot{\\mathcal{A}}^{2})^{p}\\frac{x-t}{r_{k}-1}+\\dot{\\mathcal{A}}^{2}\\left(x^{(t-1)}-x^{(t-2)}\\right)_{\\mathcal{S}_{x}-1}}\\\\ {=}&{(1+\\dot{\\mathcal{A}}^{2})^{p}\\frac{x_{0}}{r_{k}}+\\dot{\\mathcal{A}}^{2}\\left(1+\\dot{\\mathcal{A}}^{2}\\right)\\dot{\\mathcal{S}}_{x}-1+\\dot{\\mathcal{A}}^{2}\\left((1+\\dot{\\mathcal{A}}^{2})^{p}\\frac{x-t}{r_{k}-2}\\right)}\\\\ &{+\\dot{\\mathcal{A}}^{2}\\left(x^{(t-2)}-x^{(t-3)}\\right)_{\\mathcal{S}_{x}-1}}\\\\ {=}&{(1+\\dot{\\mathcal{A}}^{2})^{p}\\frac{x_{0}}{r_{k}}+\\dot{\\mathcal{A}}^{2}\\left(1+\\dot{\\mathcal{A}}^{2}\\right)\\dot{\\mathcal{S}}_{x}-1+\\dot{\\mathcal{A}}^{2}\\left(1+\\dot{\\mathcal{A}}^{2}\\right)\\dot{\\mathcal{S}}_{x}^{(t-2)}}\\\\ &{+\\dot{\\mathcal{A}}^{2}\\left(x^{(t-2)}-x^{(t-3)}\\right)_{\\mathcal{S}_{x}-1}}\\\\ {=}&{(1+\\dot{\\\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Note S0 = supp(r(0)), then r(S01):t $r_{S_{1:t}}^{(0)}=r_{S_{0:t}}^{(0)}$ , we continue to have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{}&{\\displaystyle\\pmb{x}^{(t)}-\\pmb{x}^{(t-1)}=(1+\\tilde{\\alpha}^{2})\\sum_{i=1}^{t-1}\\tilde{\\alpha}^{2(t-i-1)}\\pmb{r}_{S_{i:t-1}}^{(i)}+\\mathbf{F}\\tilde{\\alpha}^{2(t-1)}\\pmb{r}_{S_{0:t-1}}^{(0)},\\qquad\\forall t\\ge1}\\\\ &{}&{\\tilde{\\alpha}^{2}(\\pmb{x}^{(t)}-\\pmb{x}^{(t-1)})\\overline{{\\mathscr{S}}}_{t}=(1+\\tilde{\\alpha}^{2})\\sum_{i=1}^{t-1}\\tilde{\\alpha}^{2(t-i)}\\pmb{r}_{S_{i:t-1}\\cap\\bar{\\mathscr{S}}_{t}}^{(i)}+\\mathbf{F}\\tilde{\\alpha}^{2t}\\pmb{r}_{S_{0:t-1}\\cap\\bar{\\mathscr{S}}_{t}}^{(0)},\\qquad\\forall t\\ge1}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "The rest follows readily. ", "page_idx": 40}, {"type": "text", "text": "Lemma D.5 (The nonhomogeneous difference equation). Given $\\pmb{y}^{(1)}=\\pmb{\\Lambda}\\pmb{y}^{(0)}$ , equations ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\pmb{y}^{(t+1)}-2\\pmb{\\Lambda}\\pmb{y}^{(t)}+\\pmb{y}^{(t-1)}:=\\pmb{f}^{(t)}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "have the following solutions ", "page_idx": 40}, {"type": "equation", "text": "$$\n{\\pmb y}^{(t)}=Z_{t}{\\pmb y}^{(0)}+\\sum_{k=1}^{t-1}H_{k,t}{\\pmb f}^{(k)},\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where ", "page_idx": 40}, {"type": "equation", "text": "$$\nZ_{t}=\\left\\{\\begin{array}{l l}{\\mathbf{diag}\\left(1,\\ldots,\\cos(\\theta_{i}t),\\ldots,(-1)^{t}\\right)}&{\\;\\;f o r\\;b i p a r t i t e\\;g r a p h s}\\\\ {\\;\\;}\\\\ {\\mathbf{diag}\\left(1,\\ldots,\\cos(\\theta_{i}t),\\ldots,\\cos(\\theta_{n}t)\\right)}&{\\;\\;\\;f o r\\;n o n{\\cdot}b i p a r t i t e\\;g r a p h s,}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "equation", "text": "$$\nH_{k,t}=\\left\\{\\begin{array}{l l}{\\mathbf{diag}\\left(t-k,\\,\\dots,\\frac{\\sin(\\theta_{i}(t-k))}{\\sin\\theta_{i}},\\dots,(-1)^{t-k-1}(t-k)\\right)\\quad\\,f o r\\,b i p a r t i t e\\,g r a p h s}\\\\ {\\mathbf{diag}\\left(t-k,\\,\\dots,\\frac{\\sin(\\theta_{i}(t-k))}{\\sin\\theta_{i}},\\dots,\\frac{\\sin(\\theta_{n}(t-k))}{\\sin\\theta_{n}}\\right)\\quad\\,f o r\\,n o n{-}b i p a r t i t e\\,g r a p h s.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Theorem D.6 (Representation of $\\mathbf{\\boldsymbol{r}}^{(t)}$ for LOCHB). Given $t\\geq1$ , $\\mathbf{\\pmb{x}}^{(0)}=\\mathbf{0}$ and $\\mathbf{\\Delta}x^{(1)}=\\mathbf{\\Gamma}\\Gamma r_{S_{0}}^{(0)}$ . The residual of $\\mathbf{\\boldsymbol{r}}^{(t)}$ of LOCHB satisfies ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V^{\\top}r^{(t)}=\\tilde{\\alpha}^{t}Z_{t}V^{\\top}r^{(0)}+\\tilde{\\alpha}^{t}t\\displaystyle\\sum_{k=1}^{t-1}\\tilde{\\alpha}^{k-1}H_{k,t}V^{\\top}Q\\Gamma r_{\\mathcal{S}_{0,k}}^{(0)}\\bigg/t}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\displaystyle\\sum_{k=1}^{t-1}\\tilde{\\alpha}^{t-k}(t-k)\\displaystyle\\sum_{j=k}^{t-1}\\tilde{\\alpha}^{j-k}H_{j,t}\\left(\\frac{1+\\alpha}{1-\\alpha}-\\Lambda\\right)V^{\\top}r_{\\mathcal{S}_{k,j}}^{(k)}\\bigg/(t-k)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Proof. Follow Lemma D.4, we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{{r^{(t+1)}-Q\\Delta_{\\bar{S}_{t}}^{(t)}}}&{{=}}&{{r^{(t)}-Q\\Delta^{(t)}}}\\\\ {{}}&{{=}}&{{r^{(t)}-(1+\\tilde{\\alpha}^{2})Q r^{(t)}-\\tilde{\\alpha}^{2}Q(x^{(t)}-x^{(t-1)})}}\\\\ {{}}&{{=}}&{{-\\tilde{\\alpha}^{2}r^{(t)}+2\\tilde{\\alpha}W r^{(t)}-\\tilde{\\alpha}^{2}Q(x^{(t)}-x^{(t-1)})}}\\\\ {{}}&{{=}}&{{2\\tilde{\\alpha}W r^{(t)}-\\tilde{\\alpha}^{2}r^{(t-1)}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "So we can write the updates of LOCHB as ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{r^{(t+1)}-2\\tilde{\\alpha}\\pmb{W}r^{(t)}+\\tilde{\\alpha}^{2}r^{(t-1)}=Q\\Delta_{\\pmb{\\mathscr{S}}_{t}}^{(t)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=(1+\\tilde{\\alpha}^{2})\\displaystyle\\sum_{i=1}^{t}\\tilde{\\alpha}^{2(t-i)}Q r_{\\pmb{\\mathscr{S}}_{i,t}}^{(i)}+\\tilde{\\alpha}^{2t}Q\\Gamma r_{\\pmb{\\mathscr{S}}_{0,t}}^{(0)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Write $V^{\\top}\\boldsymbol{r}^{(t)}=\\tilde{\\alpha}^{t}\\pmb{y}^{(t)}$ , and note ", "page_idx": 41}, {"type": "equation", "text": "$$\nr^{(t)}=\\tilde{\\alpha}^{t}V y^{(t)},\\quad r_{\\overline{{S}}_{t}}^{(t)}=\\tilde{\\alpha}^{t}\\left(V y^{(t)}\\right)_{\\overline{{S}}_{t}}\\quad\\Rightarrow\\quad V^{\\top}r_{\\overline{{S}}_{t}}^{(t)}=\\tilde{\\alpha}^{t}V^{\\top}\\left(V y^{(t)}\\right)_{\\overline{{S}}_{t}}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Hence, ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle V^{\\top}r^{(t+1)}-2\\bar{\\alpha}V^{\\top}W r^{(t)}+\\bar{\\alpha}^{2}V^{\\top}r^{(t-1)}}}\\\\ {{\\displaystyle}}\\\\ {{\\displaystyle}}\\\\ {{\\Longleftrightarrow\\bar{\\alpha}^{t+1}y^{(t+1)}-2\\bar{\\alpha}\\bar{\\alpha}^{t}W y^{(t)}+\\bar{\\alpha}^{2}\\bar{\\alpha}^{t-1}y^{(t)}{\\cal F}^{(t)}{\\cal S}_{\\bar{\\alpha},t}+\\bar{\\alpha}^{2t}V^{\\top}Q{\\bf T}{\\cal P}_{{\\bar{\\alpha}},t}^{(0)}},}\\\\ {{\\displaystyle}}\\\\ {{\\displaystyle}}\\\\ {{\\Longleftrightarrow\\bar{\\alpha}^{t+1}y^{(t+1)}-2\\bar{\\alpha}\\bar{\\alpha}^{t}W y^{(t)}+\\bar{\\alpha}^{2}\\bar{\\alpha}^{t-1}y^{(t-1)}}}\\\\ {{\\displaystyle}}\\\\ {{\\displaystyle}}\\\\ {{\\Longleftrightarrow y^{(t+1)}-2{\\cal W}y^{(t)}+y^{(t-1)}}}\\\\ {{\\displaystyle}}\\\\ {{\\displaystyle}}\\\\ {{\\displaystyle}}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Then, from Lemma D.5 ", "page_idx": 41}, {"type": "equation", "text": "$$\ny^{(t)}=Z_{t}y^{(0)}+\\sum_{k=1}^{t-1}H_{k,t}f^{(k)}\\iff V^{\\top}r^{(t)}=\\tilde{\\alpha}^{t}Z_{t}V^{\\top}r^{(0)}+\\tilde{\\alpha}^{t}\\sum_{k=1}^{t-1}H_{k,t}f^{(k)}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "so expanding the error term ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\dot{\\alpha}^{i}H_{k,j}f^{(k)}=\\displaystyle\\sum_{i=1}^{k}\\dot{\\alpha}^{i}H_{k,j}f_{i}^{(k)}+\\dot{\\alpha}^{i}H_{k,j}f_{0}^{(k)}}&{}\\\\ {=(1+\\delta^{2})\\dot{\\alpha}^{i+k-1-\\frac{k}{\\alpha}}\\displaystyle E_{k},H_{k,j}Q V^{T}(V_{j}^{(k)})_{\\delta,k}}&{}\\\\ {+\\dot{\\alpha}^{i+k-1}H_{k,j}Q V^{T}(V_{j}^{(k)})_{\\delta,k}}&{}\\\\ {=(1+\\delta^{2})\\dot{\\alpha}^{i+k-1-\\frac{3}{\\alpha}}\\displaystyle\\sum_{i=1}^{k}\\dot{\\alpha}H_{k,j}Q V^{T}r_{S_{i,k}^{(k)}}^{(i)}+\\dot{\\alpha}^{i+k-1}H_{k,j}Q V^{T}r_{S_{i,k}^{(i)}}^{(0)}}&{}\\\\ {=2\\displaystyle\\sum_{i=1}^{k}\\dot{\\alpha}^{i+k-2}H_{k,i}\\displaystyle\\frac{1+\\alpha}{1-\\alpha}\\Big)Q V^{T}r_{S_{i,k}^{(i)}}^{(0)}+\\dot{\\alpha}^{i+k-1}H_{k,j}Q V^{T}r_{S_{i,k}^{(i)}}^{(0)}}&{}\\\\ {\\overset{(-1)}{\\underset{i=1}{\\sum}}\\tilde{\\alpha}^{i}H_{k,j}f^{(k)}=\\displaystyle\\sum_{i=1}^{k}\\dot{\\alpha}^{i+k-2}H_{k,i}\\displaystyle\\frac{1+\\alpha}{1-\\alpha}\\Big)Q V^{T}r_{S_{i,k}^{(i)}}^{(0)}+\\dot{\\alpha}^{i+k-1}H_{k,j}Q V^{T}r_{S_{i,k}^{(i)}}^{(0)}}&{}\\\\ {=\\displaystyle\\sum_{i=1}^{k-1}\\sum_{i=1}^{k}\\dot{\\alpha}^{i+k-2}H_{k,i}\\displaystyle\\frac{1+\\alpha}{1-\\alpha}\\Big)Q V^{T}r_{S_{i,k}^{(i)}}^{(0)}+\\frac{\\dot{\\alpha}^{i-k-1}}{\\sum}\\tilde{\\alpha}^{i+k-1}H_{k,j}Q V^{T}r_{S_{i,k}^{(i)}}^{( \n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "which when simplified, gives the relation proposed. ", "page_idx": 42}, {"type": "text", "text": "D.3 Convergence of LOCHB of Proof of Theorem D.8 ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Corollary D.7. Define ", "text_level": 1, "page_idx": 42}, {"type": "equation", "text": "$$\n\\beta_{k,t}\\triangleq\\Vert\\pmb{u}_{k,t}\\Vert_{2}/\\Vert r^{(k)}\\Vert_{2},\\qquad\\beta_{k}\\triangleq\\operatorname*{max}_{t}\\beta_{k,t}.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Then the upper bound of $\\|\\pmb{r}^{(t)}\\|_{2}$ can be characterized as ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\|r^{(t)}\\|_{2}\\le\\tilde{\\alpha}^{t}\\prod_{j=0}^{t-1}(1+\\beta_{j})y_{t},\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where $y_{t+1}-2y_{t}+y_{t-1}/((1+\\beta_{t-1})(1+\\beta_{t}))=0$ where $y_{0}=y_{1}=\\|\\pmb{r}^{(0)}\\|_{2}$ . ", "page_idx": 42}, {"type": "text", "text": "Proof. Since $\\|\\boldsymbol{u}_{k,t}\\|_{2}\\leq\\beta_{k}\\|\\boldsymbol{r}^{(k)}\\|_{2}$ , then given the final iterative updates (29) ", "page_idx": 42}, {"type": "equation", "text": "$$\nV^{\\top}{\\boldsymbol{r}}^{(t)}={\\tilde{\\alpha}}^{t}Z_{t}V^{\\top}{\\boldsymbol{r}}^{(0)}+{\\tilde{\\alpha}}^{t}{\\boldsymbol{t}}{\\boldsymbol{u}}_{0,t}+2\\sum_{k=1}^{t-1}{\\tilde{\\alpha}}^{t-k}(t-k){\\boldsymbol{u}}_{k,t}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "and since $\\|Z_{t}\\|_{2}\\leq1$ we can bound ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\|r^{(t)}\\|_{2}\\leq\\tilde{\\alpha}^{t}\\|r^{(0)}\\|_{2}+\\tilde{\\alpha}^{t}t\\beta_{0}\\|r^{(0)}\\|_{2}+2\\displaystyle\\sum_{k=1}^{t-1}\\tilde{\\alpha}^{t-k-1}(t-k)\\beta_{k}\\|r^{(k)}\\|_{2}}\\\\ &{\\iff\\|r^{(t)}\\|_{2}-2\\displaystyle\\sum_{k=1}^{t-1}\\tilde{\\alpha}^{t-k-1}(t-k)\\beta_{k}\\|r^{(k)}\\|_{2}\\leq\\tilde{\\alpha}^{t}(1+t\\beta_{0})\\|r^{(0)}\\|_{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where $t=0,1,\\ldots,T$ . The rest just follows a similar strategy shown in Corollary C.9. We have the following inequalities ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r}{I-V_{L}:=\\left(\\begin{array}{c c c c c}{1}&{0}&{0}&{\\cdots\\cdot}&{0}\\\\ {-v_{21}}&{1}&{0}&{\\cdots\\cdot}&{0}\\\\ {-v_{31}}&{-v_{32}}&{1}&{\\cdots\\cdot}&{0}\\\\ {\\vdots}&{\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}\\\\ {-v_{n1}}&{-v_{n2}}&{-v_{n3}}&{\\cdots\\cdot}&{1}\\end{array}\\right)\\left(\\begin{array}{c}{\\|r^{(1)}\\|_{2}}\\\\ {\\|r^{(2)}\\|_{2}}\\\\ {\\|r^{(3)}\\|_{2}}\\\\ {\\vdots}\\\\ {\\|r^{(T)}\\|_{2}}\\end{array}\\right)\\leq\\left(\\begin{array}{c}{\\tilde{\\alpha}^{1}(1+\\beta_{0})\\|r^{(0)}\\|_{2}}\\\\ {\\tilde{\\alpha}^{2}(1+2\\beta_{0})\\|r^{(0)}\\|_{2}}\\\\ {\\tilde{\\alpha}^{3}(1+3\\beta_{0})\\|r^{(0)}\\|_{2}}\\\\ {\\vdots}\\\\ {\\tilde{\\alpha}^{T}(1+T\\beta_{0})\\|r^{(0)}\\|_{2}}\\end{array}\\right):=c,}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where $(V_{L})_{t k}=2\\tilde{\\alpha}^{t-k}(t-k)\\beta_{k}$ . Denote each upper bound as $\\tau_{t}=\\|\\pmb{r}^{(t)}\\|_{2}$ , we will have ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\eta_{1}=\\phi_{1}\\frac{1}{w_{1}}t_{1}+\\alpha_{1}^{2}(1+t_{2})\\psi_{1}+2\\frac{\\zeta_{1}(1-t_{3})t_{2}+\\zeta_{2}(1-t_{3})t_{3}}{1+t_{1}^{2}},}\\\\ &{\\eta_{1}+\\alpha_{1}+\\frac{\\zeta_{1}(1-t_{3})}{1+t_{1}}+\\alpha_{1}^{2}(1+(t+1)\\psi_{1})\\psi_{2}+2\\frac{\\zeta_{1}(1-t_{3})\\zeta_{2}(1-t_{1})}{1+t_{1}^{2}},}\\\\ &{\\quad-\\alpha^{2}t^{1}(1+t_{1}+1)\\psi_{1}+2\\frac{\\zeta_{1}}{1+t_{1}}(t-1)\\psi_{1}^{(++1)}\\lambda_{21}+2\\frac{\\zeta_{2}(1-t_{3})}{1+t_{1}^{2}}+3\\zeta_{2}(1-t_{3})\\psi_{2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad}\\\\ &{\\dot{\\alpha}_{1}=\\dot{\\alpha}^{(1+1}(1+t_{1})\\psi_{1}+2\\frac{\\zeta_{1}(1-t_{3})t_{3}}{1+t_{1}^{2}}+3\\zeta_{1}\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad}\\\\ &{\\eta_{1+1}-\\dot{\\alpha}_{1}\\dot{\\alpha}^{(+1}\\beta_{1})\\psi_{1}+2\\frac{\\zeta_{1}(1-t_{3})t_{3}}{1+t_{1}^{2}},}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "\u03c4t+1\u22122\u03b1\u02dc\u03c4t + \u03b1\u02dc2\u03c4t\u22121 = 2\u03b1\u02dc\u03b2t\u03c4t ", "page_idx": 43}, {"type": "text", "text": "The above analysis finally leads to $\\tau_{t+1}-2(1+\\beta_{t})\\tilde{\\alpha}\\tau_{t}+\\tilde{\\alpha}^{2}\\tau_{t-1}=0$ . So for ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\tilde{\\alpha}^{t}\\prod_{j=0}^{t-1}(1+\\beta_{j})y_{t}=\\tau_{t}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "we have ", "page_idx": 43}, {"type": "equation", "text": "$$\ny_{t+1}-2y_{t}+{\\frac{y_{t}}{(1+\\beta_{t})(1+\\beta_{t-1})}}=0.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Following the same strategy in Corollary C.9, we obtain the upper bound. ", "page_idx": 43}, {"type": "text", "text": "Theorem D.8 (Convergence of LOCHB). Let the geometric mean of $\\beta_{t}$ be $\\begin{array}{r}{\\overline{{\\beta}}_{t}\\triangleq\\prod_{j=0}^{t-1}(1+\\beta_{j})^{1/t}}\\end{array}$ . Then the upper bound of $\\|\\pmb{r}^{(t)}\\|_{2}$ can be characterized as ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\|r^{(t)}\\|_{2}\\leq\\tilde{\\alpha}^{t}\\prod_{j=0}^{t-1}(1+\\beta_{j})y_{t},\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where $y_{t+1}-2y_{t}+y_{t-1}/((1+\\beta_{t-1})(1+\\beta_{t}))=0$ where $y_{0}=y_{1}=\\|\\pmb{r}^{(0)}\\|_{2}$ . Assume that there exists a constant $c\\in[0,2)$ such that $\\begin{array}{r}{\\beta_{t}\\leq1+\\frac{c\\sqrt{\\alpha}}{1-\\sqrt{\\alpha}}}\\end{array}$ . Then the total number of iterations can be bounded as ", "page_idx": 43}, {"type": "equation", "text": "$$\nT\\leq\\left\\lceil{\\frac{1+{\\sqrt{\\alpha}}}{(2-c){\\sqrt{\\alpha}}}}\\right\\rceil\\ln\\left({\\frac{y_{t}}{\\epsilon}}\\right).\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Then the total runtime $\\tau$ is bounded by ", "page_idx": 43}, {"type": "equation", "text": "$$\nT\\leq\\Theta\\left(\\frac{(1+\\sqrt{\\alpha})\\overline{{\\mathrm{vol}}}(S_{T})}{(2-c)\\sqrt{\\alpha}}\\ln\\frac{y_{t}}{\\epsilon}\\right)=\\widetilde{\\mathcal{O}}\\left(\\frac{\\overline{{\\mathrm{vol}}}(S_{T})}{(2-c)\\sqrt{\\alpha}}\\right),\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where $\\widetilde O$ hides $\\ln(y_{t}/\\epsilon)$ . ", "page_idx": 43}, {"type": "text", "text": "Proof. The first part follows from Corollary of D.7. The rest follows the same strategy of LOCCH as in Theorem 4.2. (See also Theorem 4.2.) ", "page_idx": 43}, {"type": "table", "img_path": "wT2KhEb97a/tmp/35efe10ceb3bfba569328090afd32d6fbd5ab4266fdb87ccf797a2cd1d8ff29e.jpg", "table_caption": ["Table 2: Examples of sparse linear systems "], "table_footnote": [], "page_idx": 44}, {"type": "text", "text": "D.4 Implementation of LOCHB ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "We present the implementation of LOCHB as follows: Recall the updates of LOCHB is ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\pmb{x}^{(t+1)}=\\pmb{x}^{(t)}+(1+\\tilde{\\alpha}^{2})\\pmb{r}_{S_{t}}^{(t)}+\\tilde{\\alpha}^{2}\\big(\\pmb{x}^{(t)}-\\pmb{x}^{(t-1)}\\big)_{S_{t}},\\quad\\pmb{r}^{(t+1)}=2\\tilde{\\alpha}\\pmb{W}\\pmb{r}^{(t)}-\\tilde{\\alpha}^{2}\\pmb{r}^{(t-1)}.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "The corresponding local updates are ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\Delta^{(t)}=(1+\\tilde{\\alpha}^{2})^{r^{(t)}}+\\tilde{\\alpha}^{2}\\tilde{\\pmb{x}}^{(t)}}\\\\ &{\\quad\\pmb{x}^{(t+1)}=\\pmb{x}^{(t)}+\\Delta_{S_{t}}^{(t)}}\\\\ &{\\quad r^{(t+1)}=r^{(t)}-\\Delta^{(t)}+\\frac{1-\\alpha}{1+\\alpha}\\pmb{W}\\Delta^{(t)}}\\\\ &{\\quad\\tilde{x}^{(t+1)}=\\tilde{\\pmb{x}}^{(t)}+\\Delta^{(t)}-\\Delta^{(t-1)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where if we choose $\\pmb{x}^{(0)}=\\pmb{x}^{(1)}=\\pmb{\\tilde{x}}^{(1)}=\\pmb{0}$ , $\\pmb{r}^{(0)}=\\pmb{r}^{(1)}=\\pmb{b}$ and $\\pmb{\\Delta}^{(0)}=\\mathbf{0}$ . ", "page_idx": 44}, {"type": "text", "text": "E Instances of Sparse Linear Systems ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "E.1 Table of Popular Graph-induced Linear Systems ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "This section presents most commonly used graph-induced linear system as the following ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\underbrace{\\mathbf{A}-\\sigma\\pmb{D}^{-1/2}\\mathbf{A}\\pmb{D}^{-1/2}}_{\\pmb{Q}}\\pmb{x}=\\pmb{b},\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where $Q$ is the generalized version of the perturbed normalized graph Laplacian matrix with perturbation parameter $\\sigma>0$ , and $^{b}$ is a sparse vector. A typical example of $\\begin{array}{r}{Q=I-\\frac{1-\\alpha}{1+\\alpha}D^{-1/2}\\bar{A}D^{-1/2}}\\end{array}$ with $\\Lambda=I$ and $b=2\\alpha D^{-1/2}e_{s}/(1+\\alpha)$ ", "page_idx": 44}, {"type": "text", "text": "The detailed parameters are: ", "page_idx": 44}, {"type": "text", "text": "\u2022 1. $W=\\tilde{D}^{-1/2}\\tilde{A}\\tilde{D}^{-1/2}$ and $\\tilde{\\pmb{A}}=\\pmb{I}+\\pmb{A}$ is the adjacency matrix defined on $\\mathcal{G}(\\boldsymbol{\\upnu},\\boldsymbol{\\mathcal{E}})$ by adding self-loops for all nodes, and $\\tilde{\\boldsymbol{D}}=\\boldsymbol{I}+\\boldsymbol{D}$ is defined as the augmented degree matrix by adding self-loops. $\\alpha\\,\\in\\,(0,1)$ and usually $\\alpha\\,<\\,0.5$ , The $\\pmb{I}-(1-\\alpha)\\pmb{W}$ is the perturbed augmented normalized Laplacian with perturbed parameter $\\alpha$ .   \n\u2022 2. $\\begin{array}{r}{Q=D^{-1/2}\\left(D-\\frac{1-\\alpha}{2}(D+A)\\right)D^{-1/2}=\\alpha I+\\frac{1-\\alpha}{2}\\mathcal{L}>0}\\end{array}$ and $\\begin{array}{r}{Q=\\frac{1-\\alpha}{1+\\alpha}D^{-1/2}A D^{-1/2}}\\end{array}$ . This is known as the lazy random-walk version of PPR vectors.   \n\u2022 3. $\\pmb{x}=\\alpha\\left(\\pmb{I}-(1-\\alpha)\\pmb{A}\\pmb{D}^{-1}\\right)^{-1}\\pmb{e}_{s}$ This is the standard Personalized PageRank vectors widely used for graph embeddings and graph neural network designing [7]. It is also used for decoupling for large-scale GNNs [56]. ", "page_idx": 44}, {"type": "table", "img_path": "wT2KhEb97a/tmp/acddb70c4a2d2595cabe92aeaad78ac4efc14e73c4206c4689840f4396a9c74a.jpg", "table_caption": [], "table_footnote": ["Table 3: Dataset Statistics "], "page_idx": 45}, {"type": "text", "text": "\u2022 4. Graph kernel computation for online learning. Each computed vector serves as semi-supervised learning feature vectors [24] or as online node labeling learning vectors [41, 57]. Note the target linear system when $\\sigma=1$ ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle{D^{-1/2}\\left(\\frac{\\lambda}{n}I_{n}+D-A\\right)D^{-1/2}D^{1/2}y=2\\lambda D^{-1/2}e_{s}}}}\\\\ {{\\displaystyle{\\left(\\frac{\\lambda}{n}D^{-1}+I_{n}-D^{-1/2}A D^{-1/2}\\right)D^{1/2}y=2\\lambda D^{-1/2}e_{s}.}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Hence, we have $\\begin{array}{r}{\\mathbf{A}=\\frac{\\lambda}{n}D^{-1}+I_{n},\\sigma=1,b=2\\lambda D^{-1/2}e_{s}}\\end{array}$ ", "page_idx": 45}, {"type": "text", "text": "F Experimental Details and Missing Results ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "F.1 Datasets and Preprocessing ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Following Leskovec et al. [34], we treat all 17 graphs as undirected with unit weights. We remove selfloops and keep the largest connected component when the graph is disconnected. After preprocessing, the graphs range from 169, 343 nodes in ogbn-proteins to 111, 059, 433 in ogbn-papers100M, as presented in Table 3. ", "page_idx": 45}, {"type": "text", "text": "F.2 Problems Settings and Baseline Methods ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "For solving Equation (3), we randomly select 50 source nodes $s$ from each graph. The damping factor is fixed at 0.1, i.e., $\\alpha~=~0.1$ for all experiments, and it varies within the range $\\{0.005,0.01,0.05,0.1,0.15,0.2,0.25,0.3\\}$ for others. The $\\epsilon$ is chosen from the range $\\left[2\\alpha/((1+\\alpha)d_{s}),10^{-4}/n\\right]$ . ", "page_idx": 45}, {"type": "text", "text": "For solving the local clustering problem, we follow the greedy strategy from Andersen et al. [2], where a local cluster is identified by examining the top magnitudes in PPR vectors. Specifically, we denote the boundary of $\\boldsymbol{S}$ as $\\partial(S)\\stackrel{}{=}\\{(u,v)\\in\\mathcal{E}:u\\in S,\\bar{v}\\not\\in S\\}$ . The conductance of $\\boldsymbol{S}$ is defined as ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\Phi(S)\\triangleq{\\frac{|\\partial(S)|}{\\operatorname*{min}(\\operatorname{vol}(S),2m-\\operatorname{vol}(\\mathcal{V}\\backslash S))}}.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "image", "img_path": "wT2KhEb97a/tmp/f08028f28d18c9ac1540f4a2d35465b0eae04545a1d0b4a0d8cd856ec5ed1f9c.jpg", "img_caption": ["Figure 7: The LOCSOR method compared with CGM over 18 graphs. "], "img_footnote": [], "page_idx": 46}, {"type": "image", "img_path": "wT2KhEb97a/tmp/79b29cfc85d0ab17b45ff72149b251e4c9bc839e3719272b07126990e5e87352.jpg", "img_caption": ["Figure 8: The speedup of local solvers compared with their standard counterparts. "], "img_footnote": [], "page_idx": 46}, {"type": "text", "text": "The goal of local clustering is to obtain PPR vectors using these local methods and then apply clustering algorithms to find clusters with low conductance. For the sorting process, given the approximate PPR vector $\\tilde{\\pi}$ , we sort $D^{-1/2}\\tilde{\\pi}$ in decreasing order of magnitudes. Let the ordered nodes be $v_{1},v_{2},\\ldots,v_{t}$ ; the local clustering algorithm iteratively checks the conductance reduction by $v_{1},v_{2},\\cdots,v_{k}$ where $k\\,=\\,1,2,\\ldots,t$ , and after completing all checks, it returns a subset $v_{1},v_{2},\\ldots,v_{k^{\\prime}}$ that has the minimal conductance among all examined subsets. ", "page_idx": 46}, {"type": "text", "text": "Parameter settings of baselines. For the local ISTA method [13], the precision parameter is set to $\\hat{\\epsilon}=0.5$ for all experiments. According to the algorithm\u2019s description of ISTA, the corresponding $\\rho$ value is given by $\\boldsymbol{\\bar{\\epsilon}}/(1+\\boldsymbol{\\hat{\\epsilon}})$ . For LOCSOR, the parameter $\\omega$ is calculated as $2(1+\\alpha)/(\\bar{1}+\\sqrt{\\alpha})^{2}$ . For the local FISTA, as demonstrated in [22], we adopt the same settings as for ISTA and follow its implementation guidelines. We also include preliminary results on ASPR [37]. The algorithm incorporates a parameter, $\\hat{\\epsilon}$ , to control the number of iterations in the nested Accelerated Projected Gradient Descent (APGD). We adjust $\\hat{\\epsilon}$ from low precision, $\\hat{\\epsilon}=0.1/n$ , to high precision, $\\hat{\\epsilon}=10^{-4}/n$ , to ensure the identification of a good approximation. ", "page_idx": 46}, {"type": "text", "text": "For our experiment, we used a server powered by an Intel(R) Xeon(R) Gold 5218R CPU, which features 40 cores (80 threads). The system is equipped with 256 GB of RAM. ", "page_idx": 46}, {"type": "text", "text": "F.3 Full results of Fig. 15 4 5 6 ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "In all 15 graphs, we set $\\alpha=0.1$ and $\\epsilon=0.1/n$ . For each of the testing graphs, we randomly select 50 nodes and run LOCSOR and CGM. ", "page_idx": 46}, {"type": "image", "img_path": "wT2KhEb97a/tmp/959787272e3d470362b301be558831d4c8250a2f98d03842909820c3dd2a93a6.jpg", "img_caption": ["Figure 10: Comparison of three local solvers over 15 graphs. "], "img_footnote": [], "page_idx": 47}, {"type": "text", "text": "Fig. 8 presents all speedup tests on 15 datasets. It is evident that these standard linear solvers can be localized effectively. ", "page_idx": 47}, {"type": "image", "img_path": "wT2KhEb97a/tmp/f00b4f7dc6a31ac48ea71aa990febeb9cb41d48a921a5b0ab427d1b036b9b2ac.jpg", "img_caption": ["Figure 9: The estimation error reduction tests on 7 solvers including our LOCSOR, LOCHB, and LOCCH. The experiments were conducted on 15 datasets. "], "img_footnote": [], "page_idx": 47}, {"type": "text", "text": "Fig. 9 presents the missing results on the estimation error reduction for 15 datasets. Compared with the global solver CGM, all local methods show significant speedup in the early stages. To compare our three local solvers, we zoom in on our results and present them in Fig. 10. Empirically, LOCSOR is the fastest algorithm when the parameter $\\omega$ is chosen optimally. ", "page_idx": 47}, {"type": "image", "img_path": "wT2KhEb97a/tmp/d30318589a5dad10d5c2fdb3209daf580863f30f2f069bc2e1b267b7bf8ad802.jpg", "img_caption": ["Figure 11: Estimation error as a function of the number of operations on com-friendster. We randomly select 5 different nodes and use $\\epsilon=1./n$ and $\\epsilon=1/m$ . "], "img_footnote": [], "page_idx": 48}, {"type": "image", "img_path": "wT2KhEb97a/tmp/35d5f8969e2d51735604bd01ef2083a2025c3948c22ec07622868b00c46b50d8.jpg", "img_caption": [], "img_footnote": [], "page_idx": 48}, {"type": "table", "img_path": "wT2KhEb97a/tmp/7a245dd62130a9cf2b7605b2990dab13822250dff07b9e136fca3ebf10bd36df.jpg", "table_caption": ["Figure 12: Estimation error as a function of the number of operations on ogbn-papers $100\\mathrm{m}$ . We randomly select 5 different nodes and use $\\epsilon=1./n$ and $\\epsilon=1/m$ . "], "table_footnote": ["Table 4: Summary of runtime and operations for 15 datasets. $\\langle\\epsilon=10^{-6}$ ) "], "page_idx": 48}, {"type": "image", "img_path": "wT2KhEb97a/tmp/1b4b4a4c959e0e63b602a9a5a8a60e6eb58c41506d8c1e36ef0c466bdd873862.jpg", "img_caption": ["Figure 13: Estimation error as a function of operations needed. For $\\alpha\\,=\\,0.005$ , $\\alpha\\,=\\,0.1$ , and $\\alpha=0.25$ . "], "img_footnote": [], "page_idx": 49}, {"type": "image", "img_path": "wT2KhEb97a/tmp/fcf4d99abcc24097ba4adbdee018c7ed030a0061267b02280ac7017c2c5a64b0.jpg", "img_caption": [], "img_footnote": [], "page_idx": 49}, {"type": "text", "text": "Figure 14: The graph conductance found by local graph clustering method using different local approximate methods. Experiments ran on 15 graphs. $\\bar{\\epsilon}=10^{-6}$ ) ", "page_idx": 49}, {"type": "table", "img_path": "wT2KhEb97a/tmp/9fbac0b1547f6909a673c68c8f15270607e15949d89bfcaa0bdfd765deb3bc19.jpg", "table_caption": [], "table_footnote": ["Table 5: The local conductance for six local solvers tested on 15 graphs datasets. $\\overline{{{\\epsilon}=10^{-6}}}$ ) "], "page_idx": 50}, {"type": "table", "img_path": "wT2KhEb97a/tmp/833c96278902f11cb1d5216ff4aeb966c820745a4558bb3c25589b26862c711d.jpg", "table_caption": [], "table_footnote": ["Table 6: Runtime (seconds) for six local solvers tested on 15 graphs datasets. $\\overline{{\\epsilon=10}}^{-6}$ ) "], "page_idx": 50}, {"type": "table", "img_path": "wT2KhEb97a/tmp/3344ae6d8ada0504c8f3b163955b335f70bec993bd816fd4ab85d6819468db6c.jpg", "table_caption": ["F.4 Results on local clustering "], "table_footnote": ["Table 7: Operations Needed for six local solvers tested on 15 graphs datasets. $\\overline{{\\epsilon=10^{-6}}}$ ) "], "page_idx": 51}, {"type": "text", "text": "G Related work ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Many graph applications [2, 7, 29, 15, 27, 36, 30, 44, 25, 45, 51, 48] only require solving Equ. (1) approximately. The reasons could be either the most energies of $\\pi$ are among a small set of nodes forming small subgraphs, or one wants to study large graphs by checking them locally. Given a graph $\\mathcal{G}$ with $n$ nodes and $m$ edges, there are two main types of iterative solvers for Equ. (1) as follows: ", "page_idx": 52}, {"type": "image", "img_path": "wT2KhEb97a/tmp/914a2be24faf3bb81e0d49ad64562ce77eaaf54ae660c8d0b038bb00bdb4026c.jpg", "img_caption": ["Standard iterative methods. Methods for solving linear systems have been well-established over the past decades (see textbooks of Saad [43], Golub & Figure 15: Comparison of the error reduction Van Loan [19], Young [55]). The fastest linear solver between the proposed LOCCH and the stanfor solving the symmetrized version of Equ. (1) is dard CGM on the papers100M dataset [23], the Conjugate Gradient Method (CGM) with runtime in terms of the number of operations required. complexity $\\tilde{\\mathcal{O}}(m/\\sqrt{\\alpha})$ where $m$ is the number of edges in the graph. It costs $\\Theta(m)$ to access the entire graph at each iteration; hence, it is much slower than local solvers, as demonstrated in Fig. 15. The symmetric diagonally dominant (SDD) solvers advance CGM further to have complexity $\\tilde{\\mathcal{O}}\\,\\bigl(m\\log^{c}n\\log(1/\\epsilon)\\bigr)$ [31, 46]. Anikin et al. [4] considered the PageRank problem and proposed an algorithm with runtime depending on ${\\mathcal{O}}(n)$ . This paper focuses on local algorithms where the goal is to avoid the dominant factor $m$ or $n$ by avoiding the full $Q x$ operation. "], "img_footnote": [], "page_idx": 52}, {"type": "text", "text": "Local algorithms. Local solvers, in contrast to standard counterparts, leverage the fact that the energy of $\\pi$ lives in a small portion of the graph and hence do not require $\\mathcal{O}(m)$ or ${\\mathcal{O}}(n)$ per iteration. They are advantageous for huge-scale graphs demonstrated in Fig. 15. Andersen et al. [2] used APPR to obtain an approximate of $\\pi$ for local clustering. Quite similar algorithms were developed in Berkhin [6] (bookmark-coloring algorithm) and Kloster & Gleich [28] (Gauss-Southwell procedure). ", "page_idx": 52}, {"type": "text", "text": "Under the same stopping condition as APPR, Fountoulakis et al. [13] demonstrated that APPR is equivalent to coordinate descent via variational characterization, with a runtime of $\\tilde{\\mathcal{O}}(1/(\\alpha\\epsilon))$ using ISTA where the monotonicity and conservation properties remain. Hence, it is nature to ask whether $\\tilde{\\mathcal{O}}(1/(\\sqrt{\\alpha}\\epsilon))$ could be achieved by FISTA [5] in Fountoulakis & Yang [12]. However, the difficulty is that FISTA violates the monotonicity property where the volume accessed of per-iteration cannot be bounded properly. To overcome this, Mart\u00ednez-Rubio et al. [37] proposed a nested accelerated projected gradient descent (APGD) and gradually expanding solutions so that the monotonicity property still holds. However, nested APGD requires solving subproblems accurately, which in practice may be cumbersome if the precision requirement of the inner problem is too stringent. All current local methods rely on some monotonicity property of variables to guarantee locality, which does not exist in most accelerated frameworks; thus, developing an accelerated method that is guaranteed to preserve intermediate variable sparsity remains challenging. ", "page_idx": 52}, {"type": "text", "text": "It is worth mentioning that local methods are also closely related to sublinear time and local computational algorithms [42, 1]. From the optimization perspective, the equivalence between Gauss-Seidel and coordinate descent has been considered [49, 35, 39, 50] but does not focus on local analysis. ", "page_idx": 52}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 53}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 53}, {"type": "text", "text": "Justification: The abstract and introduction clearly state the main claims made by the paper, including its key contributions and scope. ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 53}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 53}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Justification: The accelerated algorithms, such as LOCCH and LOCHB, may exhibit instability when $\\alpha$ is small. This limitation arises due to the inherent constraints of global methods. ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 53}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 53}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 53}, {"type": "text", "text": "Justification: All assumptions are clearly stated. ", "page_idx": 54}, {"type": "text", "text": "Guidelines: ", "page_idx": 54}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 54}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 54}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 54}, {"type": "text", "text": "Justification: We have provided our code for the review process and will make it publicly available upon publication. ", "page_idx": 54}, {"type": "text", "text": "Guidelines: ", "page_idx": 54}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 54}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 54}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Justification: All datasets used in this study are publicly available. ", "page_idx": 54}, {"type": "text", "text": "Guidelines: ", "page_idx": 55}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https:// nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 55}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 55}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 55}, {"type": "text", "text": "Justification: All parameter settings of our methods and baselines are included. ", "page_idx": 55}, {"type": "text", "text": "Guidelines: ", "page_idx": 55}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 55}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 55}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 55}, {"type": "text", "text": "Justification: For most of our results, we report the standard error over 50 random sampling nodes. Guidelines: ", "page_idx": 55}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). ", "page_idx": 55}, {"type": "text", "text": "\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 56}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 56}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 56}, {"type": "text", "text": "Justification: For our experiment, we used a server powered by an Intel(R) Xeon(R) Gold 5218R CPU, which features 40 cores (80 threads). The system is equipped with 256 GB of RAM. ", "page_idx": 56}, {"type": "text", "text": "Guidelines: ", "page_idx": 56}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 56}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 56}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 56}, {"type": "text", "text": "Justification: The research conducted in the paper conforms in every respect with the NeurIPS Code of Ethics. The authors have thoroughly reviewed and adhered to the guidelines, ensuring that all aspects of their work align with ethical standards. ", "page_idx": 56}, {"type": "text", "text": "Guidelines: ", "page_idx": 56}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 56}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 56}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 56}, {"type": "text", "text": "Justification: The advancements in accelerated algorithms like LOCCH and LOCHB can significantly enhance computational efficiency in various applications, contributing to faster and more effective solutions in fields such as data analysis, machine learning, and optimization. ", "page_idx": 56}, {"type": "text", "text": "Guidelines: ", "page_idx": 56}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 56}, {"type": "text", "text": "", "page_idx": 57}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 57}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety fliters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 57}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 57}, {"type": "text", "text": "[NA]   \nustification: None.   \nGuidelines: \u2022 The answer NA means that the paper does not use existing assets. \u2022 The authors should cite the original paper that produced the code package or dataset. \u2022 The authors should state which version of the asset is used and, if possible, include a URL. \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset. \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. \u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. \u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. \u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 57}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 57}, {"type": "text", "text": "Answer: [NA]   \nJustification: None.   \nGuidelines: \u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 57}, {"type": "text", "text": "", "page_idx": 58}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 58}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 58}, {"type": "text", "text": "Justification: No human subjects involved. ", "page_idx": 58}, {"type": "text", "text": "Guidelines: ", "page_idx": 58}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 58}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 58}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 58}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 58}, {"type": "text", "text": "Justification: No human subjects involved. ", "page_idx": 58}, {"type": "text", "text": "Guidelines: ", "page_idx": 58}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 58}]