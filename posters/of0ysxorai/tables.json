[{"figure_path": "OF0YsxoRai/tables/tables_5_1.jpg", "caption": "Figure 2: Optimization performance under different synthetic function and acquisition function. Sparse GP models are trained with 50 inducing variables. The offline dataset contains 2000 random data points and the online budget is 500 with batch size of 10.", "description": "This figure compares the performance of FOCALBO against other sparse Gaussian process-based Bayesian optimization methods (SVGP, WOGP, Vecchia GP) across three different synthetic functions (Shekel, Michalewicz, and a GP function) and three different acquisition functions (TS, EI, PI).  It demonstrates the superior performance of FOCALBO regardless of the acquisition function and function type. The experimental setup uses 50 inducing variables for all sparse GP models, with an offline dataset of 2000 random data points and an online budget of 500 points, evaluated in batches of 10.", "section": "5.1 Synthetic functions"}, {"figure_path": "OF0YsxoRai/tables/tables_6_1.jpg", "caption": "Table 2: Optimization performance under different synthetic function and acquisition function. Sparse GP models are trained with 50 inducing variables. The offline dataset contains 2000 random data points and the online budget is 500 with batch size of 10.", "description": "This table presents the results of a Bayesian optimization experiment using different synthetic functions (Shekel, Michalewicz, and a Gaussian process function), acquisition functions (Thompson sampling, expected improvement, and probability of improvement), and sparse Gaussian process models (SVGP, WOGP, and Vecchia GP).  The experiment used an offline dataset of 2000 points and an online budget of 500 evaluations with batches of 10.  The table compares the performance of FOCALBO with existing sparse GP-based BO methods.", "section": "5.1 Synthetic functions"}]