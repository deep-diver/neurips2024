[{"type": "text", "text": "Wasserstein Gradient Boosting: A Framework for Distribution-Valued Supervised Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Takuo Matsubara   \nThe University of Edinburgh Edinburgh, EH9 3JZ   \ntakuo.matsubara@ed.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Gradient boosting is a sequential ensemble method that fits a new weaker learner to pseudo residuals at each iteration. We propose Wasserstein gradient boosting, a novel extension of gradient boosting, which fits a new weak learner to alternative pseudo residuals that are Wasserstein gradients of loss functionals of probability distributions assigned at each input. It solves distribution-valued supervised learning, where the output values of the training dataset are probability distributions. In classification and regression, a model typically returns, for each input, a point estimate of a parameter of a noise distribution specified for a response variable, such as the class probability parameter of a categorical distribution specified for a response label. A main application of Wasserstein gradient boosting in this paper is tree-based evidential learning, which returns a distributional estimate of the response parameter for each input. We empirically demonstrate the competitive performance of the probabilistic prediction by Wasserstein gradient boosting in comparison with existing uncertainty quantification methods. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Gradient boosting is a celebrated machine learning algorithm that has achieved considerable success with tabular data [1]. Gradient boosting has been extensively used for point forecasts and probabilistic classification, yet a relatively small number of studies have been concerned with the predictive uncertainty of gradient boosting. Predictive uncertainty of machine learning models plays a growing role in today\u2019s real-world production systems [2]. It is vital for safety-critical systems, such as medical diagnoses [3] and autonomous driving [4], to assess the potential risk of their actions that partially or entirely rely on predictions from their models. Gradient boosting has already been applied in a diverse range of real-world applications, including click prediction [5], ranking systems [6], scientific discovery [7], and data competition [8]. There is a pressing need for methodology to harness the power of gradient boosting to predictive uncertainty quantification. ", "page_idx": 0}, {"type": "text", "text": "In classification and regression, we typically specify a noise distribution $p(y\\mid\\theta)$ of a response variable $y$ and use a model to return a point estimate $\\bar{\\theta(x)}$ of the response parameter for each input $x$ . In recent years, the importance of capturing uncertainty in the model output $\\theta(x)$ has increasingly been emphasised [2]. A variety of approaches have been proposed to obtain a distributional estimate $p(\\theta\\mid x)$ of the response parameter for each input $x$ [e.g. 9, 10, 11]. For example, Bayesian neural networks (BNNs) quantify uncertainty in network weights and propagate it to the space of network outputs. Marginalising the predictive distribution $p(y\\mid\\theta)$ over the distributional estimate $p(\\theta\\mid x)$ has been demonstrated to confer enhanced predictive accuracy and robustness against adversarial attacks [11]. Furthermore, the dispersion of the distributional estimate has been used as a powerful indicator for out-of-distribution (OOD) detection [12]. ", "page_idx": 0}, {"type": "image", "img_path": "cuO0DenqMl/tmp/40ce5618e63ad4b56d4d706e4b2ac938f9ffc45d240506d2daca8f49e808907f.jpg", "img_caption": ["Figure 1: Illustration of WGBoost trained on a set $\\{x_{i},\\mu_{i}\\}_{i=1}^{10}$ whose inputs are 10 grid points in $[-{3.5},3.5]$ and each output distribution is a normal distribution $\\mu_{i}(\\theta)=\\bar{\\mathcal{N}}(\\theta\\mid\\sin(\\bar{x}_{i}),\\bar{0}.5)$ over $\\theta\\:\\in\\:\\mathbb{R}$ . The blue area indicates the $95\\%$ high probability region of the conditional distribution ${\\mathcal{N}}(\\theta\\mid\\sin(x),0.5)$ . WGBoost returns $N=10$ particles (red lines) to predict the output distribution for each input $x$ . This illustration uses the Gaussian kernel regressor for every weaker learner. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "In this context, a line of research based on the concept of evidential learning has recently gained significant attention [11, 13, 14, 15]. The idea can be broadly interpreted as making use of the \u2018individual-level\u2019 posterior $p(\\theta\\mid y_{i})$ of the response parameter $\\theta$ conditional on each individual datum $y_{i}$ , which arises from the response-distribution likelihood $p(y_{i}\\mid\\theta)$ and a user-specified prior $p(\\theta)$ . If each individual-level posterior falls into a closed form characterised by some hyperparameter, neural networks can be trained by using the finite-dimensional hyperparameter as a target value for each input. Outstanding performance and computational efficiency of the existing approaches have been delivered in a wide spectrum of engineering and medical applications [16, 17, 18, 19]. However, the existing approaches are limited to neural networks and to the case where every individual-level posterior is in closed form so that the finite-dimensional hyperparameter can be predicted by proxy. In general, posterior distributions are known only up to their normalising constants and, therefore, require an approximation typically by particles [20]. ", "page_idx": 1}, {"type": "text", "text": "Without closed-form expression, each individual-level posterior needs to be treated as an infinitedimensional output for each input. This challenge poses the following fundamental question: ", "page_idx": 1}, {"type": "text", "text": "Consider a supervised learning setting whose outputs are probability distributions. Given a training set of input values and output distributions $\\{x_{i},\\bar{\\mu_{i}}\\}_{i=1}^{D}$ , can we build a model that receives an input $x$ and returns a nonparametric prediction of the output distribution? ", "page_idx": 1}, {"type": "text", "text": "Motivated by this question, we propose a general framework of Wasserstein gradient boosting (WGBoost). WGBoost receives an input and returns a particle approximation of the output distribution. Figure 1 illustrates inputs and outputs of WGBoost. In this paper, we focus on application of WGBoost to evidential learning, where the individual-level posterior $p(\\theta\\mid y_{i})$ of the response parameter $\\theta$ is used as the output distribution $\\mu_{i}$ for each input $x_{i}$ in the training set. Figure 2 compares the pipeline of evidential learning based on WGBoost with that of Bayesian learning. ", "page_idx": 1}, {"type": "text", "text": "Contributions Our contributions are summarised as follows: ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "1. Methodology of WGBoost: Section 2 establishes the general framework of WGBoost. It is a novel family of gradient boosting that returns a set of particles that approximates an output distribution assigned at each input. In contrast to standard gradient boosting that fits a weak learner to the gradient of a loss function, WGBoost fits a weak learner to the estimated Wasserstein gradient of a loss functional over probability distributions. ", "page_idx": 1}, {"type": "text", "text": "2. Application to Evidential Learning: Section 3 establishes tree-based evidential learning based on WGBoost, with the loss functional specified by the Kullback\u2013Leibler (KL) divergence. Following modern gradient-boosting libraries [21, 22] that uses second-order gradient boosting (c.f. Section 2.2), we implement a concrete second-order WGBoost algorithm built on an approximate Wasserstein gradient and Hessian of the KL divergence. 3. Experiment on Real-world Data: Section 4 demonstrates the performance of probabilistic regression, and classification with OOD detection, on real-world tabular datasets. To the author\u2019s knowledge, WGBoost is the first framework that enables evidential learning for (i) boosted tree models and (ii) cases without closed form of individual-level posteriors. ", "page_idx": 1}, {"type": "image", "img_path": "cuO0DenqMl/tmp/ebaf83c69145e85dc7215906724f416c3af7f4b63d2a2ada86cb47823f0f8eff.jpg", "img_caption": ["(a) Bayesian learning of a model $f(x,w)$ "], "img_footnote": [], "page_idx": 2}, {"type": "image", "img_path": "cuO0DenqMl/tmp/458b8ea60afd33be5b3ddf7ce199737e5d981a5a054dbfde146d2f1f9dbfac01.jpg", "img_caption": ["(b) Evidential learning based on WGBoost "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 2: Comparison of the pipeline of (a) Bayesian learning and (b) evidential learning based on WGBoost. The former uses the (global-level) posterior $p(w\\mid\\{x_{i},y_{i}\\}_{i=1}^{D})$ of the model parameter $w$ conditional on all data, and samples multiple models from it. The latter uses the individual-level posterior $p(\\theta\\mid y_{i})$ of the response parameter $\\theta$ as the output distribution of the training set, and trains WGBoost that returns a particle-based distributional estimate $p(\\theta\\mid x)$ of $\\theta$ for each input $x$ . ", "page_idx": 2}, {"type": "text", "text": "2 Wasserstein Gradient Boosting ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "This section establishes the general formulation of WGBoost. Section 2.1 recaps the notion of Wasserstein gradient flows, a \u2018gradient\u2019 system of probability distributions that minimises an objective functional in the space of probability distributions. Section 2.2 recaps the notion of gradient boosting, a sequential ensemble method that fits a new weak learner to the \u2018gradient\u2019 of the remaining loss at each iteration. Section 2.3 combines the above two notions to establish WGBoost, a novel family of gradient boosting that enables to solve distribution-valued supervised learning. ", "page_idx": 2}, {"type": "text", "text": "Notation and Setting Let $\\mathcal{X}$ and $\\boldsymbol{\\wp}$ denote the space of inputs and responses in classification and regression. Suppose $\\Theta=\\mathbb{R}^{d}$ . Let $\\mathcal{P}_{2}$ be the 2-Wasserstein space i.e. a set of all probability distributions on $\\Theta$ with finite second moment equipped with the Wasserstein metric [23]. We identify a probability distribution in $\\mathcal{P}_{2}$ with its density whenever it exits. Denote by $\\odot$ and $\\oslash$ , respectively, elementwise multiplication and elementwise division of two vectors in $\\mathbb{R}^{d}$ . Let $\\nabla$ be the gradient operator. Let $\\nabla_{\\mathrm{d}}^{2}$ be a second-order gradient operator that takes the second derivative at each coordinate i.e. $\\nabla_{\\mathrm{d}}^{2}f(\\theta)\\stackrel{\\mathrm{~\\tiny~-~}}{=}[\\partial^{2}f(\\theta)/\\partial\\theta_{1}^{2},\\ldots,\\partial^{2}f(\\theta)/\\partial\\theta_{d}^{2}]^{\\mathrm{T}}\\in\\mathbb{R}^{d}.$ . ", "page_idx": 2}, {"type": "text", "text": "2.1 Wasserstein Gradient Flow ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In the Euclidean space, a gradient flow of a function $f$ means a curve of points $x_{t}$ that solves a differential equation $(d/d t)\\bar{x}_{t}=-\\nabla f(x_{t})$ from some initial value $x_{0}$ . That is the continuous-time limit of gradient descent, which minimises the function $f$ as $t\\to\\infty$ . A Wasserstein gradient flow means a curve of probability distributions $\\mu_{t}$ minimising a given functional $\\mathcal{F}$ on the 2-Wasserstein space $\\mathcal{P}_{2}$ from some initial distribution $\\mu_{0}$ . The Wasserstein gradient flow $\\mu_{t}$ is characterised as a solution of a partial differential equation, known as the continuity equation: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\frac{d}{d t}\\mu_{t}=-\\nabla\\cdot\\left(\\mu_{t}\\nabla_{W}\\mathcal{F}(\\mu_{t})\\right)\\quad\\mathrm{given}\\quad\\mu_{0}\\in\\mathcal{P}_{2},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\nabla_{W}\\mathcal{F}(\\mu):\\Theta\\to\\Theta$ denotes the Wasserstein gradient of $\\mathcal{F}$ at $\\mu$ [24, 25]. Appendix A recaps the derivation of the Wasserstein gradient, presenting the examples for several functionals. ", "page_idx": 2}, {"type": "text", "text": "One of the elegant properties of the Wasserstein gradient flow is casting the infinite-dimensional optimisation of the functional $\\mathcal{F}$ as a finite-dimensional particle update [23]. The continuity equation (1) can be reformulated as a dynamical system of a random variable $\\theta_{t}\\sim\\mu_{t}$ , such that ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\frac{d}{d t}\\theta_{t}=-\\left[\\nabla_{W}\\mathcal{F}(\\mu_{t})\\right](\\theta_{t})\\quad\\mathrm{given}\\quad\\theta_{0}\\sim\\mu_{0},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "in the sense that the law $\\mu_{t}$ of such a random variable $\\theta_{t}$ is a weak solution of the continuity equation. Consider the case where the initial measure $\\mu_{0}$ is set to the empirical distribution $\\hat{\\mu}_{0}$ of $N$ particles $\\{\\theta_{0}^{n}\\}_{n=1}^{N}$ . Discretising the continuous-time system (2) by the Euler method with a small step size $\\nu>0$ yields an iterative update scheme of $N$ particles $\\lbrace\\dot{\\theta_{m}^{n}}\\rbrace_{n=1}^{N}$ from step $m=0$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left[\\!\\!\\begin{array}{c}{\\theta_{m+1}^{1}}\\\\ {\\vdots}\\\\ {\\theta_{m+1}^{N}}\\end{array}\\!\\!\\right]=\\left[\\!\\!\\begin{array}{c}{\\theta_{m}^{1}}\\\\ {\\vdots}\\\\ {\\theta_{m}^{N}}\\end{array}\\!\\!\\right]+\\nu\\left[\\!\\!\\begin{array}{c}{-[\\nabla_{W}\\mathcal{F}(\\hat{\\mu}_{m})](\\theta_{m}^{1})}\\\\ {\\vdots}\\\\ {-[\\nabla_{W}\\mathcal{F}(\\hat{\\mu}_{m})](\\theta_{m}^{N})}\\end{array}\\!\\!\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\hat{\\mu}_{m}$ denotes the empirical distribution of the particles $\\lbrace\\theta_{m}^{n}\\rbrace_{n=1}^{N}$ at step $m$ ", "page_idx": 3}, {"type": "text", "text": "In practice, it is common that the Wasserstein gradient of a chosen functional $\\mathcal{F}$ is not well-defined for empirical distributions. In such case, the particle update scheme (3) is not directly applicable because it depends on the Wasserstein gradient $\\bar{\\nabla_{W}}\\bar{\\mathcal{F}}(\\hat{\\mu}_{m})$ at the empirical distribution $\\hat{\\mu}_{m}$ . For example, the KL divergence ${\\mathcal{F}}(\\mu)=\\operatorname{KL}({\\bar{\\mu}}\\mid\\pi)$ with a reference distribution $\\pi$ leads to the Wasserstein gradient $[\\nabla_{W}\\mathcal{F}(\\mu)](\\theta)=-(\\nabla\\log\\pi(\\theta)-\\nabla\\log\\mu(\\theta))$ ill-defined if $\\mu$ is an empirical distribution. Hence, the particle update scheme (3) is often performed with the estimated or approximated Wasserstein gradient well-defined for empirical distributions [e.g. 26, 27, 28, 29, 30]. A main application of WGBoost in Section 3 uses the \u2018smoothed\u2019 Wasserstein gradient of the KL divergence [26]. ", "page_idx": 3}, {"type": "text", "text": "2.2 Gradient Boosting ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Gradient boosting [31] is a sequential ensemble method of $M$ weak learners $f_{1},\\ldots,f_{M}$ . It iteratively constructs an ensemble $F_{m}$ of $m$ weak learners $f_{1},\\ldots,f_{m}$ from step $m=0$ to $M$ . Given the current ensemble $F_{m}$ at step $m$ , it trains a new weak learner $f_{m+1}$ to construct the next ensemble by ", "page_idx": 3}, {"type": "equation", "text": "$$\nF_{m+1}(x)=F_{m}(x)+\\nu f_{m+1}(x),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\nu$ is a shrinkage hyperparameter called a learning rate. The initial state of the ensemble $F_{0}(x)$ at step $m=0$ is typically set to a constant that best ftis the data. Any learning algorithm can be used as a weak learner in principle, although tree-based algorithms are most used [32]. ", "page_idx": 3}, {"type": "text", "text": "The fundamental idea of gradient boosting is to train the new weak learner $f_{m+1}$ to approximate the negative gradient of the remaining error of the current ensemble $F_{m}$ . Suppose that a loss function $L$ measures the remaining error $\\bar{R_{i}({F_{m}(x_{i})})}:=L(F_{m}(x_{i}),y_{i})$ for each output vector $y_{i}\\in\\mathbb{R}^{d}$ . The new weak learner $f_{m+1}$ is fitted to the set $\\{x_{i},g_{i}\\}_{i=1}^{D}$ whose target variable $g_{i}$ is each specified by ", "page_idx": 3}, {"type": "equation", "text": "$$\ng_{i}:=-\\nabla R_{i}(F_{m}(x_{i}))\\in\\mathbb{R}^{d}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The target $g_{i}$ is often called a pseudo residual. For each data input $x_{i}$ , the boosting scheme (4) updates the output of the current ensemble $F_{m}(x_{i})$ in the steepest descent direction of the error $R_{i}(F_{m}(x_{i}))$ . Although [31] originally suggested an additional line search to determine a scaling constant of each weak learner, the line search has been reported to have a negligible influence on performance [33]. ", "page_idx": 3}, {"type": "text", "text": "In modern gradient-boosting libraries, such as XGBoost [21] and LightGBM [22], the standard practice is to use the diagonal (coordinatewise) Newton direction of the remaining error $R_{i}(F_{m}(x_{i}))$ in lieu of the negative gradient $g_{i}$ . The new base leaner $f_{m+1}$ is instead ftited to the set $\\{x_{i},g_{i}\\emptyset h_{i}\\}_{i=1}^{n}$ , where the negative gradient $g_{i}$ is divided elementwise by the Hessian diagonal $h_{i}$ given by ", "page_idx": 3}, {"type": "equation", "text": "$$\nh_{i}:=\\nabla_{\\mathrm{d}}^{2}R_{i}(F_{m}(x_{i}))\\in\\mathbb{R}^{d}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The target variable $g_{i}\\oslash h_{i}$ is the diagonal Newton direction that minimises the second-order Taylor approximation of the remaining error $R_{i}(F_{m}(x_{i}))$ for each coordinate independently. Combining the second-order gradient boosting framework with tree-based weak learners has demonstrated exceptional scalability and performance [34, 35]. Although it is possible to use the \u2018full\u2019 Newton direction as the target variable of each weak learner, the impracticality of the full Newton direction has been pointed out [e.g. 36, 37]. In addition, the coordinatewise computability of the diagonal Newton direction is suitable for popular gradient-boosting tree algorithms [36]. ", "page_idx": 3}, {"type": "text", "text": "2.3 General Formulation of Wasserstein Gradient Boosting ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Now we consider the setting of distribution-valued supervised learning, where we are given a training set of input vectors and output distributions $\\{x_{i},\\mu_{i}\\}_{i=1}^{\\bar{D}}\\subset\\mathcal{X}\\times\\mathcal{P}_{2}$ . Our goal is to construct a model that receives an input and returns a set of $N$ particles whose empirical distribution approximates the output distribution. We specify a loss functional $\\operatorname{D}(\\cdot\\mid\\cdot)$ between two probability distributions\u2014such as the KL divergence\u2014to measure the remaining error ${\\mathcal F}_{i}(\\cdot)\\,=\\,{\\mathrm{D}}(\\cdot\\,\\mid\\,\\mu_{i})$ for each $i$ -th training output distribution $\\mu_{i}$ . Our idea is to combine gradient boosting with Wasserstein gradient, where we iteratively construct a set of $N$ boosting ensembles $F_{m}^{1},\\ldots,F_{m}^{N}$ from step $m=0$ to $M$ . ", "page_idx": 3}, {"type": "text", "text": "Here, the output $F_{m}^{n}(x)$ of each $n$ -th boosting ensemble represents the $n$ -th output particle for an input $x$ . Given the current set of $N$ ensembles $F_{m}^{1},\\ldots,F_{m}^{N}$ F mN at step m, WGBoost trains a set of N new weak learners $f_{m+1}^{1},\\hdots,f_{m+1}^{N}$ and computes the next set of $N$ ensembles by ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left[F_{m+1}^{1}(x)\\right]=\\left[F_{m}^{1}(x)\\right]+\\nu\\left[\\begin{array}{c}{f_{m+1}^{1}(x)}\\\\ {\\vdots}\\\\ {F_{m+1}^{N}(x)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\nu$ is a learning rate. Similarly to standard gradient boosting, we specify the initial state of $N$ ensembles $F_{0}^{1},\\ldots,F_{0}^{N}$ at step $m\\,=\\,0$ by a set of constants. Throughout, denote by $\\hat{\\mu}_{m,i}$ the empirical distribution of the $N$ output particles $F_{m}^{1}(x_{i}),\\dots,F_{m}^{N}(x_{i})$ for each $i$ -th training input $x_{i}$ . ", "page_idx": 4}, {"type": "text", "text": "As discussed in Section 2.1, the Wasserstein gradient often needs to be estimated for empirical distributions. For better presentation, let $\\mathcal G_{i}(\\mu)$ denote an estimate of the Wasserstein gradient $\\nabla_{W}\\mathcal{F}_{i}(\\mu)$ of the $i$ -th remaining error ${\\mathcal{F}}_{i}({\\boldsymbol{\\mu}})$ , which is well-defined for any distribution $\\mu$ . If the Wasserstein gradient $\\nabla_{W}\\mathcal{F}_{i}(\\mu)$ is originally well-defined for any distribution $\\mu$ , it is a trivial choice of the estimate, i.e., $\\mathcal{G}_{i}(\\mu)=\\overset{\\cdot}{\\nabla}_{W}\\mathcal{F}_{i}(\\mu)$ . Otherwise, any suitable estimate can be used as $\\mathcal{G}_{i}(\\mu)$ . The foundamental idea of WGBoost is to train the $n$ -th new learner $f_{m+1}^{n}$ to approximate the estimated Wasserstein gradient $-\\mathcal{G}_{i}(\\hat{\\mu}_{m,i})$ evaluated at the $n$ -th boosting output $F_{m}^{n}(x_{i})$ for each $x_{i}$ , so that, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left[f_{m+1}^{1}(x_{i})\\right]\\approx\\left[-\\left[\\mathcal{G}_{i}\\left(\\hat{\\mu}_{m,i}\\right)\\right]\\left(F_{m}^{1}(x_{i})\\right)\\right]}\\\\ {\\vdots\\qquad\\qquad\\qquad\\vdots}\\\\ {f_{m+1}^{N}(x_{i})\\underline{{\\right]}}\\left[-\\left[\\mathcal{G}_{i}\\left(\\hat{\\mu}_{m,i}\\right)\\right]\\left(F_{m}^{N}(x_{i})\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "For each data input $x_{i}$ , the boosting scheme (5) approximates the particle update scheme (3) for the output particles $\\mathbf{\\bar{\\boldsymbol{F}}}_{m}^{1}(\\mathbf{\\boldsymbol{x}}_{i}),\\ldots,\\mathbf{\\boldsymbol{F}}_{m}^{N}(\\bar{\\boldsymbol{x}}_{i})$ under the estimated Wasserstein gradient. The output particles are updated in the direction to decrease the remianing error $\\mathcal{F}_{i}(\\hat{\\mu}_{m,i})=\\mathrm{\\bar{D}}(\\hat{\\mu}_{m,i}\\mid\\mu_{i})$ at each step $m$ . ", "page_idx": 4}, {"type": "text", "text": "Algorithm 1 summarises the general procedure of WGBoost. See Figure 1 for illustration of WGBoost. In Section 3, we choose the KL divergence as a loss functional D and use a kernel smoothing estimate of the Wasserstein gradient. See Appendix A for the Wasserstein gradient of other divergences. ", "page_idx": 4}, {"type": "text", "text": "Remark 1 (Stochastic WGBoost). Stochastic gradient boosting [38] uses only a randomly sampled subset of data to fit a new weak learner at each step $m$ to reduce the computational cost. The same subsampling approach can be applied for WGBoost whenever the dataset is large. ", "page_idx": 4}, {"type": "text", "text": "Remark 2 (Second-Order WGBoost). If any estimate of the Wasserstein \u2018Hessian\u2019 of the remaining error ${\\mathcal{F}}_{i}$ is available, the Newton direction of ${\\mathcal{F}}_{i}$ may also be computable [e.g. 39, 40]. Implemention of a second-order WGBoost algorithm is immediate by plugging such a Newton direction into $\\mathcal{G}_{i}(\\mu)$ in Algorithm 1. Our default WGBoost algorithm for tree-based evidential learning is built on a diagonal approximate Newton direction of the KL divergence, aligning with the standard practice in modern gradient-boosting libraries to use the diagonal Newton direction. ", "page_idx": 4}, {"type": "text", "text": "Algorithm 1: Wasserstein Gradient Boosting ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Input: training set $\\{x_{i},\\mu_{i}\\}_{i=1}^{D}$ of input $x_{i}\\in\\mathcal{X}$ and output distribution $\\mu_{i}\\in\\mathcal \u1e0a P \u1e0c _{2}$ ", "page_idx": 4}, {"type": "text", "text": "Parameter :loss D, estimate $\\bar{\\mathcal{G}}_{i}(\\mu)$ of the Wasserstein gradient $\\nabla_{W}\\operatorname{D}(\\mu\\mid\\mu_{i})$ , particle number $N$ , iteration $M$ , learning rate $\\nu$ , weak learner $f$ , initial constants $(\\hat{\\vartheta}_{0}^{1},\\ldots,\\vartheta_{0}^{N})$   \nOutput: set of $N$ boosting ensembles $(F_{M}^{1},\\cdot\\cdot\\cdot,F_{M}^{N})$ at final step $M$   \n$(F_{0}^{1}(\\cdot),\\cdot\\cdot\\cdot,F_{0}^{N}(\\cdot))\\gets(\\vartheta_{0}^{1},\\cdot\\cdot\\cdot,\\vartheta_{0}^{N})$ $\\vartriangleright$ set initial state of N boosting ensembles   \nfor $m\\gets0,\\dotsc,M-1$ do for $i\\gets1,\\ldots,D$ do $\\hat{\\mu}_{m,i}\\gets$ empirical distribution of output values $(F_{m}^{1}(x_{i}),\\dots,F_{m}^{N}(x_{i}))$ for input $x_{i}$ for $n\\leftarrow1,\\ldots,N$ do $\\begin{array}{r l}{|}&{{}g_{i}^{n}\\gets-\\left[\\mathcal{G}_{i}(\\hat{\\mu}_{m,i})\\right]\\left(F_{m}^{n}(x_{i})\\right)}\\end{array}$ \u25b7compute target value of $n$ -th new weak learner end end for $n\\leftarrow1,\\ldots,N$ do $f_{m+1}^{n}\\gets\\mathrm{fit}\\big(\\left\\{x_{i},g_{i}^{n}\\right\\}_{i=1}^{D}\\big)$ $\\vartriangleright$ fit $n$ -th new weak learner $F_{m+1}^{n}(\\cdot)\\gets F_{m}^{n}(\\cdot)+\\nu f_{m+1}^{n}(\\cdot)$ $\\vartriangleright$ set next state of $n$ -th boosting ensemble end ", "page_idx": 4}, {"type": "text", "text": "3 Application to Evidential Learning ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "This section provides our default setting to implement a concrete WGBoost algorithm for evidential learning, which enables classification and regression with predictive uncertainty. The individual-level posterior $p(\\theta\\mid y_{i})$ of a response distribution $p(y\\mid\\theta)$ is used as the output distribution $\\mu_{i}$ of the training set {xi, \u00b5i}iD=1 . Section 3.1 recaps derivation of the individual-level posterior $p(\\theta\\mid y_{i})$ , followed by Section 3.2 discussing the default choice of the prior. We choose the KL divergence as a loss functional of WGBoost. Section 3.3 recaps a widely-used estimate of the Wasserstein gradient of the KL divergence based on kernel smoothing [26]. A further advantage of the kernel smoothing estimate is that the approximate Wasserstein Hessian is available, with which Section 3.4 establishes a second-order WGBoost algorithm similarly to modern gradient-boosting libraries. ", "page_idx": 5}, {"type": "text", "text": "3.1 Derivation of Individual-Level Posteriors and Predictive Distribution ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Suppose that a response distribution $p(y\\mid\\theta)$ of a response variable $y$ is specified, as is typically done for probabilistic prediction. Suppose also that a prior distribution $p_{i}(\\theta)$ of the response parameter $\\theta$ is specified for each individual data input $x_{i}$ . For each individual data pair $(x_{i},y_{i})$ , the responsedistribution likelihood $p(y_{i}\\mid\\theta)$ and the prior $p_{i}(\\theta)$ determine the individual-level posterior ", "page_idx": 5}, {"type": "equation", "text": "$$\np(\\boldsymbol{\\theta}\\mid\\boldsymbol{y}_{i})\\propto p(\\boldsymbol{y}_{i}\\mid\\boldsymbol{\\theta})p_{i}(\\boldsymbol{\\theta})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "by Bayes\u2019 theorem. This individual-level posterior is set to the output distribution $\\mu_{i}$ of the training set $\\{\\bar{x_{i}},\\mu_{i}\\}_{i=1}^{D}$ of WGBoost. The framework of WGBoost then constructs a model that returns a particle approximation of the output distribution $\\mu_{i}(\\cdot)=p(\\cdot\\mid y_{i})$ for each data input $x_{i}$ . ", "page_idx": 5}, {"type": "text", "text": "For a new input $x$ , the constructed WGBoost model provides a set of particles $(\\theta^{1}(x),\\ldots,\\theta^{N}(x))$ as a distributional prediction $p(\\theta\\mid x)$ of the response parameter $\\theta$ . We can define a predictive distribution $p(y\\mid x)$ of the response $y$ for the new input $x$ via marginalisation of the output particles: ", "page_idx": 5}, {"type": "equation", "text": "$$\np(y\\mid x)=\\int_{\\Theta}p(y\\mid\\theta)p(\\theta\\mid x)d\\theta=\\frac{1}{N}\\sum_{i=1}^{N}p\\left(y\\mid\\theta^{i}(x)\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We can also define a point prediction $\\hat{y}$ for the new input $x$ via the individual-level Bayes action y\u02c6 = argminy\u2208Y  \u0398 U(y, \u03b8)p(\u03b8 | x)d\u03b8, which minimises the average of some error U : Y \u00d7 \u0398 \u2192R. For example, the Bayes action $\\hat{y}$ is simply the mean of the output particles if $U(y,\\theta)=(y-\\theta)^{2}$ . ", "page_idx": 5}, {"type": "text", "text": "In general, the explicit form of the individual-level posterior $p(\\theta\\mid y_{i})$ is known only up to the normalising constant. Our full algorithm in Section 3.4 requires no normalising constant of the individual-level posterior $p(\\theta\\mid y_{i})$ . Our algorithm depends only on the log-gradient of the individuallevel posterior $\\nabla\\log p(\\theta\\mid y_{i})$ that cancels any constant term by the gradient. Hence, knowing the form of the response-distribution likelihood $\\bar{p(y_{i}\\mid\\theta)}$ and the prior $p_{i}(\\theta)$ suffices. ", "page_idx": 5}, {"type": "text", "text": "Remark 3 (Difference from Bayesian Learning). Given a response distribution $p(y\\mid\\theta)$ and a model $\\theta=f(x,w)$ with the parameter $w$ , Bayesian learning of the model $f$ means the use of the posterior $p(\\stackrel{.}{w}|\\stackrel{.}{\\{x_{i},y_{i}\\}}_{i=1}^{D})$ over $w$ conditional on all data. The predictive distribution $p(y\\mid x)$ of the response $y$ is defined via marginalisation over $w$ : $\\textstyle\\int_{\\Theta}p(y\\mid\\theta=f(x,w))p(w\\mid\\{x_{i},y_{i}\\}_{i=1}^{D})d w$ . In contrast, WGBoost returns a distributional prediction $p(\\theta\\mid x)$ of the response parameter $\\theta$ , circumventing the marginalisation over the model parameter $w$ that can be ultra-high dimensional. ", "page_idx": 5}, {"type": "text", "text": "3.2 Choice of Individual-Level Priors ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The prior distribution $p_{i}(\\theta)$ of the response parameter $\\theta$ is specified at each individual data input $x_{i}$ . The approach to eliciting the prior may differ, depending on whether past data are available. If past data are available, they can be utilised to elicit a reasonable prior for unobserved data. If no past data are available, we recommend the use of a noninformative prior that have been developed as a sensible choice of prior in the absence of past data; see [e.g. 41] for the introduction. To avoid numerical errors, if a noninformative prior is improper (i.e. nonintegrable), we recommend the use of a proper probability distribution that approximates the noninformative prior sufficiently well. ", "page_idx": 5}, {"type": "text", "text": "Example 1 (Normal Location-Scale Response). Consider a scalar-valued response variable $y\\in\\mathbb R$ for regression. A normal location-scale response distribution $\\mathcal{N}(y\\mid m,\\sigma)$ has the mean and scale parameters $m\\,\\in\\,\\mathbb{R}$ and $\\sigma\\,\\in\\,(0,\\infty)$ . A typical noninformative prior of $m$ and $\\sigma$ are given by, respectively, 1 and $1/\\sigma$ which are improper. At every data point $(x_{i},y_{i})$ , we use a normal prior $\\mathcal{N}(m\\mid0,\\dot{\\sigma}_{0})$ over $m$ and an inverse gamma prior ${\\mathrm{IG}}(\\sigma\\mid\\alpha_{0},\\beta_{0})$ over $\\sigma$ , with the hyperparameters $\\sigma_{0}=10$ and $\\alpha_{0}=\\beta_{0}=0.01$ , which approximate the non-informative priors. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Example 2 (Categorical Response). Consider a label response variable $y\\in\\{1,\\ldots,k\\}$ for $k$ -class classification. A categorical response distribution $\\mathcal{C}(\\boldsymbol{y}\\mid\\boldsymbol{\\textbf{\\em q}})$ has the class probability parameter $q\\;=\\;(q_{1},\\ldots,q_{k})$ in the $k$ -dimensional simplex $\\Delta_{k}$ . If $k\\,=\\,2$ , it corresponds to the Bernoulli distribution. A typical noninformative prior of $q$ is given by $1/(q_{1}\\,\\times\\,\\cdot\\,\\cdot\\,\\times\\,q_{k})$ which are improper. At every data point $(x_{i},y_{i})$ , we use the logistic normal prior\u2014a multivariate generalisation of the logit normal distribution [42]\u2014over $q$ with the mean 0 and identity covariance matrix scaled by 10. ", "page_idx": 6}, {"type": "text", "text": "Remark 4 (Reparametrisation and Standardisation). Section 2 supposed $\\Theta\\,=\\,\\mathbb{R}^{d}$ for some dimension $d$ without no loss of generality. Any parameter that lies in a subset of the Euclidean space (e.g. $\\sigma$ in Example 1) can be reparametrised as one in the Euclidean space (e.g. $\\log\\sigma,$ ). Appendix D details the reparametrisation used for Examples 1 and 2. In addition, if one\u2019s dataset has scalar outputs of a low or high order of magnitude, we recommend standardising the outputs. ", "page_idx": 6}, {"type": "text", "text": "3.3 Approximate Wasserstein Gradient of KL Divergence ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We consider the KL divergence $\\operatorname{KL}(\\mu\\mid\\mu_{i})$ as a loss functional of WGBoost. One challenge of the KL divergence is that the resulting Wasserstein gradient $\\left[\\mathcal{G}_{i}^{\\mathrm{KL}}(\\mu)\\right](\\theta):=-\\left(\\nabla\\log\\mu_{i}(\\theta)-\\nabla\\log\\mu(\\theta)\\right)$ is not well-defined when $\\mu$ is an empirical distribution. A particularly successful solution\u2014which originates in [43] and has been applied in wide contexts [26, 44, 45]\u2014is to smooth the Wasserstein gradient through a kernel integral operator $\\begin{array}{r}{\\int_{\\Theta}[\\mathcal{G}_{i}^{\\mathrm{KL}}(\\mu)](\\bar{\\theta}^{*})k(\\theta,\\theta^{*})\\bar{d}\\mu(\\theta^{*})}\\end{array}$ [46]. By integration-bypart (see [e.g. 43]), the smoothed Wasserstein gradient, denoted $\\mathcal{G}_{i}^{*}(\\mu)$ , falls into the following form that is well-defined for any distribution $\\mu$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left[\\mathcal{G}_{i}^{*}(\\mu)\\right](\\theta):=-\\mathbb{E}_{\\theta^{*}\\sim\\mu}\\Big[\\nabla\\log\\mu_{i}(\\theta^{*})k(\\theta^{*},\\theta)+\\nabla k(\\theta^{*},\\theta)\\Big]\\in\\mathbb{R}^{d},}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\nabla k(\\theta^{*},\\theta)$ denotes the gradient of $k$ with respect to the first argument $\\theta^{*}$ . An approximate Wasserstein gradient flow based on the smoothed Wasserstein gradient $\\mathcal{G}_{i}^{*}(\\mu)$ is called the Stein variational gradient descent [43] or kernelised Wasserstein gradient flow [47]. In most cases, the kernel $k$ is set to the Gaussian kernel $k(\\theta,\\theta^{*})=\\exp(-\\|\\theta-\\theta^{*}\\|^{2}/h)$ with the scale $h>0$ . Appendix B discusses a choice of kernel. This work uses the Gaussian kernel with $h=0.1$ throughout. ", "page_idx": 6}, {"type": "text", "text": "Another common approach to approximating the Wasserstein gradient flow of the KL divergence is the Langevin diffusion approach [48]. The discretised algorithm, called the unadjusted Langevin algorithm [49], is a stochastic particle update scheme that adds a Gaussian noise at every iteration. However, several known challenges, such as asymptotic bias and slow convergence, often necessitate an ad-hoc adjustment of the algorithm [48]. Appendix B discusses a variant of WGBoost built on the Langevin algorithm, although it is not considered the default implementation. ", "page_idx": 6}, {"type": "text", "text": "3.4 Second-Order Implementation of WGBoost ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We use a diagonal (coordinatewise) approximate Wasserstein Newton direction of the $\\mathrm{KL}$ divergence, following the standard practice in modern gradient-boosting libraries [21, 22] to use the diagonal Newton direction of a loss. Similarly to smoothed Wasserstein gradient $\\mathcal{G}_{i}^{*}(\\mu)$ , the approximate Wasserstein Hessian of the KL divergence $\\operatorname{KL}(\\mu\\mid\\mu_{i})$ can be obtained through the kernel smoothing. The diagonal of the approximate Wasserstein Hessian, denoted $\\mathcal{H}_{i}^{*}(\\mu)$ , is defined by ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left[\\mathcal{H}_{i}^{*}(\\mu)\\right](\\theta):=\\mathbb{E}_{\\theta^{*}\\sim\\mu}\\Big[-\\nabla_{\\mathrm{d}}^{2}\\log\\mu_{i}(\\theta^{*})k(\\theta,\\theta^{*})^{2}+\\nabla k(\\theta,\\theta^{*})\\odot\\nabla k(\\theta,\\theta^{*})\\Big]\\in\\mathbb{R}^{d}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The diagonal approximate Wasserstein Newton direction of the KL divergence is then defined by $-\\left[\\mathcal{G}_{i}^{\\ast}(\\bar{\\mu_{}})\\right](\\cdot)\\bar{\\bigotimes}\\left[\\mathcal{H}_{i}^{\\ast}(\\mu)\\right](\\cdot)$ . Appendix C provides the derivation based on [39] who derived the Newton direction of the KL divergence in the context of nonparametric variational inference. ", "page_idx": 6}, {"type": "text", "text": "The second-order WGBoost algorithm is established by plugging it into $\\mathcal{G}_{i}(\\mu)$ in Algorithm 1, that is, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\left[\\mathcal{G}_{i}(\\mu)\\right](\\cdot)=\\left[\\mathcal{G}_{i}^{\\ast}(\\mu)\\right](\\cdot)\\oslash\\left[\\mathcal{H}_{i}^{\\ast}(\\mu)\\right](\\cdot).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Algorithm 1 under the choice (9) is considered our default WGBoost algorithm for evidential learning. We refer this algorithm to as the Wasserstein-boosted evidential learning (WEvidential). The explicit pseudocode is provided in Algorithm 2 for full clarity. ", "page_idx": 6}, {"type": "text", "text": "Algorithm 2: Wasserstein-Boosted Evidential Learning ", "page_idx": 7}, {"type": "image", "img_path": "cuO0DenqMl/tmp/32d59b70cc7aaff07f0f26cd906e1203f6715e0ac1dd655f4a94a618df26ad12.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Remark 5 (Computation). The diagonal Newton direction (9) has the computational complexity $O(N\\!\\times\\!d)$ same as that of the smoothed Wasserstein gradient. Hence, there is essentially no reason not to use the diagonal Newton direction (9) instead of the smoothed Wasserstein gradient. Although it is possible to use the full Newton direction with no diagonal approximation, the computation requires the inverse and product of $(N\\times d)\\times(N\\times d)$ matrices that result in the complexity up to $O(N^{3}\\times d^{3})$ . Appendix D presents a simulation study to compare computational time and convergence speed of four WGBoost algorithms built on different estimates of the Wasserstein gradient. ", "page_idx": 7}, {"type": "text", "text": "4 Experiment on Real-world Tabular Data ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We empirically demonstrate the performance of the WGBoost algorithm through three experiments using real-world tabular data. The first application illustrates the output of WGBoost through a simple conditional density estimation. The second application benchmarks the probabilistic regression performance. The third application demonstrates the classification and OOD detection performance. The source code is available in https://github.com/takuomatsubara/WGBoost. ", "page_idx": 7}, {"type": "text", "text": "Common Hyperparameters Throughout, we set the number of output particles $N$ to 10 and set each weak learner $f$ to the decision tree regressor [50] with maximum depth 1 for Section 4.1 and 3 for the rest. We set the learning rate $\\nu$ to 0.1 for regression and 0.4 for classification, unless otherwise stated. Appendix E contains further details, including a choice of the initial constant $\\{\\vartheta_{0}^{n}\\}_{n=1}^{N}$ . ", "page_idx": 7}, {"type": "text", "text": "4.1 Illustrative Conditional Density Estimation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We illustrate the output of WEvidential by estimating a conditional density $p(y\\mid x)$ from onedimensional scalar inputs and outputs $\\{x_{i},\\bar{y}_{i}\\}_{i=1}^{D}$ . The normal output distribution $\\mathcal{N}(y\\mid m,\\sigma)$ and the prior $p_{i}(m,\\sigma)$ in Example 1 were used to define the individual-level posterior $p(m,\\sigma\\mid y_{i})$ , in which case the output of the WGBoost algorithm is a set of 10 particles $\\{(m^{n}(x),\\sigma^{n}(\\vec{x}))\\}_{n=1}^{10}$ of the mean and scale parameters for each input $x$ . We chose the number of weak learners $M$ , drawing on an early-stopping approach used in [32], where we held out $20\\%$ of the training set as a validation set and chose the number $1\\leq M\\leq4000$ achieving the least validation error. Once the number $M$ was chosen, WEvidential was trained again using all the entire training set. ", "page_idx": 7}, {"type": "text", "text": "The conditional density is estimated using the predictive distribution (6) by WEvidential. We used two real-world datasets, bone mineral density [51] and old faithful geyser [52]. Figure 3 depicts the result for the former dataset, demonstrating that the WGBoost algorithm captures the heterogeneity of the conditional density on each input well. The result for the latter dataset is contained in Appendix E.1. ", "page_idx": 7}, {"type": "image", "img_path": "cuO0DenqMl/tmp/51f26457d971616da12245515218adf33147da9fddf7c9fe0564914e328d90c6.jpg", "img_caption": ["Figure 3: Conditional density estimation for the bone mineral density dataset (grey dots) by WEvidential, where the normal response distribution $\\mathcal{N}(y\\mid m,\\sigma)$ is used for the response variable $y$ . Left: distributional estimate (10 particles) of the location parameter $\\{m^{n}(x)\\}_{n=1}^{10}$ for each input. Right: estimated conditional density (6) through marginalisation of the output particles $\\{(m^{n}(x),\\bar{,}\\sigma^{n}(x))\\}_{n=1}^{10}$ . "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.2 Probabilistic Regression Benchmark ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We examine the regression performance of WEvidential using a standard benchmark protocol that originated in [53] and has been used in a number of subsequent works [10, 9, 32]. The benchmark protocol uses real-world tabular datasets from the UCI machine learning repository [54], each with one-dimensional scalar responses. As in Section 4.1, the normal response distribution $\\mathcal{N}(y\\mid m,\\sigma)$ and the prior $p_{i}(m,\\sigma)$ in Example 1 were used to define the individual-level posterior $p(m,\\sigma\\mid y_{i})$ . ", "page_idx": 8}, {"type": "text", "text": "We followed the data splitting protocol in [53] and randomly held out $10\\%$ of each dataset as a test set. The negative log likelihood (NLL) is measured by using the predictive distribution (6). The root mean squared error (RMSE) is measured by using the point prediction by the mean value of the predictive distribution. We chose the number of weak learners $M$ by the same approach as in Section 4.1. We repeated this procedure 20 times for each dataset, except the protein and year msd datasets for which we repeated five times and once. For the year msd dataset only, we subsampled $10\\%$ of data to fit each weak learner and used the learning rate 0.01 due to the large dataset size. ", "page_idx": 8}, {"type": "text", "text": "Table 1 compares the performance of WEvidential with five other methods: Monte Carlo Dropout (MCDropout) [9], Deep Ensemble (DEnsemble) [10], Concrete Dropout (CDropout) [55], Natural Gradient Boosting (NGBoost) [32], and Deep Evidential Regression (DEvidential) [13]. Appendix E provides further details on the experiment and a limited yet additional comparison. The WGBoost algorithm achieves the best score or a score sufficiently close to the best score most often. ", "page_idx": 8}, {"type": "table", "img_path": "cuO0DenqMl/tmp/42fccde29c210b74b6a7e5b53d233687ba5e3b3eaff098814f918d92a5ce8c1b.jpg", "table_caption": ["Table 1: The NLLs and RMSEs for each dataset, where the best score is underlined and the scores whose standard deviation ranges include the best score are in bold. Results of MCDropout, DEnsembles, CDropout, NGBoost, and DEvidential were reported in [9], [10], [55], [32] and [13] respectively. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "cuO0DenqMl/tmp/f44e2bc534a2aa1b65d043a16203acc6d846efabd7f877ca8c6460b5cfef11bc.jpg", "table_caption": ["Table 2: The classification accuracies and OOD detection PR-AUCs for each dataset, where the best score is underlined and in bold. The results other than WEvidential were reported in [14]. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "4.3 Classification and Out-of-Distribution Detection ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We examine the classification and anomaly OOD detection performance of WEvidential on two realworld tabular datasets, segment and sensorless, following the protocol used in [14]. The categorical response distribution $\\mathcal{C}(y\\mid q)$ and the prior $p_{i}(q)$ in Example 2 were used to define the individuallevel posterior $p(q\\mid y_{i})$ , in which case the output of the WGBoost algorithm is a set of 10 particles $\\{q^{n}\\}_{n=1}^{10}$ of the class probability parameter $q$ in the simplex $\\Delta^{k}$ for each input $x$ . We set the number of weak learners to 4000 without early stopping to reduce the computational cost. ", "page_idx": 9}, {"type": "text", "text": "The segment and sensorless datasets have 7 and 11 classes in total. For the segment dataset, the data subset that belongs to the last class was kept as the OOD samples. For the sensorless dataset, the data subset that belongs to the last two classes was kept as the OOD samples. For each dataset, $20\\%$ of the non-OOD samples is held out as a test set to measure the classification accuracy. There exist several ways of defining a OOD score for each input [56]. For the WGBoost algorithm, the inverse of the maximum norm of the output-particle variance was used as the OOD score. We measured the OOD detection performance by the area under the precision recall curve (PR-AUC), viewing non-OOD test data as the positive class and OOD data as the negative class. We repeated this procedure five times. ", "page_idx": 9}, {"type": "text", "text": "Table 2 compares the performance of WEvidential with four other methods: MCDropout, DEnsemble, and Distributional Distillation (DDistillation) [57], and Posterior Network (PNetwork) [14]. Appendix E provides further details on the experiment. Figure 4 exemplifies how the dispersion of the output particles differ between OOD and non-OOD inputs. WEvidential demonstrates a high classification and OOD detection accuracy simultaneously. Although PNetwork has the best OOD detection performance for the sensorless dataset, the performance of the WGBoost algorithm also exceeds $80\\%$ , which is distinct from MCDropout, DEnsemble, and DDistillation. ", "page_idx": 9}, {"type": "image", "img_path": "cuO0DenqMl/tmp/81489a12fa70c910c0d3a0c597d0989713e72fca54e37675933fb40c6f2b4e6e.jpg", "img_caption": ["Figure 4: Examples of the output particles (red dot) of WEvidential on the segment dataset, where the coloured area indicate the kernel density estimation of the output particles for each class. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "5 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work established the general framework of WGBoost and developed the concrete algorithm WEvidential for evidential learning. The established framework of WGBoost offers exciting avenues for future research. Important directions for future study include (i) exploring alternative loss functionals to the KL divergence, (ii) investigating the convergence properties, and (iii) evaluating robustness of obtained predictive uncertainty in comparison to other methods. A particular limitation of WGBoost may arise when data are not tabular, as is the case of standard gradient boosting. These questions require careful examination and are critical for future study. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Ravid Shwartz-Ziv and Amitai Armon. Tabular data: Deep learning is not all you need. Information Fusion, 81:84\u201390, 2022.   \n[2] Moloud Abdar, Farhad Pourpanah, Sadiq Hussain, Dana Rezazadegan, Li Liu, Mohammad Ghavamzadeh, Paul Fieguth, Xiaochun Cao, Abbas Khosravi, U. Rajendra Acharya, Vladimir Makarenkov, and Saeid Nahavandi. A review of uncertainty quantification in deep learning: Techniques, applications and challenges. Information Fusion, 76:243\u2013297, 2021.   \n[3] Eric Topol. High-performance medicine: the convergence of human and artificial intelligence. Nature Medicine, 25:44\u201356, 2019.   \n[4] Sorin Grigorescu, Bogdan Trasnea, Tiberiu Cocias, and Gigel Macesanu. A survey of deep learning techniques for autonomous driving. Journal of Field Robotics, 37(3):362\u2013386, 2020.   \n[5] Matthew Richardson, Ewa Dominowska, and Robert Ragno. Predicting clicks: estimating the click-through rate for new ads. In Proceedings of the 16th International Conference on World Wide Web, page 521\u2013530, 2007.   \n[6] Christopher Burges. From ranknet to lambdarank to lambdamart: An overview. Learning, 11, 2010.   \n[7] Byron P. Roe, Hai-Jun Yang, Ji Zhu, Yong Liu, Ion Stancu, and Gordon McGregor. Boosted decision trees as an alternative to artificial neural networks for particle identification. Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment, 543(2):577\u2013584, 2005.   \n[8] James Bennett and Stan Lanning. The Netfilx prize. In Proceedings of the KDD Cup Workshop 2007, pages 3\u20136, 2007.   \n[9] Yarin Gal and Zoubin Ghahramani. Dropout as a Bayesian approximation: Representing model uncertainty in deep learning. In Proceedings of The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research, pages 1050\u20131059. PMLR, 2016.   \n[10] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. In Advances in Neural Information Processing Systems, volume 30, 2017.   \n[11] Murat Sensoy, Lance Kaplan, and Melih Kandemir. Evidential deep learning to quantify classification uncertainty. In Advances in Neural Information Processing Systems, volume 31, 2018.   \n[12] Jakob Gawlikowski, Cedrique Rovile Njieutcheu Tassi, Mohsin Ali, Jongseok Lee, Matthias Humt, Jianxiang Feng, Anna Kruspe, Rudolph Triebel, Peter Jung, Ribana Roscher, Muhammad Shahzad, Wen Yang, Richard Bamler, and Xiao Xiang Zhu. A survey of uncertainty in deep neural networks. Artificial Intelligence Review, 56:1513\u20131589, 2023.   \n[13] Alexander Amini, Wilko Schwarting, Ava Soleimany, and Daniela Rus. Deep evidential regression. In Advances in Neural Information Processing Systems, volume 33, pages 14927\u2013 14937, 2020.   \n[14] Bertrand Charpentier, Daniel Z\u00fcgner, and Stephan G\u00fcnnemann. Posterior network: Uncertainty estimation without OOD samples via density-based pseudo-counts. In Advances in Neural Information Processing Systems, volume 33, pages 1356\u20131367. Curran Associates, Inc., 2020.   \n[15] Dennis Thomas Ulmer, Christian Hardmeier, and Jes Frellsen. Prior and posterior networks: A survey on evidential deep learning methods for uncertainty estimation. Transactions on Machine Learning Research, 2023.   \n[16] Edouard Capellier, Franck Davoine, Veronique Cherfaoui, and You Li. Evidential deep learning for arbitrary lidar object classification in the context of autonomous driving. In 2019 IEEE Intelligent Vehicles Symposium (IV), pages 1304\u20131311, 2019.   \n[17] Patrick Hemmer, Niklas K\u00fchl, and Jakob Sch\u00f6ffer. Deal: Deep evidential active learning for image classification. In 2020 19th IEEE International Conference on Machine Learning and Applications (ICMLA), pages 865\u2013870, 2020.   \n[18] Ava P. Soleimany, Alexander Amini, Samuel Goldman, Daniela Rus, Sangeeta N. Bhatia, and Connor W. Coley. Evidential deep learning for guided molecular property prediction and discovery. ACS Central Science, 7(8):1356\u20131367, 2021.   \n[19] Jakob Gawlikowski, Sudipan Saha, Anna Kruspe, and Xiao Xiang Zhu. An advanced Dirichlet prior network for out-of-distribution detection in remote sensing. IEEE Transactions on Geoscience and Remote Sensing, 60:1\u201319, 2022.   \n[20] Andrew Gelman, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B. Rubin. Bayesian Data Analysis. Chapman and Hall/CRC, 3rd ed. edition, 2013.   \n[21] Tianqi Chen and Carlos Guestrin. XGBoost: A scalable tree boosting system. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201916, page 785\u2013794, 2016.   \n[22] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu. LightGBM: A highly efficient gradient boosting decision tree. In Advances in Neural Information Processing Systems, volume 30, 2017.   \n[23] C\u2019edric Villani. Topics in Optimal Transportation. Americal Mathematical Society, 2003.   \n[24] Luigi Ambrosio, Nicola Gigli, and Giuseppe Savar\u00e9. Gradient Flows In Metric Spaces and in the Space of Probability Measures. Birkh\u00e4user Basel, 2005.   \n[25] Filippo Santambrogio. {Euclidean, metric, and Wasserstein} gradient flows: an overview. Bulletin of Mathematical Sciences, 7:87\u2013154, 2017.   \n[26] Qiang Liu. Stein variational gradient descent as gradient flow. In Advances in Neural Information Processing Systems, volume 30, 2017.   \n[27] Jos\u2019e Antonio Carrillo, Katy Craig, and Francesco S. Patacchini. A blob method for diffusion. Calculus of Variations and Partial Differential Equations, 58(53), 2019.   \n[28] Yifei Wang, Peng Chen, and Wuchen Li. Projected Wasserstein gradient descent for highdimensional Bayesian inference. SIAM/ASA Journal on Uncertainty Quantification, 10(4):1513\u2013 1532, 2022.   \n[29] Dimitra Maoutsa, Sebastian Reich, and Manfred Opper. Interacting particle solutions of Fokker-Planck equations through gradient-log-density estimation. Entropy (Basel), 22(8):802, 2020.   \n[30] Ye He, Krishnakumar Balasubramanian, Bharath K. Sriperumbudur, and Jianfeng Lu. Regularized Stein variational gradient flow. arXiv:2211.07861, 2022.   \n[31] Jerome H. Friedman. Greedy function approximation: A gradient boosting machine. The Annals of Statistics, 29(5):1189\u20131232, 2001.   \n[32] Tony Duan, Avati Anand, Daisy Yi Ding, Khanh K. Thai, Sanjay Basu, Andrew Ng, and Alejandro Schuler. NGBoost: Natural gradient boosting for probabilistic prediction. In Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 2690\u20132700. PMLR, 2020.   \n[33] Peter B\u00fchlmann and Torsten Hothorn. Boosting Algorithms: Regularization, Prediction and Model Fitting. Statistical Science, 22(4):477 \u2013 505, 2007.   \n[34] Leo Grinsztajn, Edouard Oyallon, and Gael Varoquaux. Why do tree-based models still outperform deep learning on typical tabular data? In Advances in Neural Information Processing Systems, volume 35, pages 507\u2013520, 2022.   \n[35] Piotr Florek and Adam Zagdan\u00b4ski. Benchmarking state-of-the-art gradient boosting algorithms for classification. arXiv:2305.17094, 2023.   \n[36] Zhendong Zhang and Cheolkon Jung. GBDT-MO: Gradient-boosted decision trees for multiple outputs. IEEE Transactions on Neural Networks and Learning Systems, 32(7):3156\u20133167, 2021.   \n[37] Tianqi Chen, Sameer Singh, Ben Taskar, and Carlos Guestrin. Efficient Second-Order Gradient Boosting for Conditional Random Fields. In Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics, volume 38 of Proceedings of Machine Learning Research, pages 147\u2013155. PMLR, 2015.   \n[38] Jerome H. Friedman. Stochastic gradient boosting. Computational Statistics & Data Analysis, 38(4):367\u2013378, 2002.   \n[39] Gianluca Detommaso, Tiangang Cui, Youssef Marzouk, Alessio Spantini, and Robert Scheichl. A Stein variational Newton method. In Advances in Neural Information Processing Systems, volume 31, 2018.   \n[40] Yifei Wang and Wuchen Li. Information Newton\u2019s flow: second-order optimization method in probability space. arXiv:2001.04341, 2020.   \n[41] Malay Ghosh. Objective priors: An introduction for frequentists. Statistical Science, 26(2):187\u2013 202, 2011.   \n[42] J. Aitchison and S. M. Shen. Logistic-normal distributions: Some properties and uses. Biometrika, 67(2):261\u2013272, 1980.   \n[43] Qiang Liu and Dilin Wang. Stein variational gradient descent: A general purpose Bayesian inference algorithm. In Advances in Neural Information Processing Systems, volume 29, 2016.   \n[44] Dilin Wang, Zhe Zeng, and Qiang Liu. Stein variational message passing for continuous graphical models. In Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 5219\u20135227. PMLR, 2018.   \n[45] Alexander Lambert, Fabio Ramos, Byron Boots, Dieter Fox, and Adam Fishman. Stein variational model predictive control. In Proceedings of the 2020 Conference on Robot Learning, volume 155 of Proceedings of Machine Learning Research, pages 1278\u20131297. PMLR, 2021.   \n[46] Anna Korba, Adil Salim, Michael Arbel, Giulia Luise, and Arthur Gretton. A non-asymptotic analysis for Stein variational gradient descent. In Advances in Neural Information Processing Systems, volume 33, pages 4672\u20134682, 2020.   \n[47] Sinho Chewi, Thibaut Le Gouic, Chen Lu, Tyler Maunu, and Philippe Rigollet. SVGD as a kernelized Wasserstein gradient flow of the chi-squared divergence. In Advances in Neural Information Processing Systems, volume 33, pages 2098\u20132109, 2020.   \n[48] Andre Wibisono. Sampling as optimization in the space of measures: The Langevin dynamics as a composite optimization problem. In Proceedings of the 31st Conference On Learning Theory, volume 75 of Proceedings of Machine Learning Research, pages 2093\u20133027. PMLR, 2018.   \n[49] Gareth O. Roberts and Richard L. Tweedie. Exponential convergence of Langevin distributions and their discrete approximations. Bernoulli, 2(4):341 \u2013 363, 1996.   \n[50] Leo Breiman, Jerome Friedman, R.A. Olshen, and Charles J. Stone. Classification and Regression Trees. Chapman and Hall/CRC, 1984.   \n[51] Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of Statistical Learning. Springer New York, 2009.   \n[52] Sanford Weisberg. Applied Linear Regression. John Wiley & Sons, 1985.   \n[53] Jose Miguel Hernandez-Lobato and Ryan Adams. Probabilistic backpropagation for scalable learning of Bayesian neural networks. In Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pages 1861\u20131869. PMLR, 2015. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "[54] Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. ", "page_idx": 13}, {"type": "text", "text": "[55] Yarin Gal, Jiri Hron, and Alex Kendall. Concrete dropout. In Advances in Neural Information Processing Systems, volume 30, 2017.   \n[56] Jingkang Yang, Kaiyang Zhou, Yixuan Li, and Ziwei Liu. Generalized out-of-distribution detection: A survey. arXiv:2110.11334, 2024.   \n[57] Andrey Malinin, Bruno Mlodozeniec, and Mark Gales. Ensemble distribution distillation. In International Conference on Learning Representations, 2020.   \n[58] Filippo Santambrogio. Optimal Transport for Applied Mathematicians. Birkh\u00e4user Cham, 2015.   \n[59] Mingxuan Yi and Song Liu. Bridging the gap between variational inference and Wasserstein gradient flows, 2023.   \n[60] Michael Arbel, Anna Korba, Adil Salim, and Arthur Gretton. Maximum mean discrepancy gradient flow. In Advances in Neural Information Processing Systems, volume 32, 2019.   \n[61] Richard Jordan, David Kinderlehrer, and Felix Otto. The variational formulation of the Fokker\u2013 Planck equation. SIAM Journal on Mathematical Analysis, 29(1):1\u201317, 1998.   \n[62] Grigorios A. Pavliotis. Stochastic Processes and Applications. Springer New York, 2014.   \n[63] Vern I. Paulsen and Mrinal Raghupathi. An Introduction to the Theory of Reproducing Kernel Hilbert Spaces. Cambridge University Press, 2016.   \n[64] Alex Leviyev, Joshua Chen, Yifei Wang, Omar Ghattas, and Aaron Zimmerman. A stochastic Stein variational Newton method. arXiv:2204.09039, 2022.   \n[65] Alex Smola, Arthur Gretton, Le Song, and Bernhard Sch\u00f6lkopf. A Hilbert space embedding for distributions.   \n[66] Shun ichi Amari. Information Geometry and Its Applications. Springer Tokyo, 2016.   \n[67] Pavel Izmailov, Wesley J. Maddox, Polina Kirichenko, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wilson. Subspace inference for bayesian deep learning. In Proceedings of 35th Conference on Uncertainty in Artificial Intelligence, 2019. ", "page_idx": 13}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "This appendix contains the technical and experiment details referred to in the main text. Appendix A recaps derivation of the Wasserstein gradient. Appendix B discusses the variant of WGBoost built on the unadjusted Langevin algorithm. Appendix C derives the diagonal approximate Wasserstein Newton direction used for WEvidential. Appendix D provides simulation studies on kernel choice of WEvidential and comparison of WGBoost algorithms built on different estimate of the Wasserstein gradient. Appendix E contains the additional details of the experiment presented in the main text. ", "page_idx": 14}, {"type": "text", "text": "A Derivation and Example of Wasserstein Gradient ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "This section recaps derivation of the Wasserstein gradient of a functional $\\mathcal{F}$ , with examples of common divergences. The Wasserstein gradient depends on a function on $\\Theta$ called the first variation [24]. The first variation $\\delta\\mathcal{F}(\\mu)/\\delta\\mu$ of the functional $\\mathcal{F}$ at $\\mu$ is a function on $\\Theta$ that satisfies ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\epsilon\\to0^{+}}\\frac{\\mathcal{F}(\\mu+\\epsilon\\nu)-\\mathcal{F}(\\mu)}{\\epsilon}=\\int_{\\Theta}\\frac{\\delta\\mathcal{F}(\\mu)}{\\delta\\mu}(\\theta)\\nu(\\theta)d\\theta\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "for all signed measure $\\nu$ s.t. $\\mu+\\epsilon\\nu\\in\\mathcal P_{2}$ for all $\\epsilon$ sufficiently small. The Wasserstein gradient $\\nabla_{W}\\mathcal{F}(\\mu)$ of the functional $\\mathcal{F}$ at $\\mu$ is derived as the gradient of the first variation (see [e.g. 24]): ", "page_idx": 14}, {"type": "equation", "text": "$$\n[\\nabla_{W}\\mathcal{F}(\\mu)](\\theta):=\\nabla\\frac{\\delta\\mathcal{F}(\\mu)}{\\delta\\mu}(\\theta).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "It is common to suppose that the functional $\\mathcal{F}$ consists of three energies, which are determined by functions $U:\\mathbb{R}\\rightarrow\\mathbb{R}$ , $V:\\Theta\\to\\mathbb{R}$ , and $W:\\Theta\\to\\mathbb{R}$ respectively, such that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal F(\\mu)=\\underbrace{\\int_{\\Theta}U(\\mu(\\theta))d\\theta}_{\\mathrm{internal~energy}}+\\underbrace{\\int_{\\Theta}V(\\theta)\\mu(\\theta)d\\theta}_{\\mathrm{potential~energy}}+\\underbrace{\\frac{1}{2}\\int_{\\Theta\\times\\Theta}W(\\theta-\\theta^{\\prime})\\mu(\\theta)d\\theta\\mu(\\theta^{\\prime})d\\theta^{\\prime}}_{\\mathrm{interaction~energy}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "For a functional $\\mathcal{F}$ that falls into the above form, the Wasserstein gradient is derived as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left[\\nabla_{W}\\mathcal{F}(\\mu)\\right](\\theta)=\\nabla U^{\\prime}(\\mu(\\theta))+\\nabla V(\\theta)+\\int_{\\Theta}\\nabla W(\\theta-\\theta^{\\prime})\\mu(\\theta^{\\prime})d\\theta^{\\prime}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $U^{\\prime}$ is the derivative of $U\\,:\\,\\mathbb{R}\\,\\rightarrow\\,\\mathbb{R}$ [23]. The $\\mathrm{KL}$ divergence ${\\mathcal{F}}(\\mu)\\,=\\,\\mathrm{KL}(\\mu\\,\\mid\\,\\pi)$ of a distribution $\\pi$ falls into the form with $U(x)=x\\log x,V(\\theta)=-\\log\\pi(\\theta)$ , and $W(\\theta)=0$ , where ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{KL}(\\mu\\mid\\pi)=\\int_{\\Theta}\\log\\mu(\\theta)\\mu(\\theta)d\\theta+\\int_{\\Theta}-\\log\\pi(\\theta)\\mu(\\theta)d\\theta.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Table 3 presents examples of Wasserstein gradients of common divergences ${\\mathcal{F}}(\\mu)=\\operatorname{D}(\\mu\\mid\\pi)$ ", "page_idx": 14}, {"type": "text", "text": "In the context of Bayesian inference, the KL divergence is particularly useful among many divergences. The Wasserstein gradient of the KL divergence requires no normalising constant of a posterior distribution $\\pi$ . This is because the Wasserstien gradient depends only on the log-gradient of the posterior $\\nabla\\log\\pi({\\boldsymbol{\\theta}})=\\nabla\\pi({\\boldsymbol{\\theta}})/\\pi({\\boldsymbol{\\theta}})$ of the target $\\pi$ , in which case the normalising constant of the target $\\pi$ is cancelled out by fraction. Hence, any posterior known only up to the normalising constant can be used as the target distribution $\\pi$ in the Wasserstein gradient of the KL divergence. ", "page_idx": 14}, {"type": "table", "img_path": "cuO0DenqMl/tmp/f53ffcab89a66032099cc1bc6f5a9f0a6cfa4cb1af612a7de9b3625d702dcd20.jpg", "table_caption": ["Table 3: Wasserstein gradients of four divergences: the KL divergence [58], the chi-squared divergence [47], the alpha divergence [59], and the maximum mean discrepancy [60]. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "B Langevin Gradient Boosting for KL Divergence ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "If a chosen functional $\\mathcal{F}$ on $\\mathcal{P}_{2}$ is the KL divergence ${\\mathcal{F}}(\\mu)=\\operatorname{KL}(\\mu\\mid\\pi)$ of a target distribution $\\pi$ , the continuity equation (1) admits an equivalent representation as the Fokker-Planck equation [61]: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{d}{d t}\\mu_{t}=\\nabla\\cdot\\left(\\mu_{t}\\nabla\\log\\pi\\right)+\\Delta\\mu_{t}\\quad\\mathrm{given}\\quad\\mu_{0}\\in\\mathcal P_{2}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\Delta$ denotes the Laplacian operator. Recall that the original continuity equation (1) can be reformulated as the deterministic differential equation (2) of a random variable $\\theta_{t}\\sim\\mu_{t}$ . In contrast, the Fokker-Planck equation (10) can be reformulated as a stochastic differential equation of a random variable $\\theta_{t}\\sim\\mu_{t}$ , known as the overdamped Langevin dynamics [62]: ", "page_idx": 15}, {"type": "equation", "text": "$$\nd\\theta_{t}=\\nabla\\log\\pi(\\theta_{t})d t+\\sqrt{2}d B_{t}\\quad\\mathrm{given}\\quad\\theta_{0}\\sim\\mu_{0},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $B_{t}$ denotes a standard Brownian motion. Note that the deterministic system (2) in the case of the KL divergence and the above stochastic system (11) are equivalent at population level, in a sense that the law of the random variable $\\theta_{t}$ in both the systems solves the two equivalent equations. ", "page_idx": 15}, {"type": "text", "text": "At the algorithmic level, however, discretisation of each system leads to different particle update schemes. Set the initial distribution $\\mu_{0}$ in (11) to the empirical distribution $\\hat{\\mu}_{0}$ of $N$ initial particles $\\{\\theta_{0}^{n}\\}_{n=1}^{N}$ . Discretising the stochastic system (11) by the Euler-Maruyama method with a step size $\\nu>0$ yields a stochastic update scheme of particles $\\lbrace\\theta_{m}^{n}\\rbrace_{n=1}^{N}$ from step : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left[\\theta_{m+1}^{1}\\right]=\\left[\\theta_{m}^{1}\\right]+\\nu\\left[\\nabla\\log\\pi(\\theta_{m}^{1})+\\sqrt{2/\\nu}\\,\\xi^{1}\\right]}\\\\ {\\vdots\\quad\\quad\\quad\\left[\\theta_{m}^{N}\\right]+\\nu\\left[\\underbrace{\\nabla\\log\\pi(\\theta_{m}^{N})}_{\\nabla\\log\\pi(\\theta_{m}^{N})}+\\sqrt{2/\\nu}\\,\\xi^{N}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where each $\\xi^{n}$ denotes a realisation from a standard normal distribution on $\\mathbb{R}^{d}$ . The above updating scheme of each $n$ -th particle is known as the unadjusted Langevin algorithm [49]. We can define a variant of WGBoost by replacing the term $\\mathcal G_{i}(\\mu)$ in Algorithm 1 with $\\nabla\\log\\mu_{i}(\\cdot)+\\sqrt{2/\\nu}\\,\\xi_{i}$ where $\\mu_{i}$ is an output distribution at each $x_{i}$ and $\\xi_{i}$ is a realisation from a standard normal distribution. The procedure is summarised in Algorithm 3, which we call Langevin gradient boosting (LGBoost). ", "page_idx": 15}, {"type": "text", "text": "Algorithm 3: Langevin Gradient Boosting ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Input: training set $\\{x_{i},\\mu_{i}\\}_{i=1}^{D}$ of input $x_{i}\\in\\mathcal{X}$ and output distribution $\\mu_{i}\\in\\mathcal \u1e0a P \u1e0c _{2}$ $N$ $M$ $\\nu$ $f$ ", "page_idx": 15}, {"type": "text", "text": "Parameter :particle number , iteration , rate , weak learner , initial constants ", "page_idx": 15}, {"type": "text", "text": "$\\left(\\vartheta_{0}^{1},\\ldots,\\vartheta_{0}^{N}\\right)$   \nOutput: set of $N$ boosting ensembles $(F_{M}^{1},\\cdot\\cdot\\cdot,F_{M}^{N})$ at final step $M$   \n$(F_{0}^{1}(\\cdot),\\cdot\\cdot\\cdot,F_{0}^{N}(\\cdot))\\gets(\\vartheta_{0}^{1},\\cdot\\cdot\\cdot,\\vartheta_{0}^{N})$   \nfor $m\\gets0,\\dotsc,M-1$ do for $n\\leftarrow1,\\ldots,N$ do for $i\\gets1,\\ldots,D$ do $g_{i}^{n}\\leftarrow\\nabla\\log\\mu_{i}(F_{m}^{n}(x_{i}))+\\sqrt{2/\\nu}\\,\\xi_{i}^{n}\\quad\\mathrm{where}\\quad\\xi_{i}^{n}\\sim\\mathcal{N}(0,I_{d})$ end $f_{m+1}^{n}\\gets\\mathrm{fit}\\left(\\left\\{x_{i},g_{i}^{n}\\right\\}_{i=1}^{D}\\right)$ $F_{m+1}^{n}(\\cdot)\\gets F_{m}^{n}(\\cdot)+\\nu f_{m+1}^{\\acute{n}}(\\cdot)$ end ", "page_idx": 15}, {"type": "text", "text": "end ", "page_idx": 15}, {"type": "text", "text": "C Derivation of Approximate Wasserstein Newton Direction ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "This section derives the diagonal approximate Wasserstein Newton direction based on the kernel smoothing. The approximate Wasserstein Newton direction of the KL divergence was derived in [39] under a different terminology\u2014simply, the Newton direction\u2014from a viewpoint of nonparametric variational inference. We place their result in the context of approximate Wasserstein gradient flows. Appendix C.1 shows the derivation of the smoothed Wasserstein gradient and Hessian. Appendix C.2 defines the Newton direction built upon the smoothed Wasserstein gradient and Hessian, following the derivation in [39]. Appendix C.3 derives the diagonal approximation of the Newton direction. ", "page_idx": 15}, {"type": "text", "text": "C.1 Smoothed Wasserstein Gradient and Hessian ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Consider the one-dimensional case $\\Theta=\\mathbb{R}$ for simplicity. For a map $T:\\mathbb{R}\\rightarrow\\mathbb{R}$ and a distribution $\\mu\\in\\mathcal \u1e0a P \u1e0c _{2}$ , let $\\mu_{t}$ be the pushforward of $\\mu$ under the transform $\\theta\\mapsto\\theta+t T(\\theta)$ defined with a timevariable $t\\in\\mathbb R$ . This means that $\\mu_{t}$ is a distribution obtained by change-of-variable applied for $\\mu$ . The Wasserstein gradient of a functional $\\mathcal F(\\mu)$ can be associated with the time derivative $(d/d t)\\mathcal{F}(\\mu_{t})$ [23]. In what follows, we focus on the KL divergence ${\\mathcal{F}}(\\mu)=\\operatorname{KL}(\\mu\\mid\\pi)$ as a loss functional. Under a condition $T\\in L^{2}(\\mu)$ , the time derivative at $t=0$ satisfies the following equality ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{d}{d t}\\operatorname{KL}(\\mu_{t}\\mid\\pi){\\Big|}_{t=0}=\\int_{\\Theta}\\,T(\\theta)\\left[\\mathcal{G}^{\\mathrm{KL}}(\\mu)\\right](\\theta)\\,d\\mu(\\theta)=\\left\\langle T,\\mathcal{G}^{\\mathrm{KL}}(\\mu)\\right\\rangle_{L^{2}(\\mu)},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\mathcal{G}^{\\mathrm{KL}}(\\mu)$ denotes the Wasserstein gradient of ${\\mathcal{F}}(\\mu)=\\operatorname{KL}(\\mu\\mid\\pi)$ with the target distribution $\\pi$ made implicit. It gives an interpretation of the Wasserstein gradient as the steepest-descent direction because the decay of the KL divergence at $t=0$ is maximised when $T=-\\mathcal{G}^{\\mathrm{KL}}(\\mu)$ . ", "page_idx": 16}, {"type": "text", "text": "The \u2018smoothed\u2019 Wasserstein gradient can be derived by restricting the transform map $T$ to a more regulated Hilbert space than $\\bar{L^{2}}(\\mu)$ . A reproducing kernel Hilbert space (RKHS) $H$ associated with a kernel function $k:\\mathbb{R}\\times\\mathbb{R}\\to\\mathbb{R}$ is the most common choice of such a Hilbert space [e.g. 26]. An important property of the RKHS $H$ is that any function $f\\in H$ satisfies the reproducing property $f(\\bar{\\theta})=\\langle\\bar{f(\\cdot)},\\bar{k}(\\bar{\\theta_{,}}\\cdot)\\rangle_{H}$ under the associated kernel $k$ and inner product $\\langle\\cdot,\\cdot\\rangle_{H}$ [63]. As discussed in [e.g. 46], applying the reproducing property in (12) under the condition $T\\in H$ and exchanging the integral order, the time derivative satisfies an alternative equality as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\frac{d}{d t}\\operatorname{KL}(\\mu_{t}\\mid\\pi)\\Big|_{t=0}=\\displaystyle\\int_{\\Theta}\\,\\,\\langle T(\\cdot),k(\\theta,\\cdot)\\rangle_{H}\\left[\\mathcal{G}^{\\operatorname{KL}}(\\mu)\\right](\\theta)\\,d\\mu(\\theta)}&{{}}\\\\ {=\\displaystyle\\left\\langle T(\\cdot),\\int_{\\Theta}\\left[\\mathcal{G}^{\\operatorname{KL}}(\\mu)\\right](\\theta)k(\\theta,\\cdot)d\\mu(\\theta)\\right\\rangle_{H}=\\langle T,\\mathcal{G}^{*}(\\mu)\\rangle_{H}}&{{}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\begin{array}{r}{[\\mathcal{G}^{*}(\\mu)](\\cdot):=\\int_{\\Theta}[\\mathcal{G}^{\\mathrm{KL}}(\\mu)](\\theta)k(\\theta,\\cdot)d\\mu(\\theta)}\\end{array}$ corresponds to the smoothed Wasserstein gradient used in the main text. The decay of the KL divergence at $t=0$ is maximised by $T=-\\mathcal{G}^{*}(\\mu)$ . ", "page_idx": 16}, {"type": "text", "text": "Similarly, the Wasserstein Hessian of the functional $\\mathcal F(\\mu)$ can be associated with the second time derivative $(d^{2}/d t^{2})\\mathcal{F}(\\mu_{t})$ [23]. As discussed in [e.g. 46], the Wasserstein Hessian of the KL divergence, denoted ${\\mathrm{Hess}}(\\mu)$ , is an operator over functions $T\\in L^{2}(\\mu)$ that satisfies ", "page_idx": 16}, {"type": "equation", "text": "$$\n{\\frac{d^{2}}{d t^{2}}}\\operatorname{KL}(\\mu_{t}\\mid\\pi){\\Big|}_{t=0}=\\langle T,\\operatorname{Hess}(\\mu)T\\rangle_{L^{2}(\\mu)}\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "See [46] for the explicit form of the Wasserstein Hessian. In the same manner as the smoothed Wasserstein gradient, applying the reproducing property in (14) under the condition $T\\,\\in\\,H$ and exchanging the integral order, the second time derivative satisfies an alternative equality as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\n{\\frac{d^{2}}{d t^{2}}}\\operatorname{KL}(\\mu_{t}\\mid\\pi){\\Big|}_{t=0}=\\left\\langle T(\\star_{1}),\\left\\langle\\,[{\\mathrm{Hess}}^{*}(\\mu)]\\,(\\star_{1},\\star_{2}),T(\\star_{2})\\right\\rangle_{H}\\right\\rangle_{H}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $[\\mathrm{Hess}^{*}(\\mu)](\\star_{1},\\star_{2}):=\\langle k(\\star_{1},\\cdot),\\mathrm{Hess}(\\mu)k(\\star_{2},\\cdot)\\rangle_{L^{2}(\\mu)}$ is the smoothed Wasserstein Hessian and the symbols $\\star_{1}$ and $\\star_{2}$ denote the variables to which each of the two inner products is taken. ", "page_idx": 16}, {"type": "text", "text": "In the multidimensional case $\\Theta=\\mathbb{R}^{d}$ , the transport map $T$ is a vector-valued function $T:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{d}$ , where a similar derivation can be repeated by replacing $L^{2}(\\mu)$ and $H$ with the product space of $d$ independent copies of $L^{2}(\\mu)$ and $H$ . It follows from Proposition 1 and Theorem 1 in [39]\u2014which derives the explicit form of (13) and (15) under their terminology, first and second variations\u2014that the explicit form of the smoothed Wasserstein gradient and Hessian is given by ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ \\ \\ \\ \\ \\ \\left[\\mathcal{G}^{*}(\\mu)\\right](\\cdot)=\\mathbb{E}_{\\theta\\sim\\mu}\\Big[-\\nabla\\log\\pi(\\theta)k(\\theta,\\cdot)-\\nabla k(\\theta,\\cdot)\\Big]\\in\\mathbb{R}^{d},}\\\\ &{\\big[\\mathrm{Hess}^{*}(\\mu)\\big]\\left(\\star_{1},\\star_{2}\\right)=\\mathbb{E}_{\\theta\\sim\\mu}\\Big[-\\nabla^{2}\\log\\pi(\\theta)k(\\theta,\\star_{1})k(\\theta,\\star_{2})+\\nabla k(\\theta,\\star_{1})\\otimes\\nabla k(\\theta,\\star_{2})\\Big]\\in\\mathbb{R}^{d\\times d}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\nabla^{2}$ denotes an operator to take the Jacobian of the gradient\u2014i.e., $\\nabla^{2}f(\\theta)$ is the Hessian matrix of $f$ at $\\theta_{}$ \u2014and $\\otimes$ denotes the outer product of two vectors. Note that both the smoothed Wasserstein gradient and Hessian are well-defined for any distribution $\\mu$ including empirical distributions. ", "page_idx": 16}, {"type": "text", "text": "C.2 Approximate Wasserstein Newton Direction ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In the Euclidean space, the Newton direction of an objective function is a direction s.t. the secondorder Taylor approximation of the function is minimised. Similarly, [39] characterised the Newton direction $T^{*}:\\dot{\\mathbb{R}}^{d}\\rightarrow\\mathbb{R}^{d}$ of the KL divergence $\\operatorname{KL}(\\mu\\mid\\pi)$ as a solution of the following equation ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left\\langle\\left\\langle\\left[\\mathrm{Hess}^{*}(\\mu)\\right](\\star_{1},\\star_{2}),T^{*}(\\star_{2})\\right\\rangle_{H}+\\left[\\mathcal{G}^{*}(\\mu)\\right](\\star_{1}),V(\\star_{1})\\right\\rangle_{H}=0\\quad\\mathrm{for~all}\\quad V\\in H.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Here $\\Theta=\\mathbb{R}^{d}$ and $H$ is the product space of $d$ independent copies of the RKHS of a kernel $k$ . To obtain a closed-form solution, [39] supposed that the Newton direction $T^{*}$ can be expressed in a form $\\begin{array}{r}{T^{*}(\\cdot)=\\sum_{i=1}^{n}W^{n}k(\\cdot,\\theta^{n})}\\end{array}$ dependent on a set of each particle $\\theta^{n}\\in\\Theta$ and associated vector-valued coefficient $W^{n}\\in\\mathbb{R}^{d}$ . Once the set of the particles is given, the set of the associated vector-valued coefficients is determined by solving the following simultaneous linear equation ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left[\\sum_{n=1}^{N}[\\mathrm{Hess}^{*}(\\mu)](\\theta^{1},\\theta^{n})\\cdot W^{n}\\right]=\\left[\\begin{array}{l}{-[\\mathcal{G}^{*}(\\mu)](\\theta^{1})}\\\\ {\\vdots}\\\\ {-[\\mathcal{G}^{*}(\\mu)](\\theta^{N})}\\end{array}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "These equations (16) can be rewritten in a matrix form [64]. Let $K:=N\\times d$ . Define a block matrix $\\mathbf{H}\\in\\mathbb{R}^{k\\times K}$ and a block vector $\\mathbf{G}\\in\\mathbb{R}^{K}$ by the following partitioning ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbf{H}=\\left({\\frac{\\begin{array}{c}{\\mathbf{H}_{11}}\\end{array}}{\\overbrace{\\mathbf{H}_{N1}\\ \\cdots\\ \\left|\\ \\mathbf{H}_{N N}\\right.}^{\\mathbf{H}_{11}}}\\\\ {\\vdots\\ \\ \\left\\langle\\ \\cdots\\ \\underbrace{\\vdots\\ }_{\\cdot}\\ \\ \\right\\rangle\\ \\ \\ \\ \\ \\ \\ \\mathrm{and}\\ \\ \\ \\ \\ \\ \\mathbf{G}=\\left({\\frac{\\mathbf{G}_{1}}{\\overbrace{\\mathbf{G}_{N}}^{\\cdot}}\\right)}}\\end{array}}\\right)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "with each block specified as $\\mathbf{H}_{i j}\\;:=\\;[\\mathrm{Hess}^{*}(\\boldsymbol{\\mu})](\\boldsymbol{\\theta}^{i},\\boldsymbol{\\theta}^{j})\\;\\in\\;\\mathbb{R}^{d\\times d}$ and $\\mathbf{G}_{i}\\;:=\\;[\\mathcal{G}^{*}(\\mu)](\\theta^{i})\\;\\in\\;\\mathbb{R}^{d}$ Define a block matrix ${\\bf K}\\in\\mathbb{R}^{K\\times K}$ and a block vector $\\mathbf{W}\\in\\mathbb{R}^{K}$ by the following partitioning ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbf{K}:=\\left(\\frac{\\mathbf{K}_{11}\\mathbf{\\beta}\\cdot\\cdot\\cdot\\cdot\\mathbf{\\beta}\\left|\\mathbf{K}_{1N}\\right.}{\\vdots\\cdot\\cdot\\cdot\\mathbf{\\beta}\\left|\\mathbf{\\beta}\\cdot\\cdot\\cdot\\mathbf{\\beta}\\right|}\\right)\\qquad\\mathrm{and}\\qquad\\mathbf{W}:=\\left(\\frac{\\mathbf{W}^{1}}{\\mathbf{\\beta}\\cdot\\mathbf{\\beta}}\\right)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "with each block of $\\mathbf{K}$ specified as $\\mathbf{K}_{i j}:=\\mathbf{I}_{d}\\times k\\big(\\theta^{i},\\theta^{j}\\big)\\in\\mathbb{R}^{d\\times d}$ , where $\\mathbf{I}_{d}$ denotes the $d\\times d$ identity matrix. Notice that $\\mathbf{W}$ is a block vector that aligns the vector-valued coefficients $\\{W^{n}\\}_{n=1}^{N}$ . Using these notations, the optimal coefficients that solve (16) is simply written as $\\mathbf{W}=-\\mathbf{H}^{-1}\\mathbf{G}$ [64]. ", "page_idx": 17}, {"type": "text", "text": "Given the optimal coefficients $\\mathbf{W}=-\\mathbf{H}^{-1}\\mathbf{G}$ , the Newton direction $T^{*}(\\theta^{n})$ evaluated at the given particle $\\theta^{n}$ for each $n=1,\\ldots,N$ can be written in the following block vector form ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left({\\frac{\\frac{T^{*}(\\theta^{1})}{\\vdots}}{T^{*}(\\theta^{N})}}\\right)=-\\left({\\frac{\\mathbf{K}_{11}\\;\\left|\\;\\cdots\\;\\left|\\;\\mathbf{K}_{1N}\\right.\\right.}{\\vdots}}\\right)\\left({\\frac{\\mathbf{H}_{11}\\;\\left|\\;\\cdots\\;\\left|\\;\\mathbf{H}_{1N}\\right.\\right.}{\\vdots}}\\right)^{-1}\\left({\\frac{\\mathbf{G}_{1}}{\\vdots}}\\right)^{-1}\\left({\\frac{\\mathbf{G}_{1}}{\\vdots}}\\right)^{-1}\\left({\\frac{\\mathbf{G}_{1}}{\\mathbf{G}_{N}}}\\right)^{-1}\\left({\\frac{\\mathbf{G}_{1}}{\\mathbf{G}_{N}}}\\right)^{-1}\\left({\\frac{\\mathbf{G}_{1}}{\\mathbf{G}_{N}}}\\right)^{-1}\\left({\\frac{\\mathbf{G}_{1}}{\\mathbf{G}_{N}}}\\right)^{-1}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "To distinguish from the standard Newton direction in the Euclidean space, we call (17) the approximate Wasserstein Newton direction. The approximate Wasserstein Newton direction yields a second-order particle update scheme. Suppose we have particles $\\lbrace\\theta_{m}^{n}\\rbrace_{n=1}^{N}$ to be updated at each step th {\u03b8nm}nN= .e  pAat rteiacclehs . fiRnee ptlhaec ianbgo tvhee  mWaatrsisceerss na gnrda e nwti tihn  tthhee  epamrtpiicrliec aul pddiasttrei sbcuhtieomn $\\mu=\\hat{\\pi}_{m}$ t hoef approximate Wasserstein Newton direction (17) provides the second-order update scheme in [39]. ", "page_idx": 17}, {"type": "text", "text": "C.3 Diagonal Approximate Wasserstein Newton Direction ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We derive the diagonal approximation of the approximate Wasserstein Newton direction, which we used for our second-order WGBoost algorithm. A few approximations of the approximate Wasserstein Newton direction were discussed in [39] for better performance of their particle algorithm. We derive the diagonal approximation so that no matrix product and inversion will be involved. Specifically, we replace the matrices $\\mathbf{K}$ and $\\mathbf{H}$ in (17) by the diagonal approximations $\\hat{\\bf K}$ and $\\hat{\\bf H}$ , that is, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{K}}=\\left(\\frac{\\mathbf{I}_{d}\\mathbf{\\Gamma}|\\mathbf{\\Gamma}\\cdot\\cdot\\cdot\\mathbf{\\Gamma}|\\mathbf{\\Gamma}\\mathbf{0}}{\\vdots}\\right)\\quad\\mathrm{and}\\quad\\hat{\\mathbf{H}}=\\left(\\frac{\\mathbf{h}_{11}\\mathbf{\\Gamma}|\\mathbf{\\Gamma}\\cdot\\cdot\\mathbf{\\Gamma}|\\mathbf{\\Gamma}\\mathbf{0}}{\\vdots}\\right),\\quad\\mathrm{\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where ${\\bf K}_{n n}\\,=\\,{\\bf I}_{d}\\,\\times\\,k(\\theta^{n},\\theta^{n})\\,=\\,{\\bf I}_{d}$ for the Gaussian kernel $k$ used in this work, and the matrix $\\mathbf{h}_{n n}\\in\\mathbb{R}^{d\\times d}$ denotes the diagonal approximation of the diagonal block ${\\bf H}_{n n}$ of $\\mathbf{H}$ . ", "page_idx": 18}, {"type": "text", "text": "Recall that ${\\bf H}_{n n}\\,=\\,[{\\mathrm{Hess}}^{*}(\\mu)](\\theta^{n},\\theta^{n})$ . Denote by $\\mathrm{Diag}(\\mathbf{A})$ the diagonal of a square matrix A. The diagonal approximation $\\mathbf{h}_{n n}$ is a diagonal matrix whose diagonal is $\\mathrm{Diag}({\\bf H}_{n n})$ . We plug the diagonal approximations $\\hat{\\bf K}$ and $\\hat{\\bf H}$ in (17). It follows from inverse and multiplication properties of diagonal matrices that the approximate Wasserstein Newton direction turns into a form ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left({\\frac{\\frac{T^{*}\\left(\\theta^{1}\\right)}{\\vdots}}{T^{*}\\left(\\theta^{N}\\right)}}\\right)=-\\left({\\frac{\\frac{\\mathbf{h}_{11}}{\\vdots}\\,\\left|\\,\\cdots\\,\\cdot\\,\\left|\\,\\mathbf{\\theta}\\right.\\,}{\\ddots\\,\\left|\\,\\cdots\\,\\cdot\\,\\left|\\,\\mathbf{\\theta}\\right.\\,}}\\right)^{-1}\\left({\\frac{\\mathbf{G}_{1}}{\\vdots}}\\right)=\\left({\\frac{-\\mathbf{G}_{1}\\odot\\,\\mathbf{D}\\mathrm{iag}\\left(\\mathbf{H}_{11}\\right)}{\\vdots}}\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "At an arbitrary particle location $\\theta$ , denote by $[{\\mathcal{H}}^{*}(\\mu)](\\theta)$ the diagonal of the smoothed Wasserstein Hessian $[\\mathrm{Hess}^{*}(\\mu)](\\theta,\\theta)$ . It is straightforward to see that the diagonal can be written as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left[\\mathcal{H}^{*}(\\mu)\\right](\\cdot)=\\mathbb{E}_{\\theta\\sim\\mu}\\Big[-\\nabla_{\\mathrm{d}}^{2}\\log\\pi(\\theta)k(\\theta,\\cdot)^{2}+\\nabla k(\\theta,\\cdot)\\odot\\nabla k(\\theta,\\cdot)\\Big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Notice that Diag $\\left({\\bf H}_{n n}\\right)=\\left[\\mathcal{H^{*}}(\\mu)\\right]\\left(\\theta^{n}\\right)$ by definition. It therefore follows from the formula (18) with $\\mathbf{G}_{n}=[\\mathcal{G}^{*}(\\mu)](\\theta^{n})$ and Diag $\\left(\\mathbf{\\dot{H}}_{n n}\\right)=\\left[\\mathcal{H}^{*}(\\mu)\\right](\\theta^{n})$ that the diagonal approximate Wasserstein Newton direction at an arbitrary particle location $\\theta$ can be independently computed by ", "page_idx": 18}, {"type": "equation", "text": "$$\n-[\\mathcal{G}^{\\ast}(\\mu)](\\theta)\\oslash\\left[\\mathcal{H}^{\\ast}(\\mu)\\right](\\theta).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We used this direction in Section 3. In the main text, the diagonal approximate Wasserstein Newton direction is defined for each loss functional $\\mathcal{F}_{i}(\\cdot)=\\mathrm{D}(\\cdot\\mid\\bar{\\mu}_{i})$ , with $\\pi=\\mu_{i}$ , using the smoothed Wasserstein gradient $\\mathcal{G}_{i}^{*}(\\mu)$ and the diagonal of the smoothed Wasserstein Hessian $\\mathcal{H}_{i}^{*}(\\mu)$ for each i-th output distribution $\\mu_{i}$ . ", "page_idx": 18}, {"type": "text", "text": "D Simulation Study for WEvidential ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "This section provides simulation studies on kernel choice of WEvidential and comparison of different estimate of the Wasserstein gradient to use in the WGBoost framework. ", "page_idx": 18}, {"type": "text", "text": "D.1 Choice of Kernel ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We used the kernel smoothing estimate of the Wasserstein gradient of the KL divergence in order to built WEvidential. The smoothed Wasserstein gradient originates in an approximate Wasserstein gradient flow called Stein variational gradient descent (SVGD) [43]. It is fairly common in SVGD in practice to use the Gaussian kernel $k(\\theta,\\theta^{*})=\\exp(-\\|\\theta-\\theta^{*}\\|^{2}/h)$ with the scale $h>0$ . For WEvidential, the scale $h$ may be viewed as a hyperparameter to choose. We recommend using the value of the scale s.t. $0.01\\leq h\\leq1.0$ in general. This work uses the value $h=0.1$ throughout the experiments in the main text. One may opt for performing more advanced tuning of the scale $h$ . ", "page_idx": 18}, {"type": "text", "text": "We provide a simulation study to examine sensitivity to the scale value. We prepared a synthetic dataset $\\{x_{i},\\mu_{i}\\}_{i=1}^{D}$ whose inputs are 200 gird points on the interval $[-3.5,3.5]$ and output distributions are normal distributions $\\mu_{i}(\\theta)\\,=\\,\\^{\\cdot}\\!{\\mathcal N}(\\bar{\\theta}\\,\\mid\\,\\sin(x_{i}),0.5)$ conditional on each $x_{i}$ . We used WEvidential with different kernel scales $h=0.001,0.01,0.1,1.0,10,100$ and fitted it to the synthetic dataset $\\{x_{i},\\mu_{i}\\}_{i=1}^{D}$ . The decision tree regressor with the maximum depth 3 was used for each weak learner. The learning rate and the number of weak learners were set to 0.1 and 100. The initial constant $\\{\\vartheta^{n}\\}_{n=1}^{10}$ of WEvidential was set to 10 grid points in the interval $[-10,10]$ . ", "page_idx": 18}, {"type": "text", "text": "We computed the output of WEvidential for 500 grid points in the interval $[-3.5,3.5]$ . We used the maximum mean discrepancy (MMD) [65] to measure the approximation error between the empirical distribution $\\hat{\\mu}_{i}$ of the output of WEvidential and the output distribution $\\mu_{i}$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathrm{MMD}^{2}\\left(\\hat{\\mu}_{i},\\mu_{i}\\right)=\\mathbb{E}_{\\theta\\sim\\hat{\\mu}_{i},\\theta^{\\prime}\\sim\\hat{\\mu}_{i}}[k_{\\mathrm{D}}(\\theta,\\theta^{\\prime})]-2\\mathbb{E}_{\\theta\\sim\\hat{\\mu}_{i},\\theta^{\\prime}\\sim\\mu_{i}}[k_{\\mathrm{D}}(\\theta,\\theta^{\\prime})]+\\mathbb{E}_{\\theta\\sim\\mu_{i},\\theta^{\\prime}\\sim\\mu_{i}}[k_{\\mathrm{D}}(\\theta,\\theta^{\\prime})]\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $k_{\\mathrm{D}}$ is a Gaussian kernel $k_{\\mathrm{D}}(\\theta,\\theta^{\\prime})=\\mathrm{exp}(-(\\theta-\\theta^{\\prime})^{2}/s)$ with the scale $s=0.025$ . The total approximation error was measured by the MMD averaged over all the inputs. Figure 5 shows the total approximation error of WEvidential for each scale value $h$ in the common log scale, together with examples of the output of WEvidential for different values of $h$ . It demonstrates that the total error is minimised by $h=0.1$ and stays in a relatively small value range for $0.01\\leq h\\leq1.0$ . ", "page_idx": 18}, {"type": "image", "img_path": "cuO0DenqMl/tmp/b5a2ceec25e6faab25452dc31a1606ef56d9aeb78df82899e0aad80fa050d4d5.jpg", "img_caption": ["Figure 5: The total MMD error and example outputs of WEvidential for different kernel scales. Panel (a): the total MMD error for different scale values $h=0.001,0.01,0.1,1.0,10,100$ both plotted in the common log scale. Panel (b): the output of WEvidential for $h=0.1$ . Panel (c): the output of WEvidential for $h=0.01$ . Panel (d): the output of WEvidential for $h=100$ . "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "D.2 Comparison of Different Wasserstein Gradient Estimates ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We compare four WGBoost algorithms built on different estimates of the Wasserstein gradient of the KL divergence. The first three algorithms set the estimate $\\mathcal G_{i}(\\mu)$ in Algorithm 1 to, respectively, ", "page_idx": 19}, {"type": "text", "text": "1. the smoothed Wasserstein gradient in (7);   \n2. the diagonal approximate Wasserstein Newton direction in (8);   \n3. the full approximate Wasserstein Newton direction in (17). ", "page_idx": 19}, {"type": "text", "text": "The fourth algorithm is LGBoost in Appendix B which is rather a variant of WGBoost. The first and third algorithms are called the first-order WEvidential and the full-Newton WEvidential, respectively. The second algorithm is our default WEvidential presented in Section 3. The first-order WEvidential is implemented by removing $h_{i}^{n}$ and replacing $g_{i}^{n}\\circledcirc h_{i}^{n}$ with $g_{i}^{n}$ in Algorithm 2. The full-Newton WEvidential is implemented by replacing $g_{i}^{n}\\circledcirc h_{i}^{n}$ in Algorithm 2 with $v_{i}^{n}$ computed by the following Algorithm 4, where $\\nabla^{2}f(\\theta)$ denotes the Hessian matrix of a function $f:\\Theta\\to\\mathbb{R}$ at $\\theta$ . ", "page_idx": 19}, {"type": "text", "text": "We prepared the same synthetic dataset $\\{x_{i},\\mu_{i}\\}_{i=1}^{D}$ as Appendix D.1 and ftited each algorithm to the dataset. We computed the output of each algorithm for 500 grid points in the interval $[-3.5,3.5]$ and measured the approximation error by the MMD in the same manner as Appendix D.1. The decision tree regressor with the maximum depth 3 was used for each weak learner, and the learning rate was set to 0.1. We used an increasing number of weak leaners up to 100 weak learners, in order to observe the decay of the approximation error and the increase of the computational time. The initial constant $\\{\\vartheta^{n}\\}_{n=1}^{10}$ for each algorithm was set to 10 grid points in the interval $[-10,10]$ , which sufficiently differs from the output distributions so that the decay of the approximation error is clear. ", "page_idx": 19}, {"type": "text", "text": "Figure 6 shows the approximation error and the computational time of each algorithm with respect to the increasing number of weak learners, together with the output of each algorithm with 100 weak learners trained. It demonstrates that WEvidential and full-Newton WEvidential reduce the approximation error most efficiently, while full-Newton WEvidential takes the longest computational time among others. As in Algorithm 4, the computation of the full approximate Wasserstein Newton direction requires the inverse and product of the $\\bar{(\\boldsymbol{N}\\times\\boldsymbol{d})}\\times(\\boldsymbol{N}\\times\\boldsymbol{d})$ block matrices, where $N$ denotes the particle number $N$ and $d$ denotes the particle dimension. The error decay of LGBoost is not only slow but also shows stochasticity due to the Gaussian noise used in the algorithm. We therefore recommend our default WEvidential for better performance and efficient computation. ", "page_idx": 19}, {"type": "text", "text": "Algorithm 4: Computation of Full Approximate Wasserstein Newton Direction ", "page_idx": 20}, {"type": "image", "img_path": "cuO0DenqMl/tmp/e77a9098cf8f2c0c8109aba86986854b93258a2e0b7bbb63667daf33b45be247.jpg", "img_caption": ["Figure 6: The approximation error and computational time of the four different WGBoost algorithms. Panel (a): the approximation error of each algorithm measured by the MMD averaged over the inputs with respect to the number of weak learners. Panel (b): the computational time with respect to the number of weak learners in common logarithm scale. Panel (c)-(f): the outputs of the four algorithms each with 100 weak learners used. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "E Additional Details of Main Experiments ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "This section describes additional details of the applications in Section 4. All the experiments were performed with $\\mathbf{X}86{-}64$ CPUs, where some of them were parallelised up to 10 CPUs and the rest uses 1 CPU. The scripts to reproduce all the experiments are provided in the source code. Appendices E.1 to E.3 describe additional details of the applications in Sections 4.1 to 4.3 respectively. Appendix E.4 describes a choice of initial constants $\\{\\bar{\\vartheta}_{0}^{\\bar{n}}\\}_{n=1}^{N}$ for the WGBoost algorithm used in Section 4. ", "page_idx": 20}, {"type": "text", "text": "E.1 Detail of Section 4.1 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The normal response distribution $\\mathcal{N}(y\\mid m,\\sigma)$ in Example 1 was used in Section 4.1. The normal response distribution has the scale parameter $\\sigma$ that lies only in the positive domain of the Euclidean space $\\mathbb{R}$ . We reparametrised it as one in the Euclidean space $\\mathbb{R}$ by the log transform $\\sigma^{\\prime}:=\\log(\\sigma)$ , which is the standard practice in Bayesian computation [20]. The inverse of the log transform is the exponential transform $\\sigma\\,=\\,\\exp(\\sigma^{\\prime})$ . The change of variable formula tells the form of the individual-level posterior on $(m,\\sigma^{\\prime})$ up to the normalising constant. Under the prior in Example 1, the individual-level posterior on $(m,\\sigma^{\\prime})$ conditional on each individual response $y_{i}$ is given by ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "equation", "text": "$$\np(m,\\sigma^{\\prime}\\mid y_{i})\\propto\\exp\\left(-{\\frac{1}{2}}{\\frac{(y_{i}-m)^{2}}{\\exp(\\sigma^{\\prime})^{2}}}\\right)\\times\\exp\\left(-{\\frac{1}{2}}{\\frac{m^{2}}{10^{2}}}\\right)\\times{\\frac{1}{\\exp(\\sigma^{\\prime})^{1.01}}}\\exp\\left(-{\\frac{0.01}{\\exp(\\sigma^{\\prime})}}\\right),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the Jacobian determinant $|d\\sigma/d\\sigma^{\\prime}|=\\exp(\\sigma^{\\prime})$ is used for the change of variable. Figure 7 shows the output of WEvidential for the old faithful geyser dataset and the conditional density estimated through (6). ", "page_idx": 21}, {"type": "image", "img_path": "cuO0DenqMl/tmp/28bfee57c256c62cef418d4545b013d1e975c118fe7740679b9c4f1821daa0ef.jpg", "img_caption": ["Figure 7: Conditional density estimation for the old faithful geyser dataset (grey dots) by WEvidential. Left: distributional estimate (10 particles) of the location parameter for each input. Right: estimated density by the predictive distribution (6) based on the output particles. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "E.2 Detail of Section 4.2 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Section 4.2 used the same reparametrisation of the normal response parameter $(m,\\sigma)$ as that of Appendix E.2. The NLL and RMSE of each algorithm on test data $\\{x_{i},\\bar{y}_{i}\\}_{i=1}^{D}$ were computed by ", "page_idx": 21}, {"type": "equation", "text": "$$\n{\\mathrm{NLL}}=-{\\frac{1}{D}}\\sum_{i=1}^{D}\\log p(y_{i}\\mid x_{i})\\quad{\\mathrm{and}}\\quad{\\mathrm{RMSE}}={\\sqrt{{\\frac{1}{D}}\\sum_{i=1}^{D}(y_{i}-{\\hat{y}}_{i})^{2}}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $p(y_{i}~\\vert~x_{i})$ and $\\hat{y}_{i}$ denote the predictive distribution and the point prediction provided by the algorithm. For WEvidential, we standardised the responses in the training data, in which case standardisation of test data were performed as yi\u2032 = (yi \u2212ytmraeiann)/ytsrtadin with the mean $y_{\\mathrm{mean}}^{\\mathrm{train}}$ and standard deviation $y_{\\mathrm{std}}^{\\mathrm{train}}$ of the training responses. Hence WEvidential provided the predictive distribution $p(y_{i}^{\\prime}\\mid x_{i})^{\\prime}$ and the point prediction $\\hat{y}_{i}^{\\prime}$ for the standardised responses $y_{i}^{\\prime}$ . With no loss of generality, the NLL and RMSE for the original responses $y_{i}$ can be computed as follows: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{NLL}=-\\frac{1}{D}\\sum_{i=1}^{D}\\log p(y_{i}\\mid x_{i})=-\\frac{1}{D}\\sum_{i=1}^{D}\\log p(y_{i}^{\\prime}\\mid x_{i})+\\log y_{\\mathrm{std}}^{\\mathrm{train}},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{RMSE}=\\sqrt{\\frac{1}{D}\\sum_{i=1}^{D}(y_{i}-(y_{\\mathrm{mean}}^{\\mathrm{train}}+y_{\\mathrm{std}}^{\\mathrm{train}}\\times\\hat{y}_{i}^{\\prime}))^{2}}=y_{\\mathrm{std}}^{\\mathrm{train}}\\sqrt{\\frac{1}{D}\\sum_{i=1}^{D}(y_{i}^{\\prime}-\\hat{y}_{i}^{\\prime})^{2}},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the equality of the NLL follows from the change of variable $p(y_{i}\\mid x_{i})=p(y_{i}^{\\prime}\\mid x_{i})/y_{\\mathrm{std}}^{\\mathrm{train}}$ and the equality of the RMSE follows from rearranging the terms. ", "page_idx": 21}, {"type": "text", "text": "We provide a brief description of each algorithm used in Section 4.2. For all the algorithms, the normal response distribution $p(y\\mid m,\\sigma)$ was specified as the response distribution, where the algorithm produces a point or distributional estimate of the response parameter $(m,\\sigma)$ at each input $x$ . ", "page_idx": 21}, {"type": "text", "text": "\u2022 MCDropout [9] trains a single neural network $F$ while dropping out each network weight with some Bernoulli probability. It can be interpreted as a variational approximation of a Bayesian neural network. It generates multiple subnetworks $\\{F^{n}\\}_{n=1}^{N}$ by subsampling the network weights by the dropout. The predictive distribution $p(y\\mid x)$ is given by the model averaging $\\begin{array}{r}{(1/N)\\sum_{i=1}^{N}p(y\\mid(m,\\sigma)=F^{n}(x))}\\end{array}$ for each input $x$ . \u2022 DEnsemble [10] simply trains independent copies $\\{F^{n}\\}_{n=1}^{N}$ of a neural network $F$ in parallel. It is one of the mainstream approaches to uncertainty quantification based on deep learning. The predictive distribution is given by the model averaging as in MCDropout. ", "page_idx": 21}, {"type": "text", "text": "\u2022 CDropout [55] consider a continuous relaxation of the Bernoulli random variable used in MCDropout to optimise the Bernoulli probability of the dropout. It generates multiple subnetworks $\\{F^{n}\\}_{n=1}^{N^{\\ast}}$ by subsampling the network weights by the dropout with the optimised probability. The predictive distribution is the same as MCDropout. \u2022 NGBoosting [32] is a family of gradient booting that use the natural gradient [66] of the response distribution as a target variable of each weak learner. In contrast to other methods, NGBoost outputs a single value $F(x)$ to be plugged into the response-distribution parameter. The predictive distribution $p(y\\mid x)$ is given by ${\\bar{p}}(y\\mid(m,\\sigma)={\\bar{F}}(x))$ for each input $x$ . \u2022 DEvidential [13] extends deep evidential learning [11], originally proposed in classification settings, to regression settings. It considers the case where the individual-level posterior of the response distribution falls into a conjugate parametric form, and predicts the hyperparameter of the individual-level posterior by a neural network. The predictive distribution is also given in a conjugate closed-form. ", "page_idx": 22}, {"type": "text", "text": "The algorithms used in Section 4.2 are computationally efficient uncertainty quantification methods that are commonly-used in practice. In the following, we provide a limited yet additional comparison of WEvidential with other Bayesian neural networks (BNNs) and a kernel-based model, Gaussian process (GP). We used a subset of the datasets in Section 4, which were used in [67] who employed BNNs with one hidden layer of 50 units. We additionally used a large-scale dataset, keggd. For the keggd dataset only, we reported the NLL and RMSE of the normalised outputs without the adjustment (19) and (20), in line with [67]. WEvidential was compared with BNNs learned by variational inference with the reparametrisation trick (VI), deterministic variational inference (DVI), stochastic weight averaging Gaussian (SWAG), principal component analysis subspace inference $\\scriptstyle(\\mathrm{{PCA+VI}})$ ), and two-layer deep Gaussian process with 50 induced points trained via expectation propagation (DGP1-50). Table 1 summarises the result. ", "page_idx": 22}, {"type": "table", "img_path": "cuO0DenqMl/tmp/fe7fc80550c6b5beb0e3bc2e15a9acd6100f49f01f2bde8ac986afd9e6ba4eea.jpg", "table_caption": ["Table 4: The NLLs and RMSEs for each dataset, where the best score is underlined. The results other than that of WEvidential were reported in [67]. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "E.3 Detail of Section 4.3 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We reparametrised the parameter of the categorical response distribution ${\\mathcal{C}}(y\\mid q)$ used in Section 4.3, similarly to the normal response distribution used in Sections 4.1 and 4.2. The categorical response distribution has a class probability parameter $q=[q_{1},\\ldots,q_{k}]$ in the simplex $\\Delta_{k}$ . We applied the log-ratio transform $q^{\\prime}:=[\\log(q_{1}/q_{k}),\\ldots,\\log(q_{k-1}/q_{k})]\\in\\mathbb{R}^{k-1}$ that maps from the simplex $\\Delta_{k}$ to the Euclidean space $\\mathbb{R}^{k-1}$ [42]. The inverse of the log-ratio transform is the logistic transform ", "page_idx": 22}, {"type": "equation", "text": "$$\nq=\\left[\\frac{\\exp(q_{1}^{\\prime})}{z_{k}},\\ldots,\\frac{\\exp(q_{k-1}^{\\prime})}{z_{k}},\\frac{1}{z_{k}}\\right]\\in\\Delta_{k}\\quad\\mathrm{where}\\quad z_{k}=1+\\sum_{j=1}^{k-1}\\exp(q_{j}^{\\prime}).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The logistic normal distribution on the original parameter $q$ corresponds to a normal distribution on the transformed parameter $q^{\\prime}$ [42]. The change of variable formula tells the individual-level posterior on $q^{\\prime}$ up to the normalising constant. Under the prior in Example 2, the individual-level posterior ", "page_idx": 22}, {"type": "text", "text": "$p(q^{\\prime}\\mid y_{i})$ conditional on each individual observed response $y_{i}$ is given by ", "page_idx": 23}, {"type": "equation", "text": "$$\np(q^{\\prime}\\mid y_{i})\\propto\\left(\\frac{1}{z_{k}}\\right)^{[y_{i}=k]}\\times\\prod_{j=1}^{k-1}\\left(\\frac{\\exp(q_{j}^{\\prime})}{z_{k}}\\right)^{[y_{i}=j]}\\times\\exp\\left(-\\frac{1}{2}\\frac{\\|q^{\\prime}\\|^{2}}{10^{2}}\\right),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $[y_{i}=j]$ is $1$ if $y_{i}$ is the $j$ -th class label and 0 otherwise. ", "page_idx": 23}, {"type": "text", "text": "We provide a brief description of each algorithm used in Section 4.3. MCDropout and DEnsemble are described in Appendix E.2. ", "page_idx": 23}, {"type": "text", "text": "\u2022 DDistillation [57] predicts the parameter of a Dirichlet distribution over the simplex $\\Delta_{k}$ by a neural network using the output of DEnsemble. The output of multiple networks in DEnsemble is distilled into the Dirichlet distribution controlled by one single network. \u2022 PNetwork [14] considers the case where the individual-level posterior of the categorical response distribution falls into a Dirichlet distribution similarly to deep evidential learning [11]. It predicts the hyperparameter of the individual-level posterior given in the form of the Dirichlet distribution. ", "page_idx": 23}, {"type": "text", "text": "For the OOD detection, the OOD score we used was the inverse of the maximum norm of the variance of the WEvidential output. There are other quantities that are possible to use as the OOD score. For example, the entropy of the predictive distribution $p(y\\mid x)$ is a quantity computable for any probabilistic classification method. We compared the OOD detection performance of WEvidential with that of NGBoost and Random Forest (RForest) based on the entropy of their predictive distributions. For reference, we also computed the OOD detection performance of WEvidential based on the entropy of the predictive distribution. For NGBoost, the decision tree regressor was used for each weak learner and 4000 weak learners were trained with the learning rate 0.01. For RForest, the maximum depth 3 was used. Table 5 shows the result on the segment dataset, demonstrating that the performance of WEvidential is higher, to a large margin, than that of NGBoost and RForest based on the entropy. ", "page_idx": 23}, {"type": "table", "img_path": "cuO0DenqMl/tmp/a643af8f57dccc9f3ed072bee86afae282ff9dd437807ee2427aeb0e0f5e94f0.jpg", "table_caption": ["Table 5: The OOD detection performance of WEvidential, NGBoost, and RForest on the segment dataset, where WEvidential (Entropy) indicates the result of WEvidential based on the entropy. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "To investigate on the effective learning rate of WEvidential for classification, we performed a simple simulation study of WEvidential using the synthetic madelon dataset. We created 600 data points of three-dimensional inputs and three-class labels. The data subset that belongs to the last class was kept as the OOD samples. We randomly held out $20\\%$ of non-OOD samples as a test set to measure the test classification accuracy. We trained WEvidential using different learning rates from 0.1 to 1.0 while fixing the number of weak learners to 4000. Table 6 shows that the result for the learning rate 0.4 demonstrates the best classification accuracy and the third best OOD detection performance. We also trained WEvidential using different numbers of weak learners from 1000 to 4000 while fixing the learning rate to 0.4. Table 7 shows that the result for the number of weak learners 4000 demonstrates the best classification accuracy and the second best OOD detection performance. While the most effective learning rate differs depending on each dataset, we drew on the insight obtained from the simulation study and set the learning rate to 0.4 for classification. ", "page_idx": 23}, {"type": "table", "img_path": "cuO0DenqMl/tmp/3b9db0b9986ded9dc1f69ea74eb8457d35fdf49d08773b7c30fb4f07a9e0619a.jpg", "table_caption": ["Table 6: The classification accuracy and OOD detection performance of WEvidential on the synthetic dataset for different learning rates, where the number of weak learners is fixed to 4000. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "Table 7: The classification accuracy and OOD detection performance of WEvidential on the synthetic dataset for different numbers of weak learners, where the learning rate is fixed to 0.4. ", "page_idx": 24}, {"type": "table", "img_path": "cuO0DenqMl/tmp/b0123ac5fe41568ec44a881ddb47fe906559cca852b9a888e9ab541ae9da682c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "E.4 Choice of Initial State of WGBoost ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In standard gradient boosting, the initial state at step $m=0$ is specified by a constant that most fits given data. Similarly, we specified the initial state $\\{\\vartheta_{0}^{n}\\}_{n=1}^{N}$ of WEvidential by a set of constants that most fits the output distributions in average. We find such a set of constants by performing an approximate Wasserstein gradient flow averaged over all the output distributions. Specifically, given the term $\\mathcal{G}_{i}(\\mu)$ in Algorithm 1, we define $\\begin{array}{r}{\\bar{\\mathcal{G}}(\\mu):=(1/D)\\sum_{i=1}^{D}\\bar{\\mathcal{G}}_{i}(\\mu)}\\end{array}$ and perform the update scheme of a set of $N$ particles $\\{\\bar{\\vartheta}_{m}^{n}\\}_{n=1}^{N}$ : ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left[\\bar{\\vartheta}_{m+1}^{1}\\right]=\\left[\\begin{array}{c}{\\bar{\\vartheta}_{m}^{1}}\\\\ {\\vdots}\\\\ {\\bar{\\vartheta}_{m}^{\\bar{N}}}\\end{array}\\right]+\\nu_{0}\\left[-[\\bar{\\mathcal{G}}(\\hat{\\pi}_{m})](\\bar{\\vartheta}_{m}^{1})\\right]}\\\\ {\\left[\\bar{\\vartheta}_{m+1}^{\\bar{N}}\\right]=\\left[\\bar{\\vartheta}_{m}^{1}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "with the learning rate $\\nu_{0}=0.01$ up to the maximum step number $m=5000$ . The initial particle locations for this update scheme were sampled from a standard normal distribution. We specified the initial state $\\{\\vartheta_{0}^{n}\\}_{n=1}^{N^{\\mathrm{~\\,~}}}$ by the set of particles $\\{\\bar{\\vartheta}_{m}^{n}\\}_{n=1}^{N}$ obtained though this scheme at $m=5000$ . ", "page_idx": 24}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: The claim made in the abstract and introduction were elaborated in the subsequent section and empirically demonstrated through the experiments. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 25}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: A concise discussion on the important remaining questions and the potential limitation were provided in the last section. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 25}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 26}, {"type": "text", "text": "Justification: This work primarily focused on the methodological development of the proposed algorithm, empirically demonstrating the performance on the various benchmarks. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 26}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The computational procedure of the proposed algorithm was clarified in the main text and appendix, with the setting to reproduce the experiments detailed. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 26}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The source code is available online with the instruction, and also submitted in the supplementary file. For the blind review process, the source code URL was masked in the manuscript. The source code URL will be unmasked after the review process. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 27}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: All the experimental setting was clarified in the main text and appendix. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 27}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: All the experiments to measure the predictive performance of the proposed algorithm were repeated several times to estimate the standard deviation of the score. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 27}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The computer resource was clarified in the appendix. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 28}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The conducted research conforms the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 28}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: This work proposed a generic algorithm that is not tied to specific applications. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 28}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 29}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: This work proposed a generic algorithm. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: The credit and license were appropriately included in the text and source code. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 30}, {"type": "text", "text": "Justification: The documentation and license were included in the source code. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 30}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: This work does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 30}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: This work does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 30}]