[{"figure_path": "cuO0DenqMl/tables/tables_8_1.jpg", "caption": "Table 1: The NLLs and RMSEs for each dataset, where the best score is underlined and the scores whose standard deviation ranges include the best score are in bold. Results of MCDropout, DEnsembles, CDropout, NGBoost, and DEvidential were reported in [9], [10], [55], [32] and [13] respectively.", "description": "This table presents the negative log-likelihood (NLL) and root mean squared error (RMSE) for several regression datasets.  It compares the performance of Wasserstein Gradient Boosting (WGBoost) against several other methods (MCDropout, Deep Ensemble, Concrete Dropout, NGBoost, and Deep Evidential Regression). The best result for each metric is underlined, and results within one standard deviation of the best are shown in bold.  The results for the comparison methods are cited from other published works.", "section": "4.2 Probabilistic Regression Benchmark"}, {"figure_path": "cuO0DenqMl/tables/tables_9_1.jpg", "caption": "Table 2: The classification accuracies and OOD detection PR-AUCs for each dataset, where the best score is underlined and in bold. The results other than WEvidential were reported in [14].", "description": "This table presents the classification accuracy and out-of-distribution (OOD) detection performance (measured by the area under the precision-recall curve, PR-AUC) of the proposed Wasserstein Gradient Boosting (WGBoost) algorithm and four other methods (MCDropout, DEnsemble, DDistillation, and PNetwork) on two real-world datasets (segment and sensorless).  The best performance for each metric is highlighted in bold and underlined.", "section": "4.3 Classification and Out-of-Distribution Detection"}, {"figure_path": "cuO0DenqMl/tables/tables_14_1.jpg", "caption": "Table 1: The NLLs and RMSEs for each dataset, where the best score is underlined and the scores whose standard deviation ranges include the best score are in bold. Results of MCDropout, DEnsembles, CDropout, NGBoost, and DEvidential were reported in [9], [10], [55], [32] and [13] respectively.", "description": "This table compares the performance of the proposed WEvidential model against five other probabilistic regression methods (MCDropout, Deep Ensemble, Concrete Dropout, Natural Gradient Boosting, and Deep Evidential Regression) across eight benchmark datasets.  The results are presented in terms of the negative log-likelihood (NLL) and root mean squared error (RMSE), two common metrics used to evaluate the performance of regression models.  The best performing model for each metric and dataset is underlined, and models with standard deviations that overlap the best score are shown in bold.", "section": "4.2 Probabilistic Regression Benchmark"}, {"figure_path": "cuO0DenqMl/tables/tables_22_1.jpg", "caption": "Table 1: The NLLs and RMSEs for each dataset, where the best score is underlined and the scores whose standard deviation ranges include the best score are in bold. Results of MCDropout, DEnsembles, CDropout, NGBoost, and DEvidential were reported in [9], [10], [55], [32] and [13] respectively.", "description": "This table compares the negative log-likelihood (NLL) and root mean squared error (RMSE) of WEvidential against five other probabilistic regression methods across various datasets.  The best performing method for each metric on each dataset is highlighted.", "section": "4.2 Probabilistic Regression Benchmark"}, {"figure_path": "cuO0DenqMl/tables/tables_23_1.jpg", "caption": "Table 5: The OOD detection performance of WEvidential, NGBoost, and RForest on the segment dataset, where WEvidential (Entropy) indicates the result of WEvidential based on the entropy.", "description": "This table compares the out-of-distribution (OOD) detection performance of three different algorithms: WEvidential, NGBoost, and Random Forest (RForest), on the `segment` dataset.  WEvidential is evaluated twice, once using its standard OOD metric and again using entropy as an alternative.  The results show the accuracy and OOD detection performance (PR-AUC) for each algorithm.  The goal is to assess which algorithm best identifies out-of-distribution samples.", "section": "4.3 Classification and Out-of-Distribution Detection"}, {"figure_path": "cuO0DenqMl/tables/tables_23_2.jpg", "caption": "Table 6: The classification accuracy and OOD detection performance of WEvidential on the synthetic dataset for different learning rates, where the number of weak learners is fixed to 4000.", "description": "This table presents the results of a simulation study on a synthetic dataset to evaluate the performance of the WEvidential algorithm with varying learning rates.  The number of weak learners was kept constant at 4000. The table shows the classification accuracy and out-of-distribution (OOD) detection performance (measured by the area under the precision-recall curve, or PR-AUC) for each learning rate.", "section": "4.3 Classification and Out-of-Distribution Detection"}, {"figure_path": "cuO0DenqMl/tables/tables_24_1.jpg", "caption": "Table 7: The classification accuracy and OOD detection performance of WEvidential on the synthetic dataset for different numbers of weak learners, where the learning rate is fixed to 0.4.", "description": "This table presents the results of a simulation study conducted to evaluate the impact of the number of weak learners on the performance of the WEvidential algorithm. The study used a synthetic dataset and varied the number of weak learners while keeping the learning rate constant at 0.4. The table shows that increasing the number of weak learners improves classification accuracy, but the effect on OOD detection performance is less clear.", "section": "4.3 Classification and Out-of-Distribution Detection"}]